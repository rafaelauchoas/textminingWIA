Artificial Intelligence 174 (2010) 105–132Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintBounded treewidth as a key to tractability of knowledge representationand reasoning ✩Georg Gottlob a, Reinhard Pichler b, Fang Wei c,∗a Computing Laboratory, Oxford University, Oxford OX1 3QD, UKb Institut für Informationssysteme, Technische Universität Wien, A-1040 Vienna, Austriac Institut für Informatik, Albert-Ludwigs-Universität Freiburg, D-79110 Freiburg i. Br., Germanya r t i c l ei n f oa b s t r a c tArticle history:Received 9 October 2008Received in revised form 6 October 2009Accepted 15 October 2009Available online 20 October 2009Keywords:Fixed-parameter tractabilityTreewidthMonadic datalogAbductionClosed world reasoningDisjunctive logic programming1. IntroductionSeveral forms of reasoning in AI – like abduction, closed world reasoning, circumscription,and disjunctive logic programming – are well known to be intractable. In fact, many ofthe relevant problems are on the second or third level of the polynomial hierarchy. Inthis paper, we show how the notion of treewidth can be fruitfully applied to this area. Inparticular, we show that all these problems become tractable (actually, even solvable inlinear time), if the treewidth of the involved formulae or programs is bounded by someconstant.Clearly, these theoretical tractability results as such do not immediately yield feasiblealgorithms. However, we have recently established a new method based on monadicdatalog which allowed us to design an efficient algorithm for a related problem in thedatabase area. In this work, we exploit the monadic datalog approach to construct newalgorithms for logic-based abduction.© 2009 Elsevier B.V. All rights reserved.In the nineteen-nineties, several forms of reasoning in AI – like abduction, closed world reasoning, circumscription, anddisjunctive logic programming – were shown to be highly intractable. In fact, many relevant problems in this area are onthe second or even third level of the polynomial hierarchy [20–22].In recent years, parameterized complexity has evolved as an interesting approach to dealing with intractability [19]. It hasturned out that many hard problems become tractable if some problem parameter is fixed or bounded by a fixed constant.If a problem enjoys this property we speak of fixed-parameter tractability (FPT, for short). In the arena of graph problems,an important parameter thus investigated is the so-called treewidth of a graph G, which is a measure of the “tree-likeness”of G. If the treewidth of the graphs under consideration is bounded by a fixed constant, then many otherwise intractableproblems become tractable, e.g., 3-colorability, Hamiltonicity, etc. More generally, treewidth can be considered for arbitraryfinite structures. Treewidth has also been applied to some areas of AI, notably to constraint satisfaction [2].A deep result and mathematical tool for deriving new FPT-results is Courcelle’s Theorem [14], which states that if someproperty of finite structures is expressible by a Monadic Second Order (MSO, for short) sentence then this property isdecidable in linear time for structures whose treewidth is bounded by a fixed constant. Courcelle’s Theorem has been further✩This is an extended and enhanced version of results published in Gottlob, Pichler and Wei (2006, 2008) [29,31]. The work was supported by the AustrianScience Fund (FWF), project P20704-N18.* Corresponding author.E-mail addresses: georg.gottlob@comlab.ox.ac.uk (G. Gottlob), pichler@dbai.tuwien.ac.at (R. Pichler), fwei@informatik.uni-freiburg.de (F. Wei).0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.10.003106G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132developed in several works, e.g., in the recent paper [24], where also an efficient algorithm for counting the solutions of aSAT or Generalized SAT problem is presented.In this paper, we revisit several intractable problems in AI. Our goal is to harness the powerful machinery of Courcelle’sTheorem in the area of knowledge representation and reasoning. We show that virtually all decision problems arising in thecontext of abduction, closed world reasoning, circumscription, and disjunctive logic programming become tractable if thetreewidth of the involved formulae or programs is bounded by some constant. The central idea for deriving these FPT-resultsis to encode the decision problems in terms of MSO sentences.Clearly, an MSO description as such is not an algorithm and it is a highly non-trivial task to turn a theoretical tractabilityresult based on Courcelle’s Theorem into a feasible computation. Actually, recipes to devise concrete algorithms based onCourcelle’s Theorem can be found in the literature, see e.g. [3,12,11,25]. The basic idea of these algorithms is to transformthe MSO evaluation problem into an equivalent tree language recognition problem and to solve the latter via an appropri-ate finite tree automaton (FTA) [17,46,47]. In theory, this generic method of turning an MSO description into a concretealgorithm looks very appealing. However, in practice, it has turned out that even relatively simple MSO-formulae may leadto a “state explosion” of the FTA [26,41]. Hence, it was already stated in [32] that the algorithms derived via Courcelle’sTheorem are “useless for practical applications”. The main benefit of Courcelle’s Theorem is that it provides “a simple wayto recognize a property as being linear time computable”. In other words, proving the FPT of some problem by showingthat it is MSO expressible is the starting point (rather than the end point) of the search for an efficient algorithm.In [30], we proposed monadic datalog (i.e., datalog where all intensional predicate symbols are unary) as a practical toolfor devising efficient algorithms in situations where the FPT has been established via Courcelle’s Theorem. Above all, weproved that if some property of finite structures is expressible in MSO then this property can also be expressed by meansof a monadic datalog program over the decomposed structure: we mean by this that the original structure is augmented withnew elements and new relations that encode one of its tree decompositions. Moreover, this approach was put to work bydesigning an efficient algorithm for the PRIMALITY problem (i.e., the problem of deciding if some attribute is part of a keyin a given relational schema). In this paper, we show that this monadic datalog approach can also be applied to difficultknowledge representation and reasoning tasks. We thus present new algorithms for logic-based abduction and we report onexperimental results with an implementation of these algorithms.1.1. Summary of resultsThe main contribution of our paper is twofold: First, we prove the fixed-parameter tractability of many relevant decisionproblems arising in abduction, closed world reasoning, circumscription, and disjunctive logic programming. Second, we showhow such theoretical tractability results can be actually turned into feasible computations. We only present new algorithmsfor logic-based abduction. However, the ideas underlying the construction of these algorithms are, of course, also applicableto the hard problems in the other areas mentioned above. Hence, the FPT-results shown here indeed open the grounds forthe development of new parameterized algorithms for these problems.1.2. Structure of the paperThe rest of the paper is organized as follows. After recalling some basic definitions and results in Section 2, we proveseveral new fixed-parameter tractability results via Courcelle’s Theorem in Section 3. In Section 4, we present our newalgorithms for logic-based abduction. Experimental results are discussed in Section 5. A conclusion is given in Section 6.Appendices A and B are provided in order to encapsulate some lengthy, technical details which are not required for theunderstanding of the main body of the text.2. Preliminaries2.1. Finite structures and treewidthAiLet τ = {R1, . . . , R K } be a set of predicate symbols. A finite structure A over τ (a τ -structure, for short) is given by a⊆ Aαi , where αi denotes the arity of R i ∈ τ . All structures and trees consideredfinite domain A = dom(A) and relations Rin this work are assumed to be finite. Hence, in the sequel, the finiteness will usually not be explicitly mentioned.A tree decomposition T of a τ -structure A is defined as a pair (cid:5)T , ( At )t∈T (cid:6) where T is a rooted tree and each At is asubset of A with the following properties: (1) Every a ∈ A is contained in some At . (2) For every R i ∈ τ and every tuple} ⊆ At . (3) For every a ∈ A, the set {t | a ∈ At} induces a(a1, . . . , aαi ) ∈ Rsubtree of T ., there exists some node t ∈ T with {a1, . . . , aαiCondition (3) above is usually referred to as the connectedness condition. The sets At are called the bags (or blocks) of T .The width of a tree decomposition (cid:5)T , ( At)t∈T (cid:6) is defined as max{| At| | t ∈ T } − 1. The treewidth of A is the minimal widthof all tree decompositions of A. It is denoted as tw(A). Note that forests are the simple loop-free graphs of treewidth atmost 1.AiFor given w (cid:2) 1, it can be decided in linear time if some structure has treewidth at most w. Moreover, in case of apositive answer, a tree decomposition of width w can be computed in linear time [8]. Strictly speaking, the result in [8]G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132107Fig. 1. Tree decomposition T of formula F .refers to tree decompositions of graphs rather than arbitrary structures. However, we can associate a graph G (the so-calledprimal or Gaifman graph) with every structure A by taking the domain elements as the vertices of the graph. Moreover,two vertices are adjacent in G if and only if the corresponding domain elements jointly occur in some tuple in A. It can beeasily shown that G has precisely the same tree decompositions as A.Unfortunately, it has been shown that this linear time algorithm is mainly of theoretical interest and its practical use-fulness is limited [39]. Recently, considerable progress has been made in developing heuristic-based tree decompositionalgorithms which can handle graphs with moderate size of several hundreds of vertices [39,9,49,10]. Moreover, in somecases, a tree decomposition of low width may be obtained from a given problem in a “natural way”. For instance, in [48],it was shown that the tree-width of the control-flow graph of any goto-free C program is at most six. A similar result wasshown for Java programs in [33]. These results opened the ground for the efficient implementation of various compileroptimization tasks like the register allocation problem.In this paper, we shall constantly have to deal with propositional formulae in CNF or, analogously, with clause sets.A propositional formula F in CNF (respectively a clause set F ) can be represented as a structure A over the alphabetτ = {cl(.), var(.), pos(. , .), neg(. , .)} where cl(z) (respectively var(z)) means that z is a clause (respectively a variable) in Fand pos(x, c) (respectively neg(x, c)) means that x occurs unnegated (respectively negated) in the clause c. We define thetreewidth of F as the treewidth of this structure A, i.e., tw(F ) = tw(A).Example 2.1. Consider the propositional formula F withF = (x1 ∨ ¬x2 ∨ x3) ∧ (¬x1 ∨ x4 ∨ ¬x5) ∧ (x2 ∨ ¬x4 ∨ x6).The corresponding structure A can be identified with the following set of ground atoms(cid:2)A =var(x1), var(x2), var(x3), var(x4), var(x5), var(x6),cl(c1), cl(c2), cl(c3),pos(x1, c1), pos(x3, c1), pos(x4, c2), pos(x2, c3), pos(x6, c3),(cid:3)neg(x2, c1), neg(x1, c2), neg(x5, c2), neg(x4, c3).A tree decomposition T of this structure is given in Fig. 1. Note that the maximal size of the bags in T is 3. Hence, thetree-width is at most 2. On the other hand, it is easy to check that the tree-width of T cannot be smaller than 2. In orderto see this, we consider the ground atoms pos(x1, c1), neg(x2, c1) pos(x2, c3), neg(x4, c3) pos(x4, c2), and neg(x1, c2) in Aas (undirected) edges of a graph. Clearly, these edges form a cycle. However, as we have recalled above, only forests arethe simple loop-free graphs of treewidth at most 1. This tree decomposition is, therefore, optimal and we have tw(F ) =tw(A) = 2.In [30], it was shown that any tree decomposition can be transformed into the following normal form in linear time:Definition 2.2. Let A be a structure with tree decomposition T = (cid:5)T , ( At )t∈T (cid:6) of width w. We call T normalized if T is arooted tree and conditions 1–4 are fulfilled: (1) All bags consist of pairwise distinct elements a0, . . . , ak with 0 (cid:3) k (cid:3) w.(2) Every internal node t ∈ T has either one or two child nodes. (3) If a node t has one child node t, then the bag At isobtained from At(cid:9) either by removing one element or by introducing a new element. (4) If a node t has two child nodesthen these child nodes have identical bags as t.(cid:9)Example 2.3. Recall the tree decomposition T from Fig. 1. Clearly, T is not normalized in the above sense. However, in canbe easily transformed into a normalized tree decomposition T (cid:9), see Fig. 2.Let A be a τ -structure with τ = {R1, . . . , R K } and domain A and let w (cid:2) 1 denote the treewidth. Then we define theextended signature τtd asτtd = τ ∪ {root, leaf , child1, child2, bag}108G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132Fig. 2. Normalized tree decomposition T (cid:9)of formula F .where the unary predicates root and leaf as well as the binary predicates child1 and child2 are used to represent the treeT of the (normalized) tree decomposition in the obvious way. For instance, we write child1(s1, s) to denote that s1 is eitherthe first child or the only child of s. Finally, bag has arity k + 2 with k (cid:3) w, where bag(t, a0, . . . , ak) means that the bag atnode t is (a0, . . . , ak). By slight abuse of notation, we tacitly assume that bag is overloaded for various values of k. Note thatthe possible values of k are bounded by a fixed constant w. For any τ -structure A with tree decomposition T = (cid:5)T , ( At )t∈T (cid:6)of width w, we write Atd to denote the decomposed structure or the structure decomposing A. This structure is obtained inAthe following way: The domain of Atd is the union of dom(A) and the nodes of T . In addition to the relations Ri withR i ∈ τ , the structure Atd also contains relations for each predicate root, leaf , child1, child2, and bag thus representing thetree decomposition T . By combining results from [8] and [30], one can compute Atd from A in linear time w.r.t. the sizeof A.Example 2.4. Recall the propositional formula F = (x1 ∨ ¬x2 ∨ x3) ∧ (¬x1 ∨ x4 ∨ ¬x5) ∧ (x2 ∨ ¬x4 ∨ x6) from Example 2.1.We have already shown how F can be represented as a structure A over the signature τ = {cl, var, pos, neg}. In order torepresent also the tree decomposition T (cid:9)from Fig. 2 by the structure Atd, we have to add the following ground atomsto A: {root(t1), child1(t2, t1), child2(t3, t1), child1(t4, t2), . . . , leaf (t16), leaf (t19), leaf (t20), bag(t1, x1, x2, x4), bag(t2, x1, x2, x4),bag(t3, x1, x2, x4), bag(t4, x1, x2), . . .}. (The numbering of the nodes t1, t2, t3, . . . corresponds to a breadth-first traversalof T (cid:9).)2.2. MSO and monadic datalogMonadic Second Order logic (MSO) extends First Order logic (FO) by the use of set variables (usually denoted by upper caseletters), which range over sets of domain elements. In contrast, the individual variables (which are usually denoted by lowercase letters) range over single domain elements. Atomic formulae in an MSO-formula ϕ over a τ -structure have one of thefollowing forms: (i) atoms with some predicate symbol from τ , (ii) atoms whose predicate symbol is a monadic secondorder variable (i.e., a set variable), or (iii) equality atoms. It is convenient to use symbols like ⊆, ⊂, ∩, ∪, and → with theobvious meaning as abbreviations. An MSO-formula ϕ(x) with exactly one free individual variable is called a unary query.The importance of MSO-formulae in the context of parameterized complexity comes from the following result:Theorem 2.5. (See [14].) Let ϕ be an MSO-sentence over some signature τ and let A be a τ -structure of treewidth w. Evaluating thesentence ϕ over the structure A can be done in time O( f (|ϕ(x)|, w) ∗ |A|) for some function f .In the sequel, we shall simply say that structures “have bounded treewidth” without explicitly mentioning w. Note thatthe fixed-parameter linearity according to Theorem 2.5 only applies to the data complexity, i.e. the formula ϕ is fixed. Thereis no such FPT-result, if we consider the combined complexity instead (i.e. also ϕ is part of the input).Datalog programs are function-free logic programs. Each of them has a unique minimal model. Evaluating a datalog pro-gram P over a structure A comes down to computing the minimal model of P ∧ A. This minimal model coincides withthe set of (ground) facts obtained as the least fixpoint of the immediate consequence operator, for details see e.g. [1]. In thesequel, we shall refer to a program P together with a structure A as an interpreted program. A datalog program is calledground if it contains no variables. Evaluating a datalog program P over a structure A is equivalent to evaluating the follow-ing ground program P (cid:9)contains all possible instances of r that we get by instantiatingevery variable in r by a constant occurring either in P or in A (the set of all these constants is referred to as the activedomain). In general, P (cid:9)is exponentially bigger than P . However, in some cases, only a small fraction of the set of possibleground instances of every rule r is necessary as we shall see, e.g., in the proof of Theorem 4.2.over A: For every clause r in P , P (cid:9)Predicates occurring only in the body of rules are called extensional, while predicates occurring also in the head of somerule are called intensional.G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132109Let A be a τ -structure with domain A and relations R⊆ Aαi , where αi denotes the arity of R i ∈ τ .In the context of datalog, it is convenient to think of the relations Ras sets of ground atoms. The set of all such groundatoms of a structure A is referred to as the extensional database (EDB) of A, i.e.: a ground atom R i(¯a) is in the EDB if andonly if ¯a ∈ RAi.A1 , . . . , RAK with RAiAiIn [30], the following connection between unary MSO queries over structures with bounded treewidth and monadicdatalog was established:Theorem 2.6. Let τ be a signature and let w (cid:2) 1. Every unary MSO-query ϕ(x) over τ -structures of treewidth w is also definable bya monadic datalog program over τtd.Moreover, for every τ -structure A, the resulting program can be evaluated in time O( f (|ϕ(x)|, w) ∗ |A|) for some function f ,where |A| denotes the size of A.Note that the connection between logic programming (with functions) and tree automata has already been studied muchearlier, see e.g. [40,23]. As far as the upper bound on the complexity is concerned, the above theorem is a special case ofTheorem 4.12 in [25].3. Fixed-parameter tractability via Courcelle’s TheoremIn this section, we show the fixed-parameter tractability w.r.t. the treewidth for many decision problems in the areaof disjunctive logic programming, closed world reasoning, circumscription, and abduction. Fundamental to these problemsis the evaluation of a propositional formula in an interpretation. We therefore start our exposition with the SAT problem,whose fixed-parameter tractability for various parameters (including cliquewidth and treewidth) was already shown in [15].3.1. SAT problemA propositional formula F is built up from propositional variables denoted as Var(F ) and the logical connectives ∨, ∧,and ¬. An interpretation of F is a mapping that assigns one of the truth values true or false to each variable occurring inF . It is convenient to identify interpretations with subsets X of Var(F ) with the intended meaning that the variables inX evaluate to true, while all other variables evaluate to false. If F evaluates to true in X , then X is called a model of F .(cid:9) ⊂ X . We write F 1 |(cid:14) F 2 if the formulaMoreover, X is called a minimal model, if there exists no model XF 1 → F 2 is valid, i.e., F 1 → F 2 is true in every interpretation X ⊆ Var(F 1) ∪ Var(F 2). Likewise, if F = {F 1, . . . , Fm} is a set offormulae and F a single formula, we write F |(cid:14) F if the formula F 1 ∧ · · · ∧ Fm → F is valid. The following result is folklore[5,44].of F with X(cid:9)Theorem 3.1. For every propositional formula F , there exists a formula Fhold:(cid:9)in CNF, with Var(F ) ⊆ Var(F(cid:9)), s.t. the following properties(1) For every interpretation X , s.t. X is a model of F , there exists an interpretation Y with X ⊆ Y and (Y \ X) ⊆ (Var(F(cid:9)) \ Var(F )),s.t. Y is a model of F.(2) For every interpretation Y , s.t. Y is a model of F(cid:9)(cid:9), the interpretation Y ∩ Var(F ) is a model of F .Moreover, such an F(cid:9)can always be found in linear time and, thus, also the size of F(cid:9)is linearly bounded by the size of F .(cid:9)Proof. Given a propositional formula F with variables in V , we first compute the parse tree T of F . This is clearly feasiblein linear time. This parse tree can be considered as a Boolean circuit where each node in the tree corresponds to a gate,the leaf nodes (labeled with variables) are the input gates, the root node is the output gate and each edge of the parse treeis oriented from the child to its parent. The construction of a CNF follows the well-known reduction from the CIRCUIT SATproblem to the SAT problem, see e.g. [44], Example 8.3:In addition to the variables V , our CNF Fcontains one additional variable for each internal node g of the parse tree T .(cid:9)For leaf nodes, we may simply use the corresponding propositional variable x as the name of this node. The clauses of Fare obtained for each internal node g of T by a case distinction over the possible operations represented by this node:(1) Suppose that g corresponds to a NOT-operation. Then g has precisely one incoming arc, say from node h. Then weadd the two clauses (¬g ∨ ¬h) and (g ∨ h) to F. That is, these clauses express the condition g ↔ (¬h).(cid:9)(2) Suppose that g corresponds to an AND-operation. Then g has two incoming arcs, say from nodes h1 and h2. Then. That is, these clauses express the conditionwe add the three clauses (¬g ∨ h1), (¬g ∨ h2), and (¬h1 ∨ ¬h2 ∨ g) to Fg ↔ (h1 ∧ h2).(cid:9)(3) Suppose that g corresponds to an OR-operation. Then g has two incoming arcs, say from nodes h1 and h2. Then. That is, these clauses express the conditionwe add the three clauses (g ∨ ¬h1), (g ∨ ¬h2), and (h1 ∨ h2 ∨ ¬g) to Fg ↔ (h1 ∧ h2).(cid:9)(cid:9). Clearly, F(cid:9)is in CNF. The desired propertiesFinally, for the root node r of T , we also add the one-literal clause r to F(cid:9)(in particular, the sat-equivalence with F ) are easily verified. (cid:2)of F110G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132(cid:9)Note that the CNF Fconstructed in the proof of Theorem 3.1 is uniquely determined (up to variable renaming) by theas the canonical CNF of F . It can be represented byparse tree of F and hence by the form of F itself. We shall refer to F(cid:9)(. , .)}, s.t. var(.) is used to identify the variables occurringa τ -structure A(F ) with τ = {var(.), clin F and clin the usual way. In Section 2.1, we have definedthe treewidth of a propositional formula F in CNF (or, analogously, of a clause set) as the treewidth of the structurerepresenting F . Likewise, we can define the treewidth of an arbitrary propositional formula F as the treewidth of thecanonical CNF Fof F or, equivalently, as the treewidth of the τ -structure A(F ).(cid:9)(. , .) are used to represent the CNF F(cid:9)(. , .), neg(cid:9)(. , .), neg(cid:9)(.), pos(cid:9)(.), pos(.), var(.), var(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)The notion of treewidth can be naturally extended to more than one formula in CNF, e.g., let F 1 and F 2 be two propo-sitional formulae in CNF. Then (F 1, F 2) can be represented by a τ -structure A(F 1, F 2) with τ = {cli(.), vari(.), posi(. , .),negi(. , .) | 1 (cid:3) i (cid:3) 2}. We define the treewidth of F 1 and F 2 as the treewidth of the structure A. We thus havetw(F 1, F 2) = tw(A(F 1, F 2)) = tw(F 1 ∧ F 2). If F 1 and F 2 are not in CNF, then we can represent these formulae by a structure(cid:9)A over the signature τ = {vari(.), cli(. , .) | 1 (cid:3) i (cid:3) 2}, where vari(.) is used to identify the variables(cid:9)(cid:9)i(.), posoccurring in F i and cli in the usual way. Again, the treewidthof F 1 and F 2 is defined as the treewidth of the structure A.(cid:9)i(.), pos(cid:9)i(. , .) are used to represent the CNF F(cid:9)i(.), var(cid:9)i(. , .), neg(cid:9)i(. , .), neg(cid:9)i(.), varFrom results in [15], the following relationship between CNF-formulae and MSO can be easily derived:Theorem 3.2. Let F be a propositional formula in CNF which is given by a τ -structure A with τ = {cl, var, pos, neg}. Moreover, let Xdenote a set of the variables with X ⊆ Var(F ). Then the property that X is a model of F can be expressed by an MSO-formula – referredto as model( X, F ) – over the signature τ .Proof. We define the MSO-formula model( X, F ) as follows:(cid:6)pos(z, c) ∧ z ∈ Xmodel(X, F ) := (∀c) cl(c) → (∃z)(cid:4)(cid:5)(cid:5)∨neg(z, c) ∧ z /∈ X(cid:6)(cid:7).This MSO-formula expresses the following equivalence: F is true in X if and only if every clause of F is true in X . This inturn is the case if and only if every clause contains a positive occurrence of a variable z ∈ X or a negative occurrence of avariable z /∈ X . (cid:2)Clearly, Theorem 3.2 together with Courcelle’s Theorem immediately yields the fixed-parameter tractability of the SATproblem w.r.t. the treewidth of the τ -structure A (with τ = {cl, var, pos, neg}) representing the propositional formula [15].Actually, even if F is not in CNF, the property that X is a model of F can be expressed in terms of MSO:Theorem 3.3. Let F be a propositional formula with canonical CNF F(cid:9){var, cl(cid:9)can be expressed by an MSO-formula – referred to as model(cid:9)and let F be given by a τ -structure A with τ =(cid:9)}. Moreover, let X denote a set of the variables with X ⊆ Var(F ). Then the property that X is a model of F( X, F ) – over the signature τ .(cid:9), neg(cid:9), pos, varProof. We define the MSO-formula model(cid:9)( X, F ) as follows:(cid:9)Ext F (X, X(cid:9)model) := X ⊆ X(cid:9)(X, F ) := (∃X(cid:4)(cid:9) ∧ (∀z)(cid:4))Ext F (X, X(z ∈ X(cid:9)) ∧(cid:5)var(cid:6)(cid:7).)(cid:9)(cid:9), F(cid:9) ∧ z /∈ X) →(cid:5)model(X(cid:9)(cid:9)(z) ∧ ¬var(z)(cid:6)(cid:7),The auxiliary formula Ext F ( X, Xthe subformula model( X(cid:9), F(cid:9)) is precisely the CNF-evaluation from Theorem 3.2. (cid:2)(cid:9)) means that Xis an extension of the interpretation X to the variables in F(cid:9). Moreover,Finally, we also provide an MSO-formula expressing that F 1 |(cid:14) F 2 holds for two propositional formulae F 1 and F 2.Theorem 3.4. Let F 1, F 2 be propositional formulae with canonical CNFs Fτ = {vari(.), clMSO-formula – referred to as implies(F 1, F 2) – over the signature τ .(cid:9)i(. , .), neg(cid:9)i(.), pos(cid:9)i(.), var(cid:9)2 and let F 1, F 2 be given by a τ -structure A with(cid:9)i(. , .) | 1 (cid:3) i (cid:3) 2}. Then the property that F 1 |(cid:14) F 2 holds can be expressed by means of an(cid:9)1, FProof. We define the MSO-formula implies(F 1, F 2) as follows:(cid:5)(cid:9)model(X, F 2)(cid:6)(cid:7).(cid:4)(∀ X)(cid:9)model(X, F 1) →(cid:9)The subformulae modelaccording to Theorem 3.3. (cid:2)( X, F i) with i ∈ {1, 2} are the MSO-formulae expressing the evaluation of propositional formulae F i3.2. Disjunctive logic programmingA disjunctive logic program (DLP, for short) P is a set of DLP clauses a1 ∨ · · · ∨ an ← b1, . . . , bk, ¬bk+1, . . . , ¬bm. Let I bean interpretation. Then the Gelfond–Lifschitz reduct P I of P w.r.t. I contains precisely the clauses a1 ∨ · · · ∨ an ← b1, . . . , bk,G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132111s.t. for all i ∈ {k + 1, . . . , m}, bi /∈ I . An interpretation I is called a disjunctive stable model (DSM, for short) if and only if I isa minimal model of P I [27,45].Without any restrictions, the following problems are all on the second level of the polynomial hierarchy [22]:• Consistency: Does P have a DSM?• Brave Reasoning: Is a propositional formula F true in at least one DSM of P (written as P |(cid:14)b F )?• Cautious Reasoning: Is a propositional formula F true in all DSMs of P (written as P |(cid:14)c F )?The intuition of stable models is as follows: Consider the reduct of a program P with respect to some interpretation I .We can think of I as making an assumption about what is true and what is false. Consequently, we may delete those rulesfrom the program whose body is definitely false under this assumption, thus arriving at the reduct P I . Now we can computethe positive information derivable from P I . If the result is I itself, then I is called a stable model.Negation under the stable model semantics has received a lot of interest since it significantly increases the expressivepower of logic programs. For instance, even without making use of disjunctions in the rule heads, we can express the classi-cal NP-complete 3-colorability problem by the following program: Let G = (V , E) be a graph with vertices V = {v 1, . . . , vn}and edges E. Then G can be represented by a logic program consisting of the following facts:{ei, j | 1 (cid:3) i, j (cid:3) n and there exists an edge between v i and v j}.The 3-colorability of G is expressed by adding the following clauses to the program (note that the commas on the right-handside of the rules stand for logical “and”):{ri ← ¬gi, ¬bi; gi ← ¬ri, ¬bi; bi ← ¬ri, ¬gi | 1 (cid:3) i (cid:3) n},{ ← ei, j, ri, r j; ← ei, j, gi, g j; ← ei, j, bi, b j | 1 (cid:3) i, j (cid:3) n}.The stable models of this program correspond to the proper 3-colorings of the graph G. Intuitively, the first collectionof clauses makes sure that, in every stable model, exactly one of the variables ri, gi, bi is true for each i ∈ {1, . . . , n}. Thesecond collection of clauses excludes those “colorings” which would assign identical colors to the endpoints of some edge.In particular, the question if a given graph has at least one proper 3-coloring is equivalent to the consistency problem ofthe above described program, i.e., does the program have at least one stable model?Suppose that we want to ask if there exists at least one proper 3-coloring in which the vertices v 1, v 2, and v 3 havethe same color. This question corresponds to the brave reasoning problem P |(cid:14)b F , where P is the above program andF = (r1 ∧ r2 ∧ r3) ∨ (g1 ∧ g2 ∧ g3) ∨ (b1 ∧ b2 ∧ b3). Likewise, the question if v 1, v 2, and v 3 have the same color inevery proper 3-coloring corresponds to the cautious reasoning problem P |(cid:14)c F with P and F as before. Many more ex-amples of (disjunctive) logic programs with negation for solving problems in diverse application areas can be found athttp://www.kr.tuwien.ac.at/research/projects/WASP/.Below, we show that, if the treewidth of the programs P and formulae F under consideration is bounded by a constant,then we get much more favorable complexity results than in the general case. Suppose that a DLP P is given by a τ -−(. , .)}, s.t. var P (.) and clP (.) encode the variables and clauses of P .structure with τ = {var P (.), clP (.), H(. , .), B−(x, c)) means that x occurs unnegatedMoreover, H(x, c) means that x occurs in the head of c and B(respectively negated) in the body of c. Then we have:+(x, c) (respectively B+(. , .), B(cid:9)} as in Theorem 3.3. TheTheorem 3.5. Consider the signatures τ1 = {var P , clP , H, BConsistency problem of DLPs can be expressed by means of an MSO-sentence over the signature τ1. Likewise, the Brave Reasoningand Cautious Reasoning problem of DLPs can be expressed by means of MSO-sentences over the signature τ1 ∪ τ2.(cid:9)−} as above and τ2 = {var, cl(cid:9), neg(cid:9), pos+, B, var(cid:9)Proof. Recall from Theorem 3.3 that the property that X is a model of F can be expressed by the MSO-formula modelover the signature τ2. We thus have:( X, F )GL(X, Y ) := (∀c)clP (c) → (∃z)(cid:4)DSM(X) := GL(X, X) ∧ (∀ Z )H(z, c) ∧ z ∈ X∨(cid:7)Z ⊂ X → ¬GL(Z , X)(cid:6)+B(z, c) ∧ z /∈ X(cid:6)(cid:5)∨−B(z, c) ∧ z ∈ Y(cid:6)(cid:7),(cid:5),(cid:4)(cid:5)Consistency: (∃X)DSM(X),(cid:4)Brave Reasoning: (∃X)(cid:4)Cautious Reasoning: (∀ X)DSM(X) ∧ modelDSM(X) → model(cid:9)(cid:7)(X, F )(cid:9),(cid:7)(X, F ).The predicates defined above have the following meaning:GL( X, Y ) = “ X is a model of the Gelfond–Lifschitz reduct of the program P w.r.t. the interpretation Y ”.DSM( X) = “ X is a disjunctive stable model of P ”. (cid:2)112G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132Fig. 3. Tree decomposition of structure A(P ) in Example 3.6.Example 3.6. Consider the following DLP:P = {c1 : p ∨ q ← ¬r, c2 : q ← ¬p ∧ ¬s, c3 : s ∨ t ← q}.Clearly, P is consistent since, for instance, {p} is a DSM.The DLP P can be represented by the structure A(P ), which consists of the following ground atoms:(cid:2)A(P ) =var P (p), var P (q), var P (r), var P (s), var P (t),clP (c1), clP (c2), clP (c3),H(p, c1), H(q, c1), H(q, c2), H(s, c3), H(t, c3),−−+−(r, c1), B(p, c2), B(s, c2), B(cid:3)(q, c3).BA tree decomposition T of A(P ) is given in Fig. 3. The MSO-formula GL( X, Y ) from the proof of Theorem 3.5 clearlyevaluates to true over A(P ) for X = {p} and Y = {p}. Moreover, for X = {} and Y = {p}, it evaluates to false. Hence, DSM( X)evaluates to true for X = {p} and, therefore, the consistency of P is correctly established via the MSO-formula (∃ X)DSM( X).The following complexity result is an immediate consequence of Courcelle’s Theorem and Theorem 3.5.(cid:9)} as in Theorem 3.5. LetTheorem 3.7. Consider the signatures τ1 = {var P , clP , H, Ban instance of the Consistency problem of DLPs be given by a τ1-structure A of width w. Then this problem can be solved in timeO( f (w) ∗ |A|) for some function f .(cid:9)−} as above and τ2 = {var, cl(cid:9), neg(cid:9), pos+, B, varLikewise, let an instance of Brave Reasoning problem or the Cautious Reasoning of DLPs be given by a (τ1 ∪ τ2)-structure A ofwidth w. Then this problem can be solved in time O( f(cid:9)(w) ∗ |A|) for some function f(cid:9).3.3. Closed world reasoning and circumscriptionSeveral forms of closed world reasoning (CWR, for short) are proposed in the literature, namely CWA (Closed WorldAssumption), GCWA (Generalized CWA), EGCWA (Extended GCWA), CCWA (Careful CWA), and ECWA (Extended CWA). Theyare defined in terms of the following terminology: Let T (a “theory”) and F be propositional formulae and let (cid:5)P ; Q ; Z (cid:6)be a partition of Var(T ). Then we write M(T ) (respectively MM(T )) to denote the set of all models (respectively of allminimal models) of T . Moreover, we write MM(T ; P ; Q ; Z ) to denote the set of (cid:5)P ; Q ; Z (cid:6)-minimal models of T , i.e.: X ∈MM(T ; P ; Q ; Z ) if and only if X is a model of T and there exists no model Y of T with (Y ∩ P ) ⊂ ( X ∩ P ) and (Y ∩ Q ) =( X ∩ Q ).In [13], several equivalent characterizations of the closure of a theory T under the various CWR rules are provided.Below, we recall those characterizations which are best suited for our purposes here:• CWA(T ) = T ∪ {¬K | K is a positive literal s.t. T (cid:19)|(cid:14) K }.• GCWA(T ) = T ∪ {¬K | K is a positive literal and for every X ∈ MM(T ), K is false in X}.• EGCWA(T ) |(cid:14) F if and only if for every X ∈ MM(T ), X is a model of F .• CCWA(T ; P ; Q ; Z ) = T ∪ {¬K | K is a positive literal from P and for every X ∈ MM(T ; P ; Q ; Z ), K is false in X}.• ECWA(T ; P ; Q ; Z ) |(cid:14) F if and only if, for every X ∈ MM(T ; P ; Q ; Z ), X is a model of F .The Deduction problem of CWR-rule C with C ∈ {CWA, GCWA, EGCWA, CCWA, ECWA} is as follows: Given T and F (andpossibly P , Q , Z ), does C(T ) |(cid:14) F (respectively C(T ; P ; Q ; Z ) |(cid:14) F ) hold? In [20], this problem is shown to be Π p2 -completeor even harder for all rules C (cid:19)= CWA. In [28] it was shown that, in the propositional case, Circumscription coincides withthe ECWA-rule.The above formalisms of closed world reasoning provide a formal basis of various kinds of inferences of facts which arenot explicitly specified in a knowledge base. They give rise to non-monotonic reasoning in the sense that new informationmay invalidate previously derived conclusions. In [13], the classical Tweety example is used to highlight some differencesbetween the above formalisms. Consider the formula, which represents the propositional version of the Tweety example:(cid:5)(cid:6)(b ∧ ¬a) → f∧ b.The intended meaning is as follows: If Tweety is a bird (i.e., b) and Tweety is not abnormal (i.e., ¬a), then Tweety can flyf ). Moreover, we know that Tweety is a bird. We get the following closure under the various CWR-formalisms. For(i.e.,G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132113the CCWA- and ECWA-rule, we assume P = {a} (i.e., P should be minimized), Q = {b} (i.e., Q is fixed), and Z = { f } (i.e., Zvaries).• CWA(T ) = T ∪ {¬a, ¬ f }.• GCWA(T ) = T .• EGCWA(T ) = T ∪ {¬(a ∧ f )}.• CCWA(T ; P ; Q ; Z ) = T ∪ {¬a}.• ECWA(T ; P ; Q ; Z ) = T ∪ {¬a}.The CWA-rule is the only rule which may lead to inconsistency. This is indeed the case in the above example. Only theCCWA- and ECWA-rule allow us to derive the atom f , i.e., Tweety can fly. By the EGCWA-rule, the formula ¬(a ∧ f ) maybe inferred, i.e., Tweety is not abnormal or Tweety cannot fly.Again, for bounded treewidth, we get much better complexity results than in the general case. Let T and F be propo-, respectively. Moreover, suppose that the Deduction problem of CWR-rulesitional formulae with canonical CNFs TC with C ∈ {CWA, GCWA, EGCWA, CCWA, ECWA} is given by a τ -structure with τ = {varT (.), var F (.), varT (cid:9) (.), var F (cid:9) (.), clT (cid:9) (.),clF (cid:9) (.), . . .} ∪ {P , Q , Z }. The predicates varT (.) and var F (.) are used to identify the variables in the original formulae T and F .The CNFs are encoded as usual by means of the predicates varT (cid:9) (.), var F (cid:9) (.), clT (cid:9) (.), clF (cid:9) (.), etc. Finally, the partition (cid:5)P ; Q ; Z (cid:6)of Var(T ) is encoded by the unary predicates P , Q , Z . Of course, for the CWR-rules CWA, GCWA, and EGCWA, the predicatesP , Q , Z may be omitted. Then we have:and F(cid:9)(cid:9)(cid:9)Theorem 3.8. Consider the signature τ = {varT , var F , varT (cid:9) , var F (cid:9) , clT (cid:9) , clF (cid:9) , posT (cid:9) , posF (cid:9) , negT (cid:9) ,F (cid:9) , P , Q , Z } as described above. Forall of the CWR-rules CWA, GCWA, EGCWA, CCWA, and ECWA, the Deduction problem (and, hence, also Circumscription) can beexpressed by MSO sentences over the signature τ (of course, for CWA, GCWA, EGCWA, the predicates P , Q , and Z are not needed).Proof. GCWA and EGCWA are special cases of CCWA and ECWA with Q = Z = ∅. Moreover, as mentioned above, Circum-scription is equivalent to the ECWA-rule. The remaining cases are expressed by the following MSO-sentences, which make(cid:9)use of the formulae model( X, F ) and implies(F 1, F 2) according to Theorems 3.3 and 3.4, respectively:(Y ∩ P ) ⊂ (X ∩ P ) ∧ (Y ∩ Q ) = (X ∩ Q ) ∧ model(cid:9)(cid:7)(Y , T ),(cid:9)(cid:4)(X, T ) ∧ ¬(∃Y )MM(X) := modelvar(z) := varT (z) ∨ var F (z),clo1(¬z) := var(z) ∧ ¬implies(T , z),(cid:4)clo2(¬z) := var(z) ∧ z ∈ P ∧ (∀Y )(cid:7)MM(Y ) → ¬(z ∈ Y ).Deduction with CWA-rule and CCWA-rule:(cid:4)(∀ X)model(cid:9)(cid:5)(X, T ) ∧ (∀z)(cid:6)cloi(¬z) → implies(X, ¬z)→ model(cid:9)(cid:7)(X, F ).Deduction with ECWA-rule:(cid:4)(∀ X)MM(X) → model(cid:9)(cid:7)(X, F ).The above predicates have the following meaning:respectively. MM( X) means X ∈ MM(T ; P ; Q ; Z ). (cid:2)cloi(¬z) with i ∈ {1, 2} means that ¬z is in the closure of T w.r.t. the CWA-rule (for i = 1) or CCWA-rule (for i = 2),Analogously to the previous section, we immediately get the following complexity result by Courcelle’s Theorem togetherwith Theorem 3.5.(cid:9)Theorem 3.9. Consider the signature τ = {varT , var F , varT (cid:9) , var F (cid:9) , clT (cid:9) , clF (cid:9) , posT (cid:9) , posF (cid:9) , negT (cid:9) ,F (cid:9) , P , Q , Z } as in Theorem 3.8. Letan instance of Circumscription or of the Deduction problem T |(cid:14)C F for any of the CWR-rules C ∈ {CWA, GCWA, EGCWA, CCWA,ECWA} be given by a τ -structure A. Then all these problems can be solved in time O( f (w) ∗ |A|) for some function f .3.4. Propositional abductionA propositional abduction problem (PAP, for short) consists of a tuple (cid:5)V , H, M, C(cid:6), where V is a finite set of propositionalvariables, H ⊆ V is the set of hypotheses, M ⊆ V is the set of manifestations, and C is a consistent theory in the form of aclause set. A set S ⊆ H is a solution to P if C ∪ S is consistent and C ∪ S |(cid:14) M holds. In an abductive diagnosis problem,the manifestations M are the observed symptoms (e.g. describing some erroneous behavior of the system) and the clausaltheory C constitutes the system description. The solutions S ⊆ H are the possible explanations for the observed symptoms.Given a PAP P , the basic problems of propositional abduction are the following:114G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132• Solvability: Does there exist a solution of P ?• Relevance: Given h ∈ H , is h contained in at least one solution of P ?• Necessity: Given h ∈ H , is h contained in every solution of P ?In [21], the former two problems were shown to be Σ p2 -complete while the latter is Π p2 -complete.The following example from [34] should help to illustrate these definitions. Consider the following football knowledgebase:C = {weak_defense ∧ weak_attack → match_lost,match_lost → manager_sad ∧ press_angrystar_injured → manager_sad ∧ press_sad}.Moreover, let the set of observed manifestations and the set of hypotheses beM = {manager_sad},H = {star_injured, weak_defense, weak_attack}.This PAP has the following five abductive explanations (= “solutions”):S1 = {star_injured},S2 = {weak_defense, weak_attack},S3 = {weak_attack, star_injured},S4 = {weak_defense, star_injured},S5 = {weak_defense, weak_attack, star_injured}.For bounded treewidth of C, we establish below the fixed-parameter tractability of the Solvability, Relevance, andNecessity problem. A PAP P can be represented as a τ -structure with τ = {cl, var, pos, neg, H, M}, where the predi-cates cl, var, pos, neg represent the clause set C and the unary predicates H and M identify the hypotheses and man-ifestations. In order to represent an instance of the Relevance and Necessity problem, we extend the signature τ toτ (cid:9) = {cl, var, pos, neg, H, M, Dist}, where Dist is a unary relation used for distinguishing some domain element, i.e., in ad-dition to the τ -predicates for expressing the PAP, the structure contains a fact Dist(h) to express that we want to testhypothesis h for Relevance respectively necessity. Then we have:Theorem 3.10. Consider the signature τ (cid:9) = {cl, var, pos, neg, H, M, Dist}. The problems Solvability, Relevance, and Necessity ofpropositional abduction can be expressed by means of MSO sentences over the signature τ (cid:9)(of course, for solvability, the predicate Distis not used).Proof. Recall from Theorem 3.2 that the property that X is a model of a clause set (or, equivalently, a formula in CNF) C,can be expressed by the MSO-formula model( X, F ) using the predicates cl, var, pos, neg. We construct MSO-sentences forthe above three decision problems as follows:model(X, C) ∧ S ⊆ X(cid:4)Sol(S) := S ⊆ H ∧ (∃X)Solvability: (∃S)Sol(S),(cid:4)Relevance: (∃S)Sol(S) ∧ Dist ⊆ S(cid:4)Sol(S) → Dist ⊆ SNecessity: (∀S)(cid:7),(cid:7).(cid:7)(cid:4)(cid:5)∧ (∀Y )model(Y , C) ∧ S ⊆ Y(cid:6)→ M ⊆ Y(cid:7),The predicate Sol(S) is a straightforward formulation of the property “S is a solution of the PAP represented by A”, namely(i) S is a subset of the propositional variables in H , (ii) (C ∪ S) has at least one model X , and (iii) every model Y of (C ∪ S)is also a model of M. (cid:2)Usually, a refined version of the Relevance (respectively Necessity) problem is considered. Rather than asking whether his contained in some (respectively every) solution, one is interested if h is contained in some (respectively every) acceptablesolution. In this context, “acceptable” means “minimal” w.r.t. some preorder (cid:4) on the powerset 2H . Consequently, onespeaks of (cid:4)-Relevance (respectively (cid:4)-Necessity). The above treated basic abduction problems Relevance and Necessitycorrespond to the special case where (cid:4) is the equality. The other preorders studied are the following:• subset-minimality “⊆”,• prioritization “⊆P ” for a fixed number p of priorities: H is partitioned into “priorities” H 1, . . . , H p . Then A ⊆P B if andonly if A = B or there exists a k s.t. A ∩ H i = B ∩ H i for all i < k and A ∩ Hk ⊂ B ∩ Hk,G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132115• minimum cardinality “(cid:3)”: A (cid:3) B if and only if | A| (cid:3) |B|,• penalization “(cid:21)p ” (also referred to as “weighted abduction”): To each element h ∈ H , a weight w(h) is attached. Then(cid:8)(cid:8)A (cid:21)p B if and only ifh∈ A w(h) (cid:3)h∈B w(h).To illustrate the various notions of minimality in abduction, we revisit the football example from [34], which was recalledabove. S1 and S2 are ⊆-minimal, but only S1 is (cid:3)-minimal. Priorities may be used to represent a qualitative version ofprobability. For instance, suppose that for some reason we know that (for a specific team) star_injured is much less likelyto occur than weak_defense and weak_attack. This judgment can be formalized by assigning lower priority to the former.Then S2 is the only minimal solution with respect to the preorder ⊆P . If we have numeric values available for the repaircost or for the robustness of each component (e.g., based on data such as the empirically collected mean time to failure andcomponent age), then the (cid:21)p -minimal solutions correspond to the cheapest repair respectively the most likely explanation.In all of the above cases, the resulting (cid:4)-Relevance (respectively (cid:4)-Necessity) problem is on the second or third levelof the polynomial hierarchy [21]. Again, we show that the computational complexity decreases significantly if the clausaltheories C under consideration have bounded treewidth. The last two cases turn out to be a bit tricky. Below, we establishthe desired FPT-result for the first two ones:Theorem 3.11. Consider the signature τ (cid:9) = {cl, var, pos, neg, H, M, Dist}. For (cid:4) ∈ {⊆, ⊆P }, both the (cid:4)-Relevance problem and the(cid:4)-Necessity problem can be expressed by means of MSO sentences over the signature τ (cid:9).Proof. It suffices to provide an MSO encoding of the predicates Acc⊆(S) and Acc⊆P (S), which mean that S is an acceptablesolution for the preorders ⊆ and ⊆P , respectively:(cid:7)(X ⊂ S) → ¬Sol(X)(cid:4)Acc⊆(S) := Sol(S) ∧ (∀ X)X ⊂P S := (X ∩ H1 ⊂ S) ∨ (X ∩ H1 ⊆ S ∧ X ∩ H2 ⊂ S) ∨ (X ∩ H1 ⊆ S ∧ X ∩ H2 ⊆ S ∧ X ∩ H3 ⊂ S),(cid:7)(cid:4)(X ⊂P S) → ¬Sol(X)Acc⊆P (S) := Sol(S) ∧ (∀ X).,By X ⊂P S we mean that X ⊆P S and X (cid:19)= S. Note that we are only considering 3 priority levels H 1, H2, and H3 above. Thegeneralization to an arbitrary but fixed number p of priority levels is clear. (cid:2)By Courcelle’s Theorem and Theorems 3.10 and 3.11 above, we immediately get the following FPT-result:Theorem 3.12. Consider the signature τ (cid:9) = {cl, var, pos, neg, H, M, Dist}. Let (cid:4) ∈ {⊆, ⊆P } and let an instance of the (cid:4)-Relevance(cid:9)(w) ∗ |A|) for someor (cid:4)-Necessity problem be given by a τ (cid:9)function f-structure A of width w. Then these problems can be solved in time O( f.(cid:9)It remains to consider the cases (cid:4) ∈ {(cid:3), (cid:21)p}. Actually, (cid:3) is a special case of (cid:21)p , where every h ∈ H is assigned thesame weight. Unfortunately, MSO is not powerful enough to express the cardinality-comparison (cid:3) (cf. the discussion in [3]).Nevertheless, also for (cid:4) ∈ {(cid:3), (cid:21)p}, it is possible to establish the FPT-property in case of bounded treewidth via an extensionof Courcelle’s Theorem proved in [3,16]. To this end, we have to recall the definition of extremum problems via an extensionof MSO (the definition in [3] is more general than our definition below; however, for our purposes, this restricted form issufficient).Definition 3.13. Let τ be a signature and ϕ( X1, . . . , Xm) an MSO formula over signature τ with free set variables X1, . . . , Xm.Moreover, let F : Nm → N be a linear function. Then the linear extended MSO extremum problem of ϕ and F is defined asfollows:Instance. A τ -structure A together with functions gQuestion. Find the extremum (either min or max) of(cid:10)(cid:9) (cid:10)(cid:11)Fga∈ A1A1 (a), . . . ,Am (a)ga∈ AmA1 , . . . , gAm : dom(A) → N.over all tuples of sets A1, . . . , Am over the domain of A with A |(cid:14) ϕ( A1, . . . , Am).Theorem 3.14. (See [3].) Let a linear extended MSO extremum problem be given by a linear objective function F and an MSO formulaϕ( X1, . . . , Xm) over some signature τ . Moreover, let an instance of this extremum problem be given by a τ -structure A of treewidthAm : dom(A) → N. Assuming unit cost for arithmetic operations, this extremum problem can bew together with functions gsolved in time O( f (|ϕ(x)|, F , w) ∗ |(A, gAm )|) for some function f .A1 , . . . , gA1 , . . . , gThis additional expressive power suffices for expressing the acceptability of PAP-solutions w.r.t. (cid:3) and (cid:21)p :116G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132Theorem 3.15. Consider the signature τ (cid:9) = {cl, var, pos, neg, H, M, Dist}. Let (cid:4) ∈ {(cid:3), (cid:21)p} and let an instance of the (cid:4)-Relevance or(cid:4)-Necessity problem be given by a τ (cid:9)-structure A of width w. In case of (cid:21)p -abduction, the problem instance additionally contains. Assuming unit cost for arithmetic operations, these problems can be solved in time O( f (w) ∗ |A|) (for thea weight function gpreorder (cid:3)), respectively, in time O( f (w) ∗ |(A, gA)|) ( for the preorder (cid:21)p ) for some function f .AProof. It suffices to show that these problems can be decided by solving linear extended MSO extremum problems. We onlyconsider the case of (cid:21)p , since (cid:3) is a special case.Let an instance of the (cid:21)p -Relevance or (cid:21)p -Necessity problem be given as a τ (cid:9)-structure A with τ (cid:9) = {cl, var, pos, neg, H,M, Dist}. Moreover, let Sol(S) denote the MSO-formula from the proof of Theorem 3.10, expressing that S is a solution of thePAP represented by A. Clearly, the task of computing the minimal total weight over all solutions of this PAP correspondsto the linear extremum (i.e., minimum) problem defined by the MSO-formula Sol(S) and taking the identity function asAobjective function F . The instances of this extremum problem are given as τ (cid:9)being the weight function on the domain elements of A.-structure A together with the function g(cid:9)(S) be defined as Sol(cid:9)(S) (respec-Now let Sol(cid:9)(cid:9)(S)) expresses the property that S is a solution containing h (respectively not containing h). Then h is relevant iftively Soland only if the following properties hold: A |(cid:14) (∃S)Sol(S)is the same as the minimal weight over all sets S fulfilling A |(cid:14) (∃S)Sol(S). Both minimum-problems can be expressedas linear extended MSO extremum problems and are therefore solvable in the desired time bound. Likewise, the decisionproblem A |(cid:14) (∃S)Sol(S) and the minimal weight over all sets S fulfilling A |(cid:14) (∃S)Sol(cid:9)(cid:9)(S) := Sol(S) ∧ Dist ⊆ S and Sol(cid:9)(S) := Sol(S) ∧ Dist (cid:19)⊆ S, i.e., Sol(S) fits into this time bound.(cid:9)(cid:9)(S) and SolSimilarly, h is necessary if and only if the following properties hold: Either A (cid:19)|(cid:14) (∃S)Sol(cid:9)conditions are fulfilled: A |(cid:14) (∃S)Solstrictly smaller than the minimal weight over all sets S with A |(cid:14) (∃S)Soldo contain h. These decision problems as well as the minimum-problems can solved in the desired time bound. (cid:2)(S), and the minimal weight over all sets S with A |(cid:14) (∃S)Sol(S) holds or all of the following(S) is(S), i.e., all solutions of minimal weight actually(S), A |(cid:14) (∃S)Sol(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)4. New algorithms via datalogIn this section, we show how the monadic datalog approach from [30] can be exploited in order to construct concrete,efficient algorithms for the problems whose fixed-parameter tractability was established in Section 3. We shall present newalgorithms for the Solvability problem (i.e., does a given PAP have at least one solution) and for the Relevance enumerationproblem (i.e., enumerate all relevant hypotheses of a given PAP) of propositional abduction. We believe that this approachis also applicable to other problems discussed in Section 3. Indeed, recently similar algorithms have been presented forcircumscription and for disjunctive logic programming, see [36,37].It is convenient to consider a PAP P to be given as a τ -structure with τ = {cl, var, pos, neg, hyp, man}. Note that, incontrast to Section 3.4, we denote the unary predicates for the hypotheses and manifestations as hyp and man ratherthan H and M. This change of notation should help to increase readability of the datalog programs by conforming to theconvention that predicate names are normally written in lower-case letters. As in Section 3, we again start our expositionwith the SAT problem in order to illustrate the basic ideas before we tackle the harder problems in the area of abduction.Note that an efficient algorithm for exploiting bounded treewidth in the context of the SAT problem has been recentlypresented in [24]. The algorithm presented there is based on recursive splitting. In [24], the authors also outline how theirmethod can be extended to other NP-complete problems (or their #P-complete counting variants). However, our goal is totackle also problems on the second level of the polynomial hierarchy like abduction.4.1. SAT problemSuppose that a clause set together with a tree decomposition T of width w is given as a τtd-structure with τtd ={cl, var, pos, neg, root, leaf , child1, child2, bag}. Of course, we do not need the predicates hyp and man for the SAT problem.Without loss of generality, we may assume that T is in the normal form given in Definition 2.2. Note that now the domainelements are either variables or clauses. Consequently, a node t in the tree decomposition T is called a variable removalnode, a clause removal node, a variable introduction node, or a clause introduction node, respectively, depending on the kind ofelement that is removed or replaced – comparing the bag at node t with the bag at the child node of t. Additionally, as inDefinition 2.2, a node t can be a branch node or a leaf node (if it has two respectively no child nodes).In Fig. 4, we describe a datalog program which decides the SAT problem. Some words on the notation used in thisprogram are in order: We are using lower case letters v, c, and x (possibly with subscripts) as datalog variables for a singlenode in T , for a single clause, or for a single propositional variable, respectively. In contrast, upper case letters are usedas datalog variables denoting sets of variables (in the case of X, P , N) or sets of clauses (in the case of C ). Note that thesets are not sets in the general sense, since their cardinality is restricted by the maximal size w + 1 of the bags, wherew is a fixed constant. Indeed, we have implemented these “fixed-size” sets by means of k-tuples with k (cid:3) (w + 1) over{0, 1}. The SAT-program in Fig. 4 could therefore be said “quasi-monadic”, since all but one variable in the head atoms havebounded instantiations. For instance, in the atom solve(v, P , N, C), the sets P , N, C are subsets of the bag of v. Hence, eachcombination P , N, C could be represented by 3 subsets r1, r2, r3 over {0, . . . , w} referring to indices of elements in the bagG. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132117Program SAT/* leaf node. */solve(v, P , N, C1) ← leaf (v), bag(v, X, C), P ∪ N = X , P ∩ N = ∅,true(P , N, C1, C)./* variable removal node. */solve(v, P , N, C1) ← bag(v, X, C), child1(v 1, v), bag(v 1, X (cid:22) {x}, C),solve(v 1, P (cid:22) {x}, N, C1).solve(v, P , N, C1) ← bag(v, X, C), child1(v 1, v), bag(v 1, X (cid:22) {x}, C),solve(v 1, P , N (cid:22) {x}, C1)./* clause removal node. */solve(v, P , N, C1) ← bag(v, X, C), child1(v 1, v), bag(v 1, X, C (cid:22) {c}),solve(v 1, P , N, C1 (cid:22) {c})./* variable introduction node. */solve(v, P (cid:22) {x}, N, C1 ∪ C2) ← bag(v, X (cid:22) {x}, C), child1(v 1, v),bag(v 1, X, C), solve(v 1, P , N, C1), true({x}, ∅, C2, C).solve(v, P , N (cid:22) {x}, C1 ∪ C2) ← bag(v, X (cid:22) {x}, C), child1(v 1, v),bag(v 1, X, C), solve(v 1, P , N, C1), true(∅, {x}, C2, C)./* clause introduction node. */solve(v, P , N, C1 ∪ C2) ← bag(v, X, C (cid:22) {c}), child1(v 1, v), bag(v 1, X, C),solve(v 1, P , N, C1), true(P , N, C2, {c})./* branch node. */solve(v, P , N, C1 ∪ C2) ← bag(v, X, C), child1(v 1, v), bag(v 1, X, C),solve(v 1, P , N, C1), child2(v 2, v), bag(v 2, X, C), solve(v 2, P , N, C2)./* result (at the root node). */success ← root(v), bag(v, X, C), solve(v, P , N, C).Fig. 4. SAT test.of v. Since w is considered as a fixed constant, solve(v, P , N, C) is simply a succinct representation of constantly manymonadic predicates of the form solve(cid:5)r1,r2,r3(cid:6)(v).For the sake of readability, we are using non-datalog expressions involving the set operators (cid:22) (disjoint union), ∪, and ∩.Of course, they could be easily replaced by “proper” datalog expressions. For instance, P ∪ N = X can be replaced by union(P , N, X).In order to facilitate the discussion, we introduce the following notation. Let C denote the input clause set with variablesin V and tree decomposition T . For any node v in T , we write Tv to denote the subtree of T rooted at v. By Cl(v) wedenote the set of clauses in the bag of v while Cl(Tv ) denotes the set of clauses that occur in any bag in Tv . Analogously,we write Var(v) and Var(Tv ) as a short-hand for the set of variables occurring in the bag of v respectively in any bag in Tv .Finally, the restriction of a clause c to the variables in some set U ⊆ V will be denoted by c|U .The SAT-program contains three intensional predicates solve, true, and success. The crucial predicate is solve(v, P , N, C)with the following intended meaning: v denotes a node in T . P and N form a partition of Var(v) representing a truthvalue assignment on Var(v), s.t. all variables in P take the value true and all variables in N take the value false. C denotesa subset of Cl(v). For all values of v, P , N, C , the ground fact solve(v, P , N, C) shall be true in the minimal model of theinterpreted program if and only if the following condition holds:Property A. There exists an extension J of the assignment (P , N) to Var(Tv ), s.t. (Cl(Tv ) \ Cl(v)) ∪ C is true in J while for all clausesc ∈ Cl(v) \ C , the restriction c|Var(Tv ) is false in J .The main task of the program is the computation of all facts solve(v, P , N, C) by means of a bottom–up traversal of thetree decomposition. The other predicates have the following meaning: true(P , N, C1, C) means that C1 contains preciselythose clauses from C which are true in the (partial) assignment given by (P , N). We do not specify the implementationof this predicate here. It can be easily achieved via the extensional predicates pos and neg. The 0-ary predicate successindicates if the input structure is the encoding of a satisfiable clause set.The SAT-program has the following properties.Lemma 4.1. The solve-predicate has the intended meaning described above, i.e., for all values v, P , N, and C , the ground factsolve(v, P , N, C) is true in the minimal model of the interpreted SAT-program if and only if Property A holds.Proof idea. The lemma can be shown by structural induction on T . The induction step goes via a case distinction over allpossible types of nodes. The proof is lengthy but straightforward. The details are worked out in Appendix A. (cid:2)118G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132Table 1SAT example.Node713196543120solve factssolve(7, {x3}, ∅, {c1}), solve(7, ∅, {x3}, ∅)solve(13, ∅, {x5}, {c2}), solve(13, {x5}, ∅, c2)solve(19, {x6}, ∅, {c3}), solve(19, ∅, {x3}, ∅)solve(6, {x1, x3}, ∅, {c1}), solve(6, ∅, {x1, x3}, ∅),solve(6, {x1}, {x3}, {c1}), solve(6, {x3}, {x1}, {c1})solve(5, {x1}, ∅, {c1}), solve(5, ∅, {x1}, ∅), solve(5, ∅, {x1}, {c1})solve(4, {x1, x2}, ∅, {c1}), solve(4, {x1}, {x2}, {c1}), solve(4, {x2}, {x1}, ∅),solve(4, ∅, {x1, x2}, {c1}), solve(4, {x2}, {x1}, {c1})solve(3, {x1, x2}, ∅, ∅), solve(3, {x1}, {x2}, ∅),solve(3, {x2}, {x1}, ∅), solve(3, ∅, {x1, x2}, ∅)solve(1, {x1, x2, x4}, ∅, ∅), solve(1, {x1, x2}, {x4}, ∅),solve(1, {x1, x4}, {x2}, ∅), solve(1, {x1}, {x2, x4}, ∅),solve(1, {x2, x4}, {x1}, ∅), solve(1, {x2}, {x1, x4}, ∅),solve(1, {x4}, {x1, x2}, ∅), solve(1, ∅, {x1, x2, x4}, ∅)solve(2, {x1, x2, x4}, ∅, ∅), solve(2, {x1, x2}, {x4}, ∅),solve(2, {x1, x4}, {x2}, ∅), solve(2, {x1}, {x2, x4}, ∅),solve(2, {x2, x4}, {x1}, ∅), solve(2, {x2}, {x1, x4}, ∅),solve(2, {x4}, {x1, x2}, ∅), solve(2, ∅, {x1, x2, x4}, ∅)solve(0, {x1, x2, x4}, ∅, ∅), solve(0, {x1, x2}, {x4}, ∅),solve(0, {x1, x4}, {x2}, ∅), solve(0, {x1}, {x2, x4}, ∅),solve(0, {x2, x4}, {x1}, ∅), solve(0, {x2}, {x1, x4}, ∅),solve(0, {x4}, {x1, x2}, ∅), solve(0, ∅, {x1, x2, x4}, ∅)Theorem 4.2. The datalog program in Fig. 4 decides the SAT problem, i.e., the fact “success” is true in the minimal model of theinterpreted SAT-program if and only if the input structure Atd encodes a satisfiable clause set C together with a tree decomposition Tof C. Moreover, for any clause set C and tree decomposition T of width at most w, the program can be evaluated in time O(2w ∗ |C|).Proof. By Lemma 4.1, the solve-predicate has the meaning according to Property A. Thus, the rule with head success readsas follows: success is true in the minimal model if and only if v denotes the root of T and there exists an assignment (P , N)on the variables in Var(v), s.t. for some extension J of (P , N) to Var(Tv ), all clauses in Cl(Tv ) = (Cl(Tv ) \ Cl(v)) ∪ C are truein J . But this simply means that J is a satisfying assignment of C = Cl(Tv ).For the upper bound on the time complexity, we observe that, in all facts solve(v, P , N, C) derived by the program, thesets P , N, C are disjoint subsets of the bag at v (which contains at most w + 1 elements). Suppose that the bag at nodev consists of k variables and l clauses with k + l (cid:3) w + 1. Then there are at most 2k ∗ 2l = 2w+1 possible instantiations ofthese two arguments.Hence, the datalog program P in Fig. 4 is equivalent to a ground program P (cid:9)where each rule of P is replaced byO(2w ∗ |C|) ground rules, which can be computed in time O(2w ∗ |C|). In total, we can evaluate the program P in Fig. 4over an input formula C in CNF together with a tree decomposition T (given as a τtd-structure Atd, whose size is linearw.r.t. the size of C) as follows: we first compute the equivalent ground program P (cid:9)with O(2w ∗ |C|) rules and then evaluateP (cid:9)and the size of Atd, i.e., in time O(2w ∗ |C| + |Atd|) = O(2w ∗ |C|). (cid:2)in linear time [18,42] w.r.t. the size of P (cid:9)Example 4.3. Let F = (x1 ∨ ¬x2 ∨ x3) ∧ (¬x1 ∨ x4 ∨ ¬x5) ∧ (x2 ∨ ¬x4 ∨ x6) be the propositional formula from Example 2.1and consider the normalized tree decomposition T in Fig. 2. We shall run the SAT-program on the structure representingF and T . We process the tree nodes in a bottom–up manner. The solve-facts derived at each tree node are presented inTable 1.We start with the leaf nodes 7, 13, and 19. At node 7, the variable x3 can be assigned either the value true or false. If x3is assigned the value true ( P = {x3} and N = ∅), then clause c1 is true. Hence, we obtain the fact solve(7, {x3}, ∅, {c1}). If x3is assigned the value false ( P = ∅ and N = {x3}), then clause c1 (restricted to the variable x3) evaluates to false. Thus, weobtain the second fact solve(7, ∅, {x3}, ∅). By analogous arguments, we obtain the solve-facts for the leaf nodes 13 and 19.Node 6 is a variable introduction node, which introduces the variable x1 (which is not present in node 7). Again, x1 canbe either set to true or false. For each of the assignments, we test whether c1 evaluates true. Therefore, at node 6, we obtainfour solve-facts with all possible truth assignments for x1 and x3.Node 5 is a variable removal node, which removes the variable x3 (which was present in node 6). We remember theeffect of the truth assignment to the removed variable (in this case x3) by maintaining different sets of clauses in the solve-facts. Thus, at node 5, there exist two facts for the truth assignment P = ∅ and N = {x1}, namely one with C = {c1} and onewith C = ∅ – see the second and the third solve-fact in Table 1.G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132119Program Solvability/* leaf node. */solve(v, S, 0, P , N, C1, d) ← leaf (v), bag(v, X, C), svar(v, S), S ⊆ P ,P ∪ N = X , P ∩ N = ∅, check(P , N, C1, C, d)./* variable removal node. */aux(v, S, i, 0, P , N, C1, d) ← bag(v, X, C), child1(v 1, v), bag(v 1, X (cid:22) {x}, C),solve(v 1, S, i, P (cid:22) {x}, N, C1, d), x /∈ S.aux(v, S, i, 0, P , N, C1, d) ← bag(v, X, C), child1(v 1, v), bag(v 1, X (cid:22) {x}, C),solve(v 1, S, i, P , N (cid:22) {x}, C1, d).aux(v, S, i, 1, P , N, C1, d) ← bag(v, X, C), child1(v 1, v), bag(v 1, X (cid:22) {x}, C),solve(v 1, S (cid:22) {x}, i, P (cid:22) {x}, N, C1, d)./* clause removal node. */solve(v, S, i, P , N, C1, d) ← bag(v, X, C), child1(v 1, v), bag(v 1, X, C (cid:22) {c}),solve(v 1, S, i, P , N, C1 (cid:22) {c}, d)./* variable introduction node. */solve(v, S, i, P (cid:22) {x}, N, C1 ∪ C2, d1) ← bag(v, X (cid:22) {x}, C), child1(v 1, v),bag(v 1, X, C), solve(v 1, S, i, P , N, C1, d1), check({x}, ∅, C2, C, d2).solve(v, S, i, P , N (cid:22) {x}, C1 ∪ C2, d1 or d2) ← bag(v, X (cid:22) {x}, C), child1(v 1, v),bag(v 1, X, C), solve(v 1, S, i, P , N, C1, d1), check(∅, {x}, C2, C, d2).solve(v, S (cid:22) {x}, i, P (cid:22) {x}, N, C1 ∪ C2, d1) ← bag(v, X (cid:22) {x}, C), child1(v 1, v),bag(v 1, X, C), solve(v 1, S, i, P , N, C1, d1), check({x}, ∅, C2, C, d2), hyp(x)./* clause introduction node. */solve(v, S, i, P , N, C1 ∪ C2, d1) ← bag(v, X, C (cid:22) {c}), child1(v 1, v),bag(v 1, X, C), solve(v 1, S, i, P , N, C1, d1), check(P , N, C2, {c}, d2)./* branch node. */aux(v, S, i1, i2, P , N, C1 ∪ C2, d1 or d2) ← bag(v, X, C),child1(v 1, v), bag(v 1, X, C), child2(v 2, v), bag(v 2, X, C),solve(v 1, S, i1, P , N, C1, d1), solve(v 2, S, i2, P , N, C2, d2)./* variable removal and branch node: aux ⇒ solve */solve(v, S, i, P , N, C, d) ← aux(v, S, i1, i2, P , N, C, d), reduce(v, S, i, i1, i2)./* result (at the root node). */success ← root(v), bag(v, X, C), solve(v, S, i, P , N, C, ‘n’), not solve(cid:9)(v, S, i).(cid:9)solve(v, S, i) ← bag(v, X, C), solve(v, S, i, P , N, C, ‘y’).Fig. 5. Solvability test.The variable introduction node 4 follows the same principles as node 6. Note however that the number of solve-facts isnot doubled by the introduction of variable x2. Indeed, the facts solve(5, ∅, {x1}, ∅) and solve(5, ∅, {x1}, {c1}) both give riseto the fact solve(4, {x2}, {x1}, {c1}). Hence, we have only 5 solve-facts at node 4.Node 3 is a clause removal node where the clause c1 is removed (comparing the bag of node 3 with node 4). Only thosetruth assignments in solve-facts from node 4 “survive”, where the clause c1 evaluates to true. Actually, in this example, onlythe fact solve(4, {x2}, {x1}, ∅) violates this condition. However, there also exists a fact solve(4, {x2}, {x1}, {c1}) at node 4. Thelatter gives rise to the fact solve(3, {x2}, {x1}, ∅).The traversal from node 3 to 1 introduces the new variable x4, which is trivial, because there does not exist any clausein node 1. Thus for each of the eight truth assignments there is a solve-fact in node 1. The traversal from node 13 to 8, aswell as from 19 to 14 are almost identical to those from node 7 to 1. Hence we do not illustrate the solve-facts explicitly.Finally, at the root node 0, we check whether there is a truth assignment of the variables that makes all the clauses inthe root true. This is trivial in our example. In fact, we obtain that all possible truth assignments to the variables x1, x2 andx4 are solutions, since they can all be extended to models of F by choosing appropriate truth values for x3, x5, and x6.4.2. Solvability problemThe SAT-program from the previous section can be extended to a Solvability program via the following idea: Recall thatS ⊆ H is a solution of a PAP P = (cid:5)V , H, M, C(cid:6) if and only if C ∪ S is consistent and C ∪ S |(cid:14) M holds. We can thus think ofthe abduction problem as a combination of SAT and UNSAT problems, namely C ∪ S has to be satisfiable and all formulaeC ∪ S ∪ {¬m} for any m ∈ M have to be unsatisfiable. Suppose that we construct such a set S along a bottom–up traversalof T . Initially, S is empty. In this case, C ∪ S = C is clearly satisfiable (otherwise the abduction problem makes no sense)and C ∪ S ∪ {¬m} is also satisfiable for at least one M (otherwise the abduction problem is trivial). In other words, C ∪ S hasmany models – among them are also models where some m ∈ M is false. The effect of adding a hypothesis h to S is thatwe restrict the possible number of models of C ∪ S and of C ∪ S ∪ {¬m} in the sense that we eliminate all models where h120G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132is false. Hence, the goal of our Solvability algorithm is to find (by a bottom–up traversal of the tree decomposition T ) a setS which is small enough so that at least one model of C ∪ S is left while all models of C ∪ S ∪ {¬m} for any m ∈ M areeliminated.The program in Fig. 5 realizes this SAT/UNSAT-intuition. For the discussion of this program, it is convenient to introducethe following additional notation. We shall write H(v), M(v), H(Tv ) and M(Tv ) to denote the restriction of H and M to thevariables in the bag of v or in any bag in Tv , respectively. Of course, the unary predicates hyp and man are now containedin τtd.The predicate solve(v, S, i, P , N, C, d) has the following intended meaning: At every node v, we consider choices S ⊆H(v). (P , N) again denotes an assignment on the variables in Var(v) and C ⊆ Cl(v) denotes a clause set, s.t. (Cl(Tv ) \Cl(v)) ∪ C is true in some extension J of (P , N) to Var(Tv ). But now we have to additionally consider the chosen hypothesesin H(Tv ) and the manifestations in M(Tv ) which decide whether J is a candidate for the SAT and/or UNSAT problem. Asfar as H is concerned, we have to be careful as to how S ⊆ H(v) is extended to ¯S ⊆ H(Tv ). For a different extension ¯S,different assignments J on Var(Tv ) are excluded from the SAT/UNSAT problems, since we only keep track of assignmentsJ where all hypotheses in ¯S are true. Hence, we need a counter i ∈ {0, 1, 2, . . .} as part of the solve-predicate in order todistinguish between different extensions of S ⊆ H(v) to ¯S ⊆ H(Tv ). As far as M is concerned, we have the argument d withpossible values ‘y’ and ‘n’ indicating whether some manifestation m ∈ M(Tv ) is false in J . For the UNSAT problem, we takeinto account only those assignments J where at least one m ∈ M is false.Then the program has the following meaning: For all values of v, S, i, and for any extension ¯S of S with ¯S ⊆ (H(Tv ) \H(v)) ∪ S, we define:Property B. Property B For all values of P , N, C , and u, the fact solve(v, S, i, P , N, C, u) is true in the minimal model of the interpretedprogram if and only if there exists an extension J of the assignment (P , N) to Var(Tv ), s.t. (Cl(Tv ) \ Cl(v)) ∪ ¯S ∪ C is true in J whilefor all clauses c ∈ Cl(v) \ C , the restriction c|Var(Tv ) is false in J . Moreover, u = ‘y’ if and only if some m ∈ M(Tv ) is false in J .The predicate svar in Fig. 5 is used to select sets of hypotheses, i.e., svar(v, S) is true for every subset S ⊆ H(v). Thepredicate check extends the predicate true from the SAT-program by additionally setting the d-bit, i.e., check(P , N, C1, C, d)if and only if true(P , N, C1, C). Moreover, d = ‘y’ if and only if N contains some manifestation.Finally,the predicates aux and reduce have the following purpose: As was mentioned above,insolve(v, S, i, P , N, C, d) is used to keep different extensions ¯S ⊆ H(Tv ) of S apart. Without further measures, we wouldthus loose the fixed-parameter tractability since the variable elimination nodes and branch nodes lead to an exponen-tial increase (w.r.t. the number of hypotheses in H(Tv )) of the number of extensions ¯S. The predicates aux and reduceremedy this problem as follows: In the first place, we compute facts aux(v, S, i1, i2, P , N, C, d), where different exten-sions ¯S of S are identified by pairs of indices (i1, i2). Now let v and S be fixed and consider for each pair (i1, i2) theset F (i1, i2) = {(P , N, C, d) | aux(v, S, i1, i2, P , N, C, d) is true in the minimal model}. The predicate reduce(v, S, i, i1, i2)maps pairs of indices (i1, i2) to a unique index i. However, if there exists a lexicographically smaller pair ( j1, j2) withF (i1, i2) = F ( j1, j2), then (i1, i2) is skipped. In other words, if two extensions ¯S with index (i1, i2) and ( j1, j2) are notj1, j2, P , N, C, d) with exactly thedistinguishable at v (i.e., they give rise to facts aux(v, S, i1, i2, P , N, C, d) and aux(v, S,same sets of values (P , N, C, d)), then it is clearly sufficient to keep track of exactly one representative. The predicate reducecould be easily implemented in datalog (with negation), see Appendix B. However, we prefer to consider it as a built-inpredicate which can be implemented very efficiently via appropriate hash codes.the index iAnalogously to Lemma 4.1 and Theorem 4.2, the Solvability program has the following properties.Lemma 4.4. The solve-predicate has the intended meaning described above, i.e., for all values v, S, i, and for any extension ¯S of S with¯S ⊆ (H(Tv ) \ H(v)) ∪ S, Property B holds.Proof sketch. The lemma can be shown by structural induction on T . We restrict ourselves here to outlining the ideasunderlying the various rules of the Solvability program. The induction itself is then obvious and therefore omitted.(1) Leaf nodes. The rule for a leaf node v realizes two “guesses” so to speak: (i) a subset S of the hypotheses in H(v)and (ii) a truth assignment (P , N) on the variables in Var(v). The values of S, P , N have to fulfill several conditions,namely S ⊆ P , P ∪ N = X , and P ∩ N = ∅. The remaining variables in this rule are fully determined. In particular, theatom check(P , N, C1, C, d) is used to compute the set of clauses C1 ⊆ C which are true in the assignment (P , N) and thed-bit is set depending on whether some manifestation is set to false by this assignment or not.(2) Variable removal node. The three rules distinguish, in total, 3 cases, which may arise when a variable x is removedfrom the bag at the child node v 1 of v, namely: was x set to true or false and was x part of the solution or not. Of course,if x was part of the solution then its truth value must be true. The three rules for a variable removal node treat these casesin the order (true, not solution), (false, not solution), (true, solution).The removal of x may have the effect that the solve-facts which are true in the minimal model for some values (v, S, i)and (v, S, j) become identical even if they were different for (v 1, S, i) and (v 2, S, i). Hence, the rule for variable removalnodes first generates aux-facts. The corresponding solve-facts are then generated only after the duplicate elimination via thereduce-predicate.G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132121Recall that the index i in the solve-facts solve(v, S, i, . . .) is used to keep different extensions ¯S ⊆ H(Tv ) of S apart.Analogously, in the aux-facts aux(v, S, i1, i2, . . .), the pair (i1, i2) does this job. Therefore, the aux-facts for the first twocases (where x /∈ ¯S) and in the third case (where x ∈ ¯S) have different pairs of indices (i, 0) and (i, 1), respectively.(3) Clause removal node. The rule for a clause removal node v checks if the clause c that is removed at v evaluates to truein an extension J of (P , N). This check is carried out here because (by the connectedness condition) only at the removal ofc we can be sure that c|Var(Tv ) is identical to c. Hence, if c evaluates to false in an extension J of (P , N) to Var(Tv ), thenthere is no way to turn the truth value of c into true by an appropriate assignment to the variables elsewhere in the treedecomposition.(4) Variable introduction node. Analogously to the variable removal node, the three rules distinguish, in total, 3 cases,which may arise when a variable x is introduced at the node v, namely: is x set to true or false and is x part of the solutionor not. Of course, if x is part of the solution then its truth value must be true. The three rules for a variable introductionnode treat these cases in the order (true, not solution), (false, not solution), (true, solution).The third rule has the additional atom hyp(x) in the body to make sure that x indeed is a hypothesis when it is addedto the solution. In all three cases, the check-predicate in the rule body checks (i) if additional clauses C2 evaluate to truebecause of the chosen truth value of x, and (ii) if some manifestation is thus set to false. Of course, the check (ii) is onlyrelevant if x is set to false.(5) Clause introduction node. When a new clause c is introduced, we have to check if c evaluates to true in the assignment(P , N) or not. This is done by the atom check(P , N, C2, {c}, d2) in the rule body. The variable C2 thus has one of the values{c} or ∅. Clearly, by the connectedness condition, no variables occurring in c can occur in Var(Tv ) \ Var(v). Hence, it playsno role which extension J of (P , N) to Var(Tv ) is considered.(6) Branch node. The rule for a branch node v combines the solve-facts for the child nodes v 1 and v 2 provided that theyhave identical values for S, P , N. By the connectedness condition, we can be sure that the variables and clauses occurringboth in Tv1 and Tv2 also occur in the bags of v 1, v 2, and v. Hence the extensions ¯S1 and ¯S2 (respectively,J 1 and J 2) areconsistent if and only if ¯S1 ∩ H(v) = S = ¯S2 ∩ H(v) holds (respectively, if and only if J 1 and J 2 restricted to Var(v) coincidewith (P , N)).Note that it may happen that combining the solve-facts for some values (v 1, S, i1) and (v 2, S, i2) yields the same valuesof (P , N, C, d) as if we combine the solve-facts for some values (v 1, S, j1) and (v 2, S, j2). Hence, the rule for branch nodesfirst generates aux-facts. The corresponding solve-facts are then generated only after the duplicate elimination via the reduce-predicate. (cid:2)Theorem 4.5. The datalog program in Fig. 5 decides the Solvability problem of PAPs, i.e., the fact “success” is true in the minimal modelof the interpreted program if and only if Atd encodes a solvable PAP P = (cid:5)V , H, M, C(cid:6) together with a tree decomposition T of theclause set C. Moreover, for any PAP P = (cid:5)V , H, M, C(cid:6) and tree decomposition T of width at most w, the program can be evaluated intime O(25∗3w+2 ∗ |P|).Proof sketch. By Lemma 4.4, the solve-predicate indeed has the meaning according to Property B. Thus, the rule with headsuccess reads as follows: The PAP P has a solution S if and only if there exists a set S ⊆ H(v) (i.e., S = S ∩ H(v)) and thereexists an extension ¯S of S to H(Tv ) (i.e., ¯S = S; this extension is identified by the index i), s.t. the following conditionshold:(1) Cl(Tv ) ∪ ¯S has a model, i.e., there exists an assignment (P , N) on Var(v) which can be extended to J on Var(Tv ), s.t.Cl(Tv ) ∪ ¯S is true in J , and(2) Cl(Tv ) ∪ ¯S does not have a model in which some m ∈ M is false, i.e., there does not exists an assignment (P , N) onVar(v) which can be extended to J on Var(Tv ), s.t. Cl(Tv ) ∪ ¯S is true in J and at least one m ∈ M is false in J .As in the proof of Theorem 4.2, the upper bound on the time complexity is obtained via an upper bound on the possibleground instantiations of each rule in the Solvability program. First consider the solve(v, S, i, P , N, C, d)-predicates: Supposethat the bag at some node v consists of k variables and l clauses with k + l (cid:3) w + 1. Then we get the following upper boundon the possible value combinations:For (S, P , N), we have at most 3k possibilities, i.e., a variable can either occur in N (i.e., it is set to false) or both in Sand P (i.e., it is set to true and added to the solution S) or in P but not in S (i.e., it is set to true and but not added tothe solution S). For C , we have the upper bound 2l, and for d there are at most 2 possible values. In total, the numberof combinations of (S, P , N, C, d) is bounded by 3k ∗ 2l ∗ 2 (cid:3) 3k ∗ 3l ∗ 3 = 3w+2. For the possible values of i, recall fromj of distinct “indices”, the set of facts solve(v, S, i, . . .) and the set of factsthe above considerations that, for every pair i,solve(v, S, j, . . .) must be different. Hence, for every value of v and S, there are at most 23w+2possible values of i. In total,we thus get the upper bound O(3w+2 ∗ 23w+2 ∗ |P|) on the possible ground instantiations of the solve(v, S, i, P , N, C, d)-predicate.For the remaining predicates used in the program in Fig. 5 (in particular, the aux-predicate but also the remainingauxiliary predicates like the reduce-predicate detailed in Appendix B), the worst situation that can happen is that a predicatecontains two indices i, j. Hence, the program in Fig. 5 is equivalent to a ground program with O(3w+2 ∗ 23w+2 ∗ 23w+2 ∗|P|) =O(25∗3w+2 ∗ |P|) rules. The Solvability program can thus be evaluated by first computing the equivalent ground program122G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132Table 2Solvability Example (1).Node765431solve factssolve(7, 0, ∅, {x3}, ∅, {c1}, ‘n’), solve(7, 0, ∅, ∅, {x3}, ∅, ‘n’)solve(6, 0, ∅, {x1, x3}, ∅, {c1}, ‘n’), solve(6, 0, ∅, {x1}, {x3}, {c1}, ‘n’),solve(6, 0, ∅, {x3}, {x1}, {c1}, ‘n’), solve(6, 0, ∅, ∅, {x1, x3}, ∅, ‘n’),solve(6, 0, {x1}, {x1, x3}, ∅, {c1}, ‘n’), solve(6, 0, {x1}, {x1}, {x3}, {c1}, ‘n’)solve(5, 0, ∅, {x1}, ∅, {c1}, ‘n’),solve(5, 0, ∅, ∅, {x1}, {c1}, ‘n’), solve(5, 0, ∅, ∅, {x1}, ∅, ‘n’),solve(5, 0, {x1}, {x1}, ∅, {c1}, ‘n’)solve(4, 0, ∅, {x1, x2}, ∅, {c1}, ‘n’), solve(4, 0, ∅, {x1}, {x2}, {c1}, ‘n’),solve(4, 0, {x2}, {x1, x2}, ∅, {c1}, ‘n’), solve(4, 0, ∅, {x2}, {x1}, {c1}, ‘n’),solve(4, 0, ∅, ∅, {x1, x2}, {c1}, ‘n’), solve(4, 0, {x2}, {x2}, {x1}, {c1}, ‘n’),solve(4, 0, ∅, {x2}, {x1}, ∅, ‘n’), solve(4, 0, {x2}, {x2}, {x1}, ∅, ‘n’),solve(4, 0, {x1}, {x1, x2}, ∅, {c1}, ‘n’), solve(4, 0, {x1}, {x1}, {x2}, {c1}, ‘n’),solve(4, 0, {x1, x2}, {x1, x2}, ∅, {c1}, ‘n’)solve(3, 0, ∅, {x1, x2}, ∅, ∅, ‘n’), solve(3, 0, ∅, {x1}, {x2}, ∅, ‘n’),solve(3, 0, {x2}, {x1, x2}, ∅, ∅, ‘n’), solve(3, 0, ∅, {x2}, {x1}, ∅, ‘n’),solve(3, 0, ∅, ∅, {x1, x2}, ∅, ‘n’), solve(3, 0, {x2}, {x2}, {x1}, ∅, ‘n’),solve(3, 0, {x1}, {x1, x2}, ∅, ∅, ‘n’), solve(3, 0, {x1}, {x1}, {x2}, ∅, ‘n’),solve(3, 0, {x1, x2}, {x1, x2}, ∅, ∅, ‘n’)solve(1, 0, ∅, {x1, x2, x4}, ∅, ∅, ‘n’), solve(1, 0, ∅, {x1, x2}, {x4}, ∅, ‘n’),solve(1, 0, ∅, {x1, x4}, {x2}, ∅, ‘n’), solve(1, 0, ∅, {x1}, {x2, x4}, ∅, ‘n’),solve(1, 0, {x2}, {x1, x2, x4}, ∅, ∅, ‘n’), solve(1, 0, {x2}, {x1, x2}, {x4}, ∅, ‘n’),solve(1, 0, ∅, {x2, x4}, {x1}, ∅, ‘n’), solve(1, 0, ∅, {x2}, {x1, x4}, ∅, ‘n’),solve(1, 0, ∅, {x4}, {x1, x2}, ∅, ‘n’), solve(1, 0, ∅, ∅, {x1, x2, x4}, ∅, ‘n’),solve(1, 0, {x2}, {x2, x4}, {x1}, ∅, ‘n’), solve(1, 0, {x2}, {x2}, {x1, x4}, ∅, ‘n’),solve(1, 0, {x1}, {x1, x2, x4}, ∅, ∅, ‘n’), solve(1, 0, {x1}, {x1, x2}, {x4}, ∅, ‘n’),solve(1, 0, {x1}, {x1, x4}, {x2}, ∅, ‘n’), solve(1, 0, {x1}, {x1}, {x2, x4}, ∅, ‘n’),solve(1, 0, {x1, x2}, {x1, x2, x4}, ∅, ∅, ‘n’),solve(1, 0, {x1, x2}, {x1, x2}, {x4}, ∅, ‘n’)and then evaluating the latter in linear time [18,42] w.r.t. the size of the ground program and the size of Atd. Analogouslyto the proof of Theorem 4.2, all this is feasible within the time bound O(25∗3w+2 ∗ |P|). (cid:2)Example 4.6. Consider the PAP P = (cid:5)V , H, M, C(cid:6) where C is the clause set corresponding to the CNF-formula F fromExample 2.1, i.e., C = {(x1 ∨ ¬x2 ∨ x3), (¬x1 ∨ x4 ∨ ¬x5), (x2 ∨ ¬x4 ∨ x6)}. Additionally, we assume that H = {x1, x2} andM = {x5, x6}. Finally, we have V = {x1, x2, x3, x4, x5, x6}. The tree decomposition T in Fig. 2 is also a tree decomposition ofthe structure representing this PAP. We shall run the Solvability program on this PAP. Again, we process the tree nodes ofT in a bottom–up manner.Table 2 shows the solve-facts derived at nodes 1, 3, 4, 5, 6 and 7. In principle, the processing is very similar to theSAT-program, except that we have to maintain the additional parameters S, i, and d. Below, we highlight some differencescompared with the run of the SAT-program discussed in Example 4.3.The number of solve-facts at the variable introduction node 6 is now three times the number of solve-facts at node 7.The reason for this is that we have 3 possibilities how to proceed with the new variable x1 (which we assume to be ahypothesis): We can set x1 to false (i.e., it is added to N) or we can set it to true without adding it to S (i.e., it is addedto P ) or we can set it to true and also add it to S (i.e., it is added to P and S). Node 4 is a variable introduction nodewith new variable 4. Note that we only get 11 (rather than 12) solve-facts at node 4. The reason for this is that bothfacts, solve(5, 0, ∅, ∅, {x1}, {c1}, ‘n’) and solve(5, 0, ∅, ∅, {x1}, ∅, ‘n’) give rise to the fact solve(4, 0, ∅, ∅, {x1, x2}, {c1}, ‘n’). Whentraversing from node 4 to node 3, the number of solve-facts decreases from 11 to 9 due to the two solve-facts at node 4where the parameter C has the value ∅ (in particular, C does not contain clause c1 even though node 3 is a clause removalnode with the removal of clause c1). Finally, when traversing from node 3 to node 1, the number of solve-facts is multipliedby 2 (rather than by 3 as in node 6) since x4 is not a hypothesis. Hence, there are only two possibilities how to proceedwith x4: We either add it to P or to N. At tree node 1, we thus end up with 18 solve-facts.The traversal from node 13 to 9 is illustrated in Table 3. This time we have a manifestation variable x5, which has to betaken care of by the value of the parameter d. We leave out the illustration for the traversal from node 19 to 15, becauseit follows almost exactly the same pattern as that from node 13 to 9. We only present the solve-facts at node 15 (whichare very similar to node 9). We very briefly discuss the derivation of the solve-facts at these nodes: At node 13, the d-valueindicates if a manifestation (in this x5) is set to false (i.e., d = ‘y’) or not (i.e., d = ‘n’). The transition from node 13 to 12 isanalogous to the transition from node 7 to 6 in Table 2. At the variable removal node 11, the number of solve-facts is notdecreased compared with node 12, since we now also have to take the d-value into account. At the variable introductionnode 10, we double the number of solve-facts (since the new variable x4 is not a hypothesis). When traversing from nodeG. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132123Table 3Solvability Example (2).Node13121110915solve factssolve(13, 0, ∅, ∅, {x5}, {c2}, ‘y’), solve(13, 0, ∅, {x5}, ∅, ∅, ‘n’)solve(12, 0, ∅, {x1}, {x5}, {c2}, ‘y’), solve(12, 0, ∅, ∅, {x1, x5}, {c2}, ‘y’),solve(12, 0, {x1}, {x1}, {x5}, {c2}, ‘y’), solve(12, 0, ∅, {x1, x5}, ∅, ∅, ‘n’),solve(12, 0, ∅, {x5}, {x1}, {c2}, ‘n’), solve(12, 0, {x1}, {x1, x5}, ∅, ∅, ‘n’)solve(11, 0, ∅, {x1}, ∅, {c2}, ‘y’), solve(11, 0, ∅, ∅, {x1}, {c2}, ‘y’),solve(11, 0, {x1}, {x1}, ∅, {c2}, ‘y’), solve(11, 0, ∅, {x1}, ∅, ∅, ‘n’),solve(11, 0, ∅, ∅, {x1}, {c2}, ‘n’), solve(11, 0, {x1}, {x1}, ∅, ∅, ‘n’)solve(10, 0, ∅, {x1, x4}, ∅, {c2}, ‘y’), solve(10, 0, ∅, {x1}, {x4}, {c2}, ‘y’),solve(10, 0, ∅, {x4}, {x1}, {c2}, ‘y’), solve(10, 0, ∅, ∅, {x1, x4}, {c2}, ‘y’),solve(10, 0, {x1}, {x1, x4}, ∅, {c2}, ‘y’),solve(10, 0, {x1}, {x1}, {x4}, {c2}, ‘y’),solve(10, 0, ∅, {x1, x4}, ∅, {c2}, ‘n’), solve(10, 0, ∅, {x1}, {x4}, ∅, ‘n’),solve(10, 0, ∅, {x4}, {x1}, {c2}, ‘n’), solve(10, 0, ∅, ∅, {x1, x4}, {c2}, ‘n’),solve(10, 0, {x1}, {x1, x4}, ∅, {c2}, ‘n’), solve(10, 0, {x1}, {x1}, {x4}, ∅, ‘n’)solve(9, 0, ∅, {x1, x4}, ∅, ∅, ‘y’), solve(9, 0, ∅, {x1}, {x4}, ∅, ‘y’),solve(9, 0, ∅, {x4}, {x1}, ∅, ‘y’), solve(9, 0, ∅, ∅, {x1, x4}, ∅, ‘y’),solve(9, 0, {x1}, {x1, x4}, ∅, ∅, ‘y’), solve(9, 0, {x1}, {x1}, {x4}, ∅, ‘y’),solve(9, 0, ∅, {x1, x4}, ∅, ∅, ‘n’),solve(9, 0, ∅, {x4}, {x1}, ∅, ‘n’), solve(9, 0, ∅, ∅, {x1, x4}, ∅, ‘n’),solve(9, 0, {x1}, {x1, x4}, ∅, ∅, ‘n’)solve(15, 0, ∅, {x2, x4}, ∅, ∅, ‘n’), solve(15, 0, ∅, {x2}, {x4}, ∅, ‘n’),solve(15, 0, ∅, {x4}, {x2}, ∅, ‘n’), solve(15, 0, ∅, ∅, {x2, x4}, ∅, ‘n’),solve(15, 0, {x2}, {x2, x4}, ∅, ∅, ‘n’), solve(15, 0, {x2}, {x2}, {x4}, ∅, ‘n’),solve(15, 0, ∅, {x2, x4}, ∅, ∅, ‘y’),solve(15, 0, ∅, {x2}, {x4}, ∅, ‘y’), solve(15, 0, ∅, ∅, {x2, x4}, ∅, ‘y’),solve(15, 0, {x2}, {x2}, {x4}, ∅, ‘y’), solve(15, 0, {x2}, {x2, x4}, ∅, ∅, ‘y’)10 to node 9, the number of solve-facts is reduced by 2 since there are two solve-facts in node 10 with parameter valueC = ∅. Node 15 is very similar to node 9; the role of x6, x2, and c3 on the path from 19 up to 15 essentially corresponds tothe role of x5, x1, and c2 on the path from 13 up to 9. The most significant difference between these two paths is that x1and x5 occur in c2 negatively while x2 and x6 occur in c3 positively. By the different sign of x5 and x6, the d-values ‘y’ and‘n’ are swapped when we compare the solve-facts at node 9 with the solve-facts at node 15. The different sign of x1 and x2has the effect that there is one more solve-facts at node 15 than at node 9. The reason for this is, that at node 15, only thefact solve(15, 0, ∅, {x4}, {x2}, ∅, ‘y’) is missing (i.e., if x4 and x6 are set to true and x2 to false, then c3 evaluates to false). Onthe other hand, at node 9, the two facts solve(9, 0, ∅, {x1}, {x4}, ∅, ‘n’) and solve(9, 0, {x1}, {x1}, {x4}, ∅, ‘n’) are missing (i.e.,when x1 is set to true and x4 and x5 are set to false, then c2 evaluates to false; since the hypothesis x1 is true, it can beeither added to S or not).The solve-facts at node 8 and 14 are presented in Table 4. We obtain these facts from the solve facts at node 9 (respec-tively 19) and allowing for x2 (respectively x1) the three possible cases that this new variable is added to P only or N onlyor to both N and P and S. We do not explicitly present the facts at the root node. However, it can now be easily verified-predicate will be true for any value of (i, S). Note that i = 0 in allthat the success-fact cannot be derived because the solvesolve-facts at the root, since all hypotheses appear in the bag at node 0. Hence, for every value of S at node 0, there exists-predicate is true forexactly 1 extension to the hypotheses at bags below node 0 (namely S itself). Intuitively, the solveany value of (i, S) with i = 0 and S ∈ {∅, {x1}, {x2}, {x1, x2}}, because any truth assignment on {x1, x2, x4} can be extendedto the remaining variables by setting x3 and x6 to true and x5 to false, s.t. all clauses in C are true but one manifestation(namely x5) is false.(cid:9)(cid:9)Formally, this property can be seen as follows: Note that there are 18 facts in node 8 where the d-value is ‘y’. These 18facts correspond to all possible values of the parameters S, P , N for the variables x1, x2, and x4. The d-value indicates thaton the considered extensions of these truth assignments, some manifestation (namely x5) is assigned the truth value false.At node 2, through the datalog rule for branch nodes, we will again get 18 facts with all possible assignments of x1, x2and x4, whose d-value is ‘y’. This same set of value combinations will also be present in the solve-facts at the root node 0.But then, for every value of S, the solve-predicate will be true. Thus, the success-fact cannot be derived by the program andwe conclude that the PAP P has no solution.(cid:9)4.3. Relevance enumeration problemThe problem of computing all relevant hypotheses can be clearly expressed as a unary MSO-query and, thus, by amonadic datalog program. Indeed, it is straightforward to extend our Solvability program to a program for the Relevance124G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132Table 4Solvability Example (3).Node814solve factssolve(8, 0, ∅, {x1, x2, x4}, ∅, ∅, ‘y’), solve(8, 0, ∅, {x1, x4}, {x2}, ∅, ‘y’),solve(8, 0, {x2}, {x1, x2, x4}, ∅, ∅, ‘y’), solve(8, 0, ∅, {x1, x2}, {x4}, ∅, ‘y’),solve(8, 0, ∅, {x1}, {x2, x4}, ∅, ‘y’), solve(8, 0, {x2}, {x1, x2}, {x4}, ∅, ‘y’),solve(8, 0, ∅, {x2, x4}, {x1}, ∅, ‘y’), solve(8, 0, ∅, {x4}, {x1, x2}, ∅, ‘y’),solve(8, 0, {x2}, {x2, x4}, {x1}, ∅, ‘y’), solve(8, 0, ∅, {x2}, {x1, x4}, ∅, ‘y’),solve(8, 0, ∅, ∅, {x1, x2, x4}, ∅, ‘y’), solve(8, 0, {x2}, {x2}, {x1, x4}, ∅, ‘y’),solve(8, 0, {x1}, {x1, x2, x4}, ∅, ∅, ‘y’), solve(8, 0, {x1}, {x1, x4}, {x2}, ∅, ‘y’),solve(8, 0, {x1, x2}, {x1, x2, x4}, ∅, ∅, ‘y’), solve(8, 0, {x1}, {x1, x2}, {x4}, ∅, ‘y’),solve(8, 0, {x1}, {x1}, {x2, x4}, ∅, ‘y’), solve(8, 0, {x1, x2}, {x1, x2}, {x4}, ∅, ‘y’),solve(8, 0, ∅, {x1, x2, x4}, ∅, ∅, ‘n’), solve(8, 0, ∅, {x1, x4}, {x2}, ∅, ‘n’),solve(8, 0, {x2}, {x1, x2, x4}, ∅, ∅, ‘n’), solve(8, 0, ∅, {x2, x4}, {x1}, ∅, ‘n’),solve(8, 0, ∅, {x4}, {x1, x2}, ∅, ‘n’), solve(8, 0, {x2}, {x2, x4}, {x1}, ∅, ‘n’),solve(8, 0, ∅, {x2}, {x1, x4}, ∅, ‘n’), solve(8, 0, ∅, ∅, {x1, x2, x4}, ∅, ‘n’),solve(8, 0, {x2}, {x2}, {x1, x4}, ∅, ‘n’), solve(8, 0, {x1}, {x1, x2, x4}, ∅, ∅, ‘n’),solve(8, 0, {x1}, {x1, x4}, {x2}, ∅, ‘n’), solve(8, 0, {x1, x2}, {x1, x2, x4}, ∅, ∅, ‘n’)solve(14, 0, ∅, {x1, x2, x4}, ∅, ∅, ‘n’), solve(14, 0, ∅, {x2, x4}, {x1}, ∅, ‘n’),solve(14, 0, {x1}, {x1, x2, x4}, ∅, ∅, ‘n’), solve(14, 0, ∅, {x1, x2}, {x4}, ∅, ‘n’),solve(14, 0, ∅, {x2}, {x1, x4}, ∅, ‘n’), solve(14, 0, {x1}, {x1, x2}, {x4}, ∅, ‘n’),solve(14, 0, ∅, {x1, x4}, {x2}, ∅, ‘n’), solve(14, 0, ∅, {x4}, {x1, x2}, ∅, ‘n’),solve(14, 0, {x1}, {x1, x4}, {x2}, ∅, ‘n’), solve(14, 0, ∅, {x1}, {x2, x4}, ∅, ‘n’),solve(14, 0, ∅, ∅, {x1, x2, x4}, ∅, ‘n’), solve(14, 0, {x1}, {x1}, {x2, x4}, ∅, ‘n’),solve(14, 0, {x2}, {x1, x2, x4}, ∅, ∅, ‘n’), solve(14, 0, {x2}, {x2, x4}, {x1}, ∅, ‘n’),solve(14, 0, {x1, x2}, {x1, x2, x4}, ∅, ∅, ‘n’), solve(14, 0, {x2}, {x1, x2}, {x4}, ∅, ‘n’),solve(14, 0, {x2}, {x2}, {x1, x4}, ∅, ‘n’), solve(14, 0, {x1, x2}, {x1, x2}, {x4}, ∅, ‘n’),solve(14, 0, ∅, {x1, x2, x4}, ∅, ∅, ‘y’), solve(14, 0, ∅, {x2, x4}, {x1}, ∅, ‘y’)solve(14, 0, {x1}, {x1, x2, x4}, ∅, ∅, ‘y’), solve(14, 0, ∅, {x1, x2}, {x4}, ∅, ‘y’)solve(14, 0, ∅, {x2}, {x1, x4}, ∅, ‘y’), solve(14, 0, {x1}, {x1, x2}, {x4}, ∅, ‘y’),solve(14, 0, ∅, {x1}, {x2, x4}, ∅, ‘y’), solve(14, 0, ∅, ∅, {x1, x2, x4}, ∅, ‘y’),solve(14, 0, {x1}, {x1}, {x2, x4}, ∅, ‘y’), solve(14, 0, {x2}, {x1, x2}, {x4}, ∅, ‘y’),solve(14, 0, {x2}, {x2}, {x1, x4}, ∅, ‘y’), solve(14, 0, {x1, x2}, {x1, x2}, {x4}, ∅, ‘y’),solve(14, 0, {x2}, {x1, x2, x4}, ∅, ∅, ‘y’), solve(14, 0, {x2}, {x2, x4}, {x1}, ∅, ‘y’),solve(14, 0, {x1, x2}, {x1, x2, x4}, ∅, ∅, ‘y’)enumeration problem: Suppose that some hypothesis h occurs in the bag of the root r of T . Then h is relevant if and onlyif there exists a subset S ⊆ H(r) and an index i, s.t. h ∈ S and S can be extended to a solution ¯S i ⊆ H(Tr) of the PAP P .Naively, one can compute all relevant hypotheses by considering the tree decomposition T as rooted at various nodes, s.t.each h ∈ H is contained in the bag of at least one such root node. Obviously, this method has quadratic time complexityw.r.t. the data size.However, one can do better by computing the solve-facts at each node v in T simultaneously both for a bottom–uptraversal of T and for a top–down traversal of T (by means of a new predicate solve ↓). The tree decomposition can ofcourse be modified in such a way that every hypothesis h ∈ H occurs in at least one leaf node of T . Moreover, for everybranch node v in the tree decomposition, we insert a new node u as new parent of v, s.t. u and v have identical bags.Hence, together with the two child nodes of v, each branch node is “surrounded” by three neighboring nodes with identicalbags. It is thus guaranteed that a branch node always has two child nodes with identical bags – no matter where T isrooted.Then the relevant hypotheses can be obtained via the solve ↓ (v, . . .) facts which are true in the minimal model of theinterpreted program for all leaf nodes v of T (since these facts correspond precisely to the solve(v, . . .) facts if T wererooted at v). The details of the additional datalog rules required compared with the Solvability test are given in Fig. 6. Ofcourse, we now also need a new predicate aux ↓, which plays the analogous role for the definition of the solve ↓-predicate asthe aux-predicate in Fig. 5 did for the definition of solve. Further analogies between the definition of solve ↓ and solveconcernthe various node types: For a top–down traversal, the root node plays the role that the leaf node formerly played. The childnode of a variable (respectively clause) introduction node plays in the top–down traversal the role of a variable (respectivelyclause) removal node in the bottom–up traversal, i.e.: the bag at such a node has one variable (respectively clause) less thanthe bag of the previously visited node. Likewise, the child node of a variable (respectively clause) removal node plays in thetop–down traversal the role of a variable (respectively clause) introduction node. The only tricky case is that of a branchnode. Note that our program in Fig. 6 does not derive any solve ↓-facts for the branch nodes themselves. For deriving thesolve D-facts at the child nodes of a branch node, we distinguish two cases: If the top–down traversal continues with thefirst child v of a branch node b, then the parent of b plays the role of v 1 and the second child of b plays the role of v 2.Care has to be taken that the solve ↓-predicate at node v is defined in terms of the solve ↓-predicate at the parent of bbut in terms of the solve ↓-predicate at the second child of b. Analogously, the solve ↓-predicate at v is defined if v is thesecond child of a branch node.G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132125Program Relevance/* root node. */solve ↓ (v, S, 0, P , N, C1, d) ← root(v), bag(v, X, C), svar(v, S), S ⊆ P ,P ∪ N = X , P ∩ N = ∅, check(P , N, C1, C, d)./* child of variable introduction node. */aux ↓ (v, S, i, 0, P , N, C1, d) ← bag(v, X, C), child1(v, v 1), bag(v 1, X (cid:22) {x}, C),solve ↓ (v 1, S, i, P (cid:22) {x}, N, C1, d), x /∈ S.aux ↓ (v, S, i, 0, P , N, C1, d) ← bag(v, X, C), child1(v 1, v), bag(v 1, X (cid:22) {x}, C),solve ↓ (v 1, S, i, P , N (cid:22) {x}, C1, d).aux ↓ (v, S, i, 1, P , N, C1, d) ← bag(v, X, C), child1(v 1, v), bag(v 1, X (cid:22) {x}, C),solve ↓ (v 1, S (cid:22) {x}, i, P (cid:22) {x}, N, C1, d)./* child of clause introduction node. */solve ↓ (v, S, i, P , N, C1, d) ← bag(v, X, C), child1(v, v 1), bag(v 1, X, C (cid:22) {c}),solve ↓ (v 1, S, i, P , N, C1 (cid:22) {c}, d)./* child of variable removal node. */solve ↓ (v, S, i, P (cid:22) {x}, N, C1 ∪ C2, d1) ← bag(v, X (cid:22) {x}, C), child1(v, v 1),bag(v 1, X, C), solve ↓ (v 1, S, i, P , N, C1, d1), check({x}, ∅, C2, C, d2).solve ↓ (v, S, i, P , N (cid:22) {x}, C1 ∪ C2, d1 or d2) ← bag(v, X (cid:22) {x}, C), child1(v, v 1),bag(v 1, X, C), solve ↓ (v 1, S, i, P , N, C1, d1), check(∅, {x}, C2, C, d2).solve ↓ (v, S (cid:22) {x}, i, P (cid:22) {x}, N, C1 ∪ C2, d1) ← bag(v, X (cid:22) {x}, C), child1(v, v 1),bag(v 1, X, C), solve ↓ (v 1, S, i, P , N, C1, d1), check({x}, ∅, C2, C, d2), hyp(x)./* child of clause removal node. */solve ↓ (v, S, i, P , N, C1 ∪ C2, d1) ← bag(v, X, C (cid:22) {c}), child1(v, v 1),bag(v 1, X, C), solve ↓ (v 1, S, i, P , N, C1, d1), check(P , N, C2, {c}, d2)./* first child of branch node. */aux ↓ (v, S, i1, i2, P , N, C1 ∪ C2, d1 or d2) ← bag(v, X, C),child1(v, b), child1(b, v 1), bag(v 1, X, C), child2(v 2, b), bag(v 2, X, C),solve ↓ (v 1, S, i1, P , N, C1, d1), solve(v 2, S, i2, P , N, C2, d2)./* second child of branch node. */aux ↓ (v, S, i1, i2, P , N, C1 ∪ C2, d1 or d2) ← bag(v, X, C),child2(v, b), child1(b, v 1), bag(v 1, X, C), child1(b, v 2), bag(v 2, X, C),solve ↓ (v 1, S, i1, P , N, C1, d1), solve(v 2, S, i2, P , N, C2, d2)./* child nodes of variable introduction and branch nodes: aux ⇒ solve */solve ↓ (v, S, i, P , N, C, d) ← aux ↓ (v, S, i1, i2, P , N, C, d), reduce ↓ (v, S, i, i1, i2)./* result (at leaf nodes). */relevant(h) ← leaf (v), bag(v, X, C), solve ↓ (v, S, i, P , N, C, ‘n’),(cid:9) ↓ (v, S, i), h ∈ S.not solve(cid:9) ↓ (v, S, i) ← bag(v, X, C), solve ↓ (v, S, i, P , N, C, ‘y’).solveFig. 6. Relevance enumeration program.The resulting algorithm works in linear time since it essentially just doubles the computational effort of the Solvabilityprogram. In total, we thus get the following result.Theorem 4.7. The datalog program in Fig. 6 solves the Relevance enumeration problem of PAPs, i.e.: Suppose that the program isapplied to a τtd-structure Atd encoding a PAP P = (cid:5)V , H, M, C(cid:6) together with a tree decomposition T of the clause set C. Then,for every h ∈ H , the fact relevant(h) is true in the minimal model of this program and the input τtd-structure Atd if and only if thehypothesis h is relevant in the PAP P . Moreover, for any PAP P = (cid:5)V , H, M, C(cid:6) and tree decomposition T of width at most w, theprogram can be evaluated in time O(25∗3w+2 ∗ |P|).Note that in all programs presented in Section 4, we consider the tree decomposition as part of the input. It has alreadybeen mentioned in Section 2.1 that, in theory, for every given value w (cid:2) 1, it can be decided in linear time (w.r.t. thesize of the input structure), if some structure has treewidth at most w. Moreover, in case of a positive answer, a treedecomposition of width w can also be computed in linear time, see [8]. We have also mentioned in Section 2.1, that thepractical usefulness of this linearity is limited due to excessively big constants [39]. At any rate, the improvement of treedecomposition algorithms is an area of very active research and considerable progress has recently been made in developingheuristic-based tree decomposition algorithms [39,9,49,10].126G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132Table 5Processing time in ms for the SAT problem.tw333333#V531347395532 765337 859#Cl948522593449 149506 790Table 6Processing time in ms for the Solvability problem.tw33333333333#H12347111519232731#M12347111519232731#V36912213345576981935. Implementation and experimental results5.1. Datalog approach#tn24195215723 793207 4382 120 361#Cl12347111519232731MD (ms)MiniSat (ms)0.040.20.88.165.5643.9< 10< 10< 10< 1040430#tn312213469105141193229265301MD0.30.50.60.91.52.32.93.94.75.36.1MONA870171012 160––––––––To test the performance and, in particular, the scalability of our new algorithms, we have implemented our SAT andSolvability programs in C++.The SAT experiments were conducted on a PC with Intel(R) Pentium(R) D 3.00 GHz CPU, 1 GB of main memory, 2 GBof virtual memory, running Linux (Ubuntu) with kernel 2.6.22-14-generic. The experiments of the Solvability program wereconducted on Linux kernel 2.6.17 with a 1.60 GHz Intel Pentium(M) processor and 512 MB of memory. We measured theprocessing time on different input parameters such as the number of variables, clauses, hypotheses, and manifestations. Thetreewidth in all the test cases was 3.Due to the lack of available test data, we generated a balanced normalized tree decomposition and test data sets withincreasing input parameters by expanding the tree in a breadth-first style. We have ensured that all different kinds of nodesoccur evenly in the tree decomposition.We compared the performance of our program with the open-source SAT solver MiniSat, see http://minisat.se/. Theoutcome of the tests for the SAT problem are shown in Table 5. In the table, #V, #Cl and #tn represent the number ofvariables, the number of clauses and the number of tree nodes, respectively. Note that the time measurement for MiniSat isrestricted to the unit of seconds, thus the time consumption under 10 ms could not be obtained.The results show that, for low treewidth of the underlying formulae, the performance of our method is comparablewith an up-to-date SAT solver – with the SAT solver being faster by a factor of approximately 1.5 in most cases. Of course,this comparison should take into account that the current SAT solvers have evolved from intensive research and numerouscompetitions over 20 years, whereas our system is just a prototype implementation. Moreover, it has been recently observedthat SAT solvers actually do take advantage of low treewidth without being specifically tuned for such instances, see [4]. Thepotential of our datalog-based approach is therefore not to be seen as a competitor of up-to-date SAT solvers but rather asa method that is applicable to problems that go far beyond SAT. In particular, our method is not restricted to NP-problems.This is illustrated by the application to abduction in our paper or to circumscription and disjunctive logic programming in[36,37]. Without the restriction to bounded treewidth, all these problems lie on the second level of the polynomial hierarchy.The outcome of the tests for the Solvability problem is shown in Table 6 where tw stands for the treewidth; #H, #M,#V, #Cl and #tn stand for the number of hypotheses, manifestations, all variables, clauses and tree nodes, respectively. Theprocessing time (in ms) obtained with our implementation of the monadic datalog approach are displayed in the columnlabeled “MD”. The measurements nicely reflect an essentially linear increase of the processing time with the size of theinput. Moreover, there is obviously no “hidden” constant which would render the linearity useless.We also wanted to compare our implementation with the standard MSO-to-FTA approach [3,25]. For this purpose, wecarried out some experiments with a prototype implementation using MONA [38] for the MSO model checking. The timemeasurements of these experiments are shown in the last column of Table 6. Due to problems discussed in Section 5.2,MONA does not ensure linear data complexity. Hence, all tests below line 3 of the table failed with “out-of-memory” errors.G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132127General Generic MSO-to-FTA approachInput: MSO-formula ϕ, treewidth w, structure AOutput: if tw(A) (cid:2) w then output “Yes” / “No”, else output “tw(A) > w”.Transformation of the MSO-formula ϕ.I. from ϕ and w, compute the MSO-formula ϕ∗;Computation of a colored, binary tree T ∗.II. compute a tree decomposition T of A with width w;if tw(A) > w then HALT with result “tw(A) > w”;III. from A and T , compute a colored, binary tree T ∗;MSO-model checking.IV. check if ϕ∗evaluates to true over T ∗;(a) compute an FTA equivalent to ϕ∗(b) check if the FTA accepts the tree T ∗:;Fig. 7. Generic MSO-to-FTA approach.Fig. 8. System architecture of MSO-to-FTA approach.Moreover, also in cases where the exponential data complexity does not yet “hurt”, our datalog approach outperforms theMSO-to-FTA approach by a factor of 100 or even more.5.2. MSO-to-FTA approachRecipes to devise concrete algorithms based on Courcelle’s Theorem can be found in the literature, see e.g. [3,12,11,25].The principal steps of these approaches are described in Fig. 7.The problem of evaluating an MSO-formula ϕ over a finite structure A of treewidth at most w is transformed into anequivalent problem of evaluating an MSO-formula ϕ∗, whichdepends on the original formula ϕ and the fixed treewidth w, has to be computed (step I). The computation of the colored,binary tree T ∗proceeds in two steps: First, a tree decomposition T of the input structure A with width w is computed(step II). Then a colored, binary tree T ∗has the sametree structure as T . The final step is the actual model-checking (step IV). The problem of evaluating the MSO-formula overthe tree T ∗is thus reduced to an equivalent tree language recognition problem. To this end, the formula ϕ∗is transformedinto an equivalent FTA (step IVa). Then it is checked if T ∗corresponding to A and T is computed (step III). In particular, T ∗. To this end, the formula ϕ∗over colored, binary trees T ∗is accepted by this FTA (step IVb).We have implemented the Solvability problem of propositional abduction based on the MSO-formula ϕ in the proof ofTheorem 3.10. This MSO-formula was transformed into ϕ∗in an ad hoc manner – depending on the chosen treewidth w.Rather than computing explicitly the tree decomposition T , we simply considered the tree decomposition as an additionalpart of the input. Our transformation of T into T ∗essentially implements the algorithm from [25] – apart from somesimplifications which are possible here (e.g., due to the fact that we have no predicates over sets of constants in the inputdatabase). For the last step, we decided to take advantage of an existing MSO model checking tool, namely MONA [38]. Tothe best of our knowledge, MONA is the only existing such tool.Experimental results with our prototype implementation on several instances of the Solvability problem are reportedin the last column of Table 6. The observed runtime is in sharp contrast to the linear time behavior according to thetheoretical results shown in Section 3. An analysis of the various components of our prototype has revealed that the wayhow MONA evaluates an MSO-formula ϕ∗and(a representation of) the tree T ∗to a single formula whose validity is then checked. The correct way of handling this modelchecking problem according to [25] is depicted in Fig. 8. The steps I, II, and III are clear. However, it is crucial to divide stepIV into two substeps: step IVa, which computes a finite tree automaton from the MSO-formula ϕ∗, and step IVb, which runsthe FTA on every tree T ∗. The important point to notice is that steps I and IVa (depicted on the left-hand side of Fig. 8)are carried out once and for all while the steps II, III, and IVb (shown on the right-hand side) are repeated for every singleinput structure Ai .is very problematical in that it combines the MSO-formula ϕ∗over a tree T ∗128G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132In contrast, MONA also considers the ground atoms encoding T ∗as part of the formula and then compiles the resultingMSO-formula (consisting of ϕ∗) into an FTA. In other words, the size of the FTA grows (exponen-tially!) with the size of T ∗. Consequently, on the one hand, the runtime needed by MONA is exponential. On the otherhand, the state explosion of the FTA led to an “out-of-memory” error of MONA already for very small problem sizes. Inthe experiments described in Table 6, this negative effect already happened for problem instances with 12 variables and4 clauses.and the encoding of T ∗In fact, we have also used the algorithm of transforming the MSO formulae together with the ground atoms of thefinite structure into finite tree automata. Surprisingly, the transformation failed with an “out-of-memory” error. This iscounter-intuitive because the formula itself should is clearly smaller than the formula together with the ground atoms ofthe structure. According to our test results, it worked at least for instances with small sizes. The reason was that, witha very small set of ground atoms, the optimization module of MONA managed to remove many useless states during thetransformation. However, when using MONA with the MSO-formula only, the optimization approach seems to be not ineffect.We therefore made another attempt following the MSO-to-FTA approach by implementing step IVa in Fig. 8 ourselves.Alas, this attempt led to failure yet before we were able to feed any input data to the program. The principal problem withthe MSO-to-FTA approach is the “state explosion” of the resulting FTA (cf. [26,41]), which tends to occur even if one wantsto evaluate comparatively simple MSO-formulae over trees. The situation gets even worse if we consider MSO on structureswith bounded treewidth, since the original (possibly simple) MSO-formula ϕ over a finite structure first has to be transformedinto an equivalent MSO-formula ϕ∗over trees. Hence, this transformation (e.g., by the algorithm in [25]) leads to a muchmore complex formula (in general, even with additional quantifier alternations) than the original formula. In summary, wehave come to the same conclusion as Grohe in [32], namely, that the algorithms derived via Courcelle’s Theorem are “uselessfor practical applications” and that the main benefit of Courcelle’s Theorem is that it provides “a simple way to recognize aproperty as being linear time computable”.6. Conclusions and future workIn this paper, we have used Courcelle’s Theorem to prove the fixed-parameter tractability of a whole range of problemsin knowledge representation and reasoning. More precisely, we have shown that many problems in the area of abduction,closed world reasoning, circumscription, and disjunctive logic programming are solvable in linear time if the treewidth ofthe considered formulae or programs is bounded by some constant. We have also experimented with a prototype implemen-tation based on the standard MSO-to-FTA approach [25]. However, in accordance with [32], we have come to the conclusionthat this generic way of turning the MSO characterization of a problem into an algorithm is not feasible in practice – despitethe theoretical fixed-parameter linearity of this approach.Thus, in the second part of this work, we have investigated an alternative method for turning theoretical tractability re-sults obtained via Courcelle’s Theorem into feasible computations. Based on the monadic datalog approach presented in [30],we have constructed new algorithms for logic-based abduction. The experimental results obtained with an implementationof these algorithms underline the feasibility of this approach.The datalog programs presented in this paper were obtained by an ad hoc construction rather than via a generic trans-formation from MSO. Nevertheless, we are convinced that the idea of a bottom–up propagation of certain SAT and UNSATconditions is quite generally applicable. As a next step, we are therefore planning to devise new algorithms based on ourmonadic datalog approach also for other problems in the area of knowledge representation and reasoning. In fact, for cir-cumscription and for disjunctive logic programming, related algorithms have already been presented recently, see [36,37].In this paper, the monadic datalog approach from [30] has only been applied to decision problems (e.g., is a given PAPsolvable?) or to the enumeration problem of a unary query with one free individual variable (e.g., for a given PAP, computeall relevant hypotheses). In the future, we are planning to extend this approach to other kinds of problems like countingproblems (e.g., how many solutions does a given PAP have?) or enumeration problems for more general queries (e.g., for agiven PAP, compute all solutions). In fact, first steps in this direction have already been made in [36,37].Our notion of treewidth of the problems considered here corresponds to the treewidth of the primal graph of the un-derlying structures, i.e.: this graph has as vertices all domain elements occurring in the structure; moreover, two verticesare adjacent in the primal graph if the corresponding domain elements jointly occur in some tuple in the structure. Clearly,also other mappings of the knowledge representation and reasoning problems to graphs – in particular, to directed graphs– are conceivable. Moreover, recently, other interesting notions of decompositions and width have been developed, see e.g.[7,6,43,35]. We are planning to investigate their applicability to the problems studied here.AcknowledgementsWe are very grateful to the anonymous referees as well as to Michael Jakl, Stefan Rümmele, and Stefan Woltran for theirvaluable comments on previous versions of this article.G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132129Appendix A. Proof of Lemma 4.1We have to show that the solve-predicate computed by the SAT-program has the intended meaning: For all values ofv, P , N, C , the ground fact solve(v, P , N, C) is true in the minimal model of the interpreted program if and only if thefollowing condition holds:Property A. There exists an extension J of the assignment (P , N) to Var(Tv ), s.t. (Cl(Tv ) \ Cl(v)) ∪ C is true in J while for all clausesc ∈ Cl(v) \ C , the restriction c|Var(Tv ) is false in J .The proof goes by structural induction on the tree decomposition.Base case. For a leaf node v, exactly those facts solve(v, P , N, C) with C ⊆ Cl(v) are true in the minimal model, for whichthe following condition holds: ∀c ∈ C , c|Var(v) evaluates to true in the assignment (P , N) and ∀c ∈ Cl(v) \ C , c|Var(v) evaluatesto false in (P , N).The only extension J of (P , N) to Var(Tv ) is (P , N) itself. Hence, solve(v, P , N, C) is true in the minimal model if andonly if Property A holds.Induction step – “only if”-direction. Suppose that for arbitrary values of v, P , N, C , the ground fact solve(v, P , N, C) is true inthe minimal model of the interpreted program. In order to show that Property A holds, we distinguish five cases accordingto the five types of internal nodes.(1) Variable removal node with removal of variable x. Let solve(v, P , N, C) be true in the minimal model and let v 1denote the child of v. Then either solve(v 1, P (cid:22) {x}, N, C) or solve(v 1, P , N (cid:22) {x}, C) is also true in the minimal model.We restrict ourselves to the first case here. The latter is treated analogously. By the induction hypothesis, there exists anextension J of the assignment (P (cid:22) {x}, N) to Var(Tv1 ) = Var(Tv ), s.t. (Cl(Tv ) \ Cl(v)) ∪ C is true in J while for all clausesc ∈ C , the restriction c|Var(Tv ) is false in J . Then J is also the desired extension of (P , N).(cid:9)(cid:9)(cid:9). Hence,(cid:9) \ {c} for some C(2) Clause removal node with removal of clause c. Let solve(v, P , N, C) be true in the minimal model and let v 1 denote(cid:9)) is true in the minimalthe child of v. By construction, C is of the form C(cid:9)model. By the induction hypothesis, there exists an extension J of (P , N) to Var(Tv ) = Var(Tv1 ), s.t. (Cl(Tv1 ) \ Cl(v 1)) ∪ Cis true in J while for all clauses d ∈ Cl(v 1) \ C, the restriction d|Var(Tv ) is false in J . Note that Cl(v) ∪ {c} = Cl(v 1) and(cid:9)J is also the desired extension of (P , N) in the node v, i.e., (Cl(Tv ) \ Cl(v)) ∪ C = (Cl(Tv1 ) \ Cl(v 1)) ∪ CC ∪ {c} = Cis true in J . Moreover, for every d in Cl(v) \ C = Cl(v 1) \ C(cid:9) ⊆ Cl(v 1), s.t. solve(v 1, P , N, C, the restriction d|Var(Tv ) is false in J .(3) Variable introduction node with introduction of variable x. Let solve(v, P (cid:22) {x}, N, C) be true in the minimal modeland let v 1 denote the child of v. The case solve(v, P , N (cid:22) {x}, C) is treated analogously and, therefore, omitted here. Thereexist sets C1 and C2 with C = C1 ∪ C2, s.t. solve(v 1, P , N, C1) and true({x}, ∅, C2, Cl(v)) are true in the minimal model.The latter condition means that C2 contains exactly those clauses in Cl(v) where the propositional variable x occurs inunnegated form. Of course, we have Cl(Tv1 ) = Cl(Tv ) and Var(Tv1 ) = Var(Tv ) \ {x}. By the induction hypothesis, there existsan extension J 1 of (P , N) to Var(Tv1 ), s.t. (Cl(Tv1 ) \ Cl(v 1)) ∪ C1 is true in J 1 while for all clauses c ∈ Cl(v 1) \ C1, therestriction c|Var(Tv1 ) is false in J 1. Now consider the extension J of (P (cid:22) {x}, N) to Var(Tv ) which is obtained by extendingJ 1 to Var(Tv ) = Var(Tv1 ) ∪ {x} with J (x) = true. By construction, all clauses in C \ C1 have a positive occurrence of x and aretherefore true in J . Hence, all clauses in (Cl(Tv ) \ Cl(v)) ∪ C are indeed true in J . On the other hand, by the definition of C ,for all clauses c ∈ Cl(v) \ C , the restriction c|Var(Tv ) is false in J . Hence,J is indeed the desired extension of the assignment(P (cid:22) {x}, N).(4) Clause introduction node with introduction of clause c. We have Var(Tv1 ) = Var(Tv ) and Cl(Tv1 ) = Cl(Tv ) \ {c}. Bythe connectedness condition, c|Var(Tv ) = c|Var(v). Hence, the truth value of c|Var(Tv ) in any extension of (P , N) to Var(Tv )coincides with the truth value of c|Var(v) in (P , N). Now let solve(v, P , N, C) be true in the minimal model and let v 1 denotethe child of v. By construction, there exist facts solve(v 1, P , N, C1) and true(P , N, C2, {c}) with C = C1 ∪ C2 which are truein the minimal model. Thus, by the induction hypothesis, there exists an extension J of (P , N) to Var(Tv1 ) = Var(Tv ), s.t.(Cl(Tv1 ) \ Cl(v 1)) ∪ C1 is true in J while for all clauses d ∈ Cl(v 1) \ C1, the restriction d|Var(Tv ) is false in J . We claim that Jis the desired extension of (P , N) in v.To see this, first consider the case that c|Var(v) is false in (P , N). Hence, C2 = ∅ and, therefore, C = C1. Moreover, (Cl(Tv ) \Cl(v)) ∪ C = (Cl(Tv1 ) \ Cl(v 1)) ∪ C1 is true in J . On the other hand, for all clauses d in (Cl(v) \ C) = (Cl(v) \ C1), we eitherhave d ∈ Cl(v 1) \ C1 or d = c. In either case, the restriction d|Var(Tv ) is false in J .It remains to consider the case that c|Var(v) is true in (P , N). In this case, C2 = {c} and C is of the form C = C1 ∪ {c}.We have (Cl(Tv ) \ Cl(v)) ∪ C = (Cl(Tv1 ) \ Cl(v 1)) ∪ C1 ∪ {c}. We are considering the case that c is true in (P , N) and thusin J . Hence, (Cl(Tv ) \ Cl(v)) ∪ C is true in J . On the other hand, for all clauses d in (Cl(v) \ C) = (Cl(v 1) \ C1), the restrictiond|Var(Tv ) is false in J .(5) Branch node. Suppose that solve(v, P , N, C) is true in the minimal model with C = C1 ∪ C2, s.t. solve(v 1, P , N, C1)and solve(v 2, P , N, C2) are also true in the minimal model. By the induction hypothesis, for each i ∈ {1, 2}, the assignment(P , N) can be extended to some interpretation J i on Var(Tv i ), s.t. (Cl(Tv i ) \ Cl(v)) ∪ Ci is true in J i while for all clausesc ∈ Cl(v) \ Ci , the restriction c|Var(Tvi ) is false in J i . We are using the fact that Cl(v) = Cl(v i) holds. Note that, by thedefinition of normalized tree decompositions and by the connectedness condition, Var(Tv1 ) ∩ Var(Tv2 ) = Var(v). Moreover,130G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132J 1 and J 2 coincide on Var(v) since they both extend (P , N). Hence, we may define an extension J of (P , N) to Var(Tv ) asfollowsJ (x) =⎧⎨⎩(P , N)(x)J 1(x)J 2(x)if x ∈ Var(v),if x ∈ Var(Tv1 ) \ Var(v),if x ∈ Var(Tv2 ) \ Var(v).We claim that J has the desired property: Every clause c in (Cl(Tv ) \ Cl(v)) ∪ C is contained in at least one of the sets(Cl(Tv i ) \ Cl(v)) ∪ Ci with i ∈ {1, 2}. Hence, each such clause c is true in J by the induction hypothesis. Likewise, for everyclause c ∈ Cl(v) \ C , both restrictions c|Var(Tvi ) are false in J . Hence, also c|Var(Tv ) is false in J .Induction step – “if”-direction. Suppose that for arbitrary values of v, P , N, C , Property A holds, i.e., v denotes a node in Tand (P , N) is an assignment on the variables Var(v). Moreover, C is an arbitrary clause set with C ⊆ Cl(v) and there existsan extension J of (P , N) to Var(Tv ), s.t. (Cl(Tv ) \ Cl(v)) ∪ C is true in J while for all clauses d ∈ Cl(v) \ C , the restrictiond|Var(Tv ) is false in J . We have to show that then the fact solve(v, P , N, C) is indeed true in the minimal model of theinterpreted program. Again, we distinguish the five cases according to the five types of internal nodes.(1) Variable removal node with removal of variable x. Note that Cl(Tv ) = Cl(Tv1 ), Cl(v) = Cl(v 1), Var(Tv ) = Var(Tv1 ), andVar(v) = Var(v 1) \ {x}, where v 1 is the child of v. We have to distinguish two cases depending on whether J (x) = true orJ (x) = false. We only treat the first case here. The second one goes analogously. Since J (x) = true,J is also an extensionof (P (cid:22) {x}, N). By assumption, (Cl(Tv ) \ Cl(v)) ∪ C = (Cl(Tv1 ) \ Cl(v 1)) ∪ C is true in J while for all clauses d ∈ Cl(v) \ C =Cl(v 1) \ C , the restriction d|Var(Tv ) = d|Var(Tv1 ) is false in J . Hence, by the induction hypothesis, solve(v 1, P (cid:22) {x}, N, C) is truein the minimal model and, therefore, solve(v, P , N, C) is also true.(2) Clause removal node with removal of clause c. Note that Cl(Tv ) = Cl(Tv1 ), Cl(v) = Cl(v 1) \ {c}, Var(Tv ) = Var(Tv1 ),(cid:9) = C ∪ {c}. By assumption, all clauses in (Cl(Tv ) \ Cl(v)) ∪ C =, the restriction d|Var(Tv ) =(cid:9)) is true in the minimal model and, therefore,and Var(v) = Var(v 1), where v 1 is the child of v. Let C(Cl(Tv1 ) \ Cl(v 1)) ∪ Cd|Var(Tv1 ) is false in J . Hence, by the induction hypothesis, solve(v 1, P , N, Csolve(v, P , N, C) is also true in the minimal model.are true in J . On the other hand, for all clauses d in Cl(v) \ C = Cl(v 1) \ C(cid:9)(cid:9)(3) Variable introduction node with introduction of variable x. In this case, we have Cl(Tv1 ) = Cl(Tv ), Cl(v 1) = Cl(v),Var(Tv1 ) = Var(Tv ) \ {x}, and Var(v 1) = Var(v) \ {x}, where v 1 is the child of v. We distinguish two cases depending onwhether x ∈ P or x ∈ N. We only treat the first case here. The second one goes analogously. Consider the assignmentJ to Var(Tv1 ) = Var(Tv ) \ {x}. We define(P \ {x}, N) on Var(v 1) = Var(v) \ {x}. Moreover, let J 1 denote the restriction ofC1 = {c ∈ C : c|Var(Tv1 ) is true in J 1}. By the connectedness condition, the clauses in (Cl(Tv1 ) \ Cl(v 1)) = (Cl(Tv ) \ Cl(v)) donot contain the variable x. Hence, they have the same truth value in J 1 as in J , namely true. Thus, (Cl(Tv1 ) \ Cl(v 1)) ∪ C1is true in J 1. On the other hand, for every d ∈ Cl(v 1) \ C1, the restriction d|Var(Tv1 ) is false in J 1. Thus, by the inductionhypothesis, solve(v 1, P \ {x}, N, C1) is true in the minimal model.Now let C2 be defined as C2 = {c ∈ C : x occurs unnegated in c}. Then C = C1 ∪ C2 holds. This is due to the fact that allclauses in C are true in J . Hence, they are already true in J 1 or they become true in J because of an unnegated occurrenceof x. Moreover, by the definition of the true-predicate, the fact true({x}, ∅, C2, C) is true in the minimal model. Thus, alsosolve(v, P , N, C) is true in the minimal model.(4) Clause introduction node with introduction of clause c. In this case, we have Cl(Tv1 ) = Cl(Tv ) \ {c}, Cl(v 1) = Cl(v) \ {c},Var(Tv1 ) = Var(Tv ) and Var(v 1) = Var(v), where v 1 is the child of v. By the connectedness condition, c|Var(Tv ) = c|Var(v)holds. Let C2 be defined as C2 = {c} if c|Var(v)In either case, the facttrue(P , N, C2, {c}) is true in the minimal model.is true in (P , N) and C2 = ∅ otherwise.Now let C1 = C \ C2. Clearly, (Cl(Tv1 ) \ Cl(v 1)) ∪ C1 ⊆ (Cl(Tv ) \ Cl(v)) ∪ C holds. By assumption, (Cl(Tv ) \ Cl(v)) ∪ C is truein J . Hence, (Cl(Tv1 ) \ Cl(v 1)) ∪ C1 is also true in J . Moreover, for all clauses d in Cl(v 1) \ C1 ⊆ Cl(v) \ C , the restrictiond|Var(Tv1 ) = d|Var(Tv ) is false in J . Hence, by the induction hypothesis, solve(v 1, P , N, C1) is true in the minimal model. Thus,by construction, also solve(v, P , N, C) is true in the minimal model.(5) Branch node. Let v 1, v 2 denote the children of v and let i ∈ {1, 2}. By the definition of branch nodes, we have Cl(v) =Cl(v 1) = Cl(v 2) and Var(v) = Var(v 1) = Var(v 2). Let Ci be defined as Ci = {c ∈ C : c|Var(Tvi ) is true in J }. By assumption, forevery c ∈ C , the restriction c|Var(Tv ) is true in J . Hence, for every c ∈ C , at least one of c|Var(Tv1 ) and c|Var(Tv2 ) is true in J .Thus, C = C1 ∪ C2.Let J i denote the restriction ofJ to Var(Tv i ). By the connectedness condition, for any clause d ∈ (Cl(Tv i ) \ Cl(v i)), thetruth value of d|Var(Tvi ) in J i is identical to the truth value of d in J , namely true. Hence, (Cl(Tv i ) \ Cl(v i)) ∪ Ci is true in J i .Now consider an arbitrary clause d ∈ Cl(v i) \ Ci = Cl(v) \ Ci ⊇ Cl(v) \ C . If d is in Cl(v) \ C then d|Var(Tv ) is false in Jand, therefore, also d|Var(Tvi ) is false in J i . On the other hand, if d is in C but not in Ci then, by the definition of Ci ,d|Var(Tvi ) is also false in J i . Hence, in any case, the restriction d|Var(Tvi ) is false in J i . Thus, by the induction hypothesis,solve(v i, P , N, Ci) is true in the minimal model for both i ∈ {1, 2}. Thus, solve(v, P , N, C) with C = C1 ∪ C2 is also true in theminimal model.G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132131Auxiliary Predicatescheck(P , N, C1, C, ‘y’) ← true(P , N, C1, C), x ∈ N, man(x).check(P , N, C1, C, ‘n’) ← true(P , N, C1, C), not check(P , N, C1, C, ‘y’).defined(v, S, i1, i2) ← aux(v, S, i1, i2, P , N, C, d),equal(v, S, i1, i2, j1, j2) ← defined(v, S, i1, i2), defined(v, S, j1, j2),not diff (v, S, i1, i2, j1, j2).diff (v, S, i1, i2, j1, j2) ← defined(v, S, i1, i2), defined(v, S, j1, j2),aux(v, S, i1, i2, P , N, C, d), not aux(v, S, j1, j2, P , N, C, d).diff (v, S, i1, i2, j1, j2) ← defined(v, S, i1, i2), defined(v, S, j1, j2),aux(v, S, j1, j2, P , N, C, d), not aux(v, S, i1, i2, P , N, C, d).less( j1, j2, i1, i2) ← j1 < i1.less( j, j2, j, i2) ← j2 < i2.reduced_smaller(v, S, i, i1, i2) ← less( j1, j2, i1, i2), reduce(v, S, i, j1, j2).duplicate(v, S, i1, i2) ← less( j1, j2, i1, i2), equal(v, S, i1, i2, j1, j2).open(v, S, i, i1, i2) ← j < i, not reduced_smaller(v, S, j, i1, i2).reduce(v, S, i, i1, i2) ← 0 (cid:2) i < K , defined(v, S, i1, i2),not reduced_smaller(v, S, i, i1, i2), not duplicate(v, S, i1, i2),not open(v, S, i, i1, i2).Fig. B.1. reduce and auxiliary predicates.Appendix B. Datalog definition of the reduce-predicateAs was mentioned in Section 4.2, we find it preferable to think of the reduce-predicate as a built-in predicate which canbe implemented very efficiently via appropriate hash codes. However, this is not a necessity, since it could also be imple-mented in datalog (with negation). A datalog implementation of the reduce-predicate and of several auxiliary predicates isgiven in Fig. B.1.The idea of reduce(v, S, . . .) is twofold: (1) We want to detect “duplicates”: Two pairs (i1, i2) and ( j1, j2) are duplicates ifand only if for all values of (P , N, C, d), a fact aux(v, S, i1, i2, P , N, C, d) is true in the minimal model ⇔ aux(v, S, j1, j2, P ,N, C, d) is true in the minimal model. In such a situation, we keep the lexicographically smallest pair (i1, i2) and deleteall other pairs. (2) We map the pairs of indices (i1, i2) which are not deleted in (1) in lexicographic order to contiguousvalues i. The functionality of reduce can be provided very efficiently via hash codes for the facts aux(v, S, i1, i2, . . .). InFig. B.1, we show that it is also definable in datalog.The only rule with head predicate reduce guarantees that a pair (i1, i2) of indices is mapped onto a single index i ifand only if the following conditions are fulfilled: (i) the pair (i1, i2) is defined, i.e., there exists at least one aux-fact forthis combination (v, S, i1, i2), (ii) no lexicographically smaller pair ( j1, j2) is mapped to i, (iii) there is no lexicographicallysmaller pair ( j1, j2), s.t. (i1, i2) and ( j1, j2) give rise to exactly the same set of ground atoms aux(v, S, _, _, P , N, C, d) whichare true in the minimal model of the interpreted program, and (iv) for all indices j with j < i there exists a pair ( j1, j2)smaller than (i1, i2) that is mapped onto j.The intended meaning of the predicates check and reduce has already been explained in Section 4.2. The other pred-icates in Fig. B.1 have the following meaning. defined(v, S, i1, i2) means that there exists at least one aux-fact for thiscombination (v, S, i1, i2). equal(v, S, i1, i2, j1, j2) is true in the minimal model if and only if (i1, i2) and ( j1, j2) giverise to exactly the same set of ground atoms aux(v, S, _, _, P , N, C , d) which are true in the minimal model of the pro-gram, while diff (v, S, i1, i2, j1, j2) means that (i1, i2) and ( j1, j2) give rise to different sets of ground atoms aux(v, S, _,if ( j1, j2) is lexicographically smaller than (i1, i2). With_, P , N, C, d).reduced_smaller(v, S, i, i1, i2) we can check if some index pair ( j1,j2), which is lexicographically smaller than (i1, i2), ismapped onto i. duplicate(v, S, i1, i2) means that there exists a lexicographically smaller pair ( j1, j2), s.t. (i1, i2) and ( j1, j2)give rise to exactly the same set of ground atoms aux(v, S, _, _, P , N, C , d) true in the minimal model; in other words, (i1, i2)is a duplicate. Finally, open(v, S, i, i1, i2) means that there exists an index j < i which has not been used as the target of themapping defined by the reduce-predicate, i.e., there exists no index pair ( j1, j2) smaller than (i1, i2), s.t. ( j1, j2) is mappedonto j.less( j1, j2, i1, i2) is true in the minimal modelReferences[1] S. Abiteboul, R. Hull, V. Vianu, Foundations of Databases, Addison–Wesley, 1995.[2] S. Arnborg, Efficient algorithms for combinatorial problems with bounded decomposability—A survey, BIT 25 (1) (1985) 2–23.[3] S. Arnborg, J. Lagergren, D. Seese, Easy problems for tree-decomposable graphs, J. Algorithms 12 (2) (1991) 308–340.[4] A. Atserias, J.K. Fichte, M. Thurley, Clause-learning algorithms with many restarts and bounded-width resolution, in: Proc. SAT’09, in: Lecture Notes inComputer Science, vol. 5584, Springer, 2009, pp. 114–127.132G. Gottlob et al. / Artificial Intelligence 174 (2010) 105–132[5] M. Baaz, U. Egly, A. Leitsch, Normal form transformations, in: J.A. Robinson, A. Voronkov (Eds.), Handbook of Automated Reasoning, vol. 1, ElsevierScience, 2001, pp. 273–333, Chapter 5.[6] D. Berwanger, A. Dawar, P. Hunter, S. Kreutzer, DAG-width and parity games, in: Proc. STACS’06, in: Lecture Notes in Computer Science, vol. 3884,2006, pp. 524–536.[7] D. Berwanger, E. Grädel, Entanglement—A measure for the complexity of directed graphs with applications to logic and games, in: Proc. LPAR’04, in:Lecture Notes in Computer Science, vol. 3452, 2005, pp. 209–223.[8] H.L. Bodlaender, A linear-time algorithm for finding tree-decompositions of small treewidth, SIAM J. Comput. 25 (6) (1996) 1305–1317.[9] H.L. Bodlaender, A.M.C.A. Koster, Safe separators for treewidth, Discrete Math. 306 (3) (2006) 337–350.[10] H.L. Bodlaender, A.M.C.A. Koster, Combinatorial optimization on graphs of bounded treewidth, Comput. J. 51 (3) (2008) 255–269.[11] R.B. Borie, Generation of polynomial-time algorithms for some optimization problems on tree-decomposable graphs, Algorithmica 14 (2) (1995) 123–137.[12] R.B. Borie, R.G. Parker, C.A. Tovey, Automatic generation of linear-time algorithms from predicate calculus descriptions of problems on recursivelyconstructed graph families, Algorithmica 7 (5–6) (1992) 555–581.[13] M. Cadoli, M. Lenzerini, The complexity of propositional closed world reasoning and circumscription, J. Comput. Syst. Sci. 48 (2) (1994) 255–310.[14] B. Courcelle, Graph rewriting: An algebraic and logic approach, in: J. van Leeuwen (Ed.), Handbook of Theoretical Computer Science, vol. B, ElsevierScience Publishers, 1990, pp. 193–242.[15] B. Courcelle, J.A. Makowsky, U. Rotics, On the fixed parameter complexity of graph enumeration problems definable in monadic second-order logic,Discrete Appl. Math. 108 (1-2) (2001) 23–52.[16] B. Courcelle, M. Mosbah, Monadic second-order evaluations on tree-decomposable graphs, Theor. Comput. Sci. 109 (1–2) (1993) 49–82.[17] J. Doner, Tree acceptors and some of their applications, J. Comput. Syst. Sci. 4 (5) (1970) 406–451.[18] W.F. Dowling, J.H. Gallier, Linear-time algorithms for testing the satisfiability of propositional Horn formulae, J. Log. Program. 1 (3) (1984) 267–284.[19] R.G. Downey, M.R. Fellows, Parameterized Complexity, Springer, New York, 1999.[20] T. Eiter, G. Gottlob, Propositional circumscription and extended closed world reasoning are Π p[21] T. Eiter, G. Gottlob, The complexity of logic-based abduction, J. ACM 42 (1) (1995) 3–42.[22] T. Eiter, G. Gottlob, On the computational cost of disjunctive logic programming: Propositional case, Ann. Math. Artif. Intell. 15 (3/4) (1995) 289–323.[23] G. Filé, Tree automata and logic programs, in: Proc. STACS’85, in: Lecture Notes in Computer Science, vol. 182, 1985, pp. 119–130.[24] E. Fischer, J.A. Makowsky, E.V. Ravve, Counting truth assignments of formulas of bounded tree-width or clique-width, Discrete Appl. Math. 156 (4)2 -complete, Theor. Comput. Sci. 114 (1993) 231–245.(2008) 511–529.[25] J. Flum, M. Frick, M. Grohe, Query evaluation via tree-decompositions, J. ACM 49 (6) (2002) 716–752.[26] M. Frick, M. Grohe, The complexity of first-order and monadic second-order logic revisited, Ann. Pure Appl. Logic 130 (1–3) (2004) 3–31.[27] M. Gelfond, V. Lifschitz, The stable model semantics for logic programming, in: Proc. ICLP/SLP’88, MIT Press, 1988, pp. 1070–1080.[28] M. Gelfond, H. Przymusinska, T.C. Przymusinski, On the relationship between circumscription and negation as failure, Artif. Intell. 38 (1) (1989) 75–94.[29] G. Gottlob, R. Pichler, F. Wei, Bounded treewidth as a key to tractability of knowledge representation and reasoning, in: Proc. AAAI’06, AAAI Press,2006, pp. 250–256.[30] G. Gottlob, R. Pichler, F. Wei, Monadic datalog over finite structures with bounded treewidth, in: Proc. PODS’07, ACM, 2007, pp. 165–174.[31] G. Gottlob, R. Pichler, F. Wei, Abduction with bounded treewidth: From theoretical tractability to practically efficient computation, in: Proc. AAAI’08,AAAI Press, 2008, pp. 1541–1546.[32] M. Grohe, Descriptive and parameterized complexity, in: Proc. CSL’99, in: Lecture Notes in Computer Science, vol. 1683, 1999, pp. 14–31.[33] J. Gustedt, O.A. Mæhle, J.A. Telle, The treewidth of Java programs, in: Proc. ALENEX’02: 4th International Workshop on Algorithm Engineering andExperiments, Revised Papers, in: Lecture Notes in Computer Science, vol. 2409, Springer, 2002, pp. 86–97.[34] M. Hermann, R. Pichler, Counting complexity of minimal cardinality and minimal weight abduction, in: Proc. JELIA’08, in: Lecture Notes in ComputerScience, vol. 5293, Springer, 2008, pp. 206–218.[35] P. Hunter, S. Kreutzer, Digraph measures: Kelly decompositions, games, and ordering, Theor. Comput. Sci. 399 (2008) 206–219.[36] M. Jakl, R. Pichler, S. Rümmele, S. Woltran, Fast counting with bounded treewidth, in: Proc. LPAR’08, in: Lecture Notes in Computer Science, vol. 5330,2008, pp. 436–450.[37] M. Jakl, R. Pichler, S. Woltran, Answer-set programming with bounded treewidth, in: Proc. IJCAI’09, 2009, pp. 816–822.[38] N. Klarlund, A. Møller, M.I. Schwartzbach, MONA implementation secrets, Int. J. Found. Comput. Sci. 13 (4) (2002) 571–586.[39] A.M.C.A. Koster, H.L. Bodlaender, S.P.M. van Hoesel, Treewidth: Computational experiments, Electronic Notes in Discrete Mathematics 8 (2001) 54–57.[40] G. Marque-Pucheu, Rational set of trees and the algebraic semantics of logic programming, Acta Inf. 20 (1983) 249–260.[41] H. Maryns, On the implementation of tree automata: Limitations of the naive approach, in: Proc. TLT’06: 5th Int. Treebanks and Linguistic TheoriesConference, 2006, pp. 235–246.[42] M. Minoux, LTUR: A simplified linear-time unit resolution algorithm for horn formulae and computer implementation, Inf. Process. Lett. 29 (1) (1988)1–12.[43] J. Obdržálek, DAG-width: Connectivity measure for directed graphs, in: Proc. SODA’06, ACM Press, 2006, pp. 814–821.[44] C.H. Papadimitriou, Computational Complexity, Addison–Wesley, 1994.[45] T.C. Przymusinski, Stable semantics for disjunctive programs, New Generation Comput. 9 (3/4) (1991) 401–424.[46] J.W. Thatcher, J.B. Wright, Generalized finite automata theory with an application to a decision problem of second-order logic, Mathematical SystemsTheory 2 (1) (1968) 57–81.[47] W. Thomas, Languages, automata, and logic, in: Handbook of Formal Languages, vol. III, Springer, New York, 1997, pp. 389–455.[48] M. Thorup, All structured programs have small tree-width and good register allocation., Inf. Comput. 142 (2) (1998) 159–181.[49] F. van den Eijkhof, H.L. Bodlaender, A.M.C.A. Koster, Safe reduction rules for weighted treewidth, Algorithmica 47 (2) (2007) 139–158.