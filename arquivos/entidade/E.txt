Artificial Intelligence 165 (2005) 1–35www.elsevier.com/locate/artintE-generalization using grammarsJochen BurghardtInstitute for Computer Architecture and Software Technology, Berlin, GermanyReceived 11 January 2003Available online 14 March 2005AbstractWe extend the notion of anti-unification to cover equational theories and present a method basedon regular tree grammars to compute a finite representation of E-generalization sets. We presenta framework to combine Inductive Logic Programming and E-generalization that includes an ex-tension of Plotkin’s lgg theorem to the equational case. We demonstrate the potential power ofE-generalization by three example applications: computation of suggestions for auxiliary lemmas inequational inductive proofs, computation of construction laws for given term sequences, and learningof screen editor command sequences. 2005 Elsevier B.V. All rights reserved.Keywords: Equational theory; Generalization; Inductive logic programming1. IntroductionMany learning techniques in the field of symbolic Artificial Intelligence are based onadopting the features common to the given examples, called selective induction in theclassification of [14], for example. Syntactical anti-unification reflects these abstractiontechniques in the theoretically elegant domain of term algebras.In this article, we propose an extension, called E-anti-unification or E-generalization,which also provides a way of coping with the well-known problem of representationchange [12,29]. It allows us to perform abstraction while modeling equivalent represen-tations using appropriate equations between terms. This means that all equivalent repre-sentations are considered simultaneously in the abstraction process. Abstraction becomesinsensitive to representation changes.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.01.0082J. Burghardt / Artificial Intelligence 165 (2005) 1–35In 1970, Plotkin and Reynolds [30,31,33] introduced the notion of (syntactical) anti-unification of terms as the dual operation to unification: while the latter computes the mostgeneral common specialization of the given terms, if it exists, the former computes themost special generalization of them, which always exists and is unique up to renaming. Forexample, using the usual 0-s representation of natural numbers and abbreviating s(s(0)) tos2(0), the terms 0 ∗ 0 and s2(0) ∗ s2(0) anti-unify to x ∗ x, retaining the common functionsymbol ∗ as well as the equality of its arguments.While extensions of unification to equational theories and classes of them have beeninvestigated [17,20,36], anti-unification has long been neglected in this respect, except forthe theory of associativity and commutativity [32] and so-called commutative theories [1].For an arbitrary equational theory E, the set of all E-generalizations of given terms isusually infinite. Heinz [2,22] presented a specially tailored algorithm that uses regular treegrammars to compute a finite representation of this set, provided E leads to regular congru-ence classes. However, this work has never been internationally published. In this paper,we try to make up for this neglect, giving an improved presentation using standard gram-mar algorithms only, and adding some new theoretical results and applications (Sections 4,5.3 below).In general, E-anti-unification provides a means to find correspondences that are onlydetectable using an equational theory as background knowledge. By way of a simple ex-ample, consider the terms 0 and s4(0). Anti-unifying them purely syntactically, withoutconsidering an equational theory, we obtain the term y, which indicates that there is nocommon structure. If, however, we consider the usual defining equations for (+) and (∗),see Fig. 1 (left), the terms may be rewritten nondeterministically as shown in Fig. 1 (right),and then syntactically anti-unified to x ∗ x as one possible result. In other words, it isrecognized that both terms are quadratic numbers.Expressed in predicate logic, this means we can learn a definition p(x ∗ x) from the ex-amples p(0) and p(s4(0)). Other possible results are p(s4(0) ∗ x), and, less meaningfully,p(x + y ∗ z). The closed representation of generalization sets by grammars allows us tofilter out generalizations with certain properties that are undesirable in a given applicationcontext.After some formal definitions in Section 2, we introduce our method of E-generalizationbased on regular tree grammars in Section 3 and briefly discuss extensions to more so-phisticated grammar formalisms. As a first step toward integrating E-generalization intoInductive Logic Programming (ILP), we provide, in Section 4, theorems for learning de-terminate or nondeterminate predicate definitions using atoms or clauses. In Section 5,we present applications of determinate atom learning in different areas, including induc-1.2.3.4.x + 0 = xx + s(y) = s(x + y)x ∗ 0 = 00=E0 ∗ 0s4(0) =E s2(0) ∗ s2(0)syn. anti-un.syn. anti-un.x ∗ s(y) = x ∗ y + xyx ∗ xFig. 1. Equations defining (+) and (∗) (left). E-generalization of 0 and s4(0) (right).J. Burghardt / Artificial Intelligence 165 (2005) 1–353tive equational theorem-proving, learning of series-construction laws and user support forlearning advanced screen-editor commands. Section 6 draws some conclusions.2. DefinitionsWe assume familiarity with the classical definitions of terms, substitutions [13], andHorn clauses [24]. The cardinality of a finite set S is denoted by #S. A signature Σ is aset of function symbols f , each of which has a fixed arity; if some f is nullary, we callit a constant. Let V be an infinite set of variables. TV denotes the set of all terms over Σand a given V ⊆ V. For a term t, var(t) denotes the set of variables occurring in t; if it isempty, we call t a ground term. We call a term linear if each variable occurs at most oncein it.By {x1 (cid:3)→ t1, . . . , xn (cid:3)→ tn}, or {xi (cid:3)→ ti | 1 (cid:1) i (cid:1) n}, we denote a substitution that mapseach variable xi to the term ti . We call it ground if all ti are ground. We use the postfixnotation tσ for application of σ to t, and σ τ for the composition of σ (to be applied first)and τ (second). The domain of σ is denoted by dom σ . A term t is called an instance of aterm t (cid:5) if t = t (cid:5)σ for some substitution σ . In this case, we call t more special than t (cid:5), andt (cid:5) more general than t. We call t a renaming of t (cid:5) if σ : V → V is a bijection.A term t is called a syntactical generalization of terms t1 and t2, if there exist substitu-tions σ1 and σ2 such that tσ1 = t1 and tσ2 = t2. In this case, t is called the most specificsyntactical generalization of t1 and t2, if for each syntactical generalization t (cid:5) of t1 and t2there exists a substitution σ (cid:5) such that t = t (cid:5)σ (cid:5). The most specific syntactical generalizationof two terms is unique up to renaming; we also call it their syntactical anti-unifier [30].An equational theory E is a finite set of equations between terms. (=E) denotes the= {t (cid:5) ∈ T{} |smallest congruence relation that contains all equations of E. Define [t]Et (cid:5) =E t} to be the congruence class of t in the algebra of ground terms. The congruenceclass of a term is usually infinite; for example, using the equational theory from Fig. 1, we= {t (cid:5) ∈ Tdom σ | t (cid:5)σ =E t} denote the sethave [0]Eof all terms congruent to t under σ .= {0, 0 ∗ s(0), 0 + 0 ∗ 0, . . .}. Let [t]σE(cid:1)A congruence relation (=1) is said to be a refinement of another congruence relation(=2), if ∀t, t (cid:5) ∈ TV : t =1 t (cid:5) ⇒ t =2 t (cid:5). In Section 5.1, we need the definition t1 ≡E t2 ift1σ =E t2σ for all ground substitutions σ with var(t1) ∪ var(t2) ⊆ dom σ ; this is equivalentto the equality of t1 and t2 being inductively provable [13, Section 3.2].We call an n-ary function symbol f a constructor if n functions π fi (t) =E ti) ↔ t =E f (t1, . . . , tn). The π f1 , . . . , π fn exist suchthat ∀t, t1, . . . , tn : (i are called selectorsassociated to f . As usual, we assume additionally that f (s1, . . . , sn) (cid:12)=E g(t1, . . . , tm) (cid:12)=Ex for any two constructors f (cid:12)= g, any variable x and arbitrary terms si, tj . On this assump-tion, some constants can also be called constructors. No selector can be a constructor. If fis a constructor, then [f (t1, . . . , tn)]EE, . . . , [tn]= f ([t1]i=1 π fA term t is called a constructor term if it is built from constructors and variables only.Let t and t (cid:5) be constructor terms. If tσ1 =E tσ2, then ∀x ∈ var(t): xσ1 =E xσ2. If tσ =E t (cid:5),then tσ (cid:5) = t (cid:5) for some σ (cid:5) such that xσ (cid:5) is a constructor term and xσ (cid:5) =E xσ for each x ∈ V.A (nondeterministic) regular tree grammar [10,37] is a triple G = (cid:13)Σ, N , R(cid:14). Σ isa signature, N is a finite set of nonterminal symbols and R is a finite set of rulesE).n4J. Burghardt / Artificial Intelligence 165 (2005) 1–35of the form N ::= f1(N11, . . . , N1n1) | . . . | fm(Nm1, . . . , Nmnm) or, abbreviated, N ::=mi=1 fi(Ni1, . . . , Nini ). Each fi(Ni1, . . . , Nini ) is called an alternative of the rule. We as-sume that for each nonterminal N ∈ N , there is exactly one defining rule in R with N asits left-hand side.Given a grammar G and a nonterminal N ∈ N , the language LG(N) produced by Nis defined in the usual way as the set of all ground terms derivable from N as the startsymbol. We omit the index G if it is clear from the context. We denote the total number ofalternatives in G by #G.In Section 4, we will use the following predicate logic definitions. To simplify no-tation, we sometimes assume all predicate symbols to be unary. An n-ary predicate p(cid:5)can be simulated by a unary p using an n-ary tupling constructor symbol and definingp((cid:13)t1, . . . , tn(cid:14)) ⇔ p(cid:5)(t1, . . . , tn). An n-ary predicate p is called determinate wrt some back-ground theory B if there is some k such that wlog each of the arguments k + 1, . . . , nhas only one possible binding, given the bindings of the arguments 1, . . . , k [25, Sec-tion 5.6.1]. The background theory B may be used to define p, hence p’s determinacydepends on B. Similar to the above, we sometimes write p(t1, . . . , tn) as a binary pred-icate p((cid:13)t1, . . . , tk(cid:14), (cid:13)tk+1, . . . , tn(cid:14)) to reflect the two classes of arguments. For a binarydeterminate predicate p, the relation {(cid:13)s, t(cid:14) | s, t ∈ T{} ∧ B |= p(s, t)} corresponds to afunction g. We sometimes assume that g is defined by equations from a given E, i.e., thatB |= (p(s, t) ↔ g(s) =E t).A literal has the form p(t) or ¬p(t), where p is a predicate symbol and t is a term.We consider a negation to be part of the predicate symbol. We say that the literals L1and L2 fit if both have the same predicate symbol, including negation. We extend (=E)to literals by defining p(t1) =E p(t2) if t1 =E t2. For example, (¬divides((cid:13)1 + 1, 5(cid:14))) =E(¬divides((cid:13)2, 5(cid:14))) if 1 + 1 =E 2.A clause is a finite set C = {L1, . . . , Ln} of literals, with the meaning C ⇔ L1 ∨· · ·∨Ln.We consider only nonredundant clauses, i.e., clauses that do not contain congruent liter-als. For example, {p(x + 0), p(x)} is redundant if x + 0 =E x. We write C1 ⊆E C2 if∀L1 ∈ C1 ∃L2 ∈ C2: L1 =E L2; if C2 is nonredundant, L2 is uniquely determined by L1.We say that C1 E-subsumes C2 if C1σ ⊆E C2 for some σ . In this case, the conjunctionof E and C1 implies C2; however, there are other cases in which E ∧ C1 |= C2 but C1does not E-subsume C2. For example, {¬p(x), p(f (x))} implies, but does not subsume,{¬p(x), p(f (f (x)))}, even for an empty E.A Horn clause is a clause {p0(t0), ¬p1(t1), . . . , ¬pn(tn)} with exactly one positive lit-eral. It is also written as p0(t0) ← p1(t1) ∧ · · · ∧ pn(tn). We call p0(t0) the head literal,and pi(ti) a body literal for i = 1, . . . , n. Like [25, Section 2.1], we call the Horn clauseconstrained if var(t0) ⊇ var(t1, . . . , tn).We call a Horn clause p0(s0, t0) ←mi=1 qi(ti) semi-determinatewrt some background theory B if all pi are determinate wrt B, all variables xi are dis-tinct and do not occur in s0, var(si) ⊆ var(s0) ∪ {x1, . . . , xi−1}, and var(t1, . . . , tm) ⊆var(s0, x1, . . . , xn). Semi-determinacy for clauses is a slight extension of determinacy de-fined by [25, Section 5.6.1], as it additionally permits arbitrary predicates qi . On the otherhand, [25] permits xi = xj for i (cid:12)= j ; however, pi(si, xi) ∧ pj (sj , xi) can be equivalentlytransformed into pi(si, xi) ∧ pj (sj , xj ) ∧ xi =E xj .(cid:1)ni=1 pi(si, xi) ∧(cid:1)J. Burghardt / Artificial Intelligence 165 (2005) 1–3553. E-generalizationWe treat the problem of E-generalization of ground terms by standard algorithms onregular tree grammars. Here, we also give a rational reconstruction of the original approachfrom [22], who provided monolithic specially tailored algorithms for E-anti-unification.We confine ourselves to E-generalization of two terms. All methods work similarly for thesimultaneous E-generalization of n terms.3.1. The core methodDefinition 1 (E-generalization). For an equational theory E, a term t is called an E-generalization, or E-anti-unifier, of terms t1 and t2 if there exist substitutions σ1 and σ2such that tσ1 =E t1 and tσ2 =E t2. In Fig. 1 (right), we had t1 = 0, t2 = s4(0), t = x ∗ x,σ1 = {x (cid:3)→ 0}, and σ2 = {x (cid:3)→ s2(0)}.As in unification, a most special E-generalization of arbitrary terms does not normallyexist. A set G ⊆ TV is called a set of E-generalizations of t1 and t2 if each member is anE-generalization of t1 and t2. Such a G is called complete if, for each E-generalization tof t1 and t2, G contains an instance of t.As a first step towards computing E-generalization sets, let us weaken Definition 1 byfixing the substitutions σ1 and σ2. We will see below, in Sections 4 and 5, that the weakeneddefinition has important applications in its own right.Definition 2 (Constrained E-generalization). Given two terms t1, t2, a variable set V , twosubstitutions σ1, σ2 with dom σ1 = dom σ2 = V and an equational theory E, define the setof E-generalizations of t1 and t2 wrt σ1 and σ2 as {t ∈ TV | tσ1 =E t1 ∧ tσ2 =E t2}. Thisset equals [t1]σ1E∩ [t2]σ2E .If we can represent the congruence class [t1]E as some regular tree languageLG1(N1) and LG2(N2), respectively, we can immediately compute the set of constrainedE-generalizations [t1]σ1E : The set of regular tree languages is closed wrt union,Eintersection and complement, as well as under inverse tree homomorphisms, which coverE and [t2]∩ [t2]σ2t1t2(cid:5)(cid:1)(cid:1)(cid:2)(cid:3)(cid:3)(cid:4)(cid:5)E[t1]E[t2]Eσ1(cid:2)··············(cid:4)σ2(cid:3)(cid:3)(cid:4)(cid:5)(cid:5)(cid:1)(cid:1)(cid:2)[t1]σ1E[t2]σ2E(cid:3)(cid:3)(cid:4)(cid:1)(cid:1)(cid:2) [t1]σ1E∩ [t2]σ2E(cid:5) tConstrained E-generalization:Unconstrained E-generalization:σ1, σ2 externally prescribedσ1, σ2 computed from [t1]E, [t2]EFig. 2. E-generalization using tree grammars.6J. Burghardt / Artificial Intelligence 165 (2005) 1–35substitution application as a special case. Fig. 2 gives an overview of our method for com-puting the constrained set of E-generalizations of t1 and t2 wrt σ1 and σ2 according toDefinition 2:• From ti and E, obtain a grammar for the congruence class [ti]E, if one exists; thediscussion of this issue is postponed to Section 3.2 below.• Apply the inverse substitution σi to the grammar for [ti]E to get a grammar for[ti]σiE , using some standard algorithm, e.g., that from [10, Theorem 7 in Section 1.4].This algorithm takes time O(#N · size(σi)) for inverse substitution application, wheresize(σi) =size(xσi) is the total number of function symbols occurring in σi .E , using the product-automaton• Compute a grammar for the intersection [t1]σ1E∩ [t2]σ2x∈dom σiconstruction, e.g., from [10, Section 1.3], which takes time O(#N1 · #N2).• Each member t of the resulting tree language is an actual E-generalization of t1 and(cid:2)t2. The question of enumerating that language is discussed later on.Based on this result, we show how to compute the set of unconstrained E-generalizationsof t1 and t2 according to Definition 1, where no σi is given. It is sufficient to computetwo fixed universal substitutions τ1 and τ2 from the grammars for [t1]E and tolet them play the role of σ1 and σ2 in the above method (cf. the dotted vectors in Fig. 2).Intuitively, we introduce one variable for each pair of congruence classes and map them toa kind of normalform member of the first and second class by τ1 and τ2, respectively.E and [t2]We give below a general construction that also accounts for auxiliary nonterminals notrepresenting a congruence class, and state the universality of τ1, τ2 in a formal way. ForE share the same grammar G;the sake of technical simplicity, we assume that [t1]this can easily be achieved by using the disjoint union of the grammars for [t1]E.E and [t2]E and [t2]Definition 3 (Normal form). Let an arbitrary tree grammar G = (cid:13)Σ, N , R(cid:14) be given.A non-empty set of nonterminals N ⊆ N is called maximal ifL(N) (cid:12)= {}, but(cid:3)(cid:5) (cid:1) N. Define Nmax = {N ⊆ N | N (cid:12)= {}, N maximal}. ChooseN ∈N(cid:5) L(N) = {} for each NN ∈N(cid:3)some arbitrary but fixed• maximal N(t) ⊇ {N ∈ N | t ∈ L(N)} for each t ∈• maximal N(t) for each t ∈ TV \• ground term t(N) ∈N ∈N L(N),L(N) for each N ∈ Nmax.(cid:3)(cid:4)N ∈N(cid:4)N ∈N L(N),The mappings N(·) and t(·) can be effectively computed from G. We abbreviate t = t(N(t));this is a kind of normalform of t. Each term not in any L(N), in particular each nongroundterm, is mapped to some arbitrary ground term, the choice of which does not matter. Fora substitution σ = {x1 (cid:3)→ t1, . . . , xn (cid:3)→ tn}, define σ = {x1 (cid:3)→ t1, . . . , xn (cid:3)→ tn}. We alwayshave xσ = xσ .Lemma 4 (Substitution normalization). For all N ∈ N , t ∈ TV , and σ ,(1) t ∈ L(N) ⇒ t ∈ L(N), and(2) tσ ∈ L(N) ⇒ tσ ∈ L(N).J. Burghardt / Artificial Intelligence 165 (2005) 1–357Proof. From the definition of N(·) and t(·), we get t ∈ L(N) ⇒ N ∈ N(t) and N ∈ N ⇒t(N) ∈ L(N), respectively.(1) Hence, t ∈ L(N) ⇒ N ∈ N(t) ⇒ t ∈ L(N).(2) Induction on the structure of t:• If t = x ∈ V and xσ ∈ L(N), then xσ = xσ ∈ L(N) by 1.• Assuming N ::= . . . f (N11, . . . , N1n) | . . . | f (Nm1, . . . , Nmn) . . ., we havef (t1, . . . , tn) σ ∈ L(N)⇒ ∃i (cid:1) m ∀j (cid:1) n: tj σ ∈ L(Nij )⇒ ∃i (cid:1) m ∀j (cid:1) n: tj σ ∈ L(Nij )⇒ f (t1, . . . , tn) σ ∈ L(N)by Definition L(·)by Induction Hypothesis(cid:1)by Definition L(·)Lemma 5 (Universal substitutions). For each grammar G, we can effectively compute twosubstitutions τ1, τ2 that are universal for G in the following sense. For any two substitu-tions σ1, σ2, a substitution σ exists such that for i = 1, 2, we have ∀t ∈ Tdom σ1∩ dom σ2∀N ∈ N : tσi ∈ L(N) ⇒ tσ τi ∈ L(N ).Proof. Let v(N1, N2) be a new distinct variable for each N1, N2 ∈ Nmax. Define τi ={v(N1, N2) (cid:3)→ t(Ni) | N1, N2 ∈ Nmax} forlet σ ={x (cid:3)→ v(N(xσ1), N(xσ2)) | x ∈ dom σ1 ∩ dom σ2}. Then σ τi and σi coincide on var(t),and hence tσi ∈ L(N) ⇒ tσ τi ∈ L(N) by Lemma 4.2. (cid:1)i = 1, 2. Given σ1 and σ2,Example 6. We apply Lemma 5 to the grammar G consisting of the topmost three rulesin Fig. 3. The result will be used in Example 8 to compute some set of E-generalizations.We have Nmax = {{N0, Nt }, {N1, Nt }}, since, e.g., 0 ∈ L(N0) ∩ L(Nt ) and s(0) ∈L(N1) ∩ L(Nt ), while L(N0) ∩ L(N1) = {}. We choose(cid:5)N(t) =t({N0, Nt }) = 0,t({N1, Nt }) = s(0).We abbreviate, e.g., v({N0, Nt }, {N1, Nt }) to v01. This way, we obtainif t ∈ L(N0),else,{N0, Nt }{N1, Nt }and(cid:7)(cid:6)v00 (cid:3)→ 0, v01 (cid:3)→ 0, v10 (cid:3)→ s(0), v11 (cid:3)→ s(0)(cid:7)(cid:6)v00 (cid:3)→ 0, v01 (cid:3)→ s(0), v10 (cid:3)→ 0, v11 (cid:3)→ s(0)τ1 =τ2 =and.Given t = x ∗ y, σ1 = {x (cid:3)→ 0 + 0, y (cid:3)→ 0} and σ2 = {x (cid:3)→ s(0), y (cid:3)→ s(0) ∗ s(0)} forexample, we obtain a proper instance v01 ∗ v01 of t using τ1 and τ2:L(N0) (cid:22) (0+0) ∗ 0L(N0) (cid:22) 0 ∗ 0σ1←− x ∗ yτ1←− v01 ∗ v01σ2−→ s(0) ∗ (s(0)∗s(0)) ∈ L(N1)τ2−→ s(0) ∗ s(0)∈ L(N1).The computation of universal substitutions is very expensive because it involves com-puting many tree-language intersections to determine the mappings N(·) and t(·). AssumeN = Nc ∪ No, where Nc comprises nc nonterminals representing congruence classes andNo comprises no other ones. A maximal set N may contain at most one nonterminal8J. Burghardt / Artificial Intelligence 165 (2005) 1–35(cid:9)(cid:8)from Nc and an arbitrary subset of No; however, no maximal N may be a proper sub-set of another one. By some combinatorics, we get (nc + 1) ·as an upper boundon #Nmax. Hence, the cardinality of dom τi is bounded by the square of that number. Inour experience, no is usually small. In most applications, it does not exceed 1, resultingin # dom τi (cid:1) (nc + 1)2. Computing the τi requires no + 1 grammar intersections in theworst case, viz. when No ∪ {Nc} for some Nc ∈ Nc is maximal. In this case, dom τi israther small. Since the time for testing emptiness is dominated by the intersection compu-o (cid:1) #Gno+1, we get a time upper bound oftation time, and (nc + 1) ·O(#Gno+1) for computing the τi .(cid:1) (nc + 1) · nno/2nono/2nono/2(cid:9)(cid:8)If the grammar is deterministic, then each nonterminal produces a distinct congruenceclass [26, Section 2], and we need compute no intersection at all to obtain τ1 and τ2. We get# dom τi = #N 2. In this case, N(·), t(·), and v(·, ·) can be computed in linear time from G.However, in general a nondeterministic grammar is smaller in size than its deterministiccounterpart.Theorem 7 (Unconstrained E-generalization). Let an equational theory E and two groundterms t1, t2 be given. Let G = (cid:13)Σ, N , R(cid:14) be a tree grammar and N1, N2 ∈ N such thatLG(Ni) = [ti]E is a completeset of E-generalizations of t1 and t2. A regular tree grammar for it can be computed fromG in time O(#G2 + #Gno+1).E for i = 1, 2. Let τ1, τ2 be as in Lemma 5. Then, [t1]τ1∩ [t2]τ2E∩ [t2]τ2Proof. If t ∈ [t1]τ1E , then tτ1 =E t1 and tτ2 =E t2, i.e., t is an E-generalization of t1Eand t2. To show the completeness, let t be an arbitrary E-generalization of t1 and t2, i.e.,∩ [t2]τ2tσi =E ti for some σi . Obtain σ from Lemma 5 such that tσ τi ∈ [ti]Econtains the instance tσ of t. (cid:1)E. Then [t1]τ1ESince the set of E-generalizations resulting from our method is given by a regular treegrammar, it is necessary to enumerate some terms of the corresponding tree language inorder to actually obtain some results. Usually, there is a notion of simplicity (or weight),depending on the application E-generalization is used in, and it is desirable to enumeratethe simplest terms (with least weight) first. The minimal weight of a term in the languageof each nonterminal can be computed in time O(#G · log #G) by [6]. After that, it is easyto enumerate a nonterminal’s language in order of increasing weight in time linear to theoutput size using a simple PROLOG program.Example 8. To give a simple example, we generalize 0 and s(0) wrt the equational theoryfrom Fig. 1. Fig. 3 shows all grammars that will appear during the computation. For now,assume that the grammar G defining the congruence classes [0]E is alreadygiven by the topmost three rules in Fig. 3. In Example 10 below, we discuss in detail howit can be obtained from E. Nevertheless, the rules of G are intuitively understandable evennow; e.g., the rule for N0 in the topmost line reads: A term of value 0 can be built by theconstant 0, the sum of two terms of value 0, the product of a term of value 0 and any otherterm, or vice versa. Similarly, L(N1) = [s(0)]E and L(Nt ) = T{}. In Example 6, we alreadycomputed the universal substitutions τ1 and τ2 from G.E and [s(0)]J. Burghardt / Artificial Intelligence 165 (2005) 1–3590|v10v10|v11| N0 ∗Nt| N0 +N0s(N0) | N0 +N1 | N1 +N00| s(Nt ) | Nt +Nt|N0∗ +N0∗|0|s(N0∗)|N0∗ +N1∗|N1∗ +N0∗N0 ::=N1 ::=Nt ::=N0∗::=v00|v01N1∗::=Nt∗ ::=v00|v01|v10|v11|0|s(Nt∗)| Nt∗ +Nt∗|N∗0 +N∗0N∗0::=v00N∗1::=|s(N∗0)|N∗0 +N∗1|N∗1 +N∗0|v11N∗t ::=v00|v01|v10|v11|0|s(N∗t )| N∗t +N∗tN00::=v00N01::=v01N0t ::=v00|v01Nt0 ::=v00Nt1 ::=|v11Ntt ::=v00|v01|v10|v11|0| s(Ntt ) | Ntt +Ntt| Nt ∗N0| N1 ∗N1| Nt ∗Nt|N0∗ ∗Nt∗|Nt∗ ∗N0∗|N1∗ ∗N1∗| Nt∗ ∗Nt∗|N∗0 ∗N∗t |N∗t ∗N∗0|N∗1 ∗N∗1| N∗t ∗N∗t|N00 +N00| N00 ∗Ntt |N0t ∗Nt0|Nt0 ∗N0t | Ntt ∗N00|N01 ∗Nt1|Nt1 ∗N01|N00 +N01|N01 +N00| N0t ∗Ntt | Ntt ∗N0t| N0t +N0t| Nt0 ∗Ntt | Ntt ∗Nt0| Nt0 +Nt0| Nt1 ∗Nt1|s(Nt0)| Nt0 +Nt1 | Nt1 +Nt0| Ntt ∗Ntt|v10|0|0v01v01|0|0Fig. 3. Grammars G (top), Gτ1 , Gτ2 and G12 (bottom) in Examples 8 and 10.Fig. 3 shows the grammars Gτ1 and Gτ2 resulting from inverse substitution application,defining the nonterminals N0∗, N1∗, Nt∗ and N∗0, N∗1, N∗t , respectively. For example,LGτ1 (N0∗) = [0]τ1E , where the rule for N0∗ is obtained from that for N0 by simply includingall variables that are mapped to a member of LG(N0) by τ1. They appear as new constants,i.e., ΣGτ1 = ΣGτ2 = ΣG ∪ {v00, . . . , v11}. For each t ∈ LGτ1 (N0∗), we have tτ1 =E 0.The bottommost 6 rules in Fig. 3 show the intersection grammar G12 obtained from akind of product-automaton construction and defining N00, . . . , Ntt . We have LG12(Nij ) =LGτ1 (Ni) ∩ LGτ2 (Nj ) for i, j ∈ {0, 1, t}. By Theorem 7, LG12 (N01) is a complete set of E-generalizations of 0 and s(0) wrt E. We have, e.g., N01 → N01 ∗ Nt1 → v01 ∗ v01, showingthat 0 and s(0) are both quadratic numbers, and (v01 ∗ v01)τ1 = 0 ∗ 0 =E 0, (v01 ∗ v01)τ2 =s(0) ∗ s(0) =E s(0). By repeated application of the rules N01 ::= . . . N01 ∗ Nt1 . . . andNt1 ::= . . . Nt1 ∗ Nt1 . . . , we can obtain any generalization of the form v01 ∗ · · · ∗ v01, witharbitrary paranthesation. By way of a less intuitive example, we have v01 ∗ s(v10 + v10) ∈LG12(N01).3.2. Setting up grammars for congruence classes[t1]E was deferred in Section 3.1. It is discussed below.The question of how to obtain a grammar representation of the initial congruence classesE and [t2]Procedures that help to compute a grammar representing the congruence class of aground term are given, e.g., in [26, Section 3]. This paper also provides a criterion foran equational theory inducing regular tree languages as congruence classes:Theorem 9 (Ground equations [26]). An equational theory induces regular congruenceclasses iff it is the deductive closure of finitely many ground equations E.Proof. To prove the if direction, start with a grammar consisting of a rule Nf (t1,...,tn) ::=f (Nt1, . . . , Ntn) for each subterm f (t1, . . . , tn) occurring in E. For each equation10J. Burghardt / Artificial Intelligence 165 (2005) 1–35(t1 =E t2) ∈ E, fuse the nonterminals Nt1 and Nt2 everywhere in the grammar. Then suc-cessively fuse every pair of nonterminals whose rules have the same right-hand sides. Theresult is a deterministic grammar G such that t1 =E t2 iff t1, t2 ∈ LG(N) for some N . Inaddition to this nonoptimal but intuitive algorithm, McAllester gives an O(n · log n) algo-rithm based on congruence closure [15], where n is the written length of E. The proof ofthe only if direction need not be sketched here. (cid:1)In order to compute a complete set of E-generalizations of two given ground terms t1and t2, we do not need all congruence classes to be regular; it is sufficient that those of t1and t2 are. More precisely, it is sufficient that (=E) is a refinement of some congruencerelation (=G) with finitely many classes only, such that [ti]E= [ti]G for i = 1, 2.i>nE, . . . , [sn(0)]Example 10. Let Σ = {0, s, (+), (∗)}, and let E be the equational theory from Fig. 1.To illustrate the application of Theorem 9, we show how to obtain a grammar defining[0]E for arbitrary n ∈ N. Obviously, (=E) itself has infinitely many congru-ence classes. However, in order to consider the terms 0, . . . , sn(0) only, the relation (=G),(cid:4)[si(0)]E and C =E, . . . , [sn(0)]defined by the classes [0]E , is sufficient. This rela-(cid:4)ntion is, in fact, a congruence wrt 0, s, (+), and (∗): in a term t ∈E, a memberi=0tc of C can occur only in some subterm of the form 0 ∗ tc or similar, which always equals0, regardless of the choice of tc.For n = 1, we may choose a representative term 0, s(0), and c for the classes [0]E,[s(0)]E , and C, respectively. We may instantiate each equation from Fig. 1 with all possi-ble combinations of representative terms, resulting in 3 + 32 + 3 + 32 ground equations.After adding the equation c = s(s(0)), we may apply Theorem 9 to obtain a determin-istic grammar Gd with LGd (N0) = [0]E , and LGd (Nc) = C. Theequations’ ground instances, and thus Gd , can be built automatically. The grammar Gd isequivalent to G from Fig. 3 (top), which we used in Example 8. The latter is nondetermin-istic for the sake of brevity. It describes [0]E by N0 and N1, respectively, whileL(Nt ) = [0]EE, LGd (Ns(0)) = [s(0)]E and [s(0)]∪ [s(0)]E[si(0)]∪ C.In [2, Corollary 16], another sufficient criterion is given. Intuitively, it allows us toconstruct a grammar from each equational theory that describes only operators building uplarger values (normal forms) from smaller ones:Theorem 11 (Constructive operators). Let E be given by a ground confluent andNoetherian term-rewriting system. For each term t ∈ T{}, let nf (t) denote its uniquenormal form; let NF be the set of all normal forms. Let (≺) be a well-founded partialordering on NF with a finite branching degree, and let ((cid:24)) be its reflexive closure. Ifti (cid:24) nf (f (t1, . . . , tn)) for all f ∈ Σ, t1, . . . , tn ∈ NF and i = 1, . . . , n, then for each t ∈ T{},the congruence class [t]E is a regular tree language.Proof. Define one nonterminal Nt for each normal-form term t ∈ NF. Whenever t =Ef (t1, . . . , tn) for some t1, . . . , tn ∈ NF, include an alternative f (Nt1, . . . , Ntn) into theright-hand side of the rule for Nt . Since ti (cid:24) t for all i, there are only finitely many suchalternatives. This results in a finite grammar G, and we have LG(Nnf (t)) = [t]E for allJ. Burghardt / Artificial Intelligence 165 (2005) 1–3511terms t. Let ai denote the number of i-ary function symbols in Σ and m denote the maxi-mi=0 ai · #NFi normal-form computations to build G, whichmal arity in Σ. Then we needhas a total of these many alternatives and #NF rules. Its computation takes O(#G) time. (cid:1)(cid:2)By way of an example, consider the theory E consisting of only equations 1. and 2. inFig. 1. E is known to be ground-confluent and Noetherian and to lead to NF = {sn(0) |n ∈ N}. Defining si(0) ≺ sj (0) ⇔ i < j , we observe that si(0) (cid:24) si+j (0) = nf (si(0) +sj (0)); similarly, sj (0) (cid:24) nf (si(0) + sj (0)) and si(0) (cid:24) nf (s(si (0))). Hence, E leads toregular congruence classes by Theorem 11. For example, there are just five ways to obtain aterm of value s3(0) from normal-form terms using s and (+). Accordingly, N3 ::= s(N2) |N3 + N0 | N2 + N1 | N1 + N2 | N0 + N3 defines [s3(0)]E.If their respective preconditions are met, Theorems 9 and 11 allow us to automaticallycompute a grammar describing the congruence classes wrt a given theory E. In practice,with Theorem 9 we have the problem that E is rarely given by ground equations only, whileTheorem 11 requires properties that are sufficient but not necessary. For example, not evenfor the whole theory from Fig. 1 is there an ordering such that si(0) (cid:24) 0 = nf (si(0) ∗ 0)and 0 (cid:24) si(0) = nf (0 + si(0)).So far, it seems best to set up a grammar scheme for a given E manually. For exam-ple, for E from Fig. 1, it is easy to write a program that reads n ∈ N and computes agrammar describing the congruence classes [0]E: The grammar consists ofthe rules for N0 and Nt from Fig. 3 (top) and one rule Ni ::= s(Ni−1) |j =0Nj + Ni−j |ij ·k=iNj ∗ Nk for each i = 1, . . . , n. Similarly, grammar schemes for the theory of listoperators like append, reverse, etc. and other theories can be implemented.E, . . . , [sn(0)]The lack of a single algorithm that computes a grammar for each E from a sufficientlylarge class of equational theories restricts the applicability of our E-generalization methodto problems where E is not changed too often. Using grammar schemes, this restriction canbe relaxed somewhat. The grammar-scheme approach is also applicable to theories givenby conditional equations or even other formulas, as long as they lead to regular congruenceclasses and the schemes are computable.A further problem is that not all equational theories lead to regular congruence classes.Consider, for example, subtraction (−) on natural numbers. We have 0 =E si(0) − si(0) forall i ∈ N. Assume that (=E) is a refinement of some (=G) with finitely many classes; thensi(0) =G sj (0) for some i (cid:12)= j . If (=G) is a congruence relation, 0 =G si(0) − si(0) =G= [0]G is impossible. Thus, if ansi(0) − sj (0), although 0 (cid:12)=E si(0) − sj (0). Hence, [0]Eoperator like (−) is defined in E, we cannot E-generalize using our method.However, we can still compute an approximation of the E-generalization set in suchcases by artificially cutting off grammar rules after a maximum number of alternatives.This will result in a set that still contains only correct E-generalizations but that is usuallyincomplete. For example, starting from the grammar rulesN0 ::= 0N1 ::=N2 ::=| N0 − N0 | N1 − N1 | N2 − N2s(N0) | N1 − N0 | N2 − N1s(N1) | N2 − N0 ,12J. Burghardt / Artificial Intelligence 165 (2005) 1–35we obtain only E-generalization terms t whose evaluation never exceeds the value 2 onany subterm’s instance. Depending on the choice of the cut-off point, the resulting E-generalization set may suffice for a given application.To overcome the limited expressiveness of pure regular tree grammars, it would be de-sirable to extend the results of Section 3.1 to automata with equality tests. However, forautomata with equality tests between siblings [4,10] or, equivalently, shallow systems ofsort constraints [38], we have the problem that this language class is not closed underinverse substitution application [4, Section 5.2]. For reduction automata [9,10] and gener-alized reduction automata [8], it seems to be still unknown whether they are closed underinverse substitution application. Moreover, the approach using universal substitutions τ1, τ2nj =1 tj ∈ L(Nij ) ⇒ f (t1, . . . , tn) ∈ L(N), which isstrongly depends on the fact thatneeded in the proof of Lemma 4(2). In other words, the rule N ::= . . . f (Ni1, . . . , Nin)is not allowed to have additional constraints. For these reasons, our approach remains lim-ited to ordinary regular tree grammars, i.e., those without any equality constraints. Thefollowing lemma shows that we cannot find a more sophisticated grammar formalism thatcan handle all equational theories, anyway.(cid:1)Lemma 12 (General uncomputability of E-generalization). There are equational theoriesE such that it is undecidable whether the set of constrained E-generalizations for certaint1, t2, σ1, σ2 is empty. Such a set cannot be represented by any grammar formalism that isclosed wrt language intersection and allows testing for emptiness.Proof. We encode a version of Post’s correspondence problem into an E-generalizationproblem for a certain E. Let a set {(cid:13)a1, b1(cid:14), . . . , (cid:13)an, bn(cid:14)} of pairs of nonempty strings overa finite alphabet A be given. It is known to be undecidable whether there exists a sequence1, i2, . . . , im such that m (cid:2) 1 and a1 · ai2· · · bim [35, Section 2.7], where· · · aim“·” denotes string concatenation. Let Σ = A ∪ {1, . . . , n} ∪ {(·), f, a, b}, and let E consistof the following equations:= b1 · bi2(x · y) · z = x · (y · z),f (a, x · i, y · ai) = f (a, x, y)f (b, x · i, y · bi) = f (b, x, y)for i = 1, . . . , n,for i = 1, . . . , n,where x, y, and z are variables. Then, the congruence class [f (a, 1, a1)]E and [f (b, 1, b1)]Eequals the set of all admitted Post sequences of ai and bi , respectively. Let σ1 = {x (cid:3)→ a}∩and σ2 = {x (cid:3)→ b},[f (b, 1, b1)]σ2then the set of constrained E-generalizations [f (a, 1, a1)]σ1EE is nonempty iff the given correspondence problem has a solution. (cid:1)4. Learning predicate definitionsIn this section, we relate E-generalization to Inductive Logic Programming (ILP),which seems to be the closest approach to machine learning. We argue in favor of anoutsourcing of equational tasks from Horn program induction algorithms, similar to whathas long been common practice in the area of deduction. From a theoretical point ofJ. Burghardt / Artificial Intelligence 165 (2005) 1–3513B (cid:12)|= F +Necessity:B ∧ h |= F +Sufficiency:B ∧ h (cid:12)|= falseWeak Consistency:Strong Consistency: B ∧ h ∧ F − (cid:12)|= falseFig. 4. Requirements for hypothesis generation according to Muggleton [28].view, a general-purpose theorem-proving algorithm is complete wrt equational formulas,too, if all necessary instances of the congruence axioms s =E t ⇒ f (s) =E f (t) ands =E t ∧ p(s) ⇒ p(t) are supplied. However, in practice it proved to be much more ef-ficient to handle the equality predicate (=E) separately using specially tailored methods,like E-unification for fixed, or paramodulation for varying E.Similarly, we show that integrating E-anti-unification into an ILP algorithm helps torestrict the hypotheses-language bias and thus the search space. In particular, learning ofdeterminate clauses can be reduced to learning of atoms using generalization wrt an Edefining a function for each determinate predicate.We investigate the learning of a definition of a new predicate symbol p in four differentsettings. In all cases, we are given a conjunction F ⇔ F + ∧ F − of positive and negativeground facts, and a background theory B describing an equational theory E. In this section,we always assume that E leads to regular congruence classes. We generate a hypothesish that must explain F using B. From Inductive Logic Programming, up to four require-ments for such a hypothesis are known [28, Section 2.1]. They are listed in Fig. 4. Moreprecisely, the Necessity requirement does not impose a restriction on h, but forbids anygeneration of a (positive) hypothesis, provided the positive facts are explainable without it.Muggleton remarks that this requirement can be checked by a conventional theorem proverbefore calling hypothesis generation. The Weak and Strong Consistency requirements co-incide if there are no negative facts; otherwise, the former is entailed by the latter. In [16,Section 5.2.4], only Sufficiency (called Completeness there) and Strong Consistency arerequired.We show under which circumstances E-generalization can generate a hypothesis satis-fying the Sufficiency and both Consistency requirements. The latter are meaningful in anequational setting only if we require that (=E) is nontrivial, i.e., ∃x, y: ¬ x =E y. Withoutthis formula, false could not even be derived from an equational theory, however nonsen-sical.Given E, we require our logical background theory B to entail the reflexivity, symmetryand transitivity axiom for (=E), a congruence axiom for (=E) wrt each function f ∈ Σand each predicate p occurring in B, the universal closure of each equation in E, andthe nontriviality axiom for (=E). As a more stringent alternative to the Necessity require-ment, we assume that B is not contradictory and that the predicate symbol p for which adefinition has to be learned does not occur in B, except in p’s congruence axiom.To begin with, we investigate the learning of a definition of a unary predicate p by an¬p(ti). For sets T +, T − ofi=1 p(ti) and F − ⇔atom h ⇔ p(t). Let F + ⇔ground terms and an arbitrary term t, definemi=n+1(cid:1)n(cid:1)+−hh(t, T(t, T+−) ⇔ ∀t) ⇔ ∀t(cid:5) ∈ T(cid:5) ∈ T+ ∃χ: tχ =E t− ∀χ: tχ (cid:12)=E t(cid:5)(cid:5).and14J. Burghardt / Artificial Intelligence 165 (2005) 1–35We name the substitutions χ instead of σ in order to identify them as given by hin the proofs below.+ and h−Lemma 13 (Requirements). Let ti, t (cid:5)arbitrary term. Let F + ⇔nand T − = {tn+1, . . . , tm}. Then:(cid:1)i be ground terms for i = 1, . . . , m and let t be an¬p(ti). Let T + = {t1, . . . , tn}i=1 p(ti) and F − ⇔mi=n+1(cid:1)1) ∨ · · · ∨ p(t (cid:5)(1) B ∧ p(t) |= p(t (cid:5)(2) B ∧ p(t) |= F + iff h+(t, T +).(3) B ∧ p(t) ∧ F − (cid:12)|= false iff hn(cid:5)) iff tσ =E t (cid:5)−(t, T −).i for some i, σ .Proof.(1) The if direction is trivial. To prove the only if direction, observe that B has a Herbrandmodel containing no instances of p. If we add the set {p(t (cid:5)(cid:5)) | ∃σ ground: tσ =E t (cid:5)(cid:5)}to that model, we get a Herbrand model of B ∧ p(t). In this model, p(t (cid:5)1) ∨ · · · ∨ p(t (cid:5)n(cid:5) )holds only if some p(t (cid:5)i ) holds, since they are all ground. This implies in turn that p(t (cid:5)i )is among the p(t (cid:5)(cid:5)), i.e., tσ =E t (cid:5)i for some ground substitution σ .1) ∨ · · · ∨ p(t (cid:5)n(cid:5)), paraphrasing Strong Consistency as(2) Follows from (1) for n(cid:5) = 1.(3) Follows from (1) for ¬F − ⇔ p(t (cid:5)B ∧ p(t) (cid:12)|= ¬F −. (cid:1)The following Lemma 14, and Lemma 20 below for the determinate case, are the work-horses of this section. They show how to apply E-generalization to obtain a hypothesesterm set from given positive and negative example term sets. In the theorems based onthese lemmas, we only need to enclose the hypotheses terms as arguments to appropriatepredicates.Lemma 14 (Hypotheses). For each finite set T + ∪ T − of ground terms, we can compute aregular set H = H14(T +, T −) such that for all t ∈ TV :t ∈ H ⇒ h∃σ : tσ ∈ H ⇐ h++(t, T(t, T++−−) ∧ h) ∧ h(t, T(t, T−−),).andProof. Let T + = {t1, . . . , tn}, let G be a grammar defining [t1]E. Obtain theuniversal substitutions τ1, . . . , τn for G from Lemma 5. All τi have the same domain. Usingthe notations from Definition 3, letE, . . . , [tn]The set S is finite, but large; it has #N #N n(cid:4)(cid:7)σ | dom σ = dom τ1 ∧ ∀x ∈ dom σ ∃N ∈ Nmax: xσ = t(N).(cid:3)ni=1E and H − =E. Define H = H + \ H −; all these sets are regular tree languages and canelements. Define H + =[ti]τi[t (cid:5)]σt (cid:5)∈T −S =σ∈Smax(cid:4)max(cid:6)be computed using standard grammar algorithms.• For t ∈ H , we trivially have h−(t, T −) does not hold, i.e.,+(t, T +). Assume that htσ =E t (cid:5) for some σ and t (cid:5) ∈ T −. Since var(t) ⊆ dom τ1 by construction, we may15⇒J. Burghardt / Artificial Intelligence 165 (2005) 1–35• If h⇒ t ∈ [t (cid:5)]σassume wlog dom σ = dom τ1; hence σ ∈ S. From Lemma 4(2), we get tσ ∈ [t (cid:5)]Etσ ∈ [t (cid:5)]E+(t, T +) ∧ h−(t, T −), then tχi =E ti for some χi , i = 1, . . . , n. Using Lemma 5,E for someE , hence tσ ∈ H +. If we had tσ ∈ [t (cid:5)]σ (cid:5)we get some σ such that tσ ∈ [ti]τit (cid:5) ∈ T − and σ (cid:5) ∈ S, then tσ σ (cid:5) =E t (cid:5), contradicting hE, contradicting t /∈ H −.−(t, T −). (cid:1)Theorem 15 (Atomic definitions). Let t1, . . . , tm be ground terms. Let F + ⇔and F − ⇔such thatni=1 p(ti)¬p(ti) be given. We can compute a regular set H = H15(F +, F −)mi=n+1(cid:1)(cid:1)• each p(t) ∈ H is a hypothesis satisfying the Sufficiency and the Strong Consistency• for each hypothesis satisfying these requirements and having the form p(t), we haverequirement wrt F +, F −; andp(tσ ) ∈ H for some σ .Proof. Define T + = {ti | i = 1, . . . , n} and T − = {ti | i = n+1, . . . , m}. By Lemma 13(2)and (3), p(t) is a hypothesis satisfying the Sufficiency and the Strong Consistency re-−(t, T −), respectively. By Lemma 14, we may thus choosequirement iff hH = {p(t) | t ∈ H14(T +, T −)}, which is again a regular tree language. (cid:1)+(t, T +) and hThe time requirement of the computation from Theorem 15 grows very quickly if neg-ative examples are given. Even for deterministic grammars, up to (m − n) · #N #N ninversesubstitution applications are needed, each requiring a renamed copy of the original gram-mar. If only positive examples are given, the time complexity is O(#Gn + #Gno+1), whichallows nontrivial practical applications.= LG(N0) and [s(0)]EBy way of an example, consider again the equational theory E from Fig. 1. Let F + ⇔0 (cid:1) 0 ∧ 0 (cid:1) s(0) and F − ⇔ true be given. In Example 8, we already computed a grammarG describing [0]= LG(N1), see Fig. 3 (top). The congruenceEclass of, say, (cid:13)0, 0(cid:14) could be defined by the additional grammar rule N(cid:13)0,0(cid:14) ::= (cid:13)N0, N0(cid:14).Instead of that rule, we add N0(cid:1)0 ::= (N0 (cid:1) N0) to the grammar, anticipating that any(cid:13)t1, t2(cid:14) ∈ H14 will be transformed to (t1 (cid:1) t2) ∈ H15 by Theorem 15, anyway. Similarly,we add the rule N0(cid:1)1 ::= (N0 (cid:1) N1). The universal substitutions we obtain following theconstruction of Lemma 14 are simply τ1 and τ2 from Example 8. We do not extend them toalso include variables like v({N0(cid:1)0}, {N0(cid:1)1}) = v0(cid:1)0,0(cid:1)1 in their domain because neitherv0(cid:1)0,0(cid:1)1 ∈ H15 nor v(cid:13)0,0(cid:14),(cid:13)0,1(cid:14) ∈ H14 would make sense. From a formal point of view,retaining the τi from Example 8 restricts H15 and H14 to predicates and terms of the formt1 (cid:1) t2 and (cid:13)t1, t2(cid:14), respectively.After lifting the extended G wrt τ1, τ2, we obtain the grammar G12 from Fig. 3 (bottom),extended by some rules like N0(cid:1)0,0(cid:1)1 ::= (N00 (cid:1) N01). By Theorem 15, each element ofH15(F +, F −) = LG12(N0(cid:1)0,0(cid:1)1) is a hypothesis satisfying the Sufficiency and the StrongConsistency requirement. Using the variable naming convention from Example 8, membersof H15 are, e.g.:1. v00 (cid:1) v012. v00 (cid:1) v01 ∗ v013. v00 ∗ v00 (cid:1) v014. v00 ∗ v01 (cid:1) v11 ∗ v015. 0 (cid:1) v016. v00 (cid:1) v00 + v01.16J. Burghardt / Artificial Intelligence 165 (2005) 1–35Hypothesis 1 intuitively means that ((cid:1)) relates every possible pair of terms. Hypotheses 2and 3 indicate that F + was chosen too specifically, viz. all examples had quadratic numbersas the right or left argument of ((cid:1)). Similarly, hypothesis 5 reflects the fact that all leftarguments are actually zero. While 0 (cid:1) x is a valid law, x (cid:1) y ⇔ x =E 0 is not a validdefinition. Similarly, no variant of x (cid:1) x can be found in H15 because it does not cover thesecond example from F +. Hypothesis 6 is an acceptable definition of the ((cid:1)) relation onnatural numbers; it corresponds to x (cid:1) y ⇔ ∃z ∈ N : y =E x + z. If we take F + as above,but F − ⇔ s(0) (cid:12)(cid:1) 0, we get S as the set of all 24 substitutions with domain {v00, . . . , v11}and range {0, s(0)}. The resulting grammar for H15 is too large to be shown here. H15 stillcontains hypotheses 5 and 6 from above, but no longer 1 to 4.We now demonstrate how E-generalization can be incorporated into an existing Induc-tive Logic Programming method to learn clauses. To be concrete, we chose the methodof relative least general generalization (r-lgg), which originates from [30] and forms thebasis of the GOLEM system [27]. We show how to extend it to deal with a given equationalbackground theory E.Theorem 16 (Clausal definitions). Let two ground clauses C1 and C2 be given. We cancompute a regular set H = lggE(C1, C2) such that:• each C ∈ H is a clause that E-subsumes C1 and C2; and• each clause E-subsuming both C1 and C2 also subsumes an element of H .i.e., hIf {p1(t1), . . . , pm(tm)} ∈ H ,Proof. Let M = {(cid:13)L1, L2(cid:14) | L1 ∈ C1 ∧ L2 ∈ C2 ∧ L1 fits L2}. Assuming M ={(cid:13)pi(t1i), pi(t2i)(cid:14) | i = 1, . . . , m}, let T + = {(cid:13)t11, . . . , t1m(cid:14), (cid:13)t21, . . . , t2m(cid:14)} and T − = {}.Let H = {{pi(ti) | i = 1, . . . , m} | (cid:13)t1, . . . , tm(cid:14) ∈ H14(T +, T −)}. H is again a regular treelanguage, because the regular H14(T +, T −) is the image of H under the tree homomor-phism that maps {p1(x1), . . . , pm(xm)} to (cid:13)x1, . . . , xm(cid:14), cf. [10, Theorem 7 in Section 1.4].then {p1(t1χi), . . . ,pm(tmχi)} ⊆E Ci for i = 1, 2, where χi are the substitutions from the definition of+. Conversely, let some clause C E-subsume both C1 and C2. We assume wloghC = {p1(t1), . . . , pn(tn)} and tj σi =E tij for some σi for i = 1, 2 and j = 1, . . . , n. ByLemma 5, some σ exists such that tj σ τi =E tij . Choosing t (cid:5)= tj σ for j = 1, . . . , n andjt (cid:5)= v(N(t1j ), N(t2j )) for j = n + 1, . . . , m, we obtain t (cid:5)j τi =E tij for j = 1, . . . , m. Hence,j+((cid:13)t (cid:5)h(cid:14), T +, T −) holds, i.e., Cσ ⊆ {p1(t (cid:5)+((cid:13)t1, . . . , tm(cid:14), T +, T −),To compute H , the grammar defining all [tij ]E must be extended to also defineE. Since only nonterminals for congruence classes are added, no additional[(cid:13)ti1, . . . , tim(cid:14)]language intersections are necessary to compute the extended τi . (cid:1)1), . . . , pm(t (cid:5)1, . . . , t (cid:5)m)} ∈ H .mFor an empty E, we have lggE(C1, C2) = {lgg(C1, C2)}, and Theorem 16 impliesPlotkin’s lgg theorem [30, Theorem 3] as a special case. In the terminology of Fig. 4,we have F + ⇔ C1 ∧ C2 and F − ⇔ true. The Consistency requirement is satisfied if somepredicate symbol p different from (=E) occurs in both C1 and C2 but not in B, exceptfor p’s congruence axiom. In this case, each hypothesis h will have the form p(. . .) ∨ . . . ,hence B ∧ h cannot be contradictory. The set lggE(C1, C2) is a subset of, but is not equalto, the set of all hypothesis clauses satisfying Sufficiency. Usually, there are other clausesJ. Burghardt / Artificial Intelligence 165 (2005) 1–3517that imply both C1 and C2 but do not E-subsume both. The same limitation applies toPlotkin’s syntactical lgg.Theorems 16 and 15 share a special case: lggE({p(t1)}, {p(t2)}) from Theorem 16equals H15(p(t1) ∧ p(t2), true) from Theorem 15. In this case, Theorem 15 is strongerbecause it ensures that the result set contains all sufficient hypotheses. On the other hand,Theorem 16 allows a more general form of both hypotheses and examples.To illustrate Theorem 16, consider a well-known example about learning family re-lations. We use the abbreviations d—daughter, p—parent, f—female, e—eve, g—george,h—helen, m—mary, n—nancy, and t—tom. Let the background knowledge K ⇔ p(h, m) ∧p(h, t) ∧ p(g, m) ∧ p(t, e) ∧ p(n, e) ∧ f (h) ∧ f (m) ∧ f (n) ∧ f (e) and the positive ex-amples F1 ⇔ d(m, h) and F2 ⇔ d(e, t) be given. By generalizing relative to K, i.e., bycomputing lgg((F1 ← K), (F2 ← K)), and by eliminating all body literals containing avariable not occurring in the head literal, the clausal definition of the daughter relationd(vme, vht) ← p(vht, vme) ∧ f (vme) ∧ K results.In addition, using the abbreviation s–spouse, let the equations E = {s(g) = h, s(h) =g, s(n) = t, s(t) = n} be given. The congruence classes of, e.g., g and h can be describedby Ng ::= g | s(Nh) and Nh ::= h | s(Ng). We obtain by Theorem 16 all clauses of theform d(vme, tht) ← p(t (cid:5)ht, vme) ∧ p(tgn, vme) ∧ f (vme) for any tht, t (cid:5)E andtgn ∈ [g]τ1E . In order to obtain a constrained clause as before, we first choose someEtht for the head literal, and then choose t (cid:5)∩Tvar(tht)and [g]τ1∩ Tvar(tht), respectively. We use the standard intersection algorithm for treeEgrammars mentioned in Section 3.1 for filtering, which in this case requires linear time in∩ [n]τ2the grammar size for [h]τ1E . Choosing the smallest solutions for tht,Et (cid:5)ht, and tgn, we obtain d(vme, vht) ← p(vht, vme) ∧ p(s(vht), vme) ∧ f (vme), which reflectsthe fact that our background knowledge did not describe any concubinages.ht and tgn from the filtered sets [h]τ1E and [g]τ1∈ [h]τ1E∩ [n]τ2E∩[t]τ2E∩ [n]τ2∩ [t]τ2∩ [t]τ2htEE1, t (cid:5)1) ∧ p(s(cid:5)Below, we prove a learning theorem similar to Theorem 15, but that yields onlythose atomic hypotheses p(s, t) that define a determinate predicate p. Formally, weare looking for those p(s, t) that satisfy ∀s(cid:5)|=p(s(cid:5)2)) ⇒ (B |= t (cid:5)2). In such cases, we say that the hypothesis p(s, t)is determinate. Determinacy of a hypothesis is essentially a semantic property [25, Sec-tion 5.6.1]; it is even undecidable for certain background theories. In order to compute theset of all determinate hypotheses, we have to make a little detour by defining a notion ofweak determinacy, which is equivalent to a simple syntactic criterion (Lemma 18).2: (B ∧ p(s, t) ∧ s(cid:5)=E s(cid:5)2=E t (cid:5)1, s(cid:5)2, t (cid:5)1, t (cid:5)1, t (cid:5)11Since B does not imply anything about p except its congruence property, we as-sume B ⇔ B(cid:5)(cid:5) ∧ (∀x, y, x(cid:5), y(cid:5): x =E x(cid:5) ∧ y =E y(cid:5) ∧ p(x, y) → p(x(cid:5), y(cid:5))), where p doesnot occur in B(cid:5)(cid:5). We replace the full congruence axiom about p by a partial one: B(cid:5) ⇔B(cid:5)(cid:5) ∧ (∀x, y, y(cid:5): y =E y(cid:5) ∧ p(x, y) → p(x, y(cid:5))). We call a hypothesis p(s, t) weakly deter-minate if ∀s(cid:5), t (cid:5)2). For example,using E from Fig. 1, p(x ∗ y, x + y) is a weakly, but not ordinarily, determinate hypothesis.We define for sets T +, T − of ground-term pairs and arbitrary terms s, t:2: (B(cid:5) ∧ p(s, t) |= p(s(cid:5), t (cid:5)2)) ⇒ (B(cid:5) |= t (cid:5)1) ∧ p(s(cid:5), t (cid:5)=E t (cid:5)1, t (cid:5)1+−hh(s, t, T(s, t, T+−) ⇔ ∀(cid:13)s) ⇔ ∀(cid:13)s(cid:5)(cid:5)(cid:5)(cid:14) ∈ T(cid:5)(cid:14) ∈ T, t, t+ ∃σ : sσ = s− ∀σ : sσ (cid:12)= s(cid:5) ∧ tσ =E t(cid:5) ∨ tσ (cid:12)=E t(cid:5)(cid:5).andWe have a lemma similar to Lemma 13, with a similar proof, which is omitted here.18J. Burghardt / Artificial Intelligence 165 (2005) 1–35Lemma 17 (Weak requirements). Let si, ti, s(cid:5)i, t (cid:5)Let F + ⇔mi=n+1and T − = {(cid:13)sn+1, tn+1(cid:14), . . . , (cid:13)sm, tm(cid:14)}. Then:i=1 p(si, ti) and F − ⇔(cid:1)n(cid:1)i be ground terms and s, t arbitrary terms.¬p(si, ti). Let T + = {(cid:13)s1, t1(cid:14), . . . , (cid:13)sn, tn(cid:14)}(1) B(cid:5) ∧ p(s, t) |= p(s(cid:5)1, t (cid:5)(2) B(cid:5) ∧ p(s, t) |= F + iff h(3) B(cid:5) ∧ p(s, t) ∧ F − (cid:12)|= false iff h1)∨· · ·∨p(s(cid:5)+(s, t, T +).n(cid:5) , t (cid:5)−(s, t, T −).n(cid:5)) iff sσ =s(cid:5)i∧ tσ =E t (cid:5)i for some i, σ .Lemma 18 (Syntactic criterion).(1) If var(t) ⊆ var(s), then the hypothesis p(s, t) is weakly determinate.(2) Each weakly determinate hypothesis p(s, t) has a weakly determinate instancep(sσ, tσ ) with var(tσ ) ⊆ var(sσ ) and B(cid:5) |= p(s, t) ↔ p(sσ, tσ ).Proof.1) ∧ p(s(cid:5), t (cid:5)(1) If B(cid:5) ∧ p(s, t) |= p(s(cid:5), t (cid:5)∧ tσ2 =E t (cid:5)2by Lemma 17(2). Hence, σ1 and σ2 coincide on var(s) ⊇ var(t), and we get t (cid:5)=E1tσ1 = tσ2 =E t (cid:5)2.2), we have sσ1 = s(cid:5) = sσ2 ∧ tσ1 =E t (cid:5)(2) Define σ = {x (cid:3)→ a | x ∈ var(t) \ var(s)}, where a is an arbitrary constant. Thenvar(tσ ) ⊆ var(sσ ) by construction. Since B(cid:5) |= p(s, t) → p(sσ, tσ ), we have the sit-uation that p(sσ, tσ ) is again weakly determinate.Since sσ = s, B(cid:5) ∧ p(s, t) |= p(s, t) ∧ p(s, tσ ). Since p(s, t) is weakly determinate,this implies t =E tσ . Since B(cid:5) ensures that p is E-compatible in its right argument,we have B(cid:5) ∧ p(sσ, tσ ) |= p(s, t). (cid:1)1Lemma 19 (Equivalence for constructor terms). Let s be a constructor term.(1) p(s, t) is a weakly determinate hypothesis iff it is a determinate one.(2) B ∧ p(s, t) |= p(s(cid:5), t (cid:5)) iff B(cid:5) ∧ p(s, t) |= p(s(cid:5), t (cid:5)), if s(cid:5) is an instance of s.(3) B ∧ p(s, t) |= p(s(cid:5), t (cid:5)) iff B(cid:5) ∧ p(s, t) |= p(s(cid:5), t (cid:5)), if s(cid:5) is a constructor term.Proof. All if directions follow from B |= B(cid:5).(1) Obtain some σ from Lemma 18(2) such that p(sσ, tσ ) is again weakly determi-nate, var(sσ ) ⊇ var(tσ ), and B(cid:5) |= p(s, t) ↔ p(sσ, tσ ). From the latter, we getB |= p(s, t) ↔ p(sσ, tσ ).1) ∧ p(s(cid:5)Hence, if B ∧ p(s, t) ∧ s(cid:5)=E1s(cid:5)2 for some σ1, σ2 by Lemma 13(2). Since s2is a constructor term, we have xσ σ1 =E xσ σ2 for each x ∈ var(sσ ) ⊇ var(tσ ). Hence,t (cid:5)1=E s(cid:5)|= p(s(cid:5)1, t (cid:5)2∧ tσ σ2 =E t (cid:5)=E sσ σ2 and tσ σ1 =E t (cid:5)12) holds, we have sσ σ1 =E s(cid:5)=E tσ σ1 =E tσ σ2 =E t (cid:5)2.(2) Obtain sχ =E s(cid:5) and tχ =E t (cid:5) for some χ from Lemma 13(2) and the definition of h+.Since sσ (cid:5) = s(cid:5) for some σ (cid:5), we have sσ (cid:5) =E sχ . Since s is a constructor term, we havexσ (cid:5) =E xχ for all x ∈ var(s) ⊇ var(t). Hence, tσ (cid:5) =E tχ =E t (cid:5). By Lemma 17(2), thisimplies B(cid:5) ∧ p(s, t) |= p(s(cid:5), t (cid:5)).2, t (cid:5)1J. Burghardt / Artificial Intelligence 165 (2005) 1–3519(3) Follows from (2), since sχ =E s(cid:5) implies that s(cid:5) is an instance of s. (cid:1)Lemma 19 ends our little detour. It ensures that weak and ordinary determinacy coincideif we supply only constructor terms to the input argument of a hypothesis p. On the onehand, this is a restriction because we cannot learn a hypothesis like p(x ∗ x, x), whichdefines a partial function realizing the integer square root. On the other hand, it is oftendesirable that a hypothesis correspond to an explicit definition, i.e., that it can be appliedlike a rewrite rule to a term s by purely syntactical pattern matching. Tuples built usingthe operator (cid:13). . .(cid:14) are the most frequently occurring special cases of constructor terms. Forexample, a hypothesis p((cid:13)x, y(cid:14), x + 2 ∗ y) may be preferred to p((cid:13)2 ∗ x, y(cid:14), 2 ∗ (x + y))because the former is explicit and implies the latter wrt E from Fig. 1. Lemma 19(2) allowsus to instantiate (cid:13)x, y(cid:14) from the former hypothesis arbitrarily, even with non-constructorterms like (cid:13)2 ∗ 1, z1 + z2(cid:14).The following lemma corresponds to Lemma 14, but leads to reduced time complex-ity. It does not need to compute universal substitutions because it uses constrained E-generalization from Definition 2. It still permits negative examples, handling them moreefficiently than Lemma 14. They may make sense even if only determinate predicates areto be learned because they allow us to exclude certain undesirable hypotheses withoutcommitting to a fixed function behavior.Lemma 20 (Weakly determinate hypotheses). For each finite set of ground term pairsT + ∪ T −, we can compute a regular set H = H20(T +, T −) such that for all s, t ∈ TV :(cid:13)s, t(cid:14) ∈ H ⇒ h+∃σ : (cid:13)sσ, tσ (cid:14) ∈ H ⇐ h+(s, t, T(s, t, T++−−) ∧ h) ∧ h(s, t, T(s, t, T−−) ∧ var(s) ⊇ var(t),) ∧ var(s) ⊇ var(t).andProof. Assume T + = {(cid:13)si, ti(cid:14) | i = 1, . . . , n} and T − = {(cid:13)si, ti(cid:14) | i = n+1, . . . , m}. For{1, . . . , n} ⊆ I ⊆ {1, . . . , m}, let sI be the most specific syntactical generalization of {si |i ∈ I }, with sI σI,i = si for each i ∈ I . Such an I is called maximal if ∀{1, . . . , n} ⊆ I (cid:5) ⊆{1, . . . , m}: sI = sI (cid:5) ⇒ I (cid:5) ⊆ I , where (=) denotes term equality up to renaming.For example, if T + = {(cid:13)a + a, ta(cid:14)} and T − = {(cid:13)b + b, tb(cid:14), (cid:13)b + c, tc(cid:14)}, then {1, 2} and{1, 2, 3} are maximal, but {1, 3} is not. Since s{1,2} = x + x can be instantiated to a + a andb + b, we must merely ensure that t{1,2}{x (cid:3)→ a} (cid:12)=E ta and t{1,2}{x (cid:3)→ b} (cid:12)=E tb in order−(s{1,2}, t{1,2}, T −). However, for s{1,3} = x + y, it is not sufficient to ensureto obtain ht{1,3}{x (cid:3)→ a, y (cid:3)→ a} (cid:12)=E ta and t{1,3}{x (cid:3)→ b, y (cid:3)→ c} (cid:12)=E tc. Since s{1,3} happens to be−(s{1,3}, t{1,3}, T −) could be violated if t{1,3}{x (cid:3)→ b, y (cid:3)→ b}instantiable to b+b as well, h=E tb. Therefore, only generalizations sI of maximal I should be considered.i∈I,i>nE . EachE, . . . , [tm]such set TI can be computed from [t1]E by standard tree grammar algorithms.Given the grammar for each TI , it is easy to compute a grammar for their tagged unionH = {(cid:13)sI , tI (cid:14) | I ∈ I ∧ tI ∈ TI }. To prove the properties of H , first observe the following:Let I be the set of all maximal I . For I ∈ I, let TI =(cid:3)ni=1[ti]σI,iE[ti]σI,i(cid:4)\(1) We always have var(tI ) ⊆ dom σI,1 ⊆ var(sI ). The first inclusion follows from tI ∈TI ⊆ [t1]σI,1E , the second from the definition of σI,1.20J. Burghardt / Artificial Intelligence 165 (2005) 1–35(2) If I is maximal and sI σ = si for some i ∈ {1, . . . , m} and σ , then i ∈ I :Since sI σI,j = sj for j ∈ I and sI σ = si , the term sI is a common generalizationof the set {sj | j ∈ I } ∪ {si}. Hence, its most special generalization, viz. sI ∪{i}, is aninstance of sI . Conversely, sI ∪{i} is trivially a common generalization of {sj | j ∈ I };hence sI is an instance of sI ∪{i}. Therefore, sI ∪{i} = sI , which implies i ∈ I because Iis maximal.• If s, t are given such that h• If I ∈ I and tI ∈ TI , then trivially sI σI,i = si and tI σI,i =E ti for each i (cid:1) n. AssumesI σ = si and tI σ =E ti for some σ and some i > n. By (2), we have i ∈ I , and there-fore sI σI,i = si . Hence, σI,i and σ coincide on var(sI ) ⊇ var(tI ), using (1). We gettI σI,i = tI σ =E ti , which contradicts tI /∈ [ti]σI,iE .−(s, t, T −) hold, let I = {1, . . . , n} ∪+(s, t, T +) and h= si}. Then, s is a common generalization of {si | i ∈ I }, and{i | n < i (cid:1) m ∧ ∃σ (cid:5)we have sσ = sI for some σ .We show I ∈ I: Let I (cid:5) be such that sI (cid:5) = sI and let i ∈ I (cid:5), then sσ σI (cid:5),i = sI σI (cid:5),i =sI (cid:5)σI (cid:5),i = si , hence i ∈ I . Since i was arbitrary, we have I (cid:5) ⊆ I , i.e., I is maximal.For i (cid:1) n, we have sσ σI,i = sI σI,i = si = sσi . In other words, σ σI,i and the σi ob-+(s, t, T +) coincide on var(s) ⊇ var(t). Hence, tσ σI,i = tσi =E ti , i.e.,tained from htσ ∈ [ti]σI,iE . For i > n and i ∈ I , we still have sσ σI,i = si , as above. Hence tσ cannotbe a member of [ti]σI,iE . Therefore, (cid:13)sσ, tσ (cid:14) ∈ H . (cid:1)i : sσ (cid:5)iTheorem 21 (Atomic determinate definitions). Let F + ⇔(cid:1)i=1 p(si, ti) and F − ⇔¬p(si, ti) be given such that each ti is ground and each si is a ground constructormi=n+1term. Then, we can compute a regular set H = H21(F +, F −) such that(cid:1)n• each p(s, t) ∈ H is a determinate hypothesis satisfying the Sufficiency and the StrongConsistency requirement wrt F +, F −; and• for each determinate hypothesis satisfying these requirements and having the formp(s, t) with s a constructor term, we have p(sσ, tσ ) ∈ H for some σ .Proof. Let T + = {(cid:13)si, ti(cid:14) | i = 1, . . . , n} and T − = {(cid:13)si, ti(cid:14) | i = n + 1, . . . , m}. DefineH = {p(s, t) | (cid:13)s, t(cid:14) ∈ H20(T +, T −)}, which is again a regular tree language.• If p(s, t) ∈ H , then (cid:13)s, t(cid:14) ∈ H20(T +, T −), i.e., h−(s, t, T −) and var(s) ⊇var(t) hold. By Lemma 18(1), p(s, t) is weakly determinate; by Lemma 17(2) and (3),it satisfies the requirements wrt B(cid:5). By construction of Lemma 20, s is a constructorterm. Hence, by Lemma 19(1) and (3), p(s, t) is determinate and satisfies the require-ments wrt B, respectively.+(s, t, T +), h• Let p(s, t) be a determinate hypothesis satisfying the requirements wrt B, where sis a constructor term. By Lemma 19(1) and (3), it is also weakly determinate andsatisfies the requirements wrt B(cid:5), respectively. Obtain σ from Lemma 18(2) suchthat p(sσ, tσ ) additionally satisfies var(tσ ) ⊆ var(sσ ). By Lemma 17(2) and (3), we−(sσ, tσ, T −), respectively. By Lemma 20, we havethen have h(cid:13)sσ σ (cid:5), tσ σ (cid:5)(cid:14) ∈ H20(T +, T −) for some σ (cid:5), i.e., p(sσ σ (cid:5), tσ σ (cid:5)) ∈ H .+(sσ, tσ, T +) and hJ. Burghardt / Artificial Intelligence 165 (2005) 1–3521To compute H , the union of up to (m − n) · 2m−n, the intersection of n and the differencebetween two grammars are needed. No additional grammar intersection is necessary tocompute any universal substitution. (cid:1)Examples of the application of Theorem 21 are given in Section 5.We now show that learning a semi-determinate clause by lgg can be simulated by learn-ing an equivalent constrained clause using E-generalization. By analogy to the above,obtain the background theory B(cid:5) from B by replacing the full congruence axiom for p0with a partial one. Lemma 22 shows how a semi-determinate clause C can be transformedinto an equivalent constrained clause dlr(C). Theorem 23 simulates lgg-learning of C bylggcE-learning of dlr(C).(cid:1)Lemma 22 (Determinate literal removal). Let a semi-determinate clause C ⇔ (p0(s0, t0) ←(cid:1)nmi=1 qi(ti)) be given such that pi(si, xi) ⇔ gi(si) =E xi . Let σ =i=1 pi(si, xi) ∧mi=1 qi(tiσ )) is a con-{xn (cid:3)→ gn(sn)} . . . {x1 (cid:3)→ g1(s1)}. Then, dlr(C) ⇔ (p0(s0σ, t0σ ) ←strained clause that defines the same relation for p0 wrt B(cid:5), and hence also wrt B.(cid:1)Proof. From the properties of semi-determinacy, we have s0σ = s0. Since p0 does notoccur in B(cid:5) outside its partial congruence axiom, we can use the following property of SLDresolution [11]: B(cid:5) ∧ (p0(s0, t0) ← C) |= p0(s, t) iff s0σ (cid:5) = s ∧ t0σ (cid:5) =E t and B(cid:5) |= Cσ (cid:5)for some σ (cid:5). A similar property holds for dlr(C).The proofs of both directions are based on establishing xiσ σ (cid:5) =E xiσ (cid:5). This propertyfollows from pi(siσ (cid:5), xiσ (cid:5)) and the definitions of gi and σ , when (B(cid:5) ∧ C |= p0(s, t)) ⇒(B(cid:5) ∧ dlr(C) |= p0(s, t)) is proved. When the converse direction is shown, it is establishedby extending σ (cid:5) to var(dlr(C)) ∪ {x1, . . . , xn} defining xiσ (cid:5) = xiσ σ (cid:5). (cid:1)Theorem 23 (Clausal determinate definitions). We use the abbreviation D = {¬pi(s, t) |s, t ∈ T{} ∧ pi determinate ∧ B(cid:5) |= pi(s, t)}. Let two ground Horn clauses C1 and C2 begiven, such that each body literal of each Ci is entailed by B(cid:5) and is not an element of D.We can compute a regular tree language H = lggcE(C1, C2) such that:• each member C ∈ H is a constrained clause that E-subsumes C1 and C2;2) with C(cid:5)• and for each semi-determinate clause C ⊆ lgg(C1 ∪ C(cid:5)1, C2 ∪ C(cid:5)1, C(cid:5)2⊆ D,dlr(C) subsumes some member of H .Proof. For i = 1, 2, let p0(s0i, t0i) be the head literal of Ci . Let M be the set ofall pairs (cid:13)L1, L2(cid:14) of body literals L1 from C1 and L2 from C2 such that L1 fits L2.Assuming M = {(cid:13)qj (tj 1), qj (tj 2)(cid:14) | j = 1, . . . , k}, define T + = {(cid:13)s01, (cid:13)t01, t11, . . . , tk1(cid:14)(cid:14),(cid:13)s02, (cid:13)t02, t12, . . . , tk2(cid:14)(cid:14)} and T − = {}. Define H = {p0(s0, t0) ← q1(t1) ∧ · · · ∧ qk(tk) |(cid:13)s0, (cid:13)t0, t1, . . . , tk(cid:14)(cid:14) ∈ H20(T +, T −)}.H is again a regular tree language because H20(T +, T −) is the image of H un-der the tree homomorphism that maps the term p0(x0, y0) ← q1(y1) ∧ · · · ∧ qk(yk) to(cid:13)x0, (cid:13)y0, y1, . . . , yk(cid:14)(cid:14). Since T − = {}, the set H20(T +, T −) contains at least one element(cid:13)s0, (cid:13)t (cid:5)(cid:14)(cid:14), and the left component of an element of H20(T +, T −) is always s0.0, t (cid:5)1, . . . , t (cid:5)k22J. Burghardt / Artificial Intelligence 165 (2005) 1–35• For each clause p0(s0, t0) ← q1(t1) ∧ · · · ∧ qk(tk) in H , we have var(s0) ⊇+(s0, (cid:13)t0, t1, . . . , tk(cid:14), T +, T −) by Theorem 20. Hence,var(t0, t1, . . . , tk) and h{p0(s0, t0), ¬q1(t1), . . . , ¬qk(tk)}χi ⊆E Ci .• Assume(cid:8)(cid:9)p0(s0, t0) ← p1(s1, x1) ∧ · · · ∧ pn(sn, xn) ∧ q1(t1) ∧ · · · ∧ qm(tm)⊆ lgg(C1 ∪ C(cid:5)1, C2 ∪ C(cid:5)2)is a semi-determinate clause. Then, (¬qj (tj σi)) ∈ Ci for some σi —we assumewlog tj σi =E tj i . Moreover, ¬pj (sj σi, xj σi) is a member of C(cid:5)⊆ D, implyingiis entailed by B(cid:5), where σ denotes the substitu-that xj σi =E gj (sj σi) = xj σ σition from dlr(C) computation by Lemma 22. Since dom σ = {x1, . . . , xn}, we havexσ σi =E xσi for all variables x. Therefore, tj σ σi =E tj σi =E tj i , and s0σ σi =s0σi = s0i because var(s0) is disjoint from the domain of σ . Hence, we can ex-tend the clause dlr(C) = {p0(s0σ, t0σ ), ¬q1(t1σ ), . . . , ¬qm(tmσ )} to some superset{p0(s0σ, t0σ ), ¬q1(t1σ ), . . . , ¬qm(tmσ ), ¬qm+1(t (cid:5)+k)} that satisfies hand var(s0σ ) ⊇ var(tj σ ) ∪ var(t (cid:5)m+1), . . . , ¬qk(t (cid:5)j (cid:5)) and is therefore a member of H .To compute lggcfine [(cid:13)s0i, (cid:13)t0i, t1i, . . . , tki(cid:14)(cid:14)]needed; no universal substitution needs to be computed. (cid:1)E must be extended by two rules to de-E as well. One intersection of the two extended grammars isE, the grammar defining [tj i]The form of Theorem 23 differs from that of Theorem 16 because neither C nor dlr(C)need E-subsume the other. To establish some similarity between the second assertion ofthe two theorems, note that a subsumed clause defining a predicate leads to a more specificdefinition that its subsuming clause. Let C(cid:5) subsume C1 and C2 and contain a nontrivialhead literal p0(. . .). Then C(cid:5) also subsumes C = lgg(C1, C2). By Theorem 23, dlr(C) sub-sumes some member of lggcE(C1, C2). That member thus leads to a more specific definitionof p0 than C(cid:5).In order to duplicate a most flexible lgg approach, Theorem 23 allows a literal pre- andpostselection strategy, to be applied before and after lgg computation, respectively. Bothmay serve to eliminate undesirable body literals from the lgg result clause. Preselectioncan be modeled using the Ci and C(cid:5)i , while postselection is enabled by choosing C (cid:2)lgg(C1 ∪ C(cid:5)2). In all cases, Theorem 23 provides a corresponding constrainedclause from lggcE(C1, C2) is sufficient wrt F + ⇔ C1 ∧ C2 andF − ⇔ true. Each such C is consistent if some predicate symbol q occurs in both C1 andC2, but not in B, except for its congruence axiom.E(C1, C2), which is equivalent to, or more specific than, C.Similar to Theorem 16, each C ∈ lggc1, C2 ∪ C(cid:5)Again similar to the nondeterminate case, H21(p0(s01, t01) ∧ p0(s02, t02), true) equalsE({p0(s01, t01)}, {p0(s02, t02)}) from Theorem 23. In this common special case, Theo-lggcrem 21, but not Theorem 23, ensures that the result set contains all sufficient hypotheses.On the other hand, Theorem 23 ensures that for each purely determinate clause, i.e., aclause without any nondeterminate qi in its body, lggcE(C1, C2) contains a clause leadingto an equivalent, or more specific, definition of p0. In other words, lgg-learning of purelydeterminate clauses can be simulated by lggcE-learning of atoms.J. Burghardt / Artificial Intelligence 165 (2005) 1–3523p0(b, bbb) ∧ p0(ε, b)a(ε, y) = ya(a(x, y), z) = a(x, a(y, z))a(x, ε) = xq(ε, d) ∧ q(b, d) ∧ q(c, e) ∧ q(bb, d) ∧ q(bc, e) ∧ q(bcb, e)FP a(ε, y, y)a([v | x], y, [v | z]) ← a(x, y, z)KqKa a(ε, ε, ε)a(ε, b, b)a(ε, bb, bb) ∧ a(b, b, bb) ∧ . . .a(ε, bbb, bbb) ∧ a(b, bb, bbb) ∧ . . .∧∧ a(b, ε, b)∧lgg p0(vb,ε, vbbb,b)← a(vb,ε, vb,ε, vbb,ε)∧ a(vbb,ε, b, vbbb,b)∧ q(vbb,ε, d)EGlggcENε ::= ε | a(Nε, Nε)Nb ::= b | a(Nε, Nb)| a(Nb, Nε)| a(Nb, Nb) . . .Nbb ::=a(Nε, Nbb)Nbbb ::=a(Nε, Nbbb) | a(Nb, Nbb). . .p0(vb,ε, a(a(vb,ε, vb,ε), b))← q(a(vb,ε, vb,ε), d)Fig. 5. Comparison of ILP using lgg and lggcE .Let us compare the ILP methods using lgg and lggcE in an example. Assume part of thebackground knowledge describes lists with an associative append operator and a neutralelement ε (nil). Lines 2–3 of Fig. 5 show a Horn program P and an equational theoryE, each of which formalizes that knowledge, where v, x, y, z denote variables, a denotesappend and b, c, d, e below will denote some constants. Moreover, let a conjunction Kq offacts about a predicate q be given, as shown in the fourth line of Fig. 5. We abbreviated,e.g., q([b, c, b], [e]) to q(bcb, e). Let F ⇔ p0(b, bbb) ∧ p0(ε, b) be given. Let us assumefor now that a preselection strategy chose K (cid:5)q⇔ q(ε, d) ∧ q(bb, d).E can use the first part of background knowledge directly. Most ILPsystems, including GOLEM, restrict background theories to sets of ground literals. Hence,they cannot directly use equality as background knowledge because this would requireformulas like p0(x, y) ← eq(y, y(cid:5)) ∧ p0(x, y(cid:5)) in the background theory. While Plotkin’slgg is also defined for nonground clauses, it has not been defined for clause sets like P .Moreover, since F contains only ground literals, all relevant arguments of body literalsmust be ground to obtain the necessary variable bindings in the generalized clause. Thus,we have to derive the conjunction Ka of all ground facts implied by P that could be relevantin any respect.Neither lgg nor lggcOn the other hand, E has to be transformed into a grammar G in order to com-E. We can do this by Theorem 11 with (≺) as the lexicographic path ordering,pute lggcwhich is commonly used to prove termination of the rewrite system associated with E[13, Section 5.3]. Alternatively, we could instantiate a predefined grammar scheme likei=0 a(Nx1...xi , Nxi+1...xn ). At least we do not have to rack ourNx1...xnbrains over the question of which terms might be relevant—it is sufficient to define thecongruence classes of all terms occurring in F or K (cid:5)q .::= n=0 ε | n=1 x1 | nLines 5–8 of Fig. 5 show the preprocessed form Ka and G of P and E, respectively.Observe that a ground literal a(s, t, u) in the left column corresponds to a grammar alter-native Nu ::= . . . a(Ns, Nt ) in the right one. It is plausible to assume that there are at leastas many literals in Ka as there are alternatives in G. Next, we compute(cid:5)q ), (p0(ε, b) ← Ka ∧ K(cid:8)(p0(b, bbb) ← Ka ∧ K(cid:9)(cid:5)q )andlgg24J. Burghardt / Artificial Intelligence 165 (2005) 1–35(cid:8)(p0(b, bbb) ← KlggcE(cid:5)q ), (p0(ε, b) ← K(cid:9)(cid:5)q )EEEEand apply some literal postselection strategy. A sample result is shown in the bottom partE results in the set of all terms p0(vb,ε, tbbb,b) ← q(tbb,ε, d)of Fig. 5. More precisely, lggcfor any tbbb,b ∈ [bbb]{vb,ε(cid:3)→b}∩ [b]{vb,ε(cid:3)→ε}. The choiceof tbbb,b and tbb,ε on the right-hand side in Fig. 5 corresponds to the choice of body literalsabout a on the left; both sides are equivalent definitions of p0. If the postselection strategychooses a(vb,ε, b, vbb,b) ∧ a(vbb,b, vb,ε, vbbb,b) ∧ a(vb,ε, vb,ε, vbb,ε) on the left, we needonly to choose tbbb,b = a(a(vb,ε, b), vb,ε) on the right to duplicate that result. However, ifpreselection chooses different literals about q, e.g., K (cid:5)⇔ q(bc, e) ∧ q(c, e), we have toqE and [c]recompute the grammar G to include definitions for [bc]and tbb,ε ∈ [bb]{vb,ε(cid:3)→b}∩ [ε]{vb,ε(cid:3)→ε}E result clause is always a constrained one, whereas lgg yields a determinateclause. The reason for the latter is that function calls have to be simulated by predicatecalls, requiring extra variables for intermediate results. The deeper a term in the lggcE clauseis nested, the longer the extra variable chains are in the corresponding lgg clause. If K (cid:5)⇔qtrue is chosen, lggcWhen the lggcE approach is used, the hypotheses search space is split. Literal pre- andpostselection strategies need to handle nondeterminate predicates only. The preselectedliterals, i.e., K (cid:5)q , control the size and form of the grammar G. Choices of, e.g., tbbb,b ∈LG(Nbbb,b) can be made independently of pre- and postselection, each choice leading tothe condensed equivalent of a semi-determinate clause.E yields an atom rather than a proper clause.The lggcE.Filtering of, e.g., LG(Nbbb,b) allows us to ensure additional properties of the resultclause if they can be expressed by regular tree languages. For example, orienting eachequation from E in Fig. 5 left to right generates a canonical term-rewriting system R.Since all terms in E are linear, a grammar GNF for the set of normal forms wrt R can beobtained automatically from E. Choosing, e.g., tbbb,b ∈ LG(Nbbb,b) ∩ LGNF (NNF) ensuresthat no redundant clause like p0(vb,ε, a(vb,ε, a(vb,ε, b))) ← q(a(vb,ε, vb,ε), d) can result.In classical ILP, there is no corresponding filtering method of similar simplicity.5. ApplicationsIn this section, we apply E-generalization in three different areas. In all cases, we usethe paradigm of learning a determinate atomic definition from positive examples only. Asindicated in Section 4, this is the most restrictive paradigm, and it therefore allows the mostefficient algorithms. We intend to demonstrate that the notion of E-generalization can helpto solve even comparatively ambitious tasks in Artificial Intelligence at the first attempt.We make no claim to develop a single application to full maturity. Instead, we cover avariety of different areas in order to illustrate the flexibility of E-generalization.5.1. Candidate lemmas in inductive proofsAuxiliary lemmas play an important role in automated theorem proving. Even in purefirst-order logic, where lemmas are not strictly necessary [18], proofs may become expo-nentially longer without them and are consequently harder to find. In induction proofs,J. Burghardt / Artificial Intelligence 165 (2005) 1–3525which exceed first-order logic owing to the induction axiom(s), using lemmas may be un-avoidable to demonstrate a certain theorem.By way of a simple example, consider an induction proof of the multiplicative asso-ciativity law using the equational theory from Fig. 1. In the inductive case, after applyingequation 4. from Fig. 1 and the induction hypothesis, the distributivity law is needed asa lemma in order to continue the proof. While this is obvious to a mathematically expe-rienced reader, an automated prover that does not yet know the law will get stuck at thispoint and require a user interaction because the actual term cannot be rewritten any further.In this simple example, where only one lemma is required, the cross-fertilization techniqueof [3] would suffice to generate it automatically. However, this technique generally fails ifseveral lemmas are needed.(cid:1)nIn such cases, we try to simulate mathematical intuition by E-generalization in order tofind a useful lemma and allow the prover to continue; i.e., to increase its level of automa-tion. We consider the last term t1 obtained in the proof so far (x ∗ (y ∗ z(cid:5)) + x ∗ y in theexample) and try to find a new lemma that could be applied next by the prover. We are look-ing for a lemma of the form t1 ≡E t2 for some t2 such that ∀σ ground: t1σ =E t2σ holds.Using Theorem 21, we are able to compute the set of all terms t2 such that t1σ =E t2σholds at least for finitely many given σ .We therefore choose some ground substitutions σ1, . . . , σn with var(t1) = {x, y, z(cid:5)} astheir domain, and let F + ⇔i=1 p((cid:13)xσi, yσi, z(cid:5)σi(cid:14), t1σi). We then apply Theorem 21 tothis F + and F − ⇔ true. (See Fig. 6, where the partial congruence property of p was usedto simplify the examples in F +.) Using the notation from Lemma 20, we have just one Iin I, viz. I = {1, . . . , n}, since we do not supply negative examples. Therefore, we onlyhave to compute TI =The most specific syntactical generalization sI of {(cid:13)x, y, z(cid:5)(cid:14)σ1, . . . , (cid:13)x, y, z(cid:5)(cid:14)σn} neednot be (cid:13)x, y, z(cid:5)(cid:14) again. However, we always have (cid:13)x, y, z(cid:5)(cid:14)σ (cid:5) = sI for some σ (cid:5). If wethat are sufficiently different, we can ensure that σ (cid:5) has an inverse. Thischoose σiis the case in Fig. 6, where σ (cid:5) = {x (cid:3)→ v021, y (cid:3)→ v310, z(cid:5) (cid:3)→ v201}. By Theorem 21,B(cid:5) ∧ p((cid:13)x, y, z(cid:5)(cid:14), tI σ (cid:5)−1) implies p((cid:13)x, y, z(cid:5)(cid:14)σi, t1σi) for each tI ∈ TI and i = 1, . . . , n.Since it trivially also implies p((cid:13)x, y, z(cid:5)(cid:14)σi, tI σ (cid:5)−1σi), we obtain t1σi =E tI σ (cid:5)−1σi usingdeterminacy.[t1σi]σiE .(cid:3)ni=1Therefore, defining t2 = tI σ (cid:5)−1 ensures that t1 and t2 have the same value under eachsample substitution σi . This is a necessary condition for t1 ≡E t2, but not sufficient. Beforeusing a lemma suggestion t1 ≡E t2 to continue the original proof, it must be checked fort1 =x ∗( y ∗ z(cid:5) )+ x ∗ yp((cid:13) xσi , yσi , z(cid:5)σi (cid:14),( x ∗( y ∗ z(cid:5) )+ x ∗ y ) σi ))))F + ⇔ p((cid:13) 0 ,s3(0),s2(0)(cid:14),∧ p((cid:13)s2(0), s(0) , 0 (cid:14),∧ p((cid:13) s(0) , 0 , s(0) (cid:14),0s2(0)0H (cid:22)t2 =p((cid:13) v021 , v310 , v201 (cid:14), v021∗(v310∗v201 + v310 )x ∗( y ∗ z(cid:5) + y ))Fig. 6. Generation of lemma candidates by Theorem 21.26J. Burghardt / Artificial Intelligence 165 (2005) 1–35validity by a recursive call to the induction prover itself. Two simple restrictions can helpto eliminate unsuccessful hypotheses:• Usually, only those equations t1 ≡E t2 are desired that satisfy var(t2) ⊆ var(t1). Forexample, it is obvious that x ∗ (y ∗ z(cid:5)) + x ∗ y ≡E x + v123 is not universally valid. Thisrestriction of the result set is already built into Theorem 21. Any lemma contradictingthis restriction will not appear in the grammar language. However, all its instances thatsatisfy the restriction will appear.• Moreover, if E was given by a ground-convergent term-rewriting system R [13, Sec-tion 2.4], it makes sense to require t2 to be in normal form wrt R. For example,x ∗ (y ∗ z(cid:5)) + x ∗ y ≡E (x + 0) ∗ (y ∗ z(cid:5) + y ∗ s(0)) is a valid lemma, but redundant,compared with x ∗ (y ∗ z(cid:5)) + x ∗ y ≡E x ∗ (y ∗ z(cid:5) + y). The closed representation ofthe set TI as a regular tree language allows us to easily eliminate such undesired termst2. For left-linear term-rewriting systems [13, Section 2.3], the set of all normal-formterms is always representable as a regular tree language; hence terms in non-normalform can be eliminated by intersection. For rewriting systems that are not left-linear,we may still filter out a subset of all non-normal-form terms. If desired by some appli-cation, TI could also be restricted to those terms tI that satisfy V (cid:5) ⊆ var(tI ) ⊆ V forarbitrarily given variable sets V (cid:5), V .The more sample instances are used, the more of the enumerated lemma candidates will bevalid. However, our method does not lead to learnability in the limit [19] because normallyany result language will still contain invalid equations—regardless of the number of samplesubstitutions. It does not even lead to PAC-learnability [39], there currently being no way tocompute the number of sample substitutions depending on the required δ and (cid:8) accuracies.In the associativity law example from above, we get, among other equations, the lemmasuggestion x ∗ (y ∗ z(cid:5)) + x ∗ y ≡E x ∗ (y ∗ z(cid:5) + y), which allows the prover to continuesuccessfully. This suggestion appears among the first ten, if TI is enumerated by increasingterm size. Most of the earlier terms are variants wrt commutativity, like x ∗ (y ∗ z(cid:5)) + x ∗ y≡E (y + y ∗ z(cid:5)) ∗ x.Fig. 7 shows some examples of lemma candidates generated by our prototypical im-plementation (see Section 5.4). The column Theory shows the equational theory used. Wedistinguish between the truncating integer division (//) and the true division (/). For ex-ample, 7 // 3 =E 2, while 7/3 is not defined. The grammar rules that realize these partialfunctions are something like N2 ::= . . . | N6 // N3 | N7 // N3 . . . | N6/N3. The integer re-mainder is denoted by (%). We embedded the two-element Boolean algebra {0, 1} into thenatural numbers, with 1 corresponding to true. This allows us to model relations like (<)and logical connectives. The functions (↑) and (↓) compute the maximum and minimum oftwo numbers, respectively. The function dp doubles a natural number in 0-s representation,ap concatenates two lists in cons-nil representation, rv reverses a list, and ln computes itslength as a natural number. The cube theory formalizes the six possible three-dimensional90◦-degree rotations of a cube, viz. left, right, up, down, clockwise and counter-clockwise,as shown in Fig. 8 (right).The column Lemma shows the corresponding lemma, its right-hand side having beensupplied, its left-hand side generated by the above method. Note, for example, the differ-J. Burghardt / Artificial Intelligence 165 (2005) 1–3527LemmaTheory+,∗(x + y) + z = x + (y + z)+,∗x ∗ (y + z) = x ∗ y + x ∗ z+,∗x ∗ y = y ∗ x+,∗(x ∗ y) ∗ z = x ∗ (y ∗ z)+,∗(x ∗ y) ∗ z = x ∗ (y ∗ z)x/z + y/z = (x + y)/z+,−,∗,/,//,%+,−,∗,/,//,% ((x % z)+(y % z)) % z = (x + y) % z(x // y) ∗ y = x − (x % y)+,−,∗,/,//,%x = (x ∗ y) // y+,−,∗,/,//,%+,∗,<+,∗,<+,∗,<,↑,↓+,∗,<,↑,↓+,∗,<,↑,↓+,∗,dp+,∗,dp+,∗,dp¬,∧,∨,→¬,∧,∨,→ap,rvap,rvap,rvap,rvdp(x) = x + xx ∗ dp(y) = dp(x ∗ y)¬(x ∧ y) ⇔ y → ¬x¬(x ∧ y) ⇔ ¬x ∨ ¬yx < y ⇔ x ∗ z < y ∗ zx < y ⇔ x + z < y + zx < y ⇔ x < x ↑ yx = x ↓(x ↑ y)ap(rv(x), rv(y)) = rv(ap(y, x))ap(x, ap(y, z)) = ap(ap(x, y), z)dp(x) + dp(y) = dp(x + y)(x ↑ y) + (x ↓ y) = x + yx = rv(rv(x))rv(rv(x)) = xap,rv,lnap,rv,lncubecubeln(ap(x, y)) = ln(x) + ln(y)ln(cons(x, ap(y, z))) = s(ln(y) + ln(z))lf (cc(x)) = up(lf (x))lf (cc(x)) = cc(up(x))Fig. 7. Generated lemma candidates.Rhs1,1,30,2,20,00,0,20,0,2,45,1,30,1,16,0,32,1,30,1,10,1,10,1,13,0,35,1,52,40,40,01,1,1,01,1,1,02,2,23,3,20,20,21,22,31,11,1No6.10.3.31.3.2.1.1.4.3.20.6.7.2.2.4.13.1.6.1.1.4.1.4.10.1.2.Time2117072226332419206173041701417495817495847128456784267061130830889296114211818ence between the lemmas x = rv(rv(x)) and rv(rv(x)) = x. The column Rhs indicates thesize of t1σi for i = 1, . . . , n, which is a measure of the size of grammars to be intersected.For arithmetic and list theories, the value of each number t1σi and the length of each listt1σi is given, respectively. The column No shows the place in which the lemma’s right-handside appeared in the enumeration sequence, while the column Time shows the required run-time in milliseconds (compiled PROLOG on a 933 MHz PC). Both depend strongly on thenumber and size of the example ground instances. The dependence of No can be seen inlines 4 and 5.The runtime also depends on the grammar connectivity. In a grammar that includes,e.g., (−) or (<), each nonterminal can be reached from any other, while in a grammarconsidering, e.g., only (+) and (∗), only nonterminals for smaller values and Nt can bereached. If the grammar defines Nt , N0, . . . , N6, computing [0]σ1E leads toE8 · 8 · 8 intersection nonterminals in the former case, compared with 2 · 3 · 3 in the latter. For∩ [1]σ2E∩ [1]σ328J. Burghardt / Artificial Intelligence 165 (2005) 1–35Fig. 8. Law computation by Theorem 21 (left). Cube rotations (right).this reason, runtimes are essentially independent of the Rhs sizes for the 2nd to 4th theory.The exception in line 6 is due to a larger input grammar.5.2. Construction laws of seriesA second application of E-generalization consists in the computation of constructionlaws for term series, as in ordinary intelligence tests. The method is also based on Theo-rem 21 and is explained below.For technical reasons, we write a series in reverse order as a cons-nil list, using an infix“.” for the reversed cons to enhance readability. We consider suffixes of this list and appenda number denoting its length to each of them. We use a binary predicate p(l.s, n) to denotethat the suffix s of length l leads to n as the next series element. We apply Theorem 21 tothe k last leads to relations obtained from the given series, see Fig. 8 (left), where k mustbe given by the user. Each result has the form p(l.s, n) and corresponds to a rewrite rulel.s (cid:2) n that computes the next term from a series suffix and its length. By construction,the rewrite rule is guaranteed to compute at least the input terms correctly. A notion ofcorrectness is not formally defined for later terms, anyway.Fig. 9 shows some computed construction laws. Its first line exactly corresponds to theexample in Fig. 8 (left). The column Theory indicates the equational theory used. Theternary function if realizes if · then · else, with the defining equations if (s(x), y, z) = yand if (0, y, z) = z, and the unary function ev returns s(0) for even and 0 for odd naturalnumbers. Using if and ev, two series can be interleaved (cf. line 5).The column Series shows the given term series, sn(0) being abbreviated to n. The num-ber k of suffixes supplied to our procedure corresponds to the number of series terms tothe right of the semi-colon. Any computed hypothesis must explain all these series terms,but none of the earlier ones. The column Law shows the computed hypothesis. The placewithin the series is denoted by vp, the first term having place 0, the second place 1, and soon. The previous series term and the last but one are denoted by v1 and v2, respectively.The column No shows the place in which the law appeared in the enumeration sequence. Inline 5, some formally smaller (wrt height) terms are enumerated before the term shown inFig. 9, but are nevertheless equivalent to it. The column Time shows the required runtime inmilliseconds on a 933 MHz machine, again strongly depending on k and the size of seriesterms.The strength of our approach does not lie in its finding a plausible continuation of agiven series, but rather in building, from a precisely limited set of operators, a nonrecursiveJ. Burghardt / Artificial Intelligence 165 (2005) 1–3529Fig. 9. Computed construction laws.algorithm for computing the next series terms. Human superiority in the former area isdemonstrated in line 9, where no construction law was found. The strength of the approachin the latter area became clear by the series 0, 0; 1, 0, 0, 1, shown in line 7. We had notexpected any construction law to exist at all, because the series has a period relative primeto 2 and the trivial solution v3 had been eliminated by the choice of k (a construction lawmust compute the first 1 from the preceding 0s).It is decidable whether the result language H21 is finite; in such cases, we can make pre-cise propositions about all construction laws that can be expressed using the given signatureand equational theory. For example, from line 9 we can conclude that no construction lawcan be built from the given operators.5.3. Generalizing screen editor commandsBy way of another application, we employed E-generalization for learning complexcursor-movement commands of a screen-oriented editor like UNIX vi. For each i, j ∈ N,let pi,j be a distinct constant denoting the position of a given file at column i and line j ; letP = {pi,j | i, j ∈ N}. For the sake of simplicity, we assume that the screen is large enoughto display the entire contents of the file, so, we do not deal with scrolling commands forthe present.Assuming the file contents to be given, cursor-movement commands can be modeled aspartial functions from P to itself. For example, d(pi,j ) = pi,j +1 if j + 1 (cid:1) li, undefinedotherwise, models the down command, where li denotes the number of lines in the file.The constant H = p1,1 models the home command. Commands may depend on the filecontents. For example,(cid:6)W (pi,j ) = mini(cid:7)(cid:5) (cid:1) co(j ) ∧ ch(pi(cid:5)−1,j ) ∈ SP ∧ ch(pi(cid:5),j ) /∈ SP,(cid:5) | i < i30J. Burghardt / Artificial Intelligence 165 (2005) 1–35a b c d e f g h i j k l m n o p q r s t u v w1 CURSOR MOTIO N COMMANDS :2 l left3 r right4 u up5 d downH h omem m atching ( )W n ext wordB p rev wordb2 ::= l(c2) | r(a2) | u(b3) | d(b1) .c2 ::= l(d2) | r(b2) | u(c3) | d(c1) |W(a2)|W(b2)| B(d2)...B(k2) .k2 ::= l(l2) | r(j2) | u(k3) | d(k1) |B(l2) |B(m2)| W(c2)...W(j2) .Fig. 10. Example file contents (left). Corresponding grammar excerpt (right).if the minimum is defined, models the next word command, where co(j ), ch(p), and SPdenote the number of columns of line j , the character at position p, and the set of spacecharacters, respectively. From a given file contents, it is easy to compute a regular treegrammar G that describes the congruence classes of all its positions in time linear to thefile size and the number of movement commands. Fig. 10 gives an example. For the sakeof brevity, columns are “numbered” by lower-case letters, and, e.g., L(b2) = [pb,2]E. Notethat the file contents happen to explain some movement commands.Using E-generalization, two or more cursor movements can easily be generalized toobtain a common scheme. Given the start and end positions, s1, . . . , sn and e1, . . . , en, weapply Theorem 21 to F + ⇔ p(s1, e1) ∧ · · · ∧ p(sn, en) and get a rule of the form p(x, t),where t ∈ T{x} is a term describing a command sequence that achieves each of these move-ments.For n = 1, we can compute the simplest term that transforms a given starting positioninto a given end position. This is useful to advise a novice user about advanced cursor-movement commands. Imagine, for example, that a user had typed the commands l, l, l,l, l, l, l, l, l to get from position pk,2 to pb,2. The term of least height obtained from Theo-rem 21, viz. p(x, l(B(x))), indicates that the same movement could have been achieved bysimply typing the commands B, l. Each command could also be assigned its own degreeof simplicity, reflecting, for example, the number of modifier keys (like shift) involved, ordistinguishing between simple and advanced commands. In the former case, the simplestterm minimized the overall numbers of keys to be pressed.No grammar intersection is needed if n = 1. Moreover, the lifting of G can be done inconstant time in this case. In the example, it is sufficient to include an alternative . . . x . . .into the right-hand side of the rule for k2. Therefore, a simplest term can be computedin an overall time of O(#G · log #G). Changes in the file content require recomputationof the grammar and the minimum term sizes. In many cases, but not if, for example, aparenthesis is changed, local content changes require local grammar changes only. It thusseems worthwhile to investigate an incremental approach, which should also cover weightrecomputation.For n > 1, the smallest term(s) in the result language may be used to implement anintelligent approach to repeat the last n movement command sequences. For example, thesimplest scheme common to the movements p(pm,2, po,2) and p(pn,4, pv,4) wrt the filecontent of Fig. 10 is computed as p(x, d(W (u(x)))). Since the computation time growsexponentially with n, it should be small.J. Burghardt / Artificial Intelligence 165 (2005) 1–3531In our prototypical implementation, we considered in all the vi commands h, j, k, l,H, M, L, +, -, w, b, e, W, B, E, 0, $, f, F, %, {, and } and renamed some of them to give themmore suggestive identifiers. We allow search for single characters only. In order to considernontrivial string search commands as well, the above approach should be combined with(string) grammar inference [23,34] to learn regular search expressions. Moreover, com-mands that change the file content should be included in the learning mechanism. Andlast but not least, a satisfactory user interface for these learning features is desirable, e.g.,allowing us to define command macros from examples.5.4. Prototypical implementationWe built a prototypical implementation realizing the E-generalization method fromSection 3.1 and the applications from Section 5. It comprises about 4,000 lines of PRO-LOG code. Fig. 11 shows its architecture, an arrow meaning that its source function usesits destination function. The application module allows us to learn series laws, candidatelemmas, and editor cursor commands (edt cmds). The anti-unification module containsalgorithms for syntactic (synt au), constrained (cs e-au) and unconstrained (uc e-au) E-anti-unification. The grammar-generation module can compute grammars for a given filecontent (edt grm), for any set {t ∈ T | V ⊆ var(t) ⊆ W } (var grm), and for the set of nor-mal forms wrt E (nf grm). The grammar algorithms module allows us to test an L(N) forfiniteness, emptiness, and a given member t, to compute intersection and complement oftwo languages, to simplify a grammar, and to generate Nmax from Lemma 5 (max nt s) anda grammar for T{} (top nt). For the sake of clarity, we omitted the dashed lines around thepre- and postprocessing module. The former merely contains code to choose exm instancesfor lemma generation. The latter does term evaluation to normal form, and enumerationand minimal weight computation for L(N). The prototype still uses monolithic, speciallytailored algorithms for E-anti-unification, as originally given in [22], rather than the com-bination of standard grammar algorithms described in Section 3.1. For this reason, functionintersect uses cs e-au as a special case, viz. σi = {}, rather than vice versa. However, allFig. 11. Prototype architecture.32J. Burghardt / Artificial Intelligence 165 (2005) 1–35other uses relations would remain unchanged in an implementation strictly based on thispaper.All runtime figures given in this paper are taken from the prototype. Currently, anefficiency-oriented re-implementation in C is planned. Moreover, it will use the availablememory more efficiently, thus allowing us to run larger examples than when using PRO-LOG. The PROLOG prototypical implementation and, in future, the C implementation canbe downloaded from the web page http://swt.cs.tu-berlin.de/∼jochen/e-au.6. Conclusions and future workWe presented a method for computing a finite representation of the set of E-generalizations of given terms and showed some applications. E-generalization is able tocope with representation change in abstraction, making it a promising approach to an oldbut not yet satisfactorily solved problem of Artificial Intelligence. Our approach is basedon standard algorithms for regular tree grammars. It thus allows us to add filtering compo-nents in a modular fashion, as needed by the surrounding application software. The closedform of an E-generalization set as a grammar and its simple mathematical characteriza-tion make it easy to prove formal quality properties if needed for an application. Using astandard grammar language enumeration algorithm, the closed form can be converted to asuccession form.Our method cannot handle every equational theory E. To use the analogy with E-deduction, our method corresponds to something between E-unification (concerned with aparticular E in each case) and paramodulation (concerned with the class of canonical E).On the other hand, neither partial functions nor conditional equations basically prevent ourmethod from being applicable.In order to demonstrate that E-generalization can be integrated into ILP learning meth-ods, we proved several ways of combining lgg-based ILP and E-generalization. Predicatedefinitions by atoms or clauses can be learned. If desired, the hypotheses space can berestricted to determinate hypotheses, resulting in faster algorithms. Learning of purely de-terminate clauses can be reduced to learning of atoms by E-generalization. An lgg-learnerfor constrained clauses with built-in E-generalization can learn a proper superclass, calledsemi-determinate predicates here. We provide completeness properties for all our hypothe-ses sets.Using E-generalization, the search space is split into two parts, one concerned with se-lection of nondeterminate literals, the other with selection of their argument terms. Whilethe first part is best handled by an elaborate strategy from classical ILP, the second canbe left to a grammar language enumeration strategy. For example, the O(#G · log #G) al-gorithm to find a term of minimal complexity within a tree language apparently has nocorresponding selection algorithm for determinate literals in classical ILP. Separating bothsearch space parts allows us to modularize the strategy algorithms and to use for each partone that best fits the needs of the surrounding application.Experiments with our prototypical implementation showed that comparatively ambi-tious AI tasks are solvable at the first attempt using E-generalization. We focus on sketch-ing applications in a number of different areas rather than on perfectly elaborating a singleJ. Burghardt / Artificial Intelligence 165 (2005) 1–3533application. By doing so, we seek to demonstrate the flexibility of E-generalization, whichis a necessary feature for any approach to be related to intelligence. In [2, Section 8], furtherapplications were sketched, including divergence handling in Knuth–Bendix completion,guessing of Hoare invariants, reengineering of functional programs, and strengthening ofinduction hypotheses. The method given in [5] to compute a finite representation of thecomplete equational theory describing a given set of finite algebras is essentially based onE-generalization, too. It is shown there that the complete theory can be used to implementfast special-purpose theorem provers for particular theories.Based on this experience, we venture to suggest that E-generalization is able to sim-ulate an important aspect of human intelligence, and that it is worth investigating further.In particular, the restrictions regular tree grammars impose on the background equationaltheory E should be relaxed. In this paper, we briefly looked at some well-known represen-tation formalisms that are more expressive than regular tree grammars but with negativeresults. It remains to be seen whether there are other more expressive formalisms that canbe used for E-generalization. The attempt should also be made to combine it with higher-order anti-unification [21,40]. Such a combination is expected to allow recursive functionsto be learned from examples.As indicated above, the applications of E-generalization could certainly be improved.Lemma generation should be integrated into a real induction prover, in particular totest its behavior in combination with the rippling method [7]. While rippling suggestschecking homomorphic laws like f (g(t1), . . . , g(tn)) =E g(cid:5)(f (t1, . . . , tn)) for validity,E-generalization is able to suggest lemmas of arbitrary forms. Empirical studies on series-based intelligence tests, e.g., using geometrical theories about mirror, shift, rotate, etc.,should look for a saturation effect: Is there one single reasonable equational backgroundtheory that can solve a sufficiently large number of common tests? And can a reasonableintelligence quotient be achieved by that theory? Currently, we are investigating the useof E-generalization in analogical reasoning [12], a new application that does not fit intothe schemas described in Section 4. The aim is to allow problems in intelligence tests tobe stated in other ways than mere linear series, e.g., to solve (A : B) = (C : X), whereA, B, C are given terms and X is a term which should result from applying a rule to C thatat the same time transforms A into B.AcknowledgementsUte Schmid, Holger Schlingloff and Ulrich Geske provided valuable advice on presen-tation.References[1] F. Baader, Unification, weak unification, upper bound, lower bound, and generalization problems, in: Proc.4th Conf. on Rewriting Techniques and Applications, in: Lecture Notes in Comput. Sci., vol. 488, Springer,Berlin, 1991, pp. 86–91.[2] J. Burghardt, B. Heinz, Implementing anti-unification modulo equational theory, Arbeitspapier 1006, GMD,June 1996.34J. Burghardt / Artificial Intelligence 165 (2005) 1–35[3] R.S. Boyer, J.S. Moore, A Computational Logic, Academic Press, New York, 1979.[4] B. Bogaert, S. Tison, Equality and disequality constraints on direct subterms in tree automata, in: Proc.STACS 9, in: Lecture Notes in Comput. Sci., vol. 577, Springer, Berlin, 1992, pp. 161–172.[5] J. Burghardt, Axiomatization of finite algebras, in: Proc. KI-2002, in: Lecture Notes in Artificial Intelligence,vol. 2479, Springer, Berlin, 2002, pp. 222–234.[6] J. Burghardt, Weight computation of regular tree languages, Technical Report 001, FIRST, December 2003.[7] A. Bundy, F. van Harmelen, A. Smaill, A. Ireland, Extensions to the rippling-out tactic for guiding inductiveproofs, in: Proc. 10th CADE, in: Lecture Notes in Artificial Intelligence, vol. 449, Springer, Berlin, 1990,pp. 132–146.[8] A.-C. Caron, H. Comon, J.-L. Coquidé, M. Dauchet, F. Jacquemard, Pumping, cleaning and symbolic con-straints solving, in: Proc. ICALP, in: Lecture Notes in Comput. Sci., vol. 820, 1994, pp. 436–449.[9] A.-C. Caron, J.-L. Coquidé, M. Dauchet, Automata for reduction properties solving, J. Symbolic Com-put. 20 (2) (1995) 215–233.[10] H. Comon, M. Dauchet, R. Gilleron, F. Jacquemard, D. Lugiez, S. Tison, M. Tommasi, Tree automatatechniques and applications. Available from http://www.grappa.univ-lille3.fr/tata, October 2001.[11] K.L. Clark, Predicate logic as a computational formalism, Research Report, Imperial College, 1979.[12] M. Dastani, B. Indurkhya, R. Scha, An Algebraic Method for Solving Proportional Analogy Problems,Dublin City University, Dublin, 1997.[13] N. Dershowitz, J.-P. Jouannaud, Rewrite systems, in: Handbook of Theoretical Computer Science, vol. B,Elsevier, Amsterdam, 1990, pp. 243–320.[14] T.G. Dietterich, R.S. Michalski, A Comparative Review of Selected Methods for Learning from Examples,Springer, Berlin, 1984, pp. 41–82.[15] P.J. Downey, R. Sethi, R.E. Tarjan, Variations on the common subexpression problem, J. ACM 27 (4) (1980)758–771.[16] S. Džeroski, Inductive Logic Programming and Knowledge Discovery in Databases, MIT Press, Cambridge,MA, 1996, pp. 117–152.[17] M. Fay, First-order unification in an equational theory, in: Proc. 4th Workshop on Automated Deduction,1979.[18] G. Gentzen, Untersuchungen über das logische Schließen, 1932.[19] E.M. Gold, Language identification in the limit, Inform. and Control 10 (1967) 447–474.[20] J.H. Gallier, W. Snyder, Complete sets of transformations for general E-unification, Theoret. Comput.Sci. 67 (1989) 203–260.[21] R.W. Hasker, The replay of program derivations, PhD thesis, Univ. of Illinois at Urbana-Champaign, 1995.[22] B. Heinz, Anti-Unifikation modulo Gleichungstheorie und deren Anwendung zur Lemmagenerierung, Doc-toral dissertation, TU Berlin, December 1995.[23] V. Honavar, R. Parekh, Grammar Inference, Automata Induction, and Language Acquisition, Marcel Dekker,New York, 1999.[24] R. Kowalski, Predicate logic as programming language, Memo 70, Dept. of Comp. Logic, School of Artif.Intell., Univ. Edinburgh, 1973.[25] N. Lavrac, S. Džeroski, Inductive Logic Programming: Techniques and Applications, Ellis Horwood, NewYork, 1994.[26] D. McAllester, Grammar rewriting, in: Proc. CADE-11, in: Lecture Notes in Artificial Intelligence, vol. 607,Springer, Berlin, 1992.[27] S. Muggleton, C. Feng, Efficient induction of logic programs, in: Proc. 1st Conf. on Algorithmic LearningTheory, Tokyo, Omsha, 1990, pp. 368–381.[28] S. Muggleton, Inductive logic programming: Issues, results and the challenge of learning language in logic,Artificial Intelligence 114 (1999) 283–296.[29] S. O’Hara, A model of the redescription process in the context of geometric proportional analogy problems,in: Proc. AII ’92, Dagstuhl, Germany, in: Lecture Notes in Artificial Intelligence, vol. 642, Springer, Berlin,1992, pp. 268–293.[30] G.D. Plotkin, A note on inductive generalization, in: B. Meltzer, D. Michie (Eds.), Machine Intelligence,vol. 5, Edinburgh Univ. Press, 1970, pp. 153–163.[31] G.D. Plotkin, A further note on inductive generalization, in: B. Meltzer, D. Michie (Eds.), Machine Intelli-gence, vol. 6, Edinburgh Univ. Press, 1971, pp. 101–124.J. Burghardt / Artificial Intelligence 165 (2005) 1–3535[32] L. Pottier, Generalisation de termes en theorie equationelle, Cas associatif-commutatif, Report 1056, INRIA,1989.[33] J.C. Reynolds, Transformational systems and the algebraic structure of atomic formulas, in: B. Meltzer, D.Michie (Eds.), Machine Intelligence, vol. 5, Edinburgh Univ. Press, 1970, pp. 135–151.[34] Y. Sakakibara, Recent advances of grammatical inference, Theoret. Comput. Sci. 185 (1997) 15–45.[35] U. Schöning, Theoretische Informatik—kurzgefaßt, Spektrum-Hochschultaschenbuch, Heidelberg, 1997.[36] J.H. Siekmann, Universal Unification, Univ. Kaiserslautern, 1985.[37] J.W. Thatcher, J.B. Wright, Generalized finite automata theory with an application to a decision problem ofsecond-order logic, Math. Syst. Theory 2 (1) (1968).[38] T.E. Uribe, Sorted unification using set constraints, in: Proc. CADE-11, in: Lecture Notes in Comput. Sci.,vol. 607, 1992, pp. 163–177.[39] L.G. Valiant, A theory of the learnable, Comm. ACM 27 (1984) 1134–1142.[40] U. Wagner, Combinatorically restricted higher order anti-unification. Master’s thesis, TU Berlin, April 2002.