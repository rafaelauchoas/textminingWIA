Artificial Intelligence 210 (2014) 78–122Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintThe dropout learning algorithmPierre Baldi∗, Peter SadowskiDepartment of Computer Science, University of California, Irvine, CA 92697-3435, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 11 June 2013Received in revised form 18 February 2014Accepted 18 February 2014Available online 24 February 2014Keywords:Machine learningNeural networksEnsembleRegularizationStochastic neuronsStochastic gradient descentBackpropagationGeometric meanVariance minimizationSparse representationsDropout is a recently introduced algorithm for training neural networks by randomlydropping units during training to prevent their co-adaptation. A mathematical analysisof some of the static and dynamic properties of dropout is provided using Bernoulligating variables, general enough to accommodate dropout on units or connections, andwith variable rates. The framework allows a complete analysis of the ensemble averagingproperties of dropout in linear networks, which is useful to understand the non-linearcase. The ensemble averaging properties of dropout in non-linear logistic networks resultfrom three fundamental equations: (1) the approximation of the expectations of logisticfunctions by normalized geometric means, for which bounds and estimates are derived;(2) the algebraic equality between normalized geometric means of logistic functions withthe logistic of the means, which mathematically characterizes logistic functions; and (3) thelinearity of the means with respect to sums, as well as products of independent variables.The results are also extended to other classes of transfer functions, including rectifiedlinear functions. Approximation errors tend to cancel each other and do not accumulate.Dropout can also be connected to stochastic neurons and used to predict firing rates,and to backpropagation by viewing the backward propagation as ensemble averagingin a dropout linear network. Moreover, the convergence properties of dropout can beunderstood in terms of stochastic gradient descent. Finally, for the regularization propertiesof dropout, the expectation of the dropout gradient is the gradient of the correspondingapproximation ensemble, regularized by an adaptive weight decay term with a propensityfor self-consistent variance minimization and sparse representations.© 2014 The Authors. Published by Elsevier B.V.Open access under CC BY-NC-ND license.1. IntroductionDropout is a recently introduced algorithm for training neural networks [27]. In its simplest form, on each presentationof each training example, each feature detector unit is deleted randomly with probability q = 1 − p = 0.5. The remainingweights are trained by backpropagation [40]. The procedure is repeated for each example and each training epoch, shar-ing the weights at each iteration (Fig. 1.1). After the training phase is completed, predictions are produced by halving allthe weights (Fig. 1.2). The dropout procedure can also be applied to the input layer by randomly deleting some of theinput-vector components—typically an input component is deleted with a smaller probability (i.e. q = 0.2).The motivation and intuition behind the algorithm is to prevent overfitting associated with the co-adaptation of featuredetectors. By randomly dropping out neurons, the procedure prevents any neuron from relying excessively on the output ofany other neuron, forcing it instead to rely on the population behavior of its inputs. It can be viewed as an extreme form of* Corresponding author.E-mail address: pfbaldici@uci.edu (P. Baldi).http://dx.doi.org/10.1016/j.artint.2014.02.0040004-3702 © 2014 The Authors. Published by Elsevier B.V.Open access under CC BY-NC-ND license.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12279Fig. 1.1. Dropout training in a simple network. For each training example, feature detector units are dropped with probability 0.5. The weights are trainedby backpropagation (BP) and shared with all the other examples.Fig. 1.2. Dropout prediction in a simple network. At prediction time, all the weights from the feature detectors to the output units are halved.bagging [17], or as a generalization of naive Bayes [23], as well as denoising autoencoders [42]. Dropout has been reportedto yield remarkable improvements on several difficult problems, for instance in speech and image recognition, using wellknown benchmark datasets, such as MNIST, TIMIT, CIFAR-10, and ImageNet [27].In [27], it is noted that for a single unit dropout performs a kind of “geometric” ensemble averaging and this propertyis conjectured to extend somehow to deep multilayer neural networks. Thus dropout is an intriguing new algorithm forshallow and deep learning, which seems to be effective, but comes with little formal understanding and raises severalinteresting questions. For instance:1. What kind of model averaging is dropout implementing, exactly or in approximation, when applied to multiple layers?2. How crucial are its parameters? For instance, is q = 0.5 necessary and what happens when other values are used? Whathappens when other transfer functions are used?3. What are the effects of different deletion randomization procedures, or different values of q for different layers? Whathappens if dropout is applied to connections rather than units?4. What are precisely the regularization and averaging properties of dropout?5. What are the convergence properties of dropout?To answer these questions, it is useful to distinguish the static and dynamic aspects of dropout. By static we refer toproperties of the network for a fixed set of weights, and by dynamic to properties related to the temporal learning process.We begin by focusing on static properties, in particular on understanding what kind of model averaging is implementedby rules like “halving all the weights”. To some extent this question can be asked for any set of weights, regardless of thelearning stage or procedure. Furthermore, it is useful to first study the effects of droupout in simple networks, in particularin linear networks. As is often the case [8,9], understanding dropout in linear networks is essential for understandingdropout in non-linear networks.Related work. Here we point out a few connections between dropout and previous literature, without any attempt at beingexhaustive, since this would require a review paper by itself. First of all, dropout is a randomization algorithm and as suchit is connected to the vast literature in computer science and mathematics, sometimes a few centuries old, on the useof randomness to derive new algorithms, improve existing ones, or prove interesting mathematical results (e.g. [22,3,33]).80P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Second, and more specifically, the idea of injecting randomness into a neural network is hardly new. A simple Google searchyields dozen of references, many dating back to the 1980s (e.g. [24,25,30,34,12,6,37]). In these references, noise is typicallyinjected either in the input data or in the synaptic weights to increase robustness or regularize the network in an empiricalway. Injecting noise into the data is precisely the idea behind denoising autoencoders [42], perhaps the closest predecessorto dropout, as well as more recent variations, such as the marginalized-corrupted-features learning approach described in[29]. Finally, since the posting of [27], three articles with dropout in their title were presented at the NIPS 2013 conference:a training method based on overlaying a dropout binary belief network on top of a neural network [7]; an analysis of theadaptive regularizing properties of dropout in the shallow linear case suggesting some possible improvements [43]; and asubset of the averaging and regularization properties of dropout described primarily in Sections 8 and 11 of this article [10].2. Dropout for shallow linear networksIn order to compute expectations, we must associate well defined random variables with unit activities or connectionweights when these are dropped. Here and everywhere else we will consider that a unit activity or connection is set to 0when the unit or connection is dropped.2.1. Dropout for a single linear unit (combinatorial approach)We begin by considering a single linear unit computing a weighted sum of n inputs of the formS = S(I) =n(cid:2)i=1w i Ii(1)where I = (I1, . . . , In) is the input vector. If we delete inputs with a uniform distribution over all possible subsets of inputs,or equivalently with a probability q = 0.5 of deletion, then there are 2n possible networks, including the empty network.For a fixed I , the average output over all these networks can be written as:E(S) = 12n(cid:2)NS(N , I)(2)where N is used to index all possible sub-networks, i.e. all possible edge deletions. Note that in this simple case, deletionof input units or of edges are the same thing. The sum above can be expanded using networks of size 0, 1, 2, . . . , n in theform(cid:3)(cid:4)(cid:5)(cid:6) (cid:2)(cid:7)w i Ii + w j I j+ · · ·(cid:8)w i Ii+E(S) = 12n0 +n(cid:2)i=11(cid:2)i< j(cid:2)n(cid:7)(cid:6)n − 1n − 1= 2n−1In this expansion, the term w i Ii occurs(cid:7)(cid:6)(cid:7)(cid:6)1 +n − 11n − 12+ · · · +times. So finally the average output isE(S) = 2n−12n(cid:5)w i Ii=n(cid:2)i=1n(cid:2)i=1w i2Ii+(cid:4)(3)(4)(5)Thus in the case of a single linear unit, for any fixed input I the output obtained by halving all the weights is equal to thearithmetic mean of the outputs produced by all the possible sub-networks. This combinatorial approach can be applied toother cases (e.g. p (cid:3)= 0.5) but it is much easier to work directly with a probabilistic approach.2.2. Dropout for a single linear unit (probabilistic approach)Here we simply consider that the output is a random variable of the formS =n(cid:2)i=1w iδi Ii(6)where δi is a Bernoulli selector random variable, which deletes the weight w i (equivalently the input Ii ) with probabilityP (δi = 0) = qi . The Bernoulli random variables are assumed to be independent of each other (in fact pairwise independence,as opposed to global independence, is sufficient for all the results to be presented here). Thus P (δi = 1) = 1 − qi = pi . Usingthe linearity of the expectation we have immediatelyP. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122E(S) =n(cid:2)i=1w i E(δi)Ii =n(cid:2)i=1w i pi Ii81(7)This formula allows one to handle different pi for each connection, as well as values of pi that deviate from 0.5. If all theconnections are associated with independent but identical Bernoulli selector random variables with pi = p, thenE(S) =n(cid:2)i=1w i E(δ)Ii =n(cid:2)i=1w i p Ii(8)(cid:9)Thus note, for instance, that if the inputs are deleted with probability 0.2 then the expected output is given by 0.8i w i Ii .Thus the weights must be multiplied by 0.8. The key property behind Eq. (8) is the linearity of the expectation with respectto sums and multiplications by scalar values, and more generally for what follows the linearity of the expectation withrespect to the product of independent random variables. Note also that the same approach could be applied for estimatingexpectations over the input variables, i.e. over training examples, or both (training examples and subnetworks). This remainstrue even when the distribution over examples is not uniform.If the unit has a fixed bias b (affine unit), the random output variable has the formS =n(cid:2)i=1w iδi Ii + bδb(9)The case where the bias is always present, i.e. when δb = 1 always, is just a special case. And again, by linearity of theexpectationE(S) =n(cid:2)i=1w i pi Ii + bpb(10)where P (δb = 1) = pb. Under the natural assumption that the Bernoulli random variables are independent of each other, thevariance is linear with respect to the sum and can easily be calculated in all the previous cases. For instance, starting fromthe most general case of Eq. (9) we haveVar(S) =n(cid:2)i=1w 2i Var(δi)I 2i+ b2 Var(δb) =n(cid:2)i=1w 2i piqi I 2i+ b2 pbqb(11)with qi = 1 − pi . S can be viewed as a weighted sum of independent Bernoulli random variables, which can be approximatedby a Gaussian random variable under reasonable assumptions.2.3. Dropout for a single layer of linear unitsWe now consider a single linear layer with k output unitsS i(I) =n(cid:2)j=1w i j I jfor i = 1, . . . , k(12)In this case, dropout applied to input units is slightly different from dropout applied to the connections. Dropout applied tothe input units leads to the random variablesS i(I) =n(cid:2)j=1w i jδ j I jfor i = 1, . . . , kwhereas dropout applied to the connections leads to the random variablesS i(I) =n(cid:2)j=1δi j w i j I jfor i = 1, . . . , k(13)(14)In either case, the expectations, variances, and covariances can easily be computed using the linearity of the expectationand the independence assumption. When dropout is applied to the input units, we get:E(S i) =n(cid:2)j=1w i j p j I jfor i = 1, . . . , k(15)82P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Var(S i) =n(cid:2)j=1w 2i j p jq j I 2jfor i = 1, . . . , kCov(S i, Sl) =n(cid:2)j=1w i j wlj p jq j I 2jfor 1 (cid:2) i < l (cid:2) kWhen dropout is applied to the connections, we get:E(S i) =n(cid:2)j=1w i j pi j I jfor i = 1, . . . , kVar(S i) =n(cid:2)j=1w 2i j pi jqi j I 2jfor i = 1, . . . , kCov(S i, Sl) = 0 for 1 (cid:2) i < l (cid:2) k(16)(17)(18)(19)(20)Note the difference in covariance between the two models. When dropout is applied to the connections, S i and Sl areentirely independent.3. Dropout for deep linear networksIn a general feedforward linear network described by an underlying directed acyclic graph, units can be organized intolayers using the shortest path from the input units to the unit under consideration. The activity in unit i of layer h can beexpressed as:i (I) =Sh(cid:2)(cid:2)l<hjwhli j Slj with S 0j= I j(21)Again, in the general case, dropout applied to the units is slightly different from dropout applied to the connections. Dropoutapplied to the units leads to the random variables=Shi(cid:2)(cid:2)l<hjwhli j δlj Slj with S 0j= I jwhereas dropout applied to the connections leads to the random variables=Shi(cid:2)(cid:2)l<hji j whlδhli j Slj with S 0j= I j(22)(23)When dropout is applied to the units, assuming that the dropout process is independent of the unit activities or theweights, we get:(cid:10)(cid:11)(cid:2)EShi=(cid:2)whli j plj E(cid:11)(cid:10)Sljfor h > 0(24)l<hjj ) = I j in the input layer. This formula can be applied recursively across the entire network, starting from thewith E(S 0input layer. Note that the recursion of Eq. (24) is formally identical to the recursion of backpropagation suggesting the useof dropout during the backward pass. This point is elaborated further at the end of Section 10. Note also that although theexpectation E(Shi ) is taken over all possible subnetworks of the original network, only the Bernoulli gating variables in theprevious layers (l < h) matter. Therefore it coincides also with the expectation taken over only all the induced subnetworksof node i (comprising only nodes that are ancestors of node i).Remarkably, using these expectations, all the covariances can also be computed recursively from the input layer to theoutput layer, by writing Cov(Sh(cid:4)i , Shi(cid:4) ) = E(Sh(cid:2)i Sh(cid:2)(cid:4)i(cid:4) ) − E(Shi )E(Sh(cid:13)i(cid:4) ) and computing(cid:2)(cid:2)(cid:2)(cid:2)(cid:12)(cid:2)(cid:2)(cid:11)(cid:10)E(cid:4)i ShShi(cid:4)= E(cid:4)(cid:4)lwhi(cid:4) j(cid:4) δl(cid:4)(cid:4)j(cid:4) Slj(cid:4)=whli j wh(cid:4)(cid:4)li(cid:4) j(cid:4) E(cid:10)(cid:4)jδlδlj(cid:4)(cid:11)(cid:10)E(cid:4)j SlSlj(cid:4)(cid:11)(25)l<hjl(cid:4)<h(cid:4)j(cid:4)l<hl(cid:4)<h(cid:4)jj(cid:4)under the usual assumption that δlindependent when l (cid:3)= lthe usual independence assumptions, E(Shj(cid:4) is independent of Sljδl, we have in this case E(δl(cid:4)j(cid:4) . Furthermore, under the usual assumption that δlj(cid:4) ) = plj) = pli(cid:4) ) can be computed recursively from the values of E(Slj(cid:4) , with furthermore E(δlj(cid:4) arej . Thus in short underj Slj(cid:4) ) in lower layers,j and δlor j (cid:3)= jj Sljδli Shj pljδl(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)whli j δlj Slj(cid:4)(cid:4)(cid:4)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12283with the boundary conditions E(Ii I j) = Ii I j for a fixed input vector (layer 0). The recursion proceeds layer by layer, fromthe input to the output layer. When a new layer is reached, the covariances to all the previously visited layers must becomputed, as well as all the intralayer covariances.When dropout is applied to the connections, under similar independence assumptions, we get:(cid:2)(cid:2)(cid:11)(cid:10)EShi=i j whlphli j E(cid:11)(cid:10)Sljfor h > 0l<hj(26)j ) = I jwith E(S 0in the input layer. This formula can be applied recursively across the entire network. Note again thatalthough the expectation E(Shi ) is taken over all possible subnetworks of the original network, only the Bernoulli gatingvariables in the previous layers (l < h) matter. Therefore it is also the expectation taken over only all the induced subnet-works of node i (corresponding to all the ancestors of node i). Furthermore, using these expectations, all the covariancescan also be computed recursively from the input layer to the output layer using a similar analysis to the one given abovefor the case of dropout applied to the units of a general linear network.In summary, for linear feedforward networks the static properties of dropout applied to the units or the connections using Bernoulligating variables that are independent of the weights, of the activities, and of each other (but not necessarily identically distributed) canbe fully understood. For any input, the expectation of the outputs over all possible networks induced by the Bernoulli gating variablesis computed using the recurrence equations (24) and (26), by simple feedforward propagation in the same network where each weightis multiplied by the appropriate probability associated with the corresponding Bernoulli gating variable. The variances and covariancescan also be computed recursively in a similar way.4. Dropout for shallow neural networksWe now consider dropout in non-linear networks that are shallow, in fact with a single layer of weights.4.1. Dropout for a single non-linear unit (logistic)Here we consider that the output of a single unit with total linear input S is given by the logistic sigmoidal functionO = σ (S) =11 + ce−λS(27)Here and everywhere else, we must have c (cid:3) 0 There are 2n possible sub-networks indexed by N and, for a fixed input I ,each sub-network produces a linear value S(N , I) and a final output value O N = σ (N ) = σ (S(N , I)). Since I is fixed, weomit the dependence on I in all the following calculations. In the uniform case, the geometric mean of the outputs is givenbyG =(cid:14)N1/2nNOLikewise, the geometric mean of the complementary outputs (1 − O N ) is given by(cid:14)(1 − O N )1/2n(cid:4) =GNThe normalized geometric mean (NGM) is defined byNGM = GG + G(cid:4)The NGM of the outputs is given by(cid:10)(cid:11)O (N )NGM=(cid:15)[(cid:15)[N σ (S(N ))]1/2n + [N σ (S(N ))]1/2n(cid:15)N (1 − σ (S(N )))]1/2n=(cid:15)1 + [11−σ (S(N ))σ (S(N ))N]1/2nNow for the logistic function σ , we have1 − σ (x)σ (x)= ce−λxApplying this identity to Eq. (31) yields(cid:10)(cid:11)O (N )=NGM(cid:15)1 + [1N ce−λS(N )]1/2n=1 + c[e−λ1(cid:9)N S(N )/2n ](cid:10)(cid:11)E(S)= σ(28)(29)(30)(31)(32)(33)(35)(36)(37)(38)(39)84P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122(cid:9)where here E(S) =(cid:11)σ (S)NGM(cid:10)(cid:11)E(S)= σN S(N )/2n. Or, in more compact form,(cid:10)(34)Thus with a uniform distribution over all possible sub-networks N , equivalent to having i.i.d. input unit selector variablesδ = δi with probability pi = 0.5, the NGM is simply obtained by keeping the same overall network but dividing all the(cid:9)weights by two and applying σ to the expectation E(S) =ni=1It is essential to observe that this result remains true in the case of a non-uniform distribution over the subnetworks N ,such as the distribution generated by Bernoulli gating variables that are not identically distributed, or with p (cid:3)= 0.5. Forthis we consider a general distribution P (N ). This is of course even more general than assuming the P is the product of nindependent Bernoulli selector variables. In this case, the weighted geometric means are defined by:w i2 Ii .(cid:14)G =O P (N )NNand(cid:14)(1 − O N )P (N )(cid:4) =GNand similarly for the normalized weighted geometric mean (NWGM)NWGM = GG + G(cid:4)Using the same calculation as above in the uniform case, we can then compute the normalized weighted geometric meanNWGM in the form(cid:10)(cid:11)O (N )=(cid:15)NWGM(cid:10)(cid:11)O (N )=NWGM(cid:9)(cid:15)N σ (S(N ))P (N )(cid:15)N σ (S(N ))P (N ) +1N ( 1−σ (S(N ))σ (S(N )(cid:15)1 +)P (N )N (1 − σ (S(N )))P (N )=1 + ce−λ1(cid:9)N P (N )S(N )(cid:10)(cid:11)E(S)= σN P (N )S(N ). Thus in summary with any distribution P (N ) over all possible sub-networks N , including thewhere here E(S) =case of independent but not identically distributed input unit selector variables δi with probability pi , the NWGM is simply obtainedby applying the logistic function to the expectation of the linear input S. In the case of independent but not necessarily identicallydistributed selector variables δi , each with a probability pi of being equal to one, the expectation of S can be computed simply bykeeping the same overall network but multiplying each weight w i by pi so that E(S) =(cid:9)ni=1 pi w i Ii .Note that as in the linear case, this property of logistic units is even more general. That is for any set of S 1, . . . , Sm andi=1 P i = 1) and associated outputs O 1, . . . , O m (with O = σ (S)), weany associated probability distribution P 1, . . . , P m (have NWGM(O ) = σ (E) = σ (i P i S i). Thus the NVGM can be computed over inputs, over inputs and subnetworks, or overother distributions than the one associated with subnetworks, even when the distribution is not uniform. For instance, ifwe add Gaussian or other noise to the weights, the same formula can be applied. Likewise, we can approximate the averageactivity of an entire neuronal layer, by applying the logistic function to the average input of the neurons in that layer, aslong as all the neurons in the layer use the same logistic function. Note also that the property is true for any c and λand therefore, using the analyses provided in the next sections, it will be applicable to each of the units, in a networkwhere different units have different values of c and λ. Finally, the property is even more general in the sense that the samecalculation as above shows that for any function f(cid:9)(cid:9)m(cid:10)(cid:10)σ(cid:11)(cid:11)(cid:10)(cid:10)(cid:11)(cid:11)f (S)= σEf (S)NWGMand in particular, for any k(cid:10)(cid:10)(cid:11)(cid:11)(cid:10)NWGMσSk= σ(cid:11)(cid:11)(cid:10)ESk4.2. Dropout for a single layer of logistic unitsIn the case of a single output layer of k logistic functions, the network computes k linear sums S i =i = 1, . . . , k and then k outputs of the form(40)(41)(cid:9)nj=1 w i j I j forO i = σi(S i)(42)The dropout procedure produces a subnetwork M = (N1, . . . , Nk) where Ni here represents the corresponding sub-networkassociated with the i-th output unit. For each i, there are 2n possible sub-networks for unit i, so there are 2kn possiblesubnetworks M. In this case, Eq. (39) holds for each unit individually. If dropout uses independent Bernoulli selectorvariables δi j on the edges, or more generally, if the sub-networks (N1, . . . , Nk) are selected independently of each other,then the covariance between any two output units is 0. If dropout is applied to the input units, then the covariance betweentwo sigmoidal outputs may be small but non-zero.4.3. Dropout for a set of normalized exponential unitsP. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12285We now consider the case of one layer of normalized exponential units. In this case, we can think of the network asj=1 w i j I j for i = 1, . . . , k and then k outputshaving k outputs obtained by first computing k linear sums of the form S i =of the form(cid:9)nO i =eλS i(cid:9)kj=1 eλS j=(cid:9)1 + (1j(cid:3)=i eλS j )e−λS i(43)Thus O i is a logistic output but the coefficients of the logistic function depend on the values of S j for j (cid:3)= i. The dropoutprocedure produces a subnetwork M = (N1, . . . , Nk) where Ni represents the corresponding sub-network associated withthe i-th output unit. For each i, there are 2n possible sub-networks for unit i, so there are 2kn possible subnetworks M.We assume first that the distribution P (M) is factorial, that is P (M) = P (N1) . . . P (Nk), equivalent to assuming that thesubnetworks associated with the individual units are chosen independently of each other. This is the case when usingindependent Bernoulli selector applied to the connections. The normalized weighted geometric average of output unit i isgiven byNWGM(O i) =(cid:15)M((cid:15)(cid:9)kl=1eλSi (N(cid:9)kj=1 ei )λS j (Nj ) )P (M)M(eλSl (Nl )(cid:9)λS j (Nkj=1 ej ) )P (M)Simplifying by the numeratorNWGM(O i) =i ) )P (M)Factoring and collecting the exponential terms gives1 +(cid:9)kl=1,l(cid:3)=i(cid:15)1M( eλSl (Nl )eλSi (NNWGM(O i) =(cid:9)1 + e−M λP (M)S i(Ni )1(cid:9)kl=1,l(cid:3)=i e(cid:9)M λP (M)Sl(Nl)(44)(45)(46)1 + e−λE(S i )NWGM(O i) =1(cid:9)kl=1,l(cid:3)=i eλE(Sl)= eλE(S i )(cid:9)kl=1 eλE(Sl)Thus with any distribution P (N ) over all possible sub-networks N , including the case of independent but not identically distributedinput unit selector variables δi with probability pi , the NWGM of a normalized exponential unit is obtained by applying the normalizedexponential to the expectations of the underlying linear sums S i . In the case of independent but not necessarily identically distributedselector variables δi , each with a probability pi of being equal to one, the expectation of S i can be computed simply by keeping thesame overall network but multiplying each weight w i by pi so that E(S i) =(cid:9)nj=1 p j w i I j .(47)5. Dropout for deep neural networksFinally, we can deal with the most interesting case of deep feedforward networks of sigmoidal units,1 described by a setof equations of the formO hi= σ hi(cid:11)(cid:10)Shi= σ(cid:6)(cid:2)(cid:2)l<hj(cid:7)with O 0j= I jwhli j O ljDropout on the units can be described byO hi= σ hi(cid:11)(cid:10)Shi= σ(cid:6)(cid:2)(cid:2)l<hj(cid:7)with O 0j= I jwhli j δlj O ljusing the selector variables δlj and similarly for dropout on the connections. For each sigmoidal unitNWGM(cid:10)(cid:11)O hi=(cid:15)(cid:15)i )P (N )N (O h(cid:15)N (1 − O hi )P (N )N (O hi )P (N ) +1 Given the results of the previous sections, the network can also include linear units or normalized exponential units.(48)(49)(50)86P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122and the basic idea is to approximate expectations by the corresponding NWGMs, allowing the propagation of the expectationsymbols from outside the sigmoid symbols to inside.(cid:10)(cid:16)σE(cid:11)(cid:17)S(N , I)≈ NWGM(cid:16)(cid:17)O (N , I)(cid:10)(cid:16)E= σ(cid:17)(cid:11)S(N , I)More precisely, we have the following recursion:(cid:11)(cid:10)EO hiNWGM(cid:11)(cid:10)EShi=≈ NWGM(cid:11)(cid:10)= σ hO hii(cid:2)(cid:2)(cid:10)(cid:11)O hi(cid:10)(cid:16)(cid:11)(cid:17)ShEii j plj Ewhl(cid:11)(cid:10)O lj(51)(52)(53)(54)l<hjEqs. (52), (53), and (54) are the fundamental equations underlying the recursive dropout ensemble approximation in deep neuralnetworks. The only direct approximation in these equations is of course Eq. (52) which will be discussed in more depth in Sectionsi are identical over all possible subnetworks N . However, even when8 and 9. This equation is exact if and only if the numbers O hthe numbers O hi are not identical, the normalized weighted geometric mean often provides a good approximation. If the networkcontains linear units, then Eq. (52) is not necessary for those units and their average can be computed exactly. The onlyfundamental assumption for Eq. (54) is independence of the selector variables from the activity of the units or the value ofthe weights so that the expectation of the product is equal to the product of the expectations. Under the same conditions,the same analysis can be applied to dropout gating variables applied to the connections or, for instance, to Gaussian noiseadded to the unit activities.Finally, we measure the consistency C(O hi , I) of neuron i in layer h for input I by the variance Var[O hi (I)] taken over allsubnetworks N and their distribution when the input I is fixed. The larger the variance is, the less consistent the neuron is,and the worse we can expect the approximation in Eq. (52) to be. Note that for a random variable O in [0, 1] the varianceis bound to be small anyway, and cannot exceed 1/4. This is because Var(O ) = E(O 2) − (E(O ))2 (cid:2) E(O ) − (E(O ))2 =E(O )(1 − E(O )) (cid:2) 1/4. The overall input consistency of such a neuron can be defined as the average of C(O hi , I) taken overall training inputs I , and similar definitions can be made for the generalization consistency by averaging C(O hi , I) over ageneralization set.Before examining the quality of the approximation in Eq. (52), we study the properties of the NWGM for aver-aging ensembles of predictors, as well as the classes of transfer functions satisfying the key dropout NWGM relation(NWGM( f (x)) = f (E(x))) exactly, or approximately.6. Ensemble optimization propertiesThe weights of a neural network are typically trained by gradient descent on the error function computed using theoutputs and the corresponding targets. The error functions typically used are the squared error in regression and the relativeentropy in classification. Considering a single example and a single output O with a target t, these errors functions can bewritten as:Error(O , t) = 12(t − O )2and Error(O , t) = −t log O − (1 − t) log(1 − O )(55)Extension to multiple outputs, including classification with multiple classes using normalized exponential transfer functions,is immediate. These error terms can be summed over examples or over predictors in the case of an ensemble. Both errorfunctions are convex up (∪) and thus a simple application of Jensen’s theorem shows immediately that the error of anyensemble average is less than the average error of the ensemble components. Thus in the case of any ensemble producingoutputs O 1, . . . , O m and any convex error function we have(cid:2)(cid:6)(cid:2)(cid:7)Errorpi O i, t(cid:2)pi Error(O i, t) or Error(E) (cid:2) E(Error)iiNote that this is true for any individual example and thus it is also true over any set of examples, even when these are notidentically distributed. Eq. (56) is the key equation for using ensembles and for averaging them arithmetically.In the case of dropout with a logistic output unit the previous analyses show that the NWGM is an approximation toE and on this basis alone it is a reasonable way of combining the predictors in the ensemble of all possible subnetworks.However the following stronger result holds. For any convex error function, both the weighted geometric mean WGM andits normalized version NWGM of an ensemble possess the same qualities as the expectation. In other words:(cid:6)(cid:14)(cid:7)Errori(cid:6)Error(cid:15)Opii , t(cid:2)(cid:15)i O(cid:15)(cid:2)ipiii Opii+i(1 − O i)pipi Error(O i, t) or Error(WGM) (cid:2) E(Error)(cid:7), t(cid:2)(cid:2)ipi Error(O i, t) or Error(NWGM) (cid:2) E(Error)(56)(57)(58)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12287In short, for any convex error function, the error of the expectation, weighted geometric mean, and normalized weighted geometricmean of an ensemble of predictors is always less than the expected error.fis convex and g is increasing, then the composition f (g) is convex. This is easily shown by di-Proof. Recall that ifrectly applying the definition of convexity (see [39,16] for additional background on convexity). Eq. (57) is obtained byapplying Jensen’s inequality to the convex function Error(g), where g is the increasing function g(x) = ex, using the pointslog O 1, . . . , log O m. Eq.(58)is obtained by applying Jensen’s inequality to the convex function Error(g), where g is the in-creasing function g(x) = ex/(1 + ex), using the points log O 1 − log(1 − O 1), . . . , log O m − log(1 − O m). The cases where someof the O i are equal to 0 or 1 can be handled directly, although these are irrelevant for our purposes since the logistic outputcan never be exactly equal to 0 or 1.Thus in circumstances where the final output is equal to the weighted mean, weighted geometric mean, or normal-ized weighted geometric mean of an underlying ensemble, Eqs. (56), (57), or (58) apply exactly. This is the case, forinstance, of linear networks, or non-linear networks where dropout is applied only to the output layer with linear, logistic,or normalized-exponential units.Since dropout approximates expectations using NWGMs, one may be concerned by the errors introduced by such ap-proximations, especially in a deep architecture when dropout is applied to multiple layers. It is worth noting that the resultabove can be used at least to “shave off” one layer of approximations by legitimizing the use of NWGMs to combine modelsin the output layer, instead of the expectation. Similarly, in the case of a regression problem, if the output units are linearthen the expectations can be computed exactly at the level of the output layer using the results above on linear networks,thus reducing by one the number of layers where the approximation of expectations by NWGMs must be carried. Finally,as shown below, the expectation, the WGM, and the NWGM are relatively close to each other and thus there is some flex-ibility, hence some robustness in how predictors are combined in an ensemble, in the sense that combining models withapproximations to these quantities may still outperform the expectation of the error of the individual models.Finally, it must also be pointed out that in the prediction phase once can also use expected values, estimated at somecomputational cost using Monte Carlo methods, rather than approximate values obtained by forward propagation in thenetwork with modified weights.7. Dropout functional classes and transfer functions7.1. Dropout functional classesDropout seems to rely on the fundamental property of the logistic sigmoidal function NWGM(σ ) = σ (E). Thus it isnatural to wonder what is the class of functions f satisfying this property. Here we show that the class of functions f definedon the real line with range in [0, 1] and satisfyingGG + G(cid:4) ( f ) = f (E)(59)for any set of points and any distribution, consists exactly of the union of all constant functions f (x) = K with 0 (cid:2) K (cid:2) 1 and all−λx). As a reminder, G denotes the geometric mean and Glogistic functions f (x) = 1/(1 + cedenotes the geometric meanof the complements. Note also that all the constant functions with f (x) = K with 0 (cid:2) K (cid:2) 1 can also be viewed as logisticfunctions by taking λ = 0 and c = (1 − K )/K (K = 0 is a limiting case corresponding to c → ∞).(cid:4)Proof. To prove this result, note first that the [0, 1] range is required by the definitions of G and G, since these imposethat f (x) and 1 − f (x) be positive. In addition, any function f (x) = K with 0 (cid:2) K (cid:2) 1 is in the class and we have shownthat the logistic functions satisfy the property. Thus we need only to show these are the only solutions.By applying Eq. (59) to pairs of arguments, for any real numbers u and v with u (cid:2) v and any real number 0 (cid:2) p (cid:2) 1,(cid:4)any function in the class must satisfy:f (u)p f (v)1−pf (u)p f (v)1−p + (1 − f (u))p(1 − f (v))1−p(cid:10)= fpu + (1 − p)v(cid:11)(60)Note that if f (u) = f (v) then the function f must be constant over the entire interval [u, v]. Note also that if f (u) = 0 andf (v) > 0 then f = 0 in [u, v). As a result, it is impossible for a non-zero function in the class to satisfy f (u) = 0, f (v 1) > 0,and f (v 2) > 0. Thus if a function fin the class is not constantly equal to 0, then f > 0 everywhere. Similarly (and bysymmetry), if a function fin the class is not constantly equal to 1, then f < 1 everywhere.Consider now a function fwhere. Eq. (60) shows that on any interval [u, v] fthis interval, by letting x = pu + (1 − p)v or equivalently p = (v − x)/(v − u) the function is given byin the class, different from the constant 0 or constant 1 function so that 0 < f < 1 every-is completely defined by at most two parameters f (u) and f (v). Onf (x) =(cid:10)1 +1− f (u)f (u)1(cid:11) v−xv−u(cid:10)(cid:11) x−uv−u1− f (v)f (v)(61)88orwithandP. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122f (x) =11 + ce−λx(cid:6)c =1 − f (u)f (u)(cid:7) vv−u(cid:6)1 − f (v)f (v)(cid:7) −uv−u(cid:6)λ = 1v − ulog1 − f (u)f (u)f (v)1 − f (v)(cid:7)Note that a particular simple parameterization is given in terms off (0) = 11 + candf (x) = 12for x = log cλ(62)(63)(64)(65)[As a side note, another elegant formula is obtained from Eq. (60) for f (0) by taking u = −v and p = 0.5. Simple algebraicmanipulations give:1 − f (0)f (0)=(cid:6)1 − f (−v)f (−v)(cid:6)(cid:7) 12(cid:7) 121 − f (v)f (v)(66).] As a result, on any interval [u, v] the function f must be: (1) continuous, hence uniformly continuous; (2) differentiable,in fact infinitely differentiable; (3) monotone increasing or decreasing, and strictly so if fis constant; (4) and therefore fmust have well defined limits at −∞ and +∞. It is easy to see that the limits can only be 0 or 1. For instance, for the limitat +∞, let u = 0 and v(cid:4) = α v, with 0 < α < 1 so that v(cid:4) → ∞ as v → ∞. Then(cid:11)(cid:4)(cid:10)fv=(cid:10)1 +1− f (0)f (0)11−α(cid:10)(cid:11)(cid:11)α1− f (v)f (v)(67)As v(cid:4) → ∞ the limit must be independent of α and therefore the limit f (v) must be 0 or 1.Finally, consider u1 < u2 < u3. By the above results, the quantities f (u1) and f (u2) define a unique logistic functionon [u1, u2], and similarly f (u2) and f (u3) define a unique logistic function on [u2, u3]. It is easy to see that these twologistic functions must be identical either because of the analycity or just by taking two new points v 1 and v 2 withu1 < v 1 < u2 < v 2 < u3. Again f (v 1) and f (v 2) define a unique logistic function on [v 1, v 2] which must be identical to theother two logistic functions on [v 1, u2] and [u2, v 2] respectively. Thus the three logistic functions above must be identical.In short, f (u) and f (v) define a unique logistic function inside [u, v], with the same unique continuation outside of [u, v].From this result, one may incorrectly infer that dropout is brittle and overly sensitive to the use of logistic non-linearfunctions. This conclusion is erroneous for several reasons. First, the logistic function is one of the most important andwidely used transfer functions in neural networks. Second, regarding the alternative sigmoidal function tanh(x), if wetranslate it upwards and normalize it so that its range is the [0, 1] interval, then it reduces to a logistic function since−2x). This leads to the formula: NWGM((1 + tanh(x))/2) = (1 + tanh(E(x)))/2. Note also that the(1 + tanh(x))/2 = 1/(1 + eNWGM approach cannot be applied directly to tanh, or any other transfer function which assumes negative values, since Gand NWGM are defined for positive numbers only. Third, even if one were to use a different sigmoidal function, such as1 + x2, when rescaled to [0, 1] its deviations from the logistic function may be small and lead to fluctu-arctan(x) or x/ations that are in the same range as the fluctuations introduced by the approximation of E by NWGM. Fourth and mostimportantly, dropout has been shown to work empirically with several transfer functions besides the logistic, including forinstance tanh and rectified linear functions. This point is addressed in more detail in the next section. In any case, forall these reasons one should not be overly concerned by the superficially fragile algebraic association between dropout,NWGMs, and logistic functions.√7.2. Dropout transfer functionsIn deep learning, one is often interested in using alternative transfer functions, in particular rectified linear functionswhich can alleviate the problem of vanishing gradients during backpropagation. As pointed out above, for any transferfunction it is always possible to compute the ensemble average at prediction time using sampling. However, we can showthat the ensemble averaging property of dropout is preserved to some extent also for rectified linear transfer functions, aswell for broader classes of transfer functions.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12289To see this, we first note that, while the properties of the NWGM are useful for logistic transfer functions, the NWGMis not needed to enable the approximation of the ensemble average by deterministic forward propagation. For any transferfunction f , what is really needed is the relation(cid:10)(cid:11)f (S)E(cid:10)(cid:11)E(S)≈ f(68)Any transfer function satisfying this property can be used with dropout and allow the estimation of the ensemble atprediction time by forward propagation. Obviously linear functions satisfy Eq. (68) and this was used in the previous sectionson linear networks. A rectified linear function RL(S) with threshold t and slope λ has the form(cid:18)RL(S) =if S (cid:2) t0λS − λt otherwise(69)and is a special case of a piece-wise linear function. Eq. (68) is satisfied within each linear portion and will be satisfiedaround the threshold if the variance of S is small. Everything else being equal, smaller value of λ will also help the approx-imation. To see this more formally, assume without any loss of generality that t = 0. It is also reasonable to assume that Sis approximately normal with mean μS and variance σ 2S —a treatment without this assumption is given in Appendix A. Inthis case,(cid:10)RL(cid:11)E(S)= RL(μS ) =(70)(cid:18)if μS (cid:2) 00λμS otherwiseOn the other hand,(cid:10)(cid:11)RL(S)E=+∞(cid:19)λS01√2πσS− (S−μS )22σ 2Se+∞(cid:19)dS = λ(σS u + μS )− μSσS1√2πe− u22 duand thus(cid:10)(cid:11)RL(S)E= λμS Φ(cid:7)(cid:6)μSσS+ λσ√2π− μ2S2σ 2Sewhere Φ is the cumulative distribution of the standard normal distribution. It is well known that Φ satisfies1 − Φ(x) ≈ 1√2π1x− x22ewhen x is large. This allows us to estimate the error in all the cases. If μS = 0 we have(cid:20)(cid:20)E(cid:10)(cid:11)RL(S)(cid:10)− RLE(S)(cid:11)(cid:20)(cid:20) = λσ√2π(71)(72)(73)(74)and the error in the approximation is small and directly proportional to λ and σ . If μS < 0 and σS is small, so that |μS |/σSis large, then Φ(μS /σS ) ≈ 1√2π(cid:11)(cid:20)(cid:11)(cid:20) ≈ 0RL(S)(cid:10)− RLσS|μS | eS andS /2σ 2E(S)(cid:20)(cid:20)E−μ2(75)(cid:10)And similarly for the case when μS > 0 and σS is small, so that μS /σS is large. Thus in all these cases Eq. (68) holds. As weshall see in Section 11, dropout tends to minimize the variance σS and thus the assumption that σ be small is reasonable.Together, these results show that the dropout ensemble approximation can be used with rectified linear transfer functions. It is alsopossible to model a population of RL neurons using a hierarchical model where the mean μS is itself a Gaussian random variable. Inthis case, the error E(RL(S)) − RL(E(S)) is approximately Gaussian distributed around 0. [This last point will become relevant inSection 9.]More generally, the same line of reasoning shows that the dropout ensemble approximation can be used with piece-wise lineartransfer functions as long as the standard deviation of S is small relative to the length of the linear pieces. Having small angles betweensubsequent linear pieces also helps strengthen the quality of the approximation.Furthermore any continuous twice-differentiable function with small second derivative (curvature) can be robustly approximatedby a linear function locally and therefore will tend to satisfy Eq. (68), provided the variance of S is small relative to the curvature.In this respect, a rectified linear transfer function can be very closely approximated by a twice-differentiable function byusing the integral of a logistic function. For the standard rectified linear transfer function, we haveS(cid:19)S(cid:19)RL(S) ≈=σ (x) dx =−∞−∞11 + e−λxdx(76)With this approximation, the second derivative is given by σ (cid:4)(S) = λσ (S)(1 − σ (S)) which is always bounded by λ/4.90P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Finally, for the most general case, the same line of reasoning, shows that the dropout ensemble approximation can be used withany continuous, piece-wise twice differentiable, transfer function provided the following properties are satisfied: (1) the curvature ofeach piece must be small; (2) σS must be small relative to the curvature of each piece. Having small angles between the left and righttangents at each junction point also helps strengthen the quality of the approximation. Note that the goal of dropout training isprecisely to make σS small, that is to make the output of each unit robust, independent of the details of the activities ofthe other units, and thus roughly constant over all possible dropout subnetworks.8. Weighted arithmetic, geometric, and normalized geometric means and their approximation propertiesm(cid:9)To further understand dropout, one must better understand the properties and relationships of the weighted arithmetic,geometric, and normalized geometric means and specifically how well the NWGM of a sigmoidal unit approximates itsexpectation (E(σ ) ≈ NWGMS(σ )). Thus consider that we have m numbers O 1, . . . , O m with corresponding probabilitiesi=1 P i = 1). We typically assume that the m numbers satisfy 0 < O i < 1 although this is not always necessaryP 1, . . . , P m (for the results below. Cases where some of the O i are equal to 0 or 1 are trivial and can be examined separately. The case ofinterest of course is when the m numbers are the outputs of a sigmoidal unit of the form O (N ) = σ (S(N )) for a given inputI = (I1, . . . , In). We let E be the expectation (weighted arithmetic mean) E =mi=1 P i O i and G be the weighted geometrici=1 P i(1 − O i) be the expectation of the complements, andmean G =. When 0 (cid:2) O i (cid:2) 1 we also let E(cid:15)(cid:4) = 1 − E. The normalizedG(cid:4)). We also let V = Var(O ). We then have the following properties.weighted geometric mean is given by NWGM = G/(G + Gi=1(1 − O i)P i be the weighted geometric mean of the complements. Obviously we have Ei=1 O P i(cid:4) =(cid:4) =(cid:9)(cid:9)(cid:15)mmmi1. The weighted geometric mean is always less or equal to the weighted arithmetic meanG (cid:2) Eand G(cid:4) (cid:2) E(cid:4)(77)with equality if and only if all the numbers O i are equal. This is true regardless of whether the number O i are boundedby one or not. This results immediately from Jensen’s inequality applied to the logarithmic function. Although notdirectly used here, there are interesting bounds for the approximation of E by G, often involving the variance, such as:12 maxi O iVar(O ) (cid:2) E − G (cid:2)12 mini O iVar(O )(78)with equality only if the O i are all equal. This inequality was originally proved by Cartwright and Field [20]. Severalrefinements, such asmaxi O i − G2 maxi O iVar(O ) (cid:2) E − G (cid:2)Var(O )(79)pi(O i − G)2 (cid:2) E − G (cid:2)pi(O i − G)2mini O i − G2 mini O i(mini O i − E)1(cid:2)2 mini O ii(cid:2)12 maxi O iias well as other interesting bounds can be found in [4,5,31,32,1,2].2. Since G (cid:2) E and G(cid:4)) with equality if and only if all thenumbers O i are equal. Thus the weighted geometric mean is always less or equal to the normalized weighted geometricmean.(cid:4) (cid:2) 1, and thus G (cid:2) G/(G + G(cid:4) = 1 − E, we have G + G(cid:4) (cid:2) E3. If the numbers O i satisfy 0 < O i (cid:2) 0.5 (consistently low), thenand therefore G (cid:2) G(cid:2) EGG(cid:4)(cid:2) EE(cid:4)G + G(cid:4)(80)(81)[Note that if O i = 0 for some i with pi (cid:3)= 0, then G = 0 and the result is still true.] This is easily proved using Jensen’sinequality and applying it to the function ln x − ln(1 − x) for x ∈ (0, 0.5]. It is also known as the Ky Fan inequality[11,35,36] which can also be viewed as a special case of the Levinson’s inequality [28]. In short, in the consistently lowcase, the normalized weighted geometric mean is always less or equal to the expectation and provides a better approximationof the expectation than the geometric mean. We will see in a later section why the consistently low case is particularlysignificant for dropout.4. If the numbers O i satisfy 0.5 (cid:2) O i < 1 (consistently high), then(cid:4)GG(cid:4)(cid:2) EEand thereforeGG + G(cid:4)(cid:3) E(82)(cid:4) = 0 and the result is still true. In short, the normalized weightedNote that if O i = 1 for some i with pi (cid:3)= 0, then Ggeometric mean is greater or equal to the expectation. The proof is similar to the previous case, interchanging x and1 − x.5. Note that if G/(G + G(cid:4)) underestimates E then G(cid:4)/(G + G(cid:4)) overestimates 1 − E, and vice versa.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12291Fig. 8.1. The curve associated with the approximate bound |E − NWGM| (cid:3) E(1 − E)|1 − 2E|/[1 − 2E(1 − E)] (Eq. (87)).6. This is the most important set of properties. When the numbers O i satisfy 0 < O i < 1, to a first order of approximationwe haveG ≈ EandGG + G(cid:4)≈ Eand E − G ≈(cid:20)(cid:20)(cid:20)E − G(cid:20)G + G(cid:4)(cid:20)(cid:20)(cid:20)(cid:20)(83)Thus to a first order of approximation the WGM and the NWGM are equally good approximations of the expectation.However the results above, in particular property 3, lead one to suspect that the NWGM may be a better approximation,and that bounds or estimates ought to be derivable in terms of the variance. This can be seen by taking a second orderapproximation, which givesG ≈ E − V and G(cid:4) ≈ 1 − E − V andGG + G(cid:4)≈ E − V1 − 2Vand(cid:4)GG + G(cid:4)≈ 1 − E − V1 − 2Vwith the differencesE − G ≈ V ,1 − E − G(cid:4) ≈ V ,E − GG + G(cid:4)≈ V (1 − 2E)1 − 2V,andV (1 − 2E)1 − 2V(cid:2) Vand 1 − E − G(cid:4)G + G(cid:4)≈ V (2E − 1)1 − 2V(84)(85)(86)The difference |E − NWGM| is small to a second order of approximation and over the entire range of values of E. Thisis because either E is close to 0.5 and then the term 1 − 2E is small, or E is close to 0 or 1 and then the term V issmall. Before we provide specific bounds for the difference, note also that if E < 0.5 the second order approximation tothe NWGM is below E, and vice versa when E > 0.5.Since V (cid:2) E(1 − E), with equality achieved only for 0–1 Bernoulli variables, we have(cid:20)(cid:20)(cid:20)E − G(cid:20)G + G(cid:4)(cid:20)(cid:20)(cid:20)(cid:20) ≈ V |1 − 2E|1 − 2V(cid:2) E(1 − E)|1 − 2E|1 − 2V(cid:2) E(1 − E)|1 − 2E|1 − 2E(1 − E)(cid:2) 2E(1 − E)|1 − 2E|(87)√(cid:21)√The inequalities are optimal in the sense that they are attained in the case of a Bernoulli variable with expectation E.The function E(1 − E)|1 − 2E|/[1 − 2E(1 − E)] is zero for E = 0, 0.5, or 1, and symmetric with respect to E = 0.5. It isconvex down and its maximum over the interval [0, 0.5] is achieved for E = 0.5 −5 − 2/2 (Fig. 8.1). The function2E(1 − E)|1 − 2E| is zero for E = 0, 0.5, or 1, and symmetric with respect to E = 0.5. It is convex down and its maximumover the interval [0, 0.5] is achieved for E = 0.5 −3/6 (Fig. 8.2). Note that at the beginning of learning, with smallrandom weights initialization, typically E is close to 0.5. Towards the end of learning, E is often close to 0 or 1. In allthese cases, the bounds are close to 0 and the NWGM is close to E.Note also that it is possible to have E = NWGM even when the numbers O i are not identical. For instance, if O 1 = 0.25,and thus: E = NWGM = 0.5.O 2 = 0.75, and P 1 = P 2 = 0.5 we have G = GIn short, in general the NWGM is a better approximation to the expectation E than the geometric mean G. The property is alwaystrue to a second order of approximation. Furthermore, it is always exact when NWGM (cid:2) E since we must have G (cid:2) NWGM (cid:2) E.Furthermore, in general the NWGM is a better approximation to the mean than a random sample. Using a randomlychosen O i as an estimate of the mean E, leads to an error that scales like the standard deviation σ =V , whereas theNWGM leads to an error that scales like V .√(cid:4)92P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Fig. 8.2. The curve associated with the approximate bound |E − NWGM| (cid:3) 2E(1 − E)|1 − 2E| (Eq. (87)).When NWGM > E, “third order” cases can be found whereGG + G(cid:4)− E ≈ E − G withGG + G(cid:4)− E (cid:3) E − G(88)An example is provided by: O 1 = 0.622459, O 2 = 0.731059 with a uniform distribution (p1 = p2 = 0.5). In this case,E = 0.676759, G = 0.674577, G(cid:4) = 0.318648, NWGM = 0.679179, E − G = 0.002182 and NWGM − E = 0.002420.(cid:4) = 0. In this case, NWGM = 1, unlessExtreme cases. Note also that if for some i, O i = 1 with non-zero probability, then Gthere is a j (cid:3)= i such that O j = 0 with non-zero probability. Likewise if for some i, O i = 0 with non-zero probability, thenG = 0. In this case, NWGM = 0, unless there is a j (cid:3)= i such that O j = 1 with non-zero probability. If both O i = 1 and O j = 0are achieved with non-zero probability, then NWGM = 0/0 is undefined. In principle, in a sigmoidal neuron, the extremeoutput values 0 and 1 are never achieved, although in simulations this could happen due to machine precision. In all theseextreme cases, where the NWGM is a good approximation of E or not depends on the exact distribution of the values. Forinstance, if for some i, O i = 1 with non-zero probability, and all the other O j ’s are also close to 1, then NWGM = 1 ≈ E. Onthe other hand, if O i = 1 with small but non-zero probability, and all the other O j ’s are close to 0, then NWGM = 1 is nota good approximation of E.Higher order moments. It would be useful to be able to derive estimates also for the variance V , as well as other higherorder moments of the numbers O , especially when O = σ (S). While the NWGM can easily be generalized to higher ordermoments, it does not seem to yield simple estimates as for the mean (see Appendix C). However higher order moments ina deep network trained with dropout can easily be approximated, as in the linear case (see Section 9).Proof. To prove these results, we compute first and second order approximations. Depending on the case of interest, thenumbers 0 < O i < 1 can be expanded around E, around G, or around 0.5 (or around 0 or 1 when they are consistentlyclose to these boundaries). Without assuming that they are consistently low or high, we expand them around 0.5 by writingO i = 0.5 + (cid:8)i where 0 (cid:2) |(cid:8)i| (cid:2) 0.5. [Estimates obtained by expanding around E are given in Appendix B]. For any distributioni(0.5 +P 1, . . . , P m over the m subnetworks, we have E(O ) = 0.5 + E((cid:8)) and Var(O ) = Var((cid:8)). As usual, let G =(cid:8)i)P i = 0.5i O P i(cid:15)(cid:15)(cid:15)=ii(1 + 2(cid:8)i)P i . To a first order of approximation,(cid:6)m(cid:2)m(cid:14)(cid:7)P im(cid:14)G =1+ (cid:8)i2i=1= 12(1 + 2(cid:8)i)P i ≈ 12+i=1i=1P i(cid:8)i = EThe approximation is obtained using a Taylor expansion and the fact that 2|(cid:8)i| < 1. In a similar way, we have Gand G/(G + G(cid:4) ≈ 1 − E(cid:4)) ≈ E. These approximations become more accurate as (cid:8)i → 0. To a second order of approximation, we have(cid:14)∞(cid:2)(cid:14)(cid:6)(cid:7)(cid:12)G = 12P in(2(cid:8)i)n = 12i1 + P i2(cid:8)i + P i(P i − 1)2(cid:13)(2(cid:8)i)2 + R3((cid:8)i)in=0(cid:6)(cid:7)where R3((cid:8)i) is the remainder of order threeR3((cid:8)i) =P i3(2(cid:8)i)3(1 + ui)3−P i(cid:11)(cid:10)(cid:8)2i= o(89)(90)(91)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122and |ui| (cid:2) 2|(cid:8)i|. Expanding the product gives(cid:6)(cid:7)(cid:12)G = 12(cid:14)∞(cid:2)in=0P in(2(cid:8)i)n = 121 +(cid:2)iP i2(cid:8)i +(cid:2)iP i(P i − 1)2(2(cid:8)i)2 +(cid:2)i< j(cid:13)4P i P j(cid:8)i(cid:8) j + R3((cid:8))93(92)which reduces toG = 12+(cid:2)i(cid:6)(cid:2)(cid:7)2P i(cid:8)i−(cid:2)P i(cid:8)i +iP i(cid:8)2i+ o(cid:11)(cid:10)(cid:8)2= 12+ E((cid:8)) − Var((cid:8)) + o(cid:11)(cid:10)(cid:8)2= E(O ) − Var(O ) + R3((cid:8)) (93)By symmetry, we also have(cid:14)(cid:4) =G(1 − O i)P i = 1 − E(O ) − Var(O ) + R3((cid:8))(94)iwhere again R3((cid:8)) is the higher order remainder. Neglecting the remainder and writing E = E(O ) and V = Var(O ) we haveGG + G(cid:4)≈ E − V1 − 2Vand(cid:4)GG + G(cid:4)≈ 1 − E − V1 − 2V(95)Thus the differences between the mean on one hand, and the geometric mean and the normalized geometric means on theother, satisfyE − G ≈ V and E − GG + G(cid:4)≈ V (1 − 2E)1 − 2Vand1 − E − G(cid:4) ≈ V and (1 − E) − G(cid:4)G + G(cid:4)≈ V (1 − 2E)1 − 2V(96)(97)To know when the NWGM is a better approximation to E than the WGM, we consider when the factor |(1 − 2E)/(1 − 2V )|is less or equal to one. There are four cases:(1) E (cid:2) 0.5 and V (cid:2) 0.5 and E (cid:3) V .(2) E (cid:2) 0.5 and V (cid:3) 0.5 and E + V (cid:3) 1.(3) E (cid:3) 0.5 and V (cid:2) 0.5 and E + V (cid:2) 1.(4) E (cid:3) 0.5 and V (cid:3) 0.5 and E (cid:2) V .However, since 0 < O i < 1, we have V (cid:2) E − E 2 = E(1 − E) (cid:2) 0.25. So only cases 1 and 3 are possible and in both cases therelationship is trivially satisfied. Thus in all cases, to a second order of approximation, the NWGM is closer to E than theWGM.9. Dropout distributions and approximation propertiesThroughout the rest of this article, we let W li= σ (U li) denote the deterministic variables of the dropout approximation(or ensemble network) with(cid:6)(cid:2)(cid:2)wlhi j phj W hjW li= σ(cid:7)(98)h<ljin the case of dropout applied to the nodes. The main question we wish to consider is whether W lto E(O li) for every input, every layer l, and any unit i.i is a good approximation9.1. Dropout inductionDropout relies on the correctness of the approximation of the expectation of the activity of each unit over all its dropoutsubnetworks by the corresponding deterministic variable in the formW li≈ E(cid:11)(cid:10)O li(99)for each input, each layer l, and each unit i. The correctness of this approximation can be seen by induction. For the firstlayer, the property is obvious since W 1i ), using the results of Section 8. Now assume that the propertyiis true up to layer l. Again, by the results in Section 8,= NWGM(O 1i ) ≈ E(O 194P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122(cid:11)(cid:10)EO l+1i≈ NWGM(cid:10)O l+1i(cid:11)(cid:10)(cid:10)E= σSl+1i(cid:11)(cid:11)which can be computed by(cid:10)(cid:10)EσSl+1i(cid:11)(cid:11)= σ(cid:6) (cid:2)(cid:2)wl+1hi jphj E(cid:7)(cid:11)(cid:10)O hj≈ σ(cid:6) (cid:2)(cid:2)(cid:7)wl+1hi jj W hphj= W l+1ih<l+1jh<l+1j(100)(101)The approximation in Eq. (101) uses of course the induction hypothesis. This induction, however, does not provide any senseof the errors being made, and whether these errors increase significantly with the depth of the networks. The error can bedecomposed into two terms(cid:10)(cid:16)(cid:11)(cid:17)(cid:10)(cid:11)(cid:10)(cid:10)(cid:11)(cid:11)(cid:17)(cid:16)(cid:8)li= EO li− W li=EO li− NWGMO li+NWGMO li− W li= αli+ βli(102)Thus in what follows we study each term.9.2. Sampling distributionsIn Section 8, we have shown that in general NWGM(O ) provides a good approximation to E(O ). To further understandthe dropout approximation and its behavior in deep networks, we must look at the distribution of the difference α =E(O ) − NWGM(O ). Since both E and NWGM are deterministic functions of a set of O values, a distribution can only bedefined if we look at different samples of O values taken from a more general distribution. These samples could correspondto dropout samples of the output of a given neuron. Note that the number of dropout subnetworks of a neuron beingexponentially large, only a sample can be accessed during simulations of large networks. However, we can also considerthat these samples are associated with a population of neurons, for instance the neurons in a given layer. While we cannotexpect the neurons in a layer to behave homogeneously for a given input, they can in general be separated in a smallnumber of populations, such as neurons that have low activity, medium activity, and high activity and the analysis belowcan be applied to each one of these populations separately. Letting O S denote a sample of m values O i, . . . , O m, we aregoing to show through simulations and more formal arguments that in general E(O S ) − NWGM(O S ) has a mean close to 0,a small standard deviation, and in many cases is approximately normally distributed. For instance, if the values O originatefrom a uniform distribution over [0, 1], it is easy to see that both E and NWGM are approximately normally distributed,with mean 0.5, and a small variance decreasing as 1/m.9.3. Mean and standard deviation of the normalized weighted geometric meanMore generally, assume that the variables O i are i.i.d. with mean μO and variance σ 2O . Then the variables S i satisfyingO i = σ (S i) are also i.i.d. with mean μS and variance σ 2S . Densities for S when O has a Beta distribution, or for O whenS has a Gaussian distribution, are derived in Appendices A–F. These could be used to model in more detail non-uniformdistributions, and distributions corresponding to low or high activity. For m sufficiently large, by the central limit theorem2the means of these quantities are approximately normal with:(cid:7)(cid:7)(cid:6)(cid:6)E(O S ) ∼ NμO ,and E(SS ) ∼ NμS ,σ 2Omσ 2SmIf these standard deviations are small enough, which is the case for instance when m is large, then σ can be well approx-imated by a linear function with slope t over the corresponding small range. In this case, NWGM(O S ) = σ (E(SS )) is alsoapproximately normal with(cid:6)(cid:7)(103)(104)NWGM(O S ) ∼ Nσ (μS ),t2σ 2SmNote that |t| (cid:2) λ/4 since σ (cid:4) = λσ (1 − σ ). Very often, σ (μS ) ≈ μO . This is particularly true if μO = 0.5. Away from 0.5,a bias can appear—for instance we know that if all the O i < 0.5 thenNWGM < E—but this bias is relatively small. This isconfirmed by simulations, as shown in Fig. 9.1 using Gaussian or uniform distributions to generate the values O i . Finally,note that the variance of E(O S ) and NWGM(O S ) are of the same order and behave like C1/m and C2/m respectively asm → ∞. Furthermore σ 2O= C1 ≈ C2 if σ 2O is small.If necessary, it is also possible to derive better and more general estimates of E(O ), under the assumption that S isGaussian by approximating the logistic function with the cumulative distribution of a Gaussian, as described in Appendix F(see also [41]).If we sample from many neurons whose activities come from the same distribution, the sample mean and the sampleNWGM will be normally distributed and have roughly the same mean. The difference will have approximately zero mean.To show that the difference is approximately normal we need to show that E and NWGM are uncorrelated.2 Note that here all the weights P i are identical and equal to 1/m. However the central limit theorem can be applied also in the non-uniform case, aslong as the weights do not deviate too much from the uniform distribution.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12295Fig. 9.1. Histogram of NWGM values for a random sample of 100 values O taken from: (1) the uniform distribution over [0, 1] (upper left); (2) the uniformdistribution over [0, 0.5] (lower left); (3) the normal distribution with mean 0.5 and standard deviation 0.1 (upper right); and (4) the normal distributionwith mean 0.25 and standard deviation 0.05 (lower right). All probability weights are equal to 1/100. Each sampling experiment is repeated 5000 times tobuild the histogram.9.4. Correlation between the mean and the normalized weighted geometric meanWe have(cid:16)Var(cid:17)E(O S )− NWGM[O S )] = Var(cid:16)(cid:17)E(O S )(cid:16)(cid:17)NWGM(O S )(cid:16)+ 2 Cov(cid:17)E(O S ), NWGM(O S )+ Var(105)Thus to estimate the variance of the difference, we must estimate the covariance between E(O S ) and NWGM(O S ). As weshall see, this covariance is close to null.In this section, we assume again samples of size m from a distribution on O with mean E = μO and variance V = σ 2O .To simplify the notation, we use ES , V S , and NWGMS to denote the random variables corresponding to the mean, variance,and normalized weighted geometric mean of the sample. We have seen, by doing a Taylor expansion around 0.5, thatNWGMS ≈ (ES − V S )/(1 − 2V S ).We first consider the case where E = NWGM = 0.5. In this case, the covariance of NWGMS and ES can be estimated asCov(NWGMS , ES ) ≈ E(cid:12)(cid:6)ES − V S1 − 2V S− 12(cid:7)(cid:6)ES − 12(cid:7)(cid:13)(cid:12)= E(cid:13)(E − 12 )21 − 2V S(106)We have 0.5 (cid:2) 1 − 2V S (cid:2) 1 and E(ES − 12 )2 = Var(ES ) = V /m. Thus in short the covariance is of order V /m and goes to 0as the sample size m goes to infinity. For the Pearson correlation, the denominator is the product of two similar standarddeviations and scales also like V /m. Thus the correlation should be roughly constant and close to 1. More generally, evenwhen the mean E is not equal to 0.5, we still have the approximations(cid:12)(cid:12)(cid:6)(cid:7)(cid:13)ES − V S1 − 2V S− E − V1 − 2V(cid:13)(ES − E)= E(E − ES )2 + (V − V S )(ES − E)(1 − 2V S )(1 − 2V )Cov(NWGMS , ES ) ≈ E(107)96P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Fig. 9.2. Behavior of the Pearson correlation coefficient (left) and the covariance (right) between the empirical expectation E and the empirical NWGM as afunction of the number of samples and sample distribution. For each number of samples, the sampling procedure is repeated 10,000 times to estimate thePearson correlation and covariance. The distributions are the uniform distribution over [0, 1], the uniform distribution over [0, 0.5], the normal distributionwith mean 0.5 and standard deviation 0.1, and the normal distribution with mean 0.25 and standard deviation 0.05.And the leading term is still of order V /m. [Similar results are also obtained by using the expansions around 0 or 1 given inAppendices A–F to model populations of neurons with low or high activity.] Thus again the covariance between NWGM andE goes to 0, and the Pearson correlation is constant and close to 1. These results are confirmed by simulations in Fig. 9.2.Combining the previous results we haveVar(ES − NWGMS ) ≈ Var(ES ) + Var(NWGMS ) ≈ C1m+ C2m(108)Thus in general E(O S ) and NWGM(O S ) are random variables with: (1) similar, if not identical, means; (2) variances and covariancethat decrease to 0 inversely to the sample size; (3) approximately normal distributions. Thus E − NWGM is approximately normallydistributed around zero. The NWGM behaves like a random variable with small fluctuations above and below the mean. [Of coursecontrived examples can be constructed (for instance with small m or small networks) which deviate from this generalbehavior.]9.5. Dropout approximations: the cancellation effectsTo complete the analysis of the dropout approximation of E(O l= E(O li where in general the error term (cid:8)l= αli+ βlii) − (cid:8)lW li0. Furthermore the error (cid:8)li) by W li , we show by induction over the layers thati is small and approximately normally distributed with mean= E(O lFirst, the property is true for l = 1 since W 1ii is uncorrelated with the error αli= NWGM(O 1i ) and the results of the previous sections apply immediately toi) for l > 1.i) − NWGM(O lthis case. For the induction step, we assume that the property is true up to layer l. At the following layer, we have(cid:6)(cid:2)(cid:2)W l+1i= σwl+1hi jj W hphj= σh(cid:2)ljh(cid:2)lj(cid:7)(cid:6)(cid:2)(cid:2)wl+1hi jphj(cid:11)(cid:16)(cid:10)EO hj− (cid:8)hj(cid:17)(cid:7)Using a first order Taylor expansionW l+1i≈ NWGMor more compactlyW l+1i≈ NWGM(cid:10)(cid:10)thus(cid:11)O l+1i+ σ (cid:4)(cid:6)(cid:2)(cid:2)wl+1hi jphj E(cid:7)(cid:12)(cid:11)−(cid:10)O hj(cid:2)(cid:2)wl+1hi jj (cid:8)hphj(cid:13)h(cid:2)ljO l+1i(cid:11)(cid:10)(cid:10)E− σ (cid:4)Sl+1i(cid:12)(cid:2)(cid:11)(cid:11)(cid:2)h(cid:2)ljh(cid:2)lj(cid:13)wl+1hi jj (cid:8)hphjβl+1i= NWGM(cid:10)(cid:11)O l+1i− W l+1i≈ σ (cid:4)(cid:10)(cid:10)ESl+1i(cid:12)(cid:2)(cid:11)(cid:11)(cid:2)(cid:13)wl+1hi jj (cid:8)hphjh(cid:2)lj(109)(110)(111)(112)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122As a sum of many linear small terms, βl+1i(cid:11)(cid:10)βl+1iE≈ 0is approximately normally distributed. By linearity of the expectationBy linearity of the variance with respect to sums of independent random variables(cid:2)(cid:10)(cid:2)(cid:10)(cid:10)(cid:10)(cid:10)(cid:11)(cid:11)(cid:11)(cid:10)Varβl+1i(cid:16)σ (cid:4)≈(cid:11)(cid:11)(cid:17)2ESl+1iwl+1hi j2phjVar(cid:8)hj(cid:11)297(113)(114)jh(cid:2)lThis variance is small since [σ (cid:4)(E(Sl+1))]2 (cid:2) 1/16 for the standard logistic function (and much smaller than 1/16 at thej ) is small by induction. The weights wl+1hj )2 (cid:2) 1, and Var((cid:8)hend of learning), (phare small at the beginning of learningand as we shall see in Section 11 dropout performs weight regularization automatically. While this is not observed in thesimulations used here, one concern is that with very large layers the sum could become large. We leave a more detailedstudy of this issue for future work. Finally, we need to show that αl+1are uncorrelated. Since both terms haveapproximately mean 0, we compute the mean of their productand βl+1i jiii(cid:12)(cid:10)i βl+1αl+1i(cid:11)E≈ E(cid:11)(cid:10)(cid:10)EO l+1i− NWGM(cid:10)O l+1i(cid:11)(cid:11)(cid:10)(cid:10)Eσ (cid:4)Sl+1i(cid:11)(cid:11)(cid:2)(cid:2)h(cid:2)ljwl+1hi jj (cid:8)hphj(cid:13)By linearity of the expectation(cid:10)i βl+1αl+1≈ σ (cid:4)EE(cid:10)(cid:10)(cid:11)iiSl+1(cid:11)(cid:11)(cid:2)(cid:2)wl+1hi jphj E(cid:16)(cid:10)(cid:10)E(cid:11)O l+1i− NWGM(cid:10)(cid:11)(cid:11)(cid:17)(cid:8)hjO l+1i≈ 0(115)(116)since E(E(O l+1i) − NWGM(O l+1ih(cid:2)lj= E[E(O l+1))(cid:8)hji and NGWM(O li) − NWGM(O l+1j ) ≈ 0.i) can be viewed as good approximations to E(O l)]E((cid:8)hiIn summary, in general both W li) with small deviations that areapproximately Gaussians with mean zero and small standard deviations. These deviations act like noise and cancel each other to someextent preventing the accumulation of errors across layers.These results and those of the previous section are confirmed by simulation results given by Figs. 9.3, 9.4, 9.5, 9.6, and9.7. The simulations are based on training a deep neural network classifier on the MNIST handwritten characters datasetwith layers of size 784-1200-1200-1200-1200-10 replicating the results described in [27], using p = 0.8 for the input layerand p = 0.5 for the hidden layers. The raster plots accumulate the results obtained for 10 randomly selected input vectors.For fixed weights and a fixed input vector, 10,000 Monte Carlo simulations are used to sample the dropout subnetworksand estimate the distribution of activities O of each neuron in each layer. These simulations use the weights obtained atthe end of learning, except in the cases were the beginning and end of learning are compared (Figs. 9.6 and 9.7). In general,the results show how well the NWGM(O li) in eachlayer, both at the beginning and the end of learning, and how the deviations can roughly be viewed as small, approximatelyGaussian, fluctuations well within the bounds derived in Section 8.i approximate the true expectation E(O li) and the deterministic values W l9.6. Dropout approximations: estimation of variances and covariancesWe have seen that the deterministic values W s can be used to provide very simple but effective estimates of the valuesE(O )s across an entire network under dropout. Perhaps surprisingly, the W s can also be used to derive approximations ofthe variances and covariances of the units as follows.First, for the dropout variance of a neuron, we can use(cid:10)(cid:10)(cid:11)(cid:10)(cid:11)O li O li≈ W li or equivalently VarO li≈ W li1 − W li(cid:11)O li O li≈ W li W li or equivalently Var(cid:11)(cid:10)O li≈ 0orE(cid:10)E(cid:11)(117)(118)These two approximations can be viewed respectively as rough upperbounds and lower bounds to the variance. For neuronswhose activities are close to 0 or 1, and thus in general for neurons towards the end of learning, these two bounds aresimilar to each other. This is not the case at the beginning of learning when, with very small weights and a standard logistici) ≈ 0 (Figs. 9.8 and 9.9). At the beginning and the end of learning, the variances aretransfer function, W lismall and so “0” is the better approximation. However, during learning, variances can be expected to be larger and closerto their approximate upper bound W (1 − W ) (Figs. 9.10 and 9.11).= 0.5 and Var(O lFor the covariances of two different neurons, we use(cid:10)E(cid:11)(cid:10)(cid:11)(cid:10)EO liO hj(cid:11)= EO li O hj≈ W li W hj(119)This independence approximation is accurate for neurons that are truly independent of each other, such as pairs of neuronsin the first layer. However it can be expected to remain approximately true for pairs of neurons that are only loosely coupled,98P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Fig. 9.3. Each row corresponds to a scatter plot for all the neurons in each one of the four hidden layers of a deep classifier trained on the MNIST dataset(see text) after learning. Scatter plots are derived by cumulating the results for 10 random chosen inputs. Dropout expectations are estimated using 10,000dropout samples. The second order approximation in the left column (blue dots) correspond to |E − NWGM| ≈ V |1 − 2E|/(1 − 2V ) (Eq. (87)). Bound 1 isthe variance-dependent bound given by E(1 − E)|1 − 2E|/(1 − 2V ) (Eq. (87)). Bound 2 is the variance-independent bound given by E(1 − E)|1 − 2E|/(1 −2E(1 − E)) (Eq. (87)). In the right column, W represent the neuron activations in the deterministic ensemble network with the weights scaled appropriatelyand corresponding to the “propagated” NWGMs. (For interpretation of the references to color in this figure legend, the reader is referred to the web versionof this article.)i.e. for most pairs of neurons in a large neural networks at all times during learning. This is confirmed by simulations(Fig. 9.12) conducted using the same network trained on the MNIST dataset. The approximation is much better than simplyusing 0 (Fig. 9.13).For neurons that are directly connected to each other, this approximation still holds but one can try to improve it byj feeding directly into the neuron with outputintroducing a slight correction. Consider the case of a neuron with output O hO li j . By isolating the contribution of O hi (h < l) through a weight wlh(cid:6)(cid:2)(cid:2)O li= σwf <lk(cid:3)= jlfikδ fk Ofk+ wlhi j δhj O hj≈ σ(cid:7)(cid:6)(cid:2)j , we have(cid:7)lfikδ fk Ofk+ σ (cid:4)(cid:2)wf <lk(cid:3)= j(cid:6)(cid:2)(cid:2)wf <lk(cid:3)= j(cid:7)fklfikδ fk Owlhi j δhj O hj(120)with a first order Taylor approximation which is more accurate when wlhwell satisfied at the beginning of learning or with sparse coding). In this expansion, the first term is independent of O hj are small (conditions that are particularlyj andi j or O hP. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12299Fig. 9.4. Similar to Fig. 9.3, using the sharper but potentially more restricted second order approximation to the NWGM obtained by using a Taylor expansionaround the mean (see Appendix B, Eq. (202)).its expectation can easily be computed as(cid:6)(cid:7)(cid:7)(cid:6)(cid:6)(cid:2)(cid:2)Eσwf <lk(cid:3)= jlfikδ fk Ofk≈ σE(cid:6)(cid:2)(cid:2)wf <lk(cid:3)= jlfikδ fk O(cid:6)(cid:2)(cid:2)(cid:7)(cid:7)fk= σf <lk(cid:3)= j(cid:7)wlfik pfk Wfk= W lhi j(121)Thus here W lhis simply the deterministic activation of neuron i in layer l in the ensemble network when neuron j in layeri jh is removed from its inputs. Thus it can easily be computed by forward propagation in the deterministic network. Using afirst-order Taylor expansion it can be estimated byW lhi j≈ W li− σ (cid:4)(cid:11)(cid:10)U liIn any case,wlhi j phj W hj(cid:11)(cid:10)EO li O hj≈ W lhi j W hj(cid:6)(cid:2)(cid:2)(cid:6)σ (cid:4)+ Ewf <lk(cid:3)= j(cid:7)(cid:7)fkwlhi j phj E(cid:10)lfikδ fk OO hj O hj(cid:11)(122)(123)100P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Fig. 9.5. Similar to Figs. 9.3 and 9.4. Approximation 1 corresponds to the second order Taylor approximation around 0.5: |E − NWGM| ≈ V |1 − 2E|/(1 − 2V )1−([0.5V ]/[E(1−E)]) (see Appendix B, Eq. (202)).(Eq. (87)). Approximation 2 is the sharper but more restrictive second order Taylor approximation around E:E−(V /2E)Histograms for the two approximations are interleaved in each figure of the right column.Towards the end of learning, σ (cid:4) ≈ 0 and so the second term can be neglected. A slightly more precise estimate can beobtained by writing σ (cid:4) ≈ λσ when σ is close to 0, and σ (cid:4) ≈ λ(1 − σ ) when σ is close to 1, replacing the correspondingexpectation by W lhi j . In any case, to a leading term approximation, we havei j or 1 − W lh(cid:11)(cid:10)EO li O hj≈ W lhi j W hj(124)The accuracy of these formula for pairs of connected neurons is demonstrated in Fig. 9.14 at the beginning and end oflearning, where it is also compared to the approximation E(O lj . The correction provides a small improvementat the end of learning but not at the beginning. This is because it neglects a term in σ (cid:4)which presumably is close to 0 atthe end of learning. The improvement is small enough that for most purposes the simpler approximation W lj may beused in all cases, connected or unconnected.j ) ≈ W li W hi W hi O hP. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122101Fig. 9.6. Empirical distribution of NWGM − E is approximately Gaussian at each layer, both before and after training. This was performed with Monte Carlosimulations over dropout subnetworks with 10,000 samples for each of 10 fixed inputs. After training, the distribution is slightly asymmetric because theactivation of the neurons is asymmetric. The distribution in layer one before training is particularly tight simply because the input to the network (MNISTdata) is relatively sparse.10. The duality with spiking neurons and with backpropagation10.1. Spiking neuronsThere is a long-standing debate on the importance of spikes in biological neurons, and also in artificial neural networks,in particular as to whether the precise timing of spikes is used to carry information or not. In biological systems, there aremany examples, for instance in the visual and motor systems, where information seems to be carried by the short termaverage firing rate of neurons rather than the exact timing of their spikes. However, other experiments have shown thatin some cases the timing of the spikes are highly reproducible and there are also known examples where the timing ofthe spikes is crucial, for instance in the auditory location systems of bats and barn owls, where brain regions can detectvery small interaural differences, considerably smaller than 1 ms [26,19,18]. However these seem to be relatively rare andspecialized cases. On the engineering side the question of course is whether having spiking neurons is helpful for learning102P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Fig. 9.7. Empirical distribution of W − E is approximately Gaussian at each layer, both before and after training. This was performed with Monte Carlosimulations over dropout subnetworks with 10,000 samples for each of 10 fixed inputs. After training, the distribution is slightly asymmetric because theactivation of the neurons is asymmetric. The distribution in layer one before training is particularly tight simply because the input to the network (MNISTdata) is relatively sparse.or any other purposes, and if so whether the precise timing of the spikes matters or not. There is a connection betweendropout and spiking neurons which might shed some, at the moment faint, light on these questions.A sigmoidal neuron with output O = σ (S) can be converted into a stochastic spiking neuron by letting the neuron“flip a coin” and produce a spike with probability O . Thus in a network of spiking neurons, each neuron computes threerandom variables: an input sum S, a spiking probability O , and a stochastic output (cid:10) (Fig. 10.1). Two spiking mechanismscan be considered: (1) global: when a neuron spikes it sends the same quantity r along all its outgoing connections; and(2) local or connection-specific: when a neuron spikes with respect to a specific connection, it sends a quantity r alongthat connection. In the latter case, a different coin must be flipped for each connection. Intuitively, one can see that thefirst case corresponds to dropout on the units, and the second case to droupout on the connections. When a spike is notproduced, the corresponding unit is dropped in the first case, and the corresponding connection is dropped in the secondcase.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122103Fig. 9.8. Approximation of E(O li ) and for the variance for neurons in aMNIST classifier network before and after training. Histograms are obtained by taking all non-input neurons and aggregating the results over 10 randominput vectors.i corresponding respectively to the estimates W li and by W li ) by W li W li O li (1 − W lFig. 9.9. Histogram of the difference between the dropout variance of O li ) in a MNIST classifier network beforeand after training. Histograms are obtained by taking all non-input neurons and aggregating the results over 10 random input vectors. Note that at thebeginning of learning, with random small weights, E(O li and its approximate upperbound W l≈ 0.5, and thus Var(O li (W li ) ≈ 0 whereas W li (1 − W li ) ≈ 0.25.i ) ≈ W liFig. 9.10. Temporal evolution of the dropout variance V (O ) during training averaged over all hidden units.104P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Fig. 9.11. Temporal evolution of the difference W (1 − W ) − V during training averaged over all hidden units.Fig. 9.12. Approximation of E(O lj for pairs of non-input neurons that are not directly connected to each other in a MNIST classifier network,before and after training. Histograms are obtained by taking 100,000 pairs of unconnected neurons, uniformly at random, and aggregating the results over10 random input vectors.j ) by W li W hi O hFig. 9.13. Comparison of E(O lafter training. As shown in the previous figure, W lneurons, uniformly at random, and aggregating the results over 10 random input vectors.j ) to 0 for pairs of non-input neurons that are not directly connected to each other in a MNIST classifier network, before andj provides a better approximation. Histograms are obtained by taking 100,000 pairs of unconnectedi W hi O hP. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122105Fig. 9.14. Approximation of E(O lj for pairs of connected non-input neurons, with a directed connection from j to i in a MNISTclassifier network, before and after training. Histograms are obtained by taking 100,000 pairs of connected neurons, uniformly at random, and aggregatingthe results over 10 random input vectors.i j and W lj ) by W li W lhi W hi O hFig. 9.15. Histogram of the difference between E(σ (cid:4)(S)) and σ (cid:4)(E(S)) (see Eq. (220)) over all non-input neurons, in a MNIST classifier network, before andafter training. Histograms are obtained by taking all non-input neurons and aggregating the results over 10 random input vectors. The nodes in the firsthidden layer have 784 sparse inputs, while the nodes in the upper three hidden layers have 1200 non-sparse inputs. The distribution of the initial weightsare also slightly different for the first hidden layer. The differences between the first hidden layer and all the other hidden layers are responsible for theinitial bimodal distribution.106P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Fig. 10.1. A spiking neuron formally operates in 3 steps by computing first a linear sum S, then a probability O = σ (S), then a stochastic output (cid:10) of sizer with probability O (and 0 otherwise).To be more precise, a multi-layer network is described by the following equations. First for the spiking of each unit:(cid:18)=(cid:10)hii with probability O hrh0otherwiseiin the global firing case, and(cid:18)(cid:10)hji=ji with probability O hrh0otherwiseiin the connection-specific case. Here we allow the “size” of the spikes to vary with the neurons or the connections, withspikes of fixed-size being an easy special case. While the spike sizes could in principle be greater than one, the connectionto dropout requires spike sizes of size at most one. The spiking probability is computed as usual in the formO hi= σ(cid:11)(cid:10)Shiand the sum term is given by(cid:2)(cid:2)=Shiwhli j (cid:10)ljl<hjin the global firing case, and(cid:2)(cid:2)=Shiwhli j (cid:10)li j(127)(128)(129)l<hjin the connection-specific case. The equations can be applied to all the layers, including the output layer and the inputlayer if these layers consist of spiking neurons. Obviously non-spiking neurons (e.g. in the input or output layers) can becombined with spiking neurons in the same network.In this formalism, the issue of the exact timing of each spike is not really addressed. However some information aboutthe coin flips must be given in order to define the behavior of the network. Two common models are to assume completeasynchrony, or to assume synchrony within each layer. As spikes propagate through the network, the average output E((cid:10))of a spiking neuron over all spiking configurations is equal to r times the size its average firing probability E(O ). As we haveseen, the average firing probability can be approximated by the NWGM over all possible inputs S, leading to the followingrecursive equations:(cid:10)(cid:11)(cid:11)= rhi EO hiin the global firing case, or(cid:11)(cid:10)(cid:11)= rhji EO hi(cid:10)(cid:10)hiE(cid:10)(cid:10)hjiEin the connection-specific case. Then(cid:10)(cid:11)(cid:10)(cid:10)(cid:11)(cid:10)EO hi≈ NWGMO hi= σE(cid:11)(cid:11)Shiwith(cid:2)(cid:2)(cid:11)(cid:10)EShi=(cid:11)(cid:10)(cid:10)lj=whli j E(cid:2)(cid:2)i j rlwhlj E(cid:11)(cid:10)O ljl<hjl<hj(125)(126)(130)(131)(132)(133)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122107Fig. 10.2. Three closely related networks. The first network operates stochastically and consists of spiking neurons: a neuron sends a spike of size r withprobability O . The second network operates stochastically and consists of logistic dropout neurons: a neurons sends an activation O with a dropoutprobability r. The connection weights in the first and second networks are identical. The third network operates in a deterministic way and consists oflogistic neurons. Its weights are equal to the weights of the second network multiplied by the corresponding probability r.in the global firing case, or(cid:11)(cid:10)EShi=(cid:2)(cid:2)(cid:11)(cid:10)(cid:10)li j=whli j E(cid:2)(cid:2)i j rlwhli j E(cid:11)(cid:10)O lj(134)l<hjl<hjin the connection-specific case.In short, the expectation of the stochastic outputs of the stochastic neurons in a feedforward stochastic network can be approxi-mated by a dropout-like deterministic feedforward propagation, proceeding from the input layer to the output layer, and multiplyingeach weight whli j )—which acts as a dropout probability parameter—of the corresponding pre-synaptic neuron. [Operating a neuron in stochastic mode is also equivalent to setting all its inputs to 1 and using dropout onits connections with different Bernoulli probabilities associated with the sigmoidal outputs of the previous layer.]i j by the corresponding spike size rlj (or rlIn particular, this shows that given any feedforward network of spiking neurons, with all spikes of size 1, we can ap-proximate the average firing rate of any neuron simply by using deterministic forward propagation in the correspondingidentical network of sigmoidal neurons. The quality of the approximation is determined by the quality of the approxima-tions of the expectations by the NWGMs. More generally, consider three feedforward networks (Fig. 10.2) with the sameidentical topology, and almost identical weights. The first network is stochastic, has weights whli j , and consists of spikingneurons: a neuron with activity O hi , and 0 otherwise (a similar argument canbe made with connection-specific spikes of size rhji ). Thus, in this network neuron i in layer h sends out a signal that hasinstantaneous mean and variance given byi sends a spike of size rhi with probability O hE = rhi O hiand Var =(cid:10)(cid:11)2(cid:10)rhiO hi1 − O hi(cid:11)for fixed O hi , and short-term mean and variance given by(cid:10)rhiand Var =1 − EO hiO hiO hii E(cid:11)(cid:10)E(cid:10)(cid:11)(cid:10)(cid:11)(cid:10)2(cid:11)(cid:11)E = rh(135)(136)when averaged over all spiking configurations, for a fixed input.The second network is also stochastic, has identical weights to the first network, and consists of dropout sigmoidali with probability rhi , and 0 otherwise (a similar argument can be madeji ). Thus neuron i in layer h sends out a signal that has instantaneousneurons: a neuron with activity O hi sends a value O hwith connection-specific dropout with probability rhexpectation and variance given byE = rhi O hiand Var =(cid:10)(cid:11)2(cid:10)rhiO hi1 − rhi(cid:11)for a fixed O hi , and short-term expectation and variance given by(cid:11)(cid:10)(cid:10)(cid:10)(cid:11)(cid:11)(cid:11)E = rhi EO hiand Var = r VarO hi+ EO hi2rhi(cid:10)1 − rhi(137)(138)when averaged over all dropout configurations, for a fixed input.The third network is deterministic and consists of logistic units. Its weights are identical to those of the previous twonetworks except they are rescaled in the form whlj . Then, remarkably, feedforward deterministic propagation in the thirdi jnetwork can be used to approximate both the average output of the neurons in the first network over all possible spikingconfigurations, and the average output of the neurons in the second network over all possible dropout configurations. In× rl108P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122particular, this shows that using stochastic neurons in the forward pass of a neural network of sigmoidal units may besimilar to using dropout.Note that the first and second network are quite different in their details. In particular the variances of the signals senti , then the variance is greater in the dropout(cid:3) 0.5, then the variance is greater in thei < O hby a neuron to the following layer are equal only when O hinetwork. When rhi , which is the typical case with sparse encoding and rhispiking network. This corresponds to the Poisson regime of relatively rare spikes.i . Whenr hi > O h= rhIn summary, a simple deterministic feedforward propagation allows one to estimate the average firing rates in stochastic,even asynchronous, networks without the need for knowing the exact timing of the firing events. Stochastic neurons can beused instead of dropout during learning. Whether stochastic neurons are preferable to dropout, for instance because of thedifferences in variance described above, requires further investigations. There is however one more aspect to the connectionbetween dropout, stochastic neurons, and backpropagation.10.2. Backpropagation and backpercolationAnother important observation is that the backward propagation used in the backpropagation algorithm can itself beviewed as closely related to dropout. Starting from the errors at the output layer, backpropagation uses an orderly alternat-ing sequence of multiplications by the transpose of the forward weight matrices and by the derivatives of the activationfunctions. Thus backpropagation is essentially a form of linear propagation in the reverse linear network combined withmultiplication by the derivatives of the activation functions at each node, and thus formally looks like the recursion ofEq. (24). If these derivatives are between 0 and 1, they can be interpreted as probabilities. [In the case of logistic activationfunctions, σ (cid:4)(x) = λσ (x)(1 − σ (x)) and thus σ (cid:4)(x) (cid:2) 1 for every value of x when λ (cid:2) 4.] Thus backpropagation is computing thedropout ensemble average in the reverse linear network where the dropout probability p of each node is given by the derivative of thecorresponding activation. This suggests the possibility of using dropout (or stochastic spikes, or addition of Gaussian noise),during the backward pass, with or without dropout (or stochastic spikes, or addition of Gaussian noise) in the forward pass,and with different amounts of coordination between the forward and backward pass when dropout is used in both.Using dropout in the backward pass is still faced with the problem of vanishing gradients since units with activities closeto 0 or 1, hence derivatives close to 0, lead to rare sampling. However, imagine for instance six layers of 1000 units each,fully connected, with derivatives that are all equal to 0.1 everywhere. Standard backpropagation produces an error signal−6 by the time the first layer is reached. Using dropout in the backpropagation instead selectsthat contains a factor of 10on average 100 units per layer and propagates a full signal through them, with no attenuation. Thus a strong error signalis propagated but through a narrow channel, hence the name of backpercolation. Backpropagation can be thought of as aspecial case of backpercolation, because with a very small learning rate backpercolation is essentially identical to backpropa-gation, since backpropagation corresponds to the ensemble average of many backpercolation passes. This approach of coursewould be slow on a computer since a lot of time would be spent sampling to compute an average signal that is providedin one pass by backpropagation. However it shows that exact gradients are not always necessary and that backpropagationcan tolerate noise, alleviating at least some of the concerns with the biological plausibility of backpropagation. Furthermore,aside from speed issue, noise in the backward pass might help avoiding certain local minima. Finally, we note that sev-eral variations on these ideas are possible, such as using backpercolation with a fixed value of p (e.g. p = 0.5), or usingbackpropagation for the top layers followed by backpercolation for the lower layers and vice versa. Detailed investigation ofthese issues is beyond the scope of this paper and left for future work.11. Dropout dynamicsSo far, we have concentrated on the static properties of dropout, i.e. properties of dropout for a fixed set of weights.In this section we look at more dynamic properties of dropout, related to the training procedure and the evolution of theweights.11.1. Dropout convergenceWith properly decreasing learning rates, dropout is almost sure to converge to a small neighborhood of a local minimum(or global minimum in the case of a strictly convex error function) in a way similar to stochastic gradient descent instandard neural networks [38,13,14]. This is because it can be viewed as a form of on-line gradient descent with respect tothe error functionError = E TENS =(cid:2)(cid:2)INP (N ) f w(cid:10)(cid:11)O N , t(I)=P (N ) f w(cid:10)(cid:11)O N , t(I)(cid:2)I×N(139)of the true ensemble, where t(I) is the target value for input I and f w is the elementary error function, typically thesquared error in regression, or the relative entropy error in classification, which depends on the weights w. In the case ofdropout, the probability P (N ) of the network N is factorial and associated with the product of the underlying Bernoulliselector variables.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122109Thus dropout is “on-line” with respect to both the input examples I and the networks N , or alternatively one can form anew set of training examples, where the examples are formed by taking the cartesian product of the set of original exampleswith the set of all possible subnetworks. In the next section, we show that dropout is also performing a form of stochasticgradient descent with respect to a regularized ensemble error.Finally, we can write the gradient of the error above as:∂ E TENS∂ wlhi j=(cid:2)(cid:2)IN :δhj=1P (N )=∂ f w∂ whli j(cid:2)(cid:2)IN :δhj=1P (N )∂ f w∂ SliO hj (N , I)(140)If the backpropagated error does not vary too much around its mean from one network to the next, which seems reasonablein a large network, then we can replace it by its mean, and similarly for the activity O hj . Thus the gradient of the trueensemble can be approximated by the product of the expected backpropagated error (post-synaptic terms) and the expectedpre-synaptic activity(cid:6)(cid:6)(cid:7)(cid:7)j W hphj(141)∂ E TENS∂ wlhi j≈ E∂ f w∂ Sli(cid:10)(cid:11)phj EO hj≈ E∂ f w∂ Sli11.2. Dropout gradient and adaptive regularization: single linear unitAs for the static properties, it is instructive to first consider the simplest case of a single linear unit. In the case of a singlelinear unit trained with dropout with an input I , an output O = S, and a target t, the error is typically quadratic of theform Error = 12 (t − O )2. Let us consider the two error functions E ENS and E D associated with the ensemble of all possiblesubnetworks and the network with dropout. In the linear case, the ensemble network is identical to the deterministicnetwork obtained by scaling the connections by the dropout probabilities. For a single input I , these error functions aredefined by:(cid:4)E ENS = 12(t − O ENS)2 = 12t −(cid:5)2n(cid:2)i=1pi w i Iiand(cid:4)E D = 12(t − O D )2 = 12t −(cid:5)2n(cid:2)i=1δi w i Ii(142)(143)Here δi are the Bernoulli selector random variables with P (δi = 1) = pi , hence E D is a random variable, whereas E ENS isa deterministic function. We use a single training input I for notational simplicity, otherwise the errors of each trainingexample can be combined additively. The learning gradients are of the form ∂ E∂ w= −(t − O ) ∂ O∂ w , yielding:= ∂ E∂ O∂ O∂ w∂ E ENS∂ w i= −(t − O ENS)pi Iiand∂ E D∂ w i= −(t − O D )δi Ii = −tδi Ii + w iδ2i I 2i+(cid:2)j(cid:3)=iw jδiδ j Ii I j(144)(145)The last vector is a random vector variable and we can take its expectation. Assuming as usual that the random variablesδi ’s are pairwise independent, we have(cid:6)(cid:7)E∂ E D∂ w i(cid:10)(cid:11)= −t − E(O D |δi = 1)pi Ii= −tpi Ii + w i pi I 2i+(cid:2)j(cid:3)=iw i pi p j Ii I j = −(t − O ENS)pi Ii + w i I 2i (pi)(1 − pi)which yields(cid:6)E(cid:7)= ∂ E ENS∂ w i+ w i I 2i Var δi = ∂ E ENS∂ w i+ w i Var(δi Ii)∂ E D∂ w iThus, in general the dropout gradient is well aligned with the ensemble gradient. Remarkably, the expectation of the gradient withdropout is the gradient of the regularized ensemble errorE = E ENS + 12n(cid:2)i=1w 2i I 2i Var δi(148)(146)(147)110P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122The regularization term is the usual weight decay or Gaussian prior term based on the square of the weights and ensuring that theweights do not become too large and overfit the data. Dropout provides immediately the magnitude of the regularization term whichis adaptively scaled by the square of the input terms and by the variance of the dropout variables. Note that here pi = 0.5 is the valuethat provides the highest level of regularization and the regularization term depends only on the inputs, and not on the target outputs.Furthermore, the expected dropout gradient is on-line also with respect to the regularization term since there is one termfor each training example. Obviously, the same result holds for an entire layer of linear units. The regularization effect ofdropout in the case of generalized linear models is also discussed in [43] where it is also used to derive other regularizers.11.3. Dropout gradient and adaptive regularization: deep linear networksSimilar calculations can be made for deep linear networks. For instance, the previous calculation can be adapted imme-diately to the top layer of a linear network with T layers with(cid:10)ti − O T= −i(cid:11)j O lδlj∂ E D∂ w Ti jland(cid:7)(cid:6)E∂ E D∂ w T li j= ∂ E ENS∂ w T li j+ w T li j Var(cid:11)(cid:10)j O lδlj(149)(150)which corresponds again to an adaptive quadratic regularization term in w T li j , with a coefficient associated for each inputj O lwith the corresponding variance of the dropout pre-synaptic neuron Var(δlj).To study the gradient of any weight w in the network, let us assume without any loss of generality that the deepnetwork has a single output unit. Let us denote its activity by S in the dropout network, and by U in the deterministicensemble network. Since the network is linear, for a given input the output is a linear function of wS = α w + β and U = E(S) = E(α)w + E(β)(151)The output is obtained by summing the contributions provided by all possible paths from inputs to output. Here α and βare random variables. α corresponds to the sum of all the contributions associated with paths from the input layer to theoutput layer that contain the edge associated with w. β corresponds to the sum of all the contributions associated withpaths from the input layer to the output layer that do not contain the edge associated with w. Thus the gradients are givenby∂ E D∂ w= −(t − S)∂ S∂ w= (α w + β − t)αand∂ E ENS∂ w= −(t − U )(cid:10)=∂ U∂ wE(α)w + E(β) − t(cid:11)E(α)The expectation of the dropout gradient is given by(cid:7)(cid:6)E∂ E D∂ w= (α w + β − t)α = E(cid:11)(cid:10)α2w + E(αβ) − t E(α)This yields the remarkable expression(cid:7)(cid:6)E∂ E D∂ w= ∂ E ENS∂ w+ w Var(α) + Cov(α, β)(152)(153)(154)(155)Thus again the expectation of the dropout gradient is the gradient of the ensemble plus an adaptive regularization term which hastwo components. The component wVar(α) corresponds to a weight decay, or quadratic regularization term in the error function. Theadaptive coefficient Var(α) measures the dropout variance of the contribution to the final output associated with all the input-to-outputpaths which contain w. The component Cov(α, β) measures the dropout covariance between the contribution associated with all thepaths that contain w and the contribution associated with all the paths that do not contain w. In general, this covariance is small andequal to zero for a single layer linear network. Both α and β depend on the training inputs, but not on the target outputs.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12211111.4. Dropout gradient and adaptive regularization: single sigmoidal unitFor a single sigmoidal unit something quite similar, but not identical holds. With a sigmoidal unit O = σ (S) = 1/(1 +−λS ), one typically uses the relative entropy errorce(cid:11)(cid:10)t log O + (1 − t) log(1 − O )E = −(156)We can again consider two error functions E ENS and E D . Note that while in the linear case E ENS is exactly equal to theensemble error, in the non-linear case we use E ENS to denote the error of deterministic network which approximates theensemble network.By the chain rule, we have ∂ E∂ w= ∂ E∂ O∂ O∂ S∂ S∂ w with∂ E∂ O= −t1O+ (1 − t)11 − Oand∂ O∂ S= λO (1 − O )Thus finally grouping terms together∂ E∂ w= −λ(t − O )∂ S∂ w(157)(158)Thus the overall form of the derivative is similar to the linear case up to multiplication by the positive factor λ whichis often fixed to one. However the outputs are non-linear which complicates the comparison of the derivatives. We useO = σ (S) in the dropout network and W = σ (U ) in the deterministic ensemble approximation. For the ensemble network= −λ(t − W )pi Ii = −λ(cid:11)(cid:10)t − σ (U )∂ E ENS∂ w ipi Ii = λt − σ(cid:6)(cid:6)(cid:2)(cid:7)(cid:7)For the dropout network∂ E D∂ w i= −λ(t − O )δi Ii = λt − σ(cid:6)(cid:7)(cid:7)w jδ j I jδi Ii(cid:6)(cid:2)jTaking the expectation of the gradient gives(cid:6)(cid:7)(cid:6)(cid:7)(cid:13)(cid:7)= −λt − Ew jδ j I j|δi = 1pi Iiw j p j I jpi Iij(159)(160)(161)Using the NWGM approximation to the expectation allows one to take the expectation inside the sigmoidal function so that(cid:7)(cid:7)(cid:6)(cid:6)(cid:7)≈ −λt − σw j p j I j − w i pi Ii + w i Iipi Ii = −λU + Ii w i(1 − pi)pi Ii(162)(cid:10)(cid:10)t − σ(cid:11)(cid:11)The logistic function is continuously differentiable everywhere so that one can take its first-order Taylor expansionaround U :(cid:6)(cid:7)≈ −λ(cid:10)t − σ (SENS) − σ (cid:4)(cid:11)(SENS)Ii w i(1 − pi)pi Iiwhere σ (cid:4)(x) = σ (x)(1 − σ (x)) denotes the derivative of σ . So finally we obtain a result similar to the linear case∂ E D∂ w i≈ ∂ E ENS∂ w i+ λσ (cid:4)(U )w i I 2i Var(δi) = ∂ E ENS∂ w i+ λσ (cid:4)(U )w i Var(δi Ii)The dropout gradient is well aligned with the ensemble approximation gradient. Remarkably, and up to simple approximations, theexpectation of the gradient with dropout is the gradient of the regularized ensemble errorE = E ENS + 12λσ (cid:4)(U )n(cid:2)i=1w 2i I 2i Var(δi)(165)The regularization term is the usual weight decay or Gaussian prior term based on the square of the weights and ensuring that theweights do not become too large and overfit the data. Dropout provides immediately the magnitude of the regularization term whichis adaptively scaled by the square of the input terms, the gain λ of the sigmoidal function, by the variance of the dropout variables, andthe instantaneous derivative of the sigmoidal function. This derivative is bounded and approaches zero when SENS is small or large.Thus regularization is maximal at the beginning of learning and decreases as learning progresses. Note again that pi = 0.5 is the valuethat provides the highest level of regularization. Furthermore, the expected dropout gradient is on-line also with respect to(163)(164)∂ E D∂ w i∂ E D∂ w i∂ E D∂ w iEEEE(cid:6)(cid:7)(cid:6)(cid:2)(cid:12)σj(cid:6)(cid:2)j112P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122the regularization term since there is one term for each training example. Note again that the regularization term dependsonly on the inputs, and not on the target outputs. A similar analysis, with identical results, can be carried also for a setof normalized exponential units or for an entire layer of sigmoidal units. A similar result can be derived in a similar wayfor other suitable transfer functions, for instance for rectified linear functions by expressing them as integrals of logisticfunctions to ensure differentiability.11.5. Dropout gradient and adaptive regularization: deep neural networksIn deep neural networks with logistic transfer functions at all the nodes, the basic idea remains the same. In fact, fora fixed set of weights and a fixed input, we can linearize the network around any weight w and thus Eq. (155) applies“instantaneously”.To derive more specific approximations, consider a deep dropout network described byO hi= σ hi(cid:11)(cid:10)Shi= σ(cid:6)(cid:2)(cid:2)(cid:7)l<hjwhli j δlj O ljwith O 0j= I j(166)with layers ranging from h = 0 for the inputs to h = T for the output layer, using the selector random variables δlcorresponding approximation ensemble network is described byj . TheW hi= σ hi(cid:11)(cid:10)U hi= σ(cid:6)(cid:2)(cid:2)(cid:7)whli j plj W ljwith W 0j= I j(167)l<hjusing a new set of U and W distinct variables to avoid any confusion. In principle each node could use a different logisticfunction, with different c and λ parameters, but to simplify the notation we assume that the same logistic function is usedby all neurons. Then the gradient in the ensemble network can be computed by∂ E ENS∂ whli j= ∂ E ENS∂ U hi∂ U hi∂ whli jwhere the backpropagated error can be computed recursively using∂ E ENS∂ U hi=(cid:2)(cid:2)l>hk∂ E ENS∂ U lkwlhki phi σ (cid:4)(cid:11)(cid:10)U hiwith the initial values at the top of the network= −λ(cid:10)ti − W Ti(cid:11)∂ E ENS∂ U Ti(168)(169)(170)Here ti is the i-th component of the target vector for the example under consideration. In addition, for the pre-synapticterm, we have∂ U hi∂ whli j= plj W ljLikewise, for the dropout network,with∂ E D∂ whli j= ∂ E D∂ Shi∂ Shi∂ whli j=∂ E D∂ Shi(cid:2)(cid:2)l>hk∂ E D∂ Slkwlhkiδhi σ (cid:4)(cid:11)(cid:10)Shiand the initial values at the top of the network= −λ(cid:11)(cid:10)ti − O Ti∂ E D∂ S Tiand the pre-synaptic term∂ Shi∂ whli j= δlj O lj(171)(172)(173)(174)(175)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122113Consider unit i in the output layer T receiving a connection from unit j in a layer l (typically l = T − 1) with weight w T li j .The gradient of the error function in the dropout network is given by= −λ(cid:11)(cid:10)ti − O Tij O lδlj= −λ(cid:10)(cid:10)ti − σS Ti∂ E D∂ w T li j(cid:11)(cid:11)j O lδlj= −λ(cid:10)(cid:10)ti − σS T li j+ w T li j δlj O lj(cid:11)(cid:11)j O lδlj(176)using the notation of Section 9.5: S T li jterms gives:= Sli− w T li j δlj O lj . Using a first order Taylor expansion to separate out independent≈ −λ(cid:10)(cid:10)ti − σS T li j(cid:11)− σ (cid:4)(cid:11)(cid:10)S T li j∂ E D∂ w T li jw T li j δlj O lj(cid:11)j O lδljWe can now take the expectation of the gradient(cid:7)(cid:6)E∂ E D∂ w T li j≈ −λ(cid:10)ti − E(cid:10)σ(cid:10)S T li j(cid:11)(cid:11)(cid:11)j W lplj+ λE(cid:11)(cid:11)(cid:10)(cid:10)σ (cid:4)S T li j(cid:10)w T li j plj E(cid:11)O lj O lji j )) = σ (U T li j )) ≈ σ (E(S T l(cid:10)(cid:11)(cid:10)σ (cid:4)O lpli j λ(ES T li jj O lj E(cid:10)ji j ) = W T l(cid:10)(cid:11)i j− σ (cid:4)(cid:11)U Ti+ w T l≈ W Ti− σ (cid:4)(U Ti j plj W lji )w T l(cid:11)j W lplj plj W ljNow, using the NWGM approximation E(σ (S T l(cid:7)(cid:6)E∂ E D∂ w T li j≈ −λ(cid:10)ti − W Ti(cid:11)j W lpljwhich has the form(cid:6)(cid:7)E∂ E D∂ w T li j≈ ∂ E ENS∂ w T li j+ w T li j A(177)(178)(179)(180)where A has the complex expression given by Eq. (179). Thus we see again that the expectation of the dropout gradient in thetop layer is approximately the gradient of the ensemble network regularized by a quadratic weight decay with an adaptive coefficient.Towards the end of learning, if the sigmoidal functions are saturated, then the derivatives are close to 0 and A ≈ 0.Using the dropout approximation E(O lj) ≈ W lj together with E(σ (cid:4)(S T li j )) ≈ σ (cid:4)(U Ti ) (Fig. 9.15, Eq. (220)) produces themore compact approximation(cid:7)(cid:6)E∂ E D∂ w T li j≈ −λ(cid:10)ti − W Ti(cid:11)j W lplj+ w T li j λσ (cid:4)(cid:10)U Ti(cid:11)(cid:10)Varj O lδlj(cid:11)(181)similar to the single layer-case and showing that dropout tends to minimize the variance Var(δlj thus A can be further approximated as A ≈ σ (cid:4)(U Tmation of Section 9.5 E(O lj W lcan also write the expected gradient as a product of a post-synaptic backpropagated error and a pre-synaptic expectationj). Also with the approxi-j). In this case, wej O lj(1 − plj) ≈ W li )plj W lj O l(cid:7)(cid:6)E∂ E D∂ w T li j(cid:10)≈−λ(cid:10)ti − W Ti(cid:11)+ λw T li j σ (cid:4)(cid:11)(cid:10)(cid:10)U Ti1 − plj W lj(cid:11)(cid:11)j W lplj(182)With approximations, similar results appear to be true for deeper layers. To see this, the first approximation we make isj of the immediate pre- and post-synapticto assume that the backpropagated error is independent of the product σ (cid:4)(Shterms, so thati )δlj P l(cid:6)E∂ E D∂ whi jl(cid:7)(cid:6)(cid:2)(cid:2)= El>hk(cid:7)∂ E D∂ Slkwlhkiδhi(cid:10)Ei σ (cid:4)δh(cid:11)(cid:10)Shij O lδlj(cid:11)=(cid:2)(cid:2)El>hk(cid:6)∂ E D∂ Slk(cid:7)wlhki phi E(cid:11)(cid:10)(cid:10)i σ (cid:4)δhShij O lδlj(cid:11)(183)(cid:10)This approximation should be reasonable and increasingly accurate for units closer to the input layer, as the presence andactivity of these units bears vanishingly less influence on the output error. As in the case of the top layer, we can use afirst-order Taylor approximation to separate the dependent terms in Eq. (183) so that E(δhj) is approximatelyequal to(cid:10)δhiShli jWe can approximate E(σ (cid:4)(Shlσ (cid:4)(cid:4)(U hi j W lj whl(cid:10)σ (cid:4)≈ σ (cid:4)(U hi ) − σ (cid:4)(cid:4)(U h(cid:11)(cid:10)(cid:11)(cid:11)(cid:10)j O lδli j δ j O li j )) by σ (cid:4)(U hli j ) and use a similar Taylor expansion in reverse to get E(σ (cid:4)(Shli j E(O lj whli )pl(cid:11)(cid:16)(cid:10)σ (cid:4)j) so that(cid:11)(cid:10)i j )) ≈ σ (cid:4)(U hi σ (cid:4)(Sh+ σ (cid:4)(cid:4)− σ (cid:4)(cid:4)(cid:10)σ (cid:4)(cid:16)σ (cid:4)i ) −= ph≈ ph+ phi )pl(184)(185)i )δlj O li pli plσ (cid:4)(cid:4)j O lwhlwhlShli jShli jShli jO lji j EO lj Ej E(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)(cid:17)EEE(cid:10)(cid:10)(cid:10)(cid:11)(cid:11)(cid:11)(cid:11)(cid:10)(cid:11)(cid:10)(cid:10)(cid:10)(cid:10)(cid:11)(cid:17)jjjjU hij whlpli j EO lji plj EO lji plphj EU hiShli jO lj114P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122Collecting terms, finally gives(cid:10)(cid:11)(cid:11)(cid:10)i σ (cid:4)δhEShij O lδlj≈ phi plj(cid:10)(cid:16)σ (cid:4)U hi(cid:11)(cid:10)EO lj(cid:11)− σ (cid:4)(cid:4)(cid:11)(cid:10)U hij whlpli j E(cid:10)(cid:11)(cid:11)(cid:10)EO ljO lj(cid:10)(cid:10)σ (cid:4)(cid:4)+ EShli j(cid:11)(cid:11)whli j E(cid:10)O lj O ljor, by extracting the variance term,(cid:10)(cid:11)(cid:11)(cid:10)(cid:10)i σ (cid:4)δhEShij O lδlj≈ phi plj EO lj(cid:11)(cid:10)(cid:11)σ (cid:4)U hi+ phi σ (cid:4)(cid:4)(cid:11)(cid:10)U hiwhli j Var(cid:11)(cid:10)j O lδljCombining this result with Eq. (183) gives(cid:7)(cid:6)E∂ E D∂ whli j≈ ∂ E ENS∂ whli j+ whli j A(cid:11)(cid:17)(186)(187)(188)where A is an adaptive coefficient, proportional to σ (cid:4)(cid:4)(U hi )Var(δlj). Note that it is not obvious that A is always positive—a requirement for being a form of weight decay—especially since σ (cid:4)(cid:4)(x) is negative for x > 0.5 in the case of the standardsigmoid. Further analyses and simulations of these issues and the underlying approximations are left for future work.j O lIn conclusion, the approximations suggest that the gradient of the dropout approximation ensemble ∂ E ENS/∂ whli j and the expecta-tion of the gradient E(∂ E D /∂ whli j ) of the dropout network are similar. The difference is approximately a (weight decay) term linear inwhli j with a complex, adaptive coefficient, that varies during learning and depends on the variance of the pre-synaptic unit and on theinput. Thus dropout has a built in regularization effect that keeps the weights small. Furthermore, this regularization tends also to keepthe dropout variance of each unit small. This is a form of self-consistency since small variances ensure higher accuracy in the dropoutensemble approximations. Furthermore, since the dropout variance of a unit is minimized when all its inputs are 0, dropout has also abuilt-in propensity towards sparse representations.11.6. DropinIt is instructive to think about the apparently symmetric algorithm we call dropin where units are randomly and inde-pendently set to 1, rather than 0 as in dropout. Although superficially symmetric to dropout, simulations show that dropinbehaves very differently and in fact does not work. The reason can be understood in terms of the previous analyses sincesetting units to 1 tends to maximize variances, rather then minimizing them.11.7. Learning phases and sparse codingFinally, in light of these results, we can expect roughly three phases during dropout learning:1. At the beginning of learning, when the weights are random and very small, the total input to each unit is close to 0for all the units and the consistency is high: the output of the units remains roughly constant across subnetworks (andequal to 0.5 if the logistic coefficient is c = 1.0).2. As learning progresses, the sizes of the weights increase, activities tend to move towards 0 or 1, and the consistenciesdecreases, i.e. for a given input the dropout variance of the units across subnetworks increases, and more so for unitsthat move towards 1 than units that move towards 0. However, overall the regularization effect of dropout keeps theweights and variances small. To keep variances small, sparse representations tend to emerge.3. As learning converges, the consistency of the units stabilizes, i.e. for a given input the variance of the units acrosssubnetworks becomes roughly constant and small for units that have converged towards 1, and very small for units thathave converged towards 0. This is a consequence of the convergence of stochastic gradient.σ (Sh(cid:9)For simplicity, let us assume that dropout is carried only in layer h where the units have an output of the form O hii ) and Sh(cid:11)(cid:10)j is a constant since dropout is not applied to layer l. Thusj . For a fixed input, O l(cid:11)j whli j δl(cid:10)(cid:11)l<h(cid:2)(cid:10)j O l(cid:11)(cid:9)=(cid:10)i=VarShi=whli j2O lj2plj1 − plj(189)l<hunder the usual assumption that the selector variables δlj are independent of each other. A similar expression is obtained ifdropout is applied in the same way to the connections. Thus Var(Shi ), which ultimately influences the consistency of unit iin layer h, depends on three factors. Everything else being equal, it is reduced by: (1) Small weights which goes togetherwith the regularizing effect of dropout, or the random initial condition; (2) Small activities, which show that dropout isnot symmetric with respect to small or large activities, hence the failure of dropin. Overall, dropout tends to favor smallactivities and thus sparse coding; and (3) Small (close to 0) or large (close to 1) values of the dropout probabilities plj . Thesparsity and learning phases of dropout are demonstrated through simulations in Figs. 11.1, 11.2, and 11.3.P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122115Fig. 11.1. Empirical distribution of final neuron activations in each layer of the trained MNIST classifier demonstrating the sparsity. The empirical distribu-tions are combined over 1000 different input examples.Fig. 11.2. The three phases of learning. For a particular input, a typical active neuron (red) starts out with low dropout variance, experiences an increase invariance during learning, and eventually settles to some steady constant consistency value. A typical inactive neuron (blue) quickly learns to stay silent. Itsdropout variance grows only minimally from the low initial value. Curves correspond to mean activation with 5% and 95% percentiles. This is for a singlefixed input, and 1000 dropout Monte Carlo simulations. (For interpretation of the references to color in this figure legend, the reader is referred to the webversion of this article.)Fig. 11.3. Consistency of active neurons does not noticeably decline in the upper layers. ‘Active’ neurons are defined as those with activation greater than0.1 at the end of training. There were at least 100 active neurons in each layer. For these neurons, 1000 dropout simulations were performed at each timestep of 100 training epochs. The plot represents the dropout mean standard deviation and 5%, 95% percentiles computed over all the active neurons in eachlayer. Note that the standard deviation does not increase for the higher layers.116P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–12212. ConclusionWe have developed a general framework that has enabled the understanding of several aspects of dropout with goodmathematical precision. Dropout is an efficient approximation to training all possible sub-models of a given architecture andtaking their average. While several theoretical questions regarding both the static and dynamic properties of dropout requirefurther investigations, for instance its generalization properties, the existing framework clarifies the ensemble averagingproperties of dropout, as well as its regularization properties. In particular, it shows that the three standard approaches toregularizing large models and avoiding overfitting: (1) ensemble averaging; (2) adding noise; and (3) adding regularizationterms (equivalent to Bayesian priors) to the error functions, are all present in dropout and thus may be viewed in a moreunified manner.Dropout wants to produce robust units that do not depend on the details of the activation of other individual units. Asa result, it seeks to produce units with activities that have small dropout variance, across dropout subnetworks. This partialvariance minimization is achieved by keeping the weights small and using sparse encoding, which in turn increases theaccuracy of the dropout approximation and the degree of self-consistency. Thus, in some sense, by using small weights andsparse coding, dropout leads to large but energy efficient networks, which could potentially have some biological relevanceas it is well known that carbon-based computing is orders of magnitude more efficient than silicon-based computing.It is worth to consider which other classes of models, besides, linear and non-linear feedforward networks, may benefitfrom dropout. Some form of dropout ought to work, for instance, with Boltzmann machines or Hopfield networks. Further-more, while dropout has already been successfully applied to several real-life problems, many more remain to be tested.Among these, the problem of predicting quantitative phenotypic traits, such as height, from genetic data, such as singlenucleotide polymorphisms (SNPs), is worth mentioning. While genomic data is growing rapidly, for many complex traitswe are still in the ill-posed regime where typically the number of loci where genetic variation occurs exceeds the numberof training examples. Thus the best current models are typically highly (L1) regularized linear models, and these have hadlimited success. With its strong regularization properties, dropout is a promising algorithm that could be applied to thesequestions, using both simple linear or logistic regression models, as well as more complex models, with the potential foralso capturing epistatic interactions.Finally, at first sight dropout seems like another clever hack. More careful analysis, however reveals an underlying webof elegant mathematical properties. This mathematical structure is unlikely to be the result of chance alone and leads oneto suspect that dropout is more than a clever hack and that over time it may become an important concept for AI andmachine learning.AcknowledgementsWork supported in part by grants NSF IIS-0513376, NSF-IIS-1321053, NIH LM010235, and NIH NLM T15 LM07443. Wewish also to acknowledge a hardware grant from NVIDIA. We thank Julian Yarkony for feedback on the manuscript.Appendix A. Rectified linear transfer function without Gaussian assumptionHere we consider a rectified linear transfer function RE with threshold 0 and slope λ. If we assume that S is uniformlydistributed over the interval [−a, a] (similar considerations hold for intervals that are not symmetric), then μS = 0 andσS = a/3. We have RL(E(S)) = 0 and E(RL(S)) =(cid:22)0 λx(1/2a) dx = λa/4. In this casea(cid:20)(cid:10)(cid:20)RL(cid:11)E(S)− E(cid:10)RL(S)(cid:11)(cid:20)(cid:20) = λa4(190)This difference is small when the standard deviation is small, i.e. when a is small, and proportional to λ as in the Gaussiancase. Alternatively, one can also consider m input (dropout) values S 1, . . . , Sm with probabilities P 1, . . . , P m. We then have(cid:9)i P i S i (cid:2) 0i P i S i > 0(cid:18)(cid:10)RL(cid:11)E(S)=(cid:10)(cid:11)RL(S)E= λ(cid:9)ififi P i S i(cid:9)0λ(cid:2)P i S ii:S i>0andThus(cid:20)(cid:10)(cid:20)RL(cid:11)E(S)− E(cid:10)(cid:11)(cid:20)(cid:20) =RL(S)(cid:23)(cid:9)(cid:9)λλi:S i>0 P i S ii:S i(cid:2)0 P i|S i|In the usual case where P i = 1/m this yields(cid:10)(cid:9)(cid:23)(cid:20)(cid:10)(cid:20)RL(cid:11)E(S)− ERL(S)(cid:11)(cid:20)(cid:20) =λ 1mλ 1m(cid:9)i:S i >0 S ii:S i (cid:2)0|S i|(cid:9)(cid:9)i P i S i (cid:2) 0i P i S i > 0(cid:9)(cid:9)i S i (cid:2) 0i S i > 0ifififif(191)(192)(193)(194)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122117Again these differences are proportional to λ and it is easy to show they are small if the standard deviation is small using,for instance, Tchebycheff’s inequality.Appendix B. Expansion around the mean and around zero or oneB.1. Expansion around the meanUsing the same notation as in Section 8, we consider the outputs O 1, . . . , O m of a sigmoidal neuron with associatedi P i = 1) and O i = σ (S i). The difference here is that we expand around the mean and write(cid:9)probabilities P 1, . . . , P m (O i = E+(cid:8)i . As a result(cid:14)(cid:14)G =O P ii= Eii(cid:6)(cid:7)P i1 − (cid:8)iE(195)(196)(197)(198)(199)(200)(201)and(cid:4) =G(cid:14)(1 − O i)P i = (1 − E)i(cid:7)P i(cid:6)(cid:14)i1 − (cid:8)i1 − EIn order to use the Binomial expansion, we must further assume that for every i, |(cid:8)i| < min(E, 1 − E). In this case,+ P i(P i − 1)2(cid:13)+ R3((cid:8)i)1 + P iG = E= EP in(cid:8)iE(cid:8)iE(cid:8)iE∞(cid:2)(cid:7)n(cid:7)(cid:6)(cid:14)(cid:14)(cid:7)(cid:6)(cid:6)(cid:12)2in=0iwhere R3((cid:8)i) is the remainder of order three. Expanding and collecting terms gives(cid:12)G = E1 +(cid:2)iP i(cid:8)iE+(cid:2)iP i(P i − 1)2(cid:7)2(cid:6)(cid:8)iE+(cid:2)i(cid:3)= jP i P j(cid:8)iE(cid:8) jE(cid:13)+ R3((cid:8))(cid:9)Noting that(cid:12)G = Ei P i(cid:8)i = 0, we finally have≈ E − V1 − VE 22E(cid:13)+ R3((cid:8))and similarly by symmetry(cid:4) ≈ (1 − E) −GV2(1 − E)As a result,G + G(cid:4) ≈ 1 − 12VE(1 − E)VE(1−E)wherebining the results above yields(cid:2) 1 is a measure of how much the distribution deviates from the binomial case with the same mean. Com-NWGM = GG + G(cid:4)≈E − V2EVE(1−E)1 − 12(202)In general, this approximation is slightly more accurate than the approximation obtained in Section 8 by expanding around0.5 (Eq. (87)), as shown by Figs. 9.4 and 9.5, however its range of validity may be slightly narrower.B.2. Expansion around zero or oneConsider the expansion around one with O i = 1 − (cid:8)i , G =requires (cid:8)i < 1, which is satisfied for every O i . We have(cid:15)i(1 − (cid:8)i)P i , and G(cid:4) =(cid:15)i((cid:8)i)P i . The binomial expansionG =(cid:14)∞(cid:2)(cid:6)in=0(cid:7)P in(−1)n(cid:8)ni=i(cid:12)(cid:14)1 − P i(cid:8)i + P i(P i − 1)2(cid:13)+ R3((cid:8)i)(cid:8)2iwhere R3((cid:8)i) is the remainder of order three. Expanding and collecting terms givesG = E − 12V + R3((cid:8)) ≈ E − 12V(203)(204)P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122118andG(cid:4) ≈ 1 − E − 12VAs a result,G + G(cid:4) ≈ 1 − VThusandNWGM = GG + G(cid:4)≈ 2E − V2 − 2V(cid:6)E − NWGM ≈ −(cid:7)V1 − VE − 12This yields various approximate bounds|E − NWGM| (cid:4) 12V1 − V(cid:2) 12E(1 − E)1 − E(1 − E(cid:2) 16and|E − NWGM| (cid:4)(cid:20)(cid:20)(cid:20)E − 1(cid:20)2(cid:20)(cid:20)(cid:20)(cid:20)E(1 − E)1 − E(1 − E)(cid:2) 12E(1 − E)1 − E(1 − E(cid:2) 16(205)(206)(207)(208)(209)(210)Over the interval [0, 1], the function f (E) = E(1−E)1−E(1−E)E = 1, and reaches its maximum for E = 0.5 with f (0.5) = 1and Gand yields(cid:4)is positive and concave down. It satisfies f (E) = 0 for E = 0 and3 . Expansion around 0 is similar, interchanging the role of GNWGM ≈ 1 − E − 0.5V1 − Vfrom which similar bounds on |E − NWGM| can be derived.Appendix C. Higher order moments(211)It would be useful to have better estimates of the variance V and potentially also of higher order moments. We haveseen0 (cid:2) V (cid:2) E(1 − E) (cid:2) 0.25(212)Since V = E(O 2) − E(O )2 = E(O 2) − E 2, one would like to estimate E(O 2) or, more generally, E(O k) and it is tempting touse the NWGM approach, since we already know from the general theory that E(O k) ≈ NWGM(O k). This leads to(cid:10)(cid:11)O k=(cid:15)NWGM(cid:15)i(O ki )P i(cid:15)i(1 − O ki )P ii(O ki )P i +=(cid:15)1 +1(cid:10) (1−O i )(1+O i +···O k−1O kiii(cid:11))P iFor k = 2 this gives(cid:10)(cid:11)EO 2≈ NWGM(cid:10)(cid:11)=O 2(cid:15)(cid:10)i1(1−O i )O i1 +(1+O i )O i=(cid:11)P i1 + ce−λE(S)1(cid:15)i(2 + ce−λS i )P i(213)(214)However one would have to calculate exactly or approximately the last term in the denominator above. More or less equiv-alently, one can use the general fact that NWGM(σ ( f (S))) = σ (E( f (S))), which leads in particular to(cid:11)(cid:11)(cid:10)(cid:10)σSk(cid:11)(cid:11)(cid:10)(cid:10)ESk= σNWGMBy inverting the sigmoidal function, we haveS = 1λc O1 − Owhich can be expanded around E or around 0.5 using log(1 + u) =letting O = 05 + (cid:8), gives(215)(216)(cid:9)∞n=1(−1)n+1un/n for |u| < 1. Expanding around 0.5,S = 1λlog c + 1λ(cid:3)∞(cid:2)n=0(2(cid:8))2n+12n + 12P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122(cid:8)≈ 1λlog c + 4λ(cid:8)119(217)where the last approximation is obtained by retaining only up to second order terms in the expansion. Thus with thisapproximation, we have(cid:6)(cid:11)(cid:10)ES 2≈ E1λlog c + 4λ(cid:8)(cid:7)2(cid:6)= E1λlog c + 4λ(cid:7)2(O − 0.5)(218)We already have an estimate for E = E(O ) provided by NWGM(O ). Thus any estimate of E(S 2) obtained directly, or throughNWGM(σ (S 2)) by inverting Eq. (215), leads to an estimate of (O 2) through Eq. (218), and hence to an estimate of thevariance V . And similarly for all higher order moments.However, in all these cases, additional costly information seem to be required, in order to get estimates of V that aresharper than those in Eq. (212), and one might as well directly sample the values O i .Appendix D. Derivatives of the logistic function and their expectationsFor σ (x) = 1/(1 + ce−λx)2 and the−λx), the first order derivative is given by σ (cid:4)(x) = λσ (x)(1 − σ (x)) = λcesecond order derivative by σ (cid:4)(cid:4)(x) = λσ (x)(1 − σ (x))(1 − 2σ (x)). As expected, when λ > 0 the maximum of σ (cid:4)(x) is reachedwhen σ (x) = 0.5 and is equal to λ/4.As usual, let O i = σ (S i) for i = 1, . . . , m with corresponding probabilities P 1, . . . , P m. To approximate E(σ (cid:4)(S)), we can−λx/(1 + ceapply the definition of the derivative(cid:10)σ (cid:4)(cid:11)(S)E= E(cid:6)limh→0σ (S + h) − σ (S)h(cid:7)= limh→0E(σ (S + h)) − E(σ (S))h≈ limh→0σ (E(S) + h) − σ (E(S))h(219)using the NWGM approximation to the expectation. Note that the NWGM approximation requires 0 (cid:2) σ (cid:4)(S i) (cid:2) 1 for every i,which is always satisfied if λ (cid:2) 4. Using a first order Taylor expansion, we finally get:(cid:10)σ (cid:4)(cid:11)(S)E≈ limh→0σ (cid:4)(E(S))hh= σ (cid:4)(cid:10)(cid:11)E(S)To derive another approximation to E(σ (cid:4)(S)), we have(cid:10)σ (cid:4)(cid:11)(S)E≈ NWGM(cid:10)(cid:11)(S)σ (cid:4)=1 +(cid:15)i1(1−σ (cid:4)(S i ))P i(σ (cid:4)(S i))P i=(cid:15)(cid:10)i1λc eλS i + 2λ11 +− 1 + cλ e−λS i(cid:11)P i(220)(221)As in most applications, we assume now that c = λ = 1 to slightly simplify the calculations since the odd terms in theTaylor expansion of the two exponential functions in the denominator cancel each other. In this case(cid:10)σ (cid:4)(cid:11)(S)E≈ NWGM(cid:10)(cid:11)(S)σ (cid:4)=1 +(cid:15)(cid:10)i3 +1(cid:9)∞n=1=(cid:11)P i1 + 3(cid:15)(cid:10)i1 +1(cid:9)∞n=1(cid:11)P i2(λS i)2n3(2n)!2(λS i)2n(2n)!(222)Now different approximations can be derived by truncating the denominator. For instance, by retaining only the term cor-responding to n = 1 in the sum and using (1 + x)α ≈ 1 + αx for x small, we finally have the approximation(cid:10)σ (cid:4)(cid:11)(S)E≈14 + λ2 E(S 2i )=14 + λ2(Var(S) + (E(S))2)Appendix E. Distributions(223)Here we look at the distribution of O and S, where O = σ (S) under some simple assumptions.E.1. Assuming S has a Gaussian distributionUnder various probabilistic assumptions, it is natural to assume that the incoming sum S into a neuron has a Gaussiandistribution with mean μ and variance σ 2 with the densityfS (s) = 1√2π σ− (s−μ)22σ 2e(224)120P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122In this case, the distribution of O is given byF O (o) = P (O (cid:2) o) = PS (cid:2)(cid:6)−1λlog1 − oco(cid:7)λ log 1−o−1(cid:19)co=0−s−μ)2e2σ 2 ds1√2π σwhich yields the densityf O (o) = 1√2π σλ log 1−o−1co2σ 2− (e−μ)21λ1o(1 − o)(225)(226)In general this density is bell-shaped, similar but not identical to a beta density. For instance, if μ = 0 and λ = c = 1 = σf O (o) = 1√2π(1 − o)−1− 12 log 1−oo o−1+ 12 log 1−oo(227)E.2. The mean and variance of SConsider a sum of the form S =have mean μO and variance σ 2approximately Gaussian by the central limit theorem, with(cid:9)ni=1 w i O i . Assume that the weights have mean μw and variance σ 2w , the activitiesO , and the weights and the activities are independent of each other. Then, for n large, S isE(S) = nμwμOandVar(S) = nVar(w i O i) = n(cid:16)(cid:10)E(cid:11)(cid:11)(cid:10)EO 2iw 2i− E(w i)2 E(O i)2(cid:16)(cid:10)(cid:17)= nσ 2w+ μ2w(cid:11)(cid:10)σ 2O+ μ2O(cid:11)− μ2wmu2O(cid:17)In a typical case where μw = 0, the variance reduces toVar(S) = n(cid:10)(cid:16)σ 2wσ 2O+ μ2O(cid:11)(cid:17)E.3. Assuming O has a Beta distribution(228)(229)(230)The variable O is between 0 and 1 and thus it is natural to assume a Beta distribution with parameters a (cid:3) 0 and b (cid:3) 0with the densityf O (o) = B(a, b)oa−1(1 − o)b−1with the normalizing constant B(a, b) = Γ (a + b)/Γ (a)Γ (b). In this case, the distribution of S is given byF S (s) = P (S (cid:2) s) = P(cid:10)(cid:11)O (cid:2) σ (s)=σ (s)(cid:19)B(a, b)oa−1(1 − o)b−1 do0which yields the densityf S (s) = B(a, b)σ (s)a−1(cid:10)(cid:10)(cid:11)b−1λσ (s)1 − σ (s)(cid:11)1 − σ (s)= λB(a, b)σ (s)a(cid:10)(cid:11)b1 − σ (s)(231)(232)(233)In general this density is bell-shaped, similar but not identical to a Gaussian density. For instance, in the balanced casewhere a = b,f S (s) = λB(a, b)σ (s)a(cid:10)(cid:11)a = λB(a, b)1 − σ (s)(cid:6)(cid:7)ace−λs(1 + ce−λs)2(234)Note, for instance, how this density at +∞ decays exponentially like ea quadratic one as in the exact Gaussian case.−λas with a linear term in the exponent, rather thanP. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122121Appendix F. Alternative estimate of the expectationHere we describe an alternative way for obtaining a closed form estimate of E(O ) when O = σ (S) and S has a GaussianS , which is a reasonable assumption in the case of dropout applied to largedistribution with mean μS and variance σ 2networks. It is known that the logistic function can be approximated by a cumulative Gaussian distribution in the form11 + e−S≈ Φ0,1(α S)(235)(cid:22)1√x−∞−t2/2 dt for a suitable value of α. Depending on the optimization criterion, different but rea-where Φμ,σ 2 (x) =sonably close values of α can be found in the literature such as α = 0.607 [21] or α = 1/1.702 = 0.587 [15]. Just equating√the first derivatives of the two functions at S = 0 gives α =2π /4 ≈ 0.626. In what follows, we will use α = 0.607. In anycase, for the more general logistic case, we have2π σe11 + ce−λS≈ Φ0,1(cid:10)(cid:11)α(λS − log c)As a result, in the general case,+∞(cid:19)E(O ) ≈Φ0,1−∞(cid:10)(cid:11)α(λS − log c)− (S−μS )22σ 2sedS1√2πσSIt is easy to check that(cid:6)(cid:7)−μσ= Φμ,σ 2 (0)Φ0,1Thus+∞(cid:19)E(O ) ≈P (Z < 0|S)−∞− (S−μS )22σ 2se1√2πσSdS = P (Z < 0)(236)(237)(238)(239)where Z |S is normally distributed with mean −λS + log c and variance 1/α2. Thus Z is normally distributed with mean−λμs + log c and variance σ 2S+ α−2, and the expectation can be estimated byE(O ) ≈ P (Z < 0) = Φ−λμs+log c,σ 2S+α−2 (0) = Φ0,1(cid:7)(cid:6)λμS − log c+ α−2σ 2SFinally, using in reverse the logistic approximation to the cumulative Gaussian distribution, we have(cid:6)E(O ) ≈ Φ0,1(cid:7)≈λμS − log c+ α−2σ 2S1 + eIn the usual case where c = λ = 1 this gives1− 1αλμS(cid:24)σ 2S−log c+α−2=1 + e1− λμS(cid:24)−log cS α21+σ 2E(O ) ≈1S α2)−1/2μS−(1+σ 2≈1−(1+0.368σ 2S )−1/2μS1 + e1 + e(240)(241)(242)using α = 0.607 in the last approximation. In some cases this approximation to E(O ) may be more accurate than theNWGMS approximation but there is a tradeoff. This approximation requires a normal assumption on S, as well as knowingboth the mean and the variance of S, whereas the NWGM approximation uses only the mean of S in the form E(O ) ≈NWGM(O ) = σ (E(S)). For small values of σ 2S , the estimatein Eq. (242) converges to 0.5 whereas the NWGM could be arbitrarily close to 0 or 1 depending on the values of E(S)μS . Inpractice this is not observed because the size of the weights remains limited due to the dropout regularization effect, andthus the variance of S is also bounded.S the two approximations are similar. For very large values of σ 2Note that for non-Gaussian distributions, artificial cases can be constructed where the discrepancy between E and theNWGM is even larger and goes all the way to 1. For example there is a large discrepancy for S = −1/(cid:8) with probability1 − (cid:8) and S = 1/(cid:8)3 with probability (cid:8), and (cid:8) close to 0. In this case E(O ) ≈ 0 and NWGM ≈ 1.122P. Baldi, P. Sadowski / Artificial Intelligence 210 (2014) 78–122References[1] J. Aldaz, Self improvement of the inequality between arithmetic and geometric means, J. Math. Inequal. 3 (2) (2009) 213–216.[2] J. Aldaz, Sharp bounds for the difference between the arithmetic and geometric means, arXiv preprint, arXiv:1203.4454, 2012.[3] N. Alon, J.H. Spencer, The Probabilistic Method, John Wiley & Sons, 2004.[4] H. Alzer, A new refinement of the arithmetic mean geometric mean inequality, J. Math. 27 (3) (1997).[5] H. Alzer, Some inequalities for arithmetic and geometric means, Proc. R. Soc. Edinb., Sect. A, Math. 129 (02) (1999) 221–228.[6] G. An, The effects of adding noise during backpropagation training on a generalization performance, Neural Comput. 8 (3) (1996) 643–674.[7] J. Ba, B. Frey, Adaptive dropout for training deep neural networks, in: C. Burges, L. Bottou, M. Welling, Z. Ghahramani, K. Weinberger (Eds.), Advancesin Neural Information Processing Systems, vol. 26, 2013, pp. 3084–3092.[8] P. Baldi, K. Hornik, Neural networks and principal component analysis: Learning from examples without local minima, Neural Netw. 2 (1) (1988) 53–58.[9] P. Baldi, K. Hornik, Learning in linear networks: a survey, IEEE Trans. Neural Netw. 6 (4) (1994) 837–858.[10] P. Baldi, P.J. Sadowski, Understanding dropout, in: Advances in Neural Information Processing Systems, vol. 26, 2013, pp. 2814–2822.[11] E.F. Beckenbach, R. Bellman, Inequalities, Springer-Verlag, Berlin, 1965.[12] C.M. Bishop, Training with noise is equivalent to Tikhonov regularization, Neural Comput. 7 (1) (1995) 108–116.[13] L. Bottou, Online algorithms and stochastic approximations, in: D. Saad (Ed.), Online Learning and Neural Networks, Cambridge University Press,Cambridge, UK, 1998.[14] L. Bottou, Stochastic learning, in: O. Bousquet, U. von Luxburg (Eds.), Advanced Lectures on Machine Learning, in: Lecture Notes in Artificial Intelligence,vol. 3176, Springer Verlag, Berlin, 2004, pp. 146–168.[15] S.R. Bowling, M.T. Khasawneh, S. Kaewkuekool, B.R. Cho, A logistic approximation to the cumulative normal distribution, J. Ind. Eng. Manag. 2 (1)(2009) 114–127.[16] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.[17] L. Breiman, Bagging predictors, Mach. Learn. 24 (2) (1996) 123–140.[18] C. Carr, M. Konishi, A circuit for detection of interaural time differences in the brain stem of the barn owl, J. Neurosci. 10 (10) (1990) 3227–3246.[19] C.E. Carr, M. Konishi, Axonal delay lines for time measurement in the owl’s brainstem, Proc. Natl. Acad. Sci. 85 (21) (1988) 8311–8315.[20] D. Cartwright, M. Field, A refinement of the arithmetic mean-geometric mean inequality, Proc. Am. Math. Soc. (1978) 36–38.[21] D.D.R. Cox, The Analysis of Binary Data, vol. 32, CRC Press, 1989.[22] P. Diaconis, Bayesian numerical analysis, in: Statistical Decision Theory and Related Topics IV, vol. 1, 1988, pp. 163–175.[23] R.O. Duda, P.E. Hart, D.G. Stork, Pattern Classification, second ed., Wiley, New York, NY, 2000.[24] D. Gardner, Noise modulation of synaptic weights in a biological neural network, Neural Netw. 2 (1) (1989) 69–76.[25] S.J. Hanson, A stochastic version of the delta rule, Physica D 42 (1) (1990) 265–272.[26] G. Harnischfeger, G. Neuweiler, P. Schlegel, Interaural time and intensity coding in superior olivary complex and inferior colliculus of the echolocatingbat molossus ater, J. Neurophysiol. 53 (1) (1985) 89–109.[27] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R.R. Salakhutdinov, Improving neural networks by preventing co-adaptation of feature detectors,http://arxiv.org/abs/1207.0580, 2012.[28] N. Levinson, Generalization of an inequality of Ky Fan, J. Math. Anal. Appl. 8 (1) (1964) 133–134.[29] L. Maaten, M. Chen, S. Tyree, K.Q. Weinberger, Learning with marginalized corrupted features, in: Proceedings of the 30th International Conference onMachine Learning (ICML-13), 2013, pp. 410–418.[30] K. Matsuoka, Noise injection into inputs in back-propagation learning, IEEE Trans. Syst. Man Cybern. 22 (3) (1992) 436–440.[31] A.M. Mercer, Improved upper and lower bounds for the difference an-gn, J. Math. 31 (2) (2001).[32] P.R. Mercer, Refined arithmetic, geometric and harmonic mean inequalities, J. Math. 33 (4) (2003).[33] M. Mitzenmacher, E. Upfal, Probability and Computing: Randomized Algorithms and Probabilistic Analysis, Cambridge University Press, 2005.[34] A.F. Murray, P.J. Edwards, Enhanced mlp performance and fault tolerance resulting from synaptic weight noise during training, IEEE Trans. Neural Netw.5 (5) (1994) 792–802.[35] E. Neuman, J. Sándor, On the Ky Fan inequality and related inequalities i, Math. Inequal. Appl 5 (2002) 49–56.[36] E. Neuman, J. Sandor, On the Ky Fan inequality and related inequalities ii, Bull. Aust. Math. Soc. 72 (1) (2005) 87–108.[37] Y. Raviv, N. Intrator, Bootstrapping with noise: An effective regularization technique, Connect. Sci. 8 (3–4) (1996) 355–372.[38] H. Robbins, D. Siegmund, A convergence theorem for non negative almost supermartingales and some applications, in: Optimizing Methods in Statistics,1971, pp. 233–257.[39] R.T. Rockafellar, Convex Analysis, vol. 28, Princeton University Press, 1997.[40] D. Rumelhart, G. Hintont, R. Williams, Learning representations by back-propagating errors, Nature 323 (6088) (1986) 533–536.[41] D.J. Spiegelhalter, S.L. Lauritzen, Sequential updating of conditional probabilities on directed graphical structures, Networks 20 (5) (1990) 579–605.[42] P. Vincent, H. Larochelle, Y. Bengio, P. Manzagol, Extracting and composing robust features with denoising autoencoders, in: Proceedings of the 25thInternational Conference on Machine Learning, ACM, 2008, pp. 1096–1103.[43] S. Wager, S. Wang, P. Liang, Dropout training as adaptive regularization, in: C. Burges, L. Bottou, M. Welling, Z. Ghahramani, K. Weinberger (Eds.),Advances in Neural Information Processing Systems, vol. 26, 2013, pp. 351–359.