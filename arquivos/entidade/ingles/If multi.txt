Artificial Intelligence 171 (2007) 365–377www.elsevier.com/locate/artintIf multi-agent learning is the answer,what is the question?Yoav Shoham ∗, Rob Powers, Trond GrenagerDepartment of Computer Science, Stanford University, Stanford, CA 94305, USAReceived 8 November 2005; received in revised form 14 February 2006; accepted 16 February 2006Available online 30 March 2007AbstractThe area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory andartificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendasthat ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result infirmer foundations for the area.1© 2007 Published by Elsevier B.V.1. IntroductionThe topic of learning in multi-agent systems, or multi-agent learning (MAL henceforth), has a long history ingame theory, almost as long as the history of game theory itself.2 As early as 1951, fictitious play [10] was proposedas a learning algorithm for computing equilibria in games and there have been proposals for how to evaluate thesuccess of learning rules going back to [23] and [5]. Since that time hundreds, if not thousands, of articles have beenpublished on the topic, and at least two books ([20] and [54]).In Artificial Intelligence (AI) the history of single-agent learning is as rich if not richer, with thousands of articles,many books, and some very compelling applications in a variety of fields (for some examples see [29,40], or [50]).While it is only in recent years that AI has branched into the multi-agent aspects of learning, it has done so with* Corresponding author.E-mail addresses: shoham@cs.stanford.edu (Y. Shoham), powers@cs.stanford.edu (R. Powers), grenager@cs.stanford.edu (T. Grenager).1 This article has a long history and owes many debts. A first version was presented at the NIPS workshop, Multi-Agent Learning: Theory andPractice, in 2002. A later version was presented at the AAAI Fall Symposium in 2004 [Y. Shoham, R. Powers, T. Grenager, On the agenda(s) ofresearch on multi-agent learning, in: AAAI 2004 Symposium on Artificial Multi-Agent Learning (FS-04-02), AAAI Press, 2004]. Over time ithas gradually evolved into the current form, as a result of our own work in the area as well as the feedback of many colleagues. We thank themall collectively, with special thanks to members of the multi-agent group at Stanford in the past three years. Rakesh Vohra and Michael Wellmanprovided detailed comments on the latest draft which resulted in substantive improvements, although we alone are responsible for the views putforward. This work was supported by NSF ITR grant IIS-0205633 and DARPA grant HR0011-05-1.2 Another more recent term for the area within game theory is interactive learning.0004-3702/$ – see front matter © 2007 Published by Elsevier B.V.doi:10.1016/j.artint.2006.02.006366Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377something of a vengeance. If in 2003 one could describe the AI literature on MAL by enumerating the relevant articles,today this is no longer possible. The leading conferences routinely feature articles on MAL, as do the journals.3While the AI literature maintains a certain flavor that distinguishes it from the game theoretic literature, thecommonalities are greater than the differences. Indeed, alongside the area of mechanism design, and perhaps thecomputational questions surrounding solution concepts such as the Nash equilibrium, MAL is today arguably one ofthe most fertile interaction grounds between computer science and game theory.The MAL research in both fields has produced some inspiring results. We will not repeat them here, since we cannotbe comprehensive in this article, but nothing we say subsequently should be interpreted as belittling the achievementsin the area. Yet alongside these successes there are some indications that it could be useful to take a step back and aska few basic questions about the area of MAL. One surface indication is the presence of quite a number of frustratingdead ends. For example, the AI literature attempting to extend Bellman-style single-agent reinforcement learning tech-niques (in particular, Q-learning [53]) to the multi-agent setting, has fared well in zero-sum repeated games (e.g., [36]and [38]) as well as common-payoff (or ‘team’) repeated games (e.g., [14,31,52]), but less well in general-sum sto-chastic games (e.g., [21,26,37]) (for the reader unfamiliar with this line of work, we cover it briefly in Section 4).Indeed, upon close examination, it becomes clear that the very foundations of MAL could benefit from explicit dis-cussion. What exact question or questions is MAL addressing? What are the yardsticks by which to measure answersto these questions? The present article focuses on these foundational questions.To start with the punch line, following an extensive look at the literature we have reached two conclusions:• There are several different agendas being pursued in the MAL literature. They are often left implicit and conflated;the result is that it is hard to evaluate and compare results.• We ourselves can identify and make sense of five distinct research agendas.Not all work in the field falls into one of the five agendas we identify. This is not necessarily a critique of workthat doesn’t; it simply means that one must identify yet other well-motivated and well defined problems addressedby that work. We expect that as a result of our throwing down the gauntlet additional such problems will be defined,but also that some past work will be re-evaluated and reconstructed. Certainly we hope that future work will alwaysbe conducted and evaluated against well-defined criteria, guided by this article and the discussion engendered by itamong our colleagues in AI and game theory. In general we view this article not as a final statement but as the start ofa discussion.In order to get to the punch line outlined above, we proceed as follows. In the next section we define the formalsetting on which we focus. In Section 3 we illustrate why the question of learning in multi-agent settings is inherentlymore complex than in the single-agent setting, and why it places a stress on basic game theoretic notions. In Section 4we provide some concrete examples of MAL approaches from both game theory and AI. This is anything but a com-prehensive coverage of the area, and the selection is not a value judgment. Our intention is to anchor the discussionin something concrete for the benefit of the reader who is not familiar with the area, and—within the formal confineswe discuss in Section 2—the examples span the space of MAL reasonably well. In Section 5 we identify five differentagendas that we see (usually) implicit in the literature, and which we argue should be made explicit and teased apart.We end in Section 6 with a summary of the main points made in this article.A final remark is in order. The reader may find some of the material in the next three sections basic or obvious;different readers will probably find different parts so. We don’t mean to insult anyone’s intelligence, but we err on theside of explicitness for two reasons. First, this article is addressed to at least two different communities with somewhatdifferent backgrounds. Second, our goal is to contribute to the clarification of foundational issues; we don’t want tobe guilty of vagueness ourselves.3 We acknowledge a simplification of history here. There is definitely MAL work in AI that predates the last few years, though the relative delugeis indeed recent. Similarly, we focus on AI since this is where most of the action is these days, but there are also other areas in computer sciencethat feature MAL material; we mean to include that literature here as well.Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–3773672. The formal settingWe will couch our discussion in the formal setting of stochastic games (aka Markov games). Most of the MALliterature adopts this setting, and indeed most of it focuses on the even more narrow class of repeated games. Fur-thermore, stochastic games also generalize Markov Decision Problems (MDPs), the setting from which much of therelevant learning literature in AI originates. These are defined as follows.A stochastic game can be represented as a tuple: (N, S, (cid:3)A, (cid:3)R, T ). N is a set of agents indexed 1, . . . , n. S is a setof n-agent stage games. (cid:3)A = A1, . . . , An, with Ai the set of actions (or pure strategies) of agent i (note that weassume the agent has the same strategy space in all games; this is a notational convenience, but not a substantiverestriction). (cid:3)R = R1, . . . , Rn, with Ri : S × (cid:3)A → R giving the immediate reward function of agent i for stage game S.T : S × (cid:3)A → Π(S) is a stochastic transition function, specifying the probability of the next stage game to be playedbased on the game just played and the actions taken in it.We also need to define a way for each agent to aggregate the set of immediate rewards received in each state. Forfinitely repeated games we can simply use the sum or average, while for infinite games the most common approachest=1 δt rt , where rt is the reward received atare to use either the limit average or the sum of discounted awardstime t.(cid:2)∞A repeated game is a stochastic game with only one stage game, while an MDP is a stochastic game with only oneagent.While most of the MAL literature lives happily in this setting, we would be remiss not to acknowledge the literaturethat does not. Certainly one could discuss learning in the context of extensive-form games of incomplete and/orimperfect information (cf. [28]). We don’t dwell on those since it would distract from the main discussion, and sincethe lessons we draw from our setting will apply there as well.Although we will not specifically include them, we also intend our comments to apply at a general level to largepopulation games and evolutionary models, and particularly replicator dynamics (RD) [47] and evolutionary stablestrategies (ESS) [49]. These are defined as follows. The replicator dynamic model assumes a population of homoge-neous agents each of which continuously plays a two-player game against every other agent. Formally the setting canbe expressed as a tuple (A, P0, R). A is the set of possible pure strategies/actions for the agents indexed 1, . . . , m. P0mi=1 P0(i) = 1. R : A × A → R is the immediate rewardis the initial distribution of agents across possible strategies,function for each agent with R(a, a(cid:6)) giving the reward for an agent playing strategy a against another agent playingstrategy a(cid:6). The population then changes proportions according to how the reward for each strategy compares to theaverage reward: dt (Pt (a)) = Pt (a)[ut (a) − u∗a Pt (a)ut (a). A strat-tegy a is then defined to be an evolutionary stable strategy if and only if for some (cid:4) > 0 and for all other strategies a(cid:6),R(a, (1 − (cid:4))a + (cid:4)a(cid:6)) > R(a(cid:6), (1 − (cid:4))a + (cid:4)a(cid:6)).a(cid:6) Pt (a(cid:6))R(a, a(cid:6)) and u∗], where ut (a) =(cid:2)(cid:2)(cid:2)=tAs the names suggest, one way to interpret these settings is as building on population genetics, that is, as represent-ing a large population undergoing frequent pairwise interactions. An alternative interpretation however is as a repeatedgame between two agents, with the distribution of strategies in the population representing the agent’s mixed strategy(in the homogeneous definition above the two agents have the same mixed strategy, but there exist more general defi-nitions with more than two agents and with non-identical strategies). The second interpretation reduces the setting tothe one we discuss. The first bears more discussion, and we do it briefly in Section 4.And so we stay with the framework of stochastic games. What is there to learn in these games? Here we need to beexplicit about some aspects of stochastic games that were glossed over so far. Do the agents know the stochastic game,including the stage games and the transition probabilities? If not, do they at least know the specific game being playedat each stage, or only the actions available to them? What do they see after each stage game has been played—onlytheir own rewards, or also the actions played by the other agent(s)? Do they perhaps magically see the other agent(s)’mixed strategy in the stage game? And so on.In general, games may be known or not, play may be observable or not, and so on. We will focus on known,fully observable games, where the other agent’s strategy (or agents’ strategies) is not known a priori (though in somecase there is a prior distribution over it). In our restricted setting there two possible things to learn. First, the agentcan learn the opponent’s (or opponents’) strategy (or strategies), so that the agent can then devise a best (or at leasta good) response. Alternatively, the agent can learn a strategy of his own that does well against the opponents, withoutexplicitly learning the opponent’s strategy. The first is sometimes called model-based learning, and the second model-free learning.368Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377In broader settings there is more to learn. In particular, with unknown games, one can learn the game itself. Somewill argue the restricted setting is not a true learning setting, but (a) much of the current work on MAL, particularlyin game theory, takes place in this setting, and (b) the foundational issues we wish to tackle surface already here. Inparticular, our comments are intended to also apply to the work in the AI literature on games with unknown payoffs,work which builds on the success of learning in unknown MDPs. We will have more to say about the nature of‘learning’ in the setting of stochastic games in the following sections.3. On some special characteristics of multi-agent learningBefore launching into specifics, we wish to highlight the special nature of MAL. There are two messages we wouldlike to get across, one aimed at AI researchers specifically and one more broadly. Both lessons can be gleaned fromsimple and well-known examples.Consider the game described in Fig. 1. In this game the row player has a strictly dominant strategy, Down, andso seemingly there is not much more to say about this game. But now imagine a repeated version of this game. Ifthe row player indeed repeatedly plays Down, assuming the column player is paying any attention, he (the columnplayer) will start responding with Left, and the two will end up with a repeated (Down, Left) play. If, however, the rowplayer starts repeatedly playing Up, and again assuming the column player is awake, he may instead start respondingby playing Right, and the two players will end up with a repeated (Up, Right) play. The lesson from this is simpleyet profound: In a multi-agent setting one cannot separate learning from teaching. In this example, by playing hisdominated strategy, the row player taught the column player to play in a way that benefits both. Indeed, for this reasonit might be more appropriate to speak more neutrally about multi-agent adaptation rather than learning. We will notfight this linguistic battle, but the point remains important, especially for computer scientists who are less accustomedto thinking about interactive considerations than game theorists. In particular, it follows there is no a priori reason toexpect that machine learning techniques that have proved successful in AI for single-agent settings will also proverelevant in the multi-agent setting.The second lesson we draw from the well-known game of Rochambeau, or Rock-Paper-Scissors, given in Fig. 2.As is well known, this zero-sum game has a unique Nash equilibrium in which each player randomizes uniformlyamong the three strategies. One could conclude that there is not much more to say about the game. But suppose youentered a Rochambeau tournament. Would you simply adopt the equilibrium strategy?If you did, you would not win the competition. This is no idle speculation; such competitions take place routinely.For example, starting in 2002, the World Rock Papers Scissors Society (WRPS) standardized a set of rules for in-ternational play and has overseen annual International World Championships as well as many regional and nationalevents throughout the year. These championships have been attended by players from around the world and have at-tracted widespread international media attention. The winners are never equilibrium players. For example, on October25th, 2005, 495 people entered the competition in Toronto from countries as diverse as Norway, Northern Ireland,the Cayman Islands, Australia, New Zealand and the UK. The winner was Toronto Lawyer Andrew Bergel, who beatFig. 1. Stackelberg stage game: The payoff for the row player is given first in each cell, with the payoff for the column player following.Fig. 2. Rock-Paper-Scissors.Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377369Californian Stan Long in the finals. His strategy? “[I] read the minds of my competitors and figure out what they werethinking. I don’t believe in planning your throws before you meet your opponent”.These tournaments of course are not a perfect match with the formal model of repeated games. However, weinclude the example not only for entertainment value. The rules of the RPS tournaments call for ‘matches’ betweenplayers, each match consisting of several ‘games’, where a game is a single play of RPS. The early matches adopteda “best of three of three” format, meaning that the player who wins a best of three set garners one point and requirestwo points to take the match. The Semi-Finals and the Final Match used the “best of three of five” format, meaningthat the player who wins a best of three set garners one point and requires three points to take the match. And so thecompetition really consisted of a series of repeated games, some of them longer than others.4Entertainment aside, what do we learn from this? We believe that this is a cautioning tale regarding the predictiveor prescriptive role of equilibria in complex games, and in particular in repeated games. There are many examplesof games with complex strategy spaces, in which equilibrium analysis plays little or no role—including familiarparlor games, or the Trading Agent Competition (TAC), a computerized trading competition.5 The strategy space ina repeated game (or more generally a stochastic game) is immense—all mappings from past history to mixed strategiesin the stage game. In such complex games it is not reasonable to expect that players contemplate the entire strategyspace—their own or that of the opponent(s). Thus, (e.g., Nash) equilibria don’t play here as great a predictive orprescriptive role.Our cautioning words should be viewed as countering the default blind adoption of equilibria as the driving conceptin complex games, but not as a sweeping statement against the relevance of equilibria in some cases. The simpler thestage game, and the longer its repetition, the more instructive are the equilibria. Indeed, despite our example above,we do believe that if only two players play a repeated RPS game for long enough, they will tend to converge to theequilibrium strategy (this is particularly true of computer programs, that don’t share the human difficulty with throwinga mental die). Even in more complex games there are examples where computer calculation of approximate equilibriawithin a restricted strategy space provided valuable guidance in constructing effective strategies. This includes thegame of Poker ([33] and [4]), and even, as an exception to the general rule we mentioned, one program that competedin the Trading Agent Competition [13]. Our point has only been that in the context of complex games, so-called“bounded rationality”, or the deviation from the ideal behavior of omniscient agents, is not an esoteric phenomenonto be brushed aside.4. A (very partial) sample of MAL workTo make the discussion concrete, it is useful to look at MAL work over the years. The selection that follows isrepresentative but very partial; no value judgment or other bias are intended by this selection. The reader familiar withthe literature may wish to skip to Section 4.3, where we make some general subjective comments.Unless we indicate otherwise, our examples are drawn from the special case of repeated, two-person games (asopposed to stochastic, n-player games). We do this both for ease of exposition, and because the bulk of the literatureindeed focuses on this special case.We divide the coverage into three parts: techniques, results, and commentary.4.1. Some MAL techniquesWe will discuss three classes of techniques—one representative of work in game theory, one more typical of workin AI, and one that seems to have drawn equal attention from both communities.4 We do acknowledge some degree of humor in the example. The detailed rules in http://www.rpschamps.com/rules.html make for additionalentertaining reading; of note is the restriction of the strategy space to Rock, Paper and Scissors, and explicitly ruling out others: “Any use of Dy-namite, Bird, Well, Spock, Water, Match, Fire, God, Lightning, Bomb, Texas Longhorn, or other non-sanctioned throws, will result in automaticdisqualification”. The overview of the RPS society and its tournaments is adapted from the inimitable Wikipedia, the collaborative online ency-clopedia, as available on January 2, 2006. Wikipedia goes on to list the champions since 2002; we note without comment that they are all maleTorontonians. The results of the specific competition cited are drawn from the online edition of the Boise Weekly dated November 2, 2005. TheBoise Weekly starts the piece with “If it weren’t true, we wouldn’t report on it”.5 http://tac.eecs.umich.edu.370Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–3774.1.1. Model-based approachesThe first approach to learning we discuss, which is common in the game theory literature, is the model-based one.It adopts the following general scheme:1. Start with some model of the opponent’s strategy.2. Compute and play the best response.3. Observe the opponent’s play and update your model of her strategy.4. Goto step 2.Among the earliest, and probably the best-known, instance of this scheme is fictitious play [10]. The model issimply a count of the plays by the opponent in the past. The opponent is assumed to be playing a stationary strategy,and the observed frequencies are taken to represent the opponent’s mixed strategy. Thus after five repetitions ofthe Rochambeau game in which the opponent played (R, S, P , R, P ), the current model of her mixed strategy is(R = 0.4, P = 0.4, S = 0.2).There exist many variants of the general scheme, for example those in which one does not play the exact bestresponse in step 2. This is typically accomplished by assigning a probability of playing each pure strategy, assigningthe best response the highest probability, but allowing some chance of playing any of the strategies. A number ofproposals have been made of different ways to assign these probabilities such as smooth fictitious play [18] andexponential fictitious play [19].A more sophisticated version of the same scheme is seen in rational learning [30]. The model is a distribution overthe repeated-game strategies. One starts with some prior distribution; for example, in a repeated Rochambeau game,the prior could state that with probability 0.5 the opponent repeatedly plays the equilibrium strategy of the stage game,and, for all k > 1, with probability 2−k she plays R k times and then reverts to the repeated equilibrium strategy. Aftereach play, the model is updated to be the posterior obtained by Bayesian conditioning of the previous model. Forinstance, in our example, after the first non-R play of the opponent, the posterior places probability 1 on the repeatedequilibrium play.4.1.2. Model-free approachesAn entirely different approach that has been commonly pursued in the AI literature [29], is the model-free one,which avoids building an explicit model of the opponent’s strategy. Instead, over time one learns how well one’sown various possible actions fare. This work takes place under the general heading of reinforcement learning,6 andmost approaches have their roots in the Bellman equations [3]. The basic algorithm for solving for the best policy ina known MDP starts by initializing a value function, V0 : S → R, with a value for each state in the MDP. The valuefunction can then be iteratively updated using the Bellman equation:Vk+1 ← R(s) + γ maxaT (s, a, s(cid:6))Vk(s(cid:6))(cid:3)s(cid:6)The optimal policy can then be obtained by selecting the action, a, at each state, s, that maximizes the expected value:(cid:2)s(cid:6) T (s, a, s(cid:6))Vk(s(cid:6)). Much of the work in AI has focused strategies for rapid convergence, on very large MDPs,and in particular on unknown and partially observable MDPs. While this is not our focus, we do briefly discussthe unknown case, since this is where the literature leading to many of the current approaches for stochastic gamesoriginated.For MDPs with unknown reward and transition functions, the Q-learning algorithm [53] can be used to computean optimal policy.Q(s, a) ← (1 − αt )Q(s, a) + αtV (s) ← maxQ(s, a)a∈A(cid:4)R(s, a) + γ V (s(cid:6)(cid:5))As is well known, with certain assumptions about the way in which actions are selected at each state over time andconstraints on the learning rate schedule, αt , Q-learning can be shown to converge to the optimal value function V ∗.6 We note that the term is used somewhat differently in the game theory literature.Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377371The Q-learning algorithm can be extended to the multi-agent stochastic game setting by having each agent simplyignore the other agents and pretend that the environment is passive:Qi(s, ai) ← (1 − αt )Qi(s, ai) + αtVi(s) ← maxQi(s, ai)ai ∈Ai(cid:4)Ri(s, (cid:3)a) + γ Vi(s(cid:6)(cid:5))Several authors have tested variations of the basic Q-learning algorithm for MAL (e.g., [48]). However, this ap-proach ignores the multi-agent nature of the setting entirely. The Q-values are updated without regard for the actionsselected by the other agents. While this can be justified when the opponents’ distributions of actions are stationary, itcan fail when an opponent may adapt its choice of actions based on the past history of the game.A first step in addressing this problem is to define the Q-values as a function of all the agents’ actions:Qi(s, (cid:3)a) ← (1 − α)Qi(s, (cid:3)a) + α(cid:4)Ri(s, (cid:3)a) + γ Vi(s(cid:6)(cid:5))We are however left with the question of how to update V , given the more complex nature of the Q-values.For (by definition, two-player) zero-sum SGs, Littman suggests the minimax-Q learning algorithm, in which V isupdated with the minimax of the Q values [36]:(cid:6)(cid:7)s, (a1, a2)V1(s) ← maxP1(a1)Q1(cid:3)P1∈Π(A1)mina2∈A2a1∈A1Later work (such as the joint-action learners in [14] and the Friend-or-Foe Q algorithm in [37]) proposed otherupdate rules for the Q and V functions focusing on the special case of common-payoff (or ‘team’) games. A stagegame is common-payoff if at each outcome all agents receive the same payoff. The payoff is in general different indifferent outcomes, and thus the agents’ problem is that of coordination; indeed these are also called games of purecoordination.The work on zero-sum and common-payoff games continues to be refined and extended (e.g., [8,32,34,52]). Muchof this work has concentrated on provably optimal tradeoffs between exploration and exploitation in unknown, zero-sum games; this is a fascinating topic, but not germane to our focus. More relevant are the most recent efforts in thisline of research to extend the “Bellman heritage” to general-sum games (e.g., Nash-Q by [25] and CE-Q by [21]). Wedo not cover these for two reasons: The description is more involved, and the results have been less satisfactory; moreon the latter below.4.1.3. Regret minimization approachesOur third and final example of prior work in MAL is no-regret learning. It is an interesting example for tworeasons. First, it has some unique properties that distinguish it from the work above. Second, both the AI and gametheory communities appear to have converged on it independently. The basic idea goes back to early work on how toevaluate the success of learning rules [5,23], and has since been extended and rediscovered numerous times over theyears under the names of universal consistency, no-regret learning, and the Bayes envelope (see [16] for an overviewof this history). We will describe the algorithm proposed in [24] as a representative of this body of work. We start bydefining the regret, r ti (aj , si) of agent i for playing the sequence of actions si instead of playing action aj , given thatthe opponents played the sequence s−i .r ti (aj , si|s−i) =(cid:6)Rt(cid:3)k=1aj , sk−i(cid:7)− R(cid:7)(cid:6)i , sksk−ii (aj , si), 0) at each time step t + 1.The agent then selects each of its actions with probability proportional to max(r tRecently, these ideas have also been adopted by researchers in the computer science community (e.g., [17,27,55]).Note that the application of approaches based on regret minimization has been restricted to the case of repeatedgames. The difficulties of extending this concept to stochastic games are discussed in [39].4.2. Some typical resultsOne sees at least three kinds of results in the literature regarding the learning algorithms presented above, andothers like them. These are:372Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–3771. Convergence of the strategy profile to an (e.g., Nash) equilibrium of the stage game in self play (that is, when allagents adopt the learning procedure under consideration).2. Successful learning of an opponent’s strategy (or opponents’ strategies).3. Obtaining payoffs that exceed a specified threshold.Each of these types comes in many flavors; here are some examples. The first type is perhaps the most common inthe literature, in both game theory and AI. For example, while fictitious play does not in general converge to a Nashequilibrium of the stage game, the distribution of its play can be shown to converge to an equilibrium in zero-sumgames [46], 2 × 2 games with generic payoffs [41], or games that can be solved by iterated elimination of strictlydominated strategies [42].Similarly in AI, in [38] minimax-Q learning is proven to converge in the limit to the correct Q-values for anyzero-sum game, guaranteeing convergence to a Nash equilibrium in self-play. This result makes the standard as-sumptions of infinite exploration and the conditions on learning rates used in proofs of convergence for single-agentQ-learning. Claus and Boutilier [14] conjecture that both single-agent Q-learners and the belief-based joint actionlearners they proposed converge to an equilibrium in common payoff games under the conditions of self-play anddecreasing exploration, but do not offer a formal proof. Friend-or-Foe Q and Nash-Q were both shown to convergeto a Nash equilibrium in a set of games that are a slight generalization on the set of zero-sum and common payoffgames.Rational learning exemplifies results of the second type. The convergence shown is to correct beliefs about theopponent’s repeated game strategy; thus it follows that, since each agent adopts a best response to their beliefs aboutthe other agent, in the limit the agents will converge to a Nash equilibrium of the repeated game. This is an im-pressive result, but it is limited by two factors; the convergence depends on a very strong assumption of absolutecontinuity, and the beliefs converged to are only correct with respect to the aspects of history that are observablegiven the strategies of the agents. This is an involved topic, and the reader is referred to the literature for more de-tails.The literature on no-regret learning provides an example of the third type of result, and has perhaps been the mostexplicit about criteria for evaluating learning rules. For example, in [19] two criteria are suggested. The first is thatthe learning rule be ‘safe’, which is defined as the requirement that the learning rule guarantee at least the minimaxpayoff of the game. (The minimax payoff is the maximum expected value a player can guarantee against any possibleopponent.) The second criterion is that the rule should be ‘consistent’. In order to be ‘consistent’, the learning rule mustguarantee that it does at least as well as the best response to the empirical distribution of play when playing againstan opponent whose play is governed by independent draws from a fixed distribution. They then define ‘universalconsistency’ as the requirement that a learning rule do at least as well as the best response to the empirical distributionregardless of the actual strategy the opponent is employing (this implies both safety and consistency) and show thata modification of the fictitious play algorithm achieves this requirement. In [20] they strengthen their requirement byrequiring that the learning rule also adapt to simple patterns in the play of its opponent. The requirement of ‘universalconsistency’ is in fact equivalent to requiring that an algorithm exhibit no-regret, generally defined as follows, againstall opponents.∀(cid:4) > 0,(cid:8)(cid:9)limt→inf1tmaxaj ∈Ai(cid:10)r ti (aj , si|s−i)< (cid:4)(cid:11)In both game theory and artificial intelligence, a large number of algorithms have been show to satisfy universalconsistency or no-regret requirements. In addition, recent work [6] has tried to combine these criteria resulting inGIGA-WoLF, a no-regret algorithm that provably achieves convergence to a Nash equilibrium in self-play for gameswith two players and two actions per player. Meanwhile, the regret matching algorithm [24] described earlier guaran-tees that the empirical distributions of play converge to the set of correlated equilibria of the game.Other recent work by [2] has addressed concerns with only requiring guarantees about the behavior in the limit.Their algorithm is guaranteed to achieve (cid:4)-no-regret payoff guarantees with small polynomial bounds on time anduses only the agent’s ability to observe what payoff it receives for each action.Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–3773734.3. Some observations and questionsWe have so far described the work without comment; here we take a step back and ask some questions about thisrepresentative work.Our first comment concerns the settings in which the results are presented. While the learning procedures applybroadly, the results for the most part focus on self play (that is, when all agents adopt the learning procedure underconsideration). They also tend to focus on games with only two agents. Why does most of the work have this particularfocus? Is it technical convenience, or is learning among more than two agents, or among agents using different learningprocedures, less relevant for some reason?Our second comment pertains to the nature of the results. With the exception of the work on no-regret learning, theresults we described investigate convergence to equilibrium play of the stage game (albeit with various twists). Is thisthe pertinent yardstick? If the process (say of self play between two agents) does not converge to equilibrium play,should we be disturbed? More generally, and again with the exception of no-regret learning, the work focuses on theplay to which the agents converge, not on the payoffs they obtain. Which is the right focus?No-regret learning is distinguished by its starting with criteria for successful learning, rather than a learning pro-cedure. The question one might ask is whether the particular criteria are adequate. In particular, the requirement ofconsistency ignores the basic lesson regarding learning-vs.-teaching discussed in Section 3. By measuring the perfor-mance only against stationary opponents, we do not allow for the possibility of teaching opponents. Thus, for example,in an infinitely repeated Prisoners’ Dilemma game, no-regret dictates the strategy of always defecting, precluding thepossibility of cooperation (for example, by the mutually reinforcing Tit-For-Tat strategies).Our goal here is not to critique the existing work, but rather to shine a spotlight on the assumptions that have beenmade, and ask some questions that get at the basic issues addressed, questions which we feel have not been discussedas clearly and as explicitly as they deserve. In the next section we propose an organized way of thinking about thesequestions.5. Five distinct agendas in multi-agent learningAfter examining the MAL literature—the work surveyed here and much else—we have reached the conclusion thatthere are several distinct agendas at play, which are often left implicit and conflated. We believe that a prerequisitefor success in the field is to be very explicit about the problem being addressed. We ourselves can identify fivedistinct possible goals of MAL research. There may well be others, but these are the ones we can identify. Theyeach have a clear motivation and a success criterion that will allow researchers to evaluate new contributions, evenif people’s judgments may diverge regarding their relative importance or success to date. They can be caricatured asfollows:1. Computational2. Descriptive3. Normative4. Prescriptive, cooperative5. Prescriptive, non-cooperativeWe can now consider each of the five in turn.The first agenda is computational in nature. It views learning algorithms as an iterative way to compute propertiesof the game, such as solution concepts. As an example, fictitious play was originally proposed as a way of comput-ing a sample Nash equilibrium for zero-sum games [10], and replicator dynamics has been proposed for computinga sample Nash equilibrium in symmetric games. Other adaptive procedures have been proposed more recently forcomputing other solution concepts (for example, computing equilibria in local-effect games [35]). These tend not tobe the most efficient computation methods, but they do sometimes constitute quick-and-dirty methods that can easilybe understood and implemented.The second agenda is descriptive—it asks how natural agents learn in the context of other learners. The goal hereis to investigate formal models of learning that agree with people’s behavior (typically, in laboratory experiments), orpossibly with the behaviors of other agents (for example, animals or organizations). This same agenda could also be374Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377taken to apply to large-population models, if those are indeed interpreted as representing populations. This problem isclearly an important one, and when taken seriously calls for strong justification of the learning dynamics being studied.One approach is to apply the experimental methodology of the social sciences. There are several good examples ofthis approach in economics and game theory, for example [15] and [11]. There could be other supports for studying agiven learning process. For example, to the extent that one accepts the Bayesian model as at least an idealized modelof human decision making, one could justify Kalai and Lehrer’s model of rational learning.7 However, it seems to usthat sometimes there is a rush to investigate the convergence properties, motivated by the wish to anchor the centralnotion of game theory in some process, at the expense of motivating that process rigorously.8The centrality of equilibria in game theory underlies the third agenda we identify in MAL, which for lack ofa better term we called normative, and which focuses on determining which sets of learning rules are in equilibriumwith each other. More precisely, we ask which repeated-game strategies are in equilibrium; it just so happens that inrepeated games, most strategies embody a learning rule of some sort. For example, we can ask whether fictitious playand Q-learning, appropriately initialized, are in equilibrium with each other in a repeated Prisoner’s Dilemma game.Although one might expect that game theory purists might flock to this approach, there are very few examples of it.In fact, the only example we know originates in AI rather than game theory [9], and it is explicitly rejected by at leastsome game theorists [18]. We consider it a legitimate normative theory. Its practicality depends on the complexity ofthe stage game being played and the length of play; in this connection see our discussion of the problematic role ofequilibria in Section 3.The last two agendas are prescriptive; they ask how agents should learn. The first of these involves distributedcontrol in dynamic systems. There is sometimes a need or desire to decentralize the control of a system operating ina dynamic environment, and in this case the local controllers must adapt to each other’s choices. This direction, whichis most naturally modeled as a repeated or stochastic common-payoff (or ‘team’) game, has attracted much attentionin AI in recent years. Proposed approaches can be evaluated based on the value achieved by the joint policy and theresources required, whether in terms of computation, communication, or time required to learn the policy. In this casethere is rarely a role for equilibrium analysis; the agents have no freedom to deviate from the prescribed algorithm.Examples of this work include [12,14,22] to name a small sample. Researchers interested in this agenda have accessto a large body of existing work both within AI and other fields such as control theory and distributed computing.In our final agenda, termed ‘prescriptive, non-cooperative’, we ask how an agent should act to obtain high rewardin the repeated (and more generally, stochastic) game. It thus retains the design stance of AI, asking how to design anoptimal (or at least effective) agent for a given environment. It just so happens that this environment is characterizedby the types of agents inhabiting it, agents who may do some learning of their own. The objective of this agenda is toidentify effective strategies for environments of interest. An effective strategy is one that achieves a high reward in itsenvironment, where one of the main characteristics of this environment is the selected class of possible opponents. Thisclass of opponents should itself be motivated as being reasonable and containing opponents of interest. Convergenceto an equilibrium is not a goal in and of itself.There are various possible instantiations of the term ‘high reward’. One example is the no-regret line of work whichwe discussed. It clearly defines what it means for a reward to be high enough (namely, to exhibit no regret); we alsodiscussed the limitations of this criterion. A more recent example, this one from AI, is [7]. This work puts forward twocriteria for any learning algorithm in a multi-agent setting: (1) The learning should always converge to a stationarypolicy, and (2) if the opponent converges to a stationary policy, the algorithm must converge to a best response. Thereare possible critiques of these precise criteria. They can be too weak since in many cases of interest the opponentswill not converge on a stationary strategy. And they can be too strong since attaining a precise best response, withouta constraint on the opponent’s strategy, is not feasible. But this work, to our knowledge, marks the first time a formalcriterion was put forward in AI.A third example of the last agenda is our own work in recent years. In [45] we define a criterion parameterized bya class of ‘target opponents’; with this parameter we make three requirements of any learning algorithm: (1) (Targeted7 Although this is beyond the scope of this article, we note that the question of whether one can justify the Bayesian approach in an interactivesetting goes beyond the familiar contravening experimental data; even the axiomatic justification of the expected-utility approach does not extendnaturally to the multi-agent case.8 It has been noted that game theory is somewhat unusual in having the notion of an equilibrium without associated dynamics that give rise to theequilibrium [1].Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377375optimality) The algorithm must achieve an (cid:4)-optimal payoff against any ‘target opponent’, (2) (Safety) The algorithmmust achieve at least the payoff of the security level strategy minus (cid:4) against any other opponent, and (3) (Auto-compatibility) The algorithm must perform well in self-play (the precise technical condition is omitted here). We thendemonstrate an algorithm that provably meets these criteria when the target set is the set of stationary opponents ingeneral-sum two-player repeated games. More recent work has extended these results to handle opponents whose playis conditional on the recent history of the game [44] and settings with more than two players [51].6. SummaryIn this article we have made the following points:1. Learning in MAS is conceptually, not only technically, challenging.2. One needs to be crystal clear about the problem being addressed and the associated evaluation criteria.3. For the field to advance one cannot simply define arbitrary learning strategies, and analyze whether the resultingdynamics converge in certain cases to a Nash equilibrium or some other solution concept of the stage game. Thisin and of itself is not well motivated.4. We have identified five coherent agendas.5. Not all work in the field falls into one of these buckets. This means that either we need more buckets, or somework needs to be revisited or reconstructed so as to be well grounded.There is one last point we would like to make, which didn’t have a natural home in the previous sections, butwhich in our view is important. It regards evaluation methodology. We have focused throughout the article on formalcriteria, and indeed believe these to be essential. However, as is well known in computer science, many algorithms thatmeet formal criteria fail in practice, and vice versa. And so we advocate complementing the formal evaluation withan experimental one. We ourselves have always included a comprehensive bake-off between our proposed algorithmsand the other leading contenders across a broad range of games. The algorithms we coded ourselves; the gameswere drawn from GAMUT, an existing testbed (see [43] and http://gamut.stanford.edu). GAMUT is available to thecommunity at large. It would be useful to have a learning-algorithm repository as well.To conclude, we re-emphasize the statement made at the beginning: This article is meant to be the beginning ofa discussion in the field, not its end.References[1] K. Arrow, Rationality of self and others in an economic system, Journal of Business 59 (4) (1986).[2] B. Banerjee, J. Peng, Efficient no-regret multiagent learning, in: AAAI, 2005.[3] R. Bellman, Dynamic Programming, Princeton University Press, 1957.[4] D. Billings, N. Burch, A. Davidson, R. Holte, J. Schaeffer, T. Schauenberg, D. Szafron, Approximating game-theoretic optimal strategies forfull-scale poker, in: The Eighteenth International Joint Conference on Artificial Intelligence, 2003.[5] D. Blackwell, Controlled random walks, in: Proceedings of the International Congress of Mathematicians, vol. 3, North-Holland, Amsterdam,1956, pp. 336–338.[6] M. Bowling, Convergence and no-regret in multiagent learning, in: Advances in Neural Information Processing Systems, vol. 17, MIT Press,Cambridge, MA, 2005.[7] M. Bowling, M. Veloso, Rational and convergent learning in stochastic games, in: Proceedings of the Seventeenth International Joint Confer-ence on Artificial Intelligence, 2001.[8] R. Brafman, M. Tennenholtz, R-max, a general polynomial time algorithm for near-optimal reinforcement learning, Journal of MachineLearning Research 3 (2002) 213–231.[9] R. Brafman, M. Tennenholtz, Efficient learning equilibrium, Artificial Intelligence 159 (1–2) (2004) 27–47.[10] G. Brown, Iterative solution of games by fictitious play, in: Activity Analysis of Production and Allocation, John Wiley and Sons, New York,1951.[11] C. Camerer, T. Ho, J. Chong, Sophisticated EWA learning and strategic teaching in repeated games, Journal of Economic Theory 104 (2002)137–188.[12] Y.-H. Chang, T. Ho, L.P. Kaelbling, Mobilized ad-hoc networks: A reinforcement learning approach, in: 1st International Conference onAutonomic Computing (ICAC 2004), 2004, pp. 240–247.[13] S.-F. Cheng, E. Leung, K.M. Lochner, K. O’Malley, D.M. Reeves, L.J. Schvartzman, M.P. Wellman, Walverine: A walrasian trading agent,Decision Support Systems 39 (2005) 169–184.376Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377[14] C. Claus, C. Boutilier, The dynamics of reinforcement learning in cooperative multiagent systems, in: Proceedings of the Fifteenth NationalConference on Artificial Intelligence, 1998, pp. 746–752.[15] I. Erev, A.E. Roth, Predicting how people play games: reinforcement leaning in experimental games with unique, mixed strategy equilibria,The American Economic Review 88 (4) (1998) 848–881.[16] D. Foster, R. Vohra, Regret in the on-line decision problem, Games and Economic Behavior 29 (1999) 7–36.[17] Y. Freund, R.E. Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, in: Computational LearningTheory: Proceedings of the Second European Conference, Springer-Verlag, Berlin, 1995, pp. 23–37.[18] D. Fudenberg, D. Kreps, Learning mixed equilibria, Games and Economic Behavior 5 (1993) 320–367.[19] D. Fudenberg, D. Levine, Universal consistency and cautious fictitious play, Journal of Economic Dynamics and Control 19 (1995) 1065–1089.[20] D. Fudenberg, D.K. Levine, The Theory of Learning in Games, MIT Press, Cambridge, MA, 1998.[21] A. Greenwald, K. Hall, Correlated Q-learning, in: Proceedings of the Twentieth International Conference on Machine Learning, 2003, pp. 242–249.[22] C. Guestrin, D. Koller, R. Parr, Multiagent planning with factored mdps, in: Advances in Neural Information Processing Systems (NIPS-14),2001.[23] J.F. Hannan, Approximation to Bayes risk in repeated plays, Contributions to the Theory of Games 3 (1957) 97–139.[24] S. Hart, A. Mas-Colell, A simple adaptive procedure leading to correlated equilibrium, Econometrica 68 (2000) 1127–1150.[25] J. Hu, M. Wellman, Nash Q-learning for general-sum stochastic games, Journal of Machine Learning Research 4 (2003) 1039–1069.[26] J. Hu, P. Wellman, Multiagent reinforcement learning: Theoretical framework and an algorithm, in: Proceedings of the Fifteenth InternationalConference on Machine Learning, 1998, pp. 242–250.[27] A. Jafari, A. Greenwald, D. Gondek, G. Ercal, On no-regret learning, fictitious play, and Nash equilibrium, in: Proceedings of the EighteenthInternational Conference on Machine Learning, 2001.[28] P. Jehiel, D. Samet, Learning to play games in extensive form by valuation, NAJ Economics 3 (2001).[29] L.P. Kaelbling, M.L. Littman, A.P. Moore, Reinforcement learning: A survey, Journal of Artificial Intelligence Research 4 (1996) 237–285.[30] E. Kalai, E. Lehrer, Rational learning leads to Nash equilibrium, Econometrica 61 (5) (1993) 1019–1045.[31] S. Kapetanakis, D. Kudenko, Reinforcement learning of coordination in heterogeneous cooperative multi-agent systems, in: Proceedings ofthe Third Autonomous Agents and Multi-Agent Systems Conference, 2004.[32] M. Kearns, S. Singh, Near-optimal reinforcement learning in polynomial time, in: Proceedings of the Fifteenth International Conference onMachine Learning, 1998, pp. 260–268.[33] D. Koller, A. Pfeffer, Representations and solutions for game-theoretic problems, Artificial Intelligence 94 (1) (1997) 167–215.[34] M. Lauer, M. Riedmiller, An algorithm for distributed reinforcement learning in cooperative multi-agent systems, in: Proceedings of the 17thInternational Conference on Machine Learning, Morgan Kaufman, 2000, pp. 535–542.[35] K. Leyton-Brown, M. Tennenholtz, Local-effect games, in: Proceedings of the Eighteenth International Joint Conference on Artificial Intelli-gence, 2003, pp. 772–780.[36] M.L. Littman, Markov games as a framework for multi-agent reinforcement learning, in: Proceedings of the 11th International Conference onMachine Learning, 1994, pp. 157–163.[37] M.L. Littman, Friend-or-foe Q-learning in general-sum games, in: Proceedings of the Eighteenth International Conference on Machine Learn-ing, 2001.[38] M.L. Littman, C. Szepesvari, A generalized reinforcement-learning model: Convergence and applications, in: Proceedings of the 13th Inter-national Conference on Machine Learning, 1996, pp. 310–318.[39] S. Mannor, N. Shimkin, The empirical Bayes envelope and regret minimization in competitive Markov decision processes, Mathematics ofOperations Research 28 (2) (2003) 327–345.[40] T. Mitchell, Machine Learning, McGraw Hill, 1997.[41] K. Miyasawa, On the convergence of learning processes in a 2 × 2 non-zero-person game, Research Memo 33 (1961).[42] J. Nachbar, Evolutionary selection dynamics in games: Convergence and limit properties, International Journal of Game Theory 19 (1990)59–89.[43] E. Nudelman, J. Wortman, K. Leyton-Brown, Y. Shoham, Run the GAMUT: A comprehensive approach to evaluating game-theoretic algo-rithms, in: AAMAS, 2004.[44] R. Powers, Y. Shoham, Learning against opponents with bounded memory, in: Proceedings of the Nineteenth International Joint Conferenceon Artificial Intelligence, 2005.[45] R. Powers, Y. Shoham, New criteria and a new algorithm for learning in multi-agent systems, in: Advances in Neural Information ProcessingSystems, vol. 17, MIT Press, Cambridge, MA, 2005.[46] J. Robinson, An iterative method of solving a game, Annals of Mathematics 54 (1951) 298–301.[47] P. Schuster, K. Sigmund, Replicator dynamics, Journal of Theoretical Biology 100 (1983) 533–538.[48] S. Sen, M. Sekaran, J. Hale, Learning to coordinate without sharing information, in: Proceedings of the Twelfth National Conference onArtificial Intelligence, Seattle, WA, 1994, pp. 426–431.[49] J.M. Smith, Evolution and the Theory of Games, Cambridge University Press, 1982.[50] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge, MA, 1998.[51] T. Vu, R. Powers, Y. Shoham, Learning against multiple opponents, in: Proceedings of the Fifth International Joint Conference on AutonomousAgents and Multi Agent Systems, 2006.[52] X. Wang, T. Sandholm, Reinforcement learning to play an optimal Nash equilibrium in team Markov games, in: Advances in Neural Informa-tion Processing Systems, vol. 15, 2002.Y. Shoham et al. / Artificial Intelligence 171 (2007) 365–377377[53] C. Watkins, P. Dayan, Technical note: Q-learning, Machine Learning 8 (3/4) (1992) 279–292.[54] H.P. Young, Strategic Learning and Its Limits, Oxford University Press, Oxford, 2004.[55] M. Zinkevich, Online convex programming and generalized infinitesimal gradient ascent, in: ICML, 2003.