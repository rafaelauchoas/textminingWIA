ELSEVIER Artificial Intelligence 97 ( 1997) 169-193 Artificial Intelligence Defaults and relevance in model-based reasoning ’ Roni Khardon”v*, Dan Roth b-2 a Aiken Computation Laboratory, Harvard University. Cambridge, MA 02138, USA b Department of Applied Mathematics and Computer Science, Weizmann Institute of Science, Rehovot 76100, Israel Received September 1995; revised May 1996 Abstract representations Reasoning with model-based is an intuitive paradigm, which has been shown to be theoretically sound and to possess some computational advantages over reasoning with formula-based representations of knowledge. This paper studies these representations and further substantiates the claim regarding their advantages. In particular, model-based representations are shown to efficiently support reasoning in the presence of varying context information, handle efficiently fragments of Reiter’s default logic and provide a useful way to integrate learning with reasoning. Furthermore, these results are closely related to the notion of relevance. The use of relevance information is best exemplified by the filtering process involved in the algorithm developed for reasoning within context. The relation of defaults to relevance is viewed through the notion of context, where the agent has to find plausible context information by using default rules. This view yields efficient algorithms for default reasoning. Finally, it is argued that these results support an incremental view of reasoning in a natural way, and the notion of relevance to the environment, captured by the Learning to Reason framework, is discussed. @ 1997 Elsevier Science B.V. Keywords: Knowledge representation; Common-sense reasoning; Learning to reason; Reasoning with models; Context; Default reasoning * Corresponding author. Email: roni@das.harvard.edu. Research supported by AR0 under grant DAALOS- 92-G-01 15 and by ONR grant NOOOl4-96-I-0550. ’ An earlier version of the paper appears in Proceedings of the International Joint Conference on Art$icial Intelligence (IJCAI-95) ? Email: danr@wisdom.weizmann.ac.il. Research supported by the Feldman Foundation. Part of this work was done while at Harvard University, supported by NSF grant CCR-92-00884, DARPA AFOSR-F4962-92-J-0466 and ONR grant NOOO14-96-1-0550. 0004-3702/97/$17.00 @ 1997 Elsevier Science B.V. All rights reserved. P/ISOOO4-3702(97)00044-l 170 R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 1. Introduction A considerable amount of work on the theoretical foundations of artificial intelligence has been devoted to capturing some of the intuitive notions of human reasoning. An introspective view suggests that the notion of relevance is central to human reasoning. Many “common-sense” reasoning situations are characterized by the abundance of po- tentially relevant information sources. Yet, humans seem to pick up just the relevant information and ignore the irrelevant. This ability may account for the speed with which reasoning is performed in everyday situations. In this paper, relevance is viewed as a notion that can be used to reduce the computational cost of reasoning, by focusing on information that pertains to the situation or task at hand, and ignoring the information which does not bear on this situation. We study several tasks in the framework of logi- cal reasoning, and show that relevance information can indeed be useful. In particular, we show that when performing reasoning with models-namely, when reasoning is per- formed by considering examples from the world we reason about-relevance information can be used to efficiently tackle several reasoning tasks. The generally accepted framework for the study of reasoning in intelligent systems is the knowledge-based system approach. The idea is to store the knowledge in some representation language with a well defined meaning assigned to its sentences. The sentences are stored in a knowledge base (KB) which is combined with a reasoning mechanism that is used to determine what can be inferred from the sentences in the KB. Various knowledge representations can be used to represent the knowledge in a knowledge-based system. Different representation systems (e.g., a set of logical rules, a probabilistic network) are associated with corresponding reasoning mechanisms, each [ 14,181. Given a logical knowledge with its own merits and range of applications base, for example, reasoning can be abstracted as a deduction task: determine whether a sentence, assumed to capture the situation at hand, is logically implied by the knowledge base. It is also widely agreed that a large part of our everyday reasoning involves arriving that are not entailed by our “theory” of the world. Many conclusions at conclusions are derived in the absence of information that is sufficient to imply them. This type of reasoning is naturally nonmonotonic since further evidence may force us to revise our conclusions. Several formalizations trying to capture this situation have been studied, and of particular interest to us here are theories for reasoning with “defaults” (see e.g. [20] ). In this approach, the true knowledge about the world is augmented by a set of default rules that are meant to capture “typical” cases. The quest is for a reasoning system that, given a query, responds in a way that agrees with what we know about the world and (some of) the default assumptions, and at the same time supports our intuition about a plausible conclusion. Computational considerations, however, render this approach inadequate for common- sense reasoning. This is true not only for the task of deduction, but also for many other forms of reasoning that have been developed. All those were shown to be even harder to compute than the original formulation [ 23,251. This holds in particular for various formalizations of default reasoning [ 6,17,26], where the increase in complexity is clearly at odds with the intuition that reasoning with defaults should somehow reduce R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 171 the complexity expressiveness paper we show that model-based above-mentioned results. difficulties, of reasoning. This remains true, even when we severely of the knowledge base, the default rules and the queries allowed. representations can be used to overcome and that the notion of relevance is useful in deriving restrict the In this some of the these We incorporate the notion of relevance into the study of reasoning by introducing completes this situation the task of reasoning within context. normally We model context-specific where some additional formalize is that the availability easier, by restricting performs model-based efficient constraining reasoning. reasoning It has been argued a lot of missing context by augmenting the agent’s knowledge information when answering queries that in real life situations, one [ 121. the world with task, base. We about is therefore a deduction to the knowledge information. Reasoning within context information is added this task as the problem of reasoning within a varying context. The intuition task information to reason about. As we show, if the agent can be easily used, and yields information context of additional the domain one needs should make the reasoning then context reasoning reasoning assignments, is presented, In model-based [ 5,9] examples) It is not hard to motivate a model-based as a set of models the knowledge base is represented of the world rather than a logical formula describing (satisfying the query on these it. When a query models. from a cognitive point of view and indeed most of the proponents of this approach have been cognitive [ 3,4,11 J, who have alluded psychologists from examples” on a qualitative basis. In the AI community Levesque’s notion of “vivid” frames-theory to the notion of “reasoning this approach can be seen as an example of to Minsky’s [ 151 and to some of the work in case-based [ 12,131, and is somewhat is performed by evaluating related [ lo]. to reasoning reasoning approach Given a model-based representation task of deciding whether KB implies straightforward way: Evaluate LY on all the models a model of KB which does not satisfy KB /= a. Clearly, then, by definition, But representing KB by explicitly holding all model-based approach becomes representation if the model-based this approach verifies and still support correct deduction. feasible reasoning of the knowledge base KB, and a query a, the in a cy (denoted KB /= a) can be performed If you find that all the models of KB (Y, then KB &c: a, otherwise in the representation. representation conclude contains the implication, and yields correct deduction. the possible models is not plausible. A if KB can be replaced by a small model-based The theory of model-based representations developed [ 91 (generalizing the the- ory developed languages tion. It is shown based setting, propositional efficient before. reasoning in [5] for the case of Horn expressions) the propositional for which model-based representations that in many cases the model-based variables in the domain). Thus, representation can be obtained in which deduction support efficient deduction and abduc- in the formula- in the number of setting, correct and in cases where such algorithm were not known is small in the model-based (polynomial is NP-hard When reasoning within context, our general knowledge with additional has an easy and natural constraints, those implementation that are relevant to the current when using model-based about the world is combined task The situation, This representations. in characterizes 172 R. Khardon, D. Roth/Art@cial Intelligence 97 (1997) 169-193 simply as a background filters out models that are not consistent with the context in the representation which are not relevant to the algorithm current context, namely models information. The remaining models are used as before for reasoning with models. The filtering process can thereby speeding be performed for which this simple up the reasoning. We characterize algorithm works correctly and efficiently. We note that reasoning with models is essential advantage when reasoning within context. The filtering for providing algorithm context information cannot be performed when reasoning with formulas, the context changes, languages does not necessarily make the task easier. several propositional process whenever the computational and additional default reasoning Intuitively, conflicting) is a generalization the task of default also aims at capturing the standard knowledge to weed out non-relevant “by default”, namely using some notion of rele- cases and focus vance in that it enforces additional constraints on the typical cases. In default reasoning [ 191 an agent is given a representation of the rules, and has to assess whether a world, and a set of (sometimes query q can be concluded and the of reasoning within con- default rules. We show that default reasoning rules, which may be conflicting. Namely, text, in which the reasoner has many context is, but rather default rules do not express explicitly what the relevant context cases can be ignored, a new implicitly source of computational rules are not compatible with each other. In this case, the agent can only use a subset of these rules for deriv- such subsets. To ing its conclusions, reach a conclusion (called extensions in default time. In some sense, conclusion computational and there may be many possibilities the agent must enumerate and perform “contexts” to one context at a in order to derive a the reasoning through possible contexts the various possible relative is at odds with the notion of relevance and is the source of the additional scenarios. While non-typical is introduced when default reasoning), the need in default reasoning. capture all plausible for choosing information to search difficulty difficulty representations in these cases, model-based In particular, in an accessible Nevertheless, we show that in some cases model-based out of the computational difficulty. tations capture all possible contexts contexts, and then use a subroutine task. We give efficient algorithms tasks, for several classes of world knowledge, default that in some cases for cases results provide knowledge base is represented with formulas. the task of diagnosis solutions efficient for reasoning within context for (both credulous and skeptical) form. Thus, one can enumerate to perform the inference default reasoning rules and queries. We also show techniques. Our if the to be solvable can be performed by similar that are not known provide a way represen- the Our notion of relevance be also used at a higher or context situation, example, restricted in order using based representations such approximations, information was used to prune it is also possible if it is known language to support correct then an approximation as a way to reduce the computational that of relevance for the tusk. While above, level, the knowledge representation to prune a representation that all the queries presented of KB relative reasoning with these queries. This relative to the agent come to this language to a general cost of reasoning can relevance for a particular task. For from some is sufficient idea can be formalized the model- capture the reasoning In fact, as well. Here again, the notion of least upper bound approximations [9,27]. used for reasoning within context and default and thus use this notion of relevance R. Khardon, D. Roth/Art$cial Intelligence 97 (1997) 169-193 173 representations are essential since formula-based representations of these model-based approximations representation, do not support efficient agent can construct a representation views reasoning. Our results show that knowledge, which holds within a specific context and is avail- can be used to reason correctly within the view that by pasting the notion of able in a form of a model-based this context. Therefore, our treatment of reasoning within context supports an intelligent together many “narrower” relevance tation there. This intuitive son framework Learning the relevant construct a represen- in the sense that it supports correct reasoning to Rea- two results in the in order to exploit idea is formalized [ 71, and is discussed framework in a more general setting also in [ 8,22,3 11. We discuss from different contexts. This suggests to the environment. Namely, the agent can incrementally of the world incrementally in the reasoning process. that use model-based to its environment, in the Learning that is relevant representations information to Reason this paper studies reasoning with model-based their computational regarding advantages. representations, the claim information, representations To summarize, further substantiates model-based of varying context provide a useful way to integrate we suggest computational relevant We discuss based representations. cost of reasoning to the current situation, that it is useful In particular: several aspects of relevance, are shown to efficiently support reasoning handle efficiently fragments of Reiter’s default learning with reasoning. On a more philosophical to view these results through the notion of relevance: the information can be reduced by using only the general task being performed, or the environment. and show that their use is enabled by model- and In particular, in the presence logic and level, the that is (1) The use of relevance involved in the algorithm (2) The relation of defaults information is best exemplified for reasoning within context. to relevance can be viewed through by the filtering process the notion of context, resulting in efficient algorithms (3) Relevance restricting (4) The Learning knowledge information the attention to Reason representation for default reasoning. can also be used at a level of a global sufficient to information allows framework that is relevant for the environment. for the task. incrementally for task by a priori constructing a The rest of the paper is organized as follows: In Section 2 we introduce and the notation used throughout from the theory of reasoning with models. within context and present an efficient model-based discuss default we discuss knowledge Section 7 we briefly present relevance, some definitions the paper. In Section 3 we briefly present some results the task of reasoning for it. In Section 5 we In Section 6 for a task. In to its relation to diagnosis. relevant and reasoning with models and an application and Section 8 concludes with a summary. In Section 4 we discuss approximations the Learning as capturing information to Reason framework algorithm 2. Preliminaries We consider problems of reasoning where the “world” tion W : (0, 1)” 4 (0, 1). We use interchangeably the terms propositional is modeled as a Boolean func- expression 174 R. Khardon, D. Roth/Artij?cial Intelligence 97 (1997) 169-193 and Boolean functions. We denote classes of Boolean and likewise function, for propositional language and a class of Boolean functions by .7=-, G, and functions by f,g. in attribute LetX={xr,... are mappings (0, 1)" with the associated is a conjunction in the world. Assignments (xi V EJ) A (xg V 37 V x4) of literals, and a DNF formula of literals, and a CNF formula the number of 1 bits in the assignment and can take the value 1 or 0 to indicate whether the natural mapping. Assignments denotes , x,,} be a set of variables, each of which is associated with a world’s is from X to (0, l}, and we treat in (0, 1)" are x. of clauses. is a CNF formula with two clauses. A term of terms. For example, is monotone is Horn if every clause if there are at if there are at most is k-quasi-monotone DNF if there are attribute true or false them as elements denoted by x, y, z, and weight(x) A clause is a disjunction For example, is a conjunction (xl AX;?) V (x3 AZfiA x4) is a DNF formula with two terms. A CNF formula if all the literals in it has at most one positive most k positive k negative literals at most k negative Every Boolean CNF representation we mean CNF size of f, denoted representation in each clause. A DNF formula literals in each term. function has many possible and, in particular, both a ]DNF( f) 1, the in any CNF literal. A CNF formula in each clause. It is a k-quasi-reversed-Horn and a DNF representation. By the DNF size of f, denoted number of terms in any DNF representation (unnegated). A CNF formula \CNF( f) 1, is the minimum number of clauses in it are positive of f. Similarly, is k-quasi-Horn is a disjunction representations the minimum literals of f. An assignment to in the literature f if f(x) = 1; such an assignment x E (0, 1)” satisfies model of f. If f is a theory of the “world”, a satisfying referred we mean confusion f-’ equivalent f + g if and only that every model of f is also a model of g. Throughout can arise, we identify a Boolean that the connective “implies” “subset or equal” f k g, the paper, when no f with the set of its models, namely is ( k) used between Boolean (C) used for subsets of (0, 1)“. That is, assignment of f is sometimes as a possible world. By “f implies g”, denoted to the connective ( 1) . Observe x is called a if f C g. functions function 3. Reasoning with models In this section we briefly present some results from the monotone functions section have appeared elsewhere. For a detailed discussion the theory of reasoning with models [2] and see [ 91. [5,9]. All the results theory of Boolean in this Consider a propositional strategy The model-based implication uses a set of models algorithm evaluates a(x) = 0, then the algorithm Clearly, all models too large, making the model-based (satisfying relation using model evaluation. Fig. 1 describes knowledge for the deduction problem W + cy is to try and verify base W and let LY be a propositional query. the the algorithm MBR, which base. When presented with a query cy the x is found such that in r. If a counterexample r as a knowledge (Y on all the models returns “No”. Otherwise solves approach of W. However, assignments) this procedure computationally the inference problem is the set of if r the set of all models might be approach infeasible. A model-based it returns “Yes”. R. Khardon. D. Rorh/Ar@cial Inrelligence 97 (1997) 169-193 115 Algorithm MBR( r, a) : Test set: A set r 2 W of possible assignments. Test: If there is an element x E r which does not satisfy a, return “No”. Otherwise, return “Yes”. Fig. 1. MBR: model-based reasoning becomes useful the test set, and still perform reasonably good inference. if one can show that it is possible to use a fairly small set of models as In the rest of this section we describe general conditions this can be done. An example of the technical notions presented here is given at the end of the section. under which 3.1. Monotone theory Definition 1 (Order). We denote by 6 (0, l}“, the one induced by the order 0 < 1. That is, for x,y E (0, l}“, x < y if and only if Vi, if x ~3 b < y @ b xi ,< yi. For an assignment (Here @ is the bitwise addition modulo 2.) We say that x > y if and only if x > y and x # y. b E (0, 1)” we define x &, y if and only the usual partial order on the lattice Intuitively, if bi = 0 then the order relation on the ith bit is the normal order; if bi = 1, the order relation is reversed, that is, 1 <b; 0. The monotone extension of z E (0, 1)” with respect to b is defined as Mb(Z) = {x 1 x ab z}. The monotone extension off with respect to b is defined as Mb(f) = {x 1 x &, z, for some z E f}. that throughout we treat the function Notice and therefore respect the above notation to b is defined as f as the set of its satisfying is natural. The set of minimal assignments of f with assignments, mjn(f> = {z 1 z E f, such that Vy E f, z #by}. Definition 2 (Basis). A set B is a basis for f class of functions F if it is a basis for all the functions in F. if f = jjbEB Mb( f). B is a basis for a The importance of these definitions [2] is that every Boolean function has a basis B, and can be represented as follows: f = /jJ%(f) &B = /j v Mb(z) bEB zEminb(f) (1) 176 R. Khardon, D. Roth/Artijicial Intelligence 97 (1997) 169-193 This {0, 1)” satisfies representation f: yields a necessary and sufficient condition describing when x E Corollary 3. Let B be a basis for f, and x E (0, l}n. Then, f(n) for every basis element b E B there exists z E minb( f) such that x >b z. = 1 if and only if It is known [2] DNF representation. representation 1 CNF( f) /. Some important CNF size of the function: that for every b, the size of minb( f) Further, a set of assignments which falsify every clause is bounded by the size of its in a CNF f has a basis whose size is bounded by function classes have a small fixed basis, irrespective of the of f is a basis for f. Therefore, l Horn formulas: A basis for this class is BH = {u E (0, 1)” ( weight(u) > n - is falsified by an assignment l}, in BH. Clearly, J&J = n + 1. l k-quasi-Horn formulas: BH~ = {u E {0, 1)” 1 weight(u) 3 n - k} is a basis for this since every Horn clause class. Clearly, formulas. IBH~I = 0( nk). Similarly, there is a basis for k-quasi-reversed-Horn construction {a,, l logn-CNF formulas: A Boolean clauses contain at most O(logn) combinatorial a set of assignments assumes (n, k)-universal therefore for k = log n one can construct the function with a CNF representation literals. A basis for this class is derived using a called an (n, k) -universal set is . . . a,} C (0, 1)” such that every subset of k variables to see that an for any clause of length k and that sets of size 0( n3) and therefore it forms a basis for the class of k-CNF formulas. set. An (n, k) -universal set includes a falsifying in the ai’S. It is easy all of its 2k possible (n, log n) -universal It is known assignments assignment in which [ 1,161 IBI~~~-cNFI = Wn3>. l Common queries: A function is common if every clause in its CNF representation is taken from one of the above classes. The union of the bases for these classes basis, Bc, for all common queries. functions. We refer to this class as the class of common is a 3.2. Deduction We can now characterize a model-based knowledge base for which the algorithm MBR is successful. Definition 4. For a function the set of all minimal assignments f E F, the set I’ = r; of f with respect of characteristic models of f is to a set B c (0, 1)". Formally, r$ = U{z E mp(f)}. bEB If, in addition, B is a basis for a class G of functions, model-based representation of f with respect describe common queries, and a bound on the size of the model-based result of the theory of reasoning with models, to queries the basic in 9. The following representation. its application then we say that ry is the theorems to R. Khardon, D. Roth/ArtiJicial Intelligence 97 (1997) 169-193 171 and cr E 0, Theorem 5 (Khardon where B be a basis for 6. Then f k cr if and only if for every u E r;, a(u) = 1. That Let f be any Boolean and Roth function, [ 91). is, model-based deduction using TT, is correct. Corollary 6 (Khardon common query a, f k a if and only if for every u E TF, and Roth [9] ). Let f be any Boolean function. Then for any (u(u) = 1. That is, model- based deduction using rf”“, is correct. Theorem 7 (Bshouty size of the model-based [2] ). Let f be any Boolean function, and B a basis. Then, the representation off is We note that this bound is tight in the sense that for some functions there are functions with an exponential-size DNF and a linear-size model-based to compare [ 91. It is also interesting It does however allow for an exponential required. is indeed DNF Namely, representation the size of other representations the (Horn CNF) cases where is exponentially representation well as other properties of characteristic models, see [ 93. for functions. Examples representation formula the size of this representation [5] show to that there are in is small and the model-based large, and vice versa. For a discussion of these issues, as the size of the gap in other cases. Example. Let f have the CNF representation function The satisfying assignments of f are: 3 {OOOO,OOOl, 0010,1101}. f has 12 (out of the 16 possible) satisfying assignments. The non- to be able If we want to answer all possible Horn queries with respect to use the Horn basis BH = {1111,1110,1101,1011,0111}. to f we Each of the models need for each of these, minb ( f) = {b}. For 1111,0111,1011,1110 b = 1101, the minimal lattice and checking which of the satisfying assignments of f are minimal. This yields mint tot ( f) = {1100,1111,1001,0101}. 0101, 1110). Note that it includes only 7 out of the 12 satisfying assignments f and therefore satisfies elements can be found by drawing We therefore get that rf”” = {1111,0111,1011,1100,1001, the corresponding therefore model-based f. Furthermore, find the counterexample deduction for the Horn query of f. never makes (~1 = 1011 and deduce Clearly, in general r;H 2 f, and on queries mistakes xi A x3 + x2, reasoning with correctly that f #al. We note that in general, that are implied by rf” will f k LY is co-NP-complete, is given if f even when (Y is a Horn query. in its CNF representation, solving the problem 3 An element of (0, I}” denotes an assignment to the variables xt , , xn (i.e., 0011 means xt = x2 = 0, andq=x4=1). 178 R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 4. Reasoning within context It has been argued that in real life situations, one normally completes a lot of miss- ing “context” information when answering queries [ 121. For example, if asked at a conference how long it takes to drive to the airport, we would probably assume (un- less specified otherwise) that the question refers to the city in which the conference is held, rather than to the place where we live (and have been to the airport more times). This corresponds to assigning the value “true” to the attribute “here” for the purpose of answering the question. Sometimes we need a more expressive language to describe our assumptions regarding the current context and assume, say, that some rule applies [ 261. For example, we may assume (in the “conference” context) that if someone has a car, then it is a rental car. Thus, reasoning within context may be viewed as a deduction task, where some additional constraining information is added to the knowledge base. Our use of context is closely related to the notion of rele- vance, since one can use context information in order to concentrate on the relevant knowledge and ignore the irrelevant. Intuitively, this should also make the reasoning task easier. Indeed, as we show, this holds in a formal sense when using model-based reasoning. Let W be a Boolean function that describes our knowledge about the world. A “first principle” way to formalize the above intuition is the following: we want to deduce a query (Y from W, if (Y can be inferred from W given that the query refers to the current context. Namely, the instances of W which are relevant to the query must also satisfy the context condition d, a conjunction of some literals and rules. We denote this question by W k=d a. Notice that it is possible that W kd cy but W p a, if all the satisfying assignments of W that do not satisfy cr do not satisfy d. Formalized this way, the problem W b=d a is equivalent to the problem W A d k a. Thus, a theorem proving approach to reasoning does not necessarily provide any computational advantage in solving this reasoning problem. Let W E 3, cr E 6 and let B be a basis for 9. From Theorem 5 it is clear that given the set of characteristic models for W A d, model-based reasoning can be used GA& to solve the reasoning problem W A d k a. However, we consider here a more general problem: given rg we are interested in performing inference according to /==d with it, where the “context condition” d may vary. From our model-theoretic definition of the connective b=d it is clear that if one has all the models of W then, by filtering out all the models that do not satisfy d and performing test on the remaining models, one answers W A d + a correctly. The algorithm C-MBR, presented in Fig. 2 does just that, with the set r. the model-based The following theorems show that, under some conditions, a compact model-based representation behaves like the complete set of models of a theory. Namely, the filtering algorithm C-MBR provides correct reasoning. Theorem 8. Given rg, W k=d a for every d such that B is a basis for d -+ a. the algorithm C-MBR correctly solves the reasoning problem R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 179 Algorithm C-MBR(T, d, a): Test set: Consider only Test: If there is such an element which does not satisfy (Y, return “No”. those elements of r which satisfy d. Otherwise, return “Yes”. Fig. 2. C-MBR: model-based reasoning within context. (d -+ a). Therefore, 5, when B is a basis can be used for model-based it. Models of W that do not satisfy d are useless as counterexamples Proof. Clearly, W +d cr E W A d k LY E W b z V a E W b from Theorem reasoning with since d --+ cx always holds and therefore, correct answer. the test set of Algorithm C-MBR produces for d -+ a, l$ 0 the Theorem 9. The following correct reasoning within context. conditions on a, B and d guarantee that C-MBR supports (i) Let cr be a k-quasi-Horn query and B = BH~,,, a basis for (k + r)-quasi-Horn that can be represented as a r-quasi-monotone then B is a basis for d -+ a. In particular, when k = 1 and r = 0, this theories. If d is a Boolean function DNF holds for any Horn formula LY and any monotone Boolean function d. theories. query and B a basis for 2logn-CNF (ii) Let (Y be a logn-CNF If d is a then B is a basis for conjunction d -+ a. of up to logn arbitrary rules (disjunctions) Proof. Assume CNF expression AjEJcj. In this case, first that d is given as a DNF expression Viclti and cy is given as a = A (i$VCj). iEI,.iEJ first the case k = 1, r = 0. Since d is monotone, For case (i), consider a DNF expression for d is monotone, general, every term in d can contribute at most k positive For case (ii), d is a CNF expression with at most logn clauses, and therefore can be written as a DNF expression literals. Therefore (< V Cj) has at most 2 log n literals. and therefore at most r positive (K V Cj) is in (k + r)-quasi-Horn. (q V Cj) is a Horn disjunction. literals, and cj can contribute to (5 V Cj). Therefore, term has at most logn term in In in which every literals every Finally, notice that we do not need to get d as a DNF expression. The analysis uses this to the desired class, but the algorithm evaluates to it and the filtering algorithm. expression d and (Y directly, using to show that d -+ cy belongs the representation given 0 180 R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 It is interesting to note that the size of the expression large. However, for d -+ a described in the in the analysis. in the algorithm. Rather, filtering examples it appears only this expression above proof might be exponentially We do not actually compute to d is sufficient. according that our definition Notice also in a particular of characteristic models, the set of and ultimately context, depends on the basis B. This may seem filtered models used confusing at first since a class of queries may have more than one possible basis and the choice of B is arbitrary. However, note that any basis for the class of queries can be used models This is similar the same Boolean different minimal in this class, and therefore a set of characteristic to reason with the queries. knowledge using formulas; in many ways, and even using several there is no unique all the queries that is based on it captures all the information from the same class (for example, functions for Boolean to the situation function representations representation that arises when representing can be represented in CNF form). to represent in general needed The approach presented representation in this section can be viewed as a process of augmenting r with a set of rules. Given a model-based representation a in W cannot help in answering queries, since model-based r$, of W, any rule that holds filter out any assignment not hold in W and thus augmenting W with them modifies we have shown, representation with respect deductive case. that is slightly to a basis larger of W, and is thus redundant. However, it does not rules do the set of conclusions. As the context in order to reason within context, we need to maintain a model-based than the basis in the pure 4.1. Context and relevance Our treatment of context information appeals information to the intuitive notion of relevance. The d to filter out the irrelevant irrelevant base. The information algorithm C-MBR uses the context in the knowledge that do not correspond the current context on the facilitates the filtering elements algorithm the context changes. This way, the reasoning itself will be faster since at any time only evaluation on the models of the current context is required. efficient of the representation. A natural approach would be to perform in the background, whenever to the current context. The use of a model-based it only requires evaluating information is the set of models representation in this case filtering, since This should be contrasted with a formula-based information the computational is conjuncted with the formula W, and theorem proving necessarily representation, where adding context task. There, the formula d is used. Since W A d is not to reason with than W, the task, in general, does not become easier. does not necessarily simpler help 5. Default reasoning with models Default rule captures conditions, reasoning is a formal framework for arguing about the idea reminiscent that under normal of context information, are satisfied. circumstances we may assume typical cases. A default that certain In default reasoning, how- R. Khardon, D. Roth/Artificial Intelligence 97 (I 997) 169-l 93 181 that combines these conclusions, is, at least partly, aimed at capturing is not obvious for such context. This suggests than one option can be viewed as a generalization the from the set of rules that default of reasoning within context where, given a Intuitively, default some notion of relevance, by ignoring ever, one might have many rules whose conclusions may be conflicting. As a result correct context, and there may be more reasoning query, the agent has to search for a correct context before reasoning. reasoning the non-typical reasoning problem reasoning, for the correct context. Nevertheless, representations enumerating can be found. cases. Therefore, easier. However, default to make the than deductive search as our results show, in some cases model-based difficulties can be traced to the above-mentioned form, and thus by solution on one context at a time, an efficient can capture all the possible contexts contexts, or concentrating and the computational the use of defaults is actually harder in an accessible is expected (irrelevant) reasoning We will concentrate here on a special case of Reiter’s default logic [ 191, applied to propositional logic. logic, default rules have the form y, which should read as “if CL In Reiter’s default to assume p then conclude 7”. The case with p = y is called holds and it is consistent normal defaults, and CY is called a prerequisite. The discussion below considers normal In this case, we denote by D the set defaults with empty prerequisites, of Boolean treat a collection of rules as their conjunction. That is, D(x) = 1 means AdED d(x) = 1. functions p, and say that D is the set of default rules. We sometimes denoted by b. 2 Definition 10. For normal defaults with empty prerequisites rule is simple The rule is positive literal. The rule is positive if ,LI is a positive if p is a single simple literal. f we define: A default function. if p is any monotone Notice that the theory for diagnosis [ 211 and the closed world defaults [ 191 can be described using simple defaults. theory rules, and W is a (D, W) where D is a set of default expression. An extension of (D, W) is defined using a fixed point operator and simpler theorem gives an alternative the following is a pair A default propositional [ 191. For our special definition: case (The operator Th( R) denotes the theorem closure of R.) [ 21, p. 881). Let D be a set of normal defaults with empty pre- Theorem 11 (Reiter requisites. E is an extension of (D, W) if and only if E = Th( W A S) , for some maximal subset4 S of D such that W A S is consistent. Using this theorem as the definition subset S with each extension E. We denote with W, we get that an extension E includes q if and only [ 191 a knowledge original base is defined formalization for extension we can identify a maximal consistent this subset by SE. Since SE is consistent if W A SE b q. In the is if there to imply a query 4 To avoid confusion we emphasize rule can be added that no additional function S2 is the conjunction that S and D are considered in the sense to it while preserving consistency with W. Thus, if S1 C S2, the Boolean as sets of rules. S is maximal of more rules and therefore S2 + .SI 182 R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 in which an extension default reasoning; skeptical default reasoning. the query holds. Following the case in which all extensions [6,29] we call this task credulous is called are taken into consideration Formally, (D, W) and a propositional given a default exists an extension E of (D, W) such that q E E. the credulous default reasoning task CDEF( D, W, q) is defined as follows: there expression q, decide whether theory The skeptical default reasoning task SDEF( D, W, q) is defined as follows: given a for all the (D, W) and a propositional q, decide whether expression theory default extensions E of (D, W), q E E. Clearly, if W is consistent with the set of all rules is only one subset S of D, the one which contains all these rules. In this case the context in the general case is that W and skeptical default earlier. The main difficulty which arises to reasoning within in D, then reasoning reduce there consistent maximal both credulous D, as discussed may not be consistent with all of D. results on default reasoning represen- results we present hold there is no known efficient solution when reasoning with formulas. The case. There, randomized reasoning using a model-based [9], than in the deductive (under is somewhat more subtle the efficient Next we present positive for problems representation. relation solutions where presented given a formula-based is NP-hard literals, tation. As in the case of deductive in cases where exact complexity efficient reductions) task soning are positive algorithm representation. expression, Thus, strictly speaking, we do not prove an advantage our results provide efficient algorithms before. the knowledge base is a single positive for this class of problems, which is polynomial This representation, as is the case, for example, [28] when and the query in cases where though, may be exponential for the problems used in the reduction in the size of the Horn in [ 281. in this special case. Nevertheless, they were not known to exist that are NP-hard In the current case, the default is Horn, all the default rea- rules literal. Our results provide an in the size of the model-based We present two algorithms, CD-MBR and SD-MBR, which handle and skeptical default algorithm5 abduction reasoning developed tasks, respectively. Both algorithms in 151 and used in [91. 5.1. Credulous default reasoning the credulous to the are similar in r. When We start by describing representation receives the algorithm CD-MBR, which is presented of W. (The monotone basis will be defined in Fig. 3. Let r = later.) r, D and a query q as input. It starts by enumerating TW be a model-based The algorithm CD-MBR all the models it sets S to be the set of all the rules in D that this model satisfies. The algorithm tests whether W A S k q by calling If the answer next model been found it finds a model z in which the query holds (i.e., q( z ) = 1) , then to decide whether W bs q. to test the in r have been scanned and no good extension has is “Yes” the algorithm If all the models in r. the algorithm returns “Yes”; otherwise, the procedure C-MBR it continues says “No”. 5 Our results were inspired by the connections between abduction and default reasoning developed in 1251. R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 183 Algorithm CD-MBR(T, D, q): Do for all models z E r such that q( z ) = 1 Let S= {d E D 1 d(z) = 1) If C-MBR( r, S, q) answers “Yes”, return “Yes”. EndDo Return “No”. I* No extension found *I Fig. 3. CD-MBR: default reasoning with a model-based representation. Assume the algorithm to it is q. The following is run with a model-based r = rf: and the query two conditions on B and D are used to characterize representation presented the cases in which the algorithm is successful. Condition 12. B is a basis for 5’ --f q, for all S c D. Condition 13. For all S G D, for all u such that S(U) = 1, there is a prime t of S, and a basis element b E B, such that t(u) = t(b) = 1. implicant Theorem 14. Given rk, task CDEF( D, W q) correctly, whenever Condition 12 and Condition 13 hold. the algorithm CD-MBR solves the credulous default reasoning Proof. We need to prove: returns “Yes” then the desired extension exists, and if the algorithm if there is an extension (i) (ii) For (i), since Condition W A S k q. By construction, is a maximal clearly W A S* k W A S k q, and the required extension exists. 12 holds, Theorem 8 implies that contains q, then the algorithm subset of D (with respect S is a subset of D for which WA S is consistent. that C-MBR returns “Yes”. is correct, to the property of consistency with W), that is, If S* > S then For (ii), assume that there is an extension E that contains q. By definition, the existence of E implies and, WA SE + q. Thus, there is an assignment also q(u) = 1. Condition that there exists a subset SE C D such that W A SE is consistent u E W such that SE(U) = 1 and therefore 13 implies that there exists a prime b E B such that SE(U) = t(u) = t(b) = 1. Thus, u, and b agree on all the literals appear in t, and therefore is such that w &, u (and since u E W, w like that always exists), since W A SE /= q also q(w) = 1. t of SE and a basis element that for all z such that z &, U, t( z ) = SE(Z) = 1. Thus, if w E I$, then SE(W) = 1 and implicant Now, consider the set S that the algorithm CD-MBR uses in the iteration the algorithm will compute includes SE since SE(W) = 1, and returned by C-MBR correctly is correct, due to Condition 0 By construction clearly answer the algorithm to SE. (The set The 12 and Theorem 8. Therefore is exactly SE since SE is maximal.) responds “Yes”. a set which for w E r$. is identical The following lemmas identify cases in which the required conditions hold. 184 R. Khardon, D. Roth/Art@cial Intelligence 97 (1997) 169-l 93 Lemma 15. If the set D of defaults consists of (i) positive defaults, or (ii) simple defaults with 6 r negative literals, or (iii) up to log n default rules, then there is a small basis which satis$es Condition 13. basis that contains in which all the negative term, and b = 1” satisfies the condition. That is, the Condition implicant of S 2 D is 13 holds for Proof. For (i), since D is a set of positive defaults, every prime a monotone 1”. For (ii), every subset S G D is a conjunction every monotone includes implicant, of literals, and S has a single prime literals of S assigned 0, and all other literals an assignment assigned 1, satisfies Condition literals, if D contains up to r negative so does every subset of it S, and the basis &.tr suffices. For (iii), every subset S & D has a CNF expression with at most logn clauses, and therefore also a DNF expression if u satisfies S, u also satisfies in which every term has at most logn literals. Thus, a prime basis B 13 holds, since t must satisfy at least one contains an (n, log n) universal of the elements itself. Thus, every basis which than set, Condition literals. Therefore, t with no more 13. In particular, if the monotone implicant there. logn 0 The following definition captures the cases for which model-based default reasoning is correct: Definition 16. The (D, &, r) these cases by 7D following that guarantee efficient solution (for tractable defaults). cases describe simultaneous on classes for the default reasoning problem. We denote restrictions (i) D: a set of positive defaults; &: the class of k-quasi-Horn queries. The corre- sponding model-based representation: r = r$. (ii) D: a set of simple defaults with up to r negative literals; &: the class of k- quasi-Horn queries. The corresponding model-based representation: r = .Fr. (iii) D: a set of at most logn default rules (disjunctions); Q: the class of logn-CNF queries. The corresponding model-based representation: r = r31w ‘K~. Lemma 17. Condition 12 and Condition 13 are satisfied for all (D, &, r) in TD. Proof. Condition 12 holds as a direct consequence of Theorem 9. Condition 13 holds as a direct of Lemma 15: For (i), since 1” E But+,. For (ii), since BH, C BH~+, , and for (iii), since B2 togn_c~r contains a (n, log n) -universal set. implication 0 Using Theorem 14 and Lemma 17 we get: Corollary 18. The algorithm CD-MBR solves the credulous default reasoning task CDEF( D, W, q) correctly, for all q E Q and for all (D, &, T) in TV. R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 185 Algorithm SD-MBR(I’, D, q): If for all models z E r q( z ) = 0, return “No”. Do for all models z E r such that q( z ) = 1 LetS={dED(d(z)=l} S,,, = max Do for all d E D \ S /* Consider S as a potential maximal subset */ if there exists y E r such that W(y) = 1, S(y) = 1, and d(y) = 1 then S&, = no-r~~~ EndDo If S,,, = max and C-MBR( r, S, q) answers “No”, return “No”. EndDo Return “Yes”. I* All extensions are good *I Fig. 4. SD-MBR: default reasoning with a model-based representation. 5.2. Skeptical default reasoning only extension the credulous is a maximal The only difference between in one. A legitimate in the latter we respond affirmatively than just rules. For credulous default reasoning sets are considered were considered skeptical which includes S satisfies W A SE /= q. Therefore, presented in Fig. 4, tests for maximal reasoning, we need and the skeptical if the query holds it was sufficient and it was not important tasks is that reasoning in all extensions rather subset of the set D of default that all the maximal sets they could not affect the response of the algorithm). For subset S subset SE which the main stage of the algorithm SD-MBR, sets, since a consistent that some non-maximal is not maximal might satisfy W A S # q while every maximal as well (since as candidates, the maximal to guarantee to identify subsets. representation It starts by enumerating of IV. The algorithm SD-MBR all the models Let r = Tw be a model-based is consistent with W. (In consistent r, D and a query q as input. finds a model z in which of all the rules indeed a maximal that it is sufficient it is ignored algorithm W /=s q. If the answer to look for another maximal of D, have been “Yes”. and (i.e., q(z) = l), in D that this model satisfies. The algorithm the query holds subset, by checking whether there the correctness proof of the algorithm we show to test this condition using elements of r.) If S is not maximal the algorithm goes on to the next assignment tests whether WA S b q by calling is “No” the algorithm the procedure C-MBR returns “No”, and otherwise set S. If all the models tested and no bad extension has been found in r, and the corresponding in r. When receives it it sets S to be the set then tests whether S is is any superset of S that then the to decide whether it continues subsets says in r. Otherwise, then the algorithm Theorem 19. Given r&, the algorithm SD-MBR solves the skeptical default reasoning task SDEF( D, K q) correctly whenever Condition 12 and Condition 13 hold. 186 R. Khardon, D. Roth/Artijicial Intelligence 97 (1997) 169-193 Proof. The proof is similar to that of Theorem 14. We need to prove: if the algorithm if all extensions returns “Yes” then all extensions contain q, then the algorithm (i) (ii) first case (i) . The proof of Theorem 14 shows that every extension S is indeed Consider loop considered, test in SD-MBR. That is, no superset of S which is consistent with W is found. Therefore all extensions for C-MBR, which gives a correct answer. Thus, for some z E r. Since an extension S is maximal, returns “Yes” then all extensions to the subroutine contain q, and if the algorithm returns “Yes”. are passed the inner it passes contain q. For (ii) we have that all extensions contain q. Assume first that there exists z E Z$, the first step and goes into the flagged max (i.e., as extensions the algorithm passes to extensions. Assume loop. We argue loop) that all the subsets S identified indeed correspond such that q( z ) = 1, and therefore inner after the inner and that for some d, S* = S A d is consistent with W. Consider loop inner know that there S* as well, and will detect know required. in which d is the candidate is some u E W such therefore added that s* (u) = 1. But, Condition is a y E r which satisfies S*. Thus, that S is not maximal in the to S. Since S* is consistent with W we 13 holds for the algorithm this fact, set $a, = no-max, and ignore S as required. As in case (i) we says “Yes” as that the answers of C-MBR are correct and thus the algorithm the iteration there We now prove that, as assumed above, there exists z E rk that since all extensions Notice extension E, there exists x E W A SE A q. As above this, and thus the assumption holds. y E ri contain q, for all extensions, satisfying this implies 0 and in particular such that q( z ) = 1. for some that there is also a Using Theorem 19 and Lemma 17 we get: Corollary 20. The algorithm SD-MBR solves the skeptical default reasoning task SDEF( D, W q) correctly, for all q E & and for all (D, Q, r> in ‘TV. 5.3. Application: Diagnosis using models One of the useful applications of default logic is for the problem of circuit diagnosis for example [ 211. Consider gate and one or gate. In order to diagnose possible problems every gate, a new variable denoting will correspond variables, the circuit d +- a A b; e +- d V c, composed of one and in the circuit we add, for In our example Ni these new to the and gate and N2 will correspond of the circuit can be described by that it is operating normally. to the or gate. Using the functionality W = (Nlab -+ d)(Nld ---f a)(Nld -+ b)(Nzc ---f e)(Nzd --+ e)(Nze --+ cvd). Under normal conditions we want to assume This is captured and in general by having one rule for each gate’s “normality” in our example by the set of positive simple default rules D = { 2, variable. that all the gates are operating normally. $}, In the absence of any further evidence, considering the default problem that there gates are operating normally. is only one extension, D. This should be interpreted ( W, D) reveals that all the as stating R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-l 93 187 Algorithm Diug-MBR(T, 0, D, q): Let ro = {z E r 1 O(z) = 1). Call SD-MBR( ro, D, q) and answer in the same way. Fig. 5. Diag-MBR: model-based diagnostic reasoning. Suppose, however, that we observe that c = 1 and e = 0. In such a case the circuit can reasoning problem the default as W’ = W A cZ, and we need to consider It is easy to see that in this case there is only one extension which includes N1 be described ( W', 0). but does not include N2. This should be interpreted as stating that a minimal explanation for the fault is that gate number 2 is faulty. it is not always the case that observations exactly determine Of course, the fault in that we observe a = 1, b = 1, and e = 0. In this case In one extension we have Ni but not N2 that could the possible scenarios for example, the circuit. Suppose, W’ = W r\abZ, and ( W’, D) has two extensions. and in the other we have N2 but not Nt. This identifies have caused correctly. the skeptical default the following the fault?’ It is easy to see that the answer If we want to know whether interpretation: the circuit the fault in the circuit; either the and gate or the or gate are not functioning implies d -+ a, it makes sense to use reasoning paradigm with (W’, D) and the query d + a. This has the specifics of “can d -+ a be deduced without knowing is “No” (due to the case Ni = 0). We now show how to apply our positive results for default reasoning to the problem of that in the problem of diagnosis our knowledge about the world varies diagnosis. Observe with the observations. Therefore On top of that we have the normality of the gates. This suggests The algorithm context the algorithm SD-MBR. information. to support Then the observations the default serve as a kind of context rules that capture the algorithm Diag-MBR, on in Fig. 5. 0 serve as the it uses the set of filtered models as the knowledge base for the assumptions described information. first uses filtering as in C-MBR, where the observations As the following theorem shows, for the strategy to succeed it is sufficient that the expression (0 ---) (S -+ q) > is supported as a query by the characteristic models. Theorem 21. The algorithm Diag-MBR solves the diagnosis task correctly as long as 0 includes at most r observations, q is a k-quasi-Horn expression, and I’ = l$?. the set of observation Proof. Given 0, SDEF( D, (W A 0), q). As in Theorem considered by the algorithm, used properly. and the requirement 19 we need that the subroutine is to solve the default that all extensions to show for reasoning within context task are is Let SE be an extension of WA 0. Namely, there is a model u E WA 0 A SE. Since SE is a subset of D it includes only positive 0 has at most r literals BH~,. includes there is an assignment literals. Also, by the conditions of the theorem, therefore at most r negative that b which agrees with both 0 and SE on all literals. Thus, z E r, z <b u such that z agrees with b and u on all the literals literals). This an assignment implies (and 188 R. Khardon, D. Roth/Arti$cial Intelligence 97 (1997) 169-193 in SE and 0. The assignment extension SE will be properly that an extension is maximal due to the existence of z.) z will survive identified. then (Note it is maximal the filtering stage of the algorithm, and the if the algorithm claims it would be detected since otherwise that as before, to test whether When an extension S is identified by the algorithm SD-MBR, it uses a subroutine the question q follows given the context S. In our case, call to C-MBR to W A 0 A S k q. Using the question Theorem 8 we get that this is answered correctly as long as B is a basis for 5 V 3 V q. Since S is monotone, and 0 has at most r literals (W A 0) ks q which this holds in our case. is equivalent is whether 0 5.4. Default reasoning and relevance The use of defaults in reasoning is motivated by the desire in the presence of incomplete given with respect to “typical” cases, namely, decision-making information relevant should, to the current situation. Moreover, intuitively, contribute somehow to allow for efficient the information. The goal is to exploit rules, when it is the relevant cases the default this process of identifying to reducing the complexity of reasoning. However, it turns out that when using default logic as the default reasoning this goal is missed. While each rule in the representation a set of default task is defined default tries to capture all plausible to all these scenarios. logic can be traced to this search for possible scenarios. in relation rules framework, a local assumption, scenarios, and the default reasoning constitutes In fact, the computational difficulties in As we have shown, the relation of defaults to relevance can be viewed through reasoning the notion of context. Namely, default of reasoning within context where through the use of default can, in some cases, capture all possible extensions one can consider one context at a time and use the filtering on the relevant in each of these contexts. This default reasoning rules. We have also shown information algorithms. can be viewed as a generalization information representations that model-based context in an accessible form. In these cases, in order to focus to yield efficient technique is shown the agent has to find plausible 6. Relevance to the task The view we take on common-sense reasoning a very complex world not be omniscient, of tasks. Given a complete description perhaps, can perform as well using only partial that we relax the requirements, of the world that may be hard to represent exactly. Luckily, but rather has to perform well on a fairly wide, but restricted, is that the agent has to function the question is whether in order to reason correctly with respect information that is relevant in the agent need set the agent needs to it, or to the task. the One way computational to prune prune a knowledge perform. to study this issue is via our notion of relevance-a way to reduce cost of reasoning. While before we used relevance or context information the knowledge representation for a particular situation, representation relative to the general task the agent it is also possible is supposed to to R. Khmdon. D. Roth/Artificial Intelligence 97 (1997) 169-193 189 Consider the deduction problem, and suppose that our agent were to wander in a in which all queries are restricted in some form, or belong that the agent needs to answer correctly only queries be wrong on queries not in Q, as it is not going to some language in &, and may to be queried on those world &. This means (potentially) anyway. it is known In this case, that an incomplete to support correct deduction. This can be formalized using the notion of a least upper bound representation, introduced by Kautz and Selman [ 271. Intuitively, In to 8. of W which belong capture all the conclusions support exact deduction with respect queries approximations description of the world is sufficient these approximations it is shown that these [9] in Q. that we have used earlier to the class of queries. Thus, our results the use of model-based In fact, the model-based representations such approximations relative notion of relevance to the task as well. Moreover, is essential to exploit should be in a form example, of the with this representation based efficiently. the relevance that supports the task is answering least upper bound of W with respect representation suppose logn-CNF to the task. The reason efficient solutions of the reasoning problem. For to reason and therefore not feasible. The existence of a compact model- the reasoning of this least upper bound, enables us to perform to logn-CNF queries. Given a CNF representation it is NP-hard in the paper capture implicitly use the representations is that the representation 7. Learning to reason sections support an incremental that a model-based representation view of reasoning can be used to The results presented in previous in a natural way. We have shown reason correctly when some additional This relevant has a complete context. information knowledge augments constraining the agents’ knowledge context and aids information in deriving to this context. We call this a top-down solution, since it assumes base, but uses only parts of it, depending is supplied. conclusions that the agent on the current It is conceivable, though, for an agent to have only some of the models, from some specific context d. In such a case, our results this context approach. This approach that come agent reasons correctly within this a bottom-up can construct “narrower” reason correctly supports many possible contexts, even those never experienced. those models the that (although not within every context). We call agent supports together many to a more complete knowledge base, which incrementally In each of those, views from different contexts. by pasting the agent that an intelligent and, eventually, a representation of the world is guaranteed it constructs the view show This intuitive approach can be cast in a more general that learn, nature of reasoning. its task is the same world the inductive performs learning. This intuition There, an agent first wanders around unknown distribution D which governs is captured In systems that supplies in the distribution framework which emphasizes the agent the world in which the agent with the information when theory [ 301. free model of learning in the world, observing examples drawn from some in the world. Then, the occurrences of instances 190 R. Khardon, D. Roth/Artijicial Intelligence 97 (1997) 169-193 to classify its task, namely instances. The agent the same arbitrary “world” the agent has to perform to err on some set of instances Thus, used to measure formulations the world that does not depend on the world it functions deductions). is allowed as long as the measure of this set under D is small. in the learning phase is that supplies later. This intuition was not captured by early of description in some way to make arbitrary the agent has an exact formula-based and its performance the agent’s performance in (e.g., by the ability of reasoning, where a CNF formula), the information (traditionally, is defined In [ 71 a general into the study of reasoning. learning ideas above access to its favorite interact with performance reasoning with queries cy from some query whether W implies cy. this interface In this framework framework Learning to Reason is defined, the intelligent interface, and is also given a grace period its representation6 the agent is given in which it can KB of the world W. The is measured only after this period, when the agent is presented to the world, and has to answer that incorporates and construct language, relevant This framework for a formal study of yet another manifestation allows as a way to reduce the computational of relevance may call it relevance to the environment. Namely, be measured by some criterion it is shown in [7] additional reasoning power. that through cost of reasoning. the performance of our notion In this case we of an agent has to in. Indeed, truly gains that depends on the world the agent functions the agent this interaction with the world, We briefly describe can focus on relevant results rely on the use of model-based two results which emphasize how, within information and what they gain from using this framework, agents these it. As before, representations. A sampling approach Suppose we have access W (e.g., the “conference” samples according W A d. It can be shown can answer correctly is evasive (Formally, to random examples to the distribution D that governs from a certain context d in our world context discussed above). This may allow us to take random in of instances random examples all questions of length < p, which are not evasive. A statement it is falsified very rarely. [ 71 that a sample of m = (p/e) the occurrences ln( l/S) if it is not implied by the world but, in practice, (Y is evasive if W A d w cr but PrD [ W A d A Z] < E.) This exemplifies the notion of relevance to the environment: interacts with (the non-evasive is defined by the distribution D, and the queries to this environment. are defined relative the environment the agent the agent cares about Notice framework set of models, which supports exact reasoning, probably-approximately-correct in the to the usage of model-based for reasoning within context. Now, instead of having a fixed and well defined a random set is used and we require only representations reasoning. queries) that this is very similar h Note that in this framework we need to distinguish between the world W and the agent’s representation KB. R. Khardon, D. Roth/ArtQicial intelligence 97 (1997) 169-193 191 Theory approximation and restricted queries The utility of knowledge approximations for capturing information relevant to the task in Section 6. We note that the use of these approximations is advantageous was discussed for other reasons as well. In [7] still not within bound approximations appropriate to the task and relevance environment, it is shown that while exact learning of functions reach, one can learn the model-based representations of functions. Thus, one can learn such approximations, is for the least upper in the the ideas of relevance and then use it for reasoning, to the environment. combining 8. Conclusions Reasoning with models is an intuitive paradigm In this paper we presented more evidence that has been shown to be theoret- for the utility of model-based representations in the as well as some restricted cases of default rea- support efficient reasoning that the notion of relevance can be naturally and successfully In particular, ically sound. representations. presence of varying context soning. We further argued utilized when reasoning with models. these information, varying about efficiently directly The basic computational context. We modeled task we considered this situation the world with context-specific using a model-based representation. information, by augmenting is the problem of reasoning within a the knowledge we have this task algorithm, and showed how to solve the filtering Our solution, information. (possibly conflicting) the relation of defaults default reasoning, In default implements the idea of ignoring irrelevant an agent may have many contexts. As we have shown, in different acquired vance can be viewed through be viewed as a generalization find plausible der certain an accessible rithms. form and this can be exploited restrictions, model-based representations information through context the notion of context. Namely, default of reasoning within context where reasoning the agent has the use of default rules. Furthermore, capture all possible scenarios to yield efficient default reasoning rules, to rele- can to un- in algo- The significance of these results is that they are achieved as natural extensions of reasoning, deductive tation does not support efficient reasoning. and hold in cases in which the traditional formula-based represen- Moreover, we have shown that these results support an incremental framework view of reasoning and the notion of in a natural way, and discussed it has been shown that relevance as manifested the model-based discussed here can be learned efficiently. This can be combined with context specific default rules that are acquired via rote learning or other learning processes to work in a plausible way. in it. In particular, within this framework representations the Learning to Reason [24] These results can be viewed as providing some theoretical style reasoning, where a set of “typical cases” of case-based representation. More work is needed issues. support for the usefulness is used as a knowledge of these in order to gain a deeper understanding 192 R. Khardon, D. Roth/Artificial Intelligence 97 (1997) 169-193 We believe that an effective use of the relevant information is an important part of any efficient solution of the reasoning serve as a good example, natural way. since task. In this respect, model-based representations they allow to exploit various aspects of relevance in a Acknowledgments We thank the anonymous reviewers for comments that helped to improve the presen- tation of this paper. References ] 1 I N. Alon, J. Bruck, J. Naor, M. Naor and R. Roth, Construction of asymptotically good low-rate error- correcting codes through pseudo-random graphs, IEEE Trans. Inform. Theory 38 (1992) 509-516. [2] N.H. Bshouty, Exact learning via the monotone theory, 1form. and Comput. 123 (1995) 146-153. [ 31 PN. Johnson-Laird, Mental Models (Harvard University Press, Cambridge, MA, 1983). [4] PN. Johnson-Laud and R.M.J. Byrne, Deduction (Lawrence Erlbaum, London, 1991). [S] H. Kautz, M. Kearns and B. Selman, Horn approximations of empirical data, Arttjkial Intelligence 74 (1995) 129-145. ]6] H. Kautz and B. Selman, Hard problems for simple default logics, Artificial Intelligence 49 ( 1991) 243-279. ]7 1 R. Khardon and D. Roth, Learning to reason, in: Proceedings AAAI-94, Seattle, WA ( 1994) 682-687. [8] R. Khardon and D. Roth, Learning to reason with a restricted view, in: Workshop on Computational Learning Theory (1995) 301-310. ]9] R. Khardon and D. Roth, Reasoning with models, Artificial Intelligence 87 ( 1996) 187-213. [ 101 J. Kolodner, Case-Eased Reasoning (Morgan Kaufmann, Los Altos, CA, 1993). [ 111 S.M. Kosslyn, Image and Mind (Harvard University Press, Cambridge, MA, 1983). [ 121 H.J. Levesque, Making believers out of computers, Arttficial Intelligence 30 (1986) 81-108. [ 131 H.J Levesque, Is reasoning too hard?, in: Proceedings 3rd NIX Research Symposium (1992). [ 141 J. McCarthy and P.J. Hayes, Some philosophical problems from the standpoint of artificial intelligence, in: B. Meltzer and D. Michie, eds., Machine Intelligence, Vol. 4 (Edinburgh University Press, Edinburgh, 1969). [ 15 1 M. Minsky, A framework for representing knowledge, in: PH. Winston, ed., The Psychology of Computer Vision (McGraw-Hill, New York, 1975); also in: R.J. Brachman and H.J. Levesque, Readings in Knowledge Representation (Morgan Kaufmann, Los Altos, CA, 1985). [ 161 J. Naor and M. Naor, Small-bias probability spaces: Efficient constructions and applications, SIAM I. Compuf. 22 ( 1993) 838-856. [ 171 C.H. Papadimitriou, On selecting a satisfying truth assignment, in: Proceedings 32nd Annual IEEE Symposium on Foutuiations of Computer Science (1991). [ 181 J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference (Morgan Kaufmann, Los Altos, CA, 1988). [ 191 R. Reiter, A logic for default reasoning, Art@cial Intelligence 13 (1980) 81-132. [ 201 R. Reiter, Nonmonotonic reasoning, in: Annual Reviews of Computer Science (Annual Reviews Inc., Palo Alto, CA, 1987) 147-188. [21] R. Reiter, A theory of diagnosis from first principles, Artificial Intelligence 32 ( 1987) 57-95. [ 221 D. Roth, Learning to reason: the non-monotonic case, in: Proceedings IJCAI-95, Montreal, Que. ( 1995) 1178-l 184. [23] D. Roth, On the hardness of approximate reasoning, Arttficial Intelligence 82 ( 1996) 273-302. 1241 D. Schuurmans and R. Greiner, Learning default concepts, in: Proceedings IOth Canadian Conference on Artificial Intelligence (CSCSI-94), Banff, Alta. ( 1994). R. Khardon, D. Roth/Art$cial Intelligence 97 (1997) 169-193 193 [25] B. Selman, Tractable default Toronto, Ont. ( 1990). reasoning, Ph.D. Thesis, Department of Computer Science, University of [ 261 B. Selman and H. Kautz, Model-preference 1271 B. Selman and H. Kautz, Knowledge compilation [ 281 B. Selman and H.J. Levesque, Abductive and default default and theory approximation, theories, Artificial Intelligence 45 ( 1990) 287-322. J. ACM 43 ( 1996) 193-224. core, in: Proceedings a computational reasoning: AAAI-90, Boston, MA ( 1990) 343-348. [ 291 D.S. Touretzky, multiple inheritance J.F. Horty and R.H Thomason, A clash of intuitions: the current state of nonmonotonic systems, in: Proceedings IJCAI-87, Milan, Italy (Morgan Kaufmann, Los Altos, CA, 1987). [ 301 L.G. Valiant, A theory of the learnable, Comm. ACM 27 (1984) 1134-l 142. [ 3 1 ] L.G. Valiant, Rationality, in: Workshop on Computational Oarning Theory ( 1995) 3-14. 