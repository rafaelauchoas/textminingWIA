Artificial Intelligence 189 (2012) 19–47Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDiscovering hidden structure in factored MDPsAndrey Kolobov∗, Mausam, Daniel S. WeldDept. of Computer Science and Engineering, University of Washington, Seattle, WA 98195, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 1 August 2010Received in revised form 8 April 2012Accepted 9 May 2012Available online 15 May 2012Keywords:Markov Decision ProcessMDPPlanning under uncertaintyGeneralizationAbstractionBasis functionNogoodHeuristicDead end1. IntroductionMarkov Decision Processes (MDPs) describe a wide variety of planning scenarios rangingfrom military operations planning to controlling a Mars rover. However, today’s solutiontechniques scale poorly, limiting MDPs’ practical applicability. In this work, we proposealgorithms that automatically discover and exploit the hidden structure of factored MDPs.Doing so helps solve MDPs faster and with less memory than state-of-the-art techniques.Our algorithms discover two complementary state abstractions — basis functions andnogoods. A basis function is a conjunction of literals; if the conjunction holds true in a state,this guarantees the existence of at least one trajectory to the goal. Conversely, a nogood is aconjunction whose presence implies the non-existence of any such trajectory, meaning thestate is a dead end. We compute basis functions by regressing goal descriptions through adeterminized version of the MDP. Nogoods are constructed with a novel machine learningalgorithm that uses basis functions as training data.Our state abstractions can be leveraged in several ways. We describe three diverseapproaches — GOTH, a heuristic function for use in heuristic search algorithms suchas RTDP; ReTrASE, an MDP solver that performs modified Bellman backups on basisfunctions instead of states; and SixthSense, a method to quickly detect dead-end states.In essence, our work integrates ideas from deterministic planning and basis function-basedapproximation, leading to methods that outperform existing approaches by a wide margin.© 2012 Elsevier B.V. All rights reserved.Markov Decision Processes (MDPs) are a popular framework for modeling problems involving sequential decision-makingunder uncertainty. Examples range from military-operations planning to user-interface adaptation to the control of mobilerobots [1,36]. Unfortunately, however, existing techniques for solving MDPs, i.e. deciding which actions to execute in varioussituations, scale poorly, and this dramatically limits MDPs’ practical utility.Humans perform surprisingly well at planning under uncertainty, largely because they are able to recognize and reuseabstractions, generalizing conclusions across different plans. For example, after realizing that the walls of a particular Marscrater are too steep for the rover to escape, a human planner would abandon attempts to collect any of the rock samples inthe crater, while a traditional MDP solver might rediscover the navigational problem as it considered collecting each samplein turn.This article presents new algorithms for automatically discovering and exploiting such hidden structure in MDPs. Specif-ically, we generate two kinds of abstraction, basis functions and nogoods, each of which describes sets of states that share asimilar relationship to the planning goal. Both basis functions and nogoods are represented as logical conjunctions of statevariable values, but they encode diametrically opposite information. When a basis function holds in a state, this guarantees* Corresponding author.E-mail addresses: akolobov@cs.washington.edu (A. Kolobov), mausam@cs.washington.edu (Mausam), weld@cs.washington.edu (D.S. Weld).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.05.00220A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47that a certain trajectory of action outcomes has a positive probability of reaching the goal. Our algorithms associate weightswith each basis function, encoding the relative quality of the different trajectories. In contrast, when a nogood holds in astate, it signifies that the state is a dead-end; no trajectory can reach the goal from this state. Continuing the Mars roverexample, a conjunction that described presence in the steep-walled crater would be a nogood.Our notions of basis function and nogood are similar to the rules learned in logical theories in explanation-based learn-ing and constraint satisfaction [27,14], but our work applies them in a probabilistic context (e.g., learns weights for basisfunctions) and provides new mechanisms for their discovery. Previous MDP algorithms have also used basis functions [21,39], but to perform generalization between different problems in a domain rather than during the course of solving a singleproblem. Other researchers have used hand-generated basis functions in a manner similar to ours [22,23,20], but we presentmethods for their automatic generation.1.1. Discovering nogoods and basis functionsWe generate basis functions by regressing goal descriptions along an action outcome trajectory using a determinizedversion of the probabilistic domain theory. Thus, the trajectory is potentially executable in all states satisfying the basisfunction. This justifies performing Bellman backups on basis functions, rather than states — generalizing experience acrosssimilar states. Since many basis functions typically hold in a given state, the value of a state is a complex function of theapplicable basis functions.We discover nogoods using a novel machine learning algorithm that operates in two phases. First it generates candidatenogoods with a probabilistic sampling procedure using basis functions and previously discovered dead ends as trainingdata. It then tests the candidates with a planning graph [6] to ensure that no trajectories to the goal could exist from statescontaining the nogood.1.2. Exploiting nogoods and basis functionsWe present three algorithms that leverage our basis function and nogood abstractions to speed MDP solution and reducememory usage.• GOTH uses a full classical planner to generate a heuristic function for an MDP solver for use as an initial estimate of statevalues. While classical planners have been known to provide an informative approximation of state value in probabilisticproblems, they are too expensive to call from every newly visited state. GOTH amortizes this cost across multiple statesby associating weights to basis functions and thus generalizing the heuristic computation. Empirical evaluation showsGOTH to be an informative heuristic that saves MDP solvers considerable time and memory.• ReTrASE is a self-contained MDP solver based on the same information-sharing insight as GOTH. However, unlike GOTH,which sets the weight of each basis function only once to provide the starting guess at states’ values, ReTrASE learnsthe basis functions’ weights by evaluating each function’s “usefulness” in a decision-theoretic way. By aggregating theweights, ReTrASE constructs a state value function approximation and, as we show empirically, produces better policiesthan the participants of the International Probabilistic Planning Competition (IPPC) on many domains while using littlememory.• SixthSense is a method for quickly and reliably identifying dead ends, i.e., states with no possible trajectory to the goal, inMDPs. In general, this problem is intractable — one can prove that determining whether a given state has a trajectoryto the goal is PSPACE-complete [19]; therefore, it is unsurprising that modern MDP solvers often waste considerableresources exploring these doomed states. SixthSense acts as a submodule of an MDP solver, helping it detect and avoiddead ends. SixthSense employs machine learning, using basis functions as training data, and is guaranteed never togenerate false positives. The resource savings provided by SixthSense are determined by the fraction of dead ends inthe MDP’s state space and reach 90% on some IPPC benchmark problems.In the rest of the paper, we present these algorithms, discuss their theoretical properties, and evaluate them empirically.Section 2 reviews the background material and introduces relevant definitions, illustrating these with a running example.Sections 3, 4, and 5 present descriptions of and empirical results on GOTH, ReTrASE, and SixthSense respectively. Section 6discusses potential extensions of the presented algorithms. Finally, Section 7 describes the related work and Section 8concludes the paper.2. Preliminaries2.1. ExampleThroughout the paper, we will be illustrating various concepts with the following scenario, called GremlinWorld. Con-sider a gremlin that wants to sabotage an airplane and stay alive in the process. To achieve the task, the gremlin can pickup several tools. The gremlin can either tweak the airplane with a screwdriver and a wrench, or smack it with a hammer.A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4721(define (domain GremlinWorld)(:types tool)(:predicates (has ?t - tool)(gremlin-alive)(plane-broken))(:constants Wrench - toolScrewdriver - toolHammer - tool)(:action pick-up:parameters (?t - tool):precondition (and (not (has ?t))):effect (and (has ?t)))(:action tweak:parameters ():precondition (and (has Screwdriver):effect (and (plane-broken)))(has Wrench))(:action smack:parameters ():precondition (and (has Hammer)):effect (and (plane-broken)(probabilistic 0.9)(and (not (gremlin-alive))))))(define (problem GremlinProb)(:domain GremlinWorld)(:init (gremlin-alive))(:goal (and (gremlin-alive) (plane-broken))))Fig. 1. A PPDDL-style description of the example MDP, GremlinWorld, split into domain and problem parts.However, smacking will, with high probability, lead to accidental detonation of the airplane’s fuel, which destroys the air-plane but also kills the gremlin. Fig. 1 describes this setting in Probabilistic Planning Domain Description Language (PPDDL).As we introduce relevant terminology in subsequent subsections, we will formally define the corresponding MDP.2.2. BackgroundMarkov Decision Processes (MDPs). In this paper, we focus on probabilistic planning scenarios modeled by discrete factoredstochastic-shortest-path (SSP) MDPs with an initial state. In general, MDPs are defined as tuples of the form (cid:3)S, A, T , C(cid:4),where• S is a set of states.• A is a set of actions.• T is a transition function S × A × S → [0, 1] giving the probability of moving from si to s j by executing action a.• C is a map S × A → R specifying action costs.The MDPs we consider in this paper are a specific kind defined as a tuple (cid:3)X , A, T , C, G, s0(cid:4), where A, T , and C are asabove and• X is a set of state variables s.t. every conjunction of literals over all variables in X is a state of the MDP. Therefore, witha slight abuse of notation, we can set S = 2Xin the general MDP definition.• G is a set of (absorbing) goal states.• s0 is the start state.• All action costs are positive, i.e. C is a map S × A → R+.1We assume that both the state space (2X) and the action space (A) are finite. Another assumption we make is that eachaction of the MDP has a precondition, a conjunction of literals describing the states in which the action can be executed.1 This requirement is actually stricter, although much easier to state, than in the original SSP MDP’s definition [5]. The original statement allows costs tobe completely arbitrary as long as each policy that does not reach the goal incurs an infinite cost. However, the algorithms in this paper apply to all MDPsfalling under that definition as well.22A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47(:action pick-up-0:parameters (?t - tool):precondition (and (not (has ?t))):effect (and (has ?t)))(:action tweak-0:parameters ():precondition (and (has Screwdriver):effect (and (plane-broken)))(has Wrench))(:action smack-0:parameters ():precondition (and (has Hammer)):effect (and (plane-broken)))(:action smack-1:parameters ():precondition (and (has Hammer)):effect (and (plane-broken)(not (gremlin-alive))))Fig. 2. All-outcomes determinization of the GremlinWorld domain.Our example, GremlinWorld, can be formulated as an MDP using five state variables, gremlin-alive, plane-broken,has(Hammer), has(Wrench), and has(Screwdriver), abbreviated as G, P , H , W , and S respectively. Therefore, X = {G, P , H,W , S}. The problem involves five actions, A = {pick-up(Screwdriver), pick-up(Wrench), pick-up(Hammer), tweak( ), smack( )}.Each action has a precondition; e.g., the smack( ) action’s precondition is a single-literal conjunction (has Hammer), sosmack( ) can only be used in states where the gremlin has a hammer. Actions’ preconditions and effects compactly specifythe transition function T . For simplicity, we make C assign the cost of 1 to all actions, which conforms to the restriction onC imposed by the SSP MDP definition. G is the set of all states where the gremlin is alive and the airplane is broken. Finally,we assume that the gremlin starts alive with no tools and the airplane is originally intact, i.e. s0 = (G, ¬P , ¬H, ¬W , ¬S).Solving an MDP means finding a good (i.e., cost-minimizing) policy π : S → A that specifies the actions the agent shouldtake to eventually reach the goal. The optimal expected cost of reaching the goal from a state s, termed the optimal valuefunction V∗(s), satisfies the following conditions, called Bellman equations:∗∗VV(s) = 0 if s ∈ G, otherwise(cid:4)(cid:3)(cid:2)(s) = mina∈AC(s, a) +Ts, a, ss(cid:7)∈S(cid:5)(cid:7)(cid:4)∗(cid:7)sV(cid:6)(cid:5).Given V∗(s), an optimal policy may be computed as follows:(cid:3)(cid:6)C(s, a) +(cid:4)T(cid:5)(cid:7)(cid:4)∗(cid:5)(cid:7)s.s, a, sV(cid:2)(s) = arg mina∈Aπ ∗s(cid:7)∈SSolution methods. The above equations suggest a dynamic programming-based way of finding an optimal policy, calledvalue iteration (VI) [3]. VI iteratively updates state values using Bellman equations in a Bellman backup until the valuesconverge. VI has given rise to many improvements. Trial-based methods, e.g., RTDP [2], try to reach the goal multiple times(in multiple trials) and update the value function over the states in the trial path, successively improving the policy duringeach Bellman backup. A popular variant, LRTDP, adds a termination condition to RTDP by labeling states whose values haveconverged as ‘solved’ [7]. Compared to VI, trial-based methods save space by considering fewer irrelevant states. LRTDPserves as the testbed in our experiments, but the approach we present can be used by many other search-based MDPsolvers as well, e.g., LAO[24].∗Determinization. Successes of a number of planners starting with FFReplan [42] have demonstrated the promise of deter-minizing the domain (the set of all actions) of the given MDP, i.e. disregarding the probabilities in the transition function, andworking only with the state transition graph. Our techniques use the all-outcomes determinization [42] Dd of the domainD at hand. Namely, note in the example in Fig. 1 that each action a, besides precondition c, has outcomes o1, . . . , on withrespective probabilities p1, . . . , pn. For example, thesmack ( ) action has outcomes o1 = P ∧ ¬G with p1 = 0.9 and o2 = Pwith p1 = 0.1. The all-outcomes determinization Dd, whose example for the GremlinWorld domain is shown in Fig. 2, con-tains, for every action a in the original domain, the set of deterministic actions a1, . . . , an, each with a’s precondition c andeffect oi . Dd, coupled with a description of the state space, the initial state, and the goal, can be viewed as a deterministicMDP in which a plan from a given state to the goal exists if and only if a corresponding trajectory has a positive probabilityin the original probabilistic domain D. Importantly, the state of the art in classical planning makes solving a determin-istic problem much faster than solving a probabilistic problem of a comparable size. Our abstraction framework exploitsA. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4723these facts to efficiently extract the structure of the given MDP by finding plans in Dd and processing them as shown inSection 2.3.Heuristic functions. We define a heuristic function, hereafter termed simply as heuristic, as a value function that initializesthe state values for an MDP algorithm. Heuristic values tend to be derived, automatically or otherwise, from the structureof the problem at hand. The properties of the heuristic determine how quickly a planning algorithm converges and whetherthe resulting policy is optimal. Algorithms like VI, which update the value of every state in each iteration, converge to theoptimal policy faster the closer the heuristic is to V. In trial-based algorithms like LRTDP, heuristics help avoid visitingirrelevant states. To guarantee convergence to an optimal policy, trial-based MDP solvers typically require the heuristic to beadmissible, i.e. to never overestimate V(importantly, admissibility is not a requirement for convergence to a policy). How-ever, inadmissible heuristics tend to be more informative in practice, approximating Vbetter on average. Informativenessoften translates into a smaller number of explored states (and the associated memory savings) with reasonable sacrificesin optimality. In this paper, we adopt the number of states visited by a planner under the guidance of a heuristic as themeasure of that heuristic’s informativeness and show how basis functions let us derive a highly informative heuristic, GOTH,at the cost of admissibility.∗∗∗A successful class of MDP heuristics is based on the all-outcomes determinization of the probabilistic domain D athand [8]. To obtain a value for state s in D, determinization heuristics try to approximate the cost of a plan from s to a goalin Dd (finding a plan itself even in this relaxed version of an MDP is generally NP-hard). For instance, the FF heuristic [26],denoted hFF , ignores the negative literals (the delete effects) in the outcomes of actions in Dd and attempts to find the cost ofthe cheapest solution to this new relaxed problem. As hFF is, in our experience, the most informative general MDP heuristic,we use it as the baseline to evaluate the performance of GOTH.Planning graph. Our work makes use of the planning graph data structure [6], a directed graph alternating between propo-sition and action “levels”. The 0-th level contains a vertex for each literal present in an initial state s. Odd levels containvertices for all actions, including a special no-op action, whose preconditions are present (and pairwise “nonmutex”) in theprevious level. Subsequent even levels contain all literals from the effects of the previous action level. Two literals in a levelare mutex if all actions achieving them are pairwise mutex at the previous level. Two actions in a level are mutex if theireffects are inconsistent, one’s precondition is inconsistent with the other’s effect, or one of their preconditions is mutex atthe previous level. As levels increase, additional actions and literals appear (and mutexes disappear) until a fixed point isreached. Graphplan [6] uses the graph as a polynomial-time reachability test for the goal, and we use it in a procedure todiscover nogoods in Section 5.2.3. Definitions and essentialsLet an execution trace e = s, a1, s1, . . . , an, sn, a sequence where s is the trace’s starting state, a1 is a probabilis-tic action applied in s that yielded state s1, and so on. An example of an execution trace from GremlinWorld is(cid:7) = (G, ¬P , ¬H, ¬W , ¬S), pick-up(Hammer), (G, ¬P , H, ¬W , ¬S), smack( ), (G, P , H, ¬W , ¬S).eWe define a trajectory of an execution trace e to be a sequencet(e) = s, out(a1, 1, e), . . . , out(an, n, e)where s is e’s starting state, and out(ak, k, e) is a conjunction of literals representing the particular outcome of action ak(cid:7)) = (G, ¬P , ¬H, ¬W , ¬S), H , P is a trajectory of the examplethat was sampled at the k-th step of e’s execution. E.g., t(eexecution trace e.(cid:7)(cid:7)) just shown is a goal trajectory. A suffix ofWe say that t(e) is a goal trajectory if the last state sn of e is a goal state; t(et(e) is a sequenceti(e) = out(ai, i, e), . . . , out(an, n, e)for some 1 (cid:2) i (cid:2) n.Suppose we are given an MDP and a goal trajectory t(e) of some execution trace in this MDP. Let prec(a) denote theprecondition of a (a literal conjunction) and lit(c) stand for the set of literals forming conjunction c. Imagine using t togenerate the following sequence of literal conjunctions:b0 = G,bi =(cid:7)(cid:8)(cid:8)lit(bi−1) ∪ lit(cid:4)out(an−i+1, n − i + 1, e)(cid:5)(cid:9)(cid:4)\ lit(cid:5)(cid:9)prec(an−i+1)for 1 (cid:2) i (cid:2) n.This can be done with a simple multistep procedure. We start with b0 = G, the MDP’s goal conjunction. Afterwards,at step i (cid:3) 1, we first remove from bi−1 the literals of action an−i+1’s outcome at the (n − i + 1)-th step of e. Then, weconjoin the result to the literals of an−i+1’s precondition, obtaining conjunction bi . We call this procedure regression of thegoal through trajectory t(e), orregression for short.24A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47As an example, consider regressing trajectory t(e(cid:7)) from GremlinWorld. In this case, b0 = G = G ∧ P . First we removefrom b0 literal P , the outcome of the last action, smack( ), of e. The result is G. Then, we add to it the precondition ofsmack( ), literal H , producing G ∧ H . Thus, b1 = G ∧ H . Similarly, we remove from b1 the outcome of pick-up(Hammer) andadd the precondition of this action, which is empty, to the result, obtaining b2 = G. At this point regression terminates.(cid:7)A basis function is defined to be a literal conjunction b produced at some step of regressing the goal through some tra-jectory. Whenever all literals of a basis function (or of a conjunction of literals in general) are present in state s we say thatthe conjunction holds in or represents s. For instance, b1 = G ∧ H from the above example holds in state (G, ¬P , H, ¬W , S).An alternative view of a basis function b is a mathematical function fb : S → {0, 1} having the value of 1 in all states inwhich conjunction b holds and 0 in all others.Basis functions are a central concept behind the algorithms in this paper, so it is important to understand the intuitionbehind them. Any goal trajectory is potentially a causally important sequence of actions. Regressing it gives us preconditionsfor the trajectory’s suffixes. Basis functions are exactly these trajectory suffix preconditions. Thus, regression of the trajec-tories can be thought of as unearthing the relevant causal structure necessary for the planning task at hand. Moreover, ourbasis functions are that causal structure.There are often many trajectories whose preconditions are consistent with (i.e., are a subconjunction of) a given basisfunction. We say that a basis function b enables a set of goal trajectories T if the goal can be reached from any staterepresented by b by following any of the trajectories in T assuming that Nature chooses the “right” outcome for each actionof the trajectory.Since each basis function is essentially a precondition (for a trajectory), it typically holds in many states of the MDPat hand. Therefore, obtaining a goal trajectory t(e) from some state lets us generalize this qualitative reachability informa-tion to many other states via basis functions yielded by regressing the goal through this trajectory. Moreover, t(e) mayhave interesting numeric characterizations, e.g. cost, probability of successful execution, etc. To generalize these quantitativedescriptions across many states as well, we associate a weight with each basis function. The semantics of basis functionweight depends on the algorithm, but in general it reflects the quality of the set of trajectories enabled by the basis function.Now, consider the value of an MDP’s state. As preconditions, basis functions tell us which goal trajectories are possiblefrom that state. Basis function weights tell us how “good” these trajectories are. Since the quality of the set of goal tra-jectories possible in a state is a strong indicator of the state’s value, knowing basis functions with their weights allows forapproximating the state value function.As we just showed, a problem’s causal structure can be efficiently derived from its goal trajectories via regression. Thus,a relatively cheap source of trajectories would give us a way to readily extract the structure of the problem. Fortunately,at least two such methods exist. The first one is based on this insight that whenever a trial in an MDP solver reaches thegoal we get a trajectory “for free”, as a byproduct of the solver’s usual computation. The caveat with using this techniqueas the primary strategy of getting trajectories is the time it takes an MDP solver’s trials to start attaining the goal. Indeed,the majority of trials at the beginning of planning terminate in states with no path to the goal, and it is at this stage thatknowing the problem’s structure would be most helpful for improving the situation. Therefore, our algorithms mostly rely ona different trajectory generation approach. Note that any trajectory in an MDP is a plan in the all-outcomes determinizationDd of that MDP and vice versa. Since classical planners are very fast, we can use them to quickly find goal trajectories inDd from several states of our choice.By definition, basis functions represent only the states from which reaching the goal is possible. However, MDPs alsocontain another type of states, dead ends, that fall outside of the basis function framework as presented so far. Such states,in turn, can be classified into two kinds; explicit dead ends, in which no actions are applicable, and implicit ones, which dohave applicable actions but no sequence of them leads to the goal with a positive probability. In GremlinWorld, there areno explicit dead ends but every state with literal ¬G is an implicit dead end.To extend information generalization to dead ends as well, we consider another kind of literal conjunctions that we callnogoods. Nogoods’ defining property is that any state in which a nogood holds is a dead end. Notice the duality betweennogoods and basis functions: both have exactly the same form but give opposite guarantees about a state. Whereas a staterepresented by a basis function provably cannot be a dead end, a state represented by a nogood certainly is one. Despite therepresentational similarity, identifying nogoods is significantly more involved than discovering basis functions. Fortunately,the duality between the two allows using the latter to derive the former and collect the corresponding benefits, as one ofthe algorithms we are about to present, SixthSense, demonstrates.3. GOTH heuristic3.1. MotivationOur presentation of the abstraction framework begins with an example of its use in a heuristic function. As alreadymentioned, heuristics reduce trial-based MDP solvers’ resource consumption by helping them avoid visiting many states(and memoizing corresponding state-value pairs) that are not part of the final policy. The most informative MDP heuristics,e.g., hFF , are based on the all-outcomes determinization of the domain. However, although efficiently computable, suchheuristics add an extra level of relaxation of the original MDP, besides determinizing it. For instance, hFF is liable to highlyA. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4725underestimate the state’s true cost because in addition to discarding the domain’s probabilities it ignores actions’ deleteeffects (i.e., negative literals, such as ¬G, in actions’ outcomes) in the determinized version.On the other hand, a lot of promise has been shown recently by several probabilistic planners that solve full (non-relaxed) determinizations, e.g., FFReplan, HMDPP [29], and others. It is natural to wonder, then, whether the improvedheuristic estimates of using a full classical planner on non-relaxed determinized domains would provide enough gains tocompensate for the potentially increased cost of heuristic computation.As we show in this section, the answer is “No and Yes”. We propose a new heuristic called GOTH (Generalization OfTrajectories Heuristic) [32], which efficiently produces heuristic state values using deterministic planning. The most straight-forward implementation of this method, in which a classical planner is called every time a state is visited for the first time,does produce better heuristic estimates and reduces search but the cost of so many calls to the classical planner vastlyoutweighs any benefits. The crucial observation we make is that basis functions provide a way to amortize these expensiveplanner calls by generalizing the resulting heuristic values to give guidance on similar states. By performing this generaliza-tion in a careful manner, one may dramatically reduce the amount of classical planning needed, while still providing moreinformative heuristic values than heuristics with more levels of relaxation.3.2. GOTH descriptionGiven a problem P over a probabilistic domain D, an MDP solver using GOTH starts with GOTH’s initialization. Duringinitialization, GOTH determinizes D into its classic counterpart, Dd (this operation needs to be done only once). Our im-plementation performs the all-outcomes determinization because it is likely to give much better value estimates than thesingle-outcome one [42]. However, more involved flavors of determinization described in the Related Work section mayyield even better estimation accuracy.Calling a deterministic planner. Once Dd has been computed, the probabilistic planner starts exploring the state space. Forevery state s that requires heuristic initialization, GOTH first checks if it is an explicit dead end. This check is in place forefficiency, since GOTH should not try to use more expensive methods of analysis on such states.For state s that is not an explicit dead end GOTH constructs a problem P s with the original problem’s goal and s asthe initial state, feeding P s along with Dd to a classical planner, denoted as DetPlan in the pseudocode of Algorithm 1,and setting a timeout. If s is an implicit dead end DetPlan either proves this or unsuccessfully searches for a plan until thetimeout. In either case, it returns without a plan, at which point s is presumed to be a dead end and assigned a very highvalue taken to be ∞. If s is not a dead end, DetPlan usually returns a plan from s to the goal. The cost of this plan is takenas the heuristic value of s. Sometimes DetPlan may fail to find a plan before the timeout, leading the MDP solver to falselyassume s to be a dead end. In practice, we have not seen this hurt GOTH’s performance.Regression-based generalization. By using a full-fledged classical planner, GOTH produces more informative state estimatesthan hFF , as evidenced by our experiments. However, invoking the classical planner for every newly encountered state iscostly; as it stands, GOTH would be prohibitively slow. To ensure speed, we modify the procedure based on the insight aboutbasis functions and their properties as shown in the pseudocode of Algorithm 1. Whenever GOTH computes a deterministicplan, it first regresses it, as described in Section 2. Then it memoizes the resulting basis functions with associated weightsset to the costs of the regressed plan suffixes. When GOTH encounters a new state s, it minimizes over the weights ofall basis functions stored so far that hold in s. In doing so, GOTH sets the heuristic value of s to be the cost of thecheapest currently known trajectory that originates at s. Thus, the weight of one basis function can become generalized asthe heuristic value of many states. This way of computing a state’s value is very fast, and GOTH employs it before invokinga classical planner. However, s’s heuristic value may be needed even before GOTH has any basis function that holds in s. Inthis case, GOTH uses the classical planner as described above, computing a value for s and augmenting its basis functionset. Evaluating a state first by generalization and then, if generalization fails, by classical planning greatly amortizes the costof each classical solver invocation and drastically reduces the computation time compared to using a deterministic planneralone.Weight updates. Different invocations of the deterministic planner occasionally yield the same basis function more thanonce, each time potentially with a new weight. Which of these weights should we use? The different weights are caused bya variety of factors, not the least of which are non-deterministic choices made within the classical planner.2 Thus, the basisfunction weight from any given invocation may be unrepresentative of the cost of the plans for which this basis function isa precondition. For this reason, it is generally beneficial to assign a basis function the average of the weights computed forit by classical planner invocations so far. This is the approach we take on line 27 of Algorithm 1. Note that to compute theaverage we need to keep the number of times the function has been re-discovered.2 For instance, LPG [18], which relies on a stochastic local search strategy for action selection, may produce distinct paths to the goal even when invokedtwice from the same state, with concomitant differences in basis functions and/or their weights.26A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47of M holds in s thenreturn a large penalty // e.g., 1 000 000(cid:7)return minbasis functions f that subsume s{M[ f ]}declare problem P s ← (cid:3)init. state s, goal G(cid:4)declare plan pl ← DetPlan(Dd, P s, T )if pl == none thenAlgorithm 1 GOTH heuristic.1: Input: probabilistic domain D, problem P = (cid:3)init. state s0, goal G(cid:4), determinization routine Det, classical planner Det Plan, timeout T , state s2: Output: heuristic value of s3:4: compute global determinization Dd = Det(D)5: declare global map M from basis functions to weights6:7: function computeGOTH(state s, timeout T )8: if no action a of D is applicable in s thenreturn a large penalty // e.g., 1000 0009:10: else if a nogood holds in s then11:12: else if some member f13:14: else15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33:34:35:36:37: end ifdeclare action a ← pl[i]weight ← weight + Cost(s, a)f ← ( f ∪ precond(a)) − effect(a)if fdeclare basis function f ← goal Gdeclare weight ← 0for all i = length(pl) through 1 doupdate M[ f ] by incorporating weight into M[ f ]’s running averageend forif SchedulerSaysYes thenlearn nogoods from discovered dead endsreturn a large penalty // e.g., 1 000 000insert (cid:3) f , weight(cid:4) into Mend ifreturn weightis not in M thenend ifend ifelseelseDealing with implicit dead ends. The discussion so far has ignored an important detail. When a classical planner is calledon an implicit dead end, by definition no trajectory is discovered, and hence no basis functions. Thus, this invocation isseemingly wasted from the point of view of generalization: it does not contribute to reducing the average cost of heuristiccomputation as described thus far.As it turns out, we can, in fact, amortize the cost of discovery of implicit dead ends in a way similar to reducing theaverage time of other states’ evaluation. To do so, we use the known dead ends along with stored basis functions to derivethe latter’s duals in our information-sharing framework, nogoods. We remind the reader that nogoods generalize dead endsin precisely the same way as basis functions do with non-dead ends and therefore help recognize many dead ends withoutresorting to classical planning. The precise nogood learning mechanism is called SixthSense and is described in Section 5. Itneeds to be invoked at several points throughout GOTH’s running time as prescribed by a scheduler that is also described inthat section. For now, we abstract away the operation of SixthSense on lines 32–34 of Algorithm 1. With nogoods available,positively deciding whether a state is a dead end is as simple as checking whether any of the known nogoods subsumes it(lines 8–9 of Algorithm 1). Deterministic planning is necessary to answer the question only if none do.Speed and memory performance. To facilitate empirical analysis of GOTH, it is helpful to look at the extra speed andmemory cost an MDP solver incurs while using it.Concerning GOTH’s memory utilization, we emphasize that, similar to hFF and many other heuristics, GOTH does notstore any of the states it is given for heuristic evaluation. It merely returns heuristic values of these states to the MDPsolver, which can then choose to store the resulting state-value pairs or discard them. However, to compute the values,GOTH needs to memoize the basis functions and nogoods it has extracted so far. As our experiments demonstrate, the setof basis functions and nogoods discovered by GOTH throughout the MDP solver’s running time is rather small and is morethan compensated for by the reduction in the explored fraction of the state space due to GOTH’s informativeness, whencompared to hFF .Timewise, GOTH’s performance is largely dictated by the speed of the employed deterministic planner(s) and the numberof times it is invoked. Another component that may become significant is determining the “cheapest” basis function thatholds in a state (line 11 of Algorithm 1), as it requires iterating, on average, over a constant fraction of known basisfunctions. Although faster solutions are possible for this pattern-matching problem, all that we are aware of (e.g., [16]) payfor the increase in speed with degraded memory performance.A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4727Fig. 3. GOTH outperforms hFF on Machine Shop, Triangle Tireworld, and Blocksworld in speed by a large margin.Theoretical properties. Two especially important theoretical properties of GOTH are the informativeness of its estimates andits inadmissibility. The former ensures that, compared to hFF , GOTH causes MDP solvers to explore fewer states. At the sametime, like hFF , GOTH is inadmissible. One source of inadmissibility comes from the general lack of optimality of deterministicplanners. Even if they were optimal, however, employing timeouts to terminate the classical planner occasionally causesGOTH to falsely assume states to be dead ends. Finally, the basis function generalization mechanism also contributes toinadmissibility. The set of discovered basis functions is almost never complete, and hence even the smallest basis functionweight known so far may be an overestimate of the state’s true value, as there may exist an even cheaper goal trajectoryfrom this state that GOTH is unaware of. In spite of theoretical inadmissibility, in practice using GOTH usually yields verygood policies whose quality is often better than of those found under the guidance of hFF .3.3. Experimental resultsOur experiments compare the performance of a probabilistic planner using GOTH to that of the same planner under theguidance of hFF across a wide range of domains. In our experience, hFF , included as a part of miniGPT [8], outperforms allother well-known MDP heuristics on most IPPC domains, e.g., the min-min and atom-min heuristics supplied in the samepackage. Our implementation of GOTH uses a portfolio of two classical planners, FF and LPG. To evaluate a state, it launchesboth planners as in line 12 of Algorithm 1 in parallel and takes the heuristic value from the one that returns sooner. Thetimeout for each deterministic planner for finding a plan from a given state to a goal was 25 seconds. We tested GOTH andhFF as part of the LRTDP planner available in the miniGPT package. Our benchmarks were six probabilistic domains, five ofwhich come from the two most recent IPPCs with goal-oriented problems: Machine Shop [37], Triangle Tireworld (IPPC-08),Exploding Blocks World (IPPC-08 version), Blocks World (IPPC-06 version), Elevators (IPPC-06), and Drive (IPPC-06). All ofthe remaining domains from IPPC-06 and IPPC-08 are either easier versions of the above (e.g., Tireworld from IPPC-06) orhave features not supported by our implementation of LRTDP (e.g., rewards, universal quantification, etc.) so we were notable to test on them. Additionally, we perform a brief comparison of LRTDP + GOTH against FFReplan, since it shares someinsights with GOTH. In all experiments except measuring the effect of generalization, the planners had a 24-hour limitto solve each problem. All experiments for GOTH, as well as those forReTrASE and SixthSense, described in Sections 4.3and 5.3 respectively, were performed on a dual-core 2.8 GHz Intel Xeon processor with 2 GB of RAM.Comparison against hFF . In this subsection, we use each of the domains to illustrate various aspects and modes of GOTH’sbehavior and compare it to the behavior of hFF . As shown below, on five of the six test domains LRTDP + GOTH substantiallyoutperforms LRTDP + hFF .We start the comparison by looking at a domain whose structure is especially inconvenient for hFF , Machine Shop.Problems in this set involve two machines and a number of objects equal to the ordinal of the corresponding problem. Eachobject needs to go through a series of manipulations, of which each machine is able to do only a subset. The effects ofsome manipulations may cancel the effects of others (e.g., shaping an object destroys the paint sprayed on it). Thus, theorder of actions in a plan is critical. This domain illuminates the drawbacks of hFF , which ignores delete effects and doesnot distinguish good and bad action sequences as a result. Machine Shop has no dead ends.Figs. 3 and 4 show the speed and memory performance of LRTDP equipped with the two heuristics on problems fromMachineShop (and two other domains) that at least one these planners could solve without running out of memory. Asimplied by the preceding discussion of GOTH’s space requirements, the memory consumption of LRTDP + GOTH is measuredby the number of states, basis functions, and nogoods whose values need to be maintained (GOTH caches basis functionsand LRTDP caches states). In the case of LRTDP + hFF all memory used is only due to LRTDP’s state caching because hFF byitself does not memoize anything. On Machine Shop, the edge of LRTDP + GOTH is clearly vast, reaching several orders ofmagnitude. In fact, LRTDP + hFF runs out of memory on the three hardest problems, whereas LRTDP + GOTH is far fromthat.Concerning policy quality, we found the use of GOTH to yield optimal or near-optimal policies on Machine Shop. Thiscontrasts with hFF whose policies were on average 30% more costly than the optimal ones.28A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47Fig. 4. GOTH’s advantage over hFF on Machine Shop, Triangle Tireworld, and Blocksworld in memory is large as well.Fig. 5. The big picture: GOTH provides a significant advantage on large problems. (Note that the axes are on a Log scale.)The Triangle Tireworld domain, unlike Machine Shop, does not have structure that is particularly adversarial for hFF .However, LRTDP + GOTH noticeably outperforms LRTDP + hFF on it too, as Figs. 3 and 4 indicate. Nonetheless, neitherheuristic saves enough memory to let LRTDP solve past problem 8. In terms of solution quality, both planners find optimalpolicies on the problems they can solve.The results on Exploding Blocks World (EBW, Fig. 5) are similar to those on Triangle Tireworld, where the LRTDP +GOTH’s more economical memory consumption eventually translates to a speed advantage. Importantly, however, on severalEBW problems LRTDP + GOTH is superior to LRTDP + hFF in a more illustrative way: it manages to solve four problems onwhich LRTDP + hFF runs out of space. The policy quality of the planners is similar.The Drive domain is small, and using GOTH on it may not provide significant benefit. On Drive problems, planners spendmost of the time in decision-theoretic computation but explore no more than around 2000 states. LRTDP under the guidanceof GOTH and hFF explores roughly the same number of states, but since so few of them are explored generalization doesnot play a big role and GOTH incurs the additional overhead of maintaining the basis functions without getting a significantbenefit from them. Perhaps surprisingly, however, GOTH sometimes leads LRTDP to find policies with higher success rates(coverage), while never causing it to find worse policies than hFF . The difference in policy quality reaches 50% on the Drivedomain’s largest problems. Reasons for this are a topic for future investigation.On the remaining test domains, Elevators and Blocksworld, LRTDP + GOTH dominates LRTDP + hFF in both speed andmemory while providing policies of equal or better quality. Figs. 3 and 4 show the performance on Blocksworld as anexample. Classical planners in our portfolio cope with determinized versions of these domains very quickly, and abstractionensures that the obtained heuristic values are spread over many states. Similar to the situation for EBW, the effectivenessof GOTH is such that LRTDP + GOTH can solve even the five hardest problems of Blocksworld, which LRTDP + hFF couldnot.Fig. 5 provides the big picture of the comparison. For each problem we tried, it contains a point whose coordinatesare the logarithms of the amount of time/memory that LRTDP + GOTH and LRTDP + hFF took to solve that problem. Thus,points that lie below the Y = X line correspond to problems on which LRTDP + GOTH did better according to the respectivecriterion. The axes of the time plot of Fig. 5 extend to log2(86 400), the logarithm of the time cutoff (86 400 s, i.e. 24 hours)that we used. Similarly, the axes of the memory plot reach log2(10 000 000), the number of memoized states/basis functionsat which the hash tables where they are stored become too inefficient to allow a problem to be solved within the 86 400 sA. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4729Table 1Average ratio of the number of states memoized by LRTDP under the guidance of hFF to the number under GOTH acrosseach test domain. The bigger these numbers, the more memory GOTH saves the MDP solver compared to hFF .EBW2.07EL4.18TTW1.71DR1.00MS14.40BW7.72Fig. 6. GOTH is much faster with generalization than without.time limit. Thus, the points that lie on the extreme right or top of these plots denote problems that could not be solvedunder the guidance of at least one of the two heuristics. Overall, the time plot shows that, while GOTH ties or is slightlybeaten by hFF on Drive and smaller problems of other domains, it enjoys a comfortable advantage on most large problems.In terms of memory, this advantage extends to most medium-sized and small problems as well, and sometimes translatesinto a qualitative difference, allowing GOTH to handle problems that hFF cannot.Why does GOTH’s and hFF ’s comparative performance differ from domain to domain? For insight, refer to Table 1. Itdisplays the ratio of the number of states explored by LRTDP + hFF to the number explored by LRTDP + GOTH, averagedover the problems that could be solved by both planners in each domain. Thus, these numbers reflect the relative infor-mativeness of the heuristics. Note the important difference between the data in this chart and memory usage as presentedon the graphs: the information in the table disregards memory consumption due to the heuristics, thereby separating thedescription of heuristics’ informativeness from a characterization of their efficiency. Associating the data in the table withthe relative speeds of LRTDP + hFF and LRTDP + GOTH on the test domains reveals a clear trend; the size of LRTDP +GOTH’s speed advantage is strongly correlated with its memory advantage, and hence with its advantage in informative-ness. In particular, GOTH’s superiority in informativeness is not always sufficient to compensate for its computation cost.Indeed, the 1.71× average reduction (compared to hFF ) in the number of explored states on Triangle Tireworld is barelyenough to make good the time spent on deterministic planning (even with generalization). In contrast, on domains likeBlocksworld, where GOTH causes LRTDP to visit many times fewer states than hFF , LRTDP + GOTH consistently solves theproblems much faster.Benefit of generalization. Our main hypothesis regarding GOTH has been that generalization is vital for making GOTHcomputationally feasible. To test it and measure the importance of basis functions and nogoods for GOTH’s operation, weran a version of GOTH with generalization turned off on several domains, i.e. with the classical planner being invokedfrom every state passed to GOTH for evaluation. (As an aside, note that this is akin to the strategy of FFReplan, with thefundamental difference that GOTH’s state values are eventually overridden by the decision-theoretic training process ofLRTDP. We explore the relationship between FFReplan and GOTH further in the next subsection.)As expected, GOTH without generalization proved to be vastly slower than full GOTH. For instance, on Machine ShopLRTDP + GOTH with generalization turned off is approximately 30–40 times slower (Fig. 6) by problem 10, and the gapis growing at an alarming rate, implying that without our generalization technique the speedup over hFF would not havebeen possible at all. On domains with implicit dead ends, e.g. Exploding Blocks World, the difference is even more dramatic,reaching over two orders of magnitude.Furthermore, at least on the relatively small problems on which we managed to run LRTDP + GOTH without generaliza-tion, we found the quality of policies (measured by the average plan length) yielded by generalized GOTH to be typicallybetter than with generalization off. This result is somewhat unexpected, since generalization is an additional layer of ap-proximation on top of determinizing the domain. We attribute this phenomenon to our averaging weight update strategy.As pointed out earlier, the weight of a basis function (i.e., the length of a plan, in the case of non-generalized GOTH) fromany single classical planner invocation may not be reflective of the basis function’s quality, and non-generalized GOTH willsuffer from such noise more than regular GOTH. In any event, even if GOTH without generalization yielded better policies,its slowness would make its use unjustifiable in practice.One may wonder whether generalization can also benefit hFF the way it helped GOTH. While we have not conductedexperiments to verify this, we believe the answer is no. Unlike full deterministic plan construction, finding a relaxed plansought by hFF is much easier and faster. Considering that the generalization mechanism involves iterating over many of the30A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47available basis functions to evaluate a state, any savings that may result from avoiding hFF ’s relaxed plan computation willbe negated by this iteration.Computational profile. An interesting aspect of GOTH’s modus operandi is the fraction of the computational resources anMDP solver uses that is due to GOTH. E.g., across the Machine Shop domain, LRTDP + GOTH spends 75–90% of the time inheuristic computation, whereas LRTDP + hFF only 8–17%. Thus, GOTH is computationally much heavier but causes LRTDP tospend drastically less time exploring the state space.Comparison against FFReplan. One can find similarities between the techniques used by GOTH and FFReplan. Indeed, bothemploy deterministic planners, FFReplan — for action selection directly, while GOTH — for state evaluation. One key dif-ference again lies in the fact that GOTH is not a complete planner, and lets a dedicated MDP solver correct its judgment.As a consequence, even though GOTH per se ignores probabilistic information in the domain, probabilities are (or can be)nonetheless taken into account during the solver’s search for a policy. FFReplan, on the other hand, ignores them entirely.Due to this discrepancy, performance of FFReplan and a planner guided by GOTH is typically vastly distinct. For instance,FFReplan is faster than most decision-theoretic planners. On the other hand, FFReplan has difficulty dealing with proba-bilistic subtleties. It is known to come up with very low success rate policies on probabilistically interesting problems, e.g.,on almost all problems of Triangle Tireworld’06 [35]. LRTDP + GOTH can handle such domains much better. E.g., as statedabove, it produces optimal, 100% success-rate policies on the first eight out of ten problems of the even harder version ofTriangle Tireworld that appeared at IPPC’08.3.4. SummaryGOTH is a heuristic function that provides an MDP solver with informative state value estimates using costs of plansin the deterministic version of the given MDP. Computing such plans is expensive. To amortize the time spent on theircomputation, GOTH employs basis functions, which generalize the cost of one plan to many states. As the experimentsshow, this strategy and the informativeness of state value estimates make GOTH into a more effective heuristic than thestate of the art, hFF .4. RETRASE4.1. MotivationIn GOTH, the role of information transfer via basis functions and nogoods was primarily to reuse computation in the formof classical planner invocations and thus save time. In this section, we present an MDP solver called ReTrASE, RegressingTrajectories for Approximate State Evaluation, initially described in [31] that employs basis functions in a similar way butthis time chiefly for the purpose of drastically reducing the memory footprint.Many dynamic programming-based MDP algorithms, e.g. VI and (L)RTDP, suffer from the same critical drawback — theyrepresent the state value function extensionally, i.e., as a table, thus requiring memory (and time) exponential in the numberof MDP variables. Since this extensional representation grows very rapidly, these approaches do not scale to handle real-world problems. Indeed, VI and RTDP typically exhaust memory when applied to large problems from the IPPC.Two broad approaches have been proposed for avoiding creation of a state/value table. One method consists in computingthe policy online with the help of a domain determinization, such as the all-outcomes one. In online settings, the policyneeds to be decided on-demand, only for the current state at each time step. This makes maintaining a state-value tableunnecessary (although potentially useful). Running a classical planner on a domain determinization helps choose an actionin the current state without resorting to this table. Determinization-based planners, e.g., FFHop [43], are often either slowdue to invoking a classical planner many times or, as in the case of FFReplan, disregard the probabilistic nature of actionsand have trouble with probabilistically interesting [35] domains, in which short plans have a low probability mass.The other method, dimensionality reduction, maps the MDP state space to a parameter space of lower dimension. Typically,the mapping is done by constructing a small set of basis functions, learning weights for them, and combining the weightedbasis function values into the values of states. Researchers have successfully applied dimensionality reduction by manuallydefining a domain-specific basis function set in which basis functions captured some human intuition about the domain athand. It is relatively easy to find such a mapping in domains with ordinal (e.g., numeric) state variables, especially when thenumeric features correlate strongly with the value of the state, e.g., gridworlds, “SysAdmin” and “FreeCraft” [22,23,20]. Incontrast, dimensionality reduction is difficult to use in nominal (e.g., “discrete” or “logical”) domains, such as those used inthe IPPC. Besides not having metric quantities, there is often no valid distance function between states (indeed, the distancebetween states is usually asymmetric and violates the triangle equality). It is extremely hard for a human to devise basisfunctions or a reduction mapping in nominal domains. The focus of section is an automatic procedure for doing so.To our knowledge, there has been little work on mating decision theory, determinization, and dimensionality reduction.With the ReTrASE algorithm, we are bridging the gap, proposing a fusion of these ideas that removes the drawbacks ofeach. ReTrASE learns a compact value function approximation successful in a range of nominal domains. Like GOTH, itA. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4731(cid:7)(cid:7), s)end while(cid:7) ← arg mina{ExpActCost(a, s)}declare state s ← s0declare numSteps ← 0while numSteps < L dodeclare action aModifiedBellmanBackup(as ← execute action ain snumSteps ← numSteps + 1Algorithm 2 ReTrASE.1: Input: probabilistic domain D, problem P = (cid:3)init. state s0, goal G(cid:4), trial length L, determinization routine Det, classical planner DetPlan, timeout T2: declare global map M from basis functions to weights3: declare global set DE of dead ends4: compute global determinization Dd of D5:6: // Do modified RTDP over the basis functions7: for all i = 1 : ∞ do8:9:10:11:12:13:14:15:16: end for17:18: function ExpActCost(action a, state s)19: declare array So ← successors of s under a20: declare array P o ← probs of successors of s under a21: return cost(a) +22:23: function Value(state s)24: if s ∈ DE then25:26: else if some member f27:28: else29:30:31: end if32:33: function ModifiedBellmanBackup(action a, state s)34: for all basis functions f35:36: end for(cid:7)return minbasis functions f that hold in s{M[ f ]}GetBasisFuncsForS(s)return Value(s)return a large penalty // e.g., 1 000 000M[ f ] ← ExpActCost(a, s)i P o[i]Value(So[i])in s that enable a doof M holds in s then(cid:10)does so by first obtaining a set of basis functions automatically by planning in a determinized version of the domain athand. However, being a full probabilistic planner, unlike GOTH, it also learns the weights for these basis functions by thedecision-theoretic means and aggregates them to compute state values as other dimensionality-reduction methods do. Thus,as opposed to GOTH, ReTrASE tries to incorporate the probabilistic information lost at the determinization stage back intothe solution. The set of basis functions is normally much smaller than the set of reachable states, thus giving our planner alarge reduction in memory requirements as well as in the number of parameters to be learned, while the implicit reuse ofclassical plans thanks to basis functions makes it fast.We demonstrate the practicality of ReTrASE by comparing it to the top IPPC-04, 06 and 08 performers and other state-of-the-art planners on challenging problems from these competitions. ReTrASE demonstrates orders of magnitude betterscalability than the best optimal planners, and frequently finds significantly better policies than the state-of-the-art approx-imate solvers.4.2. ReTrASE descriptionThe main intuition underlying ReTrASE is that extracting basis functions in an MDP is akin to mapping the MDP to alower-dimensional parameter space. In practice, this space is much smaller than the original state space, since only therelevant causal structure is retained,3 giving us large reduction in space requirements. Solving this new problem amountsto learning weights, a quantitative measure of each basis function’s quality. There are many imaginable ways to learn them;in this paper, we explore one such method — a modified version of RTDP.The weights reflect the fact that basis functions differ in the total expected cost of goal trajectories they enable as wellas in the total probability of these trajectories. At this point, we stress that ReTrASE makes two approximations on its wayto computing an MDP’s value function, and the first of them is related to the semantics of basis function weights andimportance. Any given basis function enables only some subset T of the goal trajectories in a given state, and is obliviousto all other trajectories in that state. The other trajectories may or may not be preferable to the ones in T (e.g., because theformer may lead the agent to the goal with 100% probability). Therefore, the importance of the trajectories (and hence ofcorresponding basis functions!) depends on the state. Our intuitive notion of weights ignores this subtlety, since the weight3 We may approximate this further by putting a bound on the number of basis functions we are willing to handle in this step.32A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47Algorithm 3 Generating basis functions.1: Input: probabilistic domain D, problem P = (cid:3)init. state s0, goal G(cid:4), determinization routine Det, classical planner DetPlan, timeout T2: declare global map M from basis functions to weights3: declare global set DE of dead ends4: compute global determinization Dd of D5:6: function GetBasisFuncsForS(state s)7: declare problem P s ← (cid:3)init. state s, goal G(cid:4)8: declare plan pl ← DetPlan(Dd, P s, T )9: if pl == none then10:insert s into DE11: else12:13:14:15:16:17:18:19:20: end ifdeclare action a ← pl[i]cost ← cost + cost(a)f ← ( f ∪ precond(a)) − effect(a)insert (cid:3) f , cost(cid:4) into M if fdeclare basis function f ← goal Gdeclare cost ← 0for all i = length(pl) through 1 dois not in M yetend forof a basis function does not vary with states in which this basis function holds. Thus, a weight is in effect the reflection ofan “average” importance of a basis function across the states it represents.The above details notwithstanding, the differences among basis function weights exist partly because each trajectoryconsiders only one outcome for each of its actions. The sequence of outcomes the given trajectory considers may be quiteunlikely. In fact, getting some action outcomes that the trajectory does not consider may prevent the agent from ever gettingto the goal. Thus, it may be much “easier” to reach the goal in the presence of some basis functions than others.Now, given that each state is generally represented by several basis functions, what is the connection between the state’svalue and their weights? In general, the relationship is quite complex: under the optimal policy, trajectories enabled byseveral basis functions may be possible, causing some trajectories to factor into weights of several basis functions simul-taneously. However, determining the subset of basis functions enabling these trajectories is at least as hard as solving theMDP exactly. Instead, we approximate the state value by the minimum weight of all basis function that represent the state.This amounts to saying that the “better” a state’s “best” basis function is, the “better” is the state itself, and is the secondapproximation ReTrASE makes.Thus, deriving useful basis functions and their weights gives us an approximation to the optimal value function.Algorithm’s operation. For a step-by-step example of operation of ReTrASE, whose pseudo code is presented in Algorithm 2,please refer to the proof of Theorem 1. ReTrASE starts by computing the determinization Dd of the domain. As in GOTH,we use Dd to rapidly compute the basis functions. The algorithm explores the state space by running modified RTDPtrials, memoizing all the dead ends and basis functions it learns along the way. Whenever during state evaluation (line 21)ReTrASE finds a state that is neither a known dead-end nor has any basis function that holds in it, ReTrASE uses theregression procedure GetBasisFuncsForS(.) presented in Algorithm 3 to generate a basis function for it. Regression yieldsnot only the basis functions but also an approximate cost of reaching the goal in Dd from any state with the given basisfunction via the given plan. We use this value to initialize the corresponding basis function’s weight. As in GOTH, if thedeterministic planner can prove the non-existence of a plan or simply cannot find a plan before some timeout, the state inquestion is deemed to be a dead end (line 10 of Algorithm 3).For each state s visited by the modified RTDP, the ModifiedBellmanBackup(.) routine updates the weight of each basis(line 33). The new weight of each such basis functioncanshould be reflected in these basis functions’cannot be executed wherever basis functions that do not enable it hold, so the expected costfunction that enables the execution of the currently optimal action abecomes the expected cost of action abe executed in any state where basis functions hold; hence, the quality of aweights. Analogously, other aof those actions is irrelevant to determining their weights.. The intuitive reason for updating the basis functions enabling ais that a(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)Theoretical properties. A natural question about ReTrASE is that of convergence. To answer it, we proved the followingnegative result:Theorem 1. There are problems on which ReTrASE may not converge.Proof. By failing to converge we mean that, on some problems, depending on the order in which basis functions arediscovered ReTrASE may indefinitely oscillate over a set of several policies with different expected costs. One such MDP Mis presented in Fig. 7, which shows M’s transition graph and action set. Solving M amounts to finding a policy of minimumexpected cost that takes the agent from state s0 to state g and uses actions a1 — a5. The optimal solution to M is a linearplan s0 − a1 − s1 − a4 − s4 − a5 − g.A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4733Fig. 7. An example MDP on which ReTrASE fails to converge.To see that ReTrASE fails to converge on M, we simulate ReTrASE’s operation on this MDP. Recall that ReTrASE executesa series of trials, all originating at s0. (cid:2)Trial 1. To choose an action in s0, ReTrASE needs to evaluate states s1 and s2. It does not yet have any basis functions todo that, so it uses the procedure in Algorithm 3 to generate them, together with initial estimates for their weights.Suppose the procedure first looks for a basis function for s1 and finds plan s1 − a4 − s4 − a5 − g. Regressing it yieldsthe following basis function-weight pairs: W ( A ∧ B ∧ C ∧ D) = 0, W ( A ∧ B ∧ C) = 1, W ( A ∧ B) = 2. A ∧ B is the only basisfunctions that holds in s1 so far. Therefore, the current estimate for the value of s1, V (s1), is 2. Accordingly, the currentestimate for the value of action a1 in s0, Q-value(s0, a1), becomes C(a1) + V (s1) = 4.Next, suppose that for state s2, GetBasisFuncsForS finds plan s2 − a3 − g. Regressing it yields one basis function-weightpair in addition to the already discovered ones, W ( A ∧ D) = 1. Function A ∧ D is the only one that holds in s2, so we getV (s2) = 1 and Q-value(s0, a2) = 2.Now ReTrASE can choose an action in s0. Since at the moment Q-value(s0, a1) > Q-value(s0, a2), it picks a2 and executesit, transitioning to s2.In s2, ReTrASE again needs to evaluate two actions, a3 and a4. Notice that a4 leads to s5, which is a dead end.GetBasisFuncsForS discovers this fact by failing to produce any basis functions. Thus, V (s5) is a very large dead-endpenalty, e.g. 1 000 000, yielding Q-value(s2, a4) = 1 000 001. However, a3 may also lead to dead end s3 with P = 0.5, soQ-value(s2, a3) = 500 001. Nonetheless, a3 is more preferable, so this is the action that ReTrASE picks in s2.At this time, ReTrASE performs a modified Bellman backup in s2. The only known basis function that holds in s2 andenables the chosen action a3 is A ∧ D. Therefore, ReTrASE sets W ( A ∧ D) = Q-value(s2, a3) = 500 001.Executing a3 in s2 completes the trial with a transition either to goal g or to dead end s3.Trial 2. This time, ReTrASE can select an action in s0 without resorting to regression. Currently, V (s1) = 2, since A ∧ B withW ( A ∧ B) = 2 is the minimum-weight basis function in s1. However, V (s2) = 500 001 due to the backup performed duringtrial 1. Therefore, Q-value(s0, a1) = 4 but Q-value(s0, a2) = 500 002, making a1 look more attractive. So, ReTrASE chooses a1,causing a transition to s1.In s1, the choice is between a3 and a4. The values of both are easily calculated with known basis functions, Q-value(s1, a3) = 500 001 and Q-value(s1, a4) = 2.The natural choice is a4, and ReTrASE performs the corresponding backup. The basis functions enabling a4 in s1 areA ∧ B and A ∧ D. Their weights become Q-value(s1, a4) = 2 after the update.The rest of the trial does not change any weights and is irrelevant to the proof.Trial n. Crucially, basis function A ∧ D, whose weight changed in the previous trials, holds both in state s1 and in state s2.Due to the update in s2 during trial 1, W ( A ∧ D) became large and made s1 look beneficial. On the other hand, thanksto the update in s1 during trial 2, W ( A ∧ D) became small and made s2 look beneficial. It is easy to see that this cyclewill continue in subsequent trials. As a result, ReTrASE will keep on switching between two policies, one of which issuboptimal. (cid:2)Overall, the classes of problems on which ReTrASE may diverge are hard to characterize generally. Predicting whetherReTrASE may diverge on a particular problem is an area for future work. We maintain, however, that a lack of theoretical34A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47Fig. 8. Memory usage on logarithmic scale: ReTrASE is dramatically more efficient than both LRTDP + OPT and LRTDP + hFF .guarantees is not indicative of a planner’s practical performance. Indeed, several IPPC winners, including FFReplan, havea weak theoretical profile. The experimental results show that ReTrASE too performs very well on many of the planningcommunity’s benchmark problems.4.3. Experimental resultsOur goal in this subsection is to demonstrate two important properties of ReTrASE — (1) scalability and (2) quality ofsolutions in complex, probabilistically interesting domains. We start by showing that ReTrASE easily scales to problemson which the state-of-the-art optimal and non-determinization-based approximate planners run out of memory. Then, weillustrate ReTrASE’s ability to compute better policies for hard problems than state-of-the-art approximate planners.Implementation details. ReTrASE is implemented in C++ and uses miniGPT [8] as the base RTDP code. Our implementationis still in the prototype stage and does not yet fully support some of the PPDDL language features used to describe IPPCproblems (e.g. universal quantification, disjunctive goals, rewards, etc.).Experiment setup. We report results on six problem sets — Triangle Tire World (TTW) from IPPC-06 and -08, Drive fromIPPC-06, Exploding Blocks World (EBW) from IPPC-06 and -08, and Elevators from IPPC-06. In addition, we ran ReTrASE on afew problems from IPPC-04. Since our implementation does not yet support such PPDDL features as universal quantification,we were unable to test on the remaining domains from these competitions. However, we emphasize that most of the sixdomains we evaluate on are probabilistically interesting and hard. Even the performance of the best IPPC participants onmost of them leaves a lot of room for improvement, which attests to their informativeness as testbeds for our planner.To provide a basis for comparison, for the above domains we also present the results of the best IPPC participants.Namely, we give the results of the IPPC winner on that domain, of the overall winner of that IPPC, and ours. For thememory consumption experiment, we run two VI-family planners, LRTDP with inadmissible hFF (LRTDP + hFF ), and LRTDP+ OPT — LRTDP with Atom-Min-1-Forward|Min-Min heuristic [8]. Both are among the best-known and top-performingplanners of their type.We ran ReTrASE on the test problems under the restrictions resembling those of IPPC. Namely, for each problem, ReTrASEhad a maximum of 40 minutes for training, as did all the planners whose results we present here. ReTrASE then had 30attempts to solve each problem. In IPPC, the winner is decided by the success rate — the percentage of 30 trials in whicha particular planner managed to solve the given problem. Accordingly, on the relevant graphs we present both ReTrASE’ssuccess rate and that of its competitors.While analyzing the results, it is important to be aware that our ReTrASE implementation is not optimized. Consequently,ReTrASE’s efficiency is likely even better than indicated by the experiments.Comparing scalability. We begin by showcasing the memory savings of ReTrASE over LRTDP + OPT and LRTDP + hFF onthe Triangle Tire World domain. Fig. 8 demonstrates the savings of ReTrASE to increase dramatically with problem size. Infact, neither LRTDP variant is able to solve past problem 8 as both run out of memory, whereas ReTrASE copes with all tenproblems. Scalability comparisons for other domains we tested on yield generally similar results.Other popular approximate algorithms (aside from LRTDP + hFF ) do not suffer from the scalability issues as much asLRTDP. Thus, it is more meaningful to compare ReTrASE against them on the quality of solutions produced. As we show,ReTrASE’s scalability allows it to successfully compete on IPPC problems with any participant.Comparing solution quality: Success rate. Continuing with the Triangle Tire World domain, we compare the success ratesof ReTrASE, RFF[41] — the overall winner of IPPC-08, and HMDPP [29] — the winner on this particular domain. Notethat Triangle Tire World, perhaps, the most famous probabilistically interesting domain, was designed largely to confoundA. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4735Fig. 9. ReTrASE achieves perfect success rate on Triangle Tire World-08.Fig. 10. ReTrASE is at par with the competitors on Drive.Fig. 11. ReTrASE dominates on Exploding Blocks World-06.solvers that rely on domain determinization [35], e.g., FFReplan; therefore, performance on it is particularly important forevaluating a new planner. Indeed, as Fig. 9 shows, on this domain ReTrASE ties with HMDPP by achieving the maximumpossible success rate, 100%, on all ten problems and outperforms the competition winner, which cannot solve problem 10at all and achieves only 83%-success rate on problem 9.On the IPPC-06 Drive domain, ReTrASE also fares well (Fig. 10). Its average success rate is just ahead of the unofficialdomain winner (FFReplan) and of the IPPC-06 winner (FPG), but the differences among all three are insignificant.For the Exploding Blocks World domain on the IPPC-06 version (Fig. 11), ReTrASE dominates every other planner by awide margin on almost every problem. Its edge is especially noticeable on the hardest problems, 11 through 15. On themost recent EBW problem set, from IPPC-08 (Fig. 12), ReTrASE performs very well too. Even though its advantage is not asapparent as in IPPC-06, it is nonetheless ahead of its competition in terms of the average success rate.The Elevators and Triangle Tire World-06 domains are easier than the ones presented above. Surprisingly, on many of theElevators problems ReTrASE did not converge within the allocated 40 minutes and was outperformed by several planners.We suspect this is due to bad luck ReTrASE has with basis functions in this domain. However, on TTW-06 ReTrASE was thewinner on every problem.Comparing solution quality: Expected cost. On problems where ReTrASE achieves the maximum success rate it is interest-ing to ask how close the expected trajectory cost that its policy yields is to the optimal. The only way we could find outthe expected cost of an optimal policy for a problem is by running an optimal planner on it. Unfortunately, the optimal36A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47Fig. 12. ReTrASE outmatches all competitors on Exploding Blocks World-08, although by a narrow margin.Table 2Success rates on some IPPC-04 problems.Problem nameexploding-blockg-tire-problemFFHop93.33%60%ReTrASE100%70%planner we used, LRTDP + OPT, scales enough to solve only relatively small problems (at most a few million states). Onsuch problems we found ReTrASE to produce trajectories of expected cost within 5% of the optimal.Comparison to FFHop. FFReplan has been a very powerful planner and a winner of at least one IPPC. However, recentbenchmarks defeat it by exploiting its near-complete disregard for probabilities when computing a policy. Researchers haveproposed a powerful improvement to FFReplan, FFHop [43], and demonstrated its capabilities on problems from IPPC-04.Unfortunately, due to the current lack of support for some PPDDL language features we were not able to run ReTrASEon most IPPC-04 domains. Table 2 compares the success rates of the two planners on the IPPC-04 problems we did test.Even though ReTrASE performs better on these problems, the small size of the experimental base makes the comparison ofReTrASE and FFHop inconclusive.While we do not test on all IPPC domains our current experimental evaluation clearly demonstrate ReTrASE’s scalabilityimprovements over the VI-family planners and its at-par or better performance on many competition problems comparedto state-of-the-art systems.4.4. SummaryReTrASE is an MDP solver based on a combination of state abstraction and dimensionality reduction. It automaticallyextracts basis functions, which provide a compact representation of the given MDP while retaining its causal structure.Simultaneously with discovering basis functions, it learns weights for the already discovered ones using modified Bellmanbackups. These weights let ReTrASE evaluate states without memoizing state values explicitly. Such an approach allowsReTrASE to solve larger problems than the best performers of several recent IPPCs.5. SIXTHSENSE5.1. MotivationAlthough basis functions efficiently generalize information about states from which reaching the goal is possible, theyhave nothing to say about dead ends. As a result, algorithms that use only basis functions for information transfer cannotavoid either caching dead ends or rediscovering them every time they run into them. In fact, the issue of quickly andreliably recognizing dead ends plagues virtually all modern MDP solvers. For instance, in IPPC-2008 [9], the domains witha complex dead-end structure, e.g., Exploding Blocks World, have proven to be the most challenging. Surprisingly, however,there has been little research on methods for effective discovery and avoidance of dead ends in MDPs. Of the two types ofdead ends, implicit ones confound planners the most, since they do have executable actions. However, explicit dead endscan be a resource drain as well, since verifying that none of the available actions are applicable in a state can be costly ifthe number of actions is large.Broadly speaking, existing planners use one of two approaches for identifying dead ends. When faced with a yet-unvisitedstate, many planners (e.g., LRTDP) apply a heuristic value function (such as hFF ), which hopefully assigns a high cost to dead-end states. This method is fast to invoke but often fails to catch many implicit dead ends due to the problem relaxationinevitably used by the heuristics. Failure to detect them causes the planner to waste much time in exploring the statesreachable from implicit dead ends, these states being dead ends themselves. Other MDP solvers use state value estimationA. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4737approaches that recognize dead ends reliably but are very expensive; for example, RFF, HMDPP, and ReTrASE employ fulldeterministic planners. When a problem contains many dead ends, these MDP solvers may spend a lot of their time launch-ing classical planners from dead ends. Indeed, most probabilistic planners would run faster if recognizing dead ends wasnot so computationally expensive.In this section, we complete our abstraction framework by presenting a novel mechanism, SixthSense, to do exactly this— quickly and reliably identify dead-end states in MDPs. Underlying SixthSense, pioneered in [33], is a key insight: largesets of dead-end states can usually be characterized by a compact logical conjunction, a nogood, which “explains” why nosolution exists. For example, a Mars rover that flipped upside down will be unable to achieve its goal, regardless of itslocation, the orientation of its wheels, etc. Knowing this explanation lets a planner quickly recognize millions of states asdead ends. Crucially, dead ends in most MDPs can be described with a small number of nogoods.SixthSense learns nogoods by generating candidates with a bottom-up greedy search (resembling that used in rule in-duction [12]) and tests them to avoid false positives with a planning graph-based procedure. A vital input to this learningalgorithm are basis functions, derived as shown in the previous sections. SixthSense is provably sound — every nogoodoutput represents a set of true dead ends. We empirically demonstrate that SixthSense speeds up two different types ofMDP solvers on several IPPC domains with implicit dead ends and show the performance improvements SixthSense gives toGOTH and ReTrASE. Overall, SixthSense tends to identify most of the dead ends that the solvers encounter, reducing mem-ory consumption by as much as 90%. Because SixthSense runs quickly, it also gives a 30–50% speedup on large problems.With these savings, it enables planners to solve problems they could not previously handle.5.2. SixthSense descriptionAn MDP may have an exponential number of dead end states, but often there are just a few “explanations” for why astate has no goal trajectory. A Mars rover flipped upside down is in a dead-end state, irrespective of the values of the othervariables. In the Drive domain of IPPC-06, all states with the (not (alive)) literal are dead ends. Knowing these explanationsobviates the dead-end analysis of each state individually and the need to store the explained dead ends in order to identifythem later.Our method, SixthSense, strives to induce explanations as above in the factored MDP setting and use them to help theplanner recognize dead ends quickly and reliably. Formally, its objective is to find nogoods, conjunctions of literals with theproperty that all states in which such a conjunction holds are dead ends. After at least one nogood is discovered, wheneverthe planner encounters a new state, SixthSense notifies the planner if the state is represented by a known nogood andhence is a dead end.To discover nogoods, we devise a machine learning generate-and-test algorithm that is an integral part of SixthSense.The “generate” step proposes a candidate conjunction, using some of the dead ends the planner has found so far as trainingdata. For the testing stage, we develop a novel planning graph-based algorithm that tries to prove that the candidate isindeed a nogood. Nogood discovery happens in several attempts called generalization rounds. First we outline the generate-and-test procedure for a single round in more detail and then describe the scheduler that decides when a generalizationround is to be invoked. Algorithm 4 contains the learning algorithm’s pseudocode.Generation of candidate nogoods. There are many ways to generate a candidate but if, as we conjecture, the number of ex-planations/nogoods in a given problem is indeed very small, naive hypotheses, e.g., conjunctions of literals picked uniformlyat random, are very unlikely to pass the test stage. Instead, our procedure makes an “educated guess” by employing basisfunctions according to one crucial observation. Recall that, by definition, basis functions are preconditions for goal trajecto-ries. Therefore, no state represented by them can be a dead end. On the other hand, any state represented by a nogood, bythe nogoods’ definition, must be a dead end. These facts combine into the following observation: a state may be generalizedby a basis function or by a nogood but not both.Of more practical importance to us is the corollary that any conjunction that has no conflicting pairs of literals (a literaland its negation) and contains the negation of at least one literal in every basis function (i.e., defeats every basis function)is a nogood. This fact provides a guiding principle — form a candidate by going through each basis function in the problemand, if the candidate does not defeat it, picking the negation of one of the basis function’s literals. By the end of the run,the candidate provably defeats all basis functions in the problem. The idea has a big drawback though: finding all basisfunctions in the problem is prohibitively expensive. Fortunately, it turns out that making sure the candidate defeats onlya few randomly selected basis functions (100–200 for the largest problems we encountered) is enough in practice for thecandidate to be a nogood with reasonably high probability (although not for certain, motivating the need for verification).Therefore, before invoking the learning algorithm for the first time, our implementation acquires 100 basis functions byrunning the classical planner FF. Candidate generation is described on lines 5–11.So far, we have not specified how exactly defeating literals should be chosen. Here as well we can do better than naiveuniform sampling. Intuitively, the frequency of a literal’s occurrence in the dead ends that the MDP solver has encounteredso far correlates with the likelihood of the literal’s presence in nogoods. The algorithm’s sampleDefeatingLiteral subroutinesamples a literal defeating basis function b with a probability proportionate to the literal’s frequency in the dead endsrepresented by the constructed portion of the nogood candidate. The method’s strengths are twofold: not only does it take38A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47end ifreturn failurec = c \ {L}end ifend forif c does not defeat b thenfor all literals L ∈ c doif checkWithPlanningGraph(setL, c \ {L}, g) == success thendeclare literal L = sampleDefeatingLiteral(setDEs, b, c)c = c ∪ {L}Algorithm 4 SixthSense.1: Input: training set of known non-generalized dead ends setDEs, set of basis functions setBFs, set of nogoods setNG, goal g, set of all domain literals setL2:3: function learnNogood(setDEs, setBFs, setNGs, g)4: // construct a candidate5: declare candidate conjunction c = {}6: for all b ∈ setBFs do7:8:9:10:11: end for12: // check candidate with planning graph, and prune it13: if checkWithPlanningGraph(setL, c, g) then14:15:16:17:18:19: else20:21: end if22: // if we got here then the candidate is a valid nogood23: empty setDEs24: add c to setNG25: return success26:27: function checkWithPlanningGraph(setL, c, g)28: for all literals G in (g \ c) dodeclare conjunction c29:30:if PlanningGraph(c’) == success then31:32:33: end for34: return success35:36: function sampleDefeatingLiteral(setDEs, b, c)37: declare counters C¬L for all L ∈ b \ c38: for all d ∈ setDEs do39:40:41:42:43:44: end for45: return a literal L(cid:7) = c ∪ ((setL \ (¬c)) \ {G})for all L ∈ b s.t. ¬L ∈ d dosampled according to P (Lif c generalizes d thenreturn failureC¬L + +end forend ifend if(cid:7)(cid:7)) ∼ C L(cid:7)into account information from the solver’s experience but also lets literals’ co-occurrence patterns direct creation of thecandidate.Nogood verification. If in the above candidate generation procedure we used the set of all basis functions that exist fora given MDP, verifying the resulting candidate would not be necessary. The set of states represented by at least one ba-sis function from this exhaustive set would itself be the exhaustive set of non-dead-end states. Therefore, any generatedcandidate would only represent dead-end states, and thus would be a true nogood.However, in general we do not have all possible basis functions at our disposal. Consequently, we need to verify thatthe candidate created by the algorithm from the available basis functions is indeed a nogood. Let us denote the problem ofestablishing whether a given conjunction is a nogood as NOGOOD-DECISION.Theorem 2. NOGOOD-DECISION is PSPACE-complete.Proof. First, we show that NOGOOD-DECISION ∈ PSPACE. To verify that a conjunction is a nogood, we can verify that eachstate this conjunction represents is a dead end. For each state, such verification is equivalent to establishing plan existencein the all-outcomes determinization of the MDP. This problem is PSPACE-complete [11], i.e., is in PSPACE. Thus, nogoodverification can be broken down into a set of problems in PSPACE and is in PSPACE itself.To complete the proof, we point out that the already mentioned problem of establishing deterministic plan existence isan instance of NOGOOD-DECISION, providing a trivial reduction to NOGOOD-DECISION from a PSPACE-complete problem. (cid:2)A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4739In the light of Theorem 2, we may realistically expect an efficient algorithm for NOGOOD-DECISION to be either sound orcomplete, but not both. A sound algorithm would never conclude that a candidate is a nogood when it is not. A completeone would pronounce a candidate to be a nogood whenever the candidate is in fact a nogood. A key contribution of thispaper is a sound algorithm for identifying nogoods. It is based on the observation that all the per-state checks in the naivescheme above can be replaced by only a few whose running time is polynomial in the problem size. Although sound, theoperation is incomplete, i.e. may reject some candidates that are in fact nogoods. Nonetheless, this check is effective atidentifying nogoods in practice.To verify a candidate c efficiently, we group all non-goal states represented by c into several superstates of c. We definea superstate of a candidate c to be a set consisting of c’s literals, of the negation of one of the goal literals that are notpresent in c, and of all literals over all other variables in the domain. As an example, suppose the complete set of literals inour problem is { A, ¬ A, B, ¬B, C, ¬C, D, ¬D, E, ¬E}, the goal is A ∧ ¬B ∧ E, and the candidate is A ∧ C . Then the superstatesour algorithm constructs for this candidate are { A, B, C, D, ¬D, E, ¬E} and { A, B, ¬B, C, D, ¬D, ¬E} (the negation of a goalliteral in each superstate is highlighted in bold).The intuition behind this definition of superstates of c is as follows. Every non-goal state s represented by c is “contained”in one of superstates of c in the sense that there is a superstate of c containing all of s’s literals. Moreover, if a superstatehas no trajectory to the goal, no such trajectory exists for any state contained in the superstate, i.e. these states are all deadends. Combining these two observations, if no goal trajectory exists from any superstate of c then all the states representedby the candidate are dead ends. By definition, such a candidate is a nogood.Accordingly, to find out whether the candidate is a nogood, our procedure runs the planning graph algorithm on eachof the candidate’s superstates using determinized actions. Each instance returns success if and only if it can reach the goalliterals and resolve all mutexes between them. The initial set of mutexes it feeds to the planning graph are just the mutexesbetween each literal and its negation.Theorem 3. The candidate conjunction is a nogood if each of the planning graph expansions on the superstates either (a) fails to achieveall of the goal literals or (b) fails to resolve mutexes among any two of the goal literals.Proof. Since the planning graph is sound, failure on all superstate expansions indicates the candidate is a true nogood (lines27–34). (cid:2)Our procedure is incomplete for two reasons. First, since each superstate has more literals than any single state itcontains, it may have a goal trajectory that is impossible to execute from any state. Second, the planning graph algorithm isincomplete by itself; it may declare plan existence when no plan actually exists.At the cost of incompleteness, our algorithm is only polynomial in the problem size. To see this, note that each plan-ning graph expansion from a superstate is polynomial in the number of domain literals, and the number of superstates ispolynomial in the number of goal literals.If the verification test is passed, we try to prune away unnecessary literals (lines 13–18) that may have been includedinto the candidate during sampling. This analog of Occam’s razor strives to reduce the candidate to a minimal nogood andoften gives us a much more general conjunction than the original one at little extra verification cost. At the conclusion ofthe pruning stage, compression empties the set of dead ends that served as the training data so that the MDP solver can fillit with new ones. The motivation for this step will become clear once we discuss scheduling of compression invocations.Scheduling. Since we do not know a priori the number of nogoods in the problem, we need to perform several generaliza-tion rounds. Optimally deciding when to do that is hard, if not impossible, but we have designed an adaptive schedulingmechanism that works well in practice. It tries to estimate the size of the training set likely sufficient for learning an ex-tra nogood, and invokes learning when that much data has been accumulated. When generalization rounds start failing,the scheduler calls them exponentially less frequently. Thus, very little computation time is wasted after all nogoods thatcould reasonably be discovered have been discovered. (There are certain kinds of nogoods whose discovery by SixthSense,although possible, is highly improbable. We elaborate on this point in the Discussion section.)Our algorithm is inspired by the following tradeoff. The sooner a successful round happens, the earlier SixthSense canstart using the resulting nogood, saving time and memory. On the other hand, trying too soon, with hardly any trainingdata available, is improbable to succeed. The exact balance is difficult to locate even approximately, but our empirical trialsindicate three helpful trends: (1) The learning algorithm is capable of operating successfully with surprisingly little trainingdata, as few as 10 dead ends. The number of basis functions does not play a big role provided there is more than about100 of them. (2) If a round fails with statistics collected from a given number of dead ends, their number usually needs tobe increased drastically. However, because learning is probabilistic, such a failure could also be accidental, so it is justifiableto return to the “bad” training data size occasionally. (3) A typical successful generalization round saves the planner enoughtime and memory to compensate for many failed ones. These three regularities suggest the following algorithm.• Initially, the scheduler waits for a small batch of basis functions, setBFs in Algorithm 4, and a small number of deadends, setDEs, to be accumulated before invoking the first generalization round. For reasons above, in our implementationused the initial settings of |setBFs| = 100 and |setDEs| = 10 for all problems.40A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47• After the first round and including it, whenever a round succeeds the scheduler waits for a number of dead ends unrec-ognized by known nogoods equal to half of the previous batch size to arrive before invoking the next round. Decreasingthe batch size is usually worth the risk according to observations (2) and (3) and because the round before succeeded.If a round fails, the scheduler waits for the accumulation of twice the previous number of unrecognized dead endsbefore trying generalization again.Perhaps unexpectedly, in many cases we have seen very large training sets decrease the probability of learning a no-good. This phenomenon can be explained by training sets of large sizes sometimes containing subcollections of dead endscaused by different nogoods. Consequently, the literal occurrence statistics induced by such a mix make it hard to generatereasonable candidates. This finding has led us to restrict the training batch size (setDEs in Algorithm 4) to 10 000. If, due toexponential backoff, the scheduler is forced to wait for the arrival of n > 10 000 dead ends, it skips the first (n − 10 000)and retains only the latest 10 000 for training. For the same locality considerations, each training set is emptied at the endof each round (line 23).Theoretical properties. Before presenting the experimental results, we analyze SixthSense’s properties. The most importantone is that the procedure of identifying dead ends as states in which at least one nogood holds is sound. It follows directlyfrom the nogood’s definition.Importantly, SixthSense puts no bounds on the nogood length, being theoretically capable of discovering any nogood.One may ask: are there any nontrivial bounds on the amount of training data for SixthSense to generate a nogood of agiven length with at least a given probability? As the following argument indicates, even if such bounds exist they are likelyto be of no use in practice. For SixthSense to generate any given nogood, the training data must contain many dead endscaused by this nogood. However, depending on the structure of the problem, most such dead ends may be unreachable fromthe initial state. If the planning algorithm that uses SixthSense never explores those parts of the state space (e.g., LRTDP),no amount of practically collectable training data will help SixthSense discover some of the nogoods with high probability.At the same time, we can prove another important property of SixthSense:Theorem 4. Once a nogood has been discovered and memoized by SixthSense, SixthSense will never discover it again.Proof. This fact is a consequence of using only dead ends that are not recognized by known nogoods to construct thetraining sets, as described in the Scheduling subsection, and erasing the training data after each generalization attempt.According to Algorithm 4, each nogood candidate is built up iteratively by sampling literals from a distribution induced bytraining dead ends that are represented by the constructed portion of the candidate. Also, we know that no training deadend is represented by any known nogood. Therefore, the probability of sampling a known nogood (lines 5–11) is 0. (cid:2)Regarding SixthSense’s speed, the number of frequently encountered nogoods in any given problem is rather small,which makes identifying dead ends by iterating over the nogoods a very quick procedure. Moreover, a generalization roundis polynomial in the training data size, and the training data size is linear in the size of the problem (due to the length ofthe dead ends and basis functions). We point out, however, that obtaining the training data theoretically takes exponentialtime. Nevertheless, since training dead ends are identified as a part of the usual planning procedure in most MDP solvers,the only extra work to be done for SixthSense is obtaining a few basis functions. Their required number is so small thatin nearly every probabilistic problem, they can be quickly obtained by invoking a speedy deterministic planner from severalstates. This explains why in practice SixthSense is very fast.Last but not least, we believe that SixthSense can be incorporated into nearly any existing trial-based factored MDPsolver, since, as explained above, the training data SixthSense requires is either available in these solvers and can becheaply extracted, or can be obtained independently of the solver’s operation by invoking a deterministic planner.5.3. Experimental resultsOur goal in the experiments was to explore the benefits SixthSense brings to different types of planners, as well as togauge the effectiveness of nogoods and the amount of computational resources taken to generate them. We used three IPPCdomains as benchmarks: Exploding Blocks World-08 (EBW-08), Exploding Blocks World-06 (EBW-06), and Drive-06. IPPC-06and -08 contained several more domains with dead ends, but their structure is similar to that of the domains we chose. Inall experiments, we restricted each MDP solver to use no more than 2 GB of memory.Structure of dead ends in IPPC domains. Among the IPPC benchmarks, we found domains with only two types of implicitdead ends. In the Drive domain, which exemplifies the first of them, the agent’s goal is to stay alive and reach a destinationby driving through a road network with traffic lights. The agent may die trying but, because of the domain formulation, thisdoes not necessarily prevent the car from driving and reaching the destination. Thus, all of the implicit dead ends in thedomain are generalized by the singleton conjunction (not alive). A few other IPPC domains, e.g., Schedule, resemble Drive inhaving one or several exclusively single-literal nogoods representing all the dead ends. Such nogoods are typically easy forSixthSense to derive.A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4741Fig. 13. Time and memory savings due to nogoods for LRTDP + hFF (representing the “Fast but Insensitive” type of planners) on 3 domains, as a percentageof resources needed to solve these problems without SixthSense (higher curves indicate bigger savings; points below zero require more resources withSixthSense). The reduction on large problems can reach over 90% and even enable more problems to be solved (their data points are marked with a ×).Fig. 14. Resource savings from SixthSense for LRTDP + GOTH/NO 6S (representing the “Sensitive but Slow” type of planners).EBW-06 and -08’s dead ends are much more complex, and their structure is unique among the IPPC domains. In theEBW domain, the objective is to rearrange a number of blocks from one configuration to another, and each block mightexplode in the process. For each goal literal, EBW has two multiple-literal nogoods explaining when this literal cannot beachieved. For example, if block b4 needs to be on block b8 in the goal configuration then any state in which b4 or b8explodes before being picked up by the manipulator is a dead end, represented either by nogood (not (no-destroyed b4)) ∧(not (holding b4)) ∧ (not (on b4 b8)) or by (not (no-destroyed b8)) ∧ (not (on b4 b8)). We call such nogoods immediateand point out that EBW also has other types of nogoods, described in the Discussion section. The variety and structuralcomplexity of EBW nogoods makes them challenging to learn.Planners. As pointed out earlier, MDP solvers can be divided into two groups according to the way they handle dead ends.Some of them identify dead ends using fast but unreliable means like heuristics, which miss a lot of dead ends, causingthe planner to waste time and memory exploring useless parts of the state space. We will call such planners “fast butinsensitive” with respect to dead ends. Most others use more accurate but also more expensive dead-end identificationmeans. We term these planners “sensitive but slow” in their treatment of dead ends. The monikers for both types applyonly to the way these solvers handle dead ends and not to their overall performance. With this in mind, we demonstratethe effects SixthSense has on each type.Benefits to fast but insensitive. This group of planners is represented in our experiments by LRTDP with the hFF heuristic.We will call this combination LRTDP + hFF , and LRTDP + hFF equipped with SixthSense — LRTDP + hFF + 6S for short.Implementationwise, SixthSense is incorporated into hFF . When evaluating a newly encountered state, hFF first consults theavailable nogoods produced by SixthSense. Only when the state fails to match any nogood does hFF resort to its traditionalmeans of estimating the state value. Without SixthSense, hFF misses many dead ends, since it ignores actions’ delete effects.Fig. 13 shows the time and memory savings due to SixthSense across three domains as the percentage of the resourcesLRTDP + hFF took to solve the corresponding problems (the higher the curves are, the bigger the savings). No data pointsfor some problems indicate that neither LRTDP + hFF nor LRTDP + hFF + 6S could solve them with only 2 GB of RAM.There are a few large problems that could only be solved by LRTDP + hFF + 6S. Their data points are marked with a × andsavings for them are set at 100% (e.g., on problem 14 of EBW-06) as a matter of visualization, because we do not know howmuch resources LRTDP + hFF would need to solve them. Additionally, we point out that as a general trend, problems grow42A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47Fig. 15. SixthSense speeds up ReTrASE by as much as 60% on problems with dead ends. The plot shows this trend on the example of problem 12 ofEBW-06.in complexity within each domain with the increasing ordinal. However, the increase in difficulty is not guaranteed for anytwo adjacent problems, especially in domains with a rich structure, causing the jaggedness of graphs for EBW-06 and -08.As the graphs demonstrate, the memory savings on average grow very gradually but can reach a staggering 90% on thelargest problems. In fact, on the problems marked with a ×, they enable LRTDP + hFF + 6S to do what LRTDP + hFF cannot.The crucial qualitative distinction of LRTDP + hFF + 6S from LRTDP + hFF explaining this is that since nogoods help theformer recognize more states as dead ends it does not explore (and hence memoize) their descendants. Notably, the timesavings are lagging for the smallest and some medium-sized problems (approximately 1–7). However, each of them takesonly a few seconds to solve, so the overhead of SixthSense may be slightly noticeable. On large problems, SixthSense fullycomes into its element and saves 30% or more of the planning time.Benefits to sensitive but slow. Planners of this type include top IPPC performers RFF and HMDPP, as well as ReTrASE andothers. Most of them use a deterministic planner, e.g., FF, on a domain determinization to find a plan from the given stateto the goal and use such plans in various ways to construct a policy. Whenever the deterministic planner can prove non-existence of a path to the goal or fails to simply find one within a certain time, these MDP solvers consider the state fromwhich the planner was launched to be a dead end. Due to the properties of classical planners, this method of dead-endidentification is reliable but expensive. To model it, we employed LRTDP with the GOTH heuristic. GOTH evaluates stateswith classical planners, so including or excluding SixthSense from GOTH allows for simulating the effects SixthSense has onthe above algorithms. As SixthSense is part of the standard GOTH implementation, GOTH without it is denoted as GOTH/NO6S. Fig. 14 illustrates LRTDP + GOTH’s behavior. Qualitatively, the results look similar to LRTDP + hFF + 6S but there is asubtle critical difference — the time savings in the latter case grow faster. This is a manifestation of the fundamentaldistinction of SixthSense in the two settings. For the “Sensitive but Slow”, SixthSense helps recognize implicit dead endsfaster (and obviates memoizing them). For the “Fast but Insensitive”, it also obviates exploring many of the implicit deadends’ descendants, causing a faster savings growth with problem size.Benefits to ReTrASE. ReTrASE is perhaps the most natural of MDP solver to be augmented with SixthSense. It already usesbasis functions to store information about non-dead-end states, and utilizing nogoods would allow it to capitalize on theabstraction framework even more, providing additional insights into the benefits for other planners that might employ theabstraction framework to serve all of their state space representation needs.To measure the effect of SixthSense on ReTrASE and get a different perspective on the role of SixthSense than in theprevious experiments, we ran ReTrASE and ReTrASE + 6S for at most 12 hours on each of the 45 problems of the EBW-06, -08, and Drive sets, and noted the policy quality, as reflected by the success rate, at fixed time intervals. For smallerproblems, we measured policy quality every few seconds, whereas for larger ones — every 5–10 minutes. Qualitatively, thetrends on all the problems were similar, so here we study them on the example of problem 12 from the EBW-06 set, one ofthe hardest problems attempted. For this problem, after 12 hours of CPU running time ReTrASE + 6S extracted and learnedweights for 62 267 basis functions; it also discovered 79 623 dead ends states. Out of these dead ends, 18 392 were identifiedby ReTrASE + 6S running a deterministic planner starting at them having this planner fail to find a path to the goal. Theremainder, i.e. 77%, were discovered with 15 nogoods that SixthSense derived. Since every deterministic planner call from anon-dead-end state typically yields several basis functions, SixthSense saved ReTrASE at least (79 623 − 18 392)/(62 267 +79 623) ≈ 43% of classical planner invocations, with accompanying time savings. On the other hand, ReTrASE’s running timeis not occupied solely by basis function extraction — a significant fraction of it consists of basis function weight learningand state space exploration. Besides, SixthSense, although fast, was not instantaneous. Therefore, based on this model weexpected the overall speedup caused by SixthSense to be less than 40% and likely also less than 30%.With this in mind, please refer to Fig. 15 showing the plots of policy quality yielded byReTrASE and ReTrASE + 6Sversus time. As expected intuitively, the use of SixthSense does not change ReTrASE’s pattern of convergence, and theshape of the two plots are roughly similar. (If allowed to run for long enough both planners should converge to policiesA. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4743of the same quality, although the plots do not show this.) However, surprisingly, the time it takes ReTrASE + 6S to arriveat a policy of the quality ReTrASE gets after 12 hours of execution turns out to be about 5.5 hours. Thus, the speedupSixthSense has yielded is considerably larger than predicted by our model, roughly 60% versus the expected 30 or less.Additional code instrumentation revealed an explanation for this discrepancy. The model just sketched implicitly assumesthat the time cost of a successful deterministic planner call (one that yields basis functions) and one that proves the stateto be a dead end to be the same. This appears to be far from reality; the latter, on average, was over 4 times moreexpensive. With this factor taken into account, the model would forecast a 77% time savings on classical planner calls dueto employment of SixthSense. With the adjustments we described earlier that need to be made to this figure, it agrees wellwith the actual data.Regarding memory savings, SixthSense helps ReTrASE as well, but the picture here is much clearer. Indeed, since Re-TrASE memoizes only basis functions (with weights), dead ends, and nogoods, a 43% reduction in the total number of theseas predicted by our model straightforwardly translates to the corresponding memory reduction our experiments showed.We point out, however, that even without SixthSense, ReTrASE’s memory requirements are very low compared to otherMDP solvers, and reducing them even further is a less significant performance gain than the boost in speed.Last but not least, we found that SixthSense almost never takes more than 10% of LRTDP + hFF + 6S’s or LRTDP +GOTH’s running time. For LRTDP + hFF + 6S, this fraction includes the time spent on deterministic planner invocations toobtain the basis functions, whereas in LRTDP + GOTH, the classical plans are available to SixthSense for free. In fact, asthe problem size grows, SixthSense eventually gets to occupy less than 0.5% of the total planning time. As an illustration ofSixthSense’s operation, we found out that it always finds the single nogood in the Drive domain after using just 10 deadends for training, and manages to acquire most of the statistically significant immediate dead ends in EBW. In the availableEBW problems, their number is always less than several dozens, which, considering the space savings they bring, attests tonogoods’ high efficiency.5.4. SummaryGOTH is a machine learning algorithm for discovering the counterpart of basis functions, nogoods. The presence of anogood in a state guarantees the state to be a dead end. Thus, nogoods help a planner quickly identify dead ends withoutmemoizing them, helping save memory and time. GOTH serves as a submodule of a planner that periodically attempts to“guess” nogoods using dead ends the planner visited and basis functions the planner discovered as training data. It checkseach guess using a sound planning graph-based verification procedure. Depending on the type of MDP solver, GOTH vastlyspeeds it up, reduces its memory footprint, or both, on MDPs with dead-end states.6. DiscussionThe experiments indicate that the proposed abstraction framework is capable of advancing the state of the art in planningunder uncertainty. Nonetheless, there are several promising directions for future improvement.Making structure extraction faster. Even though employment of basis functions in GOTH renders GOTH much faster thanotherwise, the relatively few classical planner invocations that have to be made are still expensive, and GOTH’s advantage ininformativeness is not always sufficient to secure an overall advantage in speed for the MDP solver that uses it. Incidentally,we noticed that on some of the domains ReTrASE spends a lot of time discovering basis functions that end up havinghigh weights (i.e., are not very “important”). We see two ways of handling the framework’s occasional lack of speed indiscovering useful problem structure.The first approach is motivated by noticing that the speed of basis function extraction depends critically on how fastthe available deterministic planners are on the deterministic version of the domain at hand. Therefore, the speed issue canbe alleviated by adding more modern classical planners to the portfolio and launching them in parallel in the hope thatat least one will be able to cope quickly with the given domain. Of course, this method may backfire when the numberof employed classical planners exceeds the number of cores on the machine where the MDP solver is running, since theplanners will start contending for resources. Nonetheless, up to that limit, increasing the portfolio size should only help. Inaddition, using a reasonably-sized portfolio of planners may help reduce the variance and the average of the time it takesto arrive at a deterministic plan.The above idea is an extensional approach to accelerate the domain structure extraction, one that increases the per-formance of the algorithm by making more computational resources available to it. There is also an intensional one, thatimproves the algorithm itself. The ultimate reason for frequent discovery of “useless” basis functions via deterministic plan-ning is the fact that a basis function’s importance is largely determined by the probabilistic properties of the correspondingtrajectory, something the all-outcomes determinization completely discards. An alternative would be to give classical plan-ners a domain determinization that retains at least some of its probabilistic structure. Although seemingly paradoxical,such determinizations exist, e.g. the one proposed by the authors of HMDPP. Its use could improve the quality of obtainedbasis functions and thus reduce the deterministic planning time spent on discovering subpar ones. Different determiniza-44A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47(:action be-evil:parameters ():precondition (and (gremlin-alive)):effect (and(if (and (has Screwdriver) (has Wrench))(and (plane-broken)))(if (and (has Hammer))(and (plane-broken)(probabilistic 0.9(and (not (gremlin-alive))))))))Fig. 16. Action with conditional effects.tion strategies may also ease the task of the classical planners provided that the determinization avoids enumerating alloutcomes of every action without significant losses in solution quality.Lifting representation to first-order logic. Another potentially fruitful research direction is increasing the power of ab-straction by lifting the representation of basis functions and nogoods to first-order logic. Such representation’s benefits areapparent, for example, in the EBW domain. In EBW, besides the immediate nogoods, there are others of the form “blockb is not in its goal position and has an exploded block somewhere in the stack above it”. Indeed, to move b one wouldfirst need to remove all blocks, including the exploded one, above it in the stack, but in EBW exploded blocks cannot berelocated. Expressed in first-order logic, the above statement would clearly capture many dead ends. In propositional logic,however, it would translate to a disjunction of many ground conjunctions, each of which is a nogood. Each such groundnogood separately accounts for a small fraction of dead ends in the MDP and is almost undetectable statistically, preventingSixthSense from discovering it.Handling conditional effects. So far, we have assumed that an action’s precondition is a simple conjunction of literals.PPDDL’s most recent versions allow for a more expressive way to describe an action’s applicability via conditional effects.Fig. 16 shows an action with this feature. In addition to the usual precondition, this action has a separate preconditioncontrolling each of its possible effects. Depending on the state, any subset of the action’s effects can be executed.The presented algorithms currently do not handle problems with this construct for two reasons.First, regression as defined in Section 2.2 does not work for conditional effects. However, its definition can be easilyextended to such cases. As a starting step, consider a goal trajectory t(e) and suppose that outcome out(ai, i, e), part of t(e),is the result of applying action ai in state si−1 of e. Denote the precondition of k-th conditional effect of ai as cond_preck(ai).When e was sampled, conjunction out(ai, i, e) was generated in the following way. For every k, it was checked whethercond_preck(ai) holds in si−1. If it did, the dice were rolled to select the outcome of the corresponding conditional effect.Denote this outcome as cond_outk(ai, i, e). Furthermore, let lit(cond_outk(ai, i, e)) = ∅ (i.e., let cond_outk(ai, i, e) be an emptyconjunction) if cond_preck(ai) does not hold in si .By definition, cond_preck(ai) can be empty in either of two cases:• If cond_preck(ai) does not hold in si−1;• If cond_preck(ai) holds in si−1 but while sampling cond_outk(ai, i, e) we happened to pick an outcome that does notmodify si−1.In the light of this fact, define the cumulative precondition of out(ai, i, e) ascu_prec(cid:4)(cid:5)out(ai, i, e)= prec(ai) ∧cond_preck(ai)(cid:12)(cid:12) lit(cid:4)(cid:5)cond_outk(ai, i, e)(cid:15)= ∅(cid:2)(cid:7)(cid:11)(cid:6)(cid:13)kand observe thatout(ai, i, e) =(cid:7)(cid:11)(cid:13)cond_outk(ai, i, e).kThus, cu_prec(out(ai, i, e)) is a conjunction of preconditions of those conditional effects of ai that contributed at least oneliteral to out(ai, i, e). In other words, it is the minimum necessary precondition of out(ai, i, e). Therefore, to extend regressionto actions with conditional effects we simply substitute cu_prec(out(ai, i, e)) for prec(ai) into the formulas for generatingbasis functions from Section 2.2 to obtainb0 = G,bi =(cid:7)(cid:8)(cid:8)lit(bi−1) ∪ lit(cid:4)out(an−i+1, n − i + 1, e)(cid:5)(cid:9)(cid:4)\ lit(cid:4)(cid:5)(cid:5)(cid:9)cu_precout(ai, i, e)for 1 (cid:2) i (cid:2) n.Unfortunately, there is a second, practical difficulty with making GOTH, ReTrASE, and SixthSense work in the presenceof conditional effects. Recall that our primary way of obtaining goal trajectories for regression is via deterministic planning.A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4745Determinizing an ordinary probabilistic action yields the number of deterministic actions equal to the number of originalaction’s outcomes. In the presence of conditional effects this statement needs qualification. Each conditional effect can bethought of as describing an “action within an action” with its own probabilistic outcomes. These “inside actions” need not bemutually exclusive. Therefore, the number of outcomes of an action with conditional effects is generally exponential in thelatter’s number. As a consequence, determinizing such actions may lead to a blowup in problem representation size. Furtherresearch is needed to identify special cases in which the determinization of conditional effects can be done efficiently.Beyond stochastic shortest path MDPs. So far, the probabilistic planning community has predominantly concentrated onstochastic shortest path (SSP) MDPs and its subclasses (e.g., the discounted cost MDPs). However, there are interestingproblems beyond SSP MDPs as well. As an example, consider the SysAdmin domain [9], in which the goal is to keepa network of computers running for as long as possible. This type of reward maximization problems has received littleattention up till now, although there has been a recent attempt to tackle it efficiently with heuristic search [34].Extending the abstraction framework to reward-maximization MDPs is a potentially impactful research direction. How-ever, it meets with a serious practical as well as theoretical difficulty. Recall that the natural deterministic analog of SSPMDPs are shortest path problems. Researchers have studied them extensively and developed a wide range of very efficienttools for solving them, such as FF, LPG, LAMA, and others. As shown earlier, techniques presented here critically rely onthese tools for extracting the basis functions and estimating their weights. However, the closest classical counterpart ofprobabilistic reward-maximization scenarios are longest path problems. Known algorithms for various formulations of thissetting are at best exponential in the state space size, explaining the lack of fast solvers for them. In their absence, the in-vention of alternative efficient ways of extracting important causal information is the first step on the way to extendingabstraction beyond SSP MDPs.Abstraction framework and existing planners. Despite improvements being possible, our abstraction framework is usefuleven in its current state, as evidenced by both the experimental and theoretical results. Moreover, it has a property thatmakes its use very practical; the framework is complementary to the other powerful ideas incorporated in successful solversof the recent years, e.g., HMDPP, RFF, FFHop, and others. Thus, abstraction can greatly benefit many of these solvers withno or few sacrifices on their part, and also inspire new ones. As an example, note that FFReplan could be enhanced withabstraction in the following way. It could extract basis functions from deterministic plans it is producing while trying toreach the goal and store each of them along with their weight and the last action regressed before obtaining that particularbasis function. Upon encountering a state subsumed by at least one of the known basis functions, “generalized FFReplan”would select the action corresponding to the basis function with the smallest weight. Besides an accompanying speed boost,which is a minor point in the case of FFReplan since it is very fast as is, FFReplan’s robustness could be greatly improved,since this way its action selection would be informed by several trajectories from the state to the goal, as opposed to justone. Employed analogously, basis functions could speed up FFHop, an MDP solver that has great potential but is somewhatslow in its current form. In fact, we believe that virtually any algorithm for solving SSP MDPs could have its convergenceaccelerated if it regresses the trajectories found during policy search and carries over information from well explored partsof the state space to the weakly probed ones with the help of basis functions and nogoods. We hope to verify this conjecturein the future. At the same time, solvers of discounted-reward MDPs are unlikely to gain much from the kind of abstractionproposed in this paper, even though mathematically the described techniques will work even on this MDP class. Discounted-reward MDPs can be viewed as SSP MDPs where each action has some probability of leading directly to the goal [4]. Asa result, any sequence of actions in a discounted-reward MDP is a goal trajectory. This leads to an overabundance of basisfunctions, potentially making their number comparable to the number of states in the problem.A different approach for having abstraction benefit existing planners is to let ReTrASE produce a value function estimateand to allow another planner, e.g. LRTDP, complete the solution of the problem starting from this estimate. This idea ismotivated by the fact that it is hard to know when ReTrASE has converged on a given problem (and whether it ever will).Therefore, it makes sense to have an algorithm with convergence guarantees take over from ReTrASE at a certain point.Empirical research is needed to determine when the switch from ReTrASE to another solver should happen.7. Related workIn spirit, the concept of extracting useful state information in the form of basis functions is related to explanation-based learning (EBL) [30,27]. In EBL, the planner would try to derive control rules for action selection by analyzing its ownexecution traces. In practice, EBL systems suffer from accumulating too much of such information, whereas the approacheswe have presented do not. The idea of using determinization followed by regression to obtain basis functions has parallelsto some research on relational MDPs, which uses first-order regression on optimal plans in small problem instances toconstruct a policy for large problems in a given domain [21,39]. However, our function aggregation and weight learningmethods are completely different from theirs.ReTrASE, in essence, exploits basis functions to perform dimensionality reduction, but basis functions are not the onlyknown alternative to serve this purpose. Other flavors of dimensionality reduction include algebraic and binary decisiondiagram (ADD/BDD), and principle component analysis (PCA) based methods. SPUDD, Symbolic LAO*, and Symbolic RTDP46A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–47are optimal algorithms that exploit ADDs and BDDs for a compact representation and efficient backups in an MDP [25,15].While they are a significant improvement in efficiency over their non-symbolic counterparts, these optimal algorithms stilldo not scale to large problems. APRICODD, an approximation scheme developed over SPUDD [40], showed promise, but it isnot clear whether it is competitive with today’s top methods since it has not been applied to the competition domains.Some researchers have applied non-linear techniques like exponential-PCA and NCA for dimensionality reduction [38,28].These methods assume the original state space to be continuous and hence are not applicable to typical planning bench-marks.In fact, most basis function-based dimensionality reduction techniques are not applied in nominal domains. A notableexception is FPG [10], which performs policy search and represents the policy compactly with a neural network. Our exper-iments demonstrate that ReTrASE outperforms FPG consistently on several domains.The use of determinization for solving MDPs in general was inspired by advances in classical planning, most notablythe FF solver [26]. The practicality of the new technique was demonstrated by FFReplan [42] that used the FF planneron an MDP determinization for direct selection of an action to execute in a given state. More recent planners to employdeterminization that are, in contrast to FF-Replan, successful at dealing with probabilistically interesting problems includeRFF-RG/BG [41]. At the same time, the latter kind of algorithms typically invokes a deterministic planner many more timesthan our techniques do. This forces them to avoid all-outcomes determinization as these invocations would be too costlyotherwise. Other related planners include Temptastic [44], precautionary planning [17], and FFHop [43].The employment of determinization for heuristic function computation was made famous by the FF heuristic, hFF [26],originally part of a classical planner by the same name. LRTDP [7] and HMDPP [29] adopted this heuristic with no modifi-cations as well. In particular, HMDPP runs hFF on a “self-loop determinization” of an MDP, thereby forcing hFF ’s estimates totake into account some of the problem’s probabilistic information.To our knowledge, there have been no previous attempts to handle identification of dead ends in MDPs. The “Sensitivebut Slow” and “Fast but Insensitive” mechanisms were not actually designed specifically for the purpose of identifyingdead ends and are unsatisfactory in many ways. One possible reason for this omission may be that most MDPs studiedby the Artificial Intelligence and Operations Research communities until recently had no dead ends. However, MDPs withdead ends have been receiving attention in the past few years as researchers realized their probabilistic interestingness [35].Besides the analogy to EBL, SixthSense can also be viewed as a machine learning algorithm for rule induction, similar inpurpose, for example, to CN2 [12]. While this analogy is valid, SixthSense operates under different requirements than mostsuch algorithms, because we demand that SixthSense-derived rules (nogoods) have zero false-positive rate. Last but notleast, our term “nogood” shares its name with and closely mirrors the concept from the areas of truth maintenance systems(TMSs) [13] and constraint satisfaction problems (CSPs) [14]. However, our methodology for finding nogoods has little incommon with algorithms used in that literature.8. ConclusionsA central issue that limits practical applicability of automated planning under uncertainty is scalability of availabletechniques. In this article, we have presented a powerful approach to tackle this fundamental problem — an abstractionframework that extracts problem structure and exploits it to spread information gained by exploring one part of the MDP’sstate space to many others.The components of the framework are the elements of problem structure called basis functions and nogoods. The basisfunctions are preconditions for those sequences of actions (trajectories) that take the agent from some state to the goal withpositive probability. As such, each applies in many of the MDP’s states, sharing associated reachability information acrossthem. Crucially, basis functions are easy to come by via fast deterministic planning or even as a byproduct of the normalprobabilistic planning process. While basis functions describe only MDP states from which reaching the goal is possible,their counterparts, nogoods, identify dead ends, from which the goal cannot be reached. Crucially, the number of basisfunctions and nogoods needed to characterize the problem space is typically vastly smaller than the problem’s state space.Thus, the framework can be used in a variety of ways that increase the scalability of the state of the art methods for solvingMDPs.We have described three approaches illustrating the framework’s operation, GOTH, ReTrASE, and SixthSense. The ex-perimental results show the promise of the outlined abstraction idea. Although we describe several ways to enhance ourexisting framework, even as is it can be utilized to qualitatively improve scalability of virtually any modern MDP solver andinspire the techniques of tomorrow.References[1] D. Aberdeen, S. Thiébaux, L. Zhang, Decision-theoretic military operations planning, in: ICAPS’04, 2004.[2] A. Barto, S. Bradtke, S. Singh, Learning to act using real-time dynamic programming, Artificial Intelligence 72 (1995) 81–138.[3] R. Bellman, Dynamic Programming, Princeton University Press, 1957.[4] D. Bertsekas, J. Tsitsiklis, Neuro-Dynamic Programming, Athena Scientific, 1996.[5] D. Bertsekas, Dynamic Programming and Optimal Control, Athena Scientific, 1995.[6] A. Blum, M. Furst, Fast planning through planning graph analysis, Artificial Intelligence 90 (1997) 281–300.[7] B. Bonet, H. Geffner, Labeled RTDP: Improving the convergence of real-time dynamic programming, in: ICAPS’03, 2003, pp. 12–21.A. Kolobov et al. / Artificial Intelligence 189 (2012) 19–4747[8] B. Bonet, H. Geffner, mGPT: A probabilistic planner based on heuristic search, Journal of Artificial Intelligence Research 24 (2005) 933–944.[9] D. Bryce, O. Buffet, International planning competition, uncertainty part: Benchmarks and results, http://ippc-2008.loria.fr/wiki/images/0/03/Results.pdf,2008.[10] O. Buffet, D. Aberdeen, The factored policy-gradient planner, Artificial Intelligence Journal 173 (2009) 722–747.[11] Tom Bylander, The computational complexity of propositional STRIPS planning, Artificial Intelligence 69 (1994) 165–204.[12] Peter Clark, Tim Niblett, The CN2 induction algorithm, in: Machine Learning, 1989, pp. 261–283.[13] Johan de Kleer, An assumption-based tms, Artificial Intelligence 28 (1986) 127–162.[14] R. Dechter, Constraint Processing, Morgan Kaufmann Publishers, 2003.[15] Z. Feng, E. Hansen, Symbolic heuristic search for factored Markov decision processes, in: AAAI’02, 2002.[16] C. Forgy, Rete: A fast algorithm for the many pattern/many object pattern match problem, Artificial Intelligence 19 (1982) 17–37.[17] J. Foss, N. Onder, D. Smith, Preventing unrecoverable failures through precautionary planning, in: ICAPS’07 Workshop on Moving Planning and Schedul-ing Systems into the Real World, 2007.[18] A. Gerevini, A. Saetti, I. Serina, Planning through stochastic local search and temporal action graphs, Journal of Artificial Intelligence Research 20 (2003)239–290.[19] J. Goldsmith, M. Littman, M. Mundhenk, The complexity of plan existence and evaluation in probabilistic domains, in: UAI’97, 1997.[20] Geoff Gordon, Stable function approximation in dynamic programming, in: ICML, 1995, pp. 261–268.[21] C. Gretton, S. Thiébaux, Exploiting first-order regression in inductive policy selection, in: UAI’04, 2004.[22] C. Guestrin, D. Koller, C. Gearhart, N. Kanodia, Generalizing plans to new environments in relational MDPs, in: IJCAI’03, Acapulco, Mexico, 2003.[23] Carlos Guestrin, Daphne Koller, Ronald Parr, Shobha Venkataraman, Efficient solution algorithms for factored MDPs, Journal of Artificial IntelligenceResearch 19 (2003) 399–468.[24] E. Hansen, S. Zilberstein, LAO[25] J. Hoey, R. St-Aubin, A. Hu, C. Boutilier, SPUDD: Stochastic planning using decision diagrams, in: UAI’99, 1999, pp. 279–288.[26] J. Hoffman, B. Nebel, The FF planning system: Fast plan generation through heuristic search, Journal of Artificial Intelligence Research 14 (2001) 253–: A heuristic search algorithm that finds solutions with loops, Artificial Intelligence 129 (1–2) (2001) 35–62.∗302.[27] S. Kambhampati, S. Katukam, Q. Yong, Failure driven dynamic search control for partial order planners: An explanation based approach, ArtificialIntelligence 88 (1996) 253–315.[28] Philipp Keller, Shie Mannor, Doine Precup, Automatic basis function construction for approximate dynamic programming and reinforcement learning,in: ICML’06, 2006, pp. 449–456.[29] E. Keyder, H. Geffner, The HMDPP planner for planning with probabilities, in: Sixth International Planning Competition at ICAPS’08, 2008.[30] C. Knoblock, S. Minton, O. Etzioni, Integrating abstraction and explanation-based learning in PRODIGY, in: Ninth National Conference on ArtificialIntelligence, 1991.[31] A. Kolobov, Mausam, D. Weld, ReTrASE: Integrating paradigms for approximate probabilistic planning, in: IJCAI’09, 2009.[32] A. Kolobov, Mausam, D. Weld, Classical planning in MDP heuristics: With a little help from generalization, in: ICAPS’10, 2010.[33] A. Kolobov, Mausam, D. Weld, SixthSense: Fast and reliable recognition of dead ends in MDPs, in: AAAI’10, 2010.[34] A. Kolobov, Mausam, D. Weld, Heuristic search for generalized stochastic shortest path mdps, in: ICAPS’11, 2011.[35] Iain Little, Sylvie Thiébaux, Probabilistic planning vs. replanning, in: ICAPS Workshop on IPC: Past, Present and Future, 2007.[36] Mausam, E. Benazara, R. Brafman, N. Meuleau, E. Hansen, Planning with continuous resources in stochastic domains, in: IJCAI’05, 2005, p. 1244.[37] Mausam, P. Bertoli, D. Weld, A hybridized planner for stochastic domains, in: IJCAI’07, 2007.[38] Nicholas Roy, Geoffrey Gordon, Exponential family PCA for belief compression in POMDPs, in: NIPS’02, MIT Press, 2003, pp. 1043–1049.[39] S. Sanner, C. Boutilier, Practical linear value-approximation techniques for first-order MDPs, in: UAI’06, 2006.[40] R. St-Aubin, J. Hoey, C. Boutilier, APRICODD: Approximate policy construction using decision diagrams, in: NIPS’00, 2000.[41] F. Teichteil-Königsbuch, U. Kuter, G. Infantes, Incremental plan aggregation for generating policies in MDPs, in: AAMAS’10, 2010.[42] S. Yoon, A. Fern, R. Givan, FF-Replan: A baseline for probabilistic planning, in: ICAPS’07, 2007, pp. 352–359.[43] S. Yoon, A. Fern, S. Kambhampati, R. Givan, Probabilistic planning via determinization in hindsight, in: AAAI’08, 2008.[44] H.L.S. Younes, R.G. Simmons, Policy generation for continuous-time stochastic domains with concurrency, in: ICAPS’04, 2004, p. 325.