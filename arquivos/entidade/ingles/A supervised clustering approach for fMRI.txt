A supervised clustering approach for fMRI-based inference of brain statesVincent Michela, Alexandre Gramforta, Ga¨el Varoquauxa, Evelyn Egerb, Christine Keribinc,d, BertrandThiriona,∗aParietal team INRIA Saclay-Ile-de-France, France.bINSERM U562, Gif/Yvette, France - CEA/DSV/I2BM/Neurospin/LCOGNc Select team INRIA Saclay-Ile-de-France, France.dUniversit´e Paris Sud, Laboratoire de Math´ematiques, UMR 8628, Orsay, France.1102rpA82]VC.sc[1v4035.4011:viXraAbstractWe propose a method that combines signals from many brain regions observed in functional Magnetic Res-onance Imaging (fMRI) to predict the subject’s behavior during a scanning session. Such predictions sufferfrom the huge number of brain regions sampled on the voxel grid of standard fMRI data sets: the curse ofdimensionality. Dimensionality reduction is thus needed, but it is often performed using a univariate featureselection procedure, that handles neither the spatial structure of the images, nor the multivariate nature of thesignal. By introducing a hierarchical clustering of the brain volume that incorporates connectivity constraints,we reduce the span of the possible spatial configurations to a single tree of nested regions tailored to the sig-nal. We then prune the tree in a supervised setting, hence the name supervised clustering, in order to extracta parcellation (division of the volume) such that parcel-based signal averages best predict the target informa-tion. Dimensionality reduction is thus achieved by feature agglomeration, and the constructed features nowprovide a multi-scale representation of the signal. Comparisons with reference methods on both simulatedand real data show that our approach yields higher prediction accuracy than standard voxel-based approaches.Moreover, the method infers an explicit weighting of the regions involved in the regression or classificationtask.Keywords: fMRI, brain reading, prediction, hierarchical clustering, dimension reduction, multi-scaleanalysis, feature agglomeration1. IntroductionInferring behavior information or cognitive statesinverse infer-from brain activation images (a.k.a.ence) such as those obtained with functional Mag-netic Resonance Imaging (fMRI) is a recent approachin neuroimaging [1] that can provide more sensitiveanalysis than standard statistical parametric mappingprocedures [2]. Specifically, it can be used to assessthe involvement of some brain regions in certain cog-nitive, motor or perceptual functions, by evaluatingthe accuracy of the prediction of a behavioral vari-able of interest (the target) when the classifier is in-stantiated on these brain regions. Such an approach∗Corresponding author: bertrand.thirion@inria.frPreprint submitted to Pattern Recognitioncan be particularly well suited for the investigationof coding principles in the brain [3].Indeed cer-tain neuronal populations activate specifically whena certain perceptual or cognitive parameter reaches agiven value. Inferring the parameter from the neu-ronal activity and extracting the spatial organizationof this coding helps to decode the brain system.Brain decoding requires to define a prediction func-tion such as a classifier that relates the image data torelevant variables. Many methods have been testedfor classification or regression of activation images(Linear Discriminant Analysis, Support Vector Ma-chines, Lasso, Elastic net regression and many oth-ers), but in this problem the major bottleneck remainsthe localization of predictive regions within the brainMay 31, 2018   volume (see [4] for a review). Selection of relevantregions, a.k.a. feature selection, is important both toachieve accurate prediction (by alleviating the curseof dimensionality) and understand the spatial distri-bution of the informative features [5].In particu-lar, when the number of features (voxels, regions)is much larger (∼ 105) than the numbers of samples(images) (∼ 102), the prediction method overfits thetraining set, and thus does not generalize well. Todate, the most widely used method for feature se-lection is voxel-based Anova (Analysis of Variance),that evaluates each brain voxel independently. Thefeatures that it selects can be redundant, and are notconstrained by spatial information, so that they canbe spread across all brain regions. Such maps aredifficult to interpret, especially compared to standardbrain mapping techniques such as Statistical Para-metric Maps [6]. Constructing spatially-informed pre-dictive features gives access to meaningful maps (e.g.by constructing informative and anatomically coher-ent regions [7]) within the decoding framework ofinverse inference.A first solution is to introduce the spatial infor-mation within a voxel-based analysis, e.g. by addingregion-based priors [8], by using a spatially-informedregularization [9] or by keeping only the neighbor-ing voxels for the predictive model, such as in thesearchlight approach [10]; however the latter approachcannot handle long-range interactions in the informa-tion coding.A more natural way for using the spatial infor-mation is called feature agglomeration, and consistsof replacing voxel-based signals by local averages(a.k.a. parcels) [11, 12, 13, 14]. This is motivatedby the fact that fMRI signal has a strong spatial co-herence due to the spatial extension of the underly-ing metabolic changes and of the neural code [15].There is a local redundancy of the predictive infor-mation. Using these parcel-based averages of fMRIsignals to fit the target naturally reduces the numberof features (from ∼ 105 voxels to ∼ 102 parcels).These parcels can be created using only spatial in-formation, in a purely geometrical approach [16], orusing atlases [17, 18]. In order to take into accountboth spatial information and functional data, cluster-ing approaches have also been proposed, e.g. spec-tral clustering [14], Gaussian mixture models [19],K-means [20] or fuzzy clustering [21]. The opti-mal number of clusters may be hard to find [19, 22],but probabilistic clustering provides a solution [23].Moreover, as such spatial averages can lose the fine-grained information, which is crucial for an accuratedecoding of fMRI data [1, 4, 24], different resolu-tions of information should be allowed [25].In this article, we present a supervised clusteringalgorithm, that considers the target to be predictedduring the clustering procedure and yields an adap-tive segmentation into both large regions and fine-grained information, and can thus be considered asmulti-scale. The proposed approach is a generaliza-tion of [26] usable with any type of prediction func-tions, in both classification and regression settings.Supervised clustering is presented in section 2, andis illustrated in section 3 on simulated data. In sec-tion 4, we show on real fMRI data sets in regres-sion and classification settings, that our method canrecover the discriminative pattern embedded in animage while yielding higher prediction performancethan previous approaches. Moreover, supervised clus-tering appears to be a powerful approach for the chal-lenging generalization across subjects (inter-subjectinverse inference).2. MethodsPredictive linear modelLet us introduce the following predictive linearmodel for regression settings:y = X w + b ,(1)where y ∈ Rn represents the behavior variable and(w, b) are the parameters to be estimated on a train-ing set comprising n samples. A vector w ∈ Rp canbe seen as an image; p is the number of features (orvoxels) and b ∈ R is called the intercept (or bias).The matrix X ∈ Rn×p is the design matrix. Each rowis a p-dimensional sample, i.e., an activation map re-lated to the observation. In the case of classificationwith a linear model, we have:y = sign(X w + b),(2)where y ∈ {−1, 1}n and “sign” denotes the signfunction. The use of the intercept is fundamental in2practice as it allows the separating hyperplane to beoffseted from 0. However for the sake of simplicityin the presentation of the method, we will from nowon consider b as an added coefficient in the vector w.This is done by concatenating a column filled with1 to the matrix X. We note Xj the signal in the jthvoxel (feature) vj.ParcelsWe define a parcel P as a group of connectedvoxels, a parcellation P being a partition of the wholeset of features in a set of parcels:∀j ∈ [1, . . . , p] , ∃k ∈ [1, . . . , δ]: vj ∈ P k,(3)such that∀(k, k)′ ∈ [1, . . . , δ]2 s.t. k 6= k′, P k∩P k′= ∅ (4)where δ is the number of parcels and P k the kth par-cel. The parcel-based signal Xp is the average signalof all voxels within each parcel (other representationcan be considered, e.g. median values of each par-cel), and the kth row of Xp is noted Xpk:Xpk =j|vj∈P k Xjpk(5)Pwhere pk is the number of voxels in the parcel P k.Bayesian Ridge RegressionWe now detail Bayesian Ridge Regression (BRR)which is the predictive linear model used for regres-sion in this article, and we give implementation de-tails on parcel-based averages Xp. BRR is based onthe following Gaussian assumption:i=Np(y|Xp, w, α) =N (yi|Xp,iw, α−1)(6)i=1YWe assume that the noise ǫ is Gaussian with precisionα (inverse of the variance), i.e. p(ǫ|α) = N (0, α−1In).For regularization purpose, i.e. by constraining thevalues of the weights to be small, one can add a Gaus-sian prior on w, i.e. p(w|λ) = N (w|0, λ−1Ip), thatleads to:p(w|Xp, y, α, λ) ∝ N (w|µ, Σ) ,(7)3where:µ = αΣXpΣ = (λIp + αXpT yT Xp)−1(cid:26)(8)In order to have a full Bayesian framework andto avoid degenerate solutions, one can add classi-cal Gamma priors on α ∼ Γ(α; α1, α2) and λ ∼Γ(λ; λ1, λ2):Γ(x; x1, x2) = xx12 xx1−1 exp−xx2Γ(x1)and the parameters update reads:ˆλ = γ+2λ1µT µ+2λ2ˆα =Pi=nn−γ+2α1i=1 (yi−Xp,iµ)2+2α2,((9)(10)i=pi=1αsiλ+αsiwhere γ =, and si are the eigenvalues ofT Xp. In the experiments detailed in this article,Xpwe choose λ1 = λ2 = α1 = α2 = 10−6, i.e. weaklyinformative priors.PBRR is solved using an iterative algorithm thatmaximizes the log likelihood; starting with α = 1and λ = 1, we iteratively evaluate µ and Σ usingEq. (8), and use these values to estimate γ, ˆλ andˆα, using Eq. (10). The convergence of the algorithmis monitored by the updates of w, and the algorithmis stopped if kws+1 − wsk1 < 10−3, where ws andws+1 are the values of w in two consecutive steps.var(yt)2.1. Supervised clusteringIn this section, we detail an original contribu-tion, called supervised clustering, which addressesthe limitations of the unsupervised feature agglom-eration approaches. The flowchart of the proposedapproach is given in Fig. 1.We first construct a hierarchical subdivision ofthe search domain using Ward hierarchical cluster-ing algorithm [27]. The resulting nested parcel setsconstructed from the functional data is isomorphicto a tree. By construction, there is a one-to-one map-ping between cuts of this tree and parcellations of thedomain. Given a parcellation, the signal can be rep-resented by parcel-based averages, thus providing alow dimensional representation of the data (i.e. fea-ture agglomeration). The method proposed in thiscontribution is a greedy approach that optimizes thecut in order to maximize the prediction accuracy basedon the parcel-based averages. By doing so, a par-cellation of the domain is estimated in a supervisedlearning setting, hence the name supervised cluster-ing. We now detail the different steps of the proce-dure.2.1.1. Bottom-Up step: hierarchical clusteringIn the first step, we ignore the target information– i.e.the behavioral variable to be predicted – anduse a hierarchical agglomerative clustering. We addconnectivity constraints to this algorithm (only adja-cent clusters can be merged together) so that onlyspatially connected clusters, i.e. parcels, are cre-ated. This approach creates a hierarchy of parcelsrepresented as a tree T (or dendrogram) [28]. Asthe resulting nested parcel sets is isomorphic to thetree T , we identify any tree cut with a given par-cellation of the domain. The root of the tree is theunique parcel that gathers all the voxels, the leavesbeing the parcels with only one voxel. Any cut of thetree into δ sub-trees corresponds to a unique parcel-lation Pδ, through which the data can be reduced to δparcels-based averages. Among different hierarchi-cal agglomerative clustering, we use the variance-minimizing approach of Ward algorithm [27] in or-der to ensure that parcel-based averages provide afair representation of the signal within each parcel.At each step, we merge together the two parcels sothat the resulting parcellation minimizes the sum ofsquared differences within all parcels (inertia crite-rion).2.1.2. Top-Down step: pruning of the tree TWe now detail how the tree T can be pruned tocreate a reduced set of parcellations. Because the hi-erarchical subdivision of the brain volume (by suc-cessive inclusions) is naturally identified as a treeT , choosing a parcellation adapted to the predictionproblem means optimizing a cut of the tree. Eachsub-tree created by the cut represents a region whoseaverage signal is used for prediction. As no optimalsolution is currently available to solve this problem,we consider two approaches to perform such a cut(see Fig. 2). In order to have ∆ parcels, these twomethods start from the root of the tree T (one uniqueparcel for the whole brain), and iteratively refine the4Figure 2: Top-Down step (Pruning of the tree) - step 2.1.2. Inthe unsupervised cut approach, (left) Ward’s tree is divided into6 parcels through a horizontal cut (blue). In the supervised cutapproach (right), by choosing the best cut (red) of the tree givena score function ζe, we focus on some specific regions of thetree that are more informative.parcellation:• The first solution consists in using the inertiacriterion from Ward algorithm:the cut con-sists in a subdivision of the Ward’s tree into its∆ main branches. As this does not take intoaccount the target information y, we call it un-supervised cut (UC).• The second solution consists in initializing thecut at the highest level of the hierarchy andthen successively finding the new sub-tree cutthat maximizes a prediction score ζ (e.g. ex-plained variance, see Eq.(11) below), while us-ing a prediction function F (e.g. Support Vec-tor Machine [29]) instantiated with the parcels-based signal averages at the current step. As ina greedy approach, successive cuts iterativelycreate a finer parcellation of the search vol-ume, yielding the set of parcellations P1, .., P∆.More specifically, one parcel is split at eachstep, where the choice of the split is drivenby the prediction problem. After δ such stepsof exploration, the brain is divided into δ + 1parcels. This procedure, called supervised cut(SC), is detailed in algorithm 1.2.1.3. Model Selection step: optimal sub-treeTIn both cases, a set of nested parcellations is pro-duced, and the optimal model among the availablecuts still has to be chosen. We select the sub-treeTζ. The corre-that yields the optimal prediction scoresponding optimal parcellation is then used to createbparcels on both training and test sets. A predictionbbFigure 1: Flowchart of the supervised clustering approach. Bottom-Up step (Ward clustering) - step 2.1.1: the tree T is constructedfrom the leaves (the voxels in the gray box) to the unique root (i.e. the full brain volume), following spatial connectivity constraints.Top-Down step (Pruning of the tree) - step 2.1.2: the Ward’s tree is cut recursively into smaller sub-trees, each one correspondingto a parcellation, in order to maximize a prediction accuracy ζ. Model selection - step 2.1.3: given the set of nested parcellationsobtained by the pruning step, we select the optimal sub-treeT , i.e. the one that yields the optimal value for ζ.bfunction is thus trained and tested on these two setof parcels to compute the prediction accuracy of theframework.2.2. Algorithmic considerationsThe pruning of the tree and the model selectionstep are included in an internal cross-validation pro-cedure within the training set. However, this internalcross-validation scheme rises different issues. First,it is very time consuming to include the two stepswithin a complete internal cross-validation. A sec-ond, and more crucial issue, is that performing an in-ternal cross-validation over the two steps yields manydifferent sub-trees (one by fold). However, it is noteasy to combine these different sub-trees in order toobtain an average sub-tree that can be used for pre-diction on the test set [30]. Moreover, the differentoptimal sub-trees are not constructed using all thetraining set, and thus depend on the internal cross-validation scheme. Consequently, we choose an em-pirical, and potentially biased, heuristic that consistsof using sequentially two separate cross-validationschemes Ce and Cs for the pruning of the tree andthe model selection step.2.3. Computational considerationsOur algorithm can be used to search informativeregions in very high-dimensional data, where otherIndeed, the highestalgorithms do not scale well.number of features considered by our approach is ∆,and we can use any given prediction function F , evenif this function is not well-suited for high dimen-sional data. The computational complexity of theproposed supervised clustering algorithm depends thuson the complexity of the prediction function F , andon the two cross-validation schemes Ce and Cs. Atthe current iteration δ ∈ [1, ∆], δ + 1 possible fea-tures are considered in the regression model, and theregression function is fit n(δ + 1) times (in the caseof a leave-one-out cross-validation with n samples).Assuming the cost of fitting the prediction functionF is O(δα) at step δ, the overall cost complexity ofthe procedure is O(n∆(2+α)).In general ∆ ≪ p,and the cost remains affordable as long as ∆ < 103,which was the case in all our experiments. Highervalues for ∆ might also be used, but the complexityof F has to be lower.The benefits of parcellation come at a cost re-garding CPU time. On a subject of the dataset onthe prediction of size (with a non optimized Pythonimplementation though), with ∼ 7.104 voxels, the5construction of the tree raising CPU time to 207 sec-onds and the parcels definition raising CPU time (In-tel(R) Xeon(R), 2.83GHz) to 215 seconds. Neverthe-less, all this remains perfectly affordable for standardneuroimaging data analyzes.Algorithm 1: Pseudo-code for supervised cutSet a number of exploration steps ∆, a scorefunction ζ, a prediction function F , and twocross-validation schemes Ce and Cs.Let Pδ be the parcellation defined at thecurrent iteration δ and Xpδ the correspondingparcel-based averages.Construct T using Ward algorithm.Start from the root of the tree T , i.e.P0 = {P0} has only one parcel P0 thatcontains all the voxels.Pruning of the tree Tfor δ ← 1 to ∆ do2} according to T .1, Piforeach Pi ∈ Pδ−1 do- Split Pi → {Pi- Set Pδ,i = {Pδ−1\Pi} ∪ {Pi- Compute the correspondingparcel-based signal averages Xpδ,i.- Compute the cross-validated scoreζe,i(F ) with the cross-validationscheme Ce.1, Pi2}.- Perform the split i⋆ that yields the highestscore ζe,i⋆(F ).- Keep the corresponding parcellation Pδand sub-tree Tδ.Selection of the optimal sub-treefor δ ← 1 to ∆ doT- Compute the cross-validated scoreζs,δ(F ) with the cross-validation schemeCs, using the parcellation Pδ.bReturn the sub-treeparcellationζs,δ⋆(F ).bTδ⋆ and correspondingPδ⋆, that yields the highest scoreb2.4. Performance evaluationOur method is evaluated with a cross-validationprocedure that splits the available data into trainingand validation sets. In the following, (Xl, yl) are alearning set, (Xt, yt) a test set and ˆyt = f (Xt ˆw)6refers to the predicted target, where ˆw is estimatedfrom the training set. For regression analysis, theperformance of the different models is evaluated us-ing ζ, the ratio of explained variance:ζ(yt, ˆyt) =var(yt) − var (yt − ˆyt)var(yt)(11)This is the amount of variability in the response thatcan be explained by the model. A perfect predictionyields ζ = 1, a constant prediction yields ζ = 0). Forclassification analysis, the performance of the differ-ent models is evaluated using a standard classifica-tion score denoted κ , defined as:κ(yt, ˆyt) =i, ˆyti)nti=1 δ(ytnt(12)Pwhere nt is the number of samples in the test set, andδ is Kronecker’s delta.2.5. Competing methodsIn our experiments, the supervised clustering iscompared to different state of the art regularizationmethods. For regression experiments:• Elastic net regression [31], requires setting twoparameters λ1 (amount of ℓ1 norm regulariza-tion) and λ2 (amount of ℓ2 norm regulariza-tion). In our analyzes, an internal cross-validationprocedure on the training set is used to opti-mize λ1 ∈ {0.2˜λ, 0.1˜λ, 0.05˜λ, 0.01˜λ}, where˜λ = kXT yk∞, and λ2 ∈ {0.1, 0.5, 1., 10., 100.}.• Support Vector Regression (SVR) with a linearkernel [29], which is the reference method inneuroimaging. The regularization parameter Cis optimized by cross-validation in the range of10−3 to 10 in multiplicative steps of 10.For classification settings:• Sparse multinomial logistic regression (SMLR)classification [32], that requires an optimiza-tion similar to Elastic Net (two parameters λ1and λ2).• Support Vector Classification (SVC), which isoptimized similarly as SVR.All these methods are used after an Anova-basedfeature selection as this maximizes their performance.Indeed, irrelevant features and redundant informa-tion can decrease the accuracy of a predictor [33].This selection is performed on the training set, andthe optimal number of voxels is selected in the range{50, 100, 250, 500} within a nested cross-validation.We also check that increasing the range of voxels(i.e. adding 2000 in the range of number of selectedvoxels) does not increase the prediction accuracy onour real datasets. The implementation of Elastic netis based on coordinate descent [34], while SVR andSVC are based on LibSVM [35]. Methods are usedfrom Python via the Scikit-learn open source pack-age [36]. Prediction accuracies of the different meth-ods are compared using a paired t-test.3. Simulated data3.1. Simulated one-dimensional dataWe illustrate the supervised clustering on a sim-ple simulated data set, where the informative featureshave a block structure:X ∼ N (0, 1) and y = Xw + ǫ(13)with ǫ ∼ N (0, 1) and w is defined as wi ∼ U 1.250.75 for20 ≤ i ≤ 30, wi ∼ U −0.75−1.25 for 50 ≤ i ≤ 60, andwi = 0 elsewhere, where U ba is the uniform distribu-tion between a and b. We have p = 200 features andn = 150 images. The supervised cut is used with∆ = 50, Bayesian Ridge Regression (BRR) as pre-diction function F , and procedures Ce and Cs are setto 4-fold cross-validation.3.2. Simulated neuroimaging dataThe simulated data set X consists in n = 100 im-ages (size 12×12×12 voxels) with a set of four cubicRegions of Interest (ROIs) (size 2×2×2). We call Rthe support of the ROIs (i.e. the 32 resulting voxelsof interest). Each of the four ROIs has a fixed weightin {−0.5, 0.5, −0.5, 0.5}. We call wi,j,k the weight ofthe (i, j, k) voxel. To simulate the spatial variabilitybetween images (inter-subject variability, movementartifacts in intra-subject variability), we define a newsupport of the ROIs, called ˜R such as, for each im-age, half (randomly chosen) of the weights w are set7to zero. Thus, we have ˜R ⊂ R. We simulate thesignal in the (i, j, k) voxel of the lth image as:Xi,j,k,l ∼ N (0, 1)(14)The resulting images are smoothed with a Gaussiankernel with a standard deviation of 2 voxels, to mimicthe correlation structure observed in real fMRI data.The target y for the lth image is simulated as:yl =wi,j,kXi,j,k,l + ǫl(15)X(i,j,k)∈ ˜Rand ǫl ∼ N (0, γ) is a Gaussian noise with standarddeviation γ > 0. We choose γ in order to have asignal-to-noise (SNR) ratio of 5 dB. The SNR is de-fined here as 20 times the log of the ratio betweenthe norm of the signal and the norm of the addednoise. We create a training set of 100 images, andthen we validate on 100 other images simulated ac-cording to Eq. 14-15. We compare the supervisedclustering approach with the unsupervised cluster-ing and the two reference algorithms, Elastic net andSVR. The two reference methods are optimized by4-fold cross-validation within the training set in therange described below. We also compare the meth-ods to a searchlight approach [10] (radius of 2 and3 voxels, combined with a SVR approach (C = 1)),which has emerged as a reference approach for de-coding local fine-grained information within the brain.Both supervised cut and unsupervised cut algo-rithms are used with ∆ = 50, Bayesian Ridge Re-gression (BRR) as prediction function F , and opti-mized with an internal 4-fold cross-validation.3.3. Results on one-dimensional simulated dataThe results of the supervised clustering algorithmare given in Fig. 3. On the top, we give the tree T ,where the parcels found by the supervised cluster-ing are represented by red squares, and the bottomrow are the input features. The features of interestare represented by green dots. We note that the al-gorithm focuses the parcellation on two sub-regions,while leaving other parts of the tree unsegmented.The weights found by the prediction function basedon the optimal parcellation (bottom) clearly outlinesthe two simulated informative regions. The predictedweights are normalized by the number of voxels ineach parcel.3.4. Results on simulated neuroimaging dataWe compare different methods on the simulateddata, see Fig. 4. The predicted weights of the twoparcel-based approaches are normalized by the num-ber of voxels in each parcel. Only the supervisedclustering (e) extracts the simulated discriminativeregions. The unsupervised clustering (f) does not re-trieve the whole support of the weights, as the createdparcels are constructed based only on the signal andspatial information, and thus do not consider the tar-get to be predicted. Elastic net (h) only retrieves partof the support of the weights, and yields an overlysparse solution which is not easy to interpret. SVR(g) approach yields weights in the primal space thatdepend on the smoothness of the images. The search-light approach (c,d), which is a commonly used brainmapping techniques, shows here its limits: it doesnot cope with the long range multivariate structureof the weights, and yields very blurred informativemaps, because this method naturally degrades dataresolution.4. Experiments and results on real data4.1. Details on real dataWe apply the different methods to analyze tensubjects from an fMRI dataset related to the studyof the visual representation of objects in the brain(see [37] for details). During the experiment, tenhealthy volunteers viewed objects of two categories(each one of the two categories is used in half ofthe subjects) with four different exemplars in eachcategory. Each exemplar was presented at three dif-ferent sizes (yielding 12 different experimental con-ditions per subject). Each stimulus was presentedfour times in each of the six sessions. We averageddata from the four repetitions, resulting in a total ofn = 72 images by subject (one image of each stim-ulus by session). Functional images were acquiredon a 3-T MR system with eight-channel head coil(Siemens Trio, Erlangen, Germany) as T2*-weightedecho-planar image (EPI) volumes. Twenty transverseslices were obtained with a repetition time of 2s (echotime, 30ms; flip angle, 70◦; 2 × 2 × 2-mm voxels;0.5-mm gap). Realignment, normalization to MNIspace, and General Linear Model (GLM) fit wereFigure 3: Illustration of the supervised clustering algorithm ona simple simulated data set. The cut of the tree (top, red line)focuses on the regions of interest (top, green dots), which al-lows the prediction function to correctly weight the informativefeatures (bottom).(a) Trueweights(b) AnovaF-scores(c) SearchlightSVR (r = 2)(d) SearchlightSVR (r = 3)(e) Supervisedclustering(f) Unsupervisedclustering(g) SVRCross-validated(h) Elastic netCross-validatedFigure 4: Comparisons of the weights given by the differentprocedures (b-h) with the true weights (a). Only the super-vised cut algorithm (e) retrieves the regions of interest. Forthe searchlight approach (c, f), the images show the explainedvariance obtained using the voxels within a sphere centered oneach voxel.performed with the SPM5 software1. In the GLM,the time course of each of the 12 stimuli convolvedwith a standard hemodynamic response function wasmodeled separately, while accounting for serial auto-1http:// www.fil.ion.ucl.ac.uk/spm/software/spm58correlation with an AR(1) model and removing low-frequency drift terms with a high-pass filter with acut-off of 128 s. In the present work we used the re-sulting session-wise parameter estimate images. Allthe analysis are performed on the whole brain vol-ume.Regression experiments: The four different ex-emplars in each of the two categories were pooled,leading to images labeled according to the 3 possiblesizes of the object. By doing so, we are interestedin finding discriminative information to predict thesize of the presented object. This reduces to a regres-sion problem, in which our goal is to predict a simplescalar factor (size or scale of the presented object).We perform an inter-subject regression analysison the sizes. This analysis relies on subject-specificfixed-effects activations, i.e. for each condition, thesix activation maps corresponding to the six sessionsare averaged together. This yields a total of twelveimages per subject, one for each experimental condi-tion. The dimensions of the real data set are p ∼ 7 ×104 and n = 120 (divided into three different sizes).We evaluate the performance of the method by cross-validation (leave-one-subject-out). The parametersof the reference methods are optimized with a nestedleave-one-subject-out cross-validation within the train-ing set, in the ranges given before. The supervisedclustering and unsupervised clustering are used withBayesian Ridge Regression (BRR) (as described insection 3.3 in [38]) as prediction function F .In-ternally, a leave-one-subject-out cross-validation isused and we set the maximal number of parcels to∆ = 75. The optimal number of parcels is thus se-lected between 1 and 75 by a nested cross-validationloop.A major asset of BRR is that it adapts the reg-ularization to the data at hand, and thus can copewith the different dimensions of the problem: in thefirst steps of the supervised clustering algorithm, wehave more samples than features, and for the laststeps, we have more features than samples. The twohyperparameters that governed the gamma distribu-tion of the regularization term of BRR are both setto 10−6 (the prior is weakly informative). We donot optimize these hyperparameters, due to compu-tational considerations, but we check that with moreinformative priors we obtain similar results in the re-gression experiment (0.81 and 0.79 with respectivelyλ1 = λ2 = 0.01 and λ1 = λ2 = 1.).Classification experiments: We evaluate the per-formance on a second type of discrimination whichis object classification. In that case, we averaged theimages for the three sizes and we are interested indiscriminating between individual object shapes. Foreach of the two categories, this can be handled as aclassification problem, where we aim at predictingthe shape of an object corresponding to a new fMRIscan. We perform two analyses corresponding to thetwo categories used, each one including five subjects.In this experiment, the supervised clustering andunsupervised clustering are used with SVC (C =0.01) as prediction function F . Such value of Cyields a good regularization of the weights in the pro-posed approach, and the results are not too sensitiveto this parameter (67.5% for C = 10)4.2. Results for the prediction of sizeThe results of the inter-subjects analysis are givenin Tab.1. Both parcel-based methods perform betterthan voxel-based reference methods. Parcels can beseen as an accurate method for compressing infor-mation without loss of prediction performance. Fig.5 gives the weights found for the supervised cut, thetwo reference methods and the searchlight (SV R withC = 1 and a radius of 2 voxels), using the wholedata set. As one can see, the proposed algorithmyields clustered loadings map, compared to the mapsyielded by the voxel-based methods, which are verysparse and difficult to represent. Compared to thesearchlight, the supervised clustering creates moreclusters that are also easier to interpret as they arewell separated. Moreover, the proposed approachyields a prediction accuracy for the whole brain anal-ysis, a contrario to the searchlight that only gives alocal measure of information.The majority of informative parcel are located inthe posterior part of the occipital cortex, most likelycorresponding to primary visual cortex, with few ad-ditional slightly more anterior parcels in posteriorlateral occipital cortex. This is consistent with theprevious findings [37] where a gradient of sensitivityto size was observed across object selective lateraloccipital ROIs, while the most accurate discrimina-tion of sizes is obtained in primary visual cortex.9MethodsSVRElastic netUC - BRRSC - BRRmean ζ0.770.780.830.82std ζ max ζ min ζ0.580.970.110.650.970.10.730.970.080.70.930.08p-val to UC0.08170.0992-0.8184Table 1: Explained variance ζ for the different methods inThe p-values are computedthe Size prediction analysis.using a paired t-test.The unsupervised cut (UC) algo-rithm yields the best prediction accuracy (leave-one-subject-outcross-validation). The supervised cut (SC) yields similar resultsas UC (the difference is not significant). The two voxel-basedapproaches yield lower prediction accuracy than parcel-basedapproaches.ten times less features than voxel-based approaches,the prediction accuracies of parcel-based approachesare higher. The lower performances of SVC and SMLRcan be explained by the fact that voxel-based ap-proaches can not deal with inter-subject variability,especially in such cases where information can beencoded in pattern of voxels that can vary spatiallyacross subjects.MethodsSVCSMLRUC - SVCSC - SVCMethodsSVCSMLRUC - SVCSC - SVCmean κ48.3342.565.070.0std κ max κ min κ25.075.015.7233.3358.339.4650.075.08.9850.083.3310.67p-val to SC0.0063 **0.0008 **0.1405-mean (std) κ mean (std) κ Mean nb. feat.(voxels/parcels)cat.240.0(6.2)41.6(9.1)68.3(9.7)75(5.2)cat. 156.6(17.8)43.3(9.7)63.3(8.5)65(12.2)4151502117Table 2: Top – Classification performance κ for the differentmethods in the Object prediction analysis. The p-values arecomputed using a paired t-test. The supervised cut (SC) al-gorithm yields the best prediction accuracy (leave-one-subject-out cross-validation). Both parcels-based approaches are sig-nificantly more accurate and more stable than voxel-based ap-proaches. Bottom – Details of the results for the two categoriesand mean number of features (voxels or parcels) for the differ-ent methods. We can notice that parcels yield a good compres-sion of information has with more than ten times less features,parcel-based approaches yield higher prediction accuracy.4.3. Results for the prediction of shapeThe results of the inter-subjects analysis are givenin Tab.2. The supervised cut method outperformsthe other approaches.In particular, the classifica-tion score is 21% higher than with voxel-based SVCand 27% higher than with voxel-based SMLR. Bothparcel-based approaches are significantly more accu-rate and more stable than the voxel-based approaches.The number of features used show the good compres-sion of information performed by the parcels. WithFigure 5: Results for prediction of size. Maps of weights foundby supervised cut, the two reference voxel-based methods andthe searchlight. The proposed algorithm creates very inter-pretable clusters, compared to the reference methods, which isrelated to the fact that they do not consider the spatial struc-ture of the image. Moreover, the supervised clustering yieldssimilar maps as searchlight, but also retrieves some additionalclusters.5. DiscussionIn this paper, we have presented a new methodfor enhancing the prediction of experimental vari-ables from fMRI brain images. The proposed ap-10proach constructs parcels (groups of connected vox-els) by feature agglomeration within the whole brain,and allows to take into account both the spatial struc-ture and the multivariate information within the wholebrain.Given that an fMRI brain image has typically 104to 105 voxels, it is perfectly reasonable to use inter-mediate structures such as parcels for reducing thedimensionality of the data. We also confirmed bydifferent experiments that parcels are a good way totackle the spatial variability problem in inter-subjectsstudies. Thus feature agglomeration is an accurateapproach for the challenging inter-subject general-ization of brain-reading [39, 4]. This can be ex-plained by the fact that considering parcels allows tolocalize functional activity across subjects and thusfind a common support of neural codes of interest(see Fig. 6). On the contrary, voxel-based methodssuffer from the inter-subject spatial variability andtheir performances are relatively lower.Subject 1Subject 2Subject 3VoxelsVoxelsVoxelsParcelsParcelsParcelsFigure 6: Illustration of feature agglomeration to cope withinter-subject variability. The regions implied in the cognitivetask are represented by disks of different colors. The popu-lations of active neurons are not exactly at the same positionacross subjects (top), and the across subjects mean signal in in-formative voxels (bottom) carries very weak information. Thus,it is clear that, in this case, voxel-based decoding approacheswill perform poorly. However, the mean of informative voxelswithin each region across subjects (bottom) carries more infor-mation and should yield an accurate inter-subject prediction.Our approach entails the technical difficulty ofoptimizing the parcellation with respect to the spa-tial organization of the information within the im-age. To break the combinatorial complexity of theproblem, we have defined a recursive parcellation ofthe volume using Ward algorithm, which is further-11more constrained to yield spatially connected clus-ters. Note that it is important to define the parcel-lation on the training database to avoid data overfit.The sets of possible volume parcellations is then re-duced to a tree, and the problem reduces to findingthe optimal cut of the tree. We propose a super-vised cut approach that attempts to optimize the cutwith respect to the prediction task. Although findingan optimal solution is infeasible, we adopt a greedystrategy that recursively finds the splits that most im-prove the prediction score. However, there is still noguarantee that the optimal cut might be reached withthis strategy. Model selection is then performed aposteriori by considering the best generalizing par-cellation among the available models. Additionally,our method is tractable on real data and runs in a veryreasonable of time (a few minutes without specificoptimization).In terms of prediction accuracy, the proposed meth-ods yield better results for the inter-subjects study onthe different experiments, compared to state of the artapproaches (SVR, Elastic net, SVC and SMLR). Thesupervised cut yields similar or higher prediction ac-In the size pre-curacy than the unsupervised cut.diction analysis, the information is probably coarserthan in the object prediction analysis, and thus thesimple heuristic of unsupervised cut yields a goodprediction accuracy. Indeed, the unsupervised clus-tering still optimizes a cost function by selecting thenumber of parcels that maximizes the prediction ac-curacy. Thus, in simple prediction task such as theregression problem detailed in this article, this ap-proach allows to extract almost all the relevant in-formation. However, in the prediction of more fine-grained information, such as in the classification task,the UC procedure does not provide a sufficient ex-ploration of the different parcellations, and does notextract all the relevant information. Contrariwise,the SC approach explores relevant parcellations us-ing supervised information, and thus performs betterthan UC.In terms of interpretability, we have shown onsimulations and real data that this approach has theparticular capability to highlight regions of interest,while leaving uninformative regions unsegmented, andit can be viewed as a multi-scale segmentation scheme[26]. The proposed scheme is further useful to lo-cate contiguous predictive regions and to create in-terpretable maps, and thus can be viewed as an in-termediate approach between brain mapping and in-verse inference. Moreover, compared to a state ofthe art approach for fine-grained decoding, namelythe searchlight, the proposed method yields similarmaps, but additionally, takes into account non-localinformation and yields only one prediction score cor-responding to whole brain analysis. From a neuro-scientific point of view, the proposed approach re-that differences be-trieves well-known results, i.e.tween sizes (or between stimuli with different spatialenvelope in general) are most accurately representedin the signals of early visual regions that have smalland retinotopically laid-out receptive fields.More generally, this approach is not restrictedto a given prediction function and can be used withmany different classification/regression methods. In-deed, by restricting the search of the best subset ofvoxels to a tree pruning problem, our algorithm al-lows us to guide the construction of the predictionfunction in a low-dimensional representation of a high-dimensional dataset. Moreover, this method is notrestricted to brain images, and might be used in anydataset where multi-scale structure is considered asimportant (e.g. medical or satellite images).In conclusion, this paper proposes a method forextracting information from brain images, that buildsrelevant features by feature agglomeration rather thansimple selection. A particularly important propertyof this approach is its ability to focus on relativelysmall but informative regions while leaving vast butuninformative areas unsegmented. Experimental re-sults demonstrate that this algorithm performs wellfor inter-subjects analysis where the accuracy of theprediction is tested on new subjects. Indeed, the spa-tial averaging of the signal induced by the parcella-tion appears as a powerful way to deal with inter-subject variability.References[1] D. D. Cox, R. L. Savoy, Functional magnetic resonanceimaging (fMRI) ”brain reading”: detecting and classify-ing distributed patterns of fMRI activity in human visualcortex, NeuroImage 19 (2) (2003) 261–270.[2] Y. Kamitani, F. Tong, Decoding the visual and subjectivecontents of the human brain, Nature Neuroscience 8 (5)(2005) 679–685.[3] P. Dayan, L. F. Abbott, Theoretical Neuroscience: Com-putational and Mathematical Modeling of Neural Sys-tems, The MIT Press, 2001.[4] J.-D. Haynes, G. Rees, Decoding mental states frombrain activity in humans, Nature Reviews Neuroscience7 (2006) 523–534.[5] M. K. Carroll, G. A. Cecchi, I. Rish, R. Garg, A. R. Rao,Prediction and interpretation of distributed neural activitywith sparse models, NeuroImage 44 (1) (2009) 112 – 122.[6] K. Friston, A. Holmes, K. Worsley, J. Poline, C. Frith,R. Frackowiak, Statistical parametric maps in functionalimaging: A general linear approach, Human Brain Map-ping 2 (1995) 189–210.[7] D. Cordes, V. M. Haughtou, J. D. Carew, K. Arfanakis,K. Maravilla, Hierarchical clustering to measure connec-tivity in fmri resting-state data, Magnetic resonance imag-ing 20 (4) (2002) 305–317.[8] M. Palatucci, T. Mitchell, Classification in very high di-mensional problems with handfuls of examples, in: Prin-ciples and Practice of Knowledge Discovery in Databases(ECML/PKDD), Springer-Verlag, 2007.[9] V. Michel, A. Gramfort, G. Varoquaux, B. Thirion, TotalVariation regularization enhances regression-based brainactivity prediction, in: 1st ICPR Workshop on Brain De-coding 1st ICPR Workshop on Brain Decoding - Patternrecognition challenges in neuroimaging - 20th Interna-tional Conference on Pattern Recognition, 2010, p. 1.[10] N. Kriegeskorte, R. Goebel, P. Bandettini, Information-based functional brain mapping, Proceedings of the Na-tional Academy of Sciences of the United States of Amer-ica 103 (2006) 3863 – 3868.[11] G. Flandin, F. Kherif, X. Pennec, G. Malandain, N. Ay-ache, J.-B. Poline, Improved detection sensitivity in func-tional MRI data using a brain parcelling technique, in:Medical Image Computing and Computer-Assisted Inter-vention (MICCAI’02), Vol. 2488 of LNCS, 2002, pp.467–474.[12] T. M. Mitchell, R. Hutchinson, R. S. Niculescu, F. Pereira,X. Wang, M. Just, S. Newman, Learning to decode cogni-tive states from brain images, Machine Learning V57 (1)(2004) 145–175.[13] Y. Fan, D. Shen, C. Davatzikos, Detecting cognitive statesfrom fmri images by machine learning and multivariateclassification, in: CVPRW ’06: Proceedings of the 2006Conference on Computer Vision and Pattern RecognitionWorkshop, 2006, p. 89.[14] B. Thirion, G. Flandin, P. Pinel, A. Roche, P. Ciuciu, J.-B. Poline, Dealing with the shortcomings of spatial nor-malization: Multi-subject parcellation of fMRI datasets,Hum. Brain Mapp. 27 (8) (2006) 678–693.[15] K. Ugurbil, L. Toth, D.-S. Kim, How accurate is magneticresonance imaging of brain function?, Trends in Neuro-sciences 26 (2) (2003) 108 – 114.[16] D. Kontos, V. Megalooikonomou, D. Pokrajac, A. Lazare-12on Pattern Analysis and Machine Intelligence 27 (2005)957–968.[33] G. Hughes, On the mean accuracy of statistical patternrecognizers, Information Theory, IEEE Transactions on14 (1) (1968) 55–63.[34] J. Friedman, T. Hastie, R. Tibshirani, Regularization pathsfor generalized linear models via coordinate descent,Journal of Statistical Software 33 (1).[35] C.-C. Chang, C.-J. Lin, LIBSVM: a library foratsupporthttp://www.csie.ntu.edu.tw/˜cjlin/libsvm(2001).vector machines,availablesoftware[36] scikit-learn, http://scikit-learn.sourceforge.net/, version0.2 (downloaded in Apr. 2010).[37] E. Eger, C. Kell, A. Kleinschmidt, Graded size sensitivityof object exemplar evoked activity patterns in human locsubregions, J. Neurophysiol. 100(4):2038-47.[38] C. M. Bishop, Pattern Recognition and Machine Learn-ing (Information Science and Statistics), 1st Edition,Springer, 2007.[39] K. A. Norman, S. M. Polyn, G. J. Detre, J. V. Haxby, Be-yond mind-reading: multi-voxel pattern analysis of fmridata., Trends Cogn Sci 10 (9) (2006) 424–430.vic, Z. Obradovic, O. B. Boyko, J. Ford, F. Makedon, A. J.Saykin, Extraction of discriminative functional MRI acti-vation patterns and an application to alzheimer’s disease,in: Med Image Comput Comput Assist Interv. MICCAI2004, 2004, pp. 727–735.[17] N. Tzourio-Mazoyer, B. Landeau, D. Papathanassiou,F. Crivello, O. Etard, N. Delcroix, B. Mazoyer, M. Jo-liot, Automated anatomical labeling of activations in spmusing a macroscopic anatomical parcellation of the MNIMRI single-subject brain, NeuroImage 15 (1) (2002) 273–289.[18] M. Keller, M. Lavielle, M. Perrot, A. Roche, Anatomi-cally Informed Bayesian Model Selection for fMRI GroupData Analysis, in: 12th MICCAI, 2009.[19] B. Thyreau, B. Thirion, G. Flandin,J.-B. Poline,Anatomo-functional description of the brain: a proba-bilistic approach, in: Proc. 31th Proc. IEEE ICASSP,Vol. V, 2006, pp. 1109–1112.[20] S. Ghebreab, A. Smeulders, P. Adriaans, Predicting brainstates from fMRI data: Incremental functional principalcomponent regression, in: Advances in Neural Informa-tion Processing Systems, MIT Press, 2008, pp. 537–544.[21] L. He, I. R. Greenshields, An mrf spatial fuzzy clusteringmethod for fmri spms, Biomedical Signal Processing andControl 3 (4) (2008) 327 – 333.[22] P. Filzmoser, R. Baumgartner, E. Moser, A hierarchicalclustering method for analyzing functional mr images,Magnetic Resonance Imaging 17 (6) (1999) 817 – 826.[23] A. Tucholka, B. Thirion, M. Perrot, P. Pinel, J.-F. Man-gin, J.-B. Poline, Probabilistic anatomo-functional parcel-lation of the cortex: how many regions?, in: 11thProc.MICCAI, LNCS Springer Verlag, 2008.[24] J.-D. Haynes, G. Rees, Predicting the orientation of invis-ible stimuli from activity in human primary visual cortex,Nature Neuroscience 8 (5) (2005) 686–691.[25] P. Golland, Y. Golland, R. Malach, Detection of spatialactivation patterns as unsupervised segmentation of fMRIdata, Med Image Comput Comput Assist Interv. MICCAI2007, 2007, pp. 110–118.[26] V. Michel, E. Eger, C. Keribin, J.-B. Poline, B. Thirion, Asupervised clustering approach for extracting predictiveinformation from brain activation images, MMBIA’10.[27] J. H. Ward, Hierarchical grouping to optimize an objectivefunction, Journal of the American Statistical Association58 (301) (1963) 236–244.[28] S. C. Johnson, Hierarchical clustering schemes, Psy-chometrika 2 (1967) 241–254.[29] C. Cortes, V. Vapnik, Support-vector networks, MachineLearning 20 (3) (995) 273–297.[30] J. J. Oliver, D. J. Hand, On pruning and averaging deci-sion trees, in: ICML, 1995, pp. 430–437.[31] H. Zou, T. Hastie, Regularization and variable selectionvia the elastic net, J. Roy. Stat. Soc. B 67 (2005) 301.[32] B. Krishnapuram, L. Carin, M. A. Figueiredo, A. J.Hartemink, Sparse multinomial logistic regression: Fastalgorithms and generalization bounds, IEEE Transactions13