ELSEVIER Artificial Intelligence 92 ( 1997) 169-227 Artificial Intelligence Map learning with uninterpreted sensors and effecters ’ David Pierce *, Benjamin J. Kuipers * Department of Computer Sciences, University of Texas at Austin, Taylor Hall, Austin, TX 78712, USA Received March 1996; revised October 1996 Abstract This paper presents a set of methods by which a learning agent can learn a sequence of increasingly abstract and powerful interfaces to control a robot whose sensorimotor apparatus and environment are initially unknown. The result of the learning is a rich hierarchical model of the robot’s world (its sensorimotor apparatus and environment). The learning methods rely on generic properties of the robot’s world such as almost-everywhere smooth effects of motor control signals on sensory features. At the lowest level of the hierarchy, the learning agent analyzes the effects of its motor control signals in order to define a new set of control signals, one for each of the robot’s degrees of freedom. It uses a generate-and-test approach to define sensory features that capture important aspects of the environment. It uses linear regression to learn models that characterize context-dependent effects of the control signals on the learned features. It uses these models to define high-level control laws for finding and following paths defined using constraints on the learned features. The agent abstracts these control laws, which interact with the continuous environment, to a finite set of actions that implement discrete state transitions. At this point, the agent has abstracted the robot’s continuous world to a finite-state world and can use existing methods to learn its structure. The learning agent’s methods are evaluated on several simulated robots with different sensorimotor systems and environments. @ 1997 Elsevier Science B.V. Keywords: Spatial semantic hierarchy; Map learning; Cognitive maps; Feature leaming; Abstract interfaces; Action models; Changes of representation * Corresponding ’ This work has taken place in the Qualitative Reasoning Group at the Artificial author. E-mail: kuipers@cs.utexas.edu. University of Texas at Austin. Research of the Qualitative Reasoning Group IRI-9216584 Research Program under grant no. 003658-242. and IRI-9504138, Intelligence Laboratory, The in part by NSF grants by NASA grants NCC 2-760 and NAG 2-994, and by the Texas Advanced is supported 2 E-mail: dmpierce@cs.utexas.edu. 0004-3702/97/$17.00 PIISOOO4-3702(96)00051-3 @ 1997 El sevier Science B.V. All rights reserved. 170 D. Pierce. B.J. Kuipers/Artijkial Intelligence 92 (1997) 169-227 1. Introduction Suppose into an unknown to the environment, a creature emerges are sufficient to clarify environment, with no knowledge to support such a learning process? of what its sensors are sensing or what its effecters are effecting. How can such a creature to learn about the nature of its environment‘? learn enough about its sensors and effecters capabilities What primitive is idealized This problem embodies knowledge designed and programmed effecters appropriate goals of the robot. A real biological evolution, the sensorimotor of the environment. We idealize both of these to the problem learning agent with very little domain-specific a number scientific value, allowing a newly-designed We report here on one along with several variations general problem. the goals and results of our research. A real robot in by engineers who select sensors and to the through to the demands faced by an individual to apply In addition to its practical value in system. instance of this problem, to the learning methods. learning agent would be of considerable robot to learn the properties of its own sensorimotor knowledge, but with the ability the range of possible solutions that solves a specific embodies knowledge, that begin to explore domain-independent of sophisticated, laws appropriate of the organism and implement this idealized that matches capabilities organism acquired learning control agent Henceforth, we make a distinction between the learning that the learning agent and agent must the robot. The learn how to apparatus is a machine is uninterpreted, meaning or simulated) apparatus (physical robot is comprised of a set of sensors and effecters. use. The robot’s sensorimotor The sensorimotor that the agent that is learning how to use the robot has no a priori knowledge of the meaning of the sensors, of the structure of the sensory system, or of the effects of the motor’s control signals. From the learning as a raw sense vector s apparatus agent’s perspective, and a raw motor control vector u. The former the current values of all of the sensors. The latter is a vector of real numbers, called control signals, produced by the learning agent and sent to the robot’s motor apparatus. The agent’s situation learning is represented is a vector of real numbers giving the sensorimotor is illustrated in Fig. 1. This paper solves the learning problem by presenting a set of methods that the learning agent can use to learn ( 1) a model of the robot’s set of sensors, (2) a model of the robot’s motor apparatus, and (3) a set of behaviors that allow the learning agent to abstract world to a discrete world of places and paths. the robot’s continuous on a simulated mobile robot with a ring of These methods have been demonstrated distance These sensors. learning methods comprise agent a priori. They incorporate and control based on a particular structure or even the dimensionality a body of knowledge that is given a knowledge of basic mathematics, multivariate theory. The learning methods are domain independent set of sensors or effecters and do not make assumptions of the robot’s environment. to the learning analysis, in that they are not about the In the rest of this paper, we describe a number of learning methods and show how of a robot’s world agent as it develops an understanding they are used by a learning D. Pierce, B.J. Kuipers/Artijicial Inrelligence 92 (1997) 169-227 171 Sensory input Control I \ 2 0 \ / I Fig. I. The learning problem addressed is illustrated by this interface between a learning agent and a teleoperated is to learn a model of the robot and its environment with no initial knowledge of the meanings of the sensors or the effects of the control signals robot in an unknown environment. The learning agent’s problem that nothing changes when the control signals are all zero), in this paper (except by defining learning agent’s problem and solution are given below: a sequence of increasingly powerful abstract interfaces to the robot. The Problem. Given: A robot with an uninterpreted, almost-everywhere approximately linear sensori- motor apparatus Learn: Descriptions in a continuous, static environment. of the structure of the robot’s sensorimotor apparatus and environ- ment and an abstract interface to the robot suitable for prediction and navigation. Solution. Representation: A hierarchical model. At the bottom of the hierarchy models of the robot’s sensorimotor abstraction of the robot’s environment apparatus. At the top of the hierarchy are egocentric is a discrete defined by a set of discrete views and actions. the objects for learning Method: A sequence of statistical and generate-and-test methods of the hierarchical model. An almost-everywhere approximately linear sensorimotor apparatus satisfies the fol- functions real-valued of the motor control vector. A continuous world to time of the sensor values can be approximated includes is one whose state can be represented by a vector state variables. A discrete world, on the other hand, is rep- robot longitude lowing: The derivatives with respect by linear both the robot and its environment) x of continuous, resented by a finite set of states. The primary example (e.g., in a continuous world with and latitude) it is facing). A static world is one whose state does not change except as the result of a nonzero motor control vector. A static world exhibits no inertia. When the motor controls go to zero, stop. In a static world, there are no active agents (e.g., the robot comes pedestrians) two for its position in which and one for its orientation in this paper 3 is a mobile three state variables: the robot itself. to an immediate the direction besides (which (i.e., ’ Experiments with other robots are described in connection with particular learning methods. 172 D. Pierce, B.J. Kuipers/Art@cinl Intelligence 92 (1997) 169-227 its world, The learning to move efficiently refers to the ability agent’s goal is to understand to another. These definitions do not apply perfectly that is, to learn a model of it for prediction and navigation. Prediction refers to the ability to predict the effects from one to the learning agent’s world: must be discovered or invented by the learning agent level the global structure of a world. People do not understand from visual the learning based on the the suitable of the motor control signals. Navigation place places do not exist a priori-they itself. The raw sense vector and the raw motor control vectors are at the wrong of abstraction their world scenes agent must also use abstractions. raw sense vector, it needs world The hierarchy [ 16,18-201. features and behaviors. Understanding and accompanying descriptions. the spatial semantic hierarchy thus requires a hierarchy of features, behaviors, is called use abstractions its continuous world, to make predictions to understand Instead of trying in terms of sequences of visual to places and objects. to learn high-level for describing agent uses images-they In order learning that the 1.1. The spatial semantic hierarchy The spatial semantic hierarchy (SSH) is a hierarchical structure for a substantial body larger knowledge, the large-scale spatial structure of its environment. showing how a cognitive map can be built on sensorimotor of common-sense interaction with the world. The cognitive map is the body of knowledge (“Large-scale” about significantly be constructed environment.) problem the sensory we obtained was successful, changes than the sensory horizon of the agent, meaning over time as the agent solution an agent has here means that the map must its travels for the cognitive mapping sensors, we focus on learning that solution. The result some subtle but important by integrating Since we already have an SSH-based features and control strategies necessary robot with a ring of distance to cognitive mapping.4 to the SSH approach but at the same for a simulated time revealed observations to support required through topological, sensorimotor, of five levels: level, the abstract and metrical. At the sensorimotor The spatial semantic hierarchy is comprised causal, robot is defined by the raw sense vector, a set of primitive actions of freedom of the robot, Section 3), and a set of learned action models are learned control vectors on features. Local state variables are learned and behaviors and path-following by the set of local state variables, homing behaviors, the cuusul abstracted view and the set of currently level, sense vectors are abstracted to a finite set of actions control, to the interface (one for each degree level, effects of motor for homing to the robot is defined behaviors. At are interface gives the current to a finite set of views and behaviors (Section 7). The abstract (Section 5). The abstract features. At the control the context-dependent and path-following are defined to predict applicable in order interface actions. The contribution of this paper This paper’s work is complementary in which all levels of the descriptive ontology were engineered of the learning agent was on learning the structure of the environment. is a set of methods for learning to the work done by Kuipers and Byun these first three levels. [ 18,191 by hand, and the focus The agent 4 The most important change is the use of local state variables (Section 4). D. Pierce, B.J. Kuipers/Arti&ial Intelligence 92 (1997) 169-227 173 control to the topological laws from a fixed set to form the control levels. At the topological selected appropriate abstracted ambiguities representation level, the topological map is supplemented with distances, directions, information. level, which was level, perceptual states map to the same view) are resolved and a global of the world’s structure as a finite-state graph is learned. At the metrical and other metrical (in which multiple and metrical By showing how to learn paper lays the groundwork semantic hierarchy using only domain-independent the first three levels of the spatial semantic hierarchy, this for building a learning agent that can learn the entire spatial knowledge. 1.2. Overview apparatus and a set of behaviors Sections 2 through 7 describe a sequence of methods for learning a model of a robot’s sensorimotor to abstract the robot’s continuous world to a discrete world of places and paths. Fig. 30 summarizes the entire set of representations, have been described learning methods, and resulting behaviors, that allow the learning agent in the rest of the paper. in detail after they a method for learning Section 2 describes for representing a set of robust, a model of the structure of the robot’s sensory apparatus. Section 3 describes a method for learning a model of the structure of the robot’s motor apparatus. Section 4 describes a method for learning a set of variables for suitable the robot’s state learning space. Section 6 describes a number of experiments in the previous sections) methods. Finally, Section 7 shows how to define an abstract interface apparatus the continuous the local state of the robot. Section 5 describes a method for navigation repeatable behaviors the generality and some limitations of these learning from to those described that demonstrate that abstracts sensorimotor (in addition through to a discrete sensorimotor apparatus. to the learning problem described is an instance of the more general solution outlined These learning methods provide a particular solution in Section 1. This particular below: solution (i) Apply a generate-and-test (ii) Try to learn how to control algorithm to produce a set of scalar features. the generated scalar features. Those that can be controlled are identified as local state variables. (iii) Define homing behaviors-behaviors that move a local state variable to a target value. (iv) Define path-following behaviors-behaviors a local state variable at its target value. that are presented The set of learning methods final word on the problem of learning Instead sequence of steps. Future work will involve both improving and identifying it is one path to the goal. Clearly, alternate paths to the solution. to use an uninterpreted sensorimotor there are other ways to instantiate in this paper does not represent the apparatus. the above the current set of methods that move the robot while keeping The learning methods and experimental each section describes a learning method, method, the source of information of the method applied to a simulated robot. results are interleaved the representations throughout the paper: or objects produced by the used by the method, and one or more demonstrations 174 D. Pierce, B.J. Kuipers/ArtQicinl Intelligence 92 (1997) 169-227 1.3. Contributions The results of this research are the following: (i> (ii) (iii) of a learning agent that can solve a nontrivial the demonstration learning problem; the identification that a robot must have to support such a learning agent; the identification that enable useful cognitive maps of complex environments. of a set of learning methods and intermediate to go from no domain-specific though not unique of a plausible the learning agent instance of the set of primitive capabilities representations knowledge to learning methods are interesting These source of information available motor apparatus and, second, each provides a method give the learning way of interacting with the robot’s environment. agent a new way of understanding through experimentation with an uninterpreted for exploiting the robot’s sensory that information sensori- to input or a new in their own right. First, each one identifies a result of this work The beginning this result can support conditions learning. to the end of an idealized is an existence proof, demonstrating but important from learning problem. We hope one path further work to establish minimal sets of primitives, the that necessary for success, and the limits of this heterogeneous bootstrapping method for inference to move As intended, three methods learning method laws are specific towards generality capabilities, we required set of features and control used for these experiments. The on the type of robot and environments the learned the type of environment to the robot itself used. We in these results. First, as we needed of the low- and then our inference methods make about the nature of the robot and also has some degree of dependence used to add primitive choice of robot or environment, level symbolic or neural-net mechanisms. 5 Second, we attempted make explicit, or the environment. temporal and spatial continuity of the sensory several key steps in our learning method empirically sensorimotor the paper. Naturally, limited. systems and different environments. the generality we are able inputs. 6 Third, we tested the generality of robot throughout remains These results are shown to establish by these means several feature generators require almost-everywhere that they be independent that they be plausible them to different the assumptions For example, to implement to minimize, by applying using and In spite of the limitations of an existence proof, we believe is important. have demonstrated ing methods can be used trol [ 391) have constructed laws. Only a very it shows how a heterogeneous a deep hierarchy of sensory learning methods to construct few previous similarly deep concept hierarchies. Second, such as AM First, that the approach we set of learn- features and con- (see also the knowledge con- [22] 5 We did not always follow this restriction in the implementation sophisticated method called principal component principal component analysis may be implemented h Real sonar sensors may not satisfy as a neural network this requirement due to specular analysis [ 151 as a feature itself. For example, we use a fairly identification method. However, [ 291. reflection, a property of sonar sensors that makes them difficult to use, even in systems that are engineered by hand. D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 175 in this hierarchy shows how a foundational can be grounded in continuous sensorimotor domain of symbolic common-sense interaction with a continuous tained knowledge world. 2. Learning a model of the sensory apparatus The learning agent’s first step is to learn a model of the robot’s in this step is a set of groups of related the sensors. The source of information apparatus) layout of ratus. The output of the learning method used of the sensory of the physical the sequence choosing motor control vectors ing method robots. in detail and demonstrates of values produced by the robot’s sensors while randomly. The rest of this section describes the method on two very different (i.e., sensory appa- the learned model sensors and a description for this step is the agent wanders by the learn- simulated 2.1. A simulated robot these define in it. The robot For concreteness, the nearest object room of dimensions they take on when from the front. The 21st element in each of 24 directions. These have a maximum the learning methods are illustrated with a particular robot and en- 6 meters vironment. The robot’s world is simulated as a rectangular is itself by 4 meters. The room has a number of walls and obstacles modeled as a point. The robot has 29 sensors. Each sensor’s value lies between 0.0 and 1 .O. Collectively, the raw sense vector s, which is the input from the robot to the learning agent. The first 24 elements of the raw sense vector give the distances to the nearest objects which numbered clockwise value of 0.2. The 25th element slowly compass. The element with value 1 corresponds the robot motor apparatus. and left treads. Moving backward motion; moving Moving learning agent only knows control vector has two elements. Its two motor control signals a0 and at tell how fast to move the right the treads together at the same speed produces pure forward or at the same speed produces pure rotation. the treads at different in a circular arc. The the robot agent does not know what any of these sensors or effecters do. The learning that that robot’s raw sense vector has 29 elements and its raw motor the battery’s voltage, which decreases from an initial value of 1.0. The 26th through 29th elements comprise a digital (E, N, W, or S) in which is no sensor noise. The robot has a “tank-style” value of 1.0, is beyond one meter away. The sonars are is defective and always returns a them in opposition speeds causes is a sensor giving to the direction is most nearly facing. There to move 2.2. A language of features learning agent develops The learning whose current value ues of the robot’s raw sense vector. The type of the feature of the robot’s sensory apparatus by in this paper, time is a function by the history of current and past val- is determined by the type new features. A feature, is completely an understanding as defined determined over 176 D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 value feature (thus a vector image element, focused constructs. The group feature in this paper are the following: is one whose value at any point is a vector.) The types of features used of that function’s time vector, group, matrix, scalar field (or image), jield, and vector field element. Scalar, vector, and matrix dard mathematical in Section 2.3. The image and image-element focused-image element vector The ator is a rule features. in scalar, image, vector features are based on stan- is defined in Section 2.4. The and vector-field- in Section 3.2. Examples of features are the raw sense features). agent produces new features using feature generators. A feature gener- that creates a new feature or set of features based on already existing and the elements of the raw sense vector in Section 4.1.1. The vector-field are defined feature) (a type of vector feature) features (a vector features are defined is defined learning feature (scalar 2.3. Discovering related sensory subgroups A sensory apparatus may contain a structured array of similar sensors. Examples of in a video camera, agent uses the group-feature generator to such arrays of similar sensors. A group feature is a vector feature, X, whose in an array of such arrays are a ring of distance sensors, an array of photoreceptors and an array of touch sensors. The learning recognize elements, xi, are all related similar sensors). in some way (e.g., all correspond to sensors generator array of sensors The group-feature is based on the following engineered that typically varies continuously with sensor position robot and nearby objects), in the array “behave similarly”. Two sensors are said to behave similarly observation. Given a well- that measure a property the the following holds: Sensors that are physically close together (e.g., a ring of distance the distance between sensors) (e.g., if in time tend to be similar and ( 1) the two sensors’ values at each instant (2) the two sensors’ frequency distributions are similar. Given a scalar feature X, the frequency distribution gives, for each of n subintervals the variable assumes a value in that subinterval. in the variable’s domain, (dist x) is an n-element vector that that the percentage of time Corresponding to these two criteria are two distance metrics (examples of matrix features) that are used by the group-feature l The first metric dl is based on the principle generator. sensors generally have similar values. The metric as a matrix feature: that in a continuous world, adjacent for vector feature x, is defined, dl,i.;(t) = Here, dl,ij( t) is the distance between variable r is a time index ranging from 0 to t. sensors xi and Xj measured at time t. The l The second metric d2 is based on the observation that sensors in a homogeneous array have similar sensors can be distinguished frequency distributions. For example, an array of binary from an array of photoreceptors touch by the fact that the D. Pierce, B.J. Kuipers/Arti_iicial Intelligence 92 (1997) 169-227 177 dl 4 Fig. 2. Two measures of dissimilarity, feature after the robot has wandered dl,ij and dz,ij, between for five minutes. The coordinates the ith and jth elements of the raw sensory i and j. are indices types of sensors have radically different different touch sensors can assume value 0 or 1 whereas photoreceptors value from a continuous intervals of absolute differences range. dz,ij is proportional in frequency for elements i and j: frequency distributions. Binary can assume any to the sum over the distribution the subintervals of the frequency distributions. the frequency distributions use 50 subintervals the im- uniformly distributed In where 1 ranges over plementation, over the range [-l,l]. computes these two distance metrics over a period of several minutes strategy: choose a random repeat. 7 The values for 5 minutes the robot using it for one second robot has explored ( 10 time steps); the following agent moves This generator while the learning motor control vector; execute of the distance metrics, dl and d2, after the example (3000 observations) The group-feature ( 1) formation of subgroups of sensors are given in Fig. 2. generator exploits that are similar according to all of the distance these distance metrics in two steps: (2) metrics, and taking of related sensors. the transitive closure of the similarity relation to form closed subgroups Formation of subgroups of similar sensors first step The group-feature subgroups similar according of similar to each distance metric dk: generator’s sensors. Elements is to use the distance metrics dk to form i M j, if they are i and j are similar, written 7 Our experiments have shown that this strategy is more effective for efficiently exploring a large subset of the robot’s state space than choosing motor control vectors randomly at each time step. 178 D. Pierce, B.J. Kuipers/Artz@ial Intelligence 92 (1997) 169-227 i z j iff Vk: i zk j. j requires the use of a threshold. One way to define this threshold, to be more robust than the use of a constant, is this: The definition of i q that has proven Ek,i = 2 y$dk,rj). Each element its neighbors. Elements and dk,ri < &k,j, these constraints i has its own threshold based on the minimum distance i and j are considered similar if and only from i to any of if both dk,ij < &k,i and vice versa. Combining that is if j is close to i from i’s perspective gives i %:k j if dk,;,j < Id{&k,i, l?k,,j}. Formation of closed subgroups generator’s The group-feature second step is to take the related-to relation N. Consider the transitive closure of the the ring of tend sensors relation to produce sensors. Adjacent to be very similar according similarity distance metric, but sensors on opposite since they detect information In spite of this fact, the entire array of distance This the similarity relation M. Two elements if i x j or if there exists some other element k such that i N k and k N j: sides of the ring may be dissimilar from distinct and uncorrelated the related-to relation N as the transitive is accomplished by defining sensors should be grouped i and j are related to each other, written (according again regions of the environment. to the distance to dl) together. closure of i N j, i ,-, j iff i N” j V 3k: (i N k) A (k N j). The related-to an equivalence straightforward is described relation N is clearly relation. Computing (e.g., as a group feature of s. reflexive, symmetric, the relation N for i and j given and transitive and is therefore is the relation x [4] ). An equivalence class of the relation N, if not a singleton, For the example robot, the raw sensory feature has 29 elements. (one of which is defective), a battery-voltage In order, these sensor, and a digital compass. The distance metric for 3000 steps. For each of the elements of the raw sensory is computed while the robot wanders the set feature, sensors are: 24 distance four-element randomly of similar elements {j ( i M j} is computed and shown below: (0 1 2 3 23) (0 1 2 22 23) (0 1 2 3 4) (1 2 3 4 5) (2 3 4 5 6) (3 4 5 6 7) (4 5 6 7) (5 6 7 8 9) (7 8 9 10) (7 8 9 10 11) (8 9 10 11 12) (9 10 11 12 13) (13 14 15 16 17) (10 11 12 13 14) (14 15 16 17 18) (20) (19 21 22 23) (11 12 13 14 15) (15 16 17 18 19) (12 13 14 15 16) (16 17 18 19) (17 18 19 21) (0 1 21 22 23) (0 21 22 23) (26) (27) (28). (24) (25) Notice that the distance sensors are grouped together sensors. For example, element 0. The related-to similarity the group (0 1 2 22 23) contains relation N is obtained by taking relation and is described by the following equivalence classes: into groups of neighboring two elements on each side of the transitive closure of the D. Pierce, B.J. Kuipers/Art$cinl Intelligence 92 (1997) 169-227 179 (0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 21 22 23) (20) defective (24) battery voltage (25) east (26) north (27) west (28) south The distance sensors. sensors have all been grouped together into a group containing no other 2.4. A structural model of the sensory apparatus (which generator represents into subgroups in the array. This is accomplished is a first step but it tells nothing about the the structure of the group of sensors). An image The grouping of the sensors by the image-feature relative positions of the sensors is a rule that takes a group feature and associates generator. The image-feature in order to produce an image a position vector with each element of the group feature feature is a function over time, completely determined by the current and past values of the raw time sense vector, whose value at any given list is a scalar with an associated position vector of image elements. An image element (a vector of n real numbers in a continuous, space). An example of the use of an image feature the photoreceptors intensities The image-feature elements that captures metric dt . This means image should be equal di . Expressed mathematically, the structure of an array of sensors as reflected that the distance between to the distance between of positions in the distance in the to the metric the positions of any two elements those elements according image feature y should satisfy hitting task of the is to find an assignment is an image. An image the pattern of light that represents is to represent is an ordered n-dimensional in a camera. a position generator feature to I/ (PO8 Yi) - (Pas Yj) (I = di,ij, (pos yi) is the position vector associated with the ith element where 11 (pos yi) - (pos y,j) 11 is the Euclidean distance between elements. Finding a set of positions satisfying the above equation in the image and the positions of the ith and jth is a constraint-satisfaction If the group feature x has n elements, problem. constraints. * Specifying parameters: is placed somewhere on the x axis; 2 for the third, which x-y plane; etc. Thus, then the metric dl provides n( n - 1)/2 requires n( n- 1) /2 is placed at the origin; 1 for the second, which is placed somewhere on the n - 1 the positions of n points in n- 1 dimensions n position vectors of dimension 0 for the first point, which the constraints, to satisfy ‘The metric can be represented as a symmetric matrix with zeros on the diagonal. Such a matrix has n(n - 1) /2 free parameters. 180 D. Pierce, B.J. Kuipers/Artificial Intelligence 92 (1997) 169-227 are required. Solving using a technique called metric scaling remains The problem [ 151. 9 for the position vectors given the distance constraints can be done that n points of dimension for large n. In general, sensory arrays are I-, 2-, or 3-dimensional the smallest number of dimensions to use, if not objects. that are is a method the given constraints without excessive error, where the error is given n - 1 are inconvenient for finding meaningless, What needed by the equation is needed to satisfy E = i ~(/l(pOSy,) - (POSY.j)II - dij)*, lJ the dimensions according to their contribution toward Metric scaling helps by ordering minimizing Ignoring the error term. all but the first dimension yields a rough description (i.e., using only of the sensory array with large error (unless the first element of the position the is a one-dimensional vectors), array really that has zero error but contains called a “scree diagram” that is accounted dimensions. The image-feature to m where m maximizes variance The set of two-dimensional is shown sensors the data accounted in Fig. 3(b). in (Fig. 3(a)) that shows for by each dimension, object). Using all n - 1 dimensions a lot of useless information. to subjectively the amount of variance choose the number of dimensions a2( m) - a*( m + 1) where u*(m) Statisticians yields a description use a graph in the data the right number of to be equal is the For the example, m = 2. for the group of distance for by the mth dimension. positions found by metric scaling generator chooses the expression The set of (n - 1)-dimensional these positions position vectors optimally describes the structure of a are projected onto a subspace of lower dimensionaiity, group, but when the resulting description in n - 1 dimensions To compensate in a small-dimension relaxation algorithm adjusted slightly The process continues until each iteration. lo The relaxation the vector of positions many dimensions the relaxation global minimum group of distance for this, a relaxation algorithm is no longer optimal. Elements are generally too close together in the two-dimensional that were the right distance apart projection. is used to find the best set of positions in n - 1 dimensions. The is the given distances space to approximate is an iterative process. On each iteration, each position vector in a direction that reduces the value of the error term E (defined above). the error is very small or ceases to decrease appreciably on algorithm could be used without metric scaling by simply randomly. Metric scaling provides initializing It shows how for the image feature, and it provides a starting point for finds a local but not to the that the algorithm two benefits. the chance of the error function. The application of the relaxation algorithm are needed algorithm, decreasing sensors is illustrated in Fig. 3(c) . 9 It seems plausible that metnc scaling could be implemented component analysis implement principal of an input matrix In See [ 32, p. 651 for a description of the algorithm. into a set of eigenvectors. [29] since in both cases the main computation is the decomposition using a neural net analogous to that used to D. Pierce, B.J. Kuipers/Artijcial Intelligence 92 (1997) 169-227 181 Metric scaling eigenvalues I 1 7 a , 3.5, 2. 0. a (vertical account Fig. 3. Learning of variance dimensions group of distance produce positions of the relaxation to the defective distance a structural model of a ring of distance axis) accounted for by each dimension sensors. is used to assign positions position vectors are projected onto the first two dimensions (a) The scree diagram gives the amount that the first two to elements of the to axis) and shows (horizontal (b) Metric scaling for most of the variance. sensors. The 22-dimensional the representation for the group of distance sensors is more obvious shown above. algorithm (c) A relaxation algorithm is used to find a set of two-dimensional j/pi - pj 11 = dij, (The usefulness that best satisfies in the example of the next section.) Notice the gap corresponding the constraints sensor. The element with index 0 corresponds to the robot’s forward sensor, To summarize, the image-feature generator takes a group feature x and produces an image feature y whose position vectors pi are found using metric scaling and a relaxation algorithm so that they approximately the constraints satisfy IIPi -P,;II =kClxi(t) -xi(t)1 t the dimensionality while keeping experiment that is used later to analyze is a structural description of the robot’s ring of distance sensors the robot’s motor apparatus. of the position vectors pi small. The result of the (Fig. 3 (c) ) 2.5. Learning a sensory model of the roving eye The learning methods are further demonstrated using a more fanciful robot called a “roving eye.” Its primary sensory array is a retina of photoreceptors. in two directions and rotation) This robot is a simulation of a small camera mounted on the movable platform of an X-Y plotter, pointing down at a square picture 10 centimeters on a side. The camera sees one square centimeter of the picture at a time. The robot has 3 degrees of freedom and its state space is described by three state (translation in (two for position variables Fig. 4(a). The actual picture used is as before except sensors has been replaced by a 5 by 5 retinal looking down on a picture. The motor control vector of this robot has three array forward or elements: backward). The robot’s structure in Fig. 4(b). The sensory to the left or right), and slide (for motion that the ring of distance and one for orientation). rotate, slip (for motion is shown system is shown The results parallel identified seven equivalence those of the previous classes: six singletons experiment. The group-feature and one candidate generator for application 182 D. Pierce, B.J. Kuipers/Arlificial InteNigence 92 (1997) 169-227 Picture I IX image a Fig. 4. (a) The robot is 10 centimeters wide. (b) The picture used for the roving-eye coast. is a “roving eye” that can see a 1 centimeter wide image experiment b that that is part of a picture is a close-up view of the Oregon Metric scaling eigenValUeS 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 4 3 2 ’ 9 8 7 14 6 '1 12 [I 19 24 '8 17 ,6 23 22 21 2. 0 5 10 IS 0 LI 12345678910 a b C array. for the group of photoreceptors Fig. 5. (a) The metric-scaling organized in a two-dimensional scaling of photoreceptors. algorithm, with the previous (c) The final set of positions set of positions as initial values. scree diagram for the group of photoreceptors indicates that the sensors are (b) The 2-D projection of the set of positions produced by metric of the grid structure of the array provides an initial approximation relaxation are produced the constraint-satisfaction using generator. Metric scaling produces that the sensory array of the image-feature indicating scaling assigns positions onto positions produced by metric scaling the distances resulting the first two dimensions in the resulting set of positions is shown is best modeled as a two-dimensional to each element of the group feature. Projecting the mapping produces shown is improved by the relaxation the scree diagram of Fig. S(a) object. Metric these positions in Fig. 5(b). The set of so that algorithm the distance metric dl. The image more closely match in Fig. 5(c) . D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 183 3. Learning a model of the motor apparatus Using system, interface the learning that identifies the types of motion its learned model of the robot’s sensory to the robot is capable of producing step is to learn a model of the robot’s motor apparatus. The result of the learning new abstract motor apparatus motion. The source of information motion feature (a type of field feature, defined choosing motor control vectors a wall, from the wall. I’ The image agent’s second is a that the robot’s and that tells how to produce each type of for this step is the sequence of values of a learned in Section 3.2) as the agent wanders by is touching it is facing away input to in terms of the locations of sensors define spatial as well as temporal derivatives, features can be defined, even without knowledge of the physical structure of the environment. The learning agent uses the new motion it is capable of turning but cannot change to define spatial attributes of the sensory in the image. With spatial attributes, its motor apparatus using In the simulations, its position unless feature makes the following it is possible if the robot it possible to analyze so motion randomly. feature steps: (i> (ii) (iii) (iv) (v) feature. is discretized of the effects vectors, {z&}. is used to decompose infinite space of motor It is this average value into a finite set of representative the effect of that control vector. control vector many times in different . The agent repeatedly executes locations and measures that Sample the space of motor control vectors. The robot’s control vectors Compute average motion vector fields (amvfs) each representative the average value of the resulting motion characterizes Apply principal component analysis (PCA). The set of computed amvf s is a representation PCA set of representative combination. Identify primitive actions. Each principal amvf s produced by the representative that produces is identified as a primitive action and can be used to produce motion the robot’s degrees of freedom. De$ne a new abstract inter&ace. For each degree of freedom, that allows the agent to specify signal degree of freedom. is matched the eigenvector to find a control vector control vectors that effect or its opposite. Such a motor control vector, if it exists, for one of that the motor apparatus this set into a basis set of principal eigenvectors, a amvfs from which all amvf s may be produced by linear a new control for that is capable of producing. the amount of motion is defined against interface is a new abstract The result of the learning to the robot comprised of a new set of control signals, one per degree of freedom of the robot. The new interface hides the robot’s motor apparatus details of the motor apparatus. For example, whether a mobile uses tank-style the interface presents learned agent with two control signals: one for rotating and one for advancing. These the control treads or a synchro-drive mechanism, the robot’s motor apparatus using to further characterize signals are used the learned ” The use of a physical prevent the robot from damaging itself. robot would require a provision such as an innate obstacle-avoidance behavior to 184 D. Pierce, B.J. Kuipers/Art$cial Intelligence 92 (1997) 169-227 static and dynamic action models in the rest of this section. in detail (Sections 4 and 5). Steps 1 through 5 are explained 3.1. Sample the space of motor control vectors The choice of the set of representative motor control vectors must satisfy first, they must adequately space of possible eflects motor control vectors must be dense enough amvf that corresponds that produces that effect can be found. two criteria: cover the space of possible motor control vectors so that the of represented. Second, (e.g., an a motor control vector so that, given a desired effect the distribution is adequately to one of the robot’s degrees of freedom), (amvfs) it suffices of 1 where its magnitude that the motor apparatus respectively, it is easy the effects of a uniformly distributed 32 and 100 vectors have been found is approximately set of unit motor to Since we have already made the assumption to characterize (A unit vector has a magnitude linear, control vectors. is equal the square root of the sum of squares of its elements.) For two- and three-dimensional spaces of motor control vectors, be adequate. For the 2-D case, distributed on the unit circle. The ith of n vectors has value (cos( 2ri/n), For the 3-D case, a set of vectors uniformly using unit sphere points collection of electrons on a charged sphere-each possible. These vectors are used as the representative motor control vectors for sampling to any the continuous dimension. ) . is found to lie on the l), and the target distance between any pair of to a of vectors as space of average motion vector fields. This method generalizes algorithm of Section 2.4. The vectors are constrained vector is as far from its neighbors to find a set of vectors than 2. The resulting on the unit sphere to have magnitude that are uniformly the relaxation configuration is analogous distributed sin( 2k/n) is much larger (i.e., to 3.2. Compute average motion vector fields feature field is an ordered a motion vector-field A vector field feature list of vector field elements. A vector field element is a function over time, completely determined by the current and past values of the raw sense vector, whose value at any given time is a vectorpeld. A vector vector with an associated position vector. Given (specifically, vector-field the amount of motion detected at the corresponding is measuring, is a image feature x, (motion x) denotes a feature) whose elements measure to an (e.g., a local object to another on minimum time steps due to the motion of the robot. A vector from the position of the subsequent first image element the motion of that object and is an example of a local motion vector. A list of local motion vectors, one for each image element, there is a property of the image feature from one image element to the position of the second in the robot’s environment, is a motion vector field. To understand what that, corresponding points suppose or discontinuity) in the image. that changes this feature represents location The detection of these motion vectors does not require sophisticated object recognition. It simply image feature. The spatial information spatial and temporal requires information, both of which are provided by an is provided by the positions of the elements of the D. Pierce, B.J. Kuipers/Artificial Intelligence 92 (1997) 169-227 185 information the temporal to time. A temporal is provided by the derivatives of the elements’ values image; as vectors of values represented with respect sequence of images, and associated positions, can be viewed as an intensity t) that maps image function E(p, as a function of time. Such a function has both a positions iP, also called spatial derivative, gP and a temporal derivative, E,. The spatial derivative the gradient in which increases most rapidly. in an image detected by a robot’s sensory array corresponds the intensity A large gradient that gives the direction of E, is a vector to values, called in image-position coordinates intensities, detectable property of the environment relative move. This motion gradient will, an informal motivation flow at a point optical results for the optical flow constraint to have magnitude in an image to a such as the edge of an object. If the object moves in the image will in the image with a large is the [ Ill, which defines the edges detected equation -Et/lll?ppII and direction EPp: in intensity. A point in the presence of motion, also have a large temporal derivative. This in a change to a robot’s sensory array (or vice versa), Here, Ili?P’pll is the magnitude of the vector zP, equal to the square root of the sum of squares of the elements of gP. A problem with this formulation is that if the magnitude (or zero), to error (or is undefined). of _l?P is small the goal here is to measure average motion over time and since the measurement optical Since of the flow is more precise at edges or, in general, when the gradient Ep is large, we then the calculation is prone -+ have found value of: it useful to weight the expression using the term ll,!?Pp112 and measure the v = -E&. vision applications, In most computer images are represented for the spatial derivative at a point (picture elements). With such a representation, spaced as regularly to it is straightforward in the image. The images so we use a different flow measured at from i (as to the distance between is given below, where the vector of positions associated with feature n, and (val X) denotes arrays of pixels define an approximation as defined here, however, do not have such a regular structure approach to defining what we call the sensoryflowjGld. element element defined elements (pas x) denotes the vector of values associated with feature x: i is taken to be a weighted sum of local motion vectors Vii in the direction i to element j where j ranges over all of the elements close to element in Section 2.3). The weight i and j. The precise definition of the motion operator is inversely proportional The sensory pos (motion x) “Gf pos x, (vaZ(moti0n.x)); zffVij/llpij(l, 186 D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 Fig. 6. Examples of average motion vector fields (anzvfs) associated motor control vectors in the lower-left average local motion vector with each position position, direction, and magnitude of one of these average (shown in an image as collections of line segments) (represented comer of each picture). An amvf associates represents (see Fig. 3). Each line segment and an the local motion vectors. vij = -Ef,iI?p ij 9 E,,i = $(valx)i, jj ,_ = ((vuzx).j - (Valx)i) PJ.1 llPi,jIl & 1lPi.j II ’ IJpij(j is the distance Here, E,,i is the temporal derivative of the intensity component Using of gradient Ep at element the motion operator, sentative motor control vector ui is amvfi=p((motionx) ( (u=d)), in the image between the positions of element: i in the direction function for element toward element j. i and j; i; and Ep,ij is the the definition of the amvf associated with the ith repre- where x is the image feature control vector used to control the average value of its argument. steps during which ui was taken. Examples after the learning randomly (ten time steps). agent has wandered that has already been learned the motor apparatus, and ,u is an operator (Section 2.4)) II is the motor that computes In this case, the average value is taken over all time in Fig. 6. These are obtained are shown choosing a representative motor control vector and executing for 20 minutes using the exploration strategy of it for one second 3.3. Apply principal component analysis The goal of this step is to find a basis set for the space of effects of the motor i.e., a set of representative motion vector fields from which all of the motion apparatus, D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 187 110= (-0.66 0.74) Ill= (0.74 0.68) Fig. 7. The first four eigenvectors the space of average motion vector corresponds to a forward robot’s control vectors shown. front sensor. The robot’s motor apparatus translation motion. and the standard deviations of the associated fields. The first corresponds In these diagrams, for principal to a pure rotation motion and the second the the top-left element the motor the first two effects directly using is associated with can produce components vector fields may be produced by linear combination. This type of decomposition may be performed using principal for an introduction. Oja [29] discusses how a neural network can function as a Principal Component Analyzer. Ritter et al. [ 371 show that self-organizing maps seen as a generalization component for a variable y produces (See Mardia et al. [25] of PCA.) analysis of a set of values [ 131 can be component analysis (PCA). unit vectors {v’}, called eigenvectors, ith principal yi. In practice, y may be approximated the remaining component throwing analysis may be performed using a technique the variable y. The Principal of orthogonal sis set for y and eigenvector of the first few eigenvectors while component composition tion of each principal tell how ple values Fig. 7. [35], which important identifies a set that may be viewed as a ba- of y is the dot product of as a linear combination ones away. ‘* Principal called singular value de- the standard devia- of the standard deviations the sam- in are shown in the experiment each eigenvector for y. The first four eigenvectors obtained is for the purposes of approximating the eigenvectors and computes component. The relative magnitudes 3.4. Identify primitive actions In the previous step, principal component by matching analysis was used to determine a basis set the set of eigenvectors. The goal of this step those effects. This is of effects for the motor apparatus, namely, is to discover which motor control vectors can be used to produce accomplished for each i and j, the angle motor control vectors. The matching 0i.i between is defined by the equation cos B,.; = vi amvfj where the vector fields vi and amvfi are treated as simple vectors by vector flattening that and ignoring the positions of the local motion vectors. An angle near zero indicates the eigenvectors with the amvfs of all of the representative local motion vectors into a single nm-dimensional and the jth amvf. This angle their n m-dimensional involves computing, the ith eigenvector ‘*The principal eigenvector components are ordered according to their standard deviations. This means that the first accounts for the most variance in the set of observed values for y, and so forth. 188 D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 ~~~:;:: (-0.17 -0.99 -0.05) (0.03 0.03 -1.00) (-0.99 0.13 0.04) Fig. 8. The first four principal eigenvectors two correspond motor apparatus to pure translation motions and the third corresponds can produce and associated singular values for the roving-eye robot. The first to a pure rotation motion. The robot’s the first three effects directly using the motor control vectors shown. to the eigenvector. An angle near 180 degrees the ith eigenvector and ui- to the opposite of the eigenvector. then action & is defined the amvf is similar is similar within 45 degrees, is most collinear with vector whose amvf assume that uic M -&, Ui d;f ; ( &+ _ u’-). Subsequently, action. The values of z& are shown are shown experiment in Fig. 8. is most anti-linear. that the robot’s motor apparatus they can be approximated indicates that the amvf If any amvf s match the ith eigenvector to to be the motor control vector whose amvf to be the motor control laws (Section 5) of control In the case that ui+ = -&. by plus and minus ui respectively, where is defined I3 The definitions implying is linear, this will be used as the definition of the ith primitive for the roving-eye in Fig. 7. The analogous results 3.5. Dejine a new abstract interface is defined The goal of this step is to define a new interface that abstracts away the details of the motor apparatus. For each of the robot’s degrees of freedom, a new control that degree of freedom. Negative signal the robot in the opposite direction. For the robot of values of the control (left and right) and one for the example, advancing is defined by the following (forward and backward). The effect of the control signals equation: there are two control signals, one for turning for producing motion along to the robot signal move U = u#P + U,Ul, where ~0 and UI (which u1 are the primitive actions corresponding range from - 1 to 1) are the new control signals and u” and to the first two principal eigenvectors. 3.6. Discussion The learning methods described robot for which synchro-drive in this section have also been applied to a simulated the motor control signals directly specify how fast to I3 This matching of anzvfs, criterion is more restrictive than it appears. In a high-dimensional space such as the space it is highly unlikely that two random vectors will define an angle less than 45 degrees. D. Pierce, B.J. Kuipers/Artifcial Intelligence 92 (1997) 169-227 189 respectively. The details of that experiment and tank-style turn and advance, synchro-drive identical effects details of the motor apparatus, providing the robot’s degrees of freedom. The than motor control capabilities. learned rather signals, robots demonstrate abstract in [ 3 11. The are given two different motor apparatuses with in sensory it is grounded is the same for both: the a new set of control signals, one for each of it abstracts away interface, since The learning methods described in the previous learned signals are used in Section 5 to define behaviors section. The result in this section build on the sensory is a new abstract for navigation. image structure interface whose control 4. Local state variables The result of the agent’s learning so far is an abstract interface that includes a model of apparatus. The model of the sensory apparatus represented primarily by the positions of the elements of the is the description the robot’s sensorimotor of its physical structure learned actions motion image feature. The model of the motor apparatus that tells in each. the agent how many degrees of freedom is the set of learned primitive it has, and how to produce behaviors. that gives the distance The agent’s ultimate goal is to abstract the continuous world of the robot to a cognitive places with well- to is viewed as a discrete set of recognizable them. The cognitive map gives the learning agent the ability the world map by which defined paths connecting predict places. Learning the robot through for moving behaviors must be repeatable state always moves few examples of such path-following the robot the cognitive map requires the effects of high-level behaviors and to navigate among a set of recognizable behaviors these its state space. In order to be useful for prediction, in the sense that executing a behavior from a given initial to the same final state. The following paragraph gives a that the agent learn path-following If the learning agent has a feature this feature constant, the feature constant would correspond feature based on the retina from the robot to the wall then it the robot move while keeping and it knows how to make can make the robot follow the wall. For a robot with a retina (Section 2.5), a feature as behavior. simple as the sum of all of the inputs could be used to define a path-following to following Moving while keeping a path of is a line detector, which intensity. A more complex constant could be used as the basis for a line-following compass giving value would move omni-directional that the robot robot’s sensor would decrease with distance behavior could be based on an error signal level detected by the sensor and a nominal value sensor’s behavior. For a robot with a continuous behavior based on the compass’s in a constant direction. Finally, consider a robot with an to a light mounted on the robot and suppose responding is in a dark room with white walls. The amount of light detected by the the light in the middle of the the robot’s heading, a path-following the nearest wall. A wall-following that was the difference between the robot photo-sensor range of values). (e.g., a value from In this section and the next, we describe the following three-step method for learning path-following behaviors: 190 D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 (1) (2) (3) that find a set of features variables and use them to define error signals; for minimizing learn behaviors that move the robot while keeping learn behaviors learning the error signals; and the the errors near zero. agent can control, called local state This section shows how to learn local state variables. Section 5 shows how to use them to define path-following behaviors. What is required of a local state variable is that it be controllable, agent must know how its control signals affect it. A feature the following definition: is controllable i.e., the learning if it meets Definition. Let ii be the vector of control signals u,i. A scalar feature vi is a local state variable if the effect of the control signals on yi can be approximated locally by (1) where mi is nonzero. Determining whether a feature dependent value of mi is the job of the static action model information for this step is the set of learned wanders by using its learned primitive actions. is a local state variable while the context- (Section 4.2). The source of features produced while the learning agent learning the constraint are analogous i = 0 reduces in the following to state variables Local state variables then a state variable, state space by one. If y is a local state variable, the dimensionality reduces the constraint does not have access its knowledge called the robot’s state space. sense. If x is of the robot’s j = 0 reduces of the robot’s motor control vector space by one. I4 In other words, the learning agent local state variables using it does have access. They are in to be defined everywhere the robot’s degrees of freedom by one. Since to the robot’s state space, the dimensionality the constraint of motor control vector space local state variables because they are not required it defines to which then An important may be moved in the definition variables has two components: feature to see if it satisfies feature of local state variables to a target value yi* using a simple control of the homing behaviors is that they are controllable: feature vi law. This fact is exploited (Section 5.2). The discovery of local state (Section 4.1), and testing each generating new features the definition of local state variable (Section 4.2). 4.1. Generating new features If a sensory system does not directly provide useful that are useful. A generate-and-test features, approach using the tank-style mobile robot for local state variables in which that are better candidates features experiment generate following scalar features of the raw sense vector. to it may be possible in the is demonstrated the agent learns new than are the elements I4 If ? = 0, then by Fq. ( I ), ii must lie in the subspace perpendicular to vector rni D. Pierce, B.J. Kuipers/Art$cial Intelligence 92 (1997) 169-227 191 4.1.1. A set of feature generators In this paper, we identify a small set of feature generators for local state variables. Our feature generators for a robot with a rich sensorimotor transformations new features as candidates tially a special case of the functional are appropriate demonstrate, tuses. We do not claim the robots described ments. sufficient for a particular set of environments that this particular set of feature generators in this paper nor that it is sufficient for all robots and environ- that are used to produce are essen- of [ 391. These feature generators and are, as we will appara- for and sensorimotor is necessary apparatus The generated features are based on a set of generic mathematical fields, and vector (e.g., than on a human- list of salient properties of a robot’s environment. The feature generators used constructs fields) rather scalar scalars, vectors, matrices, generated for the experiments l splitter l vmin and vmax described in this paper are described below: takes a vector feature of length n and produces n scalar features. to vector features of length greater apply than 1. They provide two different ways to reduce a vector feature to a scalar feature. l group and image (described in the sensory apparatus. Group and image features are not scalar features and thus are not able to serve as local state variables, but they do serve as the basis for higher-level features that may turn out to be useful. in Section 2) identify useful structure and lmax (local-max) features. A focused-image to image features. They produce field) is a (scalar field is used to mask the scalar field. It can be viewed as (0 or 1) . The focus attention on particular properties of an image, e.g., local minima or for which each element has an associated weight apply feature field, Boolean the boolean feature l Zmin (local-min) focused-image pair where an image weights maxima. l tracker applies to focused-image pairs). From features and produces the focused generator produces one image-element features image-element image produced by the lmin gen- for each local ab- in order feature a form of focus of attention, in the image. The tracker away small changes implements in value and position of an image element that tracks an interesting property of the robot’s environment the tracker (single value-position erator, minimum stracting to produce a feature such as the minimum distance 0 val extracts a scalar value feature from an image-element to a nearby object. feature. the “roving eye” robot, we would augment operators. An interesting appropriate This set of feature generators has proven successful sensors. To handle for features based on a variety of convolution masks and other two-dimensional processing generators of functional [22]. We conjecture broad class of mobile by developing require one or more new feature generators; set. to a generally would converge that a reasonably robots and that such a set of feature generators can be discovered for a small subset of that class: initially, each new robot would the set of feature generators for the robot with a ring of distance this set with generators image- is to define a general set of feature to the small and general set of AM to a sized set of feature generators will apply to learning mobile robots, analogous the performance used by Shen transformations open problem to replicate applicable eventually solutions [39] 192 D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 4.1.2. Generating and testing features The generate-and-test following feature. This feature steps in a continuous is marked as new. process of learning loop. Initially, potentially there is only one feature, useful features executes the the raw sensory (i) Each generator (ii) The features is applied to each new feature to which it is applicable. that were new are marked as old, and the features just generated are marked as new. approaches is an important to learning, controlling concern. Without any constraints, In generate-and-test of possibilities generated on each iteration of the generate-and-test are several ways to constrain search. In the current to set a limit on the number of generations is to limit way where population the search through a large space the number of features loop may grow exponentially. There the depth of the it is possible that are created. A second in genetic algorithms is used to a certain number. This method requires a fitness a search algorithm. One way is to limit of the generate-and-test of new features the breadth of the search. This method size is constrained implementation algorithm, vector Fig. 9. The complete hierarchy of features and generators in the learning agent’s feature-learning process used to produce candidate local state variables. The feature generators are shown in bold face; the feature types are shown in italics. D. Pierce, B.J. Kuipers/Art$cial Intelligence 92 (1997) 169-227 193 tester, as a feature a search space is to limit to tell which members of the population can be defined measure measure third way to constrain learning problem, old feature at each step of the generate-and-test feature reasonably small by using strongly to group features). is limited small and the number of generators learning problem typed generators though the branching are worthy of survival. Such a fitness this has not been done here. A factor. For the feature- for each factor for the is kept is kept generator only applies that are generated process. The branching in two ways: the total number of generators to any given feature that apply this is the average number of new features (e.g., the image-feature 4.1.3. An experiment In the experiments agent’s described in this paper, the combinatorial this, we devised an experiment process. At the top of the figure choosing unit motor control vectors and executing explosion of features form deep but narrow hierarchies with a tractable has not been an issue. The generators the agent explores in which set of features. To study them for one second by randomly for ( 10 time steps) each. Fig. 9 shows the complete hierarchy of features and generators is the raw sense feature-learning the learning from the sequence of feature vector s. We refer to each feature using a name derived tr = tracker). generators to the feature Thus, to the raw sense vector s. The features produced by applying shown in the figure are, from top to bottom and from left to right: s, s-g, s-vmin, s-vmax, so, s1, . . .1 s28, s-g-h, s-g-vmin, s-g-vm, g-im-lmax-tr, robot’s position, Each of the generated tested (Section 4.2) s-g-im-lmin-tr-val (the leaves of the tree of generated there may be multiple scalar features im = image, the umin generator to see if it can serve as a local state variable. s- on the features. is and s-g-im-lmax-tr-val. Notice that feature results or s-g-im-lmax-tr-val (where g = group, s-g-im-lmin-tr-val, feature generator used to produce that, depending from applying s-g-im-bin-tr, for example, s-g-im-bin, s-g-idma, the group s-g-vmin features) 4.2. The static action model for a feature proceeds the behavior of the feature without The purpose of the static action model action is to predict tries to predict Eq. (1) of Section 4) of the static action model learning agent which primitive the feature as a function of the action being then the agent tries to predict If a feature local state variable. simple matter variable moves l5 With the information to define homing behaviors toward a target value. is action dependent the context-dependent and is predictable of the form given (a set of equations in the behavior of each scalar feature. The learning in three steps. In the first step, the into account the behavior of action, effect of that action on the feature. then it can serve as a in the static action model, it is a the robot so that the local state taken. If this fails for a primitive contained for moving in all contexts, taking is being used. If it fails, then it tries to predict I5 One could use a less constrained then predictable it results constraining in a given context, definition because definition of local state variable: it is a local state variable in more robust control for that context. We have chosen laws. if a feature is action dependent and the more 194 D. Pierce, B.J. Kuipers/Artifciul Intelligence 92 (1997) 169-227 Aso vs. At As24 vs. At Emin vs. At Fig. IO. Plots of A!‘! (vertical axis) versus At (horizontal axis). used by the learning agent to try to predict the behavior of feature J’; independently is used, A.v; and Ar are reset to 0 (at the center of each plot). From m,, r;. and yi are computed From unpredictable of the motor control vector. Whenever a new motor control vector the sets of (At, A.v;) points, statistics ri between Ay; and At. are shown are the correlations that features su and lmin (short is barge and r is small) and that ~4 is constant the learning agent concludes for s-g-im-bin-tr-vu/) text). The numbers these statistics (y < 0.001). (see (r When trying to predict the effects of actions on features, for approximately because on the assumption the control linear relationships laws used to define path-following that these relationships are approximately between action magnitudes behaviors linear. the learning agent looks and feature derivatives (Section 5) are based 4.2.1. An action-independent model The first step toward modeling to predict its behavior explores by repeatedly (ten time steps). a correlator. This produces as a function of time to is the last where a primitive independently choosing the behavior of a feature yi is to see if it is possible of the motor control vector being used. The agent it for one second that we call a set of statistics based on the plot of the feature’s value axis is At = t - to for the horizontal the motor control vector changed. The vertical axis gives the behavior of the feature using a device action and executing (Fig. 10). The coordinate time It analyzes Ayi = yi(t> - yi(to>. The statistics are mi, ri, and yi. The value of rni is the slope of the line that best fits the set of (At, Ayi) points. The value of ri is the correlation between variables Ayi and At. The value of yi is the ratio of the standard deviations of Ayi and At. It is a measure of how fast the feature changes are if yi < 0.001. It is increasing defined if any of these properties holds. if ri > 0.6; decreasing Otherwise, the behavior of the it is unpredictable feature using an action-dependent model. in terms of these statistics. The feature is constant as a function of time. A number of properties and the learning agent tries to predict It is predictable if ri < -0.6. for use as local state variables. The rest are candidates the features s-vmin, s-vmax, ~20 (the broken distance sensor), as constant and are thus not and s-g-vmux are all diagnosed for the next step in the For the running example, (the battery voltage), ~24 suitable learning of the static action model. 4.2.2. An action-dependent model If the previous yi, then the learning step failed to produce a model agent uses one correlator that predicts for each primitive the behavior of a feature its to analyze action D. fierce, B.J. Kuipers/Artificial InfelliRence 92 (1997) 169-227 195 As0 vs. uoAt As0 vs. u,At Almin vs. uoAt Almin vs. ulAt Fig. is possible unpredictable is constant Il. Plots of A>‘; versus UjAr for two features and two primitive actions. These are used to see if it sg is the behavior of the feature as a function of the motor control vector. Feature to predict for action un (r is small and y is large) but predictable for action ~1 (r is large). Feature bin for action UQ (y < 0.001) but unpredictable for action UI (r is small and y is large), the relationship selecting primitive actions and executing In this case, the correlator characterizes the statistics mij (the slope of the line effect on the feature. U.iAt and Ayi where At and Ayi are defined as before. The agent continues by randomly computes points), Yii (the correlation deviations’ of u,iAt and Ayi). A feature yi,, < yi/4. The properties U.i are defined as before. For each predictable form between to explore them for a second at a time. It that best fits the set of (UjAt, Ayi) between UjAt and Ayi), and y;j (the ratio of the standard signal u,; if constant for control signal and predictable signal pair, a rule of the is labeled increasing, decreasing, feature-control for control ( Jji = iTIi,jl_.lj ( is added actions, For to the static action model. If a feature is predictable for all of the primitive then the feature itself is predictable. example the running for primitive action ug (rotating). (Fig. 1 l), all of the distance features SO, $1, ~2, ~3, and ~23; to increase sensors are found The effect of 141 (advancing) to be is to features sg through ~14. Its effect for features SJ-sg, s15-s19, ~21, and ~22. The discrete compass sensors for ~1. The features s-g-vmin and for ~1. Feature for both primitive actions. One might is only defined when as for ug. In fact, lmax, which too rapidly with small unpredictable decrease is unpredictable ~25 through s2s are unpredictable s-g-im-lmin-tr-vu1 s-g-im-lmax-tr-val guess the robot constant. that lmax would be constant fluctuates lmin) are constant lmax) for uo and unpredictable for uo and constant to be diagnosed is in a corner, is unpredictable (a.k.a. (a.k.a. turns 4.2.3. A context-dependent model If u,~ has an effect on yi that is unpredictable, partition of sensory space into a discrete set of contexts so that the relationship approximated for local state variable yi and control signal U,i, is an integer-valued for each context. l6 In general, a context feature by a linear equation feature that takes on Z;j, then the learning agent tries to find a can be I6 This approach is analogous to Drescher’s marginal attribution 17 I. 196 D. Pierce, B.J. Kuipers/Artl@ial Intelligence 92 (1997) 169-227 a finite set of values. This set defines a partition of the robot’s state space into a finite set of contexts defined by the predicates Zi, = k. One way to define a context feature into a finite set of intervals, is to first choose a feature x and divide is then defined {Zk}, where each if the by zii = k iff x E Ik. Using value of x is a good predictor of the effect of the control signal nj on the feature yi. TO test the hypothesis can be used 2i.j = k. for the effect of uj on yi, a correlator Uj’s effect on y1 for each context defined by the predicate its range of values its own context. The context feature x to define a set of contexts that x is a good predictor interval defines is appropriate to determine feature each of a large set of features to see if they improve the predictability signal’s effect features is expensive. Heuristics can be used to use in defining contexts. For example, to guide it makes sense the search to first in the sense that are closely related to the feature being analyzed, in the tree of features produced by the generate-and-test together Testing of a control for relevant look at features that they are close process. is implemented: Currently, only one such heuristic then use the position of that element if a feature is based on the value of the a element of an image, feature, context. Since into a discrete set of contexts. For it is trivial example, there are 23 possible positions and these can be used to break sensory space into a partition of 23 contexts each defined by the predicate 2i.j = k where zi,; is an integer feature whose value is between 0 and 22 and identifies there is a discrete set of possible positions the space of possible positions in the case of the fmin and lmax features, the position associated with the local minimum or maximum. in the image for an image-element to define to break For each context zij = k, a correlator is used to try to predict the effect of u,i on yi given while computing that the robot is in that context. The agent continues y;jk. The properties the statistics mijk, r,,k, and to explore consfanf, randomly increasing, decreasing, of the form and predictable are defined as before. For each predictable context, a rule j; z mi.jk U.j t if zij = k is added a “constant primitive is predictable for that action. to the static action model. If milk is 0, then the predicate context” is useful action’s effect on the feature (which for defining path-following is predictable for every context, behaviors). z;j = k defines If the then the feature For the running example, the only features with associated context features are lmin and tmax. (constant) lmin is already predictable The effect of u] on lmin is predictable lmin for contexts O-5 and 19-22, and to increase 6 and 18 (in which (see Fig. 12). The effect of no on lmax is unpredictable The effect of ut is to decrease for contexts 8-16. The effect is unpredictable the robot’s heading for control signal ~0, for every context. Its effect is to decrease it for contexts 7-17. For contexts lmin is constant to the wall), is parallel for almost every context. lmax for contexts O-5 and 20-22 and to increase it for contexts 6, 7, 17, and 18. D. Pierce, B.J. Kuipers/Artificial Intelligence 92 (1997) 169-227 197 -0.985 -” ‘,., ‘Y.,. . A k=O I 0.996 k=12 Fig. 12. Example plots of A>ji versus UJ At for the s-g-im-lmin-tr-val are used to see if it is possible and the current context. For action III. feature and 12, respectively. feature for three different contexts. These the behavior of the feature as a function of the motor control vector lmin is decreasing, constant, and increasing for contexts 0, 6, to predict the only feature and action-dependent that is both predictable At this point a local state variable) where k is the current value of the context the local minimum generator. This generator minimum the robot’s three in the neighborhood range. (and is thus is Zmin. Its behavior can be modeled by the equation pi = rniikut the location of l&n was produced by the trucker lmin features, one for each local feature. The number of local state variables depends on of a corner, but just one if only a single wall is within in the input location. There are two local state variables in the image feature. The feature actually produces multiple feature z.ii that represents in the neighborhood of a T-intersection, image 5. Learning control laws The goal of this step is to learn a suitable set of homing and path-following behaviors the set of local state variables the by for this step are the learned static action using the results of the preceding and the set of primitive actions. Recall local state variables are the lmin features, the learning model that for the robot of the running experiment, identified as controllable (Section 4.2) and dynamic action model agent. The sources of information the only features (Section 5.3.2). specifically, sections, A behavior, as the term is used in this paper, is an object with four components, is a function signals. The upp component is currently called output, upp, done, and init. The output component vector of motor control indicates whether be 0 (indicating in between is applicable) The done signal the behavior behavior has finished. The init signal is an input signal that tells the behavior state information itself (in case it has internal that returns a is a scalar function whose value applicable. The value of this feature may that the behavior less than 100% that the the behavior that the behavior or 1 (indicating a certainty that needs to be reset). is not applicable) or some number that tells when is applicable). is a Boolean to initialize (indicating function Path-following ( 1) continuous (2) behaviors (3) behaviors are learned in three steps: behaviors error signals are defined; for minimizing for moving while keeping are learned are learned the error signals; the error signals near zero. 198 D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 5.1. DeJining error signals for control laws The learning and navigation uses path- an error signal near a agent’s approach in which to exploration, mapping, the robot moves while maintaining to the right side of the corridor behavior based on an error signal following behaviors zero. An example of a path-following person walking down a corridor. The error signal is e = (y” - y) where y is the distance and y* is a constant from the person in the corridor. that depends on the person, his mood, and the number of other people If the error The error signal if it is is positive, negative, he moves by using (i.e., straight) path from one end of the the control the path, he ends up in the same corridor place. to the other, and each time the person follows is used the person moves from the wall) while walking; law is efficient and repeatable: to the right. The control follows an efficient to the left (away law for moving law, the person the corridor. in a control in Britain) involves along (left In this example, y is a local state variable. The agent’s approach to defining path- is to first define error signals of the form e = y* - y for each local following behaviors state variable y. I7 5.2. Learning homing behaviors The purpose of a homing behavior is to move an error signal behaviors based on that error signal will become applicable. While path-following would be possible ior given an error signal done. The homing behavior independent model. control law to use reinforcement-learning methods [24,33], most of the relevant can be defined in Fig. 13, drawing on as an toward zero so that it to learn a homing behav- has already been learning instance of the generic, domain- in the static action the knowledge a rule of the form >ii = miiku,j where For each local state variable yi and control signal u,i, a homing behavior includes the error is close the error e = y: - yi. It is applicable for reducing static action model done when definition characterize The components possible context k (Fig. 13). A homing behavior in Fig. 14. robot is illustrated the effects of u,i on yi. This partition of the homing behavior to zero. Its output mijk is nonzero. is given by a simple control is defined in every context zi,i = k for which the It is law. The to is described by the set of contexts {k}. for each for the mobile that the agent learns (app, output, and done) are defined is based on the partition of sensory space used by the static action model 5.3. Learning path-following behaviors The previous section presented a method for learning homing behaviors a given error signal. In this section, a method is presented that minimize for moving while minimizing ” Choosing learning agent chooses a value equal to half the feature’s maximum value. target value JJ* for a feature y is beyond an optimal the scope of this paper. The implemented D. Pierce, B.J. Kuipers/Artijicial intelligence 92 (1997) 169-227 199 For exh context zii = Ic, where ?Lijk = WJJ -e;+- W2 s e; dt for each feature zij. The applicability is defined the goal vi = $+. The applicability to local state variable ~‘i and for each primitive action Fig. 13. A homing behavior and output are defined as functions of the current context as defined achieve by the context rijk between Uj and j; has a magnitude of 1 .O and a minimum value of zero if the correlation has a magnitude of 0.5 or less. law with parameters 5 = I .O, w = 0.05 (see 12 1 ] ) The output that minimizes is close to zero. The init function resets the value of the integral of the error to zero. the difference between vi and y,?. The behavior has a maximum value of 1.0 if the correlation is given by a proportional-integral is done when this difference (PI) control .j Fig. 14. An example of a homing behavior apparatus. The agent’s static action mode1 predicts the value of local state variable (a) applicable e=\1* is done when y FZ y*. in this context, - ~1, and (c) !‘. This information for the mobile robot with distance sensors and tank-style motor the second primitive action U’ decreases that in this context is used in the definition of a homing behavior that is the error (b) uses primitive action u’ to move the robot so as to minimize the error signal. The result two steps: involves behavior ( 1) learning how to move the necessary (2) learning defined by the minimum is a path-following behavior. Learning a path-following in the general direction feedback for error correction that keeps the error near zero and to avoid straying off the path of the error signal. The learning agent uses its static action model to determine which primitive action to use to provide motion along a path. It learns a dynamic action model to tell how to use the remaining to provide error correction. primitive actions 200 D. Pierce, B.J. Kuipers/Art$cial Intelligence 92 (1997) 169-227 5.3.1. Learning open-loop path-following behaviors The static action model does not give the agent enough information behaviors with error correction, but it does give the agent enough to define closed- in- to define open-loop path-following lacks error correction loop path-following formation behavior which call that the static action model action uj has no effect on local state variable yi. local state variable yi and primitive is in turn useful for defining path-following but is useful For each behaviors. t8 An open-loop path-following for learning action model, behaviors with error correction. Re- identifies constant contexts ZQ = k in which primitive the dynamic context are defined, one for each direction of motion. The for each constant action uj, zi.1 = k, two open-loop behaviors’ outputs are given by behaviors u=up+ c UC& 6 +.i are used in learning where up = &uj and 1 ug ( < 1. The ug components the dynamic action model. The purpose of an open-loop path-following is to allow the agent to learn the effects of the orthogonal control signals on the feature while motor control to use the other control signals vector up is used. t9 With this knowledge, for error correction. The definition of open-loop path-following is summarized in Fig. 15. A behavior behavior becomes applicable the current behavior or start a new one. is done when the robot strays too far off the path or when a new that the agent has a choice to make: to continue it is possible behaviors indicating behavior turning) example, For the mobile robot of the running behavior based on u” (for It is applicable whenever yi = y; since, according has no effect on yi. There (for advancing) is facing parallel has value 6 or 18). Fig. 20(b) as the learning robot’s environment. there is an open-loop path-following for each local state variable yi (see Fig. 16(a)). turning behavior based on u’ the robot feature zij shows a trace of the behavior of the robot that results the for each feature y; (see Fig. 16(b) ). It is applicable when to the object being detected by yi (that agent uses its learned open-loop path-following is also an open-loop path-following to the static action model, is, when context to explore behaviors 5.3.2. The dynamic action model The static action model predicts the context-dependent effects of a control signal on the local state variables. The dynamic action model predicts the context-dependent effects of control signals on the local state variables while an open-loop path-following behavior is being executed. The dynamic action model tells, for each open-loop path-following of each orthogonal base action), on the local state variable action (each primitive action other than the path-following the effect behavior’s that is used in the definition of the path-following behavior, control I* In a closed-loop minimizes I9 The primitive actions are orthogonal other (see Section 3.3). that error. law, an error signal is used as feedback to determine a motor control vector that to each other in the sense that their amvfs are orthogonal to each D. Pierce, B.J. Kuipers/Art@ciul Inrelli~ence 92 (1997) 169-227 201 app = Iv,* - Yil YT output = u~+~usu* J#.i < 0.1 A zzj = k done = Iv,’ - Yil > 0.4 Y;” V (a new behavior becomes applicable) behavior that ufi maintains vi constant according is defined for each local state variable vi, for each primitive (or opposite) up, and for each constant context z;j = k. The predicate zij = k defines a constant context is applicable Fig. 15. An open-loop path-following action if it implies when small orthogonal every 3 seconds. Only one of the us’s is nonzero at a time. The behavior large or a new behavior becomes applicable. the error signal ?I,? - vi is small. The output has two components: learning of the dynamic action model, the orthogonal component changes is too to the static action model. The behavior a base motor control vector and a is done when the error signal component. During (a) (b) J’; = J),: is applicable whenever vi = j~f since no never changes Fig. 16. Two examples of open-loop path-following constraint based on primitive action u1 (advancing) is parallel robot’s heading r = y,* - >I, near zero. behaviors. (a) A behavior based on u” (for turning) and the value of yi. (b) A behavior and constraint >‘i = Y,? is applicable whenever >jr = )I,? and the the error in this context ut keeps to the wall on its left (i.e., z;j = 18) since the dynamic action model, an exploration behavior is chooses applicable homing and open-loop path-following it is no longer applicable, regression is used actions us and the features yi in the context of running behavior based on feature yi, motor control vector UP = fuj, regressors linear to learn or a new path-following the relationships behaviors. behavior the the open-loop path- and context test the hypotheses >;i = mjjk&lu,j and between error signal. To learn runs until applicable. Linear behavior’s used that randomly A behavior becomes orthogonal following Zi,j = k. While Yi ” = mi,jkszUs by computing and lr;jkat ) > 0.6, then the rule it is running, the correlations rijk& between ~8 and yi(“). If ri,jks] > rijkS2 1 ji = mijkstua, if 2i.j = k A u = +d + U& 1 202 D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 O,O,O,l,l 0,0,0,1,2 0, 13% O,l 0,1,6,0,2 the relationships measured by the linear regressors the dynamic the plots give the values of i, j, k, 6, and n, where n is the number of the used in learning illustrating Fig. 17. Plots action model. The labels under derivative of y, being while an open-loop path-following (instances of thin) produced by the tracker generator. The second on )itr and yi) respectively while an open-loop 74),1 = 6. This is the context the robot heading parallel path-following in which tested. The first two plots show the effect of U’ (advancing) on ~‘0 and .$ij respectively behavior based on u” is executed. Here ~‘0 is one of the local state variables two plots show the effect of u” (turning) in context behavior based on U’ is executed to a wall on its right. is added to the dynamic action model. Otherwise, if Ir+s2/ > 0.6, then the rule Yi = mijk82& if zij = k A II = z&j + U& action model. Otherwise, the relationship between ug and yi is to the dynamic is added either zero or unpredictable. that the mobile Suppose 2o robot of the running experiment has a wall to its left and tc’ the signals changes is parallel the distance path-following to the wall (Fig. (mijk = 0). Therefore, to the wall, vi, constant the effects of other control I6( b) ) . In this context, primitive action that its heading (advancing) maintains behavior based on u1 and yi is applicable. While executing open-loop In this this behavior, ji = rnit~,2uc. This is because example, uc affects to the wall and this direction turning the wall as it advances. determines Examples of the linear regressors used to learn the dynamic action model for the robot action of the running effect on y; while any of the open-loop behaviors based on model, u” has a predictable behaviors based on uO, the effect of U’ is executing. For the open-loop path-following U’ on vi is unpredictable. the second derivative of the feature: relative the robot’s direction of motion (i.e., ug) can be diagnosed. how fast the robot moves in Fig. 17.21 According example are illustrated toward or away from to the dynamic 5.3.3. Learning closed-loop path-following behaviors The final step the open-loop in learning path-following path-following behaviors behaviors is to add error correction to in order to define closed-loop path-following 2o For the dynamic action model, Informally, Together, these give it is necessary to consider both first and second derivatives of the features. that is, tti,j = thjsus. rule and the fact that Uj is constant the product ji = th;jnj, .v; = t’ijuj = rnjsusuj = mijs2us. using this is because us may affect the derivative of Wr;j in the equation for a path-following *’ The linear regressors operate on filtered versions of ~1; and nj to remove noise that would otherwise hide the relationship the signals. The signals are filtered using a moving average taken over several seconds. behavior. between D. Pierce, B.J. Kuipers/Arti$cial Intelligence 92 (1997) 169-227 203 Fig. 18. Defining closed-loop add error correction behavior. path-following to an open-loop path-following In this example, a small turning motion behaviors. The learning agent uses the dynamic action model to behavior in order to obtain a closed-loop path-following is used to keep the robot on the path as it advances. that primitive action U’ leaves feature y; (the distance is one that receives it uses to modify the case where from the environment feedback its motor control signals so as to a agent knows, because of its static action is facing parallel the robot again the learning behaviors. A closed-loop behavior in the form of an error signal which to minimize the error. Consider wall on its left. In this context, model, constant. Moreover, signal ua (turning) sufficient along wall), in Fig. 18). Because of the error correction path-following small perturbations themselves. the wall. If yi goes below then the agent knows to increase is robust behavior to define a closed-loop path-following taken. Together, to the wall) approximately the agent knows, because of its dynamic action model, how control is affects yi while u ’ is being the robot if the robot gets too close to the to turn right as shown using control signal ~0, the apparatus, in the action models the value of ua (i.e., implemented in the shape of the wall, and even inaccuracies in the face of noise its target value (i.e., that robustly moves in the sensorimotor this information behavior template behavior is defined using the generic A closed-loop path-following in Fig. 19 for each constraint y = y*, for each primitive action or opposite up = &uj, and for each constant context z = k. The predicate z = k (where z is a vector of context features zii if for each z,j E z and and k is a vector of context values ki) defines a constant context ki E k, Zi,, = ki defines a constant context to the static action model. The variable vector up is used in context k. The behavior are near their target values (i.e., y z y*) and when z = k indicating model predicts behavior that the agent now has a choice-to choose a new one. For the example lijkarl is the correlation between u6 and yj”’ while motor control is applicable when all of the elements of y that the static action that motor control vector UP keeps the error vector y* -y near zero. The is done when a new path-following continue behavior becomes the current path-following the set of path-following for y; and WI according contains behaviors behavior or to applicable behaviors indicating robot, for in place as well as for following walls. For the behavior based on u1 (advancing), turning is predictable the effect of the orthogonal primitive action u” on the local state variables and thus it can be used for error correction. For the behaviors based on u” (turning), no error correction is used since the effect of U’ is unpredictable. 22 learning agent ** The implemented learn a context-dependent effect of U’ could become predictable control law. dynamic action model for each open-loop path-following static action model. An extension would be to In this way the behavior. learns a context-dependent and the action could be used for error correction in a context-dependent 204 D. Pierce, B.J. Kuipers/Art$cial Intelligence 92 (1997) 169-227 app z vyj E y : IYi’-Yil ~ Yi’ < 0.1 > A vz;j E z : (Zij = ki) output = up + c 7&j u6 J#j done = 3yi E y : IYi’ - Yil > 0,4 ( Yt > V (a new behavior becomes applicable) vhere us = c 2% Y,EY 2cw -ei+- mijk61 cd2 - mijk62 W2 mijkdl s ei dt ei + 2cw - mijk62 ei 0, otherwise Yt - Yi. IL& = U& = U&i ei = = if bijml > hj~~1, 0.6 if hjk621 > hjkdll, 0.6 vector of target values; u fi = fuj behavior. Here, y is a vector of local state variables v;; y* Fig. 19. Definition of a closed-loop path-following z is the corresponding vector of is a vector of context to the context values k;. The equation static action model. The values of ,nijksa and rijksjr are taken from the dynamic action model. Simple PI and PD (proportional-derivative) the primary effect of us is on j; or y;, respectively. Again, 5 = I .O, w = 0.05. are used (see [ 2 I] ) depending on whether is one of the primitive actions or their opposites; local state variable y;; and k is the corresponding in which up maintains y constant according features Zi. one for each z = k defines a context controllers 1 (a) t (b) a simple world at three randomly while Fig. 20. Exploring applicable learning a model of its sensorimotor the dynamic homing and open-loop path-following action model. (c) The robot explores by randomly choosing applicable homing and closed-loop path-following behaviors based on the dynamic action model. apparatus. behaviors based on the static action model while learning (b) The robot explores by randomly (a) The robot wanders levels of competence. choosing D. Pierce, B.J. Kuipers/Arrijicial Intelligence 92 (1997) 169-227 205 Fig. 20 shows the behavior of the robot at three different stages as the agent behaviors. Section 6 demonstrates the learning and path-following behaviors for a rectangular environment learns of the set a containing and a T-shaped environment. in this section are used as the basis for an exploration In Section 7, the path-following and mapping the agent to develop a discrete abstraction of the robot’s continuous the set of path-following of homing number behaviors strategy world. of obstacles learned that allows 6. Additional experiments the sensorimotor sections have demonstrated The previous to learn agent may use hierarchy. The purpose of this section addition and some els. to those described of the methods limitations in the previous and control is to describe sections) a set of learning methods levels of the spatial a number of experiments that demonstrate that a learning semantic (in the generality lev- and control for learning the sensorimotor The learning methods are first demonstrated for the mobile robot in a cluttered that the learned model of the sensorimotor in which environment the model was learned, after its control-level the control level and demonstrates room. apparatus applies be- the learning agent learning has been a set of learned path- level ap- is agent to demonstrate environment in which that the learning of the control the learning it was learned, to an empty room where it again demonstrates the learned path-following to demonstrate the particular environment to a new, T-shaped it re-learns behaviors. Finally, the particular Then, yond is transferred erased. Here following plies beyond transferred behaviors. Sections 6.4 and 6.5 describe failed and explain why generator methods which the image-feature structure of the ring of distance the learning ways in which ways in which two experiments they failed. Section 6.4 describes fails to produce a ring-shaped in which various of the learning in of the in which the the learning methods can fail. Finally, Section 6.7 identifies a number of the learning methods can be improved. agent fails to discover any local state variables. Section 6.6 summarizes sensors. Section 6.5 describes an experiment an experiment representation 6.1. A cluttered room The environment used in this experiment is a rectangular room with dimensions six meters by four meters, containing lated mobile robot used tion 2.1. throughout four rectangular obstacles this section is the same as that described (Fig. 23). The simu- in Sec- 6.1 .I. Modeling the sensory apparatus The first step in modeling the robot’s sensory apparatus is to apply the group-feature generator. The learning agent computes distance metrics dl and d2 after wandering 20 minutes. Their values are qualitatively for in Fig. 2. The group- to those shown similar 206 D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 ,,a . (._,,i 0.69 0.731 -0.73 0.69) Fig. 21. Example amvf s and associated motor control vectors for the cluttered-room experiment feature generator in modeling learning wandering are qualitatively identifies the robot’s sensory apparatus the same groups as those in Section 2.3. The second step generator. The is to apply the image-feature agent computes distance metric dt for the group of 23 related sensors after for 40 minutes. 23 The outputs of the metric scaling and relaxation algorithm similar to those shown in Fig. 3. 6.1.2. Modeling the motor apparatus The first step in modeling the robot’s motor apparatus is to characterize the ef- and the learning time steps). The first four eigenvectors fects of each of a large set of representative motor control vectors. In this experiment, are chosen. Eight exam- 100 representative motor control vectors of unit magnitude in Fig. 21. These ple amvfs their associated motor control vectors are shown choos- repeatedly for 60 minutes, were obtained while agent wandered it for one second ing a representative motor control vector at random and executing (ten analy- component produced by principal to a pure rotation motion and sis from are shown two motor control vec- the second tors None of the other eigenvectors match any of the amvfs. Notice actions is analysis the above primitive dom. here very closely match the robot’s motor apparatus actions can be used in Fig. 7. The result of the that for each degree of free- translation motion. The are shown under the two principal that in Fig. 22. The first corresponds eigenvectors. the primitive two degrees of freedom to a pure actions identified that to produce motion those shown as primitive corresponds identified and has long wandering periods so that the robot adequately explores room, shorter periods were used because the group- and image-feature 23 We use fairly tered, rectangular converged. See Section 6.7 for a discussion they have converged. of improved feature generators its state space. For the unclut- generators quickly detect when that automatically D. Pierce, B.J. Kuipers/Artifcial Intelligence 92 (1997) 169-227 207 1.40 1.10 0.27 0.22 \ 1 u"= (-0.706 0.707) ul= (0.728 0.683) Fig. 22. The first four eigenvectors and the primitive actions for the cluttered-room experiment. behaviors. When no other behavior the robot while the learning is applicable, agent randomly it randomly in the middle. The learning agent begins by using a homing behavior taken by Fig. 23. The path path-following robot is initially obstacle and then using a path-following are the results of homing behaviors behavior to move along it. The diagonal that move the robot from a wall to a path. learned homing selects and selects primitive actions. The the long in the comers to move toward trajectories 61.3. Learning behaviors As described in Section 4, the learning agent as local state variables identifies (they are the only generated the set of local-minimum features (s-g-im-lmin-tr-val) features that are identified as both action dependent and predictable). The learned static and dynamic action models are qualitatively (turning) in Sections 4 and 5. In the first can use open-loop It uses this information a trace of a random behaviors. path-following this experiment, primitive action behavior based on the second the learning agent again discovers for error correction while executing (advancing) to define closed-loop path-following exploration behavior demonstrating the learning that similar to those learned it an primitive action. behaviors. Fig. 23 shows learned agent’s 6.2. Re-learning the behaviors in a T-shaped room For this experiment, the robot was moved to a T-shaped environment (i.e., static action model, dynamic action model, and learned behaviors) was erased. Its task was to begin with an apparatus and learn an appropriate set of homing intact model of the robot’s sensorimotor the cluttered learning agent’s control-level the learning room from and 208 D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 Fig. 24. Re-leaming behaviors in a T-shaped room. Path-following behaviors based on the advancing primitive action produce the straight-line trajectories that are parallel to the walls. Path-following behaviors based on the turning primitive action leave the robot in the same place while changing the robot’s heading. The homing behaviors based on the advancing action produce most of the rest of the trajectories shown in the picture. A few of the trajectories are produced by a random wandering behavior that is used whenever none of the other behaviors are applicable. (The learning agent selects its behaviors stochastically and occasionally selects a random wandering behavior even when other behaviors are applicable.) behaviors. The environment and path-following corridors long and 1.5 meters wide. The shorter corridor wide. to form a T. The corridor connected used in this experiment forming consists of two the top of the T is 6 meters long and 1.5 meters is 4.5 meters The learning agent successfully learns the open-loop behaviors. Fig. 24 shows a trace of a random exploration learned behaviors. This experiment model of the sensorimotor applicable demonstrates that were in the second environment. apparatus and closed-loop path-following behavior demonstrating the that both the set of features and the are learned in the first environment 6.3. Using the behaviors in an empty room room For this experiment, the robot was moved from the T-shaped environment rectangular of the robot’s sensorimotor Fig. 25 shows a random exploration behavior demonstrating do not apply only 6 meters by 4 meters). The learning to an empty agent’s model apparatus and its set of learned behaviors were left intact. that the learned behaviors in which they were learned. to the environment (of dimensions 6.4. A long and narrow room This experiment demonstrates not produce a ring-shaped The environment is six meters representation used in this experiment an instance in which the image-feature generator does of the structure of the ring of distance sensors. room. The room is a long, narrow, rectangular to confuse long and one half meter wide. This environment was designed D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 209 Fig. 25. Using the learned homing and path-following behaviors in an empty room. the image-feature on opposite sides of the ring are often similar: the long walls of the room, then the sensor opposite on the opposite On the other hand, if a sensor returns a large value, then there is a good chance sensor opposite generator. Since the room is so narrow, the values of distance sensors to one of to the wall than 0.5). that the to it detects the distance (less side of the room. Both sensors produce a small value to it will also return a large value. If a sensor detects the distance If opposite sensors return similar values, on average, then the image-feature will place image feature will capture them close together in the image feature. It is unlikely, the ring structure of the array of distance sensors. generator in this case, that the together. The outputs of the metric scaling and relaxation is the same as before: The distance sensors algorithm are scree diagram on the left, the structure 64.1. Modeling the sensory apparatus The result of the group-feature generator is best captured by a four-dimensional of points to the metric-scaling are all grouped shown in Fig. 26. According of the array of sensors no arrangement any two points approximates distance metric dl. The middle of the set of points generated by the metric-scaling shows the results of the relaxation the ring of sensors are close together algorithm. 24 Notice in the image. 2s in fewer than four dimensions the distance between corresponding figure below shows the projection onto two dimensions algorithm. The figure on the right in that are adjacent that sensors representation-there for which the distance between sensors as measured by is 6.4.2. Modeling the motor apparatus eigenvectors The first four principal are shown in Fig. 27. The method actually identifies for the space of average motion vector fields the turning motor control vector 24 The metric-scaling algorithm, the relaxation algorithm, and the definition of the image and motion features can all handle images of arbitrary dimension. However, in the current implementation, we have constrained the image feature to be two-dimensional. A goal for future research is to remove this artificial constraint and test the methods on sensory arrays that ate genuinely three-dimensional. 25 Though the results are not shown here, we have also run the relaxation algorithm for this distance metric in three dimensions. In that case the resulting pattern of sensors resembles the pattern of stitching on a baseball. 210 D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 ytric scaling eigenvalues i: 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 12345678910 2 122 0” I6 8 II I2 ld6 IX I8 Iis 147 s4; 9 8 7 21 22 23 0 I 4 3 2 6 5 ‘O II I2 I3 I4 IS I6 17 ‘8 I9 Fig. 26. The outputs of the metric-scaling and relaxation algorithms for the narrow-room experiment u”= (-0.723 0.680) u’=(-0.532 -0.837) u2= (-0.877 -0.481) Fig. 27. The first four eigenvectors and the primitive actions for the narrow-room experiment. correctly. The second primitive action cant turning component The second and third primitive vector that produces a pure advancing motion. to it. The method erroneously is primarily an advancing identifies actions are both poor approximations action but has a signifi- three primitive actions. of a motor control 6.5. A circular room an instance demonstrates This experiment in which cover any local state variables. The robot’s environment in diameter. The results of the learning of the sensorimotor by the set of principal eigenvectors ing agent fails happened. local state variables. The following actions corresponding two primitive to discover identifies any the learning is a circular agent fails to dis- room three meters apparatus are summarized in Fig. 28. The learn- but this to turning and advancing, explains why analysis and primitive actions shown For a feature to be a local state variable, For a feature to be predictable, for all possible contexts. predictable. the feature must be known environments, nearby objects or walls) were identified what was learned by the learning agent (and represented the robot in the rectangular local-minimum environment: features the it must be both action-dependent the effects of the primitive In the rectangular and actions on and T-shaped the robot [which give distances to as local state variables. Here is a summary of for in the static action model) from D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 211 PlFlFl_C u” = (-0.7460.659) u’ = (0.7650.632) Fig. 28. The first four eigenvectors and the primitive actions for the circular-room experiment. l The first primitive action effects of the primitive (turning) does not affect the local-minimum features. The action are thus predictable for all contexts. l The effect of the second primitive action (advancing) is context dependent: - When the robot is facing toward a wall, the primitive action reliably decreases the value of the local-minimum feature. - When the robot is facing away from a wall, the primitive action reliably increases the value of the local-minimum feature. the primitive to the wall (in either direction), the learning agent’s learned static to that described above, but with the following exception: When to the wall, the effect of the second primitive action on the is facing parallel feature for the difference. When to a straight wall, a robot can move for many steps without changing for the linear regression to the wall significantly. This is why it is possible is unpredictable. Here is an explanation is facing parallel - When action the value of the feature constant. (with the circular environment), the robot leaves For this experiment action model is identical the robot local-minimum facing parallel the distance tester that analyzes good approximation, robot can only advance a few steps without changing conclusion that the learning that the effect of advancing agent is unpredictable the effect of the primitive zero in this context. action that its effect is, to a the to the wall. The only is regression tester is able to draw from the linear in this context. In the circular world, on the other hand, the distance to conclude 6.6. Failure modes Sections 6.4 and 6.5 gave two examples of cases in which to learn a set of homing exhaustive The next section discusses how the learning methods may be improved. the learning methods described list of ways in which and path-following the learning agent failed behaviors. This section provides a more in this paper can fail. Modeling the sensory apparatus If there is no structured array of sensors, only small or singleton groups and the image-feature is an array of sensors but the sensors do not adequately the environment, of the structure of those sensors. For example, then the values of adjacent then the group and image features will fail to produce a representation sensors may not be similar enough if there are only four distance sensors, for the group-feature then the group-feature generator will produce generator will not apply. If there sample a continuous property of 212 D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 them together. to group generator does not adequately feature generators, reflect the structure of the sensory apparatus. then the measured the environment explore inter-sensor If the environment before applying is large and the learning agent the group- and image- distance metrics may not accurately Representing motion The motion-feature generator case here, or given a priori by then successive meaningful the motion-feature image-feature results. requires the robot’s designer). an image generator will not apply. If the robot’s motion values are unrelated, (either feature as is the feature, is so fast that then the motion feature will fail to produce learned, is no image If there Modeling the motor apparatus that process identifies primitive (i.e., motor control vectors The matching identify a primitive whose amvfs match action long enough and the values of the amvfs are still fluctuating with time). This is one possible explanation (i.e., if the learning agent has not wandered if the amvj’s have not converged just two primitive actions the principal eigenvectors) can fail to correctly in Section 6.4.2. for the failure to identify actions Generating candidate local state variables the definition of local state variable is not general enough. In such a case, none of the generated The discovery of local state variables may fail if the language of features and feature scalar features in described is too gen- explosion In this paper, we identified a small set of feature generators and then demon- and sensorimotor if the language of features and generators agent will quickly become bogged down in a combinatorial features. generators would satisfy Section 6.5). On the other hand, eral, the learning of mostly useless that are appropriate strated apparatuses. for a robot with a rich sensorimotor for a particular apparatus set of environments that they are sufficient (as in the experiment Learning action models The learning agent will fail to correctly long enough it does not explore for the linear-regression case that the learning agent must learn the relationships and a feature for a large number of contexts, experiment with the motor control vector in each of those contexts. the method requires learn the static and dynamic action models if calculations In the between a motor control vector that the learning agent to converge. Learning path-following behaviors The learning of path-following If none of the primitive behaviors can depend on the set of learned primitive any of the local state actions can be used to maintain actions. variables constant, then no path-following behaviors will be learned. In the experiments described of the preceding methods, which means failure of a preceding method. This observation, the methods are interesting methods short. First, in this paper, each learning method builds on the results is the that one source of failure for a method if left unqualified, in their own right sells the learning (for example, the D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 213 modeling of the motor apparatus could be applied was given by the robot’s designer nature of the learning is partially example, success of the image-feature of generate and test. generator but is instead the discovery than being rather an artifact of our particular learned). Second, to a sensory system whose structure the sequential learning problem. For the the result of an independent process in general, depend on of local state variables does not, 6.7. Future work Section 6.6 identified a number of ways in which the learning methods can fail. This section provides suggestions for improvements to the learning methods. Improved feature testers One way that several of the learning methods can fail is by jumping if the group-feature then the output of the generator may be incorrect. to a conclusion generator uses a distance metric before If then the model of the actions are identified before the amvfs have converged, prematurely. For example, the distance metric has converged, primitive motor apparatus may be incorrect. In these examples, the distance metrics to characterize that are used testers-features lem of drawing premature conclusions is meaningful. output value. For example, tester’s output fluctuating. It can do this by providing the confidence slowly) (changing is stable and the amvfs are examples other features. A solution of feature to the prob- is to have each feature tester tell when its output a measure of confidence to its level for a tester might be close to 1 if the is still and close to 0 if the tester’s output in addition It uses a separate tester, the confidence level should be a function of the set of for example, how the static action model uses linear tester for each (feature, primitive level level interval26 then the confidence tester based on that context should be zero. The confidence in terms of the 90% confidence linear regression tester might be defined triple. If the robot is never in a given context, the input variables. The smaller confidence level. Associating steps listed in the previous interval, the confidence the levels with features could the chance section by reducing inaccurate or incomplete models. For a linear regression it has received. Consider, testers. inputs regression action, context) for any linear regression for a linear regression for the correlation between greater improve all of the learning of producing the tester’s confidence An improved static action model learning agent uses behaviors-behaviors The following variable constant. based on primitive actions. according path-following In the current 26 See, for example, [ 15, p.4151. the static action model that move implementation, to define a set of open-loop path- a local state behaviors are the robot while maintaining open-loop path-following to the static action model, it can be used as the “base action” If a primitive action maintains then a local state variable constant, behavior. Using only primitive actions as base actions is a limitation for a of 214 D. Pierce, B.J. Kuipers/Artijicid Intelligence 92 (1997) 169-227 the current improved vectors, not just more path-following path-following component implementation. The method for learning path-following if the static action model could predict the primitive behaviors actions. With a more comprehensive could be defined. For example, behaviors would be the effects of arbitrary motor control static action model, room, a in the circular behavior could be based on a motor control vector with a large advancing and a small turning component. to improving One approach the static action model would be to discretize the space into a set of representative motor control vectors and then of all motor control vectors actions. Another approach to learn models of all of these instead of just would be to use a neural network effects of arbitrary actions. The network could then serve as the static action model and could be used to find base actions [ 121 to learn to predict the context-dependent for path-following the primitive behaviors. Reinforcement learning It may be possible to use reinforcement behaviors without and path-following models. An advantage of such an approach model of the sensorimotor difficult action models to train more than one behavior at a time [44] whereas for a large number of features simultaneously. apparatus has been learned. A disadvantage learning [ 3,23,36,42,45 ] to learn homing the need for the primitive actions or explicit action that a particular is that it does not presume it is possible is that it is to learn Learning composite primitive actions Consider a robot that is capable of rotating and advancing that is always oriented in identifying sensors distance Section 2 will succeed steps of Section 3 will succeed one direction, Section 3, as currently actions since and one for translating in identifying the robot is not capable of directly and that has a ring of in the same direction. The learning methods of the structure of the ring of sensors. The first three in direction. The fourth step of two corresponding in two directions. two basic motions: one for translating in a perpendicular translating primitive implemented, will fail to identify to extend (action sequences). the learning of primitive This suggests a topic for future research: actions to allow for composite actions In the example of the robot with the fixed sensor ring, a primitive action could be composed of a turn to a particular direction to learn followed by an advance. An alternate first homing and path-following learning primitive to learn a model of a sensorimotor effect on the sensors. behaviors directly using reinforcement illustrates section: learning without action has no immediate for which an important that it is more difficult is that of the preceding actions. This example apparatus solution 7. From continuous world to finite-state world The learning agent has made the transition to local state variables and high-level behaviors spatial semantic hierarchy). The goal of the next step is to abstract from the continuous sensorimotor sensorimotor from raw senses and motor control vectors level of the apparatus by defining (which comprise finite sets of to a discrete the control apparatus D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 21.5 vi2 0 v8 I I a b apparatus the view name Fig. 29. A demonstration sensorimotor provides and path-following (~(1 > 0). The solid arrows During of the discrete abstract to select appropriate behaviors interface of the discrete the room. At each step, the interface homing behaviors. The dotted arrows represent behaviors based on left turn motor control vectors represent behaviors based on forward advance motor control vectors (U 1 > 0). in the figure on the right. state, and a finite set of applicable interface. We used the abstract the 12 unique views shown to drive it around this exploration, (e.g., VI) that the current the robot identifies identifies and actions. The source of information for this step is the set of learned behaviors views (including the knowledge of when each is applicable). For any given state of the robot, there is a finite set of homing and path-following the corresponding from sense vectors running terminate apparatus. Exe- behavior until it terminates. behaviors. These behaviors are the actions of the discrete sensorimotor cuting one of the actions involves The set of states in which actions mapping is implemented if their Euclidean is new then a new view matches one previously vector. this mapping to be similar sense vector is created and associated with it. If the current sense vector seen, it is associated with the same view as the previous sense to symbols called views. In our experiments, two sense vectors are judged If the current is also finite. These states are named via a using a matching predicate: than a small constant. distance is less a homing behavior two path-following This interface abstracts (u2), leads action is executing, identifies the first l-degree-of-freedom behavior interface Fig. 29 demonstrates the only available minates v3 where ing two plicable. Choosing during around Using ~6. The robot’s experience schemas. This knowledge chy. a user-guided the room the interface the current view and lists this interface. the (ul), Initially terminates, from continuous the wandering behavior this behavior time to discrete is undefined. When the current is the wandering behavior. When is applicable. Selecting behaviors based on uo (turning) time. While a path-following the behavior set of applicable behaviors. no wall is within sensor range and ter- to view leads are applicable. Select- this point, are ap- the behavior of the robot rest of the exploration corner. to the southeast to view that it has returned as a collection of (x, Aj, Vk) triples, called is the basis for the causal level of the spatial semantic hierar- to 06. The figure shows that eventually it to ~12. The the robot the learning agent recognizes behaviors based on ul to view ~5. At is represented path-following (advancing) it again returns leads leads the first leads exploration (not shown) its matching predicate, to view ~4. Selecting 216 D. Pierce, B.J. Kuipers/Artifcial Intelligence 92 (1997) 169-227 8. Learning the topology of the environment In Section 1.1, we described control, causal, levels: sensorimotor, learning agent that has learned in Sections 2 and 3. The control was learned be used to learn the remaining topological, the spatial semantic hierarchy, which is comprised of five a level was learned in Sections 4 and 5. The causal level learning could the first three levels: The sensorimotor and metrical. We have demonstrated level was learned levels of the spatial semantic hierarchy. in Section 7. We now describe how the result of the agent’s The robot’s path-following behaviors constrain its motion to a one-dimensional sub- is the basis for an abstraction as a graph (a set of nodes and a set of edges connecting together). The edges correspond space of the robot’s complete state space. This 1-D skeleton of the robot’s environment nodes produced by path-following minate, the agent stops to choose one of the currently construct that is, states where a new path-following to paths-trajectories this graph. behaviors. The nodes correspond the in the robot’s state space to states where paths ter- and applicable paths. The agent’s goal is to behavior becomes applicable In the case where views uniquely identify states, the problem track, for each state it has seen, of all the actions applicable is straightforward. The at that state. the edge there until or randomly) it from view K to Vk, it adds (intelligently time agent keeps Each (F, Al, V,) to the graph. It continues are no state-action it takes an action, Aj, to explore pairs that it has not explored. that takes exploration idea: If the the information In the case that views do not uniquely identify states, a more sophisticated is required. Such strategies are generally based on the following identify the current state, the agent supplements sense vector with the sense vectors of nearby states. With enough area, the current state can be uniquely information to traverse each path. With can be added to the topological this information, navigation by record- including identified. representation strategy current view does not uniquely current about the surrounding Finally, metrical ing the time shortest-path planning taken To summarize, the learning is possible. sensorimotor a continuous abstracting with a finite set of sense values and actions. Understanding difficult. Our learning agent demonstrates a continuous world has been extensively to the problem of understanding studied (see Section 9.1). agent has made a critical change of representation to a discrete by apparatus a continuous world is very sensorimotor apparatus a way to reduce the problem of understanding a discrete world, a problem that 9. Related work The work mentioned in this section deals with for predicting model of an environment. A complete model of an environment the input/output sufficient the sensory of actions. partial model may be learned. input that will be received In some cases, from the environment a complete model learning behavior of the environment, is impractical, in response i.e., for predicting to any sequence in which case a the general problem of learning is a description a that is D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 211 Methods for learning a model of an environment into two types: those that deal with finite-state worlds and those that deal with continuous worlds. Examples type are given of the first type are given in agent can abstract a Section 9.2. Our contribution learning methods robot’s continuous world may be applied. in Section 9.1. Examples of the second to show how a learning to a finite-state world to which finite-state can be divided has been 9.1. Inferring the structure of jinite-state worlds The task of inferring the structure of a finite-state environment is the task of finding a captures behavior, automaton its actions, that accurately is NP-complete the input-output it has been shown that finding [ 1,9]. With active finite-state behavior of the environ- ment. In the case that the learning agent is passively given examples of the environment’s the smallest automaton consistent input/output learning, with the behavior tively chooses tractable. Kuipers TOUR model, a method for understanding cognitive maps. Dudek et al. [ 81 generalize Kuipers and Byun’s for discriminating topological map-learning algorithm using active experimentation cal states. Angluin [ 381 improve on Angluin’s and passively algorithm start state after each experiment). the agent ac- the discrete spatial worlds based on a theory of for identi- received counterexamples. Rivest and Schapire [ 21 gives a polynomial-time and provide algorithms the problem becomes that does not require and give a version the reset operation [ 18,191 strategy [ 171 describes perceptually (returning in which to the Dean et al. [5] have extended Rivest and Schapire’s The key to their method is probabilistic. that actions are deterministic FSAs. They assume states to senses uncertainty washes out. Dean, Basye, and Kaelbling techniques for a variety of stochastic employs sensory partial knowledge learning method called marginal than state transitions a statistical effects of actions rather automata. Drescher’s theory is “going to handle stochastic but that the output function mapping the [6] give a good review of learning [7] schema mechanism attribution. Schemas emphasize for representing and are ideal in circles” until Wei-Min Shen’s LIVE system in stochastic worlds. [40] (and experimentation) from experience learning nition When experiments to refine the environment the boundary learns the structure of a finite-state environment discrimination concept defi- between positive and negative examples of the concept. it. His complementary to a hypothesized within is only partially observable, LIVE uses locally distinguishing to test the hypothesized properties of unobserved algorithm exploits observed counterexamples A primary focus of the work of Shen and other constructive is the learning of new features. At this level of description, [ 10,28, our approach and in terms of the actual methods used and the domains of 39,411 Shen’s are similar. However, applicability, the two approaches are very different and are in fact complementary. We focus on feature-learning methods applicable in a 2-dimensional to robots with continuous-valued of a 3-dimensional approximation sensors and control signals situated world. We provide a language of features and generators especially suitable with structured continuous laws. Shen, on the other hand, focuses on learning arrays of sensors. Our emphasis is on learning of sensory for robots features and rules (consisting control state variables. inductionists 218 D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 of conditions, and predictions descriptions rorure(0)), actions, and predictions) are expressed in terms of “percepts”, which are high-level, that are expressed symbolically. The conditions symbolic (e.g., parameters (e.g., ON(disk, peg)). The actions may have continuous but each action rather than continuous. is atomic to learn The two approaches might be combined uses our methods the world in terms of discrete states and actions. viewing to learn relationships nonspatial knowledge doors. in the following way: a learning agent first apparatus, It then uses Shen’s methods expressed to acquire terms of these states and actions such as the effects of pushing objects, flipping switches, or opening to navigate using its continuous sensorimotor and in A more specific example of a potential combination of the two approaches discrimination learning in learning context-dependent (see Section 4.2.3). We currently signal u has a predictable testing a large set of features use a brute-force method for determining effect on a given feature x. The method to see if any can be used to define a partition state space as a set of contexts relationship between u and x. We expect that in each context there is a such as Shen’s that methods could be used to generate such partitions more learning such of Shen’s complementary models whether a control involves of the robot’s linear simple, complementary efficiently discrimination and more intelligently. is the use action 9.2. Inferring the structure of continuous worlds similar solutions on physical robots. Applying automaton an abstraction finite-state simulation of it requires representation. Kuipers and Byun continuous-to-discrete correspond sitions. These constructs have to be manually with a different sensorimotor [ 141 have engineered Lin and Hanson [ 241 use a physical to discrete apparatus. Mataric abstraction states and its local control strategies correspond learning methods [ 18,191 demonstrate problem from a continuous environment an engineered to the real world or a continuous to a discrete to the for the NX robot. NX’s distinctive places to state tran- in order to apply to a robot [ 26,271 and Kortenkamp & Weymouth redesigned solution learning to train robot, called Ratbot, with 16 sonar sensors and 16 learning of a topological map of locally distinctive places. to demonstrate than engineering the corridor-following the local control (e.g., corridor infrared sensors Their work is inspired by the work of Kuipers and Byun, but they use reinforcement strategies, learning27 The target behaviors following) example, when moves along rather are specified by a human behavior, into obstacles. them by hand. teacher. For the robot is rewarded when it running is complementary [ 30-32,341 the desirable behaviors by defining appropriate learn on specifies this by first learning a set of local state variables and then using error signals. Homing and path-following to that of Lin and Hanson. They specify the robot reward signals and then letting agent, on the other hand, teacher. It does them to define a set of that its own how its own target behaviors, behaviors are then specified as behaviors the need for the human the corridor without the rewards. Our Our approach eliminating to gain learning 27 The reinforcement-learning algorithm is a neural-network version of Q learning 123,431. D. Pierce, B.J. Kuipers/Artifkial Intelligence 92 (1997) 169-227 219 minimize this is accomplished given any knowledge the error signals or move the robot while maintaining in a domain-independent manner-the about corridors or corridor-following behaviors. them near zero. All of robot does not need to be Once learning the error signals are defined, there are a number of ways in which to directly define that the learning behaviors might be learned. Reinforcement the homing and path-following agent be given some knowledge of control the hom- is one ap- ing and path-following proach. 28 The approach used in this paper is to learn static and dynamic action models the effects of actions on the local state variables and then to use these that characterize behaviors. This approach does models theory, but the re- require quired knowledge to consists of domain-independent combine our approach with that used by Lin and Hanson’s Ratbot to produce a learn- knowledge nor a knowledge of control ing method agent and a neural-net theory. The error signals would be defined as for our learning version of Q learning would be used to learn the local control strategies based on those error signals. The control inputs their derivatives, to motor control signals. in this way includes and their integrals, the PI and PD control laws would be implemented include as mappings the error signals, that uses neither domain-dependent laws used by our implemented laws that can be defined It would be interesting then the set of control learning agent. If the sensory from sensory templates. inputs An important agent learns, given difference between our approach and that of Lin and Hanson at the feature the current context, is that level rather than at the behavior the effects of each primitive learning agent learns, given the current context, to take in order to produce a particular behavior. There are two advantages level. First, what is learned about one feature may be used is action behavior the feature constant). signals on multiple learning agent our approach handles context dependence level: Our learning action on each feature. Lin and Hanson’s which action to learning to define multiple behaviors, to increase or decrease used action (in which a primitive Second, agent can learn the learning features simultaneously, whereas to learn one behavior at a time. e.g., a homing behavior the value of the feature) is used to move while maintaining (in which a primitive and a path-following the effects of motor control for Lin and Hanson’s it is only possible at the feature 10. Discussion 10.1. What is the value of an existence proof? As discussed the beginning strating one path from a single path has been demonstrated, way and find alternate routes. in Section 1.3, the results presented here are an existence proof, demon- learning problem. Once the to the end of a complex however narrow, future research can broaden We have made some progress toward assessing by applying and second by applying our methods the same learning methods to a significantly the width and solidity of the path, first robot (Section 2.5), different to a variety of different environments systematically 28 In earlier work we explored the use of reinforcement learning to learn homing behaviors [ 331 220 D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 to demonstrate designed step toward determining to learn a meaningful and predictable both success and failure of the methods how much and what type of sensory cognitive map of a continuous environment, (Section 6). This is a input a robot must have and how observable the environment must be for the robot to be able to comprehend it. The existence proof demonstrates that a hard and interesting have a heterogeneous rithms. While system and the environment can be eliminated assumptions would be a significant by future solution, combining the strengths of several focused this solution does rely on a number of assumptions (Section 6.6), we believe about the sensorimotor that several of those assumptions research (Section 6.7). An irreducible minimum set of scientific result. learning problem may learning algo- 10.2. Why learn what can be programmed directly? of its sensorimotor trouble This paper has shown, among other apparatus. There are several reasons why it is worthwhile things, how a learning agent can learn a model to take the to learn what could be directly programmed by a robot’s designer. is inevitable For example, random variation if one of a set of distance does not take sensor failure into fails, the learning intervention. These in the position or direction of sensors the failure with no additional human Sensor variation and failure. Direct programming consideration. methods will accommodate methods will also accommodate sensors. Such variation distance Generality. Ideally, one learning algorithm sorimotor solution A deeper understanding of the problem domain. The design of the learning required the learning to exploit comprise a deeper understanding if robots are mass produced. to many different the process of designing agent and that information. agent that could be exploited by algorithms and learning algorithms types of sen- a particular These sources of information of the problem domain. 29 of sources of information and thus can replace for each sensorimotor of general-purpose the identification the development apparatuses apparatus. learning applies 10.3. What about innate goals? We have characterized a robot for its innate goals concern methods we have developed during a random walk through goal-directed behavior. (e.g., survival, in terms of its set of sensors and effecters, without learning the sensory effects of actions, either in this paper) or during curiosity, pain avoidance). function by observing the environment (as described The Reactive behavior here. With a goal such as pain avoidance, learn a reflexive behavior learning in pursuit of innate goals can support the learning methods described for example, a learning agent might quickly for obstacle avoidance. Such a behavior would help keep the the agent out of danger as it applies learning methods. On the higher-level 29 Of course, designing a learning agent does not guaruntee a deeper understanding of the problem domain. An opaque method such as neural net or genetic algorithm learning could conceivably learn a model of its sensorimotor apparatus without teaching us anything about perception, behavior, or map building. D. Pierce, B.J. Kuipers/Artijkial Intelligence 92 (1997) 169-227 221 other hand, by operating could receive a biased set of experiences and observations in the background of goal-directed behavior, the learning agent Conversely, the learned methods can serve as a foundation the agent has learned higher-level sensorimotor in an action space of larger granularity, describing (see Fig. 20), and making it easier to achieve of the environment. for goal-directed learning. primitives, it can search for at a higher innate goals such as survival and the environment When behaviors level curiosity. 10.4. How general are the learning methods? This paper has identified and demonstrated sensorimotor a number of generic methods for modeling lists several examples is used that subsumes a more specific object or method apparatus. This section and using an uninterpreted where a generic object or method but is more general because it makes fewer assumptions. scalar rather fields) require constructs (for example, (which would the identification for characterizing fields, and vector The learned features the sensors are sensing to objects). The method are based on a set of generic mathematical learned by the learning agent in the example are defined list of salient properties of a robot’s environment. The method (e.g., than on a human- for identifying that make no assump- the method does not assume the ef- is based on spatial and temporal derivatives, not motion and tracking of objects). The local in terms of the scalars, vectors, matrices, generated the structure of a sensory apparatus uses generic distance metrics tions about what that the sensors measure distances fects of motor control vectors of objects state variables purely generic concept of local minimum, is only meaningful which on error signals derived the example needed no concept of wall when defining path-following laws are found by analyzing used in the control local state variables without any understanding or local state variables. The views of the learning agent’s discrete abstract the terminal the robot’s designer. laws are based agent of behaviors. The laws. The parameters relationships between control signals and of the meanings of the control signals interface are only to to the robot’s designer. The learned control the concept of distance-to-wall, local state variables-the states of path-following behaviors, as opposed using generic control to places meaningful its path-following are implemented the learned behaviors learning rather from than Related to the concept of generality is the concept of extensibility. The current im- plementation may be extended by adding new types of features and feature generators. For example, new distance metrics could be used with the group-feature capture new ways of distinguishing and recognizing generators. the method for generating types of sensors; local state variables could be made more general by adding new feature generator different to 10.5. Changes of representation Each abstract interface that the agent learns provides a new representation to reason with. 222 D. Pierce, B.J. Kuipers/Arri$cial Intelligence 92 (1997) 169-227 l At the sensorimotor sensor correlations structure The learned motor capabilities level, the group and image-feature inter- generators feature, which has substantially more analyze the than the raw sense vector. to produce image set of primitive actions provide a new representation of the robot’s that is grounded in sensory effects. and features are learned the primitive actions are grounded l At the control level, behaviors egocentric. Whereas over time, of the external environment the homing and path-following behaviors are grounded as reflected by the local state variables. that are no longer purely in sensory effects averaged in the structure level, l At the causal state space and trajectories, which can then be represented in the topological map. the continuous is reduced to a finite set of states as the nodes and edges of a graph 11. Summary is tenuous: This paper has presented a sequence of learning methods sufficient nitive map of a robot’s continuous world in the absence of domain-dependent of the robot’s sensorimotor object that the sequence would not even apply. While is interesting lem investigated through experimentation vides a method of understanding environment. for learning a cog- knowledge apparatus or of the structure of its world. The reader may then the subsequent methods that each of the learning methods learning prob- available and each pro- agent a new way input or a new way of interacting with the robot’s with an uninterpreted that information for exploiting the robot’s sensory sensorimotor to give the learning identifies a source of information here. Each learning method this is true, we maintain in its own right and if any method failed, the particular is applicable apparatus beyond generators the fact that, in a well-engineered property of the environment, The learning methods are summarized of a motor apparatus using a set of primitive below and in Fig. 30. Section 2 showed how to learn a structural model of a sensory array of sensors sampling the layout of the sensors similarities. Section 3 showed how to use this continuous based on inter-sensor to first define motion detectors and then use them to characterize actions, one for each local state to use the group and image-feature apparatus. They exploit an almost-everywhere may be reconstructed structural knowledge the capabilities of the robot’s degrees of freedom. Section 4 showed how to recognize variables-scalar linear functions of the motor control signals. The effects of the primitive actions on the local state variables are captured by the static action model. Section 5 showed how to use the static action model behaviors, how to the effects of the primitive actions on the local learn a dynamic action model execute, and how to use the state variables while open-loop path-following behaviors. Finally, dynamic Section 7 showed how discrete abstract interface to a finite-state world. By using to define a its continuous world as the target abstraction, features whose derivatives can be approximated to define homing and open-loop path-following the learning agent to abstract by context-dependent to use the homing robust, closed-loop and path-foliowing the finite-state action model path-following that allows automaton to predict behaviors behaviors to define D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 223 Sensorimotor Level Raw senses and actions Sensory slructure Motion detection Primlive actions Control Level , ’ ’ ,’ Homing behaviors I i Open-loop path-following behaviors Y=Y’ -,d----- --L) w (>- -----. Causal Level Fig. 30. A graphical of the first three levels of the spatial semantic hierarchy. summary of the learning methods used in this paper, showing the objects learned at each the learning world. agent inherits a powerful set of methods for inferring the structure of its In the biological world, the newly hatched organism agent. However, we hope learning than our knowledge reported here will shed light on the structure and learnability about an agent’s evolution, development, relationship with and learning of spatial knowledge of these methods embodies that an exploration of fundamental a great deal more like that knowledge into insights the in biological organisms. The potential for application is much more direct. New robots, with new sensors and effecters, are being designed and built all the that humans have never directly time. Robots will one day be sent into environments to mechanical robots its world. If so, it could provide 224 D. Pierce, B.J. Kuipers/Art$cial Intelligence 92 (1997) 169-227 (e.g., experienced created robot through autonomous methods presented here are a step in that direction. to be able to orient the deep ocean floor or the surface of another planet). For a newly to its sensorimotor experimentation would be of substantial value. We believe system and its environment itself that the Appendix A. Computational complexity summarizes This appendix the complexities of the various learning methods described is potentially in the size of the raw sense vector and the depth of the tree of generated reduced by using in this paper. The overall complexity of the sequence of learning methods exponential features. an appropriate set of feature generators as is explained this level of complexity In our experience, can be drastically in Section A.3. A.I. Modeling the sensory apparatus the distance metric dl (used by both the group-feature generator) generator and the Computing is of complexity O(n*T> where n is the number of elements image-feature in the raw sense vector and T is the number of time steps taken before the group-feature generator the group-feature generator) each time step (to update the group-feature Identifying for each element of the raw sense vector at and an 0( n*) computation when is applied. Computing requires an O( 1) computation for a total complexity of 0( nT + n*) . the distance metric d2 (used by is an O(n*) computation. Using the frequency distributions) is applied subgroups generator transitive similar to closed identify with an iterative The relaxation dependence the actual complexities. complexity subgroups is an O(n3) algorithm for which each computation. iteration The metric-scaling an O(n3) involves algorithm is also iterative with each iteration being O(n*). Since of the number of iterations on n is unknown, In our experiments, T is much greater of the sensory-modeling step can be approximated by 0( n*T) . 3o these are lower the limits on than n so the overall closure is performed computation. A.2. Modeling the motor apparatus The calculation of the motion feature requires an 0( n*) computation at each time step. The calculation analysis algorithm overall complexity of the amvfs is thus of complexity 0( n*T) . The principal is of complexity O(n3). Again, since T is much greater by O(n*T). can be approximated component than IZ, the A.3. Identifying local state variables The first step in identifying If every subset of the current set of defined features can be used to produce a new set of features, features will be at least O(2”) where then is to generate new features. local state variables the complexity of generating and testing X’ An open problem is to predict the value of T for each learning method that requires an exploration phase. D. Pierce, B.J. Kuipers/Artijicial Intelligence 92 (1997) 169-227 225 this potential set of feature generators that collapse many explosion can be avoided by using an appropriate n is the number of elements of the raw sense vector. In our experience, combinatorial (e.g., generators that only apply to certain create arbitrary group features. The second subsets of the raw sense vector-it types of features). For example, is to compute input features into a small set of output features, or the group generator does not creates at most n non-overlapping step in identifying local state variables of the computation features model. The complexity where s is the number of singleton of time steps over which the action-independent The complexity c is the average number of contexts associated with each feature. the static action is O(sT) that have been learned and T is the number of actions, is O(sTuc) where is O(sTu) where a is the number of primitive is learned. The complexity of the computation of the context-dependent model of the action-dependent model of the computation the model model A.4. Learning control laws The number of open-loop path-following local state variables, behaviors a is the number of primitive is O(uac) where u is the number and c is the of learned average number of contexts associated with each local state variable. The complexity is O( uacT( a - 1)). The number of action model of the computation In practice, is O( 2’~2~). behaviors generated behaviors can be made much less than this upper bound. the number of path-following For example, the terms 2” and 2’ can be replaced by u3 and c3 by only defining path- following behaviors whose error vectors are based on at most three local state variables. is kept reasonable by the of the dynamic path-following the number of path-following In our experiments, (worst-case) closed-loop behaviors actions, facts: following ( I) (2) the number of learned the number of contexts constant is small relative local state variables in which a primitive action maintains to the total number of contexts. is small and a local state variable In our example, u is at most 3 and c is around 20. Acknowledgements The authors would like to thank Rick Froom, Wan Yik Lee, Risto Miikkulainen, Ray for their Mooney, Lyn Pierce, Mark Ring, Boaz Super, and two anonymous technical, editorial, and moral support. reviewers References 1 I 1 D. Angluin, On the complexity of minimum inference of regular sets, Inform. and Control 39 ( 1978) 337-350. 12 1 D. Angluin, Learning regular sets from queries and counterexamples, Inform. and Cornput. 75 ( 1987) 87-106. [ 3 1 D. Chapman and L.P. Kaelbling, Learning from delayed reinforcement in a complex domain, Tech. Rept. TR-90-11, Teleos Research, Palo Alto, CA, 1990. 226 D. Pierce, B.J. Kuipers/Artifcial Intelligence 92 (1997) 169-227 141 T.H. Cormen, C.E. Leiserson Cambridge, MA, 1990). and R.L. Rivest, Introduction to Algorithms (MIT Press/McGraw-Hill, ISI T.L. Dean, D. Angluin, K. Basye, S. Engelson, L.P Kaelbling, E. Kokkevis and 0. Maron, functions and an application learning, to map Inferring in: Proceedings finite automata with stochastic AAAI-92, San Jose, CA ( 1992) 208-214. output 16 I T.L. Dean, K. Basye and L.P Kaelbling, Uncertainty in graph-based map learning, in: J.H. Connell and S. Mahadevan, eds., Robot Learning (Kluwer Academic Publishers, Boston, MA, 1993) 171-192. 1 I] G.L. Drescher, Made-Up Minds: A Constructivist Approach to Art@cial Intelligence (MIT Press, Cambridge, MA, 199 1). ( S] G. Dudek, M. Jenkin, E. Milios and D. Wilkes, Robotic exploration as graph construction, IEEE Trans. Robotics and Automation 7 (6) (1991) 859-865. [9] E.M. Gold, Complexity of automaton identification from given data, Inform. and Control 37 ( 1978) 302-320. [ lo] K. Hiraki, Abstraction of sensory-motor features, in: Proceedings 16th Annual Conference of the Cognitive Science Society (Lawrence Erlbaum Associates, Hillsdale, NJ, 1994). 1 111 B.K.l? Horn, Robot Vision (MIT Press, Cambridge, MA, 1986). I 12 I MI. Jordan and D.E. Rumelhart, Forward models: supervised learning with a distal teacher, Cognitive Sci. 16 (1992) 307-354. 113 1 T. Kohonen, SelfOrganization and Associative Memory (Springer, Berlin, 2nd ed., 1988). [ I4 J D. Kortenkamp and T. Weymouth, Topological mapping for mobile robots using a combination of sonar and vision sensing, in: Proceedings AAAI-94, Seattle, WA, 1994. [ 15 I W.J. Krzanowski, Principles of Multivariate Analysis: A User’s Perspective, Oxford Statistical Science Series 1 16J B.J. Kuipers, An ontological (Clarendon Press, Oxford, 1988) hierarchy for spatial knowledge, in: Proceedings 10th International Workshop on Qualitative Reasoning about Physical Systems, Fallen Leaf Lake, CA, 1996. 1 171 B.J. Kuipers, Modeling [ 181 B.J. Kuipers and Y.-T. Byun, A robust, qualitative method spatial knowledge, Cognitive Sci. 2 (1978) 129-153. for robot spatial learning, in: Proceedings AAAI-88, St. Paul, MN (1988) 774-779. [ 191 B.J. Kuipers and Y.-T. Byun, A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations, 120 ] B.J. Kuipers and T.S. Levitt, Navigation J. Robotics and Autonomous Sysrems 8 ( 199 1) 47-63. and mapping in large-scale space, AI Magazine 9 (2) (1988) 25-43. 121 1 B.C. Kuo, Automutic Control Systems (Prentice-Hall, 1221 D.B. Lenat, On automated scientific Hayes, D. Michie and L.I. Mikulich, 251-286. Englewood Cliffs, NJ, 4th ed., 1982). formation: A case study using theory in: J.E. eds., Machine Intelligence 9 (Halsted Press, New York, 1977) the AM program, 123 1 L.-J. Lin, Reinforcement learning University, Pittsburgh, PA ( 1993). [ 24 1 L.-J. Lin and S.J. Hanson, On-line for robots using neural networks, Ph.D. Thesis, Carnegie Mellon learning for indoor navigation: Preliminary results with RatBot, in: Proceedings NIPS93 Robot Learning Workshop, 1993. 125 J K.V. Mardia, J.T. Kent and J.M. Bibby, Multivariate Analysis (Academic Press, New York, 1979). [26J M.J. Mataric, Navigating with a rat brain: A neurobiologically-inspired model robot for spatial in: J.-A. Meyer and S.W. Wilson, eds., Prom Animals to Animats: Proceedings representation, International Conference on Simulation of Adaptive Behavior (MIT Press/Bradford MA, 1991) 169-175. Books, Cambridge, fst [ 271 M.J. Mataric, Integration of representation into goal-driven behavior-based robots, IEEE Trans. Robotics and Automation 8 (3) ( 1992) 304-3 12. I28 1 C.J. Matheus, The need for constructive in: L.A. Bimbaum and G.C. Collins, eds., Proceedings Eighth International Conference on Machine Learning, Ithaca, NY (Morgan Kaufmann, San Mateo, CA, 1991) 173-177. induction, [29] E. Oja, A simplified neuron model as a principal component analyzer, J. Math. Biology 15 ( 1982) 267-273. [30] D. Pierce, Learning in: L.A. and G.C. Collins, eds., Proceedings Eighth International Conference on Machine Learning, a set of primitive actions with an uninterpreted sensorimotor apparatus, Bimbaum Ithaca, NY (Morgan Kaufmann, San Mateo, CA, 1991) 338-342. D. Pierce, B.J. Kuipers/Art@cial Intelligence 92 (1997) 169-227 227 1311 D. Pierce, Learning IEEE International in: Proceedings turn and travel actions with an uninterpreted Conference on Robotics and Automation, Los Alamitos, CA (IEEE Computer Society sensorimotor apparatus, 1321 I.331 I341 1351 1361 1371 1381 139 140 I41 I 142 Press, Silver Spring, MD, 1991) 246-251. D. Pierce, Map learning with uninterpreted Austin, 1995; hap://ftp.cs.utexas.edu/pub/qsim/papers/Pierce-PhD-95.ps.Z D. Pierce and B.J. Kuipers, Learning hill-climbing a mobile International Conference on Simulation of Adaptive Behavior, robot, in: J.-A. Meyer and S.W. Wilson, eds., From Animals to Animatst Proceedings 1st functions as a strategy for generating behaviors in (MIT Press/Bradford Books, Cambridge, sensors and effecters, Ph.D. Thesis, University of Texas at also: Tech. Rept. TR AI9 I-137, AI Laboratory, University of Texas at Austin. to explore and build maps, in: Proceedings AAAI-94, Seattle, WA Press, Cambridge, MA, 1994). MA, 1991) 327-336; D. Pierce and B.J. Kuipers, Learning (AAAI/MIT W.H. Press, S.A. Teukolsky, W.T. Vetterling and B.P. Flannery, Numerical Recipes University Press, Cambridge, M. Ring, Continual learning 1994. H.J. Ritter, T. Martinez and K.J. Schulten, Neural Computation in reinforcement 1988). environments, Ph.D. Thesis, University of Texas at Austin, and Self:Organitin,q Maps: An in C (Cambridge Reading, MA, 1992). Inference of finite automata using homing sequences. &i~rrn. and Comput. (Addison-Wesley, fntroduction R.L. Rivest and R.E. Schapire, 103 2 ( 1993) 299-347. W.-M. Shen, Functional transformations in AI discovery systems, Art@%1 Intelligence 41 (1990) from Learning 257-272. W.-M. Shen, Autonomous W.-M. Shen and H.A. Simon, Rule creation and rule learning IJCAI-89, Detroit, MI (1989) 675-680. Proceedings R.S. Sutton. Integrated architectures programming, in: B.W. Porter and R.J. Mooney, eds., Proceedings 7th International klachine Learning, Austin, TX (Morgan Kaufmann, San Mateo, CA, 1990) 216-224. through environmental the Environment (Freeman, New York, 1994). for learning, planning and reacting based on approximating exploration, in: dynamic Conference on 143 I C.J.C.H. Watkins, Learning (44 1 S. Whitehead, from delayed rewards, Ph.D. Thesis, King’s College, Cambridge ( 1989). J. Karlsson and J. Tenenberg, Learning multiple goal behavior via task decomposition and (KIuwer Academic dynamic policy merging, Publishers, Boston, MA, 1993) 45-78. in: J.H. Connell and S. Mahadevan, eds., Robot Leurning 145 1 R.J. Williams, Reinforcement-learning connectionist systems, Tech. Rept. NU-CCS-87-3, College of Computer Science, Northeastern University, Boston, MA ( 1987). 