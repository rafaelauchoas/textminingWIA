2202raM71]DS.sc[1v20490.3022:viXraRobust and Complex Approach of Pathological Speech Signal AnalysisJiri Mekyskaa,b, Eva Janousovac, Pedro Gomez-Vildad, Zdenek Smekala,b, Irena Rektorovae,f,∗,Ilona Eliasovae,f, Milena Kostalovag,f, Martina Mrackovae,f, Jesus B. Alonso-Hernandezh,Marcos Faundez-Zanuyi, Karmele L´opez-de-Ipi˜najaDepartment of Telecommunications, Brno University of Technology, Technicka 10, 61600 Brno, Czech RepublicbSIX Research Centre, Technicka 10, 61600 Brno, Czech RepubliccInstitute of Biostatistics and Analyses, Masaryk University, Kamenice 3, 62500 Brno, Czech RepublicdFacultad de Informatica, Universidad Politecnica de Madrid, Campus de Montegancedo, s/n, 28660 Boadilla del Monte,Madrid, SpaineFirst Department of Neurology, St. Anne’s University Hospital, Pekarska 53, 65691 Brno, Czech RepublicfApplied Neuroscience Research Group, Central European Institute of Technology, Masaryk University, Komenskeho nam. 2,60200 Brno, Czech RepublicgDepartment of Neurology, Faculty Hospital and Masaryk University, Jihlavska 20, 63900 Brno, Czech RepublichInstitute for Technological Development and Innovation in Communications (IDeTIC), University of Las Palmas de GranCanaria, 35001 Las Palmas de Gran Canaria, SpainiEscola Universitaria Politecnica de Mataro, Tecnocampus, Avda. Ernest Lluch 32, 08302 Mataro, Barcelona, SpainjDepartment of Systems Engineering and Automation, University of the Basque Country UPV/EHU, Av de Tolosa 54,20018 Donostia, SpainAbstractThis article presents a study of the approaches in the state-of-the-art in the field of pathological speech signalanalysis with a special focus on parametrization techniques. It provides a description of 92 speech featureswhere some of them are already widely used in this field of science and some of them have not been tried yet(they come from different areas of speech signal processing like speech recognition or coding). As an originalcontribution, this work introduces 36 completely new pathological voice measures based on modulationspectra, inferior colliculus coefficients, bicepstrum, sample and approximate entropy and empirical modedecomposition. The significance of these features was tested on 3 (English, Spanish and Czech) pathologicalvoice databases with respect to classification accuracy, sensitivity and specificity. To our best knowledge theintroduced approach based on complex feature extraction and robust testing outperformed all works thathave been published already in this field. The results (accuracy, sensitivity and specificity equal to 100.0 ±0.0 %) are discussable in the case of Massachusetts Eye and Ear Infirmary (MEEI) database because of itslimitation related to a length of sustained vowels, however in the case of Pr´ıncipe de Asturias (PdA) Hospitalin Alcal´a de Henares of Madrid database we made improvements in classification accuracy (82.1 ± 3.3 %)and specificity (83.8 ± 5.1 %) when considering a single-classifier approach. Hopefully, large improvementsmay be achieved in the case of Czech Parkinsonian Speech Database (PARCZ), which are discussed inthis work as well. All the features introduced in this work were identified by Mann-Whitney U test assignificant (p < 0.05) when processing at least one of the mentioned databases. The largest discriminativepower from these proposed features has a cepstral peak prominence extracted from the first intrinsic modefunction (p = 6.9443 · 10−32) which means, that among all newly designed features those that quantify1   especially hoarseness or breathiness are good candidates for pathological speech identification. The articlealso mentions some ideas for the future work in the field of pathological speech signal analysis that can bevaluable especially under the clinical point of view.Keywords: pathological speech, disordered voice, dysarthria, speech processing, bicepstrum, non-lineardynamic features1. IntroductionVoice Pathology description and characterization has always demanded attention from the physiologicaland medical fields [1], as well as from the voice function point of view [2]. The term voice is describedby [3] both in a broad and a narrow sense. In the broad sense voice may be taken as synonymous of speech,therefore terms as Voice over IP (VoIP) can be found in the literature and media with the meaning ofspeech data on internet. In the narrow sense voice refers to the vibration of the vocal folds. Speech soundsresulting from the interaction of this vibration with the Oro-Naso-Pharyngeal Tract (ONFT) are referred to,as voiced. Speech sounds produced by turbulent flow within the ONFT are termed voiceless. Phonation isthe recommended term to refer to vocal fold vibration. A speaker showing an anomalous vocal fold vibrationpattern is referred as dysphonic, and aphonic if there is no vocal fold vibration at all. If no anomalies arepresent the speaker is referred as normophonic.Dysphonic voice is a perceptual and subjective term associated to Voice Pathology. Dysphonia is theperceptual quality of voice signaling that something wrong is happening in the phonation organs (mainlythe larynx and its associated structures). Voice Pathologies and Dysphonic Voice are thus intrinsicallyrelated. The classification of Voice Pathologies or Disorders is described in [3] as tissue infection (e. g.laryngitis, bronchitis, croup,. . . ), systemic changes (e. g. dehydratation, pharmacological and drug effects,hormonal changes,. . . ), mechanical stress (e. g. vocal nodules, polyps, ulcers, granulomae, laryngocele, hem-orrhage,. . . ), surface irritation (e. g. laryngitis, leukoplakia, gastroesophageal reflux,. . . ), tissue changes (e. g.laryngeal carcinoma, keratosis, papillomas, cysts,. . . ), neurological and muscular changes (e. g. bilateral andunilateral vocal fold paralysis, Parkinson’s Disease (PD), Amyotrophic Lateral Sclerosis (ALS), myotonicdystrophy, Huntington’s Chorea, myasthenia gravis,. . . ), and abnormal muscle patterns (e. g. conversionaphonia or dysphonia, spasmodic dysphonia, mutational dysphonia, ventricular phonation,. . . ). In [4] a de-scription of vocal pathologies can be found. A last important group of neurological diseases which leavea correlate in voice and speech is that of cognitive origin, Alzheimer’s Disease (AD) being the most relevant∗Corresponding author. Address: First Department of Neurology, St. Anne’s University Hospital, Pekarska 53, 65691 Brno,Czech Republic. Telephone number: +420 549 497 825.Email address: irena.rektorova@fnusa.cz (Irena Rektorova)Preprint submitted to NeurocomputingMarch 18, 2022one for their impact in well-being and in aging specialized-attention demand. Some references on the influ-ence of AD in speech and voice can be found in [5–8]. Going a step further, emotional alterations (eithertemporary or persistent) leave also correlates in the speech and phonation signature, and may be subjectsof further study by acoustic analysis [9, 10]The relationship between acoustic correlates and voice pathology has been clinically established in thelast decades, subjectively and quantitatively [11, 12]. Acoustic Voice Quality Analysis (AVQA) is a wideterm for a set of different methodologies designed to quantify acoustic correlates giving a definition of thequality of phonation or speech production. Therefore AVQA would be the procedural way to objectivelyquantify anomalies manifested in Dysphonic Voice. Modern signal processing technologies provide estimatesof voice and speech correlates in time and frequency [13], allowing to better visualize and quantify phonationpatterns. Spectral techniques facilitate the study of pathologic phonation, establishing relations betweenharmonic-harmonic and harmonic-formant ratios, which were found as important correlates to organic voicepathology [14]. Similarly, harmonic-noise ratios were found significant in characterizing certain types ofdysphonic pathologies [15, 16]. Time domain estimates, as jitter, shimmer and open-closed phase quotientsare also used in describing dysphonic voice [17, 18]. These descriptions gave rise to AVQA as a specificfield [19].Under the point of view of AVQA the following objectives can be established in order of difficulty:dysphonic voice detection, dysphonic voice grading, and dysphonic voice classification according to etiology.Dysphonic voice detection would be the task of assigning normophonic (normative) or dysphonic (non-normative) labels to a given phonation produced by a specific speaker. The determination of the dysphonicgrade is traditionally carried out by independent referees according to a subjective criterion on a given scale.One of the most popular is GRBAS (grade, roughness, breathiness, asthenia and strain) [11]. The relativedependence of the assigned grade to the referee’s subjective opinion, results in wide grading differencesamong referees. To overcome this problem requires the design of clinical assessment methodologies [20].The task of dysphonic voice classification according to etiology is far more difficult, as a given acousticcorrelate may be attributed to different pathologies. If this problem is stated in terms of associating acousticcorrelates to specific pathologies, the potential risk is that it will remain unsolved for long, because it is anill-posed problem (a many-to-one subjective mapping). A preliminary step to be covered first is to definethe implications of different pathologies in the vocal function, especially at the level of the larynx. Underthe functional point of view, the following main behaviors may be observed in the abnormal operation of thevocal folds: asymmetric vibration, contact defects and dystonia (hypo-, hyper-tension and tremor). Mostorganic larynx pathologies reproduce either one or another behavior, or all of them. Asymmetric vocalfold vibration is to be expected in pathologies as polyps, cysts, carcinomae, vocal fold paralysis, ulcers,cysts, papillomae, etc., and produces acoustic correlates as jitter, shimmer, poor harmonic-noise ratios,unbalance, sub- and inter-harmonics, etc. On its turn, contact defects are to be expected in pathologies3as polyps, nodules, edemae, cysts, etc., where full closure of the glottal gap is not granted by vocal foldadduction and produce acoustic correlates as open and close phase perturbations, recovery phase attenuation,harmonic display reduction, etc. Other pathologies, especially those of neurological origin produce deviationsin biomechanical parameters, as vocal fold tension (hypo- and hyper-tonic) and tremor, which can alsobe manifested as modulations in amplitude or frequency, and as changes in the vocal fold tension. Asmany organic pathologies induce a hyper-tonic behavior, the main problem when dealing with correlatesproduced by neurological pathology is to differentiate their origin from that of organic origin. Contactdefect pathologies can also show asymmetric vocal fold vibration, and this may also be the case in agingvoice (presbyphonia). To establish differentiation criteria for using acoustic correlates when dealing withorganic, neurologic or aging-induced perturbations, is a major issue in AVQA [21].Another problem regarding AVQA is the lack of good reference baselines to establish the methodologiesfor voice pathology classification from acoustic analysis. Rigorous databases, acquired to represent eachof the different pathologies under well-defined sample population size and recording conditions are scant.Many times the problem comes from the simultaneous presence of different larynx pathologies in the samepatient, either related or unrelated (e.g.it is common to find a counter-lateral lesion as a consequenceof a unilateral polyp). The problem then is how to decide if a specific acoustic correlate is produced byone cause or another. Most of the times acoustic databases are produced by laryngological services aspart of examination protocols [22, 23], but this is not always the case, as many laryngologists prefer todepend on visual exploration, neglecting the possibilities offered by AVQA for different reasons [24]. Speechtherapists rely more on acoustic exploration, but many times it is mainly restricted to measurements ofvocal effort, long term frequency analysis, respiratory efficiency, or distortion measures. Therefore voicerecords from speakers ranked by etiology and severity index, using compatible standards (digitalization,channel, microphones) are scant. Most of the available databases are either incomplete, inconsistent (madeup of recordings taken under different conditions) or deficient (many non-frequent pathologies are not wellrepresented in sample size). Besides, there is a lack of good normative databases, as most of the recordsproduced by medical services contain information from dysphonic voice, but normative speakers are absentor poorly represented (this is the case of the most widely used database [25]). This is a severe limitation forsystematic AVQA. Another problem is cross-lingual representation. As far as sustained vowels are concerned,this would not be a problem, but it becomes a major obstacle when segmental parameters are involved, asin the use of passages (either read or spontaneous).After all these considerations, it may be said that the aim of AVQA is to design the best methodology touse Voice Correlates in Voice Pathology detection, grading and classification. Several steps are to be assumedand methodologically formulated for such. Voice Pathologies and Correlates have to be well connected byadequate physiological and biomechanical modeling of larynx dynamics to understand how acoustic correlatesand organic dysfunctions could be associated [26]. This requirement is of vital importance in meeting the4challenge of pathology classification from acoustic correlates. On its turn, inverse methods to obtain robustand significant estimates of voice correlates are of great relevance [27]. These are to be combined with otherparameterization methods based in direct time and frequency domain estimates to produce rich featuresets [28]. Finally, good Statistical Pattern Matching and Machine Learning methodologies have to be usedto reduce redundancy and to make a selection by relevance within the feature set, to determine the bestcombinations for each specific task (detect, grade, classify), and procure specific results under quality criteria(sensitivity, specificity and accuracy) [29]. These methods have to be contrasted on generally-acceptedbenchmark databases [30].Having in mind all these conditions the present study is aimed to describe a wide set of tests on differentdatabases, using an exhaustive set of acoustic features, by means of well-known classifiers to estimatethe performance of the methods and features regarding specificity, sensitivity and accuracy in dysphonicvoice detection tasks. There are already many publications describing the methods of pathological speechdetection. Usually the authors use just a limited set of speech features and most of the publications presentthe detection accuracies tested only on one monolingual database [31–37]. Probably the widest range offeatures describing different aspects of speech was tested in a work of Tsanas et al. [34]. Regarding thedatabases there are just a few publications considering 2 different data sets [38–41].To sum up the state-of-the-art approaches in the field of pathological voice analysis there is still a lack ofpublications providing a complex overview of features quantifying pathological speech and providing strongconclusions supported by a robust testing. Therefore this work has 4 main goals:1. According to complex parameterization and consequent robust testing identify features that have thelargest discriminative power in the field of pathological speech analysis.2. Design new features that can quantify hoarseness, breathiness and non-linearities in pathological speechsignals.3. Prove that the proposed large set parameterization approach can provide better classification results(with respect to classification accuracy, sensitivity and specificity) than those published in the field ofpathological speech analysis by the other researchers.4. Select a database that has high potential for the future, especially in terms of speech features design,tuning and testing.The paper is organized as follows: the classically used features as well as the newly designed ones arediscussed in section 2, section 3 describes the 3 databases, section 4 describes the testing procedure, section 5is devoted to the experimental results and discussion. The conclusions are given in section 6.52. FeaturesThe aim of this work was to explore significance of the mostly used speech features when focusing on theability of differentiation between healthy and pathological speech. The features that are usually known indifferent fields of speech signal processing (speech recognition, enhancement, denoising, identification etc.)and newly designed features originally introduced in this paper will be investigated as well.Due to the limited size of this paper the features that were not originally introduced in this work will bementioned without their deeper description by an algorithm. However each parameter will be accompaniedby a reference where the reader can find further information.2.1. Features Describing PhonationProbably the most popular features describing pathological voice are fundamental frequency F0 andparameters describing its variability in time (jitter): PPQ5 (five-point Pitch Perturbation Quotient), RAP(Relative Average Perturbation), jittloc (average absolute difference between consecutive periods, dividedby the average period), jittabs (average absolute difference between consecutive periods), jittddp (averageabsolute difference between consecutive differences on neighbour glottal periods, divided by the averageperiod) [39, 41–43]. These features are good candidates especially for quantification of voice tremor [44].A disadvantage of previously mentioned measurements is that the values of the features are highlydependent on gender and variable acoustic environment. To overcome this disadvantage Little et al. proposedthe PPE (Pitch Period Entropy) [45]. During the calculation of PPE, logarithmic semitone scale, inversefiltering and entropy estimation are incorporated. Another method which is close to jitter is a measureof standard deviation (std) of the time that vocal folds are apart (GQopen) and in collisions respectively(GQclosed) [34].To effectively describe hypophonia or intensity perturbations, short-time energy E or pitch-level vari-ations (shimmer) can be used [34, 42, 46]. In this work 6 kinds of shimmer will be considered (they arecalculated similarly to jitter but the intensity is used): APQ3 (three-point Amplitude Perturbation Quo-tient), APQ5, APQ11, shimmloc, shimmddp, shimmdB (average absolute base-10 logarithm of the differencebetween the amplitudes of consecutive periods) [43, 47].Another feature describing speech intensity is TKEO (Teager-Kaiser Energy Operator) [48]. The ad-vantage over simple E is that it takes into account also signal frequency. It has been shown that speechcontains dominant modulation frequencies in a range 2–20 Hz with maximum at approximately 4 Hz [49].The 4 Hz modulation energy (ME) was selected as a feature which is related to a measure describing en-ergy distribution in power spectra. Similarly MPSD (Median of Power Spectral Density), usually calledmedian frequency, was selected [50]. The last feature in this category is LSTER (Low Short-Time EnergyRatio) [51]. This feature is usually used for differentiation between speech and music signals because speech6exhibits higher variations in energy per 10–30 ms frames. However this feature was selected for analysis ofpathological speech as well. It is considered that patients will reach higher values of this feature in the caseof maintained vowels due to the inability to sustain the same amount of airflow during the whole phonation.2.2. Features Describing Tongue MovementFrequencies of first three formants F1, F2, F3 and their bandwidths B1, B2, B3 are related to volumesof vocal tract cavities. Especially the volume of throat and oral cavity is modified by the tongue position.According to this fact it is possible to consider formants as a measure of tongue movement [52, 53].2.3. Features Describing Speech QualitySigns of vocal fold dysfunctions are usually associated with breathiness or hoarseness [32]. From signaltheory point of view these dysfunctions are characteristically decreasing voice quality, under a simplifiedconsideration, by additive noise. This implies that methods based on speech quality measurements cansuitably quantify vocal folds impairment and describe its progress. Besides breathiness and hoarseness thesemethods can be also used for analysis of hypernasality caused by an improper work of soft palate [54].Probably the simplest quality measure feature is ZCR (Zero-Crossing Rate) and its modification HZCRR(High Zero-Crossing Rate Ratio) which takes into account a variation of ZCR in time [51]. Another simplemeasure is FLUF (Fraction of Locally Unvoiced Frames) which can describe an impossibility of carrying outperiodical glottal closure [55].The next three measures are based on the variation of spectrum values between adjacent frames. Specif-ically these are SF (Spectral Flux) [56], SDBM (Spectral Distance Based on Module) and SDBP (SpectralDistance Based on Phase) [55].We have also used several features based on cepstral analysis.It was shown that cepstrum and itsrahmonics correlate well with the perception of breathiness [57]. Moreover from the nature of cepstrumit can be said that it is a kind of periodicity measure and therefore it should also predict roughness. Themost famous feature based on the real cepstrum is CPP (Cepstral Peak Prominence) originally introduced byHillenbrand et al. [57]. Besides this feature, PECM (Pitch Energy Cepstral Measure) [55] and VR (Variationin Ratio between the second/first harmonic within the derived cepstral domain) were also used [55].The other quality measure features are based on an estimation of the level of noise present. We will testthe significance of HNR (Harmonic-to-Noise Ratio) [37, 38, 54, 58], NHR (Noise-to-Harmonic Ratio) [37, 59],NNE (Normalized Noise Energy) [60], GNE (Glottal-to-Noise Excitation ratio) [58], SPI (Soft PhonationIndex) [59] and VTI (Voice Turbolence Index) [59]. The methods differ especially in the estimation of noise.The last feature in this category is SSD (Segmental Signal-to-Dysperiodicity ratio). It was shown thatthis feature correlates strongly with the perceived degree of the speaker’s hoarseness [38].72.4. Segmental FeaturesAlthough some of the features mentioned in the other categories can be denoted as segmental as well(they are calculated from 10–30 ms segments), here the segmental features are considered as matrices (notonly vectors) calculated from the whole signal. Probably the most popular segmental features in the fieldof speech signal analysis are MFCC (Mel Frequency Cepstral Coefficients) [33, 36, 42, 54]. The advantageof these coefficients is that they can indirectly detect slight misplacements of articulators [34]. In this work20 MFCC coefficients are extracted. The coefficient number zero is replaced by an estimate of log-energy.Although MFCC are generally used, there is a lack of publications that compare these features toother segmental ones for the purpose of pathological speech analysis. Therefore we decided to include othersegmental features. We extracted 20 mel frequency cepstral coefficients but in this case the bank of triangularfilters was adjusted to the equal loudness curve [61]. We call these features MFCCE. The other two sets offeatures derived from MFCC are LFCC (Linear Frequency Cepstral Coefficients) and CMS (Cepstral MeanSubtraction coefficients). In the case of LFCC the bank of triangular filters is equidistantly spread in thefrequency scale [62]. CMS is a kind of standardized z-score of MFCC (subtraction of mean and division bystd over the time).MSC (Modulation Spectra Coefficients) can provide information complementary to MFCC [36, 63]. Thesefeatures can capture a class of source mechanism characteristics related to voice quality [33].In the next step features based on linear prediction were extracted. LPC (Linear Predictive Coeffi-cients) [42], PLP (Perceptual Linear Predictive coefficients) [64], LPCC (Linear Predictive Cepstral Coeffi-cients) [65], LPCT (Linear Predictive Cosine Transform coefficients) [42] and ACW (Adaptive ComponentWeighted coefficients) [65] were tested. The advantage of PLP over simple LPC or MFCC is that it alsotakes into account an adjustment to the equal loudness curve and intensity-loudness power law [64]. Theadvantage of LPCC and LPCT over “classic” LPC is that a transformation is used into the cepstral domainand thus the values do not correlate much. The advantage of ACW is that these coefficients are less sensitiveto channel distortion [65].The last segmental features used in this work are parameters that analyze amplitude modulations invoice using a biologically-inspired model of the inferior colliculus [66]. These features are called ICC (InferiorColliculus Coefficients).Segmental features are sometimes extended by their 1st and 2nd order regression coefficients (∆ and ∆∆respectively). In this work used the ∆ coefficients.2.5. Features Based on BispectrumAlonso et al. proved that a greater presence of quadratic coupling is observed in healthy voice whencomparing it to the pathological one [55]. It is probably due to a fact that healthy voice is characterized by8a vocal tract which is more non-linear than in the case of pathological voices. This quadratic coupling canbe appropriately described by bispectrum and features derived from this 2D signal.There were proposed measures such as BII (Bicoherence Index Interference), HFEB (High FrequencyEnergy of one-dimensional Bicoherence), LFEB (low Frequency Energy of one-dimensional Bicoherence),BMII (Bispectrum Module Interference Index) and BPII (Bispectrum Phase Interference Index) [55].2.6. Features Based on Wavelet DecompositionThe wavelet transform is widely used especially in the field of coding and speech denoising. However itsapplication can be found in the field of pathological speech analysis too [67]. Detail coefficients after thedecomposition can be used to estimate the present noise and consequently it is possible to calculate SNR(Signal-to-Noise ratio). In fact this is just another method of voice quality measurement.According to some measurements we have empirically selected 7 wavelets: 10th, 15th, 20th-order Daubechieswavelets; 10th, 15th, 20th-order symlet wavelets and 5th-order coiflet wavelet. We will mark the featuresderived from the detail coefficients of the wavelet transform as SNRW(wvl ), where wvl corresponds to thespecific wavelet (e. g. daub15).2.7. Features Based on Empirical Mode DecompositionRecently, in speech processing new methods based on EMD (Empirical Mode Decomposition) have beenused. Using EMD it is possible to decompose the arbitrary non-linear and time-varying signal into countableand usually a small number of IMF (Intrinsic Mode Functions). These functions are modulated in amplitudeand frequency and their sum gives the original signal.Tsanas et al. proposed several measures of SNR and NSR based on the IMFs [34]. The time-varyinghigh frequency components are present in the first few IMFs. Therefore these first few IMFs can be usedto represent the noise in the signal and the rest of IMFs can be used for a representation of the usefulinformation. According to this consideration features like IMF-SNRTKEO (based on Teager-Kaiser EnergyOperator), IMF-SNRSEO (based on Squared Energy Operator), IMF-SNRSE (based on Shannon Entropy),IMF-NSRTKEO, IMF-NSRSEO and IMF-NSRSE have been introduced.2.8. Non-linear Dynamic FeaturesTension on the vocal folds can significantly differ in the case of pathological speech. Voice becomesaperiodic, noisy-like, and it is very difficult to find any regularities in the signal. There is a frequentpresence of sub-harmonics and chaos which can lead to a failure of conventional techniques of speech signalanalysis. However it was shown that these kinds of signals can be sufficiently described by non-lineardynamical analysis [32, 37, 40, 54].The first representative in this category is CD (Correlation Dimension) which statistically measuresattractor geometry in the phase space. This measure is related to a number of independent variables9necessary for generating the attractor [32, 45, 47]. Another dimension measure FD (Fractal Dimension)is based on a number of basic building blocks that form a pattern [32, 68]. We will also use complexitymeasures like ZL (Ziv-Lempel complexity) which quantify the regularity embedded in a time series [69].Possible long-term dependencies in the speech signal will be described by HE (Hurst Exponent) [54].Another set of measures is based on entropy. We will use SHE (Shannon Entropy), RE (second-orderR´enyi Entropy) [70], CE (Correlation Entropy) [40, 70], RBE1 (first-order R´enyi Block Entropy) [40], RBE2(second-order R´enyi Block Entropy) [40], AE (Approximate Entropy) [32, 71, 72], SE (Sample Entropy) [72]and RPDE (Recurrence Probability Density Entropy) [37]. Generally the entropy is a measure of uncertaintyand it can be used to quantify the complexity of a system. R´enyi entropies quantify the loss of information intime in a dynamic system [40], correlation entropy gives an indication of the predictability of the nonlineartime series [70] and RPDE represents the uncertainty in the measurement of the pitch period [37]. Theonly difference between AE and SE is that SE does not evaluate a comparison of embedding vectors withthemselves.Another measure we have considered in this work is FMMI (First Minimum of Mutual Informationfunction) which was found by Henriquez et al. as a feature that better discriminates among the differentvoice qualities of the multiquality database [40]. To include also a measure of sensitivity to an initialcondition, the LLE (Largest Lyapunov Exponent) was selected [32, 54]. We also used detrended fluctuationanalysis (DFA) to characterize the self-similarity of the graph of a signal from a stochastic process [34, 37].In this field NSE (Normalized Scaling Exponent) and FA (Fluctuation Amplitudes) were evaluated.2.9. High-level FeaturesThe features that are calculated for each speech segment separately form a vector or a matrix at theoutput of the parametrization process. This representation must be then transformed to a scalar value tobe able to carry out the next processing like statistical analysis, classification etc. This is usually doneby an extraction of some kind of statistics. These statistics are called high-level features while parametersextracted directly from the speech signal are called local features.If the local feature is represented bya matrix, then the high-level feature is calculated for each row separately (this monitors feature changes intime). We have extracted 60 high-level features:• max, min, position of max, position of min, relative pos. of max, relative pos. of min• range, relative range, interquartile range, rel.interquartile range, interdecile range, rel.interdecilerange, interpercentile range, rel. interpercentile range, studentized range• mean, geometric mean, harmonic mean, mean excluding 10 %, 20 %, 30 %, 40 % and 50 %, of outliers,median, mode10• var, std, mean absolute deviation, median absolute dev., geometric standard dev., coefficient of varia-tion, index of dispersion• 3rd, 4th, 5th and 6th moment, kurtosis, skewness, Pearson’s 1st and 2nd skewness coefficient• 1st, 5th, 10th, 20th, 30th, 40th, 60th, 70th, 80th, 90th, 95th and 99th percentile, 1st and 3rd quartile• slope, offset and error of linear regression• modulation, Shannon entropy, second-order R´enyi entropy2.10. Newly Designed FeaturesIn sec. 2.1 to sec. 2.8 we have presented 92 local features that are already used in the field of speech signalprocessing. However during the experiments these parameters will be extended by the other 36 features thatare originally introduced in this work.2.10.1. Features Based on Modulation SpectraAn initial step of modulation spectra calculation employs a short-time Fourier transform (STFT) of thediscrete input speech signal s[n] with length N :S[k, m] =N −1(cid:88)n=0s[n]w[n − mL]e−jk 2πN n,(1)k = 0, 1, . . . , N − 1,m = 0, 1, . . . , M − 1,where w[n] is a window function (in our case a Hamming window) with a step of L samples and M numberof speech segments. Consequently the power spectra |S[k, m]|2 is extracted and filtered by a bank ofP triangular filters equidistantly spaced out in the mel scale. This procedure forms a matrix X[p, m]with subbands p = 1, 2, . . . , P . A distribution of amplitudes of each subband envelope X[p, m] of thevoiced speech signal has a strong exponential component which is suppressed by logarithmization and meansubtraction [33]:ˆX[p, m] = ln (X[p, m]) − ln (X[p, m]),(2)where ∗ corresponds to the average operator over m. Next the frequency analysis of subband envelopes isperformed using the discrete Fourier transform (DFT):Ψ[p, l] =M −1(cid:88)m=0ˆX[p, m]e−jl 2πM m,l = 0, 1, . . . , M − 1,11(3)where p and l denote the acoustic and modulation frequency respectively. In the last step of modulationspectra extraction the second power of each subband is taken and normalized which partially suppresses theeffect of training and testing conditions mismatch:Ψn[p, l] =Ψ[p, l]l Ψ[p, l](cid:80).(4)The features we are proposing are based on a function ψ[l] which is extracted from the normalizedmodulation spectra according to:ψ[l] =P(cid:88)p=1Ψn[p, l].(5)Due to the instability of vocal fold vibrations pathological speech exhibits larger energy spread on highermodulation frequencies. This fact can be sufficiently expressed in function ψ[l]. Fig. 1 illustrates function ψ[l]calculated for healthy and pathological female vowels [a] obtained from the database Pr´ıncipe de Asturias(PdA) Hospital in Alcal´a de Henares of Madrid [73, 74]. As it can be seen the peak of function ψ[l] in thecase of healthy voice is higher, narrower and more shifted to lower modulation frequencies than in the caseof the pathological one.Figure 1: Function ψ[l] calculated for healthy and pathological female vowels [a] obtained from the database PdA [73, 74].The proposed features derived from ψ[l] are MSER (Modulation Spectra Energy Ratio), MFP (Mod-ulation Frequency of Peak) and RPHM (Relative Peak Height of Modulation spectra). MSER is definedas:MSER =ψ[l]l5 Hz(cid:80)l=0M −1(cid:80)l=l5 Hz+1,ψ[l](6)where l5 Hz is a sample corresponding to 5 Hz modulation frequency (this limit was empirically found). MFPis defined as:MFP = arg maxl12(ψ[l]) ,(7)0246810050010001500l [Hz]y[l] healthy voicepathological voicewhere MFP is given in Hz. Finally RPHM can be calculated according to the following expression:RPHM =,(8)ψ[i] − r[i]ψ[i]i = arg max(ψ[l]) ,lwhere r is a linear regression line of ψ[l] for l = l5 Hz, . . . , M − 1 and i is the index at which the function ψ[l]reaches maximum value.2.10.2. Features Based on Inferior Colliculus CoefficientsUsing inferior colliculus coefficients (ICC) it is possible to extract the frequency content of the modulationenvelopes applied to different bands of an auditory stimuli [66]. Similarly to modulation spectra, the ICCextraction process employs at the beginning STFT and consequent calculation of power spectra |S[k, m]|2.However in the next step instead of the bank of the triangular filters, the bank of P mel-spaced gammatonefilters is applied. We used filters defined by the the impulse response:g[n] =(cid:19)o−1(cid:18) nfs(cid:18) 2πfcnfsb = 24.7 (cid:0)4.37−3fc + 1(cid:1) ,· cos(cid:19)−2πbnfs,· e(9)where fc is the center frequency in Hz, o the filter order (in our case 4) and fs the sampling frequency inHz. The DFT on the subband envelopes was applied next, this time without prior to normalization:T [p, l] =∞(cid:88)m=−∞X[p, m]e−jl 2πM m.(10)Finally the magnitude spectrum of each envelope |T [p, l]| is filtered by a bank of Q = 13 resonance filterswith exponentially spaced frequencies from 12 to 107 Hz. In this work we used resonance filters defined bythe following transfer function:H(z) =0.1z2 − 0.09(cid:17)(cid:16) 2πfcfsz2 − 1.8 cosz + 0.81.(11)At the output of this process a matrix Ξ[p, q] (q = 1, 2, . . . , Q) of ICC for the input signal s[n] is extracted.The proposed features are based on a function ξ[p] which is extracted from Ξ[p, q] according to:ξ[p] =Q(cid:88)q=1ln (Ξ[p, q]) .(12)Contrary to ψ[l] this function reflects the misplacement of articulators better than the instability of vocalfold vibrations. In Fig. 2 it is possible to see an example of ξ[p] calculated for the pathological and healthyvoice obtained from the PdA database. The features derived from ξ[p] are ICER (Inferior Colliculus Energy13Ratio) and RPHIC (Relative Peak Height of Inferior Colliculus). The ratio ICER is defined by the followingexpression:ICER =12(cid:80)p=1ξ[p]20(cid:80)p=13ξ[p].(13)Similarly to RPHM the RPHIC is extracted using the regression line but in this case r[p] is calculated forp = 13, . . . , 20:RPHIC =ξ[i] − r[i]ξ[i],i = arg maxp(ξ[p]) .(14)Figure 2: Function ξ[p] calculated for the healthy and pathological female vowels [a] obtained from the database PdA [73, 74].2.10.3. Features Based on BicepstrumWe will use a definition of the real bicepstrum c[n1, n2] similar to the definition of the real cepstrum:c[n1, n2] =1N 2N −1(cid:88)k1,k2=0ˆB[k1, k2]ej 2πN (k1n1+k2n2),ˆB[k1, k2] = ln (|B[k1, k2]|) ,(15)(16)where the bispectrum B[k1, k2] is calculated as the DFT of triple correlation or circular triple correlationγ[n1, n2] [75]. In our work we used the second approach based on γ[n1, n2]:N −1(cid:88)n1,n2=0γ[n1, n2]e−j 2πN (k1n1+k2n2),(17)B[k1, k2] =γ[n1, n2] =N −1(cid:88)δ[n]δ[n + n1]δ[n + n2],1Nn=0δ[i] = s[(n + i) mod N ].145101520200250300px[p] heal.path.The first feature we are proposing in the present work is BCII (BiCepstral Index Interference):BCII =b[n1, n2] =1|max (b[n1, n2])|·1N 2 − 1N −1(cid:88)·N −2(cid:88)|b[n1, n2 + 1] − b[n1, n2]| ,n2=0(cid:12)(cid:12)cm[n1, n2](cid:12)(cid:12),|cm[n1, n2]|n1=0(cid:12)M −1(cid:12)(cid:80)(cid:12)(cid:12)m=0M −1(cid:80)m=0(18)(19)where cm[n1, n2] is the bicepstrum calculated from the mth speech segment. The two features introducednext are HFEBC (High Frequency Energy of one-dimensional BiCepstral index) and LFEBC (Low FrequencyEnergy of one-dimensional BiCepstral index):LFEBC =HFEBC =L(cid:80)n=0N −1(cid:80)n=0ρ[n],ρ[n]N −1(cid:80)n=L+1N −1(cid:80)n=0ρ[n],ρ[n]ρ[n] = c[n, n],L =fsfmax,(20)(21)(22)where ρ[n] (for n = 0, 1, . . . , N − 1) is the one-dimensional bicepstral index and fmax the maximum expectedfundamental frequency (in our case fmax = 350 Hz).It is supposed that pathological voice contains much more white noise (symmetrically distributed) thanhealthy voice. This is especially due to incorrect glottal closure. This kind of noise disappears in the realcepstrum estimated by ρ[n] (see Fig. 3), therefore a difference between the real cepstrum c[n] (estimatedusing DFT) and the one-dimensional bicepstral index can estimate noise components of the analyzed signal.According to this idea BCMII (BiCepstrum Module Interference Index) and BCPII (BiCepstrum PhaseInterference Index) are proposed:BCMII =BPMII =1max (ηm[m])·|ηm[m + 1] − ηm[m]| ,1max (ηp[m])·1N 2 − 1M −2(cid:88)m=01N 2 − 1M −2(cid:88)··|ηp[m + 1] − ηp[m]| ,m=015(23)(24)ηm[m] =ηp[m] =N −1(cid:88)n=0N −1(cid:88)n=0(|cm[n]| − |ρm[n]|)2 ,(ang (˜cm[n]) − ang (˜ρm[n]))2 ,(25)(26)where cm[n] and ρm[n] are the mth frame real cepstrum and the one-dimensional bicepstral index respectively.˜cm[n] is the complex cepstrum and ˜ρm[n] is the one-dimensional bicepstral index where the absolute valuein eq. (16) was not taken.Figure 3: Comparison of the one-dimensional bicepstral index ρ[n] and the real cepstrum c[n] calculated for a maintained vocal[a] uttered by a healthy speaker. Only first 101 samples are displayed.The other features, based on c[n] and ρ[n] are LCBCER (Low Cepstra/BiCepstra Energy Ratio) andHCBCER (High Cepstra/BiCepstra Energy Ratio):LCBCER =HCBCER =L(cid:80)n=0L(cid:80)n=0|c[n]|,|ρ[n]|N −1(cid:80)n=L+1N −1(cid:80)n=L+1|c[n]|.|ρ[n]|(27)(28)According to these equations we also propose the features LSBER (Low Spectra/Bispectra Energy Ratio) andHSBER (High Spectra/Bispectra Energy Ratio), however the ratios of spectrum S[k] and one-dimensionalbispectral index ϑ[k] for L = (cid:98)N/2(cid:99) are calculated in this case.The other two features BCMD (BiCepstral Module Distance) and BCPD (BiCepstral Phase Distance)are based on distance measures. The values of these features for the mth speech segment are:BCMD =N −1(cid:88)||˜cm+1[n1, n2]| −(29)n1,n2=0− |˜cm[n1, n2]|| ,16020406080100-0.200.2nc[n], r[n] c[n]r[n]BCPD =N −1(cid:88)|ang (˜cm+1[n1, n2]) −n1,n2=0− ang (˜cm[n1, n2])| ,(30)where ˜cm[n1, n2] is the complex bicepstrum calculated without the absolute value in eq. (16). Similarly toeq. (29) and (30) features BMD (Bispectral Module Distance) and BPD (Bispectral Phase Distance) are alsoextracted, where Bm[k1, k2] is used instead of ˜cm[n1, n2].2.10.4. Different Kernel Based Approximate and Sample EntropyApproximate entropy (AE) is a measure of regularities inside the analyzed time series. The advantageof AE is that it can robustly estimate the system complexity using just a limited number of samples (100–5000) [76]. To define AE we need to firstly reconstruct a state space using Takens’ embedding theorem: [77]x[n] = [s[n], s[n + τ ], . . . , s[n + (m − 1)τ ]] ,(31)n = 0, 1, . . . , N − 1 − (m − 1)τ,where x[n] is an embedding vector, m is an embedding dimension and τ is a time delay (τ = 1 for AE).Next, the regularity quantity of a particular pattern C[i, m, r] is defined as:C[i, m, r] =1N − mN −m(cid:88)j=0κ (i, j, r) .Finally AE can be calculated according to expression: [32]AE = Φ[m, r] − Φ[m + 1, r],Φ[m, r] =1N − mN −m(cid:88)i=0ln (C[i, m, r]) .(32)(33)(34)A disadvantage of AE is its dependence on the signal length due to the self comparison of points in theattractor. This fact can be avoided using the sample entropy (SE) which does not evaluate the comparisonof embedding vectors among themselves: [78]SE = Γ[m, r] − Γ[m + 1, r],Γ[m, r] =Cx[i, m, r] =1N − m1N − mN −m(cid:88)ln (Cx[i, m, r]) ,i=0N −m(cid:88)j=0,i(cid:54)=jκ (i, j, r) .(35)(36)(37)The usual values for embedding dimension are m = 1, 2. In this work we used m = 2 which provides moredetailed reconstruction. Finally, a function must be defined κ (i, j, r) we call kernel function. Originally the17AE or SE are based on:κ (i, j, r) = Θ{r − d (x[i], x[j])},d (x[i], x[j]) = maxk|s[i + k] − s[j + k]|,k = 0, 1, . . . , m − 1,where Θ is the Heaviside function and r a radius in our case calculated according to:r = 0.2std (s[n]) .(38)(39)(40)We will denote these entropies as AE(Heaviside) and SE(Heaviside) respectively. Orozco-Arroyave et al.proposed AE(Gaussian) and SE(Gaussian) based on the Gaussian kernel: [78]κ (i, j, r) = exp−(cid:18)(cid:107)x[i] − x[j](cid:107)210r2(cid:19).In this work we propose AE and SE based on the other 6 kernels: exponential kernelκ (i, j, r) = exp−(cid:18)(cid:107)x[i] − x[j](cid:107)2r2κ (i, j, r) = exp−(cid:18)(cid:107)x[i] − x[j](cid:107)r(cid:19)(cid:19);;Laplacian kernel,circular kernel,κ (i, j, r) =(cid:18)arccos2π−(cid:19)(cid:107)x[i] − x[j](cid:107)r(cid:18) (cid:107)x[i] − x[j](cid:107)r−(cid:19)2,−2π(cid:107)x[i] − x[j](cid:107)r(cid:115)1 −for (cid:107)x[i] − x[j](cid:107) < r, zero otherwise; spherical kernel,κ (i, j, r) = 1 −3(cid:107)x[i] − x[j](cid:107)2r(cid:18) (cid:107)x[i] − x[j](cid:107)r+(cid:19)3,+12for (cid:107)x[i] − x[j](cid:107) < r, zero otherwise; Cauchy kernel,for (cid:107)x[i] − x[j](cid:107) < r, zero otherwise and triangular kernel,κ (i, j, r) =11 + (cid:107)x[i]−x[j](cid:107)2r,for |x[i] − x[j]| < r, zero otherwise.κ (i, j, r) = 1 −|x[i] − x[j]|r,18(41)(42)(43)(44)(45)(46)(47)2.10.5. Features Based on Empirical Mode DecompositionAs was mentioned in sec. 2.7 Tsanas et al. proposed several measures of SNR and NSR based on EMD: [34]IMF-SNR =IMF-NSR =I(cid:80)i=43(cid:80)i=12(cid:80)i=1I(cid:80)i=3µiµi,ˆµiˆµi,(48)(49)where µi is a parameter, or mean sequence value, calculated from the original ith IMF and I is the totalnumber of the IMFs. In the case of ˆµ the ith IMF was logarithmized before the consequent parametrization.To extract the sequence from IMF, Tsanas et al. used SEO (Squared Energy Operator) and TKEO. Asa parameter they also used SHE.We have extended this idea on these parameters: IMF-SNRRE (based on second-order R´enyi Entropy),IMF-SNRZCR (based on Zero-Crossing Rate) and IMF-NSRRE. In the case of IMF-SNRRE µi is defined as:J(cid:88)µi = − log2p2 (cid:0)xij(cid:1) ,(50)j=1where p2 (cid:0)xi(cid:1) is the probability P (cid:0)IMFi = xi1, xiIn the case of IMF-SNRZCR µi is calculated according to:(cid:1) and (cid:8)xijj2, . . . , xiJ(cid:9) are the possible values of the ith IMF.µi =1NN −1(cid:88)n=1|sgn (fi[n]) − sgn (fi[n − 1])| ,(51)where fi[n] is the ith IMF.The time-varying high frequency components present in the 1st IMF represent the noise part of thespeech signal. We propose a new feature IMF-FD which is based on the fractal dimension calculated fromthe 1st IMF. This complexity measure appropriately quantifies the amount of noise present in the signal andindirectly describes hoarseness or breathiness. The feature is defined as:IMF-FD =log10 N(cid:16)log10 N + log10(cid:17)NN +0.4NchNch =N −1(cid:88)n=1|sgn (f1[n]) − sgn (f1[n − 1])| .(52)Another feature based on the first intrinsic mode function is IMF-CPP (Cepstral Peak Prominenceextracted from the 1st IMF). It can be calculated as:IMF-CPP =c[i] − r[i]c[i],19(53)i = arg maxn(c[n]) ,n =fsfmax, . . . , N − 1,where c[n] is real cepstrum calculated from f1[n] and r[i] is a linear regression line of c[n] for n =fs/fmax, . . . , N − 1. We consider fmax = 350 Hz.The last feature proposed in this work is IMF-GNE (Glottal-to-Noise Excitation ratio based on the 1stIMF). The whole procedure of IMF-GNE extraction can be described in following steps:1. Segment the speech signal and extract the 1st IMF for each frame.2. Repeat step 3–6 for each segment.3. Do an inverse filtering of the 1st IMF.4. For each band of 1000 Hz with 1000 Hz step get the Hilbert envelope (the absolute value of analyticalsignal).5. Calculate cross correlation functions for all possible combinations of Hilbert envelopes and pick themaximum of each function.6. Pick the maximum from all the maxima in 5.3. DatabasesTo provide robust results 3 (English, Spanish and Czech) databases have been used during the testingprocedure. Each database represents a different language group (Germanic, Romanic and Slavic). Thisapproach is advantageous from the cultural difference point of view. Speakers of different languages exhibitespecially different prosodic characteristics. The aim of this work is to find features significant for theparticular language, but we have focused on a selection of features that are language-independent as well.3.1. MEEI Disordered Voice DatabaseThe Massachusetts Eye and Ear Infirmary (MEEI) database [25] has been for many years used asa benchmark in the field of pathological speech analysis. This commercially available database consists of53 healthy and 657 pathological speakers with different pathologies (e. g. adductor spasmodic dysphonia,conversion dysphonia, erythema, hyperfunction, etc.). The data of each speaker contain 12 s of a standardtext “The Rainbow Passage” [79] and a sustained phonation of the vowel [a] pronounced as in the word“father”. The recordings are sampled at fs = 50 kHz or fs = 25 kHz. For our purpose only the vowels [a]are used.Although MEEI consists of approximately 700 speakers in total, the age distributions between normaland pathological speakers are not matched. Therefore the amount of pathological speakers is usually lim-ited to 173 according to criteria published by Parsa and Jamieson (we call this “limited version of MEEI20Table 1: Statistical characteristics of the MEEI database used in this work.NumberMean ageAge rangeSTD of ageSpeakersMaleFemale MaleFemale MaleFemale MaleFemaleHealthyPathological21703210338.8141.734.1637.5926–5922–5226–5821–518.499.407.878.19database”) [15]. The statistical characteristics of MEEI database used in this work can be found in Table 1.Although this database is very popular and very often used, its size and data are considered insufficient.Table 2 shows detection results (obtained on the MEEI database) of several works. The highest accuracyreached by Henriquez et al. is 99.69 % [40]. However using the same features and same experimental setupthe authors achieved an accuracy of 82.47 % using the “Multiquality” database [40]. This fact shows thatdespite the popularity of the MEEI database there is a need to introduce new, larger and more complex(when considering the speech tasks) benchmark databases in order to provide more reliable results andconclusions. A discussion on the reliability of results mentioned in Table 2 can be found in sec. 3.4.Table 2: Summary of pathological speech detection results obtained on the MEEI database.ReferenceAccuracy [%]Henriquez et al. (2009) [40]Diabazar et al. (2002) [80]Parsa and Jamieson (2000) [15]Alpan et al. (2010) [81]Hariharan et al. (2010) [82]Arias-Londono et al. (2011) [83]99.6999.4498.7098.7098.4598.233.2. PdA DatabaseThe second database we have used is Pr´ıncipe de Asturias (PdA) database [73, 74]. This databaseconsists of 239 healthy and 200 pathological speakers with different organic pathologies (e. g. nodules,polyps, oedemas, carcinomas, etc.). Every speaker uttered a sustained Spanish vowel [a]. The recordingsare sampled at fs = 25 kHz. The statistical characteristics of this database can be found in Table 3. Table 4shows detection results (obtained on the PdA database) of two available works. As can be seen, the accuraciesare not so high as in the case of MEEI database. Moreover, the PdA consists of more speakers than thelimited version of MEEI. All these facts highlight the high potential of the PdA for future use.3.3. PARCZ DatabaseThe last database we have included in our test is the Czech Parkinsonian Speech Database (PARCZ)recorded at St. Anne’s University Hospital in the Czech Republic. This database consists of 52 healthy21Table 3: Statistical characteristics of the PdA database used in this work.NumberMean ageAge rangeSTD of ageSpeakersMaleFemale MaleFemale MaleFemale MaleFemaleHealthyPathological1017413812634.4448.0535.2936.7118–7811–768–719–7216.2413.8914.7313.14Table 4: Summary of pathological speech detection results obtained on the PdA database.ReferenceAccuracy [%]Arias-Londono et al. (2011) [74]Vasilakis and Stylianou (2009) [39]84.1577.68Table 5: Statistical characteristics of the PARCZ database used in this work.NumberMean ageAge rangeSTD of ageSpeakersMaleFemale MaleFemale MaleFemale MaleFemaleHealthyPathological2636262165.6566.2262.1568.8149–8345–8746–8749–869.029.209.509.00speakers and 57 speakers with Parkinson’s disease (PD) who suffer from hypokinetic dysarthria [84]. Thisdatabase contains 91 speech tasks (free speech, reading text, maintained vowels, diadochokinetic tasks, etc.)which are used for an analysis of speech dysfunctions that usually accompany PD. However for our purposeonly the sustained Czech vowel [a] is used. The recordings are sampled at fs = 48 kHz. The statisticalcharacteristics of the PARCZ database can be found in Table 5. As it can be seen, contrary to the MEEI orPdA, PARCZ is more focused on elder people.3.4. The Rule of 30To decide whether the corpus size is sufficient for the robust conclusions Doddington et al. introduced“the rule of 30” which comes directly from the binomial distribution, assuming independent trials [85]. Therule is “To be 90 % confident that the true error rate is within ±30 % of the observed error rate, there mustbe at least 30 errors.”If we apply this rule to the original MEEI database (710 speakers), then the observed error rates below4.25 % cannot be considered as reliable. Moreover if the MEEI database is limited to 226 speakers, thenthreshold reaches 13.27 %. In other words, although the results in Table 2 predict promising approaches forpathologic speech detection results, we must be critic about these values.If we apply this rule to the PdA (436 speakers) or PARCZ (109 speakers) databases, we get the thresholdsof 6.88 % and 27.52 % respectively. According to this values it can be said that the results obtained withthe PdA are more reliable (contrary to PARCZ).224. ExperimentsAll the databases were resampled to fs = 16 kHz and in dependence on the next processing the datahave been divided into 9 groups. Two approaches have been considered: gender-dependent and genderindependent. Each database is randomly divided into 75 % and 25 % training and testing subsets respectively.The classifier is evaluated consequently. This procedure (data split, classifier tuning and evaluation) isrepeated 100 times. The resulting accuracy (ACC), sensitivity (SEN ) and specificity (SP E) are calculatedaccording to:ACC =SEN =SP E =T P + T NT P + T N + F P + F N· 100 [%],T PT P + F NT NT N + F P· 100 [%],· 100 [%],(54)(55)(56)where T P (True Positive) and F P (False Positive) represent the number of correctly identified pathologicalspeakers and number of speakers diagnosed as pathological, but being healthy. Similarly, T N (True Negative)and F N (False Negative) represent the total number of correctly identified healthy speakers, and pathologicalspeakers evaluated as healthy controls.Before classification the training data are z-score normalized. The testing data are normalized by sub-tracting the training set mean and dividing by the training set standard deviation for each feature.4.1. Parameterization ProgramsSeveral toolboxes and programs have been used for the purpose of feature extraction. Features based onthe detrended fluctuation analysis have been calculated using FastDFA [86]. The glottal quotients (GQopenand GQclosed) were extracted using the algorithm DYPSA implemented in VOICEBOX [87]. To estimatethe largest Lyapunov exponent (LLE) TSTOOL has been used [88]. The software Praat has been used toestimate the fundamental frequency F0, all kinds of jitter and shimmer, harmonic-to-noise ratio (HNR) andformant frequencies (F1, F2, F3) [89]. The rest of features (108 in total), including the features introducedin this work, have been implemented in the Neurological Disorder Analysis Tool (NDAT) developed at theBrno University of Technology [90].4.2. Feature SelectionConsidering all possible local and high-level feature combinations the parameterization process extractsapproximately 28,000 features for each speaker. This feature space reduced for each scenario using thefiltering feature selection approach based on the non-parametric Mann-Whitney U test. The significancelevel was set to α = 0.05. This feature selection method is relatively simple, but there are several seriousstudies indicating that there are scenarios where simple univariate methods perform similarly or even better23than more complex methods. For instance Haury et al. show that a simple Student’s t-test provides betterresults than SVM-RFE (Support Vector Machine Recursive Feature Elimination), GFS (Greedy ForwardSelection), LASSO (Least Absolute Shrinkage and Selection Operator) or elastic net [91].After the evaluation a list of the ten most significant features is drawn up. Finally the density estimationplots (computed using kernel density estimation with Gaussian kernels) of the most significant features inall scenarios are given.4.3. Classification MethodsIn our test we have used 2 classifiers: SVM (Support Vector Machine with a radial kernel) and RF(Random Forest). Regarding the SVM, the parameter kernel gamma γ and penalty parameter C wereoptimized using a grid search for possible values.5. ResultsA summary of pathological speech detection results expressed by accuracy, sensitivity and specificity canbe found in Table 6. This table also provides some statistics related to the number of features selected in eachscenario. In the case of MEEI database the accuracy, sensitivity and specificity were equal to 100.0 ± 0.0 %in all scenarios (considering both genders together and separately) when using an RF classifier. A discussionfocused on the credibility of these results is given below. In comparison to RF the SVM classifier providedslightly worse results.In the case of the PdA database the best results were found when classifying by RF too. The accuracy(80.9 ± 5.1 %) and specificity (85.4 ± 6.7 %) are larger for male speakers while the sensitivity (77.2 ± 7.7 %) islarger for the female ones. When considering both genders together the accuracy (82.1±3.3 %) and sensitivity(80.0±5.9 %) reach the best values in the frame of all PdA scenarios, however the specificity is approximately1.6 % lower than in the case of the male-only scenario. In comparison to the best accuracies published byArias-Londono et al. our approach provides a lower accuracy by 2.05 %, however it should be highlightedthat the authors used classifier score fusion, which is not considered in this work [74]. When looking attheir single-classifier solution, the authors reported these values: 81.70 % (accuracy), 80.50 % (sensitivity),82.91 % (specificity). Comparing these values to our results, we can say that our approach provides betteraccuracy (by 0.40 %) and specificity (by 0.89 %), but worse sensitivity (by 0.50 %). Moreover, Arias-Londonoet al. used in their work an older version of PdA database which has only 199 healthy speakers, while ourversion has 239. Therefore our results should be more trustable. In comparison to the work of Vasilakis etal. our approach outperformed the classification results provided by the authors [39].Regarding the PARCZ database the results are much worse than in the case of MEEI or PdA. In scenarioC1 (female speakers) the best accuracy was 67.1±8.3 % (classification by RF), sensitivity 35.3±30.3 % (SVM)24and specificity 91.4±11.3 % (RF). The accuracy (67.3±10.7 %) and sensitivity (50.4±20.2 %), both obtainedusing the SVM, were slightly better in the case of male speakers. Vice versa, the specificity (83.6 ± 16.0 %)was worse. Finally in scenario C3 (both genders) the best accuracy was 67.9 ± 6.0 % (RF), sensitivity39.3 ± 14.9 % (SVM) and specificity 87.5 ± 8.5 % (RF). It should be also mentioned that in comparison toMEEI or PdA the PARCZ database exhibits much larger standard deviations.The poor classification results are caused probably by the fact that PD patients were in different pro-gression stages of hypokinetic dysarthria, from first to more advanced ones (this fact most likely explains thelarge standard deviations that were obtained). It means that we were performing a two-class classificationover the data that can be split into approximately 4 classes (1 healthy and 3 levels of dysarthria: mild,moderate and severe). It can be an issue for future work to develop a system that would not only identifythe presence of pathological speech, but also estimate the level of voice pathology. The PARCZ databaseis a good candidate for a development of such a system. The system can be interesting especially if we areable to detect the first stages of different disorders so that the doctors can start the treatment early andslow down the progress. One of the works that deals with this issue has been published by Henriquez etal. [40]When looking into a number of significant features selected by the Mann-Whitney U test, it can beconcluded that this number positively correlates with the classification accuracy. For example in the case ofboth genders the number of selected features is 15, 521 ± 231 (MEEI), 11, 540 ± 419 (PdA) and for PARCZonly 1, 750 ± 331.Table 6: Summary of pathological speech detection results represented as mean ± std [%] (SVM – Support Vector Machine witha radial kernel, RF – Random Forest, F – female, M – male, MF – all genders).ScenarioNo. of sel. featuresAccuracySensitivitySpecificityDataset Gender(mean ± std)SVMRFSVMRFSVMRFMEEIMEEIMEEIPdAPdAPdAPARCZPARCZFMMFFMMFFM13, 996 ± 28899.5 ± 1.5100.0 ± 0.099.3 ± 2.0100.0 ± 0.0100.0 ± 0.0100.0 ± 0.013, 561 ± 39899.2 ± 1.7100.0 ± 0.099.1 ± 2.1100.0 ± 0.099.3 ± 3.3100.0 ± 0.015, 521 ± 23199.9 ± 0.4100.0 ± 0.099.8 ± 0.5100.0 ± 0.099.9 ± 0.7100.0 ± 0.09, 726 ± 44775.7 ± 4.378.5 ± 4.972.8 ± 6.577.2 ± 7.778.4 ± 6.879.6 ± 7.39, 721 ± 45878.6 ± 5.180.9 ± 5.171.0 ± 10.174.7 ± 9.884.2 ± 6.185.4 ± 6.711, 540 ± 41977.7 ± 3.282.1 ± 3.374.9 ± 5.380.0 ± 5.980.1 ± 5.083.8 ± 5.12, 141 ± 41565.9 ± 11.967.1 ± 8.335.3 ± 30.310.3 ± 16.979.0 ± 15.391.4 ± 11.32, 121 ± 48967.3 ± 10.766.5 ± 10.350.4 ± 20.242.6 ± 21.679.4 ± 14.583.6 ± 16.0PARCZMF1, 750 ± 33165.4 ± 7.667.9 ± 6.039.3 ± 14.931.0 ± 14.679.3 ± 10.087.5 ± 8.5IDM1M2M3P1P2P3C1C2C3The ten most significant features selected by Mann-Whitney U test in all considered scenarios can befound in Tables 7 – 15. The density estimation plots (computed using kernel density estimation with Gaussiankernels) of the most significant features in these scenarios can be seen in Fig. 4. In all MEEI scenarios (M1 – 3)the 10 most significant features produce equivalent p values and they are sorted alphabetically. Regardingscenarios M1 (females) and M3 (both genders) the most discriminative features are those derived from25MSC (Modulation Spectra Coefficients) while in the case of M2 (males) features based on ACW (AdaptiveComponent Weighted coefficients) and FADFA (Fluctuation Amplitudes of Detrended Fluctuation Analysis)are mainly selected. Looking at Fig. 4 a) – c) it can be concluded that the relative interpercentile range offirst modulation spectra coefficients and the third moment of the first adaptive component weighted cepstralcoefficients show always a single value for pathological speech. Moreover, in the case of male speakers thesingle value also represents the healthy one.In other words, regarding the MEEI database we can usea very simple classifier (theoretically a decision tree with only one node) to differentiate between healthyand pathological speech. This fact supports our classification accuracies equal to 100.0 ± 0.0 %. But thequestion is: Are these results trustable enough?The MEEI database itself introduces many issues related to the credibility of the results. The mainproblem is that both the healthy and disordered speech were recorded in a different way. First of allthe healthy one was sampled at fs = 50 kHz and the disordered one at fs = 25 kHz. In our experimentall databases were resampled to fs = 16 kHz but some small differences will remain among signals. Butprobably the most serious problem is that the duration of a sustained vowel is 3 s and 1 s for healthy anddisordered voice respectively. Therefore all high-level or local features that somehow reflect the signal length(entropy, range, std, duration, etc.) can provide very good discriminative results. This can also be a caseof the relative interpercentile range mentioned in Table 7 and 9. Although it is a relative measure it isstill dependent on the length of the input vector. We made a small experiment repetitively and randomlygenerating vectors with length N = 10i for i = 1, 2, 3, 4 and with a normal distribution. After that wecalculated the relative interpercentile range of these vectors and found out that the value of this measure isincreasing with the decreasing vector length.To avoid the problem of different vowel durations one can skip features dependent on this property but wewould loose parameters that can be potentially very good candidates for pathological speech identification.The other approach is to take only a one-second segment from the healthy speech but in our opinion this isnot a good solution. The first second segment could be selected but then we are losing the sustained partof the signal (used for estimating jitter) and phonation trail. If we take only the sustained part, then wewill loose information about phonation onset and offset. It has been shown that an analysis of these partsis useful for example for the dysarthric speech description [90, 92]. Therefore there is no elegant solutionthat would not distort the results. Moreover, Malysla et al. mention that some speakers were recordedin different sites and over potentially different channels [66]. In conclusion, although the MEEI databaseis a very popular benchmark in the field of pathological speech analysis, the results obtained using thisdatabase should be taken very carefully. An introduction of a new English disordered voice database is thushighly important for the future evaluation of new state-of-the-art speech signal processing techniques.In scenario P1 (PdA, males) all the most significant features are based on UCPP (Unsmooth CepstralPeak Prominence). On the other hand, in the case of scenarios P2 (females) and P3 (both genders) all the26Table 7: 10 most significant features selected by Mann-Whitney U test in scenario M1: MEEI, females (MSC – ModulationSpectra Coefficients).Featurep valuerelative interpercentile range of 1st MSC2.8610 · 10−30relative interpercentile range of 10th MSC 2.8610 · 10−30relative interpercentile range of 11th MSC 2.8610 · 10−30relative interpercentile range of 12th MSC 2.8610 · 10−30relative interpercentile range of 13th MSC 2.8610 · 10−30relative interpercentile range of 14th MSC 2.8610 · 10−30relative interpercentile range of 15th MSC 2.8610 · 10−30relative interpercentile range of 16th MSC 2.8610 · 10−30relative interpercentile range of 17th MSC 2.8610 · 10−30relative interpercentile range of 18th MSC 2.8610 · 10−30Table 8: 10 most significant features selected by Mann-Whitney U test in scenario M2: MEEI, males (ACW – AdaptiveComponent Weighted coefficients, FADFA – Fluctuation Amplitudes of Detrended Fluctuation Analysis).Feature3rd moment of 1st ACW4th moment of 1st ACW5th moment of 1st ACW6th moment of 1st ACWmean absolute deviation of 1st ACWp value2.5336 · 10−212.5336 · 10−212.5336 · 10−212.5336 · 10−212.5336 · 10−21mean excluding 50 % outliers of 1st ACW 2.5336 · 10−21mean of 1st ACWoffset of linear regression of 1st ACWposition of max. of FADFArelative position of min. of FADFA2.5336 · 10−212.5336 · 10−212.5336 · 10−212.5336 · 10−21selected features are based on IMF-CPP (Cepstral Peak Prominence of first Intrinsic Mode Function) whichis originally introduced in this work. However looking at Tables 10 – 12 it is evident that the features listedhere correlate significantly (e. g. mean and median, std and var, etc.). We have not carried out an analysisof correlation in this work due to a large number of features, but some statistics related to the most popularones have been published, for example by Tsanas et al. [29]In the case of scenario C1 (PARCZ, females) there were selected features based on LPCC (Linear Pre-dictive Cepstral Coefficients), GNE (Glottal-to-Noise Excitation ratio), MPSD (Median of Power SpectralDensity) and GQopen (Qlottis Quotient – vocal folds are apart). Regarding scenario C2 (males), the 10most significant features are based on UCPP, LPCT (Linear Predictive Cosine Transform coefficients), CMS(Cepstral Mean Subtraction coefficients), IMF-NSRRE (Noise-to-Signal Ratio derived from IMF based onsecond-order R´enyi Entropy) and AE (Laplacian: Approximate Entropy based on Laplacian kernel). And27Table 9: 10 most significant features selected by Mann-Whitney U test in scenario M3: MEEI, all (MSC – Modulation SpectraCoefficients).Featurep valuerelative interpercentile range of 1st MSC1.0560 · 10−49relative interpercentile range of 10th MSC 1.0560 · 10−49relative interpercentile range of 11th MSC 1.0560 · 10−49relative interpercentile range of 12th MSC 1.0560 · 10−49relative interpercentile range of 13th MSC 1.0560 · 10−49relative interpercentile range of 14th MSC 1.0560 · 10−49relative interpercentile range of 15th MSC 1.0560 · 10−49relative interpercentile range of 16th MSC 1.0560 · 10−49relative interpercentile range of 17th MSC 1.0560 · 10−49relative interpercentile range of 18th MSC 1.0560 · 10−49Table 10: 10 most significant features selected by Mann-Whitney U test in scenario P1: PdA, females (UCPP – UnsmoothCepstral Peak Prominence).Featuremode of UCPPmean of UCPPmean excluding 10 % outliers of UCPP60th percentile of UCPPmean excluding 20 % outliers of UCPPmean excluding 40 % outliers of UCPPmedian of UCPPmean excluding 30 % outliers of UCPPmean excluding 50 % outliers of UCPP90th percentile of UCPPp value1.6224 · 10−192.9605 · 10−193.6871 · 10−194.2979 · 10−194.5230 · 10−194.9362 · 10−194.9719 · 10−195.0820 · 10−195.0820 · 10−196.6980 · 10−19finally, when considering both genders, there were selected features based on LFCC (Linear Frequency Cep-stral Coefficients), MPSD, FADFA and ICC (Inferior Colliculus Coefficients). Generally p values are muchlower than in the case of MEEI or PdA databases. This is also closely related to the poor classificationresults mentioned in Table 6.To summarize the discussion about the databases we can say that the MEEI database should no longerbe used as a benchmark. The more challenging one is PARCZ database, however it has some disadvantagesfrom the scientific point of view: it is in Czech language, which is not a widespread language in the world;it is focused only on PD people with hypokinetic dysarthria; and it is not really suitable for a binaryclassification (at least 4 classes should be considered). Probably the most suitable database for the evaluationof pathological speech identification methods is PdA. The classification accuracies are still challenging andthey can be significantly improved. Another advantage of this database is that it is freely available for28Figure 4: Density estimation plots (computed using kernel density estimation with Gaussian kernels) of the most significantfeatures in scenarios M1 – 3, P1 – 3 and C1 – 3. Black colour represents healthy speech and grey the pathological one. a) ScenarioM1 (MEEI, females): relative interpercentile range of 1st modulation spectra coefficients; b) Scenario M2 (MEEI, males): 3rdmoment of 1st adaptive component weighted cepstral coefficients; c) Scenario M3 (MEEI, all): relative interpercentile range of1st modulation spectra coefficients; d) Scenario P1 (PdA, females): mode of unsmooth cepstral peak prominence; e) ScenarioP2 (PdA, males): median absolute deviation of cepstral peak prominence of first intrinsic mode function; f) Scenario P3 (PdA,all): error of linear regression of cepstral peak prominence of first intrinsic mode function; g) Scenario C1 (PARCZ, females):coefficient of variance of 11th linear predictive cepstral coefficients; h) Scenario C2 (PARCZ, males): slope of unsmooth cepstralpeak prominence; i) Scenario C3 (PARCZ, all): interquartile range of 18th linear frequency cepstral coefficients.290.60.8111.521st MSC fiP fia)-6-4-20x 10-440121st ACW fiP fib)0.60.8111.521st MSC fiP fic)010200.020.040.060.080.10.12UCPP fiP fid)0.10.20.30.424681012IMF-CPP fiP fie)0.10.20.30.40.551015IMF-CPP fiP fif)-202012311th LPCC fiP fig)-0.1-0.0502468101214UCPP fiP fih)0.40.60.8123418th LFCC fiP fii)Table 11: 10 most significant features selected by Mann-Whitney U test in scenario P2: PdA, males (IMF-CPP – Cepstral PeakProminence of first IMF).Featuremedian absolute deviation of IMF-CPPinterquartile range of IMF-CPP60th percentile of IMF-CPP3rd quartile of IMF-CPPerror of linear regression of IMF-CPPmedian of IMF-CPPmean absolute deviation of IMF-CPPmean excluding 50 % outliers of IMF-CPP70th percentile of IMF-CPPmean excluding 40 % outliers of IMF-CPPp value6.4974 · 10−171.2600 · 10−168.6433 · 10−161.1901 · 10−151.3454 · 10−151.4129 · 10−152.0881 · 10−152.2461 · 10−152.3014 · 10−153.1541 · 10−15Table 12: 10 most significant features selected by Mann-Whitney U test in scenario P3: PdA, all (IMF-CPP – Cepstral PeakProminence of first IMF).Featureerror of linear regression of IMF-CPPmedian absolute deviation of IMF-CPPmean absolute deviation of IMF-CPP80th percentile of IMF-CPPinterquartile range of IMF-CPPstd. of IMF-CPPvar. of IMF-CPP3rd quartile of IMF-CPP70th percentile of IMF-CPPinterdecile range of IMF-CPPp value6.9443 · 10−321.3082 · 10−311.8834 · 10−314.5625 · 10−315.3469 · 10−315.5387 · 10−315.5387 · 10−315.7880 · 10−317.0861 · 10−317.4045 · 10−31research purposes. The only disadvantage is the limitation to Spanish language. This should not be sucha big problem in the case of sustained vowel [a], but there will certainly be cultural differences when dealingwith the spoken text analysis.Another question is, whether analysis of vowel [a] is really the best way to identify pathological speech, atleast in the field of vowels analysis (not considering the other speech tasks like read/repeated/spontaneouswords, sentences, etc.). Although most of researchers automatically use sustained vowel [a], just a fewpublications report classification accuracies based on analysis of the other vowels. For this purpose Henriquezet al. made an experiment where they tried to identify pathological speech based on analysis of 5 Spanishvowels separately ([a], [e], [i], [o], [u]) [40]. They observed that in comparison to the other vowels theclassification based on vowel [a] provides slightly better results. Probably the choice of vowel [a] is goodwhen classifying the pathological speech generally (but still this should be proved by robust testing in future).30Table 13: 10 most significant features selected by Mann-Whitney U test in scenario C1: PARCZ, females (LPCC – LinearPredictive Cepstral Coefficients, GNE – Glottal-to-Noise Excitation ratio, MPSD – Median of Power Spectral Density, GQopen –Qlottal Quotient (vocal folds are apart)).Featurecoeff. of var. of 11th LPCCposition of max. of GNEp value1.0832 · 10−041.3509 · 10−04index of dispersion of 11th LPCC 1.6228 · 10−04min. of MPSD1st percentile of MPSDmodulation of MPSDrelative range of MPSDharmonic mean of MPSDmodulation of GQopenrelative range of GQopen3.0797 · 10−043.1013 · 10−043.1013 · 10−043.1013 · 10−043.1085 · 10−044.0151 · 10−044.0151 · 10−04Table 14: 10 most significant features selected by Mann-Whitney U test in scenario C2: PARCZ, males (UCPP – Unsmooth Cep-stral Peak Prominence, LPCT – Linear Predictive Cosine Transform coefficients, CMS – Cepstral Mean Subtraction coefficients,IMF-NSRRE – Noise-to-Signal Ratio derived from IMF based on second-order R´enyi Entropy, AE(Laplacian) – ApproximateEntropy based on Laplacian kernel).Featureslope of UCPP40th percentile of 2nd LPCToffset of linear regression of 5th CMS1st quartile of 2nd LPCToffset of linear regression of IMF-NSRREmean excluding 40 % outliers of 2nd LPCTmean excluding 50 % outliers of 2nd LPCToffset of linear regression of AE (Laplacian)mean excluding 10 % outliers of 2nd LPCTmean excluding 20 % outliers of 2nd LPCTp value3.4500 · 10−054.2438 · 10−054.2438 · 10−055.2089 · 10−056.3797 · 10−057.0546 · 10−057.0546 · 10−057.0546 · 10−057.7966 · 10−057.7966 · 10−05However, as soon as we focus on a specific pathology, we can get better results when analysing another vowel.For instance Orozco-Arroyave et al. classified hypokinetic dysarthria in patients with Parkinson’s diseaseand found out that it is more advantageous to analyse vowel [o] [78].Our initial idea was also to try the inter-database classification (training the classifier on one databaseand testing it using the other one). However the classification results were very poor. It was caused mainly bythese two facts: 1) When we trained the classifier on MEEI database, the features reflecting the signal lengthwere selected. But these features were not so significant when testing them on PdA or PARCZ databases.2) The PARCZ database is very different from MEEI or PdA, because it contains only one specific speechdisorder (hypokinetic dysarthria), while the other two databases contain many different voice pathologies.31Table 15: 10 most significant features selected by Mann-Whitney U test in scenario C3: PARCZ, all (LFCC – Linear FrequencyCepstral Coefficients, MPSD – Median of Power Spectral Density, FADFA – Fluctuation Amplitudes of Detrended FluctuationAnalysis, ICC – Inferior Colliculus Coefficients).Featureinterquartile range of 18th LFCCp value1.8665 · 10−05median absolute deviation of 18th LFCC 1.9509 · 10−05min. of MPSDmodulation of MPSDrelative range of MPSDharmonic mean of MPSD40th percentile of FADFAmedian absolute deviation of 2nd ICC1.9970 · 10−052.0055 · 10−052.0055 · 10−052.1543 · 10−053.7451 · 10−055.7179 · 10−05mean excluding 10 % outliers of 2nd ICC 5.9621 · 10−05mean excluding 20 % outliers of 2nd ICC 5.9621 · 10−05From the feature selection point of view it is interesting to point out that many segmental parametershave been found significant. Some of them (MSC, ACW, LPCC, LPCT, CMS and ICC) were also selectedas the 10 most significant ones. Although the segmental features are not very frequent when analysingpathological speech (except MFCC, MSC and ICC), their potential seems to be high. In fact, to our bestknowledge, features like MFCCE, LFCC, CMS or ACW were used for this purpose for the first time in thepresent research.In this work we have introduced 36 new speech features. Just one (IMF-CPP) has been mentioned amongthe 10 most significant ones (Table 11 and 12), however the rest of them are significant (p < 0.05) as well,see Table 16. In the case of MEEI database we firstly checked if high-level features do not reflect the lengthof signal. If not we kept the feature in the table. If yes we checked the next most significant variant of thelocal parameter. At the top positions of this table we can see mainly features based on modulation spectra,bicepstrum and approximate entropy.6. ConclusionsThis work provides an insight into the robust and complex approach of pathological speech analysis. Toour best knowledge this is the first contribution providing a complex evaluation of feature significance fromdifferent fields of speech signal processing (e. g. speech analysis, recognition, coding, enhancement etc.). Itis also the first contribution deriving conclusions according to robust tests where 3 (English, Spanish, Czech)databases were used. These languages belong to 3 different language groups (Germanic, Romanic, Slavic).In general, the work has 4 goals, yet each of them has its conclusion.1) According to complex parameterization and consequent robust testing identify features that have thelargest discriminative power in the field of pathological speech analysis. Unfortunately most of the works32Table 16: Best significance levels (computed using the Mann-Whitney U test) selected for all 36 features originally introducedin this work (F – female, M – male, MF – all genders).Local featureHigh-level featureIMF-CPP (Cepstral Peak Prominence extractederror of linear regressionp value6.9443 · 10−32Sc. ID Dataset GenderP3PdAMFfrom the 1st IMF)MFP (Modulation Frequency of Peak)RPHM (Relative Peak Height of Modulation spec-tra)MSER (Modulation Spectra Energy Ratio)---BCPD (BiCepstral Phase Distance)harmonic meanHFEBC (High Frequency Energyofone-dimensional BiCepstral index)LFEBC (Low Frequency Energy of one-dimensional--BiCepstral index)AE (triangular kernel)offset of linear regressionBCII (BiCepstral Index Interference)-AE (exponential kernel)SE (exponential kernel)1st quartilemedianIMF-GNE (Glottal-to-Noise Excitation ratio basedmeanon the 1st IMF)AE (Cauchy kernel)AE (spherical kernel)SE (spherical kernel)meanoffset of linear regressionmean excluding 40 % outliersIMF-SNRRE (based on second-order R´enyi En-meantropy)LCBCER (Low Cepstra/BiCepstra Energy Ratio)meanIMF-FD (based on Fractal Dimension)ICER (Inferior Colliculus Energy Ratio)BMD (Bispectral Module Distance)AE (circular kernel)SE (circular kernel)SE (triangular kernel)BPD (Bispectral Phase Distance)median-median4th momentstdmeanmeanIMF-NSRRE (based on second-order R´enyi En-90th percentiletropy)IMF-SNRZCR (based on Zero-Crossing Rate)meanHCBCER (High Cepstra/BiCepstra Energy Ratio)1st percentileSE (Cauchy kernel)harmonic meanBCPII (BiCepstrum Phase Interference Index)RPHIC (Relative Peak Height of Inferior Collicu---lus)BCMD (BiCepstral Module Distance)harmonic meanHSBER (High Spectra/Bispectra Energy Ratio)meanAE (Laplacian kernel)SE (Laplacian kernel)offset of linear regression80th percentileBCMII (BiCepstrum Module Interference Index)-LSBER (Low Spectra/Bispectra Energy Ratio)mean excluding 20 % outliers3.2291 · 10−281.5614 · 10−272.9477 · 10−264.9663 · 10−192.5381 · 10−15M3M3M3M3M3MEEIMEEIMEEIMEEIMEEIMFMFMFMFMF2.5381 · 10−15M3MEEIMF7.7587 · 10−159.1669 · 10−151.4641 · 10−145.5444 · 10−146.0985 · 10−141.3344 · 10−131.4599 · 10−132.5551 · 10−133.1582 · 10−125.6242 · 10−121.4092 · 10−112.8611 · 10−115.3843 · 10−117.3659 · 10−101.3186 · 10−091.3839 · 10−091.0753 · 10−081.1298 · 10−081.5834 · 10−084.7190 · 10−085.1617 · 10−081.7589 · 10−072.2653 · 10−071.7894 · 10−063.2552 · 10−064.2554 · 10−067.5456 · 10−061.1755 · 10−043.7662 · 10−04P3P3P3P3M3M3P3P3P3M3M3M3M3P3P3P2P3P2M3P3M1M1M2M3M1P2P2P3M2PdAPdAPdAPdAMEEIMEEIPdAPdAMEEIMEEIMEEIMEEIMEEIPdAPdAPdAPdAPdAMEEIPdAMEEIMEEIMEEIMEEIMEEIPdAPdAPdAMEEIMFMFMFMFMFMFMFMFMFMFMFMFMFMFMFMMFMMFMFFFMMFFMMMFM33published in the field of pathological speech analysis provides conclusions based on a limited set of param-eterization methods. In other words there is still a lack of publications providing a complex overview offeatures quantifying pathological speech and providing strong conclusions supported by a robust testing.Our work is unique in this way, because together it provides testing based on 128 local features. We madethe wide overview of all these features used for pathological speech quantification so that the researcherscan find out what exactly the specific feature quantifies and how it can be implemented. We used featuresdescribing phonation, tongue movement, speech quality (including non-linear dynamic features and featuresbased on bispectrum/bicepstrum, empirical mode decomposition, wavelet decomposition) and segmentalfeatures.Using the non-parametric Mann-Whitney U test we observed that among all parameterization techniquesthose based on segmental features provide the best classification results. To our best knowledge this is thefirst work that tested the significance of 11 segmental features. The largest discriminative power can beobtained thanks to especially segmental features like modulation spectra coefficients, adaptive componentweighted coefficients and linear predictive cepstral coefficients. Although the segmental features are not veryfrequent when analysing pathological speech, their potential seems to be high. However, their disadvantage isthat they are usually difficult to be interpreted clinically. This is probably the reason they are not frequentlyused. Although they can provide good classification results they don’t say much about specific pathologyor speech dysfunction.2) Design new features that can quantify hoarseness, breathiness and non-linearities in pathological speechsignals. Clinical signs of vocal fold dysfunctions are usually associated with breathiness or hoarseness.Moreover voice can become aperiodic, noisy-like, and it is very difficult to find any regularities in the signal.Sometimes there is a frequent presence of sub-harmonics and chaos, which can lead to a failure of conventionaltechniques of speech signal analysis and which requires new parameterization methods developed specificallyfor pathological speech description.We introduced 36 new measures based on modulation spectra, inferior colliculus coefficients, bicepstrum,sample and approximate entropy and empirical mode decomposition. Features based on modulation spectraquantify instability of vocal fold vibrations and complements features based on inferior colliculus coefficientsthat reflect the misplacement of articulators. Due to incorrect glottal closure the pathological voice containsmuch more white noise that can be effectively quantified by proposed features derived from bicepstrum.New features based on empirical mode decomposition are able to describe the noise component of analysedsignal as well. Finally we proposed different kernel-based approximate and sample entropies to measureregularities inside the signal.These novel features were statistically processed by the non-parametric Mann-Whitney U test. All ofthem have been identified as significant and they have passed the feature selection process in at least onedatabase. Moreover, some of them were listed among top ten significant features selected in specific scenario34(they outperformed the other conventional features). In other words they helped to improve classificationresults in terms of accuracy, sensitivity and specificity and due to their effective quantification abilities theyhave high impact on the future work.3) Prove that the proposed large set parameterization approach can provide better classification results(with respect to classification accuracy, sensitivity and specificity) than those published in the field of patho-logical speech analysis by the other researchers. We tested the significance of all the mentioned parameterson 3 (English, Spanish, Czech) databases. In the case of the Massachusetts Eye and Ear Infirmary (MEEI)database we get accuracy, sensitivity and specificity equal to 100.0 ± 0.0 %, which are the best results thathave been published in the frame of this database (Henriquez et al. reached 99.69 % [40]). However, weare very critical with these results. Therefore we discussed their trustability and the viability of using theMEEI database as a benchmark for pathological speech signal analysis.The results obtained with the PdA database are more challenging. When we considered the single-classifier approach, we reached the accuracy 82.1 ± 3.3 %, which is the best among the published numbers(Arias-Londono et al. published 81.7 % [74]).Regarding the last Czech Parkinsonian Speech Database (PARCZ), we obtained poor accuracy (67.9 ±6.0 %), however, this is probably due to the fact that we used a binary classifier for a multi-class database.We would like to do more experiments with this database in a near future splitting the data into healthyspeech and mild, moderate and severe dysarthria.4) Select a database that has high potential for the future, especially in terms of speech features design,tuning and testing. Due to some issues related to vowels’ length, different sampling frequencies, differentrecording conditions, etc., the MEEI database should no longer be used as a benchmark. Results obtainedusing this database are not trustable. The more challenging one is PARCZ database. Unfortunately it isfocused only on parkinsonic people with hypokinetic dysarthria and it is not really suitable for a binaryclassification (at least 4 classes should be considered).Therefore the most suitable database for the evaluation of pathological speech identification methods isPdA, where the classification accuracies are still challenging and they can be significantly improved. More-over this database is freely available for research purposes.There are many works that deal with the development of pathological voice identification methods andthere is still a lot that can be improved in this field of science. However, the researchers should go furtherand focus not only on the pathological speech identification, but also on more sophisticated analysis thatwould be more helpful for doctors and that can make the treatment or diagnosis more effective. Probablythe most important challenges to face in the next decade are:1. Identification of particular voice pathology. Identification of pathological speech itself is not so interest-35ing. There are many pathologies (adductor spasmodic dysphonia, erythema, hypokinetic dysarthria,etc.) and the issue is to classify them individually. This can be very problematic, therefore we proposeto do some kind of clustering and split this big set into subsets, for example according to the waythey are reflected in speech (problems with tongue movement, improper work of soft palate, disorderedvocal folds, etc.).2. Identification of voice pathology in its first stage or estimation of its progress. This would enabledoctors to start the treatment very early and slow down the progress.We are going to deal with these issues in future works. However this research is very dependent on gooddatabases and especially in the case of voice pathology identification, as in its first stage, there is still a lackof suitable training data.AcknowledgmentsThis work was supported by project NT13499 (Speech, its impairment and cognitive performance inParkinson’s disease), GACR 102/12/1104, COST IC1206 and project “CEITEC, Central European Insti-tute of Technology”: (CZ.1.05/1.1.00/02.0068) from the European Regional Development Fund, FEDERand Ministerio de Econom´ıa y Competitividad TEC2012-38630-C04-03 and -04 (Kingdom of Spain). Thedescribed research was performed in laboratories supported by the SIX project; the registration numberCZ.1.05/2.1.00/03.0072, the operational program Research and Development for Innovation.ReferencesReferences[1] P. H. Dejonckere, Assessment of voice and respiratory function, in: M. Remacle, H. E. Eckel (Eds.), Surgery of Larynxand Trachea, Springer Berlin Heidelberg, 2010, pp. 11–26.[2] J. G. Svec, J. Sundberg, S. Hertegard, Three registers in an untrained female singer analyzed by videokymography,strobolaryngoscopy and sound spectrography, J Acoust Soc Am 123 (1) (2008) 347–353.[3] I. R. Titze, Principles of voice production, Prentice Hall, 1994.[4] J. P. Dworkin, R. J. Meleca, Vocal Pathologies: Diagnosis, Treatment, and Case Studies, Singular Publishing Group,1997.[5] J. Illes, Neurolinguistic features of spontaneous language production dissociate three forms of neurodegenerative disease:Alzheimer’s, Huntington’s, and Parkinson’s, Brain Lang 37 (4) (1989) 628–642.[6] A. Habash, C. Guinn, D. Kline, L. Patterson, Language analysis of speakers with dementia of the Alzheimer’s type, Annalsof the Master of Science in Computer Science and Information Systems at UNC Wilmington 6 (1) (2012) 8–13.[7] K. Lopez-de Ipina, J.-B. Alonso, C. M. Travieso, J. Sole-Casals, H. Egiraun, M. Faundez-Zanuy, A. Ezeiza, N. Barroso,M. Ecay-Torres, P. Martinez-Lage, U. M. d. Lizardui, On the selection of non-invasive methods based on speech analysisoriented to automatic Alzheimer disease diagnosis, Sensors 13 (5) (2013) 6730–6745.36[8] K. Horley, A. Reid, D. Burnham, Emotional prosody perception and production in dementia of the Alzheimer’s type, JSpeech Lang Hear Res 53 (5) (2010) 1132–1146.[9] R. S. Bucks, S. A. Radford, Emotion processing in Alzheime’s disease, Aging Ment Health 8 (3) (2004) 222–232.[10] C. Gobl, A. N. Chasaide, The role of voice quality in communicating emotion, mood and attitude, Speech Commun40 (1-2) (2003) 189–212.[11] M. Hirano, Psycho-acoustic evaluation of voice, Springer-Verlag, 1981.[12] R. J. Baken, R. F. Orlikoff, Clinical Measurement of Speech and Voice, Singular Thomson Learning, 2000.[13] J. R. Deller, J. G. Proakis, J. H. Hansen, Discrete Time Processing of Speech Signals, Prentice Hall PTR, 1993.[14] J. Kuo, E. B. Holmberg, R. E. Hillman, Discriminating speakers with vocal nodules using aerodynamic and acousticfeatures, in: IEEE International Conference on Acoustics, Speech, and Signal Processing, Vol. 1, 1999, pp. 77–80.[15] V. Parsa, D. G. Jamieson, Identification of pathological voices using glottal noise measures, J Speech Lang Hear Res 43 (2)(2000) 469–485.[16] P. J. Murphy, O. O. Akande, Noise estimation in voice signals using short-term cepstral analysis, J Acoust Soc Am 121 (3)(2007) 1679–1690.[17] P. Alku, Parameterisation methods of the glottal flow estimated by inverse filtering, in: Voice Quality: Functions, Analysisand Synthesis, 2003, pp. 81–87.[18] R. Orr, B. Cranen, F. I. D. Jong, An investigation of the parameters derived from the inverse filtering of flow andmicrophone signals, in: Voice Quality: Functions, Analysis and Synthesis, 2003, pp. 35–40.[19] J. Godino-Llorente, P. Gomez-Vilda, T. Lee, Analysis and signal processing of oesophageal and pathological voices,EURASIP J Adv Sig Pr 2009 (1) (2009) 1–4.[20] N. Roy, J. Barkmeier-Kraemer, T. Eadie, M. P. Sivasankar, D. Mehta, D. Paul, R. Hillman, Evidence-based clinical voiceassessment: A systematic review, Am J Speech Lang Pathol 22 (2) (2013) 212–226.[21] P. Gomez-Vilda, V. Rodellar-Biarge, V. Nieto-Lluis, C. Munoz-Mulas, L. Mazaira-Fernandez, R. Martinez-Olalla,A. Alvarez-Marquina, C. Ramirez-Calvo, M. Fernandez-Fernandez, Characterizing neurological disease from voice qualitybiomechanical analysis, Cogn Comput 5 (4) (2013) 399–425.[22] Saarbrucken voice database (June 2014).URL http://www.stimmdatenbank.coli.uni-saarland.de/help_en.php4[23] Speech and language data repository (June 2014).URL http://crdo.up.univ-aix.fr/[24] M. M. Hakkesteegt, M. P. Brocaar, M. H. Wieringa, The applicability of the dysphonia severity index and the voicehandicap index in evaluating effects of voice therapy and phonosurgery, J Voice 24 (2) (2010) 199–205.[25] Massachusetts eye and ear infirmary, voice disorders database, version 1.03, CD-ROM, kay Elemetrics Corp., Lincoln Park,NJ (1994).[26] I. R. Titze, B. H. Story, Rules for controlling low-dimensional vocal fold models with muscle activation, J Acoust Soc Am112 (3) (2002) 1064–1076.[27] P. Alku, Glottal inverse filtering analysis of human voice production - a review of estimation and parameterization methodsof the glottal excitation and their applications, Sadhana 36 (5) (2011) 623–650.[28] J. I. Godino-Llorente, P. Gomez-Vilda, M. Blanco-Velasco, Dimensionality reduction of a pathological voice quality as-sessment system based on Gaussian mixture models and short-term cepstral parameters, IEEE T Bio-Med Eng 53 (10)(2006) 1943–1953.[29] A. Tsanas, M. Little, P. McSharry, L. Ramig, Accurate telemonitoring of Parkinson’s disease progression by noninvasivespeech tests, IEEE T Bio-Med Eng 57 (4) (2010) 884–893.[30] A. Ghio, G. Pouchoulin, B. Teston, S. Pinto, C. Fredouille, C. D. Looze, D. Robert, F. Viallet, A. Giovanni, How to37manage sound, physiological and clinical data of 2500 dysphonic and dysarthric speakers?, Speech Commun 54 (5) (2012)664–679.[31] J. Lee, A two-stage approach using Gaussian mixture models and higher-order statistics for a classification of normal andpathological voices, EURASIP J Adv Sig Pr 2012 (252) (2012) 1–8.[32] G. Vaziri, F. Almasganj, R. Behroozmand, Pathological assessment of patients’ speech signals using nonlinear dynamicalanalysis, Comput Biol Med 40 (1) (2010) 54–63.[33] M. Markaki, Y. Stylianou, J. Arias-Londono, J. Godino-Llorente, Dysphonia detection based on modulation spectral fea-tures and cepstral coefficients, in: Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conferenceon, 2010, pp. 5162–5165.[34] A. Tsanas, M. A. Little, P. E. McSharry, L. O. Ramig, Nonlinear speech analysis algorithms mapped to a standard metricachieve clinically useful quantification of average Parkinson’s disease symptom severity, J R Soc Interface 8 (59) (2010)842–855.[35] C. Fredouille, G. Pouchoulin, A. Ghio, J. Revis, J.-F. Bonastre, A. Giovanni, Back-and-forth methodology for objectivevoice quality assessment: From/to expert knowledge to/from automatic classification of dysphonia, EURASIP J Adv SigPr 2009 (1) (2009) 1–13.[36] M. Markaki, Y. Stylianou, Using modulation spectra for voice pathology detection and classification, in: Engineering inMedicine and Biology Society, 2009. EMBC 2009. Annual International Conference of the IEEE, 2009, pp. 2514–2517.[37] M. A. Little, P. E. Mcsharry, S. J. Roberts, D. A. E. Costello, I. M. Moroz, Exploiting nonlinear recurrence and fractalscaling properties for voice disorder detection, Biomed Eng Online 6 (2007) 23.[38] A. Alpan, Y. Maryn, A. Kacha, F. Grenez, J. Schoentgen, Multi-band dysperiodicity analyses of disordered connectedspeech, Speech Commun 53 (1) (2011) 131–141.[39] M. Vasilakis, Y. Stylianou, Voice pathology detection based on short-term jitter estimations in running speech, FoliaPhoniatr Logop 61 (3) (2009) 153–170.[40] P. Henriquez, J. Alonso, M. Ferrer, C. Travieso, J. Godino-Llorente, F. Diaz-de Maria, Characterization of healthy andpathological voice through measures based on nonlinear dynamics, IEEE T Audio Speech 17 (6) (2009) 1186–1195.[41] D. G. Silva, L. C. Oliveira, M. Andrea, Jitter estimation algorithms for detection of pathological voices, EURASIP J AdvSig Pr 2009 (2009) 1–9.[42] A. Gelzinis, A. Verikas, M. Bacauskiene, Automated speech analysis applied to laryngeal disease categorization, ComputMeth Prog Bio 91 (1) (2008) 36–47.[43] C. Moers, B. Mobius, F. Rosanowski, E. Noth, U. Eysholdt, T. Haderlein, Vowel- and text-based cepstral analysis ofchronic hoarseness, J Voice 26 (4) (2012) 416–424.[44] S. Skodda, W. Visser, U. Schlegel, Short- and long-term dopaminergic effects on dysarthria in early Parkinson’s disease,J Neural Transm 117 (2010) 197–205.[45] M. Little, P. McSharry, E. Hunter, J. Spielman, L. Ramig, Suitability of dysphonia measurements for telemonitoring ofParkinson’s disease, IEEE T Bio-Med Eng 56 (4) (2009) 1015–1022.[46] I. Rektorova, J. Barrett, M. Mikl, I. Rektor, T. Paus, Functional abnormalities in the primary orofacial sensorimotorcortex during speech in Parkinson’s disease, Movement Disord 22 (14) (2007) 2043–2051.[47] J. Shao, J. K. Maccallum, Y. Zhang, A. Sprecher, J. J. Jiang, Acoustic analysis of the tremulous voice: Assessing theutility of the correlation dimension and perturbation parameters, J Commun Disord 43 (2010) 35–44.[48] D. Dimitriadis, A. Potamianos, P. Maragos, A comparison of the squared energy and teager-kaiser operators for short-termenergy estimation in additive noise, IEEE T Signal Proces 57 (7) (2009) 2569–2581.[49] T. H. Falk, W.-Y. Chan, F. Shein, Characterization of atypical vocal source excitation, temporal dynamics and prosodyfor objective measurement of dysarthric word intelligibility, Speech Commun 54 (5) (2012) 622–631.38[50] M. Gonzalez-Izal,I. Rodriguez-Carreno, A. Malanda, F. Mallor-Gimenez,I. Navarro-Amezqueta, E. Gorostiaga,M. Izquierdo, sEMG wavelet-based indices predicts muscle power loss during dynamic contractions, J Electromyogr Kines20 (6) (2010) 1097–1106.[51] Y. Song, W.-H. Wang, F.-J. Guo, Feature extraction and classification for audio information in news video, in: WaveletAnalysis and Pattern Recognition, 2009. ICWAPR 2009. International Conference on, 2009, pp. 43–46.[52] G. Weismer, J. Y. Jeng, J. S. Laures, R. D. Kent, J. F. Kent, Acoustic and intelligibility characteristics of sentenceproduction in neurogenic speech disorders, Folia Phoniatr Logop 53 (1) (2001) 1–18.[53] J. Mekyska, I. Rektorova, Z. Smekal, Selection of optimal parameters for automatic analysis of speech disorders in Parkin-son’s disease, in: Telecommunications and Signal Processing (TSP), 2011 34th International Conference on, 2011, pp.408–412.[54] J. R. O. Arroyave, S. M. Rendon, A. M. Alvarez-Meza, J. D. Arias-Londono, E. Delgado-Trejos, J. F. V. Bonilla, C. G.Castellanos-Dominguez, Automatic selection of acoustic and non-linear dynamic features in voice signals for hypernasalitydetection, in: INTERSPEECH’11, 2011, pp. 529–532.[55] J. B. Alonso, J. de Leon, I. Alonso, M. A. Ferrer, Automatic detection of pathologies in the voice by hos based parameters,EURASIP J Adv Sig Pr 2001 (4) (2001) 275–284.[56] S. K. Banchhor, Discrimination between speech and music signal, International Journal of Soft Computing and Engineering2 (3) (2012) 28–31.[57] J. Hillenbrand, R. A. Houde, Acoustic correlates of breathy vocal quality: Dysphonic voices and continuous speech, JSpeech Hear Res 39 (2) (1996) 311–321.[58] D. Michaelis, T. Gramss, H. W. Strube, Glottal-to-noise excitation ratio - a new measure for describing pathologicalvoices, Acta Acust United Ac 83 (4) (1997) 700–706.[59] D. D. Deliyski, Acoustic model and evaluation of pathological voice production, in: 3rd Conference on Speech Communi-cation and Technology EUROSPEECH’93, 1993, pp. 1969–1972.[60] H. Kasuya, S. Ogawa, K. Mashima, S. Ebihara, Normalized noise energy as an acoustic measure to evaluate pathologicvoice, J Acoust Soc Am 80 (5) (1986) 1329–1334.[61] J. Makhoul, L. Cosell, LPCW: An LPC vocoder with linear predictive spectral warping, in: Acoustics, Speech, and SignalProcessing, IEEE International Conference on ICASSP ’76., Vol. 1, 1976, pp. 466–469.[62] H. Atassi, A. Esposito, Z. Smekal, Analysis of high-level features for vocal emotion recognition, in: Telecommunicationsand Signal Processing (TSP), 2011 34th International Conference on, 2011, pp. 361–366.[63] L. Atlas, S. A. Shamma, Joint acoustic and modulation frequency, EURASIP J Adv Sig Pr 2003 (7) (2003) 668–675.[64] H. Hermansky, Perceptual linear predictive (PLP) analysis of speech, J Acoust Soc Am 87 (4) (1990) 1738–1752.[65] R. Mammone, X. Zhang, R. Ramachandran, Robust speaker recognition: a feature-based approach, IEEE Signal ProcMag 13 (5) (1996) 58–71.[66] N. Malyska, T. Quatieri, D. Sturim, Automatic dysphonia recognition using biologically-inspired amplitude-modulationfeatures, in: Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP ’05). IEEE International Conferenceon, Vol. 1, 2005, pp. 873–876.[67] P. Hosseini, F. Almasganj, T. Emami, R. Behroozmand, S. Gharibzade, F. Torabinezhad, Local discriminant waveletpacket basis for voice pathology classification, in: Bioinformatics and Biomedical Engineering, 2008. ICBBE 2008. The2nd International Conference on, 2008, pp. 2052–2055.[68] R. Esteller, G. Vachtsevanos, J. Echauz, B. Litt, A comparison of waveform fractal dimension algorithms, Circuits andSystems I: Fundamental Theory and Applications, IEEE Transactions on 48 (2) (2001) 177–183.[69] M. Aboy, R. Hornero, D. Abasolo, D. Alvarez, Interpretation of the Lempel-Ziv complexity measure in the context ofbiomedical signal analysis, IEEE T Bio-Med Eng 53 (11) (2006) 2282–2288.39[70] A. W. Jayawardena, P. Xu, W. K. Li, Modified correlation entropy estimation for a noisy chaotic time series, Chaos 20 (2)(2010) 1–11.[71] H. K. Heris, B. S. Aghazadeh, M. Nikkhah-Bahrami, Optimal feature selection for the assessment of vocal fold disorders,Comput Biol Med 39 (10) (2009) 860–868.[72] J. M. Yentes, N. Hunt, K. K. Schmid, J. P. Kaipust, D. McGrath, N. Stergiou, The appropriate use of approximate entropyand sample entropy with short data sets, Ann Biomed Eng 41 (2) (2013) 349–365.[73] J. I. Godino-Llorente, P. Gomez-Vilda, F. Cruz-Roldan, M. Blanco-Velasco, R. Fraile, Pathological likelihood index as ameasurement of the degree of voice normality and perceived hoarseness, J Voice 24 (6) (2010) 667–677.[74] J. D. Arias-Londono, J. I. Godino-Llorente, M. Markaki, Y. Stylianou, On combining information from modulation spectraand mel-frequency cepstral coefficients for automatic detection of pathological voices, Logop Phoniatr Voco 36 (2) (2011)60–69.[75] M. G. Kang, K.-T. Lay, A. K. Katsaggelos, Phase estimation using the bispectrum and its application to image restoration,Opt Eng 30 (7) (1991) 976–985.[76] W.-t. Chen, Z.-z. Wang, X.-m. Ren, Characterization of surface EMG signals using improved approximate entropy, JZhejiang Univ-Sc B 7 (10) (2006) 844–848.[77] F. Takens, Detecting strange attractors in turbulence, in: Dynamical Systems and Turbulence, Warwick 1980, Vol. 898 ofLecture Notes in Mathematics, Springer Berlin Heidelberg, 1981, pp. 366–381.[78] J. R. O. Arroyave, J. D. Arias-Londono, J. F. V. Bonilla, E. Noth, Analysis of speech from people with Parkinson’s diseasetrough nonlinear dynamics, in: T. Drugman, T. Dutoit (Eds.), Advances in Nonlinear Speech Processing, Vol. 7911 ofLecture Notes in Computer Science, Springer Berlin Heidelberg, 2013, pp. 112–119.[79] G. Fairbanks, Voice and articulation drillbook, 2nd Edition, Harper and Row, New York, 1960.[80] A. Dibazar, S. Narayanan, T. Berger, Feature analysis for automatic detection of pathological speech, in: Engineering inMedicine and Biology, 2002. 24th Annual Conference and the Annual Fall Meeting of the Biomedical Engineering SocietyEMBS/BMES Conference, 2002. Proceedings of the Second Joint, Vol. 1, 2002, pp. 182–183.[81] A. Alpan, J. Schoentgen, Y. Maryn, F. Grenez, Automatic perceptual categorization of disordered connected speech, in:INTERSPEECH, 2010, pp. 2574–2577.[82] M. Hariharan, M. P. Paulraj, S. Yaacob, Time-domain features and probabilistic neural network for the detection of vocalfold pathology, Malays J Comput Sci 23 (1) (2010) 60–67.[83] J. Arias-Londono, J. Godino-Llorente, N. Saenz-Lechon, V. Osma-Ruiz, G. Castellanos-Dominguez, Automatic detectionof pathological voices using complexity measures, noise parameters, and mel-cepstral coefficients, IEEE T Bio-Med Eng58 (2) (2011) 370–379.[84] J. Mekyska, Z. Smekal, M. Kostalova, M. Mrackova, S. Skutilova, I. Rektorova, Motor aspects of speech imparment inParkinson’s disease and their assessment, Cesk Slov Neurol N 74 (6) (2011) 662–668.[85] G. R. Doddington, M. A. Przybocki, A. F. Martin, D. A. Reynolds, The NIST speaker recognition evaluation – overview,methodology, systems, results, perspective, Speech Commun 31 (2-3) (2000) 225–254.[86] M. Little, P. McSharry, I. Moroz, S. Roberts, Nonlinear, biophysically-informed speech pathology detection, in: Acoustics,Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on, Vol. 2, 2006, pp.II–II.[87] M. Brookes, Voicebox: Speech processing toolbox for matlab (October 2011).URLhttp://www.ee.ic.ac.uk/hp/staff/dmb/\protect\discretionary{\char\hyphenchar\font}{}{}voicebox/voicebox.html[88] Tstool version 1.2 (February 2009).URL http://www.physik3.gwdg.de/tstool/40[89] P. Boersma, D. Weenink, Praat: doing phonetics by computer (May 2013).URL http://www.fon.hum.uva.nl/praat/[90] I. Eliasova, J. Mekyska, M. Kostalova, R. Marecek, Z. Smekal, I. Rektorova, Acoustic evaluation of short-term effects ofrepetitive transcranial magnetic stimulation on motor aspects of speech in Parkinson’s disease, J Neural Transm 120 (4)(2013) 597–605.[91] A.-C. Haury, P. Gestraud, J.-P. Vert, The influence of feature selection methods on accuracy, stability and interpretabilityof molecular signatures, PLoS ONE 6 (12) (2011) 1–12.[92] A. M. Goberman, M. Blomgren, Fundamental frequency change during offset and onset of voicing in individuals withParkinson disease, J Voice 22 (2) (2008) 178–191.41