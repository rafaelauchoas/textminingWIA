ELSEVIER Artjficial Intelligence 83 ( 1996) 297-346 Artificial Intelligence An overview of incentive contracting * Department of Mathematics and Computer Science, Bar hn University, Ramat Gan. 52900 Israel Sarit Kraus * Received May 1994; revised January 1995 Abstract Agents me.y contract some of their tasks to other agents even when they do not share a common goal. An agent may try to contract some of the tasks that it cannot perform by itself, or that may be performed more efficiently by other agents. One self-motivated agent may convince another self-motivated agent to help it with its task, by promises of rewards, even if the agents are not assumed to be benevolent. We propose techniques that provide efficient ways for agents to make incentive contracts in varied situations: when agents have full information about the environment and each other, or when agents do not know the exact state of the world. We consider situations of repeated encounters, cases of asymmetric information, situations where the agents lack information about each other, and cases where an agent subcontracts a task to a group of agents. Situations in which there is competition among possible contractor agents or possible manager agents are also considered. In all situations we assume that the contractor can choose a level of effort when carrying out the task and we would like the contractor to carry out the task efficiently without the need of close observation by the manager. 1. Introduction Agents acting in non-collaborative environments may benefit from contracting some of their tasks to other agents. In this paper we present techniques for efficient contracting that can be used in different cases of multi-agent environments where the agents do not have a common goal and there is no globally consistent knowledge. We consider *This paper is based upon work supported by the National Science Foundation under Grants No. IRI-9423967 and the Israeli Ministry of Science and the Arts under grant 4210. and is an extension of (47 1. I would like to thank Jonathan Wilkcnfeld. Barbara Grosz and an anonymous referee for theii comments, and Sean Bngelson and Onn Sbehory for useful discussions. l E-mail: sarit@bimacs.cs.biu.ac.il. Also aftlliated with the Institute for Advanced Computer Studies, University of Maryland, College Park, MD, USA. OOC4-3702/96/$15.00 @ 19% Elsevier Science B.V. All rights reserved SSDIOOO4-3702(95)00059-3 another agent to the agents are 298 5’. Kraus/Art@d Intelligence 83 (1996) 297-346 situations where a self-motivated order to fulfill its own tasks may contract some of its own tasks to another self-motivated agent(s), An agent may benefit from contracting some of its tasks that it cannot perform by itself, or when the task may be performed more efficiently by other agents. agent that tries to carry out its own individual plan in The central question of this paper is how one agent can convince for it when do something not assumed agent can choose different agent would the manager prefers without manager the agents do not share a global task and to be benevolent. Furthermore, we consider situations where the contractor levels of effort when carrying out the task. The manager to carry out the task with the level of effort that the the need of close observation of the manager, enabling like the contractor agent to carry out other tasks simultaneously. There are two main ways to convince another self-motivated its own tasks: by threatening agent to perform a task to interfere with the agent carrying out in two forms: The first approach to help that is not among its own tasks, or by promising rewards by rewards which may be accomplished system, where one agent may promise for current help. However, as has long been recognized an efficient basis for cooperation, wishing the future, or one agent help in carrying out its own tasks. The second approach is developed purposes. [49]. This paper concentrates on subcontracting is a bartering in return is not environment. An agent to help it in task may not need system which for other for the provision of rewards, and which can later be utilized a task to another agent may not have the ability that can help in fulfilling in economics, bartering the other with future in a multi-agent another agent’s to subcontract is a monetary particularly tasks In this paper we present a model of automated agents where incentive contracting is beneficial, We propose for side payments allows given to the owners of the automated expected utilities Assuming to fulfill to use a monetary and rewards between system in a multi-agent that the agents, and where profits may be environment to maximize that increase with the monetary values, as will be explained below. that each agent has its own personal goals, contracting would allow the agents agents. The agents will be built their goals more efficiently as opposed The issue of incentive contracting has been investigated to working on their own. [ 2,3 1,40,56,88,9 types of contracts a firm and an employer or employers [ 91) ; a landlord and a tenant (e.g., in economics and game theory 1 ] ) . These works in economics and for different applications. Examples [ 6,7,64, (e.g., [2] >; an (e.g., ) ; a buyer and a seller [ 34,58,93,102] [ 721); stockholders [98], etc. In these situations (e.g., a government [ 70,771); [ 21); a professional and taxpayers (e.g., and a policy holder the last two decades during game theory consider different of these are contracts between: 781) ; a government insurance company (e.g., (e.g., exist. The first party action or a level of effort from a number of possibilities, of both parties. The second party (named “the principal”) prescribing principal determines party as a function of the principal’s observations. Despite applications, a rule (i.e., a contract) payoff rules. Before (called “the agent” in the economics and firms (e.g., the first party that specifies and a client they differ thereby affecting has the additional and managements two parties usually literature) must choose an the outcome function of the the fee to be paid to the other the similarity of the above that is the action, in several aspects, such as the amount of information (i.e., the agent) chooses S. Kraus/Arti$cial Intelligence 83 (1996) 297-346 299 to the parties, available of agents. Several concepts and techniques and game theory in the relevant economics the observations are applied literature. that are made by the principal, and the number paradigm to the principal-agent full information and techniques, information agents and bilateral in the environment. We consider varied situations of automated agent environments; situations of certainty information vs. asym- symmetric vs. partial information, situations vs. situations where there are more than two vs. uncertainty, metric automated economics mechanisms ture, that can be used for contracting these results concepts used all the situations techniques agent(s) agents that are appropriate in the various economics For each of these situations we fit appropriate litera- from the game theory or the economics of automated agents. We adjust and present all of them using uniform the different concepts i.e., translating In framework. is provided with its personal expected utilities, given the constraints of the other the paper, we use a robotics domain and an example of software introduced the contracting and game theory papers the agent that designs to the automated agents environment into a uniform the contract . Throughout to demonstrate to automated agents, that we consider, in environments to maximize techniques above. 2. Related -work in DA1 Research into two basic classes: cooperative distributed problem in cooperative distributed prob- (MA) [ 12,18,59,61,101]) in solving a in DA1 is divided solving and multi-agent systems lem solving (e.g., particular problem can be divided among a number of modules or “nodes”. The modules in a cooperative distributed problem to improve the following properties of the system considers how the work involved system are centrally designed [ 8,281. Research solving [ 81: l Performance: Concurrency may increase and may allow the system to solve large problems the speed of computation faster. and reasoning, l Reliability and stability: The modules may provide redundancy, cross-checking triangulation modules can fulfill of the results. its tasks. In case of failure of one of the modules, and the other l Modularity: Each module can be developed separately, making it easier to develop and extend the system. The modules solution include to a given problem. the development of cooperating mechanisms designed to find a Research in MA (e.g., [ 20,29,48,104,107,1 intelligent behavior among a collection of autonomous gent (possibly pre-existing) shared goals or success criteria. There among the agents. agents. lo] ) is concerned with coordinating intelli- In MA, there is no global control, and no globally for real competition is, however, a possibility (possibly heterogeneous) The MA and the cooperative distributed problem of the DA1 research. Our research the problem of a self-motivated motivated (the contractor) can choose between different agent solving falls closer systems are the two poles to the MA systems pole. We consider that tries to make another self- that the contractor the task. The main agent fulfill one of its tasks. We assume to fulfill (the manager) levels of effort when trying 300 S. Kraus/Art@cial intelligence 83 (1996) 297-346 problem that we address is how the manager should motivate the contractor to choose a level of effort that the manager prefers. The provision of incentives is in general not essential in cooperative distributed prob- lem solving systems. It is assumed that it is in the agents’ interests to help one another. This help can take the form of sharing tasks, results, or information [ 191. In task shar- ing, an agent, which cannot fulfill a task on its own, will attempt to pass the task, in whole or in part, to other agents, usually on a contractual basis [ 1011. This approach assumes that an agent not otherwise occupied will readily take on the task and do it to the best of its abilities. Similarly, results and information are shared among agents in such environments with no expectation of reciprocation [ 12,59,61]. This benevolence is based on an assumption common to many approaches to coordination: that the sys- tem’s goal is to solve the problem as best it can, thereby giving the agents shared, often implicit, global goals that they are all unselfishly committed to achieving. One of the techniques that is used in cooperative distributed problem solving for task allocation is automated contracting. In this paper we concentrate on situations where the contractor needs to choose an effort level, and the main purpose of the contracting mechanism is to convince the contractor to agree to do the sub-task while choosing the effort level preferred by the manager. In contrast, in automated contracting the contractors do not need to choose effort levels when carrying out tasks, and thus there is no need for incentive contracts. However, work on automated contracting considers other problems essential to distributed problem solving as we discuss below. A well-known framework for automated contracting is the contract net protocol [ 100, 1011. In the contract net protocol a contract is an explicit agreement between an agent that generates a task (the manager) and an agent that is willing to execute the task (the contractor). The manager is responsible for monitoring the execution of a task and processing the results of its execution, whereas the contractor is responsible for the actual execution of the task. The manager of a task announces the task’s existence to other agents. Available agents (potential contractors) then evaluate the task announcements made by several managers and submit bids for the tasks they are suited to perform. As we explained above, since all the agents have a common goal and are designed to help one another, there is no need to motivate an agent to bid for tasks or to do its best in executing it if its bid is chosen. The main problems addressed by [99-1011 are as follows: l Tusks decomposition: how to break a large task into smaller ones. l Sub-tasks distribution: how to match sub-tasks with problem solvers capable of handling them. l Synthesis of the overall solution: how to synthesize the individual results of sub- tasks into a single overall solution. In addition, they consider problems such as which information a possible contractor should send to a manager when it bids for a task and how the manager should evaluate bids, In this paper we consider incentive contracting in situations where there is only one task per contractor and therefore the problems of task decomposition and synthesis of the overall solution mentioned above do not arise. In some situations we consider the problem of distribution of a given task where there are several agents in the environment that compete for the job (see Section 5.7). However, while in the contract net the agents S. Kraus/Arti~cial Intelligence 83 (1996) 297-346 301 that bid for a task voluntarily provide the manager with correct information about their capabilities and situation, in our framework the manager needs to construct a mechanism to make the possible contractors reveal their capabilities honestly. The contract net protocol is a very general protocol for task distribution, and sev- eral refinements of the protocol were made in the last ten years. Malone et al. [67] developed a distributed scheduling protocol (DSP) based on the contract net protocol. The most important way in which DSP differs from the original contract net protocol is by its criteria for matching between tasks and agents (i.e., the problem of sub-tasks distribution). It includes two primary dimensions: ( 1) contractors select managers’ tasks in the order of tasks’ numerical priorities, and (2) managers select contractors on the basis of estimated completion times from among the contractors that satisfy the mini- mum requirements to perform the job. In addition to the problems addressed by Smith and Davis, Alalone et al. considered problems such as how to estimate the processing time of a task and, if people supply their own estimation, how to encourage them to report honestly, and how to assign priorities to tasks in order to achieve various global scheduling objectives. Similar to the original contract net protocol, in Malone et al.‘s model there is also no need to motivate the agents to bid or to make decisions in order to maximize the global expected utility of the system, and it is assumed that workstations voluntarily put their machines into a mode where the machines responds to requests for bids from the network. Also, the workstations don’t need to choose effort levels; they either carry ‘out a task or not. The DPS was tested using simulation of workstations on a network in a wide variety of situations (e.g., different processor speeds, system loads and message delay times). The results obtained in these simulations are as follows: ( 1) Substantial performance improvement results from sharing tasks among proces- sors in systems with more than light loads. (2) In many cases these benefits are still present, even when message delay times are as much as 5 to 20 percent of the average task processing time. (3) In many cases, the additional benefits from pooling tasks among more than eight or ten machines are small. (4) Large errors in estimating task processing time cause little degradation in the scheduling performance. A modified version of the contract net protocol for competitive agents in the trans- portation domain is presented in [ 941. It provides a formalization of the bidding and the decision awarding processes, based on marginal cost calculations based on local agent criteria. More important, an agent will submit a bid for a set of delivery tasks ’ only if the maximum price mentioned in the tasks’ announcement is greater than what the deliv- eries will cost that agent. A simple motivation technique is presented to convince agents to make bidls; the actual price of a contract is half way between the price mentioned in the task announcement and the bid price. As in other automated contracting systems the contractor either honors its commitment to carry out a task or it does not. There are no ’ Announcing one delivery at a time is not sufficient in general. This is due to the fact that the deliveries are dependent. For example, for two disjointed delivery sets Tl and Tz, the marginal costs that are saved by removing botkl Tl and Tz are usually larger than the sum of marginal cost that was saved by removing each of them alone 302 S. Kraus/Arf@icial Inlelligence 83 (1996) 297-336 levels of actions different out the task, the quality of the delivery), we consider. Therefore, deliveries were done or not) or incentives of action. On the other hand, problems in order to perform the task (e.g., the time it takes as in the incentive contracting framework checking whether to choose an efficient to carry that the level there is no need for monitoring (beside to the contractors in 194 1, Sandholm deals with the following challenging that are not considered part of our incentive contracting model: * tasks into sets to bargain over as atomic bargaining, l how to choose which tasks to contract out, l how to cluster l how to bid when multiple bids and awards should be handled simultaneously, of bids and awards l how to handle a large amount of messages consisting announcements from receiving the agents other agents and how to prevent faster pace than they can process, from at a b how to decide to whom to award a set of tasks. relationships is described which demonstrates In [ 941 a set of experiments in that paper reduces In [ 821 a language the total transportation for specification of complex is described. By using among that the approach presented costs among autonomous dispatch centers. relations among agents in cooperative this language, a designer of a system the agents and specify to one agent the other the the agents are not to help another agent will depend upon the designer’s distributed problem solving can define hierarchical agents’ authority on it. The “authority” agent should give to requests self-motivated, instructions. contracting addition from another agent directly it knows the other agent’s address. In this paper, we also allow both of these addressing methods. of that in for help if it knows that the other agent can help it in its task and if in the contract net protocol. This would mean for bids, an agent has the option of asking Pattison et al. suggest focused addressing as an additional mechanism that arrive from different agents. Since indicates how much emphasis to the one presented their willingness to broadcasting parameter requests agent a plan (usually intelligent constructs for multiple Subcontracting to the relevant agents, where a single called (the slaves), to be carried out by a group of agents in cooperative distributed problem solving also appears in the paradigm the and [ 13,60,90]. Werner by one-way of planning master) then hands out the pieces of the plan [ 1091 presents communication. effort and a master convincing simple master/slaves model was extended by Ephrati and Rosenschein the “slaves” more freedom still to satisfy relationship there is no need to choose a level of for than its observation. The to allow [21] the slaves’ main goal is a formal Also is no need for incentive is, the main problem rather to carry out the plan appropriately without in carrying out the plans. However, the best plan and synchronizing in the master-slave model there is finding their master’s wishes. the agent’s actions, for a master-slave contracting. That logical model other agents individuals In the last 35 years, mathematical economists models describing how resources tionally and computationally decentralized ways in an economy may be optimally have developed market mechanism in informa- ). Researchers [ 3,4,39,45,66] shared (e.g., * Some of these problems were considered by [67,99,101] but they are revisited in [ 941 while taking into consideration the specific domain. S. Kraus/Art@ial Intelligence 83 (1996) 297-346 303 ap- artificial intelligence (e.g., [41,53,106,107]) lsystems and distributed [ 1071 uses market price mechanisms to resource allocation and task distribution problems in distributed plied these models environments, where one of their main goals was to improve of the system. For example, Wellman nation and task distribution consumers librium. This method is appropriate when to the entire economy. We consider number of agents certain about may not carry out the tasks as promised. in computerized the overall performance for coordi- into to adjust prices and reach an equi- assumption, which in relationship incentive contracting when there is usually a small in the environment. We also deal with situations where agents are un- (the producers and producers and use an iterative method is applicable under systems. The agents are divided the world, and the contractors the “perfect competition” agents, each of which in distributed planning there are numerous in Wellman’s terminology) is small the agents. there contracting frameworks In most of the multi-agent [99] sys- the agents [ 104,105] presents a model of is no hierarchy among and optimization negotiations where two agents need to of the multi-attribute In our incentive contract model and in the automated among relationship and cooperate. For example, Sycara case-based that combines reasoning is used in labor management agreement. is a hierarchical (MA) where agents are self-motivated, there tems that communicate negotiation utilities. This model reach an acceptable for situations where a set of self-motivated they want costs, prefers the division resource. Zlotkin and Rosenschein rational agenlts which have symmetric Iof labor. This model In [50-521, capabilities to do as little as possible and therefore Contracting in multi-agent systems was previously autonomous a strategic negotiation model is presented agents have common goals that its tries to reach an agreement over is also applicable when to share a [ 11 l] present a theoretical negotiation model for two the agents need to minimize and identical costs for their actions. studied in [ 321. A formal definition to satisfy as soon as possible. Each agent, while wanting (and some action (or a group of agents) of the mental state of an agent one of its tasks was presented. Contracting that by taking can get another agent the “motivating” the issue of choosing considered. The main contribution for drafting beneficial contracts an effort level when carrying a task. action and the appropriate the appropriate that would like to contract out depends mainly on an agent which believes it for finding in [ 321. Also, is not explicitly of techniques in situations where the contractor agents need to choose to perform an action. However, a detailed algorithm contractor of the present paper is the presentation level by the contractor state of affairs), is not presented about a certain thus bringing effort 3. A framework for incentive contracting In the environments to be discussed below, that subcontracts , and we will refer to the agent(s) . In order to convince refer to the agent(s) manager(s) the contractor(s) do it well, the manager needs contractor’s to provide success there are two types of agents. We will one of its tasks to another agent or agents as the that may agree to carry out the tasks as it to to do the task and motivate the contractor with a beneficial contract. The intensity the contractor in carrying out the task depends on the time and work 304 S. Kraus/Ar@cial Intelligence 83 (1996) 297-346 as its effort level. We propose constructing a monetary the task. We will refer to the contractor’s which it will put into fulfilling intensity agent environment, which will provide outcomes. a way for allocating time and work in the multi- rewards and evaluating system The following are the conditions that a contracting multi-agent should satisfy accepted by all the designers of agents (for any specific distributed multi-agent domain), (for that specific domain): (CMA) framework in order for it to be l Simplicity: The contract should be simple and compute For example, inequalities, a procedure it. That is, the agents will be able to compute for the contractor the awards if finding then the agents need to have a procedure to solve them. there should be an algorithm to the details of the contract. a set of solving requires and to state these inequalities l Pareto-optima@: There should be no other contracted arrangement by both sides over the one they have reached. This means other contract where the utilities of both agents are greater than their utilities contract agreed upon. that is preferred that there will be no in the l Stability: The results should be in equilibrium3 and the contracts should be reached and executed without delay. 3.1. Agents ’ utility functions (see needs to provide the agents with a good framework A designer of an automated agent, in any environment, is uncertain of the past, present, or future environment about the relative merits of different desirable alternatives. to the agent [ 17,331. Symbolic goals are easily communicated, the agent with based on some given set of preferences. Structures of symbolic a decision mechanism goals provide for planning, when the world is perfectly controlled by the agent and the effects of all the operators are known completely and with certainty they guide the search for alternative plans and the projection process, and they also solve the horizon [ 331 for detailed discussion). However, symbolic goals do not give any problem In addition, when information and is uncertain of the agent the result of its actions, In theory offer a normative model such situations in evaluating multiple objectives and for choice under uncertainty that each designer of autonomous value agents develop a numerical utility function In situations where there is uncertainty the structures of symbolic goals are not satisfying. functions by providing risk, the designers need to decide on their agents’ attitude types of behaviors outcome equal to the expected value of an uncertain if it always prefers situation. An agent is risk prone and the agents need to make decisions under toward risk. There are three toward risk. An agent is risk averse if it always prefers to receive an situation over entering an uncertain situation over to enter an uncertain that it would like its agent to maximize. [46,108]. 4 We therefore propose then numeric utility and decision support tradeoffs .’ A pair of strategies (c, 7) is a Nash equilibrium if, given 7, no strategy of Agent 1 results in an outcome that Agent I prefers to the outcome generated by (u, 7) and similarly for Agent 2 given (T. We discuss the notion of Nash equilibrium and other equilibria concepts in Section 3.2 below. 4 The problem of integrating goals and utility is considered in [33]. S. Kraus/Artijicial Inlelligence 83 (1996) 297-346 305 is risk neutral receiving an outcome equal to its expected outcome An agent if it is indifferent between offers a formalization it is risk averse. If the function [ 23,461. risk neutral behavior for capturing is convex, for entering an uncertain the two options. Decision situation. theory is concave, it is risk prone. A linear utility function yields risk attitudes. If an agent’s utility function agent We propose that a utility function of an automated in our contracting multi- depends on the agent’s monetary gain and effort. Developing it is easier to develop such evaluations of effort and world states and assigning numerical values agent (CMA) environment a quantitative evaluation (or their owners) in situations where the agents these is a difficult problem. However, to the outcomes of their activities and there is a direct relationship are paid according and numerical utility between effizn-t and expense, domain of [94] where functions. Elxamples of such domains they make and their expenses to the value of the deliveries agents may be paid according may depend on the number of miles they travel, their speed, weather, etc. In a software agents domain, where users query an information (see Example 4.2 below), the value of the references response center’s efforts may be measured by the time and resources answer. to a query may depend on their monetary value to the user. The information provided by the information spent on searching the transportation and documents include center center as a for an to that the personality of the designer Our framework does not restrict the designer of an agent to any specific utility function toward risk) the the interactions with other agents, how the type of that will be reached, and the complexity of since we assume will affect his/her designer with ways possible outcomes of his/her a utility finding a contract. function. However, we do provide function may affect to evaluate how the choice of a utility function may affect the contract choice of the agent’s utility (e.g., his/her attitude agent’s 3.2. Equilibrium concepts in multi-agent environments strategy and The manager’s to the contractor given offer. Our desire agents use these strategies, As we consider different stability.5 in our CMA environment the contractor’s is to obtain strategy specifies how it should strategies which are in equilibrium, specifies which contract to offer to a if the the agents may become more stable. to gain respond since situations, we use different concepts of equilibrium the interaction among In simple situations, with complete in the environment, if no agent can benefit from deviating If there are n agents equilibrium strategy), given that the other agents do not deviate. For example, suppose a pair of strategies equilibrium, the contractor will not have a better response information, we use the Nash equilibrium concept. is in Nash (i.e., choose another (s,, s,) are If ( s,,~, s,) are in Nash that the manager should offer the contractor, to sc. On the other for a manager and a contractor then if s,, specifies a contract than to act according a set of strategies from its strategy ( $1, ~2,. . . , s,) respectively. s We assume that if an agent is indifferent between two options, but the other agents prefer one of these options, therl the agent will choose the option preferred by the other agents. 306 S. Kraus/Artifcial Intelligence 83 (1996) 297-346 the possible responses of the contractor according to s,, the manager’s best hand, given strategy is to offer the contract When there is incomplete indicated information, the notion of Bayesian-Nash equilibtium is useful. This equilibrium (one for each agent) and a set of strategies. A strategy combination types, set of beliefs set of beliefs form a Bayesian-Nash given equilibrium the set of beliefs, and the agents update if the strategies are in Nash equilibrium their beliefs, according to Bayes’ rule in s,. 6 e.g., agents do not know their opponents’ exact a and a includes r371. When there are several stages of interaction among the agents, the Nash equilibrium for finding equilibria induce an equilibrium is in perfect equilibrium if the agents’ strategies strategies may involve In order to rule out threats that in certain senses are not credible. such equilibria we use the concept of perfect equilibrium [97]. It can be said that a set of strategies at any stage of the interaction. There are two approaches for the type of situations we consider in this paper. The first is the straight game theory approach: a search for Nash strategies the researcher makes a guess or for perfect equilibrium it is then checks that some strategy combination so. The second standard approach: and solve using calculus. The drawback of the game mechanical it is difficult therefore strategies. 7 The maximization However, must solve manager’s maximization that will find the Nash equilibrium approach, on the other hand, is much easier to implement. is that the players the together: problem and vice versa. problem, is that it is not and set up a maximization theory approach and the number of possible guesses the problem with the maximization to develop a computer program is very large (and possibly is the economist’s their optimization is an equilibrium strategy affects the contractor’s in our context this approach to see that strategies. approach problems infinite) and In In this paper we will use, whenever possible, that the maximization include, as a constraint, problem. The maximization by the agent. That problem of the designer of the contract its opponent’s the maximization approach, with some (usu- the contrac- problem of the contract’s designer agent is, the contracts which we provide the manager). (usually (usually the expected utility of the designer of the contract responses of its opponent, which is also trying to maximize the contract, the agent must take into consideration the pos- its own expected the manager) will care. This means ally tor) maximization can be solved automatically maximize However, when designing sible utility. 3.3. Notation We use the following notations in the rest of the paper. A summary of this notation is given in Fig. I. 6 As we see in Section 7. I, there are situations where there is more than one equilibrium. In specific cases, an agent’s strategy may belong to two equilibria. If it is the first to take an action, it needs to take into consideration the possible behavior of its opponent in all equilibria. 7 In our previous work on negotiation under time constraints, we have identified perfect equilibrium strategies and proposed to develop a library of meta strategies to be used when appropriate [ 50-52 1. S. Kraus/Artificial Intelligence 83 (1996) 297-346 301 Meaning Comments Set of efforts of the contractor e,et,...,ei E Effort Set of possible monetary outcomes for carrying out a task . 4,4t, when q is a function of e E Efort , qj E OUtCoffle; q(e) E OUfC0me Set of possible monetary the contractor rewards to r,rl,...,riERewardsr(q) r is a function of q E Outcome when The contractor’s utility function The manager’s utility function Contractor’s utility from outside options (Reservation price) Efficient effort level for the manager Given contractor constraints Efficient outcome for the manager Given contractor constraints Fig. I. Notation used in the paper. level to perform that in the CMA environment, task. The set of possible outcomes the monetary outcome of performing expected utility depends on its effort a task, the expected utility of the contracting to denote specific effort levels. In all cases, that the contractor may Effort kvel: Given a task, there are several effort levels that task. We denote the set of these efforts by Effon. adopt when trying the contractor We use e, ei E Effort will decide how much effort to expend, but its decision may be influenced by the contract offered by the manager. Outcome: While the contractor’s performing the outcome of the performed Outcome. We assume effort level expended by the contractor and that it can be expressed using a monetary a task by q E Outcome. Given system. We denote the monetary value of performing a task, an effort level e E Esfort, q(e) denotes as a function of e. This function involved. That is, the more e:ffort put in by the contractor, Rewartis: the contractor to pay the contractor a reward using the CMA monetary system. We denote of possible may be a function of the outcome to carry out a task, the manager offers the set rewards by Rewards and its elements by r. The reward I E Rewards from carrying out the task (i.e., q E Outcome). function in agent depends heavily on is denoted by the outcome depends on the Utility Rewards do as little as possible and gain the highest rewards; function utility lower rewards and obtain the outcome and a decreasing Outsicr!e options: If the contractor does not accept and does not carry out the task, then it can either perform another idle. Its expected utility or others’) or remain price and we refer to it as 2. therefore, UC is a decreasing the manager’s to give function with function with the reward being paid to the contractor. from the manager task (its own is its reservation increases with the effort the better the outcome. functions: We denote -+ Iw. We assume larger outcomes. Thus, V”’ is an increasing function by V” : Outcome x Rewards by UC : Effort x to that in the CMA environment -+ Iw. The manager prefers in effort and an increasing in rewards. We denote the contractor prefers In order to convince in such a situation the contractor’s the contract function utility In the rest of the paper, in order to simplify of a variable the presentation of formulas, when the scope is clear from the context and the above notations, we will omit the precise 308 S. Kraus/Art@cial Intelligence 83 (1996) 297-346 that rewards encounters, that the manager there should be a technique definition of the variable. For example, when using r-i we will not always mention ri belongs the contractor after the task is these rewards. considerations may yield appropriate to enforce commitments to Rewards. In our system, we assume carried out. In such situations In the case of multiple behavior. Some external single encounter, e.g., the responsibility the contractor’s owner. Our last definitions to the manager. The jirst best contract will provide equal it could get when there is complete observe information observe the contractor(s)’ asymmetry intervention may be required of the manager’s owner for its contracts in a toward are concerned with the value of the contracts the manager with a profit that is and the manager can information given and constraints on writing contracts, e.g., the manager does not actions. The second best contract is Pareto-optimal the contractor(s) for enforcing reputational to a profit ’ actions. 4. Full information is known information the environment to both agents. At first we assume that all the relevant about In the simplest case the manager can observe and the situation the contractor’s effort and actions and force it to perform at the effort level preferred by the manager by paying only when the required effort is made. The amount of effort required into from the contractor will be the one that maximizes account to the contractor. However, observe be either trying the action. We consider to the contractor’s actions and its level of effort. In some cases, the manager may task at the same time, or it cannot reach the site of and it is either not possible or too costly for the manager the manager’s outcome, to be made the task fulfillment in most situations two cases in such situations: to carry out another the rewards that need taking l In Section 4.1 we consider the result of the contractor’s the case where there is no uncertainty with respect actions. to l In Section 4.2 there is uncertainty concerning the outcome of an action taken by the contractor. 4.1. Contracts under certainty Suppose both agents have full information concerning the contractor’s actions. Under the results of the contractor’s effort. If this function a forcing contract the manager does not observe is no uncertainty is a function of the contractor’s the manager can offer the contractor the manager will pay manager. effort manager’s observation. Note, the outcome won’t necessarily effort on the part of the contractor, but rather a result of the effort which provides manager with the desired outcome. about the world and about each other, but there i.e., the outcome then that required by the the task with the the be a result of the highest the to both agents, is known [ 16,34,88], which means these circumstances, actions, to itself, even without to be most profitable then it will perform the contractor only that the manager If the contractor if it provides the outcome the contract, accepts finds level S. Kraus/Artijicial Intelligence 83 (1996) 297-346 309 We assume that either the manager or the contractor In the background other managers are competing so that the manager’s equilibrium for the manager’s its reservation price-the minimum that induces for the manager’s it to agree to perform task, so that the agent’s equilibrium is one of several similar perfect some tasks to subcontract profit equals zero, or many possible utility the task. 8 task. the efficient there are several possible the to choose a reward function where that the manager would like to pay the to accept the offer, then if the the required effort level, the manager will the the contract, but does not choose this case in above, since the contract must at least provide reward should pick an effort is one of many agents that compete level, e* E Effort, that will generate level, q* E Outcome. As we explained in equilibrium, ii. The manager needs for contracting = i; and U”(e,r(q)) < ii for e # e*. ii is the minimal the contractor as little as possible, but wants the contractor the contract. Since accept that the contractor provided ii. If the contractor accepts effort level, its reward will be even less than a. We demonstrate the contractor agents compete competitors. to the contractor, contractor equals Suppose The manager output agents available contractor with the utility V(e*,r(q*)) will make contractor outcome pay appropriate the following the contractor example. reveals Example 4.1 (Contracting under certainty). Two robotics and CompC 9 are responsible (e.g., Tel-Aviv and Ramat-Gan) . Each of the companies has several autonomous mobile robots that carry oust the cleaning companies, CompM in adjacent cities tasks in these cities. lo and garbage collection for cleaning Most of th’e garbage collected by these companies are paid mainly according is used for recycling, and therefore the companies they collect and its value for recycling. The amount of garbage collected by a robot depends on the effort in the area it level with which tries to clean. it carries out the task, and the distribution to the amount of garbage of garbage Suppose one of CompM’s robots has to collect garbage of CompM, but close to subcontract to several of CompC’s some of its garbage collection far from the other robots robots. The CompM’s like robot would tasks and therefore approaches one of ’ Note that if this assumption is not made, there may be several equilibria. In such situations the designers of the agents m,ay agree upon regulations that will make all agents in the environment focus on one of them. For example, they may agree that the manager will serve as a focal arbitrator. A focal arbitrator is an agent who can determine a focal equilibrium in the environment. In such a case, the equilibrium will be similar to the case where many possible contractor agents compete for the manager’s task. One way of making the manager a foca’l arbitrator is by imposing regulations in which the contractor cannot negotiate the details of a contract; it can either accept the contract offered to it by the manager, or reject it. ’ The robots of company CompM will play the role of the managers and the robots of CompC will play the role of the contractors. I” Most of the autonomous robots up today operate indoors (e.g., Plakey’s of SRI, Polly’s of MIT, Schimmer of Stanford 1 II ,44,80] ). Mobile robots that operate in rougher terrain are usually less autonomous (e.g., DANTE II that was developed by NASA and CMU and explored the crater on Mt. Spurr volcano in Alaska) or act in well-defined environments (e.g., CALMAN-a computerized articulated lawn mower with automatic navigation that was developed at Lulea University of Technology in Sweden). It seems that on-going research on perception, mapping, and navigation in a changing environment will contribute to the construction of “cleaning” automated agents, but it is likely to be a few years before such robots are operational. 310 S. Kraus/Art@cial Intelligence 83 (1996) 297-346 robot since it wants to carry out another CompC’s robots. The CompC robot can collect garbage in three levels of effort (e) : Low, robot cannot observe Medium and High respectively denoted by 1, 2 and 3. CompM’s task simultaneously. the effort of the CompC’s The value of garbage collection function of CompM’s The utility is U”‘(q, r) = q - Y and the utility function of CompC’s robot, r) = 17 - 10/r - 2e, where I is robot it will busy itself the reward with maintenance that the best robot’s point of view is 2, in which there will be an outcome effort level from CompM’s if the of &?%. The contract if a contract in the case that it accepts tasks and its utility will be 10. It is easy is V(e, robot rejects robot. If CompC’s is q(e) = &@6. to the CompC’s that CompM’s robot offers the contract, to CompC’s the contract to calculate is reached, is 3f robot outcome and its effort level will be Medium. is v@% and 0 otherwise. This contract will be accepted by CompC’s robot Another issue of concern In a situation of complete Section 5) it should compute agent and choose Our model the same utility be information a contract, case should be equal contract, will gain its reservation price. i.e., the managers’ to maximize trying information is how the manager will choose which agent to approach. case in the incomplete the expected utility for itself from each contract with each (we consider information the one with the maximal expected utility. is also appropriate in the case where functions, but only one possible contractor. about the utilities of the managers reservation price. The outcome in the event there are several managers with In such cases, there should that they do not sign in this will offer a to the manager to its reservation price. In this case, the contractor” its expected utility under the constraint that the manager 4.2. Contracts under uncertainty there situations, concerning is uncertainty We continue to assume the contractor’s behavior. However, in this case that the agents have full information other, and that the manager does not observe subcontracting action. If the contractor chooses some effort level, then there are several possibilities an outcome. For example, suppose a cleaning automated agent subcontracts collection task and suppose at the site. If the contractor all over the area, the outcome may be similar a low level of effort and the garbage chooses a high effort level when the garbage higher and, thus, better task does not reveal a stable and maximal contract about each in most the possible outcome of an for its garbage about the distribution of the garbage is distributed to the case where the contractor chooses if the contractor In such situations the exact effort level of the contractor, and consequently, is located in one area, the outcome may be a chooses a high effort level and the garbage is all in one place. However, the outcome of performing is much more difficult. that there is uncertainty to the manager. choosing Assuming that the world may be in one of several states, neither the manager nor the contractor knows the exact state of the world when agreeing on the contract. There is the about the world during or after possibility that the contractor may gain more information I ’ Here the contractor is the focal arbitrator. S. Kraus/Artifcial Intelligence 83 (1996) 297-346 311 completing The manager, however, the task, but only after signing the contract and choosing the effort level. about the world. is not capable of gaining more information there that . . , q,,} Following [34], we also assume is a set of possible outcomes to the such that q1 < q2 < + . . < level of the contractor. distribution the state of the world and upon carrying out the task Outcome = (41,. that, given a level of effort, there is a probability contractor q,, depends upon Furthermore, we assume attached to the outcomes there is a probability Cr=, 64e,qi) = 1 and for all qi E Outcome, p(e, qi) > 0. I3 This characterizes situations where the manager effort level unambiguously. The manager’s problem that function p : Effort x Outcome -+ R, such that for any e E Effort, the to both agents. t2 Formally, we assume is not able to use the outcome that will maximize the contractor’s that is known the manager’s to determine the effort pected utility, knowing the contract, the effort contractor will be offered by the manager, contractor also [88]).‘4 the reward ri. The maximization is to find a contract ex- that the contractor may reject the contract or, even if it accepts to the level will be chosen that the (see . . , n, problem can be constructed that in the contract the manager will pay [88]. The manager’s for any q;, as follows reward later i = I,. ca.n be based only on the outcome. Let us assume Maximi:ze,, ,..,, r,l c p(Z,qi)U”‘(qi,r;) i=l with the constraints: (IR) n .. c i=l p(~9qi)uc(e^,ri) 3 2, (1) its expected utility subject ( 1) states that the manager Equation as to maximize contractor must be large enough (2) than to reject it. Constraint constraint its reservation price tries to choose the reward for the contractor, so to two constraints. First, the rewards for the rather (IR) constraint. This that the expected utility of the contractor will be at least as much as is called the individual rationality to prefer the contract the contractor to motivate (3), which is called the participation (i;). The second constraint requires ‘* A practical question is how the agents find the probability distribution. It may be that they have preliminary In the worst case, they may assume an equal distribution. The model can be information ab’out the world. easily extended to the case that each agent has different beliefs about the state of the world, i.e., has its own probability function, which is known to its opponent [ 811. “The formal model in which the outcome effort independent of the contractor’s effort level, is a special case of the model described here 134.8 I, 9 I 1. I4 As we mentioned above, we omitted the definitions of the variables in some of the formulas. In the formulas below, as well as in the rest of the paper, ri E IR and qi E Outcome. level, and in which the probabilistic function gives the probability of the state of the world which is is a function of the state of the world and the contractor’s 312 S. Kraus/Artificial Intelligence 83 (1996) 297-346 (IC), provides constraint level that the manager prefers, given the contract agreed rewards, e^ will provide the contractor with the motivation the contractor with the highest outcome. it is offered. This means it needs to choose the effort that given the In order to be able to use the above framework in the CMA environment, should be able to solve the above maximization used depend primarily on the utility next two sections. problem. The algorithms functions of the agents, as we will describe the agents that should be in the 4.2.1. Risk neutral agents and If the manager are risk neutral, the contractor can be done using any linear programming [ 83,103] in most situations, technique see for problem the solution will be very simple: example from the outcome, and the rest will go to the the manager will receive a fixed amount contractor. That is, ri = qi - C for 1 < i < n, where the constant C is determined by constraint then solving the maximization ). Furthermore, (e.g, simplex, (IR) (2) [ 981. there Suppose is an the Earth Science Data and and Space Administration (possibly from users automated that are rele- center and the user are uncertain and documents (ESDIS) information large databases that has several center System ). The and answers center the queries by providing (e.g., of the National Aeronautics receives queries Example 4.2 (Risk neutral software agents under uncertainty). information Information (NASA) agents) vant to the query. Given a query, both the information about the number of documents the query. However, (e.g., CPU time) searching will a query will be referred rience, number of documents center. in the information they both know that if the information its databases, increase. The amount of resources the user and the information to as its effort references that are relevant center’s databases to center uses more resources then its probability of finding more documents center uses in answering that the information level. In particular, based on previous expe- of the estimation center have some probabilistic that will be found given a specific effort level of the information the problem we assume In order to simplify for the information possible asked a query such that the user and the information 30 or 100 related documents. estimate that it will find 30 documents that if the information center chooses is f and the probability two effort that there are only levels the user that there are either center and the agent the Low effort level, then the probability center estimate center, Low (e = 1) and High (e = 2). Suppose I5 In addition, both the information that it will find 100 documents is f. On the other hand, if it searches with the High effort level, then the probability that it will find 30 documents If the user gets 30 documents is h, and the probability it is worth 50, while locating 100 documents that it will find 100 documents is 3. is worth 75 is Urn (30, r) = 50 - r and U”( 100, r) = 75 - r. to the user. The user’s l6 utility function Is In real situations we expect and discrete) example demonstrates I6 Note that the user plays the role of the manager and the information that the number of possible effort the technique. and also that the set of possible numbers of documents will be much levels will be much larger larger. However, (but finite this small center plays the role of the contractor. S. Kraus/Art$cial Intelligence 83 (1996) 297-346 313 The information center’s utility function is UC (r, e) = r - 10e; if it doesn’t respond to the user’s query, it works on maintaining its databases, and its expected utility will be 5, i.e., Li = 5. In solving the maximization problem above, we reach the conclusion that the user should offer the information center the reward 2& if it provides only 30 documents and 27& if it provides the user with 100 documents. The information center should choose the High level of effort and the user will always gain a profit of 47%. Similar situations may occur between the cleaning automated agents. Example 4.3 (Risk neutral robots under uncertainty). Suppose the utility function of the CompC’s robot from Example 4.1 is Uc( r, e) = r - e, and suppose that it can choose between two effort levels: Low (e = 1) and High (e = 2), and suppose that its reservation price is ii = 1. There are then two possible monetary outcomes to the garbage collection scenario: q1 = 8 and q2 = 10. The utility function of CompM’s robot remains as it was in the previous example, i.e., U’“(q, r) = q - r. If CompC’s robot chooses the Lower level of effort then the outcome will be q1 with probability i and q2 with probability $. If it takes the High level effort the probability of q1 is $ and of q2 it is i. In such situations, CompM’s robot is able to ensure itself a profit of 6%. That is, r-1 = l$ and r-2 = 3:. The robot of CompC will choose the High level effort. 4.2.2. The contractor is risk averse When the agents are not neutral toward risk, then solving the manager’s maximization problem becomes much more difficult. However, if the agents’ utility functions are carefully chosen, then an algorithm does exist. Suppose the contractor is risk averse and the manager is risk neutral (the methods are also applicable when both are risk averse). Grossman and Hart [ 311 present a three-step procedure in order to find appropriate contracts in such situations. The first step of the procedure is to find for each possible effort level, the set of reward contracts that will induce the contractor to choose that particular effort level. The second step of the procedure is then to find the contract which supports that effort level at the lowest cost to the manager. The third step of the procedure is to choose the effort level that maximizes profits, keeping in mind the need to support that effort with a costly reward contract. Formally, step one and two are as follows: Suppose the manager wants the contractor to choose the effort level e’ E Efsoort, it will need then to solve the following: n C(e’> == Minimize, ,...., r,, C gde’, qilri i=l with the constraints: n (IR) c P(e',qi)V(e',ri) i=l b & (4) (5) 314 S. Kraus/Artificial Intelligence 83 (1996) 297-346 II (IC) c a(e’,qi)U”(e’ ,ri> 2 Cp(e,qi)UC(e,ri) i=l n i=l for all e E Effort. (6) The first constraint (5) requires (its reservation price). The second constraint that the expected utility for the contractor will be at (6) level e’. it to take the effort that given the contract, least as good as its outside options requires The minimization problem can pay as little as possible problem that the preferences of the contractor over entering uncertain of its actions are independent the contractor will prefer that the manager the contractor of its actions and effort level. states to induce there is an algorithm [ 31,83,89]. if UC satisfies several properties, is looking for a contract where to choose e’. For this minimization including the property situations are independent lotteries I7 That is, the contractor’s preferences over reward After finding a set of possible values, II,. . . , r, for every e E Effort (where there could be effort levels which may be empty since contractor choose), and after finding level, to move manager will then choose the effort level that will provide the minimum to the third step, which the manager is ready expected reward C(e) the manager cannot make the set the , for any effort is easy to compute. The it with the maximal outcome: (7) to check the validity of the inequalities task is easier. After being offered a contract, The contractors computational only needs manager’s maximization validity of the individual the contract or not. When it should consider problem described are known, based on the suggested contract, the contractor in the the contractor the needs in order to decide whether to accept the contractor needs to decide which effort level to provide, to the maximization In both cases, since all variables from its effort level, similar (IC). constraints its expected utility in the participation problem. That rationality these checks are very easy. that appear as constraints is, when (IR) to check constraint Example 4.4 (Risk averse contractor under uncertainty). actly as in Example 4.3 but the designer of the robot determines be risk averse and its utility and fi = 1. function Suppose the situation is ex- that the contractor will IO/r - 2e is as in Example 4.1: Uc( r, e) = 17 - The maximization problem that the manager should solve is: M~imizf+, ,... ,r,, c fJ(z9 4i) n i=l (qi - ri> (8) with the constraints: ” In [ 89 1 the problem of finding a contract when the manager can choose an effort level from a real interval (IC) can be replaced with is considered. Rogerson the requirement In such situations a solution can be calculated using level be a stationary point for the contractor. the Kuhn-Tucker Theorem. in which the constraints the sufficient condition that the effort identifies S. Kraus/Art$cial lnrelligence 83 (1996) 297-346 2 c i=l &d,qi) 315 (9) (10) (IR) (IC) Grossman and Hart’s three-step procedure to make reward needed [ 3 l] requires the contractor that the manager first deter- choose et = 1 and what the mine minimal the minimal reward is that will make it choose e2 = 2: C(el) = Minimize,,,,, ir1 + ir2 with the constraints: (IR) ;(17-;-2)+;(17+2)>1, (IC) ;(17-+2)+;(17-$2) ,;(17+4)+;(17-$4). (11) (12) (13) this minimization is that The results of solving choose et = 1 is rl = r2 = f. A similar the minimal minimization can be stated and solved for e2 = 2. In this case the minimal reward to make the contractor choose effort level e2 = 2 is r{ = 1 and r; = 15. Finally, i.e., the manager problem using Lagrangian multipliers should check which effort level reward problem the above rewards, it prefers, given the contractor to make -r’,) - rl) + &el,q2)(q2 - r-2) and Ia(e2,ql)(qt is that the manager can obtain it should compare @(et,qt)(qt p( e2, q2) (612 - r$>. The conclusion utilities by offering r{ = 1 and ri = 16. The contractor will then compute utility from choosing effort level et (i.e., +(17-10/r:-2)+:(17-10/r;-2)) from choosing effort level e2 (i.e., $(17-10/r’,-4)+~(17-10/r~-4)),anditwill then realize will then verify that its expected utility from the offered contract $( 17 - 10/r: choose effort level e2 since its expected utility e2 is preferred by the manager. I8 from both effort levels is the same. The contractor is greater than ii (i.e., l), and will then accept the contract and from both effort levels are the same and + the largest expected its expected that its expected utility - 4) + i< 17 - 10/r; - 4) 2 and 4.2.3. Obtaining imperfect information about the contractor’s behavior Even in situations where the manager cannot observe it may be able to gain some information about its behavior. For example, the actions of the contractor, it can gain ‘s In the rest of the paper we will not specify situations, as constrain& variables are known, based on the suggested contract, in the manager’s maximization given a contract, needs only problem, the contractor the contractor’s computation procedures, the validity of the inequalities since in most of the that appear to the check done in this example. Since all to check similar this check is straightforward. 316 S. Kraus/Art$cial Intelligence 83 (1996) 297-346 process. to assume is beneficial. the manager In particular, if the contractor is called an imperfect by setting up a camera the contractor’s behavior, that the assumptions described in the garbage collection this information site. This information may (noisy) information be imperfect, and the process of getting monitoring takes effort level e, then the result of such a monitoring mechanism may be e + 6 where S is a random variable drawn from [(~a, ai ] for some finite ae, LYI . These results will enable to obtain some estimation of the contractor’s effort level. The main question is, however, whether using such monitoring We continue hold. That is, the agents have full information observe and neither agent knows the contractor carrying out the task. Under if the contractor monitoring mechanism or risk averse. I9 However, according averse, there are potential gains to monitoring. This is the case, particularly, contract: of the following judged acceptable on the basis of the monitored outcome, paid according rewards in [35]. in the beginning of Section 4.2 about each other, the manager does not the state of the world the outcome of it has been shown that from the use of any is either risk neutral is risk if a contract is action If the contractor’s the contractor will then be fixed less preferred, that appears there are no gains (to either agent) [ 351. This claim holds when the manager the state of the world, but both agents observe this idea we use a modification of an example is an optimal monitoring to the above conditions, [ 351. To demonstrate the above conditions, schedule. Otherwise, there is uncertainty if the contractor to a prespecified is risk neutral, it will receive concerning form the utility 4 1.25 function of CompC’s is UC (e, r) = P-‘.*~ - 5e Example 4.5. Suppose amples of CompM’s robot is uniformly situation is 0 which 4(c, 0) = e + 8. The monitoring uniformly distributed on [e - E, e + E] effort level e, the monitor will provide an equal probability and e + E. robots from the previous ex- , its reservation price is li = 0 and the utility function the world’s is function includes only monitors, which are for some E > 0. That is, if the contractor chooses (Y, between e - E is, as in previous examples, Um(q, r) = q - r. Suppose [0, 11 and the outcome distributed technology on then number The contract and the monitored that will be offered by the CompM cy: information robot is a function of the outcome r(s,n) = i&, 0, if ff 3 2e + 2-6e-3 - e, otherwise. The effort level chosen by the CompC’s choose 2e +2-6e-3. In such situations robot depends on E. If E < 2-‘.*‘, then it will the CompC’s robot will always get the reward $E is 0. The expected utility of CompM’s robot is f +2-5 +E-~ and its expected utility If E Z 2-I.*“, will take a lower level effort, 5 * 2-6~-3. lower than 2e + 2-6e-3 - E and CompC’s :E. level of effort, but rather value (Y will be + It may be that the monitoring robot won’t get any reward. The probability then CompC’s robot will not choose the required I9 The manager’s utility function should be monotone increasing with q - r, concave and continuously differentiable. The proof to the claim appears in [ 35, Proposition 31. S. Kraus/Artijicial Intelligence 83 (1996) 297-346 319 In addition, should be higher self-selection beliefs. Formally: in each of the n contracts offered by the manager than contracts its reservation that will maximize price. The manager should its expected utility, based on its probabilistic the contractor’s utility find a set of such subject to: (SS) Eq. (14), (IR) V(f?i,ri) >/ 2, where f(ei, ei) = qi, We demonstrate this maximization problem in the next example. (15) (16) the user the user asks center were updated center, of course, knows center a query. However, that will be found by the information the information the databases of the information Example 5.1 (Contracting under asymmetric information (sofnvare agents) ). Similar to Example 4.2, is recently as to whether uncertain that the databases can be either in state 131 = 1 or in or not. That is, the user believes the state of its databases. The state 192 = 2. The information center depends on the state number of documents function of its databases and the effort level it will choose to search with. The outcome is f(e, 0) =: e0, the user’s utility is lJ”(q, r) = q - r and the information function is Uc( e, r) = r - e2. Hence, with f( e, 0) = e0, the information center’s utility is a function of the output, reward and the state of the databases center’s utility the is r/“(s,r,e) contractor’s) 0.25 that the state of the databases 0.75 that the state of the world is 02. function function = r - (q/0)2. We also assume believes with probability is 01 (i.e., 41 = 0.25)) and it believes with probability reservation price is fi = 1, and the user (manager) that the information center’s (i.e., In such a situation the user should solve the following maximization problem: Maximize(,,,,,),i,l.2 0.25(ql - r-1) + 0.75(q2 - r-2) (17) subject to: rl - 4: 2 r2 - 4& r2 - (q2/2j2 Z rl - (q11V2, rI -4: > 1, r2 - (q2/212 3 1, 0 < ri < qi, i = 1,2. If the output function f is twice differentiable all 8, 2’ then there is an interesting result concerning in e, with fe > 0 and fee < 0 for the manager’s preference over the 21 .f; denotes the first derivative of f by e and fee is the second derivative. 320 S. Kraus/Artifcial Intelligence 83 (1996) 297-346 information available to the contractor. If the contractor has full information about the state of the world before signing the contract, then the manager’s expected utility is lower than in the case where it and the contractor have symmetric beliefs (either perfect or imperfect) about the state of the world before signing the contract [ 6, 151. This conclusion is a result of the fact that when they share the same (perfect or imperfect) state of information, the contractor can be held to its reservation level of expected utility. 5.2. Asymmetric information after reaching an agreement In some situations, the contractor is able to collect more information before it performs the agreed upon task but only after signing the contract. For example, when CompC’s robot reaches the garbage collection site, it may find out what the exact state of the world is and know for sure what the outcome will be if it takes a specific level of effort. If agreements are enforced, i.e., if the contractor cannot opt out of the agreement after it is signed, then the only difference between the previous case and the current one is, that constraints (IR) ( 16) should be about the expected utility of the contractor, rather than its eventual utilities, since at the time of the contract, the exact utility is not known to the contractor. If the agents have similar probabilistic beliefs about the state of the world when signing the contract (i.e., $i), then the constraint is as follows: (IR) n c i=l 4iU’(ei, ri) 2 fi, where f(ei, ei) = qi. (18) We demonstrate this in the following example. Example 5.2. (Risk neutral agents under asymmetric information (cleaning automated agents). Suppose the situation is exactly as in Example 4.3, and CompC’s robot can find out more information after the robots have reached a contract, but before choosing its level of effort. As in Example 4.3 the contractor can choose between two effort levels Low (e = 1) and High (e = 2) and its reservation price is ii = 1. There are then two possible monetary outcomes to the garbage collection: q1 = 8 and q2 = 10. The agents’ utility functions are the same as in Example 4.3. The world can be in one of eight possible states 81,. . . , 88 with equal probability. The outcome function is defined asfollows:For1<i<6,f(19t9i)=qt,for7<i<8 for 2 < i < 8, f (2, f+) = 42. Note that this yields the same probabilistic outcome as in Example 4.3. f(2,8t)=qtand f(l,Bi)=qz, There are two possibilities for constructing the contracts, depending on which effort level the contractor will choose if the state of the world is either 82,. . . , &. It is clear that if the state is 81, 87 or 6s the contractor will choose the Low effort level. If the manager would like the contractor to choose High effort level in states 82,. . . , o& then the manager should solve the following minimization problem (we list only the binding constraints) : Minimize,,T,, $r] + ir2 subject to: (19) S. KraudArtijkial Intelligence 83 (19%) 297-346 317 of this happening is 1 - 2-5e-4, and CompC’s robot’s expected utility is still 0, while the expected utility of CompM’s robot in this case is 1 + 5 * 2-7~-3. In both cases, CompC’s expected utility is more than 1, which is what it can expect if it does not use a monitoring mechanism. From the above results, it follows that when e > 2-‘.25, the rewards to CompC’s robot increase with E, its effort level decreases with E, and the expected utility of CompM’s robot decreases with E. These results fit the belief that as monitoring becomes less precise (i.e., E increases), the manager’s expected utility decreases. 5. Asymmetric and incomplete information There are some situations in which the contractor may have more information than the manager. First, the contractor may have obtained more information concerning the environment, e.g., the information center from Example 4.2 may know the exact state of its datalbases, while the user in that example may only have some probabilistic beliefs about the databases based on previous experience. Second, in other situations the manager may not know the utility function of the contractor. The contractor then may be one of several types that reflect the contractor’s ability to carry out its task, its efficiency or the cost of its effort. However, we assume that given the contractor’s type, its utility function would be known to its party. For example, suppose the cleaning company CompC builds robots of two types. The specifications of the robots are known to CompC’s robots and to CompM’s robots; however, CompM’s robots do not know the specific types of CompC’s robots they will encounter. In both cases, the manager could simply ask the contractor for the additional in- formation, i.e., its type or the state of the world, however the contractor will not tell the truth unless the manager provides it with a monetary incentive to do so. This will often cause inefficiency from the manager’s point of view. The search for an equilib- rium in such situations may often be extremely difficult, but there is a useful technique that, in using it, the manager can reduce the number of contracts it needs to consider, as we explain below. The manager should search for an optimal mechanism [ 141 as follows: the manager offers the contractor a menu of contracts indexed by the agent’s type (or the state of the world). The contractor can then decide whether to accept the menu of contracts or not. If it accepts the offer it sends a message to the man- ager reporting its type. The manager is then committed to the contract indexed by this type. The rewards of the contractor in each of these contracts are the functions of the outcomes. *O The big advantage of this mechanism is the revelation principle: For every contract that leads to lying, there is a contract with the same outcome for the contractor (given its type or the state of the world) but without inducement for the contractor to lie. Therefore, without loss of generality, it is enough for the manager to consider only *(’ Given the chosen contract, the contractor chooses an effort level which maximizes its own expected utility. In each of the menu’s contracts, the contractor’s expected utility should be at least us high as its expected utility if it does not sign the contract. 318 S. Kraus/Art$cial Intelligence 83 (1996) 297-346 the revelation principle. First, there interest in using to honestly contracts where it is in the contractor’s limitations are two main communication since the contractor needs to send a message its type. Second, of the manager. After the contractor advantage different one. We discuss situations of asymmetric requires strong precommitment reveals its type honestly, the contractor’s this mechanism to re-negotiate (sometime as well) the contract, and offer a these issues in Sections 5.4 and 5.6. We will consider several information. report its type [ 761, There for is a need to the manager specifying capability on the part it is often in the manager’s l In Section 5.1 we consider the case where the state of the world is known to the contractor, but not to the manager. l In Section 5.2 neither agent knows the state of the world before signing The contractor choosing its effort level. finds out that information after signing l In Section 5.3 the contractor’s information but it knows the exact state of the world only after a contract choosing the effort level). is initially better than that of the manager, (but before is signed the contract. the contract, but before l In Section 5.4 information, either before or after signing the contract. the contractor cannot predict the outcome, based on its private l In Section 5.5 both agents have some private information, e.g., they have some private information about their types. 5.1. Asymmetric information about the state of the world Suppose the world can be in one of several states, 81,. . . ,8,. If the contractor chooses the outcome will be f( e, 0) (UC (e, r) ) increases with the (r), and decreasing with its effort (e). The manager’s r) ) increases with the outcome, and decreases with its reward that the contractor knows the state of the world 8, but the state ‘of the world, having only a about that the world is in state Bi, i = 1,. . . , n by & is 0, then a level of effort e and the state of the world [ 361. As in previous cases the contractor’s utility function reward it gets from the manager utility to the contractor. We assume the manager has no definite knowledge probabilistic its belief and assume belief. We denote that ~~=, 4i = 1. function (U”(q, its private then report to this message, As we described above, in the first step of the agents’ interaction, for an outcome and a payoff the manager will (qi, ri). offer the contractor n pairs (one for each state) to The contractor will In the manager. According to the chosen the third step the contractor chooses above, based on the revelation principle, contract and the outcome. As was mentioned we will restrict our attention reports the situation of the world honestly, motivated by the contract. That is, if the state of the world the ones offered by the manager. This constraint information, the corresponding its effort level, and is paid according to direct mechanisms under which i.e., the state of the world, is the best contract among is called “self-selection”. is Of, then (qi,ri) is implemented. the contractor Formally, contract (SS) ViE {l,...,n} u’(ei, ri> B u’(e,j,r,j) where 1 < j < n, f(Oi,ei) =qir f(f?i,ej) =qj. (14) S. Kraus/Artijicial Intelligence 83 (1996) 297-346 UR) (IC) +<r, - 1) + i(rZ - 2) + i(r* - 1) > 1, r2 - i-1 >, 1. 321 (20) (21) By solving this problem we can conclude that the manager can always keep 7; of the outcome and pay the contractor rl = i and r2 = 2:. Similarly, we can formalize the problem where the contractor chooses effort level Low in states 02 - &j. The rewards should be r{ = ri = 2 and the expected utility for the manager is 6;. In order for the manager to maximize its expected utility, the first option is better since it yields the manager an expected outcome of 7$. This is higher than in Example 4.3, where its expected outcome is 6%. We would like to consider the option of monitoring in such situations. It was proved in [35] that if the contractor is risk neutral, and if it is able to get information about the exact state of the world after signing the agreement, then monitoring is not valuable. If the contractor is risk averse, monitoring may be beneficial as we will explain in Section 5.6, The manager can design a contract that will make the contractor choose the Pareto-efficient effort level for the real state of the world. If it is possible for the contractor to cancel the contract after obtaining the information about the state of the world, then this possibility should be taken into consideration when the agents agree on the contract [95]. When the contractor can opt out of an agreement, the question is what are its alternatives at that point. It may be that it can still gets its original outside options, i.e., its reservation price ii. In other situations, however, it may have already lost the original outside option, and therefore gain less from a new option. Let us denote the contractor’s new reservation price by inew. In such situations, the manage:r needs to add an additional constraint to its maximization problem. That is, in addition to constraints (14) and (18), the following constraint should be added: Vi, 1 < i < n such that f(0i,ei) = qi, U”(ei,ti) >, Pew. (22) This constraint verifies that even when the contractor finds out more information about the environment before it chooses its level of effort, it will benefit from choosing the level e; and will consequently keep the agreement. Of course, these constraints reduce the manager’s expected utility, and it will need to suggest to the contractor higher payments to make sure it won’t opt out. We will demonstrate this in the case where the contractor is risk neutral as in Example 4.3. !L3. (Risk neutral agents under asymmetric information with opting out Example (cleaning automated agents) .) Suppose the situation is exactly as in Example 5.2, but before choosing its level of effort, CompC’s robot can opt out of the agreement and get its original reservation price (i.e., aneW = ii = 1) . Therefore, instead of constraint (20), the following should be stated: 7-I - 1 > 1, r2-2 3 1. (23) The manager should then offer rl = 2 and t-2 = 3. The expected outcome for the manager will be 6.875 which is lower than in the case where the contractor cannot opt out. 322 5’. Kraus/Art$cial Intelligence 83 (1996) 297-346 5.3. Asymmetric and impegect information before contracting information the contractor’s is initially better We consider the situation where robot may initially have better information than the exact state of the world only after a contract the than CompM’s the state of the world. Only after reaching that of the manager, but that it knows is signed. For example, CompC’s garbage distribution about signing an agreement), the previous section the state of the world, and the asymmetry agreement. On the other hand, in Section 5.1, the contractor already knows the world before signing the contract. That is, the situation of this section that of Section 5.1 and Section 5.2. (after does it find out about the real garbage distribution. Note that in beliefs about arises only after reaching an the state of is between it does not have full information the garbage collection (Section 5.2), both agents have the same preliminary robot. However, in information about site #, distribution and level and the state of the world, As in previous situations, we assume that the outcome is a function of the contractor’s i.e., q = f (e, 8). At no time can the manager states of the world are 8i,&,. the manager does not know the exact that there are D possible probability that the real distribution effort observe either e or 8. Suppose that the possible such that 8; < @;+I for 1 < i ,< n. Furthermore, probability of 0, but rather knows it believes with probability distributions Before signing the contract, either, but it does know which probability assume r as follows: optimal strategy which the contractor can make a binding choice by sending a message problem of the manager Thus the maximization is pd. the contractor does not know the actual state of the world is the correct one. We function of the contractor can be written as a function of q and the from to the manager. lJ”( q, r) = r - e( q, 6) where f (e(q, 0)) 0) = q. In such situations is to design at most D distinct contracts for the manager that the utility is as follows distribution function [ 961: [36] 4d . . , O,,, Maximize( Cy ;,r;, ,..., (q;,,r;,,} 1..., (cq:‘,r:,,....cyp.r,)} D n d=l i=l pd(Bi)U”(qi, ri> (24) subject to: (IR) c pd(&)(rf - e(qf,f$)) 2 ii Vd = 1,. . .,D, n i=l (SS) Cd(ei)(rf i=l - 4qfd4)) 2 Cpd(ei)(r; - e(q[,4)) i=l Vr,d= l,..., D, UC) r” - e(qf,&) Vi,j=l,..., 2 i-y - e(qf,&) nforeachd=l,...,D, (25) (26) (27) is the probability where pd (0i) d ( pd (0;) > 0 Vi, d), q” is the output produced by the contractor contract { (4, that the state of the world is Bi according is the reward to the contractor under that contract. rc)} and $ to distribution in state 0i under S. Kraus/Artijkial Intelligence 83 (1996) 297-346 323 The first set of constraints (IR) (25) guarantees provides him with a level of expected utility price. The se’cond set of constraints honestly about the actual distribution the actual distribution). will produce 4: maximization is as in Section 5.2. problem The third set of constraints in state 8i if it chooses contract {(d, that any contract selected by the agent that is at least as good as its reservation (SS) (26) ensures that the contractor will report (i.e., will choose contract {(k, ~4)) when Q’ is that the agent (IC) (27) guarantees that if D = 1 the rf)}. Note, 5.4. Asymmei?ic information and uncertainty since 5 about information information information the private to improve it will send its prediction is as follows cannot predict by both private that the contractor an effort level. This that are characterized a message or choosing the world, after signing There are some situations the outcome may be. One example of such a situation only provides a better estimation [lo]. and the outcome based on its uncertainty. This means of information, private In what the manager offers the contractor a menu of contracts the first stage of the interaction, based on a message to the observed outcome. The contractor in addition may reject the offer or agree to it and sign a contract. In the second stage, the contractor a contract, but may gain some private before sending the as to what the outcome will be, given its level of contractor the area that it needs to clean, effort. For example, when the robot of CompC about the it determines information world’s state). This information may not be complete, but it is not known to the robot of CompM at all. In the third stage, the contractor to the manager and chooses a level of effort. In the fourth stage the outcome is observed by both agents, to the outcome and its earlier message. Note that and the contractor in such situations, itself not to leave the agreement once the agents can concentrate on the class of it has observed 6. ** Also in this case to the manager. This is contracts contracts, a truthful one due to the fact that it has been shown can be found problem of the manager in the constraints appears in which the expected utility of the agents is the same. The maximization is paid according the contractor has committed reaches of this area (i.e., it collects to send a truthful message for any untruthful to the one in Section 5.2; the contractor’s utility is replaced by its expected utility given 6. the garbage distribution information will help sends a message the contractor tha.t induce is similar [ lo], [ lo] that 5.5. Both parties have private information There are some situations where both the manager e.g., both agents have private information, to concentrate on the effect of the private actions to assume level of effort, there is a probability taken by the contractor that there is uncertainty information information and the contractor have private about their own types. To be able that the of the agents, we assume are observable by the manager. However, we continue about the outcome. That is, we assume of p which is attached distribution that, given a to the possible ** In most of the situations both agents can be made better off through the manager is better off making such a commitment. However, [ 14,26,3&M re-negotiation 1. in some situations, 324 S. Kraus/Art$cial Intelligence 83 (1996) 297-346 (as in Section 4.2). Furthermore, we assume i.e., they will agree that the contractor actions, that is known to both agents outcomes that the agents can agree on probabilistic will choose Suppose then, information, it must actively participate its level of effort, using an agreed upon probability distribution. that each of the agents has some probabilistic in order for an informed manager private formed one, mechanism design. We describe here an interaction procedure properties: The revelation principle holds, which is Pareto-optimal does strictly better [ 681. There are up to four possible stages in an interaction. its opponent’s than an uni- and not only in the that satisfies the following equilibrium types of managers, and the manager generically information the manager’s private there exists a perfect Bayesian the contractor knows for the different in the contract beliefs about to do better than when selection the manager offers a mechanism to the the first stage of the interaction, In contractor which specifies: (a) a set of possible messages (b) that each party can choose, and the contractor for each pair of messages mnr, m, that can be chosen simultaneously the manager a corresponding function of the effort level will be chosen by the contractor probabilistic manager), that the and the effort level are observable by the choice mechanism respectively, probabilistic (note by (c) pairs of outcomes and rewards. In the next stage the contractor the mechanism, it receives accepts or refuses the mechanism. If it refuses its reservation price a, and the interaction ends. (1) (2) level and is paid (3) The agents can send each other the messages simultaneously. (4) The contractor according For example, performs to the outcome. suppose the task at the appropriate effort h’ h can send the agents’ ) ] indicates the manager the contractor of the agents’ (a and b) and four possibilities, level ecr*’ ( w ic sends the message “l”, one for each combination can also be a probabilistic “1” and “2”). The manager should offer a menu of contracts the messages “a” and “b” and there are two types of managers ( 1 and 2). The set of possible messages can include two types types of contractors can send (i.e., that in- the messages types. For example, cludes that if the manager sends the mes- C~lltL”~:[a,l:e~,‘,(q,,r~,‘),...,(q,,r~,’ then the contractor will sage “a” in step (3) and the contractor function of possible effort choose effort is qn, if the outcome levels) and its reward will depend on the outcome. For example, its reward will be rgv’. Similarly, C&3’: specifies a contract when the manager sends the message “b” and the contractor sends the message “ >> 1 . As in previous cases, where the exact type of the manager does not directly influence [ 68,771 show that the manager can profit from the contractor’s The intuition behind it is subject utility of the contractor, when accepting tor’s reservation carried out, the contractor behaves to honest reports. In situations the contractor’s utilities, information. incomplete is as follows. When the manager proposes a contract, that the expected than the contrac- is the contract requires the contract, will be higher to ensure in the appropriate way, given its private to two types of constraints. The (IR) constraint price. There are also constraints the agents can limit themselves [b, 1: eh,i, (ql,rF’), . . . , (qn,$‘)] these results information that when S. Kraus/Art@cial Intelligence R3 (1996) 297-346 325 specific type. That information, that appears types, multiplied in the constraints the manager does not have private for the manager’s the manager, contracts which are functions of the manager’s type. If the contractor has incomplete the constraints need to only be held in “expectation” (IC) . When individually tion about suggested utility of the contractor ities for each of the manager’s is of this type. For example, denotes ConP’ the contract Co&‘,‘. Then, constraint and EU”’ ( Cm& manager’s pa EZJ”’ ( Conf’, ) + p/,EU”’ ( Con&’ ) 2 ii. the constraints must hold informa- over the is, the expected is the sum of the expected util- that the manager in the example described above, EUQ (ConP~‘) the contract from type is a, the to the contractor of type 1 will be EW (Conta~‘) 2 ii, (IR) will be that the manager’s p0 the is (JR) the contractor’s above. Similarly, ElP ( Cm&’ ) denotes (IR) with respect if the contractor knows type is a and with probability pb its type is b, then that with probability the constraint suppose expected utility is 1, and the contractor’s if the contractor believes type is b, the constraint if the contractor knows ) 3 12. However, that the manager’s by the probability expected utility if its type it accepts informed, by violating some of its constraints, there exists a mechanism For this reason, if the contractor its utility above its possible utility is not informed about the manager’s t,ype can increase is fully type, the manager in situations where the as long as they are it was proved in [ 681 in which all types of managers is fully informed. However, the manager must its type at the mechanism proposal stage (i.e., stage (1) above). of a given contractor offset by constraints of the other types of the managers. Actually, that in most of these situations, do strictly better than in the instances where the contractor in order to take advantage of the contractor’s refrain from revealing Otherwise, expected contractor type. This means way upon the manager’s convey of its type, should offer the same mechanism. 23 the in the case that the to be a different in any itself will regardless the manager can’t benefit from pretending that if the selection of the mechanism by the manager depends type, then the selection of the mechanism its type to the contractor. Therefore, any manager, the constraints must hold for the revealed that since all types of managers do better is not informed, tylpes. Note, information, information incomplete individual for just rather about type, than Cases in which the manager’s private information [ 691. In such situations the manager can postpone revealing The manager may wish to disclose the contractor’s total disclosure ties are more complex of generality, interaction. influence between pected utility when lower may be even formation at all. This some of th’e manager’s belief tor’s probabilistic if so then the manager’s proposal actions; and complete concealment. information which it has private than in a case where the manager does not have any private itself about information influences it is no longer its type until the contractor’s utili- true that, without loss the third stage of the to in order should balance the manager’s ex- the contractor’s utility, in- expected utility may be low, given the contrac- types. Therefore, when if the actual (even Furthermore, influences is high is because the contractor’s types denoted by “bad” that its opponent’s type is “bad” *? Maskin and Tirole I68 1 show that any equilibrium of the mechanism design presented here can be computed as a Walrasiar~ equilibrium types of manager. For more technical and formal details see [ 681. the traders are the different of a fictitious economy. In this economy, 326 S. Kraus/Arnjicial Intelligence 83 (1996) 297-346 is not “bad”), it to accept type courage contractor play an important will affect that. is not directly influenced by the manager’s the contractor must be paid correspondingly the contract. Note that in the first case we considered, where high rewards to en- the type, its original beliefs do not type than cares only about how the manager’s but no more of the mechanism, role since the contractor its behavior in the implementation 5.6. Value of information and communication There are two important questions related to asymmetric information situations [ 10, 741: ( 1) Will the manager always be better off, the more the contractor knows about the (2) The second question beneficial world? Is communication to suggest a menu of contracts informing offering only a single contract, based only on the joint observed outcome? to the manager. is it better for the manager to the contractor and ask it to send a message the manager of the current state of the world, or will it be better off is essential when communication to the manager? Meaning, is costly and a knowledgeable itively, it seems that both communications more efficient contracting. The contractor may use its knowledge actions, and with a menu of contracts the actual situation. Surprisingly, case that communications benefits, situations when less information to both questions by the manager is preferred the answer to choose the contractor may select the rewards the managers’ rather their effect depends on the exact details of the situation. There are even contractors will improve and knowledgeable Intu- contractor will allow for the correct to tailored is that it is not always the to more [ 301. is lower As we explained for the managers, or it may use the information the contractor has full private expected utility than its information ajier signing in Section 5.1, when before signing the manager’s the contract, symmetric beliefs. If the contractor acquires then its effect on the manager varies. The contractor may use its additional in two ways: It may use its information benefits two demonstrating an agreement, level of effort (see Section 4.2.3)) and can therefore be used to estimate effort level collection and may therefore be useful gained by setting up a camera the manager with an estimation of the contractor’s the (see information after signing if it is affected by the contractor’s the contractor’s in a garbage effort level information if they have the agreement, information examples then the information to take a low effort level, thereby reducing in [ lo] ). If the manager gains [ 301. For example, is only valuable to the manager. site provides the outcome to improve information is that the “self-selection” The disadvantage of communications received by the manager if the contractor has perfect private information the exact outcome, times be very restrictive so that the information This occurs particularly i.e., given an action, contracts. The manager can then replicate more, even if the contractor does not have perfect information, in which the stochastic outcome constraint can some- is not beneficial. about the world, for any “appropriate” menu of its benefits, using a single contract. Further- there are many situations are such that there is no value for communication [ 14,741. These situations it can anticipate is informative. S. Kraus/Arr@cial Intelligence 83 (1996) 297-346 327 it allows the manager If the outcome is not informative, to pay the contractor for two reasons; because however, 24 then communication It is is valuable. a more efficient valuable level of effart without having it choose correctly, and alternatively, menu contracts can be valuable even though the contractor’s action choices results from the rewards the value of communication are unchanged. to the contractor. There are, of course, situations where the manager can use the given (e.g., later contracts with information other agents). if it cannot benefit In such a case, interaction. the menu of contracts, even in the menu contracts for other purposes In such situations, it may prefer in the current to implement for making gathered 5.7. SeveraI contractors compete for the job in the environment, for example, efficiency and ability), There may be a situation where there are several agents types. If the manager does not know the types of the other agents, and the manager can choose one of them to do the job. The agents may each be of a different type (measuring, set of possible following mechanism by agents’ these reports, a level of effort that is not observable contractor depend upon the contractor’s previous cases, the manager can use, without agents report their types honestly drawn from a the indexed a set of contracts to report their types. On the basis of the manager chooses one agent [ 73 I. *’ The agent that is chosen, chooses to the chosen type and the observed outcome. As in in which the types and asks the potential contractors is appropriate: The manager announces by the manager. The rewards loss of generality, contracts or independently [ 761. 26 reported An aspect is the marginal in the design of the contracts that a specific the probability return consists of the outcome minus important manager by increasing This marginal receives, and minus The latter effect arises because, by increasing it more attractive chosen, prevent Z;. the manager makes thus, the manager must improve to the (e.g., zi) will be chosen. that the contractor to the other types of agents. that a report of zi will be to be zi. To the rewards for all the types that are higher than rewards the probability for higher type the rewards types to pretend in the expected the increase return If the agents’ types satisfy the appropriate conditions to the above described aspect, and if the highest for the manager. However, related the contract may be optimal lower than in the case where it can observe the “second best” benefits). that are (see details reported then the manager’s benefits will be the contractor’s effort level (i.e., it gets only in [ 731) type is chosen, the agents’ 24 See I74 1 for exact conditions. ” There am situations where about different aspects of the contractor Techniques in 1.54.71 I. *’ The measure of risk aversion will influence contractor averse agents the maximization to formalize in service of any risk averse manager [ 921. types are multi-dimensional. That is, the manager that are independent; for example, in such situations, and methods its capabilities problem is uncertain and its disk space. to solve it can be found in the environment. A less risk averse agent will usually have the ability the agents’ behavior when there are more than one possible to win over more risk 328 S. Kraus/Arti&ial Intelligence 83 (1996) 297-346 6. Repeated encounters the manager wants to subcontract its tasks several Suppose of contracts is signed before beginning of each encounter are possible the repeated encounters a new contract (finite) times. Two types term contracts, where one contract i.e., in the start, and short term contracts, is agreed upon by the agents. in such situations: Long 6.1. Short term contracts the is large the manager and the contractors enables Repetition of the encounters short between term contracts to reach efficient agents enough 27 and if the contractor can be “punished” the manager the average outcome, if the number of encounters [ 27,43,65,85,84], sufficiently could effort over a certain amount of time. That form an accurate estimate of is, if the manager wants to make a certain effort level of & E Effort in all the encounters, it can the expected outcome over that certain amount of time if the contractor actually the task with that effort level. The manager can keep track of the cumulative If after several is below a given function it with the expected outcome. that the cumulative outcome realizes Based on the contractor’s the contractor compute performs sum of the actual outcomes and compare encounters of the expected outcome, the function over the expected outcome imposing a “punishment” when the contractor level, can be made very “punishment” Suppose there is asymmetric low. Meanwhile, the manager it should if the agent does not do C is 1.0. on the contractor. If [ 851, then the probability of is in fact carrying out the desired effort the of eventually imposing the probability information where we assume that in each of the encoun- impose a severe “punishment” is chosen carefully is similar that in each encounter to that of Section 5.1, meaning t, the ters the situation outcome q’ is a function of the contractor’s effort level e, and the state of the world’s 8, to the other). The outcome at time t does not (which may change and the states of the world depend on the contractor’s the in the encounters manager offers a reward function of Y, (q’), and the contractor chooses its effort level based on the state of the world, then only the reward function and the effort second best contracts can be achieved and we denote in previous encounters, and identically distributed. 28 In each encounter, from one encounter actions are independently If there is a single encounter, i.e., et(&). ?’ In I85 I the number of encounters should be larger than some thresholds, but finite and known to the agents. Fudenberg et al. I271 assume that there is a terminal date, T, such that after T the manager’s profit will no longer depend on the contractor’s actions, that there will be no additional information arriving, and that the manager won’t give the contractor any further rewards. However, the contractor may be inactive for some of the periods, and in particular, the contractor may opt out before date T. They don’t assume that T is large, but rather make other assumptions such as that there is common knowledge of technology and preferences, [43 ] Malcomson and Spinnewyn [ 65 I don’t assume that T is large, but make additional assumptions about the agents’ utility functions and about the and equal access to banking. Also, Holmstrom and Milgrom environment. For example, Holmstrom and Milgrom assume that the contractor has access to unlimited saving and borrowing at the same interest rate as the manager. ‘s In I27 1 it is assumed that past actions and signals can affect current outcomes and signals, as long as these dependencies are publicly revealed. S. Kraus/Artijicial Intelligence 83 (1996) 297-346 329 level function by (r*, e*> . We denote outcome in this case for the manager and the contractor by D and 2, respectively. the first best solution by (3, e^) and the expected than the restrictions to be within epsilon they will not bother on the agents’ strategies the other agent’s one-period the average of each agent’s expected utility The notion of an epsilon equilibrium [ 851 will be used, although the epsilon equilib- in Section 3.2) of of the is, that if the agents have sufficient [24]. The main motivation In every perfect equilibrium in every period the outcome it imposes weaker imposed by the Nash equilib- an epsilon equilibrium is a pair of strategies from the for to to the other agent’s strategy. One rationale inertia, for using (defined is a Nash equilibrium restrictions rium. For any positive number epsilon, that allows expected utility of the best response epsilon equilibrium small gains realize possible rium concept is as follows: the (finite) T-period game, one-period game. On the other hand, in infinite multiple stage games (i.e., T is infinite), there are perfect in which each agent can observe equilibria of the game which result (in in the use our situations, of Pareto-optimal that for any large, then there are epsilon equilibria of the T-period positive epsilson, if T is sufficiently game (i.e., 7 is finite) which results in all or most of the com- infinite horizon repeated games ponent one-period games. That is, for epsilon equilibria, may be well approximated the number of repeated games is very large, as is indicated by the perfect equilibria “Folk Theorem”. 29 However, strategies can be limited by considering in situation by the “trigger strategies”. The epsilon equilibria trigger strategy It uses the effort level function C until the first encounter where the manager does not use the reward function i; at that encounter the contractor will optimize against the reward function announced encounters for the contractor, denoted by p, is very simple: [ 841, and first best strategies can be sustained by long finite horizon games. Unfortunately, in each one-period game, particularly the number of possible equilibria in the use of “cooperative” the first best strategies) in cooperative behavior In the same situation, and in each encounter pairs of strategies. pairs of strategies in infinite horizon of the multiple it was shown strategies” strategies, thereafter “trigger The suitable trigger strategy is too lax, then through encounter is a little more complicated. t, based on the history of outcomes In each t - 1, the manager must encounter to make the reward i or switch to the reward function r*. If its switching decide whether a large enough extra rule the contractor may be able thereby making cheating attractive. expected utility by cheating before getting caught, then there will be a substantial On the other hand, that the manager will switch to r* before the contractor ever starts cheating. probability We define C, = f(e,( 0,), O,), i.e., C, is the outcome t if the contractor uses the effort level function e, and the state of the world is 8,. We define S,, to be the the sum of outcomes outcome cumulative sum of outcomes by the end of encounter n. The random variables C, are independent in periods 1 to n, that is, S,, = Cl + . . . + C,,. We let C, denote t if the contractor uses e^, and let 3” be the corresponding rule is too strict, if the switching to accumulate in encounter in period for each encounter. for the manager says 29 This theorem theorem finite action sets at each repetition, any combination a unique outcome of a sub-game perfect equilibrium. that under certain conditions (see is called “Folk Theorem” because no one remembers who should get credit for it I 51. The [ 1,5,25] ) in any infinitely of actions observed repeated n-person game, with is in any finite number of repetitions 330 S. Kraus/Art@cial Intelligence 83 (1996) 297-346 identically and be a strictly variables increasing fi and N by: distributed since the 8, are so. Their expected value sequence of positive numbers (n > l), and define is 2. We let b, the random fi = min{n 3 1 / S,, - n2 6 -b,}, N = min{fi, T}. (28) The following each period strategy by U( (b,,)). We define B as the class of positive sequences ([851): trigger strategy should be used by the manager: Pay the contractor r*. We shall denote (6,) through N and thereafter use the reward function i in this that satisfy l b,, are strictly l There exists A > 1 such that b, 3 Abz, n > 1. increasing, and lim,,, b,/n = 0. The main result of [85] on these strategies sequence is an .s equilibrium, respectively of at least (D - E) and (2 - E). ” and yields (6,) is as follows: For any E > 0 there exists a (a( (6,) ) , p) average expected utilities the manager and contractor in B and T,, such that for all T 2 T, the pair of strategies 6.2. Long term contracts the previous In manager offering a contract prior be more complicated t - 1 encounters. section we assumed that the number of encounters and contractors may be very large. This enables the manager’s in a given time period t, to depend on the average outcome If there is a limited number of encounters since there is not enough information between strategy the for in the the contracts need to that has accumulated. to its average performance, (e.g., about is small take a low effort the state of the world that the agent is evaluated according the number of encounters in the first encounter, (i.e., each single encounter two encounters). For example, suppose is uncertainty there Section 4.2) and is “lucky” contractor encounter it can encounters. The contractor, function contracts function of the average outcomes to adjust when the number of encounters very large, such behavior will eventually be detected. is as in If the the outcome will be high, and in the second the sum of both affecting its effort over time as a the optimal is small, will not be a simple trying its effort over time as a function of its previous performance, may also arise is of its previous performance. As a result of this phenomenon, in situations where in general. The problem of the contractor if the number of encounters the number of encounters is very large. However, level without adversely is motivated therefore, to adjust [57] The problem of subcontracting when the number of repeated encounters in [57]. considered It is assumed to a long term contract encounter The outcome of each encounter unobservable known to the manager), that the manager can commit is small is the first itself before during all their encounters. level effort that will be implemented depends on the contractor’s is and the state of the world in that encounter, which is not [57], there are only two encounters (which to either agent, as in Section 4.2. Suppose xl In 1861 the situation of symmetric information with uncertainty is considered. That is, the situation of a single encounter is as in Section 4.2. It provides Pareto-optimal strategies only in the case that there are infinite encounters. S. Kraus/Artijcial Intelligence 83 (19%) 297-346 331 then the effort to the contract. it should choose is paid according the first encounter is accepted by the contractor the manager offers a binding contract. Then the reward in the outcome of that encounter, but the reward of and before the first encounter will depend upon the second encounter will depend upon the outcomes of the first and second encounters. If the contract the first encounter. The outcome of the first encounter the contractor chooses an acffort level which outcome of the second encounter the manager chooses given. When similar in the maximization encounters. Similarly, it should consider the effort le~~els chosen by the contractor the manager desires. increasing level of is observed by both agents, and the contractor In the second encounter, is a function of the outcome of the first encounter. The is also observed by both agents and the rewards are problem the contract, that appears in both (i.e., IR and IC) on the appropriate constraints in both encounters. Subject to these constraints, rewards over time in any fashion ( I) should be replaced by its expected utility function of the outcome of the first encounter. is able to update in [57] the contractor’s that the rewards to that of Section 4.2. However, that it should be an it should solve a maximization in the second encounter expected utility It was shown the manager’s expression 7. Subcontracting to a group Suppose is independent ,:hat the task the manager wants to contract out can be performed by a group its If one of the the manager choose effort levels. As in previous sections, the effort levels and the members of the group while they carry out the of agents. Each of the contractors own utility. The manager offers a contract them rejects contractors can simultaneously cannot observe task. the offer, then the manager cannot subcontract in the sense that it tries to maximize to each of the possible contractors. the task. 3’ Otherwise, 7.1. Individual outcome is observed In this section we assume that each contractor yields an observable outcome of 9; and that the overall outcome will be equal to the sum of the 9i. The advantage of using is that usually some outputs the multipl: the whole about information array of 9; s [79], actions can be estimated by comparing the state of the world can be concluded i.e., in such a situation, to form the basis for a reward of the different agents. the performances from observing to each agent the individual 7.1.1. One agent’s effort does not influence 7.1. I. I. The contractors have symmetric is a probabilistic function of its effort that the individual automated aspects are Ei, i.e., 9; = f(e;, d,ei). For example, agents case, 8 could reflect the garbage distribution the other agents’ outcomes information. Suppose level ei, that the state of the world the outcome for an agent is 0, and in the cleaning in the whole site, while ” We will also consider below the situation where. if an agent accepts the contract, it will be implemented regardless of the other agents’ responses. 332 S. Kraus/Artifciul Intellijience 83 (1996) 297-346 E; represents the garbage distribution in the exact location of contractor i. Each of the contractitrs observes 8 before it chooses its effort level, but it does not observe ei before making its choice. 32 We assume that the contractors are identical, i.e., have the same utility function UC(e, r) = U(T) - c(e) and the same abilities. We will assume that f( ei, 8, ei) = e$ + &i and that &, are the distribution functions of ai. In the first model, there is no exchange of messages between the agents. Since only the outcome is observed, this is the only thing the rewards depend upon. The main question to be asked is: Is it better to make a contract based on all the outcomes, or is it better for a contractor’s reward to depend only on its own outcome? When the contractors’ outcomes are independent, then observing all the qi provides no additional information about the contractor’s effort. In this case, the rewards should depend only on the individual outcome. Sometimes it is possible to find enough statis- tics from 41,. . . , q,,, denoted by T( (41,. . . , q,,}), about the state of the world. The rewards of a specific agent should then depend upon its individual outcome and on [79]. For example, if both 8 and E are normally distributed random V{ql*... variables, then the average value of {ql , . . . , q,,} provides sufficient statistical informa- tion for 8. When the number of contractors becomes very large, the estimation of 6 converges to the true value. In such situations, the rewards should depend on qi and on the estimation of 0. , q,,}) Another option when designing a contract for a group of contractors is to pay the contractors according to their ordinal positions alone and not according to the actual size of their output, i.e., to encourage a contest among the agents. Suppose there are two contractors; using the contest approach, there is a winner’s reward rw and a loser’s reward r/. The winner’s output qw is not necessarily worth rw, so that the winner is actually paid more than its contribution to the overall outcome. This is done in order to motivate the contractors to choose greater effort levels. A larger prize for the winner motivates greater effort by all agents and increases the manager’s outcome [79]. If the first contractor chooses effort level el, and the second chooses effort level e2, then the first one will “win” if BeI + ~1 > Oe2 + ~2. Each of the contractors tries to choose higher levels of effort in order to be paid r,,,. However, even though they both choose higher effort levels, it does not increase their probability of winning (which is, if we speak of symmetric equilibrium, $ ). The expected utility of a contractor i is therefore, i [u(y,) + U(Q) 1 - dei). (29) The details of how to compute rw and t-1 in a given situation are described in [79]. An interesting result from this is that in some situations it is possible to make the contractors choose an effort level, using the above “contest” mechanism, which is even larger than when the manager can observe the agent’s effort levels, i.e., better than the first best contract. A variation of this method is when the “winner” must win by an amount greater than a certain margin. That is, instead of ranking contractors solely on the basis of the relative position of their outcomes, the manager can rank one contractor above another if that agent’s outcome is greater than its opponent’s by a positive margin. 32 We consider the case where a contractor cm alter its effort level ufer observing q in the next section. S. Kraus/Art@cial Intelligence 83 (1996) 297-346 333 The introduction while maintanting of “margins” can lower the probability that any “prize” will be paid the same level of motivation for choosing high levels of effort. There are several other methods for possible rewards to the agent whose output for members of a group. For is the highest or punishing example, giving a reward only that came the agent generally more flexible, and reduce in last [ 791. Rewards that are based on relative performance are the risk taken by the contractors 1781. correlation 7.1.1.2. The contractors have private information. In this case we assume of the contractor’s outcomes in which each agent can only observe is a probabilistic other’s aspects and the manager cannot observe any of them. For example, of CompM of a third company, garbage collection in adjacent sites, however, does not know either distribution. that each is affected by different aspects of the state of the world, its own private “aspect” of that world. There these aspects, but agents cannot observe each if a robot robot and a robot in its own the garbage robot, its garbage collection then each of them can observe site before signing the garbage distribution the garbage distribution they gather at their sites are correlated. CompM’s the contract, and since task to a CompC’s subcontracts between Suppose f’( e’, 0’), by of and assume states with two possibilities that for each I, f’( el, (3: ) < f’( e’, 0:) that 8’ = 0; for i = 1,2. We denote there are only two agents, A and B, and two output functions I= that 13~ can be 0: or 0: (i.e., the world can be in four for each variable). For I = A, B let @(of) be the that the level of effort, e’, is for all a “good” state and 0: a “bad” state. The state variables are correlated. We denote by sp the probability of @ = 0!, given that 0’ = 0:. A, B [ 631. Then we also assume different this probability probability pf > 0 and that @(e{) + @(ok) = 1. As in previous sections, not observable. We do assume, however, e’, therefore., 0: represents positively but imperfectly that BA = B,! and similarly SF denotes We assume Agent The manager are similar contractor 1, and the state of the world, one can compute the “disutility” of producing an outcome such as q’. The contractor’s utility can, therefore, be expressed as a function in Section 5.3). We will of the rewards and reservation price assume is fi’. A typical contract to agent A in this case, is of the following I (= A, B) privately observes 8’ before signing a contract with the manager. functions thal: UC,’ (q’, rl) = d ( r’) - d’ (q’, 0’) and that the contractor’s that can be offered by the manager is risk neutral and the contractors are risk averse. Their utility (as we did, for example, in Section 7.1.1. Given to that which appears that 1 > s: > sk > 0. of BA = 0; given function of the the probability the outcome form [63]: the utility You may choose on your output, but also on what agent B will produce. qf (i E {1,2}), to produce either q;\ or q2. * Your reward, r* will depend not only to produce If you choose then l if agent B produces qf, you will be paid r-i, l if agent B produces qf, you will be paid r$, l if agent B does not sign the contract, you will be paid r$,. In [ IS] the maximization problem of the manager was stated. It restricted tractor’s output choices to a Bayes-Nash equilibrium, given that they are guaranteed the con- at (30) (31) 334 S. Kruus/Art+ciul Intelligence 83 (1996) 297-346 least their reservation price (conditional 1= A, B. on their private information). This is done for Maximize,l.,~,,i,.i~{,.2} pfrs’,(q: - r’I,) + (1 - s’,)(q’l - &)I +P:b:(q: - r:,> + (1 - s:Hq: - &>I subject to: (IR) (IQ &+(Yf, > + (1 - S(>U++z) - d”(qf,Bf) 2 fi’, i = ],2, S;uI(Tj,) + (I - Sf)L’l(r;z) - d’(qf,@) asfu’(rf,) +(I -sj)u’(r$2) -d’(q:,@f), i,j= 1,2;i #j. (32) The result of this maximization contractor r{ , > r{., and ri, = & = r-2. These contracts yield to the manager expectedoutcome. provides the output qi when a it observes 0:. The reward will satisfy the highest possible If the manager offers each agent I = A, B the choice of the manager with rewards that discourage from choosing l producing qf and receiving a probabilistic l producing q: and receiving a sure reward of rk, reward of {r’, I) r’,,}, or for both contractors that level of effort is, however, that the manager wants to implement. in the equilibrium if they choose from the contractors’ point of view, is better the constraints if there are two agents, if both agents respond as the manager the output qf when they observe that the contractor will choose there exists another pair of to then the manager will get the maximum outcome desires, i.e., sign their respective contracts and produce ensure 0:. In the case of a single agent, level. However, the desired effort strategies whose outcome, equilibrium than the outcome both agents for the manager, The outcome to always choose the outcome there is an equilibrium In particular, q{ (regardless of their observed state), and in all states they will both be strictly better off than in the equilibrium (i.e., choose q{ if the state is Sf in this case, the manager will definitely be worse and qi if the state is 02). Of course, the incentive constraints of one contractor, off. It was suggested so that its chosen strategy will provide a better outcome for the manager. But although this method does guarantee a unique equilibrium, the contractors is sug- gested in [ 631. This method, however, makes the contracts more complicated. The main idea is that the manager offers one of the contractors, e.g., A, a range of extra possible indexed by E, where 0 < E < 1 - sp. If agent A chooses one output options q?(E), of these options q?(c), that q;\(c) has some produces qf, except “E” which is costless for agent A to effect. The importance inconsequential modification that agent A sends to the manager: of E is that it acts as a signal A costless method of making preferred by the manager the “correct” strategies it is also costly to the manager. to strengthen it essentially in [IS] choose then low [ 151. “Agent B is cheating; is at least sf + E.” from my perspective, the probability that B is choosing qf3 the manager will pay In light of such a signal it an amount FB, where uB(Pf) = $#(rf,) That is, the manager pays agent B the equivalent of its expected utility as if it had observed 0;. However, from agent A, if agent B chooses qf, + (1 - st)uB(rf2). S. Kraus/Artijicial Intelligence 83 (1996) 297-346 335 A’s choice A’s choice 4; Y;(E) 4 Refuse Agent A’s payments B’s choice B 91 Agent B’s payments B’s choice 4; 4 rf + Y 4 r! - Y B 91 r* II rr’, + S(E) r* 2 4: rB II P; r:: rB II Refuse r* II rf, + S(E) ri - Y Refuse Fig. 2. The contractors payments according to Ma et al.‘s mechanism for making the contractors choose the equilibrium preferred by the manager. y > 0, S(E) and I(E) are continuous functions that are both strictly positive for 0 < E < 1 - s;“. if agent B actually chooses &, and agent A signals some E > 0 by choosing qf ( E), (r.f + y). 33 The details then agent B is compensated of the payments in Fig. 2. The continuous functions S(E) and t(a) in Fig. 2 are both strictly positive for 0 < E <: 1 - st, and satisfy a higher payment in [63] are specified that appear in A’s payments to the agents described by receiving (3:’ -t- 8) (qf, - rf, + S(E)) + (1 - s;’ - e) (9;: - r;4 - t(c)) -r-f,) + (1 - $ - e)(q$ = (s;’ +.5)(qfl - rfl). (33) scheme” it assesses is as follows. Consider preferred by the manager that agent B is choosing qf’ more ‘of this “reward idea The observed t$“. Suppose choosing 4’7 in the equilibrium with probability S > s;‘. Using construction t(c) are both positive, we can conclude to choose q?(E) rather equilibrium E > 0. The proof that this mechanism provides a unique equilibrium manager contractor A which has than B would be that is described above, e.g., together with the fact that S(E) and that for all 0 < E < (3 - s;‘) agent A prefers the output as in the to signal some the its second best outcome can be found then A does not have an incentive than qf. On the other hand, preferred by the manager, that guarantees if B chooses in [ 631, (33), 7.1.2. The contractor’s effort injluences others In this zsection we consider on its level of effort and the other contractors’ situations where the output of a contractor depends both is levels of effort. In addition, there 33 The increase y > 0 must not be too great, since it turns out that too high a compensation (Rf admit unwanted equilibria. See [ 631 for more details. + y) might 336 S. Kraus/Art@cial Intelligence 83 (1996) 297-346 symmetrical contractors, uncertainty about the state of the world. Suppose there are k possible and for each contractor i depends on some unknown i there is a finite set of possible outputs Outcome’ . . . ,q1,} and a finite set of possible effort levels, Effort;. We denote = the vector of i.e., Outcome = { (ql, q2,, . . , qk) 1 q’ E Outcome’}. features of the level of effort as we . . . , ek, O;), and 81,. . . , ok {q;, the possible outcomes by Outcome, The output of contractor world B;, in addition mentioned has a joint probability distribution another probability distribution over vectors of outcomes, as in Section 4.2. This means, case; ga : Effort, x Effort2 x . . * x Effortk x Outcome + R, such that for any e’, . . . , ek, ei E Effort;, )&ou,come gde’, . . . ,ek,4> = 1. induces distribution for any given vector of actions, that we extend @ of Section 4.2 to fits the multi-contracted to its level of effort and the other contractors’ . . , ok). This probability is denoted by fi(ej, above. The outcome (by all agents) function ~(OI,. a forcing If the manager can observe the actions chosen by the contractors contract. then, as in Sec- If the manager cannot observe it should offer will specify for any vector of outcomes tion 4. I, it can offer the contractors the effort levels, then the contract (91, , . . . , qk,), a vector of k rewards denoted by (rf ,,,,,, it, rz ,,,,, ir,. . , rt ,,,,, J. Similar in the case of one contractor, problem the maximization its expected utility given similar constraints cedure, similar any effort that minimize vation utility constraint (IR) (5) and participation given r’ , . . . , rk, the contractors will prefer e’ , . . . , ek over their other options. situations, depending between the B;), and the contractors’ utility expected utility as in the case where it can observe a first best contract) the manager should maximize to (IC) (3) and (IR) (2). A three-step pro- to the one contractor case of Section 4.2, can then be formalized. Given r’, . . . , rk, to the reser- that In some if there is perfect correlation the manager may gain similar the agents’ effort levels (i.e., as in function Q (e.g., functions34 level’s vector e’, . . . , ek, the manager the rewards, subject it should pay the contractors, (IC) (6) meaning, on the probability the expected constraint rewards should [75]. find to In some situations, however, the contracts found by the above maximization implement to uniquely may fail section. There may be other actions according contractors, others. as in the previous section, where the manager’s preferred problem actions, as in the previous to the that are better the the agent’s effort does not influence to the contract it prefers. One approach is how the manager can make the contractors is to try to strengthen is costly the set of that are related for the manager. Another possibility, contract. We may distinguish but this, of course, section, is to construct a sophisticated the constraints choose The main question actions to the contractors, as in the previous between two situations: ( 1) Actions are mutually observed by the contractors (2) Actions are only privately observed. (but not by the manager). In the first case, the contractors pick an effort level simultaneously, (but not the manager) observation The manager can try to extract information they can observe each other’s actions. There is some delay after the and the realization of the outcome, which is then used for message exchange. about the effort levels from the agents and and afterwards j4 The exact restrictions on the contractors’ utility functions and the environment can be found in [ 75 I. S. Kraus/Art$cial Intelligence 83 (1996) 297-346 331 although is known verification. the contractor to the other contractor. The manager may then appeal false information, can provide the accuracy of this information to the other agents for We will consider the case where there are only two contractors [62], denoted by - ccc). Suppose by using the agent’s to choose effort levels e: and the manager can observe the same utility the manager would in order to maximize sections, and assuming its own expected utility, function UC( e, Y) = u(r) then like the two contractors A and B, with the techniques of previous actions, et, respectively, their reservation prices. ri can be the payments the manager can observe efforts, i.e., UC( e:, I,*) = ii, and similarly, for the second contractor. Note, that since U’(e, r) = u(r) -c(e), The aim of the manager the above utilities will be awarded to report or to declare eh,, eh,, E EJj%orte, q’ E OutcomeA and 9’ E OutcomeB, that A did not report honestly. We assume is to make sure the agents that will be awarded in a unique equilibrium. The main the effort levels chosen by the agents, and then ask B to confirm find (e(:, ei) attractive and then idea is to ask A the report that for all e,,, e,“, E EfsortA, the following holds: 35 taking into consideration to contractor A if rz can be the reward u(r;) = ii + c(e:). (&e,,,,eb,, t&never (9i,9i)))i,j (et,, v eh, > + # (63(e,o,,,e6,,, (9iv9i)))i,j ( e,,, , eb,, ) . (34) Note that by condition and el,, E f$ffortB, are chosen by the agents, A cannot announce (34)) for any pair of effort levels (e,, , eb, ) , where e,, E Efforts (eo,,eh,) (&, e$,) # to examine the truthfulness eh,, (9’, 9i)) )i,,i = (P( e^,, e’j,, (q’, qi) ) )i,j. TO see how the manager can use of another’s, we will suppose A reports levels to “challenge” with (Q(Q, one agent’s report (e^,. e>) where &, E EffortA and &b E Efforte concerning chosen by the contractors. Subsequently, A’s report. If B challenges, reporting function E to give B an E : Outcome x EffortA x EffortB x EffortA x Efforts + lk and E satisfies an alternative pair of effort levels. On the manager uses the following that (&, 61~) where Co E EffOrtA incentive the truth. Let E be a function B is allowed an opportunity the pair of effort and t?b E Eff01-t~ it announces to tell such then As we mentioned cannot announce above, by condition (34), for any pair of effort levels (e,,,, eb,), A (&, Ljj) # (e,,, , eh,) with (35) Suppose report If B reports the actual effort levels pair is (e,,, eb,). B’s behavior will then depend on A’s to challenge A. (as described (e;l, Q,) = (e,,, eb, ) then it gets additional (&,&j,) # (e,,, eh,), then B prefers if A reports ( &, t&h ). First, expected reward below) of~~~y~,yj~EOurcomee((qi,9i),~,ej,,ent,e~,)~(ent~eb,~(9i~~)) which by (35) is ” Below, ( ~4 e,,, , ef,, , (q’, qj)))i,j denotes the vector of probabilities for any (q’,~$) E Outcome. 338 S. Kraus/Art$icial Intelligence 83 (1996) 297-346 A’s announces B “agrees” B “challenges” A’s announces B “agrees” B “challenges” e> = a* C r,* - Y Fig. 3. A’s rewards; 6 > 0, y > ra - f’ + S > 0. e-1, = h* If{, + h’ Fig. 4. B’s rewards; 6 > 0, y > r,* - f’ + 6 > 0. positive. Second, if A has reported truthfully that (E?~, e^b) = (eak, eb,), then by (35) the expected value is negative. Hence, B will avoid (falsely) to the contractor following mechanism: that A has been lying. Using for B from C(q,,d)EOutcome&((qi,4.i),eat,eh,,e;?,~)~(ea~,eh,, accusing A, and B’s “challenge” (q’,&) is a signal these ideas the manager should offer the l+: Contractors observe each other’s action. take actions simultaneously. Stage 1: Both contractors Stage Stage 2: Agent A announces a pair of effort levels: (&, e^b) where (e^,, &) E EffOrtA x Effortg . Stage 3: Agent B can either “agree” or “challenge”. then ment (eG,ei,). it announces (e”,, e”b) where (&,e”b) E Efforts X Efforte but If B “challenges” A’s announce- (c”, e”b) # The rewards, as a function of the outputs q’ and qj, are described the reward that satisfies o(a) = fi + min,{c(e,) for B we denote the reward that satisfies u($‘) = ii + min,,{c(eb) in Figs. 3 and 4. ( e, E Efsort~} by c’, 1 eh E We denote and similarly Effort,,} by Lo. It was shown in [62] that the following strategies form a unique perfect equilibrium if A is honest at Stage 2. The intuition behind of the described mechanism: Agent A chooses ez at Stage 1, and reports honestly at Stage 2 which action pair was chosen at Stage 1. Agent B chooses ei at Stage 1 and “agrees” at Stage 3 if and only this proof information from agent A-and uses B’s reaction as is as follows. The manager elicits then its outcome a policing device as we explained above. from E to B is depends on E. However, due to assumption valuable report honestly, the required actions. These results can easily the rewards will motivate be extended the expected outcome if and only if A has lied. In addition, given that the contractors If B accuses A of lying, (35), to the case of more than two contractors them to choose [ 621. In the case that actions are only privately observed, it is not possible to implement (i.e., the results of perfect observation the manager observes the second best of the section are appropriate only if the agents manager. is not so simple. The rewards the contractors’ It is possible, however, the first best contract, where actions). However, even the implementation the result is that of that were suggested in the beginning that the contractors may be better off (given the actions prescribed by the the follow S. Kraus/Artijiciul Intelligence 83 (1996) 297-346 339 rewards) suggested mechanism second best contract. is presented if they all deviated from the required actions. In [62] a multi-stage that makes the contractors choose the appropriate actions of the 7.2. Individual outcome is not observed in making there is a problem There are other situations there is no way for the manager in which the manager cannot observe the individual outcome the overall outcome i.e., the state of the level to find out the effort level of each of two robots have it is not the contractors (or such an outcome does not exist), but rather can only observe in the case of certainty, [42,87]. Even of all the agents’ efforts world is known, of action, since the individual agreed possible to take the vector of effort outcome is, if all agents choose them does not, none of them gets anything. it can search for a contract such that if the is 4’ 2 q( e* ) , then r-i(q) = bi and otherwise 0, such that UC (ef , bi) 2 ii. That effort level, each of them gets bi, and if any of to then figure out who collected what. If the manager wants to collect garbage, but they both put the garbage the overall output. For example, in the same truck; take the preferred the appropriate level e*, then the contractors agents, given suppose In some c:ases the contractors take sequential actions. That its part of the task which is observed by the other contractors, is, agent 1 chooses its (including in the environment, effort level and performs but not the manager. The second contractor its effort first agent’s actions, and on. After finishes the last agent out and observed by all agents some uncertainty presented effort levels exerted by contractors i+ monotonic can construct a contract enables agent 1 ,..., i - 1, to use its monitoring in Section 7.1.1: f(e’,. I,..., level then, chooses is observed by the other contractors, its part, the outcome of the whole vector its effort level, based on the and so is figured there is also to the one . . , e*) + E. If, no matter how low the for the rest of the agents for the slack and if for fixed effort levels el, . . . , ei, z is a 1,. . . , i are, it is possible function may be similar . .,e”) = z(e’,. If, in addition, the manager). the outcome n to compensate function of the effort level of the rest of the contractors, its first best outcome it can obtain then the manager [ 71. The contract i, whose choice of effort level is a function of the effort levels of agents effectively. capability in which situation Another interesting is when a group of contractors they can still be individually motivated (under appropriate conditions) to cooperate. Although a cooperation them. An ev’cn more efficient and share the outcome. Such a situation may occur, for example, are robots of CompC, can commit themselves if they can agree upon can be better for all of if the contractors work as a team if all the contractors task to maximize CompC’s profits result may be obtained level, the outcome the same general that have 1641. 8. Conclusions In incentive this paper we presented techniques contracting of a task by an agent that can be used to another agent or a set of agents in different cases where in non- 340 S. Kraus/Art$cial Intelligence 83 (1996) 297-346 tries to choose to convince environments the manager to carry out and described the task, and several such situations level the contractor is beneficial. These techniques are useful when the contractor coliaborative can choose an effort incentive We considered should be solved by the manager The contractor’s the situations, inequalities contractor needs order to decide whether on the suggested contract, which effort level, similar to find an the effort level that the manager prefers. that for itself. In most of the validity of the problem. The in to accept the contract, and since all variables are known, based the contractor needs to decide its effort in the participation constraints this check is very easy. When it should consider task the contractor needs only that of the manager. to check level to the maximization computational given a contract, in the manager’s maximization the validity of the individual that appear as constraints to design a beneficial its expected utility problem described the maximization to provide, rationality constraint to check problems is easier in order contract from than (IR) (IC). problems. The maximization these maximization to solve are much more difficult. for that can be used as the basis the manager needs problems In most of the situations we presented procedures In general, solving by a single, all purpose, method are therefore classified objective were developed ages available using a variety of practical optimization methods for automating interface between special purpose procedures pack- that can be used those procedures. The designer of the automated agent should build an problems problems is defined by the solutions of optimization and inefficient. Optimization into particular categories, where each category the chosen package and its agent’s software. there are several computer optimization function of the maximization for each case. Currently, and the constraints; is cumbersome [22] The agents’ utility functions influence the efficiency of the subcontracting itself and the computation problems. presented maximization algorithms obtained time required for finding efficient contracts and solving It is clear, that when the agents are risk neutral, all the maximization in this paper are much easier to solve. In this case the objective the maximization problems function of the problem, as well as its constraints, are linear, and there is a polynomial to solve the maximization problem. Furthermore, more efficient results are in such situations. to be risk averse, then not all utility In order to support most of the results separable shofild be additively function However, if the designer would like its agent for incentive contracting. the contractor’s utility functions are appropriate in this paper, presented in rewards and efforts in the form Uc( e, r) = u(r) and c” 2 0. However, a large set of utility these properties of the utility it seems reasonable cases, the objective as the constraints. The library maximization in the limit.36 The agent functions that agents’ utility function of the maximization routines problems, generate an iterative sequence functions seem useful also functions will satisfy satisfies these requirements, in other settings, these conditions. function may be nonlinear, - c(e) where v’ > 0, U” < 0, c’ > 0 and therefore, In these as well for solving such in to the solution conditions in available computer packages that converges that uses the routines may stage the convergence 36 In all the cases that we considered, if the utility functions satisfy the above conditions, there exist solutions to the maximization problems described in the paper. S. Kraus/Arti&ial Intelligence 83 (1996) 297-346 341 that will fit its computation results with symmetric for the different information situations considered situations are as follows: and time limitation. Below we present a summary of the in this paper. The results of contracting ( 1) If the manager can observe (2) (3) (4) to provide the contractor’s the contractor’s actions (Section 4)) then it can force the effort level preferred by the manager, and thus the its reservation price. its utility, and the contractor obtains there is no need for the manager’s observation. actions, but there is full infor- the outcome of the contractor’s actions to both agents is as in the previous case. the contractor manager maximizes If the manager does not observe mation and no uncertainty concerning (Section 4.1) , then the expected utility That is, in this situation, (Sec- If there is uncertainty tion 4.2.1), two cases (i.e., the ,agents reach a first best contract). The expected utility of the contractor will be equal its actual outcome may be less or more than its reservation price. If there is uncertainty (Section 4.2.2), previous case (i.e., expected utility then the manager’s utility will be as in the previous reach a second best contract). The contractor’s as in the previous case, but the contractor is risk averse in the expected utility will be lower than its reservation price. then the manager’s in the environment but the contractor to its reservation price; however, is risk neutral the agents is higher than (5) Monitoring (Section 4.2.3) cannot increase its utility improve the manager’s utility in case in case (4), when the contractor (3) is risk above, but it may averse. is a need for the exchange of messages. However, then the contracts should include a menu of options the in all the situations, it is in the interest of the contractor to is a summary of the results of the main in which information. Below If there is asymmetric information and there agents can consider only contracts rlsport its private honestly cases in asymmetric information ( I ) If the contractor knows situations: is higher. expected utility task but only after signing is able to collect more information before it performs the agreed the contract, and the contractor cannot opt out then the manager can get its second the state of the world but the manager does not (Sec- tion 5.1) , then the manager’s expected utility is lower than if they have symmetric beliefs and the contractor’s If the contractor upon after signing an agreement if the contractor best utility If the manager also has private mation does not directly situations, better contract). If there are several agents tions, infor- then in most of the in which all types of managers do strictly than in the first best (Section 5.2), is risk neutral. information (Section 5.7)) then in most situa- (Section 5.5), but its private there exists a mechanism the contractor’s utilities, informed contractor in the environment (i.e., even better than the fully influence (2) (3) (4) When there is more the manager can design a second best contract. than one encounter between can reach either short term contracts or enforceable in the first case are similar the agen’ts are more complicated. to those of one encounter; however, then they (Section 6), the agents long term contracts. The contracts the strategies used by 342 S. Kraus/ArtQicinl Intelligence 83 (1996) 297-346 ( I ) If the agents agreed upon short term contracts, information and the number of encounters situations, they can reach first (2) is large enough, even in asymmetric best contracts. If the number of encounters more beneficial contract. is small, then enforceable to the manager. However, it is still difficult long term encounters are to design an efficient to a group. The the individual The last set of situations considered in this paper are of contracting that are used depend on the following factors: Whether type of contracts outcome of each contractor contractor possesses private manager may be quite complicated information. influences is observed by the manager, whether the effort level of one the other agents’ outcome, and whether each of the contractors In some of these situations and may require an efficient contract two rounds of message exchanges. for the We are now in the process of applying the techniques presented in this paper to the performance of robots in a simulated environment. References [ I I D. Abrereu, F? Dutta and L. Smith, The folk theorem for repeated games: a new condition, Econometrica 62 ( 1994) 939-948. 12 I K.J. Arrow, The economics of agency, in: J. Pratt and R. Zeckhauser, eds., Principals and Apxfs: The Strcrchrre of Business (Harvard Business School Press, Cambridge, MA, 1985) 37-5 1. 13 1 K.J. Arrow and EH. Hahn, General Competitive Analysis 141 K.J. Alrow, L. Hurwicz and H. Uzawa, Studies ( Holden-Day, San Franscisco. CA, 197 I ), (Stanford Programming in Linear and Non-Linear University Press, Stanford. CA, 1958). ( S 1 R.J. Aumann, Survey of Repeated Games. Essays (Wissenschaftsverlag, Honor of Oskar Mrqenstern in Game Theory and Mathematical Economics in Bibliognphisches Institut, Mannheim, I98 1) 16 1 S. Baiman and J. Demski, Economically optimal performance evaluation and control systems, J. Accowting Rex 18 ( 1980) 184-220. [ 7 ( A. Banerjee and A. Beggs, Efficiency in hierarchies: implementing the first-best solution by sequential actions, Rand .I. Econ. 20 ( 1989) 637-645. 18 1 A.H. Bond and L. Gasser, An analysis of problems and research in DAI, in: A.H. Bond and L. Gasser, eds., Readings in Dbtributed Artl$iciul Intelligence (Morgan Kaufmann, San Mateo, CA, 1988) 3-35. ( 9 1 B. Caillaud, R. Guesnerie, I? Rey and J. Tirole, Government intervention in production and incentives theory: a review of recent contributions, Rund J. Econ. 19 (1988) l-26. I IO) J. Christensen, Communication I I I ) C. Congdon, M. Huber, D. Kortenkamp, K. Konolige, K. Myres, A. Saffiotti and E. Ruspini, Carmel in agencies, Bell J. Econ. 12 ( 1981) 661-674. versus Flakey a comparison of two winners, AI Magazine 14 ( 1993) 49-57. ( I2 I S. Conry, D.J. Macintosh and R. Meyer, DARES: a Distributed Automated REasoning System, in: Pmceediqs AAAI-90, Boston, MA ( 1990) 78-85. [ I.1 1 D. Corkill, Hierarchical planning in a distributed environment, in: Proceedings IJCAI-79, Tokyo ( 1979) 168-175. 1 14 1 D. Demougin, A renegotiation-proof mechanism for a principle-agent model with moral hazard and adverse selection, Rand J. &on. 20 ( 1989) 256-267. I I5 ) J.S. Demski and D. Sappington, Optimal incentive contracts with multiple agents, J. Econ. Theory 33 (1984) 152-171. ( 16 [ E.J. Douglas, The simple analytics of the principal-agent incentive contract, J. Econ. Educ. 20 (1989) 39-51. 1 I7 I J. Doyle, Rationality 1 I8 I E.H. Durfee, Coordination 1988). and its role in reasoning, Comput. Intell. 8 ( 1992) 376-409. #Distributed Problem Solvers (Kluwer Academic Publishers, Boston. MA, S. Kraus/Arttj?cial Intelligence 83 (1996) 297-346 343 1 I9 I E.H. Durfee, What your computer really needs to know, you learned in kindergarten, in: Proceedings AAAI-92, San Jose, CA (1992) 858-864. ( 20 I E. Ephrab and J.S. Rosenschein, The Clarke tax as a consensus mechanism among automated agents, in: Proceedings AAAI-9I, Anaheim, CA (1991) 173-178. ( 2 I 1 E. Ephratl and J.S. Rosenschein, Planning to please: following another agent’s intended plan, J. Group Decision Negotiation 2 (1993) 219-235. / 22 1 R. Fletcher, Practical Methods of Optimization (Wiley, New York, 1987). I23 1 S. French, Decision Theory: An fntroduction to the Mathematics of Rationality (Ellis Horwood, Chichester, 1986). I24 1 D. Fudenberg and D. Levine, Limit games and limit equilibrium, J. Econ. Theory 38 ( 1986) 261-279. 125 I D. Fudenberg and E. Maskin, The folk theorem in repeated games with discounting or incomplete information, Econometrica 54 (1986) 533-554. 1261 D. Fudenberg and J. Tirole, Moral hazard and renegotiation in agency contracts, Econometrica 58 (1990) 1279-1319. I27 I D. Fudenberg, J. Tirole and P Milgrom, Short-term contracts and long-term relationships, J. Econ. Theory 51 (1990) l-3. ( 28 I L. Gasser, Social concepts of knowledge and action: DA1 foundations and open systems semantics, Arr$ Intell. 47 (1991) 107-138. I29 I L. Gasser, Social knowledge and social action, in: Proceedings IJCAI-93, Washington, DC ( 1993) 751-7.57. I 30 1 F. Gjesdal, Information and incentives: the agency information problem, Rev. Econ. Stud. 49 ( 1982) 373-390. 13 I I S. Grossman and 0. Hart, An analysis of the principal-agent problem, Econometrica 51 ( 1983) 7-45. ( 32 I B.J. Grosz and S. Kraus, Collaborative plans for complex activities, Artif: Intell. ( 1996); also: Tech. Report TR-20-95, Harvard University, Center for Research and Comuting Technology ( 1995). I 33 ( P. Haddaway and S. Hanks, Issues in decision-theoretic planning: symbolic goals and numeric utilities, in: Proceedings Workshop on Innovative Approaches to Planning, Scheduling and Control, San Diego, CA ( 1990) 48-58. 1341 M. Harris and A. Raviv, Some results on incentive contracts with applications to education and employlnent, health insurance, and law enforcement, Am. Econ. Rev. 68 (1978) 20-30. ( 35 I M. Harris and A. Raviv, Optimal incentive contracts with imperfect information, J. Econ. Theory 20 ( 1979) 23 l-259. I36 ] M. Hatris and R. Townsend, Resource allocation under asymmetric information, Econometrica 49 ( 198 I ) 33-64. (371 J. Harsanyi, Games with incomplete ( 1967/ 1968) 159-182,320-334,486-502. information played by bayesian players, Manage. Sci. 14 I38 ) O.D. Hart and J. Tirole, Contract renegotiation and Coasian dynamics, Rev. Econ. Stud. 55 ( 1988) 509-54-O. I39 I G. Heal, Planning without prices, Rev. Econ. Stud. 36 ( 1969) 346-362. I40 I J. Hirshleifer and J. Riley, The Analytics of Uncertainty and Information (Cambridge University Press, Cambridge, 1992). 14 I I Y. Ho, L. Servi and R. Suri, A class of center-free resource allocation algorithms, Large Scale Syst. 1 (1980, 51-62. ( 42 I B. Holmstrom, Moral hazard in teams, Bell J. &on. 13 ( 1982) 324-340. 143 I B. Holmstrom and P. Milgrom, Aggregation and linearity in the provision of intertemporal incentives, Economefrica 55 ( 1987) 303-328. (44 ( 1. Horswill, Polly: A vision-based artificial agent, in: Proceedings AAAI-93, Washington, DC ( 1993) 824-829. 145 I L. Hurwicz, The design mechanisms for resource allocation, An. Econ. Rev. 63 ( 1963) l-30. ( 46 I R. Keeney and H. Raiffa, Decisions with Multiple Objectives: Preferences and Value Tradeofls (Wiley, New York, 1976). I47 ( S. Kraus, Agents contracting tasks in non-collaborative environments, in: Proceedings AAAI-93, Washington, DC ( 1993) 243-248. 344 S. Kraus/Art@cial Intelligence 83 (1996) 297-346 I48 I S. Kraus and D. Lehmann, Designing and building a negotiating automated agent, Cornput. Intell. 11 (1995) 132-171. ( 49 I S. Kmus, M. Nirkhe and K.P Sycara, Reaching agreements through argumentation: a logical model, in: Proceedings DA193 ( 1993) 233-247; also presented in AAAI-93 workshop on AI theories of Groups and Organizations: Conceptual and Empirical Research. I SO ( S. Kraus and J. Wilkenfeld, The function of time in cooperative negotiations, in: Proceedings AAAI-9I, Anaheim, CA ( 199 1) I79- 184. [ 5 I I S. Kraus and J. Wilkenfeld, Negotiations over time in a multi agent environment: Preliminary report, in: Proceedings IJCAI-91. Sydney, Australia ( 199 1 ) 56-61. ] 52 ) S. Kraus, .I. Wilkenfeld and G. Zlotkin, Multiagent negotiation under time constraints, Arf@ Intell. 75 ( 1995) 297-34s. 1531 J. Kurose and R. Simha, A microeconomic approach to optimal resource allocation in distributed computer systems, IEEE Trans. Compuf. 38 (1089) 705-717. ( 54 1 J. Laffont, E. Maskin and J. Rochet, Optimal nonlinear pricing with two-dimensional characteristic, in: T. Groves, R. Radner and S. Reiter, eds., Iqformation, Incentives, and Economic Mechanisms (University of Minnesota Press, Minneapolis, MN. 1987) 256-266. ) 5S J J. Laffont and J. Tirole, Adverse selection and renegotiation in procurement, Rev. Econ. Stud. 57 ( 1990) 597-625. ( 56 I J. Laffont and J. Tirole, A Theory oj’fnceniives in Procurement and Regulatian (MIT Press, Cambridge, MA, 1993). IS7 ( R.A. Lambert, Long I S8 1 M. Landsberger and I. Meilijson, Monopoly term contracts and moral hazard, Bell J. Econ. 14 ( 1983) 441-452. insurance under adverse selection when agents differ in risk aversion, J. Econ. Theory 63 ( 1994) 392-407. ) 59 1 V. Lesser, A retrospective view of FAIC distributed problem solving, IEEE Trans. Syst. Man Cybern. 21 (1991) 1347-1362. I60 I V.R. Lesser, Distributed problem solving, (Wiley, New York, 1990) 245-251. I61 I V.R. Lesser and L.D. Erman, Distributed 1144-l 163. 29 (1980) in: S.C. Shapiro, ed., Encyclopedia of Arr$cial Intelligence interpretation: a model and experiment, IEEE Trans. Cbrpuf. [ 62 I C. Ma, Unique implementation of incentive contracts with many agents, Rev. Ecnn. Stud. 51 ( 1984) 555-572. I 63 1 C. Ma, J. Moore and S. Turnbull, Stopping agents from “cheating”, 164 ) I. Macho-Stadler 165 I J. Malcomson and F. Spinnewyn, The multiperiod principal-agent J. Econ. Theory 46 ( 1988) 355-372. problem, Rev. Econ. kud. 55 ( 1988) and J. Perez-Castrillo. Moral hazard and cooueration, Econ. L&f. 35 ( 1991) 17-20. 39 I-408. 166 I E. Malinvaud, Decentralized procedures for planning, in: Activity Analysis in the Theory of Growth ctrrd PIwming (St Martin’s Press, New York, 1967) 170-208. I67 I T.W. Malone, R.E. Fikes, K.R. Grant and M.T. Howard, Enterprise: a marketlike task schedule for distributed computing environments, in: B.A. Huberman, ed., The Ecology of Computation (Notth- Holland, Amsterdam, 1988) 177-205. 168 I E. Maskin and J. Tirole, The principal-agent 58 ( 1990) 379-409. 169 I E. Maskin and J. Tirole, The principal-agent values, Economefricu Ewwnefrica 60 ( 1992) I-42. relationship with an informed principal: the case of private relationship with an informed principal II: common values, I70 I S. Matthews, Selling to risk averse buyers with unobservable tastes, J. Econ. Theory 30 ( 1983) 370- 400. 171 I R. McAfee and J. McMillan, Multidimensional incentive compatibility and mechanism design, J. Econ. Theory 46 ( 1988) 335-354. 172 I R.F? McAfee and J. McMillan, Bidding for contracts: a principal-agent analysis, Rand J. Econ. 17 (1986) 326-338. I73 I R.P. McAfee and J. McMillan, Competition 174 I N.D. Melumad and S. Reichelstein, Value of communication for agency contracts, Rand J. Econ. 18 ( 1987) 296-307. in agencies, J. Econ. Theory 47 ( 1989) 334-368. 17s I D. Mookherjee, Optimal incentive schemes with many agents, Rev. Econ. S&d. 51 ( 1984) 433-446. S. Kraus/Artijicial Intelligence 83 (1996) 297-346 345 I76 I R. Myerson, Optimal coordination mechanisms in generalized principal-agent problem, J. Math. Econ. 10 (1982) 67-81. I77 I R. Myerson, Mechanism design by an informed principal, Econometrica 51 (1983) I78 I 9. Nale’buff and J. Stiglitz, I79 I 9. N&buff and markets, Am. Econ. Rev. 73 ( 1983) 278-283. and towards a general competition, incentives: and J. Stiglitz, Prizes and theorv of compensation Information, 1767-1798. (1993) 51-62. robots from the 1993 robot competition, AI Magazine competition, Bell J. Ec&. 14 (1983) 21-43. I. Nourbakhsh, S. Morse, C. Becker, M. Balabanovic, E. Gat, R. Simmons, S. Goodridge, H. Potlapalli, D. Hinkle, K. Jung and D.V. Vactor, The winning 14 (4) F. Page, The existence of optima1 contracts 157-167. H. Pattison, D. Corkill and V. Lesser, Instantiating Huhns, Matheo, CA, 1987) 59-96. R. Pfaffenberger University Press, Ames, IA, 1976). R.H. Porter, Optimal cartel trigger price strategies, J. Econ. Theory 29 (1983) 313-338. and D. Walker, Mathematical Programmingfor Economics and Business (IOWA State in the principal-agent model, J. Math. Econ. 16 ( 1987) ed., Distributed Artificial fntelligence descriptions (Pitman/Morgan Kaufman Publishers, London/San of organizational structures, in: M.N. I 80 181 I82 I83 I 84 I85 I R. Radner, Monitoring 49 (1981) 1127-1148. cooperative agreements in a repeated principal-agent relationship, Econometrica I86 I R. Raclner, Repeated principal agents games with discounting, Econometrica 53 ( 1985) 1173-I 198. I87 I E. Rasmusen, Moral hazard I88 I E. Rasmusen, Games and lnfirmation (Basil Blackwell, Cambridge, MA, 1989). [ 89 I W. Rogerson, The first-order approach teams, Rand J. Econ. 18 (1987) 324-340. problems, Econometrica 53 (1985) 1357- to the principle-agent in risk-averse 1367. 1901 J.S. Rosenschein, IIS-119. (1982) Synchronization of multi-agent plans, in: Proceedings AAAI-82, Pittsburgh, PA I 9 I I S. Ross, The economic [ 92 I S. Ross, Equilibrium theory of agency: and agency-inadmissible the principal’s problem, Am. Econ. Rev. 63 ( 1973) 134- 139. agents and in the public agency problem, Am. Econ. Rev. 69 ( 1979) 308-312. ( 93 I A. Rubinstein and M. Yaari, Repeated insurance contracts and moral hazard, J. Econ. Theory 30 (1983) 74-95. I94 I T. Sandholm, An implementation of the contract net protocol based on marginal cost calculations, in: Proceedings AAAI-93, Washington, DC (1993) 256-262. 195 I D. Sappington, Limited liability contracts between principle and agent, L Econ. Theory 29 (1983) l-21. I96 I D. Sappington, Incentive contracting with asymmetric and imperfect precontnctual knowledge. J. Econ. Theory 34 ( 1984) 52-70. I97 I R. Selten, Re-examination of the perfectness concept for equilibrium points in extensive games, Int. J. Gtrnw Theory 4 ( 1975) 25-55. I98 I S. Shavell, Risk sharing and incentives 55-7’3. in the principal and agent relationship, Bell J. Econ. 10 ( 1979) I99 I R. Smith, The contract net protocol: high-level communication and control in a distributed problem solver, fEEE Trans. Comput. 29 ( 1980) 1104-I 113. I IO0 I R. Smith and R. Davis, Framework for cooperation in distributed problem solvers, IEEE Trans. Syst. Mwz Cybern. 29( 12) (1981) 61-70. ( IO1 I R. Smith and R. Davis, Negotiation as a metaphor for distributed problem solving, Arti$ Intell. 20 Insurance, information (1983) 63-109. M. Spence and R. Zeckhauser, 380- 39 I, W. Spivey and R. Thrall, Linear Optimization (Holt, Rinehart and Winston, New York, 1970). K. Sycara, Resolving adversarial Ph.D. Thesis. School of Information GA (1987). K.P. Sycara, Persuasive argumentation in negotiation, Theory Deck. 28 ( 1990) 203-242. to integrating case-based and analytic methods, Institute of Technology, Atlanta, and individual action, Am. Econ. Rev. 61 ( 197 I ) and Computer Science, Georgia conflicts: an approach I102 [ 103 ( 104 ( 105 346 S. Kraus/Artt$%d Intelligence 83 (1996) 297-346 I 106 1 C. Waldspurger, T. Hogg, A. Huberman, .I. Kephrat and W. Stometta, Spawn: a distributed computational economy, EEE Trans. Sofhv. Eng. 18 (1992) 103-I 17. 1 IO7 ( M.P. Wellman, A general-equilibrium approach to distributed transportation planning, in: Proceedings AAAI-92, San Jose, CA (1992) 282-289. J. Doyle, Modular 1 108 1 M.P. Wellman and utility representation for decision-theoretic planning, in: Proceedings AI Planning Systems ( 1992) 236-242. 1 109 ) E. Werner, Toward a theory of communication in: Proceedings Second Conference on Theoretical Aspects of Reasoning about Knowledge, Pacific Grove, CA ( 1988) 129-143. for multiagent planning, and cooperation 1 I IO 1 G. Zlotkin and J.S. Rosenschein, Incomplete and deception in multi-agent negotiation, in: Proceedings 1 I I I ) G. Zlotkin IJCAI-91, and J.S. Rosenschein, A domain Sydney, Australia LICAI-93, Chambery (1993) 416-422. information ( I99 I ) 225-23 I. theory for task oriented negotiation, in: Proceedings 