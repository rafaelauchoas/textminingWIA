Artificial Intelligence 173 (2009) 104–144Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintCoherence graphsEnrique Miranda a,∗, Marco Zaffalon ba Rey Juan Carlos University, Department of Statistics and Operations Research, C-Tulipán, s/n, 28933 Móstoles, Spainb IDSIA, Galleria 2, CH-6928 Manno (Lugano), Switzerlanda r t i c l ei n f oa b s t r a c tArticle history:Received 23 April 2008Received in revised form 14 August 2008Accepted 4 September 2008Available online 11 September 2008Keywords:Walley’s strong and weak coherenceCoherent lower previsionsGraphical modelsProbabilistic logicSatisfiabilityWe study the consistency of a number of probability distributions, which are allowed to beimprecise. To make the treatment as general as possible, we represent those probabilisticassessments as a collection of conditional lower previsions. The problem then becomesproving Walley’s (strong) coherence of the assessments. In order to maintain generalityin the analysis, we assume to be given nearly no information about the numbers thatmake up the lower previsions in the collection. Under this condition, we investigatethe extent to which the above global task can be decomposed into simpler and morelocal ones. This is done by introducing a graphical representation of the conditionallower previsions that we call the coherence graph: we show that the coherence graphallows one to isolate some subsets of the collection whose coherence is sufficient for thecoherence of all the assessments; and we provide a polynomial-time algorithm that findsthe subsets efficiently. We show some of the implications of our results by focusing onthree models and problems: Bayesian and credal networks, of which we prove coherence;for which we provide an optimal graphical decomposition;the compatibility problem,probabilistic satisfiability, of which we show that some intractable instances can insteadbe solved efficiently by exploiting coherence graphs.© 2008 Elsevier B.V. All rights reserved.1. IntroductionWe focus on studying the consistency of a number of conditional and unconditional distributions of some variables.In order to make our treatment as general as possible, we are going to represent these probabilistic assessments usingthe theory of coherent lower previsions developed by Walley in [33], which is based on de Finetti’s work about subjectiveprobability [10,11]. This allows us to study the case where the above distributions are imprecise, i.e., where each of themis actually a closed convex set of precise distributions, which includes as a particular case that where our assessments areprecise probabilities. It also allows us to work with any type of variable, without placing restrictions on the admissiblepossibility spaces (finite, countable, continuous). The approach by Walley includes also as particular cases most of the otherimprecise probability models appearing in the literature.Studying the consistency problem is important both for theoretical and applied reasons. On the theoretical side, it hasbeen shown by de Finetti that a subjective theory of precise probability (such as the Bayesian theory) can be founded ona single axiom of consistency. Williams [35] and later Walley have shown that this continues to hold when such a theoryis generalised to handle imprecision in probability. In these theories proving consistency is therefore a necessary step toexploit all the tools they provide us with, such as Bayes’ rule and its generalisations. The application of these tools alone,* Corresponding author.E-mail addresses: enrique.miranda@urjc.es (E. Miranda), zaffalon@idsia.ch (M. Zaffalon).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.09.001E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144105on the other hand, does not necessarily lead to self-consistent inference, even in the case of precise probabilities, as shownby Walley.On the applied side, it is a very common requirement that an inference method should not give rise to inconsistencies.This requirement is present, for example, in probabilistic logic [26], where one has to check first of all that the availableassessments are self-consistent. It is also present in the many other models and methods designed so as to give rise toa joint distribution, which is often regarded as a feature that ensures global consistency. Exactly this argument was used,for example, to support Bayesian networks versus rule-based systems already at the time of Pearl’s seminal work [27]. Butconsistency is quite a subtle concept to deal with. The following striking example adapted from [33, Section 7.3.5] showsthat the existence of a compatible joint is not always a good way to get rid of inconsistencies.Example 1. Let X1, X2 be two variables taking values in {1, 2, 3}, and assume that X1 = 3 if and only if X2 = 3, and for theother cases we have the contradictory information X1 = X2 and X1 (cid:3)= X2. We can model this by the conditional probabilitiesP ( X1 = 3| X2 = 3) = 1 = P ( X2 = 3| X1 = 3), P ( X1 = 1| X2 = 1) = 1 = P ( X1 = 2| X2 = 2) and P ( X2 = 1| X1 = 2) = 1 = P ( X2 =2| X1 = 1). Despite the contradiction, it can be checked that the assessments are compatible with the joint mass functiondetermined by P ( X1 = 3, X2 = 3) = 1, in the sense that this joint induces the above conditionals by Bayes’ rule when theconditioning events have positive probability.1The key here is that Bayes’ rule cannot be always applied because of the presence of events with zero probability; thistechnical issue prevents the contradiction from being identified. It follows that in order to check consistency we generallyneed stronger tools than those based on the existence of a compatible joint distribution. Walley’s notion of coherenceappears to be one such tool.In fact, Walley considers two different consistency concepts for conditional lower previsions, called weak coherence and(strong) coherence (these will be introduced in Section 2, along with other material about Walley’s theory). What we showin Section 3 is that a number of conditional lower previsions are weakly coherent when they can all be induced by the samejoint via Bayes’ rule (or its generalisation for the imprecise case) and marginalisations. In other words, we show that weakcoherence is the generalisation to imprecise probability of the consistency criterion based on the existence of a compatiblejoint. Coherence, on the other hand, strengthens weak coherence and it can be shown that the difference between weakand strong coherence is indeed related to conditioning on sets of probability zero (see [21]).Our goal in this paper is to simplify the verification of the weak or the strong coherence of a number of assessments. Toachieve this, we introduce in Section 5 a new graphical representation called coherence graphs. We prove in Section 6 thatcoherence graphs allow us to decompose the task of verifying weak and strong coherence in a number of simpler tasks.Specifically, they help us determine a partition of the set of assessments with the property that coherence (resp., weakcoherence) within each of the elements of the partition implies coherence (resp., weak coherence) of all the assessments.We prove moreover that this is the finest partition with this property in the case of weak coherence. Besides, this partitionof our set of assessments can be determined with a polynomial-time algorithm, which we present in Section 7.Then we move to show some of the implications of our results for artificial intelligence by considering three well-knownrelated research fields. In Section 8.1, we consider Bayesian networks [27] and their extension to imprecise probability calledcredal networks [8]. By joining coherence graphs with a notion of probabilistic independence, we show for the first timeand to a very large extent that Bayesian and credal networks are coherent models. In Section 8.2 we focus on the so-called compatibility problem (see [9] and the references therein for a recent overview), i.e., the problem of deciding whethera number of distributions has a compatible joint. In this case we exploit our results about weak coherence to delivernew graphical criteria that enable one to optimally decompose such a problem, under a very general formulation. Finally,in Section 8.3 we relate our results to a powerful form of probabilistic satisfiability based on consistency that has beenrecently proposed in [34]. In particular, we discuss how probabilistic satisfiability can be used to check the consistency ofa number of (possibly imprecise) conditional and unconditional mass functions, and we outline that this task can easilybecome intractable as a consequence of the NP-hardness of the problem [5]. Moreover, we show that coherence graphs candecompose such a task in a way that makes it possible to solve some instances of the problem that would otherwise beintractable.As we said, our results are very general, in the sense that they are applicable for variables taking values on finite orinfinite spaces, and that we can also consider precise or imprecise representations. We make nevertheless some assumptions,like the logical independence of the variables studied or the representation of our assessments through a functional definedon a sufficiently large domain. In Section 9, we comment on the extent to which these assumptions can be relaxed. Thisis an important problem in order to relate our work more tightly with other areas of research. Finally, we conclude thepaper in Section 10 with some additional discussion. To make the paper easier to read, we have relegated all the proofs toAppendix A.1 This consistency notion is what we shall call later in this paper weak coherence. Note that the contradictory assessments X1 = X2 and X1 (cid:3)= X2 can alsobe modelled by other conditional probabilities that do not even satisfy this consistency notion, for instance P ( X1 = 1| X2 = 1) = 1 = P ( X1 = 2| X2 = 1).106E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–1442. Coherent lower previsionsLet us give a short introduction to the concepts and results from the behavioural theory of imprecise probabilities thatwe shall use in the rest of the paper. We refer to [33] for an in-depth study of these and other properties, and for aninterpretation of the notions we shall introduce below.Given a possibility space Ω , a gamble is a bounded real-valued function on Ω .2 This function represents a random rewardf (ω), which depends on the a priori unknown value ω of Ω . We shall denote by L(Ω) the set of all gambles on Ω . A lowerprevision P is a real functional defined on some set of gambles K ⊆ L(Ω). It is used to represent a subject’s supremumacceptable buying prices for these gambles.We can also consider the supremum acceptable buying price for a gamble conditional on a subset of Ω . Given such aset B and a gamble f on Ω , the lower prevision P ( f |B) represents the subject’s supremum acceptable buying price forthe gamble f , updated after coming to know that the unknown value ω belongs to B, and nothing else. If we consider apartition B of Ω (for instance a set of categories), then we shall represent by P ( f |B) the gamble on Ω that takes the valueP ( f |B) if and only if ω ∈ B. The functional P (·|B) that maps any gamble f on its domain into the gamble P ( f |B) is calleda conditional lower prevision.Let us now re-formulate the above concepts in terms of variables, which are the focus of our attention in this paper.Consider variables X1, . . . , Xn, taking values in respective sets X1, . . . , Xn. For any subset J ⊆ {1, . . . , n} we shall denote byX J the (new) variableX J := ( X j) j∈ J ,which takes values in the product spaceX J := × j∈ J X j.We shall also use the notation X n for X{1,...,n}. This will be our possibility space in the rest of the paper.Definition 1. Let J be a subset of {1, . . . , n}, and let π J : X n → X J be the so-called projection operator, i.e., the operator thatdrops the elements of a vector in X n that do not correspond to indexes in J . A gamble f on X n is called X J -measurablewhen for any x, y ∈ X n, π J (x) = π J ( y) implies that f (x) = f ( y).There exists a one-to-one correspondence between the gambles on X n that are X J -measurable and the gambles on X J .We shall denote by K J the set of X J -measurable gambles.Consider two disjoint subsets O , I of {1, . . . , n}. Then P ( X O | X I ) represents a subject’s behavioural dispositions aboutthe gambles that depend on the outcome of the variables { Xk, k ∈ O }, after coming to know the outcome of the variables{ Xk, k ∈ I}. As such, it is defined on the set of gambles that depend on the values of the variables in O ∪ I only, i.e., on theset KO ∪I of the XO ∪I -measurable gambles on X n. When there is no possible confusion about the variables involved in thelower prevision, we shall use the notation P ( f |x) for P ( f | X I = x). The sets {π −1(x) : x ∈ XI } form a partition of X n. Hence,we can define the gamble P ( f | X I ), which takes the value P ( f |x) on x ∈ XI . This is a conditional lower prevision.IThis type of conditional previsions is what we are going to consider throughout the paper. We refer to [22,33] for moregeneral definitions of the following notions in this section in terms of partitions, and for domains that are not necessarily(these) linear sets of gambles. A definition of conditional previsions where we do not necessarily condition on a partitioncan be found in [35].The XI -support S( f ) of a gamble f(cid:2)S( f ) :=π −1I(x): x ∈ XI , f Iπ −1I(x)in KO ∪I is given by(cid:3)(cid:3)= 0,(1)i.e., it is the set of conditioning events for which the restriction of fis not identically zero. Here, and in the rest of thepaper, I A will be used to denote the indicator function of the set A, i.e., the function whose value is 1 in the elements of Ain the domain KO ∪I of the conditional lower prevision P ( X O | X I ), and any x ∈ XI ,and 0 elsewhere. Also, for any gamble fwe shall denote by G( f |x) the gamble I(x)( f − P ( f |x)), and by G( f | X I ) the gamble that takes the value G( f |πI ( y)) forπ −1any y ∈ X n.These assessments can be made for any disjoint subsets O , I of {1, . . . , n}, and therefore it is not uncommon to modela subject’s beliefs using a finite number of different conditional lower previsions. Then we should verify that all the as-sessments modelled by these conditional lower previsions are consistent with one another. The first requirement we makeis that for any disjoint O , I ⊆ {1, . . . , n}, the conditional lower prevision P ( X O | X I ) defined on KO ∪I should be separatelycoherent. In this case, where the domain is a linear set of gambles, separate coherence holds if and only if the followingconditions are satisfied for any x ∈ XI , f , g ∈ KO ∪I , and λ > 0:I(SC1) P ( f |x) (cid:2) inf y∈π −1I(x) f ( y).2 Although we only deal in this paper with bounded gambles, an extension of the theory to unbounded gambles can be found in [30].E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144107(SC2) P (λ f |x) = λP ( f |x).(SC3) P ( f + g|x) (cid:2) P ( f |x) + P (g|x).It shall also be interesting for this paper to consider the particular case where I = ∅, that is, when we have (uncon-ditional) information about the variables X O . We have then an (unconditional) lower prevision P ( X O ) on the set KO ofXO -measurable gambles. Then separate coherence is simply called coherence, and it holds if and only if the following threeconditions hold for any f , g ∈ KO , and λ > 0:(C1) P ( f ) (cid:2) inf f .(C2) P (λ f ) = λP ( f ).(C3) P ( f + g) (cid:2) P ( f ) + P (g).In general, separate coherence is not enough to guarantee the consistency of the lower previsions: conditional lowerprevisions can be conditional on the values of many different variables, and still we should verify that the assessments theyprovide are consistent not only separately, but also with one another. Formally, we are going to consider what we shall callcollections of conditional lower previsions.| X Im )} be conditional lower previsions with respective domains K1, . . . , Km ⊆Definition 2. Let {P 1( X O 1L(X n), where K j is the set of XO j∪I j -measurable gambles,3 for j = 1, . . . , m. Then this is called a collection on X n whenfor each j1 (cid:3)= j2 in {1, . . . , m}, either O j1| X I1 ), . . . , P m( X O m(cid:3)= O j2 or I j1(cid:3)= I j2 .This means that we do not have two different conditional lower previsions giving information about the same set of| X Im )} of conditionalvariables X O , conditional on the same set of variables X I . Given a collection {P 1( X O 1lower previsions, there are different ways in which we can guarantee their consistency.| X I1 ), . . . , P m( X O m| X Im ) be separately coherent conditional lower previsions. We say that they avoidDefinition 3. Let P 1( X O 1uniform sure loss if and only if| X I1 ), . . . , P m( X O m(cid:4)m(cid:5)j=1supx∈X n(cid:6)G j( f j| X I j )(x) (cid:2) 0,for every f j ∈ K j , j = 1, . . . , m.| X I1 ), . . . , P m( X O mDefinition 4. Let P 1( X O 1partial loss when for every f j ∈ K j , j = 1, . . . , m there is some B ∈| X Im ) be separately coherent conditional lower previsions. We say that they avoid(cid:7)mj=1 S j( f j) such that(cid:4)m(cid:5)supx∈Bj=1(cid:6)G j( f j| X I j )(x) (cid:2) 0,where S j( f j) is the XI j -support of f j given by Eq. (1).(2)The notions of avoiding partial or uniform sure loss are minimal consistency requirements that we shall use in Section 8to connect our work with probabilistic logic; nevertheless, the main focus in this paper will be made on some strongernotions, that we shall call weak and strong coherence:Definition 5. Let P 1( X O 1weakly coherent when for any f j ∈ K j , j = 1, . . . , m, j0 ∈ {1, . . . , m}, f 0 ∈ K j0 , z0 ∈ XI j0| X I1 ), . . . , P m( X O m| X Im ) be separately coherent conditional lower previsions. We say that they are,(cid:4)m(cid:5)j=1supx∈X n(cid:6)G j( f j| X I j ) − G j0 ( f 0|z0)(x) (cid:2) 0.(3)Under the behavioural interpretation, a number of weakly coherent conditional lower previsions can still present someforms of inconsistency with one another; see [33, Example 7.3.5] for an example and [33, Chapter 7] and [34] for somediscussion. Because of this, we consider yet a stronger notion, called (joint or strong) coherence:43 We use K j instead of KO j ∪I j in order to alleviate the notation when no confusion is possible about the variables involved.4 The distinction between this and the unconditional notion of coherence mentioned above will always be clear from the context.108E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Definition 6. Let P 1( X O 1are coherent when for every f j ∈ K j ,(cid:7)| X I1 ), . . . , P m( X O m| X Im ) be separately coherent conditional lower previsions. We say that they(z0)} ∪, there exists some B ∈ {π −1I j0f 0 ∈ K j0 , z0 ∈ XI j0j0 ∈ {1, . . . , m},j = 1, . . . , m,mj=1 S j( f j) such that(cid:4)m(cid:5)j=1supx∈B(cid:6)G j( f j| X I j ) − G j0 ( f 0|z0)(x) (cid:2) 0,(4)where, again, S j( f j) is the XI j -support of f j given by Eq. (1).The coherence of a collection of conditional lower previsions implies their weak coherence; although the conversedoes not hold in general, it does in the particular case when we only have a conditional and an unconditional lowerprevision. Note for instance that the conditional previsions in Example 1 in the Introduction are not coherent: if we con-sider f 1 := I{(1,1),(2,2)} and f 2 := I{(2,1),(1,2)}, then G( f 1| X2) + G( f 2| X1) < 0 in the union of the supports, which is the set{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2)}.In the next section, we prove a number of results that will help to better understand the differences between weakand strong coherence. But before we do that, we introduce a special case that will be of interest for us: that of conditionallinear previsions. We say that a conditional lower prevision P ( X O | X I ) on the set KO ∪Iis linear if and only if it is sepa-rately coherent and moreover P ( f + g|x) = P ( f |x) + P (g|x) for any x ∈ XI and f , g ∈ KO ∪I . When a separately coherentconditional lower prevision P ( X O | X I ) is linear we shall denote it by P ( X O | X I ); in the unconditional case, we shall use thenotation P ( X O ). A separately coherent unconditional linear prevision corresponds to the expectation operator (the Dunfordintegral [4]) with respect to a finitely additive probability.If we consider conditional linear previsions P 1( X O 1| X Im ) with domains K1, . . . , Km, then they arecoherent if and only if they avoid partial loss, and they are weakly coherent if and only if they avoid uniform sure loss [33,Section 7.1, page 347].| X I1 ), . . . , Pm( X O mA conditional lower prevision P ( X O | X I ) is separately coherent if and only if it is the lower envelope of a closed (inthe weak* topology) convex set of dominating conditional linear previsions, where P ( X O | X I ) is said to dominate P ( X O | X I )when for every XO ∪I -measurable gamble f , P ( f |x) (cid:2) P ( f |x) for every x ∈ XI (this is a consequence of [33, Sections 6.7.2and 6.7.4]). We shall denote this set of dominating conditional linear previsions by M(P ( X O | X I )).Finally, one interesting particular case is that where we are given only an unconditional lower prevision P on L(X n)and a conditional lower prevision P ( X O | X I ) on KO ∪I . Then weak and strong coherence are equivalent, and they both holdif and only if, for any XO ∪I -measurable f and any x ∈ XI ,(JC1) P (G( f | X I )) (cid:2) 0(JC2) P (G( f |x)) = 0.If both P and P ( X O | X I ) are linear previsions, they are coherent if and only if for any XO ∪I -measurable fP ( f ) = P (P ( f | X I )).it holds thatBefore concluding this section, it is important to remark that if a lower prevision P is coherent with a conditional lowerprevision P (·|B), then P must satisfy the property of conglomerability, which is discussed in some detail in [33, Section 6.8].This property is one of the points of disagreement between Walley’s and de Finetti’s [13] approach to conditional previsions.3. Weak and strong coherenceThe following theorem gives a new characterisation of the weak coherence of the conditionallower previsionsP 1( X O 1| X I1 ), . . . , P m( X O m| X Im ).Theorem 1. P 1( X O 1such that, for any j = 1, . . . , m,| X I1 ), . . . , P m( X O m(cid:8)P (G j( f | X I j )) (cid:2) 0 for any f in K jP (G j( f |x)) = 0for any f in K j, x in XI j .| X Im ) are weakly coherent if and only if there is some coherent lower prevision P on L(X n)In particular, conditional linear previsions P 1( X O 1prevision P on X n such that P ( f ) = P (P j( f | X I j )) for any f in K j , j = 1, . . . , m.| X I1 ), . . . , Pm( X O m| X Im ) are weakly coherent if and only if there exists a coherentRemark 1. When all the conditional previsions are linear and moreover all the spaces X1, . . . , Xn are finite, we deduce fromj = 1, . . . , m, is equivalent to the existenceTheorem 1 that the weak coherence of the conditional previsions P j( X O jof a linear prevision (a finitely additive probability) on X n inducing the conditional previsions by means of Bayes’ rule.This is not enough, however, for the conditional previsions to be coherent, because of how these conditions deal withthe problem of conditioning on sets of probability zero. For instance, the conditional previsions in Example 1 are weakly| X I j ),E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144109coherent because they are compatible with the linear prevision determined by the assessment P ( X1 = 3, X2 = 3) = 1. Weshall come back to this in Section 8.2.From this theorem, we can easily deduce the following two results that relate (weak or strong) coherence to the existenceof an unconditional lower prevision that is (weakly or strongly) coherent with the collection.| X I1 ), . . . , P m( X O mProposition 1. Let P 1( X O 1coherent unconditional lower prevision P on L(X n) such that P , P 1( X O 1| X Im ) be conditional lower previsions. They are coherent if and only if there is some| X I1 ), . . . , P m( X O m| X Im ) are coherent.Corollary 1. The conditional lower previsions P 1( X O 1ent lower prevision P on L(X n) such that P , P 1( X O 1| X I1 ), . . . , P m( X O m| X I1 ), . . . , P m( X O m| X Im ) are weakly coherent if and only if there is some coher-| X Im ) are weakly coherent.These two results allow us to understand a bit better the conceptual difference between weak coherence and (strong)coherence: from Corollary 1 and Theorem 1, weak coherence amounts to the existence of a joint that is pairwise coherentwith each of the conditional lower previsions; from Proposition 1, coherence means that there is a joint that is coherentwith all the conditional lower previsions, taken together.This difference is perhaps easier to grasp in the particular case where we deal with finite spaces and with linear con-ditional previsions. In that case, weak coherence is equivalent to the existence of a linear prevision (the expectation withrespect to a finitely additive probability) that induces each of the conditional previsions by means of Bayes’ rule. But thisdoes not guarantee that the conditional previsions are coherent, because the joint mentioned above might not be coherentwith all of them considered as a whole. That is, the conditional previsions may provoke some behavioural inconsistencieswhen taken together even if they can all be induced from the same joint. This is due to the fact that when this joint giveszero probability to a set B, then any conditional prevision P (·|B) is coherent with the joint.4. Collection templatesWith this paper we should like to deliver tools to prove coherence that are sufficiently general to be applied in mostsituations. To do this, we have to assume very little about the conditional lower previsions that are the subject of our study.In particular, we are not going to assume anything about the numbers that make up the lower previsions themselves, otherthan they produce separately coherent assessments. We do require separate coherence as it is really a minimal requirementof self-consistency for a conditional lower prevision.Abstracting away from the numbers implies that for each lower prevision we only know, apart from its being separatelycoherent, what are the variables on both sides of the conditioning bar. This can be regarded as the ‘form’ of a conditionallower prevision, or its template, as we call it in the following.) be two lower previsions on X n. We say that they have the same template( X O j1Definition 7. Let P j1= I j2 . The class of all the lower previsions on X n with the same template is just called a lower prevision= O j2 and I j1if O j1template on X n (of the generic lower previsions in the class). We denote a lower prevision template in the same way as wedenote a lower prevision (the distinction should be clear from the context): i.e., by P j( X O j) and P j2| X I j ).( X O j2| X I j1| X I j2We can identify a template with a disjoint pair of indices. A collection template is then determined by a finite numberof such pairs. This is formalised in the following definition.Definition 8. Two collections of lower previsions on X n have the same template if they contain the same number m of lowerj in {1, . . . , m} the twoprevisions, and if it is possible to order the elements in each collection in such a way that for allrespective jth lower previsions have the same template. The class of all the collections on X n with the same templateis just called a collection template on X n (of the generic collection in the class). We denote a collection template in thesame way as we denote a collection of lower previsions (again, the distinction should be clear from the context): i.e., by{P 1( X O 1| X I1 ), . . . , P m( X O m| X Im )}.An equivalent way to look at a collection template is as a collection of lower prevision templates. For this reason, weshall sometimes refer to the lower prevision templates in a collection template.The definitions just introduced allow us to state the task of this paper more precisely: i.e., to characterise what we knowabout the coherence of a collection of (separately coherent) lower previsions once we know its template alone. To thisextent, it is useful to introduce a graphical characterisation of a collection template. This is done in the next section.5. Coherence graphsIn this section, we introduce a graphical representation of collection templates based on directed graphs. We start byrecalling some terminology from graph theory.110E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144A directed graph is a structure made up of a set of nodes and a set of directed arcs between nodes. Two nodes connectedby an arc are also called its endpoints. A sequence of at least two nodes for which each pair of adjacent nodes is an arcin the graph, is called directed path between the first and the last node in the sequence (also called origin and destination,respectively). When the origin and destination coincide, we say that the path is a directed cycle, or just a cycle, for short. Ifa cycle does not contain any proper cycle, then it is said to be elementary. Note that a path uniquely identifies a sequenceof arcs; for this reason, by an abuse of terminology, we shall sometimes refer to the arcs of a path.It is useful to introduce also the notion of strong component of a directed graph. This is a maximal strongly connectedsubgraph, where a strongly connected graph is one for which there is a path for each ordered pair of nodes. A strongcomponent is said to be trivial if it is made by a single node.The predecessors of a node are all the nodes that have a directed path towards the given node. The predecessors forwhich there is a directed path made up of a single arc, are called parents. The indegree of a node is the number of itsparents. A node with indegree equal to zero is called a root. Similarly, the successors of a node are all the nodes that canbe reached from the given node following directed paths. The successors for which there is a directed path made up of asingle arc, are called children. The outdegree of a node is the number of its children. A node with outdegree equal to zero iscalled a leaf.The union of the set of parents and children of a node is called the set of its neighbours. The union of two graphs is agraph created by taking the union of their nodes and their arcs, respectively.Now we are ready to define the most important graphical notion used in this paper.Definition 9. Consider two finite sets Z = { X1, . . . , Xn} and D = {D1, . . . , Dm} of so-called actual and dummy nodes, respec-tively. Call N := Z ∪ D the set of nodes, and a given A ⊆ N × N the set of arcs. The triple (cid:9)Z, D, A(cid:10) is called a coherencegraph on Z if the following properties hold:(CG1) Z is non-empty.(CG2) All neighbours of dummy nodes are actual nodes, and vice versa.(CG3) The set of the parents and that of the children of any dummy node have an empty intersection.(CG4) Dummy nodes are not leaves.(CG5) Different dummy nodes do not have both the same parents and the same children.Fig. 1 displays the coherence graph of the assessmentsP 1( X1),P 9( X10| X13),P 2( X4| X1),P 3( X6| X5),P 4( X7| X2),P 5( X7| X3),P 10( X11| X7),P 11( X12| X8),P 12( X13| X14),P 6( X8| X3),P 13( X14| X6, X10),P 7( X8| X4),P 8( X9| X1, X5),P 14( X15, X16| X9, X12, X13).Here the actual nodes are X1, . . . , X16. Note that to make graphs easier to see, we represent dummy nodes in a simplifiedway: we do not show their labels and rather represent each of them simply as a black solid circle (this does not create aproblem since by CG5 each dummy node is univocally identified by its neighbours); moreover, when a dummy node hasexactly one parent and one child, we do not represent the arrow entering the dummy node (this is not going to causeambiguity either).Next, we show that there is a one-to-one relationship between coherence graphs on Z = { X1, . . . , Xn} and collectiontemplates on X n. To this extent, it is useful to isolate the notion of a D-structure in a coherence graph.Definition 10. Given a dummy node D of a coherence graph, we call D-structure the subgraph whose nodes are D and itsneighbours, and whose arcs are those connecting D to its neighbours.At this point we consider two functions: the first one, which we shall denote by Γ , maps a collection template| X Im )}, related to the variables { X1, . . . , Xn} =: Z , into a coherence graph on Z , with dummy{P 1( X O 1nodes {D1, . . . , Dm}. This mapping is determined by the following procedure:| X I1 ), . . . , P m( X O mFig. 1. The coherence graph for P 1, . . . , P 14.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144111(Γ 1) Let Z := { X1, . . . , Xn} be the set of actual nodes.(Γ 2) Let D := {D1, . . . , Dm} be the set of dummy nodes.(Γ 3) Let A := ∅.(Γ 4) For all j ∈ {1, . . . , m}, all i1 ∈ I j , all i2 ∈ O j , add the arcs ( Xi1 , D j) and (D j, Xi2 ) to A.The second function, which we denote by Γ −1, maps a coherence graph on Z = { X1, . . . , Xn}, with dummy nodes{D1, . . . , Dm}, into the collection template(cid:2)P 1( X O 1| X I1 ), . . . , P m( X O m(cid:3)| X Im )related to the variables { X1, . . . , Xn}. This mapping is determined by the following procedure:(Γ −11) Set the collection of lower prevision templates equal to the empty set.(Γ −12) For allj ∈ {1, . . . , m}, add P j( X O jchildren and the parents of D j , respectively.| X I j ) to the collection template, where O j and I j are the set of indexes of theThe idea behind the two functions is very simple: identifying lower prevision templates in a collection with D-structuresin the related coherence graph, and vice versa. This makes the two functions to be each other’s inverses as it is establishedin the following proposition, whose elementary proof is omitted:Proposition 2. There is a one-to-one relationship between coherence graphs and collection templates.This proposition enables us to graphically display some basic configurations of collection templates that are problematicwith respect to the coherence of the collection. One such configuration is created by collection templates whose coherencegraph contains an actual node with more than one parent, such as X8 in Fig. 1. In this case there are two different condi-tional lower previsions, P 6( X8| X3) and P 7( X8| X4), that express knowledge about X8. In this situation it is not possible todeduce the coherence of the collection only taking its template into account. The reason is that it is always possible to finda specific instance of lower previsions with the given template that is incoherent. This is enough because any claim that wemake based only on the template must be valid, by definition, for all the collections of lower previsions with the considered1) for all x3 ∈ X3, and2 in X8, and define, for any gamble f on X8, P 6( f |x3) := f (x8template. For instance, consider x812) for all x4 ∈ X4. This specific choice corresponds to use (precise) degenerate distributions that put all theP 7( f |x4) := f (x82, respectively, and this irrespective of their conditioning events. It follows that P 6( X8| X3)probability mass on x8implies that X8 = x81 and P 7( X8| X4) that X8 = x82, a contradiction.51 and x8(cid:3)= x8Another problematic configuration arises out of collection templates whose coherence graph contains a cycle; this is thecase of the actual nodes X10, X13, X14, in Fig. 1. In this case we can create a contradiction by defining the lower previsionsinvolved in the cycle as follows. Consider x1012 in X13. Let2 in X14, x132 in X10, x14(cid:3)= x13(cid:3)= x10(cid:3)= x1411(cid:8)P 9( f |x13) :=P 13(g|x6, x10) :=(cid:8)P 12(h|x14) :=if x13 = x13f (x101 )1f (x102 ) otherwise,(cid:8)if x10 = x10g(x141 )1g(x142 ) otherwise,if x14 = x14h(x131 )2h(x132 ) otherwise,2 otherwise; in other words, P 9( X10| X13) models the fact that X10 = x10for arbitrary gambles f on X10, g on X14, h on X13. These definitions again correspond to use degenerate (and precise)distributions: for example, P 9( X10| X13) corresponds to a distribution that puts all the probability mass on x101 if X13 = x131 ,and on x101 holds with probability one assuming it is2 , also with probability one. Analogously, P 13( X14| X6, X10)known that X13 = x13states that X14 = x141 ; finally, P 12( X13| X14)1 resp. X10 (cid:3)= x10that X13 = x131 , and otherwise, if X13 (cid:3)= x131 resp. X14 = x142 hold with probability one provided that X10 = x102 hold with probability one provided that X14 = x142 resp. X14 (cid:3)= x142 .1 resp. X13 = x131 , that X10 = x10At this point it is easy to see that the cycle gives rise to a contradiction. Assume that X13 = x13siderations, this implies with probability one that X10 = x101 , then that X14 = x141 , and finally that X13 = x131 . Using the above con-2 , a contradiction.5 In this example, as well as in some of the proofs in Appendix A, we use 0–1 valued probabilities to make things simpler, but we can equivalently2) for all x3 ∈ X3, and P 7( f |x4) :=use probabilities that are never degenerate as above. In this example, for instance, define P 6( f |x3) := 0.1 f (x81) for all x4 ∈ X4. This corresponds to using probability masses on (x80.1 f (x82) equal to (0.1, 0.9) and (0.9, 0.1), respectively, irrespective oftheir conditioning events. To see that these two conditional previsions are not compatible with any joint mass function P on X{3,4}, note that given anysuch P we should deduce on the one hand that P ({x8}) = 0.9, a contradiction. Also the following example based on1cycles can be re-worked, with some additional complications, using only non-degenerate probabilities.}) = 0.1 and on the other that P ({x812) + 0.9 f (x81) + 0.9 f (x81, x8112E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Repeating the argument starting with X13 (cid:3)= x131 , we obtain that X13 = x131 , another contradiction. In summary, the cycle,together with the specific choice of lower previsions, codes the contradictory statement that X13 should be equal to twodifferent values. And since we have been able to find some specific lower previsions with the given template that are inco-herent, then we cannot deduce coherence only considering the related coherence graph—just because it contains a cycle.These considerations motivate the following two definitions, which introduce some graph-based terminology that is moredirectly relevant to our subsequent results.Definition 11. We say that an actual node of a coherence graph is a (potential) source of contradiction if it has more than oneparent or if it belongs to a cycle.More formally, if an actual node X(cid:7) has more than one parent, this means that there are 1 (cid:3) i1 (cid:3)= i2 (cid:3) m such that∩ O i2 . On the other hand, the fact that X(cid:7) belongs to a cycle implies that it is involved in (at least) an elementary(cid:7) ∈ O i1cycle, and so that there are different j1, . . . , j p in {1, . . . , m} such thatO j1∩ I j2(cid:3)= ∅, O j2∩ I j3(cid:3)= ∅,. . . , O j p∩ I j1(cid:3)= ∅,and with (cid:7) belonging to one of these non-empty intersections.(5)Definition 12. A coherence graph without sources of contradiction is said to be of type A1: i.e., acyclic and with maximumindegree for actual nodes equal to one. The corresponding collection template is said to be representable as a graph of type A1,or simply A1-representable.The graph in Fig. 1 is obviously not of type A1, as there are five sources of contradiction: X7 and X8, because each ofthem has two parents; and X10, X13 and X14, because they are part of a cycle. An example of an A1 graph is given in Fig. 2.This is a subgraph of that in Fig. 1 where we have eliminated a number of elements creating sources of contradiction. Thisalso shows that A1 graphs can take complicated forms.When there are sources of contradiction in a coherence graph, it is useful to isolate some special subgraphs that arerelated to them, and which we call blocks.Definition 13. Given a source of contradiction Z , call block for Z , or B Z , the subgraph obtained by taking the union of theD-structures of the dummy nodes that are predecessors of Z .The reason why we introduce blocks is to capture the idea of the ‘area of influence’ of a certain source of contradiction.This is easy to see in the case of an actual node with more than one parent, such as X8 in Fig. 1. In this case we can again1; that P 7( X8| X4) forces X4 touse degenerate distributions in such a way that P 6( X8| X3) forces X8 to take a certain value x81 when X4 = x4take a certain value x41. In thiscase, exploiting the source of contradiction, we can force a specific value on a node such as X1 that can be far away fromthe source itself. This is useful in the proofs to show how to use a source of contradiction to actually create a contradictionsomewhere: in the example above, for instance, we can define P 1( X1) so as to force X1 to take a value different from x11,thus originating a contradiction at X1. The situation is a bit more complicated with cycles, but the effect is eventually thesame.1, and in turn, that P 2( X4| X1) forces X1 to take a certain value x11 when X8 = x8It is useful to study a bit more in detail the situations just sketched. For this, we are going to introduce the notion ofconstraining sub-block for an actual node in a block. This will be a subset of the previsions of the block with the propertythat we can assign them certain values for which there is a unique value in the actual node compatible with them. Inthe previous example the constraining sub-block for X1 would be determined by the previsions P 6( X8| X3), P 7( X8| X4) andP 2( X4| X1).Fig. 2. An example of an A1 coherence graph.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144113In a constraining sub-block, as we shall show in Proposition 3, the information flows (possibly opposite to the directionsof the arcs) so as to eventually force an actual node to take on a chosen value. This property will be used in Theorem 4 todecompose the task of verifying weak coherence in an optimal way.Consider first of all a block B Z originated by an actual node Xs with more than one parent. Then there exist 1 (cid:3)∩ O i2 . For any actual node X(cid:7) in B Z , there exists a path leading from X(cid:7) to the node Xsi1 (cid:3)= i2 (cid:3) m such that s ∈ O i1originating the block. Hence, there are i1, . . . , i p in {1, . . . , m} such that s ∈ O i1(cid:3)= ∅,and (cid:7) ∈ I i p . Note that we can assume without loss of generality that the nodes {i1, . . . , i p} are all different: otherwise,we could establish a cycle in the path from X(cid:7) to Xs, and by eliminating these cycles we should obtain another (shorter)path going from X(cid:7) to Xs where all the indices are different. We shall refer to a set of indices {i1, . . . , i p} such that(cid:3)= ∅, (cid:7) ∈ I i p as a the constraining sub-block for X(cid:7) in the block generated by Xs.s ∈ O i1Note that such a sub-block is not necessarily unique.(cid:3)= ∅, . . . , I i p−1(cid:3)= ∅, . . . , I i p−1∩ O i2 , I i2∩ O i2 , I i2∩ O i p∩ O i3∩ O i p∩ O i3Consider now a block B Z which is generated by a cycle. Then this cycle corresponds to a strong component in thecoherence graph, in the sense that for any two nodes Xs1 and Xs2 in the component there is a path going from Xs1 to Xs2and another one going from Xs2 to Xs1 ; hence, Xs1 and Xs2 belong to a cycle.Again, we have two possibilities: that a node Xs in the block B Z belongs to the strong component of the block, or thatit is a predecessor of this strong component. In the first case, Xs belongs to an elementary cycle, meaning that there existj1, . . . , j p ∈ {1, . . . , m} satisfying Eq. (5) and s ∈ O j1∩ I j2 . Consider on the other hand a predecessor X(cid:7) of this source ofcontradiction. Then X(cid:7) is a predecessor of the nodes in an elementary cycle, which we denote by Xs, and it is not itself inthe cycle. Let j1, . . . , j p be the indices of the previsions in the cycle, meaning that they satisfy Eq. (5), with s ∈ O j1∩ I j2 .Then there exist k1, . . . , kr such that (cid:7) ∈ O k1(cid:3)= ∅, and where kr is one of the indices in∩ Ik3the cycle, for instance kr = j1.(cid:3)= ∅, . . . , O kr−1∩ Ik2 , O k2∩ IkrThe indices j1, . . . , j p are all different, because the cycle is elementary. Moreover, we can also assume that the indices{k1, . . . , kr−1} are different from { j1, . . . , j p}: otherwise, we can eliminate the indices they have in common and we shouldstill have a path from X(cid:7) to the cycle. Finally, we can assume without loss of generality that the indices {k1, . . . , kr−1} areall different, because otherwise we should be able to establish a cycle in them, and by eliminating these cycles we shouldobtain another (shorter) path from X(cid:7) to the cycle. We shall refer to the indices {k2, . . . , kr−1, j1, . . . , j p} as a constrainingsub-block for X(cid:7) in the block associated to the source of contradiction Xs. Again, such a constraining sub-block may not beunique.Remark 2. Note that in this constraining sub-block we have two possibilities:• O kr−1∩ Ikr∩ O j pparent, and {k2, . . . , kr−1, kr, j p} would also be a constraining sub-block for X(cid:7) (see an example in Fig. 3).(cid:3)= ∅, and then in particular O kr−1(cid:3)= ∅. Then X(cid:7) is a predecessor of a node with more than one∩ O j p• If O kr−1∩ Ikr∩ O j p= ∅, then X(cid:7) is a predecessor of a dummy node in the cycle, and only precedes the actual nodes inthe cycle through this dummy node (see an example in Fig. 4).The distinction between these two cases will simplify the proofs of our subsequent results.As mentioned previously, the reason why we have defined the notion of constraining sub-block is because it can be usedto determine the value of the variable X(cid:7). This is stated in the following proposition:Fig. 3. Constraining sub-block for X6 in the block associated to the source of contradiction, here a node with two predecessors.Fig. 4. Constraining sub-block for X6 in the block associated to the source of contradiction, in this case a cycle.114E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Fig. 5. The areas delimited by closed lines denote the blocks and superblocks of the coherence graph.Proposition 3. Let us consider a block B Z , and any actual node X(cid:7) that belongs to B Z . Consider x ∈ X(cid:7).(1) If X(cid:7) is a source of contradiction in the block B Z , let j1 be an element of {1, . . . , m} such that (cid:7) ∈ O j1 . Then there are weakly) satisfies| X Im ) such that any prevision P which is coherent with P j1 ( X O j1| X I1 ), . . . , Pm( X O m| X I j1coherent previsions P 1( X O 1P (x) = 1.(2) If X(cid:7) is not a source of contradiction in B Z , let { j1, . . . , jk} be the indices of the previsions in a constraining sub-block for X(cid:7).) for| X Im ) weakly coherent such that any prevision P coherent with P ji ( X O ji| X I1 ), . . . , Pm( X O m| X I jiThen there are P 1( X O 1i = 1, . . . , k satisfies P (x) = 1.An important consequence of this proposition is that if two blocks happen to share an actual node, then it is possibleto create a contradiction at that node by forcing a certain value on it from one block, and a different value on it from theother block. This suggests that the blocks that share some actual nodes should be considered as a single structure, ratherthan as separate ones, in order to avoid contradictions; this is the reason for the following definition.Definition 14. Call superblock of a coherence graph, any union of all the blocks that share at least an actual node.Fig. 5 displays the three different blocks of the coherence graph under consideration: the block for X7, the block for X8,and the one for X14 (or, equivalently, for X10 or X13). The blocks for X7 and X8 have a node in common ( X3), so their unionis in a superblock, also shown in the figure. The other superblock in the graph is just B X14 .Observe that there can be many configurations of blocks in a superblock: a superblock can be made up of a single block;if it is made up of more than one block, it may be the case that some blocks coincide (for instance B X14 coincides withB X10 and B X13 ), that one of them is included in another, or that two of them share only some nodes (as B X7 and B X8 in thesame figure).It is useful to make two observations at this point. The first is that the intuition behind the notion of superblock madeup by different blocks is to delimit the joint area of influence of multiple sources of contradictions that belong to differentblocks connected by some actual nodes. The second is more formal and concerns the actual nodes of different blocks and(I i ∪ O i) of the actual nodestheir relation with superblocks: in fact, for any block B Z we can consider the set A Z :=(cid:3)= ∅, B Z1 and B Z2 belong to the same superblock. It follows from this that if we considerinvolved in B Z ; then, if A Z1two different superblocks B 1 and B2, then (I i ∪ O i) ∩ (I j ∪ O j) = ∅ for any i ∈ B1, j ∈ B2.∩ A Z2i∈B Z(cid:7)Now we use the notion of superblock in order to build a partition of the dummy nodes. The point here is that, similarlyto the case of blocks, sources of contradictions can extend their influence to every actual node in a superblock under anopportune definition of the lower previsions involved. Because of this, it is not possible to prove the coherence of thelower previsions in a superblock by simply inspecting the coherence graph: to prove it, it is necessary to know somethingmore than the bare collection template. It follows that superblocks are a kind of core entities in that we cannot provethe coherence of a collection template without first being able to prove the coherence of the lower previsions in eachsuperblock. Those core entities constitute the elements of the partition defined below, together with the lower previsionsthat do not belong to any superblock.Definition 15. Call minimal partition of the dummy nodes in a coherence graph, the partition whose elements are the sets ofdummy nodes in each superblock, and the singletons made up of the remaining dummy nodes. The corresponding partitionof {1, . . . , m} is denoted by B and is simply called the minimal partition.Note that B refers also to a partition of the related collection template, given the one-to-one correspondence betweendummy nodes and lower prevision templates. With respect to the graph in Fig. 5, we obtain the following partition of therelated collection template:E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144115(cid:2)(cid:2)(cid:3)P 1( X1), P 2( X4| X1), P 4( X7| X2), P 5( X7| X3), P 6( X8| X3), P 7( X8| X4)(cid:3)P 3( X6| X5), P 9( X10| X13), P 12( X13| X14), P 13( X14| X6, X10)(cid:3)P 8( X9| X1, X5)(cid:3)P 11( X12| X8)(cid:3)P 10( X11| X7)(cid:2)(cid:2)(cid:2),,,,,P 14( X15, X16| X9, X12, X13)(cid:2)(cid:2)(cid:3)(cid:3).Moreover, note that for A1-representable collection templates, the minimal partition is entirely made up of singletons,because their coherence graph has no sources of contradiction, and has therefore m elements.We conclude this section by remarking that if we consider B 1, B2 in the minimal partition with |B 1| > 1, |B2| = 1, i.e.,such that B1 is associated to a superblock and B 2 to an A1 component of the coherence graph, we must have O i ∩(I j ∪ O j) =∅ for any i ∈ B2, j ∈ B1: if O i ∩ O j (cid:3)= ∅ we have a node with more than one parent, and i should belong to the associatedsuperblock; and if O i ∩ I j (cid:3)= ∅ then i would be a predecessor of j, which is a predecessor of a source of contradiction, andtherefore i should be in the same block as j. Nevertheless, we may have I i ∩ (I j ∪ O j) (cid:3)= ∅ for some j ∈ B1. This will beimportant in the proofs of the results we formulate in the next section.6. Coherence graphs as tools to prove coherenceThis section formally relates the graphical notions introduced in the previous section with the notions of weak andstrong coherence, and doing so gives the most important results of this paper.In particular, the following theorem gives us conditions under which the coherence of some subsets of a collection ofconditional lower previsions implies the coherence of all the elements in the collection. It shows that it is sufficient thatthe conditional lower previsions whose indices belong to the same element in B be coherent.Theorem 2. Let {P 1( X O 1B be their associated minimal partition. If {P j( X O j{P 1( X O 1| X I1 ), . . . , P m( X O m| X I1 ), . . . , P m( X O m| X Im )} are coherent.| X Im )} be a collection of separately coherent conditional lower previsions, and let| X I j )} j∈B are coherent for any B ∈ B, then the conditional lower previsionsThe intuition behind the proof of the theorem, which we include in Appendix A, is the following. We exploit the prop-erties of the coherence graph to create a total order on a set of coherent lower previsions tightly related to our collectiontemplate. That order allows us to use the generalisation of the marginal extension theorem established in [23] to show thatthe lower previsions in that set are coherent, and from this to derive the coherence of P 1( X O 1| X I1 ), . . . , P m( X O m| X Im ).It is easy to see that a similar result holds when we work with weak coherence instead of coherence:Theorem 3. Let {P 1( X O 1any B ∈ B, {P j( X O j| X I1 ), . . . , P m( X O m| X Im )} be a collection of separately coherent conditional lower previsions such that for| X I j )} j∈B are weakly coherent. Then {P 1( X O 1| X I1 ), . . . , P m( X O m| X Im )} are weakly coherent.Next, we investigate in which sense the partition B given by Definition 15 is minimal. For this, we should like to knowif there are other partitions of {1, . . . , m} that we can use for the same end, meaning that the coherence of the conditionallower previsions within each of the elements of the partition guarantees the coherence of the collection template.A first positive result in that respect is that the partition B is indeed minimal when we are studying the problem forweak coherence:Theorem 4. Let B(cid:12)implies the weak coherence of {P 1( X O 1| X I1 ), . . . , P m( X O mbe a partition of {1, . . . , m}, and assume that, for any Bin B(cid:12)| X Im )} if and only if B is finer than B(cid:12), {P j( X O j(cid:12).| X I j )} j∈B(cid:12) are weakly coherent. Then thisThe sufficiency part in this proposition is actually Theorem 3. The idea for the necessity part is to show that, when the| X Im ) that are not weaklynecessary condition fails, we can create conditional linear previsions P 1( X O 1coherent and yet for all B| X I1 ), . . . , Pm( X O m| X I j )} j∈B(cid:12) are weakly coherent., {P j( X O jin B(cid:12)(cid:12)However, a similar result to Theorem 4 does not apply for coherence: there are instances of collection templates wherethe coherence within the elements of a partition which is not coarser than B guarantees the coherence of all of them. Onesuch case is given in the following example.Example 2. Let us consider the collection template associated to the conditional lower previsions {P 1( X1), P 2( X2| X1),P 3( X2, X3| X1)}. Its coherence graph is given in Fig. 6, and the minimal partition B associated to that graph is {1, 2, 3}.However, we can deduce the coherence of the collection template with a smaller subset. For this, let us show that the co-herence of P 2( X2| X1), P 3( X2, X3| X1) implies (actually, it is equivalent to the fact) that for any X1 × X2-measurable gamblef 0 and for any x1 ∈ X1,P 2( f 0|x1) = P 3( f 0|x1).(6)To see this, apply Eq. (4) to P 2( X2| X1) and P 3( X2, X3| X1) using j0 := 2, z0 := x1 ∈ X1, f 2 := 0, and f 3 := f 0I{x1}×X{2,3} :116E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Fig. 6. Coherence graph of {P 1( X1), P 2( X2| X1), P 3( X2, X3| X1)}.(cid:9)(cid:10)G 2(0| X1) + G 3( f 0I{x1}×X{2,3} | X1) − G 2( f 0|x1)(cid:10)G 3( f 0I{x1}×X{2,3} |x1) − G 2( f 0|x1)(cid:10)f 0 − P 3( f 0|x1) − f 0 + P 2( f 0|x1)(cid:9)0 (cid:3) supB= supπ −11 ({x1})(cid:9)= supπ −11 ({x1})= −P 3( f 0|x1) + P 2( f 0|x1),i.e., P 2( f 0|x1) (cid:2) P 3( f 0|x1). The converse inequality, P 2( f 0|x1) (cid:3) P 3( f 0|x1), follows by repeating the same argument withj0 := 3, z0 := x1, f 2 := f 0I{x1}×X2 and f 3 := 0.At this point, assume that P 2( X2| X1), P 3( X2, X3| X1) are coherent, and hence that Eq. (6) holds. Consider the expressionused to prove the coherence of the previsions P 1( X1), P 2( X2| X1), and P 3( X2, X3| X1): i.e.,(cid:11)G 1( f 1) + G 2( f 2| X1) + G 3( f 3| X1) − G j0 ( f 0|x1)G 1( f 1) + G 2( f 2| X1) + G 3( f 3| X1) − G j0 ( f 0)if j0 ∈ {2, 3}if j0 = 1,j = 1, 2, 3,for any f j ∈ K j ,j0 ∈ {1, 2, 3}, f 0 ∈ K j0 and x1 ∈ X1; verify that G 2 can be replaced by G 3 under (6). As aresult, we obtain that the conditional lower previsions P 1( X1), P 2( X2| X1) and P 3( X2, X3| X1) are coherent if and only ifP 1( X1), P 3( X2, X3| X1) are. But P 1( X1), P 3( X2, X3| X1) are always coherent because of the marginal extension theorem [33,Theorem 6.7.2], and so the coherence of P 2( X2| X1), P 3( X2, X3| X1) implies the coherence of the collection template.It remains an open problem at this stage to determine a partition with the property that the coherence within each ofits elements guarantees the coherence of the collection template, and that is minimal in the sense that it is finer than anyother partition with the same property.In this respect, the most interesting particular case would be under which conditions this minimal partition is equal to| X Im )} from their separate coher-{{1}, . . . , {m}}, that is, when we can deduce the coherence of {P 1( X O 1| X Im )} is ofence. We can deduce from Theorem 2 that when the coherence graph associated to {P 1( X O 1type A1, then the separate coherence of these previsions implies their coherence. Using Theorem 4, we can prove that beingof type A1 is also necessary for this property.| X I1 ), . . . , P m( X O m| X I1 ), . . . , P m( X O mProposition 4. Consider a collection {P 1( X O 1lowing are equivalent:| X I1 ), . . . , P m( X O m| X Im )} of separately coherent conditional lower previsions. The fol-(1) The separate coherence of {P 1( X O 1(2) The separate coherence of {P 1( X O 1(3) The coherence graph of this collection template is of type A1.| X I1 ), . . . , P m( X O m| X I1 ), . . . , P m( X O m| X Im )} implies their coherence.| X Im )} implies their weak coherence.We should like to conclude this section remarking that if the collection template is A1-representable, then we can givethe following sensitivity analysis interpretation:Theorem 5. Consider a collection of separately coherent conditional lower previsions. If their coherence graph is A1, then these lowerprevisions are lower envelopes of a family of coherent linear previsions.Hence, when the coherence graph is A1, we can interpret our coherent conditional lower previsions as a model for theimprecise knowledge of some precise coherent conditional linear previsions. The interest of this result lies in the fact thatthe lower envelopes of a family of coherent conditional linear previsions are coherent conditional lower previsions, but theconverse does not hold in general: there exist instances of coherent conditional lower previsions that are not even domi-nated by any family of coherent conditional linear previsions [33, Section 6.6.10].6 A sufficient condition for the converse to6 The previsions in the example given in that section can be written in our language as P ( X1, X2), P ( X2| X1). Note that they both belong to the sameblock, and consequently their coherence graph is not A1.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144117hold is that the spaces X1, . . . , Xn are finite [33, Theorem 8.1.10]. This theorem shows that, if the coherence graph is A1,then the coherent conditional lower previsions are also lower envelopes of coherent conditional linear previsions, no matterthe cardinality of the spaces.7. An algorithm to find the minimal partitionIn order to exploit coherence graphs as tools to check coherence, one should be able to compute the minimal partitionof a coherence graph. This is what we set out to do in the following: we give an algorithm to find the minimal partitionand then discuss its computational complexity, thus showing the efficiency of the algorithm.The rationale behind the algorithm is very simple. We start a visit of the graph from each source of contradiction, go-ing backwards with respect to the direction of the arrows, so as to identify the related block. This is done by a recursiveprocedure that tags the nodes found in the visit, assigning different tags to blocks originated by different sources of contra-diction. The only complication is that some blocks might have non-empty intersection, and as a consequence they shouldbe regarded as a single superblock. When this happens, the tags of the different blocks should be regarded as the same.To this extent, we implement a data structure that acts as a dictionary, i.e., which maps the tags of the blocks in the samesuperblock into a unique tag, referred to as the true tag, which can be regarded as the class of equivalence of those tags.The dictionary is filled by a procedure during the visits of the graph every time a node is found at the intersection of twoor more blocks.Below we describe the algorithm more precisely in a C-like language (as opposed to C, we assume that the first elementof an array has index 1 to make the code simpler to read). To make things simpler, we assume that the nodes of the graphhave been re-indexed from 1 to m + n, where the first m nodes are D1, . . . , Dm and the following ones are the actual nodes.We also assume that, as a result of previous computations, the following global data structures are available:• two integer numbers, m and n, corresponding to the number of dummy and actual nodes, respectively.• An array called node, of size m + n, whose generic element node[i] is a structure that contains the following com-ponents related to node i:– an integer called node[i].nParents containing the number of parents of node i;– an integer array called node[i].parent, of size node[i].nParents, containing the indexes of the parents ofnode i;– an integer called node[i].tag initialised to 0 to denote that node i is not tagged.• An integer called nContradictions containing the number of actual nodes that are sources of contradictions.• An integer array called contradiction, of size nContradictions, containing the indexes of the actual nodes thatare sources of contradictions.• An integer array called minPartition, of size m, which implements the dictionary.Fig. 7 reports the software code to find the minimal partition based on such data structures.The code has three procedures: findMinPartition, which is the main one; its subroutine findBlock, with param-eters i and currentTag; and mergeBlocks, with parameters tagFound and currentTag, which is a subroutine offindBlock.The purpose of findMinPartition is to assign tags to the dummy nodes and fill the array minPartition in sucha way that for any j1, j2 ∈ {1, . . . , m}, j1 and j2 belong to the same element of the minimal partition if and only if the tagsminPartition[node[j1].tag] and minPartition[node[j2].tag] coincide.findMinPartition works in two steps. In the first, up to and including line 07, the procedure enumerates thesources of contradiction and, when one is found that is not tagged, calls findBlock to tag it and all its predecessors, thusidentifying a block. findBlock also takes care, by means of mergeBlocks, to merge blocks with non-empty intersec-tion into a superblock by properly filling the array minPartition, so that at line 08 of findMinPartition all thesuperblocks have been identified. In the second step, from line 08 onwards, the procedure simply considers the dummynodes that are not in any superblock and tags them with new and increasing values of currentTag, thus identifying theremaining elements of the minimal partition.The way findBlock works is more complicated. Its main purpose is to tag the actual node i and all its predecessorsby currentTag in a recursive way. It may happen that some of the nodes found during the visit in the graph are alreadytagged (this happens at line 25 if the node found is dummy and at line 29 if it is actual). In this case, if the true tag foundis different from the current one, findBlock calls mergeBlocks to merge the two blocks related to the two differenttags. mergeBlocks does so by identifying the tags with each other.Let us recall at this point that to be really in the condition of computing the minimal partition, we should also havemethods to fill the global data structures mentioned at the beginning of this section, before running findMinPartition.These methods are actually trivial to implement apart from the one that solves the problem of identifying the actual nodesinvolved in cycles.A key observation to address such a problem is that a node belongs (at least) to a cycle in a directed graph if and only ifit belongs to a non-trivial strong component of the graph. This implies that identifying the strong components enables one to118E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144}}for(i=1;i<=m;++i) // enumerate dummy nodesif(!node[i].tag){ // that are not tagged, i.e., not in any superblockint j, k;if(!node[i].tag){ // node not tagged yet?node[i].tag=++currentTag; // give (newly created) tagminPartition[currentTag]=currentTag; // update dictionaryfindBlock(contradiction[l],++currentTag); // give tag to nodes in blockminPartition[currentTag]=currentTag; // update dictionaryint currentTag=0, i, l;for(l=1;l<=nContradictions;++l) // enumerate sources of contradictionif(!node[contradiction[l]].tag){ // contradiction not tagged yet?01 void findMinPartition(){ // identify blocks and superblocks020304050607080910111213 }1415 void findBlock(int i, int currentTag){ // identify block for given actual node16171819202122232425262728 }2930 void mergeBlocks(int tagFound, int currentTag){ // two tags must become one3132333435 }node[node[i].parent[j]].tag=currentTag; // tag itfor(k=1;k<=node[node[i].parent[j]].nParents;++k) // enumerate its parentsfindBlock(node[node[i].parent[j]].parent[k],currentTag); // recursion}else if(minPartition[node[node[i].parent[j]].tag]!=currentTag) // tagged?int l;for(l=1;l<currentTag;++l) // enumerate existing (true) tagsnode[i].tag=currentTag; // give tagfor(j=1;j<=node[i].nParents;++j) // enumerate parentsmergeBlocks(minPartition[node[i].tag],currentTag); // merge (the two blocks)}else if(minPartition[node[i].tag]!=currentTag) // (already) tagged?minPartition[l]=currentTag; // is now mapped into ‘currentTag’if(!node[node[i].parent[j]].tag){ // parent not tagged yet?mergeBlocks(minPartition[node[node[i].parent[j]].tag],currentTag); // mergeif(minPartition[l]==tagFound) // any tag ‘l’ previously mapped into ‘tagfound’Fig. 7. The software code in C-like language to find the minimal partition of a coherence graph.find the actual nodes involved in at least a cycle. Fortunately, the task of identifying the strong components is well knownand efficiently tackled by Tarjan’s algorithm in [29], so that also this part of the problem can be addressed easily.The complexity of the overall procedures, including the methods to fill the global data structures, is given in the followingtheorem.Theorem 6. The worst-case complexity to compute the minimal partition using the procedures in Fig. 7 is bounded by O (m +n +m ·n).Note that this bound is derived under the implicit assumption that the input of the problem is a coherence graph, butusually one would rather start with a collection template. This is not problematic because the lower prevision templatesin a collection are in one-to-one correspondence with the D-structures of the corresponding coherence graph, as stated inSection 5, so that converting a collection template into a coherence graph takes linear time and thus it does not increasethe complexity bound derived above.8. ApplicationsNow that the main results of this paper have been presented, we can show how they naturally relate with three impor-tant models and tools well known in artificial intelligence. In Section 8.1 we shall use our results to prove for the first timethe coherence, to a large extent, of the graphical models called Bayesian and credal networks. In Section 8.2, we shall estab-lish a tight relationship between weak coherence and the so-called compatibility problem: i.e., checking whether a numberof assessments admits a compatible joint probabilistic model. In this case, we shall show that coherence graphs allow usto optimally decompose a compatibility problems into smaller ones, for a very general version of the problem. Finally, inSection 8.3, we focus on a recently proposed coherence-based version of probabilistic satisfiability that enhances and extendssimilar problems in probabilistic logic. In this case we first give new results that detail the connection between these moretraditional approaches and Walley’s theory. Then we show that coherence graphs can be used also in this case to decomposesome important instances of probabilistic satisfiability into smaller ones.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–1441198.1. Coherence of Bayesian and credal networksLet us focus on proving the coherence of the graphical models called Bayesian nets and their extension to impreciseprobability called credal nets.Although it may seem surprising at first, these models have not been shown yet to be coherent in a strong sense. Whatis well known is that these models give rise to a joint coherent lower prevision (in our language, this means that theassessments that make up those models are pairwise coherent with such a joint). For example, a Bayesian net is equivalentto a joint mass (or density) function over the considered variables; a credal net is equivalent to a closed convex set of suchjoint mass functions (the joint coherent lower prevision is the lower envelope of the expectations computed from thosemass functions). This is often implicitly taken as evidence that the models under consideration are self consistent. But, aswe have shown in Theorem 1, the existence of such a joint is only equivalent to weak coherence; and weak coherenceleaves room to inconsistencies so that Bayesian and credal nets could still express inconsistent beliefs, for all we know.In Section 8.1.2 we show that this is not the case as the mentioned models satisfy Walley’s notion of strong coherence.We do this using coherence graphs and some additional developments introduced in Section 8.1.1 that are needed to con-nect coherence graphs with the so-called notion of strong product, a generalisation of stochastic independence to impreciseprobability. The connection is necessary as the strong product underlies the models under consideration. Finally, in Sec-tion 8.1.3 we discuss some extensions of the presented results to more general models of credal nets and also to somemajor statistical applications.8.1.1. Relating A1 graphs to the strong productIn this section we summarise some results that connect A1 graphs with a notion of probabilistic independence calledstrong independence (see [8,24]) and that have been introduced in [37].We start with a lemma7 that shows that A1 graphs naturally entail a notion of order of the corresponding lower previ-sions: in particular, that it is possible to permute the indexes of the lower previsions in such a way that the only admissiblepaths between two dummy nodes are those in which the index of the origin precedes that of the destination.8Lemma 1. (See [37, Lemma 1].) If the coherence graph associated to the collection template {P 1( X O 1then we may assume without loss of generality that for any k = 1, . . . , m, O k ∩ (k−1i=1 I i) = ∅.(cid:7)| X I1 ), . . . , P m( X O m| X Im )} is A1,Note that this result depends only on the properties of the coherence graph, and therefore it is applicable also whendealing with precise conditional previsions.Now we restrict the attention to the special case of A1 graphs originated by collections of separately coherent conditionallower previsions such that {O 1, . . . , O m} forms a partition of {1, . . . , n}. This means that for each variable there is exactly+one lower prevision expressing beliefs about it. Let us call this type of graphs A1. From Lemma 1, if a collection template+-representable, we can assume without loss of generality that I1 = ∅. Let us define{P 1( X O 1| X Im )} is A1| X A j ∪I j ) be given on the set H j of X A j+1 -A1 := ∅, A j :=measurable gambles by| X I1 ), . . . , P m( X O mj−1i=1 (I i ∪ O i) for j = 2, . . . , m + 1, and for j = 1, . . . , m let P(cid:12)j( X O j(cid:7)(cid:12)j( f |z) := P jP(cid:12)(cid:13)f (z, ·)|πI j (z)for any z ∈ X A j ∪I j and any f ∈ H j . Since P j( X O j| X A j ∪I j ). More-over, thanks to Lemma 1 and the requirement that {O 1, . . . , O m} forms a partition of {1, . . . , n}, the sets of indices of(cid:12)| X Am∪Im ) form an increasing sequence and hence theym( X O mthe conditioning variables in the previsions Psatisfy the hypotheses of the generalised marginal extension theorem established in [23, Theorem 4]. As a consequence,P| X I j ) is separately coherent for j = 1, . . . , m, so is P(cid:12)1( X O 1 ), . . . , P| X Am∪Im ) are also coherent.(cid:12)1( X O 1 ), . . . , PA similar reasoning shows that if we take for j = 1, . . . , m a conditional linear prevision P| X A j ∪I j ) on the set H j| X Am∪Im ) are jointly coherent. Moreover, taking into account thatthat dominates P{O 1, . . . , O m} is a partition of {1, . . . , n}, Theorem 3 in Ref. [23] implies that the only prevision P on X n which is coherentwith the assessments P| X A j ∪I j ), then P(cid:12)1( X O 1 ), . . . , P(cid:12)m( X O m(cid:12)m( X O m(cid:12)j( X O j(cid:12)j( X O j(cid:12)j( X O jP ( f ) = P(cid:12)1(cid:12)(cid:12). . .(cid:12)1( X O 1 ), . . . , P(cid:12)(cid:12)| X Am∪Im ) ism( X O m(cid:13)(cid:13)(cid:13)(cid:12)m( f | X Am∪Im ).(cid:12)m( X O m(cid:12)P2(cid:12)1( X O 1 ), . . . , P. . .PIn other words, Pcan be checked that P is the prevision associated to the probability mass function(7)| X Am∪Im ) give rise to a unique joint lower prevision. When X1, . . . , Xn are finite, itP (x) =m(cid:14)j=1(cid:13)(cid:12)πO j (x)|π A j ∪I j (x).(cid:12)jPAt this point we are ready to give the definition of lower envelope model.(8)7 This lemma will be used not only in this section but also in the proofs of Appendix A for other reasons, through Lemma 6.8 This order notion is similar to the graph-theoretic notion of topological ordering, but here it is applied only to the dummy nodes.120E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Definition 16. Suppose that for each λ ∈ Λ, the collection of conditional lower previsions {P λ( X O 1 ), . . . , P λ( X O m(cid:12)(cid:12)j( X O jj( X O jis s.t. for allThe coherentP λ( X O 1 ), . . . , P λ( X O m| X Am∪Im )}j = 1, . . . , m, P λ( X O j| X A j ∪I j ).lower prevision P defined as P := minλ∈Λ P λ, where P λ is the coherent prevision determined by| X Am∪Im ) and Eq. (7), is called a lower envelope model.| X A j ∪I j ) and also minλ∈Λ P λ( X O j| X A j ∪I j ) dominates P| X A j ∪I j ) = PIntuitively, a lower envelope model is a joint lower prevision that is built out of a number of conditional and uncondi-tional assessments. The interest in lower envelope models arises because it is a very common practise to build joint modelsout of smaller conditional and unconditional ones, and then to use the joint model to draw some conclusions. Lower en-velope models abstract this procedure of constructing joint models in the general case of coherent lower previsions. Asparticular cases of lower envelope models, we can consider the following:(1) If for each j = 1, . . . , m we consider all the P λ( X O j| X A j ∪I j ) in the set M(Pis the marginal extension of P(cid:12)(cid:12)m( X O m1( X O 1 ), . . . , P| X A j ∪I j ) in the set of extreme points of M(P(2) If for j = 1, . . . , m we take all the P λ( X O j| X Am∪Im ).(cid:12)j( X O jditional requirement that P λ( X O jstrong product of P 1( X O 1 ), . . . , P m( X O m|z) = P λ( X O j| X Im ).|z(cid:12)) if πI j (z) = πI j (z(cid:12)| X A j ∪I j )), with the ad-j( X O j(cid:12)), then the lower envelope model P is called the| X A j ∪I j )), then the lower prevision PThe marginal extension represents the most conservative lower envelope model built out of the assessments defined in (7).The strong product is also the most conservative lower envelope model built out of those assessments with the additionalassumption of strong independence (see for instance [37] for additional information).Theorem 7. (See [37, Theorem 2].) Consider an A1lower envelope model associated to it. Then P , P 1( X O 1 ), . . . , P m( X O m-representable collection template {P 1( X O 1 ), . . . , P m( X O m| X Im ) are coherent.+| X Im )}, and let P be aNote that this result is concerned with the original assessments, not with those defined in (7). In particular, it implies-representable collection. This is the main tool that we+that it is always coherent to build the strong product out of an A1shall use in the following.8.1.2. Bayesian and credal networksA Bayesian net [27] is made of a directed acyclic graph whose nodes are in one-to-one correspondence with the variablesX1, . . . , Xn (a well-known example of Bayesian net is given in Fig. 8). The arcs of the graph represent stochastic indepen-dences among the variables by means of the so-called Markov condition: i.e., the fact that a variable is independent of itsnon-descendant non-parents given its parents. Let X j be the generic node in the net, and denote by I j the set of indexes ofits parents. Let us assume that the variables take values on finite spaces. In this case each node of the network is associatedwith a set of conditional mass functions that in our language correspond to the conditional linear previsions P j( X j| X I j ).Fig. 8. The Asia network: a model of an artificial medical problem related to the presence of dyspnea. The nodes correspond to variables (with names inparentheses), which are all 0–1 valued in this example.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144121Fig. 9. The coherence graph obtained from the Asia Bayesian network.Note also that m = n with Bayesian nets. Specifying a Bayesian net is equivalent, through the Markov condition, to specifyinga joint mass function over the variables of interest:9P (x) =n(cid:14)j=1(cid:13)(cid:12)π j(x)|πI j (x),P jx ∈ X n.(9)Credal networks are an extension of Bayesian nets to imprecise probabilities [8]. The extension is achieved, in the caseof the so-called separately specified credal nets, about which we focus, by allowing the generic node X j to replace each ofits mass functions with the closed convex hull of a finite number of conditional mass functions. In other words, the linearprevision P j( X j| X I j ) is replaced by a coherent lower prevision P j( X j| X I j ) for all the nodes X j in the net. As an example,the Asia network can be turned into a credal net by replacing each of its local conditional probabilities with a probabilityinterval, as a closed convex set of mass functions is equivalent to a probability interval in the case of a binary variable.A credal net is equivalent to a set of Bayesian nets: if one chooses a precise mass function in each local closed convexset of the net, the credal net becomes a Bayesian net that is ‘compatible’ with the credal net. In the language of coherentlower previsions this means that the graph, together with the choice of a linear prevision P j( X j| X I j ) (cid:2) P j( X j| X I j ) at nodej = 1, . . . , n, is a Bayesian net. Therefore, a credal net can be regarded as the set of Bayesian nets originated byX j , for allchoosing the above dominating linear previsions in all the possible ways. Call P the set of joint mass functions obtainedapplying Eq. (9) to each compatible Bayesian net.Credal nets are rendered consistent with Walley’s theory of coherent lower previsions using the notion of strong extension,defined asK (P) := CH(P),where the symbol CH denotes the operation of taking the closed convex hull. Since K (P) is convex, it has extreme points,i.e., mass functions in K (P) that cannot be expressed as convex combinations of other ones in K (P). Let us denote theset of extreme points by ext(K (P)). Usually, the definition of credal networks requires that such a subset is finite. It iswell known that in this case ext(K (P)) ⊆ P ; this means that the extreme points correspond to a subset of the compatibleBayesian nets. For this reason, a credal net is usually regarded as equivalent to a finite set of Bayesian nets even if P hasinfinitely many elements. Note also that a Bayesian net is a special case of credal net. In such a case, the strong extensionis a singleton containing the joint mass function coded by the Bayesian net through Eq. (9).The strong extension is nothing else but an equivalent representation of a coherent lower prevision P ( X1, . . . , Xn), fromwhich the connection with Walley’s theory. To enforce this connection, inferences with credal nets are usually made withrespect to the strong extension rather than the initial set P . This should not be controversial because doing inference withcredal nets is usually taken to be the computation of lower and upper posterior expectations; and these stay the sameirrespective of the fact that one uses P , K (P) or ext(K (P)).At this point we are ready to show that credal nets, and consequently Bayesian nets, are coherent models. To this aim,coherence graph (see Fig. 9 for an example) and that the strongit is sufficient to show that a credal net leads to an A1extension of a credal net coincides with the strong product of the related coherence graph, as in the next theorem.+Theorem 8. The local conditional lower previsions P 1( X1| X I1 ), . . . , P n( Xn| X In ) of a credal network are A1strong product coincides with the strong extension of the network.+-representable. TheirWe can then apply Theorem 7 in a straightforward way to obtain the wanted result as an immediate corollary of theprevious theorem.9 Note that in Expression (9) the symbol π refers to the projection operator from Definition 1, and it should not be confused with the symbol used todenote the parents of a node in a Bayesian network.122E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Corollary 2. Let P 1( X1| X I1 ), . . . , P n( Xn| X In ) be the local conditional lower previsions of a credal network, and let P ( X1, . . . , Xn) beits strong extension. Then P 1( X1| X I1 ), . . . , P n( Xn| X In ) and P ( X1, . . . , Xn) are coherent.8.1.3. Some concluding remarksThere are at least two reasons that make Corollary 2 important. One is that credal and Bayesian nets are very general andimportant models, and it is important to know that they are coherent, both for theoretical and practical reasons. Actually,exploiting tools presented elsewhere [37], it is possible also to give a stronger result: that coherence is preserved evenunder the so-called updating of a credal net, under very general conditions. This means that it is not possible to produceinconsistencies by building and repeatedly using a credal net.The second reason is the (implicit) generality of the theorem. Remember that there are two basic limitations on thetraditional definition of credal nets, on which we have focused so far: the strong extension is assumed to have a finitenumber of extreme points; and the variables to take finitely many possible values. Both are related to the definition ofstrong extension and hence it is useful, and even necessary for the second question, to extend such a definition. Whatseems to be the natural way to extend it, in our view, is just adopting in its place the definition of strong product givenin Section 8.1.1. This choice allows us to propose, for the first time, a definition of credal nets for general spaces; moreover,it allows us, through Theorems 7 and 8, to immediately prove the coherence of credal nets in the general case (i.e., anykind of possibility spaces involved, and possibly infinitely many extreme points in the generalised strong extension), whichappears to be an important outcome.More generally speaking, we should like to point out that the approach used to prove the coherence of credal nets can bereplayed with opportune changes also in other important contexts. For example, some recent work [37] has again exploitedcoherence graphs together with the strong product to prove the coherence of some very general statistical models: it hasbeen shown that imprecise-probability based forms of statistical inference, such as some generalised parametric inferenceand pattern classification, are coherent. These models generalise Bayesian inference by using sets of priors to model priorknowledge (which can also correspond to a condition of near-ignorance) and use sets of likelihood functions to model veryflexibly the possible presence of a process responsible for missing values. Also in this case, coherence graphs are a key toolto prove in a relatively easy way the coherence of the mentioned models, despite their generality and complexity.A final point seems to be particularly worth of interest: both in the statistical case and with credal nets, the coherencegraphs naturally originated are only of type A1. Remember that A1 graphs lead to coherence irrespective of the numbers thatmake up the related lower previsions (provided that they are separately coherent); in other words, in the case of A1 graphscoherence is a structural component of the collection. With more general graphs this is not necessarily the case: the relatedcollection might no longer be coherent with very small changes in the numbers making up the lower previsions, in a waythat might make the check of coherence problematic due to numerical instabilities. Therefore, it is interesting to observethat some of the most commonly used models in artificial intelligence and statistics have naturally been selected with theproperty that their coherence is relatively insensitive to the mentioned instabilities. On the other hand, this confirms theimportance of A1 graphs.8.2. Compatibility of marginal and conditional probabilistic assessmentsThe results in this paper allow us to provide new insight and solutions to the problem of the compatibility of a collectionof marginal and conditional previsions. This problem has received a long-standing interest in the literature, since the seminalworks by Boole [6], Hoeffding [16], Fréchet [14] and Vorobev [32]. See [9] and the references therein for recent works inthe subject. It is related via Sklar’s theorem to the notion of copulas [28], which has applications in economics [25].Consider variables X1, . . . , Xn taking values in respective sets X1, . . . , Xn. Consider I j, O j ⊆ {1, . . . , n} such that I j ∩| X I j ) for j = 1, . . . , m. As we have mentioned before,:= ( Xi)i∈O j . If inO j = ∅, for j = 1, . . . , m, and conditional lower previsions P j( X O j:= ( Xi)i∈I j provides about the variable X O j| X I j ) models the information that the variable X I jP j( X O jparticular I j = ∅, then P j( X O j ) is simply the marginal information that we have about the variable X O j .We formulate the compatibility problem, in a very general way, as studying whether there is an extension P on X n of| X Im )}. In the case of marginalthe (marginal and conditional) lower previsions in a collection {P 1( X O 1lower previsions, this means obviously that P is an extension of them to the set of all gambles. The situation for conditionallower previsions is a bit more involved: we shall interpret compatibility of P and P ( X O | X I ) as coherence of these two lowerprevisions, in the sense considered throughout this paper (note that, as we have remarked in Section 2, in the particularcase of a conditional and an unconditional assessment weak and strong coherence are equivalent).| X I1 ), . . . , P m( X O mNote that the study we make for lower previsions is of course also valid for the particular case where the assessments are| X I j ), j = 1, . . . , m. In such a case, weprecise, that is, when we have linear conditional and unconditional previsions P j( X O jlook for a precise joint, that is, for a linear prevision which is compatible with all these assessments. Coherence implies nowthat (and, in the finite case, is equivalent to the fact) that the joint P induces all the conditionals by means of Bayes’ rulewhen conditioning on an event of positive probability (see for instance [2, Chapter 10]). The more traditional formulation ofthe problem is a special case of the previous formulation obtained by restricting the attention to marginal previsions only,and amounts to study whether there is a compatible joint P on X n whose O j -marginal is P ( X O j ), for j = 1, . . . , m.Our first, and important, result for the general compatibility problem is just Theorem 1 (see also Remark 1): from this| X Im ) are compatible if and only if they are weakly| X I1 ), . . ., P m( X O mtheorem, we know that the lower previsions P 1( X O 1E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144123coherent. This makes of weak coherence a very general characterisation of compatibility. We are not aware of any otherwork that has given such a characterisation in the general case that we consider, although a similar result has been givenin [20] for the particular situation made of unconditional linear previsions, and of finite sets X1, . . . , Xn.Theorem 1 is a key result also because it allows us to exploit in a direct way the outcomes from this paper, obtainedfor weak coherence, in the case of compatibility problems. For instance, we can use Proposition 4 to deduce that weak| X Im ) alone (i.e.,coherence, and hence compatibility, is implied by the separate coherence of P 1( X O 1without any other information about the numbers that make up the previsions) if and only if the coherence graph associatedto this collection template is A1.10 This means that if a collection of lower previsions is A1-representable, we immediatelyknow that they are compatible; this gives us a graphical criterion that is sufficient for compatibility. The criterion becomeseasier, and more obvious, if we focus on the simpler compatibility problem made only of unconditional lower previsions. Inthis case, the coherence graph cannot possess cycles; hence, it will be A1 if and only if there are no actual nodes with morethan one parent, which in turn means that the sets {O 1, . . . , O m} must be pairwise disjoint.| X I1 ), . . . , P m( X O mThe above graphical criteria may of course not be met in practice; in this case one should study the specific problem athand in order to be able to check compatibility. Yet, without going into the detail of the numbers that make up the lowerprevisions, one can still take advantage of coherence graphs to optimally decompose a compatibility problem into simplerones, i.e., those related to the subsets of the graph’s minimal partition. Actually, this is the situation in which coherencegraphs may prove to be more helpful, by having the potential to greatly reduce the complexity of the original task. Also, inour knowledge, this appears to be quite an original avenue for compatibility problems.Let us stress also that these results are very general compared to the ones established in the classical works for the com-patibility problem, because they can be applied to marginal and conditional lower previsions, instead of linear. Hence, theyare also valid in situations of ambiguous or scarce information, where the use a precise probability model is not possible(or adequate). Furthermore, in the case of infinite spaces X1, . . . , Xn, our work is based on finitely additive probabilities onthe class of all subsets of XI j ∪O j , for j = 1, . . . , m, which allows us to avoid topological and measurability assumptions onthe domains.8.3. Coherence-based probabilistic satisfiabilityThe compatibility problem is close to a different and well-known problem called probabilistic satisfiability [15,17]. A majordifference is that latter is usually defined only relative to variables with finite support (it is often defined only with respectto events). In the following we shall therefore make this assumption, too.Let us consider precise probabilities for a moment. In this case, probabilistic satisfiability consists in checking whether anumber of precise conditional and unconditional probabilities is consistent with a joint. This is usually done via algorithmsbased on linear programming. Probabilistic satisfiability is tightly related to probabilistic logic [26]. In fact, the above checkis often a first necessary step to be able to compute, again using linear programming, the probabilistic implications of theinitial assessments on new events, which is the goal of probabilistic logic.Probabilistic satisfiability has been extended to deal also with imprecise probabilistic assessments, and even with lowerand upper previsions in some recent work by Walley, Pelessoni and Vicig [34] (from now on we refer to this paper by WPVfor short). This work is somewhat atypical as it is based on a coherence-based view of probability rarely employed (anotherapproach in a similar spirit is [5]) in probabilistic logic. And yet the authors of that paper show that relying on coherenceis just the key to fix some of the problems of probabilistic logic and to develop truly general and powerful methods. Thesetwo characteristics make of WPV a natural candidate to bridge our results and probabilistic satisfiability.To this aim, in Section 8.3.1 we first consider WPV in some detail. In particular, we consider the specific notions ofconsistency used there, and give new results that make it easy to move back and forth from those notions and the moretraditional ones used in probabilistic logic. Moreover, we extend some of the theory for coherence graphs to show thatthey can decompose, in Section 8.3.2, some instances of the consistency problem of WPV into smaller ones. This is animportant result because probabilistic satisfiability is an NP-hard problem [5]; when coherence graphs allow for reducingone such problem into smaller ones, we immediately obtain the possibility to solve bigger problem instances than it waspossible before. (For an alternative, and possibly complementary approach, see [3]; in this case the focus is on heuristicconsiderations to reduce the computational burden.)8.3.1. Sufficient conditions for avoiding partial and uniform sure lossThe WPV work is made of a first part related to the satisfiability problem and a second one for the extension of theassessments to new events or variables. In the language of that paper, the first problem is one of checking whether theassessments avoid uniform loss; the second is the natural extension of the assessments.These notions are taken from the seminal work of Williams [35] about lower previsions. In the setup that we shallconsider, they are equivalent to the related ones from Walley [33, Section 7.1]. From this, it follows that avoiding uniform10 Moreover, we have proven that when the coherence graph of the collection template is of type A1, we not only deduce that these previsions are weaklycoherent, but also that they are (strongly) coherent. This allows us to give them a behavioural interpretation in terms of betting rates, as in the works byWalley [33] and de Finetti [13], and connects the compatibility problem naturally with decision making.124E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144loss is a weaker requirement than (strong) coherence. We recall that in the case of finite spaces of possibilities, whichwe are considering, avoiding uniform loss is equivalent to the notion of avoiding partial loss, introduced in Section 2. Theconclusion follows because coherence is indeed defined as a strengthening of avoiding partial loss.At first it may look surprising that WPV focuses on a consistency notion that, being weaker than coherence, leavesroom to inconsistencies in the original assessments. The reason is related to the second part of that work, i.e., the naturalextension. Walley shows that for the natural extension to be well defined, it is necessary that the original assessmentsavoid partial loss. Remember that the natural extension is a procedure that allows one to compute the (tight) probabilisticimplications of the original assessments on any new event or gamble, and in particular even on the original ones themselves.In this case such a procedure automatically corrects the possible inconsistencies left among the original assessments, makingthem (strongly) coherent. In WPV coherence is, in other words, a byproduct of the natural extension.WPV discusses at different points the relationship with former approaches, especially those based on probabilistic logic,pointing to some of their problems. Here the key to understand the difference between the approaches is in the differentnotion of loss used. Unlike WPV, which is based on avoiding uniform loss (i.e., avoiding partial loss), most of the otherapproaches are based on the weaker notion of avoiding uniform sure loss given in Definition 3. To make the connection withthe mentioned approaches more explicit, in the following proposition we give an equivalent formulation of avoiding uniformsure loss based on the existence of dominating conditional lower previsions which are weakly coherent.| X Im )} be a collection of separately coherent conditional lower previsions. If all theProposition 5. Let {P 1( X O 1spaces X1, . . . , Xn are finite, then they avoid uniform sure loss if and only if there exist weakly coherent dominating conditional linearprevisions {P 1( X O 1| X I1 ), . . . , P m( X O m| X I1 ), . . . , Pm( X O m| X Im )}.This proposition can be easily given an intuitive meaning. The existence of dominating linear previsions means thatthere must be precise probabilities in the feasible set determined by the constraints (on conditional and unconditionalprobabilities) that define a satisfiability problem; in other words, the feasible set must be non-empty. But this is not enough:the fact that they are weakly coherent means, through Theorem 1 and Remark 1, that there must be a joint mass functionfrom which one can derive, via Bayes’ rule and marginalisations, those precise probabilities. Stated differently, if we use anumber of conditional lower previsions as means to express bounds on conditional probabilities, the property of avoidinguniform sure loss is equivalent to the existence of a joint mass function that satisfies all these bounds. This should makeit clear that avoiding uniform sure loss is just the implicit condition that the more traditional approaches to probabilisticsatisfiability try to test (these questions are discussed at some length in [34, Section 2.4]).There is a further point of interest. It is possible to show that in a probabilistic satisfiability problem, avoiding uniformloss and avoiding uniform sure loss coincide if the (lower) probabilities of all the involved conditioning events are positive.This is related to the existence of the joint mass function mentioned above: in fact, since the joint mass function must berelated to the conditional assessments of the problem via Bayes’ rule, it turns out that there is no relation when Bayes’ rulecannot be applied, i.e., when the (lower) probability of a conditioning event is zero. It follows that avoiding uniform sureloss does not capture specific inconsistencies in the original assessments that arise on top of those zero probabilities (suchas in the example presented in the Introduction). This is, in particular, a source of criticism in WPV of the approaches basedon avoiding uniform sure loss (see [34, Section 3.7] for a discussion about this point). It is indeed a considerable featureof the WPV approach that it does not suffer for these kinds of inconsistencies, nor can draw wrong conclusions because ofthem.11At this point that we have some insight about the WPV approach and its relationship with ours and others, we can moveto work more closely on the relationship between coherence graphs and WPV. Our final aim is to be able to use coherencegraphs to simplify the check of avoiding uniform loss in WPV. The following result is what we need to reach our goal.Theorem 9. Let {P 1( X O 1their associated minimal partition, given by Definition 15.| X I1 ), . . . , P m( X O m| X Im )} be a collection of separately coherent conditional lower previsions, and let B be(1) If {P j( X O j| X I j )} j∈B avoid partial loss for any B ∈ B, then the conditional lower previsions {P 1( X O 1| X I1 ), . . . , P m( X O m| X Im )}avoid partial loss.(2) If {P j( X O jP m( X O m| X Im )} avoid uniform sure loss.| X I j )} j∈B avoid uniform sure loss for any B ∈ B, then the conditionallower previsions {P 1( X O 1| X I1 ), . . . ,This theorem states that the properties of coherence graphs that we have investigated in the case of weak and strongcoherence hold similarly also in the case of the losses under consideration. The similarity goes even further, as discussed inthe next remark.11 For instance, in [34, Section 3.8, Example 10] it is presented a case in which the mentioned inconsistencies lead traditional probabilistic logic to deducethat a given event is certain a posteriori whereas the methods in WPV more correctly deduce that there is complete ignorance about it, i.e., that theposterior probability for such an event lies in the interval [0, 1].E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144125Remark 3. In the particular case where the coherence graph is A1, it follows from Theorem 9 that we can deduce thatthe conditional lower previsions avoid partial or uniform sure loss just from their separate coherence; to see that beingA1 is also necessary for this property, it suffices to notice that the counterexamples in the proof of Theorem 4 are linearconditional previsions, for which weak coherence is equivalent to avoiding uniform sure loss and coherence is equivalent toavoiding partial loss. Hence, the partition is minimal in the case of A1 coherence graphs.For general (not necessarily A1) coherence graphs, the partition may not be minimal if we want to deduce the avoid-ing partial loss condition: if we consider the previsions in Example 2, it is not difficult to show that if P ( X2| X1) andP ( X2, X3| X1) avoid partial loss, then they also avoid partial loss with P ( X1). With respect to avoiding uniform sure loss,using again that the previsions in the counterexamples in the proof of Theorem 4 are linear conditional previsions, it followsthat the partition is the minimal one from which we can deduce the avoiding uniform sure loss condition.8.3.2. Bridging coherence graphs and probabilistic satisfiabilityWe can finally exploit coherence graphs in WPV. We focus in particular on using WPV to the extent of checking whether| X Im )} of separately coherent lower previsions avoids uniform loss. Recalling that wea collection {P 1( X O 1are considering finite spaces of possibilities, this can be re-phrased as using WPV to check whether a collection of closedand convex, conditional and unconditional, sets of mass functions, avoids uniform loss.| X I1 ), . . . , P m( X O mThis task can be particularly onerous for WPV. Let us recall that WPV checks that the assessments avoid uniform lossby running a linear program (or more than one). But probabilistic satisfiability is an NP-hard problem. This means thatin practice the size of the linear program, corresponding to [34, Eq. (1)], grows exponentially large with the size of theinput. In the setup that we consider, this is a consequence of the number of linear constraints in the program which growsaccording to the size of the joint possibility space X n, i.e., as an exponential function of n.Consider for example the collection of lower previsions that gives rise to the graph in Fig. 1. Say that each of the sixteenvariables under consideration takes values from a set with three elements. Then the number of constraints in the programis about 43 millions, quite a prohibitive one.But we know that we can apply Theorem 9 at this point. Theorem 9 allows us to decompose the above linear probleminto two much smaller ones, according to the superblocks in Fig. 5: one superblock of six variables and another of five.That is, in two linear problems with about 700 and 240 constraints, respectively. In this example, coherence graphs make itpossible to solve efficiently something that would be intractable otherwise.This is not an isolated case: every time a coherence graph allows some proper superblocks to be isolated, the size of thelargest linear program decreases according to an exponential function of the number of variables contained in the largestsuperblock: then, similarly to what happens also in the case of Bayesian nets, the computation is no longer exponentialin a global feature of the model, such as the number of variables, but in a local one, which has the potential to leadto efficient solutions in a number of real-world problems. There is also another advantage: using smaller linear programsreduces the risks originated by numerical instabilities: both those involved in using collections that are more general thanA1, as mentioned at the end of Section 8.1.2, and those more strictly related to the kinds of linear problems needed to checkavoiding uniform loss, as reported in [34].9. Some possible extensions of our resultsThis work is focused on the problem of the coherence of a collection of closed and convex sets of distributions. Theformalism that we have introduced is consequently based on some constant related features. One is that we work withvariables. Another one is that the joint space of possibilities X n for these variables is the product of the spaces of theindividual variables. A final one is that every lower prevision that we consider is defined on the set of all the gamblesrelative to the involved variables. Although this setup is the more general one for our aims, it is not the more general thatone could consider.In this section, we briefly investigate to what extent our results could be extended to more general setups. This couldbe useful in particular for problems of probabilistic satisfiability/logic. In fact, in such a field it is not uncommon to focuson problems where the lower previsions are not defined on all gambles; and also on problems where the joint spaceof possibilities X n is not a product space because there are so-called logical constraints between the possible values ofthe variables under consideration that make some of the joint values impossible. This holds for WPV but also for moretraditional approaches in artificial intelligence, e.g., [19,31], as well as other approaches based on coherent probabilities [7].Actually, the problems of probabilistic satisfiability are usually not even expressed in the language of previsions condi-tional on variables that we have considered in this paper, and are rather defined using events. Nevertheless, it is not difficultto consider variables that take values in the conditioning events in order to express everything in our language. Let us givean example of this:Example 3. Let us consider three events A, B and C, and assume that we are given the probabilities P ( A|B), P (C| A ∩B), P (B|C). Define the variables X1, X2, X3, where X1 takes values in {B, Bc}, X2 takes values in { A ∩ B, Ac ∩ B, A ∩Bc, Ac ∩ Bc} and X3 takes values in {C, C c}. In the language of this paper, the above assessments could be expressed asP ( X2| X1), P ( X3| X2) and P ( X1| X3), where these previsions are defined, respectively, on I A , IC and IB . In this case, theassociated coherence graph is as in Fig. 10. Since we have a cycle in the coherence graph, we can only know that these126E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Fig. 10. Coherence graph of {P ( X2| X1), P ( X3| X2), P ( X1| X3)}.assessments are not always coherent. Note however that if we were given the probabilities P ( A|B), P (C| A ∩ B) and P (B),they would be expressed in terms of the above variables as P ( X2| X1), P ( X3| X2) and P ( X1). Their associated coherencegraph is A1 and as a consequence they are always coherent.The following sections detail our investigation on the remaining problems. Section 9.1 deals with the case when the lower| X I j ), j = 1, . . . , m are defined on domains H j that are subsets of the sets K j of XO j∪I j -measurable gam-previsions P j( X O jbles. Section 9.2 discusses the problem of logical dependence. We also report on a further possible extension in Section 9.3that is concerned with the case when we know the coherence of some subsets of the collection of lower previsions, andthese subsets do not make up a partition as we have always assumed so far. Finally, we give some concluding remarks inSection 9.4.9.1. Considering smaller domains| X Ii )One of the assumptions we have made in this paper is that the domain of the conditional lower prevision P i( X O iis the whole set Ki of XO i ∪Ii -measurable gambles, i.e., the set of gambles which depend on the value that the variables| X Ii ) is a subset K(cid:12) i of Ki we can always extend it to Ki using theX O i , X Ii take. Although when the domain of P i( X O iprocedure of natural extension, we think it is also of interest to discuss in some detail this case.To simplify our reasoning, we are going to assume that K(cid:12) i is a linear subset of Ki . It is not difficult to extend ourreasoning to the case where K(cid:12) i is a non-linear set (for instance a finite set), using the work in [22].The first important thing to remark here is that the sufficient conditions we give in this paper for coherence, weakcoherence and avoiding partial and uniform sure loss in Theorems 2, 3 and 9 also hold if the domains are smaller. For this,it suffices to consider that the only assumptions in the domains we make in their proofs are that for any f ∈ K(cid:12) i and anysubset A of XIi , the gamble f I A is also in K(cid:12) i ; but this can be assumed without loss of generality as a consequence ofseparate coherence [33, Lemma 6.2.4].With respect to the necessary condition we establish in Theorem 4 for deducing weak coherence, it does not generalise toconditional lower previsions with arbitrary linear domains: if we think for instance of conditional lower previsions definedon constant gambles only, they are coherent as soon as they are separately coherent (i.e., as soon as they satisfy P i(μ| X Ii ) =μ for any constant μ ∈ R). This holds irrespective of their coherence graph, so this does not need to be A1.If we want Theorem 4 to hold in this more general situation, we need to impose some additional constraints on K(cid:12) i . Itcan be checked that all the results hold if for all i = 1, . . . , n, P i( X O i| X Ii ) satisfies the following two conditions:(SD1) For any subset A of XO i ∪Ii , its indicator function I A belongs to K(cid:12) i .(SD2) For all gambles f ∈ K(cid:12) i and all subsets A of XO i ∪Ii , the gamble f I A belongs to K(cid:12) i .Note that if P i( X O i| X Ii ) is separately coherent, we can assume without loss of generality that its domain K(cid:12) i includesall constant gambles. In that case, condition (SD1) is a consequence of (SD2). Note moreover that (SD2) cannot be seenas a consequence of separate coherence, because I A may not only depend on the conditioning variables, but also on theconditional ones. The proof of Theorem 4 implies also that when conditions (SD1) and (SD2) hold, the partition we defineis the minimal one which allows to deduce the property of avoiding uniform sure loss.9.2. Adding logical dependence considerationsAnother important issue for the applicability of our results is that of logical independence. In this paper, we have as-sumed that the variables X1, . . . , Xn are logically independent, meaning that we consider any combination (x1, . . . , xn) inX n as a possible value for the joint variable Xn. It is not uncommon, however, to consider variables that satisfy some kindof logical dependence assumption, which in the end will imply that some elements in X n are ‘structurally’ impossible valuesfor the joint variable ( X1, . . . , Xn) and therefore they should be removed from the joint possibility space.With respect to this, the first observation we have to make is that the partition of the conditional lower previsionsassociated to the superblocks is not enough to deduce weak or strong coherence (or avoiding partial or uniform sure loss)when we have in addition some logical dependence considerations; and this can happen even with finite spaces and preciseconditional previsions, as we show in the following example:Example 4. Consider two binary variables X1, X2, and take the previsions P ( X2), P ( X1) determined by P ( X2 = 1) = 1 =P ( X1 = 0). These previsions are coherent because their associated coherence graph is A1; however, they are incompatiblewith the logical dependence assumption X1 = X2, because any compatible model P will satisfy P ( X1 = 0, X2 = 1) = 1.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144127One way to model the fact that some of the joint values in X n are impossible is to use unconditional lower previsions,by giving upper probability zero to the impossible combinations. If we know for instance that the variables X1, X2 can onlyassume together values in the subset A of X{1,2}, we can make the assessment P ( A) = 1. This can be expressed by theunconditional lower prevision P ( X1, X2) on K{1,2} given by P ( f ) = infω∈ A f (ω). We discuss the use of this method togetherwith coherence graphs below. There is a cautionary note, however: strictly speaking, the fact that some joint values aregiven zero upper probability is in general only one implication of logical dependence, in the sense that the converse is notnecessarily true: an event with upper probability zero need not be regarded as ‘structurally’ impossible. In the followingwe shall refer to the case where some joint values are given upper probability zero as practical impossibility to distinguishit from logical dependence. The following discussion leads then to sufficient conditions to check coherence (or loss) notionsunder practical impossibility. When the focus in on logical dependence instead, and the difference between this and practicalimpossibility is enforced in applications, our results below should be regarded as necessary conditions. Whether these arealso sufficient conditions in general for logical dependence is an open problem that should be considered in future work.impossibility of some combinations of values as additional unconditionalAs we said, we can model the practical(cid:12)(cid:12)1, . . . , Plower previsions Pk. Then we should like to verify the coherence or weak coherence of the assessments(cid:12)(cid:12)P 1, . . . , P n, P1, . . . , Pk. For this, we can apply the reasoning in this paper and build the associated coherence graph. Itssuperblocks will produce a partition of the set of previsions with the property that it suffices to verify coherence (resp.,weak coherence, avoiding partial loss, avoiding uniform sure loss) within each of the elements of the partition to immedi-ately deduce coherence (resp., weak coherence, avoiding partial loss, avoiding uniform sure loss) of all the assessments.Note that in order to do this the first thing we must verify is that we are still working with a collection of previsions.(cid:12)( X1, X2) and we already have anotherIf a practical impossibility assumption is expressed in terms of an unconditional Passessment P ( X1, X2), then we should check whether also P ( X1, X2) satisfies such an assumption; if it does not, then ourassessments are not coherent. If it does, then we do not need to include P(cid:12)( X1, X2) in our set of assessments.Since we are working with more assessments now, the superblocks we have in the coherence graph will be bigger ingeneral, and therefore the associated partition will be coarser. In the worst of cases, if we have a practical impossibility(cid:12)( X1, . . . , Xn), and all the previsionsassumption that involves all the variables, it will be expressed as an unconditional Pwill belong to the same superblock. In that case the coherence graphs will not help to simplify the verification of coherence.The other extreme case will be that when the practical impossibility assumptions involve only the variables which arealready in the same superblock, as in the following example:Example 5. Assume that we have variables X1, . . . , X5, and that we make the assessments P 1( X1| X2), P 2( X2| X1), P 3( X5| X3)and P 4( X5| X4). Their associated coherence graph is given in Fig. 11. If we make now the assumptions X1 = X2 and X3 (cid:3)= X4,they would be included in the graph by means of the unconditional previsions P 5( X1, X2), P 6( X3, X4). The new coherencegraph would be as in Fig. 12. We see that P 1( X1| X2), P 2( X2| X1), P 3( X5| X3) and P 4( X5| X4) are coherent and compatiblewith the practical impossibility assumptions if P 1( X1| X2), P 2( X2| X1) are coherent and compatible with X1 = X2 on the onehand and P 3( X5| X3) and P 4( X5| X4) are coherent and compatible with X3 (cid:3)= X4 on the other.Hence, in some cases we only need to verify the practical impossibility assumptions within each of the superblocks, andwe should be able to deduce coherence. We could expect that in a number of cases there will be a situation intermediatebetween the two extreme ones just presented: the superblocks will grow by adding practical impossibility considerationsand the biggest one will not coincide with the entire graph. In these cases, coherence graphs will be useful as they will stillpermit to decompose the original problem in smaller ones to some degree.The situation is a bit simpler if we focus on weak coherence instead of coherence. Assume that we have a num-ber of weakly coherent conditional lower previsions, and that the practical impossibility considerations imply that onlythe values in A ⊆ X n are acceptable for the joint variable ( X1, . . . , Xn). It can be checked that the unconditional lowerprevision E defined in the proof of Theorem 1 is the smallest coherent lower prevision which is weakly coherent with| X Im ) [21]. If it also satisfies E( A) = 1, then we deduce that the assessments are also compatibleP 1( X O 1| X I1 ), . . . , P m( X O mFig. 11. Coherence graph of the collection {P 1( X1| X2), P 2( X2| X1), P 3( X5| X3), P 4( X5| X4)}.Fig. 12. The coherence graph in Fig. 11 modified so as to add considerations of practical impossibility.128E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144with the practical impossibility considerations, which is an alternative and easier sufficient condition to check. Using Propo-sition 5, it should be possible to simplify also the study of the compatibility of the avoiding uniform sure loss conditionwith practical impossibility considerations.9.3. Making non-disjoint assumptions of coherenceIn this paper, we have focused on the problem of deducing the coherence of a number of conditional lower previsionsfrom the coherence of the previsions that belong to some sets. In our formulation, we have always assumed that we aregiven a partition of the set of conditional lower previsions. We have proven that a tool for verifying coherence or weakcoherence can be to compare this partition to the minimal partition that we can derive using the superblocks of thecoherence graph.It would be interesting, however, to consider also the case where the information we are given is not made in termsof a partition, but that we are said that some sets of conditional lower previsions are coherent, and these sets are notdisjoint. For instance, we may consider the assessments P ( X1, X2), P ( X2| X1), P ( X1| X2), and we may be told that any twoof these assessments are coherent. That is, we know that P ( X1, X2), P ( X2| X1) are coherent, that P ( X1, X2), P ( X1| X2) arealso coherent (which implies then that the three assessments are weakly coherent) and also that P ( X2| X1), P ( X1| X2) arecoherent, and we would like to know if we can deduce from this the (joint) coherence of P ( X1, X2), P ( X2| X1), P ( X1| X2).Such a situation is considered for instance in [33, Section 7.9.1].One way of using our results would be to consider partitions which are finer than the information we are given, andto compare these with the minimal partition. This would provide us with a sufficient condition for deducing coherence.Nevertheless, this approach is not always fruitful: in the above example, we should not be able to derive coherence, eventhough it has been established in [33, Section 7.9.1]. A thorough study of this matter would be one of the main openproblems to consider in the future.9.4. Concluding remarksWe summarise here the three main outcomes of the previous sections.The first is a positive answer for the extension to problems based on smaller domains: coherence graphs can be used asbefore to decompose also those problems. What we lose in general here is the optimality of the decomposition in the caseof weak coherence, but this does not seem to be critical especially if we regard coherence (resp. avoiding partial loss) asthe consistency notion on which to focus rather than weak coherence (resp. avoiding uniform sure loss).The second outcome concerns what we have called statements of practical impossibility, which is a concept related tological dependence. In this case we can well include this kind of statements in a coherence graph, but we do not knowin general how much this will affect the topology of the graph, and hence the minimal partition. It is possible that thepartition stays the same (and even that the resulting graph is A1), that it grows, and in the worst case that it coincideswith the entire graph. Therefore in some cases coherence graphs will still prove to be useful also under considerations ofpractical impossibility; but the prospect to fully exploit them together with such considerations is an open problem at thistime.It is also an open problem to extend our results to the case when we know that some subsets of the assessments arecoherent and they do not form a partition.10. DiscussionCoherence can be regarded as the very essence of a theory of personal probability. But working directly with coherencecan be particularly onerous. The present paper is an attempt to deal with this difficulty in the case of Walley’s importantnotion of (strong) coherence, and to deliver tools that make easier to check it. We have been inspired in this by the lessonof graphical models, and have indeed defined a new graphical model called a coherence graph.Coherence graphs are means to render explicit the structure behind the notion of coherence. We have shown that such astructure induces a minimal partition of the available collection of lower previsions, with the characteristic that the coher-ence within each set of the partition implies the coherence of the overall collection. This result is very general: it holds forlower previsions and for any cardinality of the possibility spaces involved. In particular, since it holds for lower previsions,it is also applicable to determine the coherence of a collection of conditional linear previsions, and therefore is also usefulin the precise context. The generality of the results is also what has enabled us to apply them to problems as diverse asproving the coherence of Bayesian and credal networks, and decomposing problems of compatibility or probabilistic satis-fiability into smaller ones. On the other hand, such a generality has needed proofs that are somewhat long and technicallyinvolved. This is also due to the fact that graphs are naturally models of distributed computation and this clashes with theglobal nature of coherence. This nevertheless, the presented results are easy to exploit in practise, as we have provided apolynomial-time algorithm that computes the minimal partition of a coherence graph very efficiently.On a more theoretical level, our results appear to shed light on specific aspects of coherence, thanks especially to co-herence graphs of type A1. These graphs correspond to collections of separately coherent lower previsions that are coherentirrespective of the numerical values that make them up. They are related to the generalisation of the marginal extensionE. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144129theorem established in [23]: the relationship arises because from the A1 condition we can establish a total order on theconditional lower previsions in our collection template, and such an order is just what allows us to use the generalisedmarginal extension theorem. In this way, we have also given an easy graphical characterisation of the extent to which thetheorem can be applied: to A1-representable collection templates. Moreover, when the associated coherence graph is A1,the conditional lower previsions in the template are lower envelopes of coherent linear previsions. This does not hold forall collections of coherent conditional lower previsions, as is shown in [33, Section 6.6]. So it is remarkable that our resultslead naturally to a Bayesian sensitivity analysis interpretation of the collection of conditional lower previsions.Remember that we have shown that there are important conceptual differences between the notions of weak and strongcoherence proposed by Walley. Weak coherence is equivalent to the existence of a joint lower prevision that is coherentwith each of the assessments. In the particular case of conditional linear previsions and finite spaces, this is equivalentto the existence of a joint mass function inducing each of the conditionals by means of Bayes’ rule. The introduction ofthe notion of strong coherence is needed because some conditional lower previsions can have a common joint and still beclearly incoherent with one another. Remarkably, this happens even in the linear and finite case mentioned above.Taking this into account, we find it noteworthy that, for the problem tackled here, weak and strong coherence exhibita similar behaviour: if we have a number of assessments and all we know about them is that each of them is separatelycoherent, we can guarantee that they are weakly coherent exactly under the same conditions for which we can deduce theirjoint coherence: we just need the graph representing the collection template to be A1. More generally, we have establisheda partition of the graph for which weak coherence inside implies weak coherence of them all, and we have proven thatstrong coherence inside this partition also implies the strong coherence of all the assessments. It may be also useful torecall that completely analogous considerations hold when we consider loss notions, such as avoiding partial, or uniformsure, loss. We should also recall that there are differences: for example, we have shown that the minimal partition obtainedusing a coherence graph is indeed minimal in the case of weak coherence and not necessarily so for strong coherence.There are some important open problems related to this paper. One would be the possibility to fully exploit coherencegraphs under considerations of logical dependence, as we have proposed a partial solution to this problem. It seems to usthat to extend our proposal it will be necessary on the one hand to investigate its theoretical properties, and on the otherhand to specialise the models used in this paper. This could be done, for example, by strengthening the notion of collectiontemplate so as to insert some knowledge about the numbers that make up the lower previsions in the collection; or it couldbe done by focusing on coherence graphs with special topologies or dealing with variables taking values from finite sets(or even binary variables). These considerations could also simplify problems in probabilistic logic, different from those wehave considered in this paper. As another topic for future research in this respect, we suggest the study of the optimisationcompatibility problem [15,34], where one looks for the smallest joint which is compatible with a number of assessments. Wethink that the functional E defined in the proof of Theorem 1 should play an important role here.One possible extension we have not discussed yet would be to consider an infinite set of variables in our assessments;if we still have a finite number of conditional lower previsions, we think it should be possible to use coherence graphsto determine their coherence and their weak coherence, by making compact representations of the variables. The prob-lem is more complicated if we consider an infinite number of variables and assessments; in that case, we should first ofall generalise the coherence notions in [33] to an infinite number of assessments, and such a generalisation may not beimmediate.AcknowledgementsWe are grateful to Gert de Cooman for encouraging us to study the problems presented in this paper, and for manyhelpful comments. We should also like to thank Renato Pelessoni and Paolo Vicig for instructive discussion about theirjoint work with Peter Walley cited in the references, Reinhard Diestel and Gregory Gutin for drawing our attention on thestrong components as a way to simplify the algorithm to compute the minimal partition, and finally the referees for usefulsuggestions and comments. We acknowledge financial support by the MCYT project TSI2007-66706-C04-01, by the SwissNSF grants 200021-113820/1 and 200020-116674/1, and by the Hasler Foundation grant 2233.Appendix A. ProofsProof of Theorem 1. (⇒) This part of the proof is very similar to the one that Walley gives in [33, Theorem 8.1.8] forcoherence. For this reason, we only give a brief sketch of this part.| X I1 ), . . . , P m( X O m| X Im ) are weakly coherent. Let us define the lower prevision E on L(X n) by(cid:6)Assume that P 1( X O 1(cid:15)(cid:8)(cid:4)E( f ) := supα: ∃ f j ∈ K j, j = 1, . . . , m, s.t. supx∈X nG j( f j| X I j ) − ( f − α)(x) < 0.m(cid:5)j=1To see that E is well-defined, it suffices to note that sup f (cid:2) E( f ) (cid:2) inf f for any gamble f : given α > sup f , there are no| X Im );gambles f 1, . . . , fm satisfying the above equation or we contradict the weak coherence of P 1( X O 1and for any α < inf f we can take f 1 = · · · = fm = 0. It is also easy to see that E satisfies conditions (C1)–(C3), and as a| X Im ) are weakly coherent,consequence it is a coherent lower prevision. Let us show next that E, P 1( X O 1| X I1 ), . . . , P m( X O m| X I1 ), . . . , P m( X O m130E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144i.e., that they satisfy Eq. (3). Consider f ∈ L(X n), and gambles f j ∈ K j, j = 1, . . . , m. Consider (cid:10) > 0. Then the definition ofE implies that there are g j ∈ K j , j = 1, . . . , m, such that(cid:4)m(cid:5)j=1supx∈X n(cid:6)G j(g j| X I j ) − G( f ) − (cid:10)2< 0,where G( f ) = f − E( f ) (just consider α = E( f ) − (cid:10)2 in the definition of E). Hence, G( f ) >mj=1 G j(g j| X I j ) − (cid:10)2 .There are two possible cases in Eq. (3): that j0 ∈ {1, . . . , m} (case (a) below) or that it does not (case (b)).(cid:16)(a) Consider f 0 ∈ K j0 , z0 ∈ XI j0for some j0 in {1, . . . , m}. Then, using Eq. (A.1), the super-additivity of P j( X O jj = 1, . . . , m and the weak coherence of P 1( X O 1| X I1 ), . . . , P m( X O m(cid:6)(cid:4)| X Im ) we deduce that(A.1)| X I j ) forG j( f j| X I j ) + G( f ) − G j0 ( f 0|z0)(x)m(cid:5)G j( f j| X I j ) +m(cid:5)j=1G j(g j| X I j ) − (cid:10)2G j( f j + g j| X I j ) − (cid:10)2− G j0 ( f 0|z0)(cid:6)− G j0 ( f 0|z0)(x)(cid:6)(x) (cid:2) − (cid:10)2.supx∈X nm(cid:5)j=1(cid:4)(cid:2) supx∈X n(cid:2) supx∈X n(cid:4)j=1m(cid:5)j=1(cid:4)m(cid:5)j=1supx∈X nSince this holds for any (cid:10) > 0, we deduce that supx∈X n [(b) Take f 0 ∈ L(X n). Then, using Eq. (A.1), we see that(cid:6)G j( f j| X I j ) + G( f ) − G( f 0)(x) (cid:2) −(cid:10);(cid:16)mj=1 G j( f j| X I j ) + G( f ) − G j0 ( f 0|z0)](x) (cid:2) 0.otherwise, we should have(cid:4)m(cid:5)(cid:4)j=1m(cid:5)(cid:4)j=1m(cid:5)j=10 > supx∈X n(cid:2) supx∈X n(cid:2) supx∈X n(cid:6)G j( f j| X I j ) + G( f ) − G( f 0) + (cid:10)(x)G j( f j| X I j ) +m(cid:5)j=1G j(g j| X I j ) − G( f 0) + (cid:10)(cid:6)2(cid:6)(x)G j( f j + g j| X I j ) − G( f 0) + (cid:10)2(x),where the second inequality follows from Eq. (A.1), and the third from the super-additivity of the conditional lowerprevisions P j( X O j2 , which contradicts thedefinition of E. Since this holds for any (cid:10) > 0, we deduce that supx∈X n [| X I j ) for j = 1, . . . , m. But this means that we can raise the value E( f 0) by (cid:10)j=1 G j( f j| X I j ) + G( f ) − G( f 0)](x) (cid:2) 0.(cid:16)mHence, E, P 1( X O 1| X I j ) are weakly coherent. From [33, Section 6.5], we deduce that for any j = 1, . . . , m,| X Im ) are weakly coherent, and as a consequence, for any j = 1, . . . , m, E andf ∈ K j , and any x ∈ XI j ,| X I1 ), . . . , P m( X O mP j( X O jE(G j( f | X I j )) (cid:2) 0 and E(G j( f |x)) = 0.(⇐) Take f j ∈ K j, j = 1, . . . , m, f 0 ∈ K j0 , z0 ∈ XI j0for some j0 ∈ {1, . . . , m}. Take g j := G j( f j| X I j ), j = 1, . . . , m, g0 :=G j0 ( f 0|z0). Then we deduce from the assumption that P (g j) (cid:2) 0 for j = 1, . . . , m, and P (g0) = 0, whence g j = G j( f j| X I j ) (cid:2)G(g j) := g j − P (g j) for j = 1, . . . , m and g0 = G j0 ( f 0|z0) = G(g0) := g0 − P (g0). As a consequence,(cid:4)m(cid:5)j=1supx∈X n(cid:6)(cid:6)G j( f j| X I j ) − G j0 ( f 0|z0)(x) (cid:2) supx∈X nG(g j) − G(g0)(x) (cid:2) 0,(cid:4)m(cid:5)j=1where the second inequality follows from the coherence of P . We deduce that P 1( X O 1coherent.| X I1 ), . . . , P m( X O m| X Im ) are weaklyFor the second statement, let P 1( X O 1| X Im ) be weakly coherent conditional linear previsions. Use[33, Section 6.5.5] to deduce that any linear prevision P that dominates the coherent lower prevision P , that existsbecause of the first part of the theorem, will satisfy P (G j( f | X I j )) = 0 for any j = 1, . . . , m and any f ∈ K j , whenceP ( f ) = P (P j( f | X I j )). The converse implication follows trivially from the first part of the theorem. (cid:2)| X I1 ), . . . , Pm( X O mE. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144131Proof of Proposition 1. The direct implication is a consequence of Theorem 8.1.8 in [33], taking into account that we canalways include in this family an unconditional lower prevision P defined in the constant gambles. The converse implicationis trivial. (cid:2)Proof of Corollary 1. The direct implication has been established in the proof of Theorem 1. The converse implication istrivial. (cid:2)Before we prove Proposition 3, we are going to give three lemmas that show how we can constrain the probability ofany actual node in a block. The first two lemmas will be applied to blocks associated to a node with more than one parent,while Lemma 4 will be employed when dealing with blocks associated to a cycle. Lemma 2 will also be employed when weshow how the notion of weak coherence can be verified through smaller parts in Theorem 4.1, xiLemma 2. Let us consider xi2K1, . . . , Km by12 P j( f ) := f ((xi∈ Xi for i = 1, . . . , n. Define the previsions P 1( X O 11)i∈O j ) if I j = ∅, andif y = (xi(cid:8)P j( f | y) :=f ((xif ((xi1)i∈O j , y)2)i∈O j , y) otherwise,1)i∈I j| X I1 ), . . . , Pm( X O m| X Im ) with respective domainsif I j (cid:3)= ∅, for any j = 1, . . . , m, y ∈ XI j and f ∈ K j . Then the previsions P 1( X O 1| X I1 ), . . . , Pm( X O m| X Im ) are coherent.Proof. First of all, it follows immediately that these previsions satisfy conditions (SC1)–(SC3) in Section 2, and are thereforeseparately coherent. Moreover, since they are all linear, coherence is equivalent to avoiding partial loss (Eq. (2)). Hence, wemust prove that for any f j in K j ,mj=1 S j( f j) suchthatj = 1, . . . , m, not all of which are equal to 0, there exists some B in(cid:7)m(cid:5)j=1supx∈BG j( f j| X I j )(x) (cid:2) 0,(A.2)where, in order to simplify the notation, in the case where I j = ∅ we also use G j( f j| X I j ) to denote G j( f j).If there is some j ∈ {1, . . . , m} such that I j = ∅ and f j (cid:3)= 0, then S j( f j) = X n and Eq. (A.2) becomesm(cid:5)j=1supx∈X nG j( f j| X I j )(x) (cid:2) 0,and this condition holds trivially by considering x := (xi1)i=1,...,n, for which all the terms in the sum are equal to 0.Let us assume next that I j (cid:3)= ∅ for j = 1, . . . , m, i.e., that we are dealing with conditional linear previsions only. Notethat we can assume without loss of generality that f j (cid:2) 0 for all j = 1, . . . , m: otherwise, it suffices to consider for each j| X I j ) = G j( f j| X I j ).the gamble f(cid:12)j), where= f j − inf f j (cid:2) 0, which satisfies G j( fFor any j = 1, . . . , m, let us define the sets A j := π −1O j ∪I j(cid:3)(cid:13)(cid:3)(cid:2)( A(cid:3)(cid:2)(cid:2)(cid:12)j(cid:12)j(cid:12)jA:=(xi1)i∈O j∪I j∪(xi2)i∈O j×(cid:12)XI j\(xi1)i∈I jand the gambles g j := f jI A j , where I A j is the indicator function of A j . Since both f j and I A j are XO j∪I j -measurable, wededuce that g j belongs to K j . Moreover, given y ∈ XI j ,(cid:8)P j(g j| y) =g j((xig j((xiif y = (xi1)i∈O j , y)2)i∈O j , y) otherwise,1)i∈I jwhence P j(g j| X I j ) = P j( f j| X I j ). Since g j (cid:3) f j for any j = 1, . . . , m because f j (cid:2) 0, this implies that G j(g j| X I j ) (cid:3) G j( f j| X I j )j. Moreover, given y ∈ A j , we have that G j(g j| X I j )( y) = 0, and if y /∈ A j , G j(g j| X I j )( y) = −P j(g j|πI j ( y)) =for all−P j( f j|πI j ( y)) (cid:3) 0, taking into account that the gamble f j is non-negative by assumption.Now, if g j = 0 for all j = 1, . . . , m, then G j(g j| X I j )(x) = 0. Since G j( f j| X I j ) (cid:2) G j(g j| X I j ), we deduce that G j( f j| X I j )(x) (cid:2)j = 1, . . . , m, and considering any element in the union of the supports we seej = 1, . . . , m,0 for any x ∈ X n and for allthat Eq. (A.2) holds. Assume now that g j(cid:16)(cid:3)= 0 for some j ∈ {1, . . . , m}. Since (xi1)1(cid:2)i(cid:2)n ∈ A jfor all(cid:7)1)1(cid:2)i(cid:2)n = 0. Hence, if there is some B ∈mj=1 S j(g j) such that (xi1)1(cid:2)i(cid:2)n ∈ B, then Eq. (A.2) holds.mj=1 S j(g j), let j1 be the smallest integer such that g j1such that B1 = { y1} × X.I cj1(cid:3)= 0. Consider B1 ∈ S j1 (g j1 ). Thenmj=1 G j(g j| X I j )(xiIf (xi1)1(cid:2)i(cid:2)n /∈ B for any B ∈(cid:7)there is some y1 ∈ XI j112 We use here the one-to-one correspondence between gambles on XO j ∪I j and gambles in K j .132E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144m| X I j12)i∈I cj1Let z1 := { y1} × (xi1)i∈I j1)(z1) = 0. If, or we should have that (xi(cid:16)j=1 G j(g j| X I j )(z1) (cid:2) 0, we deduce that Eq. (A.2) holds. If)(z1) < 0. Since G j2 (g j22 otherwise. Note that y1 (cid:3)= (xixiand therefore G j1 (g j1then there is some j2 ∈ {1, . . . , m} such that G j2 (g j2we deduce the existence of some B 2 ∈ S j2 (g j2 ) such that z1 ∈ B2. For this B2, there is some y2 ∈ XI j2{ y2} × X1)i∈I j2whose ith component is the ith component of z1 if i ∈ I j2 , and is xiMoreover, for any i = 1, . . . , n, if the ith component of z1 is xiwe define the mapping h : X n → {0, . . . , n} such that h(z) is the number of i such that πi(z) is equal to xih(z2) (cid:2) h(z1). If we had h(z2) = h(z1), then it would be z2 = z1, whence z1 ∈ A j2 and G j2 (g j2Hence, it is h(z2) > h(z1).(cid:16)∈ B1. That is, the i-th component of z1 is the i-th component of y1 if i ∈ I j1 , and is equal to1)1(cid:2)i(cid:2)n ∈ B1, a contradiction. As a consequence, z1 ∈ A j1 ,j=1 G j(g j| X I j )(z1) < 0,)(z) = 0 for any z outside S j2 (g j2 ),such that B2 =denote the element of X n)(z2) = 0.| X I j22, then so is the ith component of z2 by definition; hence, if2, we have that)(z1) = 0, a contradiction., using the same reasoning as before. Let z2 := { y2} × (xi2 otherwise. Then z2 ∈ A j2 , whence G j2 (g j2. Note that y2 (cid:3)= (xi2)i∈I cj2| X I j2| X I j2| X I j2I cj2(cid:16)mmj=1 G j(g j| X I j )(z2) (cid:2) 0, we deduce that Eq. (A.2) holds. If this sum is negative, then there is some j3 such)(z2) < 0. We define then z3 as the element of X n whose ith component is the ith component of z2 if2 otherwise. Then it belongs to the same element of S j3 (g j3 ) than z2, and moreover h(z3) > h(z2).If we follow this procedure, we obtain a sequence of elements (zn) in the union of the supports of g j such that h(zn) >h(zn−1) for all n. But since 0 (cid:3) h(z) (cid:3) n for all z ∈ X n, this process must be finite. If we stop with some z for which h(z) < n,(cid:16)j=1 G j(g j| X I j )(z) (cid:2) 0. On the other hand, if we stop with zthis means that there is some z in(cid:7)such that h(z) = n, that is, with z = (ximj=1 S j(g j) and, since it belongs to(cid:16)j=1 G j(g j| X I j )(z) = 0. This showsmA j for all j, we havethat P 1( X O 1mj=1 S j(g j) such that2)1(cid:2)i(cid:2)n, this means that (xi2)1(cid:2)i(cid:2)n belongs to(cid:16)j=1 G j( f j| X I j )(z) (cid:2)mj=1 G j(g j| X I j )(z) = 0. As a consequence,| X Im ) are coherent. (cid:2)| X I1 ), . . . , Pm( X O m(cid:16)(cid:7)mmAgain, if| X I j3that G j3 (g j3i ∈ I j3 , and is xi∈ Xi for i = 1, . . . , n. Let us define the previsions P 1( X O 1| X I1 ), . . . , Pm−1( X O m−1| X Im−1 ) with respective1)i∈O j ) if I j = ∅, and(cid:3)= xiLemma 3. Take xi21domains K1, . . . , Km−1 by P j( f ) := f ((xi(cid:8)if y = (xif ((xif ((xiP j( f | y) :=1)i∈O j , y)2)i∈O j , y) otherwise,if I j (cid:3)= ∅ for any y ∈ XI j , f ∈ K j . Let Pm( X O mf ((xi1)i∈I j1)i∈O m , y) for any y ∈ XIm and f ∈ Km if Im (cid:3)= ∅. Then P 1( X O 1| X I1 ), . . . , Pm( X O m| X Im ) are weakly coherent.| X Im ) be given by P m( f ) := f ((xi1)i∈O m ) for any f ∈ Km if Im = ∅, and Pm( f | y) :=Proof. It is easy to see that these previsions satisfy conditions (SC1)–(SC3) in Section 2, and are therefore separately coher-ent. Since moreover they are all linear, they are weakly coherent if and only if for any f j ∈ K j , j = 1, . . . , m,m(cid:5)j=1supx∈X nG j( f j| X I j )(x) (cid:2) 0.Now, since G j( f j| X I j )( y) = 0 for y = (xiP 1( X O 1| X I1 ), . . . , Pm( X O m| X Im ) are weakly coherent. (cid:2)1)i=1,...,n, and for allj = 1, . . . , m, we deduce that Eq. (A.3) holds and therefore(A.3)Next, we show that the value of a parent of a dummy node in a cycle which is not in the cycle itself can also bedetermined by the previsions in the cycle:Lemma 4. Consider indices j1, . . . , j p satisfying Eq. (5), i.e., determining a cycle, and let k ∈ I j1weakly coherent previsions P 1( X O 1for i = 1, . . . , p satisfies P (x) = 1.| X I1 ), . . . , Pm( X O m| X Im ) such that any joint prevision P which is coherent with P ji ( X O ji\ O j p . Then for any x ∈ Xk there are| X I ji)Proof. Take (cid:7)i ∈ I jiconsider xi1(cid:3)= xi∩ O ji−1 for i = 1, . . . , p, where for simplicity of notation we make j0 := j p . For each i ∈ {1, . . . , n}, let us2 in Xi , and such that xk1:= x. Let us define P 1( X O 1| X I1 ), . . . , Pm( X O m| X Im ) with domains K1, . . . , Km by(cid:8)P j1 ( f | y) :=f ((xif ((xi2)i∈O j11)i∈O j1, y)(cid:7)11 , y (cid:3)= (xiif π(cid:7)1 ( y) = x1)i∈I j1, y) otherwisefor any y ∈ XI j1, f ∈ K j1 ,(cid:8)P js ( f | y) :=f ((xif ((xi1)i∈O js2)i∈O js, y)if π(cid:7)s ( y) = x(cid:7)s1, y) otherwiseE. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144133for any y ∈ XI jsand, f ∈ K js , s = 2, . . . , p and let us define, for j /∈ { j1, . . . , j p}, P j( f ) = f ((xi(cid:8)1)i∈O j ) for any f ∈ K j if I j = ∅,P j( f | y) :=f ((xif ((xiif y = (xi1)i∈O j , y)2)i∈O j , y) otherwise,1)i∈I jif I j (cid:3)= ∅ for any y ∈ XI j and any f ∈ K j .These previsions satisfy conditions (SC1)–(SC3) in Section 2, and are therefore separately coherent. Moreover, they arealso weakly coherent. For this, we have to show that for any f j ∈ K j , j = 1, . . . , m, Eq. (A.3) holds. But it is easy to see thatG j( f j| X I j )( y) = 0 for y = (xi1)i=1,...,n, and this for all j = 1, . . . , m.Next, we show that any joint P which is coherent with each of the conditional previsions P j1 ( X O j1| X I j1), . . . ,) and that the second is equal to 0. In order to prove this, we are going to use that for i = 2, . . . , p,}) = 1 for all i ∈ I j1 . We are going to show that the first of these terms isP j p ( X O j p(cid:12)| X I j p(cid:13)(cid:7)ix1(cid:12)(cid:12)(cid:13)1) = 1. For this, note first of all that for any i = 1, . . . , p,) must satisfy P (xk(cid:12)(cid:7)i| X I ji−1x1) = 1 for any i = 1, . . . , p. In particular,= 1,P ji−1= P+ P(cid:7)ix2(cid:7)ix2(cid:13)(cid:13)(cid:13)(cid:13)(cid:12)(cid:12)P+ PP ji−1(cid:7)ibecause P ji−1 (x1(cid:13)| X I ji−1| X I ji−1(cid:7)1(cid:7)11 , πi(z) ∈ {xiz: π(cid:7)1 (z) = xx2where the second equality holds because P ({xi1, xiequal to P ((xi(cid:7)i) + P ji−1 (x2(cid:12)(cid:2)| X I ji−1(cid:12)+ P1 = P= P(cid:7)1x1(cid:12)(cid:13)21)i∈I j1} ∀i ∈ I j1(cid:3)(cid:13)+ P(cid:12)(cid:13),(cid:7)1x21, xi2(cid:12)(cid:2)Pz ∈ X n: π(cid:7)i+1 (z) (cid:3)= π(cid:7)i (z)(cid:3)(cid:13)(cid:12)(cid:2)(cid:12)= P= P(cid:7)i(cid:7)i}, π(cid:7)i+1 (z) (cid:3)= π(cid:7)i (z)z: π(cid:7)i (z) ∈ {x1 , x2(cid:7)i(cid:7)iz: π(cid:7)i (z) ∈ {x1 , x2(cid:3)}, π(cid:7)i+1 (z) (cid:3)= π(cid:7)i (z)(cid:12)(cid:2)P ji(cid:3)(cid:13)(cid:13)(cid:12)(cid:12)(cid:13)P= P= 0,i = 2, . . . , p.where we are using the notation (cid:7)p+1 := (cid:7)1. Hence,(cid:7)i+1(cid:7)i+1(cid:7)i(cid:7)i1 , x2 , xxx12(cid:7)i(cid:7)i}) = 1, valid for i = 1, . . . , p,The equality P ({ximplies that P (x1 , x2(cid:7)i(cid:7)i(cid:7)21 , π(cid:7)i (z) ∈ {x3, . . . , p}). Take z such that π(cid:7)2 (z) = x},1 , x2(cid:7)i1 , π(cid:7)i+1 (z) = xin {2, . . . , p − 1} such that π(cid:7)i (z) = xsome i(cid:7)i(cid:7)21 ) = P ((x1 )i=2,...,p). A completely similar argument shows that P (xP (x(cid:7)1(cid:7)1(cid:7)12 ) = P (xNow, P (x2 , x2 , x(cid:13)(cid:13)(cid:12)(cid:7)p(cid:7)12 , xx1(cid:7)2(cid:7)1(cid:7)22 ) = P (P j1 (x2 , x2(cid:12)(cid:12)(cid:7)2(cid:7)12 , xx1)) = 0, whence P (x| X I j1(cid:13)(cid:12)(cid:7)ix1(cid:7)12 ,x= 0,= P= P(cid:3) P(cid:7)1x2P(cid:13)(cid:12)(cid:13)i=2,...,pP (xusing Eq. (A.4), and therefore P (x(cid:7)2(cid:7)1(cid:7)11 , z) = P (x2 ), because P (x1 , z, x(cid:12)(cid:13)(cid:12)(cid:13)(cid:12)(cid:7)11 , zx(cid:7)2(cid:7)11 , z, xx2= P= PP(cid:7)12 ) = 0. Consider on the other hand z ∈ XI j1(cid:7)2(cid:7)1(cid:7)1(cid:7)2)) = 0. Hence,1 ) = P (P j1 (x1 , z, x1 , z, x1(cid:12)(cid:12)(cid:7)ix2| X I j1(cid:13)(cid:7)p(cid:7)11 , xx2(cid:7)11 , z,x= 0,(cid:3) P(cid:13)(cid:13)i=2,...,p(cid:7)1again using Eq. (A.4). Hence, P ({z : π(cid:7)1 (z) = x1 , πi(z) ∈ {xiP ((xi). In particular, P (xk1) = 1. (cid:2)1)i∈I j1(cid:13)(cid:13)| X I ji= P (0) = 0,(A.4)(cid:7)i(cid:7)i(cid:7)2(cid:7)21 , π(cid:7)i (z) ∈ {x}, i =1 ) = P ({z: π(cid:7)2 (z) = x1 , x2(cid:7)ii = 3, . . . , p and such that z (cid:3)= (x1 )i=2,...,p . Then there is(cid:7)i+1, and Eq. (A.4) implies that P (z) = 0. We deduce that2(cid:7)i(cid:7)22 ) = P ((x2 )i=2,...,p).(cid:7)1(cid:7)2(cid:7)1(cid:7)22 ) = P (x1 ) + P (x2 , x2 , x(cid:7)21 ). As a consequence,\{(cid:7)1} different from (xi1)i∈I j1\{(cid:7)1}. Then1, xi2} ∀i ∈ I j1}) = P ((xi1)i∈I j1) and therefore 1 = P (x(cid:7)1(cid:7)12 ) =1 ) + P (xProof of Proposition 3. Let X(cid:7) be a source of contradiction. Then, because any source of contradiction has parents in thecoherence graph, there exists some i1 ∈ {1, . . . , m} such that (cid:7) ∈ O i1 . Apply Lemma 2 with x(cid:7):= x. Then we obtain:= x(cid:7)2) satisfies P (x) =| X Ii1a set of coherent conditional linear previsions, and any joint P which is coherent with P i1 ( X O i1P (P i1 (x| X Ii1Consider next a node X(cid:7) which is a predecessor of a source of contradiction, and x ∈ X(cid:7). We have the following possi-)) = P (1) = 1.1bilities:∩ O i p(cid:3)= ∅, and (cid:7) ∈ I i p . Apply Lemma 3 with m := i1 and x(cid:7)| X Ii1(1) Assume first of all that there is a successor Xs of the node X(cid:7) which has more than one parent. Let us consider a(cid:3)=constraining sub-block for X(cid:7) in this block, i.e., different i1, . . . , i p in {1, . . . , m} such that s ∈ O i1∅, . . . , I i p−1:= x; then any joint P which is coherent with)) =1) = P (P i2 (xs1) = P (P i1 (xs| X Ii1P i1 ( X O i1∩ O i3 . A similar reasoning allowsP (xs1, (xi1)i∈Ii21)i∈Ii21) = 1. Hence, in this case) = 1, and by following this procedure we also deduce that P (x(cid:7)us to show that P ((xiwe can define weakly coherent conditional previsions P 1, . . . , Pm such that any prevision P which is coherent withP i1 ( X O i1)) = 1. As a consequence, we have that 1 = P (xs1) = 1 for any t ∈ I i2) will satisfy P (xs), whence P ((xi) = 1, and in particular P (xt), . . . , P i p ( X O i p) satisfies P (x(cid:7)∩ O i2 , I i21) = 1.1)i∈Ii3∩ O i3| X Ii2| X Ii1| X Ii p111134E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144(2) Consider next the case where none of the successors of X(cid:7) has more than one parent. Then X(cid:7) must belong to a blockoriginated by a cycle. In particular, X(cid:7) is the predecessor of some node in an elementary cycle in the block. If X(cid:7) is aparent of a dummy node in the cycle but is not in the cycle itself, the result follows from Lemma 4.If neither of the previous possibilities holds, then there must be a path in the graph connecting X(cid:7) with a dummy nodein the elementary cycle. We consider then a constraining sub-block for X(cid:7) in the block associated to this cycle. Take(cid:3)= ∅, and k1, . . . , kr in {1, . . . , n} such∩ I j3then j1, . . . , j p in {1, . . . , m} such that O j1that {k1, . . . , kr−1} ⊆ {1, . . . , n} \ { j1, . . . , j p}, (cid:7) ∈ O k1(cid:3)= ∅, and kr = j1. Let us consider∩ Ikr∩ Ik2 , O k2ti ∈ O ki| X Im ) defined inTake xi1the proof of Lemma 4. Then these previsions are weakly coherent, and any prevision P which is coherent with| X I j1) = 1, becauseP j1 ( X O j1= I j1 , whence 1 = P (xtr −1) = 1, and intr−1 ∈ Ikr) = 1, and by1) = 1 for any t ∈ Ikr−1particular P (xtfollowing this procedure we also deduce that P (x(cid:7)1)i∈I j1) = P (P kr−1 (xtr −1), whence P ((xi∩ O kr−2 . A similar reasoning allows us to show that P ((xitr−1) = 1. As a consequence, we have that P (x1| X Ikr−11)i∈Ikr−1for i = 1, . . . , n, and let us consider the previsions P 1( X O 1∩ Iki+1 for i = 2, . . . , r − 1.(cid:3)= xi2∩ I j1(cid:3)= ∅, . . . , O kr−1(cid:3)= ∅, . . . , O j p∩ Ik3| X I1 ), . . . , Pm( X O m1)i∈Ikr−11)i∈Ikr−2), . . . , P j p ( X O j p) satisfies P ((xi)) = P (xtr −1(cid:3)= ∅, O j2| X I j p∩ I j2∈ Xi, (xi1111) = 1 for any P coherent withP k1 ( X O k1| X Ik1. . . ,P kr−1 ( X O kr−1),| X Ikr−1).P j1 ( X O j1| X I j1),. . . ,P j p ( X O j p| X I j p),This completes the proof. (cid:2)In order to simplify the proofs of Theorems 2 and 3, we are going to establish a couple of results (Lemmas 5 and 6)that will be common to these proofs.Lemma 5. Let us consider a non-empty subset J of {1, . . . , m}. Let B1 be a partition of J , and define, for each C ∈ B1, the set B C :=(cid:7){j∈C (I j ∪ O j)}. Then, if the sets {B C : C ∈ B1} are pairwise disjoint,13 the following statements hold:| X I j )} j∈C are coherent for all C ∈ B1, then the lower previsions {P j( X O j| X I j )} j∈C are weakly coherent for all C ∈ B1, then the lower previsions {P j( X O j| X I j )} j∈ J are coherent.| X I j )} j∈ J are weakly coherent.(1) If {P j( X O j(2) If {P j( X O jProof.(1) Consider f j ∈ K j for j ∈ J , f 0 ∈ K j0 , z0 ∈ XI j0for some j0 ∈ J . Assume that I j (cid:3)= ∅ for any j such that f j (cid:3)= 0; otherwise,the result follows from the second statement and the reduction theorem [33, Theorem 7.1.5]. Let C0 be the element ofB1 that includes j0. The coherence of {P j( X O jI cj0such that| X I j )} j∈C0 implies the existence of some D C0∈ ∪ j∈C0 S j( f j) ∪ {z0} × X(cid:17) (cid:5)(cid:18)G j( f j| X I j ) − G j0 ( f 0|z0)(x) (cid:2) 0;supx∈D C0j∈C0hence, for any (cid:10) > 0 there is some xC0(cid:18)G j( f j| X I j ) − G j0 ( f 0|z0)(cid:17) (cid:5)(xC0 ) (cid:2) −(cid:10).∈ D C0 such thatj∈C0On the other hand, for any C (cid:3)= C0 in B1, there is some D C ∈(cid:5)j∈Csupx∈D CG j( f j| X I j )(x) (cid:2) 0;(cid:7)j∈C S j( f j) such thathence, given (cid:10) > 0 there is some xC ∈ D C such that(cid:5)j∈CG j( f j| X I j )(xC ) (cid:2) −(cid:10).(A.5)(A.6)Let us consider now an element z ∈ X n satisfying πB C (z) = πB C (xC ) for any C ∈ B1; such an element exists because thesets {B C : C ∈ B1} are pairwise disjoint. Then we deduce from Eqs. (A.5) and (A.6) that(cid:17)(cid:5)(cid:18)G j( f j| X I j ) − G j0 ( f 0|z0)(z) (cid:2) −|B1|(cid:10),j∈ J13 This assumption means that given C1 (cid:3)= C2 ∈ B1, the subgraphs associated to B C1 and B C2 are not connected.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144135and moreover z ∈ D C for any C ∈ B1 (and in particular for one of them). Since we can do this for any (cid:10) > 0, we deducethat the conditional lower previsions {P j( X O j| X I j )} j∈ J are coherent.(2) Using a similar reasoning and the notations from the previous point, we deduce that for any (cid:10) > 0 there is some xC0such that(cid:17) (cid:5)j∈C0(cid:18)G j( f j| X I j ) − G j0 ( f 0|z0)(xC0 ) (cid:2) −(cid:10).On the other hand, for any C (cid:3)= C0 in B1, there is some xC such that(cid:5)j∈CG j( f j| X I j )(xC ) (cid:2) −(cid:10).Now, since the sets {B C : C ∈ B1} are pairwise disjoint, we deduce from these two equations that for any element z ofX n such that πB C (z) = πB C (xC ) for all C ∈ B1,(cid:17)(cid:5)(cid:18)G j( f j| X I j ) − G j0 ( f 0|z0)(z) (cid:2) −|B1|(cid:10).j∈ JAgain, since we can do this for any (cid:10) > 0, we deduce that the conditional lower previsions {P j( X O jcoherent. (cid:2)| X I j )} j∈ J are weakly(cid:7)| X Im )} be a separately coherent collection template whose graph is A1. Let I be a non-j=1 O j , and let x ∈ XI . Then for any f j ∈ K j, j = 1, . . . , m, j0 ∈ {1, . . . , m}, f 0 ∈mLemma 6. Let {P 1( X O 1empty subset of {1, . . . , n} which is disjoint withK j0 , z0 ∈ XI j0| X I1 ), . . . , P m( X O m(cid:6),(cid:4)m(cid:5)j=1supy∈π −1I(x)G j( f j| X O i ) − G j0 ( f 0|z0)( y) (cid:2) 0.(A.7)Proof. We are going to assume that the gamble G j0 ( f 0|z0) is not identically equal to zero; the result when G j0 ( f 0|z0) = 0k−1follows as a corollary. From Lemma 1, we can assume that O k ∩ (j=1 I j) = ∅ for any k = 1, . . . , m. For any such k, let us| X I∪Ik∪ Ak−1 ) on the class Hk of XI∪ Ak -measurable gamblesdefine the set Ak :=bys=1(O s ∪ I s), and the previsions Q( X O k(cid:7)k(cid:7)k(cid:13)f (x, ·)|πIk (x)Note that this is well-defined because (I ∪ Ak−1) ∩ O k = ∅.( f |x) := P kQ(cid:12).kk( X O kFor any k = 1, . . . , m, Q| X I1 ), . . . , P m( X O mfollows from the separate coherence of P 1( X O 1creasing sequence of variables. Applying the marginal extension theorem for variables in [23], we conclude that Qare coherent.14| X I∪Ik∪ Ak−1 ) satisfies conditions (SC1)–(SC3) and is, therefore, separately coherent. This| X Im ). On the other hand, they are conditional on an in-Consider f j ∈ K j, j = 1, . . . , m, j0 ∈ {1, . . . , m}, f 0 ∈ K j0 , z0 ∈ XI j0π −1j0−1j=1 G j( f j| X I j ) andh2 :=(x)belong to H j , it follows that g j ∈ H j for j = 1, . . . , j0 − 1. Moreover, any set E in the XI∪I j ∪ A j−1 -support of g j is included inπ −1(g j| X I∪I j ∪ A j−1 )( y) = f j( y) − P j( f j| X I j )( y). We deduce from the coherence. Let us define the gambles h1 :=(x) for j = 1, . . . , j0 − 1. Then, since both f j and IG j( f j| X I j ) − G j0 ( f 0|z0). Let us also define g j := f jI, . . . , Q1mmj= j0π −1(cid:16)(cid:16)IIIof Q(x). Besides, for any y ∈ π −1, . . . , QIthat there is some E ∈(cid:6)j0−11(x), g j( y) − Qj0−1j=1 S j(g j) such that(cid:7)j(cid:4)supy∈Ej0−1(cid:5)j=1g j − Q(g j| X I∪I j ∪ A j−1 )j( y) (cid:2) 0,whence(cid:4)j0−1(cid:5)(cid:6)f j − P j( f j| X I j )I(x)supy∈π −1h1( y) = supy∈π −1( y) = supy∈π −1j=1(x)Consider (cid:10) > 0, and let y1 ∈ E satisfy h1( y1) (cid:2) −(cid:10). Let E1 := π −1(it vanishes from the following equations) and E 1 := π −1(x)III∪ A j0(cid:4)j0−1(cid:5)j=1(cid:6)g j − Q(g j| X I∪I j ∪ A j−1 )j( y) (cid:2) 0.( y1). Note that if j0 = 1 then we simply have h1 = 0−1(x). There are two possibilities:I14 For this, note that the domain Hk of Qmeasurable gambles, and that the partitions on the conditioning side are increasingly finer.( X O kk| X I∪Ik∪ Ak−1 ), which is the set of XI∪ Ak -measurable gambles, is included in the set of XI∪Ik+1∪ Ak -136E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144• If E1 ∩ π −1I j0(z0) = ∅, then let us define g j := f jIE1 for j = j0, . . . , m. Then g j ∈ H j for allelement in the support of g j is included in E1. We deduce from the coherence of QE2 ∈ ∪mj0j = j0, . . . , m, and anythat there is some, . . . , Qmj= j0S j(g j) such that(cid:4)m(cid:5)(cid:6)supy∈E2whencej= j0(cid:4)m(cid:5)supy∈E1j=1g j − Q (g j| X I∪I j ∪ A j−1 )( y) (cid:2) 0,(A.8)(cid:6)f j − P j( f j| X I j ) − G j0 ( f 0|z0)( y) = supy∈E1(cid:4)m(cid:5)(cid:6)f j − P j( f j| X I j )j=1( y) = supy∈E1h1( y) + h2( y) (cid:2) −(cid:10),taking into account Eq. (A.8), that the gamble h1 is identically equal to h1( y1) on E1 and that sup y∈E1sup y∈E2h2( y) (cid:2) 0. As a consequence,h2( y) (cid:2)supy∈π −1I(x)h1( y) + h2( y) (cid:2) supy∈E1h1( y) + h2( y) (cid:2) −(cid:10),and since we can do this for any (cid:10) > 0, we deduce that Eq. (A.7) holds.• If E1 ∩π −1−1 (z1). Note that E2 := π −1(z0) (cid:3)= ∅, we consider z1 in this intersection, and let y2 := πI∪I j0I∪I j0I j0E1 ∩ π −1(z0). Let us consider the gambles g j := f jIE2 for j = j0, . . . , m. It follows from the coherence of QI j0that there is E3 ∈∪ A j0(cid:7)mj= j0S j(g j) ∪ π −1I∪I j0∪ A j0−1∪ A j0( y2) ⊆−1, . . . , Qmj0( y2) such that(cid:6)g j − Q (g j| X I∪I j ∪ A j−1 ) − G j0 (g0| y1)( y) (cid:2) 0;(A.9)note that it follows from the definition of the gambles g j, j = j0, . . . , m and of y1 that E3 ⊆ E2, and as a consequencealso E3 ⊆ E1 ⊆ π −1m(cid:5)(x). Hence,(cid:4)I(cid:4)m(cid:5)j= j0supy∈E3supy∈π −1I(x)(cid:2) supy∈E3= supy∈E3= supy∈E3(cid:6)f j − P j( f j| X I j ) − G j0 ( f 0|z0)( y)j=1(cid:4)m(cid:5)(cid:4)j=1m(cid:5)j=1(cid:6)f j − P j( f j| X I j ) − G j0 ( f 0|z0)( y)(cid:6)g j − Qj(g j| X I∪I j ∪ A j−1 ) − G j0 (g0| y1)( y)h1( y) + h2( y) (cid:2) −(cid:10),taking into account that the gamble h1 is identically equal to h1( y1) on E1, which is a superset of E3, and Eq. (A.9).Since we can do this for any (cid:10) > 0, we deduce that Eq. (A.7) holds.This completes the proof. (cid:2)| X Im )} be a separately coherent collection template whose graph is A1. Then, for anyCorollary 3. Let {P 1( X O 1f j ∈ K j, j = 1, . . . , m, j0 ∈ {1, . . . , m}, f 0 ∈ K j0 , z0 ∈ XI j0| X I1 ), . . . , P m( X O m,(cid:6)(cid:4)m(cid:5)supy∈Bfor some B ∈G j( f j| X O j ) − G j0 ( f 0|z0)( y) (cid:2) 0j=1(cid:7)mj=1 S j( f j) ∪ π −1I j0(z0).Proof. The result follows by using the same reasoning as in the previous proof, now with I = ∅. The main difference is thatnow π −1(x) = X n, and we should have g j = f j for j = 1, . . . , j0 − 1. By repeating the same arguments we conclude that(cid:4)(cid:6)Im(cid:5)supy∈E2j=1G j( f j| X O j ) − G j0 ( f 0|z0)( y) (cid:2) 0for some E2 included in some B ∈(cid:7)mj=1 S j( f j) ∪ π −1I j0(z0). (cid:2)E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144137Proof of Theorem 2. Let us consider gambles f j ∈ K j for j = 1, . . . , m,show that(cid:6)j0 ∈ {1, . . . , m}, f j0∈ K j0 and z0 ∈ XI j0, and let usG j( f j| X O j ) − G j0 ( f 0|z0)(x) (cid:2) 0(A.10)(cid:4)m(cid:5)j=1(cid:7)supx∈Efor some E ∈mj=1 S j( f j) ∪ π −1I j0(z0).Let B1 be the subset of B given by the sets C with |C| > 1, i.e., those associated to superblocks. Assume that B1 (cid:3)= ∅; ifB1 = ∅, then the result follows from Corollary 3. For any C ∈ B1 let us consider the set B C :=j∈C (I j ∪ O j). Then it followsfrom the definition of the superblocks that the sets {B C : C ∈ B1} are pairwise disjoint subsets of {1, . . . , n}. Let us definej∈ J c G j( f j| X I j ) − I J c ( j0)G j0 ( f 0|z0).J :=j∈ J S j( f j) ∪j∈ J G j( f j| X I j ) − I J ( j0)G j0 ( f 0|z0) and h2 :=| X I j )} j∈ J are coherent. As a consequence, there is some E ∈From Lemma 5, the previsions {P j( X O jC . Define the gambles h1 :=C∈B1(cid:16)(cid:16)(cid:7)(cid:7)(cid:7)I J ( j0)π −1I j0(z0) such thath1(x) (cid:2) 0.supx∈EConsider (cid:10) > 0, and let x1 ∈ E satisfy h1(x1) (cid:2) −(cid:10). Let us consider I :=h1 is XI -measurable, and that π −1( y1) is included in E because I j ⊆ I for any j ∈ J .C∈B1The coherence graph of the previsions {P j( X O j| X I j )} j∈ J c is A1 because they do not belong to any superblock. Moreover,for any j ∈ J c , I ∩ O j = ∅, because of the comments at the end of Section 5. Hence, we can apply Lemma 6 to deduce that(cid:7)B C and y1 := πI (x1). Note that the gambleI(cid:5)supx∈π −1h2(x) = supx∈π −1I( y1)( y1)Since h1 is identically equal to h1(x1) (cid:2) −(cid:10) on π −1j∈ J cIG j( f j| X I j ) − I J c ( j0)G j0 ( f 0|z0)(x) (cid:2) 0.( y1) h2(x) (cid:2) −(cid:10), and( y1) is included in E we deduce that supx∈E h1(x) + h2(x) (cid:2) −(cid:10). Since we can make a similar reasoning for any( y1) h1(x) + h2(x) (cid:2) −(cid:10) + supx∈π −1( y1), supx∈π −1IIIbecause π −1(cid:10) > 0, we deduce that Eq. (A.10) holds. (cid:2)IProof of Theorem 3. We proceed in the same way as in the proof of Theorem 2. The main difference is that, instead ofEq. (A.10), we now only need to prove that(cid:4)m(cid:5)j=1supx∈X n(cid:6)G j( f j| X I j ) − G j0 ( f 0|z0)(x) (cid:2) 0(A.11)for any f j ∈ K j, j = 1, . . . , m, j0 ∈ {1, . . . , m}, f 0 ∈ K j0 and z0 ∈ XI j0.Using the notations from the previous proof, if B1 = ∅, then the result follows from Corollary 3. Assume then thatB1 (cid:3)= ∅. We deduce from Lemma 5 that the previsions {P j( X O j| X I j )} j∈ J are weakly coherent. As a consequence, for any(cid:10) > 0 there exists some x1 in X n such that h1(x1) (cid:2) −(cid:10). Let y1 := πI (x1) ∈ XI . Now, since the coherence graph of theprevisions {P j( X O j| X I j )} j∈ J c is A1 because they do not belong to any superblock, we can apply Lemma 6 to deduce thatsupx∈π −1I( y1)h2(x) = supx∈π −1( y1)j∈ J cI(cid:5)G j( f j| X I j ) − I J c ( j0)G j0 ( f 0|z0)(x) (cid:2) 0.h1 is identically equal to h1(x1) (cid:2) −(cid:10) on π −1Iwe can make a similar reasoning for any (cid:10) > 0, we deduce that Eq. (A.11) holds. (cid:2)( y1), whence supx∈π −1( y1) h1(x) + h2(x) (cid:2) −(cid:10) + supx∈π −1II( y1) h2(x) (cid:2) −(cid:10). SinceProof of Theorem 4. The sufficiency part is a consequence of Theorem 3. Let us show the necessity part.(cid:12) ∈ B(cid:12)Assume ex absurdo that B is not finer than B(cid:12). Then there exists some B ∈ B with non-empty intersection with more. It follows that B must be the set of indices of the lower previsions in a superblock of the coherence graph:than one Botherwise, the cardinality of B would be 1. We shall identify the subset B of {1, . . . , m} with the corresponding superblockin the coherence graph. Then B is the union of a finite number of different blocks, B Z1 , . . . , B Zk , which are originated bydifferent sources of contradiction. We shall denote by J i the set of indexes of the lower previsions represented in the blockB Z i , for i = 1, . . . , k.One of the following possibilities must hold:(cid:12)2 in B(cid:12)(a) There exists some i ∈ {1, . . . , k} such that J i has non-empty intersections with different B1, B(cid:12)⊆ B(b) Condition (a) does not hold and there are i1 (cid:3)= i2 in {1, . . . , k} such that J i12 for different B(cid:12)1, J i2⊆ B.(cid:12)(cid:12)1, B(cid:12)2 in B(cid:12).In case (a), we have two possibilities: that the block associated to J i(points (a1) and (a2) below), or that it corresponds to a cycle (points (a3) and (a4) below).is related to a node with more than one parent138E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144We are going to prove that in any of these cases there is a collection of separately coherent conditional linear previsions| X I j )} j∈B(cid:12) are weakly| X Im )} that are not weakly coherent and such that for any B| X I1 ), . . . , P m( X O m, {P j( X O jin B(cid:12)(cid:12){P 1( X O 1coherent.(a) Assume that J i has non-empty intersections with different B. Then one of the following possibilities must(cid:12)1, B(cid:12)2 in B(cid:12)(cid:12)1(cid:3)= B(cid:12)2. Consider yshold:(a1) There exists an actual node Xs in B Z i with two or more parents, corresponding to lower previsions in different∩ O i2 , and(see Fig. 13). Formally, this holds if and only if there are i1 (cid:3)= i2 ∈ J i such that s ∈ O i1elements of B(cid:12)(cid:12)(cid:12)i1 ∈ B(cid:3)= ys1, i2 ∈ B2 for BDefine coherent (and in particular weakly coherent) conditional previsions P 1( X O 1means of Lemma 2 such that P i1 ( ys1herent with P i1 ( X O i1to define coherent P| X Im ) by) is identically equal to 1, whence any joint P that is weakly co-:= ys:= ys1). Apply Lemma 2 again with xs2(cid:12)( ys) is the constant gamble on 1, whence2i22) = 1. Define now the conditional previsions| X Ii1) satisfies P ( ys| X I1 ), . . . , Pany joint P weakly coherent with PQ 1( X O 1| X Im ) such that P| X Ii2| X I1 ), . . . , Q m( X O m| X I1 ), . . . , Pm( X O m(cid:12)m( X O m(cid:12)( X O i2i21) = 1 (use xs) satisfies P ( ys| X Ii1(cid:12)1( X O 12 in Xs.:= xs2:= xs2| X Ii2111Q j( X O j| X I j ) :=(cid:8)| X Im ) by| X I j )| X I j ) otherwise.if j ∈ BP j( X O j(cid:12)j( X O jP(cid:12)1| X I j )} j∈B(cid:12) areThese previsions are separately coherent and moreover for any Bweakly coherent. Given any coherent prevision P on L(X n) such that P ( f ) = P (Q j( f | X I j )) for any f ∈ K j2) =and for any j = 1, . . . , m, we should have P ( ys2) = 1, a contradiction. Using Theorem 1, we deduce thatP (Q i2 ( ys2Q 1( X O 11) = P ( ys| X Im ) are not weakly coherent.| X Ii2| X I1 ), . . . , Q m( X O m)) = 1 on the one hand, and P ( ys)) = 1 on the other; hence, P ( ysthe previsions {Q j( X O j1) = P (Q i1 ( ys| X Ii11(cid:12) ∈ B(cid:12)(a2) Assume that for any node Xs in the block B Z i with more than one parent all the previsions corresponding toits parents belong to the same element of B(cid:12). Then there must be an actual node X(cid:7) in the graph which is apredecessor of a node Xs with more than one parent and such that the previsions in a constraining sub-block forX(cid:7) in the block associated to Xs belong to the same element B, and there is another arc pointing at X(cid:7)whose associated prevision belongs to some BFormally, this holds if and only if there are i1, . . . , i p, i p+1 in J i such that s ∈ O i1(cid:3)= ∅, I i pO i pConsider x(cid:7)1joint P weakly coherent with {P i j ( X O i j(cid:12)1( X O 1PDefine Q 1( X O 1∩ O i p+1 , and {i1, . . . , i p} ∈ B2 in X(cid:7). Use Proposition 3 to define weakly coherent P 1( X O 1| X Im ) such that any1) = 1. Apply now Lemma 2 to define coherent| X Im ) such that any joint P coherent with P)} j=1,...,p satisfies P (x(cid:7). See Fig. 14 for an example.| X I1 ), . . . , Pm( X O m∩ O i p+1(cid:3)= x(cid:7)(cid:3)= ∅, . . . , I i p−1) satisfies P (x(cid:7)(cid:12)1, i p+1 ∈ B| X I1 ), . . . , P(cid:3)= ∅, (cid:7) ∈ I i p∩ O i2 , I i2| X Im ) by1 of B(cid:12)1 in B(cid:12)(cid:12)m( X O m( X O i p+1| X Ii p+12) = 1.∩ O i3| X Ii j(cid:12)i p+1(cid:3)= B(cid:12)2.∩(cid:12)2(cid:12)(cid:12)| X I1 ), . . . , Q m( X O m(cid:8)Q j( X O j| X I j ) :=P j( X O j(cid:12)j( X O jPif j ∈ B| X I j )| X I j ) otherwise.(cid:12)1| X I j )} j∈B(cid:12) areThen these previsions are separately coherent and moreover for any Bweakly coherent. Now, given any coherent prevision P on L(X n) s.t. P ( f ) = P (Q j( f | X I j )) for any f ∈ K j andwe have that {Q j( X O j(cid:12) ∈ B(cid:12)Fig. 13. An example of the graphical situation considered in point (a1). In this case X7 has two parents in different sets of B(cid:12), and we can create acontradiction on it by inducing two different marginals through the two different parent previsions. We use the dashed line to denote that one of theparent previsions is in an element of B(cid:12)and the other is in another element.Fig. 14. An example of the graphical situation considered in point (a2). In this case we can create a contradiction on X1, (i.e., X(cid:7)) by inducing two differentmarginals, one from its parent and the other one from X3 through the constraining sub-block connecting X1 and X3 via X8 (i.e., Xs ). We use the dashedline to denote that the prevision having X1 in the conditional side does not belong to the constraining sub-block.E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144139Fig. 15. An example of the graphical situation considered in point (a3). In this case we can create a contradiction because the parent of X14 is in a differentelement of B(cid:12)compared to the previous dummy node in the elementary cycle, as denoted by the dashed lines.for any j = 1, . . . , m, we should have P (x(cid:7)conditional previsions Q 1( X O 1| X I1 ), . . . , Q m( X O m1) = 1 = P (x(cid:7)| X Im ) are not weakly coherent.2), a contradiction. Using Theorem 1, we deduce that the(cid:12)1 and B(a3) Both (a1) and (a2) do not hold. This means that the block B Z i , whose set of indices J i has non-empty intersections(cid:12)2, is not generated by an actual node with more than one parent, but by a cycle. Take anywith different Belementary cycle in such a cycle.Assume first that not all the dummy nodes in the elementary cycle belong to the same element of B(cid:12)that there are two adjacent dummy nodes in the elementary cycle that belong to different elements of B(cid:12)Fig. 15 for an example).That is, assume the existence of different j1, . . . , j p in J i such that O j1(cid:12)and such that j1 ∈ B2. Take (cid:7)i ∈ I jij0 := j p .Consider xi11, yiDefine then yi(cid:8)∈ Xi for i = 1, . . . , n, and apply Lemma 2 to define coherent P 1( X O 12 in Xi for i = 1, . . . , n by(cid:8)if i = (cid:7)1xi2xi1 otherwise,(cid:3)= ∅∩ O ji−1 for i = 1, . . . , p, where for simplicity of notation we makeif i = (cid:7)1xi1xi2 otherwise.. This implies(see| X I1 ), . . . , Pm( X O m(cid:3)= ∅, . . . , O j p(cid:12)1, j2 ∈ B(cid:3)= ∅, O j2| X Im ).:=yi1(cid:3)= xi2∩ I j2∩ I j1∩ I j3yi2:=Apply again Lemma 2 with these values to define coherent conditional previsions Pand define then the conditional previsions Q 1( X O 1| X I1 ), . . . , Q m( X O m| X Im ) by(cid:12)1( X O 1| X I1 ), . . . , P(cid:12)m( X O m| X Im ),Q j( X O j| X I j ) :=if j ∈ B| X I j )| X I j ) otherwise.(cid:12)1(cid:8)P j( X O j(cid:12)j( X O jP| X I j1Let us show that Q j1 ( X O j1Then, from Theorem 1, there is a coherent prevision P that is coherent with Q j1 , . . . , Q j p . For any i = 1, . . . , p,(cid:12)) are not weakly coherent: assume ex-absurdo that they are.), . . . , Q j p ( X O j p(cid:13)(cid:13)(cid:13)(cid:13)(cid:12)(cid:12)(cid:13)(cid:12)(cid:12)(cid:12)(cid:13)| X I j pP(cid:7)ix1+ P(cid:7)ix2= PQ ji−1(cid:7)ix1| X I ji−1+ PQ ji−1(cid:7)ix2| X I ji−1= 1,(cid:7)ibecause P ji−1 (x1(cid:7)i| X I ji−1Q ji−1 (x1(cid:12)(cid:2)| X I ji−1(cid:7)i) + Q ji−1 (x2) = P| X I ji−1(cid:7)i) + P ji−1 (x| X I ji−12) = 1 for any i = 1, . . . , p. Hence, for i = 2, . . . , p,| X I ji−1(cid:3)(cid:13)(cid:3)(cid:13)| X I ji−1) + P(cid:7)i(x2(cid:7)i(x1(cid:12)ji−1(cid:12)ji−1(cid:12)(cid:2)(cid:3)(cid:2)Pz ∈ X n: π(cid:7)i+1 (z) (cid:3)= π(cid:7)i (z)= P= P(cid:12)z: π(cid:7)i (z) ∈(cid:12)(cid:2)Q jiz: π(cid:7)i (z) ∈(cid:7)i(cid:7)i1 , xx2(cid:2)(cid:7)i(cid:7)i1 , xx2, π(cid:7)i+1 (z) (cid:3)= π(cid:7)i (z)(cid:3)(cid:3), π(cid:7)i+1 (z) (cid:3)= π(cid:7)i (z)(cid:13)(cid:13)| X I ji= P (0) = 0,) = 1 for any i = 1, . . . , p, whencewhere we are using the notation (cid:7)p+1 := (cid:7)1. Hence,(cid:12)(cid:13)(cid:12)(cid:13)P= 0= P(cid:7)i+1(cid:7)i2 , xx1(cid:7)i(cid:7)i2 ) = 1, valid for i = 1, . . . , p,1 ) + P (x(cid:7)i+1(cid:7)i1 , xx2for i = 2, . . . , p.(cid:7)2(cid:7)21 , π(cid:7)i (z) ∈1 ) = P ({z: π(cid:7)2 (z) = xThe equality P (x(cid:7)i(cid:7)i(cid:7)i(cid:7)i(cid:7)i(cid:7)2{x1 , π(cid:7)i (z) ∈ {x}, i = 1, . . . , p}). Take z such that π(cid:7)2 (z) = x}, i = 1, . . . , p}, and such that z (cid:3)= (x1 , x1 , x1 )i=1,...,p .22(cid:7)i+1(cid:7)iThen there is some i in {2, . . . , p} such that π(cid:7)i (z) = x1 , π(cid:7)i+1 (z) = x, and Eq. (A.12) implies that P (z) = 0. We2(cid:7)i(cid:7)i(cid:7)2(cid:7)21 ) = P ((x2 ) = P ((x1 )i=1,...,p). A completely similar argument shows that P (xdeduce that P (x2 )i=1,...,p).(cid:7)i(cid:7)2(cid:7)1(cid:7)2(cid:7)2)) = 0, and as a consequence P (x1 )i=1,...,p) = 0. A completely1 ) = P ((x1 ) = P (Q j1 (x1 , xNow, P (x1(cid:7)i(cid:7)2(cid:7)22 )i=2,...,p) = 0. But this implies that 1 =2 ) = P ((x2 ) = 0, whence P (x) are not weakly coherent.), . . . , Q j p ( X O j p| X I j1(a4) Assume next that all the dummy nodes in the elementary cycle chosen in the previous point belong to the same(cid:12)2 (see Fig. 16 for(cid:12)1, but that there is a predecessor X(cid:7) of one of these dummy nodes which belongs to another Bsimilar argument shows that P (xP (x(cid:7)2(cid:7)22 ) = 0, a contradiction. Hence, Q j1 ( X O j11 ) + P (ximplies that P (x| X I j1(cid:7)12 , x| X I j p(cid:7)11 , x(A.12)Ban example).140E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144Fig. 16. An example of the graphical situation considered in point (a4). In this case all the dummy nodes in the elementary cycle belong to the same elementof B(cid:12), but a dummy predecessor of the elementary cycle belongs to a different element, as denoted by the dashed line. We create then a contradiction onXl = X6 by inducing a marginal from its parent that is different from the marginal induced through the elementary cycle.Fig. 17. An example of the graphical situation considered in point (b). In this case we create a contradiction on Xl = X3, which is a node shared by twoadjacent blocks, as denoted by the dashed lines. The contradiction is obtained by inducing a marginal from one block and a different marginal from theother block.∩ Ik3∩ I j2∩ I j3(cid:3)= ∅, O j2∩ Ik2 , O k2(cid:3)= ∅, . . . , O j p(cid:3)= ∅, and∩ I j1j1, . . . , j p in {1, . . . , m} such that O j1Consider then differentin {1, . . . , m} such thatassume that X(cid:7) is a predecessor of the dummy node j1. Then there are k1, . . . , kr{k1, . . . , kr−1} ⊆ {1, . . . , m} \ { j1, . . . , j p}, (cid:7) ∈ O k1(cid:3)= ∅, and kr = j1. That is, the in-dices {k2, . . . , kr−1, j1, . . . , j p} determine a constraining sub-block for X(cid:7) in the block associated to the elementarycycle.Assume that j1, . . . , j p, k2, . . . , kr−1 ∈ B(cid:3)= Btion 3 to define weakly coherent conditional previsions P 1( X O 1| X I j ) for j ∈ { j1, . . . , j p, k2, . . . , kr−1} satisfies P (x(cid:7)with P j( X O j(cid:12)(cid:12)m( X O m1( X O 1herent conditional lower previsions P2) = 1. Let us consider then Q 1( X O 1P(cid:12)12 in X(cid:7). Then we can use Proposi-| X Im ) such that any P coherent1) = 1. Similarly, we can use Lemma 2 to define co-| X Im ) such that any joint P which is coherent with| X I1 ), . . . , Q m( X O m(cid:12)1. Take x(cid:7)| X I1 ), . . . , Pm( X O m| X Im ) defined by(cid:12)1 and k1 ∈ B(cid:3)= ∅, . . . , O kr−1) satisfies P (x(cid:7)| X I1 ), . . . , P(cid:12)2 for B( X O k1| X Ik1∩ Ikr(cid:3)= x(cid:7)(cid:12)k1(cid:8)(cid:12)21Q j( X O j| X I j ) :=P j( X O j(cid:12)j( X O jPif j ∈ B| X I j )| X I j ) otherwise;then it follows that for any B ∈ B(cid:12)prevision P which is coherent with each of them should satisfy P (x(cid:7)| X Im ) are not weakly coherent.other. Hence, Q 1( X O 1the previsions {Q j( X O j| X I1 ), . . . , Q m( X O m| X I j )} j∈B(cid:12) are weakly coherent. However, any coherent1) = 1 on the one hand, and P (x(cid:7)2) on the⊆ B⊆ B(cid:12)1, J i2(cid:12)2 and the blocks B Z i1(b) If (a) does not hold, then for any i = 1, . . . , k, the set of indices associated to the block B Z i is included in some B(cid:12) ∈ B(cid:12).Since B has non-empty intersection with different elements of B(cid:12), and the definition of B implies that any block B Z1in B has another block B Z2 in B such that B Z1 and B Z2 share an actual node, we deduce that there are 1 (cid:3) i1 (cid:3)= i2 (cid:3) k, B Z i2share some actual node X(cid:7): we can start with a block of B andsuch that J i1move to the adjacent blocks until we arrive to one which is not included in the same element of B(cid:12). The result thenfollows from the fact that two adjacent blocks share at least one actual node. See Fig. 17 for an example.Consider two different values x(cid:7)1. . . , Pm( X O msatisfies P (x(cid:7)The same proposition guarantees the existence of weakly coherent conditional previsions Psuch that any joint P that is weakly coherent with {P2) = 1.Define now conditional previsions Q 1( X O 1| X Im ) that are weakly coherent and such that any joint that is weakly coherent with {P j( X O j1) = 1.2 in X(cid:7). Then applying Proposition 3, there are conditional previsions P 1( X O 1(cid:12)j( X O j| X I1 ), . . . , Q m( X O m| X I1 ),| X I j )} j∈ J i1| X I j )} j∈ J i2| X Im ) by| X I1 ), . . . , Psatisfies P (x(cid:7)(cid:12)m( X O m(cid:12)1( X O 1(cid:3)= x(cid:7)| X Im )(cid:8)Q j( X O j| X I j ) :=P j( X O j(cid:12)j( X O jPif j ∈ B| X I j )| X I j ) otherwise.(cid:12)1| X I j )} j∈B(cid:12) are weaklyThen these previsions are separately coherent and moreover for any Bcoherent. Now, given any coherent prevision P on L(X n) which is coherent with all these conditional previsions, we| X Im ) areshould have P (x(cid:7)2), a contradiction. Using Theorem 1, we deduce that Q 1( X O 1not weakly coherent. (cid:2)we have that {Q j( X O j| X I1 ), . . . , Q m( X O m1) = 1 = P (x(cid:7)(cid:12) ∈ B(cid:12)Proof of Proposition 4. It is trivial that the first statement implies the second. That the second statement implies the thirdfollows from Theorem 4, once we remark that the coherence graph is of type A1 if and only if the minimal partition isB = {1, . . . , m}. Finally, Corollary 3 guarantees that the third statement implies the first. (cid:2)E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144141Proof of Theorem 5. For each i ∈ {1, . . . , m}, the separate coherence of the conditionallower prevision P i( X O iimplies that it is the lower envelope of a family of separately coherent conditional previsions Mi := {P i( X O iNow,plate as {P 1( X O 1{P 1( X O 1| X Ii )| X Ii )}.for each i = 1, . . . , m, they belong to the same collection tem-| X Im )}. Proposition 4 implies then that they are jointly coherent, and therefore| X Im )} is the lower envelope of a family of coherent conditional previsions. (cid:2)if we select a conditional prevision in Mi| X I1 ), . . . , P m( X O m| X I1 ), . . . , P m( X O mProof of Theorem 6. We start by defining a bound on the number of arcs in a coherence graph. Remember that a coherencegraph can be regarded as the union of its D-structures. Since a D-structure can be made of n arcs at most, we deduce thatthe number of arcs in a coherence graph is bounded by m · n.It is also useful to consider the number of different blocks in a coherence graph: since a block can be identified withone the sources of contradiction that originates it, and because a source of contradiction is an actual node, the number ofdifferent blocks is bounded by the number of actual nodes, namely n.Now, consider that one effect of the interplay between findMinPartition and findBlock is a visit of the entiregraph. This means that all its arcs are traversed once and hence the visit has worst-case complexity O (m · n), given theprevious bound on the number of arcs. This complexity takes into account the fact that findBlock stops its visit at acertain node if this is tagged already, thus preventing an arc from being visited more than once.To find out the overall complexity of the procedures, now we have to consider that the second effect jointly produced byfindMinPartition and findBlock, through mergeBlocks, is to fill the array minPartition. This array is scannedevery time a new, different block is found. Since the size of the array is m and the number of different blocks is boundedby n, as said above, filling the array minPartition has worst-case complexity O (m · n).Overall, we deduce that findMinPartition, findBlock, and mergeBlocks, have a joint worst-case complexitygiven by O (m · n) (also considering that the remaining calculations done by them have a complexity dominated by such anexpression).Finally, in order to define the complexity to find the minimal partition, we must consider that the three procedures aboveare based on the global data structures described at the beginning of Section 7. Therefore, we have to calculate also thecomplexity to create such structures. In this case the computation that dominates the others, with respect to computationalcomplexity, is the identification of the sources of contradiction. Identifying the actual nodes with more than a parent iseasy, and takes O (n). Finding out all the nodes that belong at least to a cycle in the graph is more complicated. Yet, asstated at the end of Section 7, this is an immediate byproduct of the identification of the strong components of the graph;and the latter is a task that Tarjan’s algorithm in [29] solves in time O (m + n + m · n).This complexity dominates the one arising from findMinPartition, findBlock, and mergeBlocks, whence weobtain that the complexity to find the minimal partition is just O (m + n + m · n). (cid:2)Proof of Theorem 8. We first consider how to convert the graph of a credal net into the related coherence graph: it issufficient, for each node X j of the credal net, to insert a dummy node D j between X j and its parents X I j , so as to make D jparent of X j and child of X I j , and to remove the arcs from X I j to X j . It is immediate from this conversion procedure to seethat the coherence graph resulting from that of a credal net is A1: the coherence graph is acyclic because so is the graphof a credal net; moreover, each actual node X j has exactly one parent. Another immediate consequence of the procedure isthat permuting the indexes of the conditional lower previsions in the coherence graph as in Lemma 1 can be done simplyby making them consistent with the partial order entailed by the graph of the credal net. Therefore, we can assume withoutloss of generality that the set A j defined in Section 8.1.1 after Lemma 1 indexes exactly the non-descendant non-parents ofX j , for all j = 1, . . . , n.+Now we are ready to show that the strong extension of a credal network and the strong product of the related coherencegraph coincide.Consider a prevision P dominating the strong extension obtained as in the definition of the strong product by applying| X A j ∪I j ) is a synthetic ex-(cid:12)) =Eq. (8) to a collection {P 1( X O 1pression for a set of separately coherent conditional previsions {P j( X O jP j( X O j| X An∪In )}. Here the generic element P j( X O j(cid:12)(cid:12))}. For an x ∈ X n, it holds that|z))): z ∈ X A j ∪I j , P j( X O j| X A1∪I1 ), . . . , Pn( X O n|z) ∈ ext(M(P(cid:12)j( X O j|z|z(cid:12)(cid:12)) if πI j (zn(cid:14)(cid:12)) = πI j (z(cid:13)(cid:12)πO j (x)|π A j ∪I j (x)P jP (x) =j=1(cid:13)(cid:12)π j(x)|πI j (x),P j=n(cid:14)j=1(A.13)where the first passage is just Eq. (8) and the second depends on two considerations. The first is that O jindexes thevariable X j alone when we deal with the coherence graph obtained from a Bayesian net. The second is more important:X j does not depend stochastically on its non-descendant non-parents ( X A j ) given its parents ( X I j ). This follows because weknow that the chosen element of M(P|z)) must be the same over all the values that X A j can take, by definition of|z)) is an extreme point ofP j( X O jM(P j( X O j(cid:12)j( X O j|πI j (z))), and this only depends on X I j .| X A j ∪I j ); and because, by definition of P| X A j ∪I j ), an extreme point of M(P(cid:12)j( X O j(cid:12)j( X O jIt follows that Eqs. (9) and (A.13) coincide, and hence that the selected element P dominating the strong product corre-sponds to the joint mass function of a Bayesian net obtained choosing the assessments P j( X j| X I j ) (cid:2) P j( X j| X I j ) j = 1, . . . , n,142E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144i.e., to a compatible Bayesian net. Therefore P is a linear prevision that dominates the strong extension. Since this holds forall the previsions P in the definition of the strong product, then the strong product dominates the strong extension.Now consider an extreme point of the strong extension, i.e., equivalently, a joint linear prevision P . It is well known(see [1, Proposition 1] for a proof) that P is obtained by applying Eq. (9) to mass functions corresponding to ex-i.e., equivalently, to a collection {P 1( X1| X I1 ), . . . , Pn( Xn| X In )}, withtreme points of the local conditional credal sets,P j( X j|z) ∈ ext(M(P j( X j|z))) for all z ∈ XI j ,j = 1, . . . , n. From this, defining the new collection of conditional previsions| X An∪In )} as in the definition of the strong product, and applying Eq. (A.13), we see that there{P 1( X O 1is an element of the strong product that coincides with P . Since this holds for all the extreme points of the strong extension,then the strong extension dominates the strong product. (cid:2)| X A1∪I1 ), . . . , Pn( X O nm| X I1 ), . . . , P m( X O m| X Im )} avoidProof of Proposition 5. We start by proving the direct implication. Assume that {P 1( X O 1uniform sure loss. Given f j ∈ K j for j = 1, . . . , m, this implies that supx∈X nj=1 G j( f j| X I j )(x) (cid:2) 0. Let us define D :={G j( f j| X I j ) : f j ∈ K j for some j}. Applying [33, Lemma 3.3.2], we deduce the existence of a linear prevision P satisfyingP (G j( f j| X I j )) (cid:2) 0 for all f j ∈ K j, j = 1, . . . , m, and in particular P (G j( f j|x)) (cid:2) 0 for all f j ∈ K j, x ∈ XI j , j = 1, . . . , m.Let us prove the existence, for every j = 1, . . . , m and every x ∈ X j of a linear conditional prevision P j( X O j|x) dominating|x) such that P (Ix( f − P j( f |x))) = 0 for all f ∈ K j ; from this, taking into account that the spaces are finite, we shall| X I j ) and is coherent with P for| X Im ) dominating our conditional lowerP j( X O jdeduce the existence of a conditional linear prevision P j( X O jall j = 1, . . . , m, and as a consequence of weakly coherent P 1( X O 1previsions.| X I j ) that dominates P j( X O j| X I1 ), . . . , Pm( X O m(cid:16)|x) is uniquely determined by P using Bayes’ rule. To see that it dominates P j( X O jConsider then j ∈ {1, . . . , m} and x ∈ XI j . There are two possibilities: if P (x) > 0, then the conditional linear previ-|x), assume ex-absurdofor which P j( f |x) > P j( f |x). Then it follows that 0 (cid:3) P (Ix( f − P j( f |x))) <|x)sion P j( X O jthe existence of some gamble f ∈ K jP (Ix( f − P j( f |x))) = 0, a contradiction. Finally, it P (x) = 0 we simply consider any linear conditional prevision P j( X O jthat dominates P j( X O j|x) and it is automatically coherent with P .The converse implication follows once we realise that (i) weakly coherent conditional previsions in particular avoiduniform sure loss, and (ii) if some conditional lower previsions are dominated by others which avoid uniform sure loss,then they also avoid uniform sure loss. (cid:2)The next lemma is needed in the proof of Theorem 9. It is the counterpart, for avoiding partial and uniform sure loss, ofLemma 5.Lemma 7. Let us consider a non-empty subset J of {1, . . . , m}. Let B1 be a partition of J , and define, for each C ∈ B1, the set B C :=(cid:7){j∈C (I j ∪ O j)}. Then, if the sets {B C : C ∈ B1} are pairwise disjoint, the following statements hold:| X I j )} j∈C avoid partial loss for all C ∈ B1, then the lower previsions {P j( X O j| X I j )} j∈C avoid uniform sure loss for all C ∈ B1, then the lower previsions {P j( X O j| X I j )} j∈ J avoid partial loss.| X I j )} j∈ J avoid uniform sure loss.(1) If {P j( X O j(2) If {P j( X O jProof.(1) Consider f j ∈ K j for j ∈ J , f 0 ∈ K j0 , z0 ∈ XI j0for some j0 ∈ J . Assume that I j (cid:3)= ∅ for any j such that f j (cid:3)= 0; otherwise,the result follows from the second statement, using also the reduction theorem [33, Theorem 7.1.5].For any C in B1, there is some D C ∈j∈C S j( f j) such that(cid:7)(cid:5)j∈Csupx∈D CG j( f j| X I j )(x) (cid:2) 0;hence, given (cid:10) > 0 there is some xC ∈ D C such that(cid:5)j∈CG j( f j| X I j )(xC ) (cid:2) −(cid:10).(A.14)Let us consider now an element z ∈ X n satisfying πB C (z) = πB C (xC ) for any C ∈ B1; such an element exists because thesets {B C : C ∈ B1} are pairwise disjoint. Then we deduce from Eq. (A.14) that(cid:17)(cid:5)(cid:18)G j( f j| X I j )(z) (cid:2) −|B1|(cid:10),j∈ Jand moreover z ∈ D C for all C ∈ B1 (and in particular for one of them). Since we can do this for any (cid:10) > 0, we deduce| X I j )} j∈ J avoid partial loss.that the conditional lower previsions {P j( X O j(2) Using a similar reasoning and the notations from the previous point, we deduce that for any (cid:10) > 0 and for any C in B1,E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144143there is some xC such thatG j( f j| X I j )(xC ) (cid:2) −(cid:10).(cid:5)j∈CNow, since the sets {B C : C ∈ B1} are pairwise disjoint, we deduce from this equation that for any element z of X n suchthat πB C (z) = πB C (xC ) for all C ∈ B1,(cid:17)(cid:5)(cid:18)G j( f j| X I j )(z) (cid:2) −|B1|(cid:10).j∈ JAgain, since we can do this for any (cid:10) > 0, we deduce that the conditional lower previsions {P j( X O juniform sure loss. (cid:2)| X I j )} j∈ J avoidProof of Theorem 9. We shall prove the result for avoiding partial loss; the proof for avoiding uniform sure loss is analogous.Let us consider gambles f j ∈ K j for j = 1, . . . , m, and let us show thatG j( f j| X O j )(x) (cid:2) 0(A.15)(cid:6)(cid:4)m(cid:5)j=1(cid:7)supx∈Efor some E ∈mj=1 S j( f j).Let B1 be the subset of B given by the sets C with |C| > 1, i.e., those associated to superblocks. Assume that B1 (cid:3)= ∅; ifB1 = ∅, then the result follows from Corollary 3. For any C ∈ B1 let us consider the set B C :=j∈C (I j ∪ O j). Then it followsfrom the definition of the superblocks that the sets {B C : C ∈ B1} are pairwise disjoint subsets of {1, . . . , n}. Let us defineJ :=C . Define the gambles h1 :=j∈ J G j( f j| X I j ) and h2 :=j∈ J c G j( f j| X I j ).| X I j )} j∈ J avoid partial loss. As a consequence, there exists some E ∈From Lemma 7, the previsions {P j( X O jj∈ J S j( f j)C∈B1(cid:16)(cid:16)(cid:7)(cid:7)(cid:7)such thath1(x) (cid:2) 0.supx∈E(cid:7)B C and y1 := πI (x1). Note that the gambleConsider (cid:10) > 0, and let x1 ∈ E satisfy h1(x1) (cid:2) −(cid:10). Let us consider I :=h1 is XI -measurable, and that π −1( y1) is included in E because I j ⊆ I for any j ∈ J .C∈B1IThe coherence graph of the previsions {P j( X O j| X I j )} j∈ J c is A1 because they do not belong to any superblock. Moreover,for any j ∈ J c , I ∩ O j = ∅, because of the comments at the end of Section 5. Hence, we can apply Lemma 6 to deduce thatsupx∈π −1h2(x) = supx∈π −1I( y1)( y1)Since h1 is identically equal to h1(x1) (cid:2) −(cid:10) on π −1j∈ J cI(cid:5)G j( f j| X I j )(x) (cid:2) 0.( y1) h2(x) (cid:2) −(cid:10), and( y1) is included in E we deduce that supx∈E h1(x) + h2(x) (cid:2) −(cid:10). Since we can make a similar reasoning for any( y1) h1(x) + h2(x) (cid:2) −(cid:10) + supx∈π −1( y1), supx∈π −1IIIbecause π −1(cid:10) > 0, we deduce that Eq. (A.15) holds. (cid:2)IReferences[1] A. Antonucci, M. Zaffalon, Decision-theoretic specification of credal networks: A unified language for uncertain modeling with sets of Bayesian net-works, International Journal of Approximate Reasoning 49 (2) (2008) 345–361.[2] B. Arnold, E. Castillo, J.M. Sarabia, Conditional Specification of Statistical Models, Springer, 1999.[3] M. Baioletti, A. Capotorti, S. Tulipani, B. Vantaggi, Elimination of Boolean variables for probabilistic coherence, Soft Computing 4 (2000) 81–88.[4] K.P.S. Bhaskara Rao, M. Bhaskara Rao, Theory of Charges, Academic Press, London, 1983.[5] V. Biazzo, A. Gilio, T. Lukasiewicz, G. Sanfilippo, Probabilistic logic under coherence: Complexity and algorithms, Annals of Mathematics and ArtificialIntelligence 45 (1–2) (2005) 35–81.[6] G. Boole, An Investigation on the Laws of Thought, on Which are Founded the Mathematical Theories of Logic and Probabilities, Walton and Maberley,London, 1854. Reprinted by Dover, New York, 1961.[7] A. Capotorti, B. Vantaggi, Locally strong coherence in inference processes, Annals of Mathematics and Artificial Intelligence 35 (1–4) (2002) 125–149.[8] F.G. Cozman, Separation properties of sets of probabilities, in: C. Boutilier, M. Goldszmidt (Eds.), Uncertainty in Artificial Intelligence (Proceedings ofthe Sixteenth Conference), Morgan Kaufmann, San Francisco, 2000, pp. 107–115.[9] C. Cuadras, J. Fortiana, J.A. Rodriguez-Lallena, Distributions with Given Marginals and Statistical Modelling, Kluwer Academic Publishers, 2002.[10] B. de Finetti, Sul significato soggettivo della probabilità, Fundamenta Mathematicae 17 (1931) 298–329.[11] B. de Finetti, La prévision: ses lois logiques, ses sources subjectives, Annales de l’Institut Henri Poincaré 7 (1937) 1–68, English translation in [18].[12] B. de Finetti, Teoria delle Probabilità, Einaudi, Turin, 1970.[13] B. de Finetti, Theory of Probability, vol. 1, John Wiley & Sons, Chichester, 1974. English translation of [12].[14] M. Fréchet, Sur les tableaux de correlation dont les marges sont donns, Ann. Univ. Lyon, Section A, Series 3 14 (1951) 53–77.[15] P. Hansen, B. Jaumard, M. Poggi de Aragão, F. Chauny, S. Perron, Probabilistic satisfiability with imprecise probabilities, International Journal of Approx-imate Reasoning 24 (2–3) (2000) 171–189.144E. Miranda, M. Zaffalon / Artificial Intelligence 173 (2009) 104–144[16] W. Hoeffding, Masstabinvariante korrelationtheorie, Schriften des Mathematischen Instituts und des Institud für Angewndte Mathematik der UniversitätBerlin 5 (1940) 181–233.[17] B. Jaumard, H. Hansen, M. Poggi de Aragão, Column generation methods for probabilistic logic, ORSA Journal on Computing 3 (1991) 135–148.[18] H.E. Kyburg Jr., H.E. Smokler (Eds.), Studies in Subjective Probability, Wiley, New York, 1964. Second edition (with new material) 1980.[19] T. Lukasiewicz, Probabilistic deduction with conditional constraints over basic events, Journal of Artificial Intelligence Research 10 (1999) 199–241.[20] F. Mauch, On the existence of probability distributions with given marginals, Theory of Probability and its Applications 48 (3) (2004) 541–548.[21] E. Miranda, Updating coherent previsions on finite spaces, Technical Report TR 08/04, Rey Juan Carlos University, Technical Reports on Statistics andDecision Sciences, 2008.[22] E. Miranda, G. de Cooman, Coherence and independence in non-linear spaces, Technical Report TR 05/10, Rey Juan Carlos University, Technical Reportson Statistics and Decision Sciences, 2005.[23] E. Miranda, G. de Cooman, Marginal extension in the theory of coherent lower previsions, International Journal of Approximate Reasoning 46 (1) (2007)188–225.[24] S. Moral, A. Cano, Strong conditional independence for credal sets, Annals of Mathematics and Artificial Intelligence 35 (1–4) (2002) 295–321.[25] R. Nelsen, An Introduction to Copulas, Springer, New York, 1999.[26] N.J. Nilsson, Probabilistic logic, Artificial Intelligence 28 (1986) 71–87.[27] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo, CA, 1988.[28] A. Sklar, Fonctions de répartition à n-dimensions et leurs marges, Publications de l’Institute de Statistique de l’Université de Paris 8 (1959) 229–231.[29] R.E. Tarjan, Depth-first search and linear graph algorithms, SIAM Journal on Computing 1 (1972) 146–160.[30] M.C.M. Troffaes, G. de Cooman, Extension of coherent lower previsions to unbounded random variables, in: Intelligent Systems for Information Pro-cessing: For Information to Applications, North Holland, Amsterdam, 2003, pp. 277–288.[31] L.C. van der Gaag, Computing probability intervals under independency constraints, in: P.P. Bonissone, M. Henrion, L.N. Kanal, J.F. Lemmer (Eds.), UAI’90: Proceedings of the Sixth Annual Conference on Uncertainty in Artificial Intelligence, Elsevier, 1991, pp. 457–466.[32] N.N. Vorobev, Consistent families of measures and their extensions, Theory of Probability and its Applications VII (2) (1962) 147–163.[33] P. Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall, London, 1991.[34] P. Walley, R. Pelessoni, P. Vicig, Direct algorithms for checking consistency and making inferences for conditional probability assessments, Journal ofStatistical Planning and Inference 126 (2004) 119–151.[35] P.M. Williams, Notes on conditional previsions, Technical report, School of Mathematical and Physical Science, University of Sussex, UK, 1975. Reprintedin [36].[36] P.M. Williams, Notes on conditional previsions, International Journal of Approximate Reasoning 44 (3) (2007) 366–383.[37] M. Zaffalon, E. Miranda, Conservative inference rule for uncertain reasoning under incompleteness, 2008, submitted for publication.