ArticleConnectome-based machine learning models arevulnerable to subtle data manipulationsGraphical abstractAuthorsMatthew Rosenblatt,Raimundo X. Rodriguez,Margaret L. Westwater, ...,R. Todd Constable, Stephanie Noble,Dustin ScheinostCorrespondencematthew.rosenblatt@yale.edu (M.R.),dustin.scheinost@yale.edu (D.S.)In briefImperceptible data manipulations candrastically increase or decreaseperformance in machine learning modelsthat use high-dimensional neuroimagingdata. These manipulations could achievenearly any desired predictionperformance without noticeable changesto the data or any changes in otherdownstream analyses. The feasibility ofdata manipulations highlights thesusceptibility of data sharing andscientific machine learning pipelines tofraudulent behavior.Highlightsd Enhancement attacks falsely improve the performance ofconnectome-based modelsd Adversarial attacks degrade the performance ofconnectome-based modelsd Subtle data manipulations lead to large changes inperformanceRosenblatt et al., 2023, Patterns 4, 100756July 14, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100756llPlease cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSArticleConnectome-based machine learning modelsare vulnerable to subtle data manipulationsMatthew Rosenblatt,1,9,* Raimundo X. Rodriguez,2 Margaret L. Westwater,3 Wei Dai,4 Corey Horien,2 Abigail S. Greene,2R. Todd Constable,1,2,3,5 Stephanie Noble,3 and Dustin Scheinost1,2,3,6,7,8,*1Department of Biomedical Engineering, Yale School of Engineering and Applied Science, New Haven, CT 06510, USA2Interdepartmental Neuroscience Program, Yale School of Medicine, New Haven, CT 06510, USA3Department of Radiology & Biomedical Imaging, Yale School of Medicine, New Haven, CT 06510, USA4Department of Biostatistics, Yale School of Public Health, New Haven, CT 06510, USA5Department of Neurosurgery, Yale School of Medicine, New Haven, CT 06510, USA6Department of Statistics & Data Science, Yale University, New Haven, CT 06510, USA7Child Study Center, Yale School of Medicine, New Haven, CT 06510, USA8Wu Tsai Institute, Yale University, New Haven, CT 06510, USA9Lead contact*Correspondence: matthew.rosenblatt@yale.edu (M.R.), dustin.scheinost@yale.edu (D.S.)https://doi.org/10.1016/j.patter.2023.100756THE BIGGER PICTURE In recent years, machine learning models using brain functional connectivity havefurthered our knowledge of brain-behavior relationships. The trustworthiness of these models has notyet been explored, and determining the extent to which data can be manipulated to change the results isa crucial step in understanding their trustworthiness. Here, we showed that only minor manipulations ofthe data could lead to drastically different performance. Although this work focuses on machine learningmodels using brain functional connectivity data, the concepts investigated here apply to any scientificresearch that uses machine learning, especially with high-dimensional data. As machine learning becomesincreasingly popular in many fields of scientific research, data manipulations may become a major obstacleto the integrity of scientific machine learning.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYNeuroimaging-based predictive models continue to improve in performance, yet a widely overlookedaspect of these models is ‘‘trustworthiness,’’ or robustness to data manipulations. High trustworthinessis imperative for researchers to have confidence in their findings and interpretations. In this work, weused functional connectomes to explore how minor data manipulations influence machine learning predic-tions. These manipulations included a method to falsely enhance prediction performance and adversarialnoise attacks designed to degrade performance. Although these data manipulations drastically changedmodel performance, the original and manipulated data were extremely similar (r = 0.99) and did not affectother downstream analysis. Essentially, connectome data could be inconspicuously modified to achieveany desired prediction performance. Overall, our enhancement attacks and evaluation of existingadversarial noise attacks in connectome-based models highlight the need for counter-measures thatimprove the trustworthiness to preserve the integrity of academic research and any potential translationalapplications.INTRODUCTIONHuman neuroimaging studies have increasingly used machinelearning approaches to identify brain-behavior associationsthat generalize to novel samples.1,2 They do so by aggregatingweak yet informative signals occurring throughout the brain.3,4Machine learning models for functional connectomes (‘‘con-nectome-based models’’)5–7 are among the most popularPatterns 4, 100756, July 14, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSmethods for establishing brain-behavior relationships, andthey have successfully characterized the neural correlates ofvarious clinically relevant processes,8 including general cogni-tive ability,9 psychiatric disorders,7,10 affective states,11 andabstinence in individuals with substance use disorder.12Recent work has uncovered bias, or lack of fairness acrossgroups, in connectome-based models,13–15 including predic-tion failure in individuals who defy stereotypes.15 Although im-provements in accuracy6 and fairness (i.e., race, age, orgender bias)13–15 of connectome-based models are crucialfor improving the quality of academic studies and the potentialfor clinical translation, accurate and bias-free models are notenough. Connectome-based models should also have hightrustworthiness, which we define as robustness to data ma-nipulations.In other words, the output or performance ofa trustworthy model remains similar despite minor changesto the input (i.e., X data). Without a high degree of trustworthi-ness, researchers may not be able to have confidence in theirfindings and ensuing interpretations, as even minor modifica-tions to the data could dramatically alter results.Although trustworthiness has been explored from variousperspectives in the machine learning literature, including pri-vacy16 and explainability,17 here we examine trustworthinessthrough the lens of robustness to data manipulations.18 A pop-ular form of data manipulation specific to machine learning isadversarial noise (i.e., adversarial attacks), where a pattern(or ‘‘noise’’) deliberately designed to trick a machine learningmodel is added to data to cause misclassification.19,20 Theseattacks have been investigated in various contexts, includingcybersecurity,21,22 image recognition,20,23 and medical imagingor recordings.24–26 For neuroimaging, adversarial attacks maybecome problematic in the more distant future (e.g., in clinicalapplications25,27).A more immediate concern is the potential for data manipula-tions to falsely enhance prediction performance in researchstudies. Although the majority of scientific researchers seek toperform ethical research, data manipulations are more commonthan one might expect.28–33 For example, an analysis by Bik et al.showed that about 2% of biology papers contained a figure withevidence of intentional data manipulation.31 Furthermore, 2%of scientists admitted to fabrication/falsification, and 14%admitted to seeing their colleagues fabricate/falsify in a survey.32As data manipulation can result in wasted grant money andfuture research endeavors, determining themisdirection ofextent to which the prediction performance of connectome-based models can be falsely enhanced or diminished via datamanipulations is crucial.In this work, we investigated the trustworthiness of connec-tome-based predictive models. Specifically, we introduce thefor connectome-based‘‘performance enhancement attack’’models, where data are injected with small,inconspicuouspatterns to falsely improve the prediction performance ofa specific phenotype. We also explore the effectiveness of ad-versarial noise attacks on connectome-based models. Whereasadversarial noise attacks manipulate only the test data to changea particular prediction, enhancement attacks modify the entiredataset (i.e., training and test data) to falsely improve perfor-mance. In both cases—enhancement attacks and adversarialnoise attacks—we find that subtle manipulations drastically2 Patterns 4, 100756, July 14, 2023Articlechange predictions in four large datasets. Overall, our findingsdemonstrate that currentimplementations of connectome-based models are highly susceptible to data manipulations,which points toward the need for preventive measures built into study designs and data sharing practices.RESULTSFunctional MRI data were obtained from the Adolescent BrainCognitive Development (ABCD) study,34 the Human Connec-tome Project (HCP),35 the Philadelphia NeurodevelopmentalCohort (PNC),36 and the Southwest University LongitudinalImaging Multimodal (SLIM) study.37 The first three datasets(ABCD, HCP, and PNC) were used to demonstrate enhance-ment and adversarial attacks for prediction of IQ and self-re-ported sex. SLIM was introduced to demonstrate enhance-ment with a clinically relevant measure (state anxiety). Allanalyses were conducted on resting-state data. For SLIM,we downloaded fully preprocessed functional connectomes.For ABCD and PNC, raw data were registered to commonspace as previously described.38,39 For HCP, we startedwith the minimally preprocessed data.40 Next, standard, iden-tical preprocessing steps were performed across all datasetsusing BioImage Suite41 (see experimental procedures). In allcases, data were parcellated into 268 nodes with the Shenatlas.42 After excluding participants for excessive motion(>0.2 mm), missing nodes due to lack of full brain coverage,or missing task or behavioral data, 3,362 individuals in theABCD dataset, 506 individuals in the HCP dataset, 562 indi-viduals in the PNC dataset, and 445 individuals in the SLIMdataset remained. In the following sections, we first compre-hensively characterize the effects of performance enhance-ment attacks, and then evaluate adversarial noise attacks.For enhancement attacks, we show that inconspicuous pat-terns can be added to an entire connectome dataset to falselyimprove performance. For adversarial attacks, we demon-strate that connectome-based models are particularly vulner-able to adversarial manipulations at test time, which are de-signed to degrade performance (Figure 1).Baseline model performanceTo evaluate trust, we trained baseline regression models offluid intelligence (IQ) and classification models of self-reportedsex. These models provide a good benchmark for trustworthi-ness because of their wide availability in datasets and promi-nence in the literature.5,43–46 For the regression models, weused ridge regression connectome-based predictive modeling(rCPM)44 with nested 10-fold cross-validation and 10% featureselection. Regression models of IQ were evaluated using Pear-son’s correlation coefficient r and the cross-validated R2, calledq2,47 between the measured and predicted IQ scores. Wefound near zero correlations for ABCD, which is consistentwith Li et al.,14 and low correlations for HCP and PNC (seeTable S1).For classification of self-reported sex, we trained both linearsupport vector machine (SVM) and logistic regression modelswith all available features using nested 10-fold cross-validationand L2 regularization. We also evaluated the accuracy of classifiersof self-reported sex and found relatively high success in all threePATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756ArticlellOPEN ACCESScorrelated with IQ (Figure 2, top). To enhance the data for IQprediction, we randomly selected 20% of all edges across allparticipants (i.e., the same exact edges were selected for all par-ticipants) and then added an individual-specific pattern that wascorrelated (or anti-correlated) with each participant’s fluid intelli-gence score. We varied the magnitude of this pattern andrepeated model training and evaluation with 10-fold cross-vali-dation, recording changes in the correlation between measuredand predicted IQ (Figure 2). Results were easily manipulatedeven with a low-magnitude enhancement pattern, achieving anear-perfect correlation between measured and predicted IQscores (r > 0.9) for corrupted connectomes that still maintainedan extremely high edge-wise correlation (r z 0.99) with their orig-inal counterparts. For the results in Figures 1 and 2, we selectedamong large regularization parameters with nested cross-valida-regularization parameter madetion, but using a smallerenhancement attacks even more effective (Figure S1). In addi-tion, enhancement attacks are effective against not only linearmodels but also neural networks (Table S2). These results sug-gest that minor changes to a functional connectivity matrix canundermine the trustworthiness of predictive models, even inthe context of open science practices.Figure 1. Summary of the manipulations investigated in this studyThe left half shows a typical connectome-based pipeline. The right halfshows where each manipulation can be applied in the pipeline. Red textindicates attacks that degrade performance, while green text indicates at-tacks that falsely enhance performance. Enhancement attacks are applied toall data. These attacks are relevant for false enhancement of academicstudies or open-source data. They can be applied at multiple points in theprocessing pipeline (time-series enhancement or connectome enhancement)to falsely enhance performance or alter neuroscientific interpretations. Ad-versarial noise attacks are applied to only the test data, on the basis of themodel coefficients. These attacks have implications in potential translationalapplications.datasets for both SVM and logistic regression, although SVM hadhigher prediction accuracy (Table S1). In the following sections, wewill describe how the performance metrics in Table S1 can bedrastically altered through inconspicuous manipulations tothe data.Performance enhancement attacks are effective andnearly unnoticeableTo date, most research on data manipulations for machinelearning has focused on corrupting data to decrease model ac-curacy.48 However, here, we investigated the feasibility of datamanipulations designed to increase the accuracy (which welabel ‘‘enhancement attacks’’) in ways that cannot be readilydetected by the human eye or by changes in downstreamanalyses. Current neuroimaging open science standardswould not offer protection against data manipulations thatfalsely enhance performance without statistically altering theconnectomes.First, we trained a model to predict IQ with resting-state con-nectomes in ABCD (n = 3,262), HCP (first session, n = 506), andPNC (n = 562) with rCPM,44 using 10-fold cross-validation and10% feature selection performed using the features most highlyPerformance enhancement attacks preserveindividualityAlthough the enhanced connectomes appeared almost visuallyidentical to the original connectomes (Figure 2), we also investi-gated if the enhancement patterns affected other downstreamanalyses. If common, downstream analyses were not affected,this would make it difficult to determine if connectomes havebeen manipulated. First, we varied the mean absolute value ofthe enhancement pattern and trained rCPM models to predictIQ scores in ABCD, HCP, and PNC. As we increased the scaleof the enhancement pattern, prediction performance greatlyincreased, with correlations between measured and predictedIQ achieving r > 0.9 (Figures 2 and 3A). Corroborating visualinspection,the correlations between edges of originaland enhanced connectomes remained very high (r values zIn addition, a participant-wise0.99)Kolmogorov-Smirnov test49 suggested no significant differencesin edge distributions between original and enhanced data(median p > 0.9999).(Figure 3A,top row).Using the enhanced IQ data, we next trained SVM classifiersfor self-reported sex and found that sex classification accuracystayed essentially constant(Figure 3A, bottom row), evenwhen the prediction performance for IQ was drastically different.Moreover, we compared functional connectome fingerprintingwith the original and enhanced data in HCP. Functional connec-tomes have previously been used as participant-specific ‘‘finger-prints’’50,51 that can accurately identify participants acrossdifferent fMRI sessions or tasks. To perform fingerprinting wecalculated the edge-wise correlation between the first (Rest1)and second (Rest2) resting-state connectomes in HCP. The pre-dicted identity of the participant was the connectome from theother session with the highest edge-wise correlation.51 We per-formed this identification process in each fold of our 10-foldcross-validation, so each identification procedure included10% of the 506 participants in the HCP dataset. Whether usingthethe original orthe enhanced Rest1 connectomes,Patterns 4, 100756, July 14, 2023 3PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSidentification rate remained the same (p values = 1; Figure 3A,bottom row, HCP). Furthermore, we performed fingerprinting inHCP following the procedure used in Figure 3A, except weonly used edges corresponding to various subnetworks, as pre-viously defined with the Shen 268 atlas51,52 (Figure S2). Evenwhen only using a single subnetwork to fingerprint, there wasessentially no difference in accuracy for original and enhancedconnectomes (median p value > 0.5 for each subnetwork; me-dian overall p = 1). The fingerprinting results indicate thatenhancement attacks preserved the individuality of connec-tomes at both the whole-brain and subnetwork levels, despitehaving a large effect on IQ prediction.Finally, we evaluated several graph properties,includingstrength, assortativity, and clustering coefficient, in the originaland enhanced connectomes.53 Despite the sensitivity of graphtheory metrics to minor changes,54 the correlation betweenthese node-level metrics in the original and enhanced connec-tomes was very high (r z 0.99) (Figure 3C). As such, enhance-ment attacks appear to uniquely strengthen brain-behavior as-sociations with the phenotype of interest, making them difficultto detect even with other analyses.Performance enhancement attacks can be used to alterinterpretationsIn addition to falsely improving predictive ability, enhancementmanipulations can be leveraged to reinforce a particular brain-behavior relationship. For this example, we used rCPM with10-fold cross-validation to predict state anxiety (State-Trait Anx-4 Patterns 4, 100756, July 14, 2023ArticleFigure 2. Main pipeline of performanceenhancement attacksThis example is shown for prediction of IQ in theHCP dataset with resting-state connectomes andrCPM. The original dataset results in a predic-tion performance of r = 0.18 between measuredand predicted IQ. Enhancement patterns (meanenhancement pattern shown) are added to theoriginal connectome proportional to each partici-pant’s Z-scored IQ. For the sake of visualization, wemultiplied the enhancement patterns by 120, 80,and 40, or else they would be too small to see. Thecorresponding enhanced connectomes maintainaverage correlations of r z 0.99 with the originalconnectomes, but the prediction performance isgreatly enhanced. The networks labeled on theconnectomes are as follows: MF, medial-frontal; FP,fronto-parietal; DMN, default mode; MOT, motor; VI,visual I; VII, visual II; VAs, visual association; SAL,salience; SC, subcortical; and CBL, cerebellum.51,52iety Inventory)55 in the SLIM dataset, whichis an open-source dataset of prepro-cessed connectomes. We first exploredthe prediction performance in unalteredconnectomes and found essentially nopredictive power (Figure 4, top row). As inthe previous section, we successfullymanipulated random edges to increaseprediction performance to r = 0.93 (Fig-ure 4, middle row). Then, we altered onlyedges involved in the salience network toenhance prediction to r = 0.9 (Figure 4, bottom row). This influ-enced model coefficients to be dominated by edges of thesalience network and thus would suggest that the saliencenetwork can predict anxiety scores in this dataset. The same tar-geted pattern injection could be similarly performed for othersubnetworks to enforce a different interpretation. These resultshighlight the potential power and importance of enhancement at-tacks, which could not only alter performance, but also supportan unfounded neuroscientific interpretation of a clinically rele-vant phenotype.Performance enhancement extends beyondpreprocessed connectomesBecause the previous sections demonstrated that perfor-mance enhancement attacks are highly effective against con-nectomes, a potential solution would be to always release rawor time-series data. Therefore, we investigated whether theseattacks could be implemented earlier in the processing pipe-line, such as on node time-series data. In this example (Fig-ure 5), we manipulated time-series data from HCP (rest ses-sion 1, n = 506) that was parcellated into 268 nodes with theShen atlas42 to falsely enhance prediction of IQ. We selecteda pattern—in this case we arbitrarily selected a low-frequencysinusoid—to add to or subtract from each node’s time course,scaled by a factor proportionalto each participant’s IQ,to increase or decrease correlations between nodes. The re-sulting time-series data (median r = 0.994) and the calculatedconnectomes (mean r = 0.991) were very similar, but thePATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756ArticlellOPEN ACCESSFigure 3. Performance enhancement attacksonly cause minor changes to connectomes(A) Data are enhanced to predict IQ measure-ments in ABCD, HCP, and PNC for 100 iterationsof different enhancement patterns (all 100 itera-tions are shown as points; there is a lot of overlapbetween iterations). The x axis reflects themean absolute value of the enhancement patternadded at the edge level (i.e., the absolute mean ofthe enhancement pattern across all participantsfor the 20% of edges we altered). At x = 0, there isno enhancement. As a larger enhancementpattern is added,the prediction performanceincreases to r > 0.9,(prediction correlation)although the edge-wise correlation betweenoriginal and enhanced connectomes is stillr z 0.99. In the second row of (A), enhancementattacks are shown to not affect downstream an-alyses, which included a sex classification modeland participantidentification (‘‘fingerprinting’’)for HCP.(B)Identification rates by subnetwork betweenRest1 original/enhanced and Rest2 connectomes inHCP.(C) Several graph metrics, including strength, as-sortativity, and clustering coefficient, were calcu-lated for the original connectomes and enhancedconnectomes, using the largest scale of enhancement presented in (A). The correlation between these metrics for original and enhanced connectomes ispresented in (C), with error bars representing the SD of the correlation across participants.prediction of IQ drastically increased using the enhanced data(Figure 5). Additional representative node time-series tracesare shown in the Figure S3. The efficacy of performanceenhancement on node time-series data showed data manipu-lations are not exclusive to preprocessed connectomes andthat more complex algorithms may be able to manipulateraw data to achieve desired results.Adversarial noise degrades connectome-based modelaccuracyIn contrast to the previous sections where data were manipu-lated to falsely enhance performance, other manipulations aredesigned to decrease prediction accuracy, most notably adver-sarial noise attacks.19 Adversarial noise attacks have been effec-tively implemented in numerous fields,20–26 where only minormanipulations to the test data (i.e., X data) are required to changethe prediction. We set out to determine the extent to which con-nectome-based models are susceptible to adversarial attacks asa measure of trustworthiness.this method required knowledge ofFor a SVM classifier of self-reported sex in ABCD, HCP, andPNC, we used a gradient-based method21 to create adversarialnoise at the time of model testing on the basis of the model pa-rameters. Notably,themodel parameters. For each fold of our 10-fold cross-valida-tion, a single sex-specific adversarial pattern was updatedand added to each test connectome until all connectomeswere classified incorrectly (i.e., accuracy = 0; Figure 6). Asthe mean absolute value (on the edge level) of the attackincreased, more connectomes were classified incorrectly (Fig-ure 6). Even when manipulating the data to achieve 0% accu-racy, the original and adversarial connectomes showed verystrong edge-wise correlations (r (cid:1) 0.99). We repeated this anal-ysis for logistic regression and found a similar trend (Figure S4).Hence, even very subtle adversarial attacks can completelydegrade connectome-based modeling pipelines.Adversarial noise is small and does not significantlychange a connectomeAll analyses in this section were performed using the minimumadversarial noise magnitude required for 0% accuracy.Crucially, we found that the mean absolute value of adversarialnoise (whole-connectome level) required to trick connectome-based classifiers was small (between 0.01 and 0.03 to achieve0% accuracy). As a result of the small magnitude, addition ofthe adversarial noise caused no apparent visual differencesbetween the real and corrupted connectomes, and the distri-bution of edge values was nearly identical (Figure 6). More-over, as adversarial noise was smallin magnitude, real andcorrupted connectomes maintained a high edge-wise correla-tion (r values z 0.99). We also investigated the mean absolutevalue of the adversarial noise across 10 canonical resting-statenetworks, previously defined with the Shen 268 atlas.51,52 Thescale of the adversarial attacks was small across each subnet-work (Figure 7A), though notably the within-network noisevalues (diagonal elements of matrices in Figure 7A) were signif-icantly larger than between networks (p < 0.004 for all threedatasets).Next, we investigated individual differences in both realand adversarial connectomes through functional connectomefingerprinting. We performed the fingerprinting identification pro-cess in each fold of our 10-fold cross-validation, so each identi-fication procedure included 10% of the 506 participants in theHCP dataset. The identification rate between Rest1 and Rest2connectomes was 96.4% when using unmodified Rest1 dataPatterns 4, 100756, July 14, 2023 5PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSArticleFigure 4. Performance enhancement attacks in the SLIM datasetThis example is shown for prediction of state anxiety in the SLIM dataset with resting-state connectomes and rCPM. In the top row, prediction with the originaldataset shows poor performance (r z 0). In the second row, as in Figure 2, an enhancement pattern proportional to the state anxiety measure can be added torandom edges to enhance performance while maintaining very high correlations between the original and enhanced connectomes (r z 0.99). In the bottom row,an enhancement pattern can be added to specific subnetworks to alter interpretation. Here, we targeted the enhancement pattern to the salience subnetwork,and the resulting coefficients reflect that edges in the salience network dominate the prediction outcome.and 96.3% when using adversarial Rest1 data. There was nosignificant difference in the identification rate for real and adver-sarial Rest1 scans when using any ofthe available tasksto perform fingerprinting (median p value > 0.14 for eachsubnetwork; median overall p = 0.5; Figure 7B). Furthermore,we performed identification between Rest1 original/adversarialand Rest2 original connectomes using only edges correspond-ing to each subnetwork and found very similar identification ratesacross every subnetwork (median p value > 0.38 for each sub-network; median overall p = 0.58; Figure 7C). Overall, adversarialnoise attacks preserved the individual uniqueness of connec-tomes in a fingerprinting paradigm at both the whole-brain leveland the subnetwork level.DISCUSSIONIn this study, we demonstrated that three types of connectome-based models (rCPM, SVM, logistic regression) were fooled bysmall and simple data manipulations, thus suggesting a needfor improvements in trustworthiness. We introduced ‘‘enhance-ment attacks,’’ which falsely increased prediction performancefrom r = 0–0.2 to r > 0.9, and we also applied adversarial noiseattacks to reduce model accuracy from (cid:1)80% to 0%. Despitethe large differences in performance between the original andmanipulated data, the edges were highly correlated (r z 0.99)6 Patterns 4, 100756, July 14, 2023and downstream analyses (identification rate, sex classification,graph topology) were unaffected. Overall, nearly any desiredprediction performance could be obtained via minor data manip-ulations, which presents a concern for a wide range of settingsfrom scientific integrity to potential down-the-line clinicalapplications.Enhancement attacks falsely increase performance of ma-chine learning models via data manipulation, and they are distinctfrom adversarial noise attacks in both implementation and moti-vation. Although adversarial noise attacks alter only the test datato degrade model performance, enhancement attacks alter theentire dataset (i.e., training and test data) to falsely improve per-formance.In academic settings, a researcher might useenhancement attacks to make prediction performance higherand more publishable, or to support an unfounded neuroscien-tific claim. Similarly, in a commercial setting, a start-up coulduse enhancement attacks to deceive investors and increase thevaluation of its company. In contrast, one might use adversarialnoise to evade a model in a real-world application of machinelearning, such as to bypass computer virus detection software.With enhancement attacks, we demonstrated that connec-tome data can be manipulated to falsely enhance performanceor provide evidence for a baseless interpretation. Althoughsharing data, especially processed data, has many benefits,56–58data sharing is not a universal safeguard against dataPATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756Articlemanipulations. Beyond enhancement attacks affecting individ-ual studies, if enhanced data are shared on openly available re-positories, independent researchers could unknowingly publishresults with enhanced data and never be aware of any manipu-lation. This could potentially set forth a vicious cycle in whichperformance benchmarks are overly optimistic, leading to incen-tives to overfit in other studies or not publish lower performingmodels. Overall, an enhanced dataset circulating within the neu-roimaging community would cause wasted resources andpossibly harmful neuroscientific conclusions. We still advocatefor sharing data,59 including preprocessed data (when appro-priate) to lower barriers to entry and minimize duplication ofeffort. But the potential for sharing data manipulated in undetect-able ways should be acknowledged.Furthermore, adversarial noise attacks degrade the accuracyof classification models of self-reported sex. These attackswould primarily occur in clinical applications of(e.g.,into a diagnostic category),misclassification of an individualwhich are currently limited by other existing roadblocks.60,61 Still,adversarial noise attacks illustrate the fragility of connectome-based predictive models by finding the minimum manipulationrequired to change classification outcomes.fMRIllOPEN ACCESSFigure 5. Time series performance enhance-ment attacksNode time-series data can be manipulated byadding a pattern with amplitude proportional to theIQ of each participant to increase/decrease thecalculated functional connectivity between specificnodes. In this case, we chose a sinusoid pattern toadd to the time-series data. A representative node isshown in this figure. The correlations betweenoriginal and enhanced time-series (r = 0.988) andresulting connectome (r = 0.985) data are very high,despite large differences in prediction performance(r = 0.15 vs. r = 0.77).See also Figure S3.enhancementature. One proposed driving factor is thehigh dimensionality of data.62,63 Similarly,the high dimensionality of connectomedata is likely contributing to the effective-ness ofattacks. Forinstance, in an extreme case, considera dataset with only one feature; the sin-gle feature would need to be modifiedgreatly to establish a strong pattern inthe data and thus enhance performance.However, with thousands to tens of thou-sands of features, such as in connec-tome data, each feature can be manipu-lated in a very minor way so that thechanges to the data are not suspiciousor noticeable. Although each individualmanipulated feature is nearly identicalto the unmodified feature, the effects ofmodifying many features are cumulative.As we showed in this work, minor manip-ulations via enhancement attacks are small enough to pre-serve the individuality of each participant’s connectome butlarge enough to falsely establish strong multivariate patternsthus leading to falsely improvedin high dimensions,performance.An important remaining question is how to make connec-tome-based pipelines (and general scientific machine learningpipelines) more trustworthy. In the machine learning literature,defenses to adversarial attacks, called ‘‘adversarial de-fenses,’’64–66 have been widely studied. However, the samestrategies for defending against enhancement attacks maynot apply because enhancement attacks alter the entire dataset(i.e., not just the test data), which makes distinguishing betweentrue signal and false manipulations more difficult. Two ways toreduce the risk for enhancement attacks are (1) data prove-nance tracking with a tool such as DataLad67 or blockchains68and (2) generalization of models to external datasets. Yetmany neuroimaging studies do not include either of these twomethods. In addition, adherence to ethical principles, rigorousstudy designs, and awareness of the limitations of connec-tome-based models are helpful strategies to prevent datamanipulation.The underlying factors behind the existence of adversarialexamples have been widely studied in computer science liter-There are several final methodological considerations of ourwork. First, trustworthiness has multiple definitions in machinePATTER 100756Patterns 4, 100756, July 14, 2023 7Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSArticleFigure 6. Adversarial attack accuracy as afunction of magnitude of attack for our threedatasets and SVM classifiers of self-reportedsexThe x axis reflects an increase in the size of the at-tacks, represented as the mean absolute valueof the added noise pattern, while the y axis showsaccuracy on the manipulated data. The experimentis repeated for 100 different random seeds and SDsacross the 100 iterations are shown (very smallSDs). At three points for the HCP line, representativeconnectomes are shown, as well as histograms withedge values for the original connectomes, adver-sarial connectomes, and adversarial noise pattern.Above each representative connectome is theedge-wise correlation with the original connectomeSee also Figure S4.learning, such as connectome-basedmodels. Although trust is just one aspectof ethics in machine learning, it can com-plement ethical benchmarks69–71thathave been designed to ameliorate otherrampantin machinelearning models.72,73 Future efforts toimprove trustworthiness will be neces-sary to ensure fair and ethical machine learning practices inneuroimaging.ethicalissuesEXPERIMENTAL PROCEDURESResource availabilityLead contactRequests for further information and resources should be directed to the leadcontact, Matthew Rosenblatt (matthew.rosenblatt@yale.edu).Materials availabilityThis study did not generate new unique reagents.Data and code availabilityd All four datasets used in this study are open-source: ABCD (NIMH DataArchive, https://nda.nih.gov/abcd),34 HCP (ConnectomeDB database,https://db.humanconnectome.org),35 PNC (dbGaP Study, accessioncode: phs000607.v3.p2, https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs000607.v3.p2),36and SLIM (INDI,http://fcon_1000.projects.nitrc.org/indi/retro/southwestuni_qiu_index.html).37 Data collection was approved by the relevant ethics reviewboard for each of the four datasets.d BioImage Suite tools used for processing can be accessed at (https://bioimagesuiteweb.github.io/alphaapp/). MATLAB scripts for trust ana-lyses are available on GitHub (https://github.com/mattrosenblatt7/trust_connectomes) and Zenodo74 (https://doi.org/10.5281/zenodo.7750583).DatasetsWe used classification and regression models in four open-source data-sets—the ABCD study,34 the HCP,35 the PNC,36 and the SLIM study37—to evaluate the robustness of connectome-based models to several stylesof adversarial attacks. These datasets were selected because they arecommonly used, relatively large, open-source fMRI datasets. Althoughmany other fMRI datasets exist, these four datasets are representative ofthe field and allow us to evaluate enhancement and adversarial attacksin various scenarios. The first three datasets (ABCD, HCP, and PNC)were used to demonstrate the effectiveness of enhancement andlearning, but we define it as robustness to data manipulations.In addition, enhancement attacks could be applied to corruptany statistical analysis or machine learning models with othermodalities, but the high dimensionality of connectome-basedmachine learning models makes them a prime target for incon-spicuous enhancement manipulations. As specific patternsmust be added to the data to cause performance enhance-ment, enhancement attacks do not extend to cross-datasetpredictions. For adversarial attacks on linear models, the noisegeneration method is proportional to model coefficients andcan be directly calculated. The adversarial attack method pre-sented in this paper also requires knowledge of the model pa-rameters, but more advanced attacks likely can achieve similarperformance without access to model parameters, such as bylearning a surrogate model on another dataset.21 In addition,in the present results, SVM is more easily attacked than logis-tic regression; however,logistic regression suffers from alower baseline accuracy (Figure S4). Finally, this study onlyexamined trust in connectome-based models and is not anexhaustive test of all neuroimaging modalities and models,though the methodology in this framework can apply to anymachine learning pipeline, especially those using high-dimen-sional data.The neuroimaging community is beginning to recognize andexplore the issues concerning ethics in machine learning, witha particular focus on bias in datasets and connectome-basedmodels.13–15 Trust is distinct from bias and represents anequally important, yet widely overlooked, facet of ethics inneuroimaging models. Whereas bias describes performancediscrepancies due to a static trait, trust involves manipulatingthe data to cause a different outcome. The ability to easilymanipulate data to completely change results underscoresthe need for improving trustworthiness of scientific machine8 Patterns 4, 100756, July 14, 2023PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756ArticlellOPEN ACCESSFigure 7. Downstream effects of adversarialnoise attacks(A) Breakdown of SVM adversarial noise into sub-networks. Brighter colors reflect higher mean ab-solute value of noise in that subnetwork.(B) Identification rates in original and adversarialconnectomes in the HCP dataset. The original oradversarial Rest1 scans were compared to con-nectomes in another session (Rest2) or task. Theconnectome with the highest edge-wise correlationwas selected as the predicted identity. The errorbars represent the SD of identification rate across100 random seeds.(C) Using original or adversarial Rest1 scans, weidentified participants on the basis of their correla-tions with the original Rest2 scans. For this portion,we used only a specific subset of edges corre-sponding to each subnetwork to predict the identity.adversarial attacks in prediction of IQ and self-reported sex, respectively.The SLIM dataset was used only to illustrate an example of enhancementof a clinically relevant prediction (state anxiety).The ABCD first release consists of 4,524 participants ranging from 9 to 10years old in the United States from 21 different acquisition sites. The HCP900 subjects release has imaging data from 897 healthy adults ages 22–35years in the United States. The PNC dataset first release contains data fromyouth ages 8–21 years in the greater Philadelphia area, with multimodal neuro-imaging data collected in 1,000 participants. The SLIM dataset has 595 healthyyoung adults ages 17–27 years in China.ProcessingFor the ABCD and PNC datasets, fMRI data were motion corrected and the Shenatlas was warped to single participant space as previously described.38,39 For theHCP dataset, we started with the minimally preprocessed HCP data.40 Furtherpreprocessing steps were performed using BioImage Suite41 and are the sameacross studies. Several covariates of no interest were regressed from partici-pants’ functional data including linear and quadratic drifts, mean cerebrospinalfluid signal, mean white matter signal, and mean global signal. For additional con-trol of possible motion-related confounds, a 24-parameter motion model(including six rigid body motion parameters, six temporal derivatives, and theseterms squared) was regressed from the data. The data were temporallysmoothed with a Gaussian filter (approximate cutoff frequency = 0.12 Hz). Wethen applied a canonical gray matter mask defined in common space, so onlyvoxels in the gray matter were used in further calculations. Denoised data wereparcellated into 268 nodes using the Shen atlas.42 Next, the mean time coursesof each node pair were correlated, and correlation coefficients were Fisher trans-formed, generating a connectome for each participant. For the HCP dataset, con-nectomes for each phase encoding (i.e., RL and LR) were calculated indepen-dently and then averaged together. For the SLIM dataset, preprocessedconnectomes were downloaded, with preprocessing steps described in.37 Afterexcluding participants for excessive motion (>0.2 mm) and missing nodes due tolack of full brain coverage, 3,362 individuals in the ABCD dataset, 506 in the HCPdataset, 561 in the PNC dataset, and 445 in the SLIM dataset were retained.Baseline regression modelsFor all baseline regression models, we trained ridge-regression connectome-based predictive models (rCPM)44 in MATLAB (The MathWorks) with 10-foldcross-validation and a nested 10-fold cross-validation to select the L2 regula-rization parameter, l. For feature selection, we correlated each edge withthe phenotype of interest and picked the top 10%of edges with the lowest p values. In the nestedfolds, we performed a grid search for l. We usedMATLAB’s default settings for a grid search over l(detailed description: https://www.mathworks.com/help/stats/lasso.html). In sum, we searchedover a geometric sequence with the maximum being the largest l that givesa nonnull model and the minimum being lmax*10(cid:3)4. l is then selected as thelargest l for which the mean squared error (MSE) is within 1 standard errorof the minimum MSE. With this method, the l is generally very high, oftenl z 100–150 (log[l] z 4.6–5). To explore the effectiveness of enhancementattacks across a wide variety of l, we also included a parameter sensitivityanalysis (Figure S1).For Table S2, we compared ridge regression models and neural networks inHCP resting-state data using 100% of available features in both cases andenhancing 100% of edges, as opposed to 20% in the analyses in the maintext. We used ridge regression with a regularization parameter of 1,000 andall default parameters for the neural network (MLPRegressor function in sci-kit-learn75). These results demonstrate that enhancement attacks at variousscales are still effective in neural networks. Furthermore, in theory, neural net-works should be able to learn non-linear enhancement patterns, whereas ridgeregression models cannot.For ABCD, HCP, and PNC, the phenotype of interest was a fluid intelligence(IQ) measurement. For ABCD, Raven’s progressive matrices76 were used,scaled by age (mean 6.31, SD 2.56, range 1–17, median 6), and the same mea-sure, though not scaled by age, was used for HCP (mean 17.54, SD 4.45, range5–24, median 19). For PNC, IQ was assessed using the Penn Matrix Reasoningtest (mean 12.28, SD 4.04, range 0–23, median 12).77 For SLIM, the phenotypeof interest was the state anxiety score, as assessed by the State-Trait AnxietyInventory55 (mean 35.66, SD 8.28, range 20–65, median 35).The main metric we used to determine prediction performance was Pear-son’s r between original and predicted phenotypes. We also reported thecross-validation R2, called q2,47 which is defined asq2 = 1 (cid:3) MSEðby; yÞMSEðy; yÞ = 1 (cid:3) NMSEBaseline classification modelsWe trained both SVM (linear kernel) and logistic regression models in MATLABto predict self-reported sex in ABCD, HCP, and PNC. The self-reported sexof participants by dataset was: ABCD (1,653 female, 1,609 male), HCP(270 female, 236 male), and PNC (318 female, 244 male). Models weretrained with 10-fold cross-validation, with nested 5-fold cross-validationto select an L2 regularization parameter. For the L2 regularization hyperpara-the ‘‘fitclinear’’meter search, we used MATLAB’s default search forPatterns 4, 100756, July 14, 2023 9PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSfunction (https://www.mathworks.com/help/stats/fitclinear.html), which isBayesian optimization, as described in (https://www.mathworks.com/help/stats/bayesianoptimization.html). Essentially, it uses Bayesian optimizationto search within the range of [1e-5/number of training samples, 1e5/numberof training samples]. We used accuracy as our primary evaluation metric,though we also reported sensitivity and specificity in Table S1.Connectome enhancementWe enhanced IQ prediction in ABCD, HCP, and PNC. To enhance connectomedata, we first Z-scored the phenotypic measurements.yz) y (cid:3) ysyThen, we randomly selected 20% of all edges e to manipulate. For each ofthe selected edges, we added a value with magnitude proportional to eachparticipant’s Z-scored IQ.ei;j ) ei;j ± k (cid:4) yz;j;where ei,j represents edge i for participant j, and yz,j is the Z-scored phenotypefor participant j.Whether we added or subtracted each value was randomly determined foreach edge. The results presented in Figure 3 used k = {0, 0.004, 0.007, 0.01,0.014, 0.02, 0.03}. k = 0 was used as a reference and means that no enhance-ment was performed.After injecting the enhancement pattern into each connectome, rCPMmodels were re-trained, as described above, to predict each phenotype.Because we injected patterns proportional to the phenotype of interest, wewould expect performance to be falsely enhanced. We repeated enhancementfor each value of k for 100 different random seeds and recorded r and q2 ateach iteration.Enhancement downstream analysesIn addition to r and q2, we evaluated enhanced connectomes in several otheranalyses—self-reported sex classification,functional connectome finger-printing, and investigation of graph properties—to determine their similaritywith original connectomes.For self-reported sex classification, we used the data that was enhanced forIQ prediction to train a SVM classifier as described in baseline classificationmodels. At each different scale of the enhancement k, we compared the sexclassification performance to that of the unaltered dataset, using accuracyas the main metric.We also performed functional connectome fingerprinting50,51 following themethod of Finn et al.51 We performed fingerprinting only in the HCP datasetbecause it has two resting-state scans. A total of 50 or 51 participants wereused at a time for fingerprinting evaluation, corresponding to all the connec-tomes in a single fold of the 10-fold cross-validation enhancement pipeline(10% of 506 HCP participants). With 50 participants, the identification rate ofrandom guessing is 2%. To predict identity, we first calculated the edge-wise correlation of each participant’s Rest1 connectome with their Rest2 con-nectome, and the predicted identity of each participant corresponded to theRest2 connectome with the highest edge-wise correlation with their Rest1connectome. We repeated the fingerprinting process using both originalRest1 connectomes and Rest1 connectomes that had been enhanced for IQprediction.In a further implementation of fingerprinting, we identified participants usingonly edges that are part of selected subnetworks, as previously defined withthe Shen 268 atlas.51,52 10 networks were defined on the basis of nodes (Fig-ure S2): medial-frontal (MF), fronto-parietal (FP), default mode (DMN), motor(MOT), visual I (VI), visual II (VII), visual association (VAs), salience (SAL),subcortical (SC), cerebellum (CBL). On the basis of these 10 networks, wedefined 55 subnetworks, where each subnetwork was defined as edgesbelonging to each pair of networks. For example, 10 subnetworks involvedthe MF network: MF-MF, MF-FP, MF-DMN, MF-MOT, MF-VI, MF-VII,MF-VAs, MF-SAL, MF-SC, MF-CBL. The fingerprinting procedure followedthe same process as above, except edge-wise correlations were calculatedonly with edges belonging to one of the 55 subnetworks.10 Patterns 4, 100756, July 14, 2023ArticleMoreover, we evaluated several graph properties for the positive edgesonly, including strength, assortativity, and clustering coefficient, in the originaland enhanced connectomes.53 All three of these properties are node-levelmetrics, and we evaluated the similarity between original and enhanced con-nectomes by correlating the node-level metrics for each of these measures.Strength is the sum of edge weights for each node. Assortativity measureshow similar the strengths are between connected nodes. The clustering coef-ficient is the mean weight of triangles for each node.53Targeted enhancementIn an extension of the above attack, we constrained the injected enhancementpattern to a specific resting-state network instead of a random subsetof edges. We performed this enhancement in the SLIM37 dataset (n = 445) topredict state anxiety scores from the State-Trait Anxiety Inventory.55 Aftermanipulating edges of one network, we repeated model training with rCPM.We also evaluated the model coefficients (averaged over the 10 folds ofcross-validation) to assess how changing a specific network altered the distri-bution of coefficients across networks.Time-series enhancementWe used HCP node time-series data (268-node Shen atlas; 1,200 time points)and prediction of IQ as an example of time-series enhancement. We arbitrarilyselected an enhancement pattern as a sinusoid with four periods across the1,200 time points. For each participant, the enhancement pattern (sinusoid)was scaled by a factor proportional to their IQ score and the original correlationof that edge with the IQ scores. Then, for each edge, we found the two nodescorresponding to that particular edge. If the edge was positively correlatedwith IQ, we added the participant-specific enhancement pattern to bothnode time-series. If the edge was negatively correlated with IQ, we addedthe enhancement pattern to one node and subtracted it from the other node.After enhancing the time-series data, we computed the connectomes by takingthe correlation between each pair of nodes and applying the Fisher transform.Then, rCPM models were trained with 10-fold cross-validation to predict IQ.We recorded the similarity (Pearson’s r) between the original and enhancedtime-series data along with similarity between the resulting original and enhancedconnectomes. Although there may be other more effective strategies to enhancetime-series data, we performed this simple attack as a proof of concept.Adversarial noise attacksIn both classifiers (SVM, logistic regression), we used a gradient-based attackfollowing the method of Biggio et al.21 The attacks occurred on the test dataat the time of model testing and are ‘‘white-box’’ attacks, meaning that theyrequired access to the model parameters. Let our decision function be repre-sented by g(x) with input features x. A participant was classified as female ifg(x) < 0 and male if g(x) > 0. The goal of the adversarial noise attack was to manip-ulate all the true female (male) connectomes such that the adversarial g(x) > 0(<0). The noise for female and male connectomes were optimized separately.The loss functions wereLf = (cid:3) gðx + nf Þ ðall female connectomesÞandLm = gðx + nmÞ ðall male connectomesÞAdversarial noise was initialized to zeros and iteratively updated on the basisof the gradient:nf=m ) nf=m (cid:3) l dLf=mdnf=m;where l is the step size.For linear SVM and logistic regression, the derivative term was given by:dLf=mdnf=m= Hbfor coefficients b. For these linear models, this process just simplifies to addingadversarial noise proportional to the coefficients.PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756ArticleWe also explored how the adversarial noise was distributed across subnet-works by taking the mean absolute value of the adversarial noise in eachsubnetwork.Adversarial noise downstream analysesWe compared functional connectome fingerprinting rates between originaland adversarial Rest1 connectomes, as previously described. Notably, weperformed fingerprinting with the Rest1 connectomes and each of the eightother tasks in HCP (Rest2, gambling, language, motor, relational, social, work-ing memory, emotion). We also performed subnetwork-specific fingerprinting.StatisticsAs previously described, performance of regression models was quantified withPearson’s r or cross-validation R2, called q2, between measured and predictedphenotypes. Classifiers were assessed by prediction accuracy, sensitivity, andspecificity. For comparisons ofreal and manipulated connectomes, wecomputed Pearson’s r between original and manipulated edges, to which we referto as ‘‘edge-wise correlation’’ throughout this work. To assess the significance ofthe differences in ID rate and accuracy between original and manipulated data, weused McNemar’s test78 and reported the median p value across the 100 iterations.ACKNOWLEDGMENTSThis study was supported by NationalInsititute of Mental Health grantR01MH121095 (obtained by R.T.C. and D.S.). M.R. was supported by the Na-tional Science Foundation Graduate Research Fellowship under grant DGE-2139841. Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflectthe views of the National Science Foundation. R.X.R. was supported by the Na-tional Research Service Award (award 5T32GM100884-09) from the NationalInstitute of General Medicine. M.L.W. was supported by the National Instituteon Drug Abuse (T32DA022975). S.N. was supported by the National Institute ofMental Health (K00MH122372). Data were provided in part by the HCP, WU-Minn Consortium (principal investigators David Van Essen and Kamil Ugurbil;1U54MH091657) funded by the 16 National Institutes of Health institutes andcenters that support the National Institutes of Health Blueprint for NeuroscienceResearch; and by the McDonnell Center for Systems Neuroscience at Washing-ton University. Additional data were provided by the PNC (principal investigatorsHakon Hakonarson and Raquel Gur; phs000607.v1.p1). Support for the collec-tion of these datasets was provided by grant RC2MH089983 awarded to RaquelGur and RC2MH089924 awarded to Hakon Hakonarson. Data obtained from theSLIM study were funded by The National Natural Science Foundation of China(31271087, 31470981, 31571137, 31500885); National Outstanding young peo-ple plan, the Program for the Top Young Talents by Chongqing, the FundamentalResearch Funds for the Central Universities (SWU1509383, SWU1509451); Nat-ural Science Foundation of Chongqing (cstc2015jcyjA10106); Fok Ying Tung Ed-ucation Foundation (151023); the General Financial Grant from the China Post-doctoral Science Foundation (2015M572423, 2015M580767); Special Fundsfrom the Chongqing Postdoctoral Science Foundation (Xm2015037); and KeyResearch for Humanities and Social Sciences of Ministry of Education(14JJD880009). Furthermore, some data used in the preparation of this articlewere obtained from the ABCD Study (abcdstudy.org), held in the NIMH DataArchive (NDA). This is a multisite, longitudinal study designed to recruit morethan 10,000 children ages 9–10 years and follow them over 10 years into earlyadulthood. The ABCD study is supported by the National Institutes of Healthand additional federal partners under awards U01DA041022, U01DA041028,U01DA041048, U01DA041089, U01DA041106, U01DA041117, U01DA041120,U01DA041134, U01DA041148, U01DA041156, U01DA041174, U24DA041123,and U24DA041147. A full list of supporters is available at abcdstudy.org. A listingof participating sites and a complete listing of the study investigators can befound at abcdstudy.org/principal-investigators.html. ABCD consortium investi-gators designed and implemented the study and/or provided data but did notnecessarily participate in analysis or writing of this report. This manuscript reflectsthe views of the authors and may not reflect the opinions or views of the NIH orABCD consortium investigators. The ABCD data repository grows and changesover time. The ABCD data used in this report came from NIMH Data Archive dig-ital object identifier 10.15154/1504041.llOPEN ACCESSAUTHOR CONTRIBUTIONSConceptualization, M.R., R.X.R., M.L.W., and D.S.; Methodology, M.R.,M.L.W., and D.S.; Software, M.R. and R.X.R.; Formal Analysis, M.R. andD.S.; Investigation, M.R.; Data Curation, A.S.G., C.H., W.D., R.T.C, and D.S.;Writing—Original Draft, M.R. and D.S.; Writing—Review and Editing, M.R.,R.X.R., M.L.W., W.D., C.H., A.S.G., S.N., and D.S.; Visualization, M.R.,R.X.R., M.L.W., S.N., and D.S.; Supervision, M.L.W., S.N., and D.S.; FundingAcquisition, R.T.C. and D.S.DECLARATION OF INTERESTSThe authors declare no competing interests.INCLUSION AND DIVERSITYWe support inclusive, diverse, and equitable conduct of research.Received: January 6, 2023Revised: March 10, 2023Accepted: April 24, 2023Published: May 15, 2023REFERENCES1. Whelan, R., and Garavan, H. (2014). When optimism hurts: inflated predic-tions in psychiatric neuroimaging. Biol. Psychiatry 75, 746–748. https://doi.org/10.1016/j.biopsych.2013.05.014.2. Gabrieli, J.D.E., Ghosh, S.S., and Whitfield-Gabrieli, S. (2015). Predictionas a humanitarian and pragmatic contribution from human cognitiveneuroscience. Neuron 85, 11–26. https://doi.org/10.1016/j.neuron.2014.10.047.3. Cremers, H.R., Wager, T.D., and Yarkoni, T. (2017). The relation betweenstatistical power and inference in fMRI. PLoS One 12, e0184923. https://doi.org/10.1371/journal.pone.0184923.4. Noble, S., Mejia, A.F., Zalesky, A., and Scheinost, D. (2022). Improving po-wer in functional magnetic resonance imaging by moving beyond cluster-level inference. Proc. Natl. Acad. Sci. USA 119. e2203020119. https://doi.org/10.1073/pnas.2203020119.5. Shen, X., Finn, E.S., Scheinost, D., Rosenberg, M.D., Chun, M.M.,Papademetris, X., and Constable, R.T. (2017). Using connectome-basedpredictive modeling to predict individual behavior from brain connectivity.Nat. Protoc. 12, 506–518. https://doi.org/10.1038/nprot.2016.178.6. Cui, Z., and Gong, G. (2018). The effect of machine learning regression al-gorithms and sample size on individualized behavioral prediction withfunctional connectivity features. Neuroimage 178, 622–637. https://doi.org/10.1016/j.neuroimage.2018.06.001.7. Du, Y., Fu, Z., and Calhoun, V.D. (2018). Classification and prediction ofbrain disorders using functional connectivity: promising but challenging.Front. Neurosci. 12, 525. https://doi.org/10.3389/fnins.2018.00525.8. Rosenberg, M.D., Casey, B.J., and Holmes, A.J. (2018). Prediction com-plements explanation in understanding the developing brain. Nat.Commun. 9, 589. https://doi.org/10.1038/s41467-018-02887-9.9. Song, H., Finn, E.S., and Rosenberg, M.D. (2021). Neural signatures ofattentional engagement during narratives and its consequences for eventmemory. Proc. Natl. Acad. Sci. USA 118. e2021905118. https://doi.org/10.1073/pnas.2021905118.10. Nielsen, A.N., Barch, D.M., Petersen, S.E., Schlaggar, B.L., and Greene,D.J. (2020). Machine learning with neuroimaging: evaluating its applica-tions in psychiatry. Biol. Psychiatry. Cogn. Neurosci. Neuroimaging 5,791–798. https://doi.org/10.1016/j.bpsc.2019.11.007.11. Goldfarb, E.V., Rosenberg, M.D., Seo, D., Constable, R.T., and Sinha, R.(2020). Hippocampal seed connectome-based modeling predicts thefeeling of stress. Nat. Commun. 11, 2650. https://doi.org/10.1038/s41467-020-16492-2.Patterns 4, 100756, July 14, 2023 11PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSArticle12. Yip, S.W., Scheinost, D., Potenza, M.N., and Carroll, K.M.(2019).Connectome-based prediction of cocaine abstinence. Am. J. Psychiatry176, 156–164. https://doi.org/10.1176/appi.ajp.2018.17101147.29. Bucci, E.M. (2018). Automatic detection of image manipulations in theliterature. Cell Death Dis. 9, 400. https://doi.org/10.1038/biomedicals41419-018-0430-3.13. Benkarim, O., Paquola, C., Park, B.-Y., Kebets, V., Hong, S.-J., de Wael,R.V., Zhang, S., Thomas Yeo, B.T., Eickenberg, M., Ge, T., et al. (2021).The cost of untracked diversity in brain-imaging prediction. Preprint atbioRxiv. https://doi.org/10.1101/2021.06.16.448764.30. Cicconet, M., Elliott, H., Richmond, D.L., Wainstock, D., and Walsh, M.(2018). Image Forensics: detecting duplication of scientific images withmanipulation-invariant image similarity. Preprint at arXiv. https://doi.org/10.48550/arXiv.1802.06515.14. Li, J., Bzdok, D., Chen, J., Tam, A., Ooi, L.Q.R., Holmes, A.J., Ge, T., Patil,K.R., Jabbi, M., Eickhoff, S.B., et al. (2022). Cross-ethnicity/race general-ization failure of behavioral prediction from resting-state functional con-nectivity. Sci. Adv. 8, eabj1812. https://doi.org/10.1126/sciadv.abj1812.15. Greene, A.S., Shen, X., Noble, S., Horien, C., Hahn, C.A., Arora, J.,Tokoglu, F., Spann, M.N., Carrio´ n, C.I., Barron, D.S., et al. (2022). Brain–phenotype models failfor individuals who defy sample stereotypes.Nature 609, 109–118. https://doi.org/10.1038/s41586-022-05118-w.16. Brundage, M., Avin, S., Wang, J., Belfield, H., Krueger, G., Hadfield, G.,Khlaaf, H., Yang, J., Toner, H., Fong, R., et al. (2020). Toward trustworthyAI development: mechanisms for supporting verifiable claims. Preprint atarXiv. https://doi.org/10.48550/arXiv.2004.07213.17. Rawal, M., Rawat, S., and Amant. (2021). Recent advances in trustworthyexplainable artificialintelligence: status, challenges and perspectives.IEEE Transactions on Artificial Intelligence 1, 1. https://doi.org/10.1109/TAI.2021.3133846.18. Eshete, B. (2021). Making machine learning trustworthy. Science 373,743–744. https://doi.org/10.1126/science.abi5052.19. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow,I., and Fergus, R. (2013). Intriguing properties of neural networks. Preprintat arXiv. https://doi.org/10.48550/arXiv.1312.6199.20. Goodfellow, I.J., Shlens, J., and Szegedy, C. (2014). Explaining and har-nessing adversarial examples. Preprint at arXiv. https://doi.org/10.48550/arXiv.1412.6572.21. Biggio, B., Corona, I., Maiorca, D., Nelson, B.,(cid:1)Srndi(cid:3)c, N., Laskov, P.,Giacinto, G., and Roli, F. (2013). Evasion attacks against machine learningat test time. In Machine Learning and Knowledge Discovery in Databases(Springer Berlin Heidelberg), pp. 387–402. https://doi.org/10.1007/978-3-642-40994-3_25.22. Demontis, A., Melis, M., Biggio, B., Maiorca, D., Arp, D., Rieck, K., Corona,I., Giacinto, G., and Roli, F. (2019). Yes, machine learning can be moresecure! a case study on android malware detection.IEEE Trans.Dependable Secure Comput. 16, 711–724. https://doi.org/10.1109/TDSC.2017.2700270.23. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. (2021).Natural adversarial examples. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 15262–15271. https://doi.org/10.1109/CVPR46437.2021.01501.24. Paschali, M., Conjeti, S., Navarro, F., and Navab, N.(2018).Generalizability vs. Robustness: investigating medical imaging networksusing adversarial examples. In Medical Image Computing and ComputerAssisted Intervention, pp. 493–501. https://doi.org/10.1007/978-3-030-00928-1_56.25. Finlayson, S.G., Bowers, J.D., Ito, J., Zittrain, J.L., Beam, A.L., andKohane, I.S. (2019). Adversarial attacks on medical machine learning.Science 363, 1287–1289. https://doi.org/10.1126/science.aaw4399.26. Han, X., Hu, Y., Foschini, L., Chinitz, L., Jankelson, L., and Ranganath, R.(2020). Deep learning models for electrocardiograms are susceptibleto adversarial attack. Nat. Med. 26, 360–363. https://doi.org/10.1038/s41591-020-0791-x.27. Finlayson, S.G., Chung, H.W., Kohane,(2018).Adversarial attacks against medical deep learning Systems. Preprint atarXiv. https://doi.org/10.48550/arXiv.1804.05296.I.S., and Beam, A.L.28. Acuna, D.E., Brookes, P.S., and Kording, K.P. (2018). Bioscience-scalereuse. Preprint at bioRxiv.automated detection of figure elementhttps://doi.org/10.1101/269415.12 Patterns 4, 100756, July 14, 202331. Bik, E.M., Casadevall, A., and Fang, F.C. (2016). The prevalence of inap-propriate image duplication in biomedical research publications. mBio 7,e00809-16. https://doi.org/10.1128/mBio.00809-16.32. Fanelli, D. (2009). How many scientists fabricate and falsify research? Asystematic review and meta-analysis of survey data. PLoS One 4,e5738. https://doi.org/10.1371/journal.pone.0005738.33. Al-Marzouki, S., Evans, S., Marshall, T., and Roberts, I. (2005). Are thesedata real? Statistical methods for the detection of data fabrication in clin-ical trials. BMJ 331, 267–270. https://doi.org/10.1136/bmj.331.7511.267.34. Casey, B.J., Cannonier, T., Conley, M.I., Cohen, A.O., Barch, D.M.,Heitzeg, M.M., Soules, M.E., Teslovich, T., Dellarco, D.V., Garavan, H.,et al. (2018). The adolescent brain cognitive development (ABCD) study:imaging acquisition across 21 sites. Dev. Cogn. Neurosci. 32, 43–54.https://doi.org/10.1016/j.dcn.2018.03.001.35. Van Essen, D.C., Smith, S.M., Barch, D.M., Behrens, T.E.J., Yacoub, E.,and Ugurbil, K.; WU-Minn HCP ConsortiumMinn HCP Consortium (2013).The Wu-Minn human connectome Project: an overview. Neuroimage 80,62–79. https://doi.org/10.1016/j.neuroimage.2013.05.041.36. Satterthwaite, T.D., Connolly, J.J., Ruparel, K., Calkins, M.E., Jackson, C.,Elliott, M.A., Roalf, D.R., Hopson, R., Prabhakaran, K., Behr, M., et al.(2016). The Philadelphia Neurodevelopmental Cohort: a publicly availableresource for the study of normal and abnormal brain development in youth.Neuroimage 124, 1115–1119. https://doi.org/10.1016/j.neuroimage.2015.03.056.37. Liu, W., Wei, D., Chen, Q., Yang, W., Meng, J., Wu, G., Bi, T., Zhang, Q.,Zuo, X.-N., and Qiu, J. (2017). Longitudinal test-retest neuroimagingdata from healthy young adults in southwest China. Sci. Data 4, 170017.https://doi.org/10.1038/sdata.2017.17.38. Greene, A.S., Gao, S., Scheinost, D., and Constable, R.T. (2018). Task-induced brain state manipulation improves prediction of individual traits.Nat. Commun. 9, 2807. https://doi.org/10.1038/s41467-018-04920-3.39. Rapuano, K.M., Rosenberg, M.D., Maza, M.T., Dennis, N.J., Dorji, M.,Greene, A.S., Horien, C., Scheinost, D., Todd Constable, R., and Casey,B.J. (2020). Behavioral and brain signatures of substance use vulnerabilityin childhood. Dev. Cogn. Neurosci. 46, 100878. https://doi.org/10.1016/j.dcn.2020.100878.40. Glasser, M.F., Sotiropoulos, S.N., Wilson, J.A., Coalson, T.S., Fischl, B.,Andersson, J.L., Xu, J., Jbabdi, S., Webster, M., Polimeni, J.R., et al.(2013). The minimal preprocessing pipelines for the Human ConnectomeProject. Neuroimage 80, 105–124. https://doi.org/10.1016/j.neuroimage.2013.04.127.41. Joshi, A., Scheinost, D., Okuda, H., Belhachemi, D., Murphy, I., Staib, L.H.,and Papademetris, X. (2011). Unified framework for development, deploy-ment and robust testing of neuroimaging algorithms. Neuroinformatics 9,69–84. https://doi.org/10.1007/s12021-010-9092-8.42. Shen, X., Tokoglu, F., Papademetris, X., and Constable, R.T. (2013).Groupwise whole-brain parcellation from resting-state fMRI data fornetwork node identification. Neuroimage 82, 403–415. https://doi.org/10.1016/j.neuroimage.2013.05.081.43. Biswal, B.B., Mennes, M., Zuo, X.-N., Gohel, S., Kelly, C., Smith, S.M.,Beckmann, C.F., Adelstein, J.S., Buckner, R.L., Colcombe, S., et al.(2010). Toward discovery science of human brain function. Proc. Natl.Acad. Sci. USA 107, 4734–4739. https://doi.org/10.1073/pnas.0911855107.44. Gao, S., Greene, A.S., Constable, R.T., and Scheinost, D.(2019).Combining multiple connectomes improves predictive modeling of pheno-typic measures. Neuroimage 201, 116038. https://doi.org/10.1016/j.neu-roimage.2019.116038.PATTER 100756Please cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756Article45. Weis, S., Patil, K.R., Hoffstaedter, F., Nostro, A., Yeo, B.T.T., and Eickhoff,S.B. (2020). Sex classification by resting state brain connectivity. Cereb.Cortex 30, 824–835. https://doi.org/10.1093/cercor/bhz129.46. Eliot, L., Ahmed, A., Khan, H., and Patel, J. (2021). Dump the ‘‘dimor-phism’’: comprehensive synthesis of human brain studies reveals fewmale-female differences beyond size. Neurosci. Biobehav. Rev. 125,667–697. https://doi.org/10.1016/j.neubiorev.2021.02.026.47. Scheinost, D., Noble, S., Horien, C., Greene, A.S., Lake, E.M., Salehi, M.,Gao, S., Shen, X., O’Connor, D., Barron, D.S., et al. (2019). Ten simplerules for predictive modeling of individual differences in neuroimaging.Neuroimage 193, 35–45. https://doi.org/10.1016/j.neuroimage.2019.02.057.48. Biggio, B., Nelson, B., and Laskov, P. (2012). Poisoning attacks againstsupport vector machines. Preprint at arXiv. https://doi.org/10.48550/arXiv.1206.6389.49. Massey, F.J. (1951). The Kolmogorov-Smirnov test for goodness of fit.J. Am. Stat. Assoc. 46, 68–78. https://doi.org/10.1080/01621459.1951.10500769.50. Miranda-Dominguez, O., Mills, B.D., Carpenter, S.D., Grant, K.A., Kroenke,C.D., Nigg, J.T., and Fair, D.A. (2014). Connectotyping: model basedfingerprinting of the functional connectome. PLoS One 9, e111048.https://doi.org/10.1371/journal.pone.0111048.51. Finn, E.S., Shen, X., Scheinost, D., Rosenberg, M.D., Huang, J., Chun,M.M., Papademetris, X., and Constable, R.T. (2015). Functional connec-tome fingerprinting: identifying individuals using patterns of brain connec-tivity. Nat. Neurosci. 18, 1664–1671. https://doi.org/10.1038/nn.4135.52. Noble, S., Spann, M.N., Tokoglu, F., Shen, X., Constable, R.T., andScheinost, D. (2017). Influences on the test–retest reliability of functionalconnectivity MRI and its relationship with behavioral utility. Cereb.Cortex 27, 5415–5429. https://doi.org/10.1093/cercor/bhx230.53. Rubinov, M., and Sporns, O. (2010). Complex network measures of brainconnectivity: uses and interpretations. Neuroimage 52, 1059–1069.https://doi.org/10.1016/j.neuroimage.2009.10.003.54. Luo, W., Greene, A.S., and Constable, R.T. (2021). Within node connectiv-ity changes, not simply edge changes, influence graph theory measures infunctional connectivity studies of the brain. Neuroimage 240, 118332.https://doi.org/10.1016/j.neuroimage.2021.118332.55. Spielberger, C.D. (1983). Manual for the State-Trait Anxiety Inventory (FormY) (‘‘self-Evaluation Questionnaire’’) (Consulting Psychologists Press).56. Cameron, C., Yassine, B., Carlton, C., Francois, C., Alan, E., Andra´ s, J.,Budhachandra, K., John, L., Qingyang, L., Michael, M., et al. (2013). Theneuro bureau preprocessing initiative: open sharing of preprocessed neu-roimaging data and derivatives. Front. Neuroinform. 7. https://doi.org/10.3389/conf.fninf.2013.09.00041.57. Mennes, M., Biswal, B.B., Castellanos, F.X., and Milham, M.P. (2013).Making data sharing work: the FCP/INDI experience. Neuroimage 82,683–691. https://doi.org/10.1016/j.neuroimage.2012.10.064.58. Markiewicz, C.J., Gorgolewski, K.J., Feingold, F., Blair, R., Halchenko,Y.O., Miller, E., Hardcastle, N., Wexler, J., Esteban, O., Goncalves, M.,et al. (2021). OpenNeuro: an open resource for sharing of neuroimagingdata. Preprint at bioRxiv. https://doi.org/10.1101/2021.06.28.450168.59. Horien, C., Noble, S., Greene, A.S., Lee, K., Barron, D.S., Gao, S., O’Connor,D., Salehi, M., Dadashkarimi, J., Shen, X., et al. (2021). A hitchhiker’s guideto working with large, open-source neuroimaging datasets. Nat. Hum.Behav. 5, 185–193. https://doi.org/10.1038/s41562-020-01005-4.60. Dadi, K., Rahim, M., Abraham, A., Chyzhyk, D., Milham, M., Thirion, B.,and Varoquaux, G.; Alzheimer’s Disease Neuroimaging Initiative (2019).Benchmarking functional connectome-based predictive models forresting-state fMRI. Neuroimage 192, 115–134. https://doi.org/10.1016/j.neuroimage.2019.02.062.llOPEN ACCESS62. Gilmer, J., Metz, L., Faghri, F., Schoenholz, S.S., Raghu, M., Wattenberg,M., and Goodfellow, I. (2018). The relationship between high-dimensionalgeometry and adversarial examples. Preprint at arXiv. https://doi.org/10.48550/arXiv.1801.02774.63. Chattopadhyay, N., Chattopadhyay, A., Gupta, S.S., and Kasper, M.(2019). Curse of dimensionality in adversarial examples.In 2019International Joint Conference on Neural Networks (IJCNN), pp. 1–8.https://doi.org/10.1109/IJCNN.2019.8851795.64. Meng, D., and Chen, H. (2017). MagNet: a two-pronged defense against ad-versarial examples. In Proceedings of the 2017 ACM SIGSAC Conferenceon Computer and Communications Security CCS ’17 (Association forComputing Machinery), pp. 135–147. https://doi.org/10.1145/3133956.3134057.65. Qiu, S., Liu, Q., Zhou, S., and Wu, C. (2019). Review of artificial intelligenceadversarial attack and defense technologies 9, 909. https://doi.org/10.3390/app9050909.66. Zhang, Y., and Liang, P. (2019). Defending against whitebox adversarialattacks via randomized discretization. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and StatisticsProceedings of Machine Learning Research, K. Chaudhuri and M.Sugiyama, eds. (PMLR), pp. 684–693.67. Halchenko, Y., Meyer, K., Poldrack, B., Solanky, D., Wagner, A., Gors, J.,MacFarlane, D., Pustina, D., Sochat, V., Ghosh, S., et al. (2021). DataLad:distributed system for joint management of code, data, and their relation-ship. J. Open Source Softw. 6, 3262. https://doi.org/10.21105/joss.03262.68. Bell, J., LaToza, T.D., Baldmitsi, F., and Stavrou, A. (2017). AdvancingOpen Science with Version Control and Blockchains. In 2017 IEEE/ACM12th International Workshop on Software Engineering for Science(SE4Science), pp. 13–14. https://doi.org/10.1109/SE4Science.2017.11.69. Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson,B., Spitzer, E., Raji, I.D., and Gebru, T. (2019). Model cards for model re-porting. In Proceedings of the Conference on Fairness, Accountability,and Transparency FAT* ’19 (Association for Computing Machinery),pp. 220–229. https://doi.org/10.1145/3287560.3287596.70. Raji, I.D., and Yang, J. (2019). About ML: annotation and benchmarking onunderstanding and transparency of machine learning lifecycles. Preprint atarXiv. https://doi.org/10.48550/arXiv.1912.06166.71. Jiang, H., Kim, B., Guan, M., and Gupta, M. (2018). To trust or not to trust AInformation Processing Systems,In Advances in Neuralclassifier.pp. 5541–5552.72. Buolamwini, J., and Gebru, T. (2018). Gender shades: intersectional accu-racy disparities in commercial gender classification. In Proceedings of the1st Conference on Fairness, Accountability and Transparency Proceedingsof Machine Learning Research (PMLR), pp. 77–91.73. Turner Lee, N. (2018). Detecting racial bias in algorithms and machinelearning. J. Inf. Commun. Ethics Soc. 16, 252–260. https://doi.org/10.1108/JICES-06-2018-0056.74. Rosenblatt, M. (2023). Connectome-based machine learning models arevulnerable to subtle data manipulations. v1.0.0. https://doi.org/10.5281/zenodo.7750583.75. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830.76. Bilker, W.B., Hansen, J.A., Brensinger, C.M., Richard, J., Gur, R.E., andGur, R.C. (2012). Development of abbreviated nine-item forms of theRaven’s standard progressive matrices test. Assessment 19, 354–369.https://doi.org/10.1177/1073191112446655.77. Moore, T.M., Reise, S.P., Gur, R.E., Hakonarson, H., and Gur, R.C. (2015).Psychometric properties of the Penn computerized neurocognitive battery.Neuropsychology 29, 235–246. https://doi.org/10.1037/neu0000093.61. Specht, K. (2019). Current challenges in translational and clinical fMRI andfuture directions. Front. Psychiatry 10, 924. https://doi.org/10.3389/fpsyt.2019.00924.78. McNemar, Q. (1947). Note on the sampling error of the difference betweencorrelated proportions or percentages. Psychometrika 12, 153–157.https://doi.org/10.1007/BF02295996.PATTER 100756Patterns 4, 100756, July 14, 2023 13