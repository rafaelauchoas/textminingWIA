Artificial Intelligence 246 (2017) 53–85Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintHierarchical semi-Markov conditional random fields for deep recursive sequential dataTruyen Tran a,∗a Center for Pattern Recognition and Data Analytics, Deakin University Geelong, Australiab Adobe Research, Adobe, USA, Dinh Phung a, Hung Bui b, Svetha Venkatesh aa r t i c l e i n f oa b s t r a c tArticle history:Received 20 January 2015Received in revised form 12 February 2017Accepted 14 February 2017Available online 24 February 2017Keywords:Deep nested sequential processesHierarchical semi-Markov conditional random fieldPartial labellingConstrained inferenceNumerical scaling1. IntroductionWe present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of linear-chain conditional random fields to model deep nested Markov processes. It is parameterised as a conditional log-linear model and has polynomial time algorithms for learning and inference. We derive algorithms for partially-supervised learning and constrained inference. We develop numerical scaling procedures that handle the overflow problem. We show that when depth is two, the HSCRF can be reduced to the semi-Markov conditional random fields. Finally, we demonstrate the HSCRF on two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. The HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.© 2017 Elsevier B.V. All rights reserved.Modelling hierarchical depth in complex stochastic processes is important in many application domains. In a deep hier-archy, each level is an abstraction of lower level details [1–4]. This paper studies recursively sequential processes, in that each level is a sequence and each node in a sequence can be decomposed further into a sub-sequence of finer grain [2].Consider, for example, a frequent activity performed by human ‘eat-breakfast’. It may include a series of more specific activities like ‘enter-kitchen’, ‘go-to-cupboard’, ‘take-cereal’, ‘wash-dishes’ and ‘leave-kitchen’. Each specific activity can be decomposed into finer details. Similarly, in natural language processing (NLP) syntax trees are inherently hierarchical. In a partial parsing task known as noun-phrase (NP) chunking [5], there are three syntactic levels: the sentence, noun-phrases and part-of-speech (POS) tags. In this setting, the sentence is a sequence of NPs and non-NPs, and each phrase is a sub-sequence of POS tags.A popular approach to deal with hierarchical data is to build a cascaded model where each level is modelled separately, and the output of the lower level is used as the input of the level right above it (e.g. see [6]). For instance, in NP chunking this approach first builds a POS tagger and then constructs a chunker that incorporates the output of the tagger. This approach is sub-optimal because the POS tagger takes no information of the NPs and the chunker is not aware of the reasoning of the tagger. In contrast, a noun-phrase is often very informative to infer the POS tags belonging to the phrase. As a result, this layered approach may suffer from the so-called cascading error problem in that errors introduced from the lower layer propagate to higher levels.* Corresponding author.E-mail address: truyen.tran@deakin.edu.au (T. Tran).http://dx.doi.org/10.1016/j.artint.2017.02.0030004-3702/© 2017 Elsevier B.V. All rights reserved.54T. Tran et al. / Artificial Intelligence 246 (2017) 53–85A more holistic approach is to build a joint representation of all the levels. Formally, given a data sequence z we need to model and infer about the deep, nested semantic x. The main problem is to choose an appropriate representation of xso that inference can be efficient. An important class of representation is hierarchical hidden Markov model (HHMM) [2]. An HHMM is a nested hidden Markov network (HMM) in the sense that each state is also a sub HMM. Although HMMs represent only first-order Markov processes, HHMMs offer higher-order interaction. HHMMs are generative models with joint distribution Pr(x, z), where the data generating distribution Pr(z | x) must be simplified for efficient inference about the semantic Pr(x | z). An alternative is to model the discriminative distribution Pr(x | z) directly without modelling the data Pr(z). This can be more effective since arbitrary long-range and interdependent data features can be incorporated into the model.The most popular class of probabilistic structured output methods are conditional random fields (CRFs) [7], but the early models are flat. Deep variants have been introduced the past decade, including dynamic CRFs (DCRF) [8], hierarchical CRFs [9,10]), and stacked CRFs [11]. However, these methods require a fixed pre-defined hierarchy, and thus are not suitable for problems with automatically inferred topologies.To this end, we construct a novel discriminative model called Hierarchical Semi-Markov Conditional Random Field (HSCRF).1The HSCRF offers nested semantic similar to that by the HHMM but is parameterised as an undirected log-linear model. The HSCRF generalises linear-chain CRFs [7] and semi-Markov CRFs [13].To be more concrete, let us return to the NP chunking example. The problem can be modelled as a three-level HSCRF, where the root represents the sentence, the second level the NP process, and the bottom level the POS process. The root and the two processes are conditioned on the sequence of words in the sentence. Under discriminative modelling, rich contextual information can be simply encoded as features including starting and ending of a phrase, phrase length, and distribution of words falling inside the phrase can be effectively encoded. On the other hand, such encoding is much more difficult for HHMMs.We then proceed to address important issues. First, we show how to represent HSCRFs using a dynamic graphical model (e.g. see [14]) which effectively encodes hierarchical and temporal semantics. For parameter learning, an efficient algorithm based on the Asymmetric Inside–Outside of [15] is introduced. For inference, we generalise the Viterbi algorithm to decode the semantics from an observational sequence.The common assumptions in discriminative learning and inference are that the training data in learning is fully labelled, and the test data during inference is not labelled. We propose to relax these assumptions in that training labels may only be partially available. Likewise, when some labels are given during inference, the algorithm should automatically adjust to meet the new constraints.We demonstrate the effectiveness of HSCRFs in two applications: (i) segmenting and labelling activities of daily living (ADLs) in an indoor environment and (ii) jointly modelling noun-phrases and part-of-speeches in shallow parsing. Our experimental results in the first application show that the HSCRFs are capable of learning rich, hierarchical activities with good accuracy and exhibit better performance when compared to DCRFs and flat-CRFs. Results for the partially supervised case also demonstrate that significant reduction of training labels still results in models that perform reasonably well. We also show that observing a small amount of labels can significantly increase the accuracy during decoding. In shallow parsing, the HSCRFs can achieve higher accuracy than standard CRF-based techniques and the recent DCRFs.To summarise, in this paper we claim the following contributions:• Introducing a novel Hierarchical Semi-Markov Conditional Random Field (HSCRF) to model complex hierarchical and nested Markovian processes in a discriminative framework.• Developing an efficient generalised Asymmetric Inside–Outside (AIO) algorithm for full-supervised learning.• Generalising the Viterbi algorithm for decoding the most probable semantic labels and structure given an observational sequence.• Addressing the problem of partially-supervised learning and constrained inference.• Constructing a numerical scaling algorithm to prevent numerical overflow.• Demonstration of the applicability of the HSCRFs for modelling human activities in the domain of home video surveil-lance and shallow parsing of English.The rest of the paper is organised as follows. Section 2 reviews Conditional Random Fields and Hierarchical Hidden Markov Models. Section 3 continues with the HSCRF model definition. Section 4 defines building blocks required for common in-ference tasks. Section 5 presents the generalised Viterbi algorithm. Parameterisation and estimation follow in Section 6. Learning and inference with partially available labels are addressed in Section 7. Section 8 presents a method for numerical scaling to prevent numerical overflow. Section 9 documents experimental results. Section 11 concludes the paper.1 Preliminary version was published in NIPS’08 [12].T. Tran et al. / Artificial Intelligence 246 (2017) 53–8555Table 1Notations used in this paper.NotationDescription(cid:3)(cid:3)xd:di: jed:di: jζ d,si: jci, j, tτ dr, s, u, v, wRd,s,zi: jπ d,su,iAd,s,zu,v,iEd,s,zu,i(cid:5) [ζ, z]Sd(cid:6)d,si: jˆ(cid:6)d,si: j(cid:7)d,si: jˆ(cid:7)d,si: jαd,si: j (u)λd,si: j (u)δ [·], I [·]ψ(·),ϕ(·)(cid:3)and starting from time i and ending at time j, inclusive(cid:3)Subset of state variables from level d down to level dSubset of ending indicators from level d down to level dSet of state variables and ending indicators of a sub model rooted at sd, level d, spanning a sub-string [i, j]Contextual cliqueTime indicesSet of all ending time indices, e.g. if i ∈ τ d then edand starting from time i and ending at time j, inclusive= 1iStateState-persistence potential of state s, level d, spanning [i, j]Initialisation potential of state s at level d, time i initialising sub-state uTransition at level d, time i from state u to v under the same parent sEnding potential of state z at level d and time i, and receiving the return control from the child uThe global potential of a particular configuration ζ given observation sequence zThe set of state symbols at level dThe symmetric inside mass for state s at level d, spanning substring [i, j]The full symmetric inside mass for state s at level d, spanning substring [i, j]The symmetric outside mass for state s at level d, spanning substring [i, j]The full symmetric outside mass for state s at level d, spanning substring [i, j]The asymmetric inside mass for parent state s at level d, starting at i and having a child-state u which returns control to the parent or transits to a new child-state at jThe asymmetric outside mass, as a counterpart of asymmetric inside mass αd,sIndicator functionsPotential functions.i: j (u)2. PreliminariesThis section presents foundations upon which the proposed HSCRF is built: conditional random fields and hierarchical hidden Markov models. For later reference, we define mathematical notations in Table 1.2.1. Conditional random fieldsDenote by G = (V , E ) the graph where V is the set of vertices, and E is the set of edges. Associated with each vertex i is a state variable xi Let x be joint state variable, i.e. x = (xi)i∈V . Conditional random fields (CRFs) [7] define a conditional distribution given the observation z as followsPr(x | z) = 1Z (z)(cid:2)cφc(xc, z)(1)(cid:4)(cid:3)where c is the index of cliques in the graph, φc(xc, z) is a non-negative potential function defined over the clique c, and Z (z) =c φc(xc, z) is the partition function.Let {˜x} be the set of observed state variables with the empirical distribution Q (˜x), and w be the parameter vector. xLearning in CRFs is typically by maximising the (log) likelihoodw∗ = arg maxwL (w) = arg maxwQ (˜x) log Pr(˜x | z; w)(cid:5)˜xThe gradient of the log-likelihood can be computed as(cid:6)∇L (w) =(cid:5)(cid:5)Q (˜x)∇ log φc(˜xc, z) −(cid:5)Pr(xc | z)∇ log φc(xc, z)(cid:7)(2)(3)xcThus, the inference needed in CRF parameter estimation is the computation of clique marginals Pr(xc|z).c˜xTypically, CRFs are parameterised as log-linear models, i.e. φc(xc, z) = exp(wf(xc, z)), where f(.) is the feature vector (cid:6)(cid:3)and w is weight vector. Let F(x, z) =c f(xc, z) be the global feature. Eq. (3) can be written as follows56T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Fig. 1. The state transition diagram of an HHMM.∇L =(cid:5)Q (˜x)(cid:6)(cid:5)f(˜xc, z) −˜xQ (˜x)[F] −E Pr(x|z)[F]= Ec(cid:5)xc(cid:7)Pr(xc | z)f(xc, z)(4)(5)Thus gradient-based maximum likelihood learning in the log-linear setting boils down to estimating the feature expecta-tions, also known as expected sufficient statistics (ESS).2.1.1. Learning with partial labelsLet ˜x = (v, h), where v is the set of visible variables, and h is the set of hidden variables. The incomplete log-likelihood and its gradient are given as(cid:5)L =Q (˜x) log Pr(v | z) =(cid:5)˜xQ (˜x) log(cid:5)hPr(v, h | z)Q (˜x)(log Z (v, z) − log Z (z))˜x(cid:5)=˜xwhere Z (v, z) =(cid:3)h(cid:4)c φc(vc, hc, z). The gradient reads∇L = Eh|v,z [F(v, h, z)] − Ex|z [F(x, z)](cid:5)(cid:5)(cid:5)⎛=Q (˜x)⎝Pr(hc | v, z)f(vc, hc, z) −˜xchc2.1.2. Sequential models⎞Pr(xc | z)f(xc, z)⎠(cid:5)xc(6)(7)The most popular form of CRFs is linear-chain of order n, where n is typically a small integer. This allows fast estimation for sequence of the clique marginals Pr(xc | z) using a forward-backward procedure with time complexity of Olength T and K states.T K n+1(cid:13)(cid:12)A generalisation of chain-CRF is semi-Markov CRF (SemiCRF) [13], which is first-order Markovian in segments (but non-where L is Markovian in states). A forward-backward procedure is adapted accordingly with time complexity of Othe maximum segment length. In Appendix C we will show that the SemiCRF is a special case of the proposed HSCRF.T L K 2(cid:13)(cid:12)2.2. Hierarchical hidden Markov modelsHierarchical HMMs are generalisation of HMMs [16] in that a state in an HHMM may be a sub-HHMM. Thus, an HHMM is a nested Markov chain. In the temporal evolution of HHMM, when a child Markov chain terminates, it returns the control to its parent. Nothing from the terminated child chain is carried forward. Thus, the parent state abstracts out everything belonging to it. Upon receiving the return control the parent then either transits to a new parent or terminates.Fig. 1 illustrates the state transition diagram of a two-level HHMM. At the top level there are two parent states { A, B}. Parent A has three children, i.e. ch( A) = {1, 2, 3} and B has four, i.e. ch(B) = {4, 5, 6, 7}. At the top level the transitions are between A and B, as in a normal directed Markov chain. Under each parent there are also transitions between child states, which only depend on the direct parent (either A or B). There are special ending states (represented as shaded nodes in Fig. 1) to signify the termination of the Markov chains. At each time step of the child Markov chain, a child will emit an observational symbol (not shown here).The temporal evolution of the HHMM can be represented as a dynamic Bayesian network (DBN), which was first done in [17]. Fig. 2 depicts a DBN structure of 3 levels. Associated with each state is an ending indicator to signify the termination = 0, the state of the state. Denote by xdt continues, i.e. xdxdt transits to a new state, or transits to itself. There are hierarchical tt the state and ending indicator at level d and time t, respectively. When edtt and edt+1. And when ed= 1, the state xd= xdtT. Tran et al. / Artificial Intelligence 246 (2017) 53–8557Fig. 2. Dynamic Bayesian network representation of HHMMs.Fig. 3. The shared topological structure.(cid:3)(cid:3)consistency rules that must be strictly observed. Whenever a state persists (i.e. edtpersist (i.e. edtedt(cid:3) < d). Similarly, whenever a state ends (i.e edt= 0 for all d(cid:3) > d).= 1 for all dInference and learning in HHMMs follow the Inside–Outside algorithm of the probabilistic context-free grammars. Overall, the algorithm has O(K 3 D T 3) time complexity where K is the maximum size of the state space at each level, D is the depth of the model and T is the model length.When representing as a DBN, the whole stack of states x1:Dcan be collapsed into a ‘mega-state’ of a big HMM, and there-fore inference can be carried out in O(K 2D T ) time. This is efficient for a shallow model (i.e. D is small), but problematic for a deep model (i.e. D is large).= 0), all of the states above it must also = 1), all of the states below it must also end (i.e. t3. Model definitionIn this section we define the general HSCRF as a hierarchically nested Markov process. Specific log-linear parameterisation will be presented in Sec. 6.1. In an HSCRF, like its generative counterpart (HHMM, Sec. 2.2), each parent state embeds a child Markov chain whose states may in turn contain child Markov chains. The family relation is defined in a topology, which is a state hierarchy of depth D > 1. The model has a set of states Sd at each level d ∈ [1, D], i.e. Sd = {1...K d}. For each state sd ∈ Sd where 1 ≤ d < D, the topological structure also defines a set of children ch(sd) ⊂ Sd+1. Conversely, each child sd+1 has a set of parents pa(sd+1) ⊂ Sd. Unlike the original HHMMs where the child states belong exclusively to the parent, the HSCRFs allow arbitrary sharing of children between parents. For example, in Fig. 3, ch(s1 = 1) = {1, 2, 3}, and pa(s3 = 1) = {1, 2, 4}. This helps avoid an explosive number of sub-states when D is large, leading to fewer parameters and possibly less training data and time. The shared topology has been investigated in the context of HHMMs in [15].The temporal evolution in the nested Markov processes with sequence length of T operates as follows:• As soon as a state is created at level d < D, it initialises a child state at level d + 1. The initialisation continues downward until reaching the bottom level.• As soon as a child process at level d + 1 ends, it returns control to its parent at level d, and in the case of d > 1, the parent either transits to a new parent state or returns to the grand-parent at level d − 1.In hierarchical nesting, the life-span of a child process belongs exclusively to the life-span of its parent. For example, a parent i: j at time i and persists until time j. At time i the parent initialises a child state sd+1process at level d starts a new state sdwhich continues until it ends at time k < j, at which the child state transits to a new child state sd+1k+1 . The child process exits at time j, returning the control to the parent sdi: j may transit to a new parent state sdj+1:l, or end at j, returning the control to the grand-parent at level d − 1.i: j . Upon receiving the control the parent state sdWe now formally specify the nested Markov processes. Let us introduce a multi-level temporal graphical model of length T with D levels, starting from the top as 1 and the bottom as D (Fig. 4). At each level d ∈ [1, D] and time index i ∈ [1, T ], i58T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Fig. 4. The multi-level temporal model. Adapted from [18].Table 2Hierarchical constraints.• The top state persists during the course of evolution, i.e. e1= 0.• When a state finishes, all of its descendants must also finish, i.e. edi• When a state persists, all of its ancestors must also persist, i.e. edi• When a state transits, its parent must remain unchanged, i.e. edi• The bottom states do not persists, i.e. e Di• All states end at T , i.e. e1:D= 1 for all i ∈ [1, T ].= 1.1:T −1T= 1 implies ed+1:D= 1.i= 0 implies e1:d−1= 1, ed−1= 0.ii= 0.Fig. 5. An example of a state-persistence sub-graph. Adapted from [18].Fig. 6. Sub-graphs for state transition (a), initialisation (b) and ending (c). Adapted from [18].there is a node representing a state variable xdisignifying whether the state xdspecific constraints on the value assignment of ending indicators as summarised in Table 2.is an ending indicator edii ends or persists at i. The nesting nature of the HSCRFs is now realised by imposing the ∈ Sd = {1, 2, ..., K d}. Associated with each xdiThus, specific value assignments of ending indicators provide contexts that realise the evolution of the model states in both hierarchical (vertical) and temporal (horizontal) directions. Each context at a level and associated state variables form a contextual clique, and we identify four contextual clique types:• State-persistence: This corresponds to the life time of a state at a given level (see Fig. 5). Specifically, given a context , is a contextual clique that specifies the life-span [i, j] of any state (cid:15)= (1, 0, .., 0, 1)=(cid:17)(cid:16), then σ persist,di: jxdi: j, c(cid:14)c =edi−1: js = xdi: j .• State-transition: This corresponds to a state at level d ∈ [2, D] at time i transiting to a new state (see Fig. 6a). Specifically, xd−1i+1 , xdis a contextual clique that specifies the transition (cid:14)ed−1(cid:15)= 1i:i+1, c=(cid:16)(cid:17)given a context c =of xdthen σ transit,d= 0, ediii+1 at time i under the same parent xd−1i+1 .i to xdiT. Tran et al. / Artificial Intelligence 246 (2017) 53–8559Table 3Short-hands for contextual clique potentials.(cid:17)(cid:17), z, z(cid:17)(cid:16)• Rd,s,zi: j• Ad,s,zu,v,i• π d,s,zu,i• Ed,s,zu,i= ψ= ψ= ψ= ψ(cid:16)σ persist,di: j(cid:16)σ transit,diσ init,di(cid:16)σ end,di, z, z(cid:17)where s = xdi: j .where s = xd−1i+1 and u = xdi , v = xdi+1.where s = xdwhere s = xd.i , u = xd+1i , u = xd+1ii.• State-initialisation: This corresponds to a state at level d ∈ [1, D − 1] initialising a new child state at level d + 1 at time iis a contextual clique that specifies , then σ init,d, c=(cid:16)(cid:17)i , xd+1xdi(see Fig. 6b). Specifically, given a context c =the initialisation at time i from the parent xd(cid:18)edi−1(cid:19)= 1i to the child xd+1i.i• State-ending: This corresponds to a state at level d ∈ [1, D − 1] ending at time i (see Fig. 6c). Specifically, given a context i at time i with the last is a contextual clique that specifies the ending of xd, then σ end,di , xd+1xd, c=(cid:17)(cid:16)ii(cid:18)(cid:19)c == 1edichild xd+1.i(cid:12)1:T , e1:Dx1:DIn the HSCRF we are interested in the conditional setting in which the entire state variables and ending indicators 1:Tare conditioned on an observational sequence z. For example, in NLP the observation is a sequence of words and the state variables might be the part-of-speech tags and the phrases.(cid:13)(cid:12)To capture the correlation between variables and such conditioning, we define a positive potential function ψ(σ , z) over each contextual clique σ . Table 3 shows the notations for potentials that correspond to the four contextual clique types we have identified above. Details of potential specification are described in the Sec. 6.1.Let ζ =denote the set of all variables that satisfies the set of hierarchical constraints listed in Table 2. Let τ ddenote the ordered set of all ending time indices at level d, i.e. if i ∈ τ d then ed= 1. The joint potential defined for each iconfiguration is the product of all contextual clique potentials over all ending time indices i ∈ [1, T ] and all semantic levels d ∈ [1, D]:1:T , e1:Dx1:D1:T(cid:13)(cid:5)[ζ, z] =(cid:20) (cid:2)(cid:2)d∈[1,D]ik,ik+1∈τ dRd,s,zik+1:ik+1The conditional distribution is given as(cid:21) (cid:2)⎧⎨(cid:20) (cid:2)⎩d∈[1,D−1]ik∈τ d+1,ik /∈τ dAd+1,s,zu,v,ik(cid:21)(cid:20) (cid:2)ik∈τ d+1π d,s,zu,ik+1(cid:21)(cid:20) (cid:2)ik∈τ d+1⎫⎬(cid:21)Ed,s,zu,ik⎭(8)(9)Pr(ζ |z) = 1Z (z)ζ (cid:5)[ζ, z] is the partition function for normalisation.(cid:5)[ζ, z](cid:3)where Z (z) =In what follows we omit z for clarity, and implicitly use it as part of the partition function Z and the potential (cid:5)[.]. It should be noted that in the unconditional formulation, there is only a single Z for all data instances. In conditional setting there is a Z (z) for each data instance z.Remarks. The temporal model of HSCRFs presented here is not a standard graphical model [14] since the connectivity (and therefore the clique structures) is not fixed. The potentials are defined on-the-fly depending on the context of assignments of ending indicators. Although the model topology is identical to that of shared structure HHMMs [15], the unrolled temporal representation is an undirected graph and the model distribution is formulated in a discriminative way. Furthermore, the state persistence potentials capture duration information that is not available in the dynamic DBN representation of the HHMMs in [17]. The HSCRF potentials may first appear to resemble the clique templates in the discriminative relational Markov networks [19]. They are, however, different because cliques in the HSCRFs are dynamic and context-dependent.4. Asymmetric inside–outside algorithmThis section describes a core inference engine called Asymmetric Inside–Outside (AIO) algorithm. The AIO algorithm computes building blocks that are needed in inference and learning tasks, including the partition function, time-specific marginals and feature expectations.4.1. Building blocks and conditional independence4.1.1. Contextual Markov blanketsIn this subsection we define elements that are building blocks for inference and learning. These building blocks are identified given the corresponding boundaries. Let us introduce two types of boundaries: the contextual symmetric and asymmetric Markov blankets.60T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Fig. 7. (a) Symmetric Markov blanket, and (b) asymmetric Markov blanket.Definition 1. A symmetric Markov blanket at level d for a state s starting at i and ending at j is the following set(cid:16)(cid:15)d,si: j=xdi: j= s, ed:Di−1= 1, ed:Dj= 1, edi: j−1= 0(cid:17)Definition 2. Let (cid:15)d,si: j be a symmetric Markov blanket, we define ζ d,si: j and ζ d,si: j as follows(cid:17)(cid:16)ζ d,si: j=ζ d,si: j= ζ \, ed+1:Dxd+1:Di: ji: j−1(cid:16)(cid:17)i: j , (cid:15)d,sζ d,si: j(10)(11)(12)= s. Further, we definesubject to xdi: j(cid:16)(cid:17)ˆζ d,si: j=i: j , (cid:15)d,sζ d,si: jand ˆζd,si: j(cid:16)(cid:17)=i: j , (cid:15)d,sζ d,si: jFig. 7a shows an example of a symmetric Markov blanket (represented by a double-arrowed line).Definition 3. A asymmetric Markov blanket at level d for a parent state s starting at i and a child state u ending at j is the following set(cid:14)(cid:16)d,si: j (u) =xdi: j= s, xd+1j= u, ed:Di−1= 1, ed+1:Dj= 1, edi: j−1= 0(cid:15)Definition 4. Let (cid:16)d,si: j (u) be an asymmetric Markov blanket, we define ζ d,si: j (u) and ζ d,si: j(cid:16)ζ d,si: j (u) =ζ d,si: j(u) = ζ \ji: j−1 , xd+2:Dxd+1:D(cid:16)i: j (u), (cid:16)d,sζ d,s, ed+1:Di: j−1(cid:17)i: j (u)(cid:17)subject to xdi: j= s and xd+1j= u. Further, we define(cid:16)(cid:16)ˆζ d,si: j (u) =d,sˆζi: j(u) =i: j (u), (cid:16)d,sζ d,si: j (u), (cid:16)d,sζ d,s(cid:17)i: j (u)(cid:17)i: j (u)(u) as follows(13)(14)(15)(16)(17)Fig. 7b shows an example of asymmetric Markov blanket (represented by an arrowed line).Remark. The concepts of contextual Markov blankets (or Markov blankets for short) are different from those in traditional Markov random fields and Bayesian networks because they are specific assignments of a subset of variables, rather than a collection of variables.4.1.2. Conditional independenceGiven these two definitions we have the following propositions of conditional independence.T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Proposition 1. ζ d,s(cid:16)i: j , ζ d,sζ d,si: jPri: j are conditionally independent given (cid:15)d,si: j and ζ d,si: j(cid:17)(cid:17)(cid:16)(cid:16)(cid:17)| (cid:15)d,si: j= Prζ d,si: j| (cid:15)d,si: jPrζ d,si: j| (cid:15)d,si: jThis proposition gives rise to the following factorisation(cid:17)(cid:16)(cid:17)(cid:16)(cid:17)(cid:16)(cid:16)(cid:17)(cid:16)(cid:17)Pr(ζ ) = Pr(cid:15)d,si: jPri: j , ζ d,sζ d,si: j| (cid:15)d,si: j= Pr(cid:15)d,si: jPrζ d,si: j| (cid:15)d,si: jPrζ d,si: j| (cid:15)d,si: jProposition 2. ζ d,s(cid:16)i: j (u), ζ d,sζ d,sPri: j (u) and ζ d,si: j(u) are conditionally independent given (cid:16)d,si: j (u)i: j (u) | (cid:16)d,s(cid:17)i: j (u)(cid:16)i: j (u) | (cid:16)d,sζ d,s(cid:17)i: j (u)Pr(cid:16)= Pri: j (u) | (cid:16)d,sζ d,s(cid:17)i: j (u)The following factorisation is a consequence of Proposition 2Pr(ζ ) = Pr(cid:16)= Pr(cid:16)(cid:16)d,s(cid:17)i: j (u)(cid:17)i: j (u)(cid:16)d,sPrPr(cid:16)ζ d,si: j (u), ζ d,si: j(cid:16)i: j (u) | (cid:16)d,sζ d,s(cid:17)(u) | (cid:16)d,si: j (u)(cid:17)(cid:16)i: j (u)ζ d,si: jPr(u) | (cid:16)d,s(cid:17)i: j (u)The proofs of Propositions 1 and 2 are given in Appendix A.1.61(18)(19)(20)(21)(cid:16)4.1.3. Symmetric inside/outside massesi: j , (cid:15)d,sζ d,si: j , ζ d,sFrom Eq. (12) we have ζ =i: j(cid:14)(cid:15)(cid:14)d,sˆζ(cid:15)d,s, and (cid:5) , (cid:5) i: ji: ji: j , we can group local potentials in Eq. (8)(cid:14)ˆζ d,sinto three parts: (cid:5) . By ‘grouping’ we mean to multiply all the local potentials belonging to a i: jcertain part, in the same way that we group all the local potentials belonging to the model in Eq. (8). Note that although ˆζ d,si: j contains (cid:15)d,s(cid:14)i: j we do not group (cid:5) i: j separates ζ d,si: j. Since (cid:15)d,s(cid:15)(cid:14)into (cid:5) from ζ d,s(cid:15)d,si: jˆζ d,si: j(cid:17)(cid:15)(cid:15)(cid:15)(cid:15).(cid:14)By definition of the state-persistence clique potential (Fig. 3), we have (cid:5) (cid:14). The same holds for (cid:5) (cid:15)(cid:15)d,si: jd,sˆζi: j= Rd,si: j . Thus Eq. (8) can be replaced by(cid:14)(cid:5)[ζ ] =(cid:5)(cid:15)ˆζ d,si: jRd,si: j (cid:5)(cid:15)(cid:14)ˆζd,si: j(cid:14)ˆζThere are two special cases: (1) when d = 1, (cid:5) i ∈ [1, T ]. This factorisation plays an important role in efficient inference.(cid:14)ˆζ D,s= 1 for s ∈ S 1, and (2) when d = D, (cid:5) i:i1,s1:T(cid:15)We know define a quantity called symmetric inside mass (cid:6)d,si: j , and another called symmetric outside mass (cid:7)d,si: j .(22)(cid:15)= 1 for s ∈ S D and Definition 5. Given a symmetric Markov blanket (cid:15)d,sare defined asi: j , the symmetric inside mass (cid:6)d,si: j and the symmetric outside mass (cid:7)d,si: j(cid:6)d,si: j=(cid:7)d,si: j=(cid:5)(cid:14)(cid:15)(cid:5)ˆζ d,si: jζ d,si: j(cid:5)(cid:15)(cid:14)(cid:5)ˆζd,si: jζ d,si: jAs special cases we have (cid:7)1,s1:Tsymmetric inside mass ˆ(cid:6)d,s(23)(24)= 1 for i ∈ [1, T ], s ∈ S D . For later use let us introduce the ‘full’ = 1 and s ∈ S 1, and (cid:6)D,si:ii: j and the ‘full’ symmetric outside mass ˆ(cid:7)d,si: j asˆ(cid:6)d,si: j= Rd,si: j (cid:6)d,si: jandˆ(cid:7)d,si: j= Rd,si: j (cid:7)d,si: jIn the rest of the paper, when it is clear in the context, we will use inside mass as a shorthand for symmetric inside mass, outside mass for symmetric outside mass, full-inside mass for full-symmetric inside mass, and full-outside mass for full-symmetric outside mass.Thus, from Eq. (22) the partition function can be computed from the full-inside mass at the top level (d = 1)1:T Rd,s1:T(cid:5)[ζ ] =ˆζ 1,s1:TR1,s1:T(cid:6)d,sZ =(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)=(cid:15)(cid:14)ζ 1,s1:Ts∈S 1s∈S 1=ζ(cid:5)s∈S 1ˆ(cid:6)1,s1:T(25)62T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Table 4Computing the partition function from the full-inside mass and full-outside mass.• Z =• Z =• Z =(cid:3)(cid:3)(cid:3)s∈S 1s∈S Ds∈Sdˆ(cid:6)1,s1:Tˆ(cid:7)D,si:i(cid:3)for any i ∈ [1, T ]j∈[t,T ] (cid:6)d,s(cid:3)i∈[1,t]i: j (cid:7)d,si: j Rd,si: j for any t ∈ [1, T ] and d ∈ [2, D − 1]With the similar derivation the partition function can also be computed from the full-outside mass at the bottom level (d = D)Z =(cid:5)s∈S Dˆ(cid:7)D,si:i , for any i ∈ [1, T ]In fact, we will prove a more general way to compute Z in Appendix B(cid:5)(cid:5)(cid:5)Z =s∈Sdi∈[1,t]j∈[t,T ](cid:6)d,si: j (cid:7)d,si: j Rd,si: j(26)(27)for any t ∈ [1, T ] and d ∈ [2, D − 1]. These relations are summarised in Table 4.Given the fact that ζ d,si: jis separated from the rest of variables by the symmetric Markov blanket (cid:15)d,si: j , we have Proposi-tion 3.Proposition 3. The following relations hold(cid:17)(cid:16)(cid:14)(cid:15)Prζ d,si: j| (cid:15)d,si: j(cid:16)Prζ d,si: j| (cid:15)d,si: j(cid:16)Pr(cid:15)d,si: j(cid:17)(cid:17)= 1(cid:6)d,si: j= 1(cid:7)d,si: j(cid:5)(cid:5)ˆζ d,si: j(cid:15)(cid:14)ˆζd,si: j= 1Z(cid:6)d,si: j Rd,si: j (cid:7)d,si: jThe proof of this proposition is given in Appendix A.2.4.1.4. Asymmetric inside/outside masses(28)(29)(30)Recall that we have introduced the concept of asymmetric Markov blanket (cid:16)d,sLet us group all the local contextual clique potentials associated with ζ d,si: j (u) and (cid:16)d,sSimilarly, we group all local potentials associated with ζ d,si: j (u) and (cid:16)d,s(cid:14)i: j (u) into a joint potential (cid:5) (cid:14)ˆζ(cid:5) (cid:15)(u)d,si: jincludes the state-persistence potential Rd,si: j .i: j (u) which separates ζ d,s(cid:14)i: j (u) into a joint potential (cid:5) i: j (u) and ζ d,si: j (u). (cid:15)ˆζ d,s. i: j (u). Note that (cid:15)(u)ˆζd,si: jDefinition 6. Given the asymmetric Markov blanket (cid:16)d,smass λd,si: j (u), the asymmetric inside mass αd,si: j (u) and the asymmetric outside i: j (u) are defined as follows(cid:15)(cid:5)ˆζ d,si: j (u)αd,si: j (u) =(cid:5)(cid:14)λd,si: j (u) =ζ d,si: j (u)(cid:5)ζ d,si: j (u)(cid:14)(cid:5)ˆζd,si: j(cid:15)(u)(31)(32)The relationship between the asymmetric outside mass and asymmetric inside mass is analogous to that between the outside and inside masses. However, there is a small difference, that is, the asymmetric outside mass ‘owns’ the segment xdi: j= s and the associated state-persistence potential Rd,si: j , whilst the outside mass (cid:7)di: j(s) does not.T. Tran et al. / Artificial Intelligence 246 (2017) 53–8563Fig. 8. Decomposition with respect to symmetric/asymmetric Markov blankets.4.2. Computing inside massesIn this subsection we show how to recursively compute the pair: inside mass and asymmetric inside mass. The key idea here is to exploit the decomposition within the asymmetric Markov blanket. As shown in Fig. 8, an outer asymmetric Markov blanket can be decomposed into a sub-asymmetric Markov blanket and a symmetric blanket.4.2.1. Computing asymmetric inside mass from inside massAssume that within the asymmetric Markov blanket (cid:16)d,si: j (u), the child u starts somewhere at t ∈ [i, j] and ends at j, i.e. xd+1t: j= u, ed+1t: j−1= 0 and ed+1:D−1t−1= 1. Let us consider two cases: t > i and t = i.Case 1. For t > i, denote by v = xd+1t−1 . We have two smaller blankets within (cid:16)d,sassociated with the child u = xd+1t: jthe parent s. Fig. 8 illustrates the blanket decomposition. The assignment ζ d,s, and the asymmetric blanket (cid:16)d,si: j (u): the symmetric blanket (cid:15)d+1,ui:t−1(v) associated with the child v ending at t − 1 under t: ji: j (u) can be decomposed as(cid:16)ζ d,si: j (u) =i:t−1(v), ζ d+1,uζ d,st: j(cid:14)Thus, the joint potential (cid:5) (cid:15)ˆζ d,si: j (u), u = xd+1(cid:15)ˆζ d,si: j (u)(cid:14)(cid:15)ˆζ d,si:t−1(v)ˆζ d+1,ut: j= (cid:5)(cid:5)(cid:5)(cid:14)(cid:14)t: j , edt−1: j−1(cid:17)= 0, ed+1:Dt−1= 1can be factorised as follows(cid:15)v,u,t−1 Rd+1,uAd+1,st: j(33)(34)(cid:15)The transition potential Ad+1,spersistence potential Rd+1,ut: jv,u,t−1 is enabled in the context c =in the context c =Case 2. For t = i, the asymmetric blanket (cid:16)d,s(cid:16)of assignment ˆζ d,si: j (u) =activated. Thus we haveˆζ d+1,ui: j, edi−1= 1, edi: j−1(cid:14)= 0, ed+1edt−1t−1= 1, ed+1:D= 0, ed+1:D(cid:14)ed+1t: j−1= 1, xdt= 1, xd+1t: ji:t−1(v) does not exist since i > t − 1. We have the following decompositions (cid:17)= 0is , the state-initialisation potential π d,su,i. In the context c =, and the state-(cid:18)edi−1= s, xd+1t−1(cid:15)= v, xd+1(cid:19)= 1= u= ut−1.tj(cid:14)(cid:15)ˆζ d,si: j (u)(cid:5)= π d,su,i (cid:5)(cid:15)(cid:14)ˆζ d+1,ui: jRd+1,ui: j(35)Substituting Eqs. (34), (35) into Eq. (31), and together with the fact that t can take any value in the interval [i, j], and vcan take any value in Sd+1, we have the following relation(cid:15)(cid:5)ˆζ d,si:t−1(v)αd,si: j (u) =(cid:5)(cid:5)(cid:5)(cid:5)(cid:14)(cid:5)t∈[i+1, j](cid:5)v∈Sd+1(cid:5)=t∈[i+1, j]v∈Sd+1ζ d+1,uζ d,si:t−1(v)t: ji:t−1(v) ˆ(cid:6)d+1,uαd,st: j(cid:15)(cid:14)ˆζ d+1,ut: jv,u,t−1 Rd+1,uAd+1,st: j+(cid:5)(cid:15)(cid:14)ˆζ d+1,ui: jRd+1,ui: jπ d,su,i (cid:5)ζ d+1,ui: jAd+1,sv,u,t−1+ ˆ(cid:6)d+1,ui: jπ d,su,i(36)As we can see, the asymmetric inside mass α plays the role of a forward message starting from the starting time i to the ending time j. There is a recursion where the asymmetric inside mass ending at time j is computed from all the asymmetric inside masses ending at time t − 1, for t ∈ [i + 1, j].There are special cases for the asymmetric inside mass: (1) when i = j, we only havei:i (u) = ˆ(cid:6)d+1,sαd,si:iπ d,su,i(37)64T. Tran et al. / Artificial Intelligence 246 (2017) 53–85and (2) when d = D − 1, the sum over the index t as in Eq. (36) is not allowed since at level D the inside mass only spans a single index. We have the following instead(cid:5)α D−1,si: j(u) =α D−1,si: j−1 (v) ˆ(cid:6)D,uj: j A D,sv,u, j−1=v∈Sd+1(cid:5)v∈Sd+1α D−1,si: j−1 (v)R D,uj: j A D,sv,u, j−14.2.2. Computing inside mass from asymmetric inside mass(38)Notice the relationship between the asymmetric Markov blanket (cid:16)d,s(cid:17)= 1, i.e. the parent s ends at j, and (cid:16)d,sWhen ed(cid:16)jand ˆζ d,si: j (u), u = xd+1ζ d,sζ d,s=i: ji: j(cid:15)(cid:14)ˆζ d,si: j (u)(cid:16)ˆζ d,si: j (u), edˆζ d,si: jEd,su, j= (cid:5)j(cid:14)(cid:5)=(cid:15)j= 1, u = xd+1ji: j (u) will become (cid:15)d,s(cid:17)i: j with u = xd+1ji: j (u) and the symmetric blanket (cid:15)d,si: j , where d < D. . Then we have decompositions . These lead to the factorisation(39)where the state-ending potential Ed,su, jrewritten asis activated in the context c =(cid:14)edj(cid:15)= 1. Thus, the inside mass in Eq. (23) can be (cid:5)(cid:5)(cid:6)d,si: j=(cid:5)[ ˆζ d,si: j (u)]Ed,su, ju∈Sd+1(cid:5)u∈Sd+1(cid:5)u∈Sd+1==ζ d,si: j (u)Ed,su, j(cid:5)(cid:5)[ ˆζ d,si: j (u)]ζ d,si: j (u)u, jαd,sEd,si: j (u)(40)This equation holds for d < D. When d = D, we set (cid:6)D,si:ithat i = 1 and j = T .= 1 for all s ∈ S D and i ∈ [1, T ], and when d = 1, we must ensure Remark. Eqs. (36), (37), (38) and (40) specify a left-right and bottom-up algorithm to compute both the inside and asymmet-ric inside masses. Initially, at the bottom level (cid:6)D,s= 1 for i ∈ [1, T ] and s ∈ S D . Pseudo-code of the dynamic programming i:ialgorithm to compute all the inside and asymmetric inside masses and the partition function is given in Algorithm 1.Algorithm 1 Computing the set of inside/asymmetric inside masses and the partition function.Input: D, T , all the potential function values.Output: partition function Z ; (cid:6)1,s1:T , for s ∈ S 1;(cid:6)D,si:i(cid:6)d,si: j , for d ∈ [2, D − 1], s ∈ Sd and 1 ≤ i ≤ j ≤ T ;i: j (u) for d ∈ [1, D − 1], u ∈ Sd+1 and 1 ≤ i ≤ j ≤ Tαd,s= 1 for all i ∈ [1, T ] and s ∈ S DInitialise: (cid:6)D,si:ifor s ∈ S D and i ∈ [1, T ];/* At the level d=D-1 */For i = 1, 2, ..., T ; j = i, i + 1, ..., TCompute α D−1,sCompute (cid:6)D−1,si: ji: j(u) using Eq. (38)using Eq. (40)EndFor/* The main recursion loops: bottom-up and forward */For d = D − 2, D − 3, ..., 1For i = 1, 2, ..., T ; j = i, i + 1, ..., TCompute αd,sCompute αd,sCompute (cid:6)d,si:i (u) using Eq. (37) If j = ii: j (u) using Eq. (36) If j > ii: j using Eq. (40) If d > 1EndForEndForCompute Z using Eq. (25).T. Tran et al. / Artificial Intelligence 246 (2017) 53–85654.3. Computing outside massesIn this subsection we show how to recursively compute the symmetric outside mass and the asymmetric outside mass. We use the same blanket decomposition as in Section 4.2. However, this time the view is reversed as we are interested in quantities outside the blankets. For example, outside the inner symmetric Markov blanket in Fig. 8, there exists an outer asymmetric blanket and another sub-asymmetric blanket on the left.4.3.1. Computing asymmetric outside mass from outside massLet us examine the variables ζ d,si: j (u) associated with the asymmetric Markov blanket (cid:16)d,s1 ≤ i ≤ j ≤ T (see Definition 4). For j < T , assume that there exists an outer asymmetric Markov blanket (cid:16)d,sv ∈ Sd+1 and t ∈ [ j + 1, T ], and a symmetric Markov blanket (cid:15)d+1,v(v), ˆζ d+1,vdecomposition ˆζj+1:t , xd+1(cid:14)(cid:14)(cid:15), which leads to the following factorisationj+1:t right next to (cid:16)d,s(u) =(cid:14)= ud,si:tˆζ(cid:16)(cid:17)ji: j (u), for d ∈ [1, D − 1] and i:t (v) for some i: j (u). Given these blankets we have the (cid:5)ˆζd,si: j= (cid:5)ˆζd,si:tˆζ d+1,vj+1:tj+1:t Ad+1,sRd+1,vu,v, j(cid:15)(v)(cid:5)d,si: j(cid:15)(u)(41)The state transition potential Ad+1,su,v, jRd+1,v= 1, ed+1j+1:tIn addition, there exists a special case where the state s ends at j. We have the decomposition ˆζis enabled in the context c =in the context c == 0, ed+1= 0, ed+1(cid:14)ed+1(cid:15)= 1j+1:t−1.tjj(cid:14)edj(cid:15)= 1, and the state persistence potential (cid:16)ˆζd,si: j(u) =d,si: j, u = xd+1j(cid:17)(42)and the following factorisation(cid:15)(cid:14)(cid:14)(cid:15)(u)(cid:5)ˆζd,si: j= (cid:5)ˆζd,si: ji: j Ed,sRd,su, jThe ending potential Ed,su, j appears here because of the context c =(cid:14)edj(cid:15)= 1, i.e. s ends at j.Now we relax the assumption of t, v and allow them to receive all possible values, i.e. t ∈ [ j, T ] and v ∈ Sd+1. Thus we can replace Eq. (32) byλd,si: j (u) =(cid:5)(cid:5)(cid:5)(cid:5)(cid:14)(cid:5)v∈Sd+1(cid:5)t∈[ j+1,T ](cid:5)=v∈Sd+1t∈[ j+1,T ]ζ d+1,vζ d,si:t (v)j+1:ti:t (v) ˆ(cid:6)d+1,vλd,s(cid:14)(cid:15)(v)(cid:5)ˆζd,si:tˆζ d+1,vj+1:t(cid:15)j+1:t Ad+1,sRd+1,vu,v, j+(cid:5)(cid:15)(cid:14)(cid:5)ˆζd,si: ji: j Ed,sRd,su, jζ d,si: j (u)j+1:t Ad+1,su,v, j+ ˆ(cid:7)d,si: j Ed,su, j(43)for d ∈ [2, D − 2], and 1 ≤ i ≤ j ≤ T . Thus, the λd,si: j (u) can be thought as a message passed backward from j = T to j = i. Here, the asymmetric outside mass ending at j is computed by using all the asymmetric outside masses ending at t for t ∈ [ j + 1, T ].There are two special cases. At the top level, i.e. d = 1, then λd,si: j (u) is only defined at i = 1, and the second term of the RHS of Eq. (43) is included only if i = 1, j = T . At the second lowest level, i.e. d = D − 1, we cannot sum over t as in Eq. (43) since ˆ(cid:6)D,vj+1:t is only defined for t = j + 1. We have the following relation insteadλD−1,si: j(u) =(cid:5)v∈S DλD−1,si: j+1 (v) ˆ(cid:6)D,vj+1: j+1 A D,su,v, j+ ˆ(cid:7)D−1,si: jE D−1,su, j(44)4.3.2. Computing outside mass from asymmetric outside massGiven a symmetric Markov blanket (cid:15)d+1,ufor d ∈ [1, D − 1], assume that there exists an asymmetric Markov blanket (cid:16)d,st: j (u) at the parent level d, where t ∈ [1, i]. Clearly, for t ∈ [1, i − 1] there exists some sub-asymmetric Markov blanket (cid:16)d,st:i−1(v). See Fig. 8 for an illustration.i: jLet us consider two cases: t < i and t = i.Case 1. For t < i, this enables the decomposition ˆζ(cid:16)d+1,ui: j=ˆζd,st: j(u), ˆζ d,st:i−1(v), u = xd+1i: j(cid:17)factorisation(cid:15)(cid:14)(cid:5)ˆζd+1,ui: j(cid:14)ˆζd,st: j(cid:15)(u)(cid:5)(cid:14)(cid:15)ˆζ d,st:i−1(v)= (cid:5)Ad,sv,u,i−1The state transition potential Ad,sv,u,i−1 is activated in the context c =(cid:14)edi−1= 0, ed+1i−1(cid:15)= 1., which leads to the following (45)66T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Table 5Summary of basic building blocks computed in Section 4.2 and 4.3.• (cid:6)1,s• (cid:6)d,s• (cid:6)D,s• αd,s• αd,s1:T , (cid:7)1,si: j , (cid:7)d,si:i , (cid:7)D,si:i1: j (u), λd,si: j (u), λd,s1:T for s ∈ S 1i: j for d ∈ [2, D − 1], s ∈ Sd, 1 ≤ i ≤ j ≤ Tfor i ∈ [1, T ], s ∈ S D1: j (u) for d = 1, s ∈ S 1, u ∈ S 2, j ∈ [1, T ]i: j (u) for d ∈ [2, D − 1], s ∈ Sd, u ∈ Sd+1, 1 ≤ i ≤ j ≤ TCase 2. For t = i, the decomposition reduces to ˆζ(cid:16)d+1,ui: j=ˆζd,si: j(u), u = xd+1i: j(cid:17), which leads to the following factorisation(cid:15)(cid:14)(cid:5)ˆζd+1,ui: j(cid:14)ˆζd,si: j(cid:15)(u)π d,su,i= (cid:5)(cid:19)= 1(46)(cid:18)edi−1The state-initialisation potential π d,su,i plays the role in the context c =However, these decompositions and factorisations only hold given the assumption of specific values of s ∈ Sd, v ∈ Sd+1, and t ∈ [1, i]. Without further information we have to take all possibilities into account. Substituting these relations into Eq. (24), we have(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:7)d+1,ui: j=(cid:14)(cid:15)ˆζ d,st: j (u)(cid:14)(cid:15)ˆζ d,st:i−1(v)(cid:5)(cid:5)Ad+1,sv,u,i−1+(cid:5)(cid:5)(cid:14)(cid:5)ˆζd,si: j(cid:15)(u)π d,su,is∈Sd(cid:5)v∈Sd+1(cid:5)=t∈[1,i−1]ζ d,st: j (u)(cid:5)ζ d,st:i−1(v)t:i−1(v) Ad+1,sαd,sv,u,i−1λd,st: j (u)s∈Sdt∈[1,i−1]v∈Sd+1s∈Sdζ d,si: j (u)+(cid:5)s∈Sdi: j (u)π d,sλd,su,i(47)for d ∈ [2, D − 2].There are three special cases. The first is the base case where d = 0 and (cid:7)1,s1:Td = 1, we must fix the index t = 1 since the asymmetric inside mass αd,sthe RHS is included only if i = 1 for the asymmetric outside mass λd,swe only have i = j.= 1 for all s ∈ S 1. In the second case, for t:i−1 is only defined at t = 1. Also the second term in i: j (u) to make sense. In the second case, for d + 1 = D, Remark. Eqs. (43), (44) and (47) show a recursive top-down and outside-in approach to compute the symmetric/asymmetric outside masses. We start from the top with d = 1 and (cid:7)1,s= 1 for all s ∈ S 1 and proceed downward until d = D. The 1:Tpseudo-code is given in Algorithm 2. Table 5 summarises the quantities computed in Section 4.2 and 4.3.Algorithm 2 Computing the set of outside/asymmetric outside masses.Input: D, T , all the potential function values, all inside/asymmetric inside masses.Output: all outside/asymmetric outside massesInitialise: (cid:7)1,su,T for s ∈ S 1, u ∈ S 21:T/* the main recursive loops: top-down and inside-out */For d = 1, 2, ..., D − 11:T (u) = E 1,s= 1; λ1,sFor i = 1, 2, ..., T ; j = T , T − 1, ..., iCompute the asymmetric outside mass λd,sCompute the outside mass (cid:7)d,si: j using Eq. (47)i: j (u) using Eqs. (43), (44)EndForEndForAlgorithm 3 summarises the AIO algorithm for computing all building blocks and the partition function.Algorithm 3 The AIO algorithm.Input: D, T , all the potential function valuesOutput: all building blocks and partition functionCompute all inside/asymmetric inside masses using the algorithm in Algorithm 1Compute all outside/asymmetric outside masses using the algorithm in Algorithm 2T. Tran et al. / Artificial Intelligence 246 (2017) 53–8567Table 6Notations used in this section.NotationDescription(cid:6)max,d,si: jˆ(cid:6)max,d,si: jαmax,d,si: j(cid:6)arg,d,si: jαarg,d,si: jI d(u)(u)i: jThe optimal potential function of the subset of variables ζ d,si: jThe ‘full’ version of (cid:6)max,d,sThe optimal potential function of the subset of variables ζ d,sThe optimal child ud+1The optimal child vd+1t−1 that transits to ud+1t: jThe set of optimal ‘segments’ at each level d.of sji: j (u)and the time index t.5. The generalised Viterbi algorithmThe MAP assignment isζ M A P = arg maxζPr(ζ | z) = arg maxζ(cid:5)[ζ, z]The process of computing the MAP assignment is similar to that of computing the partition function. This similarity comes from the relation between the sum-product and max-product algorithm (a generalisation of the Viterbi algorithm) of [20], and from the fact that inside/asymmetric inside procedures described in Section 4.2 are essentially a sum-product. We just need to convert all the summations into corresponding maximisations. The algorithm is a two-step procedure:• In the first step the maximum joint potential is computed and local maximum states and ending indicators are saved along the way. These states and ending indicators are maintained in a bookkeeper.• In the second step we decode the best assignment by backtracking through saved local maximum states.We make use of the contextual decompositions and factorisations from Section 4.2.NotationsThis section, with some abuse of notations, uses some slight modifications to the notations used in the rest of the paper. See Table 6 for reference.We now describe the first step.5.1. Computing the maximum joint potential, maximal states and time indices(cid:14)For clarity, let us drop the notation z in (cid:5)[ζ, z]. As (cid:5)[ζ ] = (cid:5) (cid:15)ˆζ 1,s1:TR1,s1:T for s ∈ S 1 we havemaxζ(cid:5)[ζ ] = maxs∈S 1R1,s1:T maxζ 1,s1:T(cid:5)(cid:15)(cid:14)ˆζ 1,s1:TNow, for a sub-assignment ζ d,si: j(cid:14)(cid:15)ˆζ d,si: j(cid:5)maxζ d,si: j= maxu∈Sd+1Ed,su, j maxζ d,si: j (u)(cid:5)(cid:14)(cid:15)ˆζ d,si: j (u)for 1 ∈ [1, D − 1], Eq. (39) leads to(48)(49)With some slight abuse of notation we introduce (cid:6)max,d,si: jas the optimal potential function of the subset of variables i: j , and αmax,d,sζ d,si: j(u) as the optimal potential function of the subset of variables ζ d,si: j (u).Definition 7. We define (cid:6)max,d,si: jand αmax,d,si: j(cid:15)(u) as follows(cid:14)(cid:6)max,d,si: jˆ(cid:6)max,d,si: jαmax,d,si: j(cid:5)ˆζ d,si: j= maxζ d,si: j= (cid:6)max,d,sRd,si: ji: j(cid:15)(cid:14)ˆζ d,si: j (u)(cid:5)(u) = maxζ d,si: j (u)(50)(51)(52)68T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Eqs. (48) and (49) can be rewritten more compactly as(cid:15)(cid:14)ζ M A P(cid:5)(cid:6)max,d,si: j= maxs∈S 1= maxu∈Sd+1ˆ(cid:6)max,1,s1:Tu, jαmax,d,sEd,si: j(u)for d ∈ [1, D − 1]. When d = D, we simply set (cid:6)max,D,sFrom the factorisation in Eqs. (34), (35), we havei:i= 1 for all s ∈ S D and i ∈ [1, T ].(cid:14)(cid:15)ˆζ d,si: j (u)= max(cid:5)maxζ d,si: j (u)⎛⎧⎨⎩⎝ maxv∈Sd+1maxt∈[i+1, j]Rd+1,ut: jAd+1,sv,u,t−1 maxζ d,si:t−1(v)⎛⎞(cid:15)⎠ ;(cid:14)ˆζ d+1,ut: j⎝Rd+1,ui: j maxζ d+1,ui: j× maxζ d+1,ut: j(cid:5)(cid:14)(cid:15)ˆζ d,si:t−1(v)×(cid:5)(cid:14)ˆζ d+1,ui: jπ d,su,i (cid:5)⎞(cid:15)⎠⎫⎬⎭and(cid:28) (cid:29)αmax,d,si: j(u) = maxmaxv∈Sd+1maxt∈[i+1, j]αmax,d,si:t−1(v) ˆ(cid:6)max,d+1,ut: jAd,sv,u,t−1(cid:30)(cid:16)ˆ(cid:6)max,d+1,ui: j;π d+1,su,i(cid:31)(cid:17)(53)(54)(55)(56)for d ∈ [1, D − 2] and i < j. For d = D − 1, we cannot scan the index t in the interval [i + 1, j] because the maximum inside (cid:6)max,D,ut: jis only defined at t = j. We have the following insteadαmax,D−1,si: j(u) = maxv∈S Dαmax,D−1,si: j−1(v) ˆ(cid:6)max,D,uj: jA D,sv,u, j−1There is a base case for i = j, where the context c =(cid:18)edi−1(cid:19)= 1is active, thenαmax,d,si:i(u) = ˆ(cid:6)max,d+1,ui:iπ d,su,i(57)(58)Of course, what we are really interested in is not the maximum joint potentials but the optimal states and time indices (or ending indicators). We need some bookkeepers to hold these quantities along the way. With some abuse of notation let us introduce the symmetric inside bookkeeper (cid:6)arg,d,sassociated with Eq. (54), and the asymmetric inside bookkeeper αarg,d,s(u) associated with Eqs. (56), (57) and (58).i: ji: jDefinition 8. We define the symmetric inside bookkeeper (cid:6)arg,d,si: jas follows(cid:6)arg,d,si: j= u∗ = arg maxu∈Sd+1 Ed,su, jαmax,d,si: j(u)Similarly, we define the asymmetric inside bookkeeper αarg,d,si: j(u) associated with Eq. (56) for d ∈ [1, D − 2] asαarg,d,si: j(u) = (v, t)∗ = arg maxt∈[i+1, j],v∈Sd+1αmax,d,si:t−1(v) ˆ(cid:6)max,d+1,ut: jAd,sv,u,t−1if maxv∈Sd+1,t∈[i+1, j] αmax,d,si:t−1(v) ˆ(cid:6)max,d+1,ut: jv,u,t−1 > ˆ(cid:6)max,d+1,uAd,si: jπ d+1,su,iand i < j; andαarg,d,si: j(u) = undefinedotherwise. For d = D − 1, the αarg,d,si: j(u) is associated with Eq. (57)αarg,D−1,si: j(u) = arg maxv∈S D αmax,d,si: j−1(v) ˆ(cid:6)max,D,uj: jAd,sv,u, j−1(59)(60)(61)(62)The Eqs. (53), (54), (56), (57) and (58) provide a recursive procedure to compute maximum joint potential in a bottom-up = 1 for all s ∈ S D and i ∈ [1, T ]. The procedure is summarised in and left-right manner. Initially we just set (cid:6)max,D,sAlgorithm 4.i:iAlgorithm 4 Computing the bookkeepers.T. Tran et al. / Artificial Intelligence 246 (2017) 53–8569Input: D, T , all the potential function values.Output: the bookkeepers; (cid:6)arg,1,s1:T, for s ∈ S 1 and 1 ≤ i ≤ j ≤ T ;(cid:6)arg,d,si: jαarg,d,si: j, for d ∈ [2, D − 1], s ∈ Sd; (cid:6)arg,D,s(u) for d ∈ [1, D − 1], u ∈ Sd+1 and 1 ≤ i ≤ j ≤ Ti:ifor s ∈ S D and i ∈ [1, T ];Initialise: (cid:6)max,D,si:i= 1 for all i ∈ [1, T ] and s ∈ S D/* At the level d=D-1 */For i = 1, 2, ..., T ; j = i, i + 1, ..., TCompute αmax,D−1,sCompute (cid:6)max,D−1,si: j(u) using Eq. (57) and αarg,D−1,susing Eq. (54) and (cid:6)arg,D−1,si: ji: ji: jusing Eq. (59)(u) using Eq. (62)EndFor/* The main recursion loops: bottom-up and forward */For d = D − 2, D − 3, ..., 1For i = 1, 2, ..., T ; j = i, i + 1, ..., TIf j = iCompute αmax,d,si:i(u) using Eq. (58)ElseCompute αmax,d,si: j(u) using Eq. (56) and αarg,d,si:i(u) using Eq. (60)EndIfIf d > 1Compute (cid:6)max,d,si: jusing Eq. (54) and (cid:6)arg,d,si: jusing Eq. (59)EndIfEndForEndForCompute (cid:6)max,1,s1:Tusing Eq. (54) and (cid:6)arg,1,s1:Tusing Eq. (59)5.2. Decoding the MAP assignmentThe proceeding of the backtracking process is opposite to that of the max-product. Specifically, we start from the root and proceed in a top-down and right-left manner. The goal is to identify the right-most segment at each level. Formally, a segment is a triple (s, i, j) where s is the segment label, and i and j are start and end time indices, respectively. From the maximum inside (cid:6)max,d,sat level d, we identify the best child u and its ending time j from Eq. (54). This gives rise to the maximum asymmetric inside αmax,d,s(u). Then we seek for the best child v that transits to u under the same parent susing Eq. (56). Since the starting time t for u has been identified the ending time for v is t − 1. We now have a right-most segment (u, t, j) at level d + 1. The procedure is repeated until we reach the starting time i of the parent s. The backtracking algorithm is summarised in Algorithm 5.i: ji: jFinally, the generalised Viterbi algorithm is given in Algorithm 6.Working in log-space to avoid numerical overflowWith long sequence and complex topology we may run into the problem of numerical overflow, i.e. when the numerical value of the maximum joint potential is beyond the number representation of the machine. To avoid this, we can work in the log-space instead, using the monotonic property of the log function. The equations in the log-space are summarised in Table 7.6. Parameter estimationIn this section, we tackle the problem of parameter estimation by maximising the (conditional) data likelihood. Typically we need some parametric form to be defined for a particular problem and we need some numerical method to do the optimisation task.Here we employ the log-linear parameterisation, which is commonly used in the CRF setting. Recall from Section 2.1that estimating parameters of the log-linear models using gradient-based methods requires the computation of feature expectation, or expected sufficient statistics (ESS). For our HSCRFs we need to compute four types of ESS corresponding to the state-persistence, state-transition, state-initialisation and state-ending.70T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Algorithm 5 Backtracking for optimal assignment (nested Markov blankets).Input: D, T , all the filled bookkeepers.Output: the optimal assignment ζ M A Pˆ(cid:6)max,1,s∗ = arg maxs∈S 1s1:TInitialise triple buckets I 1 = {(sFor d = 1, 2, ..., D − 1For each triple (s∗, i, j) in I d∗ = (cid:6)arg,d,si: j∗Let uFor i ≤ j∗, 1, T )} and I d = {} for d ∈ [2, D]∗) is defined Then∗If αarg,d,s(ui: j∗) = αarg,d,s∗, v(ti: jAdd the triple (v∗(u∗, t∗)∗, j) to I d+1 and Set j = t∗ − 1 and u∗ = v∗ElseAdd the triple (u∗, i, j) to I d+1 and Break this loopEndIfEndForEndForEndFor∗, i, j) in the bucket I d, for d ∈ [1, D],For each stored triple (screate a corresponding set of variables (xdi: jThe joining of these sets is the optimal assignment ζ M A P= 1, edj∗, ed= si−1= 1, edi: j−1= 0).Algorithm 6 The generalised Viterbi algorithm.Input: D, T , all the potential function values.Output: the optimal assignment ζ M A PRun the bottom-up discrete optimisation procedure described in Algorithm 4.Run the top-down backtracking procedure described in Algorithm 5.Table 7MAP equations in the log-space.Log-space equationslog (cid:6)max,d,si: j= maxu∈Sd+1 {log Ed,su, j+ log αmax,d,si: j(u)}log αmax,d,si: j(u) = max+ log ˆ(cid:6)max,d+1,ut: jmaxt∈[i+1, j] maxv∈Sd+1 {log αmax,d,s}; log ˆ(cid:6)max,d+1,ui: ji:t−1+ log π d+1,s(v) +!v,u,t−1u,i+ log Ad,slog αmax,D−1,si: j(u) = maxv∈S D {log αmax,D−1,si: j−1+ log A D,s+ log ˆ(cid:6)max,D,uj: j}v,u, j−1(v) +log αmax,d,si:i(u) = log ˆ(cid:6)max,d+1,ui:i+ log π d,su,i6.1. Log-linear parameterisationEquationsEq. (54)Eq. (56)Eq. (57)Eq. (58)In our HSCRF setting there is a feature vector fd(cid:14)wσ (σ , z) associated with each type of contextual clique σ , in that φ(σ d, z) =. Thus, the features are active only in the context in which the corresponding contextual cliques appear.(cid:6)exp(cid:15)σ (σ , z)σ d fdFor the state-persistence contextual clique, the features incorporate state-duration, start time i and end time j of the state. Other feature types incorporate the time index in which the features are triggered. Specifically,Rd,s,zi: j= expAd,s,zu,v,i= expπ d,s,zu,i= expEd,s,zu,i= exp(cid:14)w(cid:14)w(cid:14)w(cid:14)w(cid:6)(cid:6)(cid:15)σ persist (i, j, z)(cid:15)(i, z)σ transit ,u,v(cid:15)(i, z)(cid:15)(i, z)σ persist,d fd,sσ transit,d fd,sσ init,d fd,sσ end,d fd,sσ init ,uσ end,u(cid:6)(cid:6)(63)(64)(65)(66)Denote by Fdσ (ζ, z) the global feature, which is the sum of all active features fdgiven assignment of ζ and a clique type σ . Recall that τ d = {ik}mfeature types are given in Eqs. (67)–(70).k=1 is the set of ending time indices (i.e. edikσ (z) at level d in the duration [1, T ] for a = 1). The four T. Tran et al. / Artificial Intelligence 246 (2017) 53–85(cid:5)ik∈τ d,k>1fd,sσ persist (ik + 1, ik+1, z)σ persist (ζ, z) = fd,sFd,sσ persist (1, i1, z) +Fd,sσ transit ,u,v(ζ, z) =(cid:5)ik /∈τ d−1,ik∈τ dfd,sσ transit ,u,v(ik, z)Fd,sσ init ,u(ζ, z) = fd,sσ init ,u,v(1, z) +Fd,sσ end,u(ζ, z) =(cid:5)ik∈τ dfd,sσ end,u,v(i, z)(cid:5)ik∈τ dfd,sσ init ,u,v(ik + 1, z)Substituting the global features into potentials in Eqs. (8), (9) we obtain the following log-linear model:Pr(ζ | z) = 1Z (z)expw(cid:6)σ c Fσ c (ζ, z)"(cid:5)c∈C#where C = {persist, transit, init, exit}.Again, for clarity of presentation we will drop the notion of z but implicitly assume that it is still in the each quantity.6.2. ESS for state-persistence featuresRecall from Section 6.1 that the feature function for the state-persistence fd,sσ persist (i, j) is active only in the context where (cid:15)d,si: j∈ ζ . Thus, Eq. (67) can be rewritten as(cid:5)(cid:5)Fd,sσ persist (ζ ) =fd,sσ persist (i, j)δi∈[1,T ]j∈[i,T ](cid:14)(cid:15)d,si: j(cid:15)∈ ζ(72)σ persist (i, j) is only active if there exists a symmetric Markov The indicator function in the RHS ensures that the feature fd,sblanket (cid:15)d,si: jin the assignment of ζ . Consider the following expectation(cid:15)(cid:15)(cid:15)(cid:14)(cid:5)Efd,sσ persist (i, j)δ∈ ζ ]=Pr(ζ )fd,sσ persist (i, j)δ(cid:14)(cid:15)d,si: j∈ ζ(cid:14)(cid:15)d,si: jζ= 1Z(cid:5)ζ(cid:5)[ζ ]fd,sσ persist (i, j)δ(cid:14)(cid:15)d,si: j(cid:15)∈ ζUsing the factorisation in Eq. (22) we can rewrite(cid:14)Efd,sσ persist (i, j)δ(cid:15)(cid:15)(cid:14)(cid:15)d,si: j∈ ζ= 1Z(cid:5)(cid:14)(cid:5)(cid:15)(cid:15)(cid:14)(cid:5)ˆζd,si: jˆζ d,si: jζi: j fd,sRd,sσ persist (i, j)δ(cid:14)(cid:15)d,si: j(cid:15)∈ ζNote that the elements inside the sum of the RHS are only non-zeros for those assignment of ζ that respect the persistent i: j , (cid:15)d,sstate sd(cid:15)(cid:14)i: j and the factorisation in Eq. (22), i.e. ζ = (ζ d,si: j ). Thus, the equation can be simplified toi: j , ζ d,s(cid:15)(cid:15)(cid:15)(cid:14)(cid:14)Efd,sσ persist (i, j)δ(cid:14)(cid:15)d,si: j∈ ζ= 1Z(cid:5)(cid:5)(cid:5)ζ d,si: jζ d,si: jˆζ d,si: j(cid:5)ˆζd,si: ji: j fd,sRd,sσ persist (i, j)= 1Z(cid:6)d,si: j (cid:7)d,si: j Rd,si: j fd,sσ persist (i, j)Using Eq. (72) we obtain the ESS for the state-persistence features(cid:14)(cid:15)(cid:15)(cid:14)(cid:5)(cid:5)Efd,sσ persist (i, j)δ(cid:14)(cid:15)d,si: j∈ ζ(cid:15)F d,sk (ζ )=Ei∈[1,T ]j∈[i,T ](cid:5)(cid:5)= 1Zi∈[1,T ]j∈[i,T ](cid:6)d,si: j (cid:7)d,si: j Rd,si: j fd,sσ persist (i, j)71(67)(68)(69)(70)(71)(73)(74)(75)(76)(77)(78)There are two special cases: (1) when d = 1, we do not sum over i, j but fix i = 1, j = T , and (2) when d = D then we keep j = i.72T. Tran et al. / Artificial Intelligence 246 (2017) 53–856.3. ESS for transition featuresRecall that in Section 6.1 we define fd,s, in which the child state ud finishes its job at time t and transits to the child state vd under the same parent sd−1 (that is sd−1is still running). Thus Eq. (68) can be rewritten as(t) as a function that is active in the context ctransit == 0, edtσ transit ,u,vt(cid:14)ed−1(cid:15)= 1Fd,sσ transit ,u,v(ζ ) =fd,sσ transit ,u,v(t)δ(cid:5)t∈[1,T −1](cid:14)ctransit ∈ ζ(cid:15)We now consider the following expectation(cid:14)ctransit ∈ ζ(t)δ(cid:5)(cid:15)(cid:15)=E(cid:14)fd,sσ transit ,u,vPr(ζ )fd,sσ transit ,u,v(cid:14)ctransit ∈ ζ(cid:15)(t)δζ= 1Z(cid:5)ζ(cid:5)[ζ ]fd,sσ transit ,u,v(cid:14)ctransit ∈ ζ(cid:15)(t)δAssume that the parent s starts at i. Since edt= 1, the child v must starts at t + 1 and ends some time later at j ≥ t + 1. We have the following decomposition of the configuration ζ that respects this assumption(cid:16)ζ =ˆζd−1,si: j(v), ˆζ d−1,si:t(u), ˆζ d,vt+1: j(cid:17)and the following factorisation of the joint potentialˆζ d,vt+1: j(cid:5)[ζ ] =(cid:5)ˆζ d−1,si:t(cid:15)(v)(cid:15)(u)d−1,si: j(cid:5)(cid:5)ˆζ(cid:14)(cid:14)(cid:14)(cid:15)t+1: j Ad,sRd,vu,v,tThe state persistent potential Rd,vpotential Ad,st+1: ju,v,t in the context ctransit .is enabled in the context c =(82)(83)(cid:14)edt= 1, edt+1: j−1= 0, edj(cid:15)= 1and the state transition Substituting this factorisation into the RHS of Eq. (81) gives us(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:14)(cid:14)1ˆζd−1,si: j(cid:15)(v)(cid:5)ˆζ d−1,si:t(cid:14)(cid:15)(u)(cid:5)ˆζ d,vt+1: j(cid:15)t+1: j Ad,sRd,vu,v,t fd,sσ transit ,u,vZi∈[1,t]j∈[t+1,T ]ζ d−1,si:twhich can be simplified to(cid:5)(u)ζ d−1,si: j(v)ζ d,vt+1: j1Z(cid:5)(cid:5)i∈[1,t]j∈[t+1,T ]λd−1,si: j(v)αd−1,si:t(u) ˆ(cid:6)d,vt+1: j Ad,su,v,t fd,sσ transit ,u,v(t)Using Eqs. (79) and (84) we obtain the ESS for the state-transition features(cid:14)(cid:14)(cid:5)Efd,sσ transit ,u,v(cid:15)(t)δ[ctransit ∈ ζ ]EFd,sσ transit ,u,v= 1Z(cid:5)t∈[1,T −1](cid:15)(ζ )=t∈[1,T −1]u,v,t fd,sAd,sσ transit ,u,v(cid:5)(cid:5)(t)αd−1,si:t(u)λd−1,si: j(v) ˆ(cid:6)d,vt+1: jWhen d = 2 we must fix i = 1 since α1,si: j (v) are only defined at i = 1.i∈[1,t]j∈[t+1,T ]i:t (u) and λ1,s6.4. ESS for initialisation featuresRecall that in Section 6.1 we define fd,sd initialises a child u at level d + 1. In this event, the context cinit =can be rewritten asσ init ,u(i) as a function at level d that is triggered at time i when a parent s at level must be activated for i > 1. Thus, Eq. (69)(cid:19)= 1(cid:18)edi−1(79)(80)(81)(86)(87)(t)(84)(85)Fd,sσ init ,u(ζ ) =fd,sσ init ,u(i)δ(cid:5)i∈[1,T ](cid:15)(cid:14)cinit ∈ ζNow we consider the following feature expectation(cid:14)(cid:15)cinit ∈ ζ(i)δ[cinit ∈ ζ ]Pr(ζ )fd,s(i)δ(cid:5)=E(cid:14)(cid:15)fd,sσ init ,uσ init ,uζ= 1Z(cid:5)ζ(cid:5)[ζ ]fd,sσ init ,u(i)δ(cid:15)(cid:14)cinit ∈ ζT. Tran et al. / Artificial Intelligence 246 (2017) 53–85For each assignment of ζ that enables fd,s(i), we have the following decompositionσ init ,u(cid:16)ζ =ˆζd,si: j(u), ˆζ d+1,ui: j(cid:17)where the context cinit activates the emission from s to u and the feature function fd,scan be factorised asσ init ,u(cid:5)[ζ ] =(cid:5)(cid:14)ˆζd,si: j(cid:14)(cid:15)(u)(cid:5)(cid:15)ˆζ d+1,ui: jRd+1,ui: jπ d,su,i73(88)(i). Thus the joint potential (cid:5)[ζ ](89)Using this factorisation and noting that the elements within the summation in the RHS of Eq. (87) are only non-zeros with such assignments, we can simplify the RHS of Eq. (87) to(cid:14)(cid:5)ˆζd,si: j(cid:14)(cid:15)(u)(cid:5)ˆζ d+1,ui: j(cid:15)Rd+1,ui: ju,i fd,sπ d,sσ init ,u(i)1Z(cid:5)(cid:5)(cid:5)j∈[i,T ]ζ d,si: j (u)ζ d+1,ui: j= 1Z(cid:5)j∈[i,T ]i: j (u) ˆ(cid:6)d+1,uλd,si: ju,i fd,sπ d,sσ init ,u(i)The summation over j ∈ [i, T ] is due to the fact that we do not know this index.Using Eqs. (86), (90) we obtain the ESS for the initialisation features(cid:14)EFd,sσ init ,u(cid:15)(ζ )(cid:5)=(cid:14)Efd,sσ init ,u(cid:15)(i)δ[cinit ∈ ζ ]i∈[1,T ](cid:5)= 1Zi∈[1,T ]u,i fd,sπ d,sσ init ,u(i)(cid:5)j∈[i,T ]i: j (u) ˆ(cid:6)d+1,uλd,si: j(90)(91)There are two special cases: (1) when d = 1, there must be no scanning of i but fix i = 1 since there is only a single initialisation at the beginning of the sequence, (2) when d = D − 1, we fix j = i for ˆ(cid:6)D,ui: jis only defined at i = j.6.5. ESS for ending featuresRecall that in Section 6.1 we define fd,sσ end,u( j) as a function that is activated when a child u at level d + 1 returns the control to its parent s at level d and time j. This event also enables the context cend =rewritten asFd,sσ end,u(ζ ) =fd,sσ end,u( j)δ(cid:5)j∈[1,T ](cid:15)(cid:14)cend ∈ ζNow we consider the following feature expectation(cid:14)cend ∈ ζ(cid:15)( j)δ[cend ∈ ζ ]Pr(ζ )fd,s( j)δ(cid:5)=E(cid:14)(cid:15)fd,sσ end,uσ end,uζ= 1Z(cid:5)ζ(cid:5)[ζ ]fd,sσ end,u( j)δ(cid:15)(cid:14)cend ∈ ζAssume that the state s starts at i and ends at j. For each assignment of ζ that enables fd,stion, we have the following decompositionσ end,u(cid:16)(cid:17)i: j (u), ˆζ d,sˆζd,si: jζ =(cid:14)edj(cid:15)= 1. Thus Eq. (70) can be (92)(93)( j) and respects this assump-This assignment has the context cend that activates the ending of u. Thus the joint potential (cid:5)[ζ ] can be factorised as(cid:5)[ζ ] =(cid:5)(cid:14)(cid:15)ˆζd,si: j(cid:14)(cid:15)ˆζ d,si: j (u)(cid:5)i: j Ed,sRd,su, jSubstituting this factorisation into the summation of the RHS of Eq. (93) yields(cid:15)ˆζ d,si: j (u)i: j Ed,sRd,si: j αd,su, jfd,s( j) =ˆ(cid:7)d,si: j (u)Ed,s(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)ˆζ(cid:14)(cid:14)(cid:15)d,si: jσ end,uu, jfd,sσ end,u( j)i∈[1, j]ζ d,si: jζ d,si: j (u)i∈[1, j](94)(95)(96)74T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Using Eqs. (92) and (96) we obtain the ESS for the exiting features(cid:14)EFd,sσ end,u(cid:15)(ζ )=(cid:5)(cid:14)Efd,sσ end,u( j)δ[edi−1(cid:15)∈ ζ ]j∈[1,T ](cid:5)= 1Zj∈[1,T ]u, jfd,sEd,sσ end,u( j)(cid:5)i∈[1, j]ˆ(cid:7)d,si: j αd,si: j (u)(97)There is a special case: when d = 1 there must be no scanning of i, j but fix i = 1, j = T .7. Partial labels in learning and inferenceSo far we have assumed that training data is fully labelled, and that testing data does not have any labels. In this section we extend the AIO to handle the cases in which these assumptions do not hold. Specifically, it may happen that the training data is not completely labelled, possibly due to lack of labelling resources. In this case, the learning algorithm should be robust enough to handle missing labels. On the other hand, during inference, we may partially obtain high quality labels from external sources. This requires the inference algorithm to be responsive to that data.7.1. The constrained AIO algorithmIn this section we consider the general case when ζ = (ϑ, h), where ϑ is the visible set labels, and h the hidden set. Since our HSCRF is also an exponential model it shares the same computation required for general CRFs (Eqs. (6) and (7)). We have to compute four quantities: the partial log-partition function Z (ϑ, z), the partition function Z (z), the ‘constrained’ ESS Eh|ϑ,z[F(ϑ, h, z)], and the ‘free’ ESS Eζ |z[F(ζ, z)]. The partition function and the ‘free’ ESS has been computed in Sections 4and 6, respectively. This section describes the other two quantities.Let the set of visible labels be ϑ = ($x, $e) where $x is the visible set of state variables and $e is the visible set of ending indicators. The basic idea is that we have to modify procedures for computing the building blocks such as (cid:6)d,si: j (u), to address constraints imposed by the labels. For example, (cid:6)d,simplies that the state s at level d starts at i and persists till i: jterminating at j. Then, if any labels (e.g. there is an $xd(cid:10)= s for k ∈ [i, j]) are seen, causing this assumption to be inconsistent, k(cid:6)d,si: j will be zero. Therefore, in general, the computation of each building block is multiplied by an identity function that enforces the consistency between these labels and the required constraints for computation of that block. As an example, we consider the computation of (cid:6)d,si: j and αd,sThe symmetric inside mass (cid:6)d,sis consistent only if all of the following conditions are satisfied:i: ji: j and αd,si: j (u).1. If there are state labels $xd2. If there is any label of ending indicator $ed3. If there is any label of ending indicator $ed4. If any ending indicator $edk at level d within the interval [i, j], then $xdi−1, then $ed= 1,k for some k ∈ [i, j − 1], then $edj is labelled, then $ed= 1.i−1kkj= s,= 0, and(cid:15)These conditions are captured by using the following identity function:(cid:14)$edi−1(cid:14)$xdk∈[i, j] = sWhen labels are observed, Eq. (40) is thus replaced by(cid:14)$edk∈[i: j−1] = 0(cid:14)(cid:6)d,si: j(cid:14)$edj= 1= δ= 1(cid:15)(cid:15)(cid:15)δδδI(cid:6)d,si: j= I(cid:14)(cid:6)d,si: j(cid:15) (cid:29) (cid:5)(cid:30)i: j (u)Ed,sαd,su, j(cid:15)(98)(99)u∈Sd+1Note that we do not need to explicitly enforce the state consistency in the summation over u since in the bottom-up and left-right computation, αd,si: j (u) is already computed and contributes to the sum only if it is consistent.Analogously, the asymmetric inside mass αd,si: j (u) is consistent if all of the following conditions are satisfied:1. The first three conditions for the symmetric inside mass (cid:6)d,s2. If the state at level d at time j is labelled, it must be u, and3. If any ending indicator $ed+1is labelled, then $ed+1= 1.i: j hold,jjThese conditions are captured by the identity function(cid:15)(cid:14)αd,si: j (u)I(cid:14)$xdk∈[i, j] = s(cid:15)δ(cid:14)$edi−1(cid:15)= 1δ(cid:14)$edk∈[i: j−1] = 0(cid:15)(cid:14)$xd+1jδ(cid:15)= uδ(cid:14)$ed+1j(cid:15)= 1(100)= δT. Tran et al. / Artificial Intelligence 246 (2017) 53–8575Thus Eq. (36) becomes(cid:6)(cid:15)(cid:14)αd,si: j (u)αd,si: j (u) = Ij(cid:5)(cid:5)k=i+1v∈Sd+1i:k−1(v) ˆ(cid:6)d+1,uαd,sk: jAd,sv,u,k−1+ ˆ(cid:6)d+1,ui: jπ d+1,su,i(101)(cid:7)k: jNote that we do not need to explicitly enforce the state consistency in the summation over v and time consistency in the i: j (u) and (cid:6)d+1,usummation over k since in bottom-up computation, αd,sare already computed and contribute to the sum only if they are consistent. Finally, the constrained partition function Z (ϑ, z) is computed using Eq. (25) given that the inside mass is consistent with the observations.Other building blocks, such as the symmetric outside mass (cid:7)d,si: j and (cid:6)d,s(cid:14)i: j are complementary and they share (d, s, i, j), the same indicator function I in an analogous way. Since (cid:7)d,scan be applied. Similarly, the pair asymmetric inside mass αd,si: j (u) and asymmetric outside mass λd,s(cid:14)and they share d, s, i, j, u, thus the same indicator function I can be applied.(cid:15)αd,si: j (u)Once all constrained building blocks have been computed they can be used to calculate constrained ESS as in Section 6without any further modifications. The only difference is that we need to replace the partition function Z (z) by the con-strained version Z (ϑ, z).i: j and the asymmetric outside mass λd,si: j (u) are complementary i: j (u), are computed (cid:15)(cid:6)d,si: j7.2. The constrained Viterbi algorithmRecall that in the Generalised Viterbi Algorithm described in Section 5 we want to find the most probable configuration ζ M A P = arg maxζ Pr(ζ | z). When some variables ϑ of ζ are labelled, it is not necessary to estimate them. The task is now to estimate the most probable configuration of the hidden variables h given the labels:hM A P = arg maxhPr(h | ϑ, z) = arg maxh(cid:5)[h, ϑ, z]It turns out that the constrained MAP estimation is identical to the standard MAP except that we have to respect the labelled variables ϑ .Since the Viterbi algorithm is just the max-product version of the AIO, the constrained Viterbi can be modified in the (u), same manner as in the constrained AIO (Section 7.1). Specifically, for each auxiliary quantities such as (cid:6)max,swe need to maintain a set of indicator functions that ensures the consistency with labels. Eqs. (98), (99) becomeand αmax,si: ji: j(cid:15)(cid:15)(cid:14)(cid:6)max,d,si: jI= δ(cid:6)max,d,si: j= I(cid:14)$xdk∈[i, j] = s(cid:15) (cid:29)(cid:14)(cid:6)max,d,si: j(cid:14)$edi−1δ(cid:15)= 1δ(cid:14)$edk∈[i: j−1] = 0(cid:30)(cid:15)(cid:14)$edjδ(cid:15)= 1maxu∈Sd+1αmax,d,si: j(u)Ed,su, jLikewise, we have the modifications to Eq. (100) and Eq. (101), respectively.(cid:15)(cid:15)(cid:15)(cid:14)αmax,d,si: j(cid:15)(u)I(cid:14)$xdk∈[i, j] = s= δ= 1δ(cid:14)$edk∈[i: j−1] = 0(cid:14)$xd+1j(cid:15)= uδ(cid:14)$ed+1j(cid:15)= 1δδ(cid:14)$edi−1(cid:28)(cid:14)αmax,d,si: j(u) = Iαmax,d,si: j(cid:15)(u)maxmaxk∈[i+1, j]maxv∈Sd+1i:k−1 (v) ˆ(cid:6)max,d+1,uαmax,d,sk: jAd,sv,u,k−1;(cid:31)ˆ(cid:6)max,d+1,ui: jπ d+1,su,i(102)(103)Other tasks in the Viterbi algorithm including bookkeeping and backtracking are identical to those described in Section 5.7.3. Complexity analysisThe complexity of the constrained AIO and constrained Viterbi has an upper bound of O(T 3), when no labels are given. It also has a lower bound of O(T ) when all ending indicators are known and the model reduces to the standard tree-structured graphical model. In general, the complexity decreases as more labels are available, and we can expect a sub-cubic time behaviour.Learning requires both the constrained ESSes and free ESSes to be computed. Regardless of labels, the free ESSes still require cubic time. Thus with less labels, the overall computation will increase slightly.76T. Tran et al. / Artificial Intelligence 246 (2017) 53–858. Numerical scalingIn previous sections, we have derived AIO-based inference and learning algorithms for both unconstrained and con-strained models. The quantities computed by these algorithms like the inside/outside masses often involve summation over exponentially many positive potentials. The potentials, when estimated from data, are not upper-bounded, causing the mag-nitude of the masses to increase exponentially fast in the sequence length T . The magnitude goes beyond the numerical capacity of most machines for moderate T . In this section we present a scaling method to reduce this numerical overflowproblem.8.1. Scaling symmetric/asymmetric inside massesLet us revisit Eq. (40). If we scale down the asymmetric inside mass αd,si: j (u) by a factor κ j > 1, i.e.α(cid:3) d,si: j (u) ←αd,si: j (u)κ j(104)then the symmetric inside mass (cid:6)d,si: jis also scaled down by the same factor. Similarly, as we can see from Eq. (36) thatαd,si: j (u) =j(cid:5)(cid:5)t=i+1v∈Sd+1i:t−1(v) ˆ(cid:6)d+1,uαd,st: jAd,sv,u,t−1+ ˆ(cid:6)d+1,ui: jπ d,su,it: j= (cid:6)d+1,ut: jwhere ˆ(cid:6)d+1,uaddition, using the set of recursive relations in Eqs. (36), (40), any reduction at the bottom level of (cid:6)D,sreduction of the symmetric inside mass (cid:6)d,sfor t ∈ [1, j] is reduced by κ j , then αd,si: jis also reduced by the same factor. In j: j will result in the , if (cid:6)d+1,ut: jRd+1,ut: ji: j and of the asymmetric inside mass αd,sis reduced by a factor of κi > 1 for all i ∈ [1, j], the quantities (cid:6)d,si: j (u), for d < D, by the same factor.1: j and αd,s1: j (u) will be reduced by a factor Suppose (cid:6)D,si:i(cid:4)i=1 κi . That isjof ˆ(cid:6)(cid:3) d,s1: j←α(cid:3) d,s1: j (u) ←ˆ(cid:6)d,s1: j(cid:4)ji=1 κiαd,s1: j (u)(cid:4)ji=1 κiIt follows immediately from Eq. (25) that the partition function is scaled down by a factor of (cid:4)Ti=1 κi(cid:3) =Z(cid:5)s∈S 1ˆ(cid:6)(cid:3) 1,s1:T=(cid:4)ZTj=1 κ j(105)(106)(107)(cid:3) 1,swhere ˆ(cid:6)1:T B1,spartition function can be computed as(cid:3) 1,s1:T= (cid:6)1:T . Clearly, we should deal with the log of this quantity to avoid numerical overflow. Thus, the log-log(Z ) = log(cid:5)s∈S 1ˆ(cid:6)(cid:3) 1,s1:T+T(cid:5)j=1log κ j(108)where (cid:6)(cid:3) 1,s1:T has been scaled appropriately.One question is how to choose the set of meaningful scaling factors {κ j}T1 . The simplest way is to choose a relatively large number for all scaling factors but making the right choice is not straightforward. Here we describe a more natural way to do so. Assume that we have chosen all the scaling factors {κi } j−1. Using the original Eqs. (36), (37), and (38), where all for d ∈ [2, D] and the sub-components have been scaled appropriately, we compute the partially-scaled inside mass (cid:6)asymmetric inside mass α(cid:3)(cid:3) d,s(cid:5)(u), for d ∈ [1, D − 1] and i ∈ [1, j]. Then the scaling factor at time j is computed as(cid:3)(cid:3) d,si: ji: j1(109)κ j =α(cid:3)(cid:3) 1,s1: j (u)s,uThe next step is to rescale all the partially-scaled variables:T. Tran et al. / Artificial Intelligence 246 (2017) 53–8577(cid:6)α(cid:3)(cid:3) d,si: jκ j(cid:3)(cid:3) d,si: jκ j(cid:3)(cid:3) D,sj: jκ j(cid:6)α(cid:3) d,si: j (u) ←(cid:6)(cid:3) d,si: j←(cid:6)(cid:3) D,sj: j←where i ∈ [1, j].(u)for s ∈ Sd, d ∈ [1, D − 1]for s ∈ Sd, d ∈ [2, D − 1]for s ∈ S D(110)(111)(112)8.2. Scaling symmetric/asymmetric outside massesIn a similar fashion we can work out the set of factors from the derivation of symmetric/asymmetric outside masses since these masses solely depend on the inside masses as building blocks. In other words, after scaling the inside masses we can compute the scaled outside masses directly, using the same set of equations described in Section 4.3.The algorithm is summarised in Algorithm 7. Note that the order of performing the loops in this case is different from that in Algorithm 1.Algorithm 7 Scaling algorithm to avoid numerical overflow.Input: D, T and all the contextual potentials.Output: Scaled inside/asymmetric inside masses, outside/asymmetric outside masses.For j = 1, 2, .., TCompute αd,sCompute κ j using Eq. (109)Rescale α1,sFor i = 1, 2, .., j1: j (u), d ∈ [1, D − 1] using Eqs. (36), (37) and (38)1: j (u) using Eq. (110)For d = 2, 3, .., D − 1Rescale αd,sRescale (cid:6)d,si: j (u) using Eq. (110)i: j using Eq. (111)EndForEndForRescale (cid:6)D,sj: j using Eq. (112)EndForCompute true log-partition function using Eq. (108).Compute the outside/asymmetric outside masses using thescaled inside/asymmetric inside masses instead of the originalinside/asymmetric inside in Eqs. (43) and (47).9. ApplicationsIn this section we present experimental results to demonstrate the capacity of the proposed HSCRFs in two applications: activity recognition and shallow parsing.9.1. Recognising indoor activitiesIn this experiment, we evaluate the HSCRFs with a relatively small dataset from the domain of indoor video surveillance. The task is to recognise indoor trajectories and activities of a person from his noisy positions extracted from video. The data was captured in [21], and was subsequently used to evaluate DCRFs in [22]. The raw data consists of 90 sequences of noisy coordinates. Each time step is manually annotated by two labels: the complex and primitive activities. There are three complex activities (preparing-short-meal, having-snack and preparing-normal-meal); and 12 primitive activities listed in Table 8. As in [22], the data is split into two sets of equal size for training and testing, respectively.We assume that state-specific features such as initialisation, transition and exiting are indicator functions. For the data-associations (i.e. embedded in state-persistence potentials) at the bottom level, we use the same feature set as in [22], which are: the ( X, Y ) coordinates, the X & Y velocities, and the speed. At the second level during duration of a state, we use average velocities and a vector of positions visited within that duration. To encode the duration into the state-persistence potentials, we employ the sufficient statistics of the gamma distribution as features fk(s, (cid:6)t) = I(s) log((cid:6)t) and fk+1(s, (cid:6)t) = I(s)((cid:6)t).78T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Table 8Primitive activities [21].No.123456ActivityDoor→CupboardCupboard→FridgeFridge→Dining chairDining chair→DoorDoor→TV chairTV chair→CupboardNo.789101112ActivityFridge→TV chairTV chair→DoorFridge→StoveStove→Dining chairFridge→DoorDining chair→FridgeTable 9Accuracy (%) for fully observed labels (left), and 50% partially observed (PO) labels (right).Alg.HSCRFDCRFflat-CRFd = 210096.5–d = 393.989.782.6Alg.PO-HSCRFPO-CRF–d = 280.2––d = 390.483.5–Fig. 9. The topology learned from data.Fig. 10. The state transition model learned from data. Primitive states are duplicated for clarity only. They are shared among complex states.For learning, labels for each sequence are provided fully for the case of fully observed state data, and partially for the case of missing state data. For testing, at each level d and time t we count an error if the predicted state is not the same as the ground-truth.9.1.1. Fully supervised learningFirstly, we examine the fully observed case where the HSCRF is compared against the DCRF at both data levels, and against the flat-CRF at bottom level. Table 9 (the left half) shows that (a) both the multilevel models significantly outperform the flat model and (b) the HSCRF outperforms the DCRF.We also test the ability of the model to learn the hierarchical topology and state transitions. Using log-linear parameter-isation described in Section 6.1, the parent–child relationship in the topology and the state-transitions are captured in the corresponding parameters. Only positive parameters are used. This is because state features are non-negative, so negative parameters mean the probabilities of these transitions are very small (due to the exponential), compared to the positive ones. For the transition at the second level (the complex activity level), we obtain all negative entries. This clearly matches the training data, in which each sequence already belongs to one of three complex activities. With this method, we are able to construct the correct hierarchical topology as in Fig. 9. The state transition model is presented in Fig. 10. There is only one wrong transition, from state 12 to state 10, which is not presented in the training data. The rest is correct.9.1.2. Partially supervised learningNext we consider partially-supervised learning in that about 50% of start/end times of a segment and segment labels are observed at the second level. All ending indicators are known at the bottom level. The results are reported in Table 9 (the T. Tran et al. / Artificial Intelligence 246 (2017) 53–8579Fig. 11. Performance of the constrained max-product algorithm as a function of available information on label/start/end time.right half). As can be seen, although only 50% of the state labels and state start/end times are observed, the model learned is still performing well with accuracy of 80.2% and 90.4% at levels 2 and 3, respectively.9.1.3. Prediction with partial labellingWe now consider the issue of using partial observed labels during decoding to improve prediction accuracy of poorly estimated models. We extract the parameters from the 10th iteration of the fully observed data case. The labels are provided at random time indexes. Fig. 11a shows the decoding accuracy as a function of available state labels. It is interesting to observe that a moderate amount of observed labels (e.g. 20–40%) causes the accuracy rate to go up considerably.9.2. POS tagging and noun-phrase chunkingIn this experiment we apply the HSCRF to the task of noun-phrase chunking. The data is from the CoNLL-2000 shared task [5], in which 8,926 English sentences from the Wall Street Journal corpus are used for training and 2,012 sentences are for testing. Each word in a pre-processed sentence is labelled by two labels: the part-of-speech (POS) and the noun-phrase (NP). There are 48 POS different labels and 3 NP labels (B-NP for beginning of a noun-phrase, I-NP for inside a noun-phrase or O for others). Each noun-phrase generally has more than one word. To reduce the computational burden, we reduce the POS tag-set to 5 groups: noun, verb, adjective, adverb and others. Since in our HSCRFs we do not have to explicitly indicate which node is at the beginning of a segment, the NP label set can be reduced further into NP for noun-phrase, and O for anything else.The POS tags are actually the output of the Brill’s tagger [23], while the NPs are manually labelled. We extract raw features from the text in the way similar to that in [8]. However, we consider only a limited vocabulary extracted from the training data in that we only select words with more than 3 occurrences. This reduces the vocabulary and the feature size significantly. We also make use of bi-grams with similar selection criteria. Furthermore, we use the contextual window of 5 instead of 7 as in [8]. This setting gives rise to about 32K raw features. The model feature is factorised as f (xc, z) =I(xc)gc(z), where I(xc) is a binary function on the assignment of the clique variables xc , and gc(z) are the raw features.We build an HSCRF topology of 3 levels where the root is just a dummy node, the second level has 2 NP states and the bottom level has 5 POS states. For comparison, we implement a DCRF, a simple sequential CRF (SCRF), and a semi-Markov CRF (SemiCRF) [13]. The DCRF has grid structure of depth 2, one for modelling the NP process and another for the POS process. Since the state spaces are relatively small, we are able to run exact inference in the DCRF by collapsing both the NP and POS state spaces to a combined state space of size 3 × 5 = 15. The SCRF and SemiCRF model only the NP process, taking the POS tags as input. The raw feature set used in the DCRF is identical to those in our HSCRF. However, the set shared by the SCRF and the SemiCRF is a little more elaborate since it takes the POS tags into account [8].Although both the HSCRF and the SemiCRF are capable of modelling arbitrary segment duration, we use a simple ex-ponential distribution as it can be processed sequentially and thus is very efficient. For learning, we use a simple online stochastic gradient ascent method since it has been shown to work relatively well and fast in CRFs [24]. At test time, as the SCRF and the SemiCRF are able to use the Brill’s POS tags as input, it is not fair for the DCRF and HSCRF to predict those labels during inference. Instead, we also give the POS tags to the DCRF and HSCRF and perform constrained inference to predict only the NP labels. This boosts the performance of the two multi-level models significantly.The performance of these models is depicted in Fig. 12 and we are interested in only the prediction of the noun-phrases since this data has Brill’s POS tags. Without Brill’s POS tags given at test time, both the HSCRF and the DCRF perform worse than the SCRF trained on POS tags. This is not surprising because the Brill’s POS tags are always given in the case of SCRF. However, with POS tags given at test time, the HSCRF consistently works better than all other models. In particular, our 80T. Tran et al. / Artificial Intelligence 246 (2017) 53–85Fig. 12. Performance of various models on Conll2000 noun-phrase chunking. HSCRF + POS and DCRF + POS mean HSCRF and DCRF with POS given at test time, respectively.HSCRF + POS is significantly better than the SCRF (with POS included as features) when training data is small. There is a subtle but important difference here: The HSCRF was trained without knowledge of POS availability at test time. In contrast, SCRF was trained with this knowledge so that good POS-based features were used.The DCRF does worse than the SCRF, even with POS tags given. This does not share the observation made in [8]. However, we use a much smaller POS tag set than [8] does. Our explanation is that the SCRF is able to make use of wider context of the given POS tags (here, within the window of 5 tags) than the DCRF (limited to 1 POS tag per NP chunk). The SemiCRF, although in theory it is more expressive than the SCRF, does not show any advantage under current setting. Recall that the SemiCRF is a special case of HSCRF in that the POS level is not modelled, it is possible to conclude that joint modelling of NP and POS levels is important.More formally, let us look at the difference between the flat setting of SCRF and Semi-CRF and the multi-level setting of DCRF and HSCRF. Let x = (xnp, xpos). Essentially, we are about to model the distribution Pr(x | z) = Pr(xnp | xpos, z) Pr(xpos | z)in the multi-level models while we ignore the Pr(xpos | z) in the flat models. During test time of the multi-level models, we predict only the xnp by finding the maximiser of Pr(xnp | xpos, z). The POS model Pr(xpos | z) seems to be a waste because we do not make use of it at test time. However, Pr(xpos | z) does give extra information about the joint distribution Pr(x | z), that is, modelling the POS process may help to get smoother estimate of the NP distribution.10. Related workHierarchical modelling of stochastic processes can be largely categorised as either graphical models extending the flat hidden (semi-)Markov models (HMM/HsMM) (e.g., the layered HMM [6], the abstract HMM [25], hierarchical HMM (HHMM) [2,15], DBN [26]), grammar-based models (e.g., PCFG [27]), or deterministic models [28,29]. These models are all directed while HSCRF is undirected.Higher-order extensions to the linear-chain CRFs have been developed recently [30,31]. These methods exploit sparsity in the state transition for efficient inference, but they are shallow models. Development in deeper structures include dynamic CRFs (DCRF) [8], hierarchical CRFs [9,10], and stacked CRFs [11]. The main difference between HSCRF and these CRF variants is that the hierarchical topology of HSCRF is dynamically inferred from data, unlike the others, where the topology is pre-defined by users. In term of inference complexity, DCRFs are not tractable in large-state settings. The hierarchical CRFs, on the other hand, are tractable but assume fixed tree structures, and therefore are not flexible to adapt to complex data. For example, the noun-phrase chunking problem does not assume prior tree structures. Rather, if such a structure exists, it can only be discovered after the model has been successfully built and learned.Our HSCRFs deal with the inference problem of DCRFs by limiting to recursive processes, and thus obtaining efficient inference via dynamic programming in the Inside–Outside family of algorithms. Furthermore, it generalises the SemiCRFs to model multilevel of semantics. It also addresses partial labels by introducing appropriate constraints to the Inside–Outside algorithms.As the HHMMs are special case of PCFG with bounded depth, HSCRFs are also special case of the conditional probabilistic context-free grammar (C-PCFG) (e.g. see [32,33]). Like HSCRFs, C-PCFG requires cubic time in sequence length to parse a sentence. However, the context-free grammar does not limit the depth of semantic hierarchy, thus making it unnecessarily difficult to map many hierarchical problems into its form. Secondly, it lacks a graphical model representation and thus does not enjoy the rich set of approximate inference techniques available in graphical models.The AIO algorithm presented in Section 4 is inspired from the AIO algorithm in HHMMs [15,2]. However, due to the log-linear parameterisation, there are no probabilistic interpretations of the inside and outside masses.T. Tran et al. / Artificial Intelligence 246 (2017) 53–8581The idea of numerical scaling presented in Section 8 can be traced back to the Pearl’s message-passing procedure [20,34]. In our AIO algorithms, the inside masses play the role of the inside-out messages. In Pearl’s method, we reduce the messages’ magnitude by normalising them at each step. The overflow problem is opposite to the underflow in directed counterparts. A similar idea has been proposed in [15] for HHMMs.The graphical model-like dynamic representation of the HSCRF appears similar to the DBN representation of the HHMMs in [17], and somewhat resembles a dynamic factor graph [35,36, Chap 9]. However, it is not exactly the standard graphical model because the contextual cliques in HSCRFs are not fixed during inference. This makes it difficult to apply approximate inference methods such as Loopy Belief Propagation (LBP) and Gibbs sampling, which are designed for fixed cliques. The Gibbs sampling can be applied to a special arrangement as in [37], but convergence is not guaranteed within limited time and our preliminary experiments have indicated no advantage compared to exact inference.11. ConclusionsIn this paper, we have presented a novel model called Hierarchical Semi-Markov Conditional Random Field which extends the standard CRFs to incorporate hierarchical and multilevel semantics. We have developed a graphical model-like dynamic representation of the HSCRF. We have derived efficient algorithms for learning and inference, especially the ability to learn and inference with partially given labels. We have demonstrated the capacity of the HSCRFs on home video surveillance data and the shallow parsing of English text, in which the hierarchical information inherent in the context helps to increase the recognition.In future work we plan to attack the computational bottleneck in large-scale settings. Although the AIO family has cubic time complexity, it is still expensive in large-scale application, especially those with long sequences. It is therefore desirable to introduce approximation methods that can provide speed/quality trade-offs. Our early work using Rao-Blackwellised Gibbs sampling shows promising results [37]. We also need to make a choice between pre-computing all the potentials prior to inference and learning, and computing them on-the-fly. The first choice requires O(D K 3 T 2) space, which is very significant with typical real-world problems, even with today’s computing power. The second choice, however, will slow the inference and learning very significantly due to repeated computation at every step of the AIO algorithm. Finally, it is interesting to see how good the HSCRFs can be an approximation to general multilevel processes, which are not necessarily recursive (e.g., HSCRF as an approximation to DCRFs). This is important because HSCRFs are tractable while DCRFs are generally not.AcknowledgementThis work is partially supported by the Telstra–Deakin Centre of Excellence in Big Data and Machine Learning.Appendix A. ProofsIn this appendix we give detailed proofs of propositions stated in the main text.A.1. Proof of Propositions 1 and 2Before proving Proposition 1 and 2 let us introduce a lemma.Lemma 1. Given a distribution of the form Pr(x) ∝ (cid:5)[x] and x = (xa, xs, xb), if there exists a factorisation(cid:5)[x] = (cid:5)[xa, xs](cid:5)[xs](cid:5)[xs, xb]then xa and xb are conditionally independent given xs.Proof. We want to prove thatSince Pr(xa, xb | xs) = Pr(xa, xb, xs)/ Pr(xa, xb, xs), the LHS of Eq. (A.2) becomesPr(xa, xb | xs) = Pr(xa | xs) Pr(xb | xs)(cid:3)Pr(xa, xb | xs) =(cid:3)xa,xb(cid:5)[xa, xs](cid:5)[xs](cid:5)[xs, xb](cid:5)[xa, xs](cid:5)[xs](cid:5)[xs, xb]xa,xb= (cid:5)[xa, xs](cid:3)(cid:5)[xa, xs]xa(cid:5)[xs, xb](cid:3)(cid:5)[xs, xb]xbwhere we have used the following fact(cid:5)xa,xb(cid:5)[xa, xs](cid:5)[xs](cid:5)[xs, xb] = (cid:5)[xs](cid:29)(cid:5)(cid:30)(cid:29)(cid:5)(cid:5)[xa, xs](cid:30)(cid:5)[xs, xb]xaxb(A.1)(A.2)(A.3)(A.4)82T. Tran et al. / Artificial Intelligence 246 (2017) 53–85(cid:3)(cid:5)To prove Pr(xa | xs) = (cid:5)[xa, xs]/ (cid:3)(cid:5)[xa, xs], we need only to show Pr(xa | xs) ∝ (cid:5)[xa, xs] since the normalisation over xa is due to xaxaPr(xa|xs) = 1. Using the Bayes rule, we havePr(xa | xs) ∝ Pr(xa, xs) =xb∝ (cid:5)[xa, xs](cid:5)[xs]∝ (cid:5)[xa, xs]Pr(xa, xs, xb)(cid:5)(cid:5)[xs, xb]xb(A.5)where we have ignored all the factors that do not depend on xa .A similar proof gives Pr(xb | xs) = (cid:5)[xs, xb]/ (cid:5)[xs, xb]. Combining this result and Eq. (A.5) with Eq. (A.3) gives us (cid:3)xbEq. (A.2). This completes the proof. (cid:2)In fact, xs acts as a separator between xa and xb. In standard Markov networks there are no paths from xa to xb that do not go through xs. Now we proceed to proving Propositions 1 and 2.Given the symmetric Markov blanket (cid:15)d,sζ d,si: j and ζ d,sbetween ζ d,si: j . The blanket completely separates the ζ d,si: j and ζ d,si: j .i: j , there are no potentials that are associated with variables belonging to both i: j . Therefore, Lemma 1 ensures the conditional independence i: j and ζ d,sSimilarly, the asymmetric Markov blanket (cid:16)d,si: j (u) separates ζ d,si: j (u) and ζ d,si: j (u) and thus these two variable sets are conditionally independent due to Lemma 1. (cid:2)A.2. Proof of Proposition 3Here we want to derive Eqs. (28), (29) and (30). With the same conditions as in Lemma 1, in Eq. (A.5) we have shown that Pr(xa | xs) ∝ (cid:5)[xa, xs]. Similarly, this extends to(cid:15)(cid:17)(cid:16)(cid:14)(cid:15)Prζ d,si: j| (cid:15)d,si: j∝ (cid:5)(cid:14)i: j , (cid:15)d,sζ d,si: j= (cid:5)ˆζ d,si: jwhich is equivalent to(cid:17)(cid:16)Prζ d,si: j| (cid:15)d,si: j=(cid:3)ζ d,si: j1(cid:14)(cid:5)ˆζ d,si: j(cid:15)(cid:14)ˆζ d,si: j(cid:15) (cid:5)(cid:15)(cid:14)(cid:5)ˆζ d,si: j= 1(cid:6)d,si: jThe last equation follows from the definition of the symmetric inside mass in Eq. (23). Similar procedure will yield Eq. (29).To prove Eq. (30), notice the Eq. (19) that says(cid:16)(cid:17)(cid:16)(cid:17)(cid:16)(cid:17)Pr(ζ ) = Pr(cid:15)d,si: jPrζ d,si: j] | (cid:15)d,si: jPrζ d,si: j| (cid:15)d,si: jor equivalentlyPr((cid:15)d,si: j ) = Pr(ζ )(cid:16)Pr1| (cid:15)d,sζ d,si: ji: j(cid:7)d,s(cid:6)d,si: ji: j(cid:5)[ ˆζ d,sd,s(cid:5)[ ˆζi: ji: j]]∝ (cid:5)[ζ ](cid:17)1(cid:16)| (cid:15)d,sζ d,si: ji: j(cid:17)Pr= (cid:5)[ ˆζ d,si: j]Rd,si: j (cid:5)[ ˆζ]d,si: j(cid:6)d,si: j(cid:5)[ ˆζ d,si: j](cid:7)d,si: jd,s(cid:5)[ ˆζi: j]= (cid:6)d,si: j Rd,si: j (cid:7)d,si: jIn the proof proceeding, we have made use of the relation in Eq. (22). This completes the proof. (cid:2)Appendix B. Computing state marginalsWe are interested in computing the marginals of state variables Pr(cid:16)(cid:17)(cid:5)(cid:16)(cid:17)(cid:5)(cid:14)(cid:15)Prxdt=Prt , ζ \xdxdt=Pr(ζ )δxdt∈ ζ(cid:13)(cid:12)xdt. We haveζ \xdt(cid:5)= 1Zζ(cid:14)ζ(cid:15)(cid:5)[ζ ]δxdt∈ ζ(A.6)(A.7)(A.8)(A.9)(A.10)(B.1)T. Tran et al. / Artificial Intelligence 246 (2017) 53–8583Fig. C.13. The SemiCRFs in our contextual clique framework.Let s = xdassumption, we have the factorisation of Eq. (22) that sayst and assume that the state s starts at i and end at j, and t ∈ [i, j]. For each configuration ζ that respects this (cid:5)[ζ ] =(cid:5)(cid:14)(cid:15)(cid:14)(cid:15)ˆζ d,si: j(cid:5)ˆζd,si: jRd,si: j(B.2)(B.3)(B.4)(B.5)(B.6)Then Eq. (B.1) becomes= s) = 1ZPr(xdt(cid:5)(cid:14)(cid:5)ˆζ d,si: j(cid:15)(cid:14)(cid:5)ˆζd,si: j(cid:15)Rd,si: j δ [t ∈ [i, j]]ζ(cid:5)= 1Zi∈[1,t]j∈[t,T ](cid:5)(cid:6)d,si: j (cid:7)d,si: j Rd,si: jThe summing over i and j is due to the fact that we do not know these indices.There are two special cases, (1) when d = 1 we cannot scan the left and right indices, the marginals are simplyPr(x1t= s) = 1Zˆ(cid:6)1,s1:Tsince (cid:7)1,s1:T= 1 for all s ∈ S 1; and (2) when d = D, the start and end times must be the same (i = j), thusPr(xDt= s) = 1Zˆ(cid:7)D,st:tsince (cid:6)D,st:t(cid:3)Since (cid:12)= 1 for all t ∈ [1, T ] and s ∈ S D .s∈Sd Pr(cid:5)= s(cid:5)(cid:5)xdt(cid:13)= 1, it follows from Eq. (B.3) thatZ =(cid:6)d,si: j (cid:7)d,si: j Rd,si: js∈Sdi∈[1,t]j∈[t,T ]This turns out to be the most general way of computing the partition function. Some special cases have been shown earlier. For example, when d = 1, i = 1 and j = T , Eq. (B.6) becomes Eq. (25) since (cid:7)1,s= 1. Similarly, when d = D, i = j = t, 1:TEq. (B.6) recovers Eq. (26) since (cid:6)D,si:i= 1.Appendix C. Semi-Markov CRFs as a special caseIn this appendix we show how to convert a semi-Markov CRF (SemiCRF) [13] into an HSCRF. SemiCRF is an interesting flat segmental undirected model that generalises the chain CRF. In the SemiCRF framework the Markov process operates at the segment level, where a segment is a non-Markovian chain of nodes. A chain of segments is a Markov chain. However, since each segment can potentially have arbitrary length, inference in SemiCRFs is more involved than the chain CRFs.Represented in our HSCRF framework (Fig. C.13), each node xt of the SemiCRF is associated with an ending indicator et , with the following contextual cliques• Segmental state, which corresponds to a single segment si: j and is essentially the state persistence contextual clique in the context c =(cid:19)(cid:18)ei−1: j = (1, 0, .., 0, 1)in the HSCRF’s terminology.• State transition, which is similar to the state transition contextual clique in the HSCRFs, corresponding to the context c = [et = 1].Associated with the segmental state clique is the potential R ss, s(cid:3) ∈ S, and S = {1, 2, ..., K }.A SemiCRF is a three-level HSCRF, where the root and bottom are dummy states. This gives a simplified way to compute the partition function, ESS, and the MAP assignment using the AIO algorithms. Thus, techniques developed in this paper for numerical scaling and partially observed data can be applied to the SemiCRF. To be more consistent with the literature of flat models such as HMMs and CRFs, we call the asymmetric inside/outside masses by the forward/backward, respectively. Since the model is flat, we do not need the inside and outside variables.i: j , and with the state transition is the potential As(cid:3),s,t , where (C.1)(C.2)84ForwardWith some abuse of notation, let ζ s1: jat j. We write the forward αt(s) as(cid:5)(cid:14)(cid:15)α j(s) =ζ s1: j, z(cid:5)ζ s1: jT. Tran et al. / Artificial Intelligence 246 (2017) 53–85(cid:12)(cid:13)x1: j−1, e1: j−1, x j = s, e j = 1=. In other words, there is a segment of state s ending As a result the partition function can be written in term of the forward as(cid:18)(cid:5)(cid:5)(cid:5)(cid:19)Z (z) =(cid:5) [ζ1:T , z] =ζ s1:T , z(cid:5)sζ s1:T=ζ1:T(cid:5)sαT (s)We now derive a recursive relation for the forward. Assume that the segment ending at j starts somewhere at i ∈ [1, j]. , which leads to the following (cid:17)1:i−1, xi: j = s, ei: j−1 = 0ζ sfor some s=(cid:16)(cid:3)(cid:3)Then for i > 1, there exists the decomposition ζ s1: jfactorisation(cid:15)(cid:14)ζ s1: j, z(cid:5)(cid:15)(cid:14)(cid:3)ζ s1:i−1= (cid:5)As(cid:3),s,i−1 R si: j(C.3)The transition potential As(cid:3),s,i−1 occurs in the context c = [ei−1 = 1], and the segmental potential R s(cid:18)(cid:19)xi: j = s, ei−1 = 1, ei: j−1 = 0(cid:15).i: j in the context c =(cid:14)For i = 1, the factorisation reduces to (cid:5) ζ s1: j, z= R s1: j . Since we do not know the starting i, we must consider all possible values in the interval [1, j. Thus, Eq. (C.1) can be rewritten as(cid:5)(cid:5)(cid:5)(cid:15)(cid:14)(cid:3)ζ s1:i−1As(cid:3),s,i−1 R si: j+ R s1: j(cid:3)αi−1(s) As(cid:3),s,i−1 R si: j+ R s1: j(cid:5)(cid:3)ζ s1:i−1α j(s) =i∈[2, j](cid:5)s(cid:3)(cid:5)i∈[2, j]s(cid:3)=BackwardThe backward is the ‘mirrored’ version of the forward. In particular, let ζ sj:Tdefine the backward βt (s) as(cid:15)(cid:5)(cid:14)ζ sj:T , zβ j(s) =(cid:5)ζ sj:TClearly, the partition function can be written in term of the backward asZ (z) =(cid:5)sβ1(s)The recursive relation for the backwardβi(s) =(cid:5)(cid:5)j∈[i,T −1]s(cid:3)R si: j As,s(cid:3), jβ j+1(s(cid:3)) + R si:T(C.4)(C.5)(cid:12)(cid:13)x j+1:T , e j:T , x j = s, e j−1 = 1=. and we (C.6)(C.7)(C.8)Typically, we want to limit the segment to the maximum length of L ∈ [1, T ]. This limitation introduces some special cases when performing recursive computation of the forward and backward. Eqs. (C.4) and (C.8) are rewritten as followsα j(s) =βi(s) =(cid:5)(cid:5)i∈[ j−L+1, j],i>1(cid:5)s(cid:3)(cid:5)j∈[i,i+L−1], j<Ts(cid:3)(cid:3)αi−1(s) As(cid:3),s,i−1 R si: j+ R s1: jR si: j As,s(cid:3), jβ j+1(s(cid:3)) + R si:T(C.9)(C.10)Finally, we can extend the HSCRF straightforwardly by allowing the bottom level states to persist. With this relaxation we have a nested semi-Markov CRF model in the sense that each segment in a Markov chain is also a Markov chain of sub-segments.ReferencesT. Tran et al. / Artificial Intelligence 246 (2017) 53–8585data, J. Mach. Learn. Res. 8 (Mar 2007) 693–723.119–134.Computer Vision, ICCV, vol. 2, Oct 2005, pp. 1284–1291.[1] Y. Bengio, learning deep architectures for AI, Found. Trends Mach. Learn. 2 (1) (2009) 1–127.[2] S. Fine, Y. Singer, N. Tishby, The hierarchical hidden Markov model: analysis and applications, Mach. Learn. 32 (1) (1998) 41–62.[3] G. Hinton, R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science 313 (5786) (2006) 504–507.[4] R. Salakhutdinov, G. Hinton, Deep Boltzmann machines, in: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, AISTATS’09, vol. 5, 2009, pp. 448–455.[5] E.F.T.K. Sang, S. Buchholz, Introduction to the CoNLL-2000 shared task: chunking, in: Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning, Lisbon, Portugal, vol. 7, 2000, pp. 127–132, http://www.cnts.ua.ac.be/conll2000/chunking/.[6] N. Oliver, A. Garg, E. Horvitz, Layered representations for learning and inferring office activity from multiple sensory channels, Comput. Vis. Image Underst. 96 (2) (2004) 163–180.[7] J. Lafferty, A. McCallum, F. Pereira, Conditional random fields: probabilistic models for segmenting and labeling sequence data, in: Proceedings of the International Conference on Machine Learning, ICML, 2001, pp. 282–289.[8] C. Sutton, A. McCallum, K. Rohanimanesh, Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence [9] L. Liao, D. Fox, H. Kautz, Extracting places and activities from GPS traces using hierarchical conditional random fields, Int. J. Robot. Res. 26 (Jan 2007) [10] S. Kumar, M. Hebert, A hierarchical field framework for unified context-based classification, in: Proceedings of the IEEE International Conference on [11] D. Yu, S. Wang, L. Deng, Sequential labeling using deep-structured conditional random fields, IEEE J. Sel. Top. Signal Process. 4 (6) (2010) 965–973.[12] T. Truyen, D. Phung, H. Bui, S. Venkatesh, Hierarchical semi-Markov conditional random fields for recursive sequential data, in: Twenty-Second Annual Conference on Neural Information Processing Systems, NIPS, Vancouver, Canada, Dec 2008, pp. 1657–1664.[13] S. Sarawagi, W.W. Cohen, Semi-Markov conditional random fields for information extraction, in: L.K. Saul, Y. Weiss, L. Botton (Eds.), Advances in Neural Information Processing Systems 17, MIT Press, Cambridge, Massachusetts, 2004, pp. 1185–1192.[14] S. Lauritzen, Graphical Models, Oxford Science Publications, 1996.[15] H.H. Bui, D.Q. Phung, S. Venkatesh, Hierarchical hidden Markov models with general state hierarchy, in: D.L. McGuinness, G. Ferguson (Eds.), Proceed-ings of the 19th National Conference on Artificial Intelligence, AAAI, San Jose, CA, Jul 2004, pp. 324–329.[16] L.R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proc. IEEE 77 (2) (1989) 257–286.[17] K. Murphy, M. Paskin, Linear Time Inference in Hierarchical HMMs, Advances in Neural Information Processing Systems (NIPS), vol. 2, MIT Press, 2002, pp. 833–840.[18] P.Q. Dinh, Probabilistic and Film Grammar Based Methods for Video Content Understanding, PhD thesis, Curtin University of Technology, 2005.[19] B. Taskar, P. Abbeel, D. Koller, Discriminative probabilistic models for relational data, in: Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence, UAI-02, Morgan Kaufmann, 2002, pp. 485–492.[20] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Francisco, CA, 1988.[21] N. Nguyen, D. Phung, S. Venkatesh, H.H. Bui, Learning and detecting activities from movement trajectories using the hierarchical hidden Markov models, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR, San Diego, CA, vol. 2, Jun 2005, pp. 955–960.[22] T. Truyen, D. Phung, H. Bui, S. Venkatesh, AdaBoost.MRF: boosted Markov random forests and application to multilevel activity recognition, in: Com-puter Vision and Pattern Recognition, New York, USA, vol. 2, June 2006, pp. 1686–1693.[23] E. Brill, Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging, Comput. Linguist. 21 (4) [24] S.V.N. Vishwanathan, N.N. Schraudolph, M.W. Schmidt, K.P. Murphy, Accelerated training of conditional random fields with stochastic gradient methods, in: Proceedings of the International Conference on Machine Learning, ICML, 2006, pp. 969–976.[25] H.H. Bui, S. Venkatesh, G. West, Policy recognition in the abstract hidden Markov model, J. Artif. Intell. Res. 17 (2002) 451–499.[26] K. Murphy, Dynamic Bayesian Networks: Representation, Inference and Learning, PhD thesis, Computer Science Division, University of California, Berke-[27] F. Pereira, Y. Schabes, Inside–outside reestimation from partially bracketed corpora, in: Proceedings of the Meeting of the Association for Computational ley, Jul 2002.Linguistics, ACL, 1992, pp. 128–135.[28] J. Chung, S. Ahn, Y. Bengio, Hierarchical multiscale recurrent neural networks, arXiv preprint, arXiv:1609.01704, 2016.[29] S. El Hihi, Y. Bengio, Hierarchical recurrent neural networks for long-term dependencies, in: Advances in Neural Information Processing Systems 8, (1995) 543–566.NIPS’95, MIT Press, Cambridge, MA, 1996, pp. 493–499.Res. 15 (2014) 981–1009.[30] N.V. Cuong, N. Ye, W.S. Lee, H.L. Chieu, Conditional random field with high-order dependencies for sequence labeling and segmentation, J. Mach. Learn. [31] X. Qian, X. Jiang, Q. Zhang, X. Huang, L. Wu, Sparse higher order conditional random fields for improved sequence labeling, in: Proceedings of the 26th Annual International Conference on Machine Learning, ACM, 2009, pp. 849–856.[32] Y. Miyao, J. Tsujii, Maximum entropy estimation for feature forests, in: Proceedings of Human Language Technology Conference, HLT, Morgan Kaufmann [33] S. Clark, J.R. Curran, Log-linear models for wide-coverage CCG parsing, in: Proceedings of the Conference on Empirical Methods in Natural Language [34] J. Yedidia, W. Freeman, Y. Weiss, Constructing free-energy approximations and generalized belief propagation algorithms, IEEE Trans. Inf. Theory 51 (7) [35] F.R. Kschischang, B.J. Frey, H.A. Loeliger, Factor graphs and the sum-product algorithm, IEEE Trans. Inf. Theory 47 (February 2001) 498–519.[36] T.T. Truyen, On Conditional Random Fields: Applications, Feature Selection, Parameter Estimation and Hierarchical Modelling, PhD thesis, Curtin Uni-[37] T. Truyen, D. Phung, S. Venkatesh, H. Bui, MCMC for hierarchical semi-Markov conditional random fields, in: NIPS’09 Workshop on Deep Learning for Speech Recognition and Related Applications, Whistler, BC, Canada, Dec 2009.Publishers Inc., 2002, pp. 292–297.Processing, EMNLP, 2003, pp. 97–104.(2005) 2282–2312.versity of Technology, 2008.