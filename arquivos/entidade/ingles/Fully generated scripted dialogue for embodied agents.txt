Artificial Intelligence 172 (2008) 1219–1244www.elsevier.com/locate/artintFully generated scripted dialogue forembodied agentsKees van Deemter a,∗, Brigitte Krenn b, Paul Piwek c, Martin Klesen d,Marc Schröder d, Stefan Baumann ea Computing Science Department, University of Aberdeen, Scotland, UKb Austrian Research Centre for Artificial Intelligence (OEFAI), University of Vienna, Austriac Centre for Research in Computing, The Open University, UKd German Research Centre for Artificial Intelligence (DFKI), Saarbruecken, Germanye IfL Phonetik, University of Cologne, GermanyReceived 13 March 2007; received in revised form 29 January 2008; accepted 14 February 2008Available online 29 February 2008AbstractThis paper presents the NECA approach to the generation of dialogues between Embodied Conversational Agents (ECAs). Thisapproach consist of the automated construction of an abstract script for an entire dialogue (cast in terms of dialogue acts), whichis incrementally enhanced by a series of modules and finally “performed” by means of text, speech and body language, by a castof ECAs. The approach makes it possible to automatically produce a large variety of highly expressive dialogues, some of whoseessential properties are under the control of a user. The paper discusses the advantages and disadvantages of NECA’s approach toFully Generated Scripted Dialogue (FGSD), and explains the main techniques used in the two demonstrators that were built. Thepaper can be read as a survey of issues and techniques in the construction of ECAs, focusing on the generation of behaviour (i.e.,focusing on information presentation) rather than on interpretation.© 2008 Published by Elsevier B.V.Keywords: Embodied conversational agents; Fully generated scripted dialogue; Multimodal interfaces; Emotion modelling; Affective reasoning;Natural language generation; Speech synthesis; Body language1. IntroductionA number of scientific disciplines have started, in the last decade or so, to join forces to build Embodied Conver-sational Agents (ECAs): software agents with a human-like synthetic voice and a computer-animated body, who canengage in a conversation in natural language. Although many techniques in this area are shared between all ECAs,this paper focuses on one particular “family” of ECAs, whose behaviour is determined by an automatically generatedscripted dialogue, rather than by autonomous agents that make their own decisions. Let us start by explaining what ascripted dialogue is.* Corresponding author.E-mail address: k.vdeemter@abdn.ac.uk (K. van Deemter).0004-3702/$ – see front matter © 2008 Published by Elsevier B.V.doi:10.1016/j.artint.2008.02.0021220K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244Scripted dialogues follow a master plan. Perhaps the most basic example of scripted dialogue is the stage dialogue,in which actors behave according to a script that was written not by themselves but by a playwright. Two actorsplaying Romeo and Juliet, for example, do what they do not because they want to, necessarily, but because someoneelse (Shakespeare, or someone adapting his work) wants them to. The communication between the actors is arguablyfake; the ‘real’ flow of information goes from the script writer to the audience. The same is true for the dialoguesbetween people in a TV commercial, where the real communication is from manufacturer to customer.This paper describes an approach to the computational production of scripted dialogues that has arisen from theNECA1 project, and which is henceforth called the NECA approach to scripted dialogue. In the NECA approach, thegeneration of dialogue behaviour is centralised: the heart of the NECA system is an automated script writing engine.This engine produces a script that can be performed by ECAs. The ECAs are comparable to actors: like their humancounterparts, they are carrying out a script that was written by someone else.ECAs appear to have entered the world of scripted dialogue in a number of systems described in [2]. Initially,scripts were mapped to words and gestures in a fairly direct manner (up to fully canned text). In this paper, however,we show how the approach can be made more powerful when combined with techniques from Natural LanguageGeneration (NLG), which is why we speak of fully generated scripted dialogue (FGSD). NLG programs are ableto express any well-formed input information in a language such as English or German, for example. NLG makesit possible to express one and the same content in many different ways. This makes it possible to create an endlessvariety of different actors, each of which acts out any role that is given to them, following a single set of rulesthat govern his or her manner of speaking and moving. This is especially important—and especially challenging—when different ECAs take on distinct ‘personalities’, and when their expressive power starts to include the expressionof emotion, as is more and more often the case. Henceforth, when we speak of ‘expressive’ dialogues, we meanmultimodal dialogues that are not only able to express factual information, but the affective state of the characters init as well.Although research on ECAs is different from work on computer games, it is instructive to compare the two en-deavours. Games programmers create characters that display sophisticated behaviours and are often able to engagein a dialogue with each other. However, the creation of such games is time consuming and involves a great deal ofhandcrafting. Even so, the amount of variation displayed by the characters tends to be limited: the number of differ-ent dialogues is typically small and these are always performed in the same way, with only minor variations. Gamescould arguably become more interesting, enjoyable and useful if the characters in them displayed more richly variedbehaviour (cf. [23] on ECAs). Taking the notion of a computer game as a point of departure, the goal of most workon ECAs can be viewed as: making it easier and cheaper to create a large variety of appealing and effective dialoguesin a controlled way. The Holy Grail of this work—which can be applied to games and more ‘serious’ applicationsalike—is to create tools that allow the (semi-)automatic construction of dialogues between believable and highly ex-pressive characters. NECA aims for that Holy Grail. It is for this reason that variation of the generated dialogues—atall levels, and involving all modalities—is such a central design constraint for NECA, which motivates many aspectsof the approach, including the choice for fully generated dialogues.Generating scripted dialogues involves a specific set of tasks, different from the ones involved in the constructionof autonomous agents. In scripted dialogue, there is no need to recognise or understand verbal input, for example.The challenge is to generate dialogues between agents who behave as if they understood each other and reactedto each other in believable ways. “Believable” implies, of course, that the content and form of the dialogues has tobe appropriate. ECA systems based on autonomous agents [13,43,72,81] interact with real people as well as withECAs. This comes naturally to them, as it were. ECA systems based on scripted dialogue, by contrast, find interactionwith people more difficult, because all possible interactions must be built into the script. However, they also havecertain advantages, particularly in terms of the alignment between modalities, and in terms of their ability to ensurethat the generated dialogues fulfil constraints on, for example, their total length, their style, and their internal coher-ence [68].This paper presents NECA’s approach to the creation of varied and expressive dialogues, with respect to all thedifferent levels and modalities, and their synchronisation. Section 2 sketches the two different applications that1 ‘NECA’ stands for Net Environment for Embodied Emotional Conversational Agents, see www.ofai.at/research/nlu/NECA/. We speak of theNECA approach or system to refer to the ideas underlying the two demonstrators developed in the project.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441221Fig. 1. eShowroom: selection of actor personality.were explored in order to test the generality of our methods. Section 3 discusses architectural issues. Section 4 de-scribes how the initial dialogue scripts are produced. Section 5 explains how these scripts are subsequently treatedby the Multimodal Natural Language Generation module. Sections 6 and 7 focus on speech and gestures respec-tively.In the course of the paper, we will explain in some detail how NECA differs from alternatives proposed in theliterature, thereby allowing the paper to be read as a review of the state of the art in the construction of ECAs, aswell as an introduction into Fully Generated Scripted Dialogue. The wide-ranging character of the paper allows someimportant issues to emerge, such as the trade-off between quality and flexibility, and the advantages of an incrementalsystem architecture. These issues are highlighted in the Conclusion (Section 8).2. Two NECA applicationsEach of the two NECA demonstrators takes an existing demonstrator as its point of departure: The eShowroomdemonstrator was inspired by work on collaborating presentation agents [1]; Socialite is an extension of the SysisNetLife platform, a community-building tool where users are represented by avatars [48]. In both cases, we havestuck with the names under which these demonstrators’ predecessors were known. Both systems, however, were verysubstantially enhanced in terms of the generality of their architecture, and in terms of the variety and quality of thedialogues produced.In the eShowroom scenario, a car sales dialogue between a seller and a buyer is simulated. The purpose of thisapplication is to entertain the site visitor and to educate him or her about cars. User interaction is restricted: users canset a few parameters which will influence the dialogue (i.e., the content of the script and how it will be ‘played’ byanimated characters). After the user has specified her/his preferences about cars (e.g., saying whether they find roadsafety particularly important), the personality of the acting characters, and the role (buyer or seller) played by a givenagent, a dialogue is generated which takes these settings into account. Fig. 1 shows the interface for selecting thecharacter’s personality. For the virtual actor Ritchie the characteristics “good humoured” and “impolite” have beenselected by the user. Fig. 2 illustrates the interface for determining the user’s preferences on the value dimensionsspecified for the product. Fig. 3 shows a typical scene from the eShowroom with the two agents (seller and buyer)located in front of a selection of cars, and a screenshot from the Socialite system.2Socialite was designed as part of a multi-user Web community (derSpittelberg.at) where the users create theirpersonal avatar, endow it with personality traits and preferences and send it to the virtual environment in order tomeet other avatars. The overall goal of an avatar is to be accepted in the community, and to reach a certain degreeof popularity. The community metaphor involves flat-sharing students who live in an area of Vienna named Spit-telberg, hence the name of the community: derSpittelberg. Socialite scenes are strongly influenced by the evolving2 The screenshot is taken from a demonstrator for an international audience, which is why the text below the animation window is an Englishtranslation of the German spoken dialogue. In the online version, the German text is displayed.1222K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244Fig. 2. eShowroom: selection of value dimensions.Fig. 3. eShowroom: typical scene.Fig. 4. Screenshot: Socialite.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441223social relations that a user is involved in. When the user is not logged on, she is represented by her avatar in theongoing (electronic) life of the community. The avatar/agent reports back every time the user logs on. Animated di-alogues simulate encounters that the user’s avatar has had with other avatars (Fig. 4). To diminish the likelihood ofproblems stemming from limited speech quality, the text of the dialogue is displayed below the animation. The frameon the left-hand side of the screen depicts the calendar functionalities including an overview of previous encoun-ters.Dialogues in eShowroom are based on a straightforward model of the world of cars and customers, with a focuson conveying information that is correct and relevant to the customer. Socialite, by contrast, had to accommodatea more colloquial conversational style, emphasising the personality and social background of the speaker. It was animportant challenge for the project to tackle both kinds of dialogues using essentially one and the same approach toScripted Dialogue. The fact that eShowroom (English) and Socialite (German) used two different languages was anadded complication.Evaluation. Several specific aspects of the NECA approach are evaluated in later sections, using whatever meth-ods seemed most suitable for the technology under discussion. Even though system-level evaluation is not the focusof this paper, it is worth summarising the main findings from a pair of field studies that were done with the twodemonstrators [33]. Beta versions of each of the two were made available to the general public for three months,accompanied by only a minimum of advertisement. In the case of Socialite, this led to 1488 logins by 66 differentusers, showing an encouraging return behaviour. Approximately half of the 66 participants visited their avatars atleast 5 times, while 20 of them did this at least 50 times. In eShowroom, where there is no user registration andeach animated presentation is self contained, we logged all those 241 presentations played during the evaluationperiod.Each user, of each of the two systems, was asked to complete a questionnaire assessing her impression of theanimated dialogues. In the most crucial questions, subjects were asked to express their agreement or disagreementon a five-point scale. As usual in questionnaires associated with field studies, only a fraction of participants com-pleted their questionnaires, resulting in 17 completed questionnaires from Socialite and 11 from eShowroom. In bothcases, a clear majority of subjects classified themselves as having considerable expertise in information technology.(As many as 64% of eShowroom users and 88% of Socialite users characterised themselves as using animated char-acters on a regular basis.) The results indicate that both demonstrators were seen as quite enjoyable. In Socialite,for example, 47% of subjects found the application enjoyable (ticking a 4 or a 5 on the agreement scale follow-ing the statement “I found the dialogue enjoyable”), 24% gave a negative opinion (a 1 or 2 on the scale), while29% were neutral (the mid-point 3 on the scale). Participants in the questionnaire judged body movements and fa-cial expressions to match the spoken words very well (Socialite: 48% positive, 40% neutral, 12% negative), but thequality of the speech was rated much lower (Socialite: 82% negative, 12% neutral, 5% positive; see Section 6 fordiscussion). In eShowroom, where this issue is of particular importance, the two characters in the dialogue werejudged as matching the parameters that the user had set for them very well (cf. Figs. 1 and 2). A puzzling findingis that female participants were much more critical of just about all aspects of both demonstrators. (Similar find-ings were reported in [21].) All these figures need to be taken with a grain of salt given paucity of respondents(both in absolute terms and as a percentage of users), whose familiarity with animated characters was, moreover,unusual.3. Architecture and representation language for scripted multimodal dialogueEach of the different modalities (text, speech and body language) that are employed in a dialogue involve ex-pressive choices, for example, concerning the words, gestures and intonation patterns that are used. All thesechoices must be properly synchronised. For example, if a particular concept is new or important, a pitch ac-cent must appear on the words that convey this concept; additionally, the mouth and eyebrows should move atthe right moment. In order to meet these challenges, NECA uses a specially designed architecture, representa-tion language, and processing model. These key aspects of the NECA approach will be introduced in this chap-ter. We start by focusing on the architecture and the processing model before discussing the representation lan-guage.1224K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244Fig. 5. Architecture realising the scripted multimodal dialogue component.3.1. An architecture for generating multimodal dialogueFig. 5 offers an overview of the NECA architecture. The Scene Generator, which uses an “Affective Reasoner”(also called Emotion Engine) to produce a Scene Description, takes the role of a playwright, planning the dialogueand generating a script. In the Scene Description, dialogue and presentation acts are specified as well as their roughtemporal coordination. The dialogue is not generated from left to right (e.g., one turn at a time), as in a conventionalinteractive system, but from the top down. The Scene Description specifies the semantic content, type, temporal order,and associated emotion of the communicative acts that the characters will perform. All this information is encoded inan XML document which is incrementally refined in the course of processing. Since the Scene Generator constructs(outlines of) dialogues, this module is specific to Scripted Dialogue. All later modules, however, use techniques thatcould equally be used to produce dialogues between autonomous agents, once each of these agents’ behaviour isspecified in the right format.The Scene Description is then handed over to the Multimodal Natural Language Generator (Section 5), whichtransforms the formal specification of the communicative acts into text. This component is also partially responsiblefor the selection of gestures. The Multimodal Output is an XML-based script specifying a set of sentences and gestureswith their temporal ordering. The task of Speech Synthesis (Section 6) is then to convey, through adequate speech,the intended meaning of the text as well as the emotion with which it is uttered.3 It also provides information onthe exact timing of utterances, syllables and phonemes, which is indispensable for the Gesture Assignment Module(Section 7). The latter module is responsible for exact timing of gestures relative to speech. Its output is a script of“animation directives”, that is, a control sequence comprising the synchronised verbal and non-verbal behaviour ofall the characters in the scene. In a last step this control sequence is converted into a data stream that is processed byan animation player. While scene generation, dialogue planning and textual surface realisation are largely applicationspecific (though important parts of their mechanisms can be reused), later components are almost entirely domainindependent.The key feature of the NECA processing model is its incrementality: each module (up to the Rendering module)adds information to the script, without ever throwing information away. This allows a module to use the informationadded at any previous stage, without compromising the pipeline. (See also Conclusion section, under “Architectureand Processing Model”.) The following section explains how this incremental process works.3 This module is also called Text/Concept-to-Speech synthesis, because its input can be text but also more abstract conceptual structures.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–124412253.2. The rich representation language (RRL)The modules of Fig. 5 presuppose a representation language that is expressive enough to represent all the in-formation that these modules produce (except the Player-Specific Rendering at the end of the pipeline), and all theinformation that they consume (except the Scene Generator, at the start). A variety of structures, usually XML com-pliant, have been designed to allow for the specification of multimodal information, but we were unable to find anythat was expressive enough to represent everything from (Discourse Representation Theory-based, see [40]) semanticinformation to words, speech and body language. To put NECA’s representation language in context, we compare itbriefly with other languages that are associated with ECAs.Mark-up languages typically define sets of mark-ups that allow a non-expert user (e.g. a web designer) to annotatea text with high-level expert information. See for instance VoiceXML (http://www.voicexml.org) for creating voiceenabled web applications, or VHML (http://www.vhml.org, Virtual Human Markup Language) for creating interactivemultimodal applications with Talking Heads or full bodied ECAs. Other examples of markup languages where text isannotated with high-level concepts are APML [22], MPML [78]. Languages of this kind were not built to representdetailed syntactic, semantic and pragmatic information.Representation languages (in our sense) are unlike mark-up languages, because they have a system-internal, ratherthan a Human-Computer Interfacing function. Existing languages of this kind have a very limited function, in theinterface between two system components [15,44,46,86]. Our own representation language had to be more general,extending and combining different aspects of existing representation languages, which is why we designed the so-called Rich Representation Language (RRL). RRL [63] combines information at all levels: the semantic level (wherethe content of the utterance is specified), but equally the textual string of words that make up an utterance, and alsoinformation about speech and body language.4NECA’s RRL5 is used for specifying a multimodal dialogue at its various stages of generation, as more and moredetail is added to the dialogue script. At the end of the pipeline, the RRL script contains sufficient information to bemapped to a chain of low-level animation directives. We start by describing the structure of the abstract script of thedialogue (i.e., the Scene Description), which contains (1) a representation of the initial common ground, (2) a repre-sentation of the participants of the dialogue, (3) a representation of the dialogue acts, and (4) a temporal ordering ofthe dialogue acts. This is the information available after Affective Scene Generation.In the following we will explain the elements of an RRL script in more detail. A full specification of the RRL XMLSchema can be found at www.ofai.at/research/nlu/NECA/RRL/index.html.1. Common Ground. The initial common ground captures the information shared by the interlocutors at the startof their conversation. It identifies the referents and specifies their properties in terms of n-ary predicates. Theinformation in the common ground is used by the MNLG module for the generation of referring expressions. Allsemantic information of the dialogue is formalised making use of Discourse Representation Theory [33].2. Participants. Each dialogue participant is provided with person data such as name and gender, appearance(= graphics design) and voice (e.g. pitch range). Each character is also equipped with information on its per-sonality and its role in the scenario. In the eShowroom scenario, for instance, the roles of the interlocutors areseller and buyer.3. Dialogue Acts. A dialogue is represented by means of individual acts which can be verbal or non-verbal. Each di-alogue act is represented as an xml element with a number of subelements including a characterisation of the act’scommunicative function (encoded in <domainSpecificAttr/>, cf. Fig. 7), its semantic content (as a Discourse Rep-resentation Structure [33]), and the prevalent emotion expressed (cf. Fig. 7, <emotionExpressed/>), as computedby the Affective Reasoner.4. Temporal Ordering of Dialogue Acts. The temporal ordering of the individual acts of a dialogue is specifiedvia a <temporalOrdering/> element. Usually, verbal dialogue acts follow a sequence of speaker contributions.Non-verbal acts such as backchannelling typically occur in parallel with dialogue acts of the speaker. Ac-4 Languages such as XSTEP [37], and ABL [55]. incorporate both declarative and procedural knowledge. They function more as programminglanguages for behaviour generation than as behaviour markup or representation.5 A full specification of the RRL XML Schema can be found on www.ofai.at/research/nlu/NECA/RRL/index.html.1226K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244cordingly <temporalOrdering/> has two subelements <seq/> and <par/> which take dialogue acts as their sub-elements.To generate text interleaved with gestural information, Multimodal Natural Language Generation (Section 5) pro-cesses the communicative function, the emotion and the semantic content, adding <sentence> and <gesture> elementsto the dialogue act. (See example below.) In eShowroom, <gesture> is a small animated clip (using 3D Charamel an-imation) that combines hand-arm gesture, posture and facial expression. In Socialite, facial expression and hand-armgesture are encoded in separate <gesture> elements, using 2D Flash animations.The information encoded in <sentence> is sent to Speech Synthesis. Synthesis produces a sound file, and an RRLscript in which <sentence> encodes the address of the sound file, the SAMPA-encoded (www.phon.ucl.ac.uk/home/sampa/home.htm) phonetic transcription of the text including syllable structure, and TOBI-encoded accentuation andprosodic boundaries [6]. See the example below.The output of the Gesture Assignment Module is an RRL specification of the animation stream, using a subsetof SMIL (Synchronised Multimedia Integration Language http://www.w3.org/TR/smil20/). All linguistic informationin <dialogueAct> is replaced by an audio element which holds the name and duration of the speech sound file. Thealignment between gestures and language-related entities (e.g. sentences, words, syllables) is made precise. The resultK. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441227is encoded in <animationSpec/> which is then input to style sheets that transform the RRL representation into aplayer-specific one.4. Affective scene generationWe aim to produce a large variety of believable dialogues. Each dialogue should match the personality of itsparticipants (as specified by the user, see Fig. 1). Moreover, the dialogues should match the interests of the user, asreflected by their choice of value dimensions (see Fig. 2), and the characters have to display the types of emotions thatfit the situation. The module that produces “skeletal” dialogues should therefore take all these factors into account.What follows is a description of the plan-based approach to affective scene generation, employed in NECA’s eS-howroom scenario.6 The approach is an extension of previous work on the generation of dialogue scenes for animatedpresentation teams [2] and on integrating models of personality and emotions into lifelike characters [3]. In NECAwe combine the dialogue act generation for the car sales domain with our mechanisms for emotion elicitation andcomputation. The result is a sequence of dialogue acts that do not only specify the semantic content of the utterancebut also the affective state of the speaker.4.1. Domain modellingDomain modelling is an essential prerequisite for automatic dialogue generation. In the eShowroom scenario thedomain model consists of two parts. The first part is a factual description of the different cars that comprises the kindof information one typically finds in a car sales brochure. In our model, each car is characterised by the followingattributes: price, horsepower, maximum speed, fuel consumption, spaciousness of interior, spaciousness of luggagecompartment, proportion of recyclable materials used in the manufacturing, and the availability of optional features(e.g., anti-lock brakes, airbags, broad tires, power windows, leather seats, and a catalytic converter). This informationis stored in a knowledge base and accessed by the dialogue planner both to inform the selection of dialogue strategiesand to specify the propositional content of the individual dialogue acts as explained in the next section.The second part of the domain model relates the attributes to the set of value dimensions that users can select toexpress their preferences: operational costs, safety, sportiness, comfort, prestige, family- and environmental friendli-6 In Socialite, emotions are not computed at runtime, but essentially a hard-wired part of the templates used by its MNLG module (cf. Section 5).1228K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244ness (see Fig. 2). The dimensions were adopted from a study of the German car market because they are particularlyrelevant for people purchasing a car. The domain model characterises an attribute in two ways. First, how relevant itis for a certain value dimension: low, medium or high. For example, the “(fuel) consumption” attribute’s relevancefor the value dimension “operational costs” is high. Second, the valence of an attribute’s value which is determinedby an evaluation function: positive or negative. For example, a consumption of 10 litters per 100 kilometres is ratednegative in the “operational costs” dimension. The same value (e.g. “230 HP”) can sometimes be rated positive in onedimension (e.g. “sportiness”) and negative in another one (e.g. “safety”).4.2. Dialogue act generationThe domain model determines to a large extent what the virtual characters can talk about, since nearly all questionsand answers in the car sales dialogues refer to the cars’ attributes. However, such a factual description does notsay anything about how this information is to be presented. This knowledge is contained in the dialogue model thatspecifies both the global and the local structure of the conversation in terms of dialogue strategies. Our sales dialoguesstart with a greeting phase, followed by the customer’s request for information about a specific car. Subsequently,a question–answer game between the customer and the sales person develops in which the features of the cars arediscussed. Finally, the customer communicates a purchase decision and, in a closing phase, the dialogue ends.In the eShowroom scenario, the dialogue planner generates the initial version of a Scripted Dialogue as a sequenceof dialogue acts. A dialogue act represents an abstract communicative function, such as requesting information (e.g.requestIf), answering a question in the affirmative, or giving feedback (e.g. agreeing). Such communicative functionscan be realised in many different ways depending, for example, on the personality of the actor. Dialogue acts usuallyfollow each other in a typical order. For example, a question about the availability of some feature might be followed bya positive or negative answer, which is then further discussed by the dialogue participants. In the dialogue model suchcombinations of dialogue acts that are frequently observed in the genre at hand are represented as dialogue strategies.Following our plan-based approach, dialogue strategies are encoded as plans that can be selected and executed bythe dialogue planner. Fig. 6 is an example of a plan for the dialogue strategy “QuestionAnswer:Boolean” introducedin the previous example. The customer requests information about a Boolean attribute, i.e. an attribute that the careither has or does not have (e.g., airbags). The dialogue planner retrieves this information from the domain model, anddepending on the attribute’s value, the sales person will confirm or disconfirm the availability. Finally, a new dialoguestrategy is triggered in which both actors discuss this new piece of information.Plans are referenced by their name. Their applicability in a given dialogue context is defined through a goal expres-sion and a precondition. Both sections can contain instantiated and uninstantiated variables (in the example denoted asstrings preceded by a dollar sign). Uninstantiated variables get their bindings when plans are selected and instantiated.The precondition specifies the initial conditions that must be fulfilled before a plan is scheduled for execution. Asshown in Fig. 6 this typically requires that some facts can be established by retrieving them from the dialogue plan-ner’s knowledge base. Goal expressions are matched against the set of goals currently pursued by the dialogue planner.To increase the variation of dialogues, multiple plans with the same goal expression (and optionally with different pre-conditions) can be specified. To inform the selection of dialogue strategies, the utility of these plans (reflecting theirgoodness of fit in a particular situation) can be specified by an integer value.The dialogue planner constantly checks which plans are applicable by matching the goal expressions of all specifiedplans against its current set of goals. Plans that match and whose preconditions are fulfilled are added to the set ofapplicable plans. The dialogue planner then chooses the plan with the highest utility value. If the choice is stillambiguous, i.e. if there are at least two applicable plans with the same utility value, one of them is randomly chosen andexecuted. By providing multiple plans with the same utility for a given situation non-determinism can be introducedin the dialogue act generation process, so that different dialogue act sequences are generated each time the dialogueplanner is invoked. During plan execution the actions in the body section of a plan are executed. The plan bodyis a procedural specification that defines how a plan’s goal can be achieved, typically by spawning some subgoalsthat will trigger new dialogue strategies. In this way, a plan tree is incrementally built by the dialogue planner inwhich the nodes represent dialogue strategies and the leaves represent the individual dialogue acts to be performedby the interlocutors. Plans may be interrupted and suspended at any time if a new plan with a higher utility becomesapplicable. This mechanism is used, for example, to adapt the dialogue generation process to the affective state of thevirtual characters as explained in the next section.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441229Fig. 6. Plan of a dialogue strategy for requesting information.Fig. 7. RRL representation of a dialogue act structure.A single dialogue contribution is encapsulated in a DialogueMove plan. The plan creates an abstract dialogueact structure containing information about the speaker, the speaker’s dominant emotion, the dialogue act type, thepropositional content if needed, and the temporal alignment with previously generated dialogue acts. Fig. 7 shows theRRL representation for such a dialogue act structure.As described in Section 2, users can assign roles and personalities to the actors, and select the value dimensionsthat interest them. These parameters are used in the precondition of the plans and influence the course and style ofthe ensuing conversation by constraining the selection of the available dialogue strategies. For example, depending ontheir mood, the two actors display different degrees of criticism or enthusiasm when discussing the car’s properties.4.3. Affect computationAffect computation in the eShowroom scenario is performed by the Affective Reasoner/Emotion Engine, based onthe cognitive model of emotions developed by Ortony, Clore, and Collins [61]. The OCC model defines emotions aspositive or negative reactions to events, actions, and objects. Events are evaluated in terms of their desirability, actionsin terms of their praiseworthiness, and objects in terms of their appeal. The subjective appraisal of the current situationis based on an agent’s goals, standards, and attitudes. The result of the appraisal process is a set of Emotion ElicitingConditions (EECs) which describe, for example, the degree to which an event is desirable and the likelihood of afuture event. The Emotion Engine maps EECs to emotion categories and their intensity. An event that is undesirablefor someone who is disliked by the agent, for example, triggers the emotion category “gloating” whereas the sameevent would have elicited “pity” if the other person was liked. The intensity of the generated emotions depends onthe EEC variables (e.g. the degree of blameworthiness) and on the personality traits specified for each agent. A decayfunction models the fact that emotions diminish over time [29]. Although sometimes criticised for its limitations as1230K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244Fig. 8. Plan of a dialogue strategy to break off the discussion.a psychological theory, the OCC model has, for the time being, established itself as a reference model for emotionsynthesis, at least for cognitively modelled embodied agents.For generating affective dialogues we combine the dialogue generation process described in the previous sectionwith our mechanism for emotion elicitation and computation. This is done via the concept of a Current Discourse State(CDS) and a set of appraisal rules. The CDS includes the previously-generated sequence of dialogue acts, the objectin focus (e.g., a particular car), and the current goals, standards, and attitudes of the agents. When a new dialogueact is generated, the appraisal rules are applied to the CDS. For example, suppose the sales person cannot answer acustomer’s question. This is appraised by the sales person as an “undesirable event” since it endangers his/her goalto come across as competent. The degree to which this is undesirable depends on how relevant this information is forthe value dimensions representing the customer’s interest. The generated EEC very undesirable is then mapped to theemotion category distress with a certain intensity. The customer can appraise the action as blameworthy if she believesthat the sales person is hiding unfavourable information. This time the EEC somewhat blameworthy is mapped to theemotion category reproach. The inferred emotions are used for updating each character’s affective state. In the end,the emotion with the highest intensity is assigned to the dialogue act representation.When the dialogue planner determines the next dialogue move, it takes the new affective states into account byevaluating the preconditions of the dialogue strategies and by selecting the one that best matches the affective statesof the characters. For instance, if the sales person repeatedly says “I don’t know”, the customer will get more andmore frustrated. If the intensity of the elicited distress emotion exceeds a certain threshold, the question-answer gameis interrupted and the closing phase is initiated. The plan for this dialogue strategy which is shown in Fig. 8 has ahigher utility value than the currently executing plan for the goal “PERFORM Discuss $car” which will therefore beinterrupted and suspended by the dialogue planner. In order to avoid an interruption in the middle of the dialogue actgeneration process (which could result in a corrupted dialogue act structure) an additional check has been included atthe end of the precondition to make sure that the last dialogue act has been finished. The first action in the body of the“BreakOffDiscussion” plan removes the suspended goal from the set of goals pursued by the dialogue planner and inthe next actions the subsequent dialogue moves of the customer and the salesperson are performed.Emotions do not only affect the sequence of dialogue acts generated by the dialogue planner, but also the way inwhich these are processed by subsequent modules. In particular, the speaker’s most dominant emotion will be usedas an additional parameter for text generation, gesture alignment, and speech synthesis. For the latter, however, theemotions generated by the Emotion Engine will be mapped to another model of emotion that is thought to be bettersuited for speech (see Section 6.2).The Emotion Engine used for affect computation in the eShowroom scenario forms the basis for Gebhard’s“A Layered Model of Affect” (ALMA, [30]). This model integrates emotions, moods and personality, covering short,medium, and long-term affect respectively. The plausibility of the generated emotions and moods was demonstrated inan empirical evaluation involving textual dialogues between two or more characters. Subjects were asked to assess theplausibility of the computer-generated emotions and moods for each character, based on these dialogues. The resultsK. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441231indicated that ALMA provides authentic believable emotions and moods [31]. Since NECA uses basically the samefunctions as ALMA for computing emotion types and intensities, these results can also be interpreted as support forthe principles behind NECA’s Emotion Engine.5. Multimodal natural language generationThe aim of the Multimodal Natural Language (MNLG) module is to express the Scene Description (see Fig. 5) innatural language and gestures appropriate to the situation. This implies, in particular, that the emotion and personalityof the speaker, as well as the factual information in the dialogue act, are taken into account. Here we sketch the designphilosophy behind the MLNG module. For details, see [64] and [67].Since the MNLG module differs from most existing NLG systems, its task and architecture are worth examiningin some detail. MNLG sits between the Scene Generator and Text/Concept to Speech Synthesis. The Scene Generatorprovides the MNLG with a specification of the content (semantic, pragmatic and emotional) and the structure of adialogue. The MNLG maps this specification to a representation of the verbal and non-verbal behaviours that con-stitute a fully-fledged dialogue. The result, a multimodal output representation, describes the combination of words,grammatical constructions and gestures that make up the dialogue. (Phonetic and prosodic realisation, and detailedtiming, are left to subsequent modules.) The output of the MNLG is not intended for human consumption; instead, itconsists of a machine-readable description of a dialogue which a team of animated agents is expected to act out.Before delving into the details of the MNLG, let us briefly highlight in what respects it differs from other naturallanguage generators.The generator described in [60] resembles the MNLG’s approach to semantics. Both generators can operate onunordered sets of statements, rather than the highly structured inputs that are required for many off-the-shelf surfacerealisers such as fuf/surge [27]. The generator in [60] is, however, unimodal and unable to cope with pragmaticconstraints, for example regarding the personality and emotions of the speaker.The MNLG’s functionality resembles that of the microplanner of an NLG system [71]. Most microplanners, how-ever, have been designed for sentence generation rather than multimodal dialogue act generation [11,56]. Some, likethe SPUD generator ([79]) can be adapted to multimodal generation [15]. But, like most systems specifically de-signed for ECAs (e.g., [41,53,70]), SPUD uses an algorithm based on integrated planning, whereas we advocate ahighly modular system (see Fig. 9), in order to support fast generation. Integrated approaches have been motivated bypsycholinguistic plausibility [42]. We make no psycholinguistic claims for the approach advocated here, but wouldlike to point out that some of the most widely cited psycholinguistic models of speaking are modular, and essentiallypipelined [52].5.1. Requirements for the MNLGNECA’s MNLG module was built with the following requirements in mind:(1) Integration of heterogeneous generation resources. One of the main determinants of a dialogue act is its semanticcontent. The semantic content that the Scene Generator can provide for a dialogue act depends on the content ofthe underlying database. For some dialogue acts, such as greetings (‘Hello, my name is Ritchie’), it seems impos-sible to generate from first principles, starting with the semantic content. MNLG therefore needs the capability tocombine full generation with templates created by human writers.(2) Integration of different factors (emotion, personality, etc.). The realisation of a dialogue script depends on morethan just semantic content. To obtain believable presentations, factors such as the personality of the speaker, theirgender and their emotional state should play a role. Therefore, we require that the MNLG take a variety of suchfactors into account when choosing how to put a given message into words.(3) Variation of expression. People say different things on different occasions, even if the circumstances regardingthe aforementioned factors are more or less identical. This means that the MNLG needs to be capable of non-deterministic generation.(4) Performance. Long delays would decrease the appreciation of end-users. For this reason, the MNLG should beable to produce output almost instantaneously.1232K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244Fig. 9. Schematic representation of MNLG architecture.(5) Re-use. The MNLG is intended to be application independent. It should be easy to port to new applications, thussaving the developers of such applications time and effort.The next section will explain how these requirements were met.5.2. Outline of the MNLG moduleLike the overall NECA system, the MNLG module has a pipelined architecture (Fig. 9). The module dialogue-ActGen generates individual dialogue acts. It, in turn, calls referringExpressionGen for the referring expressions thatneed to be incorporated into the realisation of a dialogue act.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441233Requirement 3, regarding variation in the output, is addressed by having a number of non-deterministic steps in thegeneration process: deep structure generation, for dialogue acts and the referring expressions they contain, consist ofover-generation followed by (random) selection. Gesture selection also operates through random selection of a gesturefrom a set of appropriate alternatives.In order to facilitate maintenance and re-use, the MNLG is divided into (Sicstus Prolog) modules (Requirement 5).Application-specific data are separated from generic generation algorithms so that development of new applicationsonly requires modification of the data files. The highly modular setup in combination with a pipeline architecture alsocontributes to the high performance (in terms of generation times) of the system; see the next section on evaluation(Requirement 4).One of the main tasks of the MNLG is the generation of “deep structures” for dialogue acts (i.e., pairings of contentwith verbal and non-verbal realisations) which satisfy a given set of syntactic, semantic and pragmatic constraints.These constraints constitute the input to the MNLG and are dictated by the Scene Generator. Formally, the collectionof input constraints is represented by a typed feature structure. The typing of the structures facilitates reuse andmaintenance of the system (since an explicit representation of the data structures is kept separately). The structuresare manipulated using Prolog’s fast built-in unification algorithm (through the Profit library in [28]). The linguisticresources are represented as trees whose nodes are also typed feature structures. Together, these trees make up theMNLG’s tree repository.Generation consists of matching the input feature structure with the root nodes of the trees in the repository.Matching trees may have incomplete daughter nodes (i.e., daughters that are not yet fully realised). These are recur-sively expanded by being matched with the trees in the repository, until all daughters are complete. Daughter nodeswhose semantics give rise to referring expressions are dealt with by the referringExpressionsGen module (see Fig. 9and [67]).The formalism for the trees in the repository is able to represent linguistic resources of a wide variety, includinglexical entries, spans of canned text, templates and full-fledged grammar rules. For a given input, the resources oftenallow the construction of multiple deep structure trees, one of which is selected at random. Emotion and personality arestored in the attribute currentAct, and they influence selection of words, phrases and gestures from sets of alternativesthat express the same semantic content (Requirement 2). Fig. 10 shows an example of a tree representing a template.The usual angled-brackets notation for feature structures is used; types are in italics and attributes in small capitals.Sharing of values is represented by co-indexing.When this template is called, the value of “Speaker” is unified with the name of the speaker of the current dialogueact, which ends up in the realisation of the template. Note that the template provides a full syntactic structure for anysentence generated with it, blurring the distinction between real and template-based generation in line with currentthinking in Natural Language Generation [83]. Note that the meaning of the sentence is not ‘computed’ composition-Fig. 10. Template for “I am NP”.1234K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244Fig. 11. Socialite templates for “Mir geht’s” (“I feel...”) and “beschissen” (“all fucked up”).ally from the meaning of its parts. Grammar rules with a compositional semantics are only useful where the input tothe generator consists of complex semantic representations in the first place. For example, in eShowroom this holds forthe description of cars; for these, the underlying database allows us to derive complex semantic input representations.The example in Fig. 11 derives from the Socialite application, as one might guess from its colourful use of language.It provides an example of a linguistically fully specified template (for adjp). This template combines (as indicated bythe dotted line) with a sentence template whose semantics is radically underspecified (value is “none”) and which hasa linguistically underspecified constituent (the node labelled “fragment”).5.3. Evaluation of the MNLG moduleThe MNLG goes a long way towards meeting the requirements introduced earlier in this section. We have seenthat requirements (1) to (4) have been addressed through specific design decisions. Requirement (5), involving systemperformance, was evaluated by running tests to measure average generation times on a range of examples [64]. Thesetests provided satisfactory results, with generation taking between 1/100 and 4/100 of a second per dialogue act(based on a tree repository consisting of 138 generation trees, and using a Pentium III 1200 MHz processor). Require-ment (6), on re-use of the MNLG, was evaluated by porting the MNLG: it was re-used in the Socialite demonstrator,then once again, outside the NECA project, in the epoch iGuide Virtual Tour Guide system [26].Most Socialite generation templates (e.g., Fig. 11) were originally written by professional script writers, in a formatdifferent from that used by the MNLG. The tree formalism proved to be flexible enough to accommodate these pre-existing templates: Perl scripts were written for automatically transforming these into MNLG trees, resulting in atree base of 1170 trees. The experience of implementing the epoch iGuide Virtual Tour Guide system’s generationcomponent using the MNLG was similarly encouraging.We investigated the effect of different MNLG settings, focusing on NECA’s eShowroom demonstrator, comparingdialogues with and without gestures [65]. Neither subjective user experience (as measured through a questionnaire)nor scores on a retention test differed significantly between the two conditions (between subjects; N = 28). However,users in the with-gestures condition complained significantly more about the quality of the speech, perhaps becausethe gestures detracted from the on-screen speech bubbles that accompanied speech. Furthermore, to evaluate an exten-sion of eShowroom with backchannelling gestures by the hearer, we compared dialogues with and without hearers’gestures, keeping speakers’ gestures constant (between subjects; N = 12; see [9] and [67]). We found that subjects inthe with-hearer gestures condition did significantly worse on the retention test, possibly because the hearer gestureswere too intrusive. This would be consistent with [87], where the presence of a highly expressive talking head was ar-gued to diminish task performance in some cases, because it can distract attention. A possible alternative explanationfor our findings is that the rendering of the feedback gestures may not have been good enough. For the purpose of thisparticular study, we used the Microsoft Agents technology, which does not always render simultaneous gestures bymultiple agents adequately: gestures can be a bit abrupt, for example.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441235Finally, in a study with N = 40 (see [67]), we found a small effect as a result of varying the algorithm for the gener-ation of referring expressions. A more “ego-centric” algorithm (an agent ignoring the contributions of his interlocutor)caused the agent to appear less friendly.6. SpeechThe generation of speech is performed using the text-to-speech (TTS) system MARY [77]. While existing TTStechnology is of sufficient quality to be intelligible, there is much room for improvement, particularly if personality andemotion are to be taken into account. NECA makes two contributions to this long-term goal: linguistically appropriateprosody in a dialogue, and emotional expressivity.6.1. Prosody reflecting information structureThe term “prosody” covers the supra-segmental aspects of a speech utterance: pitch, duration, and loudness.Prosody can not only convey information about the affective state of the speaker, but also about the linguistic structureof the utterance, for example by accenting new or important words, and by inserting pauses. Despite work by, for ex-ample, [35,57,69], and [36], existing TTS technology usually does not take such effects into account. Systems basedon NLG, however, are well placed to do better. This is particularly true for NECA, whose incremental processingmodel (Section 3) guarantees that semantic, syntactic and pragmatic information is available to the Speech Synthesismodule. This makes it easy, for example, to look up whether a given object represents “given” or “new” information,without having to parse and interpret text.Information structure is realised by an interplay of various linguistic means or strategies. These means are eithersyntactic (e.g. word order and specific constructions like clefts, passives and parallelism), morpho-syntactic (e.g. spe-cific particles), or prosodic (e.g. (de)accentuation and intonational phrasing) in nature and are employed by differentlanguages to different degrees (e.g. [85]). In English, intonation is the predominant linguistic marker of informationstructure, which also holds for German, although word order plays a more important role here.NECA’s treatment of prosody is based heavily on the RRL and our incremental processing model while, empiri-cally, it is informed by extensive perception tests. Here we summarise some of our main results for German [5,7,8].Broadly speaking, the results confirmed the familiar idea that new information should carry an accent while textuallygiven information is de-accented (e.g. [84]). We also found, however, that when the type of accent is taken into ac-count, it is necessary to distinguish more finely than is usually done, by taking a third type of information into account,which is sometimes called “accessible” [18,50]. Such information is neither totally new nor totally given but inferablefrom the situational or textual context. For textually inferable referents, we found that the nature of the semantic rela-tion with the antecedent determines whether an item should be accented, and which type of accent it should carry. Forexample, synonyms (elevator–lift) and the anaphors in part-whole relations (page–book) tend to behave similarly togiven information and are usually de-accented, while e.g. the anaphor in a whole-part relation (i.e. the reverse orderof the inclusion relation, e.g. book–page) is more similar to new information and should be accented.The type of accent on the subordinate expression is different from an accent marking new information, however. Itcould be shown that an early peak accent (transcribed as H+L* in terms of the often-used (G)ToBI categorisation; see[32]) is most appropriate for marking this type of accessibility, whereas a medial peak accent (symbolised as H*) isbest for marking new information.Semantic-pragmatic properties of a referring expression including its degree and type of givenness are providedby the NLG component. This information is used to assign the tags “+given” and “+accessible” (if applicable) to therespective items in the RRL script. Furthermore, a contrastive usage of a referring expression can be explicitly flagged.These markers of a referring expression’s information status are communicated to the MARY prosody module,where they affect accent placement and form: Tokens marked “+given” are ignored during accent assignment, i.e.they are de-accented, whereas tokens marked as “+accessible” are assigned an H+L* accent, and “+contrast” to-kens receive a rising accent (L+H*) with a particularly high pitch range. The default nuclear accent type assignedto new adjectives and nouns is H*. These rules enable the system to generate contextually appropriate intonationpatterns.1236K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12446.2. Emotionally expressive speechWe have argued that it is often crucial for the dialogues produced in Scripted Dialogue to be expressive in termsof the emotional state of the speaker. Two types of generating emotionally expressive speech can be distinguished:“playback” and “model” approaches. The first approach (e.g., [10,38]) treats emotions holistically by creating speechsynthesis voices from recordings spoken in certain expressive styles (e.g., angry voice, friendly voice). While thisapproach is likely to lead to highly natural emotion expression, it suffers from a lack of flexibility: Only the emotionalstates which have been recorded can be “played back”. Clearly, NECA’s goal of creating dialogues that are highlyvaried makes flexibility a key issue: the alternative would be to record and store prohibitively large amounts of speech.The second approach (e.g. [58]) models emotions in terms of the acoustic synthesis parameters corresponding tovarious emotions. This approach requires a high degree of control over acoustic parameters. Rule-based formant syn-thesis enables the modelling of a wide array of acoustic parameters, which is why it has been the technology of choicefor a number of emotional speech synthesis undertakings; however, due to lack of naturalness, it has nearly disap-peared from the landscape of commercial speech synthesis systems. Promising new approaches, such as data-drivenformant synthesis [13], are still in an early development phase. Unit selection yields the highest degree of naturalnessin speech for one speaking style (usually: neutral), but does not provide a fine-grained control over the prosodic pa-rameters. Indeed, unit selection synthesis draws its naturalness from not interfering with the recorded speech signal,and thus rarely allows for an explicit modelling of prosody. This limitation currently makes unit selection synthesisunsuitable for model-based approaches to emotional speech synthesis.A compromise between degree of flexibility/control and natural-sounding synthesis is diphone synthesis, whichallows fine-grained prosody modelling with a limited degree of distortion. It is based on the concatenation of smallrecordings of human speech, so-called “diphones” (ranging from the middle of one phone segment to the middle ofthe following phone segment), followed by a signal processing step to generate the desired prosody. Unfortunately,the voice quality inherent in the diphones appears to be inappropriate for certain emotions [58].The current work pursues a model-based approach to synthesis, i.e. it is based on an explicit model of the vocalcorrelates of emotions, realised using a diphone synthesis enhanced with a limited control over voice quality [78]. Westart with the decision on how to represent the emotional states themselves [19]. Consistent with the state of the art inspeech research, we have chosen to use emotion dimensions [20,74], a continuous framework for the representationof essential properties of emotional states. The two emotion dimensions which have emerged as most important froma large number of studies are evaluation (sometimes called valence or pleasure) and activation (sometimes calledarousal). These two are sometimes complemented with a third dimension, called power or dominance.The main task in building the model is to find a mapping from a point in the emotion dimension space to thecorresponding acoustic correlates. We constructed such a mapping based on a database analysis and a literature sur-vey [75,78]. We used the Belfast Naturalistic Emotion Database, which contains recordings of 124 English speakersexhibiting relatively spontaneous emotion [24]. This database is one of the largest collections of natural emotionalspeech available, and it is labelled according to emotion dimensions. The emotion dimension coordinates of each clipin the database were correlated with a number of acoustic measures that were semi-automatically extracted from thedatabase. Robust correlations were found, especially for the activation dimension, but also—if to a lesser extent—forthe evaluation and power dimensions. These correlations were accompanied by quantified linear prediction coeffi-cients, allowing a relatively simple deduction of rules for synthesis.As a second source of information, a literature survey was conducted. The assorted evidence found in a dozenpublications was brought together, most of which studied English speech (see [66] for details). While these articlesonly gave qualitative trends on correlations between emotion dimensions and acoustic parameters, they provided asolid baseline for what can be expected to be conveyed through acoustic voice parameters. Essentially, strong trendswere found only for the activation dimension.All this evidence was consolidated into a model that predicts prosodic and voice quality changes for each point inemotion dimension space. The evidence confirmed that the emotion dimension best conveyed in speech is activation(or “arousal”), i.e. the degree of excitation vs. passivity. According to our model, increased activation is conveyed inthe voice through prosodic effects, such as increased pitch and speaking rate, as well as the voice quality, particularlyan increased vocal effort caused by higher muscle tension.The importance of voice quality modelling for expressing emotions in synthetic speech is still a matter for de-bate [78]. In essence, the frequent presence of voice quality effects in human expressions of emotion make it desirableK. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441237Fig. 12. Effort ratings for the male diphone voice by Ger-man listeners (from [76]).Fig. 13. Effort ratings for female diphone voice and interpolatedversions, by German, French and Turkish listeners (from [82]).to model voice quality in synthetic speech. Since there are no instruments yet for modelling voice quality in diphonesynthesis (in spite of promising developments, see [43,80]), we recorded separate diphone databases for three lev-els of vocal effort, for one male and one female speaker. Both voices are publicly available for non-commercial use(http://tcts.fpms.ac.be/synthesis/mbrola.html), as the MBROLA [25] diphone databases de6 (male) and de7 (female).We tested the perceptual adequacy of our male voice in two perception tests [76]. A first test was carried out to testthe hypothesis that the three diphone sets are sufficiently similar to be recognised as belonging to the same person. Weprepared pairs of sentences, where the first and the second sentence were synthesised either using the same voice or adifferent one, at the same or a different pitch levels. Subjects were asked whether the stimuli in each pair were producedby the same speaker. Results showed that the effect of vocal effort on perceived speaker identity was relatively small– 79.9% of the sentences differing in vocal effort were perceived as the same speaker. However, there was a strongeffect of pitch level. A modification of pitch slightly beyond the range typically used in non-emotional synthesis(but still moderate in view of emotional speech) caused speaker identity ratings to drop to around or below chancelevel. Next, we tested the hypothesis that the effort intended during recording is perceived in the synthesised material.Stimuli differing in intended vocal effort and in overall pitch level were played to subjects, who rated the stimuli ona continuous scale from “without effort” to “with great effort”. Since the stimuli were amplitude-normalised, subjectswere instructed to base their ratings on the “sound of the voice” rather than the “loudness”. Results confirmed that theeffort was perceived as intended (Fig. 12).While being able to select one of three levels of vocal effort is a step forward, this is clearly a very limited amountof control. A further step towards more flexibility can be afforded by the use of voice interpolation. From the originalrecordings of the three female voice databases, we created new databases with intermediate levels of vocal effort usinga simple spectral interpolation algorithm [82]. A listening test was performed to evaluate the intended vocal effort inthe original female databases and the interpolated ones. The results show that the interpolation algorithm can createthe intended intermediate levels of vocal effort given by the original databases. This effect was largely independent ofthe language background of the subjects (Fig. 13).7. Generating dialogue-accompanying gesturesWe have arrived at the last step, where facial expressions, hand-arm gestures and postures are chosen and alignedwith speech. Since we were able to build on established techniques and procedures in this area, the description of thispart of the NECA approach will be comparatively brief. Dialogue-accompanying gestures such as facial expression,hand-arm gestures and body postures are typically generated in two phases: a planning phase and a realisation phase(see [46] and their proposal of the SAIBA framework). We discuss these two phases in turn.1238K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12447.1. Multimodal behaviour planningDuring Multimodal Natural Language Generation (MNLG), gestures are planned on the basis of the semantic andpragmatic content of the utterances and are aligned to the respective nodes in the MNLG tree. (See the example of<dialogueAct> in Section 3.2.) The actual point in time for the start of the gesture is still unknown at this stage ofprocessing. When the Gesture Assignment module starts, information on body behaviour is underspecified. This isfirstly because only information on relative alignment of verbal and non-verbal behaviour is available (see ALIGNTOand ALIGNTYPE features in the example of <dialogueAct> after MNLG in Section 3.2), and secondly because thechoice of gestures is only restricted by the features IDENTIFIER, and MODALITY (i.e. in our example all gesturesinvolving body and hips are suitable).The idea of intertwining gestural and syntactic structure has also been proposed in [16]. They describe a mecha-nism for applying the SPUD natural language generator [79] with its Lexicalized Tree Adjoining Grammar (LTAG)formalism [39,73] to multimodal generation. Integration of gestures and syntax is particularly suitable for gesturesthat can express semantic content and therefore present an alternative to linguistic expression of the same content. TheMNLG also allows for a second type of gesture generation, which is less tightly integrated with syntax. This secondtype of gesture generation concerns gestures expressing discourse function (e.g., question or assertion). Such gesturesare not part of the grammar, but are added by a separate gesture generation module which associates gestures andbody postures with particular types of dialogue acts.7.2. Temporal fine-tuning of behavioursAt a later planning stage, during Gesture Assignment, the relative alignment of utterances and gestures resultingfrom the behaviour planning stage (MNLG) is transformed into an absolute alignment according to the time constraintsimposed by speech synthesis. This approach is typical for ECA systems [44]. More generally, accessibility of prosodicand temporal information produced by the speech synthesis is crucial for a fine-grained alignment of the verbal andnon-verbal communication systems.In NECA, the speech synthesis component provides a sound file of the utterance together with an RRL file contain-ing the phonetic transcription of the utterance.7 (See Section 3.2, “Specification of <sentence> after Speech Synthe-sis”.) This information together with the constraints coming from MNLG, and the meta-level description of availablegestures in the Gesticon (see the next subsection), is then used by the Gesture Assignment module for producingplayer-independent multimodal animation directives. The animation directives are encoded in the animationSpec-element of the RRL which is then transformed into player-specific formats. For an example of an animationSpec see,once again, Section 3.2.7.3. Gesture representation—The gesticonInformation on gestures is stored in a RRL-compliant repository of behaviour descriptions which we call Gesticon.Analogous to a Lexicon in natural language, a Gesticon is a central behaviour repository relating form with meaningand function, and moreover connecting the abstract information to concrete player-specific animations.When defining the Gesticon for the NECA applications eShowroom and Socialite, we started out from descriptionscomprising some minimal information on the meaning or function of a gesture (e.g. deictic, or greeting) or facialexpression (happy, sad, disgusted, etc.), and a high-level description of form features, such as which body parts areinvolved and the relative duration of gestures and gesture phases [49]. Duration information specifies the extent towhich a gesture can be elongated or shrunk without changing its meaning. For hand-arm gestures the relative wristposition at the beginning and the end of the gesture is also stored [47]. This information is used to estimate the timerequired to move from the end of one gesture to the beginning of the following gesture.The need for representations of body behaviours that are independent of animation and player technology has arisenfrom the wish to develop planning components that are independent of individual animation and player technologies.7 For an overview on TOBI see http://www.ling.ohio-state.edu/~tobi/.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441239The Gesticon functions as a central behaviour repository relating form with meaning and function, and connecting theabstract information to concrete player-specific animations.The eShowroom animation library consists of 160 animation videos (in Charamel’s CharActor format) which definesmall sequences of overall body behaviour including hand-arm gestures for the male and the female character. Thebehaviours are built from basic graphical building blocks such as face shapes, eye and mouth shapes, hand shapes,upper arms, lower arms. For the facial display of emotions such as anger and fear, animation directives are formulatedin terms of degree of eyebrow and lip corner raise, lip stretch, and so on. In Socialite, character animation is restrictedto facial animation and hand-arm gestures. Its animation library is a collection of Flash-encoded hand-arm gestures (53base gestures) and snapshots of facial expressions (19 for the male and the female character each). Facial expressionsin Socialite are based on Ekman’s six basic emotions of (happiness, sadness, anger, fear, disgust, surprise) plus a fewfagin-style additional labels like ‘false laugh’, and ‘reproach’.The approach to animation pursued in NECA is comparable to the majority of current work on ECAs where be-haviours are realised by selecting from a set of prefabricated animations, see for instance the REA system [15], theNICE project [5], FearNot [30]. These differ from approaches where behaviours are generated; see for instance [62]for generating facial expressions from speech, Tepper et al., 2004 for generating direction-giving gestures from seman-tic representations, or [45] for driving a virtual character by means of form descriptions derived from motion capture.8. ConclusionThe NECA approach to Fully generated Scripted Dialogue (FGSD), as embodied in the eShowroom and Socialitedemonstrators developed in the NECA project build on such predecessors as those described in [14], but it represents1240K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244a significant step forward in the construction of systems involving ECAs that are able to engage in a large variety ofhighly expressive dialogues. In summarising its highlights, it will be useful to distinguish between three issues: (1)the overall paradigm of Scripted Dialogue, (2) the architecture that is used in NECA to produce scripted dialogue, and(3) the individual components of the NECA system.1. The paradigm of scripted dialogue. ECAs are widely thought to have a potentially beneficial effect on themotivation and task performance of the user of a computer application. Lester et al., for example, show that “[. . . ]the presence of a lifelike character in an interactive learning environment—even one that is not expressive—can havea strong positive effect on student’s perception of their learning experience”, calling this the Persona Effect ([51],also [23]). We have argued that Fully Generated Scripted Dialogue (FGSD) is a promising framework in which topurpose these potential benefits. We believe there to be a wealth of applications, ranging from “edutainment” (e.g.,VirtualConstructor, [59]) to advertising and e-drama (witness Carmen’s Bright IDEAS [47], FearNot! [4,34] andFaçade [55]), where it can be useful to generate a dialogue as a whole. Similarly, FGSD could be used to increasethe variety of dialogues produced by story generation systems (e.g. [12]), particularly those that are multimodal [17,55]. Computer-generated animations have become part of mainstream cinematography, as witnessed by films suchas Finding Nemo, Monsters Inc., and Polar Express; but automated creation of film content, and more specifically,dialogue content, lags behind the possibilities currently explored for graphics. We hope that the FGSD paradigmadvocated in this paper will contribute towards closing this gap.The fact that NECA’s dialogues are fully generated makes it possible to generate a huge variety of dialogues whosewording, speech and body language are in accordance with the interests, personalities and affective states of the agents.The degree of control can be further enhanced if a revision strategy is applied, which takes the output of the SceneGenerator as a first approximation that can be optimised through later operations [66,68]. Consider the eShowroomscenario, for instance. If two or more yes/no questions about a car are similar in structure while also eliciting the sameresponse, then these question/answer pairs can be merged into one aggregated question-answer pair (‘Does this carhave power windows and leather seats? Sure, it has both!’).2. Architecture and processing model. Scripted dialogues can be generated in many different ways. A distinctivefeature of the NECA system is the fact that it is based on a processing model that starts from a scene generated by theScene Generator, which is then incrementally “decorated” with more and more information, of a linguistic, phonetic,and graphical nature. The backbone of this incrementally-enhanced representation is NECA’s Rich RepresentationLanguage (RRL), which is based on XML. Perhaps the best defence for this incremental processing model lies in theexperimental and multidisciplinary nature of all work on ECAs. Partly because this is still a young research area, it isdifficult to predict which aspects of a given level of representation might be needed by later modules. This difficultyis exacerbated by the fact that researchers/programmers may only have a limited understanding of what goes on inlater modules. By keeping the generation process incremental (i.e., monotonically increasing), we guarantee that allinformation produced by a given module will be available to all later modules.Consider, for example, the information status of referents in the domain. It may not be obvious to someone workingon MNLG that the novelty or givenness (i.e., roughly, the absence or presence in the Common Ground) of an objectis of any importance to later modules; but it is of importance since, for example, this information is used by SpeechSynthesis when deciding whether to put a particular kind of pitch accent on the Noun Phrase referring to this object(Section 6.1). Our incremental processing model ensures that this information is in fact available. Undeniably, thisprocessing model can lead to XML structures that are large. As a remedy we have implemented a streaming modelwhere after Scene Generation the individual communication acts are processed in parallel. As soon as the playergenerator has finished processing an act, the result is “streamed” to the user immediately, while subsequent acts arestill being processed. This leads to a drastic reduction of response times and thus ensures real-time behaviour of thesystem.3. Individual system components. When different scientific disciplines join forces to construct an ECA-based sys-tem, it can be interesting to compare their respective contributions. Comparisons could be made across modalities, forexample, asking how basic concepts such as information structure (e.g., focus) are expressed in the different modali-ties (i.e., text, speech, and body language). Another interesting question is why emotions are modelled differently inAffective Reasoning (which uses the OCC model of [61]) and in Speech (where Schlosberg’s emotion dimensionsare thought to be more appropriate), and in facial expressions (where Ekman’s six basic emotions hold sway). Forreasons of space, we shall focus on one comparison that is particularly important given NECA’s emphasis on genericK. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441241technologies that hold promise for the longer term, namely the trade-off between quality and flexibility which hasfeatured strongly in our discussions of both Natural Language Generation and Speech Synthesis.The issues regarding quality and flexibility might be likened to a problem in the construction of real estate. Supposean architect wants to restore an old stone building in grand style. Ideally, she might want to harvest some natural stonein all the colours and shapes that the restoration work requires. But it can be difficult to find exactly the right piece, inwhich case she can either make do with a natural piece that is not exactly right, or she might have a piece of artificial(i.e., reconstituted) stone custom made.The trade-offs facing language generation, speech synthesis and gesture assignment are similar. In the case of Nat-ural Language Generation, NECA has used a combination of canned text (cf. natural stone) with fully compositionalgeneration (cf. artificial stone); in the case of speech synthesis, NECA has used a combination of diphone synthesis(comparable with grinding natural stone to a pulp which is then moulded in the desired shape) with limited control overvoice quality. In order to create suitable animations, NECA has employed libraries of player-specific, prefabricatedanimations (cf. giving architects a choice of different rooms, facades, etc.) together with meta-information concerningdimensions of scalability; this approach to graphics is comparable to parameterised unit selection in Speech Synthesis,or to the highly flexible kind of template-based Natural Language Generation advocated in [83].Closing remarks. The word ‘dialogue’ can be taken to imply interaction between a computer agent and a person. Inthis paper, we have examined an alternative perspective on dialogue, as a way to let Embodied Conversational Agentspresent information (e.g., about cars in the eShowroom system) or to tell a story (e.g. about students in the Socialitesystem). NECA’s version of Scripted Dialogue happens not to allow very sophisticated interactions with the user.(The interface of Fig. 1, Section 2, for example, only allows the user to choose between four different personalitiesand 256 different combinations of value dimensions, using a simple menu.) We believe there to be ample space forother, similarly direct applications of the fully-generated scripted dialogue (FGSD) technology, for example becausethere will always be a place for non-interactive radio, film and television. Perhaps most importantly, however, wesee a substantial future role for hybrid systems that combine FGSD with much extended facilities for letting the userinfluence the behaviour of the system (as exist in interactive drama, for example, see [4,34,54,55]).8References[1] E. André, T. Rist, Presenting through performing: On the use of life-like characters in knowledge-based presentation systems, in: ProceedingsIUI ’2000: International Conference on Intelligent User Interfaces, 2000.[2] E. André, T. Rist, S. van Mulken, M. Klesen, S. Baldes, The automated design of believable dialogues for animated presentation teams, in:J. Cassell, J. Sullivan, S. Prevost, E. Churchill (Eds.), Embodied Conversational Agents, MIT Press, Cambridge, 2000.[3] E. André, M. Klesen, P. Gebhard, S. Allen, T. Rist, Integrating models of personality and emotions into lifelike characters, in: A. Paiva (Ed.),Affective Interactions: Towards a New Generation of Computer Interfaces, in: Lecture Notes in Computer Science, vol. 1814, Springer, Berlin,2000.[4] R.S. Aylett, R. Figuieredo, S. Louchart, J. Dias, A. Paiva, Making it up as you go along—improvising stories for pedagogical purposes, in:J. Gratch, M. Young, R. Aylett, D. Ballin, P. Olivier (Eds.), 6th International Conference, IVA 2006, in: LNAI, vol. 4133, Springer, Berlin,2006, pp. 307–315.[5] S. Baumann, M. Grice, The intonation of accessibility, Journal of Pragmatics 38 (10) (2006) 1636–1657.[6] S. Baumann, M. Grice, S. Steindamm, Prosodic marking of focus domains—categorical or gradient? in: Proceedings SpeechProsody 2006,Dresden, Germany, 2006, pp. 301–304.[7] S. Baumann, M. Grice, Accenting accessible information, in: Proceedings Speech Prosody 2004, Nara, Japan, 2004, pp. 21–24.[8] S. Baumann, K. Hadelich, On the perception of intonationally marked givenness after auditory and visual priming, in: Proceedings AAIWorkshop “Prosodic Interfaces”, Nantes, France, 2003, pp. 21–26.[9] M. Bergenstråhle, Feedback gesture generation for embodied conversational agents, Technical Report ITRI-03-22, ITRI, University ofBrighton, UK, 2003.[10] M. Bulut, S.S. Narayanan, A.K. Syrdal, Expressive speech synthesis using a concatenative synthesiser, in: Proceedings of the 7th InternationalConference on Spoken Language Processing, Denver, Colorado, USA, 2002.[11] S. Busemann, H. Horacek, A flexible shallow approach to text generation, in: Proceedings 9th International Workshop on Natural LanguageGeneration, Canada, 1998, pp. 238–247.[12] Ch.B. Callaway, J.C. Lester, Narrative prose generation, Artificial Intelligence 139 (2) (2002) 213–252.[13] R. Carlson, T. Sigvardson, A. Sjölander, Data-driven formant synthesis, Progress Report No. 44, KTH Stockholm, Sweden, 2002.[14] J. Cassell, J. Sullivan, S. Prevost, E. Churchill (Eds.), Embodied Conversational Agents, MIT Press, Cambridge, MA, 2000.8 Carmen’s Bright IDEAS and FearNot! apply interactive drama to education: IDEAS is designed to help mothers of young cancer patients;FearNot! trains school children to cope with bullying. Façade is an interactive game in which the user influences the outcome of the game.1242K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244[15] J. Cassell, M. Stone, H. Yan, Coordination and context-dependence in the generation of embodied conversation, in: Proceedings First Interna-tional Natural Language Generation Conference (INLG’2000), Mitzpe Ramon, Israel, 2000, pp. 12–16.[16] J. Cassell, H. Vilhjálmsson, T. Bickmore, BEAT: The behavior expression animation toolkit, in: Proceedings ACM SIGGRAPH 2001, LosAngeles, USA, 2001, pp. 477–486.[17] M. Cavazza, M. Charles, Dialogue generation in character-based interactive storytelling, in: Proceedings AIIDE, 2005.[18] W. Chafe, Discourse, Consciousness, and Time, University of Chicago Press, Chicago/London, 1994.[19] R. Cowie, R.R. Cornelius, Describing the emotional states that are expressed in speech, Speech Communication 40 (1–2) (2003) 5–32 (SpecialIssue on Speech and Emotion).[20] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias, W. Fellenz, J. Taylor, Emotion recognition in human–computer interaction,IEEE Signal Processing Magazine 18 (1) (2001) 32–80.[21] A. De Angeli, N. Bianchi-Berthouze (Eds.), Proceedings AVI 2006 Workshop on Gender and Interaction: Real and Virtual Women in a MaleWorld, Venice, Italy, 2006.[22] B. De Carolis, C. Pelachaud, I. Poggi, M. Steedman, APML, a mark-up language for believable behavior generation, in: H. Prendinger (Ed.),Life-Like Characters. Tools, Affective Functions and Applications, Springer, Berlin, 2004.[23] D.M. Dehn, S. Van Mulken, The impact of animated interface agents: A review of empirical research, Journal of Human–Computer Stud-ies 52 (1) (2000) 1–22.[24] E. Douglas-Cowie, N. Campbell, R. Cowie, P. Roach, Emotional speech: Towards a new generation of databases, Speech Communica-tion 40 (1–2) (2003) 33–60 (Special Issue Speech and Emotion).[25] T. Dutoit, V. Pagel, N. Pierret, F. Bataille, O.V. Vrecken, The mbrola project: Towards a set of high quality speech synthesisers free of use fornon-commercial purposes, in: Proceedings 4th International Conference of Spoken Language Processing, Philadelphia, USA, pp. 1393–1396.[26] K.R. Echavarria, M. Généreux, D. Arnold, A. Day, J. Glauert, Multilingual virtual city guides, in: Proceedings Graphicon, Novosibirsk, Russia,2005.[27] M. Elhadad, FUF/SURGE Homepage, Available from: http://www.cs.bgu.ac.il/surge, 19 September 2006.[28] G. Erbach, Profit 1.54 user’s guide, University of the Saarland, December 3, 1995.[29] P. Gebhard, M. Kipp, M. Klesen, T. Rist, Adding the emotional dimension to scripting character dialogues, in: Proceedings 4th InternationalWorking Conference on Intelligent Virtual Agents (IVA’03).[30] P. Gebhard, ALMA—a layered model of affect, in: Proceedings 4th International Joint Conference on Autonomous Agents and MultiagentSystems (AAMAS’05), Utrecht, Netherlands, 2005, pp. 29–36.[31] P. Gebhard, K.H. Kipp, Are computer-generated emotions and moods plausible to humans? in: Proceedings 6th International Conference onIntelligent Virtual Agents (IVA’06), Marina Del Rey, USA, 2006.[32] M. Grice, S. Baumann, R. Benzmüller, German intonation in autosegmental-metrical phonology, in: A. Jun (Ed.), Prosodic Typology. ThePhonology of Intonation and Phrasing, Oxford University Press, Oxford, 2005, pp. 55–83.[33] E. Gstrein, C. Schmotzer, B. Krenn, Report on demonstrator evaluation results, NECA IST report D9e, July 2004, downloadable fromhttp://www.ofai.at/research/nlu/NECA/publications/publication_docs/d9e.pdf.[34] L. Hall, M. Vala, M. Hall, M. Webster, S. Woods, A. Gordon, R. Aylett, FearNot’s appearance: Reflecting children’s expectations and perspec-tives, in: J. Gratch, M. Young, R. Aylett, D. Ballin, P. Olivier (Eds.), Proceedings 6th International Conference, IVA 2006, in: LNAI, vol. 4133,Springer, Berlin, 2006, pp. 407–419.[35] J. Hirschberg, Pitch accent in context: Predicting intonational prominence from text, Artificial Intelligence 63 (1993) 305–340.[36] L. Hiyakumoto, S. Prevost, J. Cassell, Semantic and discourse information for text-to-speech intonation, in: ACL Workshop on Concept-to-Speech Technology, 1997.[37] Z. Huang, A. Eliens, C. Visser, XSTEP: A markup language for embodied agents, in: Proceedings 16th International Conference on ComputerAnimation and Social Agents (CASA’2003), IEEE Press, 2003.[38] A. Iida, N. Campbell, S. Iga, F. Higuchi, M.A. Yasumura, Speech synthesis system with emotion for assisting communication, in: ProceedingsISCA Workshop on Speech and Emotion, Northern Ireland, 2000, pp. 167–172.[39] A.K. Joshi, L. Levy, M. Takahashi, Tree adjunct grammars, Journal of the Computer and System Sciences 10 (1975) 136–163.[40] H. Kamp, U. Reyle, From Discourse to Logic, Kluwer, Dordrecht, 1993.[41] M. Kantrowitz, GLINDA: Natural language text generation in the oz interactive fiction project, Technical Report CMU-CS-90-158, School ofComputer Science, Carnegie Mellon University, Pittsburgh, PA, 1990.[42] M. Kantrowitz, J. Bates, Integrated natural language generation systems, in: Aspects of Automated Natural Language Generation, in: D. Roes-ner, O. Stock (Eds.), NAI, vol. 587, Springer, Berlin, 1992.[43] H. Kasuya, K. Maekawa, S. Kiritani, Joint estimation of voice source and vocal tract parameters as applied to the study of voice sourcedynamics, in: Proceedings 14th International Conference of Phonetic Sciences, San Francisco, USA, pp. 2505–2512.[44] S. Kopp, I. Wachsmuth, Synthesizing multimodal utterances for conversational agents, Computer Animation and Virtual Worlds 15 (1) (2004)39–52.[45] S. Kopp, T. Sowa, I. Wachsmuth, Imitation games with an artificial agent: From mimicking to understanding shape-related iconic gestures, in:X. Camurri, X. Volpe (Eds.), Gesture-Based Communication in Human–Computer Interaction, in: LNAI, vol. 2915, Springer, Berlin, 2004,pp. 436–447, http://www.techfak.uni-bielefeld.de/%7Eskopp/download/gesture_imitation_GW03.pdf.[46] S. Kopp, B. Krenn, S. Marsella, A. Marshall, C. Pelachaud, H. Pirker, K. Thorisson, H. Vilhjalmsson, Towards a common framework formultimodal generation in ECAs: The behavior markup language, in: J. Gratch, et al. (Eds.), Intelligent Virtual Agents 2006, in: LNAI,vol. 4133, Springer, Berlin, 2006, pp. 205–217.[47] A. Kranstedt, S. Kopp, I. Wachsmuth, MURML: A multimodal utterance representation markup language for conversational agents, in:Proceedings AAMAS’02 Workshop Embodied conversational agents—let’s specify and evaluate them! Bologna, Italy, 2002.K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–12441243[48] B. Krenn, B. Neumayr, E. Gstrein, M. Grice, Lifelike agents for the Internet: A cross-cultural case study, in: S. Payr, R. Trappl (Eds.), AgentCulture: Human–Agent Interaction in a Multicultural World, Lawrence Erlbaum Associates, NJ, 2004, pp. 197–229.[49] B. Krenn, H. Pirker, Defining the gesticon: Language and gesture coordination for interacting embodied agents, in: Proceedings AISB-2004Symposium on Language, Speech and Gesture for Expressive Characters, University of Leeds, UK, 2004, pp. 107–115.[50] K. Lambrecht, Information Structure and Sentence Form, Cambridge University Press, Cambridge, 1994.[51] J.C. Lester, S.A. Converse, S.E. Kahler, S.T. Barlow, B.A. Stone, R.S. Bhoga, The persona effect: Affective impact of animated pedagogicalagents, in: Proceedings CHI Conference, Atlanta, Georgia, 1997.[52] W. Levelt, Speaking: From Intention to Articulation, MIT Press, Cambridge, MA, 1989.[53] A. Loyall, Believable agents: Building interactive personalities, Ph.D. thesis, CMU, Technical Report CMU-CS-97-123.[54] S. Marsella, W.L. Johnson, C. LaBore, Interactive pedagogical drama for health interventions, in: AIED 2003, 11th International Conferenceon Artificial Intelligence in Education, Australia, 2003.[55] M. Mateas, A. Stern, Facade: An experiment in building a fully-realized interactive drama, in: Game Developer’s Conference: Game DesignTrack, San Jose, California, 2003.[56] S. McRoy, S. Channarukul, S. Ali, An augmented template-based approach to text realization, Natural Language Engineering 9 (4) (2003)381–420.[57] A. Monaghan, Intonation in a text-to-speech conversion system, Ph.D. thesis, University of Edinburgh, 1991.[58] J.M. Montero, J. Gutiérrez-Arriola, J. Colás, E. Enríquez, J.M. Pardo, Analysis and modelling of emotional speech in Spanish, in: Proceedings14th International Conference of Phonetic Sciences, San Francisco, USA, 1999, pp. 957–960.[59] A. Ndiaye, P. Gebhard, M. Kipp, M. Klesen, M. Schneider, W. Wahlster, Ambient intelligence in edutainment: Tangible interaction withlife-like exhibit guides, in: Proceedings Conference on INtelligent TEchnologies for interactive entertainment (INTETAIN’05), Madonna diCampiglio, Italy, 2005.[60] N. Nicolov, C. Mellish, G. Ritchie, Approximate generation from non-hierarchical representations, in: Proceedings 8th International Workshopon Natural Language Generation, Herstmonceux Castle, UK, 1996.[61] A. Ortony, G.L. Clore, A. Collins, The Cognitive Structure of Emotions, Cambridge University Press, Cambridge, MA, 1988.[62] C. Pelachaud, N.I. Badler, M. Steedman, Generating facial expressions for speech, Cognitive Science 20 (1) (1996) 1–46.[63] P. Piwek, B. Krenn, M. Schröder, M. Grice, S. Baumann, H. Pirker, RRL: A rich representation language for the description of agent behaviourin NECA, in: Proceedings AAMAS Workshop Embodied Conversational Agents—Let’s Specify and Evaluate Them! Bologna, Italy, 2002.[64] P. Piwek, A flexible pragmatics-driven language generator for animated agents, in: Proceedings of EACL (Research Notes), Budapest, Hun-gary, 2003.[65] P. Piwek, The effect of gestures on the perception of a dialogue between two embodied conversational agents: a pilot study, Technical ReportITRI-03-09, ITRI, University of Brighton, UK, 2003.[66] P. Piwek, K. van Deemter, Dialogue as discourse: Controlling global properties of scripted dialogue, in: Proceedings AAAI Spring Symposiumon Natural Language Generation in Spoken and Written Dialogue, Stanford, California, 2003.[67] P. Piwek, J. Masthoff, M. Bergenstrahle, Reference and gestures in dialogue generation: Three studies with embodied conversational agents,in: Proceedings AISB05 Joint Symposium on Virtual Social Agents Symposium, University of Herfordshire, UK, 2005, pp. 53–60.[68] P. Piwek, K. Van Deemter, Generating under global constraints: The case of scripted dialogue, Journal of Research on Language and Compu-tation 5 (2) (2007) 237–263.[69] S. Prevost, M. Steedman, Specifying intonation from context for speech synthesis, Speech Communication 15 (1994) 139–153.[70] W. Reilly, Believable social and emotional agents, Ph.D. thesis, Carnegie Mellon University, Pittsburgh, 1996.[71] E. Reiter, R. Dale, Building Natural Language Generation Systems, Cambridge University Press, Cambridge, 2000.[72] J. Rickel, W.L. Johnson, Animated agents for procedural training in virtual reality: Perception, cognition and motor control, Applied ArtificialIntelligence 13 (1999) 343–382.[73] Y. Schabes, Mathematical and computational aspects of lexicalized grammars, Ph.D. thesis, Computer Science Department, University ofPennsylvania, 1990.[74] H. Schlosberg, A scale for the judgement of facial expressions, Journal of Experimental Psychology 29 (1941) 497–510.[75] M. Schröder, Speech and emotion research: An overview of research frameworks and a dimensional approach to emotional speech synthesis,Ph.D. thesis, Institute of Phonetics, Saarland University (Phonus 7), 2004.[76] M. Schröder, M. Grice, Expressing vocal effort in concatenative synthesis, in: Proceedings 15th International Conference of Phonetic Sciences,Barcelona, Spain, 2003.[77] M. Schröder, J. Trouvain, The German text-to-speech synthesis system MARY: A tool for research, development and teaching, InternationalJournal of Speech Technology 6 (2003) 365–377.[78] M. Schröder, Expressing degree of activation in synthetic speech, IEEE Transactions on Audio, Speech and Language Processing 14 (4)(2006) 1128–1136.[79] M. Stone, T. Bleam, C. Doran, M. Palmer, Lexicalized grammar and the description of motion events, in: TAG+: Workshop on Tree-AdjoiningGrammar and Related Formalisms, 2000.[80] A. Tassa, J.S. Liénard, A new approach to the evaluation of vocal effort by the psola method, in: WEB-SLS, The European Student Journal ofLanguage and Speech, 2000.[81] D. Traum, J. Bos, R. Cooper, S. Larsson, I. Lewin, C. Matheson, M. Poesio, A model of dialogue moves and information state revision, TrindiProject Deliverable D2.1, 1999.[82] O. Turk, M. Schröder, B. Bozkurt, L.M. Arslan, Voice quality interpolation for emotional text-to-speech synthesis, in: Proceedings Interspeech,Lisbon, Portugal, 2005, pp. 797–800.[83] K. van Deemter, E. Krahmer, M. Theune, Real versus template-based natural language generation: A false opposition? Computational Lin-guistics 31 (1) (2005) 15–24.1244K. van Deemter et al. / Artificial Intelligence 172 (2008) 1219–1244[84] K. van Deemter, What’s New? A semantic perspective on sentence accent, Journal of Semantics 11 (1994) 1–31.[85] E. Vallduví, E. Engdahl, The linguistic realisation of information packaging, Linguistics 34 (1996) 459–519.[86] H. Vilhjalmsson, Animating conversation in online games, in: M. Rauterberg (Ed.), Entertainment Computing ICEC, in: Lecture Notes inComputer Science, vol. 3166, Springer, Berlin, 2004, pp. 139–150.[87] M. White, M.E. Foster, J. Oberlander, A. Brown, Using facial feedback to enhance turn-taking in a multimodal dialogue system, in: Proceed-ings HCI International, 2005.