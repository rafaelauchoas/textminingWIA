Artificial Intelligence 171 (2007) 1–18www.elsevier.com/locate/artintIterated belief revision, revised ✩Yi Jin 1, Michael Thielscher ∗Department of Computer Science, Dresden University of Technology, GermanyReceived 8 September 2005; received in revised form 1 November 2006; accepted 9 November 2006Available online 13 December 2006AbstractThe AGM postulates for belief revision, augmented by the DP postulates for iterated belief revision, provide widely acceptedcriteria for the design of operators by which intelligent agents adapt their beliefs incrementally to new information. These postulatesalone, however, are too permissive: They support operators by which all newly acquired information is canceled as soon as an agentlearns a fact that contradicts some of its current beliefs. In this paper, we present a formal analysis of the deficiency of the standardpostulates alone, and we show how to solve the problem by an additional postulate of independence. We give a representationtheorem for this postulate and prove that it is compatible with AGM and DP.© 2006 Elsevier B.V. All rights reserved.Keywords: Iterated belief revision; Implicit dependence; Conditional beliefs1. IntroductionThe capability of gathering information about the world and revising its beliefs based on the new informationis crucial for an intelligent agent. Belief revision therefore is a central topic in Artificial Intelligence. Technically,belief revision is the process of changing the beliefs of an agent to accommodate new, more precise, or more reliableevidence that is possibly inconsistent with the existing beliefs.The formal study of belief revision took as starting point the work of Alchourrón, Gärdenfors, and Makinson(AGM) during the first half of the 1980s [1–3]. The AGM framework studies idealized mathematical models of beliefrevision. Given an underlying logic language L, the beliefs of an agent are represented by a set of sentences in L(known as belief set) which is closed under logical consequence. New evidence is also a sentence in L, and a beliefrevision operator incorporates the new evidence into the current belief set to obtain a revised belief set. The authors ofthe original AGM framework have developed their theory under two basic assumptions regarding the new evidence:it is intended to describe facts of the static world; and it is more reliable (hence prioritized in the revision process)than the prior beliefs. The latter assumption is often referred to as primacy of update. The necessity and ideas ofdistinguishing belief revision from belief update (suitable for a situation where the new evidence describes a change✩ This article is a substantial extension of the conference paper [Y. Jin, M. Thielscher, Iterated belief revision, revised, in: Proceedings of theInternational Joint Conference on Artificial Intelligence (IJCAI), Edinburgh, Scotland, August 2005, pp. 478–483].* Corresponding author.E-mail addresses: yijin@inf.tu-dresden.de (Y. Jin), mit@inf.tu-dresden.de (M. Thielscher).1 The first author is funded by the Deutsche Forschungsgemeinschaft under grant no. Gr 334/3.0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.11.0022Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–18of the world) was first noticed by Keller and Winslett [24] and later on formalized in [22]. Belief revision where thenew evidence is not prioritized, is a relatively recent topic studied by many researchers [5,10,12,18]. In this paper, wewill concentrate on the problem of prioritized belief revision where iterations are necessary.In situations where the new evidence is consistent with the existing beliefs, the two can just be merged; we call thismild revision. More interesting and complicated are situations where the evidence conflicts with the prior beliefs, inwhich case the agent needs to remove some of its currently held beliefs in order to accommodate the new evidence.This kind of revision is referred to as severe revision [13]. To provide general design criteria for belief revisionoperators, a set of postulates has been developed [3]. As first argued by the AGM trio and later frequently repeatedby others [9,13], the guiding principle of the AGM postulates is that of economy of information, or minimal changeof belief sets, which means not to give up currently held beliefs and not to generate new beliefs unless necessary.However, Rott [33,34] has recently pointed out that “it is a pure myth that minimal change principles are the foundationof existing theories of belief revision, at least as far as the AGM tradition is concerned”. His argument is mainly basedon the fact that so-called full meet revision [1] discards all prior beliefs in a severe revision and at the same timesatisfies all AGM postulates. This implies that the AGM postulates are too weak to capture the principle of minimalchange.For the incremental adaptation of beliefs, the AGM postulates proved to be overly weak, too [8,9]. This has led tothe development of additional postulates for iterated belief revision by Darwiche and Pearl (DP), among others (e.g.,[6,13,27]).Still, however, the AGM and DP postulates together are too permissive in that they support belief revision operatorswhich assume arbitrary dependencies among the pieces of information which an agent acquires along its way. Theseoperators have a drastic effect when the agent makes an observation which contradicts its currently held beliefs: Theagent is forced to cancel everything it has learned up to this point [28,30]. In this paper, we first give a formal analysisof this problem of implicit dependence, and then we present, as a solution, an Independence postulate for iterated beliefrevision. We give a representation theorem for our new postulate and prove its consistency by defining a concrete beliefrevision operator. We also contrast the Independence postulate to the so-called Recalcitrance postulate of [28,30] andargue that the latter is too strict in that it rejects reasonable belief revision operators.The rest of the paper is organized as follows. In the next section, we recall the classical AGM approach in apropositional setting as formulated by [23], followed by the approach of [8] for iterated belief revision. In Section 3,we formally analyze the problem of the DP postulates to be overly permissive. In Section 4, we present an additionalpostulate to overcome this deficiency, and we give a representation theorem for the postulate along with a concreterevision operator. We conclude in Section 5 with a detailed comparison to related work. Proofs of the main results canbe found in Appendix A.2. BackgroundIn this paper, we will deal with a propositional language L generated from a finite set P of atomic propositions.The language is that of classical propositional logic, i.e., with the classical consequence relation (cid:3). We say thattwo sentences α and β are logically equivalent, written as α ≡ β, iff α (cid:3) β and β (cid:3) α. As usual, a propositionalinterpretation (world) is a mapping from P to {(cid:5), ⊥}. The set of all interpretations is denoted by W. If an interpretationw truth-functionally maps a sentence μ to (cid:5), then w is called a model of μ (denoted by w |= μ). Given a sentence μ,we denote by Mods(μ) the set of all models of μ.A total pre-order (cid:2) (possibly indexed) is a reflexive, transitive binary relation s.t., either α (cid:2) β or β (cid:2) α holds forany α, β. The strict part of (cid:2) is denoted by <, that is, α < β iff α (cid:2) β and β (cid:7)(cid:2) α. As usual, α = β abbreviates α (cid:2) βand β (cid:2) α. Given any set S and total pre-order (cid:2), we denote by min(S, (cid:2)) the set of minimal elements of S wrt (cid:2).2.1. KM postulatesKatsuno and Mendelzon (KM) rephrased the AGM postulates for the propositional setting [23]. The beliefs of anagent are represented by a sentence ψ in L.2 Any new evidence is a sentence μ in L, and the result of revising ψ2 As L is assumed finite, any belief set can be represented as a sentence (modulo logical consequence). In this paper, we therefore do notdistinguish belief sets from sentences.Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–183with μ is also a sentence (denoted by ψ ∗ μ) which belongs to L. This is then the reformulation of the original AGMpostulates:(KM1) ψ ∗ μ (cid:3) μ.(KM2) If ψ ∧ μ is consistent, then ψ ∗ μ ≡ ψ ∧ μ.(KM3) If μ is consistent, then ψ ∗ μ is also consistent.(KM4) If ψ1 ≡ ψ2 and μ1 ≡ μ2, then ψ1 ∗ μ1 ≡ ψ2 ∗ μ2.(KM5) (ψ ∗ μ) ∧ φ (cid:3) ψ ∗ (μ ∧ φ).(KM6) If (ψ ∗ μ) ∧ φ is satisfiable, then ψ ∗ (μ ∧ φ) (cid:3) (ψ ∗ μ) ∧ φ.Readers are referred to [14] for the motivation and interpretation of these postulates.Katsuno and Mendelzon have given a representation theorem for Postulates (KM1)–(KM6) wrt a revision mecha-nism based on total pre-orders over possible world:Definition 1. A function that maps each belief set ψ to a total pre-order (cid:2)ψ on W is called a faithful assignment overbelief sets iff• If w1, w2 |= ψ , then w1 =ψ w2.• If w1 |= ψ and w2 (cid:7)|= ψ, then w1 <ψ w2.• If ψ ≡ φ, then (cid:2)ψ = (cid:2)φ.The intuitive meaning of w1 (cid:2)ψ w2 is that w1 is at least as plausible as w2 from the viewpoint of the agent whopossesses the belief set ψ. The total pre-order (cid:2)ψ is also called a faithful ranking wrt ψ.We particularly note that the last condition in Definition 1 says that faithful rankings of logically equivalent beliefsets must be identical. This essentially prohibits the possibility that in different situations the agent has the same beliefset but with different preferences among the beliefs.Theorem 1. [23] A revision operator ∗ satisfies Postulates (KM1)–(KM6) iff there exists a faithful assignment thatmaps a belief set ψ to a total pre-order (cid:2)ψ s.t.,Mods(ψ ∗ μ) = min(cid:2)Mods(μ), (cid:2)ψ(cid:3)Although the KM postulates were meant to be a reformulation of the AGM postulates for propositional logics,there is an important difference: The AGM postulates do not constrain operations wrt varying belief sets [3], whereasPostulate (KM4) stipulates that logically equivalent belief sets revised by logically equivalent sentences must resultin logically equivalent (new) belief sets. This essentially implies that a revision operator ∗ is a function on beliefsets (modulo logical equivalence). This, in turn, is highly controversial among the belief revisionists; in fact, it iscommonly believed that this amounts to too excessive a restriction on the conditions of a faithful assignment overbelief sets [9,13,17,30].We follow this consensus and argue that for a faithful reformulation of the AGM postulates, Postulate (KM4)should be weakened as follows:(KM4(cid:9)) If μ1 ≡ μ2, then ψ ∗ μ1 ≡ ψ ∗ μ2.The principle of minimal change is often argued to be the foundation of the AGM postulates. Indeed, Postulate(KM2) says that in the case of a mild revision the agent must retain both the prior beliefs and the new evidence. Buthow about the case of severe revisions? The following finding unveils the striking fact that the AGM postulates putno constraints at all on the retention of prior beliefs in the case of a severe revision. So-called full meet revision [1],denoted by ∗a, is a revision operator which completely “forgets” the prior beliefs when they contradict the newevidence:ψ ∗a μ =ψ ∧ μ if ψ (cid:2) ¬μμotherwise(1)(cid:4)4Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–18Full meet revision is also called amnesic revision by [34]. Despite its radical behavior, amnesic revision perfectlysatisfies all of (KM1)–(KM6) [1]. Consequently, in order to sufficiently impose the principle of minimal change, theKM postulates must be strengthened.2.2. DP frameworkAs proposed by many researchers [14,36], a general belief revision operator should exploit some kind of extra-logical information concerning the preference over different beliefs to determine the revision strategy. In particular,this preference information should uniquely determine a set of conditional beliefs: An agent is said to hold a condi-tional belief α (cid:10) β (with α, β sentences in L) precisely when it will believe β after a revision with α [6,15]. TheTriviality Theorem of [14] shows that, when using the AGM postulates, then it is improper to include conditionalbeliefs into the belief sets. As a consequence, we need to distinguish a belief set (referred to as propositional beliefs)from a belief state (also called epistemic state). The latter contains, in addition to its belief set, the conditional beliefswhich determine the revision strategy. In concrete constructions of belief revision operators, this extra-logic prefer-ence information could take the form of a relation over all possible worlds (as in Definition 1, or a relation over theset of all sentences [14], or a relation over all subsets of the belief set [3].Darwiche and Pearl [9] have suggested to make this idea explicit by regarding a belief revision operator as afunction on belief states (rather than on belief sets), that is, a function which maps a prior belief state and newevidence to a revised belief state. This has resulted in Postulates (R*1)–(R*6) shown below. From a pragmatic pointof view, it is very important that a revision operator delivers a revised belief state instead of a belief set, because onlyin this way the revision operator can be iterated when another piece of new evidence arrives. This also conforms withthe criterion of categorial matching [19].As in [9], for the sake of simplicity we will abuse notation by using interchangeably a belief state Ψ and its beliefset Bel(Ψ ). For example, Ψ and Ψ ∗ μ in Postulate (R*1) refer, respectively, to the current belief state and to theposterior belief state, while Ψ ∗ μ (cid:3) μ is just shorthand for Bel(Ψ ∗ μ) (cid:3) μ. The following are the modified KMpostulates for revision operators on belief states:(R*1) Ψ ∗ μ (cid:3) μ.(R*2) If Ψ ∧ μ is consistent, then Ψ ∗ μ ≡ Ψ ∧ μ.(R*3) If μ is consistent, then Ψ ∗ μ is also consistent.(R*4) If μ1 ≡ μ2, then Ψ ∗ μ1 ≡ Ψ ∗ μ2.3(R*5) (Ψ ∗ μ) ∧ φ (cid:3) Ψ ∗ (μ ∧ φ).(R*6) If (Ψ ∗ μ) ∧ φ is satisfiable, then Ψ ∗ (μ ∧ φ) (cid:3) (Ψ ∗ μ) ∧ φ.It is easy to observe that these postulates only put constraints on the change of the logical part (propositional beliefs)of the belief state, and those constraints are exactly the same as imposed by the original AGM postulates. Therefore,we consider the modified KM postulates (R*1)–(R*6), which are sometimes referred to as a weakening of the AGMpostulates [13,30], to be in fact a proper interpretation of them.In a way symmetric to Theorem 1, Darwiche and Pearl have given a representation theorem for Postulates (R*1)–(R*6):Definition 2. A function that maps each belief state Ψ to a total pre-order (cid:2)Ψ on W is called a faithful assignmentover belief states iff• If w1, w2 |= Ψ , then w1 =Ψ w2.• If w1 |= Ψ and w2 (cid:7)|= Ψ , then w1 <Ψ w2.3 The original version of (R*4) in [9] is as follows: If Ψ1 = Ψ2 and μ1 ≡ μ2, then Ψ1 ∗ μ1 ≡ Ψ2 ∗ μ2 where Ψ1 = Ψ2 means Ψ1 and Ψ2 areequal. However, Darwiche and Pearl have not given an explicit definition of the notion of a belief state, let alone a definition of two belief statesbeing equal. This is the reason why we have deliberately refrained from using the equality and reformulated this postulate for the sake of precision.Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–185Note that the conditions for a faithful assignment over belief states are much weaker than those for a faithfulassignment over belief sets, since logically equivalent belief sets now are allowed to have distinct faithful rankings.Theorem 2. [9] A revision operator ∗ satisfies Postulates (R*1)–(R*6) iff there exists a faithful assignment that mapsa belief state Ψ to a total pre-order (cid:2)Ψ such that(cid:2)Mods(μ), (cid:2)ΨMods(Ψ ∗ μ) = min(cid:3)According to the above theorem, for any revision operator ∗ (that satisfies Postulates (R*1)–(R*6)) there exists atleast one faithful assignment over belief states for which the specified condition holds. In general, there could be morethan one such assignment; however, it is not difficult to see that if L is finite then this faithful assignment must beunique. In the sequel, we will call this the faithful assignment corresponding to ∗.Theorem 2 only says which models of the new propositional beliefs are obtained after a single revision. In order toallow for successive revisions, in each revision step it must also be fully specified how the conditional beliefs are to bemodified. Following the principle of economy of information, some restrictions should be imposed on the change ofconditional beliefs, too. By concrete counterexamples, Darwiche and Pearl have shown that the KM postulates aloneare too weak to adequately characterize iterated belief revision, because they support unreasonable revision behaviors[8]. To overcome this deficiency, they have proposed these four additional postulates [9]:(C1) If β (cid:3) μ, then (Ψ ∗ μ) ∗ β ≡ Ψ ∗ β.(C2) If β (cid:3) ¬μ, then (Ψ ∗ μ) ∗ β ≡ Ψ ∗ β.(C3) If Ψ ∗ β (cid:3) μ, then (Ψ ∗ μ) ∗ β (cid:3) μ.(C4) If Ψ ∗ β (cid:2) ¬μ, then (Ψ ∗ μ) ∗ β (cid:2) ¬μ.Motivation and interpretation for these postulates can be found in [8,9].To provide formal justifications, Darwiche and Pearl have given an extension of the above representation theoremfor Postulates (C1)–(C4):Theorem 3. [9] Suppose that a revision operator satisfies Postulates (R*1)–(R*6). The operator satisfies Postulates(C1)–(C4) iff the operator and its corresponding faithful assignment satisfy:(CR1) If w1, w2 |= μ, then w1 (cid:2)Ψ w2 iff w1 (cid:2)Ψ ∗μ w2.(CR2) If w1, w2 (cid:7)|= μ, then w1 (cid:2)Ψ w2 iff w1 (cid:2)Ψ ∗μ w2.(CR3) If w1 |= μ and w2 (cid:7)|= μ, then w1 <Ψ w2 implies w1 <Ψ ∗μ w2.(CR4) If w1 |= μ and w2 (cid:7)|= μ, then w1 (cid:2)Ψ w2 implies w1 (cid:2)Ψ ∗μ w2.This theorem gives an elegant characterization of the seemingly natural constraints that the DP postulates imposeon the change of the conditional beliefs: When Ψ is revised by μ, Conditions (CR1) and (CR2) require not to changethe relative plausibility ordering of μ-worlds (¬μ-worlds, respectively); Conditions (CR3) and (CR4) require that if aμ-world w1 is (strictly) more plausible than a ¬μ-world w2, then w1 continues to be (strictly) more plausible than w2.In addition, [9] have shown that their four postulates are consistent with the (modified) KM postulates. They didso by defining a concrete revision operator which satisfies both (R*1)–(R*6) and (C1)–(C4).Theorem 3 implies that the DP postulates together impose constraints on the change of conditional beliefs. Thefollowing result shows that it is in fact only Postulate (C2) which puts additional constraints on the retention ofpropositional beliefs.Proposition 1. Amnesic revision ∗a satisfies (C1), (C3), and (C4), but violates (C2).To our knowledge, this observation has not been formalized elsewhere before.6Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–182.3. Two radical casesA different approach to studying iterated belief revision is by defining concrete revision operators. For instance, [6]has proposed a specific revision operator (known as natural revision) which satisfies the modified KM postulates andalso the following one:(CB) If Ψ ∗ μ (cid:3) ¬β, then (Ψ ∗ μ) ∗ β ≡ Ψ ∗ β.It is easy to see that the DP postulates are a weakening of Postulate (CB), in the sense Postulate (CB) implies allof the DP postulates but not vice versa.As shown by [7,9], Postulate (CB) imposes absolute minimization on the change of conditional beliefs:Theorem 4. Suppose that a revision operator satisfies Postulates (R*1)–(R*6). The operator satisfies Postulate (CB)iff the operator and its corresponding faithful assignment satisfy(CBR) If w1, w2 |= ¬(Ψ ∗ μ), then w1 (cid:2)Ψ w2 iff w1 (cid:2)Ψ ∗μ w2.Note that now w2, w2 |= Ψ ∗ μ is the only case where the relative ordering of w1, w2 in Ψ ∗ μ is not determined,since (cid:2)Ψ ∗μ must satisfy the conditions of Definition 2. Therefore, Condition (CBR) imposes absolute minimizationon the change of conditional beliefs permitted by the modified KM postulates. At first glance, therefore, it seems thatCondition (CBR) complies with the principle of economy of information.However, the following example of Darwiche and Pearl shows that Postulate (CB) is too radical, since a severerevision forces to cancel all previous evidences under any circumstances, which is usually not desirable.Example 1. We encounter a strange new animal and it appears to be a bird, so we believe the animal is a bird. As itcomes closer to our hiding place, we see clearly that the animal is red, so we believe that it is a red bird. To removefurther doubts about the animal birdhood, we call in a bird expert who takes it for examination and concludes that it isnot really a bird but some sort of mammal. The question now is whether we should still believe that the animal is red.As argued in [9], we have every reason to keep our belief that the animal is red, since birdhood and color are not cor-related. However, natural revision requires us to give up the belief of the animal’s color: According to Postulate (CB),from bird ∗ red (cid:3) ¬(¬bird) it follows that (bird ∗ red) ∗ ¬bird ≡ bird ∗ ¬bird.The above discussion suggests that the most conservative way of changing conditional beliefs is overly strict andnot desirable in general.While natural revision is the most conservative of all possible DP revision operators, another revision operator,called lexicographic revision (with “naked evidence”) [31], sits exactly on the opposite side of the spectrum. Lexico-graphic revision satisfies, in addition to Postulates (C1) and (C2), another so-called postulate of Recalcitrance:(Rec) If β (cid:2) ¬μ, then (Ψ ∗ μ) ∗ β (cid:3) μ.Semantically, Postulate (Rec) corresponds to the following condition [30]:(RecR) If w1 |= μ and w2 |= ¬μ, then w1 <Ψ ∗μ w2.According to (RecR), all possible worlds satisfying the new evidence become more reliable than those falsifyingthe new evidence, hence (Rec) is also said to impose the principle of strong primacy of update [25], which is arguablyonly suitable when the agent has full confidence in the new evidence. Based on its semantic characterization (i.e.,Conditions (CR1), (CR2) and (RecR)), it easy to see that lexicographic revision is the least conservative of all pos-sible DP revision operators, effecting most changes in the relative ordering of worlds permitted by the KM and DPpostulates [4]. In the next section, we will give a formal analysis of the problems of the DP postulates in general andthe problems of the greatest conservatism in particular. The discussion on the problems of the least conservatism ispostponed to Section 5.Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–1873. The problem of implicit dependenceAlthough most counterexamples in [9] against the KM postulates are solved by adding the DP postulates, severalopen problems remain. Specifically, the DP postulates are consistent with (CB), hence they do not block counterexamples against natural revision.Recall Example 1, where the DP postulates, in being compatible with (CB), are not strong enough to guarantee thatthe belief of the animal’s color is retained. This can be intuitively explained as follows: After observing the animal’scolor, we are actually acquiring a new conditional belief as a side-effect, namely, that the animal is red even if itwere not a bird, that is, ¬bird (cid:10) red. But none of the DP postulates enforces the acquisition of conditional beliefs.In the sequel, we first give a formal analysis of this weakness of the DP postulates, and then we present an additionalpostulate by which this problem is overcome.It is well known (see, e.g., [14]) that if a belief state Ψ suffices to uniquely determine a revision strategy thatsatisfies the AGM (or the KM) postulates, then the belief state determines a unique, total pre-order (cid:2)Bel(Ψ ) (known asepistemic entrenchment) over L which satisfies the following conditions:(EE1) If α (cid:2)Bel(Ψ ) β and β (cid:2)Bel(Ψ ) γ , then α (cid:2)Bel(Ψ ) γ .(EE2) If α (cid:3) β, then α (cid:2)Bel(Ψ ) β.(EE3) α (cid:2)Bel(Ψ ) α ∧ β or β (cid:2)Bel(Ψ ) α ∧ β, for any α and β.(EE4) If Ψ is consistent, then Ψ (cid:2) α precisely when α (cid:2)Bel(Ψ ) β for all β.(EE5) If β (cid:2)Bel(Ψ ) α for all β, then (cid:3) α.If α <Bel(Ψ ) β, then we say that the degree of the belief in β is higher than the degree of the belief in α (wrt Ψ ).Given an epistemic entrenchment, the corresponding belief revision operator is defined by the following condition:For any β,(C*) Ψ ∗ μ (cid:3) β if either (cid:3) ¬μ or ¬μ <Bel(Ψ ) ¬μ ∨ β.Other forms of total pre-orderings on L have been proposed, e.g., [32,38]. In fact, a recent result of Rott [35] showsthat such kind of pre-orderings exist even if the revision operator satisfies only (R*1), (R*3), and (R*4). All of theseorderings require extra-logical information, that is, they cannot be determined by pure logical relations among thesentences. In the following, we focus on pre-orderings given by epistemic entrenchments; however, our analysis doesnot depend on this particular choice and can be easily adapted to the other approaches just mentioned.To begin with, we define the notion of dependence between sentences wrt a belief state as follows [11]:Definition 3. A sentence β depends on another sentence μ in belief state Ψ precisely when Ψ (cid:3) β and Ψ ∗ ¬μ (cid:2) β.Two sentences μ, β are called dependent in Ψ if either μ depends on β or β depends on μ in Ψ .Consider, now, a (non-tautological) new evidence μ. Whenever Ψ (cid:3) β, condition (C*) implies that if μ (cid:7)<Bel(Ψ )μ ∨ β, then β is (implicitly) dependent on μ in Ψ . This kind of dependency could be problematic. In particular, it ispossible that two initially independent sentences become, undesirably, dependent after a revision step. In Example 1,for instance, red becomes dependent on bird after revising by red when natural revision is used.The problem of natural revision is that it assigns the lowest degree of belief to a new evidence without assertingconditional beliefs for independence. Thus the new evidence depends on all other beliefs which survive the revisionprocess. This explains why severe revision necessarily cancels all previous evidences. Of course, this is not merelya problem of natural revision: In the revised belief state Ψ ∗ μ, regardless of the belief degree of the new evidenceμ, a belief β (logically unrelated to μ) with a lower belief degree will depend on μ unless the revision operatorexplicitly asserts the condition μ <Bel(Ψ ∗μ) μ ∨ β. In other words, a rational revision operator has to bring aboutexplicitly the conditional belief ¬μ (cid:10) β. Symmetrically, a rational revision operator also should take care of theimplicit dependence of the new evidence on other beliefs with higher degrees.8Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–184. A postulate of independenceThe analysis in the previous section shows that in order to overcome the problem of implicit dependence, therevision operator must explicitly assert some conditional beliefs. It is easy to see that the DP postulates only requirethe preservation of conditional beliefs when a belief state Ψ is revised with μ: Postulates (C1) and (C2) neither requireto add nor to remove certain conditional beliefs; Postulate (C3) requires to retain the conditional belief β (cid:10) μ; finally,Postulate (C4) requires not to obtain the new conditional belief β (cid:10) ¬μ. Since none of the DP postulates stipulatesthe addition of independence assumptions, a new postulates is necessary to avoid undesired dependencies.As already mentioned, the revision process may introduce undesirable dependencies in both directions. That isto say, it could be that the new evidence becomes dependent on existing beliefs, or that it is the other way round.Prior to stating the new postulate, we show that the DP postulates impose some constraints on the retention of theindependence information in one direction. In the presence of the KM postulates, Postulate (C2) implies the following(since (Ψ ∗ ¬μ) ≡ (Ψ ∗ μ) ∗ ¬μ):(WC2) If Ψ ∗ ¬μ (cid:3) β, then (Ψ ∗ μ) ∗ ¬μ (cid:3) β.This essentially means that if β is not dependent on the new evidence μ in Ψ , then it also does not depend on μin Ψ ∗ μ.In order to ensure the explicit assertion of independence information in the other direction, we propose the follow-ing postulate of Independence (weak version) dual to (WC2):(WInd) If Ψ ∗ ¬β (cid:3) μ, then (Ψ ∗ μ) ∗ ¬β (cid:3) μ.Suppose Ψ (cid:3) μ, then Postulate (WInd) guarantees that if some new information μ does not depend on β in Ψ ,then it also does not depend on β in Ψ ∗ μ.As it is too much to require that the new information μ is already believed (i.e, Ψ (cid:3) μ), we propose the followingpostulate of Independence (strong version):(Ind) If Ψ ∗ ¬β (cid:2) ¬μ then (Ψ ∗ μ) ∗ ¬β (cid:3) μ.It is not difficult to see that (Ind) is a strengthening of (WInd). The new postulate essentially says that if the conditionalbelief ¬β (cid:10) ¬μ is not held in Ψ , then μ does not depend on β in Ψ ∗ μ.Postulate (Ind) is sufficient to overcome the problem of implicit dependence, as can be shown by reconsideringExample 1. According to (Ind), (bird ∗ red) ∗ ¬bird (cid:3) red, given that bird ∗ ¬bird (cid:2) ¬red. This shows that the newpostulate blocks unreasonable behaviors which are admitted by the DP postulates. In Section 5, we will also arguethat Postulate (Ind) is not overly strict.4.1. A representation theoremIn order to formally justify our new postulate, we will first provide a representation theorem along the line ofTheorem 3. Thereafter, we will design a concrete belief revision operator which satisfies (Ind).Theorem 5. Suppose that a revision operator satisfies Postulates (R*1)–(R*6). The operator satisfies Postulate (Ind)iff the operator and its corresponding faithful assignment satisfy:(IndR) If w1 |= μ and w2 |= ¬μ, then w1 (cid:2)Ψ w2 implies w1 <Ψ ∗μ w2.The above theorems shows that Postulate (Ind) is quite natural and not overly constrained: Condition (IndR) re-quires that a world w1 conforming with the new evidence becomes more plausible than a world w2 violating the newevidence only if w1 was at least as plausible as w2.An immediate consequence of Theorems 3 and 5 is that Postulate (Ind) implies both (C3) and (C4).Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–189Proposition 2. Suppose that a revision operator satisfies Postulates (R*1)–(R*6). If the operator satisfies Postulate(Ind), then it also satisfies Postulates (C3) and (C4).4.2. An OCF-based iterated revision operatorWe suggest to use the modified KM postulates along with Postulates (C1), (C2), and (Ind) to govern iterated beliefrevision. To show that these postulates together are consistent, we present a concrete revision operator which satisfiesall of them. The operator is a modification of Spohn’s proposal of revising ordinal conditional functions [36], whichcan be viewed as a qualitative version of Jeffrey’s Rule of probabilistic conditioning [16].Originally, an ordinal conditional function (OCF) has been defined as a mapping k from W to the class of ordinals.As in [37], for the sake of simplicity we take the signature of an OCF k as W → N, where k(w) is called the rankof w. Intuitively, the rank of a world represents its degree of implausibility, that is to say, the lower its rank, the moreplausible is a world. An OCF encodes both a belief set and the conditional beliefs. The belief set Bel(k) is the set ofsentences which hold in all worlds of rank 0:=(2)From now on, we use an OCF and its belief set interchangeably; e.g., μ ∈ k means μ ∈ Bel(k), and k ∧ μ denotes(cid:7)(cid:3)(cid:2)ModsBel(k)w | k(w) = 0(cid:5)(cid:6)Bel(k) ∧ μ.Given an OCF k, we can induce a ranking of sentences as follows:(cid:4)k(μ) =∞min{k(w)|w |= ¬μ} otherwiseif (cid:3) μ(3)Put in words, the rank of a sentence is the lowest rank of a world in which the sentence does not hold.4 Hence, thehigher the rank of a sentence, the firmer the belief in it, and the belief set consists of sentences with rank greaterthan 0. In fact, it is not hard to see that an OCF k determines an epistemic entrenchment as follows:α (cid:2)k β iffk(α) (cid:2) k(β)(4)Proposition 3. Given an OCF k, the binary relation (cid:2)k defined by (4) satisfies (EE1)–(EE5).By a slight modification of Spohn’s Conditionalization, we now define a revision operator which we call reinforce-ment revision operator. Like Conditionalization, reinforcement revision allows to assign different evidence degrees tonew evidences; standard KM/DP revision is easily obtained as a special case by using a fixed value in all iterations [9].An OCF k is revised according to new evidence μ with evidence degree m > 0 as follows:(k∗μ.m)(w) =k(w) − k(¬μ)k(w) + mif w |= μotherwise(5)(cid:4)Reinforcement revision is distinct from Spohn’s Conditionalization [36] in three aspects. First of all, it is merely arevision operator, whereas Conditionalization defines both a revision and a contraction operator (when the degree ofthe new information is 0). Secondly, in reinforcement revision the rank of the new evidence in the revised OCF is thesum of its old rank and the evidence degree, whereas in Conditionalization the rank of the new evidence is just itsevidence degree. The last, and crucial, difference is that Conditionalization does not satisfy Postulate (Ind).Assuming the same evidence degree for any new information, satisfiability of the KM postulates along with Postu-lates (C1), (C2), and (Ind) by reinforcement revision operator is a direct consequence of Theorems 2, 3, and 5.Theorem 6. Assume a fixed evidence degree for any new information. Reinforcement revision satisfies all modifiedKM postulates, DP postulates, and Postulate (Ind).A stronger result shows that all postulates are still satisfied in the general case, where the evidence degrees variesin the course of iterated revision. To begin with, we have the following:4 In Spohn’s original proposal, the rank of a sentence is the lowest rank of a world in which it is true. So the rank of β there is equal to k(¬β)here.10Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–18Theorem 7. For any m > 0, reinforcement revision satisfies all KM postulates (R*1)–(R*6), where Bel(Ψ ) and Ψ ∗ μare, respectively, identified withBel(k) and k∗(cid:7)μ,m.To show the validity of the remaining postulates (in case of varying evidence degrees), we need the followinglemma, which fully characterizes the change of belief degrees of non-tautological sentences.Lemma 1. Let k be an arbitrary OCF and μ a new evidence with degree m, then for any non-tautological sentence β,(cid:8)∗μ,m(β) =kk(β) + mk(μ ⊃ β) − k(¬μ)min(k(μ ⊃ β) − k(¬μ), k(β) + m)if (cid:3) μ ⊃ βelse if k(μ ⊃ β) = k(β)otherwiseAs a direct consequence of Lemma 1, it can be seen that our reinforcement revision operator has indeed a rein-forcement effect, that is, the evidence degrees of the new information are accumulated.Proposition 4. Let k be an arbitrary OCF and μ a new non-tautological evidence with degree m, then∗μ,m(μ) = k(μ) + mkFrom a pragmatic point of view, this is a desirable property in particular for domains where several independentinformation sources provide new information. In this case, it is appropriate to sum up the evidence degrees of thesame information from different sources.Finally, with the help of Lemma 1, we are able to prove that reinforcement revision satisfies (C1), (C2), and (Ind),regardless of evidence degrees.Theorem 8. For arbitrary m1, m2 > 0, reinforcement revision satisfies the following conditions:5(EC1) If α (cid:3) μ, then (k∗)∗μ,m1α,m2(EC2) If α (cid:3) ¬μ, then (k∗)∗α,m2μ,m1(EInd) If there exists m such that k∗≡ k∗α,m2≡ k∗¬β,m..α,m2(cid:2) ¬μ, then (k∗)∗¬β,m2(cid:3) μ.μ,m1Theorems 7 and 8 show that Postulate (Ind) is consistent with the KM and DP postulates. On the other hand, (Ind)does not follow from these postulates, as can be seen by the fact that (Ind) is incompatible with (CB), the postulatethat characterizes natural revision.It is worth mentioning that revision operators based on OCFs are particularly suitable for implementations of beliefrevision. For instance, in [20] we have presented a method (and its implementation) for the revision of belief baseswhich is equivalent to reinforcement revision. Moreover, we have shown that the complexity of reinforcement revisionis lower than that of most well-known operators [21].5. Related work and conclusionWe have suggested to use the modified KM postulates along with Postulates (C1), (C2), and (Ind) to govern iteratedbelief revision, that is to say, any rational iterated revision operator should satisfy all of these postulates. In the beliefrevision community there is, however, an ongoing controversy on what the proper framework for studying iteratedbelief revision should be. As in Darwiche and Pearl’s original work [8], revision operators are most commonly viewedas binary functions which map a belief set and the new information to the revised belief set. This is problematicin two aspects. First of all, the revision operators studied in the AGM theory are local in the sense that a fixedbelief set is assumed. Such revision operators are more appropriately considered as unary functions, which mapthe new information μ to a revised belief set K ∗ μ, with the understanding that K is taken to be the backgroundknowledge [33]. Secondly, the extra-logical preference information should play a role in the revision process. Basedon the characterization of revision operators as unary functions, [30] have proposed to view belief revision as dynamic,5 Note that, as before, we abuse notation by simply writing k∗μ,m instead of(cid:7)Bel(k∗μ,m) etc.Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–1811in the sense that the operator itself (i.e., the revision policy) evolves after each revision by taking the revised beliefset as the new background knowledge. While theoretically sound, the idea of dynamic revision is technically quiteconfusing in the sense that realizing a dynamic revision seems like devising an algorithm which evolves after each run.Most belief revisionists maintain that (iterated) revision operators should be functions on belief states [8,25,27,33,39],although there is no consensus on what is a belief state.Furthermore, while Postulate (C1) is almost universally accepted, Postulate (C2) seems to be more problematic. Infact, it is mainly different attitudes towards Postulate (C2) which provoke the dispute over the framework of iteratedbelief revision. In defense of our framework, we argue that, according to the semantical characterization (Conditions(CR1) and (CR2)), Postulate (C2) seems just as reasonable as Postulate (C1). If being informed about μ does notchange the relative plausibility ordering of μ-worlds, why should the relative ordering of ¬μ-worlds be changed?This idea is also supported by Spohn, who argues that it is only reasonable to change the relative ordering betweenμ-worlds and ¬μ-worlds [36].In the sequel, we will first give a detailed comparison of our framework with the most prominent existing ap-proaches to iterated revision. Thereafter, we will discuss the problems of least conservatism as promised in Section 2.3.5.1. Freund and Lehmann’s proposalsFreud and Lehmann were the first to point out that Postulate (C2) is inconsistent with the original KM postu-lates [13]. To avoid the inconsistency, they have suggested to replace the DP postulates by the so-called minimalinfluence postulate:(MinInf) If Ψ1 (cid:3) ¬μ and Ψ2 (cid:3) ¬μ, then Ψ1 ∗ μ ≡ Ψ2 ∗ μ.According to (MinInf), the revision Ψ ∗ μ does not depend on Ψ at all in the case of a severe revision. This is of coursea very strong restriction, which violates the intuition that the prior beliefs should play a major role. Furthermore, inthe presence of the AGM postulates, (MinInf) implies (C1), (C3), (C4) and the following weakening of (C2):(C2(cid:9)) If Ψ (cid:3) ¬β and β (cid:3) ¬μ, then (Ψ ∗ μ) ∗ β ≡ Ψ ∗ β.Strong as it is, Postulate (MinInf) is, on the other hand, too weak to rule out amnesic revision. Moreover, from the factthat the modified KM postulates are consistent with (C2), it follows that the inconsistency of (C2) with regard to theoriginal KM postulates is due to the assumption the latter made on the signature of revision operators (i.e., that theyare functions on belief sets). As already discussed, this assumption is not accepted, if not denied, by many researchers.Therefore, the proposal of (MinInf) is in some sense not well-supported.A conclusion Freund and Lehmann have drawn is that the AGM framework is not the right one in which to studyiterated revision. In a later work, Lehmann therefore has proposed an alternative approach to iterated revision, inwhich a belief state Ψ is a finite sequence of consistent (propositional) sentences (cid:16)β1 : · · · : βn(cid:17) (the revision historyof the agent) [27]. In Lehmann’s framework, the iterated revision operator is trivial: Ψ ∗ μ is simply defined as theconcatenation (cid:16)Ψ : μ(cid:17) of Ψ and μ. Similarly, we might denote (cid:16)Ψ1 : Ψ2(cid:17) by Ψ1 ∗ Ψ2. What seems more difficult todefine, however, is a mapping “Bel” from a belief state to its belief set. For this purpose, Lehmann has proposed thefollowing set of postulates:(I1) Bel(Ψ ) is consistent.(I2) μ ∈ Bel(Ψ ∗ μ).(I3) If β ∈ Bel(Ψ ∗ μ), then μ ⊃ β ∈ Bel(Ψ ).(I4) If μ ∈ Bel(Ψ ), then Ψ ∗ Ψ1 ≡ (Ψ ∗ μ) ∗ Ψ1.(I5) If β (cid:3) μ, then ((Ψ ∗ μ) ∗ β) ∗ Ψ1 ≡ (Ψ ∗ β) ∗ Ψ1.(I6) If ¬β /∈ Bel(Ψ ∗ μ), then ((Ψ ∗ μ) ∗ β) ∗ Ψ1 ≡ ((Ψ ∗ μ) ∗ μ ∧ β) ∗ Ψ1.(I7) Bel((Ψ ∗ ¬β) ∗ β) ⊆ Bel(Ψ ) + β.Readers are referred to [27] for the relation between Lehmann’s postulates and the AGM postulates. It is worth tomention that Postulate (I5) is in fact just an adaptation of Postulate (C1).12Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–18To provide a constructive model, Lehmann has shown that his postulates characterize the so-called widening rankedrevision. A widening ranked model is a function λ which maps an ordinal to a non-empty subset of W s.t.,(1) for any n, m, if n (cid:2) m, then λ(n) ⊆ λ(m), and(2) for any w ∈ W, there exists n with w ∈ λ(n).Given a widening ranked model λ, we can inductively define a rank r(Ψ ) and a set of worlds p(Ψ ) for any belief stateΨ :• r((cid:16)(cid:17)) = 0 and p((cid:16)(cid:17)) = λ(0), and• r((cid:16)Ψ : μ(cid:17)) = mino(Ψ, μ) and p((cid:16)Ψ : μ(cid:17)) = λ(r((cid:16)Ψ : μ(cid:17))) ∩ [μ],where mino(Ψ, μ) is the minimal ordinal n s.t., n (cid:3) r(Ψ ) and λ(n) ∩ [μ] (cid:7)= ∅.The widening ranked revision (thus, essentially, the mapping Bel) is then defined as follows:Mods(Ψ ∗ μ) = p(cid:3)(cid:2)(cid:16)Ψ : μ(cid:17)Lehmann has shown that the widening ranked revision generated from a widening ranked model satisfies Postu-lates (I1)–(I7). Conversely, any revision operator that satisfies Postulates (I1)–(I7) can be constructed as wideningranked revision. A major problem with widening ranked revision is that it is based on a fixed widening ranked modelwhich is external to the agent’s beliefs. Therefore, the agent is supposed to adhere to the same revision policy regard-less of its actual beliefs. Moreover, it is not at all clear where the external, extra-logical preference information comesfrom and how it is to be interpreted. Therefore, this kind of revision has been criticized by Rott as embodying a badphilosophy [35].5.2. Revision operators with memoryKonieczny and Pérez have proposed yet another framework for iterated revision, which also considers, as theagent’s belief state, the sequence of consistent sentences the agent has learned [25]. Like in Lehmann’s approach,the revised belief state Ψ ∗ μ is just the concatenation of Ψ and μ. However, Konieczny and Pérez have suggesteda different set of postulates for iterated belief revision, which are essentially a reformulation of the AGM postulatesalong with the following one:6(H7) Ψ ∗ Ψ1 ≡ Ψ ∗(cid:9)(cid:7)(cid:10)Bel(Ψ1).Postulate (H7) is a kind of associativity law, which expresses the strong confidence in the new information. It is notdifficult to see that (H7) implies (Rec) (cf. Section 2.3).The postulates proposed by Konieczny and Pérez characterize the so-called revision operators with memory, whichare based on external faithful assignments over belief sets: Given a faithful assignment over belief sets, we can induc-tively define a ranking (cid:21)Ψ of the possible worlds for any belief state Ψ :• (cid:21)(cid:16)(cid:17)= W × W, and• for any w1, w2: w1 (cid:21)(cid:16)Ψ :μ(cid:17) w2 iff w1 ≺μ w2 or w1 =μ w2 and w1 (cid:21)Ψ w2.The revision operator with memory is then defined as follows:(cid:2)[μ], (cid:21)ΨMods(Ψ ∗ μ) = min(cid:3)(6)Just like Lehmann’s revision, a revision operator with memory assumes a fixed (external) faithful assignment, whichmeans that the agent never changes its revision policy. Hence, Rott’s criticism regarding widening ranked revisionsalso applies to revision operators with memory.6 As L is assumed finite in [25], the conjunction(cid:7)Bel(Ψ1) is a well-defined sentence.Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–1813As a special case, a so-called basic memory operator is generated from a basic faithful assignment over L whichadditionally satisfies the following condition:• If w1, w2 |= ¬μ, then w1 =μ w2.Put in words, (cid:21)μ partitions W into two levels, where the lower level contains all μ-worlds while the other levelcontains all ¬μ-worlds.In fact, a basic memory operator is equivalent to Nayak’s lexicographic revision (with “naked evidence”) (cf. Sec-tion 2.3). Not surprisingly, therefore, Konieczny and Pérez were able to show that basic memory operators also satisfyall DP postulates.In their later work, Konieczny and Pérez [26] have suggested to lift the unrealistic restriction by allowing thefaithful ranking of the new evidence to be dynamic, meaning that logically equivalent evidences may come withdistinct faithful rankings. These new revision operators have therefore been named dynamic revision operators withmemory.Konieczny and Pérez have shown that any dynamic revision operator with memory satisfies (C1), (C3), and (C4),but violates (C2). Based on this, they have criticized (C2) as too strong [25]. In particular, they have proposed thefollowing counterexample:Example 2. Consider an electric circuit containing an adder and a multiplier. The atomic propositions adder_ok andmultiplier_ok denote respectively that the adder and the multiplier are working. Initially we have no information aboutthis circuit (Ψ = (cid:16) (cid:17)), and we then learn that the adder and the multiplier are working (μ = adder_ok ∧ multiplier_ok).Thereafter, someone tells us that the adder is actually not working (β = ¬adder_ok). There is no reason to “forget”that the multiplier is working, whereas imposed by (C2) we have (Ψ ∗ μ) ∗ β ≡ Ψ ∗ β, since β (cid:3) ¬μ.In favour of (C2), we give a counterargument to Konieczny and Pérez’s criticism. First we observe that a (dynamic)revision operator with memory is not a single revision operator, unlike what the AGM framework attempts to model.Since the new information is coupled with a faithful ranking, a revision operator with memory (except basic memoryrevision) essentially is a multiple revision operator which revises a belief state with another belief state. After ob-serving that, it is no surprise that (C2) is violated since this postulate is only intended for single revision operators.This argument is supported by the fact that basic memory revision does satisfy (C2). From the perspective of singlerevision, the behavior imposed by (C2) in Example 2 is perfectly reasonable, since the evidence μ is supposed to bean atomic piece of information. Note that in case we learned adder_ok and multiplier_ok in succession, then thanksto Postulate (Ind) we will retain multiplier_ok after the ¬adder_ok-revision. In fact it is not difficult to see that if wewant the revision operator with memory to exhibit the behavior expected by Konieczny and Pérez, then the faithfulranking that comes with μ should encode the independence of multiplier_ok and adder_ok. This somehow highlightsthe subtle distinction between revising by a conjunction of sentences and revising by a set of sentences (with differentplausibility degrees) (cf. the discussions in [29]), which will be further cultivated in the future. Based on the aboveargument, we consider (C2) a well justified postulate for single revision operators, although it could be too strong formultiple revision operators.5.3. Dynamic revision operatorsIndependently, [28] have also noticed the inconsistency between (C2) and the original KM postulates. Their solu-tion to avoid inconsistency has been to view belief revision as dynamic, as mentioned above. By so doing, it becomespossible to safely accept the DP postulates. The framework of dynamic revision operators is not too different fromthe DP framework, except that the former makes explicit the idea of evolutionary revision policy in its postulates, bydistinguishing between an original and a revised policy.The problem of the DP postulates to be overly permissive has also been studied by Nayak et al. [28,30]. They havesuggested to strengthen the DP postulates by the following so-called postulate of Conjunction:77 In [30], (Conj) is written as “if μ (cid:2) ¬β, then (Ψ ∗ μ) ∗μ β ≡ Ψ ∗ (β ∧ μ)”, where ∗μ denotes the evolved operator after a μ-revision.Accordingly, they have reformulated the DP postulates in the same spirit.14Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–18(Conj) If μ (cid:2) ¬β, then (Ψ ∗ μ) ∗ β ≡ Ψ ∗ (β ∧ μ).In the presence of the modified KM postulates, (Conj) is strong enough to imply Postulate (Rec).In the following, we argue that Postulate (Conj), while strengthening the DP postulates, is overly strict. To this end,we show that even Postulate (Rec) is too strong. As shown in Section 2.3, (Rec) corresponds to the least conservatismin the DP framework. Thus, the following argument is also an analysis of the problems of the least conservatism.Postulate (Rec) says that, as long as β ⊃ ¬μ is not a tautology, it should be canceled after a successive revision byμ followed by β, no matter how strong the initial belief in β ⊃ ¬μ. A simple example shows that this behavior maynot be reasonable:Example 3. All her childhood, Alice was taught by her parents that a person who has told a lie is not a good person.So Alice believed, initially, that if Bob has told a lie then he is not a good person. After her first date with Bob, shebegan to believe that he is a good guy. Then a reliable friend of Alice warns her that Bob is in fact a liar, and Alicechooses to believe her. Now, should Alice still believe that Bob is a good guy?According to Postulate (Rec), Alice should not challenge Bob’s morality and still believe he is good, and hencedisbelieve what her parents taught her. But in fact it is at least as reasonable to give up the belief that Bob is good.This shows that Postulate (Rec) is too strict a criterion for belief revision operators.With regard to the postulate we have proposed, it is easy to see that (Ind) is a weakening of Postulate (Rec). Thisraises the question whether Postulate (Ind) weakens too much. Let us consider an example, taken from [28], which, atfirst glance, seems to show that this is indeed the case.Example 4. Our agent believes that Tweety is a singing bird. However, since there is no strong correlation betweensinging and birdhood, the agent is prepared to retain the belief that Tweety sings even after accepting the informationthat Tweety is not a bird, and conversely, if the agent were to be informed that Tweety does not sing, she would stillretain the belief that Tweety is a bird. Imagine that the agent first receives the information that Tweety is in fact not abird, and later learns that Tweety does not sing.Nayak et al. claimed that it is only reasonable to assume that the agent should, in the end, always believe thatTweety is a non-singing non-bird. Indeed, with Ψ ≡ singing ∧ bird it follows from Postulate (Rec) that (Ψ ∗ ¬bird) ∗¬singing (cid:3) ¬bird, since (cid:2) ¬singing ⊃ bird. Postulate (Ind), on the other hand, does not apply in this case. Butthe behavior which is claimed to be the only reasonable one is not generally justified. Suppose, for example, theagent initially believes firmly that ¬singing ⊃ bird. It is then possible, after revising by ¬bird, that the belief in¬singing ⊃ bird is stronger than the belief in ¬bird. In this case, after further revising by ¬singing, the agent believesthat Tweety is a bird after all.5.4. ConclusionIn this paper, we have formally analyzed the problem of implicit dependence which is intrinsic to belief revisionbut largely overlooked in the community over the past decade. As (at least a partial) solution to the problem, wehave proposed to strengthen the DP theory by a new postulate of independence. The resulting framework for iteratedbelief revision now consists of the (modified) KM postulates, (C1), (C2), and (Ind). We have informally arguedin favor of our new postulate (Ind) by means of examples, and we have given a formal justification by an elegantsemantic characterization. Also, a detailed comparison to related work has shown that our new framework is the mostsatisfactory one thus far in the literature. As a conclusion, we argue that the new framework provides better criteriafor the design of rational iterated belief revision operators.AcknowledgementWe want to thank the anonymous reviewers for their valuable comments which helped to improve the paper.Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–1815Appendix A. ProofsProposition 1. The amnesic revision ∗a satisfies (C1), (C3), and (C4), but violates (C2).Proof. Note that, in the case of the amnesic revision ∗a, a belief state is identified with its propositional beliefs.Assume (cid:3) ¬β. According to (1), we have (Ψ ∗a μ) ∗a β = β and Ψ ∗a β = β. Hence, ∗a satisfies (C1), (C2), and(C3). Moreover, (C4) is vacuously satisfied. In the rest of the proof, we consider the case (cid:2) ¬β.Assume β (cid:3) μ. We consider two cases: 1) Assume Ψ (cid:2) ¬μ. It follows from β (cid:3) μ that we have Ψ ∧ μ (cid:2) ¬β.According to (1), if Ψ (cid:2) ¬β then (Ψ ∗a μ) ∗a β = (Ψ ∧ μ) ∗a β = Ψ ∧ μ ∧ β and Ψ ∗a β = Ψ ∧ β; otherwise(Ψ ∗a μ) ∗a β = (Ψ ∧ μ) ∗a β = β and Ψ ∗a β = β. 2) Assume Ψ (cid:3) ¬μ. From β (cid:3) μ, if follows that Ψ (cid:3) ¬β. Since(cid:2) ¬β and β (cid:3) μ, we have μ (cid:2) ¬β. According to (1), (Ψ ∗a μ) ∗a β = μ ∗a β = μ ∧ β and Ψ ∗a β = β. Therefore ∗asatisfies (C1).Assume Ψ ∗a β (cid:3) μ. We consider two cases: 1) Assume Ψ (cid:2) ¬β. According to (1), Ψ ∗a β = Ψ ∧ β. It followsfrom Ψ ∧ β (cid:3) μ and Ψ (cid:2) ¬β that we have Ψ ∧ μ (cid:2) ¬β. According to (1), (Ψ ∗a μ) ∗a β is either Ψ ∧ μ ∧ β or μ ∧ β.2) Assume Ψ (cid:3) ¬β. According to (1), Ψ ∗a β = β. From Ψ ∗a β (cid:3) μ it follows that β (cid:3) μ. Since ∗a satisfies (*1),we have (Ψ ∗a μ) ∗a β (cid:3) μ. Therefore ∗a satisfies (C3).Assume Ψ ∗a β (cid:2) ¬μ. Obviously, we have (cid:2) ¬μ. Consider two cases: 1) Assume Ψ (cid:2) ¬β. According to (1),Ψ ∗a β = Ψ ∧ β. From Ψ ∧ β (cid:2) ¬μ and Ψ (cid:2) ¬β it follows Ψ ∧ μ (cid:2) β. According to (1), (Ψ ∗a μ) ∗a β = Ψ ∧ μ ∧ β.From (cid:2) ¬μ it follows Ψ ∧ μ ∧ β (cid:2) ¬μ. 2) Assume Ψ (cid:3) ¬β. According to (1), Ψ ∗a β = β. Since ∗a satisfies (*1),we have (Ψ ∗a μ) ∗a β (cid:3) β. From (cid:2) ¬β and β (cid:2) ¬μ it follows (Ψ ∗a μ) ∗a β (cid:2) ¬μ. Therefore ∗a satisfies (C4).The following counterexample shows that ∗a violates (C2). Let μ, β, and Ψ be, respectively, p, ¬p, and p ∨ q (p, qare propositional atoms). Obviously, β (cid:3) ¬μ holds. According to (1), (Ψ ∗a μ) ∗a β = ¬q and Ψ ∗a β = (p ∨ q) ∧ ¬q.Therefore ∗a violates (C2). (cid:2)For the proof of the representation theorem, we need the following observation, which is a direct consequence ofTheorem 2.Lemma 2. Suppose that a revision operator satisfies Postulates (R*1)–(R*6). If (cid:2) ¬β, then Ψ ∗ β (cid:3) μ precisely whenthere exists a world w such that w |= μ ∧ β and w <Ψ w(cid:9) for any w(cid:9) |= ¬μ ∧ β, where (cid:2)Ψ is the correspondingfaithful assignment.Theorem 5. Suppose that a revision operator satisfies Postulates (R*1)–(R*6). The operator satisfies Postulate (Ind)iff the operator and its corresponding faithful assignment satisfy:(IndR) If w1 |= μ and w2 |= ¬μ, then w1 (cid:2)Ψ w2 implies w1 <Ψ ∗μ w2.Proof. “⇐”: Assume Ψ ∗β (cid:2) ¬μ. From Lemma 2, it follows that for any world w |= β ∧ ¬μ, there exists anotherworld w(cid:9) |= β ∧ μ such that w(cid:9) (cid:2)Ψ w. Hence, since (cid:2)Ψ is total, there must be a world w1 such that w1 |= μ ∧ βand w1 (cid:2)Ψ w2 for any w2 |= ¬μ ∧ β. Condition (IndR) then implies that w1 <Ψ ∗μ w2 for any w2 |= ¬μ ∧ β. Due toLemma 2, we have (Ψ ∗ μ) ∗ β (cid:3) μ.“⇒”: Assume w1 |= μ, w2 |= ¬μ, and w1 (cid:2)Ψ w2. Let β be such that Mods(β) = {w1, w2}. From Theorem 2 itfollows that w1 ∈ Mods(Ψ ∗ β). Hence Ψ ∗ β (cid:2) ¬μ. Postulate (Ind) implies (Ψ ∗ μ) ∗ β (cid:3) μ. Due to Postulates (R*1)and (R*3), Mods((Ψ ∗ μ) ∗ β) = {w1}. From Theorem 2 it follows that w1 <Ψ ∗μ w2. (cid:2)Proposition 3. Given an OCF k, the binary relation (cid:2)k defined by (4) satisfies (EE1)–(EE5).Proof. Due to the transitivity of (cid:2) on N, (cid:2)k satisfies (EE1).Assume α (cid:3) β. By contra-position, we have ¬β (cid:3) ¬α. Hence, for any w ∈ W, if w |= ¬β then w |= ¬α. Accordingto (3), we have k(α) (cid:2) k(β), i.e., α (cid:2)k β. Thus (cid:2)k satisfies (EE2).Assume k(α) > k(α ∧ β) and k(β) > k(α ∧ β). From (3), it follows that there exists w s.t., k(w) = k(α ∧ β) andw |= ¬α ∨ ¬β. Since k(α) > k(α ∧ β), according to (3), we have w (cid:7)|= ¬α, i.e., w |= α. From w |= ¬α ∨ ¬β, it16Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–18follows that w |= ¬β. It follows from (3) that k(β) (cid:2) k(α ∧ β), which contradicts k(β) > k(α ∧ β). Thus (cid:2)k satisfies(EE3).Assume Bel(k) is consistent. According to (2), there exists w1 s.t., k(w1) = 0. From (3), it follows thatk(from(w1)) = 0. According to (2) and (3), Bel(k) (cid:2) α iff there exists w s.t., k(w) = 0 and w |= ¬α, i.e., k(α) = 0.Since k(from(w1)) = 0, we have Bel(k) (cid:2) α iff k(α) (cid:2) k(β), for any β. Thus (cid:2)k satisfies (EE4).Assume (cid:2) α. According to (3), k((cid:5)) > k(α). Hence, by contra-position, (cid:2)k satisfies (EE5). (cid:2)The following lemma is needed in the proof of Theorem 7.Lemma 3. Let k be an OCF and μ a new evidence, then for any m1, m2,Bel(k) = Bel(k∗μ,m1Proof. According to (5), k∗set of worlds with rank 0 in the revised OCF. From (2) it follows immediately that Bel(k∗∗μ,m2)μ,m(w) = 0 iff w |= μ and k(w) = k(¬μ), which means the value of m does not affect the) = Bel(k∗).μ,m2μ,m1Theorem 7. For any m > 0, reinforcement revision satisfies all KM postulates (R*1)–(R*6), where Bel(Ψ ) and Ψ ∗ μare, respectively, identified withBel(k) and k∗(cid:7)μ,m.Proof. Obviously, each OCF k can induce a faithful ranking (cid:2)Bel(k) of Bel(k) by lettingw1 (cid:2)Bel(k) w2iffμ,m(w) = 0 iff w |= μ and k(w) = k(¬μ). From (3), it is easy to see that k∗According to (5), k∗w ∈ min(Mods(μ), (cid:2)Bel(k)).k(w1) (cid:2) k(w2)μ,m(w) = 0 iffIf we fix the value of m, then according to Theorem 6, reinforcement revision satisfies all KM postulates (R*1)–(R*6). From Lemma 3, it follows that satisfiability of (R*1)–(R*6) still holds for varying values of m. (cid:2)Lemma 1. Let k be an arbitrary ordinal conditional function and μ a new evidence with degree m, then for anynon-tautological sentence β,k(β) + mk(μ ⊃ β) − k(¬μ)min(k(μ ⊃ β) − k(¬μ), k(β) + m)if (cid:3) μ ⊃ βelse if k(μ ⊃ β) = k(β)otherwise∗μ,m(β) =(cid:8)kProof. Assume (cid:3) μ ⊃ β. From (cid:2) β and (3) it follows that there exists w1 |= ¬β s.t., k(w1) = k(β) and k(w) (cid:3) k(w1)for any w |= ¬β. Since (cid:3) μ ⊃ β, we have w1 |= ¬μ. According to (5), k∗μ,m(w1) = k(w1) + m. Similarly, for anyμ,m(w) = k(w) + m. Again according to (3) we have k∗w |= ¬β we have k∗μ,m(β) = k(β) + m.Assume (cid:2) μ ⊃ β and k(μ ⊃ β) = k(β). From (cid:2) μ ⊃ β and (3) it follows that there exists w1 |= μ ∧ ¬β s.t.,μ,m(w1) = k(w1) − k(¬μ). Since k(μ ⊃ β) = k(β), according to (3) we haveμ,m(w) is either k(w) − k(¬μ) or k(w) + m.k(w1) = k(μ ⊃ β). According to (5), k∗k(w) (cid:3) k(w1) for any w |= ¬β. It follows from (5) that for any w |= ¬β, k∗Therefore, according to (3) we have k∗μ,m(β) = k(β) − k(¬μ).Assume (cid:2) μ ⊃ β and k(μ ⊃ β) (cid:7)= k(β). It is not difficult to see, according to (3), that this is possible only ifk(μ ⊃ β) > k(β). From (cid:2) μ ⊃ β and (3), it follows that there exists w1 |= μ ∧ ¬β s.t., k(w1) = k(μ ⊃ β) andk(w) (cid:3) k(w1) for any w |= μ ∧ ¬β. Analogously, there exists w2 |= ¬β s.t., k(w2) = k(β) and k(w) (cid:3) k(w2) for anyw |= ¬β. Consider two cases: 1) Assume k(μ ⊃ β) − k(¬μ) (cid:2) k(β) + m. According to (5), k∗μ,m(w1) = k(w1) −k(¬μ). For any w |= ¬β, according to (5), if w |= μ, then k∗μ,m(w) = k(w) − k(¬μ) (cid:3) k(w1) − k(¬μ); otherwisek∗μ,m(w) = k(w) + m (cid:3) k(w2) + m (cid:3) k(w1) − k(¬μ). From (3) it follows that k∗μ,m(β) = k(μ ⊃ β) − k(¬μ). 2)Assume k(μ ⊃ β) − k(¬μ) > k(β) + m. Since k(μ ⊃ β) > k(β), according to (3), we have w2 |= ¬μ. From (5) itfollows that k∗μ,m(w) = k(w) − k(¬μ) (cid:3)k(w1) − k(¬μ) > k(w2) + m; otherwise k∗μ,m(β) =k(β) + m. Therefore, k∗μ,m(w2) = k(w2) + m. For any w |= ¬β, according to (5), if w |= μ, then k∗μ,m(w) = k(w) + m (cid:3) k(w2) + m. From (3), it follows that k∗μ,m(β) = min(k(μ ⊃ β) − k(¬μ), k(β) + m). (cid:2)Proposition 4. Let k be an arbitrary OCF and μ a new non-tautological evidence with degree m, then∗μ,m(μ) = k(μ) + mkY. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–1817Proof. This is a direct consequence of Lemma 1. (cid:2)Theorem 8. For arbitrary m1, m2 > 0, reinforcement revision satisfies the following conditions:8(EC1) If α (cid:3) μ, then (k∗)∗μ,m1α,m2(EC2) If α (cid:3) ¬μ, then (k∗)∗α,m2μ,m1(EInd) If there exists m such that k∗≡ k∗α,m2≡ k∗¬β,m..α,m2(cid:2) ¬μ, then (k∗)∗¬β,m2(cid:3) μ.μ,m1Proof. If (cid:3) ¬α, Condition (EC1) holds trivially. Assume that α (cid:3) μ and (cid:2) ¬α. By (5),k∗α,m2(w) = 0iff w |= α and k(w) = k(¬α)(A.1)Likewise,∗μ,m1∗)α,m2(k(w) = 0Since α (cid:3) μ, for any w |= α we have k∗from Lemma 1 that k∗iff w |= α and kμ,m1(w) = k∗μ,m1(w) = k(w) − k(¬μ) by (5). Since μ ⊃ ¬α ≡ ¬α and (cid:2) ¬α, it follows∗μ,m1(¬α)(A.2)(¬α) = k(¬α) − k(¬μ). Hence, (A.2) is equivalent toμ,m1∗(w) = 0)α,m2This and (A.1) implies (k∗∗μ,m1(kiff w |= α and k(w) = k(¬α))∗α,m2≡ k∗α,m2μ,m1We prove Condition (EInd) by contradiction. To begin with, from the assumption that k∗. Condition (EC2) can be proved analogously.(cid:2) β and (cid:2) μ ⊃ β. Furthermore, there exists w such that k∗of (3), this implies k(β) = k(μ ⊃ β).(cid:2) ¬μ it follows that¬β,m(w) = 0, w |= ¬β ∧ μ, and k(w) = k(β). With the help¬β,mNow assume that (k∗μ,m1(w(cid:9)) = k∗and k∗k(w(cid:9)) + m1 > k(β). But from Lemma 1 it follows that k∗k∗μ,m1(w(cid:9)) = k∗(β). (cid:2)μ,m1μ,m1μ,m1)∗¬β,m2(cid:2) μ. It follows that there exists w(cid:9) such that (k∗μ,m1(β). Since k(w) = k(β) and w(cid:9) |= ¬β, we have k(w(cid:9)) (cid:3) k(w). Hence by (5), k∗(w(cid:9)) = 0, w(cid:9) |= ¬β ∧ ¬μ,(w(cid:9)) =(β) (cid:2) k(β), since (cid:2) β and (cid:2) μ ⊃ β. This contradicts)∗¬β,m2μ,m1μ,m1References[1] C.E. Alchourrón, D. Makinson, On the logic of theory change: contraction functions and their associated revision functions, Theoria 48 (1982)14–37.[2] C.E. Alchourrón, D. Makinson, On the logic of theory change: safe contraction, Studia Logica 44 (1985) 405–422.[3] C.E. Alchourrón, P. Gärdenfors, D. Makinson, On the logic of theory change: partial meet contraction and revision functions, Journal ofSymbolic Logic 50 (2) (1985) 510–530.[4] R. Booth, S. Chopra, T. Meyer, Restrained revision, in: Proceedings of the Sixth Workshop on Nonmonotonic Reasoning, Action and Change(NRAC), 2005.[5] R. Booth, A negotiation-style framework for non-prioritised revision, in: Proceedings of Theoretical Aspects of Reasoning about Knowledge(TARK), vol. 8, Morgan Kaufmann, 2001, pp. 137–150.[6] C. Boutilier, Revision sequences and nested conditionals, in: Proceedings of the International Joint Conference on Artificial Intelligence(IJCAI), 1993, pp. 519–525.[7] C. Boutilier, Iterated revision and minimal change of conditional beliefs, Journal of Philosophical Logic 25 (3) (1996).[8] A. Darwiche, J. Pearl, On the logic of iterated belief revision, in: R. Fagin (Ed.), Proceedings of Theoretical Aspects of Reasoning aboutKnowledge (TARK), Morgan Kaufmann, 1994, pp. 5–23.[9] A. Darwiche, J. Pearl, On the logic of iterated belief revision, Artificial Intelligence 89 (1997) 1–29.[10] J.P. Delgrande, D. Dubois, J. Lang, Iterated revision as prioritized merging, in: P. Doherty, J. Mylopoulos, C.A. Welty (Eds.), Proceedings ofthe International Conference on Principles of Knowledge Representation and Reasoning (KR), AAAI Press, 2006, pp. 210–220.[11] L. Fariñas del Cerro, A. Herzig, Belief change and dependence, in: Y. Shoham (Ed.), Proceedings of Theoretical Aspects of Reasoning aboutKnowledge (TARK), Morgan Kaufmann, 1996, pp. 147–162.[12] E.L. Fermé, S.O. Hansson, Selective revision, Studia Logica 63 (3) (1999) 331–342.[13] M. Freund, D. Lehmann, Belief revision and rational inference, Technical Report TR-94-16, Institute of Computer Science, The HebrewUniversity of Jerusalem, Jerusalem, 91904, Israel, 1994.[14] P. Gärdenfors, D. Makinson, Revisions of knowledge systems using epistemic entrenchment, in: Proceedings of Theoretical Aspects of Rea-soning about Knowledge (TARK), Asilomar, CA, 1988, pp. 83–95.8 Note that, as before, we abuse notation by simply writing k∗μ,m instead of(cid:7)Bel(k∗μ,m) etc.18Y. Jin, M. Thielscher / Artificial Intelligence 171 (2007) 1–18[15] P. Gärdenfors, Knowledge in Flux: Modeling the Dynamics of Epistemic States, MIT Press, 1988.[16] M. Goldszmidt, Qualitative probabilities: a normative framework for commonsense reasoning, PhD thesis, University of California at LosAngeles, 1992.[17] S.O. Hansson, Revision of belief sets and belief bases, in: D. Dubois, H. Prade (Eds.), Handbook of Defeasible Reasoning and UncertaintyManagement Systems, vol. 3: Belief Change, Kluwer Academic Publishers, Dordrecht, 1998, pp. 17–75.[18] S.O. Hansson, A survey of non-prioritized belief revision, Erkenntnis 50 (1999) 413–427.[19] S.O. Hansson, Ten philosophical problems in belief revision, Journal of Logic and Computation 13 (1) (2003) 37–49.[20] Y. Jin, M. Thielscher, Representing beliefs in the fluent calculus, in: R.L. de Mántaras, L. Saitta (Eds.), Proceedings of the European Confer-ence on Artificial Intelligence (ECAI), Valencia, Spain, IOS Press, August 2004, pp. 823–827.[21] Y. Jin, M. Thielscher, Actions and belief revision: a computational approach, in: J. Delgrande, J. Lang, H. Rott, J. Tallon, (Eds.), Belief Changein Rational Agents: Perspectives from Artificial Intelligence, Philosophy, and Economics, Dagstuhl Seminar 05321, 2005.[22] H. Katsuno, A. Mendelzon, On the difference between updating a knowledge base and revising it, in: J.F. Allen, R. Fikes, E. Sandewall (Eds.),Proceedings of Principles of Knowledge Representation and Reasoning (KR), Morgan Kaufmann, 1991, pp. 387–394.[23] H. Katsuno, A.O. Mendelzon, Propositional knowledge base revision and minimal change, Artificial Intelligence 52 (3) (1991) 263–294.[24] A.M. Keller, M. Winslett, On the use of an extended relational model to handle changing incomplete information, IEEE Transactions onSoftware Engineering SE-11 (7) (July 1985) 620–633.[25] S. Konieczny, R.P. Pérez, A framework for iterated revision, Journal of Applied Non-Classical Logics 10 (3–4) (2000).[26] S. Konieczny, R.P. Pérez, Dynamical revision operators with memory, in: S. Benferhat, E. Giunchiglia (Eds.), Ninth International Workshopon Non-Monotonic Reasoning (NMR), 2002, pp. 171–179.[27] D.J. Lehmann, Belief revision, revised, in: C.S. Mellish (Ed.), Proceedings of the International Joint Conference on Artificial Intelligence(IJCAI), 1995, pp. 1534–1540,.[28] A. Nayak, N. Foo, M. Pagnucco, A. Sattar, Changing conditional belief unconditionally, in: Proceedings of the Sixth Conference on Theoret-ical Aspects of Rationality and Knowledge (TARK), Morgan Kaufmann, 1996, pp. 119–135.[29] A. Nayak, P. Nelson, H. Polansky, Belief change as change in epistemic entrenchment, Synthese 109 (1996) 143–174.[30] A. Nayak, M. Pagnucco, P. Peppas, Dynamic belief revision operators, Artificial Intelligence 146 (2) (2003) 193–228.[31] A. Nayak, Iterated belief change based on epistemic entrenchment, Erkenntnis 4 (1994) 353–390.[32] H. Rott, A nonmonotonic conditional logic for belief revision, in: A. Fuhrmann, M. Morreau (Eds.), The Logic of Theory Change, Springer,Berlin, 1991, pp. 135–183.[33] H. Rott, Coherence and conservatism in the dynamics of belief. Part i: finding the right framework, Erkenntnis 50 (1999) 387–412.[34] H. Rott, Two dogmas of belief revision, Journal of Philosophy 97 (9) (2000) 503–522.[35] H. Rott, Coherence and conservatism in the dynamics of belief II: iterated belief change without dispositional coherence, Journal of Logic andComputation 13 (1) (2003) 111–145.[36] W. Spohn, Ordinal conditional functions: a dynamic theory of epistemic state, in: W.L. Harper, B. Skyrms (Eds.), Causation in Decision: Belief,Change and Statistics: Proceedings of the Irvine Conference on Probability and Causation, vol. II, Kluwer Academic Publisher, Dordrecht,1988, pp. 105–134.[37] W. Spohn, A reason for explanation: explanations provide stable reasons, in: W. Spohn, et al. (Eds.), Existence and Explanation, KluwerAcademic Publisher, Dordrecht, 1991, pp. 165–196.[38] M.-A. Williams, Two operators for theory bases, in: Proceedings of the Australian Joint Artificial Intelligence Conference, World Scientific,1992, pp. 259–265.[39] M.-A. Williams, Transmutations of knowledge systems, in: J. Doyle, E. Sandewall, P. Torasso (Eds.), Principles of Knowledge Representationand Reasoning (KR), Morgan Kaufmann, 1994, pp. 619–629.