ELSEVIER Artificial Intelligence 87 ( 1996) 295-342 Artificial Intelligence A probabilistic framework for cooperative multi-agent distributed interpretation and optimization of communication Y. Xiang” Department of Computer Science, University of Regina, Regina, Sask., Canada S4S OA2 Received September 1995 Abstract Multiply sectioned Bayesian networks for single-agent interpretation systems are extended into a framework for cooperative multi-agent as a Bayesian subnet. We show that the semantics of the joint probability distribution of such a system is well defined under reasonable conditions. systems. Each agent is represented distributed Unlike in single-agent systems where evidence is entered one subnet at a time, multiple agents may acquire evidence asynchronously in parallel. New communication operations are thus pro- posed to maintain global consistency. It may not be practical to maintain such consistency con- stantly due to the inter-agent “distance”. We show that, if the new operations are followed, between two successive communications, answers to queries from an agent are consistent with all local evidence, and are consistent with all global evidence gathered up to the last communication. During a communication operation, each agent is not available to process evidence for a period of time (called @line rime). Two criteria for the minimization of the off-line time, which may commonly be used, are considered. We derive, under each criterion, the optimal schedules when the communication is initiated from an arbitrarily selected agent. 1. Introduction Probabilistic a single-agent representation, reasoning paradigm. That updates in Bayesian networks (BNs) , as commonly applied, assumes is, a single processor accesses a single global network as over the network variables distribution the joint probability * E-mail: yxiang@cs.uregina.ca. 0004-3702/96/$15.00 Copyright @ 1996 Elsevier Science B.V. All rights reserved. SSDIOOO4-3702(95)00110-7 296 E Xumg/Arrificial Intellipvzce X7 (1996) 295-342 evidence becomes available and answers queries. Concurrency, marily aims at performance among multiple inference concurrent element junction and decentralization agents with multiple perspectives. The resultant is thus “ftne grained”, e.g., a node in a BN [ 151 or a clique to BNs, pri- [ 9,151, but not at modeling individual tree representation of a BN [ 91. as applied of control in the The single-agent paradigm is inadequate when uncertain are specialized to be addressed. A multi-agent (elements elements of a system between which temporal or semantic issues special each agent domain computational system’s goal cooperatively. that need is an autonomous accesses knowledge, intelligent some external reasoning differently) is performed by there is some “distance”, which may be spatial, [ I]. Such systems pose is thus required where its own partial some the subsystem. Each agent holds source and consumes information to achieve view resource. Each agent communicates with other agents Distributed artificial intelligence (DAI) addresses such “large-grained” coordinating multi-agent in DAI, e.g., blackboard systems [ 51, contract nets analyzing approaches [ 81 are essentially probabilistic approach in DAI. logic based. To our best knowledge, little has been reported the problems systems of designing and [ 2,6]. Main stream [ 31 and open systems to explore the probabilistic the problem of distributed in DAI. As defined originally system accepts evidence of objects and events reports our pilot study on applying reasoning. We address This paper erative multi-agent subclass of problems interpretation level descriptions system cation of all evidence include sensor networks, medical diagnosis by multiple complex artifacts and distributed of cooperative or self-interested paper. is needed when sensors to a centralized for collecting from some environment in the environment. A distributed evidence are distributed, interpretation and communi- site is undesirable. Examples of such systems of systems may consist in this agents image interpretation. Multi-agent agents. We consider only cooperative trouble-shooting specialists, approach interpretation, to coop- a [ 121, an and produces higher by Lesser and Erman inference for single-agent oriented and modular knowledge is based on multiply sectioned Bayesian networks Our representation which were developed and more efficient a natural extension the semantics of the joint probability distribution of a cooperative multi-agent well defined under that are used to maintain that optimize conditions. We propose new communication consistency. We derive communication the time efficiency of the communication. [ 181. We demonstrate into a multi-agent that the modularity of MSBNs allows In particular, we show that is system operations schedules representation, framework. reasonable inter-agent (MSBNs) reasoning [ 191, Section 2 briefly extension is represented semantic agent source, gathers are conditionally common distribution belief of every agent introduces BNs and single-agent MSBNs. Section 3 presents of single-agent MSBNs subnet to multi-agent MSBNs. Each cooperative that consumes its own evidence and can answer queries. When agents are cooperative, its own computational as a Bayesian re- the independent given the intersections of their subdomains and have a initial belief on their of the multi-agent intersections, system then it is shown that a joint probability is uniquely defined and is consistent with the in the system. Y Xiang/Artijicial Intelligence 87 (1996) 295-342 291 Unlike single-agent systems where evidence is entered one subnet at a time, multiple in parallel. Section 4 discusses consistency- in the system will be globally consistent that arise from the extension. Section 5 adds new belief propagation issues to the set of single-agent MSBN operations agents may acquire evidence asynchronously related erations show that agents are performed. Inter-agent vent constant maintenance operations from an agent are consistent with all local evidence gathered tent with all global evidence gathered up to the last communication. an experimental distributed after the proposed operations cost may pre- consistency. We prove that when the proposed the answers to queries so far and are consis- Section 6 presents op- communication. We how a multi-agent MSBN may be used and the associated communication demonstration task. “distance” of inter-agent are used, between communications, two successive for inter-agent to perform a interpretation operation, each agent is not available time). Such non-availability the length of the off-line to process new evidence restriction on imposes time should be minimized. During a communication for a period of time (called @-line applications. Therefore, time-critical Section 7 defines two criteria commonly multi-agent all agents we abstract the factors the activities during that can be manipulated in the system. To facilitate be used. One is based on the total length of the off-line system. The other is based on the average for the minimization length of the off-line the study of the optimal communication of the off-line time which may time for the entire time across schedules, into a graphical model, and identify the communication in optimizing these schedules. Section 8 reduces scheduling the communication subproblems the duality of the two subproblems. the optimal schedules be derived by solving only one of the subproblems. Section 9 the communication the selected is initiated time when the communication into two independent from an arbitrarily result allows that yield schedules criterion, This for each minimization and establishes communication derives, minimum agent. off-line Section 10 discusses some general issues related to this work. Our presentation as- sumes a general terminology of graph theory. 2. Multiply sectioned Bayesian networks 2.1. Bayesian networks is a triplet A BN [9,11,13,15] (N, E, I’). N is a set of nodes. Each node is labeled with a variable associated with a space. We shall use “node” and “variable” interchange- ably. Therefore, N represents a problem domain. E is a set of arcs such that D = (N, E) is a directed acyclic graph (DAG) . We refer to D as the strucrure of the BN. The arcs signify directed dependencies the linked variables. For each node Ai E N, the strengths of the dependencies on the values of Ai’S parents. For probability any three sets X, Y and Z of variables, X and Y are said to be conditionally independent given Z under probability distribution P if P (XI YZ) = P (XI Z) whenever P (Yz) > 0. in BNs is that a variable The basic dependency between from its parent nodes 7~ are quantified by a conditional distribution p(Ail~i) of Ai conditioned is conditionally assumption embedded 298 L Xiung/Artijicid Intelligence 87 (1996) 295-342 bcps pn apb mf7_a mmcb mcmp mupl Fig. MSBN I. Left: an example MSBN in the left. Right: a general hypertree for neural muscular diagnosis. Middle: structured MSBN. D’ @ D -:ji ND< D the hypertree organization of the independent probability of its non-descendants given its parents. This assumption allows P, the joint distribution (jpd), to be specified by the product P = nip(A;l’rri). 2.2. Single-agent oriented MSBNs To make the paper self-contained, we briefly introduce the single-agent oriented MS- BNs [ 18,191. An MSBN M consists of a set of interrelated Bayesian subnets. Each subnet represents in a large problem domain or total universe. Each subnet intersection of a subdomain dependencies shares a non-empty between each pair of subnets satisfies set of variables with at least one other subnet. The the d-sepset condition. Definition 1 (d-sepset). (N’ U N’, E’ U E2) and D’ Let D’ = (N’, E’) is a DAG. The intersection (i = I, 2) be two DAGs such that D = I = N’ n N* is a d-sepset between D’ if, for every Ai E I with its parents Z-; in D, either Z-; C: N’ or n-; C N2. It can be shown that, when a pair of subnets are isolated trunk lesion them conditionally renders for diagnosis of median nerve lesion upper electromyography each pair of subnets of subnets of M may be different. Subnets of M are organized (Pxut). and nerve conduction is {Medn, Cts, Pxut}. from M, their d-sepset independent. Fig. 1 (left) shows the structure of an MSBN (Cts) and plexus carpal tunnel syndrome for clinical, respectively. The d-sepset between In general, d-sepsets between different pairs ’ It consists of three subnets D’ (i = 1,2,3) subdomains, (Medn), into a hypertree is a d-sepset between a pair of subnets. A hypertree of M. Each hyperlink M ensures independent. The subnets (middle). Fig. 1 (right) depicts a general hypertree structured MSBN. in Fig. 1 (left) can be organized that each hyperlink into the hypertree structure. Each hypernode is a subnet structured the two parts of M that it connects conditionally render in Fig. 1 Each subnet in M may be multiply of nodes), e.g., D’. hypertree structured M is converted structure as its run time representation. In order to perform connected (more inference more efficiently than one path between a pair the F of the identical is a junction (JJF) in the hypertree in each subnet, into a linked junction forest Each hypernode ’ The example is taken from a fraction of PAINULIM 1 18 1 with modification. Z Xiang/Art@cial Intelligence 87 (1996) 295-342 299 Fig. 2. A linked junction forest of the MSBN in Fig. 1 (clique tree (JT) and triangulation covers tree) converted [9,11]. Each hyperlink from the corresponding subnet through moralization is a set of linkages which the d-sepset between the two corresponding in the hypertree subnets. The need for linkages can be understood as follows: when evidence in one subnet/JT, distribution The efficiency Linkages are defined, over linkages, which Linkages it can be propagated to an adjacent I. This may not be efficient over the d-sepset can be improved by exploiting form a decomposition the probability distribution of I based on conditional the conditional JT by passing if the cardinality of I is large. I. independence. Once linkages over I can be passed by passing distributions this later in this subsection. independence within illustrate is more efficient. We will are obtained as follows: is obtained the probability Definition 2 (linkage). Let I be the d-sepset between JTs T” and Tb in an LJF. First remove recursively every leaf clique C of T” that satisfies one of the following conditions. (1) cnr=0. (2) C f? I is a subset of another clique. the resultant graph by T’. recursively remove either a node from a clique of T’ or a clique from T’ as If a node x $ I is contained If a clique C becomes a subset of an adjacent clique D after (a), union C into D. The resultant in a single clique C, remove x from C. is a linkage tree Ya+’ of Ta relative to Tb. Each clique I of Ya*b is a I is the linkage host of 1. linkage from T” to Tb. The clique of Ta that contains a linkage It can be shown that a linkage linkages can be performed correctly tree is a JT. It can also be shown that belief propagation if and only if Ya*’ and Yb*’ between JTs through are identical. The MSBN in Fig. 1 (left and middle) are converted three subnets D’ (i = 1,2,3) (shown as heavy links) between pairs of JTs are defined. The linkage the clique C8, and then removing to T’ is obtained by first removing into the LJF in Fig. 2. The can be converted into three JTs T’ (i = 1,2,3). Then linkages tree of T* relative the variable apb Denote Then follows. (a) (b) 300 l! Xiun~/Artijiciul Intelligence 87 (1996) 295-342 from the clique C6 and removing prt from C7. We then obtained two cliques {Cts,Medrz} and {Pxut,Medn} T*. Their Parallel linkage hosts in T* are C6 and C7, and their hosts in T’ are C2 and C4. to the structure conversion, tree with each of which is a linkage between T’ and the conditional probability the linkage probability of M are converted in JTs of F such that a joint systenz belief of F, assembled equivalent to belief tables (unnormalized to the jpd of M. The belief table of a JT T is tables stored at nodes of cliques is tables, distributions) from the belief ni Bcg(Ct) BT(N) = njBs,(q) (1) is the belief table of clique separator Sj. Subscripts are used to denote where N is the set of domain variables of T, Bc,(Ci) and Bs, (5’i) object that a belief table is associated with. Let B,(I) assembled from belief tables of linkages fashion as Eq. ( I ) (recall takes the form table of clique C; the I tree in the similar linkage tree is a JT). The joint system belief of F be the belief table of a d-sepset in the corresponding that a linkage is the belief B F (u) = IIiBT’(N’) n; B,j(l.i) ’ (2) tables are unnormalized probability where U = Ui N’ is the total universe. Since belief distributions, B,v( U) is proportional to the jpd of F, pF(u) = I-p,(I.i) n, PT~(N’) ’ where P denotes a probability distribution. (3) To answer queries by efficient local computation if all JTs are internally is locally consistent the same set of variables, different belief distribution. respect boundary consistent. F is boundary consistent to their d-sepset. F is globally consistent in F, consistent, it must be consistent. F onto the identical marginal if each pair of adjacent JTs are consistent with and i.e., when marginalized locally consistent in a JT yield if it is both tables A set of operations are developed to achieve consistency during evidential we assume in the above reference. that F is initially globally consistent. Details on initialization reasoning: can be found After evidence is no longer globally defined and CollectEvidence is entered into a JT, the JT is no longer consistent. UnifyBelief internally brings a JT internally and F It is (an outward belief propagation within a JT) proposed by Jensen et al. [9]. consistent. consistent (an inward belief propagation) in terms of DistributeEvidence ). Let T be a JT in an LJF. When Unif yBelief is initiated Operation 3 (Unif yBelief in T, the following are performed: ( 1) A clique C is arbitrarily (2) CollectEvidence (3) When C has finished CollectEvidence, is called selected. in C. DistributeEvidence is called in C. Y XianglAr@cial Intelligence 87 (1996) 295-342 301 For example, suppose Unif yBelief is performed first belief propagates in T3 of Fig. 2 and the clique C9 is from C 11 to C 10 and from to C 10, and then belief propagates from Cl0 to C9. During DistributeEvidence, from C9 to ClO, and then from Cl0 to Cl 1 and C12. This brings selected. During CollectEvidence, Cl2 first belief propagates T3 internally When consistent. is available evidence relative to variables in a JT, it is entered EnterEvidence. relevant cliques with the evidence again by calling Unif yBelief. EnterEvidence enters evidence by multiplying function and then brings the JT internally the belief For example, suppose median motor conduction block by tables of consistent (mm& = true) in the variable the belief such that all incompatible with the observation will be set to 0. In Fig. 2, the clique C9 contains table of C9 will be modified is observed study of a patient. the nerve conduction mmcb. During EnterEvidence, configurations Then UnifyBelief of {Medn,mmcb} in C9. is called between the belief of a JT T relative It is defined adjacent Belief propagation JTs in F are performed with UpdateBelief. It updates level operation AbsorbTbrougbLinkage. consistent. Given a linkage and its two hosts (one at each JT involved), AbsorbTbroughLinkage updates the belief other host over the linkage. table of one host by the marginalization in terms of a lower to an adjacent of the belief JT, and brings T internally table of the ). Let L = {LI, . . . , Lk} be the set of linkages between Operation 4 (UpdateBelief JTs Ta and Tb. Let Cf and Cf be the linkage hosts of Li in Ta and Tb, respec- tively. When UpdateBelief to Tb, the in each CF to absorb from following C,!’ through Li. After each AbsorbThroughLinkage, is called in C,!. are performed: AbsorbTbroughLinkage DistributeEvidence in To to update its belief is called is called relative In Fig. 2, suppose UpdateBelief is called from C2 to C6 through in C6. Then belief propagates followed by DistributeEvidence in T* to update the linkage {Cts,Medn} its belief relative to T’. followed by from C4 to C7 through the linkage in the in C7. Note that if all variables {Medn, Cts, Pat} has three possible values, then the probability the d-sepset has 27 - 1 = 26 independent values. By exploring the d-sepset, we only pass the distributions distribution the conditional over the two linkages First, belief propagates DistributeEvidence {Pxut,Medn} d-sepset over independence within with 8 + 8 = 16 values. DistributeBelief initiated at a JT T causes an outward belief propagation in F. If F was globally in T followed by DistributeBelief consistent before evidence is entered to T, then after EnterEvidence Operation 5 (DistributeBelief). When DistributeBelief ( 1) T’ updates (2) T’ calls DistributeBelief its belief relative is called in T, F is again globally consistent. Let T’ and Tj be two adjacent JTs in an I_JF. in T’ by Tj, the following are performed: to Ti by UpdateBelief. in all adjacent JTs except Tj. 302 E Xiun~/Ar/ljicitrl Intelli,qcncx X7 (I 996) 295-342 In Fig. 2, suppose T’ has acquired new clinical evidence. When DistributeBelief is called and T3 to DistributeBelief. in T’ (the caller is the system and only the step (2) The two JTs will then update their belief relative is performed), it calls T* to T’. is only needed for initialization. Global by a more efficient operation In single-agent MSBNs, DistributeBelief during evidential reasoning is maintained After allows the hyperpath the user has entered multiple pieces of evidence the user to shift attention to another into a JT, target JT. It maintains from the current JT to the target JT. along in the hypertree consistency ShiftAttention. ShiftAttention consistency tAttention). Operation 6 (Shif form a simple path in the hypertree from p shift attention to T'-' . belief relative Let 7’0, T' , from p to Tj, for i = I to j, UpdateBelief , TJ be a subset of JTs in an LJF that to its is called is called in T’ to update to T,‘. When ShiftAttention In Fig. he wants then to D* and ShiftAttention, I (right), to shift attention suppose to D”. Note the user currently to D’, ShiftAttention that subnets D4,. focuses his attention on D’ /T’.’ If will propagate belief from D’ this , D7 are not computed during hence the efficiency over DistributeBelief. JT, enter some evidence, query A user may start with a particular shift to another JT, and repeat these actions for a number of times. Note that, in such into the current JT. It can be shown level in to queries provided by the JT is consistent with all the evidence attention a single-agent that, after ShiftAttention, the sense that answers entered so far in the entire WE We will come back to this point in Section 4. the target JT is always consistent is always entered context, evidence at the global the subnet, 3. Representing multiple agents in MSBNs In this section, we extend systems for distributed the single-agent MSBNs interpretation to cooperative and homogeneous and consider the semantics of such a multi-agent system. 3.1. The semantics ofjoint system belief As described ing each subdomain with a subnet. From the viewpoint of reasoning represents [ 181 consists of three subnets which represents a neurologist’s tives of the neuromuscular perspectives. The jpd of the MSBN represents in Section 2, an MSBN represents a large problem domain by represent- agents, an MSBN the coherent multiple perspectives of a single agent. For example, PAINULIM three different perspec- clinical, EMG and nerve conduction the subjective belief of the single agent. its own perspective system, each agent can be considered as holding of the domain. This partial perspective may be over a specialty, over a period of time, In a multi-agent diagnostic domain: ’ The diagram in fact shows the MSBN M, not the LJF F. We abuse the illustration a bit since M and F share the same hypertree structure. Z Xiang/Artijciul Intelligence 87 (1996) 295-342 303 systems, with a modification or over a spatial area. The modular to multi-agent one agent’s multiple perspectives of a domain, a multi-agent MSBN represents multiple agents in a domain each of which holds one distinct perspective of the domain. Each subnet corresponds of MSBN allows a natural extension instead of representing to one such perspective. of the semantics: representation arises: What is the interpretation it represent? We will first discuss of the jpd of such a system? then intuitively this issue and A natural question Whose belief does justify our interpretation a computer its components formally. system. are commonly Consider information It processes to determine in designing since each vendor follows a set of protocols the internal structure of a component as if it follows a single will. How much knowledge as a whole, even coherently supplied by different vendors. This coherence the functional follows a common protocol, a vendor and the entire system to the though is achieved interface of a component. As long as the interface has the freedom will function designer of the system? He only needs to know the functional and not their internal including complex and data abstraction team work computer networks structures. all vendors who supply components systems [ 71. Layered [ 161. are commonly approach is necessary interfaces of components is built by a group of designers as well as the system designer. Building in such a way has become a common practice. Procedural abstraction software systems by [ 171 and applied is commonly to develop complex used in operating In a sense, the system systems and he trusts is also giving that they meet, the doctor the patient will completely. Suppose he can. When for diagnosis. After the patient will and Next consider a human “system” consisting of a patient and a family doctor. Suppose the doctor’s and to a ther- the is reached, (the doctor uses the therapy). Situa- from a specialist. (of medical exper- that the patient has no medical knowledge of his problem expertise treatment know apy which symptom himself the system as a whole demonstrates to reach tions Who tise) . like is the designer of this system? the doctor does not experience how the diagnosis tell all that the doctor needs he will prescribe this are not uncommon when a user the doctor follow. Even the patient does not understand a coherent belief on symptoms reaches a diagnosis, the best diagnosis is seeking advice It’s the demand the diagnosis) the diagnosis (the patient and supply follows though and The above two scenarios illustrate of different agents may demonstrate of each individual An agent must trust the information what he really believes. This is possible a common goal (versus self-interested). a system consisting that, under certain conditions, a coherent joint belief or will consistent with that is that agents are cooperative. supplied by others and must also supply others with towards if all agents in the system are working agent. Clearly one of the conditions Another condition is conditional independence. It is not necessary to pass to other components to supply others with all that he believes. A component needs can and should hide other details In structured parameters. How the mapping specified regarding how the supplied is obtained. a procedure the input and output from input to output is performed needs not be concerned in the protocol, information header only specifies the information in a complex programming, and for each agent system only it 304 Y Xiung/Artflcial Intelligence 87 (1996) 295-342 agent engaged by the caller of the procedure. A doctor only needs to inform and the therapy. He does not need to explain how the diagnosis to a particular information perform is irrelevant, namely, on that certain amount of information. the patient of the diagnosis In general, task, there is usually a certain amount of to about how other agents think of other agents conditioned from other agents, once exchanged, the agent is conditionally its own task. Beyond to help the agent that is sufficient the information in a particular that amount, independent is reached. Let us formalize the above idea by first introducing a third condition. Definition 7. Let N = A U B be a problem domain such that A n B $8. Let Q(A) and and R(B) are said to be con- R(B) be probability represents marginalization. sistent if CAiB Q(A) = CBIA R(B) , where the summation over A and B. Q(A) distributions In other words, Q(A) and R(B) are consistent if they yield the same distribution when marginalized The following to A n B. lemma is due to Dawid and Lauritzen. We reformulated in our notation. Lemma 8 (see [4] ). Let N = A U B be a set of variables. Let Q(A) and R(B) be probability distributions over A and B and let them be consistent. Then there exists a unique probability distribution P(N) =Q(A)R(BIAnB) wheneverR(AnB) >0 such that P(N) = Q(A)> (1) &\A (2) CN\B P(N) = R(B) and (3) A is conditionally independent of B given A n B under P. agents. Suppose the subdomain B. Let A and p can only perceive let (Y and p be two cooperative Now subdomain of (Y be represented by Q(A) the other agent’s belief on the intersection A f? B is sufficient cr and /3. Suppose Q(A) and R(B) belief Q( A f? B) = R( A n B) on the intersection A n B. Then, according there exists a unique probability and R(B) A n B. the belief and that of p be represented by R(B). Suppose knowing the tasks of i.e., the two agents share the same to Lemma 8, (Y can only perceive the subjective that is consistent with both Q(A) of A and B conditioned and that it satisfies distribution P(N) the conditional are consistent, to coordinate independence on to multi-agent Coming back to our extension of MSBNs systems, suppose we form an MSBN M with the two agents CY and p. Suppose M consists of two subnets Sa and SD over the subdomains A and B such that their d-sepset of S” corresponds boundary consistency When M is globally Lemma 8. to p’s belief. The of the two agents’ belief. in to LY’S belief and the distribution of Sfi corresponds of M corresponds consistent, is A f? B. Now the distribution the jpd defined by Eq. (3) to the consistency is identical to P(N) Dawid and Lauritzen have generalized Lemma 8 to the case of more than two distri- butions. We reformulated in our notation as follows: E Xiang/Artijicial Intelligence 87 (1996) 295-342 305 tree and Ct be Theorem 9 (see [ 41) . Let N be a set of variables. Let T be a junction a clique of T such that Ui Ct = N. Let Qc, (Ci) be the probability distribution over the clique Ci such that distributions in T are consistent. Let Sj be a clique separator in T and Qs, (Sj) be the distribution over Sj computed from the distribution of any one of its adjacent cliques. Then there exists a unique probability distribution for each pair of adjacent cliques such that ( 1) for each clique Ci, xNiCi PT( N) = Qc,( Ci) and (2) cliques Ct and Cj with for each pair of adjacent conditionally independent of Cj given Sk under PT. their separator Sk, Ci is According to Theorem 9, if we organize a set of cooperative agents in the hypertree are conditionally such that adjacent agents then the jpd of the MSBN defines a coherent that a multi-agent MSBN can be constructed builds one computational of MSBNs provides between intersection subnets how belief propagation the guidance subdomains should be a d-sepset, should be performed. as how the agents should be conditionally the overall organization agent based on its own expertise into an MSBN independent and consistent, joint belief among all agents. This implies by multiple developers. Each developer in a subdomain. The theory the the interface of should be a hypertree, etc., and should be connected, independent, i.e., 3.2. MSBNs ensure disciplined communication One might still wonder what difference same set of agents without organized others by sending messages and treating messages It should be clear that the belief propagation a multi-agent MSBN makes compared to a into an MSBN. Can each agent cooperate with received as evidence? in MSBNs performed by UpdateBelief is in fact message passing. The messages are the probability distributions However, message passing First, the distribution is disciplined. on the entire d-sepset between a pair of subnets, in an MSBN over linkages. in the form of is not the other agent such that a coherent is also not allowed since less information it would not be sufficient tables over all linkages, must be passed each time. Passing belief allowed since joint system belief can be warranted. Passing more information it is useless. More in an MSBN must flow along importantly, messages to inform and Shif tAttention. an agent cr may send a message Initially is possible. lated fashion as in DistributeBelief sequence of events /3 based on a piece of evidence. After updating an agent y. After updating message belief and count problem the same evidence if all agents are deterministic or logical. However, from y is based on the same evidence originated its belief, y may send a message the hypertree Otherwise, in a regu- the following to an agent to the its causes no it will create false belief with to CX. Not knowing from LY, (Y will update twice. Such circular evidence propagation its belief, p may send a message 306 Y Xiang/Art$cial lnrellipm 87 (1996) 295-342 Fig. 3. An artifact consistmg of five subsystems support if agents’ knowledge no evidential of circular evidence propagation reasoning hypertree structure of MSBNs and the way DistributeBelief operate ensures in probabilistic that no circular evidence propagation occur among agents. is uncertain or probabilistic. The problem [ 151. The is discussed in and ShiftAttention 3.3. Illustration of multi-agent MSBNs We use two examples First, to illustrate let us consider monitoring the above general discussion. or trouble-shooting a complex in Fig. 3. The system is made of five subsystems UO-lJ4. The external artifact system as shown input variables of the system are a, b, c, h, m, Y and w, and the external output variables are by different u, 1, y and z, as shown in the figure. Suppose subsystems are manufactured vendors. Each vendor may build a computational the knowledge of the functional structure of the subsystem. Each agent subsystem. To monitor &he entire artifact system, we can form a multi-agent let these agents cooperate. that encodes and the faulty behavior of parts and of the internal is capable of monitoring or trouble-shooting the corresponding system and agent Suppose external inputs are independent of each other. Then each computational agent (not in Fig. 3). Finally, agents must agree on a prior distribution over their interfacing of other agents given the variables the agent to others that connect there lJ1 receives is no feedback between input from UO, the agent for Ul simply accepts subsystems of the input variables the agent set by the agent for U2 simply accepts for UO. Since U2 receives the prior distribution set by the other two agents. Now all the semantic conditions as is the case in this the prior input of the required the independent and consistent) are met and we can organize is independent labeled variables. We assume example. Since distribution from both UO and Ul, input variables (cooperative, agents into an MSBN. The above illustration conditional is independent of the particular application domain of an artifact system. To make the example more concrete, we fill each box of Fig. 3 with a digital circuit as in Fig. 4. Digital circuits are used simply because to understand the MSBN, in Fig. 3 is needed and only the knowledge of the internal although from vendor its faulty behavior may vary to vendor. For example, U1 and U2 may be supplied by different vendors. the example can be safely assumed. Note that, in integrating the knowledge of the interface of each circuit as shown the function of each gate is commonly is not necessary. Furthermore, structure of each circuit the reader’s knowledge defined, L Xiang/ArtiJicial Intelligence 87 (1996) 295-342 307 Fig. 4. A digital system consisting of five circuits. U? Fig. 5. Left: the computational the hypertree organization of the five agents as an MSBN. agents in the form of Bayesian subnets for the five circuits in Fig. 4. Right: is replaced by another with the same functional faulty behavior, but an AND gate in U2 it is faulty. The vendor of each circuit, not If interface but to encode such knowledge. the corresponding agent without disturbing is in the best position An AND gate in Ul may have the stuck-at-0 may output correctly 40% of time when the designer of the artifact system, a circuit from a vendor from a different vendor, we simply the rest of the MSBN. The new MSBN will still perform (left) shows the five computational circuits the designer of the MSBN? There and the one who integrates agents. Such a multi-agent users each evidence entering interacting with one computational in Fig. 4. Fig. 5 (right) and queries. replace agents shows the hypertree organization is a group of them including in the form of Bayesian agent. The interaction subnets inference coherently. Fig. 5 for the five of the MSBN. Who is those who build agents system may be used by multiple consists of local 308 Y: Xiun~/Artificial Intelligence 87 (I 996) 295-342 Our second example extends that by Lauritzen and Spiegelhalter [ 1 I] for a single- agent system: (6) may be due to tuberculosis (p). A recent visit to Asia (v) Example 10. Dyspnoea chitis is a known information, from a radiology L: X-ray (x) and laminagraphy test (p) and biopsy (L) or bron- (5) risk factor for both r and L. After an initial diagnosis based on the above lab tests to further discriminate tests for r and lab and a biology tests as well: sputum r and L, a clinician may request lab has two relevant the chances of 7, while smoking between lab. Radiology lab has two relevant (a). Biology lung cancer increases (0). (r), clinical, involves In order expertise information radiological in a subdomain This fictitious example three medical subdomains: to diagnose a patient with dyspnoea, biological. domains may be needed to help gather relevant cisions. Each medical practitioner a biologist) may be assisted by a computational domain helping him or her diagnose and communicate with other practitioners diagnosis. As we argued to ensure independent performed. This agent may demand assistance an MSBN is formed on the fly. This However, as is proposed and commonly anism a mechanism, this paper. and from all three sub- (cooperative). A human decision maker often needs assistants from other decision makers and to help make de- (either a doctor or a radiologist or agent (a BN) specialized on the sub- during in Section 3.2, these agents should be organized as an MSBN agent may be developed by an is to be that an agents such that in the contract net [3]. the contract net does not have a mech- reasoning coherently. Multi-agent MSBNs will provide such the scope of of an MSBN to the digital system example. from a pool of potentially developer. The MSBN may be integrated before any diagnosis is the approach applied, the coherent communication. to perform probable It is also possible Each individual the dynamic cooperative formulation is beyond is similar although taken Fig. 6 shows an example MSBN for the three medical subdomains mentioned above. that only other agents’ belief on the two indepen- its own evidence of a patient, all three agents share is, the three and (consistent). lung cancer. That distribution over the two diseases (conditional to an agent the d-sepset between pairs of subnets the example, we have assumed in interpretating that, prior to observation of tuberculosis In constructing diseases matters dence). We also assume the same belief on the likelihood subnets have the same prior probability Therefore, knowledge how evidence tuberculosis conflicting. For example, the radiologist, but the result of biopsy my be negative, suggesting biologist. Each subnet will interpret edge. By communicating MSBNs three subnets will come to a coherent diagnosis. and lung cancer. Note that the evidence are needed and will be introduced its own evidence according with other subnets using (some new operations in the corresponding subdomain is (7,~). Each subnet encodes the should be used in diagnosing lung cancer from different subdomains may be to to the to its encoded knowl- the belief propagation operations of the in Section S), the opposite the result of X-ray may be positive, suggesting The LJF for the MSBN is shown linkage is created between a pair of JTs, which in Fig. 7. Note that, for this example, only a single is not the general case (compare with K Xiang/Art$cial Intelligence 87 (1996) 295-342 309 Fig. 6. A multi-agent MSBN representing three medical specialists diagnosing a patient with dyspnoea. D’: clinical subnet, D*: radiological subnet, D”: biological subnet. Fig. 7. The LJF for the multi-agent MSBN in Fig. 6. Separators between cliques are shown in solid lines. Linkages between JTs are shown in dotted bands. Fig. 2). Note also that the three agents are not only semantically be remotely located and must communicate through a computer network. different, but may also 4. Consistency issues in multi-agent MSBNs The semantic extension of MSBNs to multi-agent systems implies that all the technical in applicable to the construction of multi-agent MSBNs. constraints the construction in multi-agent systems raises new issues regarding consistency, which must be addressed. To appreciate in single- these issues, we use Fig. 1 (right) agent MSBNs. of single-agent MSBNs must be followed to review how consistency In addition, evidential is maintained reasoning For a single-agent MSBN, evidence always comes focusing on. The corresponding in Fig. 1 (right) single user is currently active. Suppose D’, then D3, and then D5. The user enters some evidence is active. Consistency or Shif tAttention the user of the MSBN of the MSBN can be maintained using either DistributeBelief (Section 2.2). towards the subdomain or subnet that the subdomain is called focuses attention on the subnet into each of the subnets as it Suppose DistributeBelief to D’ and wants evidence propagates new evidence up-to-date. Similarly, to activate D3, DistributeBelief (Operation 5) is used. As soon as the user has entered in D’. It it is to D3 and before he activates to the entire MSBN. When D3 is made active afterwards, after the user has entered evidence can be called 310 K Xiuq/Artijicial lntelligetrce 87 (1996) 295342 Ds, DistributeBelief D5. can be called in D’. It brings all subnets up-to-date including the subnet always brings DistributeBelief to be up-to-date, that the user shifts attention evidence only along the hyperpath to the next active subnet. For the above example, during is made active, which may not be necessary. Alternatively, Shif tAttention the entire MSBN globally consistent before a new subnet only since no evidence will ensures be entered elsewhere and y10 query will be issued in any other subnet. This is achieved by in the hypertree MSBN from the currently propagating the attention shift active subnet the attention from D’ to D”, belief propagates from D” to D* to D4 and then to D5. It can shift from D3 to D’, belief propagates be shown is concerned, its belief state after ShiftAttention The answers the entire MSBN, namely, the entire MSBN hyperpath consistency and hence requires is alway preferred is consistent with the evidence acquired at global consistent. Since ShiftAttention less computation in though only maintains from D’ to D* and then to D”. During [ 191 that, as far as the target subnet to queries at the target subnet to that after DistributeBelief. in single-agent MSBNs. (first D” and then D5) than DistributeBelief, the target subnet is not globally is consistent is identical level even it 4.1. How to regain global consistency? from the current subnet as well as DistributeBelief The fact that Shif tAttention tain consistency depends directly on the fact that evidence subnet and nowhere else. This can be understood by noting agate evidence in parallel. multi-agent the hypertree, none of Since sources of new information the two operations any more. Belief propa- gation must follow a different process which we shall refer to as communication. We propose agents may acquire evidence asynchronously can be used to ensure global consistency to main- are sufficient is always entered at the current that both operations prop- In a (the source of new information) and prove their properties are now scattered the corresponding system, multiple in Section 5. throughout operations outward. 4.2. What is the consistency level between communications? In a single-agent MSBN, after Shif tAttention at a global evidence consistent the entire MSBN. level. If EnterEvidence to the subnet and at a global level, i.e., answers to bring (Section 2.2) it internally to queries the newly active subnet is performed consistent, is consistent then the subnet to evidence acquired is consistent to enter is still in subsequently In a multi-agent MSBN, the situation is quite different. After evidence subnets, neither has the knowledge of the evidence entered into two different other subnet. Due to the inter-agent “distance” we may not be able to perform communication constantly. A question consistency communications agent after additional Section 5. evidence which regain global consistency, what is the consistency is acquired? We prove a theorem frequently that must be answered and the associated communication enough is, between is entered into the cost, to maintain global two successive level of each in to answer this L Xiang/Arr@cial Intelligence 87 (I 996) 295-342 311 5. Maintaining consistency through communication 5. I. Added operations for regaining global consistency As discussed the single-agent MSBN operations in Section 4, parallel evidence acquisition for maintenance in a multi-agent MSBN, we extend renders at multiple of global consistency. To belief prop- in a single JT [9, 101 to the LJF of an MSBN. Jensen et al.‘s method path (a unique path exists between any in an LJF must be performed over multiple in single-agent MSBNs with the latter problem has been solved through a single in a JT). Belief propagation the inward-outward information agents invalid regain consistency agation method propagates belief two cliques linkages. Fortunately, the operation UpdateBelief (Section 2). this idea, we add two new operations CollectNewBelief .4 CollectNewBelief causes an inward belief propagation 3 and Comnmni- calls CollectNewBelief from multiple agents (JTs) and DistributeBelief inward first and then outward in an LJF. to propagate to the entire Following cateBelief CommunicateBelief evidence obtained LJF. Operation 11 (CollectNewBelief the LJF or an adjacent the following are performed: ). Let T be a JT in an LJF. Let caller JT in the hypertree. When CollectNewBelief be either in T, is called ( 1) T calls CollectNewBelief in all adjacent JTs except caller if caller is a JT. (2) After each JT being called has finished CollectNewBelief, to the JT by UpdateBelief. is associated with JTs. with respect CollectNewBelief T updates its belief Operation 12 (CommunicateBelief). LJF F, the following are performed: When CommunicateBelief is initiated at an (1) A JT T in F is arbitrarily (2) CollectNewBelief (3) When T has finished CollectNewBelief, CommunicateBelief selected. in T. is called is associated with the LJF. DistributeBelief is called in T. Each node corresponds Example 13. Fig. 8 shows how belief propagates Belief. agents corresponds at an arbitrarily control two adjacent is initiated proceeds by first propagating terminal agents along solid arrows, and then propagating belief to an agent in the system. The link between selected agent T’. CollectNewBelief through an LJF during Communicate- to the set of linkages between them. Suppose the operation from T’ towards 3 The operation CollectBelief deals with a simpler consistency problem and can only be used for initialization. It is thus computationally less expensive than CollectNewBelief. [ 191 is similar in form to CollectNewBelief. But CollectBelief 4 The operation Belief [ 191 is similar in form to CommunicateBelief. But the former deals with a simpler consistency problem (initialization). It is thus computationally less expensive than CommunicateBelief. Initialization 312 E Xian~/Arbjicial fr~trllip~e 87 (1996) 295-342 two nodes represents Fig. 8. Belief propagation between arrows during DistributeBelief. illustrate belief propagation during CommunicateBelief the set of linkages between during CollectNewBelief, in an WE Each node represents is initiated the JTs. The operation illustrate belief propagation and solid arrows a JT. Each link from T’. Dotted terminal from ceeds by propagating time required belief propagation. agents back belief for control propagation to T’ along dotted arrows. Then DistributeBelief pro- terminal agents along solid arrows. The can usually be ignored compared with that for from T’ towards Note that CommunicateBelief T4 and T” may perform UpdateBelief during CollectNewBelief. relative be said on DistributeBelief to its terminal neighbors is operationally relative “semi-parallel”. to their terminal neighbors It is “parallel” in that in parallel in that p must perform UpdateBelief It is “semi’‘-parallel in sequence during CollectNewBelief. as well. The same can 5.2. Consistency after and between communications We answer CommunicateBelief the two questions is performed, raised the multi-agent MSBN in Section 4. First, we show that after is globally consistent. Theorem 14 (Multi-agent Let F be a globally consistent L.JF 5 con- verted from an MSBN of a hypertree structure. Let Z be a subset of JTs of F. After the following operations, F is globally consistent. ( 1) For each JT in Z, use EnterEvidence pieces of evidence consistency). to enterjnite into the JT (2) Use CommunicateBelief to communicate belief among JTs in F. We convert Proof. By assumption F is globally consistent before any EnterEvidence. into a JT 7’ and prove the theorem using Y. For each JT T’ of F with the F conceptually domain N’, union ail its cliques into one huge clique denoted by W’. Note that W’ = N’. Let the nodes of Y be those huge cliques. For each pair of adjacent JTs T’ and Tj of F 5 To be more precise, F should also be supportive and separable [ 191. We will not go into that level of technical detail here. E Xiang/Art$cial Intelligence 87 (1996) 295-342 313 in the hypertree, S’j = Z’j where Z’j is the intersection since F has a hypertree structure. connect W’ and Wj in T and denote their separator by S’j. Note that of T’ and Tj in F. The resultant graph 7’ is a JT Assign to each clique W’ in ?P a belief table Bwi (N’) = Bri( N’). Assign separator S’j a belief whose joint system belief table Bsij(Z’j) = B,zj(Z’j). The resultant r is a consistent is proportional to the joint system belief of F. to each JT EnterEvidence in T’ of F corresponds to multiplying Bw, (N’) in T by the evidence It updates the joint system belief and causes inconsistency. Communicate- function. Belief a DistributeBelief. Evidence uteEvidence in r called consists of the selection of a JT p The CollectNewBelief in Wc. The DistributeBelief in Y. The resultant 7 is again consistent. involved call UpdateBelief the JT of F and a CollectNewBelief in F corresponds is internally in F corresponds and DistributeBelief of the JT involved. Therefore, after CommunicateBelief, followed by to a Collect- to a Distrib- which maintains consistent. Both the F is lo- of F. Hence F is of 7’ implies boundary consistency After each EnterEvidence, CollectNewBelief internal consistency cally consistent. The consistency globally consistent. 0 at each agent and information CommunicateBelief involves both local computation over agents among multiple “distance”. Due to may not be performed exchange involved each agent may CommunicateBelief op- acquire multiple pieces of evidence between can be erations and may have to answer queries before level of these performed. The second question we address answers a JT is consistent with all local evidence acquired so far and is consistent with all global evidence acquired up to the last communication. two successive CommunicateBelief the next CommunicateBelief to queries. Theorem 15 shows that, between This is the best that one can expect. two successive communications, is: what is the consistency frequently. Therefore, cost the Theorem 15 (Semi-up-to-date). tree structure. Let Z be a subset of JTs of F. Let F be an UF converted from an MSBN of hyper- After a CommunicateBelief in F followed by a finite number of EnterEvidence to each JT in Z, the marginal distributions obtained in a JT T E Z are identical as would be obtained if only the EnterEvidence operations in T were performed after the CommunicateBelief. to the CommunicateBelief Pmof. We shall refer first CommunicateBelief. Among all the EnterEvidence change the BTs none of the EnterEvidence show that if a second CommunicateBelief the BTs of T will not change at all: in the theorem as the mentioned After the operation, F is globally consistent by Theorem 14. in T in T, and none of the other operations has any effect on T. Suppose operations performed outside T was ever performed. We JT, in F with T as the initiating in Z, only those performed operations performed is called CollectNewBelief called in T does not change BTs in T since F ary consistent change BTs in T is trivially after the first CommunicateBelief. true. 0 That DistributeBelief is bound- does not 314 Y Xiun~/Artijicitrl Intelligence 87 (I 996) 295-342 6. An experimental demonstration We demonstrate how communication works in a multi-agent distributed interpretation system with the digital system example Fig. 5) may be built by a different vendor who encodes (one circuit) depicted in Fig. 3. in Fig. 4. Recall that each agent (a subnet the knowledge about a subsystem in into the agent. The designer of the MSBN only needs the knowledge as In simulating the cooperation in the tive-agent MSBN, we use the following trarily chosen numeric parameters: For each external probability is 0.5. For each gate, the prior probability AND gate. it produces gate and each NOT gate, the corresponding This uniform assignment convenience. of numeric parameters the correct output 20% of time when arbi- input (a, b, c, h, m, r, w), the prior is 0.01. For each it is faulty. For each OR that it is faulty percentages are 70% and 50%, respectively. is not necessary but used simply for For all simulations, we assume h = l? m = 1, r = 0 and w = 0. We assume faulty and produce generate a complete description the following input: a = 0, 0 = 1, c = 0, that the gates G9 in Ul and G7 in U2 are incorrect output, and every other gate is normal. This allows us to external about the state of the system. We assume its subdomain with an exception that each agent only has a limited access to this true state of the world. variables of gates that the value of variable k is not accessible First, each agent can only access the values (0 or 1) of input/output within to any agent. The values any agent. For example, 4 but not values of G5, G6, G7 and G8. This assumption interpretation from the available evidence. to of gate variables the agent U2 can only access the values of g, m, n, o, p and the fact that an in the world but has to infer system cannot directly observe everything (normal or faulty) are not accessible simulates Second. the values of i/o variables are revealed order. The agent has no control over this order. For instance, U2 may receive simulation, it may receive the evidence the evidence to an agent sequentially in one simulation, in the order o = 1, g = 1 ,p = 1, m = 1, in the order p = 1, o = 1, q = 0, m = 1,. . . . in a random the agent . In another Three sets of simulations were run using an extended version of WEBWEAVR [ 181. The the probabilistic reasoning environment infrequent WEBWEAVR-II, communication third set had sults through last two sets to demonstrate rem 15. the operations from between agents. The second set had constant communication between agents. We will compare communication the first two sets to demonstrate defined in Section 5. We will compare the effect of cooperation the effect of infrequent communication first set had no and the the re- among agents from the in Theo- the results analyzed 6.1. No communicatiorz In the first set of ten simulations, the values of gates. No communication agents was to identify U2 contained each agent “observed” and inferred with other agents was performed. The goal of the faulty gates correctly as soon as possible. Since only CJl and its subdomain the faulty gates, we focused on these two agents. Z Xiang/Arti$cial Intelligence 87 (1996) 295-342 315 Table 1 Summary of the simulation results Simulation set 1 Simulation set 2 Simulation set 3 Obs (G9) Obs (G7) Obs (G9) Obs (G7) Obs (G9) Obs (G7) SimuO Simu I Simu2 Simu3 Simu4 Simu5 Simu6 Simu7 Simu8 Simu9 Aver 5 6 6 6 6 6 4 4 6 5 5.4 6 3 6 6 3 6 3 2 3 2 4 4 4.8 5 5 5 4 3 4 4.8 3 4.26 2 2 3 4.8 1 4.6 4 2 2 3 4 5 5 5 5 4 4 4 5 4 2 2 4 5 2 5 4 2 2 4 2.84 4.5 3.2 available to the agent Ul, For each agent, we first determined the belief state that it may reach when from its subdomain. This state represents = 1, p1 (GlO = nomzaflevi) = 0.993, pl (Gl 1 = nonnalleui) it had the best the agent can come up with given that it only works alone. When the com- received all the information interpretation plete set of observations was entered = 0.99 P,(G9 =fuultylevi) the agent whose to identify and pi (G12 = nomzallevi) belief are expressed by the probability values. Call this belief state the ideal state of the agent. When the complete set of observations was entered to the agent U2, the ideal state consisted of p2( G7 =fuuZtyjevi) = 0.6, p2( G6 =fuultyjevi) = 0.4, p2( G8 =fuultylevi) = 0.002 and p2( G5 = fuulty)evi) = 0.005. The agent detected did not confirm as to whether G6 or G7 was faulty since a key variable k was not observable. function of the circuit, but it was uncertain it had the posterior probabilities = 0.995. Subscripts that observations to the normal are used We then randomly generated ten sequences of observations for each of the two relevant for an agent was ten simulations. one by one using EnterEvidence. In each simulation, one sequence the number of observations in columns 2 and 3 of Table 1. For the ten simulations, agents and performed used. For each sequence, we entered observations As soon as the agent reached we stopped and recorded are listed of observations is 5.4 observations. The corresponding observation of observations interpretation the ideal belief state within a range of 10% of fluctuation, that had been entered. The results the average number to the ideal Since each the average number the to reach a belief state close for U2 is 4.0 observations. gives an indication of the efficiency when the agent has to perform is processed with some cost, i.e., time or other resources, for the agent Ul task alone. number needed 6.2. Constant communication In the second set of ten simulations, we generated randomly vations for each of the five agents. In each simulation, one sequence ten sequences of obser- for an agent was 316 if Xiung/Artijicial Intelligence 87 (1996) 295-342 from UO to iJ4, one observation was performed. We then entered used. We entered observations CommunicateBelief agent and repeated agents was entered only once. After each CommunicateBelief, state of U1 and U2. We recorded either Ul or U2 reached a belief state close to the ideal. for each agent. Then for each for a variable shared by two or more the belief each agent had made when this process. An observation the number of observations the next observation we check The ideal state for Ul was the same as before, but the ideal state for U2 was changed to p2 (G7 = fuulty(evi) = 0.999 and p2 (G6 = faultylevi) = 0.007. This much sharpened together evidence about k, which was not possible belief was obtained by agents’ pooling when U2 was working alone. are listed The results of the simulations the average number of observations in columns 4 and 5 of Table 1. For the ten to reach a simulations, per agent needed for (12 is belief state close to the ideal is 4.26 observations. The corresponding 2.84 observations. The average number decreased by 21% and 29% for U 1 and U2 due to cooperation. number for Ul 6.3. Infrequent communicatiorl In the last set of ten simulations, in the second set of simulations were used. The same experimental method was used except two observations that CommunicateBelief instead of one. was performed after each agent had made the same observation sequences exactly Each agent still reached the average number of observations the ideal belief state after enough observations had been made. The results of the simulations are listed in columns 6 and 7 of Table 1. For the ten simulations, to reach per agent needed for U2 to the ideal is 4.5 observations. The corresponding a belief state close is 3.2 observations. The average number decreased by 16% and 20% for (II and 112 is compared case, which shows still better than no communication. More observations per agent were made compared to the case of constant communication, in time. locally was not exchanged since evidence obtained to the no cooperation for Ul number that infrequent communication It should be indicated exchange due to infrequent communi- cation may cause agents to believe quite differently. For example, suppose the following observations that delays of information into the agents: are entered UO: d= I, c=O; u2: q=o, o= I; u4: w=o, x=0; Ul: h= I, g= I; u3:1=1, u=l. that g = I, U2 reasons After CommunicateBelief, Given that G6 is faulty (p2( G7 = faultylevi) = 0.035). Suppose Ul subsequently (II change believe at this moment, U2 will change Ul and U2 have pl (k = Olevi) = pz(k = Olevi) = 0.976. from both input of the OR gate G6 being 0 and concludes (p2( G6 = faultylevi) = 0.97) producing o = 1 and that G7 is normal i = 0, which makes its belief on k to p1 (k = O(evi) = 0.013. At this moment, Ul and U2 is performed to pz(G6 = fuulty(evi) = 0.02 and the value of k. If communication its belief sharply things regarding totally opposite observes Y Xiang/Artifcial Intelligence 87 (I 996) 295-342 317 its old p2 ( G7 = fuulty(evi) = 0.987. However, without communication, U2 will continue belief which is out-of-date relative to the knowledge of the entire system. (vs logic) in probabilistic reasoning Such situation is very natural x is not certain, a new observation may change since as long as its an agent’s belief on a proposition belief on x to either extreme (0 or 1) . analysis analysis In addition as possible, a sensitivity as frequently of G6. Note the above example, to perform CommunicateBelief at U2 will reveal to the value of k. Therefore, to take some action, e.g., to replace a gate that is believed the above locally. that U2’s belief on if U2 is also it may faulty, 112 may perform the premature that such exchange does not bring either Ul or U2 glob- in other agents of correctly. to the proba- than that contained to exchange in order situation may be avoided by letting each agent perform a sensitivity For the value of G6 and G7 is very sensitive responsible defer the action and wait for the next communication. a belief exchange with Ul, which will bring U2 up-to-date replacement ally consistent. Neither has the system. However, In general, bilistic in agents far away belief with adjacent efit from CommunicateBelief portunity its to ben- of others. Such exchange does not require a full scale of and thus is less expensive. We leave the exploration of this op- it may be sufficient a given agent’s belief on its subdomain in agents close to it in the hypertree it makes sense contained from agents prior to a critical and sensitive decision for U2 to interpret is more sensitive to future work. the knowledge the knowledge its subdomain for an agent it. Therefore, is happening Alternatively, information and avoid of what 7. Off-line time during communication 7.1. Off-line time of each agent During CommunicateBelief, the BT of a JT T with domain N may be changed through CollectNewBelief and DistributeBelief ( 1) (Through CollectNewBelief of CollectNewBelief) UpdateBelief (Through DistributeBelief operation UpdateBelief (2) that T performs has completed relative in the following ways: .) When each adjacent JT of T (except its CollectNewBelief, to the JT may change Br( N). in T, the is called to the caller may change the caller the operation .) When DistributeBelief relative that T performs BT(N). It follows from the proof of Theorem 14 that BT( N) should not be modified by new (during CollectNewBelief ) and in the process of Cornmunicate- in T between should not be performed (during DistributeBelief) the first UpdateBelief is, EnterEvidence operations. Otherwise, CommunicateBelief the two the global will not regain external evidence between the last UpdateBelief Belief. UpdateBelief consistency. That Since not being able to process evidence within a time interval restriction the length of the interval should be minimized. We define imposes on time-critical such interval of time as follows: applications, 318 I’. Xiung/Arti’ciul Intelligence 87 (1996) 295-342 Definition 16 (Junction CommunicateBelief UpdateBelief UpdateBelief tion is A(T) = 7 -- t. operation tree of-line time). Let T be a JT in an LJF F. During t be time when involved by T is started. Let r be the instant of time when involved by T is completed. The off-line time of T during communica- a the first the last instant of in F, the let 7.2. Factors affecting off-line time Different JTs in an LJF may have different off-line time during communication de- pending on several factors: A CommunicateBelief operation is started in an LJF at an arbitrarily which WC refer affects the off-line to as the communication time of each agent in the system. root. The choice of root is one factor selected JT, that the effect of the root on T4’s off-line to T6. T7 and T8 ( sequentially) in the figure, during CollectNewBelief, time in Fig. 8. Since T4 must then al- to T4. Afterwards, T4 must wait for the its turn relative in the rest of the system and wait for first, and to T2 first and then allow p, T7 and T8 to perform UpdateBelief T4 must perform UpdateBelief relative to During DistributeBelief, relative to perform UpdateBelief of CollectNewBelief Example 17. We consider T’ is selected as the root as shown perform UpdateBelief low T’ completion in DistributeBelief. relative T3 ( sequentially). dictates The above order of operations that TJ becomes off-line as soon as belief from T6, T7 and T8 to T4 starts. T4 remains off-line when belief further to T’ through T2 and then back to T4 from T’. T4 becomes available after be- involves 13 sequen- propagation propagates lief propagates back to T6, T7, and T8 through T4. The total process tial UpdateBelief relative in the best case, where T’ performs DistributeBelief ( 16 in the worst case). its four neighbors If Th instead of T’ is selected as the root, then T4 will be off-line to T* first among for only a period operations of time needed to perform 8 sequential UpdateBeliefs. Another factor is the order in which CollectNewBelief is performed by each agent relative to its neighbors. Example 18. Consider T7 in Fig. 8 where the root is T’. During CollectNewBelief, T” must perform UpdateBelief selected, T7 must become off-line before Th and T8, and prolonged to T6, T7 and T* sequentially. its off-line accordingly. relative If T7 is first time will be We rcfcr to the order in which multiple neighbors against, during CollectNewBelief, are selected by an agent to per- as the collection order of the form UpdateBelief agent. Similarly, we refer to the order in which multiple neighbors are selected by an agent as the distribution order of the during DistributeBelief, to perform UpdateBelief, agent, which is a third factor affecting each agent’s off-line time. L Xiang/Artijcial Inrelligence 87 (1996) 295-342 319 Example 19. Consider T7 in Fig. 8 where the root is T’. During DistributeBelief, T6, T7 and T8 must perform UpdateBelief selected, T7 can become shortened accordingly. to T4 sequentially. its off-line before T6 and T*, and available relative If T7 is first time will be The last factor JT Tk. This neighbor the time complexity time complexity performed by T’ relative and Tk) performance in T’ is generally different is the time complexity time complexity of UpdateBelief is fixed once by T’ relative of UpdateBelief by a JT T’ relative to a the LJF is constructed. Note that to Tk may not be the same as the of UpdateBelief by Tk relative to Tk involves multiple to T’. This is because UpdateBelief (the number of linkages between T’ of Unif yBelief in T’ [ 191. The time required by Unif yBelief than that in Tk. 7.3. Off-line time of a multi-agent system The difference of off-line of the entire system. We consider time across agents calls for a measurement of off-line two alternatives which may commonly be used: time the first JT in F becomes off-line during a CommunicateBelief Definition 20 (Absolute off-line time). Let F be an LJF. Let t be the instant of time when operation. Let 7 be the instant of time when the last JT becomes available again. The absolute off-line time of F is A&s = 7 - t. The absolute off-line time indicates of the system as a whole even though some agents are available earlier the non-availability than others. Definition 21 (Average on-line time). Let F be an LJF. Let A(T’) be the off-line of a JT T’ (i= off-line time of F is A,,, = (Cy,, A (T’) ) /n. time operation. The average in F during a CommunicateBelief l,..., n) The average off-line Both definitions time indicates the average non-availability of multiple agents. can be modified over a subset of JTs if availability is concerned with only the subset. 7.4. A graphical model for off-line time study Based on the above analysis, given an LJF and a chosen off-line time measurement, we may manipulate such that the off-line communication, that we can concentrate communication root, collection the communication time is minimized. To avoid distraction by unnecessary e.g., the number of linkages and the numerical belief computation, order and distribution order details of so time, we abstract on the four factors that determine the off-line in an LJF into the following graphical model. Model 22 (Graphical communication model). Given an undirected and an arbitrary node A as the root, the tree is converted to a rooted tree R. and weighted tree, 320 Y Xiang/Art$icial Intelligence 87 (I 996) 295-342 For each node X of R, if X #A, place an itr-agent at X. For each node Y of R, if Y has k children, place k out-agents at Y. The agents traverse R according to the following rules: C I ) To start with, each parent node Y with leaf children selects one child X, according to some order Oi,,( Y). Once selected. X sends Y, which takes time We,, the inward direction After one child’s to move from X to its in-agent that is the weight associated with the link (X, Y) in leaf to root). (from in-agent arrives at Y, the next child, selected according to O;,,(Y), sends its in-agent to Y. After a parent Y has received all the in-agents for selection by its own parent Z. Once selected by Z, Y sends Z. The inward movement of in-agents continues in this fashion. from its children, Y is ready to its in-agent (2) After the root A receives all in-agents completed and an outward movement A selects one child X, according from its children, starts. the inward movement is to some order O,,,,(A). A then sends one that is the weight takes time w,,,(X) out-agent associated with the link (A, X) to move from A to X, which in the outward direction. After an out-agent of A reaches the destination, A selects another child accord- to the child. The process continues ing to O,,,,(A) until all out-agents of A reach their destination. and sends another out-agent After an out-agent to O,,,,(X), according The process continues from A reaches a child X, X selects to child nodes and sends its out-agents the last out-agent in this fashion until its own children, in sequence. in R reaches its leaf destination, and the outward movement halts. The above model characterizes the CommunicateBelief operation correctly as far as the off-line time is concerned: ( I ) The original undirected tree corresponds to the WE Each node corresponds to a JT of the LJF. ( 2) The root A corresponds to the communication root. (3) The inward movement of in-agents corresponds CollectNewBelief, the belief propagation during DistributeBelief. and the outward movement of out-agents to the belief propagation during to corresponds (4) Given a parent node Y and a child node X, wi,(X) required for Y to perform UpdateBelief to the time required for X to perform UpdateBelief relative corresponds to the time to X, and wout( X) corresponds to Y. relative (5) Oi,(X) corresponds to the collection order (Section 7.2) of X, and O,,,(Y) corresponds to the distribution order of Y. (6) The time instant, when the first in-agent to the time instant when the corresponding towards X, corresponds off-line. The time instant when X corresponds for entering evidence. The interval between the off-line time of the JT represented by X. the last out-agent to the time instant when the corresponding the two instants JT becomes available thus corresponds to from a child of a node X moves JT becomes from X arrives at a child of Y. Xiang/Artijicial Intelligence 87 (1996) 295-342 321 Note that the graphical model reflects the semi-parallel pattern of communication is discussed time is to increase in Example 13. It will be seen that the key aspect of minimization the degree of parallelism by manipulating the relevant factors. that of off-line We will use the following notations: We shall say that a non-leaf node X is off at the to X. We If X is a leaf node in the rooted tree, then X is off as from the first child selected by X starts moving the instant. time instant when the in-agent use t,f( X) to denote soon as its in-agent leaves X. We shall say that a non-leaf node X is on at the time instant when its last out-agent If X is a leaf, then X is on to denote time of the from its parent. We shall say that the off-line the out-agent the instant. arrives at a child of X. We use t,,(X) when it receives node X is A(X) = t,,(X) - t,f(X). We shall call the inward movement of in-agents collection and the outward movement of out-agents distribution. For collection, we use trdy (Y) to denote the time instant when from its children and is ready for its parent a non-leaf node Y receives to select. For a leaf node Y, we assign starts. We use t,,,,,(Y) the in-agent of Y arrives at its parent to come from its parent. For the root A, we assign and Y starts to wait for an out-agent to be the instant when collection the time instant when the last in-agent to denote t&,(Y) t,,,(A) = trdv(A). For distribution, we use t,l (X) to denote to be the instant when distribution by its parent Y such that Y is about to send an out-agent t,y,, (A) instant when X receives For the root A, we assign tcpr (A) = tsel( A). the out-agent from Y (distribution starts. We use t,,,,(X) relative the time instant when a node X is selected to X. For the root A, we assign the time to denote to Y is completed). Example 23. Fig. 9 illustrates system. The figure in the left shows the rooted tree R with the root A, and leaves D, E, F and G. It also shows the in-agent and out-agents of each node, and the in-weight and the out-weight of each link. the graphical communication model for a seven-agent Fig. 9 (middle) in the rooted shows collection tree R. The collection order used is instant left to right for each parent node. At time from arrives at B at t = 5. Thus E starts to wait for distribution selection by A at the same point t = 0, E is off. Its in-agent at t = 5, and B is ready for in time. Parallel to the above activity, at t = 0, F (the left-most child) is selected by C to send its in-agent which arrives at C at t = 1. Then C selects the next child G, and the in-agent of G arrives at C at t = 4. Thus, F is waiting at t = 1, G is waiting at t = 4, and C is ready at t = 4. According to the left-to-right at A at t = 9. Then C is selected, whose selected, whose in-agent order, A selects B at t = 5. The in-agent of B arrives arrives at A at t = 11. Then D is in-agent arrives at t = 19, and collection is completed. Fig. 9 (right) shows distribution which follows collection tion order used is right to left. At t = 19, A sends an out-agent t = 26. D is on at this moment since it has no child. immediately. The distribu- to D, which arrives at C is then selected and another out-agent of A arrives at C at t = 29. The last out-agent of A arrives at B at t = 35, and A is on at this moment. Parallel to G of the last out-agent of A, at t = 29, C sends its first out-agent to the movement 322 E Xiun~/Arfijiciul Inrelligence X7 (I 996) 295-342 (5.19.19) (35.40.40) (31,35.35) (29.31.31) Fig. 9. A graphical model for communication in a seven-agent system. The in-weight of a link is indicated by an upward arrow and the associated label, and the out-weight of a link is indicated by a downward arrow and the associated label. Left: the weighted tree R rooted at A. The in-agent and out-agents of each node, as well as the in-weight and the out-weight of each link are shown. Middle: collection in R with only the in-weight of each link shown. Each node X is labeled will a triple ( rof/( X) , trdv (X), tWLI, (X) ). Right: distribution in R I(,~ (X) ) with only the out-weight of each link shown. Each node X is labeled will a triple (t,,/(X) , rC,~, ( X), which arrives at t = 3 1, and then G is on. C then sends another out-agent arrives at t = 35. At this moment, both C and F are on. its out-agent At t = 35, B sends to E, which arrives at t = 40. At this moment, both to F which B and E are on. Distribution, as well as the entire communication, is then completed. All activities during the communication nodes of the two figures. The off-line thus be calculated. For instance, A(C) = 35 - 0 = 35 and A(D) = 26 - absolute off-line A,,,.,.=(30+40+35+ are fully specified by the tuples used to label the entire system can I 1 = 1.5. The time is is A<,/,.,. = 40 - 0 = 40. The average off-line time of each node and 15+40+35+30),‘7=32.1. time of the system as a collection schedule. A distribution off-line We will refer to a specification of the timing of every node’s activity during collection, schedule time. In in its activity as soon idling cannot contribute in which is any e.g., the computer network delay, we such as the labeling of Fig. 9 (middle), is similarly defined. Our goal is to find schedules with the minimum both schedules of Example 23, we assumed as the activity positively some nodes delay practical assume to our goal, we will exclude from our consideration is possible without any delay. Since unnecessary that the delay has been modeled the belief propagation, that a node engages in the link weights. unnecessarily. On the other hand, those schedules their activities to delay if there reason The schedule time: During collection, A follows in Example 23 is not optimal. We illustrate the absolute off-line the left-to-right order, and thus waits until I = 5 when B is ready. However, another child D of A is ready at t = 0, but is selected only at t = I I. If D is selected in their activity earlier, and A will complete collection is a shorter absolute off-line to D schedule has a higher degree of parallelism: The parallel activities is that the resultant time. The difference made by switching first, both A and D can be engaged earlier. The consequence and start distribution the first child this using E Xiang/Arr$cial Intelligence 87 (1996) 295-342 323 E to B, and F, G to C in the previous schedule are now also paralleled by the activity D to A. In the remaining mization of the off-line the space, the optimization part of this paper, we use the graphical model time with an arbitrarily given communication to study the mini- root. Limited by of the root is beyond the scope of this paper. 8. Reduction of communication scheduling In this section, we reduce the communication the collection lems: two are dual problems directly into the solution of the other. scheduling and the distribution in the sense that the solution scheduling problem into two subprob- scheduling. We then show that the to one of them can be extended 8.1. Problem reduction Let us first consider at the time instant t = tl and terminate denote Aa- is independent the time interval between of At_2, the minimization of the absolute off-line t = to and terminate at t = tl . Let distribution the time interval between start start at the time instant to and t1 by Ae-1, and tl and t2 by Al-2. We have Aabs = Aa- + At-z. Since time. Let collection at t = t2. Denote min(A,b,) = min(Ao_t) + min(Al_2), where and distribution collection schedules. We can optimal collection the minimization in the left-hand schedules, the first minimization schedules, and the second in the right-hand side of the equation in side is over all possible distribution is over all collection the right-hand is over all side thus study the optimal communication schedules by studying the schedules and the optimal distribution schedules independently. Next, let us consider the minimization of the average off-line the off-line tl - t,(X) time of X during distribution. We therefore obtain time is A(X) = ton(X) -t&X> is the off-line time of X during collection, = (t,,,(X) - tl) + (tl -&f(X)). time. For each node X, Note that - tl is the off-line and t,,(X) the minimization where schedules S,,. Since have is over all possible collection schedules SC and distribution the minimization over & has no effect on the first summation, we Although of’ interval over &. We therefore obtain the minimization t,,,,(X, ) ~.- 11. Thus the second summation over SC affects the value of tl, it has no effect on the length is only affected by the minimization schedules can be studied by studying This again implies the optimal collection that the optimal communication schedules and the optimal distribution schedules independently. 8.2. Duality of collection and distribution A comparison of‘ collection two activities. Both proceed movement of in-agents outward movement ancestor may be engaged in distribution be engaged through and distribution the tree structure shows the great similarity between the pattern. Inward from the children of a node must proceed sequentially. So must of out-agents of a node to its children. Two nodes of a common in parallel. So may they in a semi-parallel in collection relative relative to their children to their children in parallel. We establish the duality of the two activities with respect to the off-line time in Theorem 24. The proof is in Appendix A. Theorem 24 (Duality). out-weight of each link identical. Let R be a weighted tree rooted at A with the in-weight and Let $1 be a distribution schedule which starts from A at td and terminates at rd. Let the distribution order of a parent node Y be denoted as Od( Y) in this schedule. A schedule S,. for collection, that starts at t,. and terminates at A at rC, can be obtained by requiring each parent node Y to follow a collection order that is opposite to O,,(Y) such that the followirlgs hold: ( I ) r, - t,, = r,l - td. and (2) for every node X, T, ~~- t,,J( X) The converse (obtaining & ,from SC ) is also true. in S,. is identical to t,,,,(X) - td in &. Given a rooted tree R, Theorem 24 implies schedule in R, we can treat w;,, for each link as w,,~, for the link, and find the optimal distribution tree. Once the optimal distribution schedule reverse converse the two activities is also correct. We therefore only need to find the optimal schedule is found, we can schedule. The for one of that, to find the optimal collection is the optimal collection order and the resultant for any given off-line in the modified the distribution time criterion. schedule L Xinng/Artificial Inielligence 87 (1996) 295-342 325 (9.10,10,10) (6.99.9) (4.9.9.9) Fig. 10. Left: a rooted Wee R for distribution. The distribution schedule is shown by labeling each node X with (tse/(X), fCpt (X), ton (X), ton( X) - td) where fd = 0. Right: R’ is obtained from R by treating Win as wm,. A collection schedule with fC = 0 is obtained from the schedule in R as described by Theorem 24. The schedule is shown by labeling each node X with (r,ff (X), rrdy (X), t,,vo,r (X), 7C - roff (X)) Example 25. Fig. 10 (left) specified. The distribution is a rooted tree R for distribution with a distribution schedule order for each parent is right to left. Fig. 10 (right) shows a rooted tree R’ identical the win of the same link. A collection schedule to R except the w,,,( of each link in R in the figure is for R’ is specified the two conditions of Theorem 24. The collection order for each parent Note that we have rd - rd = 14 - 0 in R, and T, - fc = 14 - 0 in R’. Comparing the in R and R’, we see that the last attributes that label corresponding nodes - td in R and r, - f,f( X) in R’) have identical values. becomes that satisfies left to right. four-tuples (t,,(X) 9. Optimal communication schedules In this section, we derive distribution time and distribution tree. These results can then be used to obtain duality. schedules with the minimum average off-line the optimal collection schedules with the minimum absolute off-line time, given a rooted through schedules 9.1. Distribution schedules with minimum absolute off-line time Distribution starts from the root. Consider first a rooted branching with two possible orders: 0$(A) factor 2 as shown in Fig. 11. Distribution = (I?, C) and O::,‘(A) = (C, B). tree with depth 2 and with in the root A can be performed Using O$ (A), we obtain Ai!: = max(w,,,(B) + w&C) + w,,,(F) + w&G), w,,,(B) + w&D) + w,,,(E) ). 326 Y Xiun,y/Artijicial lnrelli~ence X7 (1996) 295342 Fig. I 1. A rooted tree of depth 2 for distribution. Since distribution children of the same parent can be treated equivalently being the sum of w,,~ s of the original from a parent of leaves to the leaf children is sequential, all leaf leaf with its w,,~ as a single leaves involved. We can therefore write Al!; = max(w,,,(B) + wl,,,r(C) + VV,,~JFG). w,,I(Bl + w&DE) 1, where w,,,t( FG) = w,,,t ( F) + w,,,~( G) and FG denotes the equivalent single leaf. Similarly, using O:,$ (A), we obtain Ai!: = max(w,,,,(C) + w,,,t(B) + w&DE), w,,,(C) + w,,,(FG)). The optimal order is the one with a smaller Al-?. We claim that if w,,,~( DE) < w,,,!( FG), the condition, A:!: can be simplified then O!:)(A) is the optimal order: Under into Ai!: = w,,t (B) + w,,,~( C) + w,,~( FG) which is larger than or equal to both entries of A!!:, namely, w,,,(C) + w,,[(B) + wout(DE) and wljut (Cl + woUt (FGJ. The above result suggests (i.e., w,>,,~ (B) and w,,,,~ (C) ) are not critical, but the sums of out-weights at the bottom level (i.e., w,,,( DE) 26. The and w,,,~( FG)) proof is in Appendix A. in fact is general as is shown that the out-weights at the top level are. This result in Proposition 26 (Optimal distribution Proposition Let R be a tree rooted at A for distribution. Let the depth of R be 2. Let the children of the root be , X,,. Let the sum of out-weights of children of X; be u, such that UI < ~‘2 6 . . . < X1, in trees of depth 2). schedule . L’Il The absolute distribution off-line time Al-2 in R is minimized if the distribution order ofAisO,,,r(A)=(Xn,...,X~). Example 27. According if 0 ,,,,, (A) = (C,B,D). 3 + 6 + 7 + 0) = I6 which corresponding to right. to Proposition The minimum Al-2 is a 24% improvement 26, A 1-2 for R in Fig. 9 will be minimized is Al-2 = max(3 + (4 + 2),3 + 6 + 5, over Al-2 = 21 in Example 23. A in Fig. 12. The distribution order used is left optimal schedule is shown E Xiang/Artijcial Intelligence 87 (1996) 295-342 327 Fig. 12. R: a rooted tree for distribution. R’: R with the left-right order of nodes B, C, D re-arranged according to Owr (A) = (C, B, D) determined by Proposition 26. The distribution schedule is shown by the label ( tset (X), tcp, (X) , ton(X), ton(X) - td) at each node X. The distribution starts at id = 0. Algorithm 1 (Order arrangement for distribution). Input: A rooted tree of depth M with the out-weight of each link defined. Output: The rooted tree with the left-right order of children of each parent rearranged. begin D:=M-1 for each leaf Z of depth D do o(Z) :=o for each node Y of depth D with child nodes Xl,. . . ,X, do u(Y) := C”,l Wout(Xi) D:=D-1 while D 2 0 do for each node Y of depth D with n child nodes do arrange children of Y and index them from left to right as XI,. . . , X, such that v(Xt) < ... 6 u(X,) v(Y) := max( e,, . . . ,el) where ei = (C”,iW,,t(Xk)) +u(Xi> for each leaf Z of depth D do := 0 u(Z) D:=D-1 end Algorithm 1 and Theorem 29 generalize Proposition 1 rearranges the left-right order of children topologically order becomes 26 to an arbitrary tree rooted for each node explicit. Theorem 29 for distribution, Algorithm such that establishes the optimal distribution the optimal schedule. Example 28. Fig. 13 illustrates Algorithm u(E) = 8, u(F) = 14, u(G) = 0, o(H) = 15, u(l) = 0, and v(J) = 18. After pass of the while loop, children the first two for loops, we have the first from left to right in the order G, E, F of B are arranged 1. After 328 l! Xiung/Art#cial Intelligence 87 (1996) 295-342 8 K L MNOPU VWXY K (0) L MNOPU (0) ,U) (0) (0) (0) (01 CO, (01 (0, (“1 (01 VWXY Z Fig. 13. R: a rooted tree for distribution Each node X is labeled with (I’(X) ) as computed by Algorithm operarlon. R’: R after being processed according I. to Algorithm I. based on their L’ values. Children of D are arranged J. The L’ values for nodes B, C, D are assigned as c(B) = 19, u(C) = 0, u(D) = 27. from left to right in the order I, H, After the second pass of the while right in the order C. B, D, and !!(A) loop. children of root A are arranged is assigned the value c(A) = 36. from left to Theorem 29 (Optimal distribution with root A. The absolute off-line Algorithm 1 and the distribution order for each node is right to left. The minimum Al-2 is given bJ c( A ) us computed by Algorithm I. time Al-2 is minimized schedule). Let R be a rooted tree for distribution if R is arranged according to Proof. We prove by induction. Let the depth of R be M. The statement according Assume is true for M = K (K 3 2). Consider M = K + I. Assume to Algorithm 1, and the root A of R has n (n 3 I ) children to Proposition 26. that the statement that R is arranged according 6,. . , Y,,. Each child is the root of a subtree. assumption, the distribution By the inductive {I,. , H}) is minimized if the right-to-left order is followed. The minimum time for the subtree rooted at F (i E is time is true if M = 2 u(K). For i = I.. . , Iz, replace w,,,,( X,) = c( I$). The condition of Proposition optimize the distribution minimized by the induction A,_, is minimal the subtree rooted at x by a single (x,X;) with resultant the 26. Therefore distribution using the right-to-left order at A will time for each subtree rooted at 8 is time. Since the distribution is a tree of depth 2 rooted at A that satisfies link assumption, and the distribution at root A is also optimized, for Ad = k + I and the minimum value is given as u(A). 0 Example 30. The absolute distribution arranging R as R’ and following is o(A) = 36. off-line by the right-to-left distribution order. The minimum Al-2 time for R in Fig. I3 is minimized I’. Xiang/Art@cial Inlelligence 87 (1996) 295-342 329 Fig. 14. A general rooted tree for distribution. 9.2. Distribution schedules with minimum average off-line time Let R be a tree rooted at A for distribution tl. We denote c( start at the time instant is over every node Y of R rooted at A. The optimal distribution minimizes (Fig. 14). Let the distribution operation t,,( Y) - t1 ) by CR(A) where the summation schedule is one that the contribution to @R(A) made by the nodes in a subtree rooted at (TR (A). We first consider a parent node, say, V of m leaf children (see Fig. 14). We have aR(V> = (ton(V) - tl) + &.(zk~ - tl). k=l If we substitute t,,(V) = t,,,,(V) + c”,, wOUt( zk), we can rewrite g.R(V) =e Wdzk) + (m-t l)(tcljt(V) -tl) +e(ton(zkj -&t(v)) (4) k=l k=l first entry (summation) The (product) is independent orders of nodes outside of nodes which we denote by a(V), we have used its minimization R. in the subtree (T(V) is independent of the distribution order. The second entry of the distribution order of V. It is dependent of the distribution the subtree rooted at V, and is dependent of the number m + 1 tcpt( V) ), that that the rest of order of V. Note the fact to signify of the distribution the sum, the last entry X:=1 (t,,( Zk) - rooted at V. Only rooted at V from is dependent the subtree to denote instead of (TR( V) can be studied by isolating Next, we consider the contribution to (+R( A) made by the nodes in a subtree rooted at an arbitrary non-leaf node Y (Fig. 14). We obtain ~R(Y)=(bn(Y) -tl) +xgR(Xk) =~wm (xk) + (t,,,(y) k - tl> + cgR(xkh k k Denote the number of descendants of & by nk, we obtain 330 Y Xiang/Arti$cid Inrellipv~ce 87 (1996) 295-342 CAR =dxk) + (nr + I)(f,,,,(Xk) - tl) =d&) + CfZk + 1 )(t,pt(Xk) -t,,,,(Y)) + (?Zk + 1)(&y,(Y) - t,). If we substitute of Y by 77, we obtain the above expression into VR( Y) and denote the number of descendants ~C?(Y)=Cni,U,(Xn) -t(rl-t l)(f,,,,(Y) -r1) + c ‘dxk) + x(nk 1; I + 1 )(k,,(Xk) - tc,,r(Y)). (5) (summation) is independent is dependent of the distribution order. The second entry The first entry (product) the subtree rooted at Y. and is dependent of the number 7 + I of nodes in the subtree rooted at Y. The third is dependent of the distribution orders of nodes in the subtree rooted entry (summation) at each child of Y. Only is dependent of the distribution order of Y. of the distribution orders of nodes outside the last entry (summation) Note that Eq. (4) is a special case of Eq. (5) if we let Y = V, Xk = Zk, nk = 0, g( Xk > = 0 and t,,,~ ( xk ) = f,,, ( zk) . Another special case of Eq. (5) is cR( A ). If we let Y = A, t,,,,(Y) = tl, we obtain qR ( A ) = c I.~‘,,uI (Xk) + cdxk) + x(nk + I)(fc[dxk) - fl 1. (6) !. k h implies The above analysis for R, we need only to search locully, i.e., to find the optimal distribution order of each node lemma prepares Y, taking for Theorem 32. Its proof is in Appendix A. the number of descendants of Y. The following that, in order to find the optimal distribution into account schedule Lemma 31. Let Y be u rzon-leaf node in o rooted X1,. , X,,, (m > 0) where Xk has nk descendants. tree. Let the child rzodes of Y be The sumnzation @ = cr=, (nk + 1) ( tCp,( Xk ) - tC.,,, ( Y) ) is minimized if w,,,,(Xt)/(nl + 1) < wc,,,(X2)/(~z2+ 1) < .‘. 6 ~dX,,)/(n,,,+ 1) and the distributiorz order of Y is O,,,,(Y) = ( Xi, . , X,,,) Theorem 32 (Optimal distribution tree rooted at A. For each non-leaf node Y of R, let the m > 0 child nodes of Y be indexed fronz left to right as XI,. . X,,,, and let the number of descendants of Xi be n; such that Let R be a weighted schedule). ( X2) w,>,,r ( XI I/ ( III + 1 1 < ~~~,u,r /! 112 + 1 ) 6 < ~VOl,f (Xr,, ) /c ~Z,i! + 1 1 . T/ze average distributiorz o&line tinze (TR( A) is minimized if the distribution order of Y is left to right for every Y. Proof. First, consider root A be X1,X2>.... the case where the depth of R is D = I. Let the child nodes of I! Xiang/Artijicial Intelligence 87 (1996) 295-342 331 Fig. 15. R: a rooted tree for distribution. R’: R with the left-right order of nodes B, C, D re-arranged to Theorem 32. The distribution each node X. The distribution schedule starts at rd = 0. is shown by the label ( tset( X) , tcpt (X) , ton (X), ton(X) - td) at according According to Eq. (6), when D = 1, we obtain (+R(A) = c Wout (Xk) + ~&Jt(Xk) - t1). k k To minimize (TR (A), we only need to minimize fl = ~&t(Yk) ment -tl) is thus true when D = 1. can be minimized the second sum. According to Lemma 3 1, if the left-to-right order is followed. The state- Assume that the statement the left-right is true when the depth of R is D = d 2 1. That is, gR( A) is arranged as specified and the order of child nodes is minimized when left-to-right distribution order is followed. We consider OR = c wour (xk> + c(+(xk) k k + xbk k + l)(&p,(Xk) - tl> when D = d + 1. Based on the analysis made with Eq. (5), minimization and minimization performed is followed. Each U( &) can be minimized by following to the inductive of each a(&) and can thus be if the left-to-right order the left-to-right order according 0 to Lemma 31, q is minimized of $ = zk( nk + 1) ( tcp, (&) assumption. The statement - tt ) are independent, separately. According is proven. Example 33. Fig. 15 shows an optimal distribution Theorem 32. We can compare There, no preferred distribution chosen. The consequence the preferred distribution (TR’( A) = 16 + 9 + 14 + 16 + 5 + 9 + 14 = 83. shows this schedule with the distribution order for node C and O,,,(C) to in Example 27. is arbitrarily is (TR!( A) = 16 + 9 + 14 + 16 + 7 + 9 + 14 = 85. Here, is order for node C is O,,,(C) = (G, F). The consequence schedule obtained This comparison that an optimal schedule under according = (E G) schedule time the absolute off-line time criterion. However, for the criterion may not be optimal under absolute off-line time, both examples happen the average off-line to have the identical value 16. 332 Y Xiuq/Artijcial Intelligence 87 (I 996) 295-342 A Km9.9) Ii/ R’ /I h I H C (3.9.15.15) E&.14.14.14)F/ + (8.12.12.12) (6.8.8.8) / m.3.8.x) t 5” tl 2 Jy 1 ‘\\\ 1 E P.8) Gi ‘% (9.11.11,lI) ~11.15.15.15 Fig. 16. Comparison of the optimal distribution schedules under different criteria. A schedule with the minimum absolute off-line time is shown in R. A schedule with the minimum average off-line time is shown in R’. Each , I,-,,, (X) , ton(X) . I<,,, ( X) - td) at each node X in the corresponding schedule is shown by the label (f,,/(X) rooted tree. The distribution starts at td = 0. schedules two distribution identical in left-right order of child nodes). The schedule Example 34. Fig. 16 shows (subject to a difference is optimal under The distribution 14 + 12 + 8 = 69. The schedule shown in R’ is optimal under criterion, obtained to Theorem 32. The distribution has Ai-2 = 15 - 0 and (TR(A) = 9 + 8 + 15 + 8 + 11 + 15 = 56. tree in R the absolute off-line to Theorem 29. order is right to left. It has A!_2 = 14 - 0 and UR(A) = 9 + 14 + 12 + time is left to right. It the average off-line order time criterion, obtained according rooted shown according in an Example 34 shows that, given a rooted time criterion may not be the optimal under the example off-line and vice versa. Though due to the duality of distribution and collection. tree, an optimal schedule under the absolute time criterion is general the average off-line involves distribution only, the conclusion 10. Discussion 10.1. DAL tree structure and conditional independence In an MSBN, the intersection between each pair of subnets must satisfy such that the pair is conditionally independent condition seen from Section 3, the semantics of joint probability multi-agent to bring distribution is undefined without subnets up-to-date, system two adjacent on the d-sepset between it is sufficient them and nothing else. this condition. With the d-sepset. given distribution of a cooperative in order this condition, to pass the new probability One of the major concerns as possible global consistency As argued by Pearl in DA1 is how to balance to reduce the need and the need to maintain the traffic of communication. independence as much [ 151, a tree structure makes use of conditional the d-sepset It can be Z Xiang/Art$cial Intelligence 87 (1996) 295-342 333 in singly connected BNs In singly connected BNs, a dependency mediator and the most efficient conditionally independence of the internal connected BNs [9]. independent and allows coherent elements. Our study on MSBNs highlights pet-tree structure. This structural preference of the conditional sentation of multiply pair of elements crease of complexity cases. network. is an internal ture, a dependency mediator independence venience, but rather as a mental probabilistic tion of multiply endeavor. construct techniques connected BNs, and In the JT representation nor as an occasional reasoning clique of multiply (a group of variables). passage among a group of into a hy- information the MSBNs that are organized is a more general case of the exploration [ 141 and in the JT repre- If we call an element which can render a a “dependency mediator”, we see an in- in the three structures of dependency mediators the in node is an internal connected BNs, a dependency mediator struc- In MSBNs/IJPs [ 151, conditional con- for which we must passively wait, of for mathematical of a hypertree assumption that we should actively create. The progression from singly connected BNs, to the JT representa- is just one example of such to MSBNs/LJFs is a subnet/JT. As argued by Pearl should not be viewed as a restrictive grace of nature 10.2. Communication in MSBNs and belief propagation in a JT Our CommunicateBelief operation and DistributeEvidence in a multi-agent MSBN in a single is a direct extension JT by Jensen et al. in a single-agent MSBN by Xiang the multi-linkage belief propagation of CollectEvidence [9, lo] and et al. [ 191. If we concentrate on the overall pattern of information tion in an MSBN works in the same way (a semi-parallel by a semi-parallel outward movement As commonly that all nodes of a JT are centralized. The special pattern of information support concurrent the object-oriented processing applied, BNs are single-agent considered uninteresting. oriented. Therefore, in a tree structure) implementation is reasonably as indicated inward movement as belief propagation flow, we see that communica- followed in a JT. it is taken as granted flow is used to in Jensen et al. [ lo]. However, In a multi-agent MSBN, efficiency of communication work has thus been devoted the agents are very likely to be spatially distributed, becomes to this issue. a necessary concern. Part of the effort the in this Once the issue in concurrency control is resolved, however, to the belief propagation JT is large and the computation in a JT, if parallel processing of a JT is desired, is time critical, and the hardware is available. the result can be applied i.e., when the 10.3. Future research interpretation In this pilot study, we proposed a probabilistic for cooperative multi-agent condition- distributed ally independent into an MSBN and the semantics of the joint probability distribution of the MSBN is well defined. Such inference among multiple agents. We an organization that if agents are cooperative, then they can be organized the coherent probabilistic systems. We showed and (initially) framework consistent, ensures 334 K Xiang/Ar@cial Intelligence 87 (1996) 295-342 operations in addition oriented MSBNs. We showed proposed new communication in single-agent sistency among agents after evidence has been gathered by agents asynchronously parallel. We proposed scheduling algorithms the unavailability and minimize There are many directions to the belief propagation operations con- in of agents for evidence processing. for future that these operations the communication that optimize can maintain operations research. These cooperative justification from a pool of potentially in Section 3.3, include dynamic agents of in Section 6.3, refinement of belief propagation application and optimization of the communication to specific problem integration structures, operations localized root as indicated formal construc- in a demand-driven communica- and their of interpreta- in Sec- as mentioned tion of an MSBN fashion tion as outlined implementation, tion and action tion 7.4. Appendix A. Appendix Let R be a weighted tree rooted at A with the in-weight and Theorem 24 (Duality). out-weight of each link identical. Let Sd be a distribution schedule which starts from A at t,r and terminates at rd. Let the distribution order of a parent node Y be denoted as Od( Y) in this schedule. A schedule SC for collection, that starts at t, and terminates at A at r,, can be obtained by requiring each parent node Y to follow a collection order that is opposite to Od( Y) such that the followings hold: rC-tC=rd-tdrand (I) (2) for every node X, r, - t,,f( X) The converse (obtaining Sd from SC) is also true. in S,. is identical to t,,(X) - td in Sd. Proof. We start with an Sd and construct are satisfied. Once true. the mapping is created, an SC such that the above the converse of the theorem two conditions is trivially We assume that for each parent node Y with m child nodes, the child nodes are arbitrarily indexed as XI, order of Y in Sd by od( Y) = (XI, we can then characterize & as follows: . , X,,. Without losing generality, we denote the notation defined . . , X,,). Using c Lpty) = td rc,,t(Y) = k(Y) + w(Y) if Y is the root, if Y is not the root, r,,,,(Y) = f,,‘f (Y) + x5, w( Xk 1 if Y is not a leaf, t,rr(Xi) = f‘,,),(Y) + c;:: W(XL) if X, is not the root, t,,,, (X) = f,,” ( w , ~(1 = max(G,,(z)) if X is a leaf, max ( ) over every node Z. the distribution in Section 7.4, (A.1) We construct a collection schedule SC from & by requiring the collection follow resultant schedule SC can be characterized order O,(Y) = (X,,, . as follows: . , X1 ) which each parent node Y to to od( Y). The is opposite E Xiang/Artifcial Intelligence 87 (1996) 295-342 335 4@(X) = 7d - bz(m + tc r@(Y) = Q(X”,) trdy (X) = t, &d?(Y) = &JJ$(~) + c”,, W(Xk) &r(Y) = &-dy(Y) + W(y) LTt(X) = t,(X) + w(X) &r(Y) = bdy(Y) 7c = trdy(Y) if X is a leaf, if Y is not a leaf, if X is a leaf, if Y is not a leaf, if Y is neither the root, nor a leaf, (A.21 if X is a leaf, if Y is the root, if Y is the root. We show inductively that S, and & satisfy an R with depth D = 1. That Consider ,X,,. For &, using Eq. (A.l), we obtain X,,... czI W(Xi) which implies the two conditions is, R has a root node A with leaf children = td and stated in the theorem. t,,,(A) = td + t,,(A) t,,(A) -ld=eW(Xi). i=l For child nodes, we obtain ton (Xi) = t,,,, (Xi) = td + Elk, w( Xk) which implies &m(Xi) - fd = 2 W(Xk). k=l Maximization Over Ton reSUh in Td = td + CL, W(Xi) and Td - “, td = c W(Xi>- i=l (A.3) (A.4) (A.51 From Eq. (A.2) and & specified by Eqs. (A.3), (A.4), and (A.5), we derive 5, as follows: For each leaf, we have t&Xi) = rd - to,(Xi) •k f, = t, i- c&+t W(Xk). For the root, we obtain tc + CL, W(Xk). The above implies r,f(A) = to~(Xxnl) = tc and rc = t,dv(A) = t,~(X,) + CL, w(X,) = rc - t, = A c i=l w( xi) = Td - tdy m Tc - &g(A) = cW(Xk) = &m(A) - td, i=l tof(Xi) = 2 W(Xk) = ton(Xi) Tc - - td. k=l Therefore, the statement is true when D = 1. 336 K Xiung/ArtiJiciul lnrelligence 87 (I 996) 29.5342 that the two conditions Assume Suppose we add some child nodes in the theorem hold for any R of depth D = d > 1. to the leaves of R at depth d to form a new tree R’. The depth of R’ is D = d + 1. We show that the two conditions hold for R’ as well. We will add a prime mark (‘) to a quantity (e.g., Sd and SC) associated with R’ to distinguish time it from that associated with R. ti of Si be the same as td of Sd, i.e., tl = td. Then, for each which node Y of depth < d - 1 and each leaf Y of depth d, we have tL (Y) = t,,(Y) implies td, r,, and t()f) or an object Let the starting (e.g., r:,,(Y) -t; = t,,(Y) -- td. (A.6) Let X be a node of depth d = t,,(X) Eq. (A. 1) , we have t:,,,(X) in R’ with m leaf children: Zt,. + C,“=, w( zi) which implies . . , Z,,. Based on t:,,(X) - t; = t,,(X) - tll + c w(Z). ;=I (A.7) For each leaf Zj of depth d + 1 with parent X, we obtain t:,,,( Zj) = t,,(X) + ri=, w( Zi) which implies t;,(q) - t:, = t,,,tw - tfl + f: w(Z,). The termination of R’. The above completely specifies Si. time is ~2 = max( tb,( X) ) 3 rd where maximization (A.8) is over all nodes We construct Sl_ from Si based on Eq. (A.2). Let us assign t:. = t, - (T; - 3-d) (A.9) which means than Sd. that Si starts earlier than SC by an amount of time by which Si is longer For each leaf Z; of depth d + 1 with parent X, we obtain t:,R(Zj)=7:1-t:,n(Zj)+tl=t,+7d-t,,,(X) -eW(Z,), i=l (A.lO) where the first equality and (A.8). We also obtain is due to Eq. (A.2) and the second equality is due to Eqs. (A.9) t:&(X) = $$(Zm) + c w(Zi) = t, + rd - t,,(X), (A.1 1) i=l where the first equality Eqs. (A.1 1) and (A.2) parent in R’. is due to Eq. (A.2) and the second equality = t,,f( X) is due to Eq. (A. 10). for each X that is a leaf in R but a imply t&,,(X) For each leaf at depth < d, E Xiang/Artificial Intelligence 87 (1996) 295-342 331 t&f(x) ‘7; - t&(X) -+ t; = t, + 7d - t;,(X) =&+Td--f,,(X) =bf(x), (A.12) where the second equality is due to Eq. (A.9) and the third equality for the timing of activities of all nodes above that fLd,,( X) of nodes of depth d and &(X) condition (A.1 1) and Note a boundary Eqs. (A.12) node of depth f d, the timing of its activity in-agents obtain rI_ = r, which from children of a node at depth d) the first condition implies in SL (excluding is exactly in the theorem: together with the inductive assumption is due to Eq. (A.6). of leaves of depth < d form them. Therefore, that, for each the inward movement of the same as in SC. We thus imply r: - t: =rc-tc+r~-rd=rd-td+r&-rd=7&-td=7&-t&, (A.13) where inductive the first equality assumption. is due to Eq. (A.9) and the second equality is derived by the Finally, we consider r: - t&( ) in terms of four exclusive and exhaustive cases of the node involved: For each leaf Zj of depth d + 1 with parent X, we have r:.-f~~(Zj)=(f:+r~-td) - &+rd-ton(x) =--ld+t,.(X)+CW(z~)=tbn(Zj)-fh, (A.14) i=l the first equality where to Eq. (A.9) and the third equality is due to Eq. (A.8). is due to Eqs. (A.13) and (A.10)) the second equality is due For each leaf X of depth 6 d, we obtain r: - t&(X) =ri - (r& -t&,(X) + t:) =ri - t& - (r& - t:,(X)> =tL,(X) - t&, where the second equality is obtained using Eq. (A.13). For each non-leaf node X of depth d, we derive r:.-_~~(X)=rc-ft:~(Z,,)=r,- =t&(X> -r&, fc+rd-ro,((x)-~W(Zi) ( “, i=l (A.15) (A.16) where the second equality is due to Eq. (A.lO) and the third equality is due to Eq. (A.7). For each non-leaf node X of depth < d, since ri = r,, f$( X) = t,#( X), tb,( X) = t,,(X) and fh = td, we have r: - t&(X) = t&(X) - fi. The second condition of the theorem is proven. Cl (A.17) 338 Y Xiung/Artificial Intelligence 87 (1996) 295-342 in trees of depth 2). Let R be a tree the children of root be . , X,. Let the sum of out-weights of children of Xi be v, such that VI < 02 < . . < if the distribution order distribution Proposition 26 (Optimal rooted at A for distribution. Let the depth of R be 2. Let XI,. v,. The absolute distribution off-line time Al-1 in R is minimized of A is O,,,,(A) = (X,, . . . ,X1 1. schedule Proof. According to the order 06:: (A) = (X,, , . . , XI ), the distribution time is A” 1 1_2 = max(w(&) + u,,, WC&) + WC&I) + 4-1,. . , , W(X~)+‘.‘+W(Xi)+Vi,...3W(Xfl)+“‘+W(XI)+V~) in max() where we have written w( Xi) instead of w,,,( Xi) for simplicity. Note we have listed the in the order consistent with the order in which Xi appears in O:,!,‘(A). entries to its right. Note if W(Xi) appears In this order, also that each entry has exactly one v among in every entry in an entry, its addends. it appears Let 0C2’(A) = (X’ 0Uf . . . , Xi ) be any order distinct , n}) ‘and X: # X: for i # j. We denote Xk (k E {I,... by w( Xi), and the sum of weights of children of X,! by vi. The distribution 0C2’(A) 0ll, is from O:,L,‘( A) where Xi is some the weight associated with Xi time under A i!i=max(el,,eL_ ,,..., ei) where e: = + Ui. First we simplify Aif: by removing superfluous entries, Suppose v,, is an addend of e; and d < II, or equivalently, X,, is not the first in O,$ (A). Then e:, . . . , em+, can be elimi- nated from max() without altering the value of AI!:, since v, = max:& (vi) and therefore e: < e; when i > d. After the elimination, we have AiT: = max(e&, ei_i , . . . , e{ ). Suppose now o,, (m E { 1,. value among eL_, , the L’ contained . e:.,l can all be eliminated from max ( ) , II - l} ) is an addend of e:, and m is the highest through e{. With the same argument index as above, in eI_, Repeating this process, we end up with AiT: = max(e2, e:, . . . , e:) where u, is an , VI is an addend of e:, such that n > m > . . . > 1. as the expanded form, and addend of el, v,,, is an addend of ef, . We will refer to the expression of A,_, (2) before simplification as the simpli$ed refer to the expression after simplification form. Example. Suppose 0::; (A) = (X4, X5, X2, X3, Xi ) . The expanded form of Ai!: is Al!: = max(w(X4) +~‘4, w(X4) + I + ~3, w(X4) + I + w(X2) fv2, W(X4) + W(X5) + w(X2) + w(X3) + 03, w(X4) + w(Xs) + w(X,> + w(X3) + bV(Xl) + VI 1. After removing superfluous entries, the simplified form is Aif: = max(w(X4) + w(G) + L’S, w(X4) + w(X5) + 4x2) + w(Xj) + ~3, w(X4) + w(X,) + w(X2) + w(X3) + w(X1) +vl). E Xiang/Artificial Intelligence 87 (1996) 295-342 339 Next, we prove a lemma which states the following: (2) Lemma. After A,_2 + w(X,) (Cw(Xi>) c W(Xi). is simplified into Ai’: = max(e&, . . . , e:, . . . , e:), + urn, then all of w(X,,),...,w(X,+l) if ei = in are addends Proof. It suffices to show that ek has all of w(X,), . . . , w(X,,,+t ) as its addends. Consider Ui (n > i 2 m + 1) . In the expanded that is either before in an entry simply say that ui appears before form of A I::, Ui appears as an addend (to the left of) ei or after e:. In the following, we (after) e:. If Ui appears before e:, then e: must contain the addend W(Xi). It cannot appear after e:, since entries in the simplified A,_, (2) has a decreasing order for the index of addend u. If ui had appeared after e: in the expanded eliminated in the simplification process. The lemma is then proven. 0 form of Al::, then e: would have been Finally, we show that Ai!: < A{:!, by identify’ g m one entry e: in the simplified Al”_: such that Al!: < e:. Suppose A;!; = + uk, kE {l,...,n}. We search for an entry e: in the simplified Aif: such that e: has Ok as an addend. We consider three exclusive and exhaustive cases: the following Case 1. If such an e: is found, by the above lemma, we have Ai!: < e:. If an entry with the addend uk is not found, we search for an entry ei with an entry that ei has an addend Q (I > k) and e6 has an addend uj to its right such eb next (k > j). Case 2. If such e: and e; are found, we show that e: has all of w( X,,), . . . , w( xk) as its addends. By the lemma above, e: has all of w(X,), show that et also has all of w(Xj_t ), . . . , w(&) By the lemma, eb must have all of w( Xl-t), . . . , w(Xl) as its addends. We need to as its addends. . . . , w( & ) as its addends. Therefore, in form of A!‘_:, all of Q-I,. the expanded to the left of eb. The question (iE{I-1,. Ifanyui is whether . . , k}) had appeared . . , uk must have appeared as addends they appear before ei or after e:. in entries in an entry after ek in the expanded form of A:!:, since i > j, at least one such entry appeared would have been included simplified A!:$. Therefore, andeihasw(Xi) in the each ui (i E (1 - 1,. . . , k}) must have appeared before ei, fori=Z-l,...,kasaddends. Case 3. If the above specified e: is found but there exists no eb as specified above, entry in the simplified form of Ail:, we show that ei contains i.e., e: is the right-most addends W(X,),...,W(Xk). the left-most Consider all of w(X,), . . . , w( Xi ). The simplification entry ei in the expanded form of Al!:. This entry contains the right-most process does not eliminate 340 l! Xiung/Arfijicial Intelligence 87 (I 996) 295-342 entry of the expanded A:!:. Therefore, entry as the expanded the simplified form: e:. = e{ which contains addends w(X,,), form has the identical . . . , w(Xk). right-most We have shown that Ai!: < Aili holds for an arbitrary order O::,‘(A). Therefore, O’“(A) minimizes Al-z. 011 I 0 tree. Let the child nodes of Y be , X,,, (m > 0) where Xk has nk descendants. The summation Cc, = cF=, (nk + Lemma 31. Let Y be a non-leaf node in a rooted X1, 1) ( f,,,, ( Xk ) - tCpr ( Y) ) is minimized if . ~vm,r(xl)/(m + 1) G w,,,ll ( X2 ) / (112 + 1 ) 6 G MJ,,,( L ) /(n,, + 1) and the distribution order of Y is O,,,( Y) = ( XI, . , X,,) Proof. When O,>,,(Y) therefore is followed, we have t,L,t(Xk) - t,,,(Y) = Et, w(Xi), and ,,I $=-J+l k=l l,~w(x;) i=l + l)W(X,) + e(M2 + 1 )W(X;) + ,..f k(nn, i=l + ])w(X,) = I C(nl !=I = W(xI) i=l + 1) + W(X2) 111 C(ni i=2 ,I, C(% i=l + 1) + .. + W(X,,) e(q i=m + 1) where we have written w(Xi) instead of w,,,(Xi) for simplicity. Assume that the following condition holds: ~(XI )/(nl + 1) 6 w(X2)/(rz2 + I) 6 .‘. < w(X,,)/(n,, + 1) Whenm=2,~=w(X~)(n~+l+n~+I)+w(X2)(n~+l) If instead derivation, we have 9’ = w(X2) (nl + 1 + n2 + I) + w(Xl) the other possible order O:,,,,(Y) = (X2, XI ) is followed, using ifO,,,(Y) isfollowed. the similar (n] + 1). The difference 9’-~=~v(X-~)(n~+1)-ww(X1)(n2+1) by assumption. In general, for each Xk, @ contains sum by P: >Osincew(X,)/(nl+l) <w(X2)/(n2+1) exactly m Xk-related addends, we denote their f(nk+l f I)W(Xk) +...+ (n,,r+ l)W(Xk). Note that each addend in P (in the form (ni + 1) w( X,i) ) is unique. Given an arbitrary distribution order OiSUr( Y) = (X1,, . , . , X,,, ) where i’ E { 1,. , . , m} in O:,,,(Y), we also have exactly m and where 1’ = k, i.e., Xk appears at Ith location Xk-related addends in #‘. we denote their sum by P’: I! Xiang/Art$cial Intelligence 87 (1996) 295-342 341 P’= (rtk + l)w(X*,) + * * .+ cm + l)W(Xl-If) + (4 + l)W(Xk) +(nr+lt + l)W(Xk) +...+ (n,,t + l)W(X&). Note that each addend in P' is also unique. We show that P' - P 3 0 for every Xk. To show that, it is sufficient to create an f from the set of addends of P' to the set of addends of P one-to-one such that p’ - p > 0 where p’ is an addend of P' and p = f( p') . correspondence A typical addend of P' is either in the form (nk + 1) w( Xi!) or in the form (nit + l)W(Xk). Suppose p’ = (ttk + l)w(Xil). If i’ < k, there exists a unique p = (nk + l)w(Xi,) in P and p’ - p = 0. If i’ > k, there exists a unique p = (nil + l)w( Xk) in P and p’ -p 2 0 since w(Xit)/(nij + 1) by assumption. + 1) Z w(X~)/(Q Suppose p’ = (nil + 1) w( Xk) . If i’ < k, there exists a unique p = (nk + 1) W( Xii ) in If i’ > k, + 1) < w(Xk)/(nk + 1) by assumption. P and p’ -p > 0 since W(Xi,)/(nif thereexistsauniquep=(nit+l)w(Xk) inPandp’-p=O. Therefore, P' - P 2 0 for every Xk, and the order O,,,(Y) minimize 9. 0 Acknowledgements This work is supported by the Dean’s Research Funding from Faculty of Science, Uni- the General NSERC Grant from University of Regina, and Research versity of Regina, Grant OGP0155425 References from NSERC. [ l] A.H. Bond and L. Gasser, An analysis of problems and research in DAI, in: A.H. Bond and L. Gasser, eds., Readings in Distribufed Artificial Intelligence (Morgan Kaufmann, Los Altos, CA, 1988) 3-35. [2] A.H. Bond and L. Gasser, eds., Readings in Distributed Artificial Intelligence (Morgan Kaufmann, Los Altos, CA, 1988). [3] R. Davis and R.G. Smith, Negotiation as a metaphor for distributed problem solving, Art$ Intell. 20 (1983) 63-109. [4] A.P. Dawid and S.L. I-au&en, Hyper Markov laws in the statistical analysis of decomposable graphical models, Ann. Stat. 21 (1993) 1272-1317. [ 51 L.D. Erman, EA. Hayes-Roth, V.R. Lesser and D.R. Reddy, The Hearsay-II speech-understanding system: integrating knowledge to resolve uncertainty, Comput. Surveys 12 (1980) 213-253. [6] L. Gasser and M.N. Huhns, eds., Distributed Artificial Intelligence Vol. II (Morgan Kaufmann, Los Altos, CA, 1989). [7] C. Ghezzi, M. Jazayeri and D. Mandrioli, Fundamentals of Sofrware Engineering (Prentice-Hall, Englewood Cliffs, NJ, 1991). [S ] C.E. Hewitt, Offices are open systems, ACM Trans. Ofice hform. Syst. 4 (1986) 271-287. [ 91 F.V. Jensen, S.L. Lauritzen and K.G. Olesen, Bayesian updating in causal probabilistic networks by local computations, Comput. Stat. Quart. 4 ( 1990) 269-282. [lo] EV. Jensen, K.G. Olesen and SK. Andersen, An algebra of Bayesian belief universes for knowledge- based systems, Nerworks 20 (1990) 637-659. [ 111 S.L. Lauritzen and D.J. Spiegelhalter, Local computation with probabilities on graphical structures and their application to expert systems, J. Roy. Stat. Sot. Ser. B 50 ( 1988) 157-244. 342 Y Xiang/Artificial hteliigence 87 (1996) 295-342 29 (1980) [ 12 1 V.R. Lesser and L.D. Erman, Distributed 1144-t 163. [ I3 J R.E. Neapolitan, P mbabilistic Retrsoning 1 141 J. Pearl, Fusion, propagation, [ 15 I J. Pearl, Probabilisric Reasoning Kaufmann, Los Altos, CA, 1988). in interpretation: a model and experiment, IEEE Trans. Comput. in Experr Sysrems (Wiley, New York, 1990) and structuring in belief networks, Artif Intell. 29 ( 1986) 241-288. Intelligent Systems: Networks of Plausible inference (Morgan [ 16 1 W.A. Shay, Understanding Data Communications [ 17 ] A. Silberschatz [ 18 1 Y. Xiang, B. Pant, A. Eisen, M.P. Beddoes and D. Poole. Multiply and P.B. Calvin, Operating system Concepts and Nefwurks (PWS Publishing, Boston, MA, 1995). (Addison-Wesley, Reading, MA, 1994) sectioned Bayesian networks for neuromuscular diagnosis, Artif. Intell. Med. 5 ( 1993) 293-314. 119 1 Y. Xiang, D. Poole and M.P. Beddoes, Multiply sectioned Bayesian networks and Junction forests for large knowledge based systems, Compt. Infell. 9 ( 1993) 171-220. 