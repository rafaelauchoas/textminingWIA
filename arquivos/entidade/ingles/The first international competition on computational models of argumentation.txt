Artificial Intelligence 252 (2017) 267–294Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintThe first international competition on computational models of argumentation: Results and analysis ✩Matthias Thimm a,∗a Institute for Web Science and Technologies, Universität Koblenz–Landau, Germanyb Université Côte d’Azur, CNRS, Inria, I3S, France, Serena Villata ba r t i c l e i n f oa b s t r a c tArticle history:Received 14 June 2016Received in revised form 15 June 2017Accepted 18 August 2017Available online 30 August 2017Keywords:Formal argumentationAlgorithmsWe report on the First International Competition on Computational Models of Argumenta-tion (ICCMA’15) which took place in the first half of 2015 and focused on reasoning tasks in abstract argumentation frameworks. Performance of submitted solvers was evaluated on four computational problems wrt. four different semantics relating to the verification of the acceptance status of arguments, and computing jointly acceptable sets of arguments. In this paper, we describe the technical setup of the competition, and give an overview on the submitted solvers. Moreover, we report on the results and discuss our findings.© 2017 Elsevier B.V. All rights reserved.1. IntroductionArgumentation is a core technique for humans to reach conclusions in the presence of conflicting information and mul-tiple alternatives. It is used both as a means for persuasion in dialogues as well as one owns deliberation mechanism. An argument can be regarded as some concise set of pieces of information that supports a certain conclusion, such as “As Tweety is a bird and birds usually fly, Tweety supposedly flies”. Arguments may support contradicting conclusions—consider e.g. “As Tweety is a penguin and penguins do not fly, Tweety does not fly despite the fact that he is a bird”—and the pro-cess of argumentation aims at comparing and weighing arguments and counterarguments and ultimately deciding which arguments prevail. While the field of argumentation theory [96] studies the structure and interaction of arguments from a philosophical perspective, within artificial intelligence, the field of computational models of argumentation [6,8] has gained some attention in recent years. In general, this field is concerned with logical formalizations of models of argumentation that can be used by automatic reasoning systems to cope with uncertainty and inconsistency. Thus, these models are closely related to approaches to non-monotonic reasoning and offer a novel perspective on those. After some earlier works of e.g. Pollock [82] and Simari & Louie [87], abstract argumentation frameworks have been proposed by Dung [35] as a general and abstract formalism to represent arguments and their interactions and have, since then, been most influential. In abstract ar-gumentation frameworks, arguments are represented as vertices in a directed graph and an arc from a vertex A to a vertex B means that A is a counterargument for B or that A “attacks” B. Thus, this model abstracts from most issues of argu-mentation scenarios—including the inner structure of arguments—and provides a clean formal view on the issue of conflict between arguments. Given an abstract argumentation framework the central question is to decide whether arguments are acceptable, i.e., whether they “survive” the attacks of their counterarguments due to backing by other arguments. A set of jointly acceptable arguments is then also called extension.✩This paper was submitted to the Competition Section of the journal.* Corresponding author.E-mail address: thimm@uni-koblenz.de (M. Thimm).http://dx.doi.org/10.1016/j.artint.2017.08.0060004-3702/© 2017 Elsevier B.V. All rights reserved.268M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Abstract argumentation provides a nice framework to discuss issues of non-monotonic reasoning in general as many other non-monotonic formalisms such as default theory and logic programs under the stable model semantics can be cast into abstract argumentation frameworks, cf. [35]. On the other hand, the multitude of different semantics and extensions go beyond the expressivity of previous formalisms and provide a novel general approach to non-monotonic reasoning, cf. e.g. [39]. This makes abstract argumentation frameworks a versatile knowledge representation formalism. Many research topics have been spawned around these frameworks including, among others, semantical issues [4], extensions on support [31], quantitative approaches [38,90,65], and in particular algorithms [30]. The computational challenges of various reasoning problems are vast and range up to the second level of the polynomial hierarchy for certain semantics [40,44]. Among the first implementations for reasoning with abstract argumentation frameworks—which appeared around 2008—were Dungine [88] and ASPARTIX [47]. More followed in the years after and, starting from 2013 up till now, a number of comparative analyses among argumentation solvers have been conducted, e.g., [42,10,43,95,11,12,14,27], in order to address a systematic performance comparison. Following the tradition of the communities of other approaches to knowledge representation and reasoning, such as the SAT and the Answer Set Programming (ASP) communities, a public competition for solver evaluation was planned soon after.This paper reports on the First International Competition on Computational Models of Argumentation (ICCMA’15) which took place in the first half of 2015. The results of the competition had been officially presented at the International Work-shop on Theory and Applications of Formal Argument (TAFA’15) which was co-located with the 24th International Joint Conference on Artificial Intelligence (IJCAI’15) in Buenos Aires, Argentina. The competition called for solvers on four classical computational problems in abstract argumentation frameworks wrt. the four classical semantics proposed in [35], includ-ing enumerating all extensions of a particular semantics and deciding whether a certain argument is contained in all of them. Submitted solvers were evaluated wrt. their runtime performance on these tasks on a series of artificially generated argumentation frameworks.Abstract argumentation frameworks are arguably the most investigated formalism for formal argumentation. However, there are also formalisms for structured argumentation, such as deductive argumentation [8] and defeasible logic program-ming [55]. In structured argumentation, arguments are a set of (e.g. propositional) formulas (the support of an argument) that derive a certain conclusion (the claim of an argument). The attack relation between arguments is then derived from logical inconsistency. For ICCMA’15 only problems of abstract argumentation have been considered as this is simple and well-understood formalism for representing computational argumentation. However, considering tracks on structured argu-mentation may be a worthwhile endeavor for future competitions.The competition received 18 solvers from research groups in Austria, China, Cyprus, Finland, France, Germany, Italy, Romania, and the UK. The solvers were based on different approaches and algorithmic design patterns to solve problems, ranging from reductions to SAT or ASP problems to novel heuristic algorithms. This paper gives an overview on the setup of the competition, the submitted solvers, and the results. More specifically, the remainder of this paper is organized as follows. In Section 2 we provide some necessary background on abstract argumentation and give an overview on the computational tasks considered in the competition. In Section 3 we describe the technical setup of the competition, including the approach for benchmark generation, the used evaluation methodology, and the technical interface requirements. In Section 4 we give an overview on the submitted solvers. Afterwards, we present and analyze the results of the competition in Section 5 and we discuss the lessons learned from this first experience in Section 6. We conclude with a summary in Section 7. Appendix Aprovides pseudo code of the graph generators used for creating the benchmark graphs of the competition. Appendix B gives detailed graph-theoretic statistics on the benchmark graphs.2. Background and competition overviewIn the following, we give a brief overview on abstract argumentation, the computational problems considered in the competition, and some brief overviews on answer set programming and satisfiability solving. The latter are intended to provide some formal background on the inner workings of solvers based on reductions to those.2.1. Abstract argumentationAbstract argumentation frameworks [35] take a very simple view on argumentation as they do not presuppose any internal structure of an argument. Abstract argumentation frameworks only consider the interactions of arguments by means of an attack relation between arguments.Definition 1 (Abstract argumentation framework). An abstract argumentation framework AF is a tuple AF = (Arg, →) where Argis a set of arguments and → is a relation → ⊆ Arg × Arg.For two arguments A, B ∈ Arg the relation A → B means that argument A attacks argument B. Abstract argumentation frameworks can be concisely represented by directed graphs, where arguments are represented as nodes and edges model the attack relation. Note that we only consider finite argumentation frameworks here, i.e., argumentation frameworks with a finite number of arguments.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294269Fig. 1. A simple argumentation framework.Example 1. Consider the abstract argumentation framework AF = (Arg,→) depicted in Fig. 1. Here it is Arg = {A1, A2, A3,A4, A5} and → = {(A2, A1), (A2, A3), (A3, A4), (A4, A5), (A5, A4), (A5, A3), (A5, A6), (A6, A6)}.Semantics are usually given to abstract argumentation frameworks by means of extensions [35]. An extension E of an argumentation framework AF = (Arg, →) is a set of arguments E ⊆ Arg that gives some coherent view on the argumentation underlying AF.In the literature [35,22] a wide variety of different types of semantics has been proposed. In the competition we focused on the four classical semantics of Dung [35], namely grounded, complete, preferred, and stable semantics. For a set of + = {B | ∃A ∈ S : A → B} denote arguments S ⊆ Arg let Sthe set of attacked arguments of S.− = {B | ∃A ∈ S : B → A} denote the set of attackers of S and let SDefinition 2. Let AF = (Arg, →) be an argumentation framework.1. A set of arguments E ⊆ Arg is conflict-free iff there are no A, B ∈ E with A → B.2. An argument A ∈ Arg is acceptable with respect to a set of arguments E ⊆ Arg iff for every B ∈ Arg with B → A there is A(cid:7) ∈ E with A(cid:7) → B.3. A set of arguments E ⊆ Arg is an admissible extension iff it is conflict-free and all A ∈ E are acceptable with respect to E.4. A set of arguments E ⊆ Arg is a complete extension (CO) iff it is admissible and there is no A ∈ Arg \ E which is acceptable with respect to E.5. A set of arguments E ⊆ Arg is a grounded extension (GR) iff it is complete and E is minimal with respect to set inclusion.6. A set of arguments E ⊆ Arg is a preferred extension (PR) iff it is complete and E is maximal with respect to set inclusion.7. A set of arguments E ⊆ Arg is a stable extension (ST) iff it is complete and E ∪ E+ = Arg.If E is some extension we say that each A is accepted wrt. E. The intuition behind admissibility is that an argument can only be accepted if there are no attackers that are accepted and if an argument is not accepted then there has to be an acceptable argument attacking it. The idea behind the completeness property is that all acceptable arguments should be accepted. The grounded extension is the minimal set of acceptable arguments and uniquely determined [35]. A preferred extension is a maximal set of acceptable arguments and a stable extension is a complete extension that attacks all arguments not contained in it. Note that for complete, preferred, and stable semantics, their extensions are not necessarily uniquely defined and that for stable semantics an extension does not necessarily exist [35].For the remainder of the paper we use σ to denote any semantics of GR, CO, PR, ST.Example 2. Consider again the argumentation framework AF in Fig. 1. The complete extensions of AF are E 1 = {A2}, E 2 ={A2, A4}, and E 3 = {A2, A5}. Furthermore, E 1 is the grounded extension, E 2 and E 3 are both preferred extensions, and only E 3 is stable.An alternative approach to define the semantics of an argumentation framework is to use labelings instead of extensions [21].Definition 3. (AF-labeling) Let AF = (Arg, →) be an abstract argumentation framework. An AF-labeling is a total function lab : Arg → {in, out, undec}. We define in(lab) = {ai ∈ Arg|lab(ai) = in}, out(lab) = {ai ∈ Arg|lab(ai) = out}, undec(lab) = {ai ∈Arg|lab(ai) = undec}.While extensions only allow for a two-valued assessment of the justification status of an argument—either the argu-ment is in the extension or it is not—labelings allow a three-valued assessment where the additional assessment value “undec” represents an undecided assessment. Similar conditions as in Definition 2 can be defined for labelings in order to formalize when a labeling is conflict-free, admissible, complete, etc., e.g., a labeling lab is conflict-free iff there are no A, B ∈ in(lab) with A → B. Indeed, labeling-based and extension-based semantics are equivalent [21] through the following transformations. If E is a conflict-free (admissible, complete, . . . ) extension then the labeling lab defined through in(lab) = E, +) is a conflict-free (admissible, complete, . . . ) labeling. Furthermore, if lab is a out(lab) = Econflict-free (admissible, complete, . . . ) labeling then in(lab) is a conflict-free (admissible, complete, . . . ) extension. For this reason we may use the terms labeling and extension interchangeably., and undec(lab) = Arg \ (E ∪ E+270M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Table 1Computational complexity of important decision problems in (C-c denotes completeness for com-plexity class C). P is the class of decision problems solvable by a deterministic Turing machine in polynomial time; NP (resp. coNP) is the class of decision problems where the Yes (resp. No) in-stances can be accepted by a non-deterministic Turing machine in polynomial time; (cid:3)P2 is the class of decision problems where the complement can be decided by a non-deterministic Turing machine, that has additionally access to an NP-oracle, in polynomial time, see also [81].σCOPRGRSTCredσNP-cNP-cin PNP-cSkeptσin P(cid:3)P2 -cin PcoNP-cVerσin PcoNP-cin Pin PExistsσtrivialtrivialtrivialNP-cExists¬∅σNP-cNP-cin PNP-c2.2. Computational problemsThe most important decision problems discussed in the context of abstract argumentation are as follows (let σ be any semantics):CredσInput:An argumentation framework AF = (Arg, →)and an argument A ∈ ArgSkeptσOutput: Yes iff A is contained in at least one σ -extension of AFAn argumentation framework AF = (Arg, →)and an argument A ∈ ArgInput:Output: Yes iff A is contained in all σ -extensions of AFVerσInput:An argumentation framework AF = (Arg, →)and a set E ⊆ ArgOutput: Yes iff E is a σ -extension of AFExistsσExists¬∅σAn argumentation framework AF = (Arg, →)Input:Output: Yes iff AF has at least one σ -extensionAn argumentation framework AF = (Arg, →)Input:Output: Yes iff A has at least one non-empty σ -extensionThe decision problem Credσ is about credulous acceptance of an argument, i.e., whether it is contained in any σ -extension. The problem Skeptσ is about skeptical acceptance of an argument, i.e., whether it is contained in all σ -extensions. Further-more, Verσ is about verifying whether a given set of arguments is indeed a σ -extension. Finally, the problems Existsσ andExists¬∅relate to existence problems of extensions. Note that Existsσ is trivial for most semantics except stable semantics, σas they guarantee the existence of at least one extension. The harder decision problem Exists¬∅is about checking whether σthere exist a non-empty σ -extension.Table 1 gives an overview on the computational complexity of the decision problems discussed above. The results on the grounded semantics as well as ExistsPR and ExistsCO follow immediately from the properties of these semantics shown in [35]. The remaining results for complete semantics are initially by [32]. The results for stable and preferred semantics follow from their corresponding results for logic programming [33], except the SkeptPR result which is from [36]. For a more detailed discussion of these results and the employed techniques see [41,44]. As can be seen, grounded semantics is the only semantics where all five decision problems are tractable. A naive algorithm for computing the grounded extension can easily be given: first, all arguments that have no attackers are added to an empty extension E and those arguments and all arguments that are attacked by one of these arguments are removed from the framework; then this process is repeated; if one obtains a framework where there is no unattacked argument, the final set E is the grounded extension. Clearly, this is a polynomial algorithm that can be used to solve all the above decision problems wrt. grounded semantics. The problems related to complete and stable semantics usually reside on the first level of the polynomial hierarchy and are thus intractable in practice. Preferred semantics is usually assessed to be computationally harder than the other semantics and particularly the decision problem Skeptσ lies on the second level of the polynomial hierarchy.Functional problems, such as computing all σ -extensions of an argumentation framework AF, have not been investigated much in the literature. This is in line with general research on computational complexity as functional problems may also heavily depend on the size of the output. However, the computational complexity of the corresponding decision problems are usually sufficient to judge the hardness for the functional problems as well.Still, solving functional problems is important for the actual usability of systems using abstract argumentation and have therefore been considered in the competition as well. More precisely, the problems considered in the competition are given as follows (with the actual naming convention used for the competition):M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294271DC-σInput:An argumentation framework AF = (Arg, →)and an argument A ∈ ArgOutput: Yes iff A is contained in at least one σ -extension of AFDS-σInput:(equivalent to Credσ )An argumentation framework AF = (Arg, →)and an argument A ∈ ArgOutput: Yes iff A is contained in all σ -extensions of AFSE-σEE-σ(equivalent to Skeptσ )An argumentation framework AF = (Arg, →)Input:Output: any σ -extension E of AFor NO if there are no σ -extensionsAn argumentation framework AF = (Arg, →)Input:Output: the set {E 1, . . . , En} of all σ -extensions of AFIn the above notation, the abbreviation DC stands for “decide credulous”, DS for “decide skeptical”, “SE” for “some extension”, and “EE” for “enumerate extensions”. In the following, we refer to DC, DS, SE, and EE as computational problems (or simply problems) and to a combination of a problem and a semantics, e.g. SE-PR, as a track. In the competition we considered the four problems in combination with each of the four discussed semantics, resulting in a total of 16 tracks. For each track, the aim of the competition was to evaluate solvers on how fast instances of these tracks could be correctly solved. Solvers were permitted to enter the competition if they supported at least one of these 16 tracks, but were not obliged to support all of them.2.3. Argumentation, answer-set programming and satisfiability solversIn the following, we provide some basics about Answer-set Programming and Satisfiability solvers. This background is intended to support the reader in understanding the main insights of the competition solvers, described in Section 4, implementing such encodings.2.3.1. Answer-set programmingAnswer set programming (ASP) [57] is a modern approach to declarative programming, where a user focuses on declar-atively specifying her problem. ASP has its roots in deductive databases, logic programming, logic-based knowledge repre-sentation and reasoning, constraint solving, and satisfiability testing. It can be applied in a uniform way to search problems in the classes P, NP, and NPNP in applications like planning, decision support, model checking, and many more.As discussed in [93], ASP relies upon:1. the representation of knowledge in terms of disjunctive logic programs with negation as failure (possibly including explicit negation and various forms of constraints),2. the interpretation of these logic programs under the stable model/answer set semantics and its extensions (dealing with explicit negation and constraints), and3. efficient computational mechanisms, called ASP solvers, to compute answer sets for grounded logic programs.We fix a countable set U of domain elements, called constants. An atom is an expression p(t1, . . . , tn), where p is a predicate of arity n ≥ 0, and each ti is either a variable or an element from U . An atom is called ground if it is free of variables. BUdenotes the set of all ground atoms over U .A disjunctive rule r is of the forma1| . . . |an ← b1, . . . , bk, not bk+1, . . . , not bmwith n ≥ 0, m ≥ k ≥ 0, n + m > 0, where a1, . . . , an, b1, . . . , bm are literals, and not represents default negation. The headof r is the set H(r) = {a1, . . . , an} and the body of r is B(r) = {b1, . . . , bk, not bk+1, . . . , not bm}. Furthermore, we have that −(r) = {bk+1, . . . , bm}. A rule r is normal if n ≤ 1 and a constraint is normal if n = 0. A rule r+(r) = {b1, . . . , bk} and BB+(r). A rule r is ground if no variable occurs in r. A fact is a ground rule without is safe if each variable in r occurs in Bdisjunction and empty body. An input database is a set of facts. A program is a finite set of disjunctive rules. For a program π and an input database D, we write π (D) instead of D ∪ π . If each rule in a program is normal (resp. ground), we call the program normal (resp. ground). For any program π , let U π be the set of all constants appearing in π . Gr(π ) is the set of rules rσ obtained by applying, to each rule r ∈ π , all possible substitutions σ from the variables in r to elements of U π . An −(r) ∩ I = ∅. An interpretation interpretation I ⊆ BU satisfies a ground rule r iff H(r) ∩ I (cid:14)= ∅ whenever BI satisfies a ground program π , if each r ∈ π is satisfied by I. A non-ground rule r (resp., a program π ) is satisfied by an interpretation I iff I satisfies all groundings of r (resp., Gr(π )). We have that I ⊆ BU is an answer set of π iff it is a subset-minimal set satisfying the Gelfond–Lifschitz reduct π I = {H(r) ← B−(r) = ∅, r ∈ Gr(π )}. For a program π , we denote the set of its answer sets by A S(π ).+(r) ⊆ I and B+(r)|I ∩ B272M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294ASP is particularly well-suited for enumeration problems since these systems enumerate by default all solutions of a given program, thus enabling the enumeration of extensions of an abstract argumentation framework in an easy manner. Moreover, disjunctive ASP is capable of expressing problems being even complete for the second level of the polynomial hierarchy, which is of interest for abstract argumentation considering that several semantics such as the preferred semantics are of this high complexity, cf. the previous section.Several approaches have been proposed in the literature for computing the extensions of abstract argumentation frame-works using ASP solvers, e.g., [76,47,53,99]. All these approaches rely upon the mapping of an argumentation framework into a logic program whose answer sets are in one-to-one correspondence with the extensions of the original abstract argumentation framework. The approaches differ in the kinds of extensions they focus on, and in the mappings and corre-spondences they define. For an exhaustive overview, we refer the reader to [93]. In Section 4, we will provide the specific features of the ASP solvers which participated in the competition.2.3.2. Satisfiability solversA propositional formula over a set of Boolean variables is satisfiable iff there exists a truth assignment of the vari-ables such that the formula evaluates to true. Checking whether such an assignment exists is the satisfiability (SAT) problem [101]. SAT solvers largely owe their success to efficient search heuristics (e.g., [75]) and conflict-driven back-tracking [86].(cid:2)Let us consider the standard setting of propositional logic over a set P = {a, b, c, . . .} of propositional atoms, and the standard logical connectives ∧, ∨, ¬, denoting conjunction, disjunction, and negation, respectively. A literal is an atom p ∈ Por its negation ¬p. A clause C is a set of literals representing the disjunction l∈C l. A propositional formula in Conjunctive Normal Form (CNF) is a conjunction of clauses. An interpretation I : P → {true, f alse} maps atoms to Boolean values. An interpretation I satisfies a formula ϕ (I |= ϕ) if ϕ evaluates to true under the assignment determined by I. A formula ϕ is satisfiable if there exists an interpretation I such that I |= ϕ, and unsatisfiable otherwise. A satisfiability solver is a decision procedure which determines whether a given formula ϕ, in CNF, is satisfiable or not. State-of-the-art SAT solvers are capable of solving instances with hundreds of thousands of literals and clauses. SAT solvers operate in the following way: conflict clauses derived from a previous instance ϕ can be retained in a subsequent run of the solver on a formula ψ if ϕ ⊆ ψ . Moreover, the back-tracking capabilities of SAT solvers make it possible to fix a tentative assignment (or assumption, respectively) for a subset S of A in form of a conjunction of literals over S. Assumptions can be discarded in subsequent calls. This capability to perform iterative calls is crucial to the performance of the SAT-based algorithms proposed for abstract argumentation problems.One method for using SAT solvers in abstract argumentation is to reduce the argumentation problem at hand to a formula in propositional logic. Reductions of this kind make sophisticated SAT solvers amenable for the field of argumentation. Using classical propositional logic to evaluate abstract argumentation frameworks was first advocated by [7] and then extended to quantified propositional logic [1,49] to efficiently reduce abstract argumentation problems with complexity beyond NP. Several implementations show how modern SAT technology can be used for solving such hard problems in the area of argumentation. In Section 4, we will provide the specific features of the SAT solvers which participated to the competition.3. Technical setup and evaluationIn this section, we give an overview on how the benchmarks for the competition were generated (Section 3.1), describe the evaluation measures (Section 3.2), and give some details on the execution of the competition (Section 3.3).3.1. BenchmarksThe availability of real-world benchmarks for argumentation problems was, at the point of time of the competition, quite limited, some few exceptions are [20,19] or AIFdb.1 However, these benchmarks are tailored towards problems of argument mining [102] and their representation as abstract argumentation frameworks usually lead to topologically simple graphs, such as cycle-free graphs. These kinds of graphs are not suitable for comparing the computational performance of solvers for abstract argumentation problems, as, e.g., all classical semantics coincide with grounded semantics on cycle-free graphs [35], for which all considered computational problems are tractable, cf. Section 2.3. Another possibility to obtain benchmark examples is to utilize other problem areas such as automatic planning, satisfiability, or other reasoning prob-lems and encode these problems as abstract argumentation frameworks. Although these problem transformations generally lead to complex and challenging graphs, they are all structurally similar. As a consequence, solvers optimized for specific graph-theoretic features may have an advantage over other solvers. In order to be able to distinguish the computational performance on all considered semantics and on different graph-theoretic features, we decided to use artificially generated graphs as benchmarks, in line with the preliminary performance evaluation of [10].In order to provide challenging benchmarks for the abstract argumentation setting, we created three different graph generators called GroundedGenerator, StableGenerator, and SccGenerator, each addressing different aspects 1 http :/ /corpora .aifdb .org.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294273of computationally hard benchmark graphs. These graph generators implement heuristic algorithms for generating graphs with specific features such as a large number or size of extensions of a specific semantics. Another possibility to generate graphs for argumentation problems would have been to use exact methods such as algorithms solving the realizability problem [37] or the recently proposed method for synthesizing frameworks of [77]. Given a specific set of extensions these methods would construct an argumentation framework with exactly this set of extensions. We decided to use heuristic algorithms instead of these exact methods because of the following two reasons. First, exact methods rely on a deterministic approach to construct an argumentation framework. Consequently, the generated graphs possess similar graph-theoretic features that could be exploited by specific solvers. Although it is possible to alter these algorithms in order to incorporate some randomness this would then lead to heuristic algorithms as well that do not necessarily give the desired result. Second, solving the realizability problem is computationally hard. Initial experiments showed that it was more feasible to run our heuristic algorithms and test whether the resulting graphs have sufficiently good characteristics. We will now briefly outline algorithms for these generators and the features of graphs generated by the algorithms. Pseudo code formalizations of the algorithms can be found in Appendix A.A: GroundedGenerator This graph generator aims at generating graphs with a large grounded extension. As all exten-sions of all considered semantics always contain the grounded extension [35], graphs generated by this generator test whether solvers can exploit this property to efficiently compute extensions.Given some upper bound “maxA” for the number of arguments and some predefined probability “p”, this generator first randomly determines the actual number “A” of arguments (uniformly distributed in {1, . . . , maxA}) which are named a1, . . . , a A . Afterwards, for each pair i, j = 1, . . . , A with j < i an attack between ai and a j is added with probability “p”. The resulting intermediate graph component is therefore guaranteed to be acyclic and possibly not fully connected. Afterwards random attacks are added between the not-yet connected arguments and the graph component from before (uniformly distributed).B: StableGenerator This graph generator aims at generating graphs with many stable extensions (and therefore also many complete and preferred extensions). Graphs generated by this generator pose huge combinatorial challenges for solvers addressing the computational tasks of determining (skeptical or credulous) acceptance of arguments and enumerating extensions.After having determined the number of arguments “A” as in GroundedGenerator, this generator first identifies a set of arguments grounded to form an acyclic subgraph which will contain the grounded extension. Afterwards another subset M (a candidate for a stable extension) of arguments is randomly selected and attacks are randomly added from some arguments within M to all arguments neither in M nor grounded. This process is repeated until a number of desired stable extensions M is reached.C: SccGenerator The third graph generator aims at generating graphs which feature many strongly-connected compo-nents and are therefore challenging for solvers which do not rely on decomposition techniques [72].After having determined the number of arguments “A” as in GroundedGenerator, in a first step these arguments are partitioned (with a uniform distribution) into a given number N of components C1, . . . , Cn. Within each component attacks are added randomly with a high probability given as a parameter (and thus likely forming a strongly connected component). In-between components attacks are randomly added with less probability (also given as parameter), but only from a component Ci to C j with i > j (in order to avoid having few large strongly connected components).The source code for the above generators can be found in the source code repository2 of probo [25], the benchmark suite used to run the competition, which will be discussed in more detail in Section 3.3. In contrast to the preliminary perfor-mance evaluation of [10], we decided to use these proprietary graph generators, instead of well-known graph models from network theory such as the Erdös–Rényi [52], Watts–Strogatz [100], or Barabási–Albert [3] models, because of the following reason. Graph models from network theory are designed to explain the topology of e.g. social networks. An important con-cept often (indirectly) implemented in graph models is that of triangle closure, i.e., the tendency of nodes directly connecting to the neighbors of its neighbors (as in the saying “the friend of my friend is also my friend”). From the perspective of challenging benchmarks for abstract argumentation, this feature often trivializes computation. Initial experiments suggest that the generated graphs contain empty or very small grounded extensions, usually no stable extensions, and very few and small complete and preferred extensions. The latter observation is also due to the fact that these graph models aim at modeling the “small world” property of many real-world graphs.3 This leads to many arguments directly or indirectly being in conflict with each other.For each of the three benchmark generators A, B, and C, we generated 72 argumentation graphs of different sizes and partitioned each set into three equal-sized subsets of small, medium, and large instances. This results in 9 test sets, each having 24 argumentation graphs (see Appendix B for the exact numbers of arguments and attacks). We conducted some 2 http :/ /sourceforge .net /p /probo /code /HEAD /tree /trunk /src /net /sf /probo /generators/.3 This property basically states that there are always “relatively short” paths from any node to every other node [100]; for example the theory of “six degrees of separation” suggests that in the social network of the known world the longest shortest path between any two persons is six.274M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294preliminary experiments using alpha versions of available solvers in order to check whether these graphs are not too easy or too hard. There we discovered that the test set corresponding to the largest argumentation graphs generated by B was too difficult for every solver. As a consequence, the whole test set was removed from the evaluation. All other test sets seemed to be appropriate to be used for the actual competition. Therefore, the 192 instances used for the evaluation in the competition consisted of• 24 small-sized argumentation graphs from generator A (test set 1)• 24 medium-sized argumentation graphs from generator A (test set 2)• 24 large-sized argumentation graphs from generator A (test set 3)• 24 small-sized argumentation graphs from generator B (test set 4)• 24 medium-sized argumentation graphs from generator B (test set 5)• 24 small-sized argumentation graphs from generator C (test set 6)• 24 medium-sized argumentation graphs from generator C (test set 7)• 24 large-sized argumentation graphs from generator C (test set 8)All argumentation graphs of the competition can be downloaded from the competition website.4 Appendix B gives some more detailed statistics on the benchmark graphs. For a discussion on the relationships between these statistics and argumentation-specific properties see [95].3.2. Evaluation measuresThe aim of the competition was to measure and compare the computational performance of the submitted solvers on solving instances of the problems presented in Section 2.3. For the problems SE (compute some extension) and EE (compute all extensions) we used every one of the 192 argumentation graphs (see previous section) as an individual instance for each semantics. For the problems DS (decide skeptical acceptance) and DC (decide credulous acceptance) we randomly selected three arguments from every argumentation graph, yielding in total 576 instances for each semantics.For each instance of a track, a solver was given 10 minutes time to compute the answer. In case of a timeout or a wrong answer, the solver received zero points for this instance.5 If it gave the correct answer within the time limit, it received one point and the actual runtime for solving the instance was saved separately. For each track, the cumulative number of points was used as the main ranking criterion (solvers which received more points were ranked higher than solvers with less points). If two or more solvers reached the same number of points, their cumulative runtimes on all correctly solved instances were compared to break ties (solver with smaller total runtime were ranked higher than solvers with larger total runtime). This procedure results in a total of 16 rankings of the solvers, one for each track.For those solvers, which supported all 16 tracks of the competition, we aggregated their scores in the individual tracks to obtain a global ranking using Borda count. For that, every solver received one point for every first place in any ranking, two points for every second place in any ranking, and so on. A global ranking was obtained by ordering the resulting total number of points from smallest to largest.3.3. Competition detailsThe competition was realized using the benchmark framework probo [25], which provides the possibility to run the instances on the individual solvers, verify the results, measure the runtime, and log the results accordingly. The softwareprobo is written in Java and requires the implementation of a simple command line interface from the participating solvers.6All benchmark graphs were made available in two file formats. The trivial graph format7 (TGF) is a simple representation of a directed graph which simply lists all appearing vertices and edges. The ASPARTIX format (APX) [47] is an abstract argumentation-specific format which represents an argumentation framework as facts in a logic programming-like way.In order to verify the answers of solvers, the solutions for all instances were computed in advance using the Tweety libraries for logical aspects of artificial intelligence and knowledge representation [91]. Tweety contains naive algorithms for all considered semantics that implement the formal definitions of all semantics in a straightforward manner and thus provides verified reference implementations for all considered problems.Besides serving as the benchmark framework for executing the competition, probo also contains several abstract classes and interfaces for solver specification that could be used by participants in order to easily comply with the solver interface 4 http :/ /argumentationcompetition .org /2015 /iccma2015 _benchmarks .zip, note that the test sets are numbered differently in the competition and on the website; in particular, test sets 6, 7, and 8 from above are numbered 7, 8, and 9 there to accommodate for the removed test set 6.5 The initial policy for wrong answers was to disqualify the solver completely for the track. However, quite a few solvers occasionally produced wrong results and in order to provide a comprehensive picture on the state-of-the-art we revised this policy; the final results would differ only slightly when enforcing this policy though (see Tables 5–8).6 See http :/ /argumentationcompetition .org /2015 /iccma15notes _v3 .pdf for the formal interface description.7 http :/ /en .wikipedia .org /wiki /Trivial _Graph _Format.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294275Table 2Supported tracks of the participating solvers.No.SolverSE(cid:2)COLabSATSolver (cid:2)(cid:2)ArgSemSAT(cid:2)ArgToolsCegartixDungellZJU-ARGASPARTIX-VCoQuiAASASPARTIX-DConArgGRISASGLLamatzSolverProGraphDIAMONDCarneadesprefMaxSATASSA(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)123456789101112131415161718PRGRSTEECOPRGRSTDCCOPRGRSTDSCO(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)PRGR(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2)(cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)(cid:2) (cid:2)ST(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)specification. We also provided a tutorial8 and a generic shell-script9 that implements the solver interface specification, in order to enable participants to implement their solvers in a way that is compatible with the competition requirements.The competition itself was executed on a cloud computing platform available at the University of Koblenz–Landau, which provides 320 cores with 2.9 GHz each and 2.2 TB of usable RAM. For each test set of benchmark graphs we used a single virtual machine with 1 CPU and 8 GB of RAM to run all tracks on the set. The results for all tracks were aggregated afterwards.4. ParticipantsIn this section, we provide a description of the solvers which participated in the competition, and we classify them with respect to their supported tracks. Note that the solvers are numbered according to their registration number.Table 2 gives an overview on the participating solvers, and their supported tracks.Table 3 gives some further information on the solvers, i.e., development country, programming language and paradigm, total number of lines of code, and a reference to contributions describing the solver in more detail. Most of the solvers have been developed in Europe with the exception of the ZJU-ARG solver from China and the ArgTools solver from Jordan. Moreover, we can note that solvers have been developed mainly using logic and object-oriented programming languages.Table 4 lists the solvers’ license information (when available), and provides a link to their source code repositories. Most of the repositories are available under the GNU GPL license, while some of them chose more specific licenses like the MIT license. In general, however, the source code of all solvers participating in the competition has been made available for research purposes, which was also a requirement for participating.In the following, we give some details on the individual solvers that participated in the competition. For the complete system descriptions, we refer the reader to [92].LabSATSolverThe LabSAT solver [17] solves all tasks of the competition wrt. all semantics. It encodes the labeling approach [21] as a boolean satisfiability problem (SAT) following the proposal of Cerutti and colleagues [23]. Roughly, the approach proposed in [23], called PrefSAT, is a depth-first search in the space of complete extensions to identify those that are maximal, namely preferred extensions and enumerate them. Each step of the search process requires the solution of a SAT problem through the invocation of a SAT solver. The algorithm is based on the idea of encoding the constraints corresponding to complete labelings of an AF [21] as a SAT problem, and then iteratively producing and solving modified versions of the initial SAT problem according to the needs of the search process. For more details about the encoding, we refer the reader to [23]. Complete and preferred extensions are computed by the LabSAT solver using the PrefSAT approach [23]. To compute the stable extension, additional clauses are added to the SAT solver (i.e., the label undec is excluded). The SAT solver used in LabSAT is lingeling [9]. It is worth noticing that the grounded semantics is computed without the use of the SAT solver, relying on a Java implementation of the algorithm proposed in [74].8 http :/ /argumentationcompetition .org /2015 /iccma15probotutorial _v2 .pdf.9 http :/ /sourceforge .net /p /probo /code /HEAD /tree /trunk /solvers /generic-interface .sh ?format =raw.276M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Table 3Detailed information of the participating solvers.No.SolverCountryProgramming languageProgramming paradigm# Lines of codeReference123456789101112131415161718LabSATArgSemSATArgToolsCegartixDungellZJU-ARGASPARTIX-VCoQuiAASASPARTIX-DConArg2GRISASGLLamatzSolverProGraphDIAMONDCarneadesprefMaxSATASSAGermanyUK, ItalyJordan, UKAustria, FinlandUKChina, LuxembourgItaly, Finland, AustriaFranceGermanyItalyUKGermanyGermanyRomaniaGermanyGermanyUK, ItalyCyprusJavaC++C++C++HaskellJavaASPC++ASPC++C++LispJavaPrologPython/ASPGoC++Javaobject-orientedobject-orientedobject-orientedobject-orienteddirect/functionalobject-orientedlogicobject-orientedlogicobject-orientedobject-orientedfunctionalobject-orientedlogiclogicN/Aobject-orientedobject-oriented13007800600085024021007501600640364421909005503054201242750N/A[17][26][79][45][97][62][85][66][54][13][83][89][68][61][51][58][94][63]Table 4Licenses and source code repositories of the participating solvers.No.SolverLicenseSource code repository123456789101112131415161718LabSATArgSemSATArgToolsCegartixDungellZJU-ARGASPARTIX-VCoQuiAASASPARTIX-DConArgGRISASGLLamatzSolverProGraphDIAMONDCarneadesprefMaxSATASSAGNU LGPMITGNU GPLGNU GPLBSD3N/AN/AGNU GPLad-hocN/AGNU GPLad-hocN/AN/AGNU GPLMPL 2.0MITN/Ahttps://github.com/fbrns/LabSATSolverhttp://sourceforge.net/projects/argsemsat/http://sourceforge.net/projects/argtools/http://www.dbai.tuwien.ac.at/research/project/argumentation/cegartix/http://www.cs.nott.ac.uk/~psxbv/DungICCMA/http://mypage.zju.edu.cn/en/beishui/685664.htmlhttp://www.dbai.tuwien.ac.at/proj/argumentation/systempage/http://www.cril.univ-artois.fr/coquiaas/https://ddll.inf.tu-dresden.de/web/Sarah_Alice_Gaggl/ASPARTIX-Dhttp://www.dmi.unipg.it/conarg/http://www.inf.kcl.ac.uk/staff/odinaldo/gris/https://github.com/kisp/asglhttps://bitbucket.org/Cloudkenny/lamatzsolver/http://cs-gw.utcluj.ro/~adrian/tools/prograph/ProGraph-ArgComp2015.tarhttp://diamond-adf.sourceforge.net/https://carneades.github.io/http://sourceforge.net/projects/prefmaxsat/http://www.mertjiandata.com/assa.htmlArgSemSATThe ArgSemSAT solver [26,23,28,29] implements a collection of algorithms for solving all tasks of the competition wrt. all semantics. ArgSemSAT-1.0 encodes the constraints corresponding to complete labelings of an AF as a SAT problem, and then iteratively produces and solves modified versions of the initial SAT problem according to the needs of the search pro-cess. As for the LabSAT solver, also ArgSemSAT implements the PrefSAT approach [23] described above, for enumerating the preferred extensions. PrefSAT first solves a SAT problem whose solutions correspond to the complete extensions of an AF, and second, a hill-climbing approach is used to find a maximal wrt. set inclusion complete extension, i.e., a preferred extension. Already computed extensions are excluded from subsequent search steps. In addition, ArgSemSAT-1.0 implements the SCC-P algorithm [24] exploiting the SCC-recursiveness schema [5] using the partial order of strongly connected com-ponents (SCCs). In SCC-P, the extensions of the frameworks restricted to the SCC not receiving any attack are computed and combined together. Then, each SCC which is attacked only by such unattacked SCCs is considered: the extensions of such a SCC are computed and merged with those already obtained. Finally, the subsequent (wrt. the partial order) SCCs are considered until no remaining SCCs are left. The schema is recursive, and the base of the recursion is reached when there is only one SCC: in this case a solver similar to PrefSAT is called. It is worth noticing that SCC-P resulted to be more efficient than PrefSAT on AFs with numerous SCCs. ArgSemSAT-1.0 exploits the Glucose solver [2] and the PrecoSAT10 solver.10 http :/ /fmv.jku .at /precosat/.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294277ArgToolsThe ArgTools solver (Argumentation Tools) [79,78] is a system based on backtracking algorithms for solving all tasks of the competition wrt. all semantics. The backtracking algorithms of ArgTools are based on exploration of an abstract binary search tree. The two key features of ArgTools are i) to enhance the backtracking search for sets of acceptable arguments by a new pruning strategy, called the global looking-ahead strategy, and ii) to set out a backtracking-based approach to decide acceptance under different semantics, i.e., whether an argument is in some/all set(s) of acceptable arguments of a given AF, without necessarily enumerating all such sets. Roughly, the global looking-ahead pruning strategy enables a backtracking procedure during traversing the search space to regularly look-ahead for dead-ends, i.e., for paths that do not lead to solutions, early enough such that considerable time is saved. For more details about this strategy, we refer the reader to [80].CEGARTIXThe CEGARTIX (Counter-Example Guided Argumentation Reasoning Tool) solver [45,43] supports the computation of credulous acceptance under semi-stable, and stage semantics, the skeptical acceptance under preferred, semi-stable, and stage semantics, it returns an arbitrary preferred extension, and enumerates all preferred extensions. Note that only the part regarding the preferred semantics is relevant for the participation in the competition. Each step in the exploration is delegated to a complete Boolean satisfiability (SAT) solver. The strategy exploited by this solver consists first in the identification of first-level fragments (NP/coNP layer) of second-level reasoning tasks for two main reasons: i) such fragments present particular sources of complexity of the considered problems, and ii) NP fragments can be efficiently reduced to the SAT problem. CEGARTIX uses the NP decision procedures as NP oracles in an iterative way. For problems complete for the second level of the polynomial hierarchy, this leads to general procedures which, in the worst case, require an exponential number of calls to the NP oracle, which is indeed unavoidable under the assumption that the polynomial hierarchy does not collapse. Nevertheless, such procedures can be designed to behave adequately on input instances that fall into the considered NP fragment and on instances for which a relatively low number of oracle calls is sufficient. CEGARTIX exploits current state-of-the-art conflict-driven clause learning (CDCL) SAT-solver technology as the underlying NP oracle. CEGARTIX employs the CDCL SAT-solver Minisat [46]. For more details about the NP decision procedure, we refer the reader to [43].DungellThe Dungell solver [97,98] supports the computation of some and all grounded, complete, preferred and stable exten-sions. The characterizing feature of Dungell consists in providing a solver that is as close to the mathematical definitions as possible. The rationale behind this feature is to tackle the problem of implementing structured argumentation models and their translations by providing a framework that allows implementation close to the mathematical specification and thus facilitates checking and formal proof of properties. Dungell implements two steps in the pipeline: first, it allows for the translation of a Carneades [59] structured argumentation framework into an abstract one, and second, it computes the extensions for the grounded, complete, preferred and stable semantics. Given an AF, it is possible to verify whether a list of arguments is conflict-free by checking that the list of attacks between those arguments is empty. Acceptability of an argu-ment with respect to a set of arguments in an AF can be determined by verifying that all its attackers are in turn attacked by an attacker in that set. This solver is one of the very few ones using functional programming, specifically Haskell, for the implementation of structured and abstract models of argumentation.ZJU-ARGThe ZJU-ARG solver [62,70] enumerates all preferred extensions, and the grounded extension of an AF. It adopts a divide-and-conquer strategy. As for the LabSATSolver, the grounded extension of an AF is computed directly by following the algorithm proposed by [74]. The main feature of the ZJU-ARG solver is the application of the notion of modularity to an argumentation framework, close to the SCC concept, as for other solvers like ArgTools. To overcome the fact that the efficiency of the SCC approach is highly limited by the maximal SCC of an AF, the solver implements a solution by exploiting the most skeptically rejected arguments of an AF. Roughly, given an AF, its grounded labeling [21] is first generated. Then, the attacks between the undecided arguments and the rejected arguments are removed. It turns out that the modified AFhas the same preferred labeling as the original AF, but the maximal SCC in it could be much smaller than that of the original AF. Since the ZJU-ARG solver adopts a divide-and-conquer strategy without employing more efficient algorithms to compute the semantics of each sub-framework, its efficiency highly depends on the topologies of the argumentation frameworks in input.ASPARTIX-VThe ASPARTIX-V solver (Answer Set Programming Argumentation Reasoning Tool–Vienna version) [85,48] supports the computation of skeptical acceptance under preferred semantics, returns a single preferred extension, and enumerates all preferred extensions. Together with an ASP encoding for preferred semantics, the answer-sets are in a 1-to-1 correspon-dence with the preferred extensions of the given argumentation framework AF. ASP solvers themselves offer enumeration of all answer-sets and returning an arbitrary one. In ASPARTIX-V, a single program is used to encode a particular argumen-tation semantics, while the instance of an argumentation framework is given as an input database. ASPARTIX-V improves the 278M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294performances of its predecessor ASPARTIX11 as follows. While preferred semantics is encoded as a disjunctive logic program making heavy use of so-called loop constructs in ASP in the ASPARTIX system, ASPARTIX-V is able to do without and uses conditional literals for enhancing the performance. Intuitively, conditional literals allow to use, e.g., a dynamic head in a disjunctive rule that contains a literal iff its condition is true. The loop constructs can be avoided by alternative charac-terizations of preferred semantics. ASPARTIX-V employs the so-called saturation technique: in the encodings for preferred semantics, a first “guess” is made for a set of arguments in the AF, and then the solver verifies if this set is admissible. To verify if this set is also a subset-maximal admissible one, ASPARTIX-V performs a second guess and verifies if this second guess is an admissible set that is a superset of the first guess. The idea is to keep this second guess small to overcome computational overhead. Additional rules then verify if the witness set represents an admissible set that may be combined with the first guess to result in a larger admissible set. If this is the case, the first guess does not represent a preferred extension. The underlying ASP solver is Clingo 4.4 [56].CoQuiAASThe CoQuiAAS solver [66,67] solves all tasks of the competition wrt. all semantics by exploiting constraint programming techniques. More precisely, it takes advantage of the encodings proposed by [7]. CoQuiAAS deals with encodings in Nega-tion Normal Form (NNF) formulae, meaning some propositional formulae where the negation operator is only applied on variables. As CoQuiAAS uses SAT solvers, which are only able to tackle propositional formulae in CNF, a translation step from NNF to CNF is required between the encodings which exist in the literature and the ones that used in the system. An interesting question for SAT solvers is to determine an interpretation which maximizes the number of satisfied constraints: this problem is called Max-SAT. We can generalize this problem, giving a weight to each constraint (Weighted Max-SAT), and if some constraints have an infinite weight (i.e., they have to be satisfied), then the problems are said to be “partial” (Partial Max-SAT). CoQuiAAS uses CNF formulae to solve problems from the first level of the polynomial hierarchy, and some encodings in the Partial Max-SAT formalism for higher complexity problems. Discovering an optimal solution of a Max-SAT instance allows to determine a set of constraints from the initial formula which is consistent, such that adding any other constraint from the initial problem makes this new problem inconsistent: a set of constraints which has this property is called a maximal satisfiable subset (MSS). The optimal solutions of the Max-SAT problem are only a subset of all the MSS of a formula. The solver approaches argumentation semantics exploiting SAT and MSS extraction. CoQuiAAS incorporates the software coMSSExtractor [60] to perform the constraint-based process.ASPARTIX-DThe ASPARTIX-D solver (Answer Set Programming Argumentation Reasoning Tool–Dresden version) [54] is a collection of ASP encodings together with dedicated solvers to solve all tasks of the competition wrt. all semantics. The general approach of ASPARTIX-D is the same the approach of ASPARTIX-V described above but differs in several details. In particular, the ASP encodings used by ASPARTIX-D are those described in [48] and the optimization applied has been presented in [42]. The main aim of the solver is to find the most suitable encodings and solver configuration. ASPARTIX-D exploits the potassco ASP solvers.12ConArgThe ConArg (Argumentation with Constraints) solver (version 2.0) [13,15] allows to enumerate all conflict-free, admis-sible, complete, stable, grounded, preferred, semi-stable, ideal, and stage extensions, to return one extension given one of these semantics, to check the credulous and skeptical acceptance for the conflict-free, admissible, complete, and stable semantics. It is a constraint programming tool where the properties of the semantics are encoded into constraints, and arguments are assigned to 1 (true) if they belong to a valid extension for that semantics, and 0 otherwise. Searching for solutions takes advantage of classical techniques, such as local consistency through constraint propagation, different heuris-tics for trying to assign values to variables, and a complete search-tree with branch-and-bound. To map an argumentation framework AF to a Constraint Satisfaction Problem (CSP),13 which is defined by a set of constraints defined over the a set of variables each with domain D, ConArg defines a variable for each argument in the AF, and each of these arguments can be taken or not in an extension, i.e., the domain of each variable is D = {1, 0}. As an example, preferred extensions are found by assigning as more arguments as possible to 1 while searching for complete extensions. The solver exploits a toolkit called Gecode 4.4.014 defined for developing constraint-based systems and applications.GRISThe GRIS (Gabbay–Rodrigues Iterative Solver) solver [83,84] allows to produce one or all of the extensions of the ar-gumentation framework under the grounded and preferred semantics, and given an argument a to decide whether it is accepted credulously or skeptically according to one of these two semantics. The peculiarity of the GRIS solver is that it 11 http :/ /www.dbai .tuwien .ac .at /research /project /argumentation /systempage/.12 http :/ /potassco .sourceforge .net.13 A CSP is a triple P = (cid:17)V , D, C(cid:18), where C is a set of constraints defined over the variables in V , each with domain D.14 http :/ /www.gecode .org.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294279works with numerical argumentation networks where arguments are given initial values in the interval [0, 1] from which equilibrium values are calculated iteratively yielding traditional extensions through the use of a characterization result [83]. An argumentation framework is represented in GRIS by means of a set of equations. As for ArgTools and the ZJU-ARG solver, also GRIS exploits strongly connected components to compute the extensions. More precisely, the solver starts by computing the strongly connected components of the network and arranging them into layers that can be used in successive computa-tion steps, following the idea of [71]. Once the layers are computed, the solver can identify the deepest layer of computation needed according to the layer depth of the input argument and this can be used to terminate the computation of decision problems as early as possible.ASGLThe ASGL solver [89] solves all tasks of the competition wrt. all semantics. ASGL uses an extension-based encoding for solutions. As for ConArg, also ASGL casts the argumentation framework as a CSP. Constraints are formalized in a so-called computation space and the algorithm is based on propagation methods to reach a fixpoint. The task of computing some preferred extension is implemented in ASGL like a classical optimization problem with branch-and-bound search (already part of standard Gecode). As soon as one solution is found, all further solutions are constrained to be better than the current solution. If no more solution can be found, the current solution is maximal. To efficiently enumerate all preferred extensions, a filtering over all complete extensions for maximality is performed. The ASGL solver, in one out of the two solvers participating to the Competition, that were written using a functional language (Lisp). It also features an interface to the Lingeling SAT Solver as an alternative solver backend. Also ASGL, together with ConArg, exploits the Gecode generic CSP solver library.LamatzSolverThe LamatzSolver system [68] is a solver for computing the grounded extension of argumentation frameworks based on a direct implementation of the characteristic function [35]. Sets like attacks are implemented as a HashMap in Java. More precisely, the computation of the grounded extension is addressed along with the following steps: it checks if the HashMap typeZero (containing unattacked arguments) is empty. If the answer is positive, than an empty HashMap called the groundedis returned, otherwise the algorithm copies each argument of typeZero to the HashMap grounded. The size of grounded is stored in a parameter prev and for each argument the defended arguments will be determined and added to the grounded. According to this process, the algorithm keeps track of the arguments attacked by the arguments in grounded. These ar-guments are stored in a HashMap out, and the attacks of these arguments are candidates for the grounded extension. The algorithm checks if all attackers of a candidate are defeated. These steps are repeated for the grounded until it does not grow anymore. Finally, the grounded Hash Map is returned.ProGraphThe ProGraph solver [61] allows to compute some extension and decides whether an argument is credulously inferred, both with respect to the stable semantics. The key feature of the ProGraph solver is that of relying on bipartite graphs. More precisely, the set of arguments is partitioned in two classes: in and out. The idea behind is that determining an extension which attacks every argument which is not in that extension can be reduced to a relaxed partitioning problem in which the initial set of arguments is split in two partitions, with the arguments from the second partition being free to attack each other. Arguments are sorted such that they will be placed from the one who attacks the most to the one who attacks the less arguments. The algorithm starts by picking a non-attacked argument and adds it to the attackers extension, and then checks if any of the arguments attacked by the selected one is in the first partition. If this is the case, the algorithm starts backtracking. Otherwise, the arguments attacked by the current argument are added to the second partition. These steps are repeated until all arguments are partitioned or until all paths fail. If only attacked arguments are left, the algorithm chooses one of them and supposes it is not attacked. A verification step stops the algorithm if at some point the attacker is to be placed in the second partition.DIAMONDThe DIAMOND solver [51,50] solves all tasks of the competition wrt. all semantics. DIAMOND employs the declarative programming paradigm of ASP, and the knowledge representation languages implemented in the solver are Abstract Dialec-tical Frameworks (ADFs) [16], which are a generalization of AFs. In addition to the computation of the semantics of an AF, DIAMOND can also compute the semantics for (bipolar) ADFs in various different input formats, decide whether a given ADF is bipolar, or transform an ADF from one representation into another. The encodings for DIAMOND are built in a modular way. To compute the models of an ADF with respect to a semantics, different modules need to be grounded together to get the desired behavior. As for other solvers described above, DIAMOND exploits the potassco ASP solvers.15CarneadesThe Carneades solver [58,59] solves all tasks of the competition wrt. all semantics. Note that Carneades originally in-cluded an implementation of a solver for Dung-like abstract argumentation frameworks, using grounded semantics, despite 15 http :/ /potassco .sourceforge .net.280M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294the focus of the Carneades project has not been abstract argumentation, but rather structured argumentation. The imple-mentation closely follows high-level specifications of abstract argumentation frameworks, and has not been optimized with the exception of the grounded semantics where the implementation keeps track of whether a mutable labeling has changed, in its main loop, and exits the loop when no changes are made, without having to explicitly test whether two labelings are equivalent. The solver’s procedures are implemented for finding the first subset of arguments which satisfy a given predicate and for applying some procedure to each subset. Using functions implementing predicates for complete and stable exten-sions, Carneades finds the first or all complete extensions and filter the complete extensions to find one or more which are also stable. Argument sets are represented as Hash tables, from arguments to Boolean values.prefMaxSATThe prefMaxSAT solver [94,24] allows to compute the extension enumeration problem for the preferred semantics. It implements an encoding of preferred extensions search using unweighted MaxSAT. The algorithm exploited in prefMaxSAT is based on the idea of encoding the constraints corresponding to admissible labelings of an AF as a MaxSAT problem, and then iteratively producing and solving modified versions of the initial problem. If at any one step a variable assignment that maximally satisfies the formula is returned, the corresponding labeling is saved in the list of found preferred extensions. Then a hard clause for eliminating the solution is added to the formula and the process is repeated. If no further solution could be found, prefMaxSAT ends and provides the set of found preferred extensions. Each step of the search process requires the solution of a MaxSAT problem. The AF is encoded in a CNF and is then provided to the MaxSAT solver. If a variable assignment that maximally satisfies the formula is returned, then i) the corresponding labeling is saved in the list of found preferred extensions; ii) a clause for eliminating the solution is added to the CNF; iii) a clause forcing to include different arguments is added to the CNF, and finally, iv) the process is repeated. If the MaxSAT solver returns that no variable assignment satisfies the constraints, prefMaxSAT ends and provides the set of found preferred extensions. As for ArgSemSAT, also prefMaxSAT exploits the Glucose solver [2].ASSAThe ASSA solver [63,64] computes one or all extensions and decide whether an argument is credulously or skeptically inferred with respect to the stable semantics. The solver implements an approach based on mathematical matrix operations to solve abstract argumentation problems. The system first creates a matrix representation of an AF. Then, all possible instances of the selected arguments are created and combined into another matrix S. Based on matrix operations, and more specifically, on left and right matrix multiplication, it is possible to navigate inside the AF to find which arguments attack the other arguments and which arguments are under attack. By constructing a matrix multiplication, it is possible to determine whether a given set of arguments is conflict-free. Using this method, all conflict-free sets are extracted, and based on some comparison to the system output matrices and the matrix S, the system is able to find all stable extensions.5. Results and analysisWe now report on the results of the competition, which evaluated the participating solvers from Section 4 using the methodology described in Section 3.16 Tables 5–8 show the obtained rankings of all solvers per track. Each table gives the rank of the solver per semantics, the number of instances where the solver had a timeout (column “#TO”), the number of incorrectly classified instances (column “#−”), the number of correctly classified instances (column “#+”), and the total runtime for all classified instances (column “RT in ms”). The column “Significant” indicates whether the performance of a solver is significantly superior to the solver ranked right after it, according to a standard Student’s T-Test with significance level 95%, cf. [18]. A “YES” indicates that the solver in the row is indeed significantly superior than the next one, a “NO” indicates that is not the case, and a “–” indicates that a significance test is not applicable—and not necessary—as the next solver correctly solved strictly less instances. Solvers are grouped by the number of correctly classified instances and ranked in each group by runtime. Therefore, note that the column on runtime is not sorted across the whole table, as solvers, which solved fewer instances within the time limit, may have a smaller total runtime on the remaining instances as solvers which solved more instances. Furthermore, solvers with identical number of solved instances and identical runtime performance are ranked equally. Table 9 shows the aggregated ranking of solvers participating in all sixteen tracks, where the column “Borda count” gives the sum of all ranks of the particular solver in all tracks.Due to the results depicted in Table 9, the International Competition of Computational Models of Argumentation awarded the following solvers with first, second, and third place, respectively:1. CoQuiAAS2. ArgSemSAT3. LabSATSolver16 The raw results and more detailed statistics can be found at http :/ /argumentationcompetition .org /2015 /results .html.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294281Table 5Results for the problem SE (“some extension”) per semantics (N = 192); “#TO” is the number of timeouts, “#−” is the number of incorrectly classified instances, “#+” is the number of correctly classified instances, “RT in ms” is the total runtime for all classified instances, and “Significant” indicates whether the performance of a solver is significantly better than the next solver (only applicable if both correctly solved the same number of instances).σCOPRGRSTRankSolver#TO#−12345678910123456789101112123456789101112345678910CoQuiAASASGLASPARTIX-DConArgArgSemSATArgToolsLabSATSolverDIAMONDCarneadesDungellCegartixArgSemSATLabSATSolverASPARTIX-VCoQuiAASASGLConArgASPARTIX-DArgToolsGRISDIAMONDCarneadesDungellCoQuiAASCarneadesLabSATSolverArgSemSATArgToolsGRISASGLASPARTIX-DConArgDungellDIAMONDASPARTIX-DArgSemSATLabSATSolverCoQuiAASConArgASGLDIAMONDArgToolsProGraphCarneadesDungellASSA0000002131371370001212151759172192192000000118407200014671446101192192000000012800000000023010315200000000013001770000012749000192#+1921921921921921921891515555192192192191190180177152133723800192192192192192192191161152120151921921911881861841519791000RT in msSignificant301703027304118905059605527901627070406450154525206300374008595901265260172984078754802454510405611053604001237918041576102815300064558900028860884903681107109301654720419140030748078689202365420213950135247602752707399308770002951607235750314742015748650446126011562420007024250YESNONONOYES–––YES–YESNO–––––––––NO–YESYESYESYESYES––––––YES––––––––NO–NOFurthermore, the solver Cegartix additionally received the award “Honorable mention” as it achieved the two first places and one second place in the three tracks it participated in (SE-PR, EE-PR, DS-PR).The statistics on timeouts and runtime performances given in Tables 5–8 show that there is a large diversity between solvers. For example, from the results of the problem SE-PR (Table 5) one can see that there are solvers without any timeout (places 1–3) and solvers not solving any instance within the time limit (place 12). Moreover, the first place (Cegartix) for this track achieved an average runtime of roughly 4,5 seconds on any instance, which is way below the timeout of 10 minutes. Similar observations can be made for the other tracks. Furthermore, many solvers performed quite differently in different tracks, compared to other solvers. For example, solver no. 16 (Carneades) achieved second place for all tracks related to grounded semantics, but only last place in all other tracks. This behavior stems from the fact that some solvers have been developed for a specific semantics (grounded semantics for Carneades), and have not been tailored towards other semantics.All four tracks related to stable semantics have been won by ASPARTIX-D, often with great lead to the second place. For example, for the problem EE-ST it solved one more instance than the second place (ArgSemSAT) but still needed only roughly a third of the total time, cf. Table 6. ASPARTIX-D is based on reductions of abstract argumentation problems to answer set programming—see also Section 4—and therefore exploits the equivalence of stable semantics in abstract ar-282M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Table 6Results for the problem EE (“enumerate extensions”) per semantics (N = 192); “#TO” is the number of timeouts, “#−” is the number of incorrectly classified instances, “#+” is the number of correctly classified instances, “RT in ms” is the total runtime for all classified instances, and “Significant” indicates whether the performance of a solver is significantly better than the next solver (only applicable if both correctly solved the same number of instances).σCOPRGRSTRankSolver#TO#−123456789123456789101112131412345678910111213123456789ASPARTIX-DArgSemSATCoQuiAASLabSATSolverASGLConArgDIAMONDArgToolsCarneadesDungellCegartixArgSemSATCoQuiAASASPARTIX-VLabSATSolverprefMaxSATASGLASPARTIX-DConArgArgToolsZJU-ARGGRISDIAMONDCarneadesDungellCoQuiAASCarneadesLamatzSolverLabSATSolverArgSemSATZJU-ARGArgToolsGRISASGLASPARTIX-DConArgDungellDIAMONDASPARTIX-DArgSemSATCoQuiAASASGLConArgArgToolsLabSATSolverDIAMONDCarneadesDungellASSA5910730422271192192122332729184665100169192192000000001174072012411115711419219201003003600000002002300171041410000000000014001770000007410400192#+18618318218216215013412100191190190189187165163151146127757242001921921921921921921921921911611521201519119018818118113511774000RT in msSignificant104081045184201776270263152079798202459560209495902612080001520400356378048966109926900477595068638506116050133811502650920304835011311302821473094978600030390870002870203385406917808012001660070418435030481085722002353550212280135974105756201708400620350814739083210002371760153024019026710006939650––NO–––––NO––NO––––––––-––NO–YESYESNOYESNOYESYES–––––––––NO––––NO–NOgumentation to answer set semantics in a direct way. Note, that also the solver DIAMOND is based on an answer set programming reduction. In contrast to ASPARTIX-D, its approach is, however, actually a two-level reduction. In a first step, abstract argumentation problems are reduced to an equivalent formalization using Abstract Dialectical Frameworks [16]. In the second step, the latter is then reduced to an answer set program. This overhead is a probable cause for the lower ranking of this solver. In addition, DIAMOND shows the higher total runtime for all incorrect classified instances, meaning that this two-level approach has a serious impact on the performances of the solver. Notice that the second solver for higher total runtime for all incorrect classified instances is ASPARTIX-D, even if the impact on the overall performance of the solver is less significant than for DIAMOND.Despite the exception from above, it can be seen that solvers that rely on a reduction to other established formalisms, such as SAT solving, constraint satisfaction problems, or, as mentioned, answer set programming, performed better than solvers implementing a direct algorithm for abstract argumentation. In fact, the first three places in Table 9 (CoQuiAAS, M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294283Table 7Results for the problem DC (“decide credulous”) per semantics (N = 576); “#TO” is the number of timeouts, “#−” is the number of incorrectly classified instances, “#+” is the number of correctly classified instances, “RT in ms” is the total runtime for all classified instances, and “Significant” indicates whether the performance of a solver is significantly better than the next solver (only applicable if both correctly solved the same number of instances).σCOPRGRSTRankSolver#TO#−1234567891234567891234567891234567891011ArgSemSATASPARTIX-DLabSATSolverCoQuiAASASGLConArgDIAMONDArgToolsCarneadesArgSemSATLabSATSolverCoQuiAASASGLDIAMONDGRISArgToolsASPARTIX-DCarneadesCoQuiAASCarneadesLabSATSolverASGLArgSemSATArgToolsGRISDIAMONDASPARTIX-DASPARTIX-DArgSemSATLabSATSolverCoQuiAASConArgASGLDIAMONDASSAArgToolsProGraphCarneades001382129935760025248931445760000000044001257300892465760000005000000124006000000002210000002460180#+576576575573568555542483057657657457156248848342605765765765765765765765545315765765755745715695445304873120RT in msSignificant101806025302801705780439500849578013877500481887901027862008849601992860412620884157066137810694193008939600183189500925802484308859509672002140360503613013246200393505102399694051339077982012345201703904658410489135046999150209662209716900373771900YES––––––––YES––––––––YESYESNOYESYESYES–––YES––––––––––ArgSemSAT, and LabSATSolver) and the honorable mention Cegartix rely on reductions to (maximum) satisfiability problems and make use of mature SAT solvers for solving argumentation problems, and all first places in all tracks use one of the three reductions mentioned above. Solvers using direct algorithms—i.e. solvers not using any other formalism than abstract argumentation—such as ArgTools and Carneades usually performed below average.Tables 10 and 11 show the performance of the solvers wrt. the different test sets, accumulated over all tracks. For each solver, the column N in each indicates the number of instances solved for each test set (be reminded that each test set contains 24 benchmark graphs and that for DS and DC problems each benchmark graph was tested three times). For each test set generated by the generators A, B, and C the corresponding cells contain the number of incorrectly classified instances and timeouts wrt. the given total number of instances N. Table 10 gives several interesting insights into the behavior of the solvers wrt. different characteristics. For example, considering solver no. 9 (ASPARTIX-D), it can be seen that it had a hard time solving instances generated by the graph generator A (which featured large graphs with a large grounded extension), but was significantly better in solving instances of graph generators B and C (both generated smaller graphs but with a more complex attack structure). However, other solvers such as no. 3 (ArgTools) and no. 10 (ConArg) featured quite the opposite behavior, solving instances of generator A usually easier than instances of B and C. Furthermore, the average behavior of solvers on the different test sets—indicated by the summed values in the last row of the table—is quite homogeneous, where data sets 4 and 5 (generator B) are slightly harder on average. But the individual different behavior of the solvers also justifies the decision to use different graph models and challenging graph features for the competition, as otherwise some solvers would have been at an advantage.Table 12 reports on the percentage of correctly classified instances for each solver for each of the tracks it participated in. We report with 0% when the solver participated in the track but without providing any correct answer, and we leave 284M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Table 8Results for the problem DS (“decide skeptical”) per semantics (N = 576); “#TO” is the number of timeouts, “#−” is the number of incorrectly classified instances, “#+” is the number of correctly classified instances, “RT in ms” is the total runtime for all classified instances, and “Significant” indicates whether the performance of a solver is significantly better than the next solver (only applicable if both correctly solved the same number of instances).σCOPRGRSTRankSolver#TO#−123456789123456789101112345678912345678910ASGLLabSATSolverConArgArgSemSATASPARTIX-DArgToolsCoQuiAASDIAMONDCarneadesArgSemSATCegartixLabSATSolverASPARTIX-VCoQuiAASDIAMONDGRISASGLArgToolsASPARTIX-DCarneadesCoQuiAASCarneadesASGLLabSATSolverArgSemSATArgToolsGRISASPARTIX-DDIAMONDASPARTIX-DLabSATSolverCoQuiAASConArgASGLDIAMONDArgSemSATASSAArgToolsCarneades000000144576011462549691111555760000000390003192331001225760000000500000036180030000000021130000072222541720#+576576576576576576575527057657557557257051550950746541805765765765765765765765354635765765735575535383543222820RT in ms9006601005630147911017003903127750485246091310621095400200576019796005520220233878909580080808113010645064302065551066759202310959009513025290093514010053402134270485217011615450242734904010071086348028315701237220204020701624015053686090300952020818370101726000SignificantNOYESNOYESYES–––––YES–––––––––YESYESNOYESYESYES–––YES–––––––––Table 9Aggregated ranking for solvers participating in all tracks.RankSolverBorda count12345678CoQuiAASArgSemSATLabSATSolverASGLASPARTIX-DArgToolsCarneadesDIAMOND4950588284119130134the cell empty when the solver did not participate in that specific track. From this view on the results of the competition it emerges that systems employing SAT-solvers perform better wrt. instance classifications than those employing ASP. Only few systems have participated in tasks where they were unable to provide correct answers, i.e., Carneades, Dungell, and ASSA. It must be noted that the common point of these three systems is of being ad-hoc implementations of abstract argumentation frameworks. These negative results are explained by the fact that these systems take too much time to compute the extensions, and thus they time out before returning any answer, e.g., Carneades is optimized for grounded semantics only.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294285Table 10Number of timeouts per solver and test set, summed up over all tracks.No.SolverN123456789101112131415161718LabSATSolverArgSemSATArgToolsCegartixDungellZJU-ARGASPARTIX-VCoQuiAASASPARTIX-DConArgGRISASGLLamatzSolverProGraphDIAMONDCarneadesprefMaxSATASSASum768768768120192481207687684803847682496768768241927824A#100301920031040010295357600961#25090192010190000023107576001103#300240192062143007027657600983B#443221014423014061030072345761101193C#60100128140000300051156000785#72378012221051406441034055400965#5003661144240210118186072325761501447Table 11Number of incorrectly classified instances per solver and test set, summed up over all tracks.No.SolverN123456789101112131415161718LabSATSolverArgSemSATArgToolsCegartixDungellZJU-ARGASPARTIX-VCoQuiAASASPARTIX-DConArgGRISASGLLamatzSolverProGraphDIAMONDCarneadesprefMaxSATASSASum768768768120192481207687684803847682496768768241927824A#1200000002000021200062188#2100000007000011460063218#30000000076000002170051344B#47187001000068007650085258#561240000000670051370079310C#6247296001000004200011300120477#722666600100203400112700116435#86610511271811310264038039255910992#8185448004000049102119001084036. Lessons learnedThe competition has substantially contributed to the advancement of the state-of-the-art of abstract argumentation solvers, but also made apparent where optimizations and new developments may take root. The best solvers of ICCMA’15 were based on reductions to other formalisms and thus used general multi-purpose tools. Although these solvers benefit from the maturity of e.g., current SAT solvers, the approach of reduction still adds some overhead. For one, translating a possibly huge abstract argumentation problem into an equivalent SAT instance and calling a SAT solver using a specific syntax may be time-consuming, despite the fact that the translation is polynomial from the perspective of computational complexity. Furthermore, the strategies of SAT solvers to solve SAT instances are tailored towards general or “typical” prob-lems expressed in SAT instances. It is not apparent that SAT instances compiled from abstract argumentation problems are included in these sets of problems. To give an analogy, consider the problem of finding shortest paths in a graph. It is pos-sible to phrase this problem as a combinatorial optimization problem and use general-purpose methods such as simulated annealing [69]. However, domain-specific algorithms such as Dijkstra’s algorithm [34] clearly outperform these general-purpose methods in their domain.17 Still, in the competition, the introduced overhead of reduction-based approaches did 17 Note that in the given analogy, the complexity classes actually differ as general combinatorial optimization is not polynomial while shortest paths problems are; however, the argument is similar for reductions between problems of the same complexity.Table 12Percentage of correctly classified instances for each track supported by the participating solvers. Empty cells represent tracks not supported by the related solver.No.Solver123456789101112131415161718LabSATSolverArgSemSATArgToolsCegartixDungellZJU-ARGASPARTIX-VCoQuiAASASPARTIX-DConArgGRISASGLLamatzSolverProGraphDIAMONDCarneadesprefMaxSATASSASECO98%100%100%28%100%100%100%100%78%9%PR100%100%69%100%0%99%98%79%92%37%93%19%0%EECO94%95%63%0%94%96%78%84%69%0%GR100%100%100%ST99%100%50%62%0%100%83%79%100%99%7%100%97%100%96%95%47%78%0%0%PR97%98%66%99%0%39%98%98%78%76%37%84%21%0%95%GR100%100%100%62%100%100%83%79%100%99%100%7%100%ST60%98%50%0%97%99%94%94%38%0%0%DCCO99%100%83%99%100%96%98%94%0%PR100%100%83%99%73%84%99%97%0%GR100%100%100%100%92%100%100%96%100%ST99%100%84%99%100%99%98%54%94%0%92%DSCO100%100%100%99%100%100%100%91%0%PR99%100%80%99%99%98%72%88%88%89%0%GR100%100%100%100%92%100%100%80%100%ST100%61%48%99%100%96%96%93%0%55%286M.Thimm,S.Villata/ArtificialIntelligence252(2017)267–294M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294287not significantly outweigh the maturity of the utilized tools. This fact indicates that focused research and development of domain-specific approaches to abstract argumentation may outperform reduction-based approaches in the future. Whilst on the one side, we expect the next generation of solvers to outperform the general purpose SAT-solvers exploited by the systems participating in the competition, on the other side, we should question about the actual necessity of doing so. Ap-plications of such abstract argumentation solvers to concrete usage scenarios are hard to find, and recent results in the argument mining community [19] show that existing graphs extracted from real natural language argumentation interac-tions, e.g., online dialogues in blogs, do not (almost) present cycles among the arguments. Moreover, such dialogues end up with pretty small graphs (e.g., 40 nodes for an online debate about a specific topic). These observations seem to sug-gest that the actual need of the community is not to outperform SAT-based solvers. However, it must be noticed that the results provided by the argument mining community are still preliminary, and there is actually a potential in mining for huge argumentation graphs reporting the views of hundreds of users about a certain topic widely discussed on the Web (e.g., including the opinions reported on blogs, social networks, online debate platforms). In conclusion, even if the fact of outperforming existing solvers does not answer a present need in the community, it will in the near future in combination with argument mining techniques.Concerning the semantics, being ICCMA’15 the first edition of such a competition, it was decided to focus on the four standard semantics [35]. Given the results, this appears to be a reasonable choice, as many of the solvers were unable to tackle the whole range of tracks. It would have been useless to provide even more semantics, as it would affected only very few solvers. The next ICCMA competition scheduled for 2017 will consider also ideal, semi-stable and stage semantics, in addition to the four standard semantics.Concerning the input graphs, we believe that we covered a sufficient range of graph structures in order to avoid penal-izing some implementations over others. In general, we conclude that one of the main insights of the competition is that there is still great potential for developing new sophisticated algorithmic approaches to abstract argumentation problems.Finally, the ASPARTIX format has emerged as the standard format for the input data, and almost all participants have adapted their solvers to accept such an input format. This is in line with other well-known competitions like for instance the SAT solver competition, where the standard input format is the DIMACS CNF format.7. ConclusionsThis paper gave an overview on the First International Competition of Computational Models of Argumentation (IC-CMA’15). We described the computational tasks of the competition, its technical setups, and presented the participants. Furthermore, we reported on its results and provided some analysis and interpretation. Being the first instance of ICCMA, the organizers were very satisfied with the engagement of the community and its 18 submitted solvers. The competition provided a common background to compare different solvers developed in the last years in the computational models of argumentation community with the adoption, on the one side of novel algorithms, and on the other side, of SAT- and ASP-based standard solutions, to address heterogeneous goals. Thanks to the results the competition publicly made available, informed decisions about the choice of the right solver to adopt with respect to the computational task to be performed are now possible. The results show unsurprisingly that SAT-based and ASP-based solvers outperform ad-hoc algorithms, but there is still great potential for developing new sophisticated algorithmic approaches to abstract argumentation problems.The competition has also highlighted new needs that the ICCMA steering committee is evaluating in order to propose new tracks for the upcoming editions of the competition. First of all, a track about structured argumentation frameworks has been envisioned, and it is currently under discussion. The main issue is that there is no structured formalism with a suffi-cient number of competing solvers, and addressing a comparison of systems implementing different formalisms is extremely laborious. The lack of existing benchmarks for such a task is another issue in this direction. Second, a track about natural language argumentation is envisaged as well. The idea is to include an argument mining tack where tasks such as arguments detection and relations prediction are proposed to the systems. Also in this case, the main issue is the lack of a common annotation schema, and as a consequence, of a common annotated benchmark to compare all the existing approaches.The competition was the first in an upcoming series of competitions, the next instance is planned for 2017.18AcknowledgementsWe would like to thank all participants of ICCMA’15 for submitting solvers and actively engaging in the effort of ad-vancing implementations for computational models of argumentation. We also thank the ICCMA steering committee for providing support and shaping the competition.Appendix A. Algorithms of benchmark generatorsWe now provide pseudo code for the algorithms used to generate the benchmark graphs of the competition. Algorithm 1shows the code for GroundedGenerator (Generator A), Algorithm 2 shows the code for StableGenerator (Genera-tor B), and Algorithm 3 shows the code for SccGenerator (Generator C).18 http :/ /argumentationcompetition .org /2017 /index .html.288M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Algorithm 1 A GroundedGenerator.Require: maxA (maximal number of arguments)Require: p (probability of attack)1: A = random integer in {1, . . . , maxA}2: G = (V,E), V = {a1, . . . , a A}, E = ∅3: unconnected = {a1, . . . , a A}4: for all i = 1, . . . , A do5:for all j = 1, . . . , i − 1 doif random number in [0, 1] is smaller p thenE = E ∪ (ai, a j)unconnected = unconnected \ {ai}8:9: for all b ∈ unconnected do10:k = random integer in {1, . . . , A}if coin flip shows heads thenE = E ∪ {(b, ak)}elseE = E ∪ {(ak, b)}6:7:11:12:13:14:15: return GAlgorithm 2 B: StableGenerator.Require: maxA (maximal number of arguments)Require: minNumExtensions (approx. minimal number of stable extensions)Require: maxNumExtensions (approx. maximal number of stable extensions)Require: minSizeOfExtensions (approx. minimal size of a stable extension)Require: maxSizeOfExtensions (approx. maximal size of a stable extension)Require: minSizeOfGrounded (approx. minimal size of grounded extension)Require: maxSizeOfGrounded (approx. maximal size of grounded extension)Require: p (probability of attack)1: A = random integer in {1, . . ., maxA}2: X = random integer in {minNumExtensions,. . . ,maxNumExtensions}3: S = random integer in {minSizeOfExtensions,. . . ,maxSizeOfExtensions}4: R = random integer in {minSizeOfGrounded,. . . ,maxSizeOfGrounded}5: G = (V,E), V = {a1, . . . , a A}, E = ∅6: grounded = {a1, . . . , aR }7: for all i = 1, . . .,R do8:for all k = 0, . . . , i − 1 do9:if random number in [0, 1] is smaller p thenE = E ∪ (ai, ak)10:11: for all j = 1, . . ., X do12:Let M be a random set of S arguments in Vfor all i = R + 1, . . . , A doif ai /∈ M thenLet ak be a random argument in ME = E ∪ (ak, ai)13:14:15:16:17: return GAppendix B. Statistics on benchmark graphsTables B.13–B.16 give a detailed overview on the structure and properties of the benchmark graphs considered for ICC-MA’15. In particular, the table show for each graph of each test set the number of arguments and number of attacks, the average in-degree (= number of attackers), the (global) clustering coefficient C C , the number of strongly connected com-ponents (SCCs), its density, the number of complete, preferred, and stable extensions, the size of the grounded extension, and the average size of its complete, preferred, and stable extensions. For an argumentation framework AF = (Arg, →), the global clustering coefficient [73] is defined as (let A (cid:2) B denote “A → B or B → A” for any A, B ∈ Arg):C C(AF) =|{{A1, A2, A3} ⊆ Arg | A1 (cid:2) A2, A2 (cid:2) A3, A3 (cid:2) A1}||{{A1, A2, A3} ⊆ Arg | A1 (cid:2) A2, A2 (cid:2) A3}|In other words, C C(AF) is the ratio of the number of undirected triangles in AF and the number of connected triples of arguments and is thus a value in [0, 1]. Large values indicate a high clustering of the arguments. For an argumentation framework AF = (Arg, →), its density is defined as| → ||Arg|(|Arg| − 1)D(AF) =M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294289Algorithm 3 SccGenerator.Require: maxA (maximal number of arguments)Require: maxNumSccs (approx. maximal number of SCCs)Require: pinner (probability of attack in SCCs)Require: pouter (probability of attack between SCCs)Require: pscc (probability to connect two SCCs)1: A = random integer in {1, . . ., maxA}2: N = random integer in {1, . . . , maxNumSccs}3: G = (V,E), V = {a1, . . . , a A}, E = ∅4: C[1] = ∅, . . . , C[N] = ∅5: for all i = 1, . . ., A do6:k = random integer in {1, . . ., N}C[k] = C[k] ∪ {ai}7:8: for all i = 1, . . . , N do9:for all arg1 ∈ C[i] dofor all arg2 ∈ C[i] doE = E ∪ {(arg1, arg2)}12:13: for all i = 1, . . . , N-1 do14:for all j = i + 1, . . . , N doif random number in [0, 1] is smaller than pinner thenif random number in [0, 1] is smaller than pscc thenfor all arg1 ∈ C[i] dofor all arg2 ∈ C[ j] doif random number in [0, 1] is smaller than pouter thenE = E ∪ {(arg1,arg2)}10:11:15:16:17:18:19:20: return GTable B.13Statistics on the benchmark graphs of test sets 1 (graphs 1–24) and 2 (graphs 25–48); #Arg = number of arguments, #Att = number of attacks, in =average in-degree, cc = clustering coefficient, dens. = density, #sccs = number of strongly connected components, #CO = number of complete extensions, #PR = number of preferred extensions, #ST = number of stable extensions, |GR| = size of grounded extension, |CO| = average size of complete extensions, |PR| = average size of preferred extensions, |S T | = average size of stable extensions.ID123456789101112131415161718192021222324252627282930#Arg190918802223251141662033263626962743294326312846122413151278162418663144279737403600358937373713221928383896445948274685#Att3668355549566277172894143688672807508866368938153148417301633265935049886785514045129571293114018138564936804215163198862322221863in19.2118.9122.325.041.50.3826.1227.027.3729.4426.228.6512.1313.1612.7816.3718.7831.4528.0837.5635.9936.0337.5137.3222.2528.3438.9244.648.1146.67cc0.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.01dens.#sccs#CO#PR#ST|GR|0.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.01245302350472658285393739246389310340254331231256465577190537239317386355497390568387560548111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111183184189193217183199199199200198197154160166183186208200215216208214213187196216219231224|CO|183.0184.0189.0193.0217.0183.0199.0199.0199.0200.0198.0197.0154.0160.0166.0183.0186.0208.0200.0215.0216.0208.0214.0213.0187.0196.0216.0219.0231.0224.0|PR|183.0184.0189.0193.0217.0183.0199.0199.0199.0200.0198.0197.0154.0160.0166.0183.0186.0208.0200.0215.0216.0208.0214.0213.0187.0196.0216.0219.0231.0224.0|ST|183.0184.0189.0193.0217.0183.0199.0199.0199.0200.0198.0197.0154.0160.0166.0183.0186.0208.0200.0215.0216.0208.0214.0213.0187.0196.0216.0219.0231.0224.0(continued on next page)290M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Table B.13 (continued)ID313233343536373839404142434445464748#Arg245030052684495934785900447060176546395519455257193739941152395431682085#Att60109059723324612121413472020063362084292315584379727616377015976134615641100294348in24.5330.1526.9549.6334.9158.8544.8860.1865.5739.4119.5352.5319.4740.011.6939.5631.6620.86cc0.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.01dens.#sccs#CO#PR#ST|GR|0.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.01343276424729657438375572275228302238245190260588247263111111111111111111111111111111111111111111111111111111191208197228206237227240242229185231182222152217206193|CO|191.0208.0197.0228.0206.0237.0227.0240.0242.0229.0185.0231.0182.0222.0152.0217.0206.0193.0|PR|191.0208.0197.0228.0206.0237.0227.0240.0242.0229.0185.0231.0182.0222.0152.0217.0206.0193.0|ST|191.0208.0197.0228.0206.0237.0227.0240.0242.0229.0185.0231.0182.0222.0152.0217.0206.0193.0Table B.14Statistics on the benchmark graphs of test sets 3 (graphs 49–72) and 4 (graphs 73–96); #Arg = number of arguments, #Att = number of attacks, in =average in-degree, cc = clustering coefficient, dens. = density, #sccs = number of strongly connected components, #CO = number of complete extensions, #PR = number of preferred extensions, #ST = number of stable extensions, |GR| = size of grounded extension, |CO| = average size of complete extensions, |PR| = average size of preferred extensions, |ST| = average size of stable extensions.ID495051525354555657585960616263646566676869707172737475767778798081828384858687#Arg719161426961487278616977724284027521783383667318559377465774839394736499936069586518854862195013141299289277202286280276291232258197280300193#Att516553768348482236576186348531525437056056661615186991053649313996007833330705588971342252876224843942442731573870725121384244163995028036726141433325027139928746in71.8361.3569.6548.5678.769.5672.5583.9875.3478.5483.5773.3156.1477.5657.7384.0794.765.0193.6169.6265.1285.5862.2450.112.7414.2114.414.432.519.8113.119.4914.2314.389.7213.7914.289.572.42cc0.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.020.030.030.040.020.020.030.020.030.040.030.050.030.020.01dens.#sccs#CO#PR#ST|GR|0.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.010.020.050.050.050.010.030.050.030.050.060.040.070.050.030.0130539446934136744341046357375450388112671722423347436290519365433592562370105666914246666666791111111111111111111111113604311591163269131417011111111111111111111111140321121821461071111111111111111111111111113232012082145107002452382442392552512452572512492582432342532312632622462612522382582362381943315312333433316|CO|245.0238.0244.0239.0255.0251.0245.0257.0251.0249.0258.0243.0234.0253.0231.0263.0262.0246.0261.0252.0238.0258.0236.0238.034.7129.527.333.049.123.033.8830.3320.025.1737.1123.3833.713.040.31|PR|245.0238.0244.0239.0255.0251.0245.0257.0251.0249.0258.0243.0234.0253.0231.0263.0262.0246.0261.0252.0238.0258.0236.0238.043.238.039.53.068.833.038.1344.037.029.7541.525.536.433.052.0|ST|245.0238.0244.0239.0255.0251.0245.0257.0251.0249.0258.0243.0234.0253.0231.0263.0262.0246.0261.0252.0238.0258.0236.0238.044.0638.039.5–68.83–38.1344.037.029.7541.825.536.43––M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294291Table B.14 (continued)ID888990919293949596#Arg#Attin26229224625930016229029328737628435424629021841642741214.379.7314.399.539.6913.4814.3614.5814.36cc0.040.020.040.020.020.060.030.030.03dens.#sccs#CO#PR#ST|GR|0.060.030.060.040.030.080.050.050.0566666665615111153524131792412030792412533333233|CO|5.040.43.037.2745.7315.3329.821.525.0|PR|5.050.03.041.4349.4421.536.7540.033.0|ST|–50.0–41.4349.4421.536.7540.033.0Table B.15Statistics on the benchmark graphs of test sets 5 (graphs 97–120) and 6 (graphs 121–144); #Arg = number of arguments, #Att = number of attacks, in =average in-degree, cc = clustering coefficient, dens. = density, #sccs = number of strongly connected components, #CO = number of complete extensions, #PR = number of preferred extensions, #ST = number of stable extensions, |GR| = size of grounded extension, |CO| = average size of complete extensions, |PR| = average size of preferred extensions, |ST| = average size of stable extensions.ID979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144#Arg#Attin400400400400400400400400400400400400400400400400400400400400400400400400283319334189397242260217308332318379330332306379360285360185291374381366379383539541541383562563545556550374378375559377546384374374575376379558214250329984011431871669841451335935362443223379533301529872154995584569.59.5913.4913.5513.539.5914.0514.0913.6313.9213.779.379.459.3913.989.4513.659.69.379.374.399.429.4813.977.597.869.875.210.115.957.227.6631.9843.7110.5624.6710.9913.367.3110.0214.8110.5614.724.737.3913.3614.6512.48cc0.020.020.020.020.020.020.020.020.020.020.020.020.020.020.020.020.020.020.020.020.030.020.020.020.090.070.090.050.050.060.080.10.230.420.10.30.090.140.060.060.120.120.120.050.060.130.110.11dens.#sccs#CO#PR#ST|GR|0.020.020.030.030.030.020.040.040.030.030.030.020.020.020.040.020.030.020.020.020.040.020.020.040.030.020.030.030.030.020.030.040.10.130.030.070.030.040.020.030.040.040.040.030.030.040.040.03312041321929212341313240222021324021212921312030617537100769852526428928208968222121938923232832477111217836365362353528815216280014496284811442514768418082003010228212551111146242342412423237114641112131824221113112550001026242332412323000000000000000000000000889111079712810888989658891074103240000000030000112000|CO|44.6736.539.546.2948.437.09.07.034.08.044.8658.7549.3358.538.6752.1742.843.055.6736.035.3352.647.3343.08.835.844.2813.864.6413.53.435.00.00.02.03.251.06.694.06.92.56.133.020.277.333.91.04.43|PR|63.065.051.552.455.07.09.07.056.08.051.066.6770.069.053.563.051.2561.566.7564.049.063.566.052.013.010.07.024.07.019.04.838.50.00.04.04.52.010.675.013.03.511.55.527.012.08.02.07.33|ST|63.065.051.552.455.0–––56.0–52.066.6770.069.053.563.051.6761.566.7564.049.065.066.052.0––––––––––––––––––––––––292M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294Table B.16Statistics on the benchmark graphs of test sets 7 (graphs 145–168) and 8 (graphs 169–192); #Arg = number of arguments, #Att = number of attacks, in =average in-degree, cc = clustering coefficient, dens. = density, #sccs = number of strongly connected components, #CO = number of complete extensions, #PR = number of preferred extensions, #ST = number of stable extensions, |GR| = size of grounded extension, |C O | = average size of complete extensions, |PR| = average size of preferred extensions, |ST| = average size of stable extensions.ID145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192#Arg826777754311780500315393499442243455636417679649458652490351696733703757461457755671795769778595507837374868853847940960996481935939489992964583#Att504411791711284256712112735407118056329362839118646726091410594253466226119731904167029031361121538278170714841565277410192515220943393179330272297326411551109392711119551865044017018in61.0715.1822.79.1432.9224.2386.8310.3623.6612.75120.8562.4118.6611.238.4321.7412.9838.88135.15174.361.0625.9923.7638.362.952.4620.384.1521.4819.320.124.668.0923.014.0824.1339.7921.1732.2123.9332.78240.16116.9928.87244.49188.014.57291.92cc0.270.280.080.060.120.20.350.060.150.080.50.360.10.060.190.10.090.150.350.50.00.120.10.170.010.020.060.010.070.060.060.020.010.080.020.070.120.070.110.080.120.50.230.10.50.320.010.5Referencesdens.#sccs0.070.020.030.030.040.050.280.030.050.030.50.140.030.030.060.030.030.060.280.50.00.040.030.050.010.010.030.010.030.030.030.010.020.030.010.030.050.030.030.020.030.50.130.030.50.190.00.5818346520142631639143450122739132168624281439534546617424849301416382904218472642261531138521#CO4923916214441861320101141129721127921324222121871906527490349561112111212541#PR#ST|GR||CO|1415112242113311811182112420342411921614421111111111810000002000000000000080002400400019216044000000000008000000002000000000000349000123119013600034480360000000000015204.55.00.510.271.50.05.57.07.171.50.04.06.1510.844.00.010.82.00.00.0356.675.01.00.0130.5137.344.83140.53.439.171.5125.6558.971.3364.883.50.00.00.01.00.00.00.01.00.03.0162.330.0|PR|9.07.51.018.03.00.09.010.511.52.50.08.09.3319.08.00.018.54.00.00.0360.59.02.00.0134.67142.97.33143.05.514.03.0152.566.03.083.956.00.00.00.02.00.00.00.02.00.06.0171.50.0|ST|––––––9.0–––––––––––––360.5–––134.67––143.0–––152.566.0–83.95–––––––––––171.5–[1] O. Arieli, M.W.A. Caminada, A QBF-based formalization of abstract argumentation semantics, J. Appl. Log. 11 (2) (2013) 229–252.[2] G. Audemard, J. Lagniez, L. Simon, Improving glucose for incremental SAT solving with assumptions: application to MUS extraction, in: Theory and Applications of Satisfiability Testing, SAT 2013, 16th International Conference Proceedings, Helsinki, Finland, July 8–12, 2013, in: Lect. Notes Comput. Sci., vol. 7962, Springer, 2013, pp. 309–317.[3] A.-L. Barabási, R. Albert, Emergence of scaling in random networks, Science 286 (5439) (1999) 509–512.[4] P. Baroni, M. Caminada, M. Giacomin, An introduction to argumentation semantics, Knowl. Eng. Rev. 26 (4) (2011) 365–410.[5] P. Baroni, M. Giacomin, G. Guida, SCC-recursiveness: a general schema for argumentation semantics, Artif. Intell. 168 (1–2) (2005) 162–210.[6] T.J.M. Bench-Capon, P.E. Dunne, Argumentation in artificial intelligence, Artif. Intell. 171 (2007) 619–641.[7] P. Besnard, S. Doutre, Checking the acceptability of a set of arguments, in: Proceedings of the 10th International Workshop on Non-Monotonic Reasoning, NMR 2004, Whistler, Canada, June 6–8, 2004, 2004, pp. 59–64.[8] P. Besnard, A. Hunter, Elements of Argumentation, The MIT Press, 2008.[9] A. Biere, Lingeling essentials, a tutorial on design and implementation aspects of the SAT solver lingeling, in: Fifth Pragmatics of SAT Workshop, Workshop of the SAT 2014 Conference, Part of FLoC 2014 During the Vienna Summer of Logic, POS-14, July 13, 2014, Vienna, Austria, in: EPiC Ser. Comput., vol. 27, EasyChair, 2014, p. 88.M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294293[10] S. Bistarelli, F. Rossi, F. Santini, A first comparison of abstract argumentation systems: a computational perspective, in: D. Cantone, M.N. Asmundo (Eds.), Proceedings of the 28th Italian Conference on Computational Logic, 2013, pp. 241–245.[11] S. Bistarelli, F. Rossi, F. Santini, A first comparison of abstract argumentation reasoning-tools, in: ECAI 2014 – 21st European Conference on Artificial Intelligence, Including Prestigious Applications of Intelligent Systems, PAIS 2014, 18–22 August 2014, Prague, Czech Republic, 2014, pp. 969–970.[12] S. Bistarelli, F. Rossi, F. Santini, Benchmarking hard problems in random abstract AFs: the stable semantics, in: Computational Models of Argument – Proceedings of COMMA 2014, 2014, pp. 153–160.[13] S. Bistarelli, F. Rossi, F. Santini, ConArg2: a constraint-based tool for abstract argumentation, in: [92], pp. 33–36, arXiv:1510.05373, 2015.[14] S. Bistarelli, F. Rossi, F. Santini, A comparative test on the enumeration of extensions in abstract argumentation, Fund. Inform. 140 (3–4) (2015) [15] S. Bistarelli, F. Rossi, F. Santini, ConArg: a tool for classical and weighted argumentation, in: Computational Models of Argument: Proceedings of [16] G. Brewka, S. Ellmauthaler, H. Strass, J.P. Wallner, S. Woltran, Abstract dialectical frameworks revisited, in: Proceedings of the 23rd International Joint 263–278.COMMA 2016, vol. 287, 2016, p. 463.Conference on Artificial Intelligence, IJCAI’13, 2013.[17] F. Brons, LabSAT-solver: utilizing Caminada’s labelling approach as a Boolean satisfiability problem, in: [92], pp. 1–3, arXiv:1510.05373, 2015.[18] M.G. Bulmer, Principles of Statistics, Dover Publications, 1979.[19] E. Cabrio, S. Villata, Node: a benchmark of natural language arguments, in: Proceedings of the 5th International Conference on Computational Models of Argument, COMMA 2014, in: Front. Artif. Intell. Appl., vol. 266, IOS Press, 2015, pp. 449–450.[20] E. Cabrio, S. Villata, F. Gandon, A support framework for argumentative discussions management in the web, in: Proceedings of the 10th Extended Semantic Web Conference, ESWC’13, Springer-Verlag, 2013, pp. 412–426.[21] M. Caminada, On the issue of reinstatement in argumentation, in: Proceedings of the 10th European Conference on Logics in Artificial Intelligence, [22] M. Caminada, Semi-stable semantics, in: P. Dunne, T. Bench-Capon (Eds.), Proceedings of the First International Conference on Computational Models JELIA’06, 2006, pp. 111–123.of Argument, COMMA’06, IOS Press, 2006, pp. 121–130.[23] F. Cerutti, P.E. Dunne, M. Giacomin, M. Vallati, Computing preferred extensions in abstract argumentation: a SAT-based approach, in: Theory and Applications of Formal Argumentation – Second International Workshop, TAFA 2013, Beijing, China, August 3–5, 2013, in: Lect. Notes Comput. Sci., vol. 8306, Springer, 2013, pp. 176–193, Revised Selected Papers.[24] F. Cerutti, M. Giacomin, M. Vallati, M. Zanella, An SCC recursive meta-algorithm for computing preferred labellings in abstract argumentation, in: Principles of Knowledge Representation and Reasoning: Proceedings of the Fourteenth International Conference, KR 2014, Vienna, Austria, July 20–24, 2014, AAAI Press, 2014.[25] F. Cerutti, N. Oren, H. Strass, M. Thimm, M. Vallati, A benchmark framework for a computational argumentation competition, in: Proceedings of the 5th International Conference on Computational Models of Argument, 2014, pp. 459–460.[26] F. Cerutti, M. Vallati, M. Giacomin, ArgSemSAT-1.0: exploiting SAT solvers in abstract argumentation, in: [92], pp. 4–7, arXiv:1510.05373, 2015.[27] F. Cerutti, M. Vallati, M. Giacomin, Where are we now? State of the art and future trends of solvers for hard argumentation problems, in: Computa-tional Models of Argument – Proceedings of COMMA 2016, 2015, pp. 207–218.[28] F. Cerutti, M. Vallati, M. Giacomin, Efficient and off-the-shelf solver: jArgSemSAT, in: Computational Models of Argument: Proceedings of COMMA [29] F. Cerutti, M. Vallati, M. Giacomin, jArgSemSAT: an efficient off-the-shelf solver for abstract argumentation frameworks, in: Proceedings of the Fif-teenth International Conference on Principles of Knowledge Representation and Reasoning, AAAI Press, 2016, pp. 541–544.[30] G. Charwat, W. Dvorak, S.A. Gaggl, J.P. Wallner, S. Woltran, Methods for solving reasoning problems in abstract argumentation – a survey, Artif. Intell. [31] A. Cohen, S. Gottifredi, A.J. Garcia, G.R. Simari, A survey of different approaches to support in argumentation systems, Knowl. Eng. Rev. 29 (5) (2014) 2016, vol. 287, 2016, p. 465.220 (2015) 28–63.513–550.[32] S. Coste-Marquis, C. Devred, P. Marquis, Symmetric argumentation frameworks, in: Proceedings of the 8th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, ECSQARU’05, in: Lect. Notes Comput. Sci., vol. 3571, Springer, 2005, pp. 317–328.[33] E. Dantsin, T. Eiter, G. Gottlob, A. Voronkov, Complexity and expressive power of logic programming, ACM Comput. Surv. 33 (3) (2001) 374–425.[34] E.W. Dijkstra, A note on two problems in connexion with graphs, Numer. Math. 1 (1959) 269–271.[35] P.M. Dung, On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games, Artif. [36] P.E. Dunne, T.J.M. Bench-Capon, Coherence in finite argument systems, Artif. Intell. 141 (1–2) (2002) 187–203.[37] P.E. Dunne, W. Dvoˇrák, T. Linsbichler, S. Woltran, Characteristics of multiple viewpoints in abstract argumentation, Artif. Intell. 228 (2015) 153–178.[38] P.E. Dunne, A. Hunter, P. McBurney, S. Parsons, M. Wooldridge, Weighted argument systems: basic definitions, algorithms, and complexity results, Intell. 77 (2) (1995) 321–358.Artif. Intell. 175 (2) (2011) 457–486.[39] P.E. Dunne, C. Spanring, T. Linsbichler, S. Woltran, Investigating the relationship between argumentation semantics via signatures, in: Proceedings of the 25th International Joint Conference on Artificial Intelligence, IJCAI’16, 2016.[40] P.E. Dunne, M. Wooldridge, Complexity of abstract argumentation, in: Argumentation in Artificial Intelligence, Springer, 2009, pp. 85–104, Ch. 5.[41] P.E. Dunne, M. Wooldridge, Complexity of abstract argumentation, in: Argumentation in Artificial Intelligence, Springer, 2009, pp. 85–104.[42] W. Dvorák, S.A. Gaggl, J.P. Wallner, S. Woltran, Making use of advances in answer-set programming for abstract argumentation systems, in: Ap-plications of Declarative Programming and Knowledge Management – 19th International Conference, INAP 2011, and 25th Workshop on Logic Programming, WLP 2011, Vienna, Austria, September 28–30, 2011, in: Lect. Notes Comput. Sci., vol. 7773, Springer, 2011, pp. 114–133, Revised Selected Papers.[43] W. Dvorák, M. Järvisalo, J.P. Wallner, S. Woltran, Complexity-sensitive decision procedures for abstract argumentation, Artif. Intell. 206 (2014) 53–78.[44] W. Dvoˇrák, Computational Aspects of Abstract Argumentation, Ph.D. thesis, Technische Universität Wien, 2012.[45] W. Dvoˇrák, M. Järvisalo, J.P. Wallner, S. Woltran, CEGARTIX v0.4: a SAT-based counter-example guided argumentation reasoning tool, in: [92], pp. 12–14, arXiv:1510.05373, 2015.[46] N. Eén, N. Sörensson, An extensible SAT-solver, in: Proceedings of the International Conference on Theory and Applications of Satisfiability Testing 2004, in: Lect. Notes Comput. Sci., vol. 2919, Springer, 2004, pp. 502–518.[47] U. Egly, S.A. Gaggl, S. Woltran, ASPARTIX: implementing argumentation frameworks using answer-set programming, in: Logic Programming, Proceed-ings of the 24th International Conference, ICLP 2008, Udine, Italy, December 9–13, 2008, in: Lect. Notes Comput. Sci., vol. 5366, Springer, 2008, pp. 734–738.[48] U. Egly, S.A. Gaggl, S. Woltran, Answer-set programming encodings for argumentation frameworks, Argum. Comput. 1 (2) (2010) 147–177.[49] U. Egly, S. Woltran, Reasoning in argumentation frameworks using quantified Boolean formulas, in: Computational Models of Argument: Proceedings of COMMA 2006, September 11–12, 2006, in: Front. Artif. Intell. Appl., vol. 144, IOS Press, Liverpool, UK, 2006, pp. 133–144.[50] S. Ellmauthaler, H. Strass, The DIAMOND system for computing with abstract dialectical frameworks, in: Computational Models of Argument: Pro-ceedings of COMMA 2014, 2014, pp. 233–240.294M. Thimm, S. Villata / Artificial Intelligence 252 (2017) 267–294[51] S. Ellmauthaler, H. Strass, DIAMOND: a system for computing with abstract dialectical frameworks, in: [92], pp. 51–53, arXiv:1510.05373, 2015.[52] P. Erdös, A. Rényi, On random graphs I, Publ. Math. 6 (1959) 290–297.[53] W. Faber, S. Woltran, Manifold answer-set programs for meta-reasoning, in: Logic Programming and Nonmonotonic Reasoning, Proceedings of the 10th International Conference, LPNMR 2009, Potsdam, Germany, September 14–18, 2009, in: Lect. Notes Comput. Sci., vol. 5753, Springer, 2009, pp. 115–128.[54] S.A. Gaggl, N. Manthey, ASPARTIX-D: ASP argumentation reasoning tool – Dresden, in: [92], pp. 29–32, arXiv:1510.05373, 2015.[55] A. Garcia, G.R. Simari, Defeasible logic programming: an argumentative approach, Theory Pract. Log. Program. 4 (1–2) (2004) 95–138.[56] M. Gebser, R. Kaminski, B. Kaufmann, T. Schaub, Clingo = ASP + control: preliminary report, CoRR arXiv:1405.3694 [abs], 2014.[57] M. Gelfond, V. Lifschitz, Classical negation in logic programs and disjunctive databases, New Gener. Comput. 9 (1991) 365–385.[58] T.F. Gordon, Carneades ICCMA: a straightforward implementation of a solver for abstract argumentation in the Go programming language, in: [92], pp. 54–57, arXiv:1510.05373, 2015.[59] T.F. Gordon, H. Prakken, D. Walton, The Carneades model of argument and burden of proof, Artif. Intell. 171 (10–15) (2007) 875–896.[60] É. Grégoire, J. Lagniez, B. Mazure, An experimentally efficient method for (MSS, coMSS) partitioning, in: Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27–31, 2014, Québec City, Québec, Canada, AAAI Press, 2014, pp. 2666–2673.[61] S. Groza, A. Groza, ProGraph: towards enacting bipartite graphs for abstract argumentation frameworks, in: [92], pp. 49–50, arXiv:1510.05373, 2015.[62] Q. Guo, B. Liao, ZJU-ARG: a decomposition-based solver for abstract argumentation, in: [92], pp. 19–21, arXiv:1510.05373, 2015.[63] E. Hadjisoteriou, M. Georgiou, ASSA: computing stable extensions with matrices, in: [92], pp. 62–65, arXiv:1510.05373, 2015.[64] E. Hadjisoteriou, Computing argumentation with matrices, in: Proceedings of the 2015 Imperial College Computing Student Workshop, 2015, p. 29.[65] A. Hunter, A probabilistic approach to modelling uncertain logical arguments, Int. J. Approx. Reason. 54 (1) (2013) 47–81.[66] J.-M. Lagniez, E. Lonca, J.-G. Mailly, CoQuiAAS: application of constraint programming for abstract argumentation, in: [92], pp. 25–28, arXiv:1510.05373, 2015.[67] J.M. Lagniez, E. Lonca, J.G. Mailly, CoQuiAAS: a constraint-based quick abstract argumentation solver, in: 2015 IEEE 27th International Conference on Tools with Artificial Intelligence, JCTAI, November 2015, pp. 928–935.[68] N. Lamatz, LamatzSolver-v0.1: a grounded extension finder based on the Java-collection-framework, in: [92], pp. 45–48, arXiv:1510.05373, 2015.[69] D. Lawrence, Genetic Algorithms and Simulated Annealing, Pitman Publishing, 1987.[70] B. Liao, Toward incremental computation of argumentation semantics: a decomposition-based approach, Ann. Math. Artif. Intell. 67 (3–4) (2013) 319–358.[71] B. Liao, Efficient Computation of Argumentation Semantics, Intelligent Systems Series, Academic Press, 2014.[72] B. Liao, L. Jin, R.C. Koons, Dynamics of argumentation systems: a division-based method, Artif. Intell. 175 (2011) 1790–1814.[73] R.D. Luce, A.D. Perry, A method of matrix analysis of group structure, Psychometrika 14 (1) (1949) 95–116.[74] S. Modgil, M.W. Caminada, Proof theories and algorithms for abstract argumentation frameworks, in: Argumentation in Artificial Intelligence, Springer Publishing Company, Inc., 2009, pp. 105–129.[75] M.W. Moskewicz, C.F. Madigan, Y. Zhao, L. Zhang, S. Malik, Chaff: engineering an efficient SAT solver, in: Proceedings of the 38th Design Automation Conference, DAC 2001, Las Vegas, NV, USA, June 18–22, 2001, ACM, 2001, pp. 530–535.[76] J.C. Nieves, U. Cortés, M. Osorio, Preferred extensions as stable models, Theory Pract. Log. Program. 8 (4) (2008) 527–543.[77] A. Niskanen, J.P. Wallner, M. Järvisalo, Synthesizing argumentation frameworks from examples, in: Proceedings of the 22nd European Conference on Artificial Intelligence, ECAI 2016, in: Front. Artif. Intell. Appl., vol. 285, IOS Press, 2016, pp. 551–559.[78] S. Nofal, K. Atkinson, P.E. Dunne, Algorithms for decision problems in argument systems under preferred semantics, Artif. Intell. 207 (2014) 23–51.[79] S. Nofal, K. Atkinson, P.E. Dunne, ArgTools: a backtracking-based solver for abstract argumentation, in: [92], pp. 8–11, arXiv:1510.05373, 2015.[80] S. Nofal, K. Atkinson, P.E. Dunne, Looking-ahead in backtracking algorithms for abstract argumentation, Int. J. Approx. Reason. 78 (2016) 265–282.[81] C.H. Papadimitriou, Computational Complexity, Addison Wesley, 1994.[82] J.L. Pollock, Justification and defeat, Artif. Intell. 67 (1994) 377–407.[83] O. Rodrigues, GRIS: computing traditional argumentation semantics through numerical iterations, in: [92], pp. 37–40, arXiv:1510.05373, 2015.[84] O. Rodrigues, Introducing EqArgSolver: an argumentation solver using equational semantics, in: Proceedings of the First International Workshop on Systems and Algorithms for Formal Argumentation, SAFA’2016, in: CEUR Workshop Proc., vol. 1672, 2016, pp. 22–33.[85] A. Ronca, J.P. Wallner, S. Woltran, ASPARTIX-V: utilizing improved ASP encodings, in: [92], pp. 22–24, arXiv:1510.05373, 2015.[86] J.P.M. Silva, K.A. Sakallah, GRASP – a new search algorithm for satisfiability, in: ICCAD, 1996, pp. 220–227.[87] G.R. Simari, R.P. Loui, A mathematical treatment of defeasible reasoning and its implementation, Artif. Intell. 53 (2–3) (1992) 125–157.[88] M. South, G. Vreeswijk, J. Fox, Dungine: a Java dung reasoner, in: Proceedings of the 2008 Conference on Computational Models of Argument: Proceedings of COMMA 2008, IOS Press, Amsterdam, The Netherlands, 2008, pp. 360–368.[89] K. Sprotte, ASGL: argumentation semantics in Gecode and Lisp, in: [92], pp. 41–44, arXiv:1510.05373, 2015.[90] M. Thimm, A probabilistic semantics for abstract argumentation, in: Proceedings of the 20th European Conference on Artificial Intelligence, ECAI’12, [91] M. Thimm, Tweety – a comprehensive collection of Java libraries for logical aspects of artificial intelligence and knowledge representation, in: Pro-ceedings of the 14th International Conference on Principles of Knowledge Representation and Reasoning, KR’14, July 2014, pp. 528–537.[92] M. Thimm, S. Villata (Eds.), System Descriptions of the First International Competition on Computational Models of Argumentation, ICCMA’15, 2015, [93] F. Toni, M. Sergot, Argumentation and answer set programming, in: Logic Programming, Knowledge Representation, and Nonmonotonic Reasoning: Essays in Honor of Michael Gelfond, in: Lect. Notes Comput. Sci., vol. 6565, Springer, 2011, pp. 164–180.[94] M. Vallati, F. Cerutti, W. Faber, M. Giacomin, prefMaxSAT: exploiting MaxSAT for enumerating preferred extensions, in: [92], pp. 58–61, [95] M. Vallati, F. Cerutti, M. Giacomin, Argumentation frameworks features: an initial study, in: Proceedings of the 21st European Conference on Artificial August 2012.arXiv:1510.05373.arXiv:1510.05373, 2015.Intelligence, ECAI’14, 2014.[96] F.H. van Eemeren, B. Garssen, E.C.W. Krabbe, F.A.S. Henkemans, B. Verheij, J.H.M. Wagemans, Handbook of Argumentation Theory, Springer, 2014.[97] B. van Gijzel, Dungell: a reference implementation of Dung’s argumentation frameworks in Haskell, in: [92], pp. 15–18, arXiv:1510.05373, 2015.[98] B. Van Gijzel, H. Nilsson, A principled approach to the implementation of argumentation models, in: Proceedings of the 2014 Conference on Compu-tational Models of Argument, 2014, pp. 293–300.[99] T. Wakaki, Preference-based argumentation capturing prioritized logic programming, in: Argumentation in Multi-Agent Systems – 7th International Workshop, ArgMAS 2010, Toronto, ON, Canada, May 10, 2010, in: Lect. Notes Comput. Sci., vol. 6614, Springer, 2010, pp. 306–325, Revised Selected and Invited Papers.[100] D.J. Watts, S.H. Strogatz, Collective dynamics of small-world networks, Nature 393 (6684) (1998) 440–442.[101] G. Weissenbacher, S. Malik, Boolean satisfiability solvers: techniques and extensions, in: Software Safety and Security – Tools for Analysis and Verifi-cation, in: NATO Sci. Peace Secur. Ser. D Inf. Commun. Secur., vol. 33, IOS Press, 2012, pp. 205–253.[102] S. Wells, Argument mining: Was ist das?, in: Proceedings of the 14th International Workshop on Computational Models of Natural Argument, CMNA14, 2014.