Artificial Intelligence 171 (2007) 107–143www.elsevier.com/locate/artintLearning action models from plan examples usingweighted MAX-SATQiang Yang a,∗, Kangheng Wu a,b, Yunfei Jiang ba Department of Computer Science and Engineering, Hong Kong University of Science and Technology,Clearwater Bay, Kowloon, Hong Kong, Chinab Software Institute, Zhongshan University (Sun Yat-Sen University), Guangzhou, ChinaReceived 21 October 2005; received in revised form 14 November 2006; accepted 27 November 2006Available online 25 January 2007AbstractAI planning requires the definition of action models using a formal action and plan description language, such as the stan-dard Planning Domain Definition Language (PDDL), as input. However, building action models from scratch is a difficult andtime-consuming task, even for experts. In this paper, we develop an algorithm called ARMS (action-relation modelling system)for automatically discovering action models from a set of successful observed plans. Unlike the previous work in action-modellearning, we do not assume complete knowledge of states in the middle of observed plans. In fact, our approach works when noor partial intermediate states are given. These example plans are obtained by an observation agent who does not know the logicalencoding of the actions and the full state information between the actions. In a real world application, the cost is prohibitivelyhigh in labelling the training examples by manually annotating every state in a plan example from snapshots of an environment.To learn action models, ARMS gathers knowledge on the statistical distribution of frequent sets of actions in the example plans. Itthen builds a weighted propositional satisfiability (weighted MAX-SAT) problem and solves it using a MAX-SAT solver. We laythe theoretical foundations of the learning problem and evaluate the effectiveness of ARMS empirically.© 2006 Elsevier B.V. All rights reserved.Keywords: Learning action models; Automated planning; Statistical relational learning1. IntroductionAI planning systems require the definition of action models, an initial state and a goal. In the past, various actionmodelling languages have been developed. Some examples are STRIPS [13], ADL [12] and PDDL [14,17]. With theselanguages, a domain expert sits down and writes a complete set of domain action representation. These representationsare then used by planning systems as input to generate plans.However, building action models from scratch is a task that is exceedingly difficult and time-consuming even fordomain experts. Because of this difficulty, various approaches [4,5,18,34,37,42,43] have been explored to learn action* Corresponding author.E-mail address: qyang@cse.ust.hk (Q. Yang).URL: http://www.cse.ust.hk/~qyang.0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.11.005108Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143models from examples. In knowledge acquisition for planning, many state of the art systems for acquiring actionmodels are based on a procedure in which a computer system interacts with a human expert, as illustrated by Blytheet al. [5] and McCluskey et al. [29]. A common feature of these works is that they require states just before or aftereach action to be known. Statistical and logical inferences can then be made to learn the actions’ preconditions andeffects.In this paper, we take one step towards automatically acquiring action models from observed plans in practice.The resultant algorithm is called ARMS, which stands for Action-Relation Modelling System. We assume that eachobserved plan consists of a sequence of action names together with the objects that each action uses. The intermediatestates between actions can be partially known; that is, between every adjacent pair of actions, the truth of a literal canbe totally unknown. This means that our input can be in the form of action names and associated parameter list withno state information, which is much more practical than previous systems that learn action models. Suppose that wehave several observed plans as input. From this incomplete knowledge, our ARMS system automatically guesses theapproximately correct and concise action models that can explain most of the observed plans. This action model isnot guaranteed to be completely correct, but it can serve to provide important initial guidance for human knowledgeeditors.Consider an example input and output of our algorithm in the Depot problem domain from an AI Planning com-petition [14,17]. As part of the input, we are given relations such as (clear ?x:surface) to denote that ?x is clear on topand that ?x is of type “surface”, relation (at ?x:locatable ?y:place) to denote that a locatable object ?x is located at aplace ?y. We are also given a set of plan examples consisting of action names along with their parameter list, such asdrive(?x:truck ?y:place ?z:place), and then lift(?x:hoist ?y:crate ?z:surface ?p:place). We call the pair consisting of anaction name and the associated parameter list an action signature; an example of an action signature is drive(?x:truck?y:place ?z:place). Our objective is to learn an action model for each action signature, such that the relations in thepreconditions and postconditions are fully specified.A complete description of the example is shown in Table 1, which lists the actions to be learned, and Table 2, whichdisplays the training examples. From the examples in Table 2, we wish to learn the preconditions, add and delete listsof all actions. Once an action is given with the three lists, we say that it has a complete action model. Our goal is tolearn an action model for every action in a problem domain in order to “explain” all training examples successfully.An example output from our learning algorithms for the load(?x ?y ?z ?p) action signature is:actionload(?x:hoist ?y:crate ?z:truck ?p:place)pre:del:add:(at ?x ?p), (at ?z ?p), (lifting ?x ?y)(lifting ?x ?y)(at ?y ?p), (in ?y ?z), (available ?x), (clear ?y)Table 1Input domain description for Depot planning domainDomaintypesrelationsactionsDepotplace locatable - objectdepot distributor - placetruck hoist surface - locatablepallet crate - surface(at ?x:locatable ?y:place)(on ?x:crate ?y:surface)(in ?x:crate ?y:truck)(lifting ?x:hoist ?y:crate)(available ?x:hoist)(clear ?x:surface)drive(?x:truck ?y:place ?z:place)lift(?x:hoist ?y:crate ?z:surface ?p:place)drop(?x:hoist ?y:crate ?z:surface ?p:place)load(?x:hoist ?y:crate ?z:truck ?p:place)unload(?x:hoist ?y:crate ?z:truck ?p:place)Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143109Table 2Three plan traces as part of the training examplesInitialStep1StateStep2Step3StateStep4StateStep5Step6Step7Step8Step9GoalPlan1I1lift(h1 c0 p1 ds0),drive(t0 dp0 ds0)load(h1 c0 t0 ds0)drive(t0 ds0 dp0)(available h1)unload(h0 c0 t0 dp0)(lifting h0 c0)drop (h0 c0 p0 dp0)(on c0 p0)Plan2I2lift(h1 c1 c0 ds0)(lifting h1 c1)load(h1 c1 t0 ds0)lift(h1 c0 p1 ds0)load(h1 c0 t0 ds0)drive(t0 ds0 dp0)unload(h0 c1 t0 dp0)drop(h0 c1 p0 dp0)unload(h0 c0 t0 dp0)drop(h0 c0 c1 dp0)(on c1 p0)(on c0 c1)Plan3I3lift(h2 c1 c0 ds0)load(h2 c1 t1 ds0)lift(h2 c0 p2 ds0),drive(t1 ds0 dp1)unload(h1 c1 t1 dp1),load(h2 c0 t0 ds0)drop(h1 c1 p1 dp1),drive(t0 ds0 dp0)unload(h0 c0 t0 dp0)drop(h0 c0 p0 dp0)(on c0 p0)(on c1 p1)I1: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at t0 dp0), (at p1 ds0), (clear c0), (on c0 p1), (available h1), (at h1 ds0).I2: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at t0 ds0), (at p1 ds0), (clear c1), (on c1 c0), (on c0 p1), (available h1), (at h1 ds0).I3: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at p1 dp1), (clear p1), (available h1), (at h1 dp1), (at p2 ds0), (clear c1), (on c1 c0), (on c0p2), (available h2), (at h2 ds0), (at t0 ds0), (at t1 ds0).We wish to emphasize an important feature of our problem definition, which is the relaxation of observable stateinformation requirements. The learning problem would be easier if we knew the states just before and after eachaction. However, in reality, what is observed before and after each action may just be partially known. In this paper,we address the situation where we know little about the states surrounding the actions. Thus we do not know for sureexactly what is true just before each of load(h1 c0 t0 ds0), drive(t0 ds0 dp0), unload(h0 c0 t0 dp0), drop(h0 c0 p0dp0) as well as the complete state just before the goal state in the first plan in Table 2. Part of the difficulty in learningaction models is due to the uncertainty in assigning state relations to actions. In each plan, any relation such as (on c0p0) in the goal conditions might be established by the first action, the second, or any of the rest. It is this uncertaintythat causes difficulties for many previous approaches that depend on knowing states precisely.In our methodology, the training plans can be obtained through monitoring devices such as sensors and cameras, orthrough a sequence of recorded commands through a computer system such as UNIX. These action models can thenbe revised using interactive systems such as GIPO.It is thus intriguing to ask whether we can approximate an action model in an application domain if we are given aset of recorded action signatures as well as partial or even no intermediate state information. In this paper, we take afirst step towards answering this question by presenting an algorithm known as ARMS. ARMS proceeds in two phases.In phase one of the algorithm, ARMS finds frequent action sets from plans that share a common set of parameters.In addition, ARMS finds some frequent relation-action pairs with the help of the initial state and the goal state. Theserelation-action pairs give us an initial guess on the preconditions, add lists and delete lists of actions in this subset.These action subsets and pairs are used to obtain a set of constraints that must hold in order to make the plans correct.We then transform the constraints extracted from the plans into a weighted MAX-SAT representation [25,30,49],solve it and produce action models from the solution of the SAT problem. The process iterates until all actions aremodelled. While the action models that we learn in this paper are deterministic in nature, in the future we will extendthis framework to learning probabilistic action models to handle uncertainty.For a complex planning domain that is represented by hundreds of relations and actions, the correspondingweighted MAX-SAT representation is likely to be too large to be solved efficiently as the number of clauses canreach tens of thousands. In response, we design a heuristic method for modelling the actions approximately. We then110Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143measure the correctness of the model using a definition of error rates and redundancy rates. For testing the model cor-rectness, we design a cross-validation method for evaluating the learned action models under different experimentalconditions such as the different amount of observations and the number of plans.Our previous work [46] reported on the feasibility of learning action models from action sequences in two planningdomains. In this paper, we describe the ARMS system in further detail, add additional constraints that allow partialobservations to be made between actions, prove the formal properties of the system, and evaluate ARMS on all STRIPSplanning domains from a recent AI Planning Competition.The rest of the paper is organized as follows. The next section discusses related work in more detail. Section 3defines the problem of learning action models from plan examples. Section 4 presents the ARMS algorithm. Section 5presents the experimental results. Section 6 concludes with a discussion of future work.2. Related work2.1. Learning from state imagesThe problem of learning action descriptions is important in AI Planning. As a result, several researchers havestudied this problem in the past. In [43], [34] and [18], a partially known action model is improved using knowledgeof intermediate states between pairs of actions in a plan. These intermediate states provide knowledge for whichpreconditions or post-conditions may be missing from an action definition. In response, revision of the action modelis conducted to complete the action models. In [43] a STRIPS model is constructed by computing the most specificcondition consistent with the observed examples. In [42] a decision tree is constructed from examples in order to learnpreconditions. However, these works require there to be incomplete action models as input, and learning can only bedone when the intermediate states can be observed. Compared to these works, our work can learn an action modelwhen no state information or only incomplete state information is available.2.2. Inductive logic programmingIn [37] an inductive logic programming (ILP) approach is adopted to learn action models. Similarly, in [4], asystem is presented to learn the preimage or precondition of an action for a TOP operator using ILP. The examplesused require the positive or negative examples of propositions held in states just before each action’s application. Thisenables a concept for the preimage of an action to be learned for each state just before that action. ILP can learn wellwhen the positive and negative examples of states before all target actions are given. However, in our problem, only asequence of “bare” action names are provided in each example with only the initial and goal conditions known. To thebest of our knowledge, no work so far has been done to apply ILP to our case. Even though one can still use logicalclauses to enumerate the different possibilities for the precondition, the number of such possibilities is going to behuge. Our SAT-based solution provides an elegant control strategy for resolving ambiguities within such clauses.2.3. Knowledge acquisition in planningAnother related thrust adopts an approach of knowledge acquisition, where the action models are acquired by in-teracting with a human expert [5]. Our work can be seen as an add-on component for such mixed-initiative knowledgeediting systems which can provide advice for human users, such as GIPO [29]. The DISTILL system [44], which isa method used to learn program-like plan templates from example plans, shares similar motivation with our work.The aim is to automatically acquire plan templates from example plans, where the templates can be used for planningdirectly. Compared to our work, we are more focused on learning the action models, where planning will still bedelegated to a planner for new problems. In [44], the focus is more on how to extract plan templates from exampleplans as a substitute for the planner. The plan templates represent the structural relations among actions. Also relatedto our work is the work of Garland and Lesh [16], who introduced the idea of evaluating a plan’s partial success evenwhen the action models are incomplete. In [27], a programming by demonstration (PBD) problem is solved using aversion-space learning algorithm, where the goal is to acquire the normal behavior in terms of repetitive tasks. Whena user is found to start a repetitive task of going through a sequence of states, the system can use the learned actionsequence to map from initial to goal states directly. This objective relates to our goal in that the observed user tracesQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143111are used to learn sequences of actions. However, a difference is that with the PBD system [27], the aim is to learnaction sequences rather than action models.Plan traces have also been exploited to learn situation-action rules in [3], where a decision-tree learning algorithmis applied to learn if-then rules that represent a control strategy for an automated agent. From a sequence of sensorreadings and control actions taken by human users that represent a complex man-machine operation, such as flying aBoeing 747 jetliner, situation-action rules can be learned to map from sensor readings to actions taken. The conditionpart of the rules can be considered as a form of preconditions learned to guide the application of the actions in thethen part of the rules.Action-model learning has also been considered for more complex forms of planning as well. CaMeL [21–23] is acandidate-elimination based learning algorithm that acquires the method-preconditions of a hierarchical task network(HTN) planning domain for a given set of task-decomposition schemata [10,45]. These schemata are also known asmethods [32,33]. The input of CaMel consists of collections of plan traces and derivational trees that are used to derivethe plan traces. Their aim is to learn preconditions that control when a decomposition method is applicable. In orderto overcome the problem of the lack of training data, in the CeMel system, a stronger form of input is consideredas compared to ours, in that the inference process for each plan trace is also taken as part of the input. It would beinteresting in future work to consider how to incorporate this additional form of problem-solving knowledge in thelearning process, as well as how to extend the learning problem to HTN schemata. We will discuss these issues furtherin the future work part of the paper.2.4. Satisfiability problemsA propositional logic formula F can always be transformed into a conjunctive normal form as the conjunctionof m clauses, where each clause is the disjunction of a set of literals. A literal is either a variable or the negation of avariable. Given a formula F , we can define a satisfiability problem (SAT) asGiven a formula F in conjunctive normal form, is there a truth assignment of the logical variables that makes Ftrue?The satisfiability problem is known to be NP-Complete [15]. There are two classes of solutions to the SAT problem.The first class consists of exact algorithms including logical approaches [8] and integer programming approaches [24].The second class are heuristics based, such as the GSAT heuristic [38]. These algorithms search for a satisfying truthassignment for a SAT problem.Recently, extensions of the SAT problems have been studied by several researchers. Related to our problem oflearning action models is the weighted MAX-SAT problem which can be stated asGiven a collection C of m clauses, C1, . . . , Cm involving n logical variables, with clause weights wi , find a truthassignment that maximizes the total weight of the satisfied clauses in C.The weighted MAX-SAT problems are also shown to be NP-hard. Goemans and Williamson [19] have shown that itis feasible to approximate MAX-SAT within a factor of 0.758 from the optimal solution in polynomial time.The weighted MAX-SAT problems are getting increasing attention in theory and in practice. As a result, severalefforts have been made to implement MAX-SAT solvers. In [26,39], an efficient algorithm for solving weightedMAX-SAT problems was presented, and a software tool was made available [11]. Richardson and Domingos [35,36]presented a unified framework for statistical relational learning using a Markov logic network (see a more detaileddiscussion in Section 2.5). It was suggested that MaxWalkSat can be applied for finding the approximately satisfyingassignments of truth values to the ground predicates as a result of logical inference.In [7], Borchers and Furman describe a two-phase algorithm for solving MAX-SAT and weighted MAX-SATproblems. In the first phase, they use a GSAT heuristic to find a good solution to solving the problem. This solutionserves as a seed in generating heuristics for the second phase. In the second phase, they use an enumeration procedurebased on the Davis–Putnam–Loveland algorithm [8,28] to find a provably optimal solution. The first heuristic stageimproves the performance of the algorithm by obtaining an upper bound on the minimum number of unsatisfiedclauses that can be used in pruning branches of the search tree, and use this upper bound to guide further searches.112Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143When compared with an integer programming approach for the problem, they found their algorithm to be better thanthe integer programming methods for some classes of problems, while the latter is better for other classes. In thispaper, we call their method the weighted MAX-SAT solution. The solution is available as an implemented software athttp://infohost.nmt.edu/~borchers/maxsat.html.In this paper, we concern ourselves mainly with the problem of how to convert an action-model learning problemto a weighted MAX-SAT problem. Once this conversion is done, the weighted MAX-SAT problem is then solvedusing any proven algorithms. Also, in the discussion that follows, we use the term “constraints” and “clauses” inter-changeably.2.5. Markov logic networksAnother related area is Markov logic networks (MLN), which integrates statistical learning in Markov networksand symbolic logic in a powerful and unified learning and inferencing framework [9,35,36]. In a Markov network,there is a set of variables that correspond to the network nodes, X = (Xi, . . . , i = 1 . . . n), where the joint distributionis defined in a log-linear model. Over this network, a first order logic framework is represented by including the logicconstructs, that is, constants, predicates, and truth value assignments. In addition, a knowledge base is given thatincludes a set of first order logic formulae. Two tasks are defined for learning. The first task is to learn the structureof the Markov network, which can be solved using an inductive logic programming approach (ILP). The second taskis to learn the parameters of a MLN, the most important component being the weights associated with the features,which can be learned using methods such as conjugate gradient or iterative scaling. Once learned, the MLN can beused to accomplish a number of inferencing tasks.Once learned, the MLN can be used to perform logical inference. Inference can be in the form of asking a queryof a knowledge base, and computation can be done to find truth-value assignments that satisfy all knowledge baseformulae. One method to accomplishing this task is to exploit a weighted MAX-SAT solver such as MAXWalkSat,which maximizes the sum of weights of satisfied clauses, or conversely, minimizes the sum of weights of unsatisfiedclauses.The MLN can be used to model and learn the action-model-learning problem described subsequently. There is adirect mapping between the components in action models and the structure and parameters of a MLN. We will returnto this correspondence in Section 5.2.2.6. Relation to SLAF algorithmsAmir et al. [2,40,41] presented a tractable and exact solution to a version of the action-model learning problemusing a technique known as Simultaneous Learning and Filtering (SLAF). In this version of the learning problem,the aim is to learn the actions’ effects in a partially observable STRIPS domain. The input consists of a finite set ofpropositional fluents, a set of world states, a finite set of actions as well as a transition relation that maps from statesto states by actions. The training examples consist of sequences of actions where partial observations of states aregiven. The objective is to build a complete explanation of observations by models of actions through a ConjunctiveNormal Form (CNF) formula. By tying the possible states of fluents with the effect propositions in the action models,the complexity of the CNF encoding can be controlled to permit exact solutions efficiently in some circumstances.This work is closely related to ours due to the similar objectives of learning action models. However, the problemsbeing solved by the SLAF techniques are different from ours. In Amir et al.’s work, the aim is to learn action modelsfrom state observations; if the state observations were empty, then their technique will not work. In addition, the SLAFmethods find exact solutions. In contrast, in our case, we aim to learn approximate models from the action sequenceswith only possibly unknown intermediate states for training.2.7. PDDL backgroundPDDL (Planning Domain Definition Language) is designed to formally specify actions and plans in a unified andexpressive language inspired by the well-known STRIPS formulation of planning problems [14,17]. There are severalmain components of PDDL. The actions are represented in a set of schemata, where each action consists of an actionname, a typed parameter list, a precondition list and an effect list. The effects can be partitioned into an add list and aQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143113Table 3A domain description in PDDL(define(:requirements(:types(:predicates(:action:parameters:precondition:effect))(domain vehicle):strips :typing)vehicle location fuel-level)(at ?v:vehicle ?p:location)(fuel ?v:vehicle ?f:fuel-level)(accessible ?v:vehicle ?p1:location ?p2:location)(next ?f1:fuel-level ?f2:fuel-level))drive(?v:vehicle ?from:location ?to:location ?fbefore:fuel-level ?fafter:fuel-level)(and (at ?v ?from), (accessible ?v ?from ?to), (fuel ?v ?fbefore),(next ?fbefore ?fafter))(and (not (at ?v ?from)), (at ?v ?to), (not (fuel ?v ?fbefore)), (fuel ?v ?fafter))delete list following the STRIPS formalism. When applied to a state, the semantics of an action specifies the conditionin which the action can be applied, as well as the outcome state as a result. The pre- and postconditions of actions areexpressed as logical propositions and logical connectives, where the postconditions can be split into add and deletelists. An action can be instantiated first before being applied to a state. For example, an action schema might specifyload(?x ?y) which specifies the action of loading object ?x into vehicle ?y. When being applied, ?x can be replacedby an object Book1 whereas ?y can be replaced by Truck2, resulting in an action load(Book1 Truck2). In particular,an action a can be applied to a state s if all of the action’s preconditions are members of the state s. The next state canbe obtained by deleting the delete list relations and then adding the add-list relations to s. One important extensionof PDDL over the STRIPS language is the ability to specify typed parameters. For example, ?x : vehicle in Table 3specifies that the variable ?x can only be bound to an object that is a vehicle.Table 3 illustrates how PDDL can be used to model a domain in which a vehicle can move between locations[14] (where the relations preceded by “not” operator can be considered as delete-list items). There are five levels oflanguage descriptions in PDDL 2.1, each allowing more expressivity such as consumable resource requirements. Inthis paper, we focus on the STRIPS level of PDDL 2.1 [14]. An example of a problem description in the vehicledomain is shown in Table 4, which includes both an initial state and a goal state.3. Problem statementIn this paper, we will learn a PDDL (level 1, STRIPS) representation of actions. A planning task P is specifiedin terms of a set of objects O, an initial state I, a goal formula G, and a set of operator schemata O. I, G, and Oare based on a collection of predicate symbols. States are sets of logical relations that are instantiated with actualparameters. Every action has a precondition list, which is a list of formulas that must hold in a state just before theaction for the action to be applicable. An action also has an add list and a delete list that are sets of atoms. A planis a partial order of actions that, when successively applied to the initial state in consistent order, yields a state thatsatisfies the goal formula.The input to the action-learning algorithm ARMS is a set of plan examples, where each plan example consists of(1) a list of propositions in the initial state, (2) a list of propositions that hold in the goal state, and (3) a sequenceof action signatures consisting of action names and actual parameters (that is, instantiated parameters). Each plan issuccessful in that it achieves the goal from the initial state. However, the action models for some or all actions maybe incomplete in that some preconditions, add and delete lists of actions may not be completely specified. In the inputinstances to ARMS, the plan examples may only provide action names such as drive(t0, ds0, dp0), where t0, ds0 anddp0 are objects, but the plan examples do not give the action’s preconditions and effects. These are to be learned byour algorithm.In each plan, the initial and goal states are replaced by two special actions whose models do not need to be learned.The first action of a plan, which is denoted by αinit, replaces the initial state. This action has no preconditions and hasas its add list all the initial-state relations. Its delete list is empty. Similarly, the last action of a plan, which is αgoal,replaces the goal state. The preconditions of αgoal are the goal relations, but the effects are empty.114Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Table 4A problem instance associated with the vehicle domain(define(:domain(:objects(:init)(:goal))(problem vehicle-example)vehicle)truck car - vehiclefull half empty - fuel-levelParis Berlin Rome Madrid - location)(at truck Rome)(at car Paris)(fuel truck half)(fuel car full)(next full half)(next half empty)(accessible car Paris Berlin)(accessible car Berlin Rome)(accessible car Rome Madrid)(accessible truck Rome Paris)(accessible truck Rome Berlin)(accessible truck Berlin Paris)(and (at truck Paris)(at car Rome))Our objective is to learn a complete set of action definitions with precondition, add and delete lists assigned toeach action schema. This collection is called an action model. In order to ensure the generality of the learned actionmodels and to avoid overfitting the models to the training examples, the action models are not required to be 100%correct. Thus, we will define the notion of an error rate which is evaluated on a separate set of plans known as a testset. For each action in the test set, we replace it with the learned action model. We then define the following metricsto evaluate an action model.We formally define the correctness of a plan following the STRIPS model. We consider a plan as a sequence ofactions, and a state as a set of atoms.Definition 1. A plan is said to be correct with respect to an action model if according to the action model, (1) eachaction’s preconditions hold in the state just before that action and (2) all goal propositions hold after the last action.If a precondition of an action is not satisfied in the preceding state of an action in a plan example, then we say anerror occurs, and we use E(a) = E(a) + 1 to count a’s error in P .Definition 2. In a plan P , one count of error is said to have occurred if an action’s precondition p is not true in thestate just before the action a in P . Let E(P ) be the total count of errors in a plan P . Then the error rate of P is(cid:2)E(P ) =(cid:2)a∈Pa∈P E(a)|precond(a)|For a set of plans ΣP , we can define an error rate of an action model with respect to the plans in ΣP similarly asthe total count of errors E(ΣP ) in ΣP over the total number of preconditions of all actions in ΣP .We can conversely define the correctness of an action model with respect to a plan or a set of plans.Definition 3. An action model is correct with respect to a plan if according to the action model, the error count is zerofor the plan. Similarly, an action model is said to be correct with respect to a set of plans if the error rate is zero forthe set of plans.Another aspect of the action model is that we wish to preserve the usefulness of all actions in the training examples.This is important because one can easily assign all goals to one of the actions in a plan while leaving the preconditionsQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143115and post-conditions of all other actions empty; this model is clearly not an ideal one, even though the resulting plansare still correct. However, by doing so, most of the actions in a plan are rendered useless. In our learning framework, toensure the learned action model is non-trivial, we first require that every action’s add list is non-empty. Furthermore,we apply a basic assumption that every action’s add list member in an example plan is useful for some later actions inthe same plan. Conversely, we can define the notion of redundancy as follows.Definition 4. An add-list element r of an action a in a plan P is non-redundant in P if there exists an action b orderedafter a in the plan, where r is in the precondition of b, and there is no action between a and b that adds r. Otherwise,r is said to be redundant.The intuition is that an action’s add-list relation r is redundant in a plan P if this relation is not “useful” inachieving the precondition of any later actions in the plan. The number of redundant add list elements in all exampleplans provides a measure of how small the action model is. In the extreme case, if all relations are in the add list of allactions, then the example plans are all made trivially correct. However, in such a case, the level of redundancy for theaction model is also at a maximum. Thus, it is a tricky task to balance the number of relations in an action model andthe number of errors incurred by the action model on a set of training plans.We can measure the redundancy rate of actions in a set of plan examples as follows. For every action a, let R(a)be the number of add list relations that are not used to achieve a precondition of a later action in the plan. Then for aset of plans ΣP , the redundancy rate is:(cid:2)(1)R(ΣP ) =(cid:2)a∈ΣPR(a)|addlist(a)|a∈ΣPFinally, we define our ideal target of learning.Definition 5. An action model is concise if it is both correct and has the lowest redundancy rate among all correctaction models with respect to a set of example plans ΣP .Intuitively, we consider an action model to be a good one if it explains as many plan examples as possible inthe testing plan examples, and if it is the simplest one among all such correct models. Note that a concise modelis not necessarily the smallest model among all correct action models, since it might still be possible to furtherreduce the number of add-list relations if one simultaneously removes some precondition relations. Such global op-timization would be difficult to achieve computationally. Thus we settle for the notion of a concise model in thiswork.Note also that in our problem statement, we assumed that, in our learning problem, the actions are observed whilethe states are not. If, however, the actions themselves are not observable, but the states are observable, then it isnecessary to know whether for every pair of adjacent observed states, the same action occurred. In this case, if thestate values are continuous, then the first problem is one of time-series segmentation, where the continuous signalsequence is a time series of sensor values. The segmentation problem is one in which one determines the beginningand end of each action. Subsequently, one can still apply our learning algorithm to learn the actions’ preconditionsand effects. An initial attempt at solving this problem is [47,48]. If the states are discrete-valued, then the problemis one of predicting a minimal set of actions and the associated models, such that the number of actions, sizes ofprecondition and effect sets, as well as the model’s error rate, are minimized according to the minimal descriptionlength principle [20].4. The ARMS algorithm4.1. OverviewGiven a set of correct example plans ΣP , we wish to uncover the constraints and use them to confine the space oflearned models. To illustrate our ideas, consider a simple example. Let there be a single plan P in which the initialstate contains two relations p1, p2, and the goal has one relation g. Let P contain two actions a1 and a2 where a1 isordered before a2. There is an additional relation p3 that is not present in the initial state.116Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143To learn action models for a1 and a2, we can consider the following possibilities:(1) p1 and p2 both belong to the precondition list of a1, and a2 adds the goal g. a1’s add and delete lists and a2’sprecondition list are all empty.The problem with this action model is that a1 becomes a redundant action, and the order between a1 and a2remains unexplained.(2) An alternative model is to assign p1 to the precondition of a1 and p2 to the precondition of a2. a1’s add listcontains p3 which is a precondition of a2. Finally, a2 adds the goal g. In this model, both a1 and a2’s delete listsare empty.In the second model, both actions are non-redundant. In addition, the explained plan is correct. Therefore, this isa preferred model.In general, for a given set of plans ΣP , there may be a large number of potential action models. To find the modelsthat have low error and redundancy rates, we uncover a number of constraints from the plans. The first type of con-straints constrain the preconditions, add and delete lists for any single actions. These constraints must be universallytrue. Therefore, the precondition of an action cannot appear in the add list of an action, and the delete list must be asubset of the precondition list. This constraint is known as the action constraint. The second type of constraint restrictsthe reason why an action a1 occurs before another action a2 in a plan P . This may be because a1 achieves a precon-dition p for a2, or a1 deletes a relation r that is added back by a2, or both a1 and a2 require a common preconditionr. This type of constraint applies to two or more actions in a plan, and is called plan constraints. Finally, if we observea relation r to be true between two actions a1 and a2, we can give r higher priority in assigning it to the add list ofa1 and precondition list of a2. We also wish to limit the size of preconditions, add and delete lists of all actions to beno more than a predefined number k. We call these types of preference constraints information constraints.Because for any example plan set SigmaP , there may be a large number of constraints that can be uncovered,we prefer to use the statistical information to generate a relatively small set of constraints as the basis of learning.In particular, we apply frequent set mining to find the frequent subsets of actions in which to apply the plan andinformation constraints. We describe the algorithms in detail in the next section.4.2. Algorithm and constraintsOur ARMS algorithm is iterative, where in each iteration, a selected subset of actions in the training plan examplesare learned. An overview of our algorithm is as follows:The ARMS AlgorithmInput: Variables, Constants and Relations in the Domain; A set of correct plan examples.Output: A set of action models ΘStep 1 We first convert all action instances to their schema forms by replacing all constants by their correspondingvariable types. Let Λ be the set of all incomplete action schemata. Initialize the action models in Θ by the set ofempty action schemata.Step 2 For the unexplained actions in Λ, build a set of information and action constraints based on individual actionsand call it Ω. Apply a frequent-set mining algorithm [1] to find the frequent sets of connected actions and relations(see the next subsection). Here connected means the actions and relations must share some common parameters.Let the frequent set of action-relation pairs be Σ.Step 3 Build a weighted maximum satisfiability representation Γ based on Ω and Σ based on the constraints andfrequency information uncovered in Step 2.Step 4 Solve Γ using a weighted MAX-SAT solver. In the experiments, we used both http://www.nmt.edu/~borchers/maxsat.html and the MaxWalkSat solver [26,39]. Section 6.3.4 provides more details on the application ofMaxWalkSat.Step 5 Select a set A of learned action models in Λ with the highest weights. Update Λ by Λ − A. Update Θ byadding A. If Λ is not empty, go to Step 2.Step 7 Output the action models Θ.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143117The algorithm starts by initializing the plans by replacing the actual parameters of the actions by variables of the sametypes. This ensures that we learn action models for the schemata rather than for the individual instantiated actions.Subsequently, the algorithm iteratively builds a weighted MAX-SAT representation and solves it. In each iteration,a few more actions are explained and are removed from the incomplete action set Λ. The learned action models inthe middle of the program help reduce the number of clauses in the SAT problem. ARMS terminates when all actionschemata in the example plans are learned.Below, we explain the major steps of the algorithm in detail.4.3. Step 1: Initialize plans and variablesA plan example consists of a sequence of action instances. We convert all such plans by substituting all occurrencesof an instantiated object in every action instance with the variables of the same type. If the object has multiple types,we generate a clause to represent each possible type for the object. For example, if an object o has two types Blockand Table, the clause becomes: {(?o = Block) or (?o = Table)}. We then extract from the example plans all sets ofactions that are connected to each other; two actions a1 and a2 are said to be connected if their parameter-type listhas non-empty intersection. The parameter mapping {?x1 =?x2, . . .} is called a connector.4.4. Step 2: Build action and plan constraintsAs introduced in Section 2, a weighted MAX-SAT problem consists of a set of clauses representing their conjunc-tion, where each clause is associated with a weight value representing the priority in satisfying the constraint. Givena weighted MAX-SAT problem, a weighted MAX-SAT solver finds a solution by maximizing the sum of the weightvalues associated with the satisfied clauses.In the ARMS system, we have four kinds of constraints to satisfy, representing three types of clauses. They areaction, information and plan and relation constraints.4.4.1. Action constraintsAction constraints are imposed on individual actions. These constraints are derived from the general axioms ofcorrect action representations. A relation r is said to be relevant to an action a if they are the same parameter type. Letprei, addi and deli represent ai ’s precondition list, add-list and delete list. The general action constraints are translatedinto the following clauses for building the SAT:(1) (Constraint A.1) The intersection of the precondition and add lists of all actions must be empty.prei∩ addi = φ.(2) (Constraint A.2) In addition, if an action’s delete list includes a relation, this relation is in the action’s preconditionlist. Thus, for every action, we require that the delete list is a subset of the precondition list.deli ⊆ prei.To illustrate action constraints, consider the following example.Example 1. Consider the action signature load(?x:hoist ?y:crate ?z:truck ?p:place) in the Depot domain of Interna-tional Planning Competition (IPC, see Table 2). From the domain description, the possible relations of the actionsignature load(?x ?y ?z ?p) are (at ?x ?p), (available ?x), (lifting ?x ?y), (at ?y ?p), (in ?y ?z), (clear ?y), and (at ?z ?p).The precondition list pregoal of the last action consists of (on ?y:crate ?s:surface). Suppose that the primary effect ofthe action load(?x ?y ?z ?p) is (in ?y ?z). For this action, the action constraints are given as follows:• The preconditions include all possible relations that are joined together by a disjunction. The possible relations ofthe add and delete list are (lifting ?x ?y), (at ?y ?p), (in ?y ?z), (clear ?y), or (at ?z ?p).• Constraint A.1 can be encoded as the conjunction of the following clauses,– (lifting ?x ?y)∈ addi ⇒ (lifting ?x ?y) /∈ prei118Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143– (at ?y ?p)∈ addi ⇒ (at ?y ?p) /∈ prei– (in ?y ?z)∈ addi ⇒ (in ?y ?z) /∈ prei– (clear ?y)∈ addi ⇒ (clear ?y) /∈ prei– (at ?z ?p)∈ addi ⇒ (at ?z ?p) /∈ prei– (lifting ?x ?y)∈ prei– (at ?y ?p)∈ prei– (in ?y ?z)∈ prei– (clear ?y)∈ prei– (at ?z ?p)∈ prei⇒ (at ?y ?p) /∈ addi⇒ (in ?y ?z) /∈ addi⇒ (clear ?y) /∈ addi⇒ (at ?z ?p) /∈ addi• Constraint A.2 can be encoded as follows,⇒ (lifting ?x ?y) /∈ addi– (lifting ?x ?y)∈ deli ⇒ (lifting ?x ?y)∈ prei– (at ?y ?p)∈ deli ⇒ (at ?y ?p)∈ prei– (in ?y ?z)∈ deli ⇒ (in ?y ?z)∈ prei– (clear ?y)∈ deli ⇒ (clear ?y)∈ prei– (at ?z ?p)∈ deli ⇒ (at ?z ?p)∈ prei4.4.2. Information constraintsThe information constraints are used to explain why the optionally observed intermediate states exist in a plan. Theconstraints thus derived are given high priority because they need not be guessed.Suppose we observe a relation p to be true between two actions an and an+1, and p, ai1 , . . . , and aik share thesame parameter types. We can represent this fact by the following clauses, given that ai1 , . . . , and aik appear in thatorder.• (Constraint I.1) The relation p must be generated by an action aik (0 (cid:2) ik (cid:2) n), that is, p is selected to be in the∪ · · · ∪ addik ), where ∪ means logical “or”.• (Constraint I.2) The last action aik must not delete the relation p; that is, p must not be selected to be in the deleteadd-list of aik . p ∈ (addi1∪ addi2list of aik: p /∈ delik.Example 2. Consider the intermediate state (lifting h0 c0) in Plan 1 of Table 2. This can be generated by any of thepreceding actions lift(h1 c0 p1 ds0), load(h1 c0 t0 ds0) or unload(h0 c0 t0 dp0). However, it cannot be deleted byunload(h0 c0 t0 dp0). The SAT clauses are as follows:• (I.1) (lifting?x?y) ∈ (addlift ∪ addload ∪ addunload);• (I.2) (lifting?x?y) /∈ delunload.Finally, we consider a ‘soft’ type of information constraint to define pairs of relations and actions where the relationr frequently occurs before action a. This is often the case when the initial states of a plan contain many relations, someof which often occurs before the first action of a plan. In this case, we predict that these facts are preconditions of theaction. This constraint applies also to other parts of a plan, where the partially observable states are in the middle of aplan.• (Constraint I.3) We define the weight value of a relation-action pair (p, a) as the occurrence probability of thispair in all plan examples. If the probability of a relation-action pair is higher than the probability threshold θ ,then we set a corresponding relation constraint p ∈ PRECONDa, which receives a weight value equal to its priorprobability.Example 3. Consider the example in Table 2. A relation-action pair (clear c0), lift(h1 c0 p1 ds0) (sharing a parameter{c0}) from Plan1 and relation-action pair (clear c1), lift(h1 c1 c0 ds0) (sharing a parameter {c1}) from Plan2 can begeneralized to (clear ?y)∈ prelift (labelled with {?y}). Thus, the occurrence count for (clear ?y), lift(?x ?y ?z ?p) withthe parameter label ?y is at least two. Later, in Section 4.4.3, we will define this count to be the support count forfrequent-sequence analysis. Table 5 shows examples of the I.3 type of constraints.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143119Table 5Examples soft information constraintsParameters{?y}{?x, ?p}{?x }{?z, ?p}{?y, ?p}{?y, ?z}{?x, ?y}{?y, ?z}Information constraints(clear ?y) ∈ prelift(at ?x ?p) ∈ prelift(available ?x) ∈ prelift(at ?z ?p) ∈ prelift(at ?y ?p) ∈ prelift(on ?y ?z) ∈ prelift(at ?x ?y) ∈ predrive(on ?y ?z) ∈ adddrop4.4.3. Plan constraintsThe plan constraints represent the relationship between actions in a plan to ensure that a plan is correct and thatthe actions’ add list relations are not redundant. First, for each action in a plan, we generate a constraint for therequirement for a plan to be correct. The first of the plan constraints is as follows:• (Constraint P.1) Every precondition p of every action b must be in the add list of a preceding action a and is notdeleted by any actions between a and b.• (Constraint P.2) In addition, at least one relation r in the add list of an action must be useful in achieving aprecondition of a later action. That is, for every action a, an add list relation r must be in the precondition of alater action b, and there is no other action between a and b that either adds or deletes r.While constraints P1 and P2 provide the general guiding principle for ensuring a plan’s correctness, in practicethere are too many instantiations of these constraints. This is because for a given action’s precondition, any precedingaction could serve as an “establisher” of the relation. To ensure efficiency, we design heuristics that consider thereasons that can explain why two actions frequently co-exist. These heuristics allow us to restrict ourselves to onlya small subset of frequent action pairs. When explaining which action adds a relation for the precondition of whichother actions, we give higher priority to these actions.Our heuristic is based on the notion of association-rule mining in data mining area [1]. The following constraintsP.3–P.6 are applied to a set of action pairs that co-occur frequently. The rationale of these constraints is that if a pairof actions occur frequently enough in the plan examples, there must be a reason for their frequent co-existence. Thus,the possible reasons for this co-occurrence can be enumerated to uncover these reasons.Our first task is to find frequent action pairs that occur in the plans. We apply the Apriori algorithm [1] to findfrequent action pairs (cid:8)ai, aj (cid:9) where 0 (cid:2) i < j (cid:2) n − 1 are the indexes of the actions. To capture how often a frequentpair occurs in the plan examples in a training set of plan examples, we make the following definitions.Definition 6. Let S be an action pair (cid:8)ai, aj (cid:9), 0 (cid:2) i < j (cid:2) n − 1. The support count of the pair S in a set of plans ΣPis the number of times that this pair occurs in the plans from ΣP .The support count is an integer value. We can equivalently talk about the support rate which is the percentage ofthe occurring pairs:Definition 7. Let S be an action pair (cid:8)ai, aj (cid:9), 0 (cid:2) i < j (cid:2) n − 1, where n is the number of actions in a plan. Thesupport rate of the pair S in a set of plans ΣP is the percentage that this pair occurs in ΣPsupport count ((cid:8)ai, aj (cid:9))ZΣP(cid:4)(cid:3)(cid:8)ai, aj (cid:9)support rate=where ZΣP is the total number of action pairs in ΣP .‘Frequent pairs’ is a relative term. Here we follow the data mining literature to define a probability threshold θ , sothat all action pairs whose support rate is above or equal to θ are considered frequent enough. We apply the subsequent120Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143constraints only to explain the frequent action pairs. Of course, θ has to be defined differently for each domain, and thisis indeed a free parameter we wish to tune in our later experimental part. Note that unlike general association rules,we do not suffer from the problem of generating too many redundant association rules as in data mining research,since we only apply the Apriori algorithm to find the frequent pairs. These pairs are to be explained by the learningsystem later.Let there be an action pair (cid:8)ai, aj (cid:9), 0 (cid:2) i < j (cid:2) n − 1.• (Plan constraint P.3) One of the relevant relations p must be chosen to be in the preconditions of both ai and aj ,but not in the delete list of ai ,∃p (p ∈ (prei∩ prej ) ∧ p /∈ (deli))• (Constraint P.4) The first action ai adds a relevant relation that is in the precondition list of the second action ajin the pair,∃p (p ∈ (addi ∩ prej ))• (Constraint P.5) A relevant relation p that is deleted by the first action ai is added by aj . The second clause isdesigned for the event when an action re-establishes a fact that is deleted by a previous action.∃p (p ∈ (deli ∩ addj ))• Let prei, addi and deli represent ai ’s precondition list, add-list and del-list, respectively. The above plan con-straints can be combined into one constraint Φ(ai, aj ) in ARMS. Φ(ai, aj ) is restated as:∩ prej ) ∧ p /∈ (deli)) ∨ (p ∈ (addi ∩ prej )) ∨ (p ∈ (deli ∩ addj )))∃p ((p ∈ (preiAs an example, consider the Depot domain (Table 2).Example 4. Suppose that lift(?x1:hoist ?y1:crate ?z1:surface ?p1:place), load(?x2:hoist ?y2:crate ?z2:truck ?p2:place))is a frequent pair. We explain the connection between the two actions, lift and load, by selecting a candidate relation.The relevant parameters that share the same type are ?x1 = ?x2, ?y1 = ?y2, ?p1 = ?p2. The relations that refer tothese parameters are (at ?x ?p), (available ?x), (lifting ?x ?y), (at ?y ?p), and (clear ?y). From this action pair, the SATclauses can be constructed as follows (we use ?x to represent the parameter type “hoist”, ?y to represent “crate”, and?p to represent “place”):• At least one relation among (at ?x ?p), (available ?x), (lifting ?x ?y), (at ?y ?p), and (clear ?y) is selected to explainthe connection between lift(?x ?y ?z ?p) and load(?x ?y ?z ?p).• If f (?x), where f (?x) can be (at ?x ?p), (available ?x), (lifting ?x ?y), (at ?y ?p), and (clear ?y), is selected toexplain the connection between lift(?x ?y ?z ?p) and load(?x ?y ?z ?p), then one of the following is true: (a) f (?x)is in the precondition list of action load(?x ?y ?z ?p) and the add list of lift(?x ?y ?z ?p), (b) f (?x) is in the deletelist of action lift(?x ?y ?z ?p) and add list of load(?x ?y ?z ?p), or (c) f (?x) is in the precondition list of actionlift(?x ?y ?z ?p), but not in the del list of action lift(?x ?y ?z ?p), and in the precondition list of load(?x ?y ?z ?p).4.5. Step 3: Build and solve a weighted MAX-SAT problemIn solving a weighted MAX-SAT problem in Step 3, each clause is associated with a weight value between zeroand one. The higher the weight, the higher the priority in satisfying the clause. In assigning the weights to the threetypes of constraints in the weighted MAX-SAT problem, we apply the following heuristics:Action constraints Every action constraint receives a constant weight WA(a) for an action a. The weight for actionconstraints is set to be higher than the weight of information constraints.Information constraints Every partial information constraint receives a constant weight WI (r) for a relation r. Theconstant assignment is empirically determined too, but they are generally the highest in all constraints’weights.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143121Table 6Examples of highly supported information constraintsLabel{?y}{?x, ?p}{?x}{?z, ?p}{?y, ?p}{?y, ?z}{?x, ?y}{?y, ?z}Information constraints(clear ?y) ∈ prelift(at ?x ?p) ∈ prelift(available ?x) ∈ prelift(at ?z ?p) ∈ prelift(at ?y ?p) ∈ prelift(on ?y ?z) ∈ prelift(at ?x ?y) ∈ predrive(on ?y ?z) ∈ adddropSupport countSupport rate (%)3333331310010010010010010033100For information constraints I.3, we define the weight value of a relation-action pair (p, a) as the occur-rence probability of this pair in all plan examples. If the probability of a relation-action pair is higher thanthe probability threshold θ , then we set a corresponding relation constraint p ∈ PRECONDa, which receivesa weight value equal to its prior probability. If not, the corresponding relation constraint receives a constantweight of a lower value which is determined empirically. This assignment corresponds to a new type ofheuristic constraints, which is especially useful for partial information constraints.Example 5. Consider the example in Table 2. A relation-action pair (clear c0), lift(h1 c0 p1 ds0) (sharing a parameter{c0}) from Plan1 and relation-action pair (clear c1), lift(h1 c1 c0 ds0) (sharing a parameter {c1}) from Plan2 can begeneralized to (clear ?y)∈ prelift (labelled with {?y}). Thus, the support count for (clear ?y), lift(?x ?y ?z ?p) with theparameter label ?y is at least two. Table 6 shows all information constraints along with their support rate values in theprevious example.Plan constraints The weight value of a plan constraint WP (a1, a2) is higher than the probability threshold (or aminimal support-rate value) θ . This value is set equal to the prior probability of the action pair (a1, a2).In the experiments, we will vary the value of the threshold θ to verify the effectiveness of algorithm. Whenconsidering subsequences of actions from example plans, we only consider those sequences whose supportsare over θ .Example 6. Consider the example in Table 2. An action sequence (cid:8)lift(h1 c0 p1 ds0), load(h1 c0 t0 ds0)(cid:9) that sharesparameters {h1, c0, ds0} from Plan1 and an action sequence (cid:8)lift(h1 c1 c0 ds0), load(h1 c1 t0 ds0)(cid:9) that sharesparameters {h1, c1, ds0} from Plan2 can be generalized to lift(?x1 ?y1 ?z1 ?p1), load(?x2 ?y2 ?z2 ?p2) (labelled with{?x1 = ?x2, ?y1 = ?y2, ?p1 = ?p2}). The connector {?x1 = ?x2, ?y1 = ?y2, ?p1 = ?p2} indicates that the parameters?x1, ?y1 and ?p1 in action lift(?x1 ?y1 ?z1 ?p1) are the same as the parameters ?x2, ?y2 and ?p2 in action load(?x2Table 7Examples of all action constraintsLabel{?x1 = ?x2, ?y1 = ?y2, ?p1 = ?p2}{?z1 = ?x2, ?p1 = ?y2}{?x1 = ?z2, ?z1 = ?p2}{?y1 = ?y2, ?p1 = ?p2}{?x1 = ?x2, ?p1 = ?p2}{?x1 = ?x2, ?p1 = ?p2}{?x1 = ?z2, ?z1 = ?p2}{?y1 = ?p2}{?p1 = ?p2 = ?y3 }{?z1 = ?x2 = ?z3 }{?z1 = ?p2 = ?p3 }{?p1 = ?p2 = ?p3 = ?y4}{?z1 = ?p2 = ?p3 = ?p4 }Plan constraintsSupport countSupport rate (%)Φ(lift, load)Φ(load, drive)Φ(drive, unload)Φ(unload, drop)Φ(load, lift)Φ(drop, unload)Φ(drive, load)Φ(drive, load)Φ(lift, load, drive)Φ(load, drive, unload)Φ(drive, unload, drop),Φ(load, lift, load, drive)Φ(drive, unload, drop, unload)54452111444211008080100402020208080804020122Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143?y2 ?z2 ?p2), respectively. Thus, the support for lift(?x1 ?y1 ?z1 ?p1), load(?x2 ?y2 ?z2 ?p2) with the parameter label{?x1 = ?x2, ?y1 = ?y2, ?p1 = ?p2} is at least two. Table 7 shows all plan constraints along with their support countand rate values.4.6. Step 5: Update initial states and plansARMS starts with a set Λ of incomplete action models. As more action schemata are fully learned, that is, when themaximum size of the action’s preconditions and add/delete lists reach an upper bound value U , the action is consideredas learned. All learned actions are removed from the set Λ in this step. If a learned action appears in the beginning ofa plan, ARMS updates the initial state by executing the actions in the current initial state, which produces a new initialstate.5. Properties of the ARMS algorithm5.1. Formal properties and evaluation metricsGiven a set of plan examples, we can find a large number of action models. Some models are superior to others.An advantage of ARMS is that when the action and plan constraints are satisfied by a set of plans, the plan is correct.Theorem 1. For a given set of training example plans ΣP , when the action constraints (A.1) and (A.2), and the planconstraint (P.1) are satisfied for all actions, the plans in P are correct.Proof. This theorem follows from the definition of plan correctness, which states that a plan is correct if all precon-ditions of all actions are true in the state just before that action (the goal included). Consider any single plan P in theplan set ΣP . For an action b, when the plan constraint (P.1) is satisfied, each of b’s preconditions r ∈ PRECOND(b) isin the add list of some action a before b, and no other actions between a and b delete r. The action constraints (A.1)and (A.2) defined in Section 4.4.1 ensure that r is not deleted by a itself. These conditions therefore ensure that r istrue just before b. Thus the plan is correct. (cid:2)The following theorem establishes that in the ideal case, no relation in the add list of an action model is redundantin any of the example plans.Theorem 2. For a given set of training example plans ΣP , when the plan constraint (P.2) is satisfied by all actions,the learned action models have zero redundancy rate with respect to ΣP (thus, they are concise).Proof. We wish show that in each example plan, every add list relation is used to establish some precondition of alater action in the same plan. This is ensured by the constraint (P.2), which states that every add list member of anaction a is useful in adding a relation r that is a member of a precondition of a later action b, and that no other actionbetween a and b adds or deletes r. Thus, the redundancy rate is zero for the action model on the training plans ΣP . (cid:2)It is important to guarantee that a learned action model is correct and has a low redundancy rate. In this case, weconsider the learned model to be approximately concise (see Definition 5. To achieve this objective, ARMS uses agreedy algorithm by explaining only the action pairs that are sufficiently frequent, where the concept of frequency isdefined by a probability lower bound value θ ). As a result, it is still possible that some preconditions of some actionsin the learned model are not explained, if the preceding actions do not appear frequently enough. We will use the errorrates E(P ) to estimate the degree of correctness of a plan, and the redundancy rate R(P ) to measure the degree ofredundancy in the training plans. We will show these two metrics as a function of several training plan parameters inthe next section.Finally, the number of clauses generated by ARMS is polynomial in the number of relations in the domain, plansizes, the number of training plans and the number of actions.Theorem 3. The number of clauses generated by ARMS for a set of example plans is polynomial in the number ofrelations, action schemata and the size of the example plans used for training.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143123Proof. Because the process of generating the constraints follows a finite sequence of steps, the complexity of ARMSmainly depends on the number of clauses and variables in the SAT problem.Consider a planning domain in which there are A actions in each plan and N plans. Let U be upper bound onthe number of the relations in the preconditions and postconditions of each action. Then the number of action con-straints is O(A ∗ U ∗ N). The number of plan constraints is O(A2 ∗ U ∗ N). The number of information constraints isO(U ∗ A ∗ N ). Thus, the total number of clauses is bounded by O(A2 ∗ N ∗ A ∗ U ). (cid:2)Note that due to the local-search nature of the ARMS algorithm, we cannot guarantee that the learned action modelis the smallest one theoretically. However, since the weighted MAX-SAT algorithm generates a solution for a SATproblem parsimoniously, the action model generated is generally small, thus the learned model is approximatelysimple. In the next section, we empirically verify the redundancy rates of the learned models.5.2. Relation to Markov logic networksIn the related works section, we have briefly mentioned that there exists a strong relation between action-modellearning and Markov logic networks (MLN). MLN combines Markov networks and first order logic in a unifiedlearning and inference framework [9,35,36]. In a Markov network, there is a set of variables that corresponds to thenetwork nodes, X = (Xi, . . . , i = 1 . . . n), where the joint probability distribution is defined in a log-linear model. Overthis network, a first order logic framework is represented by including the logic constructs that consist of constants,predicates and truth value assignments. In addition, a knowledge base is given that includes a set of first order logicformulae.In the formal definition of a MLN, there is a set of binary valued nodes for each possible grounding of eachpredicate appearing in the MLN [9,35,36]. There is also a set of features corresponding to each formula Fi in thenetwork, where the value of each Fi is binary. Associated with a formula is a weight value. The higher the weight, thestronger the features are. A strong feature of the MLN formalism is its ability to perform learning, where there are twotasks to be performed [36]. The first task is to learn the structure of the Markov network, which can be solved usingan inductive logic programming approach (ILP). The second task is to learn the weight parameters of a MLN, whichcan be learned using methods such as conjugate gradient or iterative scaling. Once learned, the MLN can be used toaccomplish a number of interesting inference tasks. For example, in collective classification, MLN can be applied topredict the classes of related objects such as linked Web pages, by considering the objects together.The ARMS algorithm can be considered as a special case of the MLN algorithm. In particular, we invent threepredicates as follows:• We invent a new predicate InPrecond such that, for a literal P and action A, InPrecond takes P and A asarguments in the form of InPrecond(P , A) to denote the fact that the literal P is assigned to the preconditionof the action A.• We invent a new predicate InAdd such that, for a literal E and action A, InAdd takes E and A as arguments inthe form of InAdd(E, A) to denote the fact that the literal E is assigned to the effects of the action A.• Similarly, we define InDelete(E, A) for the delete list items.Then, we can convert all the constraints mentioned in Section 4.2 by these new predicates. For example, the actionconstraint (A.1) (Section 4.4.1) can be represented as a knowledge-base (KB) formula:∀P ∈ Literals, A ∈ Actions.InPrecond(P , A) ⇒ ¬InAdd(P , A)which states that the intersection of preconditions and add list of all actions are empty.We can represent the action-model learning problem as a MLN learning problem, as follows. As stated above, weinvent new predicates InPrecond, InAdd and InDelete that apply to two types of objects: relations and actions.Relations are constructed using the domain specific predicates, variables and constants such as (on ?x,?y). Relationsbetween predicates include logical axioms that encode specific constraints in a problem domain. For example, in theblocks world, an axiom states that the predicate (clear ?x) = True preclude that there exists an object ?y, such that(on ?y,?x) = True. These axioms, as well as the constraints, form the relations between the nodes in the MLN. Theweights of the hard constraints, such as the above example, are set to be the highest. The action, information and124Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143plan constraints receive their weights according to the procedure in Section 4.4. Then, we can use an algorithm forsolving the corresponding weighted MAX-SAT problem for obtaining an approximate solution, in which the sumof all weights are maximized within limits imposed by the computational resources. In the experimental section,we describe two implementations of ARMS using the systems MaxSatSolver and MaxWalkSat, respectively. We willreport that the performance of these two weighted MAX-SAT solvers are similar in terms of the approximate solutionsfound and the CPU time spent.6. Experimental resultsIn this section we assess the effectiveness of the learned action model empirically. An action model may be incom-plete and error-prone with respect to a set of example plans. A previous evaluation method [16] applied a planningsystem to the incomplete action model, and then assessed the model’s degrees of correctness. In this work, we have anavailable set of example plans that can be split into training and testing sets. We adapt the cross-validation evaluationmethod by making full use of all available example plans. We split the given plan examples into two separate sets:a training set and a testing set. The training set is used to build the model, and the testing set is defined for assess-ing the model. We take each test plan in the test data set in turn and evaluate whether the test plans are correct andnon-redundant when modelled by the learned action model.6.1. Experimental setup and planning domainsIn order to evaluate ARMS, we generated example plans using an existing planning tool from the planning domainsin International Planning Competition 2002.1 The example plans are generated using the MIPS planner.2 In eachdomain, we first generated 200 plan examples. We then applied a five-fold cross-validation by dividing the examplesinto five parts of equal size. We selected four of the five parts which consist of 160 plan examples as the trainingset from four folds and use the remaining fold with 40 separate plan examples as the test set. This is repeated fivetimes using a different fold for the test data. We plot the mean values of various metrics with the associated error bars(corresponding to 95% confidence intervals) obtained from the five-fold cross-validation experiments. We ran all ofour experiments on a personal computer with 768 MB of memory and a Pentium Mobile Processor 1.7 GHz CPU,with the Linux operating system. Also, in all experiments, we set the upper bound U for all actions according to themaximum such value in the actual domain description. This parameter can be experimentally found by varying Ufrom a small value to a large value, and checking the resultant error and redundancy rates of the action models in alltest plans. The best such value can be determined this way empirically.The six domains are the Depots, Driverlog, Zenotravel, Satellite, Rover and Freecell planning domains. Featuresof the six domains are summarized in Table 8. In this table, we summarize the number of action schemata in eachdomain (# actions), the number of different predicates in each domain (# relations), the maximum number of relationsin a precondition list or a postcondition list of each domain (Max Pre/Eff), the maximum number of relations in theTable 8Features of the problem domainsDomain featuresDomain namesDepotsDriverlogZenotravelSatelliteRoverFreecell# actions# relationsMax Pre/EffMax InitialMax Goals# PlansAvg. Length564243200106534882002655419820024586285200165256636200231011101495200271 http://planning.cis.strath.ac.uk/competition/.2 http://www.informatik.uni-freiburg.de/~mmips/.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143125Table 9The learned action model for the Depot domain, θ = 10%ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:drive(?x:truck ?y:place ?z:place)(at ?x ?y)(at ?x ?z)(at ?x ?y)lift(?x:hoist ?y:crate ?z:surface ?p:place)(at ?x ?p),(available ?x),(at ?y ?p),(on ?y ?z), (clear ?y),(at ?z ?p)(lifting ?x ?y),(clear ?z)(at ?y ?p),(clear ?y),(available ?x),(on ?y ?z)drop(?x:hoist ?y:crate ?z:surface ?p:place)(at ?x ?p),(at ?z ?p),(clear ?z),(lifting ?x ?y)(available ?x),(clear ?y),(on ?y ?z)(lifting ?x ?y),(clear ?z)load(?x:hoist ?y:crate ?z:truck ?p:place)(at ?x ?p),(at ?z ?p),(lifting ?x ?y)(in ?y ?z),(available ?x),(at ?y ?p), (clear ?y)(lifting ?x ?y)unload(?x:hoist ?y:crate ?z:truck ?p:place)(at ?x ?p) (at ?z ?p) (available ?x) (in ?y ?z), (clear ?y)(lifting ?x ?y)(in ?y ?z),(available ?x),(clear ?y)Table 10The learned action model for the Driverlog domain, with θ = 10%ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:ACTIONPRE:ADD:DEL:load-truck (?obj:obj ?truck:truck ?loc:location)(at ?truck ?loc), (at ?obj ?loc)(in ?obj ?truck)(at ?obj ?loc)unload-truck(?obj:obj ?truck:truck ?loc:location)(at ?truck ?loc), (in ?obj ?truck),(at ?obj ?loc)(in ?obj ?truck)board-truck(?driver:driver ?truck:truck ?loc:location)(at ?truck ?loc),(at ?driver ?loc),(empty ?truck),(driving ?driver ?truck)(empty ?truck),(at ?driver ?loc)disembark-truck(?driver:driver ?truck:truck ?loc:location)(at ?truck ?loc),(driving ?driver ?truck)(at ?driver ?loc),(empty ?truck)(driving ?driver ?truck)drive-truck(?truck:truck ?loc-from:location ?loc-to:location ?driver:driver)(at truck ?loc-from),(driving ?driver ?truck), (path ?loc-from ?loc-to)(at truck ?loc-to),(empty ?truck)(at truck ?loc-from)walk(?driver:driver ?loc-from:location ?loc-to:location)(at ?driver ?loc-from), (path ?loc-from ?loc-to)(at ?driver ?loc-to)(at ?driver ?loc-from)initial state (Max Initial), and goal state (Max Goals) in the training examples, the number of example plans used fortraining and testing, and finally, the average number of actions in the training plans (Avg. Length). This summary tableserves to provide a sense of the complexity of the learning task in each domain. As we can see, the Freecell domainhas the most demanding learning task, with 10 action schemata, 11 relations and long plan lengths. The Depot domain126Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Table 11Varying the probability threshold in the planning domains (Depots)Probability threshold (%)012345678910305070DepotsE0.160.190.12 ± 0.030.24 ± 0.060.270.270.26 ± 0.040.190.190.190.190.190.190.19Table 12Varying the probability threshold in the planning domains (Driverlog)Probability threshold (%)Driverlog012345678910305070E0.090.080.060.050.050.050.050.050.050.050.050.050.050.05R0.180.200.170.24 ± 0040.250.250.22 ± 0060.110.110.110.110.110.110.11R0.14 ± 0.010.12 ± 0010.140.06 ± 0.050.040.040.040.040.040.040.040.040.040.04Table 13Varying the probability threshold in the planning domains (Zenotravel)Probability threshold (%)Zenotravel012345678910305070E00000000000000R0.130.130.130.130.130.130.130.130.10 ± 0.040.09 ± 0.040.09 ± 0.040.09 ± 0.040.09 ± 0.040.09 ± 0.04CPU (sec)9.6 ± 0.59 ± 0.77.8 ± 0.4776.8 ± 0.477.2 ± 1.17.2 ± 0.47.2 ± 0.47.4 ± 0.57.4 ± 0.57.4 ± 0.57.4 ± 0.5CPU (sec)53.4 ± 5.551.4 ± 3.042.8 ± 2.541.4 ± 2.334.8 ± 14.627 ± 16.520.6 ± 16.321.2 ± 17.221.4 ± 17.021.4 ± 17.021.4 ± 17.421.4 ± 17.421.4 ± 17.421.4 ± 17.4CPU (sec)7 ± 0.76.8 ± 0.47.2 ± 0.47.2 ± 0.87.2 ± 0.47.6 ± 0.56.8 ± 0.47.2 ± 0.47.8 ± 0.47 ± 0.77777Clauses6607 ± 44292 ± 32967 ± 725705 ± 884310 ± 3310 ± 3284 ± 57180 ± 3180 ± 3180 ± 3180 ± 3180 ± 3180 ± 3180 ± 3Clauses349 ± 3312 ± 4138 ± 2370 ± 1961616159 ± 441 ± 1741 ± 1741 ± 1741 ± 1741 ± 1741 ± 17Clauses103 ± 269 ± 262 ± 652 ± 640404040404040404040is one of the simplest in contrast. For all domains, we generated 200 example plans, which are subsequently split intotraining and testing sets.In our experiments, the following independent variables are varied:Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143127Table 14Varying the probability threshold in the planning domains (Satellite)Probability threshold (%)0123456789105070SatelliteE0.21 ± 0.080.23 ± 0.040.26 ± 0.010.26 ± 0.010.25 ± 0.030.21 ± 0.030.200.200.200.23 ± 0.070.26 ± 0.090.26 ± 0.090.26 ± 0.09Table 15Varying the probability threshold in the planning domains (Rover)Probability threshold (%)60616263646566676869707580RoverE0.680.66 ± 0.040.67 ± 0.020.67 ± 0.020.66 ± 0.010.66 ± 0.010.66 ± 0.010.670.67 ± 0.010.67 ± 0.010.67 ± 0.010.67 ± 0.010.67 ± 0.01Table 16Varying the probability threshold in the planning domains (Freecell)Probability threshold (%)60616263646566676869707580FreecellE0.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.01R0.07 ± 0.050.05 ± 0.020.030.030.04 ± 0.020.07 ± 0.020.070.070.070.070.07 ± 0.010.07 ± 0.010.07 ± 0.01R0.07 ± 0.020.06 ± 0.010.06 ± 0.020.07 ± 0.020.080.080.080.09 ± 0.010.080.080.080.080.08R0.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.010.47 ± 0.01CPU (sec)65.8 ± 0.45.8 ± 0.45.6 ± 0.55.4 ± 0.55.4 ± 0.55.4 ± 0.55.8 ± 0.45.2 ± 0.44.8 ± 1.64 ± 1.94 ± 1.94 ± 1.9CPU (sec)224 ± 17217 ± 11210 ± 21192 ± 15181 ± 1176 ± 12167 ± 11166 ± 9172 ± 8169 ± 2168 ± 4168 ± 4168 ± 4CPU (sec)388 ± 6387 ± 6390 ± 5390 ± 5390 ± 6390 ± 5390 ± 5390 ± 6390 ± 5390 ± 5389 ± 5389 ± 5389 ± 5Clauses688 ± 8140 ± 8134134137 ± 792 ± 2382828285 ± 687 ± 787 ± 787 ± 7Clauses62700 ± 677451078 ± 1335551078 ± 1335543368 ± 133553565729734 ± 1025923811 ± 1025926762 ± 1537123811 ± 102591788817887 ± 117887 ± 117887 ± 1Clauses91 ± 191 ± 191 ± 191 ± 191 ± 191 ± 191 ± 191 ± 191 ± 191 ± 191 ± 191 ± 191 ± 1128Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Fig. 1. Varying the probability threshold in four planning domains (Error Rate).Fig. 2. Varying the probability threshold in four planning domains (Redundancy Rate).The probability threshold θ This is used to control the frequent action pairs used in enforcing the plan constraints.When θ increases from zero to 100%, we expect to have fewer clauses in our weighted MAX-SAT formula-tion.The degree of partial information This is the amount of relational information that we can observe between the ac-tions in the training plans (partial information represented in percentage of the original states). This variableis defined to test how the ARMS algorithm might benefit from some partial information that can be occasion-ally observed in the middle of plans. Let p be a value that ranges from zero to 100%. We use p to controlthe amount of partial information that can be observed after an action, in terms of a total of p ∗ N relationswhere N is the total number of relations in a plan’s actions after summing up their preconditions, add anddelete lists together. When p ranges from zero to 100%, the amount of available state information increases.The plan length This is the average number of actions in the example plans. These three variables are varied fromsmall to large to test the performance of ARMS.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143129Fig. 3. Varying the probability threshold in four planning domains (CPU Time).Fig. 4. Varying the probability threshold in four planning domains (Clause Number).In all experiments, we wish to confirm the following hypotheses:Hypothesis 1 ARMS can learn accurate action models with low error rates and redundancy rates for domains underdifferent experimental conditions.Hypothesis 2 ARMS can accomplish the learning task within a reasonable amount of computational time.The first hypothesis will be verified by checking the average error rates E accumulated on the learned models whenthese models are applied to the testing examples, as well as the associated redundancy rates R. The second hypothesiswill be verified through the average CPU time and the number of clauses incurred on the 160 training plan examples(we reserve 40 for testing). We use “±” to denote the range of the error bar for the error rate and redundancy rate asshown in subsequent tables.130Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Fig. 5. Varying the partial state information (Error Rate).Fig. 6. Varying the partial state information (Redundancy Rate).In all subsections except the last one, we use the weighted MAX-SAT solver tool [6] to implement ARMS. In theSection 6.3.4, we replace MaxSatSolver with the MaxWalkSat system and report the comparison results.6.2. Learning in the depot domainTo give the readers an intuitive feeling of the resultant model learned by ARMS, we first describe the output gen-erated by ARMS for the Depot and Driverlog domains. In these learned models, it is possible to visually compare thelearned action models with the ground-truth action models, which are the PDDL domain descriptions from IPC 2002.In this example, the action model we use to generate the example plans are called the ideal action model (Mi ).From the plan examples, we learn a model Ml. In the table below, if a relation appears in both models, then it is shownby a normal font. If a relation appears only in Mi but not in Ml, then it is shown by an italic font. Otherwise, if aQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143131Fig. 7. Varying the partial state information (CPU Time).Fig. 8. Varying the partial state information (Clause Number).relation will only appear in Ml and not in Mi , it is shown in bold font. As can be seen in Tables 9 and 10, most partsof the learned action models are correct with respect to the ground-truth domain descriptions.6.3. Varying independent variablesWe conducted a series of systematic evaluations of ARMS in the above-mentioned six PDDL-STRIPS domains fromthe IPC 2002 competition. In these experiments, we vary the three independent parameters and measure the error rate,redundancy rate, CPU time and robustness through cross-validation. In the subsequent experiments, E is the errorrate, and R is the redundancy rate. CPU time is measured in seconds, and the values are the means of five-fold crossvalidation.132Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Fig. 9. Varying the number of plans (Error Rate).Fig. 10. Varying the number of plans (Redundancy Rate).6.3.1. Varying the probability thresholdThe probability threshold θ can be varied from 0% to 100%. When θ is high, fewer frequent action pairs remainfor building the clauses in the weighted SAT formula, and as a result the computation becomes more efficient at a costof potentially higher error and redundancy rates. In these experiments, the degree of partial information α is set to bezero (no observation of intermediate states in the middle of plans).The results of varying θ in a range from zero to 100% in are given in Tables 11–16, and an excerpt of the data in asmaller range (from zero to 0.1%) in the table is shown graphically in Figs. 1–4 for ease of understanding.We first focus on four of the six domains that show consistent performance in the entire probability threshold rangefrom zero to 100%. As can be seen from Figs. 1–4, as we increase the probability threshold to 0.1%, the error ratesof two of the domains, Depot and Satellite, generally increase with rather unstable behavior. In contrast, in the samerange, the error rates of Zenotravel and Driverlog domains are quite stable and low. The redundancy rates for all fourdomains generally decrease in this smaller range, but stays stable over the larger range (Tables 11–16). On the CPUtime and “Clause Number” charts, one can see that the efficiency of the system for all domains dramatically increases.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143133Fig. 11. Varying the number of plans (CPU Time).Fig. 12. Varying the number of plans (Clause Number).Over the 100% range, we can see from Tables 11–16 that, as the probability threshold θ increases, the number ofclauses decreases. At the same time, the CPU time decreases. This can be understood because when the probabilitythreshold increases, fewer actions and relations are encoded into clauses. Thus, the computational work load alsodecreases. Two domains do not produce any result for θ below 60%, although they can give high quality actionmodels when θ is above 60%. We can also see that when θ increases, the action models become slightly less accurate,which can also be seen from the increasing error rates. This is because the number of plan constraints decreases whenthe probability threshold θ increases, resulting in the ARMS system producing a simple action model when the numberof plan constraints are small. Thus, the MaxSatSolver system becomes under-constrained. Overall, we can see that theerror rate E and the redundancy rate R stay roughly at the same level when θ increases to 100% for all six domains.This indicates to us that the learned model is relatively stable while the efficiency can be improved greatly when weincrease the probability threshold.134Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Fig. 13. Learning in the Depot domain using MaxWalkSat by varying probability threshold θ (Error Rate).Fig. 14. Learning in the Depot domain using MaxWalkSat by varying probability threshold θ (Redundancy Rate).For two of the six domains, Rover and Freecell, learning becomes too computationally expensive to run on ourPC when the value of θ is less than 60%. Thus, we can only learn action models within our computational resourcesafter the θ = 60% threshold. This is because these two domains are more complex than the other four, as can be seenfrom the large number of relations and initial state relations in these domains (see Table 8). From Figs. 1–4, we canalso see that the Depot domain has the highest error rates. This is due to the fact that in the Depot domain, almostany two actions can be paired up as a frequent action pair. As a result, the clause numbers are very large, resultingin inaccurate solutions being found for the weighted MAX-SAT problem. This is confirmed by the “clause number”chart in Figs. 1–4, where the average number of clauses for the Depot domain is the highest. This tells us that ARMScan perform well when the number of potential clauses is small and when the number of frequent pairs is also small.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143135Fig. 15. Learning in the Depot domain using MaxWalkSat by varying probability threshold θ (CPU Time).Fig. 16. Learning in the Depot domain using MaxWalkSat by varying probability threshold θ (Clause Number).6.3.2. Varying the degree of partial informationAs mentioned in the beginning of the section, the degree of partial information is designed to test how the ARMSalgorithm might benefit from some partial information that can be occasionally observed in the middle of plans. Thisvariable is denoted by p which is a value that ranges from zero to 100%. When p ranges from zero to 100%, theamount of available state information increases. The observed relations can then be used to explain frequent pairs ofactions as additional clauses. These clauses that involve additional observations are given higher weights than otherrelations. Thus, as we increase p, we expect to see an increase in accuracy, as well as an increase in the number ofclauses.The experimental results are shown in Figs. 5–8. In all these experiments, we have fixed the probability thresholdθ at 10%. As we can see, when more intermediate state information is known among the plans, the number of clausesbecomes larger and the CPU time increases. However, knowing more information between the states makes the so-lution better, which can be seen from the error and redundancy rates. As we expected, the Freecell domain produces136Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Fig. 17. Learning in the Depot domain using MaxWalkSat by varying the degree of partial information p (Error Rate).Fig. 18. Learning in the Depot domain using MaxWalkSat by varying the degree of partial information p (Redundancy Rate).too many clauses for ARMS to handle due to its large number of relations, when we enforce the partial informationconstraints. Thus, no results are reported for this domain. We can conclude that when the number of relations aresmall, knowing more state information helps with accuracy and conciseness of the learned action model at a cost ofmore demanding computation.6.3.3. Varying the number of plansIn order to test whether the size of the training set affects the quality of the learned model, we have conductedexperiments that vary the number of plans in the training data set. In Figs. 9–12, we vary the number of plans in thetraining set from 32 to 160 and record the error and redundancy rates for the quality of the model. We also record thenumber of clauses and the CPU time. As we can see, as the training set increases its size, the error rate E and theredundancy rate R stay more or less at the same level. However, the CPU time and the number of clauses increase asQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143137Fig. 19. Learning in the Depot domain using MaxWalkSat by varying the degree of partial information p (CPU Time).Fig. 20. Learning in the Depot domain using MaxWalkSat by varying the degree of partial information p (Clause Number).expected. This shows that our system is quite stable in learning the action models even when the training set sizes aresmall.6.3.4. Using MaxWalkSat as the weighted MAX-SAT solverMaxWalkSat is another powerful solver for finding an approximate solution to weighted MAX-SAT problems[11,26]. It aims to minimize the weights associated with unsatisfied clauses, while local minima are avoided byrandomly flipping the assignment of a variable in an unsatisfied clause [26]. By applying a different MAX-SAT solver,we hope to establish the fact that our learning algorithm is general in that it is independent of the actual tool that weselect. As a local search algorithm, MaxWalkSat terminates when the sum of the weights of the un-satisfied clausesfalls below a predefined target cost value. We have conducted a series of experiments where we replaced MaxSatSolverby MaxWalkSat in solving a weighted MAX-SAT problem. We are able to show similar learning performance as inMaxSatSolver experiments.138Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Fig. 21. Learning in the Depot domain using MaxWalkSat by varying the Number of Plans (Error Rate).Fig. 22. Learning in the Depot domain using MaxWalkSat by varying the Number of Plans (Redundancy Rate).We illustrate the results of using MaxWalkSat solver in ARMS. Figs. 13–16 plot the error rate, redundancy rate,CPU time and average number of clauses in the SAT formulae for the Depot domain. Figs. 17–20 show the error rate,redundancy rate, CPU time and average number of clauses as a function of the amount of partial information knownin the intermediate states in a plan. Finally, Figs. 21–24 show the same metrics as a function of the number of plans inthe training data set from the Depot domain. Comparing these figures and Figs. 1–12, we can see that the MaxWalkSatand MaxSatSolver performs similarly in terms of both the quality of the solution and the efficiency.Finally, we compared the CPU times of MaxWalkSat and MaxSatSolver systems on the same planning problems,for both training and testing, for the Depot domain. The results are shown in Figs. 25–27. In this figure, the error barsare obtained from five fold cross validation experiments. As can be seen from the figures, the systems show similarperformance and trends in solving the Depot domain learning problems.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143139Fig. 23. Learning in the Depot domain using MaxWalkSat by varying the Number of Plans (CPU Time).Fig. 24. Learning in the Depot domain using MaxWalkSat by varying the Number of Plans (Clause Number).7. Conclusions and future workIn this paper, we have developed an algorithm for automatically learning action models from a set of plan exampleswhere the intermediate states can be unknown. With a domain description and a set of successful plan traces, we canlearn an action model that approximates the ideal models designed by human experts. We have shown empiricallythat it is feasible to learn these models in a reasonably efficient way using a weighted MAX-SAT solution even whenthe intermediate states are not observed, which removes a significant burden on the accumulation of the training data.Our learned action models can then be edited by experts before they are applied for practical planning.While we are able to learn reasonable action models from plan traces, there are a few limitations of the currentwork, which we plan to overcome in our future extensions.140Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143Fig. 25. A comparison of MaxWalkSat and MaxSat Solver (Probability Threshold).Fig. 26. A comparison of MaxWalkSat and MaxSat Solver (Threshold of Partial State Information).(1) Our algorithm starts from an empty action model. However, in the real world, it is often the case that thereis a partially completed existing action model to start with. It would be interesting to consider how to extendARMS to allow a possibly imperfect or incomplete action model to be part of the input. We conjecture that insuch situations, the existing action models can be used to bias the learning process, by simplifying the weightedMAX-SAT formulation, thus making the learning more effective.(2) We have assumed that ARMS is given a correct set of plan traces as training data. However, the real world is full ofnoise, which exists in the form of incomplete observed action sequences or even incorrect observations of actions.Furthermore, some plans may fail to achieve some goals. Thus, it would be interesting to consider how to extendARMS to handle noise in the training data.(3) Although we have evaluated the system using two new error measures, we still have to find a way to put thelearned models to test in plan generation. A major difficulty in doing so is the ability to evaluate the quality ofQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143141Fig. 27. A comparison of MaxWalkSat and MaxSat Solver (Number of Plans).the generated plans using the learned models. We believe this can be accomplished by interactively repairing themodels with the help of human experts.(4) The STRIPS language we considered in our paper is still too simple to model many real world situations. We wishto extend to the full PDDL language including quantifiers, time and resources. We believe that quantification canbe learned by considering further compression of the learned action model from a SAT representation. Anotherdirection is to allow actions to have duration and to consume resources.(5) We wish to explore the application of ARMS iteratively on sets of actions in a collection of plans in the orderof decreasing support measure, by following the planning with an abstraction approach. This allows the mostfrequent action sets to be explained first, thus causing fast convergence and producing superior action models.(6) As we have pointed out in Section 5.4, there is a strong connection between ARMS algorithm, the action-modellearning problem and the framework of Markov Logic Networks (MLN). We have pointed out how to model theaction-model learning problem using the MLN by mapping the various components, including the constraints andweights, so that a solution to MLN corresponds to a learned action model. In the future, we wish to explore thisconnection further, especially on the issue of how to allow more expressive types of action models to be learned.(7) Another possibility is to involve humans in the loop of the learning process. In the mix-initiative planning frame-work of Myers et al. [31], human users and computer systems work together in completing a planning task. Whilea system identifies possible candidates to be selected in completing a plan sketch, the human users make choicesamong the options. In our future work, we will consider how to involve human users in the learning process, sothat when too many potential options exist for explaining frequent sets of actions in the training plans, humanexperts can be biased about making the final selection in order to obtain a high quality action model.(8) In the related work section, we reviewed the CaMeL system [23], which acquires the method preconditions of ahierarchical task network (HTN) planning domain. We mentioned that in [23], plan traces as well as the deriva-tional trees for these traces are taken as input into the learning system, which then produces a set of preconditionsfor HTN schemata. It would be interesting to consider two possible directions of future works. First, it would beinteresting to include in the input of ARMS additional forms of domain theory such as the derivational traces intothe learning process. With this additional knowledge, we would expect that the weighted MAX-SAT solution canbe made more effective due to the additional bias in learning. A second direction is to extend ARMS to learningHTN method preconditions as well as the HTN schemata themselves. Such an extension and the comparison tothe CaMel system would be an interesting future work.(9) Finally, in the future we wish to evaluate best to determine the upper bound U of pre and post conditions foraction models. This parameter can be experimentally found by varying U from a small value to a large value in142Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143order to search for the lowest error and redundancy rates on test plans. The best such value can be determinedempirically. We wish to continue to evaluate this strategy in our future work.AcknowledgementWe wish to thank the Hong Kong RGC grant 621606 for supporting this research.References[1] R. Agrawal, R. Srikant, Fast algorithms for mining association rules, in: Proceedings of the 20th International Conference on Very Large DataBases (VLDB), Santiago de Chile, Chile, Morgan Kaufmann, 1994, pp. 487–499.[2] E. Amir, Learning partially observable deterministic action models, in: Proceedings of the International Joint Conference on Artificial Intelli-gence (IJCAI 2005), Edinburgh, Scotland, UK, August 2005, pp. 1433–1439.[3] M. Bain, C. Sammut, A framework for behavioural cloning, Machine Intelligence 15 (1996).[4] S. Benson, Inductive learning of reactive action models, in: Proceedings of the International Conference on Machine Learning (ICML 1995),Stanford University, Stanford, CA, 1995, pp. 47–54.[5] J. Blythe, J. Kim, S. Ramachandran, Y. Gil, An integrated environment for knowledge acquisition, in: Proceedings of the 2001 InternationalConference on Intelligent User Interfaces (IUI2001), Santa Fe, NM, 2001, pp. 13–20.[6] B. Borchers, J. Furman, http://infohost.nmt.edu/~borchers/maxsat.html.[7] B. Borchers, J. Furman, A two-phase exact algorithm for MAX-SAT and weighted MAX-SAT problems, Journal of Combinatorial Optimiza-tion 2 (4) (1999) 299–306.[8] M. Davis, H. Putnam, A computing procedure for quantification theory, Journal of The Association for Computing Machinery 7 (1960)201–215.[9] P. Domingos, S. Kok, H. Poon, M. Richardson, P. Singla, Unifying logical and statistical AI, in: Proceedings of the Twenty-First NationalConference on Artificial Intelligence (AAAI 2006), Boston, MA, July 2006.[10] K. Erol, J.A. Hendler, D.S. Nau, Htn planning: Complexity and expressivity, in: Proceedings of the National Conference on Artificial Intelli-gence (AAAI 1994), Seattle, WA, 1994, pp. 1123–1128.[11] H. Kautz, et al., http://www.cs.washington.edu/homes/kautz/walksat/.[12] W.H. Evans, J.C. Ballegeer, N.H. Duyet, ADL: An algorithmic design language for integrated circuit synthesis, in: Proceedings of the 21stDesign Automation Conference on Design Automation, 1984.[13] R.E. Fikes, N.J. Nilsson, Strips: A new approach to the application of theorem proving to problem solving, Artificial Intelligence 2 (1971)189–208.[14] M. Fox, D. Long, PDDL2.1: An extension to pddl for expressing temporal planning domains, Journal of Artificial Intelligence Research 20(2003) 61–124.[15] M.R. Garey, D.S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, W.H. Freeman and Company, 1979.[16] A. Garland, N. Lesh, Plan evaluation with incomplete action descriptions, in: Proceedings of the Eighteenth National Conference on AI(AAAI 2002), Edmonton, Alberta, Canada, 2002, pp. 461–467.[17] M. Ghallab, A. Howe, C. Knoblock, D. McDermott, A. Ram, M. Veloso, D. Weld, D. Wilkins, PDDL—the planning domain definitionlanguage, 1998.[18] Y. Gil, Learning by experimentation: Incremental refinement of incomplete planning domains, in: Eleventh Intl. Conf. on Machine Learning,1994, pp. 87–95.[19] M.X. Goemans, D.P. Williamson, Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite pro-gramming, Journal of the ACM 42 (1995) 1115–1145.[20] P.D. Grunwald, I.J. Myung, M.A. Pitt, Advances in Minimum Description Length Theory and Applications, MIT Press, Cambridge, MA,2005.[21] O. Ilghami, H. Munoz-Avila, D.S. Nau, D.W. Aha, Learning preconditions for planning from plan traces and HTN structure, Journal ofArtificial Intelligence Research 20 (2003) 379–404.[22] O. Ilghami, H. Munoz-Avila, D.S. Nau, D.W. Aha, Learning preconditions for planning from plan traces and htn structure, in: Proceedings ofthe International Conference on Machine Learning (ICML 2005), Bonn, Germany, 2005.[23] O. Ilghami, D.S. Nau, H. Munoz-Avila, Camel: Learning method preconditions for htn planning, in: Proceedings of the Sixth InternationalConference on AI Planning and Scheduling AIPS-02, Toulouse, France, 2002, pp. 168–178.[24] R.G. Jeroslow, J. Wang, Solving propositional satisfiability problems, Annals of Mathematics and AI 1 (1990) 167–187.[25] H. Kautz, B. Selman, Pushing the envelope: Planning, propositional logic, and stochastic search, in: Proceedings of the Thirteenth NationalConference on Artificial Intelligence (AAAI 1996), Portland, OR, 1996, pp. 1194–1201.[26] H. Kautz, B. Selman, Y. Jiang, A general stochastic approach to solving problems with hard and soft constraints, in: The Satisfiability Problem:Theory and Applications, 1997.[27] T. Lau, P. Domingos, D.S. Weld, Version space algebra and its application to programming by demonstration, in: Proc. 17th InternationalConf. on Machine Learning, Morgan Kaufmann, San Francisco, CA, 2000, pp. 527–534.[28] D.W. Loveland, Automated Theorem Proving: A Logical Basis, North-Holland, New York, 1978.[29] T. Leo McCluskey, D. Liu, R.M. Simpson, GIPO II: HTN planning in a tool-supported knowledge engineering environment, in: Proceedingsof the International Conference on Automated Planning and Scheduling (ICAPS 2003), Trento, Italy, 2003, pp. 92–101.Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143143[30] M.W. Moskewicz, C.F. Madigan, Y. Zhao, L. Zhang, S. Malik, Chaff: Engineering an efficient sat solver, in: Proceedings of the 38th DesignAutomation Conference (DAC), 2001.[31] K.L. Myers, P. Jarvis, W.M. Tyson, M. Wolverton, Mixed-initiative framework for robust plan sketching, in: Thirteenth International Confer-ence on Automated Planning and Scheduling (ICAPS-03), BC, Canada, AAAI Press, 2003, pp. 256–266.[32] D.S. Nau, T.-C. Au, O. Ilghami, U. Kuter, J.W. Murdock, D. Wu, F. Yaman, Shop2: An HTN planning system, Journal of Artificial IntelligenceResearch 20 (2003) 379–404.[33] D.S. Nau, T.-C. Au, O. Ilghami, U. Kuter, J.W. Murdock, D. Wu, F. Yaman, Applications of shop and shop2, IEEE Intelligent Systems 20 (2)(2005) 34–41.[34] T. Oates, P.R. Cohen, Searching for planning operators with context-dependent and probabilistic effects, in: Proceedings of the ThirteenthNational Conference on AI (AAAI 1996), Portland, OR, 1996, pp. 865–868.[35] M. Richardson, P. Domingos, Markov logic networks, Technical Report, 2004.[36] M. Richardson, P. Domingos, Markov logic networks, Machine Learning 62 (1–2) (July 2006) 107–136.[37] G. Sablon, D. Boulanger, Using the event calculus to integrate planning and learning in an intelligent autonomous agent, in: Current Trendsin AI Planning, IOS Press, 1994, pp. 254–265.[38] B. Selman, H. Kautz, An empirical study of greedy local search for satisfiability testing, in: Proceedings of the Eleventh National Conferenceon Artificial Intelligence (AAAI-93), Washington, DC, 1993, pp. 46–51.[39] B. Selman, H. Kautz, B. Cohen, Local search strategies for satisfiability testing, Cliques, Coloring, and Satisfiability: Second DIMACSImplementation Challenge 26 (October 11–13, 1993).[40] D. Shahaf, E. Amir, Learning partially observable action schemas, in: Proceedings of the Twenty-First National Conference on ArtificialIntelligence (AAAI 2006), Boston, MA, July 2006.[41] D. Shahaf, A. Chang, E. Amir, Learning partially observable action models: Efficient algorithms, in: Proceedings of the Twenty-First NationalConference on Artificial Intelligence (AAAI 2006), Boston, MA, July 2006.[42] W. Shen, Autonomous Learning from the Environment, Computer Science Press/W.H. Freeman and Company, 1994.[43] X. Wang, Learning by observation and practice: An incremental approach for planning operator acquisition, in: Proceedings of the TwelfthInternational Conference on Machine Learning (ICML 1995), 1995, pp. 549–557.[44] E. Winner, M. Veloso, Analyzing plans with conditional effects, in: Proceedings of the Sixth International Conference on AI Planning andScheduling (AIPS 2002), Toulouse, France, 2002.[45] Q. Yang, Formalizing planning knowledge for hierarchical planning, Computational Intelligence Journal 6 (2) (1990) 12–24.[46] Q. Yang, K. Wu, Y. Jiang, Learning action models from plan examples with incomplete knowledge, in: Proceedings of the Fifteenth Interna-tional Conference on Automated Planning and Scheduling (ICAPS 2005), Monterey, CA, 2005, pp. 241–250.[47] J. Yin, D. Shen, Q. Yang, Z.-N. Li, Activity recognition through goal-based segmentation, in: Proceedings of the Twentieth National Confer-ence on Artificial Intelligence (AAAI-05), Pittsburgh, PA, 2005, pp. 28–34.[48] J. Yin, Q. Yang, Integrating hidden Markov models and spectral analysis for sensory time series clustering, in: Proceedings of the Fifth IEEEInternational Conference on Data Mining, Houston, TX, 2005.[49] H. Zhang, SATO: An efficient propositional prover, in: Proceedings of the 14th International Conference on Automated Deduction (CADE1997), Townsville, North Queensland, Australia, 1997, pp. 272–275.