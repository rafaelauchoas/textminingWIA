Artificial Intelligence 171 (2007) 453–490www.elsevier.com/locate/artintPartially observable Markov decisionprocesses with imprecise parametersHideaki Itoh ∗, Kiyohiko NakamuraDepartment of Computational Intelligence and Systems Science, Interdisciplinary Graduate School of Science and Engineering,Tokyo Institute of Technology, 4259-G3-46 Nagatsuta-cho, Midori-ku, Yokohama, Kanagawa 226-8502, JapanReceived 11 May 2005; received in revised form 7 March 2007; accepted 16 March 2007Available online 24 March 2007AbstractThis study extends the framework of partially observable Markov decision processes (POMDPs) to allow their parameters,i.e., the probability values in the state transition functions and the observation functions, to be imprecisely specified. It is shownthat this extension can reduce the computational costs associated with the solution of these problems. First, the new framework,POMDPs with imprecise parameters (POMDPIPs), is formulated. We consider (1) the interval case, in which each parameter isimprecisely specified by an interval that indicates possible values of the parameter, and (2) the point-set case, in which each prob-ability distribution is imprecisely specified by a set of possible distributions. Second, a new optimality criterion for POMDPIPs isintroduced. As in POMDPs, the criterion is to regard a policy, i.e., an action-selection rule, as optimal if it maximizes the expectedtotal reward. The expected total reward, however, cannot be calculated precisely in POMDPIPs, because of the parameter impre-cision. Instead, we estimate the total reward by adopting arbitrary second-order beliefs, i.e., beliefs in the imprecisely specifiedstate transition functions and observation functions. Although there are many possible choices for these second-order beliefs, weregard a policy as optimal as long as there is at least one of such choices with which the policy maximizes the total reward. Thusthere can be multiple optimal policies for a POMDPIP. We regard these policies as equally optimal, and aim at obtaining one ofthem. By appropriately choosing which second-order beliefs to use in estimating the total reward, computational costs incurred inobtaining such an optimal policy can be reduced significantly. We provide an exact solution algorithm for POMDPIPs that doesthis efficiently. Third, the performance of such an optimal policy, as well as the computational complexity of the algorithm, areanalyzed theoretically. Last, empirical studies show that our algorithm quickly obtains satisfactory policies to many POMDPIPs.© 2007 Elsevier B.V. All rights reserved.PACS: 07.05.Mh; 02.50.LeKeywords: POMDP; Second-order beliefs; Parameter set; Probability interval* Corresponding author.E-mail addresses: hideaki@dis.titech.ac.jp (H. Itoh), nakamura@dis.titech.ac.jp (K. Nakamura).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.03.004454H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–4901. IntroductionThe theory of partially observable Markov decision processes (POMDPs) is normative for sequential decisionmaking under uncertainty [2,15,35,51]. It provides a general framework for designing intelligent agents [8,30], andseveral real-world applications have been reported (e.g., [29,38]). Let us consider a toy example called the tigerproblem:Imagine an agent standing in front of two closed doors. Behind one of the doors is a tiger and behind the other is alarge reward. If the agent opens the door with the tiger, then a large penalty is received (presumably in the form ofsome amount of bodily injury). Instead of opening one of the two doors, the agent can listen, in order to gain someinformation about the location of the tiger. Unfortunately, listening is not free; in addition, it is also not entirelyaccurate. There is a chance that the agent will hear a tiger behind the left-hand door when the tiger is really behindthe right-hand door, and vice versa [30].What is the best action-selection rule for this agent? By a standard POMDP, this problem can be modeled asfollows: The possible states of the environment are sl (meaning that the tiger is behind the left-hand door) and sr(behind the right-hand door). Assume that the agent’s initial belief in the states is that sl and sr are equally probable(i.e., Pr(sl) = Pr(sr ) = 0.5). The agent can choose its action among LEFT (open the left door), RIGHT (open the rightdoor), and LISTEN (listen to the tiger). The action LEFT results in +10 reward when the state is sr , but −100 when itis sl, while these rewards are reversed for the action RIGHT. The action LISTEN costs as much as −1 reward, but theagent obtains a noisy observation TL (meaning that the tiger is likely to be behind the left-hand door) or TR (the tigeris likely to be behind the right-hand door). If the state is sl, TL is observed with probability 0.85 and TR is observedwith probability 0.15 (i.e., Pr(TL|sl) = 0.85 and Pr(TR|sl) = 0.15), while we similarly have Pr(TR|sr ) = 0.85 andPr(TL|sr ) = 0.15.An action-selection rule for the agent is called a policy. A policy is called optimal when it maximizes the expectedtotal reward. In this example, the optimal policy is (1) choose the action LISTEN several times until the agent’s beliefthat the tiger is on the right (or left) side becomes sufficiently strong and then (2) choose the action LEFT (or RIGHT)accordingly. POMDP theory tells us how the optimal policy can be derived.POMDPs, however, cannot be used when their parameters (e.g., Pr(TL|sl) = 0.85) are not specified precisely. Theparameters remain imprecise for various reasons [5,13,32] including limited data, insufficient inference time, dis-agreement among experts [34,49], and model abstraction [12,20,23]. Here we mention four examples. First, supposethat Pr(TL|sl) is estimated by an experiment that examines the frequency with which TL is observed when the stateis sl. Such an estimate is subject to a statistical error. For this case, intervals (e.g., 95% confidence intervals) maybe used to express the uncertainty. Second, suppose that a human expert can be consulted to determine the value ofPr(TL|sl). The expert would determine the value by his or her own subjective belief; he or she might say that Pr(TL|sl)would be equal to 0.85. However, he or she might find it hard to explain why Pr(TL|sl) should be precisely 0.85 andnot 0.849 for example. Thus in this situation intervals can be used to express the expert’s belief more faithfully. Third,suppose that there are multiple experts consulted. Even if each expert could specify a precise value for each parameter,the values might differ from each other. In this case, a set of distributions may be adopted to express the disagreeduncertainty. For example, we have (Pr(TL|sl), Pr(TR|sl)) ∈ {(0.84, 0.16), (0.85, 0.15)} if one expert specified the dis-tribution (Pr(TL|sl), Pr(TR|sl)) as (0.84, 0.16), while another expert specified it as (0.85, 0.15). Last, suppose thatthe tiger problem is an abstracted version of a more complex problem. For example, the probability distribution ofobserving TL or TR, given sl, might actually also depend on the temperature of the sonic sensor. Suppose that if thetemperature is high (denoted by thigh), the distribution is (Pr(TL|sl, thigh), Pr(TR|sl, thigh)) = (0.84, 0.16). Similarly,if the temperature is low, the distribution is (Pr(TL|sl, tlow), Pr(TR|sl, tlow)) = (0.85, 0.15). The agent, however, maywant to neglect such a small dependence on the temperature. In this case, again a set of distributions may be adoptedto express the abstracted uncertainty as (Pr(TL|sl), Pr(TR|sl)) ∈ {(0.84, 0.16), (0.85, 0.15)}.Motivated by these examples, in this paper, we introduce POMDPs with imprecise parameters (POMDPIPs). Weconsider two cases. One is the interval case, in which each parameter is imprecisely specified by an interval, e.g.,Pr(TL|sl) ∈ [0.84, 0.86]. The other is the point-set case, in which each distribution is specified by a set of distributions,e.g., (Pr(TL|sl), Pr(TR|sl)) ∈ {(0.84, 0.16), (0.85, 0.15)}. In this paper, we will consider the parameter imprecision inthe state transition functions (i.e., the probability functions that model how a state is changed by each action) and theH. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490455observation functions (i.e., the probability functions that model which observation is obtained). Other imprecisions,such as the imprecision in the reward function, remain to be considered in the future.Another motivation for the introduction of POMDPIPs arises from the fact that the POMDPs are computationallyexpensive to solve [36,42]. By solving a POMDP, we mean obtaining its optimal policy. Although several algorithmsthat can solve POMDPs in finite time [11,24,51,57,58] have been developed, within a given non-prohibitive time pe-riod, only relatively small-sized POMDPs can be solved by these algorithms. This high computational cost can be dueto the fact that the algorithms seek the optimal policy that strictly maximizes the expected reward. In many problems,however, such a strict optimization is meaningless, since the expected reward cannot be precisely evaluated becauseof the parameter imprecision. For such problems, it may often be sufficient to maximize the expected reward that isroughly estimated by using the imprecise parameters. Such rough optimization may require a lower computationalcost.Thus motivated, in this paper, we will formulate an optimality criterion for POMDPIPs in the following manner.We will begin by considering a hypothetical situation in which strict optimization can be performed for POMD-PIPs. To perform strict optimization, we need more information than POMDPIPs. Let us assume hypotheticallythat the agent can specify correct second-order beliefs (e.g., [19,41]), i.e., the beliefs in the models, where wedefine a model as a pair of the state transition function and the observation function. For instance, take the ex-ample of the abstracted uncertainty above, in which the probability distribution of observing TL or TR was either(Pr(TL|sl, thigh), Pr(TR|sl, thigh)) = (0.84, 0.16) or (Pr(TL|sl, tlow), Pr(TR|sl, tlow)) = (0.85, 0.15), depending on thetemperature of the sonic sensor. Suppose, for simplicity, that the other probability distributions in the model (i.e.,the state transition function and the observation function) are specified precisely (see Section 3 for the general case).Thus, we have two models, one for the high temperature and the other for the low temperature, which we denote bymhigh and mlow, respectively. Suppose hypothetically that the agent has performed a detailed experiment and foundthat the temperature is high or low with the same probability 0.5. Then its second-order belief is that mhigh and mloware equally probable. If such additional beliefs are given, the total reward can be defined exactly, and hence strictoptimization can be performed.Then we will introduce a relaxed optimality criterion. Recall that the second-order beliefs are not given in POMD-PIPs. None of the second-order beliefs are considered to be less reliable than the others. Thus, we adopt arbitrarysecond-order beliefs to estimate the total reward. We consider a policy optimal as long as it maximizes the estimatedtotal reward. We refer to the policies that satisfy this optimality criterion as “quasi-optimal” policies. By “quasi” wemean that the policies are optimized by using the second-order beliefs that are not necessarily correct.There are two possible approaches for adopting arbitrarily-selected second-order beliefs instead of the correct (butunknown) beliefs. When we estimate the total reward, we use a second-order belief for multiple purposes, i.e., not onlyto estimate the reward obtained immediately after each action but also to estimate the future rewards after the actionand the subsequent observation. One approach is to adopt a single arbitrarily-selected second-order belief instead ofthe correct belief, and use it for all of these purposes. The other approach is to use multiple arbitrarily-selected second-order beliefs instead of the correct belief, and use different beliefs for different purposes. Although further research isnecessary for detailed comparison, we will argue in Section 3.3 that the latter is at least not always disadvantageous interms of the performance of the optimal policy obtained and that it is advantageous in terms of computational costs.Thus, we will choose the latter approach in this study.Next, we will provide an exact algorithm to obtain the quasi-optimal policies. The algorithm exploits the factthat we have allowed the second-order beliefs to be adopted arbitrarily. To avoid confusion with the second-orderbeliefs, let us refer to the beliefs in the states as the first-order beliefs. A first-order belief is said to be reachablewhen it becomes the agent’s first-order belief after some actions and observations. In POMDPs, it is usual that a largenumber of first-order beliefs are reachable, and this makes it hard for us to calculate the optimal policy. Our algorithmreduces the number of reachable first-order beliefs by picking the second-order beliefs with which same first-orderbeliefs are reached repeatedly. We will show that such second-order beliefs can be picked efficiently by solving linearprogramming problems.We will also provide a theoretical bound on the amount of reward loss that can occur by using a quasi-optimal pol-icy, when compared with the optimal policy in the hypothetical situation in which the correct second-order beliefsare given and the strict optimization is conducted. Furthermore, we study empirically the conditions whereby thequasi-optimal policies have satisfactory performance and are easy to obtain by our algorithm.456H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490Our optimality criterion is closely related to E-admissibility [33,34]. If we take the former approach above, i.e.,adopt a single arbitrarily-selected belief instead of the correct but unknown belief, our criterion is exactly the same asE-admissibility because the quasi-optimal policy is optimal with regard to the adopted single belief. However, sincewe select the latter approach, i.e., adopt multiple arbitrarily-selected beliefs, the quasi-optimal policy is not alwaysoptimal with regard to a single belief. Note that we do not argue that our criterion is always better than E-admissibility.Detailed comparisons await future research.In POMDP literature, Cozman et al. [14] has already considered (using the E-admissibility criterion) parameterimprecision with the motivation of reducing computational costs. However, their study was limited to the case wherethe agent’s actions do not affect the state of the environment, and the observation probability functions were limitedto Gaussian distributions. The present study considers a more general case.There are several other studies of decision-making under parameter imprecision with various definitions of opti-mality [21,34,50,54]. For (fully observable) Markov decision processes (MDPs) with parameter imprecision, severalalgorithms have been provided to obtain max-min policies [20,48,56], max-max policies [20,48], all E-admissible poli-cies [55], and maximal policies [27]. For influence diagrams (IDs) with parameter imprecision, admissible policieshave been studied [10,17,18]. The ideas in these studies may, in principle, be applied to POMDPs. However, theircomputational complexity is at least that of solving the corresponding problem whose parameters are precise. Thusnone of these approaches offers a means of reducing computational complexity when applied to POMDPs.Other studies have been devoted to solving large-sized POMDPs. One approach is to develop approximate algo-rithms [28], e.g., grid-based algorithms [7,28,59], α-vector-based algorithms [44,46,47,52,53], and policy-gradientmethods [1]. Another approach is to exploit the structure of each POMDP problem: e.g., factored state representation[9,25] and hierarchies [26]. Their combinations, e.g., the approximate algorithms for factored state representation [16,37], have also been studied. However, as yet no study has explored the possibility of exploiting parameter imprecisionto reduce computational costs.This paper is organized as follows; Section 2 reviews POMDPs and their two transformations (history-state MDPsand belief-state MDPs), as a background for the following sections. Section 3 formulates POMDPIPs and their quasi-optimal policies. In Section 4 we provide an algorithm to obtain the quasi-optimal policies. The performances of thequasi-optimal policies and the provided algorithm are analyzed theoretically in Section 5 and empirically in Section 6.Section 7 concludes this study.2. POMDPsIn this section, we review POMDPs and their two transformations (see, e.g., [6,28] for introductory explanations).2.1. DefinitionWe assume discrete time steps. We define a POMDP by a tuple (S, A, Θ, {Th | h ∈ H }, {Oh | h ∈ H }, R, p0) where• S is a finite set of the states.• A is a finite set of actions.• Θ is a finite set of observations.• {Th | h ∈ H } is a set of state transition functions. Th : S × A × S → [0, 1] is the state transition function for agiven h ∈ H , where H is a set of histories that is defined as follows. Let st ∈ S, at ∈ A, and ot ∈ Θ denotethe state, action, and observation at time t (= 0, 1, 2, . . .), respectively. Let ht := (a0, o1, a1, o2, . . . , at−1, ot )denote the history up to time t.1 We say that the length of such a history is t. We further introduce Ht :={(a0, o1, a1, o2, . . . , at−1, ot ) | a0, a1, . . . , at−1 ∈ A, o1, o2, . . . , ot ∈ Θ}, which is the set of all the possible his-tories whose length is t. Finally, let H :=t=0 Ht be the set of all the histories of any length. For everyh ∈ H, s, s(cid:6) ∈ S and a ∈ A, let Th(s, a, s(cid:6)) be the probability that state s(cid:6) is reached from state s on action aafter history h. Note that the state transition function is history-dependent, i.e., Th1 and Th2 may be differents(cid:6)∈S Th(s, a, s(cid:6)) = 1 for all h ∈ H, s ∈ S and a ∈ A;functions if h1 and h2 are different histories. It holds that(cid:2)∞(cid:3)1 For notational simplicity, we assume that the agent does not observe o0 at time t = 0. The modification of this assumption to include o0 isstraightforward.H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490457Fig. 1. An influence diagram of a single time step of a POMDP.• {Oh | h ∈ H } is a set of observation functions. Oh : S × A × Θ → [0, 1] is the observation function for a givenh ∈ H . Note that these functions are also history-dependent. Oh(s(cid:6), a, o) is the probability that observation o iso∈Θ Oh(s(cid:6), a, o) = 1 for allgiven to the agent when state s(cid:6) is reached on action a after history h. It holds thath ∈ H, s(cid:6) ∈ S and a ∈ A;• R : S × A × S → R is the reward function, where R(s, a, s(cid:6)) denotes the reward that the agent gains when state s(cid:3)changes into s(cid:6) on action a, for all s, s(cid:6) ∈ S and a ∈ A;• p0 : S → [0, 1] is the prior probability function of the initial state s ∈ S. It holds that(cid:3)s∈S p0(s) = 1.The process proceeds as follows; at time t = 0, the process has an initial state s0 ∈ S with probability p0(s0),and the history is empty (i.e., h0 = ∅). At time t (= 0, 1, 2, . . .), the state and the history is denoted by st andht = (a0, o1, a1, o2, . . . , at−1, ot ) ∈ Ht , respectively. The agent selects an action at ∈ A by which it changes thecurrent state st into the subsequent state st+1 with probability Tht (st , at , st+1). The agent then observes ot+1 ∈ Θwith probability Oht (st+1, at , ot+1), and gains reward rt := R(st , at , st+1), whereupon the history is changed intoht+1 = (a0, o1, a1, o2, . . . , at−1, ot , at , ot+1) ∈ Ht+1. The relations between these variables within a single time stepare shown in Fig. 1.For the agent to select its action at at time t, we assume that the POMDP tuple (S, A, Θ, {Th | h ∈ H }, {Oh | h ∈H }, R, p0) and the past history ht = (a0, o1, a1, o2, . . . , at−1, ot ) are available. The action selection rule of the agent,which is a mapping from the available information to an action, is called a policy.Solving a POMDP is to find the optimal policy that maximizes an objective function. We adopt the ‘infinite-horizondiscounted sum of the expected rewards’ criterion, in which the objective function is defined asγ t−1rt,(1)(cid:6)(cid:4)∞(cid:5)Et=1where 0 (cid:2) γ < 1 is a discount factor, and the expectation E{·} is taken over all the possible process paths.22.2. History-state MDPSolving a POMDP problem is equivalent to solving a (fully observable) Markov decision process (MDP), which iscalled the history-state MDP.In history-state MDPs, we take an alternative view of the POMDP process (Fig. 2). At time t = 0, it starts with theinitial history that is the empty history ∅. At time t (= 0, 1, 2, . . .), let h be the current history. The agent selects anaction a ∈ A, observes o ∈ Θ, and gains some reward. The history h is then changed into a new history.2 The algorithm that we will provide later can be modified for the finite-horizon cases, in which the objective function is E{termination time ˜T .(cid:3) ˜Tt=1 rt } for a given458H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490Fig. 2. History tree for a POMDP with two actions (a1 and a2) and two observations (o1 and o2). Each open circle corresponds to a history, whichis treated as a state in the history-state MDP. We associate each history, h, with the corresponding belief bh. The subscripts of b indicate the historywith which the belief is associated; e.g., b11 is the belief associated with the history h = (a1, o1).Taking this view, we can consider the Bellman equation to derive the optimal policy. We require some furtherfunctions for this purpose. First, let bh, which is termed the belief, be a probability function where bh(s) is theprobability that the current state is s, given that the past history is h. It can be calculated recursively as follow; Tobegin with, let the initial belief for the empty history h0 = ∅ beb∅ = p0.(2)Note that ‘=’ here means that b∅ and p0 are identical functions, i.e., ‘b∅(s) = p0(s) for all s ∈ S.’ Next, for everyh ∈ H , a ∈ A and o ∈ Θ, let ‘h; (cid:8)a, o(cid:9)’ denote the history in which a and o have followed h. The belief bh;(cid:8)a,o(cid:9) iscalculated from bh by Bayes’ rule asOh(s(cid:6), a, o)s(cid:6)(cid:6)∈S Oh(s(cid:6)(cid:6), a, o)s∈S Th(s, a, s(cid:6))bh(s)(cid:3)s∈S Th(s, a, s(cid:6)(cid:6))bh(s)bh;(cid:8)a,o(cid:9)(s) =(3)(cid:3)(cid:3)(cid:6)for every s(cid:6) ∈ S.Second, let P (o|h, a) be the probability that o is observed, given that action a is taken after history h. It can beLast, let ρ(h, a) be the average reward that the agent will gain by taking action a at history h. We havecalculated asP (o|h, a) :=(cid:5)s(cid:6)∈S(cid:6)Oh(s, a, o)(cid:5)s∈STh(s, a, s(cid:6))bh(s).ρ(h, a) :=R(s, a, s(cid:6))Th(s, a, s(cid:6))bh(s).(cid:5)(cid:5)s∈Ss(cid:6)∈S(cid:7)Now, by the principle of optimality [4], we can write the Bellman equation as∗V(h) = maxa∈Aρ(h, a) + γ(cid:5)o∈ΘP (o|h, a)V∗(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9)for every h ∈ H , where V ∗ : H → R is called the optimal value function. The optimal policy is defined as the mappingμ∗ : H → A that satisfies(cid:7)∗μ(h) = arg maxa∈Aρ(h, a) + γfor every h ∈ H .P (o|h, a)V∗(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9)(cid:5)o∈Θ(4)(5)(6)(7)H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–4904592.3. Belief-state MDPSolving the history-state MDP in the previous section is equivalent to solving another MDP problem called thebelief-state MDP [2] if we restrict the T ’s and O’s as follows.First, we restrict T ’s, so that they satisfy Th1= bh2 holds for any h1 and h2 ∈ H . Note again= Th2 whenever bh1that the ‘=’ symbols here denote equality of functions. In other words, we impose the condition that Th1 (s, a, s(cid:6)) =Th2 (s, a, s(cid:6)) for all s, s(cid:6) ∈ S and a ∈ A, whenever the beliefs after h1 and h2 are the same (i.e., bh1 (s) = bh2 (s) for all= bh2 we treat Th1 and Th2 as identical. Consequently, we re-define the set ofs ∈ S). With this restriction, whenever bh1T ’s as {Tb | b ∈ B} instead of {Th | h ∈ H }, where B is the set of all the possible beliefs, i.e., B := {b | b = bh, h ∈ H }.= bh2 holds for any h1 and h2 ∈ H .Second, we similarly restrict O’s, so that they satisfy Oh1Consequently, we re-define O’s from {Oh | h ∈ H } to {Ob | b ∈ B}.= Oh2 whenever bh1With the T ’s and O’s restricted, the belief b ∈ B becomes a sufficient statistic. That is, after any history, the belief b(and also the tuple (S, A, Θ, {Tb | b ∈ B}, {Ob | b ∈ B}, R, p0)) summarizes all the information available at thattime for the agent to predict what will happen (together with its probability) in the future. Thus, the Bellman equation(Eq. (6)) can be re-written as(cid:7)∗V(b) = maxa∈Aρ(b, a) + γ(cid:5)o∈ΘP (o|b, a)V∗(cid:10)(cid:9)(cid:8)τ (b, a, o)for every b ∈ B, where ρ(b, a) and P (o|b, a) are defined (re-defined from Eqs. (5) and (4)) as(cid:5)(cid:5)(cid:5)(cid:5)ρ(b, a) :=R(s, a, s(cid:6))Tb(s, a, s(cid:6))b(s),P (o|b, a) :=(cid:6)Ob(s, a, o)Tb(s, a, s(cid:6))b(s),s∈Ss(cid:6)∈Ss(cid:6)∈Ss∈Sand τ , which we call the belief-update function, is defined ass∈S Tb(s, a, s(cid:6))b(s)(cid:3)(cid:3)(cid:3)(cid:6)τ (b, a, o)(s) :=Ob(s(cid:6), a, o)s(cid:6)(cid:6)∈S Ob(s(cid:6)(cid:6), a, o)s∈S Tb(s, a, s(cid:6)(cid:6))b(s)The optimal policy can be re-written as the mapping μ∗ : B → A that satisfies(cid:5)o∈ΘP (o|b, a)V∗(cid:10)(cid:9)(cid:8)τ (b, a, o)for every s(cid:6) ∈ S.(cid:7)∗μ(b) = arg maxa∈Aρ(b, a) + γfor every b ∈ B.3. POMDPIPs3.1. Definition(8)(9)(10)Here, we describe the formulation of POMDPs with imprecise parameters (POMDPIPs). In POMDPIPs, theprocess proceeds in exactly the same way as POMDP in Section 2.1, except that the agent knows Th and Oh ateach history h ∈ H only imprecisely. A POMDPIP is defined by a tuple (S, A, Θ, T M , OM , R, p0), where• S, A, Θ, R and p0 are the same as those defined in POMDPs. Furthermore, let H be the set of all the histories.3• T M is what we call the model-set function for the state transition functions {Th|h ∈ H }. We consider two cases(Fig. 3).4 One is what we call the interval case, in which T M : S × A × S → I indicates the range of the possiblevalues of Th(s, a, s(cid:6)) for each s, s(cid:6) ∈ S, a ∈ A, and h ∈ H , where I is the set of all the intervals within [0, 1]. Forexample, suppose that we have T M (s, a, s(cid:6)) = [0.8, 0.9] for certain s, a, and s(cid:6). This means that, for each h ∈ H ,we have Th(s, a, s(cid:6)) ∈ [0.8, 0.9] for the s, a, and s(cid:6). The other case we consider is what we call the point-set3 Here we consider the imprecision only on Th and Oh. In case p0 is imprecise, an equivalent POMDPIP in which only Th and Oh are imprecisecan be constructed by introducing an auxiliary state. Further research is required to handle imprecision on R.4 For simplicity we consider these two cases separately. It is straightforward to consider a combination of both of these cases.460H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490Fig. 3. Example of Th(s, a, ·) in POMDPIPs. The figure shows a probability simplex Δ|S| for |S| = 3, in which Th(s, a, ·) = (p1, p2, p3), forcertain h ∈ H, s ∈ S, and a ∈ A, is located. (A) the interval case. For each s(cid:6) ∈ S, Th(s, a, s(cid:6)) should be within the interval specified by T M (s, a, s(cid:6)).Consequently, Th(s, a, ·) should lie within a convex area (striped in the figure). (B) the point-set case. Th(s, a, ·) should be one of the probabilityfunctions that T M (s, a) specifies (the three dots in the figure).n(cid:3)i=1 pi = 1}. Next, let Δ∗case, in which T M : S × A → Δ∗|S| indicates the probability functions that Th(s, a, ·) can possibly be identical to,where Δ∗|S| is defined as follows. First, for any n (cid:3) 0, let Δn be the n-dimensional probability simplex, i.e., Δn ={(p1, p2, . . . , pn) | pi (cid:3) 0 for all i,n be the set of all the finite sets of different pointsin Δn. Thus, for each s ∈ S and a ∈ A, T M (s, a) is a finite set of probability functions. For example, supposethat there are three states, i.e., S = {s1, s2, s3}, and that T M (s, a) = {(0.8, 0.1, 0.1), (0.85, 0.1, 0.05)} for a certains and a. This means that, for each h ∈ H , we have Th(s, a, ·) = (0.8, 0.1, 0.1) or Th(s, a, ·) = (0.85, 0.1, 0.05),where by the relation Th(s, a, ·) = (p1, p2, . . .) we mean that Th(s, a, s1) = p1, Th(s, a, s2) = p2, and so on.• OM is the model-set function for the observation functions. OM is defined in the same way as T M , that is, forthe interval case, OM : S × A × Θ → I indicates the range of possible values of each parameter. Thus, we haveOh(s(cid:6), a, o) ∈ OM (s(cid:6), a, o) for every s(cid:6) ∈ S, a ∈ A, o ∈ Θ, and h ∈ H . For the point-set case, OM : S × A → Δ∗|Θ|indicates the possible probability functions. Thus we have Oh(s(cid:6), a, ·) ∈ OM (s(cid:6), a) for each s(cid:6) ∈ S, a ∈ A, andh ∈ H .5The information available to the agent for the selection of its action at at time t is the tuple (S, A, Θ, T M , OM , R,p0) and the past history ht .Let us define some basic notions for later use. We call a pair of state-transition function Th and observation functionOh for each h ∈ H a model. Further, let M0 denote the set of all the possible models that is defined without regard tothe model-set functions. That is, we define M0 as(cid:7)M0 :=(T , O) | T : S × A × S → [0, 1],(cid:5)T (s, a, s(cid:6)) = 1 for all s ∈ S and a ∈ A,O : S × A × Θ → [0, 1],s(cid:6)∈S(cid:5)(cid:6)O(s, a, o) = 1 for all s(cid:10)(cid:6) ∈ S and a ∈ A.o∈ΘFinally, let M denote the set of all the possible models that is defined with regard to the model-set functions. For theinterval case,(cid:11)M :=(T , O) | (T , O) ∈ M0, T (s, a, s(cid:6)) ∈ T M (s, a, s(cid:6)) for all s, s(cid:6)O(s, a, o) ∈ OM (s(cid:6), a, o) for all s(cid:6) ∈ S, a ∈ A, and o ∈ Θ(cid:6) ∈ S, and a ∈ A,(cid:12),5 Note that Th and Oh are assumed to be history-dependent. This assumption should be natural for many problems, where the values of theparameters in Th and Oh may fluctuate due to unknown or neglected dynamics (e.g., the dynamics of the sonic sensor’s temperature in Section 1).H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490461i.e., M is the set of all models such that every parameter in its state transition function T and observation function Ois within the interval specified by the corresponding model-set function T M or OM .For the point-set case,(cid:11)M :=(T , O) | (T , O) ∈ M0, T (s, a, ·) ∈ T M (s, a) for all s ∈ S and a ∈ A,, a) for all s(cid:12)(cid:6) ∈ S and a ∈ A,, a, ·) ∈ OM (sO(s(cid:6)(cid:6)i.e., M is the set of all models such that every probability distribution in its state transition function T and observationfunction O is identical to one of the possible distributions specified by the corresponding model-set function T Mor OM .3.2. A “truly” optimal policyBefore we formulate the optimality criterion for POMDPIPs in the next section, let us consider a hypotheticalsituation where the optimal policies can be defined in a normative way as in POMDPs. This provides a basis for us toformulate a relaxed optimality criterion for POMDPIPs in the next section.In POMDPIPs, we have assumed that the agent knows only that, for each h ∈ H , the model mh(:= (Th, Oh)) isa member of M. One way to deal with this uncertainty about the model is to use the second-order belief, i.e., thebelief in the models. Let us suppose hypothetically that the agent had more information (in any form) to specify itssecond-order belief by a probability density function bMh (mh) be the probability that,given a history h, the model mh will govern the process immediately after h.h : M0 → [0, 1], where we let bMFor instance, let us again consider the example of the abstracted uncertainty in Section 1. Suppose that the modelmh = (Th, Oh) depends on the temperature of the sonic sensor, and that it is mhigh and mlow for high and low tem-peratures, respectively. If the agent performs a detailed experiment and finds that the temperature is high or low withh (mh) = 1/2(δ(mh − mhigh) + δ(mh − mlow)),the same probability, then the agent sets the second-order belief as bMwhere δ is Dirac’s delta function.The second-order belief bMh should be naturally assumed to satisfybMh (mh) = 0 for mh /∈ M,and(cid:13)mh∈MbMh (mh) dmh = 1(11)(12)for every h ∈ H .6 We call Eqs. (11) and (12) the permissibility condition for any second-order belief. That is, we saythat a function f : M0 → [0, 1] satisfies the permissibility condition, if and only if it satisfies f (m) = 0 for m /∈ Mand(cid:14)m∈M f (m) dm = 1.Now we can consider a modified version of POMDPs, in which the process proceeds in exactly the same wayas the original POMDPs in Section 2.1, except that the model mh for each h ∈ H is determined stochastically withprobability bMh (mh). We refer to this modified POMDP with given second-order beliefs as a hypothetical POMDP.In hypothetical POMDPs, there are no imprecise parameters. Thus we can define the optimal policy as that whichmaximizes the discounted sum of the expected rewards (Eq. (1)). We call these optimal policies the truly optimalpolicies.For later use, let us derive Bellman equations that the truly optimal policy satisfies. Let us consider an equivalenthistory-state MDP as in Section 2.2 (Fig. 2).First, the belief in the states, bh : S → [0, 1] for each h ∈ H , is calculated recursively as follows. To avoid confusion,we refer to this belief as the first-order belief. Let the initial first-order belief beb∅ = p0.(13)Next, for every h ∈ H , a ∈ A and o ∈ Θ, the belief bh;(cid:8)a,o(cid:9) is calculated from bh by Bayes’ rule as6 We implicitly assume that the integral in Eq. (12) and the others throughout this paper exist.Then, by the principle of optimality [4], the truly optimal policy can be defined as the mapping μ∗ : H → A that462H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490(cid:14)mh=(Th,Oh)∈M Oh(s(cid:6), a, o)(cid:3)(cid:3)bh;(cid:8)a,o(cid:9)(s(cid:6)) =(cid:9)(sfor all s(cid:6) ∈ S, where we re-defined the belief-update function τ of Eq. (9).=: τs(cid:6)(cid:6)∈S Oh(s(cid:6)(cid:6), a, o)(cid:6)),(cid:14)mh=(Th,Oh)∈M(cid:8)bh, a, o, bMhs∈S Th(s, a, s(cid:6))bh(s)bM(cid:3)h (mh) dmhs∈S Th(s, a, s(cid:6)(cid:6))bh(s)bMh (mh) dmhsatisfies the Bellman equations (exactly the same as Eqs. (6) and (7))(cid:5)(cid:7)∗V(h) = maxa∈Aρ(h, a) + γP (o|h, a)V(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9),∗∗μ(h) = arg maxa∈Aρ(h, a) + γo∈Θ(cid:5)o∈ΘP (o|h, a)V(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9),∗(cid:7)(cid:13)ρ(h, a) :=mh=(Th,Oh)∈M(cid:13)P (o|h, a) :=mh=(Th,Oh)∈M(cid:5)(cid:5)s∈Ss(cid:6)∈SR(s, a, s(cid:6))Th(s, a, s(cid:6))bh(s)bMh (mh) dmh,(cid:5)s(cid:6)∈S(cid:6)Oh(s, a, o)(cid:5)s∈STh(s, a, s(cid:6))bh(s)bMh (mh) dmh.for each h ∈ H , where we re-define ρ(h, a) and P (o|h, a) (from Eqs. (5) and (4), respectively) as(14)(15)(16)(17)(18)Below are some notes regarding the second-order beliefs introduced in this section.First, although we have introduced the beliefs in the models (i.e., the beliefs in mh’s), there are other types of beliefsthat could be considered instead. For example, we could introduce the beliefs in the conditional distributions, i.e., thebeliefs in Th(s, a, ·)’s, for each s ∈ S, a ∈ A, and h ∈ H , and the beliefs in Oh(s(cid:6), a, ·)’s, for each s(cid:6) ∈ S, a ∈ A, andh ∈ H . Another example is the beliefs in the sets of all the models, i.e., the beliefs in {mh|h ∈ H }’s. Comparisonsamong them remain to be studied in the future (but see the following paragraph).Second, we will actually use the second-order beliefs in the conditional distributions when we provide a solutionalgorithm in Section 4. A belief in the conditional distributions is equivalent to a belief in the models if the latter isdecomposable into a product of the beliefs in the conditional distributions (see Section 4.2). We will use only thesedecomposable beliefs when we provide a solution algorithm in Section 4. Thus, we actually consider the beliefs in theconditional distributions as far as the solution algorithm is concerned. However, we began with the non-decomposedsecond-order beliefs in this section because they are a wider class of the beliefs that could be utilized in a wider rangeof problems, and the theoretical result in Section 5.1 is applicable to these beliefs.Last, the parameter imprecision handled with the second-order beliefs in this paper can be handled equivalentlywith the convex hulls of possible probability measures (or the credal sets) [13,33]. Obviously, a second-order beliefbMh (mh) that satisfies the permissibility condition (Eqs. (11) and (12)) can be referred to also as a member of theconvex hull of the possible measures over the models, i.e., the convex hull of fi(mh) = δ(mh − mih), i = 1, 2, . . . ,where fi : M0 → [0, 1], each mih is a member of M, and δ is Dirac’s delta function. Similarly, as will become obviousin Section 4.2, using a second-order belief in the conditional distributions is equivalent to using a convex hull ofpossible conditional distributions.3.3. Quasi-optimal policiesIn the previous section, we defined the truly optimal policies with a strict optimal criterion, assuming that theagent could specify the second-order beliefs. In POMDPIPs, however, the agent has no precise idea how to specifythe second-order beliefs. In this section, we formulate a relaxed optimality criterion, allowing the agent to employarbitrary functions for the second-order beliefs.First, recall that, to define the truly optimal policy, the second-order belief bMh has to be determined in orderto Bayes-update the first-order belief by Eq. (14) after every h ∈ H , a ∈ A, and o ∈ Θ. Since bMh is unknown inPOMDPIPs, we allow the agent to employ any function that satisfies the permissibility condition (Section 3.2) for thispurpose. Let ˆbMh,a,o denote the employed function.H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490463Fig. 4. History tree for a POMDPIP with two actions and two observations. The possibly-correct first-order beliefs are calculated recursively usingthe possibly-correct second-order beliefs; ˆb11 = τ ( ˆb∅, a1, o1, ˆbM∅,a1,o1 ), ˆb12 = τ ( ˆb∅, a1, o2, ˆbM∅,a1,o2 ), and so on.In the following, we call bMh in Section 3.2 the “correct” second-order belief. By “correct” we stress that bMh hasbeen defined as the belief that the agent would employ if sufficient information is available. On the other hand, we callˆbMh,a,o the “possibly-correct” second-order belief. By “possibly correct” we mean that it could possibly be identical tothe correct belief.Let ˆbh denote the first-order beliefs that are calculated by these possibly-correct second-order beliefs. That is, ˆbhfor each h ∈ H is calculated recursively (Fig. 4) asˆb∅ = p0andˆbh;(cid:8)a,o(cid:9) = τ(cid:8)ˆbh, a, o, ˆbMh,a,o(cid:9)(19)(20)for each h ∈ H , a ∈ A, and o ∈ Θ. We call bh in Section 3.2 the correct first-order beliefs. We call ˆbh the possibly-correct first-order beliefs.Second, recall that, for each h ∈ H , the second-order belief bMh is also used to estimate the expected one-stepreward, ρ(h, a), and the probability of the next observation, P (o|h, a), in Eqs. (17) and (18), respectively. Again, weallow the agent to employ any function that satisfies the permissibility condition for these purposes. Let ˆbMh denotethe employed function, which we shall also label as the possibly-correct second-order beliefs.Last, we define the quasi-optimal policy as the solution of Eqs. (13)–(18) in which bMh is replaced with arbitrary,h . That is, the quasi-optimal policy is the policy ˆμ∗ : H → A that satisfiesbut possibly-correct, functions ˆbM(cid:7)∗ˆV(h) = maxa∈Aˆρ(h, a) + γˆP (o|h, a) ˆV∗(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9),h,a,o and ˆbM(cid:5)o∈Θ(21)464H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490(cid:7)∗ˆμ(h) = arg maxa∈Aˆρ(h, a) + γˆP (o|h, a) ˆV∗(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9),(cid:5)o∈Θfor every h ∈ H , where(cid:13)ˆρ(h, a) :=mh=(Th,Oh)∈M(cid:13)ˆP (o|h, a) :=mh=(Th,Oh)∈M(cid:5)(cid:5)s∈Ss(cid:6)∈SR(s, a, s(cid:6))Th(s, a, s(cid:6)) ˆbh(s) ˆbMh (mh) dmh,(cid:5)s(cid:6)∈S(cid:6)Oh(s, a, o)(cid:5)s∈STh(s, a, s(cid:6)) ˆbh(s) ˆbMh (mh) dmh,(22)(23)(24)in which ˆbh is defined by Eqs. (19) and (20). The hat mark (ˆ) on V ∗, μ∗, ρ(h, a), and P (o|h, a), has been introducedin order to stress that these functions are not calculated with the “correct” (but unknown) second-order beliefs.In summary, a quasi-optimal policy is the policy that satisfies the same Bellman equations as the hypotheticalPOMDPs, except that the second-order beliefs are replaced with arbitrary, but possibly-correct, ones. We regard quasi-optimal policies as the solution of POMDPIPs. We now proceed to make some additional comments regarding thesepolicies.We first note that, for each h ∈ H , we have replaced a correct second-order belief bMh,a,o for each a ∈ A and o ∈ Θ and ˆbMh with multiple possibly-correctsecond-order beliefs ( ˆbMh ). We could have instead used a single possibly-correct second-order belief. Doing this may sometimes have its own merit; if we use a single belief, then the quasi-optimal policy is guaranteed to be the truly optimal policy for at least one possible hypothetical POMDP, and hencecan be termed as E-admissible [33,34].However, in this paper, we study the use of multiple beliefs for the following reasons. First, at least for somePOMDPIPs, using multiple beliefs can lead to a more robust policy than using a single belief; it can be risky to relyon a single belief. We provide a simple example in Appendix A. Second, since by using multiple beliefs we have ahigher degree of freedom, the quasi-optimal policy should be easier to obtain. This is a useful property because oneof the motivations in this paper is to solve the problems with low computational costs.Note that we do not argue that the use of multiple beliefs is always better than the use of a single belief. Furtherresearch is necessary for detailed comparisons (however, see also Section 6.1 for an empirical study). Note also thatthe algorithm in the next section can be modified for the use of a single belief, although the modification will increasethe computational costs.We further note that, in our definition, the possibly-correct first-order belief ˆbh for each history h ∈ H is a singleprobability function. Another possibility is to use a set of probability functions (e.g., [13,18]). This possibility maybe worth investigating in the future. The manner of changing the set of probability functions, given a new action andobservation, is currently a topic of debate [3,22]; there can be multiple future research directions. In this paper, we donot use a set of probability functions. We regard the quasi-optimal policy as an optimal policy (in a broader sense) foran agent who is allowed to use only a single probability function for expressing its belief in the states.Note also that although we have introduced the second-order beliefs, we do not use them to make any inferenceabout the models. In a future study, it may be interesting to use the second-order beliefs to make some inferenceabout the uncertain models, e.g., Bayesian inference to identify the true model from the action-observation history.Although strict inference may be impossible because of prohibitive computational costs, we would be able to focuson identifying the true model to the extent that the remaining uncertainty does not significantly affect the total reward.Making such an inference in POMDPIPs can be an interesting future research topic.We finally note that, since ˆbMh,a,o and ˆbMh are arbitrary, there can be multiple quasi-optimal policies for a singlePOMDPIP. We regard any of those policies as the solution of the POMDPIP. In the next section, we will provide anefficient algorithm to obtain one of these quasi-optimal policies.4. Algorithm for solving POMDPIPs4.1. Determining ˆbMh,a,oThe first step in our algorithm is to determine ˆbMcondition. Recall that if we determine all the ˆbMh,a,o for each h ∈ H , a ∈ A, and o ∈ Θ under the permissibilityh,a,o’s, then all the possibly-correct first-order beliefs ( ˆbh for everyH. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490465h ∈ H ) are given by Eqs. (19) and (20). Let ˆB be the set of all the different first-order beliefs so derived; i.e., ˆB := { ˆb |ˆb = ˆbh, h ∈ H }. We try to keep | ˆB|, i.e., the number of the different first-order beliefs, as small as possible in orderto reduce computational costs. This is achieved using the following procedure, which we term the FIND-A-SMALL-BELIEF-SET procedure.We determine these ˆbMin Fig. 4 for example, we first calculate ˆb∅ (i.e., set it equal to p0), then determine ˆbMˆbM∅,a1,o2 , calculate ˆb12, . . . , determine ˆbMto make ˆbh;(cid:8)a,o(cid:9) (= τ ( ˆbh, a, o, ˆbMFor example, in Fig. 4, suppose that we have already determined ˆbMˆb11, and ˆb12. Now, we search for the ˆbMor ˆb12. For this search, we use the IS-FEASIBLE procedure in Section 4.2. If such a ˆbMotherwise we employ an arbitrary ˆbMh,a,o’s in a breadth-first manner, and calculate the ˆbh’s whenever it becomes possible; that is,∅,a1,o1 , calculate ˆb11, determine(a1,o1),a1,o1 , calculate ˆb1111, . . ., and so forth. In determining each ˆbMh,a,o, we tryh,a,o)) identical to one of the other first-order beliefs that have already been calculated.∅,a1,o2 , and that we have calculated ˆb∅,∅,a1,o1 and ˆbM∅,a2,o1 )) is identical to one of ˆb∅, ˆb11,∅,a2,o1 for which ˆb21 (= τ ( ˆb∅, a2, o1, ˆbM∅,a2,o1 is found, it is adopted,∅,a2,o1 under the permissibility condition.If it is possible to make one first-order belief identical to another, we will then also make the descendant first-and second-order beliefs identical. For example, suppose that ˆb21 was made identical to ˆb11. Since we determine(a1,o1),a1,o2 , . . ., respectively, we have ˆb2111 = ˆb1111,(a1,o1),a1,o1 , ˆbM(a2,o1),a1,o1 , ˆbMˆbMˆb2112 = ˆb1112, and so on. By following this rule, we can skip the determination and calculation of the beliefs followingˆb21; we need only consider the beliefs following ˆb11.(a2,o1),a1,o2 , . . ., to be the same as ˆbMWe continue determining and calculating the beliefs, skipping them whenever possible. Eventually, we see that weh,a,o’s and ˆbh’s have been determinedcan skip all the remaining ones (proof in Section 5.2). This means that all the ˆbMand calculated, respectively. We may then proceed to the next step described in Section 4.3.4.2. IS-FEASIBLE procedureThe IS-FEASIBLE procedure searches, under the permissibility condition, for a function ˆbM : M0 → [0, 1] forwhich τ ( ˆb, a, o, ˆbM ) = ˆb(cid:6) holds, given two first-order beliefs ˆb and ˆb(cid:6), an action a, an observation o, and a part ofthe POMDPIP tuple (S, A, Θ, T M , OM ). The search for the desired second-order belief in the FIND-A-SMALL-BELIEF-SET procedure (in Section 4.1) can be performed by using this procedure several times. Each time theIS-FEASIBLE procedure is used, we let ˆb(cid:6) be one of the other first-order beliefs that have already been calculated,and we let ˆb be ˆbh. For example, in Fig. 4, suppose again that we have already determined ˆbM∅,a1,o2 , andthat we have calculated ˆb∅, ˆb11, and ˆb12. We now have to search for the ˆbM∅,a2,o1 ) isidentical to ˆb∅, ˆb11, or ˆb12. This search can be performed by using the IS-FEASIBLE procedure three times withˆb(cid:6) = ˆb∅, ˆb11, or ˆb12 and with ˆb = ˆb∅.∅,a1,o1 and ˆbM∅,a2,o1 for which τ ( ˆb∅, a2, o1, ˆbMWe begin with replacing the required task with an easier one. Recall that our task is to search, under the permissi-bility condition, for ˆbM for which(cid:6)(cid:6)ˆb(s) =(cid:14)(cid:14)m=(T ,O)∈M O(s(cid:6), a, o)(cid:3)(cid:3)s(cid:6)(cid:6)∈S O(s(cid:6)(cid:6), a, o)m=(T ,O)∈Ms∈S T (s, a, s(cid:6)) ˆb(s) ˆbM (m) dm(cid:3)s∈S T (s, a, s(cid:6)(cid:6)) ˆb(s) ˆbM (m) dmfor every s(cid:6) ∈ Sholds, where ˆb, ˆb(cid:6), a, and o are given. We restrict our search to those ˆbM that can be decomposed according to(cid:8)O(s(cid:8)T (s, ˜a, ·)ˆbM (m) =(cid:9), ˜a, ·)(cid:9) (cid:15)(cid:15),(cid:6)Gs(cid:6), ˜aFs, ˜as∈S, ˜a∈As(cid:6)∈S, ˜a∈Awhere m = (T , O) ∈ M0, and Fs, ˜a (for every s ∈ S and ˜a ∈ A) and Gs(cid:6), ˜a (for every s(cid:6) ∈ S and ˜a ∈ A) are probabilitydensity functions. Here we denote an action by ˜a to distinguish it from the action, a, that is a constant (specified bythe FIND-A-SMALL-BELIEF-SET procedure) in this procedure. Note that such decomposable second-order beliefsalways exist. Note also that, by this restriction of the search, we might fail to find a desired ˆbM even when it exists.Hence the number of resultant first-order beliefs, | ˆB|, might increase. However, the restricted search can be performedquickly, and the total computational time may be reduced. Note that we can still find a quasi-optimal policy despitethe restriction.(25)(26)466H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490Fig. 5. Example of the averaged model function ˆT (s, ˜a, ·). The striped areas indicate where ˆT (s, ˜a, ·) can be located. Compare this figure withFig. 3, letting a = ˜a. (A) the interval case. ˆT (s, ˜a, s(cid:6)) should be in the same convex area that constrained Th(s, ˜a, s(cid:6)) in Fig. 3. (B) the point-setcase. ˆT (s, ˜a, ·) should be a convex combination of the possible probability functions that Th(s, ˜a, ·) in Fig. 3 could be identical to.Let us define the averaged model functions as(cid:13)ˆT (s, ˜a, s(cid:6)) :=T (s, ˜a, s(cid:6))Fs, ˜aT (s, ˜a,·)∈Δ|S|(cid:9)(cid:8)T (s, ˜a, ·)dT (s, ˜a, ·)for each s, s(cid:6) ∈ S, and ˜a ∈ A, and(cid:13)(cid:6)ˆO(s, ˜a, ˜o) :=O(s(cid:6), ˜a,·)∈Δ|Θ|(cid:6)O(s, ˜a, ˜o)Gs(cid:6), ˜a(cid:8)O(s(cid:6)(cid:9), ˜a, ·)dO(s(cid:6), ˜a, ·)(27)(28)for each s(cid:6) ∈ S, ˜a ∈ A, and ˜o ∈ Θ. Again, we denote an observation by ˜o to distinguish it from the specified observa-tion o. From Eqs. (26)–(28), we can re-write Eq. (25) as(cid:6)(cid:6)ˆb(s) =(cid:3)(cid:3)s∈S(cid:3)ˆO(s(cid:6), a, o)ˆT (s, a, s(cid:6)) ˆb(s)ˆO(s(cid:6)(cid:6), a, o)s(cid:6)(cid:6)∈SˆT (s, a, s(cid:6)(cid:6)) ˆb(s)s∈Sfor every s(cid:6) ∈ S.(29)From the permissibility condition together with Eqs. (26) and (27), we have that, for each s ∈ S and ˜a ∈ A, ˆT (s, ˜a, ·)should be a convex combination of the possible probability functions that the model-set function T M specifies. Inthe interval case (Fig. 3A), the possible probability functions are those within a convex area that are constrained bythe intervals for each parameter. Thus, their convex combination, ˆT (s, ˜a, ·), is also constrained by the same intervals(Fig. 5A). In the point-set case (Fig. 3B), the possible probability functions are those that are directly specified by themodel-set functions. Thus ˆT (s, ˜a, ·) is a convex combination of these probability functions (Fig. 5B). We call theseconditions that ˆT (s, ˜a, ·) for each s ∈ S and ˜a ∈ A should satisfy the convexity conditions for ˆT . Similarly we haveconditions for ˆO(s(cid:6), ˜a, ·) for each s(cid:6) ∈ S and ˜a ∈ A, which we call the convexity conditions for ˆO.In summary, the search for a ˆbM that satisfies both Eq. (25) and the permissibility condition has been replaced withthe search for a pair of the averaged model functions, ( ˆT , ˆO), that satisfies Eq. (29) and the convexity conditions forˆT and for ˆO.Whether such a pair of the averaged model functions exists is easily determined by the feasibility test of Fig. 6.A proof of the validity of the test is detailed in Appendix B. Henceforth, we shall label the implementation of this testthe IS-FEASIBLE procedure. The averaged model functions ( ˆT , ˆO), if they exist, can be derived from the solution ofthis test.7In the procedure and thereafter, we let T , T , O, and O denote the lower and upper bounds of each parameter; inthe interval case, the lower bounds are defined by7 Let ˆO(s(cid:6), a, o) = Z/q(s(cid:6)) for every s(cid:6) ∈ S. Employ arbitrary values (under the convexity conditions) for the undetermined parameters.H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490467(A)Procedure IS-FEASIBLE (for the interval case)input: ( ˆb, a, o, ˆb(cid:6), S, A, Θ, T M , OM )output: True or FalseTest if there is a solution that satisfies the following constraints:variables:ˆT (s, a, s(cid:6)) ∈ R for all s, s(cid:6) ∈ S,q(s(cid:6)) ∈ R for all s(cid:6) ∈ S,Z ∈ R,constraints:T (s, a, s(cid:6)) (cid:2) ˆT (s, a, s(cid:6)) (cid:2) T (s, a, s(cid:6)) for all s, s(cid:6) ∈ S,(cid:3)(cid:3)ˆT (s, a, s(cid:6)) = 1 for all s ∈ S,ˆb(s) ˆT (s, a, s(cid:6)) = ˆb(cid:6)(s(cid:6))q(s(cid:6)) for all s(cid:6) ∈ S,(cid:2) q(s(cid:6)) (cid:2)ZO(s(cid:6),a,o) for all s(cid:6) ∈ S,s(cid:6)∈Ss∈SZO(s(cid:6),a,o)Z (cid:3) 0.Return True if a solution is found. Return False otherwise.end procedure(B)Procedure IS-FEASIBLE (for the point-set case)input: ( ˆb, a, o, ˆb(cid:6), S, A, Θ, T M , OM )output: True or FalseTest if there is a solution that satisfies the following constraints:∈ [0, 1] for all s ∈ S and i = 1, . . . , |T M (s, a)|variables:ˆT (s, a, s(cid:6)) ∈ R for all s, s(cid:6) ∈ S,λisq(s(cid:6)) ∈ R for all s(cid:6) ∈ S,Z ∈ R,constraints:(cid:3)i λiˆT (s, a, s(cid:6)) =(cid:3)(cid:3)(s, a, s(cid:6)) for all s, s(cid:6) ∈ S,s T Mi= 1 for all s ∈ S,ˆb(s) ˆT (s, a, s(cid:6)) = ˆb(cid:6)(s(cid:6))q(s(cid:6)) for all s(cid:6) ∈ S,(cid:2) q(s(cid:6)) (cid:2)ZO(s(cid:6),a,o) for all s(cid:6) ∈ S,i λiss∈SZO(s(cid:6),a,o)Z (cid:3) 0.Return True if a solution is found. Return False otherwise.end procedureFig. 6. Sub-routine that checks if the averaged model functions ( ˆT , ˆO) that change ˆb into ˆb(cid:6)after a ∈ A and o ∈ Θ can exist. (A) is for the intervalcase; (B) is for the point-set case. In (B), we define |T M (s, a)| as the number of possible probability functions that T M (s, a) specifies. Eachprobability function is indexed as T M(s, a), where i = 1, 2, . . . , |T M (s, a)|.i(cid:6)T (s, ˜a, s(cid:6)O(s) := min T M (s, ˜a, s, ˜a, ˜o) := min OM (s(cid:6)),, ˜a, ˜o),(cid:6)for all s, s(cid:6) ∈ S, ˜a ∈ A, and ˜o ∈ Θ. In the point-set case, they are defined byT (s, ˜a, s(cid:6)) :=minT (s, ˜a,·)∈T M (s, ˜a)T (s, ˜a, s(cid:6)),O(s(cid:6), ˜a, ˜o) :=minO(s(cid:6), ˜a,·)∈OM (s(cid:6), ˜a)O(s(cid:6), ˜a, ˜o),(30)(31)(32)(33)for all s, s(cid:6) ∈ S, ˜a ∈ A, and ˜o ∈ Θ. The upper bounds are defined by substituting max for min in the above definitions.The feasibility test in Fig. 6 can be conducted efficiently; all the constraints are linear, and, in the interval case,there are |S|(|S| + 1) + 1 variables and 2|S|(|S| + 2) + 1 (in)equalities, which are almost minimum in comparison468H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490with the number of the parameters and the bounds within the problem at hand.8 It is also efficient for the point-setcase. The feasibility test can be solved by sub-procedures of the linear programming routines that are implemented inmany programming languages. The worst-case complexity is a polynomial order of |S| [31]. See also Section 5.2 forthe computational complexity of the overall algorithm.For later use, we define some notations here. Recall that we use this procedure when we determine ˆbMh,a,o for eachh ∈ H , a ∈ A, and o ∈ Θ in the FIND-A-SMALL-BELIEF-SET procedure. We denote the desired averaged modelfunctions, if found, by ˆTh,a,o and ˆOh,a,o. If they are not found, we employ arbitrary ˆT and ˆO that satisfy the convexityconditions, and denote them by ˆTh,a,o and ˆOh,a,o, also. Thus, ˆTh,a,o and ˆOh,a,o always indicate (implicitly) the ˆbMh,a,oto be employed.4.3. Determining ˆbMhThe second step in our algorithm is to determine, for each h ∈ H , the ˆbMAlthough there are several possible ways to do this, here we determine them ash that satisfies the permissibility condition.ˆbMh,a,o(mh)for all mh ∈ M0,(34)h (mh) = 1ˆbM|A(cid:10)Θ|(cid:5)a∈A,o∈Θfor every h ∈ H . That is, we take the average of the ˆbh,a,o’s. Clearly, if the ˆbMthen so will the ˆbMh ’s.h,a,o’s satisfy the permissibility condition,Recall that, in the previous section, we have replaced the determination of ˆbMand ˆOh,a,o; we have determined ˆbMhowever, that we need ˆbMin Eqs. (23) and (24) with the right-hand side of Eq. (34), and using Eqs. (26)–(28), we haveh,a,o with the determination of ˆTh,a,oh explicitly from Eq. (34). Recall,h only for the evaluation of ˆρ(h, a) and ˆP (o|h, a) from Eqs. (23) and (24). Substituting ˆbMh,a,o only implicitly. Thus we cannot determine ˆbMhˆρ(h, a) = 1|A(cid:10)Θ|ˆP (o|h, a) = 1|A(cid:10)Θ|(cid:5)(cid:5)(cid:5)R(s, a, s(cid:6)) ˆTh, ˜a, ˜o(s, a, s(cid:6)) ˆbh(s),˜a∈A, ˜o∈Θ(cid:5)s∈Ss(cid:6)∈S(cid:5)ˆOh, ˜a, ˜o(s(cid:6), a, o)(cid:5)ˆTh, ˜a, ˜o(s, a, s(cid:6)) ˆbh(s).˜a∈A, ˜o∈ΘThus ˆρ(h, a) and ˆP (o|h, a) are given in terms of the ˆTh,a,o’s,determined.s∈Ss(cid:6)∈SˆOh,a,o’s, and ˆbh’s, all of which have already been4.4. Dynamic programming over the possibly-correct first-order beliefsThe last step in our algorithm is to solve the Bellman equations, i.e., Eqs. (21), (22), (35), and (36). As in Sec-tion 2.3, solving these equations is equivalent to solving a belief-state MDP:(cid:7)( ˆb) = maxˆρ( ˆb, a) + γa∈A∗ˆV(cid:7)ˆρ( ˆb, a) + γ( ˆb) = arg maxa∈A∗ˆμ(cid:5)o∈Θˆρ( ˆb, a) = 1|A(cid:10)Θ|ˆP (o| ˆb, a) = 1|A(cid:10)Θ|(cid:5)˜a∈A, ˜o∈Θ(cid:5)o∈Θ(cid:5)(cid:5)s∈Ss(cid:6)∈S(cid:5)˜a∈A, ˜o∈Θs(cid:6)∈SˆP (o| ˆb, a) ˆV∗(cid:8)τ(cid:8)ˆb, a, o, ˆbMˆb,a,o(cid:5)ˆP (o| ˆb, a) ˆV∗(cid:8)(cid:8)τˆb, a, o, ˆbMˆb,a,o(cid:10)(cid:9)(cid:9),(cid:10)(cid:9)(cid:9),R(s, a, s(cid:6)) ˆT ˆb, ˜a, ˜o(s, a, s(cid:6)) ˆb(s),ˆO ˆb, ˜a, ˜o(s(cid:6), a, o)(cid:5)s∈SˆT ˆb, ˜a, ˜o(s, a, s(cid:6)) ˆb(s),(35)(36)(37)(38)(39)(40)8 To be precise, the procedure in Fig. 6 is correct only under the condition that ˆb(cid:6)(s(cid:6)) (cid:11)= 0 and (O(s(cid:6), a, o), O(s(cid:6), a, o)) (cid:11)= (0, 0) for all s(cid:6) ∈ S. Forthe general case, we require a little more complicated procedure that is described in Appendix B.H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490469ˆP , ˆT , and ˆO are all re-defined on ˆB, instead of H : for example, we substitute ˆρ( ˆb, a) for allwhere ˆV ∗, ˆμ, ˆρ,the ˆρ(h, a)’s for which ˆbh is the same. To see that solving these equations is equivalent to solving Eqs. (21), (22),(35), and (36), note that we have, by construction, ˆTh1,a,o = ˆTh2,a,o,h1,a,o) =τ ( ˆbh2 , a, o, ˆbM= ˆbh2 holds, for any h1, h2 ∈ H , a ∈ A, and o ∈ Θ.Finally, if | ˆB| is finite, the quasi-optimal policy can be obtained by solving these equations (Eqs. (37)–(40)) nu-ˆOh1,a,o = ˆOh2,a,o, and τ ( ˆbh1 , a, o, ˆbMh2,a,o) whenever ˆbh1merically, for example by the value iteration methods [6].5. Theoretical analyses5.1. A bound on the reward losses of the quasi-optimal policiesWe provide a bound on the amount of reward loss that may occur by using a quasi-optimal policy instead ofspecifying the correct second-order beliefs and performing the strict optimization. Note that this bound is applicablenot only to the quasi-optimal policies obtained by the algorithm in Section 4 but also to quasi-optimal policiesobtained by any other method.max and (cid:10)OIn the following, let (cid:10)Tmax denote the maximum imprecision of the parameters in Th and Oh, i.e.,(cid:16)(cid:16)T (s, a, s(cid:6)) − T (s, a, s:= max(41)(cid:16)(cid:16))(cid:6)(cid:10)Tmaxs,s(cid:6)∈S, a∈Aand(cid:10)Omax:=maxs(cid:6)∈S, a∈A, o∈Θ(cid:16)(cid:16)O(s(cid:6), a, o) − O(s(cid:6)(cid:16)(cid:16)., a, o)(42)Let V ˆμ∗ˆμ∗ that is evaluated in the hypothetical POMDP. Let V μ∗evaluated in the hypothetical POMDP. Note that, by definition, V μ∗ (cid:3) V ˆμ∗be the value (i.e., the infinite-horizon discounted sum of the expected rewards) of a given quasi-optimal policybe the value of the truly optimal policy μ∗ that is also.Theorem 1. The reward loss, i.e., the difference between V ˆμ∗and V μ∗+ 16γ d)Rmax + (1 + 15γ )γ d( ˆV, is bounded bymax + ˆV μ∗ˆμ∗max),(1 − γ )2V μ∗ − Vˆμ∗ (cid:2) ((1 − γ )|S|(cid:10)Tmaxwhered := |S|(cid:10)TRmax := maxmax+ |Θ|(cid:10)Omax(cid:16)(cid:16)R(s, a, s+ 2|Θ(cid:10)S|(cid:10)T(cid:16)(cid:16),)(cid:6)max(cid:10)Omax,s,s(cid:6)∈S, a∈AandˆV μmax:=maxh∈H, a∈A, o∈Θ(cid:16)(cid:16) ˆV μ(cid:8)h; (cid:8)a, o(cid:9)(cid:9)(cid:16)(cid:16)for any policy μ : H → A, in which ˆV μ : H → R is the solution of(cid:5)(cid:9)(cid:8)h; (cid:8)μ(h), o(cid:9)(cid:9)(cid:8)o|h, μ(h)(cid:9)(cid:8)h, μ(h)ˆV μ(h) = ˆρˆV μ+ γˆP(43)(44)(45)(46)for every h ∈ H , where ˆρ and ˆP are the functions in Eqs. (21) and (22) that the quasi-optimal policy ˆμ∗ satisfies.o∈ΘProof. See Appendix C. (cid:2)Here are some notes on how this bound can be calculated (or upper-bounded):First, both ˆVis easy to calculate.Second, ˆVmax and ˆV μ∗ˆμ∗ˆμ∗max is readily available if our algorithm (Section 4) has been used to derive the quasi-optimal policyˆμ∗. Note that ˆV μ in Eq. (46) is an extension of ˆV ∗ in Eq. (21) in the sense that ˆV μ is the value (that is estimated bymax are upper-bounded by Vmax := Rmax + γ Rmax + γ 2Rmax + · · · = Rmax/(1 − γ ), which470H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490ˆμ∗max by Eq. (45).using possibly-correct beliefs) for any policy, whereas ˆV ∗ is the value for the quasi-optimal policy ˆμ∗. Thus, ˆV ˆμ∗identical to ˆV ∗. Since our algorithm calculates ˆV ∗, it is easy to obtain ˆVmax is upper-bounded by the larger value of ˆVLast, for ˆV μ∗ˆμ∗max. Otherwise, ˆV μupper-bounded by ˆVcalculated in exactly the same manner as ˆVoptimal policy ˆμ∗max, there is a tighter bound than Vmax. If R(s, a, s(cid:6)) is non-negative for all s, a, and s(cid:6), then ˆV μ∗max isˆμ∗−,−ismaxˆμ∗max, except that the reward function R is replaced by −R and the quasi-− maximizes the estimated total negative reward).9max, andmax are sufficiently small, this bound is looser than the default bound 2Rmax/(1 − γ ) that can be applied to any(cid:10)Opolicy.10 Given the presence of the (1 − γ )−2 factor, our bound is of limited practical use for problems in whichγ is close to 1. Tighter bounds remain to be derived in future studies. We will study the reward loss empirically inSection 6.Thus, this theoretical bound can be easily calculated. Unfortunately, this is not a tight bound. Unless γ , (cid:10)T− is optimized for this modified problem (i.e., ˆμ∗ˆμ∗max and ˆV, where ˆVˆμ∗max−,−is5.2. Computational complexity of the provided algorithmHere we summarize the computational costs our algorithm (Section 4) incurs in solving for the quasi-optimal policy.First, we show that the FIND-A-SMALL-BELIEF-SET procedure of Section 4.1, which uses the IS-FEASIBLEprocedure discussed in Section 4.2, terminates with finite | ˆB| under moderate conditions. Suppose, for example,that every parameter has non-zero imprecision (i.e., for every parameter, the lower bound (Eqs. (30)–(33)) does notinto which a first-order belief ˆb can be Bayes-updated (Eq. (29))equal the upper bound). Let us consider a set Δ(cid:6)ˆbby choosing a set of averaged model functions ( ˆT and ˆO) arbitrarily under the convexity conditions. Note that Δ(cid:6)ˆbis a subset of the probability simplex Δ|S|. Let bc be the first-order belief that is added when the desired averagedmodel functions are not found in the IS-FEASIBLE procedure (Section 4.2). Since every parameter has non-zeroimprecision, we can always place bc inside Δ(cid:6)at a positive distance from its boundary; i.e., we can guarantee that theˆbset Δ(cid:6)ˆbincludes a non-empty ball(cid:11)b | (cid:10)b − bc(cid:10) (cid:2) (cid:10)r , b ∈ Δ|S|(cid:12)K =(cid:6)⊂ Δˆb,(47)where (cid:10)r > 0 is the radius of the ball, and (cid:10) · (cid:10) denotes the L1 norm. Thus, when the FIND-A-SMALL-BELIEF-SET procedure adds a new first-order belief, this belief should be a distance of more than (cid:10)r from the other first-orderbeliefs. Since every first-order belief is in the simplex Δ|S|, the procedure cannot continue adding new first-orderbeliefs an infinite number of times. It has therefore been proved that the procedure terminates with a finite numberof the first-order beliefs. The condition that every parameter has non-zero imprecision can be relaxed, as long as thenon-empty ball is guaranteed to exist.As is evident from this discussion, the number of first-order beliefs | ˆB| can, in the worst case, increase exponentiallywith |S|. As described below, | ˆB| is an important factor in the computational complexity of the provided algorithm.However, currently, we have no other theoretical results concerning | ˆB|. We will instead provide some empiricalresults in the next section; for example, we will show that | ˆB| does not always increase exponentially with |S|.Next, we quantify the computational complexity of the present algorithm. We use the notation O(·) to indicatethe order of the complexity. First, for the FIND-A-SMALL-BELIEF-SET procedure, we need, in the worst case,O(| ˆB|2|A(cid:10)Θ|) iterations of the IS-FEASIBLE procedure. Second, as noted in Section 4.2, the IS-FEASIBLE proce-dure can be conducted efficiently; in the interval case, the complexity is O(poly(|S|)), that is the polynomial order of|S|. In the point-set case, it is O(poly(|S|NT )), where NT := maxs∈S,a∈A |T M (s, a)|. Third, to construct the belief-state MDP (Eqs. (39) and (40)), we require O(| ˆB||A|2|Θ|2|S|2) time. Last, to solve the belief-state MDP (Eqs. (37)and (38)) by the value iteration method, we need O(| ˆB||A||Θ|) time per iteration.∗9 The former bound is derived by noting that ˆV μ∗additionally that | ˆV μwhere we define ˆV μ,−10 This default bound is obtained by observing that any policy may miss at most 2Rmax reward at each time step. Calculating the discounted sumover an infinite horizon gives this bound; i.e., 2Rmax + 2Rmaxγ + 2Rmaxγ 2 + · · · = 2Rmax/(1 − γ ).∗(h; (cid:8)a, o(cid:9))| (cid:2) max( ˆV μfor any μ in the same manner as ˆV μ, except that we replace the reward function R with −R.(h; (cid:8)a, o(cid:9)) holds for any h, a, and o. The latter bound is derived by noting(h; (cid:8)a, o(cid:9)) hold for any h, a, and o,∗(h; (cid:8)a, o(cid:9))) and − ˆV μ∗(h; (cid:8)a, o(cid:9)), − ˆV μ∗(h; (cid:8)a, o(cid:9)) (cid:2) ˆV ˆμ(h; (cid:8)a, o(cid:9)) (cid:2) ˆV∗−,−ˆμH. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490471We can make a small modification to our algorithm, which often reduces the computational cost. Combin-ing the complexity of the FIND-A-SMALL-BELIEF-SET procedure and that of the IS-FEASIBLE procedure, wesee that the time that the IS-FEASIBLE procedure requires is O(| ˆB|2|A||Θ|poly(|S|)) in the interval case, andO(| ˆB|2|A||Θ|poly(|S|NT )) in the point-set case. These can be changed into O(| ˆB||A||Θ|(poly(|S|) + | ˆB||S|))and O(| ˆB||A||Θ|(poly(|S|NT ) + | ˆB||S|)), respectively, by the following modification. Recall that, in the FIND-A-SMALL-BELIEF-SET procedure, given each ˆbh and a ∈ A and o ∈ Θ, we search through all the candidates (i.e.,all the first-order beliefs that have already been calculated), seeking one that τ ( ˆbh, a, o, ˆbMh,a,o) can be identical to.However, in many cases, searching through all the candidates can be avoided; we can search through only a smallnumber of the candidates that are likely to include the required belief. To do this, we first calculate a tentativelyBayes-updated belief ˆb(cid:6)(cid:6) = τ ( ˆbh, a, o, ˆbMh,a,o that satisfies the permissibility condition. Thenwe pick k-nearest neighbors of ˆb(cid:6)(cid:6) among all the candidate beliefs, and we search only within them. Note that, withthis modification, we can still find the quasi-optimal policy. Although this modification may increase the resultant sizeof ˆB, empirically, it was found to significantly reduce the total computational time. In the next section we adopt thistechnique with k = 5, using the L1 norm for measuring the distance between the first-order beliefs. Further, note thatwith this modification, the FIND-A-SMALL-BELIEF-SET procedure of Section 4.1 still terminates with finite | ˆB|under moderate conditions. This is proved in exactly the same manner as in the beginning of this section if we let ˆb(cid:6)(cid:6)be bc, i.e., if we use ˆb(cid:6)(cid:6) as the first-order belief to be added when the search fails.h,a,o), using an arbitrary ˆbMThere is another modification that we can make to our algorithm in order to reduce the computational cost. Whenthe number of states, |S|, is large, it takes long time to solve the linear programming problem in the IS-FEASIBLE pro-cedure. Recall that it takes O(poly(|S|)) time in the interval case and O(poly(|S|NT )) time in the point-set case. Wecan modify the procedure so that we search only for the values for ˆO. The transition function ˆT is set arbitrarily (underthe convexity conditions). With ˆT fixed, the procedure (Fig. 6) requires O(|S|2) time for both the interval case andthe point-set case. This modification often reduced the solution time when |S| is large. We use this modification inSections 6.2 and 6.3. Note that since this modification restricts the search space of the IS-FEASIBLE procedure, thenumber of required first-order beliefs might increase. However, this modification often reduced significantly the timespent in each search, and consequently the total solution time was reduced. Note also that with this modification, theFIND-A-SMALL-BELIEF-SET procedure of Section 4.1 still terminates with finite | ˆB| under moderate conditions.For example, if every parameter in ˆO has non-zero imprecision, the non-empty ball K in Eq. (47) can be guaranteedto exist, and hence the procedure terminates with finite | ˆB|.6. ExperimentsWe applied our algorithm to several POMDPIPs. We report only results for the interval case. Those for the point-setcase are expected to be similar. All results were obtained by Matlab codes on a Pentium4 PC.116.1. Small-sized POMDPIPsHere, we used nine small-sized POMDPs for which the optimal policies are known [11,24,57].First, for each POMDP (Table 1) and a parameter (cid:10) ∈ {0.0125, 0.025, 0.05, 0.1, 0.2, 0.4, 0.6} that controls theimprecision, we constructed a POMDPIP as follows. We set T M as T M (s, a, s(cid:6)) := [T (s, a, s(cid:6)) − (cid:10), T (s, a, s(cid:6)) +(cid:10)] for all s, s(cid:6) ∈ S and a ∈ A, where T (s, a, s(cid:6)) is the parameter value of the original POMDP. Similarly we setOM (s(cid:6), a, o) := [O(s(cid:6), a, o) − (cid:10), O(s(cid:6), a, o) + (cid:10)] for all s(cid:6) ∈ S, a ∈ A and o ∈ Θ, where O(s(cid:6), a, o) is the originalparameter value. We made these bounds saturated as 0 or 1, when they exceeded the range of [0, 1]. We then calculatedthe maximum imprecision (cid:10)Tmax (Eqs. (41) and (42)), and let (cid:10)max be the maximum of these.max and (cid:10)OSecond, we applied our algorithm to each of these POMDPIPs, and obtained the quasi-optimal policies. Since thealgorithm sometimes requires arbitrary averaged model functions ( ˆT and ˆO), we calculated “typical” model functionsand used them whenever necessary. To make the typical model functions, we first calculated the middle points of thes(cid:6) ˆT (s, a, s(cid:6)) = 1), weupper and lower bounds. Since such points sometimes break the sum-to-1 constraints (e.g.,(cid:3)11 The codes are freely available at http://www.brn.dis.titech.ac.jp/˜hideaki/pomdpips/index.htm.472H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490Table 1Small-sized POMDPs with known optimal policiesTest problemTiger1D maze4× 34 × 3 COCheesePart paintingNetworkShuttleAircraft ID|S|2411111147812|A|324444436|Θ|2261172255Fig. 7. Small-sized POMDPIPs; the reward losses of the quasi-optimal policies evaluated in the original POMDPs.then picked the nearest points (measured by Euclidean norm) that do not violate all the constraints (both the sum-to-1constraints and the convexity conditions).Then we studied whether the quasi-optimal policies obtained were nontrivial (Fig. 7). For each POMDPIP problem,we assumed that the original POMDP was the true environment, i.e., the hypothetical POMDP in Section 3.2. We, i.e., the value of the obtained quasi-optimal policy in the original POMDP. We also evaluated V μ∗evaluated V ˆμ∗,i.e., the value of the truly optimal policy of the original POMDP. The difference, V μ∗ − V ˆμ∗, indicates the reward lossthat occurred by using the quasi-optimal policy, instead of identifying the true environment and performing the strictoptimization for it. Fig. 7 shows the reward losses, each of which was normalized by V μ∗. For all of the problems, thereward losses were small when (cid:10)max was small. This indicates that the quasi-optimal policies found were nontrivial.These policies are admissible as solutions of the POMDPIPs in the sense that they are almost optimal for at least onepossibly true environment.Furthermore, we studied robustness of the obtained policies (Fig. 8). In this study, we regard a policy to be robustif the reward loss, V μ∗ − V ˆμ∗, is small for various true environments. For each POMDPIP problem, we randomlygenerated twenty hypothetical POMDPs. For each of the hypothetical POMDPs, we calculated the reward loss in thesame way as in Fig. 7. Then we calculated the largest reward loss in these twenty environments. The results (Fig. 8)suggest that, in many problems, the reward loss is kept small as long as (cid:10)max is within a range. However, in Aircraft IDand Tiger, relatively large reward losses were observed. In these problems, the agent receives a huge negative rewardby taking some action in some state. To avoid the huge negative reward, the first-order belief (i.e., the belief in thestate) needs to be inferred precisely. For such problems, quasi-optimal policies may not be a good solution; we mayneed precise probabilities and strict optimization, or we may need other policies such as maximin.We compared the robustness of the quasi-optimal policies with that of the E-admissible policies. Note that thequasi-optimal policies could be more robust than the E-admissible policies (Section 3.3 and Appendix A). For eachPOMPDIP problem, we obtained three kinds of E-admissible policies: (1) the optimal policy for the original POMDPproblem, (2) the optimal policy for the POMDP that consists of the “typical” model functions, and (3) the optimalH. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490473Fig. 8. Small-sized POMDPIPs; the largest reward losses of the quasi-optimal policies evaluated in twenty randomly-generated hypotheticalPOMDPs. See Fig. 7 for the legend.Fig. 9. Small-sized POMDPIPs; solution time (left) and | ˆB| (right). See Fig. 7 for the legend.policy for a randomly-generated hypothetical POMDP. For each of these E-admissible policies, we calculated thelargest reward loss as in Fig. 8. Unfortunately, the results (not shown) were not significantly different from those ofthe quasi-optimal policies in Fig. 8. Among the three kinds of the E-admissible policies tested, the first ones (i.e.,the optimal policies for the original POMDPs) were the most robust (i.e., the largest reward loss was the smallest)in most of the problems. Thus, we compared these E-admissible policies with the quasi-optimal policies. Out of the63 POMDPIP problems (9 original POMDP problems times 7 values of (cid:10)max) in Fig. 8, these E-admissible policieswere more robust than the quasi-optimal policies in 29 problems. In the other 34 problems, the quasi-optimal policieswere more robust. Thus, the quasi-optimal policies tended to be more robust. However, the difference was subtle, andfurther research is required for detailed comparisons.Fig. 9 (left) shows the cpu time required to obtain the quasi-optimal policies. Our algorithm terminated in a rea-sonably short time in many cases. For example, for the “Aircraft ID” problem, the shortest solution time ever reportedis 27,676 seconds [57]. Although direct comparison is impossible, our algorithm required 5,152 seconds when (cid:10)maxwas around 0.05, and only 467 seconds when it was around 0.1 (with a degraded value, though). For the “Network”problem, the shortest solution time ever was 140 seconds [57], whereas our algorithm required 35 seconds when (cid:10)maxwas around 0.05 and 11 seconds when it was around 0.1. In some of the other problems, however, our algorithmrecorded longer solution times than those reported for other algorithms.The number of beliefs, | ˆB|, is plotted in Fig. 9 (right). It may be noted that the plots of the | ˆB| appear similar tothose of the cpu time. This result is in agreement with expectations due to the algorithm’s complexity (Section 5.2).We further note that | ˆB| increases as (cid:10)max becomes smaller. All the plots in the figure appear to be well approximated474H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490by a straight line. Since the figure is the log-log scale plot, we may conclude that | ˆB| is approximately proportional topoly(1/(cid:10)max).The slopes of the plots in Fig. 9 clearly depend on the problem being solved; The slopes of the plots of the AircraftID and Part painting problems are large, whereas they are smaller for the Cheese and 4 × 3 CO problems. Thisindicates that | ˆB| does not depend directly on the size of the problem (i.e., |S|, |A| and |Θ|). Rather, it appears thatthe slopes tended to be smaller for problems for which |Θ| is large compared with |S|. This result is in agreementwith expectations; if |Θ| is large, and if by each o ∈ Θ the agent tends to be able to obtain a relatively large amountof information about the state s, then the first-order beliefs ˆbh ∈ ˆB will be mostly located around the rims of theprobability simplex (i.e., ˆbh(s) (cid:13) 1 for most s ∈ S), and hence only a small number of the beliefs would be requiredin the FIND-A-SMALL-BELIEF-SET procedure of Section 4.1.6.2. POMDPIPs with nearly full observationsNext we study larger-sized problems. As suggested in the previous section, our algorithm can be expected toquickly solve POMDPIPs in which the agent’s first-order beliefs are mostly located around the rims of the probabilitysimplex. As an example, we consider here some problems in which the agent can perform low-noise observations ofthe states.First, we constructed POMDPs. They are maze-type environments with n states and four actions, and the agent’sstate can be observed by n kinds of observations (i.e., |S| = |Θ| = n, |A| = 4); we constructed three mazes withn = 320, 640, and 1280. These POMDPs are simple models of a navigation problem in which a robot does not havea complete capability of moving and sensing, but has a capability close to this condition. The agent’s action changes,with a probability of 0.9, its current state to another one in agreement with its intention. However, with a probabilityof 0.1, the agent stays in the same state or moves to an unintended state that is selected uniformly randomly. Eachobservation corresponds to each state by a one-to-one mapping, and usually (with probability 0.9) the agent observesthe current state correctly. However, with a probability of 0.1, it observes a false state that is selected uniformlyrandomly. The initial state is distributed uniformly over the states. There is a single goal state. A reward of 1 is givenfor reaching the goal state, there is no reward otherwise. We set γ = 0.95.Having constructed the POMDPs, we made POMDPIPs from these POMDPs, in the manner described in Sec-tion 6.1.Then we obtained their solutions. We used a modified algorithm in which the IS-FEASIBLE procedure searchesonly for the values in ˆO (see Section 5.2 for details). We set the transition function ˆT as the “typical” model functiondefined in Section 6.1, instead of setting it by linear programming.The results are shown in Figs. 10–12. The formats are the same as those of Figs. 7–9, except that, in Figs. 10 and, of each problem was calculated approximately by the Perseus algorithm11, the value of the truly optimal policy, V μ∗[52,53] because we could not solve these large problems exactly.Fig. 10. POMDPIPs with nearly full observations; the reward losses of the quasi-optimal policies evaluated in the original POMDPs.H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490475Fig. 11. POMDPIPs with nearly full observations; the largest reward losses of the quasi-optimal policies evaluated in twenty randomly-generatedhypothetical POMDPs. See Fig. 10 for the legend.Fig. 12. POMDPIPs with nearly full observations; solution time (left) and | ˆB| (right). See Fig. 10 for the legend.The quasi-optimal policies were obtained successfully for these POMDPIPs, even for the cases in which |S| =|Θ| = 1240, without a significant reduction in the performance. The policies obtained were suggested to be nontrivial(Fig. 10) and robust (Fig. 11) when the parameter imprecision was small. As expected, the algorithm terminated ina reasonable time (Fig. 12 left) with a small number of first-order beliefs (Fig. 12 right). In all of the problems, thenumber of required first-order beliefs was less than 4 times larger than |S|.6.3. Problems that have not been solvedAlthough our algorithm was able to solve the large-sized problems up to |S| = 1240 in Section 6.2, it failed to solvesome other problems in a reasonable amount of time. We report on these problems in this section. Further research isrequired to solve these problems.We tried to solve the Tag-Avoid problem [44], Cycle10 problem, and 3leg10 problem [45,47] (Table 2). For eachof these POMDP problems, we created POMDPIPs in the manner described in Section 6.1, with (cid:10) = 0.0125, 0.1, 0.4,0.6, or 0.8. In each POMDPIP problem, we applied the algorithm used in Section 6.2, which searches only for thevalues of the parameters in ˆO. The algorithm used in Section 6.1, which searches for the values of the parameters inˆT and ˆO, was not applicable due to running out of memory.For each POMDPIP problem, we allowed the algorithm to run for up to 48 hours. For (cid:10) ∈ {0.0125, 0.1, 0.4, 0.6},none of the problems were solved within the time limit. Many first-order beliefs had been generated (Table 2), andmore beliefs were being generated. For (cid:10) as large as 0.8, the algorithm terminated after generating a tractable number476H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490Table 2Problems for which the algorithm did not terminate or terminated but failed to find a satisfactory solutionTestproblem|S||A||Θ|# beliefs (×104)generated within 48 h(cid:10) = 0.01250.1Tag-AvoidCycle103leg10a Shown with (time elapsed [s], normalized reward loss of the policy found).870128012803.22.92.93022521213.12.82.80.42.02.82.80.61.41.41.6# beliefs (×100)when terminateda0.8439 (759, 2.1)33 (23, 0.41)127 (76, 0.42)of beliefs (Table 2) for every problem. However, the policies found were unsatisfactory; the normalized reward loss,which we calculated in the same manner as in Fig. 10, was 0.41 or larger.7. ConclusionIn this paper, we formulated POMDPIPs and their quasi-optimal policies, and provided an efficient algorithm toobtain these policies. We also provided a theoretical bound on the reward losses of the quasi-optimal policies and thecomputational complexity of the algorithm. Empirical studies showed that the algorithm can find nontrivial policiesin a reasonable time for many POMDPIPs.There are several directions for future research.First, characteristics of the quasi-optimal policy can be studied more deeply. For example, derivation of tightertheoretical bounds may be investigated. Also, detailed comparisons between the quasi-optimal policies and the E-admissible policies remain to be performed (Section 6.1).Second, robustness of the policy can be pursued further. In this paper, we did not attempt to obtain the mostrobust policies possible. For some problems, the quasi-optimal policies were relatively less robust (Section 6.1).It would be desirable if we could obtain the robust policies, e.g., using the maximin approach, within a reasonabletime. Recently, Nilim and El Ghaoui proposed handling imprecision by using likelihood-bounded sets (as opposed tointervals) in order to efficiently obtain robust policies in MDPs [39,40]. It would be interesting to investigate POMDPsas well.Third, other types of the solution algorithm can be considered. Although our algorithm solved many POMDPIPs,it failed (Section 6.3) to solve the problems for which recent POMDP algorithms are able to find good (althoughapproximate) policies [43,45,47]. Our algorithm directly constructs grid-based belief state MDPs. It can suffer fromexponential growth of the number of first-order beliefs, as in the other grid-based approaches [7,28,59]. Other ap-proaches, e.g., those based on α-vectors [24,44,46,47,51–53,57], may offer reduced solution times for the classes ofPOMDPIPs for which our algorithm is slow.Last, we note that our algorithm can also be used as an approximate planning method for large-sized POMDPs.Even when parameters are given precisely, we can choose to introduce a parameter imprecision, balancing the gains inreduction in solution time against the reduction in accuracy. The empirical results (Section 6) are encouraging becausethey suggest that satisfactory policies will be obtained within a certain range of the parameter imprecision. Note thatthe idea that the parameter imprecision can be exploited to reduce the computational complexity of solving POMDPsis orthogonal to other approximation methods (reviewed in Section 1), i.e., every combination of this idea and otherapproximation methods can be pursued.AcknowledgementsWe thank Minseok Kim, Shigemi Sawa, and the reviewers for their valuable comments. We also thank AnthonyR. Cassandra and Joelle Pineau for making the codes of the POMDP problems available. We thank Matthijs Spaanfor making the code of the Perseus algorithm available. This work is supported in part by the Grant-in-Aid for YoungScientists (B-15700180) from the Ministry of Education, Culture, Sports, Science and Technology of Japan.H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490477Appendix A. An example in which using multiple second-order beliefs leads to a more robust policy thanusing a single second-order beliefLet us consider a very simple POMDPIP problem. Suppose that there are four states, S = {s1, s2, s3, s4}, twoactions, A = {a1, a2}, and two observations, Θ = {o1, o2}. Suppose p0 = (1, 0, 0, 0), i.e., the process starts withs1. Let us consider the point-set case. Let T M be T M (s, a) = {(0, 0, 0, 1)} for all s and a, except T M (s1, a1) ={(0, 0.4, 0.6, 0), (0, 0.6, 0.4, 0)}. That is, the state always changes to s4 no matter which action the agent takes, exceptthat it changes to s2 or s3 with the imprecise probability if the agent takes action a1 in state s1. Let OM be OM (s(cid:6), a)= {(0.5, 0.5)} for all s(cid:6) and a; that is, the state is completely unobservable. Let R be R(s, a, s(cid:6)) = 0 for all s, a, ands(cid:6), except that R(s, a, s(cid:6)) = 1 for s = s2, a = a1, and s(cid:6) = s4, and that R(s, a, s(cid:6)) = 1 for s = s3, a = a2, and s(cid:6) = s4.That is, a reward is gained only when the agent takes action a1 in state s2 or when the agent takes action a2 in state s3.For this POMDPIP problem, let us consider the history tree shown in Fig. 4. The possibly-correct first-orderbeliefs are (1) ˆb∅ = p0 = (1, 0, 0, 0), (2) ˆb11, which is the belief after taking action a1 and observing o1, equals aconvex combination of (0, 0.4, 0.6, 0) and (0, 0.6, 0.4, 0), (3) ˆb12 also equals a convex combination of (0, 0.4, 0.6, 0)and (0, 0.6, 0.4, 0), and (4) (0, 0, 0, 1) otherwise. Note that ˆb11 and ˆb12 are calculated depending on the second-orderbeliefs ˆbM∅,a1,o2 , respectively. Thus, if only a single second-order belief is allowed, ˆb11 and ˆb12 should beidentical. If multiple second-order beliefs are allowed, ˆb11 and ˆb12 can be different.∅,a1,o1 and ˆbMNow, let us consider which action is to be taken after each action-observation history. Since a reward can be gainedonly when the state is in s2 or s3, the initial action (i.e., the action after history ∅) should be a1. For the same reason,the actions after two actions (i.e., actions after ˆb1111, ˆb1112, and so on) do not affect the total reward. Thus, the onlyactions to be optimized are those immediately after ˆb11 and ˆb12.Let us consider the action after ˆb11. Recall that a unit reward is gained for action a1 in state s2 or action a2 in states3. The estimated rewards for actions a1 and a2 depend on the value of ˆb11, as shown in Fig. A.1. For example, ifˆb11(s2) = 0.4 (which means ˆb11 = (0, 0.4, 0.6, 0)), then the estimated reward is 0.4 for action a1 and 0.6 for actiona2; hence action a2 should be taken.The same holds for the action after ˆb12. Thus, if only a single second-order belief is allowed, then ˆb11 and ˆb12 areidentical, and hence the actions after ˆb11 and ˆb12 are identical. Let us suppose, for example, that ˆb11(s2) = ˆb11(s3) =0.4. Then action a2 is selected both after ˆb11 and ˆb12.On the other hand, if multiple second-order beliefs are allowed, and if ˆb11 and ˆb12 are different, then the actionsafter ˆb11 and ˆb12 can be different. Let us suppose, for example, that the agent adopts ˆb11(s2) = 0.4 and ˆb12(s2) = 0.6.Then action a2 is selected after ˆb11 and action a1 is selected after ˆb12. Since both ˆb11 and ˆb12 are reached withprobability 0.5, this agent selects action a1 or a2 with probability 0.5.Now, suppose that the correct second-order beliefs are given (as in Section 3.2). Depending on their values, theexpected total reward varies from 0.4γ to 0.6γ for the agent who always selects a2 (where γ is the discount factor).On the other hand, the expected reward is always 0.5γ for the agent who selects a1 or a2 with probability 0.5.Fig. A.1. Reward expected by taking action a1 or a2 after ˆb11.478H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490Thus, although the agent who adopts a single second-order belief gains more reward than the agent with multiplebeliefs if the belief adopted is correct, the worst case performance can be better in the agent with multiple beliefs. Inthis sense, the use of multiple beliefs can lead to a more robust policy.Note that in other parts of this paper, we use the term “robust” with a slightly different meaning; i.e., we regarda policy to be robust if the reward loss incurred by using the policy, instead of using the optimal policy for eachenvironment, is small for various environments (Section 6). The conclusion in Appendix A is not changed by adoptingthis meaning because the maximum reward loss is 0.2γ for the agent who always selects a2 and 0.1γ for the agentwho selects a1 or a2 with probability 0.5.Note that we do not argue that the use of multiple beliefs is always better. Note also that, in our algorithm, wedo not try to find a policy that is as robust as possible. The example in this section suggests the use of randomizedpolicies (i.e., stochastic action-selection rules) for robustness. Finding such robust policies is beyond the scope of thepresent paper. The only purpose of introducing this simple example is to show that it is not always advantageous touse a single second-order belief compared to the use of multiple second-order beliefs.Appendix B. Derivation of the IS-FEASIBLE procedureFirst, we consider the interval case. In the IS-FEASIBLE procedure, the query that we want to answer as true orfalse is formalized as follows:Q1: There exists ˆT (s, a, s(cid:6)) ∈ R for each s and s(cid:6) ∈ S, ˆO(s(cid:6), a, o) ∈ R for each s(cid:6) ∈ S, and Z ∈ R, s.t.Z > 0,(cid:5)Z =s,s(cid:6)∈Sˆb(s) ˆT (s, a, s(cid:6)) ˆO(s(cid:6), a, o),(cid:6)) (cid:2) ˆT (s, a, s(cid:6)) (cid:2) T (s, a, s(cid:6))for all s, s(cid:6) ∈ S,T (s, a, s(cid:5)ˆT (s, a, s(cid:6)) = 1 for all s ∈ S,s(cid:6)∈SO(s(cid:5)(cid:6)(cid:6), a, o) (cid:2) ˆO(sˆb(s) ˆT (s, a, s(cid:6), a, o) (cid:2) O(s) ˆO(s, a, o)/Z = ˆb(cid:6)(cid:6), a, o)for all s(cid:6)(cid:6))(sfor all s(cid:6) ∈ S,(cid:6) ∈ S(B.1)(B.2)(B.3)(B.4)(B.5)(B.6)s∈Shold.There are some non-linear equations in this query. In the following, we transform this query into an equivalentquery that consists of linear equations only.To begin with, in Q1, Eq. (B.2) is unnecessary because we already have Eqs. (B.1) and (B.6) and(cid:3)s(cid:6)∈Sˆb(cid:6)(s(cid:6)) = 1holds by construction. Consequently, the query Q1 is equivalent to:Q2: There exists ˆT (s, a, s(cid:6)) ∈ R for each s and s(cid:6) ∈ S, ˆO(s(cid:6), a, o) ∈ R for each s(cid:6) ∈ S, and Z ∈ R, s.t. Eqs. (B.1),(B.3)–(B.6) hold.Q1 is equivalent to Q2, since Q1 is true (or false) when Q2 is true (or false), respectively.Next, let us divide S into mutually exclusive sets S1, S2, S3, and S4, as:(cid:6), a, o) = 0, s(cid:6) ∈ S(cid:12),(cid:11)s(cid:11)s(cid:11)s(cid:11)s(cid:6)(cid:6)(cid:6)(cid:6)(cid:6) | ˆb(cid:6) | ˆb(cid:6) | ˆb(cid:6) | ˆb(s(s(s(s(cid:6)(cid:6)(cid:6)(cid:6)S1 :=S2 :=S3 :=S4 :=) (cid:11)= 0, O(s(cid:6)) (cid:11)= 0, s, a, o) = 0, O(s(cid:12),(cid:6) ∈ S − S1(cid:6)) = 0, O(s, a, o) = 0, s) = 0, O(s(cid:6), a, o) (cid:11)= 0, s(cid:6) ∈ S(cid:6) ∈ S(cid:12),(cid:12).Note that the sets S1, S2, S3, and S4 form a partition of S. It is straightforward to see that Q2 is equivalent to:Q3: There exists ˆT (s, a, s(cid:6)) ∈ R for each s and s(cid:6) ∈ S, ˆO(s(cid:6), a, o) ∈ R for each s(cid:6) ∈ S, and Z ∈ R, s.t.Eqs. (B.1), (B.3), (B.4),H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490ˆO(s(cid:5)(cid:6), a, o) = 0 for all sˆb(s) ˆT (s, a, s) ˆO(s(cid:6) ∈ S1,, a, o)/Z = ˆb(cid:6)(cid:6)s∈S(cid:6)(cid:6)(s) (cid:11)= 0for all s(cid:6) ∈ S1,O(s(cid:5)(cid:6)(cid:6), a, o) (cid:2) ˆO(sˆb(s) ˆT (s, a, s(cid:6), a, o) (cid:2) O(s) ˆO(s, a, o)/Z = ˆb(cid:6)(cid:6), a, o) (cid:11)= 0 for all s(cid:6)(cid:6))(sfor all s(cid:6) ∈ S2,(cid:6) ∈ S2,s∈S0 (cid:2) ˆO(s(cid:5)(cid:6), a, o) (cid:2) O(s) ˆO(sˆb(s) ˆT (s, a, s(cid:6)(cid:6), a, o)for all s(cid:6) ∈ S3,(cid:6), a, o)/Z = 0for all s(cid:6) ∈ S3,s∈Ss∈S(cid:6)0 < O(s(cid:5), a, o) (cid:2) ˆO(s) ˆO(s(cid:6)ˆb(s) ˆT (s, a, s(cid:6), a, o) (cid:2) O(s(cid:6), a, o)(cid:6), a, o)/Z = 0for all s(cid:6) ∈ S4,for all s(cid:6) ∈ S4479(B.7)(B.8)(B.9)(B.10)(B.11)(B.12)(B.13)(B.14)hold.Note that, in Eq. (B.9), we have O(s(cid:6), a, o) (cid:11)= 0, since, if O(s(cid:6), a, o) = 0, then, from 0 (cid:2) O(s(cid:6), a, o) (cid:2) O(s(cid:6), a, o),it is concluded that O(s(cid:6), a, o) = O(s(cid:6), a, o) = 0, which means that s(cid:6) is in S1, not in S2.Next, we prove that Q3 is equivalent to:Q4: S1 is an empty set, and there exists ˆT (s, a, s(cid:6)) ∈ R for each s and s(cid:6) ∈ S, ˆO(s(cid:6), a, o) ∈ R for each s(cid:6) ∈ S, andZ ∈ R, s.t.(cid:6)Eqs. (B.1), (B.3), (B.4),, a, o) (cid:2) ˆO(sˆb(s) ˆT (s, a, s(cid:6)O(s(cid:5), a, o) (cid:2) O(s) ˆO(s(cid:6)(cid:6)(cid:6), a, o) (cid:11)= 0 for all s, a, o)/Z = ˆb(cid:6)(cid:6))(sfor all s(cid:6) ∈ S2,(cid:6) ∈ S2,s∈SˆO(s(cid:5)(cid:6), a, o) > 0 for all sˆb(s) ˆT (s, a, s(cid:6) ∈ S2,) = 0 for all s(cid:6)(cid:6) ∈ S4s∈Shold.To prove that Q3 is equivalent to Q4, check that Q4 is true if Q3 is true, and that Q3 is true if Q4 is true. Tocheck the former, note the followings: For s(cid:6) ∈ S1, Eqs. (B.7) and (B.8) can never be satisfied. For s(cid:6) ∈ S2, Eqs. (B.9)ˆO(s(cid:6), a, o) > 0 holds. For s(cid:6) ∈ S4, Eqs. (B.13) and (B.14) are satisfied only ifand (B.10) can be satisfied only if(cid:3)ˆb(s) ˆT (s, a, s(cid:6)) = 0.s∈STo check the latter, note the followings: For s(cid:6) ∈ S3, Eqs. (B.11) and (B.12) can always be satisfied by setting(cid:6)ˆO(s, a, o) = 0,which is always possible by the definition of S3. For s(cid:6) ∈ S4, Eq. (B.13) can always be satisfied by setting(cid:6)ˆO(s, a, o) arbitrarily s.t. O(s(cid:6), a, o) (cid:2) ˆO(s(cid:6), a, o) (cid:2) O(s(cid:6), a, o),(B.15)(B.16)which is always possible by definition.Next, let us define q(s(cid:6)) = Z/ ˆO(s(cid:6), a, o) for all s(cid:6) ∈ S2. Now Q4 is equivalent to:Q5: S1 is an empty set, and there exists ˆT (s, a, s(cid:6)) ∈ R for each s and s(cid:6) ∈ S, q(s(cid:6)) ∈ R for each s(cid:6) ∈ S2, and Z ∈ R,s.t.Eqs. (B.1), (B.3), (B.4),ZO(s(cid:6), a, o)(cid:3) q(s(cid:6)) (cid:3)ZO(s(cid:6), a, o)for all s(cid:6) ∈ S2,(B.17)480H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490(cid:5)s∈Sˆb(s) ˆT (s, a, s(cid:6)) = ˆb(cid:6)(s(cid:6))q(s(cid:6))for all s(cid:6) ∈ S2,(cid:6)q(s(cid:5)) < ∞ for all sˆb(s) ˆT (s, a, s(cid:6) ∈ S2,) = 0 for all s(cid:6)(cid:6) ∈ S4s∈S(B.18)(B.19)(B.20)ZO(s(cid:6),a,o)= ∞ when O(s(cid:6), a, o) = 0 in Eq. (B.17). Note thatZO(s(cid:6),a,o)(cid:11)= ∞ since O(s(cid:6), a, o) (cid:11)= 0hold, where we definefor any s(cid:6) ∈ S2.In Q5, Eq. (B.19) is unnecessary, since we have Eq. (B.18) in which the left-hand side is finite and ˆb(cid:6)(s(cid:6)) (cid:11)= 0 bydefinition of S2. Thus, finally, we have that Q5 is equivalent to:Q6: S1 is an empty set, and there exists ˆT (s, a, s(cid:6)) ∈ R for each s and s(cid:6) ∈ S, q(s(cid:6)) ∈ R for each s(cid:6) ∈ S2, and Z ∈ R,s.t. Eqs. (B.1), (B.3), (B.4), (B.17), (B.18), and (B.20) hold.Therefore, if S1, S2, and S4 are empty (which is often the case), we obtain the IS-FEASIBLE procedure in Fig. 6.Otherwise, we need a little more (but almost negligible) computational cost to answer the Q6 correctly; we need to testif S1 is empty and to include Eq. (B.20) as a constraint. We also need to use Eqs. (B.15) and (B.16) to set ˆO(s(cid:6), a, o)for s(cid:6) ∈ S3 and S4, respectively. Still, note that we can easily answer Q6, since all the constraints are linear.We can consider the point-set case in a similar manner. The query that we want to answer as true or false isformalized as follows:Q7: There exists ˆT (s, a, s(cid:6)) ∈ R for each s and s(cid:6) ∈ S, ˆO(s(cid:6), a, o) ∈ R for each s(cid:6) ∈ S, λiss(cid:6) ∈ [0, 1] for all s(cid:6) ∈ S and i = 1, . . . , |OM (s(cid:6), a)|, and Z ∈ R, s.t.i = 1, . . . , |T M (s, a)|, νiZ > 0,(cid:5)Z =ˆb(s) ˆT (s, a, s(cid:6)) ˆO(s(cid:6), a, o),s,s(cid:6)∈ST (s, a, s(cid:6)) =(cid:5)isT Mλii(s, a, s(cid:6))for all s, s(cid:6) ∈ S,λis= 1 for all s ∈ S,O(s(cid:6), a, o) =(cid:5)is(cid:6)OMνii (s(cid:6), a, o)for all s(cid:6) ∈ S,νis(cid:6) = 1 for all s(cid:6) ∈ S,ˆb(s) ˆT (s, a, s(cid:6)) ˆO(s(cid:6), a, o)/Z = ˆb(cid:6)(cid:6))(sfor all s(cid:6) ∈ S(cid:5)i(cid:5)i(cid:5)s∈S∈ [0, 1] for all s ∈ S and(B.21)(B.22)hold.The constraints, Eqs. (B.21) and (B.22) with νis(cid:6) ∈ [0, 1], are equivalent to Eq. (B.5) (note that a and o are fixedhere). After replacing them with Eq. (B.5), we can obtain the final form in Fig. 6 in the same way as the interval casedescribed above.Appendix C. Proof of Theorem 1Here, we provide the proof of Theorem 1. This proof is based on McAllester et al. [37].The proof strategy is as follows. Note that Theorem 1 gives a bound on the error of the optimal value that can occurby using possibly-correct second-order beliefs instead of the correct ones. To prove Theorem 1, we will bound variouserrors that can occur by using possibly-correct second-order beliefs instead of the correct ones. After proving somebasic properties of the norm and the probability functions in Lemmas 1–4, we first bound the error of the first-orderbelief after an action and an observation in Lemma 5. Next, using Lemma 5, we bound the error of the first-order beliefH. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490481after t steps in Lemma 6. Subsequently, using Lemma 6, we bound the error of the value of a policy in Lemma 7.Finally, using Lemma 7, we bound the error of the optimal value to prove Theorem 1.We begin by providing basic lemmas. Let (cid:10) · (cid:10) denote the L1 norm; that is, for any probability function P (s), let(cid:10)P (s)(cid:10) :=|P (s)|. For this norm, we have the following lemmas:(cid:3)sLemma 1. Let P and Q be two probability functions on the same set. We have(cid:17)(cid:17)(cid:17) (cid:2) 2.(cid:17)P (s) − Q(s)Proof.(cid:17)(cid:17)(cid:17) =(cid:17)P (s) − Q(s)(cid:5)(cid:16)(cid:16)(cid:16) (cid:2)(cid:16)P (s) − Q(s)(cid:5)P (s) +ss(cid:5)sQ(s) = 2.(cid:2)Lemma 2. (Modified from Lemma 15 in [37].) Let X and O be two sets. Let P and Q be any two probability functionson X × O. Let P (o) denote the marginal probability function on O, i.e., P (o) =x P (x, o), and similarly for Q(o).Let P (x|o) be the conditional probability function on X, i.e., P (x|o) = P (x, o)/P (o) if P (o) (cid:11)= 0. Let P (x|o) be anarbitrary probability function if P (o) = 0. Similarly for Q(x|o). We then have the following.(cid:17)(cid:17)(cid:17) +(cid:17)P (x, o) − Q(x, o)(cid:17)(cid:17)(cid:17) (cid:2)(cid:17)P (x|o) − Q(x|o)(cid:17)(cid:17)(cid:17).(cid:17)P (o) − Q(o)(cid:3)Eo∼P (o)Proof. Let O+ be the set of o for which P (o) (cid:11)= 0 holds. First, we prove that for any o ∈ O+, we have(cid:16)(cid:16)(cid:16).(cid:16)P (o) − Q(o)(C.1)P (o)(cid:5)x∈XQ(x, o)P (o)(cid:16)(cid:16)(cid:16)(cid:16) =− Q(x|o)When Q(o) (cid:11)= 0, Eq. (C.1) holds becauseP (o)(cid:5)x∈XQ(x, o)P (o)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)− Q(x, o)Q(o)(cid:16)(cid:16)(cid:16)(cid:16)(cid:5)Q(x, o)P (o)(cid:16)(cid:16)(cid:16)(cid:16) = P (o)− Q(x|o)x∈X(cid:16)(cid:16)1(cid:16)= P (o)(cid:16)Q(o)P (o)(cid:16)(cid:16)(cid:16).(cid:16)P (o) − Q(o)− 1=(cid:16)(cid:16)(cid:16)(cid:16)(cid:5)x∈X(cid:16)(cid:16)(cid:16)Q(x, o) = P (o)(cid:16)1P (o)− 1Q(o)(cid:16)(cid:16)(cid:16)(cid:16)Q(o)When Q(o) = 0, Eq. (C.1) holds because(cid:16)(cid:16)(cid:16)(cid:16)P (o)(cid:5)x∈XQ(x, o)P (o)(cid:16)(cid:16)(cid:16)(cid:16) = P (o)− Q(x|o)(cid:5)(cid:16)(cid:16)(cid:16)(cid:16)Q(x|o)(∵ Q(x, o) = 0 when Q(o) = 0)x∈X(cid:16)(cid:16)(cid:16).(cid:16)P (o) − Q(o)(∵ Q(o) = 0)= P (o) =Therefore, Eq. (C.1) holds for any o ∈ O+.Next, we prove the lemma as follows:(cid:5)(cid:17)(cid:17)(cid:17) =(cid:17)P (x|o) − Q(x|o)Eo∼P (o)P (o)(cid:5)o∈O+(cid:5)x∈X(cid:5)(cid:16)(cid:16)(cid:16)(cid:16)P (x, o)/P (o) − Q(x|o)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16) +(cid:5)(cid:5)(cid:5)(cid:16)(cid:16)(cid:16)(cid:16)P (o)(cid:2)=(cid:2)(cid:5)o∈O+P (o)− Q(x, o)P (x, o)P (o)P (o)x∈X(cid:16)(cid:16)(cid:16) +(cid:16)P (x, o) − Q(x, o)o∈O+,x∈X(cid:17)(cid:17)(cid:17) +(cid:17)P (x, o) − Q(x, o)o∈O+x∈X(cid:16)(cid:16)(cid:16)(cid:16)P (o) − Q(o)o∈O+(cid:17)(cid:17)(cid:17).(cid:17)P (o) − Q(o)(cid:2)(cid:16)(cid:16)(cid:16)− Q(x|o)(cid:16)Q(x, o)P (o)(∵ Eq. (C.1))Next, let us consider the hypothetical POMDP in which the correct second-order beliefs are specified (Section 3.2).For later use, let Pr(a|h, μ) denote the probability of taking action a ∈ A, given history h ∈ H and policy μ. Define482H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490(cid:3)P (s(cid:6), h(cid:6)|h, s, t, μ) as the probability that action-observation sequence h(cid:6) is generated and the final state is s(cid:6), giventhat time t has elapsed after history h had ended with state s and that policy μ is used. For a first-order belief b, defineP (s(cid:6), h(cid:6)|h, b, t, μ) :=s∈S b(s)P (s(cid:6), h(cid:6)|h, s, t, μ). Note that h(cid:6) is an action-observation sequence whose length is t;to denote this, we say h(cid:6) ∈ Ht , with a little abuse of notation.(cid:3)Let us define P (h(cid:6)|h, b, t, μ) as P (h(cid:6)|h, b, t, μ) :=s(cid:6)∈S P (s(cid:6), h(cid:6)|h, b, t, μ). Also, define P (s(cid:6)|h(cid:6), h, b, t, μ)as P (s(cid:6)|h(cid:6), h, b, t, μ) := P (s(cid:6), h(cid:6)|h, b, t, μ)/P (h(cid:6)|h, b, t, μ) if P (h(cid:6)|h, b, t, μ) (cid:11)= 0. If P (h(cid:6)|h, b, t, μ) = 0,letP (s(cid:6)|h(cid:6), h, b, t, μ) be an arbitrary probability function on S. We denote P (s(cid:6)|h(cid:6), h, b, t, μ) by P (s(cid:6)|h(cid:6), h, b) for short,since t and μ are redundant, given h(cid:6), h, and b. For these functions, we have the following bounds:Lemma 3. Let b and ˆb be any first-order beliefs, t be any elapsed time, μ be any policy, and h be any history. Thenwe have(cid:17)(cid:17)P (h(cid:17)(cid:17) (cid:2) (cid:10)b − ˆb(cid:10).(cid:6)|h, ˆb, t, μ)(cid:6)|h, b, t, μ) − P (hProof.(cid:17)(cid:17)P (h(cid:6)|h, b, t, μ) − P (h(cid:17)(cid:17) =(cid:6)|h, ˆb, t, μ)(cid:5)(cid:5)(cid:16)(cid:16)(cid:16)(cid:16)P (h(cid:6)|h, s, t, μ)b(s) −(cid:5)(cid:16)(cid:16)(cid:16)(cid:6)|h, s, t, μ) ˆb(s)(cid:16)P (hh(cid:6)∈Ht(cid:5)s∈S(cid:5)s∈S(cid:16)(cid:16)(cid:16) = (cid:10)b − ˆb(cid:10).(cid:16)b(s) − ˆb(s)(cid:6)|h, s, t, μ)P (h(cid:2)(cid:2)h(cid:6)∈Hts∈SLemma 4. Let b and ˆb be any first-order beliefs, t be any elapsed time, μ be any policy, and h be any history. Thenwe have(cid:5)(cid:17)(cid:17)P (s(cid:6)|h, b, t, μ)P (h(cid:6)(cid:6)|h, h, b) − P (s(cid:6)(cid:6)|h(cid:17)(cid:17) (cid:2) 2(cid:10)b − ˆb(cid:10)., h, ˆb)h(cid:6)∈HtProof.(cid:5)(cid:17)(cid:17)P (s(cid:6)|h, b, t, μ)P (h(cid:6)(cid:6)|h, h, b) − P (s(cid:6)(cid:6)|h(cid:17)(cid:17), h, ˆb)h(cid:6)∈Ht(cid:2)(cid:6)(cid:17)(cid:17)P (s, h(cid:17)(cid:17)P (h+(cid:2) (cid:10)b − ˆb(cid:10)(cid:6)(cid:6)|h, b, t, μ) − P (s(cid:6)|h, b, t, μ) − P (h(cid:17)(cid:17)(cid:6)|h, ˆb, t, μ), h(cid:17)(cid:17) (∵ Lemma 2)(cid:6)|h, ˆb, t, μ)(∵ proof is similar to Lemma 3)+ (cid:10)b − ˆb(cid:10)= 2(cid:10)b − ˆb(cid:10).(∵ Lemma 3)(cid:2)Next, we prove the bounds on the errors of the possibly-correct first-order beliefs in Section 3.3, compared to thecorrect beliefs of the hypothetical POMDPs. In the following, suppose that the possibly-correct second-order beliefsˆbMh,a,o and ˆbMh have been determined for every h ∈ H , a ∈ A, and o ∈ Θ. First, we provide a bound on the error thatmay be caused by a one-step update of a first-order belief.Lemma 5. Let b be any first-order belief. Let h be any history. Let b(cid:6) and ˆb(cid:6) be the correct and possibly-correctbeliefs which are Bayes-updated from b, i.e., b(cid:6) := τ (b, a, o, bMh,a,o), respectively. Let b(cid:6) be anarbitrary probability function on S if τ (b, a, o, bMh ) cannot be calculated (i.e., if the denominator is zero in Eq. (14)).Similarly for ˆb(cid:6). Define d as Eq. (43). Then we haveh ) and ˆb(cid:6) := τ (b, a, o, ˆbM(cid:5)(cid:8)(cid:8)a, o(cid:9)|h, b, 1, μ(cid:9)(cid:10)bP(cid:6) − ˆb(cid:6)(cid:10) (cid:2) 4d.a∈A,o∈ΘH. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490483Proof. First, define Q(s(cid:6), o|h, b, a) and Q(o|h, b, a) as(cid:5)(cid:13)(cid:6), o|h, b, a) :=Q(sandQ(o|h, b, a) :=Then we havemh=(Th,Oh)∈M(cid:5)s(cid:6)∈S(cid:6)Q(s, o|h, b, a).(cid:6)Oh(s, a, o)Th(s, a, ss∈S(cid:6))b(s)bMh (mh) dmh(cid:6)b(s) = Q(s(cid:6), o|h, b, a)Q(o|h, b, a)for all s ∈ Swhen Q(o|h, b, a) (cid:11)= 0. Similarly, define ˆQ(s(cid:6), o|h, b, a) and ˆQ(o|h, b, a) as(cid:13)(cid:6)ˆQ(s, o|h, b, a) :=mh=(Th,Oh)∈M(cid:6)Oh(s, a, o)(cid:5)s∈STh(s, a, s(cid:6))b(s) ˆbMh,a,o(mh) dmhandˆQ(o|h, b, a) :=(cid:5)s(cid:6)∈S(cid:6)ˆQ(s, o|h, b, a).Then we haveˆb(cid:6) =ˆQ(s(cid:6), o|h, b, a)ˆQ(o|h, b, a)for all s ∈ Swhen ˆQ(o|h, b, a) (cid:11)= 0. Next, define their differences as(cid:6)(cid:6)(cid:6), o|h, b, a) − ˆQ(s, o|h, b, a)(cid:10)(s, o|h, b, a) := Q(s(C.2)(C.3)and(cid:10)(o|h, b, a) := Q(o|h, b, a) − ˆQ(o|h, b, a).Note that(cid:5)a∈A, o∈Θ(cid:8)(cid:8)a, o(cid:9)|h, b, 1, μ(cid:9)(cid:10)bP(cid:6) − ˆb(cid:6)(cid:10) =(cid:5)a∈APr(a|h, μ)(cid:5)o∈ΘQ(o|h, b, a)(cid:10)b(cid:6) − ˆb(cid:6)(cid:10)(C.4)holds. For any h ∈ H , first-order belief b, and a ∈ A, if o ∈ Θ satisfies Q(o|h, b, a) (cid:2) 2|(cid:10)(o|h, b, a)|, then we haveQ(o|h, b, a)(cid:10)b(cid:6) − ˆb(cid:6)(cid:10) (cid:2) 4(cid:2) 4(cid:16)(cid:16)(cid:16)(cid:16)(cid:10)(o|h, b, a)(cid:5)(cid:16)(cid:16)(cid:10)(s(cid:6)(cid:16)(cid:16)., o|h, b, a)(∵ Lemma 1 and assumption)(C.5)s(cid:6)∈SIf o ∈ Θ satisfies Q(o|h, b, a) > 2|(cid:10)(o|h, b, a)|, then Q(o|h, b, a) (cid:11)= 0 and ˆQ(o|h, b, a) (cid:11)= 0 should hold (proof bycontradiction), and hence we haveQ(o|h, b, a)(cid:10)b(cid:6) − ˆb(cid:6)(cid:10) = Q(o|h, b, a)= Q(o|h, b, a)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:16)(cid:5)s(cid:6)∈S(cid:5)s(cid:6)∈SQ(s(cid:6), o|h, b, a)Q(o|h, b, a)Q(s(cid:6), o|h, b, a)Q(o|h, b, a)−ˆQ(s(cid:6), o|h, b, a)ˆQ(o|h, b, a)(cid:16)(cid:16)(cid:16)(cid:16)− Q(s(cid:6), o|h, b, a) + (cid:10)(s(cid:6), o|h, b, a)Q(o|h, b, a) + (cid:10)(o|h, b, a)(cid:16)(cid:16)(cid:16)(cid:16)(∵ Eqs. (C.2) and (C.3))=(cid:16)(cid:16)(cid:16)(cid:16)(cid:5)s(cid:6)∈SQ(s(cid:6), o|h, b, a)(cid:10)(o|h, b, a) − Q(o|h, b, a)(cid:10)(s(cid:6), o|h, b, a)Q(o|h, b, a) + (cid:10)(o|h, b, a)(cid:16)(cid:16)(cid:16)(cid:16)484H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490(cid:5)s(cid:6)∈S(cid:5)< 2= 2(cid:16)(cid:16)(cid:16)(cid:16)Q(s(cid:6), o|h, b, a)(cid:10)(o|h, b, a) − Q(o|h, b, a)(cid:10)(s(cid:6), o|h, b, a)Q(o|h, b, a)(cid:16)(cid:16), o|h, b, a))(cid:10)(o|h, b, a) − (cid:10)(s(s(cid:6)(cid:6)(cid:6)(cid:16)(cid:16)b(cid:16)(cid:16)(cid:16)(cid:16)(∵ assumption)s(cid:6)∈S(cid:16)(cid:16)(cid:16) + 2(cid:16)(cid:10)(o|h, b, a)(cid:2) 2(cid:16)(cid:16)(cid:10)(s(cid:6)(cid:16)(cid:16) (cid:2) 4, o|h, b, a)(cid:5)s(cid:6)∈S(cid:16)(cid:16)(cid:10)(s(cid:6)(cid:16)(cid:16)., o|h, b, a)(cid:5)s(cid:6)∈SThus, together with Eq. (C.5), we have(cid:6) − ˆbQ(o|h, b, a)(cid:10)b(cid:6)(cid:10) (cid:2) 4(cid:16)(cid:16)(cid:10)(s(cid:5)(cid:6)(cid:16)(cid:16), o|h, b, a)for any h ∈ H , first-order belief b, a ∈ A, and o ∈ Θ.s(cid:6)∈SNote that we have(cid:5)(cid:5)(cid:16)(cid:16)(cid:10)(s(cid:6)o∈Θs(cid:6)∈S(cid:5)(cid:5)=s(cid:6)∈So∈Θ−(cid:13)(cid:16)(cid:16) =, o|h, b, a)(cid:16)(cid:16)(cid:16)(cid:16)mh=(Th,Oh)∈M(cid:13)(cid:5)(cid:5)(cid:16)(cid:16)Q(so∈Θs(cid:6)∈S(cid:6)Oh(s, a, o)(cid:5)s∈S(cid:6), o|h, b, a) − ˆQ(s(cid:6)(cid:16)(cid:16), o|h, b, a)Th(s, a, s(cid:6))b(s)bMh (mh) dmh(cid:6)Oh(s, a, o)(cid:5)s∈STh(s, a, s(cid:6))b(s) ˆbMh,a,o(mh) dmh(cid:16)(cid:16)(cid:16)(cid:16)mh=(Th,Oh)∈M(cid:5)(cid:5)(cid:5)b(s)max(T 1,O1),(T 2,O2)∈Mo∈Θ(cid:5)s(cid:6)∈S(cid:5)s∈S(cid:5)(cid:16)(cid:16)O1(s(cid:6), a, o)T 1(s, a, s(cid:6)) − O2(s(cid:6), a, o)T 2(s, a, s(cid:6)(cid:16)(cid:16))(cid:16)(cid:16)(cid:8)O0(s(cid:6), a, o) + (cid:10)1O (s(cid:6)(cid:9)(cid:8), a, o)T 0(s, a, sb(s)(cid:8)max(cid:9),(T 0+(cid:10)2(cid:9)(cid:8)T 0+(cid:10)1T ,O0+(cid:10)1Os(cid:6)∈So∈Θ(cid:8)O0(s−s∈S(cid:6), a, o) + (cid:10)2O )∈M) + (cid:10)2(where we let (T 0, O0) be an arbitrary model in M)(cid:16)(cid:16)O0(s=T 0(s, a, sT ,O0+(cid:10)2, a, o)O (sb(s)(cid:5)(cid:5)(cid:5)T (s, a, s(cid:6)(cid:6)(cid:6)(cid:9)(cid:16)(cid:16))s(cid:6)∈So∈Θs∈S+ T 0(s, a, s(cid:5)(cid:5)(cid:5)(T 0+(cid:10)1(cid:6)(cid:6)(cid:8)(cid:10)1O (s)(cid:8)O0(sb(s)(cid:9)∈Mmax(cid:8)T ,O0+(cid:10)1T 0+(cid:10)2O ),, a, o) − (cid:10)2, a, o)(cid:10)TO (sT ,O0+(cid:10)2O(cid:9)+ (cid:10)1O (s, a, o)(cid:6))(cid:10)O+ T 0(s, a, s(cid:6)(cid:6)(cid:6)max(cid:6)(cid:8)(cid:10)1T (s, a, s, a, o)(cid:6)) − (cid:10)2T (s, a, s(cid:6)(cid:9))(cid:6), a, o)(cid:10)1+ (cid:10)TT (s, a, smax(cid:10)Omax) − (cid:10)2O (smax(cid:10)Omax+ (cid:10)Tmax(cid:6), a, o)(cid:10)2(cid:9)T (s, a, s(cid:2)=(cid:2)s(cid:6)∈So∈Θ= |S|(cid:10)Ts∈S+ |Θ|(cid:10)Omax(cid:10)OmaxThe desired result is obtained by combining this result with Eqs. (C.4) and (C.6). (cid:2)+ 2|S(cid:10)Θ|(cid:10)T:= d.maxmax(C.6)(cid:6)) + (cid:10)1T (s, a, s(cid:6)(cid:9))(cid:6)(cid:16)(cid:16))(C.7)Next, we provide the following error bound on the first-order beliefs after any time t (cid:3) 0. Let Eμexpectation over all the t-length histories h ∈ Ht , each of which occurs with the probability P (h|∅, p0, t, μ).h∈Ht{·} be theLemma 6. Let bh be the correct belief of the hypothetical POMDP. Let ˆbh be the possibly-correct belief ; let ˆbh bean arbitrary probability function on S if it cannot be calculated (i.e., if the denominator is zero in Eq. (14)). For anyt (cid:3) 0, we haveEμ(cid:10)bh − ˆbh(cid:10) (cid:2) 16dt.h∈HtProof. First, we define a generalized possibly-correct first-order belief, ˆP (s(cid:6)|h(cid:6), h, b), which is calculated by Bayes-updating b repeatedly with the possibly-correct second-order beliefs, assuming that b is the belief after history h. Thatis, let ˆP (s(cid:6)|∅, h, b) = b(s(cid:6)) hold for any s(cid:6) ∈ S, h ∈ H , and first-order belief b. Also, if we have ˆb(cid:6) := τ (b, a, o, ˆbMh,a,o),ˆP (s(cid:6)|(cid:8)a, o(cid:9); h(cid:6)(cid:6), h, b) = ˆP (s(cid:6)|h(cid:6)(cid:6), h; (cid:8)a, o(cid:9), ˆb(cid:6)) hold, for any s(cid:6) ∈ S, h, h(cid:6)(cid:6) ∈ H , a ∈ A and o ∈ Θ, wherethen letH. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490485(cid:8)a, o(cid:9); h(cid:6)(cid:6) is the action-observation history in which h(cid:6)(cid:6) follows a and o. If we cannot calculate τ (b, a, o, ˆbMdue to a zero denominator in Eq. (14), then let ˆP (s(cid:6)|(cid:8)a, o(cid:9); h(cid:6)(cid:6), h, b) be an arbitrary probability function on S.h,a,o)Next, note thatEμh∈Ht(cid:10)bh − ˆbh(cid:10) =(cid:5)h∈Ht(cid:17)(cid:17)P (sP (h|∅, p0, t, μ)(cid:6)|h, ∅, p0) − ˆP (s(cid:17)(cid:17)(cid:6)|h, ∅, p0)holds. Thus, we need to prove that the right-hand side of this equation is not larger than 16dt. We prove here a moregeneral equation:(cid:5)(cid:17)(cid:17)P (s(cid:6)|h, b, t, μ)P (h(cid:6)(cid:6)|h, h, b) − ˆP (s(cid:6)(cid:6)|h, h, b)(cid:17)(cid:17) (cid:2) 16dt(C.8)h(cid:6)∈Htfor any h ∈ H , first-order belief b, t (cid:3) 0, and policy μ. The proof is by induction on t. Eq. (C.8) holds for t = 0, sincewe have P (s(cid:6)|∅, h, b) = ˆP (s(cid:6)|∅, h, b) = b(s(cid:6)). Assume that Eq. (C.8) holds for t. For t + 1, we have(cid:5)(cid:17)(cid:17)P (s(cid:6)|h, b, t + 1, μ)P (h(cid:6)(cid:6)|h, h, b) − ˆP (s(cid:6)(cid:6)|h(cid:17)(cid:17), h, b)h(cid:6)∈Ht+1=(cid:5)(cid:8)(cid:8)a, o(cid:9)|h, b, 1, μP(cid:9) (cid:5)(cid:8)(cid:6)(cid:6)|h; (cid:8)a, o(cid:9), bh(cid:6)P, t, μ(cid:9)(cid:8)h(cid:6)(cid:6)∈Hta∈A, o∈Θ(cid:17)(cid:8)(cid:17)P(cid:6)(cid:6)(cid:6)|h(cid:6)(cid:6)(cid:6)|h×s(∵ Divide h(cid:6) as h(cid:6) = (cid:8)a, o(cid:9); h(cid:6)(cid:6). Define b(cid:6) and ˆb(cid:6) as in Lemma 5.), h; (cid:8)a, o(cid:9), ˆb, h; (cid:8)a, o(cid:9), b− ˆP(cid:9)(cid:17)(cid:17)(cid:9)s(cid:6)(cid:6)(cid:8)(cid:8)a, o(cid:9)|h, b, 1, μP(cid:9) (cid:5)(cid:8)(cid:6)(cid:6)|h; (cid:8)a, o(cid:9), bhP(cid:6), t, μ(cid:9)(cid:5)=(cid:8)a∈A, o∈Θ(cid:17)(cid:17)P(cid:6)(cid:6)(cid:6)|h×s(cid:5)+, h; (cid:8)a, o(cid:9), b− P(cid:9)(cid:6)(cid:8)P(cid:8)a, o(cid:9)|h, b, 1, μh(cid:6)(cid:6)∈Ht(cid:8)(cid:6)(cid:6)(cid:6)|hs(cid:9) (cid:5)(cid:9)(cid:17)(cid:17)(cid:6), h; (cid:8)a, o(cid:9), ˆb(cid:8)P(cid:6)(cid:6)|h; (cid:8)a, o(cid:9), bh(cid:9)(cid:6), t, μa∈A, o∈Θ(cid:17)(cid:17)P(cid:6)(cid:6)(cid:6)|h(cid:8)s(cid:5)(cid:9), h; (cid:8)a, o(cid:9), ˆb(cid:8)h(cid:6)(cid:6)∈Ht− ˆP(cid:6)|h(cid:6)(cid:6)(cid:9)(cid:6) − ˆb(cid:10)b(cid:8)a, o(cid:9)|h, b, 1, μ(cid:8)s(cid:6)P×(cid:2) 2, h; (cid:8)a, o(cid:9), ˆb(cid:9)(cid:17)(cid:17)(cid:6)(cid:6)(cid:10)(∵ Lemma 4)a∈A, o∈Θ(cid:5)+(cid:8)P(cid:8)a, o(cid:9)|h, b, 1, μ(cid:9)a∈A, o∈Θ(cid:5)(cid:16)(cid:8)(cid:16)P(cid:6)(cid:6)|h; (cid:8)a, o(cid:9), bh(cid:6)(cid:9)(cid:8)(cid:6)(cid:6)|h; (cid:8)a, o(cid:9), ˆbh(cid:6)− P, t, μ, t, μ(cid:9)(cid:16)(cid:16)h(cid:6)(cid:6)∈Ht(cid:17)(cid:8)(cid:17)Ps(cid:5)(cid:6)(cid:6)(cid:6)|hP(cid:9)(cid:6)− ˆP, h; (cid:8)a, o(cid:9), ˆb(cid:8)(cid:8)a, o(cid:9)|h, b, 1, μ(cid:8)(cid:6)(cid:6)(cid:6)|hs(cid:9) (cid:5)(cid:9)(cid:17)(cid:17)(cid:6), h; (cid:8)a, o(cid:9), ˆb(cid:8)(cid:6)(cid:6)|h; (cid:8)a, o(cid:9), ˆbhP(cid:9)(cid:6), t, μ××+a∈A, o∈Θ(cid:17)(cid:8)(cid:17)P(cid:6)(cid:6)(cid:6)|h×s(cid:5)(cid:9), h; (cid:8)a, o(cid:9), ˆb(cid:8)h(cid:6)(cid:6)∈Ht(cid:8)− ˆP(cid:6)|h(cid:6)(cid:6)s(cid:9)(cid:6) − ˆb(cid:10)b(cid:8)a, o(cid:9)|h, b, 1, μ(cid:6)P(cid:6)(cid:10)(cid:2) 2, h; (cid:8)a, o(cid:9), ˆb(cid:9)(cid:17)(cid:17)(cid:6)a∈A, o∈Θ(cid:5)+ 2(cid:8)(cid:8)a, o(cid:9)|h, b, 1, μ(cid:9)(cid:10)bP(cid:6) − ˆb(cid:6)(cid:10)(∵ Lemmas 1 and 3)a∈A, o∈Θ(cid:5)(cid:8)P+(cid:8)a, o(cid:9)|h, b, 1, μ(cid:9) (cid:5)(cid:8)P(cid:6)(cid:6)|h; (cid:8)a, o(cid:9), ˆbh(cid:6), t, μ(cid:9)(cid:8)×a∈A, o∈Θ(cid:17)(cid:17)P(cid:6)(cid:6)(cid:6)|hs(cid:2) 8d + 8d(cid:6), h; (cid:8)a, o(cid:9), ˆb(∵ Lemma 5)(cid:9)(cid:8)s− ˆPh(cid:6)(cid:6)∈Ht(cid:6)(cid:6)(cid:6)|h, h; (cid:8)a, o(cid:9), ˆb(cid:9)(cid:17)(cid:17)(cid:6)486H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490+ 16dt= 16d(t + 1).(∵ assumption)Thus, Eq. (C.8) holds for t + 1. This completes the proof. (cid:2)Next, we provide an error bound on the values of a given policy. For a policy μ, let V μ(h) be the value of history hfor the hypothetical POMDP, which is defined as the solution to(cid:7)(cid:5)(cid:5)V μ(h) =Pr(a|h, μ)ρ(h, a) + γP (o|h, a)V μ(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9),a∈Ao∈Θwhere ρ(h, a) and P (o|h, a) are defined as Eqs. (17) and (18), respectively. Similarly, define the quasi-value of historyh, which we denote by ˆV μ(h), as the solution to(cid:5)(cid:5)(cid:7)ˆV μ(h) =Pr(a|h, μ)ˆρ(h, a) + γˆP (o|h, a) ˆV μ(cid:10)(cid:9)(cid:8)h; (cid:8)a, o(cid:9),a∈Ao∈Θwhere ˆρ(h, a) and ˆP (o|h, a) are defined as Eqs. (23) and (24), respectively.Let us call V μ := V μ(∅) the value of policy μ, and ˆV μ := ˆV μ(∅) the quasi-value of policy μ. Let Rmax and ˆV μmaxbe defined as Eq. (44) and Eq. (45), respectively. For any policy μ : H → A, define W μ asW μ := ((1 − γ )|S|(cid:10)Tmax+ 16γ d)Rmax + (1 + 15γ )γ d ˆV μmax(1 − γ )2.Then we have the following.Lemma 7. For any policy μ, we have|V μ − ˆV μ| (cid:2) W μ.Proof. First, we prove that(cid:16)(cid:16)(cid:16) (cid:2) γ Eμ(cid:16)V μ(h) − ˆV μ(h)Eμh∈Hth(cid:6)∈Ht+1(cid:16)(cid:16)V μ(h(cid:6)(cid:16)(cid:16))) − ˆV μ(h(cid:6)max)t + |S|(cid:10)T+ 16d(Rmax + γ ˆV μmaxRmax + γ d ˆV μmax.(C.9)To prove this, note that we have(cid:16)(cid:16)(cid:16)(cid:16)V μ(h) − ˆV μ(h)(cid:5)Eμh∈Ht= Eμh∈Ht(cid:5)(cid:16)(cid:16)(cid:16)(cid:16)ρ(h, a) + γPr(a|h, μ)o∈Θ(cid:16)(cid:16)(cid:16)(cid:16)ρ(h, a) − ˆρ(h, a)Pr(a|h, μ)P (o|h, a)V μ(cid:9)(cid:8)h; (cid:8)a, o(cid:9)− ˆρ(h, a) + γˆP (o|h, a) ˆV μ(cid:16)(cid:16)(cid:9)(cid:8)(cid:16)h; (cid:8)a, o(cid:9)(cid:16)(cid:5)o∈Θ(cid:2) Eμh∈Hta∈A(cid:5)a∈A+ γ Eμh∈Ht+ γ Eμh∈Ht(cid:5)a∈A(cid:2) Eμh∈Ht(cid:5)a∈A(cid:5)(cid:16)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)(cid:16)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)(cid:5)o∈Θ(cid:5)P (o|h, a)V μ(cid:9)(cid:8)h; (cid:8)a, o(cid:9)−P (o|h, a) ˆV μ(cid:9)(cid:8)h; (cid:8)a, o(cid:9)−a∈A(cid:16)(cid:16)(cid:16)(cid:16)ρ(h, a) − ˆρ(h, a)Pr(a|h, μ)o∈ΘP (o|h, a) ˆV μˆP (o|h, a) ˆV μ(cid:16)(cid:16)(cid:9)(cid:8)(cid:16)h; (cid:8)a, o(cid:9)(cid:16)(cid:16)(cid:16)(cid:9)(cid:8)(cid:16)h; (cid:8)a, o(cid:9)(cid:16)(cid:5)o∈Θ(cid:5)o∈Θ(cid:16)(cid:16)V μP (o|h, a)(cid:9)(cid:8)h; (cid:8)a, o(cid:9)(cid:8)h; (cid:8)a, o(cid:9)(cid:9)(cid:16)(cid:16)+ γ Eμh∈Ht+ γ Eμh∈Ht(cid:5)a∈A(cid:5)a∈A(cid:5)Pr(a|h, μ)o∈Θ(cid:16)(cid:5)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)o∈ΘP (o|h, a) −(cid:5)o∈Θ− ˆV μ(cid:16)(cid:16)(cid:16)ˆP (o|h, a)(cid:16)ˆV μmax,H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490487for which we haveEμ(cid:5)h∈Ht(cid:16)(cid:16)(cid:16)(cid:16)ρ(h, a) − ˆρ(h, a)Pr(a|h, μ)a∈A= Eμh∈Ht−(cid:5)a∈A(cid:13)(cid:13)(cid:16)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)mh=(Th,Oh)∈M(cid:5)(cid:5)(cid:5)(cid:5)s∈Ss(cid:6)∈SR(s, a, s)Th(s, a, s(cid:6)R(s, a, s(cid:6))Th(s, a, s(cid:6))bh(s) ˆbMh (mh) dmh(cid:6)) ˆbh(s) ˆbMh (mh) dmh(cid:16)(cid:16)(cid:16)(cid:16)(cid:13)(cid:16)(cid:16)(cid:16)(cid:16)bh(s)Th(s, a, s(cid:6))bMh (mh) dmh(cid:13)(cid:16)(cid:16)(cid:16)(cid:16)bh(s)Th(s, a, s(cid:6))bMh (mh) dmhmh=(Th,Oh)∈M(cid:5)(cid:2) RmaxEμh∈Hta∈A(cid:13)s∈Ss(cid:6)∈SPr(a|h, μ)(cid:5)(cid:5)s∈Ss(cid:6)∈S− ˆbh(s)Th(s, a, s(cid:6)) ˆbMmh=(Th,Oh)∈M(cid:5)(cid:2) RmaxEμh∈HtPr(a|h, μ)a∈A(cid:13)(cid:5)(cid:5)s∈Ss(cid:6)∈S− ˆbh(s)Th(s, a, s(cid:6)mh=(Th,Oh)∈M+ RmaxEμh∈HtPr(a|h, μ)(cid:5)a∈A(cid:13)(cid:5)(cid:5)s∈Ss(cid:6)∈Smh=(Th,Oh)∈M(cid:16)(cid:16)(cid:16)(cid:16)h (mh) dmhmh=(Th,Oh)∈M(cid:16)(cid:16)(cid:16)(cid:16))bMh (mh) dmh− ˆbh(s)Th(s, a, s(cid:6)) ˆbMh (mh) dmhmh=(Th,Oh)∈M(cid:16)(cid:16)(cid:16)(cid:16)(cid:13)(cid:16)(cid:16)(cid:16)(cid:16)ˆbh(s)Th(s, a, s(cid:6))bMh (mh) dmhmh=(Th,Oh)∈M(cid:2) RmaxEμh∈Ht(cid:10)bh − ˆbh(cid:10) + RmaxEμh∈Ht(cid:13)−Th(s, a, s(cid:6)) ˆbMh (mh) dmh(cid:5)a∈A(cid:16)(cid:16)(cid:16)(cid:16)Pr(a|h, μ)(cid:5)s∈Sˆbh(s)(cid:5)s(cid:6)∈S(cid:13)(cid:16)(cid:16)(cid:16)(cid:16)mh=(Th,Oh)∈MTh(s, a, s(cid:6))bMh (mh) dmhmh=(Th,Oh)∈M(cid:8)16dt + |S|(cid:10)Tmax(cid:9)Rmax(cid:2)andγ Eμh∈Ht(cid:5)a∈APr(a|h, μ)(cid:5)o∈Θ(∵ by Lemma 6 and by a proof similar to Eq. (C.7)),(cid:16)(cid:16)V μP (o|h, a)(cid:9)(cid:8)h; (cid:8)a, o(cid:9)− ˆV μ(cid:8)h; (cid:8)a, o(cid:9)(cid:9)(cid:16)(cid:16) = γ Eμh(cid:6)∈Ht+1(cid:16)(cid:16)V μ(h(cid:6)) − ˆV μ(h(cid:6)(cid:16)(cid:16),)and alsoEμh∈Ht(cid:5)a∈A(cid:16)(cid:16)(cid:16)ˆP (o|h, a)(cid:16)(cid:5)o∈Θ(cid:16)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)(cid:5)P (o|h, a) −(cid:16)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)(cid:5)o∈Θ(cid:13)(cid:5)a∈A(cid:13)o∈Θ(cid:5)mh=(Th,Oh)∈M(cid:5)s∈S(cid:6)Oh(s, a, o)(cid:13)mh=(Th,Oh)∈M(cid:5)s(cid:6)∈S(cid:16)(cid:5)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)a∈Ao∈Θmh=(Th,Oh)∈M= Eμh∈Ht−(cid:5)o∈Θ(cid:2) Eμh∈HtTh(s, a, s(cid:6))bh(s)bMh (mh) dmh(cid:5)s(cid:6)∈S(cid:6)Oh(s, a, o)(cid:5)s∈STh(s, a, s(cid:6)) ˆbh(s) ˆbMh (mh) dmh(cid:16)(cid:16)(cid:16)(cid:16)(cid:5)s(cid:6)∈S(cid:6)Oh(s, a, o)(cid:5)s∈STh(s, a, s(cid:6))bh(s)bMh (mh) dmh488H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490−(cid:5)o∈Θ+ Eμh∈Ht(cid:5)−o∈Θ(cid:13)(cid:5)(cid:6)Oh(s, a, o)Th(s, a, s(cid:6)) ˆbh(s)bMh (mh) dmh(cid:16)(cid:16)(cid:16)(cid:16)(cid:5)s∈Smh=(Th,Oh)∈M(cid:5)s(cid:6)∈S(cid:16)(cid:16)(cid:16)Pr(a|h, μ)(cid:16)(cid:5)o∈Θa∈A(cid:13)(cid:13)(cid:5)s(cid:6)∈S(cid:6)Oh(s, a, o)(cid:5)s∈Smh=(Th,Oh)∈M(cid:5)(cid:5)s(cid:6)∈S(cid:6)Oh(s, a, o)Th(s, a, ss∈S(cid:6)) ˆbh(s) ˆbMh (mh) dmh(cid:16)(cid:16)(cid:16)(cid:16)Th(s, a, s(cid:6)) ˆbh(s)bMh (mh) dmhmh=(Th,Oh)∈M(cid:10)bh − ˆbh(cid:10)(cid:5)Pr(a|h, μ)(cid:2) Eμh∈Ht+ Eμh∈Hta∈A(cid:13)−(cid:6)Oh(smh=(Th,Oh)∈M(cid:5)(cid:5)o∈Θs(cid:6)∈S(cid:5), a, o)s∈S(cid:5)(cid:6)Oh(s, a, o)Th(s, a, s(cid:6)) ˆbh(s)bMh (mh) dmh(cid:13)(cid:16)(cid:16)(cid:16)(cid:16)mh=(Th,Oh)∈MTh(s, a, s(cid:6)) ˆbh(s) ˆbMh (mh) dmhs∈S(cid:16)(cid:16)(cid:16)(cid:16)(cid:2) 16dt + d(∵ by Lemma 6 and by a proof similar to Eq. (C.7)).Taken together, we obtain(cid:16)(cid:16)(cid:16) (cid:2) γ Eμ(cid:16)V μ(h) − ˆV μ(h)Eμh∈Hth(cid:6)∈Ht+1(cid:16)(cid:16)V μ(h(cid:6)) − ˆV μ(h(cid:6)(cid:16)(cid:16) +)(cid:8)16dt + |S|(cid:10)Tmax(cid:9)Rmax + γ (16dt + d) ˆV μmaxand hence Eq. (C.9).|S|(cid:10)TNext, let us re-write this equation by defining Yt := EμmaxRmax + γ d ˆV μmax. Then we haveY0 (cid:2) γ Y1 + B (cid:2) γ (γ Y2 + A + B) + B (cid:2) γh∈Ht(cid:2) A(γ + 2γ 2 + · · ·) + B(1 + γ + γ 2 + · · ·) = ASubstituting back A = 16d(Rmax + γ ˆV μmax) and B = |S|(cid:10)T(note that |V μ − ˆV μ| equals to Y0). (cid:2)|V μ(h) − ˆV μ(h)|, A := 16d(Rmax + γ ˆV μmax), and B :=(cid:9)(cid:18)(cid:8)γ (γ Y3 + 2A + B) + A + B11(1 − γ )(1 − γ )2maxRmax + γ d ˆV μ−+ B (cid:2) · · ·(cid:19)(cid:18)+ B(cid:19).11 − γmax into this equation proves the LemmaV μ∗ − Vˆμ∗ (cid:2) V μ∗ − ˆVNow, we can prove Theorem 1 asˆμ∗ + W(cid:2) V μ∗ − ˆV μ∗ + W(cid:2) V μ∗ − V μ∗ + Wˆμ∗ + W μ∗= W.(∵ Lemma 7)(∵ Definition of ˆμ∗)(∵ Lemma 7)ˆμ∗ˆμ∗ˆμ∗ + W μ∗(cid:2)References[1] D. Aberdeen, J. Baxter, Scaling internal-state policy-gradient methods for POMDPs, in: International Conference on Machine Learning(ICML-02), Sydney, Australia, July 2002, pp. 1–12.[2] K.J. Aström, Optimal control of Markov decision processes with incomplete state estimation, Journal of Mathematical Analysis and Applica-tions 10 (1965) 174–205.[3] T. Augustin, On the suboptimality of the generalized Bayes rule and robust Bayesian procedures from the decision theoretic point ofview—a cautionary note on updating imprecise priors, in: Proceedings of 3rd International Symposium on Imprecise Probabilities and theirApplications (ISIPTA-03), 2003.[4] R. Bellman, Dynamic Programming, Princeton Univ. Press, Princeton, NJ, 1957.[5] J.M. Bernard, T. Seidenfeld, M. Zaffalon (Eds.), Proceedings of the Third International Symposium in Imprecise Probabilities and its Appli-cations, Carleton Scientific, 2003.[6] D.P. Bertsekas, Dynamic Programming and Optimal Control, vol. 2, second ed., Athena Scientific, Belmont, MA, 2001.[7] B. Bonet, An epsilon-optimal grid-based algorithm for partially observable Markov decision processes, in: Proc. 19th International Conf. onMachine Learning (ICML-02), Morgan Kaufmann, 2002, pp. 51–58.H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490489[8] C. Boutilier, T. Dean, S. Hanks, Decision-theoretic planning: Structural assumptions and computational leverage, Journal of Artificial Intelli-gence Research 11 (1999) 1–94.[9] C. Boutilier, D. Poole, Computing optimal policies for partially observable decision processes using compact representations, in: Proceedingsof the Thirteenth National Conference on Artificial Intelligence (AAAI-96), Portland, OR, AAAI Press/The MIT Press, 1996, pp. 1168–1175.[10] J. Breese, K. Fertig, Decision making with interval influence diagrams, in: Proceedings of the 6th Annual Conference on Uncertainty inArtificial Intelligence (UAI-91), New York, Elsevier Science, 1991, pp. 467–478.[11] A. Cassandra, M.L. Littman, N.L. Zhang, Incremental Pruning: A simple, fast, exact method for partially observable Markov decisionprocesses, in: D. Geiger, P.P. Shenoy (Eds.), Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), San Francisco, CA, Morgan Kaufmann, 1997, pp. 54–61.[12] L. Chrisman, Independence with lower and upper probabilities, in: Proceedings of the 12th Annual Conference on Uncertainty in ArtificialIntelligence (UAI-96), San Francisco, CA, Morgan Kaufmann, 1996, pp. 169–177.[13] F.G. Cozman, Credal networks, Artificial Intelligence 120 (2000) 199–233.[14] F.G. Cozman, E. Krotkov, Quasi-Bayesian strategies for efficient plan generation: application to the ‘planning to observe’ problem, in: E.Horvitz, F.V. Jensen (Eds.), Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96), San Francisco, CA,Morgan Kaufmann, 1996, pp. 186–193.[15] A. Drake, Observation of a Markov process through a noisy channel, PhD thesis, Massachusetts Institute of Technology, 1962.[16] Z. Feng, E.A. Hansen, Approximate planning for factored POMDPs, in: Proceedings of the 6th European Conference on Planning (ECP-01),Toledo, Spain, September 2001.[17] K. Fertig, J. Breese, Interval influence diagrams, in: Proceedings of the 5th Annual Conference on Uncertainty in Artificial Intelligence(UAI-90), New York, Elsevier Science, 1990, pp. 149–161.[18] K.W. Fertig, J.S. Breese, Probability intervals over influence diagrams, IEEE Transactions on Pattern Analysis and Machine Intelligence 15 (3)(1993) 280–286.[19] H. Gaifman, A theory of higher order probabilities, in: Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning aboutKnowledge, Morgan Kaufmann, 1986, pp. 275–292.[20] R. Givan, S.M. Leach, T. Dean, Bounded-parameter Markov decision processes, Artificial Intelligence 122 (1–2) (2000) 71–109.[21] I.J. Good, Good Thinking: The Foundations of Probability and its Applications, University of Minnesota Press, Minneapolis, 1983.[22] A.J. Grove, J.Y. Halpern, Updating sets of probabilities, in: Proceedings of the 14th Annual Conference on Uncertainty in Artificial Intelligence(UAI-98), San Francisco, CA, Morgan Kaufmann, 1998, pp. 173–182.[23] V. Ha, P. Haddawy, Theoretical foundations for abstraction-based probabilistic planning, in: Proceedings of the 12th Annual Conference onUncertainty in Artificial Intelligence (UAI-96), San Francisco, CA, Morgan Kaufmann, 1996, pp. 291–298.[24] E.A. Hansen, Solving POMDPs by searching in policy space, in: Proceedings of the Fourteenth Conference on Uncertainty in ArtificialIntelligence (UAI-98), 1998, pp. 211–219.[25] E.A. Hansen, Z. Feng, Dynamic programming for POMDPs using a factored state representation, in: Artificial Intelligence Planning Systems(AIPS-00), 2000, pp. 130–139.[26] E.A. Hansen, R. Zhou, Synthesis of hierarchical finite-state controllers for POMDPs, in: Thirteenth International Conference on AutomatedPlanning and Scheduling (ICAPS-03), June 2003.[27] D. Harmanec, Generalizing Markov decision processes to imprecise probabilities, Journal of Statistical Planning and Inference 105 (2002)199–213.[28] M. Hauskrecht, Value-function approximations for partially observable Markov decision processes, Journal of Artificial Intelligence Re-search 13 (2000) 33–94.[29] M. Hauskrecht, H. Fraser, Planning treatment of ischemic heart disease with partially observable Markov decision processes, Artificial Intel-ligence in Medicine 18 (2000) 221–244.[30] L.P. Kaelbling, M.L. Littman, A.R. Cassandra, Planning and acting in partially observable stochastic domains, Artificial Intelligence 101(1999) 99–134.[31] N. Karmarkar, A new polynomial-time algorithm for linear programming, Combinatorica 4 (1984) 373–395.[32] P.E. Lehner, K.B. Laskey, D. Dubois, An introduction to issues in higher order uncertainty, IEEE Transactions on Systems, Man and Cyber-netics, Part A 26 (3) (1996) 289–293.[33] I. Levi, On indeterminate probabilities, Journal of Philosophy 71 (1974) 391–418.[34] I. Levi, The Enterprise of Knowledge, MIT Press, Cambridge, MA, 1980.[35] W.S. Lovejoy, A survey of algorithmic methods for partially observed Markov decision processes, Annals of Operations Research 28 (1991)47–66.[36] C. Lusena, J. Goldsmith, M. Mundhenk, Nonapproximability results for partially observable Markov decision processes, Journal of ArtificialIntelligence Research 14 (2001) 83–103.[37] D.A. McAllester, S. Singh, Approximate planning for factored POMDPs using belief state simplification, in: Proceedings of the FifteenthConference on Uncertainty in Artificial Intelligence (UAI-99), 1999, pp. 409–416.[38] M. Montemerlo, J. Pineau, N. Roy, S. Thrun, V. Verma, Experiences with a mobile robotic guide for the elderly, in: Proceedings of the NationalConference of Artificial Intelligence (AAAI-02), Edmonton, AB, July 2002, pp. 587–592.[39] A. Nilim, L. El-Ghaoui, Robustness in Markov decision problems with uncertain transition matrices, in: Advances in Neutral InformationProcessing Systems 16 (NIPS-03), MIT Press, Cambridge, MA, 2004.[40] A. Nilim, L. El-Ghaoui, Robust control of Markov decision processes with uncertain transition matrices, Operations Research 53 (2005)780–798.[41] G. Paaß, Second order probabilities for uncertain and conflicting evidence, in: Proceedings of the 6th Annual Conference on Uncertainty inArtificial Intelligence (UAI-91), New York, Elsevier Science, 1991, pp. 447–456.490H. Itoh, K. Nakamura / Artificial Intelligence 171 (2007) 453–490[42] C.H. Papadimitriou, J.N. Tsitsiklis, The complexity of Markov decision processes, Mathematics of Operations Research 12 (3) (1987) 441–450.[43] J. Pineau, Tractable planning under uncertainty: Exploiting structure, PhD thesis, Robotics Institute, Carnegie Mellon University, Pittsburgh,PA, 2004.[44] J. Pineau, G. Gordon, S. Thrun, Point-based value iteration: An anytime algorithm for POMDPs, in: Proceedings of the Eighteenth Interna-tional Joint Conference on Artificial Intelligence (IJCAI-03), AAAI Press, Menlo Park, CA, 2003.[45] P. Poupart, Exploiting structure to efficiently solve large scale partially observable Markov decision processes, PhD thesis, Department ofComputer Science, University of Toronto, Toronto, Ontario, Canada, 2005.[46] P. Poupart, C. Boutilier, Bounded finite state controllers, in: Advances in Neural Information Processing Systems 16 (NIPS-03), MIT Press,Cambridge, MA, 2004.[47] P. Poupart, C. Boutilier, VDCBPI: An approximate scalable algorithm for large scale POMDPs, in: Advances in Neural Information ProcessingSystems 17 (NIPS-04), MIT Press, Cambridge, MA, 2005.[48] J.K. Satia, R.E. Lave, Markovian decision processes with uncertain transition probabilities, Operations Research 21 (1973) 728–740.[49] T. Seidenfeld, M.J. Schervish, Two perspectives on consensus for (Bayesian) inference and decisions, IEEE Transactions on Systems, Manand Cybernetics 20 (2) (1990) 318–325.[50] G. Shafer, A Mathematical Theory of Evidence, Princeton Univ. Press, Princeton, NJ, 1976.[51] E.J. Sondik, The optimal control of partially observable Markov processes, PhD thesis, Stanford University, 1971.[52] M.T.J. Spaan, N. Vlassis, Perseus: Randomized point-based value iteration for POMDPs, Journal of Artificial Intelligence Research 24 (2005)195–220.[53] N. Vlassis, M.T.J. Spaan, A fast point-based algorithm for POMDPs, in: Benelearn 2004: Proceedings of the Annual Machine LearningConference of Belgium and the Netherlands, Brussels, Belgium, 2004, pp. 170–176.[54] P. Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall, London, 1991.[55] C.C. White, H.K. Eldeib, Parameter imprecision in finite state, finite action dynamic programs, Operations Research 34 (1986) 120–129.[56] C.C. White, H.K. Eldeib, Markov decision processes with imprecise transition probabilities, Operations Research 43 (1994) 739–749.[57] N.L. Zhang, W. Zhang, Speeding up the convergence of value iteration in partially observable Markov decision processes, Journal of ArtificialIntelligence Research 14 (2001) 29–51.[58] W. Zhang, N.L. Zhang, Restricted value iteration: Theory and algorithms, Journal of Artificial Intelligence Research 23 (2005) 123–165.[59] R. Zhou, E.A. Hansen, An improved grid-based approximation algorithm for POMDPs, in: Proceedings of the 17th International Joint Con-ference on Artificial Intelligence (IJCAI-01), 2001, pp. 707–716.