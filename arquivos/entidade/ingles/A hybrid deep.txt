A hybrid deep-learning approach for complex biochemicalnamed entity recognitionJian Liu1,3,10, Lei Gao2,*, Sujie Guo1,4, Rui Ding1,5, Xin Huang6, LongYe7,8, Qinghua Meng8, Asef Nazari9, and Dhananjay Thiruvady91 HeFei University of Technology, Hefei, 2300092 CSIRO, Waite Campus, Urrbrae, SA 5064, Australia3 Intelligent Interconnected Systems Laboratory of Anhui Province, Hefei Universityof Technology4 Shanghai Engineering Center for Microsatellites, Shanghai 201203, China5 Xi’an Jiaotong University, Xi’an,710049, China6 College of Computer and Information Engineering, Tianjin Normal University,Tianjin 300387, China7 School of Mechatronic Engineering and Automation, Shanghai University, Shanghai200072, China8 Faculty of Business Information, Shanghai Business School, Shanghai 201400, China9 School of Information Technology, Deakin University, Geelong, Australia10 Anhui Province Key Laboratory of Industry Safety and Emergency Technology,Hefei 230601, Anhui, P.R. China*Corresponding author:Dr. Lei GaoSenior Research ScientistCSIRO Land and WaterPrivate Mail Bag 2, Waite RoadGlen Osmond SA 5064, AustraliaPh: +61-8-8273 8109Fax: +61-8-8303 8750Email: lei.gao@csiro.au1Abstract:Named entity recognition (NER) of chemicals and drugs is a critical domain ofinformation extraction in biochemical research. NER provides support for text miningin biochemical reactions, including entity relation extraction, attribute extraction, andmetabolic response relationship extraction. However, the existence of complex namingcharacteristics in the biomedical field, such as polysemy and special characters, makethe NER task very challenging. Here, we propose a hybrid deep learning approach toimprove the recognition accuracy of NER. Specifically, our approach applies theBidirectional Encoder Representations from Transformers (BERT) model to extract theunderlying features of the text, learns a representation of the context of the text throughBi-directional Long Short-Term Memory (BILSTM), and incorporates the multi-headattention (MHATT) mechanism to extract chapter-level features. In this approach, theMHATT mechanism aims to improve the recognition accuracy of abbreviations toefficiently deal with the problem of inconsistency in full-textlabels. Moreover,conditional random field (CRF) is used to label sequence tags because this probabilisticmethod does not need strict independence assumptions and can accommodate arbitrarycontext information. The experimental evaluation on a publicly-available dataset showsthatinthe proposed hybrid approach achieves the best recognition performance;substantially improves performance in recognizing abbreviations,particular,polysemes, and low-frequency entities, compared with the state-of-the-art approaches.For instance, compared with the recognition accuracies for low-frequency entitiesproduced by the BILSTM-CRF algorithm, those produced by the hybrid approach ontwo entity datasets (MULTIPLE and IDENTIFIER) have been increased by 80% and21.69%, respectively.itKeywords: Named entity recognition; Deep learning; Bi-directional Long Short-TermMemory (BILSTM); Conditionalrandom field (CRF); Bidirectional EncoderRepresentations from Transformers (BERT); Multi-head attention (MHATT)21. Introductionartificialintelligence, particularly deep learning,In recent years, artificial intelligence has helped increase the interactions amongtheoretical chemistry, computational chemistry, and synthetic chemistry. Deep neuralnetworks have been recently used to analyze the rationality of chemical synthesis andfind a large number of reverse synthetic routes in a short amount of time [1]. Thesecomputational tools make reaction analysis faster than manual approaches and allowefficient predictions of reactions of possible reagent combinations. There is no doubtthatrevolutionizing ourunderstanding of chemistry [2, 3].Despite the advances,there are still severalimportant scientific activities and processes for the extraction of information that aredone manually, taking plenty of experts’ time. A number of these activities could beefficiently managed using Natural Language Processing (NLP) and other artificialintelligence tools [4, 5, 6]. As an example, the need for an intelligent tool that canautomatically extract materials and chemical entities from the literature in thechemistry-related fields, is vital and urgent. The necessity of such tools provides themotivation to explore the Named Entity Recognition (NER) technology [7].in the field of chemical drugs,isNER refers to the identification of entities that have a specific meaning in the text,including names, names of places, and proper nouns [8, 9]. It was first presented as aconcept and motivated as a research area at the Message Understanding Conference(MUC-6) in 1995 [10]. NLP, as an important tool in information extraction, helps theprocess of subsequent relationship extraction, event extraction, and disambiguation. Todeal with NER, several methods including methods based on rules and dictionaries,statistical methods, and hybrid approaches have been used. Methods based on rules anddictionaries usually perform better when the task of recognition is in a specific corpus,such as the Dl-cotrain algorithm using decision tables proposed by Kwak et. al. [11].However, rule-based methods are limited in their ability of effectively carrying outrecognition tasks, since they rely on many complex rules. Moreover,they arecontext-sensitive, and require expert knowledge in specific fields and substantial effortsin maintaining rules and dictionaries. To overcome these drawbacks,the HiddenMarkov Hodel (HMM),the ConditionalRandom Field (CRF), and classical machine learning methods such as the supportvector machines [12] have gradually replaced the aforementioned traditional methods.the maximum entropy model (MaxEnt),The NER approaches based on machine learning models, consist of tasks that canbe divided into pure artificial features, supervised tasks, semi-supervised tasks, andunsupervised tasks. Although traditional statistical methods based on artificial featureshave shown improvements in the field of data security, they suffer from deficienciessuch as being computationally expensive, requiring large overheads for recognition, orpoor generalizability. Supervised machine learning methods, such as the HMM namedentity classifier,is based on a word similarity smoothing technique [13], and ischaracterized by high recognition accuracy. However, training data for these models aremanually labeled, which tends to be a laborious task. Moreover, they need large trainingdata, and this can be particularly problematic when the available data are scarce.3Similarly, semi-supervised and unsupervised techniques need very large amounts oftraining data. However, they have been very effective when dealing with low-frequencydata. For example, Julian Brooke presented an NER system targeted specifically atfiction uses unlabeled data to obtain results [14], however, the approach clearly takesadvantage ofrule-basedsegmentation, for instance, depends on reliable capitalization of names, which is oftennot present in social media, or in most non-European languages. What is more, thisapproach cannot be successfully applied in cases where texts are relatively short.specific properties ofliterature. The initial(English)With advances in deep learning and the large availability of computationalresources, methods based on deep learning have demonstrated obvious advantages overtraditional methods in NLP, and they have gradually become mainstream in the field ofNER. They not only address some of the shortcomings of supervised machine learningmethods (too computationally-intensive and time-consuming), but they also alleviateissues such as the lack of generalizability and large recognition workload commonlyseen in machine learning methods based on artificial features.NEC Labs America pioneered the idea of using deep learning for NLP [15].Collobert et al. utilized embedding and multi-layer one-dimensional convolutionstructures for four typical NLP problems such as part-of-speech tagging. They used acombination of Convolutional Neural Networks(CNN) and CRF to achievegroundbreaking results in the optimization of the CONLL2003 corpus in the field ofuniversal named entity recognition [16]. Subsequently, with the advent oflongshort-term memory (LSTM) and Bidirectional Long Short-Term Memory (BILSTM)based on Recurrent Neural Network (RNN), the performance for NLP has graduallyimproved. The BILSTM-CRF model designed achieved an approximate F-value of 89%in the corpus [17]. Furthermore, the F-value of the CNN-LSTM model established byChiu et al. exceeded 90% [18]. In 2016, Ma et al. proposed a method for end-to-endsequence tagging by BILSTM-CNNs-CRF, which led to the F-value of 91.21% for theCONLL2003 corpus [17].Despite deep learning methods obtaining excellent performance in a generic NER,the performance of NER technologies in some specific fields such as chemistry, biology,finance, and so forth is still far from the ideal. Therefore, NER-related tasks in somefields need significantly more attention, as extracting biochemical entity informationfrom text databases or scientific literature can assist interdisciplinary researchers in thefield of biochemistry [19, 20].There are two main difficulties in the identification of named entities in the field ofbiochemistry in comparison with generic NER. First, there has not yet been a unifiednaming methodology. There exist multiple expression methods for the same entity inaddition to complicated and irregular naming issues including differences in Englishthe number and types ofabbreviations, special characters, and so forth. Second,biochemical entities are huge, and they are growing rapidly. At the moment, the numberof new synthetic drugs is increasing exponentially, and new drugs are constantly beingproduced.To deal with the challenges of NER in the field of biochemistry, Leaman et al.developed the tmChem system, which achieves an F-value of 87.395% on the4CHEMDNER dataset [21]. In another work, Zheng used an attention-based recognitionmethod to increase the F-value to 90.77% via an attention mechanism to extract achapter’s features, which ensures the consistency of labels [16]. However, the word2vecword vector from Zheng’s method does not solve the problem of polysemy, whichreduces the overall accuracy of recognition. In addition, the training of deep learningmodel usually requires a large number of manually labeled data, which can be a hugetask in the fields of biology and medicine.combinesIn order to cope with the above challenges of named entity recognition inthis paper proposes a hybrid model (BERT-BILSTM-MHATT-CRF,biochemistry,BBMC) which mainlyapproaches: Bidirectional EncoderfourRepresentations from Transformers (BERT), Bi-directional Long Short-Term Memory(BILSTM), Multi-Head Attention (MHATT), and Conditional Random Field (CRF).The main contributions of this paper are summarized as follows: (1) The above four keyapproaches are well integrated to address different challenging aspects of NER and theperformance of this hybrid deep learning model is validated on a public-availabledataset.thefrequently-used word2vec model which produces word vector is replaced by a betterrepresentation model (BERT). (3) The multi-head attention mechanism in cognitiveneuroscience is innovatively integrated to the BILSTM model to extract chapter-levelfeatures. (4) To improve the recognition rate, the softmax results of deep learning areconnected to the CRF layer to make use of the dependency between tags.information and address polysemy,(2) To learn high-level abstractThe rest of the paper is organized as follows. Section 2 presents the BBMC modelarchitecture and its four key modules: (1) BERT which has a large number of semanticfeatures and part-of-speech features, (2) BILSTM which can predict the relationshipbetween text sequences and tags, (3) MHATT that can extract information from multipleaspects of sentences, and (4) CRF which can predict the relationship between tags(because BERT can only predict the relationship between text sequence and label). Theexperimental datasets and setup of experiments are presented in Section 3. Section 4demonstrates performance evaluation of the proposed hybrid model and comparisonwith existing approaches on the CHEMDNER dataset. The final section concludes ourresearch efforts.2. The hybrid BBMC model for biochemical named entity recognition2.1. ArchitectureThe architecture of the proposed BBMC model is shown in Figure 1. The modelconsists of four parts: the BERT module, the BILSTM module, the MHATT moduleand the CRF module. The BERT module is first used to process the input data to obtainthe underlying features of the input text, such as part-of-speech and characters, andprimary sentence features. Next, the hidden layer of BILSTM stores two values thatinclude forward and backward calculations. In this way, the target word can use theinformation at both the beginning and the end of the training process. Then, we apply5the attention module to ensure that it can extract information of interest at the level ofsentences. This module can address the problem of inconsistency, identify abbreviationsand full-text labels (to a certain extent), and reduce the difficulty in training the model.Finally, the CRF layer decodes the output of the previous module into an optimalsequence and outputs the annotation information. The functions and underlyingprinciples of each module are described in subsequent sections.Figure 1. An overview of the BBMC model.2.2. The BERT moduleCurrently, pre-training models are preferred in typical NLP tasks. The mostprevalent one is word2vec, which was developed in an open source project by Google[22]. The maximum dictionary size of the word2vec’s pre-training model can be in themillions, and the maximum dataset size of word2vec’s pre-training model can reach 100the similarity between words can bemillion. Once trained, via word embedding,measured, which is empirically efficient for most tasks. However, word2vec, as a staticpre-training model, cannot address the problem of polysemy. There are two mainstrategies to apply pre-training models to NLP tasks. The first strategy is to usefeature-based language models, such as Embeddings from Language Models (ELMO)[3]. The ELMO model introduces a contextual dynamic adjustment of word vectors to6address the polysemy problem. However, the BILSTM structural feature extractioncapability used is weaker than the Transformer we propose. The second strategy is touse the fine-tuning language model, such as the Generative Pre-trained Transformer(GPT) [23]. This is a type of Transformer-based language model that can be adapted tomultiple NLP tasks. It works by employing a pre-training language model for varioustasks with very few (if any) changes to the model structure. However, it only uses aone-way language model, which limits its use in specific applications.The BERT model that can perform multi-task learning based on a two-way deepnetwork Transformer [24] is proposed. This model has created many advances in NLPand proved to be a good complement to the abovementioned models, alleviating theirshortcomings. There are mainly two new component models proposed in BERT, theMasked Language Model (MLM) and Next Sentence Prediction (NSP). The MLMrandomly masks 15% of the words at each iteration, and the goal of the model is tothese words given their context. Using transformer encoder with a strongpredictextraction ability and using bidirectional training form, the MLM is ideally suitable forlong sequence text named entity recognition tasks. The function of NSP is to understandrelationships between sentences, and its specific role is to replace the sentence order ofthe corpus randomly, then predict the probability of the next sentence based on theprevious sentence, and then continue to cycle training to get the results. In the trainingprocess, the loss functions of MLM and NSP are added to learn at the same time.In this work, we adopt Google’s open-source BERT pre-training model. The modelinput is a single sentence (cid:1)≔(cid:3)(cid:1)1,…,(cid:1)(cid:7)(cid:8) . BERT consists of three different layers,namely the Token layer, the Segment layer and the Position layer. The summed valuesof these layers are used as input to Transformer. The Token layer produces the wordvector embedding associated with the word. The Segment layer distinguishes whichsentence the word belongs to while the Position layer distinguishes the position of theword in the sequence. This process is represented in Figure 2.Figure 2. The structure of the Transformer.After the processing provided by multiple Transformer modules, the BERT modelis used to extract the features of the input sequence. Compared to the BILSTM, BERT7processes the sequence step by step. Multiple transformer modules run in parallel toraise the speed of decoding and has strong capabilities for feature extraction.The BERT model is excellent at feature extraction. It has been empirically verifiedthat the fine-tuned BERT model has similar accuracy to the popular BILSTM-CRFmodel in NER task of chemicals and drugs. The main difference is that the latter canextract specific characteristics within a problem. However, the costs associated with thelatter model is extremely high, such as a large amount of training time and high-leveltraining difficulty. For these reasons, we combine the advantages of both models byadopting the BERT model instead of the traditional word2vec, and adopting a strategyof applying the BISLTM layer onto the BERT model.2.3. The BILSTM moduleThe input text can be processed by the BERT module to directly output thecorresponding recognition results according to the sentence probability vector. However,this method can yield relatively poor outcomes. A more effective approach is to onlyuse the output of BERT as a representation of deep features such as part-of-speech,semantics, and primary sentence-level features. This output is then used as an input tothe LSTM model to further extract context information.LSTM, as a special case of RNN, is a cyclic RNN with long-term memory [25, 26].Its network structure consists of one or more units with forgetting and memoryfunctions, which overcomes the problem of traditional RNN gradient dispersion. Thisenables the network to retain the previous information selectively [27, 28]. In the caseof biochemical named entity recognition, there are many cases where multiple wordsconstitute an entity. With LSTM, the characteristics of long-distance dependence can belearned, thereby the ability of the model to identify long-sequence entities can beimproved.Let us denote the output sequence of the BERT model as (cid:9)≔(cid:3)(cid:9)1,…,(cid:9)(cid:7)(cid:8) . Thecorresponding LSTM diagram is shown in Figure 3.Figure 3．The internal structure of a LSTM/memory cell.8The LSTM has two transmission states: the cell state (cid:10)(cid:11)In comparison, the general RNN only has the single delivery state ℎ(cid:11)works can be formulated as follows,.and the hidden state ℎ(cid:11). How the LSTM(cid:13)(cid:11) (cid:14) σ (cid:15)(cid:11).(cid:17)(cid:9)(cid:11),ℎ(cid:11)(cid:18)1(cid:19)(cid:13)(cid:20) (cid:14) σ (cid:15)(cid:20).(cid:17)(cid:9)(cid:11),ℎ(cid:11)(cid:18)1(cid:19)(cid:13)(cid:21) (cid:14) σ (cid:15)(cid:11)(cid:21).(cid:17)(cid:9)(cid:11),ℎ(cid:11)(cid:18)1(cid:19)(cid:10)(cid:11) (cid:14) (cid:13)(cid:20)⨀(cid:10)(cid:11)(cid:18)1 (cid:23) (cid:13)(cid:11)⨀Zℎ(cid:11) (cid:14) (cid:13)(cid:21)⨀ tanh (cid:30)(cid:10)(cid:11)(cid:31)(1)(2)(3)(4)(5)where (cid:13), (cid:13)(cid:11), (cid:13)(cid:20)and (cid:13)(cid:21)represent the i-th input unit status, input gate, forgetting gate,and output gate, respectively. (cid:15)(cid:11)represent the (cid:11)-th input gate, forgettingrepresents the i -thgate, and output gate weight matrix, respectively. Moreover, ℎ(cid:11)hidden layer state, ⨀ represents Hadamard Product of matrix multiplication and " isthe sigmoid function.and (cid:15)0, (cid:15)(cid:20)From the above equations, one can note that the hidden state ℎ(cid:11)acquires information in the forward direction. We denote this using ℎ#⃗(cid:11)consists of LSTM modules for forward and reverse directions [7]. Compared with theof the LSTM only. BILSTMunidirectional LSTM model, including the bidirectional output [ℎ(cid:11)# ⃗# ,ℎ(cid:11)⃖ ## ] can better capturethe two-way semantic dependence. Most current mainstream models use the advantagesof BILSTM in the extraction of context information for feature extraction. In thedomain of biochemically named entities, long sequence named entities are prevalent,however they are not well recognized using the BERT model. For this reason, wesupplement the feature extraction of the BERT model and subsequently apply theBILSTM module to extract context information.2.4. The Multi-Head Attention moduleAtthe present, BILSTM is stillthe mainstream model of NER, which canmemorize long text sequence features in theory. However, due to its gradient diffusion9As mentioned previously,phenomenon, the application effect of BILSTM model needs to be improved [29]. Inorder to address this defect, attention mechanisms have been introduced into the field ofcomputer vision [30]. The mechanisms can further enhance the memory ability andfeature extraction ability of the BILSTM model for long sequence text, to a certainextent. They can also be used to address full-text label inconsistency and abbreviationrecognition, and reduce the difficulty of model training.there are a large number ofin biochemical NER,abbreviated entities and long sequence named entities. For abbreviated entities, mostauthors use relevant abbreviations in articles after the first description of the relevantentities. Such articles usually have detailed descriptions of entities in earlier sections,and ordinary models can correctly identify corresponding entities based on, for example,context specific information. In other cases, the author uses abbreviations withoutintroductions. Therefore, it is difficult for a conventional model to determine the correctentity label by contextual features. This may result in inconsistency in the labels of theentity throughout the entire article.For long phrase NER, there are also many long-named entities (such as cyclicin longer sentences. Such entities requireguanosine monophosphate) which existstronger model processing capabilities including long sequence feature extraction. Onepossible solution is the current mainstream BILSTM-CRF model, where the CRF layeris labelled according to the features extracted by BILSTM. However, the model usessentences as the basic processing unit and does not consider the features of the entiretext. Moreover, in the processing of longer sequence texts, LSTM has been proven tois weaker than the model of attentionhave a feature extraction capability thatmechanisms such as Transformer. Therefore, we adopt attention mechanisms toimprove recognition accuracy.As mentioned above, the BILSTM only focuses on modelling continuous contextdependencies and ignores discrete context patterns. Though, discrete context correlationplays an important role in sequence labelling tasks. In general, for a given word, itslabel depends not only on its own semantic information and neighbouring context, butalso on individual word information in the same sequence. This will greatly affect theaccuracy of labelling. The study by Tan et al. proved that the self-attention mechanismcan efficiently improve the performance on some NLP tasks, such as NER and POSl[31].The Google mind team used the attention mechanism to classify images based onthe RNN model [32]. Since then, the attention mechanism has been increasingly used inthe field of computer vision. The Google machine translation team applied a largenumber of self-attention mechanisms to learn text representation and yielded excellentresults[29].Inspired by these recent advances, we apply the Multi-Head Attention mechanismto obtain relevant chapter-level features in biochemical NER from multiple angles andlevels. By inputting the hidden layer output of the three settings of BILSTM, theinformation vector processed by the attention module is added by a certain weight toovercome the above disadvantages.10The Attention function takes a query and a set of key-value mappings as inputs, andoutputs a weighted sum of values. The weight assigned to each value is calculated bythe compatibility function of query and the corresponding key. The basis of theMulti-Head Attention used in this paper is the Scaled Dot-Product Attention mechanism,which uses the dot product to calculate the similarity between the query and eachkey[24]. The formula is given by&’((cid:7)’(cid:11)(cid:21)(cid:7) ),*,+ (cid:14) ,(cid:21)(cid:20)’-./(cid:30))*(cid:9)01(cid:31)+(6)where ), * and + represent matrix representations of query, key and value, respectively.The dot product is used to obtain the weight coefficient of the value corresponding toeach key. Weights represent the importance of information and determine which valuesshould be adjusted. The role of01is to control the dot product of Q and K, which isnot so large as to avoid the problem of gradient disappearance after the processing bysoftmax. The process is shown in Figure 4.Figure 4. Structure of the Multi-Head Attention module.Considering the necessity of extracting information from different representationsubspaces at different positions, splicing many different Scaled Dot-Product Attentionsresult in a Multi-Head Attention. The specific process is to linearly project ℎ times ofthe queries ), keys * and values + respectively.)ℎ(.0(cid:11) (cid:14) &’((cid:7)’(cid:11)(cid:21)(cid:7) )(cid:15)(cid:11),*(cid:15)(cid:11)21,+(cid:15)(cid:11)345’(cid:11)6(.0 ),*,+ (cid:14) (cid:10)(cid:21)(cid:7)7.’(cid:30)ℎ(.0(cid:11),…,ℎ(.0ℎ(cid:31)(7)(8)11), (cid:15)(cid:11)where (cid:15)(cid:11)and the Concat function concatenates the results.* and (cid:15)(cid:11)+ represent the corresponding weight matrices of ), * and +,In summary, the distinguishing feature of the multi-head attention mechanism fromthe traditional attention mechanism is thatto learn relevantinformation in different representation subspaces. This feature can be well adapted tothe recognition of the same entity in different contexts, especially the recognition ofpolysemy and abbreviations. Therefore, this paper integrates the Multi-Head attentionmechanism into the model to improve the consistency of its full-text label recognition.it allows the model2.5. The CRF moduleThe current literature regards the NER as a sequence labelling task. We apply theInside-Outside-Beginning (IOB) mechanism to mark the corpus. The structure of theannotation is shown in Table 1.Table 1. The Structure of IOB.Entity tagStart tagIntermediate tagEnd tagTRIVIALB-TRIVIALI-TRIVIALE-TRIVIALFAMILYB-FAMILYI-FAMILYE-FAMILY…NON-ENTITY…O…O…OLabels are not independent of each other, therefore, a key step is still to deal withthe relationship between tags after obtaining the labelled dataset. One possibility is theCRF model, which is a conditional probability model that can be used to label ordereddata. This model can handle the mutual constraint relationship between tags and solvethe problem of sequence labelling effectively. Therefore, we apply the CRF model toprocessing the output of the Multi-head Attention module. The Viterbi algorithm is usedto prediction and seeking the globally optimal annotation sequence[33].We denote the sentence sequence as /≔(cid:3)/1,…,/(cid:7)(cid:8) . After the processing via theabove module, we obtain a (cid:7) × - matrix 8, where (cid:7) is the number of input words andis the probability of the label (cid:11) of the- is the number of label types. The entry 8(cid:11)9word 9 occurring in the sentence. We denote :≔(cid:3):1,…:(cid:11),…:(cid:7)(cid:8) as a tag sequence, thusthe model computes the corresponding score:;7(cid:21)<((cid:30)/,:(cid:31) (cid:14)(cid:7)=(cid:11)(cid:18)1(cid:7)(cid:23)18(cid:11),:(cid:11) (cid:23)&:(cid:11)(cid:18)1:(cid:11)=(cid:11)(cid:18)1(9)12is the transition probability from :(cid:11)where &(cid:11)9the normalized probabilityto :9. We then apply softmax to obtain8 : / (cid:14)exp (cid:30),7(cid:21)<( /,: (cid:31):‚ exp (cid:30),7(cid:21)<((cid:30)/,:‚(cid:31)(cid:31)∑(10)For model training, the following maximized log likelihood function is used:5(cid:21)C8 :/ / (cid:14) ,7(cid:21)<( /,:/ (cid:18) log (cid:30)exp (cid:30),7(cid:21)<((cid:30)/,:‚(cid:31)(cid:31)(cid:31)(11)=:‚Finally, the model uses the Viterbi algorithm as follows to solve the optimal path inthe prediction process.:∗ (cid:14) arg -./:‚ ,7(cid:21)<((cid:30)/,:‚(cid:31)(12)3. Experimental setup3.1. DatasetThe dataset that we use is the CHEMDNER corpus, provided by the BioCreative(https://biocreative.bioinformatics.udel.edu/news/biocreative-iii/), which is aprojectcommunity-driven project for the evaluation of NLP tasks in the biological domain [19].It contains 84,435 manually labelled chemical entities. The details of the data setting areshown in Table 2.Table 2. Main entity categories and their numbers in the dataset.CategoryDevelopmentTrainingEntireTestAbstractssetting3,500Abstracts with CEM 2,916Nr.journalsNr.chemicalsNr.MentionsTRIVIALSYSTEMATICABBREVIATIONFORMULAFAMILYIDENTIFIERMULTIPLENO CLASS1938,52029,4788,8326,6564,5384,4484,09067220240settingsettingcorpus3,5002,9071888,67729,5268,9706,8164,5214,1374,223639188323,0002,47818810,0008,3012037,56319,80525,35184,3557,8085,66640593,4433,6225131994125,61019,13813,11812,02811,9351,82458911313It can be seen that the proportions of different types of entities in the text aredifferent, and some entities appear less frequently in the dataset, therefore we call themas low-frequency entities. The recognition accuracy of a low-frequency entity isrelatively poor because the number of samples is small and most of them are specialentities.3.2. Parameter settingsAlthough the BERT model can be fine-tuned, its optimization performance islimited. This hybrid model integrates BILSTM, MHATT and CRF;thehyperparameters need to be optimised. The model hyperparameters are presented inTable 3. Among them, the weight attenuation coefficient (weight_decay) is used toprevent over-fitting. The purpose of gradient cropping coefficient (clip) is to preventgradient explosion.therefore,Table 3. The hyperparameters and their values used in the BBMC model.ParametersDescriptionInitial learning rateLoss rateEmbedded layer dimensionlrdropoutembedding_dimenc_hidden_dim Coding layer dimensionlstm_dimkey_dimval_dimnum_headsweight_decayclipSingle layer LSTM dimensionThe size of * in the Attention moduleThe size of + in the Attention moduleThe size of I in the Attention moduleWeight attenuation coefficientGradient croppingValue0.0010.5768256128646430.0153.4. Performance comparisonTo evaluate the performance of the BBMC model, we compare its performancewith that of five state-of-the-art models: BILSTM-CRF (BC), BERT-BILSTM-CRFand(BBC), ELMO-BILSTM-CRFELMO-BILSTM-MULATT-CRF (EBMC).(EBC), BILSTM-MULATT-CRF(BMC),The bi-directional structure of BC can obtain the sequence information of context,therefore, it is widely used in tasks such as named entity recognition [34]. BERT inBBC can pre-train dynamic word vector which can express different semantics indifferent contexts [24]. ELMO in EBC, as a deep contextual word representation, canmodel complex features of words [35]. Attention mechanism is added to BC model toavoid complex feature engineering in traditional work [34]. The purpose of MULATT’slong attention mechanism in BMC is to replenish the key information of sequence frommultiple aspects [29].It is noted that BC, BBC, and EBC do not use the attention mechanism; BC andBMC use the GloVe word vector while BBC and EBMC use the ELMO pre-training14model; EBC and BBMC use the BERT pre-training model. To ensure the fairness, thehyper parameters of all models are consistent.In order to evaluate the model performance, the accuracy, recall and F-score areused as the evaluation metrics.4. Experimental results and discussion4.1. Abbreviations and polysemy labelsIn order to explain the inconsistency of full-text labelling and polysemy labellingmentioned above, this paper analyses different models for identifying special entities.Due to the space limitation, we illustrate with only two examples, as shown in Table 4.Table 4. Performance comparison of different entity identification methods (Left:effects recognized by the proposed BBMC model. Right: Effects recognized byBILSTM -CRF)ModelBBMCBILSTM-CRFThe fabrication of patternedThe fabrication of patternedmicrostructures inmicrostructures inpoly(dimethylsiloxane) (PDMS) is apoly(dimethylsiloxane)(PDMS) is aprerequisite for soft lithography .prerequisite for soft lithography . Herein ,Herein , curvilinear surface reliefcurvilinear surface relief microstructures inmicrostructures in PDMS arePDMS are fabricated through a simplefabricated through a simple three -three - stage approach combiningstage approach combiningmicrocontact printing ( CP ) , selectivemicrocontact printing ( CP ) , selectivesurface wetting / dewetting and replicasurface wetting / dewetting and replicamolding ( REM ) . First , using an originalAbbreviationmolding ( REM ) . First , using anPDMS stamp (first - generation stamp)original PDMS stamp (first -with linear relief features , ……Finally ,generation stamp) with linear reliefbased on a REM process , the PEG -dotfeatures , ……Finally , based on aarray on gold substrate is used to fabricateREM process , the PEG -dot array ona second - generation PDMS stamp withgold substrate is used to fabricate amicrocavity array , and the second -second - generation PDMS stamp withgeneration PDMS stamp is used tomicrocavity array , and the second -generate third - generation PDMS stampgeneration PDMS stamp is used towith microbump array .generate third - generation PDMSstamp with microbump array .15Silver was found to be only active inSilver was found to be only active in formform of free silver ions ( FSI ) .of free silver ions ( FSI ) .…………PolysemyHigh glucose insulin and free fattyHigh glucose insulin and free fatty acidacid concentrations synergisticallyconcentrations synergistically enhanceenhance perilipin 3 expression andperilipin 3 expression and lipidlipid accumulation in macrophages .accumulation in macrophages .Table 4 mainly marks the identification of two individual words: PDMS and free.The PDMS is an abbreviation of patterned microstructures in poly (dimethylsiloxane).The words in bold and underlined in Table 4, meaning to be correctly identified asentities. Table 4 shows that the BBMC model recognizes both the full name and theabbreviation. In contrast, the BILSTM -CRF model only correctly identifies the fullname. The reason is that BBMC model introduces an attention mechanism to extractchapter-level information, which ensures the label consistency of the full text. Theindividual word Free is a polysemous word. For example, in the phrase of free silverions, the free word is not an entity and is used to modify the entity silver ions. Thephrase of free fatty acid, as a fixed phrase, can be identified by the BBMC model. Inother words, the BBMC model can recognize polysemous words in different situations.However, the BILSTM-CRF model is relatively ineffective in recognizing polysemouswords due to lack of the representation ability of sentence features. This also proves thatthe BERT model can better adjust the word vector representation according to thecontext compared to pre-training models such as GloVe [36].4.2. Visual analysis of the BERT modelInstead of using the traditional word vector as the input, the BERT model uses theattention-based transformer structure. This section reports the analysis results about theattention distribution of BERT in NER. For better demonstration, this paper selects“systematic has been used for thousands of years, although systematic is clearly toxic tomost mammalian organ systems” as an input text and three experimental examples areshown in Figures 5(a)-(c). At the top of the figure, the user can select one or moreattention heads that are represented by the coloured squares. The word systematic in thepicture is the entity to be recognized. It can be seen in Figure 5(a) that the second“systematic” on the right pays more attention to “systematic” and “although” in front ofit and “is” behind it. From Figure 5(b), the first “systematic” on the right has a highdegree of attention to the “has”, “years” and “systematic”. Therefore, the subsequentmodel can determine the annotation type of the word by the part-of-speech and therelationship between the words before and after. Moreover, the attention mechanism ofcross sentence can also effectively improve the adaptability of subsequent models tolong sentences. In Figure 5(c), the word “although”, as a conjunction, focuses on theseparators,is veryimportant for NLP tasks, but for NER tasks, it results in active weakening of focusingon such words. To sum up, the BERT model has a stronger tendency of attention tothe relationship between sentences. This special effectthatis,16meaningful entities to be recognized, while weakening the attention of unimportantwords enhances the training efficiency and accuracy of the model.(a)(b)(c)Figure 5. Examples of attention-head views produced from BBMC. (a) and (b)demonstrate different attention-head views of the word “systematic” in differentpositions, and (c) depicts attention-head view for the conjunction of “although”.Self-attention is represented as lines connecting the tokens that are showing solicitudefor (left) with the tokens being attended to (right). Colours sign the appropriate attentionhead(s), while line weight mirrors the attention level.4.3. Low-frequency and high-frequency entity recognitionLow-frequency word entity recognition is more challenging than high-frequencyword entity recognition. Figure 6 shows the numbers of seven different entity datasetsABBREVIATION,(MULTIPLE,SYSTEMATIC, and TRIVIAL; see Section 3.1 for more details of the entity datasets)and the recognition accuracies (represented by F-value) for these entities by six differentmodels (BC, BBC, EBC, BMC, EBMC, BBMC, as shown in Table 5).IDENTIFIER,FORMULA,FAMILY,17Figure 6. The numbers of seven different categories of entities (Figure 6(a)) and therecognition accuracies (represented by F-value) for seven entity categories achieved bysix different models (Figure 6(b)). BC: BILSTM-CRF, BBC: BERT-BILSTM-CRF,EBC:EBMC:BMC:ELMO-BILSTM-MULATT-CRF, and BBMC: BERT-BILSTM-MULATT-CRF.BILSTM-MULATT-CRF,ELMO-BILSTM-CRF,Figure 6 demonstrates that the six models achieve higher accuracies in recognisinghigh-frequency words (such as SYSTEMTIC and TRIVIAL datasets). However, there isstill room for improving the recognition accuracy in the case of low-frequency words(such as MULTIPLE and IDENTIFIER datasets). The performance of BBMC and BBCshows improvements in the accuracy of recognizing low-frequency entities. Thissuggests the large-scale pre-training retention of the BERT model is able to supplementfive models in identifyingscarce training data. BBMC outperforms the otherlow-frequency entities. For example, compared with the recognition accuracies forlow-frequency entities produced by BC in MULTIPLE and IDENTIFIER datasets,those produced by BBMC are increased by 80% and 21.69%, respectively. Overall, forlow-frequency, middle-frequencyand(entitiesABBREVIATION datasets), and high-frequency words, BBMC nearly achieves the bestrecognition performance.FORMULA,FAMILY,in4.4. Performance comparison with existing modelsThree key findings can be drawn from the results in Table 5. Firstly, BMC, EBMC,and BBMC can achieve better performance by using the attention mechanism. Thisthe attention mechanism can eliminate data noise and combinedemonstrates thatchapter-level information to improve recognition accuracy. Secondly, a model canachieve a higher F-value by adding BERT or ELMO than the models adding GloVe.This demonstrates that dynamic adjustment of word vectors can improve the accuracyof polysemy recognition. Thirdly, a model which uses BERT will yield slightimprovement over a model which uses ELMO. This is mainly due to the fact that the18Transformer used by BERT is more powerful than the BILSTM feature extraction usedby ELMO.Table 5. Model performance comparison.AbbreviationBCBBCEBCBMCEBMCBBMCModelBILSTM-CRFBERT-BILSTM-CRFELMO-BILSTM-CRFBILSTM-MULATT-CRFELMO-BILSTM-MULATT-CRFBERT-BILSTM-MULATT-CRFP (%) R (%)87.7391.3188.689.988.989.290.1591.489.0590.989.991.8F (%)89.4889.289.0490.7789.9690.84The evaluation results show that the proposed hybrid model outperform existingmodels in the literature. The outstanding performance is achieved due to the followingreasons. First, the BERT model is introduced in this paper as a component/module,compared to the traditional use of the word2vec and GloVe, is capable of adjusting thevector of the target word according to the context, to some extent, and account for wordpolysemy. The Transformer is introduced in relation to the word vector, which providesstrong feature extraction capabilities. At the same time, the features retained by thelarge-scale pre-training model improve the recognition rate of entities in categorieswhich suffer from data scarcity. Second, we use Multi-Head Attention in addition toBILSTM to process specific information. The use of chapter-level information reducesthe occurrence of inconsistency in full-text entity tags. This also improves therecognition ability of the model for long sequence sentences. Third, in order to avoidthe final use of softmax to calculate the labelling results, we apply the CRF layer. Theuse of dependencies between tags improves the recognition rate.5. ConclusionWe propose a new hybrid deep-learning model for NER in biochemistry. Themodel is underpinned by innovatively integrating BERT, BILSTM, MHATT, and CRForganically. Through experiments and comparative analyses, we have shown that thenew model has outstanding performance in identifying abbreviations, polysemouswords, both low-frequency and high-frequency words, as well as ensuring theconsistency of full-text labels. This paper confirms the effectiveness of the multi-headattention mechanism in the field of naturallanguage processing to improve theperformance of models. At the same time, this paper further verifies that Transformer’sability to extract features is better than BILSTM, and Transformer has lower trainingdifficulty. Therefore, our future work will further make greater use of the Transformerinstead of BILSTM, in order to verify the advantages and stability of Transformercompared to BILSTM through different types of text and application fields, with theaim of achieving higher recognition accuracy.In addition, we have noticed in our research that when further performing the taskof relationship extraction, the accuracy difference of NER is further enhanced in theextracted results. This means that the error of the model may further propagate, and19hence expand in downstream tasks. To this end, the joint training of the model andrelationship extraction in this paper will effectively reduce the propagation error.AcknowledgmentsThis work was supported in part by the National Natural Science Foundation ofChina (No. 61906057, 61703306), the Fundamental Research Funds for the CentralUniversities, the Fundamental Research Funds for the Central Universities of China,and Natural Science Foundation of Anhui Province.References1234567891011Segler, M.H.S., Preuss, M., & Waller, M.P. 2018. Planning chemical syntheses withdeep neural networks and symbolic AI. Nature, 555, 604-610.Zhai, C.X., Li, T.J., Shi, H.Y., & Yeo, J.J. 2020. Discovery and design of softpolymeric bio-inspired materials with multiscale simulations and artificial intelligence.J Mater Chem B, 8, 6562-6587.Sun, X.L., Sun, S.L., Yin, M.Z., & Yang, H. 2020. Hybrid neural conditional randomfields for multi-view sequence labeling. Knowl-Based Syst, 189.Bukowski, M., Farkas, R., Beyan, O., Moll, L., Hahn, H., Kiessling, F., &Schmitz-Rode, T. 2020. Implementation of eHealth and AI integrated diagnostics withmultidisciplinary digitized data: are we ready from an international perspective? EurRadiol.Saha, S.K., Mitra, P., & Sarkar, S. 2012. A comparative study on feature reductionapproaches in Hindi and Bengali named entity recognition. Knowl-Based Syst, 27,322-332.Ronran, C., & Lee, S., 2020. Effect of Character and Word Features in BidirectionalLSTM-CRF for NER, 2020 IEEE International Conference on Big Data and SmartComputing (BigComp). Publishing, pp. 613-616.Alvarez, O., & Fernandez, J.L. 2019. Using artificial intelligence methods to speed updrug discovery. Expert Opin Drug Dis, 14, 769-777.Rill, N., Mukhortava, A., Lorenz, S., & Tessmer, I. 2020. Alkyltransferase-like proteinclusters scan DNA rapidly over long distances and recruit NER to alkyl-DNA lesions. PNatl Acad Sci USA, 117, 9318-9328.Goh, H.N., Soon, L.K., & Haw, S.C. 2013. Automatic dominant character identificationin fables based on verb analysis - Empirical study on the impact of anaphora resolution.Knowl-Based Syst, 54, 147-162.Grishman, R., & Sundheim, B., 1996. Message Understanding Conference- 6: A BriefHistory. Publishing, pp. 466-471.Kwak, B.K., & Cha, J.W. 2005. Named entity tagging for Korean using DL-CoTrainalgorithm. Lect Notes Comput Sc, 3689, 589-594.2012131415161718192021222324252627Liu, B., Gao, L., Li, B., Marcos-Martinez, R., & Bryan, B.A. 2020. Nonparametricmachine learning for mapping forest cover and exploring influential factors. LandscapeEcology, 35, 1683-1699.Horn, H., Schoof, E.M., Kim, J., Robin, X., Miller, M.L., Diella, F., Palma, A.,Cesareni, G., Jensen, L.J., & Linding, R. 2014. KinomeXplorer: an integrated platformfor kinome biology studies. Nature Methods, 11, 603-604.Brooke, J., Baldwin, T., & Hammond, A. 2016. Bootstrapped Text-level Named EntityRecognition for Literature. Proceedings of the 54th Annual Meeting of the Associationfor Computational Linguistics (Acl 2016), Vol 2, 344-350.Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. 2011.from Scratch. J Mach Learn Res, 12,Natural Language Processing (Almost)2493-2537.Zheng, J.Y., Xiao, X.G., Wang, B.H., Zhu, Y., & Yang, L.J. 2019. A New Method forAbbreviation Prediction via CNN-BLSTM-CRF. J Phys Conf Ser, 1267.Ma, X.Z., & Hovy, E. 2016. End-to-end Sequence Labeling via Bi-directionalLSTM-CNNs-CRF. Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics, Vol 1, 1064-1074.Chiu, J.P.C., & Nichols, E. 2015. Named Entity Recognition with BidirectionalLSTM-CNNs. CoRR, abs/1511.08308.Krallinger, M., Leitner, F., Rabal, O., Vazquez, M., Oyarzabal, J., & Valencia, A. 2015.CHEMDNER: The drugs and chemical names extraction challenge. Journal ofCheminformatics, 7, S1.Tang, Z., Wan, B., & Yang, L. 2020. Word-Character Graph Convolution Network forChinese Named Entity Recognition. IEEE/ACM Transactions on Audio, Speech, andLanguage Processing, 28, 1520-1532.Leaman, R., Wei, C. & Lu, Z. 2013. NCBI at the BioCreative IV CHEMDNER Task :Recognizing chemical names in PubMed articles with tmChem. Proceedings of thefourth BioCreative challenge evaluation workshop, 2, 8.Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. 2013. DistributedRepresentations of Words and Phrases and their Compositionality. Proceedings of the26th International Conference on Neural Information Processing Systems, 2, 9.Conneau, A., & Lample, G. 2019. Cross-lingual Language Model Pretraining. Advancesin Neural Information Processing Systems 32 (Nips 2019), 32.Cai, L.K., Song, Y., Liu, T., & Zhang, K.L. 2020. A Hybrid BERT Model ThatIncorporates Label Semantics via Adjustive Attention for Multi-Label TextClassification. Ieee Access, 8, 152183-152192.Hochreiter, S., & Schmidhuber, J. Long short-term memory. Neural computation, 9, 46.Jin, Y., Xie, J., Guo, W., Luo, C., Wu, D., & Wang, R. 2019. LSTM-CRF NeuralNetwork With Gated Self Attention for Chinese NER. IEEE Access, 7, 136694-136703.Ye, L., Gao, L., Marcos-Martinez, R., Mallants, D., & Bryan, B.A. 2019. ProjectingAustralia's forest cover dynamics and exploring influential factors using deep learning.Environ. Modell. Softw., 119, 407-417.21282930313233343536Huang, X., Gao, L., Crosbie, R.S., Zhang, N., Fu, G., & Doble, R. 2019. GroundwaterRecharge Prediction Using Linear Regression, Multi-Layer Perception Network, andDeep Learning. Water, 11, 1879.Kumar, A., Narapareddy, V.T., Aditya Srikanth, V., Malapati, A., & Neti, L.B.M. 2020.Sarcasm Detection Using Multi-Head Attention Based Bidirectional LSTM. Ieee Access,8, 6388-6397.Wei, B., Hao, K., Gao, L., & Tang, X. 2020. Bio-Inspired Visual Integrated Model forMulti-Label Classification of Textile Defect Images. IEEE Transactions on Cognitiveand Developmental Systems, 1-1.Tan, Z.X., Wang, M.X., Xie, J., Chen, Y.D., & Shi, X.D. 2018. Deep Semantic RoleLabeling with Self-Attention. Thirty-Second Aaai Conference on Artificial Intelligence /Thirtieth Innovative Applications of Artificial Intelligence Conference / Eighth AaaiSymposium on Educational Advances in Artificial Intelligence, 4929-4936.Lin, L., Luo, H., Huang, R.J., & Ye, M. 2019. Recurrent Models of Visual Co-Attentionfor Person Re-Identification. Ieee Access, 7, 8865-8875.Gao, X.Q., Liu, R.K., & Kaushik, A. 2021. Hierarchical Multi-Agent Optimization forResource Allocation in Cloud Computing. Ieee T Parall Distr, 32, 692-707.Wu, Tang, G.G., Wang, Z.R., Zhang, Z., & Wang, Z. 2019. An Attention-BasedBiLSTM-CRF Model for Chinese Clinic Named Entity Recognition. Ieee Access, 7,113942-113949.Huang, & Zhao, W. 2020. Combination of ELMo Representation and CNN Approachesto Enhance Service Discovery. Ieee Access, 8, 130782-130796.Wu, Zhao, S.L., & Li, W.B. 2020. Phrase2Vec: Phrase embedding based on parsing.Inform Sciences, 517, 100-127.22