Artificial Intelligence 322 (2023) 103951Contents lists available at ScienceDirectArtificial Intelligencejournal homepage: www.elsevier.com/locate/artintSpectral complexity-scaled generalisation bound of complex-valued neural networksHaowen Chen a,b,c, Fengxiang He d,b,∗a Department of Mathematics, ETH Zürich, 8092 Zürich, Switzerlandb JD Explore Academy, JD.com, Inc., Beijing, 100176, Chinac Department of Mathematics, Faculty of Science, The University of Hong Kong, Hong Kong Special Administrative Regiond Artificial Intelligence and its Applications Institute, School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, United Kingdome School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington NSW 2008, Australia, Shiye Lei e, Dacheng Tao e,ba r t i c l e i n f oa b s t r a c tArticle history:Received 12 November 2021Received in revised form 22 May 2023Accepted 27 May 2023Available online 5 June 2023Keywords:Complex-valued neural networksGeneralisationSpectral complexityComplex-valued neural networks (CVNNs) have been widely applied in various fields, primarily in signal processing and image recognition. Few studies have focused on the generalisation of CVNNs, although it is vital to ensure the performance of CVNNs on unseen data. This study is the first to prove a generalisation bound for complex-valued neural networks. The bounds increase as the spectral complexity increases, with the dominant factor being the product of the spectral norms of the weight matrices. Furthermore, this work provides a generalisation bound for CVNNs trained on sequential data, which is also affected by the spectral complexity. Theoretically, these bounds are derived using the Maurey Sparsification Lemma and Dudley entropy integral. We conducted empirical experiments on various datasets including MNIST, ashionMNIST, CIFAR-10, CIFAR-100, Tiny ImageNet, and IMDB by training complex-valued convolutional neural networks. The Spearman rank-order correlation coefficient and the corresponding p-values on these datasets provide strong proof of the statistically significant correlation between the spectral complexity of a network and its generalisation ability, as measured by the spectral norm product of the weight matrices. The code is available at https://github .com /LeavesLei /cvnn _generalization.© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons .org /licenses /by-nc -nd /4 .0/).1. IntroductionComplex-valued neural networks (CVNNs) have garnered significant attention in various fields, such as signal processing [1,2], voice processing [3], and image reconstruction [4]. To reduce complex operations, it is natural to link CVNNs to two-dimensional real-valued neural networks with fewer degrees of freedom [5,6]. A complex number consists of a real part and imaginary part, which can alternatively be expressed as amplitude and phase. When performing computations using complex numbers, distinct arithmetic operations are applied separately to the real and imaginary parts.Several recent studies endeavoured to investigate the different properties of CVNNs and built basic algorithms for their implementation. For example, Nitta [7,8,9,10] proved the orthogonality of the decision boundary of complex-valued neu-* Corresponding author at: Artificial Intelligence and its Applications Institute, School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, United Kingdom.E-mail address: F.He@ed.ac.uk (F. He).https://doi.org/10.1016/j.artint.2023.1039510004-3702/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons .org /licenses /by-nc -nd /4 .0/).H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951rones, addressed the redundancy problem of the parameters of CVNNs, extended the backpropagation algorithm to complex numbers, and Trabelsi et al. [11] organised the essential components of complex-valued deep neural networks, such as complex convolutions, complex batch normalisation, and complex weight initialisation. Empirical studies were conducted to examine the experimental performance of CVNNs. Hirose and Yoshida [5] used different neural networks, including CVNNs, to process signals of different coherence and Nitta [7] found that for the same computational cost, CVNNs display a higher learning speed than real-valued neural networks.Previous studies have shown satisfactory experimental performance for CVNNs. However, there is still a lack of theoretical analysis of their generalisation ability. This gap in understanding has motivated us to derive a generalisation bound for CVNNs.This is the first study to provide theoretical evidence for the generalisation performance of CVNNs. We propose novel upper bounds which positively correlate with the spectral complexity of CVNNs trained on both independent identically distributed (i.i.d.) and sequential data. The spectral complexity-scaled upper bounds suggest a direct correlation between the generalisation ability of CVNNs and the spectral norm product of their complex-valued weight matrices.From an empirical perspective, the experiments were conducted to investigate the influence of spectral complexity on the generalisation ability. Specifically, we trained CVNNs using stochastic gradient descent (SGD) on six standard datasets: CIFAR-10, CIFAR-100, MNIST, FashionMNIST, Tiny ImageNet, and IMDB. Excess risks were collected for analysis. When the training error is almost zero across all datasets, the excess risk equals the test accuracy and is informative in expressing gen-eralisation ability. In addition, because the change in the spectral norm product of the weight matrices primarily contributes to the change in spectral complexity, it is used to simulate spectral complexity. Our experimental results demonstrate a strong correlation between the spectral-norm product and excess risk, which is consistent with our theoretical analysis. The code is available at https://github .com /LeavesLei /cvnn _generalization.The remainder of this paper is organised as follows. Section 2 presents the motivation behind the research and pro-vides a review of related work. Section 3 provides an introduction to the preliminaries of complex-valued neural networks. Section 4 presents the theoretical results, while Section 5 presents the experimental results. In Section 6, a comparison is made between CVNNs and real-valued neural networks to explore the novelties and advantages of the proposed bound. In Section 7, the practical applications of the proposed theorems in spectral normalisation algorithms are discussed in detail.2. Motivation and related worksComplex values are widely adopted in different neural networks for their biological [12], computational [7,13], and representational advantages [14,15].From a biological perspective, Reichert and Serre [12] proposed that the complex-valued neuronal unit is a more ap-propriate abstraction in modelling the activity of neurones in the brain than a real-valued unit. To better process cortical information, the modelling mechanism must consider both firing rate and spike timing. In incorporating these two elements into deep neural networks, the amplitude of a complex-valued neuron represents the firing rate and the phase represents the spike timing. When two inputs of an excitatory complex-valued neuron have similar or dissimilar phase information, the magnitude of the net input may increase or decrease depending on whether the phases are similar, which correspond to synchronous and asynchronous situations, respectively. The incorporation of complex values into deep neural networks helps construct richer and more versatile representations.Regarding the computational aspect, Danihelka et al. [13] combined long short-term memory (LSTM) with the concept of holographic reduced representations and used complex values to increase the efficiency of information retrieval. Experiments showed that this method achieves a faster learning speed on multiple memorisation tasks. Nitta [7] extended the back-propagation algorithm to complex values, preserving the basic idea of real-valued back-propagation, with updates conducted on both real and imaginary parts. Through experiments, it was demonstrated that under the same time complexity, the learning speed of complex backpropagation is definitely faster than the real speed when the learning rate is low, that is, less than 0.5.Complex-valued neural networks also provide advantages over real-valued neural networks in terms of representational ability. Arjovsky et al. [14] proposed a unitary recurrent neural network (RNN) with unitary matrices as the weight matrix, to circumvent the well-studied gradient vanishing and gradient exploding issues. The unitary matrix is the generalised form of the orthogonal matrices in the complex field, and the absolute value of its eigenvalue is 1. Compared to an orthogonal matrix, a complex-valued matrix has a richer representation, particularly in applications of the discrete Fourier Transform. Wisdom et al. [15] further proposed full-capacity unitary RNNs, thereby improving the performance over unitary evolution RNN (uRNN).Given these advantages and applications of CVNNs, an increasing number of researchers have been investigating the properties of complex-valued neural networks to provide a basic framework for the implementation of CVNNs. Nitta [8]demonstrated that the decision boundary of a two-layered complex-valued network is orthogonal, and for a three-layered network, the decision boundary is nearly orthogonal. This reflects the computational power and versatility of complex values. In their work, Trabelsi et al. [11] provided the building blocks for complex-valued deep neural networks, including complex batch normalisation and complex weight initialisation strategies. They also compared the performances of different activation functions on three datasets: CIFAR-10, CIFAR-100, and SVHN.2H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951While there are studies presenting empirical results on the generalisation performance of complex-valued neural net-works [7,5], there is still a lack of theoretical evidence to support these findings. Therefore, our study aims to present the first upper bound for the generalisation error of CVNNs.Various complexity measures have been proposed to derive an upper bound for the generalisation error of real-valued neural networks, such as VC-dimension and Rademacher complexity [16]. Bartlett et al. [17] proved a margin-based multi-class generalisation bound based on covering number and Rademacher complexity. These two tools are also used in our work. Compared with Bartlett et al. [17]’s work, our work defines the spectral complexity of CVNNs by defining the spectral norm of a complex-valued matrix and providing generalisation bounds for complex-valued neural networks when processing regression tasks.3. PreliminariesThis section introduces complex-valued neural networks (CVNNs) and presents the notations used in the theoretical analysis.3.1. Model constructionEach layer of CVNN consists of several complex-valued neurones, as described below. The input signals, weight pa-rameters, threshold values, and output signals are all complex numbers in complex-valued neurones. Assuming that the complex-valued neurone n is linked with m neurones in the previous layer, the net input to this neurone n is described as follows:T ninput=m(cid:2)i=1W in Xin + Hn.Here, T ninput denotes the complex-valued network input of neurone n and W in denotes the weight connecting the n and i neurones from the previous layer. Xin denotes the complex-valued input signal from neurone i to neurone n and Hndenotes the threshold value of neurone n. If we denote Re(T ninput, input, respectively, then the output of neurone n can be input) as the real and imaginary parts of T ninput as the amplitude and phase of T ninput) and Im(T n(cid:3)(cid:3)(cid:3) and θ ninput(cid:3)(cid:3)(cid:3)T nrespectively, and described as follows:(cid:4)T noutput= frRe(T n(cid:5)input)(cid:4)(cid:5)input)Re(T n+ f iorT noutputi f p= e(cid:4)(cid:5)θ ninput(cid:4)(cid:3)(cid:3)(cid:3)T ninput(cid:3)(cid:5)(cid:3)(cid:3).fa(1)(2)Equation (1) describes the output derived by applying the activation function separately to the real and imaginary parts, whereas Equation (2) describes the situation when the activation function is applied to the amplitude and phase, where fr is the activation function applied to the real part; f i is the activation function applied to the imaginary part; f p is the activation function applied to the phase; and fa is the activation function applied to the amplitude.3.2. Complex-valued activation functionsSeveral forms of complex-valued activation functions corresponding to real-valued functions have been proposed.Arjovsky et al. [14] has proposed a modReLU activation function, which preserves the phase information and applies the real-valued ReLU function to the amplitude. This function is described as follows:modReLU(z) = ReLU(|z| + b)eiθz =(cid:6)(|z| + b) z|z|0if |z| + b ≥ 0otherwise.In this formula, |z| denotes the amplitude of the complex number z and b ∈ R denotes the threshold for the amplitude of z.Nitta [10] applied the hyperbolic tangent function to the real and imaginary parts of the input complex number to propose the following activation function:σ (z) = tanh(Re(z)) + i tanh(Im(z)),√where i =−1, tanh(u) def= (exp(u) − exp(−u))/(exp(u) + exp(−u)), u ∈ R.These two functions represent two main types of complex-valued activation functions with one applied to the real and imaginary parts, and the other applied to the amplitude and phase values. There are other variations of activation functions, 3H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951such as zReLU and CReLU [18]. These different activation functions have different properties in terms of satisfying the Cauchy-Riemann equations. Therefore, it is important to carefully select activation functions based on the specific task at hand and the characteristics of the data.3.3. Basic notations and definitionsSuppose S = {(z1, y1), (z2, y2), (z3, y3), ..., (zn, yn)|zi ∈ Z ⊂ Cd Z , yi ∈ Y ⊂ CdY } is the training sample set, where yi is the corresponding label of zi , d Z and dY are the dimensions of the z and y separately. We define D as a distribution following (zi, yi).Assume that the network has L layers, and in the ith layer, an ρi -Lipschitz activation function σi : Cdi → Cdi (activation functions such as the CReLU function and hyperbolic tangent function can be used here. Their Lipschitz properties are proven in Appendix A) and a weight matrix Ai ∈ Cdi−1×di is applied to the input matrix passed from the previous layer. Let σi(0) = 0, A = ( A1, A2, ..., A L), and FA be the function computed by CVNNs:F A(z) := σL ( A LσL−1 ( A L−1 · · · σ1 ( A1z))) ,(cid:7)(cid:8)with output FA(z) ∈ CdL. For input data {z1, z2, ..., zn}, a matrix Z ∈ Cn×d can be formed by collecting each zi in the ith row. Therefore, the output of this neu-ral network can be expressed as FA(Z T ), the ith column of which is FA(zi).it is assumed that d0 = d Z = d, dL = dY , and W = max{d0, d2, ..., dL}To avoid ambiguity, it is necessary to clarify the definition of a complex-valued matrix norm. The norm of any complex matrix [ Ai, j] ∈ Cd×k is defined as the norm of a corresponding real-valued matrix as follows:(cid:10)(cid:9)(cid:9)(cid:11)(cid:9)(cid:9)p(cid:9)(cid:9)(cid:10)(cid:3)(cid:3) Ai, j(cid:3)(cid:3)(cid:11)(cid:9)(cid:9)(cid:5)=p ,Ai, jwhere [ Ai, j] denotes the matrix whose i, jth entry is Ai, j . In this paper, the L2 norm is calculated entrywise, which means that the L2 matrix norm is defined to be the Frobenius norm, i.e.,(cid:12)(cid:2)(cid:2)(cid:3)(cid:3) Ai, j(cid:3)(cid:3)2.(cid:8) A(cid:8)2(cid:5)=ijMoreover, (cid:8)·(cid:8)σ denotes the spectral norm:(cid:8) A(cid:8)σ := sup=1(cid:8)v(cid:8)2(cid:13)(cid:8) A v(cid:8)2=λmax( A∗ A),∗denotes the Hermitian transpose of A, and λmax denotes the largest absolute value of eigenvalues of A. Meanwhile, where A(cid:8) A(cid:8)p,q is defined as:(cid:4)(cid:9)(cid:9) A:;1(cid:8) A(cid:8): (cid:5)=(cid:9)(cid:9)(cid:9)p,q(cid:9)(cid:9)p ,(cid:9)(cid:9) A:;2(cid:9)(cid:9)p , ...,(cid:9)(cid:9) A:;m(cid:9)(cid:9)(cid:5)(cid:9)(cid:9)(cid:9)qpfor A ∈ Cd×m.To investigate generalisation ability, it suffices to derive a high probability bound for the generalisation error:E(z, y)∼D[l(F A(z), y)] − 1nn(cid:2)i=1l(F A(zi), yi),where l(FA(z), y) : Z × Y → R denotes the loss function, which is usually set asl(F A(z), y) = ||F A(z) − y||2.Finally, the spectral complexity RA of neural network FA is defined as follows [17]:⎞RA :=ρi (cid:8) Ai(cid:8)σ(cid:14)L(cid:15)i=1(cid:16) ⎛⎝(cid:10)i(cid:9)(cid:9)(cid:9) A(cid:9)2/32,1(cid:8) Ai(cid:8)2/3σL(cid:2)i=13/2⎠.This complexity measure plays a crucial role in the generalisation bound presented in the next section.4H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 1039514. Main theorems and proof sketch4.1. Generalisation boundIn this section, the main theorems of this study are presented.Theorem 1 (Independent and identically distributed data). Let S = {(z1, y1), (z2, y2), (z3, y3), ..., (zn, yn)} be a sample dataset of size n with elements drawn independently and identically from distribution D. Given activation functions σi (σi is ρi -Lipschitz and σi(0) = 0) and weight matrices A = ( A1, A2, ..., A L), as stated in Section 3.3, then with probability of at least 1 − δ, the corresponding complex-valued neural network must satisfy the following:E(z, y)∼D[l(F A(z), y)] − 1nn(cid:2)i=1l(F A(zi), yi)√+ 36 (cid:8)Z (cid:8)2≤ 8Mn322 ln(2W ) ln(n)RAn(cid:12)ln 2δ2n,+ 3Mwhere l(FA(z), y) = ||FA(z) − y||2 denotes the loss function, and l(FA(z), y) ≤ M for any (z, y).It is observed that there is no explicit occurrence of any combinatorial parameters, such as L (the depth of neural networks). However, this upper bound depends on L implicitly, as RA is formed by each layer’s weight matrix norm and the Lipschitz constant of each layer’s activation function.The full proof is provided in Appendix B, and a proof sketch is presented in Section 4.2.Theorem 2 (Sequential data). Consider S = {(z1, y1), (z2, y2), (z3, y3), ..., (zn, yn)} as a sample dataset, where (zt)t≥1 is a sequence of random data adopted to filter (At)t≥1. Given the activation functions σi (σi is ρi -Lipschitz and σi(0) = 0), and weight matrices A = ( A1, A2, ..., A L), as stated in Section 3.3, then with a probability of at least 1 − δ, the corresponding complex-valued neural network must satisfy the following:1nn(cid:2)t=1(E [l (zt, yt) | At−1] − l (zt, yt))√+ 24 (cid:8)Z (cid:8)2≤ 8Mn2 ln(2W ) ln(n)RAn(cid:12)+ Mln 2δ2n,where l(z, y) = ||FA(z) − y||2 denotes the loss function, and l(z, y) ≤ M for any (z, y).Theorem 2 illustrates the generalisation capability of complex-valued neural networks when dealing with sequential data. The proof sketch of this theorem is omitted from the main text because there exists some overlap with Theorem 1; however, the full proof is shown in Appendix D. In the Appendix, we also present the definitions of sequential Rademacher com-plexity, sequential covering number, and sequential Dudley entropy integral, which were proposed in the work of Rakhlin et al. [19].4.2. Proof sketchIn this section, we provide the proof of Theorem 1, using the following lemmas:The proof is presented in three steps: I) An upper bound for the covering number is obtained: N ({Z A : A ∈Cd×m, (cid:8) A(cid:8)≤ a, (cid:8)}), as in Lemma 1. II) Starting with a single layer and applying the induction method, an upper bound for the covering number of the entire network is derived. This result is illustrated in Lemma 2. III) The upper bound of Rademacher complexity is derived via Dudley entropy integral and the above covering number bound, further combining this bound with Theorem 3.q,sIn preparation for the proof, we first state Theorem 3, which is a crucial tool for step III. This theorem derives the generalisation bound for regression in the case of L p loss function through Rademacher complexity. Let us recall the theorem presented by Mohri et al. [16].Theorem 3 ([16]). Let l : Z × Y → R be an L p loss function bounded by M > 0; F be the hypothesis set; family G = {(x, y) (cid:12)→l(FA(x), y) : FA ∈ F }, then for any δ, with probability at least 1 − δ, the following inequality holds:(cid:12)E(x, y)∼D[l(x, y)] ≤ 1mm(cid:2)i=1l (xi, yi) + 2 ˆRS (G) + 3Mlog 2δ2m5H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951where ˆRS (G) denotes the empirical Rademacher complexity of family G.Obviously, to bound the generalisation error, it suffices to derive an upper bound for the Rademacher complexity of the loss function family G = {(x, y) (cid:12)→ l(FA(x), y) : FA ∈ F }, which is realised through the first and second steps. In the following paragraphs, we illustrate the proof in detail, in three steps.Step IIn this step, we obtain a matrix covering for the set of matrix products Z A ( Z represents the data matrix passed to the present layer, and A is instantiated as the weight matrix) under L2 norm. Lemma 1 is derived as follows:Lemma 1. (p, q) and (r, s) are two conjugate exponents with p ≤ 2. Let a, b and (cid:8) be three positive real numbers, and let d and m be two positive integers. Impose a constraint on the norm of Z such that (cid:8)Z (cid:8)p(cid:24)≤ b. Therefore, we have(cid:23)(cid:22)(cid:5)(cid:4)(cid:21)ln NZ A : A ∈ Cd×m, (cid:8) A(cid:8)q,s ≤ a, (cid:8), (cid:8) · (cid:8)2≤a2b2m2/r(cid:8)2ln(4dm).The proof of Lemma 1 is based on the Maurey sparsification lemma. This lemma inspired us to cover the targeting set using a sparsifying convex hull of complex-valued matrices, which is constructed using the product of the rescaled data j . Moreover, to prove Theorem 1, constraints are imposed on || A||2,1matrix Z [20] and some “standard matrices” such as ei eT(i.e. q = 2, s = 1), instead of || A||2, which helps avoid the occurrence of combinatorial numbers such as L and W outside the log term in the upper bound [17].Step II After obtaining the matrix covering upper bound in Step I, the idea is extended to the proof of the entire network covering number upper bound, thereby obtaining Lemma 2. The proof of Lemma 2 relies on induction and on Lemma 1.Lemma 2. (σ1, σ2, σ3, ..., σL) are fixed activation functions with each σi being ρi − Lipschitz. The spectral norm bound of matrix Aiis denoted by si and the matrix (2,1) norm is denoted by bi (i ∈ {1, 2, ..., L}). Given Z as the fixed data matrix, where Z ∈ Cn×d, and each row denotes a group of data points, for any (cid:8), we have⎞⎛ln N (F, (cid:8), (cid:8) · (cid:8)2) ≤(cid:21)(cid:7)(cid:8)(cid:7)(cid:8)(cid:8)Z (cid:8)24W 22 ln(cid:8)2L(cid:15)⎝j=1⎠j ρ2s2j(cid:14)(cid:25)L(cid:2)(cid:16)3(cid:26)2/3bisi,(cid:22)i=1(cid:9)(cid:9)(cid:9)(cid:9) A(cid:10)iwhere F :=complex-valued neural networks FA, and W denotes the maximum of {d0, d1, ..., dL}.: A = ( A1, . . . , A L) , (cid:8) Ai(cid:8)σ ≤ si,≤ biFAZ T2,1is the family of outputs generated by feasible choices of In general, we separate the proof of Lemma 2 into two parts: The first part determines the relationship between the entire network upper bound and the matrix covering bounds of the previous L layers, which is addressed in Appendix B.2Lemma 6. The second part combines Lemmas 1 and 6, which together provide Lemma 2 through the induction technique.Step IIISince we are only deriving a bound for the covering number of the whole network N (F , (cid:8), (cid:8)·(cid:8)2) is not sufficient. We still have to derive an upper bound for the empirical Rademacher complexity of the loss function family ˆRS (G). It is natural to connect these two concepts using the Dudley entropy integral. However, some preparation work is required to satisfy the condition for using the standard Dudley entropy integral.The standard Dudley entropy integral only illustrates the relationship between N (G, (cid:8), (cid:8)·(cid:8)2)has upper bounds N (G, (cid:8), (cid:8)·(cid:8)2) by N (F , (cid:8), (cid:8)·(cid:8)2) and ˆRS (G). Hence, Lemma 3Lemma 3. Given family F := {FA(Z ) : A ∈ B1 × · · · × BL} and family G := {(z, y) (cid:12)→ l (FA(z), y) : FA ∈ F }, the covering number of these two families satisfyN (F, (cid:8), (cid:8)·(cid:8)2) ≥ N (G, (cid:8), (cid:8)·(cid:8)2),Let l (FA(z), y) = (cid:8)FA(z) − y(cid:8)2.(3)Moreover, because the range of the loss function that we adopt does not lie in [0, 1], Lemma 4 calculates the covering number after rescaling.Lemma 4. If a coefficient, say α > 0, is multiplied by the targeting set G and distance constant (cid:8), then the covering number remains unchanged, that is,N (G, (cid:8), (cid:8)·(cid:8)2) = N (αG, α · (cid:8), (cid:8)·(cid:8)2).Here, αG represents a set obtained by scaling α to each element in G.6H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Using Lemmas 3 and 4, the Rademacher complexity of G can be bounded through the Dudley entropy integral. We prove Theorem 1 by substituting ˆRS (G) into Theorem 3 for the value of the upper bound obtained. A detailed proof is provided in Appendix B.3.5. Results of experimentsIn this section, we present the experimental results of training complex-valued neural networks using SGD on six datasets: MNIST, FashionMNIST, CIFAR-10, CIFAR-100, Tiny ImageNet, and IMDB.Before presenting the experimental results, a summary of the two upper bounds is derived in Theorems 1 and 2. Both for the independent identically distributed and sequential data cases, we showed that the derived upper bound scales with the spectral complexity of the complex-valued neural network:RA :=(cid:14)L(cid:15)i=1ρi (cid:8) Ai(cid:8)σ(cid:16) ⎛⎝(cid:10)i(cid:9)(cid:9)(cid:9) A(cid:9)2/32,1(cid:8) Ai(cid:8)2/3σL(cid:2)i=1⎞3/2⎠.(cid:14)(cid:25)(cid:28)(cid:16)Li=1(cid:14)(cid:25)(cid:10)i(cid:26)3/2(cid:9)(cid:9)(cid:9) A(cid:9)2/32,1(cid:8) Ai (cid:8)2/3σ(cid:9)(cid:9)(cid:9) A(cid:9)2/32,1(cid:8) Ai (cid:8)2/3σLi=1(cid:10)i(cid:28)are equivalent, the factor we assume that the change in change in RA is (cid:4)(cid:27)(cid:5)Li=1 ρi (cid:8) Ai(cid:8)σThe formula for the spectral norm RA consists of two parts: the Lipschitz constant of this neural network and another factor related to the sum of quotients of weight matrix norms . Since the two norms (cid:4)(cid:27)(cid:5)Li=1 ρi (cid:8) Ai(cid:8)σ(cid:14)(cid:25)(cid:28)Li=1(cid:10)i(cid:9)(cid:9)(cid:9) A(cid:9)2/32,1(cid:8) Ai (cid:8)2/3σ(cid:16)(cid:26)3/2remains in the interval [C1, C2] for some constants C1 and C2. Therefore, (cid:16)(cid:26)3/2is minor. Then, in the training process, the part which dominates the of activation functions (ρi ) remain unchanged. Therefore, we use the change in the spectral norm product (simulate the changing trend in RA.for the Lipschitz constant of the neural network. This is because the Lipschitz constants (cid:8) Ai(cid:8)σ ) to Li=1(cid:27)5.1. Spectral norm of the weight matrixFirst, the calculation of the spectral norm of the complex weight matrix in each convolutional layer is demonstrated.Considering the complex-valued kernel W = X + Y i in each layer, where X and Y are real-valued kernels, because each convolutional kernel corresponds to a matrix transformation [21], the real-valued weight matrices of kernels X and Y can be derived, which are denoted by C and D. Hence, the complex-valued weight matrix A of each layer can be expressed as C + Di. Then, by definition, the spectral norm of the complex-valued matrix A is:(cid:13)(cid:8) A v(cid:8)2 =λmax ( A∗ A)(cid:8) A(cid:8)σ : = sup(cid:8)v(cid:8)2=1(cid:29)=λmax(cid:7)C T C + D T D + (C T D − C D T )i(cid:8).Here, because A∗A is a Hermitian matrix, it has only real eigenvalues.5.2. Complex-valued convolutional neural networksIn our experiments, we trained complex-valued convolutional neural networks (CVCNN) and complex-valued multi-layer perceptrons (CVMLP) on different datasets. The IMDb dataset was used for training the CVMLP. The proposed complex-valued neural network architectures are described in detail in Appendix E.2.The CVCNN architecture consists of three types of layers: convolutional, maxpooling, and fully connected layers. The last layer is a CVMLP. The convolutional and maxpooling layers in CVCNN are analogous to the hidden layers in CVMLPs, as is the case in real-valued neural networks [22]. The operations on the convolutional and maxpooling layers can be expressed as matrix-vector multiplication, as shown in [23]. To analyze these results, we considered the convolutional and maxpooling layers separately.5.2.1. Matrix multiplication interpretation of convolution operationsFirst, we interpret the convolution process using matrix multiplication.The convolutional layer usually receives as input a matrix of dimensions [h1 ∗ w 1 ∗ d1], which is a 3D matrix with length h1, width w 1, and height d1. Further, the kernels are defined as follows. Assuming that the number of output channels is d2, we have d2 kernels, which can be written as a matrix with dimensions [h2 ∗ w 2 ∗ d1]. Finally, we assume that the output matrix has dimensions [h3 ∗ w 3 ∗ d2]. Fig. 2 shows the input, kernel, and output matrices of the convolution process.7H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951The relationship between the output and input matrix dimensions can be expressed as follows:w 3 = (w 1 − w 2 + 2p + 1)/sh3 = (h1 − h2 + 2p + 1)/swhere p denotes the padding number, and s indicates the stride number. Without loss of generality, we assume that p = 0and s = 1. The next step is to flatten the input matrix into a vector and write these d2 3D kernel matrices into a 2D matrix.Because the input matrix is of dimension [h1 ∗ w 1 ∗ d1], it can be flattened into a vector x ∈ Rh1 w1d1 such that(cid:5)(cid:4)x =(1,1), x1x1(1,2), . . . , xd(h,w), . . . , xd1(1,1), . . . , xd1(h1,w 1)T.Here, xdIn terms of constructing the kernel matrix, we first consider the convolutional matrix K i(h,w) denotes the (h, w, d)th entry of the input matrix (0 < h ≤ h1, 0 < w ≤ w 1, 0 < d ≤ d1).(cid:4)(h,1,d), . . . , kiki(h,d) formed by the vector h,w,d denotes the (h, w, d)th entry of the ith kernel (0 < h ≤ h2, 0 < w ≤ w 2, ∈ Rw2 is ki(h,d)0 < d ≤ d1. 0 < i ≤ d2). The convolutional matrix K iformed as follows:(h,d) induced by the vector ki(cid:4)(h,1,d), . . . , kiki∈ Rw2 , where ki(h,w2,d)(h,w2,d)(h,d)==(cid:5)(cid:5)⎡⎢⎢⎢⎢⎣(h,2,d) ki(h,1,d) kiki(h,1,d) kiki0. . .. . ....0(h,3,d)(h,2,d) ki. . .0. . .. . .(h,3,d)ki(h,1,d)ki(h,w 2,d). . .. . .ki(h,2,d)0ki(h,w 2,d). . .ki(h,3,d)and K i(h,d) is the Toeplitz matrix in R(w1−w2+1)×w1 .The second step is to construct the convolutional matrix K iR(h1−h2+1)(w1−w2+1)×h1 w1 , which is constructed by considering K i. . .. . .. . .. . . ki⎤⎥⎥⎥⎥⎦,00...(h,w 2,d)d induced by matrix K i(h,d). K id is a Toeplitz block matrix in (h,d) as its components. We can write K id as follows:⎡⎢⎢⎢⎢⎣K i. . .(2,d) K i(1,d) K iK i(3,d)(h2,d)(2,d) K i(1,d) K iK i. . .0. . .. . .. . .. . .(1,d) K iK i. . .0(3,d)(2,d)...00K i(h2,d). . .K i(3,d). . .. . .. . .. . . K i⎤⎥⎥⎥⎥⎦00...(h2,d)Therefore, the entire convolutional matrix K ∈ Rd2(h1−h2+1)(w1−w2+1)×h1 w1d1 induced by the d2 kernels, can be viewed as a block matrix by taking K id as its components. It is constructed as follows:K =⎡⎢⎢⎢⎢⎣K 11K 21...K d21K 12K 22. . .K d22⎤⎥⎥⎥⎥⎦. . . K 1d1. . . K 2d1.... . .. . . K d2d1Consequently, we regard K as the weight matrix of the convolutional layer and apply its spectral norm to calculate the generalisation bound.For the maxpooling layer, there exists a weight matrix with entries of either 0 or 1, depending on which entry x is (1,1) (1 ≤ d ≤ d1) is reserved, then the weight matrix M ∈reserved for the maxpooling process. For instance, if each xdRd1×h1 w1d1 of the maxpooling layer is⎡⎢⎢⎢⎣1 0 . . . 0 0 . . . 0 0 . . . 00 0 . . . 1 0 . . . 0 0 . . . 0.........0 0 . . . 0 0 . . . 1 0 . . . 0.....................⎤⎥⎥⎥⎦ .M is a sparse matrix and its (i, ih1 w 1) entry equals 1.In conclusion, CVCNNs are a type of complex-valued neural network that still fits the theoretical analysis presented in Section 4.8H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Fig. 1. Plots of excess risk and spectral norm product (SN prod) as functions of the epoch. The right y-axis denotes the spectral norm product, and the left y-axis denotes the excess risk.Fig. 2. The input matrix, kernel, and output matrix in a convolution process.5.3. ResultsThe architectures of the complex-valued neural networks of this study are described in Appendix E.2. The datasets used were MNIST, FashionMNIST, CIFAR-10, CIFAR-100, Tiny ImageNet, and IMDB. Descriptions of these datasets are presented in Appendix E.1. We trained CVCNNs using SGD on MNIST, FashionMNIST, CIFAR-10, CIFAR-100, and Tiny ImageNet to investi-gate the generalisation bound derived in Theorem 1 and the CVNN trained on IMDB to investigate the generalisation bound derived in Theorem 2 for sequential training data. The results are presented in Fig. 1.Fig. 1 displays excess risk and spectral norm product as a function of epoch. In addition, we performed Spearman rank-order correlation tests on all excess risks and spectral norm products of MNIST, FashionMNIST, CIFAR-10, CIFAR-100, IMDB, and Tiny ImageNet. Spearman’s rank-order correlation coefficients (SCCs) and p-values show that the correlation between the spectral norm product and generalisation ability is statistically significant (p < 0.0051), as listed in Table 1. These results strongly support our theoretical findings.1 The definition of “statistically significant” has various versions, such as p < 0.05 and p < 0.01. This paper used a more rigorous approach (p < 0.005).9H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Table 1SCC and p values of the spectral norm product and excess risk.CIFAR-10SCCpCIFAR-100SCCpMNISTSCCp0.99IMDB3.703 × 10−2280.804.124 × 10−230.999.044 × 10−142FashionMNISTTiny ImageNetSCCpSCCpSCCp0.996.118 × 10−1940.993.703 × 10−1420.994.060 × 10−1256. Comparison with real-valued neural networks6.1. Theoretical analysisIn previous studies, Bartlett et al. [17] proved that the generalisation bound of real-valued neural networks (RVNNs) is positively correlated with spectral complexity. In his work, spectral complexity is defined as the product of real-valued weight matrix spectral norms, as defined in the following formula, where Ai denotes the weight matrices of real-valued neural networks, and Mi denotes the reference matrices.(cid:16) ⎛⎝(cid:9)(cid:9)2/32,1(cid:9)(cid:9) AL(cid:2)L(cid:15)(cid:10)i(cid:10)i3/2⎞⎠(cid:14)RA :=ρi (cid:8) Ai(cid:8)σi=1i=1− M(cid:8) Ai(cid:8)2/3σThe generalisation bound we presented for CVNNs is similar to the one for real-valued neural networks (RVNNs) pre-sented in [17]. Theorem 4 illustrates the relationship between the generalisation bounds of RVNNs and their spectral complexity.(cid:7)Theorem 4 ([17]). Let nonlinearities (σ1, . . . , σL) and reference matrices be given as above (that is, σi is ρi -Lipschitz and σi(0) = 0). Then for (x, y), (x1, y1), . . . , (xn, yn) drawn i.i.d. from any probability distribution over Rd × {1, . . . , k}, with probability at least 1 − δ over ((xi, yi))ni=1, every margin γ > 0 and network FA : Rd → Rk with weight matrices A = ( A1, . . . , A L) satisfyM1, . . ., M L(cid:8)$%(cid:14)Prarg maxjF A(x) j (cid:13)= y≤ &Rγ (F A) + ’O(cid:8)X(cid:8)2 RAγ nwhere &Rγ ( f ) ≤ n−1(cid:28)i 1)f (xi ) yi≤γ +max j(cid:13)= yif (xi ) j*and (cid:8) X(cid:8)2 =((cid:16)ln(1/δ)nln(W ) +(cid:29)(cid:28)(cid:8)xi(cid:8)22.i(4)The main difference between the generalisation bounds of CVNNs and RVNNs lies in how the spectral complexity and spectral norm of the weight matrix are defined. In our theorem, we redefine the spectral norm of a complex-valued matrix, instead of reformulating the complex-valued matrix into a constrained real-valued matrix to calculate its spectral norm. By doing so, we preserve the unique structure of complex-valued matrices and avoid increasing computational costs. Reformu-lating a complex-valued matrix into a real-valued matrix can result in a considerable increase in the matrix dimensions, making the calculation of its spectral norm more complicated. Additionally, our contribution lies in proving that the gener-alisation bound is positively correlated with the defined spectral complexity, which inspired us to consider the effectiveness of spectral normalisation in CVNNs, as discussed in Section 7.6.2. Empirical analysisEmpirical comparisons of CVNNs and RVNNs have been conducted in various forms in the literature. For instance, Trabelsi et al. [11] controlled the number of parameters of both RVNNs and CVNNs with a trade-off on width and depth, while Nitta [9] compared the learning speed of CVNNs and RVNNs under the same architecture or with a controlled number of parameters. However, it is challenging to impose a fair constraint on CVNNs and RVNNs during comparison due to the abundance of studies on real-valued neural networks compared to the limited work on investigating the best architecture for complex-valued neural networks. Although this study presents two methods and roughly compares the generalisation ability of CVNNs and RVNNs, the results cannot be used as conclusive evidence to indicate which type of neural network is better. A fair comparison of their performances remains an open problem.In this study, we compared the performance of CVNNs with those of different RVNNs. The first type of RVNN has the same number of parameters as CVNN, while the second type has the same architecture as CVNN. The results are presented in Fig. 3.10H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Fig. 3. Change in excess risk with the number of epochs when complex-valued and real-valued neural networks are trained on different datasets. The solid line represents the performance of complex-valued neural networks. The dashed line represents the performance of real-valued neural networks with the same number of parameters. The dotted line represents the performance of real-valued neural networks with the same architecture.As shown in Figure 3, it is difficult to draw conclusions when comparing the generalisation performance of complex-valued and real-valued neural networks. Controlling for the architecture, the generalisation ability of real-valued neural networks is superior to that of complex-valued neural networks. However, when controlling for the number of parameters, the performance of CVNNs is better when trained on CIFAR-10 and CIFAR-100.7. ApplicationsIn this section, we demonstrate that the spectral regularisation algorithm is a practical application of our theory. Be-cause the generalisation bound of complex-valued neural networks correlates positively with the product of the spectral norms of the weight matrices, it is natural to investigate whether adding a regularisation term of the spectral norm to the loss function decreases excess risk in real-life training of complex-valued neural networks. We conducted experiments on MNIST, FashionMNIST, CIFAR-10, CIFAR-100, IMDB, and TingImageNet. The empirical results show that applying the spectral regularisation algorithm decreases excess risk significantly, fully supporting our idea. The spectral regularisation algorithm is presented in Algorithm 1. The empirical results are presented in Fig. 4.8. ConclusionsIn this study, we propose two generalisation bounds for complex-valued neural networks for i.i.d. and sequential data. Our analysis shows that the bounds scale with spectral complexity, which includes the spectral norm product of weight matrices as a factor. We also provide empirical evidence to support our theoretical findings. Our work contributes to the understanding of the generalisation ability of complex-valued neural networks and encourages further exploration of their unique properties.Declaration of competing interestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.11H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Fig. 4. Plots of excess risk as a function of epoch. The right y-axis denotes the spectral norm product, and the left y-axis denotes the excess risk. The solid line denotes CVNNs without spectral regularisation, whereas the dashed line denotes the results after adding spectral regularisation.Algorithm 1: SGD with complex-valued spectral regularisation.Input: Initialised network parameter θ 0, # training epoch N, spectral regularisation factor λ, learning rate αOutput: Optimised network parameter θ Nfor t = 0 to N − 1 doSample a minibatch {(xi , yi )}kfor l = 1 to L doi=1 from the training data.Express the l-th layer weight matrix Al as Cl + Dl i where Cl and Dl are real-valued matricesCompute the layer-wise complex-valued spectral norm, (cid:8) Al(cid:8)σ =l Dl − Cl D TC Tl Cl + D TC Tl Dl +λmax(cid:29)(cid:7)(cid:7)l(cid:8)(cid:8)iendCompute Lt = 1Update θ t+1 = θ t − α∇θ t(cid:28)kkLti=1 (cid:11)( fθ t (xi ), yi ) + λ Li=1(cid:8) Ai (cid:8)σ(cid:28)endData availabilityData will be made available on request.AcknowledgementsMr Shiye Lei was supported in part by Australian Research Council Projects FL170100117 and IH180100002.Appendix A. Lipschitz properties of several activation functionIn this section, our goal is to prove three types of activation functions that are widely used in complex-valued neural networks being Lipschitz continuous.The first one is introduced in [10]:σ1(z) = tanh(Re(z)) + itanh(Im(z)).12H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951This activation function applies the hyperbolic tangent function on both the real part and the imaginary part. Since the derivative of the hyperbolic tangent function is upper bounded by 1, hence we can see that σ1 is 1-Lipschitz in each coordinate, if we view the real part and imaginary part as different coordinates. Then we have(cid:9)(cid:9)σ1 (z1) − σ1)(cid:10)(cid:8)(cid:9)(cid:9)(cid:7)(cid:15)1zptanh (Re (z1)) − tanh(cid:8)(cid:8)(cid:11)(cid:7)Re(cid:7)(cid:15)1zp +)(cid:7)Re (z1) − Re(cid:9)(cid:9)(cid:15)i(cid:9)(cid:9)zi − zp .(cid:8)(cid:8)(cid:7)(cid:15)1z(cid:7)p +Im (z1) − Im=≤=(cid:10)tanh (Im (z1)) − tanh(cid:7)(cid:8)(cid:8)* 1pp(cid:15)1z(cid:8)(cid:8)(cid:11)* 1pp(cid:7)Im(cid:7)(cid:15)1zThe first inequality holds because the hyperbolic tangent function is 1-Lipschitz.The second type of activation function is the C ReLU function introduced in [11]:σ2(z) = ReLU (Re(z)) + i ReLU (Im(z)).This function also operates separately on both the real part and the imaginary part. The proof process is quite similar to the first one since the ReLU function is also 1-Lipschitz.(cid:8)(cid:9)(cid:9)(cid:7)(cid:9)(cid:9)σ1 (z1) − σ1)(cid:10)(cid:15)1zptanh (Re (z1)) − tanh(cid:8)(cid:8)(cid:11)(cid:7)Re(cid:7)(cid:15)1zp +)(cid:7)Re (z1) − Re(cid:9)(cid:9)(cid:15)i(cid:9)(cid:9)zi − zp .(cid:8)(cid:8)(cid:7)(cid:15)1z(cid:7)p +Im (z1) − Im=≤=(cid:10)tanh (Im (z1)) − tanh(cid:7)(cid:8)(cid:8)* 1pp(cid:15)1z(cid:8)(cid:8)(cid:11)* 1pp(cid:7)Im(cid:7)(cid:15)1zThe third type of activation function isσ3(z) = tanh(|z|) exp(iθ),where θ = arg(z). If we write z = x + yi and use vector notation to represent the real part and imaginary part of the operation, we will get⎡⎢⎣,=+Re(σ3(z))Im(σ3(z))tanh[(x2 + y2)tanh[(x2 + y2)12 ]12 ]x(x2+ y2)y(x2+ y2)1212⎤⎥⎦ .Notice that(cid:3)(cid:3)(cid:3)(cid:3)tanh[(x2 + y2)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)≤ 1.12 ]1(x2 + y2)12Hence, we have the following inequality11(cid:3)(cid:3)(cid:3)(cid:3)tanh[(x2(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)tanh[(x2(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)tanh[(x2(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)1≤+=+ y21)tanh[(x21+ y21)(x2112(x21(x21+ y21)12 ]+ y21)12 ]+ y21)12 ]x1+ y21)x1+ y21)x2+ y21)121212(x21(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)12 ]|x1 − x2| +− tanh[(x22+ y22)12 ]− tanh[(x21+ y21)12 ]x2+ y22)x2+ y21)1212(x22(x21− tanh[(x22+ y22)12 ](cid:3)(cid:3)(cid:3)(cid:3)(cid:3)12 ]+ y21)tanh[(x21+ y21)(x211212x2+ y2(x22)2− tanh[(x2(x22(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)Noted that, assume g(x) = tanh(x)x, then through calculation, we have 13+ y22)12 ](cid:3)(cid:3)(cid:3)(cid:3)(cid:3)12|x2|.2+ y22)(cid:3)(cid:3)(cid:3) bounded by 1. Hence, g(x) is 1-Lipschitz.(cid:3)g(cid:15)(x)H. Chen, F. He, S. Lei et al.Therefore, we haveArtificial Intelligence 322 (2023) 10395112 ](cid:3)(cid:3)(cid:3)(cid:3)(cid:3)|x2|+ y22)2+ y22)1212 ]+ y21)tanh[(x21+ y21)2 − (x22(x21+ y21)121≤(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(x21(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)− tanh[(x2(x22(cid:3)(cid:3)(cid:3) |x2|(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)+ y22)1212≤≤|x2|− y22+ y22)x21+ y21)(x21(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)+ y2− x21212 + (x22(x1 + x2)2 + (x2+ y21)2( y1 + y2)2 + (x2+ y21)2≤α|x1 − x2| + α| y1 − y1|+|x2|(x21(x2111+ y22)+ y22)|x2|(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)1212|x1 − x2|| y1 − y2|for some constant α such that |x2| ≤ α.Then, we can bound the first coordinate by(cid:3)(cid:3)(cid:3)(cid:3)tanh[(x2(cid:3)1+ y21)12 ]x1+ y21)12(x21− tanh[(x22+ y22)12 ](cid:3)(cid:3)(cid:3)(cid:3)(cid:3)x2+ y22)12(x22≤(α + 1)|x1 − x2| + α| y1 − y1|.Without loss of generality, the second coordinate is bounded by α|x1 − x2| + (α + 1)| y1 − y1|.Finally, we have(cid:8)σ3(z1) − σ3(z2)(cid:8)(cid:7)p((α + 1)|x1 − x2| + α| y1 − y2|)p + (α|x1 − x2| + (α + 1)| y1 − y2|)p(cid:8) 1p(cid:7)M|x1 − x2|p + M| y1 − y2|p(cid:8) 1p = (2α + 1) (cid:8)z1 − z2(cid:8)p ,≤≤where M = (2α + 1)p , z1 = x1 + iy1 and z2 = x2 + iy2.Hence, we have proved that σ3(z) = tanh(|z|)exp(iθ) is Lipschitz continuous.Appendix B. Proof of Theorem 1B.1. Proof of Lemma 1Before proving Lemma 1, we first introduce Maurey’s sparsification lemma [24,17].Lemma 5 (Maurey’s sparsification lemma [24]). In a Hilbert space H equipped with norm || · ||, consider f ∈ H such that f =αi (cid:13)= 0. Then, for any positive integer k, there always exist non negative n(cid:28)i=1αi giwhere gi ∈ H, αi are positive real numbers, and α =integers k1, k2, ..., kn such that n(cid:28)i=1ki = k such thatn(cid:28)i=1(cid:9)(cid:9)(cid:9)(cid:9)(cid:9) f − αk(cid:9)(cid:9)2(cid:9)(cid:9)(cid:9)ki gin(cid:2)i=1≤ αkn(cid:2)i=1αi (cid:8)gi(cid:8)2 ≤ α2k(cid:8)gi(cid:8)2maxii.e.(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)n(cid:2)i=1αiαn(cid:2)i=1kikgi2(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)≤n(cid:2)i=1gi −αik(cid:8)gi(cid:8)2 ≤ αk(cid:8)gi(cid:8)2 .maxi14H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Proof. Define k i.i.d. random variable W 1, W 2, ..., W k such that P (W 1 = α gi) = αiα . Let W =(cid:28)ki=1 W ik. Therefore,E[W ] = E[W 1] = f .Hence, we haveE[(cid:8) f − W (cid:8)2] = 1k2E[(cid:16)( f − W i),k(cid:2)( f − W i)(cid:17)]i=1k(cid:2)i=1k(cid:2)i=1= 1k2 E[(cid:8) f − W i(cid:8)2]E[(cid:8) f − W 1(cid:8)2](E[(cid:8)W 1(cid:8)2] − (cid:8) f (cid:8)2)= 1k= 1k≤ 1kn(cid:2)=i=1≤ α2kE[(cid:8)W 1(cid:8)2]αikα· α2 (cid:8)gi(cid:8)2(cid:8)gi(cid:8)2 .maxiSince for a random variable, the minimal value it can take is at most the value of expectation, hence, there must exist a (cid:28)ki=i W i , andsequence of k numbers (l1, l2, ..., lk) ∈ {1, 2, ..., n}k, such that W i = α gli , W =(cid:8)W − f (cid:8)2 ≤ α2k(cid:8)gi(cid:8)2 .maxiTo finish the proof, we assign integer ki mentioned in the lemma to beki =k(cid:2)j=1=i. (cid:2)1gl jAs Bartlett et al. [17] indicated, the Maurey sparsification lemma only discussed the L1 norm case. Zhang [20] generalized this lemma to create bounds for non-L1 norm cases, which is also applicable in our proof of Lemma 1.Proof of Lemma 1. Given the data matrix Z ∈ Cn×d, re-scaling each column of the matrix Z and get a matrix Y ∈ Cn×d, where(cid:9)(cid:9)(cid:9) .(cid:9)Z:; jY :; j = Z:; j/-Set N = 4dm, k =a2b2m2/r/(cid:8)2.{V 1, V 2, ..., V N } =(cid:21)σ Y eieTj(cid:21)∪σ Y cieTj, and α = am1/r(cid:8) X(cid:8)p . To construct an appropriate convex hull, we define(cid:22): σ ∈ {−1, +1} , i ∈ {1, 2, ..., d} , j ∈ {i, 2, ..., m}(cid:22): σ ∈ {−1, +1} , i ∈ {1, 2, ..., d} , j ∈ {i, 2, ..., m}and(cid:6)(cid:6)αkαkC ==N(cid:2)i=1k(cid:2)ki V i : ki ≥ 0,N(cid:2)i=1ki = k//V lm: (l1, . . . , lm) ∈ [N]k,m=1where ki(cid:5)=(cid:28)km=1 1lm=i .Here, ei defines the d-dimensional standard vector, e j defines the m-dimensional standard vector, and ci defines the d-dimensional vector in which only the ith entry equals −1, and other entries equal 0.√15H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Because of the way V i defined and p ≤ 2, we havemaxi(cid:8)V i(cid:8)2≤ maxi{(cid:8)Y ei(cid:8)2 , (cid:8)Y ci(cid:8)2} = maxi{(cid:8)Y ei(cid:8)2} = maxi(cid:8) Xei(cid:8)(cid:8) Xei(cid:8)2p≤ 1.The first equality is due to the definition of complex-valued vector norms, and the second equality holds because of the Next, it suffices to prove that C is a cover of monotonicity of matrix norm in terms of p.(cid:9)(cid:9)(cid:9)Z A − αkby (cid:8) for some (k1, ..., kN ).Ni=1 ki V i(cid:9)(cid:9)(cid:9)(cid:28)2Define M ∈ Rd×m where the element of each row j equals (cid:9)(cid:9) Z:; j(cid:9)(cid:9)p, hence we have0Z A : A ∈ Cd×m, (cid:8) A(cid:8)q,s ≤ a1. To prove this, we desire to bound Z A = Y (M (cid:19) A),where (cid:19) represents the Hadamard product.(cid:25)(cid:9)(cid:4)(cid:9)(cid:9)(cid:9) Z:,1(cid:9)(cid:8)M(cid:8)p,r =(cid:9)(cid:9) Z:,d(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)p , . . . ,p(cid:5)(cid:9)(cid:9)(cid:9), . . . ,p(cid:9)(cid:9)(cid:9)(cid:4)(cid:9)(cid:9) Z:,1(cid:9)(cid:9)= m1/r(cid:9)(cid:9) Z:,d(cid:9)(cid:9)p(cid:5)(cid:9)(cid:9)(cid:9)pp , . . . ,⎞1/p⎠Zpi, j⎛⎝d(cid:2)n(cid:2)= m1/ri=1j=1= m1/r(cid:8)Z (cid:8)p.(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:4)(cid:9)(cid:9) Z:,1⎛p , . . . ,(cid:9)(cid:9) Z:,d⎞1/p(cid:5)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)p(cid:26)(cid:9)(cid:9)(cid:9)(cid:9)rp= m1/r⎝(cid:9)(cid:9) Z:, j(cid:9)(cid:9)ppd(cid:2)j=1⎠Hence, if we denote S = M (cid:19) A, we have(cid:8)S(cid:8)1 ≤ (cid:16)M, | A|(cid:17) ≤ (cid:8)M(cid:8)p,r(cid:8) A(cid:8)q,s ≤ m1/r(cid:8)Z (cid:8)pa = α.We can see that Z A indeed lies in a convex hull related with {V 1, V 2, ..., V N }:Z A = Y Md(cid:2)m(cid:2)(cid:4)(cid:7)(cid:8)ReMi, jeie(cid:10)j+ Im(cid:7)(cid:8)Mi, jcieTj(cid:5)= Yi=1j=1d(cid:2)m(cid:2)= (cid:8)M(cid:8)1(cid:14)(cid:7)ReMi j(cid:8)M(cid:8)1(cid:8)(cid:4)(cid:5)+Y eie(cid:10)j(cid:8)(cid:4)(cid:7)ImMi j(cid:8)M(cid:8)1Y cie(cid:10)j(cid:16)(cid:5)i=1j=1∈ α · conv ({V 1, . . . , V N }) ,where conv ({V 1, . . . , V N }) denotes the convex hull formed by {V 1, V 2, ..., V N }.Finally, by Lemma 5, there exist non-negative integers (k1, k2, ..., kN ) such that(cid:9)(cid:9)(cid:9)(cid:9)(cid:9) Z A − αkN(cid:2)i=1ki V i(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)22=(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)Y M − αkN(cid:2)i=1ki V i(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)22(cid:8)V i(cid:8)2≤ α2maxkia2m2/r(cid:8)Z (cid:8)2p≤k≤ (cid:8)2.Hence, C is a covering of the desire set. Since the cardinality of set C equals Nk , we have the target inequality:(cid:4)(cid:21)ln NZ A : A ∈ Cd×m, (cid:8) A(cid:8)q,s ≤ a(cid:22)(cid:23)(cid:5), (cid:8), (cid:8) · (cid:8)2≤(cid:24)a2b2m2/r(cid:8)2ln(4dm). (cid:2)16H. Chen, F. He, S. Lei et al.B.2. Proof of Lemma 2Artificial Intelligence 322 (2023) 103951(cid:9)(cid:9)(cid:9)(cid:9) Ai&Z i − &AiAs stated in the third section, this lemma shall be proved by mathematical induction. The basic idea is as follows. Denotes Z i to be the data set passing from the i-1th layer to the ith layer ( Z 0 = Z T ). According to Lemma 1, assume that fixed specific layer i, there exists a sequence of covering matrices (&A0, &A1, ..., &Ai−1) for i-1 previous layers, and a covering matrix &Ai such that ≤ (cid:8) for some (cid:8) > 0. As a consequence, the input data for the i+1th layer shall be Z i+1 = σi+1( Ai Z i), and &Z i+1 = σi+1(&Ai&Z i).(cid:9)(cid:9)(cid:9)(cid:9) Ai Z i − &Ai(cid:9)(cid:9)&Z i22(cid:7)(cid:9)(cid:9)(cid:9) Ai Z i − Ai(cid:9)&Z i2(cid:9)(cid:9)(cid:9)(cid:9) Z i − &Z i2≤ ρi≤ ρi≤ ρi (cid:8) Ai(cid:8)σ(cid:9)(cid:9) Z i+1 − &Z i+1(cid:9)(cid:9) Ai+ ρi(cid:8)i.&Z i − &Ai&Z i&Z i(cid:9)(cid:9)+(cid:8)22Since the first term of the right-hand side part depends on the inductive hypothesis, hence intuitively, we can see that the covering number upper bound depends on the product of spectral norms of all covering matrices. The detailed proof is illustrated as follows.spaces are equipped with (cid:8)·(cid:8)and the first layer’s input Z ∈ V 1 have the constraint: (cid:8) Z (cid:8)We first define two sequences of vector space {V 1, V 2, ..., V L}, and {W 2, W 3, ..., W L+1}. The first sequence of vector W . For each layer’s input matrix, Z i ∈ V i , V , and the second sequences are equipped with (cid:8)·(cid:8)≤ B.Moreover, under our assumptions, Ai can be viewed as a linear operator: V i → W i+1, and the norm of each linear Voperation is defined as:(cid:8) Ai(cid:8)V →W= sup| Z |V ≤1(cid:8) Ai Z (cid:8)W= ci.σi can be treated as a mapping from W i → V i , and the ρi -lipschitz property means(cid:9)(cid:9)σi(z) − σi(z(cid:15)(cid:9)(cid:9))≤ ρiV(cid:9)(cid:9)z − z(cid:9)(cid:9)(cid:15)W .With these preparations, we claim the following lemma which is based on a similar lemma raised by Bartlett et al. [17].Lemma 6 ([17]). Assume that a sequence of positive numbers ((cid:8)1, . . . , (cid:8)L), along with Lipschitz non-linear mappings (σ1, . . . , σL)(where σi is ρi - Lipschitz), and linear operator norm bounds (c1, . . . , cL) as described above are given. Suppose the sequence of matrices A = ( A1, . . . , A L) lies within B1 ×· · ·×BL where Bi are classes satisfying the property that each Ai ∈ Bi has (cid:8) Ai(cid:8)≤ ci . l= j+1 ρlcl, the complex-valued neural network images F :=Let data Z be given with (cid:8)Z (cid:8){FA(Z ) : A ∈ B1 × · · · × BL, (cid:8)Z (cid:8)≤ B} has the following covering number bound≤ B. Then, define τ :=j≤L (cid:8) jρ jV →W(cid:28)(cid:27)VLVN (F, τ , (cid:8)·(cid:8)V ) ≤L(cid:15)i=1(cid:7)supA1,..., Ai−1∈B∀ j<i. A jj(cid:8)(cid:4)(cid:21)N(cid:7)Ai FA1,..., Ai−1(cid:8)(Z ) : Ai ∈ Bi, (cid:8)i, (cid:8) · (cid:8)W.(cid:22)(cid:5)Proof. The lemma is proved by Mathematical induction.A sequence of covering set {F1, F2, ..., FL} is constructed where Fi covers W i .Base case: When i=1, we have F1 to be constructed according to Lemma 1, and|F1| ≤ N ({ A1 Z : A1 ∈ B1} , (cid:8)1, (cid:8) · (cid:8)W ) =: N1.Inductive Hypothesis: Assume that for i=n, we can find a (cid:8)n -covering Fn for set 0AnFA1,...,An−1 (Z ) : An ∈ Bn1such that:|Fn| ≤n(cid:15)l=1Nl.Induction Step: For every element F ∈ Fn, construct an (cid:8)n+1-cover Gn+1(F ) of{ An+1σn(F ) : An+1 ∈ Bn+1} .Since these covers are proper, meaning F = An+1 F ( A1,..., An)(Z ) for some matrices ( A1, . . . , An) ∈ B1 × · · · × Bn, it follows that(cid:7)0NAn+1 F A1,..., An (Z ) : An+1 ∈ Bn+11(cid:8), (cid:8)n+1, (cid:8) · (cid:8)W|Gn+1(F )| ≤ supA1,..., An∈B(cid:7)(cid:8)∀ j≤i. A j=: Nn+1.j17H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Lastly, we can form the cover2Fn+1 :=Gn+1(F ),F ∈Fnwhose cardinality satisfies|Fn+1| ≤ |Fn| · Nn+1 ≤n+1(cid:15)l=1Nl.Define H := {σL(F ) : F ∈ FL}. It’s trivial to see that the cardinality of H is the same as FL . Then, it suffices to show that H is indeed a covering of F . If we fix any ( A1, . . . , A L) satisfying the constraints, then recursively, we denoteF 1 = A1 Z ∈ W 2, G i = σi (F i) ∈ V i+1F i+1 = Ai+1G i ∈ W i+2.In other words, we need to prove that there exist &G L ∈ H such that (cid:9)(cid:9)G L − &G L(cid:9)(cid:9)V≤ τ .(cid:9)(cid:9) Ai&G i−1 − &F i(cid:9)(cid:9)W≤ (cid:8)i, and set &G i := σi(cid:8)(cid:7)&F i.Base case: Set &G 0 = Z .Inductive hypothesis: Choose &F i ∈ Fi withInduction Step:(cid:9)(cid:9)G i+1 − &G i+1(cid:9)(cid:9)V≤ ρi+1≤ ρi+1≤ ρi+1 (cid:8) Ai+1(cid:8)⎛(cid:9)(cid:9)(cid:9)F i+1 − &F i+1(cid:9)W(cid:9)(cid:9)(cid:9)F i+1 − Ai+1(cid:9)&G i(cid:9)(cid:9)G i − &G ii(cid:15)V →W(cid:2)W≤ ρi+1ci+1⎝(cid:8) jρ jρlcll= j+1j≤ii+1(cid:15)(cid:2)=(cid:8) jρ jρlcl&G i − &F i+1 |(cid:9)(cid:9)W+ ρi+1(cid:9)(cid:9)(cid:9)(cid:9) Ai+1+ ρi+1(cid:8)i+1V⎞⎠ + ρi+1(cid:8)i+1j≤i+1l= j+1= γ .Hence, we get proved. (cid:2)To prove Lemma 2, the key idea is to apply the result of Lemma 1 and Lemma 6.Proof of Lemma 2. To begin with, we assume the same setting as above. However, to prove Lemma 2, (cid:8)·(cid:8)and the operator norm is set to the spectral norm, i.e. (cid:8) Ai(cid:8)defined as2, = (cid:8) Ai(cid:8)σ . Also, the sequence of number {(cid:8)1, (cid:8)2, ..., (cid:8)L} are = (cid:8)·(cid:8)= (cid:8)·(cid:8)V →WWV(cid:8)i :=(cid:27)αi(cid:8)j>i ρ j s jρiwhere αi := 1¯α(cid:26)2/3(cid:25)bisi,¯α :=(cid:25)L(cid:2)j=1(cid:26)2/3.b js jBy this setting, we find that the γ defined in Lemma 6 satisfies(cid:2)L(cid:15)(cid:8) jρ jτ ≤ρlsl =j≤Ll= j+1(cid:2)j≤Lα j(cid:8) = (cid:8).Then(cid:7)F|S , (cid:8), (cid:8) · (cid:8)2(cid:8)ln N≤≤L(cid:2)i=1L(cid:2)i=1(cid:4)(cid:21)ln N(cid:7)Ai FA1,..., Ai−1(cid:5)(cid:4)(cid:8)(cid:10)Z(cid:22): Ai ∈ Bi, (cid:8)i, (cid:8) · (cid:8)2(cid:7)supA1,..., Ai−1∈B∀ j<i. A jj(cid:8)(cid:7)supA1,..., Ai−1∈B∀ j<i, A jj(cid:8)(cid:25)3ln N(cid:7)FA1,..., Ai−1(cid:5)(cid:10)(cid:4)(cid:8)(cid:10)Z(cid:10) :( Ai)(cid:9)(cid:9)(cid:9) A(cid:9)(cid:9)(cid:9)(cid:10)i2,118(cid:5)4(cid:26)≤ bi, (cid:8)i, (cid:8) · (cid:8)2H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951≤L(cid:2)i=1(cid:7)supA1,..., Ai−1∈B∀ j<i. A jj(cid:8)(cid:9)(cid:9)(cid:9)F(cid:7)b2iA1,..., Ai−1(cid:8)2i(cid:8)(cid:10)(cid:7)(cid:8)(cid:10)Z(cid:9)(cid:9)2(cid:9)2(cid:4)(cid:5)ln4W 2.The first equality holds because we use L2 norms here. Hence the covering number for a matrix and its transpose are (cid:9)(cid:9)(cid:9)Fthe same. To further simplify the formula, we can upper bound (cid:8)(cid:10)(cid:9)(cid:9)(cid:9)by(cid:10)Z(cid:7)2(cid:7)(cid:8)A1,..., Ai−12(cid:9)(cid:9)(cid:9)(cid:9)F(cid:7)A1,..., Ai−1(cid:5)(cid:10)(cid:4)(cid:8)(cid:10)Z(cid:9)(cid:9)(cid:9)F(cid:7)=(cid:9)(cid:9)(cid:9)(cid:9)2(cid:4)(cid:8)(cid:10)Z(cid:5)(cid:9)(cid:9)(cid:9)2(cid:4)A1,..., Ai−1(cid:4)(cid:5)(cid:10)Z− σi−1(0)(cid:8)2Ai−1 F= (cid:8)σi−1(cid:9)(cid:9)(cid:9) Ai−1 F≤ ρi−1(cid:7)A1,..., Ai−2(cid:8)(cid:4)(cid:7)(cid:8)A1,..., Ai−2(cid:9)(cid:9)(cid:9)F(cid:7)A1,..., Ai−2(cid:5)(cid:10)X(cid:4)(cid:8)(cid:9)(cid:9)− 0(cid:9)2(cid:5)(cid:9)(cid:9)(cid:9)(cid:10)Z.2≤ ρi−1 (cid:8) Ai−1(cid:8)σInductively, we have(cid:9)(cid:9)(cid:9)(cid:9)F(cid:7)maxjA1,..., Ai−1(cid:5)(cid:10)(cid:4)(cid:8)(cid:10)Ze j(cid:9)(cid:9)(cid:9)(cid:9)2≤ (cid:8)Z (cid:8)2(cid:9)(cid:9)(cid:9)(cid:9) A jρ jσ .i−1(cid:15)j=1Finally, we obtain(cid:7)ln NF|S , (cid:8), (cid:8) · (cid:8)2L(cid:2)(cid:8)≤(cid:27)b2i(cid:8)Z (cid:8)22j<i ρ2j(cid:8)2i(cid:9)(cid:9) A j(cid:9)(cid:9)2σ(cid:4)(cid:5)ln4W 2(cid:8)(cid:7)supA1,..., Ai−1∈B∀ j<i. A jj(cid:27)b2i B2i=1L(cid:2)i=1(cid:7)(cid:7)B2 lnB2 ln≤==(cid:4)(cid:5)ln4W 2j s2jj<i ρ2(cid:8)2i(cid:8) (cid:27)4W 2Lj=1 ρ2j s2j(cid:8)2(cid:8) (cid:27)4W 2(cid:8)2Lj=1 ρ2j s2jL(cid:2)i=1(cid:4)b2ii s2α2i(cid:5)¯α3. (cid:2)B.3. Proof of Theorem 1As stated in the third section, the main theorem we used to prove Theorem 1 is the Dudley Entropy Integral. The standard Dudley Entropy Integral introduces a method to obtain Rademacher complexity bound via covering number [16].Theorem 5 ([16]). Let F be a real-valued function class taking values in [0, 1], and assume that 0 ∈ F . Then(cid:8)(cid:7)RF|S≤ infα>0⎛⎜⎝4α√n+ 12n√n6(cid:29)α(cid:7)log NF|S , ε, (cid:8) · (cid:8)2⎞(cid:8)dε.⎟⎠Proof. [17] Let N ∈ N be arbitrary and let εi =N(cid:7)F|S , εi, (cid:8) · (cid:8)2, so that(cid:8)√n2−(i−1) for each i ∈ [N]. For each i, let V i denote the cover achieving ∀ f ∈ F ∃v ∈ V i(cid:16)1/2( f (xt) − vt)2≤ εi,(cid:14)n(cid:2)t=1and |V i| = N(cid:7)F|S , εi, (cid:8) · (cid:8)2(cid:8). For a fixed f ∈ F , let v i[ f ] denote the nearest element in V i . Then19H. Chen, F. He, S. Lei et al.(cid:14)n(cid:2)(cid:16)Artificial Intelligence 322 (2023) 103951Esupf ∈F(cid:14)εi f (xt)t=1$n(cid:2)= E≤ E+ E(cid:14)(cid:14)supf ∈Fsupf ∈Fsupf ∈F$t=1n(cid:2)$t=1n(cid:2)t=1(cid:4)(cid:8)tf (xt) − v Nt(cid:5)[ f ]+%(cid:16)(cid:5)[ f ](cid:4)(cid:8)tf (xt) − v Nt%(cid:16)(cid:8)t v 1t[ f ].N−1(cid:2)n(cid:2)(cid:4)(cid:8)t(cid:14)i=1t=1N−1(cid:2)+Esupf ∈Fi=1v it[ f ] − v i+1t$n(cid:2)t=1(cid:4)(cid:8)t(cid:5)[ f ]−n(cid:2)t=1v it[ f ] − v i+1t%(cid:16)(cid:8)t v 1t[ f ]%(cid:16)(cid:5)[ f ]For the third term, observe that it suffices to take V 1 = {0}, which implies⎡$n(cid:2)(cid:2)f ∈Ft=1⎣E sup(cid:8)%⎤(cid:8)t v 1t[ f ]= 0⎦ .The first term may be handled using Cauchy-Schwarz as follows:⎡$n(cid:2)(cid:2)(cid:4)(cid:8)tf (xt) − v Nt(cid:5)[ f ]%⎤⎦f ∈F((cid:8)t)2t=1899:supf ∈Fn(cid:2)(cid:7)t=1f (xt) − v Nt(cid:8)[ f ]2⎣E sup(cid:8)899:≤En(cid:2)√t=1nεN .≤Last to take care of are the terms of the form$n(cid:2)(cid:4)(cid:8)tt=1E sup(cid:8)v it[ f ] − v i+1t(cid:5)[ f ]%.01For each i, let W i =⎡$n(cid:2)(cid:2)⎣f ∈Ft=1E sup(cid:8)v i[ f ] − v i+1[ f ] | f ∈ F%⎤⎦ ≤ E supw∈W i[ f ] − v i+1(cid:5)[ f ]v it(cid:4)t(cid:8)t%(cid:8)t wt,$n(cid:2)t=1. Then |W i| ≤ |V i| |V i+1| ≤ |V i+1|2,and furthermore899:supw∈W in(cid:2)t=1w 2t= supf ∈F(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)v i[ f ] − v i+1[ f ](cid:9)2(cid:9)(cid:9)(cid:9)(cid:9)≤ sup(cid:9)v i[ f ] − ( f (x1) , . . . , f (xn))(cid:9)f ∈F(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)( f (x1) , . . . , f (xn)) − v i+1[ f ](cid:9)22+ supf ∈F≤ εi + εi+1= 3εi+1.With this observation, the standard Massart finite class lemma [16] implies$n(cid:2)%supw∈W iE(cid:8)899:≤2 supw∈W i(cid:8)t wtt=1n(cid:2)t=1(wt)2 log |W i| ≤ 3(cid:13)2 log |W i|εi+1 ≤ 6(cid:13)log |V i+1|εi+1.20H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Collecting all terms, establishesE(cid:8) supf ∈Fn(cid:2)t=1(cid:8)t f (xt) ≤ εNN−1(cid:2)√n + 6(cid:29)εi+1log N(cid:7)F|S , εi+1, (cid:8) · (cid:8)2(cid:8)i=1N(cid:2)√≤ εNn + 12(cid:29)(εi − εi+1)log N(cid:7)F|S , εi, (cid:8) · (cid:8)2(cid:8)i=1√n6(cid:29)√n + 12≤ εNεN+1(cid:7)log NF|S , ε, (cid:8) · (cid:8)2(cid:8)dε.Finally, select any α > 0 and take N be the largest integer with εN+1 > α. Then εN = 4εN+2 < 4α, and so√n6(cid:29)√n + 12εN(cid:7)log NF|S , ε, (cid:8) · (cid:8)2(cid:8)dε√≤4αn + 12εN+1√n6(cid:29)log Nα(cid:7)F|S , ε, (cid:8) · (cid:8)2(cid:8)dε. (cid:2)However, it’s worth noticing that N (F , (cid:8), (cid:8)·(cid:8)2) can not be directly used in Theorem 3 to obtain the upper bound of ˆRS (G). Hence we raise Lemma 3 and Lemma 4 to make it applicable. We shall first prove these two lemmas.Proof of Lemma 3. Consider H ⊂ F is a cover of family F which satisfies that the cardinality of H equals the covering number of F . Then for any FA ∈ F , we have a corresponding h ∈ H such that(cid:8)FA(Z ) − h(Z )(cid:8)2≤ (cid:8).Then consider (cid:8)FA(Z ) − Y (cid:8)∈ G, we have2|(cid:8)FA(Z ) − Y (cid:8)2− (cid:8)h(Z ) − Y (cid:8)2| ≤ |(cid:8)FA(Z ) − Y − h(Z ) + Y (cid:8)2= |(cid:8)FA(Z ) − h(Z )(cid:8)≤ (cid:8).|2|Therefore, it’s trivial that ¯H = {(cid:8)h(Z ) − Y (cid:8)Hence, the covering number of G is less than the covering number of F . (cid:2): h ∈ H} is a cover of G, and the cardinality of H equals that of ¯H.2Proof of Lemma 4. Consider H ⊂ G is a cover of family G which satisfies that the cardinality of H equals the covering number of G. For any g ∈ G, there exist h ∈ H such that(cid:8)α g − αh(cid:8)2= α (cid:8)g − h(cid:8)2Therefore, αH is a cover of αG.≤ α(cid:8).Vice versa, if αH is a cover of αG, then H is a cover of G.Hence, Lemma 4 is get proved. (cid:2)After all preparations have been done, the proof of Theorem 1 is given as follows:(cid:21)Proof of Theorem 1. Consider family F =FA (z) : A = ( A1, . . . , A L) , (cid:8) Ai(cid:8)σ ≤ si,G = {(z, y) (cid:12)→ l (F A(z), y) : F A ∈ F} .As a consequence of Lemma 3,(cid:7)NF|S , (cid:8), (cid:8) · (cid:8)2(cid:8)≥ N(cid:7)G|S , (cid:8), (cid:8) · (cid:8)2(cid:8)(cid:9)(cid:9) A(cid:22)(cid:9)(cid:9)≤ bi2,1, and family(cid:10)iwhen the loss function is set to be l(FA(z), y) = (cid:8)FA(z) − y(cid:8)2. Since in the standard Dudley Entropy Integral, it requires the value of loss function to be always located in the interval [0, 1], and we make the assumption that l(FA(z), y) ≤ Malways holds for the given data set, hence, we can rescale the loss function by 1M .21H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Define ¯G =0(z, y) (cid:12)→ 1N (G, M(cid:8), (cid:8) · (cid:8)2) = NM l (FA(z), y) : FA ∈ F¯G, (cid:8), (cid:8) · (cid:8)2(cid:8)(cid:7).1, then Lemma 4 indicatesTherefore(cid:7)Nln N≤¯G|S , (cid:8), (cid:8) · (cid:8)2(cid:7)¯G|S , (cid:8), (cid:8) · (cid:8)2(cid:8)(cid:7)(cid:8)Z (cid:8)24W 22 lnM2(cid:8)2(cid:8)(cid:7)≤ N(cid:8)F|S , M(cid:8), (cid:8) · (cid:8)2(cid:7)(cid:8)(cid:8),≤ ln N⎛L(cid:15)⎝j=1⎠j ρ2s2jF|S , M(cid:8), (cid:8) · (cid:8)2⎞(cid:26)(cid:14)(cid:25)L(cid:2)2/3(cid:16)3.bisi(cid:5) (cid:25)i=1Lj=1 s2j ρ2jIf we denote R = (cid:8)Z (cid:8)22 lnAs stated in Theorem 3,(cid:8) (cid:4)(cid:27)(cid:7)4W 2ˆRS ( ¯G) = Eσ[sup¯g∈ ¯G= Eσ[supg∈G1n1nn(cid:2)i=1n(cid:2)i=1σi¯g(zi)]σi1Mg(zi)](cid:28)(cid:26)32/3(cid:4)(cid:5)Li=1bisi, then we have ln N(cid:7)¯G|S , (cid:8), (cid:8) · (cid:8)2(cid:8)≤ RM2(cid:8)2 .(cid:7)ln N¯G|S , ε, (cid:8) · (cid:8)2⎞(cid:8)dε⎟⎠= 1MˆRS (G)⎛⎜⎝⎛⎜⎝(cid:14)≤ infα>0≤ infα>0= infα>04α√n4α√n4α√n+ 12n√n6(cid:29)α√n6(⎞⎟⎠+ 12nα√+ 12RMnlndεRM2(cid:8)2(cid:16)√nα.To make the upper bound neater, we make a simple choice at α = 1n , hence,ˆRS (G) ≤ 4Mn3/2√+ 18 (cid:8) Z (cid:8)22 ln(2W ) ln nRAn.Plugging this upper bound into Theorem 2, the desired result can be obtained. (cid:2)Appendix C. PAC learnability of complex-valued neural networksIn this section, we desire to present the proof which shows that complex-valued neural networks are PAC-learnable.We denote f S to be the empirical error minimizer, i.e., f S = argf ∈Fl ( f (zi), yi). Similarly, fmin 1nis the expected error n(cid:28)i=1,l ( f (zi), yi). R( f ) and &R( f ) respectively represents the expected error and the empirical minimizer: f = argf ∈Ferror.+min E 1nn(cid:28)i=1The concept of PAC-learnable is defined as follows.Definition 1 (PAC-learnable). Let F be a hypothesis set. A is a PAC-learnable algorithm if there exists a polynomial function poly (·, ·, ·, ·) such that for any (cid:8) > 0 and δ > 0, for all distributions D over Z , the following holds for any sample size m ≥ poly(1/(cid:8), 1/δ, n, size (c)):PS∼Dm[R ( f S ) − R( f ) ≤ (cid:8)] ≥ 1 − δ.Here f and f S are defined above.22H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Corollary 1. Define the loss function to be l(FA(z), y) = ||FA(z) − y||2, and is upper-bounded by a constant M. For a complex-valued neural network: FA(z) := σL ( A LσL−1 ( A L−1 · · · σ1 ( A1z))), where activation functions σi are ρi -Lipschitz, it is PAC-learnable.Proof. It suffices to prove that R( f S ) − R( f ) ≤ (cid:8) via the generalization upper bound under high probability.SinceR( f S ) − R( f ) = R( f S ) − &R( f S ) + &R( f S ) − R( f )≤ R( f S ) − &R( f S ) + &R( f ) − R( f ),the last inequality holds because f S is the empirical error minimizer, therefore, we have|R( f S ) − R( f )| ≤≤(cid:3)(cid:3)(cid:3)R( f S ) − &R( f S ) + &R( f ) − R( f )(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3) +(cid:3)R( f S ) − &R( f S )(cid:3)&R( f ) − R( f )(cid:3)(cid:3)(cid:3) .(cid:3)&R( f ) − R( f )≤ 2 supf ∈FHenceP (|R( f S ) − R( f )| ≤ (cid:8)) ≥ P(cid:14)(cid:3)(cid:3)(cid:3) ≤ (cid:8)(cid:3)&R( f ) − R( f )2supf ∈F(cid:16).As in Theorem 1, we have for any f ∈ F⎛⎝P(cid:3)(cid:3)(cid:3) ≤ 8M(cid:3)R( f ) − &R( f )n32≥1 − δ.√+ 36|| Z ||22ln(2W )ln(n)RAn(cid:12)⎞⎠ln 2δ2n+ 3MNotice that, these two statements are equivalent:(cid:3)(cid:3)(cid:3) ≤ (cid:8)(cid:3)&R( f ) − R( f )2(cid:3)(cid:3)(cid:3) ≤ (cid:8)(cid:3)R( f ) − &R( f )2⇔∀ f ∈ F,supf ∈F.Hence, we can claim that if√+ 36|| Z ||2(cid:8)2≥ 8Mn322ln(2W )ln(n)RAn(cid:12)ln 2δ2n,+ 3Mthen(cid:14)Psupf ∈F(cid:3)(cid:3)(cid:3) ≤ (cid:8)(cid:3)&R( f ) − R( f )2(cid:16)≥ 1 − δi.e.P (|R( f S ) − R( f )| ≤ (cid:8)) ≥ 1 − δ.Hence, we can get the conclusion that if⎛n ≥ 8(cid:8)3⎝8M + 36 (cid:8)Z (cid:8)2(cid:13)2 ln (2W )RA + 3M(cid:12)⎞3⎠,ln 2δ2thenP (|R( f S ) − R( f )| ≤ (cid:8)) ≥ 1 − δ.Therefore, PAC-learnability of complex-valued neural networks get proved. (cid:2)Appendix D. Generalization of sequential dataIn this section, we aim at proving Theorem 2. Theorem 2 shows an extension of generalization to sequential data case. Therefore, sequential analogues of complexities [19] are presented in this section to complete the proof.23H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951D.1. Sequential Rademacher complexityIn the case of classical complexity measure, we use the expectation of the supremum of Rademacher process to define the Rademacher complexity. In the sequential Rademacher case, the intuition is quite similar. Rakhlin et al. [19] illustrated a binary tree process to be the analogue of Rademacher process, which coincides with Rademacher process under i.i.d.assumption, but behaves differently in general. The notion of a tree is defined as following:“A Z -valued tree z of depth n is a rooted complete binary tree with nodes labelled by elements of Z . We identify the tree z with the sequence (z1, . . . , zn) of labelling functions zi : {±1}i−1 (cid:12)→ Z which provide the labels for each node. Here, z1 ∈ Z is the label for the root of the tree, while zi for i > 1 is the label of the node obtained by following the path of length i − 1 from the root, with +1 indicating ‘right’ and −1 indicating ‘left’. A path of length n is given by the sequence (cid:8) = ((cid:8)1, . . . , (cid:8)n) ∈ {±1}n. For brevity, we shall often write zt((cid:8)), but it is understood that zt only depends only on the prefix ((cid:8)1, . . . , (cid:8)t−1) of (cid:8). Given a tree z and a function f : Z (cid:12)→ R, we define the composition f ◦ z as a real-valued tree given by the labelling functions ( f ◦ z1, . . . , f ◦ zn).” [19]Therefore, the definition of sequential Rademacher complexity is stated in Definition 2.Definition 2 ([19]). For a Z -valued tree z with depth n, then the sequential Rademacher complexity of a function classGsq := {(zt, yt ) (cid:12)→ l (FA(z), y) : FA ∈ F } is defined as follows:$1nsupf ∈Fn(cid:2)t=1%(cid:8)tl ( f (zt((cid:8))) , yt),Rsqn (Gsq, z) = EandRsqn (Gsq) = supzRsqn (Gsq, z).Here (cid:8)t is the Rademacher variables taking value from {+1, −1} with equal probability.D.2. Sequential Rademacher complexity generalization boundWhen investigating the relation between generalization error and Rademacher complexity, we have the following theo-rem.Theorem 6. Given function class F , sample S = {(z1, y1), (z2, y2), ..., (zn, yn)} where (zi, yi ) are i.i.d. data points, we have%$%$1nn(cid:2)i=1Esupf ∈Ff (zi, yi) − E [ f ]≤ Ef (zi, yi) − E [ f ]≤ 2R(F)1nsupf ∈Fn(cid:2)i=1where R(F ) = E[&RS (F )].For sequential Rademacher complexity, Rakhlin et al. [19] proved a similar theorem.Theorem 7. Given function class F , sample S = {(z1, y1), (z2, y2), ..., (zn, yn)} where (z1, y1) are sequential data points, then the following inequality holds:$E1nsupf ∈Fn(cid:2)t=1(E [ f (zt, yt) | At−1] − f (zt, yt))≤ 2Rsqn (F)%where Rsqn (F ) denotes the sequential Rademacher complexity.If the function class F is bounded, i.e. for any f ∈F , (cid:8) f (cid:8)∞ ≤M, then the generalization error 1nis sharply concentrated around its expectation, which leads to Corollary 2.(cid:8)− f (zt, yt)(cid:28)nt=1(cid:7)E [ f (zt, yt) | At−1]Corollary 2. Assume that for the target function class, any f ∈ F , we have (cid:8) f (cid:8)∞ ≤ M. Given sample S = {(z1, y1), (z2, y2), ...,(zn, yn)} where (z1, y1) are sequential data points, then under probability at least 1 − δ the following inequality holds:(cid:12)1nn(cid:2)t=1(E [ f (zt, yt) | At−1] − f (zt, yt)) ≤ 2Rsqn (F) + Mlog 2δ2n.24H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Proof. This corollary is a consequence of McDiarmid’s Inequality and Theorem 6. By McDiarmid’s Inequality, since (cid:8) f (cid:8)∞ ≤M, we haveP (|(cid:5)(F) − E[(cid:5)(F)]| ≥ t) ≤ 2 exp(cid:7)E [ f (zt, yt) | At−1] − f (zt, yt)(cid:28)nt=1where (cid:5)(F ) = 1nplexity upper bound. (cid:2)(cid:4)−2nt2/M2(cid:5)(cid:8). Then by Theorem 6, we can get the sequential Rademacher com-As a consequence of Corollary 1, it’s necessary to bound the sequential Rademacher complexity if we want to prove the generalization upper bound. This leads to the introduction of sequential Dudley Entropy Integral.D.3. Sequential Dudley entropy integralBefore stating the sequential Dudley Entropy Integral, we first present the definition of sequential covering number [19]Definition 3 (Sequential covering number). A set C is a sequential α-cover (with respect to (cid:11)p -norm) of F ⊆ RZof depth n ifon a tree z∀ f ∈ F, ∀(cid:8) ∈ {±1}n, ∃c ∈ Cs.t.|ct((cid:8)) − f (zt((cid:8)))|p≤ α.(cid:14)1nn(cid:2)t=1(cid:16)1/pThe sequential covering number of a function class F on a given tree z is defined asN sqp (α, F, z) = minp (α, F , n) := supzand N sq01|C| : C is an α-cover w.r.t. (cid:11)p-norm of F on zp (α, F , z).N sq,Rakhlin et al. [19] provides the sequential version Dudley Entropy Integral as follows:Theorem 8 (Sequential Dudley Entropy Integral). For p ≥ 2, the sequential Rademacher complexity of a function class F ⊆ [−1, 1]Zon a Z -valued tree of depth n satisfies16(cid:29)⎧⎨Rsqn (F) ≤ infα⎩4α + 12√nlog N sq2 (δ, F, n)dδα⎫⎬⎭ .Notice that for the classical α-cover of F with regard to l2 norm, denote it by V , we have for any given data matrix Z , and for any FA ∈ F , there exist v ∈ V such that(cid:8)FA(Z ) − v(Z )(cid:8)2≤ α.Since given a set of sequential data, (cid:8)FA(Z ) − v(Z )(cid:8)=2(cid:12)n(cid:28)t=1(FA(zt ) − v(zt ))2.Hence, if(cid:8)FA(Z ) − v(Z )(cid:8)2≤ α,then we have899: 1nn(cid:2)t=1(FA(zt) − v(zt))2 ≤ α√n.Hence, as a consequence of Lemma 2,ln N sq2 (α√n, Gsq, n) ≤ ln N (G, α, (cid:8)·(cid:8)2) ≤(cid:8)Z (cid:8)2 ln 4W 2α2sqA,Rwhere Gsq denotes the loss function family of the sequential data set, Runder the case of sequential data set, and G denotes the loss function family of the i.i.d. data set.sqA denotes the spectral complexity of the CVNNs Hence, we have(cid:7)ln N sq2α, Gsq, n(cid:8)≤(cid:8)Z (cid:8)2 ln 4W 2nα2sqA.R25H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951Supplementary Table E.2Detailed model architectures for different datasets.MNIST/FashionMNIST/CIFAR-10/CIFAR-1005 × 5, 10maxpool, 2 × 25 × 5, 20maxpool, 2 × 2fc-500Tiny ImageNet5 × 5, 10maxpool, 2 × 2(5 × 5, 20) × 2maxpool, 2 × 2fc-500IMDBfc-500fc-200absfc-10/100, softmaxabsfc-200, softmaxabsfc-2, softmaxD.4. Proof of Theorem 2As a consequence of the previous Sections D.1-D.3, we have(cid:7)ln N sq2(cid:8), Gsq, n(cid:8)≤(cid:8)Z (cid:8)2 ln 4W 2n(cid:8)2sqARfor any (cid:8) ∈ R+.Therefore, by Lemma 4 and the sequential Dudley Entropy Integral, we can derive the following bound for the sequential Rademacher complexity:n (Gsq) ≤ 4MRsqn+ 12√ln nRAMnwhere M denotes the upper bound for the loss function.After plugging the above inequality into Corollary 2, we can get the desired bound stated in Theorem 2.Appendix E. Additional experiments detailsThe section provides all the additional details of our experiments. The code is available at https://github .com /LeavesLei /cvnn _generalization.E.1. DatasetsOur experiments are conducted on six datasets: MNIST [25], FashionMNIST [26], CIFAR-10, CIFAR-100, [27], IMDB [28], and Tiny ImageNet [29]. The details of these datasets are shown as follows.• MNIST consists of 60, 000 training images and 10, 000 test images from 10 different classes. It can be downloaded from http://yann .lecun .com /exdb /mnist/.• FashionMNIST consists of 60, 000 training images and 10, 000 test images from 10 different classes. It can be down-loaded from https://github .com /zalandoresearch /fashion -mnist.• CIFAR-10 consists of 50, 000 training images and 10, 000 test images from 10 different classes, and CIFAR-100 has the same data as CIFAR-10 while images in CIFAR-100 belong to 100 classes. CIFAR-10 and CIFAR-100 can be downloaded from https://www.cs .toronto .edu /~kriz /cifar.html.• IMDB is a movie reviews sentiment classification dataset, in which each of training and test sets consists of 25, 000movie reviews from 2 different classes. It can be downloaded from http://ai .stanford .edu /~amaas /data /sentiment/.• Tiny ImageNet consists of 100, 000 training images and 10, 000 test images from 200 different classes. It can be down-loaded from http://cs231n .stanford .edu /tiny-imagenet -200 .zip.For the image datasets, i.e., MNIST, FashionMNIST, CIFAR-10, CIFAR-100, and Tiny ImageNet, we normalize each pixel of the images from the datasets to the range of [0, 1] before feeding them into the neural network. For the IMDB dataset, we perform data pre-processing following https://github .com /manavgakhar /imdbsentiment /blob /master /IMdB _sentiment _analysis _project .ipynb.E.2. Model architecturesWe employ the Python package complexPyTorch [30] to implement our CVNNs, which include complex-value CNNs and complex-value MLPs. The detailed architectures of CVNNs are presented in Supplementary Table E.2, and all the parameters in these network architectures are complex values except for the last layer.In Supplementary Table E.2, “5 × 5, 10” denotes that the convolutional layer has 5 × 5 kernel size and 10 output channels. The strides for all convolutional layers are set to 1. “fc-500” denotes the fully-connected layer with the output features of 26H. Chen, F. He, S. Lei et al.Artificial Intelligence 322 (2023) 103951500. All convolutional layers and fully-connected layers are followed the ReLU layer except for the last layer. “abs” is the absolute layer that computes the absolute value of each element in input and can convert complex values to real values.E.3. Implementation detailsThis section provides all the additional implementation details for our experiments.Model training. We employ SGD to optimize all the models with momentum = 0.9.Training strategy for MNIST and FashionMNIST. Every model is trained by SGD for 100 epochs, in which the batch size is set as 1024, and the learning rate is fixed to 0.01.Training strategy for CIFAR-10 and CIFAR-100. Models are trained by SGD for 100 epochs, in which the batch size is set as 128, and the learning rate is fixed to 0.01.Training strategy for IMDB. Models are trained by SGD for 100 epochs, in which the batch size is set as 512. The learning rate is initialized as 0.01 and decayed by 0.2 every 40 epoch.Training strategy for Tiny ImageNet. Models are trained by SGD for 100 epochs, in which the batch size is set as 128. The learning rate is initialized as 0.01 and decayed by 0.2 every 40 epoch.References[1] S.L. Goh, D.P. Mandic, Nonlinear adaptive prediction of complex-valued signals by complex-valued prnn, IEEE Trans. Signal Process. 53 (2005) 1827–1836.[2] A. Hirose, R. Eckmiller, Behavior control of coherent-type neural networks by carrier-frequency modulation, IEEE Trans. Neural Netw. 7 (1996) [3] H. Sawada, R. Mukai, S. Araki, S. Makino, Polar coordinate based nonlinear function for frequency-domain blind source separation, IEICE Trans. Fundam. [4] E.K. Cole, J.Y. Cheng, J.M. Pauly, S.S. Vasanawala, Analysis of deep complex-valued convolutional neural networks for mri reconstruction, arXiv preprint [5] A. Hirose, S. Yoshida, Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence, IEEE Trans. Neural 1032–1034.Electron. Commun. Comput. Sci. 86 (2003) 590–596.arXiv:2004 .01738, 2020.Netw. Learn. Syst. 23 (2012) 541–551.[6] A. Hirose, Complex-Valued Neural Networks, vol. 400, Springer Science & Business Media, 2012.[7] T. Nitta, An extension of the back-propagation algorithm to complex numbers, Neural Netw. 10 (1997) 1391–1415.[8] T. Nitta, Orthogonality of decision boundaries in complex-valued neural networks, Neural Comput. 16 (2004) 73–97.[9] T. Nitta, On the inherent property of the decision boundary in complex-valued neural networks, Neurocomputing 50 (2003) 291–303.[10] T. Nitta, Redundancy of the parameters of the complex-valued neural network, Neurocomputing 49 (2002) 423–428.[11] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J.F. Santos, S. Mehri, N. Rostamzadeh, Y. Bengio, C.J. Pal, Deep complex networks, arXiv [12] D.P. Reichert, T. Serre, Neuronal synchrony in complex-valued deep networks, arXiv preprint arXiv:1312 .6115, 2013.[13] I. Danihelka, G. Wayne, B. Uria, N. Kalchbrenner, A. Graves, Associative long short-term memory, in: International Conference on Machine Learning, preprint arXiv:1705 .09792, 2017.PMLR, 2016, pp. 1986–1994.[14] M. Arjovsky, A. Shah, Y. Bengio, Unitary evolution recurrent neural networks, in: International Conference on Machine Learning, PMLR, 2016, pp. 1120–1128.[15] S. Wisdom, T. Powers, J. Hershey, J. Le Roux, L. Atlas, Full-capacity unitary recurrent neural networks, Adv. Neural Inf. Process. Syst. 29 (2016) 4880–4888.[16] M. Mohri, A. Rostamizadeh, A. Talwalkar, Foundations of Machine Learning, MIT Press, 2018.[17] P. Bartlett, D.J. Foster, M. Telgarsky, Spectrally-normalized margin bounds for neural networks, arXiv preprint arXiv:1706 .08498, 2017.[18] N. Guberman, On complex valued convolutional neural networks, arXiv preprint arXiv:1602 .09046, 2016.[19] A. Rakhlin, K. Sridharan, A. Tewari, Sequential complexities and uniform martingale laws of large numbers, Probab. Theory Relat. Fields 161 (2015) 111–153.[20] T. Zhang, Statistical analysis of some multi-category large margin classification methods, J. Mach. Learn. Res. 5 (2004) 1225–1251.[21] P.-C. Guo, A Frobenius norm regularization method for convolutional kernels to avoid unstable gradient problem, arXiv preprint arXiv:1907.11235, 2019.2017.[22] A. Botalb, M. Moinuddin, U. Al-Saggaf, S.S. Ali, Contrasting convolutional neural network (cnn) with multi-layer perceptron (mlp) for big data analysis, in: 2018 International Conference on Intelligent and Advanced System (ICIAS), IEEE, 2018, pp. 1–5.[23] Y.H. Geum, A.K. Rathie, H. Kim, Matrix expression of convolution and its generalized continuous form, Symmetry 12 (2020) 1791.[24] G. Pisier, Remarques sur un résultat non publié de B. Maurey, in: Séminaire Analyse Fonctionnelle, 1981, pp. 1–12.[25] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proc. IEEE 86 (1998) 2278–2324.[26] H. Xiao, K. Rasul, R. Vollgraf, Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, arXiv preprint arXiv:1708 .07747, [27] A. Krizhevsky, G. Hinton, Learning multiple layers of features from tiny images, Technical Report, Citeseer, 2009.[28] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, C. Potts, Learning word vectors for sentiment analysis, in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Portland, Oregon, USA, 2011, pp. 142–150, http://www.aclweb .org /anthology /P11 -1015.[29] Y. Le, X. Yang, Tiny imagenet visual recognition challenge, CS 231N 7 (2015) 7.[30] M.W. Matthès, Y. Bromberg, J. de Rosny, S.M. Popoff, Learning and avoiding disorder in multimode fibers, Phys. Rev. X 11 (2021) 021060, https://doi .org /10 .1103 /PhysRevX .11.021060.27