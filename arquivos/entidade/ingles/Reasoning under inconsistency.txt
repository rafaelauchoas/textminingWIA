Artificial Intelligence 174 (2010) 799–823Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning under inconsistency: A forgetting-based approach ✩Jérôme Lang a, Pierre Marquis b,∗a LAMSADE-CNRS / Université Paris-Dauphine, Place du Maréchal de Lattre de Tassigny, 75775 Paris Cedex 16, Franceb CRIL-CNRS / Université d’Artois, rue Jean Souvraz, S.P. 18, 62307 Lens Cedex, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 15 September 2009Received in revised form 23 April 2010Accepted 23 April 2010Available online 29 April 2010Keywords:Knowledge representationReasoning under inconsistencyForgetting1. IntroductionIn this paper, a fairly general framework for reasoning from inconsistent propositionalbases is defined. Variable forgetting is used as a basic operation for weakening pieces ofinformation so as to restore consistency. The key notion is that of recoveries, which are setsof variables whose forgetting enables restoring consistency. Several criteria for definingpreferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (orboth). Our framework encompasses several previous approaches as specific cases, includingreasoning from preferred consistent subsets, and some forms of information merging.Interestingly, the gain in flexibility and generality offered by our framework does not implya complexity shift compared to these specific cases.© 2010 Elsevier B.V. All rights reserved.Reasoning from inconsistent pieces of information, represented as logical formulas, is an important issue in ArtificialIntelligence. Thus, there are at least two very different contexts where inconsistent sets of formulas have to be dealt with.The first one is when the formulas express beliefs about the real world, that may stem from different sources. In thiscase, inconsistency means that some of the formulas are just wrong. The second one is when the input formulas expresspreferences (or goals, desires) expressed by different agents (or by a single agent according to different criteria). In this case,inconsistency does not mean that anything is incorrect, but that some preferences will not be able to be fulfilled. Even ifthe nature of the problems is very different whether we are in one context or the other one, many notions and techniquesthat can be employed to reason from inconsistent sets of formulas are similar.Whatever the nature of the information represented, classical reasoning is inadequate to derive significant consequencesfrom inconsistent formulas, since it trivializes in this situation (ex falso quodlibet sequitur). This calls for other inference rela-tions which avoid the trivialization problem (namely, paraconsistent inference relations) but there is no general consensusabout what such relations should be. Actually, both the complexity of the problem of reasoning under inconsistency and itssignificance are reflected by the number of approaches that have been developed for decades and can be found in the liter-ature under various names, like paraconsistent logics, belief revision, argumentative inference, information merging, modelfitting, arbitration, knowledge integration, knowledge purification, etc. (see [7,5] for surveys).Corresponding to these approaches, many different mechanisms to avoid trivialization can be exploited. A first taxonomyallows for distinguishing between active approaches, where inconsistency is removed by identifying wrong pieces of beliefthrough knowledge-gathering actions (see e.g. [34,36,28]) or by the group of agents agreeing on some goals to be given✩This is an extended and revised version of a paper that appeared in the Proceedings of the 8th International Conference on Knowledge Representationand Reasoning (KR’02), 2002, pp. 239–250.* Corresponding author.E-mail addresses: lang@lamsade.dauphine.fr (J. Lang), marquis@cril.univ-artois.fr (P. Marquis).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.023800J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823up after a negotiation process, from passive approaches, where inconsistency is dealt with. In this latter case, trivializationis avoided by weakening the set of consequences that can be derived from the given base (a set of formulas). In thepropositional case, this can be achieved by two means:(1) By weakening the consequence relation of classical logic while keeping the base intact. Such an approximation by below ofclassical entailment can be achieved, which typically leads to paraconsistent logics.(2) By weakening the input base while keeping classical entailment. The pieces of information from the initial base are weakenedsuch that their conjunction becomes consistent. This technique is at work in so-called coherence-based approaches toparaconsistent inference (see e.g. [46,24,25,11,3,41,44,4] for some of the early references), where weakening the inputbase consists in inhibiting some of the pieces of information it contains (by removing them). It is also at work in beliefmerging (see e.g. [37,47,31,40] for some of the early references). Belief merging, and especially distance-based mergingconsists in weakening the pieces of information by dilating them: the piece of information φ, instead of expressing thatthe real world is for sure among the models of φ, now expresses that it is close to be a model of φ (the further a worldω from the models of φ, the less plausible it is that ω is the real world).The above dichotomy between (1) and (2) is reminiscent of the dichotomy between actual and potential contradictions, asdiscussed in [7,5]. Actual contradictions tolerate inconsistency by reasoning with a set of inconsistent statements, whereaspotential contradictions are prevented from arising by putting individually consistent yet jointly inconsistent informationtogether.In the rest of this paper we deal with potential inconsistencies and focus on the class of approaches, consisting inweakening the input base. While existing weakening-based approaches work well on some families of problems, there aretypical examples that they fail to handle in a satisfactory way (see Section 6 for a detailed discussion), the reason being thatwhile some of these approaches take account for the relative importance of pieces of information (or of the correspondingsources), they do not handle the relative importance of atoms in the problem at hand. This is problematic in many situationswhere some atoms are less central than others, especially when some atoms are meaningful only in the presence of others.For instance, it makes little sense to reason about whether John’s car is grey if there is some strong conflict about whetherJohn actually has a car. Or, in a preference merging context, suppose that a group of co-owners of a residence tries to agreeabout whether a tennis court or a swimming pool should be built: if there is no agreement about whether the swimmingpool is to be constructed, any preference concerning its colour must be (at least temporarily) ignored (this would not bethe case for its size, however, because it influences its price in a dramatic way).More generally, it is sometimes the case that ignoring a small set of propositional atoms of the formulas from an incon-sistent set renders it consistent. When reasoning from inconsistent beliefs, this allows for giving some useful informationabout the other atoms (those that are not forgotten); information about other atoms can be processed further (for instancethrough knowledge-gathering actions) if these atoms are important enough. When trying to reach a common decision froman inconsistent set of preferences, ignoring small sets of atoms allows for making a decision about all other atoms; thedecision about the few remaining atoms can then take place after a negotiation process among agents.In the following, we define a framework for reasoning from inconsistent propositional bases, using forgetting [10,39,33]as a basic operation for weakening formulas. Belief (or preference) bases are viewed as (finite) vectors of propositional for-mulas, conjunctively interpreted. Without loss of generality, each formula is assumed to be issued from a specific source ofinformation (or a specific agent). Forgetting a set X of atoms in a formula consists in replacing it by its logically strongestconsequence which is independent of X , in the sense that it is equivalent to a formula in which no atom from X occurs [33].The key notion of our approach is that of recoveries, which are sets of atoms whose forgetting enables restoring consistency.The intuition of this simple principle is that if a collection of pieces of information is jointly inconsistent, weakening it byignoring some atoms (for instance the least important ones) can help restoring consistency and derive reasonable conclu-sions. Several criteria for defining preferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (or both).Our contributions are composed of the following models and results. We first define a general model for using variableforgetting in order to reason under inconsistency. We show that our model is general enough to encompass several classesof paraconsistent inference relations, including reasoning from preferred consistent subbases (Propositions 4.1 and 4.2) andvarious types of belief merging (Propositions 4.3, 4.4 and 4.5). Our framework does not only recover known approaches asspecific cases (which would make its interest rather limited) but it allows for new families of paraconsistent inferences,including homogeneous inferences, where propositional variables have to be forgotten in a homogeneous way from thedifferent sources, and abstraction-based inferences, where the most specific variables are forgotten. Our contribution withrespect to the computational study of these forgetting-based inferences starts with a result stating that these inferencerelations are not more difficult to compute than some of the well-known specific cases (Proposition 5.1) and also includecomplexity results about homogeneous inferences (Proposition 5.2) and tractable fragments (Proposition 5.3).Our framework offers several advantages compared with many existing approaches to paraconsistent reasoning. Themain ones are three fold. First, the inference relations defined in our framework are typically less cautious than inferencerelations stemming from other approaches to paraconsistent reasoning (especially those based on the selection of consistentsubsets, but also some forms of merging) because it is based on a weakening mechanism (namely, forgetting) that is morefine-grained than the weakening mechanisms at work in such approaches (inhibition and dilation of formulas). Accordingly,J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823801more information can be derived from inconsistent bases. Second, it is quite general and flexible. As to generality, ourframework encompasses several previous approaches as specific cases, including reasoning from preferred consistent subsets,and forms of merging (mainly because inhibition and dilation of formulas can be achieved via forgetting). As to flexibility,our framework enables us to model situations where some sources of information are considered more reliable than othersin an absolute way, but also relatively to some topics of interest. Some form of equity between some sources of informationcan also be achieved by imposing to forget the same atoms in the formulas associated to the sources. Third, the gain ingenerality and flexibility offered by our framework does not imply a complexity shift compared with these specific cases.The rest of this paper is organized as follows. Formal preliminaries including a presentation of variable forgetting aregiven in Section 2. Our general framework is presented in Section 3. In Section 4 we give several particular cases of ourgeneral approach, including well-known frameworks (namely, reasoning from consistent subsets, and some forms of merg-ing); we also give specific cases that are not covered by any of the existing classes of approaches. In Section 5 we brieflydiscuss the computational aspects of our framework. Especially, we show that although it is quite general, our frameworkdoes not imply a complexity shift compared to the well-known specific classes of approaches that are recovered. In Sec-tion 6 we discuss in detail the benefits of our general approach compared to these specific cases. We mention some relatedwork in Section 7 and we draw some conclusions in Section 8. Full proofs are given in Appendix A.2. Formal preliminaries2.1. Propositional logicPROPPS denotes the propositional language built up from a finite set PS of symbols (also referred to as atoms or variables),the Boolean constants (cid:3) (true) and ⊥ (false), and the connectives ¬, ∨, ∧, ⇒, ⇔, ⊕. Var(φ) denote the set of propositionalvariables occurring in the formula φ. φx←0 (resp. φx←1) denotes the formula obtained by replacing in φ every occurrenceof symbol x by ⊥ (resp. (cid:3)).An interpretation (or world) ω over PS is an assignment of a truth value to each variable of PS. Ω = 2PS is the set ofall interpretations. Formulas are interpreted in the classical way. Every finite set of formulas is interpreted conjunctively.Mod(φ) denotes the set of models of φ. Rather than denoting interpretations by sets of variables, we choose to denote themby words over the set of {x, ¯x | x ∈ PS}. For instance, if PS = {a, b, c, d}, ω = a¯b¯cd is the interpretation that assigns a and dto true and b and c to false. |(cid:12) denotes logical entailment and ≡ denotes logical equivalence. Finally, ω and ω(cid:14)being twointerpretations, Diff (ω, ω(cid:14)) is the set of propositional variables assigned different truth values by ω and ω(cid:14). For instance,Diff (a¯b¯cd, ab¯c ¯d) = {b, d}.If X ⊆ PS, an X -interpretation ω X is a truth assignment to the variables in X , that is, an element of 2 X . While it isnot the case that every formula of PROPPS can be given a truth value in an X -assignment, the notion of satisfaction of aformula φ from PROP X by an X -interpretation ω X is well defined (and coincides with the standard notion of satisfaction).The projection of an interpretation ω on a subset of variables X of PS, denoted by ω↓ X , is the X -interpretation which is therestriction of ω to X .2.2. ForgettingOur approach to restore consistency is based on (variable) forgetting, also known as projection. Forgetting can be definedas follows (see [39,33] for more details):Definition 2.1 (Forgetting). Let φ be a formula from PROPPS and V ⊆ PS. The forgetting of V in φ, noted ∃V .φ, is a formulafrom PROPPS that is inductively defined up to logical equivalence as follows:• ∃∅.φ ≡ φ;• ∃{x}.φ ≡ φx←0 ∨ φx←1;• ∃({x} ∪ V ).φ ≡ ∃V .(∃{x}.φ).For example, ∃{a}.¬a ∧ b ≡ b and ∃{a}.(a ∨ b) ≡ (cid:3).When V is a singleton {x}, we typically write ∃x.φ instead of ∃{x}.φ.As the notation used suggests it, the forgetting of V = {v 1, . . . , vk} in φ is equivalent to the quantified Boolean formula∃v 1.(. . . .(∃vk.φ) . . .).∃V .φ represents the logically strongest consequence ψ (unique up to logical equivalence) of φ that is independentand Var(ψ (cid:14)) ∩ V = ∅.of V , where “ψ is independent of X ” means that there exists a formula ψ (cid:14)Accordingly, forgetting a set of variables within a formula leads to weaken it. To be more precise, if V ⊆ W holds, then∃V .φ |(cid:12) ∃W .φ holds. Moreover, φ is consistent if and only if ∃Var(φ).φ is valid (see [33]).from PROPPS s.t. ψ ≡ ψ (cid:14)Many characterizations of forgetting, together with complexity results, are reported in [33]. Noticeably, for every V ⊆ PSand every formula φ from PROPPS, we have ∃V .φ ≡ ∃V φ.φ, where V φ = V ∩ Var(φ) – which means that forgetting variablesthat do not appear in a formula does not have any effect.802J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823The following characterization of forgetting [33] will be used further in the paper: for any φ ∈ PROPPS, V ⊆ PS and|(cid:12) ∃V .φ if and only if there exists an interpretation ω ∈ Ω such that ω |(cid:12) φ and∈ 2ω ¯Vω↓ ¯V = ω ¯V .¯V where ¯V = PS \ V , we have ω ¯V3. Reasoning from preferred recoveries3.1. Bases and recoveriesWe start by defining propositional bases.Definition 3.1 (Base). A base B is a vector (cid:21)φ1, . . . , φn(cid:22) of n formulas from PROPPS, where n is a positive integer.Each i (1 (cid:2) i (cid:2) n) identifies a source of information and φi denotes the piece of information conveyed by source i. Notethat it can be the case that a formula occurs more than once in B, which can be used to model the situation where twodifferent sources (or more) give the same information.B = (cid:21)φ1, . . . , φn(cid:22) is conjunctively interpreted, so that it is said to be inconsistent if and only ifthe remaining case, it is said to be consistent. B = (cid:21)φ1, . . . , φn(cid:22) and Bif and only if for each i ∈ {1, . . . , n}, we have φi ≡ φ(cid:14)(cid:14) = (cid:21)φ(cid:14)i . Finally, we note Var((cid:21)φ1, . . . , φn(cid:22)) =1, . . . , φ(cid:14)n(cid:3)ni=1 Var(φi).(cid:2)ni=1 φi is inconsistent; in(cid:14)(cid:22) are said to be equivalent, noted B ≡ B,The key notions of our approach are forgetting vectors and recoveries. A forgetting vector consists of sets of variablesto be forgotten in each formula from the base. These sets of variables need not be identical, but they should obey someconstraints bearing on the forgetting process:Definition 3.2 (Forgetting context). Let B = (cid:21)φ1, . . . , φn(cid:22) be a base. A forgetting context for B is a consistent propositionalformula C over F PS,n = {forget(x, i) | x ∈ PS, i ∈ {1, . . . , n}} (a set of propositional atoms). forget(x, i) means that atom x maybe forgotten in φi .For instance, ¬forget(x, i) means that forgetting variable x in φi is forbidden. This is helpful to model the situation wheresource i is fully reliable to what concerns x, i.e., what i says about x must be taken for sure. forget(x, i) ⇒ forget( y, i) enablesto express that for some sources (i), some atoms ( y) can be significant only in presence of others (x) so that forgetting thelatter imposes to forget the former. forget(x, i) ⇒ forget(x, j) can be used to force some sources of information (here i and j)to be considered on equal terms w.r.t. weakening. For instance, if φi and φ j are together inconsistent and consistency canbe recovered by forgetting x in φi , then it could be expected that x should also be forgotten in φ j .(cid:2)x, y∈PSWhile the notion of forgetting context has already been considered in [35,6], this way of representing them al-lows for going beyond previous representations based on binary relations; the generalization is both in terms of ex-pressiveness and spatial efficiency. Thus, a binary relation R over PS × {1, . . . , n} (see [6]), where (x, i)R( y, j) meansthat if x is forgotten in φi , then y must be forgotten in φ j can be easily (i.e., in linear time) encoded as a formula(cid:2)i, j∈{1,...,n}(forget(x, i) ⇒ forget( y, j)). Conversely, the forgetting context ¬forget(x, i) ∨ ¬forget( y, i) (stating that insource i at most one of the two atoms x and y may be forgotten) cannot be represented by such a binary relation R.Representing forgetting contexts by propositional formulas also allows for taking advantage of existing approaches for man-aging propositional representations. Consider for instance the following update problem: if B becomes independent from anatom x (through an update) (resp. if a source i becomes unavailable), the forgetting constraint corresponding to the updatedbase can be characterized by the forgetting of {forget(x, i) | i ∈ {1, . . . , n}} (resp. {forget(x, i) | x ∈ Var(B)}) in the forgettingcontext C for B.Whereas we do not put any restriction (other than consistency) on forgetting contexts, clearly some contexts are intu-itively more satisfactory than others, and in many practical situations, representing contexts by definite, binary clauses asin [6] or as specific classes of such clauses as in [35], can prove to be sufficient. However, in the rest of the paper, suchassumptions are unnecessary, which is why we prefer to allow every possible context. The only two (very weak) conditionson contexts that are often desirable and that we will need later in the paper are:• Downward normality: {¬forget(x, i) | x ∈ Var(B), i ∈ {1, . . . , n}} ∪ {C} is consistent;• Upward normality: {forget(x, i) | x ∈ Var(B), i ∈ {1, . . . , n}} ∪ {C} is consistent.Downward (resp. upward) normality means that the forgetting context does not exclude the possibility not to forgetanything (resp. the possibility to forget everything).Example 3.1. (Inspired from [31].) As a matter of illustration, let us consider the following preference merging scenario.Suppose that a group of four co-owners of a residence tries to agree about building a new tennis court t and/or a newswimming pool s. If it is constructed, the swimming pool can be either red (sr ) or blue (sb). If both a tennis court and apool – respectively, one of them, none of them – are (is) constructed, the induced cost is 2 money units (c2) – respectively,1 unit (c1), nothing (c0). The first co-owner would not like to spend more than 1 unit, and prefers a red swimming pool,J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823803should it be constructed. The second co-owner would like to improve the quality of the residence by the construction ofat least a tennis court or a swimming pool and would prefer a blue swimming pool, should it be constructed. The thirdco-owner just prefers a swimming pool to be built, whatever its colour. The fourth co-owner would like both a swimmingpool (of whatever colour) and a tennis court to be built.Of course, if there is no agreement about whether the swimming pool is to be constructed, any preference concerning itscolour must be ignored. Furthermore, it is meaningless to forget about whether the pool should be blue without forgettingwhether it should be red: either we forget about the colour of the swimming pool or not. Similarly, either we forget aboutthe expenses, i.e., we forget {c0, c1, c2} or not (i.e., we forget none of them).Clearly enough, the preferences of the group are jointly inconsistent. This scenario can be encoded in our frameworkusing the following base B = (cid:21)φ1, φ2, φ3, φ4, φ5(cid:22) and forgetting context C:• φ1 = (s ⇒ (sr ⊕ sb)) ∧ (¬s ⇒ ¬sr ∧ ¬sb) ∧ (c2 ⇔ (s ∧ t)) ∧ (c1 ⇔ (s ⊕ t)) ∧ (c0 ⇔ (¬s ∧ ¬t));• φ2 = (c0 ∨ c1) ∧ (s ⇒ sr);• φ3 = (s ∨ t) ∧ (s ⇒ sb);• φ4 = s;• φ5 = s ∧ t;• C is the conjunction of the following formulas:(cid:2)¬forget(x, 1);(cid:2)––x∈Var(φ1)i=2(forget(s, i) ⇒ forget(sr, i)) ∧ (forget(sr, i) ⇔ forget(sb, i)) ∧ (forget(c0, i) ⇔ forget(c1, i)) ∧ (forget(c1, i) ⇔5forget(c2, i)).(cid:2)x∈Var(φ1)Given that¬forget(x, 1) is a logical consequence of C, φ1 expresses an integrity constraint which cannot beweakened under any circumstance (this integrity constraint expresses that the swimming pool must be either red or blue ifit is constructed, as well as the logical definition of the induced price from the number of equipments built). φ2, φ3, φ4 andφ5 encode the preferences of the co-owners. The second conjunct of C ensures that sr and sb are irrelevant if there is noagreement on s, that sr is irrelevant if and only if sb is, and that for any j, k ∈ {0, 1, 2}, c j is irrelevant if and only if ck is. Inthe situation where all co-owners must be considered on equal terms with respect to the set of variables to be forgotten,a third conjunct must be added to C:i, j=2 forget(x, i) ⇒ forget(x, j); indeed, if we forget any atom x in φi forsome i (cid:3) 2 then x has to be forgotten in every φ j for j (cid:3) 2.x∈Var(B)(cid:2)5(cid:2)We can now define the central notions of this paper: those of forgetting vector, projection and recovery.Definition 3.3 (Forgetting vector). Let B = (cid:21)φ1, . . . , φn(cid:22) be a base and C a forgetting context for B. For any vector (cid:23)V =(cid:21)V 1, . . . , V n(cid:22) of subsets of PS, F (cid:23)V denotes the interpretation over F PS,n which assigns forget(x, i) to true if and only if x ∈ V i ,for each x ∈ PS and each i ∈ {1, . . . , n}. (cid:23)V is said to be a forgetting vector for B given C if and only if F (cid:23)V is a model of C.FC(B) denotes the set of all forgetting vectors for B given C.For any forgetting vector (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) for B given C, we defineObserve that the forgetting vectors for a base B do not depend directly on the formulas φi in it, but on the number n of(cid:23)V = V 1 ∪ · · · ∪ V n.(cid:3)such formulas. Contrastingly, FC(B) heavily depends on C.Definition 3.4 (Projection). Let B = (cid:21)φ1, . . . , φn(cid:22) be a base, C a forgetting context for B, and (cid:23)V a forgetting vector for Bgiven C. The projection of (cid:23)V on B is the set of formulas B | (cid:23)V = {∃V i.φi | i ∈ {1, . . . , n}}.Definition 3.5 (Recovery). Let B = (cid:21)φ1, . . . , φn(cid:22) be a base, and C a forgetting context for B. A vector (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) ofsubsets of PS is a recovery for B given C if and only if the following two conditions hold:(1) (cid:23)V is a forgetting vector for B given C.(2) B | (cid:23)V is consistent.RC(B) ⊆ FC(B) denotes the set of all recoveries for B given C. B is said to be recoverable with respect to C if and only ifRC(B) (cid:24)= ∅.Example 3.2. Let B and C be as in Example 3.1. All the vectors (cid:23)V 1 to (cid:23)V 9 considered in Table 1 are recoveries for B given C,while the following ones are no recoveries for B given C:• (cid:21)∅, {t}, {t}, {t}, {t}(cid:22), because ∃V 1.φ1 ∧ ∃V 2.φ2 ∧ ∃V 3.φ3 ∧ ∃V 4.φ4 ∧ ∃V 5.φ5 is inconsistent;• (cid:21)∅, {s}, {s}, {s}, {s}(cid:22), because it does not satisfy the constraints of C: if we forget s we also have to forget sb and sr .804Table 1J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823Recovery(cid:23)V 1 = (cid:21)∅, {s, sb, sr }, {s, sb, sr },{s, sb, sr }, {s, sb, sr }(cid:22)(cid:23)V 2 = (cid:21)∅, {t, sb, sr }, {t, sb, sr },{t, sb, sr }, {t, sb, sr }(cid:22)(cid:23)V 3 = (cid:21)∅, {c0, c1, c2, sb, sr },{c0, c1, c2, sb, sr },{c0, c1, c2, sb, sr },{c0, c1, c2, sb, sr }(cid:22)(cid:23)V 4 = (cid:21)∅, {sb, sr }, {sb, sr }, ∅, {t}(cid:22)(cid:23)V 5 = (cid:21)∅, {c0, c1, c2}, {sb, sr }, ∅, ∅(cid:22)(cid:23)V 6 = (cid:21)∅, {sb, sr }, ∅, ∅, {t}(cid:22)(cid:23)V 7 = (cid:21)∅, ∅, ∅, {s, sb, sr }, {s, sb, sr }(cid:22)(cid:23)V 8 = (cid:21)∅, {c0, c1, c2, sb, sr }, ∅, ∅, ∅(cid:22)(cid:23)V 9 = (cid:21)∅, ∅, {sb, sr }, ∅, {t}(cid:22)∃V 1.φ1φ1φ1φ1φ1φ1φ1φ1φ1φ1∃V 2.φ2c0 ∨ c1c0 ∨ c1(cid:3)c0 ∨ c1s ⇒ src0 ∨ c1(c0 ∨ c1) ∧(s ⇒ sr )(cid:3)(c0 ∨ c1) ∧(s ⇒ sr )∃V 3.φ3∃V 4.φ4∃V 5.φ5B | (cid:23)V(cid:3)(cid:3)s ∨ ts ∨ ts ∨ t(s ∨ t) ∧(s ⇒ sb)(s ∨ t) ∧(s ⇒ sb)(s ∨ t) ∧(s ⇒ sb)s ∨ t(cid:3)sssss(cid:3)sstss ∧ tss ∧ tsts ∧ tsφ1 ∧ ¬s ∧ t ∧ c1φ1 ∧ s ∧ ¬t ∧ c1φ1 ∧ s ∧ t ∧ c2φ1 ∧ s ∧ ¬t ∧ c1φ1 ∧ s ∧ t ∧ sr ∧ c2φ1 ∧ s ∧ ¬t ∧ sb ∧ c1φ1 ∧ ¬s ∧ t ∧ c1φ1 ∧ s ∧ t ∧ sb ∧ c2φ1 ∧ s ∧ ¬t ∧ sr ∧ c1The other columns of the table (except the rightmost one) give formulas equivalent to the “local” projections ∃V i.φi , andthe last column gives formulas equivalent to the projection of the corresponding recovery on B.By construction, replacing the pieces of information φi of B by the projection on B of any of its recoveries is sufficientto restore consistency, provided that B is recoverable.In the general case, it may happen that RC(B) is empty, because some atoms must not be forgotten. For instance, inExample 3.1, if C is strengthened, so that now c0 and c1 are protected in φ2 and s and t are protected in φ5, then thecorresponding RC(B) is empty. Now, another important sufficient condition for a base not to be recoverable is when oneof the φiis inconsistent then even forgetting all atoms from PS will not help us restoringconsistency, because ∃PS.φi ≡ ∃PS.⊥ ≡ ⊥. Therefore, for any forgetting vector (cid:23)V for B given C, (B | (cid:23)V ) is inconsistent.Conversely, when each formula φi of B is consistent and the forgetting context C is equivalent to (cid:3), then B is recoverable(especially, the forgetting vector (cid:23)V such that each V i is equal to Var(B) is a recovery for B given C).is inconsistent. Indeed, if φiIt is easy to show that for any base B, the logical strength of its forgetting context C has a direct impact on the set ofall forgetting vectors FC(B) for B given C, hence on the set of all recoveries RC(B) for B given C; to be more precise, wehave for any B:If C |(cid:12) C(cid:14)then FC(B) ⊆ FC(cid:14) (B) and RC(B) ⊆ RC(cid:14) (B).The number of forgetting vectors FC(B) for B given C may vary between 1 (remind that C is a consistent formula)and 2n|PS|, while the number of recoveries RC(B) for B given C may vary between 0 and 2n|PS|.3.2. Forgetting contextsAs expected, our definition of forgetting contexts is not very restrictive so that many different forgetting contexts canbe considered. A first distinguished forgetting context, viewed as a baseline, is the standard forgetting context, denoted CS ,where no constraint bears on the atoms to be forgotten:Definition 3.6 (Standard context). The standard forgetting context CS for B = (cid:21)φ1, . . . , φn(cid:22) is defined by CS = (cid:3).In this context, every atom can be forgotten in any piece of information, and atoms can always be forgotten in anindependent way. Obviously enough, the standard forgetting context for a base is the logically weakest one (up to logicalequivalence). Imposing stronger constraints on the forgetting vectors is a simple way to focus on expected recoveries.Thus, in some situations, the atoms forgotten in each of the pieces of information must be identical so that all sources ofinformation (or, more generally, a distinguished subset G among them) are weakened in the same way. This can be capturedby considering homogeneous contexts:Definition 3.7 (Homogeneous context). A forgetting context C for B = (cid:21)φ1, . . . , φn(cid:22) is homogeneous for G ⊆ {1, . . . , n} (ho-mogeneous for short, when G = {1, . . . , n}) if and only ifx∈Var(B) forget(x, i) ⇒ forget(x, j) is a logical consequencei, j∈Gof C.(cid:2)(cid:2)J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823805A simple case when C should not be homogeneous (unless B is consistent) is when some of the pieces of information φi¬forget(x, i) is expected to be a logical consequence of C. As explained before,must be left intact: for such a φi ,this is useful to encode integrity constraints (or revision formulas), i.e., formulas which must be protected since they arerequired to be true (just like φ1 in Example 3.1).x∈Var(φi )(cid:2)Another distinguished family of forgetting contexts consists of the binary ones, where, as soon as an atom is forgottenfrom any source of information then all other atoms from a distinguished subset V ⊆ Var(B) must be forgotten from φi .Definition 3.8 (Binary context). A forgetting context C for B = (cid:21)φ1, . . . , φn(cid:22) is binary for V ⊆ Var(B) (binary for short, whenV = Var(B)) if and only ifx, y∈V forget(x, i) ⇒ forget( y, i) is a logical consequence of C.(cid:2)ni=1(cid:2)Binary contexts are useful for scenarios where one considers that a source of information is unreliable as soon as oneof the pieces of information it gives is involved in a contradiction (and in such a case, no information from the source arepreserved: every “useful” atom from V is forgotten from it).3.3. Preferred recoveriesGenerally, many recoveries for a base given a forgetting context are possible, but most of the time, some of the corre-sponding recoveries are more expected than others. What are usually available are selection policies (or preference criteria)which characterize such expected recoveries independently from the base, in an implicit way. While forgetting contexts onlyspecify hard constraints on possible recoveries, preference criteria specify soft constraints, which have to be optimized, sothat optimal recoveries are the preferred ones.As expected, many preference criteria can be considered so as to capture in formal terms several intuitions about the waythe pieces of information should be merged. For instance, in some situations, we may prefer recoveries (cid:23)V = (cid:21)V 1, . . . , V n(cid:22)for B = (cid:21)φ1, . . . , φn(cid:22) in which the set V i of atoms forgotten in some of the φi ’s are as close to each other as possible, andideally coincide (which always is the case whenever homogeneous contexts are considered). In this extreme case, i.e., whenV 1 = · · · = V n, (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) is said to be homogeneous. Thus, if both (cid:23)V = (cid:21){a}, {b, c}(cid:22) and (cid:23)V(cid:14) = (cid:21){a}, {a, c}(cid:22) are recoveriesfor B, (cid:23)Vshould (in some contexts) be preferred to (cid:23)V because it is “more homogeneous” in some sense.(cid:14)We may also prefer recoveries that lead to forget minimal sets of atoms, where minimality may be defined using apreference criterion induced by some priorities or penalties on atoms. Finally, we may prefer recoveries that preserve asmuch as preferred pieces of information as possible (like in the coherence-based approaches to paraconsistent reasoning).Formally, each preference criterion associates to every base B and every forgetting context C for B a preorder (i.e., areflexive and transitive relation) (cid:25) on FC(B), called a preference relation.Definition 3.9 (Preference relation on forgetting vectors). Given a base B = (cid:21)φ1, . . . , φn(cid:22) and a forgetting context C for it, apreference relation is a reflexive and transitive relation (cid:25) on FC(B).for (cid:23)V (cid:25) (cid:23)V(cid:14)As usual, we write (cid:23)V (cid:2) (cid:23)V(cid:14) (cid:24)(cid:25) (cid:23)V , and (cid:23)V ∼ (cid:23)Vfor (cid:23)V (cid:25) (cid:23)Vand (cid:23)Vand (cid:23)V(cid:14) (cid:25) (cid:23)V .(cid:14)(cid:14)(cid:14)The most preferred elements from RC(B) are defined as the minimal ones for (cid:25).One could have thought of selecting the most expected recoveries by strengthening the forgetting context C underconsideration. However, such an approach is generally not feasible, because determining these most expected recoveriescannot be done a priori, that is, these recoveries are of course dependent on the base B, and their computation from B isfar from easy.Typically, the preference relation (cid:25) will satisfy the following monotonicity, or even strong monotonicity, condition (al-though we do not need to require it):• (cid:25) satisfies monotonicity if and only if ∀ (cid:23)V , (cid:23)V• (cid:25) satisfies strong monotonicity if and only if ∀ (cid:23)V , (cid:23)V(cid:14) ∈ FC(B), if (cid:23)V ⊆p(cid:14)(cid:23)V(i.e., for every i we have V i ⊆ V(cid:14) ∈ FC(B), if (cid:23)V ⊂p(cid:14)(cid:23)V(i.e., for every i we have V i ⊂ V(cid:14)i ), then (cid:23)V (cid:25) (cid:23)V(cid:14).(cid:14)i ), then (cid:23)V (cid:2) (cid:23)V(cid:14).(cid:14)(cid:23)VNote that ⊆p satisfies strong monotonicity (and a fortiori, monotonicity). It is easy to prove that for any (cid:23)V , (cid:23)V(cid:14) ∈ FC(B), if(cid:23)V ⊆pas in (cid:23)V . Accordingly, among the recoveries fromRC(B), the minimal ones w.r.t. (cid:25) = ⊆p lead to projections that preserve as much information as possible given C. This isthe rationale for the monotonicity property.holds, since we forget at least as much in (cid:23)Vthen B | (cid:23)V |(cid:12) B | (cid:23)V(cid:14)(cid:14)Example 3.3. Let B and C be as in Example 3.1. We have (cid:23)V 6 ⊆p(cid:23)V 4. Accordingly, B | (cid:23)V 6 |(cid:12) B | (cid:23)V 4.Depending on the problem at hand, many other properties on (cid:25) can be imposed so as to capture various intuitionsabout the result of the merging process. We give two of them, which echo the properties imposed on forgetting contextsand will play an important role further.806J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823Definition 3.10 (Homogeneity). Let B = (cid:21)φ1, . . . , φn(cid:22) be a base and C a forgetting context for B. A preference relation (cid:25)is said to satisfy the homogeneity property if and only if for any (cid:23)V ∈ FC(B), if (cid:21)(cid:23)V (cid:22) is in FC(B) then (cid:23)V ∼(cid:3)(cid:21)(cid:23)V , . . . ,(cid:3)(cid:3)(cid:3)(cid:23)V , . . . ,(cid:23)V (cid:22).The intuitive meaning of homogeneity is that as soon as an atom is forgotten in some of the φi ’s then it is exactly asbad as if it were forgotten everywhere.Definition 3.11 (Binaricity). Given a base B = (cid:21)φ1, . . . , φn(cid:22), and a forgetting context C for B, a preference relation (cid:25) is saidto satisfy the binaricity property if and only if, for any (cid:23)V , (cid:23)V(cid:14) ∈ FC(B) such that for every i ∈ {1, . . . , n} we have V i (cid:24)= ∅ if(cid:14)and only if V.(cid:24)= ∅, then (cid:23)V ∼ (cid:23)V(cid:14)iThe intuitive meaning of binaricity is that no matter how many atoms we forget in φi , as soon as we forget at least oneof them it is exactly as bad as if we were to forget all of them (or more precisely, all that we are allowed to forget). Observethat binaricity is equivalent to: (cid:23)V ∼ (cid:23)V∗= ∅ if V i = ∅1 , . . . , V= Var(φi) otherwise.and V(cid:22) is defined by for every i ∈ {1, . . . , n}, V, where V∗ = (cid:21)V∗n∗i∗∗iIn many cases, it is desirable to assume that the preference relation is a complete preorder. In this situation, it can beequivalently represented by a ranking function μ from FC(B) to N.The preference relation (cid:25)μ induced by μ is the complete preorder defined by ∀ (cid:23)V ∈ FC(B), (cid:23)V (cid:25)μif and only if(cid:14)). We say that a ranking function μ is decomposable if and only if there exists a total function μ0 : 2PS → Nμ( (cid:23)V ) (cid:2) μ( (cid:23)Vand a total function H : Nn → N such that μ( (cid:23)V ) = H(μ0(V 1), . . . , μ0(V n)).(cid:23)V(cid:14)Intuitively, μ0( X) is the penalty for forgetting X in some φi , and H(p1, . . . , pn) is the aggregation of penalties p1, . . . , pn.3.4. Forget-based inferencesWe are now in position to introduce the family of inference relations |≈Cletting C and (cid:25) to vary. We call them forget-based inference relations.(cid:25) that can be defined in our framework byDefinition 3.12 (Skeptical forget-based inference). Given a base B and a forgetting context C for it, let (cid:25) be a preferencerelation on FC(B) (possibly induced by a ranking function μ).• A recovery (cid:23)V ∈ RC(B) is said to be preferred (w.r.t. (cid:25)) if and only if it is minimal in RC(B) with respect to (cid:25), i.e., there(cid:14) (cid:2) (cid:23)V . The set of all preferred recoveries in RC(B) (w.r.t. (cid:25)) is denoted by Pref (RC(B), (cid:25)).(cid:25) ψ , if and• Let ψ be any formula from PROPPS. ψ is said to be (skeptically) inferred from B w.r.t. (cid:25), denoted by B |≈C(cid:14) ∈ RC(B) such that (cid:23)Vis no (cid:23)Vonly if for any preferred recovery (cid:23)V ∈ Pref (RC(B), (cid:25)), we have B | (cid:23)V |(cid:12) ψ .As far as skeptical inference is concerned, among the preferred recoveries for B, only the maximal ones with respect to⊆p are relevant for inference, in the sense that the other ones can be ignored without modifying the inference relation |≈C(cid:25).Indeed, such maximal elements correspond to the logically weakest projections.Clearly, there is no reason for considering only skeptical inferences and not defining also adventurous forget-based in-ferences, where ψ is inferred if and only if B | (cid:23)V |(cid:12) ψ holds for some preferred recovery (cid:23)V ∈ Pref (RC(B), (cid:25)). We can alsoconsider prudent forget-based inferences, obtained by first determining the pointwise union of all preferred recoveries for Bw.r.t. (cid:25), that is,(cid:4)(cid:23)V prudent =(cid:5)(cid:5)V 1, . . . ,V n(cid:23)V ∈Pref (RC (B),(cid:25))(cid:23)V ∈Pref (RC (B),(cid:25))(cid:6)and then inferring ψ if and only if (a) (cid:23)V prudent satisfies the constraints imposed by C and (b) B | (cid:23)V prudent |(cid:12) ψ . Intuitively,one first determines all preferred recoveries, and then, in each φi of B, if C allows it, we forget all atoms that are mentionedin at least one recovery in RC(B) at rank i.However, for the sake of brevity, we will focus mainly on skeptical forget-based inference; in the rest of this section, wepresent some interesting properties it satisfies.First, when (cid:25) is a complete relation, one can derive a characterization of skeptical forget-based inference in terms ofpreferred models, à la Shoham [48]. We need the following definition:Definition 3.13 ((cid:2)(cid:25)). Let B be a base, C a forgetting context for B and (cid:25) a complete preference relation on FC(B). We firstdefine ΩB = {ω ∈ Ω | ∃ (cid:23)V ∈ RC(B), ω |(cid:12) B | (cid:23)V }.• For any ω ∈ ΩB , we note(cid:7)(cid:8)RC(B)(ω), (cid:25)Pref(cid:7)(cid:9)= min(cid:23)V ∈ RC(B)(cid:10)(cid:10) ω |(cid:12) B | (cid:23)V(cid:11)(cid:8), (cid:25).J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823807• For any ω, ω(cid:14) ∈ ΩB , we note ω (cid:2)(cid:25) ω(cid:14)Pref (RC(B)(ω(cid:14)), (cid:25)) (note that because (cid:25) is complete, whether ω (cid:2)(cid:25) ω(cid:14)of (cid:23)V ω and (cid:23)V ω(cid:14) ).if and only if (cid:23)V ω and (cid:23)V ω(cid:14) are in, respectively, Pref (RC(B)(ω), (cid:25)) andholds is independent of the particular choiceProposition 3.1. Let B be a base, C a forgetting context for B, (cid:25) a complete preference relation on FC(B), and ψ a formula. We haveB |≈C(cid:25) ψ if and only if ω |(cid:12) ψ holds for every ω ∈ min(ΩB , (cid:2)(cid:25)).Whether this property can be generalized to the case (cid:25) is not complete is a non-trivial question – since it is not centralto the paper, we leave it out.Now, it is easy to check that, whatever the forgetting context, two equivalent bases have the same skeptical forget-based consequence; furthermore, every logical consequence of a skeptical forget-based consequence of a base B also is askeptical forget-based consequence of B (stated otherwise, skeptical forget-based inference satisfies the properties of “syntaxindependence” and “right weakening”). Other standard properties like here because a base is not a simple set of formulas,but a vector of formulas (hence an implicit “comma” connective is at work in our setting [30]). Such properties could beextended (possibly in several ways) to the case of vectors of formulas (see [30]) but would fail to be satisfied even in thesimple case when B contains a single formula (i.e., B is a vector of dimension 1) and the standard forgetting context for itis considered.It is also easy to show that |≈C(cid:25) satisfies:• If RC(B) (cid:24)= ∅ then B (cid:24)|≈C• If (cid:25) is such that (cid:23)∅ = (cid:21)∅, . . . , ∅(cid:22) is the unique minimal element of Pref (RC(B), (cid:25)) when it belongs to it, then if B is(cid:25) ⊥;consistent, we have for any formula ψ , B |(cid:12) ψ if and only if B |≈C(cid:25) ψ .The first property shows that a weak form of paraconsistency is achieved by skeptical forget-based inference, whilethe second one shows that under some conditions, a valuable information preservation property is also satisfied. Theseconditions are guaranteed provided that the base, the context and/or the preference relation satisfy some rather weakconditions:• If every φi in B is consistent and C is upward normal then B (cid:24)|≈C• If B is consistent, (cid:25) is strictly monotonic and C is downward normal, then for any formula ψ , B |(cid:12) ψ if and only if(cid:25) ⊥;B |≈C(cid:25) ψ .For the first point, if C is upward normal then (cid:21)PS, . . . , PS(cid:22) is a recovery for B given C. For the second point, if C isdownward normal then (cid:21)∅, . . . , ∅(cid:22) is a recovery for a consistent B given C; since (cid:25) is strictly monotonic, (cid:21)∅, . . . , ∅(cid:22) is theunique preferred recovery for B given C.Observe that considering the standard forgetting context CS and ⊆p as a preference relation on forgetting vectors isenough to ensure both conditions.We may want to compare the sets of forget-based inference relations obtained respectively by letting the set of forgettingcontexts vary and by letting the set of preference vary (that we may call the “discriminative power” offered respectively byforgetting contexts and by preference relations). As a matter of fact, when skeptical inference is considered, the discrim-inative power offered by forgetting contexts is the same as the discriminative power offered and by preference relationscoincide: every skeptical consequence of a base B given a forgetting context C for B and a preference relation (cid:25) on FC canbe recovered as a skeptical consequence of B given the standard forgetting context CS and a preference relation (cid:25)(cid:14)on FCSsuch that (cid:23)V satisfies Cwhich includes (cid:25) and satisfies the following two conditions: (i) for any two forgetting vectors (cid:23)V , (cid:23)Vand (cid:23)Vsatisfy C, then (cid:23)V (cid:2)(cid:14) (cid:23)Vsuch that both (cid:23)V and (cid:23)V(cid:14)holds if and only if (cid:23)V (cid:2) (cid:23)Vexists.) Then by construction the preferred recoveries forB given C w.r.t. (cid:25) coincide with the preferred recoveries for B given CS w.r.t. (cid:25)(cid:14); and (ii) for any two forgetting vectors (cid:23)V , (cid:23)V. (Clearly, such a preference relation (cid:25)(cid:14)does not, then (cid:23)V (cid:2)(cid:14) (cid:23)V.(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)This result typically shows that the two approaches pointed out in order to focus on expected recoveries (namely, for-getting contexts and preference relations) are equivalent from an expressiveness point of view when skeptical inference isconsidered. However, keeping both concepts in the setting makes sense. On the one hand, as mentioned previously, somesignificant computational effort must be spent to go from a preference criterion (e.g. “prefer homogeneous recoveries") anda base to the corresponding set of preferred recoveries; stated otherwise, since preference relations are typically not partof the input, generating a forgetting context which qualifies only the preferred recoveries can be very expensive, especiallyfrom the point of view of computational space. On the other hand, forgetting contexts are more explicit (they are part ofthe input) and are useful for specifying hard constraints on the recoveries.4. On the generality of our frameworkWe now show how several well-known paraconsistent inference relations belong to the family of forget-based relations.The key idea is that forget-based weakening is fine-grained enough to emulate other weakening mechanisms at work in808J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823previous approaches to reasoning under inconsistency, especially formula inhibition (considered in approaches based onpreferred consistent subbases), formula dilation (considered in distance-based merging) and abstraction-based techniques.4.1. Reasoning from preferred consistent subbasesLet us first recall the definition of inferences drawn from preferred consistent subbases also called maxcons inferencerelations (see e.g. [46,24,25,11,3,44,4]).For avoiding heavy definitions, we consider only bases consisting of consistent formulas. This is without loss of generalitywhen considering maxcons inference relations, since inconsistent formulas do not participate to any consistent subbases;accordingly, they can be removed from the base at start without any change on the inference relation.Let B = (cid:21)φ1, . . . , φn(cid:22) be a finite and non-empty vector of formulas such that each φi (1 (cid:2) i (cid:2) n) is consistent. For anyX ⊆ {1, . . . , n} we denote B X = {φi | i ∈ X}. Let Cons(B) = { X ⊆ {1, . . . , n} | B X is consistent} – Cons(B) is isomorphic to theset of consistent subbases of B.Let (cid:4) be a preorder on subsets of {1, . . . , n} (also called preference relation). Typically, (cid:4) is monotonic, and even strictlymonotonic, with respect to set containment; however, we do not have to require this. Among usual preference relations, wefind set inclusion: X (cid:4) Y if and only if X ⊇ Y , and cardinality: X (cid:4) Y if and only if | X| (cid:3) |Y |. Let Pref (Cons(B), (cid:4)) be theset of all maximal (w.r.t. (cid:4)) elements of Cons(B). Then, by definition:An inference relation |≈∀and every formula ψ , we have B |≈∀(cid:2) is a maxcons inference relation if and only if for every base B consisting of consistent formulas(cid:2) ψ if and only if ∀ X ∈ Pref (Cons(B), (cid:4)), B X |(cid:12) ψ .There are two ways of characterizing maxcons inference relations as forget-based inference relations: either by imposinga condition on the preference relation (Proposition 4.1) or by imposing a condition on the forgetting context (Proposi-tion 4.2).Proposition 4.1. An inference relation |≈ is a maxcons inference relation if and only if for every base B consisting of consistent formulas,there exists a preference relation (cid:25) on FCS (B) satisfying binaricity and such that for any formula ψ , B |≈ ψ if and only if B |≈CS(cid:25) ψ ,where CS is the standard forgetting context.Proposition 4.2. An inference relation |≈ is a maxcons inference relation if and only if for every base B consisting of consistent formulas,there exists a preference relation (cid:25) on FCB (B) such that for any formula ψ , B |≈ ψ if and only if B |≈CB(cid:25) ψ , where CB is the logicallyweakest binary context, defined byx, y∈PS forget(x, i) ⇒ forget( y, i).(cid:2)ni=1(cid:2)The first direction of Proposition 4.1 (respectively Proposition 4.2) expresses that with a preference relation satisfyingbinaricity and the standard forgetting context (respectively, with the logically weakest binary forgetting context), inferencefrom preferred recoveries comes down to inference from preferred maximal consistent subbases. The other direction ofPropositions 4.1 and 4.2 states that the set of all maxcons inference relations is contained in the set of forget-based inferencerelations, and therefore that the latter family is at least as general as the former. In particular, if preference over forgettingvectors is defined by ∀ (cid:23)V , (cid:23)V(cid:25) ψ if and only ifS |(cid:12) ψ holds for any maximal (w.r.t. set inclusion) consistent subbase S of B. Other criteria such that maximum cardinality,“discrimin” or “leximin”, or minimum penalty, can be recovered as well. The assumption that each φi (1 (cid:2) i (cid:2) n) in B isconsistent is necessary (and sufficient) to ensure that B is recoverable given its standard forgetting context. It can be made= (cid:3) otherwise,without loss of generality, just because B and Bis consistent. As an immediate consequence of Propositions 4.1have the same consistent subbases, and each formula in Band 4.2, our framework encompasses other important frameworks, like supernormal default theories with priorities [11]and syntax-based belief revision [41], as specific cases.if and only if {i | V i (cid:24)= ∅} ⊆ {i | Vis consistent and φ∗i(cid:14) ∈ FCS (B), (cid:23)V (cid:25) (cid:23)V(cid:24)= ∅} then B |≈CS(cid:22), where φ∗i= φi if φ∗1 , . . . , φ∗∗ = (cid:21)φ∗(cid:14)i∗n(cid:14)iIt is easy to show that a generalization of Proposition 4.1 (resp. Proposition 4.2) holds, replacing the forgetting context CS(resp. CB) by any logically stronger context CP . The resulting forget-based inferences correspond to maxcons inferences withprotected formulas. Here is an example of such relations:Example 4.1. Let us step back to Example 3.1. Let CP be defined by(cid:2)x∈PS¬forget(x, 1). Define (cid:25)I and (cid:25)C by• (cid:23)V (cid:25)I• (cid:23)V (cid:25)C(cid:23)V(cid:14)(cid:23)V(cid:14)if and only if {i | V i (cid:24)= ∅} ⊆ {i | Vif and only if |{i | V i (cid:24)= ∅}| (cid:2) |{i | V(cid:14)i(cid:24)= ∅};(cid:14)i(cid:24)= ∅}|.Clearly, (cid:25)I and (cid:25)C both satisfy the binaricity property.J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823809Consider the preferred recoveries for (cid:25)Ilisted below, together with their projection on B (in fact, there exist otherpreferred recoveries, but each of them is pointwise included in one of these listed below):(cid:21)∅, PS, ∅, ∅, ∅(cid:22) φ1 ∧ s ∧ t ∧ sb ∧ c2(cid:21)∅, ∅, PS, ∅, ∅(cid:22) φ1 ∧ s ∧ sr ∧ ¬t ∧ c1(cid:21)∅, ∅, ∅, PS, PS(cid:22)φ1 ∧ ¬s ∧ t ∧ c1.Therefore we have B |≈CP(s ∧ t) ⇒ sb and B |≈CP(cid:25)I(cid:25)IWith (cid:25)C , the two preferred recoveries are (cid:21)∅, PS, ∅, ∅, ∅(cid:22) and (cid:21)∅, ∅, PS, ∅, ∅(cid:22). Therefore, B |≈CP(cid:25)C¬t ⇒ sr .s holds.4.2. Belief mergingA merging operator (see e.g. [37,47,31,40]) maps any vector B = (cid:21)φ1, . . . , φn(cid:22) of consistent propositional formulas into anew consistent propositional formula Merge(B), also viewed as its set of models. When n is fixed, we define an n-mergingoperator Mergen(B) as the restriction of Merge to n-uples of formulas. In order to simplify notations, we will writeMerge((cid:21)φ1, . . . , φn(cid:22)) instead of Mergen((cid:21)φ1, . . . , φn(cid:22)) in the following.4.2.1. Distance-based belief mergingA special class of n-merging operators is the class of decomposable merging operators induced by (pseudo-)distances andaggregation functions (see [29]). Let d : Ω × Ω → N be a total function, called a pseudo-distance, satisfying ∀ω, ω(cid:14) ∈ Ω ,, and d(ω, ω(cid:14)) = d(ω(cid:14), ω). Let (cid:6) be a total function from Nn to N, monotonic in each of itsd(ω, ω(cid:14)) = 0 if and only if ω = ω(cid:14)arguments; it is called an aggregation function.For every B = (cid:21)φ1, . . . , φn(cid:22) such that each φi (1 (cid:2) i (cid:2) n) is consistent, the decomposable n-merging operator Merge(cid:6)duced by d and (cid:6) is defined by Merge(cid:6)d(ω, φi) = minω(cid:14)|(cid:12)φi d(ω, ω(cid:14)). Then, by definition, an inference relation |≈Mby d and (cid:6) if and only if for every base B consisting of n consistent formulas and every formula ψ , we have:d in-d(B) = {ω ∈ Ω | d(ω, B) is minimal}, where d(ω, B) = (cid:6)(d(ω, φ1), . . . , d(ω, φn)) andd,(cid:6) inducedd,(cid:6) is the n-merging inference relation |≈MB |≈Md,(cid:6) ψ if and only if Merge(cid:6)d(B) ⊆ Mod(ψ).In order to characterize the decomposable n-merging inference relations as forget relations, we need to focus on thosebased on a differential pseudo-distance [32]. A pseudo-distance d is said to be differential if and only if there exists a totalfunction f : 2PS → N satisfying (i) f (∅) = 0 and (ii) ∀ A, B ⊆ PS, if A ⊆ B, then f ( A) (cid:2) f (B), and such that ∀ω, ω(cid:14) ∈ Ω ,d(ω, ω(cid:14)) = f (Diff (ω, ω(cid:14))). In simpler words, if d is a differential pseudo-distance then d(ω, ω(cid:14)) is determined by the setof propositional atoms on which ω and ω(cid:14)differ. The literature on merging mainly makes use of two pseudo-distances:the Hamming distance dH , which counts the number of symbols in which the two models differ, and the so-called drasticdistance dD , which is 1 as soon as the two models differ (and 0 otherwise). These two distances are differential: for dH wehave f ( X) = | X|, and for dD we have f (∅) = 0 and f ( X) = 1 if X (cid:24)= ∅.Proposition 4.3. |≈ is a decomposable n-merging inference relation induced by a differential pseudo-distance and an aggregationfunction if and only if for every base B containing n formulas, there exists a complete preference relation (cid:25)μ on FCS (B), induced by aranking function μ satisfying decomposability, such that for any formula ψ , B |≈ ψ if and only if B |≈CS(cid:25)μ ψ .The first direction of Proposition 4.3 expresses that under the assumptions of completeness of (cid:25)μ, decomposability, andunder the standard forgetting context, forget-based inferences come down to inferences defined from a merging operator.The other direction expresses that a particularly interesting subclass of inferences from merging operators is contained inthe set of forget-based inference relations. In particular, usual arbitration and majority merging operators (see e.g. [37,31])are recovered by letting (1) μi( A) = | A| for each i ∈ {1, . . . , n}, which implies that the induced distance d is the Hammingdistance between interpretations, and (2) (cid:6) is a standard aggregation function, like max, leximax or +. Integrity constraintscan also be expressed by adding to the context a specific constraint whose effect is to protect formulas (similarly as in thelast paragraph of Section 4.1).Example 4.2. Let us consider Example 3.1 again. We now take C = CS and10 · |V 1|, |V 2|, |V 3|, |V 4|, |V 5|μ(V ) = max(cid:7)(cid:8).The role of factor 10 in 10 · |V 1| is to ensure that the integrity constraint φ1 has priority over all other formulas: since|PS| < 10, it will always be worse to forget a single atom from φ1 than forgetting any number of variables from φ2, . . . , φ5.μ satisfies decomposability: μ0( X) = | X| and H(p1, p2, p3, p4, p5) = max(10 · p1, p2, p3, p4, p5).Because φ1 is consistent, the preferred recoveries are those for which V 1 = ∅ and the maximum number of variablesto be forgotten in each of the φi ’s is minimal. Forgetting at most one variable in each φi (for i (cid:3) 2) allows for recoveringin φ3, or (b) to forget t in φ5 and either s orconsistency: it suffices either (a) to forget c0 or c1 in φ2 and s or sr810J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823sr in φ2 or s or sb in φ3. This gives many preferred recoveries (all satisfying μ(V ) = 1), among which (cid:21)∅, {sr}, ∅, ∅, {t}(cid:22),(cid:21)∅, {c0}, {s}, {s}, {s}(cid:22), (cid:21)∅, {c0}, {s}, {s}, {s}(cid:22), and so on.We can check that B |≈CS(cid:25)μ(cid:25)μ (s ∧ t) ⇒ sr . This last inference is an effect of equity (reflected by theaggregation function): if agent 2 is forced to give up her subgoal c0 ∨ c1 (by forgetting c0 or c1) then she is ensured to seeher other subgoal s ⇒ sr satisfied.s ∨ t and B |≈CSFinally, while distance-based merging can be extended to non-classical propositional settings (by considering multiple-valued semantics as in [2]), Proposition 4.3 cannot be generalized directly to those settings; indeed, the notion of variableforgetting we use is anchored on classical logic; it prevents from deriving non-trivial information from an inconsistentformula, while some propositional settings based on multiple-valued semantics do not suffer from this drawback.4.2.2. Belief merging à la Delgrande and SchaubAnother interesting merging operator which proves to be a particular case of forget-based skeptical inference is one ofthe operators proposed by Delgrande and Schaub [17] and based on variable renaming. For the sake of simplicity, we donot consider integrity constraints.Let B = (cid:21)φ1, . . . , φn(cid:22) be a base where each φi (i ∈ {1, . . . , n}) is consistent. Let Br =(cid:2)ni=1 rename(φi, i) where rename(φi, i){xi ⇔is the formula obtained from φi by replacing each occurrence of each atom x appearing in φi by xi . Let EQ =x0 | i ∈ {1, . . . , n}}. A fit for B is a subset X of EQ consistent with Br . X is a maximal fit for B if and only if it is a fit for Band none of its strict supersets is a fit for B. Lastly, the Delgrande–Schaub merging (cid:7)DS(B) is defined by, for any formula ψ ,x∈Var(B)(cid:3)(cid:7)DS(B) |(cid:12) ψ if and only if for any maximal fitX for B,Br ∧(cid:12)x∈ Xx |(cid:12) rename(ψ, 0).Example 4.3. Let B = (cid:21)a ∧ b ∧ c, ¬a, ¬a ∨ ¬b(cid:22). There are four maximal fits for B: EQ \ {a1 ⇔ a0}; EQ \ {a2 ⇔ a0, a3 ⇔ a0};EQ \ {a2 ⇔ a0, b3 ⇔ b0}; EQ \ {a2 ⇔ a0, b1 ⇔ b0}. We can check that (cid:7)DS(B) ≡ c ∧ (a ∨ b).This merging operator turns out to have a very simple characterization in terms of forgetting-based inference:Proposition 4.4. Let B be a base consisting of consistent formulas and ψ be a formula. (cid:7)DS(B) |(cid:12) ψ if and only if B |≈CSCS is the standard forgetting context and (cid:25) = ⊆p .(cid:25) ψ , whereA similar result would hold with integrity constraints: let I be a formula representing some (hard) integrity constraints,then B would contain a formula φn+1 = rename(I, n + 1) in B, and the forgetting context would forbid to forget any atomin φn+1.4.2.3. Conflict-based belief mergingIn [23], conflict-based merging operators have been introduced. In this setting, the “distance” between an interpretationand a formula is not evaluated as a numerical measure of the conflicts between them (for instance the minimum numberof variables to be switched in the interpretation to make it a model of the formula) as usual in distance-based merging,but by the conflicts themselves, i.e., the minimal sets (for set inclusion) of propositional atoms which differ between theinterpretation and the closest models of the formula. Given a profile of such formulas (a base B), each interpretation ω canbe associated with a set diff (ω, B) of vectors of sets of propositional atoms (i.e., a set of forgetting vectors) such that ω isa model of the corresponding recoveries. This set of vectors expresses in some sense how close ω is to B and preferencerelations (cid:25)B over such sets are defined in order to define the models of the merging (cid:7)diffμ (B) (as the minimal models of agiven integrity constraint μ w.r.t. (cid:25)B ). Formally:Let B = (cid:21)φ1, . . . , φn(cid:22) be a base consisting of consistent formulas φi (i ∈ {1, . . . , n}). Let ω, ω(cid:14)be interpretations from Ω ,and let μ be a formula.• A conflict vector (cid:23)V for ω given B is a recovery for B given CS such that ω |(cid:12) B | (cid:23)V and for no recovery (cid:23)V(cid:14)for B givenCS such that ω |(cid:12) B | (cid:23)V(cid:14)we have (cid:23)V(cid:14) ⊂p(cid:23)V ;• diff (ω, B) is the set of all conflict vectors for ω given B;• Let (cid:25) be a binary relation on forgetting vectors for B given CS . ω (cid:25)B ω(cid:14)diff (ω(cid:14), B), we have (cid:23)V ω (cid:25) (cid:23)V ω(cid:14) ;μ (B) is given by Mod((cid:7)diff• (cid:7)diffμ (B)) = min(Mod(μ), (cid:25)B ).if and only if ∃ (cid:23)V ω ∈ diff (ω, B) s.t. ∀ (cid:23)V ω(cid:14) ∈Conflict-based merging can be recovered as a specific forget-based inference relation whenever (cid:25) is a complete preorderincluding ⊆p . For the sake of simplicity, we assume that there are no integrity constraints, i.e., μ is a valid formula. Thenwe have:J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823811Table 2Preference relation (cid:25)(cid:25)1(cid:25)2Preferred recoveriesthat are maximal w.r.t. ⊆p(cid:23)V 1, (cid:23)V 2, (cid:23)V 3(cid:23)V 1, (cid:23)V 2, (cid:23)V 6|≈C(cid:25)s ∨ ts ⊕ tProposition 4.5. Let B be a base consisting of consistent formulas and let (cid:25) be a complete preorder on FCS (B) which includes ⊆p .For any formula ψ , we have (cid:7)diff(cid:3) (B) |(cid:12) ψ if and only if B |≈CS(cid:25) ψ .Again, a similar result would hold in presence of an integrity constraint (which should be treated as a protected formula).4.3. Homogeneous forget-based inferencesUp to now, we showed in this section that reasoning from preferred consistent subbases and some forms of merging areparticular cases of forget-based inference relations. Now, there are many other forget-based inference relations which donot belong to any of the previous families.A first example is the family of homogeneous forget-based inference relations. Such relations can be obtained by requiringthe forgetting context to be homogenous, or alternatively, requiring the preference relation (cid:25) to satisfy the homogeneityproperty (which is less demanding).When C is homogeneous, it is sufficient to focus on homogeneous recoveries, of the form (cid:23)V = (cid:21)V , . . . , V (cid:22). In sucha context, we abuse notations and simply denote (cid:23)V by the set V of variables that are uniformly forgotten. Thus,∃V .φi . Likewise, when homogeneous inference relations are considered, it is enough to define((cid:21)φ1, . . . , φn(cid:22) | V ) =a preference relation on subsets of PS (instead of vectors of subsets of PS). Many such relations can be obtained by lettingthe preference relation on 2PS vary, in quite the same manner as maxcons inference relations are obtained by letting thepreference relation on subsets vary. We may for instance minimize the set of forgotten variables (i.e., V (cid:25) Vif and only); we call the associated inference relation the prototypical homogeneous forget-based inference relation given C.if V ⊆ VAlternatively, we may minimize the number of forgotten variables, or, more generally, make use of a predefined penaltyfunction or priority preordering on variables.(cid:2)ni=1(cid:14)(cid:14)Example 4.4. We step back to Example 3.1 again. Consider the preference relations (cid:25)1 and (cid:25)1 below on 2PS and the inducedhomogeneous inference relations |≈C. We define the mapping k : 2Var(B) → N by k = kc + ks + kt where kc(V ) = 3(cid:25)1if {c0, c1, c2} ⊆ V , kc(V ) = 0 otherwise; ks(V ) = 3 if {s, sb, sr} ⊆ V , ks(V ) = 1 if V ∩ {s, sb, sr} = {sb, sr}, and ks(V ) = 0otherwise; kt(V ) = 2 if t ∈ V , kt(V ) = 0 otherwise.and |≈C(cid:25)2• (cid:25)1: V (cid:25)1 V• (cid:25)2: V (cid:25)2 V(cid:14)(cid:14)if and only if V ⊆ Vif and only if k(V ) (cid:2) k(V;(cid:14)(cid:14)).(cid:25)1 minimizes the set of atoms forgotten everywhere while (cid:25)2 minimizes their cumulated cost.The results are synthesized in Table 2. The rightmost column gives the most general consequence of B w.r.t. |≈C(cid:25), whichcan be represented using the two atoms, s and t, only.4.4. Abstraction-based inferencesIn many situations, the propositional variables have different levels of abstraction, and inconsistency can be avoided byforgetting more specific variables and keeping more abstract ones.Example 4.5. A theft has been committed in a house, three witnesses saw somebody leaving it through a window. Thefirst witness saw a young ( y) man (m) with a grey (g) cap (c), the second one an old (o) man with a black (bl) bonnet(bo) and the third one a lady (l) with a brown (br) beret (be). The three witnesses are highly contradictory since man/lady,old/young, cap/bonnet/beret, grey/black/brown are conflicting pieces of information. Nevertheless some significant pieces ofinformation can be derived by abstracting away the concepts occurring in the three witnesses: all witnesses agree that theysaw a person (p) with a dark (d) hat (h).The point is that such inferences are forget-based inferences. Indeed, the replacement scheme at work can be achievedby first conjoining the contents of each source of information with a formula, representing for instance a taxonomy andlinking specific concepts to more general ones, then forgetting concepts in each resulting formula, from the most specificones to the most general ones, until consistency is recovered.Here the initial base is B = (cid:21)φ1, φ2, φ3(cid:22) with φ1 = y ∧m ∧ g ∧ c, φ2 = o ∧m ∧ bl ∧ bo, φ3 = l ∧ br ∧ be. The theory linking theconcepts together can be represented by φ = (¬m ∨ ¬l) ∧ (¬o ∨ ¬ y) ∧ (¬c ∨ ¬bo) ∧ (¬c ∨ ¬be) ∧ (¬bo ∨ ¬be) ∧ (¬g ∨ ¬bl) ∧(¬g ∨ ¬br) ∧ (¬bl ∨ ¬br) ∧ ((m ∨ l) ⇒ p) ∧ ((c ∨ bo ∨ be) ⇒ h) ∧ ((g ∨ bl ∨ br) ⇒ d).812J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823B is first turned into the base (cid:21)φ1 ∧ φ, φ2 ∧ φ, φ3 ∧ φ(cid:22). A stratification (i.e., a totally ordered partition) S = (cid:21)C1, . . . , Ck(cid:22)of Var(φ) can then be defined so that the less significant concepts (e.g. the most specific ones) are ranked first, for instanceit can be S = (cid:21){ y, o, c, bo, be, g, bl, br}, {m, l}, {p, d, h}(cid:22). The abstraction of level i (i ∈ 1, . . . , k) of each φ j ∧ φ ( j ∈ {1, . . . , n})i−1l=1 Cl.(φ j ∧ φ). Now, a for-w.r.t. S is the most general consequence of φ j ∧ φ which is independent ofi−1getting vector (cid:23)V is abstraction-based given S when each of the V j ( j ∈ {1, . . . , n}) in it is equal tol=1 Cl for some i(i ∈ 1, . . . , k), called the abstraction level of V j , and noted a(V j). Back to the example, we have C1 = { y, o, c, bo, be, g, bl, br},C2 = {m, l} and C3 = {p, d, h}; and the following forgetting vectors are abstraction-based:i−1l=1 Cl, namely ∃(cid:3)(cid:3)(cid:3)• (cid:23)V 1 = (cid:21)C1 ∪ C2, C1 ∪ C2, C1 ∪ C2(cid:22), for which we have (B | (cid:23)V 1) ≡ ∃(C1 ∪ C2).(ϕ ∧ ϕ1) ∧ ∃(C1 ∪ C2).(ϕ ∧ ϕ2) ∧ ∃(C1 ∪ C2).• (cid:23)V 2 = (cid:21)C1, C1, C1 ∪ C2(cid:22), for which we have (B | (cid:23)V 2) ≡ ∃C1.(ϕ ∧ ϕ1) ∧ ∃C1.(ϕ ∧ ϕ2) ∧ ∃(C1 ∪ C2).(ϕ ∧ ϕ3) ≡ m ∧ p ∧ d ∧ h;• (cid:23)V 3 = (cid:21)C1 ∪ C2, C1 ∪ C2 ∪ C3, ∅(cid:22), for which we have (B | (cid:23)V 3) ≡ ∃(C1 ∪ C2).(ϕ ∧ϕ1) ∧ ∃(C1 ∪ C2 ∪ C3).(ϕ ∧ϕ2) ∧ ∃∅.(ϕ ∧ϕ3) ≡(ϕ ∧ ϕ3) ≡ p ∧ d ∧ h;l ∧ br ∧ be ∧ p ∧ d ∧ h.Note that (cid:23)V 1 is homogeneous, whereas (cid:23)V 2 and (cid:23)V 3 are not.The stratification S can be exploited in different ways in our setting. Considering the standard forgetting context for theresulting base, a preference relation (cid:25)S over forgetting vectors can be defined so that it is such that for any abstraction-based forgetting vectors (cid:23)V and (cid:23)V(cid:14)when (cid:6)(a(V 1), . . . , a(V n)) (cid:2) (cid:6)(a(Vn)) for some aggregation function (cid:6)(like max, leximax or +). In addition to this, the forgetting context may also be required to be homogeneous. The conse-quences p, d, h (and sometimes others) can then be derived using skeptical forget-based inference.(cid:14)1), . . . , a(V, (cid:23)V (cid:25)S(cid:23)V(cid:14)(cid:14)Apart of the families of forget-based inferences to which we devoted some attention in Sections 4.1, 4.2 and 4.3, manyother interesting families of forget-based inference relations can be defined. For the sake of illustration, we give here anexample of such a family, which is suitable in the situation where reliability between sources is a matter of topic. (Note thatthis family does not degenerate into one of the previous types of inference.)Here, a topic simply is a propositional atom x and a source i conveys some information about it as soon as φi is notindependent of x, i.e., φi (cid:24)≡ ∃{x}.φi ,1 as explained in the following example:Example 4.6. Let B = (cid:21)φ1, φ2(cid:22), with φ1 = a ∧ b, φ2 = ¬a ∧ ¬b. Let us consider the standard forgetting context CS for B.Assume that source 1 is more reliable to what concerns a than to what concerns b and conversely, source 2 is more reliableto what concerns b than to what concerns a. To capture this in our framework, the following ranking function can beki(v) and k1(a) = 2,considered (among other possible choices): μ((cid:21)V 1, V 2(cid:22)) = μ0(V 1) + μ0(V 2) where μ0(V i) =k1(b) = 1, k2(a) = 1, k2(b) = 2. Then the (unique) preferred recovery for B is (cid:21){b}, {a}(cid:22) and therefore we have B |≈CSa ∧ ¬b.(cid:25)μv∈V i(cid:13)5. Computational complexityWe now investigate the computational complexity of forget-based inference relations. We suppose the reader familiar2 of the polynomialwith computational complexity (see e.g. [43]), and especially the complexity classes NP, (cid:7)phierarchy.2 and Π pWe have obtained the following results:Proposition 5.1.(1) Given a base B and a forgetting context C for it, determining whether B is recoverable is NP-complete.(2) Provided that the preference relation (cid:25) on FCS (B) can be decided in polynomial time, the inference problem associated to |≈C(cid:25) isin Π p2 .(3) If moreover (cid:25) is induced by a ranking function μ computable in polynomial time, the inference problem associated to |≈C(cid:25) isin (cid:7)p2 .These results show that the gain in generality and flexibility offered by our framework does not induce a complexity shiftcompared with many more specific approaches, such as maxcons inferences, that are already Π p2 -complete when maximalityis with respect to set-inclusion. This holds as well for distance-based merging where the inference problems associated tosome operators are (cid:7)p2 -complete in the general case (see [38,27]), while the problem of forget-based inference with acomplete preference relation induced by a polytime ranking function is not harder. Hardness results for our forget-basedinference relations (in the general case) can be easily obtained from known results for maxcons inference relations, beliefrevision or merging (see e.g. [18,13,42,27]), thanks to the forget-based characterizations of such approaches, as reportedin Section 4. Hardness also holds for some prototypical homogeneous forget-based inference relations. Since homogeneousforget-based inferences are new, we establish and prove the corresponding result.1 For a different approach to inconsistency handling where topics are taken into account, see [14].J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823813Proposition 5.2. Skeptical inference for the prototypical homogeneous forget-based inference given a forgetting context C is in Π pis Π p2 -complete for some forgetting contexts C.2 . ItIn the general case (especially, when (cid:25) is not induced by a ranking function), our complexity results show that twoindependent sources of complexity must be dealt with. One of them lies in the possibly exponential number of preferredrecoveries which must be taken into account. The other one originates from the complexity of classical entailment inpropositional logic.One way to circumvent this intractability consists in compiling the base (see e.g. [12,15]). Such a compilation may consistin computing off-line the set of preferred recoveries, or only the maximal elements of it w.r.t. ⊆p . In this situation, thesource of complexity due to the number of preferred recoveries disappears (in practice, this is significant only when thenumber is small enough, which is always the case when prudent inference is considered); accordingly, the complexity of(skeptical or prudent) inference goes down to coNP. The other source of complexity can also be removed by imposing somerestrictions on the base B. In this situation, on-line forget-based inference is tractable.Proposition 5.3.(1) Provided that Pref (RC(B), (cid:25)) – or only the maximal elements of it w.r.t. ⊆p – is part of the input, the inference problem associatedto |≈CC(cid:25) (or |≈(cid:25)prudent) is in coNP.(2) If moreover each formula from B belongs to a propositional fragment (a subset of PROPPS) that is tractable for clausal entailment,) is in P whenstable for conjunction, and stable for new variable renaming,2 the inference problem associated to |≈CCNF queries are considered.C(cid:25) (or |≈(cid:25)prudentInterestingly, several well-known classes that are tractable for clausal entailment are also stable for conjunction and fornew variable renaming; for example, this is the case for the class of Krom formulas (CNF formulas consisting of binaryclauses) and for the class of CNF Horn formulas.6. DiscussionSince the family of forget-based inferences covers many different subfamilies of ways of remedying inconsistency, it isworth to comparing them on simple examples and discuss their respective merits informally.Let us start by considering the following two bases:Example 6.1. Let B = (cid:21)φ1, φ2(cid:22), and let B(cid:14) = (cid:21)φ(cid:14)1, φ(cid:14)2(cid:22), with:= a ∧ b= ¬a ∧ ¬bφ(cid:14)1φ(cid:14)2a ⇔ b(cid:3)φ1 = (a ∨ b) ∧ cφ2 = ¬a ∧ ¬b(cid:25)1 φ1 ∨ φ2(cid:25)2 c ∧ (¬a ∨ ¬b)(cid:25)3 (a ∨ b ∨ c) ∧ (¬a ∨ b) a (cid:24)⇔ b(cid:25)4 c ∧ (¬a ∨ ¬b)(cid:25)5 c ∧ (¬a ∨ ¬b)(cid:3)(cid:3)Each cell of the second column (resp. third column) of this table contains a formula equivalent to the conjunctively-interpreted set gathering all consequences of B (resp. B) w.r.t. skeptical forget-based inference.(cid:14)What should be concluded from the inconsistent bases B and B, especially when no information about the (absoluteor relative to topics) reliability of the two sources are available? There is, of course, no absolute answer to this question.The result depends on various assumptions on the merging process, and the variety of conclusions obtained through manyparaconsistent inference relations, as reported in the table, exemplifies it. The important point here is that our frameworkenables to capture each of them (here the standard forgetting context is considered for both bases):(cid:14)• (cid:25)1 corresponds to inference from maximal (w.r.t. ⊆) consistent subsets.• (cid:25)2 (resp. (cid:25)3) corresponds to distance-based merging where d is the Hamming distance and (cid:6) = + (resp. (cid:6) = max).• (cid:25)4 = ⊆p corresponds to merging à la Delgrande–Schaub.• (cid:25)5 is the prototypical homogeneous inference relation given CS .2 This means that for every formula φ from PROPPS and for every substitution rename(.,.) such that rename(φ, V ) is the formula obtained by renamingin a uniform way every variable from V occurring in φ into a new variable (not occurring in φ) if φ belongs to the class under consideration, this is alsothe case for rename(φ, V ) whatever V .814J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823Now, let us have a critical examination of the two well-known families of weakening-based approaches to paraconsistentreasoning, namely coherence-based approaches and distance-based merging.Coherence-based approaches. This quite popular family of approaches relies on a drastic weakening mechanism: the inhi-bition of some formulas from the base. It has been studied in depth, both from the logical side and from the computationalone, and its success can be explained by several factors, like the fact that it is quite simple in essence and that it en-compasses other important frameworks, including supernormal default theories with priorities [11] and syntax-based beliefrevision [41], as specific cases.Nevertheless, the coherence-based approach suffers in its actual turn from several important drawbacks which originatefrom both its high-level of syntax sensitivity and the simple weakening mechanism it relies on. Especially, it is not adaptedto situations where several sources of information must be merged (and not only because two different sources can conveythe same information).In order to illustrate such limitations, let us consider the following scenario. Let B be a base gathering informationstemming from two different sources:• The first source states φ1 = a ∧ b ∧ c;• The second source states φ2 = ¬a ∧ ¬b.Defining what are the expected consequences of B is not so simple (despite the simplicity of B). It may depend on thereliability of each source, and the relative independence of each piece of information conveyed by the sources. Specifically,it can be the case that γ1 = ¬a ∧ b ∧ c is expected, or that γ2 = ¬(a ⇔ b) ∧ c is expected. Indeed:• γ1 is expected whenever the first source is very reliable to what concerns b, but not so much to what concerns a, andthe converse holds for the second source. Furthermore, c is expected because there is no argument against it.• γ2 is expected because its models are in-between those of each source.The point is that none of these two consequences can be derived through B skeptically interpreted under the standardcoherence-based approach. Actually, there are only two maximal (w.r.t. ⊆ or cardinality) subsets of B: {φ1} and {φ2}. If φ1is strictly preferred to φ2, then φ1 is the consequence; if φ2 is strictly preferred to φ1, then φ2 is the consequence; if bothsources are equally preferred, then φ1 ∨ φ2 would be the consequence.The problem stems from the fact that each formula from B is taken as a whole, so if it is preferred, it is preferred asa whole and if it must be weakened so as to restore consistency, it is inhibited as a whole. Thus, if φ1 is preferred to φ2,every piece of information conveyed by φ1 is preferred to the possibly conflicting corresponding piece of information in φ2while this is not always expected. Furthermore, if φ1 is inhibited, every piece of information conveyed by φ1 is inhibited aswell, even those that are not involved in any contradiction.Of course, the problem can be partially solved by replacing each φi by a (logically equivalent) subset of its consequences,referred to as a decomposition of φi . However, this is not a panacea. Indeed, this solution leads to give much importance tothe syntax since several decompositions may easily give rise to different consequences. While the choice of a decompositionfor φi can be viewed as an implicit (and compact w.r.t. representation) way to express some preferences over the logicalconsequences of φi , the problem of finding out the “right decomposition” of φi (given as a single formula) remains unsolved.For instance, assume that φi = a ∧ (¬a ∨ b) is known as more reliable to what concerns b than to what concerns a; if φimust be weakened because it contradicts the piece of information ¬a ∨ ¬b (stemming from a more prioritary source),b is an expected consequence but it is not a direct subformula of the conjunctive formula φi , hence some computationaleffort must be spent to derive it. Here, it would be required to derive explicitly the most general consequence of φi thatis about b so as to give it some priority. The problem is that the computational resource to be spent so as to achieve thisgoal is high, even if we neglect the time which is consumed (this could be reasonable, provided that the decompositionis performed only once and off-line): indeed, in the general case and under the usual assumptions of complexity theory,i equivalent to ∃(PS \ X).φi , namely the most general consequence of φi that is about a given set X ofthere is no formula φ Xvariables and s.t. |φ X| is polynomial in |φi| (see e.g. [33]). Another dimension that is not handled in a satisfying way by theicoherence-based approach concerns equity in the weakening process. Indeed, in some situations, it can be expected thatconsistency is recovered by weakening each source in the same way. Removing the same number of formulas from eachsource does not solve this problem at all since what is important is the logical contents of each source, while the numberof formulas in a decomposition mainly is irrelevant.Compared with the weakening by inhibition mechanism at work in the coherence-based approaches, weakening byvariable forgetting is typically less drastic; indeed, instead of inhibiting a whole formula – or equivalently, replacing it by (cid:3)– it is possible to keep all its consequences that are not involved in any contradiction (see e.g. the piece of information c inExample 6.1). Subsequently, more information can be preserved.Distance-based merging. As recalled in Section 4.2.1, most approaches to distance-based merging are based on a notionof (pseudo-)distance between worlds (see e.g. [37,47,31,40,29]). Distance-based merging consists in weakening the piecesof information by dilating them: the piece of information φ, instead of expressing that the real world is for sure amongJ. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823815the models of φ, now expresses that it is close to be a model of φ, with respect to some given distance d (the further aworld ω from the models of φ, the less plausible it is that ω is the real world). A merging operator then maps a profileB = (cid:21)φ1, . . . , φn(cid:22) of n consistent propositional formulas into a merged base whose models are the closest to B.Distance-based merging operators from the literature are typically decomposable, that is, the distance between an in-terpretation ω and a base B = (cid:21)φ1, . . . , φn(cid:22) is computed as d(ω, B) = (cid:6)(d(ω, φ1), . . . , d(ω, φn)), where (cid:6) is an aggregationfunction. Furthermore, a decomposable merging operator is anonymous if (cid:6) is symmetric (which means that all formulas φiin B are treated equally).Even if this framework has a good level of generality, due to the variety of distance functions that can be chosen, almostall papers we know focus on the following two distances: the Hamming distance dH and the so-called drastic distance dD .However, even considering wider classes of distances will not really help solving simple examples such as the followingone.Let B = (cid:21)φ1, φ2, φ3, φ4, φ5, φ6(cid:22) where φ1 = (a ∧ b ∧ c) ∨ (¬a ∧ ¬b), φ2 = ¬c ∧ (a ⇔ b), φ3 = ¬a ∨ ¬c, φ4 = c ∧ (a ∨ b),φ5 = c and φ6 = ¬c. Assume also that the six sources are equally reliable, which requires (cid:6) to be symmetric.A quick examination on B reveals that c seems to be the source of conflict. Forgetting the information about c in eachof the φi ’s would lead to the consistent base (cid:21)a ⇔ b, a ⇔ b, (cid:3), a ∨ b, (cid:3), (cid:3)(cid:22), whose conjunction is simply equivalent to a ∧ b.Note that forgetting any other single variable (a or b) in B will not help solving the inconsistency (even forgetting both aand b will not). Therefore, first ignoring c in the process of merging the sources of information is worth doing: it gives someuseful information about everything except c (if information about c is really needed and beliefs (resp. goals) are considered,then one may perform knowledge-gathering actions (resp. start a negotiation process) so as to make a decision about c).This process is even more significant if c is a variable of secondary importance. For instance, in a decision-theoretic context,where the φi ’s represent goals, each being held by an agent, and the interpretations of Ω represent the set of alternativeswhich can be jointly chosen by the group of agents, an inconsistency simply means that the agents cannot jointly satisfytheir goals; the fact that forgetting c is simple way of getting rid of inconsistency means that a reasonable decision can bemade about a and b, leaving the decision about c undecided (again, this is even more relevant when c is not of primaryimportance).However, we will have a hard time obtaining this result a ∧ b with distance-based merging. For instance, take d to bethe Hamming distance dH . For any interpretation ω ∈ Ω , let δH (ω, B) = (cid:21)dH (ω, φi) | i = 1, . . . , 6(cid:22). We have:δH (abc, B) = (cid:21)0, 1, 1, 0, 0, 1(cid:22);δH (a¯bc, B) = (cid:21)1, 2, 1, 0, 0, 1(cid:22);δH (¯abc, B) = (cid:21)1, 2, 0, 0, 0, 1(cid:22);δH (¯a¯bc, B) = (cid:21)1, 1, 0, 1, 0, 1(cid:22);δH (ab¯c, B) = (cid:21)1, 0, 0, 1, 1, 0(cid:22);δH (a¯b¯c, B) = (cid:21)1, 1, 0, 1, 1, 0(cid:22);δH (¯ab¯c, B) = (cid:21)0, 1, 0, 1, 1, 0(cid:22);δH (¯a¯b¯c, B) = (cid:21)0, 0, 0, 2, 1, 0(cid:22).We see that three interpretations have three 1’s and three 0’s: abc, ab¯c and ¯ab¯c. Therefore, no anonymous decomposablemerging operator based on dH can output a ∧ b as the merging of B. In particular, if (cid:6) = + then the merging of B isequivalent to (a ∧ b) ∨ (¬a ∧ ¬c); if (cid:6) = leximax then the merging of B is equivalent to (a ∧ b) ∨ (¬a ∧ b ∧ ¬c); and if(cid:6) = max then it is (a ∧ b) ∨ (a ∧ ¬b ∧ ¬c) ∨ (¬a ∧ b ∧ ¬c) ∨ (¬a ∧ ¬b ∧ c).It is important noticing that the failure of distance-based merging to get simple results such as a ∧ b in the aboveexample is not due to the specific choice of the Hamming distance. Similar counterexamples could be obtained with anydistance (although it does not seem easy to construct a counterexample that would work for all distances). The reason whywe fail to get such simple outputs is the decomposability of the merging operator.7. Other related workOur approach consists in exploiting variable forgetting in order to reason in a non-trivial way from inconsistent bases.Both the problem under consideration (reasoning under inconsistency) and the method used to solve it (variable forgetting)have received much attention for years, and are described in depth in several previous papers, especially [5] and [33]. Thisexplains why we refrain from presenting them here again exhaustively and focus instead on recent and/or closely relatedwork about forgetting and/or reasoning under inconsistency.Conflict resolving by forgetting. The principle we make use of, namely, forgetting a preferred set of atoms so as to make abase consistent, has been applied recently in a few other places.[50,52,51] and [20,21] use forgetting for resolving conflicts in logic programs. These two approaches differ on the natureof the logic programs considered (classical logic programs with negation as failure in [50,52,51]; disjunctive logic programswith strong negation and negation as failure in [20,21]), but the principle at work for solving conflicts is similar: startingfrom a finite collection of logic programs (one for each of a set of agents), one looks for sets of atoms or literals to forget ineach of the logic programs so that the resulting set of logic programs is consistent.In [19], using mappings from logic programs to ontologies, notions and techniques for forgetting in logic programs areadapted to forgetting concepts in ontologies, which in turn can be a way similar to ours for resolving conflicts when merging816J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823ontologies: for instance, if the conflict between two ontologies is caused by some concept C , C will be forgotten from oneof these ontologies or from both.[22] define the forgetting of actions in a domain description (expressed in a propositional action language), and mentionas a potential application the search for joint plans for several agents, each of whom has her own goals about which actionsshould be in the joint plan. When agents’ goals conflict, a minimal set of actions to forget is looked for, and a joint plan (notmentioning these actions) is constructed. Once again, the methodology at work is in the same vein as ours, the differencebeing in the nature of the base and the objects to be forgotten.Reasoning under inconsistency. The closest work to our own one is [6] which elaborates on our previous work [35]. Inboth works, forgetting is used as a key mechanism for reasoning under inconsistency. While the present work includes anumber of characterization results and complexity results which were not available in previous papers, it goes also furtherconceptually speaking by considering a more general notion of forgetting context (as discussed previously).In [16], an inference relation in the setting of 3-valued paraconsistent logics is defined and studied. Using inconsistencyforgetting as a key mechanism for recovering consistency, it guarantees that the deductive closure of any formula is classi-cally consistent and classically closed. This allows to interpret a classically inconsistent formula as a set of classical worlds(hence to reason classically from it). The notion of forgetting considered in this paper is not the one from classical proposi-tional logic, as used here. The approach enables to reason in a non-trivial way from a single inconsistent formula, which isnot feasible using forget-based inference. Conversely, since it basically considers a single formula as input, this approach isnot suited to merging (for which distinguishing the various sources of information is important). Furthermore, it relies onquite a drastic morphology restriction of the language (only ¬, ∨ and ∧ are allowed).[9] gives a family of paraconsistent inference relations based on so-called signed systems. The formal framework theyconsider is a propositional framework not containing irreducible contradictions (like ⊥ or ¬(cid:3)) and they focus on formulasin Negation Normal Form (NNF).3 Every formula Σ is associated to a default theory (cid:21)Σ ±, DΣ (cid:22) where:• Σ ±is a formula in the language PROPPS± where PS± = {xevery occurrence of a positive literal x by the positive literal xpositive literal x−.+ | x ∈ PS} ∪ {x− | x ∈ PS}; Σ ±is obtained by replacing in Σand every occurrence of a negative literal ¬x by the+• DΣ = {δx | x ∈ Var(Σ)} is a set of prerequisite-free default rules(cid:14)δx =: x+ ⇔ ¬x−(x ⇔ x+) ∧ (¬x ⇔ x−)(cid:15)Negation is given a special treatment in such signed systems; first, every literal is rendered independent from its negationthrough renaming, this ensures the consistency of Σ ±, given the restriction put on the language; then, the correspondingdependence relations are re-introduced in a parsimonious way, so that consistency is preserved. Several inference relationscan be defined from (cid:21)Σ ±, DΣ (cid:22) (see [9] for details). Among them is skeptical unsigned inference (resp. skeptical signed infer-ence): a formula ψ of PROPPS is a skeptical unsigned (resp. signed) consequence of Σ , denoted Σ u ψ (resp. Σ s ψ ) ifand only if ψ (resp. ψ ±) belongs to every extension of (cid:21)Σ ±, DΣ (cid:22).None of these inference relations is a forget-based inference relation. First of all, in signed systems, non-trivial inferencescan be drawn starting from any single inconsistent formula Σ . For instance, we have a ∧ ¬a ∧b (cid:2)u c and a ∧ ¬a ∧b (cid:2)s c, whileforget-based inference trivializes when starting from B = (cid:21)a ∧ ¬a ∧ b(cid:22). One may object that this is because variable forgettingcannot be used to restore consistency and that the problem would disappear if the input formula Σ were a conjunctionof consistent formulas. However, this is not the case. On the one hand, as far as signed inference is considered, the setof consequences of an inconsistent Σ is not guaranteed to be consistent; for instance, both a and ¬a are skeptical signedconsequences of a ∧ ¬a ∧ b. Contrastingly, the set of consequences w.r.t. any skeptical forget-based inference relation ofany base consisting of consistent formulas is consistent, unless it is the language itself. Hence there is no way to associateevery formula Σ like a ∧ ¬a ∧ b to a vector B of consistent formulas such that the skeptical signed consequences of Σcoincide with the consequences of B w.r.t. a skeptical forget-based inference relation. On the other hand, as far as unsignedinference is considered, the choice of B has a great impact on the consequences that can be drawn w.r.t. forget-basedinferences. If a ∧ ¬a ∧ b is associated to B = (cid:21)a, ¬a ∧ b(cid:22), then the skeptical unsigned consequences of Σ (i.e., the classicalconsequences of b) can be recovered as the skeptical consequences of B w.r.t. the prototypical homogeneous forget-basedinference relation given CS . If a ∧ ¬a ∧ b is associated to B = (cid:21)a, ¬a, a ∨ b(cid:22), b can no longer be derived using the prototypicalhomogeneous forget-based inference relation given CS .Similar conclusions can be drawn for settings extending signed systems, especially those based on signed quantifiedBoolean formulas [1].3 This is without loss of expressiveness since every propositional formula can be turned into an equivalent NNF, but not without loss of succinctnesswhen connectives like ⇔ belongs to the morphology of the language.J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–8238178. ConclusionWe have proposed a new framework for reasoning from inconsistent propositional bases, which proceeds by selectingsubsets of atoms to be forgotten in the pieces of information, respecting some fixed constraints. We have shown that thisframework encompasses many existing frameworks as specific cases, including reasoning from preferred consistent subsets,and several forms of merging, and that the gain obtained in flexibility is not balanced by a complexity shift w.r.t. inference.As evoked in the introduction, our approach is based on classical entailment so that the logical contents of the baseare weakened instead of the inference relation itself. This gives to our family of forget-based relations some pros and somecons. Thus, if the base under consideration consists of a single inconsistent formula, our approach is not suited (as well asthe coherence-based approaches and the merging ones, which are also based on classical entailment). Paraconsistent logicsbased on subclassical entailment relations can then be used with profit. Conversely, since weakening bears on the inferencerelation in paraconsistent logics, it is uniform and the fact that information comes from several sources typically4 is notexploited. For instance, it is not possible to derive b from the (conjunctively-interpreted) set of formulas {a ∧ (¬a ∨ b), ¬a}using the paraconsistent logic LPm [45] (this is because ¬a ∨ b is a consequence of ¬a), while we can easily get thisconsequence in our setting.Further work can be conducted along several directions.First, inconsistency handling by variable forgetting can be easily generalized to other settings than propositional logic.All we need is a language involving several variables equipped with a forgetting operation. We have already mentioned inSection 7 how conflict solving via forgetting can be done in various forms of logic programs as well as in propositionalaction descriptions. There is no reason why we should stop here. For instance, variable forgetting has been defined in multi-agent settings [49] as well as in the propositional modal logic of knowledge S5 [53], which pave the way towards conflictsolving via forgetting in these settings. More generally, we could generalize the principle to many settings where variableforgetting (or equivalently, projection or marginalization) makes sense; this is the case for other propositional settings (e.g.when modalities or multiple-valued semantics are considered) but also for many non-propositional settings: for instance,the pieces of information given by the sources could be constraints, joint probability distributions, etc.Finally, weakening formulas by variable forgetting can be useful in many practical situations. For instance, if a series oftests has to be performed so as to “solve” inconsistency [34,36], preferred recoveries give some hints about the variablesthat should be tested first, namely, the most important ones that have to be forgotten. Similarly, in a decision-theoreticcontext where the φi ’s are no longer beliefs but preferences of several agents, variables which do not need to be forgottenare these the agents agree on, and preferred recoveries help finding the variables about which negotiation should start.AcknowledgementsThe authors thank Philippe Besnard and Torsten Schaub for many helpful discussions about the topic of this paper. Theyare also grateful to the anonymous reviewers for a number of interesting comments and suggestions.Appendix AProof of Proposition 3.1. ΩB is empty if and only if B is not recoverable. When ΩB is empty, every ψ is such that ∀ω ∈min(ΩB , (cid:2)(cid:25)), ω |(cid:12) ψ . Similarly, when B is not recoverable, we have B |≈C(cid:25) ψ for every ψ . We assume now that ΩB (cid:24)= ∅.• Let us first consider (cid:23)V ∈ Pref (RC(B), (cid:25)). Let ω be any model of B | (cid:23)V , and (cid:23)V ω ∈ Pref (RC(B)(ω), (cid:25)). Let us showthat ω is minimal w.r.t. (cid:2)(cid:25) in ΩB . First, we obviously have that ω ∈ ΩB . Now, towards a contradiction, assume thatthere exists an ω(cid:14) ∈ ΩB such that ω(cid:14) <(cid:25) ω. Let (cid:23)V ω(cid:14) ∈ Pref (RC(B)(ω(cid:14)), (cid:25)). Since ω(cid:14) <(cid:25) ω, we have that (cid:23)V ω(cid:14) (cid:2) (cid:23)V ω; andbecause (cid:23)V ∈ Pref (RC(B), (cid:25)), we have (cid:23)V ∼ (cid:23)V ω, henceforth (cid:23)V ω(cid:14) (cid:2) (cid:23)V , which contradicts the fact that (cid:23)V ∈ Pref (RC(B), (cid:25)).• Conversely, let ω ∈ min(ΩB , (cid:2)(cid:25)), and (cid:23)V ω ∈ Pref (RC(B)(ω), (cid:25)). Let us show that (cid:23)V ω ∈ Pref (RC(B), (cid:25)). Towards acontradiction, assume not. Then there must exist an ω(cid:14) ∈ ΩB and a (cid:23)V ω(cid:14) ∈ Pref (RC(B)(ω(cid:14)), (cid:25)) such that (cid:23)V ω(cid:14) (cid:2) (cid:23)V ω.Therefore, we have ω(cid:14) <(cid:25) ω, which contradicts ω ∈ min(ΩB , (cid:2)(cid:25)). (cid:3)Proof of Proposition 4.1. We say that a forgetting vector (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) is binary if for every i, either V i = PS or V i = ∅.Let us first consider the following notations:• Let B = (cid:21)φ1, . . . , φn(cid:22). For any subset S of {1, . . . , n}, we define f (S) as the binary vector (cid:23)V S = (cid:21)V 1, . . . , V n(cid:22) where foreach i ∈ {1, . . . , n}, V i = ∅ if φi ∈ S and V i = Var(φi) otherwise.• For any forgetting vector (cid:23)V = (cid:21)V 1, . . . , V n(cid:22), we define g( (cid:23)V ) = S (cid:23)V as the subset of {1, . . . , n} defined by S (cid:23)V= {i | V i = ∅},and the binary forgetting vector associated with (cid:23)V is (cid:23)V∗ = f (g( (cid:23)V )), i.e., V∗i= ∅ if V i = ∅ and V= Var(φi) otherwise.∗i4 There are nevertheless some exceptions, like [8].818J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823is a one-to-one mapping between binary forgetting vectors and subsets of {1, . . . , n}, and if (cid:23)V is binary thenfClearly,f (g( (cid:23)V )) = (cid:23)V .Now, for any S ⊆ B,(B | (cid:23)V S ) = {φi | i ∈ S} = B S .Therefore,S ∈ Cons(B)if and only if(cid:23)V S ∈ RCS (B)(1)(2)and the restriction of f to Cons(B) is a one-to-one mapping between Cons(B) and RCS (B).Next, we define the following one-to-one mapping between preference relations on Cons(B) and preference relations onRCS (B) satisfying binaricity. Let (cid:25) be a preference relation over RCS (B) satisfying binaricity. Let (cid:5) = h((cid:25)) be the relationon Cons(B) defined by: for any X, Y ∈ Cons(B), X (cid:5) Y if and only if (cid:23)V X (cid:25) (cid:23)V Y . Clearly, (cid:5) is a preorder because (cid:25) is apreorder, henceforth (cid:5) is a preference relation over consistent subsets of {1, . . . , n}.Now, we show(3)for any (cid:23)V , (cid:23)W ∈ RCS (B) we have (cid:23)V (cid:25) (cid:23)W if and only ifS (cid:23)V∗ ∈ RCS (B). Indeed, (cid:23)V ⊆p(cid:5) S (cid:23)W .First, for any (cid:23)V ∈ RCS (B), (cid:23)V ∈ RCS (B) implies (cid:23)Vshows that the consistency of (B | (cid:23)V ) implies the consistency of (B | (cid:23)V(cid:23)W ∼ (cid:23)Wnow equivalent to S (cid:23)V, from which we get (cid:23)V (cid:25) (cid:23)W if and only if (cid:23)V(cid:5) S (cid:23)W , because S (cid:23)V= S (cid:23)V ∗ and S (cid:23)W∗= S (cid:23)W ∗ .∗ (cid:25) (cid:23)W∗∗∗), which∗and, which by definition of h is equivalent to S (cid:23)V ∗ (cid:5) S (cid:23)W ∗ , which is∗). Now, (cid:5) satisfies binaricity, therefore (cid:23)V ∼ (cid:23)V, henceforth (B | (cid:23)V ) |(cid:12) (B | (cid:23)V(cid:23)VThis implies that h is a bijection between the set of preference relations on RCS (B) satisfying binaricity and the set ofpreference relations on Cons(B). For any preference relation (cid:5) on Cons(B), let k((cid:5)) = (cid:25) be the relation on RCS (B) definedby (cid:23)X (cid:25) (cid:23)Y if and only if S (cid:23)X(cid:5) S (cid:23)Y . Then k((cid:25)) is a preference relation on RCS (B) satisfying binaricity, and due to (3) we havek = h−1.Now we show that(cid:7)S ∈ PrefCons(B), (cid:5)(cid:8)if and only if(cid:23)V S ∈ Pref(cid:7)(cid:8)RCS (B), (cid:25).(4)For (⇒): assume (cid:23)V S /∈ Pref (RCS (B), (cid:25)). This means that either (cid:23)V S /∈ RCS (B), in which case S /∈ Cons(B), or that thereexists W ∈ RCS (B) such that W (cid:2) V ; in the latter case, S W ≺ S, and S W ∈ Cons(B) because W ∈ RCS (B), thereforeS /∈ Pref (Cons(B), (cid:5)).For (⇐): assume S is not a (cid:5)-preferred consistent subset of {1, . . . , n}. This means that either S is not consistent, in(cid:14) ≺ S, in which case (cid:23)V S(cid:14) ≺ (cid:23)V S and (cid:23)V S is not a preferred(cid:14) ∈ Cons(B) such that Swhich case (cid:23)V S /∈ RCS (B), or there exists Srecovery for B.We are now ready for proving Proposition 4.1.(⇒) Let |≈ be a maxcons inference relation, which by definition means that there exists a preorder (cid:5) on subsets of(cid:3), i.e., such that B |≈ ψ if and only if for any X ∈ Pref (Cons(B), (cid:5)) we have B X |(cid:12) ψ . Let−1((cid:5)). (cid:25) satisfies binaricity. From (4) we have X ∈ Pref (Cons(B), (cid:5)) if and only if (cid:23)V X ∈ Pref (RCS (B), (cid:25)). From{1, . . . , n} such that |≈ = |≈∀(cid:25) = h(1) we have (B | (cid:23)V X ) = B X , therefore (B | (cid:23)V X ) |(cid:12) ψ if and only if B X |(cid:12) ψ . Therefore, B |≈ ψ if and only if B |≈CS(⇐) Let (cid:25) be a preference relation on RCS (B) satisfying binaricity. Let (cid:5) = h((cid:25)). Then (cid:5) is a preference relation on(cid:25) is a maxcons inference, and again due to (1) and (4) we have B |≈CS(cid:25) ψ if and only if B |≈∀(cid:3) ψ , i.e., |≈CS(cid:25) ψ .{1,...,n}2relation. (cid:3)Proof of Proposition 4.2. The proof is very similar to that of Proposition 4.1. For every S ⊆ {1, . . . , n}, define the forgettingvector (cid:23)V S by (cid:23)V S = (cid:21)V 1, . . . , V n(cid:22), where V i = ∅ if φi ∈ S and V i = Var(φi) otherwise. Note that (cid:23)V S satisfies the logicallyweakest binary context. Conversely, for every forgetting vector (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) satisfying the logically weakest binarycontext, let S (cid:23)V= {i | V i = ∅}. Note that (B | (cid:23)V S ) = B S and (B | (cid:23)V ) = B S (cid:23)V.Let |≈ be a maxcons inference relation, associated with the preference relation (cid:5), that is, |≈ = |≈∀relation on forgetting vectors by (cid:23)V (cid:25) (cid:23)Varguments as those used in the proof of Proposition 4.1, we get that |≈∀if and only if S (cid:23)V(cid:5) S (cid:23)V (cid:14) . Then we have (cid:23)V (cid:25) (cid:23)V(cid:3) and |≈CB(cid:14)(cid:14)(cid:25) coincide.Conversely, let |≈CBbinary context. Define the preference relation (cid:5) over subsets of {1, . . . , n} by S (cid:5) Ssimilar arguments as those used in the proof of Proposition 4.1, we get that |≈∀(cid:25) be a forget-based inference relation associated with a preference relation (cid:25) and the logically weakestif and only if (cid:23)V S (cid:25) (cid:23)V S(cid:14) . Again, with(cid:25) coincide. (cid:3)(cid:3) and |≈CB(cid:14)Proof of Proposition 4.3. Let us first notice that any decomposable ranking function μ can be equivalently represented bya pair (cid:21)μ0, H(cid:22). Now, there exists a straightforward one-to-one mapping between decomposable ranking functions (cid:21)μ0, H(cid:22)if and only if S (cid:23)V(cid:3). Define the preference(cid:5) S (cid:23)V (cid:14) . With similarJ. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823819and pairs (cid:21) f , (cid:6)(cid:22) defining a merging operator based on a differential distance. Let us call t this mapping, defined by t(μ) =t((cid:21)μ0, H(cid:22)) = (cid:21) f , (cid:6)(cid:22) with f = μ0 and (cid:6) = H .Now, let μ be a decomposable ranking function on subsets of PS and d be its corresponding differential distance definedby d(ω, ω(cid:14)) = μ0(Diff (ω, ω(cid:14))). Let μB (ω) = min({μ( (cid:23)V ) | (cid:23)V ∈ RCS (B) and ω |(cid:12) B | (cid:23)V }). We first establish that d(ω, B) =μB (ω) holds for any ω ∈ Ω .Let ω ∈ Ω and B = (cid:21)φ1, . . . , φn(cid:22). Since (cid:6) is monotonic, we have:d(ω, B) = min= min= min(cid:8)(cid:11)(cid:8)(cid:7)(cid:9)(cid:7)(cid:9)(cid:7)(cid:9)(cid:6)(cid:6)(cid:7)d(ω, φ1), . . . , d(ω, φn)(cid:7)d(ω, ω1), . . . , d(ω, ωn)(cid:8)(cid:7)Diff (ω, ω1)μ0(cid:7)(cid:8) (cid:10)(cid:10) ω1 |(cid:12) φ1, . . . , ωn |(cid:12) φn(cid:7)(cid:8)(cid:8)(cid:8) (cid:10)(cid:7)(cid:11)(cid:8)(cid:11)(cid:8)(cid:10) ω1 |(cid:12) φ1, . . . , ωn |(cid:12) φn.HDiff, . . . , μ0ω, ωnLet (cid:23)V = (cid:21)Diff (ω, ω1), . . . , Diff (ω, ωn)(cid:22). Using Corollary 5 of [33], for every i ∈ {1, . . . , n} we have ω |(cid:12) ∃Diff (ω, ωi).φi becauseωi |(cid:12) φi . Therefore, ω |(cid:12) (B | (cid:23)V ), hence, (cid:23)V ∈ RCS (B). Now, H(μ0(Diff (ω, ω1)), . . . , μ0(Diff (ω, ωn))) = μ( (cid:23)V ) by definitionof μ. Therefore, d(ω, B) = min({μ( (cid:23)V ) | (cid:23)V ∈ RCS (B) and ω |(cid:12) B | (cid:23)V }) = μB (ω).Now, since the standard forgetting context C is considered, (cid:21)PS, . . . , PS(cid:22) is a forgetting vector for B. Since each φi (i ∈{1, . . . , n}) is consistent, every interpretation ω ∈ Ω is a model of B | (cid:21)PS, . . . , PS(cid:22), showing that ΩB = Ω . Furthermore,given ω, ω(cid:14) ∈ Ω , μB (ω) (cid:2) μB (ω(cid:14)) if and only if each (cid:23)V in min({μ( (cid:23)V ) | (cid:23)V ∈ RCS (B) and ω |(cid:12) B | (cid:23)V }) and each (cid:23)Vinmin({μ( (cid:23)V ) | (cid:23)V ∈ RCS (B) and ω(cid:14) |(cid:12) B | (cid:23)V }) are such that (cid:23)V (cid:25)μif and only if ω (cid:2)(cid:25)μ ω(cid:14)d,∗ ψ if and only if for every ω ∈ Ω such that d(ω, B) = minω(cid:14)∈Ω d(ω(cid:14), B) we have ω |(cid:12) ψ , which, using aprevious equality d(ω, B) = μB (ω), is equivalent to: for every ω ∈ Ω such that μB (ω) = minω(cid:14)∈Ω μB (ω(cid:14)) we have ω |(cid:12) ψ ,which is also equivalent to: for every ω ∈ min(ΩB , (cid:2)(cid:25)μ ) we have ω |(cid:12) ψ .Lastly, B |≈M(cid:23)V.(cid:14)(cid:14)Since (cid:25)μ is complete (by construction), Proposition 3.1 shows that this is equivalent to B |≈CS(cid:25)μ ψ . (cid:3)−1( (cid:23)V ) = {xi ⇔ x0 | x ∈ PS, 1 (cid:2) i (cid:2) n, x /∈ V i}.Proof of Proposition 4.4. We start by introducing the following notations: for any X ⊆ EQ , let F ( X) = (cid:21)V 1, . . . , V n(cid:22) whereeach V i is defined by V i = {x | x0 ⇔ xi /∈ X}. Clearly, F is bijective and for any vector (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) of subsets of PS wehave F(cid:3)nWe associate with PS the sets PS0, . . . , PSn s.t. for each i ∈ {0, . . . , n}, PSi = {xi | x ∈ PS}. Every interpretation M overi=0 PSi can be represented by a tuple (ω0, . . . , ωn) s.t. for each i ∈ {0, . . . , n}, ωi is the restriction of M to PSi . Finally,given an interpretation ωi over PSi , we can associate to it in a bijective way the interpretation ω∗i over PS s.t. ∀x ∈ PS,i (x) = ωi(xi). Clearly enough, for each i ∈ {1, . . . , n} and each formula φ ∈ PROPPS, ω∗ω∗|(cid:12) φ if and only if ωi |(cid:12) rename(φ, i).The proof now goes by successively proving the following lemmata.iLemma 8.1. X ⊆ EQ is a fit for B if and only if F ( X) is a recovery for B given CS .Lemma 8.2. X ⊆ EQ is a maximal fit for B if and only if F ( X) is a preferred recovery w.r.t. ⊆p for B given CS .Lemma 8.3.(cid:7)Mod(cid:14)(cid:8)B|F (X)=ω∗0∈ 2PS(cid:10)(cid:10)(cid:10) M |(cid:12) Br ∧(cid:12)x∈ X(cid:15)x for some M = (ω0, ω1, . . . , ωn).Lemma 8.4. Br ∧(cid:2)x∈ X x |(cid:12) rename(ψ, 0) if and only if (B | F ( X)) |(cid:12) ψ .Proof of Lemma 8.1. Let X ⊆ EQ be a fit for B, which means that Br ∧M = (ω0, ω1, . . . , ωn) of Br ∧x∈ X x over(cid:3)ni=0 PSi . By definition of Br we have(cid:2)(cid:2)x∈ X x is consistent, therefore there exists a modelfor each i ∈ {1, . . . , n}, ωi |(cid:12) rename(φi, i)(cid:2)and M |(cid:12)x∈ X x is equivalent to(1)for all xi ⇔ x0 in X, ω0(x0) = ωi(xi).This shows that for each i ∈ {1, . . . , n}, ω∗those x s.t. xi ⇔ x0 /∈ X , namely those x ∈ V i . Therefore, ω∗vector is admissible for the standard forgetting context, this shows that (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) is a recovery for B given CS .i (a model of φi ) on every variable from PS except possibly on0 is a model of ∃V i.φi for each i ∈ {1, . . . , n}. Since every forgetting0 coincides with ω∗Conversely, let V = (cid:21)V 1, . . . , V n(cid:22) be a recovery for B given CS , then∃V i.φi is consistent. Hence it has a model ω andthere exist n models ω∗i coincide on PS \ V ifor every i ∈ {1, . . . , n}. Let ω0 be the interpretation over PS0 s.t. ∀x0 ∈ PS0 we have ω0(x0) = ω(x). Let M = (ω0, ω1, . . . , ωn).(cid:2)Condition (a) implies M |(cid:12) Br and condition (b) implies M |(cid:12)x∈ X x, which shows that X is afit for B. (cid:3)|(cid:12) φi for each i ∈ {1, . . . , n} and (b) ω and ω∗n over PS such that (a) ω∗x∈ X x, therefore M |(cid:12) Br ∧1, . . . , ω∗(cid:2)ni=1(cid:2)i820J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823Proof of Lemma 8.2. Let X be a fit for B. If X is not maximal then there exists a fit XLemma 8.1, F ( X(cid:14)) is a recovery for B given CS and obviously, F ( XConversely, let (cid:23)V be a recovery for B given CS . If (cid:23)V is not preferred w.r.t. (cid:25) = ⊆p then there exists a recovery (cid:23)V(cid:14)for B such that X(cid:14) ⊃ X . Then, by(cid:14)) (cid:2)p F ( X), therefore F ( X) cannot be a maximal fit for B.for(cid:14)) for B (thanks to Lemma 8.1) is such that−1( (cid:23)V(cid:14)B given CS such that (cid:23)V−1( (cid:23)VF(cid:14) ⊂p−1( (cid:23)V ), therefore F(cid:14)) ⊃ F(cid:23)V , which entails that the associated fit F−1( (cid:23)V ) is not a maximal fit for B. (cid:3)Proof of Lemma 8.3. Let M = (ω0, ω1, . . . , ωn) be a model of Br ∧Lemma 8.1, we derive that ω∗x∈ X x over0 is a model of ∃V i.φi for each i ∈ {1, . . . , n}. Hence, ω∗(cid:2)(cid:3)ni=0 PSi . Using the same proof as in0 is a model of B | (cid:23)V with (cid:23)V = F ( X).Conversely, let ω∗0 be any model of B |(cid:12) (cid:23)V where (cid:23)V is a recovery for B given CS . Then for all i ∈ {1, . . . , n}, ω∗|(cid:12) ∃V i.φi ,0 and ω∗which by Corollary 5 of [33], entails that there exists an interpretation ω∗|(cid:12) φi and ω∗icoincide on all variables from PS except possibly on those of V i . Then let M = (ω0, ω1, . . . , ωn). The fact that ω∗|(cid:12) φi forall i ∈ {1, . . . , n} entails that ωi |(cid:12) rename(φi, i) for all i ∈ {1, . . . , n}, hence M |(cid:12) Br . Finally, the fact that for all i ∈ {1, . . . , n},0 and ω∗ω∗x∈ X x, and this concludes theproof. (cid:3)i coincide on all variables from PS except possibly on those of V i shows that M |(cid:12)i over PS such that ω∗(cid:2)0ii(cid:3)noverB | (cid:23)V |(cid:12) ψ .(cid:2)Proof of Lemma 8.4. Assume Br ∧we have M |(cid:12) rename(ψ, 0), which is equivalent to ω0 |(cid:12) rename(ψ, 0) and also equivalent to ω∗x∈ X x |(cid:12) rename(ψ, 0). Let (cid:23)V = F ( X). Then for all M = (ω0, ω1, . . . , ωn) |(cid:12) Br ∧|(cid:12) ψ .By Lemma 8.3, every model ω∗0 of B | (cid:23)V over PS is such that there exists a model M = (ω0, ω1, . . . , ωn) of Br ∧0i=0 PSi . Hence we must have ω∗0|(cid:12) ψ , and since this must hold for every model ω∗x∈ X x0 of B | (cid:23)V , we conclude thatx∈ X x(cid:2)(cid:2)Conversely, assume that B | (cid:23)V |(cid:12) ψ . Towards a contradiction, assume that Br ∧ists an interpretation M = (ω0, ω1, . . . , ωn) which satisfies Br ∧However, Lemma 8.3 shows that ω∗(cid:2)(cid:2)x∈ X x but does not satisfy rename(ψ, 0). Hence ω∗x∈ X x (cid:24)|(cid:12) rename(ψ, 0). Then there ex-(cid:24)|(cid:12) ψ .00 must be a model of B | (cid:23)V . This contradicts the fact that B | (cid:23)V |(cid:12) ψ . (cid:3)(cid:2)Proof of Proposition 4.4. By definition, B |≈DS ψ is equivalent to Br ∧x∈ X x |(cid:12) rename(ψ, 0) for any maximal fit X for B,which, using both Lemmata 8.2 and 8.4, is equivalent to B | (cid:23)V |(cid:12) ψ for every recovery (cid:23)V for B given C, that is preferredw.r.t. ⊆p . (cid:3)Proof of Proposition 4.5. We take advantage of Proposition 3.1. We obviously have ΩB = Ω since by assumption B con-sists of consistent formulas and the forgetting context is the standard one (indeed (cid:21)PS, . . . , PS(cid:22) is a forgetting vector for Bgiven CS ). Hence it is enough to show that for any interpretations ω, ω(cid:14) ∈ Ω , we have ω (cid:25)B ω(cid:14). First,observe that since (cid:25) includes ⊆p , for any interpretation ω ∈ Ω , we have Pref (RC(B)(ω), (cid:25)) ⊆ diff (ω, B), and for any inter-) ∈ diff (ω, B) (resp. ∈ diff (ω(cid:14), B)), there exists (cid:23)W (resp. (cid:23)W (cid:14)) ∈ Pref (RC(B)(ω), (cid:25))pretations ω, ω(cid:14) ∈ Ω , for every (cid:23)V (resp. (cid:23)V(resp. ∈ Pref (RC(B)(ω(cid:14)), (cid:25))) such that (cid:23)W (cid:25) (cid:23)V (resp. (cid:23)W (cid:14) (cid:25) (cid:23)V (cid:14)). Now:if and only if ω (cid:2)(cid:25) ω(cid:14)(cid:14)• If ω (cid:25)B ω(cid:14)then by definition ∃ (cid:23)V ω ∈ diff (ω, B) s.t. ∀ (cid:23)V ω(cid:14) ∈ diff (ω(cid:14), B), we have (cid:23)V ω (cid:25) (cid:23)V ω(cid:14) . Hence there exists (cid:23)W ω ∈Pref (RC(B)(ω), (cid:25)) s.t. (cid:23)W ω (cid:25) (cid:23)V ω. Furthermore, diff (ω(cid:14), B) is not empty since every ω(cid:14)is a model of (B | (cid:21)PS, . . . , PS(cid:22)).Hence for each (cid:23)V ω(cid:14) ∈ diff (ω(cid:14), B), we can find a (cid:23)W ω(cid:14) ∈ Pref (RC(B)(ω(cid:14)), (cid:25)) s.t. (cid:23)W ω(cid:14) (cid:25) (cid:23)V ω(cid:14) . Since (cid:23)W ω(cid:14) belongs todiff (ω(cid:14), B), we must have (cid:23)V ω (cid:25) (cid:23)W ω(cid:14) Since (cid:25) is a preorder, from (cid:23)W ω (cid:25) (cid:23)V ω and (cid:23)V ω (cid:25) (cid:23)W ω(cid:14) , we get that (cid:23)W ω (cid:25) (cid:23)W ω(cid:14) .This shows that ω (cid:2)(cid:25) ω(cid:14)(cid:23)V ω ∈ diff (ω, B). Since (cid:25) is a complete preorder, (cid:23)V ω(cid:14) ∈ Pref (RC(B)(ω(cid:14)), (cid:25)) is such that (cid:23)V ω(cid:14) (cid:25) (cid:23)Vdiff (ω(cid:14), B). Hence, by transitivity of (cid:25), (cid:23)V ω ∈ diff (ω, B) is such that (cid:23)V ω (cid:25) (cid:23)Vthat ω (cid:25)B ω(cid:14), then by definition ∃ (cid:23)V ω ∈ Pref (RC(B)(ω), (cid:25)) ∃ (cid:23)V ω(cid:14) ∈ Pref (RC(B)(ω(cid:14)), (cid:25)) s.t. (cid:23)V ω (cid:25) (cid:23)V ω(cid:14) . We haveω(cid:14) for every (cid:23)V (cid:14)ω(cid:14) ∈ω(cid:14) ∈ diff (ω(cid:14), B). This showsω(cid:14) for every (cid:23)V (cid:14)• Conversely, if ω (cid:2)(cid:25) ω(cid:14). (cid:3).(cid:14)(cid:14)Proof of Proposition 5.1.• Point 1.◦ Membership. Let B = (cid:21)φ1, . . . , φn(cid:22) be a base and let C be a forgetting context for B. The following non-deterministicalgorithm can be used to determine whether B is recoverable:1. Guess V 1, . . . , V n n subsets of Var(B);2. Guess ω, ω1, . . . , ωn n + 1interpretations over Var(B);3. For each i ∈ {1, . . . , n} doCheck that F (cid:23)VCheck that ωi is a model of φi;Check that ω coincides with ωi on Var(φi) \ V i.|(cid:12) C where (cid:23)V = (cid:21)V 1, . . . , V n(cid:22);The last point in the algorithm above enables to check that ω |(cid:12) ∃V i . φi holds for each i ∈ {1, . . . , n}. This impliesthat B | (cid:23)V is consistent, hence B is recoverable.J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823821◦ Hardness. Easy, by reduction from sat. To every CNF formula Σ consisting of n clauses γ1, . . . , γn, we associate inpolynomial time the base B = (cid:21)γ1, . . . , γn(cid:22) and the forgetting context C =¬forget(x, i). Clearly enough,only one recovery is possible given C: (cid:21)∅, . . . , ∅(cid:22). This vector actually is a recovery for B if and only if Σ is satisfiable.2 . The following non-deterministic• Point 2. Let us consider the complementary problem and show that it belongs to Σ p(cid:2)ni=1x∈PS(cid:2)algorithm can be used to determine whether B (cid:24)|≈C1. Guess V 1, . . . , V n n subsets of Var(B)s.t. (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) is a recoveryfor B given C and B | (cid:23)V (cid:24)|(cid:12) ψ ;2. Check that (cid:23)V is a preferred recovery(cid:25) ψ :for B w.r.t. (cid:25).The proof of Point 1 above shows that the first step of this algorithm can be done in non-deterministic polynomialtime (just add a final step “check that ω (cid:24)|(cid:12) ψ holds” to the non-deterministic algorithm given in the proof ofPoint 1). This is also the case for the second step. Indeed, once (cid:23)V has been guessed, in order to check that it is not apreferred recovery for B, it is sufficient to guess a recovery (cid:23)Vfor B (again, just one call to an NP oracle) and to checkin deterministic polynomial time that (cid:23)V(cid:14) (cid:2) (cid:23)V .(cid:14)• Point 3.(i) First of all, for any possible value k ∈ N which can be taken by μ( (cid:23)V ), it is possible to guess V 1, . . . , V n n subsets ofVar(B) s.t. (cid:23)V = (cid:21)V 1, . . . , V n(cid:22) is a recovery for B given C (see the proof of Point 1 above), and s.t. μ( (cid:23)V ) = k (just adda final step “check that μ( (cid:23)V ) = k holds” to the non-deterministic algorithm in the proof of Point 1). Sinceμ( (cid:23)V ) can be computed in deterministic polynomial time, we obtain a non-deterministic algorithm to check in timepolynomial in the cumulated size of B, C and k whether there exists a recovery (cid:23)V for B s.t. μ( (cid:23)V ) = k.(ii) Now, if μ can be computed in time polynomial in the size of the input then the size of the binary representationof μ( (cid:23)V ) for any (cid:23)V ∈ RC(B) is bounded by a polynomial in the input size. Accordingly, the value of μ( (cid:23)V ) isbounded by a (simple) exponential in the input size. Through binary search, a polynomial number of calls to thenon-deterministic algorithm described above is sufficient to find out the minimal value min ∈ N s.t. there exists arecovery (cid:23)V for B given C and μ( (cid:23)V ) = min.(iii) Once min has been computed, a last call to an NP oracle is sufficient to guess a preferred recovery (cid:23)V for B s.t.B | (cid:23)V (cid:24)|(cid:12) ψ (just add a final step “check that ω (cid:24)|(cid:12) ψ holds” to the non-deterministic algorithm given in theproof of Point 1).(iv) The fact that (cid:7)p2 is closed for the complement concludes the proof. (cid:3)Proof of Proposition 5.2. Membership comes directly from item 2 of Proposition 5.1. Hardness is obtained by a reductionfrom skeptical inference from supernormal default theories without background theory. The latter problem consists in de-termining, given a set (cid:7) = {φ1, . . . , φn} of propositional formulas and a propositional formula α, whether S |(cid:12) α holds forevery maximal consistent subset S of (cid:7). It is known to be Π p2 -complete [26]. We map any instance (cid:21)(cid:7), α(cid:22) of this problemto the following instance of skeptical inference from prototypical homogeneous forget-based inference:• B = (cid:21)h1 ⇒ φ1, . . . , hn ⇒ φn, h1, . . . , hn(cid:22), where each hi (i ∈ {1, . . . , n}) is a fresh propositional symbol (not belonging toVar((cid:7)) ∪ Var(α));• ψ = α;(cid:2)• C =2ni=1((cid:2)x∈Var((cid:7))∪Var(α)¬forget(x, i)) ∧(cid:2)i, j∈1,...,2n(cid:2)nk=1(forget(hk, i) ⇒ forget(hk, j)).Since C is homogeneous, any forgetting vector for B given C is of the form (cid:23)V J = (cid:21)V J , . . . , V J (cid:22), where V J ⊆ {h1, . . . , hn}.Let J = {i ∈ {1, . . . , n} | hi ∈ V J }.We have∃V J .(hi ⇒ φi) ≡(cid:14)(cid:3)hi ⇒ φiif i ∈ J ,if i /∈ Jwhereas∃V J .hi ≡(cid:14)(cid:3) if i ∈ J ,if i /∈ J .hiTherefore,n(cid:12)(cid:7)(B | (cid:23)V J ) ≡∃V J .(hi ⇒ φi) ∧ ∃V J .hi(cid:12)(cid:8)≡(cid:3) ∧(cid:12)(cid:7)(cid:8)(h j ⇒ φ j) ∧ (h j)≡(cid:12)(h j ∧ φ j).i=1Since {h1, . . . , hn} ∩ Var((cid:7)) = ∅, (cid:23)V Jis apreferred recovery w.r.t. ⊆p for B given C if and only if {φ j ∈ (cid:7) | j /∈ J } is a maximal (w.r.t. ⊆) consistent subset of (cid:7).Therefore, we have that for every maximal consistent subset S of (cid:7), S |(cid:12) α holds if and only if B |≈C⊆pis a recovery for B given C if and only if {φ j ∈ (cid:7) | j /∈ J } is consistent, and (cid:23)V Jψ . (cid:3)j /∈ Jj /∈ Jj∈ J822J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823Proof of Proposition 5.3. We first show thatn(cid:12)i=1∃V i.φi |(cid:12) ψ if and only ifn(cid:12)i=1rename(φi, V i) |(cid:12) ψ.i .rename(φi, V i), where V iConsider any quantified formula of the form ∃V i.φi . Since quantified variables are dummy ones, ∃V i.φi∃V iby xi . As a consequence, we get thatwith i (cid:24)= j, no variable from V i(cid:3)alent to ∃ni=1 V i(cid:2)(cid:3)i=1 rename(φ, V i)) |(cid:12) ψ if and only if∃nni=1 V ii .(is equivalent to= {xi | x ∈ V i} and rename(φi, V i) is the formula obtained by renaming in φi every x ∈ V ii .rename(φ, V i). Now, for every i, j ∈ {1, . . . , n}i .rename(φ, V i) is equiv-i , we have that∃V i(cid:2)ni=1 rename(φ, V i)). Finally, since ψ is independent from the new variables(cid:2)ni=1jj .rename(φ, V j). Hence, we have that(cid:2)n(cid:2)ni=1i occurs in any ∃Vi=1 rename(φ, V i) |(cid:12) ψ .∃V i.φ is equivalent to(cid:3)ni=1 V i(cid:2)ni=1∃V ii .(i(cid:2)ni=1(1) If Pref (RC(B), (cid:25)) – or only the maximal elements of it w.r.t. ⊆p – are provided then deciding whether B = (cid:21)φ1, . . . , φn(cid:22)(cid:25) ψ holds simply amounts to deciding whether B | (cid:23)V |(cid:12) ψ for every given (cid:23)V . Now, each B | (cid:23)V |(cid:12) ψ canis such that B |≈Cbe polynomially reduced to an instance of the prototypical coNP-complete problem unsat. Indeed, (cid:21)φ1, . . . , φn(cid:22) | (cid:23)V |(cid:12) ψi=1 rename(φi, V i) ∧ ¬ψ isholds if and only ifunsatisfiable.To conclude, it is enough to observe that a linear number of unsat instances can be polynomially reduced to a singleone (through variable renaming), and since coNP is closed under polynomial reduction, the conclusion follows.A similar demonstration can be achieved for prudent inference (the first step consists in computing (cid:23)V prudent which canbe done easily in polynomial time when Pref (RC(B), (cid:25)) – or only the maximal elements of it w.r.t. ⊆p – are given aspart of the input; the second step is to polynomially reduce B | (cid:23)V prudent |(cid:12) ψ to an instance of unsat, as above.i=1 rename(φi, V i) |(cid:12) ψ if and only if∃V i.φi |(cid:12) ψ if and only if(cid:2)n(cid:2)n(2) If in addition every φi of B belongs to a propositional fragment which is stable by conjunction and by new variable(cid:2)ni=1 rename(φi, V i) belongs as well to this fragment. If thisi=1 rename(φi, V i) |(cid:12) ψ holds can be achieved in poly-renaming, then for each given (cid:23)V (resp. for (cid:23)V = (cid:23)V prudent),fragment is tractable for clausal entailment, deciding whethernomial time when ψ is a CNF formula. (cid:3)(cid:2)nReferences[1] O. Arieli, Paraconsistent reasoning and preferential entailments by signed quantified boolean formulae, ACM Transactions on Computational Logic 8 (3)(2007).[2] O. Arieli, Distance-based paraconsistent logics, International Journal on Approximate Reasoning 48 (3) (2008) 766–783.[3] C. Baral, S. Kraus, J. Minker, Combining multiple knowledge bases, IEEE Transactions on Knowledge and Data Engineering 3 (2) (1991) 208–220.[4] S. Benferhat, C. Cayrol, D. Dubois, J. Lang, H. Prade, Inconsistency management and prioritized syntax-based entailment, in: Proceedings of the 13thInternational Joint Conference on Artificial Intelligence (IJCAI’93), 1993, pp. 640–645.[5] L.E. Bertossi, A. Hunter, T. Schaub (Eds.), Inconsistency Tolerance, Lecture Notes in Computer Science, vol. 3300, Springer, 2005.[6] Ph. Besnard, Remedying inconsistent sets of premises, International Journal on Approximate Reasoning 45 (2) (2007) 308–320.[7] Ph. Besnard, A. Hunter, Introduction to actual and potential contradictions, in: Handbook of Defeasible Reasoning and Uncertainty Management Sys-tems, vol. 2, Kluwer Academic Publishers, 1998, pp. 1–11.[8] Ph. Besnard, T. Schaub, Circumscribing inconsistency, in: Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI’97),1997, pp. 150–155.[9] Ph. Besnard, T. Schaub, Signed systems for paraconsistent reasoning, Journal of Automated Reasoning 20 (1) (1998) 191–213.[10] G. Boole, An Investigation of the Laws of Thought, Macmillan Publishers, Basingstoke, 1854.[11] G. Brewka, Preferred subtheories: An extended logical framework for default reasoning, in: Proceedings of the 11th International Joint Conference onArtificial Intelligence (IJCAI’89), 1989, pp. 1043–1048.[12] M. Cadoli, F.M. Donini, A survey on knowledge compilation, AI Communications 10 (1997) 137–150 (printed in 1998).[13] C. Cayrol, M.-C. Lagasquie-Schiex, Th. Schiex, Nonmonotonic reasoning: From complexity to algorithms, Annals of Mathematics and Artificial Intelli-gence 22 (3–4) (1998) 207–236.[14] L. Cholvy, Automated reasoning with merged contradictory information whose reliability depends on topics, in: Proceedings of the European Conferenceon Symbolic and Quantitative Approaches to Reasoning and Uncertainty (ECSQARU’95), 1995, pp. 125–132.[15] S. Coste-Marquis, P. Marquis, Compiling stratified belief bases, in: Proceedings of the 14th European Conference on Artificial Intelligence (ECAI’00),2000, pp. 23–27.[16] S. Coste-Marquis, P. Marquis, Recovering consistency by forgetting inconsistency, in: Proceedings of the 11th European Conference on Logics in ArtificialIntelligence (JELIA’08), in: Lecture Notes in Artificial Intelligence, vol. 5293, Springer, 2008, pp. 113–125.[17] J. Delgrande, T. Schaub, Two approaches for merging belief bases, in: Proceedings of the 9th European Conference on Logics in Artificial Intelligence(JELIA’04), in: Lecture Notes in Computer Science, vol. 3229, 2004, pp. 426–438.[18] Th. Eiter, G. Gottlob, On the complexity of propositional knowledge base revision, updates and counterfactuals, Artificial Intelligence 57 (1992) 227–270.[19] Th. Eiter, G. Ianni, R. Schindlauer, H. Tompits, K. Wang, Forgetting in managing rules and ontologies, in: Proceedings of the 2006 IEEE/WIC/ACMInternational Conference on Web Intelligence (WI’06), 2006, pp. 411–419.[20] Th. Eiter, K. Wang, Forgetting and conflict resolving in disjunctive logic programming, in: Proceedings of the 21st National Conference on ArtificialIntelligence (AAAI’06), 2006, pp. 238–243.[21] Th. Eiter, K. Wang, Semantic forgetting in answer set programming, Artificial Intelligence 172 (14) (2008) 1644–1672.[22] E. Erdem, P. Ferraris, Forgetting actions in domain descriptions, in: Proceedings of 22nd AAAI Conference on Artificial Intelligence (AAAI’07), 2007,pp. 409–414.[23] P. Everaere, S. Konieczny, P. Marquis, Conflict-based merging operators, in: Proceedings of the 11th International Conference on Principles of KnowledgeRepresentation and Reasoning (KR’08), 2008, pp. 348–357.J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823823[24] R. Fagin, J.D. Ullman, M.Y. Vardi, On the semantics of updates in databases, in: Proceedings of the 2nd ACM Symposium on Principles of DatabaseSystems (PODS’83), 1983, pp. 352–355.[25] M.L. Ginsberg, Counterfactuals, Artificial Intelligence 30 (1986) 35–79.[26] G. Gottlob, Complexity results for nonmonotonic logics, Journal of Logic and Computation 2 (1992) 397–425.[27] S. Konieczny, J. Lang, P. Marquis, Distance-based merging: A general framework and some complexity results, in: Proceedings of the 8th InternationalConference on Principles and Knowledge Representation and Reasoning (KR’02), 2002, pp. 97–108.[28] S. Konieczny, J. Lang, P. Marquis, Quantifying information and contradiction in propositional logic through test actions, in: Proceedings of the 18thInternational Joint Conference on Artificial Intelligence (IJCAI’03), 2003, pp. 106–111.[29] S. Konieczny, J. Lang, P. Marquis, DA2 merging operators, Artificial Intelligence 157 (2004) 45–79.[30] S. Konieczny, J. Lang, P. Marquis, Reasoning under inconsistency: The forgotten connective, in: Proceedings of the 19th International Joint Conferenceon Artificial Intelligence (IJCAI’05), 2005, pp. 484–489.[31] S. Konieczny, R. Pino Pérez, On the logic of merging, in: Proceedings of the 6th International Conference on Knowledge Representation and Reasoning(KR’98), 1998, pp. 488–498.[32] C. Lafage, J. Lang, Propositional distances and compact preference representation, European Journal of Operation Research 160 (2005) 741–761.[33] J. Lang, P. Liberatore, P. Marquis, Propositional independence – formula-variable independence and forgetting, Journal of Artificial Intelligence Re-search 18 (2003) 391–443.[34] J. Lang, P. Marquis, In search of the right extension, in: Proceedings of the 7th International Conference on Knowledge Representation and Reasoning(KR’00), 2000, pp. 625–636.[35] J. Lang, P. Marquis, Resolving inconsistencies by variable forgetting, in: Proceedings of the 8th International Conference on Principles and KnowledgeRepresentation and Reasoning (KR’02), 2002, pp. 239–250.[36] P. Liberatore, F.M. Donini, Verification programs for abduction, in: Proceedings of the 14th European Conference on Artificial Intelligence (ECAI’00),2000, pp. 166–170.[37] P. Liberatore, M. Schaerf, Arbitration (or how to merge knowledge bases), IEEE Transactions on Knowledge and Data Engineering 10 (1) (1998) 76–90.[38] P. Liberatore, M. Schaerf, Brels: A system for the integration of knowledge bases, in: Proceedings of the 7th International Conference on KnowledgeRepresentation and Reasoning (KR’00), 2000, pp. 145–152.[39] F. Lin, R. Reiter, Forget it!, in: Proceedings of the AAAI Fall Symposium on Relevance, 1994, pp. 154–159.[40] J. Lin, A.O. Mendelzon, Knowledge base merging by majority, in: Dynamic Worlds: From the Frame Problem to Knowledge Management, Kluwer, 1999.[41] B. Nebel, Syntax-based approaches to belief revision, in: Belief Revision, in: Cambridge Tracts in Theoretical Computer Science, vol. 29, CambridgeUniversity Press, 1992, pp. 52–88.[42] B. Nebel, How hard is it to revise a belief base?, in: Handbook of Defeasible Reasoning and Uncertainty Management Systems, vol. 3, Belief Revision,Kluwer Academic, 1998, pp. 77–145.[43] Ch.H. Papadimitriou, Computational Complexity, Addison–Wesley, 1994.[44] G. Pinkas, R.P. Loui, Reasoning from inconsistency: A taxonomy of principles for resolving conflict, in: Proceedings of the 3rd International Conferenceon Knowledge Representation and Reasoning (KR’92), 1992, pp. 709–719.[45] G. Priest, Minimally inconsistent LP, Studia Logica 50 (1991) 321–331.[46] N. Rescher, R. Manor, On inference from inconsistent premises, Theory and Decision 1 (1970) 179–219.[47] P.Z. Revesz, On the semantics of arbitration, International Journal of Algebra and Computation 7 (2) (1997) 133–160.[48] Y. Shoham, A semantical approach to nonmonotonic logics, in: Proceedings of the 2nd IEEE Symposium on Logic in Computer Science (LICS’87), 1987,pp. 275–279.[49] K. Su, G. Lv, Y. Zhang, Reasoning about knowledge by variable forgetting, in: Proceedings of the 9th International Conference on Principles and Knowl-edge Representation and Reasoning (KR’04), 2004, pp. 576–586.[50] K. Wang, A. Sattar, K. Su, A theory of forgetting in logic programming, in: Proceedings of the 20th National Conference on Artificial Intelligence andthe Seventeenth Innovative Applications of Artificial Intelligence Conference (AAAI’05), 2005, pp. 682–688.[51] Y. Zhang, N.Y. Foo, Solving logic program conflict through strong and weak forgettings, Artificial Intelligence 170 (8–9) (2006) 739–778.[52] Y. Zhang, N.Y. Foo, K. Wang, Solving logic program conflict through strong and weak forgettings, in: Proceedings of the 19th International JointConference on Artificial Intelligence (IJCAI’05), 2005, pp. 627–634.[53] Y. Zhang, Y. Zhou, Properties of knowledge forgetting, in: Proceedings of the 12th International Workshop on Non-Monotonic Reasoning (NMR’08),2008, pp. 68–75.