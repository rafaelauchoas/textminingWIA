3202nuJ21]YC.sc[2v13220.5032:viXraConnecting the Dots in Trustworthy Artificial Intelligence: From AIPrinciples, Ethics, and Key Requirements to Responsible AI Systemsand RegulationNatalia Díaz-Rodrígueza,∗, Javier Del Serb,c,∗, Mark Coeckelberghd, Marcos López de Pradoe,f,g,Enrique Herrera-Viedmaa and Francisco HerreraaaDepartment of Computer Science and Artificial Intelligence, DaSCI Andalusian Institute in Data Science and Computational Intelligence, University ofGranada, Granada 18071, SpainbTECNALIA, Basque Research and Technology Alliance (BRTA), 48160 Derio, SpaincDepartment of Communications Engineering, University of the Basque Country (UPV/EHU), 48013 Bilbao, SpaindDepartment of Philosophy, University of Vienna, Vienna, 1010, AustriaeSchool of Engineering, Cornell University, Ithaca, NY, 14850, United StatesfADIA Lab, Al Maryah Island, Abu Dhabi, United Arab EmiratesgDepartment of Mathematics, Khalifa University of Science and Technology, Abu Dhabi, United Arab EmiratesA R T I C L E I N F OA B S T R A C TKeywords:Trustworthy AIAI EthicsResponsible AI systemsAI RegulationRegulatory SandboxTrustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over threemain pillars that should be met throughout the system’s entire life cycle: it should be (1) lawful,(2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining trulytrustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actorsthat are part of the system’s life cycle, and considers previous aspects from different lenses. A moreholistic vision contemplates four essential axes: the global principles for ethical use and developmentof AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and thementioned pillars and requirements. The seven requirements (human agency and oversight; robustnessand safety; privacy and data governance; transparency; diversity, non-discrimination and fairness;societal and environmental wellbeing; and accountability) are analyzed from a triple perspective:What each requirement for trustworthy AI is, Why it is needed, and How each requirement can beimplemented in practice. On the other hand, a practical approach to implement trustworthy AI systemsallows defining the concept of responsibility of AI-based systems facing the law, through a givenauditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work,and a concept of utmost necessity that can be realized through auditing processes, subject to thechallenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AIculminates in a debate on the diverging views published lately about the future of AI. Our reflectionsin this matter conclude that regulation is a key for reaching a consensus among these views, and thattrustworthy and responsible AI systems will be crucial for the present and future of our society.1. IntroductionWe are witnessing an unprecedented upsurge of Artifi-cial Intelligence (AI) systems. Despite its important histori-cal development, in the last years AI has vigorously enteredall professional and social domains of applications, fromautomation to healthcare, education and beyond. Recently, afamily of generative AI (DALL-E1 [1], Imagen2 [2] or largelanguage model products such as ChatGPT3) have sparkeda significant amount of debates. These arise as a concern onwhat this could mean in all fields of application and whatimpact they could have.The views expressed in this article are the authors’, and are notnecessarily the views of the institutions they are affiliated with.∗These authors contributed equally. Corresponding authors: natalia-diaz@ugr.es (N. Díaz-Rodríguez), javier.delser@tecnalia.com (J. Del Ser).1DALL-E, https://openai.com/product/dall-e-2, accessed on April25th, 2023.2Google Imagen, https://imagen.research.google/, accessed on April25th, 2023.3Chat Generative Pre-Trained Transformer (Chat GPT), https://chat.openai.com/chat, accessed on April 25th, 2023.These debates, active for years now, pose questions re-garding the ethical aspects and requirements that AI systemsmust comply with. They emerge from the ethical principles’perspective, from the regulation ones, from what it means tohave fair AI, or from the technological point of view, on whatan ethical development and use of AI systems really mean.The notion of trustworthy AI has attracted particular interestacross the political institutions of the European Union (EU).The EU has intensively worked on elaborating this conceptthrough a set of guidelines based on ethical principles andrequirements for trustworthy AI [3].Trustworthy AI is a holistic and systemic approach thatacts as prerequisite for people and societies to develop,deploy and use AI systems [3]. It is composed of three pillarsand seven requirements: the legal, ethical, and technicalrobustness pillars; and the following requirements: humanagency and oversight; technical robustness and safety; pri-vacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmentalN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 1 of 30   Connecting the Dots in Trustworthy Artificial IntelligenceFigure 1: Our holistic approach to attain responsible AI systems from trustworthy AI breaks down trustworthy AI into 4 criticalaxes: assuring the principles for ethical development and use of AI, philosophically reflecting on AI ethics, complying with AIregulation, and fulfilling Trustworthy AI requirements.wellbeing; and accountability. Although the previous def-inition is based on requirements, there is a larger multidi-mensional vision. It considers the ethical debate per se, theethical principles and a risk-based approach to regulation,backed up by the EU AI Act [4].The goal of this paper is to become a primer for re-searchers and practitioners interested in a holistic vision oftrustworthy AI from 4 axes (Fig. 1): from ethical principlesand AI ethics, to legislation and technical requirements. Ac-cording to this vision, our analysis tackles the main aspectsof trustworthy AI in a non-exhaustive but technical fashion,by:• Providing a holistic vision of the multifaceted notion oftrustworthy AI that considers its diverse principles for eth-ical use and development of AI, seen from internationalagencies, governments and the industry.• Breaking down this multidimensional vision of trustwor-thy AI into 4 axes, to reveal the intricacies associated toits pillars, its technical and legal requirements, and whatresponsibility in this context really means.• Examining requirements for trustworthy AI, addressingwhat each requirement actually means, why it is necessaryand proposed, and how it is being addressed technologi-cally. While this paper is not intended to be an exhaustivereview, we will delve into an overview of technical possi-bilities to address the aforementioned seven key require-ments for trustworthy AI.• Analyzing AI regulation from a pragmatic perspective tounderstand the essentials of the most advanced legal pieceexisting so far, the European Commission perspective,and to fully grasp its practical applicability.• Defining responsible AI systems as the result of connect-ing the many-sided aspects of trustworthy AI above. Thisis the notion we advocate for, in order to truly attain trust-worthy AI. Their design should be guided by regulatorysandboxes.• Dissecting currently hot debates on the status of AI,the moratorium letter to pause giant AI experiments, thecurrent movements around an international regulation andour positioning based on the previous analyses.By bridging the gap from theory (AI Principles, Ethics,and Key Requirements) to practice (Responsible AI Systemsand Regulation), our holistic view offered in this work aimsto ultimately highlight the importance of all these elementsin the development and integration of human-centered AI-based systems into the everyday life of humans, in a naturaland sustainable way.The paper is organized as follows: Section 2 revises themost widely recognized AI principles for the ethical useand development of AI (axis 1). Section 3 considers axis2: a philosophical approach to AI ethics. Section 4 (axis 3)presents the current risk-based viewpoint to AI regulation.Section 5 analyzes axis 4, i.e., key requirements to imple-ment trustworthy AI from a technical point of view. Section 6discusses the practical applicability of trustworthy AI by firstclosing the loop through the necessary definition of respon-sible AI systems, and then exposing the requirements forhigh-risk AI systems to comply with the law. It also proposesthe use of regulatory sandboxes as a challenge to test theN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 2 of 30From Trustworthy AI to Responsible AI SystemsPrinciples for ethical use and development of AI (Section 2)Artificial Intelligence regulation: A risk-based approach(Section 4)A philosophical approach to AI ethics(Section 3)Pillars and Requirements of Trustworthy AI(Section 5)Human agency & oversightTechnical robustness & safetyPrivacy & data governanceTransparencyDiversity, non-discrimination & fairnessSocietal & environmental wellbeingAccountabilityRobustnessLawfulnessEthicsConnecting the Dots in Trustworthy Artificial Intelligenceformer requirements via auditing, and a practical blueprintas a case study for AI healthcare. We end this section byelaborating on the needs of emerging AI systems (includinggeneral purpose AI and neuroscience technology), which de-mand evolved evaluation protocols and dynamic regulation.Section 7 follows by dissecting currently hot debates on thestatus of AI, from the AI moratorium letter to regulationas the key for consensus, including a reflection on the gapto be closed between regulation and technological progress.Finally, Section 8 draws concluding remarks, and highlightsthe aforementioned convergence between AI technology andregulation as the beacon for research efforts that safely bringthe benefits of this technology to humanity.2. Principles for ethical use and developmentof Artificial IntelligenceA large set of declarations and guidelines for the ethicaluse and development of AI has bloomed. These declarationslead to different similar approaches for introducing sets ofprinciples as a departure point for discussing about theresponsible development of AI.In this section we will analyze three different principledeclarations. We will start in Section 2.1 with the generalUNESCO Recommendation on the Ethics of Artificial In-telligence, and continue in Section 2.2 taking a practicalperspective from the industry point of view through the prin-ciples of Responsible AI by Design in Practice by Telefónica,a global telecommunication and media company with largepresence in the Spanish and Portuguese markets. Finally,in Section 2.3 we analyze the ethical principles based onfundamental rights associated to the European approach.2.1. UNESCO recommendation on the ethics ofArtificial IntelligenceIn November 2021, UNESCO proposed in Paris a Rec-ommendation on the Ethics of AI. Recognizing that AI canbe of great service to humanity and all countries can benefitfrom it, but also can raise fundamental ethical concerns (candeepen existing divides and inequities in the world), andaccounting for the Universal Declaration of Human Rights(1948) and the rest of relevant international recommenda-tions and declarations, the UNESCO Recommendation onthe Ethics of Artificial Intelligence, [5] are the following:1. Proportionality and do no harm: AI methods shouldnot infringe upon the foundational values in these rec-ommendations, should be based on rigorous scientificfoundations, and final human determination should apply.2. Safety and security: Unwanted harms such as safety risks,and vulnerabilities to attacks (security risks) should beavoided and addressed throughout the life cycle of AIsystems.3. Fairness and non-discrimination: AI actors should pro-mote social justice and safeguard fairness. Member Statesshould tackle digital divides ensuring inclusive accessand equity, and participation in the development of AI.4. Sustainability: The continuous assessment of the human,social, cultural, economic and environmental impact ofAI technology should be carried out with “full cog-nizance of the implications of AI for sustainability” asa set of constantly evolving goals.5. Right to Privacy, and Data Protection: Privacy must berespected, protected and promoted throughout the AI lifecycle.6. Human oversight and determination: Member Statesshould ensure that it is always possible to attribute ethicaland legal responsibility for any stage of the life cycle ofAI systems, as well as in cases of remedy related to AIsystems, to physical persons or existing legal entities.7. Transparency and explainability: Transparency is nec-essary for relevant liability regimes to work effectively.AI actors should commit to ensuring that the algorithmsdeveloped are explainable, especially in cases that im-pact the end user in a way that is not temporary, easilyreversible or otherwise low risk.8. Responsibility and accountability: “The ethical responsi-bility and liability for the decisions and auctions basedin any way on an AI system should always ultimately beattributable to AI actors”9. Awareness and literacy: Public awareness and under-standing of AI technologies and the value of data shouldbe promoted through open and accessible education, civicengagement, digital skills and AI ethics training. Allsociety should be able to take informed decisions abouttheir use of AI systems and be protected from undueinfluence.10. Multi-stakeholder and adaptive governance and collabo-ration: “Participation of different stakeholders through-out the AI system life cycle is necessary for inclusiveapproaches to AI governance, enabling the benefit to beshared by all, and to contribute to sustainable develop-ment”.The proposed principles are accompanied by values topromote, e.g., human rights and fundamental freedoms. Val-ues and principles are designed to be respected by all actorsinvolved in the AI system life cycle, being amenable ofchange through amendments to existing and new legislationand business guidelines, since they must comply with inter-national law, the United Nations Charter and Member States.2.2. Telefónica’s principles of Responsible AI byDesign in PracticeEnterprises also need to cope with and adapt to newdemands of AI products and associated risks. The previousrecommendations are also aligned with the more genericprinciples for AI defined by the Berkman Klein Centerfor Internet & Society at Harvard University that startedbeing developed since 2016: Principled AI maps ethical andN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 3 of 30Connecting the Dots in Trustworthy Artificial Intelligencerights-based approaches to principles for AI to address is-sues related to the potential threats of AI to both individualsand society as a whole. Derived from these, in industry, e.g.,Telefónica defines the so-called 5 principles of ResponsibleAI by Design in Practice [6] as:1. Fair AI: the output of AI systems must not lead todiscrimination.2. Transparent and explainable AI: people should knowwhether they are communicating with a person or an AI-based system.3. Human-centered AI (AI for Social Good, Human-centeredAI [7]): AI products and services must be aligned with theUN Sustainable Development Goals.4. Privacy and security by design: standards should be con-sidered during all life cycles, also from the ResponsibleResearch and Innovation Guidelines [8].5. Extend them to any third party.The adoption of these and similar principles is part ofnew awareness strategies being carried out in companies,sometimes known as change management. Telefónica’s ap-proach is only one example of such adoption. This impliesa change in organizations culture to take into account andimplement these principles on a day-to-day basis.2.3. Ethical principles based on fundamentalrightsIn Europe, the foundations of trustworthy AI adhereto the four ethical principles proposed by the EuropeanCommission’s High-Level Expert Group (HLEG) [3]. Theseare based on fundamental rights, to which AI practitionersshould always strive to adhere, in order to ensure the devel-opment, deployment and use of AI systems in a trustworthyway. Trustworthy AI is grounded in fundamental rights andreflected by the European Commission’s Ethical Principles:1. Respect for human autonomy: Ensure freedom and au-tonomy of humans interacting with AI systems implieshumans should keep full and effective self-determinationover themselves and the ability to take part on democraticprocesses; AI systems should not "unjustifiably subor-dinate, coerce, deceive, manipulate, condition or herdhumans, but rather, argument, complement and empowerhuman cognitive, social and cultural skills, leave oppor-tunity for human choice and securing human oversightover work processes" in AI systems, e.g., support humansin the work environment and support the creation ofmeaningful work.2. Prevention of harm4: AI systems should not “cause norexacerbate harm or adversely affect humans”. AI sys-tems should “protect human dignity, mental and physicalintegrity, be technically robust and assure they are not4Harm can be individual or collective, can include intangible harm tosocial, cultural, political or natural environments and all living beings.open to malicious use”. For instance, they should besupervised so they do not exacerbate adverse impacts dueto information asymmetries or unequal balance of power.3. Fairness: Fairness is closely related to the rights to Non-discrimination, Solidarity and Justice. Although there aremany different interpretations of fairness, the EuropeanCommission advocates for having both: a) a substantivedimension of fairness that "commits to ensure equal andjust distribution of benefits and costs, commits to freefrom unfair bias, discrimination and stigmatization, im-plies respecting the principle of proportionality betweenmeans and ends and a careful balancing of competinginterests and objectives" [3]. b) a procedural dimensionallowing to "contest and seek redress against decisionstaken by AI systems or who operates them". To achievethis, the entity responsible for the decision must be iden-tifiable, while decision making processes should be ex-plainable.4. Explainability: Being crucial for building and maintain-ing users trust in the AI system, explainability meansthat processes need to be transparent, the capabilitiesand purpose of AI systems openly communicated, anddecision -to the extent possible- explainable to thosedirectly and indirectly affected. When a decision cannotbe duly contested (e.g., because explaining a particularmodel output or decision and what combination of inputfactors contributed to it is not always possible), then otherexplainability measures may be required (traceability,auditability and transparent communication on the capa-bilities of the AI system). This will depend on the contextand severity of consequences if an output is erroneous.These ethical principles are placed in the context of AIsystems. They act as ethical imperatives, and advocate forAI systems to strive to improve individual and collectivewellbeing.As we can see, the mobilization has been worldwide:from the Montréal Declaration for a responsible develop-ment of AI – an initiative of University of Montréal–, to theEthics of AI recommendations led by international organ-isations such as UNESCO, passing by the adoption led byindustry. All sets of principles share terminology, commongrounds on human rights, and agree on the relevance ofpreserving human decisions and responsibilities, which arethe most prominent features of ethics of AI.3. A philosophical approach to ArtificialIntelligence ethicsEthics is an academic discipline which is a subfield ofphilosophy and generally deals with questions such as “Whatis a good action?”, “What is the value of a human life?”,“What is justice?”, or “What is the good life?” [3].Aligned with the European Commission ethics guide-lines [3], our ethical vision of AI consists of five main actions[9]. These can help smooth the way to attain ethical AI.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 4 of 30Connecting the Dots in Trustworthy Artificial IntelligenceNext, we develop these, taking a philosophical approach toAI ethics:1. Use philosophy and science to examine and criticallydiscuss assumptions around the role that AI and humansplay in these scenarios and discussions. For example, onecould critically discuss claims that are made about thepossibility of Artificial General Intelligence or human-level AI. Large language models, for instance, may givethe impression that they have a human-like level of intel-ligence, but work very differently than the human brainand make many mistakes that humans would not make.This also leads to the question regarding the differencesbetween humans and machine, and is also linked to thequestion concerning the moral status of AI. For example,it has been claimed that a chatbot was sentient, while itdid not meet the criteria for sentience. That being said, itis not always clear what these criteria are. AI makes usre-visit philosophical questions concerning moral status.2. Observe attentively the nature of AI and which functionsit is assigned to perform today within its diversity ofapplications. We should recognize the pervasiveness ofAI. One reason why it is important to ask ethical ques-tions about AI is that it is pervasive: it is used in manyapplications such as search, text generation, recommen-dations for commercial products, and so on. In the ethicalanalysis, we need to pay attention to the details of eachapplication3. Discuss the most concrete and pressing ethical and socialproblems that AI presents in terms of how it is beingapplied today. AI raises a number of ethical questionssuch as privacy and data protection, safety, responsibility,and explainability. For example, a chatbot can encour-age someone to take their life. Does this mean that theapplication is unsafe? How can we deal with this risk?And if something happens, who is responsible? Typically,there are many people involved in technological action.It is also important to be answerable to those who areaffected by the technology [10], for example in the caseof a suicide5 the company may need to be answerable tothe family of the victim. Furthermore, it is important thatwhen AI offers recommendations for decisions, it is clearon what basis these recommendations and decisions aretaken. One problem is that this is usually not clear in thecase of deep learning. In addition, there are societal im-plications such as potential unemployment caused by theautomation that is enabled by AI, and the environmentalcosts of AI and its infrastructures through energy use andcarbon emissions linked to the use of the algorithms, thestorage of data, and the production of hardware.4. Investigate AI policies for the near future. There arenow already many policy documents on AI, for examplethe Ethics Guidelines for Trustworthy AI produced bythe European Commission’s High-Level Expert Group5https://coeckelbergh.medium.com/chatbots-can-kill-d82fde5cf6caon AI [3] and the Recommendation on the Ethics ofArtificial Intelligence [5]. These documents need to becritically investigated. For example, in the beginning, lessattention was given to environmental consequences of AI.A more general problem is that principles and lists ofethical considerations are not sufficient; there is still agap between those principles and implementation in thetechnology, in standards, and in legal regulation.5. Ask ourselves whether the attention that concentratesthe public discourse in AI is useful as we face otherproblems, and whether AI should be our unique focus ofattention. Given that we also face other global problemssuch as climate change and poverty, it is important toask the question regarding prioritization: Is AI the mostimportant problem we face? And if not - if, for instance,we insist on climate change being the main and mosturgent global problem - how does AI impact and perhapscontribute to this problem, and how can it perhaps help tosolve it? Reflection on these challenges will be importantin the coming years.Once expressed the ethics of AI from the philosophicalperspective, the next section will analyze the regulation ofAI.4. Artificial Intelligence regulation: Arisk-based approachIn the currently hot debate of AI, a fundamental aspectis regulating AI for it to be righteous. The most advancedregulation to date is the European Commission’s AI Actproposal6 for the regulation of AI [4].In this section we are presenting AI regulation from twoangles; first in Section 4.1 from the perspective of risk of AIsystems and then, in Section 4.2, we make a deeper analysisinto high-risk AI systems.4.1. A risk-based approach to regulate the use ofArtificial Intelligence systemsThe AI Act draft proposal for a Regulation of the Euro-pean Parliament and of the Council laying down harmonizedrules on AI [4] is the first attempt to enact a horizontalAI regulation. The proposed legal framework focuses onthe specific use of AI systems. The European Commissionproposes to establish a technology-neutral definition of AIsystems in EU legislation and defines a classification for AIsystems with different requirements and obligations tailoredto a “risk-based approach”, where the obligations for an AIsystem are proportionate to the level of risk that it poses.The rules of the AI Act specifically consider the riskscreated by AI applications by proposing a list of high-riskapplications, setting clear requirements for AI systems forhigh-risk applications, defining specific obligations for AI6On April 27th, 2023, the Members of European Parliament (MEPs)reached a political agreement on the AI Act, https://www.euractiv.com/section/artificial-intelligence/news/meps-seal-the-deal-on-artificial-intelligence-act/, accessed on May 1st, 2023.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 5 of 30Connecting the Dots in Trustworthy Artificial IntelligenceFigure 2: AI Act criticality pyramid and risk-based approach regulatory system for the use of algorithmic systems; SS stands forsubsequent articles (figure extended from the EU Portal8 and inspired from [11] and [12]).users and providers of high risk applications, proposinga conformity assessment before the AI system is put intoservice or placed on the market, proposing enforcementafter it is placed in the market, and proposing a governancestructure at European and national levels.The four levels of risk [4] outlined by the AI Act are thefollowing (Figure 2):• Minimal or No risk: The vast majority of AI systemscurrently used in the EU fall into this category. Theproposal allows the free use of minimal-risk AI. Volun-tarily, systems providers of those systems may choose toapply the requirements for trustworthy AI and adhere tovoluntary codes of conduct (Art. 69 - Codes of Conduct)7.When a compliant AI systems presents a risk, the relevantoperator will be required to take measures to ensure thesystem no longer presents that risk, withdraw the systemfrom market, or recall the risk for a reasonable periodcommensurate with the nature of the risk (Art. 67 -Compliant AI systems which present a risk). For instance:AI-enabled video games or spam filters.• Limited risk: Systems such that users should be awarethat they are interacting with a machine so they can takean informed decision to continue or step back. Thesehave to comply with specific information/transparencyobligations; for instance, chatbots, and systems generatingdeepfakes or synthetic content.7Codes of conduct are encouraged by the Commission and the MemberStates to foster the voluntary application to AI systems other than high-riskAI systems (HRAIs) “on the basis of technical specification and solutionsthat are appropriate means of ensuring compliance with such requirementsin light of the intended purpose of the systems” (Art. 69).8Regulatory framework proposal on Artificial Intelligence, h t t p s ://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai,accessed on April 25th, 2023.• High-risk AI systems (HRAIs): Systems that can have asignificant impact on the life chances of a user (Art. 6);they create an adverse impact on people’s safety or theirfundamental rights9. Eight types of systems fall into thiscategory; these are subject to stringent obligations andmust undergo conformity assessments before being put onthe European market, e.g. systems for law enforcement oraccess to education. They will always be high-risk whensubject to third-party conformity assessment under thatsectorial legislation.• Unacceptable risk: AI systems considered a clear threatto the safety, livelihoods and rights of people will be pro-hibited in the EU market (Title II — Prohibited ArtificialIntelligence Practices, Art. 5). For example: Social scor-ing, facial recognition, dark-patterns and manipulative AIsystems, e.g., voice assistance systems that encouragedangerous behavior, or real time remote biometric iden-tification systems in public spaces for law enforcement.As we can see, very differently to the Chinese, government-centric, approach, or the US industry-owned-data approachto AI, the EU is taking a human-centric approach to regulatethe use of AI. This risk scenario-based approach regulatesusages rather than models and technology themselves. Thisis the position we defend.Since the beginning of 2023, the European Parliamenthas been considering amendments to the law proposing howto conduct fundamental rights impact assessments and otherobligations for users of HRAIs. Issues still to be finalizedinclude closing the list of HRAI scenarios above exposed,9As protected by the EU Charter of Fundamental Rights (source: https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683).N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 6 of 30Level1: ApplicationswithunacceptableriskLevel2: ApplicationswithhighriskLevel3: ApplicationswithlimitedriskLevel4: ApplicationswithoutriskorwithminimalriskVideogamesPurchaserecommendationIndustrial prognosisChatbotsDeepfakesEmotionrecognitionAutonomousVehicleSafetyLawenforcementSocial & educationapprovalproceduresMedical diagnosisFacial recognitionSocial scoringSubliminal/obscure manipulationConnecting the Dots in Trustworthy Artificial Intelligenceprohibited practices, and details concerning the use of copy-righted content to train AI systems10 and the regulation ofgeneral purpose AI systems (GPAIS)11. The first consider-ation indicates the EU Parliament will force AI operatorsto reveal whether they use protected content. The secondrequest emerges from the Privacy and Access Council ofCanada, who agrees that GPAIS carry serious risks andharmful unintended consequences, and must not be exemptunder the EU AI Act, or equivalent legislation elsewhere.A recent definition of GPAIS can be found in [13]: “An AIsystem that can accomplish a range of distinct valuable tasks,including some for which it was not specifically trained”.It has also been referred to as foundation model [14, JRCGlossary, pag. 32]), but really a GPAIS refers to a modelof different nature, beyond the generative AI or foundationmodels that can be considered as specific cases of GPAIsystems. A point of agreement among all definitions to dateis the capability of a GPAIS to accomplish tasks beyondthose for which it was originally trained. This is one of themain reasons why GPAIS have become a pivotal topic ofdebate in what refers to AI regulation. Section 6.5 will delvefurther into this.4.2. High-risk Artificial Intelligence systemsThe European AI Act is predicted to become the globalstandard for AI regulation12 by unifying within a singleframework the concept of risk acceptability and the trust-worthiness of AI systems by their users [15]. The risk-basedapproach of the AI Act specifically categorizes as HRAIsthe following eight kind of AI systems (AI Act, Annex III -High-risk AI systems referred to in Art. 6(2) [4]):1. Surveillance systems (e.g., biometric identification andfacial recognition systems for law enforcement)2. Systems intended for use as security components in themanagement and operation of critical digital infrastruc-tures (road traffic and water, gas, heat and electricitysupply).3. Systems to determine access, admission or assignmentof people to educational institutions or programs or toevaluate people (for the purpose of evaluating learningoutcomes, learning processes or educational programs).4. Systems intended to be used for recruitment or selectionof personnel, screening or filtering of applications andevaluation of candidates, or systems for making decisions10Financial Times, European parliament prepares tough measures overuse of AI, https://www.ft.com/content/addb5a77-9ad0-4fea-8ffb-8e2ae250a95a?shareType=nongift, accessed on April 25th, 2023.11Privacy and Access Council of Canada, Five considerations to guidethe regulation of “General Purpose AI”, https://pacc-ccap.ca/five-considerations-to-guide-the-regulation-of-general-purpose-ai/, accessed onApril 25th, 2023.12The EU AI Act’s Risk-Based Approach: High-Risk Systems and WhatThey Mean for Users, https://futurium.ec.europa.eu/en/european-ai-alliance/document/eu-ai-acts-risk-based-approach-high-risk-systems-and-what-they-mean-users, accessed on April 25th, 2023.on promotion and termination of contractual relation-ships, assignment of tasks based on individual behaviorand the evaluation of performance and behavior.5. Systems for assessing the eligibility for public benefitsor assistance, assessing creditworthiness or establish-ing credit scores. Systems for dispatching or prioritizingemergency first response services (firefighters, medicalfirst aid, etc.).6. Systems to assess the risk of a person committing crimeor recidivism, or the risk that he or she is a potentialoffender.• Systems intended for use as polygraphs or to detectemotional state, or to assess the reliability of evidencein the course of an investigation or prosecution ofcrime.• Systems for predicting the occurrence or re-occurrenceof crimes based on profiles of people or assessment ofpersonality traits and characteristics or past criminalbehavior.• Systems for profiling individuals by law enforcementauthorities in the course of detecting, investigating orprosecuting crimes.7. Systems intended for use by competent public authorities(such as polygraphs or to detect the emotional state ofindividuals):• Risk assessment systems, including security risks, ir-regular immigration or health risk posed by a personseeking to enter a member state.• Systems for the examination of applications for asy-lum, visas and residence permits and claims associatedwith the eligibility of people applying for status.8. Systems intended for the administration of justice anddemocratic processes (intended to act on behalf of theauthorities in the administration of justice for the inter-pretation of acts or law and the application of the lawto a particular set of facts, or evaluation of reliability ofevidence).One fact worth noting in the AI Act is its special em-phasis on the importance of taking into account, whenclassifying AI systems, the result of the AI system in relationwith the decision or action taken by a human, as well as theimmediacy of its effect (AI Act Intro, (32) [4]).5. Trustworthy Artificial Intelligence: Pillarsand RequirementsIn a technical sense, trustworthiness is the confidence ofwhether a system/model will act as intended when facing agiven problem [16]. This confidence generates trust in theuser of the model (the audience), which can be supportedfrom multiple perspectives. For instance, trust can be fos-tered when a system provides detailed explanations of itsN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 7 of 30Connecting the Dots in Trustworthy Artificial Intelligencedecisions [17]. As Lipton puts it, a person can be moreconfident when using a model if he/she understands how itworks and how it produces its decisions [18]. Likewise, trustcan be bolstered if the user is offered guarantees that themodel can operate robustly under different circumstances,that it respects privacy, or that it does not get affected bybiases present in the data from which it learns.Trustworthiness is, therefore, a multifaceted requisite forpeople and societies to develop, deploy and use AI systems,and a sine qua non condition for the realization of thepotentially vast social and economic benefits AI can bring[3]. Moreover, trustworthy does not concern only the systemitself, but also other actors and processes that take their partduring the AI life cycle. This requires a holistic and systemicanalysis of the pillars and requirements that contribute to thegeneration of trust in the user of an AI-based system.This section addresses this need by first dissecting thethree pillars that set the basis for trustworthy AI – namely,lawfulness, ethics and robustness (Subsection 5.1) – fol-lowed by a thorough analysis of the seven requirementsproposed by the European Commission’s High-Level Ex-pert Group (HLEG): human agency and oversight (Sub-section 5.2); technical robustness and safety (Subsection5.3); privacy and data governance (Subsection 5.4); Trans-parency (Subsection 5.5); diversity, non-discrimination andfairness (Subsection 5.6); societal and environmental well-being (Subsection 5.7); and accountability (Subsection 5.8).Definitions (what does the requirement stand for?), motiva-tions (why is the requirement relevant for trustworthiness?)and a short glimpse at methodologies (how can the require-ment be met in AI-based systems?) will be given for each ofthese requirements in their respective sections.5.1. The three pillars of trustworthy ArtificialIntelligenceIn general, a pillar can be understood as a fundamentaltruth of a given idea or concept, from which key require-ments to realize the idea can be formulated. Similarly toconstruction engineering, pillars are essential for buildingup the concept of trustworthy AI: each pillar is necessarybut not sufficient on its own to achieve trustworthy AI. Keyrequirements can contribute to one or several pillars, justlike construction elements such as concrete, formwork orcantilevers are used to help pillars support the structureof the building. These requirements must be continuouslyensured throughout the entire life cycle of AI systems,through methodologies that must not only be technical, butalso involve human interaction.According to the EU Ethical Guidelines for TrustworthyAI [3], pillars of trustworthy AI systems are defined as threebasic properties that such systems should possess:• Pillar 1: Lawful. Trustworthy AI systems should complywith applicable laws and regulations, both horizontally(i.e. the European General Data Protection Regulation)and vertically (namely, domain-specific rules that areimposed in certain high-risk application domains, such asmedical or finance).• Pillar 2: Ethical. Besides their compliance with the law,trustworthy AI systems should also adhere to ethical prin-ciples and values. The rapid technological developmentof current AI-based system rises ethical questions thatare not always addressed synchronously by regulatoryefforts. The democratized usage of large language modelsand misinformation using deepfakes are two avant-gardeexponents of the relevance of Ethics as one of the pillarsof trustworthy AI.• Pillar 3: Robust. Trustworthy AI systems should guaranteethat they will not cause any unintentional harm, workingin a safe and reliable fashion from both technical (perfor-mance, confidence) and social (usage, context) perspec-tives.Trustworthy AI stands on these three pillars. Ideally, theyshould act in harmony and pushing in synergistic directionstowards the realization of trustworthy AI. However, tensionsmay arise between them: for instance, what is legal is notalways ethical. Conversely, ethical issues may require theimposition of law amendments that become in conflict withprevalent regulations. Trustworthy AI must guarantee ethi-cal principles and values, obey the laws, and operate robustlyso as to attain its expected impact on the socioeconomicenvironment in which it is applied.The above three pillars lie at the heart of the HLEGguidelines [3], which establish the seven requirements fortrustworthy AI. As depicted in Figure 3, each requirementspans several components or dimensions in which the re-quirement becomes of special relevance for the design andoperation of an AI-based system. Apart from recommendingtechnical and non technical methods, the guidelines alsoinclude an Assessment List for Trustworthy Artificial Intel-ligence, ALTAI, for self-assessment of AI systems and formaking the 7 requirements operative [19].The next subsections elaborate in detail on these require-ments:5.2. Requirement 1: Human agency and oversightWHAT does it mean? AI systems should empower humanbeings, allowing them to make informed decisions and fos-tering their fundamental rights. At the same time, properoversight mechanisms need to be ensured, which can beachieved through human-in-the-loop, human-on-the-loop,and human-in-command approaches. In other words, AI-based systems must support human autonomy and decisionmaking.WHY is it important for trustworthiness? This require-ment is necessary for autonomy and control. The unfairmanipulation, deception, herding and conditioning of AI-based systems can be a threat to the individual autonomy,rights and freedom of their users. Therefore, trustworthy AIsystems should provide the means for the user to supervise,evaluate and freely adopt/override a decision made by suchsystems, avoiding decisions that are automatically madewithout humans being involved in the process.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 8 of 30Connecting the Dots in Trustworthy Artificial IntelligenceFigure 3: Pillars and requirements of Trustworthy AI [3].HOW can this requirement be met in practice? Twodimensions underlie this first requirement, namely, humanagency (Subsection 5.2.1) and human oversight (Subsection5.2.2). We now analyze different methodological approachesthat can be adopted in these two dimensions:• Human-in-command (HIC), namely, the capability of thesupervisor to oversee the overall activity of the AI systemincluding its broader economic, societal, legal and ethicalimpacts, and ensuring that decisions produced by the AIsystem can be overridden by the human.5.2.1. Human agencyMechanisms for human oversight will depend on thearea of application and potential risk. For the preservationof human rights, human-compatible [20], human-centricapproaches [21, 22, 14], AI for social good [23, 7] andhuman computation or interactive machine learning [24]are computing paradigms aligned with this requirement.However, more structured toolkits (along the lines of [25]or C-Suite [26]) will need to be materialized for a smoothdomain-specific consideration of this requirement. In termsof technical tools to reach different audiences, languageappears as the universal means of communication amonghumans and machines, and thus, AI models using naturallanguage processing and/or counterfactual and natural lan-guage explanations [27] will be relevant to help humanssupervise and take the most appropriate decision based onthe output of AI systems.5.2.2. Human oversightDifferent degrees of human involvement in the supervi-sion of AI-based systems can be specified [3]:• Human-in-the-loop (HITL), which refers to the ability ofthe supervisor to intervene in every decision cycle of thesystem being monitored [24].• Human-on-the-loop (HOTL), which stands for humanintervention during the design and monitoring cycles ofthe AI-based system.Depending on the application under consideration, mech-anisms supporting one of the above levels of human over-sight can be designed. Methods proposed so far are largelydomain-specific, since user-algorithm interfaces vary de-pending on the capabilities and background of the supervisorand the design of the AI-based solution.5.3. Requirement 2: Technical robustness andsafetyWHAT does it mean? This second requirement includesseveral functionalities all aligned with the prevention of un-intentional harm and the minimization of the consequencesof intentional harm. These include the resilience of AI-basedsystems (to attacks and security), ensuring fallback plans(in case something goes wrong), general safety, and beingaccurate, reliable and reproducible. Robustness and safetyrefer to the need of AI systems to be secure, reliable androbust enough to errors and/or inconsistencies in all phasesof the life cycle [28].WHY is it important for trustworthiness? AI-based sys-tems deployed on real-world scenarios can undergo changesin their operating environment that can induce changes attheir inputs (e.g. concept drift). Likewise, such changes canbe the result of the interaction of malicious users with theAI-based system in an adversarial fashion. Disregardingwhether such changes are intentional or not, the trustworthi-ness of the AI-based system is subject to the capability of themodel to mitigate the impact of these changes in their issuedN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 9 of 30EthicsTrustworthy Artificial IntelligenceLawfulnessRobustnessAccountabilitySocietal and environmental wellbeingDiversity, non-discrimina-tion & fairnessTransparencyPrivacy and data governanceTechnical robustness and safetyHuman agency and oversightRequirement 5Requirement 4Requirement 3Requirement 2Requirement 1Requirement 7Requirement 6Subsection5.8Subsection5.7Subsection5.6Subsection5.5Subsection5.4Subsection5.3Subsection5.2Connecting the Dots in Trustworthy Artificial Intelligencepredictions. Likewise, in risk-critical applications trustwor-thy AI systems should evaluate relevant safety measuresand endowed with functionalities to fall back when the AI-based system deviates from its expected behavior as per themonitored measures. Finally, reliability and reproducibilityconnects tightly with trustworthiness in what refers to theverification of the expected operation and performance ofAI-based systems. When AI-based systems are to be usedin different contexts and deployed in different systems, suchcomponents are vital to ensure that the system at handresiliently accommodates the differences and particularitiesthat may arise in each context/system, ultimately working asexpected.HOW can this requirement be met in practice? Method-ologies that can be explored to support this requirementcan be analyzed over three dimensions: technical robustness(Subsection 5.3.1), safety (Subsection 5.3.2) and repro-ducibility (Subsection 5.3.3).5.3.1. Technical robustnessWhen dealing with an AI-based system, robustness andreliability are properties that refer to the ability of the sys-tem to have comparable performance on atypical data withrespect to typical operational regimes [29]. Robustness canbe established in the face of different circumstances: whenwe hope a model to be robust, it is due to the fact thatthe model may degrade, be perturbed or affected during itsfuture usage. It is desirable to have a model that is robust interms of its generalization or generative capabilities, againstadversarial attacks or models, or against data perturbations.Systematically, several levels of robustness can be dis-tinguished in AI-based systems [30]:• Level 0 (no robustness or standard training): this firstlevel of robustness refers to the one provided by theAI-based system by itself, without any risk mitigationfunctionalities or additions added to its design. This levelconcerns generalization capabilities such as being robustto distributional changes caused by spurious features ordata instances. Despite the lack of specific risk mitigationmeasures, some naive information provided by certainnaive AI models (e.g. quantification of epistemic con-fidence) can be exploited to detect when the AI-basedsystem is not working in its expected operational regime.• Level 1 (generalization under distribution shifts): thissecond level of robustness considers techniques aimed tomitigate different types of changes in data. Data changesor shifts include covariate shift, prior probability shift,concept drift and confounding shift, depending on thechange happening in the distribution of the input vari-ables, the output of the model, the statistical relationshipbetween the inputs and outputs, or the change of a variablethat influences both inputs and outputs, respectively [31].In this level we can also place the generalized frame-work of Out-of-Distribution (OoD) detection [32], whichrefers to anomaly detection, novelty detection and openset recognition, the latter referring to the capability ofthe model to detect, characterize and incorporate newunknown patterns to its knowledge base (e.g. new classesin a classification problem). Level 1 of robustness againstthese data shifts can be approached by concept drift detec-tion and adaptation techniques, OoD detection methods orclass-incremental learning schemes, to mention a few.• Level 2 (robustness against a single risk): this third worst-case robustness tackles a single point of risk, e.g., thepresence of adversarial examples. Assessing this levelrequires model inspection and intervention (e.g., activemodel scanning, probing to find failure cases, adversarialdefenses against different attack modes).• Level 3 (robustness against multiple risks): It extends theformer to multiple risks (e.g., common data corruptions,spurious correlations).• Level 4 (universal robustness): this level is reached by AI-based systems that are proven to be effectively robust toall known risks.• Level 5 (human-aligned and augmented robustness): itfurthers complements level 4 by aligning human-centeredvalues and user feedback, automatically augmenting exist-ing robustness demands as per the requirements, contextand usage of the AI-based system. This level should betargeted by high-risk AI-powered applications.The robustness of the AI-system system should be acore part of the risk management strategy adopted by theowner of the system itself, hence becoming a core partof their accountability procedures. Indeed, AI maintenanceframeworks should ease achieving robustness and AI statustracking and control through the AI life cycle [33]. Mon-itoring can be produced either passively (by periodicallymeasuring different quantitative metrics related to robust-ness over the data, model, or both) or actively (emulatingthe circumstances under which the robustness of the modelcan be thought to be compromised (e.g. emulated adversarialattack instances or perturbations of known samples). Inboth cases, AI maintenance frameworks can detect modeldegradation through time by detecting systematic deviationsof the aforementioned metrics in data and models [34].Interestingly, areas currently under study in AI research aimin this direction, endowing AI-based systems with the abilityto learn continually from infinite streams of varying data[35], to quantify and communicate their confidence in theiroutputs [36], or to characterize and consolidate new patternsarising from data over time [37].We end the discussion about how technical robustnesscan be supported in AI-based systems by highlighting thepotential that techniques used to address other requirementscan bring to technical robustness. For instance, explainabil-ity techniques can help make models more robust, since theycan show which features are more stable to out of distributionchanges in the input or adversarial attacks. Likewise, theintensity of changes needed to reach a target adversarial con-fidence score in counterfactual generation can be a reliableN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 10 of 30Connecting the Dots in Trustworthy Artificial Intelligenceestimator of the extent to which a certain data instance canbe considered to be out of distribution [38]. All in all, theseexamples are a few among the multiple cases in which afunctionality added to an AI-based system can simultane-ously contribute to several requirements for trustworthiness.5.3.2. SafetyEvolving from a generic Information Technologies con-text, safety in AI [39, 40, 41] is developing in relation to thealignment with human values. In this sense, concrete proto-cols and procedures are challenging to define, but necessaryfor AI safety. Safety in AI concerns several unsolved researchissues [40], including:• Attaining robustness as the objective of withstanding haz-ards, and building systems less vulnerable to adversarialthreats such as adversarial perturbations which cause highconfidence mistakes, and robust to long tails.• Facilitating tools to inspect AI-based systems, identifyhazards and anomalies, calibrate them, identify honestoutputs, and detect emergent capabilities. One risk ofAI systems that links with the need for safety tools isthat they may carry backdoors [42]: backdoored modelsbehave correctly in nearly all scenarios, except in chosenscenarios taught to behave incorrectly due to the trainingon poisoned data as a way to have backdoors injected.These are problematic, specially in foundational modelsthat serve as the architectural backbone of downstreammodels, all evolved from originally poisoned data frommassive training datasets [40].• Defining safety objectives in order to steer models, ei-ther internally (how models should learn to guaranteecompliance with safety metrics) and externally (how suchsafety compliance can be safely pursued). Problems in thisregard include:– Value learning, as the inability of AI systems to codehuman values (e.g., happiness, sustainability, mean-ingful experiences or safe outcomes). Although givingopen-world inputs to models can partially tell apartpleasant and unpleasant states, utility values of suchstates are no ground truth values, and are a result ofthe model’s own learned utility function [43].– Proxy gaming: This is a phenomenon due to the fact thatoptimizers and adversaries can manipulate objectiveproxies. As Goodhart’s law states, a measure ceasesto be a reliable indicator when it becomes the target.For instance, proxy gaming occurs as reward hacking inreinforcement learning. Similarly, objective countablemetrics end up substituting human values when opaqueAI models are forced to learn by optimizing a singlequantitative measure13. Therefore, merely acquiring aproxy for human values is insufficient: models mustalso be resilient to solvers seeking to manipulate it.13These are also known as weapons of math destruction [44] that maycontain pernicious feedback loops that perpetuate stereotypes and biases[45] if they do not consider context nor a concrete person’s features, butrather those of its neighbors.5.3.3. ReproducibilityOnce robustness and safety have been addressed, animportant dimension in this key requirement for trustworthyAI is reproducibility. It can be defined as the ability of AIexperiments to exhibit the same behavior when repeatedunder the same conditions. Reproducibility is related toreplicability, which refers to the capability to independentlyachieve non-identical conclusions that are at least similarwhile differences in sampling, research procedures and dataanalyses may exist [14]. Since both concepts are essentialparts of the scientific method, the National Information Stan-dards Organization (NISO) and the Association for Comput-ing Machinery (ACM) redefine these concepts as:• Repeatability (same team, same experimental setup),which means that an individual or a team of individualscan reliably repeat his/her/their own experiment.• Replicability (different team, same experimental setup):an independent group of individuals can obtain the sameresult using artifacts which they independently develop intheir entirety.• Reproducibility (different team, different experimentalsetup with stated precision): a different independent groupcan obtain the same result using their own artifacts.It should be clear that when formulated in the contextof trustworthy AI systems, one should regard an experimentin the above definitions as the performance, robustness andsafety evaluation of a given AI-based system. This evalua-tion can be done by different groups (as in research) or bya certification lab (as in commercial software-based solu-tions). The extent to which reproducibility can be guaranteedin trustworthy AI systems depends on the confidentialityof the system or the singularity of the experimental setupfor which the AI-based system was developed, among otherconstraining circumstances. For instance, in mild contexts(as in research), reproducibility of experiments by thirdparties is often favored by public releases of the source codeimplementing the AI-based system being proposed.5.4. Requirement 3: Privacy and data governanceWHAT does it mean? This requirements assures the re-spect for privacy and data protection thorough the AI systemlife cyle (design, training, testing, deployment and oper-ation), adequate data governance mechanisms taking intoaccount the quality and integrity of the data and its relevanceto the domain, and also ensures legitimized access to dataand processing protocols.WHY is it important for trustworthiness? AI systemsbased on digital records of human behavior can be capa-ble of inferring individual preferences and reveal personalsensitive information such as the sexual orientation, age,gender, religious or political views. Since AI-based systemslearn from data, systems must guarantee that such personalinformation is not revealed while data is processed, storedand retrieved throughout the AI life cycle, facilitating meansN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 11 of 30Connecting the Dots in Trustworthy Artificial Intelligenceto trace how data is used (governance) and verifying thatprotected information is not accessed (privacy awareness)during the life cycle phases. If such guarantees are not pro-vided, AI-based systems will not be trusted by end users, norwill they conform to existing legislation (e.g. the EuropeanGDPR). Citizens should have full control over their data,and their data will not be unlawfully or unfairly used toharm or discriminate against them [28]. This requirementis important to preserve human rights such as the rightto privacy, intimacy, dignity or the right to be forgotten.Keeping the usage and scope of the data limited, protectedand informed is paramount, since digital information canbe used towards clustering a person into profiles that maynot reflect reality, while reinforcing stereotypes, historicaldifferences among minorities, or perpetuate historical orcultural biases [44].HOW can this requirement be met in practice? In thefollowing we analyze technologies that can maintain dataprivacy in AI-based systems (Subsection 5.4.1), and strate-gies to deal with data governance as quality and integrityprocesses (Subsection 5.4.2).5.4.1. Data privacyIn order to land down the data privacy requirement intoactual technologies, we emphasize the relevance of Feder-ated learning (FL) [46, 47], homomorphic computing [48]and differential privacy (DP) [49] as examples of privacy-aware technologies in the current AI landscape:• In FL, a model is trained across multiple decentralizeddevices without moving the data to a central location. Indoing so, instead of delivering all the data to a centralserver, devices learn models locally using their own data,so that only numerical model updates are sent to thecentral server. The central server aggregates the updatedmodel parameters from all the devices or servers to createa new model. This allows learning a global model lever-aging all data in situations where the data is sensitive.Besides preserving the privacy of local data, FL canreduce communication costs and accelerate the modeltraining.• In homomorphic computing, data can be processed inencrypted form without the need for deciphering it first.As a result, data remains secure and private by performingoperations directly on encrypted data. By using speciallydevised mathematical operations, the underlying structureof data is preserved while it is processed, so that the resultof the computation, which is also encrypted, stays thesame. Only authorized parties having the decryption keycan access this information. Homomorphic computingcan be an effective way to implement privacy-aware pre-processing, training and inference in AI-based systems.• Finally, DP enables processing and learning from datawhile minimizing the risk of identifying individuals inthe dataset at hand. To this end, DP injects random noiseto the data before it is processed. This noise is calibratedto guarantee that the data remains statistically accurate,while concealing any information that could be used toidentify individuals and thereby, compromise their pri-vacy. The amount of noise added to data balances betweenthe level of privacy protection provided by DP and theperformance degradation of the AI-based system whencompared to the case when no noise is injected.By resorting to any of the above technologies (alsocombinations of them), the privacy of individuals in thedatasets is preserved, minimizing their risk of harm.5.4.2. Data governance: Quality and integrity of dataand access to dataData protocols must govern data integrity and access forall individuals even if these are not users of the AI system.Only duly qualified staff, with explicit need and competence,should be allowed to access individuals’ data. As a part of AIgovernance, data governance calls for a broader level regula-tion than a single country or continent regulation. This con-text has motivated guidelines and recommendations for AIgovernance over the years emphasizing on the importanceof ensuring data quality, integrity and access. An examplecan be found in the Universal Guidelines for AI publishedin 2018 [50], which were endorsed by 70 organizations and300 experts across 40 countries. In these guidelines, DataQuality Obligation was established as one of the principlesthat should be incorporated into ethical standards, adoptedin regulations and international agreements, and embeddedinto the design of AI-based systems. These recommenda-tions helped inform the OECD AI Principles (2019), theUNESCO Recommendation on AI Ethics (2021), the OSTPAI Bill of Rights (2022), and the EU AI Act and the Councilof Europe Convention on AI.In terms of guidelines to implement data governance,the Information Commissioner’s Officer (ICO) has proposedrecommendations on how to use AI and personal data appro-priately and lawfully [51]. Among these, there are actionssuch as taking a risk-based approach when developing anddeploying AI – “addressing risk of bias and discrimination atan early stage”, “ensuring that human reviews of decisionsmade by AI is meaningful”, “collect only data needed andno more”, and “working with external suppliers to ensurethe use of AI will be appropriate”.At the European level, the European Strategy for Dataestablished in 2020 aims at making the EU a role modelfor a society empowered by data. This strategy has givenrise to the European Data Governance Act [52] to facilitatedata sharing across sectors and Member States. In particular,the EU Data Governance Act intends to make public sectordata available for re-use, promote data sharing among busi-nesses, allow the use of personal data through a “personaldata-sharing intermediary”, help exercising rights under theGeneral Data Protection Regulation (GDPR), and allowingdata use on altruistic grounds [52].N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 12 of 30Connecting the Dots in Trustworthy Artificial IntelligenceLater in 2022, the European Union strategy for dataproposed the Data Act [53]14, a regulation harmonizing ruleson fair access to and use of data. In practice, this regulationcomplements the Data Governance Act by specifying whocan create value from data and under which circumstances.In practice, the Data Act will take action to 1) increase legalcertainty for companies and consumers who generate data,on who can use what data and under which conditions, 2)prevent abuse of contractual imbalances that hinder fair datasharing. 3) provide means to the public sector to access dataof interest held by the private sector; 4) set the frameworkconditions for customers. Therefore, the benefits of the DataAct for consumers and business include, from achievingcheaper aftermarket prices for connected objects, to newopportunities to use services based on data access, andbetter access to data produced by devices. Serving these twoEU regulations, ten European common data spaces exist,ranging from industry to mobility .5.5. Requirement 4: TransparencyWHAT does it mean? Transparency is the property thatensures appropriate information reaches the relevant stake-holders [29]. When it comes to AI-based systems, differ-ent levels of transparency can be distinguished [54]: sim-ulatability (of the model by a human), its decomposability(the ability to explain the model behavior and its parts),and algorithmic transparency (understanding the process ofthe model and how it will act for any output). Anotherclassification establishes transparency at the algorithmic,interaction and social levels [55], emphasizing the role of thestakeholder audience to which the explanation is targeted:developer, designer, owner, user, regulator or society.WHY is it important for trustworthiness? In the contextof trustworthy AI systems, data, the system itself and AIbusiness models should be transparent. Humans must beinformed of systems capabilities and limitations and alwaysbe aware that they are interacting with AI systems [3].Therefore, explanations should be timely, adapted and com-municated to the stakeholder audience concerned (laypersonregulator, researcher or other stakeholder), and traceabilityof AI systems should be ensured.HOW can this requirement be met in practice? Thedimensions to be treated within this requirement concerntraceability, explainability and communication, which areessential for realizing transparent AI-based systems. In thefollowing we will first explain what traceability stands for(Subsection 5.5.1), the current state of the art on explainableAI (Subsection 5.5.2), and mechanisms for communicatingAI systems decisions (Subsection 5.5.3).14Data Act & Data Act Factsheet, https://digital-strategy.ec.europa.eu/en/policies/data-act, accessed on April 25th, 2023.5.5.1. TraceabilityTraceability is defined as the set of mechanisms andprocedures aimed to keep track of the system’s data, devel-opment and deployment processes, normally through doc-umented recorded identification [14]. Traceability and log-ging from the early design stages of the AI-based systemscan help auditing and achieving the required level of trans-parency according to the needs of the concerned audience.In this regard, special attention must be paid to prove-nance tools [56], which ease the traceability or lineage ofdata and model decisions, hence contributing to the require-ment of transparency. In this area, the use of Blockchainmechanisms are promising towards guaranteeing the in-tegrity of data used to train (and explain) machine learningmodels, i.e., the provenance of databases, their associatedquality, bias and fairness.5.5.2. ExplainabilityThe so-called eXplainable AI (XAI) [54] field is widelyand globally being recognized as a crucial feature for thepractical deployment of trustworthy AI models. Existingliterature and contributions already made in this field includebroad insights into what is yet to be achieved [54, 57, 58].Efforts have been invested towards defining explainabilityin machine learning, extending previous conceptual propo-sitions and requirements for responsible AI by focusing onthe role of the particular audience for which explanations areto be generated [54]: Given an audience, an explainable AI isone that produces details or reasons to make its functioningclear or easy to understand.Explainability techniques are blooming as tools to sup-port algorithmic auditing. They have emerged as a necessarystep to validate and understand the knowledge captured byblack-box models, i.e., a system in which only inputs andoutputs are observed without knowing the internal detailsof how it works. This can be problematic, as we cannotpredict how the system may behave in unexpected situations(connecting with the technical robustness requirement, Sub-section 5.3), or how it can be corrected if something goeswrong (linked to the accountability requirement, Subsec-tion 5.8). Explaining which input factors contribute to thedecisions of complex black-box algorithms can provide auseful global view of how the model works, jointly withtraceability methods and a clear and adapted communicationof information to the target audience.Since the quality of explanations depends on the audi-ence and the motivation for which they are generated, severaltaxonomies of XAI techniques have been proposed overthe years [54]. A primary distinction can be done betweenmodel-agnostic and model-specific approaches to explainingmachine learning models, the difference being whether theXAI technique can be applied to any machine learning modeldisregarding their structure and learning algorithm. Anotherdistinction can be done between ex-ante and post-hoc XAItechniques, depending on the moment at which explainabil-ity is addressed (before or after the model is designed andtrained). On one hand, ex-ante techniques (also referred to asN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 13 of 30Connecting the Dots in Trustworthy Artificial Intelligencethe explainable-by-design paradigm) make AI models aspireto provide an explanation that avoids the construction ofadditional models or extra complexity (layers or mechanismsnot originally part of the original one), so that explanationsare as faithful to the real reasoning carried out by the modelas possible. On the other hand, post-hoc XAI techniquesusually add artifacts around the original AI model or build asurrogate of it – a local approximation or simpler versionof the original one – in order to more easily explain theoriginal one (for example, LIME [59]). Likewise, some XAItechniques may use external knowledge (e.g. from the web,Wikipedia, forums) [60], for instance, to explain languagemodels or dialogue models that interactively answer ques-tions about a model’s particular decision.Other criteria to categorize XAI tools can be formulated,such as the format of the issued explanations (e.g., attri-bution methods [61], counterfactual studies [62], simplifiedmodel surrogates [63]) or the hybridization of explanationsexpressed in different modalities, such as visual and textual(e.g., linguistic summaries [64], ontologies [65], or logicalrules defined on top of knowledge graphs [66], to cite afew). Natural language explanations [67, 27], quantitativemeasures of the quality of explanations [68, 69], and modelsthat support their learning process with formal symbolicbasis such as language, rules, compositional relationships orknowledge graphs (neural-symbolic learning and reasoning[66]) are key for explanations to be understood by non-expertaudience. These interfaces allow such users to assess theoperation of the model in a more intelligible fashion, hencesupporting the human agency and oversight requirement fortrustworthy AI systems (Subsection 5.2).5.5.3. CommunicationThe third dimension of transparency is how the audienceis informed about the AI-based system, namely, how expla-nations or information tracked about the system’s operationis communicated to the user. Humans should know whenthey are interacting with AI systems, as well as be notifiedabout their performance, instructed about their capabilities,and warned about their limitations. The same holds whenconveying the model’s output explanation and its function-ing to the user. The adaptation of the explanation must be inaccordance to the specifics of the AI system being explainedand the cognitive capabilities (knowledge, background ex-pertise) of the audience.Therefore, communication is a crucial dimension, sothat all aspects related to transparency are delivered to theaudience in a form and format adapted to their backgroundand knowledge. This is key to attain trust in the audienceabout the AI-based system at hand.5.6. Requirement 5: Diversity, non-discriminationand fairnessWHAT does it mean? This requirement contemplates dif-ferent dimensions: the avoidance of unfair bias, diversityfostering, accessibility to all regardless any disability, andthe involvement of stakeholders in the entire AI system lifecycle. All these dimensions of this manifold requirementshare an ultimate purpose: to ensure that AI-based systemsdo not deceive humans nor limit their freedom of choicewithout reason. Therefore, it is a requirement tightly linkedto the ethical and fairness principles that underlie trustwor-thiness in AI (Section 2).WHY is it important for trustworthiness? This require-ment is necessary to broaden the impact of AI to all socialsubstrates, as well as to minimize the negative effects thatautomated decisions may have in practice if data inheritsbiases hidden in the data from which models are learned.Unfair bias in data must be avoided as decisions drawn by amodel learned from such data could have multiple negativeimplications, including the marginalization of vulnerablegroups, the exacerbation of prejudice or discrimination [3].HOW can this requirement be met in practice? Meth-ods to tackle this requirement can be classified depending onthe specific dimension they support: as such, methods to en-force diversity, non-discrimination, accessibility, universaldesign and stakeholder participation are briefly revisited inSubsection 5.6.1, whereas Subsection 5.6.2 describes how toachieve fairness in AI-based systems. Finally, Section 5.6.3examines mechanisms to avoid unfair bias.5.6.1. Diversity, non-discrimination, accessibility,universal design and stakeholder participationAI systems should take into account all human abilities,skills and requirements, and ensure accessibility to them.Developing methodologies based on the requirement of non-discrimination and bias mitigation is paramount to ensurethe alignment of requirements to the compliance with ethicalvalues. Assuring properties of non-discrimination, fairnessand bias mitigation restrict the systematic differences treat-ing certain groups (of people or objects) with respect toothers [29]. A practical example of recommendation encour-ages, e.g., hiring from diverse backgrounds, cultures anddisciplines to assure opinion diversity.This requirement involves the inclusion of diverse dataand people, and ensures that individuals at risk of exclusionhave equal access to AI benefits. Concrete implementationsof this requirement range from quantifying the impact ofdemographic imbalance [70] and the effects of missing data(which, as a matter of fact, has been shown to be beneficialin terms of fairness metrics [71]).In what refers to diversity, it advocates for the needs forheterogeneous and randomly sampling procedures for dataacquisition, diverse representation of a population that in-cludes minorities, and the assurance for non-discriminatingautomated processes that lead to unfairness or biased mod-els. Diversity can be applied at the technical level dur-ing model training by penalizing the lack of diverse pro-totypes on latent space areas with challenging separationbetween classes [72]. Alternatively, the lack of diversitycan be counteracted by means of imbalanced learning orby informing data augmentation. When placing the focuson the solutions of an AI-based system, their diversity is aN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 14 of 30Connecting the Dots in Trustworthy Artificial Intelligencevery relevant component to guarantee non-biased outcomes.Frameworks unifying quality and diversity optimization canguarantee the diversity of generated solutions that may laterserve in robotics to learn behaviorally diverse policies [73].From a global perspective, the so-called discrimination-conscious by-design paradigm collective refers to method-ologies where discrimination detection and prevention isconsidered from the beginning of the design of the AI-based system through fairness [74]. Methods adopting thisparadigm include discrimination-aware data mining [75],compositional fairness, interpretation of sanity checks andablation studies.In summary, diversity must be enforced both in the datafrom which models are learned and among the stakeholders,i.e., fostering the inclusion of minorities (practitioners andusers) [7, 22] or using methodologies such as participatorydesign for accessibility [76]. Universal Design principles,which consider accessibility and “design for all” [3] dur-ing development, governance, policy and decision makingprocesses is one way to facilitate AI life cycles that takeinto account what is beneficial for everyone, accounting fordifferent conditions and situations, and no discrimination.To further enable universal design and stakeholder par-ticipation, often feedback – even after deployment – issought for stakeholder participation and consultation. Oneway to achieve this is through active learning for machinelearning systems. Active learning allows for the integrationof users’ feedback while models are learned, and enablesinteractivity with the user, one of the goals targeted byhuman-centered AI [77] and AI for social good [23].5.6.2. FairnessThe second dimension of this requirement is fairness,namely, techniques aimed to reduce the presence of unfairoutputs elicited by AI-based systems. An unfair algorithmcan be defined as that producing decisions that favor a partic-ular group of people. Following the comprehensive view onthis topic published in [78], biases leading to such unfair de-cisions can be propagated from the data to the AI algorithm(including measurement, omitted variable sampling, or rep-resentation biases, among others); from the algorithm to theuser (as in algorithmic, popularity or evaluation biases); orfrom the user to the data (respectively, biases induced inthe production of content, temporal, historical and/or socialbiases).Fairness guarantees in the decisions of AI-based systemshas been approached extensively in the literature, reportingbias targeting methods that can be classified in three largegroups:• Pre-processing methods, where the available data aretransformed for the source of bias to be reduced and atbest, removed.• In-processing methods, which modify the learning algo-rithm of the model at hand (by e.g. changing the objectivefunction at hand or imposing constraints to the optimiza-tion problem) so as to minimize the effect of biases in thetraining process.• Post-processing methods, which operate on the outputof the model (for instance, by reassigning the predictedclass for a query instance) without modifying its learningalgorithm or the training data from which it was learned.In general, it is widely acknowledged that fairness canbe achieved by sacrificing accuracy to a certain extent [79].However, it is also possible to debias machine learning mod-els from the influence of spurious features to even improvetheir performance [80]. Another trade-off is between fairnessand privacy. Here, adversarial learning [81] can simultane-ously learn a predictor and an adversary that models a pro-tected variable, and by minimizing the adversary capacity topredict this protected variable, accurate predictions can showless stereotyping of the protected variable, almost achievingequality of odds as a fairness notion.An important concept to be acquainted with when deal-ing with fairness in AI-based systems is fairwashing: as arisk of rationalization, fairwashing is the promotion of a falseperception that a machine learning model respects ethicalvalues through an outcome explanation and fairness metric[82]. This makes it critical to characterize the manipulabilityof fairwashing [83], as well as LaundryML approaches [82]to better audit unfair opaque models.5.6.3. Avoidance of unfair biasData and models can be exposed to a large set of potentialbias-inducing phenomena. Ensuring diversity, representa-tiveness and completeness in data and models needs to bea core part of the full AI life cycle (design, developmentand deployment phases of AI-based systems). Bias can beuncovered through proxy discrimination by models, sinceproxy variables are likely to be picked up, showing featuresas proxy that otherwise would not have been considered, i.e.,zip codes in predictive policing [44]. As has been shown inthe previous dimension, bias is not only algorithmic, but ex-tends beyond the limits of models in a vicious cycle: startingwith human activity bias, data bias, leads to sampling biason the web (specially to be considered in the use of data tolearn generative models), algorithmic bias, interaction biasand finally, self-selection bias that can revert back into thealgorithm a second-order bias [84].Bias mitigation techniques include several approaches[45, 85], from generic requirements and toolboxes [86] toconcrete taxonomies of bias [87, 88] at different stages ofthe AI life cycle [89]. Different notions of fairness can bealso defined [78, 90], including causal fairness – which relieson causal relations and requires establishing causal graphs– or counterfactual fairness. Causality can help debuggingalgorithmic bias mitigation or explaining models [91], e.g.,causal mediation analysis can help uncover disparate impactof models by estimating the fairness associated to differentexplaining variables [92].N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 15 of 30Connecting the Dots in Trustworthy Artificial Intelligence5.7. Requirement 6: Societal and environmentalwellbeingWHAT does it mean? AI-based systems should benefit allhumankind, not only at the present time but also in futuregenerations. Therefore, AI-based systems must be sustain-able and environmentally friendly, so that the technologicaladoption of AI does not entail a progressive depletion ofnatural resources and maintains an ecological balance [29].Therefore, dimensions supporting this requirement includesustainability and environmental friendliness, as well as acareful assessment of the social and societal impacts of AI.WHY is it important for trustworthiness? AI systemsshould increase positive social change and enhance sus-tainability and ecological responsibility. Although they canbe an effective tool to mitigate climate change [93, 94],greenhouse gases emitted by the computationally intensivetraining processes of complex AI-based systems can exacer-bate existing social and ethical challenges linked to AI [95].For instance, training only one single AI model can emitas many CO2 emissions as five cars in their entire lifetime.Computational and environmental costs grow proportionallyto the complexity of the model in terms of its numberof parameters [96]. In particular, this study was done forlarge language models [97] which cost about 8.4 tons peryear, where an average carbon footprint for a person yearlyis around 4. Although emissions are amortized over themodel lifetime, the recent ChatGPT model was estimatedto consume 1,287 MWh that translates into a cost of 522tCO2e [98]. Therefore, energy and policy considerationsare to be taken into account by institutions and companiesimplementing AI [97].HOW can this requirement be met in practice? Thisrequirement is currently approached from two different an-gles: sustainability and environmental wellbeing (Subsec-tion 5.7.1) and societal wellbeing (Subsection 5.7.2).5.7.1. Sustainability and environmental wellbeingSustainable AI [99] considers a holistic perspective thatspans from models to data algorithms and hardware, andhow software-hardware co-design can help mitigate carbonfootprints of AI model life cycles (design, training and de-ployment stages). As mentioned previously, sustainable AIfinds its motivation in the costly energy consumption of largeAI models. Thus, sharing key learned lessons, best designpractices, metrics, and standards is key for a sustainabledevelopment of AI systems. Technical contributions aimedto implement this requirement for the sustainability of AIare at the core of the Green AI research area [100], whichstudies efficient and ecologically aware designs of AI-basedalgorithms, systems and assets.Many strategies to attain this requirement have beenproposed over the years to reduce the environmental impactof AI models, with emphasis on those characterized bya large number of parameters and requiring long traininglatencies (e.g., deep neural networks). Among others:• Assessment of the environmental impact of AI-basedsystems with e.g., carbon footprint calculators15 [101].Evaluating the factors that influence AI’s greenhouse gasemissions is the first step towards mitigating its negativeeffects [95].• Selection of the most relevant and necessary data, i.e.,with smart data approaches [102].• Model compression [103, 104], e.g. using quantization[105], distillation techniques [106, 107] or acceleration[108] techniques.• Consideration of efficiency as an evaluation metric and asa price tag to make models greener and more inclusive forresearchers having limited resources [100].• Use of models that can rapidly adapt to new situations,domains and similar tasks by virtue of learning function-alities specifically devoted to this adaptation (e.g., mul-titask, few-shot learning, AutoML, meta-learning, neuralarchitecture search or open-ended learning. This familyof GPAIS can provide more efficient, sustainable and lessdata depending AI systems.• Deployment of models on cloud computing servers fedwith renewable energy sources, to minimize CO2 emis-sions.5.7.2. Societal wellbeingAt the societal level, AI can improve social welfare. AI-based systems can perform routine tasks in an autonomoussafer, and more efficient fashion, enhancing productivityand improving the quality of life of humankind. In thepublic administration AI can speed up processes, smooth ad-ministrative bottlenecks and save paperwork. Furthermore,it can aid policy making and help city planners, e.g., byvisualizing the consequences of climate change, predictingfuture floods, or identifying urban heat islands. Possibilitiesfor the society at large to benefit from AI developments haveexploded in recent years with the progressive digitization ofalmost all sectors of activity. Infrastructure planning, healthand hunger, equality and inclusion, education, economicempowerment, security and justice are among those sectorswhere AI can unleash its full potential to foster use cases ofsocietal impact.Bringing such benefits of AI into practice is, therefore,a matter of leveraging such amounts of available data inAI-based systems. Such AI-based systems address learningtasks that solve a problem of societal impact, such as theones exemplified above. However, since decisions issued bythe AI-based system affect human beings and are subjectto social scrutiny, other requirements of trustworthy AIbecome of utmost relevance, including fairness, privacy,transparency or human oversight. Above all, the importanceof AI ethics and regulation becomes paramount in societal15Greenhouse Gases Equivalencies Calculator, https://www.epa.gov/energy/greenhouse-gases-equivalencies-calculator-calculations-and-references, accessed on April 25th, 2023.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 16 of 30Connecting the Dots in Trustworthy Artificial Intelligencewellbeing, since decisions issued in use cases arising in edu-cation, justice and security have to comply with fundamentalhuman rights and the legal restrictions in force.5.8. Requirement 7: AccountabilityWHAT does it mean? This last requirement of trustworthyAI systems imposes the provision of mechanisms to ensureresponsibility and accountability for the development, de-ployment, maintenance and-or use of AI systems and theiroutcomes. Auditability, which enables the assessment ofalgorithms, data and design processes, plays a key role inaccountability, namely, the attribution of the results of theactions that were taken based on the outcome of the AI-based system. Accountability, therefore, implies the min-imization of harm and reporting of negative impact, thecommunication of design trade-offs to the user, and theimplementation of adequate and accessible redress strategiesassociated to AI-based systems. Therefore, auditability andaccountability are closely related to each other and lie at thecore of responsible AI systems, which are later discussed inSection 6.1.WHY is it important for trustworthiness? The requiredauditability property of Trustworthy AI systems demandsthe development of practical tools [109] that are capableof verifying desirable properties of neural networks suchas stability, sensitivity, relevance or reachability [110], aswell as metrics beyond explainability [111, 112, 113, 114,115], such as on traceability, data quality and integrity.Auditability is becoming increasingly important when stan-dards are being materialized touching upon all AI require-ments. This includes IEEE, ISO/IEC and CEN/CENELEC,which are implementing concrete guidelines to apply trust-worthy AI requirements in industrial setups (see [29, 11]for an overview). At the national level, the German stan-dardization road map on AI within DIN/DKE [11] is aclear exponent of the standardization efforts made by differ-ent governments to dictate how practical AI-based systemsshould be audited.On the other hand, accountability is a key requirementto be able to recourse [116] when an AI model contributesto making a proven wrong decision, issuing explanationsand recommendations to cases that are unfavorably treatedby such decision. Accountability is a matter of compliancewith ethical and legal standards, answerability, reporting andoversight, and attribution and enforcement of consequences[117]. Therefore, when framed under AI regulatory stan-dards and ethical principles like the ones discussed in thiswork, accountability becomes crucial for AI-based systemsto distribute cost, risks, burdens and liabilities among thedifferent stakeholders participating in its life cycle.HOW can this requirement be met in practice? Simi-larly to other requirements, we next analyze how the differentdimensions spanned by this requirement can be tackledin practice. In doing so, Subsection 5.8.1 deals with ac-countability, whereas Subsection 5.8.2 addresses auditabil-ity. The minimization and reporting of negative impactsis discussed in Subsection 5.8.3. Finally, Subsection 5.8.4describes methods for algorithmic redress.5.8.1. AccountabilityMechanisms of accountability are especially relevant inhigh-risk scenarios, as they assign responsibility for deci-sions in the design, development and deployment phasesof the AI system. Tools to attain this requirement involvealgorithmic accountability policy toolkits (e.g., [118]), thepost-hoc analysis of the output of the model (e.g. via localrelevance attribution methods) or algorithms for causal in-ference and reasoning [119]. Since accountability is linkedto the principle of fairness, it is closely related to risk man-agement since unfair adverse effects can occur. Therefore,risks must be identified and mitigated transparently so theycan be explained to and verified by third parties. Therefore,techniques and tools for auditing data, algorithms and de-sign processes are required for accountable decisions issuedby AI-based systems. An overview on 16 risk assessmentframeworks is available in [120], whereas built-in deriskingprocesses at design and development phases can be found in[86, 121]. These processes operationalize risk managementin machine learning pipelines, including explainability andbias mitigation. Another set of resources to tackle bias andfairness are discussed in [86].Emerging trade-offs between requirements should bestated and assessed with regards to the risk they pose toethical requirements and compromise of fundamental rights,since no AI system should be used when no risk-free trade-off for these can be found [3]. Consequently, AI models use-ful for accountability often involve multi-criteria decisionmaking and pipelines at the MLOps level that help delineateand inform such trade-offs to the user.5.8.2. AuditabilityThe AI Act has been interpreted as the European ecosys-tem to conduct AI auditing [122]. In the strict sense, theneed for certifying systems that embed AI-based function-alities in their design is starting to permeate even withinthe international ISO standards for AI robustness. In suchstandards, formal methods for requirement verification orrequirement satisfaction, typical of software engineering,are being extended towards verifying desirable propertiesof AI models. More specifically, in order to certify neuralnetworks, properties such as stability, sensitivity, relevanceor reachability are sought [110].In terms of auditing procedures, especially when the AIsystem interacts with users, grading schemes adapted to theuse case [123] are in need for validating models. Examplesinclude the System Causability Scale [123] or the MuirTrust Scale [124], which are widely adopted in human robotinteraction and robotics and rely on predictability (To whatextent the robot behavior [the output of the AI-based system]can be predicted from moment to moment?), reliability (ToN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 17 of 30Connecting the Dots in Trustworthy Artificial Intelligencewhat extent can you count on the system to do its job?),competence (What degree of faith does the user have on thesystem for it to cope with similar situations in the future?)and trust (How much does the user trust the system overall?).5.8.3. Minimization and reporting of negative impactsand trade-offsThe urgent need for developing stable and verifiablemechanisms for auditing AI-based systems becomes morerelevant in the case of generative AI, which has grown somaturely that it is difficult to distinguish between human-created multimodal content and those generated by ma-chines. If these are not properly identified, they can gen-erate confusion and deception, which may have negativeconsequences for society, such as the manipulation of publicopinion or the dissemination of fake news.A promising stream along these lines proposes to landthe implementation of verifiable claims [125], which aredefined as those falsifiable claims for which evidence andarguments can be provided to influence the probability thatsuch claims are true. This proposal stems from the effortsof developers, regulators and other AI stakeholders, and theneed to understand what properties of AI systems can becredibly demonstrated, through what means, and what trade-offs or commitments should and can be quantified. While thedegree of certainty achievable varies across different claimsand contexts, the idea is to demonstrate that greater degreesof evidence can be provided for claims about AI develop-ment than is typically done today to facilitate auditing them.5.8.4. RedressLastly, once the risk has turned into a confirmed incident,it is paramount that the user is aware of the possibilityto redress, preserving his/her trust when adverse or unfairimpact takes place [3]. Redress is related to the concept ofalgorithmic recourse [116], and consists of a procedure tocorrect or reverse an AI system outcome that is consideredwrong. A key to trustworthy AI is ensuring adequate redressagainst decisions made by AI systems and by humans op-erating them through accessible mechanisms to their userswhen these fail, without forgetting vulnerable persons orcollectives. Redress mechanisms are to be ensured, and com-plemented with accountability frameworks and disclaimers,since certification will obey particular application domains,and cannot replace responsibility. Machine unlearning [126],counterfactual explanations [127] or the analysis of disparateimpacts [128] can be also regarded as techniques that cansupport redress in AI-based systems.6. Trustworthy Artificial Intelligence fromtheory to practice and regulation:responsible Artificial Intelligence systemsSo far we have exposed the vision of trustworthy AI thathas been tackled in most of the literature: from a theoreticalpoint of view, and mainly based on principles and recom-mendations. In this section we highlight the importance oftackling trustworthy AI from a practical perspective. A clearmapping from trustworthy AI principles and requirementsinto operative protocols that can be automated, verified andaudited does not always exist. To achieve this, the fieldneeds blueprints and standard models to be adopted andstandardized. In what follows we stress on the utmost impor-tance of having practical regulatory scenarios (regulatorysandboxes) and the final output of processes implementingHRAIs using trustworthy AI: a responsible AI system.According to this idea, the section is organized as fol-lows. First, Subsection 6.1 defines the nuanced yet necessarynotion of responsible AI systems, to comply with bothtrustworthy AI requirements and the law in force. Then,Subsection 6.2 describes the technical requirements that theimplementation of HRAIs will legally require in practice.Then, Subsection 6.3 presents how these requirements aregoing to be evaluated by regulators and auditors throughregulatory sandboxes. Subsection 6.4 examines whether allthese steps can be connected and applied through a blueprintproposal to implement trustworthy AI in healthcare. Finally,Subsection 6.5 examines the implications of new HRAIS andemerging AI systems, justifying the necessity of a dynamicregulation and flexible evaluation protocols to deal with newhigh-risk scenarios supported by these systems.6.1. Responsible Artificial Intelligence systemsA little prior to trustworthy AI is the term responsibleAI, which has been widely used quite as a synonym. How-ever, it is necessary to make an explicit statement on thesimilarities and differences that can be established betweentrustworthy and responsible AI. The main aspects that makesuch concepts differ from each other is that responsibleAI emphasizes the ethical use of an AI-based system, itsauditability, accountability, and liability.In general, when referring to responsibility over a cer-tain task, the person in charge of the task assumes theconsequences of his/her actions/decisions to undertake thetask, whether they result to be eventually right or wrong.When translating this concept of responsibility to AI-basedsystems, decisions issued by the system in question must beaccountable, legally compliant, and ethical. Other require-ments for trustworthy AI reviewed in this manuscript (suchas robustness or sustainability) are not relevant to responsi-bility. Therefore, trustworthy AI provides a broader umbrellathat contains responsible AI and extends it towards consid-ering other requirements that contribute to the generation oftrust in the system. It is also worth mentioning that providingresponsibility over AI products links to the provision ofmechanisms for algorithmic auditing (auditability), whichis part of requirement 7 (Accountability, Subsection 5.8).Stressing on the importance of a responsible developmentof AI, we now define the responsibility associated to AIsystems, following the discussed features.Definition. A Responsible AI system requires ensuring au-ditability and accountability during its design, developmentN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 18 of 30Connecting the Dots in Trustworthy Artificial Intelligenceand use, according to specifications and the applicable reg-ulation of the domain of practice in which the AI system isto be used.In the following we discuss in depth these features:1. Auditability: As an element to aid accountability, a thor-ough auditing process aims to validate the conformityof the AI-based asset under target to 1) vertical or sec-torial regulatory constraints; 2) horizontal or AI-wideregulations (e.g., EU AI Act); and 3) specifications andconstraints imposed by the application for which it isdesigned. It is important to note that auditability refersto a property sought for the AI-based system, whichmay require transparency (e.g. explainability methods,traceability), measures to guarantee technical robustness,etc. This being said, the auditability of a responsibleAI system may not necessarily cover all requirementsfor trustworthy AI, but rather those foretold by ethics,regulation, specifications and protocol testing adapted tothe application sector (i.e., vertical regulation).2. Accountability: which establishes the liability of deci-sions derived from the AI system’s output, once its com-pliance with the regulations, guidelines and specifica-tions imposed by the application for which it is designedhas been audited. Again, accountability may comprisedifferent levels of compliance with the requirements fortrustworthy AI defined previously.In the context of the European approach and AI Act,this translates into a required pre-market use of regulatorysandboxes, and the adaptability of the requirements and reg-ulation for trustworthy AI into a framework for the domainof practice of the AI system.6.2. Artificial Intelligence systems’ compliancewith regulation in high-risk scenariosIt has been concluded in the previous section that theconformity of requirements are central for the definition ofresponsible AI systems. In Europe, regulatory requirementsin force for the deployment of AI-based systems are pre-scribed based on the risk of such systems to cause harm.Indeed, the AI Act agreed by the European Parliament, theCouncil of the European Union, and the European Com-mission, is foreseen to set a landmark piece of legislationgoverning the use of AI in Europe and regulating this tech-nology based on the definition of different levels of risks:minimal, limited and HRAIs. In these categories differentrequirements for trustworthy AI and levels of complianceare established, so that regulatory obligations are enforcedtherefrom.Furthermore, the European Commission has also askedthe European Committee for Standardization (CEN), theEuropean Committee for Electrotechnical Standardization(CENELEC) and the European Telecommunications Stan-dards Institute (ETSI) to develop standards aimed to coverdifferent practical aspects of AI systems, including foun-dational AI standards, data standards related to AI, BigData and analytics, use cases and applications, governanceimplications of AI and computational approaches of AI.Ethical, societal concerns and AI trustworthiness also appearamong the prioritized areas of these standardization bodies.Among these defined levels of risk associated to AI-based systems, those at the top of this classification (HRAIs)are subject to stringent obligations. HRAIs are demandedto comply with the AI Act through the fulfillment of thefollowing seven requirements (AI Act, Chapter 2 [4]):1. Adequate risk assessment and mitigation systems (Art. 9- Risk management system).2. High quality of the datasets feeding the system to mini-mize risks and discriminatory outcomes (Art. 10 - Dataand data governance; Art. 9 - Risk management system).3. Logging of activity to ensure traceability of results (Art.12 - Record Keeping; 20 - Automatically generated logs).4. Detailed documentation providing all information nec-essary on the system and its purpose for authorities toassess its compliance (Art. 11 - Technical documentation;Art. 12 - Record-keeping).5. Clear and adequate information to the user (Art. 13 -Transparency).6. Appropriate human oversight measures to minimise risk(Art. 14 - Human oversight).7. High level of robustness, security and accuracy (Art. 15- Accuracy, robustness and cybersecurity).HRAIs must undergo conformity assessments beforeentering the EU market. One of the most complete guidanceprocedures assisting on complying with AI Act regulation isthe CapAI procedure for conducting conformity assessmentof AI systems [129]. It describes notions and metrics, check-lists and other procedures to comply with the new legislation.Since the AI Act imposes obligations on providers, im-porters, distributors, and users, the latter can be deemed asproviders in certain cases. For instance, if a user slightlymodifies or uses a ready-made AI-based product such asChatGPT for a different purpose, this makes him/her becomeresponsible and accountable for the system’s consequences,depending on the conditions that define HRAIs. This is whyin order to realize trustworthy AI that is compliant withthe law, we advocate for the development of responsibleAI systems, i.e., systems that not only make a responsibleimplementation that fulfills the requirements for trustworthyAI, but also comply with the AI regulation.In practice, HRAIs providers must work to make theirassets meet these requirements, including post-market mon-itoring plans [122] (AI Act Art. 61 - Post-market monitoringby providers and post-market monitoring plan for high-riskAI systems) to document the performance throughout thesystem’s lifetime, in a way that vague concepts become veri-fiable criteria that strengthen the assessment safeguards andinternal checks. Likewise, conformity assessments (AI Act,N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 19 of 30Connecting the Dots in Trustworthy Artificial IntelligenceArt. 19 and Art. 43) will be ruled by internationally harmo-nized testing principles, guaranteeing high-quality testing.These tests can depart from ad-hoc procedures and protocolsfor the domain at hand. This is the case of the German stan-dardization roadmap on AI [11], which proposes conformityassessments based on several steps: calibration, inspection,audit, validation and verification.This need for harmonized testing protocols, monitoringplans and conformity assessment procedures is the mainreason for the emergence of the concept of AI regulatorysandboxes, which are next detailed and discussed.6.3. Artificial Intelligence regulatory sandboxes: Achallenge for auditing algorithmsOnce requirements for HRAIs have been established, theremaining challenge is to make the AI system comply withthem appropriately. Such requisites (AI Act, Chapter 2, Art.8-15) motivate the need for a test environment where toaudit AI-based systems by safe and harmonized proceduresestablished by the latter. Regulatory sandboxes are indeedrecommended by the AI Act (Chapter 5, Art. 53-54). Con-cretely, the AI Act establishes that algorithms should complywith regulation and can be tested in a safe environmentprior to entering the market. This auditing process can beimplemented via regulatory sandboxes.In order to successfully undertake AI auditing processesunder the new regulation, industry, academia and govern-mental actors are forced to adapt their processes and teamsto comply with the law. Regulatory sandboxes act as test bedsand safe playgrounds that allow assessing the compliance ofAI systems with respect to regulation, risk mitigation strate-gies, conformity assessments, accountability and auditingprocesses established by the law. Figure 4 illustrates the twostages where sandboxes play a crucial role: i) pre-marketauditability and conformity check, and ii) post-market mon-itoring and accountability. The figure illustrates not only thedifferent stakeholders participating in these two stages, butalso the articles in the AI Act where each step within theprocess is described.In the current context of rapidly evolving AI products,sandboxes allow market stakeholders and business players toexplore and experiment with new and innovative products,services or businesses under the supervision of a regulator.However, the idea of resorting to a sandbox to explore,evaluate and gauge complex technology is not new, nor ex-clusive of AI systems. They have already been used in othercontexts to test and validate Fintech [130] or Blockchain16technologies in the European Union. The objective of thesecontrolled environments is to test innovative technologies fora limited time, for innovators and regulators to cooperate17.The AI Act also contains measures with the aim to reducethe regulatory burden on Small and Medium Enterprises16Launch of the European Blockchain Regulatory Sandbox. https://digital- strategy.ec.europa.eu/en/news/launch- european- blockchain-regulatory-sandbox, accessed on April 25th, 2023.17First regulatory sandbox on Artificial Intelligence presented: https://digital-strategy.ec.europa.eu/en/news/first-regulatory-sandbox-artificial-intelligence-presented(SMEs) and startups, prioritize them, and to reduce theirtime to market by ensuring legislation can be implementedin two years. The intended goal is to support innovationand small-scale providers, getting apart from the regulationstifling innovation critique.The benefits of sandboxes is that they support the de-velopment, testing and validation of innovative AI systemsunder the direct supervision and guidance of competentauthorities (AI Act Art. 53). Furthermore, they allow ex-perimenting by derogation (by putting aside certain rulesor laws), and experimentation by devolution, which requiresbroad supra/national frameworks to establish guidelines thatempower and help local governments to establish a regula-tion in a particular area. This enables differences among gov-ernment levels by considering local preferences and needs asa means to stimulate innovative policies.When it comes to the challenges faced by sandboxes,there is a concern for the lack of proper methodologicalassessments that are indicative of the possible impact ofAI on the society [131]. This concern fosters the needfor cross-border and multi-jurisdictional regulatory sandboxstandardization [132], as well as generic AI standardization[133]. Governments will have to find a balance between EUcoordination and national procedures to avoid conflicts in theimplementation of the regulation [134]. Specifically in theAI Act (Art. 53), participants in the sandbox remain liableunder applicable liability legislation. Eligibility criteria andparticipants obligations and rights is to be set up in imple-menting acts.Derived from the above challenge, we note that sand-boxes are still far from maturity. This leads to two mainaspects that remain unresolved: 1) the design of sandboxeswith guidelines that rapidly and effectively permit algorith-mic auditing; and 2) the development of intelligent systemsfor high-risk scenarios that are validated through the nec-essary auditing processes. Important efforts are currentlydriven towards addressing these aspects as two additionalfundamental challenges. At European level, Spain is leadinga pilot to set up a regulatory sandbox according to theEuropean AI Act legislation.Together with sandboxes to work in practice, additionalfuture mechanisms will include the certification or qualitycontrol within a regulatory framework. In this sense, Spainis starting to develop a national seal of quality to certifythe security and quality of AI technology used in Spain.In cooperation with industry, they will set up the technicalcriteria for companies to obtain this seal, and develop toolsto facilitate the certification process, e.g., developing self-assessment software. Several companies will be open thepossibility to grant the seal, which will be voluntary for AIcompanies to obtain. At the international level, one efforttowards this end is the IEEE CertifAIEd program18 to as-sess ethics of Autonomous Intelligent Systems via certifica-tion guidance, assessment and independent verification. Thismark is meant for IEEE authorized assessors and certifiers18IEEE CertifAIEd: https://engagestandards.ieee.org/ieeecertifaied.html, accessed on June 6th, 2023.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 20 of 30Connecting the Dots in Trustworthy Artificial IntelligenceFigure 4: Diagram showing the role of sandboxes before (ex-ante) and after (post-hoc) the AI-based system has been deployedin the market. Sandboxes permit to evaluate the conformity of the AI-based system w.r.t. technical specifications, horizontal& vertical regulation, and ethical principles in a controlled and reliable testing environment. Once conformity has been verified,sandboxes can be used to interface with the deployed AI-based asset via the established monitoring plan, so that informationabout its post-market functioning can be collected and processed. This information is used by the national supervisory authorityto evaluate the compliance: if needed, the authority asks for corrective actions and/or reports serious incidents/a continued lackof compliance to the European Commission. Articles in the AI Act related to each step are cited in the diagram.to perform an independent review and verification to granta mark and certificate based on ontological specificationsfor Ethical Privacy, Algorithmic Bias, Transparency, andAccountability.We expect that the first experiences and results of run-ning regulatory sandboxes and their alignment with certi-fication activities will permit to learn lessons, to improveAI systems and eventually, to support the progressive pro-liferation of responsible AI systems deployed in practicalscenarios. We believe that sandbox assessment should beperiodically performed by independent and impartial assess-ment bodies to certificate and audit AI systems during theirlifetime.6.4. Practical case study in Artificial Intelligencefor healthcareAt the time of writing (April 2023), the AI Act regulationdraft is constantly being updated through different amend-ments, due in part to new versions of AI products accessibleto the general public. Concerned with how essential is thetranslation of principles and regulation into specific pro-cesses, it becomes necessary to have blueprint models andprotocols that serve to assess how trustworthy AI systemsare.The blueprint for Trustworthy AI Implementation Guid-ance and Assurance for Healthcare is one step taken in thisdirection. Figure 5 shows the proposal by the coalition forhealth AI [135], based on collaboration, guiding principlesand leadership actions. It is aligned with the AI risk man-agement framework from the National Institute of Standardsand Technology (NIST).In particular, to ensure trustworthiness this frameworkdescribes four key functions [135]:• Mapping the framing AI risks,• Measuring quantitatively and qualitatively these risks andimpacts,N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 21 of 30Post-marketmonitoring(post-hoc)Title VIII: POST-MARKET MONITORING, INFORMATION SHARING, MARKET SURVEILLANCE-Chapter1: Post-marketmonitoring-Chapter 2: Sharing of information on incidents and malfunctioning-Chapter3: EnforcementTitle X: CONFIDENTIALITY AND PENALTIESTimePre-marketmonitoring(ex ante)Title III: HIGH-RISK AI SYSTEMS-Chapter3: Obligations of Users of High-Risk AI Systems and other Parties-Chapter 4: Notifying Authorities and Notified Bodies-Chapter5: Standards, ConformityAssessment, Certificates, RegistrationDEPLOYMENTAI-based systemAudienceModelVertical regulationEthical principlesTechnical documentationSandboxHorizontal regulationCertificates(Art. 44)EU declarationofconformity(Art. 48)CE markingofconformity(Art. 49) Documentretention(Art. 50)Registration(Art. 51)EU databasefor stand-alone high-risk AI systems (Art. 60)Title VI: GovernanceChapter4: European Artificial Intelligence BoardStandards, guidance, goodpractices(Art. 58)AUDITABILITY & CONFORMITYMONITORING & ACCOUNTABILITYExpert groupsCorrective actions (Art. 21)Authorities & notifiedbodiesQuality managementsystem(Art. 17)Post-marketmonitoringplan (Art. 61)National supervisory authorityAccess to data and documentation(Art. 64)Reporting of serious incidents and of malfunctioning(Art. 62)European CommissionProcedure for dealing with AI systems presenting a risk at national level (Art. 65)AI-based systemAudience (end-users)Interaction, personal data, …Control & configurationLogsUsage statisticsPerformance metricsData collectionUnion safeguard procedure (Art. 66)Penalties (Art. 71)Market surveillance and control of AI systems in the Union market (Art. 63)Technical documentation(Art. 11)Record-keeping(Art. 12)Subcontractors of notified bodies (Art. 34)Connecting the Dots in Trustworthy Artificial Intelligenceresounds with our formulated definition of a responsibleAI system, showing that a system as such, depending onits domain of application, may require different degrees ofcompliance with the requirements for trustworthiness.The blueprint analyzed in [135] recognizes the difficul-ties on building ecosystems when multiple guidelines are leftout in the wild without a standardization consensus. It callsfor mapping socio-technical scenarios to resolve tensionsamong principles, an ISO-based approach to professionalresponsibility, and institutionalizing trustworthy AI Systems(that is, responsible AI systems).As a follow-up of the blueprint, the coalition for healthAI [135] suggests:• Setting up an engaged assurance lab and advisory serviceindependent infrastructure.• Institutionalizing trustworthy AI systems (responsible AIsystems).• Promoting a Coalition of the Willing through interestingstrategies that can be applied in health AI to drive apositive change.Other practical frameworks exist. They count with strate-gies to implement ethics and the governance of AI systems inhealth to separate the factors affecting trustworthy medicalAI into design (data and algorithm aspects) and application.This is done through controls strategies [136] at both designand application phases. First, the ethical governance systemdeparts from social needs and ethical values, which leadto ethical principles to be enforced at the research stage.After that, those principles guide the ethical norms that allowperforming risk assessment, and later make the law andregulation concrete. In particular, the framework in [137]aims at avoiding situations that can have dire consequencesfor patients. For instance, integrating the applied ethics Z-Inspection® [138] process to map and assess tensions insocio-technical scenarios in trustworthy AI. Another pro-posal [139] formulates ten commandments (i.e., high-levelethical principles) that should be met by medical AI-basedsystems. Such commandments are formulated without thetheoretical aspects underneath for the sake of an easierunderstanding and verification of all stakeholders involvedin this domain. European fundamental rights also providelegal and ethical guidelines for the adoption, developmentand application of medical AI [140]. These strategies and theblueprint are advancing the research in the area, and resultswill be extensible to other domains to attain trustworthy AI.Despite the clear establishment of the seven require-ments for HRAIs within the AI Act described in Section6.2, the particular implementation steps to be taken within aparticular area of application often remain under-specified.It becomes evident that the AI-based system is stringentlydependent on the sector of application, as well as on thecoupling of the requirements for trustworthy AI to justifyexisting regulations and standards. Therefore, for a givendomain of practice, an overarching consideration of theFigure 5: The Coalition for Health AI (CHAI) Blueprintfor Trustworthy AI implementation guidance and assurance[135].It considers obligations on reliability and testability,transparency, mitigation of biases particular to the domain,privacy, security and resilience, among other aspects.• Managing the allocation of risk resources, and a cross-cutting,• Governance via risk management.Based on these functions, they define values to set thekey elements of trustworthy AI in healthcare [135]:1. Useful: valid and reliable, testable, usable and beneficial.These values are closely linked to social wellbeing (Re-quirement 6, Subsection 5.7) and auditability (Require-ment 7, Subsection 5.8).2. Safe, which is related to technical robustness and safety(Requirement 2, Subsection 5.3).3. Accountable and transparent, with clear connectionsto accountability (Requirement 7, Subsection 5.8) andtransparency (Requirement 4, Subsection 5.5).4. Explainable and interpretable, echoing several dimen-sions of the transparency requirement.5. Fair with harmful bias managed (systemic bias, com-putational and statistical biases and human-cognitive bi-ases). The consideration of fairness and the focus onthe management of consequences of harmful biases arepresent in requirement 5 (Diversity, non-discriminationand fairness, Subsection 5.6), particularly in the avoid-ance of unfair biases. Furthermore, requirement 7 (ac-countability, Subsection 5.8) also regards the minimiza-tion of negative impacts, either due to harmful biases orto other consequences.6. Secure and resilient, which relate to the dimension oftraceability (requirement 4, Subsection 5.5) and technicalrobustness (Requirement 2, Subsection 5.3).7. Privacy-enhanced, which is coupled with requirement 3– Privacy and data governance (Subsection 5.4).It is important to underscore that some dimensions ofthe requirements for trustworthy AI discussed in Section5 are not reflected (at least, explicitly) in the above list ofvalues, e.g. environmental wellbeing or reproducibility. ThisN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 22 of 30•Collaboration•Guiding principles•LeadershipIntegrated Data Infrastructure for Discovery, Evaluation and AssuranceHealth System Preparedness & AssessmentAI Tools, Trustworthiness and Transparency across the LifecycleConnecting the Dots in Trustworthy Artificial Intelligencecomplete scenario is needed from the Trustworthy AI prac-tical point of view. The field needs to further specify legalrequirements, risk assessment tools for the ethical impactof the AI system, data privacy and data governance mod-els, ad-hoc risk management systems and conformity as-sessments, and rest of essential elements evaluated in theregulatory sandboxes testing the scenario. This may alsospur the emergence of generic trustworthy AI frameworks(regulatory sandboxes) that can be potentially adapted todifferent domains, as it is discussed in [141].6.5. Urgent needs for emerging AI systems,dynamic regulation, and evaluation protocolsThe widespread use and repercussion of the achieve-ments of emerging AI systems, such as GPAIS or neuro-science technology, have brought to the public arena thepotentials and implications of new high-risk scenarios sup-ported by these technological advances. In this section wediscuss potential issues to be tackled to regulate new HRAIsas well as future emerging AI systems. We discuss and arguethat regulation should be dynamic and malleable to establishthe boundaries of new high-risk scenarios supported bytechnological AI advances. Likewise, we also highlight theneed for flexible evaluation procedures that can be adaptedin an agile way to cope with the fast evolution of AI systems.Indeed, the rapid pace at which AI evolves over time canunexpectedly give rise to new high-risk scenarios beyondthose defined by regulation, such as the AI Act (Section6.2). This requires regulatory protocols to cope with newemerging applications. In the case of the European AI Act,on 11th May 2023, MEPs endorsed new transparency andrisk-management rules for AI systems19. MEPs expandedthe classification of high-risk areas to include those thatcould compromise or harm people’s health, safety, funda-mental rights or the environment. Such revised classificationof high-risk scenarios also considered AI systems used toinfluence voters in political campaigns, as well as recom-mender systems (with more than 45 million users) utilizedby social media platforms. Intrusive and discriminatory usesof AI-based biometric systems have been also identified asprohibited AI systems, such as:• “Real-time” remote biometric identification systems inpublicly accessible spaces;• “Post” remote biometric identification systems, with theonly exception of law enforcement for the prosecution ofserious crimes and only after judicial authorization;• Biometric categorization systems using sensitive charac-teristics (e.g. gender, race, ethnicity, citizenship status,religion, political orientation);• Predictive policing systems (based on profiling, locationor past criminal behavior);19AI Act: a step closer to the first rules on Artificial Intelligence, https://www.europarl.europa.eu/news/en/press-room/20230505IPR84904/ai-act-a-step-closer-to-the-first-rules-on-artificial-intelligence, accessedon June 6th, 2023• Emotion recognition systems in law enforcement, bordermanagement, workplace, and educational institutions; and• Indiscriminate scraping of biometric data from social me-dia or CCTV footage to create facial recognition databases(violating human rights and right to privacy).In this revision of the AI Act, MEPs have also proposedtailored regulatory regimes for new and fast-evolving devel-opments in the field of AI and GPAIS. Since GPAIS aresystems that have a wide range of possible uses withoutsubstantial modification and fine-tuning, generative foun-dation models are examples of rapidly evolving areas forwhich, if regulation is not set in place, consequences maybe hard to revert. Such systems must guarantee robust theprotection of fundamental rights, health and safety and theenvironment, democracy and rule of law. To this end, suchemerging AI systems must assess and mitigate risks, complywith design, information and environmental requirements,and be registered in the EU database. Furthermore, addi-tional transparency requirements have been demanded forgenerative foundation models such as GPT: they must informthat the content is generated by an AI model, the modelmust be designed to avoid generating illegal content andpublishing summaries or copyrighted content used duringtraining. Jurisdiction at national level will also need to beadapted to different considerations demanded by differentsectors, e.g., the public sector or labor sector.Another area in which regulation and technology yethave to advance is in copyright management of generatedartwork produced by fundation models. Although the AIAct requires to disclose the use of copyrighted material inthe training data, there is no current way to detect when AIgenerated content may be directly related to existing contentprotected by copyright, nor it is clear who owns the intellec-tual property of generative models outputs [142, 143].Besides GPAIS, other emerging AI-based technologiesalso require specialized adjustments of ongoing regulatoryefforts. This is the case of neurotechnology, such as braininterfaces. The needs to handle novel applications neverused before become evident by recent research [144] thatshows the potential of “mind-reading” [145]. For instance,the study in [144] shows the potential of leveraging lan-guage models as an autoregressive prior to generate novelsequences that can decode structured sequential informationin the form of text from brain signals. Although the study ofhuman imagination decoding shows human cooperation isrequired for the approach to work, this may not be a requisitein the future. Even if decoding is not accurate yet, thesesystems could be used maliciously.These recent results attained by neurotechnology call forraising awareness about the risks posed by brain decodingtechnology, and for the design of regulation and policies topreserve fundamental rights such as mental privacy. A rolemodel in this direction is the novel neurorights regulationN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 23 of 30Connecting the Dots in Trustworthy Artificial Intelligencepioneered by Chile20. Their neuroprotection agenda (whichis closely followed up by Spain) leads the way to the reg-ulation of brain technology and legislation of advances inAI-supported medicine and science from a human rights’point of view. This is of utmost importance to avoid mentalmanipulation, as mental diseases are the grand pandemicof XXI century. If used appropriately, AI based technologyapplied to the diagnosis and treatment of mental diseases hasa great potential to improve patients’ quality of life.In order for regulation to evolve synchronously withtechnological advances (and vice versa), evaluation proce-dures must be flexible and dynamically adaptable to accom-modate the implications of these advances over time. Morecomprehensive evaluation tools are required to be adoptedby the AI community of practitioners and users if we aspireto synergistic solutions that can complement governmentalefforts. In particular, ML and AI model evaluation is nor-mally based on leader-board benchmarks that do not alwaysreflect reality, and may have a detrimental effect when theyare not faithful to reality. A more accountable evaluationshould consider aggregated metrics. Improperly constructedbenchmarks may, for instance, reflect unrealistic overesti-mation of the capabilities of a model when predicting overminority classes. This may lead to hazards that end upobfuscating the real benefits of AI.To avoid these issues, important guidelines for robustevaluation practices [146] include:1. Granular performance reporting protocols with break-downs across the features that have demonstrated affect-ing performance.2. Designing benchmarks to test capabilities and to signif-icantly vary on important features of the problem spaceand labeling instances to allow for granular analyses (e.g.as the Holistic Evaluation of Language Models bench-mark [147]).3. Record all results, successful or failing, partial or fullytraced, in supplementary material or public repositoriesfor each run and validation split separately (e.g., in med-ical AI [148]).4. Enable researchers follow-up instance-level analyses byincluding data labels and annotations of those instances.However, prospective evaluation methodologies as theone described above should be versatile and extensible toembrace and incorporate new performance metrics, evalu-ation protocols or even modeling tasks proposed along theyears. But most importantly: their sought flexibility shouldnot give rise to exceptional cases that would undermine thevalidity and applicability of regulations in force.We conclude that given the fast pace at which AI isprogressing in the last months, it is of paramount importanceto have a dynamic regulation from a double perspective:20NeuroRights Foundation, https://neurorightsfoundation.org/,accessedonJune06th,2023., which has taken a step towards the first Neurorightslaw in this countrythe appearance of risk-based scenarios and the emergenceof novel AI systems. Only in this way the regulation willfacilitate the realization of responsible AI systems, in par-allel to the development of methodologies for algorithmicauditing and the clearance of responsibilities in the use ofsuch systems.7. From the Artificial Intelligence moratoriumletter to regulation as the key for consensusAt the time of writing, a global debate is held aroundthe moratorium letter published by several renowned re-searchers calling for a pause in large AI experimentation21.The letter can be interpreted as a contribution to pointingout the gap between the fast advance of high-powered AIsystems and the regulation. The letter also highlights that:“AI research and development should be refo-cused on making today’s powerful, state-of-the-art systems more accurate, safe, interpretable,transparent, robust, aligned, trustworthy, andloyal.”Following up this moratorium letter, several declarationsand written statements by reputed experts have been pub-lished to approach the AI conundrum between ethics, regula-tion and technological progress from different perspectives.Among them, we highlight the interview with G. Hinton22,in which he states that “We need to find a way to controlartificial intelligence before it’s too late”. Interestingly un-der the scope of this work, he has also underscored thedifferent nature of intelligent systems when compared tohuman intelligence, and thereby the need for establishingregulation for these artificial systems:“Our brains are the result of evolution andhave a series of integrated goals — such as nothurting the body, hence the notion of damage;eating enough, hence the notion of hunger. Mak-ing as many copies of ourselves as possible,hence the sexual desire. Synthetic intelligence,on the other hand, hasn’t evolved: we’ve builtit. Therefore, it doesn’t necessarily come withinnate goals. So, the big question is, can wemake sure that AI has goals that benefit us?This is the so-called alignment problem. And wehave several reasons to be very concerned.”A similar line of thinking has been expressed by Harari23,emphasizing on the pressing immediacy at which regulation21Future of Life Institute, Pause giant AI experiments: An open letter,https://futureoflife.org/open- letter/pause- giant- ai- experiments/,accessed on April 25th, 202322Geoffrey Hinton: "We need to find a way to control artificial intelli-gence before it’s too late", https://english.elpais.com/science-tech/2023-05-12/geoffrey-hinton-we-need-to-find-a-way-to-control-artificial-intelligence-before-its-too-late.html, accessed on June 4th, 202323Yuval Noah Harari argues that AI has hacked the operating system ofhuman civilisation, https://www.economist.com/by-invitation/2023/04/28/yuval-noah-harari-argues-that-ai-has-hacked-the-operating-system-of-human-civilisation, accessed on June 4th, 2023.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 24 of 30Connecting the Dots in Trustworthy Artificial Intelligenceis needed to match the speed of AI technological develop-ment with the public use of AI systems:“We can still regulate the new AI tools, butwe must act quickly. Whereas nukes cannotinvent more powerful nukes, AI can make ex-ponentially more powerful AI. The first crucialstep is to demand rigorous safety checks beforepowerful AI tools are released into the pub-lic domain. Just as a pharmaceutical companycannot release new drugs before testing boththeir short-term and long-term side-effects, sotech companies shouldn’t release new AI toolsbefore they are made safe. We need an equiv-alent of the Food and Drug Administration fornew technology, and we need it yesterday.”Another example is the proposal made by Sam Altman(OpenAI co-founder) before the US Senate to defend thebenefits of this revolutionary technology, claiming that AIregulation should ensure that the public has access to itsmany advantages24:“[...] we are not alone in developing this tech-nology. It will be important for policymakersto consider how to implement licensing regula-tions on a global scale and ensure internationalcooperation on AI safety, including examiningpotential intergovernmental oversight mecha-nisms and standard-setting.”Finally, the manifesto on AI risk supported by multiplescientists and notable figures in the AI landscape has claimedto elevate the mitigation of AI risks to the priority levels ofother humanity-threatening fatalities25:“Mitigating the risk of extinction from AI shouldbe a global priority alongside other societal-scale risks such as pandemics and nuclearwar.”The controversy held around these and other statementsis whether humanity is close to or far from the moment atwhich AI can pose a realistic threat to its own existence. Un-questionably, triggering this debate has ignited even furtherthe need for ethical and regulatory frameworks that regulatewhether and how AI-based systems can be trusted and usedin practical setups.After this latter manifesto, the Center for AI Safetyhas just published a new document entitled Existing policyproposals aimed at present and future harms’26. The aim ofthis one-page document is to describe three proposals that, inour view, promote AI safety. We follow with a short analysisof these proposals:24Written Testimony of Sam Altman Chief Executive Officer OpenAI,https://www.washingtonpost.com/documents/0668f6f4-d957-4b94-a745-2aa9617d1d60.pdf?itid=lk_inline_manual_18, accessed on June 4th, 2023.25Statement on AI Risk, https://www.safe.ai/statement-on-ai-risk,accessed on June 04th, 2023.26Existing Policy Proposals Targeting Present and Future Harms, https://https://www.safe.ai/post/three- policy- proposals- for- ai- safety,accessed on June 07th, 2023.• Legal Liability for AI harms: The first issue highlighted inthe document is the need for establishing improved legalliability frameworks for the accountability of damagescaused by the application of AI systems. GPAIS are alsoreferred in the document for the ill-advised implemen-tation of legal exemptions to absolve GPAIS developersof liability, as such exemptions could unfairly shift theentire burden of responsibility from large corporationsto smaller actors, users and communities lacking thenecessary resources, access, and capabilities to effectivelyaddress and alleviate all risks.• Increased regulatory scrutiny: The second problem em-phasized in this document is the need for a greater regu-latory inspection during the development of AI systems,extending beyond the application layer to encompass theentire product lifecycle. It underscores the importance ofholding companies responsible for the data and designchoices they make when developing these models. In linewith this proposal, increased transparency and regula-tions over training data are crucial to address algorithmicbias effectively, and to prevent companies from unfairlyleveraging copyrighted materials through data modelingwithout compensating their creators.• Human supervision of automated systems: The thirdtheme in the document is the importance of human over-sight in the implementation of HRAIs. Human oversightcan contribute to lessening potential concerns with biasand the propagation of false or misleading informationthrough AI systems. An explicit reference is done to theEU’s regulatory proposal, with a positive emphasis on theimportance therein granted to the human oversight in thedeployment of HRAIs.Our position, as we put it in this manuscript, is that"regulation is a key for consensus" among these divergingvoices to cast light over the shadows of modern AI technolo-gies. For this to occur, technologies, methodologies and toolssupporting the development, auditability and accountabilityof responsible AI systems are of utmost importance to copewith high-risk scenarios and to meet regulatory constraints.To finish this section, we pay attention to a final pointmade by the authors of the paper [15]. Unfortunately, thisroad towards consensus is not exempt of their own risks.Indeed, conflating trust and trustworthiness with the accept-ability of risks blurs the distinction between acceptabilityjudgments made by domain experts and the trustworthinessof AI systems implemented in society [15]. It has beenargued that trust is improbable to be produced on demandand impossible on command, as “trust engineering” maybackfire and not achieve its goal. Focused on trust andtrustworthiness in AI in the public sector, [15] argues onthe four acute challenges facing the European Commission’sattempt to signal the trustworthiness of AI through its pro-posed regulation: the uncertainty about the antecedents ofperceived trust in public institutions that utilize AI; the threatof misalignment between trustworthiness and degrees ofN. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 25 of 30Connecting the Dots in Trustworthy Artificial Intelligencetrust; concealed behavioral factors behind the acceptabilityof risks; and the need for impartial intermediaries.Despite these and other curves in the road, regulation canbe an unquestionable driving force to consolidate and putall these diverging voices on the same page. Regulation hasfavored consensus about the benefits and restrictions of tech-nological advances that have evolved faster than expected,permeating quickly into the society (e.g., social networks,Internet or mobile communications). AI should not be anexception. There is still a long way to go before we havefully aligned AI technology and regulation, developing re-sponsible AI systems adapted to each risk scenario and fullyleveraging the latest advances in the field. For this to occur,the European regulatory model based on risk-based use casescenarios can serve as a guiding light for the maturity andimplementation of ethical, legal and technical frameworks,fostering the creation of industrial and institutional instru-ments (e.g. AI sandboxes or AI ethics board [149]) thatguarantee that AI-based products and services comply withtheir requirements.8. Concluding remarksFor years now, the ever-growing capabilities of AI-powered systems have stimulated debates about the impact,benefits, implications and risks brought by AI systems tothe industry and society. The ground-breaking potential oflarge generative AI models such as ChatGPT and GPT4 hasreinvigorated this debate, since their near general-purposecapabilities learned from multimodal data can support awide variety of intended and unintended purposes andtasks, by generating content that is hardly distinguishablefrom that made by humans. This notorious advance hasreinvigorated the relevance and momentum of trustworthyAI systems, particularly in what refers to 1) the ethical usageof these models, and 2) the need for regulatory directives thatestablish what, when and how AI systems can be adopted inpractical applications.In this context, this manuscript has shed light on theprinciples, pillars and requirements to be met by trustworthyAI systems to be considered as such. To this end, we have de-parted from mature regulation/supervisory frameworks de-veloped around trustworthy AI (e.g. AI Act) to provide cleardefinitions of all related concepts, placing emphasis on whateach requirement for trustworthiness in AI stands for, whythey contribute to generating trust in the user of an AI-basedsystem, and how such requirements can be met technically.Regarding the latter, a short tour over technological areasthat can contribute to each of these requirements has beenoffered. Our study has also overviewed ethical principles forthe development of AI, which establish an overarching setof recommendations that ensure that this discipline will beadvanced under social and ethical standards. The study hasbeen complemented by a discussion on practical aspects tobe considered in the design, development and use of trust-worthy AI systems, stressing on the importance of assessingtheir conformity to regulations (auditability) and explaininghow their decisions are issued (accountability). These twopractical aspects must be met by responsible AI systems.Further along this line, accountability and explainabilityhave permeated deeply into the recommendations recentlyissued for the development of trustworthy medical AI, a risk-critical sector in large demand for trust when embracing newtechnological advances. Our analysis of such recommenda-tions has exposed that auditability and accountability are atthe core of the guidelines proposed in this area; togetherwith ethics, data governance and transparency. Medical AIexemplifies the paramount relevance of considering all theserequirements for trustworthiness along the entire AI cycle.For a given domain of practice, we need to assess thecomplete scenario from the Trustworthy AI practicalpoint of view, that is, all essential elements auditedin regulatory sandboxes for scenario testing, togetherwith clear accountability protocols. Above all, thedevelopment of responsible AI systems as the finaloutput of the chain is essential and must be the goalfor current AI designs and developments.In summary, we hope that this paper serves as a referencefor researchers, practitioners and neophytes who are new tothe world of AI, with interest in trustworthy AI from a holis-tic perspective. A well-rounded analysis of what trust meansin AI-based systems and its requirements as the one offeredin this manuscript is a key for the design and developmentof responsible AI systems throughout their life cycle. Weshould not regulate scientific progress, but rather productsand its usage. As we emphasize in this paper, regulationis the key for consensus, and for this purpose, trustworthyAI and responsible AI systems for high risk scenarios areimperative, as they will contribute to the convergence be-tween technology and regulation, the advance of science,the prosperity of our economies, and the good of humanity,subject to legal requirements and ethical principles.9. AcknowledgmentsN. Díaz-Rodríguez is currently supported by a MarieSkłodowska-Curie Actions (MSCA) Postdoctoral Fellow-ship with agreement ID: 101059332 and the LeonardoScholarship for Researchers and Cultural Creators 2022from the BBVA Foundation. J. Del Ser has received fundingsupport from the Spanish Centro para el Desarrollo Tec-nológico Industrial (CDTI) through the AI4ES project, andfrom the Basque Government (Eusko Jaurlaritza) throughthe Consolidated Research Group MATHMODE (IT1456-22). F. Herrera has received funding support from theSpanish Ministry of Science and Innovation (grant PID2020-119478GB-I00).Declaration of competing interestThe authors declare that they have no known competingfinancial interests or personal relationships that could haveappeared to influence the work reported in this paper.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 26 of 30Connecting the Dots in Trustworthy Artificial IntelligenceReferences[1] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford,M. Chen, I. Sutskever, Zero-shot text-to-image generation, in: Inter-national Conference on Machine Learning, PMLR, 2021, pp. 8821–8831.[2] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans,J. Ho, D. J. Fleet, M. Norouzi, Photorealistic text-to-image diffusionmodels with deep language understanding, in: S. Koyejo, S. Mo-hamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh (Eds.), Advances inNeural Information Processing Systems, Vol. 35, Curran Associates,Inc., 2022, pp. 36479–36494.URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ec795aeadae0b7d230fa35cbaf04c041-Paper-Conference.pdf[3] European Commission High-Level Expert Group on AI, Ethicsguidelines for trustworthy AI (2019).[4] European Union, Proposal for a Regulation of the European Parlia-ment and of the Council Laying down harmonised rules on ArtificialIntelligence (Artificial Intelligence Act) and amending certain UnionLegislative Acts. COM/2021/206 final (2021).[5] UNESCO, Recommendation on the ethics of artificial intelligence,Digital Library UNESDOC (2020).URL en.unesco.org[6] R. Benjamins, A. Barbado, D. Sierra, Responsible AI by design inpractice, in: Proceedings of the Human-Centered AI: Trustworthi-ness of AI Models & Data (HAI) track at AAAI Fall Symposium,2019.[7] G. Pisoni, N. Díaz-Rodríguez, H. Gijlers, L. Tonolli, Human-centered artificial intelligence for designing accessible cultural her-itage, Applied Sciences 11 (2) (2021) 870.[8] B. C. Stahl, D. Wright, Ethics and privacy in AI and big data:Implementing responsible research and innovation, IEEE Security& Privacy 16 (3) (2018) 26–33.[9] M. Coeckelbergh, AI ethics, MIT Press, 2020.[10] M. Coeckelbergh, Artificial intelligence, responsibility attribution,and a relational justification of explainability, Science and engineer-ing ethics 26 (4) (2020) 2051–2068.[11] W. Wahlster, C. Winterhalter, German standardization roadmap onartificial intelligence, DIN/DKE, Berlin/Frankfurt (2020) 100.[12] L. Edwards, The EU AI Act: a summary of its significance and scope,Ada Lovelace Institute, Expert explainer Report (2022) 26.[13] S. Campos, R. Laurent, A Definition of General-Purpose AI Sys-tems: Mitigating Risks from the Most Generally Capable Models,Available at SSRN 4423706 (2023).[14] M. Estévez Almenzar, D. Fernández Llorca, E. Gómez, F. Mar-tinez Plumed, Glossary of human-centric artificial intelligence,Tech. Rep. JRC129614, Joint Research Centre (2022).[15] J. Laux, S. Wachter, B. Mittelstadt, Trustworthy artificial intelli-gence and the European Union AI act: On the conflation of trustwor-thiness and acceptability of risk, Regulation & Governance n/a (n/a).arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/rego.12512, doi:https://doi.org/10.1111/rego.12512.URL https://onlinelibrary.wiley.com/doi/abs/10.1111/rego.12512[16] E. Tjoa, C. Guan, A survey on explainable artificial intelligence(XAI): Toward medical XAI, IEEE Transactions on Neural Net-works and Learning Systems 32 (11) (2020) 4793–4813.[17] D. Doran, S. Schulz, T. R. Besold, What does explainable AI reallymean? A new conceptualization of perspectives, arXiv preprintarXiv:1710.00794 (2017).[18] Z. C. Lipton, The mythos of model interpretability: In machine learn-ing, the concept of interpretability is both important and slippery,Queue 16 (3) (2018) 31–57.[19] European Commission High-Level Expert Group on AI, The Assess-ment List for Trustworthy Artificial Intelligence (ALTAI) for selfassessment (2020).[20] C. Widmer, M. K. Sarker, S. Nadella, J. Fiechter, I. Juvina,B. Minnery, P. Hitzler, J. Schwartz, M. Raymer, Towards Human-Compatible XAI: Explaining Data Differentials with Concept Induc-tion over Background Knowledge, arXiv preprint arXiv:2209.13710(2022).[21] B. Lepri, N. Oliver, A. Pentland, Ethical machines: the human-centric use of artificial intelligence, Iscience (2021) 102249.[22] G. Pisoni, N. Díaz-Rodríguez, Responsible and human centric AI-based insurance advisors, Information Processing & Management60 (3) (2023) 103273.[23] N. Tomašev, J. Cornebise, F. Hutter, S. Mohamed, A. Picciariello,B. Connelly, D. C. Belgrave, D. Ezer, F. C. v. d. Haert, F. Mugisha,et al., Ai for social good: unlocking the opportunity for positiveimpact, Nature Communications 11 (1) (2020) 2468.[24] A. Holzinger, Interactive machine learning for health informatics:when do we need the human-in-the-loop?, Brain Informatics 3 (2)(2016) 119–131.[25] World Economic Forum, Empowering AI leadership an oversighttoolkit for boards of directors, Tech. rep. (2019).[26] World Economic Forum, Empowering AI Leadership: AI C-SuiteToolkit , Tech. rep. (2022).[27] E. Cambria, L. Malandri, F. Mercorio, M. Mezzanzanica, N. Nobani,A survey on XAI and natural language explanations, InformationProcessing & Management 60 (1) (2023) 103111.[28] L. Floridi, Establishing the rules for building trustworthy AI, NatureMachine Intelligence 1 (6) (2019) 261–262.[29] R. Mariani, F. Rossi, R. Cucchiara, M. Pavone, B. Simkin, A. Koene,J. Papenbrock, Trustworthy AI – Part 1, Computer 56 (2) (2023) 14–18.[30] P.-Y. Chen, P. Das, AI Maintenance: A Robustness Perspective,Computer 56 (2) (2023) 48–56.[31] K. R. Varshney, Trustworthy machine learning and artificial intelli-gence, XRDS: Crossroads, The ACM Magazine for Students 25 (3)(2019) 26–29.[32] J. Yang, K. Zhou, Y. Li, Z. Liu, Generalized out-of-distributiondetection: A survey, arXiv preprint arXiv:2110.11334 (2021).[33] A. Ruospo, E. Sanchez, L. M. Luza, L. Dilillo, M. Traiola, A. Bosio,A survey on deep learning resilience assessment methodologies,Computer 56 (2) (2023) 57–66.[34] S. Speakman, G. A. Tadesse, C. Cintas, W. Ogallo, T. Akumu,A. Oshingbesan, Detecting systematic deviations in data and models,Computer 56 (2) (2023) 82–92.[35] T. Lesort, V. Lomonaco, A. Stoian, D. Maltoni, D. Filliat, N. Díaz-Rodríguez, Continual learning for robotics: Definition, framework,learning strategies, opportunities and challenges, Information fusion58 (2020) 52–68.[36] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu,M. Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya,et al., A review of uncertainty quantification in deep learning: Tech-niques, applications and challenges, Information Fusion 76 (2021)243–297.[37] J. Parmar, S. Chouhan, V. Raychoudhury, S. Rathore, Open-worldmachine learning: applications, challenges, and opportunities, ACMComputing Surveys 55 (10) (2023) 1–37.[38] R. S. Zimmermann, W. Brendel, F. Tramer, N. Carlini, Increasingconfidence in adversarial robustness evaluations, in: A. H. Oh,A. Agarwal, D. Belgrave, K. Cho (Eds.), Advances in Neural Infor-mation Processing Systems, 2022.URL https://openreview.net/forum?id=NkK4i91VWp[39] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schul-man, D. Mané, Concrete problems in AI safety, arXiv preprintarXiv:1606.06565 (2016).[40] D. Hendrycks, N. Carlini, J. Schulman, J. Steinhardt, Unsolvedproblems in ml safety, arXiv preprint arXiv:2109.13916 (2021).[41] S. Mohseni, H. Wang, C. Xiao, Z. Yu, Z. Wang, J. Yadawa, Tax-onomy of machine learning safety: A survey and primer, ACMComputing Surveys 55 (8) (2022) 1–38.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 27 of 30Connecting the Dots in Trustworthy Artificial Intelligence[42] T. Gu, K. Liu, B. Dolan-Gavitt, S. Garg, Badnets: Evaluating back-dooring attacks on deep neural networks, IEEE Access 7 (2019)47230–47244.[43] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song,J. Steinhardt, Aligning AI with shared human values, Proceedingsof the International Conference on Learning Representations (ICLR)(2021).[60] N. F. Rajani, B. McCann, C. Xiong, R. Socher, Explain yourself!leveraging language models for commonsense reasoning, in: Pro-ceedings of the 57th Annual Meeting of the Association for Com-putational Linguistics, Association for Computational Linguistics,Florence, Italy, 2019, pp. 4932–4942. doi:10.18653/v1/P19-1487.URL https://aclanthology.org/P19-1487[61] K. Abhishek, D. Kamath, Attribution-based xai methods in com-[44] C. O’neil, Weapons of math destruction: How big data increasesputer vision: A review, arXiv preprint arXiv:2211.14736 (2022).inequality and threatens democracy, Crown, 2017.[45] R. B. Parikh, S. Teeple, A. S. Navathe, Addressing bias in artificialintelligence in health care, Jama 322 (24) (2019) 2377–2378.[46] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,V. Ivanov, C. Kiddon, J. Konečn`y, S. Mazzocchi, B. McMahan, et al.,Towards federated learning at scale: System design, Proceedings ofMachine Learning and Systems 1 (2019) 374–388.[47] N. Rodríguez-Barroso, G. Stipcich, D. Jiménez-López, J. A. Ruiz-Millán, E. Martínez-Cámara, G. González-Seco, M. V. Luzón, M. A.Veganzones, F. Herrera, Federated learning and differential privacy:Software tools analysis, the Sherpa.ai FL framework and method-ological guidelines for preserving data privacy, Information Fusion64 (2020) 270–292.[48] C. Marcolla, V. Sucasas, M. Manzano, R. Bassoli, F. H. Fitzek,N. Aaraj, Survey on fully homomorphic encryption, theory, andapplications, Proceedings of the IEEE 110 (10) (2022) 1572–1609.[49] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,K. Talwar, L. Zhang, Deep learning with differential privacy, in:Proceedings of the 2016 ACM SIGSAC Conference on Computerand Communications Security, 2016, pp. 308–318.[50] Public Voice coalition, Universal Guidelines for Artificial Intelli-gence, https://thepublicvoice.org/ai- universal- guidelines/,online [accessed April 20th, 2023] (2018).[51] Information Commissioner’s Office (ICO), How to use AI andpersonal data appropriately and lawfully, https://ico.org.uk/media/for-organisations/documents/4022261/how-to-use-ai-and-personal-data.pdf, online [accessed April 20th, 2023] (2022).[52] E. Union, Regulation (EU) 2022/868 of the European Parliamentand of the Council of 30 May 2022 on European data governanceand amending Regulation (EU) 2018/1724 (Data Governance Act)(2022).[53] E. Union, Proposal for a REGULATION OF THE EUROPEANPARLIAMENT AND OF THE COUNCIL on harmonised rules onfair access to and use of data (Data Act) (2022).[54] A. Barredo Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot,S. Tabik, A. Barbado, S. García, S. Gil-López, D. Molina, R. Ben-jamins, et al., Explainable Artificial Intelligence (XAI): Concepts,taxonomies, opportunities and challenges toward responsible AI,Information Fusion 58 (2020) 82–115.[55] K. Haresamudram, S. Larsson, F. Heintz, Three levels of AI trans-parency, Computer 56 (2) (2023) 93–100.[56] B. Pérez, J. Rubio, C. Sáenz-Adán, A systematic review of prove-nance systems, Knowledge and Information Systems 57 (2018) 495–543.[57] A. Holzinger, M. Dehmer, F. Emmert-Streib, R. Cucchiara, I. Au-genstein, J. Del Ser, W. Samek, I. Jurisica, N. Díaz-Rodríguez,Information fusion as an integrative cross-cutting enabler to achieverobust, explainable, and trustworthy medical artificial intelligence,Information Fusion 79 (2022) 263–278.[58] S. Ali, T. Abuhmed, S. El-Sappagh, K. Muhammad, J. M. Alonso-Moral, R. Confalonieri, R. Guidotti, J. Del Ser, N. Díaz-Rodríguez,F. Herrera, Explainable Artificial Intelligence (XAI): What we knowand what is left to attain Trustworthy Artificial Intelligence, Infor-mation Fusion (2023) 101805.[59] M. T. Ribeiro, S. Singh, C. Guestrin, "Why should I trust you?"Explaining the predictions of any classifier, in: Proceedings ofthe 22nd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, 2016, pp. 1135–1144.[62] R. Guidotti, A. Monreale, F. Giannotti, D. Pedreschi, S. Ruggieri,F. Turini, Factual and counterfactual explanations for black boxdecision making, IEEE Intelligent Systems 34 (6) (2019) 14–23.[63] J. van der Waa, E. Nieuwburg, A. Cremers, M. Neerincx, EvaluatingXAI: A comparison of rule-based and example-based explanations,Artificial Intelligence 291 (2021) 103404.[64] K. Kaczmarek-Majer, G. Casalino, G. Castellano, M. Dominiak,O. Hryniewicz, O. Kamińska, G. Vessio, N. Díaz-Rodríguez, Ple-nary: Explaining black-box models in natural language throughfuzzy linguistic summaries, Information Sciences 614 (2022) 374–399.[65] V. Bourgeais, F. Zehraoui, B. Hanczar, GraphGONet: a self-explaining neural network encapsulating the Gene Ontology graphfor phenotype prediction on gene expression, Bioinformatics 38 (9)(2022) 2504–2511.[66] N. Díaz-Rodríguez, A. Lamas, J. Sanchez, G. Franchi, I. Donadello,S. Tabik, D. Filliat, P. Cruz, R. Montes, F. Herrera, EXplainableNeural-Symbolic Learning (X-NeSyL) methodology to fuse deeplearning representations with expert knowledge graphs: The Mon-uMAI cultural heritage use case, Information Fusion 79 (2022) 58–83.[67] L. Salewski, A. Koepke, H. Lensch, Z. Akata, CLEVR-X: A VisualReasoning Dataset for Natural Language Explanations, in: Interna-tional Workshop on Extending Explainable AI Beyond Deep Modelsand Classifiers, Springer, 2022, pp. 69–88.[68] G. Vilone, L. Longo, Notions of explainability and evaluation ap-proaches for explainable artificial intelligence, Information Fusion76 (2021) 89–106.[69] I. Sevillano-Garcia, J. Luengo, F. Herrera, REVEL framework tomeasure local linear explanations for black-box models: Deep learn-ing image classification case study, International Journal of Intelli-gent Systems 2023 (2023) 8068569.[70] I. Hupont, C. Fernández, Demogpairs: Quantifying the impact ofdemographic imbalance in deep face recognition, in: 14th IEEEInternational Conference on Automatic Face & Gesture Recognition(FG 2019), IEEE, 2019, pp. 1–7.[71] M.-P. Fernando, F. Cèsar, N. David, H.-O. José, Missing the missingvalues: The ugly duckling of fairness in machine learning, Interna-tional Journal of Intelligent Systems 36 (7) (2021) 3217–3258.[72] A. H. Gee, D. Garcia-Olano, J. Ghosh, D. Paydarfar, Explaining deepclassification of time-series data with learned prototypes, in: CEURworkshop proceedings, Vol. 2429, NIH Public Access, 2019, p. 15.[73] A. Cully, Y. Demiris, Quality and diversity optimization: A unifyingmodular framework, IEEE Transactions on Evolutionary Computa-tion 22 (2) (2017) 245–259.[74] S. Hajian, F. Bonchi, C. Castillo, Algorithmic bias: From discrim-ination discovery to fairness-aware data mining, in: Proceedings ofthe 22nd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, 2016, pp. 2125–2126.[75] D. Pedreshi, S. Ruggieri, F. Turini, Discrimination-aware data min-ing, in: Proceedings of the 14th ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining, 2008, pp. 560–568.[76] N. Díaz-Rodríguez, G. Pisoni, Accessible cultural heritage throughexplainable artificial intelligence, in: Adjunct Publication of the 28thACM Conference on User Modeling, Adaptation and Personaliza-tion, 2020, pp. 317–324.[77] B. Shneiderman, Human-centered AI, Oxford University Press,2022.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 28 of 30Connecting the Dots in Trustworthy Artificial Intelligence[78] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, A. Galstyan, Asurvey on bias and fairness in machine learning, ACM ComputingSurveys (CSUR) 54 (6) (2021) 1–35.[79] X. Gu, Z. Tianqing, J. Li, T. Zhang, W. Ren, K.-K. R. Choo, Pri-vacy, accuracy, and model fairness trade-offs in federated learning,Computers & Security 122 (2022) 102907.[80] M. Du, R. Tang, W. Fu, X. Hu, Towards debiasing DNN models fromspurious feature influence, in: Proceedings of the AAAI Conferenceon Artificial Intelligence, Vol. 36, 2022, pp. 9521–9528.[81] B. H. Zhang, B. Lemoine, M. Mitchell, Mitigating unwanted biaseswith adversarial learning, in: Proceedings of the 2018 AAAI/ACMConference on AI, Ethics, and Society, 2018, pp. 335–340.[82] U. Aïvodji, H. Arai, O. Fortineau, S. Gambs, S. Hara, A. Tapp,Fairwashing: the risk of rationalization, in: International Conferenceon Machine Learning, PMLR, 2019, pp. 161–170.[83] U. Aïvodji, H. Arai, S. Gambs, S. Hara, Characterizing the risk offairwashing, Advances in Neural Information Processing Systems 34(2021) 14822–14834.[84] R. Baeza-Yates, Bias on the web, Communications of the ACM61 (6) (2018) 54–61.[85] A. Balayn, C. Lofi, G.-J. Houben, Managing bias and unfairness indata for decision support: a survey of machine learning and dataengineering approaches to identify and mitigate bias and unfairnesswithin data management and analytics systems, The VLDB Journal30 (5) (2021) 739–768.[86] J. Silberg, J. Manyika, Notes from the AI frontier: Tackling bias inAI (and in humans), McKinsey Global Institute 1 (6) (2019).[87] G. Smith, I. Rustagi, Mitigating Bias in Artificial Intelligence,An Equity Fluent Leadership Playbook, Berkeley Haas Center forEquity, Gender and Leadership (2020).URL https://haas.berkeley.edu/wp-content/uploads/UCB_Playbook_R10_V2_spreads2.pdf[88] A. Gulati, M. A. Lozano, B. Lepri, N. Oliver, BIASeD: BringingIrrationality into Automated System Design, in: Proceedings of theThinking Fast and Slow and Other Cognitive Theories in AI (inAAAI 2022 Fall Symposium), Vol. 3332, 2022.[89] H. Suresh, J. Guttag, A framework for understanding sources of harmthroughout the machine learning life cycle, in: Equity and access inalgorithms, mechanisms, and optimization, 2021, pp. 1–9.[90] S. Barocas, M. Hardt, A. Narayanan, Fairness and Machine Learn-ing: Limitations and Opportunities, fairmlbook.org, 2019, h t tp ://www.fairmlbook.org.[91] J. Pearl, D. Mackenzie, The Book of Why, Basic Books, 2018.[92] N. Díaz-Rodríguez, R. Binkyt˙e, W. Bakkali, S. Bookseller,P. Tubaro, A. Bacevičius, S. Zhioua, R. Chatila, Gender and sex biasin COVID-19 epidemiological data through the lenses of causality,Information Processing & Management 60 (3) (2023) 103276. doi:https://doi.org/10.1016/j.ipm.2023.103276.URL https://www.sciencedirect.com/science/article/pii/S0306457323000134[93] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski, A. La-coste, K. Sankaran, A. S. Ross, N. Milojevic-Dupont, N. Jaques,A. Waldman-Brown, et al., Tackling climate change with machinelearning, ACM Computing Surveys (CSUR) 55 (2) (2022) 1–96.[94] S. Salcedo-Sanz, J. Pérez-Aracil, G. Ascenso, J. Del Ser, D. Casillas-Pérez, C. Kadow, D. Fister, D. Barriopedro, R. García-Herrera,M. Restelli, et al., Analysis, characterization, prediction and attribu-tion of extreme atmospheric events with machine learning: a review,arXiv preprint arXiv:2207.07580 (2022).[95] J. Cowls, A. Tsamados, M. Taddeo, L. Floridi, The AI gambit:leveraging artificial intelligence to combat climate change – opportu-nities, challenges, and recommendations, AI & Society (2021) 1–25.[96] K. Hao, Training a single AI model can emit as much carbon as fivecars in their lifetimes, MIT technology Review 75 (2019) 103.[97] E. Strubell, A. Ganesh, A. McCallum, Energy and policy consid-erations for deep learning in NLP, in: Proceedings of the 57thAnnual Meeting of the Association for Computational Linguistics,Association for Computational Linguistics, Florence, Italy, 2019, pp.3645–3650. doi:10.18653/v1/P19-1355.URL https://aclanthology.org/P19-1355[98] D. Patterson, J. Gonzalez, U. Hölzle, Q. Le, C. Liang, L.-M.Munguia, D. Rothchild, D. R. So, M. Texier, J. Dean, The carbonfootprint of machine learning training will plateau, then shrink,Computer 55 (7) (2022) 18–28.[99] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani,K. Maeng, G. Chang, F. Aga, J. Huang, C. Bai, et al., SustainableAI: Environmental implications, challenges and opportunities, Pro-ceedings of Machine Learning and Systems 4 (2022) 795–813.[100] R. Schwartz, J. Dodge, N. A. Smith, O. Etzioni, Green AI, Commu-nications of the ACM 63 (12) (2020) 54–63.[101] A. Lacoste, A. Luccioni, V. Schmidt, T. Dandres, Quantify-ing the carbon emissions of machine learning, arXiv preprintarXiv:1910.09700 (2019).[102] J. Maillo, I. Triguero, F. Herrera, Redundancy and complexitymetrics for big data classification: Towards smart data, IEEE Access8 (2020) 87918–87928.[103] G. C. Marinó, A. Petrini, D. Malchiodi, M. Frasca, Deep neuralnetworks compression: A comparative survey and choice recom-mendations, Neurocomputing 520 (2023) 152–170.[104] R. Mishra, H. P. Gupta, T. Dutta, A survey on deep neural networkcompression: Challenges, overview, and solutions, arXiv preprintarXiv:2010.03954 (2020).[105] D. Becking, M. Dreyer, W. Samek, K. Müller, S. Lapuschkin, ECQ:Explainability-Driven Quantization for Low-Bit and Sparse DNNs,in: International Workshop on Extending Explainable AI BeyondDeep Models and Classifiers, Springer, 2022, pp. 271–296.[106] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neuralnetwork, arXiv preprint arXiv:1503.02531 (2015).[107] R. Traoré, H. Caselles-Dupré, T. Lesort, T. Sun, N. Díaz-Rodríguez,D. Filliat, Continual reinforcement learning deployed in real-lifeusing policy distillation and Sim2Real transfer, in: ICML Workshopon Multi-Task and Lifelong Reinforcement Learning, 2019.[108] Y. Cheng, D. Wang, P. Zhou, T. Zhang, A survey of model com-pression and acceleration for deep neural networks, arXiv preprintarXiv:1710.09282 (2017).[109] R. V. Zicari, J. Amann, F. Bruneault, M. Coffee, B. Düdder, E. Hick-man, A. Gallucci, T. K. Gilbert, T. Hagendorff, I. van Halem,et al., How to assess trustworthy AI in practice, arXiv preprintarXiv:2206.09887 (2022).[110] ISO/IEC, ISO/IEC TR 24029-1, Information technology — Arti-ficial Intelligence (AI) – Assessment of the robustness of neuralnetworks - Part 1: Overview, https://www.iso.org/standard/77609.html (2021).[111] D. V. Carvalho, E. M. Pereira, J. S. Cardoso, Machine learninginterpretability: A survey on methods and metrics, Electronics 8 (8)(2019) 832.[112] J. H.-w. Hsiao, H. H. T. Ngai, L. Qiu, Y. Yang, C. C. Cao, Roadmapof designing cognitive metrics for explainable artificial intelligence(XAI), arXiv preprint arXiv:2108.01737 (2021).[113] A. Rosenfeld, Better metrics for evaluating explainable artificialintelligence, in: Proceedings of the 20th International Conferenceon Autonomous Agents and MultiAgent Systems, 2021, pp. 45–50.[114] R. R. Hoffman, S. T. Mueller, G. Klein, J. Litman, Metricsfor explainable AI: Challenges and prospects, arXiv preprintarXiv:1812.04608 (2018).[115] F. Sovrano, S. Sapienza, M. Palmirani, F. Vitali, A survey onmethods and metrics for the assessment of explainability under theproposed AI Act, in: The Thirty-fourth Annual Conference on LegalKnowledge and Information Systems (JURIX), Vol. 346, IOS Press,2022, p. 235.[116] A.-H. Karimi, J. von Kügelgen, B. Schölkopf, I. Valera, Towardscausal algorithmic recourse, in: International Workshop on Extend-ing Explainable AI Beyond Deep Models and Classifiers, Springer,2022, pp. 139–166.[117] C. Novelli, M. Taddeo, L. Floridi, Accountability in artificial intel-ligence: what it is and how it works, AI & Society (2023) 1–12.N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 29 of 30[118] A. Institute, Algorithmic Accountability Policy Toolkit (2018).e0000016.Connecting the Dots in Trustworthy Artificial Intelligence[138] R. V. Zicari, J. Brodersen, J. Brusseau, B. Düdder, T. Eichhorn,T. Ivanov, G. Kararigas, P. Kringen, M. McCullough, F. Möslein,et al., Z-inspection®: a process to assess trustworthy AI, IEEETransactions on Technology and Society 2 (2) (2021) 83–97.[139] H. Muller, M. T. Mayrhofer, E.-B. Van Veen, A. Holzinger, Theten commandments of ethical medical AI, Computer 54 (07) (2021)119–123.[140] K. Stöger, D. Schneeberger, A. Holzinger, Medical artificial intelli-gence: the european legal perspective, Communications of the ACM64 (11) (2021) 34–36.[141] J. Baker-Brunnbauer, TAII Framework for Trustworthy AI systems,ROBONOMICS: The Journal of the Automated Economy 2 (2021)17.[142] Editorials, Writing the rules in ai-assisted writing, Nature MachineIntelligence 469 (5) (2023) 469–469. doi:https://doi.org/10.1038/s42256-023-00678-6.[143] C. T. Zirpoli, Generative artificial intelligence and copyright law,United States Congressional Research Service, CRS Legal Sidebar,(February 23, 10922 (5 pages) (2023).[144] J. Tang, A. LeBel, S. Jain, A. G. Huth, Semantic reconstructionof continuous language from non-invasive brain recordings, NatureNeuroscience (2023) 1–9.[145] S. Reardon, Mind-reading machines are here: is it time to worry?,Nature 617 (7960) (2023) 236–236.[146] R. Burnell, W. Schellaert, J. Burden, T. D. Ullman, F. Martinez-Plumed, J. B. Tenenbaum, D. Rutar, L. G. Cheke, J. Sohl-Dickstein,M. Mitchell, et al., Rethink reporting of evaluation results in ai,Science 380 (6641) (2023) 136–138.[147] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluationof language models, arXiv preprint arXiv:2211.09110 (2022).[148] T. Hernandez-Boussard, S. Bozkurt, J. P. Ioannidis, N. H. Shah,MINIMAR (MINimum Information for Medical AI Reporting): de-veloping reporting standards for artificial intelligence in health care,Journal of the American Medical Informatics Association 27 (12)(2020) 2011–2015.[149] J. Schuett, A. Reuel, A. Carlier, How to design an AI ethics board,arXiv preprint arXiv:2304.07249 (2023).URL https://ainowinstitute.org/aap-toolkit.pdf[119] B. Kim, F. Doshi-Velez, Machine learning techniques for account-ability, AI Magazine 42 (1) (2021) 47–52.[120] B. Xia, Q. Lu, H. Perera, L. Zhu, Z. Xing, Y. Liu, J. Whittle, Towardsconcrete and connected AI risk assessment (C2AIRA): A systematicmapping study (2023). arXiv:2301.11616.[121] J. A. Baquero, R. Burkhardt, A. Govindarajan, T. Wallace, DeriskingAI by design: How to build risk management into AI development,McKinsey & Company (2020).[122] J. Mökander, M. Axente, F. Casolari, L. Floridi, Conformity assess-ments and post-market monitoring: A guide to the role of auditingin the proposed european AI regulation, Minds and Machines 32 (2)(2022) 241–268.[123] A. Holzinger, A. Carrington, H. Müller, Measuring the quality ofexplanations: the system causability scale (SCS) comparing humanand machine explanations, KI-Künstliche Intelligenz 34 (2) (2020)193–198.[124] Z. Han, H. Yanco, Communicating missing causal information toexplain a robot’s past behavior, ACM Transactions on Human-RobotInteraction 12 (1) (2023) 1–45.[125] M. Brundage, S. Avin, J. Wang, H. Belfield, G. Krueger, G. Hadfield,H. Khlaaf, J. Yang, H. Toner, R. Fong, et al., Toward trustworthyAI development: mechanisms for supporting verifiable claims, arXivpreprint arXiv:2004.07213 (2020).[126] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia,A. Travers, B. Zhang, D. Lie, N. Papernot, Machine unlearning, in:IEEE Symposium on Security and Privacy (SP), IEEE, 2021, pp.141–159.[127] S. Verma, V. Boonsanong, M. Hoang, K. E. Hines, J. P. Dickerson,C. Shah, Counterfactual explanations and algorithmic recoursesfor machine learning: A review, in: NeurIPS 2020 Workshop: MLRetrospectives, Surveys & Meta-Analyses (ML-RSA), 2020.[128] S. Barocas, A. D. Selbst, Big data’s disparate impact, California LawReview (2016) 671–732.[129] L. Floridi, M. Holweg, M. Taddeo, J. Amaya Silva, J. Mökander,Y. Wen, CapAI-A procedure for conducting conformity assessmentof AI systems in line with the EU artificial intelligence act, Availableat SSRN 4064091 (2022).[130] R. Parenti, Regulatory sandboxes and innovation hubs for fintech,Study for the Committee on Economic and Monetary Affairs, PolicyDepartment for Economic, Scientific and Quality of Life Policies,European Parliament, Luxembourg (2020) 65.[131] F. Pop, L. Adomavicius, Sandboxes for responsible artificial intelli-gence. eipa briefing september 2021. (2021).[132] K. Yordanova, The EU AI Act-Balancing human rights and innova-tion through regulatory sandboxes and standardization (2022).[133] J. Soler Garrido, S. Tolan, I. Hupon Torres, D. Fernandez Llorca,V. Charisi, E. Gomez Gutierrez, H. Junklewitz, R. Hamon, D. FanoYela, C. Panigutti, AI Watch: Artificial intelligence standardisationlandscape update, Tech. rep., Joint Research Centre (Seville site)(2023).[134] T. Madiega, A. L. Van De Pol, Artificial intelligence act and regu-latory sandboxes. EPRS European Parliamentary Research Service.June 2022 (2022).URL https://www.europarl.europa.eu/RegData/etudes/BRIE/2022/733544/EPRS_BRI(2022)733544_EN.pdf[135] Coalition for Health AI (CHAI), Blueprint for trustworthy AI imple-mentation guidance and assurance for healthcare (2023).URL https://www.coalitionforhealthai.org/papers/Blueprint%20for%20Trustworthy%20AI.pdf[136] J. Zhang, Z.-M. Zhang, Ethics and governance of trustworthy med-ical artificial intelligence, BMC Medical Informatics and DecisionMaking 23 (1) (2023) 1–15.[137] J. Amann, D. Vetter, S. N. Blomberg, H. C. Christensen, M. Coffee,S. Gerke, T. K. Gilbert, T. Hagendorff, S. Holm, M. Livne, et al., Toexplain or not to explain?– Artificial intelligence explainability inclinical decision support systems, PLOS Digital Health 1 (2) (2022)N. Díaz-Rodríguez, J. Del Ser et al.: Preprint submitted to ElsevierPage 30 of 30