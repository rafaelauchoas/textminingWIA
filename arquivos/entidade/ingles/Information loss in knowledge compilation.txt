Artificial Intelligence 174 (2010) 585–596Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintInformation loss in knowledge compilation:A comparison of Boolean envelopesPeter Schachte, Harald Søndergaard∗, Leigh Whiting, Kevin HenshallDepartment of Computer Science and Software Engineering, The University of Melbourne, Victoria 3010, Australiaa r t i c l ei n f oa b s t r a c tArticle history:Received 18 March 2009Received in revised form 14 March 2010Accepted 27 March 2010Available online 1 April 2010Keywords:Boolean approximationCo-clonesKnowledge basesTractable inferenceSince Selman and Kautz’s seminal work on the use of Horn approximation to speed up thequerying of knowledge bases, there has been great interest in Boolean approximation forAI applications. There are several Boolean classes with desirable computational propertiessimilar to those of the Horn class. The class of affine Boolean functions, for example,has been proposed as an interesting alternative to Horn for knowledge compilation. Toinvestigate the trade-offs between precision and efficiency in knowledge compilation,we compare, analytically and empirically, four well-known Boolean classes, and theircombinations, for ability to preserve information. We note that traditional evaluationwhich explores unit-clause consequences of random hard 3-CNF formulas does not tellthe full story, and we complement that evaluation with experiments based on a variety ofassumptions about queries and the underlying knowledge base.© 2010 Elsevier B.V. All rights reserved.1. Introduction: Boolean approximationThe problem of efficient inference from propositional knowledge is of fundamental importance for symbolic reasoning,as used for example in circuit verification. Knowledge compilation, as introduced by Selman and Kautz [16] is a particulartechnique that uses approximations of a knowledge base to speed up querying, at the expense of completeness. As is oftenthe case with powerful ideas, the approach is simple. Let a knowledge base ϕ be given and let ϕ↑be a logical consequence(an upper approximation) of ϕ, with ϕ↑belonging to some class (Horn, for example) of Boolean functions for which thequestion of logical consequence is tractable. For any query α, a positive answer to the question “ϕ↑ |(cid:4) α?” affirms “ϕ |(cid:4) α.”A negative answer is of no help. However, if a lower approximation ϕ↓is also available (as assumed in the Selman–Kautz framework), a negative answer to the question “ϕ↓ |(cid:4) α?” can similarly be used to answer “ϕ |(cid:4) α” in the negative.Otherwise one has to fall back on some standard (expensive) method for resolving “ϕ |(cid:4) α,” or answer “don’t know.”We shall follow Kavvadias et al. [8] and refer to a unique best upper approximation as an “envelope” (and to the dualconcept as a “core”—although we will not need that much, as cores are not well-defined for the classes discussed in thispaper).Three factors determine the effectiveness of knowledge compilation: (1) the time required to compute the approxima-tions, amortised over all queries of the same knowledge base; (2) the time saved by evaluating queries using approxima-tions; and (3) the proportion of queries for which the approximate knowledge base yields a definite answer. The secondfactor is determined by the choice of Boolean function class with tractable inference, and the first by availability of anefficient algorithm for calculating approximations in the class. Both of these aspects have received considerable attention,* Corresponding author. Tel.: +61 3 8344 1342; fax: +61 3 9348 1184.E-mail addresses: schachte@unimelb.edu.au (P. Schachte), harald@unimelb.edu.au (H. Søndergaard), leighwhiting@gmail.com (L. Whiting),kevin.henshall@gmail.com (K. Henshall).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.03.003586P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596including in-depth study [10]. Del Val [6] has shown that Selman and Kautz’s Horn envelope algorithm carries over to allBoolean function classes closed under subsumption. He has proposed an improved algorithm that is applicable if, addition-ally, the complement of a class is closed under resolution. Moreover, del Val has discussed the case of first-order predicatelogic, showing how the original concepts can be extended in that direction as well [6].While computational questions have received much attention, less has been said about information loss in knowledgecompilation. In their study, Selman and Kautz [16] analysed how well the generated approximations preserve information.They applied their method to a large number of hard propositional formulas in 3-CNF and analytically derived an estimateof how many queries of the forms x, x ∨ y, and x ∨ y ∨ z could be answered successfully, based on the approximationsalone. The results were very encouraging, particularly as the less expressive class of conjunctive (unit Horn) functions weresubstituted for the Horn class to simplify analysis. One aim of this note is to extend and complement the Selman–Kautzanalysis with an empirical analysis of fidelity, not only for Horn approximations, but also for (upper) Krom, contra-dualHorn, and affine approximations.A second aim is to understand how fidelity varies with underlying assumptions about knowledge bases and queries.Selman and Kautz’s analysis was for random hard 3-CNF knowledge bases. These are strong Boolean functions, close to halfof which are unsatisfiable and lead to unsatisfiable approximations, while the rest typically have relatively few models. Eachapplication of knowledge compilation is different, but it must be expected that some applications involve somewhat weakerBoolean functions, or at least that inconsistent knowledge bases be repaired. How well will approximation to differentclasses preserve information in that setting? Here we empirically measure information loss in approximation to the fourdifferent classes and their combinations, using three different test sources: (1) random Boolean functions, (2) random 3-CNF, including random hard 3-CNF (as in the Selman–Kautz analytical investigation), and (3) structured functions arisingfrom encodings of combinatorial problems. We measure information loss in two different ways: by counting the number ofmodels added in envelopes, and by calculating the fraction of random queries entailed by the original function but not byits approximation. Results from the different experiments are given in Sections 5–7.A third aim is to investigate to what extent combinations of Boolean classes can improve the success rate for acceleratedquery-answering. This question presents itself naturally, owing to this observation (Proposition 2): For any query α whichcan be expressed in 3-CNF, whether a knowledge base ϕ entails α can be decided completely from ϕ’s Horn and contra-dual Horn envelopes. An interesting corollary is that, given the assumptions used in Selman and Kautz’s analysis [16], onecan obtain not just better results by using both envelopes, but perfect ones—100% accuracy is achieved without the needfor any lower approximations. Related conclusions are drawn in Section 8.2. Boolean co-clonesLet B = {0, 1} and let V be a countably enumerable set of variables. A valuation μ : V → B is an assignment of truthvalues to the variables in V . Let I = V → B denote the set of V -valuations. A Boolean function over V is a functionϕ : I → B. We let B denote the set of all Boolean functions over V . The ordering on B is the usual: x (cid:2) y iff x = 0 ∨ y = 1.B is ordered pointwise, so that the ordering relation corresponds exactly to classical entailment, |(cid:4). A valuation μ is a modelfor ϕ, denoted μ |(cid:4) ϕ, if ϕ(μ) = 1. We use the usual connectives, including + for exclusive or. We let (cid:8)x, y, z(cid:9) denote themedian1 operation: (cid:8)x, y, z(cid:9) = (x ∧ y) ∨ (x ∧ z) ∨ ( y ∧ z). These connectives will also be applied to valuations with the obviousintention (pointwise application).In this study we are concerned with the four Schaefer classes [1]:Krom (K): ϕ is Krom iff for all valuations μ, μ(cid:11), (cid:8)μ, μ(cid:11), μ(cid:11)(cid:11)(cid:9) |(cid:4) ϕ whenever μ |(cid:4) ϕ, μ(cid:11) |(cid:4) ϕ, and μ(cid:11)(cid:11) |(cid:4) ϕ. Syntacti-cally, K is the set of functions that can be written in conjunctive normal form with at most two literals per clause,and its members are also referred to as 2-CNF or bijunctive., and μ(cid:11)(cid:11)Horn (H): ϕ is Horn iff for all valuations μ and μ(cid:11)that can be written in conjunctive normal formclause., (μ ∧ μ(cid:11)) |(cid:4) ϕ whenever μ |(cid:4) ϕ and μ(cid:11) |(cid:4) ϕ. H is the set of functions((cid:4)1 ∨ · · · ∨ (cid:4)k), k (cid:3) 0, with at most one positive literal (cid:4) per(cid:2)Contra-dual Horn (N): ϕ is contra-dual2 Horn iff for all valuations μ and μ(cid:11)These are the functions that can be written in conjunctive normal formnegative literal (cid:4) per clause.Affine (A): ϕ is affine iff for all valuations μ, μ(cid:11), and μ(cid:11)(cid:11), (μ + μ(cid:11) + μ(cid:11)(cid:11)) |(cid:4) ϕ whenever μ |(cid:4) ϕ, μ(cid:11) |(cid:4) ϕ, and μ(cid:11)(cid:11) |(cid:4) ϕ [15].A Boolean function is affine iff it can be written as a conjunction of terms c0 + c1x1 + c2x2 + · · · + ckxk, whereci ∈ {0, 1} and xi ∈ V for all i ∈ {0, . . . , k}.3, (μ ∨ μ(cid:11)) |(cid:4) ϕ whenever μ |(cid:4) ϕ and μ(cid:11) |(cid:4) ϕ.((cid:4)1 ∨ · · · ∨ (cid:4)k), k (cid:3) 0, with at most one(cid:2)There are other classes of interest, including k-Horn [4] and k-quasi-Horn, for which envelopes are also well-defined.Some well-studied classes, however, including the class of unate functions and the class of renamable Horn functions, do not1 Or “majority” operation; we prefer the terminology and notation suggested by Knuth [9].2 We follow Halmos [7] in using this term.3 In the cryptography/coding community, “affine” is used for what Post [12] called an “alternating” function, that is, a function that can be writtenc0 + c1x1 + c2x2 + · · · + ck xk , k (cid:2) 0. The class of alternating functions is not closed under conjunction.P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596587Fig. 1. Lattice of co-clones—top part.provide unique best upper approximations.4 It is inconvenient to have the question “Does ψ follow from the approximationof ϕ?” answered non-deterministically, so we insist that the relation between a Boolean function and its envelope should bea proper function.5 This corresponds to the technical requirement that a Boolean function class be a co-clone [11]—roughly aBoolean function collection that can be characterised via “model closure” under a set of connectives (as we did for the fourclasses above). It is well known that every co-clone contains 1 and is closed under conjunction and existential quantification.Hence for any co-clone C, the C-envelope of a Boolean function ϕ is defined simply as{ψ ∈ C | ϕ |(cid:4) ψ}.(cid:2)Recall that a clone is any family of functions, containing the projection functions and closed under composition. TheBoolean clones form a lattice under subset ordering, and this lattice is (a subset of) Post’s well-known lattice [12], fromwhich one easily reads Post’s famous functional completeness result for propositional logic. The relations between theclones, the lattice of co-clones, and the classes identified by Schaefer in his dichotomy result for generalised satisfiability [15]are well explained by Böhler et al. [1].Fig. 1 shows the top end of the lattice of co-clones, including K, H, N, A, and B. The classes II0 and II1 are Schaefer’s0-valid and 1-valid classes, respectively.6 That is, ϕ ∈ II0 iff (λv.0) |(cid:4) ϕ, and ϕ ∈ II1 iff (λv.1) |(cid:4) ϕ. The class IN2 is the set ofBoolean functions ϕ satisfying the model-closure constraint that (¬μ) |(cid:4) ϕ whenever μ |(cid:4) ϕ. The class IN is the intersectionII0 ∩ II1 ∩ IN2 (in this expression we could equivalently leave out either of the first two).For K, H, and N, there are well-known linear-time algorithms for deciding satisfiability (SAT) of formulas in CNF, andfor A, satisfiability is decidable in polynomial time, assuming the usual matrix form representation for affine functionsin modulo-2 arithmetic. Simple reductions to SAT show that entailment can similarly be decided in polynomial time forthese classes. Let C be any one of K, H, N, or A. The entailment ϕ |(cid:4) γ1 ∧ · · · ∧ γn holds exactly when each γi is a logicalconsequence of ϕ, so it suffices to consider entailment of each conjunct separately. But ϕ |(cid:4) γ amounts to the satisfiabilityof ϕ ∧ ¬γ , a formula which can be translated to a conjunction of C constraints in linear time. Moreover, ϕ ∧ ¬γ belongs toC whenever ϕ does.The proposition below communicates the sense in which the four Schaefer classes are appropriate for this study. Wehave already argued why we focus on Boolean co-clones (for envelopes to be unique), and Proposition 1 then says that,assuming a CNF presentation, classes not contained in the Schaefer classes do not allow for efficient entailment tests, unlessP = co-NP. To be precise, let EC be the entailment problem for class C, an instance of which is of the form (ϕ, ψ), with ϕ ∈ C,and ϕ and ψ presented in CNF. The decision problem posed in EC is “Does ϕ |(cid:4) ψ hold?”Proposition 1. For each co-clone C not contained in K, H, N or A, EC is co-NP complete.Proof. The complement of EC has an obvious polynomial time verifier. For an instance (ϕ, ψ), the verifier uses as certificatean interpretation that satisfies ϕ but not ψ . Inspection of Fig. 1, together with the fact that all omitted co-clones are subsetsof K, H, N, or A, then makes it clear that we only need to show EIN co-NP hard. We do this by providing a sequence ofh(cid:4) EIN, where UNSATC is the unsatisfiability problemthree polynomial-time mapping reductions UNSATBfor class C.f(cid:4) UNSATIN2For the first reduction, consider f defined by f (ϕ) = ϕ ∨ ϕ◦dual function. Note that ϕ and ϕ◦be computed in polynomial time, note that f (ϕ) =is ϕ’s contra-dual, that is, the negation of ϕ’sare equi-satisfiable and so f (ϕ) ∈ IN2 is satisfiable iff ϕ is. To see that a formula for f canis obtained by simply changing{γ ∨ η | γ ∈ ϕ ∧ η ∈ ϕ◦}, and that ϕ◦, where ϕ◦g(cid:4) EIN2(cid:2)4 For example, x → y and x ← y are unate, but x ↔ y is not, so the “unate envelope” of the latter is not well-defined.5 Where there is no unique best upper approximation, two minimal upper approximations may suffer exponentially different information loss. Section 3discusses this phenomenon (in the dual—for maximal lower approximations). Without an insistence on classes with unique best approximations, we wouldneed to discuss actual approximation algorithms, and their probability of producing a better or worse minimal upper approximation.6 Here we use the nomenclature of Böhler et al. [1].588P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596the sign of each literal in ϕ. The second reduction is performed by g, defined by g(ϕ) = (ϕ, 0)—clearly a polynomial-timereduction. For the third reduction, consider the function h defined by(cid:3)h(ϕ, ψ) =(ϕ, ψ)(II(ϕ), II(ψ)) otherwiseif ϕ is 0-valid or 1-validwhere II yields the 0-valid 1-valid envelope. Note that II(ϕ) can be obtained in polynomial time by replacing each ϕ-clauseγ by{u ∨ ¬v ∨ γ | u, v occur in ϕ}. (cid:2)(cid:2)3. Lower and upper approximationsA class closed under disjunction offers a unique weakest implicant, or “core”—dual to the concept of an envelope. How-ever, none of the classes we consider are closed under disjunction. In the absence of unique best lower approximations it iscommon to use maximal lower approximations from a given class.7 Selman and Kautz [16] show how to derive a maximallower Horn approximation by repeated use of so-called Horn strengthening (removal of k − 1 positive literals from a clausethat has k > 1 positive literals). There has been much interest in the task of finding maximal lower approximations, inparticular for Horn approximations [2].However, different maximal approximations may have staggeringly different information content, and so should betreated with great care. Zanuttini [17] elegantly makes this point for K, H, and A with a single example. He considersthe Boolean n-place function that has the 2n−3 + 2 models {m100 | m ∈ {0, 1}n−3} ∪ {00 . . . 010, 00 . . . 001}. Maximal lowerHorn approximations include {00 . . . 010} and the exponentially larger {m100 | m ∈ {0, 1}n−3}. Maximal lower Krom approx-imations include {00 . . . 010, 00 . . . 001} and the exponentially larger {m100 | m ∈ {0, 1}n−3} ∪ {00 . . . 010}. Maximal loweraffine approximations are {00 . . . 010, 00 . . . 001} and the exponentially larger {m100 | m ∈ {0, 1}n−3}.Upper-approximation in the same classes avoids this kind of divergence. Nevertheless, for each class C studied here, theloss of information involved in C upper-approximation can be considerable: the number of models added to a function canbe exponential in its number of variables. Below are examples of n-place Boolean functions that can produce the vacuousenvelope 1. They show that the C-envelope may in the worst case incur a great loss of information.(1) With n (cid:3) 3, the function(cid:2){xi ∨ x j ∨ xk | 1 (cid:2) i < j < k (cid:2) n} is in N and has n(n + 1)/2 + 1 models. Its K envelope has 2nmodels.(2) With n (cid:3) 2, the function(cid:2)its A envelope, have 2n models.(cid:2)(3) With n (cid:3) 2, the functionA envelope, have 2n models.{¬xi ∨ ¬x j | 1 (cid:2) i < j (cid:2) n} is in H (and K) and has n + 1 models. Its N envelope, as well as{xi ∨ x j | 1 (cid:2) i < j (cid:2) n} is in N (and K) and has n + 1 models. Its H envelope, as well as itsNotably, the Horn envelope combined with the contra-dual Horn envelope achieves complete coverage for queries thatare presented in 3-CNF:Proposition 2. Let ϕ be a Boolean function and let ϕ(cid:11)in 3-CNF. Then ϕ |(cid:4) ψ iff, for each clause C of ψ , ϕ(cid:11) |(cid:4) C or ϕ(cid:11)(cid:11) |(cid:4) C .and ϕ(cid:11)(cid:11)be its Horn and contra-dual Horn envelopes. Let ψ be a Boolean formulaProof. The “if” part is immediate, as ϕ |(cid:4) ϕ(cid:11), and so, since each clause of ψ is a logical consequence of ϕ, ψ istoo. For the “only if” part, assume that ϕ |(cid:4) ψ and let C be an arbitrary clause of ψ . Then ϕ |(cid:4) C , and since C ∈ H or C ∈ N,C is a logical consequence of either ϕ(cid:11)(or both). (cid:2)and ϕ |(cid:4) ϕ(cid:11)(cid:11)or ϕ(cid:11)(cid:11)Proposition 2 can be strengthened. Using results from del Val [5], one can show that the proposition’s assertion holdsis the 3-CNF contra-dual Horn envelope. Of course, not allis the 3-CNF Horn envelope and ϕ(cid:11)(cid:11)even if we assume that ϕ(cid:11)queries have a 3-CNF presentation; there are Boolean functions that cannot be so expressed.4. Experimental methodOur aim is to empirically evaluate the loss of information incurred by the approximation of knowledge bases to classesthat guarantee a unique best approximation and tractable entailment checking. Only (sub-classes of) the four Schaeferclasses satisfy these requirements. As subclasses correspond to greater information loss, we consider only the four. Westress that no single representation is ideal for all four classes. CNF works well for K, H, and N but is less suitable for A.A modulo-2 arithmetic matrix form is ideal for A, but unsuitable for K, H, and N. Hence we use the more neutral ReducedOrdered Binary Decision Diagrams (ROBDDs) [3] and associated envelope algorithms [13,14]. From an efficiency point ofview, our representation is adequate for all classes, but ideal for none. But since our focus is on measuring information loss,7 We avoid the commonly used term GLB for a maximal lower approximation, as it has a different use in lattice theory.P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596589Fig. 2. Information lost (models gained) by envelopes (random functions).Fig. 3. 3-CNF queries dropped (percentages) by envelopes (random functions).we are less concerned about the most efficient representation for a given class. ROBDDs provide a uniform base for ourexperiments, and also allow us to generate Boolean functions that are truly random, in the sense that every valuation hasan equal probability of being a model.5. First experiment: randomly generated Boolean functionsWe first explore information loss due to approximation in random Boolean functions. To this end, we generate Booleanfunctions of 24 variables such that each valuation has probability p of being a model, varying p from 1 in 215 to1 in 224.For each random Boolean function we evaluate the information loss from taking K, H, N and A envelopes in two differentways. Firstly, we compute the number of models of the original function and of each of its envelopes, and consider thatevery model of an envelope that is not a model of the original function is information lost by that approximation. We thencalculate the percentage of loss as follows. Let m be the number of models of the original function and n be the number ofmodels of its envelope. The percentage of information loss is then 100 · (n − m)/(224 − n). The results of this calculation foreach envelope over functions of varying strengths is presented in Fig. 2. The number of models of the original function is590P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596Fig. 4. 6-CNF queries dropped (percentages) by envelopes (random functions).Fig. 5. Information lost vs. function size (random functions).presented logarithmically along the X axis, with the strongest functions on the left, while the Y axis shows the percentageof information loss. Note that all approximations lie imperceptibly above the X axis (with A and K slightly worse than Hand N) up to functions with about 16 models. H and N are indistinguishable.Our second appraisal of information loss counts the proportion of random queries that are correctly answered by theapproximation. A query is “dropped” if it is entailed by the original function but not by the envelope. We follow the traditionin the literature of using single clause 3-CNF queries (that is, disjunctions of three literals). We also consider 6-CNF queries,to see if weaker queries give different results. We do not consider queries consisting of conjunctions of clauses, as theycould be handled by separately checking if each clause is entailed. Each query is formed by generating 3 (or 6) literals atrandom, ensuring that no two literals involve the same variable, and each envelope is tested with 1000 3-CNF and 10006-CNF queries, considering only queries that are indeed entailed by the original function. All of this is repeated for 100random original formulas, and averages are calculated. We show no results where fewer than 1% of the random queries(1000 queries overall) are entailed by the original function.One might wonder whether queries that cannot be answered for one approximation might be handled successfully byanother. This is of interest beyond selecting the most effective approximation: one might well use two separate types ofP. Schachte et al. / Artificial Intelligence 174 (2010) 585–596591Fig. 6. 3-CNF queries dropped vs. function size (random functions).Fig. 7. 6-CNF queries dropped vs. function size (random functions).approximation, and successfully answer the query if either is able to answer it. Although this requires computing twoapproximations of a knowledge base, it may be worthwhile in applications where the knowledge base does not changeoften. However it will be wasteful if both approximations tend to fail for the same queries. Thus we additionally presentresults for each pair of approximation domains, where we consider a pair to answer a query successfully if either does (orboth do, of course). Figs. 3 and 4 show the results for 3-CNF and 6-CNF queries, respectively.We also confirmed that these results hold for formulas of different numbers of variables, ranging from 10 to 90, butsimilar strengths. Fig. 5 also shows the percentage of information loss computed as above plotted against the numberof variables in each randomly generated function. In each case, for a function of n variables, we set the probability ofeach valuation being a model such that there would be approximately n models. These results would seem to show littleinformation loss once the number of variables rises above 20 or 30, however Figs. 6 and 7, showing the percentage of3-CNF and 6-CNF queries dropped by each approximation, refute this conclusion. Here again we fix the probability of eachvaluation being a model to approximately n in 2n. As above, in all cases, 100 random Boolean functions were used, and592P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596Fig. 8. Information lost (models gained) by envelopes (3-CNF functions).Fig. 9. 3-CNF queries dropped (percentages) by envelopes (3-CNF functions).1000 3-CNF and 1000 6-CNF random test queries were used for each approximation. The charts show a fairly consistentdegree of information loss, when measured by queries dropped, regardless of number of variables.86. Second experiment: random 3-CNF including hard 3-CNFThe second experiment is like the first, except that arbitrary random functions have been replaced by random functionsexpressible as 3-CNF formulas. This provides a baseline for comparison with related work which has almost exclusivelyconsidered the (hard) 3-CNF case. In the first experiment we generated formulas for a variety of model-probability dis-tributions. Here instead we generate formulas that have a variety of clause-to-variable ratios. (Each formula is translatedto ROBDD form, and the various approximations are then produced from that.) In all cases the formulas denote 24-placeBoolean functions, but explore a range of numbers of clauses from 8 to 148, including 103, which yields hard 3-CNF func-tions.8 The fluctuations across the graphs are due to random variation, and the fact that we approximated the n/2n model probability as 1 in 2n−(cid:18)0.5+log2(n)(cid:19)The systematic error caused by this rounding would tend to raise values for 50, 30, 60, and 70 variables, and lower them for 90, relative to the others..P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596593Fig. 10. 6-CNF queries dropped (percentages) by envelopes (3-CNF functions).Fig. 11. Information lost vs. function size (3-CNF functions).As in the first experiment, we measure information loss in terms of the models added by each envelope. For a givennumber of clauses, we again test 100 random functions, this time 3-CNF. The result is shown in Fig. 8. The point corre-sponding to 103 clauses (the hard 3-CNF case) falls around 2 models on the X axis. Although it is not readily discerniblefrom the figures, at the hard 3-CNF point, on average K approximation adds 1 model out of a possible 224, H and N eachadd 4, and A adds 211.Similarly, the proportions of 3-CNF and 6-CNF queries dropped by the various approximations (over 1000 tests each) arepresented in Figs. 9 and 10, again including results for pairs of envelopes. At the hard 3-CNF point, for both 3- and 6-CNFqueries, K performs best and A worst (still dropping less than 1% of queries).Again we have verified that these results are fairly insensitive to the size of the functions involved. Fig. 11 showsinformation lost (as percentage of models gained) through approximation for hard 3-CNF formulas of sizes ranging from10 to 90 variables (43–387 clauses). Figs. 12 and 13 show information loss as measured by dropped queries, for hard 3-CNFformulas over the same size range. Once again, despite all approximations gaining a very small percentage of models, theproportion of queries dropped is not negligible, and is fairly independent of size. Note that A approximation loses the mostinformation, K the least, and the combination of H and N again answers all 3-CNF and almost all 6-CNF queries correctly.594P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596Fig. 12. 3-CNF queries dropped vs. function size (3-CNF functions).Fig. 13. 6-CNF queries dropped vs. function size (3-CNF functions).7. Third experiment: structured Boolean functionsRandomly generated Boolean functions are arguably unrepresentative of real world knowledge bases that show somestructure, so the third experiment investigates information loss for structured Boolean functions, sourced from SAT-basedproblem-solving. Table 1 gives the size of some sample problems and shows how many additional models are created bythe various envelopes. Note that although some of the numbers are large, in no case is more than three percent of thepossible models added. To give a better understanding of what this means for typical query-answering, Table 2 shows thepercentage of 3-CNF queries dropped by the various envelopes and their pairwise combinations, and Table 3 does the samefor 6-CNF queries. Each formula used is satisfiable and has been queried with 1000 random 3-CNF queries and 1000 random6-CNF queries. The formulas are: ais6.cnf, an all-interval-series instance from SATLIB; colour.cnf, 4-colouring a mapof the 7 western-most contiguous US states; queensn.cnf, solving the n-queens problem; sudokun.cnf, solving a 4 × 4sudoku instance using a unary encoding with n squares already filled; sud1_n.cnf, solving a 9 × 9 sudoku instance usinga binary encoding with n squares already filled; sud2_n.cnf, similar, but a different instance; and sv4.cnf, disprovinga software verification assertion. Note that for varying n, the queensn.cnf tests involve varying numbers of variablesP. Schachte et al. / Artificial Intelligence 174 (2010) 585–596595Table 1Information lost (number of models gained) by envelopes (structured functions).Ais6ColourQueens5Queens6Sudoku1Sudoku2Sudoku3Sudoku4Sud1_22Sud1_23Sud1_24Sud2_26Sv4Variables612826376464646432432432432458Clauses5819727649724124224324415,88315,88715,89115,899150K43,89010,4252761915,128228237616418,750641431896Table 23-CNF queries dropped (percentages) by envelopes (structured functions).Ais6ColourQueens5Queens6Sudoku1Sudoku2Sudoku3Sudoku4Sud1_22Sud1_23Sud1_24Sud2_26Sv4K5.30.08.40.02.81.61.50.50.10.00.00.10.3H8.50.04.20.00.91.21.40.60.40.20.30.80.5N77.7100.081.618.225.511.68.92.81.80.30.51.10.8A87.7100.085.818.228.116.914.24.64.70.90.52.40.0Table 36-CNF queries dropped (percentages) by envelopes (structured functions).Ais6ColourQueens5Queens6Sudoku1Sudoku2Sudoku3Sudoku4Sud1_22Sud1_23Sud1_24Sud2_26Sv4K8.66.59.70.38.42.51.60.40.30.00.00.20.3H2.02.34.90.32.41.91.81.10.60.20.10.80.3N64.1100.064.48.836.012.58.12.51.10.40.30.40.3A87.099.884.17.443.221.214.73.03.51.00.21.80.0KH0.50.04.20.00.60.60.60.10.00.00.00.00.1KH2.02.34.90.32.20.90.90.40.10.00.00.00.2H1782913261196815258138945553229901KN4.80.04.20.02.21.00.90.40.10.00.00.10.2KN5.46.51.40.06.31.30.40.00.10.00.00.20.1N672,5555,688,3579511115,132,387599177533241,0262704239247A1,048,5522,096,9122464131,00010062441016,777,17011912440KA5.30.08.40.02.81.61.50.50.10.00.00.10.0KA8.56.58.60.08.32.51.60.40.30.00.00.10.0HN0.00.00.00.00.00.00.00.00.00.00.00.00.0HN0.32.30.00.00.90.00.00.00.00.00.00.00.0HA0.50.04.20.00.60.90.80.10.20.10.10.50.0HA1.92.34.00.02.11.31.00.40.50.20.00.50.0NA77.7100.081.618.225.311.08.02.21.70.20.10.70.0NA64.199.862.87.435.211.67.21.40.80.30.10.20.0but similar numbers of models, while each group of Sudoku tests involve the same number of variables, but considerabledifferences in the numbers of models.Again, the results show that HN is the strongest, giving almost perfect results. Also, considering the Sudokun andSud1_n results, we again see that stronger Boolean functions (larger n) suffer less loss of information and dropped queries.These tests further show a bias not exhibited by the earlier tests: approximation loses more or less information, and dropsfewer or more queries, depending on the idiosyncrasies of the knowledge bases and queries involved. Consider, for example,Table 3, where for queens5, K does not improve on H approximation, while it improves N to surpass the KH combination.Also, contrary to earlier results, A approximation performs better than N in many cases.8. ConclusionsWe have considered four classes of Boolean functions for use in knowledge compilation from the standpoint of infor-mation and consequence preservation, and showed that these are the only maximally precise classes guaranteeing unique596P. Schachte et al. / Artificial Intelligence 174 (2010) 585–596best approximation and polynomial time entailment checking. Empirical evaluation of these approximations leads to severalinteresting observations:• On the whole, knowledge compilation works well for strong knowledge bases.• Querying pairs of domains is beneficial—for the right combinations of domains. In particular, HN, the combination ofHorn and its contra-dual, uniformly gives the best results. This combination always answers 3-CNF queries precisely,with no need for lower approximation. As shown in Fig. 10 and Table 3, HN also drops very few 6-CNF queries for3-CNF knowledge bases.• Affine approximation (A) generally performs worst of the domains we have considered, and none of the other domainsbenefit much from combination with A. In a few of the structured test cases, A outperformed N, due to the nature of theknowledge base; in particular, the Sv4 test case happens to be affine. For random knowledge bases, A always performsworst, though note that all test queries were disjunctive, not affine. It is difficult to recommend affine approximationfor knowledge compilation, except in cases where the knowledge base or queries are expected to be nearly affine.• Krom approximation (K), however, does appear to be useful. For 3-CNF knowledge bases, K approximations answermore 3-CNF queries correctly than any of the other approximations. For strong knowledge bases—3-CNF functions witha clause:variable ratio of at least 2.6 or Boolean functions with around 64 models—the phenomenon extends to 6-CNFqueries. Remarkably, this is true despite the fact that both H and N generally give stronger approximations than K. It isalso interesting that the precision of the KH and KN combinations is significantly better than any single domain, onlybeing surpassed by HN.• Whether we test random or 3-CNF knowledge bases, H and N rarely diverge. This is not surprising: these domains arecontra-duals, so on average unbiased tests tend to behave similarly for H and N approximation. However it is strikingthat H and N behave very differently for almost all of the structured tests, probably owing to a natural human tendencyto favour implications with a single consequent over implications with a single antecedent.• As shown by Figs. 6, 7, 12, and 13, the query-answering performance of each approximation or combination is inde-pendent of the number of variables of the knowledge base. However, query-answering performance is very sensitive tothe strength of the knowledge base, as shown by Figs. 3, 4, 9, and 10. The structured tests, all of which involved ratherstrong knowledge bases, but range from 26 to 324 variables, confirm this (Tables 2 and 3).• Conclusions should not be drawn from the hard 3-CNF case alone. Of the 100 random hard 3-CNF functions in ourexperiments, 75 had fewer than 3 models. Those are all, by definition, already in both the Krom and the affine classes.The last two points suggest that the success of knowledge compilation is very sensitive to the nature of the knowledge base andqueries themselves. Where queries fall into a class with tractable inference, approximation of the knowledge base to thatclass may lose information, but will not drop any queries. Considering only the case of 3-CNF queries on approximations ofhard 3-CNF knowledge bases does not adequately predict the behaviour for other shapes of knowledge bases and queries.AcknowledgementsWe thank the reviewers, who all provided us with detailed and helpful advice. In particular, the comments made afterProposition 2 are due to the reviewers.References[1] E. Böhler, N. Creignou, S. Reith, H. Vollmer, Playing with Boolean blocks, part II: Constraint satisfaction problems, ACM SIGACT News 35 (1) (2004)22–35.[2] Y. Boufkhad, Algorithms for propositional KB approximation, in: Proc. 15th Nat. Conf. Artificial Intelligence, AAAI Press/MIT Press, 1998, pp. 280–285.[3] R. Bryant, Graph-based algorithms for Boolean function manipulation, IEEE Transactions on Computers C 35 (8) (1986) 677–691.[4] R. Dechter, J. Pearl, Structure identification in relational data, Artificial Intelligence 58 (1992) 237–270.[5] A. del Val, An analysis of approximate knowledge compilation, in: Proc. IJCAI’95, vol. 2, 1995, pp. 830–836.[6] A. del Val, First order LUB approximations: Characterization and algorithms, Artificial Intelligence 162 (2005) 7–48.[7] P.R. Halmos, Lectures on Boolean Algebras, Springer-Verlag, 1963.[8] D. Kavvadias, C. Papadimitriou, M. Sideri, On Horn envelopes and hypergraph transversals, in: K. Ng, P. Raghavan, N. Balasubramanian, F. Chin (Eds.),Proc. Fourth Int. Symp. Algorithms and Computation, in: Lecture Notes in Computer Science, vol. 762, Springer-Verlag, 1993, pp. 399–405.[9] D.E. Knuth, Introduction to Combinatorial Algorithms and Boolean Functions, The Art of Computer Programming, vol. 4, Addison-Wesley, 2008.[10] P. Liberatore, Compilation of Intractable Problems and Its Application to Artificial Intelligence, PhD thesis, University of Rome “La Sapienza”, Italy, 1998.[11] N. Pippenger, Theories of Computability, Cambridge University Press, 1997.[12] E.L. Post, The Two-Valued Iterative Systems of Mathematical Logic, Princeton University Press, 1941. Reprinted in M. Davis, Solvability, Provability,Definability: The Collected Works of Emil L. Post, Birkhäuser, 1994, pp. 249–374.[13] P. Schachte, H. Søndergaard, Boolean approximation revisited, in: I. Miguel, W. Ruml (Eds.), Proc. SARA 2007, in: Lecture Notes in Artificial Intelligence,vol. 4612, Springer-Verlag, 2007, pp. 329–343.[14] P. Schachte, H. Søndergaard, L. Whiting, K. Henshall, An algorithm for affine approximation of binary decision diagrams, Chicago Journal of TheoreticalComputer Science, in press.[15] T.J. Schaefer, The complexity of satisfiability problems, in: Proc. Tenth Ann. ACM Symp. Theory of Computing, 1978, pp. 216–226.[16] B. Selman, H. Kautz, Knowledge compilation and theory approximation, Journal of the ACM 43 (2) (1996) 193–224.[17] B. Zanuttini, Approximation of relations by propositional formulas: Complexity and semantics, in: S. Koenig, R. Holte (Eds.), Proc. SARA 2002, in:Lecture Notes in Artificial Intelligence, vol. 2371, Springer-Verlag, 2002, pp. 242–255.