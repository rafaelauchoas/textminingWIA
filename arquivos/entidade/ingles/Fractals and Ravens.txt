Artificial Intelligence 215 (2014) 1–23Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintFractals and RavensKeith McGreggor∗, Maithilee Kunda, Ashok GoelDesign & Intelligence Laboratory, School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA 30332, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 4 June 2013Received in revised form 14 April 2014Accepted 13 May 2014Available online 20 May 2014Keywords:AnalogyVisual reasoningFractal representationsComputational psychometrics1. IntroductionWe report a novel approach to visual analogical reasoning, one afforded expressly by fractalrepresentations. We first describe the nature of visual analogies and fractal representations.Next, we exhibit the Fractal Ravens algorithm through a detailed example, describe itsperformance on all major variants of the Raven’s Progressive Matrices tests, and discussthe implications and next steps. In addition, we illustrate the importance of consideringthe confidence of the answers, and show how ambiguity may be used as a guide for theautomatic adjustment of the problem representation. To our knowledge, this is the firstpublished account of a computational model’s attempt at the entire Raven’s test suite.© 2014 Elsevier B.V. All rights reserved.Despite references to its role as core to cognition [33,35], analogy defies a singular definition [58]. In one way, analogymay be seen as a process of transference, a mapping of knowledge from one situation to another, based upon a judgmentof the similarity between the two situations [12,19,20,22–27,37,40,57,67]. Gentner [24], for example, proposed that analo-gies entail transfer of relations from a source case to a target problem and that judgment of similarity between the targetand the source depends on the correspondence between the structure of their representations. Holyoak and Thagard [37]proposed that judgments of similarity between a source case and a target problem depend on multiple criteria: structuralcorrespondence, semantic similarity, and pragmatic constraints. Keane [40] proposed incremental mapping between thesource case and the target problem. Dunbar [19] found a paradox in that humans appear to exhibit significant spontaneoususe of analogies in their natural workflow but less so in laboratory settings. Kokinov and Petrov [42] describe several con-straints to facilitate the integration of analogue retrieval from memory and analogical transfer. Holyoak and Hummel [36]similarly examine findings for recruiting memory, reasoning and learning in the service of analogy making. Clement [12] de-scribes several processes of analogy such as the use of intermediate representations (or bridging analogies). Nersessian [57]similarly describes the use of generic mechanisms in scientific analogies. Our own work on model-based analogy [6,28] hasfocused on the use of semantic similarity and pragmatics constraints for evaluating similarity between source cases and tar-get problems, and identification and abstraction of generic mechanisms for transfer from a source case to a target problemin creative design [6,28,29].In contrast, case-based reasoning [1,43,49,63] views within-domain analogical reasoning as a memory task in whichmemory supplies a source case containing an almost correct solution to the target problem. Hammond [32], for example,describes retrieval of plans based on semantic similarity to the target problem and modification of the retrieved plan tomeet the target goal. Ashley and Rissland [3] describe the use of case-based reasoning in law. Smyth, Cunningham and Keane[65] describe hierarchical case-based reasoning, and Aha, Breslow and Munoz-Avila [2] describe conversational case-based* Corresponding author.E-mail addresses: keith.mcgreggor@gatech.edu (K. McGreggor), mkunda@gatech.edu (M. Kunda), goel@cc.gatech.edu (A. Goel).http://dx.doi.org/10.1016/j.artint.2014.05.0050004-3702/© 2014 Elsevier B.V. All rights reserved.2K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23Fig. 1. Problem similar to those of the Raven’s Standard Progressive Matrices test.reasoning. Our own work on case-based reasoning has focused in integration of case-based and model-based reasoning foradaptive design [30].Another line of research views analogy as a mechanism of perception, where one situation is recognized in terms ofanother [10,34,56] or as a mechanism of learning, where one situation is interpreted in terms of another [39]. Yet anotherline of research on analogy pertains to visual analogy [13–17,69,70]. In visual analogy, the source case and the targetproblem contain only modal, visual knowledge, and causality is (at most) implicit. For example, Yaner and Goel [69] describea technique for retrieving design drawings from memory that are similar to a target drawing; Davies, Goel and Yaner [16]describe the technique of constructive analogy for incrementally transferred from the source drawing to the target drawing;and Yaner and Goel [70] describe the technique of compositional modeling that builds a causal model of the target drawingby analogy to the causal model of the source drawing.Each of these schools of thought emphasizes the importance of certain aspects of analogy making, and, in turn, estab-lishes certain criteria that must be achieved by any associated methodologies through some combination of mechanismand representation. Our work concerns visual analogy, the act of forming analogies based upon purely visual perceptions,or, more formally, upon a purely visual perceptual history. We propose a new representation – the fractal representation– and corresponding mechanism for addressing a class of visual analogies that occur in computational psychometrics, andin particular, on the Raven’s Progressive Matrices test of intelligence. Although we focus our remarks on representation,visual analogy, and psychometrics, we expressly make no claims as to whether our model may be extended to provide fora cognitive account.1.1. Computational psychometricsAI research on computational psychometrics dates at least as far back as Evans’ Analogy program [21], which addressedgeometric analogy problems on the Miller Geometric Analogies test of intelligence. Bringsjord and Schimanski [7] haveproposed computational psychometrics, i.e., AI that can pass psychometric tests of intelligence, as a possible mechanism formeasuring and comparing AI.Raven’s Progressive Matrices Test suite is a set of standard and common tests of intelligence [61]. The standard versionof the test consists of 60 geometric analogy problems. Fig. 1 illustrates a problem typical to those that appear on the test.1The task in the problem is to pick one of the eight choices in the bottom of the figure for insertion in that bottom-rightelement of the 3 × 3 matrix in the top of the figure. The chosen element should best match the patterns in the rows andcolumns of the matrix.The Raven’s Progressive Matrices (RPM) test paradigm is intended to measure eductive ability, the ability to extract andprocess information from a novel situation [61]. Eductive ability stands in contrast to reproductive ability, which is theability to recall and use previously learned information.The problems from Raven’s various tests are organized into five sets. Each successive set is generally interpreted to bemore difficult than the prior set. Some of the problem sets are 2 × 2 matrices of images with six possible answers; theremaining sets are 3 × 3 matrices of images with eight possible answers.1 Throughout this paper, we use example problems that are similar to those found on Raven’s tests, due to copyright concerns and to ensure the integrityof the tests themselves. The results we report, however, are from the actual test problems.K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–233The tests are purely visual: no verbal information accompanies the tests. The test-taker is asked to select from theavailable possible answers the single answer that best completes the matrix [61].1.2. Prior approaches to RPMOver the years, different models have proposed various combinations of representations and mechanisms for solvingRPM problems. Hunt [38] gives a theoretical account of the information processing demands of certain problems fromthe Advanced Progressive Matrices (APM), in which he proposes two qualitatively different solution algorithms—“Gestalt,”which uses visual operations on analogical representations, and “Analytic,” which uses logical operations on conceptualrepresentations.Carpenter, Just, and Shell [9] describe a computational model that simulates solving RPM problems using propositionalrepresentations. Their model is based on the traditional production system architecture, with a long-term memory contain-ing a set of hand-authored productions and a working memory containing the current state of problem solving (e.g. currentgoals). Productions are based on the relations among the entities in an RPM problem, for example, the location of the darkcomponent in a row, which might be the top half in the top row of a problem, bottom-half in the bottom row, and so on.They did not test their system on the Standard Progressive Matrices (SPM), but two different versions of their system solved23 and 32 out of 34 attempted problems on the APM.Bringsjord and Schimanski [7] used a theorem-prover to solve selected RPM problems stated in first-order logic, thoughno results from this effort were reported.Lovett, Forbus and Usher [51] describe a model that extracts qualitative spatial representations from visually segmentedrepresentations of RPM problem inputs and then uses the analogy technique of structure mapping to find solutions and,where needed to achieve better analogies, to regroup or re-segment the initial inputs to form new problem representations.Again, while visual information from the RPM problems is implicit in the final representations, the structure-mapping engineis applied to these representations without any commitment to the visual nature of the encoded information. This systemwas tested against sets B through E of the SPM and correctly solved 44 out of 48 attempted problems.Cirillo and Ström [11] created a system for solving problems from the SPM that, like that of Lovett et al. [51], takesas inputs vector graphics representations of test problems and automatically extracts hierarchical propositional problemrepresentations. Then, like the work of Carpenter et al. [9], the system draws from a set of predefined patterns, derived bythe authors, to find the best-fit pattern for a given problem. This system was tested against Sets C through E of the SPMand solved 8, 10, and 10 problems, respectively.Schwering et al. [64] describe a gestalt method for addressing RPM problems. Rasmussen and Eliasmith [60] used aspiking neuron model to induce rules for solving RPM problems. Ragni, Stahl and Fangmeier [59] analyze the cognitivecomplexity of RPM problems. However, none of these report results from the addressing problems on any set of RPMproblems.Our work on the RPM was initially motivated by trying to understand cognition in autism [45]: some individuals on theautism spectrum show RPM scores much higher than what would be predicted by their performance on other standard-ized IQ tests [18]. A recent neuroimaging study compared task-related brain activity between neurotypical individuals andindividuals diagnosed with autism while solving RPM problems and found that, while both groups showed brain activationacross a distributed network of regions, the individuals diagnosed with autism showed relatively greater activation in oc-cipital regions, which are associated with visual processing, and relatively lower activation in prefrontal regions, associatedwith working memory and decision-making [66]. We analyzed behavioral and neuroimaging data across a range of cognitivetasks, including the RPM, and found that many empirical results cited in favor of other cognitive accounts of autism couldbe explained equally well by the hypothesis that certain individuals with autism are disposed towards thinking visually [45].Thus, we hypothesized that it could be both feasible and useful to develop a computational model of RPM that used solelyvisual representations. Our first model, called the “affine” or “ASTI” model [46–48] operated directly on scanned image in-puts from the test and used affine and set operations based on mental imagery (rotations, translations, image composition,etc.) to induce image transformations between images in the problem matrix and then predict an answer image basedon the final induced transformation. We tested the first ASTI model on all 60 problems from the SPM and found that itcorrectly solves 38 of the problems.1.3. Outline of the argument in this paperWe divide our argument for the use of fractal representations and reasoning for visual analogy into three parts. In thefirst part, we describe the nature of visual analogies and develop fractal representations. Next, we exhibit a computationalmodel for fractal analogical reasoning and illustrate its use through a detailed example. Lastly, we describe the performanceof our model on all major variants of the Raven’s Progressive Matrices tests – SPM, APM, CPM, and SPM Plus – and discussthe implications and next steps. We show that the performance of the fractal computational model matches or exceeds thatof any published account. We note that all the source code and images used in these examples are readily available on ourresearch group’s website.4K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–232. Visual analogies and representationsFig. 2. An example of visual analogy.Suppose we have a visual analogy, expressed symbolically as A : B :: C : D, with the symbols representing images, asshown in Fig. 2. We can interpret this as suggesting that some operation T exists which captures the relationship betweenimage A and image B (“ A is to B”). Likewise, some other operation Tis proposed which captures the relationship betweenimage C and image D (“C is to D”).(cid:3)In this manner, we may see that the analogy in such a problem rests not with the images themselves, but in the degreeto which the two operations T and Tare analogous. We can express the problem to make plain this distinction thus:(cid:3)A : B :: C : D −→ T ( A, B) :: T(cid:3)(C, D)2.1. Similarity between operationsThe nature of this similarity may be determined by a number of methods, many of which might associate visual orgeometric features to points in a coordinate space, and compute similarity as a distance metric. Tversky developed analternate approach by considering objects as collections of features, and similarity as a feature-matching process [68].We adopt Tversky’s interpretation of similarity, and thus seek to express these operations T and Tin some represen-tation which both is robust and affords sufficient feature production to permit feature matching [4]. A particular nuanceof Tversky’s approach, however, is that either the representation or the features derived from the representation must beformable into sets, as the calculation for similarity employed requires the counting of elements within sets (and their unionand intersection).Thus, we may revisit the typical visual analogy A : B :: C : D, where T and Tare now representations which meetTversky’s featural requirement. To make a comparison between the two representations, we first derive features from each,and then calculate a measure of similarity based upon those features.(cid:3)(cid:3)2.2. Similarity metricWe desire a metric of similarity that is normalized so that the value 0.0 means entirely dissimilar and the value 1.0means identical. Accordingly, we use the ratio model of similarity as described by Tversky [68], wherein the measure ofsimilarity between the two representations T and Tis calculated thus:(cid:3)(cid:5)(cid:2)(cid:3)(cid:3)(cid:2)(cid:3)(cid:2)(cid:3)(cid:2)(cid:4)(cid:2)(cid:3)ST , T(cid:3)= F(cid:3)T ∩ T/FT ∩ T(cid:3)+ αFT − T(cid:3)+ βF(cid:3) − TTwhere the operator F(Y ) derives the number of features in some set Y . The particular sets involved may be considered asindicating, respectively, those features the two representations share (T ∩ T), andthose features in T), those features in T but not in Tbut not in T (T(cid:3) − T ).(T − T(cid:3)(cid:3)(cid:3)(cid:3)Tversky [68] notes that the ratio model for matching features generalizes several set-theoretical models of similarityproposed in the psychology literature (e.g. [8] and [31]), depending upon which values one chooses for the weights αand β. Later in this discussion, we shall revisit these weights, and illustrate the significance of their choice.3. Fractals and the fractal representationBenoit Mandelbrot coined the term “fractal” from the Latin adjective fractus and its corresponding verb (frangere, “tobreak” into irregular fragments), in response to his observation that shapes previously referred to as “grainy, hydralike,in between, pimply, pocky, ramified, seaweedy, strange, tangled, tortuous, wiggly, wispy, wrinkled, and the like” could bedescribed by a set of compact, rigorous rules for their production [52]. Indeed, the computer graphics community hasgenerated fractal imagery for several decades.While the techniques for generating artificial fractal imagery are quite well known, as Mandelbrot noted many images ofreal-world artifacts appear to have “fractal” properties. If these images are “fractal” in some sense, then what formula mayunderlie these images?K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2353.1. The mathematical basis for fractals as operationsAs we have noted previously [55], the mathematical derivation of fractal representation as an operation over imagesexpressly depends upon the notion of real world images, i.e. images that are two dimensional and continuous [5]. Everyimage received by the human visual system may be construed as meeting this requirement, with the proviso that the notionof continuity has a resolution limit, and that limit plays a significant role in visual abstraction, as shall be discussed later inthis paper.Of particular relevance are two observations about images themselves, made by Mandelbrot [52]. The first is that all nat-urally occurring images appear to have similar, repeating patterns. The second observation is that no matter how closely oneexamines the real world, one may find instances of similar structures and repeating patterns. These observations suggestedto Barnsley and Hurd [5] that it is possible to describe images in terms that capture the observed similarity and repetitionalone, without regard to shape or traditional graphical elements.3.2. Collage theoremComputationally, the determination of the fractal representation of an image can be performed through the use of thefractal encoding algorithm. The collage theorem [5] at the heart of the algorithm can be stated concisely:For any particular real world image, there exists a finite set of affine transformations which, if applied repeatedly and indefinitelyto any other real world image, will result in the convergence of the latter into the former.It is important to note that the collage theorem is describing a set of transformations that are derived by mapping an imageinto another. In other words, fractal encoding determines an iterated function system that is applied repeated to somesource image, with the result that the encoded image emerges.Suppose F () is the fractal encoding of image B. Then, given any other image A:.= BF ( A2) = A3... and so on, untilF ( A1) = A2,F ( A) = A1,F ( A∞)F () is itself a finite set of affine transformations T that describe how to modify portions of an image such that convergenceis assured.F () == T == {T 1, T 2, T 3, ..., Tn}Each affine transformation may affect some or all of the given image, but it is the unordered union of their actions thatcomprises the resultant image. Thus:F ( A) = T ( A) =(cid:6)T i( A), 1 ≤ i ≤ n3.3. DependenciesThere are several, interrelated dependencies implied in the Collage theorem. These are specificity, partitioning, andsearch. We shall describe each in turn now, in the context of the theorem, and later, in the context of the algorithmand the subsequent representation.3.3.1. Dependency upon the specificity of the source and the destinationA fractal encoding is dependent not only upon the destination image, but also upon the source image, from which the setof affine transformations T is discovered. However, once the fractal encoding has been determined, the application of thatencoding to any source image will result in the target image. This dependency is to suggest that, a priori any application ofthe encoding, a particular fractal encoding is determined uniquely by a particular source image.3.3.2. Dependency upon the partitioning of the destination imageThe cardinality of set of transformations is determined exactly and solely by the partitioning scheme chosen for theimage being encoded. It is presumed that the image being encoded admits to being partitioned in some manner, however.Images may be partitioned using a variety of methods. In computer vision, one typically seeks to segment an image intoregions or shapes [62,71]. Another segmentation scheme would seek to segregate an image into two segments, a foregroundand a background [41]. Other partitionings of images may be regular, such as the division of computer-based images intopixels, at some resolution. It must be noted that the choice of partitioning scheme affects the computational complexity ofenacting the partitioning.The Collage theorem imposes no constraint upon the choice of partitioning save one, and that is that the union of allpartitions wholly covers the image to be encoded. From a topological perspective, the image B is treated as a set, and thepartitioning P () of that image into a finite collection of subsets is a cover of that set if:6K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23First, choose a partitioning scheme P to systematically divide the destination image B into a set of images, such that B ⊆ {b1, b2, b3, . . . bn}.For each image bi :· Search the source image A for an equivalent image fragment ai such that an affine transformation of ai will likely result in bi .· Collect all such transforms into a set of candidates C .· Select from the set C that transform which minimally achieves its work, according to some predetermined metric.· Let T i be the representation of the chosen transformation associated with bi .The set T = {T 1, T 2, T 3, . . .} is the fractal encoding of the image B.Algorithm 1. Fractal encoding of B in terms of A.P (B) = {b1, b2, b3, ..., bn}B ⊆bi, 1 ≤ i ≤ n(cid:6)3.3.3. Dependency upon the search of the source imageThe essential implied step of the Collage theorem is that there is a match for each subimage of the destination, as deter-mined by the partitioning, to be sought within the source image. Through this searching process, the affine transformationfor that subimage is obtained. However, the quality and character of the match, as well as the computational complexityof the algorithm, depends upon the constraints selected for comparing the destination subimage with some portion of thesource.3.4. Fractal encoding algorithmGiven a target image B and a source image A, the fractal encoding algorithm seeks to discover this particular set oftransformations T .As can be seen in Algorithm 1, the fractal encoding of image B in terms of image A consists of two phases: partitioningand searching. We shall now discuss each phase.3.4.1. PartitioningThe target image B is first partitioned into a set of other images. As we are dealing with computer images, we maysafely assume that the image B will have some finite resolution, and thus a limit as to the smallest achievable partition.Practically, this smallest resolvable unit is a unitary pixel, which would denote both a spatial location and a photometricvalue.As we noted above, the Collage theorem places a topological constraint on the partitioning scheme, and requires thatthe scheme should act as a topological cover over the image B. Such a constraint admits a wide variety of methods forpartitioning the image, but many of these partitionings may prove computationally expensive.In the interest of reducing computational complexity, we may impose two additional constraints: each subimage in thepartition must be simply connected to at least one other subimage, and the union of all of the subimages must be exactlyequivalent to the image B. Stated formally:P (B) = {b1, b2, b3, ..., bn} is a valid partitioning iff:∀i∃ j (cid:11)= i : simplyconnected(bi ∪ b j),B ≡bi, 1 ≤ i ≤ n(cid:6)andwheresimplyconnected(X) → (cid:2)x, y ⊂ X : x ∩ y = ∅One computationally inexpensive way to achieve such a constrained partitioning is to impose upon the image B a uniform,rectilinear grid, and select the subimages based upon some chosen grid size, as expressed in units of pixels.Thus, a stronger specification of the fractal encoding T may be thought of as a function of three variables, the sourceimage A, the target image B, and the partitioning scheme P :T ( A, B, P ) = {T 1, T 2, T 3, ..., Tn}where the cardinality of the resulting set is determined solely by the partitioning P . That is, each subimage bi that Pextracts from B will be represented by exactly one element of the set T .K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–237Fig. 3. Partitioning as levels of detail.Fig. 4. Local and global coordinates.3.4.2. Partitioning and level of detailChoosing a partitioning determines the level of detail at which an image is encoded. Thus, the coarsest level of detailpossible for an image is the partitioning into a single image (the whole image). The finest level of detail achievable is thatset of images wherein each image is but a single pixel.The choosing of a grid size, and of a partitioning in general, may be interpreted as an indication of the level of detail atwhich an image may be encoded. Fig. 3 illustrates the effect of partitioning an image into a variety of levels of detail, usinga regular rectangular grid.The ability to express level of detail as an artifact of partitioning, whether by controlling grid size, by altering the consis-tency of partition size, or by modification of the shape and nature of the underlying regions and their spatial arrangement(i.e. hexagonal versus rectilinear scaffolding, or polar versus Cartesian coordinates) is an important aspect of the encoding,and a key feature entailed by the fractal representation.3.4.3. SearchingThe partitioning scheme P extracts a set of images bi from the target image B. The next step of the algorithm is toperform a systematic examination of the source image A for fragments of A that can be said to best match a particularimage bi . The method by which the search is conducted may be varied, as can the meaning of what is said to be a “bestmatch.”3.4.3.1. Global and local coordinates An image bi extracted by the partitioning scheme can be considered as a region con-taining a number of pixels that are addressable in some fashion. The addressability of these pixels may be viewed as alocal coordinate system imposed upon the region. Additionally, the region described by the image bi has a location andorientation within the image B, strictly determined by the partitioning scheme. Thus, we may consider the image bi as anordered set of pixels, having both a local (intrinsic) coordinate system and extent, and a position and orientation within aglobal (within image B) coordinate system. Fig. 4 illustrates the local and global coordinate systems.However, the same partitioning scheme necessarily does not need to be applied to the source image. The entire sourceimage A may be examined in any manner for a fragment that most closely matches bi .8K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23Let C ← 0.For each pixel x ∈ bi and corresponding pixel y ∈ ak :C ← C + (Photometric(x) − Photometric( y))2The value C is then the photometric correspondence between ak and bi .Algorithm 2. Photometric correspondence.3.4.3.2. Discovering the “best match” The source image A is examined to determine which fragment of it, which we shalllabel ak, can be said to “best match” the sought-for image bi from the target image B. That is, the correspondence betweenak and bi can be said to be “best” if it is the minimum value of the following function:Correspondence(ak, bi) = PhotometricCorrespondence∀ak ⊂ A, t ∈ AdmissibleTransformations(cid:2)Transform(ak, t), bi(cid:3)where AdmissibleTransformations is a finite set of spatial transformations applied by the operator Transform() to the pixelvalues contained within ak, and PhotometricCorrespondence() is a pixel comparison operation. We shall define both of thesemomentarily.3.4.3.3. Photometric correspondence The photometric correspondence between the fragment ak from the source image andbi from the destination image is calculated to be the difference between the photometric values found in those fragmentsunder a given alignment of their pixels. We wish to propose a metric to ensure that this difference would be 0 if thetwo fragments were identical photometrically. Such an algorithm to calculate the photometric correspondence is given byAlgorithm 2.The corresponding pixel in ak is determined by imposing the same local coordinate system used in bi upon ak.The Photometric value of a pixel used in this calculation may vary according to the nature of the image itself. Forexample, if the image is in full color, the photometric value may be a triplet of actual values; if the image is monochromatic,then the photometric value will be single valued. Since it is desired to calculate a photometric correspondence that issingle-valued, a mapping from multivariate photometry to a single value is typically employed. This can be seen, globally, asmapping from one color space into another. For example, to reconcile traditional computer graphics images given in tripletsof red, green, and blue values into single grayscale values, a formula such as this may be used, which seeks to equate thecolorimetric luminance of the R G B image to a corresponding grayscale rendition [54]:Photometric(cid:2)(cid:3)(cid:18)R, G, B(cid:19)= 0.3R + 0.59G + 0.11BCareful consideration of the underlying photometric nature of the image being encoded therefore must be given, but onlyat this particular moment in the overarching algorithm for encoding. The choice of the Photometric() function determinesthe interrelationship of the image’s colorimetry and its constituent importance to the matching function.3.4.3.4. Affine transformations The fractal encoding algorithm seeks to find the best matching fragment in a source imagethat corresponds to a given image partitioned from the target image. As shown above, this matching is achieved by calcu-lating the photometric correspondence function between two fragments, while considering all admissible transformationsof the fragment from the source. The set of admissible transformations is a subset of affine transformations known assimilitude transformations.An affine transformation, in two dimensions, may be considered to be of the form:W (x, y) = (ax + by + e, cx + dy + f )where a, b, c, d, e, and f are all real numbers. This equation, which maps one point in a two-dimensional plane into anotherpoint in a two-dimensional plane, may be rewritten into matrix form like so:(cid:10)(cid:8) (cid:9)(cid:10)(cid:9)(cid:7)(cid:2)(cid:3)W(cid:18)x, y(cid:19)=a bc dxy+efIn this way it can be seen that an affine transformation is a combination of a linear transformation followed by a translation.Not all affine transformations are admissible for the fractal encoding transform, however. In particular, those that areadmissible must be invertible [5]. Intuitively, this means that each point in space can be associated with exactly and onlyone other point in space. Mathematically, this means that the inverse has this form:−1(x, y) = (dx − by − de + bf , −ex + ay + ce − af )/(ad − bc)Wand the denominator must not be equal to zero to satisfy invertibility.K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2393.4.3.5. Similitude transformations An important group of affine transformations are those that are called similitudes [5].Fig. 5. Eight similitude transformations.A similitude transformation may be expressed in one of these two forms:(cid:8) (cid:9)(cid:10)(cid:10)(cid:9)(cid:7)(cid:2)W(cid:2)W(cid:3)(cid:18)x, y(cid:19)=(cid:3)=(cid:18)x, y(cid:19)r cos θr sin θr sin θ −r cos θ(cid:7)r cos θ −r sin θr cos θr sin θ(cid:8) (cid:9)xy(cid:10)xy++(cid:9)(cid:10)efefThus, a similitude transformation is a composition of a dilation factor r, an orthonormal transformation (a rotation aboutthe angle θ where 0 ≤ θ < 2π ), and a translation (e, f ). Similitude transformations are invertible except when r = 0.3.4.3.6. Defining the AdmissibleTransformations set Given this formulation for similitude transformations, one can imaginehaving to consider a great many potential rotational angles to find the best match. Indeed, the computational complexity ofthe encoding would seem a function of the angles under consideration. In practice, we find that we may limit ourselves toconsidering only eight of these orthonormal transformations, as shown in Fig. 5.Consider the smallest region of pixels for which orthonormal transformations upon those pixels would result in a visiblechange. The size of this region is an area two pixels wide by two pixels high. This small region has four lines of symmetry.Taking into account each line of symmetry, and reflecting the pixels in the region about each in turn, we find that there areeight possible outcomes.Our implementation of the fractal encoding algorithm examines each potential correspondence under each of thesepossible transformations. These form the set of admissible transformations. The transformation from this set that yields thebest photometric correspondence is noted by the search algorithm.3.4.3.7. Translation arises from searching The searching process examines each potential fragment in a given source image forcorrespondence to a particular fragment of the target image. Let us presume that we may align the coordinate systems ofthe source and the target images such that their origins exactly coincide. Then, the relative location of a potential fragmentin the source image can be mapped to a location within the target image. This mapping, from the potential fragment’s localorigin to the particular fragment’s local origin, is a translation, and it is this mapping that forms the translation portion ofthe sought-for similitude transformation.3.4.3.8. Dilation and fractals Taken together, the orthonormal transformation and the translation provide a sufficient meansfor describing self-similarity that may exist within an image. However, that self-similarity is not quite sufficient for describ-ing how the similarity may occur at different levels of detail. The dilation factor, r, is used to invoke a contraction of space,whenever r < 1.0. The fractal encoding algorithm prescribes that the dilation factor to be used when searching may beconveniently set as r = 0.5. In practice, this entails that the source image, as a whole, may be scaled to one-half its originalsize, and then searched for photometrically corresponding fragments.Mathematically, choosing r < 1.0 ensures that the encoding derived for the entire image, if applied successively andindefinitely to an image, will cause the resulting image to converge upon the desired destination image [5].3.4.3.9. Colorimetric contraction As a final step, having located the best photometrically corresponding source fragment, thealgorithm determines a rate at which the two regions may be brought into colorimetric harmony. To do this, the averagecolorimetric description of both regions is calculated, and the distance between the two is multiplied by a dilation. Theformula our present implementation uses to calculate the colorimetric contraction is:colorContraction(ak, bi) = 0.75 ∗(cid:2)(cid:3)colorMean(bi) − colorMean(ak)where the colorMean of a region is the average of all colorimetric information available in that region, taking into accountthe multivariate nature of the underlying image as previously discussed. The derivation of the colorimetric dilation factor of0.75 is given by Barnsley and Hurd [5], and is shown to be correlated to the spatial dilation factor of 0.5.10K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23Table 1Elements of a fractal code.Spatialsx, s ydx, d yTSSource fragment originDestination fragment originOrthonormal transformationSize/shape of the regionPhotometricCOpColorimetric contractionColorimetric operation3.4.3.10. Exhaustive searching The search over the source image A for a matching fragment is exhaustive, in that each pos-sible correspondence ak is considered regardless of its prior use in other discovered transforms. By allowing for such reuse,the algorithm affords the first Mandelbrot fractal observation, the notion of repetition.3.4.3.11. Refining correspondence We note that there may be many fragments in the source image which may have identicalphotometric correspondence to the sought for fragment bi (for example, when all of the values in the two fragments areidentical). To break these potential ties, a further refinement of the correspondence function is necessary.We compute a simple distance metric upon the images, and give it a weighting. Thus, the correspondence calculatedbetween two fragments becomes:Correspondence(ak, bi) = w 1PhotometricCorrespondence(cid:2)(cid:3)Transform(ak, t), bi+ w 2Distance(ak, bi)∀ak ⊂ A, t ∈ AdmissibleTransformationswhere the weights w 1 and w 2 are chosen such that the calculation of correspondence is dominated by the value of thephotometric correspondence. This can be ensured if the following relationship is held:w 2 maximalDistance (cid:20) w1 minimalJustNoticeablePhotometricwhere maximalDistance is the longest possible distance between the origins of bi and any fragment in the correspondingsource image, and minimalJustNoticeablePhotometric is the PhotometricCorrespondence which would be calculated if thephotometric difference between bi and any fragment were so small as to be indistinguishable. Practically, we set this valuesuch that this is as small as possible yet not zero, given the color system used in the images. For example, for 8-bit greyscaleimages where the value 0 represents “black” and the value 255 represents “white,” the minimalJustNoticeablePhotometricwould be set to a value of 1.3.4.4. Fractal codesFor each image bi taken from a partitioning of the target image B, the fractal encoding algorithm locates, via exhaustivesearch over the source image A, a corresponding fragment ak which the algorithm has deemed to be most minimallydistant photometrically under a discovered transformation. The algorithm constructs a description of its discoveries, in arepresentation called a fractal code. A fractal code consists of six elements, as shown in Table 1.Note that the dilation factor, for both spatial and photometric properties, is not represented here. This is for efficiency,as these dilations are presumed to be global.Further efficiencies of expression also may be found by dropping the colorimetric operation (a way of describing how thecolorimetric contraction value is to be combined into the region). Since the set of orthonormal transformations the searchmechanism uses is finite, we may represent the transformation as a referent to that transformation’s ordinal membershipin the set. The size and shape of the region may be reduced itself, if the partitioning of the image is regular. In ourimplementation, we use a regular, uniform partitioning, which forms a grid. Thus, we can express the size and shape of theregion with a single integer, which represents the width and height of the region in pixels.3.4.5. Arbitrary selection of sourceThe choice of source image A is arbitrary. Indeed, the target image B may be fractally encoded in terms of itself, bysubstituting B for A in the above algorithm. Although one might expect that this substitution would result in a trivialencoding (in which all fractal codes correspond to an identity transform), this is not the case, a fractal encoding of B willconverge upon B regardless of chosen initial image. For this reason, the size of source fragments considered is taken tobe twice the dimensional size of the target image fragment, resulting in a contractive affine transform. Similarly, as shownabove, color shifts are made to contract. This contraction, enforced by setting the dilation of spatial transformations at 0.5,provides the second key fractal observation, that similarity and repetition occur at differing scales.3.4.6. Arbitrary ordinality of encodingThe ordinality of the set of fractal codes that comprise a fractal representation is similarly arbitrary. The partitioning Pmay be traversed in any order during the matching step of the encoding algorithm. Similarly, once discovered, the individualcodes may be applied in any order, so long as all are applied in any particular iteration.K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23113.4.7. Fractal representation is fractal encodingThe fractal encoding algorithm, while computationally expensive in its exhaustive search, represents the relationshipbetween two images (or between an image and itself) as a much smaller set of fractal codes, an instruction set for recon-stituting the relationship, with inherently strong spatial and photometric correspondence. It is through this encoding thatthe fractal representation of the relationship between those two images is derived. Indeed, the fractal representation is thefractal encoding.3.5. Features from fractalsThe fractal representation of an image is an unordered set of fractal codes, which compactly describe the geometricalteration and colorization of fragments of the source image that will collage to form the target image. While it is temptingto treat contiguous subsets of these fractal codes as features, we note that their derivation does not follow strictly Cartesiannotions (e.g. adjacent material in the destination might arise from non-adjacent source material). Accordingly, we considereach of these fractal codes independently, and construct candidate fractal features from the individual codes themselves,and not from clusters of codes.Each fractal code yields a small set of features, formed by constructing subsets of its underlying six-tuple. These featuresare determined in a fashion to encourage spatial- and photometric-agnosticism, as well as specificity. Our algorithm createsfeatures from fractal codes by constructing subsets of each of the six members of the fractal code’s tuple.We further chose to represent each feature as a concatenated string in memory. We form these strings by attaching acharacter tag to each field in the fractal code and then converting that field into string format prior to concatenation, likeso:sx, s y(source fragment origin) −→ Ssxs y(string representation)The choice of the particular tag is arbitrary, but tagging itself is not: tagging is necessary to avoid in-string matchingbetween the different kinds of fields (e.g. a numerical value may appear in multiple fields of a fractal code). Doing soattributes a world grounding to each field, and collectively to the entire fractal code.3.6. MutualityThe analogical relationship between source and target images may be seen as mutual; that is, the source is to thedestination as the destination is to the source. However, the fractal representation is decidedly one-way (e.g. from thesource to the destination). To capture the bidirectional, mutual nature of the analogy between source and destination, wenow introduce the notion of a mutual fractal representation. Let us label the representation of the fractal transformationfrom image A to image B as T A B . Correspondingly, we would label the inverse representation as T B A . We shall define themutual analogical relationship between A and B by the symbol M A B , given by this equation:M A B = T A B ∪ T B ABy exploiting the set-theoretic nature of fractal representations T A B and T B A to express M A B as a union, we afford themutual analogical representation the complete expressivity and utility of the fractal representation.3.7. Extended mutualityWe note that the mutual fractal representation of the pairings may be employed to determine similar mutual repre-sentations of triplets, quadruplets, or larger groupings of images. As a notational convention, we construct these additionalrepresentations for triplets (Mi jk) and quadruplets (Mi jkl) in a like manner:Mi jk = Mi j ∪ M jk ∪ MikMi jkl = Mi jk ∪ Mikl ∪ M jkl ∪ Mi jlThus, in a mutual fractal representation, we have the necessary apparatus for reasoning analogically about the relationshipsbetween images, in a manner that is dependent upon only features that describe the mutual visual similarity present inthose images.4. A strategy for visual analogiesAs we noted at the paper’s outset, we can interpret visual analogies as suggesting that some operation T exists thatis proposed whichcaptures the relationship between image A and image B (“ A is to B”). Likewise, some other operation Tcaptures the relationship between image C and image D (“C is to D”).(cid:3)Let us now consider a class of visual analogy puzzles, an example of which is shown in Fig. 6.12K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23Fig. 6. A visual analogy puzzle.In this problem, the image D is missing, and the challenge is to determine which of the offered candidate images wouldbest fit into the matrix. That is, we must determine which of these candidate images, if selected as image D, would establishtransformation Tas most analogous to transformation T .(cid:3)Analogies in a general sense are based on similarity and repetition e.g. [34]. We would seek to employ a suitable rep-resentation, one that affords the capture of these qualities as well as sanctions reasoning over them. As we showed in theprior sections of this paper, fractals capture self-similarity and repetition at multiple scales [52], and we propose that fractalrepresentations are an appropriate choice for addressing certain classes of analogy problems.One method for solving this puzzle is this: from this set of candidates, we may form the fractal representations from thefractal encoding of the transformation of each candidate image X in terms of image C .∀ X ∈ {candidate answers},Ω = {T 1, T 2, T 3, T 4, . . . Tn} and T(cid:3) ∈ ΩT x := FractalEncode(C, X)This provides a set of possible transformations, which we shall label Ω , from which to seek the most analogous transfor-mation Tand thereby find which candidate image was responsible for it.(cid:3)4.1. The generality of representationsWe wish to note that while in this paper we exclusively shall use fractal representations in our examples and subsequentdiscussion, our overall approach is agnostic with respect to representations, and may be used with any representation thataffords the ability for objects thus represented to be decomposed into a set of features. The approach is distinguished fromother analogical algorithms in that it presumes no explicit relationship between objects or between features of objects.4.1.1. Preliminary remarksOur approach compares each transform in the set Ω to the original transform T by means of recalling common featuresand calculating similarity metrics. First, however, we must make a few remarks on our present implementation. We chosedata structures that facilitate the storage and retrieval of information based upon aspects of the data, specifically by usinga hash table as a data structure surrogate for memory. As we will be hashing transformations into memory, we define twoadditional operators: F (), a method to generate a set of features from a given transformation; and K (), an injective hashfunction which operates solely over the domain of the features.We made the commitment to a hash table for two reasons beyond that of wishing to use features. First, we note that itis desirous to find some overlap in the features that occur between two transformations, such that a perfect overlap woulddeem the transformations perfectly analogous. The hash function K () may result in hashing multiple transformations to thesame feature, and therefore K () must operate only upon a given feature, and not take into consideration the transformationwhich gave rise to that feature. Second, F (), the method which generates features from a transformation, must do so in amanner such that each generated feature affords salience, or information content [68].4.1.2. Indexing featuresWe wish to store each transformation in the hash table memory M. The set of possible analogous transformations Ω iscombined with the original transformation T to form a new set Ω ∗, andfrom each member calculates a set of features using F (τ ). For each feature f i ∈ F (τ ), the transformation is indexed as anordered pair (K ( f i), τ ). That there likely will be hash collisions at key value K ( f i) is expected and desired.. The algorithm iterates over each member τ ∈ Ω ∗4.1.3. Calculating similarityWe must determine a measure of similarity between the original transformation T and each possible analogous transfor-mation T i ∈ Ω . Our choice of metric reflects similarity as a comparison of the number of features shared between candidatepairs taken in contrast to the joint number of features found in each pair member [30]. We desire a metric that is normal-ized with respect to the number of features under consideration. In our implementation, the measure of similarity betweenthe target transform T and a candidate transform T i is calculated using the ratio model [68]:K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2313S(T , T i) = F(T ∩ T i)/(cid:2)(cid:3)F(T ∩ T i) + αF(T − T i) + βF(T i − T )Fig. 7. The primary and candidate transformations.and F(Y ) is a function that determines the number of features that may be extracted from the set Y . We may calculatethese values effectively, using hash table retrieval as a surrogate for distinguishing and counting common and distinctfeatures within the sets T ∩ T i , T − T i , and T i − T respectively.Tversky notes that the ratio model for matching features generalizes several set-theoretical models of similarity proposedin the psychology literature, depending upon which values one chooses for the weights α and β [30]. We have found thatsignificant discrimination between candidate answers may be found by using the Jaccard similarity; that is, by settingα ← β ← 1.0, and thus favoring features from either transformation equally. As Tversky notes, by equating α and β, weensure that the calculation of similarity is symmetric with respect to the transformations under comparison.Once the algorithm has calculated the similarity function over all candidate transforms, it is a straightforward matter to, is deemed by way of Tverskydetermine which transformation has generated the maximal similarity. This transformation, Tfeatural similarity, to be the most analogous to the original transformation T .(cid:3)4.2. An exampleWe now present an example of using fractal representations and our strategy to solve the visual analogy puzzle shownin Fig. 3 above.4.2.1. The primary and candidate transformations(cid:3)In this example, the problem is to determine for which of the candidate images the transformation Tis made mostanalogous to transformation T . We first will represent T as a fractal representation, and then generate a set of candidatetransformations Ω as shown in Fig. 7.We arbitrarily may select any partitioning scheme, so long as that partitioning meets the criteria of coverage outlinedabove. For the purpose of this example, let us choose to partition each image into a series of 16 × 16 pixels, forming aregular grid. Each of our images is 134 pixels wide and 84 pixels high. Thus, each image is to be partitioned into 54 blocks.Each of these blocks will be represented by a single fractal code. In our present implementation, each fractal codegenerates 63 features. Therefore, at this partitioning, each primary transformation will be indexed into memory using the54 × 63 × 2 = 6804 features generated from mutual fractal representation of that transformation.4.2.2. Calculating similarities and selecting the most analogousAfter the primary transformation T is indexed into memory using features derived from the fractal codes, we maycalculate a similarity value for each of the candidate transformations in the set Ω , using the Tversky formula as noted.Table 2 illustrates the values calculated for each of the candidate transformations.It may be seen that the fourth transformation is the most similar to the primary transformation T , with a value of 0.842.Therefore, for this puzzle, the answer is candidate answer 4.5. Tackling Ravens with fractalsWith this strategy in mind, let us illustrate the use of fractal representations for solving Raven’s matrices problems. Weshall use by way of example the 3 × 3 matrix problem shown in Fig. 1.5.1. Simultaneous relationships, multiple constraintsAn aspect of any Raven’s problem, whether 2 × 2 or 3 × 3, is that there exist simultaneous horizontal and verticalrelationships which must be maintained by the selection of the most analogous answer. In a 2 × 2 problem, there is onehorizontal relationship and one vertical relationship that constrain the selection. In a 3 × 3 problem, there are two horizontaland two vertical relationships. In our implementation, we represent these relationships as mutual fractal representations.14K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23Table 2Candidate transformation similarities.TransformationSimilarity0.6237507350.5170487950.5529100530.842445620.5737801290.535273369Fig. 8. The simultaneous relationships.In Fig. 8, we illustrate these relationships using our example problem. As shown, relationships H1 and H2 constrainrelationship H , while relationships V 1 and V 2 constrain relationship V . We note that there are other possible relationshipsthat may be suggested by this problem: we have chosen to focus on these particular four relationships for clarity.To solve a Raven’s problem, one must select the image from the set of possible answers for which the similarity to eachof the problem’s relationships is maximal. For our example, this involves the calculation of a set of similarity values Θi foreach answer Ai :(cid:2)(cid:3)(cid:12)(cid:11)(cid:2)(cid:2)(cid:2)(cid:3)H1, H( Ai), S(cid:3)H2, H( Ai), S(cid:3)V 1, V ( Ai), SV 2, V ( Ai)Θi ←S∀i, 1 ≤ i ≤ 8where S( X, Y ) is the Tversky similarity between two sets X and Y , and H( Ai) and V ( Ai) denote the relationship formedwhen the answer image Ai is included.5.1.1. Reconciling multiple analogical relationshipsFor each candidate answer, we consider the similarity of each potential analogical relationship as a value upon an axisin a large “relationship space.” The dimensionality of this space is determined by the problem at hand. Thus, for a 2 × 2Raven’s problem, the space is 2-dimensional; for a 3 × 3 Raven’s problem, the space is 4-dimensional, using the relationshipsas shown above.We would like to determine a single value for the similarity. To do so, we treat these multidimensional sets as a vector,and determine its length, using a Euclidean distance formula:(cid:13)(cid:14)S i ←Θ 2i j∀i, 1 ≤ i ≤ 8 and ∀Θi j ∈ ΘiThus, the longer the vector, the more similar; the shorter the vector, the more dissimilar.Generally, we currently do not favor any particular relationship; that is, we do not, as an example, weight more de-cisively those values we find upon the horizontal relationships over those upon the vertical relationships. We note thatK. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2315giving preferential weighting to a relationship is a straightforward extension to the calculation above, but we also note thatchoosing which relationship to prefer may be non-trivial.5.2. ConfidenceAs illustrated by the example above, a candidate representation can be found to be the most analogous by a straightfor-ward calculation of its featural similarity, using the Tversky formula [68]. But a question quickly arises: how confident is theanswer? That is, given the variety of answer choices, even though an answer may be selected based on maximal similarity,how may that choice be contrasted with its peers as the designated answer?A potential path would be to assess the given similarity calculation as a member of the set of all such similarity calcu-lations for the collection of potential answers. Let us suppose that for a given collection of answers, we use a strategy suchas ours to calculate a corresponding set of similarity values:(cid:2)strategy{ A1, A2, A3, A4, ..., An}, problem(cid:3)→ {S1, S2, S3, S4, . . . , Sn}∀S i, 0.0 ≤ S i ≤ 1.0We may then select(cid:2)ζ ← max{S1, S2, S3, S4, . . . , Sn}(cid:3)as the maximal similarity value, and thereby deem the answer which generated that value as the most analogous. We maydetermine, additionally, how statistically distinct the value ζ is from its peers, by first calculating the mean, standard devi-ation, and standard error of the mean for the set of similarity values, and then, assuming a normal distribution, calculatingthe deviation of each of the values from the mean of the set:S iand σ =n−1(cid:14)(cid:16)(S i − μ)2∀i, 0 < i ≤ n(cid:13)(cid:15)μ = n−1σμ = σ /(cid:14)√nthenDeviations ← {D1, D2, D3, D4, ..., Dn}∀i, 0 < i ≤ n, D i = (S i − μ)/σμwhere the set Deviations is a t-distribution of the similarity values. We surmise that the most analogical answer, the onecorresponding to the maximal similarity value ζ , would have the largest positive deviation value under this reformulation:S x = ζ ← max(cid:2){S1, S2, S3, S4, . . . , Sn}(cid:2)(cid:3)iff(cid:3)∃x = y, D y = max{D1, D2, D3, D4, ..., Dn}This, then, suggests that the most analogical answer would in a sense “stand apart” from the rest of the answers. The degreeto which it “stands apart” may be interpreted as a metric of confidence in selecting the answer. Indeed, assuming a normaldistribution, we may calculate a confidence interval based upon the standard deviation, and score each of these values alongsuch a confidence scale, where 0.0 would indicate no variation at all from the answer, and 1.0 would indicate an utterlyapparent and distinct value. Thus, the problem of selecting the most analogous answer is transformed into a problem ofdistinguishing which of the possible answers is a statistical outlier.Since the set of deviations determined is a t-distribution of the similarity values, the choice of a level of confidence inselecting the answer can be made using conventional statistics. If we assume a normal distribution of the deviations, thenwe may devise an interval such that with an expected confidence of C for any deviation d, the probability of d in thatinterval is C :P (−z ≤ d ≤ z) = CThe set of deviations is a t-distribution of the similarity values. We know that if this is a normal distribution:P (−z ≤ d ≤ z) = Φ(z) − Φ(−z) = erf√z =2 erf−1(C)(cid:2)√(cid:3)z/2where Φ() is the cumulative normal distribution function and erf() is the error function. Therefore, given some value C , wecan calculate the equation above to determine the boundary.Let us suppose, as an example, that we wished to say with C = 90% that some answer Xi is the most analogous one.This would imply that the probability of that answer’s corresponding deviation D i obeys this:16K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23Given an image P containing a Raven’s problem, determine an answer.Problem SegmentationBy examination, divide P into two images, one containing the matrix and the other containing the possible answers. Further divide the matrix image intoan ordered set of either 3 or 8 matrix element images, for 2 × 2 or 3 × 3 matrices respectively. Likewise, divide the answer image into an ordered set ofits constituent individual answer choices.Let M := {m1, m2, ...} be the set of matrix element images.Let C := {c1, c2, c3, ...} be the set of individual answer choices.Let η be an integer denoting the order of the matrix image (either 2 or 3, for 2 × 2 or 3 × 3 matrices respectively).Relationship DesignationsLet R be a set of relationships, determined by the value of η as follows:If η = 2:R ← {H1, V 1} whereH1 ← MutualFractal(m1, m2)V 1 ← MutualFractal(m1, m3)Else: (because η = 3)R ← {H1, H2, V 1, V 2} whereH1 ← MutualFractal(m1, m2, m3)H2 ← MutualFractal(m4, m5, m6)V 1 ← MutualFractal(m1, m4, m7)V 2 ← MutualFractal(m2, m5, m8)Abstraction Level PreparationLet d be the largest pixel dimension for any image in the set M ∗ C .Let A := {a1, a2, ...} represent an ordered range of abstraction values wherea1 ← d, and ai ← 1/2ai−1 ∀i, 2 ≤ i ≤ (cid:22)log2 d(cid:23) and ai ≥ 2The values within A constitute the grid values to be used when partitioning the problem’s images.Algorithm 3. The Fractal Raven Algorithm, preparatory stage.√2 erf−1(0.90) = 1.644853z =Thus, if the deviation D i is larger than 1.644853, we can say with 90% confidence that the answer Xi is the most analogous.In our implementation, we chose a 95% confidence value, therefore we seek a deviation which is larger than 1.959964,approximately a 2-sigma signal.In this manner, we have the ability to determine mathematically not only which value may have the highest similarity,but also the degree to which that value is statistically distinct. We require this, for our approach considers the analogyproblem at an array of levels of abstraction. In addition, in this way may be seen the specific interplay of confidence andabstraction when solving problems of analogy, a novel aspect of our work.6. The Fractal Raven AlgorithmWe now present the Fractal Raven algorithm in pseudo-code form. We separate the algorithm into two parts: thepreparatory stage (see Algorithm 3) and the execution stage (see Algorithm 4).6.1. The Fractal Raven Algorithm: preparatory stageIn the first stage of our Fractal Raven Algorithm, an image containing the entire problem is first segmented into itscomponent images (the matrix of images, and the possible answers). Next, based upon the complexity of the matrix, thealgorithm determines the set of relationships to be evaluated. Then, a range of abstraction levels is determined.As we have implemented it, the abstraction levels are determined to be a partitioning of the given images into griddedsections at a prescribed size and regularity.6.2. The Fractal Ravens Algorithm: execution stageThe algorithm concludes by determining the confidence in the answers at each level, stopping when ambiguity is suffi-ciently resolved. Thus for each level of abstraction, the relationships implied by the kind of Raven’s problem (2 × 2 or 3 × 3)are re-represented into that partitioning. Then, for each of the candidate images, a potentially analogous relationship isdetermined for each of the existing relationships and a similarity value calculated. The vector of similarity values is reducedvia a simple Euclidean distance formula to a single similarity. The balance of the algorithm, using the deviation from themean of these similarities, continues through a variety of levels of abstraction, looking for an unambiguous answer thatmeets a specified confidence value.K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2317Given M, C, R, A, and η as determined in the preparatory stage, find the answer.PreparatoryLet E be a real number which represents the number of standard deviations beyond which a value’s answer may be judged as “confident”Let S( X, Y ) be the Tversky similarity metric for sets X and YExecutionFor each abstraction a ∈ A:• Re-represent each fractal representation r ∈ R according to abstraction a• S ← ∅• For each answer image c ∈ C :• If η = 2:H ← MutualFractal(m3, c) according to abstraction aV ← MutualFractal(m2, c) according to abstraction aΘ ← {S(H1, H), S(V 1, V )}• Else: (because η = 3)H ← MutualFractal(m7, m8, c) according to abstraction aV ← MutualFractal(m3, m6, c) according to abstraction aΘ ← {S(H1, H), S(H2, H), S(V 1, V ), S(V 2, V )}• Calculate a single similarity metric from vector Θ :(cid:17)(cid:18)t ←S ← S ∪ {t}θ 2 ∀θ ∈ Θ√n• Set μ ← mean(S)• Set σμ ← stdev(S)/• Set D ← {D 1, D 2, D 3, D 4, ..., Dn} where D i = (S i − μ)/σμ• Generate the set Z := {Z i ...} such that Z i ∈ D and Z i > E• If |Z | = 1, return the answer image Ci ∈ C which corresponds to Z i• otherwise there exists ambiguity, and further refinement must occur.If no answer has been returned, then no answer may be given unambiguously.Algorithm 4. The Fractal Raven Algorithm, execution stage.Table 3Image deviations and confidences for the example problem.ImageDeviations & confidences0.17513.84%−0.321−25.17%6.390> 99.99%0.49537.97%−1.741−91.84%−0.321−25.17%−1.741−91.84%−2.935−99.67%2000.5890.0316378−2.035−95.82%4.166> 99.99%3.48499.95%−3.384−99.93%−1.678−90.67%1.56088.12%0.25420.02%−2.366−98.20%1000.3100.019241512−1.861−93.72%2.78399.46%2.93099.66%−3.841−99.99%−2.148−96.83%2.44498.55%2.17297.02%−2.479−98.68%500.4320.0289660480.69851.47%2.17997.07%4.487> 99.99%−4.848< −99.99%−0.591−44.56%−1.361−82.64%−1.826−93.22%1.26279.31%250.6900.01538424 192−0.760−55.29%0.68150.4%3.96199.99%−4.958< −99.99%−2.825−99.53%0.89662.96%0.66849.58%2.33898.06%120.8720.0071734109 242−0.610−45.79%1.10673.12%4.100> 99.99%−5.454−100%−1.921−94.52%0.64347.99%0.21316.85%1.92294.54%60.9150.0056936436 968−1.311−81.02%0.48036.86%4.006> 99.99%−4.620< −99.99%−2.775−99.45%0.83259.49%0.57043.15%2.81799.52%30.9480.003269341 696 842grid sizeμσμcodesfeatures6.3. Addressing the exampleTable 3 shows the results of running the Fractal Ravens algorithm on the example problem, starting at an original griddedpartitioning of 200 × 200 pixels (the maximal pixel dimension of the images), and then refining the partitioning down toa grid of 3 × 3 pixels. The table gives the mean (μ), standard deviation (σμ), and number of features ( f ) for each level ofabstraction (grid). The deviation and confidence for each candidate answer are given for each level of abstraction as well.18K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23A confidence level of 95% is sought. In the table, we color a cell yellow if it exceeds the desired confidence level, and red ifit does so unambiguously for the given grid partitioning.6.4. Discussion of the example resultsThe deviations presented in Table 3 appear to suggest that if one starts at the very coarsest level of abstraction, theanswer is apparent (image choice 3). Indeed, the confidence in the answer never dips below 99.83%, across all levels ofabstraction. As we have shown, while that answer has the highest similarity value, statistically there are other alternatives.We see evidence that operating with either too sparse a data set (at the coarsest) or with too homogeneous a data set(at the finest) may be problematic. The coarsest abstraction (200 pixel grid size) offers 378 features, whereas the finestabstraction (3 pixel grid size) offers more than 1.5 million features for consideration.The data in the table continues to suggest the possibility of automatically detecting these boundary situations. We notethat the average similarity measurement at the coarsest abstraction is 0.589, but then falls, at the next level of abstrac-tion, to 0.310, only to thereafter generally increase. This constitutes further evidence for an emergent boundary for coarseabstraction.We suspect that ambiguity exists for ranges of abstraction, only to vanish at some appropriate levels of abstraction, andthen reemerges once those levels are surpassed. We see evidence of such behavior in this example, where there existsambiguity at grid sizes 100, 50, 25, and 12, then the ambiguity vanishes for grid size 6, and then reemerges for gridsize 3. This suggests that there are features within the image that are sufficiently discriminatory only at certain levels ofabstraction.7. Fractal Ravens resultsWe have tested our Fractal Ravens algorithm on all problems associated with the four main variations of the Raven’sProgressive Matrices Tests: 60 problems of the Standard Progressive Matrices test, 48 problems of the Advanced ProgressiveMatrices test, 36 problems of the Coloured Progressive Matrices test, and 60 problems of the SPM Plus test. To our knowl-edge, this is the first published account of a computational model’s attempt at the entire Raven’s test. In this section, wepresent our results and discuss our findings.7.1. Inputs used for the testTo create inputs for the Fractal Ravens algorithm, each page from the various Raven test booklets were scanned, and theresulting grayscale images were rotated to roughly correct for page alignment issues. Then, the images were sliced up tocreate separate image files for each entry in the problem matrix and for each answer choice. These separate images werethe inputs to the technique for each problem. No further image processing or cleanup was performed, despite the presenceof numerous pixel-level artifacts introduced by the scanning and minor inter-problem image alignment issues. Additionally,the fractal algorithm attempted to solve each problem independently: no information was carried over from problem toproblem, or from test variant to test variant.The code used in conducted these runs is precisely the same code as used in the earlier example. This code is availablefor download from our lab website. The images scanned, however, are copyrighted and thus are not available for download.However, we believe that the instructions for preparing the images provided above will allow for someone with access tothe Ravens materials to reproduce our results.7.2. Levels of abstraction considered and calculations performedThe images associated with each problem had a maximum pixel dimension of between 150 and 250 pixels. Accountingfor variation within each test problem, and setting a minimum grid size of 4 pixels, we therefore calculated five or six levelsof abstraction for each problem, using the formula described above for determining maximum grid size and using a strategyof halving the pixel dimension at each successively finer level of abstraction.At each level of abstraction, we calculated the similarity value for each possible answer, as proscribed by the FractalRavens algorithm. For those calculations, we used the Tversky formula, and set alpha to 1.0 and beta equal to 0.0, conformingto values used in the coincidence model by Bush and Mosteller [8]. From those values, we calculated the mean and standarddeviation, and then calculated the deviation and confidence for each answer. We made note of which answers provided aconfidence above our chosen level, and whether for each abstraction level the answer was unambiguous or ambiguous, andif ambiguous, in what manner. In those cases where ambiguity was found, we explored several different data techniques toassist in the resolution. We describe those techniques after the presentation of the performance.7.3. Assessment of Fractal Ravens performance against human normsThere are three main assessments that can be made following the administration of a Raven test to an individual: thetotal score, which is given simply as the number of correct answers; an estimate of consistency, which is obtained byTable 4SPM results.SPMTotalset Aset Bset Cset Dset ETable 5APM results.APMTotalset Aset BK. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2319Detectedcorrect5012101198Determinedby strategy38118865Ambiguousbetween 2 or 3911323Fig. 9. Human norms comparison for SPM score of 50.Detectedcorrect421032Determinedby strategy28622Ambiguousbetween 2 or 31046comparing the given score distribution to the expected distribution for that particular total score; and the percentile rangeinto which the score falls, for a given age and nationality [61]. A score is “consistent” if the difference between the actualscore and the expected score for any given set is no more than ±2 [61].7.4. Performance on the Standard Progressive Matrices testOn the Raven’s Standard Progressive Matrices test, the Fractal Ravens algorithm detected the correct answer at a 95% orhigher level of confidence on 50 of the 60 problems. The number of problems with detected correct answers per set were12 for set A, 10 for set B, 11 for set C , 9 for set D, and 8 for set E. Of the 50 problems where the correct answers detected,38 were determinable by one or more of our ambiguity-resolution strategies. Of the remaining 12 problems noted answers,all but three were ambiguous between two or three particular answers. Table 4 provides a summarization of these results.As shown in Fig. 9, the score differences for Fractal Ravens on each set were no more than ±1. For a human test-taker,this score distribution generally would indicate that the test results do provide a valid measure of the individual’s generalintellectual capacity. This score pattern illustrates that the results achieved by the algorithm fall well within typical humannorms on the SPM for all sets.Using norms from the United States, we note that a total score of 50 corresponds to the 95th percentile for childrenabout 12 years old, the 75th percentile for children around 14 years old, and the 50th percentile for children older than 16years old [61].7.5. Performance on the Advanced Progressive Matrices testOn the Raven’s Advanced Progressive Matrices test, the Fractal Ravens algorithm detected the correct answer at a 95%or higher level of confidence on 42 of the 48 problems. The numbers of problems with detected correct answers per setwere 10 for set A, and 32 for set B. Of the 42 problems where the correct answers detected, 28 were determinable by oneor more of our ambiguity-resolution strategies. Of the remaining 14 problems noted answers, all but four were ambiguousbetween two or three particular answers. Table 5 provides a summarization of the APM results.The score differences for Fractal Ravens on both APM sets were no more than ±1, indicating consistency and that theresults achieved by the algorithm fall well within typical human norms on the APM for both sets. We note that a total20Table 6CPM results.CPMTotalset Aset A Bset BTable 7SPM plus results.SPM plusTotalset Aset Bset Cset Dset EK. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23Detectedcorrect3012117Determinedby strategy241185Ambiguousbetween 2 or 36132Fig. 10. Human norms comparison for CPM score of 30.Detectedcorrect5010991111Determinedby strategy3998589Ambiguousbetween 2 or 31011422score of 42 corresponds to the 95th percentile for adults between 50 and 60 years old, and exceeds the 75th percentileperformance for adults of all measured ages [61].7.6. Performance on the Coloured Progressive Matrices testOn the Raven’s Coloured Progressive Matrices test, the Fractal Ravens algorithm detected the correct answer at a 95% orhigher level of confidence on 30 of the 36 problems. The number of problems with detected correct answers per set were12 for set A, 11 for set A B, and 7 for set B. Of the 30 problems where the correct answers detected, 24 were determinableby one or more of our ambiguity-resolution strategies. Of the remaining 6 problems noted answers, all were ambiguousbetween two or three particular answers. Table 6 provides a summarization of these results.As shown in Fig. 10, the score differences for Fractal Ravens on each set were no more than ±2, indicating consistency.This score pattern also illustrates that the results achieved by the algorithm fall well within typical human norms on theCPM for all sets.Using the United States norms, we note that a total score of 30 on the CPM test corresponds to the 95th percentile forchildren about 7 years old, the 75th percentile for children about 9 years old, and the 50th percentile for children about 11years old [61].7.7. Performance on the SPM Plus testOn the Raven’s Standard Progressive Matrices Plus test, the Fractal Ravens algorithm detected the correct answer at a95% or higher level of confidence on 50 of the 60 problems. The number of problems with detected correct answers per setwere 10 for set A, 9 for set B, 9 for set C , 11 for set D, and 11 for set E. Of the 50 problems where the correct answersdetected, 39 were determinable by one or more of our ambiguity-resolution strategies. Of the remaining 11 problems notedanswers, all but one were ambiguous between two or three particular answers. Table 7 provides a summarization of theseresults.K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2321Table 8Comparing Fractal Ravens to other models.Model/SetSPM resultsN# att.ABCDECarpenter et al. [9]“FairRaven”Carpenter et al. [9]“BetterRaven”Lovett et al. [50]Lovett et al. [51]Cirillo & Ström [11]Kunda et al. [46–48] AffineFractal Ravens22442838502448366060–––12127.8. Comparison to other computational models1012specifics unreported–11108911–1019–1058APM resultsN23322142# att.34344848A77710B16251432While this is the first published account of a computational model’s attempt at the entire suite of Raven’s tests, as wenoted in the introduction there are other computational models which have been used on some or all of certain tests. Forthose accounts that report scores, we compared their results with those achieved by the Fractal Raven algorithm. Table 8documents the performance of the Fractal Ravens algorithm against those contemporaries.Carpenter et al. [9] report results of running two versions of their algorithm (FairRaven and BetterRaven) against a subsetof the APM problems. The subset of problems chosen by Carpenter et al. reflect those whose rules and representations weredeemed as inferable by their production rule based system [9].Lovett et al. [50,51] report results from their computational model’s approach to the Raven’s SPM test. In each account,only a portion of the test was attempted, but Lovett et al. project an overall score based on the performance of the attemptedsections. The latest published account by Lovett et al. [51] reports a score of 44 out of 48 attempted problems from sets Bthrough E of the SPM test, but does not offer a breakdown of this score by problem set. Lovett et al. [51] project a scoreof 56 for the entire test, based on human normative data indicating a probable score of 12 on set A given their model’sperformance on the attempted sets.Cirillo and Ström [11] report that their system was tested against Sets C through E of the SPM and solved 8, 10, and 10problems, respectively. Though unattempted, they predict that their system would score 19 on the APM (a prediction of 7on set A, and 12 on set B).Kunda et al. [46–48] report the results of running their Affine algorithm against all of the problems on both the SPM andthe APM tests, with a detailed breakdown of scoring per test. They report a score of 38 for the SPM and a score of 21 onthe APM. More recently, this approach has given better results on the SPM, and it also has been used to address problemson the CPM [44].8. ConclusionsIn this paper, we have presented a comprehensive account of our efforts to solve the entire Standard and AdvancedRaven’s Progressive Matrices tests using fractal representations. We developed the Fractal Ravens algorithm, a computationalmodel which uses features derived from fractal representations to calculate Tversky similarities between relationships inthe test problem matrices and candidate answers, and which uses levels of abstraction, through re-representing the fractalrepresentation at differing resolutions, to determine overall confidence in the selection of an answer. Finally, we presented acomparison of the results of running the Fractal Ravens algorithm to all available published accounts, and showed that theFractal Ravens algorithm’s performance at detecting the correct answer met or exceeded those accounts. McGreggor [53]provides more details about our work.Analogy initiates with an act of being reminded. We hold that fractally representing both that triggering percept as wellas all prior percepts affords unprecedented similarity discovery, and thereby analogy-making. The Fractal Ravens algorithm,through its use of fractal representations and featural similarity evoked by recall, demonstrates these claims.Although we have focused our attention on representation and computational psychometrics, and drawn comparisonsbetween the Fractal Ravens performance and that of humans, we emphasize that what we developed within this paper areAI techniques and algorithms. While we expressly asserted no claims as to whether our model may be extended to providea cognitive accounting, given the development of our computational model and the nature of our representation, it is ourhope to spur others to explore additional computational and perhaps cognitive accounts of the intricate interplay betweenperception and analogy.AcknowledgementsThis work has benefited from many discussions with members of the Design and Intelligence Lab at Georgia Institute ofTechnology. We thank the US National Science Foundation for its support of this work through IIS Grant #1116541, entitled“Addressing visual analogy problems on the Raven’s intelligence test.”22K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–23References[1] A. Aamodt, E. Plaza, Case-based reasoning: foundational issues, methodological variations, and system approaches, AI Commun. 7 (1) (1994) 39–59.[2] D.W. Aha, L.A. Breslow, H. Muñoz-Avila, Conversational case-based reasoning, Appl. Intell. 14 (1) (2001) 9–32.[3] K.D. Ashley, E.L. Rissland, A case-based approach to modeling legal expertise, IEEE Expert 3 (3) (1988) 70–77.[4] F.G. Ashby, D.M. Ennis, Similarity measures, Scholarpedia 2 (12) (2007) 4116.[5] M. Barnsley, L. Hurd, Fractal Image Compression, A.K. Peters, Boston, MA, 1992.[6] S. Bhatta, A. Goel, An analogical theory of creativity in design, in: D. Leake, E. Plaza (Eds.), in: Lecture Notes in Computer Science, vol. 1266, 1997,pp. 565–574.[7] S. Bringsjord, B. Schimanski, What is artificial intelligence? Psychometric AI as an answer, in: International Joint Conference on Artificial Intelligence,2003, pp. 887–893, Citeseer.[8] R.R. Bush, F. Mosteller, A stochastic model with applications to learning, Ann. Math. Stat. (1953) 559–585.[9] P.A. Carpenter, M.A. Just, P. Shell, What one intelligence test measures: a theoretical account of the processing in the Raven Progressive Matrices Test,Psychol. Rev. 97 (1990) 404–431.[10] D.J. Chalmers, R.M. French, D.R. Hofstadter, High-level perception, representation, and analogy: a critique of artificial intelligence methodology, J. Exp.Theor. Artif. Intell. 4 (3) (1992) 185–211.[11] S. Cirillo, V. Ström, An Anthropomorphic Solver for Raven’s Progressive Matrices, Chalmers University of Technology, Göteborg, Sweden, 2010.[12] J.J. Clement (Ed.), Creative Model Construction in Scientists and Students: The Role of Imagery, Analogy, and Mental Simulation, Springer, Netherlands,2008.[13] D. Croft, P. Thagard, Dynamic imagery: a computational model of motion and visual analogy, in: L. Magnini, N. Nersessian (Eds.), Model-Based Reason-ing, 2002, pp. 259–274.[14] J. Davies, J. Glasgow, T. Kuo, Visuo-spatial case-based reasoning: a case study in prediction of protein structure, Comput. Intell. 22 (3/4) (2006) 194–207.[15] J. Davies, A. Goel, N. Nersessian, A computational model of visual analogies in design, Cogn. Syst. Res., Special Issue on Analogies: Integrated CognitiveAbilities 10 (2009) 204–215.[16] J. Davies, A. Goel, P. Yaner, Proteus: a theory of visual analogies in problem solving, Knowl.-Based Syst. 21 (7) (2008) 636–654.[17] J. Davies, N. Nersessian, A. Goel, Visual models in analogical problem solving, Found. Sci. 10 (1) (2005) 133–152.[18] M. Dawson, I. Soulières, M.A. Gernsbacher, L. Mottron, The level and nature of autistic intelligence, Psychol. Sci. 18 (8) (2007) 657–662.[19] K. Dunbar, The analogical paradox: why analogy is so easy in naturalistic settings yet so difficult in the psychological laboratory, in: D. Gentner, K.J.Holyoak, B.N. Kokinov (Eds.), The Analogical Mind: Perspectives from Cognitive Science, MIT Press, Cambridge, MA, 2001, pp. 313–334.[20] K. Dunbar, I. Blanchette, The in vivo/in vitro approach to cognition: the case of analogy, Trends Cogn. Sci. 5 (2001) 334–339.[21] T.G. Evans, A heuristic program to solve geometric-analogy problems, in: Proceedings of the April 21–23, 1964, Spring Joint Computer Conference,ACM, 1964, pp. 327–338.[22] B. Falkenhainer, K. Forbus, D. Gentner, The structure-mapping engine: algorithms and examples, Artif. Intell. 41 (1) (1989) 1–63.[23] K. Forbus, D. Gentner, K. Law, MAC/FAC: a model of similarity-based retrieval, Cogn. Sci. 19 (2) (1995) 141–205.[24] D. Gentner, Structure-mapping: a theoretical framework for analogy, Cogn. Sci. 7 (2) (1983) 155–170.[25] D. Gentner, A. Markman, Structure mapping and analogy and similarity, Am. Psychol. 52 (1) (1996) 45–57.[26] M. Gick, K.J. Holyoak, Analogical problem solving, Cogn. Psychol. 12 (1980) 306–355.[27] M. Gick, K.J. Holyoak, Schema induction and analogical transfer, Cogn. Psychol. 15 (1983) 1–38.[28] A. Goel, S. Bhatta, Use of design patterns in analogy-based design, Adv. Eng. Inform. 18 (2) (2004) 85–94.[29] A. Goel, B. Chandrasekaran, Case-based design: a task analysis, in: C. Tong, D. Sriram (Eds.), Artificial Intelligence Approaches to Engineering Design,Volume II: Innovative Design, Academic Press, San Diego, CA, 1992, pp. 165–184.[30] A. Goel, S. Bhatta, E. Stroulia, An early case-based design system, in: M. Maher, P. Pu (Eds.), Issues and Applications of Case-Based Reasoning, Erlbaum,1992, pp. 87–132.[31] R. Gregson, A comparative evaluation of seven similarity models, Br. J. Math. Stat. Psychol. 29 (2) (1976) 139–156.[32] K. Hammond, Case-Based Planning: Viewing Planning as a Memory Task, Academic Press, San Diego, CA, 1987.[33] D.R. Hofstadter, Analogy as the core of cognition, in: D. Gentner, K.J. Holyoak, B.N. Kokinov (Eds.), The Analogical Mind: Perspectives from CognitiveScience, MIT Press, Cambridge, MA, 2001, pp. 499–538.[34] D.R. Hofstadter (Ed.), Fluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought, Basic Books, New York,1996.[35] D.R. Hofstadter, E. Sander, Surfaces and Essences: Analogy as the Fuel and Fire of Thinking, Basic Books, 2013.[36] K.J. Holyoak, J.E. Hummel, Toward an understanding of analogy within a biological symbol system, in: The Analogical Mind: Perspectives from CognitiveScience, 2001, pp. 161–195.[37] K.J. Holyoak, P. Thagard, Mental Leaps: Analogy in Creative Thought, MIT Press, Cambridge, MA, 1996.[38] E. Hunt, Quote the Raven? Nevermore, in: L.W. Gregg (Ed.), Knowledge and Cognition, Erlbaum, Hillsdale, NJ, 1974, pp. 129–158.[39] B. Indurkhya, On the role of interpretive analogy in learning, New Gener. Comput. 8 (4) (1991) 385–402.[40] M.T. Keane, Analogical Problem Solving, Halsted Press, 1988.[41] K. Kim, T.H. Chalidabhongse, D. Harwood, L. Davis, Real-time foreground–background segmentation using codebook model, Real-Time Imaging 11 (3)(2005) 172–185.[42] B. Kokinov, A. Petrov, Integrating memory and reasoning in analogy-making: the AMBR model, in: D. Gentner, K.J. Holyoak, B.N. Kokinov (Eds.), TheAnalogical Mind: Perspectives from Cognitive Science, MIT Press, Cambridge, MA, 2001, pp. 59–124.[43] J. Kolodner, Case-Based Reasoning, Morgan Kaufmann, San Mateo, CA, 1993.[44] M. Kunda, Visual problem solving in autism, psychometrics, and AI: the case of the Raven’s Progressive Matrices intelligence test, Doctoral dissertation,School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, 2013.[45] M. Kunda, A. Goel, Thinking in pictures as a cognitive account of autism, J. Autism Dev. Disord. 41 (2011) 1157–1177.[46] M. Kunda, K. McGreggor, A. Goel, Two visual strategies for solving the Raven’s Progressive Matrices intelligence test, in: Proceedings of 25th AAAIConference on Artificial Intelligence, 2011.[47] M. Kunda, K. McGreggor, A. Goel, Reasoning on the Raven’s Advanced Progressive Matrices test with iconic visual representations, in: Proceedings of34th Annual Meeting of the Cognitive Science Society, Sapporo, Japan, 2012.[48] M. Kunda, K. McGreggor, A. Goel, A computational model for solving problems from the Raven’s Progressive Matrices intelligence test using iconicvisual representations, Cogn. Syst. Res. 22–23 (2013) 47–66.[49] D. Leake (Ed.), Case-Based Reasoning: Experiences, Lesson and Future Directions, MIT Press, Cambridge, MA, 1996.[50] A. Lovett, K. Forbus, J. Usher, Analogy with qualitative spatial representations can simulate solving Raven’s Progressive Matrices, in: Proceedings of the29th Annual Conference of the Cognitive Society, Nashville, Tennessee, 2007.[51] A. Lovett, K. Forbus, J. Usher, A structure-mapping model of Raven’s Progressive Matrices, in: Proceedings of the 32nd Annual Conference of theCognitive Science Society, Portland, Oregon, 2010, pp. 2761–2766.K. McGreggor et al. / Artificial Intelligence 215 (2014) 1–2323[52] B.B. Mandelbrot, The Fractal Geometry of Nature, W.H. Freeman, San Francisco, CA, 1982.[53] K. McGreggor, Fractal analogical reasoning, Doctoral Dissertation, School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, 2013.[54] K. McGreggor, C.M. Yerga, D. Van Brink, Color processing system, U.S. Patent No. 5,963,201 U.S. Patent and Trademark Office, Washington, DC, 1999.[55] K. McGreggor, M. Kunda, A. Goel, Fractal analogies: preliminary results from the Raven’s test of intelligence, in: Proceedings of the InternationalConference on Computational Creativity, Mexico City, Mexico, 2011, pp. 27–29.[56] M. Mitchell, Analogy Making as Perception: A Computer Model, MIT Press, Cambridge, MA, 1993.[57] N. Nersessian, Creating Scientific Concepts, MIT Press, Cambridge, MA, 2008.[58] H. Prade, G. Richard (Eds.), Computational Approaches to Analogical Reasoning: Current Trends, Springer, 2014.[59] M. Ragni, P. Stahl, T. Fangmeier, Cognitive complexity in matrix reasoning tasks, in: Proceedings of the European Conference on Cognitive Science, NBUPress, Sofia, 2011.[60] D. Rasmussen, C. Eliasmith, A neural model of rule generation in inductive reasoning, Top. Cogn. Sci. 3 (1) (2011) 140–153.[61] J. Raven, J.C. Raven, J.H. Court, Manual for Raven’s Progressive Matrices and Vocabulary Scales, Harcourt Assessment, San Antonio, TX, 2003.[62] S. Ray, R.H. Turi, Determination of number of clusters in k-means clustering and application in colour image segmentation, in: Proceedings of the 4thInternational Conference on Advances in Pattern Recognition and Digital Techniques, 1999, pp. 137–143.[63] C.K. Riesbeck, R.C. Schank, Inside Case-Based Reasoning, Lawrence Erlbaum Associates, Hillsdale, NJ, 1989.[64] A. Schwering, U. Krumnack, K.U. Kühnberger, H. Gust, Using gestalt principles to compute analogies of geometric figures, in: Proceedings of the 19thMeeting of the Cognitive Science Society, 2007.[65] B. Smyth, P. Cunningham, M. Keane, Hierarchical case-based reasoning integrating case-based and decompositional problem solving techniques, IEEETrans. Knowl. Data Eng. 13 (2001) 792–812.[66] I. Soulières, M. Dawson, F. Samson, E.B. Barbeau, C.P. Sahyoun, G.E. Strangman, T.A. Zeffiro, L. Mottron, Enhanced visual processing contributes to matrixreasoning in autism, Hum. Brain Mapp. 30 (12) (2009) 4082–4107.[67] P. Thagard, K.J. Holyoak, G. Nelson, D. Gochfeld, Analog retrieval by constraint satisfaction, Artif. Intell. 46 (1990) 259–310.[68] A. Tversky, Features of similarity, Psychol. Rev. 84 (4) (1977) 327–352.[69] P. Yaner, A. Goel, Visual analogy: viewing retrieval and mapping as constraint satisfaction, Appl. Intell. 25 (1) (2006) 91–105.[70] P. Yaner, A. Goel, Analogical recognition of shape and structure in design drawings, Artif. Intell. Eng. Des. Anal. Manuf. 22 (2) (2008) 117–128.[71] S.C. Zhu, A. Yuille, Region competition: unifying snakes, region growing, and Bayes/MDL for multiband image segmentation, IEEE Trans. Pattern Anal.Mach. Intell. 18 (9) (1996) 884–900.