2202peS82]GL.sc[3v00701.5002:viXraDistance-based Positive and Unlabeled Learning forRankingHayden S. Helm, Amitabh Basu, Avanti Athreya, Youngser Park,Joshua T. Vogelstein, Carey E. Priebe‚àóJohns Hopkins University100 Whitehead Hall3400 North Charles StreetBaltimore, MD 21218 USA‚àócorresponding author; email: cep@jhu.eduMichael Winding, Marta Zlatic, Albert CardonaUniversity of CambridgeCambridge CB2 1TN UKPatrick Bourke, Jonathan Larson, Marah Abdin, Piali Choudhury,Weiwei Yang, Christopher W. WhiteMicrosoft ResearchRedmond, WA 98052 USAAbstractLearning to rank ‚Äì producing a ranked list of items specific to a query and withrespect to a set of supervisory items ‚Äì is a problem of general interest. Thesetting we consider is one in which no analytic description of what constitutesa good ranking is available.Instead, we have a collection of representationsand supervisory information consisting of a (target item, interesting items set)pair. We demonstrate analytically, in simulation, and in real data examples thatlearning to rank via combining representations using an integer linear programis effective when the supervision is as light as ‚Äùthese few items are similar to youritem of interest.‚Äù While this nomination task is quite general, for specificity wepresent our methodology from the perspective of vertex nomination in graphs.The methodology described herein is model agnostic.Preprint submitted to Pattern RecognitionSeptember 29, 2022   1. IntroductionGiven a query, a collection of items, and supervisory information, producinga ranked list relative to the query is of general interest. In particular, learningto rank [1] and algorithms from related problem settings [2] have been used toimprove popular search engines and recommender systems and, impressively,aid in the identification of human traffickers [3].When learning to rank, for each training query researchers typically have ac-cess to (feature vector, ordinal) pairs that are used to learn an ordinal regressorvia fitting a model under a set of probabilistic assumptions [4] or via deep learn-ing techniques [5] that generalize to ranking items for never-before-seen queries.A query is an element of a set of possible queries Q and the items-to-be-rankedare elements of a nomination set N .In this paper we consider the setting in which, for a given query, we knowthe dissimilarity (from multiple perspectives) between it and a set of items tobe ranked. We are also given a set of items known to be similar to the query(positive examples). We make no model assumptions. Our goal is to leveragethe knowledge of the items known to be similar to the query to produce a newdissimilarity tailored to the query. The new dissimilarity, which in this paperis exactly a convex combination of the different dissimilarities, is then usefulfor nominating items unknown to be similar to the query. This approach tolearning a ranking scheme can gainfully be seen as combining representationsto improve inference [6].In the language of graphs: with respect to a specific vertex of interest v‚àó, andgiven a set S of important vertices and a collection of dissimilarity measures,we solve an integer linear program that weights those dissimilarities so thatthe set of points in S are ranked minimally; we then use that learned weightedrepresentation to rank all other vertices. In short, we infer an entire rankingfrom light supervision in the form of the set S.Our setting is closely related to positive and unlabeled (PU) learning [7] inthat supervision is of the form of positive examples and that unlabeled data2are assumed to be neither positive nor negative. The setting herein differs fromcanonical PU learning in two major ways. First, in PU learning the infer-ence task is to label unknown-to-be-positive objects as positive. For this task,standard classification algorithms such as Naive Bayes and Support Vector Ma-chines can be modified to produce a decision function under certain samplingassumptions [8, 9]. Our task, however, is to rank the unlabeled data. Whiletransforming standard classifiers into ranking functions is possible, the typicaltransformation returns a ranking with respect to the positive conditional distri-bution (as opposed to with respect to the query) and so we do not study it here.Second, in canonical PU learning the data consists of feature vectors. Herein weare only given dissimilarities between objects and thus can only use standard PUlearning techniques naively. We think that this change in perspective is requiredto learn a ranking function with respect to the query (again, as opposed to aranking function with respect to the positive conditional distribution). Indeed,the use of PU learning in the context of recommendation systems or rankingproblems is quite new, with Zhou et al. noting ‚ÄúPU learning has not been exten-sively explored on recommender systems ..‚Äù [10] and Zhu et al. saying ‚Äútherehas been no research on combining multiple PU learning algorithms for textranking‚Äù [11]. As far as we are aware, our ‚Äúdissimilarity-based‚Äù PU learning forranking is a novel setting and is to PU learning as dissimilarity-based patternrecognition [12] is to classical pattern recognition [13].Related to our work, [14] and the corresponding literature [15] deal with amulti-media system (a system consisting of many subsystems) where the multi-media query v‚àó‚àó is a boolean function of different v‚àói a querycorresponding to subsystem i. Each subsystem is assumed to be able produce am with v‚àó2, ..v‚àó1, v‚àóranked list for its corresponding query using fuzzy logic. In the context of ourdiscussion, it is fruitful to think of these ranked lists as coming from different(marginal of subsystem i) representations of v‚àó‚àó. Fagin proposes an optimal al-gorithm for combining these representations under a set of assumptions ‚Äì mostnotably that the boolean function that combines the subsystem queries is knownand that the subsystems are independent. These two assumptions, in partic-3ular, imply that supervisory information is not necessary.In our setting wemake no assumptions on the structure of the query nor on the relationship be-tween the representations and hence rely on supervisory information to combinerepresentations.Our work is also related to the set expansion literature [16, 17, 18]. Mostsimilar to the ideas discussed in this paper here is the algorithm SetExpan pro-posed by Shen and Wu et al. [16] where they combine information from a set ofranked lists from different contexts to iteratively add to their set of importantitems. There are three main differences between SetExpan and our approach.The first is that for SetExpan the resulting ranked list is relevant to the entireS ‚à™ {v‚àó} set and not just the vertex of interest v‚àó. Herein we do not assumesymmetry in the relationship between v‚àó and the elements of S‚àó. The secondis that SetExpan uses the elements of S ‚à™ {v‚àó} to find a subset of the repre-sentations to use in the final ranked list. We, on the other hand, our approachuses the elements of S as supervision to a learning problem that optimizes alinear combination of the original representations. Lastly, conditioned on theselected subset of representations, SetExpan uses a simple average to combinethe representations. Our approach uses a learned convex combination instead.While our set up is quite general, we study it through the lens of vertexnomination [19].2. Problem Description: Vertex NominationIn the single graph vertex nomination problem [19] we are given a graphG = (V, E) and a single vertex of interest v‚àó ‚àà V , and the task is to find otherinteresting vertices. The vertex set V can be taken to be V = [n] = {1, . . . , n}(cid:1) is a subset of all possible vertex pairs {i, j} withand the edge set E ‚äÇ (cid:0)V2i, j ‚àà [n]. The objective of vertex nomination is to return a ranked list of thecandidate vertices V \ {v‚àó} such that ‚Äúinteresting‚Äù vertices ‚Äì vertices ‚Äúsimilar‚Äùto v‚àó ‚Äì are ranked high in the nomination list. Note that in vertex nominationthe query set Q is {v‚àó} and the nomination set N is V \ {v‚àó}.4Vertex nomination is a special case of (typically) unsupervised problemsaddressed by recommender systems [20] where it is assumed that ‚Äù[i]nformationrelevant to the task is encoded in both the structure of the graph and the attributeson the edges‚Äù [21]. There have been numerous approaches to vertex nominationproposed in recent years [22, 23, 24, 25, 3, 26, 27] with each illustrating successin sometimes adversarial settings.Notably, none of these proposed nomination schemes is universally consis-tent. Recall that a universally consistent decision rule is one where the limitingperformance of the decision rule is Bayes optimal for every possible distributionof the data.In the classification setting, for example, the famed K NearestNeighbor rule [28], with appropriate restrictions on K growing with trainingset size, is in a class of decision rules known to be universally consistent, [13,Chapters 5,6]. In their foundational paper on the theoretical framework of ver-tex nomination, Lyzinski et al. show that there does not exist a universallyconsistent vertex nomination scheme [19]. Their paper complements other the-oretical [29] and empirical [30] results on the limitations of machine learningfor popular unsupervised learning problems on graphs. The successes reportedin [22, 23, 24, 25, 3, 26, 27] were all with respect to some application-specificnotion of similarity/interestingness.In this paper, in contrast to being told what is meant by similarity, weconsider the setting in which, in addition to G and v‚àó, we are given a set ofvertices S ‚äÇ (V \ {v‚àó}) explicitly known to be similar to v‚àó from which we areto learn a ranking scheme specific to the task at hand. In particular, we developa nomination scheme f that takes as input (G, v‚àó, S) ‚Äì a graph, a vertex ofinterest, and a set of vertices known to be similar to the vertex of interest ‚Äìand outputs a function that maps each vertex not equal to v‚àó to an element ofthe set [n ‚àí 1]. (We ignore the possibility of ranking ties for expediency; seeAppendix B.1 of [19] for a discussion.)We consider S‚àó ‚äÇ (V \ {v‚àó}) to be the collection of vertices that are trulysimilar to v‚àó; thus the given supervisory set S ‚äÇ S‚àó and the set (S‚àó \ S),representing an unknown truth, consists of vertices that we actually want to5identify as interesting by placing them highly in the nomination list. LettingH = {h : V \ {v‚àó} ‚Üí [n ‚àí 1]} be the set of functions, or rankers, that map avertex to an element of [n ‚àí 1] and GV be the set of graphs with vertex set V , anomination scheme is a mapping f : GV √ó V √ó 2V ‚Üí H. Our goal is to use anf such that f (G, v‚àó, S) = h(¬∑) outputs small values for elements of S‚àó.We refer to the set C = (V \ ({v‚àó} ‚à™ S)) as the candidate set and notethat (S‚àó \ S) ‚äÇ C. For evaluation purposes it is convenient to consider theset of rankers that map from C to the nomination range R = [n ‚àí 1 ‚àí |S|].When there is a possibility of confusion we denote such rankers as hC, withHC = {hC : C ‚Üí R}. For every ranker h ‚àà H there is a ranker hC ‚àà HC suchthat hC(v) is equal to h(v) minus the number of elements of S ranked higherthan v in the nomination list induced by h for all v ‚àà C.MethodsNatural Nomination, Given a DissimilarityRecall that h is a mapping from the set of vertices minus v‚àó to the set ofranks {1, . . . , n ‚àí 1}.Given a dissimilarity measure d : V √ó V ‚Üí R, a natural ranking functionto consider is one that, given a vertex v (cid:54)= v‚àó, returns the rank of the realnumber d(v‚àó, v) amongst the collection {d(v‚àó, v(cid:48))}v(cid:48)(cid:54)=v‚àó . That is, the vertexv ‚àà (V \{v‚àó}) that minimizes d(v‚àó, v) is mapped to 1 ‚Äì the top of the nominationlist ‚Äì and the vertex farthest from the vertex of interest is mapped to n ‚àí 1. Welet hd denote this mapping from the vertex set to the set [n ‚àí 1] for dissimilarityd.We emphasize that throughout our discussion d need not satisfy the sym-metry, triangle inequality or non-negativity requirements of a metric.An Integer Linear ProgramWe present an optimization problem whose solution is useful for learning torank in general and supervised vertex nomination in particular. Let {v1, . . . , vn}be a finite set of items. Without loss of generality we identify v1 = v‚àó. We6have a collection of J distinct dissimilarity measures {d1, d2, . . . , dJ } and haveknowledge of the dissimilarity between v1 and vi, i = 2, . . . , n for each of thesemeasures. We use dj(v1, vi) to denote the dissimilarity between v1 and vi in thej-th dissimilarity, j = 1, . . . , J. We are given a set S ‚äÇ {v2, . . . , vn} that wewant to rank as high as possible by choosing an appropriate weighted combina-tion of the J dissimilarities. More precisely, we wish to select a set of weightsŒ±1, . . . , Œ±J ‚â• 0 such that when the elements v2, . . . , vn are ranked according tothe dissimilarity Œ±1d1(v1, vi) + . . . + Œ±J dJ (v1, vi) (for i = 2, . . . , n), the elementsof S are as close to the top of the ranked list as possible.Formally, for a given tuple of weights Œ± = (Œ±1, . . . , Œ±J ), let hŒ±(vi) denotethe rank of vi under the dissimilarity Œ±1d1(v1, vi) + . . . + Œ±J dJ (v1, vi). We wishto solve the following optimization problem:minŒ±‚â•0maxv‚ààShŒ±(v).(1)The above problem can be formulated using the framework of integer linearprogramming (ILP). An ILP problem is an optimization problem where onewishes to minimize/maximize a linear function of a finite set of decision variablessubject to linear inequality constraints and where a subset of the variables arerequired to take only integer values. In our current setting, we model (1) asfollows.1. Introduce real valued decision variables Œ±1, . . . , Œ±J that are constrainedto be nonnegative. These are the weights we are seeking in (1). We alsoimpose the normalization constraint that Œ±1 + . . . + Œ±J = 1, since scalingthe weights by the same positive factor yields the same solutions.2. Introduce integer variables xv for each item v (cid:54)‚àà S.Impose the linearconstraints 0 ‚â§ xv ‚â§ 1; this forces xv ‚àà {0, 1} in any feasible solution.These variables are to be interpreted as follows: if xv = 0 in any solution,then v is ranked worse than every element of S (under Œ±) and if xv = 1in any solution, then v is ranked better than at least one element in S(under Œ±).73. The linear objective function involves only the xv variables: we wish tominimize (cid:80)v(cid:54)‚ààS xv, because this sum equals the number of elements thatare ranked better than at least one element of S and thus captures theobjective in (1).4. We impose the linear constraintsJ(cid:88)j=1Œ±jdj(v1, s) ‚â§J(cid:88)j=1Œ±jdj(v1, v) + M ¬∑ xv‚àÄ(s, v) ‚àà (S, C)where M := maxi,j dji . This constraint imposes the desired condition thatfor any v ‚àà C, if xv = 0, then v should be ranked worse than everyelement in S, i.e., its dissimilarity (under Œ±) from v1 should be greaterthan or equal to the dissimilarity of every element of S from v1. If xv = 1,then since M is chosen to be the maximum of all possible dissimilaritiesand the coefficients Œ±1, . . . , Œ±J sum to 1, the constraint becomes a trivialconstraint that is satisfied by all such nonnegative Œ± values. Since we areminimizing (cid:80)v(cid:54)‚ààS xv, for any Œ±, if an element v (cid:54)‚àà S is ranked worse thanevery element of S, then the optimization would set xv to 0. Thus, inany optimal solution Œ±‚àó, x‚àó to the integer program, x‚àóhŒ±‚àó(s) for some s ‚àà S.(v) < hŒ±‚àóv = 1 if and only ifOnce the problem (1) is set up as an ILP as described above, one can bringstate-of-the-art algorithms and software that employ a suite of sophisticatedideas borrowed from convex geometry, number theory and algorithm designto bear upon the problem. Python implementations based on different mixedinteger solvers including Gurobi [31], Common Optimization INterface for Op-erations Research [32] and SCIP: Solving Constraint Integer Programs [33] areavailable at https://github.com/microsoft/distPURL.We note that the computational complexity of the proposed ILP is a com-plicated function of the number of vertices, the collection of representationsconsidered, and the vertices S known to be similar to the vertex of interest v‚àó.In general, when the worst ranking element of S is ranked sufficiently poorly8then the ILP is computationally burdensome. This issue is compounded whenat least one element of S is ranked poorly.Further, the objective function (1) is but one natural choice for the problemof using the elements of S to learn a useful ranked list. Others include the min ofthe average rank of elements of S and the max of the average reciprocal rank ofelements of S. Understanding the performance and computational consequencesof different objective functions is a promising route for future research.The Solution Nomination ListThe Œ±‚àó given by the solution to the integer program induces a dissimilarityd‚àó = (cid:80)kj=1 Œ±‚àój dj, and the resultant hd‚àóprovides an nomination list learned forv‚àó from S. We note that the ranker induced by d‚àó is not necessarily unique andis an element of the set H‚àó = {h : h minimizes (1)} whose constituent decidersmap the elements of S close to the top of a nomination list.Comparing two rankersIn the simulations and real data experiments below we compare nominationschemes using Mean Reciprocal Rank (MRR). MRR is one of many measurescommonly used in information retrieval to evaluate a nomination list for a givenset of objects [34]. Let h be a ranker and N (cid:48) be a subset of the nominationobjects N . The MRR of h for N (cid:48) is the average of the multiplicative inverse(or reciprocal) of the h(s). That is,MRR(h, N (cid:48)) =1|N (cid:48)|(cid:88)s‚ààN (cid:48)1h(s).For a given N (cid:48) and two rankers h, h(cid:48), the ranker h is preferred to the ranker h(cid:48)for N (cid:48) if MRR(h, N (cid:48)) > MRR(h(cid:48), N (cid:48)). In our experiments, N (cid:48) = S‚àó \ S.A Generative Model ExampleLatent space network models [35] are random graph models where each ver-tex has associated with it a latent vector and the probability of an edge betweentwo vertices is determined by a function of two vectors, typically called a kernel.9One such latent space model is the Random Dot Product Graph (RDPG) wherethe kernel function is the inner product [36].We consider latent positions X1, .., Xniid‚àº F on Rm associated with thevertices v1, .., vn where the distribution F is such that 0 ‚â§ (cid:104)x, y(cid:105) ‚â§ 1 for all x, yin the support. Let X denote the n √ó m matrix with the Xi‚Äôs as rows. That is,X =Ô£πÔ£∫Ô£∫Ô£∫Ô£ª.Ô£ÆÔ£ØÔ£ØÔ£ØÔ£∞TX1...TXnTThen P = XXis the n √ó n RDPG connectivity probability matrix. LetT 1, . . . , T J be different embedding functions; that is, each T j : Mn ‚Üí (Rmj )ntakes as input an n √ó n matrix and outputs n points in Rmj . For example,P is an embedding withthe adjacency spectral embedding of P = UP Œ£P UTASE(P ) = UP |Œ£P |1/2. We let T (P )i denote the representation of node i re-Tsulting from the transformation T . Then TASE(P )i = Xi (up to an orthogonaltransformation). Further suppose that with each embedding function comes adissimilarity dj : Rmj √ó Rmj ‚Üí [0, ‚àû). This induces a v1-specific ‚Äúpersonal‚Äùdissimilarity matrix‚àÜv1 =Ô£ÆÔ£ØÔ£ØÔ£ØÔ£∞d1(T 1(P )1, T 1(P )1)...d1(T 1(P )1, T 1(P )n)... . ...dJ (T J (P )1, T J (P )1)...dJ (T J (P )1, T J (P )n)Ô£πÔ£∫Ô£∫Ô£∫Ô£ªcontaining the dissimilarities from the (representation of the latent position for)the vertex of interest (without loss of generality, we are letting the vertex ofinterest be index 1: v‚àó = v1) to every other vertex for every transformation interms of its induced dissimilarity. Recall that the ILP takes as input a subsetof vertices S and a personal dissimilarity matrix ‚àÜv‚àó .As above, our goal is to construct a dissimilarity d(cid:48) = (cid:80) Œ±jdj with (cid:80) Œ±j = 1,Œ±j ‚â• 0 such that d(cid:48)(v1, vs‚àó ) is ‚Äùsmall‚Äù for the elements of (S‚àó \ S), the verticestruly, but unknown to be, similar to v‚àó.10An Illustrative Analytic ExampleWe illustrate the geometry of combining representations in the RDPG modelusing Laplacian Spectral Embedding (LSE) and Adjacency Spectral Embedding(ASE) [37, 38].Given a particular realization of the Xi‚Äôs, P is fixed (non-random). Weconsider x1 = [0.5, 0.5]T and x2, . . . , x51 to be realizations from the uniform dis-tribution on the positive unit disk in R2. We consider two embedding functions:T 1(P ) = UP |Œ£P |1/2 and T 2(P ) =nUL(P )|Œ£L(P )|1/2, both truncated at em-bedding dimension m = 2, where L(P ) = D‚àí1/2P D‚àí1/2 with Dii equal to the‚àöith row sum of P . The corresponding dissimilarities are taken to be Euclideandistance.Figure 1 shows that the interpoint distance rankings induced by T 1 (ASE)and T 2 (LSE) are not necessarily the same, thus demonstrating the basis ofthe ‚Äútwo truths‚Äù phenomenon in spectral graph clustering [30]. Further, theinterpoint distance rankings from a linear combination of the two Euclideandistances is neither equal to the rankings from ASE nor the rankings fromLSE. This indicates that the solution found by the ILP can produce a superiornomination list compared to either ASE or LSE alone.An Illustrative Simulation ExampleIn the RDPG setting we do not observe P directly. Instead, we observe anind‚àº Bernoulli(Pij) for i < j, Aji = Aij, andadjacency matrix A such that AijAii = 0. Note that, save for the diagonal, P = E(A).Revisiting our analytic example, with geometry illustrated in Figure 1, weiid‚àºconsider the setting in which we observe ¬ØA = 1ki=1 Ai with A1, . . . , Ak(cid:80)kBernoulli(P ). We define ‚Äúinterestingness‚Äù based on the dissimilarity dŒ± =Œ±dASE + (1 ‚àí Œ±)dLSE where d{A,L}SE is Euclidean distance defined on thevertices after embedding P via {A,L}SE. We let S‚àó be the six closest vertices tov‚àó as defined by dŒ± after embedding the true but unknown probability matrixP .11Figure 1: Ranking of interpoint distances is not preserved across vertex representations. Eachpanel shows an embedding of the probability matrix P , with LSE on the left, the inducedembedding for Œ± = 0.2 in the center, and ASE on the right. In each panel, 9 vertices arehighlighted: 1) v‚àó ‚Äì the vertex of interest v‚àó; 2-6) S ‚Äì the five vertices closest to v‚àó as definedby d0.2 = 0.2dASE + 0.8dLSE ; and 7-9) s‚àóASE ‚Äì the closest vertex to v‚àó that isnot an element of S as defined by dLSE , d0.2 and dASE , respectively. The number near eachof the three s‚àó in each panel is the Euclidean distance between it and v‚àó for that particularembedding. Note that the ordering of these distances is not preserved across panels. Thisimplies that inference based on the ranking of the interpoint distances is not invariant to therepresentation of the vertices.0.2 and s‚àóLSE , s‚àóFigure 2 presents the results from this simulation set up for various values ofŒ± where S is the set of five closest vertices to v‚àó as defined by dŒ±. The rankersare evaluated based on where the sixth closest element, s‚àó, as defined by dŒ±, isin their respective nomination lists. The left panel shows the performance of theILP and rankers induced by dASE and dLSE when the P matrix is observed. Theright panel shows the performance of the same three schemes when k = 1000.Reciprocal rank is estimated using 100 Monte Carlo simulations. Shaded regionsindicate the 95% confidence interval for the mean. The two panels demonstratethe utility of the ILP solution for learning to nominate in both noiseless andnoisy settings. We note that when Œ± = 0 ‚Äúinterestingness‚Äù coincides exactly withthe dissimilarity defined on the representation of the vertices after embeddingvia LSE and that when Œ± = 1 ‚Äúinterestingness‚Äù coincides exactly with thedissimilarity defined on the representation of the vertices after embedding viaASE.12                / 6 (   D O S K D                    , Q G X F H G  ( P E H G G L Q J   D O S K D       Y  6 V   / 6 ( V   D O S K D      V   $ 6 (                $ 6 (   D O S K D    (a)(b)Figure 2: The proposed ILP demonstrates utility both analytically in the noiseless setting(left) and via simulation in the noisy setting (right). In both settings the definition of ‚Äúinter-estingness‚Äù is based on a linear combination of the distances induced by the adjacency andLaplacian spectral embeddings of the P matrix. The left panel shows the performance of theILP and the rankers induced by ASE and LSE when the P matrix is observed. Notice thatthe performances of the ILP and LSE (resp. ASE) are the same when Œ± = 0 (resp. 1); inthese cases ‚Äúinterestingness‚Äù coincides exactly with the distances induced by LSE and ASE.The right panel shows the performance of the same nomination scheme for observed graphsinstead of P .In general, it is possible that either ASE or LSE places s‚àó at the top oftheir respective nomination lists for any given Œ±. When this happens, as isthe case for Œ± = 0.7 for the latent positions described in Figure 1, finding alinear combination of weights that optimize (1) may not similarly place s‚àó atthe top of its corresponding nomination list. This happens, for example, whena representation that does not place s‚àó at the top of its nomination list placesthe elements of S closer to the top of its nomination list as compared to therepresentation that places s‚àó at the top. Hence, per the objective function, theweight corresponding to the representation that does not place s‚àó at the topwill be larger and the performance of the ILP may suffer, as seen in Figure 2.Real Data ExamplesWe consider three real data examples: diffusion MRI connectome, searchnavigation, and Drosophila connectome.13          D O S K D                5 H F L S U R F D O  5 D Q N 5 H F L S U R F D O  5 D Q N  R I  V    3  0 D W U L [  , / 3 / 6 ( $ 6 ( 7 U X W K          D O S K D                5 H F L S U R F D O  5 D Q N 5 H F L S U R F D O  5 D Q N  R I  V    ( V W L P D W H G (a)(b)Figure 3: For nominating the anterior cingulate cortex (ACC): histograms of the differenceof paired Mean Reciprocal Ranks (MRR) between the nomination list induced by the IntegerLinear Program (ILP) and the nomination list from ASE (a) and LSE (b). The vertex ofinterest v‚àó and interesting vertex set S were sampled from VACC . MRR was calculatedusing the remaining elements of VACC . Qualitatively, both distributions of differences areinclined to the positive. Quantitatively, Wilcoxon‚Äôs signed rank test yields p-values p ‚âà 0.0057and p < 0.00001 for ASE and LSE, respectively, demonstrating a statistically significantimprovement in nomination performance for the ILP solution.dMRIWe consider a graph GdM RI from a collection of connectomes estimated usinga diffusion MRI-to-graph pipeline [39]. Vertices represent subregions definedvia spatial proximity and edges are defined by tensor-based fiber streamlinesconnecting these regions. GdM RI has n = |V | = 40, 813 vertices and e = |E| =2, 224, 492 edges.The vertices of GdM RI each belong to exactly one of 70 Desikan regions of thebrain ‚Äì 35 anatomical regions in each of the two hemispheres [40]. Furthermore,each vertex also has a designation as either gray matter or white matter. Thus,each vertex has a region label, a hemisphere label, and a tissue type label.We consider J = 2 spectral embedding representations of GdM RI : ASE(embedding dimension m = 15) and LSE (embedding dimension m = 46). Inthe illustrative paper [30], aptly titled On a two-truths phenomenon in spec-tral graph clustering, it is demonstrated that these two representations lead totwo fundamentally different clusterings ‚Äì LSE best for the affinity structure as-14Paired differenceCounts102050Paired differenceCounts102050sociated with hemisphere (left vs. right) and ASE best for the core-peripherystructure associated with tissue type (gray vs. white). That is, there are twotruths, and the two embeddings are each best for recovering a different truth.In the conclusion to [30] the authors write ‚ÄúFor connectomics, this phenomenon[. . . ] suggests that a connectivity-based parcellation based on spectral clusteringshould consider both LSE and ASE‚Äù. The methodology developed herein allowsjust such an analysis.For illustration, we consider the target brain structure to be Desikan region‚Äùanterior cingulate cortex‚Äù (ACC). The vertex of interest v‚àó is chosen at randomfrom VACC, and the remainder of vertices in VACC are designated as S‚àó ‚Äì truly|VACC| = 746, so |S‚àó| = 745. Then S ‚äÇ S‚àó with |S| = 50 issimilar to v‚àó.randomly chosen, leaving |S‚àó \ S| = 695 truly, but unknown to be, interestingvertices out of a candidate set C = V \({v‚àó}‚à™S) with |C| = n‚àí1‚àí|S| = 40, 762.VACC includes vertices from both hemispheres and from both tissue types;loosely speaking, the ‚Äùtruth‚Äù for v‚àó, as exemplified by the characteristics of theelements of S, is a combination of the original two truths. Hence, a nominationscheme that combines LSE and ASE promises superior nomination performance.We compare the nomination schemes induced by the ASE and the LSEdistances to the ILP nomination scheme that identifies an optimal linear com-bination of the two.The competing nomination schemes are evaluated via MRR. We performedthis (v‚àó, S‚àó) sampling a total of 150 times. The paired differences between theMRR from the ILP and ASE and between the MRR from the ILP and LSEare depicted in Figure 3; positive values indicate that the ILP performs betterthan its competitor. Testing for a difference in the medians between ILP and{ASE,LSE} via Wilcoxon‚Äôs signed rank test yields p-values p ‚âà 0.0057 for ‚ÄùH0:ASE as good or better than ILP‚Äù and p < 0.00001 for ‚ÄùH0: LSE as goodor better than ILP‚Äù, demonstrating a statistically significant improvement innomination performance for the ILP solution.15(a)(b)Figure 4: For nominating phones in the Bing graph, three different phones were consideredto be a v‚àó. Each phone has an associated S‚àó consisting of 20 elements. For each v‚àó wepartition its corresponding S‚àó into 5 sets. Left: Histogram of the paired difference of the MeanReciprocal Rank (MRR) between the ILP and Singleton. The effect size of the difference inMRR is quite large in favor of the ILP; Wilcoxon‚Äôs signed rank test yields p ‚âà 0.0003. Right:Recall @ k for various k for the ILP and three ‚Äúnatural‚Äù competitors. We note that 16 is themaximum value and that the ILP convincingly outperforms the other methods.BingThe second real data example we consider is derived from Microsoft‚Äôs Bingsearch navigation data collected over the course of 2019. The search navigationdata consists of pairs of queries that are submitted to Bing in succession by auser within a browsing session. We consider the pairs of queries that correspondto a preidentified list of products in consumer electronics, household appliances,and gaming.We use this data to construct a graph where the edge weight between twoproducts in the graph is a normalized count of the pairs of queries that containthe two products. We remove self-loops and edges with edge weights belowa user-selected threshold and analyze only the largest connected component,GBing. GBing has n = |V | = 7, 876 vertices and e = |E| = 46, 062 directedweighted edges.We consider J = 100 different representations of GBing. Each representa-tion is a Node2Vec embedding [41] corresponding to different hyperparameter16Paired differenceCounts12514080120160200k0246810121416Recall @ kRecall @ k for phones in Bing graphILPSingletonPCARandomsettings.Along with the representations, we are given three vertices of interest. Allthree vertices of interest correspond to phones. For each vertex of interest v‚àó,we are given a set of vertices S‚àó known to be similar to v‚àó. These sets werehandpicked by a team of data scientists at Microsoft. Each S‚àó contains 20products. A product was included either because it is a phone produced by thesame company and is sufficiently close in generation or because it is of the samegeneration but produced by a different company.To evaluate the proposed ILP we use the following scheme. For each (v‚àó, S‚àó)pair we randomly partition S‚àó (|S‚àó| = 20) into five subsets each of size four.These subsets are then each in turn used as S for the corresponding v‚àó andthe ILP is evaluated on the remaining |S‚àó \ S| = 16 vertices via MRR. Thisprocedure resulted in a total of 15 different (v‚àó, S) pairs.We compare the linear combination of the 100 representations found bythe ILP to the nomination scheme that selects a ranker from the subset ofthe {hdj}100j=1 that minimizes the objective function in (1). That is, the secondscheme uses the ranker induced by one of the 100 representations that minimizesthe maximum rank of an element of S.If two or more rankers tie then theranker used is randomly selected from the argmin set. This scheme is referredto as ‚ÄúSingleton‚Äù because it uses only a single representation. Note that whena single representation optimizes (1) then the ILP and Singleton produce thesame ranking.The competing nomination schemes are, again, evaluated via MRR. Thepaired difference histogram and density estimate between the MRR from theILP and the average MRR from Singleton are shown in Figure 4a. Testing thehypothesis ‚ÄúH0: Singleton is as good or better than ILP‚Äù results in p ‚âà 0.0003from Wilcoxon‚Äôs signed rank test. (More to the point: ILP is strictly superiorto Singleton for all 15 cases.)17Comparison to natural competitorsThe problem setting that we consider is, as far as we know, novel and itis (generally) unfair to compare our proposed ILP to methods developed forother settings [42]. Instead, to get an understanding of the proposed method‚Äôsutility, we compare the performance (as measured by Recall at k for variousk) of the ILP to Singleton and two other ‚Äúnatural‚Äù algorithms: one based onPrincipal Component Analysis (‚ÄúPCA‚Äù) [43] and one based on random sampling(‚ÄúRandom‚Äù).In particular, for PCA we use the weight vector found by normalizing theabsolute values of the eigenvalues of the first J = 100 principal components ofthe personalized weight matrix. Thus there will be a different PCA solution foreach query. We chose J = 100 so that the number of representations in the ILPsolution and the PCA solution is the same. For Random, we randomly sampleJ = 100 length unit vectors for the same amount of compute time as taken bythe ILP and choose the best performing vector of weights, as measured by theobjective function above (1). The performances of the four algorithms (ILP,Singleton, PCA, Random) on the Bing experiment described above are shownin Figure 4b. Notably, the ILP outperforms the natural competitors.DrosophilaWe consider a synaptic-resolution connectome of the Drosophila larva brain(unpublished), including its learning and memory center (the mushroom body)[44]. The connectome consists of n = |V | = 2965 neurons. Edges are synapses‚Äì directed. There are four edge types in the connectome: axon to dendrite(|EAD| = 54303), axon to axon (|EAA| = 34867), dendrite to dendrite (|EDD| =10209), and dendrite to axon (|EDA| = 4088). This connectome was manuallyannotated from electron microscopy imagery for a single Drosophila larva brain[45, 46].We consider J = 4 different representations of the connectome obtained viaspectral embedding of the individual Laplacian matrices corresponding to thefour different directed weighted edge types. We omit embeddings correspond-18ing to the spectral embedding of the adjacency matrix because of the relativesparsity of the dendrite to axon and dendrite to dendrite networks.These spectral embeddings yield a representation of each node for each edgetype. Note that since the connectome is directed, the left and right singularvectors differ. We use the concatenation of the two (embedding dimensionm = 11) as the representation, resulting in a 22 dimensional representation foreach neuron for each edge type.The input neurons of the mushroom body (MBINs) are a well known andstudied neuron type within the Drosophila larva connectome [47]. For illustra-tion purposes we consider each of the 26 MBINs in the brain as a v‚àó and 15randomly selected MBINs as vertices known to be similar to v‚àó. We evaluatethe ILP and Singleton via MRR on the remaining 10 MBINs. Recall that Sin-gleton uses a ranker from amongst the single representations that minimizes theobjective function in (1).The paired difference histogram and estimated density between the MRRfrom the ILP and the MRR from Singleton are shown in Figure 5a. Testing thehypotheses ‚ÄúH0: Singleton is as good or better than ILP‚Äù results in p < 0.0001.This result indicates that the different edge types contain complementary in-formation that, when put together, can yield superior inferential procedures ascompared to procedures using only a single edge type.Of particular interest is the breakdown of solutions in terms of edge type.As an example, in one representative trial the best singleton is AA (axon-to-axon), while the ILP solution is the linear combination of (AA, AD, DA, DD)with weights (0.424, 0.123, 0, 0.453). Indeed: the singleton that minimizes themaximum rank of an element of S is always one of AA, AD, DD ‚Äì never DA,and the ILP solution is always a linear combination of just these three edgetypes.Potential upstream impactWhen collecting synaptic data, there are two options: collect neuron-leveldata or collect {axon, dendrite}-level data.In effect, collecting neuron-level19(a)(b)Figure 5: Each of the 26 MBINs in the connectome is considered as a v‚àó. Of the remaining 25MBINs, 15 are then chosen to be vertices known to be similar to v‚àó. The nomination schemesare evaluated on the remaining 10 MBINs. Left: The proposed ILP compared to Singleton.Wilcoxon‚Äôs signed rank test yields p < 0.0001 in favor of the ILP. Right: The proposed ILPcompared to considering the sum of the four different edge types upstream from inference.The ‚Äúwin‚Äù by the ILP demonstrates that the sum of the four edge types, which is what istypically considered for connectome-based inference, is insufficient (Wilcoxon‚Äôs signed ranktest yields p < 0.00001 in favor of the ILP).data is the same as collecting {axon, dendrite}-level data and then summingthe counts across the different edge types or ‚Äúcolors‚Äù.The four different edge types possibly contain different and complementaryinformation for inference related to neuron types in the connectome. This in-formation is likely more useful than the single summary edge weight. Indeed,we demonstrate in Figure 5b that considering the four different colors offers asignificant improvement (Wilcoxon‚Äôs signed rank test yields p < 0.00001) overconsidering the sum of the edge types, for evaluating MBINs. Hence, if theinference task is to nominate MBINs, it is likely worthwhile to devote the extraresources to measure {axon, dendrite}-level synapses as opposed to the coarser-grained neuron-level synapses.ConclusionWe have presented an integer linear programming solution for learning torank via combining representations. This task is of general interest, but for20Paired differenceCounts158Paired differenceCounts159specificity we have presented our methodology and results from the perspectiveof vertex nomination in graphs. The results presented herein ‚Äì analytic, sim-ulation, and experimental ‚Äì demonstrate that this methodology is principled,practical, and effective. Our methodology makes essentially no model assump-tions; just that we are given a query item, we know the dissimilarity (frommultiple perspectives) between it and a set of items to be ranked, and we havea set of items known to be similar to the query.The three experimental settings highlight three complementary aspects ofour vertex nomination problem. In the first ‚Äì dMRI ‚Äì the issue is how to utilizetwo different spectral embedding techniques (LSE and ASE) each known touncover different graph structure [30, 48].In the second ‚Äì Bing ‚Äì the issueis how to utilize a collection of pre-defined representations (Node2Vec, whereinoptimal hyperparameter settings are unavailable at embedding-time, and in anyevent no one setting will be optimal for all tasks) for multiple post-embeddingnomination tasks. In the third ‚Äì Drosophila ‚Äì the issue is how to utilize multipledifferent graphs (in this case, synapse types) each on the same vertex set. Inall three settings, the ILP solution successfully optimizes our ‚Äúlearning to rank‚Äùobjective function to obtain an effective ranking function.Lastly, we note that though the ILP is a method for query-based informa-tion retrieval problems in general, in domains where strong representations areavailable out-of-the-box from pre-trained models, such as computer vision andnatural language processing, the method will be less useful. Regardless, we thinkthat the approach and extensions thereof will be both practical and effective formore nuanced domains.AcknowledgementsThe authors thank Keith Levin, Zachary Lubberts, Ben Pedigo, AntonAlyakin, Eric Bridgeford, Joshua Agterberg, Jesus Arroyo, and John Conroyfor constructive comments on an earlier draft of this manuscript.21References[1] T.-Y. Liu, et al., Learning to rank for information retrieval, Foundationsand Trends in Information Retrieval 3 (3) (2009) 225‚Äì331.[2] D. Conte, P. Foggia, C. Sansone, M. Vento, Thirty years of graph matchingin pattern recognition, International Journal of Pattern Recognition andArtificial Intelligence 18 (03) (2004) 265‚Äì298.[3] D. E. Fishkind, V. Lyzinski, H. Pao, L. Chen, C. E. Priebe, et al., Vertexnomination schemes for membership prediction, The Annals of AppliedStatistics 9 (3) (2015) 1510‚Äì1532.[4] S. Robertson, H. Zaragoza, et al., The probabilistic relevance framework:Bm25 and beyond, Foundations and Trends in Information Retrieval 3 (4)(2009) 333‚Äì389.[5] A. Severyn, A. Moschitti, Learning to rank short text pairs with convolu-tional deep neural networks, in: Proceedings of the 38th International ACMSIGIR Conference on Research and Development in Information Retrieval,2015, pp. 373‚Äì382.[6] J. T. Vogelstein, H. S. Helm, R. D. Mehta, J. Dey, W. Yang, B. Tower,W. LeVine, J. Larson, C. White, C. E. Priebe, A general approach toprogressive learning (2020). arXiv:2004.12908.[7] J. Bekker, J. Davis, Learning from positive and unlabeled data:asurvey, Machine Learning 109 (4) (2020) 719‚Äì760.doi:10.1007/s10994-020-05877-5.URL http://dx.doi.org/10.1007/s10994-020-05877-5[8] F. Mordelet, J.-P. Vert, A bagging svm to learn from positive and unlabeledexamples, Pattern Recog. Lett 37. doi:10.1016/j.patrec.2013.06.010.[9] C. Elkan, K. Noto, Learning classifiers from only positive and unlabeleddata,in: Proceedings of the 14th ACM SIGKDD International Con-22ference on Knowledge Discovery and Data Mining, KDD ‚Äô08, Associa-tion for Computing Machinery, New York, NY, USA, 2008, p. 213‚Äì220.doi:10.1145/1401890.1401920.URL https://doi.org/10.1145/1401890.1401920[10] Y. Zhou, J. Xu, J. Wu, Z. Taghavi, E. Korpeoglu, K. Achan, J. He, Pure:Positive-unlabeled recommendation with generative adversarial network,in: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Dis-covery & Data Mining, 2021, pp. 2409‚Äì2419.[11] M. Zhu, W. Xiong, Y.-F. Wu, Learning to rank with only positive examples,Proceedings - 2014 13th International Conference on Machine Learning andApplications, ICMLA 2014 (2015) 87‚Äì92doi:10.1109/ICMLA.2014.19.[12] E. Pekalska, R. P. W. Duin, The Dissimilarity Representation for PatternRecognition: Foundations And Applications (Machine Perception and Ar-tificial Intelligence), World Scientific Publishing Co., Inc., USA, 2005.[13] L. Devroye, L. Gy¬®orfi, G. Lugosi, A Probabilistic Theory of Pattern Recog-nition, Vol. 31, Springer Science & Business Media, 2013.[14] R. Fagin, Combining fuzzy information from multiple systems, Journal ofComputer and System Sciences 58 (1) (1999) 83‚Äì99.[15] R. Fagin, Combining fuzzy information: an overview, ACM SIGMODRecord 31 (2) (2002) 109‚Äì118.[16] J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han, Setexpan: Corpus-basedset expansion via context feature selection and rank ensemble, in: M. Ceci,J. Hollm¬¥en, L. Todorovski, C. Vens, S. DÀázeroski (Eds.), Machine Learningand Knowledge Discovery in Databases, Springer International Publishing,Cham, 2017, pp. 288‚Äì304.[17] Y.-Y. Wang, R. Hoffmann, X. Li, J. Szymanski, Semi-supervised learning ofsemantic classes for query understanding: From the web and for the web, in:23Proceedings of the 18th ACM Conference on Information and KnowledgeManagement, CIKM ‚Äô09, Association for Computing Machinery, New York,NY, USA, 2009, p. 37‚Äì46. doi:10.1145/1645953.1645961.URL https://doi.org/10.1145/1645953.1645961[18] Y. He, D. Xin, Seisa: Set expansion by iterative similarity aggregation,in: Proceedings of the 20th International Conference on World Wide Web,WWW ‚Äô11, Association for Computing Machinery, New York, NY, USA,2011, p. 427‚Äì436. doi:10.1145/1963405.1963467.URL https://doi.org/10.1145/1963405.1963467[19] V. Lyzinski, K. Levin, C. E. Priebe, On consistent vertex nominationschemes, Journal of Machine Learning Research 20 (69) (2019) 1‚Äì39.URL http://jmlr.org/papers/v20/18-048.html[20] J. Bobadilla, F. Ortega, A. Hernando, A. Guti¬¥errez, Recommender systemssurvey, Knowledge-based Systems 46 (2013) 109‚Äì132.[21] G. Coppersmith, Vertex nomination, Wiley Interdisciplinary Reviews:Computational Statistics 6 (2) (2014) 144‚Äì153.[22] D. Marchette, C. Priebe, G. Coppersmith, Vertex nomination via at-tributed random dot product graphs, in: Proceedings of the 57th ISI WorldStatistics Congress, Vol. 6, 2011, p. 16.[23] G. A. Coppersmith, C. E. Priebe, Vertex nomination via content and con-text, arXiv preprint arXiv:1201.4118.[24] M. Sun, M. Tang, C. E. Priebe, A comparison of graph embedding methodsfor vertex nomination, in: 2012 11th International Conference on MachineLearning and Applications, Vol. 1, IEEE, 2012, pp. 398‚Äì403.[25] S. Suwan, D. S. Lee, C. E. Priebe, Bayesian vertex nomination using contentand context, Wiley Interdisciplinary Reviews: Computational Statistics7 (6) (2015) 400‚Äì416.24[26] J. Agterberg, Y. Park, J. Larson, C. White, C. E. Priebe, V. Lyzinski, Ver-tex nomination, consistent estimation, and adversarial modification (2019).arXiv:1905.01776.[27] J. Yoder, L. Chen, H. Pao, E. Bridgeford, K. Levin, D. E. Fishkind,C. Priebe, V. Lyzinski, Vertex nomination: The canonical sampling and theextended spectral nomination schemes, Computational Statistics & DataAnalysis 145 (2020) 106916.[28] E. Fix, Discriminatory analysis: nonparametric discrimination, consistencyproperties, USAF school of Aviation Medicine, 1951.[29] L. Peel, D. B. Larremore, A. Clauset, The ground truth about meta-data and community detection in networks, Science Advances 3 (5) (2017)e1602548.[30] C. E. Priebe, Y. Park, J. T. Vogelstein, J. M. Conroy, V. Lyzinski, M. Tang,A. Athreya, J. Cape, E. Bridgeford, On a two-truths phenomenon in spec-tral graph clustering, Proceedings of the National Academy of Sciences116 (13) (2019) 5995‚Äì6000. arXiv:https://www.pnas.org/content/116/13/5995.full.pdf, doi:10.1073/pnas.1814462116.URL https://www.pnas.org/content/116/13/5995[31] L. Gurobi Optimization, Gurobi optimizer reference manual (2020).URL http://www.gurobi.com[32] R. Lougee-Heimer, The common optimization interface for operations re-search: Promoting open-source software in the operations research commu-nity, IBM Journal of Research and Development 47 (1) (2003) 57‚Äì66.[33] T. Achterberg, SCIP: Solving Constraint iInteger Programs, MathematicalProgramming Computation 1 (1) (2009) 1‚Äì41.[34] D. R. Radev, H. Qi, H. Wu, W. Fan, Evaluating web-based question an-swering systems., in: LREC, 2002.25[35] P. D. Hoff, A. E. Raftery, M. S. Handcock, Latent space approaches tosocial network analysis, Journal of the American Statistical Association97 (460) (2002) 1090‚Äì1098.[36] A. Athreya, D. E. Fishkind, M. Tang, C. E. Priebe, Y. Park, J. T. Vo-gelstein, K. Levin, V. Lyzinski, Y. Qin, Statistical inference on randomdot product graphs: a survey, The Journal of Machine Learning Research18 (1) (2017) 8393‚Äì8484.[37] U. Von Luxburg, A tutorial on spectral clustering, Statistics and computing17 (4) (2007) 395‚Äì416.[38] D. L. Sussman, M. Tang, D. E. Fishkind, C. E. Priebe, A consistent adja-cency spectral embedding for stochastic blockmodel graphs, Journal of theAmerican Statistical Association 107 (499) (2012) 1119‚Äì1128.[39] G. Kiar, E. W. Bridgeford, W. R. Gray Roncal, V. Chandrashekhar,D. Mhembere, S. Ryman, X.-N. Zuo, D. S. Margulies, R. C. Craddock,C. E. Priebe, R. Jung, V. D. Calhoun, B. Caffo, R. Burns, M. P. Milham,J. T. Vogelstein, A high-throughput pipeline identifies robust connectomesbut troublesome variability, bioRxivarXiv:https://www.biorxiv.org/content/early/2018/04/24/188706.full.pdf, doi:10.1101/188706.URL https://www.biorxiv.org/content/early/2018/04/24/188706[40] R. S. Desikan, F. S¬¥egonne, B. Fischl, B. T. Quinn, B. C. Dickerson,D. Blacker, R. L. Buckner, A. M. Dale, R. P. Maguire, B. T. Hyman,et al., An automated labeling system for subdividing the human cerebralcortex on mri scans into gyral based regions of interest, Neuroimage 31 (3)(2006) 968‚Äì980.[41] A. Grover, J. Leskovec, node2vec: Scalable feature learning for networks,in: Proceedings of the 22nd ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, 2016, pp. 855‚Äì864.26[42] D. J. Hand, Classifier technology and the illusion of progress, Statisticalscience (2006) 1‚Äì14.[43] R. Vidal, Y. Ma, S. Sastry, Generalized principal component analysis(gpca), IEEE transactions on pattern analysis and machine intelligence27 (12) (2005) 1945‚Äì1959.[44] K. Eichler, F. Li, A. Litwin-Kumar, Y. Park, I. Andrade, C. M. Schneider-Mizell, T. Saumweber, A. Huser, C. Eschbach, B. Gerber, et al., The com-plete connectome of a learning and memory centre in an insect brain, Na-ture 548 (7666) (2017) 175‚Äì182.[45] T. Ohyama, C. M. Schneider-Mizell, R. D. Fetter, J. V. Aleman, R. Fran-conville, M. Rivera-Alba, B. D. Mensh, K. M. Branson, J. H. Simpson,J. W. Truman, et al., A multilevel multimodal circuit enhances action se-lection in drosophila, Nature 520 (7549) (2015) 633‚Äì639.[46] C. M. Schneider-Mizell, S. Gerhard, M. Longair, T. Kazimiers, F. Li, M. F.Zwart, A. Champion, F. M. Midgley, R. D. Fetter, S. Saalfeld, et al.,Quantitative neuroanatomy for connectomics in drosophila, Elife 5 (2016)e12059.[47] T. Saumweber, A. Rohwedder, M. Schleyer, K. Eichler, Y.-c. Chen, Y. Aso,A. Cardona, C. Eschbach, O. Kobler, A. Voigt, et al., Functional archi-tecture of reward learning in mushroom body extrinsic neurons of larvaldrosophila, Nature communications 9 (1) (2018) 1‚Äì19.[48] J. Cape, M. Tang, C. E. Priebe, On spectral embedding performance andelucidating network structure in stochastic blockmodel graphs, NetworkScience 7 (3) (2019) 269‚Äì291.27Author biographiesHayden Helm is former research faculty at the Center for Imaging Sciences atJohns Hopkins University and former research intern at Microsoft Research. Hisresearch interests include statistical pattern recognition and modern machinelearning.Amitabh Basu is an Associate Professor in the Applied Mathematics andStatistics department, with a secondary appointment in the Computer Sciencedepartment, at Johns Hopkins University. His research interests lie in Op-timization, geometry, Convex analysis, and the applications of these tools inOperations Research, Astronomy and Data Science.Avanti Athreya was a visiting assistant professor with Duke University anda postdoctoral fellow with SAMSI prior to coming to Johns Hopkins University,in 2011, where she is currently an assistant research professor. Her researchinterests include statistical inference on random graphs and multiscale networkanalysis.Youngser Park holds joint appointments in the The Institute for Compu-tational Medicine and the Human Language Technology Center of Excellenceat Johns Hopkins University. His current research interests are clustering algo-rithms, pattern classification, and data mining for high-dimensional and graphdata.Joshua T. Vogelstein is an Assistant Professor in the Department of Biomed-ical Engineering, with joint appointments in Applied Mathematics and Statis-tics, Computer Science, Electrical and Computer Engineering, Neuroscience,and Biostatistics at Johns Hopkins University.Carey E. Priebe holds joint appointments at Johns Hopkins University inthe Department of Computer Science, the Department of Electrical and Com-puter Engineering, and the Department of Biomedical Engineering, as well asthe Center for Imaging Science, the Human Language Technology Center ofExcellence, and the Mathematical Institute for Data Science.Michael Winding completed two Bachelor‚Äôs Degrees in Biology and Studio28Art at the University of Notre Dame and a PhD in Cell Biology at NorthwesternUniversity. He is a postdoc working with Marta Zlatic, previously at HHMIJanelia and currently at the University of Cambridge, UK.Marta Zlatic is a Croatian neuroscientist who is group leader at the MRCLaboratory of Molecular Biology in Cambridge, UK. Her research investigateshow neural circuits generate behaviour.Albert Cardona completed his undergraduate and graduate studies at theUniversity of Barcelona, postdoc at UCLA, started his lab at the Institute ofNeuroinformatics (Zurich), then at HHMI Janelia, and now is a ProgrammeLeader at the MRC LMB and associate professor in neuroscience at the Univer-sity of Cambridge, UK.Patrick Bourke is a Software Engineer with Microsoft Research. He hasextensive experience building and operating scalable services in industries suchas retail, financial services and cloud computing. His work at MSR focuseson data engineering and software development for graph analysis and graphmachine learning.Jonathan Larson is a Principal Data Architect at Microsoft Research. Hisapplied research work focuses on petabyte-scale data infrastructure, data sci-ence applications, network analytics, and information visualization. He has ap-plied experience in organizational science, neuroscience, cyber-security, counter-human trafficking, fraud analytics, mobile device analytics, media management,retail analytics, and real estate.Marah I. Abdin is a Research Software Dev Engineer at Microsoft Research.She has been involved in multiple projects that use programmable hardware,nano-structural hardware, and machine learning. As she moves on in her career,her passion for problem-solving grows and she strives to continue exploring otherengineering fields through progressive research.Piali Choudhury is an Engineering Manager at Microsoft Research. Shehas been involved in deep systems level programming, networking technology,search, data mining, distributed storage, infrastructure and cloud services, pre-cision medicine, real time communication platforms, integrative and applied29ML/AI. Her passion is to empower research through engineering excellence.Weiwei Yang is Principal SDE Manager at Microsoft Research. She is in-terested in resource efficient alt-SGD machine learning methods inspired bybiological learning. The applied research group she leads aims to democratizeAI by addressing issues of sustainability, robustness, scalability, and efficiencyin ML.Chris White is Managing Director, Microsoft Research Special Projects. Heleads mission-oriented research and software development teams focusing onhigh risk problems.30