Artificial Intelligence 98 (1998) 169-208 Artificial Intelligence the principle of minimum Characterizing cross-entropy within a conditional-logical framework Gabriele Kern-Isbemer ’ LG Praktische Informatik VIII. Deparrment of Computer Science, FemUniversit(it Hagen, P.0. Box 940. D-58084 Hagen, Germany Received October 1996; revised May 1997 Abstract The principle of minimum cross-entropy (ME-principle) is often used as an elegant and pow- erful tool to build up complete probability distributions when only partial knowledge is available. The inputs it may be applied to are a prior distribution P and some new information R, and it yields as a result the one distribution P’ that satisfies R and is closest to P in an information- theoretic sense. More generally, it provides a “best” solution to the problem “How to adjust P to Ii??” In this paper, we show how probabilistic conditionals allow a new and constructive approach to this important principle. Though popular and widely used for knowledge representation, condition- als quantified by probabilities are not easily dealt with. We develop four principles that describe their handling in a reasonable and consistent way, taking into consideration the conditional-logical as well as the numerical and probabilistic aspects. Finally, the ME-principle turns out to be the only method for adjusting a prior distribution to new conditional information that obeys all these principles. Thus a characterization of the ME-principle within a conditional-logical framework is achieved, and its implicit logical mechanisms are revealed clearly. @ 1998 Elsevier Science B.V. Kqwora!r: Probabilistic reasoning; Minimum cross-entropy; Conditionals; Knowledge representation; Nonmonotonic reasoning; Expert systems ’ Email: gabriele.kem-isbemer@femuni-hagende. 0004-3702/98/$19.00 @ 1998 Elsevier Science B.V. All rights reserved. PIISOOO4-3702(97)00068-4 170 G. Kern-Isberner/Art$cial Intelligence 98 (1998) 169-208 1. Introduction Within foundation and reasoning of (quantified) for nonmonotonic based upon prob- in the area of artificial representation attention theory have received theory provides the last decades, knowledge increasing ability a solid Probability and probabilistic (cf. e.g. [ 2,3,10,11,281>, tent computability uncertainty are applied effectively within domains where statistical databases are available, relationships ing and representing dependencies and symptoms fession and consumption commonsense guins are birds but do not fly”, by assigning tions. Probabilistic reasoning the question derived easily-Tweety ment. intelligence. reasoning methods expert systems make use of the consis- I] ) . Probabilistic methods (cf. [ 22,30,3 check- e.g. investigated, or between pro- to represent fly, pen- to the asser- to is a penguin and a bird may be environ- in marketing. as in the most popular example “Birds generally probabilities appropriate in itself, and so the correct answer the variables in medical diagnosis important between diseases “What about Tweety?” where Tweety does not fly in an adequately specified probabilistic they may be used is nonmonotonic Just as well, knowledge, between is represented by a probability over a set of (discrete or continuous) distribution P or by variables, probabilities. inference. On the other hand, conditionals and reasoning the notion of play a ) . The range of (cf. e.g. [ 8,25,29] Thus as well as scientific to formalize and fundamental means representation of (quantified) theory as a mathematically statements, important conditionals established and conditionals as a popular tool to them is central inferences conditional knowledge to be quite a natural are made by calculating knowledge distributions to probabilistic representation includes commonsense Usually, probabilistic a system of compatible and conditionals major part in knowledge their expressiveness proving relationships. An appropriate would close method express knowledge. Unfortunately, ally, a (consistent) only all fulfill the “most adequate” one? More generally, how to proceed tribution P is present resulting set R of conditionals, knowledge, to handle quantified uncertain knowledge, in a posterior distribution P*? that has to be adjusted the circle connecting the probabilistic representations probabilistic probabilistic probabilistic conditionals incomplete probability have to struggle against arbitrariness. Usu- each equipped with a probability, will provide so there will be a lot of distributions which to be dis- information R, in R. Which of them should be chosen if a (prior) probability to some new conditional The aim of this paper is to establish a direct and constructive representation via distributions, link between probabilistic taking prior knowledge the following four principles which mark the conditionals consistently for probabilistic knowledge conditionals into account corner-stones representation and their suitable if necessary. We develop for using quantified and updating: (Pl ) the principle of conditional preservation: this is to express that prior conditional (P2) dependencies the idea of a functional concept which underlies us to calculate a posterior distribution shall be preserved “as far as possible” under adaptation; from prior and new knowledge; the adaptation and which allows G. Kern-lsberner/Artijicial Intelligence 98 (1998) 169-208 171 (P3) (P4) of logical consistency: posterior distributions the principle tently as priors for further the principk of representation invariance: the resulting distribution dependent on the actual probabilistic representation inferences; and of the new information. shall be used consis- shall not be (P4) that solves respectively real functions. to the conditional the representation relationship (P3) (Pl ) links numerical changes structure of the new information. results of updating procedures, (P2) between prior and posterior knowledge by means of forestalls ambivalent should be self-evident within a probabilistic realizes a computable appropriate and the only method above while obeying all of the principles of maximum entropy respectively of minimum cross-entropy (ME-principles), known from statistics and information is given for adjusting of logical; consistency yielding complete1.y based on probabilistic entropy reasoning. a short introduction lead to a scheme the principles and to this scheme, is obtained, of the ME-principles at optimum reasoning in the area of quantified uncertain framework. As we will show, stated is provided by the principles both well the desired result. Thus a new characterization in Section 3). The first two axioms and (P2) will information, theory (cf. e.g. [ 6,14-16,211; invariance will be applied and of representation a prior distribution to new conditional inference method and establishing (Pl ) to (P4) fundamental conditionals 21s a most adjustment problem (Pl) inference methods, Two earlier papers to be the only functional entropy five) fundamental the ME-principles [ 17,331 succeeded [ 26,331 are concerned with characterizing too. Shore and Johnson consistent logically proving (cross-) four (respectively is attained is performed by optimizing systems. Among pendence inference procedure of minimal extent of changes actually occurring under ME-adjustment. as in of which satisfies result that inference linear equational are inde- as an changes, but very few was said about the nature or the axioms of probabilistic in [26] by Paris and Vencovska without assuming in the first place. This justifies ME-inference these authors used for their characterizations a functional, but heavily relying on solving and invariance properties inference. A similar the optimization the properties for entropy The passent paper points out a more constructive not only respects approach (conditional) show here that ME-inference it is basically determined ties where no dependency methods networks do not require conditional for reasoning with probabilistic (cf. e.g. [ 221) , probabilistic lots of probabilities knowledge properly. by conditional dependencies exists), recommending (obeying the ME-principles conditionals. Therefore, networks based on ME-techniques independence to the ME-principles. We independencies but that proper- as most adequate to Bayesian (cf. [ 30,3 1 ] ) in contrast and independence assumptions to process quantified Shore and Johnson [ 331 as well as Paris and Vencovska [26] based their char- in this paper are actually on more general probabilistic the results given acterizations Thus type of constraints from those optimization algebra, as in [ 261. Our development (cf. in [33] and in [26]. In particular, theory, as in 1331, or to transfer [ 271). Moreover, stronger, using only a proper sub- the methods used here are quite different to make use of into the context of linear explains clearly how the ME-principles may be there will be no need the problem constraints than probabilistic conditionals. 172 G. Kern-lsbemer/Art@cial Intelligence 98 (1998) 169-208 conditionals. This may improve significantly the ex- systems that use these principles for knowledge based on probabilistic features of computational and processing formula (3) completely planatory representation representation conditional-logical conditionals tion derived systems). (as, e.g. SPIRIT, cf. [ 30,311). For instance, using ( 12) in Section 4.2 revealing in Section 3 respectively it is possible the the to indicate which of the informa- to a conditional to listing active rules in rule based given by the set R actually make a contribution from the posterior distribution (similar pattern of the ME-distribution, is not (Pl) to impair fundamental to probabilistic the readability to give informal of ME-methods, in an Appendix the four principles of the paper. This paper relations. Section 3 is dedicated section provides some preliminaries and technical details will be necessary pointing out a first striking parallel between A certain amount of mathematics the ideas behind to formalize the desired to (P4) and to prove reasons for definitions and theorems, enclosing correctly results. We will endeavor all proofs organized as follows: The following to logic, so as to fix notations and describe them a brief presentation logic. Section 4 deals with the principle of conditional preservation and conditional of conditional structures. Section 5 representation which will be based on the algebraic concept which is to realize In elaborates a functional the following the last two postulates are dealt with, that of logical consistency sections, in Section 6 and that of representation invariance in Section 7. We prove how they both concept so as to determine influence them uniquely. The adaptation indeed yields in Section 8. Finally, we are able to state a unique posterior distribution, the main operator *e some easy but essential properties of +, and in the (Theorem 31). Section 9 presents and theory concluding revision. All proofs may be found result of this paper: The characterization Section 10 we address connections scheme based on this distinguished the type of the functions the idea of a computable of the ME-adjustment in the Appendix. in the functional to nonmonotonic as is proved reasoning involved solution. concept 2. Probabilistic conditionals We consider probability distributions x which are assumed propositional {u;, Ci} stands symbolizes “v determined by the values of its probability . w = UlU2U3.. ., p(w) =p(r4ljzti3 between a probability measure P and its probability for the sake of correctness fi = {W = Z$&ti3.. events: throughout . 1 Oi E {Ui, a,}}. . A propositional language _L = L(V) is defined variables to be binary. The dotted for one of the two possible outcomes of the corresponding is true”, and negation is indicated by barring, P over a finite set V = {VI, fi, l$, . . .} of literal tii E variable: Ui function p applied i.e. fii = TUi. P is uniquely events to the elementary . ..)=P(Vl=til.V2=02,V3=ti3 ,... ).Thedistinction this paper. Let 0 denote function p is not essential but used the set of all elementary the alphabet V and the classical connectives A (respectively letters A, B, C, . . . . formulas are denoted by capital Roman in the usual way, using juxtaposition) the letters of and 1. Its To each propositional formula A E L: a probability may be assigned via p(A) = events w, and A(o) = 1 Co:*(@)=1 P(W), h w ere the sum is taken over all elementary G. Kern-Isbemer/Artifcial Intelligence 98 (1998) 169-208 173 normal that the complete conjunction form of A. P(A) means disjunctive the population tween complete conjunctions of ,C (on the base of the distribution P). and elementary corresponding reflects the probability to w is a disjunct in the canonical that an arbitrary element of be- the correspondence events induces a probabilistic interpretation has properties which are described by A. Thus conditional to a probabilistic Now C is extended language C* by introducing a con- in the following way: A probabilistic conditional is an expression A * B [A-] ditional operator + and probabilities (or a probabilistic rule, both terms are used synonymously) with antecedent A E C, conclusion B E L and probability x E [0, 11. It is to represent syntactically assertions A -+ B weighted with a degree of cer- tainty X. A probabilistic fact has the form B [ x] , B E C, x E [ 0, 1 ] and is considered to the conditional T u-t B [ x] , where T is tautological. to be equivalent [0, 11) is a flat conditional L” = {.4 v) B[x] non-classical conditional language, the 1 A, B E L, x E operator u-) must not be nested. Antecedent and conclusion of a conditional conditional are propositional A semantic formulas. interpretation of probabilistic conditionals is given by conditional proba- iff p(A) > 0 and that all antecedents If P is a distribution, P fulfills A -+ B[x], P k A - B[x], = X. In the sequel, we will tacitly assume bilities: p (B(A) = p(AB) /p(A) of conditio’nals have positive probabilities. In general, we have x = p(BIA) iff p(A) > 0 and (1 - n)p(AB) = xp(AB), /p(AB) the proportion of individuals so the quotient p(AB) It represents property B to those that B is not true of. Thus it is crucial (cf. conditional, not only within a probabilistic of the conditional A -+ B. or objects with property A which also have of the for the acceptability [ 251). the probability framework determines For a probability the set of all probabilistic distribution P, let 7’h( P) = {A - B[ x] E L* 1 P k A + B[x]} in P. Th(P) explicitly conditionals which are valid the conditional embodied this paper is going to deal with can now be described knowledge in P. in a more formal denote represents The problem manner: (*) Given a prior distribution P and some set of probabilistic BI[xI~,...,& distribution P* with P* k R? - B, [ x,] } C C*, how should P be modified conditionals R = {Ai u+ to yield a posterior To maintain compatibility (P* < P), the set R between prior and posterior distributions, P* has to be i.e. p (0) = 0 implies p* (w) = 0. Thus to avoid obvious is some to be P-consistent, that means is supposed this paper, we will assume without is stated explicitly P-continuous inconsistencies, distribution Q with Q < P and Q k R. Throughout further mentioning in R, i.e. if for any Q < P, Q k R implies q(w) = 0 then p(w) = 0, or there is a conditional A - B[x] E R such that either x = 1 and AB(w) = 1 or x = 0 and AB(w) = 1. that the necessity of zero posterior probabilities there In the next section, we are going to present a special solution to the adjustment problem (:r) : the distribution at optimum entropy. 174 G. Kern-lsberner/Art~cial Intelligence 98 (1998) 169-208 3. The principles of entropy (where in statistical mechanics The entropy H(P) = - C, p(w) the convention logp(o) the sum is taken over all elemen- 0 log0 = 0) of a distribution P first appeared as tary events w, using and was later interpreted by Shannon as an a physical quantity information-theoretic review, cf. [ 161). It is generalized by the notion of cross-entropy (also called relative entropy) WQ, 0 = C, do) = co for q(w) # 0) between the uniform distribution po( w) = 1 /m for all elementary two distributions Q and P. If PO denotes events w, then measure of the uncertainty to P (for a historical (with OWO/O) = 0 and q(w) log(dw)/p(w)) log(q(w)/O) inherent R(Q,Po) = -H(Q) + logm relates absolute and relative entropy. So maximizing constraints the same constraints. Therefore to minimizing is equivalent relative entropy the principle of minimum cross-entropy absolute entropy under some given to the uniform distribution under minR(Q,P) = cq(o) w log% s.t. Q is a probability distribution with Q /= R can be regarded as more general than the principle of maximum entropy maxff(Q) = - c q(w) 0 lwdw) st. Q is a probability distribution with Q /= R. (1) (2) We refer to both principles cross-Entropy for Minimum as the ME-principle, where the abbreviation ME stands both and for Maximum Entropy. information-theoretic and further is a well-known and has been studied extensively cf. Cross-entropy two distributions duction is known in statistics also called directed divergence for it lacks symmetry, in general, R(Q, P) 2 0, and R(Q,P) = 0 iff Q = P (cf. [6,32]). so it is not a metric. But cross-entropy [ 321; cf. [ 341). values to yield best expectation references measure of dissimilarity (for a brief, but informative In particular, optimizing between intro- entropy (cf. [ 14,161). Cross-entropy is i.e. R( Q, P) and R( P, Q) differ is positive, that means we have For a distribution P and some P-consistent set R of probabilistic rules distribution Pe = P, (P, R) i.e. P, solves P (cf. [6]), that fulfills R and has minimal (1). relative entropy there is a to the prior The condition Q b R imposed on a distribution Q can be transformed equivalently into a system of linear equality constraints grangian techniques, we may represent Pe in the form for the probabilities q( 0). Using the La- (3) G. Kern-isbemer/Artijicial Intelligence 98 (1998) 169-208 175 with the cri’s being exponentials in R, and cy,-, = exp( ho - 1)) where ho is the Lagrange multiplier of the Lagrange multipliers, one for each conditional of the constraint C&?(w) = 1. By construction, P, satisfies all conditionals in 72: pe(BilAi) = Xi, which is equivalent to ( 1 - x)pe(AiBi) nonlinear equations = XPe(Aig) for all i, 1 < i < n. SO (~1,. . . , LY, are solutions of the Xi cyi = ,- Cw:AiS,(o)=l Ptw) llj+i,AjB,(o)=I &i-” nj+i,,4jF(,)=* cyj’j (4) * -XiCw:AiB;(o)=lP(W) IIj+i,*,L3,(o)=l @:-"j llj+i,,,(,)=, cyi"' > 0, Xi E (03 1)~ with (pi =CO, Xi= 1, 1 <i<n, = 0, Xi = 0, the {conventions coo = 1, 00~ ’ = 0 and 0’ = 1. (~0 arises simply as a normalizing rule when P is modified. using factor. Each ai symbolizes It depends on the prior distribution P, the other rules and probabilities it corresponds distinguished way-on in R and-in to. a the impact of the corresponding of the conditional the probability the formulas above appear deterrently complex at first sight, (3) shows rather Though clearly how factor cyo, at most which satisfy on whether w satisfies conditionals rules the fundamental affect the (conditional) to. the ME-adjustment to a rule is carried out: Apart the probabilities of those complete of this rule. And the new probability the antecedent the conclusion or not. In particular, of Th(P) whose antecedents do not fulfill an antecedent in 72 remain unchanged. This means that the ME-adaptation principles of conditional logics: Asserting knowledge about states which from conjunctions depends the probabilities the normalizing o are changed in addition of all of any of the respects one of should only the conditional may be applied a conditional This intuitive and reasonable principle of conditional preservation will be elaborated more deeply in the next section. But first, we will illustrate formulas shows knowledge processing representation example the second example deals with transitive by using at optimum entropy the probabilistic (cf. [ 3 11) . the use of the ME-approach and (3) and (4) by two simple but informative the benefits of the examples. The first information, whereas results were obtained expert system SPIRIT which realizes knowledge processing in the case of conflicting inference. All numerical students are adults”, “Usually, Example I ( Conjicting information). A knowledge base is to be built up representing “Typically, and “Mostly, students attached are not employed” with probabilities to of 0.99, 0.8 and 0.9, variables A = Being an Adult, S = respectively. Let A, S, E denote Being a Student, may be written as R = {s y-) a[xl],u x3 = 0.1. -No prior u-) e[xz],s is at hand, so we start from and E = Being Employed. The quantified subjectively the propositional u-) e[xJ]}, XI = 0.99, x2 = 0.8, the uniform distribution. adults are employed” information information conditional 176 G. Kern-lsbemer/Artificial Intelligence 98 (1998) 169-208 We are interested combines the evidences a and s conflicting with respect to E. in the probability of the conditional us rcc) e the antecedent of which SPIRIT calculates Pe (elas) = 0.1009, which is much more closer to x3 than to x2. So as we should to be definitely a subset of the set conveyed by the third rule is this preference of the more specific knowledge the more specific expect. If we set x1 = 1, assuming of adults, a probabilistic necessity, and may also be seen clearly by using a clearly but not completely, the set of students (3) and (4) : s dominates information Let PL denote the ME-solution to the same problem as before, except that now xi = 1 instead of xi = 0.99. For arbitrary x2,x3, we obtain with x2 cp + 1 ff2 = 1 - x2 $x3 + 1’ x3 --=_-(y a3 = 1 - x3 *;-x2 Cqx2 x3 -1 l-x3 2 * implies This constant priors are being canceled, and that (~1 = co). This shows pd( e ]as) = x3. at once CX~CX~ = x3/( 1 - x3) that the normalizing (note factor and the solves information Thus ME-inference in an elegant way the problem of conflicting evidences. dominates more general knowledge by virtue of the inherent mech- Specific anisms, without any external preferential and without rankings as in [ 10,121. The weight of a rule is encoded by its logical structure and its probability, It is only the ap- its dependency on other rules being given implicitly. plication of the ME-principle which combines thus allowing a convenient modularity of knowledge rules to yield inferences, the probabilistic or hierarchical representation. as in [4,20], structures In the second example, we will make once more use of symbolic calculations to reveal knowledge processing. the three propositional Example 2 (Transitivity). Let Y, S, C denote Being Young, S = Being Single and C = Having Children. We know young people are usually have children with xi = 0.9, x2 = 0.85. Again we take the uniform distribution A calculation with SPIRIT shows y --+ c[ 0.8151, connecting both rules transitively. By rule can be proved: use of the formulas variables Y = that (with probability 0.9) and that mostly, singles do not 0.85). Here we have R = {y -+ s[xr 1, s -++ E[x~]} as prior information. (3) and (4)) a more general (with probability (or assume) transitive singles inference Pe: For arbitrary xi, x2, we obtain for the ME-distribution with x1 *I = 1-2 ** 2 Ly2 + 1, x2 a2= 1 _x2’ G. Kern-lsbemer/Art@ial Intelligence 98 (1998) 169-208 177 This yields qLy;‘x2 = 2x1(1 -x2) 1 --xi and PAYE) -= PdYC) 1 +2x1x2 - Xi 1 - 2X1X2 + Xi ’ proving pe(Ely) = $1+2x1x2 --Xl). (5) Of course, the correctness of this formula is independent of the particular meanings of the propositional variables involved. So (5) states a general transitive inference rule for problems with an analogous knowledge structure. More ME-deduction rules may be found in 1: 191. These examples are but to give an idea of the soundness and the power of the ME- principle. The rest of this paper will be dedicated to its development from a conditional- logical point of view. 4. The principle of conditional preservation 4. I. Conditional structures Following Calabrese [ 51 (and, earlier, De Finetti [ 71)) a conditional A ?c) B can be represented as a generalized indicator function (B ]A) on elementary events, setting 1, &GAB, (BIA.)(w) = 0, w E AB, u, o $A, { the non-classical where u stands for unde$ned. This definition captures excellently character of conditionals within a probabilistic framework. According to it, a conditional is a function that polarizes AB and AB, leaving x untouched. Due to their non-Boolean nature, coaditionals are rather complicated objects. In particular, it is not an easy task to handle the relationships between them so as to preserve conditional dependencies “as far as possible” under adaptation. To make the problem plain and to point out a possible way to solve it, we give an example which is taken from [36] and which illustrates a phenomenon also well known under the name “Simpson’s paradox”. This example is based on a real-life investigation. Example 3 (Florida murderers). During the six year period 1973-1979, about 5000 murder cases were recorded in the US state of Florida, and the following distribution P mirrors the sentencing policy in those years (for further references, cf. [ 36, pp.46ff] ). The propositional variables involved are V = Victim (of the murder) is black respectively white, b E {ub, uW}, M = Murderer is black respectively white, ti E {mb, m,}, and D = Murderer is sentenced to Death, ci E {d,6}. 178 G. Kern-lsberner/Art$cial Intelligence 98 (1998) 169-208 P: v,mwd 0.015 1 v,m,d 0.4353 v,mbd 0.0101 vwmbd 0.0502 ubmwd 0 vbmwd 0.0233 VbTtlbd 0.0023 Vbmbd 0.4637 Thus P implies m, + d[0.0319], mb + d[0.0236], seemingly so justice however, become strikingly of the victim, is also taken into account: apparent passed sentences without if the third variable V, revealing respect of color of skin. Differences, the color of skin v,m, -+ d[0.03351, uwmb us d[0.1675], vbmw - d[Ol, ubmb -+ d [ 0.00491. If e.g. the probability of the rules tiriz ++ d containing manner. of the conditional mb u) d [ 0.02361 is to change, important information should be preserved the probabilities in an adequate This last example and the conditionals environment. illustrates a strange but typical behavior involved may have. Let us look upon that marginal distributions in an abstract this problem Suppose P is a distribution over a set of variables P + a u-) b[ x] . In which way may a third variable C affect this conditional, can be said about the probability of ad y-) b in P? containing A, B, and suppose i.e. what Roughly, there are two possibilities. at all, that is to say we have p( blat) = p( bla). We may classify behavior, showing B and C to be conditionally independent given a (cf. calculation, we see that p (blat) = p (bla) straightforward In the first case, C does not affect a u-) b[x] this as a monotonic [ 361). By a iff p(abclp(ab4 p(ak)p(abE) = l . In the second, more usual case, we have p( blue) f p (bla), and consequently p(abdp(abG + 1 p(abc)p(abE) * Thus departures tonicity, to introduce interaction quotient from conditional independence-and thereby a logical aspect-may be measured by the cross product the extent of nonmono- ratio or p(abc)p(ak) p(a&)p(abE) . G, Kern-Isbemer/ArtiJicial Intelligence 98 (1998) 169-208 179 A reasonable of a ys b then is that posterior demand for a posterior distribution P* adapted interaction should be the same as prior interaction, to a changed probability i.e. p*(ubc)p*(&) = p(ubc)p(&) p*(ubc)p*(abZ) p(ubc)p(ubE)’ (6) In statistics, the variables logarithms of such expressions are used to measure the interactions between [ 13,361) interaction involved (cf. In the general case, we consider . influ- formulas A, B instead of variables A, B, joint (instead of one single variable) on the conditional A us B, into account. Thus the notion involving more elementary and being based appropriately a logical and ences of groups of variables and, last not least, we have to take a set of conditionals of (statistical) events both in the numerators on R. The comments meaning offers a suitable way to carry out the necessary generalization point of view: In (6), quotients has to be generalized, and in the denominators above following Example 3 give interaction from a conditional-logical two sets of elementary that fits the intention of this paper better events are related than a statistical interpretation quotients in the numerator, a y-) b is once confirmed to each other with respect and {a&, ubE} in the denominator. to P In both sets, (by abc respectively ubE) and once refuted so both sets show the same behavior with regard to the new idea of a behavior or structure with respect to ‘R may events. We choose a group of elementary for sets or multi-sets and P*: {abc, a6c) the conditional (by a& respectively conditional be formalized theoretical presentation. easily a&), a us b[x]. This in R = {At u-) Bt[xl],...,A, To each conditional Ai UC) Bi[xi] we two symbols ui, bi. Let FR = (~1, bl, . . . , a,, b,,) be the free abelian group with . . . a: b? and each element can be identified by [ 231). The commutativity of FR in R shall be effective all at a time, without associate generators al, bl , . . . , a,, b”, i.e. FR consists of all elements of the form @by’ with integers zi, wi E Z (the ring of integers)), its exponents corresponds assuming to the fact that the conditionals any order of application. is isomorphic so that FE to Z2” (cf. -+ B,[x,]} For each i, 1 < i < n, we define a function Ui : D -+ FR by setting ai if (BilAi)(o) = 1, ai(w) = bi if (BilAi)(w) ~0, 1 i if (BilAi)(w) = U. represents U;(O) in which event w. The neutral element 1 of FR corresponds elementary of A; -+ Eli [ Xi] in case that the antecedent Ai is not satisfied. The function the conditional Ai + Bi[Xi] the manner to the non-applicability applies to the c=crR: a+ FR, a(o) = d := n gi(m) = n ui I<i<n 1<ig. AiBi(wkl bi r]: l<i<’ - A,B,(u)=l (7) describes respect to R. Having lthe all-over effect of R on o. cJ’ is called the conditional structmz of o with the same conditional structure defines an equivalence relation 3~ 180 G. Kern-lsbemer/AriifciaI Intelligence 98 (1998) 169-208 on 0. For each elementary both of them because each conditional structure The notion of conditional event w, ma contains at most one of each ai or biy but never applies is generalized to o in a well-defined way. to multi-sets in a straightforward way. A multi-set more than once in a multi-set. Multi-sets use the following notation: is a collection of elements as a set, except that each element may occur are also known as bugs. In the sequel, we will Notation 4 (Multi-set). A multi-set containing each element yj occurring with multiplicity mj, of such a multi-set The cardinality is the sum xi mj of its multiplicities. a finite number of elements yl, y2,. . ., is denoted as {yl : ml, y2 : m2, . . .}. Definition 5 (Conditional denote a multi-set of elementary called the conditional structure of L?, . structure of multi-sets). Let 0, = {WI : q , . . , , o,, events. The element Oi’ := nlGiGnr, a(wi)” : rnr, } E FR is the conditional Thus represented with each Ui occurring with exponent &.aiColj=ai by a group element which structure of a multi-set 01 = {WI : ~1,. . . , w,, : rm,} is ai, bi of FR, rk = & ~BiJAi~~or~=l rk, and each b; is a product of the generators q that the corresponding in case in 0,). So the exponent of ai in q OcCUrkg With exponent ck ai(wt)=b, rk = ck: ~B.IA.~~ot~=O rk (note that each of the sums may be zero to any of the elements events multiplicity, events in 01 which confirm the conditional Ai +-+ Bi, each event being counted with its and in the same way the exponent of bi indicates the number of elementary the number of elementary that refute Ai 4 Bi. cannot be applied cbhitional indicates Oi’ encodes this information in an elegant manner. We could have used a simpler positive and negative e.g. with of conditionals. But making use of a group structure allows us to form representation tuples of natural numbers representing representation, applicabilities products and thus provides a more convenient structures. Moreover, obviously (see Theorem 11) . the group element parallels the structure of its posterior probability, corresponding to an elementary and handling of conditional (7) as we will see later on event Definition 6 (R-equivalence onl,: rm,} and & = (~1: sl,...,~,,: of multi-sets). Two multi-sets RI = { 01 : rl , . . . , s,,} of elementary events with equal cardinali- i.e. iff their conditional ties &k(m, structures with respect to R are identical. rk = ClgrGnr2 q are R-equivalent iff q = q, The additional prerequisite that both multi-sets must have the same cardinality to the fact that actually, requiring account, Recalling the remarks the normalizing equal numbers of elementary following Definition conditional T [ 1 ] has also to be taken in both multi-sets. events 5 above, an immediate description of is due into R-equivalent multi-sets can be stated: Lemma 7. vnz2 : s,,,~} are R-equivalent Two multi-sets 01 = (01: rl,..., iff all of the equations ok, : r,,} and L$ = (~1: SI ,..., G. Kern-lsbemer/Art$cial Intelligence 98 (1998) 169-208 181 c 1 <k<m, rk= c I<l+n~ Sly c k: (BilAi)(wt)=l c k: (B,jA,)(w)=O rk = rk = c [: (B,IAi)(vr)=l c I: (BilA;)(v~)=Il Sl, Sl holdforalli=l,...,n. 4.2. C-adaptations R-equivalent multi-sets show an equal, indistinguishable behavior with respect to 72, so the corresponding posterior probabilities should be, in some sense, quite similar. Conditional structures, however, are abstract objects, independent of probabilities, and we have to relate them to the distributions involved appropriately. This can be achieved by considering the relative change function p*/p, giving rise to the following idea: If the changes the prior distribution undergoes when being adapted to R is to be based on R in a reasonable and intelligible way there should be no difference in the relative changes of probabilities of R-equivalent multi-sets (as defined below). Adaptations following this idea will be called c-adaptations. Definition 8 (C-adaptation). Let P* be a distribution which fulfills R. P* is called a c-adaptation of P to R iff it satisfies the following two conditions: (i) (ii) = 0 if and only if p(o) = 0 or there is a conditional in R such that either Xi = 1 and Ai&( w) = 1, or Xi = 0 and For an; w E a, p*(o) A, VJ Bi [ xi] A,Bi(w) = 1. The R-equivalence of any two multi-sets 01 = (01 : rl, . . . , w,, : rm,} and sm2} of elementary events Wk, v[ with p ( Wk) , p (vr) > 0 &={v,: implies Sl,...,V,*,: p*(O,)Q . . .p*(Wm,yml = p*(v,)s’ P(W)” . ..P(%.Yrnl P(W)” * 1 ‘p*(vm*)S~2 . . .p(vm*)% ’ which is equivalent to P*(wY’ p*(vI)Q * * ‘p*(Wm,yml = p(q)” . . .p*(vm,)S”2 P(Q)“‘. . . .p(wm,)‘~l . .p(Vm2)+ * for p*(Ok),P*(VI) # 0, 1 < k 6 1121, 1 < 2 < m2. (8) (9) The first condition (i) above states that the posterior distribution is P-continuous and is positive otherwise if not demanded explicitly by R. Note that in particular, two R- equivalent multi-sets have the same cardinality. This precondition ensures that an equal number of factors is involved on both sides of (8) respectively both in numerators and denominators in (9). This allows to interpret (9) in fact as a generalized interaction quotient. So, starting from problems involving interaction quotients, we discovered the 182 G. Kern-Isberner/Ar!l~cial Inrelligence 98 (1998) 169-208 structures as an adequate means to describe to R and to relate their probabilities (8) and (9)) we ended up with the behavior of sets to one another. the intended events with respect concept of conditional of elementary By virtue of the equivalence generalization Another between of interaction quotients. of conditional interpretation have the same conditional multi-sets equal conditional weight, and (9) describes a balanced structure with respect system. structures makes (9) directly intelligible: if two they represent to R, then Notation 9. For a prior distribution P and some P-consistent conditionals, the set of all c-adaptations let C (P, R) denote of P to R: set R of probabilistic C( P, R) := {P," ] Pz is a c-adaptation of P to R}. Theorem 11 respectively Corollary 12 will show that ME-adaptations are c-adaptations, so C( P, 77,) # 0 for P-consistent R (cf. Corollary 13). A c-adaptation and R as a guideline the adaptation problem for changes. (*) : is completely based both on P and R, using P as a reference point to It realizes perfectly a conditional-logical approach Postulate P* EC(P,R). (Pl): conditional preservation. The solution P* of (*) is a c-adaptation: Let us consider the example presented in Section 4.1. the benefits of all these technical definitions when being applied to Example 3 (continued). Assume relationship this new information. associated with R. The conditional follows: between mb and d, say mb 4 d [ 0.031, and we want P to be adjusted that in a following year, we observe a slightly changed to and let two symbols at, bl be structures with respect to R are calculated easily as so we have R = {mb u-t d [0.03]}, (v,m,d)a= (v,m,d)” = (ubm,d)a = (vbm,d)” = 1, (U&n&)” = (Ubmbd)” = Ul , (Uwmbd)” = (f.$,mbd)” = bl. Consider conditional R it has to satisfy the (multi-)sets fit = {v,mbd, structures q = at bl = q. Therefore vbmbd} and 02 = {vbmbd, v,mbrf) with equal of P to for P* to be a c-adaptation f’*hvmbd)f’*(vbmb~) _ I’hvmbd)P@bmb~) P*(vbmbd)P*(hvmb~) - P(ubmbd>J’(hvmb& ’ which corresponds to (6). Thus the concept of conditional structures helps to get a technically clear and precise formalization of the intuitive idea of conditional preservation. G. Kern-Isberner /Artificial Intelligence 98 (I 998) 169-208 183 The definition of a c-adaptation given above, however, is rather a technical and abstract but conveys a simple but important the 8 when regarding one. It describes the conditional-logical hardly any idea of its actual appearance and form. Nevertheless, property of c-adaptations may be seen at once from Definition case I&( := j&n,l = 1: behavior of such a distribution, Lemma 110. Let P* E C (P 72). Then for any two elementary events 01, w2 E 0, p (01) , p (~2) # 0, with equal conditional structures 07 = UT, we have P*(“‘l) -- P(Wl) P”(WZ> = ~. P(~2) The next theorem provides a catchy and easy characterization of c-adaptations: Theorem 11 (Characterization {Al + Bl[n~l,...,& Let P* denote a distribution. of c-adaptations). Suppose P is a distribution and R = is a P-consistent set of probabilistic conditionals. -+ B,[x,]} P* is a c-adaptation of P to R if and only if there are real numbers (~0, cur, al, . . . , a,+, “; withao>Oanda~,cu; ,..., a$,cr; satisfying the positivity condition a++; > 0, a+=Oi#x;=O, (yi=Oi#x;=l (10) (11) (12) and the adjustment condition (1 - X;)f$ c o: A,Bi(o)=l P(W) cyi’ I_I j+i a; j--J j+r - .4,Bjb~4 AjBj,“kl = XiU!F o: A;~(o)=l j+i *jBjco,=’ jtt - AjBj(&l,=l 1 6 i Q n, such that P*(w) =~oP(@) 4 n IQ<,, AiB;,w)=l ai n ,+,a AiBi(“)=l for all elementary events w. Thus probability values of c-adaptations parallel the conditional corresponding the Appendix. Comparing elementary events (cf. (7) ) . The proof of this theorem can be found structures of the in (3) and (4) to ( 12), ( 11) and (lo), we get as an immediate consequence: Corollary 12. Any ME-adaptation is a c-adaptation. Because ME-adaptations exist for priors P and P-consistent sets R, we have Corollary 13. For any prior distribution P and any P-consistent set R of probabilistic conditionals, C (P, R) # 8. 184 G. Kern-Isbemer/ArtQicial Intelligence 98 (1998) 169-208 So c-adaptations generalize the concept of ME-adaptations conditional-logical Distributions environment. We will make use of c-adaptations of this type will play a major part in the rest of this paper. and embed it into a in the form ( 12). Notation 14. Let P be a distribution, numbers such that and let UT, ay1, . . . , CY$, a; be nonnegative real Then P[ar,a;, . . . ,a:, a;] denotes the distribution with probability function p[ff;,ff; ,..., Ly;, a,l(w) = LyoP(@) n a+ I<i<,, Ap;(co)=l - a;, I-I I<i<” AiBj(Okl The normalizing Note that P [ CUT, a;,. . . , a:, cu; ] is P-continuous. factor crc is completely determined by P and CY;‘, crl, . . . , a,f , a;. According to Theorem 11, for any c-adaptation P* of P to R, there are nonnega- tive real weight factors a:, a;, Define . . . ,aL,a;]. P[a:,a;, . . . , con+, a; satisfying (10) and (11) such that P* = wf(P*) := {(Ly+;,.. . ,a,+,a,> E KP” 1 (C+&..,& a; ) satisfies ( 10) and ( 11) and P*=P[cuT,a); ,..., CY,‘,CY,]} for any P* E C( P, 72). In general, weight so that card(wf(P*)) determined, > 1. factors of c-adaptations are not uniquely As the proof of Theorem 11 shows, ( 10) ensures that all premises Ai occurring in R have positive probabilities to P[c+q ,..., a,f,n,] in P [ CT;‘, a;, . , . , a$, LY; 1, and ( 11) then is equivalent FR. Corollary 15. Let P be a distribution, and suppose R is a P-consistent set of prob- . . , CY:, a; are reals satisfying ( 10) then P[ CUT, (Y;, . . . , abilistic rules. If CUT, al,. ~,+,a;] ,..., a~,ff~fifjll(ll). ~RiifSff~,a~ Therefore wF(P,R):= u wf(P*) P*EC(P,R) = {(+q ,..., (Y,+,cy,) ER2” 1 (a$q- ,.*., a,+,a,) satisfies (10) and (11)). (13) G. Kern-Isberner/Artificial Intelligence 98 (1998) 169-208 185 So, c-adaptations actually realize quite a simple idea of adaptation to new conditional the posterior probability structure of each elementary information: When calculating check the conditional Bl[x~l,...,A,, quantities and ( 11)) i.e. so that R is satisfied. Finally, aa is computed as a normalizing make P* iI probability Example 16 briefly y-) B,[x,l} c L*, set up p*(w) . . . , a,+, a$ and then determine distribution. illustrates according appropriate event w with respect this adaptation unknowns scheme. a:, a;, to (12) with unknown (10) factor to using function p*, one only has to to R = {At ++ Example 16. Let P be a positive distribution R = {a -b b[ X] } with x E (0,l). Applying of P to R may be written as over two variables A, B, and suppose the formulas above, any c-adaptation P,* p,*(ab) =aop(ab)a;t, pt(a2;) = aop(ab)n;, J?,*(izb) = (Yop(Zlb), p,* (56) = ct’op (is) with cult, a, - >o, (1 - x)atp(ab) = xo;p(ab), CYO = ($p(ab)n: +p(Sib) +p(ab))-‘. C-adaptations provide a straightforward scheme ment probblem (*). ME-adaptations interest c-adaptations in general. to investigate which of the characteristics are a special to calculate to the adjust- solutions instance of this scheme, and it is of also hold for of ME-distributions The author proved in [ 181 that c-adaptations inde- part in Shore pendence and of subset independence which both played an outstanding and Johnson’s [33] characterization of the ME-principle. They also cope in an elegant manner with irrelevant information in that posterior marginals are determined only by conditionalls involving the respective variables (cf. [ 181; cf. [ 261). All this is due to their modular, conditional-logical structure. the properties of system possess There is another principle that ME-adaptations actually seems to fail at first sight and that can now be formulated adequately and proved in terms of c-adaptations: it is the Atomic@ Principle stating that substituting formulas for variables shall not affect the adjustment process (cf. [ 271) : Theorem 17 (Atomicity principle). Let V = {VI, fi, . . .} and V’ = {V:, Vl, . . .} be two finite disjoint sets of binary propositional variables with corresponding sets of elementary events 0 respectively O’, and let V be another binary variable not contained in either is a propositional formula that is neither a tautology of them. Suppose A E JC(V’) nor a contradiction, using only variables in V’. Let R = {At VJ Bt [xl], . . . , A,, -+ lie a set of probabilistic conditionals with antecedents Ai and consequences B,[x,]} Bi in 13(V U {V}). Let A: respectively Bd denote the formulas that arise when each occurrenclo of V in Ai respectively Bi is replaced by A, 1 < i < n, and so 72’ = {Af -+ BfM,. c L*(VU V’). --+ B,~[x,]} ..,A; 186 G. Kern-lsberner/Artificial Intelligence 98 (1998) 169-208 Consider the two distributions P’ over V U V’ and P over V U {V}, respectively, that are related via p(tiw> = CofEn,, A~~,)=~ p’( 0’0) , and suppose R to be P-consistent. Then RA is P’-consistent, and WF( P R) = WF( P’, 72”). We omit the straightforward but technical proof. This result emphasizes of the weight scheme. factors cuf, cu,, . . . , a$, a; as logical representatives the importance of an adaptation The concept of c-adaptations, that, even to be adjusted 16 shows Example and one conditional termined. In general, WF( P, R) will contain different posterior c-adaptations. Demanding concept that guides type (12) information R. in dependence arises however, in the simple case when dealing with is not perfect-it to, the resulting c-adaptation lots of elements, uniqueness means fails to satisfy uniqueness: two variables is not uniquely de- and there will be many to assume a functional of the finding of a “best solution” so that a unique distribution of the prior knowledge P and the new conditional 5. The functional concept It is not only the abstract property of uniqueness sense, i.e. a-somehow In a fundamental desirable. dence between prior distribution, distribution, all distributions P and all P-consistent quite monstrous. The knowledge let alone functional well-behavedness. introducing new (conditional) well-behaved-function that makes a functional there should be a clear and understandable information concept depen- and resulting posterior F : (P, R) H P* that works for sets R. These arguments P and R, however, are represented by them is usually huge and hard to grasp, to describe a or even differentiability such concepts as continuity relevant with respect clear what relevant information Moreover, P* should depend signijcantly only on the relevant parts of the prior P, i.e. requires making to the new information R. Treating this problem is, and how irrelevant information y-) B, [cc,] }, and suppose 9, P2 are two distributions should be handled. -)Bt[xt],...,A, =pz(wlA;) LetR={At withpt(w]Ai) for all o E nand on all parts which are relevant with respect relative changes should be insigni$cant, namely a constant in irrelevant parts) : forall i= to R, so the difference l,...,n. (due Then Pt,P2 match in their posterior to possible differences and R representing Suppose F : (P, R) +-+ P* Definition 18 (Relevance condition). assigns a P-continuous distribution that distribution P* satisfying R to any pair (P,R) with P being a set of probabilistic the relevance condition iff the following holds: Then F fulfills LetR={At with pt(o]Ai) PI(O) = 0 iff p2 (o) = 0 and such that R is PI- and P2-consistent. Let Pl := F( Pk, R), k = 1,2. Then p;(w) = 0 iff p;(w) = 0 and u-1 B, [ x,] }, and suppose PI, P2 are two distributions for all w E 0 and for all i = l,...,n, -+Bt[~tl,...,A, = pz(w]Ai) a P-consistent is a function rules. G. Kern-lsberner/Artificial Intelligence 98 (1998) 169-208 187 PT(OJ) : &(a) = const Pl(~,) P2(@) * for all w E D with pi(w) # 0, k= 1,2. Let dP denote the set of all pairs (P R) representing a solvable adjustment prob- lem (*): dP := {(P, R) 1 P distribution, R c C*, R P-consistent}. According a c-adaptation. reasonable within to postulate (Pl), the solution to the adaptation problem So we will now focus on further assumptions the special context of c-adaptations. (*) should be that appear useful and At first, the following proposition tion 18 in fact are able to capture shows that the prerequisites information the idea of relevant formulated for c-adaptations: in Defini- Proposition 19. Let R = {Al ++ Br [XI], two distributions with pl(olA;) p,(w)=OifSp2(o)=OandsuchthatRisP,-andP2-consistent. wF( P2, K!). = pz(w]Ai) . . . , A,, u-) B, [ x,]}, and suppose 9, P2 are for all w E 0 and for all i = 1,. . . ,n, ThenWF(PI,R)= Therefore distributions incorporating the same relevant conditional knowledge have the same sets of weight factors occurring in the corresponding c-adaptations. Let us henceforth assume that there is a function F, : AP 3 (p,R) t+ P,* E C(P,R) (14) that assigns specific properties of the weight factors involved. to each pair (P, ‘72) E dP a particular c-adaptation. We will describe F, by Assume (eR) Proposition20. let PJ =P[cY~,cY~ factors and set Pj I= P[(Y+,(Y~]~EI, RJ I= {Ai Y-) Bj[Xj]}jc_/. and ,..., ai, a;] E C (P, R) be a c-adaptation of P to R with weight E dP, R= {Al +J Br[xr],...,A, * B,[x,]} (cl:, ~1, . . . , a,+, a;) E wf(P,*). Suppose I U J is a partition of {l,...,n}, Then ((yf,ai)jcJEWF(P,,RJ) andP,[orjf,~~]i~J=P[a~,al,...,n,f,cu,]. So, onc:e weight factors P,* E C (1: R), stated in the text of Proposition they should yield “best” solutions (Y:, cu;, . . . , ai, cu; are chosen to yield a “best” solution as (with all notations 20). We name this property continuity (of solutions) : in C ( PI, RJ) Definition, 21 (Continuity condition). Let F, be as described continuity condition if the following holds: in ( 14). Fc satisfies the (PR) cE,‘,cX, E wf(F,(P,R)). E dP with R = {Al u-) Br[,rr],...,An Assume and set RJ I= {Aj US Bj[xjl}jeJ. Then (ai+, ay)jeJ E wf(Fc(&, R_r) ), u-) B,[x,]}. be a partition of {l,...,n}, Let ZUJ Suppose - ),..) +Y, PI := P[a’,ai];c,, i.e. F,(PI$RJ) = P~[olj+,~~]je~=Fc(P,R). 188 G. Kern-Isberner/Art~cial Intelligence 98 (1998) 169-208 Finally, FC should obey the principle of atomicity (cf. Theorem 17): Definition 22 (Atomic@ condition). Let F, be as described the atomic@ condition if for any (P, R) , (P', Rd) E dP as in Theorem 17, wf( F,( P, R) ) = wf(F,(P’,R”)). in ( 14). F, satisfies The following proposition derives necessary conditions for a function FC to fulfill the conditions of relevance, continuity and atomicity in special but important cases: Proposition 23. Assume FC as described in ( 14). (9 (ii) x E (0,l). Suppose Pt , P2 are positive distributions over two variables A, B with p1 (bla) = If F, satisfies the relevance pz( bla) , and let ‘R = (a + b[x]}, respectively /3+, /3- of F,( Pt , ‘R) condition, then the weight factors cr+, a- respectively F,( P2, R) are equal in pairs, i.e. (Y+ = /3+ and LY- = p-. Suppose FC satisfies the conditions of relevance, continuity and atomic&y, and let (P 72) E A? with positive prior P, and such that no variable occurs both in antecedent and conclusion of a conditional in R and all assigned probabilities in R are different from 0 and 1. Then the weight factors a+, a- associated in F, (P, 73) with a conditional in R only depend upon the probability x of this conditional and upon their quotient CY’/CY- , i.e. for any (P, R) , (P’, 72’) E dP, P, P’ positive, R = {A + B[x],Ar + B’, [xi I,. . .}, both sets finite, all of x,x;, xi E (0, l), no variable occurring both in antecedent and conclusion of any conditional in R and R’, and for any weight factors a’, a- respectively cx’+ , a’- associated in FC (P, R) respectively F, (P’, R’) with the conditional A + B [ x] respectively A’ -+ B’[ x] , a+/(~- = ff’f /a ‘- implies CY+ = & and CY- = a’-. . .}, 72’ = {A’ 4~-) B’[x],A; I^) B, [x,1,. Example 16 shows that, in the cases dealt with by Proposition 23(i), all pairs CRY+, cy- of weight factors have to fulfill (Y+ __=--_ cl- x PM) 1 - xp(ab) The cross ratio on the right-hand probabilities, tient of CX+ and LY-. This gives an intuitive as it is stated in Proposition represents 23 (ii). in the context of c-adaptations, we identified Thus side, depending only on prior and new conditional exactly relevant knowledge. The left-hand the quo- reason for this quotient playing a key role, side is just factors should be dependent on to give rise to a reasonable x incorporate and (the probability) all relevant knowledge concept a reasonable functional for c-adaptations may be realized by setting clearly the parameters weight functional for the weight factors. Thus concept: a+/cu- a!+ = F+(X,LU), a- = F--(&C?), (15) with two real positive F+(x, a)/F-(x, a) = a, i.e. functions F+ and F-, defined on (0,l) x R+ and related by G. Kern-Isbemer/Artijicial Intelligence 98 (1998) X9-208 F+(rc,(Y) = CkF(x,a). 189 (16) function F : dP 3 (P, 72) H P* is to work for arbitrary P and R, the As our global functions F+ and F- are assumed actually present, “smooth” concept, designed incorporating inferences we assume thus representing classical to be independent a fundamental of the prior and new information inference pattern. Moreover, them to be continuous on (0,l) so far, should also be applied to the extreme probabilities logic as a limit case by assuming to yield x IR?. The functional x E (0, l}, F+(O,O) := lii F+(x,a) = 0, F+(l,oo) := linin F+(x,cu) E R+, (17) a-II U--rcc and F-(0,0) := lii F-(x,@) E B+, F-(1,oo) 0-o := !i_nt F-(~,a) Ll-C-2 =O, (18) in accordance with (10). The resulting posterior distribution P; has the form with nonnegative tions extended real numbers ~1, . . . , a, E Rf U (0, 00) solving the n equa- ffi = Xi Z 031, 0, Xi = 0, 00, xi= 1. (20) Note that CQ E B+ for xi E (0, l), because of the positivity of both functions Ff and F- and due to the P-consistency is satisfied, and (20) corresponds extended (~1,. . . , an E R+ U (0, oo}, CY~, . . . , LY, is a solution of (20) ( 11) here. Thus, for any n nonnegative of R. So the positivity condition to the adjustment real numbers condition (10) iff F+(xl,a~),F-(n~,a~),... (10). ,F+(x,,,n,),F-(~,,a,) is a solution of (11) satisfying We sum:marize these remarks for the axiomatization of the second postulate (P2) : I-+ P,* E C(P, R) Postulate (P2): functional concept for c-adaptations. There (P, R) particular are two real positive fulfilling P *F R has the form (19) with CYI, to each adjustment and continuous the conditions that assigns c-adaptation P,? fulfilling 7% = {Al ~9 B1 [XI I,. . . , A, -+ B, [x,]}, functions F+ and F- defined on (0,l) x XX+, (17) and (18) and related by (16), such that Pi = F*(P, R) =: problem is a function F* : dP 3 (P, R) E dP a and there . . . , a, E JR+ U (0, CXJ} solving (20). 190 G. Kern-isberner/ArtiJicial Intelligence 98 (1998) 169-208 Define for (fixed) F*, F+, F- as in (P2) and for (P, R) E dP, R = {At -+ B, [x, 1, . . ..A. ~B,[x,l}, WQF(ER) :={(LYI,...,LY~) l IW~lJ(0,co) ] ((~,....,a~) solves (20)) (21) to be set of all weight quotients that belong to c-adaptations so (al,... ,a,) E WQF(~R) iff For any (eR) E dP, F*(P,R) = P[crt,... element unknown fulfill , a,) E WQF (P, R) . This disagreeable (at,... solution of (20) may be overcome by assuming the condition of uniqueness: , (Y,],v is described by a particular dependence on a special yet that the functions F+ and F- Definition 24 (Uniqueness condition). Let F+ and F- be functions (P2). Ff (al,... the uniqueness condition iff whenever E WQF (P, R) it holds that and F- satisfy ,%I), (Pl,... , &) as described in (& R) E dP and P[a,,... ,%lF=P[PI,...,PnlF. So, if Ft and F- = P[al,. i.e. F*(P,R) satisfy . . ,a,]~ the uniqueness condition for any (a~, . . . ,a,) E WQF(P,R). then F* is determined by (20), The functional concept (P2), describing weight ated by the conditions ensures recovering of relevance, continuity these properties from (P2) : factors of c-adaptations, was initi- condition and atomicity. The uniqueness Proposition 25. Let F* : dP 3 (P, R) H P$ E C (P, R) be as in (P2) with associated functions Ft and F- satisfying the uniqueness condition. Then F* fulfills the conditions of relevance, continuity and atomicity. We will see in Section 8 that the condition of uniqueness will in fact be satisfied for together with (P3) that will be determined by (P2) functions Ft and F- the special and (P4) (cf. Proposition 30). Note that the conciseness So the efforts we invested providing now an elegant of (P2) is essentially due to making use of c-adaptations. in developing this conditional-logical concept begin to pay, After having put the functional functional concept. dependencies in concrete to study which properties probabilistic P *,Q R to the functional F* (P, R), where *F is described by (52). inferences. To simplify notation, we will usually prefer the functions Ft and F- should have to guarantee terms we are now going sound the operational G. Kern-lsberner/Artt$cial Intelligence 98 (1998) 169-208 191 6. Logical consistency Surely, the adaptation scheme (19) will be considered sound only if the resulting posterior distribution a very fundamental meaning of logical consistency. can be used as a prior distribution for further adaptations. This is In particular, distribution should obtain if we first adjust P only to a subset Rt C R, and then use this posterior information R, we to the full conditional another adaptation to perform the same distribution as if we adjusted P to R in only one step. We state this demand for logical consistency as Postulate (P3) : Postulate s(P3): logical consistency. For any distribution P and any P-consistent RI, R2 c P, of adjusting P first to Rt and then adjusting identical sets the (final) posterior distribution which arises from a two-step process to Rt U 77,~ is from directly adapting P to Rt U 722. to the distribution this intermediate posterior resulting More formally, the operator *F satisfies (P3) iff the following equation holds: P*FI:R,UR2)=(P*FR,)*F(R,UR2). (22) Theorem 26. consistency then If the adjustment operator *F satisfies the postulate (P3) of logical F-(0,0) = F+(l,oo) = 1, and F- necessarily filjills the functional equation F-(x,@) = F-(n,a)F-(x,/7) forallxE(O,l),cr,/IER+. (23) (24) Because of ( 16), F+ satisfies Theorem. 26 is proved by checking condition (24) iff F- does. (23) and (24) in the very special case three variables A, B, C, and Rr, R2 are given These conditions the Appendix). -+ c[ y] } (see a logical consistent the general validity of (23) and (24). this special example which such a crucial meaning the functions Ff, F- to be independent behavior of the adaptation process for of the In fact, there is little is being should interact logic and refers to the antecedent conjunction to. The way in which two conditionals with common conclusion over that P is a positive distribution ={a+c[x]}andRp={b byRt are necessary to guarantee this example, and because we assumed actual case we thus proved arbitrariness assigned is one of the main problem treatment of this problem. equation in choosing in conditional issues (24) The functional too,) essentially: (c:f. [ 25,351). The validity of (23) and (24) ensures a sound probabilistic restricts the type of the function F- (and that of F+, Proposition 27. Let *F be an adjustment operator following (P2) such that F+ and F- satisfy (23) and (24). Then there is a (continuous) real function c(x) , n E (0,l) , 192 with G. Kern-lsberner/Artificial Intelligence 98 (1998) 169-208 lilioc(x) = 0, J% c(x) = -1 such that F- (x, a) = a’(‘), F+ (,y, a) = (Y’(‘)+~ for any positive real LY and for any x E (0,l). Especially for a = 1, this implies F-(x, 1) = F+(n, 1) = 1. (25) (26) (27) In Theorem 26 we showed how the consistency property (22) determines the part the quotients C-Q have to play in the adjustment process. We are now left with the inves- tigation of the isolated impact of the numbers Xi which represent posterior conditional probabilities. 7. Representation invariance By and large, we neglected how (conditional) knowledge is represented in R. Indeed, the principle of atomicity deals with logical equivalence of propositional formulas, but what about probabilistic equivalences, i.e. equivalences that are due to elementary proba- bility calculus? For instance, the sets of rules {A -+ B[ xl, A[y] } and {AB [ xy] , A[ y] } are equivalent in this respect because each rule in one set is derivable from rules in the other. We surely expect the result of our adjustment process to be independent of the syntactic representation of probabilistic knowledge in R: (P4): representation If two P-consistent sets of probabilistic Postulate conditionals R and R’ are probabilistically equivalent then the posterior distributions P * R and P * R’ resulting from adapting the prior P to R respectively to R’ are identical. invariance. The notion of probabilistic equivalence used here completely corresponds to that introduced in [ 261. Using the operational notation, we are able to express (P4) more formally: The adjustment operator *F satisfies (P4) iff P*FR=P*FR’ (28) for any two P-consistent and probabilistically equivalent sets R, R’ c C*. The demand for independence of syntactic representation of probabilistic knowledge (P4) gives rise to two functional equations for c(x) (cf. Proposition 27): Proposition 28. Let the functions F- and F+ describing the adaptation operator *F in (P2) be given by (26) with a continuous function c(x) fulfilling (25). Zf *F satisfies postulate (P4) the following equations hold: then for all real x,x1, x2 E (0,l) respectively (28) G. Kern-Isbemer/Art$cial Intelligence 98 (1998) 169-208 c(x) -tc(l -x) = -1, C(XX, + (1 - X)X2) = _C(X)C(Xl) - c( 1 - X)C(_Q). 193 (29) (30) The most obvious probabilistic equivalence (29). Eq. (30) is that of each two rules A us B[x] is again proved by investigating and a special chosen propositional A u-) B[ 1 .- X] . This implies but crucial adaptation problem. The relation p (bJa) = p (blac)p for arbitrarily conditionals, b[nl],aE xx1 + ( 1 - x)x2 for real X, XI, x2 E (0,l). (see Appendix). implies + b[xl],uE The validity of (28) and R’ = {u us b[y],uc yielding -+ b[x~]} the probabilistic variables A, B, C is fundamental equivalence of the two sets R = {u u-) c[ n] , UC u) ++ ~[xz]} with y = to probabilistic in this case necessarily (cla) + p (b@)p ( C~U) (30) As a consequence of (29) and (30)) we finally obtain: Theorem 29. If the operator *,G in (P2) is to meet the demands for logical consistency (P3) and .for representation invariance (P4), then F+ and F- necessarily have the forms F+(x,a) = a’-’ and F-(x, a) = a-‘, (31) respectively. The continuity of the functions Ff and F-, in particular the continuous of the extreme probabilities is essential to establish this theorem (see the Appendix). 0 and 1, i.e. the seamless encompassing of classical integrating logic, 8. Uniqueness and the main theorem So far we have proved that the demands for logical consistency and for representation the functions which we assumed determine invariance to R, as described by the functional we recognize ME-distribution that the posterior distribution necessarily if it is to yield sound and consistent inferences. concept (P2). Applying to underly of P the adjusting to ( 19) and (20) is of the same type (3) as the (31) Therefore we have nearly reached our goal. But one step is still missing: to characterize ME-inference within a conditional-logical several different moreover, guarantee invariance? ;if we assume that the resulting operator *F satisfies solutions of type (3), only one of which the functions Ff and F- Is this enough framework? Are there possibly And is this suj’kient to and representation is the ME-distribution? to fulfill (31), logical consistency of the posterior distribution The question of uniqueness problems. distribution inference, to fulfill reasonable properties, cf. [ 26,33,34]. Moreover, the conditions is at the center of all these If it can be answered positively, we will be finished: The unique posterior to ME- *F then corresponds (22) and (28) as well as many other together with (P2) uniqueness implies (cf. Proposition 25). of relevance, atomicity and continuity (3) must be the ME-distribution, and ME-inference is known of type 194 G. Kern-Isberner/Arrificial Intelligence 98 (1998) 169-208 (ai, of the solution The uniqueness . . . , CY,) of the fixpoint equation the case that the set R representing is question- able. Imagine contains twice the same rule in different notations. All that can be expected at best is a unique- ness statement if we exclude such pathological But remember for the product aiaj of the corresponding is not easy to deal with at all. (4) knowledge in the uniqueness new conditional factors. Even interested (4) cases, that we are primarily not in that of the solutions to (4). And indeed, this uniqueness distribution, by the next theorem. fitting measure of distance In its proof, we will make use of cross-entropy for distributions of type (3) (see the Appendix). of the posterior is affirmed as an excellently Proposition 30. There is at most one solution of the adaptation problem (*) of type (3)) i.e. the functional concept defined by (3 1) satisfies the uniqueness condition. The following a conditional-logical framework: theorem summarizes our results in characterizing ME-adjustment within Theorem 31 (Main Theorem). Let *e denote the ME-adjustment operator, i.e. + as- . . . , signs to a prior distribution P and some P-consistent set R = {At us Bi [xl], A,, -+ B, [x,] } of probabilistic conditionals the one distribution Pe = P ee ‘R which has minimal cross-entropy with respect to P among all distributions that satisfy R. Then + yields the only adaptation of P to R that obeys the principle of condi- tional preservation (Pl ) , realizes a functional concept (I%!) and satisfies the postulates for logical consistency (P3) and for representation invariance (P4). *e is completely described by (3) and (4). In the next section, we are going but which can be proved easily within to prove some properties of + which are already the framework pre- known about ME-inference sented here. 9. Some properties of the adjustment operator *e The uniqueness which meets fundamental of the solution of type demands for “good solutions”: (3) yields an easy but important corollary Corollary 32. P *e R = P if and only if P k R. This corollary makes obvious an idea of minimality to all of the postulates (Pl)-(P4). two properties known inherent explicit by proving Within characterize that framework, the notions of idempotence and cumulativity are fundamental reasonable nonmonotonic inference operations (cf. [ 20,241) : We will now make that of change is implicitly this minimality more the area of nonmonotonic reasoning. to from Proposition 33. The adjustment operator *e has the following properties: (i) Zdempotence: (P + R) *= R = P + R. (ii) Cumulativity: Zf Rt G 722 and P *e 721 k 722 then P *e 721 = P *e 72~. G. Kern-lsberner/Artijkial Intelligence 98 (1998) 169-208 195 As can be seen at once, cumulativity (ii) is equivalent to (ii)‘IfP*,Rt ~R*thenP*,(R1UR2)=P*eR1, which is stated as Principle 5 in [ 261. 10. Conchsion and future work Starting of the ME;-solution from a conditional-logical to the problem point of view we found another characterization Given a distribution P and a P-consistent set of probabilistic conditionals R, which way is the best for adjusting P to R? thorough embedding of the problem within the conditional-logical here conveys a clear understanding what actually makes to be the best choice-ME-inference and probabilistic conditionals framework the ME-distribu- fit perfectly The presented tion well. classical to determine The principle of conditional Then we assumed logic should underly preservation was used the structure of concept the posterior distribution. the extending crucial parameters which this concept should depend on. It was represented by means of two functions FC and F- accomplishing events satisfying that do not. So they constitute prior distribution between the antecedent of a rule that also satisfy its conclusion the decisive components to under adjustment. these elementary and those events the for the extent of distortion process, and we isolated that a (continuous) the discrimination is to be exposed the adjustment functional had to be made, and the ME-solution to any reasonable and demands independence two further preconditions were necessary logical consistency considered to be fundamental Only ization: usually exceptional way. Moreover, tion, illustrates in an understandable manner, without demand. Actually, postulates imposing idea of minimality the proper the proof of proposition 30, which states how perfectly well the approach presented here realizes ME-inference any external and abstract minimality of syntactical to arrive at the desired character- Both are representation. inference procedure. So no in a rather natural of the solu- the uniqueness arose is being made explicit by the four In Secuon 9, we showed the best within that framework reasoning. So the nonmonotonic a good candidate Introducing a functional [9] concerning Gardenfors paper may indeed be considered role of the theory that the adjustment chosen satisfies inference operation Cp (R) = Th( P *e R) seems to be two essential axioms of nonmonotonic operator + which we proved to be to provide sound quantitative inferences. concept parallels obviously the ideas of (and theory revision. The problem we were dealing with in this as a theory revision problem, with Th( P) playing the intentionally) to be revised by the new information R. These connections to nonmonotonic reasoning and theory change are topics of our ongoing research. 196 G. Kern-Isbemer/ArtiJicial Intelligence 98 (1998) 169-208 Acknowledgements I thank Christoph Beierle and Wilhelm RGdder for many hours of discussions, referees two unknown the presentation correctness of this paper. Thanks also to Jeff Paris for sending me preliminary of his papers. for their helpful comments concerning and and the versions The main part of this work was finished while I was supported by a Lise-Meitner- scholarship, Department of Science and Research, North-Rhine-Westfalia, Germany. Appendix A. Proofs Proof of Theorem 11. Let P be a distribution B, [ x,] } be P-consistent. ability: 0* = {w E 0 1 p(w) > 0). Let O* denote all elementary and R = {At v-) Bt [xl], . . . ,A, u-) events with positive prior prob- Suppose first that P* is a c-adaptation of P to R. The equivalence a partition Kt , . . . , K4 of .f2* in disjoint classes. According is constant on each equivalence relation ER induces o) class, SO assume p*(w)/p( w) = Kj for w E Kj. Let to Lemma 10, p*(w)/p( WI,..., wq E L2* be a representative system of Kt , . . . , &. For the sake of simplicity of notation, we suppose that ~1, ,..., X,rE(O,l),&~+t . . . , ~~~ > 0, K~!+~ x,E{O,l}, ,..., = . . . =Ky=Owithq’<q,andfurthermore,xt n’ < n. If Kj = 0, q’ < j < q, we have p*( wj) = 0 and P(Oj) > 0. From Definition 8(i) there must be a conditional Ai -+ Bi in R, n’ < i < IZ, such that either xi = 1 and AiBi( Wj) = 1, or X; = 0 and AiBi( oj) = 1. In the first case, set CZ+ = 1 and (Y; = 0, in the values of these two factors. In this way, any Kj = 0 (and the second case exchange thus any certain conditional 0 or 1) is dealt with. in R, i.e. any conditional with probability Let us now consider the constants Kj Z 0. Finding positive factors (~0, a:, a,, . . . , a,‘, , CY;, with 0 # p*(w) =a@n(w) CX” n n I<#<!#’ AiBi,“,‘l AiEl;(N,=l l<i<# a; amounts to solving the following system of qf equations, ff0 rI a+ II a,: = Uj, I <,<,t’ A,B,(“, ,=I I<i<#,’ - *jaicq,=, j= l,...,q’, which can be transformed into a linear equational system @=A (A.1) (A.21 with p = (logcut,loga, ,..., logcu,f,,lOg~~,logao)T E WZn’+t, A = (1OgKt ,... , log Kyt )T E matrix 0 with elements IV’ (where R denotes the field of real numbers) in (0, l}, 0j,2i = 1 iff ai = ~i, Bj,zi+t = 1 iff ai and a q’ x (2n’ + l)- = bi, G. Kern-lsbemer/Artijicial Intelligence 98 (1998) 169-208 197 @j,2n’fl = 11 for all 1 6 j < q’, 1 < i 6 n’. Let @j, 1 < j < q’, denote 0. The equational field of rationals, because each entry of 0 is either 0 or 1) between correspond Ck rmt A,, = Cl s,,, A,,, with rationals the row vectors of (over the these row vectors rmkt& = XI sn,&, must imply is solvable over R iff any linear dependencies the Aj = log Kj, i.e. ck to relations between r,, , s”,. system (A.2) Arranging and multiplying Cl s,, fl,,, with natural numbers $I both sums appropriately, we may assume xk rmt&t = the vector components, we obtain r,, , s,, . By comparing r,,e,i,zi = Cl S,,ennr,2i+tr 1 G i G n’, and Cer~,, = I sn,, the last equation bemg vahd because of 0j,zns+i = 1 for all 1 6 j 6 q . The = Cl snrenl,2ir &, rmt$t.2i+t first two equations CI: V;i(o,,,)=:b; s,/* Therefore by Lemma 7, and because P* is assumed the multi-sets {wmk : rmk}k and {c+, : sn,}r are R-equivalent to be a c-adaptation we obtain imply Ck: C,(w,t lzai rmt = Cl: Ci(on, jzui snr and Ck: oi(o.,)=bi rmr = Applying the logarithm function now yields as desired. Thus the equational (P;&... exp( pi), ,P,+,,P;,Po)T 1 < i < n’, we obtain system J? = is solvable, yielding E R 2n’+1. Setting CXJ = exp( pa) and cy+ = exp( @), a; = a solution (A.2) P*(w) =aop(w) n 4 n q- IQ<,, A;B;~‘~‘l I<i<“’ AiBf(Ukl for p* (0) # 0. Taking into account the certain conditionals, we thus have for w E a’* because o E 0 - 0*, particular, p*(BilAi) shows (11). the non-zero factors belonging are 1. For this equation holds trivially because p(w) = p* (w) = 0 in this case. In (10). Moreover, P* b 72 means This these factors cyr, CY~, . . . , an+, 0; = Xi, 1 < i < n which is equivalent satisfy to (1 - xi)p*(AiBi) to certain conditionals = xip*(AiBi). To prove the converse assume is a distribution with aa > 0, n:, CUT,. . . , a;, a; to show P"; E C(P,R). satisfying ( 10) and ( 11). We have 198 G. Kern-lsberner/Art~cial Intelligence 98 (1998) 169-208 that p(w) = 0 or there is a conditional Ai -+ Bi[xi] implies X; = 0 by (lo), Conversely, if AiBi( 0) = 1 for a conditional Ai -+ Bi [ xi] by ( IO), a+ = 0 and p* (w) = 0. The second case A&(o) dealt with, and p(w) = 0 implies p*(o) in R such that ff: = 0, thus = 1, or (Y: = 0, which implies Xi = 1, and AiBi( w) = 1. in R with Xi = 0, then, again = 1 and xi = 1 is similarly = 0 trivially. and AiBi(ti) Now consider {Z’r : Sl, . . . , Y,,, structures q = flr++r, two R-equivalent multi-sets 0, = {wr : t-1,. . . , w,,, : r,,} 1 < k < ml, 1 < 1 6 m2, with identical conditional and 0, = : &Q}, mk, v/ E a*, @(@k)ra = &GiG,,, d@)S’ = q. Then ,-k = c l<k<nzl c k: (BilAi)(w~)=l c I<l<mz t-k = Sl, c 1: (BilAi)(vr)=l c k: (B;lAi)(wk)=O rk = c 1: (BilAi)(v,)=O SIT Sl holdforalli=l,..., calculation: n according to Lemma 7. Checking condition (8) is now an easy p*(wl)” . . .pf(Wm,)r,l P(Wl)” . . .p(Wm,)r,@l l<i<n I&i+! = p*(vl)Q . . .p*(Vm2)S.)Z p(vI)s’ ..‘p(Ynlz)S”JZ . At last we have to show that P* k R. (10) implies p*(Ai) > 0 for all i = 1,. . . , n. is an i such that p*(Ai) = 0. Then for all w with For assume = 1, Ai the contrary, i.e. there P*(w) = sop(w) jyI a+ ]II a; =o, I<,<,, A,0i~N14 I<i<H AjBi,Obl thus for each such W, we have p(w) = 0, or there is a j, such that (~2 = 0 and = 1, or “j_ = 0 and Aj,Ejo(o) Aj,Bjo(0) Suppose Q is a distribution with Q k R. Then for each w E a* with Ai Xi” = 0 and Aj,Bj,(o) Q b Ai u-) Bi[xi] = 1. If ti E a*, SO either XjM = 0 or xjW = 1. = 1, either = 0. But in particular q( Ai) > 0, so there must be an 00 E 0 - 0* = 1, or XjU = 1 and Ajax(w) = 1; in any case, q(o) implies G. Kern-Isberner/Art@cial Intelligence 98 (1998) 169-208 199 with Ai( tijo) = 1 and q( 00) > 0, i.e. we have p (00) = 0 and q( wo) > 0. So Q cannot be P-continuous. of 72. Therefore p* (Ai) > 0 for the P-consistency all i= This contradicts l,...,n. For Ai * Bi [xi] E R with xi = 0, ( 10) ensures because cr+ occurs this implies p*( BilAi) = 0. Analogously, P* k Ai -+ Bi[xi] (10). For X; E (0, l), P* b Ai + Bi[xi] because of (11). that LY+ = 0, therefore p* ( AiBi) = 0, in each product p* ( w’) with AiBi( 0’) = 1. Because p* (Ai) > 0 for Xi = 1 by virtue of 0 Proof of Proposition 19. Let R = {Al IC) B1 [XI I,. . . , A,, * B, [x,]}, PI, P2 are two distributions with p1 (@[Ai) = p2(01Ai) for all w E 0 and and suppose for all 1 ,...,n, i= According (LY:, a;, to (13), xi = 0, LYE- = 0 iff Xi = 1 and PI(W) = 0 iff p*(w) = 0 and such that R . . . , a:, a;) E WF(Pl,R) is PI- and Pz-consistent. iff cr’,cw; > 0, a+ = 0 iff for all i= l,...,n. For any such i, and because of pI (@[Ai) = pz(wlAi) for all w E 0, we have for all w with Ai( Consequently, = 1: Pl(m)/pl(Ai) the equations =P2(~)/P2(4), SO PI(&) = (PI(Ai)/P2(Ai))P2(W)* above may be rewritten as (1 -Xi)O!+ c- o:AiBi(w)=l P1 (Ai) p2(Ai)p2(0) AjgjI, Oil’ $! ff’ AjBj(",4 (1 - Xi)CYyi+ o: AiB;(w)=l j+i AjBjlO)4 p *jBj,O)=l = xja, w: A&(o)=1 j+i Ajoj(hl,=l j+i - A,Bjb+l because p1 (Ai)/pz(Ai) to (+Yy; in the sums above satisfy Ai( 0) = 1.) )...) 0 > 0. Together with the positivity condition, a,+, LV; ) E WF( Pz, 72). (Note that all elementary this is equivalent events w occurring Proof of Proposition 20. Assume proposition. that all notations are as stated in the text of the 200 G. Kern-Isbemer/Arti~cinl Intelligence 98 (1998) 169-208 (cy~,cy;‘.“,& a,) E wf(P,*), SO in particular, (a;, aj)jc~ satisfy the positivity condition for RJ, and for each j E J, we have - = Xj(Yj c P(W) n (Yk+ n a; Proof of Proposition 23. Proof of (i). Let PI, P2 be positive distributions variables A,B with pl(bla) = pz(bla), PC := F,( Pk, R) E C( P, R) be c-adaptations with weight factors cx+, (Y- respectively forms, and flf, p-, respectively: let R = (~2 vg b[x]},x (12), P;, P; have k = 1,2. According over two Let the following E (0,l). to (11) and with CY+ x Pd& -=l-xplo= a!- P2W x ----=- 1 - xPz(ab) P+ P- that x # 0,l). Due to the positivity of the prior distributions, /3+,/S- are uniquely determined by F,, i.e. card(wf(F,(Pk,R))) the weight factors = 1, (note Ly+,CY- and k= 1,2. G. Kern-Isbemer/Artifcial Intelligence 98 (1998) 169-208 201 Calculating all cross-ratios p;(hb) ------:: P1 (& p;(hb) J&b) ’ we obtain relevance condition, and ay- = /3-. the three values ~yecy+/fi&+, ~a(~-//?aP-, then c~yg~y~/fleP+ = LyaLy-/@a/3- = ac/&. act/&. So if F, satisfies This implies the (Y+ = p+ Proof of (ii). Suppose P, P’ are positive prior distributions. Let R = {A us B[x], in antecedent ,... }, R’ = {A’ u-) B’[x],A’, sets of probabilistic Ar w Bt[xr] spectively P’- continuous variable occurs both Let a+, a- F,( P’, R’) with same probability the conditions of relevance, We have to show: cy+ = a’+ and (Y- = (y’-. the conditional A y-) B[x] x is assigned respectively continuity and conclusion a’+, a’- be weight factors associated of any conditional u-) B{[xi] conditionals, ,... } be two (finite) P- re- all of x, Xi, xi E (0, 1) , no in R and ‘R’. respectively in Fc (P, R) respectively A’ y-) B'[x] that the to both A * B in ‘R and A’ * B’ in R’. Let Fc satisfy . Note and atomicity, and assume a+/cy- = &+/a’-. For F, (P, R), we have the CUT, a; are associated with the remaining where i E I. Set PI = P [ a:, ai ] iEf = PI, in the notation of Definition 21. F, to satisfy Similarly, have condition, E wf(F,(P,‘,{A’ the continuity (cy’+,&) * B’[x]})) E wf(F,(Pt, therefore (cyi, a-) conditionals Ai * Bi[x’] with Pi = Pfi. In particular, we in R, is supposed {A * B{x]})). a+ Ly- - ~1 (AB) x = l-xPr(~~) and c a’- _ x pl(A’B’) 1 - xp;(A’B’)’ (AB) ‘x+/a- = cy’+/& = pi (A’B’)/pi implies pr (AB)/pr Thus pI (BIA) = pi (B’IA’). By virtue of the atomicity and B, B’ by new propositional able occurs both in antecedent thus obtain positive distributions ) with pi (615) = p{ (6jZr), and a+, LY- respectively of F,(P,,{B CY+ = cy’+ and (Y- = a’- , as desired. hence condition, we may replace A, A’ that no vari- Suitably marginalizing PI and P,‘, we over A, B (which we will denote again by PI andP,’ the weight factors cy’+, LY’- being that it follows (i), variables A and fi (note and conclusion). ug g[_x]}> respectively F,(P,‘,{ ZI u-t 6[x]}). that we assumed (A/B’), From Cl Proof of Proposition 25. Let R = {Al u-) Bt[xr],...,A’, PI, P2 are two distributions with pr (@[Ai) = Ps(wlAi) i= I,... P; =F*(Pk,R) and suppose for all o E fi and for all , n, PI (o) = 0 iff p2( w) = 0 and such that R is PI- and P2-consistent. Let u) Bn[xn]}, fork= 1,2, 202 G. Kern-lsbemer/Artt&zial Intelligence 98 (1998) 169-208 p;(w) =p2[P1,... *PnlF(w) =POP2Cw) F+(XivPi) n L$i<n AiBi,W’)4 F-(-CvPi)7 n gi<,, A;Bi(w)‘I ,a,> as in the proof cm,... . . , a,) E (al,. of Proposition WQp (P2, R). Due to uniqueness, we thus have P; = P2 [ a~, . . . , a,] F, i.e. E WQF(PI,W, (PI, . . . , /3,,) E WQF( P2, R) . Arguing In particular, 19, we see WQF( PI, R) = WQF( P2, R). P;(W) =a&(@) F+(xivai) n I<i<,# *;EQ(m)=l F-(xi*~i)- n IQ<” A,B;(u)=I Now for any o E R, we have p;(w) = 0 iff p;(o) = 0 or else a constant. This proofs the relevance property. With condition So F” satisfies the notations of Definition of uniqueness, F*(PI,RJ) the continuity condition. (aj)jcJ E WQF( PI, RJ), 21, = Pl[cUj,j E J],v = P[al,...,a,]~ therefore by the = F*(P,R). At last, Theorem 17 implies now follows from uniqueness, the technical details. Cl immediately WQF (P, R) = WQ,V (P’, 72“). Atomicity formulas. We omit of classical-logical using equivalences 26. If (22) holds in principle Proof of Theorem it is surely valid for some special distribution Letpt,... following and P *F (Rt U R2): for any adaptation carried out by *F, type of P, 721 and R2. So let P be any positive over 3 variables A, B and C, let RI = {u u-) c [xl} and R2 = {b u) c [y]}. ,pg denote . . . ,pg = p(ii6E). The table shows the three adapted distributions P *F RI, (P *F RI) *F (‘721 U 732) the prior probabilities of P, p1 = p(abc), 0 abc abE a&c a& iibc iibE &C ii6E p *F (RI uR2) P*F% (P*F%) *F(Rl UR2) aoPl~+Cv)~+(y~P) P;PIF+(wo cub~~,plF+(x,cu’)F+(x,(YI)F+(Y,PI) aopzF-(x,a)F-(Y,P) P;p2~-(d) cu~P~pzF-(x,cu’)F-(x,cul)F-(Y,Pl) ~OP3Ff(L a) P;P~F+ (xv a’) &3;p3F+(x, dF+(x, 01) ~oP~F- (~9 a) P;p4F- (~3 a’> (~~P~p4F-(x,cy’)F-(x,al) ~oP~F+(Y, PI ffoPsF_ (Yv PI ffoP7 ffOp8 PhP5 PhP6 PhP7 PAPS ~,P;P~F+(YJ~~ d&,~sF- (Y, PI > 4&P7 ff$%Pa. Postulating P*F(RIUI&) = (P*FRI)*F(R~UR~) Ff (y, p, ) , F- ( y, p) = F- (y, PI), hence /3 = /3r because of ( 16). yields LYO = ~~$36 and F+(y.P) = Further for x = 0, we see cx = cy’ = (~1 = 0 and F- (0,O) = F- (0,O) . F- (0,O). Due to ( 18), F- (0,O) # 0, hence F- (0,O) = 1. Similarly, F+ ( 1, oa) = 1. G. Kern-Isberner/ArtiJicial Intelligence 98 (1998) 169-208 203 For n # 1, the weight quotients a, CJ and (~1 may be calculated as x PDF- (Y, P) + ~4 &= I-n ’ plF+(y,p) +p3’ a’=_.- x 1-x p2 +p4 p*+p3’ x -. F-(xva’) ~2F-(y,P1) +p4 _ a1 = 1 --x F+(n,a’) ’ PIF+(~,PI) +p3 x 1-X .&I. p2F-(y,Pl) PIF+(Y,PI) +p4; +p3 thus a = (~‘a,,. Comparing again P *F (RI U 722) and (P *F RI) *F (RI U R2) we obtain F-(X,(Y) = F-(x,a’al) = F-(x,a’)F-(~,a,). (A.3) the equations For fixed x, P and y can still be chosen arbitrarily. Choosing y = 1 simplifies being only dependent that indeed any (Y’, at E IR+ may be represented appropriately. Therefore x E (0,l). y = 0 respectively these weight quotients show calculations as weight quotients by setting up P real cy’, ~yt, and all on P (and, of course, on x). Straightforward i.e. (24)) must hold for all positive for (Y, (Y’, cyt essentially, making (A.3)) 0 to be held fixed and 27. Let the preconditions Proof of Proposition x E (0,l) only as a function of 0 (even depends on many parameters other than x, at least on the prior distribution; of Theorem 26). if x is held fixed, (Y still may vary because 27 be satisfied. Assume a) be regarded it generally cf. the proof let for a moment F;(a) of Proposition := F-(x, According to [ 1, pp. 46ff] and by taking account of (24), we see FL (a) = d real constant some obtain F- (x, UT) = a’(‘) and, due to ( 16), Ff (x, a) = CYST’, and any x E (0,l). (27) now is obvious. into consideration c, and again This proves taking (26). the dependency for on x, we for any positive real CY According to (23), 1 = F- (0,O) = lim,,o,,,o F- (x, a) = lim,,c, cr+a &*); this implies lim,,a c(x) = 0. Similarly, by observing (26), (23) and (17), we obtain lim,,t c(x) = -1. 0 Proof of Proposition Suppose respectively PT = P *,v R’. According 28. All priors P in this proof are assumed to be positive. n E (0, l), and let cy first R = {A 6 B[x]} p be the factor associated with R respectively R’. Let PC = P *F R, and and R’ = {A -+ B[l -xl}, to the functional concept (P2), F+(x,a), AB(w) = 1, p;(o) = (Y;+(W) F-(X,(Y), A@w) = 1, i 1, A(o) = 0 and F+( 1 -x,/I), AB(w) = 1, p;(w) =@p(w) F-(1 -x,p), AB(w) = 1, 1, A(w) = 0, 204 with G. Kern-Isbemer/ArtiJcial Intelligence 98 (1998) 169-208 x P(AB) -g-l. LU=l~(~~) If (P4) F-(~,a) x,cu-‘). Using (26) proves (29). is satisfied then P;” = P;, -x,(Y-t = F+(l ). Together with (16), this shows F-(x, a) = a-IF-( thus implying F+(x, cu) = F- ( 1 - x, (u-l) and 1 - Now we are going to prove (30). Because of P(blU) = p(blac)p(cla) +P(4~~)P(~l~) for arbitrarily chosen variables A, B and C, the two sets of rules R={u-+C[X],ac + NIxI I, aE --+ b[X21}, R’ = {u --+ b[y],uc -)b[x~],uE--,b[_x2]} withy=xxt+(l-x)x2 are probabilistically equivalent for x, XI, x2 E (0, 1) . Because *,v is assumed to satisfy (2811, we have P *F R = P *F 72’ when applying ( 19). We list both distributions below. The correspondence between each (pi respectively pi and the conditional it belongs to should be clear. P*FR P*FR’ c(x)+1 (YOPI aI c(xt)+l a2 aoP2@, c(x) c(xz)+I ff3 ~opsa, c(x)+1 C(XI 1 ff2 PoP3P, C(Y) c(m) P2 aop4ff1 c(x) C(R) a3 PoP4P* C(Y) c(x2) P3 sops *Op6 nop7 sops POPS POP6 POP7 POP8 with X (yl=l_X C(Q) “3 &I) . p2(y3 + p4 PIa + P3 ’ Xl -.E a2= 1 -x* p,’ *3=-.- X2 1 -X2 P4 P2’ p,=Y. 1 - y p3@) + p4py p,p;(~l )+I + p2p;(x2)+1 ’ p,,x’.JL 1 -x1 PIP1 a2P;‘, p3=x:!.e= a3Pl’. 1 -X2 P2P1 (A.4) (A.5) (A.6) (A-7) C. Kern-Isbemer/Artijcial Intelligence 98 (1998) 169-208 205 These last equations (A.6) and (A.7) yield p3p;(z= l-xl -.- 1-Q Pdp x l-x’ and putting all these equations together we obtain c(xz)-c(xl) aI = 131 For P *F R. = P *F R’ to hold, we necessarily must have c(x) _ _ p-cw ffl ( and (30) now follows from (29), (A.8) and (A.9). q (A.81 (A.9) real function c(x) satisfying Proof of Theorem 29. Properties continuous c(l--x) = -l,c(xxt+(l-x)x2) (0, 1), due: to Propositions we see c( ji) = -3. From (P3) and lim,,c (P4) c(n) = 0, lim,,t imply F-(x, a) = &) with a c(x) = - 1 and c(x) + E forallrealx,xl,xf = -c(x)c(xt)-c(l-n)c(xz) 27 and 28. Choosing n = 3 in the first of these equations, the second equation, x2 4 0 yields c(xx,) = -c(x)c(x1). (A.lO) for x = i: c( ixt + ix2) = c( 1x1) + c( 5x2) for XI, x2 E (0,l). this, we obtain c(x) Using Therefore in [l, p.441, using and c(i) Together with ( 16), this shows (31). fulfills a Cauchy (A.lO) functional equation on (0, i). Similar to the proof = -3, one sees c(x) = -x 0 for all x E (0,I). Proof of Proposition 30. Suppose B, [x,] }, and let P;“, Pg be distributions of type (3), (E R) E dP, R = {At * BI [XI], . . . , A,, u-) p;(W;)=a(jp(W) (Y&f-xi I-I q-x’, I-I I<i<,# A;B,(m,=l IQ<,? A,B;(w)=l p;(w:l=pop(w) ,y n ,<iQ,, AiBi(0)=l J?,:rn n ,<i<,t Ai[li(m)=L with nonnegative real numbers {w E 0 1 p (w) > 0). Without those cases, ai = pi). SO p;(w),p;(w) (ai)c<i<n, (fii)c<;<n loss of generality, we may assume fulfilling equations (4). Let a* = (in that all Xi # 0,l > 0 for all w E II!*. We calculate the cross-entropy between P; and PT: R(P;.P;)=~P;(WO~~= 0 P;(w) 2 ~;(W%p+(o) P?(W) 2 c oEf2’ 206 G. Kern-lsberner/Artifcial Intelligence 98 (1998) 169-208 = I$(@> c UJEf2’ hz~o [ - logP0 + (1-Xi)(lOg~i-lOgPi)+ C ,<t<,, AiBi(w)=l (-xil( C I<i<,# AiLsi(” log ai - 1% pi)] = P;(w) m%~o c i&R’ - l%Pol + w+ PfCw> * C l<i$’ A;B,,&s)=l (-xi)(logai - log/%) = w%ao - logPo1 c P;(w) oEn* + C Cl -xi)(logQi-log&) l<i<n + C (-Xi)(lOg~i - lOgPi) oE.n*:A;Bi(w)=l P;(w) P:(o) l<i<n oEf2’: A&(w)=1 = [log&o - IogPol + c (1 - xi)(logLui - logPi)p;(AiBi) l<i<n + C (-x;)(lOg~i - lOgPi)p;(AiBi) I<i<Il = [loga - 1ogPol 0 ai-lOgPi)[(l -~;)p;(AiBi) -xip;(AiEi)] + C(lg l<i<n = [loga - logPo1 because p;(B;lAi) = x; for all 1 < i < n. In the same way, WP,*,P;“) = 1logPo - logaol can be derived. But now both equations together imply R(PT,P,*) = 0, for cross-entropy is nonnegative, must be identical. This proves and by using its positivity (cf. [ 321) , both distributions the proposition. 0 Proof of Corollary 32. If P ee R = P then P k R by definition of any adjustment operator. G. Kern-Isbemer/Artijicial Intelligence 98 (1998) 169-208 201 Conversely, of x;~(A$i) unchanged, Proposition if P k R then LY; = 1, 1 < i < n, provide a solution = ( 1 - xi)p(AiBi), 1 Q i < n. These factors the trivial posterior P of type (3). The uniqueness to (4) because leave the prior distribution statement of yielding 30 now implies P *e R = P. Cl Proof of Proposition 33. Idempotence is clear with Corollary 32. If RI C. R2, we have P*,R2 = P*,(R1UR2) = (P*,RI)*~R~ prerequisite P +Rl k 722, so again Corollary 32 implies References (P ++77..1) +.R2 = P *err. because of (22). By 0 [ I] J. AcuSI, Vorlesungen iiber Funktionalgleichungen und ihre Anwendungen (Birkhiiuser, Basel, 1961). [2] E.W. Adams, The Logic of Conditionals (Reidel, Dordrecht, 1975). [3] I? Batchus, Representing and Reasoning with Probabilistic Knowledge: A Logical Approach to Probabilities (MIT Press, Cambridge, MA, 1990). [4] G. Brewka, Preferred subtheories: an extended logical framework for default reasoning, in: Proceedings IJCAI-89, Vol. 2 (Morgan Kaufmann, San Mateo, CA, 1989) 1043-1048. [S] PG. Calabrese, Deduction in: I.R. Goodman, M.M. Gupta, H.T. Nguyen and G.S. Rogers, eds., Conditional Logic in Expert Systems (North-Holland, Amsterdam, inference using conditional logic and probability, 1991) 71-100. and [6] 1. Csiszt I-divergence geometry of probability distributions and minimization problems, Ann. Probab. 3 (1975) 146-158. [7] B. De Finetti, Theory of Probability, Vols. 1 and 2 (John Wiley and Sons, New York, 1974). objects [8] D. Dubois and H. Prade, Conditional and non-monotonic reasoning, in: Proceedings 2nd Internc;!tional Conference on Principles of Knowledge Representation and Reasoning (KR-91) Kaufmann, Los Altos, CA, 1991) 175-185. (Morgan [ 9 1 F? Gaerdenfors, Knowledge in Flux: Modeling the Dynamics of Episternic States (MIT Press, Cambridge, MA, 1988). [lo] H. Geffner, Default Reasoning: Causal and Conditional Theories (MIT Press, Cambridge, MA, 1992). [ 111 M. Goldszmidt, Research [ 121 M. Goldszmidt issues in qualitative and abstract probability, AI Mag. (Winter 1994) 63-66. and J. Pearl, Rank-based about evidence and actions, systems: a simple approach to belief revision, belief update in: Proceedings 3rd International Conference on Principles and reasoning of Knowledge Representation and Reasoning (KR-92), Cambridge, MA (1992) 661-672. for multidimensional for hypothesis formulation, especially entropy 1131 I.J. Good, Maximum contingency tables, Ann. Math. Statist. 34 (1963) 91 l-934. [ 141 A.J. Grove, J.Y. Halpem and D. Koller, Random worlds and maximum entropy, J. Art$ Intell. Res. 2 (1994) 33-88. [ 151 P. Hajek, T. Havranek and R. Jirousek, Uncertain Information Processing in Expert Systems (CRC Press, Boca Raton, FL, 1992). [ 161 E.T. Jaynes, Papers on Probability, Statistics and Statistical Physics (Reidel, Dordrecht, [ 171 R.W. Johnson and J.E. Shore, Comments on and correction 1983). to “Axiomatic derivation of the principle of IEEE Trans. Infornz. Theory 29 (6) maximum entropy and the principle of minimum cross-entropy”, (1983) 942-943. [ 181 G. Kern-Isbemer, Conditional logics and entropy, lnformatik Fachbericht 203, FemUniversitgt Hagen (1996). [ 191 G. Kern--Isberner, A logically sound method for uncertain reasoning with quantified conditionals, in: Proceedings 1st International Conference on Qualitative and Quantitative Practical Reasoning, ECSQARU-FAPR’97 1201 S. Kraus, D. Lehmann reasoning, preferential models and cumulative (Springer, Berlin, 1997) 365-379. and M. Magidor, Nonmonotonic logics, Artificial Intelligence 44 ( 1990) 167-207. [ 211 S. Kullback, Information Theory and Statistics (Dover, New York, 1968). 208 G. Kern-lsberner/Art$cial Intelligence 98 (1998) 169-208 [22] S.L. Lauritzen and D.J. Spiegelhalter, Local computations with probabilities in graphical structures and their applications to expert systems, J. Roy. Statist. Sot. Ser. E 50 (2) (1988) 415-448. 1231 R.C. Lyndon and P.E. Schu, Combinatorial Group Theory (Springer, Berlin, 1977). [24] D. Makinson, General patterns in nonmonotonic reasoning, in: D.M. Gabbay, C.H. Hogger and J.A. Intelligence and Logic Programming, Vol. 3 (Oxford Robinson, eds., Handbook of Logic in Art$cial University Press, Oxford, 1994) 35-l 10. [251 D. Nute, Topics in Conditional Logic (Reidel, Dordrecht, 1980). [ 261 J.B. Parts and A. Vencovska, A note on the inevitability of maximum entropy, Internat. J. Approximate Reasoning 14 (1990) 183-223. 1271 J.B. Paris and A. Vencovska, In defence of the maximum entropy inference process. Internat. J. Approximate Reasoning, to appear. [ 281 J. Pearl, Probabilistic Reasoning [29] J. Pearl, Bayesian and belief-functions formalisms for evidential reasoning: A conceptual analysis, in: (Ellis in Intelligent Systems (Morgan Kaufmann, San Mateo, CA, 1988). of the Art and Future Directions Z.W. Ras and M. Zemankova, eds., Intelligent Systems-State Horwood, Chichester, 1990) 73-l 17. [30] W. Redder and G. Kern-Isbemer, Representation and extraction of information by probabilistic logic, Inform. Systems 21 (8) ( 1997) 637-652. [31] W. Redder and C.-H. Meyer, Coherent knowledge processing at maximum entropy by spirit, in: E. Horvitz and F. Jensen, eds., Proceedings Portland, OR (Morgan Kaufmann, San Francisco, CA, 1996) 470-476. 12th Conference on Uncertainty in Artijcial Intelligence, [32] J.E. Shore, Relative entropy, probabilistic inference and AI, in: L.N. Kanal and J.F. Lemmer, eds., Uncertainty in Artificial Intelligence (North-Holland, Amsterdam, 1986) 21 l-215. [33] J.E. Shore and R.W. Johnson, Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy, IEEE Trans. Inform. Theory 26 ( 1980) 26-37. [34] J.E. Shore and R.W. Johnson, Properties of cross-entropy minimization, IEEE Trans. Infirm. Theory 27 (1981) 472-482. [ 3.51 M. Spies, Combination of evidence with conditional objects and its application to cognitive modeling, in: I.R. Goodman, M.M. Gupta, H.T. Nguyen and C.S. Rogers, eds., Conditional Logic in Expert Systems (North-Holland, Amsterdam, 1991) 181-209. 1361 J. Whittaker, Graphical Models in Applied Multivariate Statistics (John Wiley & Sons, New York, 1990). 