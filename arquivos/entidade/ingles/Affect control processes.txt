Artificial Intelligence 230 (2016) 134–172Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAffect control processes: Intelligent affective interaction using a partially observable Markov decision processJesse Hoey a,∗a David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canadab Centre for Theoretical Neuroscience, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canadac Potsdam University of Applied Sciences, Institute for Urban Futures, Kiepenheuerallee 5, 14469 Potsdam, Germany, Tobias Schröder b,c, Areej Alhothali aa r t i c l e i n f oa b s t r a c tArticle history:Received 2 April 2014Received in revised form 12 August 2015Accepted 17 September 2015Available online 3 October 2015Keywords:AffectEmotionSociologyAffect control theoryMarkov decision processIntelligent tutoring systemAssistive technologyHuman–computer interactionThis paper describes a novel method for building affectively intelligent human-interactive agents. The method is based on a key sociological insight that has been developed and extensively verified over the last twenty years, but has yet to make an impact in artificial intelligence. The insight is that resource bounded humans will, by default, act to maintain affective consistency. Humans have culturally shared fundamental affective sentiments about identities, behaviours, and objects, and they act so that the transient affective sentiments created during interactions confirm the fundamental sentiments. Humans seek and create situations that confirm or are consistent with, and avoid and suppress situations that disconfirm or are inconsistent with, their culturally shared affective sentiments. This “affect control principle” has been shown to be a powerful predictor of human behaviour. In this paper, we present a probabilistic and decision-theoretic generalisation of this principle, and we demonstrate how it can be leveraged to build affectively intelligent artificial agents. The new model, called BayesAct, can maintain multiple hypotheses about sentiments simultaneously as a probability distribution, and can make use of an explicit utility function to make value-directed action choices. This allows the model to generate affectively intelligent interactions with people by learning about their identity, predicting their behaviours using the affect control principle, and taking actions that are simultaneously goal-directed and affect-sensitive. We demonstrate this generalisation with a set of simulations. We then show how our model can be used as an emotional “plug-in” for artificially intelligent systems that interact with humans in two different settings: an exam practice assistant (tutor) and an assistive device for persons with a cognitive disability.© 2015 Elsevier B.V. All rights reserved.1. IntroductionDesigners of intelligent systems have increasingly attended to theories of human emotion, in order to build software interfaces that allow users to experience naturalistic flows of communication with the computer. This endeavour requires a comprehensive mathematical representation of the relations between affective states and actions that captures, ideally, the subtle cultural rules underlying human communication and emotional experience. In this paper, we argue that Affect Control Theory (ACT), a mathematically formalized theory of the interplays between cultural representations, interactants’ * Corresponding author.E-mail addresses: jhoey@cs.uwaterloo.ca (J. Hoey), post@tobiasschroeder.de (T. Schröder), aalhothal@cs.uwaterloo.ca (A. Alhothali).http://dx.doi.org/10.1016/j.artint.2015.09.0040004-3702/© 2015 Elsevier B.V. All rights reserved.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172135identities,1 and affective experience [1], is a suitable framework for developing emotionally intelligent agents. To accomplish this, we propose a probabilistic and decision theoretic generalisation of ACT, called BayesAct, which we argue is more flexible than the original statement of the theory for the purpose of modelling human–computer interaction. BayesAct is formulated as a partially observable Markov decision process or POMDP. The key contributions of this new theory are: (1) to represent sentiments as probability distributions over a continuous affective space, thereby allowing these sentiments to be dynamic and uncertain; (2) to propose a new kind of agent based on affect control theory that has the ability to learn affective iden-tities of interactants; (3) to integrate the affective dynamics proposed by affect control theory with standard POMDP-based artificial intelligence; and (4) to introduce explicit utility functions to affect control theory that parsimoniously trade-off affective and propositional goals for a human-interactive agent. These contributions allow BayesAct to be used as an arti-ficially intelligent agent: they provide the computerised agent with a mechanism for predicting how the affective state of an interaction will progress (based on affect control theory) and how this will modify the object of the interaction (e.g. the software application being used). The agent can then select its strategy of action in order to maximize the expected values of the outcomes based both on the application state and on its affective alignment with the human.Affect control theory arises from the long tradition of symbolic interactionism that began almost three hundred years ago with the insights of Adam Smith [2] into the self as a mirror of the society in which it is embedded: the so-called looking-glass self [2]. These insights eventually led to the modern development of structural symbolic interactionism through Mead, Cooley, and Stryker [3], and culminating in Heise’s affect control theory (ACT) [1], which this paper extends. Although ACT, and symbolic interactionism in general, are very well established theories in sociology, they have had little or no impact in artificial intelligence. This paper is the first to propose affect control theory as a fundamental substrate for intelligent agents, by elaborating a POMDP-based formulation of the underlying symbolic interactionist ideas. This new theory allows ACT to be used in goal-directed human-interactive systems, and thereby allows A.I. researchers to connect to over fifty years of sociological research on cultural sentiment sharing and emotional intelligence. The theory also contributes a generalisation of affect control theory that we expect will lead to novel developments in sociology, social psychology, and in the emerging field of computational social science [4].The main contribution of this paper is therefore of a theoretical nature, which we demonstrate in simulation. We have also implemented the theory in a simple tutoring system and in an assistive technology that is designed to assist persons with dementia. We report the results of an empirical survey and demonstrative study with human participants in the case of the tutoring system. The assistive technology is further described in [5]. Therein, a prompting system delivers audio-visual cues to a person using a variety of different affective “styles”. The mapping from non-verbal behaviours of the user to the “style” of prompt is defined by BayesAct alone.1.1. Model overviewBayesAct is a partially observable Markov decision process (POMDP, see Fig. 1(a) and Section 2.2) model of an agent interacting with an environment. The environment is modelled, as usual in a POMDP, with a set of states, X. A BayesAct agent has actions, A, available to it, and these actions change the state of the environment according to a stochastic transition function. The environment model (states) are not assumed to be observable (they are latent), but the agent has access to a set of observations (cid:2)x, from which it can infer the state of the environment by using Bayes’ rule and a stochastic observation function that relates states to observations. Finally, a utility function, R, describes the preferences of the agent on a numerical scale. The utility function can be used by a Bayesian (sequential) decision maker to optimize decisions (action choices) in the long term.BayesAct is modelling the case where the environment contains humans (or other BayesAct agents) who are partially responsible for the state dynamics. BayesAct therefore includes a latent user model as part of its state space (shown as factor Y in Fig. 1(a)). The user model describes the identity (see footnote 1) of the agent and of the human it is interacting with, and conditions (stochastically) the dynamics of the state.The identities are modelled as four concurrently evolving discrete-time non-linear dynamical systems over a three di-mensional continuous affective space. The three dimensions are: evaluation (how good/bad something is), potency (how strong/weak), and activity (how active/passive). The space is referred to as “EPA” space, and it has been found by soci-ologists to capture over 80% of the variance in affective meanings ascribed by humans across cultures and languages [6], and is in some sense “fundamental” to human emotion (see Section 2.1). It has also been used by other works in affective computing (where it is referred to as “PAD” space or Pleasure–Arousal–Dominance, see Section 2.3).BayesAct departs from other works on affective computing because it also includes the dynamics of identities in the EPA space. These dynamics are learned from datasets of human sentiments about events, measured during decades of research by sociologists in different cultures around the world, and forming part of a sociological theory called affect control theory (ACT) ([1]; see Section 2.1). As the EPA space, the dynamics are found to be culturally stable and consistent [7]. The dynamics form part of the transition function (for the identities, Y) in the POMDP (see Section 3.2). The dynamics relate an agent’s stable (through time), culturally shared affective sentiments about itself and about other agents (f), to the transient sentiments 1 The meaning of the term identity differs considerably across scientific disciplines. Here, we adhere to the tradition in sociology where it essentially denotes a kind of person in a social situation.136J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172(impressions) that are created by events in the world (τ ). Together, fundamentals and transients are factors in the user model, (Y = {f, τ }). Since each factor has two components (one for each agent), there are four dynamical systems involved. Parts of Y may have associated observations, (cid:2) f , as well, allowing inference from measured evidence. When events are consistent (congruent with an agent’s stable, culturally agreed upon sentiments), the impressions created are harmonious. When events are inconsistent, then impressions created are dissonant. The dissonance is referred to as deflection, computed as (f − τ )2. In either case (harmony or dissonance), the BayesAct dynamics provide a heuristic prescription (something like a social norm, but see Section 2.3.3) in the EPA space for both agent’s actions (see Section 3.4). The prescription arises from the basic principle of ACT: humans are motivated to reduce deflection and bring dissonant situations back to confirmation of sentiments.Every action that a BayesAct agent can take (a) has an affective aspect or “meaning” (ba in Fig. 1). For example, a tutor giving a really hard exercise to a student would be considered quite bad (mean) and powerful, whereas a really easy exercise would be considered quite nice, but rather weak (see Section 4.2.1). An assistive system that commands someone would be seen as powerful, whereas a suggestion would be seen as weaker (see Section 4.2.2). The heuristic prescription (to reduce deflection) indicates what the affective meaning (ba) of the next action (a) to take should be, and is computed as the one that will serve to minimize the deflection (will bring τ closer to f). This heuristic is thought to be used by humans as a “fast thinking” [8] mechanism to quickly make decisions in social situations. It is also believed to lead to social orders that encode solutions to social dilemmas [9].A BayesAct agent uses this heuristic to guide its search for an action to take in the POMDP model of the environment. It considers only those actions that will appear affectively similar to the heuristic guide. For example, consider a doctor interacting with a patient. The doctor will “feel” that the right actions to take will be professional, serious and empathetic, not friendly, jovial and carefree. He will therefore “advise” the patient rather than propose to go to a movie. In fact, he would have to work hard to even consider any action that would be jovial, friendly and merry. In contrast, a teacher would “feel” a need to be somewhat jovial and friendly, and would not consider actions that seemed too solemn. The affective heuristic essentially restricts the action space over which an agent must search.The next section explains ACT and POMDPs in more detail and briefly discusses related work. Section 3 then gives full details of the new BayesAct model. Section 4 discusses simulation and human experiments, and Section 5 concludes. Appendices A–E give some additional results and mathematical details that complement the main development. Parts of this paper appeared in a shortened form in [10]. More details, simulations and videos can be found at bayesact.ca.2. BackgroundThis section presents the background material necessary with Section 2.1 giving the sociological theory, Section 2.2presenting POMDPs, Section 2.3 reviewing related work.2.1. Affect control theoryAffect control theory (ACT) is a comprehensive social psychological theory of human social interaction [1]. ACT proposes that peoples’ social perceptions, actions, and emotional experiences are governed by a psychological need to minimize deflections between culturally shared fundamental sentiments about social situations and transient impressions resulting from the dynamic behaviours of interactants in those situations.Fundamental sentiments f are representations of social objects, such as interactants’ identities and behaviours or en-vironmental settings, as vectors in a three-dimensional affective space [7,11]. The basis vectors of the affective space are called Evaluation/valence, Potency/control, and Activity/arousal (EPA). The EPA space is hypothesised to be a universal organ-ising principle of human socio-emotional experience, based on the discovery that these dimensions structure the semantic relations of linguistic concepts across languages and cultures [7,6,12,13]. They also emerged from statistical analyses of the co-occurence of a large variety of physiological, facial, gestural, and cognitive features of emotional experience [14], relate to the universal dimensionality of personality, non-verbal behaviour, and social cognition [15], and are believed to corre-spond to the fundamental logic of social exchange and group coordination [15]. These three dimensions are also thought to be related directly to intrinsic reward [16], and are in correspondence with the major factors governing choice in social dilemmas [15].EPA profiles of concepts can be measured with the semantic differential, a survey technique where respondents rate affective meanings of concepts on numerical scales with opposing adjectives at each end (e.g., {good, nice} ↔ {bad, awful}for E; {weak, little} ↔ {strong, big} for P; {calm, passive} ↔ {exciting, active} for A). Affect control theorists have compiled databases of a few thousand words along with average EPA ratings obtained from survey participants who are knowl-edgeable about their culture [7]. For example, most English speakers agree that professors are about as nice as students (E), however more powerful (P) and less active (A). The corresponding EPA profiles are [1.7, 1.8, 0.5] for professor and [1.8, 0.7, 1.2] for student (values range by convention from −4.3 to +4.3 [7]). Shank [17] and Troyer [18] describe exper-iments to measure EPA fundamental sentiments related to technology and computer terms. Shank shows that people have shared cultural identity labels for technological actors, and that they share affective sentiments about these labels. He also showed that people view these technological actors as behaving socially, as was previously explored in [19].(cid:2)J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172137In general, within-cultural agreement about EPA meanings of social concepts is high even across subgroups of society, and cultural-average EPA ratings from as little as a few dozen survey participants have been shown to be extremely stable over extended periods of time [7]. In some sense, a “culture” is defined by agreement on these fundamental sentiments. For example, sub-cultures have more specific shared sentiments within a smaller group, as explored in [1]. These findings may seem surprising in light of ever-present societal conflicts as evidenced for example by competing political ideologies, but research has consistently shown that the number of contested concepts is small relative to the stable and consensual semantic structures that form the basis of our everyday social interactions and shared cultural understanding [7,20].Social events can cause transient impressions τ of identities and behaviours that deviate from their corresponding fun-damental sentiments f. ACT models this formation of impressions from events with a minimalist grammar of the form actor–behaviour–object. Extended versions of ACT’s mathematical models also allow for representing environmental settings (such as a university or a funeral) and identity modifiers (such as a boring professor or a lazy student) [21–23]. In the interest of parsimony, we will limit our present discussion to the basic actor–behaviour–object scheme. Consider, for exam-ple, a professor (actor) who yells (behaviour) at a student (object). Most observers would agree that this professor appears considerably less nice (E), a bit less potent (P), and certainly more aroused (A) than the cultural average of a professor. Such transient shifts in affective meaning caused by specific events can be described with models of the form τ = MG (f), where G and M are functions with statistically estimated parameters from empirical impression-formation studies where survey respondents rated EPA affective meanings of concepts embedded in a few hundred sample event descriptions such as the example above [1]. Linguistic impression-formation equations exist for English, Japanese, and German [7]. In ACT, the sum of squared Euclidean distances between fundamental sentiments and transient impressions is called deflection:D =w i(fi − τ i)2,(1)iwhere w i are weights (usually set to 1.0).Affective Deflection is hypothesised to correspond to an aversive state of mind that humans seek to avoid, leading to the affect control principle [24]:Definition 1 (Affect control principle). Actors work to experience transient impressions that are consistent with their funda-mental sentiments.ACT is thus a variant of psychological consistency theories, which posit in general that humans strive for balanced men-tal representations whose elements form a coherent Gestalt [25,26]. In cybernetic terminology, deflection is a control signal used for aligning everyday social interactions with implicit cultural rules and expectations [1]. For example, advising a student corresponds much better to the cultural expectation of a professor’s behaviour than yelling at a student. Corre-spondingly, the deflection for the former event as computed with the ACT equations is much lower than the deflection for the latter event. Many experimental and observational studies have shown that deflection is indeed inversely related to the likelihood of humans to engage in the corresponding social actions. For example, the deflection-minimization mecha-nism explains verbal behaviours of mock leaders in a computer-simulated business game [27], non-verbal displays in dyadic interactions [28], and conversational turn-taking in small-group interactions [29].Interact is an implementation of ACT in Java that gives a user the ability to manually simulate interactions between two persons with fixed and known identities. The software also comes with multiple databases of EPA ratings for thousands of behaviours and identities, and sets of predictive equations. Interact is available along with the databases at http :/ /www.indiana .edu /~socpsy /ACT.2.2. Partially observable Markov decision processesA partially observable Markov decision process (POMDP) [30] is a general purpose model of stochastic control that has been extensively studied in operations research [31,32], and in artificial intelligence [33,34]. A POMDP consists of a finite (cid:5)|x, a) denoting the set X of states; a finite set A of actions; a stochastic transition model Pr : X × A → (cid:2)( X), with Pr(x(cid:5)when action a is taken, and (cid:2)( X) is a distribution over X ; a finite observation set probability of moving from state x to x(cid:3)x; a stochastic observation model with Pr(ωx|x) denoting the probability of making observation ωx while the system is in state x; and a reward assigning R(x, a, xinduced by action a. The state is unobservable, but a belief state (a distribution over X ) can be computed that gives the probability of each state being the current one. A generic POMDP is shown as a Bayesian decision network in Fig. 1(a) (solid lines only).(cid:5)(cid:5)) to state transition x to xThe POMDP can be used to monitor the belief state using standard Bayesian filtering [35]. A policy can be computed that maps belief states into choices of actions, such that the expected discounted (by a factor γd < 1.0) sum of rewards is (approximately) maximised. Recent work on so-called “point-based” methods had led to the development of solvers that can handle POMDPs with large state and observation spaces [36–39].In this paper, we will be dealing with factored POMDPs in which the state is represented by the cross-product of a set of variables or features. Assignment of a value to each variable thus constitutes a state. Factored models allow for conditional independence to be explicitly stated in the model. A good introduction to POMDPs and solution methods can be found in [40].138J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172Fig. 1. Two time slices of (a) a general POMDP (solid lines) and a POMDP augmented with affective states (dotted lines); (b) a factored POMDP for Bayesian affect control theory.2.3. Related workEmotions play a significant role in humans’ everyday activities including decision-making, behaviours, attention, and perception [41–44]. This important role is fuelling the interest in computationally modelling humans’ emotions in fields like affective computing [45,46], social computing [47], social signal processing [48], and computational social science [4].Damasio has convincingly argued, both from a functional and neurological standpoint, for emotions playing a key role in decision making and for human social action [41]. His Somatic Marker Hypothesis is contrasted against the Platonic “high-reason” view of intelligence, in which pure rationality is used to make decisions. Damasio argues that, because of the limited capacity of working memory and attention, the Platonic view will not work. Instead, learned neural markers focus attention on actions that are likely to succeed, and act as a neural bias allowing humans to work with fewer alternatives. These somatic markers are “cultural prescriptions” for behaviours that are “rational relative to the social conventions and ethics”.2LeDoux [49] argues the same thing from an evolutionary standpoint. He theorises that the subjective feeling of emotion must take place at both unconscious and conscious levels in the brain, and that consciousness is the ability to relate stimuli to a sense of identity, among other things.With remarkably similar conclusions coming from a more functional (economics) viewpoint, Kahneman has demon-strated that human emotional reasoning often overshadows, but is important as a guide for, cognitive deliberation [8]. Kahneman presents a two-level model of intelligence, with a fast/normative/reactive/affective mechanism being the “first on the scene”, followed by a slow/cognitive/deliberative mechanism that operates if sufficient resources are available. Akerlof and Kranton attempt to formalise fast thinking by incorporating a general notion of identity into an economic model (util-ity function) [50]. Earlier work on social identity theory foreshadowed this economic model by noting that simply assigning group membership increases individual cooperation [51].The idea that unites Kahneman, LeDoux, and Damasio (and others) is the tight connection between emotion and action. These authors, from very different fields, propose emotional reasoning as a “quick and dirty”, yet absolutely necessary, guide for cognitive deliberation. The neurological underpinnings of this connection are discussed by Zhu and Thagard [52] who, following LeDoux [49], point to the amygdala as being the “hub” of the wheel of emotional processing in the brain, and discuss how emotion plays an important role in both the generation, and in the execution of action. They discuss two neural pathways from the sensory thalamus to amygdala that are used in action generation: the direct “low road”, and the more circuitous “high road” that makes a stop in the sensory cortex. While the low road enables fast, pre-programmed, reactive responses, the high-road enables a more careful evaluation of the situation. The two pathways complement each other, with the “low road” opting for more potentially life-saving false alarms than the high road, but giving critical guidance and focusing attention of the higher-level processing units on actions that are more likely to succeed. ACT gives a functional account of the quick pathway as sentiment encoding prescriptive behaviour, while BayesAct shows how this account can be extended with a slow pathway that enables exploration and planning away from the prescription.2.3.1. Affective computingBayesAct also aligns with work in affective computing, which is generally concerned with four main problems: affect recognition (vision-based, acoustic-based, etc.) [53,54], generation of affectively modulated signals such as speech and fa-cial expressions [55,56], the study of human emotions including affective interactions and adaptation [57], and modelling affective human–computer interaction, including embodied conversational agents [58–61].This paper does not attempt to address the first two questions concerning generation and recognition of affective signals. We assume that we can detect and generate emotional signals in the affective EPA space. There has been a large body of work in this area and many of the proposed methods can be integrated as input/output devices with our model. Our model gives the mechanism for mapping inputs to outputs based on predictions from ACT combined with probabilistic and decision theoretic reasoning. The probabilistic nature of our model makes it ideally suited to the integration of noisy sensor signals of affect, as it has been used for many other domains with equally noisy signals [62].2 Terms in quotes from [41], p. 200.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172139Our focus is primarily on the third and fourth questions of how to build intelligent interactive systems that are emo-tionally aware using established theories of emotional reasoning. To do this, we propose to leverage research in sociology on Affect Control Theory (ACT) [1]. BayesAct overcomes three key limitations of ACT. First, it accounts for the variation in consensus about sentiments explicitly as a probability distribution, allowing the model to reflect unique human experiences as well as overall cultural knowledge and situational factors leading to uncertainty about identities [7]. Second, it allows for multiple, changing and unstable identities [63]. Many contemporary philosophers and poststructuralist sociologists and have challenged the notion of a unified, stable, and authentic self, instead arguing that our interpretations of human experi-ence are constantly changing [64]. In this view, the self is “shifting, fragmented, and [comprised of] multiple, often contradictory identities, belying the notion of unity or constancy” [65]. Lastly, BayesAct accounts for noise in communication.ACT is not currently well-known in the fields of affective computing or artificial intelligence, perhaps because of its in-tellectual origins in sociology. Affective computing researchers have attended more to the dominant theories in psychology, and despite an obvious overlap in basic intellectual interests, there is often little cross-disciplinary knowledge integration between sociologists and psychologists. However, we think that ACT aligns well with two main families of emotion theories well-known in affective computing: dimensional theories and appraisal theories. Dimensional theories define emotion as a core element of a person’s state, usually as a point in a continuous dimensional space of evaluation or valence, arousal or activity, and sometimes dominance or potency [66,13,67], corresponding to the EPA dimensions of affect control theory. Ap-praisal theories come in different variants, but generally posit that emotional states are generated from cognitive appraisals of events in the environment in relation to the goals and desires of the agent [68–70,46]. For example, the classic decision tree in the Ortony, Clore and Collins appraisal (OCC) model [68] has cognitive appraisal decision nodes and emotions as leaves. Coping rules are usually devised in order to map from the appraised emotions to changes in agent behaviour.Gratch and Marsella [71] are possibly the first to propose a concrete computational mechanism for coping. Building on the work of Smith and Lazarus [72], they propose a five stage process wherein beliefs, desires, plans and intentions are first formulated, and upon which appraisals frames are computed. Appraisals are then mapped to multiple emotions (using OCC), which are then aggregated (summed) using an overall emotional state, or “mood”. Coping strategies then use a set of rules to handle the emotions either inwardly, by modifying elements of the model such as probabilities and utilities, or outwardly, by modifying plans or intentions. Lisetti and Gmytrasiewicz define specific coping mechanisms that they refer to as “action tendencies”, highlighting their importance in guiding actions [73]. However, these action tendencies only exist for nine basic discrete emotion categories, and so only provide a very coarse definition of coping mechanisms. ACT and BayesActspecify one simple coping mechanism: minimizing inconsistency in continuous-valued sentiment. This, when combined with mappings describing how sentiments are appraised from events and actions, can be used to prescribe actions that maximally reduce inconsistency.2.3.2. Appraisal and dimensional theories of emotionAs discussed in [74], affect control theory is conceptually compatible with both dimensional and appraisal theories. The connections with dimensional theories are obvious since emotions in ACT are represented as vectors in a continuous dimensional space. However, our generalisation, BayesAct, releases ACT from the constraint of representing emotion as a single point in affective space, since it represents emotions as probability distributions over this space. This allows BayesActto represent mixed states of emotions, something usually lacking from dimensional theories, but often found in appraisal theories [75].The connection of ACT with appraisal theories comes from the assumption that emotions result from subjective inter-pretations of events rather than immediate physical properties of external stimuli [74]. Appraisal theorists describe a set of fixed rules or criteria for mapping specific patterns of cognitive evaluations onto specific emotional states. The logic of ACT is quite similar: emotional states result from interpretations of observed events. The difference is that ACT emphasizes the cultural embeddedness of these interpretations through the central role of language in the sense-making process. This rea-soning stems from the origins of the theory in symbolic interactionism [76], a dominant paradigm in sociology. The reliance upon linguistic categories ensures that individual appraisals of situations follow culturally shared patterns. The reason why this approach is easily reconciled with appraisal theories is the fact that the EPA dimensions of affective space organize linguistic categories, as discussed above. These dimensions can be understood as very basic appraisal rules related to the goal congruence of an event (E), the agent’s coping potential (P), and the urgency implied by the situation (A) [74,77]. How-ever, ACT works without explicitly defining rules relating specific goals and states of the environment to specific emotions. Instead, ACT treats the dynamics of emotional states and behaviours as continuous trajectories in affective space that exist independently of any cognitive evaluations of goals, etc. Deflection minimisation is the only prescribed mechanism, while the more specific goals tied to types of agents and situations are assumed to emerge from the semantic knowledge base of the model.2.3.3. Social normsSocial norms [78] have a long history in social psychology, and, more recently in normative multi-agent systems (Nor-MAS) [79]. Although one could see the prescriptions of ACT as being normative, we emphasise that this refers to a causal prediction of affective dynamics rather than a set of logical rules. Nevertheless, logical norms, or models of other agent intentions [80], such as those described in multi-agent system research, can be implemented in the dynamics of BayesAct at the cognitive level.140J. Hoey et al. / Artificial Intelligence 230 (2016) 134–1722.3.4. AI and affective interactionRecently, significant work has emerged in affective computing that uses probabilistic reasoning to build intelligent inter-active systems. Pynadath and Marsella [60] use a POMDP model of psychological consistency theories to build interactive agents. Their model estimates the relative value of actions based on various application-specific appraisal dimensions and a variety of influence factors such as consistency, self interest and “bias”.In a similar vein, an adaptive system combining fuzzy logic with reinforcement learning is described in [81]. This model also uses application-dependent appraisal rules based on the OCC model [68] to generate emotional states, and a set of ad-hoc rules to generate actions. Bayesian networks and probabilistic models have also seen recent developments [82,83]based on appraisal theory [68]. Emotions have also been used to guide reinforcement learning. In [84] higher valence is used to push an agent to increased exploitation of current knowledge. In [85], the SOAR cognitive architecture is augmented with a reinforcement learning agent that uses emotional appraisals as intrinsic reward signals [86].Appraisal and dimensional theories of emotions are combined in the WASABI architecture [59] through primary (core feelings or gut reactions) and secondary (appraisals/interpretations) emotions. Primary (infant-like) emotions are stimulated non-consciously, and drive a core emotional state of valence (good vs. bad). A relaxation dynamics is then hypothesised in which the valence is decreased along two dimensions with varying rates, the slower one corresponding to “mood”. A third dimension is added to this space to react to the absence of any action, corresponding to “boredom”. The resulting three dimensional space is then combined with an estimate of dominance which is consciously appraised based on the current situation. A linear combination of valence, mood and dominance creates a state in the pleasure–arousal–dominance (PAD) space of Russell and Mehrabian [67]. Finally, PAD is used to generate automatic, involuntary behaviours such as the facial expressions of a virtual character, as well as deliberate, cognitive actions such as moves in the game, and coping strategies similar to those in [71]. Although this architecture bears some similarity to BayesAct in that it combines dimensional and appraisal theories, it uses ad hoc elements and coping rules.2.3.5. ApplicationsConati and Maclaren [83] use a decision theoretic model to build an affectively intelligent tutoring system, but again rely on sets of labelled emotions and rules from appraisal theories. This is typically done in order to ease interpretability and computability, and to allow for the encoding of detailed prior knowledge into an affective computing application. BayesActdoes not require a statically defined client (e.g. student) or agent (e.g. tutor) identity, but allows the student and tutor to dynamically change their perceived identities during the interaction. This allows for greater flexibility on the part of the agent to adapt to specific user types “on the fly” by dynamically learning their identity, and adapting strategies based on the decision theoretic and probabilistic model. Tractable representations of intelligent tutoring systems as POMDPs have recently been explored [87], and allow the modelling of up to 100 features of the student and learning process. Other recent work on POMDP models for tutoring systems include [88,89]. Our emotional “plug-in” would seamlessly integrate into such POMDP models, as they also use Monte-Carlo based solution methods [39]. The example we explore in Section 4.2.1 is a simplified version of these existing tutoring system models.POMDPs have been widely used in mobile robotics [90], for intelligent tutoring systems [87,83,89], in spoken-dialog systems [91], and in assistive technology [92,93]. In Section 4.2.2 we apply our affective reasoning engine to a POMDP-based system that helps a person with Alzheimer’s disease to handwash [92].3. Bayesian formulation of ACTWe are modelling an interaction between an agent (the computer system) and a client (the human), and will be for-mulating the model from the perspective of the agent (although this is symmetric). We will use notational conventions where capital symbols (F, T) denote variables or features, small symbols (f, τ ) denote values of these variables, and boldface symbols (F, T, f, τ ) denote sets of variables or values. We use primes to denote post-action variables, so xmeans the value of the variable X after a single time step.(cid:5)A human-interactive system can be represented at a very abstract level using a POMDP as shown in Fig. 1(a, solid lines). In this case, X represents everything the system needs to know about both the human’s behaviours and the system state, and can itself be factored into multiple correlated attributes. For example, in a tutoring system, X might represent the current state of the student’s knowledge, the level at which they are working, or a summary of their recent test success. The observations (cid:2)x are anything the system observes in the environment that gives it evidence about the state X. In a tutoring system, this might be what the student has clicked on, or if the student is looking at the screen or not. The system actions A are things the system can do to change the state (e.g. give a test, present an exercise, modify the interface, move a robot) or to modify the human’s behaviours (e.g. give a prompt, give an order). Finally, the reward function is defined over state-action pairs and rewards those states and actions that are beneficial overall to the goals of the system-human interaction. In a tutoring system, this could be getting the student to pass a test, for example. Missing from this basic model are the affective elements of the interaction, which can have a significant influence on a person’s behaviour. For example, a tutor who imperatively challenges a student “Do this exercise now!” will be viewed differently than one who meekly suggests “here’s an exercise you might try...”. While the propositional content of the action is the same (the same exercise is given), the affective delivery will influence different students in different ways. While some may respond vigorously to the challenge, others may respond more effectively to the suggestion.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–1721413.1. Basic formulationBayesian affect control theory (BayesAct for short) gives us a principled way to add the emotional content to a human interactive system by making four key additions to the basic POMDP model, as shown by the dashed lines in Fig. 1(a):1. An unobservable variable, Y, describes sentiments of the agent about identities and behaviours. The dynamics of Y is given by empirical measurements in ACT (see below).2. Observations (cid:2) f give evidence about the part of the sentiments Y encoding behaviours of the client.3. The actions of the agent are now expanded to be B = { A, Ba}. The normal state transition dynamics can still occur based only on A, but now the action space also must include an affective “how” for the delivery “what” of an action.4. The application-specific dynamics of X now depends on sentiments, Pr(X(cid:5), A), and will generally follow the original (cid:5)|X, A), but now moderated by sentiments. For example, X may move towards a goal, but less quickly (cid:5)|X, Ydistribution Pr(Xwhen deflection is high.Fig. 1(b) shows a graphical model of the ACT model we are proposing. We can make the association of the state S ={F, T, X}, the observations (cid:2) = {(cid:2)x, (cid:2) f }, and the action B = { A, Ba}. We denote Y = {F, T}, S = {Y, X}.Let F = {Fi j} denote the set of fundamental agent sentiments about itself where each feature Fi j, i ∈ {a, b, c}, j ∈ {e, p, a}denotes the jth fundamental sentiment (evaluation, potency or activity) about the ith interaction object: actor (agent), behaviour, or object (client). Let T = {Ti j} be similarly defined and denote the set of transient agent sentiments. Variables Fi jand Ti j are continuous valued and F, T are each vectors in a continuous nine-dimensional space. Indices will always be in the same order: sentiment on the right and object on the left, thereby resolving the ambiguity between the two uses of the index “a”. We will use a “dot” to represent that all values are present if there is any ambiguity. For example, the behaviour component of F is written Fb· or Fb for short. In a tutoring system, F would represent the fundamental sentiments the agent has about itself (Fa·), about the student (Fc·) and about the most recent (tutor or student) behaviour (Fb·). T would respresent the transient impressions created by the sequence of recent events (presentations of exercise, tests, encouraging comments, etc.). T may differ significantly from F, for example if the student “swears at” the tutor, he will seem considerably more “bad” than as given by shared fundamental sentiments about students (so τ ce < fce ).Affect control theory encodes the identities as being for “actor” (A, the person acting) and “object” (O, the person being acted upon). In BayesAct, we encode identities as being for “agent” and “client” (regardless of who is currently acting). However, this means that we need to know who is currently acting, and the prediction equations will need to be inverted to handle turn-taking during an interaction. This poses no significant issues, but must be kept in mind if one is trying to understand the connection between the two formulations. In particular, since we are assuming a discrete time model, then the “turn” (who is currently acting) will have to be represented (at the very least) in X.3 Considering time to be event-based, however, we can still handle interruptions, but simultaneous action by both agents will need further consideration.4In the following, we will use symbolic indices in {a, b, c} and {e, p, a}, and define two simple index dictionaries, dιand dα , to map between the symbols and numeric indices ({0, 1, 2}) in matrices and vectors so that, dι(a) = 0, dι(b) =1, dι(c) = 2 and dα(e) = 0, dα(p) = 1, dα(a) = 2. Thus, we can write an element of F as Fbp which will be the kth element of the vector representation of F, where k = 3dι(b) + dα(p). A covariance in the space of F might be written (cid:9) (a 9 × 9matrix), and the element at position (n, m) of this matrix would be denoted (cid:9)i j,kl where n = 3dι(i) + dα( j) and m =3dι(k) + dα(l). We can then refer to the middle 3 × 3 block of (cid:9) (the covariance of Fb with itself) as (cid:9)b·,b·. Although the extra indices seem burdensome, they will be useful later on as we will see. We will also require an operator that combines two nine-dimensional sentiment vectors w and z by selecting the first and last three elements (identity components) of wand the middle three (behaviour) elements of z:⎡⎤(cid:7)w, z(cid:8) ≡⎣⎦ .wazbwc(2)The POMDP action will be denoted here as B = { A, Ba} (behaviour of the agent), but this is treated differently than other variables as the agent is assumed to have freedom to choose the value for this variable. The action is factored into two parts: A is the propositional content of the action, and includes things that the agent does to the application (change screens, present exercises, etc.), while Ba is the emotional content of the action. Thus, Ba gives the affective “how” for the delivery “what” of an action, A. The affective action Ba = {Bae, Bap, Baa} will also be continuous valued and three dimensional: the agent sets an EPA value for its action (not a propositional action label).5 For example, a tutoring system may “command” a student to do something (Ba = {−0.1, 1.3, 1.6}), or may “suggest” instead (Ba = {1.8, 1.4, 0.8}), which is considerably more 3 Note that the “turn” can be stochastic. In general, the agent maintains a belief representing its uncertainty about whose turn it is.4 One method may be to have a dynamically changing environment noise: an agent cannot receive a communication from another agent if it is simulta-neously using the same channel of communication, for example.5 There may be constraints in the action space that must be respected at planning and at decision-making time. For example, you can’t give a student a really hard problem to solve in a way that will seem accommodating/submissive.142J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172“good” than the command. The client behaviour is implicitly represented in the fundamental sentiment variables Fb (a three dimensional vector), and we make some observations of this behaviour, (cid:2) f , another three dimensional vector. The ACT databases can be used to map words in written or spoken text to EPA values that are used as (cid:2) f , for example.Finally, a set of variables X represents the state of the system (e.g. the state of the computer application or interface). We do not assume that this system state is directly observable, and so also use sets of observation variables (cid:2)x. The state space described by X may also include affective elements. For example, a student’s level of frustration could be explicitly modelled within X. In Section 4.2.2, we describe an affective prompting system that is used for a person with a cognitive disability. In this case, we explicitly represent the “awareness” of the person as a component of X, and the “propositional” actions of prompting can change the “awareness” of the person. However, as we discuss in Section 3.2, emotions can be computed in ACT from fundamental and transient sentiments, and so such explicit representation may not be necessary.3.1.1. Transient dynamicsThe empirically derived prediction equations of ACT can be written as τ (cid:5) = M(x)G (f(cid:5), τ , x) where G is a non-linear (cid:5)operator that combines τ , f, and x, and M(X) is the prediction matrix (see Section 2.1 and [1]) that now depends on whose “turn” it is (encapsulated in X so that M(X) ≡ M(Xw ), where Xw ∈ X and xw ∈ {agent, client}).6 More precisely, we write the function G for the case when it is the turn of the agent (given by xw ) as the 29 × 1 (column) vector (equivalent to (11.16) in [1]):(cid:5)G (f, τ , xw = agent) = [1 τae τap τaa(cid:5)fbe(cid:5)(cid:5)ba τce τcp τcaffbp(cid:5)(cid:5)(cid:5)(cid:5)bp τapτcpbe τapfbp τaeτce τaeτcp τapfτaefbe τaef(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)bp τcafbp τcpfbe τcefbe τcpfba τcefτapτca τaafbp(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)]Tbp τapτcafbp τapτcpfbe τaeτcpfba τaeτcefτcpfbp(3)and for the case when it is the turn of the clientτ a· → τ c· and τ c· → τ a·):it is the same, but with agent and client indices swapped on τ (so (cid:5)G (f, τ , xw = client) = [1 τce τcp τca(cid:5)fbe(cid:5)(cid:5)ba τae τap τaaffbp(cid:5)(cid:5)(cid:5)(cid:5)bp τcpτapbe τcpfbp τceτae τceτap τcpfτcefbe τcef(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)bp τaafbp τapfbe τaefbe τapfba τaefτcpτaa τcafbp(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)]Tbp τcpτaafbp τcpτapfbe τceτapfba τceτaefτapfbp(4)The terms in these two functions are arrived at through an experimental procedure detailed in [94], and are those that have the most predictive power for the transient impressions in the empirical survey data. These two functions may also depend on other parts of X (the “settings”, see Section 12.1 of [1]). M is then a 9 × 29 matrix of empirically derived coefficients that, when multiplied by these vectors, yields the subsequent transient sentiments. M must be a function of x, since we will swap all “actor” rows with “object” rows when it is the client turn. Thus,⎤⎤⎡⎡M(xw = agent) ≡ M ≡⎣M a·M b·M c·⎦ M(xw = client) ≡⎣⎦M c·M b·M a·where M is from the ACT database.In general, we can imagine G in Equations (3) and (4) as vectors with all possible products of up to m elements from (cid:5)(cid:8). These are a set of “features” derived from the previous transients and current fundamentals. M is then a 9 × 9m(cid:7)τ , fmatrix of coefficients, many of which may be very small or zero. Empirical studies have narrowed these down to the set above, using m = 3. Ideally, we would like to learn the coefficients of M , and the features that are used in G , from data during interactions.Since G only uses the behaviour component of f(cid:5), τ , x) =(cid:5)− C (τ , x), where H and C are 9 × 3 and 9 × 1 matrices of coefficients from the dynamics M and G together. H (τ , x)fb(cid:5)(cid:5)That is, Hi j,k is the sum of all the terms in row 3dι(i) + dα( j) of MG that contain fbk divided out. Thus, the sum of bk, with f(cid:5)(cid:5)all terms in row 3dι(i) + dα( j) of MG that contain fbk. Similarly, Ci j is the sum of all terms in row 3dι(i) + dα( j)bk is Hi j,kf(cid:5)b element at all. Simply put, the matrices H and C are a refactoring of the operators MG , such of MG that contain no f(cid:5)b is obtained.that a linear function of fWe then postulate that the dynamics of τ (cid:5)will follow this prediction exactly, so that the distribution over τ (cid:5), we can also group terms together and write τ (cid:5) = M(x)G (fis deter-(cid:5)ministic and given by:(cid:5)b , the current behaviour) because behaviours are temporally independent (i.e. the behaviour at time t is not dependent 6 G depends on fon the behaviour at any previous time). This is referred to as “behaviours being recalled from memory with transients set equal to fundamentals” in [1](p. 85).(cid:5)(in fact, only fJ. Hoey et al. / Artificial Intelligence 230 (2016) 134–172(cid:5)Pr(τ (cid:5)|τ , f(cid:5), x) = δ(τ (cid:5) − τ ) = δ(τ (cid:5) − H (τ , x)fb− C (τ , x)),where(cid:7)δ(x) =1 if x = 00 otherwise3.1.2. Deflection potential143(5)(6)The deflection in affect control theory is a nine-dimensional weighted Euclidean distance measure between fundamental sentiments F and transient impressions T (Section 2.1). Here, we propose that this distance measure is the logarithm of a probabilistic potential(cid:5)ϕ(f, τ (cid:5)) ∝ e−(f(cid:5)−τ (cid:5))T (cid:9)−1(f(cid:5)−τ (cid:5)).(7)The covariance (cid:9) is a generalisation of the “weights” (Equation (1) and [1]), as it allows for some sentiments to be more significant than others when making predictions (i.e. their deflections are more carefully controlled by the participants), but also represents correlations between sentiments in general, allowing for the deflection potential to be sensitive to different (cid:5), τ (cid:5)) = D + cdirections in EPA space. If (cid:9) is diagonal with elements 1w iwhere D is the deflection from Equation (1) and c is a constant., i ∈ {1, . . . , 9}, then Equation (7) gives us log ϕ(f3.1.3. Fundamental dynamicsTo predict the fundamental sentiments, we combine the deflection potential from the previous section with an “inertial” term that stabilises the fundamentals over time. This gives the probabilistic generalisation of the affect control principle (Definition 1):(cid:5),τ ,x)−ξ(f(cid:5),f,ba,x)Pr(f−ψ(f(cid:5)|f, τ , x, ba, ϕ) ∝ e(cid:5), τ , x) = (f(cid:5) − M(x)G (f(8)(cid:5) − M(x)G (f(cid:5), τ , x)) and ξ represents the temporal “inertial” dynamics of f(cid:5)where ψ(f, (cid:5)encoding both the stability of affective identities and the dynamics of affective behaviours. ξ is such that fb is equal to ba(cid:5)(cid:5)if the agent is acting, and otherwise is unconstrained, and fc are likely to be close to fa, fc , respectively. Equation (8) can a, fbe re-written as a set of multivariate Gaussian distributions indexed by x, with means and covariances that are non-linearly dependent on f, ba and τ . The full derivation is in Section 3.3.(cid:5), τ , x))T (cid:4)−1(f3.1.4. Other factorsThe other factors in BayesAct are as follows:• R(f, τ , x) is a reward function giving the immediate reward given to the agent. We assume an additive functionR(a, f, τ , x) = R x(a, x) + R s(f, τ )(9)where R x encodes the application goals (e.g. to get a student to pass a test), andR s ∝ −(f − τ )2• Pr(x(cid:5)|x, fdepends on the deflection. The relative weighting and precise functional form of these two reward functions require further investigation, but in the examples we show can be simply defined. The affect control principle only considers R s , and here we have generalised to include other goals. Other reward functions beyond additive could also be considered.(cid:5), τ (cid:5), a) denotes how the application progresses given the previous state, the fundamental and transient senti-ments, and the (propositional) action of the agent. The dependence on the sentiments is important: it indicates that the system state will progress differently depending on the affective state of the user and agent. In Section 4.2 we explore this idea further in the context of two applications by hypothesising that the system state will more readily progress towards a goal if the deflection (difference between fundamental and transient sentiments) is low.• Pr(ω f |f), Pr(ωx|x) observation functions for the client behaviour sentiment and system state, respectively. These func-tions are stochastic in general, but may be deterministic for the system state (so that X is fully observable). It will not be deterministic for the client behaviour sentiment as we have no way of directly measuring this (it can only be inferred from data).3.2. Transition dynamicsOverall, we are interested in computing the probability distribution over the sentiments and system state given the history of actions and observations. Denoting S = {F, T, X}, and (cid:2) = {(cid:2) f , (cid:2)x}, and (cid:2)t, bt, S t are the observations, agentaction, and state at time t, we want to compute the agent’s subjective belief, given the observations, ω, and actions, b, up to time t:144J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172b(st) ≡ Pr(st|ω0, . . . , ωt, b0, . . . , bt)which can be written as(cid:8)b(st) =Pr(st, st−1|ω0, . . . , ωt, b0, . . . , bt)st−1(cid:8)∝Pr(ωt|st)Pr(st|st−1, ω0, . . . , ωt−1, b0, . . . , bt)Pr(st−1|ω0, . . . , ωt−1, b0, . . . , bt)st−1(cid:8)= Pr(ωt|st)Pr(st|st−1, bt)b(st−1)st−1= Pr(ωt|st)Eb(st−1) [Pr(st|st−1, bt)]where Pr(st|st−1, bt) is factored according to Fig. 1(b):(cid:5)(cid:5)|x, f(cid:5), a)Pr(τ (cid:5)|τ , fPr(st| . . .) = Pr(x, x)Pr(f, τ (cid:5)(cid:5)|f, τ , x, ba)(10)(11)This gives us a recursive formula for computing the distribution over the state at time t as an expectation of the transi-(cid:5))(cid:5))Pr(ω f |ftion dynamics taken with respect to the distribution state at time t − 1. Now, we have that and Pr(ω|s) = Pr(ωx|xand rewriting st ≡ sand st−1 ≡ s, we have:(cid:5)(cid:5)b(s) = Pr(ωx|x= Pr(ωx|x(cid:5)(cid:5)(cid:5))Pr(ω f |f(cid:5))Pr(ω f |f)Eb(s))Eb(s)(cid:9)(cid:5)Pr(f(cid:9)Pr(x(cid:10)(cid:5)|f, τ , x, ba, a), τ (cid:5), x(cid:5), a)Pr(τ (cid:5)|τ , f, τ (cid:5)(cid:5)(cid:5)|x, f, x)Pr(f(cid:10)(cid:5)|f, τ , x, ba)(12)The first four terms correspond to parameters of the model as explained in the last section, while we need to develop a method for computing the last term, which we do in the next section.The belief state can be used to compute expected values of quantities of interest defined on the state space, such as the expected deflectionEb(s)[D(s)] =(cid:8)sb(s)D(s),(13)(cid:11)where D(s) is the deflection of s. This gives us a way to connect more closely with emotional “labels” from appraisal theories. For example, if one wanted to compute the expected value of an emotion such as “Joy” in a situation with certain features (expectedness, events, persons, times), then the emotional content of that situation would be explic-itly represented in our model as a distribution over the E–P–A space, b(s), and the expected value of “Joy” would be Eb(s)[Joy(s)] =s b(s)Joy(s), where Joy(s) is the amount of joy produced by the fundamental and transient sentiment state s. In ACT, emotions are posited to arise from the difference between the transient and fundamental impressions of self-identity (τ a and fa, respectively, in BayesAct). A separate set of equations with parameters obtained through empirical studies is presented in Chapter 14 of [1], and emotional states (e.g. the function Joy(s)) can be computed directly using these equations. Emotional displays (e.g. facial expressions) can be viewed as a method for communicating an agent’s cur-rent appraisal of the situation in terms of affective identities that the agent perceives, and would be part of the action space ba. The perception of emotional displays would be simply integrated into BayesAct using observations of client identity, Fc . We do not further expand on direct emotion measures in this paper, but note that this may give a principled method for incorporating explicit appraisal mechanisms into BayesAct, and for linking with appraisal theories [68].3.3. Estimating behaviour sentiment probabilitiesHere we describe how to compute Pr(f(cid:5)|f, τ , x, ba). This is the agent’s prediction of what the client will do next, and is based partly on the principle that we expect the client to do what is optimal to reduce deflection in the future, given the identities of agent and client.We denote the probability distribution of interest as a set of parameters (cid:6) f . Each parameter in this set, θ f , will be a (cid:5)|f, τ , x, ba, θ f ), so that (cid:5)probability of observing a value of fthe distribution over θ f given the knowledge that τ (cid:5)given values for f, τ , ba and x. We write θ f (f(cid:5)and fare related through ϕ(f, τ ) is7:(cid:5); f, τ , x, ba) = Pr(f7 We are postulating an undirected link in the graph between τ and f. An easy way to handle this undirected link properly is to replace it with an equivalent set of directed links by adding a new Boolean variable, D, that is conditioned by both T and F, and such that Pr(D = True|τ , f) ∝ ϕ(τ , f). We then set D = True because we have the knowledge that T and F are related through ϕ(F, T), and the quantity of interest is Pr(θ f |D = True). In the text, we use the shorthand Pr(θ f |ϕ) to avoid having to introduce D.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172(cid:8)Pr(θ f |ϕ) ∝(cid:5)Pr(θ f , f, τ (cid:5), ba, f, τ , x, ϕ)f(cid:5),τ (cid:5),baf,τ ,x(cid:8)=(cid:5)ϕ(f, τ (cid:5)(cid:5))Pr(τ (cid:5)|x, f, τ )Pr(f(cid:5)|f, τ , x, ba, θ f )Pr(θ f )Pr(f)Pr(τ )Pr(x)Pr(ba)f(cid:5),τ (cid:5),baf,τ ,x(cid:8)−(f(cid:5)−τ (cid:5))T (cid:9)−1(fe(cid:5)−τ (cid:5))Pr(τ (cid:5)|x, f(cid:5), τ )θ f (f(cid:5); f, τ , x, ba)Pr(θ f )=f(cid:5),τ (cid:5),baf,τ ,x(cid:11)(cid:11)145(14)(15)(16)z we mean z∈Z (Z is the domain of Z ) and we have left off the infinitesimals (e.g. df). We have assumed where by even priors over f,τ , x and ba. The expression (16) will give us a posterior update to the parameters of the distribution over θ f .Equation (16) gives the general form for this distribution, but this can be simplified by using the determinism of some of the distributions involved, as described at the start of this section. The deterministic function for the distribution over τ (cid:5)will select specific values for these variables, and we find that:(cid:5), dτ (cid:5)(cid:8)Pr(θ f |ϕ) ∝−ψ(fe(cid:5),τ ,x)θ f (f(cid:5); f, τ , x, ba)Pr(θ f )(cid:5),baff,τ ,xwhere(cid:5)ψ(f, τ , x) = (f(cid:5)(cid:5) − MG (f, τ , x))T (cid:4)−1(f(cid:5)(cid:5) − MG (f, τ , x))is the deflection between fundamental and transient sentiments.(17)(18)Equation (17) gives us an expression for a distribution over θ f , which we can then use to estimate a distribution over given the state {f, τ , x, ba} and the known relation between fundamentals and transients, ϕ (ignoring the observations (cid:5)(cid:5) = fF(cid:2) f and (cid:2)x for now):(cid:8)Pr(f(cid:5)|f, τ , x, ba, ϕ) ∝(cid:5)Pr(θ f , f, f, τ (cid:5), τ , x, ba, ϕ)θ f ,τ (cid:5)(cid:8)=−ψ(fe(cid:5),τ ,x)θ f (f(cid:5); f, τ , x, ba)Pr(θ f |x)θ f(cid:8)−ψ(f(cid:5),τ ,x)= eθ f (f(cid:5); f, τ , x, ba)Pr(θ f |x)−ψ(f(cid:5),τ ,x)= eθ f(cid:13)(cid:12)EPr(θ f |x)(θ f )(19)(20)(21)(22)(cid:5)The first term is a distribution over fthat represents our assumption of minimal deflection, while the second is the expected value of the parameter θ f given the prior. This expectation will give us the most likely value of θ f given only the system state x. We know two things about the transition dynamics (θ f ) that we can encode in the prior. First, we know that the behaviour will be set equal to the agent’s action if it is the agent’s turn (hence the dependence on x). Second, we know that identities are not expected to change very quickly. Therefore, we have that−(f(cid:5)−(cid:7)f,ba(cid:8))T (cid:4)−1f (x)(f(cid:5)−(cid:7)f,ba(cid:8))EPr(θ f |x)(θ f ) ∝ e(23)where (cid:7)f, ba(cid:8) is f for the identities and ba for the behaviours (see Equation (2)), and (cid:4) f (x) is the covariance matrix for the inertia of the fundamentals, including the setting of behaviour fundamentals by the agent action. (cid:4) fis a set of parameters governing the strength of our prior beliefs that the identities of client and agent will remain constant over time.8 Thus, (cid:4) fis a 9 × 9 block matrix:(cid:4) f (x) =⎣⎡I 3β 2a00⎤0I 3β 2b (x)0⎦00I 3β 2c(24)8 It may also be the case that ba can change the agent identity directly, so that ba is six-dimensional and (cid:7)f, ba(cid:8) = [ba, fc ]T .146J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172−1f (x)(f(cid:5)|f, τ , x, ba, ϕ) ∝ ePr(fa and β 2where β 2change their identities over time), and β 2(f(cid:5) − (cid:7)f, ba(cid:8))T (cid:4)(cid:5) − (cid:7)f, ba(cid:8)), we therefore have that:c are the variances of agent and client identity fundamentals (i.e. how much we expect agent and client to (cid:5), f, ba, x) ≡b (x) is infinite for a client turn and is zero for an agent turn. Writing ξ(f−ψ(f(cid:5),τ ,x)−ξ(f(cid:5),f,ba,x)(25)We can now estimate the most likely value of F(cid:5)by computingF(cid:5) ∗ = arg maxf(cid:5)Pr(f(cid:5)|f, τ , x, ba, ϕ).This expression will be maximized for exactly the behaviour that minimizes the deflection as given by ψ , tempered by the inertia of changing identities given by ξ . This is the generalisation of the affect control principle (Definition 1, see also Ap-pendix A). We can rewrite this by first rewriting the matrix H asH =⎣⎡⎤⎦HaHbHcwhere Ha ≡ Ha·· (a 3 × 3 matrix giving the rows of H in which Hi j,k have i = a) and similarly for Hb and Hc . We also define I3 as the 3 × 3 identity matrix and 03 as the 3 × 3 matrix of all zeros. We can then write a matrix⎡K =⎣⎤⎦I 30303−Ha03I 3 − Hb 03−HcI 3Using K , we can now write the general form for ψ starting from Equation (18) as:(cid:5)ψ(f, τ , x) = (f(cid:5)(cid:5) − H (τ , x)fb− C (τ , x))T (cid:4)−1(f(cid:5) − C )(cid:5) − K −1C )T K T (cid:4)−1K (f(cid:5) − C )T (cid:4)−1(K f= (K f= (f(cid:5) − K −1C )(cid:5)(cid:5) − H (τ , x)fb− C (τ , x))(26)(27)(28)and thus, if we ignore the inertia from previous fundamentals, ξ , we recognize Equation (28) as the expectation of a Gaussian or normal distribution with a mean of K −1C and a covariance of (cid:4)τ ≡ K −1(cid:4)(K T )−1. Taking ξ into account means that we have a product of Gaussians, itself also a Gaussian the mean and covariance of which can be simply obtained by completing the squares to find a covariance, (cid:4)n equal to the sum in quadrature of the covariances, and a mean, μn, that is proportional to a weighted sum of K −1C and (cid:7)f, ba(cid:8), with weights given by the normalised covariances of (cid:4)n(cid:4)−1and τ(cid:4)n(cid:4), respectively.−1fPutting it all together, we have thatPr(f(cid:5)|f, τ , x, ba, ϕ) ∝ e−(f(cid:5)−μn)T (cid:4)−1n (f(cid:5)−μn)whereμn(cid:4)n = (K T (τ , x)(cid:4)−1K (τ , x) + (cid:4)= (cid:4)nK T (τ , x)(cid:4)−1C (τ , x) + (cid:4)n(cid:4)−1f (x))−1.−1f (x)(cid:7)f, ba(cid:8)(29)(30)(31)(cid:5)The distribution over fin Equation (29) is a Gaussian distribution, but has a mean and covariance that are dependent on f, τ , x and ba through the non-linear function K . Thus, it is not simple to use this analytically as we will explore further below.The “optimal” behaviour from [1] is obtained by holding the identities constant when optimising the behaviour (and similarly for identities: behaviours are held constant). See Appendix B for a reduction of the equations above to those in [1].3.4. Computing policiesThe goal here is to compute a policy π (b(S)) : (cid:2)(S) → A that maps distributions over S into actions, where b(S) is the current belief state as given by Equation (12). This policy is a function from functions (distributions) over a continuous space into the mixed continuous-discrete action space. There are two components to this mapping. First, there is the propositional action as defined by the original POMDP, and second there is the affective action defined by affect control theory.Policies for POMDPs in general can be computed using a number of methods, but recent progress in using Monte-Carlo (sampling) based methods has shown that very large POMDPs can be (approximately) solved tractably, and that this works equally well for continuous state and observation spaces [37,38]. POMCP (Partially Observable Monte-Carlo Planning [38]) is a Monte-Carlo based method for computing policies in POMDPs with discrete action and observation spaces. The continuous J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172147observation space can be handled by discretising the set of observations obtained at each step. This can be done dynamically or using a fixed grid. The continuous action space for BayesAct can be handled by leveraging the fact the we can predictwhat an agent would “normally” do in any state according to the affect control principle: it is the action that minimises the deflection. Given the belief state b(s), we have a probability distribution over the action space giving the probability of each action (see Equation (32)). This normative prediction constrains the space of actions over which the agent must plan, and drastically reduces the branching factor of the search space.Denote the “normal” or expected affective action distribution as π †(s) = π †(fb):(cid:8)(cid:8)(cid:8)(cid:8)(cid:5)b) =π †(fPr(f(cid:5)|f, τ , x, ϕ)b(s) =−(fe(cid:5)(cid:5)a,ffcs(cid:5)(cid:5)a,ffcswhere(cid:5)−μ†n)T ((cid:4)†n)−1(fμ†n(cid:4)†K T (τ , x)(cid:4)−1C (τ , x) + (cid:4)†= (cid:4)†nn = (K T (τ , x)(cid:4)−1K (τ , x) + ((cid:4)†n((cid:4)†f (x))f (x))−1)−1f,−1,(cid:5)−μ†n)b(s)(32)(33)(34)is the same as (cid:4) f given by Equation (24) with β 2and (cid:4)†b (x) set to infinity (instead of zero) so the behaviour sentiments fare unconstrained. Equation (32) computes the expected distribution over fgiven b(s) and then marginalises (sums) out the identity components9 to get the distribution over fb. A sample drawn from this distribution could then be used as an action in the POMCP method. A POMCP “rollout” would then proceed by drawing a subsequent sample from the distribution over client actions, and then repeating the sampling from Equation (32) over agent actions. This is continued to some maximum depth, at which point the reward gathered is computed as the value of the path taken. The propositional actions that update the state x are handled exhaustively as usual in POMCP by looping over them.(cid:5)(cid:11)∗ = {f∗, τ ∗, x∗} = Eb(s)[s] =The integration in Equation (32) may be done analytically if b(s) is Gaussian, but for the general case this may be ∗ − s)∗ = arg maxs b(s) as the most likely challenging and not have a closed-form solution. In such cases, we can make a further approximation that b(s) = δ(swhere ss sb(s) is the expected state (or one could use s(cid:5)state). We will denote the resulting action distribution as π †∗(fb).In this paper, we do not use the full POMCP solution, instead only taking a “greedy” action that looks one step into the future by drawing samples from the “normal” action distribution in Equation (32) using these to compute the expected next reward, and selecting the (sampled) action b(cid:8)that maximizes this:†∗a∗(cid:5)|x(cid:5), f, τ (cid:5), ba)Pr(τ (cid:5)|τ ∗(cid:5), f, x∗)Pr(f∗(cid:5)|f, τ ∗, x∗(cid:5), ba)R(f, τ (cid:5)(cid:5), x)ds(cid:5), ba ∼ π †∗(s(cid:5)(35)(cid:13))(cid:12)Pr(x†∗ab= arg maxbas(cid:5)In practice we make two further simplifications: we avoid the integration over fa and fc in Equation (32) by drawing samples (cid:5)(cid:5)b components as our sample for ba in Equation (35), and we compute the from the distribution over fand selecting the fintegration in Equation (35) by sampling from the integrand and averaging. More details and results on the full POMCP approach can be found in [9].3.5. SamplingWe return now to Equation (10) and consider how we can compute the belief distribution at each point in time. The nonlinearities in the transition dynamics that arise from the dynamics of fundamental sentiments (Equation (29)) prevent the use of an extended (or simple) Kalman filter. Instead, we will find it more convenient and general to represent b(s)using a set of N samples [95]. This will allow us to represent more complex belief distributions, including, but not limited to multi-modal distributions over identities. This can be very useful in cases where the agent believes the client to be one of a small number of identities with equal probability. In such a case, the agent can maintain multiple hypotheses, and slowly shift its belief towards the one that agrees most with the evidence accrued during an interaction. We will write the belief state as [95]:b(s) ∝N(cid:2)i=1w iδ(s − si),(36)where si = {fi, τ i, xi} and w i is the weight of the ith sample.Then, we implement Equation (10) using a sequential Monte Carlo method sampling technique, also known as a particle filter or bootstrap filter [96,95]. We start at time t = 0 with a set of samples and weights {si, w i}i=1...N , which together define a belief state b(s0) according to Equation (36). The precise method of getting the first set of samples is application (cid:5)9 If the agent is able to “set” its own identity, then the integration would be only over fc, the client identity.(cid:5)(a) draw a sample fthe sampling method),(b) draw a sample τ (cid:5)(cid:5)(c) draw a sample x148J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172dependent, but will normally be to draw the samples from a Gaussian distribution over the identities of the agent and client(with standard deviations of β 0c , resp.), and set all weights to 1.0. The agent then proceeds as follows:a and β 01. Consult the policy to retrieve a new action ba ← π (b(st )). If using the approximation in Equation (35), then we first compute the expected value of the state s2. Take action ba and receive observation ω.3. Sample (with replacement) unweighted samples from b(s) from the distribution defined by the current weights.(cid:5)i from the posterior distribution Pr(·|si, ba):4. For each unweighted sample, si , draw a new sample, sNi=1 w i si .=(cid:14)∗tfrom Equation (29) (this is a draw from a multivariate normal, and will likely be a bottleneck for from Equation (5) (this is deterministic so is an easy sample to draw),from Pr(x(cid:5), τ (cid:5), a) (application dependent).(cid:5)|x, f5. Compute new weights for each sample using the observation functions w i = Pr(ω|s6. If all weights are 0.0, set fb = ω and resample.7. The new belief state is b(s(cid:5)) according to Equation (36) with samples, where s(cid:5)= {f(cid:5)i).i, τ (cid:5)(cid:5)ii, x} goto step 1 with s ← s(cid:5).(cid:5)iAn example of the sampling step 4 is shown above to be from a proposal that is exactly Pr(fbe from some other distribution close to this.(cid:5)|f, τ , x, ba, ϕ), but this could We can compute expected values of quantities of interest, such as the deflection, by summing over the weighted set of samples (the Monte-Carlo version of Equation (13)):d(f, τ ) =N(cid:2)i=1w i(fi − τ i)2(37)We have found that, for situations in which the client identity is not known, but is being inferred by the agent, it is necessary to add some “roughening” to the distribution over these unknown identities [96]. This is because the initial set of samples only sparsely covers the identity space (for an unknown identity), and so is very unlikely to come close to the true identity. Coupled with the underlying assumption that the identities are fixed or very slowly changing, this results in the particle filter getting “stuck” (and collapsed) at whatever initial sample was closest to the true identity (which may still be far off in the EPA space, especially when using fewer particles). Adding some zero-mean white noise (in [−σr , σr]) helps solve this degeneracy. We add this noise to any unknown identity (agent or client) after the unweighted samples are drawn in step 3−1/d, where K is a constant, N is the number of samples and d is the above. As suggested by [96], we use σr = K × Ndimension of the search space (in this case 3 for the unknown identity). We use K = 1 in our experiments, and note that we are using white noise (not Gaussian noise), but that this does not make a significant difference.This so-called “roughening” procedure is well known in the sequential Monte-Carlo literature, and in particular has been used for Bayesian parameter estimation [95] (Chapter 10). Our situation is quite similar, as the client identities can be seen as model parameters that are fixed, but unknown. Finally, it may also be possible to change the amount of roughening noise that is added, slowly reducing it according to some schedule as the client identity is learned.It is also possible to mix exact inference over the application state, X, with sampling over the continuous affective space, leading to a Rao-Blackwellised particle filter [97].3.6. Python implementationWe have implemented BayesAct in Python as a class Agent that contains all the necessary methods.10 Applications can use BayesAct by subclassing Agent and providing three key application-dependent methods:• sampleXvar is used to draw a sample from X• reward produces the reward in the current state of X• initXvar is used to initialise X at the start of a simulation or runSub-classes can also implement methods for input and output mappings. For example, an input mapping function could take sentences in English and map them to EPA values based on an affective dictionary, or using sentiment analysis [98,99]. Applications can also learn these mappings by assuming the human user will be behaving according to the affect control principle: whatever the user says can be mapped to the prediction of the theory (or close to it).On top of the functions above for a sub-class of Agent, the following parameters need to be set when using BayesAct in general:10 The code is obtainable through the webpage bayesact.ca.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172149param.αdefault1.0βaβcβ 0aβ 0cγγdNσr0.010.010.010.011.00.9300N−1/3meaningstd. deviation of a diagonal uniform (cid:4), the deflection potential covariance (larger means the affect control principle is not as strong)identity inertia for agent (larger means agent shifts identities more)identity inertia for client (larger means agent thinks client will be shifting identities more)initial identity std. deviation for agent (larger means agent is more uncertain of its own identity)initial identity std. deviation for client (larger means agent is more uncertain of client’s identity)model environment noise std. deviationdiscount factor (if needed)number of samplesroughening noise4. Experiments and resultsOur goal in this section is to demonstrate, in simulation, that BayesAct can discover the affective identities of persons it interacts with, and that BayesAct can augment practical applications with affective dynamics. To establish these claims, we do the following.First, we verify both analytically and empirically that BayesAct can reproduce exactly the affective dynamics predicted by the Interact software [1]. The analytical derivation is done by reducing Equation (29) to the equations in [1] as shown in Appendix B. The empirical demonstration is done by running BayesAct alongside Interact11 and showing that the iden-tical sentiments and actions are generated. We have found a very close match across a range of different agent and clientidentities. These analytical and empirical demonstrations show that BayesAct can be used as a model of human affective dynamics to the extent that it has been shown empirically that Interact is a close model of human affective dynamics. Second, we show how, if we loosen the constraints on the client identity being fixed, BayesAct can “discover” or learn this identity during an interaction with an Interact client. Third, we show how, if both agent and client do not know the identity of their interactant, they can both learn this identity simultaneously. Fourth, we show that a BayesAct agent can adapt to a changing client identity. What this means is that an affective agent has the ability to learn the affective identity of a clientthat it interacts with. We demonstrate this under varying levels of environment noise. Finally, we postulate that, since the agent can learn the affective identity of its client, it can better serve the client in an appropriate and effective manner. We give two preliminary demonstrations of this in Section 4.2.4.1. SimulationsIn this section, we investigate two types of simulation. The first concerns agents with nearly fixed (low variance) per-sonal identities that try to learn the identity of another agent. The second shows what happens if one of the agents is changing identity dynamically. Full results are shown in Appendix C. To enable comparisons with Interact, we use action selection according to our generalised affect control principle only, using an average of 100 samples from Equation (32). These simulations therefore do not directly address how policy computation will affect an application. However, we can show that BayesAct can replicate Interact as far as deflection minimisation goes, and can find low-deflection solutions for many examples, without requiring identities to be known and fixed. We have also done simulations where Equation (35)is used with a reward function that sets R x = 0 (see Equation (9)). These results do not show any significant differences, meaning that Equation (32) is sufficient for cases where R x = 0. Videos showing dynamics of the simulations can be seen at bayesact.ca.4.1.1. Static identitiesHere we explore the case where two agents know their own self identities (so β 0a= 0.001) but do not know the identity of the other agent (so β 0c is set to the standard deviation of all identities in the database). We run 20 trials, and in each trial a new identity is chosen for each of agent and client. These two identities are independently sampled from the distribution of identities in the ACT database and are the personal identities for each agent and client. That is, we compute the mean and covariance matrix of the 500 identities in the ACT database, and then sample identities from this distribution. Then, agentand client BayesAct models are initialised with Fa set to this personal identity, Fc (identity of the other) set to the mean of the identities in the database, [0.4, 0.4, 0.5]. Fb is set to zeros, but this is not important as it plays no role in the first update. The simulation proceeds according to the procedure in Section 3.5 for 50 steps. Agents take turns acting, and actions are conveyed to the other agent with the addition of some zero-mean normally distributed “environment” noise, with standard deviation σe . Other noise models would also be possible. Agents use Gaussian observation models with uniform covariances e ). We perform 10 simulations per trial with βc = 0.001 for both agent and client. with diagonal terms γ = max(0.52, σ 2−1/3 where N is the number of samples. We use id-deflection to denote the sum of All agents use roughening noise σr = Nsquared differences between one agent’s estimate of the other agent’s identity, and that other agent’s estimate of its own identity.11 See http :/ /www.indiana .edu /~socpsy /ACT. We used the Indiana 04-05 database for identities and behaviours and the USA 1978 equations for dynamics.150J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172−1/3, model Fig. 2. Deflections of identities from simulations with different numbers of samples (N), and environment noise, σe. Roughening noise: σr = Nenvironment noise: γ = max(0.5, σe). Left column: agent; right column: client. (a) id-deflection; (b) mean deflection; (c) max deflection; (d) shows samples (squares) and true identities (triangles) after 7 iterations for one trial.Fig. 2 shows a plot of the mean (over 20 trials) of the average (over 10 experiments) final (at the last step) id-deflectionand total deflection as a function of the environment noise, σe , and sample numbers, N, for one of the agents (the other is symmetric). Also shown are the average of the maximum total deflections for all experiments in a trial. We see that only about 50 samples are needed to get a solution that is robust to environment noise up to σe = 2.0. This corresponds to enough noise to make a behaviour of “apprehend” be mis-communicated as “confide in”.12 Further examples of behaviours for different levels of deflection are shown in Table C.3 (Appendix C). Surprisingly, deflection is not strongly affected by environment noise. One way to explain this is that, since the agent has a correct model of the environment noise (γ = σe ), it is able to effectively average the noisy measurements and still come up with a reasonably low deflection solution. The deterministic program Interact would have more trouble in these situations, as it must “believe” exactly what it gets (it has no model of environment noise).Average deflection does not change significantly with the sample size, whereas maximum deflection decreases (although not strictly monotonically). The deflections however have relatively high variance (e.g. 6.6 ± 2.7 for σe = 1.0 and N = 5), and so the maximum deflections would likely become a smoother function of sample size if the number of trials was increased. Table C.4 in Appendix C shows the full results with standard deviations.Fig. 2(d) shows a sample set after 7 iterations of one experiment, clearly showing the multimodal distributions centredaround the true identities (triangles) of each interactant.13 These sample sets normally converge to near the true identities after about 15 iterations or less.Fig. 3 looks more closely at four of the trials done with N = 200 samples. The red and blue lines show the agent- and client- id-deflection (solid) and agent and client deflections (dashed), respectively, while the black line shows the deflections using Interact (which has the correct and fixed identities for both agents at all times). BayesAct allows identities to change, and starts with almost no information about the identity of the other interactant (for both agent and client). We can see that our model gets at least as low a deflection as Interact. In Fig. 3(a), the agent had Fa = [2.7, 1.5, 0.9], and the clienthad Fa = [−2.1, −1.3, −0.2], and σe = 0 (noise-free communication). These two identities do not align very well,14 and result in high deflection when identities are known and fixed in Interact (black line). BayesAct rapidly estimates the correct identities, and tracks the deflection of Interact. Fig. 3(b) is the same, but with σe = 1.0. We see that BayesAct is robust to this level of noise. Fig. 3(c) shows a simulation between a “tutor” (Fa = [1.5, 1.5, −0.2]) and a “student” (Fc = [1.5, 0.3, 0.8]) with σe = 1.0. Here we see that Interact predicts larger deflections can occur. BayesAct also gets a larger deflection, but manages to resolve it early on in the simulation. Identities are properly learned in this case as well. Fig. 3(d) has the same identities as Fig. 3(c), but with σe = 5.0. We see that BayesAct is unable to find the true identity at this (extreme) level of noise.12 However, we are comparing the expected values of identities which may be different than any mode.13 See also videos at bayesact.ca.14 These identities are closest to “lady” and “shoplifter” for agent and client respectively, but recall that identity labels come from mapping the computed EPA vectors to concepts in ACT databases [7] and are not used by BayesAct.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172151Fig. 3. Closer look at four 200 sample experiments showing id-deflection (solid lines), total deflections (dashed lines) and deflection from Interact in dotted black (red = agent, blue = client). (a) agent: Fa = [2.7, 1.5, 0.9], client: Fa = [−2.1, −1.3, −0.2], σe = 0; (b) as (a) but with σe = 1.0; (c) agent: Fa =[1.5, 1.5, −0.2], client: Fc = [1.5, 0.3, 0.8], σe = 1.0; (d) same as (c) but with σe = 5.0. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)Fig. 4. (a) Number of frames (out of 200) where id-deflection is greater than dm = 1.0, 2.0 for continual shift identity experiments, for N = 100, T = 20, σe =0.5. (b) Fundamental identity sentiments Fe of client (green, dashed line) and agent’s estimate of client’s identity (red, solid line) for an example where the client shifts identities at a speed of sid = 0.25 and remains at each of two target identities for 40 steps. agent id is [0.32, 0.42, 0.65] and client ids are [−1.54, −0.38, 0.13] and [1.31, 2.75, −0.09].4.1.2. Dynamic (changing) identitiesWe now experiment with how BayesAct can respond to agents that change identities dynamically over the course of an interaction. We use the following setup: the client has two identities (chosen randomly for each trial) that it shifts between every 20 steps. It shifts from one to the other in a straight line in E–P–A space, at a speed of sid. That is, it moves a distance of sid along the vector from its current identity to the current target identity. It stops once it reaches the target (so the last step may be shorter than sid). It waits at the target location for T steps and then starts back to the original identity. It continues doing this for 200 steps. Our goal here is to simulate an agent that is constantly switching between two identities, but is doing so at different speeds. Table C.5 and Table C.6 in Appendix C show the full results for these simulations.We first show that BayesAct can respond to a single shift in identity after the first 20 steps (so after that, T = ∞). Fig. 4(a) shows the mean number of time steps per sequence of 200 steps in which the id-deflection of the agent’s estimate of the client’s identity is greater than a threshold, dm, for σe = 0.5. The results show that BayesAct is able to maintain a low id-deflection throughout the sequence when confronted with speeds up to about 0.1. At this setting (sid = σe = 0.1), only 4frames (out of 200) have an id-deflection greater than 1.0. Fig. 4(b) shows a specific example where the client shifts between two identities, for sid = 0.25 and T = 40. The agent’s estimates of Fe are seen to follow the client’s changes, although the agent lags behind by about 30 time steps.4.2. Intelligent interactive system examplesIn this section, we give two examples where BayesAct is used to expand intelligent interactive systems. These examples are presented primarily to demonstrate that BayesAct can be easily integrated into a range of different intelligent systems. We first discuss an exam practice assistant that presents students with questions from the graduate record examination, and 152J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172allows the student to respond affectively. The second example is a cognitive orthosis that can help a person with Alzheimer’s disease to wash their hands.Fig. 5. Tutoring interface screenshots.4.2.1. Exam practice applicationWe built a simple tutoring application in which the identities for agent and client are initially set to “tutor” (Fa =[1.5, 1.5, −0.2]) and “student” (Fc = [1.5, 0.3, 0.8]), respectively, with low dynamics standard deviations of βa = βc = 0.01and σr = 0.0 (see Section 3.3). Screenshots are shown in Fig. 5. The application asks sample questions from the Graduate Record Exam (GRE) educational testing system, and the client clicks on a multiple-choice answer. The agent provides feed-back as to whether the client’s answer is correct. The client then has the opportunity to “speak” by clicking on a labelled button (e.g., “awwww come on that was too hard!”). The statement maps to a value for Fb determined in an empirical survey described below (in this case [−1.4, −0.8, −0.5]). BayesAct then computes an appropriate agent action, i.e. a vector in EPA space, which maps to the closest of a set of statements elicited in the same survey (e.g., “Sorry, I may have been too demanding on you.”).The tutor has three discrete elements of state X = { Xd, Xs, Xt} where Xd is the difficulty level, Xs is the skill level of the student and Xt is the turn. Xd and Xs have 3 integer “levels” ({0, 1, 2}) where lower values indicate easier difficulty/lower skill. This is a simplified version of the models explored by Brunskill [88], and Theocharous [89]. We use this simpler version in order to focus our attention specifically on the affective reasoning components. The tutor’s model of the student’s = xs|xs, f, τ (cid:5)) = 0.9 with the remaining probability mass distributed evenly over skill levels that differ by progress is P ( X(cid:5) − τ (cid:5))2/2 and renormalised. As deflection 1 from xs. The dynamics for all values where Xgrows, the student is less likely to increase in skill level and more likely to decrease. Thus, skill level changes inversely proportionally to deflection. The tutor gets observations of whether the student succeeded/failed ((cid:3)x = 1/0), and has an observation function P ((cid:3)x| Xd, Xs) that favours success if Xd (the difficulty level) matches Xs (the skill level). The reward is the sum of the negative deflection as in Equation (9) and R x(x) = −(xs − 2)2. It uses the approximate policy given (Section 3.4) by Equation (35) for its affective response, and a simple heuristic policy for its propositional response where it gives an exercise at the same difficulty level as the mean (rounded) skill level of the student 90% of the time, and an exercise one difficulty level higher 10% of the time. Further optimisations of this policy as described in Section 3.4 would take into account how the student would learn in the longer term.≤ xs are then multiplied by (f(cid:5)s(cid:5)sWe thus require a specification of the POMDP, and two mappings, one from the combination of client statement button labels and difficulty levels to ACT behaviours (of client), and the other from ACT behaviours (of agent) to difficulty level changes and statements to the student. We conducted an empirical online survey of 37 participants (22 female) to establish these mappings. Full survey results are shown in Appendix D. Table 1 shows a few examples of the statements, along with the best behaviour label and the EPA values from the ACT database. The relationships between the statements and behaviour labels are very clear in most cases.We conducted a pilot experiment with 20 participants (7 female) who were mostly undergraduate students of engineer-ing or related disciplines (avg. age: 25.8). We compared the experiences of 10 users interacting with the BayesAct tutor with those of 10 users interacting with a control tutor whose affective actions were selected randomly from the same set as the BayesAct tutor. The control tutor is identical to the BayesAct tutor (it uses the same POMDP for estimating student skill level, deflection, etc.), except that it uses a policy for the affective component of its action (Ba) that is a random choice, rather than according to Equation (35).15 The control tutor uses the same heuristic policy for the propositional action (selection of difficulty level) as the BayesAct tutor.15 BayesAct used 500 samples, βa = βc = 0.01, and took 4 seconds per interaction on an AMD phenom II X4 955 3.20 GHz with 8 GB RAM running Windows 7, while displaying the words “Thinking...”. The random tutor simply ignored the computed response (but still did the computation so the time delay was the same) and then chose at random.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172153Table 1Some of the expressions elicited from the survey that were used in the tutoring application, along with the best behaviour label match and the EPA values from the survey.expressionclient expressions/client correctawwww come on that was toooo hard.bah – give me something harderYou are an amazing tutor.client expressions/client incorrectawwww come on that was toooo hard. Give me something easierWhatever, I’m still so proud of myselfStupid computer program!!You are not an effective teaching tool!agent expressions/client correctGreat going! Here’s another oneWow – amazing work. Here’s an easy one for youI really hope you are enjoying this exerciseagent expressions/client incorrectI wonder if you are just too stupid for thisCome on, a little more concentration, OK?Sorry, I may have been too demanding on youI don’t think you work hard enough. Reconsider your attitude!Can we do this a little quicker now?behaviourEwhine tobrag topraisebeseechdefendyell atcriticizeencouragerewardcare forinsultadmonishapologizeblamehurry−1.23−0.422.96−1.75−1.58−2.75−2.653.382.91.37−3.41−1.520.3−2.78−0.74P1.271.692.50.650.090.921.352.621.630.631.671.330.851.150.78A0.311.351.500.13−0.21−0.131.931.6−0.10.310.810.4811.07Table 2User study results (mean ratings). T has df = 18 and p is one-tailed. Scales ranged from 1 (= not true) to 5 (= true).survey questionBayesActRandomTpCommunicating with MathTutor was similar to communicating with a human.The tutor acted as if it understood my mood and feelings while I was solving the problems it gave me.I felt emotionally connected with MathTutor.I enjoyed interacting with MathTutor.MathTutor acted as if it knew what kind of person I am.MathTutor gave awkward and inappropriate responses to how I solved the problems (reverse coded)I found MathTutor to be flexible to interact with.Using MathTutor would improve my skills in the long term.The dialogue was simple and natural.Overall, I am satisfied with this system.2.102.802.503.302.903.443.003.403.502.701.702.002.003.102.114.501.902.002.001.700.871.921.000.441.55−2.702.182.493.142.34n.s.< 0.05n.s.n.s.< 0.10< 0.01< 0.05< 0.05< 0.01< 0.05Participants completed a short survey after using the system for an average of 20 minutes. Questions were rated on a scale from 1 (= not true) to 5 (= true). Results are displayed in Table 2. Users seemed to experience the flow of com-munication with the BayesAct tutor as more simple, flexible, and natural than with the random control tutor. The mean deflection for BayesAct was 2.9 ± 2.1 while for random it was 4.5 ± 2.2. We have to treat these results from a small sample with caution, but this pilot study identified many areas for improvement, and the results in Table 2 are encouraging.4.2.2. Cognitive assistantPersons with dementia (PwD, e.g. Alzheimer’s disease) have difficulty completing activities of daily living, such as hand-washing, preparing food and dressing. The short-term memory impairment that is a hallmark of Alzheimer’s disease leaves sufferers unable to recall what step to do next, or what important objects look like, for example. In previous work, we have developed a POMDP-based agent called the COACH that can assist PwD by monitoring the person and providing audio-visual cues when the person gets “stuck” [93]. The COACH is effective at monitoring and making decisions about when/what to prompt [92]. However, the audio-visual prompts are pre-recorded messages that are delivered with the same emotion each time.An important next step will be to endow the COACH with the ability to reason about the affective identity of the PwD, and about the affective content of the prompts and responses. Here we show how BayesAct can be used to provide this level of affective reasoning. Importantly, BayesAct can learn the affective identity of the client (PwD) during the interaction. Studies of identity in Alzheimer’s disease have found that identity changes dramatically over the course of the disease [100], and that PwD have more vague or abstract notions of their self-identity [101]. In this section, we describe our handwash-ing assistant model, and then show in simulation how BayesAct may be able to provide tailored prompting that fits each individual better.We use a model of the handwashing system with 8 plansteps corresponding to the different steps of handwashing, describing the state of the water (on/off), and hands (dirty/soapy/clean and wet/dry). An eight-valued variable PS describes the current planstep. There are probabilistic transitions between plansteps described in a probabilistic plan-graph (e.g. a PwD sometimes uses soap first, but sometimes turns on the tap first). We also use a binary variable AW describing if the 154J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172PwD is aware or not. In [92], we also had a variable describing how responsive a person is to a prompt. Here, we replace that with the current deflection in the interaction. Thus, X = {PS, AW} and the dynamics of the PS are• If the PwD is aware, then if there was no prompt from the agent, she will advance stochastically to the next planstep (cid:5)|PS = s,(according to the plan-graph) with a probability that is dependent on the current deflection, D. That is Pr(PSAW = yes, D = d) = f (d) where fis a function specified manually. If she does not advance, she loses awareness.• If the PwD is aware and is prompted and deflection is high, then a prompt will likely confuse the PwD (cause her aware-ness to become “no” if it was “yes”). Again, this happens stochastically according to a manually specified distribution f p(d).• If the PwD is not aware, she will not do anything (or do something else) with high probability, unless a prompt was given and the deflection is low, in which case she will follow the prompt and will gain awareness.We have done preliminary simulations with this model by using an agent identity of “assistant” (EPA = [1.5, 0.51, 0.45]), and an initial client identity of “patient” (EPA = [0.90, −0.69, −1.05]). The client knows the identity of the agent, but the agent must learn the identity of the client. Agent and client both know their own identities. We compare two types of policies: one where the affective actions are computed with BayesAct, and the other where the affective actions are fixed. In both cases, we used a simple heuristic for the propositional actions where the client is prompted if the agent’s belief about the client’s awareness (AW) falls below 0.4.We have found that a fixed affective policy may work well for some affective identities, but not for others, whereas the actions suggested by BayesAct work well across the different identities that the client may have. For example, if the client really does have the affective identity of a “patient”, then always issuing the prompts with an EPA = [0.15, 0.32, 0.06](the affective rating of the behaviour “prompt” in the ACT database) and otherwise simply “minding” the client (EPA =[0.86, 0.17, −0.16]) leads to the client completing the task in an equal number of steps as BayesAct, and always completing the task within 50 steps. However, if the client has an affective identity that is more “good” (EPA = [1.67, 0.01, −1.03], corresponding to “elder”) or more powerful (EPA = [0.48, 2.16, 0.94], corresponding to “boss”), then this particular fixed policy does significantly worse. Example simulations and more complete results are shown in Appendix E.The full development of the BayesAct emotional add-on to the existing COACH system will require substantial future work and empirical testing, as will fully developing an affectively intelligent tutoring system like the one described in the previous section. However, in light of the preliminary results reported here, we believe that BayesAct provides a useful theoretical foundation for such endeavours.5. Conclusions and future workThis paper has presented a probabilistic and decision theoretic formulation of affect control theory called BayesAct, and has shown its use for human interactive systems. The paper’s main contributions are the theoretical model development, and a demonstration that a computational agent can use BayesAct to integrate reasoning about emotions with application decisions in a parsimonious and well-grounded way.Overall, our model uses the underlying principle of deflection minimisation from affect control theory to provide a general-purpose affective monitoring, analysis and intervention theory. The key contributions of this paper are1. A formulation of affect control theory as a probabilistic and decision theoretic model that generalises the original presentation in the social psychological literature in the following ways:(a) it makes exact predictions of sentiment dynamics using the equations of affect control theory, generalising the partial updates of ACT,(b) it removes the assumption that identities are fixed through time and allows an agent to model a changing identity,(c) it removes the assumption that sentiments (of identities and behaviours) are known exactly by modelling them as probability distributions.A thorough discussion of the theoretical significance of BayesACT for sociology is available elsewhere [102].2. A set of simulations that demonstrate some of the capabilities of this generalised model of ACT under varying environ-mental noise.3. A formulation of a general-purpose model for intelligent interaction that augments the model proposed by BayesAct in the following ways:(a) It adds a propositional state vector that models other events occurring as a result of the interaction, and models this state vector’s progression as being dependent on the affective deflection of the interaction.(b) It adds a reward function that an agent can optimise directly, allowing an agent to combine deflection minimisation with goal pursual in a parsimonious and theoretically well-grounded way.4. Demonstrative examples of building two simple intelligent interactive systems (a tutor and an assistive agent for per-son’s with a cognitive disability) that use the proposed model to better align itself with a user.Our current work is investigating methods for learning affective dictionaries automatically from text [99], and on im-plementing hierarchical models of identity [103]. In future, the measurement of EPA behaviours and the translation of EPA J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172155actions requires further study. We also plan to investigate usages of the model for collaborative agents in more complex domains, for competitive, manipulative or therapeutic agents, conversational agents, social networks, and for social simu-lations where we have more than two agents acting. Emotions have been shown to be important for decision making in general [41,73], and we believe that BayesAct can play a significant role in this regard. We also plan to investigate methods for automatically learning the parameters of the prediction equations, and the identity labels. This would allow longer-term learning and adaptation for agents. Finally, we plan to investigate how to handle more complex time structures in BayesAct, including interruptions.A BayesAct agent uses the affect control principle to make predictions about the behaviours of the agents it interacts with. This principle states that humans will act to minimise the deflection between their culturally shared fundamental (learned and slowly changing) affective sentiments and the transient sentiments created by specific events and situations. In situations where both agents follow this principle, and know each other’s affective identities, the agents do not have to compute long-term predictions: they can simply assume the predictions of the affect control principle are correct and act accordingly. A breakdown occurs if these predictions no longer hold.There are three types of breakdown. The first is simple environmental noise. However, we have seen that, alone, this does not have a significant effect. Agents essentially “ignore” other agents in the presence of environmental noise, if they can assume the other agents are following the affect control principle. In combination with the other two breakdown types, it can have a much more significant effect. The second type of breakdown can occur if one agent does not know the identity of the other agent. We have investigated this situation in detail in this paper in simulation, and found that BayesActagents can learn the affective identities of other BayesAct agents under significant environmental noise. Finally, the third type of breakdown is when one agent is deliberately trying to manipulate the other. In such cases, both agents must do more complex policy computation, as the predictive power of the affect control principle no longer holds. Computing these policies for such manipulative agents is a significant area for future work [9].For example BayesAct agents are free to use X to model other agents at any level of detail (including as full POMDPs [104]). Such more complex modelling will allow agents to reason about how other agents are reasoning (cog-nitively) about them, etc. Nevertheless, even such cognitively capable agents will need to follow the norms of the society they are trying to manipulate, otherwise other agents will be unlikely to respond predictably.In this regard, it is interesting to note that the default (normative) policy specified by Equation (32) may correspond roughly with what behavioural economists have called fast or “System 1” thinking [8]. The Monte-Carlo method for forward search can be set up to explore only actions that are nearby to this default action for each state, providing the agent with a quick-and-dirty method for quickly finding reasonable policies that will be socially acceptable or normative. In a resource limited agent, this type of fast thinking may be just enough to “get by”. Given enough time or sufficient cognitive resources, and agent may then resort to slow (“System 2”) thinking, and explore (in simulation) actions that are further away from the default. This slow thinking can lead an agent to discover slightly non-normative actions that lead to higher self reward, without giving away the fact (so remaining close enough to the normative default). The opens the door for building effective manipulative agents [9].In a similar vein, the theory of social commitments [105] argues that human relationships combine relational (affec-tive) and transactional (rational) components. Lawler argues that modern society, becoming more and more influenced by economics and individualism, is moving towards an ecology of transactional ties, creating shallow, brittle and more danger-ous (for the species) social structures [105]. His analysis carefully explores the space between these two views, and gives guidelines and arguments for how group processes can be built to take advantage of both. BayesAct also combines relational and transactional ideas, by arguing that the core driving force behind human behaviour is relational and based on shared affective sentiments about identities and behaviours. However, cognitive transactional processes come into play when rela-tions break down. Transactional relationships are set up to handle the breakdowns, but are rapidly integrated into relational processes if they are required to maintain the social order.These considerations of breakdown lead to a tantalizing avenue for future research. One way to handle breakdowns would be to increase the size of the (non-affective) state vector (denoted X in this paper). Additional values of X would be needed in order to better predict when the breakdowns occur and what the effects are. For example, an agent could learn what situations caused another agent to change identities, or could learn what types of identities are present in certain situations (called “settings” in ACT [1]). Such an increase of X is a creation of new “meaning” in an agent [106,107]. These new meanings would need to be validated with other agents, a process of negotiation attempting to get back to the easy state of “flow” where less reasoning is required [108]. A learning paradigm that is fundamentally based on affective reasoning would therefore arise. Such a paradigm has been discussed as fundamental in the phenomenological view of intelligence [107,106].AcknowledgementsWe would like to thank our study participants. We thank Pascal Poupart and Cristina Conati for helpful discussions and comments. We acknowledge funding from the Natural Sciences and Engineering Research Council of Canada (J. Hoey), DFG research fellowship #SCHR1282/1-1 (T. Schröder). A. Alhothali is supported by a grant from King Abdulaziz University of Saudi Arabia.156J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172Appendix A. Derivation of most likely behaviourWe know from Equations (10) and (12) that the belief distribution over the state at time t (denoted sb(s(cid:5)) = Pr(ω(cid:5)|s(cid:5))Eb(s)Pr(x(cid:5)(cid:5)|x, f, τ (cid:5)(cid:5), ba)Pr(τ (cid:5)|τ , f, x)Pr(f(cid:9)(cid:10)(cid:5)|f, τ , x, ba)and further, from Equation (22) thatPr(f(cid:5)|f, τ , x, ba, ϕ) ∝ e−ψ(f(cid:5),τ ,x)(cid:13)(cid:12)EPr(θ f )(θ f )(cid:5)) is given by(A.1)(A.2)is uninformative, and so only the first expectation remains. Then, if we compare (cid:5)2, and we imagine that we have deterministic dynamics for the application state X and the (cid:5)Let us first assume that the prior over θ ftwo values for s, say stransients T, then we find(cid:5)1) − b(s(cid:5)1 and s(cid:15)e−ψ(fb(s1,s1) − e(cid:5)(cid:16)−ψ(f(cid:5)2,s2)(cid:5)2) ∝ Eb(s)−ψ(f≥ e(cid:5)1,Eb(s)(s1)) − e−ψ(f(cid:5)2,Eb(s)(s2))(A.3)(A.4)where the inequality between (A.3) and (A.4) is due to the expectation of a convex function being always larger than the (cid:5)1 will be greater than the function of the expectation (Jensen’s inequality). From (A.4), we have that the probability of f(cid:5)2 if and only if:probability of f(cid:5)(cid:5)2, Eb(s)(s2))1, Eb(s)(s1)) < ψ(fψ(f(cid:5)(cid:5)that is, the deflection caused by f2. This demonstrates that our probability measure 1 is less than the deflection caused by f(cid:5)will assign higher likelihoods to behaviours with lower deflection, as expected, and so if we wish find the most over f(cid:5)likely fvalue, we have only to find the value that gives the smallest deflection by, e.g., taking derivatives and setting equal to zero. The probabilistic formulation in Equation (A.5), however, takes this one step further, and shows that the probability (cid:5)of fwill assign higher weights to behaviours that minimize deflection, but in expectation of the state progression if it is not fully deterministic.If the prior over θ fis such that we expect identities to stay constant over time, as in Equation (25), we can derive a (A.5)similar expression to (A.5), except it now includes the deflections of the fundamentals over identities:(cid:5)(cid:5)(cid:5)(cid:5)2, Eb(s)(s2)) + ξ(f1, Eb(s)(s1)) + ξ(f2, f)1, f) < ψ(fψ(f(A.6)We see that this is now significantly different than (A.5), as the relative weights of ξ (βa and βc ) and ψ (α) will play a large role in determining which fundamental sentiments are most likely. If βa (cid:16) α or βc (cid:16) α, then the agents beliefs about identities will change more readily to accommodate observed deflections. If the opposite is true, then deflections will be ignored to accommodate constant identities.Appendix B. Reduction to ACT behavioursIn this section, we show that the most likely predictions from our model match those from [1] if we use the same ap-proximations. We begin from the probability distribution of fundamentals from Equation (25), but we assume deterministic state transitions, ignore the fundamental inertia ξ , and use the formula for ψ as given by Equation (28), we getPr(f(cid:5)|f, τ , x, ba, ϕ) ∝ e−(f(cid:5)−K −1C )T K T (cid:4)−1K (f(cid:5)−K −1C )(B.1)Now we saw in Section 3.3 that this was simply a Gaussian with a mean of K −1C , and so gives us the expected (most likely or “optimal” the terms of [1]) behaviours and identities simultaneously. We can find these expected fundamentals by taking the total derivative and setting to zero(cid:17)(cid:18)(cid:5) − K −1C )(cid:5)|f, τ , x, ba, ϕ) =ddf(cid:5) Pr(f(cid:5) = K −1C (the mean of the Gaussian), as expected. ACT, however, estimates the derivatives of each of which means that fthe identities and behaviours separately assuming the others are held fixed. This is the same as taking partial derivatives of (B.1) with respect to fb only while holding the others fixed:(cid:5) − K −1C )T K T (cid:4)−1K (f(cid:5)−K −1C )T K T (cid:4)−1K (f(cid:5)−K −1C ) = 0ddf(cid:5) (f−(fe(cid:5) − K −1C )T K T (cid:4)−1K (f(f−(f(cid:5)−K −1C )T K T (cid:4)−1K (f(cid:5)−K −1C ) = 0e(B.2)(cid:18)(cid:5) − K −1C )(cid:17)∂(cid:5)∂fbNow, we recall that (writing I ≡ I 3 and 0 ≡ 03):⎡K =⎣⎤⎦I −Ha00 1 − Hb 00 −HcIJ. Hoey et al. / Artificial Intelligence 230 (2016) 134–172157So that⎡⎤⎣K −1 =I Ha(1 − Hb)−1 0(1 − Hb)−1000 Hc(1 − Hb)−1Iand if (cid:4) is a diagonal identity matrix, we can write⎦⎡K T (cid:4)−1K =⎣I−Ha H 2a0+ (1 − Hb)2 + H 2c−Ha−Hc⎤⎦0−HcITo simplify, we let a = −Ha, b = 1 − Hb, c = −Hc , and z = H 2a⎤⎡+ (1 − Hb)2 + H 2c we getK T (cid:4)−1K =⎣a 0IcazI0 c⎦we also have thatf − K −1C =⎡⎣fa −fc −(cid:19)Ca + Ha(1 − Hb)−1Cbfb − (1 − Hb)−1Cb(cid:19)Cc + Hc(1 − Hb)−1Cb(cid:20)⎤⎡⎦ =⎣(cid:20)⎤⎦yaybycwhere we have used ya, yb, yc to denote the difference between the actor identity, behaviour and object identity and their respective true means as given by the total derivative. Therefore, we have from Equation (B.2):(cid:16)(cid:15)∂∂fby2a+ 2aya yb + zy2b+ 2c yb yc + y2c= 02aya + 2zyb + 2c yc = 0and therefore that the “optimal” behaviour, f−1Cb−1(aya + c yc) + (1 − Hb)= −z∗fb∗b ispartially expanding out this is∗fb= −z−1(cid:12)−Ha(fa − Ca − CbHa(1 − Hb)−1) − Hc(fc − Cc − CbHc(1 − Hb)−1) − zCb(1 − Hb)−1(cid:13)(B.3)Now we note that, the terms from [1] can be written as followsfa − Ca−Cbfc − CcI −MI β gβ=⎡⎣⎤⎦(cid:10)(cid:9)(cid:5)and(cid:9)(cid:10)(cid:5)I −MI β S β =⎤⎦⎡⎣HaI − HbHcso that(cid:21)S Tβ I βI−M(cid:5)(cid:22)(cid:9)(cid:10)(cid:5)I −MI β gβ= −Ha(fa − Ca) − Cb(I − Hb) − Hc(fc − Cc)(B.4)and that(cid:17)(cid:21)S Tβ I βI−M(cid:5)∗b is nowso that f(cid:17)(cid:22)(cid:9)(cid:21)(cid:10)(cid:5)I −MI β S β(cid:18)−1(cid:15)=H TaHa + (I − Hb)T (I − Hb) + H TcHc(cid:16)−1−1= zS Tβ I βI−M(cid:22)(cid:9)(cid:5)(cid:10)(cid:5)I −MI β S β(cid:18)−1(cid:12)−Ha(fa − Ca) + HaCbHa(1 − Hb)(cid:15)∗fb= −×−H TaHa + (1 − Hb)T (1 − Hb) + H TcHcCb(1 − Hb)(B.5)−1 − Hc(fc − Cc) + HcHbHc(1 − Hb)−1(cid:16)(cid:13)−1158J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172Table C.3Most different behaviour pairs with a Euclidean distance less than σr , the environment noise. A – indicates that there are no behaviours that are closer than the value of σr indicated.σr≤ 0.02id1–0.050.050.050.10.10.10.10.10.20.20.20.20.20.50.50.50.50.51.01.01.01.01.02.02.02.02.02.05.05.05.05.05.0≥ 10.0≥ 10.0≥ 10.0≥ 10.0≥ 10.0quarrel withhoot atcriticizemake business proposal towhipcajoleworkbashcommandmake eyes atlook atsueask outeat withshout atmedicatebullyrestrainborrow money fromjoin up withcriticizesuenuzzlemassagedressmindapprehendharassdenouncecollaborate withhuglisten toeducatesteal fromsteal fromsteal fromsteal fromsteal fromid2–quibble withstriphushbackbiteseduceoverwhelmdistracttackleconfess todraw near tospankapproachsuggest something toknock outcaresshasslecontradictpeek atshow something toribfineconvictthankconsoleaccommodateconfide inknock outcare forkillscoff atabandonpestermake love tosexually arousehelpsavegive medical treatment to|id1 − id2|–0.0460.0330.0280.0990.0990.0950.0950.0930.200.200.200.200.200.500.500.500.500.501.01.01.01.01.02.02.02.02.02.05.05.05.05.05.07.77.67.57.47.4collecting terms and comparing to Equation (B.4), this give us exactly Equation (12.21) from [1]:(cid:18)−1(cid:17)(cid:22)(cid:21)(cid:21)(cid:22)∗fb= −S Tβ I β(cid:9)I−M(cid:5)(cid:10)(cid:5)I −MI β S βS Tβ I β(cid:9)(cid:10)(cid:5)I −MI−M(cid:5)I β gβ(B.6)Similar equations for actor and object identities can be obtained in the same way by computing with partial derivatives keeping all other quantities fixed, and the result is equations (13.11) and (13.18) from [1].Appendix C. Tabulated simulation resultsTable C.3 shows examples of behaviours for different levels of deflection. Each row shows the two behaviour labels and their actual id-deflection. The first column shows the maximum id-deflection searched for.We explore three conditions in our simulations. In the first two, the agent does not know the identity of the client, and the client either knows or doesn’t know the identity of the agent (denoted agent id known and agent id hidden, resp.). In the third case, agent and client know each other’s identities (denoted both known). We run 20 trials, and in each trial a new identity is chosen for each of agent and client. These two identities are independently sampled from the distribution of identities in the ACT database and are the personal identities for each agent and client. Then, agent and client BayesActmodels are initialised with Fa set to this personal identity, Fc (identity of the other) set to either the true identity (if known) or else to the mean of the identities in the database, [0.4, 0.4, 0.5]. Fb is set to zeros, but this is not important as it plays no role in the first update. Table C.4 shows the mean (over 20 trials) of the average (over 10 experiments) final (at the last Table C.4Deflections of identities from simulations with different numbers of samples (N), and environment noise, σe. Roughening noise: σr = Nwhere the identity is known are not shown as they are all very small (less than 10−3).−1/3, model environment noise: γ = max(0.5, σe). id-deflections in cases σeNboth knownagent id knownagent id hiddendeflectionmax deflectionid-deflectiondeflectionmax deflectionid-deflectiondeflectionmax deflection0.00.00.00.00.00.00.00.010.010.010.010.010.010.010.050.050.050.050.050.050.050.10.10.10.10.10.10.10.50.50.50.50.50.50.51.01.01.01.01.01.01.0510501002505001000510501002505001000510501002505001000510501002505001000510501002505001000510501002505001000agent3.2 ± 2.93.1 ± 3.13.3 ± 2.55.1 ± 6.64.5 ± 4.33.6 ± 2.64.3 ± 3.14.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 3client3.2 ± 2.93 ± 3.13.3 ± 2.55.1 ± 6.64.5 ± 4.33.6 ± 2.64.3 ± 3.14.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 3agent13.9411.4813.9033.2321.2812.0412.3912.3315.7325.0814.6913.6016.4511.4312.3315.7325.0814.7013.6016.4211.4312.3015.7325.0914.6913.5916.4011.4412.3315.8825.1014.7513.7116.4711.5212.3215.8825.0614.7013.6616.4511.48client14.0011.5213.9333.2221.2912.0812.3912.4315.7825.0514.6213.6416.3811.4612.3915.8025.0614.6413.6516.3811.4712.4015.9225.0814.6513.6416.4111.4712.4415.9525.2014.8413.6516.5211.5212.4115.8725.1114.7113.6316.4311.49agent0.89 ± 0.420.26 ± 0.140.2 ± 0.680.11 ± 0.260.12 ± 0.490.065 ± 0.190.046 ± 0.120.74 ± 0.160.33 ± 0.20.12 ± 0.260.11 ± 0.190.14 ± 0.310.066 ± 0.190.025 ± 0.0670.89 ± 0.460.33 ± 0.190.098 ± 0.160.11 ± 0.20.17 ± 0.370.074 ± 0.20.042 ± 0.120.76 ± 0.240.38 ± 0.340.068 ± 0.0830.11 ± 0.150.22 ± 0.490.064 ± 0.160.044 ± 0.141.2 ± 0.460.78 ± 0.260.27 ± 0.0920.26 ± 0.180.26 ± 0.250.15 ± 0.130.11 ± 0.0833.8 ± 1.82 ± 1.20.82 ± 0.480.83 ± 0.60.61 ± 0.590.49 ± 0.330.42 ± 0.52agent4.2 ± 2.14.3 ± 33.6 ± 3.24.2 ± 3.54.2 ± 3.73.7 ± 2.84.5 ± 2.73.2 ± 1.94 ± 2.73.8 ± 24.4 ± 3.24.3 ± 3.13.8 ± 1.83.5 ± 2.93.1 ± 1.74 ± 2.73.7 ± 24.4 ± 3.34.3 ± 33.8 ± 1.93.5 ± 2.93.1 ± 1.64.1 ± 2.73.8 ± 24.4 ± 3.44.4 ± 3.13.7 ± 1.83.5 ± 2.93.3 ± 1.84.3 ± 2.93.7 ± 24.4 ± 3.44.3 ± 3.13.7 ± 1.83.5 ± 2.94.6 ± 1.74.8 ± 33.9 ± 24.5 ± 3.44.4 ± 33.9 ± 1.73.5 ± 2.7client4 ± 2.54 ± 3.13.5 ± 3.24 ± 3.54 ± 3.73.6 ± 2.94.4 ± 2.72.6 ± 1.73.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 32.6 ± 1.73.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 32.6 ± 1.73.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 32.6 ± 1.73.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 33 ± 23.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 3agent20.6125.6825.9623.4719.4214.5712.0630.3519.6923.2326.0316.3819.9812.4930.3216.7317.5626.0517.5519.3512.7228.8118.4618.0726.0618.1419.7412.7121.8819.1123.0625.0718.5119.1621.5032.2821.8918.0827.9019.3920.5821.57client10.9515.4613.9915.1816.3414.5310.466.6211.228.3214.7312.1210.3210.576.5911.198.3314.7212.1310.3210.576.5911.208.3414.7212.1310.3510.596.6211.268.3614.7612.2310.4310.619.6111.288.3614.7512.1510.3810.60agent0.88 ± 0.460.43 ± 0.390.22 ± 0.420.12 ± 0.160.14 ± 0.30.093 ± 0.230.038 ± 0.121.1 ± 0.70.55 ± 10.2 ± 0.360.086 ± 0.140.051 ± 0.0960.21 ± 0.650.083 ± 0.281.3 ± 1.10.53 ± 0.830.17 ± 0.360.12 ± 0.20.051 ± 0.0860.18 ± 0.540.079 ± 0.261.2 ± 0.880.48 ± 0.750.2 ± 0.370.11 ± 0.170.053 ± 0.0760.2 ± 0.640.08 ± 0.251.5 ± 0.541 ± 0.70.42 ± 0.370.28 ± 0.150.23 ± 0.240.29 ± 0.510.14 ± 0.154.2 ± 2.32.4 ± 1.11 ± 0.570.76 ± 0.390.63 ± 0.490.57 ± 0.480.4 ± 0.21client1.1 ± 0.440.56 ± 0.710.34 ± 0.760.093 ± 0.130.04 ± 0.0640.057 ± 0.140.025 ± 0.0531.1 ± 0.350.59 ± 0.640.19 ± 0.530.16 ± 0.320.034 ± 0.0570.071 ± 0.150.044 ± 0.111.3 ± 0.730.48 ± 0.430.22 ± 0.630.18 ± 0.380.043 ± 0.0890.069 ± 0.150.051 ± 0.141.3 ± 0.620.57 ± 0.560.15 ± 0.260.17 ± 0.330.043 ± 0.0770.079 ± 0.170.035 ± 0.0711.7 ± 0.50.96 ± 0.590.36 ± 0.380.35 ± 0.350.2 ± 0.170.21 ± 0.210.13 ± 0.0633.8 ± 1.32.2 ± 0.980.88 ± 0.570.92 ± 0.720.5 ± 0.210.57 ± 0.310.44 ± 0.22agent3.8 ± 2.53.6 ± 2.63.7 ± 2.22.9 ± 1.73.7 ± 3.33.6 ± 3.23.7 ± 3.43.9 ± 2.35.6 ± 4.13.4 ± 1.83 ± 1.42.9 ± 1.74.7 ± 2.63.6 ± 2.34.2 ± 2.65.8 ± 4.23.5 ± 1.82.9 ± 1.42.9 ± 1.74.7 ± 2.53.6 ± 2.34 ± 2.45.5 ± 3.93.5 ± 1.92.9 ± 1.42.9 ± 1.74.8 ± 2.63.6 ± 2.33.5 ± 1.95.1 ± 3.63.5 ± 1.83 ± 1.43 ± 1.74.8 ± 2.43.7 ± 2.35.3 ± 2.26 ± 3.73.9 ± 1.73.2 ± 1.33 ± 1.45.1 ± 2.73.7 ± 2.2client4.2 ± 2.33.8 ± 2.33.9 ± 2.23 ± 1.93.7 ± 3.33.8 ± 3.33.6 ± 3.34.5 ± 2.86.1 ± 43.6 ± 1.83 ± 1.43.1 ± 1.85.1 ± 3.33.8 ± 2.34.6 ± 2.75.9 ± 3.73.7 ± 1.83.1 ± 1.53.1 ± 1.85.1 ± 3.33.8 ± 2.34.7 ± 2.76.2 ± 4.13.6 ± 1.93.1 ± 1.63.1 ± 1.85.1 ± 3.33.8 ± 2.34.1 ± 1.85.2 ± 3.63.7 ± 1.83.1 ± 1.63.2 ± 1.95.1 ± 3.33.8 ± 2.46.6 ± 2.76.3 ± 2.94.2 ± 1.83.4 ± 1.83.5 ± 2.15.4 ± 3.24.1 ± 2.5agent25.6129.2116.0013.3523.6515.7317.3028.4532.6314.9312.2211.9815.469.9128.5232.5819.9814.3010.1214.8010.2031.1032.5219.7615.6811.8714.9614.5823.4231.4922.5316.1811.8819.2315.2437.9731.6525.6218.8012.9120.6218.67client22.6923.4922.2912.4119.4623.7221.0731.5941.2321.7416.8110.9223.1611.6131.4141.2322.3417.1511.0523.1911.8035.9441.2322.4716.9912.0523.2312.0626.6228.4316.9320.2617.9622.2614.3236.3844.6028.2319.1924.5426.6016.60(continued on next page)J.Hoeyetal./ArtificialIntelligence230(2016)134–172159Table C.4 (continued)σeNboth knownagent id knownagent id hiddendeflectionmax deflectionid-deflectiondeflectionmax deflectionid-deflectiondeflectionmax deflection2.02.02.02.02.02.02.05.05.05.05.05.05.05.010.010.010.010.010.010.010.0agent4.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.4 ± 2.54.2 ± 3.54.3 ± 3client4.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 34.2 ± 3.14 ± 3.75 ± 4.84 ± 3.63.3 ± 2.54.2 ± 3.54.3 ± 3agent12.3215.7825.0514.6613.6316.4211.4412.2915.7225.1114.6613.6316.4311.4412.3215.7225.1214.6613.6216.4411.44client12.4115.8925.1114.6213.6616.4111.4812.4015.8025.0814.6013.6516.3911.4712.3915.7825.0714.6013.6416.3911.47agent7.6 ± 3.25.4 ± 3.12.3 ± 1.42.2 ± 1.11.6 ± 0.971.4 ± 0.621.1 ± 0.5919 ± 5.214 ± 56.5 ± 2.65.1 ± 1.64 ± 1.23.8 ± 1.23 ± 0.9526 ± 5.819 ± 712 ± 5.28.1 ± 2.86.3 ± 2.25.7 ± 2.14.5 ± 2.1510501002505001000510501002505001000510501002505001000agent6.5 ± 35.6 ± 3.34.1 ± 1.95.2 ± 3.44.7 ± 2.74.1 ± 1.63.8 ± 2.49.3 ± 37.2 ± 3.15.4 ± 1.86 ± 2.35.9 ± 2.75.4 ± 1.64.9 ± 2.111 ± 4.58.2 ± 3.67.7 ± 3.47 ± 2.37 ± 2.56.3 ± 1.46.1 ± 1.8client3 ± 23.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 33 ± 23.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 33 ± 23.6 ± 2.73.5 ± 24.2 ± 3.34.1 ± 3.33.6 ± 1.93.4 ± 3agent57.9453.6625.9033.7623.7621.5623.6968.7239.9728.7728.6929.1719.1122.5167.7041.5962.1238.4430.9619.8420.38client9.5611.278.3414.7412.1410.3410.599.5211.238.3414.7412.1310.3310.589.5411.238.3414.7312.1310.3310.57agent9.5 ± 3.85.9 ± 2.72.4 ± 0.932 ± 0.641.6 ± 0.561.4 ± 0.931.1 ± 0.3719 ± 6.115 ± 4.56.9 ± 2.25.1 ± 24.4 ± 1.53.5 ± 1.43.2 ± 127 ± 7.422 ± 5.411 ± 3.48.4 ± 2.45.8 ± 2.35.3 ± 2.34.4 ± 2client8.8 ± 2.55.7 ± 1.92.1 ± 0.912.1 ± 0.921.4 ± 0.381.7 ± 0.71.3 ± 0.4320 ± 4.813 ± 3.86.7 ± 2.55 ± 1.74 ± 0.883.5 ± 1.23.2 ± 1.128 ± 9.319 ± 6.410 ± 3.17.9 ± 2.55.5 ± 1.45.4 ± 2.34.4 ± 1.5agent7.1 ± 2.26.7 ± 3.84.6 ± 1.93.8 ± 1.43.4 ± 1.45.6 ± 2.54.2 ± 2.48.9 ± 2.58.4 ± 3.76.3 ± 1.75 ± 1.54.7 ± 1.56.6 ± 2.35.3 ± 2.211 ± 2.910 ± 4.57.2 ± 1.66.5 ± 1.85.5 ± 1.77.5 ± 2.45.9 ± 1.9client8 ± 37.7 ± 2.95.1 ± 2.34 ± 1.64.4 ± 2.55.9 ± 3.24.7 ± 2.611 ± 3.59.7 ± 3.26.1 ± 1.35.5 ± 1.66.2 ± 2.67 ± 2.46.3 ± 2.513 ± 4.311 ± 3.77.4 ± 1.97 ± 1.97 ± 2.17.3 ± 1.47.5 ± 2.2agent36.8439.8332.7720.8225.0725.2223.6951.9365.4341.3527.7523.3231.4720.6158.0375.0936.9026.8718.6429.6818.74client42.8148.7830.8520.9634.3624.2221.9177.5558.7132.8926.2124.3824.3124.6564.2347.1434.2230.4921.7223.5525.56160J.Hoeyetal./ArtificialIntelligence230(2016)134–172J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172161Table C.5Deflections of identities from simulations with different environment noise, σe, and shapeshifted id speed, sid. N = 250, agent-id-hidden, dm ≡ threshold for frame deflection.σe0.10.10.10.10.10.50.50.50.50.51.01.01.01.01.0sidid-deflectionagent0.065 ± 0.150.16 ± 0.130.51 ± 0.540.51 ± 0.970.14 ± 0.330.17 ± 0.060.31 ± 0.160.69 ± 0.480.71 ± 1.10.3 ± 0.320.41 ± 0.240.94 ± 0.991.1 ± 0.741.1 ± 1.20.63 ± 0.51client (×103)0.28 ± 0.140.27 ± 0.130.28 ± 0.140.28 ± 0.10.29 ± 0.120.31 ± 0.140.3 ± 0.130.31 ± 0.110.3 ± 0.140.29 ± 0.150.28 ± 0.130.31 ± 0.160.28 ± 0.120.27 ± 0.110.29 ± 0.130.010.10.51.02.00.010.10.51.02.00.010.10.51.02.0deflectionagent3.1 ± 2.33.6 ± 2.43.2 ± 2.33.2 ± 2.33.4 ± 2.43.1 ± 2.33.6 ± 2.43.2 ± 2.23.2 ± 2.23.3 ± 2.33 ± 23.3 ± 2.13.1 ± 2.13.2 ± 2.23.3 ± 2.3client3 ± 2.43.9 ± 2.73.4 ± 2.43.5 ± 2.53.5 ± 2.53 ± 2.43.9 ± 2.73.5 ± 2.53.5 ± 2.53.5 ± 2.53 ± 2.43.9 ± 2.73.5 ± 2.53.5 ± 2.53.5 ± 2.5num deflected framesdm = 1.04.12 ± 9.095.14 ± 12.0318.11 ± 18.3716.80 ± 18.4710.36 ± 12.56dm = 2.00.53 ± 0.960.94 ± 5.711.94 ± 4.523.69 ± 7.142.50 ± 4.726.20 ± 6.599.89 ± 13.7523.99 ± 20.2921.87 ± 19.5415.20 ± 15.4820.70 ± 19.4435.76 ± 23.9841.83 ± 23.0538.22 ± 24.1431.57 ± 20.700.84 ± 1.521.28 ± 5.692.83 ± 5.404.96 ± 9.273.21 ± 6.313.23 ± 7.134.32 ± 10.788.47 ± 11.469.12 ± 13.406.37 ± 10.64dm = 3.00.04 ± 0.200.04 ± 0.200.19 ± 1.530.35 ± 1.320.60 ± 2.160.06 ± 0.290.10 ± 0.570.18 ± 1.030.74 ± 2.680.72 ± 2.520.27 ± 1.140.69 ± 4.560.91 ± 2.921.82 ± 5.331.23 ± 4.22dm = 5.00.00 ± 0.000.00 ± 0.000.00 ± 0.000.00 ± 0.000.04 ± 0.490.00 ± 0.000.00 ± 0.000.00 ± 0.000.00 ± 0.000.03 ± 0.350.00 ± 0.000.00 ± 0.000.00 ± 0.000.01 ± 0.140.01 ± 0.07Table C.6Deflections of identities from simulations with different environment noise, σe , and shapeshifted id speed, sid. N = 100, agent–id-hidden, dm ≡ threshold for frame deflection.σe0.10.10.10.10.10.50.50.50.50.51.01.01.01.01.0sidid-deflectionagent0.048 ± 0.0510.16 ± 0.0940.74 ± 0.80.75 ± 10.37 ± 0.940.19 ± 0.070.32 ± 0.140.88 ± 0.890.94 ± 11.2 ± 1.80.43 ± 0.170.66 ± 0.231.3 ± 0.991.3 ± 1.11 ± 0.99client (×103)0.88 ± 0.450.82 ± 0.340.83 ± 0.390.85 ± 0.40.76 ± 0.510.94 ± 0.370.86 ± 0.350.84 ± 0.41 ± 0.461.1 ± 0.80.92 ± 0.420.91 ± 0.40.92 ± 0.470.88 ± 0.440.86 ± 0.420.010.10.51.02.00.010.10.51.02.00.010.10.51.02.0deflectionagent2.9 ± 1.72.9 ± 1.63.1 ± 1.43.1 ± 1.52.8 ± 1.92.9 ± 1.73 ± 1.63 ± 1.53 ± 1.63.5 ± 2.42.9 ± 1.73 ± 1.63 ± 1.43.1 ± 1.53.2 ± 1.5client2.9 ± 1.82.9 ± 1.63.3 ± 1.73.4 ± 1.62.9 ± 2.12.9 ± 1.82.9 ± 1.63.3 ± 1.73.4 ± 1.74.1 ± 3.12.9 ± 1.82.9 ± 1.63.3 ± 1.73.5 ± 1.73.7 ± 2num deflected framesdm = 1.03.60 ± 5.424.31 ± 6.8244.06 ± 44.0540.98 ± 46.9022.91 ± 36.32dm = 2.00.64 ± 0.890.63 ± 0.865.61 ± 14.759.68 ± 23.765.87 ± 15.397.80 ± 8.5612.84 ± 12.7154.06 ± 44.1849.47 ± 48.1848.40 ± 59.1626.59 ± 23.4157.30 ± 31.3889.39 ± 46.5679.89 ± 50.7159.55 ± 45.361.31 ± 2.941.39 ± 3.558.07 ± 17.3211.62 ± 24.8618.09 ± 32.082.75 ± 4.663.92 ± 8.5416.93 ± 26.9617.57 ± 29.4413.04 ± 21.26dm = 3.00.09 ± 0.280.09 ± 0.280.56 ± 2.702.40 ± 9.691.66 ± 5.690.26 ± 1.260.26 ± 1.271.11 ± 4.293.00 ± 11.315.38 ± 12.870.31 ± 0.980.57 ± 3.652.65 ± 9.293.67 ± 11.982.94 ± 7.96dm = 5.00.00 ± 0.000.00 ± 0.000.00 ± 0.000.03 ± 0.210.06 ± 0.400.00 ± 0.000.00 ± 0.000.01 ± 0.070.08 ± 0.480.32 ± 1.540.00 ± 0.000.00 ± 0.000.10 ± 1.160.08 ± 1.000.10 ± 0.71step) id-deflection for agent and client for varying numbers of samples and environment noises. Table C.4 also shows the total deflection (Equation (1)) and the maximum deflection across all experiments and time steps, for each agent. Note that the deflections are independent of the environment noise for the case where both identities are known. This is because, in this case, the actions of both agents follow exactly the dynamics as given by ACT, even with a small number of samples. When the environment noise rises, the agents both effectively ignore the observations, and more of the probability mass comes from the dynamics of the affect control principle and the identity inertia.The simulation proceeds according to the procedure in Section 3.5 for 50 steps. Agents take turns acting, and actions are conveyed to the other agent with the addition of some zero-mean normally distributed “environment” noise, with standard deviation σe . Agents use Gaussian observation models with uniform covariances with diagonal terms γ = max(0.52, σ 2e ). We perform 10 simulations per trial with βc = 0.001 for both agent and client. If the client knows the agent identity, it uses −1/3 where N is the number of samples. We use id-deflectionno roughening noise (σr = 0.0), otherwise all agents use σr = Nto denote the sum of squared differences between one agent’s estimate of the other agent’s identity, and that other agent’s estimate of its own identity.There are fewer effects to be analysed in the both known case as each agent knows exactly the identity of the other agent, and both agents follow the affect control principle and the dynamics of affect control theory. Therefore, they hardly need observations of the other agent, as these only serve to confirm accurate predictions. This is exactly what the affect control principle predicts: agents that share cultural affective sentiments and follow the affect control principle do not need to make any effort, as they maintain a harmonious balance. The agent id known case shows some of the same effects as the agent id hidden case, but for only one of the agents.Table D.7Results of matching tutor expressions to behaviour labels. (For interpretation of the references to colour please refer to the web version of this article.)client correctexpressionbest matchlabelsapplaudchallengeask aboutcomplimentagreethankencouragerewardadmonishgreetchatter1) Well, you got that one no problem, I think you’re ready for something harder2) Is that OK? Want another one?3) Great going! Here’s another one4) Wow – amazing work. Here’s an easy one for you5) Hi! Great to see you back again!6) Wow – you even solved this one! Great work!7) I really hope you are enjoying this exercise8) Nice weather today, eh?9) Can we do this a little quicker now?10) You are really smart, keep it up11) I see your point12) You should be the tutor not me13) Thank you for using our tutoring systemchallenge527ask aboutencouragerewardgreetapplaudcare forchatteradmonishcomplimentagreejoke withthank0137019000403053011001401000160200424000021560110002001300000010000350100100010001035521472440212020022150010000010001001117101000003301510001001011113001001joke with00010110110200care for013002115100100client incorrectexpressionbest matchlabelsconsoleassistapologizeinsultsympathize blamelectureinstructadmonish be discouragesarcasticcheer uphurry help advisecorrect disagreeguide1) OK, that was pretty hard, I hope this one will be more approachable for you2) Try thinking about the problem differently3) Here’s the answer to that one. Now try this one4) You need to think about it like this..5) I wonder if you are just too stupid for this6) Come on, a little more concentration, OK?7) Sorry, I may have been too demanding on you8) Seriously !! you must be Kiddingexcluded132assistexcludedinstructinsultadmonishapologizebe sarcastic000011015913020011000031000103621151500001200000170000230011076140201000011303000001115000012227000030001200300000710000502020000101000000000002110250000162J.Hoeyetal./ArtificialIntelligence230(2016)134–172Table D.7 (continued)client incorrectexpressionbest matchlabelsconsoleassistapologizeinsultsympathize blamelectureinstructadmonish be discouragesarcasticcheer uphurry help advisecorrect disagreeguide9) I don’t think you work hard enough. Reconsider your attitude!10) Don’t be sad, no one starts as a genius. Just keep working, OK?11) Can we do this a little quicker now?12) Let me show you how to do it13) I would suggest that you think more before answering14) That’s not the correct answer. The correct answer is...15) That is not what I meant16) You should work more on improving your skills17) I know how it feels to fail to answer many questions but don’t give up18) I’m giving up on you, I don’t believe you would be able to answer any questionblame0console20hurrylectureexcludedcorrectdisagreeexcluded000000sympathize5discourage0012514100000000040005221610200400001022160200102012013008561600001825331090207038021200300002102011000090000001018000260000000001503101001008111500000211400001110100000000019132200Note: EPAs of expressions were used to determine the best mach in case of ambiguity.J.Hoeyetal./ArtificialIntelligence230(2016)134–172163164J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172Table D.8Results of the EPA rating of tutor expressions. (For interpretation of the references to colour please refer to the web version of this article.)client correctexpression1) Well, you got that one no problem, I think you’re ready for something harder2) Is that OK? Want another one?3) Great going! Here’s another one4) Wow – amazing work. Here’s an easy one for you5) Hi! Great to see you back again!6) Wow – you even solved this one! Great work!7) I really hope you are enjoying this exercise8) Nice weather today, eh?9) Can we do this a little quicker now?10) You are really smart, keep it up11) I see your point12) You should be the tutor not me13) Thank you for using our tutoring systemclient incorrectexpression1) OK, that was pretty hard, I hope this one will be more approachable for you2) Try thinking about the problem differently3) Here’s the answer to that one. Now try this one4) You need to think about it like this..5) I wonder if you are just too stupid for this6) Come on, a little more concentration, OK?7) Sorry, I may have been too demanding on you8) Seriously !! you must be Kidding9) I don’t think you work hard enough. Reconsider your attitude!10) Don’t be sad, none starts as a genius. Just keep working, OK?11) Can we do this a little quicker now?12) Let me show you how to do it13) I would suggest that you think more before answering14) That’s not the correct answer. The correct answer is...15) That is not what i meant16) You should work more on improving your skills17) I know how it feels to fail to answer many questions but don’t give up18) I’m giving up on you, i don’t believe you would be able to answer any questionsurvey results1–9 scale−4.3–4.3 scaleACT database−4.3–4.3 scaleE7.935.88.387.97.138.486.375.474.237.526.286.865.8P7.3767.626.636.337.575.634.375.86.95.96.935.1survey results1–9 scaleE5.856.075.115.441.593.485.31.852.22P66.265.36.246.676.335.856.486.15A6.435.676.936.65.875.934.94.436.375.65.316.184.83A6.076.35.816.365.315.815.485.2766.116.526.154.264.633.485.785.895.746.076.265.274.315.175.364.464.156.115.155.786.634.965.266.111.486.415.22EPAE2.932.371.431.210.83.382.92.133.481.370.47−0.772.521.281.860.812.621.631.332.570.63−0.630.81.90.91.930.10.671.931.60.870.93−0.1−0.571.370.60.311.18−0.171.162.32.722.182.152.950.9−0.62.561.662.02.93P1.710.741.391.921.561.632.020.960.631.891.271.562.01A1.610.410.890.841.161.62−0.281.320.60.990.871.811.49−4.3–4.3 scaleACT database−4.3–4.3 scaleE0.851.070.110.44−3.41−1.520.3−3.15−2.781.11−0.74−0.37−1.52−0.69−0.54−0.851.11−3.52P11.260.31.241.671.330.851.481.151.520.780.890.740.170.150.781.631.41AEPA1.071.30.811.360.310.810.480.2711.151.071.260.270.36−0.040.261.112.21.640.751.85−1.88−0.61.84−0.42−1.552.14−0.58−0.280.70.062.121.65−0.460.631.170.32−0.161.540.40.311.020.751.590.30.560.6−0.41.110.20.321.610.350.280.63−0.10.22−1.69−0.44−0.56Table C.5 shows the results for the experiments with client shifting its identity after 10 steps and then staying at the new identity until 100 steps. We see that BayesAct is able to successfully recover: the id-deflection and deflection are both the same at the end of the 100 steps, regardless of sid.Table C.6 shows the mean number of time steps per sequence of 200 steps in which the id-deflection of the agent’s estimate of the client’s identity is greater than a threshold, dm. The results are shown for a variety of environment noises, σe , and identity shifting speeds, sid. The results show that BayesAct is able to maintain a low id-deflection throughout the sequence when confronted with speeds up to about 0.5 and environment noises less than σe = 0.5. At this setting (sid = σe = 0.5), only 12 frames (out of 200) have an id-deflection greater than 1.0.Appendix D. Tutoring system survey resultsParticipants in the survey were N = 37 (22 female) students (avg. age: 30.6 years). We presented them with four blocks of statements and behaviour labels, two blocks referring to agent and client behaviours conditional on a correct/incorrect answer of the client. In total, the survey contained 31 possible agent statements and 26 possible client statements plus an equal number of possibly corresponding behaviour labels. Participants were supposed to match each statement to one of Table D.9Results of matching client expressions to behaviour labels. (For interpretation of the references to colour please refer to the web version of this article.)labelswhine tobrag toaskpraisecontradictchat withanswerbe sarcasticbeam atthankchallengeagreesurprised byClient correctexpression1) awwww come on that was toooo hard2) bah – give me something harder3) Here is your answer!4) I’m sorry I suck so much5) Smart, eh. I’m so proud of myself6) Thanks for giving me this question7) Is that everything you have for me?8) Is that OK? / is this correct?9) You are an amazing tutor.10) No, this is not a good task for me11) Hey tutor, how are you today?12) I totally agree with you13)That’s very surprisingclient incorrectexpression1) awwww come on that was toooo hard. Give me something easier2) bah – give me something harder3) Here is your answer!4) I’m sorry I suck so much5) Whatever, I’m still so proud of myself6) Thanks for giving me another chance7) not so sure, but we’ll give it a try.best matchwhine tobrag toanswerexcludedexcludedthankexcludedaskpraisecontradictchat withagreeexcluded29002110000700001720210000000001000013320020000103400320000best matchlabelsbeseechbeseech12excludedexcludedexcludeddefendthankExcluded410002apologize togrin at01025002247140700020000018000suck up to34660430001002000310101230000101021341106570140121040301002010000002401000101113000110021000000010000000000000100290030thankhasslerequestyell ataskdisagree withcriticizeargue withdefend000122925540503750001200110000263000400006052300000110010101111602(continued on next page)J.Hoeyetal./ArtificialIntelligence230(2016)134–172165Table D.9 (continued)client incorrectexpression8) Stupid computer program!!9) You are not an effective teaching tool!10) Please, can we try something a little easier?11) I don’t agree with you on that12) But, there are more than one specific answer for this13) But, that has nothing to do with this topicbest matchyell atcriticizerequestdisagree withargue withargue withlabelsbeseechapologize togrin at0014001000000110000suck up to120000thankhasslerequest0000002300020013000yell at27130001askdisagree withcriticizeargue withdefend10400000126852121127030413120013116Note: EPAs of expressions were used to determine the best mach in case of ambiguity.166J.Hoeyetal./ArtificialIntelligence230(2016)134–172J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172167Table D.10Results of the EPA rating of client expressions. (For interpretation of the references to colour please refer to the web version of this article.)client incorrectexpression1) awwww come on that was toooo hard2) bah – give me something harder3) Here is your answer!4) I’m sorry I suck so much5) Smart, eh. I’m so proud of myself6) Thanks for giving me this question7) Is that everything you have for me?8) Is that OK? / is this correct?9) You are an amazing tutor.10) No, this is not a good task for me11) Hey tutor, how are you today?12) I totally agree with you13)That’s very surprisingclient incorrectexpressionsurvey results1–9 scale−4.3–4.3 scaleACT database−4.3–4.3 scaleE3.774.585.652.925.316.64.715.447.963.725.886.685.56P6.276.696.045.356.0466.215.567.55.244.926.366.24A5.316.356.083.885.296.176.045.486.54.725.25.965.72E−1.23−0.420.65−2.080.311.6−0.290.442.96−1.280.881.680.56P1.271.691.040.351.0411.210.562.50.24−0.081.361.24A0.311.351.08−1.120.291.171.040.481.5−0.280.20.960.72E−1.39–1.592.931.162.070.391.91.66P−0.8–0.732.010.742.030.540.821.27survey results1–9 scale−4.3–4.3 scaleACT database−4.3–4.3 scale1) awwww come on that was toooo hard. Give me something easier2) bah – give me something harder3) Here is your answer!4) I’m sorry I suck so much5) Whatever, I’m still so proud of myself6) Thanks for giving me another chance7) not so sure, but we’ll give it a try.8) Stupid computer program!!9) You are not an effective teaching tool!10) Please, can we try something a little easier?11) I don’t agree with you on that12) But, there are more than one specific answer for this13) But, that has nothing to do with this topicE3.253.464.422.753.426.795.542.252.354.714.674.923.79P5.655.915.745.135.096.225.915.926.355.675.886.136.5AE−1.7556.09 −1.545.48 −0.58−2.253.75.13 −1.581.7960.5464.79 −2.754.87 −2.655.63 −0.295.79 −0.336.13 −0.085.38 −1.21PAPE−0.61 −0.031.832.930.6500.911.090.740.480.13 −1.30.130.091.2210.9110.92 −0.21 −1.051.35 −0.13 −0.840.43 −0.210.630.670.750.790.880.061.13 −0.90.751.130.38 −0.90.751.52.32.010.391.690.54 −0.090.030.631.391.39A−0.5–0.161.490.411.070.850.710.87A0.331.211.49the available behaviour labels. We also asked participants to rate the affective meaning of each statement directly using the semantic differential [7]. For 14 agent statements and for 13 client statements, a clear majority of participants agreed on one specific mapping. For 13 agent statements and 5 client statements, mappings were split between two dominant options. In these cases, we compared the direct EPA ratings of the statements with EPA ratings of the two behaviour labels in question to settle the ambiguity. Standard deviations of the EPA scores were generally of the same magnitude or smaller than those reported by Heise [7] for general concepts, indicating high agreement.We discarded 4 agent statements and 8 client statements, because participants’ response patterns indicated a lack of consensus and/or unsolvable ambiguities in the mappings.16 We discarded a further 3 agent and 2 client statements because they were illogical for the tutoring application. As a result, we thus had a list of 24 agent statements and 16 client state-ments with corresponding mappings to behaviour labels from the ACT database and average EPA ratings from the survey. We implemented these behaviours as the possible actions in the BayesAct tutoring system, using the survey EPA ratings as the inputs/output (fb and ba).Tables D.7–D.10 show the results of the survey designed to map specific expressions (e.g., “Aww, come on, that was too hard for me!”) to corresponding behaviour labels (e.g., “whine to”). As described above, this was necessary to “translate” the ongoing communication between the agent and the client into the grammatical format required for Bayesian affect control theory (i.e., Agent–Behaviour–Client). We had two versions of each matching survey. The first table (labelled client correct) refer to the communication which occurred whenever the client solved a task correctly, while the expressions in the second versions of all the tables (labelled client incorrect) were used for cases in which the client did not manage to solve a task correctly.Tables D.7 shows how often the survey respondents associated the agent expressions with any of the available options for behaviour labels. The second column displays the label we picked in the end, based on the frequencies of choice and the consensus among respondents about the appropriate label. The boldface number shows the maximum across each row. Table D.8 displays average direct evaluation–potency–activity (EPA) of the expressions, without involvement of a behaviour 16 With sufficient data, we could simply learn or fit the observation function directly from the measured affective profiles.168J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172Table E.11Example simulation between the agent and a client (PwD) who holds the affective identity of “elder” (EPA = [1.67, 0.01, −1.03]). Affective actions chosen by BayesAct. Prompts are delivered if Pr(AW) falls below 0.4. Possible utterances for agent and client are shown that may correspond to the affective signatures computed. (For interpretation of the references to colour please refer to the web version of this article.)turnclient stateawps1initialclient1“[looks at sink]”agent“[looks at client]”1client“oh yes, this is good”1agent“I’m here to help, Frank”1client“this is nice”1001133actionprop.–put on soapaffect–[1.6, 0.77, −1.4]agent expectedfc[0.9, −0.69, −1.05][2.3, −0.77, −1.23]–[1.3, 0.26, −0.40][2.41, −0.81, −1.23]turn on tap[2.2, 0.90, −1.1][2.7, −0.36, −1.37]––[1.3, 0.4, 0.35][2.7, −0.37, −1.38][2.1, 0.72, −1.4][2.6, −0.34, −1.38]agent“Great job, Frank, time to rinse your hands now”rinse hands30[1.5, 0.67, 0.06][2.6, −0.34, −1.39]client“oh yes, this is good”01agent“good job Frank”client1“[looks at tap]”agent“This is nice, Frank”1client“Oh yes, good good”1agent“[looks at]”client“all done!”client11134466667rinse hands[1.9, 0.78, −1.4][2.7, −0.31, −1.44]–[1.6, 0.47, −0.13][2.7, −0.30, −1.4]turn tap off[2.0, 0.94, −1.3][2.6, −0.17, −1.24]––[1.5, 0.56, −0.35][2.6, −0.17, −1.2][2.1, 0.86, −1.42][2.8, −0.14, −0.14]dry hands[1.4, 0.66, −0.06][2.8, −0.13, −1.36]dry hands[1.94, 1.1, −1.9]––––ps00.961.03.03.03.03.04.04.05.96.06.06.0––clientaw0.720.94≈ 1.0defl.–0.231.070.990.99≈ 1.01.470.011.14≈ 0.01.500.991.11≈ 1.01.610.961.19≈ 1.0≈ 0.0≈ 0.0––1.561.221.551.55–label. As explained above, we used these ratings to cross-check the convergence of connotative meanings of the labels and expressions, as well as to resolve some ambiguities. Also shown are the EPA values from the ACT database corresponding to the most likely behaviour labels in Table D.7 (if not excluded). Analogously, Tables D.9 and D.10 display the expression-label match and EPA ratings, respectively, for the communication options that were given to the users of the software, i.e. these labels/expressions correspond to the client behaviours.We discarded a few expressions initially in the survey, where responses were so distributed that no consensus was recognizable about the meaning of these expressions. These cases are coloured light red and are labelled “excluded” in the second columns of Tables D.7 and D.9. We also discarded a few of the expressions prior to running the tutoring trials, because they were illogical given the actual application. These are coloured light grey in the tables.Appendix E. COACH system simulation resultsWe investigated the COACH system in simulation using an agent with an affective identity of “assistant”, and a clientwith an affective identity of “elder” (EPA = [1.67, 0.01, −1.03]). The client knows the identity of the agent, but the agentmust learn the identity of the client. Agent and client both know their own identities. We compare two types of policies: one where the affective actions are computed with BayesAct, and the other where the affective actions are fixed. We used a simple heuristic for the propositional actions where the client is prompted if the agent’s belief about the client’s awareness (AW) falls below 0.4.Table E.11 shows an example simulation between the agent and a client (PwD) who holds the affective identity of “elder”. This identity is more powerful and more good than that of “patient” (the default). Thus, the BayesAct agent must learn this identity (shown as fc in Table E.11) during the interaction if it wants to minimize deflection. We see in this case that the client starts with AW = “yes” (1) and does the first two steps, but then stops and is prompted by the agent to rinse his hands. This is the only prompt necessary, the deflection stays low, the agent gets a reasonable estimate of the client identity J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172169Table E.12Example simulation between the agent and a client (PwD) who holds the affective identity of “elder” (EPA = [1.67, 0.01, −1.03]). Affective actions were fixed: if prompting, it “commands” the user (EPA = [−0.09, 1.29, 1.59]) and when not prompting it “minds” the user (EPA = [0.86, 0.17, −0.16]). Possible utterances for agent and client are shown that may correspond to the affective signatures computed. (For interpretation of the references to colour please refer to the web version of this article.)turnclient stateInitialaw11Client“[looks at sink]”Agent“[looks at client]”1Client“oh yes, this is good”1Agent“[looks at client]”1Client“oh yes, this is good”1Agent“Rinse your hands now!!”0Client0“[looks at sink]”Agent“Rinse your hands now!!”0Client0“[looks at sink]”ps0011333333actionprop.–affect–agent expectedfc[0.9, −69, −1.05]Put on soap[1.6, 0.77, −1.4][2.3, −0.77, −1.23]–[0.85, 0.17, −0.16][2.41, −0.81, −1.23]ps00.961.0clientaw0.720.94≈ 1.0defl.–0.231.34Turn on tap[2.3, 0.90, −1.19][2.62, −0.42, −1.43]2.980.991.21––[0.85, 0.17, −0.16][2.7, −0.42, −1.5][2.2, 0.79, −1.47][2.6, −0.30, −1.4]Rinse hands[−0.1, 1.29, 1.59][2.6, −0.30, −1.4]–[1.9, 1.4, −1.7][2.5, −0.30, −1.3]Rinse hands[−0.1, 1.29, 1.59][2.5, −0.29, −1.3]–[1.9, 0.97, −1.9][2.4, −0.27, −1.26]3.03.03.03.03.03.03.0≈ 1.0≈ 0.0≈ 0.0≈ 0.0≈ 0.01.861.564.112.905.800.024.280.027.05Agent...continues for 48 more steps until client finally finishes ...Rinse hands30[−0.1, 1.29, 1.59][2.4, −0.26, −1.27]Table E.13Means and the standard error of the means (of each set of 10 simulations) of the number of interactions, and of the last planstep reached for simulations between agent and client with the identity as shown. Agent propositional policy is the heuristic one (prompt if awareness decreases below a threshold), and is the deflection minimizing action for BayesAct, or the fixed actions as shown.true client identityagent actioninteractionslast planstepelderBayesActpromptnon-promptpatientconvalescentbosspromptconfer withcommandpromptconfer withcommandpromptconfer withcommandpromptconfer withcommandBayesActBayesActBayesActmindmindmindmindmindmindmindmindmindmindmindmind12.8 ± 0.616.9 ± 1.412.6 ± 0.741.5 ± 5.113.0 ± 0.612.9 ± 1.012.6 ± 0.630.1 ± 6.913.3 ± 0.719.0 ± 2.013.9 ± 0.6331.6 ± 3.725.0 ± 1.550.6 ± 0.532.1 ± 2.750.6 ± 0.67.0 ± 0.07.0 ± 0.07.0 ± 0.05.3 ± 0.77.0 ± 0.07.0 ± 0.07.0 ± 0.05.3 ± 0.77.0 ± 0.07.0 ± 0.07.0 ± 0.06.7 ± 0.27.0 ± 0.03.6 ± 0.306.9 ± 0.142.7 ± 0.66(EPA = [2.8, −0.13, −1.36], a distance of 1.0). We show example utterances in the table that are “made up” based on our extensive experience working with PwD interacting with a handwashing assistant.Table E.12 shows the same client (“elder”) but this time the agent always uses the same affective actions: if prompting, it “commands” the user (EPA = [−0.09, 1.29, 1.59]) and when not prompting it “minds” the user (EPA = [0.86, 0.17, −0.16]). Here we see that the agent prompts cause significant deflection, and this causes the PwD to lose awareness (to become 170J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172confused) and not make any progress. The handwashing takes much longer, and the resulting interaction is likely much less satisfying.We have also run random simulations like the ones shown in Tables E.11 and E.12. For each set of simulations, we select a true affective identity for the client, and we either use BayesAct to select affective actions for the agent, or we use a fixed pair (one for the prompt, one for non-prompt actions). We run 10 sets of 10 simulated trials, stopping in each trial after 50 iterations or when the client finishes the task, whichever comes first. In Table E.13 we show the means and the standard error of the means (of each set of 10 simulations) of the number of interactions, and of the last planstep reached. We can see that, for client identities of “elder” and “patient”, the fixed policy of “confer with” (EPA = [1.87, 0.87, −0.35]) does equally well as BayesAct. The fixed policy of “prompt” (EPA = [0.15, 0.32, 0.06]) also does equally well for “patient”, but does more poorly (takes more iterations) for “elder”. We see the fixed policy of “command” does badly for both, but less so for “patient” than “elder”.We then look at more extreme identities. If the client has the identity of a “convalescent” (EPA = [0.3, 0.09, −0.03]), then we see the same effect as with “elder”. However, if the client has a much more powerful identity (“boss” EPA =[0.48, 2.16, 0.94], as they might believe if they used to be such an identity), then we see that all prompting methods work more slowly (the “boss” does not need prompting!!), and both fixed policies of “confer with” or “prompt” works less well than BayesAct. This is an indication that a “one-size fits all” policy may not work very well, whereas BayesAct provides a flexible and adaptive affective prompting strategy for PwD. Our future work is to integrate this with our current prototypes, and run trials with PwD.References[1] D.R. Heise, Expressive Order: Confirming Sentiments in Social Actions, Springer, 2007.[2] A. Smith, The Theory of Moral Sentiments, W. Strahan, London, 1759.[3] G.J. McCall, Symbolic interaction, in: P.J. Burke (Ed.), Contemporary Social Psychological Theories, Stanford University Press, 2006, pp. 1–23, Ch. 1.[4] C. Cioffi-Revilla, Introduction to Computational Social Science: Principles and Applications, Springer, 2014.[5] L. Lin, S. Czarnuch, A. Malhotra, L. Yu, T. Schröder, J. Hoey, Affectively aligned cognitive assistance using Bayesian affect control theory, in: Proc. of International Work-Conference on Ambient Assisted Living, IWAAL, Springer, Belfast, UK, 2014, pp. 279–287.[6] C.E. Osgood, G.J. Suci, P.H. Tannenbaum, The Measurement of Meaning, University of Illinois Press, Urbana, 1957.[7] D.R. Heise, Surveying Cultures: Discovering Shared Conceptions and Sentiments, Wiley, 2010.[8] D. Kahneman, Thinking, Fast and Slow, Doubleday, 2011.[9] N. Asghar, J. Hoey, Monte-Carlo planning for socially aligned agents using Bayesian affect control theory, in: Proc. Uncertainty in Artificial Intelligence, [10] J. Hoey, T. Schröder, A. Alhothali, Bayesian affect control theory, in: 2013 Humaine Association Conference on Affective Computing and Intelligent [11] J. Ambrasat, C. von Scheve, M. Conrad, G. Schauenburg, T. Schröder, Consensus and stratification in the affective meaning of human sociality, Proc. Natl. UAI, 2015, pp. 72–81.Interaction, ACII, 2013, pp. 166–172.Acad. Sci. 111 (22) (2014) 8001–8006.[12] C.E. Osgood, Studies of the generality of affective meaning systems, Am. Psychol. 17 (1962) 10–28.[13] C.E. Osgood, W.H. May, M.S. Miron, Cross-Cultural Universals of Affective Meaning, University of Illinois Press, 1975.[14] J.R.J. Fontaine, K.R. Scherer, E.B. Roesch, P.C. Ellsworth, The world of emotions is not two-dimensional, Psychol. Sci. 18 (2007) 1050–1057.[15] W. Scholl, The socio-emotional basis of human interaction and communication: how we construct our social world, Soc. Sci. Inf. 52 (2013) 3–33.[16] J.G. Fennell, R.J. Baddeley, Reward is assessed in three dimensions that correspond to the semantic differential, PLoS One 8 (2) (2013) e55588.[17] D.B. Shank, An affect control theory of technology, Curr. Res. Soc. Psychol. 15 (10) (2010) 1–13.[18] L. Troyer, An affect control theory as a foundation for the design of socially intelligent systems, in: Proc. AAAI Spring Symp. on Architectures for [19] B. Reeves, C. Nass, The Media Equation, Cambridge University Press, 1996.[20] A. Romney, J. Boyd, C. Moore, W. Batchelder, T. Brazill, Culture as shared cognitive representations, Proc. Natl. Acad. Sci. USA 93 (1996) 4699–4705.[21] C.P. Averett, D.R. Heise, Modified social identities: amalgamations, attributions, and emotions, J. Math. Sociol. 13 (1987) 103–132.[22] H.W. Smith, The dynamics of Japanese and American interpersonal events: behavioral settings versus personality traits, J. Math. Sociol. 26 (2002) [23] L. Smith-Lovin, The affective control of events within settings, J. Math. Sociol. 13 (1987) 71–101.[24] D.T. Robinson, L. Smith-Lovin, Affect control theory, in: P.J. Burke (Ed.), Contemporary Social Psychological Theories, Stanford University Press, 2006, [25] F. Heider, Attitudes and cognitive organization, J. Psychol. Interdiscip. Appl. 21 (1946) 107–112.[26] P. Thagard, Coherence in Thought and Action, MIT Press, 2000.[27] T. Schröder, W. Scholl, Affective dynamics of leadership: an experimental test of affect control theory, Soc. Psychol. Q. 72 (2009) 180–197.[28] T. Schröder, J. Netzel, C. Schermuly, W. Scholl, Culture-constrained affective consistency of interpersonal behavior: a test of affect control theory with nonverbal expressions, Soc. Psychol. Q. 44 (2013) 47–58.[29] D.R. Heise, Modeling interactions in small groups, Soc. Psychol. Q. 76 (2013) 52–72.[30] K.J. Åström, Optimal control of Markov decision processes with incomplete state estimation, J. Math. Anal. Appl. 10 (1965) 174–205.[31] M.L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, Wiley, New York, NY, 1994.[32] W.S. Lovejoy, A survey of algorithmic methods for partially observed Markov decision processes, Ann. Oper. Res. 28 (1991) 47–66.[33] C. Boutilier, T. Dean, S. Hanks, Decision theoretic planning: structural assumptions and computational leverage, J. Artif. Intell. Res. 11 (1999) 1–94.[34] L.P. Kaelbling, M.L. Littman, A.R. Cassandra, Planning and acting in partially observable stochastic domains, Artif. Intell. 101 (1998) 99–134.[35] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo, CA, 1988.[36] G. Shani, J. Pineau, R. Kaplow, A survey of point-based POMDP solvers, Auton. Agents Multi-Agent Syst. 27 (1) (2013) 1–51.[37] J.M. Porta, N. Vlassis, M.T. Spaan, P. Poupart, Point-based value iteration for continuous POMDPs, J. Mach. Learn. Res. 7 (2006) 2329–2367.[38] D. Silver, J. Veness, Monte-Carlo planning in large POMDPs, in: J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, A. Culotta (Eds.), Advances in Neural Information Processing Systems, NIPS, vol. 23, Curran Associates, Inc., 2010, pp. 2164–2172.[39] H. Kurniawati, D. Hsu, W. Lee, SARSOP: efficient point-based POMDP planning by approximating optimally reachable belief spaces, in: Proc. Robotics: Science and Systems, 2008, pp. 65–72.Modeling Emotion, 2004.71–92.pp. 137–164, Ch. 7.J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172171[40] P. Poupart, An introduction to fully and partially observable Markov decision processes, in: E. Sucar, E. Morales, J. Hoey (Eds.), Decision Theory Models for Applications in Artificial Intelligence: Concepts and Solutions, IGI Global, 2011, pp. 1–30, Ch. 1.[41] A.R. Damasio, Descartes’ Error: Emotion, Reason, and the Human Brain, Putnam’s Sons, 1994.[42] R.J. Dolan, Emotion, cognition, and behavior, Science 298 (5596) (2002) 1191–1194.[43] R. Pekrun, The impact of emotions on learning and achievement: towards a theory of cognitive/motivational mediators, Appl. Psychol. 41 (4) (1992) 359–376.[44] P. Thagard, Hot Thought: Mechanisms and Applications of Emotional Cognition, MIT Press, 2006.[45] R.W. Picard, Affective Computing, MIT Press, Cambridge, MA, 1997.[46] K.R. Scherer, T. Banziger, E. Roesch, A Blueprint for Affective Computing, Oxford University Press, 2010.[47] F. Wang, K. Carley, D. Zeng, W. Mao, Social computing: from social informatics to social intelligence, IEEE Intell. Syst. 22 (2) (2007) 79–83.[48] A. Vinciarelli, M. Pantic, D. Heylen, C. Pelachaud, I. Poggi, F. D’Errico, M. Schröder, Bridging the gap between social animal and unsocial machine: a survey of social signal processing, IEEE Trans. Affect. Comput. 3 (2012) 69–87.[49] J. LeDoux, The Emotional Brain: The Mysterious Underpinnings of Emotional Life, Simon and Schuster, New York, 1996.[50] G.A. Akerlof, R.E. Kranton, Economics and identity, Q. J. Econ. 115 (3) (2000) 715–753.[51] H. Tajfel, J.C. Turner, An integrative theory of intergroup conflict, in: S. Worchel, W. Austin (Eds.), The Social Psychology of Intergroup Relations, [52] J. Zhu, P. Thagard, Emotion and action, Philos. Psychol. 15 (1) (2002) 19–36.[53] R.A. Calvo, S. D’Mello, Affect detection: an interdisciplinary review of models, methods, and their applications, IEEE Trans. Affect. Comput. (2010) [54] Z. Zeng, M. Pantic, G.I. Roisman, T.S. Huang, A survey of affect recognition methods: audio, visual, and spontaneous expressions, IEEE Trans. Pattern Brooks/Cole, Monterey, CA, 1979.18–37.Anal. Mach. Intell. 31 (1) (2009) 39–58.[55] S. Hyniewska, R. Niewiadomski, M. Mancini, C. Pelachaud, Expression of affects in embodied conversational agents, in: Blueprint for Affective Comput-ing: A Sourcebook, Oxford University Press, 2010, pp. 213–221, Ch. 5.1.[56] M. Schröder, F. Burkhardt, S. Krstulovi ´c, Synthesis of emotional speech, in: Blueprint for Affective Computing: A Sourcebook, Oxford University Press, 2010, pp. 222–231, Ch. 5.2.[57] J. Steephen, HED: a computational model of affective adaptation and emotion dynamics, IEEE Trans. Affect. Comput. 4 (2) (2013) 197–210.[58] M.E. Hoque, M. Courgeon, J.-C. Martin, B. Mutlu, R.W. Picard, MACH: my automated conversation coach, in: Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp ’13, ACM, New York, NY, USA, 2013, pp. 697–706.[59] C. Becker-Asano, I. Wachsmuth, Affective computing with primary and secondary emotions in a virtual human, Auton. Agents Multi-Agent Syst. 20 (1) [60] D.V. Pynadath, S.C. Marsella, Psychsim: modeling theory of mind with decision-theoretic agents, in: Proc. of IJCAI, 2005, pp. 1181–1186.[61] J. Cassell, J. Sullivan, S. Prevost, E. Churchill (Eds.), Embodied Conversational Agents, MIT Press, 2000.[62] S. Thrun, W. Burgard, D. Fox, Probabilistic Robotics, MIT Press, Cambridge, MA, 2005.[63] L. Smith-Lovin, The strength of weak identities: social structural sources of self, situation and emotional experience, Soc. Psychol. Q. 70 (2) (2007) (2010) 32–49.106–124.[64] J. Derrida, Of Grammatology, Johns Hopkins University Press, Baltimore, 1976.[65] N.J. MacKinnon, D.R. Heise, Self, Identity and Social Institutions, Palgrave and Macmillan, New York, NY, 2010.[66] L.F. Barrett, Solving the emotion paradox: categorization and the experience of emotion, Personal. Soc. Psychol. Rev. 10 (1) (2006) 20–46.[67] J.A. Russell, A. Mehrabian, Evidence for a three-factor theory of emotions, J. Res. Pers. 11 (3) (1977) 273–294.[68] A. Ortony, G. Clore, A. Collins, The Cognitive Structure of Emotions, Cambridge University Press, 1988.[69] K.R. Scherer, A. Schorr, T. Johnstone, Appraisal Processes in Emotion, Oxford University Press, 2001.[70] K.R. Scherer, Appraisal theory, in: Handbook of Cognition and Emotion, 1999, pp. 637–663.[71] J. Gratch, S. Marsella, A domain-independent framework for modeling emotion, Cogn. Syst. Res. 5 (4) (2004) 269–306.[72] C. Smith, R. Lazarus, Emotion and adaptation, in: Pervin (Ed.), Handbook of Personality: Theory & Research, Guilford Press, New York, 1990, pp. 609–637.[73] C.L. Lisetti, P. Gmytrasiewicz, Can a rational agent afford to be affectless? A formal approach, Appl. Artif. Intell. 16 (7–8) (2002) 577–609.[74] K.B. Rogers, T. Schröder, C. von Scheve, Dissecting the sociality of emotion: a multi-level approach, Emot. Rev. 6 (2) (2014) 124–133.[75] S. Marsella, J. Gratch, P. Petta, Computational models of emotion, in: Blueprint for Affective Computing: A Sourcebook, Oxford University Press, 2010, [76] N.J. MacKinnon, Symbolic Interactionism as Affect Control, State University of New York Press, Albany, 1994.[77] K.R. Scherer, E.S. Dan, A. Flykt, What determines a feeling’s position in affective space: a case for appraisal, Cogn. Emot. 20 (1) (2006) 92–113.[78] C. Bicchieri, R. Muldoon, Social norms, in: E.N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy, spring 2014 edition, Stanford University, 2014.[79] T. Balke, C. da Costa Pereira, F. Dignum, E. Lorini, A. Rotolo, W. Vasconcelos, S. Villata, Norms in MAS: definitions and related concepts, in: G. Andrighetto, G. Governatori, P. Noriega, L.W.N. van der Torre (Eds.), Normative Multi-Agent Systems, in: Dagstuhl Follow-Ups, vol. 4, Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany, 2013, pp. 1–31.[80] F. Broz, I. Nourbakhsh, R. Simmons, Planning for human–robot interaction in socially situated tasks, Int. J. Soc. Robot. 5 (2) (2013) 193–214.[81] M.S. El-Nasr, J. Yen, T.R. Ioerger, FLAME – fuzzy logic adaptive model of emotions, Auton. Agents Multiagent Syst. 3 (2000) 219–257.[82] J. Sabourin, B. Mott, J.C. Lester, Modeling learner affect with theoretically grounded dynamic Bayesian networks, in: Proc. Affective Computing and Intelligent Interaction, Springer-Verlag, 2011, pp. 286–295.[83] C. Conati, H. Maclaren, Empirically building and evaluating a probabilistic model of user affect, User Model. User-Adapt. Interact. 19 (2009) 267–303.[84] E. Hogewoning, J. Broekens, J. Eggermont, E.G. Bovenkamp, Strategies for affect-controlled action-selection in Soar-RL, in: J. Mira, J. Àlvarez (Eds.), IWINAC, in: LNCS, vol. 4528, 2007, pp. 501–510 (Part II).[85] R.P. Marinier III, J.E. Laird, Emotion-driven reinforcement learning, in: Proc. of 30th Annual Meeting of the Cognitive, Science Society, Washington, D.C., 2008, pp. 115–120.(2013) 29:1–29:22.[86] N. Chentanez, A.G. Barto, S.P. Singh, Intrinsically motivated reinforcement learning, in: L. Saul, Y. Weiss, L. Bottou (Eds.), Advances in Neural Information Processing Systems, vol. 17, MIT Press, 2005, pp. 1281–1288.[87] J.T. Folsom-Kovarik, G. Sukthankar, S. Schatz, Tractable POMDP representations for intelligent tutoring systems, ACM Trans. Intell. Syst. Technol. 4 (2) [88] E. Brunskill, S. Russell, Partially observable sequential decision making for problem selection in an intelligent tutoring system, in: Proc. International [89] G. Theocharous, R. Beckwith, N. Butko, M. Philipose, Tractable POMDP planning algorithms for optimal teaching in “SPAIS”, in: Proc. IJCAI Workshop [90] J. Pineau, M. Montemerlo, M. Pollack, N. Roy, S. Thrun, Towards robotic assistants in nursing homes: challenges and results, Robot. Auton. Syst. 42 (3–4) Conference on Educational Data Mining, EDM, 2011.on Plan, Activity and Intent Recognition, PAIR, 2009.(2003) 271–281.[91] J.D. Williams, S. Young, Partially observable Markov decision processes for spoken dialog systems, Comput. Speech Lang. 21 (2) (2006) 393–422.pp. 213–221, Ch. 1.2.172J. Hoey et al. / Artificial Intelligence 230 (2016) 134–172(1993) 107–113.[92] A. Mihailidis, J. Boger, M. Candido, J. Hoey, The coach prompting system to assist older adults with dementia through handwashing: an efficacy study, BMC Geriatr. 8 (28) (2008).[93] J. Hoey, C. Boutilier, P. Poupart, P. Olivier, A. Monk, A. Mihailidis, People, sensors, decisions: customizable and adaptive technologies for assistance in healthcare, ACM Trans. Interact. Intell. Syst. 2 (4) (2012) 20:1–20:36.[94] L. Smith-Lovin, Impressions from events, J. Math. Sociol. 13 (1987) 35–70.[95] A. Doucet, N. de Freitas, N. Gordon (Eds.), Sequential Monte Carlo in Practice, Springer-Verlag, 2001.[96] N.J. Gordon, D. Salmond, A. Smith, Novel approach to nonlinear/non-Gaussian Bayesian state estimation, IEE Proc., F, Radar Signal Process. 140 (2) [97] A. Doucet, N. de Freitas, K. Murphy, S. Russell, Rao-Blackwellised particle filtering for dynamic Bayesian networks, in: Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, 2000, pp. 176–183.[98] B. Pang, L. Lee, Opinion mining and sentiment analysis, Found. Trends Inf. Retr. 2 (1–2) (2008) 1–135.[99] A. Alhothali, J. Hoey, Good news or bad news: using affect control theory to analyze readers’ reaction towards news articles, in: Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Denver, Colorado, 2015, pp. 1548–1558.[100] C.J. Orona, Temporality and identity loss due to Alzheimer’s disease, in: Special Issue Qualitative Research on Chronic Illness, Soc. Sci. Med. 30 (11) (1990) 1247–1256.15098621.[101] D. Rose Addis, L. Tippett, Memory of myself: autobiographical memory and identity in Alzheimer’s disease, Memory 12 (1) (2004) 56–74, pMID: [102] T. Schröder, J. Hoey, K.B. Rogers, Modeling dynamic identities and uncertainty in social interactions: Bayesian affect control theory, Am. Soc. Rev. (2015), conditionally accepted, preprint available on request.[103] J. Hoey, T. Schröder, Bayesian affect control theory of self, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2015, pp. 529–536.[104] P. Doshi, P. Gmytrasiewicz, Monte-Carlo sampling methods for approximating interactive POMDPs, J. Artif. Intell. Res. 34 (2009) 297–337.[105] E.J. Lawler, S.R. Thye, J. Yoon, Social Commitments in a Depersonalized World, Russell Sage Foundation, 2009.[106] T. Winograd, F. Flores, Understanding Computers and Cognition: A New Foundation for Design, Ablex Publishing Corporation, Norwood, NJ, 1986.[107] M. Heidegger, Being and Time, various, 1927.[108] M. Csíkszentmihályi, Flow: The Psychology of Optimal Experience, Harper and Row, New York, 1990.