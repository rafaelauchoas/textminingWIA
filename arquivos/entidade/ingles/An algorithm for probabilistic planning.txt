ELSEVIER Artificial Intelligence 76 (1995) 239-286 Artificial Intelligence An algorithm for probabilistic planning Nicholas Kushmerick*, Steve Hanks, Daniel S. Weld Department of Computer Science and Engineering, Box 352350, Universiv of Washington, Seattle, WA 98195, USA Received June 1993; revised March 1994 Abstract We define the probabilistic planning problem in terms of a probability distribution over initial world states, a boolean combination of propositions representing the goal, a probability threshold, and actions whose effects depend on the execution-time state of the world and on random chance. Adopting a probabilistic model complicates the definition of plan success: instead of demanding a plan that provably achieves the goal, we seek plans whose probability of success exceeds the threshold. In this paper, we present BURIDAN, an implemented least-commitment planner that solves problems of this form. We prove that the algorithm is both sound and complete. We then explore BURIDAN’S efficiency by contrasting four algorithms for pIan evaluation, using a combination of analytic methods and empirical experiments. We also describe the interplay between generating plans and evaluating them, and discuss the role of sc;arch control in probabilistic planning. 1. Introduction Classical planning assumes complete and deterministic information about the world for many domains: the ignition key might usually reasons. Even if a deterministic model is possible state and the effects of actions. These assumptions turning unknown be too complex outdoor weather rather than project of uncertainty: will the freeways be crowded? fail for it might to be useful. For example, when deciding between an indoor and an the to use a probabilistic model the cloud dynamics. The initial world state is also a source are inappropriate start one’s old car, but occasionally for a given domain, site for a wedding, one is likely to forecast * Corresponding author. E-mail: nick@cs.washington.edu. 0004-3702/9.5/$Q9.50 @ 1995 Elsevier Science B.V. All rights reserved SSDIOOO4-3702(94)00087-5 240 N. Kushmerick et al. /Art$cial Intelligence 76 (I 995) 239-286 This paper presents a planning algorithm that does not depend on the assumptions of complete and deterministic information. We use a probability distribution over possible world states to model imperfect information about the initial world state, and we model actions using a conditional probability distribution over changes to the world. Adopting a probabilistic model complicates the definition of plan success. Instead of terminating when it builds a plan that provably achieves the goal, our planner terminates when it builds a plan that is s@icientZy likely to succeed: our algorithm produces a plan such that the probability of the plan achieving the goal is no less than a user-supplied probability threshold, if such a plan exists. The work reported here makes several contributions. First, we define a symbolic action representation and provide it probabilistic semantics. Second, we describe an implemented algorithm, BUFUDAN, ' for probabilistic planning. Third, we prove the planner both sound and complete. Fourth, we compare the efficiency of four different probabilistic assessment algorithms both analytically and with empirical experiments. Finally, we explore the interface between the process of generating plans and the process of evaluating them. I. I. Action representation Following [ 281, we extend the standard STRIPS [ 221 representation to allow condi- tional and probabilistic effects. In STRIPS, an action is “enabled” if its preconditions are satisfied when the action is executed, in which case the action has a deterministic effect. If the preconditions do not hold, the action is “disabled”, and executing it is an error or meaningless. This simple model is not sufficient for representing actions with multiple possible consequences. BIJRIDAN models actions that can be executed in any world state, with the effect of executing the action depending on the execution-time state and on random chance. Consider the following simple action from a robot planning domain. Suppose that a robot’s grasping operation is not always successful. We model this action’s effects as depending both on the state of the world at execution time and on random chance. Specifically, we model the uncertainty of this pickup action by describing it in terms of four consequences. In two of the consequences, the robot will be holding the block after executing the action, but in the other two the world state doesn’t change. To each consequence we assign a probability which depends on the state of the world when the action is executed. For example, we might encode the fact that if the gripper is dry then the block is successfully grasped 95% of the time, but if the gripper is wet then the block is grasped only 50% of the time. Fig. 1 shows our representation of the pickup action. Propositions like GD and HB (“gripper dry” and “holding block”) characterize the relevant part of the world’s state. The pL encode the conditional probabilities that the ’ Jean Buridan (ba rE diin’ ) , 1300-l 358, a French philosopher and logician, has been credited with originat- ing probability theory. He seems to have toyed with the idea of using his theory to decide among alternative courses of action: the parable of “Buridan’s Ass” is attributed to him, in which an ass that lacked the ability to choose starved to death when placed between two equidistant piles of hay. N. Kushmerick et al, /Art$cial Intelligence 76 (1995) 239-286 241 Fig. 1. The pickup action. GD means “gripper dry”; HI3 means “holding block”. corresponding consequence is realized when the action is executed. For example, pa = 0.95 indicates that consequence LY is realized with probability 0.95 given that GD holds when the action is executed. As shown in Fig. 1, actions are encoded with binary trees. The leaves of the tree are the action’s eflects, the set of changes made to the world state if the corresponding trigger holds when the action is executed. The labels on the path from the root encode the consequence’s trigger, a conjunction expressing the conditions under which this consequence occurs. For example, executing pickup when the gripper is dry (GD), would likely (probability 0.95) cause the robot to be holding the block ( HB). * Like STEuF5’ add- and delete-lists, consequences describe changes to the world state rather than entire states. As shown in the figure, we index an action’s consequences with (Y, p, etc. The binary tree representation enforces the constraint that the triggers for all consequences of an action are mutually exclusive and exhaustive: exactly one will be realized during execution. In classical planning, a world state is described with a set of propositions. Since BURIDAN’S domains are probabilistic, we characterize the agent’s knowledge of the world not as a single state but rather as a probability distribution over possible states. In the classical paradigm, actions cause a transition from one state to another; BURDAN'S actions induce a transition from one probability distribution to another. Graphical depictions of actions like Fig. 1 might give the mistaken impression that we are assigning probabilities directly to propositions in an action’s consequences with- out regard to the state of the world at execution time. This is not the case: we are assigning probabilities to possible world states in which propositions are determinis- tically true or false. Section 2 provides a formal semantics for our action representa- tion. 1.2. The planning algorithm The job of a probabilistic planning algorithm is to construct a sequence of actions such that executing each action in turn, starting from some initial probability distribution over states, results in a final distribution in which the goal expression holds with sufficient ’ Since no set of effects contains i%, our simple model of robot grippers does not capture the phenomenon of dropping an already held block when attempting a pickup. Of course, it would be easy to elaborate our model to account for this phenomenon by introducing HB to the triggers of the action so that the effect of pickup depends on whether something is already held. 242 N. Kushmerick et al. /Art@ial Intelligence 76 (1995) 239-286 probability, where sufficiency is defined with respect to a user-supplied probability threshold. BURIDAN searches through a space of plans until it finds one that achieves the goal with sufficient probability. Each plan consists of a set of actions, a partial temporal ordering relation over the actions, a set of causal links, and a set of subgoals (each a proposition-action pair). The first two items are straightforward; the last two require some explanation. A causal link [ 381 Ai,‘zAj caches the planner’s reasoning that proposition p could is executed because consequence be true at the time action Aj (the link’s consumer) L of action Ai (the link’s producer) makes it true. The link is said to provide cuusul support for p. To realize this support, the planner must try to increase the probability that consequence L of Ai is realized and prevent p from being made false by other actions. BUFUDAN attempts the former by providing additional causal support to the triggers of Ai’s consequence c. set of subgoals-is The final component of a plan-the used for this purpose. The idea behind these pairs is analogous to a goal agenda in a classical planner: if p is a subgoal for action Aj (written p@Aj), then BURIDAN seeks to increase the probability of p at the time that Aj is executed. For example, one way to increase this probability is to add to the plan a new action that makes p true. Whenever BTJRIDAN does this (suppose it adds Ai whose consequence L makes p true), it records the decision with a BURIDAN then makes each proposition in consequence L’S trigger a causal link Ai,‘J+Aj. subgoal for Ai. When planning starts, the set of subgoals is initialized to the set of goal propositions tagged with a dummy action denoting the end of the plan. In summary, subgoals serve to focus BURLDAN's attention toward improvements to a plan that will tend to increase the probability of goal satisfaction. We say that an action Ak threatens a causal link Ai,‘%Aj if some consequence of Ak asserts p and if Ak might occur between Ai and Aj. Threatened links signify that BURIDAN'S commitments might not be met, so the planner must take evasive action. For example, BURIDAN can try to constrain the threatening action so that it must be executed before Ai or after Aj, thereby eliminating the threat. Planning starts with the null plan: a plan consisting of just two special actions initial and goal, with the constraint that initial be executed first and goal last (in the figures, we use the convention that time progresses from left to right). These special actions encode the probability distribution over initial world states and the goal expression, respectively. For example, consider a world with one block; suppose that initially the block is not held and the gripper is dry with probability 0.7. The initial action corresponding to this distribution is shown on the left side of Fig. 2. Each consequence of initial describes one of these two possible world states. The agent’s goal is encoded with a distinguished action goal. Suppose that the goal is to be holding the block, HB; the right side of Fig. 2 shows the goal action for this goal expression. goal has a single consequence that produces SUCCESS, triggered by the goal expression. BUFUDAN searches the space of partial plans, performing two operations at each visited node: ( 1) Plan assessment: Determine whether the probability that the current plan achieves the goal exceeds the probability threshold, terminating successfully if so. N. Kushmerick et al. /Art$cial Intelligence 76 (1995) 239-286 243 Fig. 2. The null plan encodes the initial state distribution and the goal. (2) Plan refinement: Otherwise, try to increase the probability of goal satisfaction by refining the current plan: l nondeterministically choose a subgoal p@Aj, and add a causal link to Aj from some new or existing action Ai that can produce p, in an attempt to increase the probability of p when Aj is executed; or l nondeterministically choose an existing causal link that is threatened, and resolve the threat. Signal failure if there are no possible refinements, otherwise continue by looping with the new partial plan. In Section 1.4 we illustrate these two operations using the example described ear- lier, but first we discuss some significant differences between the BURIDAN planning algorithm and other least-commitment planners. 1.3. Discussion Refining a plan with conditional and probabilistic operators differs from classical plan refinement (e.g. SNLP [38]) in three important ways. First, SNLP establishes a single causal link between a producing action and a con- suming action, and that link alone ensures that the link’s literal will be true when the consuming action is executed. Our planner links one of an action’s consequences to a later action. An action can have several consequences, though only one will actually oc- cur. Furthermore, a single link Ai,LSAj ensures that p will be true for action Aj only if trigger tf holds with probability one. Therefore multiple links may be needed to support a literal: even if no single link makes the literal sufficiently likely, their combination might. We lose SNLP’S clean distinction between an “open condition” (a trigger that is not supported by a link) and a “supported condition” that is guaranteed to be true. Causal support in a probabilistic plan is a cumulative concept: the more links supporting a literal, the more likely it is that the literal will be true. The concept of a threatened link is different when actions have conditional effects. Recall that Ak threatens Ai,‘sAj if some consequence of Ak asserts p and if Ak can be ordered between Ai and Aj. BURIDAN resolves threats in the same way that classical planners do: by ordering the threatening action either before the producer or after the consumer. But a plan can be sufficiently likely to succeed even if there is a threat, as long as the threat is sufficiently unlikely to occur. We can therefore resolve a threat in an additional way, by confrontation: if action Ak threatens link Ai,,sAj, plan for the occurrence of some consequence of Ak that does not make p false. 244 N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 A final difference between classical planners and BUmAN concerns the relationship between BURIDAN'S subgoals and a classical planner’s goal agenda [44]. In a classical planner, every entry on the agenda must be made true before the plan can be consid- ered a solution, but in the case of a probabilistic planner this is no longer the case. Thus BURIDAN need not consider all subgoals to devise a plan that achieves its goal with sufficient probability. Indeed, if the threshold is zero, it need consider none at all! 1.4. Example Recall the example: a robot whose gripper is possibly wet (with probability 0.3) needs to be holding a block. Alas, the pickup action is unreliable, especially when the gripper is wet. Suppose that there is also a dry action that usually (with probability 0.8) succeeds. We now illustrate the two steps of our planning algorithm with this example, assuming that a plan must be constructed that works 90% of the time. For expository purposes, we illustrate BURIDAN making the correct nondeterministic choices; in reality, considerable search is necessary to find a solution (see Section 6). 1.4.1. Plan refinement Plan refinement starts with the null plan shown in Fig. 2. Since HB is not true in any state in the initial distribution, BCJRIDAN adds an instance of the pickup action to the plan because this is the only action that can make HB true. BURIDAN creates a link from pickup’s cy consequence to the goal action. This link caches the planner’s reasoning that HB will be true because pickup makes it true, as long as the conditions under which pickup produces HB are satisfied and no intermediate actions produce HB. BURIDAN must thus try to bring about the circumstances that cause pickup to produce HB. In general BUFUDAN cannot guarantee that an action has a particular consequence, but the planner can add further refinements to make the desired consequence more likely. In our example this means trying to make the gripper dry when pickup is executed. BURIDAN can make GD true in two ways. GD is true initially with probability 0.7, so the first option is to add a causal link from initial. As we shall show in the next section, this simple plan-consisting of a single pickup action-has probability 0.815, which is less than the probability threshold 0.9. So BURIDAN adds additional causal support for GD by inserting and linking from a dry action. Fig. 3 shows the resulting plan. This plan achieves HB with probability 0.923, which exceeds the threshold of 0.9, so BIJRIDAN has successfully found a solution. 1.4.2. Plan assessment We have implemented and analyzed four algorithms that compute the probability that a plan achieves its goal; Section 5 discusses the performance tradeoffs among them. Here we illustrate the simplest assessment strategy, FORWARD, which directly implements the definition of plan success implied above. Recall that executing an action induces a transition from one probability distribution over states to another. FORWARD takes an action sequence and “executes” each action in turn, and then computes the probability N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 245 Fig. 3. This plan for holding the block, HB, works at least 90% of the time despite the fact that gripper dryness, GD, is not guaranteed. of the goal expression in the final distribution.3 Recall the simple one-action plan described above: picking up the block without first drying the gripper. The initial distribution consists of two states that differ only in and {GD, HB} (f or conciseness we write the state using whether GD holds: {GD,m} set notation). The initial probability distribution over states is: P[{GD,m}] = 0.7 and P[{GD, HB}] = 0.3. The probability distribution resulting from executing pickup in this initial distribution consists of the four states: -- -- (1) (2) (3) (4) and the is the resulting state if the initial state is in fact {GD,m} {GD, HB} pickup action is “successful”, i.e., consequence of cy of pickup is realized. (The other “successful” consequence, y, cannot happen in this initial state since one of its triggers, GD, is definitely false.) The probability of this new state is the probability of this initial state times the probability that pickup has consequence cy given this initial state, 0.7 x 0.95 = 0.665. {GD,m} consequence p; this state has probability 0.7 x 0.05 = 0.035. {m, HB} consequence y; this state has probability 0.3 x 0.5 = 0.15. -- {GD, HB} consequence 6; this state has probability 0.3 x 0.5 = 0.15. is realized if the initial state is {GD, HB} and pickup results in is realized if the initial state is {GD, HB} and pickup results in is realized if the initial state is {GD,HB} and pickup results in -- -- Since pickup is the plan’s only action, we now assess the goal HB with respect to this final state distribution. HB is true in the first and third states listed above, so the probability is the sum, 0.815, as reported earlier. 1.5. Alternative assessment algorithms The FORWARD assessment strategy, while simple, can be quite inefficient. For exam- ple, there exist domains in which the number of states with nonzero probability grows exponentially with the length of the plan. This inefficiency motivates a second focus of our research, an investigation of alternative assessment algorithms. Since the general plan assessment problem is NP-hard [ 5,8], we cannot hope to produce an assessment algorithm that runs efficiently in every domain. However, by exploiting the structure of 3 If the partially ordered plan is consistent with multiple total orders, then FORWARD performs the compu- tation for each total order and returns the minimum. 246 N. Kushmerick et al./Art$cial Intelligence 76 (1995) 239-286 the actions, goals and state space, we can sometimes realize tremendous efficiency gains. For example, while the number of states with nonzero probability may grow exponen- tially in the size of the plan, in general not all of the distinctions between the different states will be relevant to the question of whether the goal proposition holds. So one alternative assessment strategy, called QUERY, limits the growth of the state distribution by distinguishing states based on the value of only the subset of propositions relevant to the goal conjunction. Another class of algorithms reasons not about the actual state space but rather about the propositions that define the space. The insight is that while the number of states may grow exponentially, the number of propositions within a domain is constant. So a third assessment algorithm, which we call NE-WORK, maps the actions to a network of probabilistic constraints over the propositions, and then solves these constraints directly. The resulting networks tend to be more complicated than they need to be, however. For example, the network contains arcs and nodes that encode the fact that the truth value of a proposition remains unchanged across an action that does not affect it. But note that the plan itself contains explicit information about persistence: causal links are essentially a cache for deductions about persistence. We thus define a fourth algorithm, REVERSE, that traverses the plan’s causal link structure to do plan assessment. REVERSE and QUERY are also different from the other two assessment algorithms in an important way. FORWARD and NETWORK explicitly examine every totally ordered sequence of actions consistent with the plan (resulting in a large performance penalty), while REVERSE and QUERY can directly evaluate a partially ordered sequence of actions. We have implemented all four assessment algorithms; in Section 5 we present an analytical and empirical study of the tradeoffs among the various alternatives. In Section 6 we consider the assessment algorithms in a larger context. We demonstrate that speed of assessment does not always correlate well with planning speed because some assessors compute better bounds on the exact probability than others. Preliminary empirical results show that an improved interface between plan assessment and plan refinement can lead to significant speedup. 1.6. Contributions This paper describes our implemented, provably correct probabilistic planning algo- rithm. We have tested it on many small examples, including the simple Slippery Gripper example just described, an extension of this example that will be used throughout this paper to describe our algorithm in detail, and the Bomb and Toilet example [ 401 (see Section 6). We make the following advances to the field of planning: ( 1) We define an expressive action representation for which we provide a probabilistic semantics (Section 2). (2) We describe BURDAN, (Section 3). an implemented algorithm for probabilistic planning (3) We prove the planner both sound and complete (Section 4). (4) We compare the efficiency of four different probabilistic assessment algorithms both analytically and empirically (Section 5) and explore the relationship be- tween the processes of plan refinement and plan assessment (Section 6). N. Kushmerick et al. /Art@ial Intelligence 76 (1995) 239-286 24-l 2. A semantics for probabilistic planning The task of this section is to define a planning problem, and what it means to solve one. We begin by defining states and expressions, then actions and sequences of actions, and finally the planning problem and its solution. 2.1. States and expressions A state is a complete description of the world at a single point in time. A state is described using a set of propositions in which every proposition appears exactly once, possibly negated. 4 Uncertainty about the world is represented using a random variable over states. An expression is a set (implicit conjunction) of literals. We define the probability of an expression E with respect to a state s as p[m ={ 1, ifEcs, 0, otherwise, 2.2. Actions and action sequences Our model of action, taken from [ 27-291, combines a symbolic model of the changes the action makes to propositions with probabilistic parameters that represent chance (unmodeled) influences. Fig. 1 is a representation of the pickup action: if the gripper is dry (GD holds) at execution time, it makes HB true with probability 0.95, and with probability 0.05 makes no change to the world state. But if GD is false at execution, pickup makes HB true only with probability 0.5. Note that the propositions in the boxes refer to changes the action makes, not to world states. For example, it is not correct to say that the HB holds with probability 0.95 after executing pickup in a state where the gripper is dry, since the probability of HB after pickup is executed also depends on the probability of H B before execution (as well as the probability of GD before execution). We can make this intuitive definition more precise as follows. Definition 1 (Action). An action is a set of consequences {(ta,Pn,ea),...,(tg,~~,e~)} For each L, t, is an expression called the consequence’s trigger, 0 < p1 6 1, and e, is a set of literals called the efsects. The triggers must be mutually exclusive and exhaustive: VL.&J$IS] = 1, v’s, L, K . t, # t, + P[t, u t, 1 S] = 0. (2) (3) 4 We use this representation for expository purposes only; an implementation need not manipulate states explicitly. In fact our plan refinement algorithm has no explicit representation of state: it reasons directly about the state’s component propositions. Also see Section 5. 248 N. Kushmerick et al. /Artijicial Intelligence 76 (1995) 239-286 The notation Ai,‘ refers to consequence L of action Ai, and superscripts are used to refer to parts of a particular action: Ai = {. . . , (tf, Pf, ef), . . .}. The representation for the pickup action is thus {({GD},O.95, {HB}), ({GD},0.05, {}), ({m},0.5, {HB}), ({GD},0.5,{})}. A consequence defines through its set of effects a (deterministic) transition from a state to a state, defined by a function RESULT, similar to add- and delete-lists in STRIPS. Definition 2 (Change effected by a consequence). Let s be a state and e be a set of literals. Then RESULT(e, s) is defined as follows: for each proposition p, l If p E e, then p E EtESULT( e, s) and p 6 RESULT( e, s). l If p E e, then p E RJ%SULT(e, s) and p # RESULT(e, s). l Otherwise p E RESULT( e, s) iff p E s, and p E RESULT( e, s) iff p E s. An action A induces a change from a state s to a probability distribution over states s’: P[s’Is,A] = { ;P[tLls]’ if (t,,p‘,e,) E A and s’=~~w~T(e,,s), otherwise. 9 (4) Note that because an action’s triggers are mutually exclusive and exhaustive we have that C,, P [s’ 1 s, A] = 1 for all states s and all actions A. We now define the result of executing actions in sequence. The probability that a state s’ will hold after executing a sequence of actions (AJEi (,given that the world was initially in state s) is defined as follows: (5) (6) where (Ai)Lj = () if j > k. Finally, we define the probability that an expression is true after an action sequence is executed beginning in some state, and the probability of an expression after executing an action sequence given an initial probability distribution over states: P[E\s, (Ai):,] =CP[s’/s,(Ai)~,]P[&Is’], P[E[Z,,(Ai)Li] =~P[C,r.(Ai~~~]P[E’=JI. s 2.3. Planning problems and solutions (7) (8) We have given a semantics for actions in probabilistic domains. So far this discussion has been quite detached from the use of this semantics in a planning context. We are now N. Kushmerick et al./Art@ial Intelligence 76 (1995) 239-286 249 Fig. 4. The paint action. HB means “holding block”; BP means “block painted”; GC means “gripper clean”. in a position to define the input-output behavior of a probabilistic planning algorithm. The input is a planning problem. Definition 3 (Planning problem). A planning problem is a 4-tuple (Sl, 6, r, A), where 51 is a random variable over states, 6 is a expression, 0 < r < 1, and A is a set of actions. The intent is the following: ( 1) knowledge of the world at the beginning of the plan execution is characterized by Jr; the goal to be achieved is an expression 0; (2) (3) r is the probability threshold for goal satisfaction; and (4) A is the set of actions from which solutions may be constructed. Given these inputs, an algorithm for probabilistic planning must compute a totally ordered sequence of actions such that executing each action in turn induces a probability distribution over states in which the goal holds with probability no less than the threshold. Thus our final task is to give a precise meaning to the sentence “The action sequence (AJ& is a solution to the planning problem ($1, E, T, A) .” Intuitively, an action sequence is successful if the execution of the sequence achieves the goal with probability no less than the threshold. Definition 4 (Solution). Let A = ($1, Q, 7, A) be a planning problem, and (Ai):, be a (possibly empty) sequence of actions. (Ai):, is a solution to A iff each Ai E A and 2.4. Extending the example algorithm, but to fully describe BURIDAN The simple example described in Section 1.4 illustrates some aspects of our planning it is helpful to consider an extended version. Recall that we have defined two actions: pickup and dry, shown in Figs. 1 and 3. Fig. 4 illustrates a third action, paint, that paints the block (BP) but sometimes causes the gripper to become dirty (m) . For the problem’s goal, we demand that in addition to the robot needs to have it painted (BP) as well, while keeping holding the block (HB), its gripper clean (GC). To describe this example as a probabilistic planning problem we proceed as follows. Suppose that initially the block is not being held, the gripper is clean, the block is 250 N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 unpainted, and the gripper is dry with probability 0.7. Thus we have two initial states with nonzero probability, For our example, the world is initially in one of two possible states: sl = {GD, HB, GC,m}, -- .sz = {GD, HB, GC,m}, and the probability distribution over these states is characterized by a random variable 51 as follows: P[S[ = SI] = 0.7 and P[S, = s2] = 0.3. The goal is straightforward: If we are willing to consider plans with a twenty percent chance of D = { HB, BP, GC}. failure, then we set the probability threshold 7 = 0.8. Finally, we want the solution built from the three actions defined above: A = {pickup, paint, dry}. These four components together constitute the input to the planning algorithm. 3. The BURIDAN algorithm Given a planning problem, B-AN searches through a space of partial plans, terminating when it finds one corresponding to a solution. Each plan consists of a set of actions, 5 a partial temporal ordering relation ‘I<” over the actions, a set of causal finks, and a set of subgoals. There are significant differences between the last two items and the analogous SNLP concepts. A causal link caches BURIDAN’S reasoning that a particular consequence of a partic- ular action could make a literal true for a (later) action in the plan. The link Ai,‘SAj records the fact that literal p is a member of the trigger of one of action Aj’s conse- quences (Aj is the link’s consumer), and the effect set of consequence L of action Ai (the link’s producer) contains p. Action Ak threatens link Ai,b>Aj if some consequence of Ak asserts P, and if Ak can be ordered between Ai and Aj. A plan’s set of subgoals consists of the literals in the plan that BuRIDAN could try to make true at particular points in time. A subgoal is a literal annotated with a particular action, written p@Ai. BIJRIDAN adopts p@Ai as a subgoal of a plan if Ai is the producer for some link in the plan, and p is a trigger of the producing consequence. More formally, BURIDAN adopts p@Ai as a subgoal if Ai,‘>Aj is one of the plan’s links and p E t;? The set of subgoals is initialized to include all top-level goals. In Section notion of a subgoal and the 1.3 we discussed important differences between BURIDAN’s set of open conditions in classical planners such as SNLP. BURIDAN searches for a solution by performing two operations at each node in the space of plans: ( 1) Plan assessment: Compute the probability that the current plan will achieve the goal. If the probability is high enough, then the plan is a solution, and planning terminates successfully. 5 The set actually contains action instances because a plan may have more than one instance of a particular action. But since our representation “action” to refer both to actions and instances. is propositional this distinction is unimportant, and we use the term 6 Subgoals are also used to implement confrontation; see Section 3.3. N. Kushmerick et al./Artificial Intelligence 76 (1995) 239-286 251 (2) Plan refinement: Otherwise, the probability of goal satisfaction by the current plan. Each refinement generates a new partial plan. Signal choose refinements, otherwise nondeterministically try to increase refining failure if there are no possible a new partial plan, and loop. We describe each of these operations below, but first we describe the details of mnun4N's representations. 3.1. Data structures As we mentioned above, a causal link caches the planner’s reasoning proposition action makes it true. could be made true for an action because some consequence that a particular of a particular Definition 5 (Causal link). A causal link is a 4-tuple Consequence proposition supported by the link. L of action Ai is the link’s producer; Aj is the link’s consumer; p is the (Ai, L, p, Aj), written Ai,‘SAj. Each node in the space BURIDAN searches is a plan. Definition 6 (Plan). A plan is 4-tuple (A, 0, L, S), where A is a set of actions, 0 is a set of temporal ordering constraints is of the form Ai < Aj), 13 is a set of causal links, and S is a set of subgoals (each of the form p@Ai). over A (each element of which An action threatens a link when executing the action between the link’s producer and its consumer might decrease the probability of the proposition supported by the link. Definition 7 (Threat). Let (A, 0, L,S) the plan’s Ai < Ak < Aj is consistent with 0, and if there is some consequence pc be a plan, and links and Ak E A be one of the plan’s actions. Ak threatens Ai,‘J+Aj E L be one of iff K of Ak such that let Ai,‘sAj e”,. BURIDAN encodes initial. Each consequence the initial probability distribution of initial associated with the consequence encodes one possible encodes the state’s probability initial over states with a distinguished state, and the in the initial action probability distribution SI. Definition 8 (Action corresponding to a probability distribution over states). Let s”l be a random variable encoding a probability distribution over states. The action correspond- ing to $1 is INITIAL(s",) = {.. .,({},P1,eL),...}suchthatforeach~,p,=PIT,=ee,]. It is straightforward to prove that INITIAL( 51) satisfies the formal definition of an action. The goal is also encoded with a special action goal. It has a distinguished consequence marked SUCCESS and it is triggered by the goal expression. 252 N. Kushmerick et al. /Artificial Intelligence 76 (I 995) 239-286 G 0 initial a GD. HE. GC. BP s[ GD, HE. GC, BP 1 Fig. 5. initial and goal encode the initial probability distribution over states and the goal. Definition 9 (Action corresponding to a goal). Let 6 = { ~1, . . . , p,,} be an expression. The action corresponding to E is GOAL(G) = {(g, 1, {SUCCESS})}. Note that this definition of GOAL(G) does not satisfy the formal definition of an action, but it is straightforward to construct a more complicated definition that does satisfy the definition. 7 Recall the example from Section 2.4: initially the block is neither painted nor held ( BP and H B) and the gripper is certainly clean (GC), but there is only partial information about whether the gripper is dry; in one state (with probability 0.7) GD holds, but in the other it does not. Recall also that the goal is to have the block painted, the gripper clean, and the block held: 6 = {BP, GC, HB}. Fig. 5 shows the initial and goal actions for this planning problem. Now we can describe the root node of the space of plans, from which BUFUDAN starts searching for a solution. Definition 10 (Null plan). Let ($1, Q, T, A) be a planning problem. NULL-PLAN((S~,G,T,A)> = (d,O,L,S) is a plan constructed from ($1 , 0, r, A) as follows: 51) , A = {Ao, AG}, A0 = initial = INITL&( AG = goal= GOAL(G), O={Ao < AG), L=0, S = {P@AG I P E iY}. A0 must be the first action in any plan and Ao must be the last; the ordering constraints of the null plan enforce this invariant. To preserve this constraint, similar ordering constraints are added when each new action is introduced into all refinements of this plan. The set of subgoals is initialized to the set of goal propositions annotated with AG, the time just after all planned actions have been executed. ’ A formally correct goal action has 7) = IQ1 + 1 consequences defined as follows: each probability term is 1.0, and the first trigger is the negation of the fust goal literal, the second trigger is the first goal literal and . ., the IG’lth trigger is the first 191- 1 goal literals and the negation of the l@h, the negation of the second, and the 7th trigger is the entire goal expression. N. Kushmerick et al. /Artijicial Intelligence 76 (1995) 239-286 253 r B-m( else C. (S’I,~, 7, A)) 1. (d,O,L,S) + NULL-PLAN((s,,G.T,A)) 2. Do forever If ASSESS( (d, 0, L, S)) 3 7, then a. Return TOTAL-ORDER( d - {/&J , Ao} , 0) b. (d,O,L,S)cREFINE((d,O,~,S),n> If REFINE signalled failure, then Signal failure Fig. 6. The BURIDAN algorithm: top-level. 3.2. The BWUDAN algorithm: top-level We are now in a position to describe the BURIDAN algorithm; see Fig. 6. Given a planning problem ($1, Q, T, A), BURIDAN first converts the problem to the corresponding null plan. The null plan is then iteratively refined by calling the REFINE subroutine. REFINE nondeterministically chooses one possible refinement; if none are available then the planning problem has no solution and BURIDAN terminates. When the assessment algorithm determines that the plan’s probability of success is no less than 7, an arbitrary totally ordered sequence of the plan’s actions is returned. Section 3.3 explains plan refinement in detail; Section 3.4 describes assessment. (ASSESS) 3.3. Plan refinement BWUDAN’S plan refinement procedure (Fig. 7) considers all possible successors of a particular plan, and nondeterministically chooses one to return. There are two ways to refine a plan: either by resolving a threat to a causal link, or by adding a new increase the probability that a subgoal proposition holds when its link to (potentially) annotating action is executed. BURIDAN stores the set of subgoals in the S component of each plan, but the set of threatened links is easily computed dynamically from the plan’s links and actions. Link creation in BURIDAN is similar to the corresponding refinement in SNLP or UCPOP, but there are some important differences. We have already remarked that it may be desirable to add two independent actions to make a proposition true and that doing so will result in two causal links supporting the proposition. A planner that does not allow actions with disjunctive effects need not consider multiple causal support, although it may choose to do so for efficiency reasons [ 321. The whole notion of causal support is more complex in the probabilistic case. For example, linking to a new action, even if it does not involve threats, may not increase the probability of goal satisfaction. Suppose that consequences from two different actions are used to support the same subgoal. If the triggers of the two supporting consequences are identical and are supported from the same source, then they are probabilistically dependent. In this case the support they lend is not additive. For example, suppose that turning the ignition key always starts the car just in case its battery is charged, but the agent doesn’t know whether the battery 254 N. Kushmerick et al. /Art@cial Intelligence 76 (1995) 239-286 =FINE( (d,CLCS),A) 1. Choose FLAW from S or the set of threatened links. 2. If FLAW is o@Aj then add support: a. Nondeterministically add a new action Ai from n to A (also adding Aa < Ai < Ao to O), or choose an existing Ai from A such that Ai has a consequence L (chosen nondeterministically) to C ad Ai < A! to 0. b. Add Ai_,sAj C. Add q@Ai to S, for each q E tf. d. Signal failure if none of these options are possible for the current plan. that asserts p. 3. If FLAW is a threat to Ai,‘zAj by Ak, then nondeterministically choose: a. Demotion: constrain Ak < Ai, or b. Promotion: constrain Aj < Ak, or c. Confrontation: commit to consequences of Ak that do not make p false: i. Create a new safety proposition s, ii. Modify Ak so that its non-interfering consequences produce s, iii. Add s@Aj to S. d. Signal failure if none of these options are possible for the current plan. 4. Return the resulting plan. Fig. 7. The REFINE algorithm. is healthy or dead. In this case, is clearly doesn’t help to turn the key more than once. This explains why BURIDAN (unlike SNLP or UCPOP) needs a separate assessment “local” computation (i.e., one that does not look at the causal structure of routine-no the entire plan) suffices to determine plan success. Said another way, whereas in the classical paradigm causal links can eliminate the need for dynamically computing the Modal Truth Criterion, this cannot be avoided in the probabilistic case. Link promotion and demotion are identical to threat resolution in classical planners, so we do not discuss them further. 8 Confrontation is a significant departure from SNLP, however, and it deserves some additional explanation. The probability that link Ai,‘sAj succeeds in producing p for Aj is the probability that executing action Ai actually results in consequence L and that no action between Ai and Aj makes p false. Since BURIDAN need only produce a plan that succeeds with probability no less than r, it might be acceptable to allow a threatening action to remain between the link’s producing and consuming actions as long as it makes p false with sufficiently low probability. Confrontation resolves a threat in exactly this manner. Confrontation involves noting which consequences of the threatening action do not pose a threat to the link (the non- interjering consequences), and attempting to increase the probability that one of these consequences is realized when the threatening action is executed. Specifically, BURrDAN confronts a threat by modifying the threatening action so that each non-interfering consequence produces a newly created proposition s, unique to the threat, called a * We ignore separation (the addition of variable-binding constraints [5] ) since BURIDAN is propositional. Lifting techniques [ 38 ] could be used to extend BURIDAN to a more. expressive language. N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 25.5 Fig. 8 Support for holding the block. safety proposition. The safety proposition, annotated with the consumer of the link, is then adopted as an additional subgoal. Since only the non-interfering consequences of Ak can produce s, planning for the safety condition amounts to planning to make a non-interfering consequence of the threatening action occur. Of course, if an action has IZO non-interfering consequences then confrontation is inappropriate; the algorithm must in this case either promote or demote the threat instead. Example. We now demonstrate how the plan refinement algorithm constructs a plan that will succeed with probability at least 0.8 in satisfying its goal to be holding a painted block with a clean gripper. As in the example of Section 1, we simplify the presentation by assuming that REFINE makes the correct sequence of nondeterministic choices; as discussed in Section 6, BURIDAN takes about 4.5 seconds to find a solution using brute-force search. Step 1. Planning starts with the null plan, shown in Fig. 5. The subgoals for this plan are the goal propositions annotated with the goal action: S = {HB@AG, BP@&, GC@AG}. BURIDAN chooses to support the first subgoal, HB@AG, by adding an instance of the pickup action, At, and linking to this action’s a consequence with the link AI,,TAG. BUFUDAN supports the desired consequence LY of At by adopting GD@Al as a subgoal. Support for this subgoal is then provided by linking directly to the initial action Aa with the link Aa,%At. The resulting plan is shown in Fig. 8. The assessor determines that this plan is inadequate, so refinement continues. Step 2. BURIDAN next supports the subgoal of having the block painted, BP@AG, by adding a new paint action, AZ, and adding the link A$%o. Consequence p of adopts i%@Az as a paint is realized only if the block is not held. The planner te subgoal, support for which is added with a link from initial: Ao~+A~. pickup and paint are unordered, so pickup threatens this new link: if pickup is executed before paint then the block will be held when paint is executed, violating the i% trigger of paint’s consequence /3. BURlDAN resolves this threat by promoting pickup with the constraint AZ < At. BURIDAN then supports the goal of having a clean gripper by providing support to GC@Ao with the link Ao,~%AG. But paint threatens this link: paint asserts GC if either consequence c~ or consequence y is realized. The only option is confrontation, which involves adding a new safety proposition s1 to ea, the only consequence of A:! that does not cause GC. The resulting plan is shown in Fig. 9. The gray circle on the link indicates that the threat has been resolved by confrontation. 2 256 N. Kushmerick et al./Artificial Intelligence 76 (1995) 239-286 Fig. 9. Support for painting the block while keeping the gripper clean. Fig. 10. Additional support for a dry gripper. Step 3. Fig. 9’s plan has probability 0.7335 (as computed in Section 3.4). This value is less than T = 0.8 so BURIDAN tries to make the goal more probable by drying the gripper before trying to pick up the block. This is done by adding a new dry action, A3, and adding the link As,,%?Al. BURIDAN also adds additional su gport for the goal of having the gripper clean, GC@Ao, by linking from initial with &,p+Ao. This link is threatened by A2 just as Ao,,%A, was, and the threat is resolved in the same way, by confrontation; the safety proposition for this threat is ~2. The resulting plan is shown in Fig. 10. This plan has a probability of 0.831, which exceeds 7, so BUFUDAN has succeeded in finding a solution. 3.4. Plan assessment The plan assessment algorithm decides whether the probability of success for a plan exceeds the threshold T. Soundness demands only that the assessor never incorrectly identify a plan as a solution-that is, that it never identify a plan as a solution when in fact the plan’s success probability is less than T. The algorithms we implemented are somewhat more general, computing a lower bound on the exact probability of success. N. Kushmerick et al. /Artijicial Intelligence 76 (1995) 239-286 251 In this section we describe only the FORWARD assessment algorithm, a straightforward implementation of the definition of a solution to a planning problem (Definition 4); in Section 5 we describe three different algorithms. Fig. 11 presents the FORWARD algorithm. To explain the algorithm, we first define the data structure it uses to represent a probability distribution over states. Definition 11 (State distribution). a set of pairs such that each si is a state, 0 < pi 6 1, and A state distribution SD = { (~1, PI), . . . , (s,, , pn)} is c (Si, Pi)ESD P‘ ,=l (10) For example, the state distribution corresponding to the probability distribution over initial world states in the example is the set -- {({GD,~,GC,~},0.7),({GD,HB,GC,~},0.3)}. Formally, an action induces a transition from one probability distribution over states to another. FORWARD uses the EXEC function to represent this transition in terms of state distributions. Definition 12 (Execution of an action in a state distribution). Let SD = {(%Pl), * * * 9 (wh)) be a state distribution and A={(t,,~~,e~),...,(t~,~~,e,)} be an action. Then the execution of A in SD is the set: (ri,b,Pi,L) = (RESULT(e‘,si),pip‘P[t‘ISi]). (12) It is straightforward to prove that EXEC( A, SD) is a state distribution. Finally, FOR- WARD needs to evaluate the probability of an expression in a state distribution: P[EISD] = C piP[EISi]. (Sir pi)ESD (13) FORWARD uses Eqs. ( 11) and ( 12) to compute the successive state distributions that result while projecting the effects of a plan’s actions. For efficiency, FORWARD prunes zero-probability states and combines members of the state distribution that refer to the 258 N. Kushtnerick et al. /Art$icial Intelligence 76 (1995) 239-286 ASSESS(P) 1. Return FORWARD(P) FORWARD( (d, 0, J$ s)) 1. Let S2) be the state distribution corresponding to Aa E A. 2. Let G be ty (the trigger of AC’S SUCCESS consequence). 3. SCENARIOS +- SET-OF-ALL-TOTAL-ORDERS( d - 4. If SCENARIOS = 8 then MIN e 0 else MIN t- 1. 5. For each sequence (Ai):, E SCENARIOS, use Eqs. (11) and (12) to calculate {I&J, AG } , 0) . the state distribution resulting from executing each action in turn. For effi- ciency, “compress” the intermediate distributions by eliminating states with zero probability and merging identical states. 6. Use Eq. (13) to compute the probability of G in the final state distribution; update MIN whenever the sum is lower. 7. Return MIN. Fig. 11. FORWARD is the simplest of our four plan assessment algorithms. same state. After all actions have been projected, the goal expression is evaluated using Eq. (13). Complicating the assessment process is the fact that a solution is defined in terms of a totally ordered sequence of actions, while a plan’s actions might be only partially ordered. We can still compute a lower bound on the plan’s success, however, by consid- ering the minimum over all total orders consistent with the plan’s orderings. This policy is conservative in that it computes the best probability that can be expected from every total order. 9 Example. We now illustrate how the FORWARD plan assessment algorithm computes the probability of success for the plan shown in Fig. 9. This plan involves first painting the block and then picking it up. The initial state distribution SVc consists of two states with nonzero probability. They differ only in whether or not the gripper is dry: SD,, = {({GD,m, GC,m},0.7), -- ({GD, HB, GC,m},0.3)}. Projecting the effects of paint in SDa results in a state distribution with four elements: =‘I = EXEC(paint’S~o) = --- ({BP, GC, HB, GD},0.03), -- ({BP, GC, HB, GD},0.27), ({BP, GC, HB, GD},0.07), -- i ({BP, GC,i%, GD}, 0.63) I 9 Section 6.1 considers the possibility of computing the muximum over all total orders, corresponding to the best probability that could be expected from any consistent total order. N. Kushmerick et al./Artijcial Intelligence 76 (1995) 239-286 259 We then project pickup: 222~2 = ExEc( pickup, ST& ) = -- -- ({GD,i%,GC,BP},0.0315), ({GD, HB, GC, BP},O.5985), ({GD, HB, GC, BP},O.O035), ({GD, HB,x, BP},O.O665), ({GD, HB, GC, BP},O.135), ({m, HB, GC, BP},0.135), --- ({GD, HB, GC, BP},O.O15), ({m, BP},0.015) HB,E, c c Finally, the goal expression is evaluated with respect to this state distribution. Since the goal holds in 2 of the 8 states (those marked in the previous equation), we get P[{BP, GC, HB} ISD2] = 0.5985 + 0.135 = 0.7335, which is less than the threshold r = 0.8, so as described in the previous section, planning continues until producing the plan shown in Fig. 10. 4. Formal properties We now prove that the BURIDAN planning algorithm is sound and complete. We say that a probabilistic planner is sound if it never returns an action sequence whose chance of success is less than the threshold r demands. We call such a planner complete if it finds such a sequence whenever a solution exists. Note that this does not require the planner to recognize futility when no solution exists and that BURIDAN could loop in this case. lo As we shall see, the explicit correspondence between the ASSESS and FORWARD algorithms and the underlying semantics makes the proof of soundness quite straight- forward. is more difficult-we Proving completeness need to establish two things: (i) that every action sequence leading to a solution is eventually considered by the planner, and (ii) that every solution passed to the assessor is recognized as a solution. The key to the first point is showing that a plan’s set of subgoals identifies all the refinements that might increase the probability of achieving the goal. To establish the second point we start by observing that since BURIDAN'S assessor implements Definition 4 directly, it calculates by definition the exact probability of any totally ordered sequence of actions. The trick is to establish that BURIDAN will add enough ordering constraints to raise a plan’s minimum probability (taken over its total orders) over the threshold if it is possible to do so. lo It is not clear at this point whether results on classical planning with infinite state spaces propositional search and does not allow functions (thus the problem BURIDAN solves is fully decidable. On the one hand, [5,18] do not apply because BURIDAN'S state space is the fact that BURIDAN must is finite). On the other hand, through a space of probability distributions over states (which is infinite) complicates the problem. 260 N. Kushmerick et al./Artificial Intelligence 76 (1995) 239-286 First we need to reconcile the notation introduced when we characterized a planning problem and solution formally with the data structures manipulated by the planner (in particular the assessment algorithm). The FORWARD assessor manipulates a data structure called a state distribution, which is a set of pairs of the form (si, pi). There is an obvious equivalence between a state distribution and a probability distribution over states 5: (Si, pi) E SD is equivalent t0 P [3 = si] = pi* Similarly the formal exposition represented actions as conditional probabilities of the h w ereas FORWARD uses a function EXEC( A, SD) that produces a state formP[s’(.s,A] distribution. Once again the equivalence should be clear: if SV and 5 are equivalent, then (si,pi) E ExEc(A,SD) just in case pi = c,, P[siIs’,A]P[S= s’]. In the following proofs we mix the two notations (random variables over states and state distributions; conditional probabilities defining action execution and the EXEC function). 4. I. Soundness We define soundness in terms of Definition 4. A planner is sound if it never returns a plan that is not a solution. Theorem 13 (Soundness). Let A = (51, G, 7, A) be Q pfanning problem, I~BWAN( returns the action sequence (Ai):,, then (Ai)[i is a solution to A. A) refinement and assessment algorithms make this proof straightfor- Proof. B-AN'S ward. Note that BURIDAN can exit its infinite loop in only two ways. One of these exits (Line 2.c of Fig. 6) signals failure; since this return does not produce an action sequence, it does not satisfy the antecedent of the theorem and need not be considered. The other exit (Line 2.a) returns an action sequence consistent with the plan’s partial order 0 only when the probability assessment produced by FORWARD is at least r. Since FORWARD computes the minimum probability of achieving the goal taken over is guaranteed to all total orders consistent with 0, the estimate returned by FORWARD be no higher than the actual probability that Q will be achieved by (Ai):,. Transitivity ensures that (Ai)[t is a solution and BURlDAN is sound. El 4.2. Completeness There are several possible definitions of completeness. For example, one might require that the planner return alt action sequences that achieve the goal with probability greater than the threshold. This definition is silly, however, because it requires the planner to augment a solution with irrelevant actions. Our definition sidesteps the problem of irrelevant actions in the plan: we require that the planner find all essential solutions. An essential solution is an action sequence that is itself a solution but which fails to be a solution when any of its actions are removed. (The fact that an action sequence is essential does not mean that it is the shortest possible solution; there might be a completely different and much shorter sequence that also achieves the goal.) N. Kushmerick et al./Artijcial Inrelligence 76 (1995) 239-286 261 Theorem 14 (Completeness). Let A = (S,, S, r, A) be a planning problem and let (Ai):, be an essential solution (i.e., no proper subsequence is also solution) of A. Then choices such that BURIDAN( A) will return there exists a sequence of nondeteninistic (Ai);, . We prove completeness by induction on N, the length of the essential solution. In the base case (a zero-length plan) we show that the algorithm correctly recognizes the case where the initial state satisfies the goal with sufficient probability. We then make the inductive hypothesis that the algorithm will find all essential plans of length less than N. The difficulty is in showing how the ability to generate N - 1 step plans bears on a problem, A, whose essential solution has N steps. We do this by constructing a modi$ed planning problem which can be solved in N - 1 steps. Since the proof’s details are complex, we relegate them to Appendix A. 5. Efficient plan assessment Our plan refinement algorithm calls a plan assessment algorithm as a subroutine; that algorithm must compute the probability that the totally ordered completions of a partially ordered plan achieve the goal expression, or at least provide a lower bound on that probability. This section examines plan assessment in isolation. Assessing an arbitrary partially ordered plan with conditional effects is NP-hard even when all probabilities are zero or one [ 5,9]. While REFJNE doesn’t generate arbitrary partial orders, we believe that the additional complexity of probabilistic computations [ 81 can make assessment (even of a totally ordered plan) require time that is exponential in the length of the plan-the computation might require considering all combinations of consequences of every action in the plan. So our aspirations are not to produce an algorithm that works efficiently for all planning problems; instead we present four alternative algorithms and demonstrate when each does and does not perform well. Future work might attempt to integrate the best aspects of these approaches given the expected characteristics of the domain in question. Studying assessment in isolation gives us insight into why various algorithms work or don’t work, but the study is not an end unto itself. We are ultimately interested in how long it takes to generate a complete solution, not the per-plan time for refinement or assessment. As we show in Section 6, the fastest assessment algorithm does not necessarily lead to the fastest planner. By returning a better bound on the plan’s success probability, a slower assessor can speed the overall planning process considerably. 5.1. The FORWARD assessment algorithm We begin by describing the computational problems with the FORWARD algorithm described in Section 3.4. FORWARD is a straightforward implementation of our action semantics. Two features of the algorithm are important for this discussion. First, FOR- WARD projects each action through a state distribution, producing a new distribution. And second, since plan success is defined only for a totally ordered sequences of ac- 262 N. Kushmerick et al./Art$cial Intelligence 76 (1995) 239-286 tions, FORWARD computes the success probability for every totally ordered sequence consistent with the input partial order, and returns the minimum. Each of these features can lead to computational problems. First, there is a potential explosion in the size of the state distributions that are manipulated: if the original state distribution has M members, each action has z7 consequences, and the plan contains N actions, assessing even a single total order can generate a state distribution containing as many as MvN states. ” The second computational problem concerns partially ordered actions: there are as many as N! consistent total orders of the actions in an N-action plan, and the basic algorithm has to be applied once for each total order. In summary, FORWARD might make distinctions among states in the state space, and among orderings among the total orders, that are irrelevant to whether the goal is achieved. These inefficiencies can lead to degraded performance; our second algorithm, QUERY, is designed to overcome both problems. 5.2. The QUERY assessment algorithm The QUERY assessor is an adaptation of Hanks’ [27,28] projection algorithm which actually applies to a richer action representation than BURIDAN’s, including continu- ous quantities and sets, and conditional (branching) plan execution. QUERY is goal directed-it tries to articulate the state space only when doing so is necessary to decide the state of a query proposition. The basic idea is to divide an action’s consequences into equivalence classes based on how they affect the query proposition, and reason about the classes instead of about the individual consequences. For example, consider an action A with several consequences. Suppose that each of these consequences makes a goal proposition G true, but they differ on the changes they make to other proposi- tions. If G is all that is relevant to plan success, QUERY will consider A to have a single consequence class that makes G true and is realized with probability one. We omit the details of the QUERY algorithm from this paper; see instead [ 27,281. 5.3. The NETWORK assessment algorithm FORWARD and QUERY are similar in that they represent world states explicitly: both manipulate structures that represent elements of the state space. An alternative is to dispense with an explicit representation of a state and represent its component propositions directly. Instead of reasoning about actions as transformations from state distributions to state distributions, we instead reason about the circumstances under which an action makes a proposition true, makes it false, or leaves it unchanged. This strategy suggests using a belief network [43] for assessment structure whose is similar to that proposed by Dean and Kanazawa [ 1 l] and Hanks [ 301. Fig. 12 shows I1 Of course, if there are D propositions, then the state distribution can never have more than 2D distinct states. However, if the initial distribution has a small number of states with nonzero probability, then executing each action can lead to growth that is exponential in the number of actions. N. Kushmerick et al. /Arttjicial Intelligence 76 (1995) 239-286 263 Fig. 12. A probabilistic network for plan assessment. such a network for a domain with propositions {pr , . . . , pm} and a plan with action sequence (Ai):, . l2 The graph consists of two types of nodes. First there is a node for each action: the node for Ai takes a value from the set {a,. . . , q~}, where Ai has 7 consequences; the value L represents the case where consequence L of Ai is realized. Second, there is a “layer” of binary-valued nodes representing the propositions evaluated just after executing each action. The nodes for the propositions just after Ai point to the action node for Ai+i if the proposition is one of Ai+r ‘s triggers. The action node then has an arc to the proposition nodes that it might affect, The state of every proposition at one stage also affects the state of the same proposition at the next stage. Finally, there is a binary SUCCESS node that is true exactly when all propositions in the goal expression hold. We may solve this network of constraints using standard propagation techniques [ 431, provided we supply appropriate numeric parameters for the model: a conditional probability table (“link matrix”) indicating the probability that the node will take on one of its values, conditioned on the states of all the arcs that point to it. Specifically, NETWORK constructs a network such that: l Action Ai will realize consequence L with probability of if that consequence’s trigger propositions all hold. l A proposition p is true after action Ai realizes consequence L in one of two cases: either p E ef, or p was true before Ai was executed and p $Z ei. Note that the node for p just after the execution of Ai has two incoming arcs: one from the node representing Ai, indicating which consequence was realized, and the other indicating p’s state immediately prior to Ai. l The SUCCESS node is a conjunction of the propositions in E evaluated after execu- tion of the last action. Each of these boolean functions can easily be coded into an appropriate link matrix. Section 5.5 reports on an implementation of NETWORK that uses the IDEAL [54] influence-diagram processor, using the Jensen clustering algorithm. We consistently see l2 For simplicity, in Fig. 12 we assume that each action potentially affects every proposition and vice versa, and that the goal expression mentions every proposition. The implementation of NETWORK adds arcs only between nodes that influence one another. 264 N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 assessment time growing exponentially with the size of the plan. l3 One possible reason for this poor performance is that the causal network associated with a plan actually contains a lot of structure that is irrelevant to the goal, and this causes the propagation algorithm to do unnecessary work. For example, suppose that a goal proposition is actually made true by initial, and no action in the plan changes that proposition under any circumstances. The causal network must nonetheless propagate that persistence information through every stage of the plan’s execution, computing the probability of every trigger of every action in the process. This analysis suggests that a causal network could be built that eliminates this irrelevant structure; the REVERSE algorithm does just that. 5.4. The REVERSE assessment algorithm The REVERSE assessment algorithm is based on the insight that the plan’s causal link structure captures the information needed for assessment. Consider the causal link This link records the information that consequence L of Ai makes p true, and Ai,,zAj. that no intervening action makes p false as long as confronted threats to the link do not actually result in an interfering consequence. Note that p could also become true for some other reason; for example, some intervening action might make p true even though no link in the plan records this causal relationship. Thus directly examining a plan’s causal structure yields suficient conditions for goal satisfaction, and the assessed probability is a lower bound on the true probability of success. Another important feature of REVERSE is that because the link structure guides reasons directly about plans and NETWORK which explicitly reason assessment rather than just the plan’s actions, REVERSE with partially ordered actions, unlike FORWARD about every consistent total order. See also Section 6.1. REVERSE uses a plan’s causal link structure to construct an assessment expression, a boolean combination of terms that refer to earlier parts of the plan. I4 The idea is trigger is true if every component literal is true, and that a consequence’s (conjunctive) a single literal is true if any of the incoming (disjunctive) links make the literal true. Initially the assessment expression is the trigger of the goal’s SUCCESS consequence. The expression is then incrementally transformed by traversing the causal link structure according to the following rules: l The assessment expression for a trigger is the conjunction of the assessment ex- pressions of the subgoals corresponding to the trigger’s conjuncts. l The assessment expression for a subgoal is the disjunction of the assessment ex- pressions for all the links supporting the subgoal. l The assessment expression for a link is the assessment expression of the trigger for the link’s producing outcome, conjoined with a conjunction of the assessment I3 We are not promoting this method as the best candidate for solving the network-that is a topic for future research. Dean and Kanazawa [ 111 argue that a stochastic simulation technique might be more suitable, but also point out the absence of convergence bounds for these algorithms. Without a guarantee of convergence, our plan refinement algorithm is no longer sound nor complete. I4 Note that this is not an “expression” in the sense of Section 2.1: an assessment expression can be an arbitrary boolean formula, and the terms in the formula are not propositions. N. Kushmerick et al./Art$cial Intelligence 76 (1995) 239-286 265 expressions of the subgoals of the safety condition associated with confronted threats. The assessment expression is transformed using these rules until no more transformations are applicable. The probability of this expression can be then be computed directly. See Appendix B for a complete discussion of REVERSE. 5.5. Empirical conjhation So far we have motivated and described four plan assessment algorithms. We de- scribed one algorithm that directly implements the definition of success probability, and hinted at potential computational problems; we used these problems to suggest three alternative algorithms. Now we analyze in detail the performance differences between these algorithms. We have built three illustrative domains, each intended to produce different behavior in FORWARD, QUERY, and REVERSE. Each domain involves an initial action, a goal action, and a “template” for defining additional actions. By varying one aspect of the domain (e.g. the goal or the probability threshold) we can vary the number of actions required to solve the problem. For each domain, we analyze the time taken by each algorithm to assess the solution plan, as a function of plan length. 5.5.1. A domain favoring FORWARD One might think that since FORWARD and QUERY are exploring the same state space-the first blindly and the second in a manner sensitive to the query-that QUERY would always outperform FORWARD. The domain shown in Fig. 13(a) shows this is requires time linear in the length of the plan, while false: in this domain FORWARD QUERY and REVERSE require time exponential in plan length. (NETWORK poorly in all of our domains, as discussed in Sections 5.3 and 5.5.4.) performs Action Ai makes the goal proposition G true with probability 0.5 and makes propo- sitions pt, . . . , pi false with probability 1.0. A successful N-action plan is of the form (Ai):, ; this sequence makes G true with probability 1 - ( l/2N). We therefore can vary the threshold r to change the length of the plan: T = 0 requires a zero-action plan, T = 0.5 requires a one-action plan, and so on. Fig. 13 (b) shows that FORWARD will project the N-action plan without a proliferation of states; FORWARD does well in this domain because after each action in the plan there are only two states with nonzero probability. QUERY will have trouble with this domain because of a heuristic it uses when deciding what parts of the tree to make explicit: it considers actions from latest to earliest in deciding what consequence classes to build. Consider the case N = 2, so r = 0.75 and the successful plan has actions At and A2 in that order. When QUERY is asked to assess the probability of G it first considers A2 and decides to split the action into two classes that differ on their effect on G. The tirst class consists of just the (Y consequence, and the second class is {/3, y}. At this point there are two possible completions to the plan, both with probability 0.5. G is true in the first but its value could be either true or false in the second, so the bound on G’s probability is [ 0.5,l .O] , which is ambiguous with respect to the threshold. QUERY next considers At and splits its consequences into the same 266 N. Kushmerick et al. /Artijicial Intelligence 76 (1995) 239-286 (b) (IP,~P~~....PN~GL A2 0.5)--t((p,,p,.....d,.G), 0.75) AN __) ((PI,P~,...,P~,G), I-lRN) ((P~.P~~...~PN!~~ 1.0) A, . . . -c (IP,.Pz +.... PN.GI. 0.3 A?, ((p,.p,....,p,,~).o.?5) AN -((p,.p+pN.t$ lRN) (cl Fig. 13. The domain shown in (a) is efficient for FORWARD because after projecting each action there are only two states with nonzero probability, as shown in (b); (c) illustrates how QUERY’S tree branches unnecessarily at each action. 0.25, and G is true in three of them. So now the bound on G’s probability and QUERY terminates. two classes, {(Y} and {/3,r}. Now there are four completions probability is [0.75,0.7_5], would have realized in the LY branch of the tree (i.e. given that G was already the tree would grow by only one completion matter which predefined order is chosen for splitting, pathological that the three consequences for that heuristic. If QUERY had divided At into classes first it of A2 have the same effect on G’s state In that case the size of in the plan. But no that are there will exist examples for each new action true). to the plan, each with REVERSE’S performance will degrade because each additional action in the plan adds to the assessment in Appendix B, REVERSE takes time that is exponential link to the goal proposition, which adds another disjunct and as we describe an additional expression, in the number of disjuncts. Fig. 14 shows performance statistics average time to perform a single assessment for the domain shown in Fig. 13, measuring the as the function of the number of actions N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 261 0 1 2 s 4 3 plan length [acuons] 6 I 8 Fig. 14. Average assessment time for plans in FORWARD'S domain. in the plan. t5 As expected, FORWARD’S assessment time grows linearly with plan size; the other algorithms required time exponential in the number of actions in the plan. 5.5.2. A domain favoring QUERY Fig. 15 (a) shows a domain favorable to the QUERY algorithm. We vary the length of a solution plan by varying the number of conjuncts in the goal: a goal of the form {Ply. * * , pi} requires a plan with N actions. Each action Ai has two consequences, but the distinction among the consequences is irrelevant to whether the plan satisfies the goal. None of the other algorithms recognize this feature of the domain: the state space explodes for FORWARD, and REVERSE must explicitly consider the disjunction of the two links that support each goal conjunct. Fig. 15(b) shows the explosion in the state space for FORWARD’S projection. The result is a state distribution of size 2N; each state has probability 1/2N and the goal is true in all of them, thus the plan is successful with probability 1. QUERY’S projection, on the other hand (Fig. 15(c) ) does not branch at all. Each of the N actions has a single relevant consequence class, in which the corresponding proposition is made true. REVERSE has trouble with the domain once again because of disjunction in the assessment expression: each conjunct pi in the goal expression has two links pointing to it, from e: and eg. Fig. 16 shows average assessment time for this domain, again as a function of plan size. QUERY’S assessment time grows linearly with plan size, the others grow exponen- tially. l5 All experiments were performed on a Sun SPARC-IPX. Since the inter-run variation was negligible, the confidence intervals for average data were too small to plot. 268 N. Kushmerick et al./Artificial Intelligence 76 (1995) 239-286 (a) (b) _ _ (lP,.P2v...PNJr,.x* . . . . XNL 0.5) ((P,.P~.....PN.x,,xz,...,XN), 1.0) A, c ((PI& . . . . . PN,ji,.XZ . . . . . x,t. 0.5) (lP,.Pz . . . . . PNJ,.X2....xNL 0.25) < AN (lP,,P* . . . . . PN,X,,XZ ,..I x,1. lRN) < A, GP,.P* I..., pNJ,.!i~ ,... .xfq). 0.25) ((p,.p2 __ . . . . . PpJ,X,.XZ ,.... x,t.o.m . . . < A2 - -- (lP,.P2 . . . . . PN.X,J~ ,.... x,t. 0.25) Cc) . . . < A~ __ _ (lP,,P2 1.... Pp.J,X,.X~ . . . . . X,). l/2? Fig. 15. The domain shown in (a) is efficient for QUERY. Although FORWARD doubles the size of its state set after every action (b), QUERY makes no irrelevant distinctions (c) : i i : I i FORWARD - QUERY -i-s- NElWORK -B--.. REVERSE *- 0 I 2 3 5 4 plan length [actions] 6 7 R Fig. 16. Average assessment time for plans in QUERY'S domain. N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 269 . . . z?&y=o.5 . . . ;jy=o.g ~~~~, Fig. 17. An efficient domain for REVERSE. 0 1 2 3 4 5 plan length [actions] 6 7 a Fig. 18. Assessment time for REVERSE'S domain. 5.5.3. A domain favoring REVERSE We noted above that REVERSE has problems with plans that have multiple supporting causal links since these cause long assessment expressions. REVERSE works best in domains in which propositions do not require multiple support, i.e. in which each proposition is supported by a single causal link. Such is the case in the domain appearing in Fig. 17. There is a single line of causal support from the LY consequence of the initial action to goal. Thus the assessment expression is of constant length regardless of the plan’s length. Both QUERY and FORWARD have to directly consider the entire collection of states with nonzero probability, which grows exponentially with the length of the plan. Fig. 18 confirms our expectations: assessment time for REVERSE increases linearly with plan size, while both QUERY and FORWARD are exponential. QUERY does some- what better in the limit than does FORWARD because it can ignore the distinction between the two consequences that do not generate the proposition required in the next action-it produces two branches per action whereas FORWARD generates a three-fold increase in the size of the state distribution after every action. 55.4. The NETWORK algorithm We did not discuss the NETWORK algorithm above, for reasons that should now be clear: its performance was dominated by the other algorithms, and it performed essentially the same on all examples. We noted above that a generic clustering algorithm 270 N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 Table 1 Total planning time (CPU seconds) for two problems and four assessors Algorithm FORWARD QUERY NETWORK REVERSE Problem Slippery Gripper Bomb/Toilet 4.5 8.0 179.9 404.9 6.9 64.9 152.0 6736.0 is probably property of the network. inappropriate for this application, since it does not exploit the Markov 6. The assess-refine interface The experiments reported in Section 5.5 were valuable in confirming our intuition about the relationship between domain characteristics and the performance of assess- ment algorithms, but they should be interpreted narrowly for several reasons: they involved very small, carefully constructed domains, and they measured assessment time in isolation, without regard to the total time spent generating a successful plan. As a preliminary effort toward a more thorough empirical study we tested the four algorithms on two additional problems. The first is the example used throughout this paper, which we will refer to as the Slippery Gripper problem. The second is an extension of Moore’s [40] Bomb and Toilet problem, which we describe below: A robot is given two packages, and told that exactly one of them contains a bomb. It wants to defuse the bomb, and the only way to do so is to dunk the package containing the bomb in the toilet. Placing a package in the toilet might (with probability 0.05) clog the toilet, and that is to be avoided. Suppose we want to achieve both goals-the bomb defused and the toilet unclogged- with probability at least 0.9. The obvious plan is to dunk both packages, guaranteeing that the bomb is defused and incurring only a small risk of clogging the toilet. Indeed, BURIDAN builds the plan shown in Fig. 19. Table 1 shows the total planning time required by the four assessment algorithms, on both the Bomb/Toilet and Slippery Gripper problems. The real surprise here is the poor performance for REVERSE: FORWARD runs about 100 times faster that REVERSE on the Slippery Gripper problem, and about 1000 times faster on the Bomb/Toilet problem. Was the assessment of the plans generated in solving these domains pathologically difficult for REVERSE? Table 2 indicates that this is not the case: although REVERSE was the fastest assessor for Slippery Gripper and the second fastest for Bomb/Toilet, it caused many additional plans to be generated and assessed, and this resulted in significantly slower planning performance. In hindsight, the reason for this behavior is clear. Recall that a fundamental differ- ence between REVERSE and the other algorithms is REVERSE’s reliance on the plan’s N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 271 Fig. 19. BURIDAN’S “package 2 contains bomb”; TC means “toilet clogged”; D means “bomb defused”. solution to the Bomb/Toilet problem. Bl means “package 1 contains bomb”; 82 means Table 2 Average assessment time per plan (CPU microseconds), and number of plans assessed before returning a solution Slippery Gripper Bomb/Toilet Problem Algorithm Time/assess Number assessed Time/assess Number assessed FORWARD QUERY NETWORK REVERSE 11.9 429.6 1521.0 10.6 119 119 119 4756 8.2 251.9 636.7 85.2 239 239 239 24420 and consuming propositions, REVERSE underestimates this happens, REVERSE might believe structure. When a plan doesn’t contain all possible causal-link consequences goal achievement, whereas FORWARD, QUERY and NETWORK compute When succeed, even WARD, QUERY, and NETWORK which requires bound. of the exact value. to In these cases, FOR- than REVERSE that REFINE add more causal links before REVERSE can compute a tight in fact the plan does represent a solution. terminate planning much sooner links between producing is not sufficiently the probability that the plan though likely can Our hope was that REVERSE would run faster using in the plan’s link structure, and that speed would offset the fact that the planner might need to iterate a few more times to produce a complete plan. Although neither hope is manifested in these examples, should be interpreted with caution. Rvo factors tend to make REVERSE look worse than it otherwise might: these experiments the cached information 272 N. Kushmerick et al./Artijcial Intelligence 76 (1995) 239-286 l These domains are very small, and contain very little irrelevant information. There are few operators, few propositions in the state space, and few consequences per action. All these factors conspire to make FORWARD look good: both QUERY and REVERSE spend computational effort trying to separate relevant aspects of the problem from irrelevant. If there are no irrelevant aspects to the problem, this effort is obviously wasted. l Little attention was given to search-control issues. Our plan refinement algorithm used a simple search-control policy of favoring plans with fewer links and fewer actions. In reality the policy amounted to breadth-first search through the plan space. Better search control would direct the refinement algorithm toward a complete plan more quickly. We address this issue briefly in Section 6.2. 6.1. Reasoning about partial orders Recall that the assessor is given a partially ordered plan, yet it must reason about total-order completions of that plan. FORWARD and NETWORK deal with partial orders in the obvious way: they generate all completions, assess each one individually, and take the minimum. REVERSE can cope with partially ordered actions in the sense that once threats are eliminated from a plan, an action unordered with respect to a link can never decrease the probability that the proposition will be supported. Since REVERSE computes a lower bound, it can safely ignore any non-threatening actions that might be ordered within the scope of a link. QUERY reasons quite deeply about partial orders: if the order of actions within a partial order cannot affect the value of a query proposition, it will compute the proposition’s truth value without exploring any completions of that order. But in reasoning explicitly about partial orders, QUERY has usurped some of the functionality of the plan refinement algorithm. In particular, the bound QUERY returns on a plan’s success is the minimum that can be guaranteed from any completion of the plan, whereas the bound REVERSE returns is on what can be guaranteed from every completion. QUERY tells the refinement algorithm that some completion of its current plan will succeed, but the planner has to figure out which one. l6 Having the assessor reason about partial orders is a potentially powerful form of search control: the assessor can reason more efficiently about partial orders and notify the planner when it finds a successful plan, thus saving the planner from applying its slower, more general refinement methods to the same task. As a particularly simple example of such a strategy we modified FORWARD to return the maximum probability over all possible completions instead of the minimum. and NETWORK As Table 3 demonstrates, the ability for an assessor to hasten recognition of a to- tally ordered solution can reduce planning time considerably. The improvement is only realized for Slippery Gripper, because the order of the actions in this domain is sig- nificant (and therefore BURIDAN must make more ordering decisions before terminat- I6 Alternatively we could extend the interface between assessor and refinement so the former would return the successful completion. N. Kushmerick et al./Artijicial Intelligence 76 (1995) 239-286 273 Table 3 Total planning time (CPU seconds) is reduced when the assessor recognizes total orders that maximize goal probability, enabling early termination Algorithm FORWARD FORWARD-MAX NETWORK NETWORK-MAX Problem Slippery Gripper Bomb/Toilet 4.5 0.53 179.9 43.6 6.9 7.0 152.0 151.3 ing) whereas the order of the two dunk actions in Bomb/Toilet BURIDAN can leave the actions unordered). is not significant (so 6.2. Search control Although we have only started to address the question of search control, it is clear that an assessment algorithm might be able to provide information that would guide the process of plan refinement. We already saw one example where this information is readily available: the assessor tells the refinement algorithm that a solution can be found by imposing additional order on its current plan (i.e. without adding any new actions). In fact, a powerful way to view the assessment task is as one of discovering flaws in the plan and communicating that information back to the planner [ 26,28,53]. Our QUERY algorithm builds a structure called a scenario that is the basis for assessment, but is also a temporal trace of the plan’s execution. One can identify from this structure the point at which the plan’s probability of success decreased, and why. This information could be exploited in deciding which refinement to apply next. REVERSE could supply similar information: its assessment expression captures how likely various propositions are to be true at various points in the plan. One could trace back through the assessment structure to find those propositions that are either unlikely to be true, or likely to be clobbered. Once again this information could guide the establishment of new links, or the confrontation of threats. 6.3. Summary This section explored the interplay between plan assessment and plan refinement. In hindsight, BURIDAN’S simple architecture seems problematic. In order to increase plan- ning performance, it will be necessary to create a more sophisticated interface between plan assessment and refinement. Many factors influence overall planning performance: speed of assessment, the tightness of probabilistic bounding calculations, and the type of search control guidance that the assessor can provide. 274 N. Kushmerick et al. /Art@cial Intelligence 76 (I 995) 239-286 7. Related work Related work can be found in several areas: other Al approaches to probabilistic planning, robotic motion planning, decision models, and classical planning and plan evaluation techniques. 7.1. Probabilistic planning Several early pieces of work [21,41] cast planning in probabilistic or decision- theoretic terms, but did not provide concrete representations or algorithms to solve the problem. More recent work divides according to how the planning problem is defined, and how states and operators are represented. Markov decision processes Several research efforts (e.g. [ 10,331) adopt a planning model based on fully ob- servable Markov processes. There are two main differences between this work and ours. First of all, the algorithms operate directly on the state space rather than on its com- ponent propositions, and the actions are represented directly as probabilistic mappings from states to states-the algorithms do not manipulate symbolic action descriptions. (Koenig shows a translation from sTRIps-like symbolic operators to the transition-matrix representation, but the solution algorithm does not use the symbolic representation.) A more important distinction is that these approaches build a reaction strategy rather than a plan. A reaction strategy is a policy that dictates the action the agent should take for each state in the state space. A plan, on the other hand, is a sequence of actions that the agent executes without regard to the state. The assumption behind the Markov decision process approach is full observability: that the agent will always know what state it is in while it is executing its strategy-in other words, that it will be provided with accurate and immediate information about the new world state every time it executes an action. the agent will get no additional A plan embodies the opposite assumption-that it might as well plan what to do information about the world at execution time-so ahead of time. Recent extensions to BURIDAN [ 14,151 take a middle ground: that information is available at execution time, but it has to be explicitly gathered, and is potentially inaccurate. Symbolic planning approaches Farley [20] proposes a similar action representation, though he attaches probabilities directly to postconditions rather than to sets of postconditions. His planning algorithm is linear and “progressive”: it starts from the initial state (assumed unique) and builds linear plan sequences, always adding steps to the end of the plan. Manse11 [36] proposes a strategy in which the planner attacks each possible initial world state in isolation (beginning with the most likely), and uses a deterministic hi- erarchical planning algorithm to build a plan for each. After these plans are built, the algorithm tries to merge the distinct plans. This approach is similar to the “robustifi- cation” approach proposed by Drummond and Bresina [ 161. BURIDAN can be forced N. Kushmerick et al./Arhjicial Intelligence 76 (1995) 239-286 215 to operate in this mode (by allowing it to link to only a single initial state at a time), though the advantage of postponing the merging process to the end of the planning episode is not clear. Preliminary work by Goldman and Boddy [23] attacks a similar problem: build- ing plans that are likely to achieve the goal, where likely is defined in terms of a threshold. They develop an extended action and plan representation that incorporates observations and contingencies, so a comparison to C-BURIDAN [ 14,151 is more apt. Their approach to planning is quite different from ours, however. They use a determin- istic planner (based on CNL.P [ 511) and they manage uncertainty using an external probabilistic network model to assign probabilities to propositions with unknown truth values. Splitting the problem into a deterministic planner and an external mechanism for managing uncertainty is more similar to Mansell’s approach than to ours. 7.2. Robotic motion planning Robotics researchers have also considered the problem of planning with actions whose effects are uncertain. For example, Lozano-Perez, Mason and Taylor [35] introduced a backward chaining strategy (LMT) for motion planning given sensing and control uncer- tainty which has been extended by Erdmann [ 171 and others. An interesting connection between these approaches and ours is the analogy between the use of compliant motion and conditional effects for reducing uncertainty, but there are more differences than similarities. Most obvious is their emphasis on geometry. Second, they model sensing actions (but see Brost [4] ) which are omitted from BUFSDAN, though the extensions cited above address that deficiency. Third, their preimage notion of uncertainty bears more of a resemblance to a possible-worlds model of incomplete information than our probabilistic model. Fourth, their focus is on planning strategies that are guaranteed to succeed despite uncertainty (as are the Markov-process approaches above) ; in contrast, BIJRIDAN plans need only have probability of success that exceeds a user specified threshold. Donald’s work [ 131 extends the basic LMT paradigm to handle incomplete knowledge of the world’s geometry and to provide error detection and recovery. 7.3. Graphical decision models Work on graphical probabilistic and decision models (see Howard [ 3 11, Pearl [ 431, or the overview in [ 12, Chapter 71) also deals with decision making and planning problems, but has focused more on solving a given probabilistic or decision model whereas our algorithm interleaves the process of constructing and evaluating solutions. The problem modelled by an influence diagram involves choosing options from a fixed set of choices rather than constructing a course of action dynamically from a goal description. Recent work, however, has recognized the importance of interleaving the model- construction and the model-solution problems, both in general [24] and as applied to the planning problem in particular [ 501. Also see [ 31 for a survey of work in this area. 276 N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 7.4. Probabilistic temporal reasoning The representation for the NETWORK algorithm is similar to the network proposed by Dean and Kanazawa [ 111. As we discussed in Section 5, a totally ordered plan can be formulated as a probabilis- tic network allowing assessment to be performed using standard propagation techniques 1431. Although our experiments with the NETWORK assessment algorithm showed that the Jensen clustering algorithm is probably inappropriate for problems of this type, other approaches might be more suitable. Dean and Kanazawa [ 111 advocate stochastic sim- ulation techniques which would provide an approximate assessment of a totally ordered plan. A simulation can provide an approximate assessment of a totally ordered plan in polynomial time which could lead to an “approximately sound and complete” planning algorithm. A stochastic simulation technique still faces a potential combinatorial explosion in assessing a partially order plan, however, since in the worst case it would have to assess all consistent total orders. 7.5. Action representation and plan evaluation Our action representation comes from Hanks’ work [27-291 on probabilistic projec- tion. Chrisman [6] develops an action representation and projection rule for planning under uncertainty, and Martin and Allen [37] develop statistical techniques to gather probabilities like the ones our algorithm uses. None of this work directly addresses the problem of plan generation. The QUERY algorithm is described in [27,29] ; Dnmunond [ 161 presents an alterna- tive algorithm for a similar problem. Haddawy and Hanks [25] motivate building a planner such as BURIDAN. They provide a framework for constructing a restricted class of utility functions for use by a decision-theoretic planner and show circumstances under which determining whether one plan dominates another reduces to establishing bounds on the probabilities of particular propositions at particular times, which is precisely what our plan assessment algorithms compute. Doyle and Wellman [ 551 discuss the general problem of modular specification of a planner’s objectives in a decision theoretic framework. They exploit multiattribute utility theory to devise techniques for composing separate preference specifications. 7.6. Classical planning Dealing with state-dependent effects is an essential requirement for any useful prob- abilistic planner. In this regard BURIDAN can be seen as generalizing the work on planning with deterministic conditional effects, e.g. in [ 7,46,48]. A deterministic form of confrontation is used in UCPOP [ 481. Pednault’s ADL language allowed for disjunc- tive effects and he used them to solve a simple symbolic version of the “Bomb in the Toilet” example [45] which we extended in Section 6. However, no implementations of ADL (e.g., Pedestal [ 391 and UCPOP [ 481) have implemented the functionality of disjunctive effects, which BIJRIDAN does. N. Kushmerick et al./Art$icial Intelligence 76 (1995) 239-286 211 8. Conclusions BLJRIDAN represents a significant step in the development of practical algorithms for probabilistic planning. While much work remains to be done, BLJRIDAN provides a profitable basis for future study. 8.1. Implementation BURIDAN is fully implemented in Common Lisp and has been tested on many ex- amples including the ones presented in this paper. The implementation is robust (e.g. successfully searches tens of thousands of plans). Although the code has not been optimized for speed or search control, we feel that it is a solid foundation for fu- ture research. In addition, it would be excellent in an instructional setting. Send mail to bug-buridanQcs code via anonymous FIT? . Washington. edu for instructions on acquiring BURIDAN source 8.2. Summary In this paper we’ve reported on several significant advances: ( 1) We have extended the classical planning representation to handle uncertainty in the initial world state (via probability distributions over world states) and in the effects of actions (via mutually exclusive and exhaustive triggers paired with sTRIps-like effects). (2) We provided a precise probabilistic semantics for our representation. Execution of an action causes a transition from one state distribution to another. (3) We described BUFULMN, an implemented algorithm for probabilistic planning, and proved that it is both sound and complete. (4) We compared the efficiency of the FORWARD, and RE- VERSE probabilistic assessment algorithms both analytically and empirically. We characterized the strengths of each algorithm, and observe that none of the four is clearly dominant. QUERY, NETWORK (5) We noted that the fastest assessor does not necessarily lead to the fastest planner and explained why. We argued that the refine-assess architecture could be im- proved by allowing the plan assessor to provide more guidance to the plan refiner. As a simple example of this strategy, we demonstrated that considerable speedup is possible when the assessment algorithm returns action-ordering information in addition to its probability calculation. 8.3. Future work We hope to extend BURIDAN in many directions. From a purely practical perspective, BURIDAN'S functionality is limited by its propositional representation, so we plan to implement a lifted [ 38,521 version using the codesignation constraint code developed [ 481. The major challenge of this endeavor is devising an efficient means for UCPOP 278 N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 for handling the disjunctive bindings that could result when a lifted trigger condition is supported by multiple causal links from different ground consequences. Another extension would allow BURIDAN to handle probabilistic exogenous events and incorporate the model of sensing and information advanced in the UWL language [ 191. We’d like to integrate Peot and Smith’s [ 511 algorithm for generating conditional plans with this framework and to consider interleaved planning and execution [ 2,34,42] as well. Recent work on C-BURIDAN [ 14,151 has addressed some of these issues. We also hope to introduce an explicit temporal model (perhaps using ideas from ZENO [ 47,491) so we can represent deadline goals. This would allow us to consider integrating our probabilistic plan refinement algorithm with the utility model presented in [25]. On the algorithmic side we have just begun to explore methods of controlling the search for good plans. As Section 6 demonstrates, there are a number of important architectural issues which deserve exploration. We hope to develop a more sophisti- cated refine-assess interface so that the computational expense of plan assessment pays dividends by guiding subsequent refinements. We also wish to evaluate additional as- sessment methods (e.g. incremental assessment, stochastic simulation, etc.) and their relationship to plan refinement. Acknowledgments We gratefully acknowledge the comments and suggestions of Tony Barrett, Tom Dean, Denise Draper, Mike Erdmann, Keith Golden, Rex Jacobovits, Oren Etzioni, Neal Lesh, Judea Pearl, Mark Peot, Mike Wellman, Mike Williamson, and the anonymous reviewers. This research was funded in part by National Science Foundation Grants IRI- 9206733 and IRI-8957302, Office of Naval Research Grant 90-J-1904, and the Xerox Corporation. Appendix A. Proof of completeness Theorem 14 (Completeness). Let A = (?I, 6, r, A) be a planning problem and let (Ai):, be an essential solution (i.e., no proper subsequence is also solution} of A. Then there exists a sequence of nondeterministic choices such that BURIDAN( A) will return (Ai):,. To finesse issues of search control, we use (Ai): as an oracle to guide the construction of the partially ordered plan; McDermott [ 391 refers to this technique as a clairvoyant algorithm. Our implementation uses exhaustive search to ensure that every sequence of nondeterministic choices is eventually considered. We first establish a useful lemma. Recall that plan data structures contain a set of subgoals: S = {. . ., p@Ai,. e .}. We introduce one new piece of terminology to concisely N. Kushmerick et al. /Artijicial Intelligence 76 (1995) 239-286 219 refer to the result of executing action subsequences: let SV: be the state distribution produced by executing (AJLj in SD. If k < j then SD: z SD. Lemma A.1. Let A = (St, 4, r, A) be a planning problem and suppose that a call to BURIDAN( A) yields values (A, 0, L, S) such that (Ai)! is a consistent topological sort of d (excluding the initial and goal actions). Let E be an expression composed of literals all of which are subgoals in S for the same action A,,,, and let Ar be some action not in A Zf there exists 1 < m such that P[ElS~,] >P[EIS@], then REFINE can make a sequence of nondeterministic choices that will add At to A Proof. Our proof is by induction on m. Base case: m = 2. In this case 1 = 1 and we assume that P [E 1 EXEC( At, SD; )] > P [E 1 SD;], which is equivalent to P[& 1 EXEC( AI, SD)] > P [E 1 SD]. By definition, the only way that the probability of E can be greater after executing Al is if doing so increases the probability associated with the states containing E. But the only way that this could happen is if A1 has an conseQuence containing p for some p E E. But in that case REFINE line 2.a could choose to add AI to the plan since p@A2 E S. Inductive step: m > 2. The inductive hypothesis guarantees that if there exists some then Al can be added to the plan. We 1 < m - 1 such that P [E 1 Stir-‘] > P [& I SD;-‘] need to show that this holds for 1 = m - 1 as well. Suppose that P[E I ST-‘] > P[& I ST-‘]. Three (exhaustive but non-exclusive) cases can explain this relationship: ( 1) The increase in the probability of I happens before A,,,_] is executed-in other . But in that case the inductive assumption words, P [S I SDy-2] > P [E I Sq-'1 directly indicates that At could be added. (2) The increase in the probability of E occurs because including Ar causes A,_, (tr-' to contribute additional probability mass to E. Specifically, A,__, contains a consequence , p?-r , em-‘) such that er-’ makes some proposition p in & true, and P [t:-’ I SDf-“1 > P[tF-’ I SZ$‘-‘1 . But then a nondeterministic choice in REFINE line 2.a could choose this consequence to support p. So for a nondeterministic choice in REFINE line 2.a could make q@A,_ 1 every q E tr-’ a subgoal as well, the inductive assumption applies to tr-‘, and At could be added. (3) Finally, the increase in the probability of E might occur because including A, causes A,,,_, to contribute less probability mass to an consequence that makes E false. Note that in this case & must have nonzero probability before A,_, is executed, i.e. P [E I SDJ’-‘1 > 0. But if this is the case then for every proposition p E & there must be some action Ai with a consequence L that contains p, and REFINE line 2.b could add causal links Ai,‘%Am, for each of them. It must also be the case that some consequence in A,,,_1 tends to make & false, and Al tends to make that consequence less likely. In other words, A,_, must 280 N. Kushmerick et al. /Artijcial Intelligence 76 (1995) 239-286 contain a consequence (tf-' furthermore P [tc-’ 1 SDD);~-‘] < P [tf-’ 1 sV-‘] , pz-’ , ef-‘) . such that j5 E ec-‘, where p E E, and But in this case, FEFJINE would recognize the K consequence of A,__, as a threat and line 3.c could confront the threat. Confronting the threat means that the literals in the triggers of all non-interfering consequences of A,,,_, could be adopted as a subgoal in S (line 3.c.iii). Since Definition 1 states that an action’s triggers are mutually exclusive and exhaustive, P[tf-’ 1 ST-“] < P[t;-’ les that the probability of at least one of A,_I’s non- intetfering triggers will have greuter probability when Al is executed. But if so the inductive assumption is satisfied and A1 could be added to the sequence. 0 (my] pl’ . lm We are now ready to tackle the main theorem. Since the proof is somewhat complex, we sketch the high level concept before delving into the details. The proof method is induction and (unsuprisingly) the induction step is the crux. We demonstrate that a sequence of nondeterministic choices exists which returns an N step plan for a planning problem d by constructing a modijied problem which can be solved in N - 1 steps. Since the induction hypothesis states that BURIDAN can solve this easier problem, we need only show how the choices made for the modified problem lead to choices that solve A itself. Lemma A. 1 makes this (relatively) straightforward. Proof (Completeness). Given a planning problem A and an essential solution (Ai):, , we need to show two things. First we must show that REFINE can make a sequence of nondeterministic choices resulting in a plan consistent with (Ai):, . Second we must show that the FORWARD assessor will recognize that plan as a solution. Our proof is by induction on N, the number of actions in the plan. Base case: N = 0. If N = 0 then the goal is sufficiently likely without any actions being added: P [B I SD] 2 7. A call to BURIDAN will create the null plan for A and for assessment. Since there are no actions in A - {Ao, Ao}, immediately call FORWARD line 3 returns the probability of the single total order consistent with this FORWARD plan, which by assumption exceeds the threshold. BURIDAN calls TOTAL-ORDER and returns the empty sequence. Inductive step: N 2 1. The inductive assumption ensures that clairvoyant BURIDAN correctly generates solutions of the form (Ai):, for m < N. We now show that BURIJIAN finds a solution for N-action plans as well. Let A’ be the planning problem (EXEC( A,, SD), 6, T, A). By Definition 4, the length N - 1 action sequence (Ai):? is an essential solution to A’. Clairvoyant BURIDAN(A') will therefore generate a partially ordered plan, P’ = (A’, O’, _C’, S’), such that (Ai)! is a consistent topological sort. P’ is very similar to the plan that we are seeking, but its initial action is doing double duty, providing probability mass for propositions that SD and A1 provided collectively in the original solution. Now consider the execution trace of all nondeterministic choices made by clairvoyant BURIDAN while constructing P’ for A’. We can use this trace, with some modifications, to guide BURIDAN toward a solution to the original problem A. Since the only difference between A’ and A occurs in the initial state distributions SZ, N. Kushmerick et al./Artijcial Intelligence 76 (1995) 239-286 281 and EXEC(AI, SD), we need to guide BURIDAN's choice only when it tries to create a link from the initial action, Ao’-otherwise, plan refinement can proceed as it did when P’ was generated. Recall that Aa’ (the initial action of P’) corresponds to the state distribution EXEC(AI , CD). If P’ contains a link supporting p whose producing action is Ac’, then there exists an consequence of Aa or At that contains p. In that case we instruct BURIDAN to choose such an consequence and create a link from it. Note that this argument guarantees that BURIDAN will add actions AZ, . . . , AN to the plan (along with ordering constraints on them), but it does not guarantee that BURIDAN will add At to the plan: P’ might not contain a link whose producer is Aa’. But recall that no proper subsequence of (Ai):, P [G 1 my] > P [G 1 SD;]. is a solution, therefore Since 0 is a conjunction of propositions that have been adopted as subgoals in S, Lemma A.1 guarantees that there is a sequence of nondeterministic choices REFINE can make that will add At. At this point we have established that BURIDAN can add the right actions to A, but we haven’t yet guaranteed that it will add enough ordering constmints to 0. In particular we have not guaranteed that A1 will be constrained to occur first in the plan. If BUFUDAN fails to constrain At to be the first action in the plan, then FORWARD will iterate over all total orders consistent with 0 and one of these might achieve Q with probability less than 7, meaning that BURIDAN would fail to recognize the solution. We can show that it is a contradiction to assume that no sequence of nondeterministic choices will cause At to be ordered first in the plan. Let m 2 2 be the smallest number such that executing At before A,,, achieves the goal with some probability > T, while executing At immediately after A,,, achieves the goal with some probability < T. If so there must be a sequence of nondeterministic choices made by REFINEZ at lines 2.a and 3.c that create a causal link whose producer is A,,, and which is threatened by At. But if that is the case, REFINE line 3.a could demote At by adding At < A, to 0. In summary, if (Ai),zIt is an essential solution to a planning problem, then a sequence of nondeterministic decisions can cause BURIDAN to add each of actions At through AN to the plan, along with all relevant ordering constraints. FORWARD will return Min 2 7, and clairvoyant TOTAL-ORDER will return (Ai)Zt which is a solution to d. (cid:144)i Appendix B. The reverse assessment algorithm REVERSE uses the plan’s causal links to evaluate a plan. The probability that a proposition holds when a particular action is executed can be estimated by traversing the link structure that provides causal support to the proposition. The idea is to traverse the links, constructing an assessment expression, a boolean combination of causal links, triggers, subgoals and of terms. Starting from the trigger of goal’s SUCCESS outcome, the assessment expression is incrementally transformed as follows: l The assessment expression for the trigger of a consequence is the conjunction of the assessment expressions of the subgoals corresponding to the trigger’s conjuncts, conjoined with the consequence’s probabilistic term. 282 N. Kushmerick et al. /Artijicial Intelligence 76 (I 995) 239-286 The assessment expression for a subgoal is the disjunction of the assessment ex- pressions for all the links supporting the subgoal. If a subgoal has no causal support then no transformation is made. The assessment expression for a link is the assessment expression of the trigger for the link’s producing outcome, conjoined with a conjunction of the assessment expressions of the subgoals of the safety condition associated with confronted threats. These transformations are applied repeatedly until the expression is a boolean combina- tion of only subgoals without causal support and probabilistic terms for the consequences that constitute the plan’s causal structure. This expression can then be evaluated directly. Fig. B.l precisely specifies the REVERSE algorithm. REVERSE computes a lower bound on the probability of plan success. To understand and the other algorithms is that this, note that main difference between REVERSE whereas the other algorithms take into account all causal relationships implicit in the plan, REVERSE reasons about only those causal relationships explicitly represented in the plan’s link structure. There are therefore two ways in which a probability computed using causal links might differ from the value returned by the exact algorithms: l There might be a action that produces a proposition that is required by a subsequent action, yet REFINE has not installed a link between those two actions. In that case REVERSE may underestimate the proposition’s probability. l There might be a threat to an existing link that has not been resolved yet by the refinement algorithm. In that case REVERSE may overestimate the probability of the link’s supported proposition. We force REVERSE to produce a lower bound on probabilities by ignoring links that are threatened (see the PROD function in Fig. B. 1) and by leaving subgoals with no causal support untransformed (line 2.b applies only if the subgoal has causal support). When a plan is refined so that all threats are resolved and all subgoals are supported in all possible ways, then REVERSE computes the same probability as the other assessment algorithms. We have not fully investigated the computational complexity of REVERSE, but clearly the algorithm runs in time exponential in the number of disjuncts in the disjunctive normal form of the assessment expression: line 5 computes the probability of each conjunction generated by line 4, and the number of such conjunctions is exponential in the number of disjuncts. Example. We now show how REVERSE assesses the plan shown in Fig. 9. From line 1 of Fig. B.l, the initial assessment expression is simply tz. This gets transformed by several applications of lines 2.a and 2.b as follows: tz + p: A HB@& A BP@AG A GC@AG Each of these links is then expanded using line 2.~. Expanding AI,,TAG is straightforward: these links just expand into their producing outcomes. But Ao,~%AG was threatened by paint (AZ) and this threat was resolved by confrontation. So in and A2,$& N. Kushmerick et al. /Artijicial Intelligence 76 (1995) 239-286 283 XEVERSE( P) 1. Initialize the assessment expression to tz. 2. Loop: Transform a term from the assessment expression as follows: a. tf+PtA A Q@Ai pet:: b. Q@Aj + V Ai,‘sAjGPROD( Q@Aj) Ai,hJ+Aj if PROD(Q@Aj) + 8 C. Ai,,zAj + tf A A sESAFE(Ai,‘J+Aj) S @Aj until no further replacements are possible (i.e., the assessment expression consists only of literals with no causal support and terms of the form pi). 3. Convert the assessment expression to disjunctive normal form. 4. Using the probabilistic axiom P[ A V B] = P[A] + P[ B] - P[ A A B 1, compute the set of disjuncts of the DNF expression that must be conjoined to compute the probability of the expression as a whole. 5. Compute the probability of each conjunction as follows: If the conjunction contains terms of the form of and pi, or Q@Ai and p@Ai, then a. the probability of the expression is 0 otherwise b. remove duplicate terms, substitute probabilities for the remaining terms (the value of each of, and 0 for each remaining Q@Ai), and c. multiply the results 6. Add or subtract (as appropriate) the probabilities as computed by line 5 for each of the conjunctions generated by line 4. ‘ROD ( Q @Ai) returns the set of P’s unthreatened causal links supporting p@Ai. SAFJE(Ai,‘zAj) returns the set of safety propositions corresponding to confronted threats against Ai,‘sAj in P. Fig. B. 1. REVERSE plan assessment algorithm. 284 N. Kushmerick et al. /Artificial Intelligence 76 (1995) 239-286 addition to the producing outcome, A~,,%AG sl@A~, and we have: expands into the safety proposition subgoal Now applying lines 2.a, 2.b and 2.c, we have: ,o;Atf,At;AtO,AS,@AG + & A ( GD@AI A p;) A @@AZ A p;) A pz A Az,&:AG + pz A (A,,,aGzA1 A p;) A (A,,,%Az A pi> A p”, A t; +pz A(tO, Apt,) A(t; A@ ApO,A(HB@Az A&) * p: A (P”, A P:, A <P: A ~$1 A P: A (4 ,a% A ~$1 =+ p: A P: A P: A P”, A P; A P: A <t: A P;> * P: A P”, A P; A P”, A P; A P: A P”, A P;. As this point, the termination condition of line 2 is satisfied, so line 3 transforms the expression into disjunctive normal form. Since it is already just a conjunction, no transformation is needed. For the same reason, line 4 is trivial: we use line 5 to assess the probability of the entire expression. The expression contains no contradictions, so line 5.a does not apply. Rather, line 5.b first removes duplicates, substitutes numbers for the remaining terms, and multiples: P~APO,AP~,AP~AP~AP~AP~AP~~P~AP~AP~AP~ + 1.0 x 0.7 x 0.95 x 0.9 =+ 0.5985. This example illustrates that the probability computed by REVERSE is a lower bound on the exact probability that a plan achieves the goal: the other algorithms return 0.7335 when they assess this plan. In order for REVERSE to realize this exact probability, additional causal links would need to be added to the plan. For example, the link represents a way that the goal GC might be achieved that REVERSE did not A,-$AG consider. References [ 1 I J. Allen, J. Hendler and A. Tate, eds., Readings in Planning (Morgan Kaufmann, San Mateo, CA, 1990). [ 21 J. Ambroshgerson and S. Steel, Integrating planning, execution, and monitoring, in: Proceedings AAAI- 88, St. Paul, MN (1988) 735740. [ 31 J. Breese, R. Goldman aud M. Wellman, eds., Notes from the Ninth National Conference on Artijicial Intelligence (AAAI-91) Workshop on Knowledge-Based Construction of Probabilistic and Decision Models (AAAI, 1991). N. Kushmerick et al/Artificial Intelligence 76 (1995) 239-286 285 [4] R. Brost, Automatic grasp planning in the presence of uncertainty, ht. J. Robotics Research 7 ( 1) (1988) 3-17. [5] D. Chapman, Planning for conjunctive goals, Artif Intell. 32 (3) (1987) 333-377. [6] L. Chrisman, Abstract probabilistic modeling of action, in: Proceedings 1st International Conference on AI Planning Sysrems ( 1992). [7] G. Collins and L. Pryor, Achieving the functionality of filter conditions in a partial order planner, in: Proceedings AAAI-92, San Jose, CA ( 1992). [8] G. Cooper, The computational complexity of probabilistic inference using Bayesian belief networks, Artif Intell. 42 (1990) 393-405. [9] T. Dean and M. Boddy, Reasoning about partially ordered events, Artif Intell. 36 (3) (1988) 375-W reprinted in: D.S. Weld and J. de Kleer, eds., Readings in Qualitative Reasoning about Physical Systems (Morgan Kaufmann, San Mateo, CA, 1989). [lo] T. Dean, L.P. Kaelbling, J. Kiian and A. Nicholson, Planning with deadlines in stochastic domains, in: Proceedings AAAI-93, Washington, DC ( 1993). [ 111 T. Dean and K. Kanazawa, A model for reasoning about persistence and causation, Comput. Intell. 5 (1989) 142-150. [ 121 T. Dean and M. Wellman, Planning and Control (Morgan Kaufmann, San Mateo, CA, 1991). [ 131 B. Donald, A geometric approach to error detection and recovery for robot motion planning with uncertainty, Arttf Intell. 37 (1988) 223-271. [ 141 D. Draper, S. Hanks and D. Weld, A probabilistic model of action for least-commitment planning with information gathering, in: Proceedings 10th Conference on Uncertainty in Artij’ical Intelligence ( 1994). [ 151 D. Draper, S. Hanks and D. Weld, Probabilistic planning with information gathering and contingent execution, in: Proceedings 2nd International Conference on AI Planning Systems (1994). [ 161 M. Drummond and J. Bresina, Anytime synthetic projection: maximizing the probability of goal satisfaction, in: Proceedings AAAI-90, Boston, MA (1990). [ 171 M. Erdmann, On motion planning with uncettainty, AI-TR-810, MIT AI Lab, Cambridge, MA (1984). [ 181 K. Erol, D. Nau and V. Subrahmanian, When is planning decidable?, in: Proceedings 1st International Conference on Al Planning Systems (1992) 222-227. [19] 0. Etzioni, S. Hanks, D. Weld, D. Draper, N. Lesh and M. Williamson, An approach to planning with incomplete information, in: Proceedings 3rd International Conference on Principles of Knowledge Representation and Reasoning, Cambridge, MA (1992); available via FTP from pub/ai/ at ftp.cs.uashington.edu. [20] A. Farley, A probabilistic model for uncertain problem solving, IEEE Trans. Syst. Man Cybernet. 13 (4) (1983). [ 2 11 J. Feldman and R. Sproull, Decision theory and artificial intelligence II: the hungry monkey, Cogn. Sci. 1 (1977) 158-192. [22] R.E. Fikes and N.J. Nilsson, STRIPS: a new approach to the application of theorem proving to problem solving, Artif Intell. 2 (3-4) ( 1971) 189-208. [ 231 R.P. Goldman and MS. Boddy, Epsilon-safe planning (forthcoming). [24] R.P. Goldman and J.S. Breese, Integrating model construction and evaluation, in: Proceedings 8th Conference on Uncertainty in Artt&al Intelligence, Stanford, CA (1992). [25] P Haddawy and S. Hanks, Utility models for goal-directed decision-theoretic planners, Tech. Rept. 93-06-04, University of Washington, Department of Computer Science and Engineering, Seattle, WA (1993),J. AI Res. (submitted); available via FTP from pub/ai/ at ftp.cs.uashington.edu. [26] K.J. Hammond, Explaining and repairing plans that fail, Artif Intell. 45 (1990) 173-228. [27] S. Hanks, Practical temporal projection, in: Proceedings AAAI-90, Boston, MA (1990). [28] S. Hanks, Projecting plans about uncertain worlds, Ph.D. Thesis, Yale University, Computer Science Department, New Haven, CT ( 1990). [ 291 S. Hanks, Modeling a dynamic and uncertain world II: action representation and plan evaluation, Tech. Rept., University of Washington, Department of Computer Science and Engineering, Seattle, WA ( 1993). 1301 S. Hanks and D. McDermott, Modeling a dynamic and uncertain world I: symbolic and probabilistic reasoning about change, Arttf Intell. 66 (1) (1994) l-55. [ 3 1 I R. Howard and J. Matheson, Influence diagrams, in: The Principles and Applications of Decision Analysis (Strategic Decisions Group, 1984). 286 N. Kushmerick et al. /Artificial intelligence 76 (199.5) 239-286 [321 S. Kambhampati, Characterizing multi-contributor causal structures for planning, in: Proceedings 1st International Conference on AI Planning Systems (1992) 116-125. [33] S. Koenig, Optimal probabilistic and decision-theoretic planning using Markovian decision theory, UCB/CSD 92/685 Berkeley, CA (1992). 1341 K. Krebsbach, D. Olawsky and M. Gini, An empirical study of sensing and defaulting in planning, in: Proceedings 1st International Conference on AI Planning Systems (1992) 136-144. [ 351 T. Lozano-Perez, M. Mason and R. Taylor, Automatic synthesis of fine motion strategies for robots, ht. J. Robotics Research 3 ( 1) (1984) 3-24. [36] T. Mansell, A method for planning given uncertain and incomplete information, in: Proceedings 9rh Conference on Uncertainty in ArtijCcal Intelligence, Washington, DC ( 1993). 1371 N. Martin and J. Allen, A language for planning with statistics, in: Proceedings 7th Conference on Uncertainty in Artijical Intelligence, Los Angeles, CA (1991). [ 381 D. McAllester and D. Rosenblitt, Systematic nonlinear planning, in: Proceedings AAAI-91, Anaheim, CA (1991) 634-639. [39] D. McDermott, Regression planning, ht. J. Intelligent Systems 6 (1991) 357-416. [40] R. Moore, A formal theory of knowledge and action, in: J. Hobbs and R. Moore, eds., Formal Theories of the Commonsense World (Ablex, Norwood, NJ, 1985). [ 411 J.H. Munson, Robot planning, execution, and monitoring in an uncertain environment, in: Proceedings IJCAI-71, London (1971) 338-349. [42] D. Olawsky and M. Gini, Deferred planning and sensor use, in: Proceedings, DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control (Morgan Kaufmann, San Mateo, CA, 1990). [43] J. Pearl, Probablistic Reasoning in Intelligent Systems (Morgan Kaufmann, San Mateo, CA, 1988). 1441 E. Pednault, Toward a mathematical theory of plan synthesis, Ph.D. Thesis, Stanford University ( 1986). [45] E. Pednault, Synthesizing plans that contain actions with context-dependent effects, Comput. Intell. 4 (4) (1988)356-372. [ 461 E. Pednault, Generalizing nonlinear planning to handle complex goals and actions with context-dependent effects, in: Proceedings IJCAI-91, Detroit, Ml ( 1991) 240-245. [ 471 J. Penberthy, Planning with continuous change, Ph.D. Thesis, UW CSE Tech Report 93-12-01, University of Washington, Seattle, WA ( 1993). [48] J. Penberthy and D. Weld, UCPOP: a sound, complete, partial order planner for ADL, in: Proceedings 3rd International Conference on Principles of Knowledge Representation and Reasoning, Cambridge, MA ( 1992) 103-l 14; available via FTP from pub/ai/ at f tp. cs. washington. edu. [49] J.S. Penberthy and D.S. Weld, A new approach to temporal planning (preliminary report), in: Proceedings of the AAAI 1993 Symposium on Foundations of Automatic Planning: The Classical Approach and Beyond ( 1993) 112-l 16. [ 501 M. Peot and J.S. Breese, Model construction in planning, in: Notes from the Ninth National Conference on Artificial Intelligence (AAAI-91) Workshop on Knowledge-Based Construction of Probabilistic and Decision Models (AAAI, 1991) 95-100. [ 5 1 ] M. Peot and D. Smith, Conditional nonlinear planning, in: Proceedings 1st International Conference on AI Planning Systems (1992) 189-197. [ 521 J. Robinson, A machine-oriented logic based on the resolution principle, J. ACM 12 ( 1) ( 1965). [53] R. Simmons, A theory of debugging plans and interpretations, in: Proceedings AAAI-88, St. Paul, MN (1988) 94-99. [54] S. Srinivas and J. Breese, IDEAL: influence diagram evaluation and analysis in LISP; documentation and users guide, Technical Memo 23, Rockwell International Science Center (1990). [ 551 M. Wellman and J. Doyle, Modular utility representation for decision theoretic planning, in: Proceedings 1st International Conference on Al Planning Systems ( 1992) 236-242. 