Available online at www.sciencedirect.comRArtificial Intelligence 155 (2004) 1–39www.elsevier.com/locate/artintA framework for linguistic modellingJonathan LawryDepartment of Engineering Mathematics, University of Bristol, Bristol BS8 1TR, UKReceived 13 November 2001; received in revised form 7 November 2003AbstractA new framework for linguistic reasoning is proposed based on a random set model of the degree ofappropriateness of a label. Labels are assumed to be chosen from a finite predefined set of labels andthe set of appropriate labels for a value is defined as a random set-valued function from a populationof individuals into the set of subsets of labels. Appropriateness degrees are then evaluated relativeto the distribution on this random set where the appropriateness degree of a label corresponds to theprobability that it is contained in the set of appropriate labels. This interpretation is referred to as labelsemantics. A natural calculus for appropriateness degrees is described which is weakly functionalwhile taking into account the logical structure of expressions. Given this framework it is shown that abayesian approach can be adopted in order to infer probability distributions on the underlying variablegiven constraints both in the form of linguistic expressions and mass assignments. In addition, twoconditional measures are introduced for evaluating the appropriateness of a linguistic expressiongiven other linguistic information. 2003 Elsevier B.V. All rights reserved.Keywords: Random sets; Linguistic constraints; Fuzzy labels; Label semantics; Bayesian inference1. IntroductionThe limitations of classical modelling techniques to effectively capture the behaviourof complex systems has become increasingly clear over recent years. This has motivatedresearch into new, alternative modelling paradigms by the artificial intelligence community(e.g., fuzzy reasoning, possibility theory, Bayesian modelling, default reasoning: see [4,8,11,28,30]). All of these approaches share an emphasis on high level qualitative descriptionsas opposed to a more traditional low level framework. The advantage of such higher-levelknowledge representation is that it allows for the fusion of expert or background knowl-E-mail address: j.lawry@bris.ac.uk (J. Lawry).0004-3702/$ – see front matter  2003 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2003.10.0012J. Lawry / Artificial Intelligence 155 (2004) 1–39edge and knowledge derived from data. Furthermore, it tends to provide a clearer insightinto the underlying nature of the system than can be obtained from less transparent lower-level models. Another feature shared by many of the new approaches is that they providea methodology for reasoning in the presence of uncertainty. This is no coincidence, butrather is due to the fact that uncertainty and imprecision are often inherent in complexmodelling problems. This uncertainty is not only due to lack of precision or errors in mea-sured features but is often present in the model itself since the available features may notbe sufficient to provide a complete model of the system. To illustrate this point, considerthe important area of river basin modelling for flood forecasting. For this problem it isoften necessary to model river levels at a particular time point, purely in terms of rainfalland river levels at earlier times. However, in reality so many complex features influencerunoff that it is both difficult to identify the most important and practically impossible tomeasure any but a few of them. For instance, the likelihood that a given rainfall event willproduce a flood is dramatically affected by such factors as the size of the drainage basin,the topography of the basin, the amount of urban use within the basin and so on.While the development of analytical models may be impractical for many complex sys-tems, there is often data available implicitly describing the behaviour of the system. Forexample, large companies such as supermarkets, high street stores and banks collect astream of data relating to the behaviour of their customers. Such data must be analysedto provide flexible models of customer behaviour that can be used to aid a wide varietyof decision-making processes. Hence, if a higher level modelling approach is to be trulyeffective it must provide a natural knowledge representation framework for inductive learn-ing. As such it is important that it allows for the modelling of uncertainty, imprecision andvagueness in a semantically clear manner. Indeed, we should emphasise the necessity of aclear underlying semantics for any higher-level modelling paradigm since one of the fun-damental reasons for a high level approach is to provide transparent models that can beunderstood and used by practitioners in the relevant fields. This cannot be achieved if thevalidity of the underlying concepts and inference processes are either obscured or in doubt.In the sequel we will outline a new methodology for linguistic modelling and show how itcan be applied in an inductive learning context. The approach will centre on the modellingof linguistic constraints on variables as proposed by Zadeh [37] although the underlyingsemantics will be quite different.The phrase computing with words was introduced by Zadeh [42] to capture the idea ofcomputation based not on numerical values, but on natural language terms and expressions.As a general idea this is clearly of relevance to the type of modelling described above,however, we shall propose a quite different interpretation to that given in [38–40]. Thegeneral methodology for computing with words proposed by Zadeh is that of fuzzy settheory or fuzzy logic and in particular is based on the idea of linguistic variables (see [38–40]). A linguistic variable is defined as a variable that takes natural language terms such aslarge, small, tall, medium etc. as values and where the meaning of these words is given byfuzzy sets on some underlying domain of discourse. Hence, a particular expression of theform Bill is tall can be taken as expressing the fact that the linguistic variable describingBill’s height has the value tall, and such a statement has a partial truth-value correspondingto the membership degree of Bill’s actual height in the fuzzy set representing the meaningof tall. The truth-value of compound expressions such as Bill is tall or medium is thenJ. Lawry / Artificial Intelligence 155 (2004) 1–393evaluated according to a fuzzy set calculus based on some choice of t-norm or t-conorm(see [18] for an exposition).In our view the principal problem with the above approach is that the semanticsunderlying standard fuzzy logic or indeed the notion of membership function itself is ratherobscure. The difficulty is revealed by consideration of a fundamental question that shouldbe asked of all models of linguistic constraints. What information is conveyed regardingthe underlying variable? For instance, if someone asserts that Bill is tall exactly whatinformation about Bill’s height is conveyed by that assertion? In the case of fuzzy settheory, according to Zadeh [41], the latter provides a flexible constraint on the variablerepresenting Bill’s height. More specifically, it tells us that the possibility distribution onBill’s height corresponds to the membership function of the fuzzy set tall. However, thisassociation with possibility distributions does not, in itself, support the assumption of afully truth-functional calculus for membership degrees, as in fuzzy set theory (see [26]).Indeed, it does not really provide any insight into the behaviour of compound fuzzy sets.One possible solution to this difficulty is to accept that neither possibility distributions orfuzzy memberships are sufficiently intuitive to be treated as primitive notions and attemptto provide a lower-level model. If we are going to adopt the fuzzy logic methodology thenany such semantics should not only be intuitive but should also be consistent with a fullytruth-functional calculus based on a particular choice of t-norm and t-conorm. A numberof different models have been investigated and these are reviewed in [6,27].One of the most promising ideas is to view fuzzy memberships as being fixed pointcoverage functions of random sets, themselves representing uncertainty or variation in theunderlying crisp definition of a concept [12]. For instance, we might have a populationof different individuals each proposing their own set of heights that would qualify for thedescription tall. The associated random set would be a function from the set of individ-uals into the set of subsets of heights and the membership of a particular height, h, inthe fuzzy set tall would correspond to the probability of encountering an individual whoincluded h in their crisp set definition. This is the essence of the voting model for fuzzysets proposed originally by Black [3] and later by Gaines [10]. Clearly this interpretationis implicitly probabilistic in its nature and hence, it is not perhaps surprising that it doesnot fit well within the inference framework of fuzzy logic. One problem is that there isnot a one-to-one correspondence between fuzzy sets and random sets. The same fuzzy setcould be generated by a potentially infinite family of random sets (see Goodman [12]). Inpossibility theory this problem is overcome by making the assumption that the random setis consonant (i.e., the set of sets with non-zero mass constitutes a nested hierarchy). Lawry[20] justifies this by introducing the idea of an optimism parameter according to which themore optimistic a voter the more likely they are to include h in the extension of the concepttall. It is difficult to consolidate such an assumption with a fully truth-functional calculussince, in that case, a voter with a high optimism parameter would be required to be opti-mistic regarding both the concept and its negation. Lawry [20] suggests a weaker notionof negation to overcome this difficulty but nonetheless the treatment of negation remainsproblematic. In the sequel we outline a new random set based approach but where the ran-dom sets relate to sets of appropriate labels for a value. We describe a formal frameworkfor such an approach and show how it overcomes some of the problems highlighted above.This new calculus, however, will not be fully truth-functional but rather functional in a4J. Lawry / Artificial Intelligence 155 (2004) 1–39weaker, although sufficient, sense. The work described in Sections 2 and 3 is an extensionof that presented in an earlier paper [22]. This work is clearly related to random set se-mantics for fuzzy sets as proposed by Goodman [12] and Nguyen [25]. However, the latterdefines random sets on the underlying attribute universe whereas our proposed frameworkwill define random sets over labels. In our view the focus on the labels themselves providesan interesting new perspective.2. Label semanticsThe fundamental notion underlying label semantics is that when individuals makeassertions of the kind described above they are essentially providing information aboutwhat labels are appropriate for the value of some underlying variable. For simplicity, weassume that for a given context only a finite set of words is available. This is a somewhatcontroversial assumption since it might be claimed that by recursively applying hedges wecan easily generate an infinite set of labels from an initially finite set of words. In otherwords, if tall is a possible label for Bill’s height then so is very tall, quite tall, very verytall and so on. This claim is problematic, however, for a number of reasons. For instance,it would appear that the use of hedges in natural language is somewhat restricted. Onemight use the expressions very tall and quite tall but very quite tall or even quite verytall are never used. Also, there seems in practice to be a limit on the number of timeshedges can be applied to a label before it becomes nonsensical. This latter point seemsto suggest that in practice only a finite number of labels may be available even in naturallanguage. Another related difficulty with the use of hedges is determining the relationshipbetween the meaning of a word and the meaning of any new word generated from it byapplication of some hedge. In Zadeh [38–40] it is suggested that such relationships have asimple functional form. For example, if the meaning of tall is defined by a fuzzy set withmembership function µtall then Zadeh proposes that the meaning of very tall is the fuzzyset with membership µ2tall. The choice of this particular function seems relatively arbitraryand indeed, perhaps more fundamentally, it is far from apparent that there should be anysuch simple functional relationship between the meaning of a word and that of a new wordgenerated by application of a hedge. In other words, we would claim that while hedgesare a simple syntactic device for generating new labels there is no equally simple semanticdevice for generating the associated new meanings.Now let us return to the problem of interpreting natural language statements regarding,say, Bill’s height as represented by variable H . Let us suppose then that there is a fixedfinite set of possible labels for H , denoted LA, and that these labels are both known andcompletely identical for any individual who will make or interpret a statement regardingBill’s height. Given these assumptions how can we now interpret a statement such as Billis tall as asserted by a particular individual I ? We claim that one natural interpretation isthat it merely conveys the information that, according to I , tall is an appropriate label forthe value of H . In order to clarify this idea suppose I knows that H = h and that given thisinformation he/she is able to identify a subset of LA consisting of those words appropriateas labels for the value h. This set is denoted DIh which stands for the description of h givenby I . If we allow I to vary across some population of individuals V then we naturallyJ. Lawry / Artificial Intelligence 155 (2004) 1–395obtain a random set Dh from V into the power set of LA such that Dh(I ) = DIh. Giventhis we can obtain higher level information about the degree of applicability of a label toa value by defining, in this case, µtall(h) = Pr({I ∈ V | tall ∈ DI}) where the latter prob-hability is calculated on the basis of some underlying prior distribution on V . Now clearlythis is a function from Ω into [0, 1] and therefore can technically be viewed as a fuzzy set.However, we shall use the term ‘appropriateness degree’ partly because it more accuratelyreflects the underlying semantics and partly to highlight the quite distinct calculus for thesefunctions that will be introduced in the sequel. Similarly we can determine a probabilitydistribution (or mass assignment) for the random set Dh by defining ∀S ⊆ LA mh(S) == S}). Now suppose that I does not know the value of H (or alternativelyPr({I ∈ V | DIhwe do not know the value assigned to H by I ) then they (we) would naturally define aH (h) = DIH from the universe of H into the power set of LA such that DIrandom set DIh.The distribution of this random set will clearly depend on the prior information availableregarding the distribution of H . Hence, the assertion by I that Bill is tall would in this con-text be interpreted as tall ∈ DIH . Finally in the case when we have no information regardingI then we can define a random set DH from the cross product of V and the universe ofH into the power set of LA such that DH (I, h) = DIh and interpret the above statement astall ∈ DH . In order to clarify some of these ideas consider the following example wherethe objective is to provide linguistic labels for the outcome of a single throw of a dice.Example 1. Suppose the variable SCORE with universe {1, 2, 3, 4, 5, 6} gives the outcomeof a single throw of a particular dice. Let LA = {low, medium, high} and V = {I1, I2, I3}then a possible definition of DSCORE is as follows:= {low}, DI12= {low, medium}, DI22= {low}, DI32= {low},DI11DI13DI14DI15DI16= DI2= DI311= {medium}, DI23= {medium, high}, DI24= {high}, DI25= DI2= {high}.6= DI36= {medium}, DI33= {medium}, DI24= {high},= {medium, high}, DI35= {medium, low},= {medium},The value of the appropriateness measure will depend on the underlying distribution onV = {I1, I2, I3}, perhaps representing the weight of importance associated with the viewsof each individual. For instance, if we assume a uniform distribution on V then the degreeof appropriateness of low as a label for 3 is given by|{I ∈ V | low ∈ DI3|V ||{I3}||V |Overall the appropriateness degrees for each word are given by= 13=}|.µlow(1) = µlow(2) = 1, µlow(3) = 13µmedium(2) = 13µhigh(4) = 1, µhigh(5) = 1, µhigh(6) = 1.3,, µmedium(3) = 1, µmedium(4) = 1, µmedium(5) = 13,6J. Lawry / Artificial Intelligence 155 (2004) 1–39Similarly, assuming a uniform prior on V we can determine mass assignments on DSCOREfor SCORE = 1, . . . , 6. For example, if SCORE = 2 we have= {low, medium}}||V |(cid:1){low, medium}|{I ∈ V | DI2|{I1}||V |= 13m2==(cid:2).The mass assignments for each value of x are given bym1 = {low} : 1, m2 = {low, medium} : 13m3 = {medium} : 23m4 = {medium, high} : 13, {medium, high} : 1m5 = {high} : 233, {low, medium} : 1,3, {medium} : 23,, {low} : 23,, m6 = {high} : 1.In order, to determine an overall mass assignment m as SCORE varies, we need to know thedistribution on the universe {1, . . . , 6}. Assuming a uniform distribution gives, for example,(cid:1){low, medium}(cid:2)m=6(cid:3)x=1(cid:1){low, medium}(cid:2)Pr(x) =mx13+ 136= 19.Overall we have:m = {low} : 518, {low, medium} : 19,{medium} : 29, {medium, high} : 19, {high} : 518.We now consider the problem of how to interpret expressions involving compoundlabels built up using some set of logical connectives. For the scope of this paper we willconsider the four main connectives ∧, ∨, → and ¬. Firstly, let us consider the case ofnegation. How do we interpret expressions of the form Bill is not tall? We take the viewhere that negation is used to express the non-suitability of a label. In other words the abovestatement means that tall is not an appropriate label for H , or tall /∈ DH . Conjunction anddisjunction are then taken as having the obvious meanings so that Bill is tall and mediumis interpreted as saying that both tall and medium are appropriate as labels for H (i.e.,{tall, medium} ⊆ DH ), and Bill is tall or medium is interpreted as saying that either tallis an appropriate label for H or medium is an appropriate label for H (i.e., tall ∈ DHor medium ∈ DH ). In the case of implication we take very tall implies tall to mean thatwhenever very tall is an appropriate label for H so is tall (i.e., very tall ∈ DH implies thattall ∈ DH ).It will clearly be desirable to measure the appropriateness degree of such compoundexpressions for particular values of the underlying variable. For instance, given thescenario outlined in Example 1 we might want to know what is the appropriatenessdegree of the expression medium or low to the value 3. Now this expression identifiesthe set of subsets of LA which either contains low or medium (i.e., {{low}, {medium},J. Lawry / Artificial Intelligence 155 (2004) 1–397{low, medium}, {low, high}, {medium, high}, {low, medium, high}}). Hence, it is natural todefine the appropriateness degree of medium or low to 3 as the sum of the values of m3across this set (i.e., m3({low}) + m3({medium}) + m3({low, medium}) + m3({low, high}) +m3({medium, high}) + m3({low, medium, high}) = 2= 1). In the following section we3formalise the above ideas within a logical framework.+ 133. A formal framework for label semanticsIn this paper we adopt a logical formalisation for label semantics where label expres-sions are represented by propositional logic sentences. Consider a formal language consist-ing of the set of the labels LA = {L1, . . . , Ln} and the connectives ∧, ∨, → and ¬. Withinthis language we can represent compound linguistic descriptions generated recursively byapplication of the connectives:Definition 2 (Label expressions). The set of label expressions, LE, is defined recursivelyas follows:(i) Li ∈ LE for i = 1, . . . , n.(ii) If θ, ϕ ∈ LE then ¬θ , θ ∧ ϕ, θ ∨ ϕ, θ → ϕ ∈ LE.Recall from the discussion in the previous section that a label expression identifies a setof subsets of LA which capture its meaning. We now give a formal definition of this subsetfor any general label expression:Definition 3 (Appropriate label sets). Every θ ∈ LE is associated with a set of subsets ofLA (i.e., an element of 22LA), denoted λ(θ ) and defined recursively as follows:(i) λ(Li ) = {S ⊆ LA | Li ∈ S}.(ii) λ(θ ∧ ϕ) = λ(θ ) ∩ λ(ϕ).(iii) λ(θ ∨ ϕ) = λ(θ ) ∪ λ(ϕ).(iv) λ(θ → ϕ) = λ(θ ) ∪ λ(ϕ).(v) λ(¬θ ) = λ(θ ).Intuitively λ(θ ) corresponds to those subsets of LA identified as being candidates for theset of appropriate labels for x (i.e., possible values for Dx ) by expression θ . In this sense theimprecise linguistic restriction ‘x is θ ’ on x corresponds to the strict constraint Dx ∈ λ(θ )on Dx . Hence, the linguistic description Dx can provide an alternative to linguisticvariables (Zadeh [38–40]) as a means of formally representing linguistic constraints.Example 4. Let LA = {small, medium, large} thenλ(small ∧ medium) =λ(small ∨ medium) =(cid:4){small, medium}, {small, medium, large}(cid:4){small}, {medium}, {small, medium}, {small, large},{medium, large}, {small, medium, large}(cid:5),(cid:5),8J. Lawry / Artificial Intelligence 155 (2004) 1–39λ(small → medium) =(cid:4){small, medium}, {small, medium, large}, {medium, large},(cid:5){medium}, {large}, ∅,λ(¬small) =(cid:5)(cid:4){medium}, {large}, {medium, large}, ∅.The following results illustrate the clear relationship between appropriate label setsand the logical structure of the expressions that identify them. Initially, however, weintroduce some basic notation. Let Val denote the set of valuations (i.e., allocations oftruth values) on {L1, . . . , Ln}. For v ∈ Val v(Li ) = true can be taken as meaning thatLi is an appropriate label in the current context. Let LE0 = {L1, . . . , Ln} and LEn+1 =LEn ∪ {¬θ, θ ∧ ϕ, θ ∨ ϕ, θ → ϕ | θ, ϕ ∈ LEn}. Clearly we have that LE =n LEn and also,from a valuation v on LE0 the truth-value, v(θ ), for θ ∈ LE can be determined recursivelyin the usual way by application of the truth tables for the connectives.(cid:6)Definition 5. Let τ : Val → 2LA such that ∀v ∈ Val τ (v) = {Li | v(Li ) = true}.Notice that τ is clearly a bijection. Also note that for v ∈ Val τ (v) can be associatedwith a Herbrand interpretation of the language LE (see [24]).Lemma 6. ∀θ ∈ LE {τ (v) | v ∈ Val, v(θ ) = true} = λ(θ ).Proof. We prove this by induction on the complexity of θ .Suppose θ ∈ LE0, so that θ = Li for some i ∈ {1, . . . , n}. Now as v ranges across allvaluations for which Li is true, then τ (v) ranges across all subsets of LA that contain Li .Hence, {τ (v) | v ∈ Val, v(Li ) = true} = {S ⊆ LA | {Li} ⊆ S} = λ(Li ) as required.Now suppose we have ∀θ ∈ LEn, {τ (v) | v ∈ Val, v(θ ) = true} = λ(θ ) and consider anexpression θ ∈ LEn+1 then either θ ∈ LEn in which case the result follows trivially or oneof the following hold:(i) θ = φ ∧ ϕ where φ, ϕ ∈ LEn. In this case(cid:4)(cid:5)v ∈ Val | v(φ ∧ ϕ) = true(cid:4)v ∈ Val | v(φ) = true=(cid:5)(cid:4)v ∈ Val | v(ϕ) = true(cid:5).∩Therefore,(cid:4)(cid:5)τ (v) | v ∈ Val, v(φ ∧ ϕ) = true(cid:5)(cid:4)τ (v) | v ∈ Val, v(φ) = true== λ(φ) ∩ λ(ϕ)= λ(φ ∧ ϕ)(by the inductive hypothesis)by Definition 3.(cid:4)τ (v) | v ∈ Val, v(ϕ) = true(cid:5)∩(ii) θ = φ ∨ ϕ where φ, ϕ ∈ LEn. In this case(cid:4)v ∈ Val | v(φ ∨ ϕ) = true(cid:4)(cid:5)=v ∈ Val | v(φ) = true(cid:5)(cid:4)v ∈ Val | v(ϕ) = true(cid:5).∪J. Lawry / Artificial Intelligence 155 (2004) 1–399Therefore,(cid:4)(cid:5)τ (v) | v ∈ Val, v(φ ∨ ϕ) = true(cid:5)(cid:4)τ (v) | v ∈ Val, v(φ) = true== λ(φ) ∪ λ(ϕ)= λ(φ ∨ ϕ)(by the inductive hypothesis)by Definition 3.(cid:4)∪τ (v) | v ∈ Val, v(ϕ) = true(cid:5)(iii) θ = φ → ϕ where φ, ϕ ∈ LEn. In this case(cid:4)(cid:5)(cid:4)=v ∈ Val | v(φ → ϕ) = truev ∈ Val | v(φ) = false(cid:5)(cid:4)v ∈ Val | v(φ) = true=(cid:5)(cid:4)v ∈ Val | v(ϕ) = true∪(cid:5)(cid:4)v ∈ Val | v(ϕ) = true.(cid:5)∪(cid:4)τ (v) | v ∈ Val, v(ϕ) = true(cid:5)∪(by the inductive hypothesis)Therefore,(cid:4)(cid:5)τ (v) | v ∈ Val, v(φ → ϕ) = true(cid:5)(cid:4)τ (v) | v ∈ Val, v(φ) = true== λ(φ) ∪ λ(ϕ)= λ(φ → ϕ)(iv) θ = ¬φ where φ ∈ LEn. In this case(cid:5)τ (v) | v ∈ Val, v(¬φ) = true(cid:4)by Definition 3.(cid:5)(cid:4)τ (v) | v ∈ Val, v(φ) = true== λ(φ)= λ(¬φ)(by the inductive hypothesis)by Definition 3.✷Proposition 7. For θ, ϕ ∈ LE θ |= ϕ iff λ(θ ) ⊆ λ(ϕ).Proof. (⇒)(cid:4)θ |= ϕ ⇒⇒⇒ λ(θ ) ⊆ λ(ϕ)(cid:5)v ∈ Val | v(θ ) = trueτ (v) | v ∈ Val, v(θ ) = trueby Lemma 6.⊆(cid:4)(cid:4)v ∈ Val | v(ϕ) = true(cid:5)(cid:5)(cid:5)(cid:4)τ (v) | v ∈ Val, v(ϕ) = true⊆(⇐) Suppose λ(θ ) ⊆ λ(ϕ). Then λ(θ ) = {τ (v) | v ∈ Val, v(θ ) = true} and λ(ϕ) ={τ (v) | v ∈ Val, v(ϕ) = true} by Lemma 6.Therefore(cid:4)τ (v) | v ∈ Val, v(θ ) = true(cid:5)v ∈ Val | v(θ ) = true⇒(cid:4)(cid:5)(cid:4)τ (v) | v ∈ Val, v(ϕ) = true(cid:4)v ∈ Val | v(ϕ) = true(cid:5)⊆⊆(cid:5)since τ is a bijection. ✷A trivial corollary of this proposition is:Corollary 8. For θ, ϕ ∈ LE θ ≡ ϕ iff λ(θ ) = λ(ϕ).10J. Lawry / Artificial Intelligence 155 (2004) 1–39Proposition 9. If ϕ ∈ LE is inconsistent then λ(ϕ) = ∅.Proof. If ϕ ∈ LE is inconsistent then ϕ ≡ θ ∧ ¬θ so that by Corollary 8. λ(ϕ) = λ(θ ∧¬θ ) = λ(θ ) ∩ λ(¬θ ) = λ(θ ) ∩ λ(θ ) = ∅ by Definition 3. ✷In order to introduce higher level measures of appropriateness as discussed in earliersections we need to consider the logical structure of label expressions in conjunction witha set of individuals, a probability distribution on that set and a probability distribution onthe domain Ω. To allow for this we now introduce the notions of a frame and an extendedframe.Definition 10 (Frame and extended frame).(i) A frame is a tuple (cid:22)V , PV (cid:23) where V is a set of individuals and PV is a probabilitydistribution on V .(ii) An extended frame is a tuple (cid:22)V , PV , Ω, PΩ (cid:23) where (cid:22)V , PV (cid:23) is a frame and PΩ is adistribution on the underlying domain Ω.Definition 11 (Mass assignments and label appropriateness degrees).(i) Given a frame Γ = (cid:22)V , PV (cid:23) we define the mass assignment of Dx by(cid:1)(cid:4)∀S ⊆ LA mΓx (S) = PVI ∈ V | DIx= S(cid:5)(cid:2).In the case where V is finite and PV is the uniform distribution this corresponds to∀S ⊆ LA mΓx (S) =|{I ∈ V | DIx|V |= S}|.From this mass assignment we define the appropriateness measure (or degree) µΓ by∀θ ∈ LE, ∀x ∈ Ω µΓθ (x) =mΓx (S).(cid:3)S∈λ(θ)(ii) Given an extended frame Γ + = (cid:22)V , PV , Ω, PΩ (cid:23) then the mass assignment of Dx as xvaries across Ω is given by∀S ⊆ LA mΓ +(S) = PV × PΩ(cid:1)(cid:4)(cid:22)I, x(cid:23) | DIx(cid:5)(cid:2).= SFrom this mass assignment we define the general appropriateness degree by∀θ ∈ LE µΓ +θ=(cid:3)mΓ +(S).S∈λ(θ)In cases where the frame or extended frame is fixed we shall drop the superscripts Γand Γ + in the above definitions.Notice that it is not a requirement of Definition 11 that zero mass be allocatedx (∅) corresponds to the probability in frameto the empty set. In label semantics mΓJ. Lawry / Artificial Intelligence 155 (2004) 1–3911Γ that no labels are appropriate to describe x (i.e.,appropriateness degrees, allocating a non-zero value to mΓmax(µΓL1(x), . . . , µΓLn(x)) < 1.that Dx = ∅). In terms ofx (∅) has the consequence thatTrivially, by Proposition 7 we have that if θ |= ϕ then for any frame Γ ∀x ∈ Ωϕ (x) and for any extended frame Γ + µΓ +θ (x) (cid:1) µΓµΓϕ . Similarly by Corollary 8we have that if θ ≡ ϕ then for any frame Γ , ∀x ∈ Ω µΓϕ (x) and for any extendedframe Γ + µΓ +(cid:1) µΓ +θ (x) = µΓθ= µΓ +ϕ .θProposition 12. For any frame Γ , ∀θ ∈ LE, ∀x ∈ Ω µΓ¬θ (x) = 1 − µΓθ (x).Proof.µΓ¬θ (x) =(cid:3)S∈λ(¬θ)mΓx (S) =(cid:3)S∈λ(θ)mΓx (S) = 1 −(cid:3)S∈λ(θ)mΓx (S) = 1 − µΓθ (x).✷In order to investigate the behaviour of the appropriateness measure on conjunctions,disjunctions and implications we need to introduce the notion of consonant massassignments. More specifically, we will only consider frames Γ such that mΓx is consonantfor all x ∈ Ω. Here, consonance has the standard random set meaning (see [13]) that∀S, S(cid:26) ⊆ LA if both mΓx (S(cid:26)) > 0 then either S ⊆ S(cid:26) for S(cid:26) ⊆ S.x (S) > 0 and mΓConsonance of label sets implies that individuals in V differ regarding what labels areappropriate for a value only in terms of generality or specificity. This could be justifiedby the idea that all individuals share a common ordering on the appropriateness of labelsfor a value and that the composition of DIx is consistent with this ordering for each I .More formally, supposing for each element x ∈ Ω the population V shares a common totalordering (cid:27)x where for Li , Lj ∈ LA, Li (cid:27)x Lj means that Lj is as least as appropriate asa label for x as Li . In this case, when deciding on a set of appropriate labels, an individualI would be expected to be consistent with (cid:27)x so that if Li ∈ DIx then Lj will also be inDIx for all labels Lj such that Li (cid:27)x Lj . Clearly, in this case as we vary individuals acrossV then the values of DIx occurring will form a nested hierarchy. For instance, in the caseof the dice problem described in Example 1 possible appropriateness orderings for valuesSCORE = 1, . . . , 6 are as follows:high (cid:27)1 medium (cid:27)1 low, high (cid:27)2 medium (cid:27)2 low,high (cid:27)3 low (cid:27)3 medium, low (cid:27)4 high (cid:27)4 medium,low (cid:27)5 medium (cid:27)5 high, low (cid:27)6 medium (cid:27)6 high.Hence, for any individual I , if I decides that low is an appropriate label for 3 (low ∈ DI3 )then to be consistent with the ordering (cid:27)3 they must also decide that medium is anappropriate label for 3 (medium ∈ DI3 ) since medium is at least as appropriate as low asa label for 3.Notice, that the consonance assumption for random sets on labels is in one sense weakerthan the corresponding assumption for random sets on the universe Ω, since the latterrequires individuals to maintain the same level of specificity across all values in Ω. To seethis more clearly recall Example 1 and observe that mx is consonant ∀x ∈ {1, . . . , 6}. Now12J. Lawry / Artificial Intelligence 155 (2004) 1–39for each member I ∈ V the extension (associated subset of Ω) of, say medium is given by{x ∈ Ω | medium ∈ DI}. Hence, we obtain {2, 3, 4}, {3, 4, 5} and {3, 4} for I1, I2 and I3xrespectively. Clearly, however, this does not form a nested hierarchy.Proposition 13. If ∀x ∈ Ω mΓhave that ∀x ∈ Ω µΓLi ∧Lj(x) = min(µΓLi(x), µΓLj(x)).x is a consonant mass assignment then for Li , Lj ∈ LA weProof. Noticeλ(Li ∧ Lj ) = λ(Li ) ∩ λ(Lj ) =(cid:4)S ⊆ LA | {Li} ⊆ S=S ⊆ LA | {Li, Lj } ⊆ S(cid:5).(cid:4)(cid:5)(cid:4)S ⊆ LA | {Lj } ⊆ S(cid:5)∩Hence,∀x ∈ Ω µΓLi ∧Lj(x) =(cid:3)mΓx (S).S: {Li ,Lj }⊆SFor any x, since mΓx is a consonant mass assignment then it must have the form= M0 : m0, . . . , Mk : mk where Mt ⊂ Mt +1 for t = 0, . . . , k − 1. Now suppose w.l.o.g.(x) then {Li} ⊆ Mt iff {Li , Lj } ⊆ Mt for t = 0, . . . , k. ThereforemΓxthat µΓLi(x) (cid:1) µΓLjµΓLi ∧Lj(x) =(cid:3)S: {Li }⊆SmΓx (S) = µΓLi(cid:1)(x) = minµΓLi(x), µΓLj(x)(cid:2).✷Proposition 14. If for all x ∈ Ω mΓwe have that ∀x ∈ Ω µΓLi ∨Lj(x) = max(µΓLi(x), µΓLj(x)).x is a consonant mass assignment then for Li, Lj ∈ LAIn order to compare and contrast label semantics with the many-valued logic approachto fuzzy reasoning we first give a formal definition of what is meant for a calculus to bestrongly functional.Definition 15. Let w : LE × Ω → [0, 1] then w is said to be strongly functional iffthere exist functions f¬ : [0, 1] → [0, 1], f∧ : [0, 1]2 → [0, 1], f∨ : [0, 1]2 → [0, 1] andf→ : [0, 1]2 → [0, 1] such that ∀θ, ϕ ∈ LE, ∀x ∈ Ω w¬θ (x) = f¬(wθ (x)), wθ∧ϕ(x) =f∧(wθ (x), wϕ(x)), wθ∨ϕ(x) = f∨(wθ (x), wϕ(x)) and wθ→ϕ(x) = f→(wθ (x), wϕ(x))where wθ (x) is shorthand for w(θ, x).This should be contrasted with the condition of a calculus being weakly functional asdefined below.Definition 16. w : LE × Ω → [0, 1] is said to be weakly functional iff ∀θ ∈ LE there existsa function fθ : [0, 1]n → [0, 1] such that wθ (x) = fθ (wL1(x), . . . , wLn(x)).Clearly weak functionality is a strictly weaker condition than strong functionality.Strong functionality implies that the value of w for, say, a conjunction of expressionsdepends only on the value of w for the conjuncts and not on their logic structure. WeakJ. Lawry / Artificial Intelligence 155 (2004) 1–3913functionality allows for that logical structure to be taken into account. It should be notedthat weak functionality is sufficient to insure that all values for w can be determined fromits values on LA and hence the amount of information needed to be stored is still of ordern and not of order exponential in n or higher as is the case in many non-functional calculi(see [26]). In view of this we would argue that weak functionality is adequate to ensurecomputational feasibility for most real world applications.In the literature, and especially in approximate reasoning it is often the case that the onlytype of functionality considered is strong functionality. However, clearly calculi exist thatare weakly but not strongly functional; a typical example being a standard probabilisticcalculus for which the basic events are assumed to be independent. This failure todistinguish clearly between these two levels of functionality can lead to misunderstandings.For example, consider the triviality results proved by Dubois and Prade [4] and laterElkan [9] which show that no non-binary functional calculus can satisfy all the laws ofBoolean algebra. For example, only binary functional calculi can satisfy idempotence aswell as the laws of excluded middle and non-contradiction (see [5]). It is important torealise that in this case the type of functionality referred to is strong functionality. Weaklyfunctional calculi are not restricted in this manner; for instance probabilistic calculi underan independence assumption satisfy all the standard boolean laws while maintaining weakfunctionality.To see that the appropriateness measure is not strongly functional notice that despiteθ (x), µΓϕ (x))(x). From Defini-Propositions 13 and 14 it does not hold that ∀θ, ϕ ∈ LE, µΓor that µΓθ (x), µΓtion 3 we have that λ(Li ∧ ¬Lj ) = λ(Li ) ∩ λ(Lj ) and henceθ∧ϕ(x) = min(µΓLi ∧¬Ljϕ (x)). For instance, consider µΓθ∨ϕ(x) = max(µΓ(cid:3)µΓLi ∧¬Lj(x) =mΓx (S).S: Li ∈S, Lj /∈SGiven the consonance assumption we know that mΓxMt +1 for t = 0, . . . , k − 1. Now suppose that µΓ(x) (cid:1) µΓLjLiLi ∈ Mt then Lj ∈ Mt and hence µΓ= M0 : m0, . . . , Mk : mk where Mt ⊂(x) then for all t = 0, . . . , k if(x) (cid:2) µΓ(x) thenLj(x) = 0. Alternatively if µΓLiLi ∧¬LjµΓLi ∧¬Lj(x) =(cid:3)(cid:3)(cid:3)mΓx (S) =mΓx (S) −mΓx (S)S: Li ∈SS: Lj ∈S(x).= µΓLi(x) = max(0, µΓThis can be summarised by the expression µΓLi ∧¬LjLi(x), 1 − µΓis not in general the same as min(µΓ(x)) as would be given by the stronglyLjLifunctional calculus consistent with Propositions 12–14 for which f∧(a, b) = min(a, b),f∨(a, b) = max(a, b) and f¬(a) = 1 − a. As an aside, we note that this result givessome insight into the behaviour of implication in label semantics, at least at the levelof individual labels. For instance, we have that Li → Lj is logically equivalent to¬(Li ∧ ¬Lj ) and hence µΓ(x)) =Li→Ljmin(1, 1 − µΓ(x)). This corresponds to Lukasiewicz implication (see [14] orLi[18]) although it only applies here at the label level and not for more complex expressions.(x) = 1 − max(0, µΓLi(x) = 1 − µΓ(x) + µΓLj(x) − µΓLj(x) − µΓLj(x)) whichLi ∧¬LjS: Li ∈S, Lj /∈S(x) − µΓLj14J. Lawry / Artificial Intelligence 155 (2004) 1–39{µL1 (x), . . . , µLn (x)} (cid:29)⇒ [mx ] (cid:29)⇒ µθ (x) = fθ (µL1 (x) . . . µLn (x)) =(cid:7)S∈λ(θ ) mx (S)Fig. 1. Weak functionality of label semantics.To see that appropriateness degrees are weakly functional recall from elementaryrandom set theory that a consonant mass assignment [13] is uniquely defined by its fixedpoint coverage. This means that mΓx can be completely determined from the values ofL (x) > 0} orderedL (x) | L ∈ LA, µΓµΓL1such that yt > yt +1 for t = 1, . . . , k − 1 then(x) as follows: Let {y1, . . . , yk} = {µΓ(x), . . . , µΓLnmΓx= Mt : yt − yt +1,t = 1, . . . k − 1, Mk : yk, M0 : 1 − y1,(x), . . . , µΓLnθ (x) is uniquely determined by λ(θ ) and mΓθ (x) and µΓL1L (x) (cid:2) yt } for t = 1, . . . , k. Hence, since fromwhere M0 = ∅ and Mt = {L ∈ LA | µΓDefinition 11 µΓx then there is clearly afunctional relationship between µΓ(x) (see Fig. 1). In otherwords, for every linguistic expression θ there is a function fθ : [0, 1]n → [0, 1] such thatµθ (x) = fθ (µL1(x) . . . µLn(x)) where fθ is evaluated by using the consonance assumptionto infer a mass assignment on label sets and then summing the masses of sets containedin λ(θ ). It should also be noted that the appropriateness degree satisfies the laws of theexcluded middle and non-contradiction in the sense that for any frame Γ , ∀x ∈ Ω, ∀θ ∈ LEθ∧¬θ (x) = 0 as follows immediately from Propositions 9 and 12.θ∨¬θ (x) = 1 and µΓµΓThus the consonance assumption applied to label sets results in a functional calculus thatcoincides with the standard fuzzy logic connectives at the basic label level while preservingthe laws of excluded middle and non-contradiction. This should be contrasted with theconsonance assumption applied to random sets on the attribute universe which is not, initself, sufficient to generate a functional calculus across a number of fuzzy concepts.The weak functionality of label semantics brings considerable practical advantagessince we no longer need to have any knowledge of the underlying population of individualsV or their distribution PV (i.e., the frame) in order to determine mx . Rather, for reasoningwith label semantics in practice we need only define appropriateness degrees µL forL ∈ LA corresponding to the imprecise definition of each label.(cid:9)(cid:8)One possible method for calculating µΓθ (x) for a general θ ∈ LE and x ∈ Ω is asfollows: By the disjunctive normal form theorem we have that θ is logically equivalentα: α→θ α where each atom is a conjunction of literals of theto a disjunction of atoms±Li . Now it can easily been seen, from Lemma 6, that for any atom of thisform α =form λ(α) is a singleton consisting of the subset of LA made up from those labels appearingpositively in α. Also by Definition 3 and Corollary 8 we have that λ(θ ) =α: α→θ λ(α)and hence µΓx (λ(α)). (NB. We are abusing notation slightly here andtaking λ(α) to correspond to the single element of 2LA associated with α rather than the setcontaining that element.) Alternatively, it may be more convenient just to determine λ(θ )recursively according to Definition 3.α: α→θ mΓθ (x) =(cid:7)(cid:6)iIn the specific context of a particular frame we may be able to make inferences regardinglabel expressions that do not generally hold. Furthermore, since a frame effectively definesthe meaning and relationship between the members of LA, it identifies a subset of 2LAas the sets of appropriate labels that can actually occur. This restriction will make theJ. Lawry / Artificial Intelligence 155 (2004) 1–3915calculation of appropriateness degrees much less complex provided the basic labels do notoverlap semantically too much. For instance, given LA = {small, medium, large} we mayfind that in some frame Γ small only overlaps with medium, medium overlaps with smalland large and large only overlaps with medium. This means that only the following occuras sets of possible labels: ∅, {small}, {small, medium}, {medium}, {medium, large}, {large}.We can formalise this observation by defining the set of focal elements for a frame asfollows:Definition 17 (Set of focal elements). The set of focal elements for frame Γ is FΓ = {S ⊆LA | ∃x ∈ Ω, mΓx (S) > 0}.In other words, the focal sets correspond to the sets of appropriate labels that areconsistent with the definition of the labels in frame Γ . Given this concept we can definethe following natural semantic relations on LE.Definition 18.(i) (Follows from in frame Γ ) For θ, ϕ ∈ LE ϕ follows from θ in frame Γ (denotedθ |=Γ ϕ) iff λ(θ ) ∩ FΓ ⊆ λ(ϕ) ∩ FΓ .(ii) (Equivalent to in frame Γ ) For θ, ϕ ∈ LE ϕ is equivalent to θ in frame Γ (denotedϕ ≡Γ θ ) iff λ(θ ) ∩ FΓ = λ(ϕ) ∩ FΓ .(iii) For θ ∈ LE θ is universally true in frame Γ (denoted |=Γ θ ) iff λ(θ ) ∩ FΓ = FΓ .The frame Γ provides an interpretation for each label in LA, as made apparent bytheir respective appropriateness degrees, and this should be taken into account in anysubsequent reasoning. So, for instance, while it may not generally be the case that no valuecan be both small ∧ large, it certainly is true in any frame for which the appropriatenessdegrees of small and large do not overlap. The operators |=Γ and ≡Γ incorporate thisadditional information on the meaning of labels into the logical notions of ‘follows from’and ‘equivalent to’. These operators can also be defined in terms of standard propositionallogic deduction as the following result shows.Definition 19.∀S ⊆ LA αS =(cid:10) (cid:11)(cid:10) (cid:11)(cid:12)Li∧(cid:12)¬Li.Li ∈SLi /∈SLemma 20. λ((cid:8)S∈FΓαS) = FΓ .Proof. λ((cid:8)S∈FΓαS ) =(cid:6)S∈FΓλ(αS ) by Definition 3. Now16J. Lawry / Artificial Intelligence 155 (2004) 1–39(cid:10)(cid:10) (cid:11)λ(αS ) = λ(cid:10) (cid:11)(cid:12)Li∧¬Li(cid:12)(cid:12)(cid:13)Li ∈Sλ(Li ) ∩(cid:13)Li /∈Sλ(¬Li )Li ∈S(cid:13)Li /∈S(cid:4)T ⊆ LA | Li ∈ T(cid:5)∩by Definition 19(cid:13)(cid:4)T ⊆ LA | Li /∈ T(cid:5)by Definition 3Li ∈S(cid:4)T ⊆ LA | S ⊆ T(cid:8)(cid:5)∩(cid:6)Li /∈S(cid:4)(cid:5)T ⊆ LA | S ∩ T = ∅= {S}.Therefore, λ(S∈FΓ{S} = FΓ as required. ✷S∈FΓαS) =(cid:8)Proposition 21. θ |=Γ ϕ iff (S∈FΓαS ) ∧ θ |= ϕ.===Proof. (⇒)θ |=Γ ϕ ⇒ λ(θ ) ∩ FΓ ⊆ λ(ϕ) ∩ FΓ(cid:10) (cid:14)(cid:12)by Definition 18(cid:12)(cid:10) (cid:14)⇒ λ(θ ) ∩ λαS⊆ λ(ϕ) ∩ λ⊆ λ(ϕ)by Lemma 20αSS∈FΓαS⊆ λ(ϕ)by Definition 3S∈FΓ(cid:12)(cid:14)(cid:10)⇒ λθ ∧(cid:15)S∈FΓ(cid:10)⇒τ (v) | v ∈ Val, vθ ∧(cid:4)⇒ ⊆(cid:15)τ (v) | v ∈ Val, v(ϕ) = true(cid:10)⇒v ∈ Val | vθ ∧(cid:14)αSS∈FΓ⇒ ⊆⇒ θ,(cid:4)(cid:5)v ∈ Val | v(ϕ) = true(cid:14)αS |= ϕ as required.(cid:12)(cid:16)αS= true(cid:14)S∈FΓ(cid:5)(cid:12)by Lemma 6(cid:16)= truesince τ is a bijectionS∈FΓ(⇐)(cid:14)S∈FΓθ,(cid:15)αS |= ϕ(cid:10)v ∈ Val | vθ ∧(cid:15)(cid:14)S∈FΓ(cid:10)⇒v ∈ Val | vθ ∧(cid:15)(cid:10)τ (v) | v ∈ Val, vθ ∧(cid:12)(cid:16)αS= true⊆(cid:4)v ∈ Val | v(ϕ) = true(cid:5)(cid:12)(cid:16)(cid:15)(cid:10)αS(cid:12)= true⊆v ∈ Val | vϕ ∧(cid:16)= true(cid:14)S∈FΓ(cid:14)αSS∈FΓ(cid:12)(cid:16)αS= true(cid:14)S∈FΓJ. Lawry / Artificial Intelligence 155 (2004) 1–3917(cid:15)(cid:10)⊆τ (v) | v ∈ Val, vϕ ∧(cid:14)(cid:12)(cid:16)αS= truesince τ is a bijection(cid:10)⇒ λθ ∧(cid:14)S∈FΓ(cid:14)(cid:12)(cid:10)αS⊆ λϕ ∧(cid:12)αSby Lemma 6S∈FΓ(cid:10) (cid:14)⊆ λ(ϕ) ∩ λ(cid:12)αSby Definition 3S∈FΓ(cid:10) (cid:14)⇒ λ(θ ) ∩ λ(cid:12)αSS∈FΓ⇒ λ(θ ) ∩ FΓ ⊆ λ(ϕ) ∩ FΓ⇒ θ |=Γ ϕ as required.S∈FΓby Lemma 20✷Proposition 21 tells us that the information regarding the meaning of labels in LAcontained in a particular focal set S can be completely represented by the logical expressionαS . In some sense this is to be expected since for any set S, αS provides a logical descriptionof S by stating exactly what labels are and are not contained in S. The next corollaryfollows trivially from Proposition 21.Corollary 22. θ ≡Γ ϕ iff(cid:14)(cid:14)αS ∧ θ ≡αS ∧ ϕ.S∈FΓS∈FΓ(x), . . . , µΓLnWe should observe that since mΓx can be completely determined by µΓL1(x)then the set of focal elements for Γ can also be determined given only these values.Therefore, in a strong sense the full meaning of the labels in LA are captured by theirappropriateness degrees. A common example of (i) in Definition 18 is when a certainlabel is conceptually implied by another label. For instance, we might say that wheneversomeone is described as being very tall then they can also be described as tall. In fuzzyset theory this would be captured by taking the fuzzy set for very tall as a fuzzy subsetof the fuzzy set for tall. In label semantics we would expect to have a frame Γ inwhich whenever very tall was deemed an appropriate label so was tall. In other wordsvery tall |=Γ tall or alternatively |=Γ very tall → tall. In such a case it is easy to see that∀x ∈ Ω µΓtall(x) so that in this instance fuzzy set theory and label semanticswould coincide. In general, we have that for any frame Γ such that θ |=Γ ϕ then ∀x ∈ Ωθ (x) (cid:1) µΓµΓvery tall(x) (cid:1) µΓϕ (x).Example 23. Let LA = {small, medium, large}, Ω = [0, 10] and Γ be a frame such thatsmall, µΓµΓlarge are trapezoidal functions (see Fig. 2) defined bymedium and µΓµΓsmall(x) =(cid:17)12 − x20if x ∈ [0, 2],if x ∈ (2, 4],if x > 4,µΓmedium(x) =− 10x214 − x20if x < 2,if x ∈ (2, 4],if x ∈ (4, 6],if x ∈ (6, 8],if x > 8,18J. Lawry / Artificial Intelligence 155 (2004) 1–39Fig. 2. Appropriateness degrees for, from left to right, small, medium and large.µΓlarge(x) =(cid:17)0x21− 3if x < 6,if x ∈ [6, 8],if x > 8.Allowing x to vary across [0, 10] we obtain the following definition of mΓx as follows:(see Fig. 3):(cid:1){small}(cid:2)=mΓx(cid:17)13 − x0if x ∈ [0, 2],if x ∈ (2, 3],if x > 3,(cid:1){small, medium}(cid:2)=mΓx0x− 122 − x20if x < 2,if x ∈ [2, 3],if x ∈ (3, 4],if x > 4,(cid:1){medium}(cid:2)=mΓx0x − 317 − x0if x < 3,if x ∈ [3, 4],if x ∈ (4, 6],if x ∈ (6, 7],if x > 7,(cid:1){medium, large}(cid:2)=mΓx0x24 − x20if x < 6,− 3 if x ∈ [6, 7],if x ∈ (7, 8],if x > 8,J. Lawry / Artificial Intelligence 155 (2004) 1–3919Fig. 3. Mass assignments for varying x; shown from leftmΓx ({medium, large}) and mΓis equal to mΓx ({medium, large}) for x ∈ [6, 8] and is zero otherwise.x ({medium}), mΓx ({large}); mΓto right, mΓx (∅) is equal to mΓx ({small}), mΓx ({small, medium}),x ({small, medium}) for x ∈ [2, 4],(cid:17)(cid:1){large}(cid:2)=mΓx0x − 71if x < 7,if x ∈ [7, 8],if x > 8,mΓx (∅) =0x− 122 − x20x− 324 − x20if x < 2,if x ∈ [2, 3],if x ∈ (3, 4],if x ∈ (4, 6],if x ∈ (6, 7],if x ∈ (7, 8],if x > 8.This gives a set of focal elements FΓ = {∅, {small}, {small, medium}, {medium},{medium, large}, {large}} from which, for example, it follows that:µΓmedium∧¬large(x)(cid:1)= mΓx{small, medium}(cid:2)(cid:1)+ mΓx{medium}(cid:2)=0x217 − x0if x < 2,− 1 if x ∈ [2, 4],if x ∈ (4, 6],if x ∈ (6, 7],if x > 7,¬small∧medium∧¬large(x) = mΓµΓx(cid:1){medium}(cid:2)=if x < 3,0x − 3 if x ∈ [3, 4],if a ∈ (4, 6],1if x ∈ (6, 7],7 − xif x > 7,020J. Lawry / Artificial Intelligence 155 (2004) 1–39Fig. 4. Appropriateness degree µΓ(dashed line).medium∧¬large(x) (solid line) and min(µΓmedium(x), 1−µΓlarge(x)) = µΓmedium(x)¬(small∨medium)(x) = mΓµΓx(cid:1)(cid:2){large}+ mΓx (∅) =0x22 − x20x21if x < 2,− 1 if x ∈ [2, 3],if x ∈ (3, 4],if x ∈ (4, 6],− 3 if x ∈ (6, 8],if x > 8.Fig. 3 shows the values of the mass assignment mx for each focal element as x rangesacross Ω. From this we see that mass is associated with the empty set for values in theranges [2, 4] and [6, 8]. In label semantics this suggest that there are individuals in V forwhom none of the terms in LA are appropriate as labels for values in these ranges. Onemight observe that this phenomena occurs frequently in natural language especially whenlabelling perceptions generated along some continuum. For example, we occasionally en-counter colours for which none of our available colour descriptors seem appropriate. Fig. 4clearly illustrates the difference between label semantics and fuzzy logic when evaluatingcompound expressions such as, in this case, medium ∧ ¬large. It is interesting to note thatusing strongly functional fuzzy logic based on min as the conjunction function, the twostatements x is medium and x is medium but not large provide exactly the same informa-tion (i.e., they have the same memberships). In other words, the extra information that x isnot large tells us nothing. This seems highly counter intuitive. On the other hand, in labelsemantics µmedium∧¬large is zero for all values greater than seven since for such values theonly sets of appropriate labels with non-zero mass containing medium also contain large.4. Multi-dimensional label semanticsMost modelling problems involve multiple attributes or variables. Therefore, if labelsemantics is to provide an effective knowledge representation framework for linguisticJ. Lawry / Artificial Intelligence 155 (2004) 1–3921modelling it must be generalised to the multi-dimensional case. In other words, we need toprovide a means of interpreting and evaluating linguistic expressions involving more thanone variable.Specifically, consider a modelling problem with k variables (or attributes) x1, . . . , xkwith associated universes Ω1, . . . , Ωk. For each variable we define a set of labels LAj ={L1,j , . . . , Lnj ,j } for j = 1, . . . , k. In this case we ask individuals from V to provide aset of appropriate labels for each attribute value. Hence, an individual I will provide a(cid:23) for the attribute vector (cid:22)a1, . . . , ak(cid:23). In thisvector of label descriptions (cid:22)DIa1context we can extend the definitions of mass assignment and appropriateness degreegiven in Section 3 to the multi-dimensional case. Initially, however, we formally definek-dimensional linguistic expressions., . . . , DIakLet LEj be the set of label expression for variable xj generated by recursive applicationof the connectives ∧, ∨, →, ¬ to the labels in LAj . We can now define the set of multi-dimensional label expression for describing linguistic relationships between variables asfollows:Definition 24 (Multi-dimensional label expressions). MLE(k) is the set of all multi-dimensional label expressions that can be generated from the label expression LEj : j =1, . . . , k and is defined recursively by:(i) If θ ∈ LEj for j = 1, . . . , k then θ ∈ MLE(k).(ii) If θ, ϕ ∈ MLE(k) then ¬θ , θ ∧ ϕ, θ ∨ ϕ, θ → ϕ ∈ MLE(k).Any k-dimensional label expression θ identifies a subset of 2LA1 × · · · × 2LAk , denotedλ(k)(θ ), constraining the cross product of label descriptions Dx1× · · · × Dxk . In thisway the imprecise constraint θ on x1 × · · · × xk is interpreted as the precise constraintDx1× · · · × Dxk∈ λ(k)(θ ).Definition 25 (Multi-dimensional appropriate label sets). ∀θ ∈ MLE(k) λ(k)(θ ) ⊆ 2LA1 ×· · · × 2LAk such that• ∀θ ∈ LEj λ(k)(θ ) = 1λ(θ )××i"=j 2LAi .• ∀θ, ϕ ∈ MLE(k) λ(k)(θ ∧ ϕ) = λ(k)(θ ) ∩ λ(k)(ϕ).• λ(k)(θ ∨ ϕ) = λ(k)(θ ) ∪ λ(k)(ϕ).• λ(k)(θ → ϕ) = λ(k)(θ ) ∪ λ(k)(ϕ).• λ(k)(¬θ ) = λ(k)(θ ).Note that in the context of a particular frame Γ it may be more convenient to evaluateis the set of focal elements for LAj given frame Γ (seeΓ where F (j )F (j )j =1Γλ(k)(θ ) ∩ × kDefinition 17).1 λ(θ ) ⊆ LAj refers to the one dimensional appropriate label set as given in Definition 3.22J. Lawry / Artificial Intelligence 155 (2004) 1–39Example 26. Consider a modelling problem with two variables x1 and x2 for whichLA1 = {small, medium, large} and LA2 = {low, moderate, high}. Also suppose that for agiven frame Γ the focal elements for LA1 and LA2 are, respectively:(cid:4)(cid:4)F (1)ΓF (2)Γ=={small}, {small, medium}, {medium}, {medium, large}, {large}(cid:5).{low}, {low, moderate}, {moderate}, {moderate, high}, {high}(cid:5),Now according to Definition 25 we have that:(cid:1)(cid:2)(medium ∧ ¬small) ∧ ¬lowλ(2)= λ(2)(medium ∧ ¬small) ∩ λ(2)(¬low)= λ(medium ∧ ¬small) × λ(¬low).Nowandλ(medium ∧ ¬small) ∩ F (1)Γ=(cid:4){medium}, {medium, large}(cid:5)λ(¬low) ∩ F (2)Γ=(cid:4){moderate}, {moderate, high}, {high}(cid:5).Hence,(cid:2)× F (2)Γ(cid:1)λ(2)=(cid:1)∩F (1)Γ(cid:2)(medium ∧ ¬small) ∧ ¬low(cid:4)(cid:22){medium}, {moderate}(cid:23),(cid:22){medium}, {moderate, high}(cid:23),(cid:22){medium}, {high}(cid:23),(cid:22){medium, large}, {moderate}(cid:23),(cid:22){medium, large}, {moderate, high}(cid:23),(cid:5)(cid:22){medium, large}, {high}(cid:23).Definition 27 (Joint mass assignment).∀xj ∈ Ωj ∀Sj ⊆ LAj : j = 1, . . . , k m(cid:22)x1,...,xk(cid:23)(S1, . . . , Sk) =k(cid:22)j =1mxj (Sj ).Nowm(cid:22)x1,...,xk(cid:23)(S1, . . . , Sk) = PV(cid:1){I ∈ V : Dx1= S1, . . . , Dxk= Sk}(cid:2)provided we make the following conditional independence assumption. It is assumed thatfor each individual I the choice of appropriate labels for variable xj is dependent only onthe value of xj , once this is known, and is independent of the value of any other variables.This is actually quite a weak assumption and does not a prior imply independence betweenthe variables.J. Lawry / Artificial Intelligence 155 (2004) 1–3923Definition 28 (Multi-dimensional appropriateness degrees).∀θ ∈ MLE(k), ∀xj ∈ Ωj : j = 1, . . . , kµ(k)θ (x1, . . . , xk) =(cid:3)m(cid:22)x1,...,xk(cid:23)(S1, . . . , Sk)(cid:22)S1,...,Sk(cid:23)∈λ(k)(θ)(cid:3)k(cid:22)=(cid:22)S1,...,Sk(cid:23)∈λ(k)(θ)j =1mxj (Sj ).Proposition 29. If θ ∈ MLE(c) for c < k then∀xj ∈ Ωj : j = 1, . . . , k µ(k)θ (x1, . . . , xk) = µ(c)θ (x1, . . . , xc).Proof. By Definition 25 λ(k)(θ ) = λ(c)(θ )× × kj =c+12LAj and therefore,µ(k)θ (x1, . . . , xk) =(cid:3)(cid:3)m(cid:22)j =1mxj (Sj )(cid:3)k(cid:22)(cid:22)S1,...,Sc(cid:23)∈λ(c)(θ)(cid:3)(cid:22)Sc+1,...,Sk(cid:23)∈2LAc+1 ×···×2LAkc(cid:22)(cid:3)mxj (Sj ). . .mxj (Sj )(cid:22)S1,...,Sc(cid:23)∈λ(c)(θ)(cid:3)j =1c(cid:22)Sc+1⊆LAc+1(cid:23)k(cid:22)(cid:3)Sk⊆LAkj =c+1(cid:24)(cid:22)S1,...,Sc(cid:23)∈λ(c)(θ)(cid:3)j =1c(cid:22)(cid:22)S1,...,Sc(cid:23)∈λ(c)(θ)j =1mxj (Sj )mxj (Sj )j =c+1Sj ⊆LAjmxj (Sj ) = µ(c)θ (x1, . . . , xc)===as required. ✷Proposition 30. Let θj ∈ LEj : j = 1, . . . , k, then the appropriateness degree of theconditional (k−1j =1 θj ) → θk is given by(cid:9)µ(k)(cid:9)(k−1j=1 θj )→θk(x1, . . . , xk) = 1 −k−1(cid:22)j =1µθj (xj ) +k(cid:22)j =1µθj (xj ).Proof. By Definition 25 we have that(cid:25)(cid:25)(cid:25)(cid:26)(cid:26)λ(k)θj→ θk= λ(k)k−1(cid:11)j =1(cid:26)θj∪ λ(k)(θk) = λ(k)(cid:26)θj∩ λ(k)(θk).(cid:25)k−1(cid:11)j =1k−1(cid:11)j =1Now again by Definition 25 it follows that(cid:25)(cid:26)k−1(cid:11)j =1λ(k)θj∩ λ(k)(θk) = λ(θ1) × · · · × λ(θk−1) × λ(θk).24J. Lawry / Artificial Intelligence 155 (2004) 1–39Therefore,µ(k)(cid:9)((x1, . . . , xk)k−1j=1 θj )→θk(cid:23)k−1(cid:22)(cid:3)j =1Sj ∈λ(θj )k−1(cid:22)j =1µθj (xj )= 1 −(cid:23)= 1 −as required. ✷(cid:24)mxj (Sj )×(cid:27) (cid:3)(cid:28)mxk (Sk)(cid:24)(cid:29)×Sk∈λ(θk)(cid:30)1 − µθk (xk)= 1 −k−1(cid:22)j =1µθj (xj ) +k(cid:22)j =1µθj (xj )It is interesting to note that this corresponds to the use of a Reichenbach implicationoperator which, not surprisingly, is generated from the product t-conorm.Example 31. Consider a modelling problem with two variables x1, x2 each with universe[0, 10] and for which we have defined the label sets LA1 = {small1(s1), medium1(m1),large1(l1)} and LA2 = {small2(s2), medium2(m2), large2(l2)}. For both variables theappropriateness degrees for small, medium and large are defined as in Example 23. Nowsuppose we learn that:If x1 is medium but not large then x2 is mediumthen according to Proposition 30 the appropriateness degree for medium1 ∧ ¬large1medium2 is given by→µ(2)medium1∧¬large1= 1 − µmedium1∧¬large1(x1) + µmedium1∧¬large1(x1)µmedium2(x2).→medium2(x1, x2)Assuming the appropriateness degrees for small, medium and large given in Example 23then the resulting function is shown in Fig. 5.Clearly, this function provides information regarding the relationship between x1 and→ medium2. For instance, from Fig. 5 wex2 assuming the constraint medium1 ∧ ¬large1Fig. 5. Plot of the appropriateness degree for medium1 ∧ ¬large1→ medium2.J. Lawry / Artificial Intelligence 155 (2004) 1–3925can see that if x1 = 5 the it is very unlikely that 8 (cid:1) x2 (cid:1) 10. The problem of how to makeoutput predictions (for x2) given input values (for x1) is considered in detail in the sequel.Now suppose we also learn thatIf x1 is large then x2 is small.In this case we would want to evaluate the appropriateness degrees for the expression(medium1 ∧ ¬large1(cid:1)→ small2). For this expression we have→ medium2) ∧ (large1(cid:2)λ(2)=(m1 ∧ ¬l1 → m2) ∧ (l1 → s2)(cid:2)(cid:1)λ(2)(m1 ∧ ¬l1) ∪ λ(2)(m2)∩(cid:1)λ(2)(m1 ∧ ¬l1) ∩ λ(2)(m2)(cid:1)(cid:1)∪λ(m1 ∧ ¬l1) × λ(m2)∪(cid:2)(cid:2)(cid:1)(cid:1)==λ(2)(l1) ∪ λ(2)(s2)λ(2)(l1) ∩ λ(2)(s2)(cid:2).λ(l1) × λ(s2)(cid:2)(cid:2)Now(cid:1)λ(m1 ∧ ¬l1) × λ(m2) ∩(cid:5)×(cid:4){s1, m1}, {m1}=(cid:2)× F (2)F (1)Γ(cid:5)(cid:4){m2}, {m2, l2}, {l2}, ∅Γandλ(l1) × λ(s2) ∩(cid:5)(cid:4){m2}, {m2, l2}, {l2}, ∅Hence, since these two sets on 2LA1 × 2LA2 are mutually exclusive it follows that:(cid:4){l1}, {m1, l1}(cid:1)F (1)Γ× F (2)×=(cid:5)(cid:2)Γ.∀x1 ∈ Ω1, ∀x2 ∈ Ω2 µ(2)(cid:10) (cid:3)(cid:3)(m1∧¬l1→m2)∧(l1→s2)(x1, x2)mx1(S1)mx2(S2) +(cid:3)(cid:3)(cid:12)mx1(S1)mx2(S2)= 1 −(cid:1)= 1 −S1∈λ(m1∧¬l1)µm1∧¬l1(x1) ×S2∈λ(m2)(cid:1)1 − µm2(x2)(cid:2)S1∈λ(l1)(cid:1)S2∈λ(s2)1 − µs2(x2)+ µl1(x1) ×(cid:2)(cid:2).Again assuming the appropriateness degrees for small, medium and large given inExample 23 then the resulting function is shown in Fig. 6.Fig. 6. Plot of the appropriateness degree for (medium1 ∧ ¬large1→ medium2) ∧ (large1→ small2).26J. Lawry / Artificial Intelligence 155 (2004) 1–39The semantics proposed in this section are based on the idea that the meaning of vaguelinguistic expressions are determined by their use across a population of individuals. Thisis very close to the theory of vagueness proposed by Black [3]. An alternative viewpointis that fuzzy concepts are inherently vague independent of their actual use. In principle, itmay be possible to provide an operational semantics for membership functions consistentwith this interpretation but very little foundational work of this kind has been undertaken.For example, one possible semantics of this kind is based on the idea that membershipvalues are a measure of similarity to some set of prototypical exemplars of the concept(see [31,33]). However, such an alternative approach to the problem of vague conceptsis not within the scope of this paper where instead we focus entirely on the random setinterpretation.5. Conditional information from linguistic constraintsTo understand what is the information content of linguistic expressions we must alsoconsider the nature of the constraints that such expression place on the underlying variable.If it is know that Bill is tall exactly what does this tell us about Bill’s height? For example,can we determine an exact value, a distribution of values or a family of distributions? In[37] it is proposed that such constraints specify a possibility distribution on the underlyingvariable, namely that given by the membership degree of the associated fuzzy set. This initself would suggest a resulting family of probability distributions as characterised by thecorresponding possibility and dual necessity measure. However, in many applications offuzzy sets, in particular fuzzy control, so-called defuzzification techniques seem to treatthe possibility distribution as if it were a probability distribution in order to estimate aprecise value for the variable. In the case of Bill’s height the so called centre of massdefuzzification method (see [30]) would evaluate(cid:31)Ω xµtall(x) dx(cid:31)Ω µtall(x) dx.Clearly, this has no obvious semantic justification in fuzzy set theory. In addition, theassociation of membership values with fuzzy sets as discussed in [37] is more in the natureof a primitive definition rather than being a consequence of some lower level semantics foreither membership or possibility.In this section we will introduce a label semantics based approach to linguisticconstraints for which we will argue that in order to make any inferences about theunderlying variable based on a linguistic expression we must not only have knowledgeof the frame but also of the associated extended frame. Since appropriateness degrees,the analogue of fuzzy memberships in label semantics, are determined by the frame onlythen clearly we are claiming that such information alone is inadequate to draw any butthe most general conclusions from linguistic expressions. For simplicity, we will assumein the sequel that the extended frame Γ + is such that either PΩ is discrete or it has anassociated density function pΩ . Now consider a knowledge base consisting of a singlelabel expression θ with meaning Dx ∈ λ(θ ). Then according to Bayes’ theorem we caninfer the following posterior distribution on Ω:J. Lawry / Artificial Intelligence 155 (2004) 1–3927– Continuous case.∀a ∈ Ω p(a|θ ) = Pr(θ | x = a)pΩ (a)(cid:31)Ω Pr(θ | x)pΩ (x) dx.– Discrete case.∀a ∈ Ω Pr(x = a|θ ) =(cid:7)Pr(θ | x = a)PΩ (x = a)x∈Ω: PΩ (x)>0 Pr(θ | x)PΩ (x).Now according to label semanticsPr(θ | x = a) = Pr(cid:1)Dx ∈ λ(θ ) | x = a(cid:2)(cid:1)Da ∈ λ(θ )(cid:2)== Pr(cid:3)S∈λ(θ)ma(S) = µθ (a).Therefore, we obtain in the continuous case∀x ∈ Ω p(x | θ ) =(cid:31)µθ (x)pΩ (x)Ω µθ (x)pΩ (x) dxand in the discrete case∀x ∈ Ω Pr(x | θ ) =(cid:7)µθ (x)PΩ (x)x∈Ω: PΩ (x)>0 µθ (x)PΩ (x).From the above we can see that appropriateness degrees may be viewed as a likelihoodmeasure (i.e., µθ (x) can be interpreted as the likelihood that θ is an appropriate labelfor x). This is not surprising since as Dubois and Prade [6] comment, the likelihoodand random set semantics for fuzzy concepts are strongly linked. A number of authorshave independently investigated likelihood semantics for possibility or fuzzy sets outsidethe random set framework, including Hisdal [17] and Dubois, Moral and Prade [7]. AlsoThomas [32] has investigated the relationship between Bayesian reasoning and fuzzy sets.The above likelihood interpretation has important consequences regarding the level ofcondition information we can obtain from the knowledge that x is constrained by θ . Clearlygiven that appropriateness degrees are defined for LA (i.e., we have sufficient knowledgeof the frame) then our knowledge of the above posterior distribution depends entirelyon our knowledge of the prior PΩ (i.e., the associated extended frame). For example, ifwe know only that PΩ ∈ P, for some set of distributions P, then we will only be ableto determine upper and lower bounds for the posterior describing an inferred family ofposterior distributions. In the discrete case these upper and lower probabilities are definedas follows:∀x ∈ Ω Pr∗(x | θ ) = supPΩ ∈P(cid:7)µθ (x)PΩ (x)x∈Ω µθ (x)PΩ (x)and∀x ∈ Ω Pr∗(x | θ ) = infPΩ ∈P(cid:7)µθ (x)PΩ (x)x∈Ω µθ (x)PΩ (x).This essentially, corresponds to a special case of imprecise Bayesian inference as proposedby Walley [34]. However, it should be noted that often in practice surprisingly littleinformation can be inferred from such knowledge. For example, consider the scenario28J. Lawry / Artificial Intelligence 155 (2004) 1–39described in Example 1, but where our prior knowledge is that the score on the dice iseither 2 or 3 (i.e., P = {PΩ | PΩ (2) + PΩ (3) = 1}). Furthermore, suppose that we are alsoinformed that the SCORE is low. Now since the appropriateness degree of low for 2 is 1while for 3 it is only 13 one might expect that we could safely infer thatPr(SCORE = 2 | low) (cid:2) Pr(SCORE = 3 | low).A little trivial mathematics, however, reveals that this is not the case since for a priorwhere PΩ (2) = 1 we find that Pr(SCORE = 2 | low) = 1 and Pr(SCORE = 3 | low) = 0,while for a prior where PΩ (3) = 1 we obtain Pr(SCORE = 3 | low) = 1 and Pr(SCORE =2 | low) = 0. Obviously, then in terms of upper and lower bounds we can infer only thatPr(SCORE = 2 | low), Pr(SCORE = 3 | low) ∈ [0, 1] and Pr(SCORE ∈ {1, 2} | low) = 1.We see then that even in the presence of relatively specific linguistic constraints theinformation we can infer about the value of the underlying variable is strongly dependenton our prior assumptions about its distribution. For instance, the inequalityPr(SCORE = 2 | low) (cid:2) Pr(SCORE = 3 | low)as suggested above, holds if and only if= 14µlow(3)µlow(2) + µlow(3)PΩ (2) (cid:2).See [35] for an alternative semantics for linguistic concepts based on upper and lowerprobabilities.A common, although sometimes problematic (see [36] for a discussion) assumption inBayesian analysis is to assume a uniform prior on Ω. In this case we obtainPr(2 | low) =µlow(2)µlow(2) + µlow(3)= 34andPr(3 | low) =µlow(3)µlow(2) + µlow(3)= 14.Generally, the assumption of a uniform prior on Ω gives us Pr(x|θ ) proportional to µθ (x),that isp(x | θ ) =µθ (x)(cid:31)Ω µθ (x) dxin the continuous case andPr(x | θ ) =(cid:7)µθ (x)x∈Ω µθ (x)in the discrete case.Now if it is required that we estimate a precise value of x on the basis of a linguisticconstraint θ then a natural approach in the current context would be simply to determine theexpected value of Pr(x | θ ). Clearly, if a uniform prior on Ω is assumed then this gives usan expression for appropriateness degrees equivalent to the centre of mass defuzzificationmethod as described above.J. Lawry / Artificial Intelligence 155 (2004) 1–3929We now illustrate how such conditioning can be used to evaluate output values ofa system given specific input values when the relationships between input and outputvariables are described in terms of linguistic expressions. Initially, however, we observethat the conditional distributions defined above can easily be extended to the multi-dimensional case so that for θ ∈ MLE(k):∀xj ∈ Ωj , j = 1, . . . , k,p(x1, . . . , xk | θ ) =(cid:31). . .where p(x1, . . . , xk) is the prior distribution on Ω1 × · · · × Ωk.µ(k)θ (x1, . . . , xk)p(x1, . . . , xk) d$xΩ1Ωkµ(k)(cid:31)θ (x1, . . . , xk)p(x1, . . . , xk),Example 32. Recall the problem described in Example 31 where our knowledge of therelationship between variables x1 and x2 corresponds to→ medium2) ∧ (large1Assuming a uniform prior distribution on [0, 10]2 then the posterior distribution on x1 × x2is given byK ≡ (medium1 ∧ ¬large1→ small2).∀x1 ∈ Ω1, x2 ∈ Ω2p(x1, x2 | K) =µ(2)(m1∧¬l1→m2)∧(l1→s2)(x1, x2)(cid:31)0 µ(2)(m1∧¬l1→m2)∧(l1→s2)(x1, x2) dx1 dx210.(cid:31)100Now suppose we are given the value of x1 and we want to calculate the probability ofdifferent values of x2 given this information. In this case we need to evaluate the followingconditional distribution:p(x2 | K, x1) = p(x1, x2 | K)p(x1 | K)=µ(2)(m1∧¬l1→m2)∧(l1→s2)(x1, x2)(cid:31)0 µ(2)(m1∧¬l1→m2)∧(l1→s2)(x1, x2) dx210.A plot of this distribution as both x1 and x2 vary is given in Fig. 7.In the case that x1 = 6.5 the conditional density p(x2 | K, 6.5) is shown in Fig. 8.Fig. 7. Plot of the conditional density p(x2 | K, x1).30J. Lawry / Artificial Intelligence 155 (2004) 1–39Fig. 8. Plot of the conditional density p(x2 | K, 6.5).Therefore, in order to obtain an estimate of output x2 given input x1 = 6.5 we canevaluate the expected value of this distribution:10 ˆx1 =x2p(x2 | K, 6.5) dx2 = 4.5079.0In many situations our conditional information may not take the form of a linguisticexpression but rather a distribution on linguistic expressions. In label semantics suchinformation provides constraints on the distribution (mass assignment) for Dx as x varies.Here we consider only the simplest case where sufficient constraints are available tospecify a unique mass assignment on Dx . To illustrate how such specific knowledgemight be obtained let us return to the height problem where we have an extended frameΓ + = (cid:22)V , PV , Ω, PΩ (cid:23) where PΩ is a prior based on a known distribution on heights ofEuropean males. Furthermore, suppose we have a database DB of heights of a finite numberof British males so that for x ∈ Ω, PDB(x) corresponds to the probability of a male ofheight x being chosen at random from DB. Given this we can evaluate a mass assignmenton Dx conditional on the information that x is the height of a British male as follows:∀S ⊆ LA mDB(S) =(cid:3)PDB(x)mx(S).x∈Ω: PDB(x)>0Now given a posterior mass assignment mDB on Dx what information can we inferregarding the underlying variable x? According to the theorem of total probabilityp(a) =Pr(Dx = S)p(a | Dx = S).(cid:3)S⊆LAHence, if we know that ∀S ⊆ LA Pr(Dx = S) = mDB(S) then we can condition on thisknowledge as follows:(cid:3)p(a | mDB) =mDB(S)p(a | Dx = S).S⊆LANow according to Bayes theoremJ. Lawry / Artificial Intelligence 155 (2004) 1–3931Fig. 9. Appropriateness degrees for labels of diastolic blood pressure.∀a ∈ Ω p(a | Dx = S) ==Pr(Dx = S | x = a)pΩ(a)(cid:31)Ω Pr(Dx = S | x = a)pΩ (a) dama(S)pΩ (a)(cid:31)Ω ma(S)pΩ (a) da.LetΩma(S)pΩ (a) da = pm(S)be the prior mass assignment on Dx generated by prior distribution PΩ on Ω then we have∀x ∈ Ω p(x | mDB) =(cid:3)S⊆LAmDB(S)mx (S)pΩ (x)pm(S)= pΩ (x)(cid:3)S⊆LAmDB(S)pm(S)mx(S).Notice that in the case where ∀S ⊆ LA mDB(S) = pm(S), in other words when ourposterior knowledge of Dx matches our prior knowledge, then p(x | mDB) = pΩ (x) as onewould intuitively expect.Example 33. This example relates to a database stored as part of the machine learningrepository at the University of California at Irvine. It is essentially a classificationproblem but serves well to illustrate the use of label semantics to determine underlyingdistributions from data. The database itself contains the details of 768 females from thepopulation of Pima Indians living near Phoenix Arizona, USA. The diagnostic binary-valued variable investigated is whether the patient shows signs of diabetes according toWorld Health Organisation criteria. There are eight measured variables which include,number of times pregnant, plasma glucose concentration, diastolic blood pressure, tricepsskin fold thickness, 2-hour serum insulin, body mass index, diabetes pedigree functionand age. A label semantics approach has been used in [21] and [29] in conjunction with aBayesian classifier but here we shall simply use this example to show how a posteriormass assignment can be used to infer a posterior density on the underlying variable, 32J. Lawry / Artificial Intelligence 155 (2004) 1–39Fig. 10. Mass assignment on labels for the diastolic blood pressure of diabetics.diastolic blood pressure. In this case LA = {very small, small, medium, large, very large}with appropriateness degrees as shown in Fig. 9. These functions have been defined using apercentile based approach to ensure that each label covers approximately the same numberof data elements. Clearly, the set of focal elements for this frame is given by(cid:4)FΓ ={very small}, {small, very small}, {small}, {small, medium}, {medium},{medium, large}, {large}, {large, very large}, {very large}(cid:5).The extended frame is assumed to be such that Ω = [0, 122] and PΩ is the uniformdistribution on this interval. The posterior mass assignments (see Fig. 10) generated fromthe sub-database, DIAB, of diabetic individuals is given bymDIAB = {very small} : 0.0805969, {very small, small} : 0.110448,{small} : 0.101492, {small, medium} : 0.117911, {medium} : 0.07462,{medium, large} : 0.084223, {large} : 0.106608,{large, very large} : 0.222976, {very large} : 0.101119.The prior mass assignment for this domain based on a uniform prior PΩ ispm = {very small} : 0.368, {very small, small} : 0.1434, {small} : 0.04099,{small, medium} : 0.03074, {medium} : 0.02049, {medium, large} : 0.02459,{large} : 0.02869, {large, very large} : 0.09631, {very large} : 0.2459.Now, for instance, if a = 68 we have thatm68 = {small} : 0.4, {small, medium} : 0.6and therefore,J. Lawry / Artificial Intelligence 155 (2004) 1–3933Fig. 11. Posterior density on diastolic blood pressure for diabetics.(cid:27)(cid:1){small}(cid:2)mDIAB({small})p(68 | mDIAB) = 1m68pm({small})122+ mDIAB({small, medium})pm({small, medium})(cid:27)0.1014920.04099= 1122(0.4) + 0.117910.03074(cid:1){small, medium}m68(cid:28)(cid:2)(cid:28)(0.6)= 0.0269822.The full posterior density obtained is shown in Fig. 11.Another interesting issue relating to conditional inference with linguistic expressions isthat of the conditional matching of expressions. For example, suppose we are told that Billis tall then with what level of certainty, if any, can we infer that Bill is very tall? In the nextsection we propose two approaches to matching within the framework of label semantics,both probabilistic in nature.6. Matching linguistic expressionsSuppose it is known that the variable x is constrained by the linguistic expression ϕ.In this case, what is the degree to which another expression, θ , can appropriately beused to describe x. This is an important question that takes on special significance in thearea of fuzzy or possibilistic logic programming [1,2,8]. In this context a mechanism isrequired by which we can evaluate the semantic match (or unification) of an expressionθ , forming part of a query, with a given expression ϕ in the knowledge base. A numberof authors have investigate this problem but of most relevance to the current frameworkis work by Baldwin et al. [2] who introduces a measure of semantic unification based on34J. Lawry / Artificial Intelligence 155 (2004) 1–39the conditional probability of fuzzy events. This measure is also based on random sets, butdefined on the attribute universe rather than at the label level. In this section we present twomeasures of matching between expressions and discuss their respective properties. The firstapproach is as follows: If we know that linguistic expression ϕ holds then this correspondsto the event Dx ∈ λ(ϕ) and, according to Bayesian inference, we should update our priormass assignment m to obtain a posterior mass assignment mϕ as follows:∀S ⊆ LA mϕ(S) =(cid:17)pm(S)S∈λ(ϕ) pm(S)(cid:7)0if S ∈ λ(ϕ),otherwise.Interestingly mϕ can be used to show the consistency between the definition ofconditional distribution given a linguistic expression and that of conditional distributiongiven a mass assignment as is highlighted by the following proposition. (NB. In thefollowing two proofs it is assumed PΩ has a density pΩ . The finite case can be provedin a similar way.)Proposition 34. ∀x ∈ Ω p(x|mϕ) = p(x | ϕ).Proof.∀x ∈ Ω p(x | mϕ) = pΩ (x)by the definition of mϕ(cid:3)S⊆LAmϕ(S)pm(S)mx(S) = pΩ (x)(cid:3)S∈λ(ϕ)mϕ(S)pm(S)mx(S)= pΩ (x)(cid:3)S∈λ(ϕ)(cid:7)mx(S)S∈λ(ϕ) pm(S)= pΩ (x)(cid:7)(cid:7)S∈λ(ϕ) mx(S)S∈λ(ϕ) pm(S).(cid:7)NowS∈λ(ϕ) mx(S) = µϕ(x) by Definition 11 and(cid:3)(cid:3)pm(S) =mx(S)pΩ (x) dx =(cid:10) (cid:3)(cid:12)mx (S)pΩ (x) dxS∈λ(ϕ)S∈λ(ϕ)ΩS∈λ(ϕ)Ωµϕ(x)pΩ(x) dx.=ΩThereforep(x | mϕ) =(cid:31)pΩ (x)µϕ(x)Ω pΩ (x)µϕ(x) dx= p(x | ϕ).✷Given mϕ we can then evaluate the likelihood of another linguistic expression θaccording to the following definition.Definition 35 (Matching of type I).∀θ, ϕ ∈ LE µΓ +θ|ϕ=(cid:3)mϕ(S).S∈λ(θ)  J. Lawry / Artificial Intelligence 155 (2004) 1–3935It can easily be seen that this definition of match can be expressed in terms of conditionalprobabilities on Dx as follows:Proposition 36. ∀θ, ϕ ∈ LE µΓ +θ|ϕ= Pr(Dx ∈ λ(θ ) | Dx ∈ λ(ϕ)).Proof.(cid:1)PrDx ∈ λ(θ ) | Dx ∈ λ(ϕ)(cid:2)= Pr(Dx ∈ λ(θ ) ∩ λ(ϕ))Pr(Dx ∈ λ(ϕ))= Pr(Dx ∈ λ(θ ∧ ϕ))Pr(Dx ∈ λ(ϕ))by Definition 3 and by Bayes’ theorem===(cid:31)Ω Pr(Dx ∈ λ(θ ∧ ϕ) | x = a)pΩ (a) da(cid:31)Ω Pr(Dx ∈ λ(ϕ) | x = a)pΩ (a) da(cid:31)(cid:7)Ω ma(S)pΩ (a) daS∈λ(θ∧ϕ)(cid:31)Ω ma(S)pΩ (a) daS∈λ(ϕ)(cid:7)(cid:7)=(cid:3)S∈λ(θ∧ϕ)(cid:7)pm(S)S∈λ(ϕ) pm(S)=(cid:3)S∈λ(θ)S∈λ(θ∧ϕ) pm(S)(cid:7)S∈λ(ϕ) pm(S)mϕ(S).✷(cid:7)(cid:31)Ω ((cid:31)Ω (S∈λ(θ∧ϕ) ma(S))pΩ (a) da(cid:7)S∈λ(ϕ) ma(S))pΩ (a) da=Given the above it can easily be seen thatµΓ +θ|ϕ=(cid:31)Ω µθ∧ϕ(x)pΩ (x) dx(cid:31)Ω µϕ(x)pΩ (x) dx.= 1 and µΓ +¬ϕ|ϕInterestingly, when θ, ϕ ∈ LA this corresponds to the degree of subsethood measureas proposed by Kosko [19], although for compound expressions this is not the case.Also, this proposition shows that µΓ +θ|ϕ truly is the conditional extension of the generalappropriateness measure defined in Section 3, as the notation suggests. Furthermore, noticethat trivially µΓ += 0 while this is not the case for many definitions ofϕ|ϕconditional match proposed in the literature (for example, [2]). However, in label semanticsthere is an alternative definition for match that does not satisfy these properties, defined asfollows. Suppose we know that ϕ has been asserted by some individual in V then what isthe likelihood that θ will hold true for some other individual randomly chosen from V . Toevaluate this we observe that given ϕ we can determine a distribution on the underlyingvariable x, p(x | ϕ), and also that for any value of x we know the probability that θwill be deemed an appropriate label expression, µθ (x). From this we obtain the followingdefinition for the match of θ given ϕ.Definition 37 (Matching of type II).∀θ, ϕ ∈ LE π Γ +θ|ϕ=µθ (x)p(x | ϕ) dx.ΩGiven the definition of p(x|ϕ) this can be rewritten asπ Γ +θ|ϕ=(cid:31)Ω µθ (x)µϕ(x)pΩ (x) dx(cid:31)Ω µϕ(x)pΩ (x) da. 36J. Lawry / Artificial Intelligence 155 (2004) 1–39+Fig. 12. Matching of types I and II for large given medium: The dark grey area corresponds to the numerator ofπ Γlarge|medium assuming a uniform prior and the sum of the dark and light grey areas corresponds to the numeratorof µΓ+large|medium.Interestingly, taking µ to be analogous to fuzzy memberships then this corresponds tothe definition of conditional probability of fuzzy events proposed in [37]. Obviously, inthis case π Γ +¬ϕ|ϕ to 0). However, this is quiteintuitive given the prevailing interpretation for π since the fact that a particular individualdeems ϕ to be an appropriate label expression does not guarantee that all individuals will.ϕ|ϕ is not necessarily equal to 1 (or indeed π Γ +Example 38. Consider an extended frame for which Ω = [0, 1], PΩ is the uniformdistribution on [0, 1] andx−0.250.250.75−x0.250x ∈ [0.25, 0.5),x ∈ [0.5, 0.75],otherwise,µlarge =x−0.50.251−x0.250x ∈ [0.5, 0.75),x ∈ [0.75, 1],otherwise,µmedium =then we have thatµΓ +large|medium=(cid:31)10 µlarge∧medium(x) dx(cid:31)10 µmedium(x) dx(cid:31)=10 min(µmedium(x), µlarge(x)) dx(cid:31)10 µmedium(x) dx= 0.06250.25= 0.25.Alternatively,π Γ +large|medium=(cid:31)10 µlarge(x)µmedium(x) dx(cid:31)10 µmedium(x) dx== 0.04166670.25= 0.166667.0.75(cid:31)0.25 )( x−0.50.5 ( 0.75−x0.250.25 ) dxJ. Lawry / Artificial Intelligence 155 (2004) 1–3937Fig. 13. Matching of type II for medium given medium: The dark grey area represents the numerator inπ Γmedium|medium.+Now as stated above µΓ +medium|medium(cid:31)10 µ2medium(x) dx(cid:31)10 µmedium(x) dxπ Γ +medium|medium== 1 but on the other hand(cid:31)=0.50.25( x−0.250.25 )2 dx0.25= 0.1666670.25= 0.666667.7. ConclusionsA new framework for linguistic modelling, referred to as label semantics, has beenpresented, based on a random set interpretation of the measure of appropriateness of alabel for a value. A natural, weakly functional calculus for appropriateness degree, has beendescribed, which satisfies the law of the excluded middle and in general takes account of thelogical structure of compound expressions when evaluating them. This calculus can then becombined with a bayesian framework to provide a means of inferring distributions on theunderlying variable given both linguistic expressions and mass assignments. Furthermore,given a linguistic expression we have also presented methods for evaluating the likelyapplicability of other linguistic expressions based on the measures of conditional matchtypes I and II.Overall we would claim that label semantics has the potential to act as an effectivehigh level knowledge representation framework for many modelling problems. At presentapplications have centred on its use in data mining and machine learning where a numberof new methods have been developed based on the ideas proposed in Section 4 (see Lawry[21] and Randon [29]). In this context label semantics offers the prospect of combiningboth numerical and linguistic reasoning as discussed in Lawry [23]. Furthermore, itprovides a mechanism for conditioning on prior or background linguistic informationto infer probability distributions which can then be used in conjunction with modelsderived from data. In a different context, the method for estimating distributions from datadescribed in Section 4 has also been used to evaluate imprecise probabilities of failure38J. Lawry / Artificial Intelligence 155 (2004) 1–39for risk analysis in environmental engineering (see [15] and [16]). More generally, theframework outlined in this paper gives us a coherent calculus for linguistic reasoningthat may be used in a variety of decision-support problems. Certainly, in this context, thenotions of appropriateness degree and conditional match will have a central role to play.AcknowledgementsMany thanks to Tru Cao and John Shepherdson for their helpful discussions. I wouldalso like to thank one of the anonymous referees for their patience and insightful comments.This work is partially funded by a grant from the Nuffield Foundation.References[1] T. Alsinet, L. Godo, A complete calculus for possibilistic logic programming with fuzzy propositionalvariable, in: Proceedings of Uncertainty in AI 2000, Stanford, CA, 2000.[2] J.F. Baldwin, T.P. Martin, B.W. Pilsworth, Fril—Fuzzy and Evidential Reasoning in AI, Wiley, New York,1995.[3] M. Black, Vagueness: An exercise in logical analysis, Philos. Sci. 4 (1937) 427–455.[4] D. Dubois, H. Prade, An introduction to possibility and fuzzy logics, in: P. Smets, et al. (Eds.), Non-StandardLogics for Automated Reasoning, Academic Press, New York, 1988, pp. 742–755.[5] D. Dubois, H. Prade, Can we enforce full compositionality in uncertainty calculi?, in: Proc. AAAI-94,Seattle, WA, 1994, pp. 149–154.[6] D. Dubois, H. Prade, The three semantics of fuzzy sets, Fuzzy Sets and Systems 90 (1997) 141–150.[7] D. Dubois, S. Moral, H. Prade, A semantics for possibility theory based on likelihoods, J. Math. Anal.Appl. 205 (1997) 359–380.[8] D. Dubois, H. Prade, Possibility theory: Qualitative and quantitative aspects,in: D.M. Gabbay, P.Smets (Eds.), Handbook of Defeasible Reasoning and Uncertainty Management Systems, vol. 1, Kluwer,Dordrecht, 1998, pp. 169–226.[9] C. Elkan, The paradoxical success of fuzzy logic, in: Proc. AAAI-93, Washington, DC, MIT Press,Cambridge, MA, 1993, pp. 698–703.[10] B.R. Gaines, Fuzzy and probability uncertainty logics, J. Inform. Control 38 (1978) 154–169.[11] H. Geffner, Default Reasoning: Causal and Conditional Theories, MIT Press, Cambridge, MA, 1992.[12] I.R. Goodman, Fuzzy sets as equivalence classes of random sets, in: R. Rager (Ed.), Fuzzy Set and PossibilityTheory, 1982, pp. 327–342.[13] I.R. Goodman, H.T. Nguyen, Uncertainty Models for Knowledge Based Systems, North-Holland, Amster-dam, 1985.[14] P. Hajek, Fuzzy logic from the logical point of view, in: M. Bartosek, et al. (Eds.), SOFSEM 95: Theory andPractice of Informatics, in: Lecture Notes in Computer Science, vol. 1012, 1995, pp. 31–49.[15] J. Hall, J. Lawry, Imprecise probabilities of engineering system failure from random and fuzzy set reliabilityanalysis, in: Proceedings of the Second International Symposium on Imprecise Probabilities and TheirApplications, New York, 2001.[16] J. Hall, J. Lawry, Fuzzy label methods for constructing imprecise limit state functions, Structural Safety 28(2003) 317–341.[17] E. Hisdal, Are grades of membership probabilities, Fuzzy Sets and Systems 25 (1988) 325–348.[18] G.J. Klir, B. Yuan, Fuzzy Sets and Fuzzy Logic, Prentice-Hall, Englewood Cliffs, NJ, 1995.[19] B. Kosko, Neural Networks and Fuzzy Systems: A Dynamical Systems Approach to Machine Intelligence,Prentice-Hall, Englewood Cliffs, NJ, 1992.[20] J. Lawry, A voting mechanism for fuzzy logic, Internat. J. Approx. Reason. 19 (1998) 315–333.J. Lawry / Artificial Intelligence 155 (2004) 1–3939[21] J. Lawry, Label prototypes for modelling with words, in: Proceedings of The North American FuzzyInformation Processing Society 2001 Conference, 2001.[22] J. Lawry, Label semantics: A formal framework for modelling with words, in: S. Benferhat, P. Besnard(Eds.), Proceedings of Sixth European Conference on Symbolic and Quantitative Approaches to Reasoningwith Uncertainty, in: Lecture Notes in Artificial Intelligence, vol. 2143, Springer, Berlin, 2001, pp. 374–384.[23] J. Lawry, Query evaluation from linguistic prototypes, in: Proceedings of the FUZZ-IEEE 2001 Workshopon Modelling with Words, Melbourne, Australia, 2001, pp. 39–42.[24] J.W. Lloyd, Foundations of Logic Programming, Second Edition, Springer, Berlin, 1987.[25] H.T. Nguyen, On modeling of linguistic information using random sets, Inform. Sci. 34 (1984) 265–274.[26] J.B. Paris, The Uncertain Reasoners Companion: A Mathematical Perspective, Cambridge University Press,Cambridge, 1994.[27] J.B. Paris, Semantics for fuzzy logic supporting truth functionality, in: V. Novak, I. Perfilieva (Eds.),Discovering the World with Fuzzy Logic, Springer, Berlin, 2000.[28] J. Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, San Mateo, CA, 1988.[29] N.J. Randon, J. Lawry, A transparent framework for data mining using modelling with words,in:Proceedings of 2001 UK Workshop on Computational Intelligence, 2001.[30] E.H. Ruspini, P.P. Bonnisone, W. Pedtycz (Eds.), Handbook of Fuzzy Computation, Institute of PhysicsPublishing, 1998.[31] E.H. Ruspini, On the semantics of fuzzy logic, Internat. J. Approx. Reason. 5 (1991) 45–88.[32] S.F. Thomas, Fuzziness and Probability, ACG Press, Kansas, 1995.[33] E. Trillas, L. Valverde, An enquiry into indistinguishability operators, in: H.J. Skala, S. Termini, E. Trillas(Eds.), Aspects of Vagueness, Kluwer Academic Publishers, Dordrecht, 1984, pp. 231–256.[34] P. Walley, Statistical Inference with Imprecise Probabilities, Chapman and Hall, London, 1991.[35] P. Walley, G. de Cooman, A behavioural model of linguistic uncertainty, Inform. Sci. 34 (1999) 1–37.[36] S.L. Zabell, Symmetry and its discontents, in: Causation, Chance, and Credence, Vol. 1, Kluwer Academic,Dordrecht, 1988, pp. 155–190.[37] L.A. Zadeh, Probability measures of fuzzy events, J. Math. Anal. Appl. 23 (1968) 421–427.[38] L.A. Zadeh, The concept of linguistic variable and its application to approximate reasoning, Part 1, Inform.Sci. 8 (1975) 199–249.[39] L.A. Zadeh, The concept of linguistic variable and its application to approximate reasoning, Part 2, Inform.Sci. 8 (1975) 301–357.[40] L.A. Zadeh, The concept of linguistic variable and its application to approximate reasoning, Part 3, Inform.Sci. 9 (1976) 43–80.[41] L.A. Zadeh, Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets and Systems 1 (1978) 3–28.[42] L.A. Zadeh, Fuzzy logic = computing with words, IEEE Trans. Fuzzy Systems 2 (1996) 103–111.