Artificial Intelligence 170 (2006) 620–642www.elsevier.com/locate/artintIs real-valued minimax pathological?Mitja Luštrek a,∗, Matjaž Gams a, Ivan Bratko ba Department of Intelligent Systems, Jožef Stefan Institute, Jamova 39, 1000 Ljubljana, Sloveniab Faculty of Computer and Information Science, University of Ljubljana, Tržaška cesta 25, 1000 Ljubljana, SloveniaReceived 4 January 2005; received in revised form 9 January 2006; accepted 15 January 2006Available online 17 February 2006AbstractDeeper searches in game-playing programs relying on the minimax principle generally produce better results. Theoretical analy-ses, however, suggest that in many cases minimaxing amplifies the noise introduced by the heuristic function used to evaluate theleaves of the game tree, leading to what is known as pathological behavior, where deeper searches produce worse results. In mostminimax models analyzed in previous research, positions’ true values and sometimes also heuristic values were only losses andwins. In contrast to this, a model is proposed in this paper that uses real numbers for both true and heuristic values. This model didnot behave pathologically in the experiments performed. The mechanism that causes deeper searches to produce better evaluationsis explained. A comparison with chess is made, indicating that the model realistically reflects position evaluations in chess-playingprograms. Conditions under which the pathology might appear in a real-value model are also examined. The essential differencebetween our real-value model and the common two-value model, which causes the pathology in the two-value model, is identified.Most previous research reports that the pathology tends to disappear when there are dependences between the values of siblingnodes in a game tree. In this paper, another explanation is presented which indicates that in the two-value models the error of theheuristic evaluation was not modeled realistically. 2006 Elsevier B.V. All rights reserved.Keywords: Game playing; Minimax; Pathology; Game tree; Real value; Chess1. IntroductionThe minimax principle lies at the heart of almost every game-playing program. Programs typically choose the bestmove by searching the game tree, heuristically evaluating the leaves and then propagating their values to the root usingthe minimax principle. In practice, deeper searches generally produce better results. However, mathematical analysisindicates that under certain seemingly sensible conditions, the opposite is true: minimaxing amplifies the error ofthe heuristic evaluation [2,10]. This phenomenon is called the minimax pathology [10]. Nau [14] described this as“doing worse by working harder”. Evidently, game trees of real games must be different from game trees used in thetheoretical analyses in some way that eliminates the pathology. Several explanations of what property of game treesof real games might be responsible have been put forth: similarity of positions close to each other [3,11], presence ofstabilizing game-tree nodes with reliable estimates [4,16] etc. And while these properties can be shown to eliminate* Corresponding author.E-mail addresses: mitja.lustrek@ijs.si (M. Luštrek), matjaz.gams@ijs.si (M. Gams), bratko@fri.uni-lj.si (I. Bratko).0004-3702/$ – see front matter  2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.01.006M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642621the pathology, the question to what extent they are actually present in game trees of real games remains. Also, themechanism by which they achieve its goal is sometimes inadequately explained.This paper attempts to address both of these problems: we model game trees in a way that we believe reflects wellthe properties of real games and we explain the mechanism that eliminates the pathology. In our game trees, positionsclose to each other are similar. But unlike most models used in previous research, in which positions’ true values, andsometimes also heuristic values, could only be losses or wins, we use real numbers for both true and heuristic values.This makes it possible to model the similarity between a position’s descendants in a more natural way, and enables amore direct comparison to game-playing programs, which typically use a sizable range of integer values. The modelcan be analyzed mathematically to explain the mechanism that makes minimax effective. Multiple types of error anddifferent experimental settings, among them the approximation of real values by a limited number of discrete values,are used to determine when exactly is the pathology present in a real-value model. Some light is also shed on thereasons for pathological behavior of the two-value models used in previous research.The paper is organized as follows. Section 2 is a short introduction to the minimax pathology with some historicaloverview. Section 3 presents our model of minimax. Section 4 gives a mathematical analysis of minimax with real-number values. Section 5 compares our model to a chess program. Section 6 explains when and how the pathologyappears in a real-value model and examines the differences between real-value and two-value models. Section 7concludes the paper and points out some areas where further research is needed.2. Related workThe minimax pathology was discovered independently by Nau [10] and Beal [2]. It was later more thoroughlyanalyzed by many authors, among them Bratko and Gams [4], whose notation we adopt. In this section, we present abasic minimax model like the one by Beal and others.A game tree consists of nodes representing game positions and edges representing moves. In the basic model,positions can be lost or won. Negamax notation is used in this section, i.e. nodes are marked as lost or won from theperspective of the side to move. Two situations are possible: a node has at least one lost descendant, in which casethe node itself is won because one can always choose the move leading to the descendant lost for the opponent; or anode has only won descendants, in which case the node itself is lost because all moves lead to positions won for theopponent. This is shown in Fig. 1; losses are marked with “−” and wins with “+”.Virtually all research on the minimax pathology assumed game trees to have a uniform branching factor b. In thebasic model, the value of each leaf is independent of other leaves’ values. Let d be the depth of search and ki theprobability of a loss at ith ply. Plies are numbered upwards: 0 for the lowest ply of search and d for the root.Since a node can only be lost if all of its descendants are won, the relation between the values of k at consecutiveplies is governed by Eq. (1).ki+1 = (1 − ki)b.(1)The goal is to calculate the probability of incorrectly evaluating the root given the probability of an incorrectevaluation at the lowest ply of search. Two types of evaluation error are possible: a loss can be mistaken for a win(false win) or a win for a loss (false loss). Let pi and qi be the probabilities of the respective types of error at ith ply.False wins occur in nodes where all descendants should be won, but at least one of them is a false loss. Therefore pi+1can be calculated according to Eq. (2).pi+1 = 1ki+1(1 − ki)b(cid:1)1 − (1 − qi)b(cid:2)= 1 − (1 − qi)b.(2)False losses occur in nodes where some descendants should be lost, but all of those are false wins instead, whileall won descendants retain their true values. Therefore qi+1 can be calculated according to Eq. (3).Fig. 1. Types of nodes.622M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642qi+1 =11 − ki+1b(cid:3)j =1(cid:4)(cid:5)bji (1 − ki)b−j pjkji (1 − qi)b−j= (kipi + (1 − ki)(1 − qi))b − ((1 − ki)(1 − qi))b1 − ki+1.(3)If only games where both sides have an equal chance of winning at the root of the game tree are considered, kdmust be 0.5 and Eq. (1) can be used to calculate k for other plies. Values for p0 and q0 have to be chosen and Eqs. (2)and (3) can be used to calculate pd and qd . If error at the root is defined as either pd kd + qd (1 − kd ) [4] or 1/2(pd + qd )[16], it turns out that with increasing d, the error converges towards 0.5, rendering minimax useless.First attempts to explain this phenomenon were made by Bratko and Gams [4] and Beal [3]. Both came to theconclusion that the reason minimax is effective in real games is that sibling nodes have similar values. Bratko andGams argued that this property causes the formation of subtrees with reliably evaluated roots, which have a stabilizingeffect. They did not verify whether this is indeed the case in real games. Beal showed that if a sufficient numberof nodes that have all the descendants either lost or won are present in a game tree, the pathology disappears. Hesuccessfully verified his claim on king and pawn versus king chess endgame, but his results are not conclusive becausesuch an endgame is not a typical chess situation. Nau [11,13] used a simple game designed for minimax analysis toshow that a strong dependence between a node and its descendants eliminates the pathology. Pearl [16] suspected thatreal games do not have such a strong dependence. He argued that early terminations (also called blunders), which carryreliable evaluations, are the main reason for the elimination of the pathology. Both early terminations and subtreeswith similar node values proposed by Bratko and Gams [4] result in relatively error-free nodes, so these two propertiesuse basically the same mechanism to eliminate the pathology. Pearl’s calculations show that the number of necessaryearly terminations in a game tree with b = 2 is 5%, but attempts by Luštrek and Gams [9] to verify this experimentallyindicate that 20% is needed with b = 2 and even more with larger branching factors. Schrüfer [19] showed that if theerror, particularly the probability of a false loss, is sufficiently low and certain additional conditions are satisfied, agame tree is not pathological. He did not investigate whether these conditions are present in real games. Althöfer [1]showed that in the trees that are pathological according to Schrüfer, the pathology persists even if some other back-upprocedure is used instead of minimax.Some researchers used models with multiple heuristic values, while they kept the true values restricted to lossesand wins. Nau [11,13] used multiple heuristic values simply because they were a natural choice for the game hestudied. In [12], he was concerned with proving that in certain circumstances, when the evaluation function has alimited number of possible values and the branching factor is large enough, the pathology is inevitable. Later [13]he extended the proof to multiple true values. Pearl [16] also considered multiple heuristic values and concluded thatwith regard to the pathology, they behave the same way as two values. Scheucher and Kaindl [18] were the first toconsider multiple heuristic values crucial for the elimination of the pathology. They used them to design a heuristicevaluation function representative of what game-playing programs use. According to this function, not only were thevalues of the leaves of the game tree not independent of each other, it also implied that at lower plies of the gametree, positions where error is less likely are encountered more often than at higher plies. These two characteristicseliminated the pathology, but it remained unclear why the error should depend on the depth of search exactly the waythey proposed.A model with an arbitrary number of heuristic values and the same number of true values was already studied byBratko and Gams [4]. They used completely independent leaf values and this model behaved similarly to the two-valueversion. Sadikov et al. [17] used multiple values in their analysis of king and rook versus king chess endgame. Theymanaged to explain the pathology in backed-up position evaluations, but it is not known whether their conclusionapplies to cases other than the endgame they studied. Some preliminary results of the research fully described in thispaper were published by Luštrek [8].3. A model with real-number valuesThere are many games where the final outcome is multivalued, typically expressed as an integer (Othello,bridge. . .). In such games, multivalued position values, both true and heuristic, are obviously needed. In games wherethe outcome can only be a loss, a win and perhaps a draw (chess, checkers. . .), multiple values might only seemnecessary to express the uncertainty of the heuristic evaluation function, not as true values. However, even unlimitedM. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642623resources to determine the value of a position cannot remove the uncertainty: in a losing position, the best one can doagainst a fallible and not fully known opponent is to evaluate moves in terms of the probability of a loss against thisparticular opponent. More importantly, in a winning position, multiple values make it possible to maintain a directionof play, gradually moving toward a win, not just maintaining a won position without achieving the final goal. Multiplevalues are necessary to differentiate between multiple winning (or losing, if only such are available) moves. Scheucherand Kaindl [18] already discussed the need for multiple values, although they were only concerned with heuristic val-ues. They demonstrated on chess that a two-valued evaluation function performs poorly compared to a multivaluedone. Given that multivalued position values are necessary for playing well, it is natural to also use multivalued truevalues. These values are true in the sense that they guide a program to play optimally. In our minimax model we usereal values instead of multiple discrete values. This facilitates the mathematical explanation of why minimax works.The issue of granularity, i.e. how many distinct values should be used, is studied separately in Section 6.2. In partic-ular, in Section 6.2 we address the question: Are the desirable properties of our real-valued model retained when realvalues are replaced by a limited number of discrete values?3.1. Description of the modelIn our model, the values of sibling nodes are distributed around the value of their parent. The rationale for such adistribution is that the descendants of a position are only one move away from it and are therefore unlikely to have asignificantly different value. Dependence between the values of nodes and their descendants results in dependence be-tween the values of leaves, which is a departure from the Section 2’s assumption of independent leaf values. Negamaxnotation is no longer used, but the branching factor is still constant.According to our model, a game tree is constructed from the root down. A so-called auxiliary value of 0 is assignedto the root and the auxiliary values of its descendants are distributed around it. The process is repeated recursively oneach descendant until the maximum depth dmax required for a given experiment is reached. It should be noted that theauxiliary values generally do not respect the min/max rule. They serve only to establish the desired relation betweenthe values of the leaves. Such a model is similar to Nau’s incremental [11] or N-games [12] and to the model used byScheucher and Kaindl [18], as well as to some earlier analyses of the alpha-beta algorithm [5,7,15].The auxiliary values of the leaves of a tree with depth d = dmax are considered true values. True values at depthsd < dmax are obtained by performing minimax starting with the true values of the leaves. In experiments with minimaxsearch to depth d, static heuristic values are obtained by corrupting the true values at depth d with error representingthe fallibility of the heuristic evaluation function. The error at the lowest ply of search, i.e. at depth d, is called staticerror. Backed-up heuristic values are obtained by performing minimax on the static heuristic values. The procedure isillustrated in Fig. 2.The error is normally distributed noise. Normal distribution is chosen because this is the usual practice when nouseful information on the error is available. Two types of error can be observed in a node: position error, which is thedifference between the true and the heuristic value of the node, and move error, which is the probability of choosinga wrong move because of position error at the node’s descendants.The model has a number of parameters:• b—branching factor,• type of distribution of nodes’ auxiliary values around the auxiliary value of their parent,• σv—standard deviation of the above distribution,Fig. 2. Construction of game trees.624M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Fig. 3. Parameters of the model.• [−m, m]—the interval within which nodes’ values must lie,• σe—standard deviation of static position error.Some of the parameters are illustrated in Fig. 3; the curve represents probability density function of the distributionof sibling nodes’ auxiliary values (in this case normal) and µ is the auxiliary value of their parent.3.2. Experimental resultsWe investigate the behavior of minimax by randomly generating game trees and observing position and move error.The size of a game tree exceeds bd , so computations with large branching factors or depths are very time-consuming.We conducted experiments with b = 2 and b = 5. The results were similar in all cases, so with the exception of thefirst experiment (Table 1 and Fig. 4), only the results with b = 2 are presented. Only depths of up to 10 were used,because observations at greater depths yield no additional insight.Only relative values of σv, m and σe are important, so σv can be fixed to 1. In Table 1 and Fig. 4, results withnormal distribution of nodes’ values, m = ∞ and σe = 0.2 are presented. The parameters are chosen rather arbitrarily,but they will be examined later on: the value of each parameter will be varied while the others will be kept unchanged.The results are averaged over 10,000 game trees for b = 2 and 2,500 game trees for b = 5. For each tree, there are10 repetitions with randomly generated noisy values. This will be the case in all experiments in the paper with theexception of Section 6.2. Average position error is defined as the sum of absolute differences between the true and theheuristic value of the root divided by the number of trees. Average move error is defined as the number of trees wherea non-optimal move is chosen at the root divided by the number of all trees. Both position and move error at the rootwith respect to the depth of search are given in Table 1 and Fig. 4. Move error is more important because the choiceTable 1Position and move error with the initial parameter settingsd012345678910b = 2b = 5Position errorMove errorPosition errorMove error0.15990.15880.15490.15210.15010.14780.14600.14380.14150.13870.1361–0.03610.03560.03500.03390.03400.03350.03300.03250.03150.03140.15860.15450.14570.14030.13440.12830.12420.11770.11380.11080.1051–0.10080.09590.09380.09080.08520.08090.07890.07640.07490.0746M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642625Fig. 4. Position and move error with the initial parameter settings.Fig. 5. Types of distribution of nodes’ auxiliary values around the auxiliary value of their parent.Table 2Move error with different distributions of nodes’ auxiliary valuesd12345678910Normal0.03610.03560.03500.03390.03400.03350.03300.03250.03150.0314TriangularUniform0.03660.03550.03510.03470.03380.03400.03320.03320.03170.03150.03590.03520.03500.03450.03380.03390.03360.03310.03260.0314of move is the purpose of minimax. Also, the behavior of both types of error in the experiments was similar, so onlymove error will be presented in the rest of the section.As can be seen in Table 1 and Fig. 4, increased depth of search is generally beneficial.Table 2 and Fig. 6 show how the choice of the distribution of nodes’ auxiliary values affects move error at the rootwith respect to the depth of search. Normal, triangular and uniform distributions were tested to investigate the effectof varying bias towards the mean—they are shown in Fig. 5. The other parameters remain unchanged: b = 2, m = ∞and σe = 0.2.As can be seen in Table 2 and Fig. 6, the choice of the distribution has very little effect on the behavior of moveerror. Apparently it matters only that nodes’ values are similar to the value of their parent, not the exact shape of thesimilarity.626M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Fig. 6. Move error with different distributions of nodes’ auxiliary values.Table 3Move error with different values of mm = 1d123456789100.20400.19950.19520.19280.18860.18610.18370.17970.17760.1766m = 20.08200.07910.07790.07610.07480.07420.07250.06910.07040.0677m = 30.05000.04820.04780.04720.04580.04620.04360.04370.04400.0430m = 40.03830.03860.03880.03670.03690.03640.03510.03480.03520.0342m = 50.03710.03710.03650.03650.03570.03520.03430.03360.03340.0327m = ∞0.03610.03560.03500.03390.03400.03350.03300.03250.03150.0314Fig. 7. Move error with different values of m.The initial setting of m = ∞ means that nodes’ values are unconstrained, i.e. that however good or bad a positionis, it can always get better or worse respectively. In real games, position values are not unbounded: there is checkmatein chess, maximum or minimum number of pieces in Othello etc. In our model, the bound is implemented by simplysetting the nodes’ values that fall outside the [−m, m] interval to −m or m. Table 3 and Fig. 7 show how the intervalwithin which the nodes’ values must lie affects move error at the root with respect to the depth of search. The otherM. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642627dTable 4Move error with different values of σeσe = 10.16820.16190.15320.14550.13730.13080.12240.11720.10950.101912345678910σe = 0.50.09140.08730.08470.08190.07880.07560.07350.07230.06850.0661σe = 0.20.03610.03560.03500.03390.03400.03350.03300.03250.03150.0314σe = 0.10.01910.01910.01910.01850.01860.01850.01870.01820.01840.0180Fig. 8. Move error with different values of σe.parameters remain unchanged: b = 2, distribution of nodes’ values (when the values are unconstrained) is normal andσe = 0.2.As can be seen in Table 3 and Fig. 7, the pathology is not present regardless of the value of m. However, m affectsthe magnitude of move error: when m is small, move error is large because the nodes’ values are relatively closer toeach other, which makes position error more likely to change their order. When m increases to the relatively smallvalue of 4, results are very similar to those with m = ∞, and when it increases to 5, they are virtually identical.Table 4 and Fig. 8 show how standard deviation of static position error affects move error at the root with respectto the depth of search. The other parameters remain unchanged: b = 2, distribution of nodes’ values is normal andm = ∞.As can be seen in Table 4 and Fig. 8, increased depth of search is beneficial for all σe. The benefit is less pronouncedwhen σe is small, but this is to be expected since game tree search would not be beneficial at all if a perfect evaluationfunction were available.We can conclude that in our model, minimax is not pathological for a wide range of parameter settings.4. Mathematical explanationIn this section we attempt a mathematical explanation of the experimental observations in Section 3. For simplicityof the analysis we will assume that the difference between the true values of sibling nodes is always 1, which leads628M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Fig. 9. Model used in the mathematical explanation.Fig. 10. Probability density functions of heuristic values of a pair of sibling nodes at ply 0 and their parent.to the model sketched in Fig. 9. Hopefully this simplification does not affect qualitative conclusions of the analysis.Reasons why this should be the case are discussed later in this section. Also, only b = 2 is considered.A node at ply i + 1 has two descendants whose heuristic values are random variables Li (lower value) and Hi(higher value) with means µi L and µi H . Due to normally distributed error, static heuristic values of sibling nodes aredistributed normally with means µ0L and µ0H and standard deviation σe. Their means are also the nodes’ true values.Their probability density functions are given in Eqs. (4).f0L(x) =1√σe2π−(x−µ0L)22σ 2ee,f0H (x) =−(x−µ0H )22σ 2e.e1√σe2π(4)At a max ply i + 1, the value of a node is a random variable MAXi+1 = max(Li, Hi) and its probability densityfunction is calculated according to Eq. (5).x(cid:6)x(cid:6)fi+1MAX(x) = fiH (x)P (L < x) + fiL(x)P (H < x) = fiH (x)fiL(l) dl + fiL(x)fiH (h) dh.(5)−∞−∞Probability density functions of L0, H0 and MAX1 are shown in Fig. 10. Unlike L0 and H0, MAX1 is not distributednormally. Its probability density function was obtained by numerical integration.M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642629As can be seen in Fig. 10, the curve of the parent is narrower than the curves of its descendents, meaning thatthe variance of the parent is smaller. Because the difference between the true values of sibling nodes is constant, thedifference between the means of the heuristic values of sibling nodes is constant as well. Therefore a smaller variancemeans a smaller probability that the lower-valued sibling becomes, due to position error, the higher-valued sibling, i.e.a smaller move error. If the mean of the parent were equal to the mean of the higher descendant, a smaller variancewould also mean a smaller position error. Since the mean of the parent is somewhat larger, this is not necessarily thecase and will be examined later on.Let us first explain what causes the reduction of variance. If there were no interaction between the descendants,fi+1MAX would be equal to fi H . Since this is not true, two cases must be considered. When MAXi+1 is too large(MAXi+1 > µi H ), the most likely reason is that Hi is too large (Hi > µi H ). The less likely reason is that Li is muchtoo large (Li > µi H ). This is marked in Fig. 10 on the top. When MAXi+1 is too small (MAXi+1 < µi H ), this iscaused by Hi being too small (Hi < µi H ), but it can be compensated by Li being larger than Hi (Hi < Li < µi H ).The corrective effect of Li in the latter case is greater than the disturbing effect in the former case, so the impact ofLi it is more likely to be beneficial than harmful.At a min ply, the probability density function is calculated according to Eq. (6).fi+1MIN(x) = fiL(x)P (H > x) + fiH (x)P (L > x) = fiL(x)fiH (h) dh + fiH (x)fiL(l) dl.(6)∞(cid:6)∞(cid:6)xxProbability density functions at ply 0 are given by Eqs. (4), so probability density function at any ply can becalculated by repeatedly using Eqs. (5) and (6). This is shown in Fig. 11; µi H − µi L = 1 for all i.As can be seen in Fig. 11, the higher a ply, the narrower the curve of the probability density function of a node’sheuristic backed-up value. This means that the variance of the heuristic values and consequently move error decreasewith the depth of search. True values in all the cases in Fig. 11 are 0, so it can also be seen that heuristic values exhibita slight positive bias, most of which is accrued at the lowest ply of search. This is consistent with observations bySadikov et al. [17].Let MEi+1 be move error at ply i + 1. An erroneous move at ply i + 1 is chosen when the values of a pair of siblingnodes at ply i are switched, i.e. Li > Hi , so MEi+1 is calculated according to Eq. (7).(cid:7) ∞(cid:6)∞(cid:6)(cid:8)MEi+1 = P (Li > Hi) =fiH (h)fiL(l) dldh.−∞h(7)Fig. 11. Probability density functions of the values of nodes at plies 0 through 10.630M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Fig. 12. Move error at the root as a function of the depth of search (dashed line shows Monte Carlo experimental results).Fig. 13. Position error at the root as a function of the depth of search (dashed line shows Monte Carlo experimental results).How move error changes with increasing depth of search is shown in Fig. 12; MEd is move error at the root whenthe depth of search is d, µi H − µi L = 1 for all i and σe = 1. The solid line results from calculations according toEq. (7). The dashed line shows the results of Monte Carlo experiments using the model of this section (the differencebetween the true values of sibling nodes always 1). They serve to verify the correspondence between the mathematicalanalysis and random experiments.As can be seen in Fig. 12, increased depth of search reduces move error at the root.Let ti be the true value of a node at ply i and fi E probability density function of its heuristic (erroneous) value Ei .Position error PEi is the average absolute difference between ti and Ei and is calculated according to Eq. (8).∞(cid:6)PEi =|ti − x|fiE(x) dx.−∞(8)How position error changes with increasing depth of search is shown in Fig. 13; PEd is position error at the rootwhen the depth of search is d, µi H − µi L = 1 for all i and σe = 1. The solid line results from calculations accordingto Eq. (8). The dashed line again results from Monte Carlo experiments.As can be seen in Fig. 13, increased depth of search reduces position error at the root, which is in accordance withthe experimental results in Section 3. This means that the bias observed in Fig. 10 and Fig. 11 is not large enoughto cause position error to behave pathologically, probably because alternating max and min plies cause alternatingpositive and negative bias. To verify this hypothesis, behavior of move and position error in a tree with only max plieswas investigated. The results are shown in Fig. 14; µi H − µi L = 1 for all i and σe = 1.As can be seen in Fig. 14, move error is still not pathological, while position error behaves pathologically for d > 3.This result is similar to what Sadikov et al. [17] observed in their experiments on king and rook versus king chessendgame, although their trees consisted of both max and min plies. The reason for this discrepancy is not clear.In the analysis presented in this section, a constant difference between the true values of sibling nodes is assumed.However, in game trees of real games as well as game trees used in the experiments in Section 3, the differencebetween the true values of sibling nodes is not constant. The simplification in the analysis is not as problematic as itmay appear, though. This is because we investigate the search of a single game tree to different depths, therefore noM. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642631Fig. 14. Move error (left) and position error (right) at the root as a function of the depth of search in a tree with only max plies.matter how the difference between the true values of sibling nodes varies within the tree, the variation is the same insearches to all depths. Hence the difference affects the error in a similar fashion in all searches and consequently doesnot affect the pathology.5. Verification in a chess programWhether the results of Sections 3 and 4 have bearing on real games can be verified by comparing the modelproposed in Section 3 to real games. However, it is difficult to obtain true values of game-tree nodes of a sufficientlycomplex game. What can and often is done is to use values returned by a game-playing program. The game chosenis chess and the program is Crafty [6]. With ELO rating of 2,616, it is the highest-rated open-source chess programaccording to SSDF [20]. Being open-source is a requirement because the program needed to be modified for theexperiment.The comparison between the model and Crafty should be based on the values of the leaves. Distributions of leafvalues are difficult to compare in practice, though, so the comparison is performed on the relations between the staticvalues of nodes and their descendants in Crafty and auxiliary values of nodes and their descendants in our model. Thevalues of the leaves are, after all, determined by this relation in both cases.One tenth of 4.5 million positions visited in the course of a game played by Crafty were randomly chosen. Allthe positions generated during game-tree search were considered, not only positions actually occurring in the game.The differences between the static value of each node and the static values of all of its descendants were computed.Determining static values did not involve quiescence search. The results were discretized and the number of cases ineach interval counted. In Fig. 15, the results are shown as the number of cases where the difference lies within aninterval; the lower and upper 1% of the cases are omitted, otherwise the interesting interval could not be seen clearlysince the extremes are −22.78 and 20.58.As can be seen in Fig. 15, most nodes’ static values lie close to the static value of their parent. Their distributionresembles normal. This relation corresponds reasonably well to the relation between the auxiliary values of nodes inour model, where the default distribution is also normal. Since Section 3 shows that the exact type of distribution isnot crucial anyway, we feel that the results from Crafty can be considered a confirmation of the relevance of the modelpresented in Section 3 to real games.6. When does pathology occurIf a two-value model can be pathological, as shown in Section 2, should not a real-value model exhibit pathologyunder certain conditions as well? They both aspire to model the error behavior of minimax in game-playing programs.The presence of the pathology in one and the complete absence in another would be an indication that one of themlacks some fundamental property. In this section we look into this apparent contradiction between the two models andsearch for conditions under which pathology would appear in a real-value model.There are three main differences between the two-value model described in Section 2 and the real-value modeldescribed in Section 3:632M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Fig. 15. Distribution of the differences between the static value of a node and its parent.1. In the two-value model, the value of each leaf of the game tree is independent of all other leaves’ values; inthe real-value model, the leaves’ values are not independent due to the nodes’ auxiliary values being distributedaround the auxiliary value of their parent.2. In the two-value model, the error is measured as the probability of mistaking a loss for a win or vice versa andis set to the same value at the lowest ply of search regardless of the depth of search; in the real-value model, theerror is measured as position error whose probability distribution is always the same at the lowest ply of searchregardless of the depth of search, and move error for which this is not guaranteed as we show later in this section.3. And of course one model uses two values and the other real values.6.1. Independent-value modelIf the first difference is removed, a game tree is constructed by simply setting the leaves’ values independently ofeach other instead of using auxiliary intermediate values; the rest of the procedure follows the description in Section 3.We call such a model independent-value model as opposed to dependent-value model, which is the one described inSection 3. Table 5 and Fig. 16 show position and move error at the root with respect to the depth of search in theindependent-value model; b = 2, distribution of leaves’ values (this time around 0 instead of their parents’ auxiliaryvalues) is normal, m = ∞ and σe = 0.2.Table 5Position and move error at the root in the independent-value modeld012345678910Position errorMove error0.15980.14720.12330.11530.10410.09970.09440.09180.08950.08760.0862–0.29520.29160.28630.28240.27440.27200.26860.26350.25780.2577M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642633Fig. 16. Position and move error at the root in the independent-value model.As can be seen in Table 5 and Fig. 16, the independent-value model with the chosen set of parameters is notpathological. Experiments with several different sets of parameters also yielded no pathology. Considering that modelswith independent values such as the one described in Section 2 are known to be pathological, this result is on one handsomewhat surprising. On the other hand, though, the explanation of Section 4 also applies to the independent-valuemodel. Apparently two-value models are sufficiently different from real-value models that the pathology in one doesnot imply the pathology in the other, even if both are independent-valued.6.2. GranularityIn our models, we used real numbers as both true and heuristic values, so in principle there could be an infinitenumber of them. Since digital computers do not use real values in the true mathematical sense, in our simulationsthese real numbers were approximated by high-density floating-point numbers. But we know that game-playing pro-grams usually use as heuristic values a much smaller set of discrete values, most often represented by a range ofintegers. Therefore real values are not completely realistic for modeling typical game playing programs. We will nowinvestigate whether this difference affects our main findings.To see whether the issue of granularity affects the pathology, the number of possible heuristic values, or grains,in our models was restricted. The granularity of true values was set to be equal to the granularity of heuristic values,otherwise errors would occur even with the best possible evaluation function, simply because it would be too coarse-grained to differentiate between similar true values. The effect of granularity turned out to be strongly tied to thebranching factor. Table 6 and Fig. 17 show the number of grains needed to avoid the pathology with respect tothe branching factor in the dependent- and the independent-value model; the distribution of nodes’ values in thedependent-value model is normal and in the independent-value model uniform, dmax = 6, m = ∞ and σe = 0.2.As can be seen in Table 6 and Fig. 17, the dependent-value model is quite robust in the sense that even with a lowgranularity, it is not pathological. Evaluation functions of modern chess programs have ranges of several thousandvalues, so these programs are not likely to be pathological due to low granularity. The independent-value model ismuch more sensitive to the granularity: the granularity required to avoid the pathology g(b) seems to be exponentialin the branching factor. Eq. (9) gives the least-square-error approximation of the experimental data.g(b) = 7.5084 × 1.3208b − 0.5169.(9)634M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Table 6Required granularity to avoid the pathology in the dependent- andthe independent-value modelb2345678910Dependent-valuemodelIndependent-valuemodel1015191919191919191317222939527190121Fig. 17. Required granularity to avoid the pathology in the dependent- and the independent-value model.The average branching factor of chess is estimated to be between 30 and 40. g(30) = 31,670 and g(40) = 511,706,so a game with a chess-like branching factor, typical chess program granularity and independent leaf values wouldprobably be pathological.This result is similar to what Nau [12] predicted: if the heuristic evaluation function returns its minimum andmaximum values with a non-zero probability and the branching factor is large enough, the pathology will ensue. Withthe introduction of the granularity, every value in our model is indeed returned with a non-zero probability. Scheucherand Kaindl [18] also investigated the issue of granularity. Their conclusion was that it has no effect as long as itis not too small for the evaluation function to express all the knowledge of the game. In our experiments this wasnever the case, because the granularity of the heuristic values was equal to the granularity of the true values, but thepathology still appeared. Scheucher and Kaindl’s did not encounter it because their work was based on the assumptionthat a simple chess evaluation function requires 84 distinct values, and since their model was dependent-valued, ourexperiments suggest that much fewer distinct values are required for the pathology to appear.6.3. Constant static move errorIn the experiments up to this point, whenever searches to different depths were performed, static position error wasalways constant. This is a reasonable assumption, since position error directly models the fallibility of the heuristicM. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642635Table 7Static move error in the dependent- and the independent-valuemodeld12345678910Dependent-valuemodelIndependent-valuemodel0.03690.03730.03750.03760.03840.03960.04080.04320.04860.06280.29520.26020.22490.19460.16360.13750.11300.09360.07600.0628Fig. 18. Static move error in the dependent- and the independent-value model.evaluation function and it is generally accepted that the heuristic evaluation function performs equally well at all plieswithin the horizon of a single search. However, it is also not an unreasonable assumption that move error at the lowestply of search, or static move error, should be constant.Table 7 and Fig. 18 show static move error with respect to the depth of search in the dependent- and the independent-value model; b = 2, distribution of nodes’ values is normal, m = ∞ and σe = 0.2.Perhaps surprisingly, as can be seen in Table 7 and Fig. 18, static move error is not constant. In the dependent-value model, it increases with the depth of search, while in the independent-value model, it decreases with the depthof search. Differences in static move error when searching to different depths are particularly large in the independent-value model.Why static move error changes with the depth of search can be explained by observing the average differencebetween the true values of sibling nodes at successive plies. Table 8 and Fig. 19 show the results for the dependent-and the independent-value model; b = 2, distribution of nodes’ values is normal, m = ∞, σe = 0.2 and dmax = 10.As can be seen in Table 8 and Fig. 19, the behavior of the average difference between the true values of siblingnodes is the opposite of the behavior of static move error (shown in Table 7 and Fig. 18). This is not surprising: it hasalready been observed that small differences between backed-up values of sibling nodes cause large move errors andvice versa.636M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Table 8The average difference between the true values of sibling nodes atsuccessive plies in a game tree with dmax = 10 in the dependent-and the independent-value modelPly9876543210Dependent-valuemodelIndependent-valuemodel1.93131.91911.90401.89251.87631.82311.75801.64641.46381.12860.17110.21320.26220.32320.40160.49130.61080.74670.92971.1286Fig. 19. The average difference between the true values of sibling nodes at successive plies in a game tree with dmax = 10 in the dependent- andthe independent-value model.Why true backed-up values of sibling nodes in the dependent-value model are statistically further apart at higherplies can be explained by analyzing the relation between the auxiliary values of a pair of sibling nodes and thecorresponding true backed-up values. Consider a pair of non-leaf sibling nodes in a dependent-value game tree. LetaL and aH be the auxiliary values of these sibling nodes, so that aL (cid:1) aH . Let L∗ and H ∗ be the corresponding truebacked-up values; they are random variables that acquire values from the populations of the possible subtrees rootedin the two sibling nodes. The means of L∗ and H ∗ are µL∗ and µH ∗ . Both subtrees rooted in the two sibling nodes areconstructed by the same random mechanism starting with the root values aL and aH , therefore µH ∗ − µL∗ = aH − aL.Consider the difference D = H ∗ − L∗. The expected value of D, µD, is equal to µH ∗ − µL∗ = aH − aL. This followsfrom the fact that the expected value of the difference between two random variables is equal to the difference betweenthe expected values of the two variables. This property is demonstrated in Eq. (10), where X and Y are two independentrandom variables whose probability density functions are fX and fY .∞(cid:6)∞(cid:6)µX−Y =(x − y)fX(x)fY (y) dx dy−∞−∞M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642637∞(cid:6)∞(cid:6)∞(cid:6)∞(cid:6)=xfX(x)fY (y) dx dy −yfX(x)fY (y) dx dy−∞−∞∞(cid:6)(cid:7) ∞(cid:6)(cid:8)−∞−∞∞(cid:6)(cid:7) ∞(cid:6)(cid:8)=xfX(x)fY (y) dydx −yfY (y)fX(x) dxdy−∞∞(cid:6)−∞∞(cid:6)−∞−∞=xfX(x)1 dx −yfY (y)1 dy = µX − µY .−∞−∞(10)Regarding the probability of move error, the parameter that matters is the absolute difference between the true valuesof the two sibling nodes: |D| = |H ∗ − L∗|. The expected value of |D| is greater than µD which is easily seen fromEq. (11).∞(cid:6)∞(cid:6)µD =xfD(x) dx (cid:1)|x|fD(x) dx = µ|D|.−∞−∞(11)So for auxiliary values of any pair of sibling nodes, the corresponding true backed-up values are further apart onaverage. This is also illustrated in Fig. 20.Auxiliary values at all plies are distributed equally, so the average difference between the auxiliary values of a pairof sibling nodes is the same for all plies. Auxiliary values at ply 0 are also true values, therefore backed-up true valuesat ply 1 and higher are further apart on average than true values at ply 0. What makes µ|D| larger at each successiveply is that the enlarging effect is cumulative.Why true backed-up values of sibling nodes in the independent-value model are closer to each other at higher pliescan be explained as follows. The values of the leaves are distributed within an interval. Applying maximum to groupsof them results in values concentrated in a smaller interval. Applying minimum to groups of values from the newinterval results in values concentrated in an even smaller interval, etc. This is illustrated in Fig. 21.The effect of the average difference between the values of sibling nodes on static move error can be compensated byadjusting static position error to achieve constant static move error: in the dependent-value model, it has to be increasedin shallower searches, while in the independent-value model, it has to be decreased in shallower searches. Table 9 andFig. 22 show move error at the root with respect to the depth of search in the dependent- and the independent-valuemodel with constant static move error; b = 2, distribution of nodes’ values is normal and m = ∞.As can be seen in Table 9 and Fig. 22, constant static move error strengthens the benefits of minimax in thedependent-value model, but produces the pathology in the independent-value model. This is consistent with previousFig. 20. Nodes’ true backed-up values higher up in a tree are further apart in the dependent-value model.638M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Fig. 21. Explanation for nodes’ backed-up values higher up in a tree being closer to each other in the independent-value model.Table 9Move error at the root in the dependent- and the independent-valuemodel with constant static move errord12345678910Dependent-valuemodelIndependent-valuemodel0.06280.06140.06050.05990.05570.05230.04940.04760.04180.03140.06280.07340.09140.10690.13030.15010.17560.20640.23170.2577Fig. 22. Move error at the root in the dependent- and the independent-value model with constant static move error.research by Beal [3], Bratko and Gams [4], and Nau [11,12], concerned with two-value models. But it should be notedthat in two-value models, error is defined somewhat differently from move error in our real-value model.M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642639Table 10Two-value error at the root and at the lowest ply of search in the dependent- and theindependent-value modeld012345678910Dependent-value modelIndependent-value modelRoot0.04860.04690.04700.04660.04650.04530.04450.04350.04330.04220.0414Static0.04860.03210.03280.02640.02680.02270.02320.02100.02110.01990.0202Root0.20720.19870.17700.16630.15510.14650.14330.14110.13640.12930.1268Static0.20720.18930.18730.16550.15630.13160.12030.09550.08650.07000.0592Fig. 23. Two-value error at the root and static two-value error in the dependent- and the independent-value model.6.4. Conversion to two valuesA real-value model can be reduced to a two-value model by converting the real-number values to losses and wins.To do this, a threshold must be established: the values below it are considered losses and the values above it wins. Ifthe probability of a loss at the root is to be 0.5, the threshold should be the median backed-up real value at the root.Table 10 and Fig. 23 show two-value error (the probability of an incorrect evaluation of a position) at the rootand two-value error at the lowest ply of search, or static two-value error, with respect to the depth of search in thedependent- and the independent-value model; b = 2, distribution of nodes’ values is normal, m = ∞ and σe = 0.2.As can be seen in Table 10 and Fig. 23, neither the dependent- nor the independent-value model is pathological interms of two-value error. Note that even though the error at the root is mostly larger than static error in both models,this does not mean that the models are pathological: it merely means that positions at higher plies are more difficultto evaluate as losses or wins than those at lower plies. If static error were constant, the increase of error throughminimaxing would of course lead to the pathology. But as can be observed in Table 10 and Fig. 23, static two-valueerror is not constant, which also means that this experiment cannot be compared to the results of previous researchoutlined in Section 2. In order to make such a comparison possible, static position error must be adjusted so that statictwo-value error is constant. Table 11 and Fig. 24 show two-value error at the root with respect to the depth of search in640M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642Table 11Two-value error at the root in the dependent- and the independent-value model with constant static two-value errord012345678910Dependent-valuemodelIndependent-valuemodel0.02020.03060.02770.03540.03490.03910.03900.04360.04020.04380.04140.05920.06240.06090.06880.06870.08180.08510.09910.10690.12120.1268Fig. 24. Two-value error at the root of the dependent- and the independent-value model with constant static two-value error.the dependent- and the independent-value model after this adjustment; b = 2, distribution of nodes’ values is normaland m = ∞.As can be seen in Table 11 and Fig. 24, constant static two-value error produces pathology both in the dependent-and the independent-value model; pathology is stronger in the independent-value model. This result could be inter-preted as an indication that the pathology may exist even in the dependent-value model. However, the constant statictwo-value error assumption in this experiment is not appropriate. A constant static two-value error is indeed assumedin the two-value model in Section 2, but in a model where true values are real numbers, static two-value error shoulddecrease with depth. The reason is that at the lowest ply of shallower searches, nodes’ values are on average closerto the threshold separating losses from wins, hence two-value error is more likely to occur in such nodes. This isillustrated in Fig. 25; the darker area represents a higher probability of two-value error.Scheucher and Kaindl [18] investigated the relation between the nodes’ heuristic values and two-value error ingreater detail. One of the key points of their paper was precisely that static two-value error (called simply error, sincethey did not consider multiple types of error) should not be constant. If static two-value error in shallower searchesshould indeed be larger than in deeper searches, this explains why a basic two-value model such as the one describedin Section 2 is pathological: it does not model the heuristic error of a real-value game appropriately. This way ofmodeling error might be suitable for a two-value game, but two (or three) values are not enough for playing realgames like chess successfully, as was also demonstrated by Scheucher and Kaindl.M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642641Fig. 25. Why static two-value error should not be constant.7. ConclusionIn this paper we investigated the question whether minimax behaves pathologically in game trees where the truevalues of positions are real numbers. Our general conclusion is “no”, which is in agreement with the practice ofgame-playing programs, but different from earlier theoretical analyses by several other authors who studied gametrees where the true values of positions could only be losses and wins.We constructed a minimax model where game tree nodes have real-number values and nodes’ values are distributedaround the value of their parent. The pathology was not present with any configuration of parameters tested.A mathematical explanation of why minimax performs better with increased depth of search on game trees suchas the ones built according to our model was provided. The reason is that minimum/maximum of pairs of randomvariables has a smaller variance than the variables themselves. If these random variables represent error, it is thusreduced through minimaxing. The means of the heuristic values have a slight bias with respect to the true values, butwith alternating min and max plies, the bias alternates between negative and positive, preventing the heuristic valuesfrom moving too far away from the true values. Extending the theoretical analysis to trees with branching larger thantwo and general distribution of nodes’ values around the value of their parent is left for further work.The assumption that sibling nodes’ values are normally distributed around the value of their parent was verifiedin the Crafty chess program. Static values of sibling nodes in the program turned out to be close to each other andtheir distribution around their parent’s value similar to normal. This can be considered a confirmation that our modelcorresponds well to the situation in chess playing programs.Considering that pathology is common in two-value models, we tried to find under what conditions, if any, itappears in real-value models. No pathology in terms of position error was found. By default, static position error wasconstant in our experiments. If, instead, static move error was set to be constant, our model was still not pathological,but a model with independent leaves’ values did behave pathologically. This shows that similarity of nodes close toeach other in a game tree can eliminate pathology.In order to better model game-playing programs, a limited number of discrete values were assigned to positionsinstead of real values. Here the existence of the pathology depends on the granularity of heuristic values and onthe branching factor. The chances of pathology increase with branching factor and decrease with granularity. In thedependent-value model, 19 values were sufficient to avoid the pathology in all the experiments. In these experi-ments, we could only reach branching factors of up to 10, but the required granularity did not rise after b = 4. In thedependent-value model, the required granularity increases fast with the branching factor, so a program for playing agame described by this model would require a very fine-grained evaluation function.If real-value game trees are converted to two-value game trees, two-value error can be measured. Depending on theassumption regarding static heuristic error, dependent and independent-value models are either both pathological orboth non-pathological. One assumption is that static position error (noise applied to the position values) is independent642M. Luštrek et al. / Artificial Intelligence 170 (2006) 620–642of the depth of search. The other assumption is that static two-value error (probability of mistaking a lost position forwon or vice versa) is independent of the depth of search. We will here refer to these two assumptions as assumption 1and assumption 2 respectively. Both assumptions seem reasonable, but they generally cannot be both true in a real-value game tree. Assumption 1 does not produce pathology and is appropriate for real-value game trees. The basictwo-value model of Beal [2] and others makes assumption 2, which causes pathology. This assumption is, however,not appropriate for real-value game trees. It might be suitable for two-value game trees, but playing most real-lifegames by two-value evaluations does not seem to be a good idea (as discussed also by Scheucher and Kaindl [18]).AcknowledgementThis work was supported by the Slovenian Ministry of Science and Education. We would also like to thank Alek-sander Sadikov, Mihael Perman and Dana S. Nau for helpful suggestions and discussion.References[1] I. Althöfer, Generalized minimax algorithms are no better error correctors than minimax itself, in: D.F. Beal (Ed.), Advances in ComputerChess, vol. 5, North-Holland, Amsterdam, 1989, pp. 265–282.[2] D.F. Beal, An analysis of minimax, in: M.R.B. Clarke (Ed.), Advances in Computer Chess, vol. 2, Edinburgh University Press, Edinburgh,1980, pp. 103–109.[3] D.F. Beal, Benefits of minimax search, in: M.R.B. Clarke (Ed.), Advances in Computer Chess, vol. 3, Pergamon Press, 1982, pp. 17–24.[4] I. Bratko, M. Gams, Error analysis of the minimax principle, in: M.R.B. Clarke (Ed.), Advances in Computer Chess, vol. 3, Pergamon Press,1982, pp. 1–15.[5] S.H. Fuller, J.G. Gaschnig, J.J. Gillogly, An analysis of the alpha-beta pruning algorithm, Technical Report, Carnegie Mellon University, 1973.[6] R. Hyatt, Home page, http://www.cis.uab.edu/info/faculty/hyatt/hyatt.html, 2004-04-21.[7] D.E. Knuth, R.W. Moore, An analysis of alpha-beta pruning, Artificial Intelligence 6 (4) (1975) 293–326.[8] M. Luštrek, Minimax pathology and real-number minimax model, in: B. Zajc, A. Trost (Eds.), Proceedings of the Thirteenth InternationalElectrotechnical and Computer Science Conference, vol. B, Portorož, Slovenia, Slovenian Section IEEE, Ljubljana, Slovenia, 2004, pp. 137–140.[9] M. Luštrek, M. Gams, Minimaks in napaka pri ocenjavanju položajev, in: M. Bohanec, B. Filipiˇc, M. Gams, D. Trˇcek, B. Likar (Eds.),Proceedings A of the 6th International Multi-Conference Information Society IS 2003, 2003, pp. 89–92.[10] D.S. Nau, Quality of decision versus depth of search on game trees, Ph.D. thesis, Duke University, 1979.[11] D.S. Nau, An investigation of the causes of pathology in games, Artificial Intelligence 19 (3) (1982) 257–278.[12] D.S. Nau, Decision quality as a function of search depth on game trees, J. Assoc. Comput. Mach. 30 (4) (1983) 607–708.[13] D.S. Nau, Pathology on game trees revisited, and an alternative to minimaxing, Artificial Intelligence 21 (1, 2) (1983) 221–224.[14] D.S. Nau, How to do worse by working harder: The nature of pathology on game trees, in: Proceedings of 1983 IEEE International Conferenceon Systems, Man, and Cybernetics, 1983.[15] M.M. Newborn, The efficiency of the alpha-beta search on trees with branch-dependent terminal node scores, Artificial Intelligence 8 (2)(1977) 137–153.[16] J. Pearl, On the nature of pathology in game searching, Artificial Intelligence 20 (4) (1983) 427–453.[17] A. Sadikov, I. Bratko, I. Kononenko, Search vs knowledge: Empirical study of minimax on KRK endgame, in: H.J. van den Herik, H. Iida,E. Heinz (Eds.), Advances in Computer Games: Many Games, Many Challenges, Kluwer Academic, Dordrecht, 2003, pp. 33–44.[18] A. Scheucher, H. Kaindl, Benefits of using multivalued functions for minimaxing, Artificial Intelligence 99 (2) (1998) 187–208.[19] G. Schrüfer, Presence and absence of pathology on game trees, in: D.F. Beal (Ed.), Advances in Computer Chess, vol. 4, Pergamon Press,1986, pp. 101–112.[20] B. Sjörgen, Swedish Chess Computer Association website, http://w1.859.telia.com/~u85924109/ssdf/, 2004-04-21.