7002nuJ01]na-atad.scisyhp[2v2003060/scisyhp:viXraFunctional dissipation microarrays for classificationD. Napoletani ∗, D. C. Struppa †, T. Sauer ‡andV. Morozov§, N. Vsevolodov§, C. Bailey §AbstractIn this article, we describe a new method of extracting informationfrom signals, called functional dissipation, that proves to be very effec-tive for enhancing classification of high resolution, texture-rich data.Our algorithm bypasses to some extent the need to have very special-ized feature extraction techniques, and can potentially be used as anintermediate, feature enhancement step in any classification scheme.Functional dissipation is based on signal transforms, but uses thetransforms recursively to uncover new features. We generate a varietyof masking functions and ‘extract’ features with several generalizedmatching pursuit iterations.In each iteration, the recursive processmodifies several coefficients of the transformed signal with the largestabsolute values according to the specific masking function; in this waythe greedy pursuit is turned into a slow, controlled, dissipation of thestructure of the signal that, for some masking functions, enhances sep-aration among classes.Our case study in this paper is the classification of crystallizationpatterns of amino acids solutions affected by the addition of smallquantities of proteins.Keywords: Features enhancement, matching pursuit, classification, non-linear iterative maps, wavelet transforms.∗Center for Applied Proteomics and Molecular Medicine, George Mason University,Manassas, VA 20110, email: dnapolet@gmu.edu†Department of Mathematics and Computer Science, Chapman University, Orange,CA 92866‡Department of Mathematical Sciences, George Mason University, Fairfax, VA 22030§National Center for Biodefense and Infectious Diseases, George Mason University20110, Manassas, VA1   1IntroductionIn this paper we introduce the functional dissipation microarray, a featureenhancement algorithm that is directly inspired by experimental microarraytechniques [1]. Our method shows how ideas from biological methodologiescan be successfully turned into functional data analysis tools.The idea behind the use of microarrays is that if a large and diverseenough data set can be collected on a phenomenon, it is often possible toanswer many questions, even when no specific interpretation for the datais known. The algorithm we describe here seems particularly suitable forhigh resolution, texture-rich data, and bypasses to some extent the needto preprocess with specialized feature extraction algorithms. Moreover, itcan potentially be used as an intermediate feature enhancement step in anyclassification scheme.Our algorithm is based on an unconventional use of matching pursuit ([2],Chapter 9; [3]). More precisely, we generate random masking functions and‘select’ features with several generalized matching pursuit iterations. In eachiteration, the recursive process modifies several of the largest coefficients ofthe transformed signal according to the masking function. In this way thematching pursuit becomes a slow, controlled dissipation of the structure ofthe signal; we call this process functional dissipation. The idea is that someunknown statistical feature of the original signal many be detected in thedissipation process at least for some of the random maskings.This process is striking in that, individually, each feature extractionwith masking becomes unintelligible because of the added randomness anddissipation, and only a string of such feature extractions can be ‘blindly’used to some effect. There is some similarity in spirit between our approachand the beautiful results on random projections reconstructions describedin [4] and [5], with the important difference that we use several distinctrandomization and dissipation processes to our benefit so that there is astrong non-linear dynamics emphasis in our work. Moreover, we bypassaltogether reconstruction issues, focusing directly on classification in theoriginal representation space. Our method can be seen also as a new instanceof ensemble classifiers (like boosting [6] and bagging [7]), in that severalfunctional dissipations are generally pulled together to achieve improvementof classification results.Other applications of matching pursuit to classification problems includekernel matching pursuit methods [8], and also boosting methods, that canbe interpreted essentially as greedy matching pursuit methods where thechoice of ‘best’ coefficient at each iteration is made with respect to more2sophisticated loss functions than in the standard case (see [9], [10], [11]chapter 10). We stress again that, while our approach uses the structure ofmatching pursuits, only their rich dynamical properties (highlighted for ex-ample in [3]) are used for the generation of features, since in our method thewhole iterative process of modifying coefficients becomes simply an instanceof non-linear iterative maps, disjoined from approximation purposes.The case study of this paper is the classification of crystallization pat-terns of amino acids solutions affected by addition of small quantities ofproteins; such crystallization patterns show varied and interesting textures.The goal is to recognize whether an unknown solution contains one of sev-eral proteins in a database. Crystallization patterns may be significantlyaffected by laboratory conditions, such as temperature and humidity, so thedegree of similarity of patterns belonging to a same protein is subject tosome variations, adding difficulty to the classification problem. The rich-ness of the textures and their variability gives this classification problem anappeal that goes beyond the specific application from which it arises.Our basic approach is to derive long feature vectors for each amino acidreporter by our generalized recursive greedy matching pursuit. Since thecrystallization patterns are generated by stochastic processes, there is a greatlocal variability in each droplet and any significant feature must encode astatistical information about the image to be part of a robust classifier, see[10]. We can see this case study as an instance of texture classification,and it is well established that wavelets (see [2] chapter 5, [13]) and mo-ments of wavelet coefficients (see [14], or [15] for the specific problem of irisrecognition) can be very useful for these types of problems. Therefore weimplemented our method in the context of a wavelet image transform forthis case study, even though we then summarized the information obtainedwith the functional dissipation with moments of the dissipated images inthe original representation space. Other choices of statistical quantities arepossible in principle according to the specific application, since, as we showin section 2, the method can be formulated in a quite general setting.In section 2 we introduce the features enhancement method, functionaldissipation, to explore the feature space. In sections 3 and 4 we apply themethod to the crystallization data to show how the application of a suffi-ciently large number of different functional dissipations can greatly improvethe classification error rates.32 Functional dissipation for classificationIn this section we introduce a classification algorithm that is designed forcases where feature identification is complicated or difficult. We first out-line the major steps of the algorithm, and then discuss specific implemen-tations. In the following sections we apply one possible implementation ofthis method to droplet classification, and describe the results.We begin with input data divided into a training set and a test set. Wethen follow four steps as follows:A Choose a classifier and a figure of merit that quantifies the classificationquality.B Choose a basic method of generating features from the input data, andthen enhance the features by a recursive process of structure dissipa-tion, described below.C Compute the figure of merit from A for all features derived in B. For afixed integer p, search the feature space defined in B for the p featureswhich maximize the figure of merit in A on the training set.D Apply the classifier from A, using the optimal p features from C, toclassify the test set.In step A, for example, multivariate linear discrimination can be usedas a classifier. This method comes with a built-in figure of merit, the ra-tio of the between-group variance to the within-group variance. More so-phisticated classifiers often have closely associated evaluation parameters.Cross-validation or leave-one-out error rates can be used.Step B is the heart of the functional dissipation algorithm. The featuresused will depend on the problem. For two-dimensional images, orthogonalor over-complete image transforms can be used. The method of functionaldissipation is a way to leverage the extraction of general purpose featuresto generate features with increased classification power. This method usesthe transforms recursively to gradually modify the feature set.Consider a single input datum X and several invertible transforms Tk,k = 1, . . . , K, that can be applied to X (in the case study to be shownlater, X represents a 256 × 256 gray scale image and Tk represent DiscreteWavelet Transforms). At each iteration we select several coefficients from theinput datum X and we use the mask to modify the coefficients themselves.Fix positive integers K, M and set X0 = X Let A(x) be a discrete valuedfunction defined on ZZ, which we call a mask or a masking function.4Apply the following functional dissipation steps (B1)-(B3) K times. Fork = 1, . . . , K:(B1): Compute the transform TkXk−1(B2): Choose a subset S of TkXk−1 and collect the M coefficients C(m),m = 1, ..., M in S with largest absolute value in a suitable subset.(B3): Apply the mask: Set C ′(m) = A(m)C(m), and modify the corre-k (TkXk−1)′sponding coefficients of TkXk−1 in the same fashion. Set Xk = T −1to be the inverse of the modified coefficients (TkXk−1)′.At the conclusion of the K steps, features are generated by computingstatistics that describe the probability distribution of Xk, k = 0, . . . , K. Forexample, one could use m(h), h = 3, 4, the third and fourth moments ofthe set (or even more moments for large images). These statistics are usedas features, delivered by the means of functional dissipation.If we carryout these steps for N different masks An, we obtain a 2N K + 2-dimensionalfeature vector for each data input (where we counted the moments of theinput images only once).One way to view our approach is as a matching pursuit strategy (see [2]chapter 9), but used in a new and unusual way. In general, matching pursuitis used to find good suboptimal approximations to a signal. The way thisis done is by expanding a function f in some dictionary D = {g1, ..., gP }and by choosing the element in the dictionary gk for which | < f, gk > |is maximum. Given an initial approximation ˜f = 0 of f , and an initialresidue Rf = f , we set ˜f = ˜f + < f, gk > gk and Rf = Rf − < f, gk > gk.The process is repeated on the residue several times to extract successivelydifferent relevant structures from the signal.Instead, in each iteration of our algorithm we modify several of thelargest coefficients in different regions of the transformed signal accordingto the random masking functions, so that only the non-linear interaction ofthe signal and the masking function is of interest and not the approximationof the underlying signal. The residue is perturbed more and more until, inthe limit, no structure of the original signal is visible in either Rf and ˜f .This is therefore a slow, controlled dissipation of the structure of the signal.Note that the structure of the signal is dissipated, not its energy, in otherwords the images Xk are allowed in principle to increase in norm as k → ∞.We can think of the input image X as initial condition of the iterativemap defined by mask and dissipation, and the application of several dissi-pation processes allows to identify those maps for which different classes ofimages flow in different regions of the feature space as defined by the outputof the map. It seems likely that, for some non-linear maps, similar initialconditions will evolve with similar dynamics, while small differences among5classes will be enhanced by the dynamics of the map and therefore theywill be detectable as the number of iterations increase. The key questionis whether a small set of masks can generate enough variability among thedissipative processes to find such interesting dynamics. Our results showthat, for our case study, this is the case. According to this qualitative dy-namical system interpretation, our method can be seen along the lines ofdynamical system algorithms for data analysis such as those developed in[16] to approach numerical linear algebra problems.In choosing the masking functions for (B1)-(B3), the guiding idea is tohave large variability in the masks themselves so that there is a wide rangeof possible dynamics in the dissipations processes, while at the same timepreserving some of the structure of the signal from one iteration to the next(in other words we want the signals to be dissipated, but slowly). To thisextent, the masking functions are designed to assume small values so thatthe image is only slightly affected at each iteration by the change of of asubset of its coefficients as in (B1)-(B3). To respect these limitations, wedecided to take each mask as a realization of length M of Gaussian whitenoise with variance 1, we then convolve it with a low-pass filter to increasesmoothness and finally we rescale it to have a fixed maximum absolute value.More precisely, let W [m], m = 1, ..., M be a realization of Gaussian whitenoise of length M , and let g be a smoothing filter defined on all integers ZZsuch that g[m] = cos2( πm2E )11[−E,E][m] where 11[−E,E] denotes the functionthat assumes value 1 for m < E and value 0 otherwise (we follow here [2]page 440). Let now ˜W = W ∗ g be the convolution of W and g, whereW is suitably extended periodically at the boundary. Then each mask canbe written as A = α ˜W / max(| ˜W |), where α is a small real number. Thelarger E, the smoother the mask A, but the fact that each underlying W isa random process assures the necessary variability. We repeat this processN times choosing several values of E and α to generate curves of the typeshown in Figure 1.Remark 1: For our case study in section 3, we choose E to be distributedon [M/3, 0], and we choose a logarithmic distribution on this set so thatsmoother masks are favored. Specifically we take M = 1000 and E uniformlyin the set E = {299, 139, 64}. We choose α < 1 (specifically, α = 0.8) tocause slow dissipation.The randomness of the specific choice of masks is used to allow a widespanning of possible mask shapes, and we expect more general classes ofmaskings to be suitable for this method. On the other hand the question ofwhich is the smallest class of masks that allows effective feature extraction610.50−0.5010.50−0.5−10500m500m10.50−0.51000−10500m100010.50−0.51000−10500m1000Figure 1: Examples of masking functions (defined for coefficient indexesm = 1, ..., 1000) used in the iterative process (B1)-(B3).is still open.1.3 Case Study: Droplet ClassificationIn this section we describe the case study that motivated this research, theclassification of proteins by their effect on crystallization patterns of aminoacids used as reporter substances. We restrict ourselves to a database offour classes :3 proteins and control. The proteins are albumin from chickenegg white, hemoglobin from bovine blood and lysozyme, which we denoterespectively as P 1, P 2 and P 3 respectively. These proteins were added tosolutions of one amino acid, namely leucine, denoted by L. The controlsolution without protein will be denoted as W ater. Both the number ofproteins to classify and the number of reporters can increase in practice, butwe restrict our analysis to this smaller set for our purposes as already withthese few classes of proteins we can see the basic difficulty of distinguishingeven by eye some of the patterns.A crucial advance in the experimental study of droplets has been theability to generate quickly a large number of droplets to which differentamino acids have been added. Remarkably, the addition of proteins to the1A patent application has been filed on the method described in this paper with U.S.Provisional Patent Application Number 60/687,868, file date 6/7/2005.7amino acid reporters can have very different effects on the crystallizationpatterns.In some cases there is no visible difference between droplets ofamino acid plus W ater (control) and droplets of amino acid plus protein; forsome other proteins the resulting crystallization patterns are very differentfrom control [17]. Clearly the different behavior of solutions for differentamino acids can greatly facilitate classification.In Figure 2, each row shows two representative patterns of the aminoacid L solutions with the three proteins P 1, P 2, P 3 and W ater added.Note in particular that the appearance of droplets for P 1 and W ater showseveral similarities (for example large, glassy crystals at the boundary) andthat there is a significant degree of variability within each class.4 Results and DiscussionIn this section we discuss a particular implementation of the pattern classifi-cation algorithm of Section 2, and the results of applying it to the droplets.We used for our analysis 20 gray-scale images of crystallization patternsfor each combination of amino acid L, with proteins P 1,P 2, P 3 and withW ater. For each droplet Xij, i.e. the i-th image instance of protein j withaddition of amino acid L, we reduce the size of the image to a matrix of 256by 256 pixels and we normalize the image so that its mean as an array iszero and its variance is one.For purposes of comparison, we will apply the steps (B1)-(B3) in sev-eral ways. First we set the number of iterations of the dissipation to zero(K = 0), and we apply the process only once, which corresponds to a directevaluation of the 2 moments of the images. Then we apply the algorithmchoosing the 2 best features with several masks and no dissipation (K = 1)and with several masks and function dissipation (K > 1). The maskingfunctions we use are the Gaussian processes described at the end of section2 with parameters as in remark 1. Subsequently we use, for the case K = 1and K > 1, a larger number of features (namely 6, one for each distinct pairof classes), and we show that error rates are very low in this setting whenK > 1.In all cases we apply to the selected features a general purpose classifi-cation algorithm such as a 3-nearest neighborhood (3-NN) classifier to theoutput of (B1)-(B3) and we test each case by dividing the 20 instances offeature vectors for each class randomly into a training set of 15 instancesand a test set of 5 instances. We train the classifier on the training set andthen test it on the remaining 5 instances of each class. Note that we are8Figure 2: Each row of this figure shows, from top to bottom, two repre-sentative patterns of crystallization of amino acid A to which was addedrespectively: W ater, P 1, P 2,P 3.9interested in identifying each distinct protein and control reporters as well,so the total number of classes that we try to discriminate are 4, i.e. thethree proteins and W ater.We repeated each classification scheme 10000 times, using different divi-sions into training and testing sets to obtain estimated of the misclassifica-tion error rates for each class.The transforms Tk in steps (B1)-(B3) are set to be the discrete wavelettransform with Daubechies mother wavelet (8 vanishing moments). Therestricted subset of action S (as in (B2)) is the detail level at scale 2−5(highlighting intermediate scale details of the image) for k even and thedetail level at scale 2−7 (fine scale details) for k odd (see [2] section 7.7 formore on two dimensional orthogonal wavelet bases).Remark 2:The signal transforms need to be selected according to thespecifics of the problem. The potential of our method is that the featuresexploration performed by functional dissipation utilizes variations of generalpurpose signal transforms, avoiding the need to exactly adapt the transformto the characteristics of the classes of signals.Remark 3:Moments of images for large iterations can show sometimesa very distinct order of magnitude for different classes. While this is goodin general for classification, it is not so good for selecting suitable ‘best’features. The reason is that the differences among some of the classes canbe flattened if they are very close with respect to other classes. Thereforewe take the logarithm of the absolute value of moments in all the followingclassification schemes, and we scale them to have joint variance 1 and mean0.First, with K = 0 no dissipation or coefficient masking occurs. For eachdroplet image a two dimensional feature vector is extracted with (B1)-(B3)by computing the log of the norm of the third and forth moments of theimages. The 3-NN classifier trained on these features achieves classificationwith a low degree of accuracy especially for P 3: the estimated misclassifica-tion errors for the three test proteins and W ater are: for P 1 0.062; for P 20.050 for P 3 0.227 and for W ater 0.099. This poor classification is not sur-prising as statistics of images before a suitable image transform are generallyconsidered a weak classifier.Second, we now take N = 100 random functions An, n = 1, ..., N definedon ZZ as in section 2. For each An we apply steps (B1)-(B3) with K = 1iterations and we select M = 1000 coefficients. The repeated use of (B1)-(B3) for each masking An gives a 2N K + 2 = 202-dimensional feature vectorfor each image droplet (we counted only one time the moments of the inputimages). We select now the ‘best’ p features (say for example p = 2 to10be consistent with the previous case) for which the ratio of between-classvariance over within-class variance for the training sets were maximal (see[11] page 94 for more on such notion).If we use only these 2 features in the 3-NN classification of the test sets,then the estimated misclassification errors for the four classes are: for P 10.050; for P 2 0.050 for P 3 0.481 and for W ater 0.177. Not only there is noimprovement over the case K = 0, but results significantly worsen for P 3and W ater (this is so even when we let p increase). This is at first sightpuzzling as we are using also the features collected in the case K = 0, butthe features selection may well choose a feature that has better separationfor some classes, but worse for others. For other tests sets we did not observesuch peculiar phenomenon, but the error rates are consistently of the sameorder of the case K = 0.Third, we take N = 100 random masks An, n = 1, ..., N defined onZZ as in section 2 and we now turn on the function dissipation techniqueby setting K = 20 in the steps (B1)-(B3). At each iteration we selectM = 1000 coefficients.If we repeat (B1)-(B3) for each masking An, weobtain a 2N K + 2 = 4002-dimensional feature vector for each image droplet.We select now the ‘best’ p = 2 features for which the ratio of between-classvariance over within-class variance for the training sets were maximal. If weuse only these 2 moments in the 3-NN classification of the test sets, thenthe estimated misclassification errors for the four classes are:for P 1 < 10−4;for P 2 0.051 for P 3 0.059 and for W ater 0.019. For P 3 and W ater, thisis an excellent improvement with respect to the feature selection based onlogarithms of moments of the original images (the first case we consideredwith K = 0), while errors are of the same order for P 1 and P 2.The progression from K = 1 to K > 1 shows in a compelling waythat both masking function and dissipation are essential ingredients of themethod to improve error rates and masking alone (which can be seen as atype of nonlinear projection) is not sufficient, or may actually, in some cases,be harmful by itself for classification.In Figure 3 we show the third moment (skewness) for the 20 instancesof P 2 (in green) and the 20 instances of P 3 (in yellow), as we move alongthe dissipation process for one specific mask that shows improved separationamong these two classes. At k = 0 we pictured the moment distribution ofthe images when no mask and no dissipation is applied. Note that thereis no good separation between these classes for k = 0, ..., 6, then finallywe start to see a divergence of the clusters of moments and for k > 8 weobserve significant separation of the classes, the bottom subplot shows theshape of the corresponding mask. This distinct improvement of separation116420−2010.50−0.5−10510k15202004006008001000mFigure 3: The top subplot shows the logarithms of the norm of the thirdmoment (skewness) for the 20 instances of P 2 (in green) and the 20 instancesof P 3 (in yellow). They are plotted at each of k = 1, ..., 20 iterations of (B1)-(B3) for a specific mask that shows improved separation among these twoclasses. At k = 0 we plot the logarithms of the norm of the skewness ofthe input images themselves. The bottom subplot shows the correspondingmask defined for indexes m = 1, ..., 1000.12with tightly clustered classes happens, with similar dynamics, for 22 out ofthe 100 masks that we generated, while for the remaining masks we have noimprovement in separation as shown in Figure 4 or we have poorly clusteredclasses. Interestingly, most (but not all) of the masks that show improvedseparation between P 2 and P 3 assume negative values for for the first fewhundred largest coefficients. This generic shape would have been difficult topredict, but is simple enough that it raises the hope that spanning a smallspace of masks will be enough in general classification problems, greatlyreducing the training computational cost, but it is likely that thousandsof masks may be necessary for very difficult classification problems. Notethat, for different pairs of classes, the masks that improve separation mayhave very different shapes, for example, separation between P 1 and P 2 isparticularly improved by some masks that assume negative values for themiddle range coefficients and the total number of masks for which we haveimproved separation with tight clusters is smaller in this case, 13 masksout of 100. We will use now this last observation to lower even further theclassification error rates.The classification with N = 100 masks and K = 20 iterations signifi-cantly improved error rates, but we still had a roughly 5% error rate for P 1and P 2. If we relax the artificial restriction that only 2 best features areextracted we can greatly improve on this result. More specifically supposewe extract one best feature for each pair of classes, and let mab be suchfeature (we have a total of 6 such features for a 4 classes problem). Thebest features for each pair of classes are shown in Figure 5. Within thesetting of the previous classification schemes, apply to each testing imagea 3-NN classification for feature mab using only classes a and b and repeatthe classification for each of the 6 features. Finally use a majority voteamong all 6 classifications to assign a class to each testing image. With thesame N = 100 masks and K = 20 iterations as in the previous cases theestimated error rate are now: for P 1 < 10−4; for P 2 < 10−4 for P 3 0.008and for W ater 0.002. If we take only K = 1 the errors are instead large:forP 1 0.027; for P 2 0.050 for P 3 0.246 and for W ater 0.054. With both largenumber of masks and dissipation turned on there is virtually no misclassi-fication. This raises the issue of whether we are overfitting the data, so asa final test of the method, we mixed randomly all images from all classes,we divided them in four sets of 20 images each and then we applied to thesefour pseudo-classes the 6 features classification that we just described. Inthis case no mask among those that we generated and no dissipation processis able to improve classification results, which are consistently very poor ex-actly as we would expect. We summarize our results for the 5 classification1310−1−2−300.80.60.40.200510k15202004006008001000mFigure 4: The top subplot shows the logarithms of the norm of the thirdmoment (skewness) for the 20 instances of P 2 (in green) and the 20 instancesof P 3 (in yellow). They are plotted at each of k = 0, ..., 20 iterations of (B1)-(B3) for a specific mask that does not show improved separation amongthese two classes. At k = 0 we plot logarithms of norm of skewness ofthe input images themselves. The bottom subplot shows the correspondingmask defined for indexes m = 1, ..., 1000.14Class K=0 p=2 K=1 p=2 K=20 p=2 K=1 p=6 K=20 p=60.002< 10−4< 10−40.008< 10−40.0510.0590.0190.0990.0620.0500.2270.0540.0270.0500.2460.1770.0500.0500.481W aterP 1P 2P 3Table 1: Estimated error rates for each tested class for the 5 versions of the3-NN algorithm we tested. In the three cases on the left of the table we usethe 2 best features obtained by the functional dissipation with: K = 0 (nodissipation), K = 1 (one step dissipation) and K = 20; the right two casesuse K = 1 and K = 20 iterations and we select the 6 best features suchthat each optimizes separation among training sets of two different classes.In all cases (except for K = 0, where no mask is needed) we used the sameN = 100 masks.schemes that we used in Table 1.The use of many masks allow to look at data from many different (albeitunstructured) view points: in line with microarray approach, we suggestivelycall each of the elements of the feature vector a dissipative gene. When wedisplay the resulting dissipative genes in several columns, each column rep-resenting the dissipative genes for one instance of a protein or W ater, wehave what we can properly call a functional dissipation microarray. In is in-teresting to note that, supposedly, one of the weaknesses of matching pursuitis its inability, as a greedy algorithm, to find an optimal representation fora given signal; the use of randomization and dissipation turns this weaknessin a strength, at least in the setting of classification problems. This changeof perspective is in line with the idea that greedy matching pursuit methodshave a greater potential than simply being approximations to optimal solu-tions. The dynamical interaction of masks and dissipative iterations makesthe signals ‘flow’ in the feature space and it is this flow that often carriesthe most significant information on the initial conditions and therefore onthe signals themselves.AcknowledgmentsThe authors gratefully acknowledge support from DOE grant, DE-F C52-04NA25455. We also thank the anonymous referee for very useful and con-structive remarks.15543210−1−2−3P1,P2P1,P3P1,WaterP2,P3P2,WaterP3,WaterFigure 5: In this figure we plot the logarithms of the norm of the 6 bestfeatures among all those generated by (B1)-(B3) with N = 100 masks andK = 20 iterations. Each feature is chosen to optimize pairwise separationamong training sets of two different classes, labelled on the X-axis. Instancesof of P 1 are in blue, P 2 in green, P 3 in yellow and W ater in red.References[1 ] P. Baldi, G. W. Hatfield, W. G. Hatfield, DNA Microarrays and GeneExpression : From Experiments to Data Analysis and Modeling. Cam-bridge University Press (2002).[2 ] S. Mallat, A Wavelet Tour of Signal Processing, Academic Press (1998).[3 ] G. Davis, S. Mallat and M. Avelaneda, Adaptive Greedy Approxima-tions, Jour. of Constructive Approximation, 13 1997 (1), pp. 57 − 98.[4 ] E. J. Candes and J. Romberg, Practical Signal Recovery from RandomProjections, Wavelet Applications in Signal and Image Processing XI,Proc. SPIE Conf. vol. 5914 (2005), 59140S.[5 ] E. J. Candes, J. Romberg and T. Tao, Robust uncertainty principles:exact signal reconstruction from highly incomplete frequency informa-tion. IEEE Trans. Inform. Theory, 52 (2004) 489-509.[6 ]Y. Freund, R. Schapire, A short introduction to boosting, J. Japan. Soc.for Artifcial Intelligence, vol. 14 n. 5 (1999), pp. 771 − 780.16[7 ] L. Breiman. Bagging predictors. Machine Learning, 24 (1996), pp.123 − 140.[8 ] P. Vincent, Y. Bengio, Kernel matching pursuit, Mach. Learn. J. 48(2002) (1), pp. 165 − 187.[9 ] J. Friedman, T. Hastie, R. Tibshirani, Additive Logistic Regression :a Statistical View of Boosting, Annals of Statistics, 28 (2000), pp.337 − 374.[10 ] J. Friedman, Greedy function approximation: A gradient boostingmachine. Ann. Statist. 29 (2001) (5), pp. 1189 − 1232.[11 ] T. Hastie, R. Tibshirani, J. Friedman, The elements of StatisticalLearning, Springer (2001).[12 ] K. Fukunaga.Introduction to Statistical Pattern Recognition (2ndEdition ed.), Academic Press, New York (1990).[13 ] A. Laine and J. Fan, Texture classification by wavelet packet signa-tures. IEEE Trans. Pattern Anal. Mach. Intell. 15 11 (1993), pp.1186 − 1191.[14 ] Q. Jin and R. Dai, Wavelet invariants based on moment representation,Pattern Recognition Artif. Intell. 8 (1995) (3), pp. 179 − 187.[15 ] S. Noh, K. Bae, Y. Park, J. Kim, A Novel Method to Extract Featuresfor Iris Recognition System. AVBPA 2003, LNCS 2688 (2003), pp.862 − 868.[16 ] R.W. Brockett, Dynamical systems that sort lists, diagonalise matri-ces, and solve linear programming problems. Lin. Algebra Appl. 146(1991), pp. 79 − 91.[17 ] V. N. Morozov, N. N. Vsevolodov, A. Elliott, C.Bailey, Recognition ofProteins by Crystallization Patterns in an Array of Reporter SolutionMicrodroplets, Anal. Chem. vol. 78 (2006), pp. 258 − 264.17