NIH Public AccessAuthor ManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.Published in final edited form as:Neurocomputing. 2011 June 1; 74(12-13): 2184–2192. doi:10.1016/j.neucom.2011.02.014.Physical Activity Recognition Based on Motion in ImagesAcquired by a Wearable CameraHong Zhanga,*, Lu Lia,b, Wenyan Jiab, John D. Fernstromc, Robert J. Sclabassib, Zhi-HongMaod, and Mingui Sunb,d,*a Image Processing Center, Beihang University, Beijing 100191, Chinab Department of Neurosurgery, University of Pittsburgh, PA 15213, USAc Department of Psychiatry, University of Pittsburgh, PA 15261, USAd Department of Electrical and Computer Engineering, University of Pittsburgh, PA 15261, USAAbstractA new technique to extract and evaluate physical activity patterns from image sequences capturedby a wearable camera is presented in this paper. Unlike standard activity recognition schemes, thevideo data captured by our device do not include the wearer him/herself. The physical activity ofthe wearer, such as walking or exercising, is analyzed indirectly through the camera motionextracted from the acquired video frames. Two key tasks, pixel correspondence identification andmotion feature extraction, are studied to recognize activity patterns. We utilize a multiscaleapproach to identify pixel correspondences. When compared with the existing methods such as theGood Features detector and the Speed-up Robust Feature (SURF) detector, our technique is moreaccurate and computationally efficient. Once the pixel correspondences are determined whichdefine representative motion vectors, we build a set of activity pattern features based on motionstatistics in each frame. Finally, the physical activity of the person wearing a camera is determinedaccording to the global motion distribution in the video. Our algorithms are tested using differentmachine learning techniques such as the K-Nearest Neighbor (KNN), Naive Bayesian and SupportVector Machine (SVM). The results show that many types of physical activities can be recognizedfrom field acquired real-world video. Our results also indicate that, with a design of specificmotion features in the input vectors, different classifiers can be used successfully with similarperformances.KeywordsActivity recognition; classification; feature extraction; feature matching; motion histogram;multiscaleI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript1. IntroductionVideo based activity recognition has been an active field of research in computer vision andmultimedia systems [1-9]. Although numerous algorithms have been developed, a© 2011 Elsevier B.V. All rights reserved.*Corresponding author. dmrzhang@buaa.edu.cn drsun@pitt.edu.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to ourcustomers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review ofthe resulting proof before it is published in its final citable form. Please note that during the production process errors may bediscovered which could affect the content, and all legal disclaimers that apply to the journal pertain.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 2fundamental requirement has been that the target object engaging in a certain activity mustappear in the video, which is not achievable in many practical cases where the object neverappears in the video because the camera can only be mounted on the target object itself.Examples of such objects include a spaceship, an aircraft, a submarine, a vehicle, a robot, ananimal, or a person. For example, if the goal is to study an individual's physical activity overan entire day in a free-living environment, it is unrealistic to track the person with videocameras. Alternatively, with today's technological advancement, a subject can comfortablywear a small camera for the entire day. Although he/she does not appear in the recordedvideo, physical activity can be recognized indirectly by observing the recorded backgroundscene. In general, a specific activity will result in a specific motion of the camera since it ismounted on the human body and the background scene will change accordingly when thecamera is moved.We have been investigating the use of a wearable video device to monitor food intakecontinuously in obese individuals [10]. However, modification of diet (energy input)represents only half of the energy balance equation of the human body. The other half isphysical activity (energy output). A worn device that unobtrusively and automaticallyrecords physical activity will provide a powerful tool for the development of individualizedobesity treatment programs that help people lose weight and keep it off.Wearable sensors that objectively measure body motion and dynamics have been developed[11-14]. One common approach is to use accelerometers attached at multiple locations of thebody to measure both acceleration and orientation [11, 12]. The accuracy of physical activityrecognition by accelerometer-based systems generally improves as the number ofaccelerometers increases. However, the obtrusiveness of such systems makes it inconvenientto wear and use in daily life. We have thus developed a new wearable device, whichcontains a video camera and other sensors, to monitor both food intake and physical activity(see Fig. 1). The device is mounted in front of the chest using a pin, a lanyard or a pair ofmagnets, allowing it to measure the trunk motion of the upper body. While the generaldesign of the device and its food intake measurement function are published elsewhere [10,15, 16], this paper describes the algorithms utilized to process the acquired video data andrecognize several common types of physical motion and activity.Numerous vision algorithms are available for activity recognition using features extractedfrom the observed target directly[2, 3]. Unfortunately, these algorithms do not apply to ourcase where the target (a person) does not appear in the video. The key problem is thereforeto find descriptions of the physical activity in the recorded video without direct observationof the target. These descriptions can be obtained if the following two assumptions aresatisfied: 1) the motion profiles of the activities to be recognized differ from each other, and2) the background scene is rich enough so that sufficient image features can be extractedreliably.We approach the indirect activity recognition problem by investigating camera motion anddeveloping an activity detection scheme based on 2D image features. Considering that thecamera in the wearable device is usually not controlled intentionally and, as a result, theacquired images are often blurry, we match correspondent points between adjacent framesusing multiscale local features. In order to reduce errors in pixel-pair matching, we imposeuniqueness and epipolar constraints which eliminate ambiguous pixel pairs. After thecorrespondence selection process, a motion histogram is defined according to motionvectors obtained from the selected pixel pairs. For each activity video, an accumulation ofmotion vectors is evaluated based on a set of motion histograms to obtain global motioncharacteristics which finally lead to physical activity recognition.Neurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 3This paper is organized as follows. Section 2 provides an overview of our algorithms.Detailed descriptions of these algorithms are presented in Section 3. Experimental results areprovided in Section 4. In Sections 5 and 6, we summarize our work and discuss futuredirections on physical activity recognition using the wearable camera approach.2. OutlineOur framework for recognizing physical activity is shown in Fig. 2. It consists of three dataprocessing steps: feature extraction, motion representation and activity recognition. In thefirst step, local image features are extracted from which a set of “salient key points” aredetermined. In the second step, we match these key points between neighboring frames. Thematched points define a set of motion vectors. A motion histogram is then defined accordingto these vectors. In the last step, the cumulative motion over all frame pairs is evaluated.Physical activity is recognized based on motion histograms and global motioncharacteristics.3. Methods3.1. Feature ExtractionObtaining reliable correspondences of features is essential in our activity recognition systembecause inaccurate correspondences produce ambiguous motion estimation. We use localimage features which are widely investigated [17-26]. We prefer local features to globalfeatures because the local ones can be detected and represented more easily. The keyproblem here is to find salient points in each image. Shi and Tomasi [22] described a methodcalled “Good Features”, which computes the minimum eigenvalue of the covariance matrixinstead of the cost function defined in the Harris detector [23]; Lowe [17] presented a ScaleInvariant Feature Transform (SIFT) method using scale space analysis. This method isinvariant to scale, orientation and affine distortion [18-20]. Bay et al. [24, 25] proposed aSpeed-Up Robust Features (SURF) method using the 2D Haar wavelet.Although the existing methods have been well studied for activity recognition from directlyrecorded images as the input, these methods usually require these images to have reasonablyhigh quality. In our case, however, the wearable device is uncontrolled and thus the imagesacquired are often blurred. We have found that the blur of our images resulted from manyfactors, and it is hence difficult for us to choose the most suitable model for de-blurring.Occasionally, an incorrect model even aggravates noise. Hence we use a multiscale detectorto capture motion features in an ”overlooking” scale in which the extracted information isless affected by blurring than that in the raw image.In our application, the wearable camera is often used continuously to record data for morethan ten hours a day. Existing multiscale methods such as SIFT [17] and SURF [24] are notsufficiently efficient in our case. We have therefore developed a new multiscale featuredetection approach, which works rapidly and reliably on data containing noise anddistortion, such as affine distortion and illumination variation.We first define interest pixels in a single image I from the covariance matrix C at pixel p:(1)Neurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 4where Ix and Iy are, respectively, the intensity partial derivatives along the x- and y-axes,and Np is the 3x3 neighborhood of the feature pixel p. We use the minimum eigenvalue todetermine the interest pixels in the input image and reduce the effect of affine distortion[22]. Given λp of the minimum eigenvalue at pixel p, the most interested pixels (features) arechosen where λp > c · λmax , with λmax being the largest eigenvalue for the pixels in I, and cbeing set to 0.01 in our experiments.Next, we combine the above feature detection approach with a multiscale analysis. Fordiscrete images, our multiscale representation is expressed as a Gaussian pyramid accordingto the scale space theory [18, 27]. The Gaussian pyramid is a set of images {g0, g1, ⋯ , gk}constructed from the original image g0, where index k, k = 0, 1, 2, … , N, stands for the levelin the pyramid. The adjacent elements in the pyramid are related by(2)where w(m, n) is a Gaussian weighting function in the neighborhood of gk and N refers tothe number of levels in the pyramids. We find all features in gk for all levels of k using themethod in [22]. In order to normalize the results, we remap all pixels (u, v) of interest atscale level k back to the scale of original image, at (feature pixels) are shown in Fig. 3 where the red and green dots represent, respectively, theinterest pixels in the original image and in the immediate level of the Gaussian pyramid.. The selected pixels of interest3.2. Motion RepresentationIn our case, the motion of the wearable camera is closely related to the physical activity thatthe wearer performs. We first find pixel correspondences between adjacent frames in whichan epipolar constraint is imposed. Next, we define a motion histogram obtained from pixelcorrespondences in each pair of video frames. Finally, the motion histograms of all pairs ofvideo frames are used to form feature vectors for activity recognition.3.2.1. Pixel Correspondence—For a feature pixel I(x, y) in the first frame and itscorresponding pixel I′(x′, y′) in the second frame, we impose a uniqueness constraint on I′(x′,y′), given bywhere Ω′ is a neighborhood in I′ centered at I′(x′, y′), Ω′\I′(x′, y′) denotes region Ω′ excludingpixel I′(x′, y′), and α is an empirically selected threshold to enhance reliability of the matchedfeatures.In addition to Eq. (3), we impose the following epipolar constraint for corresponding pixelsI(x, y) and I′(x′, y′) [28]:(3)(4)Neurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 5where and PH are, respectively, the homogeneous coordinates in the forms PH = (x, y, 1)T for I′(x′, y′), and F is a 3×3 fundamental matrix between I′ andfor I(x, y) and I. We use the Random Sample Consensus (RANSAC) algorithm [29, 30] to eliminate thepixel pairs which do not satisfy Eq. (3). At least eight pairs of point correspondences areneeded to estimate the fundamental matrix and remove outliers [31]. The green dots andlines in Fig. 4 show an example of outlier elimination.3.2.2. Motion Distribution in Frame Pairs—Once the points of correspondence aredetermined from video frame pairs, we characterize the physical activity in the videoaccording to the direction and magnitude of motion vectors. Given a pixel correspondence[I(x, y), I′(x′, y′)] in adjacent frames, the motion vector of this pair of pixels is defined as:(5)where θi is the direction of vector Mi in the range of [0, 2π), and |Mi| is the magnitude of Mi.Because our wearable camera records the scene of the background rather than the activityperformer, the magnitude of motion may vary significantly since it is strongly related to thedistance between the background scene and the performer. We therefore propose the use ofan orientation based motion histogram to characterize motion between neighboring frames.We define an n-bin histogram h as follows. First, θi is equally divided into n bins with eachbin covering an angular range of 2π/n. Thus, n specifies the resolution of motion orientation.Next, within each bin, the number of motion vectors in direction θi, which belongs to [s · 2π/n, (s + 1) · 2π/n), s = 0, 1, … , n − 1, is counted, where s is the index of histogram bin. Inorder to reduce measurement error, we require |Mi| to be no less than a threshold t, measuredin pixels. Finally, we normalize the motion histogram h, represented as an n-dimensionalvector m by:(6)where hs is the s-th element in vector h.3.3. Activity RecognitionIn addition to the motion histogram which provides a characterization of motion in a pair offrames, we need an effective representation of motion for the entire video consisting ofnumerous frames. There exists an approach using three-dimensional spatial-temporalfeatures for action recognition [3, 5]. Although this 3D approach is effective, it has a highcomputational complexity. In our application, the daily life physical activities, such aswalking and exercising, usually consist of short segments of simple, repeated actions.Therefore, we define an activity recognition vector representing the statistical characteristicsNeurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 6of the input video segment. Our activity recognition vector consists of two parts: a set ofsummed sample values (here sample values refer to the values in the histogram bins) and thestandard deviation of these values. The summed sample values are utilized for two purposes:1) to reflect the average of the motion histogram across the segment of the activity video;and 2) to smooth out noise. The use of the standard deviation is to capture the range ofsample distribution. A collection of activity recognition vectors is used to train a patternclassifier.3.3.1. Activity Recognition Vector—Let a video contain f frames. For normalizedmotion histogram vector mj in each frame, we compute mean μj and standard deviation σj bywhere mjs is the s-th element of vector mj and j is the frame number. Under the assumptionof ergodicity, we may calculate combined statistics by(7)(8)where μ and σ are the combined mean and standard deviation, and the n + 1 dimensionalvector d is defined as the activity recognition vector (ARV).3.3.2. Training and Classification—In this section, each ARV is computationallyclassified into a certain physical activity. In the classifier, the inputs and output,respectively, are the ARVs and the activity types. We use the Support Vector Machine(SVM) as the classifier because it has been successfully applied to visual activityrecognition tasks with proven efficacy [3, 32]. Other types of classifiers may also beutilized. In our SVM implementation, an automatic parameter selection scheme was utilizedbased on the K-fold cross validation procedure[33]. In this procedure, all SVM parameterswere tested starting from the minimum and then incremented exponentially till themaximum or a pre-defined stopping criterion (maximum number of iterations) was reached.The parameters with the best cross validation result were chosen for the SVM classifier.4. Experimental ResultsThe first step of our experiments was the extraction of stable key points as candidates forfinding point correspondences. After the correspondences were determined, motion vectorswere obtained and mapped to histogram bins according to magnitudes and orientations.Activities were then characterized and recognized by statistically combining information inmotion histograms. To test the stability and compatibility of our method, we chose threepixel features and three classifiers for feature detection and activity recognition. We testedsix video sets containing real-world physical activities, including sitting-up, sitting still,walking, bowing, crouching and waist exercise. The training and testing data were acquiredusing three background scenes for each type of activity. The classifiers were trained andtested using the cross validation scheme. All the six physical activities were performed by asingle human subject. We did not use multiple subjects because we believed that the motionNeurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 7profiles among different healthy subjects of similar body heights were similar. Since thebackground of the scene exerted much stronger impact on the recognition accuracy thansubjects, we tested multiple background scenes (See Fig. 5 for example). Each set of dataconsisted of 100 video segments. The video lengths for different activities were different.However, within each activity, the video length was identical. We collected video data at arate of 10 frames per second with a screen size of 320×240.4.1. Experimental ProcedureOur comparative experiments were arranged as follows. First, we tested three local imagedescriptors, including those by Shi and Tomasi [22], the Speed-up Robust Feature descriptor[24] and our own multiscale descriptor. For the purpose of comparison, we used the SVMclassifier and two histogram configurations with resolutions n = 8 and 10. Next, we variedthe number of orientations n for the chosen classifier and descriptors and observed the trendin error rate. Finally, we tested the performance of different classifiers with varying ARVdimensions.4.2. Performance of Image FeaturesLocal features played an important role in our wearable camera case. Efficiency andaccuracy were both required because the amount of data to be processed was large. We firsttested two state-of-the-art feature extraction methods by Shi and Tomasi [22] and by Bay etal.[24].The Good Features (GF) method [22] enables the detection of occlusions, disocclusionsand features that do not correspond to a real-world point. The only parameters to be set arethe minimal distance between two key points and the minimum quality level of features tobe accepted. In our experiments, we fixed these parameters at 100 and 0.01, respectively.The window size used to estimate the covariance matrix was 3×3.The SURF detector [24] is a widely used, well performed local feature detector thatprovides scale and rotation invariability. This detector uses a similar feature generationprocedure to that used by the SIFT descriptor [17, 18] except an approximated Hessianmatrix is utilized to simplify the calculation of features. The SURF descriptor gives similaror better results under the change of view point, scale and luminance, and the processingspeed is higher[24]. In our experiments, the default 64-dimensional descriptors were used,the Hessian threshold was set to 100, and the number of octaves and the number of layers ineach octave were chosen to be 3 and 4, respectively.We first tested both methods with chosen parameters on six types of activities. The trainingsample size, test sample size and video length in each category are listed in Table 1. Thecolumns are the six individual activities.Table 2 shows the sample recognition rates of GF and SURF methods with respect to thenumber of histogram bins (8, 10 and 16) and the type of physical activity. The last columnprovides overall results. It can be observed that the SURF method over-performed the GFmethod. We believe that our videos acquired by the wearable camera contributed to theperformance difference. These videos were often blurred because of the free movement ofthe human body. With this type of input, the GF method was often ineffective in capturingfeature points. In contrast, the SURF detector was less disturbed because it was multiscale-based allowing the capture of blurred points more effectively.We evaluated the processing times of both methods for feature extraction using 600 videos.The results varied considerably. The average processing times (in seconds) are shown inTable 3 for 8 and 10 histogram bins. Our evaluation was performed on a PC platform ofNeurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 82.13GHz CPU and 4GB memory. We found that, although the SURF method was 8.3%more accurate than the GF method on average (from Table 2), the SURF method required anaverage of 2.5 times more computational time. This is a critical problem since we must beable to process real-world images acquired continuously for as long as several days.With clear advantages and drawbacks of both methods demonstrated, we tested our ownmultiresolution Good Feature detection method (MRGF). The number of resolution levelswas set to 3. The results are shown in Table 2 and Table 3.It can be observed that, although the average recognition rate of the SURF method (91.3%)was slightly better than the MRGF method (90.5%), the MRGF method reduced theprocessing time by about one-half. It can also be observed, from the last column of Table 3,that the multiscale methods (MRGF and SURF) outperformed the single scale method (GF)in all experiments. The recognition precision increased significantly from GF to MRGF inthe condition of low resolution where n equals to 8 and 10.4.3. Resolution of Motion OrientationThe motion histogram requires a choice of the number of histogram bins n which specifiesthe resolution of motion orientation n. We experimentally investigated the trend ofrecognition performance with respect to the choice of n. A comparison of the detection ratesunder different histogram resolution, image feature types and activities is given in Table 2,where the numbers of histogram bins tested were 8, 10 and 16. It can be observed that, forall types of image pixel features, the recognition rate increased with the number ofhistogram bins over all activities. Although for specific activities and image pixel features,the recognition rate fluctuated, the overall tendency was clear according to the data acquiredso far. In order to provide more proof on the relationship between recognition rate andresolution of motion orientation, we conducted another experiment focusing more on thenumber of histogram bins. The threshold of magnitude t defined in Section 3.2.2 was fixedat 3 in all experiments.Fig. 6 gives a detailed representation of the recognition performance with respect to theresolution of motion orientation. Two major conclusions can be drawn from our results.First, the average detection rate increased with n from 4 to 18. The GF, SURF and MRGFmethods all achieved higher recognition rates with a larger n. The lowest rate was with n = 4for all pixel features. Second, when the resolution reached a certain level, the improvementin accuracy became small. It was shown that the SURF and MRGF features were morestable. Since pixel feature representation is essential in the overall performance ofrecognition, the MRGF method, which provides such representation at a low computationalcost, is more advantageous than the other two methods.4.4. Classification of Recognition VectorsSpecific physical activities were recognized based on activity recognition vectors (ARVs).Our main purpose of experiments was to validate the effectiveness of ARVs. Threeindependent classification strategies, including Naive Bayes, K Nearest Neighbor andSupport Vector Machine were used in our experiments.Naive Bayes [34]—The Naive Bayes classifier requires a small amount of training data toestimate the means and variances necessary for classification. By assuming that the variablesin feature vectors are independent, only the variances of the variables for each class need tobe determined and not the entire covariance matrix.Neurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 9K-Nearest Neighbors (KNN) [34]—The K-Nearest Neighbor classifier is robust tonoise. In our experiment, we set the number of nearest neighbors K to 10 empirically. It wasfound that the size of the training set could be chosen in a wide range between 6 and 20, andthe choice was not sensitive to K.Support Vector Machine (SVM) [35-37]We found that, for different data sets, the SVM testing results varied considerably. Wesolved this problem by using a K-fold cross validation technique for parameterselection[33]. In this technique, each training set was divided into 10 subsets (folds). Theparameters with the best cross validation result were used in the classifier.The recognition results of the Naive Bayes, KNN and SVM classifiers are shown in Fig. 7. Itcan be observed that the SVM classifier outperformed other two classifiers when thehistogram resolution was large. However, the Naive Bayes and KNN classifiers excelled inspeed because the SVM classifier spent considerable time on choosing parameters. Allclassifiers achieved recognition rates above 85% for the six types of activities. The bestperformances for the KNN and Naive Bayes classifiers occurred when n was 10, while thebest performance for the SVM classifier occurred when n was 14. Despite these differences,our results show that all three classifiers were applicable to our indirectly recorded activitydata.5. DiscussionIn summary, our indirect activity recognition method mainly consisted of three dataprocessing steps: feature detection, motion representation and activity classification. Wecharacterized the activity patterns in the video using pixel features because this type offeatures was easier to obtain than other types of features, such as lines and blobs. Althoughpixel features can be detected in different ways, including the use of GF and SURF, there isa major trade-off between the accuracy and computational complexity. In our case, we usedthe MRGF detector because it provided a balance between these two factors and the featuresprovided by this detector can be used to construct the motion histograms. The onlyparameter required by the MRGF detector was the resolution of motion orientation, whichwas found to be an insensitive parameter as long as it was sufficiently large.In the activity recognition part of our study we embedded statistical information into motionvectors and used the activity recognition vector to classify and recognize activities. In ourcase, the recorded activity usually consisted of repeated activities of short times. Bysumming the motion vectors, temporal variations were reduced effectively and the featuresbecame more distinct in the activity recognition vector because of the repetition.In general, a specific activity will result in a specific motion of the camera since it ismounted on the human body. When the camera is moved, the background scene will changeaccordingly. We believe that this change contains two components: a component reflectingthe case-dependent scene of the physical world observed by the camera, and a componentspecific to the activity being performed. The change of background scenes can berepresented by extracting image features and obtaining motion vectors. Although we havenot intentionally separated the two components, our results show that activity can berecognized based on the extracted image features, under the assumptions stated below.Firstly, it is assumed that the motion profiles of the classes of activities are sufficientlydifferent. The activities investigated by this manuscript satisfy this assumption and thus canbe recognized using the presented method. However, there exist activities which do notsatisfy this assumption and the motion profiles alone do not lead to effective classification.Neurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 10In these cases, recognition has to be achieved by adding image content analysis which is notcovered by this paper.Secondly, we assume that the background scene is rich enough so that sufficient imagefeatures can be extracted robustly. It is clear that feature extraction and matching will failcompletely when the background scene is textureless (e.g., a smooth, satin white wall).Fortunately, this case is rare in practice and the stated background scene assumption issatisfied in most cases.We mounted our device containing a camera in front of the chest using a pin, a lanyard or apair of magnets. However, in our opinion, this is not the only choice and there is no vantagepoint on the human body to mount the device. In our system design, we had to considerseveral practicality constraints in selecting the mounting point. We chose the chest because:1) it is a stable location that facilitates the acquisition of high-quality images reflecting thetrunk motion of the upper body; and 2) the location is related high in the body to keepcamera view clear from being blocked by arms/hands and other objects, so the occlusionproblem, which is significant in many current activity detection systems, is greatly reduced.As a final remark, for the study of obesity, it is desirable to obtain a quantitative measure ofenergy expenditure for each type of physical activity performed by the subject who iswearing the device. This can be easily calculated using the identified activity and the lengthof time engaged in the activity (both determined by the wearable device) together with atable of energy expenditure values associated with each activity [38].6. ConclusionIn this paper, we have presented a new computational tool to study physical activity byanalyzing video data acquired using a wearable camera. Our investigation has focused onthe feasibility and the framework to perform pattern recognition by inferring from theimages showing only the surrounding scene. We have presented a multiscale scheme toidentify pixels of interest and showed that this scheme can be applied efficiently androbustly. We have also proposed the use of statistical properties of motion vectors. Asshown in our experimental results, the activity recognition vectors that we utilized can beeffectively classified with a high accuracy. The classification error decreases as theresolution of motion orientation vectors increases. Our experimental results have also shownthat different classifiers can be applied to our activity recognition vectors with comparableerror rates. The methods proposed in this work are useful in real-world applications wherethe camera can only be mounted on a target object and the data are acquired under imperfectconditions.AcknowledgmentsThis work was supported by National Institutes of Health Grant No. U01 HL91736 of the United States, theNational Natural Science Foundation of China Grant No. 60872079 and National Key Laboratory on OpticalFeature of Environment and Target Foundation of China No. 9140C6105080C61.I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.BiographiesPage 11I-NHPAAuthorManuscriptHong Zhang received the B.S. degree, M.S. degree and Ph.D. degree in electricalengineering from Hebei University of Technology, China, Harbin University of Science andTechnology, China and Beijing Institute of Technology, China in 1988, 1993 and 2002,respectively. She is currently a Professor of the School of Astronautics, Beihang University,Beijing, China. She was in the Department of Neurosurgery at the University of Pittsburghas a visiting scholar from 2007 to 2008. Her research interests include activity recognition,image restoration, image indexing, object detection and stereo vision.I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptLu Li received the B.S. degree in computer science from Hebei University of Technology,China in 2006. He is currently a Ph.D. candidate in Image Research Center, School ofAstronautics at Beihang University, China. He studied in the Department of Neurosurgery atthe University of Pittsburgh as a visiting student from 2009 to 2010. His research interestsinclude activity recognition, image understanding and stereo vision.Wenyan Jia received the B.S. degree and M.S. degree in biomedical engineering fromCapital Medical University, Beijing, China, in 1998 and 2001, respectively, and the Ph.D.degree in biomedical engineering from Tsinghua University, Beijing, China, in 2005. She iscurrently a research assistant professor of the Department of Neurosurgery at the Universityof Pittsburgh, PA. Her research interests include image processing, pattern recognition, andbiomedical signal processing.Neurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 12John D. Fernstrom received the S.B. (Life Sciences) and Ph.D. (Nutritional Biochemistry& Metabolism) from M.I.T., Cambridge MA. He was a post-doctoral fellow at the RocheInstitute of Molecular Biology, Nutley NJ, and a faculty member at M.I.T prior to becomingProfessor of Psychiatry and Pharmacology at the University of Pittsburgh, and ResearchDirector of the UPMC Weight Management Center. He has authored over 200 articles andreviews in his areas of expertise, and has edited the proceedings of five scientificconferences.Robert J. Sclabassi received the B.S.E. degree in electrical engineering from LoyolaUniversity, Los Angeles, CA, the M.S.E.E. and Ph.D. degrees in electrical engineering fromthe University of Southern California, Los Angeles, and the M.D. degree in medicine fromthe University of Pittsburgh, Pittsburgh, PA. He was with the Advanced Systems Laboratoryat TRW, Los Angeles, and a Postdoctoral Fellow at the Brain Research Institute, Universityof California, Los Angeles (UCLA), where he has also been a Faculty Member in theDepartment of Neurology and Biomathematics. He is currently the CEO of ComputationalDiagnostics, Inc. and a Professor Emeritus of neurological surgery, psychiatry, electricalengineering, mechanical engineering, and behavioral neuroscience at the University ofPittsburgh. He has authored or coauthored more than 400 papers, chapters, and conferenceproceedings. Prof. Sclabassi is a Registered Professional Engineer.Zhi-Hong Mao is an Assistant Professor in the Department of Electrical and ComputerEngineering and Department of Bioengineering at the University of Pittsburgh, Pittsburgh,PA. He received his dual Bachelor's degrees in automatic control and mathematics (1995)and M.Eng. degree in intelligent control (1998) from Tsinghua University, Beijing, China,and S.M. degree in aeronautics and astronautics (2000) and Ph.D. degree in electrical andmedical engineering (2005) from Massachusetts Institute of Technology, Cambridge. Hewas a recipient of the National Science Foundation CAREER Award and Andrew P. SageBest Transactions Paper Award of the IEEE Systems, Man and Cybernetics Society in 2010.Neurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 13Mingui Sun received the B.S. degree from Shenyang Chemical Engineering Institute,Shenyang, China, in 1982, and the M.S. and Ph.D. degrees in electrical engineering from theUniversity of Pittsburgh, Pittsburgh, PA, in 1986 and 1989, respectively. In 1991, he joinedthe faculty of University of Pittsburgh, where he is currently a Professor of neurosurgery,electrical and computer engineering, and bioengineering. His current research interestsinclude advanced biomedical electronic devices, biomedical signal and image processing,sensors and transducers, biomedical instruments, brain-computer interface, electro-neurophysiology, implantable devices, radio-frequency systems for biomedical applications,electronic and data processing systems for diet and physical activity assessment, andwearable computers. He has authored or coauthored more than 300 papers.References1. Poppe R. A survey on vision-based human action recognition. Image and Vision Computing. 2010;28(6):976–990.2. Laptev I, Caputo B, Schüldt C, Lindeberg T. Local velocity-adapted motion events for spatio-temporal recognition. Comput. Vis. Image Underst. 2007; 108(3):207–229.3. Sun J, Wu X, Yan S, Cheong LF, Chua T-S, Li J. Hierarchical spatio-temporal context modeling foraction recognition. Proceedings of IEEE International Conference on Computer Vision and PatternRecognition. 2009:2004–2011.4. Wu, J.; Osuntogun, A.; Choudhury, T.; Philipose, M.; Rehg, JM. Proceedings of the InternationalConference on Computer Vision (ICCV). Rio de: 2007. A scalable approach to activity recognitionbased on object use.5. Laptev I, Lindeberg T. Local descriptors for spatio-temporal recognition. First InternationalWorkshop on Spatial Coherence for Visual Motion Analysis. 20046. Niebles JC, Wang H, Fei-fei L. Unsupervised learning of human action categories using spatial-temporal words. Proc. BMVC. 20067. Xu D, Chang S-F. Video event recognition using kernel methods with multi-level temporalalignment. IEEE Trans. on Pattern Analysis and Machine Intelligence. 2008; 30(11):1985–1997.8. Xu D, Chang S-F. Visual event recognition in news video using kernel methods with multi-leveltemporal alignment. IEEE Int. Conf. on Computer Vision and Pattern Recognition. 20079. Duan L, Xu D, Tsang I, Luo J. Visual event recognition in videos by learning from web data. IEEEInt. Conf. on Computer Vision and Pattern Recognition. 201010. Sun M, Fernstrom J, Jia W, Hackworth S, Yao N, Li Y, et al. A wearable electronic system forobjective dietary assessment. Journal of the American Dietetic Association. 2010; 110:45–47.[PubMed: 20102825]11. Intille SS, Bao L, Bao L. Physical activity recognition from acceleration data underseminaturalistic conditions. Tech. rep. 200312. Tapia EM, et al. Real-time recognition of physical activities and their intensities using wirelessaccelerometers and a heart monitor. Proc. Int. Symp. on Wearable Comp. 200713. Li, L.; Zhang, H.; Jia, W.; Nie, J.; Zhang, W.; Sun, M. Automatic video analysis and motionestimation for physical activity classification; IEEE 36th Annual Northeast BioengineeringConference; 2010.14. Zhang H, Li L, Jia W, Fernstrom JD, Sclabassi RJ, Sun M. Recognizing physical activity fromego-motion of a camera. IEEE International Conf. of EMBS. 2010Neurocomputing. Author manuscript; available in PMC 2012 June 1.   I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 1415. Sun, M.; Yao, N.; Hackworth, SA.; Yang, J.; Fernstrom, JD.; Fernstrom, MH.; Sclabassi, RJ. Proc.Int. Symp. Digital Life Technologies: Human-Centric Smart Living Technology; Tainan, Taiwan:2009. A human-centric smart system assisting people in healthy diet and active living.16. Li, Y.; Zhang, H.; Hackworth, S.; Li, C.; Yue, Y.; Sun, M. The design and realization of awearable embedded device for dietary and physical activity monitoring; Proc. 3rd ISSCAA;Harbin, China. 2010.17. Lowe, D. Object recognition from local scale-invariant features; Seventh International Conferenceon Computer Vision (ICCV'99); 1999. p. 1150-1157.18. Lowe DG. Distinctive image features from scale-invariant keypoints. International Journal ofComputer Vision. 2004; 60:91–110.19. Lindeberg T. Feature detection with automatic scale selection. International Journal of ComputerVision. 1998; 30:79–116.20. Lindeberg T, Bretzner L. Real-time scale selection in hybrid multi-scale representations. Proc.Scale-Space'03, Springer Lecture Notes in Computer Science 2695. 2003:148–163.21. Ke Y, Sukthankar R. Pca-sift: A more distinctive representation for local image descriptors.Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition.2004:506–513.22. Shi J, Tomasi C. Good features to track. Proceedings of IEEE International Conference onComputer Vision and Pattern Recognition. 1994:593–600.23. Harris, C.; Stephens, M. A combined corner and edge detection; Proceedings of The Fourth AlveyVision Conference; 1988. p. 147-151.24. Bay H, Tuytelaars T, Gool LV. Surf: Speeded up robust features. In ECCV. 2006:404–417.25. Bay H, Ess A, Tuytelaars T, Van Gool L. Speeded-up robust features (surf). Comput. Vis. ImageUnderst. 2008; 110(3):346–359.26. Zhang H, Mu Y, You Y, Li J. Multi-scale sparse feature point correspondence by graph cuts.Science in China Series F:Information Sciences. 2010; 53(6):1224–1232.27. Burt PJ, Edward, Adelson EH. The laplacian pyramid as a compact image code. IEEE Transactionson Communications. 1983; 31:532–540.28. Hartley, R.; Zisserman, A. Multiple View Geometry in Computer Vision. Cambridge UniversityPress; 2003.29. Fischler MA, Bolles RC. Random sample consensus: a paradigm for model fitting withapplications to image analysis and automated cartography. Commun. ACM. 1981; 24(6):381–395.30. Torr PHS, Murray DW. The development and comparison of robust methods for estimating thefundamental matrix. International Journal of Computer Vision. 1997; 24:271–300.31. Ma, Y.; Soatto, S.; Kosecka, J.; Sastry, SS. An Invitation to 3-D Vision: From Images toGeometric Models. SpringerVerlag; 2003.32. Laptev I, Marszalek M, Schmid C, Rozenfeld B, Rennes I, Grenoble II, Ljk L. Learning realistichuman actions from movies. CVPR. 200833. Intel. Opencv library version 2.0, Tech. rep., Intel. 2009.http://sourceforge.net/projects/opencvlibrary/34. Fukunaga, K. Introduction to statistical pattern recognition. 2nd ed.. Academic Press Professional,Inc.; 1990.35. Cortes C, Vapnik V. Support-vector networks. Machine Learning. 1995:273–297.36. Drucker H, Burges CJC, Kaufman L, Smola AJ, Vapnik V. Support vector regression machines.NIPS'96. 1996:155–161.37. Meyer D, Leisch F, Hornik K. The support vector machine under test. Neurocomputing. 2003;55(1-2):169–186.38. Ainsworth B, Haskell W, Whitt M, Irwin M, Swartz A, Strath S, et al. Compendium of physicalactivities: an update of activity codes and met intensities. Medicine & Science in Sports &Exercise. 2000; 32(9 Suppl):S498–504. [PubMed: 10993420]Neurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 15I-NHPAAuthorManuscriptFigure 1.Wearable camera developed in our laboratory. Left: person wearing the device. Right:Circuit board of the device where the camera is positioned inside the red circle.I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 16Figure 2.Framework for activity recognitionI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 17Figure 3.Feature pixels detected by ”Good Features” [22] in the original image (red dots) and the nextlevel of the Gaussian pyramid (green dots).I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 18I-NHPAAuthorManuscriptFigure 4.(a) Pixel features are shown in red dots (b) Motion vectors (dot: location; short line:direction) computed from feature pixels. Red and green vectors indicate, respectively, thevectors satisfy and do not satisfy Eq. (4).I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 19Figure 5.Test scenes for walking.(a) Corridor, (b) Room (far), (c) Room (near), (d) OutdoorI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 20Figure 6.Recognition rate (vertical axis) of six activities using the SVM method with respect tohistogram resolution (horizontal axis).(a) top three curves: sitting-up, bottom three curves:sitting still; (b) top three curves: walking, bottom three curves: bowing;(c) top three curves:crouching, bottom three curves: waist exercise; (d) Overall rate (average over all sixactivities).I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 21I-NHPAAuthorManuscriptFigure 7.Recognition rate using Naive Bayes, K Nearest Neighbors and Support Vector Machine.Horizontal axis: resolution of motion orientation. Vertical axis: recognition rate. Imagefeatures: MRGF.I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.   Zhang et al.Page 221elbaT,)KW(gniklaw,)SS(llits-gnittis,)US(pu-gnittiS:tfelmorfsnmuloC.seitivitcafoseirogetacxisnihtgneloedivdnaeziselpmastset,eziselpmasgniniarT.semarfniderusaemsihtgneloediV.)EW(esicrexetsiawdna)RC(gnihcuorc,)WB(gniwobEWRCWBKWSSUS001001121001001180010011600100115001001elpmaStseT1515htgneLoediV001001elpmaSgniniarTNeurocomputing. Author manuscript; available in PMC 2012 June 1.I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript                    Zhang et al.Page 232elbaT,)US(pu-gnittiS:thgirottfelmorfsnmuloC.detseterew61dna01,8=nsnibmargotsihfosrebmuN.sdohtemFGRMdnaFRUS,FGfosetarnoitingoceR.)LLA(egarevalatotdna)EW(esicrexetsiaw,)RC(gnihcuorc,)WB(gniwob,)KW(gniklaw,)SS(llits-gnittisLLAEWRCWBKWSSUS%18%19%68%07%57%27%198=n%79%79%79%59%59%89%8961=n%88%49%19%28%08%88%398=n%29%69%89%68%09%49%7861=n%09%49%48%88%28%49%598=n%88%19%49%28%48%39%6801=nFG%39%59%39%98%09%69%6901=nFRUS%59%79%89%29%29%89%5961=n%39%29%79%88%09%89%3901=nFGRMNeurocomputing. Author manuscript; available in PMC 2012 June 1.I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript                             Zhang et al.Page 243elbaTdna8=nerewsnibmargotsihfosrebmunehT.soediv001deniatnocytivitcahcaE.))EW(esicrexetsiawdna)RC(gnihcuorc,)WB(gniwob,)KW(gniklaw,)SS(llits-gnittis,)US(pu-gnittisgniniatnoc,worpot(detseterewseitivitcaxiS.noitcartxeerutaefegamilacolrof)sdnocesni(semitgnissecorpegarevAEWRCWBKWSSUS3928826979774835934225223271476139231814716961774628729211213767177627525414118=n43181101=n2840928=n65418201=n7912818=n61257101=nFGFRUSFGRM01=nNeurocomputing. Author manuscript; available in PMC 2012 June 1.I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript                              