Artificial Intelligence 173 (2009) 696–721Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintProbabilistic planning with clear preferences on missing informationMaxim Likhachev a,∗, Anthony Stentz ba Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USAb The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 8 October 2007Received in revised form 27 October 2008Accepted 29 October 2008Available online 25 November 2008Keywords:Planning with uncertaintyPlanning with missing informationPartially Observable Markov DecisionProcessesPlanningHeuristic searchFor many real-world problems, environments at the time of planning are only partially-known. For example, robots often have to navigate partially-known terrains, planes oftenhave to be scheduled under changing weather conditions, and car route-finders oftenhave to figure out paths with only partial knowledge of traffic congestions. While generaldecision-theoretic planning that takes into account the uncertainty about the environmentis hard to scale to large problems, many such problems exhibit a special property: onecan clearly identify beforehand the best (called clearly preferred) values for the variablesthat represent the unknowns in the environment. For example, in the robot navigationproblem, it is always preferred to find out that an initially unknown location is traversablerather than not, in the plane scheduling problem, it is always preferred for the weather toremain a good flying weather, and in route-finding problem, it is always preferred for theroad of interest to be clear of traffic. It turns out that the existence of the clear preferencescan be used to construct an efficient planner, called PPCP (Probabilistic Planning with ClearPreferences), that solves these planning problems by running a series of deterministic low-dimensional A*-like searches.In this paper, we formally define the notion of clear preferences on missing information,present the PPCP algorithm together with its extensive theoretical analysis, describe severaluseful extensions and optimizations of the algorithm and demonstrate the usefulnessof PPCP on several applications in robotics. The theoretical analysis shows that onceconverged, the plan returned by PPCP is guaranteed to be optimal under certain conditions.The experimental analysis shows that running a series of fast low-dimensional searchesturns out to be much faster than solving the full problem at once since memoryrequirements are much lower and deterministic searches are orders of magnitude fasterthan probabilistic planning.© 2008 Elsevier B.V. All rights reserved.1. IntroductionA common source of uncertainty in planning problems is lack of full information about the environment. A robot maynot know the traversability of the terrain it has to traverse, an air traffic management system may not be able to forecastwith certainty future weather conditions, a car route-finder may not be able to predict well future traffic congestions oreven be sure about present traffic conditions, a shopping planner may not know whether a particular item will be on saleat one of the stores it considers. Ideally, in all of these situations, to produce a plan, a planner needs to reason over theprobability distribution over all the possible instances of the environment. Such planning is known to be hard [1,2].* Corresponding author.E-mail address: maximl@seas.upenn.edu (M. Likhachev).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.10.014M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721697For many of these problems, however, one can clearly name beforehand the “best” values of the variables that representthe unknowns in the environment. We call such values clearly preferred values. Thus, in the robot navigation problem, it isalways preferred to find out that an initially unknown location is traversable rather than not. In the air traffic managementproblem it is always preferred to have a good flying weather. In the problem of route planning under partially-known trafficconditions, it is always preferred to find out that there is no traffic on the road of interest. And finally, in the shoppingplanning example, it is always preferred for a store to hold a sale on the item of interest. These are just few of what webelieve to be a large class of planning problems that exhibit clear preferences on missing information. One of the reasonsfor this is that the knowledge of clear preferences on missing information is not the same as the knowledge of a best actionat a state or the value of an optimal policy. Instead, we often know at intuitive level what would be the best event for us(i.e., no traffic congestion, sale, etc.), independently of whether we choose to make use of this event or not. All the otheroutcomes, on the other hand, are of less preference to us. This intuitive information can be used in planning.In this paper we present an algorithm called PPCP (Probabilistic Planning with Clear Preferences) that is able to scaleup to very large problems by exploiting the fact that these preferences exist. PPCP constructs and refines a plan by runninga series of deterministic A*-like searches. Furthermore, by making an approximating assumption that it is not necessaryto retain information about the variables whose values were discovered to be clearly preferred values, PPCP keeps thecomplexity of each search low and independent of the amount of the missing information. Each search is extremely fast,and running a series of fast low-dimensional searches turns out to be much faster than solving the full problem at once sincethe memory requirements are much lower and deterministic searches can often be many orders of magnitude faster thanprobabilistic planning techniques. While the assumption PPCP makes does not need to hold for the algorithm to converge,the returned plan is guaranteed to be optimal if the assumption does hold.The paper is organized as follows. We first briefly go over A* search and explain how it can be used to find least-costpaths in graphs. We then explain how a planning problem changes when some of the information about the environmentis missing. In Section 4, we introduce the notion of clear preferences on missing information and briefly talk about theproblems that exhibit them. In Section 5, we explain the PPCP algorithm and how it makes use of the clear preferences. Thesame section gives an extensive theoretical analysis of PPCP that includes the correctness of the algorithm, some complexityresults as well as the conditions for the optimality of the plan returned by PPCP. In Section 6 of the paper, we describe twouseful extensions of the algorithm such as how one can interleave PPCP planning and execution. In the same section, wealso give two optimizations of the algorithm which at least for some problems can speed it up by more than a factor offour. On the experimental side, Section 7 shows how PPCP enabled us to successfully solve the path clearance problem, animportant problem in defense robotics. The experimental results in Section 8.1, on the other hand, evaluate the performanceof PPCP on the problem of robot navigation in partially-known terrains. They show that in the environments small enoughto be solved with methods guaranteed to converge to an optimal solution (such as Real-Time Dynamic Programming [3]),PPCP always returns an optimal policy while being much faster. The results also show that PPCP is able to scale up to large(costmaps of size 500 by 500 cells) environments with thousands of initially unknown locations. The experimental resultsin Section 8.2, on the other hand, show that PPCP can also solve large instances of path clearance problem and results insubstantial benefits over other alternatives. We finally conclude the paper with a short survey of related work, discussion,and conclusions.2. Backward A* search for planning with complete informationNotations. Let us first consider a planning problem that can be represented as a search for a path in a fully knowndeterministic graph G. The fact that the graph G is completely known at the time of planning means that there is nomissing information about the domain (i.e., environment). We use S to denote a state (a vertex, in the graph terminology)in the graph G. State Sstart refers to the state of the agent at the time of planning, while state Sgoal refers to the desiredstate of the agent. We use A(S) to represent a set of actions available to the agent at state S ∈ G. Each action a ∈ A(S)corresponds to a transition (i.e., an edge) in the graph G from state S to the successor state denoted by succ(S, a). Eachsuch transition is associated with the cost c(S, a, succ(S, a)). The costs need to be bounded from below by a (small) positiveconstant.Backward A* search. The goal of shortest path search algorithms such as A* search [4] is to find a path from S start toSgoal for which the cumulative cost of the transitions along the path is minimal. The PPCP algorithm we present in thispaper is based on running a series of deterministic searches. Each of these searches is a modified backward A* search—theA* search that searches from Sgoal towards Sstart by reversing all the edges in the graph. In the following, we thereforebriefly describe the operation of a backward A* search.∗(S).Suppose for every state S ∈ G we knew the cost of a least-cost path from S to S goal. Let us denote such cost by gThen a least-cost path from Sstart to Sgoal can be easily followed by starting at Sstart and always executing such action∗(succ(S, a)). Consequently, A* search tries to computea ∈ A(S) at any state S that a = arg mina∈ A(S)(c(S, a, succ(S, a)) + g∗-values. In particular, A* maintains g-values for each state it has visited so far. g(S) is always the cost of the best pathgfound so far from S to Sgoal. The pseudocode in Fig. 1 gives a simple version of backward A*. In this version, besta pointersare used to store the actions that follow the found paths.The code starts by setting g(Sgoal) to 0 and inserting Sgoal into OPEN. The code then repeatedly selects states fromOPEN and expands them—executes lines 6 through 9. At any point in time, OPEN is a set of states that are candidates for698M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–7211 g(S start) = ∞, OPEN = ∅;2 g(S goal) = 0, besta(S goal) = null;3 insert S goal into OPEN with the priority equal to g(Sgoal) + heur(S goal);4 while( g(S start) > minS∈OPEN(g(S) + heur(S)))56789remove state S with minimum priority (g(S) + heur(S)) from OPEN;such that S = succ(Sfor each action a and S(cid:6)) > c(S(cid:6)yet or g(S(cid:6)) = a;(cid:6), a, S) + g(S), besta(Sinto OPEN with the priority equal to g(S(cid:6), a)(cid:6), a, S) + g(S)if search hasn’t seen Sg(Sinsert S(cid:6)) = c(S(cid:6)(cid:6)) + heur(S(cid:6));(cid:6)Fig. 1. Backward A* search.expansion. These are also the states from which new paths to Sgoal have been found but have not been propagated to theirof S can bepredecessors yet. As a result, the expansion of state S involves checking if a path from any predecessor state Simproved by using the found path from S to Sgoal, and if so then: (a) setting the g-value of Sto the cost of the new pathfound; (b) setting action besta(Sinto OPEN. The last operationmakes sure that Sto Sgoal willbe propagated to the predecessors of Swill also be considered for expansion and, when expanded, the cost of the found path S(cid:6)) to the action a that leads to state S; and (c) inserting S.(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)The goal of A* is to expand states in such order as to minimize the number of expansions required to guarantee thatthe states on at least one of the least-cost paths from Sstart to Sgoal are expanded. Backward A* expands states in the orderof g(S) + heur(S), where heur-values estimate the cost of a least-cost path from S start to S. The heur-values must neveroverestimate (i.e., must be admissible), or otherwise A* may return a suboptimal solution. In order for each state not to be(cid:6) ∈ G such thatexpanded more than once, heur-values need to be also consistent: heur(S start) = 0 and for any two states S, S(cid:6), a, S) (cid:2) heur(S). If heur-values are consistent then every time the searchS ∈ succ(Sexpands a state S, a least-cost path from S to Sgoal has already been found and therefore a better path will never show uplater and the state will never be re-inserted into OPEN. Ordering expansions based on the summation of g- and heur-valuesmakes the search focus expansions on the states through which the whole path from S start to Sgoal looks most promising.(cid:6), a) for some a ∈ A(S(cid:6)), heur(S(cid:6)) + c(SThe search terminates when g(Sstart)—the cost of the best path found so far from Sstart to Sgoal—is at least as small asthe smallest summation of g and heur values in OPEN. Consequently, OPEN no longer contains states that belong to thepaths with smaller costs than g(Sstart). This means that A* can terminate and guarantee that the found path is optimal.The proof of this guarantee relies in one way or another on the fact that the g-values of the states on an optimal pathare monotonically decreasing: if an optimal path from Sstart to Sgoal contains a transition S → Svia some action a, then(cid:6)). This monotonicity property will show up later in the paper. In particular, while in a general case optimal∗(S) > ggplans in domains with missing information do not necessarily exhibit monotonicity of state values, we will show that incase of clear preferences, the state values on optimal plans are indeed monotonic in some sense. This will allow us to use aseries of backward A* searches to plan.∗(S∗(cid:6)Example. To make later explanations clearer, let us consider a trivial planning problem shown in Fig. 2(a). Suppose an agentneeds to buy wine and cheese, and there are two stores, store A and store B. Both stores have both products but at differentprices as shown in the figure. Initially, the agent is at home and the cost of traveling from home to each store and inbetween stores can also be translated into money (all the costs are shown in Fig. 2(a)). The planning problem is for theagent to purchase wine and cheese with the minimal cost (including the cost of travel) and return home.Fig. 2(b) shows how this problem can be represented as a graph. Each state encodes the position of the agent and whatit has already bought. Thus, Sstart is {Agent = Home, Bought = ∅} and Sgoal is {Agent = Home, Bought = wine, cheese}. Fig. 2(c)shows g-values, heuristics and priorities f = g + heur of states as computed by backward A* search that was used to finda least-cost path. The found path is shown by thicker lines. The states expanded by A* are shown in grey. For each state S,the heuristic heur(S) is the cost of moving from home to the store the agent is in at state S plus the cost of purchasing theitems that are bought at state S assuming the price is the minimum possible price across both stores (remember that thesearch is backward and therefore the heuristics estimate the cost of a least-cost path from start state to state in question).Thus, an optimal plan for the agent is to go to store A, buy cheese there, go to store B, buy wine there and then returnhome.3. Planning with missing informationIn the example above, the graph G that represents the planning problem and all of its edge costs were fully known.By planning with missing information, on the other hand, we refer to the case when the outcomes of some actions and/orsome edge costs are not known at the time of planning. In particular, there are some hidden variables whose status affectsthe outcomes and/or costs of certain actions. The status of these hidden variables is unknown to the agent at the time ofplanning. Instead, the agent has probability distribution (belief) over the possible values of these hidden variables. Duringexecution, however, the agent may sense one or more of these hidden variables at certain states. Once it senses them, theactual values of these hidden variables become known. By sensing we refer to any action that results in knowing a hiddenvariable. Sometimes, it is an explicit sensing action such as seeing if a region in front of the robot is traversable. In otherM. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721699Fig. 2. Simple example of planning with complete information.700M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721cases, the value of a hidden variable can be deduced from an outcome of an action such as trying to pick up an object andrealizing that it is too heavy to be picked up with a single robot arm. In either case, we assume that the value of the hiddenvariable that controls the outcome of an action becomes known after the action is executed. In terms of explicit sensing,this corresponds to assuming perfect sensing.For example, consider a variation of the grocery shopping problem described above. The variation, shown in Fig. 3(a), isthat store A may conduct a 50% sale on wine products, whereas store B may conduct a 50% sale on cheese products. Theagent does not know whether either of the sales is actually happening but estimates the probability of having a sale at storeA to be 40% and the probability of having a sale at store B to be 80%. This modified version of the problem correspondsto the problem of planning with missing information, in which the underlying state now includes two additional booleanvariables, each indicating whether there is a sale at the corresponding store. Let us denote these variables by Sale A andSaleB . We will use Sale A = 1 (0) to mean that store A has (doesn’t have) a sale on wine and similar notation for SaleB . Theproblem differs from the original deterministic planning because the agent does not know the status of variables Sale A andSaleB until it visits stores A and B respectively. At the same time, the problem is related but much narrower than planningfor Partially Observable Markov Decision Problems (POMDPs) due to the following two assumptions we make:• Perfect sensing. There is no noise in sensing: once the agent visits the store, it knows whether there is a sale or not.• Deterministic actions. There is no uncertainty in the outcomes of actions if values of hidden variables are known:the agent moves deterministically and the cost of each purchase is known if the status of Sale A and SaleB variables isknown.Despite these restrictions, we explain the problem of planning with missing information using the notation and termi-nology of POMDPs, similar to how it was done in [5].Belief MDP formulation. In POMDPs, an agent does not know its full state. Instead, it has a probability distributionover the possible states it can be in. A belief state is any particular value of this distribution (the dimensionality ofthe distribution can be as high as N − 1, where N is the number of possible states in the original graph G). Let usdenote belief states by X . For instance, in the grocery shopping example, the initial belief state of the agent, Xstart, is:{Agent = Home, Bought = ∅, P (Sale A = 1) = 0.4, P (SaleB = 1) = 0.8}. Note that this concisely represents a probability distri-bution over all possible states since the position of the agent and what items it has already purchased is always known andthe probability distribution over possible store sales can be represented as the probability of a sale at each store assumingstore sales are independent events.Fig. 3(b) shows the corresponding belief state-space. The notation Sale A = u and SaleB = u represents P (Sale A = 1) = 0.4and P (SaleB = 1) = 0.8 respectively. The notation Sale A = 0 (Sale A = 1), on the other hand, represents the knowledge of theagent that there is no sale (there is sale) at store A. In other words, Sale A = 0 is equivalent to the belief P (Sale A = 1) = 0.Similar notation is used for SaleB .The belief state-space is a Markov Decision Process (MDP). After every action, the agent knows precisely the belief stateit is in. Some actions, however, are probabilistic. The outcomes of these actions depend on the actual status of the corre-sponding hidden variables. Essentially, these are the actions that incorporate sensing of hidden variables. The probabilityof the outcomes of these actions thus follows the probability distribution over the hidden variables that are being sensed.Because we assume that sensing is perfect, the subgraphs that result from different outcomes of a single stochastic actionare disjoint. This also implies that an optimal policy in such a belief state-space is acyclic.Let us now explicitly split X into two sets of (discrete and of finite-range) variables, S( X) and H( X): X = [S( X); H( X)].• S( X) is the set of variables whose values are always observed. These are the variables that define the state in theoriginal graph G (i.e., Fig. 2). S( X) can also be thought of as the projection of X onto the state-space defined by thecompletely observed variables.• H( X) is the set of hidden variables that represent the missing information about the environment.So, in our example, S( X) is the location of the agent and what the agent has purchased so far, and H( X) is the status ofvariables Sale A and SaleB . We will use hi( X) to denote an ith variable in H( X).The goal of the planner is to construct a policy that reaches any state X such that S( X) = S goal (i.e., {Agent =Home, Bought = wine, cheese}) while minimizing the expected cost of execution. Fig. 3(c) shows this policy for our example.A full policy is more than a single path since some actions are probabilistic and a full policy dictates to the agent what todo at any state it may end up in during execution. According to the policy, the agent will visit store A, and then, if there isno sale at store A, it will go to store B, and it may even return to store A again, if there is no sale at store B either. This isvery different from a single path found in case of planning with complete information (Fig. 2(c)).Finding a policy of good quality is difficult for two reasons: first, a belief state-space is probabilistic and thereforedeterministic planners such as A* do not typically apply; second, and perhaps more importantly, the size of a belief state-space is exponential in the number of hidden variables. More specifically, given that X = [S( X), H( X)], the size of a beliefstate-space is roughly the number of states in the original graph G times the number of possible beliefs over hiddenvariables. In our example, the latter is 32 since there are three possible beliefs—unknown, 1 and 0—about each of the twohidden variables (i.e., Sale A and SaleB ).M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721701Fig. 3. An example of planning with missing (incomplete) information.702M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–7214. Clear preferences on missing informationIn this section we introduce the notion of clear preferences. This notion is central to the idea behind PPCP. We first,however, define several other notations and assumptions. For the sake of simplicity, the notation hi( X) = u at state X willrepresent the fact that the value of hi is unknown at X . If hi( X) (cid:8)= u, on the other hand, then the actual value of hi is knownat X (since sensing is perfect).Assuming at most one hidden variable per action. We assume that any time there is an action with uncertainty inoutcomes or uncertainty in costs (i.e., an action that involves sensing a hidden variable), the uncertainty is due to a singlehidden variable. Thus, the execution of any single action can result in deducing the value of at most one hidden variable.(In some domains, in case two or more factors control the outcome of an action, they can be combined into a single hiddenvariable.) We do allow, however, for a single hidden variable to control more than one action though. For example, in ourexample above, there could be a single hidden variable Sale, which, if 1, would imply a sale at both stores.Denoting hidden variables. We use h S( X),a to represent the hidden variable that controls the outcomes and costs ofaction a taken at S( X). By h S( X),a = null we denote the case when there was never any uncertainty about the outcomeof action a taken at state X . In other words, none of the variables in H control the outcomes of a executed at S( X),and since the underlying environment is deterministic, there is only one outcome. Thus, in the above example, the ac-tion a of moving from store A to home has hAgent=StoreA,Bought=wine,cheese,a=move = null since its outcome does not dependon the values of any of the hidden variables. The action a of moving to store A from home, on the other hand, hashAgent=Home,Bought=∅,a=movetostoreA = u since once the agent enters the store A it finds out whether there is a sale or not.Therefore, the action a executed at Xstart = [Agent = Home, Bought = ∅, Sale A = u, SaleB = u] has two possible outcomes:X1 = [Agent = StoreA, Bought = ∅, Sale A = 1, SaleB = u] and X3 = [Agent = StoreA, Bought = ∅, Sale A = 0, SaleB = u].Denoting successors. Sometimes, we will also need to refer to the set of successors in the belief state-space. In thesecases we will use the notation succ( X, a) to denote the set of belief states Y such that S(Y ) ∈ succ(S( X), a) and H(Y ) isthe same as H( X) except for h(S( X),a)(Y ) which becomes known if it was unknown at X and remains the same otherwise.The function P X,a(Y ), the probability distribution of outcomes of action a executed at state X , follows the probabilitydistribution of h S( X),a, P (h S( X),a). As mentioned above, once action a was executed at state X the actual value of h S( X),acan be deduced since we assumed the sensing is perfect and the environment is deterministic. Thus, in our example, forXstart = [Agent = Home, Bought = ∅, Sale A = u, SaleB = u], a = movetostoreA, and X1 = [Agent = StoreA, Bought = ∅, Sale A =1, SaleB = u], P Xstart,a( X1) = 0.4.Assuming independence of hidden variables. PPCP also assumes that the variables in H can be considered independentof each other. In other words, the sale event at store A is independent of the sale event at store B.Assuming clear preferences. The main assumption PPCP makes is that clear preferences on the values of the hiddenvariables are available. It requires that for each variable hi ∈ H , it is given its preferred value, denoted by b (i.e., best). Thisvalue is defined as follows.Definition 1. A clearly preferred value b for a hidden variable h S( X),a is such a value that given any state X and any action(cid:6)) = b anda such that h S( X),a is not known (that is, h S( X),a( X) = u), there exists a successor state Xsatisfies the following:(cid:2)which has h S( X),a( X(cid:6)X(cid:6) = argminY ∈succ( X,a)c(cid:3)S( X), a, S(Y )∗+ v(Y )where v∗(Y ) is the expected cost of executing an optimal policy at state Y .We will use the notation succ( X, a)b (i.e., the best successor) to denote the state Xh S( X),a( X) = u and whose h S( X),a( Xif h S( X),a( X) (cid:8)= u. There were simply no other outcomes of action a executed at X .)(cid:6)) = h S( X),a( X) otherwise. (In the latter case it may even be possible that h S( X),a( X(cid:6)whose h S( X),a( X(cid:6)) = b if(cid:6)) (cid:8)= bOur grocery shopping example satisfies this property since for each sensing action there always exist two outcomes: asale is present or not and the former one is the preferred outcome. Thus, as shown in Fig. 4, for action a = movetostoreAexecuted at Xstart = [Agent = Home, Bought = ∅, Sale A = u, SaleB = u], the preferred outcome succ( X, a)b is X1 = [Agent =Fig. 4. An example of clear preferences: it is always better (or at least no worse) to have a sale.M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721703StoreA, Bought = ∅, Sale A = 1, SaleB = u]. It is trivial to show this. If X3 is the second outcome of action a (the one that∗( X3) since X1 and X3corresponds to Sale A = 0 outcome), then c(S( Xstart), a, S( X1)) = c(S( Xstart), a, S( X3)), and vare exactly the same states with the only difference that X1 has a sale event at store A and X3 does not.∗( X1) (cid:3) vWe believe that there is a wide range of problems that exhibit clear preferences. People can often predict the outcomesthat optimize their costs-to-goal. For example, when planning a car route, a person would clearly prefer for any singleroad to be free of traffic. Similarly, when choosing a sequence of connecting flights, a person would clearly prefer for theweather to be a good flying weather and for the planes to have no delays. These preferences can be determined withoutcomputing optimal values. One of the frequent reasons for this is the fact that a particular value of a hidden variable doesnot commit the agent to any particular action. The agent is still free to choose any route or any sequence of planes. Theonly requirement is that an optimal plan in case of no traffic, a good weather, and no plane delays can not be worse thanthe corresponding optimal plan in case of a traffic, bad weather, and plane delays.Obviously, there is also a wide range of problems that do not exhibit clear preferences. For example, in case of a longflight delay, there may be a non-zero probability that the air carrier will provide customers with some sort of compensation(i.e., first-class upgrade or a free ticket). In this case, it may be less clear to a person whether he/she prefers to have aflight on-time or delayed. The actual preference depends on other penalties that will be incurred by the person if theflight is delayed (e.g., missing connecting flights, dinners, etc.) and can therefore be computed only after the expected costsof optimal plans for both outcomes are computed. Apart from the compensation and other non-obvious factors though,typically, it is clearly preferred to have a non-delayed flight: a person flying a non-delayed flight has always a freedom tostay at the arrival airport for the length of the possible delay if it is more optimal to leave the airport later. Clear preferencesjust capture the fact that the cost of an optimal plan with a non-delayed flight can not be worse than the cost of an optimalplan with a delayed flight. These preferences are often known without computing the costs of actual optimal plans.5. The PPCP algorithmThe explanation of the algorithm can be split into two steps. In the first section we present a planner that constructs aprovably optimal policy using a series of A*-like searches in the full belief state-space. It gains efficiency because each searchis a deterministic search and can use heuristics to focus its efforts. Nevertheless, each search is still very expensive sincethe size of the belief state-space is exponential in the number of hidden variables. In the following section we show howthe planner can be extended to use a series of searches in the underlying environment instead. These resulting searches areexponentially faster and are independent of the number of hidden variables. The overall solution though, is guaranteed tobe optimal only under certain conditions.5.1. Optimal planning via repeated searchesThe algorithm works by executing a series of deterministic searches. We will first describe how the deterministic searchworks and then we will show how the main function of the algorithm uses these searches to construct the first policy andthen refine it.The function that does the deterministic search is called ComputePath and is shown in Fig. 5. The search is done in thebelief state-space. It is very similar to (backward) A*. It also computes g-values of states, uses heuristic values to focus itssearch and performs repeated expansions of states in the order of g-value plus heuristic (known as f -values). The meaningof the g-values and the criterion that the solution minimizes, however, are somewhat different from the ones in A* search.∗( X), the minimum expected cost of reaching a goal fromthe state. These estimates are provided to the search by the function Main() of the algorithm (the estimates are always(cid:6), a) > 0 associated with it. It is calculatednon-negative). Then, every state-action pair X(cid:6), a), and the g-value of theusing action costs c(S( Xpreferred outcome state Y b = succ( X(cid:6)), a, S(Y )), value estimates v(Y ) for the outcome states Y ∈ succ( XSuppose that for each state X we have an estimate v( X) of v(cid:6), a) is defined as follows:(cid:6))) has a value Q v,g( X, a ∈ A(S( X(cid:6)(cid:6)(cid:3)), a, S(Y )+ v(Y ), c(cid:2)(cid:6)S( X), a, S(cid:2)(cid:3)(cid:3)(cid:3)(cid:3)(cid:2)Y b+ gY b(1)Q v,g( X(cid:6), a) =(cid:4)Y ∈succ( X (cid:6),a)(cid:6), a)b. Q v,g( X(cid:2)c(cid:2)S( XP X (cid:6),a(Y ) · maxTo understand the meaning of the above equation, consider first the standard definition of an undiscounted Q -value of anaction [6] (in terms of costs, rather than rewards though):Q ( X(cid:6), a) =(cid:4)P X (cid:6),a(Y ) ·(cid:2)(cid:2)cS( X(cid:6)(cid:3)), a, S(Y )(cid:3)+ v(Y )Y ∈succ( X (cid:6),a)(2)(cid:6), a) gives the expected cost of an optimal policy that starts at state XIn other words, a Q -value is the expectation of the sum of the immediate cost plus the value of an outcome over all-values),with the execution of action a. If the v-(cid:6), a) will also be an under-estimate. Now consider∗(Y ) for∗(Y ) since v-valuespossible outcomes of action a executed at state Xthen Q ( Xvalues are under-estimates of vthe definition of clear preferences (Definition 1). According to it, c(S( Xany successor Y ∈ succ( X. If v-values are perfect estimates (i.e., equal to corresponding v(cid:6)(cid:6), a). This also implies that c(S( X-values, however, then the computed Q ( X(cid:6)), a, S(Y b)) + v(Y b) (cid:3) c(S( X(cid:6)), a, S(Y b)) + v(cid:6)), a, S(Y )) + v(cid:6)), a, S(Y )) + v∗(Y b) (cid:3) c(S( X∗∗(cid:6)704M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721are under-estimates and therefore v(Y b) (cid:3) vimprove the Q -value estimate in Eq. (2) using the v-value of the preferred outcome:∗(Y b). Consequently, if our current v-values are imperfect, we can (potentially)Q v,v ( X(cid:6), a) =(cid:4)Y ∈succ( X (cid:6),a)P X (cid:6),a(Y ) · max(cid:2)(cid:2)cS( X(cid:6)(cid:3)), a, S(Y )+ v(Y ), c(cid:2)(cid:6)S( X), a, S(cid:3)(cid:3)(cid:2)Y b(cid:3)(cid:3)(cid:2)Y b+ v(3)Finally, the ComputePath function computes g-values of some states. The property that it guarantees is that any com-puted g-value is also an under-estimate of the corresponding v-value. These g-values, however, are improvements overthe previous v-values (one can think of them as state values after a series of specially ordered backups). Thus, instead ofv(Y b), the ComputePath function uses the newly computed g-value, g(Y b). This is exactly Eq. (1).∗It should now be straightforward to see that if the provided v-values are equal to the corresponding v-values and g(Y b)∗(Y b), then Eqs. (1) and (2) are identical. As a result, the plan that has the minimum expected cost of reachingis equal to vthe goal would then be given by a simple strategy of always picking an action with the smallest Q v,g( X, a) at any currentstate X .∗∗-values at first. Nevertheless, the search com--values as the solution to the following fixpoint equation:∗In reality, v-values may not necessarily be equal to the corresponding vputes g-values based on Q v,g -values. In particular, let us define g(cid:5)∗g( X) =0mina∈ A(S( X)) Q v,g∗ ( X, a) otherwiseif S( X) = Sgoal(4)∗These g-values are the expected costs of optimal plans under the assumption that the v-values of the non-preferredoutcomes are also the expected costs of optimal plans (in other words, the v-values of non-preferred outcomes are assumedto be perfect estimates). The g-values computed by the search in the ComputePath function are estimates of these g-values.∗In fact, it can be shown that the g-values of the states expanded by the search are exactly equal to the corresponding g-values (Theorem 3).Also, because of the max operator in Eq. (1) and the fact that all costs are positive, finite Q v,g∗ ( Xstrictly larger than g∗-values along optimal (finite-cost) paths are monotonically decreasing: gg(cid:6) = arg mina∈ A(S( X (cid:6))) Q v,g∗ ( Xag-values, and sets the g-values of relevant states to their corresponding g(cid:6), a) is always(cid:6), a)b), independently of whether v-values are correct estimates or not. This means that(cid:6))b), where∗( X(cid:6), a). This monotonicity allows us to perform a deterministic (A*-like) search which computes(cid:6)) = Q v,g∗ ( X∗(succ( X∗(succ( X(cid:6)) > g-values.(cid:6), a(cid:6), a∗∗The ComputePath function, shown in Fig. 5, searches backwards in the belief state-space from goal states towards thestate Xp on which it was called. (It is important to remember that the number of goal states in the belief state-space isexponential in the number of hidden variables, since a goal state is any state X whose S( X) = S goal.) The trajectory theInitially, v-values of states need to be non-negative and smaller than or equal to the costs of least-cost trajectories to a goal under the assumption that allhidden variables are set to their clearly preferred values.1 procedure ComputePath( Xp)2 g( Xp) = ∞, OPEN = ∅;3 for every H whose every element hi satisfies:(cid:7)(cid:6)hi = b)hi ( Xp) = u] OR [hi = hi ( Xp)[(hi = uX = [S goal; H];g( X) = 0, besta( X) = null;insert X into OPEN with g( X) + heur( Xp, X);(cid:6)))(cid:6)) + heur( Xp, Xremove X with smallest g( X) + heur( Xp, X) from OPEN;(cid:6)for each action a and state Xs.t. X ∈ succ( X(cid:6), a)(cid:7)hi ( Xp) (cid:8)= u](cid:6), a) according to formula 1;(cid:6)yet or g( X(cid:6)) = a;(cid:6), a), besta( X(cid:6)in OPEN with the priority g( X(cid:6)) > Q v,g ( X(cid:6), a)(cid:6)) + heur( Xp, X(cid:6));4567 while(g( Xp) > min X (cid:6)∈OPEN g( X89compute Q v,g ( X1011if this search hasn’t seen X(cid:6)) = Q v,g ( Xg( X12insert/update X1314 procedure UpdateMDP( Xpivot)15 X = Xpivot;16 while (S( X) (cid:8)= S goal)v( X) = g( X);17X = succ( X, besta( X))b ;1819 procedure Main()20 Xpivot = Xstart;21 while ( Xpivot! = null)2223 UpdateMDP( Xpivot);24ComputePath( Xpivot);find state X on the current policy such that S( X) (cid:8)= Sgoal and it hasv( X) < E X (cid:6)∈succ( X,besta( X))c(S( X), besta( X), S( Xif found set Xpivot to X ;otherwise set Xpivot to null;(cid:6))) + v( X(cid:6));2526Fig. 5. Optimal planning via repeated searches.M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721705search returns uses only the transitions that correspond to either deterministic actions or preferred outcomes of stochasticactions. This is implemented by starting off the search with all and only those goal states, whose hidden variables assumeunknown or preferred values if they are also unknown in H( Xp) and values equal to the corresponding hidden variables inH( Xp) otherwise (lines 3–6). The first time ComputePath is called, Xp is Xstart and therefore all the hidden variables areunknown. For the subsequent calls, however, Xp can be a different state, and the values of some of its hidden variables canbe known.Just like (backward) A* search, the ComputePath function expands states in the order of g plus heuristic and duringeach expansion (lines 8–13) updates the g-value and besta pointer of each predecessor state of the expanded state. (TheComputePath function does not retain g-values in between searches. The states that are encountered for the first time withina specific call to the function have their g-values reset anyway on lines 11–12.) It differs from A* though in that g-values arecomputed according to formula 1. In fact, it is the computation of this formula that requires the ComputePath function tosearch backwards (and not forwards). Heuristics are used to focus the search. Since the search is backward, they estimate thecost of following a least-cost trajectory from Xp to state in question. In the pseudocode, a user-provided function heur( Xp, X)returns a heuristic value for state X . These values need to be consistent in the following sense: heur( Xp, Xp) = 0 and forevery other state X and action a ∈ A(S( X)), heur( Xp, X) + c(S( X), a, S(succ( X, a)b)) (cid:2) heur( Xp, succ( X, a)b). This reduces tonormal consistency requirement on heuristics used by a backward A* if the state-space is fully deterministic, that is, noinformation is missing at the time of planning (Section 2). For instance, for our grocery shopping example it could be thesame heuristics as the one used for planning with complete information in Section 2 except that in computing the heuristicswe would use the sale prices. This is equivalent to computing heuristics under the assumption that all hidden variables areknown to have preferred values.The search finishes as soon as the g-value of Xp is no larger than the smallest priority of states in OPEN. (A min operatorover empty set is assumed to return infinity. The same assumption was made about the expectation operator on line 24.)Once the ComputePath function exits the following holds for the path from Xp to a goal state constructed by always pickingaction besta( X) at any state X and then moving into the state succ( X, besta( X))b if besta( X) has more than one outcome:∗the g-value of every state on the trajectory is equal to the g-value of the same state.The Main function of the algorithm (shown in Fig. 5) uses this fact. Starting with Xstart, it repeatedly executes searches onstates that reside on the current policy (defined by besta pointers) and whose v-values are smaller than what they shouldbe according to the v-values of the successors of the policy action (line 24). The initial v-values need to be non-negativeand smaller than or equal to the costs of least-cost trajectories to a goal under the assumption that all hidden variables areequal to b (a simple initialization to zero suffices). In particular, these values can be set to heuristics if these are admissiblewith respect to the underlying graph generated when values of all hidden variables are set to their clearly preferred values.The Main function terminates when no state on the current policy has its v-value smaller than what it should be accordingto the v-values of its successors.∗After each search, the UpdateMDP function iterates over the found path (lines 16–18) and updates the v-values of thestates on the path found by the search by setting them to their g-values (line 17). As mentioned above, these are equal to-values. (The v-values of states are retained until the termination of the algorithm.) On one hand,their corresponding gthis increases v-values and is guaranteed to correct the error between the v-values of these states and the v-values of their-values as long as v-values do notsuccessors in the policy. On the other hand, g-values. As a result, the algorithm converges, and at that time, the states on the found policy have theiroverestimate vv-values equal to their v-values and the found policy itself is optimal (the proof of this and other theorems can be foundin [7]):-values are bounded from above by v∗∗∗∗Theorem 1. The Main function in Fig. 5 terminates and at that time the expected cost of the policy defined by besta pointers is givenby v( Xstart) and is equal to the minimum expected cost of reaching a goal from Xstart.The algorithm remains correct independently of how states satisfying the condition on line 24 are selected. Typicallyhowever, out of all the states satisfying the condition, it is most beneficial to always select first the state that has thehighest probability of being reached, as it is likely to influence the cost of the policy the most. The probabilities of reachingstates on the policy can be computed in a single pass over the states on the policy starting from the start state. In addition,it is even more efficient to select the states using the following simple optimization demonstrated in Section 5.3: whenevera state X is chosen as the next Xpivot, we can backtrack from X up along the policy until we encounter the first stochastictransition (or Xstart, whichever comes first), at which point Xpivotis chosen to be the outcome of that transition, thetransition that resides on the same branch as X .5.2. Scaling up searchesEach search in the version of the algorithm just presented can be very slow because it operates in the belief state-spacewhose size is exponential in the number of hidden variables. We now describe the actual version of PPCP that addresses thisinefficiency. At a high level, the main idea is to forget the outcomes of sensing within each particular search (not within theoverall planning though!). This means that within a particular search the values of hidden variables are not being tracked—they remain the same. Consequently, each search can be performed in the original graph G (i.e., underlying environment)706M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721Initially, v-values of states need to be non-negative and smaller than or equal to the costs of least-cost trajectories to a goal under the assumption that allhidden variables are set to their clearly preferred values.1 procedure ComputePath( Xp)2 g(S( Xp)) = ∞, OPEN = ∅;3 g(S goal) = 0, besta(S goal) = null;4 insert S goal into OPEN with g(S goal) + heur(S( Xp), S goal);5 while(g(S( Xp)) > minS( X)∈OPEN g(S( X)) + heur(S( Xp), S( X)))67remove S( X) with smallest g(S( X)) + heur(S( Xp), S( X)) from OPEN;(cid:6)); H u ( Xp)], a)b)for each action a and S( Xcompute ˜Q v,g (S( Xif this search hasn’t seen S( X(cid:6))) = ˜Q v,g (S( X(cid:6)) s.t. S( X) = S(succ([S( X(cid:6)), a) according to formula 5;(cid:6)) yet or g(S( X(cid:6))) = a;(cid:6))) > ˜Q v,g (S( X(cid:6)), a), besta(S( X(cid:6)), a)g(S( Xinsert/update S( X891011(cid:6)) in OPEN with the priority g(S( X(cid:6))) + heur(S( Xp), S( X(cid:6)));v( X) = g(S( X)), v( X u ) = g(S( X)), besta( X) = besta(S( X));X = succ( X, besta( X))b ;12 procedure UpdateMDP( Xpivot)13 X = Xpivot;14 while (S( X) (cid:8)= S goal)151617 procedure Main()18 Xpivot = Xstart;19 while ( Xpivot! = null)2021 UpdateMDP( Xpivot);22ComputePath( Xpivot);find state X on the current policy such that S( X) (cid:8)= Sgoal and it hasv( X) < E X (cid:6)∈succ( X,besta( X))c(S( X), besta( X), S( Xif found set Xpivot to X ;otherwise set Xpivot to null;(cid:6))) + v( X(cid:6));2324Fig. 6. PPCP: Planning via repeated efficient searches.with costs and outcomes modified to reflect the initial settings of the hidden variables. This graph is exponentially smallerthan the full belief state-space. In other words, a search state consists of S( X) variables only and the size of the searchstate-space is therefore independent of the amount of missing information. This results in a drastic increase in efficiency ofsearches and the ability to solve much bigger problems without running out of memory. This efficiency, however, comes atthe expense of optimality, which can only be guaranteed under the conditions described later in Theorem 7. In brief, thistheorem states that optimality guarantees require that no branch of an optimal policy executes two actions that rely onthe same hidden variable whose value is a clearly preferred value. In other words, when executing optimal policy, it is notnecessary to retain information about the variables whose values were discovered to be clearly preferred values.Suppose the agent executes some action a whose outcome it is uncertain about at a belief state X . Suppose also thatthe execution puts the agent into succ( X, a)b. This means that the agent can deduce the fact that the value of the hiddenvariable h S( X),a that represents the missing information about action a is b. During each search, however, we assume thatin the future the agent will not need to execute action a or any other action whose outcome is dependent on the value ofthis hidden variable (remember that the value of a single hidden variable is allowed to control the outcomes of more thanone action). In case it does need to execute such action again, the search assumes that the value of the hidden variableis unknown again. As a result, the search does not need to remember whether h S( X),a is unknown or known to be equalto b. In fact, whenever the search needs to compute a Q v,g -value of a stochastic action, it assumes that the values of allhidden variables are unknown unless they were known to have non-preferred values in the belief state Xp, the state theComputePath function was called on. Under this assumption, the calculation of Q v,g( X)-value (Eq. (1)) becomes independentof H( X) since any hidden variable in H( X) whose value is different from the same variable in H( Xp) can only be equal tob, and these are replaced by u. Thus, each search can be done in the original graph G rather than the exponentially largerbelief state-space. The search no longer needs to keep track of H(·) part of the states, it operates directly on S(·) states.The ComputePath function, shown in Fig. 6 performs this search. While the function is called on a belief state Xp, itsearches backwards from a single state, Sgoal, towards a single state S( Xp), and the states in the search state-space consistonly of variables in S. The search assumes that each action a has only one outcome. It assumes that S( X) is an outcome(cid:6)); H u( Xp)], a)b) (line 7), where H u( Xp) is defined as H( Xp)of action a executed at S( Xbut with each hidden variable equal to b set to u. This corresponds to setting up a deterministic environment in which each(cid:6)),a in H( Xp) if itaction a executed at S( X(cid:6)),a if it is unknown in H( Xp). The heuristics need to be consistent withis known there and to the preferred value of h S( Xrespect to this environment we have just set up. The ComputePath function performs backward A* search in this state-spacewith the exception of how g-values are computed.(cid:6)) has a single outcome corresponding to the value of the hidden variable h S( X(cid:6)) if and only if S( X) = S(succ([S( XTo implement the assumption that the agent has no memory for the preferred values of the hidden variables that werepreviously observed we need to compute Q v,g -values appropriately. For any state-action pair (S( X), a), a ∈ A(S( X)) theComputePath function now computes g-values based on ˜Q v,g(S( X), a) instead. Let X u denote a belief state [S( X); H u( Xp)].We then define ˜Q v,g(S( X), a) as:M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721(cid:2)˜Q v,gS( X), a(cid:3)= Q v,g(cid:2)(cid:3), where g(cid:2)succ(cid:2)X u, a(cid:3)(cid:3)b(cid:2)(cid:2)(cid:2)succS= gX u, aX u, a(cid:3)(cid:3)(cid:3)b707(5)According to this formula, during the computation of the Q -value of action a we assume that we execute this action ata belief state X u , at which we are unaware of any hidden variables with preferred values. In the calculation of Q v,g( X u, a)(Eq. (1)) we then use v-values of the corresponding belief states. The calculation of Q v,g( X u, a) also requires the g-valueof succ( X u, a)b. The search does not compute g-values for full belief states. Instead, g(succ( X u, a)b) is substituted with˜Q v,g implements the assumption that the agent does not remember the values ofg(S(succ( X u, a)b)). This computation ofthe hidden variables whenever they are detected as b.For example, in the grocery shopping problem described in Section 3, this ComputePath function corresponds to search-ing the original graph G shown in Fig. 2. The search proceeds from Sgoal towards a state whose Agent and Bought variablesare given by S( Xp). The search is done in graph G but the prices of items are not on sale if the corresponding values ofSale A or SaleB variables are known to be 0 in H( Xp). Otherwise, the prices are assumed to be unknown and the computa-tion of the g-values is done by taking the expectation according to Eq. (5). Apart from how the g-values are computed, theComputePath functions is identical to the backward A* search performed on graph G.The Main and UpdateMDP functions (Fig. 6) operate in the exact same way as before with the exception that for eachstate X the UpdateMDP function updates, it also needs to update the corresponding belief state X u (line 15). Note that theUpdateMDP function updates an actual policy that can be executed. Therefore, the successors of besta( X) depend on thevalue of the hidden variable h S( X),a in H( X), which is not necessarily equal to the one used to set up the search environmentor the value of h S( X),a in H( X u).5.3. ExampleWe now demonstrate the operation of the PPCP algorithm as we have described it in the previous section on the problemof robot navigation in a partially-known terrain example shown in Fig. 7. At the time of planning, the robot is in cell A4and its goal is to go to cell F 4. In black are shown cells that are untraversable. There are two cells (shaded in grey) whosestatus is unknown to the robot: cell B5 and E4. For each, the probability of containing an obstacle is 0.5. In this example,we restrict the robot to move only in four compass directions. Whenever the robot attempts to enter an unknown cell, weassume the robot moves towards the cell, senses it and enters it if it is free and returns back otherwise. The cost of eachmove is 1, the cost of moving towards an unknown cell, sensing it and then returning back is 2.Fig. 7(b) shows a belief state-space for the robot navigation problem. The fully observed part of X , S( X) is the location ofthe robot, while the hidden part of X , H( X), is the status of cells E4 and B5. For example, X4 = [R = B4; E4 = u, B5 = 1],where R = B4 means that the robot is at cell B4 and E4 = u, B5 = 1 means that the status of cell E4 is still unknownFig. 7. The problem of robot navigation in a partially-known terrain.708M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721Fig. 8. An example of PPCP operation.and cell B5 is known to be blocked. The robot navigation problem exhibits clear preferences since for each sensing actionthere always exist two outcomes: a cell is blocked or unblocked and the latter one is the preferred outcome. For example,suppose the robot state is X = [R = D4; E4 = u, B5 = u] (Fig. 7(c)). Then the action a = East has two outcomes: Y 1 =[R = D4; E4 = 1, B5 = u] and Y 2 = [R = E4; E4 = 0, B5 = u]. The latter is the preferred outcome (Y 2 = succ( X, a)b), and itsatisfies the definition of clear preferences because the robot can always move from E4 back to D4 at cost of 1, implying∗(Y 2).that 1 + vFigs. 8 and 9 show how PPCP solves the problem. In the left columns of Figs. 8 and 9 we show the environment eachComputePath function sets up when executed on Xp (specified underneath the figure). Thus, when executed on Xp = [R =D4; E4 = 1, B5 = u] in Fig. 8(e), the ComputePath function assumes cell E4 is blocked whereas cell B5 is free. We also showthe heuristics (shown as h-values), the g-values and the path from S( Xp) to Sgoal (shown in grey dashed line) computed bythe search. The heuristics are Manhattan distances (summation of x and y differences) from S( Xp) to the cell in question.∗(Y 2) and therefore c(S( X), a, S(Y 1)) + v∗(Y 2) = c(S( X), a, S(Y 2)) + v∗(Y 1) = 2 + v∗(Y 1) (cid:2) 1 + v∗(Y 1) (cid:2) vIn the right column of each figure we show the current policy found by PPCP after the UpdateMDP function incorporatesthe results of the most recent search, which is shown in the left column in the same row. The states whose v-values aresmaller than what they are supposed to be according to their successors are outlined in bold. These states are candidatesfor being Xpivot in the next search iterations. As mentioned earlier, we exercise the following simple optimization in thisM. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721709Fig. 9. An example of PPCP operation (continued).example and all experiments: whenever a state X is chosen as the next Xpivot, we backtrack from X up along the policyuntil we encounter the first stochastic transition (or Xstart, whichever comes first), at which point Xpivot is chosen to bethe outcome of this transition that resides on the same branch as X . For example, in Fig. 9(d) Xpivot is chosen to be state[R = B4; E4 = u, B5 = 1] (robot is at B4, cell E4 is unknown and cell B5 is known to contain an obstacle) as a result ofbacktracking from state [R = D4; E4 = u, B5 = 1].In computing ˜Q v,g , the v-values of belief states that do not yet exist are assumed to be equal to the Manhattan distancesto the goal. These distances are also used to initialize the v-values of new belief states. The difference between the way theComputePath function computes g-values and the way A* computes them can be seen well in Fig. 8(g). There, the g-valueof cell C4 is 8, which is 1 + g(D4). The g-value of cell D4, on the other hand, is 7, despite the fact that g(E4) = 1, and710M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721the reason is that the v-value of the state [R = D4; E4 = 1, B5 = u] that corresponds to the “bad” outcome of going eastat cell D4 is 10. The v-value of this state has just been updated in the previous iteration (Fig. 8(f)). When computing ˜Q v,gaccording to Eq. (5), the ComputePath function accounts for this v-value. As a result, the path the ComputePath functioncomes up in this iteration is going through cell B5, which is different from the path that would have been produced byrunning normal A* assuming all cells free. In fact, the latter path is exactly the path computed by the ComputePath functionin its first iteration (Fig. 8(c)).Fig. 9(h) shows the policy that PPCP returns after it converges. In general, the expected cost of the found policy isbounded from above by the cost of the policy in which the robot always forgets the outcome of sensing if it was a preferredoutcome. If an optimal policy does not require remembering preferred outcomes, then the policy returned by PPCP is alsoguaranteed to be optimal. In our example, this memoryless property would mean that during each search, the plannerassumes that as soon as the robot successfully enters cell E4, for example, and therefore finds out that is free, it resets backthe status of cell E4 to an unknown cell. (The non-preferred outcomes of sensing are never reset, and so if the robot findsthat the cell E4 is blocked, then this information is always used.)While in the example, an optimal plan does not need to remember the status of any cell the robot has successfullyentered, it is possible to set up an environment when it will be sub-optimal. An optimal policy for such environment wouldrequire the robot to sense a cell but then come back from it, and use the fact that the cell is free at a later time. InSection 6.1 we will discuss how this memoryless assumption of PPCP can be relaxed.Independently of whether the memoryless property is satisfied or not however, the algorithm is guaranteed to convergein a finite amount of time. It is important to emphasize that forgetting the best outcomes of hidden variables is onlyhappening at the level of each search iteration (i.e., execution of the ComputePath function). The Main function constructs apolicy in which none of the states forget anything. Thus, in Fig. 6, the Main function calls UpdateMDP function that updatesthe policy in the full belief state-space using the path found by each search iteration. This UpdateMDP function iterates overthe found path while retaining the values of the hidden variables, both preferred and non-preferred (lines 15–16, Fig. 6). Thiscan also be seen in the Figs. 8–9 example, where the constructed policy does not forget the preferred outcomes. Instead,forgetting these preferred outcomes happens only at the level of each search iteration.5.4. Theoretical propertiesWe now present some of the theorems about the algorithm. They are supposed to give a sense as to why the algorithmconverges and what it converges to. All the theorems together with their proofs can be found in [7].The first few theorems relate the properties of the ComputePath function to the properties of the (backward) A* search.For example, the following theorem, perhaps unsurprisingly, states that each execution of the ComputePath function expandseach state at most once, the same guarantee A* makes. (Here and in the following when comparing with A* we assume theheuristics are consistent.)Theorem 2. No state is expanded more than once during a single execution of the ComputePath function.The statement of the next theorem is once again very similar to the property (backward) A* maintains: every state withf -value (which is the summation of g-value and heuristics) smaller than or equal to the smallest f -value of the states inOPEN has its g-value equal to its goal distance, the cost of a shortest path from the state to the goal. While A* computesthe cost of a shortest path, the ComputePath function computes the cost of a path which takes into account v-values of-values defined in Eq. (4)“bad” outcomes. To put it formally, let us re-define gfor states X in the full belief state-space). For given v-values and a given pivot state Xp, g-values are the solution to thefollowing fixpoint equation:(cid:3)S( X)-values for states S( X) (as opposed to gif S( X) = Sgoal(6)=(cid:5)g(cid:2)∗∗∗∗0mina∈ A(S( X))˜Q v,g∗ (S( X), a) otherwiseThese g∗them.-values are goal distances for the ComputePath function, and the g-values of the computed states are equal toTheorem 3. Assuming function v is non-negative, at line 5 in Fig. 6, for any state S( X) with (heur(S( X)) < ∞ ∧ g(S( X)) +heur(S( X)) (cid:3) g(S( X(cid:6)) ∈ OPEN), it holds that g(S( X)) = g(cid:6))) + heur(S( X∗(S( X)).(cid:6))) ∀S( XGiven the terminating condition of the while loop in ComputePath and the fact that h(S( Xp)) = 0 since heuristics are∗(S( Xp)). Also, same as in A*, the states on the foundconsistent, it is clear that after ComputePath terminates g(S( Xp)) = gpath all have g- plus heur-values smaller than or equal to the g-value of the goal of the search (S( Xp)). Therefore, they-values according to Theorem 3. The proof of the nextall are going to have their g-values equal to their corresponding gtheorem uses this fact, since UpdateMDP sets v-values of the states on the found path to their g-values. This theoremshows that updating the v-values of states on the trajectory found by ComputePath makes states at least as consistent asthe v-values of the policy successors. In other words, the update removes negative Bellman errors on the path returned bythe ComputePath function.∗M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721711Theorem 4. Suppose that before the ComputePath function is executed, for every state X it is true that 0 (cid:3) v( X) (cid:3) v( X u). Then afterthe UpdateMDP returns, it holds that for each state X whose v-value was just updated by UpdateMDP function it holds that v( X) = 0if S( X) = Sgoal and v( X) (cid:2) E X (cid:6)∈succ( X,besta( X))(c(S( X), besta( X), S( X(cid:6))) otherwise.(cid:6))) + v( XFor the purpose of the following theorem let us also define v u -values which are somewhat similar to v-values. v u( X)is the minimum expected cost of a policy under which the preferred values of hidden variables are forgotten as soon asthey are observed. v u -values are upper bounds on the expected cost of the policy PPCP returns.∗(cid:5)v u( X) =0mina∈ A(S( X)) Q v u ,v u ( X u, a) otherwiseif S( X) = SgoalThe next theorem says that v-values after each iteration can never decrease. The reason is that each ComputePathfunction computes the lowest possible g-values, namely g-values, for the states on the found path and then the UpdateMDPfunction sets v-values to them. Assuming properly initialized v-values, the proof that after each iteration v-values can onlyincrease can be done by induction on executions of the UpdateMDP functions. The next theorem also proves that v-valuesare bounded from above by the corresponding v u -values. The proof of this statement can also be done by induction based-values the ComputePath function sets g-values to can never be larger than v u -values as long v-on the fact that the gvalues were not exceeding corresponding v u -values before the ComputePath function was executed.∗∗Theorem 5. v-values of states are monotonically non-decreasing but are bounded from above by the corresponding v u -values.After each iteration of the algorithm the value of state Xp (and possibly others) is corrected by either changing its bestapointer or making an increase in its v-value. The number of possible actions is finite. The increases, on the other hand, arebounded from below by a positive constant because the belief state-space is finite (since we assumed perfect sensing anda finite number of possible values for each hidden variable). Therefore, the algorithm terminates. Moreover, at the time oftermination v-value of every state on the policy is no smaller than the expectation over the immediate cost plus v-valueof the successors. Therefore, the expected cost of the policy can not be larger than v( Xstart). This is summarized in thefollowing theorem.Theorem 6. PPCP terminates and at that time the cost of the policy defined by besta pointers is bounded from above by v( Xstart) whichin turn is no larger than v u( Xstart).The final theorem gives the conditions under which the policy found by PPCP is optimal. It states that the found policy. We use ρ∗( X) tois optimal if the memory about preferred outcomes is not required in an optimal policy, notated by ρ∗denote a pointer to the action dictated by policy ρ∗at the belief state X .Theorem 7. Suppose there exists a minimum expected cost policy ρ∗itholds that h S( X),ρ∗( X)( X) (cid:8)= b. Then the policy defined by besta pointers at the time PPCP terminates is also a minimum expected costpolicy.that satisfies the following condition: for every state X ∈ ρ∗The condition h S( X),ρ∗( X)( X) (cid:8)= b means that whenever an agent executes a policy action ρ∗( X) at state X , the hiddenvariable that controls the outcomes of this action is not known to have a preferred outcome or there is no hidden variablethat controls the outcomes (i.e., there was never any uncertainty about the outcome of the action). If this property holds forany action on the policy, then there is no need for the agent to retain information about the values of hidden variables ithas already observed to have clearly preferred values. Typically, this means that the knowledge about these hidden variableshas been exercised immediately and there is no need to remember them anymore. Another way of stating the memorylessproperty of PPCP is that if no branch of an optimal policy executes two actions that rely on the same hidden variable andassume its value is a clearly preferred value, then PPCP is guaranteed to find an optimal policy.6. Extensions and optimizationsIn the following first two sections we describe some useful extensions of PPCP algorithm. The first extension makes PPCPapplicable to problems for which no acceptable policy exists that forgets the preferred outcomes of sensing. The secondextension makes PPCP useable on real-time systems by making it possible to interleave planning with PPCP and execution.The last two sections describe general optimizations of the PPCP algorithm that prove to be very effective for the pathclearance problem. Both optimizations leave the theoretical properties of the algorithm such as convergence and optimalityunder certain conditions unchanged and can be considered general optimizations of PPCP.6.1. Overcoming memoryless propertyEach search of PPCP assumes that the policy does not need to remember the outcomes of sensing if they are preferredoutcomes. In the Fig. 8 example, for instance, the robot senses a cell when trying to enter it. If it is free then the robot712M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721enters it and does not really need to remember that it is free afterwards, its future path does not involve entering the samecell again. It only needs to remember if some cells turn out to be blocked, and policies generated by PPCP do retain thisinformation.There are many problems, however, that do require remembering preferred outcomes, and there are at least severalsimple approaches to relaxing the memoryless property of PPCP. Before trying them though, perhaps the first thing oneshould see is if it is enough to just make sure that sensing action is done as part of the action that requires the knowledgeof the status of the hidden variable. This is essentially how we set up the robot navigation problem in Fig. 8. The robotsenses a cell and then enters it or backs up in a single action. Therefore, the preferred outcome of sensing is used in thesame action as sensing itself and does not need to be remembered.A slightly more complex solution to this problem is to augment each state S( X) with the last k preferred outcomes ofsensing. During each search done by the ComputePath function, each sensing operation results in a preferred outcome, asbefore, but now the corresponding hidden variable is pushed onto the queue of maximum size k. Thereafter, sensing doesnot need to be done for the value of this hidden variable as long as it remains in the queue (i.e., does not get pushed outby the new results of sensing). For example, in the robot navigation problem S( X) will now consist of the location of therobot plus the last k locations that were sensed by the robot. The addition of k last preferred outcomes of sensing makeseach execution of the ComputePath function more expensive, however. In particular, the size of the state-space on whichthe search is done now grows roughly by a factor of |H|k, where |H| is the number of hidden variables. Therefore, k shouldbe set to a small number such as two or three to keep the complexity of each search low. This approach is good for theproblems in which the results of most recent sensing are more likely to be useful.If the addition of k last preferred outcomes of sensing is still not sufficient, one can also just split the vector of hiddenvariables H( X) into two sets: H( X) and H( X). The first one can be hidden variables whose preferred outcomes the Com-putePath function does keep track of, same as in the pseudocode of the ComputePath function in Fig. 5. The second set ofhidden variables, H( X), are the ones whose preferred outcomes the ComputePath function does not retain, same as in thepseudocode of the ComputePath function in Fig. 6. The version of PPCP that uses this approach generalizes PPCP given inSections 5.1 and 5.2. The pseudocode of this generalized version is given in [7]. (In fact, this is the version all of the proofsare derived for.) The approach of splitting H( X) into two sets is suitable for the problems in which one can have a goodidea about which preferred outcomes are likely to be useful later and which are not.6.2. Interleaving planning and executionIn many cases we would like to be able to interleave planning with execution. The agent can then start executingwhatever current plan it has and while executing it, a planner can work on improving the plan. This way the agent doesnot need to wait for the planner to fully converge.Interleaving planning with PPCP with execution can be done as follows. The agent first executes PPCP for several seconds.The loop in the Main function of PPCP is then suspended (right after the UpdateMDP function returns on line 21, Fig. 6),and the agent starts following the policy currently found by PPCP as given by besta pointers. During each agent move, Xstartstate maintained by PPCP is updated to the current state of the agent and the main loop of PPCP is resumed for anotherfew seconds. After it is suspended again, the policy that the agent currently follows is compared against the policy thatPPCP currently has and is updated to it only if the latter has a higher probability of reaching a goal location. If the policythe agent currently follows has a higher probability of reaching the goal then the agent continues to follow it. This way weavoid changing policies every time PPCP decides to explore a different policy but has not explored much of the outcomeson it yet. The probabilities of reaching a goal for an acyclic policy can be computed in a single pass over the states on thepolicy in their topological order starting with the start state.Once PPCP converges to a final policy, the agent can follow the policy without re-executing PPCP again unless the agentdeviates from its path due to actuation errors. If the agent does deviate significantly from the plan generated by PPCP, thenPPCP can be used to re-plan. There is no need to re-plan from scratch. Instead, Xstart is updated to the new state of theagent, the old policy is discarded, and the main loop of PPCP is resumed. Note that the values of all state variables in PPCPare preserved. It will then automatically re-use them to find a new policy from the current agent position much faster thanif PPCP was re-executed from scratch.6.3. Reducing the number of search iterationsAs mentioned previously, during each search whenever the ComputePath function encounters action a executed at stateS( X) and the outcome is not known according to H( Xp), then in evaluating Eq. (1), the ComputePath function uses thev-values of non-preferred outcomes. The v-values are estimates of the goal distances. If these non-preferred outcomes havenever been explored by PPCP, then the v-values are initial estimates of the cost-to-goal from them and are likely to be muchlower than what they should really be. This means that the ComputePath function will return a path that uses the state-action pair (S( X), a), and only in the future iterations will PPCP find out that the v-value of these non-preferred outcomesshould really be higher and this state-action pair should have been avoided.Fig. 10 gives such example for the robot navigation in a partially-known environment problem. When solving the en-vironment in Fig. 10(a), PPCP at some point invokes the ComputePath function on state Xp = [R = A4; C 4 = 1, C 6 = u].M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721713Fig. 10. The comparison of a search by PPCP (b) without the optimization vs. (c) with the optimization described in Section 6.3.Suppose that by this time PPCP has already computed the v-value of state [R = B6; C 4 = 1, C 6 = 1] as 12. This is thecost of getting to the goal from cell B6 if both cells C4 and C6 are blocked. During the current search, when comput-ing the g-value of cell C5, PPCP will query the v-value of state [R = C 5; C 4 = 1, C 6 = 1] as needed by Eq. 1. If thev-value of this state has never been computed previously, PPCP initializes it to some admissible estimate such as Man-hattan distance from C5 to the goal cell, which is 4 (Fig. 10(b)). After evaluating Eq. (1), the g-value of cell C5 becomes 6(= 0.5 max(2 + 4, 1 + 5) + 0.5 max(1 + 5, 1 + 5)). Consequently, the search returns the path shown in Fig. 10(b) that goesthrough cells C5 and C6.One optimization we propose is to use the v-values of neighboring states to obtain more informative v-values of statesthat have not been explored yet. Thus, in the example, we can deduce that the v-value of state [R = C 5; C 4 = 1, C 6 = 1] canbe at least 10: the v-value of state [R = B6; C 4 = 1, C 6 = 1], which is 12, minus an upper bound on the minimum cost ofgetting from [R = B6; C 4 = 1, C 6 = 1] to state [R = C 5; C 4 = 1, C 6 = 1], which we can easily compute as 2. More formally,suppose we are interested in estimating the v-value of some state X . We can then take some (small) region R of statesaround X whose H(·) part is the same as in H( X). Using each state Y ∈ R and an upper bound cu(Y , X) on getting fromstate Y to state X , we can then estimate v( X) as:v( X) = maxY ∈R(cid:2)(cid:3)v(Y ) − cu(Y , X)(7)The upper bounds, cu(·, X) can be computed via a single backward Depth-First Search from X . In some problems theycan also be obtained a priori. To see that Eq. (7) is a valid update for v( X) consider the following trivial proof that v( X)remains admissible (does not overestimate the minimum expected cost of getting to goal) provided an admissible valuev(Y ) for each Y ∈ R. Let v∗(Y ) denote the minimum expected cost of getting to the goal from Y . Then:v(Y ) (cid:3) v∗(Y ) (cid:3) cu(Y , X) + v∗( X)∗( X) is bounded from below by v(Y ) − cu(Y , X) and by setting v( X) to it we guarantee the admissibility of v( X).Thus, vThe only change to the algorithm is that in Fig. 6 on line 15 the v-value update is now a maximum between the oldv-value and the g-value because setting the initial v-values according to Eq. (7) can now sometimes result in v-valueslarger than their estimates computed by the search (i.e., g-values). In other words, the new line 15 is now as follows:15 v( X) = maxv( X), gS( X)vX u= maxvX u(cid:2)(cid:2)(cid:2)(cid:3)(cid:2)(cid:2)(cid:3)(cid:3),(cid:2)(cid:3), g(cid:3)(cid:3),S( X)(cid:2)besta( X) = besta(cid:3)S( X)Fig. 10(c) shows the operation of the ComputePath function that uses this optimization. The g-value of cell C5 now iscomputed as 9 (= 0.5 max(2+ 10, 1+ 5)+ 0.5 max(1+ 5, 1+ 5)) because it uses the v-value of state [R = B6; C 4 = 1, C 6 = 1]to better estimate the v-value of [R = C 5; C 4 = 1, C 6 = 1] — the non-preferred outcome of moving from C5 towards C6.Consequently, the search returns a very different path and PPCP never has to explore the path through cells C5 and C6that would have been returned without the optimization. The proposed optimization can substantially cut down on theoverall number of search iterations PPCP has to do. This significantly overcomes the expense of computing better estimatesof v-values for non-preferred outcomes.6.4. Speeding up searchesPPCP repeatedly executes A*-like searches. As a result, much of the search efforts are repeated and it should be beneficialto employ the techniques such as D* [8], D* Lite [9] or Adaptive A* [10] that are known to significantly speed up repeatedA* searches. We use the last method because it guarantees not to perform more work than A* search itself and moreimportantly requires little changes to our ComputePath function.11 It would be an interesting direction for future work to investigate how the ComputePath function PPCP uses can be made incremental in the same wayD* and D* Lite extended A* to an incremental version.714M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721Fig. 11. Path clearance problem.The idea is simple and is as follows. This optimization computes more informed heuristic values, heur(S( X)), that areused to focus each search. heur(S( X)) is a heuristic value that (under) estimates a distance from S( Xp) to S( X) under theassumption that all hidden variables whose values are unknown are set to b. The heuristics need to be consistent. Initially,before any search iteration is done, we compute the start distance (the cost of a least-cost path from S( Xstart) to the statein question) of every state in S(·) assuming that execution of every stochastic action results in a preferred outcome (inother words, the search is done on the deterministic environment where the value of each hidden variable is set to b). Inthe Fig. 8 example, it means that we compute the distance from cell A4 to every other cell assuming cells E4 and B5 arefree. We can do this computation via a single Dijkstra’s search. Let us denote the computed value for each state S( X) byheur(S( Xstart), S( X)).∗The computed value heur(S( Xstart), S( X)) is a perfect estimate of the start distance in the environment where everyhidden variable is set to b. Therefore it is a good heuristic value to use when the ComputePath function is invoked withXp = Xstart. The problem, however, is that the ComputePath function is most of the time called to find a path from someother state Xp (cid:8)= Xstart. We employ the same principle as in [10] that allows us to use our heur-values anyway: for everystate S( X), its heuristic value heur(S( X)) can be improved as follows∗(cid:2)∗(cid:2)(cid:3)(cid:3)(cid:2)(cid:2)(cid:2)∗(cid:3)S( X)heur= maxheur(cid:3), heurS( X)(cid:3)S( Xstart), S( X)− heurS( Xstart), S( Xp)∗(Note that S( X) does not retain the value of heur(S( X)) from one search to another.) For the same reasoning as in [10],the updated heur(S( X)) is guaranteed not to overestimate the actual distance from S( Xp) to S( X) and to remain a consistentfunction.7. Application to path clearanceThe problem of path clearance is the problem of planning for a robot whose task is to reach its goal as quickly as possiblewithout being detected by an adversary [11,12]. The robot does not know beforehand the precise locations of adversaries,but has a list of their possible locations. When navigating, the robot can come to a possible adversary location, sense itusing its long range sensor and go around the area if an adversary is detected or cut through this area otherwise.The example in Fig. 11 demonstrates the path clearance problem. Fig. 11(b) shows the traversability map of the satelliteimage of a 3.5 km by 3 km area shown in Fig. 11(a). The traversability map is obtained by converting the image into adiscretized 2D map where each cell is of size 5 by 5 meters and can either be traversable (shown in light grey color) or not(shown in dark grey color). The robot is shown by the blue circle and its goal by the green circle.2 Red circles are possibleadversary locations and their radii represent the sensor range of adversaries (100 meters in this example). The radii canvary from one location to another. The locations can be specified either manually or automatically in places such as narrowpassages. Each location also comes with a probability of containing an adversary (50% for each location in the example): thelikelihood that the location contains an adversary. The probabilities can vary from one location to another.The path the robot follows may change any time the robot senses a possible adversary locations (the sensor range ofthe robot is 105 meters in our example). A planner, therefore, needs to reason about possible outcomes of sensing beforethe execution and to generate a policy that dictates which path the robot should take as a function of the outcome of eachsensing. Ideally, the generated policy should minimize the expected traversal distance. Finding such policy with guaranteeson its optimality, however, corresponds to planning with missing information about the environment. In fact, the pathclearance problem is very much equivalent to the problem of planning for a robot navigating in a partially-known terrain.The difference is that in the path clearance problem, detecting an adversary blocks a large area resulting in a long detour.An adversary location has also a tendency to be placed in such places that it blocks the whole path and the robot has tobackup and choose a totally different route. As a result, the detours can be much costlier than in the case of navigation ina partially-known terrain, even when the amount of uncertainty is much less.2 For colors see the web version of this article.M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721715Fig. 12. Solving path clearance problem with freespace assumption.Solving path clearance using assumptive planning. To avoid the computational complexity, a robot operating in apartially-known terrain often performs assumptive planning [8,13,14]. In particular, it often just follows a shortest pathunder the assumption that all unknown areas in the environment are free unless the robot has already sensed them oth-erwise. This is known as a freespace assumption [14]. The robot follows such path until it either reaches its goal or sensesnew information about the environment. In the latter case, the robot re-computes and starts following a new shortest pathunder the freespace assumption.The freespace assumption is also applicable to the path clearance problem. The robot can always plan a path under theassumption that no adversary is present unless sensed otherwise. This assumption makes path clearance a deterministicplanning problem and therefore can be solved efficiently. The fact that the robot ignores the uncertainty about the adver-saries, however, means that it risks having to take long detours, and the detours in the path clearance problem tend to belonger than in the problem of navigation in a partially-known terrain as we have previously explained.For example, Fig. 12(a) shows the path computed by the robot that uses the freespace assumption. According to the path,the robot tries to go through the possible adversary location A (shown in Fig. 11(b)) as it is on the shortest route to thegoal. As the robot senses the location A, however, it discovers that the adversary is present in there (the red circle becomesblack after sensing). As a result, the robot has to take a very long detour. Fig. 12(b) shows the actual path traversed by therobot before it reaches its goal.Solving path clearance using PPCP planning. Turns out that the path clearance problem can be efficiently solved usingPPCP. Same as in the robot navigation in a partially-known terrain, in the path clearance problem, there are also clearpreferences for the values of unknowns. The unknowns are m binary variables, one for each of the m possible adversarylocations. The preference for each of these variables is to have a value false: no adversary is present.Differently from our simple example in Fig. 7, however, in path clearance, the robot has a long range sensor. It thereforemay sense whether an adversary is present before actually reaching the area covered by the adversary. As a result, it needsto remember the preferred outcomes if it wants to sense adversaries from a long distance. To address this, we augmentS( X) with the last k = 3 preferred outcomes of sensing as described in Section 6.1.Fig. 13 shows the application of PPCP to the path clearance example in Fig. 11. Before the robot starts executing anypolicy, PPCP plans for five seconds. Fig. 13(a) shows the very first policy produced by PPCP (in black color). It is a single pathto the goal, which in fact is exactly the same as the path planned by planning with the freespace assumption (Fig. 12(a)).PPCP produced this path within few milliseconds in its first iteration. At the next step, PPCP refines the policy by executinga new search which determines the cost of the detour the robot has to take if the first adversary location on the foundpath contains an adversary. The result is the new policy (Fig. 13(b)). PPCP continues in this manner and at the end offive seconds allocated for planning, it generates the policy shown in Fig. 13(c). This is the policy that is passed to therobot for execution. Each fork in the policy is where the robot tries to sense an adversary and chooses the correspondingbranch.As explained in Section 6.2, we interleave planning with execution. Thus, while the robot executes the plan, PPCP im-proves it relative to the current position of the robot. Fig. 13(d) shows the new position of the robot (the robot travelsat the speed of 1 meter per second) and the current policy generated by PPCP after 15 seconds since the robot wasgiven its goal. Fig. 13(e) shows the position of the robot and the policy PPCP has generated after 30 seconds. At thispoint, PPCP has converged and no more refinement is necessary. Note how the generated policy makes the robot gothrough the area on its left since there are a number of ways to get to the goal and therefore there is a high chancethat one of them will be available. Unlike the plan generated by planning under freespace assumption, the plan gener-ated by PPCP avoids going through location A. Fig. 13(f) shows the actual path traversed by the robot. It is 4123 meterslong while the length of the trajectory traversed by the robot that plans with freespace assumption (Fig. 12(b)) is 4922meters.716M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721Fig. 13. Solving path clearance problem with PPCP.8. Experimental analysis8.1. Navigation in a partially-known terrainIn this section, we use the problem of robot navigation in unknown terrain to evaluate the performance of PPCP algo-rithm (without optimizations). In all of the experiments we used randomly generated fractal environments that are oftenused to model outdoor environments [15]. A robot was allowed to move in eight directions, and the cost of each move inbetween two traversable cells was defined as the distance between the centers of the corresponding cells times the cost oftraversing the target cell (according to its fractal value). The cost of sensing and discovering an initially unknown cell to beuntraversable was set to the cost of moving towards the cell and then moving back into the source cell.In the first set of experiments we compared the performance of PPCP with three optimal algorithms: VI (value iteration),LAO* [16], and RTDP [3]. All three can be used to plan in finite-size belief state-spaces, and the latter two have been shownto be competitive with other planners in belief state-spaces [5]. To make VI more efficient and scalable, we first performeda simple reachability analysis from the initial belief state, and then ran VI only on the reachable portion of the belief state-space. Both PPCP and LAO* used the following (admissible and consistent) heuristics to estimate distances in between anytwo states with coordinates (x1, y1) and (x2, y2):(cid:2)|x1 − x2|, | y1 − y2|(cid:2)|x1 − x2|, | y1 − y2|(cid:2)|x1 − x2|, | y1 − y2|(cid:2)max− min2 min√(cid:3)(cid:3)+(cid:3)(cid:3)The same heuristics were also used to initialize the state values when running VI and RTDP algorithms.Fig. 14(a) shows the time it takes to converge, the percent of solved environments (the environments were declaredto be unsolved when an algorithm ran for more than 15 minutes), and the solution costs for the four algorithms for theM. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721717Fig. 14. Experimental results.environments of size 17 by 17 cells. The number of unknown locations increases from 6 to 18 and for each number theresults are averaged over 25 environments.The figure shows that PPCP converges faster than the other algorithms and the differences in speeds grow large veryfast with the increase in the number of unknown locations. More importantly, PPCP was able to solve all environmentsin all cases. (We do not give numbers for VI for more than 6 unknowns and LAO* for more than 10 unknowns becausethey were running out of memory on almost all environments.3) Fig. 14(a) also shows that in all the cases the solutionreturned by PPCP turned out to be the same as the one returned by other algorithms, an optimal solution. (An interestingand potentially important by-product of these results is an implication that, at least in randomly generated environments,an optimal navigation in a partially-known environment does not really need to memorize the cells that turn out to befree.) Finally, Fig. 14(b) shows the rate of convergence (v-value of start state) of the algorithms for one of the environmentswith 6 unknowns (note the log scale of the time).Besides the algorithms we compared PPCP against, there are other efficient algorithms such as HDP [17], MCP [18],FF-replan [19] and FPG [20] that can be used to plan in finite belief state-spaces. While we have not compared theirperformance, we believe they would show the performance similar to the one exhibited by RTDP and LAO* since they allhave to perform planning in the belief state-spaces that are exponential in the number of unknowns.The second set of experiments shows that PPCP can be applied to the problem of robot navigation in environments oflarge size and with large number of unknown locations. Fig. 14(c) compares the performance of PPCP against a strategy ofplanning with freespace assumption. The comparison is done on the environments of size 500 by 500 cells with the numberof unknown locations ranging from 1000 (0.4% of overall size) to 25,000 (10%). (The size of the corresponding belief state-spaces therefore ranges from 250,000 · 31000 to 250,000 · 325,000.) Unlike in the previous experiments, in these ones therobot was moving and was given only 1 second to plan during each of its moves (for both planning with PPCP and planningwith freespace assumption). This amount of time was always sufficient for planning with freespace assumption to generatea path. The PPCP planning, however, was interleaved with execution as described in Section 6.2. In most experiments,PPCP converged to a final policy after several tens of moves. Fig. 14(c) summarizes the execution costs of two approaches3 In many domains, LAO* runs much better than VI. In our domain however, the performance of LAO* was comparable to VI and much worse than thatof RTDP. We believe that the reason for this was the fact that the heuristics were not that informative since the costs of cells were often much larger thanones. If the heuristics do not focus efforts well, then VI with a reachability analysis may even become more efficient than LAO* due to its much smalleroverhead.718M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721averaged over 25 randomly generated fractal environments for each row in the table. The results show that the cost ofthe trajectory traversed by the robot with PPCP planning is consistently smaller than the one traversed by the robot withfreespace assumption planning.8.2. Path clearanceIn this section, we study the performance of PPCP algorithm on the path clearance problem. In all of the experimentswe used the extended version of PPCP that allowed it to remember k = 3 last preferred outcomes (described in Section 6.1).In all of our experiments we again used randomly generated fractal environments to model outdoor environments. On topof these fractal environments, however, we also superimposed a number of randomly generated paths in between randomlygenerated pairs of points. The paths were meant to simulate roads through forests and valleys and that are usually presentin outdoor terrains. Figs. 15(a, b) show typical environments that were used in our experiments. The lighter colors representmore easily traversable areas. All environments were of size 500 by 500 cells, with the size of each cell being 5 by 5 meters.The test environments were split into two groups. Each group contained 25 environments. For each environment in thegroup I we set up 30 possible adversary locations at randomly chosen coordinates but in the areas that were traversable.(The size of the corresponding belief state-space is 250,000 · 330.) Fig. 15(a) shows a plan the PPCP algorithm with bothoptimizations (described in Sections 6.3 and 6.4) has generated after full convergence for one of the environments in groupI. For each environment in the group II we set up 10 possible adversary locations. (The size of the corresponding beliefstate-space is 250,000 · 310.) The coordinates of these locations, however, were chosen such as to maximize the length ofdetours. This was meant to simulate the fact that an adversary may often be set at a point that would make the robot takea long detour. In other words, an adversary is often set at a place that the robot is likely to traverse. Thus, the environmentsin group II are more challenging. Fig. 15(b) shows a typical environment from the group II together with the plan generatedby PPCP with both optimizations. The shown plan has about 95% probability of reaching the goal (in other words, the robotexecuting the policy has at most 5% chance of encountering an outcome for which the plan had not been generated yet). Incontrast to the plan in Fig. 15(a), the plan for the environment in group II is more complex—the detours are much longer—and it is therefore harder to compute. For each possible adversary location the probability of containing an adversary wasset at random to a value in between 0.1 and 0.9.We have run two sets of experiments on these environments. In the first set we compared the unoptimized PPCP al-gorithm to the PPCP algorithm with the two optimizations we have described in Sections 6.3 and 6.4. Table 1 shows theresults for the group I averaged over all of the environments in it. The algorithms were run until full convergence in orderto obtain the comparison results. According to them the number of states expanded by the unoptimized PPCP is about fivetimes more and its run-time is also close to five times longer than for the optimized PPCP. The unoptimized PPCP has alsoconverged on less environments within 15 minutes.In the second set of experiments we compared the execution cost of the robot planning with our optimized PPCP versusthe execution cost of the robot planning with freespace assumption [14]. Unlike in the previous experiments, the robot wasmoving and had 5 seconds to plan while traversing 5 meter distance. This amount of time was always sufficient for planningwith freespace assumption to generate a path. The PPCP planning, on the other hand, was interleaved with execution as wehave explained in Section 6.2.Fig. 15. The example of environments used in testing and the plans generated by PPCP for each.Table 1The comparison of unoptimized and optimized PPCP on Group I environments. The convergence times are given for the environments on which bothalgorithms converged within 15 minutes.# of expansionsTime to convergence (secs)Converged within 15 minutesUnoptimized PPCPOptimized PPCP59,759,71711,911,585281.8360.8164%92%M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721719Table 2The overhead in execution cost of navigating using planning with freespace assumption over navigating using planning with PPCP.freespacefreespace2freespace3Overhead in execution costGroup Ino penalty5.4%0.5%2.1%Group IIno penalty5.2%4.9%4.3%Group Iwith penalty35.4%4.8%0.0%Group IIwith penalty21.6%17.0%12.7%Table 2 shows the overhead in the execution cost incurred by the robot that plans with the freespace assumption overthe execution cost incurred by the robot that uses PPCP for planning. The rows freespace2 and freespace3 correspond tomaking a cost of going through a cell that belongs to a possible adversary location twice and three times higher than whatit really is, respectively. One may scale costs in this way in order to bias the paths generated by the planner with freespaceassumption away from going through possible adversary locations. The results are averaged over 8 runs for each of the 25environments in each group. For each run the true status of each adversary location was generated at random according tothe probability having an adversary in there.The figure shows that planning with PPCP results in considerable execution cost savings. The savings for group I envi-ronments were small only if biasing the freespace planner was set to 2. The problem, however, is that the biasing factor isdependent on the actual environment, the way the adversaries are set up and the sensor range of an adversary. Thus, theoverhead of planning with freespace for the group II environments is considerable across all bias factors. In the last twocolumns we have introduced penalty for discovering an adversary. It simulated the fact that the robot runs the risk of beingdetected by an adversary when it tries to sense it. In these experiments, the overhead of planning with freespace assump-tion becomes very large. Also, note that the best bias factor for freespace assumption has now shifted to 3 indicating that itdoes depend on the actual problem. Overall, the results indicate that planning with PPCP can have significant benefits anddo not require any tuning.9. Related workIn general, planning with missing (incomplete) information about the environment and with sensing is a special class ofplanning for Partially Observable Markov Decision Processes (POMDPs) [5]. As a result, theoretically, algorithms for solvingPOMDPs are also applicable to solving the problem of planning with missing information. Unfortunately, however, planningoptimally for POMDPs, in general, and planning with missing information, in particular, is known to be intractable [1,2].Various approximations techniques have been proposed instead [21–28]. For example, grid-based approaches such as [28–30] solve POMDPs by putting specialized grids over infinite belief state-spaces, thereby converting the planning probleminto solving a finite-size but usually very large MDP. Point-based approaches such as [24,26,27,31] approximate the valuefunction over the whole belief space by computing it for a relatively small set of reachable points in the belief space.Factorization-based approaches such as [21–23] use factored representation of belief states. Baral and Son have developedapproximation techniques for solving planning with missing information problems [32].A number of approaches capable of planning with missing information have also been based on the idea of using heuris-tic searches in one way or another [3,5,16–18,31,33–35]. For example, LAO* [16]—one of the algorithms that we used in ourexperiments—is an efficient combination of dynamic programming and A*-like extensions developed specifically for planningin MDPs. It has also be shown, however, to be able to find policies in the belief state-spaces [5]. MCP [18] can also efficientlyfind optimal policies by running a series of A*-like searches in the belief state-spaces with sparse stochasticity. HSVI [31]and FSVI [35] incorporate some of the ideas behind heuristic searches into the point-based approaches. MAA* [34] is analgorithm for solving finite-horizon decentralized POMDPs optimally using A*-like processing. Similarly to how we used itin our experiments, RTDP [3] can also be used to find solutions to POMDP problems by planning in belief state-spaces [5].Many of the abovementioned algorithms are capable of solving general POMDP problems. It is important to realizehowever, that the problem we are addressing in this paper is a much narrower (and simpler) than solving a general POMDP.For one, we assume that the underlying problem is deterministic and there is only uncertainty about some actions due tomissing information about the environment. We also assume sensing is perfect which entails a finite size belief state-spacewith an acyclic optimal policy. Most importantly, however, we concentrate on the class of problems for which there are clearpreferences on the missing information. The most relevant to our work, perhaps, is the algorithm in [36], developed for theproblem of robot navigation in a partially-known terrain. Similarly to our definition of clear preferences, their planner hastaken advantage of the idea that the cost of the plan if a cell is free can not be larger than the cost of the plan if the cellis occupied. Based on this idea, they proposed a clever planner that is capable of finding optimal policies much faster thanother optimal approaches.The goal of our work, however, is to avoid dealing with the exponentially large belief state-spaces altogether, which isrequired to guarantee the optimality of the solution. This allows us to solve very efficiently and without running out ofmemory large environments with a large amount of missing information. The cost is the solution optimality guarantee,which can only be made under certain conditions.720M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–72110. Discussion and future workBesides its efficiency and low memory requirements, the other important advantages of the PPCP algorithm in our opin-ion are its simplicity and ease of implementation. PPCP is easy to implement because it is really just running a series of A*searches on the instances of underlying problem, each of which is made deterministic by making the necessary assumptionsabout the pieces of missing information. For example, in the path clearance problem, PPCP reduced to running a seriesof A* searches (with the exception of how g-values are computed) to find paths in the environments. Each environmenthad some adversaries present and some not, as specified in Xp. Therefore, the implementation of the algorithm was rathertrivial.The main disadvantage of PPCP is that it can only provide optimality guarantees under certain conditions (as describedin Section 5.4). It is our hope, however, that it might be possible to derive general bounds on the sub-optimality of thesolutions returned by PPCP for the cases when these conditions are not satisfied. Interestingly, in our experiments all of thesolutions returned by PPCP were optimal when compared on the environments small enough to be solved by algorithmsthat can find provably optimal solutions.Experimentally, PPCP works also for problems in which clear preferences are not so clear. That is, even though a partic-ular outcome of sensing is thought to be a preferred outcome, it does not satisfy Definition 1. In our opinion, it would bevaluable to analyze the behavior of PPCP for such problems from a theoretical side. In particular, it would be interesting toderive a function that relates the sub-optimality of PPCP to how much the clear preferences are not satisfied.Finally, in this paper we concentrated on the notion of clear preferences on the missing information. There are othercommon sources of uncertainty, however. One direction for future research is therefore to explore whether the notion ofclear preferences can be extended to cover other types of uncertainty such as sensor noise and uncertainty in actuation.For instance, in the latter case, one can also sometimes name the preferred outcomes of actions. Thus, a robot movingalong a cliff clearly prefers not to slip. Sometimes, these preferences are clear and sometimes they can be “nearly” clear(i.e., sometimes a slip outcome may turn out to be a good outcome at the end). In either case, however, it would be in-teresting to investigate whether clear preferences could be assumed and used to construct a planner capable of dealingin real-time with large-scale problems exhibiting both the uncertainty in actuation and the uncertainty in the environ-ment.11. ConclusionsMost of us are not very good in planning under uncertainty. When faced with such a task, we never try to derive a planthat minimizes the expected cost. Instead, we will typically reason only about few contingencies and assume that in all theother cases the fortune will look upon us. The key to being able to do this, however, is the fact that we usually know (orassume) ahead of time what is good for us.One of the goals of this paper was to formally define this notion of clear preferences on missing information aboutthe environment. A second goal of the paper was to show how the existence of these clear preferences can be used toconstruct an efficient planner PPCP. By making use of these preferences, PPCP solves the planning problem by running aseries of deterministic A*-like searches in the space of the original (deterministic) planning problem (and not in the beliefstate-space that is exponential in the number of unknowns). The complexity of each of these searches is the same as thecomplexity of planning after making some assumptions about all of the unknowns, which is a common way to make real-time planning possible. This makes PPCP highly efficient and scalable to large-scale planning problems with large amountsof uncertainty.In our theoretical analysis, we have shown that once converged, the plan returned by PPCP is guaranteed to be optimalunder certain conditions. In our experimental analysis, we have shown that PPCP can be successfully used for planning inpartially-known terrains and for solving the path clearance problem, both important problems in robotics. For both prob-lems, PPCP could scale to much larger environments and with much more uncertainty than previously possible. We are alsocurrently working on applying PPCP to several other planning problems in robotics including navigation under uncertaintyin the position of moving objects such as humans and planning an autonomous landing for unmanned helicopters underuncertainty in the safety of multiple landing sites. We therefore hope that this paper will stimulate more research on thenotion of clear preferences on uncertainty, will make available to others an efficient algorithm for probabilistic planning withmissing information, and finally, will encourage a wider use of planning under uncertainty for real-time robots operating inlarge-scale environments.AcknowledgementsThis work was sponsored by the U.S. Army Research Laboratory, under contract Robotics Collaborative Technology Alliance(contract number DAAD19-01-2-0012). The views and conclusions contained in this document are those of the authors andshould not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratoryor the U.S. Government.M. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721721References[1] C.H. Papadimitriou, J.N. Tsitsiklis, The complexity of Markov Decision Processes, Mathematics of Operations Research 12 (3) (1987) 441–450.[2] C. Baral, V. Kreinovich, R. Trejo, Computational complexity of planning and approximate planning in the presence of incompleteness, Artificial Intelli-gence 122 (1–2) (2000) 241–267.[3] A. Barto, S. Bradtke, S. Singh, Learning to act using real-time dynamic programming, Artificial Intelligence 72 (1995) 81–138.[4] P.E. Hart, N.J. Nilsson, B. Raphael, A formal basis for the heuristic determination of minimum cost paths, IEEE Transactions on Systems, Science, andCybernetics SSC-4 (2) (1968) 100–107.[5] B. Bonet, H. Geffner, Planning with incomplete information as heuristic search in belief space, in: S. Chien, S. Kambhampati, C. Knoblock (Eds.), Proc.6th International Conf. on Artificial Intelligence Planning and Scheduling, AAAI Press, Breckenridge, CO, 2000, pp. 52–61.[6] M. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, John Wiley and Sons, 1994.[7] M. Likhachev, A. Stentz, PPCP: The proofs, Tech. Rep., University of Pennsylvania Philadelphia, PA, 2008.[8] A. Stentz, The focussed D* algorithm for real-time replanning, in: Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),1995.[9] S. Koenig, M. Likhachev, D* Lite, in: Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI), 2002.[10] S. Koenig, M. Likhachev, Adaptive A*, in: Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),2005, poster abstract.[11] M. Likhachev, A. Stentz, Goal directed navigation with uncertainty in adversary locations, in: Proceedings of the International Conference on IntelligentRobots and Systems (IROS), 2007.[12] M. Likhachev, A. Stentz, Path clearance using multiple scout robots, in: Proceedings of the Army Science Conference (ASC), 2006.[13] I. Nourbakhsh, M. Genesereth, Assumptive planning and execution: a simple, working robot architecture, Autonomous Robots Journal 3 (1) (1996)49–67.[14] S. Koenig, Y. Smirnov, Sensor-based planning with the freespace assumption, in: Proceedings of the IEEE International Conference on Robotics andAutomation (ICRA), 1996.[15] A. Stentz, Map-based strategies for robot navigation in unknown environments, in: AAAI Spring Symposium on Planning with Incomplete Informationfor Robot Problems, 1996.[16] E. Hansen, S. Zilberstein, LAO*: A heuristic search algorithm that finds solutions with loops, Artificial Intelligence 129 (2001) 35–62.[17] B. Bonet, H. Geffner, Faster heuristic search algorithms for planning with uncertainty and full feedback, in: Proceedings of the 18th International JointConference on Artificial Intelligence, 2003, pp. 1233–1238.[18] M. Likhachev, G. Gordon, S. Thrun, Planning for Markov Decision Processes with sparse stochasticity, in: Advances in Neural Information ProcessingSystems (NIPS) 17, MIT Press, Cambridge, MA, 2004.[19] S. Yoon, A. Fern, R. Givan, FF-replan: A baseline for probabilistic planning, in: Proceedings of the International Conference on Automated Planning andScheduling (ICAPS), 2007.[20] O. Buffet, D. Aberdeen, The factored policy gradient planner, in: Proceedings of the Fifth International Planning Competition (IPC), 2006.[21] C. Boutilier, D. Poole, Computing optimal policies for partially observable decision processes using compact representations, in: Proceedings of theNational Conference on Artificial Intelligence (AAAI-96), AAAI Press/The MIT Press, Portland, OR, 1996, pp. 1168–1175.[22] X. Boyen, D. Koller, Tractable inference for complex stochastic processes, in: Proceedings of the International Conference on Uncertainty in ArtificialIntelligence (UAI), 1998, pp. 33–42.[23] C. Guestrin, D. Koller, R. Parr, Solving factored pomdps with linear value functions, in: Proceedings of the Workshop on Planning under Uncertaintyand Incomplete Information, 2001.[24] K. Poon, A fast heuristic algorithm for decision-theoretic planning, Ph.D. thesis, The Hong Kong University of Science and Technology, 2001.[25] N. Roy, G. Gordon, Exponential family pca for belief compression in POMDPs, in: Advances in Neural Information Processing Systems, 2002.[26] J. Pineau, G. Gordon, S. Thrun, Point-based value iteration: An anytime algorithm for POMDPs, in: Proceedings of the International Joint Conference onArtificial Intelligence (IJCAI), 2003.[27] M. Spaan, N. Vlassis, A point-based POMDP algorithm for robot planning, in: Proceedings of the IEEE International Conference on Robotics and Au-tomation, 2004, pp. 2399–2404.[28] B. Bonet, An (cid:3)-optimal grid-based algorithm for Partially Observable Markov Decision Processes, in: Proceedings of the International Conference onMachine Learning, 2002.[29] W.S. Lovejoy, Computationally feasible bounds for partially observed Markov Decision Processes, Operations Research 39 (1) (1991) 162–175.[30] R. Zhou, E.A. Hansen, An improved grid-based approximation algorithm for POMDPs, in: Proceedings of the International Joint Conference on ArtificialIntelligence (IJCAI), 2001.[31] T. Smith, R.G. Simmons, Heuristic search value iteration for POMDPs, in: Proceedings of the International Conference on Uncertainty in ArtificialIntelligence (UAI), 2004.[32] Baral, Son, Approximate reasoning about actions in presence of sensing and incomplete information, in: Proceedings of International Logic ProgrammingSymposium (ILPS), 1997.[33] R. Washington, BI-POMDP: Bounded, incremental, partially-observable Markov-model planning, in: Proceedings of the European Conference on Planning(ECP), 1997, pp. 440–451.[34] F.C.D. Szer, S. Zilberstein, Maa*: A heuristic search algorithm for solving decentralized POMDPs, in: Proceedings of the International Conference onUncertainty in Artificial Intelligence (UAI), 2005.[35] G. Shani, R.I. Brafman, S.E. Shimony, Forward search value iteration for POMDPs, in: Proceedings of the International Joint Conference on ArtificialIntelligence (IJCAI), 2007.[36] D. Ferguson, A. Stentz, S. Thrun, PAO* for planning with hidden state, in: Proceedings of the 2004 IEEE International Conference on Robotics andAutomation, 2004.