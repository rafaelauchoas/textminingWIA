ELSEVIER Artificial Intelligence 85 ( 1996) 181-243 Artificial Intelligence The effect of resource limits and task complexity on collaborative planning in dialogue Marilyn A. Walker * AT&T Research Laboratories, 600 Mountain Ave., Rm. 20-441, Murray Hill, NJ 07974, USA Received January 1995; revised October 1995 Abstract to mitigate in communicative testbed whose design ( 1) agents’ resource action can be designed This paper shows how agents’ choice is motivated by the corpus analysis. Experiments the interaction between (2) agents’ choice limits in the context of particular features of a collaborative planning the task. effect of their resource language behavior based on a statistical about effective I first motivate a number of hypotheses analysis of a corpus of natural collaborative planning dialogues. These hypotheses are then tested in a dialogue in the testbed limits in attentional capacity and inferential examine that capacity; tasks affect required, and for communication must be defined tolerance relative to the agents’ resource limits and the features of the task. Algorithms that are inefficient for inferentially simple, low coordination or fault tolerant tasks are effective when tasks require for coordination or complex that, prima facie, appear inefficient, and provide the occurrence of utterances in human dialogues the basis for the design of effective algorithms limited choice for communicative agents. inferences, or are fault intolerant. The results provide an explanation for errors. The results show that good algorithms complexity, degree of belief coordination features of communicative such as inferential in communication; task difficulty for resource and (3) 1. Introduction Agents may engage in conversation to make a plan, or to be social. At each point to establish a contract, agents must make communicative say it. This paper focuses on agents’ communicative dialogues, dialogues whose purpose choices about what planning is to discuss and agree on a plan for future action, choice to say and how and when in collaborative to for a range of reasons, e.g. to acquire information, in a dialogue, * E-mail: walker@research.att.com. 0004-3702/96/$15.00 SSD10004-3702(95)00114-X Copyright @ 1996 Published by Elsevier Science B.V. All rights reserved IX? M./l. ll/llXr,-/~~~rrr~ic~rlcl lw!lilgr,rw x.5 (I9961 IHl-243 execute their algorithms and potentially action, relatively unexplored resource of collaborative the degree of belief coordination limits. such as limits planning factors A primary dimension example. consider a simple furnishing by ( I ) and believes that agents’ choices that plan. 1 will argue for language behavior. must be determined with respect in models of collaborative planning dialogues: and inferential capacity; and (2) in attentional in communicative to two ( I ) agents’ features complexity, tasks that affect task difficulty, such as inferential required, and tolerance for errors. of communicative choice is the degree of explicitness. task of agent A and agent B trying For to agree on a plan for realized a two-room house. Imagine that A wants B to believe the proposition that B can infer this from the propositions realized in (2): ’ ( I I in the study. II‘ we agree to put the green couch in the study, we will have a matched pair of furniture t 2 ) c a) I propose C t-11 We intend ( c) Two furniture that we put the green couch in the study. to put the green chair in the study. items of the same color in the same room achieve a matched pair. In naturally occurring dialogues, A may product utterances ( 3 )-( 6 ). or other variations [ 30, 109. 13.3, I3 I 1. realizing the propositions in (3) (4) (5) (6) A: I propose A: We intend couch in the study. A: Two furniture pair. We intend green couch A: I propose pair. that we put the green couch to put the green chair in the study. I propose in the study. that we put the green items of the same color in the same room achieve a matched that we put the in the study. I propose to put the green chair in the study. that we put the green couch in the study. That will get us a matched fact: for any communicative levels of explicitness. the more or less explicit choices in (3 )-( 6) illustrate a general The communicative act. the same effect can be achieved with a range of acts at various This raises a key issue: On what basis does A choose among in C 3 )-C 6) ? versions of the proposal that has been suggested elsewhere The single constraint A should not say information such as “don’t CONSTRAINT: infer. The IUXNINDANC~ DUNDANCY that B could ple dictums Maxim “do not make your contribution more informative operators constraints on planning plans possibility for what A can say is (3). J So. if we assume for the generation [2. 13.27.86.92 that B knows CONSTRAINT in the literature is the FE- that B already knows, or in the form of sim- tell people what they already know”. as Grice’s Quantity appears than is required” [47] and as and recognition of communicative (2b) and (2~). then the only ’ These ewmples arc I‘rom the domain of Design-World to be discussed in Section 4 and are abstractions from naturally occurring examples in which the propositions realized here are realized in a number of different ways. Here the focus i< on the logical relationships between the content of each proposition: (?a) and (2b) arc minor premises and (2~) is the major premise for the inference under discussion. M.A. Walker/Art@cial Intelligence 85 (1996) 181-243 183 The REDUNDANCY CONSTRAINT is based on the assumption implicit leave B could a combination I will show infer, any information in other words, of retrieving she believes that agent B can always “fill that B already knows or she believes facts from memory and making that agent A should that is missing” by In Section 2, in what inferences. the REDUNDANCY surprising since the that agents in natural dialogues consistently violate CONSTRAINT. I will argue that this should not be particularly REDUNDANCY CONSTRAINT is based on several simplifying assumptions: (i) UNLIMITED WORKING MEMORY ASSUMPTION: everything an agent knows is always available for reasoning; (ii) LOGICAL OMNISCIENCE ASSUMPTION: (iii) ASSUMPTION: FEWEST UTTERANCES that should be minimized; agents are logically omniscient; utterance production is the only process (iv) No AUTONOMY ASSUMPTION: assertions and proposals by agent A are ac- cepted by default by agent B. these assumptions do not always When agents are autonomous hold, and the problem of communicative for the paper and resource limited, choice remains. The plan is implemented the relationship choice, resource of communicative planning presented from natural collaborative is as follows: Section 2 motivates planning dialogues. These hypotheses a number of hypotheses limits and task features using are the basis in Section 3. Then Section 4 describes planning dialogues called about evidence of a model of collaborative how the model Design-World, which supports experiments on the interaction of agents’ communicative choice, the steps of the method applied so far, and motivate for testing the extent theoretical to other tasks, agent properties, the hypotheses. Section 5 presents to which implications in Section 4.1, I review as a method results and discusses the of these results and the extent to which they can be generalized limits, and features of the task. At this point, the hypotheses were confirmed, and then Section 6 discusses in a testbed for collaborative the use of simulation and communication the experimental strategies. resource 2. Communicative choice in dialogue about Naturally dialogues are design, problem planning [ 14,21,30,37,49,97,104,128,141,143]. collaborative occurring diagnostic or advice giving dialogues to generate hypotheses and task features, collaborative from a corpus of dialogues but I will also draw on data from collaborative computer solving, In order to agent properties in naturally occurring dialogues. Most of the examples discussed below are excerpts [98] ,2 and from a radio talk show for financial planning the relation of communicative advice construction, this section examines design, collaborative [ 28,139,142,143]. communicative planning choice choice Dialogue, add to what is is modeled as a process by which conversants to be already mutually believed or intended. This set of assumed mutual beliefs assumed support dialogues in general, * The corpus consists of 55 dialogues from 5 hours of live radio broadcast, where each dialogue ranged in length from 23 to 100 turns. planning dialogues, is called the DISCOURSE MODEL, or the COMMON the current state of the world and mutual beliefs and intentions and intentions In collaborative about future action and the efficiency of the planning process must be affected by agents’ algorithms communicative GROUND [ II9,140]. to add mutual beliefs about a plan for that the efficacy of the final plan for to the discourse model. the conversants are attempting It is obvious choice. However previous work has not systematically varied factors that affect communicative choice, such as resource has been based on the REDUNDANCY assumptions simplifying (but see [78, 147. 14X] ). To explore the relation of communicative choice limits and task complexity. Furthermore, most previous work CONSTRAINT, and apparently, its concomitant the analysis of naturally occurring collaborative planning dialogues on communicative INFORMATIONALLY the REDUNDANCY REDUNDANI‘ IJTIERANCES, IRUs. defined as:3 acts that violate to effective collaborative planning, in this paper focuses CONSTRAINT. These acts are Definition of informational DUNDANT in a discourse redundancy. An utterance u, is INFORMATIONALLY RE- situation S (i) (ii) a proposition if II, expresses already been said in S; if II, expresses a proposition implicates p, has already been said in S. pL, and another utterance nj pi. and another utterance n, that entails p; has that presupposes or A statistical analysis of‘ the tinancial advice corpus showed that about 12% of the in Section 1. this should not be particularly the definition of IRUs reflects several simplifying utterances are IRUs. As mentioned since reflects definition all the entailments from propositions definition reflects saying an utterance discourse model. The fact that IRUs occur shows that the simplifying no1 valid. the LOGICAL OMNISCIENCE ASSUMPTION because of propositions uttered the NO AUTONOMY inferences in a discourse become part of the discourse model. 4 The that merely for adding p, to the are assumptions assumptions. For example. it assumes II, that expresses a proposition p, is sufficient in a discourse and certain default ASSUMPTION because it assumes surprising the that uttered The distributional analysis suggests that there are at least three functional categories of IRUs: Communicative l Attitude: functions of IRUs. to provide evidence supporting beliefs about mutual understanding and acceptance. ’ The first part of the definition is a variation on Hirschberg‘s detinition of redundant [ S9) which is used in is also the basis of information theoretic work such her theory of scalar implicature. This view of information ab 16 1 4 Presuppositions implicatures and are tuo type5 of default inferences tagged defaults separately from entailments hut found no evidence analysis II?31 I ( 44.46.69.82. for a functional I24 I. The corpus (see difference M.A. Walker/Artijicial Infelligence 8.5 (1996) 181-243 185 to manipulate the locus of attention of the discourse participants by to augment the evidence supporting beliefs that certain inferences Attention: making a proposition Consequence: are licensed. salient. IRUs have ANTECEDENTS the content of the IRU either through direct assertion or by an inferential above u,j is an antecedent realized in the definition of IRUs were relations between ture relations, features such as the intonational relation of the IRU to adjacent utterances. and logical the IRU and its antecedent, in the dialogue which are the utterances that originally chain; functions in part on struc- analysis also analyzed utterance realization of the IRU, the form of the IRU, and the for ui. The three communicative features based such as textual distance, discourse identified by correlations with distributional relations. The distributional Below, I will briefly give examples of each type of IRU.’ For each explain how the four simplifying the utterance task properties under which IRUs function as hypothesized is informationally assumptions redundant. Then we will consider hypothetical of previous dialogue models predict above. type I will that agent and 2.1. Attitude IRUs supporting the speaker’s attitude IRUs provide evidence by demonstrating Attitude acceptance by another agent of a declarative utterance, in (7.26). M and H have been discussing invested occurring are in CAPS. in dialogue. An Attitude is given examples below, the ANTECEDENTS (individual retirement in IRAs IRU, said with a falling beliefs about mutual understanding and to an assertion or proposal made typical in (7.27)) where M repeats what H has asserted. funds how M and her husband can handle In (7), and in the other naturally of the IRUs are italicized and the IRUs accounts). intonation (7) (24) H: That is correct. It could be moved around so that each of you have two thousand. (25) M: I see. (26) H: Without penalty. (27) M: WITHOUT PENALTY. (28) H: Right. (29) M: And the fact that I have a, an account of my own from a couple of years ago, when I was working, doesn’t affect this at all. s Each communicative function given above includes a number of subtypes the hypothesis is tested in every experiment by the model of attention/working IRUs are a rehearsal mechanism, In addition, as an aid to memory, by these examples. propositions that agents The hypothesis hypothesis) was considered indications of hesitation or planning what to say, such as disfluencies IRUs. say IRUs because in [ 1311, but I as yet have found no evidence they cannot that think of anything i.e. agents that will not be represented repeat memory. (the DEAD AIR it. For example, other and long pauses, are not associated with else to say to support Header: prccondition: want-precondi Lion: cffcct: -__ . .._.____-.~ _ INPORMC speaker, hearer, proposition KNOWI spcakcr, proposition), speaker want INFORMS speaker, hearer. proposition KNOWI hearer. proposition). ) . Fig. i t_)rtlnlllon 01 the INFORM pian operalor in 1 ? I. The IRU in ( 27 ) provides direct evidence According indirectly serted. to arguments elaborated evidence provides that M heard exactly what H said [ 8.2 I 1. below and elsewhere 1 13 I, 1331, M’s response therefore believes what H has as- that she accepts and The classilication of ( 7.27 1 as 311 IRU li)llows horn the NO-AUTONOMY ASSUMP- operator ‘I‘lON. The NO-AUTONOMY “cooperative” FORM planning words. a cooperative the speaker has previously through. in response to H’s inform ASSUMPTION or “helpful”. For example, the motivation is usually characterized from 12) in Fig. I is that the hearer hearer always accepts and therefore believes asscrtcd. But if the effect of the INFORM is cooperative. as an agent being for the plan effect of the IN- In other (or knows) what act always goes IRU in (7.27). then there is no reason for M to choose to produce an Attitude in f 7.26). In recent work. the plan clf‘ect shown in Fig. I is treated as a default [ 5 I, 67.96, 105 J inferring [ 51.961. rule handles the default acceptance of assertions default rule CDR2 handles Pcrrault’a bclicl‘ transfer form acts). while Grosz and Sidncr’s conversational the default acceptance of proposals ceptancc of an assertion of P or a proposal the hearer and on whether or not the hearer previously believed or intended -P. However. Attitude for the default not to go through. and the hearer does not previously believe or intend are common when paraphrases his response (in- inferring inference of ac- to achieve P depends on the cooperativity of to achieve in many situations. where there is no reason is cooperative IRUs and then repeats or the caller asks the talk show host a question to the question with an Attitude In advice givin g dialogues, In both cases, the default IRUs arc produced -P, yet Attitude to achieve the hearer IRU. [ 9.20.2 1 ]. They allow for undcrstandin g to be implicitly Clark and Schaefer proposed that Attitude standing that the amount of explicit positive evidence should bc “sufficient However. Clark and Schaefer do not address do not distinguish between more. or Icss positive evidence. and thus lead an agent to produce an Attitude IRUs provide positive evidence of under- conveyed, but say for current purposes”. they transfer since and indicating acceptance. Further- require more about what features of current purposes they make no predictions indicating understanding the question of belief IRU. Thus, neither predictions acceptance goes through by default without the addition of defaults nor the positive evidence model makes any about when an agent should produce an Attitude IRU, since the inference of the Attitude IRU. In order to explain the function of Attitude IRUs, the NO-AUTONOMY must be replaced with the assumption accept or rcject each utterance act that is intended In Section 3. these observations dialogue. Results testing hypotheses are incorporated ASSUMPTION that hearers always either explicitly or implicitly their beliefs or intentions. planning to produce Attitude into a model of collaborative to the choice to change related from M.A. Walker/Artificial Intehgence M (1996) 181-243 187 IRUs are presented elsewhere paper. 2.2. Consequence IRUs [ 131, 133,135], and will not be discussed further in this Consequence IRUs make inferences explicit. For example, consider (8.17): (8) ( 15) H: Oh no. IRAs were available as long as you are not a participant in an existing pension. J. Oh I see. Well I did work, I do work for a company that has a pension. (16) ( 17) H: Ahh. THEN YOU’RE NOT ELIGIBLE FOR EIGHTY ONE. In (8), (8.15) realizes a biconditional inference realizes an inference tax year of 1981, by the inference rule, (8.16) instantiates from rule of modus from the LOGICAL OMNISCIENCE AS- one of the and (8.15) tollens. that follows premises of this rule, and (8.17) for the particular (8.16), The definition of (8.17) as an IRU follows of utterances should not occur. However then (8.17) SUMPTION. If all entailments model nor artificial agents are logically omniscient enough inference heavy planning rules [ 721, especially and processing time to make all the relevant inferences [45,55,63,73,95]. are automatically added it is well known to the discourse that neither human Agents might not have even when they know all the relevant speech in real time has since producing and interpreting requirements. Thus plausible hypotheses are that: l HYPOTH-Cl: they made the inference that is made explicit. Agents choose to produce Consequence IRUs to demonstrate that l HYPOTH-C2: Agents choose to produce Consequence IRUs to ensure that the other agent has access to inferrable information. These hypotheses In addition, IRUs to ensure manner, even when, inference. are motivated by the fact that agents are not logically omniscient. to produce Consequence in a timely information the in the case of hypothesis C2, agents may choose that the other agents have access to inferrable they believe the other agent is capable of making in principle, However, much of communicative efficiency from what has been said. Thus plausible relies on the fact that agents do make refinements of hypotheses Cl some inferences and C2 are that: l HYPOTH-C3: The choice measure of “how hard” the inference to produce a Consequence is. to produce a Consequence IRU is directly related to a IRU is directly related to a l HYPGTH-C4: The choice measure of “how important” the inference is. l HYPOTH-C5: The choice degree to which they have made. to produce a Consequence IRU is directly related the task requires agents to be coordinated on the inferences to the that of these hypotheses entails Confirmation does not hold whenever processing goals. that the FEWEST UTTERANCES ASSUMPTION the conversational to achieving is relevant effort 2.3. Attentiorl IRUs Attention IRUs manipulate the locus of attention of the discourse participants by making a proposition related f 9) said by agent A to agent B while walking to the assertion or proposal salient. Attention that the speaker to work: IRUs often realize facts that are inferentially is making. For example, consider (9) (a) Let’s walk along Walnut St. ( b) IT’S SHORTER. Agent B already knew that the Walnut Street route was shorter. so, by the REDUNDANCY CONSTRAINT, A should have simply said (9a ). of (9b) as an IRU reflects The classification an agent knows is always available the UNLIMITED WORKING MEMORY for reasoning, then agents to include utterances such as (9b). However, choices that human agents have limited attention/working memory [5,91,95]. artificial agents with limited time to access memory also have If everything assumption. should never make communicative it is well known and resource-bounded limited attentional capacity. If we define SALIENT propositions as those that are accessible agent at a particular point (9b’) to provide B with a salient reason Similar observations to ( IO): apply in time 1 102, 1031, then a possible hypothesis to accept A’s proposal to a resource limited is that A said to walk along Walnut St. ( 10) (a) Clinton has to take a stand on abortion rights for poor women. ( b) HE’S THE PRESIDENT. the difference i lob) In order Here salient. to reflect a PROPOSAL whereas a salient WARRANT assertion. 6 reason is already known to the discourse participants, to account for ( lob) we must modify but saying the specific hypothesis in utterance type hetween (9a) and ( lOa). Utterance ( 10a) is an INFORM. In ( IO). A said ( lob) to accept A’s assertion about Clinton’s obligations. Utterance it makes it above is to provide B with is a in A’s (9b) (9a) for adopting A’s proposal in (9a). and f lob) is SUPPORT for belief l HYPtYI%AI : Agents produce Attention IRUs to support the processes of deliber- ating beliefs and intentions. that the production 01‘ Attention limits limited working memory Hypothesis Al means of the fact that agents’ as the basis of deliberation. The hypothesis salient make DISCOURSE INFERENCE CONSTRAINT: information to support interpretation IRUs is a surface manifestation the accessibility that the function of Attention and reasoning is formulated of beliefs used is to IRUs in the l HYPOTH-A2: There is a DISCOURSE INFERENCE CONSTRAINT whose effect is that inferences salient ( in working memory). in dialogue are derived from propositions that are currently discourse ’ The relationship 189 1. or the inference of the speaker’s two views are functionally intention equivalent 16 I, 93 I. between these utterances ha been characterized as the inference of a discourse relation 192 1. Moser and Moore and Hobbs have argued that these M.A. Walker/Art$cial Intelligence 85 (1996) 181-243 189 The DISCOURSE INFERENCE CONSTRAINT is quite general since A intends B to make may be any deductions, commonsense as WARRANT inferences inferring part of A’s plan, or inferring to the dialogue defaults, related the inferences that such as logical relations such or SUPPORT [92,134,141]. illustrating and agents’ communicative A more complex inferential processing, has been telling H, the talk show host, how all her money question the relationship in (11.3): example choice is dialogue of limited working memory, ( 11). The caller E is invested and then poses a (11) (3) (4) (5) (6) (7) (8) they are, E: . . .and I was wondering-should H: Well it’s difficult But I would suggest presume E: yes, H: then I would around, E: uh huh, H: Now in addition, how old are you? I continue on with the certificates or, to tell because we’re so far away from any of them. all of these are 6 month certificates and I this-if like to see you start spreading some of that money (discussion about retirement investments consisting of 14 utterances) (21) (22) (23) (24) E: uh huh and (a) H: But as far as the certificates are concerned, (b) I’D LIKE THEM SPREAD OUT A LITTLE BIT. (c) THEY’RE ALL 6 MONTH CERTIFICATES. E: yes, H: and I don’t like putting all my eggs in one basket. . . . about realize hypothesis investments. in utterances the discussion about retirement in (22b) and (22~) (4)-(7)) retirement in (4)-(7) that the information established as mutually be- (8) in which is that, at (22a), H believes two propositions thus they are IRUs. Utterance Since the information The utterances initiates a subdialogue lieved invest- digression is not discussed. ments consists of 14 utterances in expressed a plausible [ 1361. However, H expects E to use this information to (4)-( 7) is no longer salient is an make two inferences: INSTANCE OF the negatively evaluated condition of having all your eggs in one basket; to spread the certificates and (2) a content-based out a little bit. Here, infer- inference, WARRANT, ence, INSTANCE OF, in the first case and a deliberation-based in the second. made and that H is basing his communicative CONSTRAINT. In addition get that these choice on the DISCOURSE INFERENCE therefore, we see two types of inferences: that having all your investments for E to adopt the intention that this is a WARRANT examples of Attention in 6 month certificates that H produces to the naturally It appears to ensure inferences occurring IRUs (1) for the DISCOURSE INFERENCE CONSTRAINT is the distribution of evidence that make inferences 2.2. Fig. 2 contrasts explicit the distribution such as the Consequence of Consequence IRUs, another source of IRUs IRU in dialogue (B), Section IRUs and paraphrases, which Antecedents Antecedents salient not salient 24 x 43 39 Consequence IRUs Paraphrase IRUs Fig 2. Distribution of Consequence IKUs that make inferences explicit, as compared with paraphrases, according to whether their antecedents are currently salient. logically inference rule such as modus ponens. A key difference are syntactic or semantic are distinguished in which an IRU can relate transformations from paraphrases by requiring to the prior discourse.7 arc IWO different ways of a single antecedent utterance Paraphrases [ 65,901. Inferences the application of a logical is that inferences have multiple antecedents while paraphrases do not. A priori we would not expect inferences and paraphrases. whether more (If = I ). This distributional CONSTRAINT because whenever we have evidence to be salient. premises are likely to that INFERENCES are (,y' = 4.835, p < 0.05. for the DISCOURSE INFERENCE that an inference has been made, the are salient. s However Fig. 2 shows to distribute differently with respect as two types of entailments, lo have salient premises fact provides evidence than PARAPHRASES their antecedents likely The data discussed above provide evidence for the DISCOURSE INFERENCE CON- however it is clear STRAINT. by the limits on working memory. inferential multancously as follows: can be directly salient for the inference complexity that the effect of the constraint is strongly determined a corollary of the constraint In particular, related to the number of premises to be made. These hypotheses can be summarized is that that must be si- l HYPOTH-A3: The choice to produce an Attention IRU is related inferential to make task-related complexity inferences. of a task as measured by the number of premises to the degree of required l HYPOTH-A4: The choice is resource which an agent to produce an Attention limited in attentional capacity. IRU is related to the degree to it is obvious that various Finally. of inferential and those made about Consequence sis AS. complexity, tasks can be characterized that observations IRUs also apply about belief coordination to Attention in terms of the degree to IRUs. giving hypothe- similar l HYPOTH-AS: The choice the task requires agents which made. to produce an Attention IRU is related to be coordinated on the inferences to the degree to that they have In the next sections we will see how WC can test these hypotheses ’ The other categories are repetitions. making implicatures explicit and making presuppositions explicit. ’ For the corpus analysis. salient utterances are those within the last two turns. This measure is not perfect but it is replicable. M.A. Walker/Artificial Intelligence 85 (1996) 181-243 191 3. Modeling resource limited collaborative planning dialogues The naturally occurring examples discussed section gave rise to a in the previous in which communicative as to the situations improve that plan was constructed. the efficacy of a collaborative to include choices plan or the efficiency of the the details planning that will be used as the basis of the dialogue the hypotheses can be tested. In thinking about models of in terms of six features: In this section, I will specify I have found it useful to consider models number of hypotheses IRUs could either dialogue by which of a model of collaoorative simulation in which testbed collaborative (i) (ii) limits: whether in mutual beliefs, resources, and thus whether any aspect of resource consumption, planning, agent architecture, role of resource have limited or minimize the mutual belief model: whether mutual beliefs, and whether defaults utterance act types: types of acts available agents and the effects of each act on the cognitive collaborative mixed initiative, 9 plan evaluation: mine how good the collaborative planning planning process, initiative: whether one agent plan is. dialogues how the collaborative (iii) (iv) (v) (vi) the agents constructing plan there is an attempt to either maximize the collaborative and if so which aspects, the function of the dialogue the mutual belief model is binary or allows is to establish for for agents to communicate with other state of the agents and the is the initiator or both agents have equal plan is evaluated and what factors deter- and there are precise models of how hearers are not specific about all of these Most accounts of collaborative features. For example, features, although some accounts provide rich models of particular that provided here Smith and Guinn provide a richer model of mixed [ 53,117], infer the utterance act type or [ 1,86,112, 1271. However, the intention communicative to my knowledge no previous work has included a specification of the agent architecture, the relationship the plan evaluation process. The remainder of this section provides a specification each of these features. limits, and for to language behavior, the role of resource of the architecture a particular underlying initiative action than 3. I. Agent architecture, mutual belief and resource limits Both the agent architecture and the role of resource agent architecture based on the IRMA architecture in Fig. 3 [ 7,991. The IRMA architecture behavior of agents in dialogue. The basic components of the modified IRMA architecture are: has not previously been used to model for resource-bounded limits are addressed by adopting an agents, shown the y This is also called “control” in natural dialogue control between the agents or primarily known by one agent is primarily determined by whether [52,139]. [ 116.1431. Walker and Whittaker and Guinn argued information relevant that the distribution to the task is distributed of BELIEF DELIBERATION ATTENTIONMiORKlNG MEMORY (AWM) MEANS-END REASONER FILTERING MECHANISM I COMPATIBILITY FILTER I . . . . . . / I MESSAGE PLANNING (mediated by dialogue strategies) I V > INTENTION DELIBERATION COMMUNICATIVE ACTIONS Fig. 3. The IRMA agent architecture for resource-hounded agents with limited attention (AWM). to some degree a database of an agent’s intentions. This includes decides what an agent wants to believe when there is conflicting Beliefs: a database of an agent’s beliefs. This includes beliefs that an agent believes to be mutual Belief deliberation: evidence. Intentions: believes Plan library: what an agent knows about plans as recipes Means-end options Filtering mechanism: plans. Options deemed compatible the agent’s existing are passed along to the deliberation process. ” reasons about how to fill in existing partial plans, proposing for the plans an agent has in mind. for compatibility with that serve as subplans to achieve goals. to some degree. checks options to be mutual that an agent intentions reasoner: “’ The filtering mechanism presented in I 7 j and used in TileWorld is more complex than that presented here because that work explored the issue of when current intentions get over-ridden. M.A. Walker/Art$cial Intelligence 8.5 (1996) 181-243 193 types of desires but here I assume that their [ 341. agents may have different is to maximize utility Desires: only desire Intention deliberation: based on desires such as maximizing Attention/ working memory ing memory and the retrieval of current beliefs and intentions means-end decides which of a set of options reasoner. utility). ( AWM) : the limited attention module constrains work- that are used by the to pursue (by an evaluation the architecture has been extended with a model For the purpose of modeling dialogue, that allows for different degrees of mutual belief. For the purpose of of mutual belief exploring this architecture has been extended with a model of limited attention. All of the modules are standard except for the AWM module described in detail below, and the mutual belief module which will be briefly described. the effects of resource bounds on attention, Attention/working memory model The model of limited AWM is a cognitively results on human memory and learning based model adapted from [ 761, which [4,31,.57,75,118,121,129]. for using a cognitively based model of AWM is to model in naturally occurring dialogues and to test a theory of collaborative the behavior commu- fits many empirical The motivation of agents nication with humans. ” The key properties of the model are that (1) model and can be varied recently are more likely to be salient more likely to be salient [ 581. These recency and frequency to explore different limits [ 761; (3) the hypothesized this model and its role in testing the hypotheses. functions of IRUs. Below I will discuss a particular effects are a key aspect of the AWM model for testing of implementation limits on AWM are parameters of the items encountered more are items encountered more frequently [ 51; (2) in chronological the world are stored is modeled as a three-dimensional space in which propositions sequence according AWM perceiving a moving memory pointer. The sequence of memory random walk through memory with each loci a short distance items are encountered multiple times, they are stored multiple the sequence of storage locations are stochastically the simulation from to the location of a from the previous one. If [ 581. The fact that times is random means that the recency and frequency effects is used in simulation, loci used for storage constitutes determined. This means that when this model produces different results each time. acquired When an agent retrieves items from memory, and spreads out in a spherical the fact that search from location follows Hamming distance. For example, distance actual 1 away would be (010) locations are calculated modulo fashion. The resource to a particular is restricted if the current memory pointer (001) search starts from the current pointer limited aspect of AWM in search radius defined loci is (0 0 O), the loci The (-100) (O-IO) (100). (00-I) the memory size. I’ Some of the features of the model hold for processors been discussed more recently are more likely to be accessible with little effort, and that incoming can displace other information in general, such as the feature from working memory. that items that have information I’)-1 MA. Wulkr~/Artificud lnrellr,~ence X5 (1996) It?-243 The limit on the search radius defines the subset of the belief and intentions database the fact that the pointer moves has the effect that the salient is always changing. Effectively, as new facts are added, others are displaced and that is SALIENT. In addition, subset become no longer salient, so that the SALIENT predicate the AWM parameter limit defines to test the effect of different in Section 5 in order is dynamic. that will be varied The search resource radius that Landauer performed showed that, for a task requiring in the ex- limitations. remembering periments Experiments whether a word belonged an AWM of 7 reproduces tests have been performed investigated below. Human performance the model can be parameterized to a list of words. the human performance for human performance so that in [ 571. Since no systematic tasks on the collaborative are run at LOW, MID and HIGH AWM settings. planning results to fall somewhere in the middle of these ranges. the experiments is assumed The model of’ attention/working memory DISCOURSE INFERENCE CONSTRAINT introduced hypothesized that the discourse member from propositions derived shown in Fig. 3, limits to the subset of beliefs defined as being SALIENT. inference constraint that are currently the beliefs accessible that are currently ( AWM) provides a means of testing the in Section 2.3. Re- are in discourse in working memory. The AWM model, as for means-end reasoning and deliberation that inferences states in AWM [55,66.68,95]. These beliefs are This model contrasts with the standard view of inference, where if an agent believes inference constraint P and believes P - Q. then the agent believes Q. The discourse provides a principled way of limiting premises of P and P + Q to be salient. An axiomatization SALIENT and inference rules as follows for each inference inference in modeling humans by requiring requires the the predicate rule schema [ 6 1, 1331: Irlfkrencr under the discoursr ir@rerlce cwstmirlt Say(A.B, PI -4 Salient(B. PI. Salient(B, P) .A BEL(B, P) A Salient(B. P - Q) A BEL( B. P --i Q) .- BEL(B,Q). The first inference proposition P, that P becomes proposition P and an inference then B can use them when the SALIENT predicate holds. rule states that whenever agent A says an utterance rule states for B. The second salient rule, P --t Q. are both believed by B and salient to B that realizes that whenever a for B. to infer Q. The model of AWM must be consulted to determine The model of mutual belief belief 16. 19,841, extended beliefs with qualitative beliefs database left to inference and some inferences endorsements is based on Lewis’ shared environment model of mutual to support different degrees of mutual belief by tagging at the time that they are formed and stored in the to be [ 29, 401 421. Different degrees of mutual belief allow some actions to be defaults. This makes it possible to distinguish M.A. Walker/Artijicial Intelligence 85 (1996) 181-243 the explicit acceptance of a proposal and the acceptance of a proposal between in the absence of evidence is possible and information to distinguish to the contrary. When agents are not logically omniscient, it between mutual beliefs about what has been mutually that has been discussed in the dialogue. See [ 131,133] 195 inferred inferred for more detail. 3.2. Discourse acts, utterance acts, and mixed initiative The overall structure of the discourse in collaborative planning dialogues is primarily determined by the task structure a dialogue segment task. Each subpart of the task consists of in which agents negotiate what they should do for that part of the [49,85,100,111]. As discussed in Section 2.1, we wish to abandon ASSUMPTION. the dialogue or initiate a subdialogue to initiate The model should allow either agent about a new part of the task [ 32,139]. For each agent to be able to do this, knowledge about the task must be distributed between so that each participant has a basis for means-end the NO AUTONOMY and deliberation. the participants reasoning Furthermore, agents should be able to ACCEPT or REJECT one another’s proposals. remain open for negotiation to coming up with a collaborative plan for the higher that the has agreed Each plan step contributing even if both agents are committed level goal. This contrasts with models initiator makes must be accepted by the non-initiator, to work on a collaborative for each plan substep once the non-initiator in which proposals level goal must to a higher [26,51]. plan Discourse acts and mixed initiative To engage in collaborative planning, agents take turns sending messages, and each turn may consist of one or more DISCOURSE ACTS. Discourse acts are OPENING, CLOSING, PROPOSAL, ACCEPTANCE, REJECTION and CLAFURCATION. These are higher level acts that are composed of primitives called UTIERANCE ACTS, which will be described below. The schema of discourse actions shown acts and which discourse acts can be combined schema For each step in the domain plan: in Fig. 4 controls into a single the sequence of discourse turn. I2 The discourse act PLAN. is the basis of an algorithm by which agents achieve a COLLABORATIVE (i) (ii) (iii) (iv) reasoning a subdialogue about options agents perform means-end agents deliberate about which options are preferable; individual individual then one agent initiates to the other agent, based on the options actions then the proposal may be subject CEPTED or REJECTED by the other agent, by calculating whether utility. to the satisfaction of their goals; consisting minimally in a reasoning that contribute identified to CLARIFICATION, after which it is either AC- it maximizes in the domain; of a PROPOSAL cycle, about I2 This schema cannot describe all discourse action transitions in every required extension is to allow multiple proposals to be simultaneously type of dialogue open f 107.1131. [80.81,1 IO]. One --- .- KEY: - Speaker Chanpr Possibly NO Sprnker Change e-. ’ -_, ’ M:iy he IMl’L.I(‘IT ~____________ ; OPENING : ___~ v PROPOSAL /I \ / ,%CCEPTANCE 1 .___________, REJECTlON I CLARIFICATION - I This algorithm The requirement follows COLLABORATIVE from PRINCIPLE: ties the discourse act schema that agents must indicate whether replacing the assumption of cooperativity in Fig. 4 to the IRMA architecture. they accept or reject each proposal in earlier work [2,511 with the l COLLABORATIVE PRINCIPLE: Conversants must provide evidence of a detected discrepancy in belief as soon as possible. PRINCIPLE W;LS proposed PLANNING PRINCIPLES of Whittakerand ( 1391. The COLLABORATIVE; PRINCIPLE means Stenton in [ 13 I 1, and is an abstraction of the [ 1431 and Walker that speakers must If the the next action by the hearer in order to detect the effects of their utterances. the dialogue and provides no evidence of a belief discrepancy, The COLLABORATIVE COLLABORATIVE and Whittaker monitor the hearer continues inference of acceptance is licensed as a default 139.1311. from the IRMA agent architecture The fact that agents evaluate both assertions and proposals before deciding what to in Fig. 3. As the figure to intention or belief ASSUMPTION (see also from other agents by assessing as Fig. 3 shows, the to use believe or intend follows shows, incoming messages about intentions deliberation. This provides while specifying [ 34,38.4 the support this evaluation beliefs them. that can serve as supports or warrants must be salient for these processes the warrants takes place under constraints rvhy an agent would accept or reject another agent’s proposal I ] ). Agents evaluate assertions and proposals of limited working memory, the basis for abandoning for proposals. Finally, and beliefs are subject the No AUTONOMY for assertions since and Utterance ucts Fig. 4 shows the discourse act schema acts are composed of utterance actually perform. Each discourse act can be performed that provides acts, which are the primitive the basis for dialogue. Discourse acts that an agent can the in different ways by varying M.A. Walker/Artificial Intelligence 85 (1996) 181-243 197 number and type of utterance may not include additional acts that it consists of. For example, a proposal may or (9). the hearer, as in example that can convince information There are seven utterance act types: OPEN, CLOSE, PROPOSE, ACCEPT, REJECT, ASK and SAY, which are realized via the schemas below: , ?speaker ?hearer ?option) (Propose (Ask ?speaker ?hearer ?belief) , (Say ?speaker ?hearer ?belief) , ?speaker ?hearer ?option) (Accept (Reject ?speaker ?hearer ?belief) , , (Reject ?speaker ?hearer ?option) (Open ?speaker ?hearer ?option) , (Close ?speaker ?hearer ?intention) , . by means-end The content of each utterance act can be an OPTIONS and INTENTIONS representing domain plan act constructed reasoning 3. An OPTION is an act that has not been committed to by both agents is an act that has been committed an intention or implicitly. The option is the current proposal. a and deliberation as shown in Fig. to by both agents. An INTENTION [ 7,991. An option only becomes plan if it is ACCEPTED by both agents, either explicitly and what is rejected in a REJECT schema is a counter-proposal in the collaborative The content of an utterance act may also be a belief. These beliefs are either those by the other agent, or inferences made that the in ASK actions have variables that the is a belief schema in a rejection that an agent starts with, beliefs communicated by the agent during the conversation. Beliefs addressee speaker believes for the option to instantiate. The belief is a reason attempts to reject the proposal, such as a belief that the preconditions in the proposal do not hold [ 139 1. Examples of these utterance acts in dialogue will be given B processes each of the seven messages specified below, Store means store in AWM, for eventual database. The processing with reference to the IRMA agent architecture. (i) Agent A: (Propose ?speaker ?hearer ?option) . in Section 4. Below how In the effects storage in the beliefs involved with each incoming message should be understood that A can send is specified. long-term Agent B: (a) Filter: check whether ?option is compatible with current beliefs, e.g. that no current beliefs contradict Infer and store the preconditions (ME-reason) reason (b) (c) Means-end its preconditions. of ?option. about intention the ?option contributes to. (d) Deliberate means-end Indicate (e) by evaluating reasoning. the ?option against other options generated by results of deliberation by an accept or reject. (ii) Agent A: (Ask ‘?speaker ?hearer ?belief). Agent B: Retrieve beliefs matching (say ?speaker ?hearer ?belief) with the variable Belief. ?belief from memory instantiated and respond with for each matching 198 ( iii ) (iv) f vi) ( vii ) ( viii) Agent A: (Say ‘!speaker ?hearer ?belief). Agent B: Store ?beIief. Agent A: C Accept ?speaker ?hearer ?option ) . I3 Agent B: (a) Store ( intend A B ?option 1. ” ( h) Store (act-effects Agent A: (Reject ‘?speaker ?hearer ?option). Agent B: (a) Deliberate ‘?option ) ?option in comparison with own current proposal that was it’ better than your current proposal. then reject with reason for rejection. If rejecting ?option rejected. ( t-11 Accept ?option (c) Agent A: (Reject Agent B: Store ?belief. Agent A: (Open ?speaker ?hearer ?option 1. Agent B: Mark the discourse segment Agent A: (Close ?speaker ?hearer ?intention). Agent B: Close the discourse ‘!speaker ?hearer ?belief). segment that matches ?option as open. for ?intention. These acts and their effects determine the structure of the dialogue and its effect on the mental state of the converaants. 3.3. Plati evaluation There are three components of the plan evaluation process that have different effects and the third on collaborative to the model of evaluation applied: planning. Two features are related to the task definition (i) iii) (iii) intended and whether any beliefs related the degree of belief coordination: whether some or all of the itztetztiotzs associated with a plan must be mutually to the intended acts must also be mutually believed; and fault tolerance: whether task determinacy is fault tolerant or more or less satisfiable; the model must specify what is to be optimized and whose resource consumption is to be minimized the task has only one solution, or for performance evaluation. planning Different theories of collaborative required the agents belief coordination is to not require divide up the plan requiring agreement on how the subcomponents coordination, mutually reflect different views of the degree of for agents have a collaborative plan. The minimal approach [ 35,531. Rather agents to establish mutual beliefs at all and separately plan each component, without are planned. At the next level of belief to be the plan level of belief coordination, the intended acts of the collaborative [51,79, 125, 1271. At the highest into subcomponents it is common to require intended I7 This ih a simpiitication since the form of the acceptance determines the endorsement type on the mutual belief that is added to the beliefs database. ” This represents that both agents are committed to the option while the binding of the “option specifies the agent who will execute the option. h4.A. Walker/Artijcial Intelligence 85 (1996) 181-243 199 agents must both mutually support of the plan. In addition, the intended the plan such as the WARRANT it is possible intend all intentions beliefs to require actions will satisfy should also be mutually believed. and mutually believe any beliefs that provide reasons that inferences that for adopting a step about other goals that In this work, the assumption is that the degree of belief coordination in the experiments is discussed below will be and the experiments will vary whether intentions, required that are derived from explicitly discussed level intended, a feature of the task. The minimal that all intentions must be mutually WARRANTS, and inferred must also be mutual. Task determinacy intentions have an effect on communicative and fault because tolerance they are directly planning in the plan. If a partial plan has some utility, collaborative tolerable constructing measure of the quality of the final plan can be determined in the plan, and that partial plans can also be evaluated, satisfiable. a partial plan related in choice to how much uncertainty is then making a mistake or only that a from the utility of each step so that the task is more or less I assume is not catastrophic. For task determinacy, With respect to evaluating performance, I assume that the agents are working as a team, and as a team they attempt dialogue and minimize planning team’s performance from Clark’s assumption purpose with LEAST COLLABORATIVE with other approaches that it maximizes that conversants [ 351. A final choice has to do with which processes their own expected utility the team’s consumption in dialogue attempt EFFORT [ 18,21,22]. in which agents only participate in communication in a collaborative to optimize the of resources. This follows to achieve their dialogue This approach contrasts to the degree collaborative effort consists of. A is the primary efficiency measure ASSUMPTION. Since all types of IRUs to i.e. (2) (3) planning, in memory; and is defined with reference in collaborative stored beliefs interpreting utterances; required effort is that the number of utterances is the FEWEST UITERANCES in this work collaborative and to all the processes this assumption, assumption this common [ 15,471; violate the agent architecture ( 1) retrieval processes necessary related communicative reasoning processes by other agents. With respect are those communicative and inferences filtering. Collaborative processes are the combined processes that access AWM, the plan to access previously to generating and to the IRMA architecture that operate on beliefs stored in memory and those communicated (Fig. 3), retrieval processes databases, and generation of messages, and the costs for both agents for all of these processes: library and the beliefs and intentions processes of deliberation, means-end are the modules effort includes for perception reasoning, COLLABORATIVE EFFORT = (the total cost of communication for both agents) +(the total cost of inferences for both agents) +( the total cost of retrievals for both agents). Collaborative This definition process. Given of the quality of the problem effort is defined for the whole dialogue and not on a per utterance basis. the specification of the plan evaluation is the difference between a measure and the other assumptions the above definitions, support performance solution and COLLABORATIVB EFFORT. 700 M.A. Walker/Artij?cial lntelli,qence 8.5 (I 996) 1X1-243 PERFORMANCE = QUALITY OF SOLUTION - COLLABORATIVE EFFORT. Since the agents’ desires are simply to maximize utility, the quality of the solution is measured by the utility of the resulting plan with respect to the agents’ utility functions. 4. Design-World 4.1. Methodological basis of Design- World Design-World is a testbed the model of collaborative for a theory of collaborative planning the use of the Design-World in dialogue discussed in developing testbed theory of communication in collaborative as an instance of a general method, and then describes planning, which communication, in- in Section 3. In and testing a de- first describes the testbed and its parameters. The method can be this section stantiates order to motivate feasible Design-World implementation characterized as well as the task and communication by the steps below: (i) Generate hypotheses about the features of a model of collaborative planning dialogues from a statistical analysis of human-human dialogue corpora. (ii) Produce a functional characterization of the model, specifically that could affect task outcome, or claims about the including the efficacy of the (iii) the model as a testbed so that (some of) independently motivated modules these parameters can be for other aspects of parameters model. Implement controlled, while using testbed. The hypotheses were discussed resource bounds, for belief coordination, effort or increase The next step C iv) Test the hypotheses and the resulting model against different situations controlled by parameter settings. that were generated by the statistical analysis of the dialogue corpora of in Section 2. These hypotheses that under constraints are roughly task inferential complexity, communicative task fault tolerance. and task requirements choices to include IRUs can reduce collaborative plan. the quality of solution of the collaborative is to produce a functional characterization of the model features of a model of collaborative (a partial planning choices. resource the claims that motivated limits and communicative ). In Section 3. I discussed formalization In that interact with an agent’s autonomy. work, the model is the final result of the research. However, this leaves non-experimental In formal the model empirically the model and need to be made, and it is not always characterizations, many simplifying assumptions clear that the results carry over to complex domains where the simplifying do not hold. While the model presented here is empirically based on statistical analysis of a corpus of naturally occurring dialogues, many of the hypotheses discussed above can only provide weak to models of agents’ processing. Corpus analysis are related is desirable support for these hypotheses. Thus another source of empirical verification in order to develop a well-specified for the Design-World theory. This is the motivation and defeasible assumptions unverified. testbed. M.A. Walker/Artificial Intelligence 85 (I 996) 181-243 201 to consider the parameters Next, it is necessary that could affect the outcome or claims about the efficacy of the model and then implement the model as a testbed so that at least some of these parameters can be controlled. The use of independently motivated modules for other aspects of the testbed guarantees and in the small” that the testbed also makes that the testbed actually is a case of “experimentation tests something, it less likely 1541. Parameters that affect the efficacious use of IRUs have already been discussed: complexity, and requirements these for belief coordi- for resource bounds, and is implemented is independently limited agents, which task inferential resource bounds, include nation. The AWM model introduces a parameter as part of the IRMA architecture for resource motivated duces in simulation many well-known the utterance In addition, research on collaborative corpora [ 12,17,106,113,120,127,139,143]. [ 7,991. The AWM model itself is also independently motivated, since it repro- [ 76,118]. results on human memory and learning acts and their effects are independently motivated by other planning dialogues and by the statistical analysis of dialogue related to tasks, the testbed To introduce parameters two agents must form a collaborative where in the rooms of a two-room house. The task is based on cooperative design for experiments available inferential errors and usefulness of partial solutions. task plan as to how to arrange some furniture tasks used cooperative work for which a corpus of dialogues was ( 1) for (2) degree of belief coordination is designed around a simple task can be varied along [ 1421. However, three dimensions: on distributed complexity; the simple tolerance required; (3) These three task dimensions inferential the task by increasing of agent communication dimensions tasks. Section 4.2 will introduce the task variations. will describe enable us to generalize algorithms represent very different complexity provides in simple versus tasks. For example, varying on the performance complex information inferentially from the specific task in Design-World tasks. The to real-world the Standard version of the task, and then Section 4.4 To introduce parameters related to the interaction of communicative choice with task complexity tion strategies tive choice parameters The experiments and resource limits, agents are designed IRUs. Section 4.5 will describe so that they vary their communica- the communica- to include or not include that will be used in the experimental reported in Section 5 will examine limits; (2) communicative strategies; and (3) in the testbed have several ( 1) resource iments be implemented; vide empirical verification of hypotheses about the function of particular communicative that provided by corpus analysis and researcher’s strategies beyond tion describes of the collaborative this domain, the domain, the communicative strategies and the task variations. intuitions. This sec- in flaws in the model; and they highlight potential ( 1) they demonstrate the implementation planning model functions: (2) results presented in Section 5. the interaction of three factors: task definition. The exper- that the model can they pro- (3) 4.2. Design- World collaborative planning domain In Design-World, two artificial parameterizable the design of the floor plan of a two-room house agents converse in order to agree on [ 133,142]. The DESIGN-HOUSE plan 203 MA. Wdker/Art~fic~rul Intellipxce 85 (1996) IXI-243 SJ 54 KEY: F = FUCHSIA C; = GREEN P= PURPLE P El G PI a G 53 ROOM # I IDESIGN WORLD (‘OLLABORATIVE PLAN: 434 points Fi g. 5. One tinal state for Design-World Standard task: represents the collaborative plan achieved by the dialogue, 434 points. the agents requires the beginning of the simulation, plan and that it requires of furniture negotiated via a (simulated) that can be used to agree on how to DESIGN-ROOM-1 and DESIGN-ROOM-T. At both agents know the structure of the DESIGN-HOUSE in each room. Each agent has 12 items design plan final collaborative four furniture items in the plan. A potential is shown dialogue Each furniture in Fig. 5. item has a value that is used both as a WARRANT to support deliber- and as the basis for calculating in Section 3. The values items range over furniture the these values will be used to test hypotheses about ation (the utility of an act that is under consideration), the QUALITY OF SOLUTION of the final plan. as discussed on the furniture items range from 10 to 56, and both agents’ these values. Since beliefs about function of IRUs, both agents know the values of all the furniture of the dialogue. Design-World ning algorithm about the furniture OPTIONS-these item furniture artificial language from that language agents negotiate a collaborative discussed in italics for part of designing into a room. This items that can be used in the design. Means-end options are the content of PROPOSALS to the other agent is illustrated by dialogue that the agents communicate with and a gloss automatically plan- reasoning reasoning generates in Section 3. Each agent carries out means-end to PUT a the items at the beginning the collaborative plan following (12). which generated room- I. includes (12) (1) (2) (agent-bill BILL: First, put the green rug it1 the study. ( Propose agent-bill put-act KIM: Then, let’s put the green lamp itz the study. (Propose agent-kim put-act agent-kim option- 10: green rug room- 1 ) ) agent-bill option-33: green lamp room-l (agent-kim ) ) M.A. Walker/Art@cial Intelligence 85 (1996) 181-243 203 (3) (4) (5) (6) agent-kim option-45: green couch room-l ) ) let’s put the purple couch in the study. agent-bill option-56: purple couch room- 1) ) BILL: Then, let’s put the green couch in the study. (Propose agent-bill put-act (agent-bill KIM: No, instead (Reject agent-kim put-act (agent&m BILL: PUTTING IN THE GREEN COUCH IS WORTH 56. (Say agent-bill score (option-45: BILL: (Reject agent-bill (agent-bill put-act agent-kim bel-265: (agent-bill It is better to put the green couch in the study. agent-kim option-56: green couch room- 1) ) green couch room-l) put-act 56)) agent-kim of the dialogue, At the beginning that (score green rug 56). When she receives Bill’s proposal as shown to accept or reject evaluates evaluating the score proposition in memory. Thus for supporting deliberation. to decide whether to retrieve the scores of furniture in order she will attempt about that proposal the proposal the propositions in memory has stored the proposition in ( 12.1), she it. As part of stored earlier items are WARRANTS PRINCIPLE discussed to ACCEPT or REJECT it [ 34,135]. Proposals As discussed in Section 3, the agents retain to be implicitly ACCEPTED because both want to agree on a plan for designing agent deliberates whether are inferred follows ACCEPTED, either proposal becomes a mutual 1311. from the COLLABORATIVE or explicitly, implicitly intention that contributes Agents REJECT a proposal if deliberation even though their autonomy the agents the house. Thus, on receiving a proposal, an (1) and (2) [ 139,143]. This is that was the content of the [ 101,113, in Section 3. If a proposal to the final design plan they are not rejected then the option a better option, based on evaluating generated by means-end in (12.3), for pursuing rejection as a counter-proposal nicated PRINCIPLES [ 1391. When an agent (12.5) and (12.6), ( 12.5), agent-bill agent-kim’s reminds agent-kim proposal. reasoning. For example, option-45, is based on observations and proposes option-56 leads them in (12.4) Kim rejects to believe the utility of the competing that they know of they have options the proposal instead. The form of the is commu- PLANNING as in In of the value of the green couch, before rejecting to reject another agent’s to support rejection, its proposal. in the COLLABORATIVE about how rejection information intends in naturally occurring dialogue as codified the agent includes additional 4.3. Agent architecture implementation in Design- World The agent architecture used in the Design-World simulation ified IRMA architecture, aspects of the architecture that AWM is implemented, For the experiments in Fig. 3 and discussed shown that are specific to Design-World and the way that belief deliberation are the plan library, is implemented. below, the total size of AWM is set to 16, but memory retraces If the path of the memory pointer around, and there is no overwriting. is wrap- its steps environment is the mod- in Section 3 [ 7,991. The only the way 204 M.A. Wulker/Artijic~iul lntellipnce X5 (1996) I;Yl-243 so that the current memory simply added. Thus memory capacity is unbounded. loci already has something stored in it, the new item is Since hypothesis A4 relates to the degree be able to compare Thus. all experiments make comparisons over three ranges of AWM the performance settings; of agents who are more or less attention to which AWM is limited. we want to limited. strategies search radius parameter varies from LOW communicative between different the AWM (radius of 3 and 4), lo MID AWM agents arc severely attention (radius of 6 and 7) to HIGH AWM (radius 01 limited agents, wherease almost AWM 1 I and 16). LOW AWM everything an agent knows is salient for HIGH AWM agents. The is not salient, and deliberation. of belief deliberation. the agent can accept to Landauer’s AWM model. The limits on AWM plays a critical role in determining bcr that only salierzt beliefs can be used in means-end for a proposal that if the warrant if the agent only knows of one option, proposal. However, proposal on the assumption show the impact of resource implementation agents’ performance. Remem- so reasoning the agent cannot properly evaluate a the is better than doing nothing. Section 5 will that any option limits on performance. For more details see [ 76.1351. that are already present. Thus an agent’s belief deliberation for the purpose of Design-World, was is tied directly ever deleted or modified. Rather, new beliefs are added which effectively compete with process depends on beliefs and applying an algorithm collecting a set of related beliefs which may be contradictory, about to determine what the agent believes is an emergent property of the belief the state of the world supercedes old information retrieval mechanism: the stochastic aspect of retrieval means to believe to decide “out-of-date” in the world. As we will see in that, in casts where the outdated beliefs were encountered with Section 5, this means greater frequency repeatedly, agents who can access all of their beliefs are more likely beliefs recently added are more likely to be retrieved. However ( see [ 39.431 ) The fact that new information to believe out of date beliefs.” and “forget” recent changes In that model, nothing that it is possible and thus stored for an agent propositions, in memory in memory to decide stored The plan library contains domain plans as discourse plans in detail discussed for the discourse acts shown in Section 4.5. 1.1. Design- World tasks for Design-house as well in Fig. 4. The discourse plans will be and its subgoals, The Design-World task as a plan is simple since it involves to higher level goals. as shown which contribute modified difficult testbed the performance according to the general to perform well. These modifications task, and affect the degree evaluation. in Fig. 6. However, task features discussed are applicable above so that to other tasks besides to which different aspects of the task contribute linear planning of subgoals the task is easily it is more the to I5 It i\ unclear whether mance. However predict. For example, I used them the previous evening this prediction of the belief deliberation algorithm it is easy to think of examples of humans making the kind of error I commonly believe (falsely) that I have eggs at home in the refrigerator. is consistent with human perfor- that this model would even though for quiche M.A. Walker/Artijcial Intelligence RS (1996) 181-243 205 DESIGN HOUSE DESIGN ROOM- 1 DESIGN ROOM-Z Fig. 6. Standard version of the task, fault tolerant and partial solutions acceptable. There are four versions of the task that will be used to test the hypotheses Matched-pair, fault tasks are more difficult because tolerant and requires they task is inferentially The other in Section 2: Standard, Zero-non-matching-beliefs, Standard coordination. belief coordination beliefs and Matched-pair inferential complexity. The Zero-invalids required, and magnify simple, task is fault intolerant. the effect of mistakes. The Zero-non-matching- tasks each explore different aspects of belief coordination and introduced and Zero-invalids. The low levels of belief the degree of increase Standard task low The Standard levels of belief coordination. for each valid step in the plan. task provides a baseline and is inferentially fault tolerant and the requires so that QUALITY OF SOLLJTION for a particular dialogue consists of the sum of all the furniture the task is defined so that partial pieces items in a room is a valid plan, rather solutions items. This choice about task than requiring determinacy makes of different restrictions. resource that each room must have all four furniture to see the gradient effect on performance are possible. Any number of furniture The Standard In addition, it possible is defined simple, task In addition the Standard task is fault tolerant. If agents make a mistake in planning invalid steps in their collaborative plan, subtracted from the score. Thus for making mistakes due to inserting the point values for invalid steps in task agents are that are not in the Standard steps in plans task and insert the plan are simply not heavily penalized actually executable. The Standard those by means-end options, and act-effect relies on one premise: the premise ?option) supports multiple premises about processing is inferentially reasoning inferences the premise simple because to generate options, after committing (has ?agent ?item) supports deliberation, (score ?item ?score) inferring to be simultaneously effort in the Standard salient. However, task by making inferences the agent’s only those by deliberation are to evaluate to an action. Each of these inferences reasoning, (intend A B requires to test hypotheses it easier to access these inferential supports means-end and the premise it is possible the effect of ?option. Thus, none of these processes M.A. Wulker/ArrQiciul Inlelli&xcr 8-S (1996) 181-243 DESIGN ROOM- I I>ESICiN ROOM-2 \’ ” \ \’ \’ ” I /’ Fig. 7. Tasks can differ as to the level of tnutual beliet required. Some tasks require that W. a reason for doing I? is mutually believed and others don‘t. It is also possible to test the effect of resource limits since these premises premises. must be accessible to perform optimally on the task. The degree of belief coordination in the Standard to coordinate intentions on the intentions corresponding are always explicitly discussed task is low because agents are in is always to put-acts as shown so that coordination only required Fig. 6. These achieved. The Zero-non-nzatchitlg-hrlirj~ task The Zero-non-matching-beliefs task increases agents to base their deliberation for adopting an intention requiring the same WARRANTS shows the structure of beliefs about In the Zero-non-matching-beliefs also be mutually believed. This is not generally because agents A and B can mutually believe necessarily agents have only one option under consideration. of that one option in order to decide whether agreeing on what that utility intentions task, as shown, the warrants underlying required and warrants the degree of belief coordination by process on the same beliefs. They must have in order to do well on this task. Fig. 7 goal. intentions must in forming a collaborative plan that they have maximized utility without in the general case, when the utility they do not need to evaluate for the Design-house is. Furthermore, to accept or reject it. The Zero-non-matching-beliefs AS, introduced perform well on the task, where the beliefs are those used in deliberation. in Section 2 by increasing task provides a basis for testing hypotheses Al and to the degree of belief coordination required task models particular tasks since for agents to agree on the reasons for carrying out a particular types of real-world The Zero-non-matching-beliefs it is not always necessary action. For example, company, any agreement An agreement pay is possible because the company’s on a plan, but have different in the negotiation that is reached between is agreed to by each party for different the union and the management reasons. for a shorter work week is supported by the union because more overtime for those who want to work more and is supported by the management of a insurance premiums will be lower. However, reasons for doing so, they may change if two agents agree their beliefs and M.A. Walker/Artijcial Intelligence 85 (1996) 181-243 207 MATCHED-PAIR 2 KEY: P = PUT(A,F,R,T) Fig. 8. Making additional inferences: Matched-pair-two-room task. Each PUT intention contributes both to a Design-room goal as well as a Matched-pair goal. under different conditions. in which agents agree on both for doing those actions. Under The most stable, long-term, collaborative to be performed, the actions these conditions as the agents will be should task examines one extreme of belief revision to revise their intentions in a compatible way and intention the Zero-non-matching-beliefs for deliberation. intentions their plans will be those well as the reasons more likely be simpler. Thus coordination Matched-pair tasks the number of independent Another aspect of belief coordination inferences. There are two task definitions creasing working memory. These are: ( 1) Matched-pair-same-room, room. Fig. 8 shows PUT a furniture ing a matched pair goal. A Matched-pair inference of a Matched-pair the Matched-pair-two-room item in a room can potentially that increase is two furniture contribute premises has to do with coordinating inferential that must be simultaneously complexity beliefs based on by in- in available and (2) Matched-pair-two- version of the task. Each intention to another to intention of achiev- items of the same color. The is based on the minor premises shown in (13): ( 13) (a) (b) (c) (Intend A B (Put ?agent ?item-1 ?room-1 ) ) (Intend A B (Put ?agent ?item-2 ?room-2)) (Color ?item-I ) (Color ?item-2) ) (Equal is more demanding for resource limited agents than the processing this inference in the Standard Making needed the plan, available. they must access tasks, could contribute accessed, and furthermore, premise. the WAFZRANT task. In the Standard task, in order the agents must access at least one belief about a furniture In order evaluate to properly the w ARRANT the option for that option. represented In contrast, to agree on one step of they have item item by that furniture in both Matched-pair that in (13) must also be items for both beliefs must be accessed for both furniture to a Matched-pair goal. In addition, each version of the Matched-pair the premises task requires one additional i i Fig. 9. In Zero-invalids task. invalid steps invalidate the whole plan tasks two Matched-pair in the rooms. Matched-pair-same-room The difference same or different (Equal ?room- 1 ?room-2) while Matched-pair-two-room that NOT (Equal ?room-1 ?room-2). Because premise memory at the time that a proposal one room before starting another, to be salient in the that the additional premise in the agents always complete the necessary premise shown in ( 13a) is more likely requires ( 13a) is inferred and stored the matches are the additional premise in the Matched-pair-same-room is whether requires is accepted, and because task. As discussed in Sections I and 7, WC wish to provide a test of hypothesis A2, the and examine how it affects the coordination of inference inference constraint, discourse in collaborative inference should our measure of inferential to draw an inference, both the Matched-pair-same-room room task increase planning. Hypotheses A3 and A5 together interact with the agent’s ability complexity complexity. inferential imply that the complexity of to stay coordinated on inferences. Since required premises is the number of independent task and the Matched-pair-two- Evaluating the QUALITY of: SOLUTION for the Matched-pair inferences on inferences, since both Matched-pair phasis on coordinating agents make the Matched-pair task measures how well agents are coordinated on the inferred from the intentions to Matched-pair are counted the sum of the utilities of the two furniture (50 points). that were explicitly agreed upon. Only the intentions tasks reflects the em- that both in order to score points for matched pairs. The that follow that contribute tasks require intentions in the final solution. and the utility of these intentions is items, plus the utility of the Matched-pair Zero-itlvalids task The Zero-invalids task is fault intolerant task is that any mistake invalids determinacy: while less than 8 items that would be counted as valid solutions valid solutions in the Zero- is a feature of task there are still many possible 8 step plans, all of the solutions with task are not (see Fig. 9). The assumption the whole plan. This for the Zero-invalids for the Standard invalidates task. M.A. Walker/Artificial Intelligence 8.5 (1996) 181-243 209 This intolerance. tasks, a mistake can invalidate task is an example of one extreme of fault the invalid step may be adequate. For example, tolerant a task is depends on the interdependency solution. For some partial solutions without furnishing make a mistake and assume the room building and the whole plan may be invalid executed. furnished a tower, each step depends on the successful In general, how fault of different subparts of the problem for other tasks, the whole solution, in a task like to have both a couch and a chair, but if the agents room, in a task such as step if a step to put down a foundation block cannot be they can use a chair that will end up in a different and usable. On the other hand, a room it may be desirable execution of the previous is still partially that it would Note that an agent can reject another agent’s proposal based on believing add an invalid step to the plan, as shown in the rejection utterance act schema in Section 3.2. Since agents have to agree on each step of the plan, an invalid step can only be inserted for into the plan the plan do not hold. if both agents have failed that the preconditions to remember 4.5. Varying communicative strategies in the types of utterance acts that the discourse acts are composed that controls how agents participate the discourse act schema Section 3 discussed dialogue, and discussed of. Which utterance acts a discourse act decomposes STRATEGIES which codify different communicative discourse placing different expansions action. Agents are parameterized of discourse plans Varying an agent’s communicative strategies provides ses about the potential benefits of IRUs. Varying act is the basis of the four communicative Close-consequence; (3) Explicit-warrant; of these strategies are hypothesized limits, under solution Figs. show a plan operator the assumptions about into depends on COMMUNICATIVE choices for how to do a particular strategies by for different communicative in their plan libraries. strategies and (4) Matched-pair-inference-explicit. the basis for testing the hypothe- the degree of explicitness of a discourse (2) All resource and the definition of quality of (1) All-implicit; tested below: to mitigate agents’ attentional and inferential their architecture acts and the integration for the task. lo-13 by Walker and Rambow discourse Sidner’s theory of discourse precisely defined by the collaborative in Section 3. Each discourse act, such as a proposal, represents CONTRIBUTOR achieving for each strategy. These operators draw on work [ 1381, and also make use of Moser and Moore’s definitions of and Grosz and in the plan operators are planning model and agent architecture discussed is composed of a CORE act which the primary purpose of the act such as a propose utterance act, as well as a the likelihood of act, such as a warrant, whose purpose the intention of the core [ 92,145,146]. The predicates is to increase [50,89,93,94]. of rhetorical structure theory (RST) All-implicit strategy The All-implicit strategy in which a PROPOSAL decomposes the plan operator trivially in Fig. 10. This strategy is an expansion of a discourse plan to make a PROPOSAL, act of PROPOSE. See in (3) choice shown to the communicative is the communicative 210 MA. Wdker/Arti’c~rtd Intelligence 85 (I 996) 181-243 NAME: EFFECT: CONSTRAINTS: (?speaker, ?hearer, ?act) Proposal-All-implicit (desire (and ‘?hearer (do ?hearer ?act) ?utility-act) (option ?act) (salient ?hearer (utility ?act ?utility-act) ) ) CORE: (propose ?speaker ?hearer ?act ) Fig. IO. The IJKOPOSAI. plan operator tbr an All-implicit agent in Section 1, and provides a baseline strategy CONSTRAINT. The experiments below will compare All-implicit discussed below. In dialogue ( 12) in Section 4.1. both Design-World strategy with the performance of agents using that is consistent with the REDUNDANCY the performance of agents using the the other proposal strategies agents communicate using the ( I ). (2). and (3). As in proposals. information strategy, and the proposals are shown in utterances All-implicit Fig. IO shows, the All-implicit leaving it up to the other agent strategy to retrieve the All-implicit includes no additional them from memory. (2) and reasoning The constraints on using below, agents are parameterized an OPTION generated by means-end to the hearer. In the experiments consistently, hearer knows that proposal and deliberate whether the hearer will accept or reject the proposal depends on other options knows about. Clearly the proposal does not specify for all proposal operators. strategy are that ( I ) the proposed ?act is is SALIENT that the utility to use this strategy the is that the hearer will evaluate the act. However, the hearer these other options. Thus the effect of that the action will be intended by the hearer. This holds so that an agent using is always salient. The effect of the proposal the degree strategy assumes everything the speaker cannot predict the hearer desires the All-implicit to which The All-implicit strategy can be used by agents in Section 4.4, since the agents are capable of making tasks inferences or accessing to fill in what has been left implicit with this strategy. Other inferences drawn IO. For example, leaving to infer which other intention makes a match with the option discussed memory by the hearer from the proposal utterance act are not shown an agent can use the All-implicit it up to the other agent currently under consideration. in tither of the Matched-pair in any of the Design-World strategy in Fig. tasks, ( 14). agent CLC uses the Close-consequence Close-consequence In dialogue for this strategy statements, segment. A contributor makes the inference explicit study, they no longer have the green rug (act-effect strategy. The plan operator is explicit CLOSING such as C I4.2), on the completion of the intention associated with a discourse to CLC’s CLOSING discourse act is an IRU such as ( 14.3) : CLC the green rug in the that since they have agreed on putting I I. The core of the strategy inference). is shown in Fig. ( 14) ( 1) BILL: Then. let’s put [he greetz rug in the study. (Propose agent-bill put-act (agent-bill agent-clc option-30: green rug room- I ) ) M.A. Walker/Ar@cial lnrelligence 85 (1996) 181-243 211 NAME: EFFECT: CONSTRAINTS: CONTRIBUTOR: CORE: ( ?speaker,?hearer,?act) Close-consequence (and (effect ?act ?effect) ) ?act) ) ) (salient ?hearer (be1 ?hearer (intend ?speaker ?hearer ?act) (open-segment (closed-segment ?act) ) (and (say ?speaker ?hearer (close ?speaker ?hearer ?act) (effect ?act ?effect) ) Fig. Il. The CLOSING plan operator for a Close-consequence agent. (2) CLC: So, we’ve agreed to put the green rug in the study. (Close agent-clc agent-bill (agent-bill put-act intended-30: green rug room- 1) ) (3) CLC: AND WE NO LONGER HAVE GREEN RUG. (agent-bill (Say agent-clc agent-bill bel-48: hasn’t green rug) ) the naturally The Close-consequence models made explicit located at the close of a discourse of the Design-World occurring tasks discussed strategy of making example inferences in Section 2.2. In both cases an inference explicit at the close of a segment is that follows from what has just been said, and the inference is sequentially segment. This strategy can be used by agents in any in Section 4.4. explicit, inferences The Close-consequence the agents go on to the next phase of the plan, strategy will be used to test hypothesis C2 about potential the All-implicit and will be contrasted with (12) in Section 4.2 that the infererategy will be explicit, and inferences strategy where no closing acts are produced. benefits of making strategy where no closing acts are produced. Note in dialogue leaving both used to test hypothesis C2 about potential benefits of making will be contrasted with the All-implicit (12) Note the inference of both acceptance the plan, make. However, Close-consequence the experiments not difficult capability in Section 4.2 that both the agents go on to the next phase of to in are to make. See [ 1371 for a discussion of experiments which vary an agent’s for the other agent is not a good test of other hypotheses because both agents always make act-effect in dialogue leaving to make these inferences. and these inferences and closing inferences, Explicit-warrant The Explicit-warrant strategy varies the proposal discourse in each proposal. The plan operator in (15). Remember RANT IRUs the dialogue excerpt for adopting utility of the proposal, which are mutually believed at the outset of the dialogues. (15), that a WARRANT and here WARRANTS are the score propositions act by including WAR- is given in Fig. 12 and exemplified by is a reason for an intention that give the In the WARRANT IRU in (15.1) contributes (core act) in (15.2). to the proposal the intention, ( 15) ( 1) IEI: PUTTING (Say agent-iei score (option-2: IN THE GREEN RUG IS WORTH 56. agent-iei2 bel-2: put-act (agent-iei green rug room-l) 56) ) NAME: EFFECT: Proposal-Explicit-warrant (?speaker, ?hearer, ?act) (and (desire ?hearer (do ‘?hearer ?act) ?utility-act) ) ) ( salient ?hearer ( utility ?act ?utility-act) CONSTRAINTS: ( and (option ?act ) CONTRIBUTOR: CORE: C not-salient ?hearer (utility ?act ?utility-act) ) ( say ?speaker ?hearer (utility ?act ?utility-act) ( propose ?speaker ?hearer ?act ) ) ) Fig. I?. The I’KOPOSAI. plan oprralor for an Explicit-warrant agent. ( 2 ) IEI: Thelen, let’s put the greet1 rug in the study. (Propose agent-iei agent-iei2 option-2: ) ) put-act (agent-iei green rug room-l in Fig. 12 specilies that an effect of using The plan operator utility of the proposal option the Explicit-warrant in deliberation, involved with determining which facts are relevant from memory. A constraint on using the Explicit-warrant of the proposal act is not already salient. the is salient. Since warrants are used by the other agent the processing them is that the utility strategy can save the other agent for deliberation and retrieving plan operator this plan is that in natural dialogues as shown in the naturally occurring example in In the experiments below. agents are parameterized with the result that an agent using the Explicit-warrant is never salient maintain strategy also occurs dialogue C 2). a dynamic model of what is salient for the hearer. See 1641 for experiments This strategy can be used by agents to use this strategy consistently, that the warrant strategy assumes to in which an agent attempts for the other agent. The Explicit-warrant in Section 4.4. The Explicit-warrant IRUs to support produce Attention It can also be used is related In the Standard agents. hood that agents coordinate steps. In the Zero-non-matching to the degree task this is predicted in any of the Design-World tasks discussed strategy provides a test of hypothesis Al: agents beliefs and intentions. the processes of deliberating to test hypothesis A4: to which an agent the choice is resource to produce an Attention limited in attentional beliefs their beliefs about to improve the performance task this strategy should of resource increase the warrants underlying IRU capacity. limited the likeli- different plan The Matched-pair-inference-explicit strateg The Matched-pair-inference-explicit strategy expands acts. See Fig. 13. The contributor intended, while the PROPOSAL discourse act of the proposal consists of the core is a propose utterance act, as to two communicative statement about what is already in (16.6) followed by (16.7) in one turn: ih ” The names of agents who use the Matched-pair-inference-explicit strategy are a numbered version of the string “IMI” which stands for Implicit acceptance, matched inference. M.A. Walker/Ar@cial Intelligence 85 (1996) 181-243 213 NAME: EFFECT: CONSTRAINTS: CONTRIBUTOR: CORE: (and ( ?speaker, ?hearer, ?act 1) (do ?hearer ?actl ) ?utility-act) (intend ?speaker ?hearer ?act2) ) ) Proposal-matched-pair (desire ?hearer (and (salient ?hearer (option ?act 1) (matched-pair (salient ?hearer (not (salient ?hearer (intend ?speaker ?hearer ?acd))) (say ?speaker ?hearer (intend ?speaker ?hearer ?acd)) (propose ?speaker ?hearer ?act 1) (utility ?act 1 ?utility-act) ?act 1 ?acd) ) ) Fig. 13. The PROPOSAL plan operator for a Matched-pair-inference-explicit agent. ( 16) (6) (7) agent-imi (agent-imi2 IM12: WE AGREED TO PUT THE PURPLE COUCH (Say agent-imi2 put-act IM12: Then, let’s put the purple rug in the living room. (Propose agent-imi2 put-act agent-imi option-SO: purple rug room-2) ) purple couch room-l ) ) intended-5 1: (agent-imi2 IN THE STUDY. The statement in (16.6) is an IRU, because it realizes information previously inferred the IRU in dialogue by both agents, and models is the variation discussed used in the Matched-pair constraint. As Fig. 13 shows, a constraint on using has inferred a Matched-pair in Section 1 in choice (4). This strategy tasks as a way of testing hypothesis A2, the discourse is only intended to be inference this plan is that the speaking agent ( 11) . Matched-pair-inference-explicit for the option being proposed. Although this strategy strategy is specifically for making premises test of a general inferentially ences. For example, the clauses (Generates complex, and which also require agents to generalize this strategy for (Matched-pair ?act 1 A ?acd ?act3), where the generates ?actl Note that the effect of using this strategy pair inference, rather A constraint on using agents parameterized with for the hearer. See [64] for experiments model of the other agent’s attentional the effect is that the premise this strategy is that this premise this strategy always assume state. to other cases of plan-related tied to Matched-pair for inferences inferences, salient, to remain coordinated it provides a in tasks that are on infer- inferences, ?act2) could be replaced with the more general [ 33,971. the Matched- is salient. inference is not already salient. However, is not salient that the premise in which agents attempt to maintain a dynamic is not that the hearer makes for the desired is to be inferred relation 4.6. Plan evaluation Section 3.3 specified a model of how collaborative plans are evaluated in terms of QUALITY OF SOLUTION in order to be able to measure Section 4.4 defined quality of solution examine in performance trade-offs and COLLABORATIVE EFFORT. Design-World the quality of a solution as well as collaborative is constructed effort. tasks. We want to for all of the Design-World between strategy choices. 214 M.A. Wulker/Artrficrd lntelli~ence 85 (1996) 181-243 that IRU effort. Thus, It is obvious can be related trade-offs versus the total cost of inference versus these total cost of communication retrieval for both agents’ collaborative cannot simply add up the number of retrievals, that makes an inference a Consequence becomes part of the discourse model. However, anyway, inference without processing the potential benefit of Attention effort for retrieval while not increasing communication be beneficial. This hypothesis the benefit of the strategy the Consequence the extra utterance of the Consequence to calculate collaborative to the relative contributions of the total cost of effort, we inferences and messages. Consider that that an inferred belief explicit ensures if the inference would have been made the the cost of IRU. A similar argument holds for IRU reduces overall an Attention effort to the same degree, it will IRU would have been greater is dependent upon whether the effort to make in a general form. IRUs. Whenever is given below than l HYPOTH-I 1: Strategies solution are beneficial. that reduce collaborative effort without affecting quality 01 This hypothesis convenience from Section 3.3: follows directly from the definition of performance repeated here for PERFORMANCE = QUALITY OF SOLIJTION - COLLABORATIVE EFFORT. We need processes because implementation are parameterized of inference; then defined as: to introduce parameters for the effort involved with each of the component they are not strictly comparable, and because dependent. Thus agents’ by ( 1) COMMCOST: retrieval, inference and communicative cost of sending a message; and ( 3) RETCOST: cost of retrieval from memory. Collaborative these modules are costs (2) INFCOST: cost effort is COLLABORATIVE EFFORT= (COMMCOST x total messages for both agents) +(INFCOST x total inferences for both agents) + (RETCOST x total retrievals for both agents). to explore is free; (2) when retrieval effort dominates other processing three extremes effort dominates other processing We will use these cost parameters processing when communication modeling various varying plan library and working memory are implemented. Varying in which communication models situations the values of these parameters experimental the cost of retrieval models different assumptions rather than the absolute values. instantiations outcomes. planning of the agent architecture given the utilities of the steps in this space: ( 1) when costs; and (3) support in Fig. 3. For example, about how the beliefs database, costs. The parameters is very costly. The relation between in the plan determines the cost of communication and As an example of the effect of varying the plots of performance shown in Figs. 14 and 1.5 for LOW, MID and HIGH AWM. In these figures, is plotted on the x-axis and number of simulations level distributions distributions performance are given by bars on the y-axis. The performance the increase in QUALITY OF SOLUTION that we would expect with increases in Fig. 14 demonstrate in AWM, these costs, consider at that performance MA. Walker/Artijcial Intelligence 85 (1996) 181-243 Fig. 14. Performance All-implicit HIGH AWM agent% agents when all processing for dialogues between two are for LOW, MID and x n x distributions showing the effect of AWM parameterization is free. The three performance distributions E P a ADL It _ x 11 Fig. 15. Performance dialogues between agents. AWM retcost = 0.001. distributions two All-implicit showing agents. The three performance the effect of increased retrieval cost for each AWM for are for LOW, MID and HIGH range distributions 116 M.A. Wulker/Arr$ctal /ntelli#enc~e 8.5 (1996) 181-243 . costs. I7 Fig 15 shows what happens when processing given no processing is not free: here a retrieval cost of 0.001 means that every memory access reduces quality of solution that the utilities of plan steps range between 10 and by I/ 1000 of a point in reasoning the whole beliefs database 56). As Fig. 15 shows, similarly since HIGH AWM agents perform does not always MID AWM agents. improve performance the ability (remember to access to 21.7. Summuc: mupping from naturully ocwrring data to Design- World experiments Section 2 proposed hypotheses about the function of IRUs in human to human col- dialogues. and then Section 3 presented a model for collaborative in Section 2. Section 4 then described as a testbed of the model. and Sections 4.4 and 4.5 introduced a number dialogues based on the observations laborative planning planning Design-World of parameters of the testbed that are intended dialogues and support between the naturally clarify the basis for the experiments The testbed and the experimental dialogues to model the features of the human-human testing of the hypotheses. Here I wish to summarize occurring and the design of the testbed the mapping in order to in the next section. parameters are based on the following mapping be- examples communication in naturally occurring the mapping of a WARRANT is tied to the agent architecture. Third, memory CAWM) which has been shown limits on these processes are modeled by extending relation between an act and a belief strategy collaborative planning dialogues and the testbed. First, the planning aspects of human processing are modeled with the IRMA architecture, the IRMA architecture to model the processing of relation is modeled as seen in the the mapping assumes in tween human-human and deliberation and resource with a model of attention/working a limited but critical set of properties of human processing. Second, dialogue between an act and a belief with a WARRANT Explicit-warrant that arbitrary relation is such as those required tasks such as those in based on the assumption that task difficulty ( 1 ) inferential com- the financial advice domain can be related to three abstract features: (3) plexity as measured by the number of premise a degree of belief coordination that plan; and (3) it is reasonable planning dia- logues by using domain plan utility for a measure of the quality of solution and defining the cost to achieve in natural dialogues to content-based for doing well on the Matched-pair for making an inferences; inferences and beliefs underlying inferences tasks. Fifth, the mapping task determinacy to evaluate the performance of the agents that solution as collaborative and fault tolerance. Finally in Section 4.5. Fourth, such as that discussed required on intentions, the mapping assumes t I 1) can be mapped in naturally occurring effort, appropriately in Design-World in Design-World in collaborative parameterized. content-based such as (9) to example inferences required ” These distributions that 200 runs would guarantee than or equal and S greater approximately 133 samples different were found. strategies was tested approximate Beta distributions stable results. The Beta distribution with the largest variance, [ I44 1, and this approximation was used to determine for parameters R to I. is the uniform distribution. This largest variance distribution would require [ I IS, 144 1. An empirical evaluation of the adequacy of this sample size for three runs of 100: no differences showed up in alternate to see if any differences M.A. Walker/Artijicial Intelligence 85 (1996) 181-243 217 The details of this mapping and provides orative planning results excellent environment critical aspects of human-human to the human-human for testing specifies how the testbed implements the basis for extrapolating the model of collab- from the testbed experimental dialogues that are being modeled. The testbed provides an that the model captures to the extent the hypotheses dialogues. 5. Experimental results 5.1. Statistically evaluating performance between resource the interaction The experiments varies AWM over task, 200 dialogues tasks, communication examine limits. Every experiment and AWM MID, and HIGH. In order to run an experiment on a particular communicative for a particular AwM model parameter yields a performance agents hypothesized agents no collaborative setting are shown for very resource (MID), each dialogue simulation has a different strategies three ranges: LOW, strategy the result. The AWM limited agents (LOW), and resource unlimited (with agents for each AWM for QUALITY OF SOLUTION range are simulated. Because (HIGH). Sample performance from runs of two All-implicit effort subtracted) for each AwM is probabilistic, to be similar distributions distribution to human agents in Fig. 14. To test our hypotheses, we want to compare the performance task, under different asssumptions for a particular costs. To see the effect of communicative municative strategies limits and processing the whole range of AWM settings, we first run a two-way analysis of variance with AWM as one factor and communication whether: (1) AWM alone nication strategy alone is a significant there is an interaction factor in predicting performance; factor in predicting performance; between communication strategy as another. ‘* The anova strategy and AWM. is a significant (anova) tells us (2) commu- and (3) whether of two different com- about resource strategy and AWM over is. Furthermore, whenever strategy aids or hinders performance, However, anova alone does not enable us to determine the particular Awh4 range at which a communication and many of the hypothe- ses about the benefits of particular communication strategies are specific to how resource limited an agent for effects of one value of AWM and negatively strategy cannot be seen from the anova alone. Therefore, we conduct planned compar- [ 701, within each isons of strategies using [ 25, TO]. I9 On AWM range setting is BENEFICIAL for a the basis of these comparisons we can say whether a strategy particular to determine which AWM range the strategy affects task for a particular AWM range. strategy affects performance for another value of AwM, the modified Bonferroni test (hereafter MB) the potential positively ‘s The experimental performance distributions are not normal and the variance is not the same over different samples, however anova is robust against the violation of these assumptions under the conditions in these experiments [ 25,701. I’) According to the modified Bonferroni test, the significant F values for the planned comparisons reported below are 3.88 for p < 0.05, 5.06 for p < 0.025, 6.66 for p < 0.01, and 9.61 for p < 0.002. 21x A strategy A is BENEFICIAL as compared range, significantly in the same task situation, with the same cost settings, than the mean of B. according greater to the modified Bonferroni if the mean of A is test to a strategy B, for a particular AWM (MB) test. The converse 01‘ BENEFICIAL is DETRIMEN~‘AL: in the same A strategy A is DETRIMENTAL. ;IS compared range. is significantly ( MB ) test. less than the mean of B. according task situation, with the same cost settings. to the modified Bonferroni if the mean of A test to a strategy B. for LI particular AWM Strategies ference between both BENEFICIAL and DETRIMENTAL strategies are compared over. i.e., a strategy may be beneficial and detrimental riced not be tither BENEFIC‘IAL, or DETRIMENTAL. there may be no dif- two strategies. Also with the definition given above a strategy may be on the range of AWM that the two for LOW AWM agents for HIGH AWM agents. depending strategy 1 and strategy 2. In the comparisons A DIFFERENCE PLOT such as that in Fig. 16 is used to summarize two strategies, Close-consequence, 2 is the All-implicit are plotted on the y-axis against AWM ranges on the x-axis. Each point represents range. These plots summarize a comparison of below, strategy 1 is either and strategy two strategies in the plot in the means of 200 runs of each strategy at a particular AWM strategy. DiJfemrzces in performance means between from 1200 simulated dialogues. or Matched-pair-inference-explicit *O Explicit-warrant. the information the difference 5.2. Standcud task for a DESIGN-HOUSE plan, constructed via the dialogue, task is defined so that the QUALITY OF SOLUTION that is the sum of tolerant because that the Standard Remember agents achieve the utilities of each valid step in their plan. The and is fault subtracted mistakes. Furthermore, are required these cases, to make these inferences. premise. to make are those for deliberation the point values All-implicit agents do fairly well at the Standard from the score, with the effect that agents are not heavily penalized the task has low inferential complexity: task has multiple correct solutions for invalid steps in the plan are simply for making the only inferences agents In both of to access a single minor and means-end reasoning. agents are only required processing is free, as shown in the performance increase, HIGH AWM agents don’t do as well as when retrieval too much effort on retrieval during collaborative costs they expend HIGH AWM distribution HIGH AWM agents have in Fig. 14 with that the potential to benefit task, under assumptions that all plot in Fig. 14. However, as retrieval is free, because the task, that in Fig. 15. Thus for the Standard strategies from communication planning. Compare ?‘) In experiments with Close-consequence because the use of this strategy one agent will ever produce a closing statement in the simulations. is constrained only one agent in a dialogue uses the Close-consequence strategy is open. See Fig. 1 I. Since only for any dialogue segment, only one agent is given the option to when the dialogue segment M.A. Walker/Artificial Intelligence 85 (1996) 18/-243 219 the total effort reduce the task has minimal for deliberation, which agents, since the task is fault not contribute such as Close-consequence Below we will compare Close-consequence tolerant, strategy. for retrieval, when inferential the Explicit-warrant they might otherwise make non-optimal complexity, retrieval agents are still penalized easy access is not free. In addition, although that is used to information strategy provides, could benefit LOW AWM although decisions. Furthermore, for making errors since errors do strategies task, communication the number of errors could be beneficial. to performance. Thus for the Standard that can reduce the All-implicit strategy to the Explicit-warrant strategy and the Explicit-warrant The Explicit-warrant strategy can be used IRUs to support to test hypothesis beliefs and It can also be used to test hypothesis A4: the choice to produce an Attention in attentional capacity. strategy will result in higher performance that they can access is free by ensuring Al: agents produce Attention intentions. IRU is related Thus one prediction is that the Explicit-warrant for LOW AWM agents even when processing the warrant and use it in deliberation, in the Standard the processes of deliberating to the degree to which an agent is resource limited task thus making better decisions. in the performance means between the Explicit-warrant Fig. 16 plots the differences the effect of AWM and the Explicit-warrant strategy and the All-implicit anova exploring shows that AWM has a large effect on performance is no main effect for communicative an interaction strategy for LOW, MID and HIGH AWM agents. A two-way task strategy for the Standard (F = 336.63, p < 0.000001). There there is (F = 1.92, p < 0.16). However, choice (F = 1136.34, p < 0.000001). between AWM and communicative strategy By comparing performance within a particular AWM range for each strategy we can (MB) test show the modified Bonferonni interact with communicative see which AWh4 settings using neither beneficial implicit strategy, MB(HIGH) = 0.39, ns). Note that there is a trend towards being detrimental The hypothesis nor detrimental if all processing at MID AWM. based on that the Explicit-warrant is the All- in the Standard is free (MB(LOW) = 0.29, ns; MB(MID) = 2.79, ns; strategy in comparison with the Explicit-warrant strategy task, strategy. The planned comparisons the corpus analysis was strategies that include Further analysis of this result suggests a hypothesis not apparent IRUs. However, this hypothesis that LOW AWh4 agents might is from the limited that is more for resource from working memory is useful for deliberation, effect of an IRU can be cancelled IRUs may displace other information any beneficial from communicative benefit disconfirmed. corpus analysis: agents because useful. In this case, despite making options. When agents are very resource important salient displaces the warrant the fact that the warrant information information that can be used limited making an optimal decision to generate other is not as as being able to generate multiple options. The Explicit-warrant that reduce collaborative 11: strategies prediction proposal, when accessing memory has some processing is that by providing the Explicit-warrant cost. strategy can also be used in the Standard task to test hypothesis effort overall may be beneficial. Thus, another a proposal with every the warrant used in deliberating strategy has the potential to reduce resource consumption 220 M.A. Wulker/ArtiJiciul h~ell~gr~~e 8.5 (I 996) 181-243 cost - iei-iei2 bill-kim C= 0 , I = 0 , R = 0 Fig. 16. If processing I of two Explicit-warrant infcost = 0, retcost = 0. is free. Explicit-warrant is neither beneficial nor detrimental for all AWM settings: strategy agents and strategy 7- of two All-implicit agents: task = Standard, commcost = 0. cost - iei-iei2 bill-kim C= 1 , I = 1 , R = 0.01 Fig. 17. Explicit-warrant costs: strategy commcost = I. infcost = I. retcost = 0.0 I. 1 is two Explicit-warrant is beneficial for MID and HIGH AWM agents when retrieval dominates processing task = Standard, agents and strategy 2 is two Ail-implicit agents: at each AWM strategy, for LOW AWM agents because the Explicit- = 86.43, p < 0.002). for HIGH AWM M.A. Walker/Artificial Intelligence 85 (1996) 181-243 221 Fig. 17 plots the differences the All-implicit and strategy retrieval effort dominates and the Explicit-warrant processing, 0.000001). O.Ol), and an interaction p < 0.000001). The planned shows There in the performance means between strategy for LOW, MID and HIGH AWM the Explicit-warrant agents when processing. A two-way anova exploring strategy for the Standard that AWM has a large effect on performance is also a main effect for communicative the effect of AM task, when retrieval cost dominates (F = 330.15, p < (F = 5.74, p < (F = 1077.64, between AWM and communicative strategy choice comparisons using in the Standard the MB test to compare performance task, in comparison with the All-implicit range show that, the Explicit-warrant strategy (MB(LOW) warrant The Explicit-warrant agents ( MB(HIGH) beliefs necessary with each proposal, As an additional strategy = 0.27, ns). However, hypothesis is neither beneficial nor detrimental 11 is confirmed is beneficial for MID AWM agents (MB(MID) strategy also tends towards improving performance = 2.07, p < 0.10). For higher AWM values, for deliberating the proposal are made available this trend is because the in the current context so that agents don’t have to search memory test of hypothesis 11, a final experiment for them. strategy costs. Fig. 18 plots in a situation where the differences tests the Explicit-warrant the cost of communica- in the performance strategy and the All-implicit agents when communication the effect of AWM and the Explicit-warrant strategy for LOW, MID effort dominates processing. A two-way for the Standard strategy that AWM has a large is also a main effect for between AWM effort dominates processing, (F = 409.52, p < 0.000001). There (F = 28.12, p < O.OOOOOl), and an interaction shows (F = 960.24, p < 0.000001) the All-implicit other processing the Explicit-warrant strategy against tion dominates means between and HIGH AWM anova exploring task, when communication effect on performance communicative and communicative The planned strategy choice comparisons using strategy effort dominates = 0.12, ns). However, the MB test to compare performance in this situation, when communication is neither beneficial nor detrimental strategy at each AWM processing, range show that for MID AWM agents the Explicit-warrant for both ( MB(MID) LOW and HIGH AWM agents = 39.65, p < 0.01) . Since this strategy includes an extra utterance with every proposal and provides no clear benefits, effort dominates Zero-non-matching-beliefs task has low coordination this situation with that in the task, we will see that this is due to the fact that the Standard requirements. to performance processing. Below, when we compare = 7.69, p < 0.01; MB(HIGH) task when communication the Explicit-warrant in the Standard it is detrimental is detrimental (MB(LOW) Close-consequence The Close-consequence strategy of making task to a measure of “how to test hypothesis C4: Standard related task is fault tolerant, every invalid step reduces Making explicit decreases inferences error. important” act-effect the choice the inference inferences to produce explicit can be used in the is IRU the Standard the quality of solution of the final plan. this kind of the likelihood of making a Consequence though is. Even ‘22 M.A. Walker/Art$cid Inrelli~ence 8.5 (1996) 181-243 IX. Explicit-warrant Fig. high: strategy commcost = IO, infcost = 0, retcost = 0. is detrimental I is two Explicit-warrant for LOW and HIGH AWM agents when communication agents and strategy ? is two All-implicit agents: task = Standard. effort is cost - clc-kim bill-kim C= 0 . I = 0 , R = 0 Fig. 19. Close-consequence HIGH AWM agents. Strategy 1 is the combination and strategy 2 is two All-implicit can be detrimental agents. in the Standard task for LOW AWM agents and beneficial of an All-implicit agent with a Close-consequence for agent task = Standard, commcost = 0, infcost = 0, retcost = 0. M.A. Walker/Art#cial Intelligence 85 (1996) 181-243 223 between the Close- task, when all process- The difference plot in Fig. 19 plots performance differences consequence strategy and the All-implicit ing is free. A two-way anova exploring strategy in this situation shows that AWM has a large effect on performance p < 0.000001 >, and that there is an interaction between AWM and communicative (F = 919.27, p < 0.000001). strategy, in the Standard the effect of AWM and the Close-consequence (F = 249.20, choice (MB(LOW) Planned consequence agents more to performance utterances of displacing is no difference comparisons strategy between is detrimental strategies for each AWM in comparison with All-implicit = 8.70, p < 0.01). This that make inferences for agents with LOW AWM explicit in the Close-consequence facts that could be used in means-end range shows that the Close- for LOW AWM is because generating options contributes than avoiding errors, and the additional strategy has the effect to generate options. There reasoning However, comparisons between in performance for MID AWM agents the two strategies (MB(MID) = 0.439, for HIGH AWM agents shows that ns). strategy the probability the belief deliberation in comparison with All-implicit is because agents choosing ( MB(HIGH) the Close-consequence is beneficial algorithm = 171.71, p < 0.002). See Fig. 19. This to believe out of date beliefs of HIGH AWM increases to have invalid steps about the state of the world. The result is that they are more likely the reinforcing in their plans. Thus the Close-consequence that agents will believe belief item. This result is not predicted by any hypotheses, but that they still have that furniture as discussed in Section 4.3, this property of the belief deliberation mechanism has some intuitive appeal. In any case, this result provides a data point for the benefit of a strategy for making inferences if that inference explicit when the probability of making an error increases item has been used makes is beneficial because that a furniture is not made. it less likely strategy 5.3, Zero-non-matching-beliefs task Remember lief coordination (WARRANTS). based inferences, we will compare Explicit-warrant that the Zero-non-matching-beliefs task requires a greater degree of be- by requiring agents 21 Thus it increases to agree on the beliefs underlying deliberation the importance of making particular deliberation- and can therefore be used to test hypotheses Al, A4 and A5. Below the the performance of agents using strategy with strategy in the Zero-non-matching-beliefs the All-implicit task. Fig. 20 plots the mean performance differences of the Explicit-warrant in the Zero-non-matching-beliefs strategy All-implicit the effect of AWM shows that AWM has a large effect on performance is also a main effect for communicative interaction between AWM and communicative and communicative strategy strategy for the Zero-non-matching-beliefs (F = 47 1.42, p < 0.000001) strategy and the task. A two-way anova exploring task, . There (F = 379.74, p < O.OOOOOl), and an (F = 669.24, p < 0.000001). choice 2’ Remember that in other tasks, agents do not have to agree on WARRANTS they know of only one option, the proposal. Thus when agents have limited AWM, they may accept a proposal without having retrieved warrant. in situations in order to be able to decide they do not need to retrieve the warrant in which to accept the because match - iei-iei2 bill-kim C= 0 , I = 0 , R z 0 LOW MID *TTENTIONmORKING MEMORY J HlGH Exphcit-warrant is beneficial for Zero-non-matching-beliefs task for LOW and MID AWM agents: is two task = Zero-non-matching-beliefs, strategy I Explicit-warrant agents and strategy 7 is two All-implicit agents: commcost = 0. infcost = 0, retcost = 0. match - iei-iei2 bill-kim C= 10 , I = 0 , R = 0 Fig. 7 I Explicit-warrant is beneficial for Zero-non-matching-beliefs task, for LOW and MID AWM agents, even when communication cost dominates processing: strategy I is two Explicit-warrant agents and strategy ? is two All-implicit agents: task = Zero-non-matching-beliefs commcost = IO. infcost = 0, retcost = 0. M.A. Walker/Artificial Intelligence 8S (1996) 181-243 225 Comparisons within each AWM range of the two communicative strategies in this task strategy is highly beneficial that the Explicit-warrant = 260.6, p < 0.002; MB(MID) for HIGH AWM agents MB(HIGH) for LOW and MID AWM = 195.5, p < 0.002). The strategy = 4.48, p < 0.05). When agents strategy they may fail to access a warrant. The Explicit-warrant that the agents always can access the warrant for the option under discussion. shows agents (MB(LOW) is also beneficial are resource guarantees Thus, even agents with higher values of AWM can benefit from this strategy, since the task requires Hypothesis such a high degree of belief coordination. 11 can also be tested in this task. We can ask whether limited, to make it inefficient the benefits of the Explicit-warrant high enough strategy over All-implicit. However, drive the total effort for communication Explicit-warrant strategy be reduced for this task are so strong for LOW and MID AWM agents even when communication that they cannot = 246.4, p < 0.002; is high = 242.7, p < 0.002). See Fig. 21. In other words, even when every extra by agents using Explicit- in MB(MID) WARRANT message 10, if the task warrant do better. Contrast Fig. 21 with the Standard Fig. 18. collaborative is Zero-non-matching-beliefs, effort by 10 and reduces performance task and same cost parameters increases resource (MB(LOW) limited cost However, when communication cost is high, the strategy becomes detrimental it is possible to choose to the HIGH AWM agents warrants and the increase does not offset the high communication in belief coordination cost. (MB(HIGH) = 7.56, p < 0.01). These agents can usually afforded by the Explicit-warrant 5.4. Inferential tasks: Matched-pair for access strategy in Section 4.4 (1) increase the the degree of belief coordination that follow from intentions as the Matched-pair-two- should be easier to make in the Matched-pair-same-room the same inferences The two versions of the Matched-pair of the task and tasks described increase (2) increase complexity agreed upon. Both inferential tasks agents do fairly well at making Matched-pair task requires inferential required by requiring agents to be coordinated on inferences that have been explicitly small degree: All-implicit Matched-pair-same-room room task, but these inferences since the inferential The Matched-pair and A5. The Attention inference-explicit salient, are that this strategy should be beneficial predictions AWM agents, but that HIGH AWM agents can access the necessary without Attention stronger premises are more likely tasks provide an environment that is used to test these hypotheses this strategy makes the premises IRUs. Furthermore, we predict of agents making Matched-pair that the beneficial the likelihood to be salient. increasing strategy; strategy task. thus for testing hypotheses A2, A3, A4 for Matched-pair is the Matched-pair- inferences inferences. The for MID inferential premises effect should be for LOW and possibly difficulty to a inferences. The for the Matched-pair-two-room the performance Fig. 22 plots pair-inference-explicit exploring has a large effect on performance agents the effect of AWM and communicative strategy (F = 323.93, p < 0.000001). There differences between All-implicit for the Matched-pair-same-room agents and Matched- task. A two-way anova in this task, shows that AWM is no main effect 710I I-0nII0nII0     M.A. Walker/Artificial Intelligence 85 (1996) 181-243 227 for communicative and communicative strategy choice (F = 0.03, ns), but there is an interaction between AWM (F = 1101.51, p < 0.000001). Comparisons within AWM ranges between agents using the All-implicit strategy in the Matched-pair-same-room strategy and agents using the Matched-pair-inference-explicit task (Fig. 22) show that Matched-pair-inference-explicit AWM agents or HIGH AWM agents). recently small and is restricted (MB(LOW) = 4.47, p < 0.05), but not significantly In the Matched-pair-same-room inferred and is likely to still be salient, to very resource limited agents. strategy is beneficial for Low for either MID task the content of the IRU was is relatively effect different thus the beneficial In contrast, in the Matched-pair-two-room task, the effect on performance of the larger, as we predicted. Fig. 23 plots the Matched-pair-inference-explicit of agents using strategy is much differences Matched-pair-inference-explicit the mean performance strategy and those using the All-implicit to achieve A two-way anova exploring shows that AWM has a large effect on performance is a main effect for communicative between AWM and communicative the same levels of mutual strategy choice the effect of AWM and communicative strategy. The All-implicit inference as Matched-pair-inference-explicit strategy agents do not manage agents. in this task, (F = 171.79, p < 0.000001). There (F = 57.12, p < 0.001)) and an interaction (F = 567.34, p < 0.000001). Comparisons within AWM ranges between agents using the All-implicit strategy and the Matched-pair-inference-explicit agents using task (Fig. 23) show that Matched-pair-inference-explicit MID and HIGH AWM agents (MB(LOW) MB(HIGH) increasing inferences = 38.85, p < 0.002). the ability of LOW, MID and HIGH AWM agents in the Matched-pair-two-room In other words, task. strategy in the Matched-pair-two-room is beneficial strategy for LOW, = 21.94, p < 0.01; MB(MID) = 7.71, p < 0.01; in this strategy is highly effective to make Matched-pair to be beneficial for LOW and possibly for MID AWM agents for inferences which they would otherwise be the effect of the hypothesized DISCOURSE INFERENCE for HIGH AWM agents. the it to be beneficial is due to the fact that, in the case of higher AWh4 values, We predicted the strategy to access. This confirms it gives agents access to premises because unable CONSTRAINT. However, we did not expect This surprising Matched-pair-inference-explicit the proposing agent intended other words, when agents have HIGH AWM in a situation effect strategy keeps the agents coordinated on which inference In and a they can make divergent inferences, in which multiple are possible. inferences of making strategy Thus on the corpus analysis alone. the strategy controls inferential premises salient inferential processes improves agents’ inferential coordination. in a way that was not predicted based Hypothesis 11 can also be tested the effort for communication drive Matched-pair-inference-explicit in this task. We can ask whether high enough strategy over All-implicit. to make it inefficient it is possible to choose to the Fig. 24 plots communication strategy munication = 10.46, p < 0.01). required coordinating cost the mean performance cost is high. Comparisons within each AWM differences between these two strategies when this for LOW, MID and HIGH AWM agents even with a high com- = 19.10, p < 0.01; MB(MID) = 3.94, p < 0.05; MB(HIGH) that in which this strategy was not beneficial. This result to find a task situation it would be difficult range shows In other words on inference (MB(LOW) that is still beneficial 238 M.A. Wtrlkrr/Arr1,t2c,rul lmlli,qmce X5 (1996) IX/-213 mp - imi-imi2-mpr bill-kim-mpr C= 10 , I = 0 , R = 0 in the Matched-pair-two-room I\ hcneficial for LOW. MID and HIGH AWM Fig. 24. The Matched-pair-lnfc’erence-explicit \tratrgy task even with communication cost of IO. Strategy I is two agents Matched-pair-inference-explicit agents and strategy 7 is two All-implicit agents, task = Matched-pair-two-room. commcost = IO. infcost = 0. retcost = 0. is strong the prevalence of this strategy support for the DISCOURSE INFEIENCB CONSTRAINT. which may explain in naturally occurring dialogues [ 30, 109, 1411. 5.5. Zero-invalids tusk that the Zero-invalids Remember any invalid an environment explicit by the Close-consequence invalidates intention for testing hypotheses C2 and C4 with respect strategy. the whole plan. Thus task is a fault intolerant version of the task in which task provides to the inferences made the Zero-invalids Fig. 25 plots the mean performance and agents using differences the All-implicit strategy consequence task. A two-way anova exploring task, shows that AWM has a large effect on performance There interaction between AWM and communicative is a main effect for communicative strategy between agents using the Close- strategy in the Zero-invalids strategy in this (F = 223.14. p < 0.000001). (F = 75.81, p < O.OOl), and an choice (F = 103.38, p < 0.000001). the effect of AWM and communicative The Close-consequence strategy was detrimental agents. Comparisons within AWM and agents using there are no differences Zero-invalids beneficial (MB(LOW) for MID and HIGH AWM the Close-consequence in performance task ranges between agents using the All-implicit strategy for LOW AWM in the Zero-invalids agents in the Standard task for LOW AWM strategy task show that intolerant strategy = 26.62, p < 0.002; MB(HIGH) in the fault the Close-consequence is agents (MB(MID) = 3.64, ns). However, M.A. Walker/ArtiJicial Intelligence 85 (1996) 181-243 229 inval - clc-kim bill-kim C= 0 , I = 0 , R = 0 w- Fig. 25. Close-consequence is the combination agents, task = Zero-invalids, of an All-implicit agent with a Close-consequence commcost = 0, infcost = 0, retcost = 0. is beneficial for the Zero-invalids task for MID and HIGH AWM agents. Strategy 1 agent and strategy 2 is two All-implicit = 267.72, p < 0.002). In other words, the robustness make mistakes. This is a direct result of rehearsing unlikely this strategy process by decreasing of the planning that attention is highly beneficial in increasing the frequency with which agents it inferences, making the act-effect limited agents will forget these important inferences. 6. Discussion task. In Section 3, I presented a model of collaborative action can be designed in communicative in the context of particular to features of a col- in planning that can affect either the efficacy of the in Section 5, I planning process. Then testing hypotheses about the effects of these param- to the development dialogues. These results contribute This paper showed how agents’ choice limits the effect of their resource mitigate laborative planning dialogue and discussed a number of parameters final plan or the efficiency of the collaborative presented the results of experiments eters on collaborative of the model of collaborative testbed implementation easily inter alia. into other dialogue planning incorporated planning planning is compatible with many current theories, dialogue presented here. In addition, the these results could be since algorithms [ 17,48,5 1,53,79,88,127], A secondary goal of this paper was to argue for a particular methodology theory development. The method was specified was introduced in Section 4 and Sections 4.4 and 4.5 described in Section 4.1. The Design-World the parameterizations for dialogue testbed 230 MA. Wulker/Arti’crul Intelli,pence X5 (1996) 181-233 testing that support of the model strategies were tested: and Standard; (2) Zero-non-matching-beliefs; Three situations of varying processing (4) Matched-pair-inference-explicit. the hypotheses. Four parameters ( 1) All-implicit; C 2) Close-consequence; for communicative (3) Explicit-warrant; Four parameters C 3 ) Matched-pair for tasks were tested: ( 1) (MP) ; (4) Zero-invalids. In this section, effort were tested. the hypotheses I will first summarize in Section 6.1, then I will discuss how the experimental situations not implemented 6.5 consists of concluding in the testbed. Section 6.4 proposes remarks. and the experimental results might generalize results to future work and Section 6. I. Suttwtuq of results The hypotheses that were generated by the statistical analysis of the dialogue corpora are repeated below for convenience from Sections 2 and 4.6. l HYPOTH-Cl: the inference l HYPOTH-C2: agents produce Consequence IRUs to demonstrate that they made that is made explicit. agents choose to produce Consequence IRUs to ensure that the other agent has access to inferrable information. l HYPOTH-C3: the choice measure of “how hard” the inference to produce a Consequence is. to produce a Consequence IRU is directly related to a IRU is directly related to a l HYPOTH-C4: the choice measure of “how important” the inference is. l HYPOTH-C5: the choice to produce a Consequence IRU is directly related on the inferences to the that degree to which they have made. the task requires agents to be coordinated l HYPOTH-Al: agents produce Attention IRUs to support the processes of deliber- ating beliefs and intentions. is a DISCOURSE INFERENCE CONSTRAINT whose effect is there in dialogue are derived from propositions that are currently discourse l HYPOTH-A2: that inferences salient (in working memory ). l HYPOTH-A3: inferential to make task-related complexity inferences. l HYPOTH-A4: which an agent l HYPOTH-AS: the choice is resource the choice the choice to produce an Attention IRU is related of a task as measured by the number of premises to the degree of required to produce an Attention IRU is related to the degree to limited in attentional capacity. to produce an Attention IRU is related to the degree to that they have the task requires agents to be coordinated on the inferences which made. l HYPOTH-I 1: strategies solution are beneficial. Below I will summarize to the hypotheses above. that reduce collaborative effort without affecting quality of the experimental results reported in Section 5 with respect Hypotheses C3 and C4 were tested by comparing the Close-consequence the All-implicit made explicit by the Consequence strategy in the Standard task. In this experimental setup, IRU was neither hard to make nor critical strategy with the inference for per- M.A. Walker/Art#cial Intelligence 85 (1996) 181-243 231 this inference. The results formince. Hypothesis C3 was only weakly always make Strategy is detrimental information IRU is not “hard enough”. from working memory for LOW AWM agents. This is because and because tested by the experiments because agents in Fig. 19 show that the Close-consequence IRUs can displace useful the inference made explicit with this the is not as critical as it The Standard task also provides a weak test of hypothesis C4. The fact that that making task is fault the inference it is made. At lower values of AWM, Standard tolerant means might be. However, errors can result from either not making the probability it once high. However, for HIGH AWM agents thus Standard the inference or forgetting is not that of such errors in Fig. 19 show that the probability of error is higher and in the in this case, because of their belief deliberation for HIGH AWM agents, even the Close-consequence the results shown is beneficial algorithm, strategy task. The Zero-invalids test of hypothesis C4 by increasing task provides another tance of the inference made explicit by the Close-consequence that hypothesis C4 is confirmed because the Close-consequence LOW, MID and HIGH AWM agents. In addition is beneficial task, this strategy their scores by ensuring to improve the impor- strategy. Fig. 25 shows for strategy is beneficial for the Standard to the reasons discussed they have more potential for HIGH AWM agents because that they don’t make errors. did not test hypothesis Cl because agents The experiments to actively monitor evidence designed have made. Hypothesis C5 was not tested by the experiments rectify they reject proposals whose preconditions if they detect a discrepancy do not hold. the situation from other agents as to what inferences in beliefs about act-effect in the testbed are not they might because agents always inferences: Hypotheses Al, A4 and A5 were tested by experiments in which the Explicit-warrant for LOW AWM agents. Fig. 16 shows that the Explicit-warrant strategy was compared with the All-implicit is disconfirmed is neither beneficial nor detrimental processing resource is free. This counter-intuitive limited, for LOW AWM agents for the Standard strategy in the Standard task. Hypothesis Al strategy task, when result arises because, when agents are highly that is more useful. IRUs can displace other information strategy To test hypothesis is not free. When communication 11 in this situation, we also examined two situations where pro- the for LOW and HIGH AWM agents. However, is the Explicit-warrant for MID AWM agents and there is a trend toward a beneficial effect for HIGH show that hypothesis 11 is confirmed: processing cessing Explicit-warrant retrieval when beneficial AWM agents. Thus these two situations effort has a major effect on whether a strategy other processing other processing cost dominates cost dominates is detrimental is beneficial. strategy costs, costs, We also tested hypotheses Al, A4 and A5 with experiments in which the Explicit- task (see Figs. 20 and 21). This warrant strategy was compared with the All-implicit beliefs deliberation-based in order for the Explicit-warrant tion effort. Thus inferences, include Attention strategies which strategy, which task agents by requiring in situations inferences to do well on the task. In this situation, we saw a very large beneficial is not diminished in which agents are required by increasing to be coordinated communica- on these IRUs can be very important. strategy in the Zero-non-matching- the importance increases of making to be coordinated on these inferences effect 13’7 MA. Wulker/Arr(jiciul lnfelligmce 85 (1996) 181-243 Hypotheses A2, A3, A4. and A5 were tested by experiments strategy with the All-implicit shown in Figs. 22 and 23 provide strategy included an unpredicted comparing the Matched- in the two versions of the for these benefit of Attention support agents’ performance tasks where agents must coordinate on inferences. Fig. 23 improves with the Matched- IRUs rather than divergent strategy. This can be explained by the fact that Attention that agents will make the Same inference, are possible. inferences the Matched-pair-inference-explicit tied it provides a test of a general strategy for making premises complex and require agents to remain is specifically strategy salient, when tasks are inferentially these results also complex the likelihood task. The results pair-inference-explicit Matched-pair hypotheses. However IRUs for inferentially shows that both MID and HIGH AWM pair-inference-explicit increase inferences, when multiple although inferences, Furthermore, to Matched-pair for inferences coordinated on inferences. Thus ENCE CONSTRAINT. the clauses can be replaced with a more general A ?act2 ?act3), where the generates To generalize in the strategy plan operator it provides strong support for the DISCOURSE this strategy to other cases of plan-related refer to Matched-pair that specifically inference. relation e.g. the more general (Generates is to be inferred [ 33,51,97]. INFER- inferences. inferences ?actl Hypothesis 11 was tested by examining extremes about in the Standard effort whenever a hypothesis I8 shows that high communication communication confirmed. Fig. strategy detrimental does not eliminate beliefs task. Fig. 24 shows that high communication of the Matched-pair-inference-explicit the strategy of making premises processing strategy for inferences the benefits of the Explicit-warrant effort. in cost ratios for retrieval effort and effects of IRUs was the beneficial effort can make the Explicit-warrant task. Fig. 21 shows that high communication effort strategy in the Zero-non-matching- effort does not eliminate in the Matched-pair-two-room salient is robust against extremes the benefits task. Thus in 6.2. Generalizability of the results This section addresses concerns raised in 1541 that simulation is “experimentation in the small”. Hanks writes that [54, Section 5.1.51: The ultimate value-arguably or otherwise order to do so the experimenter must demonstrate ( I ) that her results-the the designer of a system the otllJ value-of relationships inform ( 2) specification tics and world characteristics-extend problem she studied, that the solution when that same problem area is encountered and to the problem area she studied beyond experimentation that solves interesting problems. is to constrain In three things: she demonstrates between agent characteris- the particular agent, world, and in isolation will be applicable in a larger, more complex world, ( 3 ) that the relationship demonstrated experimentally actually constrains or some- how guides the design of a larger more realistic agent. The list in ( 1 )-( 3) are all different ways of saying that the results should generalize and this after all is a basic issue with all experi- beyond the specifics of the experiment, M.A. Walker/Artificial Intelligence 85 (1996) 181-243 2.13 across designed can be shown by a series of multiple experiments mental work. Typically generalizations modifying multiple variables as we have done here. For example, the task are specifically generalize manipulated briefly discuss why the results presented above are potentially generalizable. on generalizations agent architectural as those in Cohen’s “ecological to strategies tasks. However, we might also ask to what extent do the variables abstract out key properties of real situations? Below I will I will focus (2) are the same and (3) agent behaviors. These dimensions ( 1) task (or environmental) to test whether beneficial along properties; three dimensions: the modifications in the simulation communicative properties; triangle” [ 241. planning tasks. In addition, Generalizations about tasks The Design-World task was selected as a simple planning to test generalizability tasks, we examined more complex variants of the task by manipulating tion of each step. The structure of this task is isomorphic collaborative across abstract inferential quired for making a task-related on intentions, and fault tolerance of the plan. These general features can certainly be applied tasks in other domains. features could not be applied. complexity inference and (2) degree of belief coordination to think of a task or domain task that requires negotia- of many of hypothesized benefits three re- required the task determinacy as measured by the number of premises to other in which these and beliefs underlying In fact it is difficult to a subcomponent a plan; and (3) inferences features: (1) Generalizations about agent properties agents Design-World agents are artificial that are designed and deliberation and resource the IRMA architecture with a model of attention/working to model the resource aspects of human pro- limits on these processes limited qualities of human agents. The planning cessing are modeled with the IRMA architecture, are modeled by extending memory erties of human processing. The way that agents process dialogue architecture. (AWM) which has been shown set of prop- is tied to the agent to model a limited but critical The experimental as modeled by a size of memory to time to access memory. Artificial results will extend to dialogues between artificial agents to the extent similar cognitive properties. Here, we looked at a resource limit, however time agents that testbed would benefit from the strategies discussed here subset agents are often that artificial I would predict so it seems quite plausible strategies. For example, In addition, defining of the number of premises simultaneously artificial processors have when a computation inferences explicit” by communicating to the other agent might have been able inferential as a in memory bears a strong requires a large complexity correlated to memory from similar communicative in rapidly changing worlds, that those agents exhibit bound on access size is directly limited would benefit agents in the Phoenix simulation [ 241. In other work artificial agents do “make to other agents partial computations when make these computations direct consequence resemblance working set [ 1221. The experimental results should extend [ 24,36,130]. to problems agents because Design-World agents are designed to dialogues between humans and artificial it may be to model humans. However 334 MA Wulker/Artt,jicictl lnfelligerlcr X.5 (1996) 1X1-243 to change to allow the computer the definition of collaborative desirable interaction and for the human to do. Furthermore, most of the claims about the AWM model are based on a limited set of human working based memory properties, architectures effort for modeling human-computer that is easy for the computer these properties will also hold that is easy for the human such as SOAR 174,771. for other cognitively to handle processing to handle processing and to do strate- is Gerlerali:ations about agent behaviors In this work the agent behaviors that were tested were the agent communication gies. One reason to believe that they were based on observed strategies planning dialogues. and Design corpora that the strategies are general to human-human in different corpora of natural collaborative to find all three types of IRUs in the Trains, Map-task discourse It is possible [ 12,98, 128, 1421. as well as in the financial advice domain. In addition generalizations. to this empirical evidence, there are further reasons why we might expect The communicative acts and discourse acts used by Design-World agents are similar strategies based on these acts to those used in [ 12. 14, I 13. 1201. Thus communicative should be implementable in any of these systems. the strategies are based on general results based on these strategies should generalize The experimental situations because and underlying processes. the mapping of a WARRANT examples such as (9) was modeled with a WARRANT lief in Design-World. made about the use of the Explicit-warrant communication any dialogue planning domain where agents use warrants to other discourse relations between utterance acts and inference. For example, relation between an act and a belief in naturally occurring relation between an act and a be- strategy. The claims to strategy should generalize to support deliberation. such as supporting deliberation as seen in the Explicit-warrant communication Similarly. content-based inferences in natural dialogues ( 1 1) were modeled with content-based for doing well on the Matched-pair the DISCOURSE to premises INFERENCE that are currently such as that discussed inferences in Design-World situation tasks. This inferential in CONSTRAINT, that inferences in salient. Both experimental and evidence was provided in support of the discourse inference the use of the Matched-pair-inference-explicit constraint. communication to example relation such as those required to test was designed dialogue are restricted corpus-based The claims made about strategy. based on experimental where agents make premises where agents are required or planning. evidence, for inferences to make content-based should generalize and available, inferences strategy to any dialogue to any planning domain in support of deliberation The evaluation metrics applied to these strategies is a reasonable measure of the quality of solution should also generalize whenever for a dialogue domain plan utility task. 6.3. Relation to other work The model of collaborative previous work on cooperative planning dialogue dialogues presented [ 10,21,30,37,50,5 in Section 3 draws 1,67,85,97,98,141,143], from M.A. Walker/Art@cial Intelligence 85 (1996) 181-243 235 to other current research on collaborative planning [ 17, and the results are applicable 26,32,48,53,56,87,113,127,146]. The agent architecture and reasoning and means-end the model of deliberation into other work based on the work of [ 7,341, and on Pollack’s TileWorld simulation The use of IRMA as an underlying model of intention deliberation planning model was first proposed for a collaborative incorporated The architecture limited working memory, but most of the claims about recency and frequency properties, which might also be provided by other cognitively based architectures is consistent with that assumed those frameworks. The relationship such as SOAR [ 74,771. ** Since the testbed architecture in other work, is [ 991. environment to provide a basis and has been in includes a specific model of the model are based on its between discourse acts and domain-based results should be generalizable options and intentions the experimental [ 13 I-1331, [48,146]. to in to the [ 85,861 and is similar at each stage of the planning agents is based on the in the developed environment [ 12,14,127]. the belief this work is based on Litman’s model of discourse plans approach in process and theory of belief Automated Librarian project testbed reasoning mechanism and the multi-agent The emphasis on autonomy The Design-World revision of Design-World simulation environments: to optimize reasoning rapidly changing [ 14,38,40,88]. is based on the methods used in the TileWorld and Phoenix agent is a single agent rather than with another agent. limits on in which an artificial TileWorld to test a theory of the effect of resource robot worlds [ 24,54,99]. and planning the agent interacts with its environment, uses similar methods behavior between two agents. simulation attempts world in which Design-World communicative agent, strategies Design-World is also based on the method used in Carletta’s JAM simulation [ 12,100]. is for the planning how to get from one place for recovery for the Edinburgh Map-task JAM is based on the Map-task dialogue corpus, where the goal of the task the instructor, the reactive to instruct agent, the instructee, to another on the map. JAM focuses on efficient to their communicative and error recovery strategies. Given good error recovery strategies, Carletta are more efficient, but did strategies not attempt a way of the approach here provides quantifying what that a and the task definition determine when combination strategies are efficient. Future work could test Carletta’s claims about recovery strategies within is an effective or efficient limitations resource risk” communicative In contrast, from error and parameterizes agents according of the agents’ strategy, and to quantify framework. the results that “high efficiency. suggest argues this extended To my knowledge, the range of variation measured how communicative the ability of the conversants rative planning none of this earlier work has considered in communicative that affect choice, or the effects of different choices, or choice affects the construction of a collaborative plan and to stay coordinated. Nor have other theories of collabo- ideas about the agent architecture, or tested specific the factors been explicit about 22 [ 1361 discusses model of attentional which [49,50,114]. the AWM model may be useful. state the differences between an AWM-like attentional model and Grosz and Sidner’s See also [ 1071 for a discussion of other discourse phenomena stack for 336 M.A. Wulker/Ar@inl Intelligence X5 (I 996) 181-243 resource bounds municative argued in determining choice. that conversational in dialogue, In addition, no earlier work on cooperative and none have used utility as the basis for agents’ com- dialogue are major factors task-oriented agents’ resource effective conversational limits and task complexity in collaboration. strategies 6.4. Future work for future work is to investigate beneficial In agents avenue suggest A promising the experiments agents. of heterogeneous always parameterized with the same resource heterogeneous that strategies agents may be effective Attention knew about at the beginning homogeneous one agent the capabilities before for heterogeneous it forgets them. agents because is not limited, IRU strategy ones. For example. strategies for teams in dialogue were here, pairs of agents limits. Pilot studies of dialogues between for homogeneous that are not effective [ 1331 I tested an in they for if limited agent to exploit facts important information. However the other agent in which agents would tell one another about all the options is not beneficial each room. This strategy of planning IRUs can displace other useful then it can be helpful for the resource of the more capable agent by telling Another extension would be to extend the agent communication strategies or to test additional ones. For example, other work proposes a number of strategies for information that these strategies are in dialogue and provides some evidence selection and ordering [ I 1, 17. 123, 147 1. Support for these claims could be provided by efficient or efficacious Design-World in which agents used these strategies to communicate. experiments Future work could also modify to make Design-World more like TileWorld by making the properties of the world or of the task. For example, the world it would be possible change in the course of the task, by adding or removing These results may also be incorporated agents decide online which strategy determine when strategies are effective presented here show what information parison between LOW. MD algorithms Another promising avenue is make furniture. into decision as input algorithms to pursue, and investigate in which that in collaborative planning dialogues. The results an agent should consider. For example, a com- shows how to design decision additional factors and HIGH AWM agents for agents who have to decide whether to expend additional effort. from past mistakes so that they can adapt their strategies these results should be incorporated Finally, into the design of multi-agent the agents capable of remembering to the situation and learning [3]. systems and into systems solving for teaching, advice and explanation, where for example might be premised on the abilities of the learner or apprentice. for human-computer communication, the use of particular problem- such as those strategies 6.5. Concluding remarks The goal of this paper was to show how agents’ choice their algorithms resource limits In this paper. I first motivate a number of hypotheses based on a statistical for language behavior, can be designed in the context of particular features of a collaborative to mitigate in communicative action, the effect of their task. analysis planning M.A. Walker/Art@cial Intelligence 85 (1996) 181-243 237 of natural collaborative planning dialogues hypothesized in a testbed in which planning dialogues. Then a functional model of collaborative is developed based on these hypotheses, including parameters that are to affect the generalizability of the model. The model is then implemented these parameters can be varied, and the hypotheses are tested. research criteria that is based on these models must judge the initial part of the process up to specifying The method used here can be contrasted with other work on dialogue modeling. Much previous work on dialogue modeling only carries out part of the process described above: only is completed. Follow-on subjective model judged according to further model with parameters in a testbed many ways in which our initial hypotheses must be refined and further of the model and testing hypotheses and suggests to the can also be steps a is. The models developed here on the basis of empirical the model according or how elegant provides a way to check subjective evaluations the model suggested by the corpus analysis. criteria, but this work carries out additional to test the generalizability a functional model such as how well to these subjective it fits researcher’s test and refine implementation Implementing intuitions evidence tested. is the first testbed for conversational types of independent parameters systems that systemat- that are hypothesized plan negotiated through a dialogue, and the ( 1) agents’ resource The Design-World introduces testbed several different the efficacy of a collaborative ically to affect efficiency of that dialogue process. Experiments between agents’ choice task difficulty and tolerance particular assumptions corpus analysis alone. Several unpredicted and counter-intuitive limits in attentional and (3) in communication; such as inferential complexity, for errors. The results verified a number of hypotheses resource features of communicative about agents’ limits degree of belief coordination that were not possible in the testbed examined capacity and inferential the interaction (2) capacity; tasks that affect required, that depended on to test by the task property of belief coordination in the Zero-non-matching-beliefs and Matched-pair for IRUs, rather than resource the most robust benefits results were also demonstrated by the ex- in combination with resource to tasks), were shown limits alone as originally Second, I predicted other, more useful, beliefs that IRUs can be detrimental that IRUs would always be beneficial for these agents from working memory. Third, for LOW Awh4 through a side effect it would seem that HIGH AWM agents should always perform better than either LOW or MID the (1) when has some cost; and (2) when access to multiple beliefs can lead to a small shared corre- for humans and explains how these agents always have access to more information. However In this case, restricting inferential in which this is not an advantage: processes. This limit intuitively that there are two situations to potential benefits of limited working memory information to make divergent set is a natural way to limit inferences. agents periments. First, limits (as produce hypothesized. agents, but found of displacing plausible AWM agents since results showed accessing agents working sponds humans manage to coordinate on inferences in conversation [49,66,83]. These results clearly demonstrate models must be taken into account of claims be supported. of resource human-human dialogue and the experimental limited processing I have shown can account In addition, that factors not previously considered in dialogue if cooperativity, that a theory of dialogue for both the observed efficiency, or efficacy are to that includes a model in language behavior results presented here. 238 Acknowledgements The work reported in this paper has benefited Janet Cahn. Jean Carletta, from discussions with Steve Whittaker. Joshi, Ellen Prince, Mark Liberman. Max Mintz, Bonnie Webber, Scott We- Aravind instein. Candy Sidner. Owen Rambow. Beth Ann Hockey, Karen Sparck Jones, Julia Galliers. Phil Stenton, Megan Moser, Johanna Moore, Christine Nakatani, Penni Sibun, Ellen Germain, Julia Hirschberg, Alison Jerry Hobbs, Pam Jordan, Barbara Di Cawsey. Rich Thomason, Cynthia McLemorc. I am Eugenio, Susan Brennan, Rebecca Passonneau. Rick Alterman for providing me with an early grateful of the belief revision mechanism in the Automated Librarian project, and to Julia Hirschberg who provided me with tapes of the financial advice anonymous reviewers who provided many useful suggestions. talk show. Thanks also to the two and Paul Cohen. to Julia Galliers Jon Oberlander. implementation used This research was partially funded by AR0 grants DAAG29-84-K-0061 and DAAL03- 89-COO3 1 PRI, DARPA grants NO001 4-85KOOI 8 and NOOOl4-90-J- 1863, NSF grants INT-9 110856 for the 1991 Summer MCS-82- 19 196 and IRI 90- 16592 and Fellowship at the Science and Engineering University of Pennsylvania, in Japan, and Ben Franklin Grant 91S.3078C-1 and by Hewlett-Packard Laboratories. Institute References 1 I 1 J.F. Allen. Recognizing intentions from natural language utterances, in: M. Brady and R.C. Berwick. eds.. Cortq~&tiontrl Models c:f’Discmfrse I;! 1 J.F. Allen and C.R. Perrault, Analyzing 13 1 R. Alterman, R. Zito-Wolf and T. Carpenter. Interaction, comprehension and instruction usage, .I. Learn. intention in utterances, Art$ Infell. 15 ( 1988) 143-178. ( MIT Press, Cambridge, MA. 1983) Sci. 1 (1991) 273-318. 11 1 J.R. Anderson and G.H. Bower. H~trru~n A,ssocrutivr Memory (Winston and Sons, 1973). A. Baddeley. Workins Memory (Oxford University Press, Oxford. 1986). J. Banvise, The situation in logic-IV: on the model theory of common knowledge, Tech. Rept. No. 123. Center for the Study of Language and Information. Ventura Hall. Stanford University, Stanford, CA (1988). M. Bratman, D. Israel and M. Pollack. Plans and resource bounded practical reasoning, Cornput. Intell. 4 C 1988) 349-355. S.E. Brennan. Seeking and providing evidence for mutual understanding, Ph.D. Thesis, Psychology Department, Stanford University, Stanford. CA C 1990). 19 I S.E. Brennan and E.A. Hulteen, Bused S.vstems ( 1995 1. Interaction and feedback in a spoken language system. Knowledge 1 IO1 S. Carberry, Plan recognition and its use in understanding dialogue. in: A. Kobsa and W. Wahlster, eds., User Models in DiuloRue .S_vs/ems (Springer, Berlin, 1989) I33- 162. 1 11 I G. Carenini and J. Moore, Generating explanation in context, in: Proceedings International Workshop ori Intellijient User InteqCufore.7 ( 1993). I I3 ) J.C. Carletta. Risk taking and recovery in task-oriented dialogue, Ph.D. Thesis. Edinburgh University, Edinburgh ( 1992 ). 1 I 3 1 A. Cawsey, Generating interactive explanations. in: Pmceedin~s AAAI-91, Anaheim, CA ( I99 I ) 86-9 I. 1 14 1 A. Cawsey, J. Galliers, S. Reece and K. Sparck Jones, Automating the librarian: a fundamental approach using belief revision, Tech. Rept. 243, Cambridge Computer Laboratory, Cambridge, MA ( 1992). 1 15 1 A. Chapanis. ( 16 1 J. Chu-Carrel and S. Carberry, A plan-based model for response generation in collaborative task-oriented Interactive human communication, Sci. Amer. 232 ( 1975) 34-42. dialogue. in: Pm-eediqs AAAI-94, Seattle. WA C 1994) 799-805. M.A. Walker/Artificial Intelligence 85 (1996) 181-243 239 I 17 I J. Chu-Carrel and S. Carberry, Response generation in collaborative negotiation, in: Proceedings 33rd Annual Meeting of the Association for Computational Linguistics ( 1995). I181 H.H. Clark and S.E. Brennan, Grounding in communication, in: L.B. Resnick, J. Levine and S.D. Bahrend, eds., Perspectives on Socially Shared Cognition (APA, 1990). reference and mutual knowledge, I191 H.H. Clark and CR. Marshall, Definite in: A.K Joshi, B.L. Webber 198 I ) and I. Sag, eds., Elements ofDiscourse Vnderstanding (Cambridge University Press, Cambridge, 10-63. I201 H.H. Clark and E.F. Schaefer, Collaborating on contributions to conversations, Lang. Cognit. Processes 2 (1987) 19-41. [ 211 H.H. Clark and E.F. Schaefer, Contributing [22] H.H. Clark and D. Wilkes-Gibbs, Referring as a collaborative process, Cognition 22 ( 1986) l-39, 1231 P.R. Cohen, A survey of the Eighth National Conference on Artifical to discourse, Cognit. Sci. 13 ( 1989) 259-294. Intelligence: pulling together or pulling apart, AI Magazine 12 (1) (1991) 16-41. 1241 P.R. Cohen, M.1. Greenburg, D.M. Hart and A.E. Howe, Trial by fire: requirements for agents in complex environments, AI Magazine 10 (3) ( 1989) 32-48. [ 251 P.R. Cohen, Empirical Methods for Artificial Intelligence (MIT Press, Boston, MA, 1995). [26] P Cohen and H. Levesque, Teamwork, Nous 25 (1991) 1 l-24. I27 1 P.R. Cohen, On knowing what to say: planning speech acts, Tech. Rept. 118, Department of Computer Science, University of Toronto, Toronto, Ont. (1978). ]28] P.R. Cohen, The pragmatics of referring and the modality of communication, Comput. Linguist. 10 (1984) 97-146. [ 291 P.R. Cohen, Reasoning about Uncertainty: An Artificial Intelligence Approach (Pitman, Boston, MA, 1985). [ 30 I R. Cohen, Analyzing 13 11 A.M. Collins and M.R. Quillian. Retrieval the structure of argumentative discourse, Comput. Linguist. 13 ( 1987) 1 l-24. time from semantic memory, J. Verbal Learn. Verbal BehaL: cognitive and computational aspects, Ph.D. Thesis, 8 ( 1969) 240-247. 1321 N. Dahlback, Representations Linkoping University, Linkoping of discourse: ( 199 1). natural 1331 B. Di Eugenio, Understanding language clauses, Ph.D. Thesis, University of Pennsylvania, instructions: Philadelphia, PA ( 1993). a computational approach to purpose [ 341 J. Doyle, Rationality I35 ] E.H. Durfee, P Gmytrasiewics and its roles in reasoning, Comput. Intell. 18 (3) ( 1992). and J. Rosenschein, The utility of embedded communications and the ] 36 I37 138 of protocols, in: Proceedings AAAI Workshop on Planning for Interagent Communication in: Distributed Artificial Intelligence (Morgan Kaufmann, Los Altos, CA, 1987). emergence (1994). E.H. Durfee, V.R. Lesser and D.D. Corkill. Cooperation solving network, T.W. Finin, A.K. Joshi and B.L. Webber, Natural IEEE 74 (1986) 921-938. J.R. Galliers, A theoretical acknowledging multi agent conflict, Tech. Rept. 172, Computer Laboratory, University of Cambridge, Cambridge (1989). for computer models of cooperative dialogue, interactions with artificial experts, Proc. in a distributed problem through communication framework language 1391 J.R. Galliers, Belief revision and a theory of communication, Tech. Rept. 193, Computer Laboratory, University of Cambridge, Cambridge ( 1990). [40] J.R. Galliers, Autonomous belief revision and communication, in: P Gadenfors, ed., Belief Revision (Cambridge University Press, Cambridge, I41 I J.R. Galliers, Cooperative interaction 199 1) 220-246. as strategic belief revision, in: M.S. Deen, ed., Cooperating Knowledge Based Systems (Springer, Berlin, 1991) 148-163. 1421 P Giirdenfors, Knowledge in Flux : Modeling the Dynamics of Epistemic States (MIT Press, Cambridge, MA, 1988). 1431 P. Gardenfors, The dynamics of belief systems: foundations vs. coherence theories, Revue International De Philosophie ( 1990) [ 441 G. Gazdar, Pragmatics: Implicature, Presupposition and Logical Form (Academic Press. New York, 1979). ]45 1 A.I. Goldman, Epistemology and Cognition (Harvard University Press, Cambridge, MA, 1986). HP Grice, Willwn Jumes Lectures ( 1967). H.P. Grice, Logic and conversation. Ac,ts (Academic Press, New York, 1975 1 41-58. B. Grosz and S. Kraus. Collaborative plans (1993). representation B.J. Gros7, The International. Menlo Park, CA I I977 ) B.J. Gross and C.L. Sidner. Attentions. ( 1986) 175-3-04. B.J. Grosz and CL. Sidner. Planh in: P. Cole and 1. Morgan, eds.. Syntax und Semuntics Ill-Speech for group activities, in: Proceedings IJCAI-93, Chambery and use of focus iu dialogue understanding, Tech. Rept. 151, SRI intentjon and the structure of discourse, Con~plct. Linguist. 12 for discourse, in: P.R. Cohen, J. Morgan and M. Pollack, eds.. Irltrntio,zs in Communrw/ion (MIT Press, Cambridge. MA. 1990) C.I. Guinn. A computational model of dialogue initiative in collaborative discourse, AA1 Tech. Rept. FS-93-05 ( 1993 ). IS.3 1 C.I. Guinn, Meta-dialogue behaviors: improving the efficiency of human-machine dialogue, Ph.D. Thesis. Duke University, Durham, NC ( I994 ). 15-l I S. Hanks, M. Pollack and P. Cohen. Benchmarks. testbeds. controlled experimentation and the design of agent architectures. Al Mu,ytrzine 14 ( 1993 I I 55 I B. Hayes-Roth and P.W. Thomdyke. Integration of knowledge from text, J. Verbrtl Leum. Verbal Behuv. 18 (1979) 91~108. I 5h ) PA. Heeman and G. Hirst, Collaborating on referring expressions. Con~/ztf. Liquist. 21 ( 1995) 35 I - 3x3. I57 1 S. Hellyer. Frequency of stimulus presentation and short-term decrement in recall, J. Exper. Psych. 64 ( 1961) 650. and R.A. Block. Repetition and memory: evidence for a multiple trace hypothesis, J. D.L. Hintzmann Exper. Psych 88 ( I97 I ) 297-306. J. Hirschberg, A University theory of scalar m~plicaturr. of Pennsylvania, Philadelphia, PA ( I985 1. Ph.D. Thesis, Computer and Information Science, J.R. Hobbs, On the coherence and structure of discourse. Tech. Rept. CSLI-85-37. Center Ventura Hall, Stanford University. Stanford, CA ( 198.5). of Language and Information, for the Study J.R. Hobbs, Intention. information and structure in discourse. in: D. Scott and E. Hovy, eds., Burnin Iww.7 in Diswtrrse ( I994 ) A. Jameson. Knowing what others know. Ph.D. Thesis, University of Nijmegen, Nijmegen (1990). (Lawrence Erlbaum. Hillsdale, NJ, I99 I ). resource sensitive communication. in: Proceedings AAAI FuII P.N. Johnson-Laird. Deductirm P Jordan and M.A. Walker, Multi-agent Workshop on Rutionul Agency C 1995 1 equivalent Informationally A.K. Joshi. Micwwuves. Cir~Gt Theory und Infi~rmtrtion T/twr~ sentences. in: Proc~eedings ( 1964). inference on partial Infernutionul Cw@rence on A.K. Joshi, Some extensions of a hystcm Injerence Swtenrs ( Academic Press. New York. 197X) 24 I-257. A.K. Joshi, B. Webber and R.M. Weischedel, Some aspects of default reasoning information, for in: Pattern Dvected in interactive discourse, Tech. Rept. MS-CIS-86-27, University of Pennsylvania. Philadelphia, PA ( 1986). A.K. Joshi. B.L. Webber and R.M. Weischedel, Preventing false inferences. in: Proceedings Tenth Interrtutionul Cor#rence o,t Conrput~tti~,,l~tl /.iquistics, Palo Alto, CA ( 1984) 134-l 38. L. Karttunen and S. Peters, Conventional I69 I ( 70 I G. Keppel. Dcsi,qn und Anu@i.v: A RPseczrcher’.r Hundhook implicature, $ntax Semanf. 11 ( 1979). (Prentice-Hall, Englewood Cliffs, NJ, 2nd ed., 1982). 171 I A. Kldd, The consultative Computers: Designing role of an expert system. in: P. Johnson and S. Cook. eds., People und the lnterji,uw (Cambridge University Press, Cambridge, 1985). 171 I K. Konolige, Belief and incompleteness, in: J.R. Hobbs and R.C. Moore, eds., Formal Theories of the Commonsense World (Ablex. Norwood, NJ. 198.5) 359-403. 1 73 I K. Konolige, A Deduction Model of Beliej’ (Brown University Press, Providence, RI, 1986). I 74 I J.E. Laird, A. Newell and P.S. Rosenbloom. SOAR: an architecture for general intelligence, Arrif: Intell. 33 (1987) l-64. 175 I T.K. Landauer, Reinforcement ac consolidation, ~,~~~‘h. Rev. 16 ( 1969) 811-96. 15x1 1-1 I 60 I 161 I 1631 163 I64 I 65 I flfJ I67 1681 M.A. Walker/Artificial Intelligence 85 (1996) 181-243 241 properties of a model with random storage and undirected 1761 T.K. Landauer, Memory without organization: retrieval, Cognit. Psych. 7 ( 1975) 495-531. 1771 J.E Lehman, R.L. Lewis and A. Newell, Natural language comprehension in soar, Tech. Rept. CMU- CS-9 I- 117, Carnegie Mellon University, Pittsburgh, PA ( 199 1) I78 I N. Lenke, Anticipating the reader’s problmes and the automatic generation of paraphrases, in: Proceedings COLING-94, Kyoto ( 1994) I791 H.J. Levesque, P.R. Cohen and J.H.T. Nunes, On acting together. in: Proceedings AAAI-90, Boston, MA (1990). I 80 ] S.C. Levinson, Activity I81 I SC. Levinson, Some pre-observations types and language, Linguistics 17 ( 1979) 365-399. on the modelling of dialogue, Discourse Processes 4 ( 1981) 93-l 16. ] 82 ] S.C. Levinson, Pragmatics (Cambridge University Press, Cambridge, I831 S.C. Levinson, What’s special about conversational I 84 I D. Lewis, Convention (Harvard University Press, Cambridge, MA, 1969). I 85 I D. Litman, Plan recognition and discourse analysis: an integrated approach 1983). inference, 1987 Linguistics Institute Packet ( 1985). for understanding dialogues, Tech. Rept. 170, University of Rochester, Rochester, NY (1985). I861 D. Litman and J. Allen, Recognizing in: P.R. Cohen, J. Morgan and M. Pollack, eds., Intentions in Communication (MIT Press, Cambridge, MA, 1990). intentions and task-oriented and relating discourse plans, [ 87 I K.E. Lochbaum, Using collaborative plans to model the intentional structure of discourse, Ph.D. Thesis, Harvard University, Cambridge, MA ( 1994). [ 88 I B. Logan, S. Reece and K. Sparck Jones, Modelling retrieval agents with belief revision. in: Proceedings Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Springer, London, 1994) 91-100. information [ 891 W.C. Mann and S.A. Thompson, Rhetorical structure theory: description and construction structures, 83-96. in: G. Kempen, ed., Natural Language Generation (Martinus Nijhoff, Dordrecht, of text 1987) I 90 I K.R. McKeown, Paraphrasing 191 I G.A. Miller, The magical number seven, plus or minus two: some limits on our capacity questions using given and new information, Comput. Linguist. 9 ( 1983). for processing information, Psych. Rev. 3 ( 1956) 81-97. 1921 J.D. Moore and C.L. Paris, Planning text for advisory dialogues: capturing intentional and rhetorical information, Compui. Linguist. 19 (4) (1993). [ 93 j M.G. Moser and J. Moore, in: Proceedings ACL Workshop on Intentionality and Structure in Discourse Relations (1993). [94] M.G. Moser and J. Moore, Investigating cue selection and placement in tutorial discourse, in: Proceedings 33rd Annual Meeting of the Association for Computational Linguistics ( 1995). [ 951 D.A. Norman (1975) 44-46. and D.G. Bobrow, On data-limited and resource-limited processes, Cognit. Psych. 7 [96] R. Perrault, An application of default logic to speech-act theory, in: P.R. Cohen, J. Morgan and M. Pollack, eds., Intentions in Communication (MIT Press, Cambridge, MA, 1990). I 97 1 M. Pollack, Inferring domain plans in question answering, Tech. Rept. 403, SRI International Artificial Intelligence Center, University of Pennsylvania, Philadelphia, PA ( 1986). I98 I M. Pollack, J. Hirschberg and B. Webber, User participation in the reasoning process of expert systems, in: Proceedings AAAI-82, Pittsburgh, PA ( 1982). [99] M. Pollack and M. Ringuette, Introducing the TileWorld: experimentally evaluating agent architectures. in: Proceedings AAAI-90, Boston, MA (1990) 183-189. [ 1001 R. Power, A computer model of conversation, Ph.D. Thesis, University of Edinburgh, Edinburgh ] 1011 R. Power, Mutual [ 102) E.E Prince, Toward a taxonomy New York, 1981) 223-255. J. Theory Social Behav. 14 ( 1984). of given-new information, intention. in: Radical Pragmatics (Academic Press. ( 1974). [ 1031 E.E Prince, The ZPG letter: subjects, definiteness and information status, in: S. Thompson and W. Mann, eds., Discourse Description: Diverse Analyses of a Fund Raising Text (Benjamin, New York, 1992) 295-325. [ 1041 R. Reichman, Getting Computers to Talk Like You and Me (MIT Press, Cambridge, MA, 1985). ‘42 M.A. Wulker/Arr@tl Intelligence X5 (1996) 181-243 I I OS I K. Reiter, A logic [ IO6 I N. Reithinger and E. Maier. Utilizing for default reasoning, Art$ Intrll. 13 ( 1980) 8 I - 132. statistical speech act processing in verbmobil. in: Proceediqs 33rd Annunl Meeting of the Associution fi>r Compututionul Linguistics ( 1995). I 107 1 C.P. Rose, B. Di Eugenio. L. Levin and C. Van Ess-Dykema, Discourse processing of dialogues with multiple threads, in: Proceedinp 33rri Annual Meeting offhe Associationfor Computufionul Linptistics (1995). 1 108 1 J.S. Rosenschein, Rational interactlon and multi agent planning. Ph.D. Thesis, Computer Science Department. Stanford University. Stanford. CA ( 1985) J.M. Sadock, On implicnture. for conversational ( Academic Press. New York, 1978) 28 l-297. Prqmutics testing in: I? Cole, rd., .S~nrtr.~ ccnd Semuntics 9: of California Press, Berkeley, CA. E.A. Schegloff, Between micro and macro: contexts and other connections, ( University P Sibun. The Massachusetts, Amherst, MA ( 1991) C. Sidner, Plan parsing response recognition local organisation for intended I987 1. incremental generation and of in: The MicrwMucro Link text, Ph.D. Thesis. University of in discourse. Cornj~ur. Intel/. 1 ( 198.5). C. Sidner. An artificial discourse language for collaborative negotiation, in: Proceedings AAAI-94, Seattle, WA (1994) 814-820. theory of delinite anaphora comprehension C.L. Sidner, Toward a computational AI-TR-537, MIT. Cambridge, MA ( 1979). S. Siegel, Nonptrumetw Stutistics for fhe Behurrr~rul Sciences ( McGraw-Hill. and control level communication R. Smith. The contract net protocol: high Eolver, IEEE Trcln.7. Conrpu/. 12 ( 1980) R. Smith. D.R. Hipp and A.W. Biermann, A dialogue Proceedings Third Conference on Nuturul Ltrn~uu~e Processing. Trento ( 1992) M. Solomon, Scientific and human reasoning, Philos. Sci. 59 (3) C 1992). control algorithm rationality IlO4-I I 13. and in English, Tech. Kept. New York. 1956). in a distributed problem its performance, in: II091 I ]](J Ill]1 I II21 11131 11151 I1161 Ill7i R.C. Stalnaker. Assertion, in: l? Cole. rd.. .‘?y)~fcr.\. c~nd Srmunfic.r 9: Prqmutics (Academic Press, New York, 1978) 3 I S-332. I 1201 A. Stein and U. Thiel. A conversational model of multimodal interaction. in: Proceedings AAAI-93. Washington. DC ( I993 ). I I2 I I S. Stemberg, High-speed scnnmng 11221 in human memory. Science 153 ( 1967) 6.52-654. H.S. Stone, High Per$ormunce Cornpurer Archifecfure (Addison-Wesley, Reading, MA, 1987). II231 D.D. Suthers. Sequencing explanations to enhance communicative functionality, in: Proceedinp f51h Annuul Conference of’rhe Cqnitive Scienw Swietv. Boulder, CO ( I993 ). [ 1141 R. Thomason. Accommodation. meaning and implicature: interdisciplinary foundations for pragmatics. in: P.R. Cohen, J. Morgan and M. Pollack. eds.. Intentions in Communicution (MIT Press. Cambridge, MA. 1990). II751 R. Thomason, Propagating cpistemic coordination through mutual defaults-i. in: R. Parikh. ed.. Third Conference on Tfteoreriatl A.sfwcfs of Reusoning ubout Knouled~e, Pacific Grove, ‘9-39. f’roceedirqg CA ( 1990) D. Traum, Mental state in the trains-92 dialogue manager. Reusoning ubouf Men/n/ Stutes ( I993 ). D. Traum, A computational model of grounding II261 II271 in natural language conversation. Ph.D. Thesis. llniversity of Rochester, Rochester, NY I 1993 ). II281 D.R. Traum, The discourse reasoner in trains-90. TRAINS Technical Note 9 I-S, Department of Computer Science, University of Rochester, Rochester, NY ( 199 I ). I 129 I E. Tulving, The effects of presentation and recall of material in free recall learning, J. Verbul Lawn. Verbal Be~u~v. 6 ( 1967 1. E. Turner, Selecting information I I30 to communicate, in: f’roceedings AAAI Workshop on Plunning ,fijr fnterujient Communiculion ( I994 1 II31 M.A. Walker, Redundancy in collaborative dialogue, in: Proceeding.7 Fourteenth fnternutionul Conference on Compufutionul Lin,quistics ( 1992) 345-3s I. Il.12 M.A. Walker. Information and deliberation in discourse. in: Proceedings ACL Workshop ON fntentionulitv und Sfructure in Dis~xurse Rr/uti~vr.s ( 1993). in: Procredmgs AAAI Spring Symposium on M.A. Walker/Arti$cial Intelligence 85 (1996) 181-243 243 I 133 1 M.A. Walker, Informational Philadelphia, redundancy PA ( 1993). I 1341 M.A. Walker, Discourse and deliberation: Pennsylvania, and resource bounds in dialogue, Ph.D. Thesis, University of testing a collaborative strategy, in: Proceedings COLINC-94. Kyoto ( 1994). I 135 1 M.A. Walker. Rejection by implicature, in: Proceedings 20th Meeting of the Berkeley Linguistics Saciet?, (1994). [ 1361 M.A. Walker, Limited attention and discourse I 1371 M.A. Walker, Testing collaborative structure, Cornput. Linguist. (to appear). strategies by computational simulation: cognitive and task effects. in: Knowledge Based Sysfems ( 1995). [ 138 1 M.A. Walker and 0. Rambow, The role of cognitive modeling in achieving communicative intentions, in: Proceedings 7th International Conference on Natural Language Generation ( 1994). 1 139 I M.A. Walker and S. Whittaker, Mixed in: Proceedings 28th Annual Conference of rhe Association far Computational into discourse in dialogue: investigation initiative an segmentation, Linguistics, Pittsburgh, PA (1990) 70-79. I 140 I B.L. Webber, A formal approach to discourse anaphora, Ph.D. Thesis. Harvard University. Cambridge, MA (1978). 1 1411 B.L. Webber and A.K. Joshi, Taking the initiative in natural language database interaction: justifying why, in: Proceedings COLING-82, Prague (1982) 413-419. [ 142 1 S. Whittaker, E. Geelhoed and E. Robinson, Shared workspaces: how do they work and when are they useful?, Int. J. Man-Mach. Stud. 39 (1993) 813-842. I 1431 S. Whittaker and P Stenton. Cues and control in expert client dialogues, in: Proceedings 26th Annual Meeting of fhe Associafion of Compufafional Linguisfics. Buffalo, NY ( 1988) 123-130. I 1441 S. Wilks, Mafhematical Statistics (Wiley, New York, 1962). 1 1451 M.R. Young and J.D. Moore, Representation and generation University of Pittsburgh, Pittsburgh, PA ( 1994). in discourse planning, Tech. Report, I 1461 M.R. Young. J.D. Moore and M.E. Pollack, Towards a principled representation of discourse plans, in: Proceedings 16th Annual Conference of the Cognitive Science Society ( 1994). [ 147 1 1. Zukerman and R. McConachy. Generating concise discourses that addresses a user’s inferences, in: Proceedings UCAI-93, Chambery, France ( 1993). and J. Pearl, Comprehension-driven generation of meta-technical utterances in math I 148 1 I. Zukerman tutoring, in: Proceedings AAAI-86, Philadelphia, PA ( 1986) 606-611. 