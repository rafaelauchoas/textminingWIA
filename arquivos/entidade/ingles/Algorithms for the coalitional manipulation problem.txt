Artificial Intelligence 173 (2009) 392–412Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAlgorithms for the coalitional manipulation problem ✩Michael Zuckerman a,1, Ariel D. Procaccia b,∗,2, Jeffrey S. Rosenschein aa School of Engineering and Computer Science, The Hebrew University of Jerusalem, Jerusalem 91904, Israelb Microsoft Israel R&D Center, 13 Shenkar Street, Herzeliya 46725, Israela r t i c l ei n f oa b s t r a c tWe investigate the problem of coalitional manipulation in elections, which is known to behard in a variety of voting rules. We put forward efficient algorithms for the problem inBorda, Maximin and Plurality with Runoff, and analyze their windows of error. Specifically,given an instance on which an algorithm fails, we bound the additional power themanipulators need in order to succeed. We finally discuss the implications of our resultswith respect to the popular approach of employing computational hardness to precludemanipulation.© 2008 Elsevier B.V. All rights reserved.Article history:Received 15 December 2007Received in revised form 17 November 2008Accepted 20 November 2008Available online 24 November 2008Keywords:Computational social choiceVotingManipulationComputational complexity1. IntroductionSocial choice theory is an extremely well-studied subfield of economics. In recent years, interest in the computationalaspects of social choice, and in particular in the computational aspects of voting, has sharply increased.In an election, a set of voters submit their (linear) preferences (i.e., rankings) over a set of candidates. The winner ofthe election is designated by a voting rule, which is basically a mapping from the space of possible preference profiles intocandidates. A thorn in the side of social choice theory is formulated in the famous Gibbard–Satterthwaite Theorem [15,26].This theorem essentially states that for any voting rule that is not a dictatorship, there are elections in which at least oneof the voters would benefit by lying. A dictatorship is a voting rule where one of the voters—the dictator—single-handedlydecides the outcome of the election.Since the 1970s, when this impossibility result was established, an enormous amount of effort has been invested indiscovering ways to circumvent it. Two prominent and well-established ways are allowing payments [4,16,29], or restrictingthe voters’ preferences [20].In this paper, we wish to discuss a third path—the “path less taken”, if you will—which has been explored by computerscientists. The Gibbard–Satterthwaite Theorem implies that in theory, voters are able to manipulate elections, i.e., bend themto their advantage by lying. But in practice, deciding which lie to employ may prove to be a hard computational problem;after all, there are a superpolynomial number of possibilities of ranking the candidates.✩A significantly shorter version of this paper (with most of the proofs omitted) appeared in the Proceedings of the Nineteenth ACM–SIAM Symposiumon Discrete Algorithms (SODA-08). This work was also presented at the Dagstuhl Workshop on Computational Issues in Social Choice, October 2007.* Corresponding author.E-mail addresses: michez@cs.huji.ac.il (M. Zuckerman), arielpro@gmail.com (A.D. Procaccia), jeff@cs.huji.ac.il (J.S. Rosenschein).1 The author thanks Noam Nisan for a generous grant which supported this work.2 The author was supported in this work by the Adams Fellowship Program of the Israel Academy of Sciences and Humanities.0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.005M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412393Indeed, Bartholdi et al. [3] put forward a voting rule where manipulation is N P -hard. In another important paper,Bartholdi and Orlin [2] greatly strengthened the approach by proving that the important Single Transferable Vote (STV) ruleis hard to manipulate.This line of research has enjoyed new life in recent years thanks to the influential work of Conitzer, Sandholm, andLang [7].3 The foregoing paper studied the complexity of coalitional manipulation. In this setting, there is a coalition ofpotentially untruthful voters, attempting to coordinate their ballots so as to get their favorite candidate elected. The authorsfurther assume that the votes are weighted: some voters have more power than others. Conitzer et al. show that in avariety of prominent voting rules, coalitional manipulation is N P -hard, even if there are only a constant number of candidates(for more details, see Section 2). This work has been extended in numerous directions, by different authors [5,8,12,18,25];Elkind and Lipmaa [9], for example, strengthened the abovementioned results about coalitional manipulation by employingcryptographic techniques.In short, computational complexity is by now a well-established method of circumventing the Gibbard–SatterthwaiteTheorem. Unfortunately, a shortcoming of the results we mentioned above is that they are worst-case hardness results,and thus provide a poor obstacle against potential manipulators. Recent work regarding the frequency of manipulation hasargued that with many worst-case hard-to-manipulate voting rules, a potential manipulator may be able to compute amanipulation in typical settings [6,13]. In particular, Procaccia and Rosenschein [23,24] have established some theoreticalresults regarding the frequency of success of an algorithm for the coalitional manipulation problem. The matter was furtherdiscussed by Erdélyi et al. [11]. In spite of this, the question of the tractability of the manipulation problem, and in particularof the coalitional manipulation problem, in typical settings is still wide-open.Our approach and results We wish to convince the reader that, indeed, the coalitional manipulation problem can be effi-ciently solved in typical settings under some prominent voting rules, but our approach differs from all previous work. Wepresent efficient heuristic algorithms for the problem that provide theoretical guarantees. Indeed, we characterize smallwindows of instances on which our algorithms may fail; the algorithms are proven to succeed on all other instances.Specifically, we prove the following results regarding three of the most prominent voting rules (in which coalitionalmanipulation is known to be N P -hard even for a constant number of candidates):Theorem.1. In the Borda rule, if there exists a manipulation for an instance with certain weights, Algorithm 2 will succeed when given an extramanipulator with maximal weight.2. In the Plurality with Runoff rule, if there exists a manipulation for an instance with certain weights, Algorithm 3 will succeed whengiven an extra manipulator with maximal weight.3. In the Maximin rule, if there exists a manipulation for an instance with certain weights, Algorithm 1 will succeed when given twocopies of the set of manipulators.Significance in Artificial Intelligence The sharply increased interest in computational aspects of voting is motivated by numer-ous applications of voting techniques and paradigms to problems in Artificial Intelligence (AI). These applications includework in AI subfields as diverse as Planning [10], Automated Scheduling [17], Recommender Systems [14], Collaborative Fil-tering [22], Information Extraction [27], and Computational Linguistics [21].Unfortunately, in the application of voting to AI, some of the problems investigated in Social Choice Theory, and in par-ticular the issue of manipulation, become especially acute. Indeed, multiagent systems are often inhabited by heterogeneous,self-interested agents. Such agents, unlike human beings, can be designed to be rational, and constantly engaged in com-putations meant to increase their utility. In particular, a self-interested agent could seize the opportunity to manipulate anelection to its benefit if such an opportunity were computationally easy to recognize (unless specifically programmed notto).The agenda of circumventing the Gibbard–Satterthwaite Theorem via computational complexity is, once again, mostrelevant and compelling when the voters are software agents that populate a multiagent system, since the effective, boundedrationality of such agents is practically governed by the laws of computational complexity. This is why the agenda hasbecome a prominent one in AI, with numerous papers on the subject published in the major AI conferences over the lastfive years. As of yet, there are few papers on frequency of manipulation, rather than on its worst-case complexity. We feelthat this line of work on frequency of manipulation may influence the entire direction of the computational social choiceresearch agenda (see Section 5 for more details regarding work on frequency of manipulation).Structure of the articleIn Section 2 we describe the major voting rules and formulate the coalitional manipulation problem.In Section 3 we present and analyze our algorithms in three subsections: Borda, Plurality with Runoff, and Maximin. Weprovide some results regarding an unweighted setting in Section 4. In Section 5 we describe related work at length. Finally,we discuss our approach in Section 6.3 Historical note: although we cite the JACM 2007 paper, this work originated in a AAAI 2002 paper.394M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–4122. Voting rules and manipulation problemsAn election consists of a set C = {c1, . . . , cm} of candidates and a set S = {v 1, . . . , v|S|} of voters. Each voter provides atotal order on the candidates. To put it differently, each voter submits a ranking of the candidates. The voting setting alsoincludes a voting rule, which is a function from the set of all possible combinations of votes to C .We shall discuss the following voting rules (whenever the voting rule is based on scores, the candidate with the highestscore wins):• Scoring rules. Let (cid:3)α = (cid:4)α1, . . . , αm(cid:5) be a vector of non-negative integers such that α1 (cid:2) α2 (cid:2) · · · (cid:2) αm. For each voter,a candidate receives α1 points if it is ranked first by the voter, α2 if it is ranked second, etc. The score of a candidateis the total number of points the candidate receives. The scoring rules that we will consider are: Borda, where (cid:3)α =(cid:4)m − 1, m − 2, . . . , 0(cid:5); Veto, where (cid:3)α = (cid:4)1, 1, . . . , 1, 0(cid:5); and Plurality, where (cid:3)α = (cid:4)1, 0, . . . , 0(cid:5).• Maximin. For any two distinct candidates x and y, let N(x, y) be the number of voters who prefer x to y. The maximinscore of x is σ (x) = min y(cid:6)=x N(x, y).• Copeland. For any two distinct candidates x and y, let C(x, y) = +1 if N(x, y) > N( y, x) (in this case we say that x beatsy in their pairwise election), C(x, y) = 0 if N(x, y) = N( y, x), and C(x, y) = −1 if N(x, y) < N( y, x). The Copeland score ofcandidate x is σ (x) =• Plurality with Runoff. In this rule, a first round eliminates all candidates except the two with the highest plurality scores.y(cid:6)=x C(x, y).(cid:2)The second round determines the winner between these two by their pairwise election.In some settings the voters are weighted. A weight function is a mapping w : S → N. When voters are weighted, the aboverules are applied by considering a voter of weight l to be l different voters.Definition 2.1.1. In the Constructive Coalitional Weighted Manipulation (CCWM) problem in a voting rule F , we are given a set C ofcandidates, with a distinguished candidate p ∈ C , a set of weighted voters S that already cast their votes (these are thetruthful voters), and a list of weights W for a set of voters T that still have not cast their votes (the manipulators). Weare asked whether there is a way to cast the votes in T such that p wins the election under the voting rule F .2. Constructive Coalitional Unweighted Manipulation (CCUM) problem is a special case of CCWM problem where allthe weights equal 1.Remark 2.2. We implicitly assume in both questions that the manipulators have full knowledge about the other votes.Unless explicitly stated otherwise, we also assume that ties are broken adversarially to the manipulators, so if p ties withanother candidate, p loses. The latter assumption is equivalent to formulating the manipulation problems in their uniquewinner version, when one assumes that all candidates with maximal score win, but asks that p be the only winner.Theorem 2.3. (See [7].) The CCWM problem in Borda, Veto, Maximin, Copeland, and Plurality with Runoff is N P -complete, even whenthe number of candidates is constant.Throughout this paper we will use the convention that |C| = m, |S| = N and |T | = n. Whenever the voting rule is basedon scores, we will denote by σS, j(c) the accumulated score of candidate c from the voters in S and the first j voters of T(fixing some order on the voters of T ). Whenever it is clear from the context that S is fixed, we will use simply σ j(c) forthe same. Also, for G ⊆ C, 0 (cid:3) j (cid:3) n we will write σ j(G) = {σ j(g) | g ∈ G}. For two lists A, B (ordered multisets), we denoteby A + B the list that is obtained after B is appended to A.3. Weighted coalitional manipulationWe begin our contribution by presenting a general greedy algorithm for the coalitional manipulation problem. Some ofour main results concern this algorithm or its restriction to scoring rules.The greedy algorithm is given as Algorithm 1. It works as follows: the manipulators, according to descending weights,each rank p first and rank the other candidates in a way that minimizes their maximum score. This algorithm is a general-ization of the one that appeared in Bartholdi et al. [3].Definition 3.1. We refer to an iteration of the main for loop in lines 4–12 of the algorithm as a stage of the algorithm.We will use the fact that for many voting rules, if there exists a manipulation for a coalition of manipulators with weight(cid:10) ⊇ W . Normally, iflist W , then there exists a manipulation for a coalition of manipulators with weight list Wthe coalition is too small then there is no manipulation, and this is indeed what the algorithm will report. On the otherhand, if the coalition is large enough, then the greedy algorithm will find the manipulation. So there remains a window ofwhere W(cid:10)M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–4123951: procedure Greedy(C, p, X S , W )weights for voters in T , |W | = |T | = n(cid:2) X S is the set of preferences of voters in S, W is the list of(cid:2)Sort the weights in descending order(cid:2)Will contain the preferences of T(cid:2)Iterate over voters by descending weights(cid:2)Put p at the first place of the jth preference list(cid:2)Iterate over places of jth preference list(cid:2)Evaluate the score of each candidate if j would put it at the next available placePick c ∈ argminc∈C\P jP j = P j + {c}{Score of c from X S ∪ X ∪ {P j + {c}}}(cid:2)Add c to j’s preference list{Score of c based on X S ∪ X T } = {p} then(cid:2) p winsP j ← (p)for t = 2, . . . , m dosort(W )X ← ∅for j = 1, . . . , n do2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19: end procedureend forX T ← Xif argmaxc∈Celsereturn falseend ifreturn trueend forX ← X ∪ {P j }Algorithm 1. Decides CCWM.1: procedure Scoring-rules-Greedy(C, p, σ0(C), W )(cid:2)σ0(C) is the list of scores of candidatesdistributed by voters in S, W is the list of weights for voters in T , |W | = |T | = n(cid:2)Go over voters in T(cid:2)Put p at the first place of the jth preference listσ j (p) = σ j−1(p) + w jα1Let t1, t2, . . . , tm−1 s.t. ∀l, σ j−1(ctl−1 ) (cid:3) σ j−1(ctl )j votes p (cid:16) ct1for l = 1, . . . , m − 1 do(cid:16) · · · (cid:16) ctm−1σ j (ctl ) = σ j−1(ctl ) + w jαl+1{σn(c)} = {p} thenfor j = 1, . . . , n doend for2:3:4:5:6:7:8:9:10:11:12:13:14:end if15: end procedureend forif argmaxc∈Celsereturn truereturn false(cid:2)Update the scores(cid:2) p winsAlgorithm 2. Decides CCWM in Scoring rules.error, where for some coalitions there could exist a manipulation, but the algorithm may not find it. We are interested inbounding the size of this window. We first formulate the monotonicity property described above.Definition 3.2. In the context of the CCWM problem, a voting rule is said to be monotone in weights if it satisfies thefollowing property: whenever there is a manipulation making p win for manipulator set T with weight list W , there is alsoa manipulation making p win for manipulator set Twith weight list W(cid:10) ⊇ T , W(cid:10) ⊇ W ., where T(cid:10)(cid:10)Monotonicity in weights is a prerequisite for the type of analysis we wish to present. However, surprisingly, not allthe basic voting rules have this property; in particular, the prominent Copeland rule does not possess it. We show this byexample in Appendix A.3.1. BordaIn this subsection, we analyze the performance of Algorithm 1 with respect to the Borda voting rule. Note that, in thecontext of scoring rules, Algorithm 1 reduces to Algorithm 2. This algorithm first appeared in Procaccia and Rosenschein [24].In this specific instantiation of Algorithm 1, we do not require sorting of the manipulator weights, as this does not play apart in our analysis.Lemma 3.3. Scoring rules are monotone in weights.Proof. Let C be the candidate set; p ∈ C is the preferred candidate, S is the set of truthful voters, and W are the weightsfor the manipulators T . Denote |C| = m, |S| = N, |W | = |T | = n. It is enough to show that if there is a manipulation for(cid:10) = W + {w}, where w (cid:2) 1 is anthe set T , then for the same instance with manipulators Tinteger, there is also a manipulation, and the rest will follow by induction.(cid:10) = T + {v} with weight list W396M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412Let (cid:3)α = (cid:4)α1, . . . , αm(cid:5) be the score vector of the rule. Let X S be the preference orders of the voters in S, and X T be thepreference orders of voters in T that make p win. Fix some order on the voters in T . By definition, for all c ∈ C \ {p}, σn(c) <σn(p). Let the additional voter of Trank p at the first place, and report some arbitrary order on the other candidates. Thenfor all c ∈ C \ {p}, σn+1(p) = σn(p) + wα1 > σn(c) + wα1 (cid:2) σn+1(c). Hence, p wins. (cid:2)(cid:10)We are now ready to present our theorem regarding the Borda rule.Theorem 3.4. In the CCWM problem under Borda, let C be a set of candidates with p ∈ C a preferred candidate, S a set of voters whoalready cast their votes. Let W be the weight list for the set T . Then:1. If there is no ballot making p win the election, then Algorithm 2 will return false.2. If there exists a ballot making p win the election, then for the same instance with weight list W + {w(cid:2)ki=1 w(cid:10)i(cid:2) max(W ), Algorithm 2 will return true.(cid:10)1, . . . , w(cid:10)k}, where k (cid:2) 1,Before we proceed to the theorem’s proof, a short discussion is in order. Despite its mathematical formulation, one shouldnot think of Item 2 of the theorem as saying that if the algorithm fails on one instance, it would succeed on another. Rather,the theorem implies that the algorithm succeeds on any given instance such that there is a “smaller” instance (where themanipulators have less weight) on which success is possible. Here the monotonicity in weights property comes into play.Also note that Item 1 of the theorem is true for any constructive algorithm; this item (which also appears in our subsequenttheorems) is trivially satisfied.Another interesting point is that this theorem can be viewed as implying that Algorithm 2 gives some sort of additiveapproximation ratio. Formally, it seems unnatural to adopt the notion of approximation algorithms in the context of theCCWM problem. However, the exact way in which the theorem yields approximation guarantees will become apparentwhen we discuss the unweighted setting, in Section 4.= argmaxg∈C\{p}{σ0(g)}, and, by induction, for s = 1, 2, . . . : G sA key notion for the proof of the theorem is the definition of the set G W . Let W be list of weights; we de-fine G W as follows. Run the algorithm n + 1 stages with the weights W + {w}, where w is an arbitrary weight.(cid:10) ∈Let G 0WG s−1W in some stage l, 1 (cid:3) l (cid:3) n + 1}. Finally, let G W =W .Informally, G W is constructed by taking candidates that initially have maximum score, and then inductively addingcandidates that are ranked by the algorithm below candidates that were already added to the set. Since the algorithmranks stronger candidates below weaker candidates, only strong candidates are ultimately members of G W . The additionalarbitrary weight w, and the existence of stage n + 1 (when there are in fact only n manipulators with weights W ) are justa formality: we are also interested in the way the algorithm would rank the candidates after all the manipulators have casttheir ballots, but we do not care about their scores after this final “virtual” ranking.∪ {g | g was ranked below some g= G s−10(cid:3)s G s(cid:3)WWW if he isObserve that the indices s = 1, . . . are not directly related to stages l = 1, . . . , n: a candidate c is added to G s(cid:10) ∈ G s−1ranked below a candidate cW in some stage l = 1, . . . , n (e.g., not necessarily in stage s).Notice that the above definition is independent of the weight w, as this weight is used only in stage n + 1, so it does⊆ · · · ⊆ C \ {p}.= Gm−2W .not impact the preferences of the voters, and thus it does not impact G W . From the definition, G 0WFurthermore, as |C \ {p}| = m − 1, it follows that there exists 0 (cid:3) s(cid:10)+1W , and thus G W = G s(cid:10) (cid:3) m − 2 s.t. G sW⊆ G 1W= G sW(cid:10)(cid:10)We are now ready to unfold the proof of Theorem 3.4. The proof relies on Lemmata 3.5–3.13. The general intuition of theproof is as follows. Consider the candidates in G W ; we show that if there exists a manipulation, it must be possible to getthe score of p to be higher than their average score. The difficult part is to show that the average score of the candidates inG W is relatively close to the maximal final score. As a result, a few additional manipulators are sufficient to push p abovethe maximal score as well.In the first three lemmata, Lemmata 3.5–3.7, we show that the candidates in G W are the ones with highest scores andwe give a connection between their average score and the success of the algorithm in finding a manipulation. The nextstraightforward lemma formalizes the intuition that the strong candidates in G W are always ranked last by the algorithm.Lemma 3.5. Given W , the candidates in G W were ranked at each stage l, 1 (cid:3) l (cid:3) n + 1 at the |G W | last places, i.e., they were alwaysgranted the points |G W | − 1, . . . , 0.Proof. If, by way of contradiction, there exists c ∈ C \ G W that was ranked in some stage in one of the last |G W | places,then there is g ∈ G W that was ranked above c at this stage. Let s (cid:2) 0 such that g ∈ G s⇒ c ∈ G W ,a contradiction. (cid:2)W . By definition, c ∈ G s+1WLemma 3.6, directly building on Lemma 3.5, states that when the algorithm terminates, the candidates in G W have scoresthat are higher than any candidate outside the set, perhaps except p.Lemma 3.6. For all c ∈ C \ (G W ∪ {p}), it holds that σn(c) (cid:3) ming∈G W{σn(g)}.M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412397Proof. Suppose, for contradiction, that there are c ∈ C \ (G W ∪ {p}) and g ∈ G W , s.t. σn(c) > σn(g). Then in stage n + 1, cW . Then c ∈ G s+1would have been ranked below g. Let s (cid:2) 0 s.t. g ∈ G s⇒ c ∈ G W , a contradiction. (cid:2)WThe next lemma clarifies the connection between the definition of G W and Theorem 3.4. Indeed, it links the averagescore of the candidates in G W (when the algorithm terminates) and the answer returned by the algorithm.(cid:2)Lemma 3.7. Given W , |W | = n, let G W be as before. Denote by q(W ) the average score of candidates in G W after n stages: q(W ) =1|G W |σn(g). Then:g∈G W1. If σn(p) (cid:3) q(W ) then there is no manipulation that makes p win the election, and the algorithm will return false.2. If σn(p) > maxg∈G W{σn(g)}, then there is a manipulation that makes p win, and the algorithm will find it.Proof. We first prove part 1. Denote W = {w 1, . . . , wn}. We have the set G W , and we suppose that σn(p) (cid:3) q(W ). Let usconsider a ballot X T of votes in T , and let σ (cid:10)n(c) be the scores of the candidates c ∈ C implied by this ballot (including allthe votes in S). Since in Algorithm 2 p was placed at the top of the preference of each voter in T , we have that:σn(p) = σ0(p) +n(cid:4)j=1w j(m − 1) (cid:2) σ (cid:10)n(p).(1)On the other hand, since by Lemma 3.5, in Algorithm 2 the candidates of G W were ranked by all the voters in T in the last|G W | places, it follows that(cid:5)q(W ) = 1|G W |(cid:4)σ0(g) +n(cid:4)|G W |−1(cid:4)w jg∈G Wi=0Combining together (1) and (2) we get that σ (cid:10)(cid:10)( X T ) is the average of the scores), hence σ (cid:10)qj=1n(p) (cid:3) qn(p) (cid:3) σ (cid:10)(cid:6)i(cid:3) 1|G W |(cid:4)g∈G Wσ (cid:10)n(g) =: q(cid:10)( X T ).(cid:10)( X T ). There is at least one g ∈ G W such that σ (cid:10)n(g) (cid:2) qn(g), and so p will not win when X T is applied.(2)(cid:10)( X T ) (sinceAlso note that Algorithm 2 returns true only if it constructs a (valid) ballot that makes p win, and so for the caseσn(p) (cid:3) q(W ) the algorithm will return false.We now prove part 2 of the lemma. If σn(p) > maxg∈G Wand so the algorithm will find the manipulation. (cid:2){σn(g)}, then by Lemma 3.6 for all c ∈ C \ {p}, σn(p) > σn(c),Lemma 3.8 is independent of the lemmata before and after it, but is used directly in the proof of Theorem 3.4. Itgives a connection between the average score of the candidates in G W +{w} and G W , where w is the weight of someadditional manipulator. In other words, it bounds the effect that adding a manipulator has on the average score of thestrong candidates.Lemma 3.8. Let G W , q(W ) be as before. Then for w (cid:2) 1, q(W + {w}) − q(W ) (cid:3) w m−22 .⊆ G sProof. First, G W ⊆ G W +{w}, because for all s (cid:2) 0, G sWfirst n + 1 stages after any candidate in G W , and so for all gσn(g) (cid:3) 1|G W |1|G W +{w}|) = q(W ).σn(g(cid:4)(cid:4)(cid:10)g∈G W +{w}g(cid:10)∈G WW +{w}. Now, for all g ∈ G W +{w} \ G W , g was not ranked in the(cid:10) ∈ G W , σn(g) (cid:3) σn(g(cid:10)), and henceNow we can proceed:(cid:7)qW + {w}(cid:8)=1|G W +{w}|=1|G W +{w}|(cid:4)g∈G W +{w}(cid:4)σn+1(g)σn(g) +w|G W +{w}||G W +{w}|−1(cid:4)ii=0(cid:3) q(W ) + w= q(W ) + wg∈G W +{w}m−2(cid:4)m − 1ii=0.m − 222 . (cid:2)And so, q(W + {w}) − q(W ) (cid:3) w m−2σ j−1(g) (cid:2) σ j−1(gσ j−2(g) (cid:3) σ j−2(g(cid:10)).398M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412max(W ) m−2The purpose of Lemmata 3.10–3.13 is to show that for any weight list W , |W | = n it holds that maxg∈G W{σn(g)}−q(W ) (cid:3)2 . This fact is stated in Lemma 3.13, which is the only one directly used in the proof of Theorem 3.4.First we need to show that the scores of candidates in G W are concentrated, in a sense. This is intuitive, since thealgorithm doesn’t allow the score of any candidate in G W to “escape” by ranking it close to the bottom if its score becomestoo high in some stage. We will require the following definition:Definition 3.9. For an integer w (cid:2) 0, a finite non-empty set of integers A is called w-dense if when we sort the set innonincreasing order b1 (cid:2) b2 (cid:2) · · · (cid:2) bk (such that {b1, . . . , bk} = A), it holds that for all 1 (cid:3) j (cid:3) k − 1, b j+1 (cid:2) b j − w.So, formally, we want to show (Lemma 3.12) that σn(G W ) is w max-dense, where w max = max W . This will be accom-plished via a number of technical steps.Lemma 3.10. Let W be a list of weights, |W | = n. Let G W =(cid:10) ∈ G s−1gW , X ⊆ C \ {p} (perhaps X = ∅) and j, 0 (cid:3) j (cid:3) n, s.t. {σ j(g), σ j(g0(cid:3)s G sW , as before. Then for all s (cid:2) 1 and g ∈ G sW\ G s−1W there exist(cid:10))} ∪ σ j( X) is w max-dense, where w max = max(W ).(cid:3)Proof. Let s (cid:2) 1 and g ∈ G sWranked below g(cid:10)Case 1: j > 1. In this case g was ranked above g(cid:10)in stage j − 1. So we have:in stage j. We distinguish between two cases:\ G s−1W . By definition, there exist g(cid:10) ∈ G s−1W and a minimalj, 1 (cid:3) j (cid:3) n + 1, such that g was(cid:10)),(3)(4)Denote αd(h) := m− (place of h ∈ C at the preference list of voter d). Further, denote by wd the weight of voter d(cid:10)). Denote(so in stage d, h gets wdαd(h) points). g was ranked above g(cid:10) = g0, g1, . . . , gl = g be the candidates that got in stage j − 1 the points(cid:10)), and w := w j−1. Let gl = α j−1(g) − α j−1(g(cid:10)) + l), respectively. Our purpose is to show that {σ j−1(g0), . . . , σ j−1(gl)} is w-(cid:10)) + 1), . . . , w(α j−1(g(cid:10)), w(α j−1(gwα j−1(gdense, and therefore w max-dense. By definition of the algorithm,in stage j − 1, and hence α j−1(g) > α j−1(g(cid:10)σ j−2(g0) (cid:2) σ j−2(g1) (cid:2) · · · (cid:2) σ j−2(gl).Denote ut = σ j−2(gt) + wα j−1(g(cid:10)) for 0 (cid:3) t (cid:3) l. Then∀t,0 (cid:3) t (cid:3) l, σ j−1(gt) = ut + wt.So we need to show that {ut + wt | 0 (cid:3) t (cid:3) l} is w-dense. It is enough to show that:(5)(6)(a) For all t, 0 (cid:3) t (cid:3) l, if ut + wt < u0, then there exists t(b) For all t, 0 (cid:3) t (cid:3) l, if ut + wt > u0, then there exists t(cid:10)(cid:10), t < t, 0 (cid:3) t(cid:10) (cid:3) l, s.t. ut + wt < ut(cid:10) + wt(cid:10) < t, s.t. ut + w(t − 1) (cid:3) ut(cid:10) + wt(cid:10) (cid:3) ut + w(t + 1), and(cid:10) < ut + wt.Proof of (a): From (5) we getu0 (cid:2) · · · (cid:2) ul.(7)Also from (3) and (6) we have u0 (cid:3) ul + wl. Let 0 (cid:3) t (cid:3) l − 1 s.t. ut + wt < u0. Let us consider the sequence ut + wt, ut+1 +(cid:10)(cid:10) (cid:3) l s.t. ut + wt < ut(cid:10) + wtw(t + 1), . . . , ul + wl. Since ut + wt < u0 (cid:3) ul + wl, it follows that there is a minimal index t.Then ut(cid:10)−1 + w(tut(cid:10)−1 + wt(cid:10) − 1) (cid:3) ut + wt, and thus(cid:10) (cid:3) ut + w(t + 1).(cid:10), t < t(8)From (7) ut(cid:10) (cid:3) ut(cid:10)−1, and then(cid:10)(cid:10) (cid:3) ut(cid:10)−1 + wtut(cid:10) + wt.Combining (8) and (9) together, we get ut(cid:10) + wt(cid:10)analogous, by choosing tto be the maximal index such that ut(cid:10) + wt(cid:10) < ut + wt.(9)(cid:10) (cid:3) ut + w(t + 1). This concludes the proof of (a). The proof of (b) is(cid:10) ∈ G 0W ; therefore σ0(g) (cid:2) σ0(gCase 2: j = 1. We proceed by essentially reducing this case to Case 1. In Case 2 we have that s (cid:2) 2, because otherwise, ifW , because otherwise,(cid:10)) (cid:2), i.e., σ j(cid:10)−1(g(cid:10)(cid:10)). Bys = 1, then gby definition, g ∈ G s−1σ j(cid:10)−1(gcombining the last arguments, we get that σ j(cid:10)−1(g) (cid:3) σ j(cid:10)−1(g(cid:10)) = maxh∈C\{p}{σ0(h)} ⇒ g ∈ G 0W s.t. g(because otherwise g ∈ G s−1(cid:10)).W ), and it follows that σ j(cid:10)−1(g) (cid:3) σ j(cid:10)−1(g(cid:10)(cid:10)). g has never been ranked below gW . Therefore there exists gW , a contradiction. gwas ranked below gin some stage j(cid:10)(cid:10) ∈ G s−2(cid:10) /∈ G s−2(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)Let j0 be minimal s.t. σ j0 (g) (cid:3) σ j0 (gσ0(g) = σ0(g(cid:10)), hence {σ0(g), σ0(g(cid:10)). As in stage 1g was ranked below g(cid:10)(cid:10))} is 0-dense, and in particular w max-dense.Otherwise ( j0 (cid:6)= 0) it holds that σ j0−1(g) > σ j0−1(g(cid:10)) by the minimality of j0. So, we have that, it holds that σ0(g) (cid:2) σ0(g(cid:10)). If j0 = 0 then(cid:10)σ j0 (g) (cid:2) σ j0 (g),M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412399andσ j0−1(g(cid:10)) (cid:3) σ j0−1(g).These two inequalities are analogous to (3) and (4), with j − 1 replaced by j0, and the roles of g and gthis point we can proceed exactly as in Case 1, keeping in mind these cosmetic changes. (cid:2)(cid:10)exchanged. FromThe following lemma asserts that a dense set of the scores of candidates in stage j can be replaced by a dense set offinal scores by considering a possibly larger set of candidates.Lemma 3.11. Let W be a list of weights, |W | = n, w max = max(W ). Let H ⊆ C \ {p} s.t. σ j(H) is w max-dense for some 0 (cid:3) j (cid:3) n.Then there exists H(cid:10) ⊆ C \ {p} s.t. σn(H(cid:10)) is w max-dense.(cid:10), H ⊆ HProof. We have H ⊆ C \ {p} and 0 (cid:3) j (cid:3) n, s.t. σ j(H) is w max-dense. Denote H j := H . Define inductively for t = j, j +1, . . . , n − 1: Ht+1 = {g ∈ C \ {p} | minh∈Ht{σt(h)}}. Of course, for all t, Ht ⊆ Ht+1. It is easy tosee that if for some j (cid:3) t (cid:3) n − 1, σt(Ht) is w max-dense, then σt+1(Ht+1) is also w max-dense. So, we get by induction thatσn(Hn) is w max-dense, and H ⊆ Hn ⊆ C \ {p}. (cid:2){σt(h)} (cid:3) σt(g) (cid:3) maxh∈HtLemma 3.12. Let W be a list of weights, |W | = n, w max = max(W ). Let G W be as before. Then the set σn(G W ) is w max-dense.\ G s−1W , then g ∈ G sW for some s (cid:2) 1. By Lemma 3.10 there exist g1 ∈ G s−1Proof. Let g = g0 ∈ G W . If g /∈ G 0W and X1 ⊆(cid:10)C \ {p} s.t. {σ j(g0), σ j(g1)} ∪ σ j( X1) is w max-dense for some 0 (cid:3) j (cid:3) n. By Lemma 3.11 there exists X⊆ C \ {p},1, X1 ⊆ XW , then there exist g2 ∈ G s−2(cid:10)s.t. {σn(g0), σn(g1)} ∪ σn( X1. Similarly, if g1 /∈ G 0(cid:10)and X2, etc. Thus, we can build a sequence ofsets Z1, . . . , Z s+1, s.t. for all 1 (cid:3) t (cid:3) s + 1, σn(Zt) is w max-dense, g = g0 ∈ Z1 and for each 1 (cid:3) t (cid:3) s there exists gt ∈ G s−ts.t. gt ∈ Zt ∩ Zt+1, and in particular, gs ∈ G 0(cid:3)(cid:10)2) is w max-dense. Denote Z2 := {g1, g2} ∪ X(cid:10)1) is w max-dense. Denote Z1 := {g0, g1} ∪ X(cid:10)2 s.t. {σn(g1), σn(g2)} ∪ σn( X(cid:10) (cid:6)= ∅ then A ∪ A, if A ∩ AW .(cid:10)1WWW(cid:10)(cid:10)is also w-dense, and hence we get Z g :=⊆ ˆZ ⊆ C \ {p} s.t. σn( ˆZ ) is w max-W ) is w max-dense, and hence there exists ˆZ , G 0WIt is easy to see that for two w-dense sets A, As+1t=1 Zt is w max-dense. Note that σ0(G 0dense. Hence, σn(Z g ∪ ˆZ ) is w max-dense.The sets Z g ∪ ˆZ , for all g ∈ G W , all intersect in ˆZ , and their union is ˆZ ∪(cid:3)(cid:3)ˆZ ∪hence σn(G W ) is a also w max-dense. (cid:2)Z g} is w max-dense. By Lemma 3.6, for all h ∈ ˆZ ∪g∈G Wg∈G WZ g , if h /∈ G W , then σn(h) (cid:3) ming∈G WZ g . We deduce that {σn(g) | g ∈{σn(g)}, andg∈G W(cid:3)Lemma 3.13. Let W be a list of weights, |W | = n, w max = max(W ). Let G W be as before, and denote q(W ) = 1|G W |before. Then maxg∈G W{σn(g)} − q(W ) (cid:3) w maxm−22 .(cid:2)g∈G Wσn(g), asProof. Sort the members of G W by their scores after the nth stage, i.e., G W = {g1, . . . , g|G W |} s.t. for all 1 (cid:3) t (cid:3) |G W | − 1,σn(gt) (cid:2) σn(gt+1). Denote for 1 (cid:3) t (cid:3) |G W |, ut = σn(g1) − w max(t − 1), and let U = {u1, . . . , u|G W |}. |U | = |G W |, max U ={σn(g)}. By Lemma 3.12, it is easy to see that for all 1 (cid:3) t (cid:3) |G W |, σn(gt) (cid:2) ut . Consequently, q(W ) (cid:2)σn(g1) = maxg∈G W(cid:2)|G W |1|G W |t=1 ut , hence(cid:9)(cid:10)σn(g)maxg∈G W− q(W ) = u1 − q(W ) (cid:3) u1 − 1|G W ||G W |(cid:4)t=1ut = w max|G W | − 12(cid:3) w maxm − 22.(cid:2)We are finally ready to prove Theorem 3.4.Proof of Theorem 3.4. Regarding part 1, Algorithm 2 returns true only if it constructs a (valid) ballot that makes p win, andthus if there is no ballot making p win, Algorithm 2 will return false.We now prove part 2 of the theorem. Suppose that there exists a ballot making p win for weight list W , |W | = n. Let(cid:10) := W + {w(cid:2) max(W ). By Lemma 3.7, σn(p) > q(W ). From Lemma 3.8 we get by induction} for k (cid:2) 1,(cid:2)ki=1 w(cid:10)1, . . . , w(cid:10)i(cid:10)kWthat(cid:10)q(W) (cid:3) q(W ) +k(cid:4)i=1w(cid:10)i· m − 22.By Lemma 3.13 and (10) we get:(10)400M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412(cid:9)(cid:10)σn+k(g)maxg∈G W(cid:10)(cid:3) q(W(cid:10)) + max(W(cid:10)) · m − 22k(cid:4)(cid:3) q(W(cid:10)) +(cid:3) q(W ) +< σn(p) +i=1k(cid:4)i=1k(cid:4)i=1w(cid:10)i· m − 22w(cid:10)i· (m − 2)w(cid:10)i· (m − 1) = σn+k(p)and hence, by Lemma 3.7 the algorithm will find a ballot making p win for set TThis completes the proof of Theorem 3.4. (cid:2)(cid:10)with weights W(cid:10), and will return true.The following is an example where there is a manipulation for weight list W , but Algorithm 2 will find a manipulationonly for weight list W + {w(cid:10)}.(cid:10) = 1, so we are actually talking about the special case of unweightedExample 3.14. In our example W = {1, 1, 1, 1}, wcoalitions. Consider the set C = {p, 1, 2, 3, 4, 5, 6}, m = |C| = 7, N = |S| = 5. 3 voters in S voted 6 (cid:16) 5 (cid:16) 4 (cid:16) 3 (cid:16) 2 (cid:16) p (cid:16) 1,and the other 2 voters in S voted 2 (cid:16) 3 (cid:16) 4 (cid:16) 5 (cid:16) 6 (cid:16) p (cid:16) 1. When applying Algorithm 2 to this input, the voters in T willaward the candidates with the following scores (we denote by α j(c) the points that voter j gives to candidate c):4Candidate c ∈ Cσ0(c)α1(c)α2(c)α3(c)α4(c)α5(c)p566666So the cumulative scores will be as follows:Candidate c ∈ Cσ0(c)σ1(c)σ2(c)σ3(c)σ4(c)σ5(c)p51117232935105555510510152025218404042182222262630319313133192223262730420222224202224262830521131315212225262930622040406222226263030Note that after 4 stages, the algorithm still did not find a manipulation: σ4(p) = 29 < 30 = σ4(6). However, if we changethe votes of the third and fourth voters of T , then we find an appropriate ballot:Candidate c ∈ Cσ0(c)α1(c)α2(c)α(cid:10)3(c)α(cid:10)4(c)p5666610555521840333193141420220452113126220420Now the cumulative scores are:4 We assumed here that when two candidates have the same scores up until a certain stage, the current voter will award fewer points to the candidatewith lower index, but any tie-breaking rule will give the same results.M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412401Candidate c ∈ Cσ0(c)σ1(c)σ2(c)σ (cid:10)3(c)σ (cid:10)4(c)p5111723291051015202182222252831922232728420222424285212225262862222262828Evidently, for any c ∈ C \ {p}, σ (cid:10)4(p) = 29 > σ (cid:10)4(c).3.2. MaximinIn this subsection, we show that Algorithm 1 also does well with respect to the Maximin rule.Lemma 3.15. Maximin is monotone in weights.Proof. Let X S be the preference orders of the voters in S, and let X T be the preference orders of the voters in T that(cid:10) = W + {w} wheremake p win. We need to show that there are preference orders for Tw (cid:2) 1 is an integer, that make p win. Fix some order on voters in T . By definition, for all c ∈ C \ {p}, σn(c) < σn(p). Letvote with p at the first place, and some arbitrary order on the other candidates. Then for allthe additional voter of Tc ∈ C \ {p}, σn+1(p) = σn(p) + w > σn(c) + w (cid:2) σn+1(c), and so we got the ballot of votes of T(cid:10) = T + {v} with weight list Wto make p win. (cid:2)(cid:10)(cid:10)Theorem 3.16. In CCWM under Maximin, let C be the set of candidates with p ∈ C the preferred candidate, and S the set of voters whoalready cast their votes. Let W be the weight list for the set T . Then:1. If there is no ballot making p win the election, then Algorithm 1 will return false.2. If there is a ballot making p win the election, then for the same instance with weight list W(cid:10)s.t. W(cid:10) ⊇ W + W (i.e., W(cid:10)containstwo copies of W ), Algorithm 1 will return true.Let us introduce some more notation. For candidates g, gthe voters after j stages (including the voters in S) that prefer g over gfor g ∈ C , 0 (cid:3) j (cid:3) n:MIN j(g) =(cid:10)(cid:9)h ∈ C \ {g} | N j(g, h) = σ j(g).(cid:10) ∈ C and 0 (cid:3) j (cid:3) n we denote by N j(g, g(cid:10). So σ j(g) = ming(cid:10)∈C\{g} N j(g, g(cid:10)) the total weight of(cid:10)). We also denoteIn words, MIN j(g) is the set of candidates that constitute the worst opponents of g in pairwise elections at stage j. Putdifferently, these are the candidates whose competition against g defines the Maximin score of g at stage j.Fixing the set C , p ∈ C , and an order on the weight list W , we denote by f ( j) the maximal score of p’s opponentsdistributed by Algorithm 1 after j stages:f ( j) = maxg∈C\{p}σ j(g).In Algorithm 1, p is always placed at the top of each preference, and so with each voter its score grows by the weightof this voter. In our next lemma we will put forward an upper bound on the growth rate of the scores of p’s opponents.Lemma 3.17. Consider Algorithm 1 applied to the Maximin rule. Denote by w j the weight of the jth voter processed by the algorithm.Then for all 0 (cid:3) j (cid:3) n − 2, f ( j + 2) (cid:3) f ( j) + max{w j+1, w j+2}.To intuitively see why the lemma implies Theorem 3.16, notice that if there are two copies of W , the score of p wouldw∈W w.w∈W w, whereas by the lemma the score of the strongest candidate would increase by at mostincrease by 2 ·We now prove the lemma; the theorem will follow easily.(cid:2)(cid:2)Proof. Let 0 (cid:3) j (cid:3) n − 2. Let g (cid:6)= p be a candidate. By definition σ j(g) (cid:3) f ( j). We would like to show that σ j+2(g) (cid:3)f ( j) + max{w j+1, w j+2}. If σ j+1(g) (cid:3) f ( j), then σ j+2(g) (cid:3) σ j+1(g) + w j+2 (cid:3) f ( j) + max{w j+1, w j+2}, and we are done.So let us assume now that σ j+1(g) > f ( j).Define a directed graph G = (V , E), whereV = {g} ∪(cid:9)(cid:10)x ∈ C \ {p} | x was ranked below g in stage j + 1,and (x, y) ∈ E iff y ∈ MIN j(x). There is at least one outgoing edge from g in E, since otherwise there was gvoter j + 1 ranked above g, and then σ j+1(g) = σ j(g) (cid:3) f ( j), a contradiction.(cid:10) ∈ MIN j(g) that402M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412Fig. 1. The induced sub-graph G(g).In addition, we claim that for all x ∈ V \ {g} there is at least one outgoing edge from x in E. Indeed, otherwise there is(cid:10) ∈ MIN j(x) that was ranked above g in stage j + 1. Hence, we have thatxσ j+1(x) = σ j(x) (cid:3) f ( j) < σ j+1(g).∗(cid:10)(cid:10)This implies that Algorithm 1 should have ranked x above g in stage j + 1, which is a contradiction to the fact thatx ∈ V \ {g}.For x ∈ V , denote by V (x) all the vertices y in V such that there exists a directed path from x to y. Denote by G(x) the(cid:10) ∈ Uin the cycle:sub-graph of G induced by V (x). It is easy to see that G(g) contains at least one cycle. Let U be one such cycle. Let gbe the vertex that was ranked highest among the vertices of U in stage j + 1. Let g(cid:10)(g(cid:10)(cid:10), gSuppose, for contradiction, that σ j+2(g) > f ( j) + max{w j+1, w j+2}. g was ranked by j + 2 at place tat stage j + 1, it follows that σ j+1(g(cid:10)(cid:10)(cid:10)(cid:10)) = σ j(gbe the vertex before g(cid:10)) ∈ U . Since gwas ranked below g(cid:10)(cid:10)) (cid:3) f ( j).. Then g(cid:10)(cid:10)(cid:10)(cid:10)∗(cid:10)ranked by j + 2 above tf ( j) + w j+2 < σ j+2(g)) instead of g—a contradiction.∗, since otherwise when we had reached the place t, we would pick g(cid:10)(cid:10)(with score σ j+2(gwas(cid:10)(cid:10)) (cid:3)Denote by X1 all the vertices in V (g) that have an outgoing edge to gin G(g). For all x ∈ X1, g(cid:10)(cid:10)). All x ∈ X1 were ranked by j + 2 above g, since otherwise, if there was x ∈ X1, s.t. until the place tσ j(x) = N j(x, gwas not added to the preference list, then when evaluating its score on place tN j+1(x, g(cid:10)(cid:10)) + w j+1 = σ j(x) + w j+1 < σ j+2(g), and so we would put x instead of g., we would get: σ j+2(x) (cid:3) N j+2(x, g(cid:10)(cid:10)) (cid:3) N j(x, g∗Denote by X2 all the vertices in V (g) that have an outgoing edge to some vertex x ∈ X1. In the same manner we canshow that all the vertices in X2 were ranked in stage j + 2 above g. We continue in this manner, by defining sets X3, . . . ,where the set Xl contains all vertices in V (g) that have an outgoing edge to some vertex in Xl−1; the argument aboveshows that all elements of these sets are ranked above g in stage j + 2. As there is a path from g to gin G(g), we willeventually reach g in this way, i.e., there is some l such that Xl contains a vertex g0 with an edge from g to g0 (see Fig. 1).(cid:10)(cid:10)(cid:10)(cid:10) ∈ MIN j(x), i.e.,∗it still(cid:10)(cid:10)) =Thus,σ j+2(g) (cid:3) N j+2(g, g0) = N j+1(g, g0) (cid:3) N j(g, g0) + w j+1= σ j(g) + w j+1 (cid:3) f ( j) + max{w j+1, w j+2}< σ j+2(g),a contradiction. (cid:2)We are now ready to prove Theorem 3.16.Proof. We prove part 1. Algorithm 1 returns true only if it constructs a (valid) ballot that makes p win, and thus if there isno ballot making p win, Algorithm 1 will return false.We now prove part 2. Suppose that there exists a ballot Z T making p win for weight list W = {w 1, . . . , wn}. Let σ (cid:10)j(g)be the scores implied by Z T . Then:f (0) < σ (cid:10)n(p) (cid:3) σ0(p) +n(cid:4)i=1w i.(11)(cid:10) = W + W + X , where X is some list of weights (possibly empty). We need to show that σ|W (cid:10)|(p) > f (|W(cid:10)|). In, the equal weights of two copies of W will be adjacent, i.e., the order of(cid:10)Let WAlgorithm 1, after sorting the weights of Wweights in Wwill be of the form:(cid:10)x1, . . . , xq1 , w 1, w 1, xq1+1, . . . , xq2 , w 2, w 2, . . . , wn, wn, xqn+1, . . . , x| X|.By Lemma 3.17, one can prove by induction that:(cid:7)|W(cid:8)(cid:10)|f(cid:3) f (0) +| X|(cid:4)n(cid:4)w i.xi +i=1i=1(12)M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412403And so by (11) and (12) we have:σ|W (cid:10)|(p) = σ0(p) +| X|(cid:4)i=1xi + 2n(cid:4)i=1w i > f (0) +| X|(cid:4)n(cid:4)w i (cid:2) f(cid:7)|W(cid:8).(cid:10)|xi +(cid:2)i=1i=1In Appendix B we give a simple algorithm, which is tailor-made for Maximin, and also enjoys the implications of The-orem 3.16. However, this algorithm does not extend to other voting rules, as Algorithm 1 does. Moreover, we believe thatAlgorithm 1 does better when it comes to unweighted manipulation (see Sections 4 and 6).3.3. Plurality with runoffIn this subsection we present a heuristic algorithm for the CCWM problem in Plurality with Runoff. The algorithmreceives as a parameter a size of window 0 (cid:3) u (cid:3) max(W ) where it can give a wrong answer. Its running time depends onthe size of its input and on u (see below). We begin by noting:Lemma 3.18. Plurality with Runoff is monotone in weights.Proof. Let C be the candidates, p ∈ C is the preferred candidate, S is the set of truthful voters, and W are the weights formanipulators of T . Suppose that there is a ballot of votes of T that makes p win the election. We need to show that there(cid:10) = W + {w}, where w (cid:2) 1. Let g be the candidate that proceeds with p to theis a ballot making p win for the set Wsecond round in the winning ballot for W . Let the additional voter vote p (cid:16) . . . . Then the plurality score of p and g will notdecrease, while the plurality score of any other candidate will remain the same, and so p and g will proceed to the nextround in the new ballot as well. In the second round p will beat g in the new ballot, since the total weight of the voterswho prefer p to g increased, while the total weight of voters who prefer g to p remained the same. Thus, p will win theelection in the new ballot. (cid:2)We will now give an informal description of the algorithm. We go over all the candidates other than p. To each candidateg we try to assign the voters with minimal total weight, such that if these voters place g first, g continues to the secondround; the rest of the voters rank p first. If we succeeded in this way to make g and p survive the first round, and in thesecond round p beats g, then we found a valid ballot for making p win the election. If no candidate g was found in thisway, then we report that there is no ballot.A formal description of this algorithm, Algorithm 3, is given below. The following additional notations are required.Denote by β X (g) the plurality score of g from voter set X (i.e., the sum of weights of the voters in X that put g at the topof their preferences). We also use N X (g, g, andw v is the weight of voter v. Finally, for g, gv∈U w v , where U is the set of all the voters in X that prefer g to gis broken in favor of g.(cid:10) ∈ C we denote g (cid:19) gif a tie between g and g(cid:10)) =(cid:2)(cid:10)(cid:10)(cid:10)Remark 3.19. In Algorithm 3 we do not rely on the assumption that for all g (cid:6)= p, g (cid:19) p. In fact, the algorithm can dealwith any tie-breaking mechanism such that for every two distinct candidates x and y, either x (cid:19) y or y (cid:19) x, regardless ofhow the manipulators cast their votes. An example of such a tie-breaking mechanism is to favor candidates with smallerindices, according to some order on the candidates. This is not necessarily a reasonable way to break ties in, say, politicalelections, but roughly speaking it is more general than asking that p be a unique winner, the assumption underlying ourprevious results.More precisely, Plurality with Runoff differs from Borda and Maximin in the sense that it has two different rounds, andtherefore two different “scores”. Hence, the unique winner model can be interpreted ambiguously in this context. If wealways break ties against p (the algorithm supports this), p might be tied against another candidate for the second ticket tothe second round, and lose, whereas under another interpretation p would have advanced to the second round, and wouldhave won the second round by a vast majority, thus becoming a unique winner.In the next theorem we prove the correctness of Algorithm 3, and analyze its time complexity. We will see that forgetting an exact answer (u = 0), we will need running time which is polynomial in max(W ) and the rest of the input. Asthe weights in W are specified in binary representation, this requires exponential time. However, when the size of the errorwindow increases, the complexity decreases, so for u = Ω( max(W )log(max(W )) ) the complexity of the algorithm is polynomial in itsinput.Theorem 3.20. In CCWM under Plurality with Runoff, let C be the set of candidates with p ∈ C the preferred candidate, and S be theset of voters who already cast their votes. Let W be the weight list for the set T , and let u (cid:2) 0 be the error window. Then:1. If there is no ballot making p win the election, then Algorithm 3 will return false.2. If there is a ballot making p win the election, then for the same problem with voter set TW(cid:10) = W + {wn+1, . . . , wn+l}, where l (cid:2) 0,j=1 wn+ j (cid:2) u, Algorithm 3 will return true.(cid:2)l(cid:10) = T + {vn+1, . . . , vn+l} with weight list404M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–4121: procedure Plurality-with-runoff(C, p, X S , W S , W , u)(cid:2) X S is the set of preferences of votersin S, W S are the weights of voters in S, W = {w 1, . . . , wn} are the weights of voters in T , u is thesize of error windowfor g in C \ {p} doif there exists g(cid:2)Go over candidates in C \ {p}(cid:10) (cid:19) g then(cid:10) (cid:6)= g s.t. gelse(cid:10)), gcontinue(cid:10)) − βS (g)end ifif λg >λg ← maxg(cid:10)∈C\{p} βS (gλg ← maxg(cid:10)∈C\{p} βS (g(cid:2)x ∈ {0, 1}n minimizes {(cid:2)ni=1 w i then(cid:10) ∈ argmaxg(cid:10)∈C\{p}βS (g(cid:10)) − βS (g) + 1end ifx ← subset-of-weights-approximate(W , λg , u)All the voters j s.t. x j = 1 vote g (cid:16) . . .All the voters j s.t. x j = 0 vote p (cid:16) . . .if ∃g2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:end if23:end for24:25:return false26: end procedure27:28: procedure subset-of-weights-approximate(W , λg , u)(cid:10) ∈ C \ {p, g} s.t. (βS (gor (βS (g(cid:10)) > βS (p) + βT (p))(cid:10)) = βS (p) + βT (p) and greturn truecontinuecontinueelse(cid:2)If we cannot make g pass to the next round(cid:2)Go to the next candidate in the main loop(cid:2)j=1 w j x j |j=1 w j x j (cid:4) λg , ∀ j, x j ∈ {0, 1}}n(cid:2)Order of candidates except g is arbitrary(cid:2)n(cid:10) (cid:19) p) then(cid:2) p does not pass to next round(cid:2)No appropriate g was found(cid:2)W = {w 1, . . . , wn} are the weights ofend ifif (N S∪T (p, g) > N S∪T (g, p)) or (N S∪T (p, g) = N S∪T (g, p) and p (cid:19) g) then(cid:2) p beats g in the second roundvoters in T , λg is the minimum total sum of desired weights, u is the size of error window31:(cid:22) + 129:30:Check that 0 (cid:3) u (cid:3) max(W )ku ← (cid:21) u2nj=1 w j x j (cid:3)Solve by dynamic prog.: max{Let x ∈ {0, 1}n be the vector that maximizes the above sum32:return (cid:3)1 − x33:34: end procedure(cid:2)nj=1(cid:21) w jku(cid:22)x j |(cid:2)n(cid:2)nj=1 w j − λg , ∀ j, x j ∈ {0, 1}}(cid:2)(cid:3)1 is the vector of n 1’sAlgorithm 3. Decides CCWM in Plurality with Runoff with desired accuracy.3. On input C, p, X S , W S , W , u, where |C| = m, |S| = N, |W | = n, u is an integer, s.t. 0 (cid:3) u (cid:3) max(W ), the running time ofAlgorithm 3 is polynomial in m, N, log(max(W S )), n and max(W )u+1.Proof. We start with part 1. Note thatx = (x1, . . . , xn) satisfiesn(cid:4)j=1w j x j (cid:3)n(cid:4)j=1w j − λg ⇐⇒ x = (cid:3)1 − x satisfiesn(cid:4)j=1w j x j (cid:2) λg,(13)where λg is defined in Algorithm 3 as the total weight of the votes g needs in order to proceed to the second round,and x is the binary vector of length n computed in the algorithm’s subroutine. Thus when voters corresponding to weightsreturned by the function SUBSET-OF-WEIGHTS-APPROXIMATE() (see Algorithm 3) vote g (cid:16) . . . , they ensure that g proceedsto the second round. It is easy to see that whenever Algorithm 3 returns true, it actually finds a (valid) ballot making p winthe election, and so if there is no such ballot, then the algorithm will return false.We now move on to part 2. Let A W be an instance of the problem with weight list W . Suppose that there exists ballot X TY (g)(cid:10)) =w v , where∪ XT(cid:10) = W + {wn+1, . . . , wn+l},(cid:10), u.of votes in T s.t. combined with preferences X S of voters of S, it makes p win the election in A W . We will denote by β(cid:10)the plurality score of g from voter set Y under the preferences X S ∪ X T . Also, we denote NU X S ∪X T is the set of all the voters in Y that prefer g to gwhere l (cid:2) 0,(cid:10)Y (g, gunder X S ∪ X T . Let 0 (cid:3) u (cid:3) max(W ), Wj=1 wn+ j (cid:2) u. We need to show that Algorithm 3 will return true on the input WThere is a candidate g (cid:6)= p that passes together with p to the second round when applying the preferences X T together(cid:10)), and(cid:10) /∈ {p, g} and c ∈ {p, g}, if c (cid:19) g, then β(cid:10)(cid:10)) + β(cid:10)v∈U X S(cid:2)l(cid:2)(cid:10)(cid:10)S (c) + β(cid:10)T (c) (cid:2) β(cid:10)S (gT (gwith X S on A W , and thus for each candidate gif g(cid:10) (cid:19) c, then β(cid:10)(cid:10)) + β(cid:10)T (c) > β(cid:10)S (c) + β(cid:10)T (gS (g(cid:10)). Also,β(cid:10)T (p) + β(cid:10)T (g) (cid:3)n(cid:4)j=1w j.(14)M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412405Now consider Algorithm 3 applied to A W (cid:10) . If it does not reach g in the main loop, then it will exit earlier returning “true”,meaning that it will find a desired ballot making p win. Otherwise, it will reach the candidate g. λg is the minimal sum ofweights that ensures that g will continue to the second round, and henceλg (cid:3) β(cid:10)T (g) (cid:3)n(cid:4)j=1w j (cid:3)n+l(cid:4)j=1w j.(15)We will reach the function SUBSET-OF-WEIGHTS-APPROXIMATE(), and enter it with arguments Wvector x = (x1, . . . , xn+l) returned by SUBSET-OF-WEIGHTS-APPROXIMATE() satisfiesto the next round. Now we show that p will also continue to the next round. Denote by H the maximization problem(cid:10), λg and u. By (13), then+lj=1 w j x j (cid:2) λg , and so g will continue(cid:2)n+l(cid:4)maxw j x jj=1n+l(cid:4)w j x j (cid:3)s.t.n+l(cid:4)w j − λgj=1j=1x j ∈ {0, 1},∗ = { j | x j = 1, x = (x1, . . . , xn+l) is the optimal solution to H}. Denote Pfor j = 1, . . . , n + l.Let Jof the above maximization problem:(16)(cid:2)∗ =j∈ J ∗ w j . Let H(k) be the scaled versionmax(cid:21) w jk(cid:22)x jn+l(cid:4)j=1n+l(cid:4)s.t.w j x j (cid:3)j=1x j ∈ {0, 1},n+l(cid:4)j=1w j − λg(17)for j = 1, . . . , n + l.J (k) = { j | x j = 1, x = (x1, . . . , xn+l) is the optimal solution to H(k)}. Let P (k) =Letwhich we obtained in SUBSET-OF-WEIGHTS-APPROXIMATE() satisfies, for ku = (cid:21)u2(n+l)(cid:22) + 1:(cid:2)j∈ J (k) w j . Now, x = (x1, . . . , xn+l)n+l(cid:4)j=1(cid:4)(cid:4)(cid:11)kuw j (cid:2)w j x j =(cid:12)(cid:2)(cid:11)(cid:4)kuw jku(cid:12)(cid:2)(cid:4)(cid:7)j∈ J ∗(cid:8)w j − (ku − 1)w jkuj∈ J (ku )(cid:4)j∈ J (ku )w j − (ku − 1)| J=j∈ J ∗j∈ J ∗∗ − (ku − 1)| J∗|.∗| = PHence, the vector x = (cid:3)1 − x returned by the function, satisfies:n+l(cid:4)j=1w j x j (cid:3)(cid:3)(cid:3)n+l(cid:4)j=1n+l(cid:4)j=1n+l(cid:4)j=1w j − P∗ + (ku − 1)| J∗|(cid:11)(cid:11)w j − P∗ +w j − P∗ +(cid:12)(n + l)u2(n + l)(cid:12).u2By definition of P∗n+l(cid:4)j=1w j − P∗ = min, we get:(cid:13)n+l(cid:4)(cid:13)j=1n(cid:4)j=1(cid:3) min(cid:3) β(cid:10)T (g).(cid:14)w j x j (cid:2) λg, x j ∈ {0, 1}, 1 (cid:3) j (cid:3) n + lw j x j (cid:2) λg, x j ∈ {0, 1}, 1 (cid:3) j (cid:3) n(cid:14)w j x j |w j x j |n+l(cid:4)j=1n(cid:4)j=1(18)(19)(20)406M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412Combining (19) and (20), we get that for vector x returned by the function SUBSET-OF-WEIGHTS-APPROXIMATE():βT (cid:10) (g) =n+l(cid:4)j=1w j x j (cid:3) β(cid:10)T (g) +(cid:11)(cid:12).u2In the algorithm, all the voters j s.t. x j = 0 will vote p (cid:16) . . . , and so we will haveβT (cid:10) (p) ==n+l(cid:4)j=1n(cid:4)j=1w j −n+l(cid:4)j=1n(cid:4)w j x j (cid:2)(cid:15)w j + u −β(cid:10)T (g) +(cid:12)(cid:16)(cid:11)u2w j − β(cid:10)T (g) +(cid:17)u2j=1(cid:18)(cid:2) β(cid:10)T (p) +(cid:17)(cid:18).u2(21)(22)For any candidate c such that c /∈ {p, g}, c was never ranked at the top of the preference lists by Algorithm 3, and soβS∪T (cid:10) (c) = βS (c) (cid:3) β(cid:10)S∪T (c). On the other hand, by (22),βS∪T (cid:10) (p) = βS (p) + βT (cid:10) (p) (cid:2) βS (p) + β(cid:10)T (p) +(cid:17)(cid:18)= β(cid:10)S∪T (p) +(cid:18)(cid:17)u2(cid:2) β(cid:10)S∪T (p).u2Recall that p beats c in the first round under X T . It follows that p beats c in the first round under Algorithm 3, and so pwill continue to the next round.We now prove that p beats g in the next round. If g (cid:19) p, then in the winning ballot X T , N(cid:10)S (g, p) + N(cid:10)T (g, p). From (21) we get:(cid:10)T (g, p), otherwise NN(cid:10)S (p, g) + N(cid:12)(cid:11)(cid:10)(cid:10)T (p, g) (cid:2) NS (g, p) + N(cid:11)(cid:12)(cid:10)S (p, g) + N(cid:10)T (p, g) >N T (cid:10) (g, p) = βT (cid:10) (g) (cid:3) β(cid:10)T (g) +Thus, from (23):u2(cid:3) N(cid:10)T (g, p) +u2.N T (cid:10) (p, g) =n+l(cid:4)j=1w j − N T (cid:10) (g, p) (cid:2)(cid:15)w j + u −N(cid:10)T (g, p) +(cid:12)(cid:16)(cid:11)u2= N(cid:10)T (p, g) +(cid:17)(cid:18).u2n(cid:4)j=1So, for g (cid:19) p we getN(cid:10)S (p, g) + N T (cid:10) (p, g) (cid:2) N(cid:10)S (p, g) + N(cid:10)T (p, g) +> N(cid:10)S (g, p) + N(cid:10)T (g, p) +(cid:2) N(cid:10)S (g, p) + N T (cid:10) (g, p).In the same way, for p (cid:19) g we getN(cid:10)S (p, g) + N T (cid:10) (p, g) (cid:2) N(cid:10)S (g, p) + N T (cid:10) (g, p).(cid:18)(cid:12)(cid:17)(cid:11)u2u2Therefore, p wins the second round of the election, and hence the entire election; the algorithm will return true.Next, we prove part 3. Using the notation of the previous part,let P (k) be the maximum sum of weights fromW = {w 1, . . . , wn}, solving the scaled maximization problem H(k).5 There is a well-known dynamic programming algo-rithm solving the knapsack problem H(k) in time O (n P (k)) (see, e.g. [19, Chapter 9]). Furthermore, P (k) (cid:3)(cid:22) (cid:3)(cid:22) + 1 (cid:2) u+1n(cid:21) max(W )k. The algorithm sets ku = (cid:21) u2n2n , and so we have:(cid:22) (cid:3) n max(W )(cid:2)nj=1(cid:21) w jkkP (ku) (cid:3) nmax(W )ku(cid:3) nmax(W )u+12n= 2n2 max(W )u + 1.(27)Thus we can solve H(ku) in O (n P (ku)) = O (n3 · max(W )u+1polynomial in its inputs; hence, the proof is completed. (cid:2)). It is easy to see that all the other steps of Algorithm 3 are5 We slightly abuse notation here, as we defined the optimization problems for weight set W(cid:10) = {w 1, . . . , wn+l}, but the definition for the set W isanalogous.(23)(24)(25)(26)M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–4124074. Unweighted coalitional manipulationIn this section, we discuss the application of the results given above to unweighted coalitional manipulation (the CCUMproblem), and present a new theorem. We will see that some of our theorems can be translated into approximation (in theclassical sense) results in this natural setting.It is known that the CCUM problem is tractable—with respect to any voting rule that can be computed in polynomialtime—when the number of candidates is constant [7]. However, to the best of our knowledge (at the time of submission)there are no results regarding the complexity of the problem when the number of candidates is not constant, except forthe cases of STV and Second Order Copeland where CCUM is hard even when there is only a single manipulator [2,3]. Weconjecture that CCUM in Borda and Maximin is N P -complete.In the context of unweighted manipulation, one can consider the following optimization problem:Definition 4.1. In the Constructive Coalitional Unweighted Optimization (CCUO) problem, we are given the (unweighted)votes of the truthful voters. We must find the minimum number of manipulators needed in order to make p win (i.e., theminimum number of manipulators that can cast their (unweighted) votes in a way that makes p win).Then, our theorems almost directly imply the following corollary:Corollary 4.2.1. Algorithm 2 approximates CCUO in Borda up to an additive error of 1.2. Algorithm 1 is a 2-approximation algorithm for CCUO in Maximin.Proof. It is enough to show that the minimum number of manipulators needed in order to make p win, in Borda and Max-imin, must be polynomial in the rest of the input. Indeed, in this case we can apply brute-force search using Algorithms 2and 1, respectively, in order to approximate the answer. In other words, we run the algorithm once for every number ofmanipulators k ∈ {0, . . . , p(n)} for some polynomial p. The minimum k which gives a true answer in Borda (resp., Maximin)is guaranteed to be larger by at most 1 (resp., twice as large) than the optimal answer by Theorem 3.4 (resp., Theorem 3.16).So, it is sufficient to prove the following two lemmata.Lemma 4.3. Let (cid:4)α1, . . . , αm(cid:5) be a scoring protocol where α1 − αm > 0. In the CCUO problem, let C = {c1, . . . , cm−1, p} be thecandidates, S be the set of the truthful voters, |S| = N, and nbe the minimal number of manipulators such that there exists a ballotmaking p win. Then n∗ (cid:3) (N + 1)(m − 1).∗∗ = (N + 1)(m − 1). Let the manipulator 1 (cid:3) j (cid:3) (N + 1)(m − 1)Proof. We show that there exists a ballot making p win for nvote p (cid:16) · · · (cid:16) ci+1, where j − 1 ≡ i mod(m − 1), 0 (cid:3) i (cid:3) m − 2, and the rest of the order is arbitrary. With every m − 1 votersthe difference between the scores of any candidate c j and p decreases by at least α1 − αm. Moreover, for any 1 (cid:3) j (cid:3) m − 1,σ0(c j) (cid:3) σ0(p) + N(α1 − αm), and so we get: σ(N+1)(m−1)(c j) (cid:3) σ(N+1)(m−1)(p) − (α1 − αm) < σ(N+1)(m−1)(p). Hence, p willwin the election. (cid:2)Lemma 4.4. Consider the CCUO problem in the Maximin protocol. In the notation of Lemma 4.3, n∗ (cid:3) N + 1.∗ = N + 1. Let every manipulator vote p (cid:16) . . . . Then forProof. We show that there exists a ballot making p win for nevery candidate c j we get: σN+1(c j) (cid:3) N N+1(c j, p) (cid:3) N. Moreover, for any candidate c j (cid:6)= p, N N+1(p, c j) (cid:2) N + 1, and soσN+1(p) (cid:2) N + 1. Hence we get for every candidate c j , σN+1(c j) < σN+1(p), implying that p will win. (cid:2)This concludes the proof of the corollary. (cid:2)On the other hand, we have the following results:Corollary 4.5. Algorithm 3 efficiently solves the CCUM problem in Plurality with Runoff.Proof. Follows as a special case of Theorem 3.20, where the error window is u = 0, the number of additional voters is l = 0,and all the weights equal 1. (cid:2)Theorem 4.6. Algorithm 2 efficiently solves the CCUM problem in Veto.A short discussion is in order regarding CCUM in Veto. Indeed, this problem can be solved efficiently by a trivial algo-rithm. The fact that each manipulator can veto a single candidate may be interpreted as follows: each manipulator picks408M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412one candidate such that the score of p increases by 1 relative to that candidate without changing with respect to any othercandidate. Thus, we simply have to count the number of manipulators needed to guarantee that p has more points thanany other candidate. Formally, if we denote by σ0(c) the score of candidate c based on the votes in S, then clearly thereexists a vote for T making p win if and only if(cid:4)maxc∈C\{p}(cid:8)(cid:7)σ0(c) − σ0(p) + 1, 0(cid:3) |T |.In the context of CCUM in Veto, Algorithm 2 is, in a sense, an instantiation of the simple scheme described above. However,our direct proof of Theorem 4.6, given in Appendix C, is a simpler, but analogous, version of the proof of Theorem 3.4.Finally, note that Corollary 4.5 and Theorem 4.6 imply that CCUO in Plurality with Runoff and Veto is also in P .5. Relation to work on frequency of manipulationAt this point, we would like to give a more in-depth exposition of previous work regarding frequency of manipulation,and connect it with this paper.An interesting approach to the abovementioned issue was presented by Conitzer and Sandholm [6]. They noticed thatan election instance can be manipulated efficiently if it satisfies two properties: weak monotonicity—a property which issatisfied by many prominent voting rules—and another, more arguable property: the manipulators must be able to makeone of exactly two candidates win the election. Conitzer and Sandholm empirically showed that the second property holdswith high probability in different standard voting rules. This empirical validation was carried out only with respect to smallcoalitions of voters and skewed distributions over election instances.Procaccia and Rosenschein [23] leveraged some of the intuitions provided by Conitzer and Sandholm. They analyzed theprobability of the manipulators being able to affect the outcome of the election (i.e., make one of at least two candidateswin), conditioned on the fraction of manipulators. They found that for quite general distributions over election instances,if n = o(N), the manipulators cannot affect the outcome with high probability; the opposite is true if n = ω(N). Theseresults extended previous work on asymptotic strategy proofness [1,28].√√Another result was recently presented by Friedgut, Kalai and Nisan [13]. They showed that a single manipulator can finda manipulation with relatively good probability by simply switching to randomly chosen linear preferences (in particular,high probability of success can be achieved by repeating this process a polynomial number of times). This is true providedthe voting rule in question is “far from dictatorial” in some well-defined sense. The proof of this theorem is beautiful, butsadly the current proof only works for at most 3 candidates.Most closely related to this paper is another work by Procaccia and Rosenschein [24], who have attempted to establish aframework which would enable showing that manipulations are typically easy. For this purpose, they have defined the no-tion of junta distributions, which are intuitively (and arguably) “hard to manipulate”, over election instances in the coalitionalmanipulation setting. Moreover, they have defined a voting rule to be susceptible to manipulation if there is an algorithmthat decides CCWM with high probability of success, when the instances are distributed according to a junta distribution.The rationale is that if there is an algorithm that does well with respect to these especially hard junta distributions, it wouldalso do well with respect to other reasonable distributions.Procaccia and Rosenschein’s main result is that scoring rules are susceptible to manipulation, according to the foregoingdefinition. Technically, Procaccia and Rosenschein’s result is in fact a very loose bound on the window of error of Algo-rithm 2. Although their analysis holds for any scoring rule, it suffers from two major shortcomings. First, it is much looserthan the one given in this paper, and consequently does not allow for corollaries regarding unweighted coalitional manipula-tion. In contrast, our result regarding Borda is far stronger, since the window of error is much more accurately characterized.The stronger result allowed, e.g., for Corollary 4.2. A second major disadvantage of Procaccia and Rosenschein’s analysis isthat it only applies to a constant number of candidates, i.e., m = O (1). However, since the result in Procaccia and Rosen-schein deals with scoring rules in general and here the only Scoring rules we deal with are Borda and Veto, neither resultstrictly subsumes the other.Erdélyi et al. [11] discuss the notion of junta distributions at length. They show that the idea of junta distributions, whenapplied to the SAT problem, is not sufficient to classify hard-to-decide distributions. Their work is inconclusive, however,when it comes to the application of junta distributions to hardness of manipulation problems.Still, it seems that at this point we lack a link between a mathematical framework dealing with frequency of manipula-tion, and hardness on average. In light of this, we shall shortly consider the intuitive frequency of manipulation implicationsof our results, without being too formal. Our theorems imply that our algorithms err on only very specific configurationsof the voters’ weights. It might be productive to imagine points on the real line as representing the total weight of T . Inthe case of Borda, then, our algorithm would give a correct negative answer on all points to the left of some point x, anda correct positive answer on all points to the right of x + max W . The range between x and x + max W is the window oferror. This is a simplification of the situation, but a useful one nonetheless.Now, intuitively consider some “reasonable” distribution over the instances of the CCUM problem (such that weightsare randomly selected). The fact that the distribution is “reasonable” guarantees that the manipulators’ total weight isdistributed over a large range. Therefore, the probability of hitting the tiny window of error is extremely small. This (onceagain, intuitively) means that with high probability, our algorithms would correctly decide the manipulation problem.M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–4124096. DiscussionWe would like to devote this final section to a short discussion regarding extensions of our results, and their applicationsto other voting rules.We have noted above (and elaborate on, in Appendix A) that Copeland’s rule is not monotone in weights. This seemsto preclude the type of analysis which we have presented here. Nevertheless, it might be possible to obtain similar resultsif one endows the manipulators with the option to abstain from voting. In this way, any voting rule must be monotonein weights, as additional manipulators can always abstain. This is also not a major departure from our model, where themanipulators can coordinate their votes; it is only natural to assume that they can also agree not to vote at all.The prominent Single Transferable Vote (STV) rule is one that we have not discussed above. In STV, the election proceedsin rounds; each voter casts his vote for the candidate he ranks first among the remaining candidates; the candidate withlowest score is eliminated. It is difficult to apply our approach to STV, for two reasons. First, it does not have a notion ofscore (but this is also true for Plurality with Runoff). Second, it is a very hard voting rule to manipulate. Indeed, it is wellknown that STV is hard to manipulate even for a single manipulator [2]. However, in theory STV is amenable to our type ofanalysis; this remains a fascinating direction for future research.Finally, we conjecture that our analysis of the performance of Algorithm 1 with respect to CCUM in Maximin is not tight:it might be possible to lower the bound from 2 to 3/2 by using a close variant of the algorithm.AcknowledgementsThe authors would like to thank Vincent Conitzer for excellent comments on a draft of this paper, and in particularfor pointing out the alternative 2-approximation algorithm for Maximin given in Appendix B. The authors also thank theanonymous AIJ reviewers for insightful comments. This work was partially supported by Israel Science Foundation grant#898/05.Appendix A. Copeland is not monotone in weightsWhen discussing Scoring rules, Maximin, and Plurality with Runoff, we are motivated to look for approximate solutionsto the CCWM problem by the fact that these voting rules are monotone in weights. In contrast, Copeland is not monotonein weights. The next example illustrates this fact. Consider the following setting: C = {p, 1, 2, 3}, N = |S| = 6. All the weightsequal 1. The votes of the voters in S are shown in the following table:Voter in S123456Votep (cid:16) 1 (cid:16) 2 (cid:16) 3p (cid:16) 2 (cid:16) 1 (cid:16) 33 (cid:16) p (cid:16) 1 (cid:16) 23 (cid:16) p (cid:16) 2 (cid:16) 11 (cid:16) 2 (cid:16) 3 (cid:16) p2 (cid:16) 1 (cid:16) 3 (cid:16) pThe pairwise results are given in the next table. In the cell corresponding to the row of candidate g and the columnis preferred to g by b voters (i.e.,, we write “a : b” to indicate that g is preferred to gby a voters, and g(cid:10)(cid:10)(cid:10)of candidate ga = N0(g, g(cid:10)), b = N0(g(cid:10), g)):p2:42:44:214:23:32:424:23:32:4p12332:44:24:2From the above table we calculate that σ0(p) = 1, σ0(1) = σ0(2) = 0, σ0(3) = −1, so p wins the election in this setting.However, if we add another voter (with weight 1), then no matter what his vote would be, p would not win the election:if the additional voter puts 1 above 2, then 1 will win, and otherwise 2 will win.Remark A.1. It is easy to see, however, that whenever there is a manipulation for the coalition with weights W , then thereis also a manipulation for coalition with weights W + {w} + {w}, where w (cid:2) 1 is an integer: the first additional voter makesan arbitrary vote, and the second additional voter reverses the first’s ranking.410M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412Appendix B. Alternative algorithm for CCWM in maximinConsider the following simple algorithm, which we refer to as Algorithm 4. Given the list of weights W, let W ={w 1, . . . , wk} be the maximal (with respect to set inclusion) list of weighted votes such that Wcontains two copies of W ,i.e., W 1 + W 2 ⊆ W, W 1 = W 2 = W .6 Each manipulator in W 1 votes p (cid:16) c1 (cid:16) · · · (cid:16) cm−1, while every manipulator in W 2votes p (cid:16) cm−1 (cid:16) · · · (cid:16) c1. The remaining manipulators all rank p first, and the other candidates arbitrarily. The algorithmreturns true iff this ballot makes p win.(cid:10)(cid:10)(cid:10)We will now easily show that Theorem 3.16 also applies to Algorithm 4.Theorem B.1. In the Maximin rule, let C be the set of candidates with p ∈ C the preferred candidate, and S the set of voters whoalready cast their votes. Let W be the weight list for the set T . Then:1. If there is no ballot making p win the election, then Algorithm 4 will return false.2. If there is a ballot making p win the election, then for the same instance with weight list W(cid:10)s.t. W(cid:10) ⊇ W + W (i.e., W(cid:10)containstwo copies of W ), Algorithm 4 will return true.Proof. Item 1, as always, is obvious since the algorithm is constructive. For item 2, let σ ∗(c) be candidate c’s Maximin scorebased on the votes in S and the manipulator weights W which make p win. Let σ (cid:10)(c) be c’s Maximin score based on thevotes in S and the votes in W + W , according to the algorithm (notice that W ⊆ W 1, W ⊆ W 2). Finally, let σ (c) be c’sscore according to the algorithm, on the weight list W(cid:2)ki=1 wk, as for each(cid:10) ∈ C \ {c, p} and each w i in the multiset W there is exactly one manipulator in W 1 + W 2 with weight w i which ranks ccabove ci=1 wk. Moreover, clearly for any c ∈ C \ {p}, σ (cid:10)(c) (cid:3) σ ∗(c) +. Since σ ∗(p) > σ ∗(c) for any c ∈ C \ {p}, we conclude that. As before, denote W = {w 1, . . . , wk}.First, note that σ (cid:10)(p) (cid:2) σ ∗(p) +(cid:2)k(cid:10)(cid:10)∀c ∈ C \ {p}, σ (cid:10)(p) > σ (cid:10)(c).(28)In order to complete the proof, we note that σ (p) − σ (cid:10)(p) (cid:2) σ (c) − σ (cid:10)(c) for any c ∈ C \ {p}, as all the manipulators withweights W(cid:10) \ (W 1 + W 2) rank p first. Together with the above, we get that σ (p) > σ (c) for all c ∈ C \ {p}. (cid:2)Appendix C. Proof of Theorem 4.6We prove this theorem via Lemmata C.1–C.4. The proof technique is similar to that of Theorem 3.4, but the proof iseasier.First, we define the set Xn = {x ∈ C \ {p} | x was ranked last in stage j for 1 (cid:3) j (cid:3) n}. In addition, define Yn = { y ∈ C \ {p} |σn( y) (cid:2) min(σn( Xn))}. From the definition, Xn ⊆ Yn. Also, by definition:(cid:8)(cid:7)∀g /∈ Yn ∪ {p}, σn(g) < minσn(Yn).(29)We denote by α j(x) the number of points x was awarded in stage j.Lemma C.1. For all y1, y2 ∈ Yn, |σn( y1) − σn( y2)| (cid:3) 1.Proof. Let xσn(x∗ ∈ Xn s.t. σn(x∗) = min(σn( Xn)). Let y ∈ Yn. By definition, σn(x∗) + 1. Suppose for contradiction that σn( y) − σn(x∗) (cid:2) 2. Let 1 (cid:3) j (cid:3) n maximal s.t. α j(x∗) (cid:3) σn( y). We would like to show that σn( y) (cid:3)∗) = 0. Then:(cid:19)∗σ j( y) − σ j(x) =σn( y) −n(cid:4)(cid:20)(cid:19)αk( y)−∗σn(x) −n(cid:4)(cid:20)∗αk(x)k= j+1(cid:21)(cid:22)σn( y) − (n − j)(cid:21)−(cid:2)∗σn(xk= j+1(cid:22)) − (n − j)∗= σn( y) − σn(x) (cid:2) 2.Therefore σ j−1( y) − σ j−1(x∗) (cid:3) σn( y) (cid:3) σn(xσn(x∗) (cid:2) 1, and so σ j−1( y) > σ j−1(x∗), a contradiction to α j(x∗) + 1, and hence for all y1, y2 ∈ Yn, |σn( y1) − σn( y2)| (cid:3) 1. (cid:2)∗) = 0. We showed that for all y ∈ Yn,Lemma C.2. Define q(n) := 1|Yn|(cid:2)y∈Ynimplied by Z T (including votes in S). Then q(cid:2)(cid:10)(n) := 1|Yn|σ (cid:10)n( y) (cid:2) q(n).y∈Ynσn( y). Let Z T be a preference list of voters in T , and σ (cid:10)n(g) be the scores of g ∈ C which are6 To simplify notation we overload W and identify it with the list of manipulators. It is straightforward that this set can be efficiently found. Indeed, foreach weight in the list W, simply check if there is another copy, and if so, place one of them in W 1 and one in W 2.(cid:10)M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412411Proof. The above fact is true since in the algorithm, at every stage j, there is some x ∈ Xn ⊆ Yn such that α j(x) = 0, and soj(g) the number of points candidate gfor every j, the sumgets from voter j in Z T . Then:α j( y) = |Yn| − 1 is minimal. Formally, let us denote by α(cid:10)y∈Yn(cid:2)q(n) = 1|Yn|(cid:3) 1|Yn|= 1|Yn|(cid:15) (cid:4)y∈Yn(cid:15) (cid:4)y∈Yn(cid:4)y∈Yn(cid:16)(cid:8)(cid:7)|Yn| − 1σ0( y) + nσ0( y) +(cid:16)α(cid:10)j( y)n(cid:4)(cid:4)j=1y∈Ynσ (cid:10)n( y) = q(cid:10)(n).(cid:2)Lemma C.3. If σn(p) > max(σn(Yn)) then Algorithm 2 will find the manipulation that makes p win.Proof. By Eq. (29), for all g ∈ C \ {p}, σn(g) (cid:3) max(σn(Yn)), and so if σn(p) > max(σn(Yn)), then for all g ∈ C \ {p}, σn(p) >σn(g), and so the algorithm will find the manipulation. (cid:2)Lemma C.4. If σn(p) (cid:3) max(σn(Yn)) then there exists no manipulation.j(p), it follows that σn(p) (cid:2) σ (cid:10)Proof. Let Z T be a set of preferences of voters in T , and let σ (cid:10)1 (cid:2) α(cid:10)By Lemma C.1, (cid:27)q(n)(cid:28) = max(σn(Yn)). Combining the foregoing steps, we obtain:(cid:8)(cid:7)σn(Yn)n(p). There is at least one g0 ∈ Yn s.t. σ (cid:10)(cid:2) σn(p) (cid:2) σ (cid:10)(cid:10)(n) and α(cid:10)(cid:23)q(n)n(g), q= max(cid:23)qσ (cid:10)n(g0) (cid:2)(cid:2)(cid:24)(cid:24)(cid:10)n(p).We conclude that p does not win under Z T , and hence there is no ballot of votes in T that makes p win the election. (cid:2)(n)j(g) be as in Lemma C.2. As for all j, α j(p) =(cid:10)(n)(cid:28) (cid:2) (cid:27)q(n)(cid:28).(cid:10)(n)(cid:28). By Lemma C.2, (cid:27)qn(g0) (cid:2) (cid:27)qThe proof of Theorem 4.6 is completed.References[1] E. Baharad, Z. Neeman, The asymptotic strategyproofness of scoring and Condorcet consistent rules, Review of Economic Design 4 (2002) 331–340.[2] J. Bartholdi, J. Orlin, Single transferable vote resists strategic voting, Social Choice and Welfare 8 (1991) 341–354.[3] J. Bartholdi, C.A. Tovey, M.A. Trick, The computational difficulty of manipulating an election, Social Choice and Welfare 6 (1989) 227–241.[4] E.H. Clarke, Multipart pricing of public goods, Public Choice 11 (1971) 17–33.[5] V. Conitzer, T. Sandholm, Universal voting protocol tweaks to make manipulation hard, in: Proceedings of the Eighteenth International Joint Conferenceon Artificial Intelligence (IJCAI’03), Acapulco, Mexico, 2003, pp. 781–788.[6] V. Conitzer, T. Sandholm, Nonexistence of voting rules that are usually hard to manipulate, in: Proceedings of the Twenty-First National Conference onArtificial Intelligence (AAAI’06), Boston, 2006, pp. 627–634.[7] V. Conitzer, T. Sandholm, J. Lang, When are elections with few candidates hard to manipulate? Journal of the ACM 54 (3) (2007) 1–33.[8] E. Elkind, H. Lipmaa, Hybrid voting protocols and hardness of manipulation, in: ISAAC, in: Lecture Notes in Computer Science, Springer-Verlag, 2005,pp. 206–215.[9] E. Elkind, H. Lipmaa, Small coalitions cannot manipulate voting, in: FC, in: Lecture Notes in Computer Science, Springer-Verlag, 2005.[10] E. Ephrati, J.S. Rosenschein, A heuristic technique for multiagent planning, Annals of Mathematics and Artificial Intelligence 20 (1997) 13–67.[11] G. Erdélyi, L.A. Hemaspaandra, J. Rothe, H. Spakowski, On approximating optimal weighted lobbying, and frequency of correctness versus average-casepolynomial time, in: Fundamentals of Computation Theory, in: Lecture Notes in Computer Science, vol. 4639, Springer-Verlag, 2007, pp. 300–311.[12] P. Faliszewski, E. Hemaspaandra, L.A. Hemaspaandra, The complexity of bribery in elections, in: Proceedings of the Twenty-First National Conferenceon Artificial Intelligence (AAAI 2006), Boston, 2006.[13] E. Friedgut, G. Kalai, N. Nisan, Elections can be manipulated often, in: Proceedings of the Forty-Ninth Conference on Foundations of Computer Science(FOCS’08), 2008, in press.[14] S. Ghosh, M. Mundhe, K. Hernandez, S. Sen, Voting for movies: The anatomy of a recommender system, in: Proceedings of the Third Annual Conferenceon Autonomous Agents, Seattle, 1999, p. 434–435.[15] A. Gibbard, Manipulation of voting schemes, Econometrica 41 (1973) 587–602.[16] T. Groves, Incentives in teams, Econometrica 41 (1973) 617–631.[17] T. Haynes, S. Sen, N. Arora, R. Nadella, An automated meeting scheduling system that utilizes user preferences, in: Proceedings of the First AnnualConference on Autonomous Agents, Marina del Rey, California, 1997, pp. 308–315.[18] E. Hemaspaandra, L.A. Hemaspaandra, J. Rothe, Anyone but him: The complexity of precluding an alternative, Artificial Intelligence 171 (5–6) (2007)255–285.[19] D.S. Hochbaum, Approximation Algorithms for NP-Hard Problems, PWS Publishing Company, 1997.[20] H. Moulin, On strategy-proofness and single peakedness, Public Choice 35 (1980) 437–455.[21] K. Oflazer, G. Tür, Morphological disambiguation by voting constraints, in: Proceedings of the 8th Conference of the European Chapter of the Associationfor Computational Linguistics (EACL 1997), 1997, pp. 222–229.[22] D. Pennock, E. Horvitz, L. Giles, Social choice theory and recommender systems: Analysis of the axiomatic foundations of collaborative filtering, in:Proceedings of the 17th National Conference on Artificial Intelligence (AAAI 2000), 2000, pp. 729–734.[23] A.D. Procaccia, J.S. Rosenschein, Average-case tractability of manipulation in elections via the fraction of manipulators, in: The Sixth International JointConference on Autonomous Agents and Multiagent Systems (AAMAS 2007), Honolulu, Hawaii, May 2007, pp. 718–720.412M. Zuckerman et al. / Artificial Intelligence 173 (2009) 392–412[24] A.D. Procaccia, J.S. Rosenschein, Junta distributions and the average-case complexity of manipulating elections, Journal of Artificial Intelligence Re-search 28 (2007) 157–181.[25] A.D. Procaccia, J.S. Rosenschein, A. Zohar, Multi-winner elections: Complexity of manipulation, control and winner-determination, in: The TwentiethInternational Joint Conference on Artificial Intelligence (IJCAI 2007), Hyderabad, India, January 2007, pp. 1476–1481.[26] M. Satterthwaite, Strategy-proofness and Arrow’s conditions: Existence and correspondence theorems for voting procedures and social welfare func-tions, Journal of Economic Theory 10 (1975) 187–217.[27] G. Sigletos, G. Paliouras, C. Spyropoulos, M. Hatzopoulos, Combining information extractions systems using voting and stacked generalization, Journalof Machine Learning Research 6 (2006) 1751–1782.[28] A. Slinko, How large should a coalition be to manipulate an election? Mathematical Social Sciences 47 (3) (2004) 289–293.[29] W. Vickrey, Counter speculation, auctions, and competitive sealed tenders, Journal of Finance 16 (1) (1961) 8–37.