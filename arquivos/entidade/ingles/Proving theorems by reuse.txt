Artificial Intelligence 116 (2000) 17–66Proving theorems by reuseChristoph Walther (cid:3), Thomas Kolbe 1Fachbereich Informatik, Technische Universität Darmstadt, Alexanderstr. 10, D-64283 Darmstadt, GermanyReceived 22 October 1997; received in revised form 7 June 1999AbstractWe investigate the improvement of theorem proving by reusing previously computed proofs. Wehave developed and implemented the PLAGIATOR system which proves theorems by mathematicalinduction with the aid of a human advisor: If a base or step formula is submitted to the system, it triesto reuse a proof of a previously verified formula. If successful, labour is saved, because the numberof required user interactions is decreased. Otherwise the human advisor is called for providing a handcrafted proof for such a formula, which subsequently—after some (automated) preparation steps—isstored in the system’s memory, to be in stock for future reasoning problems. Besides the potentialsavings of resources, the performance of the overall system is improved, because necessary lemmatamight be speculated as the result of an attempt to reuse a proof. The success of the approach is basedon our techniques for preparing given proofs as well as by our methods for retrieval and adaptationof reuse candidates which are promising for future proof reuses. We prove the soundness of ourapproach and illustrate its performance with several examples. (cid:211)2000 Elsevier Science B.V. Allrights reserved.Keywords: Deduction and theorem proving; Machine learning; Problem solving and search; Knowledgerepresentation; Analogy; Abstraction; Reuse1. IntroductionThe improvement of problem solvers by reusing previously computed solutions isan active research area of Artificial Intelligence, emerging in the methodologies ofexplanation-based learning (EBL) [22,28,65] and analogical reasoning (AR) [14,39,68]. In EBL a problem’s solution is analyzed, yielding an explanation why the solutionsucceeds. After generalization, the explanation is used for solving (similar) new problems.(cid:3)Corresponding author. Email: chr.walther@informatik.tu-darmstadt.de. This work was supported under grantsno. Wa652/4-1,2,3 by the Deutsche Forschungsgemeinschaft as part of the focus program “Deduktion”.1 Email: kolbe@informatik.tu-darmstadt.de.0004-3702/00/$ – see front matter (cid:211)PII: S 0 0 0 4 - 3 7 0 2 ( 9 9 ) 0 0 0 9 6 - X2000 Elsevier Science B.V. All rights reserved.18C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66In AR a problem’s solution guides the solution of (similar) new problems by suggestingcorresponding inference steps. We present an approach for reusing proofs that combinesideas of EBL and AR as well as ideas from abstraction techniques [36,69].We investigate the reuse of first-order proofs within the domain of automated mathemat-ical induction [8,11,44,46,82] where similar conjectures often have similar proofs: 2 Aninduction theorem prover either proves a conjecture by first-order inferences or otherwiseassociates the conjecture with a finite set of induction formulas whose truth entail the truthof the conjecture (by means of some induction axiom). An induction formula IH ! IC iseither a step formula or a base formula in which case IH equals TRUE. Induction formulasform new conjectures serving as input to the prover, and the original conjecture is provedif eventually only (first-order) provable induction formulas, i.e., valid formulas, remain.Such formulas are proved by modifying the induction conclusion IC using axioms and theinduction hypothesis IH until TRUE is inferred. Despite this regularity the search problemof deciding when and where to apply which axiom such that the induction hypothesis IHbecomes applicable is a main challenge in automated mathematical induction [16,42,82].We call the component of an induction theorem prover which checks the validity of aninduction formula the simplifier. This component either is implemented as a (first-order)theorem prover (tailored for proving induction formulas) or as an interface to the user incase of an interactive (first-order) system. In this paper, we aim to supplement the simplifierwith a learning component in the following way: Once the simplifier has computed a proof,this proof is analyzed and then generalized in a certain sense such that it can be reusedsubsequently. Before the simplifier is asked to prove another statement, now the systemfirst looks for a previously computed proof of a similar statement and tries to reuse it. Ifthe reuse fails, the simplifier has to compute an original proof for the new statement (as itmust without a reuse facility). Otherwise (depending on the simplifier’s implementation)either the search for a proof or user interactions are saved.2. Reusing proofs—An exampleThe success of our approach is based on our techniques for preparing given proofs (byproof analysis and generalization) as well as by our techniques for proof reuse (by retrievaland adaptation methods).We illustrate our proposal by an example. We assume that functions are defined by afinite set EQ of defining equations, and we are interested in verifying that some conjecture’ follows inductively from EQ, i.e., ’ 2 Thind.EQ/ for the inductive theory Thind.EQ/ ofthe equation set EQ. If the inductive validity of such a statement ’ is verified, we mayadd ’ to a set L of lemmata, and subsequent proofs are based on the set AX VD EQ [ L ofaxioms. Now if  2 Thind.AX/ is shown for a new conjecture , then  is an inductiveconsequence of EQ since Thind.AX/ D Thind.EQ/, and  may be also inserted into L, etc.(see, e.g., [82] for a more detailed account on induction theorem proving).2 Throughout this paper induction stands for mathematical induction and should not be confused with inductionin the sense of machine learning. The reuse of previously computed induction schemas or generalizations are notsubject of our proposal.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6619For proving a statement  by induction, i.e., to verify  2 Thind.EQ/, a suitableinduction axiom from Thind.EQ/ is selected by well-known automated methods, cf.,e.g., [82], from which a set of induction formulas I is computed for  such thatI (cid:18) Thind.AX/ entails  2 Thind.EQ/. For instance, let the functions plus, sum and appbe defined by the following equations where 0 and s (respectively empty and add) are theconstructors of the sort number (respectively list): 3(plus-1)(plus-2)(sum-1)(sum-2)(app-1)(app-2)plus.0; y/ (cid:17) y;plus.s.x/; y/ (cid:17) s.plus.x; y//;sum.empty/ (cid:17) 0;sum.add.n; x// (cid:17) plus.n; sum.x//;app.empty; y/ (cid:17) y;app.add.n; x/; y/ (cid:17) add.n; app.x; y//:Now, e.g., the lemma(lem-1) plus.plus.x; y/; z/ (cid:17) plus.x; plus.y; z//can be easily proved by induction and therefore may be used as an axiom like any definingequation in subsequent deductions. We aim to prove conjectures as (lem-1) by reusingpreviously computed proofs of other lemmata. For instance consider the statement’Tx; yU VD plus.sum.x/; sum.y// (cid:17) sum.app.x; y//:We prove the conjecture ’ by induction upon the list-variable x and obtain two inductionformulas, viz. the base formula ’b and the step formula ’s as’b VD ’Tempty; yU;’s VD .8u ’Tx; uU/ ! ’Tadd.n; x/; yU:The following proof of the step formula ’sconclusion ’Tadd.n; x/; yU Dis obtained by modifying the inductionplus.sum.add.n; x//; sum.y// (cid:17) sum.app.add.n; x/; y//IC3 When presenting examples, we usually omit universal quantifiers at the top level of formulas as well as thesort information for variables.20C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66(cid:8)s VD . 8u F .G.x/; G.u// (cid:17) G.H .x; u/// !F .G.D.n; x//; G.y// (cid:17) G.H .D.n; x/; y//98>=><.1/ G.D.n; x// (cid:17) F .n; G.x//.2/ H .D.n; x/; y/ (cid:17) D.n; H .x; y//.3/ F .F .x; y/; z/ (cid:17) F .x; F .y; z//>;Cs VD>:Fig. 1. The proof shell PSs for the proof of ’s .in a backward chaining style, i.e., each statement is implied by the statement in the linebelow, where terms are underlined if they have been changed in the corresponding proofstep:plus.sum.add.n; x//; sum.y// (cid:17) sum.app.add.n; x/; y//plus.plus.n; sum.x//; sum.y// (cid:17) sum.app.add.n; x/; y//plus.plus.n; sum.x//; sum.y// (cid:17) sum.add.n; app.x; y///plus.plus.n; sum.x//; sum.y// (cid:17) plus.n; sum.app.x; y///plus.plus.n; sum.x//; sum.y// (cid:17) plus.n; plus.sum.x/; sum.y///plus.n; plus.sum.x/; sum.y/// (cid:17) plus.n; plus.sum.x/; sum.y///TRUEIC(sum-2)(app-2)(sum-2)IH(lem-1)X (cid:17) XGiven such a proof, it is analyzed to distinguish its relevant features from its irrelevantparts. Relevant features are specific to the proof and are collected in a proof catchbecause “similar” requirements must be satisfied if this proof is to be reused later on. Weconsider features like the positions where equations are applied, induction conclusions andhypotheses, general laws as X (cid:17) X, etc. as irrelevant because they can always be satisfied.So the catch of a proof is a subset of the set of leaves of the corresponding proof tree.Analysis of the above proof yields (sum-2), (app-2), and (lem-1) as the catch. For example,all we have to know about plus for proving ’s is its associativity, but not its semantics orhow plus is computed.Next the conjecture, the induction formula and the catch are generalized 4 for obtaininga proof shell which stores the essentials of the proof and serves as the base for reusing theproof subsequently. Generalization is performed by replacing function symbols by functionvariables denoted by capital letters F; G; H , etc., yielding the schematic conjecture (cid:8) VDF .G.x/; G.y// (cid:17) G.H .x; y// with the corresponding schematic induction formula (cid:8)s aswell as the schematic catch Cs for our example, cf. Fig. 1.Here we use the generalization replacement plus 7! F , sum 7! G, app 7! H , add 7! D,and therefore Eq. (1) of Cs corresponds to (sum-2), Eq. (2) to (app-2), and Eq. (3) to(lem-1).4 Not to be confused with generalization of a formula ’ as a preprocessing for proving ’ by induction.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6621(cid:8)b VD F .G.C/; G.y// (cid:17) G.H .C; y//8><Cb VD>:.4/ G.C/ (cid:17) E.5/ F .E; y/ (cid:17) y.6/ H .C; y/ (cid:17) y9>=>;Fig. 2. The proof shell PSb obtained from the proof of ’b .The base formula ’b of our example is proved byplus.sum.empty/; sum.y// (cid:17) sum.app.empty; y// ’bplus.0; sum.y// (cid:17) sum.app.empty; y//sum.y/ (cid:17) sum.app.empty; y//sum.y/ (cid:17) sum.y/TRUE(sum-1)(plus-1)(app-1)X (cid:17) X:Proof analysis yields the catch cb VD f(sum-1); (plus-1); (app-1)g, and using plus 7! F ,sum 7! G, app 7! H , empty 7! C, 0 7! E, we obtain the proof shell PSb of Fig. 2.If a new statement  shall be proved, a set of induction formulas I is computedfor . Then for proving an induction formula i 2 I by reuse, it is tested whethersome proof shell PS exists which applies for i , i.e., whether i is a (second-order)instance of the schematic induction formula of PS. If the test succeeds, the obtained(second-order) matcher is applied to the schematic catch of PS, and if all formulas of theinstantiated schematic catch can be proved (which may necessitate further proof reuses), i is verified by reuse since the truth of an instantiated schematic catch implies the truthof its (correspondingly) instantiated schematic induction formula.For example, assume that the new conjecture Tx; yU VD times.prod.x/; prod.y// (cid:17) prod.app.x; y//shall be proved, where times and prod are defined by the equations(times-1)(times-2)(prod-1)(prod-2)times.0; y/ (cid:17) 0;times.s.x/; y/ (cid:17) plus.y; times.x; y//;prod.empty/ (cid:17) s.0/;prod.add.n; x// (cid:17) times.n; prod.x//:The induction formulas computed for  are b VD Tempty; yU; s VD .8u Tx; uU/ ! Tadd.n; x/; yU:22C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66is an instance of (cid:8)s with respectto the matcher (cid:25)s VD fF =times;Obviously sG=prod; H =app; D=addg, cf. Fig. 1. Hence we may reuse the proof of ’s by instantiatingthe schematic catch Cs and subsequent verification of the resulting proof obligations:8><(cid:25)s.Cs / D>:.7/ prod.add.n; x// (cid:17) times.n; prod.x//.8/ app.add.n; x/; y/ (cid:17) add.n; app.x; y//.9/times.times.x; y/; z/ (cid:17) times.x; times.y; z//9>=:>;Formulas (7) and (8) are axioms, viz. (prod-2) and (app-2), and therefore are obviouslytrue. So it only remains to prove the associativity of times (9) and, if successful, s isproved. Compared to a direct proof of s we have saved the user interactions respectivelythe search necessary to apply the right axioms in the right place (where the associativityof times must be verified in either case). 5 Note that the reuse speculates conjecture (9)as a lemma which is required for proving conjecture . This lemma now is verified eitherdirectly or by calling the reuse procedure recursively.To complete the proof of , the base formula b D times.prod.empty/; prod.y// (cid:17)prod.app.empty; y// has to be verified, too. As this formula is an instance of (cid:8)bwith respect to the matcher (cid:25)b VD fF =times; G=prod; H =app; C=emptyg, cf. Fig. 2, theschematic catch Cb is instantiated to8><9>=(cid:25)b.Cb/ D>:.10/ prod.empty/ (cid:17) Etimes.E; y/ (cid:17) y.11/.12/ app.empty; y/ (cid:17) y:>;However, this schematic catch is only partially instantiated because the function variableE stemming from the function symbol 0 in the catch cb is not replaced by the matcher (cid:25)b.This is because the function symbol 0 does not occur in b and consequently the functionvariable E does not occur in the schematic base formula (cid:8)b of the proof shell PSb. We callsuch function variables of a proof shell free and we call all other function variables boundfunction variables.A formula ’ with a free function variable F is true iff some function exists such thatthe formula ’0 obtained from ’ by replacing F with this function is true. Thus, e.g., a trueformula, viz. the axiom (prod-1), is obtained from (10) if E is replaced by s.0/. Formallysuch replacements are represented by a second-order substitution like (cid:26)b D fE=s.0/g,and (cid:26)b is called a solution (for the free function variables) if all formulas of the totallyinstantiated catch (cid:26)b.(cid:25)b.Cb// are provable from AX: Here Eq. (12) is the axiom (app-1),equation (cid:26)b.10/ is the axiom (prod-1), and (cid:26)b.11/ simplifies to the speculated lemmaplus.y; 0/ (cid:17) y as the only remaining proof obligation. 6So generally, after finding a proof shell PS D h(cid:8); Ci which applies for a given conjecture via some matcher (cid:25) , i.e., (cid:25).(cid:8)/ D , a solution candidate (cid:26) has to be computed from the5 We choose a very simple example to illustrate the essentials of our proposal. It should be obvious that a proofof ’ and  can be computed by a state-of-the-art theorem prover without any search at all and therefore “reuse”offers no real savings in this case.6 Simplified conjectures are obtained by symbolic evaluation, cf. [82] and Section 7.3. For example, s.t1/ (cid:17)s.t2/ is simplified to t1 (cid:17) t2 and plus.s.t1/; t2/ is simplified to s.plus.t1; t2//.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6623Fig. 3. The phases of the reuse process.partially instantiated catch (cid:25).C/ in a subsequent adaptation step, and the resulting proofobligations from the totally instantiated catch (cid:26).(cid:25).C// are subject to further verifications.If one is also interested in a proof of  to be presented to a user or to be processedsubsequently, it is not sufficient just to instantiate the schematic proof P of (cid:8) (which isobtained by generalizing the proof p of ’) with the computed substitution (cid:28) VD (cid:26) (cid:14) (cid:25)because (cid:28) might destroy the structure of P . Therefore the instantiated proof (cid:28) .P / ispatched (which always succeeds) by removing void respectively inserting additionalinference steps for obtaining a proof p0 of . We do not discuss proof patching in thispaper and refer to [53,57] for details.Fig. 3 illustrates the overall organization of our approach for reusing proofs, and wediscuss the steps of the procedure in the subsequent sections.3. Proof analysis and generalizationIn this section a formal base for proof analysis and generalization is developed. We usea first-order language to represent axioms and the formulas derived from them:Definition 3.1 (Symbols, terms, formulas). LetSIG D[SIGnn2Nbe a signature for function symbols, i.e., each set SIGn holds function symbols of arity n,and let VAR be a set of variable symbols. Then T .SIG; VAR/ denotes the set of all well-formed terms over SIG [ VAR. An equation (over SIG [ VAR) is an expression of the formt1 (cid:17) t2, where t1; t2 2 T .SIG; VAR/, EQ.SIG; VAR/ is the set of all well-formed equations,and F .SIG; VAR/ is the set of all well-formed formulas, build with equations and thepredicate symbols TRUE and FALSE as atomic formulas by using the usual connectivesand quantifiers.24C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66The first-order language over the signature (cid:6) D f0; s; empty; add; plus; times; sum;prod; app; : : :g in which we write axioms and conjectures is called the object language:Definition 3.2 (Object language). Let (cid:6) be a fixed signature for function symbols and letV be a fixed set of variable symbols. Then (cid:6) is called the object signature, T .(cid:6); V/ is theset of all object terms and F .(cid:6); V/ is the object language.3.1. Simple proof analysisSince we want to investigate the principles of proof analysis and reuse, we confineourselves here with unconditional equations as the only conjectures to be proved. Hencethe set AX of axioms (i.e., defining equations and lemmata) from which inferences aredrawn is a set of (universally closed) equations of the object language. However, whenproving a conjecture we have to consider induction formulas of the form IH ! IC, cf.Section 2: Such formulas H ! C are called sequents, where H VD H1 ^ (cid:1) (cid:1) (cid:1) ^ Hn is theset of hypotheses 7 each of which has the form 8u(cid:3) t1 (cid:17) t2, the conclusion C has theform s1 (cid:17) s2, and the free variables x(cid:3) of H ! C are implicitly assumed to be universallyquantified. 8To formalize the simple proof analysis we define a calculus in which we prove (base andstep) formulas. Each rule of this calculus is built from a “conventional” inference rule bystipulating in addition which formula has to be remembered as a “relevant feature” if theparticular inference rule is used in a proof. Hence the rules are applied to expressions ofthe form h’; Ai, where ’ is a sequent and A (cid:18) AX, called the accumulator, holds the catchcollected so far.Definition 3.3 (Simple analysis calculus). The simple analysis calculus consists of thefollowing inference rules operating on pairs h’; Ai of a sequent ’ and an accumulatorA (cid:18) AX, with respect to a set of equational axioms AX. Here (cid:18) is a substitution and p is aposition in the equation C: 9(cid:15) Reflexivityh8x(cid:3) H ! t (cid:17) t; AihTRUE; Ai:(cid:15) AX-replacementh8x(cid:3) H ! C; Aih8x(cid:3) H ! CTp  (cid:18) .r/U; A [ f8u(cid:3) l (cid:17) rgiif 8u(cid:3) l (cid:17) r 2 AX, l =2 u(cid:3), Cjp D (cid:18) .l/, V.r/ (cid:18) V.l/, and dom.(cid:18) / D V.l/.7 We do not distinguish between the formula H and the set H which contains the members of the conjunctionH .8 Instead of operating on sequents H ! C, a calculus can also be defined operating on the equation C withrespect to a set of additional “local” equational hypotheses H , so that only equations are modified by the rules ofthe calculus. However we stick to the use of sequents because this eases the presentation.9 We assume familiarity with the standard notions of equality reasoning, like positions, subterm replacement,etc. [24]. Also symmetry of (cid:17) is implicitly assumed.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6625(cid:15) HYP-replacementh8x(cid:3) H ! C; Aih8x(cid:3) H ! CTp  (cid:18) .r/U; Aiif 8u(cid:3) l (cid:17) r 2 H , l =2 u(cid:3), Cjp D (cid:18) .l/, .V.r/ n V.l// \ u(cid:3) D ;, and dom.(cid:18) / D V.l/ \ u(cid:3).A sequence hh’1; A1i; : : : ; h’n; Anii of pairs of sequents ’i and accumulators Ai (cid:18) AXis a derivation in the analysis calculus from a set AX of equational axioms iff h’iC1; AiC1iresults from applying one of the rules to h’i; Aii for each i 2 f1; : : : ; n (cid:0) 1g. Derivabilityis denoted by h’1; A1i ‘aAXh’n; Ani, where AX may be omitted if appropriate.The requirement dom.(cid:18) / (cid:18) u(cid:3) guarantees that no free variables from 8u(cid:3) l (cid:17) r areinstantiated if the applied equation is a hypothesis, while this requirement is void ifthe applied equation is an axiom (as this is always universally closed). Analogously therequirements .V.r/ n V.l// \ u(cid:3) D ; respectively V.r/ (cid:18) V.l/ ensure that no new variablesare introduced in the sequent. The additional requirement dom.(cid:18) / (cid:18) V.l/ is demanded tokeep the substitution (cid:18) minimal, i.e., no variables are replaced unnecessarily.The replacement rules differ only in updating the accumulator component: If the appliedequation 8u(cid:3) l (cid:17) r is an axiom it has to be recorded in A for obtaining the catch, but ifthe applied equation is a hypothesis it is irrelevant for the learning step and A remainsunchanged.Example 3.4 (Simple proof analysis). Consider the expression hH ! g.f .c; h.y/// (cid:17): : : ; f: : :gi and the equation E VD 8x f .c; x/ (cid:17) h.x/. If E 2 H , then the expressionhH ! g.h.h.y/// (cid:17) : : : ; f: : :gi is obtained by HYP-replacement. But if E 2 AX, then AX-replacement yields hH ! g.h.h.y/// (cid:17) : : : ; fE; : : :gi.The calculus for simple proof analysis yields a finite subset of axioms for the derivationof a sequent and is sound:Theorem 3.5 (Soundness of ‘aequations and let ’ be a sequent in F .(cid:6); V/ such that h’; ;i ‘aAXAX). Let AX; A (cid:26) F .(cid:6); V/ be sets of universally closedhTRUE; Ai. Then(i) A (cid:26) AX, jAj < 1, and h’; ;i ‘aA(ii) A jD ’, and(iii) AX jD ’.hTRUE; Ai,Proof. (i) A (cid:18) AX is obvious from the definition of A by the rules of the calculus andjAj < 1 since each derivation is finite. Finally each derivation from AX also is a derivationfrom A, since no axioms from AX n A are used in the derivation.(ii) ’ can be inferred from A, cf. (i), hence A jD ’ as the reflexivity rule as well as bothreplacement rules are sound.(iii) Follows from (i) and (ii) by the monotonicity of semantical entailment. 2Given a formula  and a set of induction formulas f 0; : : : ; ng for , we try to inferh i ; ;i ‘aAXhTRUE; Aii for each i in our calculus. If successful,AX (cid:19) A0 [ (cid:1) (cid:1) (cid:1) [ An jD f 0; : : : ; ng jDind ;26C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66i.e.,  is proved, and for each i an accumulator Ai is obtained which holds the catch of theproof of i .3.2. GeneralizationGiven a formula ’ and a catch c of a proof of ’ from AX, we reuse this proof forverification of another formula  by a consistent replacement of the function symbols inc [ f’g with other function symbols or terms yielding a set of equations c0 and a formula’0 D  such that AX jD c0 holds. For example, the proof of the base formulaplus.sum.empty/; sum.y// (cid:17) sum.app.empty; y//is reused for proving the base formulatimes.prod.empty/; prod.y// (cid:17) prod.app.empty; y//using the replacements plus 7! times, sum 7! prod and 0 7! s.0/, cf. Section 2. However,in order to ease the presentation and to avoid formal clutter we prefer an indirect way offormulating those function symbol replacements: In a generalization step, the function andvariable symbols of c [ f’g are consistently replaced by symbols from a signature (cid:10) and aset U of variable symbols yielding the schematic catch C and the schematic conjecture (cid:8).These formulas are members of another first-order language, viz. the schematic languageF .(cid:10); U/. If a proof is to be reused for proving some new conjecture , the symbolsfrom (cid:10) [ U are replaced by symbols from (cid:6) [ V to match the schematic conjecture (cid:8)with . For example, for the example above, we use plus 7! F , sum 7! G, empty 7! C,app 7! H , 0 7! E, y 7! u in the generalization step and obtain the schematic conjecture(cid:8) D F .G.C/; G.u// (cid:17) G.H .C; u//, and F 7! times, G 7! prod, C 7! empty, H 7! app,E 7! s.0/, u 7! y then is used in the reuse step, cf. Section 2.Definition 3.6 (Schematic language). Let (cid:10) be a signature for function symbols and letU be a set of variable symbols such that (cid:6) \ (cid:10) D V \ U D ;. Then T .(cid:10); U/ is theset of schematic terms, EQ.(cid:10); U/ is the set of schematic equations, and F .(cid:10); U/ is theschematic language, i.e., the set of all schematic formulas.Function symbols from the schematic signature (cid:10) are denoted by capital lettersF; G; : : : and referred to as function variables subsequently (which indicates the intendedreplacements by function symbols from the object signature (cid:6)). The variables in U arealso called schematic variables. Variables and function symbols are formally replaced byschematic variables and function variables using generalization functions:Definition 3.7 (Generalization functions). A generalization function (cid:13) is an injective andpartial function (cid:13) : (cid:6) [ V ! (cid:10) [ U with finite domain dom.(cid:13) / which maps functionsymbols to function variables of the same arity and maps variables to schematic variables,i.e., f 2 dom.(cid:13) / \ (cid:6)n implies (cid:13) .f / 2 (cid:10)n and x 2 dom.(cid:13) / \ V implies (cid:13) .x/ 2 U .A generalization function (cid:13) is homomorphically extended to object formulas, i.e., (cid:13) .’/ 2F .(cid:10); U/ for each ’ 2 F .(cid:6); V/ such that (cid:6).’/ [ V.’/ (cid:18) dom.(cid:13) /.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6627A generalization function (cid:13) maps an object formula ’ to a schematic formula (cid:13) .’/.The inverse mapping, i.e., the mapping of a schematic formula (cid:8) to an object formula(cid:25).(cid:8)/, also called a total instantiation of (cid:8), is achieved by schematic substitutions andvariable renamings, cf. Section 4. We use the schematic language F .(cid:10); U/ to represent“proof ideas” in form of proof shells:Definition 3.8 (Proof shells). A pair PS D h(cid:8); Ci consisting of a schematic formula(cid:8) 2 F .(cid:10); U/ and a finite set C (cid:18) F .(cid:10); U/ of schematic formulas is a proof shell iffC jD (cid:8).Proof shells constitute the elementary building blocks for proof reuse. They arecomputed by generalizing an object formula ’ and its proof catch c to a proof shell h(cid:8); Ci:Theorem 3.9 (Proof shells from simple analysis). Let A [ f’g (cid:26) F .(cid:6); V/ such thathTRUE; Ai and let (cid:13) be a generalization function such that (cid:13) .A [ f’g/ (cid:26)h’; ;i ‘aAF .(cid:10); U/. Then h(cid:13) .’/; (cid:13) .A/i is a proof shell.Proof. A jD ’ by Theorem 3.5(ii) and consequently (cid:13) .A/ jD (cid:13) .’/ since (cid:13) is a one-to-onesymbol renaming. 2By Theorem 3.9 each proof of an object formula in the simple analysis calculus can begeneralized to a proof shell with the schematic conjecture (cid:13) .’/ and the schematic catch(cid:13) .A/. See Section 2 for examples of proof shells obtained according to Theorem 3.9.4. Instantiating proof shellsProof analysis and generalization define the proof-&-prepare phase of our reuseprocedure, cf. Fig. 3, which transforms a proof into a reusable data structure, viz. a proofshell. In the following we are concerned with the retrieve-&-reuse phase, cf. Fig. 3, whichaims at the selection and instantiation of a proof shell for a given verification task, such thatverifiable proof obligations are obtained. Before we discuss the retrieval and adaptationsteps in Section 7, the concept of admissible and total instantiations of proof shells isintroduced.A conjecture  is verified by reuse, if a proof shell h(cid:8); Ci and a certain substitution(cid:25) exists, such that (cid:25).C [ f(cid:8)g/ (cid:18) F .(cid:6); V/, (cid:25).(cid:8)/ D  and AX jD (cid:25).C/, see Section 2for examples. Such a substitution (cid:25) is computed iteratively as (cid:25)n (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) (cid:25)1 (cid:14) (cid:25)0,i.e., one computes a substitution (cid:25)0 by considering (cid:8), (cid:25)1 by considering (cid:25)0.C/, (cid:25)2by considering (cid:25)1.(cid:25)0.C// etc., until eventually (cid:25)n.: : : (cid:25)1.(cid:25)0.C [ f(cid:8)g// : : :/ (cid:18) F .(cid:6); V/and (cid:25)n.: : : (cid:25)1.(cid:25)0.(cid:8)// : : :/ D . Finally AX jD (cid:25)n.: : : (cid:25)1.(cid:25)0.C// : : :/ is verified, and ifsuccessful AX jD  is proved. We call the formulas in the intermediate steps, i.e., theformulas in (cid:25)0.C/, (cid:25)1.(cid:25)0.C//, etc. mixed formulas since they neither belong to theschematic nor to the object language:Definition 4.1 (Mixed language). T .(cid:6) [ (cid:10); V [ U/ is the set of mixed terms andF .(cid:6) [ (cid:10); V [ U/ is the mixed language, i.e., the set of all mixed formulas.28C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Since F .(cid:6); V/ [ F .(cid:10); U/ (cid:18) F .(cid:6) [ (cid:10); V [ U/, each object formula and eachschematic formula also is a mixed formula. In a mixed (or schematic) expression, schematicvariables are successively replaced with object variables and function variables aresuccessively replaced with (a composition of) function symbols by application of certainsubstitutions such that eventually an expression of the object language is obtained (aftercreating mixed expressions in the intermediate steps).Definition 4.2 (Schematic renamings, schematic substitutions). A schematic variablerenaming is an injective mapping (cid:23) : U ! V. The application of (cid:23) to a mixed term orformula is defined as the homomorphical extension of (cid:23).A schematic substitution (cid:25) : (cid:10) ! T .(cid:6) [ (cid:10); W/ is a partial function with finite domaindom.(cid:25)/ such that (cid:25).F / 2 T .(cid:6) [ (cid:10); WF / for each F 2 dom.(cid:25)/. Here W VDWFis the set of parameter variables, where WF VD fF1; F2; : : : ; Fng for F 2 (cid:10)n and WF \WG D ; for F 6D G. T .(cid:6) [ (cid:10); W/ is called the set of functional terms. The applicationof (cid:25) to a mixed term M 2 T .(cid:6) [ (cid:10); V [ U/ is inductively defined byF 2(cid:10)S(cid:25).z/ VD z(cid:25).F .M1; : : : ; Mn// VD F .(cid:25).M1/; : : : ; (cid:25).Mn//(cid:25).F .M1; : : : ; Mn// VD (cid:27) .(cid:25).F //if z 2 V [ U;if F =2 dom.(cid:25)/;if F 2 dom.(cid:25)/;where (cid:27) VD fF1=(cid:25).M1/; : : : ; Fn=(cid:25).Mn/g replaces the parameter variables Fi in (cid:25).F / 2T .(cid:6) [ (cid:10); WF /. The application of (cid:25) to a mixed formula is defined as the homomorphicalextension of (cid:25) .A schematic substitution (cid:25) is usually written as a finite set of replacement pairs suchthat F =M 2 (cid:25) iff F 2 dom.(cid:25)/ and M D (cid:25).F /. A functional term M 2 T .(cid:6) [ (cid:10); WF /corresponds to the (cid:21)-term (cid:21)F1; : : : ; Fn.M from the (cid:21)-calculus, and a schematic substitutionis also called a pure and closed second-order substitution because variables from V [ Uare neither replaced nor introduced. 10 We usually write wi (instead of Fi ) for the ithformal parameter in a functional term when we define schematic substitutions in examplesand F is obvious from the context. F =f with F 2 (cid:10)n and f 2 (cid:6)n is an abbreviationfor the replacement pair F =f .w1; : : : ; wn/ which retains the structure of terms duringinstantiations, i.e., (cid:25).F .t1; : : : ; tn// D f .(cid:25).t1/; : : : ; (cid:25).tn//. More complex instantiationslike, e.g., (cid:25) D fG=plus.w2; len.w1//; H =F .w1; w1/g for G; H 2 (cid:10)2 can severely changethe structure of terms since whole subterms can be introduced, multiplied, or deleted, as,e.g., (cid:25).G.H .x; sum.app.k; l///; s.y/// D plus.s.y/; len.F .x; x///.Now the reuse of proofs by instantiation of proof shells can be formulated:Theorem 4.3 (Reuse theorem). Let h(cid:8); Ci be a proof shell, (cid:23) be a schematic variablerenaming, and (cid:25) be a schematic substitution such that (cid:23).(cid:25).C [ f(cid:8)g// (cid:18) F .(cid:6); V/. Then(cid:23).(cid:25).C// jD (cid:23).(cid:25).(cid:8)//.Proof. C jD (cid:8) by Definition 3.8, and consequently (cid:23).(cid:25).C// jD (cid:23).(cid:25).(cid:8)// since entailmentis invariant with respect to instantiations, cf. [50]. 210 The notation using parameter variables is borrowed from [37].C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66295. Sortal reasoningThe domain of discourse in theorem proving often is many-sorted, as, e.g., in inductiontheorem proving statements about natural numbers, linear lists, trees, etc. are considered.Sortal reasoning is incorporated into logic by assigning a rangesort from a set S of sortsymbols, say S D fbool; nat; list; : : :g, to variable and function symbols, and also assigninga domainsort from S to each argument position of a function symbol. Sorts are alsoassigned to terms: The sort of a term t is the rangesort of t if t is a variable, and it isthe rangesort of f , if t D f .: : :/. A term t is well-sorted iff for each subterm f .t1; : : : ; tn/of t the sort of each ti coincides with the domainsort of f at position i. A substitution(cid:18) D fx1=t1; : : : ; xn=tng is well-sorted iff the sort of each ti coincides with the sort of xi andeach ti is well-sorted. Now in many-sorted logic one demands that(1) only well-sorted terms are used,(2) t1 and t2 have the same sort for each equation t1 (cid:17) t2 in a formula, and(3) only well-sorted substitutions are used in a deduction.Different from non-flat sort hierarchies, where an order relation is imposed on S thusrepresenting inclusion of sets, cf. [79], a flat many-sorted framework influences deductiononly when matchers (or unifiers) are computed for a variable replacement: A term t ina formula may be replaced by a term (cid:18) .r/ using the equation l (cid:17) r and a matcher (cid:18)of l and t, only if (cid:18) is well-sorted. This is guaranteed if l is not a variable, because allterms are well-sorted. However, the sorts of l and t must be explicitly compared, if l is avariable. Otherwise, (cid:18) may be ill-sorted and consequently ill-sorted terms may be formedin a deduction, as, e.g., a term plus.a; b/ may be replaced by rev.rev.plus.a; b/// usingthe equation x (cid:17) rev.rev.x// and an ill-sorted matcher which replaces a list-variable x bya nat-term plus.: : :/.Now in order to guarantee a sound reuse in a many-sorted logic, information about thesorts of “locally" quantified variables l in equations l (cid:17) r used from left to right in aproof must be memorized in the proof catch and properly considered on subsequent reuseattempts, such that provability of a statement for which the reuse succeeds is guaranteedin fact. Since this involves an awkward formal machinery for the rare case of pure variablereplacements in a proof, cf. [52], we do not allow such variable replacements in the analysiscalculus. Consequently, a proof catch is independent of sortal information.However, when instantiating a proof shell, it has to be guaranteed that the intermediatemixed formulas have at least one well-sorted ground instance. Consider, for instance, apartially instantiated catch containing the mixed formulas(1) F .G.k// (cid:17) s.sum.k//, and(2) F .n/ (cid:17) H .n/,where k is a list-variable and n is a variable of sort nat. The substitution (cid:25)1 VDfF =s; G=sumg solves (1) and yields s.n/ (cid:17) H .n/ as a further instantiation of the remainingcatch formula (2). The substitution (cid:25)2 VD fF =s.sum.w1//; G=w1g also solves (1) and heres.sum.n// (cid:17) H .n/ results from (2). But s.sum.n// is ill-sorted (as n is a nat-variable butsum : list ! nat), hence (cid:25)2 had not been considered as a solution substitution for (1). Sincethe selection of a wrong substitution may be recognized only after several instantiationsteps, machine resources are wasted on reuse attempts and backtracking mechanisms arerequired.30C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66When forming a totally instantiated catch, backtracking can be avoided if sorts areassigned also to the schematic variable symbols and the function variables. Then we call aset (cid:5) of schematic variable renamings and substitutions admissible for a set C of mixedformulas, if a sort assignment exists such that each variable renaming and substitution in(cid:5) and each formula in C is well-sorted.For instance, F : nat ! nat, G : list ! nat and H : nat ! nat is a sort assignment suchthat f.1/; .2/g as well as (cid:25)1 are well-sorted, and therefore f(cid:25)1g is admissible for f.1/; .2/g.But f(cid:25)2g is not admissible for f.1/; .2/g, since the well-sortedness of f.1/; .2/g demandsF : nat ! nat, whereas the well-sortedness of (cid:25)2 demands F : list ! nat.Admissibility of a set (cid:5) of substitutions for a set C of mixed formulas is easily tested bycomputing an equational system E.(cid:5); C/ over sort symbols from (cid:5) and C and subsequentverification, that no different sorts, as, e.g., nat and list above, are identified by E.(cid:5); C/,see [84] for formal details. For the reuse of proofs in a sorted logic, the Reuse Theorem 4.3is reformulated as:Corollary 5.1 (Reuse theorem for sorted logic). Let h(cid:8); Ci be a proof shell, (cid:23) be aschematic variable renaming, and (cid:25) be a schematic substitution such that(1) f(cid:25); (cid:23)g is admissible for C [ f(cid:8)g, and(2) (cid:23).(cid:25).C [ f(cid:8)g// (cid:18) F .(cid:6); V/.Then (cid:23).(cid:25).C// jDS (cid:23).(cid:25).(cid:8)//, where jDS denotes semantical entailment of sorted logic,cf. [29].Proof. Follows from Theorem 4.3, cf. [50]. 26. Refined proof analysisCorollary 5.1 provides the logical base for our proposal of reusing proofs: For a givenconjecture , some proof shell h(cid:8); Ci, some renaming (cid:23) and some schematic substitution(cid:25) must be found which meet requirements (1) and (2) of the Reuse Theorem and alsosatisfy  D (cid:23).(cid:25).(cid:8)//. Then AX jD (cid:23).(cid:25). 0// is verified (either directly or “by reuse” again)for each  0 2 C, and if successful AX jD  is proved.Since our proof shells are obtained from (analyzed) proofs, cf. Theorem 3.9, the successof proof reuse directly depends on the generality of what has been learned from a givenproof:Example 6.1 (Limitations of simple analysis). Let AX D ff.a/ (cid:17) a; g.a/ (cid:17) b; g.b/ (cid:17) agand consider the conjecture ’ D f.f.a// (cid:17) a. Thenhf.f.a// (cid:17) a; ;ihf.a/ (cid:17) a; ff.a/ (cid:17) agiha (cid:17) a; ff.a/ (cid:17) agihTRUE; ff.a/ (cid:17) agiAXAX‘a‘a‘aAXand the proof shell PS D hF .F .A// (cid:17) A; fF .A/ (cid:17) Agi is obtained from this proof. But PSdoes not apply for the conjecture 1 D f.g.b// (cid:17) a since the function variable F cannotC. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6631be replaced both by f and g (and the function variable A cannot be replaced both by b anda), and the reuse fails for 1. PS applies for the conjecture 2 D g.g.a// (cid:17) a, hence theinstantiated catch is computed as fg.a/ (cid:17) ag. But AX 6jD g.a/ (cid:17) a, hence the reuse fails alsofor 2.One possible reason for the failed reuses is that each conjecture requires an originalproof which differs in its structure from the proof of ’. But it may also be the case thatwhat has been learned is simply not enough, so the reuse fails only for this reason. Thelatter is true for both conjectures in the example and we improve our technique so that thereuse eventually is successful.The key idea for the improvement is to distinguish different occurrences of functionsymbols in the conjecture and the catch of the conjecture’s proof, which may be replacedby (a composition of) different function symbols without spoiling the soundness of theproof. This yields a more general proof shell, since different occurrences of a functionsymbol at the object level are generalized to different function variables at the schematiclevel when the proof shell is formed.Example 6.2 (Example 6.1 continued). We may recover from the reuse failures since both’ and AX have several occurrences of the function symbols f and a which can be separatedinto different function symbols without spoiling the soundness of the derivation: We mayuse AX(cid:3) D ff0g to prove ’(cid:3) D f.f0.a// (cid:17) a0.a/ (cid:17) a00/ (cid:17) a00; f.a0 by0; ;i0.a// (cid:17) ahf.f0000; ff/ (cid:17) ahf.a0.a/ (cid:17) a0; ff0 (cid:17) aha000.a/ (cid:17) ahTRUE; ff.a/ (cid:17) a00gi00; f.a00; f.a00/ (cid:17) a0gi0gi/ (cid:17) a‘a‘a‘aAXAXAXand obtain the proof shell PS(cid:3) D hF .F 0.A// (cid:17) A0; fF 0.A/ (cid:17) A00; F .A00/ (cid:17) A0gi from thisderivation.PS(cid:3) applies for the conjecture 1 via the matcher (cid:25)1 D fF =f; F 0=g; A=b; A0=ag,hence the partially instantiated catch C1 is computed as fg.b/ (cid:17) A00; f.A00/ (cid:17) ag. Since(cid:26)1 D fA00=ag is a solution for the free function variable in C1, i.e., (cid:26)1.(cid:25)1.C1// D fg.b/ (cid:17)a; f.a/ (cid:17) ag (cid:18) AX, 1 is proved by reuse based on the proof shell PS(cid:3).PS(cid:3) applies also for 2 via the matcher (cid:25)2 D fF =g; F 0=g; A=a; A0=ag, hence thepartially instantiated catch C2 is computed as fg.a/ (cid:17) A00; g.A00/ (cid:17) ag. Since (cid:26)2 D fA00=bgis a solution for the free function variable in C2, i.e., (cid:26)2.(cid:25)2.C2// D fg.a/ (cid:17) b; g.b/ (cid:17) ag (cid:18)AX, also 2 is proved by reuse based on the proof shell PS(cid:3).Since the original derivation of ’ from AX does not expose the possible separation ofthe occurrences of f and a, all occurrences of f are generalized to one function variable Fand all occurrences of a are generalized to one function variable A in the proof shell PS,and therefore the reuse fails for 1 and 2. So a remedy to the problem is to recognizedifferent occurrences of function symbols in a derivation which may be generalized todifferent function variables when the proof shell is constructed.32C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66To this effect, we distinguish in a first step the different occurrences of each functionsymbol in the conjecture by supplying the function symbols with different superscriptsfrom N yielding, e.g., ’0 D f1.f2.a1// (cid:17) a2. We call function symbols with superscriptsindexed function symbols and we call f the root and i the index of an indexed functionsymbol f i . We also supply the function symbols of the axioms with unique indices, wherewe assume infinitely many differently indexed copies of each axiom in the indexed axiomset, and, e.g., AX0 D ff3.a3/ (cid:17) a4; f4.a5/ (cid:17) a6; f5.a7/ (cid:17) a8; f6.a9/ (cid:17) a10; : : :g is obtainedsuch that no indexed function symbol has more than one occurrence in the indexed axiomset and the indexed conjecture.In the next step, the function symbols in the derivation are supplied with correspondingindices, where we demand that no indexed axiom is applied more than once in a derivation.This does not restrict derivability, since we have infinitely many indexed copies at ourdisposal, and we obtainhf1.f2.a1// (cid:17) a2; ;i‘ hf1.a4/ (cid:17) a2; ff3.a3/ (cid:17) a4gi‘ ha6 (cid:17) a2; ff3.a3/ (cid:17) a4; f4.a5/ (cid:17) a6gi‘ hTRUE; ff3.a3/ (cid:17) a4; f4.a5/ (cid:17) a6gi:Now we test whether the indexed derivation is a sound derivation from AX0 and we identifyindexed function symbols with common root and different indices (only) if necessary. Thismust always succeed, because the original derivation is obtained if all indexed functionsymbol with common root are identified. Here the identifications f2 D f3, a1 D a3 , f1 D f4,a4 D a5, and a2 D a6 are required for obtaining a sound derivation from AX0. Note thatall indexed function symbols in the derivation are identified, except f1, f2 and a1, a2, a4because an identification of these symbols is not necessary for a sound derivation.Finally, the recognized identifications are propagated into the conjecture ’0, the setof axioms AX0, and the derivation, yielding, e.g., ’00 D f1.f2.a1// (cid:17) a2, AX00 D ff2.a1/ (cid:17)a4; f1.a4/ (cid:17) a2; : : :g, and the derivationhf1.f2.a1// (cid:17) a2; ;i‘aAX00 hf1.a4/ (cid:17) a2; ff2.a1/ (cid:17) a4gi‘aAX00 ha2 (cid:17) a2; ff2.a1/ (cid:17) a4; f1.a4/ (cid:17) a2giAX00 hTRUE; ff2.a1/ (cid:17) a4; f1.a4/ (cid:17) a2gi‘ain the simple analysis calculus. From this derivation the proof shell PS(cid:3) from Example 6.2is obtained by generalizing ’00 and the proof catch ff2.a1/ (cid:17) a4; f1.a4/ (cid:17) a2g.Subsequently we shall develop the calculus of refined proof analysis yielding proofshells with significantly increased reusability, because more general schematic conjecturesand catches are obtained as compared to the simple analysis approach. Proof shells basedon the results of refined analysis apply more often and yield weaker prove obligations asthe above examples illustrate.The calculus of refined proof analysis emerges from the simple analysis calculus by thederivation ofC. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6633(1) an indexed conjecture from a set of indexed axioms,(2) an accumulator holding the indexed axioms used in the derivation, and(3) a collision set for the bookkeeping of the identifications of the indexed symbolsrequired for a sound derivation.Definition 6.3 (Indexed function symbols, indexed language).(cid:6) NnVD ff i j f 2 (cid:6)n; i 2 Ngis the set of indexed function symbols for the object signature (cid:6). T .(cid:6) N; V/ is called theset of indexed terms and F .(cid:6) N; V/ is the set of indexed formulas.index : T .(cid:6); V/ [ F .(cid:6); V/ ! T .(cid:6) N; V/ [ F .(cid:6) N; V/is a mapping which supplies all occurrences of all function symbols in a term respectivelyformula with fresh indices. We demand index to yield indices in ascending order startingwith 1.unindex : T .(cid:6) N; V/ [ F .(cid:6) N; V/ ! T .(cid:6); V/ [ F .(cid:6); V/is the inverse mapping which removes the indices of all indexed function symbols in anindexed term respectively formula.Strictly speaking, index is an operation with a side effect since we demand that indicesobtained by index have been “never used before”. Thus, e.g., index.g.f.g.x/; y/// Dg1.f1.g2.x/; y// may result when indexing is started, but later index.g.f.g.x/; y/// Dg3.f2.g4.x/; y// is obtained for the same input term. Consequently unindex.index.t// D tfor each t 2 T .(cid:6); V/, but index.unindex.t// 6D t for each t 2 T .(cid:6) N; V/.Definition 6.4 (Index collision set, equivalence (cid:24)K ). A pair hf i ; f j i 2 (cid:6) N (cid:2) (cid:6) N is calledan index collision and a set K (cid:18) fhf i; f j i j f 2 (cid:6); i; j 2 Ng (cid:18) (cid:6) N (cid:2)(cid:6) N is called an indexcollision set. We let (cid:24)K denote the reflexive, transitive and symmetrical closure of K, i.e.,(cid:24)K is an equivalence relation on (cid:6) N. df ieVD f k is the K-representative of f i 2 (cid:6) N iffKk VD minfj j f i (cid:24)K f j g.Index collision sets are used to represent identifications for the different occurrencesof function symbols, where “earlier” occurrences have smaller indices. Thus the K-representative of an indexed function symbol f i is the first occurrence of f which isidentified with f i to satisfy the identifications represented by K.Definition 6.5 (Congruence (cid:25)K , minimal collision set DK ). For an index collision set K,the equivalence relation (cid:24)K on (cid:6) N is extended to a congruence relation (cid:25)K on T .(cid:6) N; V/,i.e., an equivalence relation such that x (cid:25)K y iff x D y for x; y 2 V andf i .t1; : : : ; tn/ (cid:25)K f j .s1; : : : ; sn/iff f i (cid:24)K f j and t1 (cid:25)K s1; : : : ; tn (cid:25)K sn. We write t DK s iff t (cid:25)K s for a minimal indexcollision set K, i.e., t (cid:25)L s implies (cid:24)K (cid:18)(cid:24)L for each collision set L.34C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Note that only those indexed terms are K-congruent from which identical terms areobtained if the indices are removed. For instance f1.f2.x// (cid:25)K f3.f2.x// for K VD fhf1; f3ig,but f1.f2.x// 6(cid:25)K 0 f3.g1.x// for each index collision set K 0.Matching is extended to indexed terms by ignoring the indices when the clash test isperformed.Definition 6.6 (Matching of indexed terms). A substitution (cid:18) is an indexed matcher oft; s 2 T .(cid:6) N; V/ iff (cid:18) .t/ (cid:25)K s for some collision set K. (cid:18) is a minimal indexed matcher oft; s 2 T .(cid:6) N; V/ iff there is a collision set K such that (cid:18) .t/ (cid:25)K s and (cid:24)K (cid:18)(cid:24)L for eachindexed matcher (cid:21) with (cid:21).t/ (cid:25)L s for a collision set L.This means that, e.g., fx=a1g is an indexed matcher of f 1.x/ and f 2.a1/ whereas f 1.x/and g1.a1/ are non-matchable indexed terms. Further, e.g., fx=a1g or fx=a2g are minimalindexed matchers of h1.x; x/ and h2.a1; a2/, while fx=a3g is a non-minimal indexedmatcher. Minimal indexed matchers never introduce indices which do not already occurin the matched terms.The simple analysis calculus from Section 3.1 now is modified to incorporate thebookkeeping of the indices. The modified calculus operates on triples h8x(cid:3) H ! C; A; Ki,where all non-variable terms in H , C and A are indexed and an additional component, viz.the index collision set K, keeps track of the function symbols from (cid:6) N which have beenidentified in a proof.The conjecture-sequent ’ D 8x(cid:3) H ! C whose proof is going to be analyzed must beindexed before the derivation is performed, i.e., we start with the sequent index.’/. Theaxioms of AX are freshly indexed before application, such that no indexed function symbolhas more than one occurrence in the indexed accumulator and the indexed conjecture.Definition 6.7 (Refined analysis calculus). The refined analysis calculus consists of thefollowing inference rules operating on triples h’; A; Ki of an indexed sequent ’, anindexed accumulator A with unindex.A/ (cid:18) AX, and an index collision set K (cid:18) (cid:6) N (cid:2) (cid:6) N,where AX is a set of axioms. (cid:18) is a minimal indexed matcher and p is a position in C:(cid:15) Reflexivityh8x(cid:3) H ! s (cid:17) t; A; KihTRUE; A; K [ K 0iif s DK 0 t:(cid:15) AX-replacementh8x(cid:3) H ! C; A; Kih8x(cid:3)H ! CTp  (cid:18) .r 0/U; A [ f8u(cid:3) l0 (cid:17) r 0g; K [ K 0iif 8u(cid:3) l (cid:17) r 2 AX; l =2 u(cid:3);V.l/, and dom.(cid:18) / D V.l/.(cid:15) HYP-replacementl0 VD index.l/; r 0 VD index.r/; Cjp DK 0 (cid:18) .l0/; V.r/ (cid:18)h8x(cid:3) H ! C; A; Kih8x(cid:3) H ! CTp  (cid:18) .r/U; A; K [ K 0iif 8u(cid:3) l (cid:17) r 2 H; l =2 u(cid:3); Cjp DK 0 (cid:18) .l/; .V.r/ n V.l// \ u(cid:3) D ;, and dom.(cid:18) / DV.l/ \ u(cid:3).C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6635A sequence hh’1; A1; K1i; : : : ; h’n; An; Knii of triples of indexed sequents ’i, indexedaccumulators Ai with unindex.Ai/ (cid:18) AX, and index collision sets Ki (cid:18) (cid:6) N (cid:2) (cid:6) N is aderivation in the refined analysis calculus from a set AX of equational axioms iff for eachi 2 f1; : : : ; n(cid:0)1g, h’iC1; AiC1; KiC1i results from applying one of the rules to h’i ; Ai; Kii.Derivability is denoted by h’1; A1; K1i ‘akh’n; An; Kni, where AX may be omitted ifAXappropriate.In the refined analysis calculus both replacement rules not only differ in updating theaccumulator component (as they already do in the simple analysis approach), but also differin the treatment of the function indices: AX-replacement generates a freshly indexed variantof an axiom before application whereas HYP-replacement applies an already indexedhypothesis without index modifications. The indexed function symbols which have to beidentified in a replacement step are recorded by both rules in the collision component.Example 6.8 (Refined proof analysis). We resume Example 3.4. Consider the equationE VD T8x f.c; x/ (cid:17) h.x/U and the indexed expression(cid:10)(cid:11)H ! g2.f2.c2; h2.y/// (cid:17) : : : ; f: : :g; f: : :g:If (an indexed version of) E is in H , e.g., T8x f1.c1; x/ (cid:17) h1.x/U 2 H , then(cid:10)H ! g2.h1.h2.y/// (cid:17) : : : ; f: : :g; fhf2; f1i; hc2; c1i; : : :g(cid:11)is obtained by HYP-replacement. But if E 2 AX, then index.E/ VD T8x f3.c3; x/ (cid:17) h3.x/Uis used by AX-replacement yielding the indexed expression(cid:10)H ! g2.h3.h2.y/// (cid:17) : : : ; f8x f3.c3; x/ (cid:17) h3.x/; : : :g; fhf2; f3i; hc2; c3i; : : :g(cid:11):Note that we demand “: : : DK 0 : : :” and use minimal indexed matchers in the definitionof the inference rules. This entails that only function symbols are identified which mustbe identified and therefore guarantees that a most general (schematic) catch will beobtained subsequently. Otherwise, e.g., the pair hh1; h2i could also be inserted into theindex collision sets of Example 6.8 yielding an identification which is not required by theinference steps.For proving a formula ’, we try to establish hindex.’/; ;; ;i ‘akAXhTRUE; A; Ki in therefined calculus. The collision set K then contains pairs hf i ; f j i of indexed functionsymbols which have been identified in the proof. This information must be propagatedinto the accumulator A and into the conjecture index.’/ when building the proof catch,because the proof of index.’/ is based on the assumption that f i and f j denote identicalfunctions: Either(i) f i and f j are identified syntactically by replacing f i with f j (or vice versa) in theequations of A and index.’/, or(ii) f i and f j are identified semantically by insertion of the identification axiomT8x1; : : : ; xn f i.x1; : : : ; xn/ (cid:17) f j .x1; : : : ; xn/U into A.36C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66The kind of identification influences the generality of the proof shell and the effort whichmust be spent for reusing the proof:Example 6.9 (Syntactical and semantical identification). Let AX D fa (cid:17) b; a (cid:17) c; f.x; y/(cid:17) g.x; y/; : : :g and consider the conjecture ’ D f.a; a/ (cid:17) f.b; c/. Thenhf1.a1; a2/ (cid:17) f2.b1; c1/; ;; ;ihf1.b2; a2/ (cid:17) f2.b1; c1/; fa3 (cid:17) b2g; fa1 D a3gihf1.b2; c2/ (cid:17) f2.b1; c1/; fa3 (cid:17) b2; a4 (cid:17) c2g; fa1 D a3; a2 D a4gihTRUE; fa3 (cid:17) b2; a4 (cid:17) c2g; fa1 D a3; a2 D a4; f1 D f2; b1 D b2; c1 D c2gi‘akAX‘akAX‘akAXand syntactical identification yields the proof catch fa1 (cid:17) b1; a2 (cid:17) c1g for the conjecturef1.a1; a2/ (cid:17) f1.b1; c1/. After generalization, the proof shellPSsyn D hF 1.A1; A2/ (cid:17) F 1.B1; C1/; fA1 (cid:17) B1; A2 (cid:17) C1giis obtained from the proof. 11 Semantical identification yields the proof catch fa3 (cid:17)b2; a4 (cid:17) c2; a1 (cid:17) a3; a2 (cid:17) a4; f1.x; y/ (cid:17) f2.x; y/; b1 (cid:17) b2; c1 (cid:17) c2g for the conjecturef1.a1; a2/ (cid:17) f2.b1; c1/ which is generalized to the proof shell(cid:10)PSsem DF 1.A1; A2/ (cid:17) F 2.B1; C1/;fA1 (cid:17) B1; A2 (cid:17) C1; A3 (cid:17) B2; A4 (cid:17) C2; A1 (cid:17) A3;A2 (cid:17) A4; F 1.u; v/ (cid:17) F 2.u; v/; B1 (cid:17) B2; C1 (cid:17) C2g(cid:11):Obviously PSsem is more general than PSsyn in the sense that PSsem applies for aconjecture  and provable proof obligations are obtained from the catch of PSsemwhenever PSsyn applies for  such that provable proof obligations are obtained from thecatch of PSsyn, but not vice versa.For instance, PSsem applies for the conjecture  D f.a; a/ (cid:17) g.b; c/ via the matcher (cid:25) DfF 1=f; A1=a; A2=a; F 2=g; B1=b; C1=cg, hence the partially instantiated catch (cid:25).Csem/ iscomputed as fa (cid:17) b; a (cid:17) c; A3 (cid:17) B2; A4 (cid:17) C2; a (cid:17) A3; a (cid:17) A4; f.x; y/ (cid:17) g.x; y/; b (cid:17)B2; c (cid:17) C2g. Since (cid:26) D fA3=a; A4=a; B2=b; C2=cg is a solution for the free functionvariables in Csem, i.e.,AX jD (cid:26).(cid:25).Csem// D fa (cid:17) b; a (cid:17) c; a (cid:17) a; f.x; y/ (cid:17) g.x; y/; b (cid:17) b; c (cid:17) cg; is proved by reuse based on the proof shell PSsem. However, PSsyn does not apply for because the function variable F 1 cannot be replaced both by f and g, and consequentlya reuse based on PSsyn fails for .Example 6.9 illustrates that semantical identification yields more general proof shells,however for the price of larger schematic catches with more function variables as11 We use indexed function variables in examples only for illustrational purposes and the sake of the presentation.This means that, e.g., A1 and A2 are different function variables sharing no common property as, e.g., A1 andB1 have no property in common.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6637compared to syntactical identification. Hence semantical identification increases the effortof reuse as more bound function variables must be matched, more free function variablesmust be solved and more proof obligation must be verified. Correspondingly, syntacticalidentification minimizes the reuse effort, however for the price of a restricted applicabilityof the proof shell. Consequently, some compromise must be made between both extremes,and we propose a quite simple criterion to decide between both forms of identificationwhich combines the advantages of both alternatives and has proved sufficient: Boundfunction symbols, i.e., function symbols stemming from the indexed conjecture, areidentified semantically so that the schematic conjecture of a proof shell is as generalas possible thus increasing applicability. Free function symbols, i.e., function symbolsoccurring only in the proof catch, are identified syntactically so that the number of freefunction variables and the size of the schematic catch is minimized thus decreasing thereuse effort.Definition 6.10 (Identification heuristic). Let h’; ;; ;i ‘akAXcollision set K defines a mappinghTRUE; A; Ki. Then theidK : 2F .(cid:6) N;V / ! 2F .(cid:6) N;V /such that each formula  0 2 idK .A/ is obtained from  2 A by replacing each free functionsymbol f i in  with its K-representative df ieK , cf. Definition 6.4. We call idK .A/ thesyntactic catch of the proof of ’.The collision set K also defines a set of indexed formulas IDK (cid:18) F .(cid:6) N; V/ such thatK .x1; : : : ; xn/ (cid:17) f i.x1; : : : ; xn/ 2 IDK iff f i is a bound function symbolK . IDK is called the semantic catch of the proof of ’ and each member of8x1; : : : ; xn df i ewith f i 6D df ieIDK is an identification axiom.Since we assign indices in ascending order starting with the conjecture, a free functionsymbol f i is always replaced by a bound function symbol if the (cid:24)K -equivalence classof f i contains one bound function symbol at least. Thus the function symbols from theconjecture are propagated into the proof catch as far as possible (and necessary). The unionof the syntactic and the semantic catch now form the catch of the indexed conjecture’sproof, which then is generalized to a proof shell:Lemma 6.11 (Refined versus simple analysis). Let AX (cid:26) F .(cid:6); V/ and A (cid:26) F .(cid:6) N; V/ besets of universally closed equations, let K (cid:26) (cid:6) N (cid:2) (cid:6) N be a collision set and let ’ be asequent in F .(cid:6) N; V/ such that h’; ;; ;i ‘akAXhTRUE; A; Ki. Thenh’; ;i ‘aidK .A/[IDKhTRUE; idK .A/ [ IDK i:Proof. Transforming the derivation from the refined calculus into a derivation in the simpleanalysis calculus succeeds because “index disagreements” can be removed: Each step ofthe derivation ‘akwiththe AX-replacement-rule (where identification axioms from IDK are used for adaptingcolliding indices of bound function symbols if necessary) followed by one step in ‘awith the same rule which has been used in ‘akAX is transformed into a sequence of replacement steps in ‘aidK .A/IDKAX. 238C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66AX). Let AX (cid:26) F .(cid:6); V/ and A (cid:26) F .(cid:6) N; V/ be sets ofTheorem 6.12 (Soundness of ‘akuniversally closed equations, let K (cid:26) (cid:6) N (cid:2) (cid:6) N be a collision set and let ’ be a sequentin F .(cid:6) N; V/ such that h’; ;; ;i ‘akAXhTRUE; A; Ki. Then(i) unindex.A/ (cid:26) AX; jAj < 1, and h’; ;; ;i ‘ak(ii) idK .A/ [ IDK jD ’, and(iii) AX jD unindex.’/.unindex.A/hTRUE; A; Ki.Proof. (i) unindex.A/ (cid:26) AX is obvious from the definition of A by the rules of thecalculus and jAj < 1 since each derivation is finite. Finally each derivation from AXalso is a derivation from unindex.A/ since no axioms from AXnunindex.A/ are used inthe derivation.(ii) We findh’; ;i ‘ahTRUE; idK .A/ [ IDK iidK .A/[IDKby Lemma 6.11 and then the statement follows by Theorem 3.5(ii).(iii) Since idK .A/ [ IDK jD ’ by (ii), unindex.idK .A// [ unindex.IDK / jD unindex.’/.With unindex.idK .A// D unindex.A/ and jD unindex.IDK /, we obtain unindex.A/ jDunindex.’/. With unindex.A/ (cid:26) AX, cf. (i), the statement follows by the monotonicityof semantical entailment. 2Corollary 6.13 (Proof shells from refined analysis). Let A [ f’g (cid:26) F .(cid:6) N; V/, such thath’; ;; ;i ‘akunindex.A/hTRUE; A; Kiand let (cid:13) be a generalization function such that (cid:13) .A [ f’g/ (cid:26) F .(cid:10); U/. Then h(cid:13) .’/;(cid:13) .idK .A/ [ IDK /i is a proof shell.Proof. h’; ;i ‘afollows by Theorem 3.9. 2idK .A/[IDKhTRUE; idK .A/ [ IDK i by Lemma 6.11 and then the statementExample 6.14 (Identification heuristic). We resume Example 6.9 and apply the identifi-cation heuristic to the conjecture f1.a1; a2/ (cid:17) f2.b1; c1/, the catch A D fa3 (cid:17) b2; a4 (cid:17) c2gand the collision set K D fa1 D a3; a2 D a4; f1 D f2; b1 D b2; c1 D c2g: Since all functionsymbols in A are free, syntactical identification yields the syntactic catch idK .A/ D fa1 (cid:17)b1; a2 (cid:17) c1g, and the semantic catch IDK is computed as ff1.x; y/ (cid:17) f2.x; y/g, because f1and f2 are bound and K-equivalent. From the union of both catches, the proof shellPSheu D hF 1.A1; A2/ (cid:17) F 2.B1; C1/;fA1 (cid:17) B1; A2 (cid:17) C1; F 1.u; v/ (cid:17) F 2.u; v/giis obtained, which is more general than PSsyn (but less general than PSsem).For instance, PSheu applies for  D f.a; a/ (cid:17) g.b; c/ via the matcher (cid:25) D fF 1=f; A1=a;A2=a; F 2=g; B1=b; C1=cg, hence the totally instantiated catch (cid:25).Cheu/ is computed asfa (cid:17) b; a (cid:17) c; f.x; y/ (cid:17) g.x; y/g (cid:18) AX, and  is proved by reuse based on PSheu.Obviously, the proof shell PSheu applies more often than the proof shell PSsyn, e.g., for , but requires no effort for solving free variables and yields less proof obligations thanthe proof shell PSsem.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6639Definition 6.10 formulates a heuristic decision between syntactical and semanticalidentification which is independent of a reasoning domain, and modifications of thisheuristic may be useful for a particular domain under consideration. Here, for instance,theorem proving by induction is considered and indexing of a step formula like ’s , e.g.,yields.8z plus1.sum1.x/; sum2.z// (cid:17) sum3.app1.x; z///! plus2.sum4.add1.n; x//; sum5.y// (cid:17) sum6.app2.add2.n; x/; y//:Since step formulas always are proved by modifying the induction conclusion such that theinduction hypothesis becomes applicable, the function symbols of the induction conclusionare always identified with the corresponding function symbols of the induction hypothesis,and we obtain plus1 (cid:24)K plus2, sum1 (cid:24)K sum4, sum2 (cid:24)K sum5, sum3 (cid:24)K sum6, app1 (cid:24)Kapp2, and add1 (cid:24)K add2. All these function symbols are bound and therefore identifiedsemantically as Definition 6.10 demands, yielding a proof shell PS with the schematicconjecture.8w F 1.G1.u/; G2.w// (cid:17) G3.H 1.u; w///! F 2.G4.D1.m; x//; G5.v// (cid:17) G6.H 2.D2.m; u/; v//and the set I of identification axioms fF 1.u; v/ (cid:17) F 2.u; v/; G1.u/ (cid:17) G4.u/; G2.u/ (cid:17)G5.u/; G3.u/ (cid:17) G6.u/; H 1.u; v/ (cid:17) H 2.u; v/; D1.u; v/ (cid:17) D2.u; v/g as a subset of thesemantic catch. Now if PS applies for some step formula IH ! IC, corresponding functionvariables of the schematic conjecture as, e.g., F 1, F 2 or G1, G4 are replaced by the samefunctional terms, since the function symbols in the induction conclusion IC correspond tothe function symbols in the induction hypothesis IH. For example, PS applies for  D.8z times.prod.x/; prod.z// (cid:17) prod.app.x; z///! times.prod.add.n; x//; prod.y// (cid:17) prod.app.add.n; x/; y//and all identification axioms of I are instantiated to tautologies like times.x; y/ (cid:17)times.x; y/, prod.k/ (cid:17) prod.k/, etc. This example reveals that the identification heuristicapplied to step formulas yields a proof shell with many redundant schematic formulas inthe catch and a schematic conjecture with useless generality. As an obvious remedy to thisproblem, we only supply the induction conclusion of a step formula with unique indicesand use the same indices in the induction hypothesis. For instance, ’0sD.8z plus1.sum1.x/; sum2.z// (cid:17) sum3.app1.x; z///! plus1.sum1.add1.n; x//; sum2.y// (cid:17) sum3.app1.add1.n; x/; y//is used as the indexed version of ’s . The corresponding schematic conjecture.8w F 1.G1.u/; G2.w// (cid:17) G3.H 1.u; w///! F 1.G1.D1.m; x//; G2.v// (cid:17) G3.H 1.D1.m; u/; v//still applies for , but redundant proof obligations now are avoided thus decreasing thecosts of reuse.40C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Example 6.15 (Proof shells from refined proof analysis). We resume the proof ofthe step formula ’s from Section 2 for the conjecture ’ D plus.sum.x/; sum.y// (cid:17)sum.app.x; y//, but now with refined proof analysis. For the indexed step formula ’0s fromabove, a derivation h’0hTRUE; A; Ki is obtained such thats ; ;; ;i ‘akAXplus1 (cid:24)K plus4 (cid:24)K plus7;add1 (cid:24)K add2 (cid:24)K add3;sum1 (cid:24)K sum4 (cid:24)K sum5;plus2 (cid:24)K plus5; plus6 (cid:24)K plus3;add4 (cid:24)K add5;sum3 (cid:24)K sum6 (cid:24)K sum7:app1 (cid:24)K app2 (cid:24)K app3;The indexed accumulator A contains indexed copies of the applied axioms and is obtainedas8>>>>><>>>>>:A VDindex(sum-2)index(app-2)index(sum-2)index(lem-1)sum4.add2.n; x// (cid:17) plus2.n; sum5.x//app2.add3.n; x/; y/ (cid:17) add4.n; app3.x; y//sum6.add5.n; x// (cid:17) plus3.n; sum7.x//plus4.plus5.x; y/; z/ (cid:17) plus6.x; plus7.y; z//Here IDK D ; since there are no collisions between bound function symbols, and thesyntactical catch is computed asc VD idK .A/ D8>>>>><>>>>>:sum1.add1.n; x// (cid:17) plus2.n; sum1.x//.13/.14/ app1.add1.n; x/; y/ (cid:17) add4.n; app1.x; y//sum3.add4.n; x// (cid:17) plus3.n; sum3.x//.15/.16/ plus1.plus2.x; y/; z/ (cid:17) plus3.x; plus1.y; z//9>>>>>=>>>>>;:9>>>>>=>>>>>;:Now the equations in the indexed conjecture ’0s and in the indexed catch c aregeneralized by (cid:13) VD fplus1=F 1; plus2=F 2; plus3=F 3; sum1=G1; sum2=G2; sum3=G3;app1=H 1; add1=D1; add4=D4; : : :g, 12 and we obtain the proof shell PS1 D h(cid:8); Ci ofFig. 4.We illustrate the advantages of refined proof analysis with examples from the inductiondomain by comparing attempts of reusing proofs with the simple analysis approach andwith the refined approach:Example 6.16 (Proof shells from simple and refined analysis).(i) For 3 VD plus.x; 0/ (cid:17) x, the step formula 3s VD plus.x; 0/ (cid:17) x ! plus.s.x/; 0/ (cid:17)s.x/ is computed, but the proof shell PSs from Fig. 1 does not apply since thefunction variable G from the schematic conjecture cannot be replaced both by w1and 0. Hence 3s cannot be proved by reuse based on PSs . But PS1 from Fig. 4applies via the matcher (cid:25)3 VD fF 1=plus; G1;3=w1; G2=0; H 1=w1; D1=s.w2/g 1312 Here we assume appropriate generalizations of the object variables. Since the equations in the catch are(implicitly) universally quantified, e.g., the occurrences of n in (13) and (14) from Example 6.15 denote differentvariables which must also be generalized differently.13 We use expressions like “G1;3=w1” as a shorthand for “G1=w1; G3=w1”.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6641(cid:8) VD .8u F 1.G1.x/; G2.u// (cid:17) G3.H 1.x; u///! F 1.G1.D1.n; x//; G2.y// (cid:17) G3.H 1.D1.n; x/; y//8>>><>>>:.17/ G1.D1.n; x// (cid:17) F 2.n; G1.x//.18/ H 1.D1.n; x/; y/ (cid:17) D4.n; H 1.x; y//.19/ G3.D4.n; x// (cid:17) F 3.n; G3.x//.20/ F 1.F 2.x; y/; z/ (cid:17) F 3.x; F 1.y; z//9>>>=>>>;C VDFig. 4. The proof shell PS1 obtained from the proof of ’s .and an appropriate first-order variable renaming (cid:23)3. With the solution substitution(cid:26)3 VD fD4=s.w2/; F 2;3=s.w2/g for the free function variables the totally instanti-ated catch (cid:26)3.(cid:25)3.(cid:23)3.C/// is obtained as(cid:26)3.(cid:25)3.(cid:23)3.C/// VD8>>><>>>:s.x/ (cid:17) s.x/s.x/ (cid:17) s.x/s.x/ (cid:17) s.x/.21/.22/.23/.24/ plus.s.y/; z/ (cid:17) s.plus.y; z//9>>>=>>>;:Since f(cid:25)3; (cid:26)3; (cid:23)3g is admissible for C [ f(cid:8)g and AX jD (cid:26)3.(cid:25)3.(cid:23)3.C///, 3s isproved by reuse based on PS1, cf. Theorem 4.3. Here the different instantiationsG1;3=w1 versus G2=0 are essential for the success.(ii) Let 4 VD plus.len.x/; len.y// (cid:17) len.app.x; y// where the function len is given bythe defining equations(len-1)(len-2)len.empty/ (cid:17) 0;len.add.n; x// (cid:17) s.len.x//:The step formula(cid:0) 4s VD8u plus.len.x/; len.u// (cid:17) len.app.x; u//! plus.len.add.n; x//; len.y// (cid:17) len.app.add.n; x/; y//;(cid:1)from Fig. 1 applies via the matcher (cid:25)4 VDis computed for 4 and PSsfF =plus; G=len; H =app; D=addg and an appropriate first-order variable renaming(cid:23)4. Hence we instantiate the schematic catch Cs correspondingly and obtain theproof obligations8><(cid:25)4.(cid:23)4.C// VD>:len.add.n; x// (cid:17) plus.n; len.x//.25/.26/ app.add.n; x/; y/ (cid:17) add.n; app.x; y//.27/ plus.plus.x; y/; z/ (cid:17) plus.x; plus.y; z//9>=:>;But statement (25) obviously does not hold, and therefore a reuse based on PSsfails for 4s . However, also PS1 from Fig. 4 applies for 4s via the matcher (cid:25) 0VD4fF 1=plus; G1;2;3=len; H 1=app; D1=addg and an appropriate first-order variable42C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66VD fD4=add; F 2;3=s.w2/g for the4.(cid:25) 04.(cid:23)04.C/// is obtained as4. With the solution substitution (cid:26)0renaming (cid:23)0free function variables the totally instantiated catch (cid:26)08>>><>>>:4.C/// VD4.(cid:25) 0(cid:26)04.(cid:23)04len.add.n; x// (cid:17) s.len.x//len.add.n; x// (cid:17) s.len.x//.28/.29/ app.add.n; x/; y/ (cid:17) add.n; app.x; y//.30/.31/ plus.s.y/; z/ (cid:17) s.plus.y; z//4.(cid:25) 04.(cid:23)044; (cid:23)04; (cid:26)0g is admissible for C [ f(cid:8)g and AX jD (cid:26)0Since f(cid:25) 04.C///, 4s isproved by reuse based on PS1, cf. Theorem 4.3. Note that the different instantiationsF 1=plus versus F 2;3=s.w2/ are essential for the success, and this is why the reuseattempt with the proof shell PSs fails.(iii) Let 5 VD minus.plus.dhalf.x/; uhalf.x//; x/ (cid:17) 0 where the functions minus, dhalf9>>>=>>>;:and uhalf are given by the defining equations(minus-1) minus.0; y/ (cid:17) 0;(minus-2) minus.s.x/; 0/ (cid:17) s.x/;(minus-3) minus.s.x/; s.y// (cid:17) minus.x; y/;(dhalf-1)(dhalf-2)(uhalf-1)(uhalf-2)dhalf.0/ (cid:17) 0;dhalf.s.x// (cid:17) uhalf.x/;uhalf.0/ (cid:17) 0;uhalf.s.x// (cid:17) s.dhalf.x//:The step formula 5s VD minus.plus.dhalf.x/; uhalf.x//; x/ (cid:17) 0! minus.plus.dhalf.s.x//; uhalf.s.x///; s.x// (cid:17) 0;is computed for 5 and using the lemma(lem-2) plus.x; y/ (cid:17) plus.y; x/;the proof shell PS5s from Fig. 5 is obtained from the proof of 5s by simple analysis(cid:8)5S VD F .K.H .x/; G.x//; x/ (cid:17) A! F .K.H .D.x//; G.D.x///; D.x// (cid:17) A89>>>>>><>>>>>>:.32/ H .D.x// (cid:17) G.x/.33/ G.D.x// (cid:17) D.H .x//.34/ K.x; y/ (cid:17) K.y; x/.35/ K.D.x/; y/ (cid:17) D.K.x; y//.36/ F .D.x/; D.y// (cid:17) F .x; y/>>>>>>=>>>>>>;:C5S VDFig. 5. The proof shell PS5s obtained from the proof of 5s (simple analysis).C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6643(cid:8)5S0 VD F .K.H .x/; G.x//; x/ (cid:17) A! F .K.H .D.x//; G.D.x///; D.x// (cid:17) A98>>>>>><>>>>>>:.37/ H .D.x// (cid:17) G.x/.38/ G.D.x// (cid:17) D4.H .x//.39/ K.x; y/ (cid:17) K 3.y; x/.40/ K 3.D4.x/; y/ (cid:17) D6.K.x; y//.41/ F .D6.x/; D.y// (cid:17) F .x; y/>>>>>>=>>>>>>;:C5S0 VDFig. 6. The proof shell PS5s0 obtained from the proof of 5s0 (refined analysis).Now let 6 VD or.even.x/; odd.x// (cid:17) true where the functions or, even and oddare given by the defining equationsor.true; y/ (cid:17) true;(or-1)or.false; y/ (cid:17) y;(or-2)(even-1) even.0/ (cid:17) true;(even-2) even.s.x// (cid:17) odd.x/;(odd-1)(odd-2)odd.0/ (cid:17) false;odd.s.x// (cid:17) even.x/:The step formula 6s VD or.even.x/; odd.x// (cid:17) true! or.even.s.x//; odd.s.x/// (cid:17) trueis computed for 6 and the proof shell PS5s from Fig. 5 applies via the matcher(cid:25)5 VD fF =w1; K=or; H =even; G=odd; D=s; A=trueg and an appropriate first-ordervariable renaming (cid:23)5. However, f(cid:25)5; (cid:23)5g is not admissible for C5S [ f(cid:8)5Sg, because(cid:25)5 demands a sort assignment, viz. H : nat ! bool and D : nat ! nat, such thatEq. (33) : : : (cid:17) D.H .x// from C5S is ill-sorted.By refined analysis, the proof shell PS5s0 from Fig. 6 is obtained. This proof shellalso applies via the matcher (cid:25)5 and an appropriate first-order variable renaming(cid:23)5 for the step formula 6s , but here f(cid:25)5; (cid:23)5g is admissible for C5S0 [ f(cid:8)5S0g,as the well-sortedness of (cid:25)5 does not spoil the well-sortedness of, e.g., Eq. (38): : : (cid:17) D4.H .x// from C5S0 .With the solution substitution (cid:26)5 VD fD4;6=w1; K 3=or.w2; w1/g for the freefunction variables the totally instantiated catch (cid:26)5.(cid:25)5.(cid:23)5.C5S0/// is obtained as(cid:26)5.(cid:25)5.(cid:23)5.C/// VD8>>>>>><>>>>>>:.42/ even.s.x// (cid:17) odd.x/.43/ odd.s.x// (cid:17) even.x/.44/ or.x; y/ (cid:17) or.x; y/.45/ or.y; x/ (cid:17) or.x; y/.46/ x (cid:17) x9>>>>>>=>>>>>>;:44C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Since f(cid:25)5; (cid:26)5; (cid:23)5g is admissible for C5S0 [ f(cid:8)5S0g and AX jD (cid:26)5.(cid:25)5.(cid:23)5.C5S0 nf.45/g///, 6s is proved by reuse based on PS5S0 , provided the speculated lemma(45) can be verified. Note that the introduction of the free function variables as,e.g., D4 by refined analysis are essential for the success, and this is why the reuseattempt with the proof shell PS5s fails.7. Automated proof reuseThe developments of the preceding sections can be implemented in the following way:An (automated or interactive) first-order theorem prover computes proofs within the refinedanalysis calculus. From these analyzed proofs, proof shells according to Corollary 6.13 areobtained and collected in a proof dictionary PD, i.e., a collection of “proof ideas” organizedas a finite set of proof shells. Now before the theorem prover is called for verifying a newconjecture , the proof dictionary PD is searched for a proof shell PS D h(cid:8); Ci suchthat(1)  D (cid:23) (cid:14) (cid:25).(cid:8)/,(2) (cid:23) (cid:14) (cid:25).C/ (cid:26) F .(cid:6); V/, and(3) AX jDS (cid:23) (cid:14) (cid:25).C/ for some schematic variable renaming (cid:23) and some schematicsubstitution (cid:25) which are admissible for C [ f(cid:8)g.If successful, AX jDS  is verified by reuse, because (cid:23) (cid:14) (cid:25).C/ jDS  by the ReuseTheorem 5.1.We solve the problem of finding a proof shell PS and the substitutions (cid:23) and (cid:25) for agiven conjecture  and a given proof dictionary PD in three steps:In the retrieval step, the proof dictionary PD is searched for a proof shell PS D h(cid:8); Cisuch that  D (cid:23)0 (cid:14) (cid:25)0.(cid:8)/ for some schematic variable renaming (cid:23)0 and some schematicsubstitution (cid:25)0 which are admissible for C [ f(cid:8)g. The composed substitution (cid:23)0 (cid:14) (cid:25)0 iscalled a retrieval matcher of (cid:8) and  and is applied to the schematic catch C yielding thepartially instantiated catch C0 VD (cid:23)0 (cid:14) (cid:25)0.C/.If C0 6(cid:26) F .(cid:6); V/, some (cid:8)1 2 C0nF .(cid:6); V/ is selected in the adaption step and someschematic variable renaming (cid:23)1 and some schematic substitution (cid:25)1 is computed such thatf(cid:23)0; (cid:25)0g [ f(cid:23)1; (cid:25)1g is admissible for C [ f(cid:8)g and (cid:23)1 (cid:14) (cid:25)1.(cid:8)1/ 2 F .(cid:6); V/. The composedsubstitution (cid:23)1 (cid:14) (cid:25)1 is called a partial adaption candidate of C0 and is applied to C0yielding C1 VD (cid:23)1 (cid:14) (cid:25)1.C0/. If C1 6(cid:26) F .(cid:6); V/, further partial adaption candidates (cid:23)2 (cid:14) (cid:25)2,(cid:23)3 (cid:14) (cid:25)3 etc. are computed for C1, C2 etc. until eventually some totally instantiated catchCn VD (cid:23)n (cid:14) (cid:25)n.: : : (cid:23)0 (cid:14) (cid:25)0.C/ : : :/ (cid:26) F .(cid:6); V/ is obtained. Then (cid:23)n (cid:14) (cid:25)n (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) (cid:23)1 (cid:14) (cid:25)1 is atotal adaption candidate for (cid:23)0 (cid:14) (cid:25)0.C/.Finally, the system is recursively called in the verification step for proving AX jD  0 foreach  0 2 Cn. If successful, (cid:23)n (cid:14) (cid:25)n (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) (cid:23)1 (cid:14) (cid:25)1 is an adaption matcher for (cid:23)0 (cid:14) (cid:25)0.C/ andAX jD  is verified by reuse. The recursive calls of the system necessitate recursive callsof reuse for the members of Cn, and each  0 for which reuse fails must be proved directlyby the theorem prover. In such a case the proof dictionary is extended by a further proofshell based on the analyzed proof of  0. Also the original conjecture  must be proveddirectly by the theorem prover with a subsequent extension of the proof dictionary, if theC. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6645attempt to prove  by reuse fails, i.e., the verification of AX jD  0 fails for some  0 2 Cnor no proof shells can be retrieved for . 147.1. Second-order matchingSecond-order matching is the key concept for the computation of retrieval and adaptionmatchers. Since second-order matching is decidable and finitary, cf. [41], we can imaginean algorithm match.p; t/ which solves each solvable matching problem Tp C tU, i.e.,computes a finite set f(cid:22)1 (cid:14) (cid:26)1; : : : ; (cid:22)n (cid:14) (cid:26)ng of second-order substitutions for a pattern p 2T .(cid:6) [ (cid:10); V [ U/ and a target t 2 T .(cid:6); V/ such that (cid:22)i (cid:14) (cid:26)i .p/ D t for all i 2 f1; : : : ; ng,where (cid:22)i is a schematic variable renaming and (cid:26)i is a schematic substitution. We mayapply match also to patterns p 2 F .(cid:6) [ (cid:10); V [ U/ and targets t 2 F .(cid:6); V/ by treatingquantifiers and connectives like function symbols.To incorporate sortal reasoning, the matching algorithm is supplied with furtherarguments (cid:5) and M which hold the context under which the matchers are computed. (cid:5)is a finite set of schematic variable renamings or schematic substitutions and M is a finiteset of mixed formulas, and we demand in addition that f(cid:22)i; (cid:26)ig [ (cid:5) is admissible for Mfor each (cid:22)i (cid:14) (cid:26)i 2 match.p; t; (cid:5); M/. See Appendix A for a definition of the matchingprocedure incorporating sorts.7.2. Retrieval of proof shellsRetrieval is the task of selecting a proof shell PS from a proof dictionary and computingsubstitutions (cid:25) ,(cid:23) for a given conjecture  such that PS applies for  via (cid:25) ,(cid:23):Definition 7.1 (Proof shell applies). A proof shell PS D h(cid:8); Ci applies for a conjecture 2 F .(cid:6); V/ via a schematic substitution (cid:25) : (cid:10) ! T .(cid:6); W/ and a first-order variablerenaming (cid:23) : U ! V iff(1) (cid:23) (cid:14) (cid:25).(cid:8)/ D , and(2) f(cid:25); (cid:23)g is admissible for C [ f(cid:8)g.Here requirement .1/ states that (cid:23) (cid:14) (cid:25) has to be a (syntactical) schematic matcher for (cid:8)and , where (cid:25) is a pure and closed substitution for function variables, cf. Definition 4.2,and (cid:23) renames first-order variables. Requirement .1/ can be relaxed to (cid:23) (cid:14) (cid:25).(cid:8)/ ’ where ’ allows several equivalence preserving transformations like swapping the sidesof equations, reordering subformulas or skolemizing universally quantified variables, etc.These extensions to syntactical matching can either be included in the matching algorithm,cf. [19], or be dealt with in a preprocessing step, cf. [50].Applicability of a proof shell h(cid:8); Ci for a conjecture  is decided by callingmatch.(cid:8); ; ;; C [ f(cid:8)g/ and all applicable proof shells are recognized by consideringeach member of the proof dictionary.14 If the simplifier component of an induction prover (cf. Section 1) shall be supported by reuse, Cn (cid:26) Thind.AX/is tested instead of AX jD Cn, which may necessitate recursive calls of reuse for the induction formulas computedfor some members of Cn.46C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66However, this selection process has several indeterminisms which have to be resolvedby suitable heuristics:(i) One proof shell can apply for a conjecture via several matchers.(ii) Several proof shells with the same schematic conjecture can apply for a conjecture.(iii) Several proof shells with different schematic conjectures can apply for a conjecture.Indeterminism (i) is based on the non-uniqueness of second-order matching, where, e.g.,the matching problem TF .G.x/; x/ C f.g.x/; x/U has the solutions(cid:25)1 VD fF =f.w1; w2/; G=g.w1/g;(cid:25)3 VD fF =f.g.w1/; w1/; G=w1g;(cid:25)5 VD fF =f.g.w2/; w2/; G=w1g;(cid:25)2 VD fF =f.g.w1/; w2/; G=w1g;(cid:25)4 VD fF =f.g.w2/; w1/; G=w1g;(cid:25)6 VD fF =w1; G=f.g.w1/; w1/g:(cid:15) C0n(cid:15) C1n(cid:15) C2nTo resolve this indeterminism, we propose a heuristic for rating and selecting matchersaccording to their structural complexity. For example, (cid:25)1 is considered as the less complexmatcher since it is close to replacing function variables by function symbols only, whilethe other solutions are considered as more complex. The underlying motivation is toselect a matcher which preserves the given structure of the proof being reused as far aspossible, because it is more likely to find a valid instance of the schematic catch in this(cid:18) T .(cid:6); Wn/ of functionalcase. For rating matchers heuristically, we use a hierarchy Cinterms whose complexity increases with i (based on term structure and our experience onoccurrences of functional terms in matchers from examples):VD ff .w1; : : : ; wn/ j f 2 (cid:6)ng: Function variables are replaced by functionsymbols of same arity while respecting the order of arguments.VD ff .wi1; : : : ; win / j f 2 (cid:6)n; fi1; : : : ; ing D f1; : : : ; ngg: Function variables arereplaced by function symbols of same arity with possible rearrangement of theargument order.VD ff .wi1 ; : : : ; wim / j f 2 (cid:6)m; i1; : : : ; im 2 f1; : : : ; ngg: Function variables arereplaced by function symbols. Here the arguments of function symbols can berearranged, multiplied, or omitted.(cid:15) C3n(cid:15) C4nn: This set additionally includes all projections (including identity).VD Wn [ C2VD ff1.f2.: : : fk.w1; : : : ; wn/ : : :// j f1; : : : ; fk(cid:0)1 2 (cid:6)1; fk 2 (cid:6)n; k > 1g: This setincludes compositions of (several) unary function symbols with one n-ary function.VD ff1.f2.: : : fk.wi1 ; : : : ; wim / : : :// j f1; : : : ; fk(cid:0)1 2 (cid:6)1; fk 2 (cid:6)m; k > 1; i1; : : : ;(cid:15) C5nim 2 f1; : : : ; ngg: This set combines the features of C2VD T 0.(cid:6); Wn/ which is recursively defined by(cid:15) C6n– Wn (cid:18) T 0.(cid:6); Wn/,– f .wi1; : : : ; wij(cid:0)1 ; t; wijC1 ; : : : ; wik / 2 T 0.(cid:6); Wn/ if f 2 (cid:6)k, t 2 T 0.(cid:6); Wn/, andn and C4n.i1; : : : ; ij (cid:0)1; ij C1; : : : ; ik 6 n:This set includes all combinations of projections, compositions and argument re-orderings where (recursively) at most one argument position of a functional term maybe occupied by a non-variable term.(cid:15) C7nVD T .(cid:6); Wn/.For a fixed n, we findC0n(cid:18) C1n(cid:18) C2n(cid:18) C3n;C4n(cid:18) C5n(cid:18) C6n;C0n(cid:18) C4n;C2n(cid:18) C5n;C3n(cid:18) C6n;C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6647i.e., the classes form a hierarchy of decreasing simplicity. For a matcher (cid:25) , we letXrate.(cid:25)/ VDrate.minfi j F 2 (cid:10)n; (cid:25).F / 2 Cing; (cid:25).F //F 2dom.(cid:25)/be a measure for the complexity of the replacing functional terms, where rate.i; t/ is aheuristically determined function yielding a natural number for rating the complexity of afunctional term t 2 T .(cid:6); W/. We userate.i; t/ VD i C #(cid:6) .t/where #(cid:6) : T .(cid:6); W/ ! N yields the number of occurrences of function symbols ina (functional) term since this definition has proved useful in many experiments. Nowindeterminisms of kind (i) are resolved by selecting one matcher (cid:25) with minimal rate.(cid:25)/among all retrieval matchers. For obtaining an efficient implementation, the selection ofthe matcher is built into the matching algorithm such that only the selectable matcher iscomputed.Our approach for reusing proofs is based on the heuristical assumption that similarconjectures have similar proofs. However, this heuristic must fail for certain statements andso it may happen that proof shells share the same schematic conjecture (up to a renamingof the symbols in the schematic conjectures), but have different schematic catches whichresults in indeterminisms of kind (ii). For instance, consider the proof shellsPS1 D h F .A; G.v; w// (cid:17) H .K.B; v/; w/;fF .A; u/ (cid:17) u; K.B; u/ (cid:17) u; G.u; u0/ (cid:17) H .u; u0/gi;andPS2 D h F .A; G.v; w// (cid:17) H .K.B; v/; w/;0; H .B; K.B; u/ (cid:17) BfF .A; u/ (cid:17) A00; u/ (cid:17) A0/giwhich stem, e.g., from the analyzed proofs of the base case for the associativity of plusand times respectively. Both proof shells apply exactly for the same conjectures, like, e.g.,min.0; min.y; z// (cid:17) min.min.0; y/; z/ and max.0; max.y; z// (cid:17) max.max.0; y/; z/, and aheuristic selection between the applicable proof shells is required.To this effect, we compare the (partially instantiated) catches of each applicableproof shell by estimating the expected effort for computing an adaptation matcher. Weprefer catches with a minimal number of function variables, a minimal number of proofobligations, and a maximal ratio of previous reuse successes versus failures. However, thischoice point gives also room for backtracking, human guidance, or more sophisticatedselection criteria.The retrieval of proof shells is supported, if all proof shells h(cid:8); C1i; : : : ; h(cid:8); Cni sharingthe same conjecture (cid:8) are grouped into a proof volume P V D h(cid:8); fC1; : : : ; Cngi, and theproof dictionary is organized as a set of proof volumes instead. A proof volume representsdifferent “proof ideas” for one statement, and retrieval is supported since a proof dictionarybased on proof volumes has less schematic conjectures to be considered than a proofdictionary based on proof shells, cf. [54,57].48C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Indeterminism (iii) results from the flexibility of second-order matching because, e.g.,the structurally different patterns p1 VD F .G.x/; y/ and p2 VD H .x; z; K.y// both matchthe target t VD a.y; b.x// via the matchers (cid:25)1 VD fF =a.w2; w1/; G=b.w1/g and (cid:25)2 VDfH =a.w3; b.w1//; K=w1g. While we could also use the heuristic developed for (i) to selectthe applicable proof volume with the least complex retrieval matcher (if high reusability isemphasized for the price of less efficient retrieval), we propose a further restriction of theclass of useful matchers:A matcher (cid:25) : (cid:10) ! T .(cid:6); W/ is called simple iff F 2 dom.(cid:25)/ \ (cid:10)n implies (cid:25).F / Df .w(cid:11).1/; : : : ; w(cid:11).n// for some f 2 (cid:6)n and some permutation (cid:11) of f1; : : : ; ng. Hence asimple matcher replaces function variables by function symbols of the same arity wherehowever the order of arguments may be rearanged, thus covering a fairly large classof useful matchers. A proof volume simply applies (s-applies) for a conjecture  iff itapplies for  via a simple matcher, and a proof dictionary is called s-minimal iff for eachconjecture  at most one proof volume s-applies for . S-minimal proof dictionaries areobtained if the proof shells which form the proof volumes are based on the results of therefined analysis, cf. [54].S-minimal proof dictionaries and s-application reduces the number of retrieval matchersconsiderably, however for the price reduced reusability. If more than one retrieval matcheris computed for a given conjecture, the final selection is based on the heuristic developedfor case (i).7.3. Adaptation of proof shellsAfter selection of a proof shell PS D h(cid:8); Ci from the proof dictionary and thecomputation of a retrieval matcher (cid:23)0 (cid:14) (cid:25)0, some mixed formula (cid:8)1 =2 F .(cid:6); V/ isselected from the partially instantiated catch (cid:23)0 (cid:14) (cid:25)0.C/, and a partial adaption candidate(cid:23)1 (cid:14) (cid:25)1 must be computed for (cid:8)1. Since we aim to compute an adaption matcher, (cid:8)1must be matched with some object formula ’ 2 F .(cid:6); V/ satisfying AX jD ’, whichmeans that matching is performed under the theory defined by the set of axioms AX.However, the problem of deciding whether an adaption candidate also is an adaptionmatcher is undecidable because semantical entailment is. Consequently heuristics arerequired also for the adaption step, which however strongly depend on the domain underconsideration represented by the axioms AX. Therefore it seems useful to develop adaptionheuristics from heuristics developed for automated theorem proving in the domain underconsideration, such that in most cases the computed adaption candidates are in factadaption matchers.In the induction theorem proving domain, for instance, defining equations are applied toterms in an induction formula guided by the heuristic of symbolic evaluation, cf. [82]: Inthis domain each function symbol either is a constructor of some data structure or a definedfunction symbol specified by some axioms. For instance the declarationstructureempty add.number; list/ V listdefines a data structure for linear lists of natural numbers with the constructors emptyand add, and the axioms (len-1), (len-2) from Example 6.16 define the function lencomputing the length of a list. A term t is evaluated by applying the defining equationsC. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6649for the function symbols contained in t. Thus, e.g., len.add.len.empty/; z// is evaluatedvia len.add.0; z// to s.len.z// by applying the defining equations for len. This is calledsymbolic evaluation because the term to be evaluated may contain variables as, e.g.,z (which are not evaluated). Note that only defining equations are considered for theevaluation, i.e., the term s.plus.plus.x; y/; z// remains non-evaluable, even if, e.g., theassociativity of plus is given as a lemma.Here we assume a symbolic evaluator which is implemented by a terminating operationeval and computes the normal form of a term such thateval.l/ 2 T .(cid:6); V/; EQ jDS l (cid:17) eval.l/;.47/eval.eval.l// D eval.l/holds for all l 2 T .(cid:6); V/ and a set AX D EQ [ L of axioms.We use the symbolic evaluator for the computation of adaption candidates in thefollowing way: In a first step, some mixed equation l1 (cid:17) R1 (or R1 (cid:17) l1) is selectedfrom the partially instantiated catch C1 VD (cid:23)0 (cid:14) (cid:25)0.C/ such that l1 2 T .(cid:6); V/ andif no such equation exists and we compute (cid:5)1 VDR1 =2 T .(cid:6); V/. We give up,match.R1; eval.l1/; f(cid:23)0; (cid:25)0g; C [ f(cid:8)g/ otherwise. If (cid:5)1 6D ;, i.e., matching is successful,a partial adaption candidate (cid:23)1 (cid:14) (cid:25)1 is selected from (cid:5)1. Otherwise we define (cid:23)1 VD(cid:25)1 VD " and l1 (cid:17) R1 is inserted into a so-called remainder set X. Then we continue withC2 VD (cid:23)1 (cid:14) (cid:25)1.C1nfl1 (cid:17) R1g/, i.e., some equation l2 (cid:17) R2 is selected from C2 such thatl2 2 T .(cid:6); V/ and R2 =2 T .(cid:6); V/. We give up, if no such equation exists in C2 6D ; and wecompute (cid:5)2 VD match.R2; eval.l2/; f(cid:23)1; (cid:25)1 (cid:23)0; (cid:25)0g; C [ f(cid:8)g/ otherwise, etc.Adaption always terminates with some set Cn which contains no mixed equation l (cid:17) R(or R (cid:17) l) such that l 2 T .(cid:6); V/ and R =2 T .(cid:6); V/. Adaption has failed, if(cid:3) VD Cn [ (cid:23)n (cid:14) (cid:25)n.: : : (cid:23)1 (cid:14) (cid:25)1.X/ : : :/ 6(cid:18) F .(cid:6); V/:COtherwise (cid:23)n (cid:14) (cid:25)n (cid:14) (cid:1) (cid:1) (cid:1) (cid:14) (cid:23)1 (cid:14) (cid:25)1 is the computed total adaption candidate for (cid:23)0 (cid:14) (cid:25)0.C/and C(cid:3) is the totally instantiated catch (with some already proved equations removed).We call this principle for computing adaption candidates matching modulo evaluation,because it combines automated reasoning with syntactical second-order matching, as weattempt to solve the matching problem TR C eval.l/U instead of TR C lU.The procedure for computing adaption candidates has two choice points which requirefurther determination:(1) The selection of a mixed equation li (cid:17) Ri from Ci is guided by a heuristic whichprefers the mixed equation which has probably the least number of matchers (cid:26)solving eval.li / D (cid:26).Ri/, i.e., the most “constrained” of the equations (withoutfunction variables on the lhs) is chosen. We consider an equation with a high numberof function symbols and a low number of function variables as “highly constrained”,because both criteria limit the number of possible matchers, cf. [55].(2) The selection of an adaption candidate (cid:23)i (cid:14) (cid:25)i from (cid:5)i is guided by a heuristic whichprefers the “simplest” matcher solving the chosen equation l (cid:17) R, i.e., the numberof function symbols introduced by (cid:26) is minimal, cf. [55]. For example, fF =w2gis preferred to fF =len.w1/g for the equation len.x/ (cid:17) F .x; len.x//. The underlyingmotivation is to select a matcher which preserves the given structure of the proof be-ing reused, because it is more likely to find a valid instance of the schematic proofin this case.50C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Example 7.2 (Matching modulo evaluation). Reconsider conjecture 4 VD plus.len.x/;len.y// (cid:17) len.app.x; y// and its step formula 4s from Example 6.16. Since the proofshell PS1 D h(cid:8); Ci from Fig. 4 applies for 4s via the matcher (cid:25) VD fF 1=plus; G1;2;3=len;H 1=app; D1=addg and an appropriate first-order variable renaming (cid:23), cf. Example 6.16,the partially instantiated catchCp VD (cid:25).C/ D8>>>>>><>>>>>>:len.add.n; x// (cid:17) F 2.n; len.x//.48/.49/ app.add.n; x/; y/ (cid:17) D4.n; app.x; y//len.D4.n; x// (cid:17) F 3.n; len.x//.50/.51/ plus.F 2.x; y/; z/ (cid:17) F 3.x; plus.y; z//9>>>>>>=>>>>>>;is computed for the step case.We choose Eq. (48) and evaluate the left hand side (lhs) of (48) by the defining axiom(len-2) yielding s.len.x// (cid:17) F 2.n; len.x//. Now the lhs matches the rhs via the uniquematcher (cid:26)1 VD fF 2=s.w2/g which means that (48) is solved because (cid:26)1..48// D (len-2).We apply (cid:26)1 to the remaining mixed formulas and obtain(cid:26)1.Cp n f.48/g/ VD8>>><>>>:.49/ app.add.n; x/; y/ (cid:17) D4.n; app.x; y//len.D4.n; x// (cid:17) F 3.n; len.x//.50/.52/ plus.s.y/; z/ (cid:17) F 3.x; plus.y; z//9>>>=>>>;:We continue with the next mixed equation whose (say) lhs does not contain functionvariables, i.e., with (49) or with (52). Here we choose (49) and evaluating the lhs usinga defining axiom for app yields add.n; app.x; y// which matches the rhs via the uniquematcher (cid:26)2 VD fD4=addg. Now the lhs of (cid:26)2..50// is first-order and can be evaluated, i.e.,(cid:26)3 VD fF 3=s.w2/g is obtained by matching, and all free function variables are instantiatedby the adaptation candidate (cid:26) VD fF 2=s.w2/; F 3=s.w2/; D4=addg. Since the remainingproof obligation (cid:26)..52// D plus.s.y/; z/ (cid:17) s.plus.y; z// is a defining axiom for plus, (cid:26) isan adaptation matcher and the proof reuse is completed successfully.Note that symbolic evaluation is essential for the success of adaptation. For example, inthe example above, no adaptation candidate can be computed if the unevaluated (object)terms of the equations are used as targets for the matching problems. See [55] for a moredetailed account on matching modulo evaluation.After a totally instantiated catch is computed by retrieval and adaption, the resultingproof obligations are subject to further verification which may necessitate recursive callsof reuse. While recursive reuse generally increases the potential savings of search costs,it also introduces the problem of termination. For instance, the partially instantiated catchfg.b/ (cid:17) A00; f.A00/ (cid:17) ag is computed for proving f.g.b// (cid:17) a by reuse based on the proofshell PS(cid:3) from Example 6.2, but the (simplest) solution fA00=g.b/g for the matchingC. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6651problem TA00 C g.b/U yields f.g.b// (cid:17) a as the remaining proof obligation. Since this isexactly the conjecture under consideration, recursive reuse will never terminate in thisexample. A remedy to this problem is to impose restrictions on adaption candidates whichavoids infinite reuse attempts. However, such restrictions also depend on the reasoningdomain under consideration. See [56–58] for restrictions guaranteeing termination ofrecursive reuse in the domain of induction theorem proving (where all examples in thispaper obey these restrictions).8. Experiments with reusing proofsWe have implemented a prototype called the PLAGIATOR system [13,51] which consistsof a device for analyzing, generalizing and managing proofs and is based on the techniquesdiscussed above. If a statement cannot be verified by reuse, the user must support thesystem with a proof which then is analyzed, etc. The creation of a hand crafted proof issupported by a proof editor which is also part of the system. The proof editor only checkswhether inference rules are legally applied but offers no further support in finding the proof.This means that the system is really weak in its problem solving ability, thus motivatingthe system’s name—the German word for plagiarist—as it is intended to obtain a systemexhibiting an intelligent behavior only because it is able to adapt solutions provided byother intelligent devices.Thus apart from the initial proofs provided by a human advisor in the “prove” step,none of the steps of the implemented reuse procedure necessitates human support. Hence,e.g., the proof shell of Fig. 4 is automatically reused for proving the step formulas of theapparently different conjectures ’i given in Table 1 below.This table illustrates a typical session with the PLAGIATOR system: At the beginningof the session, the human advisor submits statement ’0 (in the first row) and a proof pof ’0 to the system. Then the statements ’1; ’2; : : : are presented to the PLAGIATOR,which proves the step formula for each statement only by reuse of p such that no userinteractions are required. The third column shows the subgoals speculated by the systemwhen proving a statement by reuse, i.e., the proof obligations which are returned afterinstiating the schematic catch. Here “(cid:0)” denotes that all proof obligations are simplified totautologies by evaluation (i.e., the statement is proved by reuse only), and “T: : :U” denotesthat heuristics different from the heuristics given in Section 7 are used. For example,statement ’17 is speculated when verifying ’16, which leads to speculating ’18 which inturn entails speculation of conjecture ’19, for which eventually ’25 is speculated. For ’5an instance of conjecture ’11 is speculated, viz. the formula (cid:27)5.’11/ with (cid:27)5 D fp=m VV "g.The speculated conjectures ’53; : : : ; ’56 cannot be proved by reusing the proof of ’0:’53 VD max.m; max.n; i// (cid:17) max.max.m; n/; i/:’54 VD min.m; min.n; i// (cid:17) min.min.m; n/; i/:’55 VD or.or.eq.m; n/; a/; b/ (cid:17) or.eq.m; n/; or.a; b//:’56 VD if.eq.m; n/; k; n VV l/ <> p (cid:17) if.eq.m; n/; k <> p; n VV .l <> p//:52C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Table 1Conjectures proved and lemmata speculated by reuse of ’0XXX15’0(cid:8)No:’1’2’3’4’5’6’7’8’9’10’11’12’13’14’15’16’17’18’19’20’21’22’23’24’25’26’27’28’29’30’31’32k Cl (cid:17).k <> l/’25F 1.G1.x/; G2.y// (cid:17) G3.H 1.x; y//Conjectures proved by reuseYYYk (cid:2)l (cid:17).k <> l/j k <> l j (cid:17) j l <> k jj rev.k/ j (cid:17) j k jrev.rev.k// (cid:17) kSubgoals’18’12’13’14rev.l/ <> rev.k/ (cid:17) rev.k <> l/(cid:27)5.’11/max.maxl.k/; maxl.l// (cid:17) maxl.k <> l/min.minl.n; k/; minl.n; l// (cid:17) minl.n; k <> l/plusl.m; k/ <> plusl.m; l/ (cid:17) plusl.m; k <> l/j k j C j l j (cid:17) j k <> l jncut.m; ncut.n; k// (cid:17) ncut.plus.m; n/; k/k <> .l <> p/ (cid:17) .k <> l/ <> pj k <> n VV l j (cid:17) s.j k <> l j/j k <> n VV " j (cid:17) s.j k j/rev.k <> n VV "/ (cid:17) n VV rev.k/k <> " (cid:17) k.in/m (cid:17) im(cid:2)nim (cid:2) in (cid:17) imCnm (cid:2) .n (cid:2) i/ (cid:17) .m (cid:2) n/ (cid:2) im (cid:2) i C n (cid:2) i (cid:17) .m C n/ (cid:2) im (cid:2) i C n (cid:2) i (cid:17) i (cid:2) .m C n/m (cid:2) n (cid:17) n (cid:2) mm (cid:2) s.n/ (cid:17) m C m (cid:2) nm C .i C n/ (cid:17) i C .m C n/m C n (cid:17) n C mm C .n C i/ (cid:17) .m C n/ C im C s.n/ (cid:17) s.m C n/m C 0 (cid:17) mm (cid:2) 0 (cid:17) 0m (cid:2) s.0/ (cid:17) m2m C 2n (cid:17) 2.m C n/or.mem.m; k/; mem.m; l// (cid:17) mem.m; k <> l/rm.m; k/ <> rm.m; l/ (cid:17) rm.m; k <> l/’53’54(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)T’17U’18T’19U’25T’22; ’25UT’22UT’23U’26’26(cid:0)(cid:0)(cid:0)T(cid:0)UT(cid:0)U(cid:0)’55’56C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6653If we let (cid:25)i denote the matcher for (cid:8) and the step formula of ’i ,then, e.g.,the matcher (cid:25)1 D fF 1=(cid:2); D1=add; : : :g replaces only function variables with functionsymbols, whereas, e.g., (cid:25)30 with (cid:25)30.D1/ D s.w2/ makes use of functional terms. Thisallows to reuse the proof of the step formula of ’0 involving the data structure list for aconjecture concerning the data structure number. Similarly (cid:25)5.F 1/ D app.w2; w1/ swapsthe arguments of F 1 and (cid:25)26.G1/ D w1 uses a projection to match the schematic stepformula. The applicability of a proof shell can be increased if we “freeze variables toconstants”, i.e., a (universally quantified) variable z 2 V is regarded as a new constantz 2 (cid:6)0. Freezing variables allows to match a function variable F 2 (cid:10)n with an objectterm having more than n variables. Thus, e.g., conjecture 19, i.e., the distributivity lawfor (cid:2) and C, matches the schematic conjecture (cid:8) because now (cid:25)19.G1;2;3/ D (cid:2).w1; i/ 2T .(cid:6); W/ can be used for the matcher. 16 In the cases denoted by “T: : :U” the retrievalheuristic from Section 7 chooses an unsuccessful matcher. For an alternative successfulretrieval matcher provided by the user, however, the adaptation heuristic usually worksalso in these cases.9. Related workThe way of proving theorems by mathematical induction as used here is called explicitinduction due to the explicit computation and application of induction axioms. Explicitinduction has been implemented successfully in several induction theorem proving systemslike NQTHM [11,12], INKA [5,43], and OYSTER-CLAM [15,17], which are (sometimespartly) based on the following techniques:(i) computation of induction axioms [80,81] from well-founded orderings, whichthemselves are obtained from termination proofs for algorithms [35,83],(ii) guiding the proof of induction formulas by rippling [16,42],(iii) generalization and lemma speculation [44,45,82].In [70] a method for combining (i) and (ii) by postponing the commitment to a fixedinduction axiom is presented. When applying our reuse method to induction theoremproving, we assume that induction axioms (i) are computed elsewhere (since we reuse onlythe first-order part of a proof), but we provide an alternative technique for (ii) and (iii).Note that most of the proofs of the statements in our examples can be obtainedby rippling [16,42]. This method attempts to reduce syntactical differences between aninduction conclusion and the induction hypotheses by application of lemmata and definingequations in a goal directed way. To this effect, equations carry annotations denotingQP, and15 For the sake of readability we use mathematical (infix) symbols for functions where appropriate, i.e., (cid:2), C,(cid:0), <>, j : j, VV,denote times, plus, minus, app, len, add, sum, and prod, respectively. Further revreverses a list, max respectively maxl (min respectively minl) computes the maximum (minimum) of two numbersrespectively a list of numbers, plusl sums two lists of numbers pairwise, ncut removes the last elements of a list,mem decides list membership, and rm removes all occurrences of an element of a list. We use the conventionwith respect to variable names that i; j; n; m denote numbers, k; l; p; q denote lists, and x; y; z are variables ofany sort.16 This kind of skolemization is sound because 8z: TzU is valid iff TzU is valid for a Skolem constant z and aformula TzU with a free variable z.54C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66the syntactical difference which is reduced when the equation is applied. Since ripplingand our reuse proposal are based on different principles, one method may fail while theother succeeds for a certain proof problem. For example, our proposal is not restricted tothe induction domain, whereas rippling requires an induction conclusion and inductionhypotheses to compute the differences which must be reduced. On the other hand, astatement which is easily proved by rippling cannot be proved by reuse, if no appropriateproof shell is available. Rippling also fails if the given equations cannot be annotated.For example, the step formulas 5s and 6s from Example 6.16(iii) cannot be proved byrippling, because (by the mutual recursion) no annotation for each of the defining equations(dhalf-2), (uhalf-2), (even-2) and (odd-2) can be computed. But step formula 6s can beeasily proved by reuse as the required adaption matcher is easily computed by matchingmodulo evaluation.The utilization of past problem solving experience has attracted researchers right fromthe beginning of AI and several (considerably different) methodologies have been proposedduring the years. We briefly sketch those of these proposals which can be applied totheorem proving, and discuss similarities and significant differences with our method.9.1. Reasoning by analogyAnalogical reasoning (AR) uses a representational mapping to map problems andsolutions from a source into a target domain, cf. [38,39,48] and Fig. 8(iv). If a new problem(in the target domain) can be represented as the image of a solved source problem underthe representational mapping (recognition), it is also tried to map the source problem’ssolution into the target domain (elaboration), and the result serves as a candidate for thetarget problem’s solution, but has to be verified or repaired subsequently (evaluation). Inpresent research of applying analogical reasoning to theorem proving, e.g. [9,10,14,68], asuitable source conjecture is given (or selected) and matched with the target conjecture, andthen the source proof is mapped to the target domain step by step, guiding the target proof.Thus elaboration and evaluation are interleaved, where repairing the proposed target proof(e.g., by adding or removing intermediate inference steps) is necessary if a step suggestedby the analogy cannot be applied. This leads to new problems concerning the control ofthe repairing actions.Viewed in our framework, the representational mapping is initially given by the second-order matching between conjectures and schematic conjectures (retrieval (cid:24)D recognition),then extended by instantiating the free function variables from the schematic catch(adaptation (cid:24)D elaboration) and subsequent verification of the resulting equations (prove (cid:24)Devaluation). Hence in our approach the reuse is based on the axioms (used in the proof ofa source problem) which have to be verified for the target domain, whereas the whole newproof can always be constructed in a uniform way by “patching” an instantiated schematicproof [53]. 17 The fact that our method for reusing proofs only considers the conjecture andthe axioms used in the source problem’s solution constitutes the main difference to otherapplications of analogical reasoning to theorem proving in which the reuse is based on theproof structure which has to be modified for increasing the reuse success. Consequently17 This is not required if one is concerned with provability only, i.e., the existence of some proof.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6655analogical reasoning demands that the whole proof is replayed to guarantee the soundnessof the analogical inferences.Our method provides an increased flexibility of reuses by the second-order substitutionssuch that no repair or replay is required, cf. Theorem 4.3. These benefits also hold fora further development of reasoning by analogy in a resolution logic, which is proposedin [20]. Here a given refutation of a clause set is generalized by replacing first-orderterms with second-order terms and subsequent application of so-called generalizationrules, which, e.g., separate different occurrences of function and predicate symbols (likein our proposal of refined analysis), instantiate variables, eliminate variable and functionsymbols, change the arity of function and predicate symbols, etc. These rules aim togeneralize a refutation as much as possible but to avoid generalizations which necessitatean expensive computation of retrieval matchers or cause subsequent proofs by analogy tofail. If a new clause set S is to be investigated for unsatisfiability, S is matched with thegeneralizations already computed, and a refutation for S is obtained by applying a second-order matcher, for which the match succeeded, to the generalization. For instance, given arefutation of(cid:8)S Df:q.x1; x2/; :p.x1; x3/; p.x2; x3/g; fq.a; b/g;fp.a; a/; p.b; b/g; f:p.b; x4/g(cid:9);a refuation of(cid:8)S1 Df:q.a/; :p.b/; r.c; d/g; fq.a/g; fp.b/; r.e; d/g; f:r.x; d/g(cid:9)is obtained by the proposed analogy reasoning method. However the approach fails for theclause setS2 D(cid:8)f:q.x1; x2/; :p.x1; x3/; r.x2; x3/g; fq.a; b/g;fp.a; c/; s.d; e/g; f:s.d; x4/gf:r.b; x5/g(cid:9)as the generalization of the refutation of S does not match S2. If we apply our reuseapproach to resolution, the schematic catch of Fig. 7 is obtained from the refutation of S byrefined analysis. Since clause sets are considered here, no schematic conjecture exists and anew clause set Snew must be matched with CS to test for unsatisfiability. This test succeedsiff (*) Snew jDsub (cid:25).CS/ for some second-order substitution (cid:25) , where jDsub denotes clause8>>>>>>>>><>>>>>>>>>:(1)(2)(3)(4)(5)f:Q.u1; u2/; :P .u1; u3/; R.u2; u3/ gfQ.A; B/ gfP .A; C/; S.D; E/ gf:S.D; u4/ gf:R.B; u5/g9>>>>>>>>>=>>>>>>>>>;CS VDFig. 7. The schematic catch CS for the refutation of S.56C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66set subsumption. This reasoning is sound, because CS is unsatisfiable, hence (cid:25).CS/ isunsatisfiable, and then Snew is unsatisfiable by (*). The second-order substitution(cid:25) VD(cid:8)Q=q.a/; P =p.b/; R=r.c; d/; S=r.w2; w1/; D=d; E=e(cid:9)satisfies requirement (*) for the clause set S1, because (cid:25).CS/ D S1 [ ff:r.c; d/gg. HenceS1 must be unsatisfiable, and obviously also the unsatisfiability of S2 can be proved thisway (because S2 only is a one-to-one reformulation of CS ). But this increase of reusabilitycomes with the prize of high search costs caused by the complexity of matching clause setswhen computing a retrieval matcher (cid:25) . Therefore techniques tailored for this domain, cf.[21], are more appropriate than our approach.While we discussed the transformational analogy paradigm so far which is based onmapping problems and solutions, in the derivational analogy paradigm [6,18,76,77] ratherthe problem solving process is considered for this mapping. Hence this reuse techniqueis based on past problem solving experiences, cf. [61] for an application to theoremproving. The ABALONE system of Melis and Whittle [62] uses both transformational andderivational analogy in the domain of inductive theorem proving: Given a source theorem,a proof plan is computed whose execution yields the proof of the theorem. For a targettheorem to be proved, the function symbols of the source and the axioms used in the proofare mapped to function symbols in the target and to conjectures which are required to provethe target. Then the proof plan is replayed step by step, and—if required—modified orreformulated in a goal directed way. Although there are similarities with our reuse proposal,(e.g., the association of the source and the target by second-order substitutions, indexingof function symbols to make different occurrences explicit, speculation of lemmata), bothproposals differ significantly in two aspects:(1) Whereas our reuse method is only concerned with first-order reasoning (i.e., theproofs of base and step cases), ABALONE covers also the computation of inductionschemas and generalizations.(2) Our reuse method is based on the analysis of a proof, whereas ABALONE’s analogymethod is based on a proof plan.The latter feature in particular requires several rules for proof plan reformulation, whichcorresponds to the adaption step by matching modulo evaluation in our proposal. As allfirst-order examples in the paper can be easily solved by our reuse method, it cannotbe concluded from the paper whether the increased effort for proof plan replay andreformulation also increases the success for analogy in fact. On the other hand, ABALONEfails for some of our examples, viz. ’17, ’18 and ’19 from Table 1, as reported in [86].9.2. Explanation-based learningOur proposal can be viewed as a variant of explanation-based learning (EBL), cf. [22,30,64,65,72,73] and also [28] for a survey. This paradigm aims to improve a problem solverby generalizing a concrete problem solution which is derived after explaining the solutionof an example problem using given background knowledge: At first, a given problem issolved and the problem as well as its solution are generalized using the technique of goalregression, cf. Fig. 8(i). If a new problem is an instance of the generalized problem, theC. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6657(i)(iii)(ii)(iv)Fig. 8. Related machine learning paradigms; (i) Solve & analyse; (ii) Reuse; (iii) Abstraction; (iv) Analogy.corresponding instance of the generalized solution is obtained and some specific featuresmay remain which require subsequent solution steps, cf. Fig. 8(ii).In EBL, the generalized problem is called a (non-operational) goal concept whereasthe generalized solution is the corresponding operational specialization. The backgroundknowledge is given by the domain theory. In our terminology a schematic conjecturerespectively a schematic catch resembles a goal concept respectively its operationalspecialization and the domain theory is given by the initial axioms and lemmata.However, EBL merely provides a reformulation of a solution on the same level and lacksincorporation of abstractions in the sense of [36,63]. Applied to theorem proving, goalregression computes only EBL-generalizations which replace first-order terms by first-order variables, and this is far to weak for a frequent reuse, see [27,47,67,75]. We thereforereplace first-order terms by (certain) second-order terms and this counts for a remarkableincrease of reusability compared to the standard EBL approach, cf., e.g., Section 8 forexamples.The advantages of higher-order concepts were recognized in particular by Donat andWallen [27] and by Dietzen and Pfenning [26]. Donat and Wallen generalized standardEBL using higher-order resolution in the domain of symbolic integration. In this domain,rules for symbolic integration are represented by (conditional) production rules, as, e.g.,RRR(1)(2)(3)XA d.X/ 7! .XAC1=.A C 1//, if constant.A/ & A 6D (cid:0)1,C (cid:3) A d.X/ 7! C (cid:3)sin.X/ d.X/ 7! (cid:0)cos.X/.A d.X/, if constant.C/, andRRThese rules are applied to integration problems, as, e.g.,sequence(4)x2 dx 7! 3 (cid:3) .x3=3/.3 (cid:3) x2 dx 7! 3 (cid:3)RR3 (cid:3) x2 dx, yielding the solution58C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66From such a solution sequence a generalized rule then is computed as a higher-orderresolvent of the production rules used in the sequence, and for the exampleC (cid:3)A d.X// 7! G.ZDC1=.D C1//, if constant.D/ & D 6D (cid:0)1 & constant.C/ &(5) F .RRRF .C (cid:3)A d.Y // D G.ZD d.Z// is obtained.RGeneralized rules are applied to new integration problems like the given rules, and, e.g.,2 (cid:3) x3 dx. However,2 (cid:3) .x4=4/ is is obtained by applying (5) to the integration problemsince a generalized rule encodes all rules used in a particular solution sequence, itsapplication fails for integration problems which need to reorder applied rules or to replacethem by other rules, cf. [27]. In particular, function symbols like (cid:3), sin and the symbolfor exponentiation are not generalized and, for instance, only 3 (cid:3)sin.x/ dx is computed,3 (cid:3) sin.x/ dx. The application of the generalized rule (5) fails, sinceif (5) is applied torule (3) must be used instead of (1) to solve the integral. Applying our reuse approach tothis domain, the proof shell of Fig. 9 is obtained from (4) by simple analysis.R3 (cid:3) sin.x/ dx, proof shell PSint applies forSince3 (cid:3) sin.x/ dx via the matcher (cid:25) VD fF =w1 (cid:3) w2; C=3; G=sin.w1/; X=xg yielding theF .C; G.X; A// d.X/ matchesRRRRpartially instantiated proof shell of Fig. 10.The partially instantiated catch of Fig. 10 is solvable (yielding after a simplification step(cid:0)3 (cid:3) cos.x/ as the result of the integration problem), as (cid:25)..1// is a variant of rule (2)Z(cid:8)int VDF .C; G.X; A// d.X/ 7! F .C; H .G.X; A C 1/; A C 1//if constant.C/ & constant.A/ & A 6D (cid:0)18ZZ>>>>>><>>>>>>:(1)F .D; Y / d.Z/ 7! F .D;Y d.Z//;if constant.D/Z(2)G.U; B/ d.U / 7! H .G.U; B C 1/; B C 1/;if constant.B/ & B 6D (cid:0)19>>>>>>=>>>>>>;Cint VDFig. 9. The proof shell PSint for the solution sequence (4).Z(cid:25).(cid:8)int/ VD3 (cid:3) sin.x/ dx 7! 3 (cid:3) H .sin.x/; A C 1/if constant.A/ & A 6D (cid:0)18ZZ>>>>>><>>>>>>:(cid:25)..1//D (cid:3) Y d.Z/ 7! D (cid:3)Y d.Z/;if constant.D/Z(cid:25)..2//sin.U / d.U / 7! H .sin.U /; B C 1/;if constant.B/ & B 6D (cid:0)19>>>>>>=>>>>>>;(cid:25).Cint/ VDFig. 10. The partially instantiated proof shell (cid:25).PSint/.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6659RR1/1=2g solves (cid:25)..2//, becauseand fH =(cid:0).1 (cid:0) w2sin.U / d.U / 7! (cid:0)cos.U / by rule (3) andcos.U / D .1 (cid:0) sin2.U //1=2. A similar observation can be made for the proposal of Dietzenand Pfenning [26]. For example, the generalized ruleC (cid:3) XA dx 7! C (cid:3) .XAC1=.A C 1//(6)3 (cid:3) sin.x/ dx cannot be solved3 (cid:3) x2 dx, and also hereis computed from the solution ofby (6). Our reuse proposal is independent of specific function symbols in the conjecture andof the order of rule applications, as the catch is a set whose members may be consideredin any order. Therefore other rules may be involved in the adaption step, which counts forthe success of our reuse proposal in the example above.RR9.3. Abstraction techniquesAbstraction techniques distinguish a basic and an abstract level of problems and theirsolutions, cf. [4,36,49] and also [69,78] where this methodology is applied to theoremproving. A basic problem is(1) mapped to an abstract problem by an abstraction mapping, then(2) an abstract solution is computed for the abstract problem and finally(3) a solution is obtained for the given problem by applying the inverted abstractionmapping to the abstract solution, cf. Fig. 8(iii).Our method differs from that because we solve basic problems and generalize theirsolutions instead of solving generalized problems. The reuse step (3) is similar in bothapproaches, cf. Fig. 8(i) and (ii). The disadvantage of abstraction in the above sense isthat (depending on the used abstract level) problem solving on the abstract level eitheris much more difficult than on the basic level as most control information is lost (e.g.,consider our abstract level of formulas with second-order variables) or it is simple enoughbut the basic solution obtained in the reuse step (3) is incomplete, i.e., there are still largegaps that have to be closed subsequently by expensive basic problem solving. Thereforewe use our abstract level rather for representational purposes as it provides rigorous andadaptable criteria for the relevant concepts, viz. similarity of conjectures, generalization,and instantiation.9.4. Case-based reasoningThe rapidly growing field of case-based reasoning (CBR) [1,59,60,66,85] developsapproaches which store whole solutions in a data base and rely either purely on (effectiveretrieval in) large case bases and sometimes also on adaptation techniques, e.g., [7,32,74]. The main difference to our method is that cases in CBR only consist of a setof attribute-value pairs (sometimes enriched with information on the problem solvingprocess). Therefore CBR does not seem to be very useful in knowledge-intensive domainslike theorem proving, but has more success in domains where statistical methods basedon similarity criteria (e.g., nearest neighbor concepts) apply. The PRODIGY system usesderivational analogy as a means to integrate general problem solving with CBR [77].Here derivational analogy exploit past cases to solve similar problems so that a problemsolver can successfully create its own case library. The case base consists of problemsP D .S; G/, where S and G are sets of literals (formed with predicate and constant60C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66symbols) representing the initial state S and the goal state G of P . For a new problemP 0 D .S0; G0/, the case base is searched for a similar problem .S; G/, where .S; G/ issimilar to .S0; G0/, if some of the literals in S0 “match” some members of S and some of thegoals in G0 “match” some members of G. Here “matching” means that constant symbolsin P 0 are replaced by constant symbols of P so that identical literals are obtained afterthe replacement. The quality of “matching” is rated by the number of initial state literalsand goals which “match” literals in S and G respectively. The foot-print similarity metricemphasizes goal oriented behaviour: By application of EBL techniques to the solutionderivation of a problem .S; G/, each member g of G is associated with those membersof S which are necessary for achieving the goal g. “Matching” of the initial states now isconstrained to those initial state literals from S, which are associated with the “matched”goals g.9.5. Learning and theorem provingSome approaches for learning in theorem proving have been developed which arebased on the learning of heuristics for problem solving. For instance, [23,25,33,34]merge the experiences gained from several proofs by learning a specialized heuristic(respectively an instance of a parameterized heuristic) for the considered domain usinggenetic algorithms [33], adaptation of weights [34], or recognition of useful facts [23].These methods are based on a property of the applied proof calculus (equationalreasoning by unfailing Knuth–Bendix completion [3]), viz. that there is a central decisioninstance (for the selection of critical pairs) which is controlled by a heuristic. This alsoallows for an easy combination with from-scratch theorem proving, but makes it harderto combine several specialized heuristics for different purposes (for which the teamworkapproach [2] is a partial remedy).The method of [71] for reusing proofs in software verification is based on the replay ofinference rules, where—after a failed proof attempt for a faulty conjecture and a subsequentmodification of some axioms—parts of the failed proof are reused for a new proof attempt.A similar approach for generating tactics from proofs in an interactive environment is takenin [31].10. Summary and conclusionWe have presented an approach for verifying theorems by reusing previously computedproofs: A proof is analyzed and the essentials of the proof are represented in a certaindata structure, viz. a proof shell. For a new verification task, a suitable proof shell isselected in a (heuristically guided) retrieval step and subsequently adapted (under heuristicguidance) for the verification problem under consideration. This yields new “simpler”proof obligations which are subject to further verification, either directly by a theoremprover or recursively by reuse again.This paper defines a formal framework for proof analysis, generalization, retrievaland adaptation, discusses the problems which arise, proposes solutions and verifies thesoundness of the proposals. Our approach offers two benefits, as several experiments withC. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6661an implementation of a learning component for a theorem prover, viz. the PLAGIATORsystem [13,51], reveal, cf. Section 8 and [57]:(1) User interactions are saved, and(2) the PLAGIATOR system is able to speculate lemmata by recognizing previouslycomputed solutions, which are helpful to prove a given conjecture.The latter feature is particularly important, because induction theorem proving islemmata may yield a relevantincomplete and therefore the speculation of usefulimprovement of an induction prover’s performance cf. [44,56,58].Although these benefits can be observed in the “toy domain” of unconditional equational(inductive) theorem proving, it is not known to the authors how the method behaves inmore ambitious domains, and, of course, an answer to this question is required to assessthe general usefulness of the proposal. We believe that the postulated benefits also showup in more practical domains, but nevertheless it may also happen in the worst case thatthe problem of determining the search for a proofis simply turned into the problemof determining the search for retrieval and adaptation matchers, and then nothing issaved (and worse, costs are increased by the additional overhead). First investigations areencouraging [50], but a complete answer to this question is subject to further research tobe carried out in the next stage of our project.AcknowledgementsWe like to thank D. Hutter and T. Walsh for clarifying rippling and the anonymousreferees for constructive suggestions.Appendix A. An algorithm for well-sorted second-order matchingCombining the tests .1/ and .2/ of Definition 7.1 demands for a sorted second-ordermatching algorithm, where in particular a (with respect to sort considerations) partialmatch has to be performed, since we must respect also the sort constraints imposed by Cwhile matching only (cid:8). This is because we want to exclude as early as possible syntacticalmatchers for (cid:8) and  for which there is no well-sorted total instantiation of C.To treat a formula-pair h(cid:8); i with an algorithm match for (pairs of) terms to bedeveloped subsequently from the algorithm of Huet and Lang [41], we let hR; (cid:23)i VDdecompose.(cid:8); / denote the preprocessing step of structurally comparing (cid:8) and  (upto quantified variables and terms in equations). For example, R VD TF .u/ C a.x/; G.v/ Cb; H .u; v/ C f .y/; D C cU and (cid:23) VD fu=x; v=yg results from decomposing 8u8v F .u/ (cid:17)G.v/ ^ H .u; v/ (cid:17) D and 8x8y a.x/ (cid:17) b ^ f .y/ (cid:17) c. 18 Hence (cid:23) is a renaming andR contains pairs p C t of pattern terms p 2 T .(cid:10); U/ and target terms t 2 T .(cid:6); V/if decompose succeeds, while R D T U otherwise denotes that (cid:8) and  are structurallydifferent.18 We use a PROLOG-style list notation Ta1; : : : ; anU respectively TajRU.62C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66Thus a proof shell PS D h(cid:8); Ci applies for a conjecture  via (cid:25) and (cid:23) iff hR; (cid:23)i VDdecompose.(cid:8); / succeeds, (cid:25) is a matcher for the matching problem (cid:23).R/ VD Tp1 Ct1; : : : ; pn C tnU, i.e., (cid:25).pi/ D ti for all i 2 f1; : : : ; ng, and f(cid:25); (cid:23)g is admissible forC [ f(cid:8)g. We present a matching calculus for computing (cid:25) which is invoked with the tripleh(cid:23).R/; f(cid:23)g; fgi and operates with respect to a fixed set of mixed formulas M VD C [ f(cid:8)g:Definition A.1 (Many-sorted second-order matching calculus). The many-sorted second-order matching calculus consists of four rules operating on triples hR; (cid:5); (cid:25)i where theinput component R VD Tp1 C t1; : : : ; pn C tnU is a matching problem with respect to a fixedset of mixed formulas M, the input component (cid:5) is a set of schematic substitutions andvariable renamings, and the output component (cid:25) is a schematic substitution. Let x 2 V,f 2 (cid:6)n, p1; : : : ; pn 2 T .(cid:6) [ (cid:10); V [ U/, t1; : : : ; tn; : : : ; tm; t 2 T .(cid:6); V/, F 2 (cid:10)n, andg 2 (cid:6)m:(cid:15) VariablehTx C x j RU; (cid:5); (cid:25)ihR; (cid:5); (cid:25)i:(cid:15) DecompositionhTf .p1; : : : ; pn/ C f .t1; : : : ; tn/ j RU; (cid:5); (cid:25)ihTp1 C t1; : : : ; pn C tn j RU; (cid:5); (cid:25)i:(cid:15) ProjectionhTF .p1; : : : ; pn/ C t j RU; (cid:5); (cid:25)ihT(cid:25)i .pi/ C t j (cid:25)i.R/U; (cid:5)i ; (cid:25)i (cid:14) (cid:25)iif (cid:5)i VD (cid:5) [ f(cid:25)ig is admissible for M, where (cid:25)i VD fF =wi g for some i 2 f1; : : : ; ng.(cid:15) ImitationhTF .p1; : : : ; pn/ C g.t1; : : : ; tm/ j RU; (cid:5); (cid:25)ihTG1.(cid:25)0.p1/; : : : ; (cid:25)0.pn// C t1; : : : ; Gm.(cid:25)0.p1/; : : : ; (cid:25)0.pn// C tm j (cid:25)0.R/U; (cid:5)0; (cid:25)0 (cid:14) (cid:25)iif (cid:5)0 VD (cid:5) [ f(cid:25)0g is admissible for M, where (cid:25)0 VD fF =g.G1.w1; : : : ; wn/; : : : ;Gm.w1; : : : ; wn//g for new 19 function variables G1; : : : ; Gm 2 (cid:10)n.A sequence hhR1; (cid:5)1; (cid:25)1i; : : : ; hRn; (cid:5)n; (cid:25)nii of triples of matching problems Ri , sets ofschematic substitutions and renamings (cid:5)i , and schematic substitutions (cid:25)i is a derivationin the sorted second-order matching calculus with respect to a fixed set of mixed formulasM if for each i 2 f1; : : : ; n (cid:0) 1g, hRiC1; (cid:5)iC1; (cid:25)iC1i results from applying one of the rulesto hRi ; (cid:5)i; (cid:25)ii.A schematic substitution (cid:25) 0 is called a solution for a matching problem R with respectto (cid:5) and M iff a derivation hhR; (cid:5); fgi; : : : ; hT U; (cid:5) 0; (cid:25) 0ii exists, and we letmatch.R; (cid:5); M/ VD(cid:8)(cid:25) j (cid:25) VD fF =t 2 (cid:25)0 j F 2 (cid:10).R/g; (cid:25)(cid:9)0is a solution for Rwith respect to (cid:5) and Mdenote the set of all solutions for R with respect to (cid:5) and M where the domains of theobtained substitutions are limited to the symbols occurring in R (excluding introducedfunction variables).19 Here new means that in particular (cid:5) and M do not contain any of the function variables Gj .C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6663Theorem A.2 (Matching of proof shells). Let R VD Tp1 C t1; : : : ; pn C tnU be a matchingproblem with respect to a fixed set of mixed formulas M and let (cid:5) be a set of schematicsubstitutions and variable renamings, which is admissible for M.(i) (Soundness) If hhR; (cid:5); ;i; : : : ; hTU; (cid:5) 0; (cid:25) 0ii is a derivation in the sorted matchingcalculus then (cid:25) 0.pi/ D ti for all pi C ti 2 R and (cid:5) [ (cid:25) 0 is admissible for M.(ii) (Completness modulo most Generality) If (cid:25) 00.pi/ D ti for all pi C ti 2 R such that(cid:5) [ (cid:25) 00 is admissible for M, then there is a derivation hhR; (cid:5); ;i; : : : ; hTU; (cid:5) 0; (cid:25) 0iiin the sorted matching calculus for some (cid:25) 0 (cid:18) (cid:25) 00.Proof. Follows from [41] (respectively [40]) since we have only adapted the procedure ofHuet and Lang to our notation and our restrictions. Admissibility is guaranteed by the testsin the rules of the calculus, because (cid:5) 0 as the result of the derivation is admissible for Mand (cid:25) 0 is composed from members of (cid:5) 0. 2Since the matching calculus is locally finite (i.e., each configuration only has finitelymany successors) and also each derivation is finite, a terminating, sound and completematching algorithm is obtained by the derivation of all solutions for a sorted matchingproblem.References[1] A. Aamodt, E. Plaza, Case-based reasoning: Foundational issues, methodological variations, and systemapproaches, AI Communications 7 (1) (1994) 39–59.[2] J. Avenhaus, J. Denzinger, M. Fuchs, DISCOUNT: A system for distributed equational deduction, in:Proc. 6th International Conference on Rewriting Techniques and Applications (RTA-95), Kaiserslautern,Germany, Lecture Notes in Computer Science, Vol. 914, Springer, Berlin, 1995, pp. 397–402.[3] L. Bachmair, N. Dershowitz, D.A. Plaisted, Completion without failure, in: Colloquium on the Resolutionof Equations in Algebraic Structures, Austin (1987), Academic Press, New York, 1989.[4] R. Bergmann, W. Wilke, Learning abstract planning cases, in: N. Lavrac, S. Wrobel (Eds.), MachineLearning: ECML-95 (Proc. European Conference on Machine Learning, 1995), Heraklion, Greece, LectureNotes in Artificial Intelligence, Vol. 914, Springer, Berlin, 1995, pp. 55–76.[5] S. Biundo, B. Hummel, D. Hutter, C. Walther, The Karlsruhe induction theorem proving system, in: Proc.8th International Conference on Automated Deduction (CADE-86), Oxford, UK, 1986, pp. 672–674.[6] B. Blumenthal, B.W. Porter, Analysis and empirical studies of derivational analogy, Artificial Intelligence 67(1994) 287–327.[7] K. Börner, C.H. Coulon, E. Pippig, E.-C. Tammer, Structural similarity and adaption, in: I. Smith, B. Faltings(Eds.), Proc. 3rd European Workshop on Case-Based Reasoning (EWCBR-96), Springer, Berlin, 1996,pp. 58–75.[8] A. Bouhoula, E. Kounalis, M. Rusinowitch, SPIKE: An automatic theorem prover, in: Proc. Conference onLogic Programming and Automated Reasoning (LPAR-92), St. Petersburg, Russia, Springer, Berlin, 1992.[9] T. Boy de la Tour, R. Caferra, Proof analogy in interactive theorem proving: A method to express and use itvia second order pattern matching, in: Proc. AAAI-87, Seattle, WA, 1987, pp. 95–99.[10] T. Boy de la Tour, C. Kreitz, Building proofs by analogy via the Curry–Howard isomorphism, in: Proc.Conference on Logic Programming and Automated Reasoning (LPAR-92), St. Petersburg, Russia, Springer,Berlin, 1992, pp. 202–213.[11] R.S. Boyer, J.S. Moore, A Computational Logic, ACM Monograph Series, Academic Press, New York,1979.[12] R.S. Boyer, J.S. Moore, A Computational Logic Handbook, Perspectives in Computing, Vol. 23, AcademicPress, New York, 1988.64C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66[13] J. Brauburger, PLAGIATOR—Design and implementation of a learning theorem prover, Diploma Thesis (inGerman), TH Darmstadt, 1994.[14] B. Brock, S. Cooper, W. Pierce, Analogical reasoning and proof discovery, in: Proc. 9th InternationalConference on Automated Deduction (CADE-88), Argonne, IL, 1988, pp. 454–468.[15] A. Bundy, The use of explicit plans to guide inductive proofs, in: Proc. 9th International Conference onAutomated Deduction (CADE-88), Argonne, IL, Springer, Berlin, 1988, pp. 111–120.[16] A. Bundy, A. Stevens, F. van Harmelen, A. Ireland, A. Smaill, Rippling: A heuristic for guiding inductiveproofs, Artificial Intelligence 62 (1993) 183–253.[17] A. Bundy, F. van Harmelen, C. Horn, A. Smaill, The Oyster–Clam system, in: Proc. 10th InternationalConference on Automated Deduction (CADE-90), Kaiserslautern, Germany, 1990, pp. 647–648.[18] J.G. Carbonell, Derivational analogy: A theory of reconstructive problem solving and expertise acquisition,in: R.S. Michalski, J.G. Carbonell, T.M. Mitchell (Eds.), Machine Learning: An Artificial IntelligenceApproach, Vol. 2, Morgan Kaufmann, San Mateo, CA, 1986, Chapter 14, pp. 371–392.[19] R. Curien, Second order E-matching as a tool for automated theorem proving, in: Progress in ArtificialIntelligence (Proccedings EPIA ’93), 1993, pp. 242–257.[20] G. Defourneaux, C. Bourely, N. Peltier, Semantic generalizations for proving and disproving conjectures byanalogy, J. Automat. Reason. 20 (1998) 27–45.[21] G. Defourneaux, N. Peltier, Partial matching for analogy discovery in proofs and counter-examples, in: Proc.14th International Conference on Automated Deduction (CADE-97), Townsville, Australia, 1997.[22] G. DeJong, R. Mooney, Explanation-based learning: An alternative view, Machine Learning 1 (1986) 145–176.[23] J. Denzinger, S. Schulz, Learning domain knowledge to improve theorem proving, in: M. McRobbie,J. Slaney (Eds.), Proc. 13th International Conference on Automated Deduction (CADE-96), New Brunswick,NJ, Lecture Notes in Artificial Intelligence, Vol. 1104, Springer, Berlin, 1996, pp. 62–76.[24] N. Dershowitz, J.-P. Jouannaud, Rewrite systems, in: J. van Leeuwen (Ed.), Handbook of TheoreticalComputer Science: Formal Models and Semantics, Vol. B, Elsevier Science, Amsterdam, 1990, Chapter 6,pp. 243–320.[25] R.V. Desimone, Learning Control Knowledge within an Explanation-Based Learning Framework,Ph.D. Thesis, Department of Artificial Intelligence, University of Edinburgh, 1989.[26] S. Dietzen, F. Pfenning, Higher-order and modal logic as a framework for explanation-based generalization,Machine Learning 9 (1992) 23–55.[27] M.R. Donat, L.A. Wallen, Learning and applying generalised solutions using higher order resolution, in:Proc. 9th International Conference on Automated Deduction (CADE-88), Argonne, IL, 1988, pp. 41–60.[28] T. Ellman, Explanation-based learning: A survey of programs and perspectives, ACM ComputingSurveys 21 (2) (1989) 163–221.[29] H.B. Enderton, A Mathematical Introduction to Logic, Academic Press, San Diego, CA, 1972.[30] O. Etzioni, A structural theory of explanation-based learning, Artificial Intelligence 60 (1993) 93–139.[31] A. Felty, D. Howe, Generalization and reuse of tactic proofs, in: F. Pfenning (Ed.), Proc. 5th Conferenceon Logic Programming and Automated Reasoning (LPAR-94), Lecture Notes in Artificial Intelligence,Vol. 822, Springer, Berlin, 1994, pp. 1–15.[32] A.G. Francis, A. Ram, A domain-independent algorithm for multi-plan adaptation and merging in least-commitment planners, in: D.W. Aha, A. Ram (Eds.), Adaptation of Knowledge for Reuse. Papers from the1995 AAAI Fall Symposium, Cambridge, MA, AAAI Press, 1995, pp. 19–25.[33] M. Fuchs, Learning proof heuristics by adapting parameters, in: Proc. 12th International Conference onMachine Learning, Tahoe City, CA, 1995.[34] M. Fuchs, Experiments in the heuristic use of past proof experience, in: M. McRobbie, J. Slaney (Eds.),Proc. 13th International Conference on Automated Deduction (CADE-96), New Brunswick, NJ, LectureNotes in Artificial Intelligence, Vol. 1104, Springer, Berlin, 1996, pp. 523–537.[35] J. Giesl, Termination analysis for functional programs using term orderings, in: Proc. 2nd International StaticAnalysis Symposium (SAS-95), Glasgow, Scotland, Lecture Notes in Computer Science, Vol. 983, Springer,Berlin, 1995, pp. 154–171.[36] F. Giunchiglia, T. Walsh, A theory of abstraction, Artificial Intelligence 57 (1992) 323–389.[37] W.D. Goldfarb, The undecidability of the second-order unification problem, Theoret. Comput. Sci. 13 (1981)225–230.C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–6665[38] R. Greiner, Learning by understanding analogies, Artificial Intelligence 35 (1988) 81–125.[39] R.P. Hall, Computational approaches to analogical reasoning: A comparative analysis, Artificial Intelli-gence 39 (1989) 39–120.[40] G. Huet, A unification algorithm for typed (cid:21)-calculus, Theoret. Comput. Sci. 1 (1975) 27–57.[41] G. Huet, B. Lang, Proving and applying program transformations expressed with second-order patterns,Acta Informatica 11 (1978) 11–31.[42] D. Hutter, Colouring terms to control equational reasoning, J. Automat. Reason. 18 (1997) 399–442.[43] D. Hutter, C. Sengler, INKA: The next generation,in: M. McRobbie, J. Slaney (Eds.), Proc. 13thInternational Conference on Automated Deduction (CADE-96), New Brunswick, NJ, Lecture Notes inArtificial Intelligence, Vol. 1104, Springer, Berlin, 1996, pp. 288–292.[44] A. Ireland, A. Bundy, Productive use of failure in inductive proof, J. Automat. Reason. (Special Issue onAutomation of Proofs by Mathematical Induction) 16 (1–2) (1996).[45] D. Kapur, M. Subramaniam, Lemma discovery in automating induction, in: M. McRobbie, J. Slaney (Eds.),Proc. 13th International Conference on Automated Deduction (CADE-96), New Brunswick, NJ, LectureNotes in Artificial Intelligence, Vol. 1104, Springer, Berlin, 1996, pp. 538–552.[46] D. Kapur, H. Zhang, RRL: A rewrite rule laboratory, in: E. Lusk, R. Overbeek (Eds.), Proc. 9th InternationalConference on Automated Deduction (CADE-88), Argonne, IL, Lecture Notes in Computer Science,Vol. 310, Springer, Berlin, 1988, pp. 768–769.[47] S.T. Kedar-Cabelli, L.T. McCarty, Explanation-based generalization as resolution theorem proving, in: Proc.4th International Workshop on Machine Learning, Irvine, CA, 1987, pp. 93–106.[48] R.E. Kling, A paradigm for reasoning by analogy, Artificial Intelligence 2 (1971) 147–178.[49] C.A. Knoblock, Automatically generating abstractions for planning, Artificial Intelligence 68 (1994) 243–302.[50] T. Kolbe, Optimizing proof search by machine learning techniques, Doctoral Dissertation, TechnischeHochschule Darmstadt, Shaker, Aachen, 1997.[51] T. Kolbe, J. Brauburger, PLAGIATOR—A learning prover, in: W. McCune (Ed.), Proc. 14th InternationalConference on Automated Deduction (CADE-97), Townsville, Australia, Lecture Notes in ArtificialIntelligence, Vol. 1249, Springer, Berlin, 1997, pp. 256–259.[52] T. Kolbe, S. Glesner, Many-sorted logic in a learning theorem prover, in: G. Brewka, C. Habel, B. Nebel(Eds.), Proc. 21st German Annual Conference on Artificial Intelligence (KI-97), Freiburg, Lecture Notes inArtificial Intelligence, Vol. 1303, Springer, Berlin, 1997, pp. 75–86.[53] T. Kolbe, C. Walther, Patching proofs for reuse, in: N. Lavrac, S. Wrobel (Eds.), Proc. European Conferenceon Machine Learning (ECML-95), Heraklion, Greece, Lecture Notes in Artificial Intelligence, Vol. 912,Springer, Berlin, 1995, pp. 303–306.[54] T. Kolbe, C. Walther, Proof management and retrieval, in: Proc. IJCAI’95 Workshop on Formal Approachesto the Reuse of Plans, Proofs, and Programs, Montreal, Quebec, 1995, pp. 16–20.[55] T. Kolbe, C. Walther, Second-order matching modulo evaluation—A technique for reusing proofs, in: Proc.IJCAI-95, Montreal, Quebec, 1995, pp. 190–195.[56] T. Kolbe, C. Walther, Termination of theorem proving by reuse, in: M. McRobbie, J. Slaney (Eds.), Proc.13th International Conference on Automated Deduction (CADE-96), New Brunswick, NJ, Lecture Notes inArtificial Intelligence, Vol. 1104, Springer, Berlin, 1996, pp. 106–120.[57] T. Kolbe, C. Walther, Proof analysis, generalization and reuse, in: W. Bibel, P.H. Schmitt (Eds.), AutomatedDeduction—A Basis for Applications, Vol. II, Systems and Implementation Techniques, Applied LogicSeries, Vol. 9, Kluwer Academic, Dordrecht, 1998, pp. 189–219.[58] T. Kolbe, C. Walther, On terminating lemma speculations, J. Inform. Comput. (1999), to appear.[59] J.L. Kolodner, Case-Based Reasoning, Morgan Kaufmann, San Mateo, CA, 1993.[60] R. Lopez de Mantaras, E. Plaza, Case-Based Reasoning, MLnet News, 1995.[61] E. Melis, A model of analogy-driven proof-plan construction, in: Proc. IJCAI-95, Montreal, Quebec, MorganKaufmann, San Mateo, CA, 1995, pp. 182–188.[62] E. Melis, J. Whittle, Analogy in inductive theorem proving, J. Automat. Reason. 22 (1999) 117–147.[63] R.S. Michalski, Y. Kodratoff, Research in machine learning: Recent progress, Classification of methodsand future directions, in: Y. Kodratoff, R.S. Michalski (Eds.), Machine Learning: An Artificial IntelligenceApproach, Vol. 3, Morgan Kaufmann, San Mateo, CA, 1990, pp. 3–30.66C. Walther, T. Kolbe / Artificial Intelligence 116 (2000) 17–66[64] S. Minton, Quantitative results concerning the utility of explanation-based learning, Artificial Intelligence 42(1990) 363–391.[65] T.M. Mitchell, R.M. Keller, S.T. Kedar-Cabelli, Explanation-based generalization: A unifying view,Machine Learning 1 (1986) 47–80.[66] G. Nakhaeizadeh, N. Fuhr, K. Morik, B. Bartsch-Spörl, S. Wess, Zur Diskussion, Künstliche Intelligenz 1(1996) 36–41. Themenheft Fallbasiertes Schließen.[67] X. Nie, D.A. Plaisted, Application of explanation-based generalization in resolution theorem proving, in:Z. Ras (Ed.), Methodologies for Intelligent Systems 4, North-Holland, New York, 1989, pp. 226–233.[68] S. Owen, Analogy for Automated Reasoning, Academic Press, New York, 1990.[69] D.A. Plaisted, Theorem proving with abstraction, Artificial Intelligence 16 (1981) 47–108.[70] M. Protzen, Lazy generation of induction hypotheses, in: Proc. 12th International Conference on AutomatedDeduction (CADE-94), Nancy, France, 1994.[71] W. Reif, K. Stenzel, Reuse of proofs in software verification, in: R. Shyamasundar (Ed.), Foundation ofSoftware Technology and Theoretical Computer Science, Bombay, India, 1993.[72] S. Schrödl, Explanation-based generalization for negation as failure and multiple examples, in: Proc. 12thEuropean Conference on Artificial Intelligence (ECAI-96), Budapest, Hungary, Wiley, New York, 1996,pp. 448–452.[73] A. Segre, C. Elkan, A high-performance explanation-based learning algorithm, Artificial Intelligence 69(1994) 1–50.[74] B. Smyth, P. Cunningham, Deja Vu: A hierarchical case-based reasoning system for software design, in:Proc. 10th European Conference on Artificial Intelligence (ECAI-92), Vienna, Austria, 1992.[75] F. van Harmelen, A. Bundy, Explanation-based generalisation D partial evaluation, Artificial Intelligence 36(1988) 401–412.[76] M.M. Veloso, PRODIGY/ANALOGY: Analogical reasoning in general problem solving, in: S. Wess, K.-D.Althoff, M.M. Richter (Eds.), Topics in Case-Based Reasoning—Proceedings of the 1st European Workshopon Case-Based Reasoning, Kaiserslautern, Germany, Lecture Notes in Artificial Intelligence, Vol. 837,Springer, Berlin, 1993, pp. 33–50.[77] M.M. Veloso, J.G. Carbonell, Derivational analogy in PRODIGY: Automating case acquisition, storage, andutilization, Machine Learning 10 (1993) 249–278.[78] A. Villafiorita, F. Giunchiglia, Inductive theorem proving via abstraction,in: Proc. 4th InternationalSymposium on Artificial Intelligence and Mathematics, 1996.[79] C. Walther, A Many-Sorted Calculus Based on Resolution and Paramodulation, Research Notes on ArtificialIntelligence, Pitman, London/Morgan Kaufmann, Los Altos, CA, 1987.[80] C. Walther, Computing induction axioms, in: Proc. Conference on Logic Programming and AutomatedReasoning (LPAR-92), St. Petersburg, Russia, 1992.[81] C. Walther, Combining induction axioms by machine, in: Proc. IJCAI-93, Chambery, France, 1993.[82] C. Walther, Mathematical induction, in: D.M. Gabbay, C.J. Hogger, J.A. Robinson (Eds.), Handbook ofLogic in Artificial Intelligence and Logic Programming, Vol. 2, Oxford University Press, Oxford, 1994,pp. 127–227.[83] C. Walther, On proving the termination of algorithms by machine, Artificial Intelligence 71 (1) (1994) 101–157.[84] C. Walther, T. Kolbe, Report on proving theorems by reuse, Technical Report, Technische UniversitätDarmstadt, 1998.[85] I.D. Watson, An introduction to case-based reasoning, in: I.D. Watson (Ed.), Progress in Case-BasedReasoning, Lecture Notes in Artificial Intelligence, Vol. 1020, Springer, Berlin, 1995, pp. 3–16.[86] J. Whittle, Analogy in CLAM, Master’s Thesis, Department of Artificial Intelligence, University ofEdinburgh, 1995.