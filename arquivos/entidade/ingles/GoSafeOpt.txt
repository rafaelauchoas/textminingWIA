Artificial Intelligence 320 (2023) 103922Contents lists available at ScienceDirectArtificial Intelligencejournal homepage: www.elsevier.com/locate/artintGoSafeOpt: Scalable safe exploration for global optimization of dynamical systems ✩Bhavya Sukhija a,∗Sebastian Trimpe b, Dominik Baumann c,da Department of Computer Science, ETH Zürich, Switzerlandb Institute for Data Science in Mechanical Engineering, RWTH Aachen University, Germanyc Department of Electrical Engineering and Automation, Aalto University, Espoo, Finlandd Department of Information Technology, Uppsala University, Sweden, Matteo Turchetta a, David Lindner a, Andreas Krause a, a r t i c l e i n f oa b s t r a c tArticle history:Received 31 March 2022Received in revised form 12 April 2023Accepted 14 April 2023Available online 20 April 2023Keywords:Model-free learningBayesian optimizationSafe learningLearning optimal control policies directly on physical systems is challenging. Even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. This work proposes GoSafeOpt as the first provably safe and optimal algorithm that can safely discover globally optimal policies for systems with high-dimensional state space. We demonstrate the superiority of GoSafeOpt over competing model-free safe learning methods in simulation and hardware experiments on a robot arm.© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).1. IntroductionThe increasing complexity of modern dynamical systems often makes deriving mathematical models for traditional model-based control approaches forbiddingly involved and time-consuming. Model-free reinforcement learning (RL) meth-ods [1] are a promising alternative as they learn control policies directly from data. To succeed, they need to explore the system and its environment. Without a model, this can be risky and unsafe. Since modern hardware such as robots are ex-pensive and their repairs are time-consuming, safe exploration is crucial to apply model-free RL in real-world problems. This paper proposes GoSafeOpt, a model-free learning algorithm that can search for globally optimal policies while guaranteeing safe exploration with high probability.1.1. Related workAdvances in machine learning have motivated the usage of model-free RL algorithms for obtaining control policies [2–6]. However, directly applying these methods to policy optimization presents two major challenges: (i) Machine learning algorithms often require large amounts of data. In learning control, such data is often gathered by conducting experiments ✩This paper is part of the Special Issue: “Risk-aware Autonomous Systems: Theory and Practice”.* Corresponding author.krausea@ethz.ch (A. Krause), trimpe@dsme.rwth-aachen.de (S. Trimpe), dominik.baumann@aalto.fi (D. Baumann).E-mail addresses: bhavya.sukhija@inf.ethz.ch (B. Sukhija), matteo.turchetta@inf.ethz.ch (M. Turchetta), david.lindner@inf.ethz.ch (D. Lindner), https://doi.org/10.1016/j.artint.2023.1039220004-3702/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Fig. 1. Illustrative example with disjoint safe regions in the policy space. The blue line depicts the objective, and the orange line is the constraint function. There are two safe regions that are marked in green. SafeOpt cannot explore the global optimum if it is initialized in the left region. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)with physical systems, which is time-consuming and wears out the hardware. (ii) Learning requires exploration, which can lead to unwarranted and unsafe behaviors.Challenges (i) and (ii) can be addressed jointly by Bayesian optimization (BO) with constraints. BO [7] is a class of black-box global optimization algorithms, that has been used in a variety of works [8–11] to optimize controllers in a sample-efficient manner. In constrained BO, there are two main classes of methods. On the one hand, approaches like [12–15] find safe solutions but allow unsafe evaluations during training. Herein, we focus on approaches that guar-antee safety at all times during exploration, which is crucial when dealing with expensive hardware. SafeOpt [16] and safe learning methods that emerged from it, e.g., [17–19], guarantee safe exploration with high probability by exploiting properties of the constraint functions, e.g., regularity. Unfortunately, these methods are limited to exploring a safe set con-nected with a known initial safe policy. Therefore, they could miss the global optimum in the presence of disjoint safe regions in the policy space (see Fig. 1). Disjoint safe regions appear when learning an impedance controller for a robot arm, as we show in our experiments and in many other applications [8,20,21]. To address this limitation [21] proposesGoSafe, which can provably and safely discover the safe global optimum in the presence of disjoint safe regions under mild conditions. To achieve this, it learns safe backup policies for different states and uses them to preserve safety when evaluating policies outside of the safe set. Specifically, it switches between actively exploring local safe regions in the state and policy space and safe global exploration. However, the active exploration in the state and policy space requires a coarse discretization of the space and is infeasible for all but the simplest systems with low-dimensional state spaces, [22] argues that dimension d > 3 is already challenging. As a result, GoSafe cannot only handle most real-world dynami-cal systems, and is restricted to impractical systems with low-dimensional state spaces. The concept of switching between two exploration stages is also pursued in the stagewise safe optimization algorithm proposed in [23]. However, also [23]is restricted to an optimum connected to a safe initialization. Lastly, the general idea of learning backup policies is related to safety filters and control barrier functions [24–26]. Nevertheless, those methods require either availability or learning of a dynamics model besides learning the policy and are, therefore, model-based. In this work, we focus on a model-free approach.1.2. ContributionsThis work presents GoSafeOpt, the first model-free algorithm that can globally search optimal policies for safety-critical, real-world dynamical systems, i.e., systems with high-dimensional state spaces. GoSafeOpt does not discretize and actively explores the state space. Therefore, it overcomes the main shortcomings and restrictions of GoSafe, while still performing safe global exploration. This makes GoSafeOpt the first and only model-free safe global exploration algorithm for real-world dynamical systems. Crucially, GoSafeOpt leverages the Markov property of the system’s state to learn backup policies which it uses to guarantee safety when evaluating policies outside the safe set. This novel mechanism for learning backup policies does not depend on the dimension of the state space. We provide high-probability safety guarantees for GoSafeOpt and we prove that it recovers the safe globally optimal policy under assumptions that hold for many practical cases. Finally, we validate it in both simulated and real safety-critical path following experiments on a robotic arm (see Fig. 2), which is prohibitive for GoSafe, the only competing model-free global safe search method. Further, we show that GoSafeOpt achieves considerably better performance than SafeOpt, a state-of-the-art method for local model-free safe policy search, and its high-dimensional variants. Table 1 compares GoSafeOpt to SafeOpt and GoSafe in terms of safety guarantees, scalability, global exploration, and sample efficiency. It shows that GoSafeOpt is the only method that can perform sample-efficient global exploration in high-dimensional systems while providing safety guarantees.2B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Fig. 2. Franka Emika Panda; seven degrees of freedom robot arm used for our evaluations.Table 1Comparison of GoSafeOpt and prior work on safe exploration based on their safety guarantees, scalability, global exploration, and sample efficiency.Safe explorationSAFEOPT [18]GOSAFE [21]GOSAFEOPT (ours)✓✓✓State space with dimension d > 3✓✗✓Global explorationSample efficient✗✓✓✓✗✓2. Problem settingWe consider a Lipschitz-continuous systemdx(t) = z(x(t), u(t)) dt,(1)where z(·) represents the unknown system dynamics, x(t) ∈ X ⊂ Rs is the system state and u(t) ∈ U ⊂ Rp is the input we apply to steer the system state to follow a desired trajectory xdes(t) ∈ X for all t ≥ 0. We assume that the system starts at a known initial state x(0) = x0.The control input u(t) we apply for a given state x(t) is specified by a policy π : X × A → U , with u(t) = π (x(t), a) :=π a(x(t)). The policy is parameterized by a ∈ A ⊂ Rd, where A is a finite parameter space.1 We encode our goal of following the desired trajectory xdes(t) through an objective function, f : A → R. Note, the trajectory of a deterministic system (1)is fully determined by its initial state x0 and the control policy. Therefore, the objective is independent of the state space X . We seek for a controller parametrization a ∈ A that optimizes f for a constant initial condition x0. Since the dynamics of the system in Eq. (1) is unknown, so is the objective f . Nonetheless, we assume we obtain a noisy measurement of f (a) at any a ∈ A by running an experiment. We aim at optimizing f from these measurements in a sample-efficient way. Additionally, to avoid the deployment of harmful policies, we formulate safety as a set of unknown constraints over the system trajectories that must be satisfied at all times. Similar, as for f , these constraints only depend on the parameter aand hence take the form gi : A → R for each constraint function gi , where i ∈ {1, . . . , q} := Ig and q ∈ N. The resulting constrained optimization problem with unknown objective and constraints is:f (a)subject to gi(a) ≥ 0, ∀i ∈ Ig.maxa∈A(2)We represent the objective and constraints using a scalar-valued function in a higher dimensional domain, proposed by [18]:(cid:2)h(a, i) =f (a)gi(a)if i = 0,if i ∈ Ig,(3)with Ig = {1, . . . , q}, I := {0, 1, . . . , q}, and i ∈ I. This representation will later help us in learning the unknown function.In summary, our goal is to find the optimal and safe policy parameter for the system starting from the nominal initial condition . Note, finding the optimal policy for a fixed initial x0. We refer to the solution of Eq. (2) as the safe global optimum acondition x0 is a common task in episodic RL [1].∗Solving this problem without a dynamics model and without incurring failures for generic systems, objectives, and con-straints is hopeless. The following section introduces our assumptions to make this problem tractable.1 Infinite parameter spaces can be handled via discretization (e.g., random subsampling).3B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 1039222.1. AssumptionsTo solve the problem in Eq. (2) safely, we assume to have at least one initial safe policy to start data collection without violating constraints. This initial policy could be derived from available simulators, first principles models, or by performing controlled experiments on the hardware directly. This policy can be conservative and sub-optimal. For instance, for mobile robots, a policy that barely moves the robot could be an initial safe policy.Assumption 2.1. A set S0 ⊂ A of safe parameters is known. That is, for all parameters a in S0 we have gi(a) ≥ 0 for all i ∈ Ig .In practice, similar policies often lead to similar outcomes. In other words, the objective and the constraints exhibit regularity properties. We capture this by assuming that the function h, Eq. (3), lives in an reproducing kernel Hilbert space (RKHS) [27] and has bounded norm in that space.Assumption 2.2. The function h lies in an RKHS associated to a kernel k and has a bounded norm in that RKHS (cid:8)h(cid:8)k ≤ B. Furthermore, the objective f and constraints gi are Lipschitz continuous with known constants.Without Assumption 2.2, the constraint and reward functions can be discontinuous making it impossible to infer the safety of a policy before evaluating it and to provide safety guarantees. In practical applications, such behavior is undesirable, and therefore rare. For further discussion on the practicality of this assumption, we refer the reader to [28].Next, we formalize our assumptions on the measurement model.Assumption 2.3. We obtain noisy measurements of h with the measurement noise independent and identically distributed (i.i.d.) σ -sub-Gaussian. That is, for a measurement yi of h(·, i), we have yi = h(a, i) + (cid:4)i with (cid:4)i σ -sub-Gaussian for all i ∈ I.Assumptions 2.1, 2.2, and 2.3 are common in the safe BO literature [16–18]. However, these approaches treat the evalu-ation of a policy as a black box. In contrast, we monitor the rollout of a policy to intervene and bring the system back to safety, if necessary. This can be achieved for a Markovian [29] system, like the one we consider in Eq. (1) (see Proposition A.3in the appendix).To monitor the rollouts, we assume that we receive a state measurement after every (cid:5)t seconds and that in between discrete time steps, the system cannot arbitrarily jump, i.e., its movement within these (typically small) time intervals is bounded. Note, for many robotic systems this assumption is valid. Especially, since we can choose the sampling time (cid:5)t. However, estimating this bound can be challenging. A conservative value for the bound may be estimated by performing controlled experiments, e.g., with the safe initial policy from Assumption 2.1, directly on hardware. Simulators or first principle models, if available, can also be leveraged.Assumption 2.4. The state x(t) is measured after every (cid:5)t seconds. Furthermore, for any x (t) and ρ ∈ [0, 1], the distance to x(t + ρ(cid:5)t) induced by any action is bounded by a known constant (cid:7), that is, (cid:8)x(t + ρ(cid:5)t) − x(t)(cid:8) ≤ (cid:7).Remark: Implicitly, we here assume noise-free measurements of the state for simplicity. Our method also works for the noisy case (see Appendix A.1.1), which is typical in the real world.Triggering a backup policy for a Markovian system is not sufficient to guarantee the safety of the whole trajectory for a generic constraint. Consider the case where safety is expressed as a constraint on a cost accumulated along the trajectory. Even if we are individually safe before and after triggering a backup policy, we might be unsafe overall. Therefore, we limit the types of constraints we consider.Assumption 2.5. We assume that, for all i ∈ {1, . . . , q}, gi is defined as the minimum of a state-dependent function ¯gi along the trajectory starting in x0 with controller π a. Formally:gi(a) = min(cid:10)¯gi(x),with ξ(0,x0,a) := {x0 +x(cid:10)∈ξ(0,x0,a)(cid:3)0 z(x(τ ); π a(x(τ ))dτ } the trajectory of x(t) under policy parameter a starting from x0 at time 0.t(4)An example of such a constraint is the minimum distance of the system to an obstacle. We can now provide a formal definition of a safe experiment.Definition 2.6. An experiment is safe if, for all t ≥ 0 and all i ∈ {1, . . . , q},¯gi(x(t)) ≥ 0.(5)4B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922This is a more general way of defining safety for the optimization problem from Eq. (2). In particular, where Eq. (2)only considers trajectories associated with a fixed policy parameter a, Definition 2.6 also covers the case in which different portions of the trajectory are induced by different controllers.3. PreliminariesThis section reviews Gaussian processes (GPs) and how to use them to construct frequentist confidence intervals, as well as relevant prior work on safe exploration (SafeOpt).3.1. Gaussian processesWe model our unknown objective and constraint functions using Gaussian process regression (GPR) [30]. In GPR, our (cid:5)(cid:4)prior belief is captured by a GP, which is fully determined by a prior mean function2 and a covariance function k . a, aImportantly, if the observations are corrupted by i.i.d. Gaussian noise with variance σ 2, i.e., yi = f (ai) + v i , and v i ∼N (0, σ 2), the posterior over fis also a GP whose mean and variance can be computed in closed form. Let us denote with Yn ∈ Rn the array containing n noisy observations of f , then the posterior of f at ¯a is f (¯a) ∼ N(cid:4)μn (¯a) , σ 2where(cid:10)(cid:5)n (¯a)μn (¯a) = kn (¯a) (Kn + Inσ 2)n (¯a) = k (¯a, ¯a) − kn (¯a) (Kn + Inσ 2)σ 2−1Yn,−1kTn (¯a) .(6a)(6b)(cid:4)The entry (i, j) ∈ {1, . . . , n} × {1, . . . , n} of the covariance matrix Kn ∈ Rn×n is k ai, a j∗and the data, and In is the n × n identity matrix.tures the covariance between x(cid:5), kn (¯a) = [k(¯a, a1), . . . , k(¯a, an)] cap-Eq. (6) considers the case where ffunction from Eq. (3).3.2. Frequentist confidence intervalsis a scalar function. To model the objective f and constraints gi , we use the selector To avoid failures, we must determine the safety of a given policy before evaluating it. To this end, we reason about plausible worst-case values of the constraint gi for a new policy a. We use the posterior distribution over the objective and constraints given by Eq. (6) to build frequentist confidence intervals that hold with high probability, i.e., at least 1 − δ, and are of the form:|μn−1(a, i) − h(a, i)| ≤ β1/2n σn−1(a, i), ∀i ∈ I.(7)For functions fulfilling Assumption 2.2 and 2.3, [31,32] derive an appropriate value for βn. This value depends on δ, n and the maximum information gain γn, cf., [33].33.3. SafeOpt for model-free safe explorationSafeOpt leverages the confidence intervals presented in Section 3.2 to solve black-box constrained optimization problems while guaranteeing safety for all the iterates with high probability. It ensures safety by limiting its evaluations to a set of provably safe inputs. In particular, SafeOpt defines the lower bound of the confidence interval ln as ln(a, i) = max{1/2n σn−1(a, i)}, with l0(a, i) = 0 for all a ∈ S0, i ∈ Ig and −∞ otherwise, and the upper bound un as ln−1(a, i), μn−1(a, i) − β1/2n σn−1(a, i)} with u0(a, i) = ∞ for all a ∈ A, i ∈ I. Given a set of safe parameters un(a, i) = min{un−1(a, i), μn−1(a, i) + βSn−1, it then infers the safety of nearby parameters by combining the confidence intervals with the Lipschitz continuity of the constraints:(cid:6)(cid:7)Sn :={a ∈ A | ln(a, i) − La(cid:10)(cid:8)(cid:8)a − a(cid:10)(cid:8)(cid:8) ≥ 0},(8)i∈Iga(cid:10)∈Sn−1with La the joint Lipschitz constant of f (a), gi(a). This leads to a local expansion of the safe set. Thus, in the case of disconnected safe regions, the optimum discovered by SafeOpt may be local (see Fig. 1).2 Assumed to be zero without loss of generality (w.l.o.g).3 The maximum information gain is γn := maxobservations y A , that is the amount of information y A contains about f [33].A⊂D:| A|=nI( y A ; f A ), where I( y A ; f A ) is the mutual information between f A evaluated at points in A and the 5B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Algorithm 1 Local Safe Exploration (LSE).Input: Safe set S, set of backups B, dataset D1: Recommend parameter an with Eq. (9)2: Collect R =3: B = B ∪ R, D = D ∪ {an, h(an, i) + εn}4: Update sets S, G, and MReturn: S, B, D{an, x(k)} and h(an, i) + εnk∈N(cid:9)//Eq. (8), Appendix D Definitions D.1 and D.24. GOSAFEOPTIn this section, we present our algorithm, GoSafeOpt, which combines the sample efficient local exploration of SafeOptwith global exploration to safely discover globally optimal policies for dynamical systems. To the best of our knowledge,GoSafeOpt is the first model-free algorithm that can globally search for optimal policies, guarantee safety during exploration, and is applicable to complex hardware systems.4.1. The algorithmGoSafeOpt consists of two alternating stages, local safe exploration (LSE) and global exploration (GE). In LSE, we explore the safe portion of the parameter space connected to our current estimate of the safe set. Crucially, we exploit the Markov property to learn backup policies for each state we visit during LSE experiments. During GE, we evaluate potentially unsafe policies in the hope of identifying new, disconnected safe regions. The safety of this step is guaranteed by triggering the backup policies learned during LSE whenever necessary. If a new disconnected safe region is identified, we switch to a LSEstep. Otherwise, GoSafeOpt terminates and recommends the optimum a∗ = arg maxln(a, 0).In the following, we explain the LSE and GE stages in more detail and provide their pseudocode in Algorithms 1 and 2, respectively. Algorithm 4 presents the pseudocode for the full GoSafeOpt algorithm.4.1.1. Local safe explorationSimilar to SafeOpt, during LSE we restrict our evaluations to provably safe policies, i.e., policies in the safe set, which is initialized with the safe seed from Assumption 2.1 and is updated recursively according to Eq. (8) (line 4 in Algorithm 1). We focus our evaluations on two relevant subsets of the safe set introduced in [16]: the maximizers Mn, i.e., plausibly optimal parameters, and the expanders Gn, i.e., parameters that, if evaluated, could optimistically enlarge the safe set. For their formal definitions, see [16] or Appendix D. During LSE, we evaluate the most uncertain parameter, i.e., the parameter with the widest confidence interval, among the expanders and the maximizers:a∈Sn(9)an = arg maxa∈Gn∪Mnwhere wn(a, i) = un(a, i) − ln(a, i).maxi∈Iwn(a, i),As a by-product of these experiments, GoSafeOpt learns backup policies for all the states visited during these rollouts by leveraging the Markov property. Intuitively, for any state x(t) visited when deploying a safe policy a starting from x0, we know that the sub-trajectory {x(τ )}τ ≥tis also safe because of Assumption 2.5. Moreover, this sub-trajectory is safe regardless of how we reach x(t) since the state is Markovian. Thus, a is a valid backup policy for x(t).This means we learn about backup policies for multiple states during a single LSE experiment. To make them available duringGE, we introduce the set of backups Bn ⊆ A × X . After running an experiment with policy a, we collect all the discrete state measurements in the rollout R ={a, x(k)} and add it to the set of backups, Bn+1 = Bn ∪ R (see Algorithm 1 line 3).(cid:9)We perform LSE until the connected safe set is fully explored and the optimum within the safe set is discovered. Intuitively, this happens when we have learned our constraint and objective functions with high precision, i.e., when the uncertainty among the expanders and maximizers is less than (cid:4), and yet the safe set does not expand any further,k∈Nmaxa∈Gn−1∪Mn−1maxi∈Iwn−1(a, i) < (cid:4) and Sn−1 = Sn.(10)Note, GoSafeOpt, like SafeOpt, only explores the connected safe set in the parameter space and learns backup policies via the Markov property.4.1.2. Global explorationGE aims at discovering new, disconnected safe regions. In particular, during a GE step, we evaluate the most uncertain parameter, i.e., with the highest value for maxi∈Ig wn(a, i), outside of the safe set, a ∈ A \ Sn. As this parameter is not in our safe set, it is not guaranteed to be safe. Therefore, we monitor the state during the experiment and trigger a backup policy, learned during LSE, if we cannot guarantee staying in a safe region of the state space when continuing with the current choice of policy parameters (cf. Fig. 3).6B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922x(t)mini∈Ig gi (x) = 0Trigger backup at x(t)Fig. 3. Illustration of the boundary condition. The backup policy is triggered at x(t) if we cannot guarantee with high probability that all states in a ball around x(t)are safe (see Assumption 2.4 and Section 4.1.2).Algorithm 2 Global Exploration (GE).Input: Safe set S, confidence intervals C , set of backups B,dataset D, fail sets: E , XFail1: Recommend global parameter an with Eq. (11)2: a = an , xFail = ∅, Boundary = False3: while Experiment not finished dokT(cid:3)z (x(t), π (x(t); a)) dts = Boundary Condition(x(t), B)s , xFail = x(k)(cid:9)a = aE = E ∪ {an}, XFail = XFail ∪ {xFail}{an, x(k)}, and h(an, i) + εn∗4:x(k) = x0 +if Not Boundary thent=0Boundary, aif Boundary then∗5:6:7:8:9:10: Collect R =11: if Not Boundary then12:13:Return: S, C , B, D, E , XFailk∈NB = B ∪ R and D = D ∪ {an, h(an, i) + εn}S = S ∪ a, C(a, i) = C(a, i) ∩ [0, ∞] for all i ∈ Ig .//Rollout policy//Not at boundary yet//Trigger backup policy//update fail sets//Successful global searchAlgorithm 3 Boundary Condition.Input: x, Bn1: if ∀(as, xs) ∈ Bn, ∃i ∈ Ig , ln(as, i) − Lx ((cid:8)x − xs(cid:8) + (cid:7)) < 0 then2:3: else4:Boundary = True, Calculate aBoundary = False, a∗s (Eq. (12))= {}∗sreturn: Boundary, a∗sIf a backup policy is triggered when evaluating the parameter a, we mark the experiment as failed. To avoid repeating the same experiment, we store a and the state xFail where we intervened in sets E ⊂ A and XFail ⊂ X , respectively (see line 9 in Algorithm 2). Thus, during GE, we employ the following acquisition functionan = arg maxa∈A\(Sn∪E)maxi∈Igwn(a, i).(11)This picks the most uncertain parameter, i.e., the parameter with the widest confidence interval, that is not provably safe but that has not been shown to trigger a backup policy. If the experiment was run without triggering a backup, we know that ais safe. Therefore, we add the observed values for gi and f to the dataset and the rollout R collected during the experiment to our set of backups Bn, i.e., Bn+1 = Bn ∪ R. Furthermore, we add the parameter a to our safe set and update its lower bound, i.e., ln(a, i) = 0, ∀i ∈ Ig (see lines 12 and 13 in Algorithm 2). Then, we switch to LSE to explore the newly discovered 1/2safe area. Note, the lower bound is updated again before the LSE step, i.e., ln+1(a, i) = max{ln(a, i), μn(a, i) − βn+1σn(a, i)}for all i ∈ Ig (see Algorithm 4 line 6).If A \ (Sn ∪ E) = ∅, there are no further safe areas we can discover and GE has converged.4.1.3. Boundary conditionThroughout each GE experiment, we monitor the state evolution, and, whenever a state measurement is received, we evaluate online a boundary condition to determine whether a backup policy should be triggered. Ideally, it must (i)guarantee safety, (ii) be fast to evaluate even for high-dimensional dynamical systems, and (iii) incorporate discrete-time measurements of the state. To fulfill requirement (i), the boundary condition leverages Lipschitz continuity of the constraint. 7B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922for x ∈ XFail doAlgorithm 4 GoSafeOpt.Input: Domain A, k(·, ·), S0, C0, D0, κ , η1: Initialize GP h(a, i), E = ∅, XFail = ∅, B 0 = {(a, x0) | a ∈ S0}2: while Sn expanding or A \ (Sn ∪ E) (cid:18)= ∅ do3:4:5:6:7:8:9:10:Update Cn(a, i) := [ln(a, i), un(a, i)] ∀ a ∈ A, i ∈ Igif LSE not converged (Eq. (10)) thenif Not Boundary Condition(x, Bn) thenSn+1, Bn+1, Dn+1 = LSE(Sn, Bn, Dn )E = E \ {a}, XFail = XFail \ {x}elseSn+1, Cn+1, Bn+1, Dn+1, E, XFail = GE(Sn, Cn, Bn, Dn, E, XFail)//reevaluate fail sets//Algorithm 3//Update fail sets//see Section 3.3//Perform LSE (Algorithm 1)//Perform GE (Algorithm 2)return: arg maxa∈Snln(a, 0)In particular, when we are in x (t) , we check if there is a point (as, xs) in our set of backups Bn such that xs is sufficiently close to x (t) to guarantee that as can steer the system back to safety for any state we may reach in the next time step.Boundary Condition: During iteration n, we trigger a backup policy at x if there is no point in our set of backups (as, xs) ∈ Bn∗such that ln(as, i) ≥ Lx ((cid:8)x − xs(cid:8) + (cid:7)) for all i ∈ Ig . In this case, we use the backup parameter as with the highest safety margin, that is∗sa=max{as∈A|∃xs∈X ;(as,xs)∈Bn}mini∈Igln(as, i) − Lx (cid:8)x − xs(cid:8) .(12)Since we already calculate ln(as, i) for all i ∈ Ig and as ∈ Sn offline to update the safe set (see Eq. (8)), we only need to evaluate (cid:8)x − xs(cid:8) online, which is computationally tractable for most real-world systems (e.g., O(s) for the 2-norm, where s is the dimension of X ). Thereby, it satisfies requirement (ii) and enables the application of our algorithm to complex systems with high sampling frequencies. The boundary condition is summarized in Algorithm 3.Updating Fail Sets. Parameters for which the boundary condition is triggered, i.e., parameters evaluated unsuccessfully duringGE, are added to the fail set E . However, when LSE is repeated after discovering a new region during GE, we can learn new backup policies, which makes the boundary condition less restrictive. Hence, it may happen that a parameter a for which a backup policy was triggered during a previous GE step, i.e., a ∈ E , we would not trigger a backup policy after LSE step has converged in the new safe region. Thus, after learning new backup policies during LSE, we re-evaluate the boundary condition (line 3), and update E and XFail accordingly. These states may then be revisited during further GE steps.In summary, GoSafeOpt involves two alternating stages, LSE and GE. LSE steps are similar to SafeOpt, nonetheless, they additionally leverage the Markov property of the system to learn backup policies. These backup policies are then used in GEfor global exploration. The only model-free safe exploration method that explores globally is GoSafe. However, it evaluates a completely different and expensive boundary condition, which relies on a safe set representation in the parameter and state space. This safe set is actively explored. Because of the active exploration, and expensive boundary condition, GoSafebecomes restricted to only systems with low-dimensional state spaces.Remark. GoSafeOpt is devised for the episodic RL setting where the initial state x0 is fixed and known. In several appli-cations, the initial state is not known apriori and instead sampled i.i.d. from a state distribution ρ. Our formulation can also be extended to this setting by treating the initial state as a context variable, cf. [34]. Moreover, to guarantee safety in this setting, Assumption 2.1 has to be modified such that the parameters in the initial safe seed S 0, are safe for all initial (cid:10)(cid:10)∈ supp(ρ). Then, given a context/initial state xstates in the support of ρ, i.e., x0, the acquisition function for LSE or GE0is optimized for the context. This is similar to the contextual SafeOpt algorithm [18]. The boundary condition can also be extended to incorporate the context. Finally, for a continuous state space, supp(ρ) can be discretized similarly to as inGoSafe.4.2. Theoretical resultsThis section provides safety (Section 4.2.1) and optimality (Section 4.2.2) guarantees for GoSafeOpt.4.2.1. Safety guaranteesThe main safety result for our algorithm is that GoSafeOpt guarantees safety during all experiments.Theorem 4.1. Under Assumptions 2.1 – 2.5 and with βn as defined in [18]. GoSafeOpt guarantees, for all n ≥ 0 and any δ ∈ (0, 1), that experiments are safe as per Definition 2.6 with probability at least 1 − δ.The proof of this theorem is provided in Appendix A.1. Intuitively, we can analyze the safety of LSE and GE separately. For LSE, we can leverage the results in [18], which studies it extensively. Therefore, novel to our analysis is the safety of GE. 8B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922We show that while running experiments during GE, we can guarantee that if our boundary condition triggers a backup, we are safe, and if a backup is not triggered, then the experiment is safe, i.e., we discovered a new safe parameter.4.2.2. Optimality guaranteesNext, we analyze when GoSafeOpt can find the safe global optimum a, which is the solution to Eq. (2). During LSE, we explore the connected safe region. For each safe region we explore, we can leverage the results from [18] to prove local optimality. Furthermore, due to GE, we can discover disconnected safe regions and then repeat LSE to explore them. To this end, we define when a parameter a can be discovered by GoSafeOpt (either during LSE or during GE).∗Definition 4.2. The parameter a ∈ A is discoverable by GoSafeOpt at iteration n, if there exists a set A ⊆ Sn such that a ∈ ¯Rc(cid:4) ( A) is the largest safe set we can safely reach from A (see Eq. (A.13) in Appendix A.2 or [18,21]).(cid:4) ( A). Here, ¯RcNext, we show that if the safe global optimum (solution of Eq. (2)) is discoverable as per Definition 4.2, then we can approximate it with (cid:4)-precision.∗Theorem 4.3. Let aa finite integer ˜n ≥ 0 such that afinite integer n∗ ≥ ˜n such that with probability at least 1 − δ,be a safe global optimum. Further, let Assumptions 2.1 – 2.5 hold, βn be defined as in [18]. Assume there exists is discoverable at iteration ˜n (see Definition 4.2). Then, for any (cid:4) > 0, and δ ∈ (0, 1), there exists a ∗∗f (ˆan) ≥ f (awith ˆan = arg maxa∈Sn ln(a, 0).) − (cid:4), ∀n ≥ n∗(13)In practice, GoSafeOpt tends to find better controllers than SafeOpt, which converges after LSE. This is formalized in the following proposition.Proposition 4.4. For SafeOpt, a∗is discoverable at iteration n > 0, if and only if, it is discoverable at iteration n = 0.Proposition 4.4 states that if the parameter adoes not lie in the largest safe set reachable from S0, SafeOpt will not find it. GoSafeOpt does not suffer from the same restriction because of global exploration. In Appendix A.2.2, Lemma A.18we provide additional conditions under which GoSafeOpt can find the safe global optimum. The performance benefits forGoSafeOpt are then empirically shown in Section 5.∗Remark. The safety threshold δ is used to pick the designer’s appetite for unsafe evaluations. For a large value of δ, more pa-rameters are available for sampling at each iteration. Accordingly, the method converges faster, however, while also allowing more unsafe evaluations, see [18] for more detail.4.3. Practical modificationsIn practice, we can further improve the sample and computational efficiency by introducing minor modifications. While they do not guarantee optimality, they yield good results for our evaluation in Section 5. Furthermore, all the proposed modifications do not affect the safety guarantees of the method, and thus can be safely applied in practice.4.3.1. Fixing iterations for each stageIn Algorithm 4, we perform a global search, i.e., GE, after the convergence of LSE. Nonetheless, it may be beneficial to run LSE for a fixed amount of steps and then switch to GE, before LSE’s convergence. This heuristic allows for the early discovery of disconnected safe regions, which may improve sample efficiency. Moreover, this allows “jumping” between different safe regions of the domain that, even though would be connected if we ran the current LSE to convergence, are currently disconnected. To this end, we apply the following heuristic scheme: (i) run LSE for nLSE steps, (ii) run GE for nGEsteps or until we have discovered new safe parameters, and (iii) if GE discovers a new region, return to (i). Else, return to (i) after GE completion, but with reduced nLSE. Note, the proposed scheme still retains optimality because we do not restrict the total number of iterations with the system. However, in practice, we additionally impose an upper bound on the interactions, and therefore nLSE, and nGE influence the budget of global and local exploration, this affects optimality (cf., Appendix C).4.3.2. Updated boundary conditionIf required, the boundary condition can be further modified to reduce computation time by considering only a subset of the states collected from experiments. The updated boundary condition reduces the online computation time at the expense of a more conservative boundary condition. Due to this conservatism, we lose our optimality guarantees. In practice, however, we still achieve good results (see Section 5).9B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Fig. 4. Setup for our evaluation in Section 5. We consider a safety-critical path following problem where deviations from the desired path (blue) could cause the robot to hit the wall (red box) and incur damage.Definition 4.5. Consider ηl ∈ R and ηu ∈ R such that ηl < ηu . The interior set (cid:16)I,n and marginal set (cid:16)M,n are defined as(cid:16)I,n = {xs ∈ X | (a, xs) ∈ Bn : ∀i ∈ Ig, ln (a, i) ≥ ηu}(cid:16)M,n = {xs ∈ X | (a, xs) ∈ Bn : ∀i ∈ Ig, ηl ≤ ln (a, i) < ηu}.The interior set contains the points in our set of backups Bn that are safe with high tolerance ηu , whereas the points in the marginal set are safe with a smaller tolerance ηl . We use those sets for the updated boundary condition.Updated Boundary Condition: Consider dl ∈ R and du ∈ R such that dl < du . We trigger a backup policy at x if there is not (cid:8)(cid:8) ≤ dl. In this case, we use the (cid:10)a point xs ∈ (cid:16)I,n such that (cid:8)x − xs(cid:8) ≤ du or there is not a point xsbackup parameter a∈ (cid:16)M,n such that (cid:8)(cid:8)x − x(cid:10)s∗s∗sa=max{as∈A|(as,xs)∈Bn}ln(as, i); with xs =minx(cid:10)∈(cid:16)I,n∪(cid:16)M,n(cid:8)(cid:8)x − x(cid:10)(cid:8)(cid:8) .(14)Intuitively, we define distance tolerances du , and dl for points in Bn based on their safety tolerances ηu, ηl. As for Theo-rem 4.1, we can derive appropriate values for ηu, du , respectively ηl, dl to guarantee safety.5. EvaluationWe evaluate GoSafeOpt in simulated and real experiments on a Franka Emika Panda seven degree of freedom (DOF) robot arm4 (see Fig. 2 and 4). The objective of our experiments is to demonstrate that GoSafeOpt (i) can be applied to systems with high dimensional state spaces, (ii) is successful in safely tuning control parameters in common robotic tasks such as path following with manipulators, and (iii) is superior to the existing state-of-the-art method, SafeOpt, for safe control parameter tuning of real-world robotic systems.Accordingly, in our results, we show that GoSafeOpt can scale to high dimensional systems, jump to disconnected safe regions while guaranteeing safety, and is directly applicable to hardware tasks with high sampling frequencies. In this work, we do not consider very high dimensional parameter spaces, which are in themselves challenging to tackle for methods such as SafeOpt. Methods in [35] and [22] alleviate this challenge and can be integrated with our algorithm easily. Thus, we concentrate on the novelty of our method, which is its globally safe parameter exploration, unlike SafeOpt, and scalability to high-dimensional state spaces compared to GoSafe. Specifically, the state space of systems we consider in this section is too large for GoSafe and it cannot be applied to any of our problems.Details on the objective and constraint functions are provided in Appendix B. The hyperparameters of our experiments are listed in Appendix E.In all experiments with the robot arm, we solely control the position and velocity of the end-effector. To this end, we consider an operational space impedance controller [36] with impedance gain K (see Appendix B). The state space for our problem is six-dimensional. This is prohibitively large for GoSafe (struggles with state space greater than three [22]). Therefore, we compare our method with SafeOpt.Impedance controllers for manipulators are usually tuned manually. This is often a tedious and time-consuming process. Accordingly, we show in our results that GoSafeOpt can be used to automate this tuning safely.5.1. Simulation resultsWe first evaluate GoSafeOpt in a simulation environment based on the Mujoco physics engine [37].5 For this we consider two distinct tasks, (i) reaching a desired position, and (ii) path following. We determine the impedance gain through an 4 A video of our hardware experiments and link to code are available: https://sukhijab .github .io /GoSafeOpt /main _project .html.5 The URDFs and meshes are taken from https://github .com /StanfordASL /PandaRobot .jl.10B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Fig. 5. Comparison of the safe set for simulation task between SafeOpt and GoSafeOpt after 200 iterations. The yellow regions represent the safe sets. In each figure, the optimum is represented by a blue triangle.approximate model of the system and perform feedback linearization [36]. For the resulting linear system, we design a linear-quadratic regulator (LQR) [38] with quadratic costs that are parameterized by matrices Q ∈ Rn×n and R ∈ Rp×p . Since the model is inaccurate, the feedback linearization will not cancel all nonlinearities and the LQR will not be optimal. Thus, our goal is to tune the cost matrices Q and R to compensate for the model mismatch. This approach is similar to [9]. We evaluate our methods over twenty independent runs of 200 iterations.5.1.1. Task 1: reaching a desired positionWe select a target xdes ∈ R3 for the robot. For this task, we parameterize the matrices Q and R by two parameters (qc, r) ∈ [2, 6] × [−3, 3], that trade-off accurate tracking, i.e., large qc , and small inputs, i.e., large r. We choose the objective function to encourage reaching the target as fast as possible while penalizing large end-effector velocities and control actions (see Appendix B.1 for details). Thus in total, we have an eight-dimensional task (six-dimensional state space and two-dimensional parameter space). For analysis purposes, we run a simple grid search, that we could not run outside of simulation, to get an estimate of the safe set and the global optimum. Fig. 5 depicts the (cid:4)-precise ((cid:4) = 0.1) safe set observed via grid search. From the figure, we observe that there is a disconnected safe region.Evaluation: Fig. 5 depicts the safe sets of SafeOpt and GoSafeOpt after 200 learning iterations. We see that SafeOpt cannot discover the disconnected safe region and hence is stuck at a local optimum. On the other hand, GoSafeOpt discovers the disconnected regions and can jump within connected safe sets. The learning curve of the two methods is depicted in Fig. 7. Our method performs considerably better than SafeOpt. The optimum found by our method is 0.007 (less than (cid:4) = 0.1) close to the optimum found via the grid search. SafeOpt cannot significantly improve over the initial policy. This is because the initial safe seed S0 already contains a near-optimal policy from the connected region SafeOpt explores, i.e., maxa∈S0 f (a) ≈ maxa∈ ¯Rc(cid:4) (S0) f (a). Lastly, our method also achieves comparable safety to SafeOpt (on average 99.9% compared to 100%). We encounter the failures during LSE, which corresponds to SafeOpt, one could also expect similar behavior fromSafeOpt if it were initialized in the upper region.Remark. We can increase βn to encourage conservatism and avoid all unsafe evaluations. However, this also influences the algorithm’s convergence rate. Hence, in practice, based on the task and appetite for unsafe evaluations βn has to be selected.5.1.2. Task 2: path following taskFor this experiment we define a parameterized path for the robot arm to follow xd(ρ(t)). Here, we define ρ(t) as a state to indicate progress along the trajectory, i.e., xd(0) = x0, xd(1) = xdes. The evolution of ρ(t) ∈ [0, 1] is controlled by a param-eter aρ ∈ [0, 1], that is, ρ(t) = min{t(aρ (1/100 − 1/500) + 1/500), 1}. The objective is to find optimal control parameters for Q , R, (cid:8)(cid:8)x − xdand aρ such that we progress on xd(·) as fast as possible while ensuring that ≤ ζ . In this example, we model Q , R using three parameters, qc, r, κd, where κd ∈ [0, 1] is used to weigh the velocity cost with respect to the positional cost of our state in the Q matrix (cf. Appendix B.1). Together with aρ as a parameter, this task is eleven-dimensional, with seven states (including ρ) and four parameters. This problem incorporates a challenging trade-off between fast trajectories and high-tracking performance. We compare it to SafeOptSwarm [35], a scalable version of SafeOpt for larger parameter spaces that use adaptive discretization. The results are presented in Fig. 8. Our results again show that GoSafeOpt performs con-siderably better than SafeOpt, specifically SafeOptSwarm. Furthermore, both SafeOpt and GoSafeOpt give 100% safety over all 20 runs. We also compare our method with expected improvement with constraints (EIC) [12] in Fig. 9. EIC discourages potentially unsafe regions but allows for unsafe evaluations. Our results show that EIC, and GoSafeOpt attain similar per-formance. However, EIC has considerably more unsafe evaluations (on average greater than fifteen) than GoSafeOpt, which has none.(cid:4)ρ(t)(cid:5)(cid:8)(cid:8)211B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Fig. 6. Illustration of triggering the backup policy during GE. During the global search, the policy directs the robot towards the wall in (a) and (b). A backup policy is automatically triggered by our boundary condition, once the robot gets too close to the wall. The backup policy directs the robot away from the wall (see green arrow in (c)).Fig. 7. Mean normalized objective with standard error for SafeOpt and GoSafeOpt for the eight-dimensional simulation task (20 runs).5.2. Hardware resultsWhile the simulation results already showcased the general applicability of GoSafeOpt to high dimensional systems and its ability to discover disconnected safe regions, we now demonstrate that it can also safely optimize policies on real-world systems.Control Task: We consider a path following task (see the experimental setup in Fig. 4), and model the impedance gain K as(cid:10)(cid:11)(cid:11)(cid:11)(cid:12)K = diagK x, K y, K z, 2K x, 2K y, 2K z,where K x =αx Kr,x with Kr,x > 0 a reference value used for Franka’s impedance controller and αx ∈ [0, 1.2] the parameter we would like to tune (same for y, z). Accordingly, αx, y,z = 1 corresponds to the impedance controller provided by the manufacturer. The parameter space we consider for this task is [0, 1.2]3. We require the controller to follow the known desired path while avoiding the wall depicted in Fig. 4.Optimization Problem: We choose our objective function to encourage tracking the desired path as accurately as possible and impose a constraint on the end-effector’s distance from the wall (see Appendix B.2 for more details). We receive a measurement of the state at 250 Hz and evaluate the boundary condition during GE at 100 Hz.Evaluation: The parameter space for this task is three-dimensional. Therefore, we compare our method to SafeOptSwarm[35] and run only 50 iterations for each algorithm in three independent runs. We choose a0 = (0.6, 0.6, 0.6) as our initial policy. During our experiments, both GoSafeOpt and SafeOptSwarm provide 100% safety in all three runs. For GoSafeOpt, safety during GE is preserved by triggering a backup policy if required. One such instance is shown in Fig. 6. We see in Fig. 10 that GoSafeOpt performs considerably better than SafeOptSwarm. In particular, even if we cannot prove the existence of disconnected safe regions for this task, GoSafeOpt still finds a better policy due to GE. Interestingly, the optimal value suggested by GoSafeOpt for both αx, and α y is 1.2. Therefore, in the direction of our path, GoSafeOpt suggests aggressive controls to reduce tracking error. Moreover, the controller suggested by GoSafeOpt is more aggressive than the manufacturers’ reference controller (αx = 1.0, α y = 1.0), and tracks the trajectory better.5.2.1. Choosing hyperparametersGoSafeOpt, like many safe exploration BO algorithms such as SafeOpt and GoSafe, makes assumptions on prior knowl-edge of the system (see Section 2.1). These assumptions are crucial for theoretical guarantees. In practice, they are 12B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Fig. 8. Mean normalized objective with standard error for SafeOpt and GoSafeOpt for the eleven-dimensional simulation task (20 runs).Fig. 9. Comparison of the normalized rewards with standard error and number of unsafe evaluations (numbers on top of the bars) between SafeOptSwarm, EIC, and GoSafeOpt for the eleven-dimensional simulation task (20 runs).Fig. 10. Mean normalized objective with standard error for SafeOptSwarm and GoSafeOpt for the hardware task (3 runs). The approximate location of the jump during GE is visible and indicated with a cyan cross.hard to verify. Yet, safe exploration BO methods have been successfully and safely applied to a large breath of appli-cations [23,39–42]. In our case, we leverage the available simulator to obtain a range for the hyperparameters: kernel parameters, βn, and distance metric for the boundary condition. Lastly, with βn fixed, we fine-tune the remaining param-eters by performing controlled safe experiments with the hardware. Even though this approach gives good results, recent work from [43] investigates the hyperparameter selection problem for safe BO more systematically. In general, there are a few other works which investigate the gap between theory and practice [28,44]. Nonetheless, given the potential of these algorithms for reliable and safe artificial intelligence (AI), we acknowledge that future research on bridging this gap is needed.13B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 1039226. ConclusionThis work proposes GoSafeOpt, a novel model-free learning algorithm for global safe optimization of policies for complex dynamical systems with high-dimensional state spaces. We provide for GoSafeOpt high probability safety guarantees and show that it provably performs better than SafeOpt, a state-of-the-art model-free safe exploration algorithm. We demon-strate the superiority of our algorithm over SafeOpt empirically through our experiments. GoSafeOpt can handle more complex and realistic dynamical systems compared to existing model-free learning methods for safe global exploration, such as GoSafe. This is due to a combination of an efficient passive discovery of backup policies that leverages the Markov property of the system and a novel and efficient boundary condition to detect when to trigger a backup policy. Future ex-tensions could design hybrid algorithms that leverage the Markov property and actively explore the state space. Moreover, GoSafeOpt is designed for efficient and safe controller tuning. We believe it can be applied to other dynamical systems, e.g., in legged robotics, where controller parameter tuning is a crucial component [45].Declaration of competing interestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Data availabilityNo data was used for the research described in the article.AcknowledgementsWe would like to thank Kyrylo Sovailo for helping us with the hardware experiments on the Franka Emika Panda arm and Alonso Marco for insightful discussions and for providing the EIC code. Furthermore, we would also like to thank Christian Fiedler, Pierre-Franc¸ ois Massiani, and Steve Heim for their feedback on this work.This project has received funding from the Federal Ministry of Education and Research (BMBF) and the Ministry of Culture and Science of the German State of North Rhine-Westphalia (MKW) (grant number: StUpPD_407-21) under the Excellence Strategy of the Federal Government and the Länder, the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program grant agreement No 815943, the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, and the Microsoft Swiss Joint Research Center (grant number 2019-037).Appendix A. Proofs of theoretical resultsIn this section, we provide proof for the theoretical results stated in the main body of the paper. In the following, we denote by k discrete time indices and with t continuous ones. This difference is important because, while we obtain state measurements at discrete times, we need to preserve safety at all times. Moreover, similarly to the notation in GoSafe [21], (cid:10) ≥ t} all the states in the trajectory induced by the policy a starting we denote by ξ(t,x(t),a) = {x(t) +from x(t) at time t.(cid:3)t z(x(τ ); π a(x(τ )))dτ | tt(cid:10)A.1. Safety guaranteesIn the following, we prove Theorem 4.1, which gives the safety guarantees for GoSafeOpt. Since GoSafeOpt has two stages, LSE and GE, we can study their safety separately. For LSE, [18] provides safety guarantees. Therefore, here we focus on the safety guarantees for GE and then show that combining both will guarantee the safety of the overall algorithm. To this end, we first make a hypothesis on our safe set Sn and confidence bounds ln(a, i) and un(a, i).Hypothesis A.1. Let Sn (cid:18)= ∅. The following properties hold for all i ∈ Ig , n ≥ 0 with probability at least 1 − δ:∀a ∈ Sn : gi(a, x0) ≥ 0,∀a ∈ A : ln(a, i) ≤ gi(a, x0) ≤ un(a, i).(A.1)(A.2)We leverage this hypothesis to prove that we are safe during GE and then we show that it is satisfied for GoSafeOpt. Particularly, during LSE, [18] proves that our hypothesis is fulfilled. Hence, before GE, the safe set and the confidence intervals satisfy it. In the following, we show the updates of the safe sets and the confidence intervals implemented by GEalso satisfy our hypothesis, which is sufficient to conclude that the hypothesis is satisfied for all n ≥ 0 (we will make this concrete in Lemma A.9).14B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922During GE, we receive measurements of the state in discrete times and evaluate our boundary condition to trigger a backup policy if necessary. Therefore, we first show that even with discrete-time measurements, we can still guarantee safety in continuous time.Lemma A.2. Let Assumptions 2.4 and 2.5 hold and let k+ ≥ k− ≥ 0 be arbitrary integers. If, for all integers k ∈ [k−, k+], there exists as ∈ A such that gi(as, x(k)) ≥ Lx(cid:7) for all i ∈ Ig , then ¯gi(x(t)) ≥ 0, for all t ∈ [k−(cid:5)t, (k+ + 1)(cid:5)t] and i ∈ Ig .Proof. By choice of the sampling scheme, we have that the state x(k) measured in discrete time, corresponds to the state x(k(cid:5)t) in continuous time. Hence, gi(as, x(k)) = gi(as, x(k(cid:5)t)). Consider some k ≥ 0 and as ∈ A such that gi(as, x(k(cid:5)t)) ≥Lx(cid:7). For any t ∈ [k(cid:5)t, (k + 1)(cid:5)t] we havegi(as, x(k(cid:5)t)) − gi(as, x(t)) ≤ Lx (cid:8)x(k(cid:5)t) − x(t)(cid:8)(Lipschitz continuity (Assumption 2.2))≤ Lx(cid:7).Now, since gi(as, x(k(cid:5)t)) ≥ Lx(cid:7), we have, for all t ∈ [k(cid:5)t, (k + 1)(cid:5)t] and i ∈ Ig ,gi(as, x(t)) ≥ gi(as, x(k(cid:5)t)) − Lx(cid:7) ≥ 0.(Assumption 2.4)(A.3)For our choice of constraints (Assumption 2.5) this implies ¯gi(x(t)) ≥ 0 for all i ∈ Ig and t ∈ [k(cid:5)t, (k + 1)(cid:5)t]. Finally, since this holds for all integers k with k− ≤ k ≤ k+, it also holds for all t ∈ [k−(cid:5)t, (k+ + 1)(cid:5)t]. (cid:2)Now we have established a condition that guarantees for a given time interval that ¯gi (x(t)) ≥ 0 for all i ∈ Ig .We collect parameter and state combinations during rollouts in our set of backups Bn . The intuition here is that for a Markovian system, all states visited during a safe experiment are also safe. This is important as it allows GoSafeOpt to learn backup policies for multiple states without actively exploring the state space. We formalize this in the following proposition.Proposition A.3. Let Assumption 2.5 hold. If (a, x0) is safe, that is, minx(cid:10)∈ξ(0,x0,a)is also safe, that is minx(cid:10)∈ξ(t1,x(t1),a)(cid:10)) ≥ 0 for all i ∈ Ig .¯gi(x¯gi(x(cid:10)) ≥ 0 for all i ∈ Ig , then, for all t1 ≥ 0, (a, x(t1))Proof. The system in Eq. (1) is Markovian, i.e., for any x(t1) ∈ ξ(0,x0,a) and x(t2) ∈ ξ(0,x0,a) with t2 > t1 > 0,x(t2) = x0 +t1(cid:13)0t2(cid:13)z(x(t); π a(x(t))dt +z(x(t); π a(x(t))dtt1t2(cid:13)= x(t1) +z(x(t); π a(x(t))dt.t1Therefore, a trajectory starting in x(t1) will always result in the same state evolution, independent of how we arrived at x(t1). Combining this and Assumption 2.5, we getgi(a, x(t1)) = minx(cid:10)∈ξ(t1,x(t1),a)(cid:10)¯gi(x)(cid:10)¯gi(x)≥ minx(cid:10)∈ξ(0,x0,a)= gi (a, x0)≥ 0. (cid:2)Assumption 2.5Markov PropertyAssumption 2.5In the following, we show that gi(as, x0) is a lower bound for all points (as, xs) in Bn, i.e., gi(as, xs) ≥ gi(as, x0). This will play a crucial role in showing that we preserve safety whenever we trigger a backup policy.Corollary A.4. Let Assumption 2.5 hold. For all points (as, xs) in Bn, gi(as, xs) ≥ gi(as, x0) for all i ∈ Ig .Proof. Each point (as, xs) in Bn is collected during a safe experiments (see Algorithm 1 line 3 and Algorithm 2 line 12). Therefore, xs ∈ ξ(0,x0,as). The result then follows from Proposition A.3. (cid:2)15B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Corollary A.4 shows that ln(as, i) is a conservative lower bound on gi(as, xs). Crucially, if we can observe not just the rollouts but also the constraint values gi (as, xs), we could model them with a GP to obtain a potentially less conservative lower bound. However, in our work, we only assume that we can measure gi(as, x0) (Assumption 2.3).Proposition A.3 and Corollary A.4 formalize how we collect our backup policies and leverage them in our boundary condition. In the following, we prove that experiments, where we trigger a backup policy, are safe. First, we show that if the boundary condition is triggered at a time step k∗(cid:5)t, i.e., time of trigger., then we are safe up until k∗Lemma A.5. Let the assumptions from Theorem 4.1 and Hypothesis A.1 hold. If, during GE, the boundary condition from Algorithm 3triggers a backup policy at time step k∗(cid:5)t and i ∈ Ig , ¯gi(x(t)) ≥ 0 with probability at least 1 − δ.∗ > 0, then, for all t ≤ kProof. Consider k < k∗. Since the boundary condition (Algorithm 3) did not trigger a backup policy at k, we have∃(as, xs) ∈ Bn such that ln(as, i) ≥ Lx ((cid:8)x(k) − xs(cid:8) + (cid:7)) , ∀i ∈ Ig.By Lipschitz continuity of g, we havegi(as, xs) − gi(as, x(k)) ≤ Lx (cid:8)x(k) − xs(cid:8) ,which impliesgi(as, x(k)) ≥ gi(as, xs) − Lx (cid:8)x(k) − xs(cid:8)≥ gi(as, x0) − Lx (cid:8)x(k) − xs(cid:8)≥ ln(as, i) − Lx (cid:8)x(k) − xs(cid:8)≥ Lx(cid:7)∗(A.4)(A.5)Corollary A.4Hypothesis A.1(A.4)∗ − 1. (cid:2)for all i ∈ Ig and k < k. Therefore, we can use Lemma A.2 to prove the claim by choosing k− = 0 and k+ = kLemma A.5 shows that up until the time we trigger our boundary condition, we are safe with enough tolerance (Lx(cid:7)) to , we will fulfill our constraints for guarantee safety. In the following, we show that if we trigger a safe backup policy at kall times after triggering.∗Lemma A.6. Let the assumptions from Theorem 4.1 and Hypothesis A.1 hold. If during a GE experiment with parameter aGE at time step k∗∗ ≥ 0, our boundary condition triggers the backup policy as defined as (see Eq. (12))(cid:8)(cid:8) .)(cid:8)(cid:8)x − x(kln(a, i) − Lx=a∗∗sarg max{a∈A|∃x∈X ;(a,x)∈Bn}mini∈Ig(A.6)Then for all t ≥ k∗(cid:5)t, ¯gi(x(t)) ≥ 0 for all i ∈ Ig with probability at least 1 − δ.∗ = 0, this follows by definition Proof. We want to show that Eq. (A.6) finds a parameter abecause Bn consists of safe rollouts (see Algorithm 1 line 3 and Algorithm 2 line 12) and thus, for all parameters as in Bnand i ∈ Ig , we have gi(as, x0) ≥ 0.∗s such that gi(a∗)) ≥ 0. For k∗s , x(kLet us now consider any integer k∗ > 0. Let (as, xs) ∈ Bn be arbitrary. Following the same Lipschitz continuity-based arguments as in Lemma A.5, we have for all i ∈ Ig :gi(as, x(k∗)) ≥ ln(as, i) − Lx≥ ln(as, i) − Lx≥ ln(as, i) − Lx≥ 0,∗(cid:8)(cid:8)xs − x(k(cid:4) (cid:8)(cid:8)x(k(cid:4)(cid:8)(cid:8)x(k(cid:8)(cid:8))∗ − 1) − xs∗ − 1) − xs∗) − x(k(cid:8)(cid:8)∗ − 1)(cid:5)(cid:8)(cid:8)x(k(cid:5)(cid:8)(cid:8) +(cid:8)(cid:8) + (cid:7)same as Lemma A.5(Triangle inequality)(Assumption 2.4)(A.7)∗ − 1 (see where the last inequality follows from the fact that the boundary condition was not triggered at time step kSection 4.1.2). Furthermore, from Eq. (A.7) we can conclude that there exists as ∈ A such that for some xs ∈ X , (as, xs) ∈ Bn, and ln(as, i) − Lx (cid:8)xs − x(k∗)(cid:8) ≥ 0 for all i ∈ Ig . Therefore, we have for a∗s recommended by Eq. (A.6):max{a∈A|∃x∈X ;(a,x)∈Bn}mini∈Igln(a, i) − Lx(cid:8)(cid:8)x − x(k∗(cid:8)(cid:8) ≥ 0.)Hence, gi(a∗s , x(k∗)) ≥ 0 for all i ∈ Ig with probability at least 1 − δ, which proves the claim. (cid:2)16B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Lemmas A.5 and A.6 show that, if we trigger a backup policy during GE, we can guarantee the safety of the experiment before and after switching to the backup policy, respectively.Next, we prove that, if the backup policy is not triggered during GE with parameter aGE, then aGE is safe with high probability.Lemma A.7. Let the assumptions from Theorem 4.1 and Hypothesis A.1 hold. If, during GE with parameter aGE, a backup policy is not triggered by our boundary condition, then aGE is safe with probability at least 1 − δ, that is, gi(aGE, x0) ≥ 0 for all i ∈ Ig .Proof. Assume the experiment was not safe, i.e., there exists a t ≥ 0, such that for some i ∈ Ig ¯gi(x(t)) < 0. Consider the time step k ≥ 0 such that t ∈ [k(cid:5)t, (k + 1)(cid:5)t]. Since the boundary condition was not triggered during the whole experiment, it was also not triggered at time step k. This implies that (see Section 4.1.2) there exists a point (as, xs) ∈ Bn such thatln(as, i) − Lx ((cid:8)xs − x(k)(cid:8) + (cid:7)) ≥ 0,(A.8)for all i ∈ Ig . Therefore, we have gi(as, x(k)) ≥ Lx(cid:7) (Hypothesis A.1). Hence, from Lemma A.2 we have ¯gi(x(t)) ≥ 0 for all i ∈ Ig . This contradicts our assumption that for some t ≥ 0 and i ∈ Ig , ¯gi(x(t)) < 0. (cid:2)The following Corollary summarizes the safety of GE.Corollary A.8. Under the assumptions from Theorem 4.1 and Hypothesis A.1 GoSafeOpt is safe during GE, i.e., for all t ≥ 0, ¯gi(x(t)) ≥ 0for all i ∈ Ig .∗ ≥ 0, (ii) the experiment is Proof. Two scenarios can occur during GE, (i) a backup policy is triggered at some time step kcompleted without triggering a backup policy. For the first case, Lemma A.6 guarantees that we are safe after triggering the backup policy, and Lemma A.5 guarantees that we are safe before we trigger the backup. For second scenario, Lemma A.7guarantees safety. (cid:2)We have now shown that under the assumptions of Theorem 4.1 combined with Hypothesis A.1, we can guarantee that we are safe during GE, irrespective of whether we trigger a backup policy or not. We leverage this result to show that Hypothesis A.1 is satisfied for GoSafeOpt.Lemma A.9. Let the assumptions from Theorem 4.1 hold and βn be defined as in [18]. Then, Hypothesis A.1 is satisfied for GoSafeOpt, that is, with probability at least 1 − δ for all i ∈ Ig and n ≥ 0∀a ∈ Sn : gi(a, x0) ≥ 0,∀a ∈ A : ln(a, i) ≤ gi(a, x0) ≤ un(a, i).Proof. We use induction on n.(A.9)(A.10)Base case n = 0: By Assumption 2.1, we have, for all a ∈ S0, gi(a, x0) ≥ 0 for all i ∈ Ig . Moreover, the initialization of the confidence intervals presented in Section 3.3 is as follows: l0(a, i) = 0 if a ∈ S0 and −∞ otherwise, and u0(a, i) = ∞ for all a ∈ A. Thus, it follows that l0(a, i) ≤ gi(a, x0) ≤ u0(a, i) for all a ∈ A.Inductive step: Our induction hypothesis is ln−1(a, i) ≤ gi(a, x0) ≤ un−1(a, i) and gi(a, x0) ≥ 0 for all a ∈ Sn−1 and for all i ∈ Ig . Based on this, we prove that these relations hold for iteration n.We start by showing that ln(a, i) ≤ gi(a, x0) ≤ un(a, i) for all a ∈ A. To this end, we distinguish between the different updates of the two stages of GoSafeOpt, LSE and GE. During LSE, we define ln(a, i) and un(a, i) asln(a, i) = max(ln−1(a, i), μn(a, i) − βnσn(a, i)),un(a, i) = min(un−1(a, i), μn(a, i) + βnσn(a, i)).We know that gi(a, x0) ≥ ln−1 by induction hypothesis and gi(a, x0) ≥ μn(a, i) − βnσn(a, i) with probability 1 − δ from [18]. This implies gi(a, x0) ≥ ln. A similar argument holds for the upper bound.During GE, we update ln(a, i) if the parameter we evaluate induces a trajectory that does not trigger a backup pol-icy (see Algorithm 2 line 13). For this parameter, the induction hypothesis allows us to use Lemma A.7 and conclude gi(a, x0) ≥ 0. Therefore, the update of the confidence intervals during GE also satisfies Eq. (A.10) for iteration n, thus completing the induction step for the confidence intervals.As for the confidence intervals, we distinguish between the different updates of the safe set implemented by LSE andGE. In the case of GE, we update the safe set by adding the evaluated policy parameter a only if it does not trigger a backup, i.e., Sn = Sn−1 ∪ {a}. Following the same argument as above, we can conclude gi (a, x0) ≥ 0 for all i ∈ Ig . This together with the induction hypothesis means gi(a, x0) ≥ 0 for all i ∈ Ig and a ∈ Sn in case of a GE update.17B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Now we focus on LSE. We showed Eq. (A.10) holds for n. Moreover, we know by induction hypothesis gi(a, x0) ≥ 0for all a ∈ Sn−1 and for all i ∈ Ig with high probability. The update equation for the safe set (Eq. (8)) gives for all (cid:10) ∈ Sn \ Sn−1, there exists a ∈ Sn−1 such that for all i ∈ Iga(cid:8)(cid:8) ≥ 0.(cid:8)(cid:8)a − aln(a, i) − La(cid:10)(A.11)(cid:10), x0) ≥ 0. Due to the Lipschitz continuity of the We show that this is enough to guarantee with high probability that gi(aconstraint functions, we have(cid:10)gi(a, x0) ≥ gi(a, x0) − La(cid:8)(cid:8)a − a≥ ln(a, i) − La(cid:10), x0) ≥ 0 for all i ∈ Ig and a ∈ Sn with probability at least 1 − δ also in case of an LSE step. (cid:2)Therefore, gi(a(cid:8)(cid:8)(cid:8)a − a(cid:8) ,(cid:10)(cid:8)(cid:8) ≥ 0.(cid:10)(Eq. (A.11))Lemma A.9 ensures that Hypothesis A.1 holds for GoSafeOpt. We also know that under the same assumption as The-orem 4.1 and Hypothesis A.1, we are safe during GE (see Corollary A.8). Hence, we can now guarantee safety during GE. Finally, we prove Theorem 4.1, which guarantees safety for GoSafeOpt.Theorem 4.1. Under Assumptions 2.1 – 2.5 and with βn as defined in [18]. GoSafeOpt guarantees, for all n ≥ 0 and any δ ∈ (0, 1), that experiments are safe as per Definition 2.6 with probability at least 1 − δ.Proof. We perform GoSafeOpt in two stages; LSE and GE. In Lemma A.9, we proved that for all parameters a ∈ Sn, gi(a, x0) ≥ 0 for all i ∈ Ig with probability at least 1 − δ. During LSE we query parameters from Sn (Eq. (9)). Therefore, the experiments are safe. During GE, Corollary A.8 proves that when assumptions from Theorem 4.1 and Hypothesis A.1hold, we are safe during GE for our choice of βn. Furthermore, in Lemma A.9 we proved that Hypothesis A.1 is satisfied forGoSafeOpt. Hence, we can conclude that if the assumptions from Theorem 4.1 hold, we are safe during GE at all times. (cid:2)A.1.1. Proof of boundary condition for noisy measurementsLemma A.10. Assume at each time step k we receive a noisy measurement of the state to evaluate our boundary condition, i.e., y =x + ε and ε i.i.d. Specifically, assume P ((cid:8)ε(cid:8) ≤ d/2) ≥1 − δ2. If we have for some (as, ys) ∈ Bn ( ys = xs + εs)√ln(as, i) − Lx((cid:8) y − ys(cid:8) + (cid:7) + d) ≥ 0,then with probability at least 1 − δ2 we haveln(as, i) − Lx((cid:8)x − xs(cid:8) + (cid:7)) ≥ 0.Proof. We would like to show thatln(as, i) − Lx((cid:8) y − ys(cid:8) + (cid:7) + d) ≤ ln(as, i) − Lx((cid:8)x − xs(cid:8) + (cid:7)).This implies that d ≥ (cid:8)x − xs(cid:8) − (cid:8) y − ys(cid:8).Accordingly,(cid:8)x − xs(cid:8) − (cid:8) y − ys(cid:8) ≤ (cid:8)x − xs − ( y − ys)(cid:8)(reverse triangle inequality)= (cid:8)εs − ε(cid:8) ≤ (cid:8)ε(cid:8) + (cid:8)εs(cid:8)≤ d (cid:2)(with probability at least 1 − δ2.)Following the lemma, we can come up with a more conservative boundary condition (with one step jump bound (cid:7)(cid:10) =(cid:7) + d) which still guarantees safety. However, the price we pay for not measuring our state perfectly is the additional probability term 1 − δ2. Lastly, here we only look at the influence of noisy state measurements on the boundary condition. Nevertheless, if the policy π uses some form of feedback, the noise also enters the dynamics. In this work, we assume that this influence is captured by our observation model, see Assumption 2.3.A.2. Optimality guaranteesIn this section, we prove Theorem 4.3 which guarantees that the safe global optimum can be found with (cid:4)-precision if it is discoverable at some iteration n ≥ 0 (see Definition 4.2). Then, we show in Lemma A.18 that for many practical applications, this discoverability condition is satisfied.18B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922A.2.1. Proof of Theorem 4.3We first define the largest region that LSE can safely explore for a given safe initialization S and then we show that we (cid:4) (S) and the can find the optimum with (cid:4)-precision within this region. To this end, we define the reachability operator Rcfully connected safe region ¯Rc(cid:4)(S) := S ∪ {a ∈ A | ∃aRc(cid:4) (S) by (adapted from [18,21])(cid:10) ∈ S such that gi(a, x0) − (cid:4) − La(cid:10)(cid:8)(cid:8)a − a(cid:10)(cid:8)(cid:8) ≥ 0,¯Rc(cid:4)(S) := limn→∞(cid:4)Rc(cid:4)(cid:5)n (S).∀i ∈ Ig},(A.12)(A.13)(cid:4) (S) contains the parameters we can safely explore if we know our constraint function with (cid:4) (S) with itself, (cid:4) (S) its closure. Next, we derive a property for the reachability operator, that we will leverage to provide optimality The reachability operator Rc(cid:4)-precision within some safe set of parameters S. Further, (Rcand ¯Rcguarantees.(cid:4) )n(S) denotes the repeated composition of RcLemma A.11. Let A ⊆ S, if ¯Rc(cid:4) ( A) \ S (cid:18)= ∅, then Rc(cid:4)(S) \ S (cid:18)= ∅.Proof. This lemma is a straightforward generalization of [18, Lem. 7.4]. Assume Rcimplies ¯Rcthe limit ¯Rcwhich leads to ¯Rc(cid:4) ( A) \ S = ∅. By definition Rc(cid:4) (S) = S. Furthermore, because A ⊆ S, we have ¯Rc(cid:4) (S) ⊇ S and therefore Rc(cid:4) ( A) ⊆ ¯Rc(cid:4) (S) = S. Iteratively applying Rc(cid:4) (S) \ S = ∅, we want to show that this (cid:4) to both the sides, we get in (cid:4) ( A) ⊆ ¯Rc(cid:4) (S) = S, (cid:4) (S) [18, Lem. 7.1]. Thus, we obtain ¯Rc(cid:4) ( A) \ S = ∅. (cid:2)In the following, we prove that our LSE convergence criterion (see Eq. (10)) guarantees that for the safe initialization S, we can explore ¯Rc(cid:4) (S) during LSE in finite time.Theorem A.12. Consider any (cid:4) > 0 and δ > 0. Let Assumptions 2.2 and 2.3 hold, βn be defined as in [18], and S ⊆ A be an initial safe seed of parameters, i.e., g(a, x0) ≥ 0 for all a ∈ S. Assume that the information gain γn grows sublinearly with n for the kernel k. Further let nbe the smallest integer such that (cdf. the convergence criterion of LSE in Eq. (10))∗max∗−1∪Mn∗−1maxi∈Ia∈Gnwn∗−1(a, i) < (cid:4) and Sn∗−1 = Sn∗ .Then we have that n∗is finite and when running LSE, the following holds with probability at least 1 − δ for all n ≥ n∗:¯Rc(cid:4)(S) ⊆ Sn,f (ˆan) ≥ max(cid:4) (S)a∈ ¯Rcf (a) − (cid:4),with ˆan = arg maxa∈Snln(a, 0).Proof. We first leverage the result from [18, Thm. 4.1] which provides the following worst-case bound on n∗nβn∗γ|I|n∗C1≥(cid:4)(cid:5)¯Rc0(S)(cid:4)2+ 1,∗(A.14)(A.15)(A.16)(A.17)∗is the smallest integer that satisfies Eq. (A.17). Hence, we have that nwhere C1 = 8/ log(1 + σ −2) and nis finite. The sublinear growth of γn with n is satisfied for many practical kernels, like the ones we consider in this work [31]. Next, we prove Eq. (A.15). For the sake of contradiction, assume ¯Rc(cid:4) (Sn∗ ) \ Sn∗ (cid:18)= ∅ (Lemma A.11). Therefore, there exists some a ∈ A \ Sn∗ such that for some a(cid:8)(cid:8)a − a(cid:8)(cid:8) .(cid:10)(cid:4) (S) \ Sn∗ (cid:18)= ∅. This implies, Rc(cid:10) ∈ Sn∗ = Sn∗−1 (Eq. (A.14)), we have for all i ∈ Ig, x0) − (cid:4) − La(cid:8)(cid:8)a − a, i) − La≤ un∗−1(a0 ≤ gi(a(cid:8)(cid:8) ,∗(cid:10)(cid:10)(cid:10)Therefore, a(cid:10) ∈ Gn∗−1 (see [18] or Appendix D Definition D.1) and accordingly, wn∗−1(awn∗−1(a0 ≤ gi(a(cid:10), i) < (cid:4), we have for all i ∈ Ig(cid:8)(cid:8) ≤ ln∗−1(aThis means a ∈ Sn∗ (Eq. (8)), which is a contradiction. Thus, we conclude that ¯Rc(cid:4) (S) ⊆ Sn.n ≥ n(Proposition A.13), we get ¯Rc, x0) − (cid:4) − La(cid:8)(cid:8)a − a(cid:8)(cid:8)a − a, i) − La(cid:8)(cid:8) .∗(cid:10)(cid:10)(cid:10)(cid:10)(cid:4) (S) ⊆ Sn∗ and because Sn∗ ⊆ Sn for all (A.18)(Lemma A.9)(cid:10), i) < (cid:4). Next, because 19B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Now we prove Eq. (A.16). Consider any n ≥ n∗. Note, wn∗−1(aAlgorithm 2 line 13). For simplicity, we denote the solution of arg maxa∈ ¯Rc(cid:10), i) < (cid:4) (see Algorithm 1 line 4 or (cid:10), i) < (cid:4), implies wn(a∗S . We have(cid:4) (S) f (a) as aun(a∗S , 0) ≥ f (a∗S )≥ f (ˆan)≥ ln(ˆan, 0)= maxa∈Snln(a, 0).(by definition of a(Lemma A.9)∗S )(Lemma A.9)(by definition of ˆan)∈ Mn (see Appendix D Definition D.2) and has uncertainty less than (cid:4), that is, ∗S ) − (cid:4). For the sake of contradiction assume,∗S is a maximizer, i.e., aTherefore, a∗S , i) < (cid:4). Now, we show that f (ˆan) ≥ f (awn(af (ˆan) < f (a∗S ) − (cid:4).∗SThen we obtain,ln(a∗S , 0) ≤ ln(ˆan, 0)≤ f (ˆan)∗S ) − (cid:4)< f (a∗S , 0) − (cid:4)≤ un(a∗S , 0) − wn(a≤ un(a∗= ln(aS , 0),∗, 0)which is a contradiction. Therefore, we have f (ˆan) ≥ f (a∗S ) − (cid:4). (cid:2)(A.19)(by definition of ˆan)(Lemma A.9)(by Eq. (A.19))(because wn(a(by definition of wn(a(Lemma A.9)∗S , 0) ≤ (cid:4))∗S , 0))fully connected safe region ¯RcTheorem A.12 states that for a given safe seed S, the convergence of LSE (Eq. (10)) implies that we have discovered its (cid:4) (S) and recovered the optimum within the region with (cid:4)-precision.Based on the previous results, we can show that if the safe global optimum is discoverable for some iteration n ≥ 0 (see Definition 4.2), then we can find an approximately optimal safe solution. However, to prove optimality, what we also require is that if a∗ ∈ Sn then a∗ ∈ Sn+1.Proposition A.13. Let the assumptions from Theorem 4.1 hold. For any n ≥ 0, the following property is satisfied for Sn.Sn ⊆ Sn+1,(A.20)Proof. The safe set provably increases during LSE [18, Lem. 7.1]. During GE, the safe set is only updated if a new safe parameter is found. The proposed update also has the non-decreasing property (see Algorithm 2, line 13). Hence, we can conclude that Sn ⊆ Sn+1. (cid:2)Proposition A.13 shows that if the safe global optimum aA is added to our safe set Sn, we will explore its largest reachable safe set ¯Rc(cid:4) ( A).∗ ∈ Sn, then a∗ ∈ Sn+1. Next, we prove that if a new safe region Lemma A.14. Consider any integer n ≥ 0. Let Sn be the safe set of parameters explored after n iterations of GoSafeOpt and let βnbe defined as in [18]. Consider A = Sn+1 \ Sn. If A (cid:18)= ∅, then there exists a finite integer ¯n > n such that ¯R(cid:4)c (Sn) ⊆ S ¯n with probability at least 1 − δ.c ( A) ∪ ¯R(cid:4)c ( A) \ S ¯n = ∅ and ¯R(cid:4)c (Sn) \ S ¯n = ∅ then ¯R(cid:4)c ( A) \ S ¯n (cid:18)= ∅. We know that A ⊆ Sn+1 ⊆ S ¯n (Proposition A.13). This implies RcProof. First, if ¯R(cid:4)c ( A) \ S ¯n = ∅. Assume that ¯R(cid:4)(cid:4) (S ¯n) \ S ¯n (cid:18)= ∅ (Lemma A.11). Since A (cid:18)= ∅, the safe set is expanding. For GoSafeOpt, this can either happen during LSE or during GE when a new parameter is successfully evaluated, i.e., the boundary condition is not triggered. In either case, we perform LSE till convergence. Let ¯n > n be the smallest integer for which we converge during LSE, i.e., for whichc (Sn) ⊆ S ¯n. We now show that ¯R(cid:4)c ( A) ∪ ¯R(cid:4)maxa∈G¯n−1∪M¯n−1maxi∈Iw ¯n−1(a, i) < (cid:4) and S ¯n−1 = S ¯n(A.21)holds. From Theorem A.12, we know that ¯n is finite. Consider a ∈ Rc(cid:8)(cid:8)a − asuch that 0 ≤ gi(au ¯n−1(a, i) − La(cid:8)(cid:8), which implies that, a(cid:8)(cid:8) (see Eq. (A.12)). Furthermore, S ¯n−1 = S ¯n, means a(cid:10) ∈ G¯n−1 (Appendix D Definition D.1) and therefore, w ¯n−1(a(cid:10) ∈ S ¯n(cid:10) ∈ S ¯n−1. Hence, we also have 0 ≤(cid:10), i) < (cid:4). This implies (cid:4) (S ¯n) \ S ¯n. Then we have that there exists a(cid:10), x0) − (cid:4) − La(cid:8)(cid:8)a − a(cid:10)(cid:10)20B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922(cid:10), i) − Lathat 0 ≤ l ¯n−1(acan proceed similarly to show that ¯R(cid:4)c ( A) ∪ ¯R(cid:4)¯R(cid:4)c (Sn) ⊆ S ¯n. (cid:2)(cid:10)(cid:8)(cid:8)a − a(cid:8)(cid:8). Therefore, according to Eq. (8), a ∈ S ¯n, which is a contradiction. Hence, ¯R(cid:4)c (Sn) \ S ¯n = ∅. Since we have ¯R(cid:4)c ( A) \ S ¯n = ∅ and ¯R(cid:4)c ( A) \ S ¯n = ∅. We c (Sn) \ S ¯n = ∅, we can conclude that In Lemma A.14 we have shown that for every set A that we add to our safe set, we will explore its fully connected safe region in finite time. This is crucial because it allows us to guarantee that when we discover a new region during GE, we explore it till convergence. Finally, we can now prove Theorem 4.3.∗Theorem 4.3. Let aa finite integer ˜n ≥ 0 such that afinite integer n∗ ≥ ˜n such that with probability at least 1 − δ,be a safe global optimum. Further, let Assumptions 2.1 – 2.5 hold, βn be defined as in [18]. Assume there exists is discoverable at iteration ˜n (see Definition 4.2). Then, for any (cid:4) > 0, and δ ∈ (0, 1), there exists a ∗∗f (ˆan) ≥ f (awith ˆan = arg maxa∈Sn ln(a, 0).) − (cid:4), ∀n ≥ n∗(13)∗Proof. Since, a∗) ⊆ ¯Rc¯Rc(cid:4) ( Aregion with (cid:4) precision in finite time n(cid:4) (S ¯n) (Lemma A.14), therefore, a∗). Furthermore, we have (cid:4) (S ¯n). Theorem A.12 shows that we can find the optimum in the safe such that∗ ⊆ S ˜n such that a∗ ≥ ˜n. Hence, there exists a finite integer nis discoverable at iteration ˜n, there exists a set A∗ ∈ ¯Rc(cid:4) ( A∗∗ ∈ ¯Rc∗f (ˆan) ≥ f (a) − (cid:4), ∀n ≥ nwith ˆan = arg maxa∈Sn ln(a, 0). (cid:2)∗,(A.22)A.2.2. Requirements for discovering safe sets with GE∗In the previous section, we showed that if a safe global optimum ais discoverable at some iteration ˜n, we can then find it with (cid:4)-precision. In this section, we show that if for a parameter aGE in A \ Sn, we have backup policies for all the states in its trajectory, then aGE will be eventually added to our safe set of parameters. Finally, we conclude this section by showing that for many practical cases, afulfills the discoverability condition.∗Now, we derive conditions that allow us to explore new regions/parameters during GE. To this end, we start by defining n , i.e., the states for which our boundary condition does not trigger a backup policy.a set of safe states X sDefinition A.15. The set of safe states X sn is defined as:=X sn(cid:14)(cid:7)(a(cid:10),x(cid:10))∈Bnx ∈ X(cid:15)(cid:15)(cid:15)(cid:8)(cid:8)x(cid:8)(cid:8) ≤ 1(cid:10) − xLxmini∈Ig(cid:16)(cid:10)ln(a, i) − (cid:7),.(A.23)Intuitively, if a trajectory induced by a parameter being evaluated during GE lies in X snot be triggered for this parameter. Now we will prove that this set of safe states X sproperty because it tells us that GoSafeOpt continues to learn backup policies for more and more states.n , then the boundary condition will n is non-decreasing. This is an important Lemma A.16. Let the assumptions from Theorem 4.1 hold. For any n ≥ 0, the following property is satisfied for X sn .X sn⊆ X sn+1.(A.24)Proof. The lower bounds ln(a, i) are non-decreasing for all i ∈ I by definition (see Algorithm 1 line 4 or Algorithm 2line 13). Additionally, because we continue to add new rollouts to our set of backups, we have Bn ⊆ Bn+1 (see Algorithm 1n , there exists (as, xs) ∈ Bn, such that ln(as, i) − Lx ((cid:8)x − xs(cid:8) + (cid:7)) ≥ 0 for all line 3 or Algorithm 2 line 12). For each x ∈ X si ∈ Ig . Because Bn ⊆ Bn+1 and ln+1(as, i) ≥ ln(as, i), x ∈ X sn+1. (cid:2)Next, we state conditions under which a parameter aGE ∈ A \ Sn will be discovered during GE, i.e., no backup policy would be triggered during GE, in finite time.Lemma A.17. Consider any n ≥ 0. Let Sn be the safe set of parameters explored after n iterations of GoSafeOpt and aGE a parameter in A \ Sn. Further, let the assumptions from Theorem 4.3 hold and βn be defined as in [18]. If, for all k ≥ 0, xaGE (k) ∈ X sn , where, xaGE (k)represents the state at time step k for the system starting at x0 with policy π aGE (·), then there exists a finite integer ˜n > n, such that aGE ∈ S ˜n.21B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922(cid:10)Proof. Assume that there exists no finite integer ˜n > n such that aGE ∈ S ˜n. This would imply that aGE ∈ A \ S ˜n for all ˜n > n. Thus, because aGE will never be a part of our safe set, it will never be evaluated during LSE. However, from Theorem A.12we know that LSE will converge in a finite number of iterations after which we will perform GE. Since aGE is not a part of the safe set, it can only be evaluated during GE, where parameters outside of the safe regions are queried. The parameter space A is finite and any parameter that was evaluated unsuccessfully, i.e., boundary condition was triggered, will be added to E and therefore not evaluated again (see Eq. (11) and Algorithm 2 line 9). This implies that aGE will be evaluated for some nn(cid:10) (Lemma A.16) and, therefore, for all with n < nk ≥ 0, xaGE (k) ∈ X sn(cid:10) . If when aGE is evaluated, the experiment is unsuccessful, i.e., we were to trigger a backup policy, this would imply that for some k(cid:8)(cid:8)xaGE (kln(cid:10) (a, i) ≥ Lx((cid:10)) /∈ X sThus, we had xaGE (kcontradiction. Consequently, we have aGE ∈ S ˜n after a finite number of iterations ˜n. (cid:2)) − xsn(cid:10) , which contradicts our assumption. Therefore, aGE ∈ Sn(cid:10)+1 ⊆ S ˜n (Proposition A.13), which is a (cid:10) < ˜n (since Sn(cid:10) ⊆ S ˜n, see Proposition A.13). Furthermore, X sand i ∈ Ig , there is no (as, xs) ∈ Bn(cid:10) such that(cid:8)(cid:8) + (cid:7)).⊆ X sn(cid:10)(cid:10)Lemma A.17 guarantees that, if the trajectory of parameter aGE lies in X sset, either during LSE or during GE. We utilize this result to provide a condition for the safe global optimum adiscoverable at iteration ˜n by GoSafeOpt.n , then it will eventually be added to our safe to be ∗Lemma A.18. Consider any n ≥ 0. Let Assumptions 2.1 – 2.4 hold, βn be defined as in [18], and let aIf there exists aGE ∈ A \ Sn such that∗ ∈ A be the safe global optimum. (i) for all k ≥ 0 and some n ≥ 0, xaGE (k) ∈ X sn , where xaGE (k) is the state visited by the system starting at x0 under the policy π aGE (·)at time step k, and∗ ∈ ¯Rc(cid:4) ({aGE}),(ii) athen a∗is discoverable at iteration ˜n ≥ n by GoSafeOpt with probability at least 1 − δ.Proof. Since, for all k ≥ 0, xaGE (k) ∈ X sbecause a(cid:4) ({aGE}), we can conclude that a∗ ∈ ¯Rc∗n , there exists a finite integer ˜n > n such that aGE ∈ S ˜n (Lemma A.17). Furthermore, is discoverable at iteration ˜n (see Definition 4.2). (cid:2)The condition here is interesting because empirically, for many practical cases, it is fulfilled. Crucially, optimal or near-optimal parameters tend to visit similar states as other safe policies. We add rollouts from safe policies to our set of backups Bn and therefore have backup policies for their trajectories and other trajectories that lie close to them. Therefore, the trajectories of (near-)optimal parameters lie in X sn and in this case, the safe global optimum fulfills the discoverability condition from Theorem 4.3.Appendix B. Additional information on experimentsFor all our experiments in Section 5, we consider a controller in the operational space. The operational space dynamics of the end-effector are given by [36]u(x(t)) = (cid:19)(q)¨s + (cid:20)(q, ˙q)˙s + η(q),(B.1)where s represents the end-effector position, q the joint angles, and (cid:19)(q), (cid:20)(q, ˙q), η(q) are nonlinearities representing the mass, Coriolis, and gravity terms, respectively. The state we consider is x(t) = [sT (t), ˙sT (t)]T . We apply an impedance controller:u (x(t)) = −K (x − xdes(k)) + (cid:20)(q, ˙q)˙s + η(q),(B.2)with K being the feedback gain. The torque τ applied to each of the joints can be calculated via τ = J T u(x(t)), with J the Jacobian.For our experiments, we can directly measure g(a, x(k)), where k denotes a discrete time step. Therefore, instead of using ln(as, i) for the boundary condition in Section 4.3.2, we take a lower bound over all the tuples in our set of backups, Bn, i.e., ln(as, xs, i), which could potentially reduce the conservatism of the boundary condition. Therefore, we define a GP over the parameter and state space, which contains all the points from Bn. The set Bn consists of rollouts from individual experiments, we typically add 50 − 100 data points from each experiment to Bn. As the data points of our GP increase, inference becomes prohibitively costly. To this end, we use a subset selection scheme to select a small subset of points (a, x) from Bn at random with a probability that is proportional to exp (− mini∈Ig l2n(a, x, i)). Crucially, we want to retain points that have a small lower bound such that we have low uncertainty around these points. We perform this subset selection once our GP has acquired more than nmax data points. Then, we select a subset of m < nmax points. Lastly, as 22B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922described in Section 5.2.1, for the boundary condition from Section 4.3.2, we define the distances du , dl using covariances κu , κl, respectively. Particularly, we pick du such that k(du) ≥ κu for the stationary isotropic kernel k that we use to model our GP (same for dl). This makes the choice of du more intuitive since it directly relates to the covariance function of our GP.B.1. SimulationFor the simulation task, we determine the impedance K using an infinite horizon LQR parameterized via(cid:17)(cid:18)Q r0Q =0κd Q rR = 10r−2 I3, A =, Q r = 10qc I3,(cid:18)(cid:17)(cid:17)0 I300, B =(cid:18).0I3The matrices A, B are obtained assuming that we use a feedback linearization controller [36]. However, because we instead use an impedance controller, there are nonlinearities and imprecisions in our model. The parameters qc , r, κd are tuning parameters we would like to optimize. We define the desired path xdes(k) asxdes(k) = [p T0+ kTtrajxdes(k) = xdes(ρ(k))(pdes − s0)T , 01x3]THere, ρ(·) is used to parameterize a cubic spline from x0 to xtarget. The constraint is:(cid:8)s(t) − sdes(cid:8)2− (cid:8)s(0) − sdes(cid:8)2¯g (x(t)) =(cid:4)(cid:5)x(t)¯g= ζ −(cid:8)(cid:8)x(t) − xd(cid:8)s(0) − sdes(cid:8)(cid:5)(cid:8)2(cid:4)(cid:8)2 .ρ(t)− α, α = 0.08The stage rewards, i.e., rewards received at each time step [1], are2 / (cid:8)s(0) − sdes(cid:8)22R (x(t)) = − (cid:8)s(t) − sdes(cid:8)2− 125(cid:8)tanh ˙s(t)(cid:8)22(cid:4)(cid:5)x(t)R= −νρ (ρ(t) − 1)2 − νx− 125(cid:8)(cid:8)x(t) − xd(cid:8)tanh u (x(t))(cid:8)22(cid:5)(cid:8)(cid:4)(cid:8)2ρ(t)− νu(cid:8)(cid:8)(cid:8)(cid:8)uumax(cid:8)(cid:8)(cid:8)(cid:8).2(8D task)(11D task)(8D task)(11D task)(8D task)(11D task)Additionally, to encourage fast behavior in the eight-dimensional task, we only sample parameters for which the eigen-values of A − B K are less than a fixed threshold: eig( A − B K ) ≤ −10. Although this constraint is independent of the state, it can be evaluated before each experiment and parameters can be rejected if the criterion is not fulfilled. The value for κd is heuristically set to 0.1 for the first experiment (8D task). For the eleven-dimensional task, κd is also tuned. In the eight-dimensional task, we observe that the underlying functions f , gi exhibit non-smooth behavior. Therefore we use the Matérn kernel [30] with parameter ν = 3/2 for our GP. For the remaining tasks, we use the squared exponential (SE) kernel.B.2. HardwareFor the hardware task, we define the subsequent objective and constraint functions:R (x(t)) = − (cid:8)s(t) − sdes(t)(cid:8)¯g (s(t)) = (cid:8)s(t) − sw (cid:8)2 ,P ,∞ − ψ,where sw represents the center of the wall in Fig. 4 and (cid:8)s − sw (cid:8)the wall and ψ > dw .Appendix C. Sensitivity to LSE and GE stepsP ,∞ ≤ dw defines the rectangular shaped outline around We analyze the sensitivity of the practical version of GoSafeOpt with respect to nLSE and nGE on a simple one-dimensional toy example. The toy example consists of a one-dimensional system that has the following dynamics, stage reward, and constraint:(cid:11)(cid:11)|s(k)| − 0.2|ay(k)| + v(k), with y(k) = s(k) + w(k)s(k + 1) = 1.01R(s) = −s2,¯g(s) = s2 − 0.81,23B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922Fig. C.11. Toy Example results. (a) Objective function with the safe set. (b) Performance of GoSafeOpt for different nLSE and nGE.−4) and a ∈ [−6, 5] the control parameter we would like to optimize. We consider a regulation with v(k), w(k) ∼ N (0, 10problem, i.e. we start at x0 = 0 and we would like our system to remain close to x0. We run GoSafeOpt for twenty iterations over twenty seeds for different nLSE and nGE values. Fig. C.11 depicts the safe set and the performance of GoSafeOpt∗ = −6 is the global optimum. To for different nLSE and nGE. In this example, we have two disconnected safe sets and ashow the advantage of global exploration, we initialize GoSafeOpt in the safe region which does not contain a. For all our experiments, we obtain 100% safety. Furthermore our results show, for nLSE = 5, we get faster convergence than for nLSE = 10. This is because we explore the region with the global optimum earlier. However, when we do not perform enough LSE steps then global exploration also fails and we are stuck at a bad optimum (see for instance nLSE = 1 and nGE = 10). This simple example highlights the trade-off between local exploration and global exploration steps.∗Appendix D. Additional definitionsIn this section, we present some of the definitions from SafeOpt for completeness.Definition D.1. The expanders Gn are defined as Gn := {a ∈ Sn | en(a) > 0} with en(a) = |{aLa(cid:8)(cid:8) ≥ 0}|.(cid:8)(cid:8)a − a(cid:10)(cid:10) ∈ A \ Sn, ∃i ∈ Ig : un(a, i) −Definition D.2. The maximizers Mn are defined as Mn := {a ∈ Sn | un(a, 0) ≥ maxa(cid:10)∈Sn ln(a(cid:10), 0)}.Appendix E. HyperparametersThe hyperparameters of our simulated and real-world experiments are provided in Table E.2.Table E.2Table of hyperparameters.8D task Simulation11D task SimulationHardware taskSafeOptGoSafeOptSafeOptSwarmGoSafeOptSafeOptSwarmGoSafeOptIterations1/2βna lengthscaleκ for f and gσ for f and gx lengthscale(cid:4)max LSE stepsmax GE stepsκlκuηlηunmaxm20040.12,0.121, 10.1,0.1----------20040.12,0.121, 10.1,0.10.3,0.3,0.3, 2.5,2.5,2.50.130100.900.940.40.6100050020020030.067,0.2,0.13,0.21, 10.1,0.10.5,0.5,0.5, 0.6,0.6,0.6, 100.1100100.900.940.30.75100050030.067,0.2,0.13,0.21, 10.1,0.1----------245030.1,0.1, 0.11, 10.05,0.3----------5030.1,0.1, 0.11, 10.05,0.30.3,0.3,0.3, 0.5,0.5,0.20.012050.900.940.91.11000500B. Sukhija, M. Turchetta, D. Lindner et al.Artificial Intelligence 320 (2023) 103922References[1] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, 2nd edition, The MIT Press, 2018.[2] S. Levine, C. Finn, T. Darrell, P. Abbeel, End-to-end training of deep visuomotor policies, J. Mach. Learn. Res. 17 (1) (2016) 1334–1373.[3] J. Peters, S. Schaal, Reinforcement learning of motor skills with policy gradients, Neural Netw. 21 (4) (2008) 682–697.[4] T.P. Lillicrap, J.J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, D. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint, arXiv:1509 .02971, 2015.[5] J. Kober, J.A. Bagnell, J. Peters, Reinforcement learning in robotics: a survey, Int. J. Robot. Res. 32 (11) (2013) 1238–1274.[6] S. Schaal, C.G. Atkeson, Learning control in robotics, IEEE Robot. Autom. Mag. 17 (2) (2010) 20–29.[7] J. Mockus, V. Tiesis, A. Zilinskas, The application of Bayesian methods for seeking the extremum, Towards Global Optim. 2 (117–129) (1978) 2.[8] R. Calandra, A. Seyfarth, J. Peters, M. Deisenroth, Bayesian optimization for learning gaits under uncertainty, Ann. Math. Artif. Intell. 76 (2016) 5–23.[9] A. Marco, P. Hennig, J. Bohg, S. Schaal, S. Trimpe, Automatic LQR tuning based on Gaussian process global optimization, in: IEEE International Conference on Robotics and Automation, 2016, pp. 270–277.[10] R. Antonova, A. Rai, C.G. Atkeson, Deep kernels for optimizing locomotion controllers, in: Conference on Robot Learning, 2017, pp. 47–56.[11] M. Turchetta, A. Krause, S. Trimpe, Robust model-free reinforcement learning with multi-objective Bayesian optimization, in: IEEE International Con-ference on Robotics and Automation, 2020, pp. 10702–10708.[12] M. Gelbart, J. Snoek, R. Adams, Bayesian optimization with unknown constraints, in: Conference on Uncertainty in Artificial Intelligence, 2014, [13] J.M. Hernández-Lobato, M.A. Gelbart, R.P. Adams, M.W. Hoffman, Z. Ghahramani, A general framework for constrained Bayesian optimization using information-based search, J. Mach. Learn. Res. 17 (1) (2016) 5549–5601.[14] A. Marco, D. Baumann, M. Khadiv, P. Hennig, L. Righetti, S. Trimpe, Robot learning with crash constraints, IEEE Robot. Autom. Lett. 6 (2) (2021) pp. 250–259.1439–1446.[15] S. Heim, A. Rohr, S. Trimpe, A. Badri-Spröwitz, A learnable safety measure, in: Conference on Robot Learning, 2020, pp. 627–639.[16] Y. Sui, A. Gotovos, J. Burdick, A. Krause, Safe exploration for optimization with Gaussian processes, in: International Conference on Machine Learning, 2015, pp. 997–1005.Robotics and Automation, 2016, pp. 491–496.(2021).arXiv:2101.07825, 2021.[17] F. Berkenkamp, A.P. Schoellig, A. Krause, Safe controller optimization for quadrotors with Gaussian processes, in: IEEE International Conference on [18] F. Berkenkamp, A. Krause, A.P. Schoellig, Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics, Mach. Learn. [19] C. König, M. Turchetta, J. Lygeros, A. Rupenyan, A. Krause, Safe and efficient model-free adaptive control via Bayesian optimization, arXiv preprint, [20] E.N. Gryazina, B.T. Polyak, Stability regions in the parameter space: D-decomposition revisited, Automatica 42 (1) (2006) 13–26.[21] D. Baumann, A. Marco, M. Turchetta, S. Trimpe, GoSafe: globally optimal safe robot learning, in: IEEE International Conference on Robotics and Au-tomation, 2021, pp. 4452–4458. Proofs in extended online version: arXiv:2105 .13281.[22] J. Kirschner, M. Mutny, N. Hiller, R. Ischebeck, A. Krause, Adaptive and safe Bayesian optimization in high dimensions via one-dimensional subspaces, in: International Conference on Machine Learning, 2019, pp. 3429–3438.[23] Y. Sui, V. Zhuang, J. Burdick, Y. Yue, Stagewise safe Bayesian optimization with Gaussian processes, in: International Conference on Machine Learning, [24] K.P. Wabersich, M.N. Zeilinger, A predictive safety filter for learning-based control of constrained nonlinear dynamical systems, Automatica 129 (2021) 2018, pp. 4781–4789.109597.[25] P. Wieland, F. Allgöwer, Constructive safety using control barrier functions, IFAC Proc. Vol. 40 (12) (2007) 462–467.[26] R. Cheng, G. Orosz, R.M. Murray, J.W. Burdick, End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks, in: AAAI Conference on Artificial Intelligence, 2019, pp. 3387–3395.[27] B. Schölkopf, A.J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, MIT Press, Cambridge, MA, USA, 2001.[28] C. Fiedler, C.W. Scherer, S. Trimpe, Practical and rigorous uncertainty bounds for Gaussian process regression, AAAI Conf. Artif. Intell. 35 (8) (2021) 7439–7447.[29] M.L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, John Wiley & Sons, 2014.[30] C.E. Rasmussen, C.K.I. Williams, Gaussian Processes for Machine Learning, Adaptive Computation and Machine Learning, The MIT Press, 2005.[31] N. Srinivas, A. Krause, S.M. Kakade, M.W. Seeger, Information-theoretic regret bounds for Gaussian process optimization in the bandit setting, IEEE Trans. Inf. Theory 58 (5) (2012) 3250–3265.[32] S.R. Chowdhury, A. Gopalan, On kernelized multi-armed bandits, in: International Conference on Machine Learning, 2017, pp. 844–853.[33] T.M. Cover, J.A. Thomas, Elements of Information Theory, Wiley Series in Telecommunications and Signal Processing, Wiley-Interscience, USA, 2006.[34] A. Krause, C. Ong, Contextual Gaussian process bandit optimization, in: Advances in Neural Information Processing Systems, 2011.[35] R.R. Duivenvoorden, F. Berkenkamp, N. Carion, A. Krause, A.P. Schoellig, Constrained Bayesian optimization with particle swarms for safe adaptive controller tuning, in: 20th IFAC World Congress, IFAC-PapersOnLine 50 (1) (2017) 11800–11807.[36] B. Siciliano, O. Khatib, Springer Handbook of Robotics, 2nd edition, Springer Publishing Company, Incorporated, 2016.[37] E. Todorov, T. Erez, Y. Tassa, Mujoco: a physics engine for model-based control, in: IEEE/RSJ International Conference on Intelligent Robots and Systems, [38] D.P. Bertsekas, Dynamic Programming and Optimal Control, 2nd edition, Athena Scientific, 2000.[39] A. Wischnewski, J. Betz, B. Lohmann, A model-free algorithm to safely approach the handling limit of an autonomous racecar, in: IEEE International Conference on Connected Vehicles and Expo, 2019, pp. 1–6.[40] M. Fiducioso, S. Curi, B. Schumacher, M. Gwerder, A. Krause, Safe contextual Bayesian optimization for sustainable room temperature PID control tuning, in: International Joint Conference on Artificial Intelligence, 2019, pp. 5850–5856.[41] C. König, M. Turchetta, J. Lygeros, A. Rupenyan, A. Krause, Safe and efficient model-free adaptive control via Bayesian optimization, in: IEEE International Conference on Robotics and Automation, 2021, pp. 9782–9788.[42] S.E. Cooper, T.I. Netoff, Multidimensional Bayesian estimation for deep brain stimulation using the safeopt algorithm, medRxiv (2022).[43] J. Rothfuss, C. Koenig, A. Rupenyan, A. Krause, Meta-learning priors for safe Bayesian optimization, arXiv preprint, arXiv:2210 .00762, 2022.[44] F. Berkenkamp, A.P. Schoellig, A. Krause, No-regret Bayesian optimization with unknown hyperparameters, J. Mach. Learn. Res. (2019) 1–24.[45] A. Schperberg, S.D. Cairano, M. Menner, Auto-tuning of controller and online trajectory planner for legged robots, IEEE Robot. Autom. Lett. (2022).2012, pp. 5026–5033.25