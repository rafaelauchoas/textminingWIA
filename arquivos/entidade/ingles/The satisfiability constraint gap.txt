Artificial Intelligence 81 ( 1996) 59-80 Artificial Intelligence The satisfiability constraint gap Ian l? Gent a,*, Toby Walsh b*c*l a Department of AI, Universiv of Edinburgh, 80 South Bridge, Edinburgh, UK h Mechanized Reasoning Group, IRSZ Lx. Pant6 di Povo, Trento, Italy c DISZ University of Genoa, Genoa, Italy Received May 1994; revised March 1995 Abstract We describe an experimental investigation of the satisfiability phase transition for several dif- ferent classes of randomly generated problems. We show that the “conventional” picture of easy- hard-easy problem difficulty is inadequate. In particular, there is a region of very variable problem difficulty where problems are typically underconstrained and satisfiable. Within this region, prob- lems can be orders of magnitude harder than problems in the middle of the satisfiability phase transition. These extraordinarily hard problems appear to be associated with a “constraint gap”. That is, a region where search is a maximum as the amount of constraint propagation is a min- imum. We show that the position and shape of this constraint gap change little with problem size. Unlike hard problems in the middle of the satisfiability phase transition, hard problems in the variable region are not critically constrained between satisfiability and unsatisfiability. Indeed, hard problems in the variable region often contain a small and unique minimal unsatisfiable subset or reduce at an early stage in search to a hard unsatisfiable subproblem with a small and unique minimal unsatisfiable subset. The difficulty in solving such problems is thus in identifying the minimal unsatisfiable subset from the many irrelevant clauses. The existence of a constraint gap greatly hinders our ability to find such minimal unsatisfiable subsets. However, it remains open whether these problems remain hard for more intelligent backtracking procedures. We conjecture that these results will generalize both to other SAT problem classes, and to the phase transitions of other NP-hard problems. Keywords: Search phase transitions; Satisfiability; Constraint propagation; Hard problems *Current address: Department of Computer Science, University of Strathclyde, Glasgow Gl IXH, UK. E-mail: ipg@cs.strath.ac.uk. ’ E-mail: toby@irst.it. 0004-3702/96/$15.00 @ 1996 Elsevier Science B.V. All rights reserved SSDIOOO4-3702(95)00047-X 60 1.P Gent, lY Walsh/Artijcial Intelligence 81 (19%) 59-80 1. Introduction Many randomly generated NP-hard problems display a phase transition as some pa- rameter is varied, and as the problems go from being almost always soluble to being [ 31. This phase transition is often associated with problems almost always insoluble which are typically hard to solve. In this paper, we show that with several different classes of satisfiability problems including random 3-SAT, the phase transition is indeed associated with problems which are typically hard but there are also regions of very vari- able problem difficulty in which problems are usually easy but sometimes extraordinarily hard. We identify the cause of this behaviour and show that it does not disappear with better algorithms or heuristics. We predict that similar regions of very variable problem difficulty will be found with many other NP-hard problems besides satisfiability. The extraordinarily hard problems found in these regions may be of use in analysing and comparing the performance of algorithms for NP-hard problems. 2. Satisfiability Propositional satisfiability (or SAT) is the problem of deciding if there is an assign- ment of truth values for the variables in a propositional formula that makes the formula true using the standard interpretation for logical connectives. We will consider SAT problems in conjunctive normal form (CNF) ; a formula, 2, in CNF is a conjunction of clauses, where a clause is a disjunction of literals, and a literal is a negated or un-negated variable. A standard procedure for determining satisfiability is due to Davis, Putnam, Logemann, and Loveland [ 5,6]. We call this the “Davis-Putnam procedure”. * See Fig. 1. procedure DP( 2) if .Z is empty then return satisfiable if 2 contains an empty clause then return unsatisfiable (Tautology) if 2 contains a tautologous clause c then return DP( 2 - c) (Unit propagation) if 2 contains a unit clause (1) then return DP( 2 simplified by assigning 2 to True) (Pure literal deletion) if 2 contains a literal I but not the negation of 1 then return DP( 2 simplified by assigning 1 to True) (Split) if DP( 2 simplified by assigning a literal 1 to True) is satisfiable then returu satisfiable else return DP( 2 simplified by assigning the negation of 1 to True) Fig. 1. The Davis-Putnam procedure. 2 We follow recent nomenclature [ 8,141. Davis and F’utnam [ 61 introduced the unit and pure rules, while it was Davis, Logemann, and Loveland [5] who introduced the split rule and the use of backtracking. The latter authors modestly presented the difference as merely one of implementation. in this, for example 1.P. Gent, T Walsh/Artificial Intelligence 81 (19%) 59-80 61 An empty clause contains no literals, a unit clause contains just a single literal, and a tautologous clause contains both a literal and its negation. To simplify a set of clauses by the assignment of the literal I to True, we delete every clause that contains 1 and delete the negation of 1 whenever it occurs in the remaining clauses. Note that the Davis-Putnam procedure is non-deterministic since the literal used by the split rule is unspecified. As in previous studies (e.g. [ 8,14]), we will split upon the first literal in the first clause. We call this variant of the Davis-Putnam procedure “DP”. With good heuristics for choosing the literal to split on, an efficient implementation of the Davis-Putnam procedure is still the best complete procedure for satisfiability [ 71. 3. Constant probability model The constant probability model of randomly generated problems has been the subject of considerable theoretical and experimental attention. In this model, given N variables and L clauses, each clause is generated so that it contains each of the 2N different literals with probability p. Our experiments use a variant of the constant probability model proposed in [ 111 and since used in other experimental studies [ 8,9,14]. In this problem class, if an empty or unit clause is generated, it is discarded and another clause generated in its place. This is because the inclusion of empty or unit clauses typically makes problems easier. We shall call this the “CP” model. In all our experiments, as in [ 8,9], we choose p so that 2Np = 3 and the mean clause length remains approximately constant as N varies. In [ 141, it is shown that there is a phase transition between satisfiability and unsatisfiability for CP as the ratio of clauses to variables, L/N, is varied. If 2Np is kept constant, then this phase transition occurs at L/N M 2.80 as N--too [9]. The satisfiability phase transition is of computational importance since there is an easy-hard-easy pattern in problem difficulty as we cross the phase transition with the hardest instances occurring in the phase transition [ 141. When the ratio of clauses to variables is large, problems are usually overconstrained, and thus easily shown to be unsatisfiable. When the ratio is small, problems are usually underconstrained, and a satisfying assignment can be “guessed” quickly. The hard instances tend to occur in the phase transition where the problems are neither overconstrained nor underconstrained. In [ 81, we showed that whilst median problem difficulty has a simple easy-hard- easy pattern, there is also a region of very variable and sometimes exceptionally hard problem difficulty at a high-percentage satisfiability. The worst-case problems in this region can be orders of magnitude harder than those in the middle of the satisfiability phase transition. These extraordinary problems can easily dominate the mean problem difficulty. Similar behaviour has been observed by Hogg and Williams for randomly generated 3-colourability problems [ lo]. Fig. 2(a) gives the mean and median number of branches used by DP for 1000 problems from the CP model at N = 100 with L/N from 0.1 to 6.0 in intervals of 0.1. The number of branches is the number of leaf nodes in the search tree. It pro- vides a good indication of problem difficulty and run time. The dotted line indicates the observed probability that problems were satisfiable. There is a very considerable differ- 62 I.F! Gent, 1: Walsh/Artificial Intelligence 81 (1996) 59-80 i.. 0.6 i.. 0.2 i.. 0 4 5 6 L/N 2 Pmb(sat) :- , i.. 0.8 i__ 0.6 ;_. 0.4 :.. 0.2 ;.. 0 5 L/N (b) ;hO,OOO p:oblems Fig. 2. CP problems tested using DP, N = 100. Mean and median branches. ence between mean and median performance. The worst-case mean of 62.5 branches occurs at L/N = 2.4 in a mostly satisfiable region, whilst the worst-case median of just 9 branches occurs at L/N = 3.9 in the middle of the satisfiability phase transi- tion. Despite testing 1000 problems at each point, there is a large amount of noise in the mean, especially in the region of L/N from about 2 to 4, and despite the use of a logarithmic scale. In Fig. 2(b), we therefore tested 100,000 problems per point. Due to the large cost of testing this number of problems, we restricted our attention to the region L/N = 1.5 to 5.0. Although the noise is reduced considerably by taking 100,000 problems, there is still a large difference between mean and median perfor- mance. The greatest difference is in the region that was previously noisy and where median performance is only 1 or 2 branches. There is a secondary peak in mean prob- lem difficulty of 24.3 branches at L/N = 3.0. The worst-case mean of 35.0 branches occurs at L/N = 3.6, close to the worst-case median of 9 branches in the middle of the satisfiability phase transition. The variable behaviour in the satisfiable region increases rapidly with N and eventually dominates the mean [ 81. The worst mean performance at large N therefore occurs in the satisfiable region and not in the middle of the phase transition. To explore this phenomenon further, in Fig. 3 (a) we give a breakdown in percentiles for the number of branches used from 50% (median) up to 100% (worst case) in the experiment described above in which 100,000 problems were tested at each point. The 99.9% contour, for example, gives the difficulty of the problem which took more branches than all but 0.1% of problems: in this case, this would be the hundredth hardest problem at each point. Interestingly, the worst-case contour is very noisy despite the very large number of problems tested and the logarithmic scale it is plotted on. Furthermore, the different contours are widely separated, especially at smaller values of L/N, showing how, for example, the tenth worst problem can be almost an order of magnitude harder than the hundredth worst problem. As we examine contours closer to the worst case, they peak at smaller values of L/N. In particular, the worst case was 185,902 branches at L/N = 2.9, while at L/N = 3.9, the point of worst median performance, the worst case was just 10,959 branches, more than an order of magnitude smaller. I.I? Gent, T. Walsh/Artificial Intelligence 81 (1996) 59-80 63 Pmb+at) Pmb(sat) i.. 0.6 :.. 0 2 3 4 5 L/N 0 I 2 3 4 5 6 (a) percentile branches (b) constraint propagations/splits Fig. 3. CP problems tested using DP, N = 100. In [ 81, we show that similar behaviour for CP is observed with better splitting heuristics. However, variable and difficult behaviour in the high-percentage satisfiable region is not apparent till larger N. 4. Constraint gap In the Davis-Putnam procedure, the split rule is the only rule which gives rise to exponential behaviour. The other rules simplify the problem and do not branch the search. In particular, the unit and pure rules take advantage of constraints to commit in polynomial time to particular truth assignments. Since the extremely bad worst-case performance in the mostly satisfiable region is presumably due to exponential behaviour, we therefore conjecture that both the unit and pure rules will be of less importance than the split rule in this region. In Fig. 3 (b) we plot the mean ratio of pure literal deletions to splits, of unit propa- gations to splits and of the sum of pure literal deletions and unit propagations to splits for CP at N = 100. The uppermost (solid) line shows the ratio of all constraint prop- agations to splits. Underneath this, plotted to the same scale, are the ratio of number of applications of the pure rule to splits (peaking to the left), and the ratio of number of applications of the unit rule to splits (peaking to the right). Since the split rule is merely formalized guessing, the ratio of all propagations to splits indicates the number of variable assignments that can be deduced for each guess during search. To avoid divi- sion by zero, we exclude the trivial problems which tend to occur at small L/N that are solved with no splits. Such problems can be solved in polynomial time using a simple preprocessing step which exhaustively applies the unit and pure rules. As a guide, the dotted line repeats the probability of satisfiability from Fig. 2(a). We show the effect of both unit and pure propagations individually, and of them both together. The number of pures dominates behaviour at small values of L/N, while unit propagations start to dominate at large values. The ratio of all propagations to splits shows a large peak of 73.5 at L/N = 1.3. In this region, almost all problems are trivial, being solved almost exclusively by pure literal deletion. However, with increasing L/N, the number of pure 64 1.P. Gent, 7: Walsh/Artificial Intelligence 81 (1996) 59-80 RMW 2.9 0 1 2 3 4 5 6 L/N L IN equivalent at N = l@J (a) mean min and max depth, N=lOO (b) scaling of Prob(sat), N= 25 to 250 0 1 2 3 4 5 6 Fig. 4. CP problems tested using DP literal deletions drops very rapidly, and applications of unit propagation take over. A local maximum of 33.8 propagations per split is reached at L/N = 5.2, though in this region performance is comparatively noisy. The most interesting region in Fig. 3(b) is the region where neither pure literal deletions nor unit propagations dominate behaviour, since in this region the total number of propagations shows a pronounced minimum. The minimum in the mean ratio of the sum of units and pures to splits is 9.8 and occurs at L/N = 2.5, close to the position of the hardest worst case. These graphs confirm that the unit and pure rules are not effective in the mostly satisfiable region. There appears to be a “constraint gap”; that is, there seems to be a region where the unit and pure rules are often unable to identify constraints on the truth assignments and we have to use the split rule extensively. This would suggest that the depth of search (i.e. the depth of nesting of split rule applications) would also peak in this region. In Fig. 4(a), we plot the mean minimum, and mean maximum depth of search. The peak of the minimum depth is 10.0 at L/N = 2.5 while the peak of maximum depth is 11.6 at L/N = 2.8. This coincides closely with the minimum in the ratio of the sum of units and pures to splits, and with the position of the hardest worst case. For unsatisfiable problems, a peak in minimum search depth corresponds to an exponentially larger peak in problem difficulty, as all branches must be searched to at least the minimum depth of the tree. We confirmed this by plotting the logarithm of problem difficulty for unsatisfiable problems alone. This was approximately proportional to mean minimum search depth. 5. Scaling of the constraint gap The importance of the constraint gap depends in part upon the relationship of the constraint gap to the phase transition from satisfiability to unsatisfiability. For instance, if the constraint gap occurs at or near to the satisfiability phase transition, then it is likely to prove much more costly than if the constraint gap occurs well away in a 1.P Gent, T Walsh/ArtijFcial Intelligence 81 (1996) 59-80 65 region of otherwise very easy problems. To help determine the relationship between the constraint gap and the satisfiability phase transition, we draw upon an analogy with phase transitions in physical systems. One of the most unusual and theoretically interesting phase transitions occurs in spin glasses. Each of the N atoms in a spin glass has a magnetic spin which can have only one of two values, “up” or “down” ( 1 or -1). The system therefore has 2N possible configurations. Macroscopic properties of a configuration (e.g. the energy, entropy) depend only on interactions between the spins of nearest neighbours. Due to the differences in separation of the atoms, some of these interactions are ferromagnetic (promoting alignment of spins) whilst others are anti-ferromagnetic (promoting opposite spins). The net effect is a random force. An analogy can be made between such spin glasses and randomly generated SAT problems. Each of the N variables in a truth assignment has one of two values, “True” or “False”. The system therefore has 2N possible configurations. Macroscopic property like satisfiability depend only on the interaction between variables neighbouring each other in a clause. Due to the random polarities of these variables, the net effect on a variable is a random “preference” towards True or False. Kirkpatrick et al. [ 131 have used this analogy to suggest a fascinating scaling result for random k-SAT (this randomly generated problem class contains clauses of fixed length k; it is described in Section 6). They propose that there is a fundamental function f, and values LY and v, such that Prob(sat) =f((L/N- cu)N’/“). (1) Here we show that this relation also holds for problem classes like CP which contain clauses of mixed lengths. In Fig. 4(b) we plot the probability of satisfiability for CP along the y-axis and (L/N - ~y)Nr/~ along the x-axis. For convenience, we have also multiplied the x-ordinate by loo-*/” and added LY so that the values on the x-axis give the equivalent value of L/N at N = 100. The dashed line gives the point L/N = LY. We set (Y to 2.9 as this was the experimentally observed position of the satisfiability phase transition. A value for u was found by trial and error. Using LI = 2.5, we found a very good fit with the curves for N = 25,50,75,100,150,200, and 250. These plots are never more than 0.3 apart in terms of L/N as measured at N = 100. This is a slightly less good fit than has been observed for random 3-SAT [ 131 or for random mixed SAT [9] (this problem class contains a fixed distribution of clause lengths; it is defined in Section 7). This may be because, in the CP model, the distribution of clause lengths changes slightly with problem size, or because the expected number of tautologies, which are allowed in the CP model but not in random 3-SAT or random mixed SAT, changes with problem size. The normalized ratios of units to splits and pures to splits are, like the probability of satisfiability, macroscopic properties of the satisfiability system which vary with N and L between 0 and 100%. We therefore investigated how these features of search scale as the problem size changes. Very surprisingly, the normalized ratio of units to splits seems to scale in a very simple fashion similar to that of probability of satisfiability. That is, there exists a function g, and constants cyu, v,, such that, 66 1.P. Gent, T. Walsh/Artificial Intelligence 81 (I 996) 59-80 Percentage of maximum value Percentage of maximum value lOO_ FJJ_ 3.3 I I I ~Tqw+:::_ -.. la_ 40_ 20_ 0 . . _ . . . . . %d???m ,.-/, W’Z 0 1 2 3 L/N equivslent at N = 100 4 ,* 6 5 0 1 2 3 4 5 6 L/N (a) scaling of units/splits (b) scaling of pures/splits Fig. 5. CP problems tested using DP, N = 25 to 300. units/splits max( units/splits) = g( (L/N - (Y~)N+~). ratios of units in the region of L/N for N = 25,50,75,100,150, in the same way as in Fig. 4(b). Using Q,, = 3.3 and from the graph, at very small It is particularly The normalizing maximal values are for a fixed N and varying L. In Fig. 5(a) we plot to splits for CP problems the normalized and 300. The x-axes are scaled L;,, = 2.5, we found a very good fit to Eq. (2), as can be seen clearly particularly or large values of L/N does not seem important to note that the value of CY, used here, namely 3.3, is larger than values of L/N where we earlier observed very bad worst-case behaviour, and the constraint gap. The maximum h’c’.6. We would the number of splits required expect would not increase with N. Sublinear growth in the number of branches less than linear scaling of this ratio, as otherwise to splits seems to scale as approximately scaling of this ratio suggests exponential to fit the model as closely. from approximately 2 to 4. Behaviour to be searched. ratio of units required It can be seen to be satisfiable that there is a very large peak at very low L/N, The scaling of the ratio of pures to splits is less simple than that of units. In Fig. 5(b) we show the ratios of the number of pure literal deletions to splits for the same values of N as before, normalized by the relevant maximum value on the y-axis but unscaled on the x-axis. region where most problems are proved applications of the pure rule. This peak moves slightly contrast, increasing splits seems occurs at extremely that in the constraint gap, and beyond, scaling of the number of trivial. pures to splits is indeed sublinear. For example, at L/N = 2, the ratio of pures to splits at all while increases only from 16 at N = 100 to 18 at N = 300, hardly as it was for It is harder the problem in a simply by a large number of to the right with increasing N. By the peak of ratio of pures to this peak from this peak value is very fast, and takes place with dramatically the number of variables as N increases. However, speed as N increases. Unlike unit propagations, that such problems are essentially small values of L/N, suggesting to any definite conclusion to approach size triples. the decline increasing It seems to come likely l.l? Gent, 7: Walsh/Artificial Intelligence 81 (1996) 59-80 67 unit propagations. However, it should be noted that the peak in pure applications seems to occur at significantly lower values of L/N than where we observed bad behaviour and the constraint gap. As the behaviour at low L/N is dominated by easy or trivial problems, it is possible that if these could be eliminated in a suitable way, we would observe a similar scaling to that seen with the probability of satisfiability and of unit propagations to splits. These results suggest that the constraint gap will get more pronounced with increasing N. In particular, the decline on the number of unit propagations as N increases and L/N decreases below LY, gets sharper. The behaviour of the pure rule is less clear-cut, but seems to show broadly similar behaviour. In [ 81, we showed worst-case behaviour is due to unsatisfiable problems and to unsatisfiable subproblems of satisfiable problems [ 81. As pure literal deletions only directly help to solve satisfiable problems (though at least simplifying unsatisfiable problems), we would not expect pure literal deletion to help suppress bad worst-case behaviour in the constraint gap, even if the utility of the pure literal rule did not decay. These results show that the constraint gap for CP, like the phase transition between satisfiability and unsatisfiability, occurs at a fixed value of L/N; that is, at a fixed ratio of constraints to variables. The constraint gap seems to be open between approximately L/N = 2 and 3.3. The position of the constraint gap is thus fixed relative to the position of the satisfiability phase transition. From the shape of g, the scaling of the maximal values, and the value u, it also appears that the constraint gap will become more pronounced as N increases. 6. Random k-SAT Following [ 141, many studies of the satisfiability phase transition have concentrated on the random k-SAT problem class. A problem in random k-SAT consists of L clauses, each of which has k literals chosen uniformly from the N possible variables, each literal being positive or negative with probability 3. Unlike CP, all clauses in the random k-SAT model are of the same length. In [ 141, it was shown that there is a satisfiability phase transition for random 3-SAT at L/N M 4.3, and that median behaviour displays a simple easy-hard-easy pattern through this phase transition. In [ 81 we showed that hard random 3-SAT problems can also occur in the mostly satisfiable region, and Crawford and Auton also anecdotally report this for very large problems [4]. Indeed, problems in the mostly satisfiable region can be several orders of magnitude harder than the hardest problems from the middle of the satisfiability phase transition (the point of worst median performance). These extraordinarily hard problem appear to be rarer in random 3-SAT than in CP. Although we found such hard problems in a sample of 1000 for CP, it required 100,000 for random 3-SAT. As with CP, these hard random 3-SAT problems occur in the region of a constraint gap, a minimum in the ratio of constraint propagations to splits. Again, as with CP, the constraint gap and the probability of satisfiability scale as in Eqs. (1) and (2). In Fig. 6(a) and (b) we plot the probability of satisfiability and the normalized ratios of units to splits for random 3-SAT problems for N = 10 to 70 in steps of 10. Using 68 1.R Gent, 7: Walsh/Artij?cial Intelligence 81 (1996) 59-80 Prob(sat) Perce.ntaee of maximum value 3.0 3.5 4.0 4.5 5.0 5.5 6.0 0 1 L / N equivalent at N = 50 (a) scaling of Prob(sat) 2 3 L / N equivalent at N = 50 4 5 6 (b) scaling of units/splits Fig. 6. Random 3-SAT problems tested using DP, N = 10 to 70. (Y = 4.15 and u = 1.5 (following [ 13]), cq, = 3.3 and v,, = 2.85, we found a very good fit to Eqs. ( 1) and (2). The peak ratio of units to splits seems to vary approximately as Nc6’. Again, mean minimum depth of search peaks at a small value of L/N. For example, at N = 50, the maximum value was 11.1 at L/N = 2.6. As with CP, we did not obtain a similar scaling result for the ratio of pures to splits. However, the region of many pure literal deletions again decays at a smaller value of L/N than the region of many unit propagations. To conclude, hard problems can also occur with random 3-SAT in the mostly satisfi- able region. These hard problems again appear to be associated with a constraint gap. This constraint gap occurs at a fixed value of L/N, and becomes more pronounced as N increase. The position of the constraint gap is fixed relative to the position of the satisfiability phase transition. While the satisfiability phase transition for CP seemed to be within the constraint gap (see Section 5), this does not seem to be the case for 3-SAT. The probability phase transition seems to occur at approximately 4.2, while the constraint gap seems to end at approximately 3.3. This difference may account for some aspects of the differences in behaviour between CP and 3-SAT. 7. Random mixed SAT In [ 91, we introduced a generalization of the random k-SAT model, called “random mixed SAT”. In the satisfiability phase transition, problems from random mixed SAT can be much harder than comparably sized random k-SAT problems. In the random mixed SAT model, a set of clauses is generated with respect to a probability distribution 4 on the integers. Each clause is generated as in random k-SAT except that k, the length of the clause, is chosen randomly according to 4. For example, if 9( 2) = +( 3) = 1, then clauses of length 2 and 3 appear with probability i, whilst if 4(2) = f and +(4) = 3, clauses of length 2 appear with probability f and of length 4 with probability 3. In this 1.R Gent, I: Walsh/Artificial Intelligence 81 (1996) 59-80 69 i : l ;._ 0.8 ;._ 0.6 ;._ 0.4 i._ 0.2 ’ ;.. 0 Pmb@at) i” 1 i.. 0.8 i-- o/j ;__ 0.4 ;_. 0.2 0 0 1 2 3 4 5 6 L/N 0 1 2 3 4 5 6 L/N (a) percentile branches (b) constraint propagations/splits Fig. 7. Random 2,4,4-SAT problems tested using DP, N = 100. paper, we will call these problem classes “2,3-SAT” and “2,4,4-SAT” respectively. The frequency of occurrence of an integer in the name reflects the frequency of occurrence of clauses of this length in the problem. Random k-SAT is a special case of random mixed SAT in which 4(k) = 1, and 4(j) = 0 for j # k. For a given 4, we define the density of 4, d&, as 4 =def M c k=l 4(k)(l -($P). The density gives the mean fraction of all truth assignments that are consistent with any given clause generated by q%. The expected number of models of a random mixed SAT problem with N variables and L clauses is then 2N (dg) L. The random mixed SAT model may generate problems more similar to real-world problems than random k-SAT. For example, many structured problems encode into SAT problems with mixed clause lengths (e.g. scheduling problems can be encoded into SAT using large numbers of binary clauses). The random mixed SAT model also produces problems which can, as we have said, be much harder than random k-SAT. In [9], we showed that the satisfiability phase transition for random mixed SAT for a given 4 seems to occur, as with random k-SAT, at a fixed ratio of L/N. For random 2,4,4-SAT, the phase transition occurs at L/N M 2.74, and we observed a similar kind of scaling as seen in Fig. 4(b), with constants LY = 2.74 and u = 3.5. Although random 2,4,4-SAT has the same density as random 3-SAT, problem difficulty through the satisfiability phase transition of random 2,4,4-SAT is more similar to that of CP than that of random 3-SAT. Fig. 7(a) shows the percentile branches used by DP for random 2,4,4-SAT problems at N = 100, with 10,000 problems tested at each value of L/N from 0.2 to 6.0 in steps of 0.2. Median problem difficulty shows a simple easy-hard-easy pattern, whilst the hardest problems are again found in a region of high-percentage satisfiability. The worst case was 104,885 branches at L/N = 2.8, while at L/N = 3.8 the point of worst median performance, the worst case was just 7,534 branches, two orders of magnitude smaller. As with CP, these extraordinarily 70 1.R Gent. 7: Walsh/ArtQicial Intelligence 81 (1996) 59-80 hard problems appear to be associated with a constraint gap. In Fig. 7(b) we plot the mean ratio of constraint propagations to splits for random 2,4,4-SAT at N = 100. The minimum in the mean ratio of the sum of units and pures to splits is 10.1 and occurs at L/N = 2.6, close to the position of the hardest worst case. As with CP, the depth of search also peaks in this region. The peak of the minimum depth is 9.8 at L/N = 2.4 while the peak of maximum depth is 11.3 at L/N = 2.8. It seems likely that we will observe a similar scaling of the constraint gap for 2,4,4SAT as we observed above for CP. 8. k-colourability Another way of randomly generating SAT problems is to map random problems from some other NP-hard problem into SAT. For example, the k-colourability (k-COL) of random graphs can be easily mapped into SAT. Given a graph, G the k-colourability problem is to assign one of k labels to each vertex of G so that adjacent vertices carry different labels. For a graph with n vertices and e edges, our encoding of k-COL into SAT uses II . k variables. We generate random graphs to encode into SAT by choosing e edges from the n . (n - 1)/2 possible edges uniformly at random. We use ~(n, e) to denote graphs drawn from this class. In Fig. 8 (a) we plot the breakdown in percentiles for the number of branches used by DP for encodings of 3-colourability for 1000 problems taken from ~(n, e) with n = 40 and e/n = 0.5 to 4 in steps of 0.1. The worst case was 2,905,Oll branches at e/n = 1.6, while at e/n = 2.4, the point of worst median performance, the worst case was just 4,139 branches, 3 orders of magnitude smaller. As with the other random problem classes, median problem difficulty shows a simple easy-hard-easy pattern through the k-colourability phase transition. Very similar behaviour for k-colourability was observed by Hogg and Williams using two special-purpose colouring algorithms [ 101. The constraint gap seems to be an important feature in 3-COL as well as in other problem classes of satisfiability problems. However, pure literal deletions seem less important than previously. Even at e/n = 0.4 the number of pure propagations per split is only 3.8 at n = 40, compared to a peak of 34.9 unit propagations per split at e/n = 2.65. The number of unit propagations per split seems to scale similarly with n to the earlier cases of CP and 3-SAT, suggesting that once again the constraint gap is an important way of understanding search. In Fig. 8(b) we plot normalized values of the ratio of unit propagations to splits for n = 10, 20, 25, 30, 40, scaled by Eq. (2) with the values CY, = 2.4 and CJ = 3. The depth of search also peaks in the region of extraordinarily hard problems. 9. Critically constrained problems Crawford and Auton [4] have suggested that hard problems in the satisfiability phase transition are critically constrained. That is, they are neither so underconstrained that that we can we can guess one of the many models easily, nor so overconstrained 12 Gent, T Walsh/Art$icial Intelligence 81 (1996) 59-80 71 Parentage of maximum value 2_4 0 1 2 3 4 e/n e In equivalent al n = 25 (a) percentile branches, n = 40 (b) constraint propagations/splits, n = 10 to 40 0 1 2 3 4 Fig. 8. 3-COL problems from ,y(n, e) tested with DP. determine between experiments in the experiments As the most their unsatisfiability with little search. Such problems are on the knife edge To test this hypothesis, we ran a series of and unsatisfiability. satisfiability in which we added and deleted constraints from the hardest CP problems described in Section 3. seem important aspects of behaviour to arise on unsatisfiable problems only. We took behaviour on unsatisfiable problems that they become prob- the from 100,000 CP problems at each point and mea- if we delete 20 clauses at random. for in this graph is accounted the at each point. For comparison, at each point becomes if we delete 20 clauses at random, and the dotted line gives the overall prob- that in Fig. 9(a) by the dashed that any unsatisfiable features. First, the probability line. Noise it confirms problem satisfiable line gives [ 81, we first examined the probability is shown lems 100 hardest unsatisfiable sured This by the selection of a small number of problems solid satisfiable ability of satisfiability. This graph has two interesting in the satisfiability more critically of [4] importantly, straint gap, typical unsatisfiable most search cussion of these problems of search arises precisely because strained, little ity. constrained that hard problems however, in the hardest unsatisfiable than all problems. This in the phase the region of mostly problems these unsatisfiable information problems. Yet these problems to solve of all problems and hence contain transition, transition phase the hardest unsatisfiable problems are significantly the suggestion constrained. Equally and of the con- than that require at all values of L/N. We defer detailed dis- to the next section, where we show that the large amount con- is consistent with are critically satisfiable problems are significantly are precisely less constrained the ones are not critically problems to help towards proving unsatisfiabil- Satisfiable problems are hard for rather different particular, much search can only be needed satisfiable more critically satisfiable problems ones. In to unsatisfiable lead to un- in search should be that the hardest satisfiable problems at all values of L/N. To test this, we took the 100 hardest that they from 100,000 at each point, and measured the probability This suggests if reductions subproblems. constrained reasons early 12 I.P: Gent, I: Walsh/ArtQicial Intelligence 81 (1996) 59-80 Pmb(sat) Conditional Prob ;- 1 , - _ _ _... _ .___ __. -., .I. i.. 0.8 0.8_ Conditional Pmb L _ . . _ . . .._...._____ _,_. . . .._ ‘\ :., 0.8_ 0.6_ 0.4_ 0.Z 0 Pmb@q 0.8 __ 0.6 0.4 0.2 0 (a) delking X13clauses 4 (b) adding 2Rclauses5 Fig. 9. CP, 100 hardest unsatisfiable/satisfiable problems for DP from 100,000, N = 100. 5 L/N 2 become unsatisfiable if we add 20 randomly generated CP clauses. This is shown in Fig. 9(b) by the dashed line. For comparison, the solid line gives the probability that any satisfiable problem at this point (not just one of the hardest 100) becomes unsatis- fiable if we add 20 randomly generated CP clauses. For reference, the dotted line gives the overall probability of satisfiability. It can be seen that except at large L/N (where there are very few satisfiable problems and so unusual effects may be due to noise), the worst satisfiable problems are easier to make unsatisfiable than typical problems. This suggests that the hardest satisfiable problems are more critically constrained than typical satisfiable problems. The most significant feature of the data may be that the largest discrepancy between the hardest satisfiable and all satisfiable problems is between L/N = 2 and 3. This is consistent with the observation in [8] that very hard satisfiable problems in this region arise because an incorrect choice at a branching point in search leads to a hard unsatisfiable subproblem. While unsatisfiable problems in this region are hard because they are not critically constrained, satisfiable problems can only give rise to hard unsatisfiable subproblems at an early stage in search if they are critically constrained. Even so, these problems are significantly less critically constrained than satisfiable problems in the middle of the phase transition. This helps to explain why the hardest satisfiable problems can often be solved very quickly if randomly different choices are made during search, as we showed in [ 81. These graphs show that the most critically constrained problems occur, as suggested by Crawford and Auton, in the middle of the satisfiability phase transition. We can neither add nor delete many constraints from problems in this region without changing their satisfiability. By comparison, hard unsatisfiable problems in the mostly satisfiable region are not critically constrained since we can delete many constraints without making them satisfiable. The hardest satisfiable problems than to a hard and In the next section we will argue that, whilst problems in the unsatisfiable middle of the satisfiability phase transition are difficult because they are so critically in the mostly satisfiable region are difficult because they are so constrained, uncritically constrained. in this region are more critically constrained typical problems, problem. they can reduce at early point in search problems thus 1.P: Gent, 1: Walsh/Artificial Intelligence 81 (1996) 59-80 13 Pmquniquene.5.s) Pmqsat) (” 1 ;.. 0.4 ;.. 0.2 i.. 0 (a) pro2babilily’of uniqe‘Lness (b) no. of variables Fig. 10. Minimal unsatisfiable subsets for 100 hardest unsatisfiable CP problems. N = 100 5 L/N 2 3 4 S LIN 10. Minimal unsatisfiable subsets In [ 81, we showed that hard problems in the mostly satisfiable region are either hard unsatisfiable problems or are satisfiable problems that give rise to hard unsatisfiable subproblems following an incorrect split at the start of search. To explain the difficulty of problems in the mostly satisfiable region, we will therefore focus on unsatisfiable problems. Since hard unsatisfiable problems in the mostly satisfiable region are not critically constrained, we can delete many clauses from them without making them satisfiable. This suggests that hard unsatisfiable problems from the mostly satisfiable region contain a large number of irrelevant clauses. To test this hypothesis, we computed minimal unsatisfiable subsets of the hardest unsatisfiable problems. S is a minimal unsatisjiable subset of T iff S c T, S is unsatisfiable and there does not exist R with R c S and R unsatisfiable. To compute a minimal unsatisfiable subset of T, we deleted each clause of T in turn, adding it back only if deleting it makes the set of clauses satisfiable. Unfortunately, it is too computationally expensive to compute all minimal unsatisfiable subsets. We did, however, determine if the computed minimal unsatisfiable subset is unique. S is a unique minimal unsatisfiable subset of T iff S is a minimal unsatisfiable subset of T and for all sp E S, T - (4p) is satisfiable. In Fig. 10(a), at each value of L/N, we took the 100 hardest unsatisfiable CP problems for DP from 100,000 and measured the probability that they have a unique minimal unsatisfiable subset. For reference, the dotted line gives the probability of satisfiability for all problems at this point. In Fig. 10(b) we plot the average number of variables in the computed minimal unsatisfiable subset. When there is not a unique minimal unsatisfiable subset, our method of computation will tend to underestimate the average number of variables of all the minimal unsatisfiable subsets since there are more ways of computing a small minimal unsatisfiable subset than a large one. Again, for reference, the dotted line gives the probability of satisfiability for all problems at this point. A graph of the average number of clauses in the minimal unsatisfiable subsets looks very similar in shape to that of the number of variables in the minimal 14 IX Gent, T Walsh/ArtQicial Intelligence 81 (1996) 59-80 unsatisfiable subsets. The ratio of clauses to variables in the minimal unsatisfiable subsets increases slowly from 1.15 at L/N = 2.2 to 1.29 at L/N = 4.6. The observed minimal unsatisfiable subsets are typically dominated by binary clauses, especially the very small minimal unsatisfiable subsets observed at small values of L/N. As the size of minimal unsatisfiable subsets increase, the number of non-binary clauses increases accordingly, but even so the average size of clauses we observed increased only from 2.05 at L/N = 2.0 to 2.61 at L/N = 4.6. These graphs show that hard unsatisfiable problems in the mostly satisfiable region tend to have very small and unique minimal unsatisfiable subsets which mention few variables. The difficulty in solving such problems is thus one of irrelev,ancy. DP will waste most of its time splitting on irrelevant variables, Because of the constraint gap, few constraint propagations follow each split. We are thus unlikely to simplify on one of the variables in a minimal unsatisfiable subset. Only after DP has backtracked through a large number of irrelevant variable assignments, will we discover a minimal unsatisfiable subset. As the minimal unsatisfiable subsets of hard problems in the mostly satisfiable region are small and mention few variables, they have very short proofs. This suggests that most of the computation performed by DP was (logically speaking) unnecessary. It is, however, very hard to find a short proof since our heuristics and constraint propagation must identify the minimal unsatisfiable subset from the large number of irrelevant (and satisfiable) clauses. We are much more likely to find one of the many long proofs containing large amounts of unnecessary computation than one of the few short proofs containing little unnecessary computation. This explains why in [ 81 we found that hard unsatisfiable problems in the mostly satisfiable region can sometimes have short proofs but that such proofs are hard to find. Note that a search strategy optimized for short proofs (for example, breadth-first or iterative deepening search) will not ultimately help. If the length of the shortest proof of the minimal unsatisfiable subset is 1 splits, then the search space for a complete procedure like breadth first search is O(N’). This is polynomial if and only if I is bounded. Unfortunately, even though the minimal unsatisfiable subset is small in comparison to L and N, it could increase in size with N and still be hard to identify. The length of the shortest proof could therefore easily be unbounded. For example, the size of the minimal unsatisfiable subset, and E might increase as 0( fi> or O(log(N)). These results help to explain why hard problems are much rarer in the mostly satis- liable region of random 3-SAT than in the mostly satisfiable regions of CP and random 2,4,4-SAT. CP and random 2,4,4SAT problems contain large numbers of binary clauses. It is thus not too difficult to hide a small and unique minimal unsatisfiable subset within a large satisfiable CP or random 2,4,4-SAT problem. By comparison, all clauses in a random 3-SAT problem must contain three literals. The minimal unsatisfiable subsets in random 3-SAT are therefore typically larger and mention more variables. It is thus more difficult to hide a minimal unsatisfiable subset within a satisfiable random 3-SAT problem. In the mostly satisfiable region, the problem of solving a rare unsatisfiable problem is that of identifying a single minimal unsatisfiable subset. Once found it is usually very easy to solve. However, there may be few clues available to its identification. On the other hand, in the middle of the phase transition, the identification of a minimal 1.R Gent, T. Walsh/ArtQicial Intelligence 81 (1996) 59-80 15 unsatisfiable subset is not so important, because the minimal unsatisfiable subsets are comparatively large, and indeed may not in themselves be significantly easier than the problem as a whole. Furthermore, choice points in search are less likely to be wasted, because most splits contribute towards a proof of unsatisfiability, as is seen by the large number of variables in the minimal unsatisfiable subsets. In the mostly satisfiable region, bad choices can double search time, as they may make no contribution to deriving the unsatisfiability of the unique minimal unsatisfiable subset. To conclude, hard unsatisfiable problems in the mostly satisfiable region often have very small and unique minimal unsatisfiable subsets. These minimal unsatisfiable subsets are hidden within much larger random satisfiable problems. It is thus very difficult to find such minimal unsatisfiable subsets. An analogy can be made with cryptography where it is very difficult to identify a short message if it is hidden in a long stream of white noise. By comparison, hard unsatisfiable problems in the middle of the satisfiability phase transition typically have much larger minimal unsatisfiable subsets which are not unique. 11. Binary constraints The minimal unsatisfiable subsets often contain many binary clauses, or reduce to binary clauses after just one variable assignment. Since there exists a linear time al- gorithm for the satisfiability of binary clauses [I], problems containing such minimal unsatisfiable subsets can be solved in polynomial time. We have therefore augmented DP with the following rule: (Binary) are unsatisfiable then assign the negation of 1 to True. if the binary clauses of (2 simplified with the literal 1 assigned to True) This rule has a non-deterministic choice of literal; this may affect the number of pure, unit or binary rules applied but not the number of splits or branches. We tested DP augmented with the binary rule on lOO-variable CP problems. The worst mean perfor- mance was only 2.5 branches at L/N = 3.4, more than an order of magnitude less than the performance of DP alone. (Note that it cannot be deduced from this that run time is reduced, as there is a large overhead in applying the rule.) The binary rule is able to solve many of the (previously hard) unsatisfiable problems in the variable region, sometimes without search. For example, we tested the 100 worst unsatisfiable lOO-variable problems of 100,000 CP problems tested at L/N = 2.6. This point was chosen as it has very bad performance of the worst cases, and is in the middle of the constraint gap. DP augmented with the (Binary) rule was able to solve 92 of these 100 problems without needing to search at all. That is, the binary and near-binary clauses contained enough information alone to prove unsatisfiability. By contrast, at the point of worst median performance, L/N = 3.9, only 43 of the 100 worst problems were solved without search. It can thus be seen that in terms of reducing the amount of search, augmenting DP with the (Binary) rule is highly effective. Nevertheless, it is not able to eliminate the extremely bad worst-case performance in the mostly satisfiable region. Of the 8 of the 76 I.P Gent, T Walsh/Artificial Intelligence 81 (1996) 59-80 ;.. 0.8 i.. 0.6 i.. 0.4 :.. 0 0 1 2 3 4 5 6 L/N Fig. 1 I. CP problems tested using DP with intelligent backtracking, N = 300. worst 100 unsatisfiable problems not solved trivially at L/N = 2.6, one required as many as 6,520 branches using the binary rule and another 2,454. By contrast, at L/N = 3.9, the worst of the 57 problems not solved trivially needed only 574 branches using the binary rule. Furthermore, the binary rule is not able to eliminate the constraint gap. The ratio of applications of the binary rule to splits is broadly similar in behaviour to the ratio of units to splits seen in Fig. 3(b), peaking at comparatively large values of L/N. The fact that the variable behaviour in an otherwise easy region of problems cannot be eliminated by the (Binary) rule is particularly significant. This rule was an especially good candidate to eliminate variable behaviour, since we showed above that the worst- case problems were associated with minimal unsatisfiable subsets largely containing binary clauses. Furthermore, we have shown in the past [8] that improved branching heuristics seem unable to eliminate the variable behaviour. Thus, neither better heuristics, nor better constraint propagation are able to eliminate variable behaviour. We also implemented a restricted version of the binary rule which just determines the satisfiability of the binary clauses and does not simplify on any of the literals. Although this restricted rule is less expensive, it appears to be of little use in reducing search; for CP at N = 100, 2Np = 3, it closed at most 20% of branches at large L/N but less than 3% of branches in the region of the constraint gap. It thus had little effect on mean behaviour. 12. Intelligent backtracking There are three ways in which we can try to improve the behaviour of backtracking algorithms. We can improve the quality of our branching choices, we can increase the work we do between each branch to try to decrease the amount of branching, and we can use information gained during search to improve the quality of backtracking. Improved branching heuristics appear to be unable to eliminate exceptionally hard problems [ 81. Also, as we saw in Section 11, improved constraint propagation between branching points in search appears unable to eliminate the constraint gap or exceptionally hard I.!? Gent, T. Walsh/ArtQkial Intelligence 81 (1996) 59-80 II problems. This leaves open only the question of whether intelligent backtracking can rid us of these difficult problems. To test this, we used an implementation of the Davis-Putnam procedure written by Mark Stickel which uses a form of intelligent backtracking. 3 When a branch closes, the variable assignments which contributed to the generation of inconsistency are recorded. Backtracking then jumps over all other variable assignments since they are irrelevant to the closing of the branch. Such intelligent backtracking may identify branching points which were made on irrelevant variables outside the minimal unsatisfiable subset. When it succeeds in doing so, a dramatic reduction in search is achieved. Intelligent backtracking greatly reduced the number of exceptionally hard problems for small problem sizes. However, with increasing problem size they seem to reappear. At N = 300, we tested 10,000 problems from the constant probability model with 2Np = 3, at each value of L/N from 0.2 to 6.0 in steps of 0.2, and additionally each value from 2.2 to 4.6 in steps of 0.05 to investigate the phase transition in more depth. Fig. 11 shows the percentile contours of results, and also the probability of satisfiability. The worst case observed was a sari&b/e problem that needed 1,386,500 branches at L/N = 3.2, where 90.14% of problems were soluble, and where median behaviour was only 2 branches. This is five orders of magnitude worse than the worst median behaviour of only 12 branches at L/N = 3.7, at which point the worst case we saw was 16,003 branches. Further hard problems were seen at smaller values of L/N, for example one unsatisfiable problem needed 48,269 branches at L/N = 2.7, where 99.52% of problems were soluble. It is clear from these results that rare but exceptionally hard problems still occur in the mostly soluble and easy region. However, the worst cases may occur at slightly larger values of L/N than before, and not as clearly in the constraint gap. The existence of other hard problems at smaller values of L/N suggests that the constraint gap is still important. To investigate this further, extensive computational experiments will be required, both with larger problem and sample sizes. Other forms of intelligent backtracking are possible, such as dependency directed backtracking [ 151. Baker has shown that this can eliminate exceptionally hard problems from the underconstrained region in 3-COL for lOO-node random graphs [ 21. However, it remains an open question whether this or any other form of intelligent backtracking can eliminate such hard problems as problem sizes increase. 13. Related work Phase transitions are becoming increasingly important in the study of AI systems. Huberman and Hogg [ 121 predict that many large-scale systems will undergo sudden phase transitions that affect computational performance. They show, for example, that a simple model of heuristic search changes from linear to exponential behaviour at a phase boundary. Cheeseman et al. [ 31 observed that many NP-hard problems (e.g. graph 3 This implementation lacks the pure literal deletion rule. This is unlikely to affect results qualitatively since the pure literal rule is not effective in the regions where hard problems occur. 78 1.P: Gent, I: Walsh/Artijiciai Intelligence 81 (1996) 59-80 colouring and Hamiltonian circuits) have a control parameter, that a phase transition between underconstrained (and typically soluble) problems and overconstrained (and typically insoluble) problems occurs at a critical values of this parameter, and that hard problems occur at this critical value. Williams and Hogg [ 161 give theoretical reasons why the solubility phase transition should coincide with peaks in problem difficulty. For random 3-SAT and CP, Mitchell et al. [ 141 demonstrated that the parameter is L/N, the ratio of clauses to variables, and that median performance of DP has an easy- hard-easy pattern through the phase transition with the hardest median instance occurring in the phase transition. Although they noted that the mean is influenced by a very small number of very large values, they concentrated solely on the median as they felt that “it appears to be a more informative statistic”. Our results suggest that the distribution of values is, in fact, of considerable importance in understanding problem difficulty, and that the median alone provides a somewhat incomplete picture. Crawford and Auton have also observed a secondary peak in mean problem difficulty for a tableau based procedure in a mostly satisfiable region of random 3-SAT [4]. However, they failed to observe this peak with DP and therefore speculated that it was probably an artifact of the branching heuristics used by their procedure. In fact it seems more likely that it is an artifact of the statistics they compiled, namely the number of nodes in the search tree. Obviously this is related to depth of search, and we observed a distinctive peak in this at low L/N in CP (see Section 4) and a similar one in 3-SAT (see Section 6). This would be enough to account for the secondary peak they report of approximately 13 nodes at N = 50. It was only on much larger problems, with hundreds of variables, that they report occasional extremely hard behaviour. In [8] we were able to show, however, that unusually hard problems do occur at low L/N at N = 50 in 3-SAT using a simplified variant of DP, but that it requires very large sample sizes to be seen. For this effect in 3-SAT to be studied in more detail, larger problems and larger sample sizes will be needed. Hogg and Williams have observed extremely variable problem difficulty for graph colouring using both a backtracking algorithm based on the Berlaz heuristic and a heuristic repair algorithm [lo]. They found that the hardest three colouring problems were in an otherwise easy region of graphs of low connectivity. The median search cost, by comparison, shows the usual easy-hard-easy pattern through the three colourability phase transition. They propose that these very hard problems are associated with a transition between polynomial and exponential average search cost. 14. Conclusions We have performed a detailed experimental investigation of the satisfiability phase transition for several different classes of randomly generated problems including the constant probability model, random k-SAT, random mixed SAT, and an encoding of k-COL into SAT. With each problem class, the median problem difficulty for the Davis- Putnam procedure displays an easy-hard-easy pattern with the hardest problems being associated with the satisfiability phase transition. We have shown, however, that the I./? Gent, 7I Walsh/Artificial Intelligence 81 (1996) 59-80 19 since behaviour is inadequate features. picture of easy-hard-easy “conventional” tion of problem difficulties has several other important problem classes have a region of very variable problem difficulty where problems typically underconstrained lems orders of magnitude These extraordinarily minimum of both the constraint sistent with problem variables. the distribu- all the are these regions, we have found prob- transition. to be associated with a constraint gap, a to splits. The position and shape con- to phase size. For example, both occur at fixed ratios of constraints and satisfiable. Within harder hard problems in the ratio of the constraint propagations than problems appear in the middle of the phase gap and the satisfiability are remarkably In particular, transition We have shown that the hardest problems than typical unsatisfiable constrained in the variable subset. The difficulty transition typical problems. By comparison, ent reasons. The hardest unsatisfiable critically able problems unsatisfiable minimal unsatisfiable constraint unsatisfiable search space, search the other hand, hard satisfiable problems constrained early stage in search leads to an unsatisfiable unsatisfiable unsatisfiable problems subset. subset from the many propagations time can be exponential in solving during are more critically constrained between being satisfiable and unsatisfiable hard problems in the middle of the satisfiability phase than region arise for differ- less region are actually the hardest unsatisfi- region often contain a very small and unique minimal in the variable in the variable problems. problems Indeed, such problems the irrelevant clauses. All branching points or is thus in identifying subset represent entirely wasted work. As each branching point doubles search which do not affect a variable in the minimal the in the number of irrelevant choices. On in the variable region are much more critically at an like hard a small and unique minimal subproblem, which then behaves in the same region, and contains than typical satisfiable problems. As a result, an incorrect assignment to a large number of binary clauses after just one variable assignment, the hardest unsatisfiable problems away from the probability a small minimal unsatisfiable unsatisfiable in solving is in identifying The difficulty phase transition a constraint gap greatly hinders our ability Despite the fact that the minimal or reduce addition of a rule binary constraints shown elsewhere The fact that neither heuristically eliminate this behaviour class. It remains possible exceptionally backtracking Given suggest otherwise. suggests to the Davis-Putnam reduces but does not eliminate procedure that improved branching heuristics also do not eliminate nor non-heuristically that it is likely are able to to the problem backtracking might eliminate hard problems or the constraint gap. Our results on one form of intelligent tentatively that some form of intelligent based improvements to be fundamental to find such minimal unsatisfiable subset. The existence of subsets. subsets contain many binary clauses, the to propagate on binary or near- very variable behaviour. We have this behaviour. the range of problem classes used in our experiments, we believe to other SAT problem classes, and perhaps even of other NP-hard problems. The identification of factors results may generalize transitions gap which help to make be useful both empirically understanding instances of an NP-complete for testing and improving problem algorithms the fine detail of NP-hardness. that these to the phase like the constraint like SAT hard should for and theoretically 80 I.P Gent. 7: Walsh/Art$cial Intelligence 81 (1996) 59-80 Acknowledgements This first author was supported by a SERC Postdoctoral Fellowship and the second by a HCM Postdoctoral fellowship. Part of this research was conducted whilst the second author was at INRIA-Lorraine, Nancy, France. We thank Alan Bundy, Pierre Lescanne, the members of the Mathematical Reasoning Group at Edinburgh (supported by SERC grant GR/H 23610), and the Eureca group at INRIA-Lorraine for their constructive comments and for quite a few CPU cycles. We also thank Mark Stickel for the implementation he kindly gave us. Finally we thank Alan Bundy, Paul Purdom, and a referee of this journal for comments which improved this paper. References I I I B. Aspvall, M.F. Plass and R.E. Tarjan, A linear-time algorithm for testing the truth of certain quantified Boolean formulas, Inform. Process. Lett. 8 (1979) 121-123. [ 2 I A.B. Baker, Personal communication ( 1995). [ 31 P. Cheeseman, B. Kanefsky and W.M. Taylor, Where the really hard problems are, in: Proceedings NCAI-91, Sydney, Australia (1991) 331-337. [ 4 ] J.M. Crawford and L.D. Auton, Experimental results on the crossover point in satisfiability problems, in: Proceedings AAAI-93, Washington, DC (1993) 21-27. 15 1 M. Davis, G. Logemann and D. Loveland, A machine program for theorem-proving, Commun. ACM 5 (1962) 394-397. 16 1 M. Davis and H. Putnam, A computing procedure for quantification theory, J. ACM 7 ( 1960) 201-215. 17 1 0. Dubois, P Andre, Y. Boutkhad and J. Carher, SAT versus UNSAT, Presented at the Second DIMACS Challenge Workshop (1993). 18 1 1.P Gent and T. Walsh, Easy problems are sometimes hard, ArtijI Intell. 70 (1994) 335-34.5. 191 1.P Gent and T. Walsh, The SAT phase transition, in: A.G. Cohn, ed., Proceedings ECAI-94, Amsterdam (Wiley, New York, 1994) 105-109, [ IO] T. Hogg and C. Williams, The hardest constraint problems: A double phase transition, Artif: Intell. 69 (1994) 359-377. 1 11 1 J.N. Hooker and C. Fedjki, Branch-and-cut solution of inference problems in propositional logic, Ann. Math. Art$ fntell. 1 (1990) 123-139. 1121 B.A. Huberman and T. Hogg, Phase transitions in artificial intelligence systems, Artif: Intell. 33 (1987) 155-171. I 13 1 S. Kirkpatrick, G. Gyiirgyi, N. Tishby and L. Troyansky, The statistical mechanics of k-satisfaction, in: J.D. Cowan, G. Tesauro and J. Alspector, eds., Advances in Neural Information Processing Systems 6 (Morgan Kaufmann, San Mateo, CA, 1994) 439-446. [ 14 1 D. Mitchell, B. Selman and H.J. Levesque, Hard and easy distributions of SAT problems, in: Proceedings AAAI-92, San Jose, CA ( AAAl Press/The MIT Press ( 1992) 459-465. 1 IS I R.M. Stallman and G.J. Sussman, Forward reasoning and dependency-directed backtracking in a system for computer-aided circuit analysis, Artif InteN. 9 (1977) 135-196. I I6 I C.P Williams and T. Hogg, Exploiting the deep structure of constraint problems, Artif: Intell. 70 ( 1994) 73-l 17. 