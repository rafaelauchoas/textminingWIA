llOPEN ACCESSPerspectivePatient and public involvement to build trust in artificialintelligence: A framework, tools, and case studiesSoumya Banerjee,1,* Phil Alsop,1,2 Linda Jones,1 and Rudolf N. Cardinal11University of Cambridge, Cambridge, UK2Cambridge, Cambridgeshire, UK*Correspondence: sb2333@cam.ac.ukhttps://doi.org/10.1016/j.patter.2022.100506THE BIGGER PICTURE Hype and negative news reports about artificial intelligence (AI) abound. Involving pa-tients in healthcare AI projects may help in adoption and acceptance of these technologies. We argue that AIalgorithms should be co-designed with patients and healthcare workers.We show examples of how to involve patients in AI research and how patients can build trust in algorithms.We share some best practices, case studies, a framework, and computational tools.Avenues for future work include guidelines for patient and public involvement in AI healthcare research forfunding bodies and regulatory agencies.An understanding of what AI can and cannot do, and a realistic appraisal of risks and benefits, may help inadoption and democratize access to AI for healthcare.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYArtificial intelligence (AI) is increasingly taking on a greater role in healthcare. However, hype and negativenews reports about AI abound. Integrating patient and public involvement (PPI) in healthcare AI projectsmay help in adoption and acceptance of these technologies.We argue that AI algorithms should also be co-designed with patients and healthcare workers.We specifically suggest (1) including patients with lived experience of the disease, and (2) creating a researchadvisory group (RAG) and using these group meetings to walk patients through the process of AI model build-ing, starting with simple (e.g., linear) models.We present a framework, case studies, best practices, and tools for applying participative data science tohealthcare, enabling data scientists, clinicians, and patients to work together. The strategy of co-designingwith patients can help set more realistic expectations for all stakeholders, since conventional narratives of AIrevolve around dystopia or limitless optimism.INTRODUCTIONMachine learning is increasingly becoming pervasive in health-care. Artificial intelligence (AI) is increasingly taking on a greaterrole in healthcare, especially during the current coronavirus dis-ease 2019 (COVID-19) pandemic.1 However, hype and negativenews reports about AI abound.People do not always understand or trust AI. This overlapswith other concerns people have, such as the security of theirdata. People are not always consulted about AI that might affectthem. Part of the solution is patient and public involvement (PPI).In PPI, the general public and patients are involved in research.The level of involvement varies from project to project. Beinginvolved in a project helps build trust. There is a rich history ofPPI in healthcare. However, it has not been done very much inthe context of modern AI.As misinformation spreads around AI, integrating patient andpublic involvement in healthcare AI projects and clinical trialsmay help in adoption and acceptance of these technologies.We argue that AI software should also be co-designed with pa-tients and that patients should be involved in discussions aroundAI research applied to healthcare.We advocate a collaborative approach where patients, carers,clinicians, and data scientists work together to decide what datawill be used as inputs to computer programs and understandwhy these algorithms made a particular prediction. Recent studieshave raised awareness about designing AI algorithms in closecollaboration with healthcare workers.1 Machine learning re-searchers alone may not be able to appreciate the broader impactof their work and there is a need to involve other stakeholders.2We give examples of work we have done in this area as casestudies (in the section ‘‘case studies: examples of data-focusedPatterns 3, June 10, 2022 ª 2022 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).llOPEN ACCESSresearch via a research advisory group’’), and make some gen-eral recommendations (in the sections ‘‘framework for buildingtrust and typical patient concerns’’ and ‘‘recommendations’’).We suggest a framework of how patients can build trust in AIand we share tools and resources that can be used to explainthe basics of AI to patients. We developed tools to demonstratekey concepts to the public (section ‘‘tools for outreach andinvolvement’’). We also review the current literature on trust inAI (section subsection ‘‘trust in AI and the role of PPI’’). Wehope that the approach of involving patients, clinicians, anddata scientists in a virtuous cycle of co-design will be used infuture AI projects in healthcare.CASE STUDIES: EXAMPLES OF DATA-FOCUSEDRESEARCH VIA A RESEARCH ADVISORY GROUPIn this section, we describe two projects as case studies. In latersections, we reflect on these projects and present our generalrecommendations.We were conducting research in this area, so we recruited pa-tients and formed a research advisory group (RAG). The RAGmet regularly and discussed data-focused research projectsrelated to severe mental illness. Additional details on the RAGare available in the section ‘‘framework for building trust andtypical patient concerns.’’Analysis of the effect of lithium medication on kidneyfunctionIn this section we describe a patient-led project. Patients withbipolar disorder are sometimes prescribed lithium. Lithium isan effective medication, but long-term use may lead to kidneydamage. A patient in the RAG had suggested looking at hos-pital data to investigate if discontinuing lithium can helprecover kidney function in patients with bipolar disorder takinglithium.The patient was involved in all stages of a researchproject. Our aim was to predict whether stopping lithiumintake, in patients with bipolar disorder (who have been onthe medication for a long time), is associated with reversalof drug-induced renal damage.We used observational from hospital electronic healthcare re-cords systems to answer these questions. We outline the variousdatasets that were used in this work:(1) EPIC prescription data. This is an electronic patient recordsystem operationalin Cambridge University Hospitals (CUH)from October 2014 until present. This system captures all CUH ac-tivity during its period of operation. This includes laboratory testsand prescriptions (these are recorded typically for inpatientsonly) and structured diagnostic codes (for a subset of patientsand a subset of diagnoses). This has features like age, gender,and ethnicity.(2) Meditech data. This is a laboratory system operational inCUH from 1995 until present. This system captures all laboratoryinvestigations data from CUH during its period of operation. Thishas laboratory results like creatinine.Patients with records in both EPIC and Meditech had their re-cords cross-matched before anonymization.We used the following linear mixed effects model (in the R pro-gramming language notation):2 Patterns 3, June 10, 2022PerspectiveeGFR = e0 + boff toff + bonton + ð1j pid + toff + tonÞ(Equation 1)where eGFR is the estimated glomerular filtration rate (eGFR)and is calculated from creatinine, age, gender, and ethnicity(data available from hospital electronic healthcare recordssystem) using the CKD-EPI formula.3 pid is the unique patientidentification numberin the electronic healthcare recordsystem. bon is the rate at which eGFR declines when a patientis on lithium. toffis the cumulative time spent off lithium, andton is the cumulative time spent on lithium. boffis the rate atwhich eGFR is declining for patients off lithium, and bon is therate at which eGFR is declining for patients on lithium. e0,bon, boff , ton, and toff are parameters that are estimated fromthe data.However, using these data on a few thousand patients, the re-sults were inconclusive. This motivated the need to go back tothe RAG and explain the need for more data. We took feedbackfrom patients as to whether we should apply for access to moredata. We also built a tool that explains how, in some cases,having more data can help in estimating parameters of statisticalmodels (see sections ‘‘tools for outreach and involvement’’ and‘‘framework for building trust and typical patient concerns’’).This process of performing research and getting inconclusiveresults also showed patients how research always takes timeand can lead to unexpected roadblocks.We also took the time to explain how to build statistical models.For example, we tried other, simpler formulations before we arrivedat the final model (see Equation 2). This showed patients how re-searchers always incrementally build more complex models.eGFR = e0 + boff toff + bonton + ð1j pidÞ(Equation 2)We explained these models using an example of a simplerlinear model:y = a,x + b(Equation 3)This is a linear model where the value of x is used to predict y(say eGFR). a and b are the parameters of the model, and thesecan be estimated. We explained that estimating means deter-mining the values of a and b from data. Once we explained theconcepts of a linear model, we progressed to more advancedconcepts like confidence intervals.We are currently validating ourresults in an additionalindependent cohort of patients (the Clinical Practice ResearchDatalink [CPRD] research database, which has general practicerecords from the United Kingdom4).At this stage, we also communicated to the patients a numberof caveats. Lithium is an effective medication for managingbipolar disorder,5 and the chances of patients developing renalcomplications is quite small.6 The benefit of discontinuing lithiumshould be carefully weighed against the risk of relapse of thepsychiatric disorder, as has been documented in case studies7and suggested in meta-analysis studies.6Our work may lead to randomized controlled trials to test thehypothesis that discontinuing lithium may help recover kidneyfunction in patients with bipolar disorder.PerspectivePREDICTING MORTALITY IN PATIENTS WITH SEVEREMENTAL ILLNESSPremature mortality in patients with severe mental illness (likeschizophrenia)is a public health concern. Here we outlineanother project where we used observational data from elec-tronic healthcare records to make predictions of mortality in pa-tients with schizophrenia.We developed machine learning models of mortality inschizophrenia and applied the technique of class-contrastivereasoning to improve their explicability. Class-contrastivereasoning is a technique from the social sciences8,9: the contrastis to an alternative class of exemplars. An example of a class-contrastive explanation is a selected patient who is at high riskof mortality because the patient has dementia in Alzheimer’sdisease and has cardiovascular disease. If the patient did nothave both of these characteristics, the predicted risk would bemuch lower.Briefly, the approach modifies the features until the machinelearning model produces a different prediction. The effect ofchanging features on the model output is explained visuallyusing a heatmap.10The machine learning model was trained on the training data.We then changed one feature at a time on the test data and re-corded the change in the model prediction. In this scenario,is not retrained. The change in model predictionsthe modelwas visualized as a class-contrastive heatmap.For this project, the researchers defined the hypothesis.Discussions with patients then motivated the need to developan explainable AI algorithm.10Our machine learning approach10 is summarized here:1. We used de-identified data from an electronic patient re-cord system for mental health.2. We defined a set of high-level features These includedage, psychiatric diagnostic categories (time-independentcoded diagnosis at any point during the study period),and medication categories (time-independent prescrip-tion of or use of medications; for example, anti-depres-factors that aresants). We also included bio-socialimportant in severe mentalillness, like information onmental health diagnosis, relevant risk history such as aprior suicide attempt, substance abuse, and social factorssuch as lack of family support.3. We used these features to predict death during the time ofobservation.4. We then fitted a machine learning model. Our machinelearning algorithm was based on a type of artificial neuralnetwork called an autoencoder.10 Class-contrastiveheatmaps were used to visualize the explanations of thestatistical models and machine learning predictions.The corresponding class-contrastive statements andheatmaps also aid human interpretation.We used data from the Cambridgeshire and PeterboroughNational Health Service (NHS) Foundation Trust(CPFT)Research Database. This comprises electronic healthcare re-cords from CPFT, the single provider of secondary care mentalhealth services for Cambridgeshire and Peterborough, UK, anarea in which approximately 856,000 people reside. The recordsllOPEN ACCESSwere de-identified using the CRATE software11 under NHSResearch Ethics approval (12/EE/0407, 17/EE/0442).Data included patient demographics, mental health, and phys-ical co-morbidity diagnoses: these were derived from codedInternational Classification of Diseases, tenth Revision (ICD-10)diagnoses and analysis of free text through naturallanguageprocessing (NLP) tools.12,13Dates of death were derived from the NHS Spine. We consid-ered all patients with coded diagnoses of schizophrenia who hadrecords in the electronic healthcare system from 2013 onwards.There were a total of 1,706 patients diagnosed with schizo-phrenia defined by coded ICD-10 diagnosis (diagnosiscode F20).We note that our machine learning and statistical models arenot meant to aid decision making in their present form. Additionalvalidation studies in other cohorts and evaluations will berequired to determine if these models can be used in clinical de-cision making.FRAMEWORK FOR BUILDING TRUST AND TYPICALPATIENT CONCERNSIn this section, we outline a framework for building trust in AI. Weshare some of the typical concerns that patients have about AIand how to address them. The nature of explaining and under-standing a complex model (like an AI model) requires humansto build simplified mental representations.14 Trust in an AI sys-tem can be built up slowly after understanding it at multiplelevels, ranging from personal to institutional and technological.15In order to build trust in AI algorithms, one needs to considerthe complex socio-technological milieu in which technologicalsolutions reside. Trust needs to be built not only in AI algorithms,but the training data, software, and complex environment inwhich humans are situated. These include institutions andpeople.The doctor-patient relationship is an important aspect oftrust.16 Trust in institutions and people is intimately linked to trustin health technologies.15Our patients may have had implicit faith in the institutionswhere this research was conducted (Cambridge University Hos-pitals and University of Cambridge) and the carers involved (L.J.,R.N.C.)). We note that this can be very difficult to achieve in pla-ces where there is an existing trust deficit in doctors and hospi-tals, especially in low- and medium-income countries.17This step is probably the most difficult to implement and theleast actionable from the viewpoint of an individual researcherwho is embedded in a large organization. Nevertheless, wewish to point out that trust in algorithms cannot be completelydecoupled from trust in institutions and people involved in theresearch.For existing organizations that have built trust and reputationover decades, this is quite simple. For emerging institutions,especially in developing nations, this is a very big challenge.Building trust and reputation takes decades of work. This isnot to under-emphasize the importance of algorithms but tomerely point out the complex socio-technological milieu in whichall technological solutions reside.We outline a framework below for helping patients build trust inAI. We note that there is no particular order to this framework.Patterns 3, June 10, 2022 3llOPEN ACCESS1. Recruiting patients and forming an RAG.We recruited patients and formed an RAG. We designed anadvertisement explaining the project and a person role profile,which was sent out through our local long-standing PPI groupsacross the region. The requirements were to have a lived expe-rience of mental health illness or care for someone who does.Once the applications came in, L.J. went out and met everyoneto explain further about the group and to answer any questionspeople may have had. Out of the nine original applicants, fivebecame fully involved as part of an RAG and one was involvedfrom a distance (i.e., did not meet up with the group but washappy to give advice from a distance).The RAG were then invited to a meeting to tell them about theteam projects. We then kept in touch through further emails,calls, and occasional meetings (although the in-person meetingswere curtailed with the onset of the COVID-19 pandemic).The RAG also co-produced questions in surveys that weresent out to patients. We helped them understand what we hopedto get out of the project and they then helped formulate the ques-tions needed to get the answers and helped design surveys.For recruitment to the survey study, we advertised in many waysthrough written posters and social media as this was a survey forboth patients and public. We also recruited through over 100 phys-ical and mental health trusts through the UK National Institute forHealth Research (NIHR) Clinical Research Network (CRN). Siteswere not specifically selected: through the CRN, we took on boardany site who wanted to recruit for us. They approached their pa-tients in person, staff, newsletters, social media, television screensin clinics, posters, PPI groups, users networks, and so forth. Wealso approached over 200 general practice surgeries across thecountry, who contacted patients by text, newsletters, social me-dia, posters, and television screens.A kick-off meeting was organized when the RAG wasconstituted. RAG meetings with all team members were thenheld once every 6 months over a 2-year period. More focusedmeetings on projects (see section ‘‘case studies: examples ofdata-focused research via a research advisory group’’) wereheld approximately once in 2 months.2. Problem formulation and hypothesis generation.Scoping and framing the problem is very important.18 Someproblems are relevant but cannot be answered with the data wehave. It is important to determine the intersection of a relevantproblem, and having the data and expertise to solve it. In the initialdiscussions with the RAG, we determined and scoped tworesearch questions, which can be addressed using data we hadaccess to. These are detailed in the section ‘‘case studies: exam-ples of data-focused research via a research advisory group.’’One project focused on the effect of a medication (lithium) onkidney function (section ‘‘case studies: examples of data-focused research via a research advisory group’’). The patientformulated the problem and initial hypothesis: stopping lithiumintake (a drug prescribed for bipolar disorder that can in somecases cause renal toxicity) may reverse renal damage.The patient also suggested potential roadblocks like non-adherence of lithium medication. For example, some patientsdo not take lithium consistently. Instead, a few days before a4 Patterns 3, June 10, 2022Perspectivelithium blood test, they take the medication (so that lithium is de-tected in a blood test). It is something we had not appreciatedbefore. The patient suggested this and we took steps to compu-tationally account for this non-adherence.Another project focused on data available from hospital elec-tronic healthcare records (EHRs) and used it to predict mortalityin patients with severe mental illness. Motivated by discussionswith the RAG, we decided to build an interpretable machinelearning model for this problem10 (see sections ‘‘case studies:examples of data-focused research via a research advisorygroup’’ and ‘‘predicting mortality in patients with severe mentalillness’’).3. Building trust in the data storage infrastructure.Concerns about data privacy and the fear of data exploitationare impediments to adoption of digital health technologies.15 Weexplained our data storage infrastructure to patients. Weexplained that all clinical data were stored on computers in asecure environment that was part of the NHS. Researchershad to apply for research passports to get access to thedata, and all analysis had to be performed on those secure com-puters. We explained that obtaining a research passportinvolved detailed background checks and only eligible re-searchers could get access to the computational and data stor-age infrastructure.All data were also pseudonymized, which made it extremelydifficult to identify individual patients. We took care to explainthat although the possibility of identification was minimal, no sys-tem was secure against a determined adversary. The data scien-tist also faced various roadblocks and administrative delays indata access, which only served to demonstrate the considerabledifficulties that an adversary would have to overcome.4. Addressing concerns about big data.Patients may also have concerns about big data and their databeing used. These concerns may relate to storing large amountsof data and whether it posed any risk for privacy.In order to address this concern, we explained the steps weare taking to ensure patient privacy. For example, we explainedhow all data were stored securely on NHS computers, which onlyeligible researchers had access to. The process of getting ac-cess to the data included a lengthy research passport applica-tion (which also involved a background check).We also elicited feedback from patients as to whether weshould have access to additional data. For the project onlithium-induced renal toxicity (section ‘‘case studies: examplesof data-focused research via a research advisory group,’’ sub-section ‘‘analysis of the effect of lithium medication on kidneyfunction’’), we determined that we needed follow-up data for pa-tients or access to a larger external cohort of patients in CPRD.4Before requesting access to CPRD data (unlinked andanonymized),4 we asked for the opinion of patients and ex-plained that having more data may lead to better performancein our models.As a simple example, we also demonstrated a computationaltool that showed the advantages of using big data in health-care19 (see section ‘‘tools for outreach and involvement’’). WePerspectivealso undertook surveys to understand more broadly the con-cerns that patients have about using big data in healthcare.5. Discrimination and bias in AI.Discrimination and bias is a valid concern for patients. Weshowed how this can happen in a simple situation of a facialrecognition tool20 (see section ‘‘tools for outreach and involve-ment’’). For example, if a training dataset has no data on facesof people from a certain ethnic background, then the machinelearning algorithm implementation being trained to recognizethese data before.21 Hencefaces will not have ‘‘seen’’when this machine learning algorithm implementations isused to make a prediction, it may be biased against these indi-viduals.22We showed how we are taking precautions against bias in al-gorithms and the data. We stressed the fact that the tools that webuild will ultimately need to be validated in another setting withmore patient numbers. We explained that sensitive data likedate of birth, home addresses, and NHS numbers will not bestored. Sensitive attributes contribute toward perceptions of fair-ness in data.23Data can mirror historical societal biases. A critical examina-tion and discussion of bias in data, may allow us, includingpatients, to re-envision a future where AI is used for good.226. Debunking myths about AI from contemporary discus-sions in the media.Patients may have misconceptions about AI from discussionsin the media. An online resource24 (also see section ‘‘tools foroutreach and involvement’’) can be used to debunk commonmyths surrounding AI.7. Understanding a simplified model.We simplified the problem and built a simple linear model (themodel is explained in detail in the section ‘‘case studies: exam-ples of data-focused research via a research advisory group,’’subsection ‘‘analysis of the effect of lithium medication on kidneyfunction’’). We explained the basics of fitting a linear model todata and then explained the predictions of this simple model.Following this, we explained the motivations for fitting morecomplex models. Understanding simple linear models also builtthe foundation for more complex models like deep learning. Thisalso helped patients understand how models are always builtiteratively, by progressively adding more complexity.Furthermore, we used heatmaps to visualize data and theoutput of models. We explained how heatmaps can be used toexplore complex AI models, by visualizing how the model outputchanges when the input is changed10 (see section ‘‘case studies:examples of data-focused research via a research advisorygroup,’’ subsection ‘‘predicting mortality in patients with severemental illness’’).8. Understanding AI models more broadly.We used tools to help patients understand the basics of AI anddeep learning. The Teachable Machine25 is one of these toolsthat can help patients understand deep neural networks bytraining and visualizing AI models in a Web browser.llOPEN ACCESSWe also built a tool to demonstrate the benefits of usingbig data in healthcare.19 Patients can play around withthese tools and build an understanding of these models (seesection ‘‘tools for outreach and involvement’’ for more onthese tools).9. Designing computer interfaces with patients.As an additional step, computer interfaces can also be co-de-signed with patients. An example of this is the development of asmartphone app that was co-designed with patients.26We had a long and deep engagement with patients in all stepsof AI research, from hypothesis generation to model building andunderstanding. In this way, patients felt they were involved in thisproject. This also gave a sense of agency and voice to patients(see section ‘‘patient perspective’’).27RECOMMENDATIONSWe specifically suggest (1) including patients with lived experi-ence of the disease and carers, (2) creating an RAG and usingthese group meetings to involve patients and carers in all stagesof the scientific process (starting from hypothesis generation).We also recommend explaining the process of AI model building,starting with simple (e.g., linear) models. We suggest using freelyavailable AI models that run in the browser (such as the Teach-able Machine25) to explain the basics of AI to patients. Thesemeetings should be repeated to elicit feedback from the stake-holders, explain model predictions, and get guidance on modelmodifications.In RAG meetings, we built trust and solicited commentson how patient data could and should be analyzed. Weshowed patients how we took precautions to preserveprivacy and allayed other concerns. We sought to reduce thehype around AI and explain these techniques using simpleexamples.We explained how AI will be used on clinical data and how theexpected outcomes might benefit patients. In turn, we learnedfrom patients and carers about important features of the data,and about the concerns that must be addressed to implementAI models in practice, including the potential for inadvertentdiscrimination by AI.28–30We suggest a generalframework of how patients canbuild trust in AI (see section ‘‘framework for building trust andtypical patient concerns’’). This framework can be adaptedbased on the unique requirements and financial constraints ofa project.PATIENT PERSPECTIVEPatients and carers have important research ideas about howbest to improve quality of life, manage symptoms, offer existingtreatments, or develop new interventions. Often these ideasdiffer from those prioritized by academia or the pharmaceuticalindustry.Here is why two members gotinvolved in research inour group:‘‘I decided to join the group to help make a difference using myexperience with a mental health condition, in my case beingPatterns 3, June 10, 2022 5llOPEN ACCESSPerspectivediagnosed with bipolar in 2003. To offer and share ideas and tipsin what helps me and also share my experience to help researchin the future.’’‘‘I was excited when I saw the invitation to join this group.Using more extensive data can potentially answer many vitalquestions that an 8-week drug trial simply cannot. As a serviceuser I was keen to see how we can be involved.’’TOOLS FOR OUTREACH AND INVOLVEMENTThere is a need for tools that help the public gain an understand-ing of AI. We outline some resources to demonstrate the basicsof AI to general audiences and patients. All of these are freelyavailable resources that can be demonstrated on a moderncomputer with an internet connection.1. AI models that can be trained, run, and visualized in theWeb browser, like the Teachable Machine.25An effective way to build trust and understand a model is toactively construct it.31,32 Tools like the Teachable Machine lowerthe barrier to entry by training and visualizing AI models in a Webbrowser.2. An AI model that runs in the Web browser and uses a web-cam to detect facial expressions.20This tool can be used to highlight the importance of a diversetraining set. For example, if the training set does not have data onpeople of different ethnicities, then there is a risk of discrimina-tion, because the model has not seen this kind of data before.3. A Web application that demonstrates the benefits of bigdata in healthcare.19Some patients may have concerns about collecting and usinglarge quantities of data. This tool can be used to demonstratethat, for certain diseases, we may need more data, and moredata may lead to better model predictions.4. A set of videos that demonstrate what AI can and cannotdo.33,34 These resources can help set patient expectationsabout AI.5. A resource of myths about AI.24,30Myths about AI abound. These resources24,30 can be used todebunk some of the myths surrounding AI.ENGAGEMENT, INVOLVEMENT, AND PARTICIPATIONtelevision programs, newspapers, and social media. Substantialcosts may be involved in this.Some resources outlined in the section ‘‘tools for outreach andinvolvement’’ can be used to demonstrate the basics of AI to pa-tients and the general public. These resources can be run on alaptop with an internet connection and are a low-cost solutionto raising awareness.Even though we have released computational tools and thesecan be used by researchers worldwide, these are only onecomponent of an engagement and patient involvement strategy.We had to actively collaborate and engage with patients. Weanticipate this will be especially challenging in low- and me-dium-income countries, where researchers have limitedbudgets.Patient and public participation is where people take part in aresearch study. For example, people can be recruited to aclinical trial or other research study to take part in the research.Patients can also be asked to complete a questionnaire or partic-ipate in a focus group as part of a research study. Some costswill be involved in recruiting patients and organizing meetings.PPI is where members of the public are actively involved inresearch projects and in research organizations. Patients canbe involved in identifying research priorities and formulatinghypotheses. They can be joint grant holders or co-applicantson a research project. Patients and carers can also developpatient information leaflets and other research materials.This is more complex and cost-intensive, and may require aproject manager to organize groups, arrange meetings, andliaise with patients.We suggest a framework (see the section ‘‘framework forbuilding trust and typical patient concerns’’) that shows how toengage with patients and how to adapt some of these tech-niques based on the unique budgetary constraints of a project.DISCUSSIONActively engaging patients in managing the illnesses that affectthem may lead to more sustainable and patient-centered health-care. This can be achieved by healthcare professionals workingin co-production with patients and carers.Lived experience of a disease is important in healthcareresearch.35 If properly designed, PPI can lead to better out-comes in health research.36 Considering the viewpoints ofpatients can also allow us to design future ethnographic studiesand structured interviews for designing better AI solutions.37Examining the unique relationships between patients and clinicalstaff in a healthcare organization may also help us design betterelectronic health record systems.38There are different ways in which patients and carers can beinvolved and engaged. In this section, we outline different formsof engagement, involvement, and participation. Projects canadapt some of these based on their budget and time constraints.Patient and public engagement is where information andknowledge about research is provided and disseminated. Exam-ples of engagement are science festivals open to the public andresearch open days where members of the public are invited tofind out about research. An understanding of AI and how it af-fects our society can also be effected through media, such asCaveats and limitationsThis approach is not without limitations. Setting up and runningPPI groups and meetings is a major undertaking. We had a dedi-cated project manager for PPI who is also qualified as a nurse(L.J.). Some projects may not have the resources for a projectmanager dedicated to PPI. It may be helpful to have PPI man-agers who lend their expertise to multiple research projects,thereby spreading costs across teams.39Patient expectations also need to be managed by re-searchers.36 Patients may also not have the right quantitative6 Patterns 3, June 10, 2022Perspectiveskills, in which case it may be necessary to give them training orutilize their diverse complementary skills in other ways (see sec-tion ‘‘engagement, involvement, and participation’’).The patients who are recruited or volunteer for PPI groupmeetings may also not be representative of all patients. Crucially,the perspective of marginalized patient communities may bemissed.We believe the recommendations are supported by the extantliterature (reviewed later in subsection ‘‘trust in AI and the role ofPPI’’). We note that a more thorough analysis of precisely whichrecommendations are most effective would require a humanfactors study. This may take the form of a randomized controlledtrial or a formal change management study,40,41 in order to un-derstand which steps in a roadmap lead to the most effectiveadoption of AI or trust in AI. This would require a formal quantita-tive study of which factors/steps work best. In this work, we relyon the extant literature and qualitative studies.15 We hope ourwork willinspire more quantitative recommendations on howto engage with patients.We anticipate several hurdles for generalizing this work. Wehad a PPI lead (L.J.). L.J. is qualified as a research nurse andhas spent many years caring for patients. R.N.C. is a clinicianand also has experience applying statistical learning techniquesto mental health. It is helpful if carers are also cross-trained inmedicine and data science. Having researchers who are cross-trained in medicine and the quantitative sciences and/or havean appreciation of what data science can do in healthcare maybe helpful in enabling successful PPI projects in AI.We needed long-standing collaborations and research net-works to recruit patients. We also had to actively collaborateand engage with patients. We anticipate this will be especiallychallenging in low- and medium-income countries, where re-searchers have limited budgets. Even though we have releasedcomputational tools and these can be used by researchersworldwide, these are only one component of an engagementand patient involvement strategy.Engaging with stakeholders to define a problem sometimesleads to shallow definitions. It may also be difficult to find pa-tients who are really interested in engaging with researchers.Patients may also lack quantitative skills or experience inresearch and may need training. Finally, we note that re-searchers are not incentivized to engage with patients.In our experience, this framework is likely to succeed when pa-tients really want to be involved and are curious about theresearch process. Research is also not a linear process, andcommunicating this to patients was a challenge. The frameworkis also not linear and may need to be adapted based on the idi-osyncrasies of a project.A deep engagement with patients may lead to better trust, un-derstanding, and adoption of AI technologies in healthcare.42Outreach can also help humans build trust in machines. Thismay enable better human-machine co-operation and adoptionof AI in healthcare. Moreover, an effective way to build trustand understand a complex model (like an AI model) is to activelyconstruct it.31,32Trust in AI and the role of PPIThere is a lot of discussion on how to build trust in AI models.43,44Social and institutional factors are important in building trust.15llOPEN ACCESSThere is also a push toward trusted research environmentsthat protect data and enable privacy-preserving analysis.Computational platforms like OpenSAFELY45 and federatedanalysis platforms like DataSHIELD46 can also enable computa-tion while preserving privacy of individual-level patient data.Many projects have dedicated managers who are in charge ofdata governance.47 Communicating to patients that these rolesexist and that there are people who look at data governance,ethics, and security may further bolster trust in the computationalinfrastructure.Findability, accessibility,interoperability, and reusability(FAIR) principles are also important48 and should be explainedto patients. Ethical and legal issues around the General Data Pro-tection Regulation (GDPR), right to explanation,49 and duty ofcare can also be explained to patients in RAG meetings.Patient involvement and public outreach are essential in facil-itating ethical use of EHR data.50 Patients need to know thebenefits of research using EHR data, which include answeringquestions about public health that would be unethical to pursue(for example, the effect of exposure to environmental toxins orinequalities in healthcare access).50The public also lack understanding of how medical data canbe used to improve healthcare.51 Educating patients and thegeneral public about the benefits of research based on EHRdata can help build trust.50The general public does not fully trust clinical data sharing.51However, attitudes become more positive once the benefitsare explained to them.51Adequately explaining AI research to patients and the public willhelp in getting a social license for research: research that is legiti-mate and compliant but does not have social license can be sub-ject to constant challenge.52 This was manifested in the initiative toshare general practitioner data in England (called care.data), whichultimately failed because of insufficient public outreach.52As AI is incorporated in healthcare, involving patients and thepublic will help build confidence in these technologies.Ethical considerations need to be embedded in the design anddeployment of AI solutions.53,54 One of the ethical principles ismulti-stakeholder collaboration.54 In the context of AI in health-care, this means involving patients and carers in the designand deployment of AI tools.AI practitioners in healthcare also need to think critically aboutthe ethics of data, algorithms, and practices.55 The ethics of datafocusses on data collection, re-identification risk, and privacy.The ethics of algorithms relates to the explainability of complexmachine learning algorithms. Finally, the ethics of data sciencepractice deals with unforeseen and undesired consequencesof using machine learning algorithms.Participatory design is an integral part of data science anddata ethics.56 There is a need for researchers to think criticallyabout the implications of their work and engage with the peoplewho can be affected by their work.56 Engaging with patients andmaking them active co-producers of research will help data sci-entists continually reflect on the broader ethical and moral impli-cations of their work. Patients should be made active partici-pants in AIresearch on healthcare, as is being done ingenomic research.57Working closely with patients will help AI researchers reflectcritically on the ethics of the algorithms they implement andPatterns 3, June 10, 2022 7llOPEN ACCESSremind them of their moral obligation to help patients. There is aneed to critically reflect, at each step of the scientific process, onthe ethical and moral implications of AI research in healthcare. Ithas been suggested that such continualreflection shouldbecome best practice in data science.56As AI becomes more regulated, AI researchers should beincentivized to have an (at least informal) moral code of conductand be cognizant of the impact of their work on patients. This canalso be achieved by working in close partnership with patients,the ultimate beneficiaries of research. Guidelines for PPI by theNational Institute for Health and Care Excellence (NICE) are astep in the right direction.58AI researchers have a moral obligation to explain to patientshow their data can help them. This has been called the duty ofeasy rescue.50 It has been argued that we have a moral andethical responsibility to help other people, if this action causeslittle harm or discomfort to us.59Participative design has also been used in other domains, likesustainability science, human-wildlife co-existence,60 and socialinfrastructure design.61 PPI has been used in co-designingcomplex mathematical models of infectious diseases.62 Thiscan be used to build public confidence in modeling predictionsand policies since research is co-produced with members ofthe public. Co-developmentis also used in transformativetechnologies, like gene drives for eradicating malaria, wherethe broader societalimplications need to be debated andcommunity support needs to be mobilized.63Co-defining problems with stakeholders is important in otherdomains as well, like public and social sector organizations.Co-design has been used to reform the education sector, reducesocial isolation in adults with cognitive disabilities, and restorechildren from foster care to birth families.18Blindly promoting adoption of AI without consideration of theimpact of these technologies can be detrimental. The EuropeanCommission has suggested co-design and public consultationas key components to build trust in future AI systems.64 Tighterregulations have been proposed in AI,65 as AI gets employed inhuman sentiment and emotion analysis,66 border control basedrecognition,67 and recidivism prediction.68,69 Co-on facialdesigning and actively involving the general public will help buildtrust in future AI applications.Some principles for trustworthy AI have been proposed by theEuropean Commission and are based on fundamental rights out-lined in the European Union Charter of Fundamental Rights.70They are (1) respect for human autonomy, (2) prevention of harm,(3) fairness, and (4) explainability. AI practitioners need to reflecton these principles and the broad implications of their work.There is inadequate and imbalanced stakeholder representa-tion in many of these discussions of AI.70 Citizens and represen-tatives from civil society are absentin discussions of AItechnologies in Europe.70Many have suggested that AI poses systemic risks.71 One wayto mitigate these risks in healthcare is to increase stakeholder(patient and carer) participation.Reporting guidelines and checklists have been proposed forclinical trials that include AI.72 Such guidelines ensure reproduc-ibility of AI methods applied to healthcare. We suggest that PPIand outreach should also be an integral component of checklistsand guidelines for AI research in healthcare. A recent review has8 Patterns 3, June 10, 2022Perspectivefound that most studies do not involve co-design with patients,even though it is a critical component of digital health inter-ventions.73Concluding remarksIn this work we have outlined case studies and a methodology ofhow modern data science can be applied to healthcare using aparticipatory design loop, where data scientists, clinicians, andpatients work together. We have shared some best practicesand tools that can be used for engaging with patients andexplaining AI to them.The strategy of co-designing AI algorithms with patients is abalanced approach. This can help set more realistic expecta-tions for all stakeholders since conventional narratives of AIrevolve either around dystopia or limitless optimism. We hopethat AI research in healthcare can be adopted faster if humansslowly build up trust in machines, over repeated and carefullycalibrated interactions.The approach outlined here may have implications for miti-gating risk and misinformation about AI in healthcare. Patients,data scientists, and healthcare workers can work together,thus benefiting patients.ETHICS COMMITTEE APPROVALThe CPFT Research Database operates under UK NHS ResearchEthics approvals (REC references 12/EE/0407, 17/EE/0442).ACKNOWLEDGMENTSWe thank Jenny Nelder and Jonathan Lewis for all their help during this project.We are grateful to Prof. Geoff Walsham for helpful discussions. This work isdedicated to the memory of Jeremy Brett (Peter Jeremy William Huggins)and Dr. Sushil Ghose. This work was funded by an MRC Mental Health DataPathfinder grant (MC_PC_17213). The funders had no role in study design,data collection and analysis, decision to publish, or preparation of the manu-script. This research was supported in part by the NIHR Cambridge Biomed-ical Research Centre. The views expressed are those of the authors and notnecessarily those of the MRC, the NHS, the NIHR, or the Department of Healthand Social Care.AUTHOR CONTRIBUTIONSS.B. conceptualized, designed, and directed the study and wrote the manu-script. S.B., R.N.C., L.J., and P.A. participated in the design of the study andedited the manuscript. All authors gave final approval for publication.DECLARATION OF INTERESTSR.N.C. consults for Campden Instruments and receives royalties from Cam-bridge University Press, Cambridge Enterprise, and Routledge. S.B., L.J.,and P.A. declare no competing interests.REFERENCES1. The Lancet Digital Health (2021). Artificialintelligence for COVID-19:saviour or saboteur? Lancet Digit. Heal. 3, e1.2. (2021). Room for improvement. Nat. Mach Intell. 3, 1.3. CKD-EPI Adults NIDDK. https://www.niddk.nih.gov/health-information/professionals/clinical-tools-patient-management/kidney-disease/laboratory-evaluation/glomerular-filtration-rate-calculators/ckd-epi-adults-conventional-units.Perspective4. Herrett, E., Gallagher, A.M., Bhaskaran, K., Forbes, H., Mathur, R., vanStaa, T., and Smeeth, L. (2015). Data resource profile: clinical practiceresearch datalink (CPRD). Int. J. Epidemiol. 44, 827–836.llOPEN ACCESS27. Derrick, G., and Hettrick, S. (2022). Time to celebrate science’s ‘hidden’https://doi.org/10.1038/d41586-022-00454-3.contributors.https://www.nature.com/articles/d41586-022-00454-3.Nature.5. Post, R.M. (2018). The new news about lithium: an underutilized treatmentin the United States. Neuropsychopharmacology 43, 1174–1179.28. The Lancet Digital Health (2019). There is no such thing as race in health-care algorithms. Lancet Digit. Heal. 1, e375.6. McKnight, R.F., Adida, M., Budge, K., Stockton, S., Goodwin, G.M., andGeddes, J.R. (2012). Lithium toxicity profile: a systematic review andmeta-analysis. Lancet 379, 721–728.29. McCradden, M.D., Joshi, S., Mazwi, M., and Anderson, J.A. (2020). Ethicallimitations of algorithmic fairness solutions in health care machinelearning. Lancet Digit. Heal. 2, e221–e223.7. Hajek, T.(2011). Discontinuation of lithium because of side effects.J. Psychiatry Neurosci. 36, E39–E40.30. Leufer, D. (2020). Why we need to bust some myths about AI. Patterns 1,100124.8. Sokol, K., and Flach, P. (2018). Conversational explanations of machinelearning predictions through class-contrastive counterfactual statements.In Proc. Twenty-Seventh Int. Jt. Conf. Artif. Intell. California: InternationalJoint Conferences on Artificial Intelligence Organization, pp. 5785–5786.31. Papert, S. (1986). Constructionism: A New Opportunity for Elementary Sci-ence Education (Massachusetts Institute of Technology, Media Labora-tory, Epistemology and Learning Group). https://books.google.co.uk/books/about/Constructionism.html?id=0N8-HAAACAAJ&redir_esc=y.9. Miller, T. (2019). Explanation in artificial intelligence: insights from the so-cial sciences. Artif. Intell. 267, 1–38.10. Banerjee, S., Lio, P., Jones, P.B., and Cardinal, R.N. (2021). A class-contrastive human-interpretable machine learning approach to predictmortality in severe mental illness. NPJ Schizophr 7, 1–13.11. Cardinal, R.N. (2017). Clinical records anonymisation and text extraction(CRATE): an open-source software system. BMC Med. Inform. Decis.Mak 17, 50.12. Cunningham, H., Tablan, V., Roberts, A., and Bontcheva, K. (2013). Get-ting more out of biomedical documents with GATE’s full lifecycle opensource text analytics. PLoS Comput. Biol. 9, e1002854.13. Wang, T., Oliver, D., Msosa, Y., Colling, C., Spada, G., Roguski, q., Folarin,A., Stewart, R., Roberts, A., Dobson, R.J., and Fusar-Poli, P. (2020). Imple-mentation of a real-time psychosis risk detection and alerting systembased on electronic health records using cogstack. J. Vis. Exp.,2020.10.3791/60794.32. Stager, G.S. (2016). Seymour papert (19282016). Nature 537, 308.33. Ng, A.. What machine learning can and cannot do — Coursera. https://www.coursera.org/learn/ai-for-everyone/lecture/rv1fW/what-machine-learning-can-and-cannot-do.34. Ng, A.. More examples of what machine learning can and cannot do - whatis AI? — Coursera. https://www.coursera.org/lecture/ai-for-everyone/more-examples-of-what-machine-learning-can-and-cannot-do-9n83j.35. Borg, M., Karlsson, B., Lofthus, A.M., and Davidson, L. (2011). ‘‘Hitting thewall’’: lived experiences of mental health crises. Int. J. Qual. Stud. HealthWell-being 6. https://doi.org/10.3402/qhw.v6i4.7197.36. Ball, S., Harshfield, A., Carpenter, A., Bertscher, A., and Marjanovic, S.(2019). Patient and Public Involvement in Research: Enabling MeaningfulContributionshttps://www.rand.org/pubs/research_reports/RR2678.html.Corporation).(RAND37. Walsham, G. (2006). Doing interpretive research. Eur. J. Inf. Syst. 15,320–330.14. Lenzen, V.F., and Craik, K.J.W. (1944). The nature of explanation. Philos.Rev. 53, 503. https://www.jstor.org/stable/2181361?origin=crossref.38. Kellogg, K.C. (2019). How to orchestrate change from the bottom up. Harv.Buiness Rev. 1–6.15. Adjekum, A., Blasimme, A., and Vayena, E. (2018). Elements of trust in dig-ital health systems: scoping review. J. Med. Internet Res. 20, e11254.39. Gill, P.J., and Cartwright, E. (2020). Partnering with patients in the produc-tion of evidence. BMJ Evidence-based Med. 26, 73–76.16. Goold, S.D., and Lipkin, M. (1999). The doctor-patient relationship: chal-lenges, opportunities, and strategies. J. Gen. Intern. Med. 14, S26–S33.17. Maxmen, A.. The doctor who beat ebola — pulitzer center. https://pulitzercenter.org/reporting/doctor-who-beat-ebola-and-inspires-other-survivors-care-sick.18. van der Bijl-Brouwer, M. (2019). Problem framing expertise in public andsocial innovation. She Ji 5, 29–43.19. Big data illustration tool. https://www.climbproject.org.uk/big-data-illustration.20. Facial expression recognition tool. https://www.climbproject.org.uk/machine-learning-webcam.21. Gender Shades MIT Media Lab. https://www.media.mit.edu/projects/gender-shades/results/.22. Pabst, A.A. (2021). Afro Algorithms: imagining new possibilities for race,technology, and the future through animated storytelling. Patterns 2,100327.23. Kleanthous, S., Kasinidou, M., Barlas, P., and Otterbacher, J. (2021).future developers’fairness in algorithmic decisions:Perception ofperspective. Patterns 0, 100380.40. Walsham, G. (1997). In Actor-Network Theory and IS Research: CurrentStatus and Future Prospects, Inf. Syst and Qual. Res, eds. (Boston, MA:Springer US), pp. 466–480. http://link.springer.com/10.1007/978-0-387-35309-8_23.41. Valtiner, D., and Reidl, C. (2021). On change management in the age ofartificial intelligence: a sustainable approach to overcome problems inadapting to a disruptive, technological transformation. J. Adv. Manag.Sci. 53–58.42. Orlowski, S., Matthews, B., Bidargaddi, N., Jones, G., Lawn, S., Venning,A., and Collin, P. (2016). Mental health technologies: designing with con-sumers. JMIR Hum. Factors 3, e4.43. Eshete, B. (2021). Making machine learning trustworthy. Science 373,743–744.44. Taddeo, M., and Floridi, L. (2018). How AI can be a force for good. Science361, 751–752.45. Mathur, R., Rentsch, C.T., Morton, C.E., Hulme, W.J., Schultze, A., MacK-enna, B., Eggo, R.M., Bhaskaran, K., Wong, A.Y., Williamson, E.J., and(2021). Ethnic differences in SARS-CoV-2 infection andForbes, H.COVID-19-related hospitalisation,intensive care unit admission,and death in 17 million adults in England: an observational cohort studyusing the OpenSAFELY platform. Lancet 397, 1711–1724.24. Leufer, D.. AI Myths. https://www.aimyths.org/.46. DataSHIELD. DataSHIELD — Newcastle university. http://www.datashield.25. Teachable Machine. https://teachablemachine.withgoogle.com/v1/.26. Davis, S.R., Peters, D., Calvo, R.A., Sawyer, S.M., Foster, J.M., and Smith,L. (2018). Kiss myAsthma: using a participatory design approach todevelop a self-management app with young people with asthma.J. Asthma 55, 1018–1027.47. Scholtens, S., Jetten, M., van Gelder, C.W.G., Bo¨ hmer, J., Staiger, C.,Slouwerhof, I., van der Geest, M., and van Gelder, C.W.G. (2019). TowardsFAIR Data Steward as Profession for the Lifesciences. Report of a ZonMwFunded Collaborative Approach Built on Existing Expertise. https://zenodo.org/record/3471708#.XZ77S_ZuJPa.ac.uk/.Patterns 3, June 10, 2022 9llOPEN ACCESSPerspective48. Devaraju, A., and Huber, R. (2021). An automated solution for measuringthe progress toward FAIR research data. Patterns 2, 100370.49. Goodman, B., and Flaxman, S. (2017). European union regulations onto explanation’’. AI Mag.algorithmic decision making and a ‘‘right38, 50–57.50. Mann, S.P., Savulescu, J., and Sahakian, B.J. (2016). Facilitating theethical use of health data for the benefit of society: electronic health re-cords, consent and the duty of easy rescue. Philos. Trans. R. Soc. A.Math. Phys. Eng. Sci. 374, 20160130.51. Hill, E.M., Turner, E.L., Martin, R.M., and Donovan, J.L. (2013). let’s get thebest quality research we can’’: public awareness and acceptance of con-sent to use existing data in health research: a systematic review and qual-itative study. BMC Med. Res. Methodol. 13, 72.52. Carter, P., Laurie, G.T., and Dixon-Woods, M. (2015). The social licence forresearch: why care.data ran into trouble. J. Med. Ethics 41, 404–409.53. Raso, F., Hilligoss, H., Krishnamurthy, V., Hilligoss, H., Hilligoss, H., Krish-namurthy, V., et al.. Artificial intelligence & human rights. https://cyber.harvard.edu/publication/2018/artificial-intelligence-human-rights.54. Fjeld, J., Achten, N., Hilligoss, H., Nagy, A., and Srikumar, M. (2020). Prin-cipled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI (SSRN Electron J.).55. Floridi, L., and Taddeo, M. (2016). What is data ethics? Philos. Trans. R.Soc. A. Math. Phys. Eng. Sci. 374. https://doi.org/10.1098/rsta.2016.0360.56. Leonelli, S. (2016). Locating ethics in data science: responsibility andaccountability in global and distributed knowledge production systems.Philos. Trans. R. Soc. A. Math. Phys. Eng. Sci. 374, 20160122.57. Kaye, J., Curren, L., Anderson, N., Edwards, K., Fullerton, S.M., Kanello-poulou, N., Lund, D., MacArthur, D.G., Mascalzoni, D., Shepherd, J.,and Taylor, P.L. (2012). From patients to partners: participant-centric ini-tiatives in biomedical research. Nat. Rev. Genet. 13, 371–376.58. (2021). Patient and public involvement policy. https://www.nice.org.uk/about/nice-communities/nice-and-the-public/public-involvement/public-involvement-programme/patient-public-involvement-policy.59. Singer, P. (1972). Famine, affluence, and morality. Philos. Public Aff. 1,229–243.60. Jiren, T.S., Riechers, M., Kansky, R., and Fischer, J. (2021). Participatoryscenario planning to facilitate human-wildlife coexistence. Conserv. Biol.35, 1957–1965.61. van der Bijl-Brouwer, M. (2017). Designing for socialinfrastructures incomplex service systems: a human-centered and social systems perspec-tive on service design. She Ji 3, 183–197.62. Staniszewska, S., Hill, E.M., Grant, R., Grove, P., Porter, J., Shiri, T., Tulip,S., Whitehurst, J., Wright, C., Datta, S., and Petrou, S. (2021). Developing aframework for public involvement in mathematical and economic model-ling: bringing new dynamism to vaccination policy recommendations.Patient 14, 435–445.63. Hartley, S., Ledingham, K., Owen, R., Leonelli, S., Diarra, S., and Diop, S.(2021). Experimenting with co-development: a qualitative study of genedrive research for malaria control in Mali. Soc. Sci. Med. 276, 113850.64. Leufer, D., and Janse, F.. The EU is funding dystopian Artificial Intelligencehttps://www.euractiv.com/section/digital/opinion/the-eu-is-projects.funding-dystopian-artificial-intelligence-projects/.65. Taddeo, M., and Floridi, L. (2018). Regulate artificial intelligence to avertcyber arms race comment. Nature 556, 296–298.66. Sewa project. https://www.sewaproject.eu/.67. iBorderCtrl. https://www.iborderctrl.eu/.68. Rudin, C., Wang, C., and Coker, B. (2020). The age of secrecy and unfair-ness in recidivism prediction. Harv. Data Sci. Rev. 2. https://hdsr.mitpress.mit.edu/pub/7z10o269.69. Dressel, J., and Farid, H. (2018). The accuracy, fairness, and limits of pre-dicting recidivism. Sci. Adv. 4, eaao5580.70. Hidvegi, F., and Leufer, D. (2019). Laying Down the Law on AI: Ethics Done,Now the EU Must Focus on Human Rights - Access Now. https://www.accessnow.org/laying-down-the-law-on-ai-ethics-done-now-the-eu-must-focus-on-human-rights/.71. Galaz, V., Centeno, M.A., Callahan, P.W., Causevic, A., Patterson, T.,Brass, I., Baum, S., Farber, D., Fischer, J., Garcia, D., and McPhearson,T. (2021). Artificial intelligence, systemic risks, and sustainability. Technol.Soc. 67, 101741.72. Mateen, B.A., Liley, J., Denniston, A.K., Holmes, C.C., and Vollmer, S.J.(2020). Improving the quality of machine learning in health applicationsand clinical research. Nat. Mach Intell. 2, 554–556.73. Bergin, A.D., Vallejos, E.P., Davies, E.B., Daley, D., Ford, T., Harold, G.,Hetrick, S., Kidner, M., Long, Y., Merry, S., and Morriss, R. (2020). Preven-tive digital mental health interventions for children and young people: a re-view of the design and reporting of research. Npj Digit. Med. 3, 1–9.About the authorsSoumya Banerjee is a Senior Research Fellow at the University of Cambridge.He worked in industry for many years before completing a PhD in applyingcomputational techniques to interdisciplinary topics. Over the last 18 years,he has worked closely with domain experts in finance, healthcare, immu-nology, virology, and cell biology. He has recently worked closely with clini-cians and patients on using patient and public involvement to build trust inAI algorithms.Phil Alsop is now retired from a career in the Civil Service. He began PPI inmental health research over 10 years ago, bringing along personal experienceof bipolar disorder. He also ran self-help groups and an online forum for peoplewith bipolar disorder. Seven years ago, Phil joined the CUH PPI panel and hasparticipated in document reviews, focus groups, and management meetingsfor many health conditions. He has recently been actively involved in projectsrelating to public attitudes to stratification of cancer screening.Linda Jones trained as a general nurse (RGN 1986) and spent many years asan oncology research sister at University College London and Addenbrooke’sHospital, Cambridge. In her last oncology role, she created a translationalresearch service at Addenbrooke’s with Cancer Research UK scientists.In 2018, she was appointed PPI lead for informatics, Department of Psychiatry,University of Cambridge. She recently led a UK-wide survey study, designedwith service users, examining health data sharing to transform care andimprove research. Linda also leads the PPI workstream for the HDR UK/MRC national mental health data hub, DATAMIND.Rudolf N. Cardinal trained in behavioral neuroscience (BA 1996, PhD 2001,MD 2006), general medicine (MB BChir 2001, MRCP 2007), and general/liaisonpsychiatry (MRCPsych 2009, CCT 2015). His research covers computationalpsychiatry and clinical informatics, including computational modeling of psy-chopathology, open-source software for patient-facing data capture, properuses of clinical data, and epidemiology using de-identified data, particularlyat the mental/physical health interface. He is associate professor of clinicalinformatics in the Department of Psychiatry, University of Cambridge, andan honorary consultant liaison psychiatrist at Addenbrooke’s and the RosieHospitals (Cambridgeshire and Peterborough NHS Foundation Trust; CUHNHS Foundation Trust).10 Patterns 3, June 10, 2022