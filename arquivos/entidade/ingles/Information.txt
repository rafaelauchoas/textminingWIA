Artificial Intelligence 182–183 (2012) 1–31Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintInformation-geometric approach to inferring causal directionsDominik Janzing a,∗Povilas Daniušis e, Bastian Steudel f, Bernhard Schölkopf a, Joris Mooij b, Kun Zhang a, Jan Lemeire c,d, Jakob Zscheischler a,a Max Planck Institute for Intelligent Systems, Tübingen, Germanyb Radboud University, Nijmegen, Netherlandsc Vrije Universiteit Brussel, Brussels, Belgiumd Interdisciplinary Institute for Broadband Technology, Ghent, Belgiume Vilnius University, Lithuaniaf Max Planck Institute for Mathematics in the Sciences, Leipzig, Germanya r t i c l ei n f oa b s t r a c tArticle history:Received 21 December 2010Received in revised form 10 January 2012Accepted 10 January 2012Available online 12 January 2012Keywords:Deterministic causal relationsPythagorean tripleCause–effect pairsWhile conventional approaches to causalinference are mainly based on conditional(in)dependences, recent methods also account for the shape of (conditional) distributions.The idea is that the causal hypothesis “ X causes Y ” imposes that the marginal distributionP X and the conditional distribution P Y | X represent independent mechanisms of nature.Recently it has been postulated that the shortest description of the joint distribution P X,Yshould therefore be given by separate descriptions of P X and P Y | X . Since descriptionlength in the sense of Kolmogorov complexity is uncomputable, practical implementationsrely on other notions of independence. Here we define independence via orthogonalityin information space. This way, we can explicitly describe the kind of dependence thatoccurs between P Y and P X|Y making the causal hypothesis “Y causes X” implausible.Remarkably, this asymmetry between cause and effect becomes particularly simple if Xand Y are deterministically related. We present an inference method that works in thiscase. We also discuss some theoretical results for the non-deterministic case although it isnot clear how to employ them for a more general inference method.© 2012 Elsevier B.V. All rights reserved.1. IntroductionThe problem of inferring whether X causes Y (write X → Y ) or Y causes X from observations (x1, y1), . . . , (xm, ym)that are i.i.d. drawn from P X,Y is a particularly challenging task for causal inference [1]. Although this restricted problemignores other important problems of causal inference (i.e., unobserved common causes or bidirectional influence), it isuseful for studying statistical asymmetries between cause and effect. Conventional methods for causal inference [2,3] focuson conditional independences and thus require observations from at least three variables.Extending an idea in [4,5] postulates that X → Y is only acceptable as causal hypothesis if the shortest description ofP X,Y is given by separate descriptions of P Y | X and P X . Here description length is understood in the sense of algorithmicinformation (“Kolmogorov complexity”) [6–8]. Note that the postulate is equivalent to saying that P Y | X and P X are algorith-mically independent in the sense that knowing P X does not enable a shorter description of P Y | X and vice versa. To showthat this helps in distinguishing between cause and effect for just two observed variables, [5] constructed toy models ofcausal mechanisms where the causal structure X → Y yields algorithmic dependences between P X|Y and P Y . Even though* Corresponding author.E-mail address: dominik.janzing@tuebingen.mpg.de (D. Janzing).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2012.01.0022D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31SectionContentsSection 3Section 4Postulating independence conditions (h1)–(h3) forP cause and P effect|causeJustifying the conditionsRephrasing (h1)–(h3) as orthogonalityImplications of (h3) for deterministic causalityGeneralizing (h3) via exponential familiesInference method for deterministic case based on thegeneralized condition (h3)Main referencePostulate 1 and Definition 1Lemmas 1, 2Theorem 1Theorem 2Postulate 2Sections 4.3 and 4.4Appendix AOutlook: employing orthogonality for inferring non-deterministic relations (toy examples, negative results)Lemmas 9 and 10Fig. 1. Structure of the main results.algorithmic independence between P cause and P effect|cause is an appealing formalization of independence, practical methodsmust be based on computable criteria.[9,10] described a potential asymmetry between cause and effect where independence is meant in terms of statisticalindependence between the cause and the noise term that occurs in the causal mechanism: If Y is a function of X up to anadditive noise term that is statistically independent of X , i.e.,Y = f (X) + E with E ⊥⊥ X,(1)then there is usually (up to some exceptions like the bivariate Gaussian) no such additive noise model from Y to X . In otherwords, writing X as X = g(Y ) + ˜E with some function g will not render the residual term ˜E statistically independent of Y .[11] generalizes the model class to(cid:2)Y = hf (X) + E(cid:3)with E ⊥⊥ X,(2)and show that such a “post-nonlinear (PNL) model” also exists in at most one direction, except for some special cases. IfP X,Y is consistent with (1) or (2), respectively, in one direction but not the other, one infers that direction to be the causalone that is implied by the corresponding model. For the model (1) it has been shown [12] that this kind of reasoning isjustified by the above algorithmic independence principle.Note that these inference methods do not assume that causal relations are always of the above form. They only decidefor one of the causal directions if one and only one direction admits such a model. The idea is the following: if X → Y isthe correct model, but not of the additive noise form, it is rather unlikely that it generates a joint distribution that admitsan additive noise model in the opposite direction. The reason is that this would require rather contrived adjustmentsbetween P X (the marginal distribution of the hypothetical cause) and P Y | X (the conditional distribution of the effect, giventhe cause) [12]. This article develops an information-geometric principle that does not require the restricted class of additivenoise or post-nonlinear models. To this end, we revisit additive noise models in Section 2 and show that entropies can playa key role in describing the kind of dependences between P X|Y and P Y that can occur if X causes Y . This motivates ourinformation-geometric perspective developed in Section 3, which results in an inference method for deterministic causalrelations in Section 4, with an outlook for the non-deterministic case in Appendix A. The table in Fig. 1 shows how themain results are structured.Readers who are only interested in our inference method may focus on Section 4, with Sections 4.3 and 4.4 as its mainparts. The other sections provide a general background and describe a large class of asymmetries between cause and effectthat could be helpful for developing other information-theoretic methods in the future.2. Information-theoretic view on additive noise modelsWe consider the additive noise model (1) in the low noise regime (see Fig. 2) and show how the relationship betweenthe input distribution and the conditional one is different for both directions. We use the following notational conventions.P Y |x is the distribution of Y , given a fixed value x while P Y | X denotes the entire conditional distribution. The range ofa random variable X will be denoted by D X . S(P Y |x) denotes the (differential) Shannon entropy of P Y |x for fixed x. Thefunction x (cid:5)→ S(P Y |x) will also be called the conditional entropy function. Throughout the paper we will assume that alldistributions have densities with respect to a fixed reference measure (e.g., the Lebesgue measure for real-valued variablesor the counting measure for discrete variables). This measure will never appear explicitly and should not be confused withreference probability distributions that occur all over the article. By slightly overloading notation, P X will stand for boththe distribution and the density x (cid:5)→ P X (x). We will also write P (x) instead of P X (x) whenever this causes no confusion.· · · P (x) dx will be understood as sums by interpreting dx as dμ(x) where μFor discrete variables X , integrals of the formdenotes the counting measure.Regarding (1) we observe that E ⊥⊥ X ensures that the conditional entropy function S(P Y |x) is constant in x and coincidesS(P Y |x)P (x) dx). In studying how P Y and P X|Y are then( y)|( y) is large for those y-values where | fwith the conditional entropy S(P Y | X ) (defined by the averagerelated we first assume that P X is uniform. Then, P ( y) ≈ P X ( f−1( y)) · f−1−1(cid:4)(cid:4)(cid:7)(cid:7)D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–313Fig. 2. Functional relation with small noise. The conditional entropy function S(P X| y ) is high at regions with high slope of fthis point.−1( y), i.e., small slope of f atis large. At the same time, the entropy S(P X| y) is large for y-values in regions with large | f( y)| (see Fig. 2). Hence, largeentropy S(P X| y) correlates with high density P ( y), assuming that P (x) is constant on the interval under consideration. If−1( y)) are high. WeP X is not the uniform distribution, high values of P ( y) occur at points where both | fargue later that if the peaks of P (x) do not correlate with the slope of f then the qualitative argument above still holdsand S(P X| y) again correlates with P ( y). This reasoning will be formalized in Section 3.( y)| and P X ( f−1(cid:7)(cid:7)−1The first information-geometric inference principle that we are going to state in the next section no longer assumes thatthe entropy S(P Y |x) is constant in x if X → Y is the true causal direction. Instead, it postulates that regions of large S(P Y |x)do not correlate with regions of large density P (x). The example above shows that dependences between P Y and P X|Yoccurring for the wrong causal direction can appear on the level of correlations between information-theoretic expressions(like conditional entropy) computed from the conditional P X| y and the density P ( y). We will show that correlations of thistype can be phrased as an orthogonality relation in the sense of information geometry.3. A class of testable independence relationsThe intention of this section is to postulate independence conditions between P Y | X and P X that can be tested empirically.We will describe several options to solve this task.3.1. General structure of our independence relationsThe following postulate describes the general structure that all our postulates share:Postulate 1 (General structure of independence). Assume that X causes Y . Let x (cid:5)→ h(x) ∈ R be any function for which h(x)describes local properties of the conditional P Y | X at the point X = x.1 Then the “structure function” h and P X are likely tosatisfyh(x)P (x) dx ≈h(x)U X (x) dx,(3)where U X is a reference density for X (not necessarily uniform).Note that the difference between both sides of (3) can be rephrased as a covariance if we formally consider h and P X /U X(cid:5)(cid:5)(cid:5)(cid:5)as functions of a random variable X with distribution U X :(cid:5)(cid:5)h(x)P (x) dx −h(x)U (x) dx =U (x) dx −h(x)U (x) dxP (x)U (x)U (x) dx(4)(cid:5)P (x)h(x)U (x)(cid:6)=: CovU Xh,(cid:7).P XU XTherefore (3) formalizes uncorrelatedness between the functions h and P X /U X , which is justified by the idea that the wayP X differs from U X is independent of h.The postulate remains vague regarding how to choose h and U X . We will later discuss different reasonable choices,(cid:7)(x)for instance h(x) := S(P Y |x) (for non-deterministic relations), h(x) := f(cid:7)(x) (for deterministic ones) and also h(x) := log f1 Note that we have avoided the more concise formulation “h(x) describes properties of the conditional P Y |x” for the following reason: For deterministicrelations Y = f ( X), the function h(x) := f(cid:7)(x) expresses a property of P Y |X that is local at X = x, but h(x) cannot be derived from P Y |x alone.4D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Fig. 3. Visualization of two options for interval-wise generation of a conditional P Y |X via dice throws r j . Left: P Y |X corresponding to Y = X + E where thedistribution of E|x is the uniform distribution on the interval [0, h(x)]. Right: the dice determines the slope of f for Y = f ( X) via f(cid:7)(x) := h(x).(for deterministic monotonically increasing relations). We recommend to choose “non-informative” distributions like uniformones or Gaussians for U X . If we assume that “typical” choices of P X (as long as the choice is independent of h) yield almost(cid:7)the same integral, we also have to assume that changing U X to some UX does not matter too much as long as we havechosen U(cid:7)X independently of h. This suggests some robustness under changing the reference measure.3.2. Probabilistic models as justificationEven after specifying the reference density U X and the map from the conditional P Y | X to its structure function h,a mathematical justification of (3) can only be given within a probabilistic model about how “nature chooses P X ” or howit “chooses P Y | X ”. To show this, we now consider a random process that generates functions h (which can equivalently beseen as generating random conditionals P Y | X ):Lemma 1 (Interval-wise random generation of P Y | X ). Let X, Y be real-valued. Let r j > 0 with j ∈ Z be random numbers i.i.d. drawnfrom a distribution Q (r) with standard deviation σr . We then define a piecewise constant function h via h(x) := r j for x ∈ [ j, j + 1)(Fig. 3 shows two options how h may correspond to a conditional P Y | X ). We then have for every c > 0,(cid:9)(cid:10)(cid:10)(cid:10)(cid:10)(cid:11)(cid:8)(cid:8)(cid:8)(cid:8) (cid:2) cσrh(x)U (x) dxP (x) − U (x) dxh(x)P (x) dx −(cid:13) j+1(cid:5)(cid:8)(cid:5)(cid:8)(cid:8)(cid:8)(cid:14)2(cid:12)(cid:5),with probability 1 − 1/c2 or higher.Proof. This is because(cid:5)(cid:2)h(x)(cid:3)P (x) − U (x)dx =jj(cid:12)r jj(cid:13) j+1(cid:5)(cid:14)P (x) − U (x) dxjis the sum of independent random variables, each having variance(cid:13) j+1(cid:5)(cid:14)2σ 2rP (x) − U (x) dx.jThen the statement follows from Chebyshev’s inequality, noting that the expected value ofishes. (cid:2)(cid:4)h(x)(P (x) − U (x)) dx van-The example is instructive because it shows that (3) is likely to hold regardless of P X and U X provided that the followingconditions are satisfied: First, both distributions P X and U X have been chosen independently and independently of P Y | X .P (x) − U (x) dx)2 is small. Roughly speaking,Second, both distributions are sufficiently spread out such that β :=if P X and U X have width n, then β ∈ O (1/n) and hence (3) holds up to an error in O (1/n). Neglecting one of theseconditions, one can easily construct counter examples: First, if one of the distributions P X or U X , say U X , is constructedafter having seen all the r j , U X can be constructed such that P X − U X is positive for all intervals [ j, j + 1) where r j is large(cid:4)j(j+1j(cid:15)√D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–315(cid:4)(cid:4)and negative for small r j . This results indistributions, say U X , is supported by one interval [ j, j + 1) only, we havedepends on a single r j only and can therefore strongly deviate from(cid:4)h(x)U (x) dx being systematically greater thanh(x)P (x) dx.(cid:4)h(x)U (x) dx. Second, if one of theh(x)U (x) dx = r j , i.e., the right hand side of (3)One can also construct a probabilistic model where P Y | X and thus h is fixed and instead P X is generated randomly, forinstance by the following procedure. On each interval [ j, j + 1) multiply U (x) by some random number r j . Then renormalizethe obtained function to obtain P (x). If U X is spread out over many intervals, (3) holds with high probability. We haveskipped the detailed analysis of this example because it becomes too technical.The following model assumes that P X is chosen from a prior that is invariant under a group action:Lemma 2 (Symmetric prior). Let G be a group acting on the domain of X and P(P X ) be a probability density on the set of distributionsP X that is G-invariant, i.e.,P(P X ) = P(P X ),where P X denotes the average of P X over the action of G.Then, for any fixed h we have(cid:5)(cid:5)EPh(x)P X (x) dx = EPh(x)P X (x) dx,where EP denotes the expectation over the prior P .The result follows immediately from linearity of the expectation. It suggests to choose P X as reference measure wheneverone believes that a G-invariant prior is appropriate. The fact that then the expectations of both sides of (3) coincide doesh(x)P X (x) dx are close with high probability. However, for “sufficientlynot necessarily guarantee thatlarge” groups, this follows indeed from concentration-of-measure results (see [13] and [14] for a similar statement withrotations in high-dimensional spaces). To elaborate on this for general groups would go beyond the scope of this paper.h(x)P X (x) dx and(cid:4)(cid:4)We have seen that the degree to which we can trust (3) heavily relies on the particular probabilistic models for gener-ating P X and P Y | X . Therefore, we cannot provide any confidence levels that would be valid without referring to one of themodels above. After deciding, for instance, that the example in Lemma 1 is a good model for the generation of P Y | X westill need to estimate the size of the intervals that correspond to independent random experiments. Then, we only believein (3) if the interval sizes are sufficiently small compared to the width of P X and U X . Example 2 in Section 4 shows, in thecontext of deterministic relations, that violation of (3) can easily happen for very simple P Y | X and P X if P X and U X differin large regions.We also want to mention that Postulate 1 may fail due to “intelligent design” of P X and P Y | X . This is a fundamentallimitation not only of our approach, but also of well-known postulates for causal inference like causal faithfulness [2].3.3. Independence as orthogonality in information spaceOur structure functions will be relative-entropy-like expressions because these turned out to be helpful for formalizingasymmetries between cause and effect. We introduce this terminology now. For two densities P , Q for which P is absolutelycontinuous with respect to Q , the relative entropy (or KL-distance) is defined by(cid:5)D(P (cid:10) Q ) :=logWe then define:P (w)Q (w)P (w) dw (cid:3) 0.Definition 1 (Structure functions for the conditional). Let U X and U Y be reference densities for X and Y , respectively anddenote the output distribution obtained by feeding the conditional with the reference input U X . Similarly, we will later use(cid:5)−→P Y :=(cid:5)←−P (x) :=P ( y|x)U (x) dxP (x| y)U ( y) dy.(cid:5)P ( y|x) dy = D(P Y |x (cid:10) U Y ),logh1(x) :=Then we define the following “structure functions”:P ( y|x)U ( y)P ( y|x)−→P ( y)−→P ( y)U ( y)P ( y|x) dy = D(P Y |x (cid:10)h2(x) :=h3(x) :=loglogP ( y|x) dy = h1(x) − h2(x).−→P Y ),(cid:5)(cid:5)6D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31The reason that we list all three of these functions though the third one can be represented in terms of the other twois because they all yield conditions that have an interpretation in terms of information geometry, relying on the followingconcept. Three densities (P , R, Q ) are said to form a Pythagorean triple of distributions ifD(P (cid:10) Q ) = D(P (cid:10) R) + D(R (cid:10) Q ).(5)This terminology is motivated by interpreting relative entropy as a squared distance and the triple thus satisfies thePythagorean theorem. If condition (5) holds we say that the vector connecting P with R is orthogonal to the one con-necting R with Q are orthogonal but keep in mind that this relation is neither symmetric with respect to exchanging thevectors with each other, nor with respect to reversing the arrows.We will also use the following formulation:Lemma 3 (Orthogonality in information space). Orthogonality (5) is equivalent to(cid:5)logR(w)Q (w)(cid:5)P (w) dw =logR(w)Q (w)R(w) dw.(6)The proof is given by straightforward computation. In analogy to our interpretation of (3), we can interpret (6) as “theintegral over the log term does not depend on whether it is weighted with P or R”. We then find:Theorem 1 (Three orthogonality conditions). The conditions CovU X (hi, P X /U X ) = 0 for i = 1, 2, 3 are equivalent toD(P Y , X (cid:10) U X U Y )h1= D(P Y , X (cid:10) U X P Y | X ) + D(U X P Y | X (cid:10) U X U Y ),h2= D(P Y , X (cid:10) U X P Y | X ) + D(U X P Y | X (cid:10) U X−→P Y (cid:10) U Y ).−→P Y ) + D(−→P Y )h3= D(P Y (cid:10)−→P Y ),D(P Y , X (cid:10) U XD(P Y (cid:10) U Y )Proof. Using Lemma 3, the cases h1 and h2 are straightforward computations. For case h3 note that(cid:5)−→P ( y)U ( y)−→P ( y)U ( y)loglog(cid:5)P ( y|x)P (x) dx dy =log(cid:5)P ( y|x)U (x) dx dy =log−→P ( y)U ( y)−→P ( y)U ( y)P ( y) dy−→P ( y) dy.(cid:2)and(cid:5)To geometrically justify the orthogonality assumption for h1, we consider the space V of functions of x, y and identifyeach distribution Q X,Y with the point(cid:2)(cid:3)(x, y) (cid:5)→ log Q (x, y)∈ V .Then we observe that the difference vector connecting the points P Y , X and U X P Y | X only depends on P X (in the sensethat the common term P Y | X cancels when taking the difference between the two points), while the vector pointing fromU X P Y | X to U X U Y only depends on P Y | X . In high-dimensional spaces it is likely that two vectors are close to orthogonalif they are chosen independently according to a uniform prior. Even though we do not know of any precise statement ofthis form with respect to information-geometric orthogonality, we accept this as another leading intuition on top of theinterpretation of “uncorrelatedness” given by Theorem 1. Regarding h2, we can argue in a similar way. The fact that both−→joint distributions occurring in the points U X P Y | X and U XP Y do not contain P X at all, makes it plausible that the vectorshould be orthogonal to any vector that only depends on P X . How to geometrically interpret the orthogonality given byh3 is, however, less clear, but it will be the essential one for Section 4 since it is the only one that is applicable to thedeterministic case. Condition (h1) will be used in the outlook in Appendix A.A simple example of a reasonable reference measure is the uniform distribution on an interval [a, b]. It is a naturalchoice whenever the data points are a priori restricted to [a, b]. For this example, the conditional relative entropy reducesto a conditional Shannon entropy:Example 1 (Uniform reference measure). Let the range of X and Y be restricted to the interval [0, 1] and U X and U Y be theuniform distributions on [0, 1]. Then the orthogonality condition h1 in Theorem 1 is equivalent to(cid:5)S(P Y |x)P (x) dx =1(cid:5)0S(P Y |x) dx,(7)D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–317Fig. 4. If the structure of the density of P X is not correlated with the slope of f , then flat regions of fis thus implausible because the causal mechanism f−1 appears to be adjusted to the “input” distribution P Y .induce peaks of P Y . The causal hypothesis Y → Xand(cid:2)(cid:3)S(P Y |x), P (x)= 0.CovU X(8)Hence, (7) states that regions with high entropy S(P Y |x) do not correlate with regions of high density P (x). If P Y | X andP X are chosen independently, we assume that this independence assumption will approximately hold. For additive noisemodels, this is always satisfied because (1) implies that S(P (Y |x)) is constant in x. We have already given an intuitiveargument (see also Fig. 2) why (7) is violated in the backward direction (in the low noise regime).We can define a group of cyclic shifts (Tt )t∈[0,1] with Tt (x) := (x + t) mod 1 having the uniform reference measure asunique invariant measure. Then the covariance in (8) vanishes on the average over all shifted copies of P X (cf. Lemma 2),although we do not have any result saying that it holds for most shifted copies approximately.To what extent the above orthogonality relations are approximately satisfied for real-world cause–effect pairs can only beanswered by extensive empirical studies. An interesting theoretical question, however, is in which cases the orthogonality inone direction imposes the violation of orthogonality for the converse direction. The simplest model class where this couldbe confirmed is given by deterministic invertible relations [15]. A remarkable fact is that, for the backward direction, h3 isalways positively correlated with the hypothetical input density (i.e., in fact the output). Appendix A discusses some caseswhere the relation between cause and effect is not bijective and only deterministic in one direction. There, we are also ableto show violations of orthogonality in backward direction, but sometimes additional independence conditions between P Xand P Y | X other than the orthogonality postulates turn out to be necessary.4. Deterministic invertible relationThe bijective case where Y = f ( X) and X = f−1(Y ) seems particularly challenging for causal inference. First, the absenceof noise makes additive noise model based inference impossible [9], and second, methods that use non-invertibility of thefunctional relation fail [16]. Surprisingly, the “hopeless” noiseless invertible case is one where the theory turns out tobe most elegant because violation of one of our orthogonality conditions in backward direction follows easily from theorthogonality in forward direction. Moreover, our simulations suggest that the corresponding inference method is robustwith respect to adding some noise; and also the empirical results on noisy real-world data with known ground truth wererather positive. This section largely follows our conference paper [15] but puts the ideas in a broader context and containsmore systematic experimental verifications.4.1. MotivationWe start with a motivating example. For two real-valued variables X and Y , let Y = f ( X) with an invertible differentiablefunction f . Let P X be chosen independently of f . Then regions of high density P Y correlate with regions where f has smallslope (see Fig. 4). The following lemma make this phenomenon more explicit:Lemma 4 (Correlations between slope and density). Let Y = f ( X), where f is a differentiable bijection of [0, 1] with differentiableinverse f(cid:5)and P X are uncorrelated in the sense that−1. If log f(cid:5)(cid:7)log f(cid:7)(x)P (x) dx =log f(cid:7)(x) dx,(9)then log( f(cid:5)−1)(cid:7)(cid:2)logf(cid:3)(cid:7)−1and P Y are positively correlated, i.e.,(cid:5)(x)P ( y) dy >(cid:7)log f( y) dy,unless f is the identity.8D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Fig. 5. Left: violation of (9) due to a too global deviation of P X from the uniform measure. Right: P X oscillating around the constant density ensuresuncorrelatedness.Note that the terminology “uncorrelated” is justified if we interpret fand P X as random variables on the probabilityspace [0, 1] with uniform measure (see the interpretation of (3) as uncorrelatedness). The lemma actually follows from moregeneral results shown later, but the proof is so elementary that it is helpful to see:(cid:7)1(cid:5)0(cid:2)logf(cid:3)(cid:7)−1( y)P ( y) dy −1(cid:5)(cid:2)logf(cid:3)(cid:7)−1( y) dylog f= −1(cid:5)01(cid:5)= −log f(cid:7)(cid:7)01(cid:5)(x)P (x) dx +log f(cid:7)(x) f(cid:7)(x) dx01(cid:5)(x) dx +log f(cid:7)(x) f(cid:7)(x) dx =00(cid:3)(cid:7)f(x) − 1log f(cid:7)(x) dx (cid:3) 0.1(cid:5)(cid:2)0The first equality uses standard substitution and exploits the fact that(cid:2)logf(cid:3)(cid:7)(cid:2)−1(cid:3)f (x)= − log f(cid:7)(x).(10)The second equality uses assumption (9), and the last inequality follows because the integral is non-negative everywhere.Since it can only vanish if Z is constant almost everywhere, the entire statement of Lemma 4 follows.Peaks of P Y thus correlate with regions of large slope off ) if X is the cause. One canshow that this observation can easily be generalized to the case where fis a bijection between sets of higher dimension.Assuming that P X is uncorrelated with the logarithm of the Jacobian determinant log |∇ f | implies that P Y is positivelycorrelated with log |∇ f−1|.f−1 (and thus small slope ofBefore embedding the above insights into our information-geometric framework we will show an example where thewhole idea fails:Example 2 (Failure of uncorrelatedness). Let f be piecewise linear with fThen(cid:7)(x) = a for all x < x0 and f(cid:7)(x) = b for all x (cid:3) x0.1(cid:5)0log f(cid:7)(x)P (x) dx −1(cid:5)0(cid:7)(cid:2)(x) dx = (log a − log b)(cid:2)(cid:3)[0, x0]P X(cid:3).− x0log fTherefore, uncorrelatedness can fail spectacularly whenever |P X ([0, x0]) − x0| is large, meaning that P X and the uniformmeasure differ on a larger scale as in Fig. 5, left. If P X only oscillates locally around 1, it still holds (Fig. 5, right).The fact that the logarithm of the slope turned out to be particularly convenient due to (10), is intimately related to our←−P X have straightforward generalizations to the determin-−1, respectively. If U X and U Y are the uniform distributions oninformation-geometric framework: We first observe thatistic case as the images of U X and U Y under f and g := f[0, 1], they are given by−→P Y and−→P ( y) := g( y) and←−P (x) := f(cid:7)(x).We thus obtain that (9) is equivalent to1(cid:5)01(cid:5)(cid:7)log g( y)P ( y) dy =log g(cid:7)( y)g(cid:7)( y) dy,0(11)D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–319which can be transformed to−→P ( y)U ( y)log1(cid:5)01(cid:5)P ( y) dy =log−→P ( y)U ( y)−→P ( y) dy0which is equivalent to orthogonality condition (h3).One can easily think of mechanisms in nature that violate the model of choosing the function f and the input distribu-tion P X independently because P X is the result of intelligent design or a long adaption process, like evolution in biologicalsystems. If the reward of a system can be optimized by controlling the value of y, P X may over time shifted toward regionsf and thus (P X , f ) may spectacularly violate (9). Such effects imply a fundamental limitation of ourwith large slope ofmethod.4.2. Identifiability resultsHere we rephrase the theory developed in [15] and further elaborate on the asymmetries between cause and effect.Orthogonality (h3) in Theorem 1 is the only one that is applicable to the deterministic case since it only refers to the imageof the uniform input density under the conditional P Y | X , which also exists in the deterministic case, while the others referto the conditional density P ( y|x) (which does not exist since it would correspond to a delta-“function”). Condition (h3) canbe rephrased in different ways:Theorem 2 (Equivalent formulations of orthogonality (h3)). For bijective relations, the following conditions are equivalent:(I) Orthogonality (h3) in Theorem 1:−→P Y ) + D(D(P Y (cid:10) U Y ) = D(P Y (cid:10)−→P Y (cid:10) U Y ).(II) Uncorrelatedness between input and transformed density:(cid:6)(cid:7)←−P XU X,P XU X= 0.CovU Xlog(III) Transformed orthogonality:D(P X (cid:10)←−P X ) = D(P X (cid:10) U X ) + D(U X (cid:10)←−P X ).(IV) Additivity of irregularities:D(P Y (cid:10) U Y ) = D(P X (cid:10) U X ) + D(−→P Y (cid:10) U Y ).(V) Additivity of approximation error:D(P X (cid:10)←−P X ) = D(P Y (cid:10)−→P Y ) + D(−→P Y (cid:10) U Y ).(cid:5)Proof. Condition (13) is equivalent to←−P (x)U (x)←−P (x)U (x)P (x) dx =loglog(cid:5)U (x) dx,(12)(13)(14)(15)(16)using (4). Due to Lemma 3, this is equivalent to (14). The equivalence between (12) and (14) is immediate by applying fto all distributions in (12) because the relative entropy is conserved under bijections. Equivalence between (15) and (12) is−1 to the term on the leftobtained by applying fand f to the first term on the right hand side, (15) is transformed into (16). (cid:2)−1 only to the first term on the right hand side of (12). By applying f−1Later in this section, a generalization of condition (15) will be our essential postulate. For this reason, we should mention−→the idea: the distance D(P X (cid:10) U X ) measures the irregularities of the input distribution and D(P Y (cid:10) U Y ) quantifies theirregularities of the function. The amount of irregularities of the output is thus given by the sum of these two terms. Thisis because the irregularities between input and function are independent, thus they neither “interfere” constructively nordestructively.−→P Y ) is the error of approximating P Y byCondition (16) also admits an interesting interpretation: assuming that U X and U Y are given by smoothing P X and P Y ,−→respectively, then D(P Y (cid:10)P Y , i.e., the image of the smoothed input. Then (16)implies that the output is less sensitive to smoothing the input than vice versa: imagine the case where some of the peaksof P Y stem from P X and some from f . By smoothing the peaks that are caused by f , we generate additional peaks on P X ,while smoothing the ones of P X just removes those in P Y that are due to the peaks in the non-smoothed P X . For all theseinterpretations it is essential that relative entropy is always non-negative.10D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Fig. 6. The orthogonality condition (I) is inconsistent with the analog orthogonality for the backward direction: since f←−P X , as it would be required.gular angle in information space at−→P Y implies a rectangular angle at U X rather than at−1 preserves all distances, a rectan-Theorem 3 (Violations in backward direction). Let f be non-trivial in the sense that the image of U X under f does not coincidewith U Y . If one condition (and thus all) in Theorem 2 holds, then the corresponding conditions that exchange the role of X and Y areviolated with definite sign:,CovU YD(P X (cid:10)(cid:6)←−←−P X ) + D(P X (cid:10) U X ) > D(P X (cid:10) U X ),−→P YP YU YU Ylog(cid:7)> 0,−→−→P Y ) > D(P Y (cid:10)P Y ),←−P X (cid:10) U X ) > D(P X (cid:10) U X ),−→←−P X (cid:10) U X ) > D(P Y (cid:10)P Y ).D(P Y (cid:10) U Y ) + D(U Y (cid:10)D(P Y (cid:10) U Y ) + D(←−P X ) + D(D(P X (cid:10)(17)(18)(19)(20)(21)Proof. Reordering (14) yieldsD(P X (cid:10) U X ) = D(P X (cid:10)←−←−←−P X (cid:10) U X ),P X ) + D(P X ) − D(U X (cid:10)−1 to some of the terms, but (20) and (21) followshowing inequality (17). Inequalities (19)–(21) then follow by applying falso directly from (15) and (16), respectively. (18) follows because the left hand side is the difference between the right−1hand and the left hand side of (19). The fact that (12) implies (17) can also be seen in Fig. 6. Moreover, the fact that fconserves the shape of the triangle shows that the discrepancy between the two sides of (17) is given by the “symmetrizedrelative entropy”←−P X ) < D(P X (cid:10)←−P X (cid:10) U X ) + D(U X (cid:10)D(←−P X ).(cid:2)Generalization to reference manifolds(22)The choice of the reference measure is the most delicate part of our method because the structure of a distribution P Xis represented by the vector connecting P X and U X .The uniform distribution on a certain interval may only be a reasonable choice if the range of the respective variableis a priori restricted to this interval. If a real-valued variable has unbounded range and finite variance, the Gaussian withthe same mean and variance as P X is a more natural candidate for U X (and likewise for Y ). However, U X then dependson P X via its mean and variance. A better way of expressing the above is then given by introducing families of referencedistributions rather than having a single reference distribution. We then measure irregularities by the distance of P X to theexponential family of Gaussians and represent the structure of P X by the vector that connects P X to its closest point in themanifold. The family of Gaussians is only one example of a reasonable choice. Even though it will turn out to be a usefulone in many cases, the theory below is phrased in terms of general exponential manifolds:Definition 2 (Exponential manifolds). Let Ω ⊆ Rd and assume a finite-dimensional vector space V of functions f : Ω → R isgiven. Then, V defines an exponential manifold E by the set of probability densities that can be written as2P (ω) ∝ e v(ω) ∀ω ∈ Ω.2 It is common to use slightly more general definitions [17] where the exponent also contains a fixed additional function that is not in V . Our formulationensures that E contains the constant density whenever Ω has finite measure.D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3111For any density P , D(P (cid:10) E) denotes the infimum of D(P (cid:10) Q ) with Q ∈ E . If there is a Q with D(P (cid:10) E) = D(P (cid:10) Q ), it iscalled projection of P onto E .Note that the projection is unique whenever it exists [17]. Given appropriate reference manifolds for X and Y (formaliz-ing the set of “smoothest” distributions), our inference method will be based on the following assumption:Postulate 2 (Orthogonality for reference manifolds). Let E X and EY be “reasonable” reference manifolds for X and Y , respec-tively. If X causes Y then the conditions of Theorem 2 hold approximately, where U X and U Y be the projections of P X andP Y onto E X and EY , respectively.For reference manifolds (instead of single reference distributions), this postulate requires a slightly different justification.This is explained in Appendix B.The choice of the reference manifold is the point where prior knowledge on the respective domain enters into themethod in the same way as the choice of the single reference measure did in the theory developed previously. Choosingthe family of all Gaussians has the following interesting feature: the distance to the closest Gaussian defines a scale- andlocation-invariant measure of irregularities of P X . Choosing a manifold smaller than the set of all Gaussians would keepsome of the information about location or scale, choosing a larger manifold would also remove some of the scale- andlocation-invariant information about P X . This is why the Gaussians are a natural choice at least in the one-dimensionalcase. For multi-dimensional variables X and Y , we will later see that the manifold of all Gaussians is often too largebecause it also removes the information about relative scaling of the different components of each variable X and Y . In thiscase, we will choose a proper submanifold.4.3. Inference method (general form)Having derived a long list of asymmetries between cause and effect, we have to chose one that is convenient for inferringthe causal direction. To this end, we observe that the additivity of irregularities in (15) obviously impliesD(P X (cid:10) U X ) (cid:2) D(P Y (cid:10) U Y ),whenever X causes Y . Generalizing this to reference manifolds (see Postulate 2) impliesD(P X (cid:10) E X ) (cid:2) D(P Y (cid:10) EY ),(23)−→P Y (cid:10) U Y ) = 0 (i.e., when the function is so simple that the image of U X is U Y ). Therefore,with equality if and only if D(our inference method reads:Information-Geometric Causal Inference (IGCI). Let E X and EY be manifolds of “smooth” reference distributions for Xand Y , respectively. Consider the distances of P X and P Y to E X and EY , respectively, as the complexity of the distributions.Define the complexity loss from P X to P Y byC X→Y := D(P X (cid:10) E X ) − D(P Y (cid:10) EY ).(24)Likewise, the loss from P Y to P X is given by exchanging the roles of X and Y .Then, infer that X causes Y if C X→Y < 0, or that Y causes X if C X→Y > 0.To make this rule applicable, we first derive more explicit forms of C X→Y , which still refer to general reference manifolds.Section 4.4 then describes estimators from empirical data that refer to particular reference manifolds.Lemma 5 (C X→Y as difference of Shannon entropies). Let P X and P Y be densities on Rd. Assume that U X and U Y are the projectionsof P X on E X and P Y on EY , respectively. ThenC X→Y ==(cid:2)(cid:2)(cid:3)S(U X ) − S(U Y )(cid:3)S(U X ) − S(P X )(cid:2)(cid:2)(cid:3)S(P X ) − S(P Y )(cid:3)S(U Y ) − S(P Y ).−−Proof. Since U X is the projection of P X onto E X , we have(cid:5)D(P X (cid:10) E X ) = D(P X (cid:10) U X ) = −S(P X ) −P (x) log U (x) dx = −S(P X ) + S(U X ).(25)(26)(27)To derive the last equation, we first assume that P X and all densities in E X have compact support Λ ⊂ Rd. Then Esince the vector space defining E clearly contains the constant function x (cid:5)→ 0.contains the uniform distribution U(0)Because U X is the projection of P X onto E X , (P X , U X , UX ) form a Pythagorean triple [18]. Using Lemma 3, we obtain(0)X12D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31(cid:4)P (x) log U (x) dx = S(U X ). For non-compact supports, we consider the restrictions of all densities to an increasing se-−quence of compact subsets Λn. The statement then follows by the limit n → ∞. (cid:2)The entropy difference between X and Y can also be rewritten as follows:Lemma 6 (C X→Y as mean of log Jacobi determinant). If f is a diffeomorphism between submanifolds of Rd, then(cid:8)(cid:8)(cid:8)det ∇ f (x)(cid:8)P (x) dx,logC X→Y = S(U X ) − S(U Y ) +(cid:5)where we have used the notations of Lemma 5.Proof. The entropy of Y = f ( X) reads(cid:8)(cid:8)(cid:8)det ∇ f (x)(cid:8) dx,S(P Y ) = S(P X ) +P X (x) log(cid:5)thus we haveC X→Y =(cid:2)(cid:3)S(U X ) − S(P X )−(cid:5)= S(U X ) − S(U Y ) +(cid:2)(cid:3)S(U Y ) − S(P Y )(cid:8)(cid:8)(cid:8)det ∇ f (x)(cid:8)P (x) dx.log(cid:2)(28)Note that C X→Y is invariant under joint rescaling of P X and U X (and likewise for P Y and U Y ), since S(U X ) changesby the same additive constant as det ∇ f , except for the sign. In the next subsection, we discuss some important cases ofdomains of X and Y and describe possible choices of reference manifolds and how to empirically estimate ˆC X→Y .4.4. Inference method (explicit form for reference measures on R)Lemmas 5 and 6 reduce the estimation of C X→Y and C Y →X to estimating entropies or Jacobians, respectively. In thispaper we are mainly concerned with one-dimensional continuous variables. We therefore give the explicit form of theestimators for this case, which will be used in our experiments. For completeness, we also discuss other situations inSection 4.5 and propose corresponding reference measures.Uniform reference measure on intervalsFor our motivating example of Section 4.1, where X and Y attain values in [0, 1], Lemmas 5 and 6 imply the followingtwo simple versions of IGCI:1. Entropy-based IGCI: infer X → Y whenever S(P X ) > S(P Y ).To implement this in practice, we used the entropy estimator [19]:ˆS(P X ) := ψ(m) − ψ(1) + 1m − 1m−1(cid:12)i=1log |xi+1 − xi|,(29)where the x-values should be ordered ascendingly, i.e., xi (cid:2) xi+1, and ψ is the digamma function.3 Note that here weset log 0 = 0, i.e., the points with xi+1 = xi don’t contribute to the sum. The estimate for C X→Y based on (29) is thengiven by:ˆC X→Y := ˆS(P Y ) − ˆS(P X ) = − ˆC Y → X .2. Slope-based IGCI: infer X → Y whenever(cid:8)(cid:8) f(cid:7)(cid:8)(cid:8)P (x) dx <(x)log1(cid:5)0(cid:8)(cid:8)g(cid:7)(cid:8)(cid:8)P ( y) dx.( y)log1(cid:5)0We introduce the following estimator:ˆC X→Y := 1m − 1(cid:8)(cid:8)(cid:8)(cid:8)logm−1(cid:12)i=1(cid:8)(cid:8)(cid:8)(cid:8),yi+1 − yixi+1 − xiwhere the xi values are ordered, and a similar one for ˆC Y →X .(30)(31)3 The digamma function is the logarithmic derivative of the gamma function: ψ(x) = d/dx log Γ (x). It behaves as log x asymptotically for x → ∞.D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3113With the assumptions of this section, (30) and (31) coincide exactly, because the ψ -terms cancel when taking thedifference between the estimated entropies of X and Y and because ordering the x-values is equivalent to ordering they-values. In the noisy case, the relation between both methods is not yet understood (see also Section 4.6). (31) thendiverges for m → ∞ since the difference of y-values remains finite when the difference of x-values gets closer to zero.Then one has to compensate for this by considering the difference of this estimator and its analog in the reverse direction(obtained by swapping the roles of X and Y ).Gaussian reference measure on RLet us discuss the case d = 1 first. Lemmas 5 and 6 imply that C X→Y and C Y →X remain formally the same as forthe uniform reference measure after we rescale X and Y such that they have the same variance (note that this ensuresS(U X ) = S(U Y )). In contrast, the uniform measure required all data points to lie in [0, 1]. The different scaling changesC X→Y by log σ X − log σY , where σ 2Y denote the variances of X and Y , respectively, according to the scaling used foruniform measure. Consequently, the methods may infer different directions when σ 2Y differ significantly, althoughthis did not happen that often in our real-world data experiments.X and σ 2X and σ 24.5. Inference rule for other variable ranges and reference manifoldsAlthough our experiments contained only real-valued variables, we sketch how to use IGCI also for variables with otherranges.Gaussian reference measure on RdSuppose now that both X and Y are d-dimensional real random vectors, and that fis a diffeomorphism Rd → Rd.Let both E X and EY be the manifolds of d-dimensional Gaussian distributions. The projection U X is the d-variate Gaussianwith the same mean vector and covariance matrix as X , denoted by Σ X . U Y is derived similarly. The difference of theentropies of U X and U Y thus reads 12 log(det Σ X / det ΣY ). Then we can easily compute C X→Y based on (26). Because theentropy difference S(U X ) − S(P X ) is a measure of non-Gaussianity, the method thus considers the variable that is closer toa Gaussian as the cause.Isotropic Gaussians as reference on RdWe will now show that the deterministic case of the method described in [13] and [14] relies on an assumption thatimplies Postulate 2 for a particular choice of the reference manifold. Let P X and P Y be multivariate Gaussians in Rd withzero mean and X and Y be related byY = A X,(32)where A is an invertible d × d matrix.4 For an arbitrary d × d matrix B let τ (B) = tr(B)/d denote the renormalized trace.Then [13] is based on the assumption that X → Y implies approximatelyτ (ΣY ) = τ (Σ X )τ(cid:2)(cid:3),A A T(33)where Σ X and ΣY denote the covariance matrices of X and Y , respectively. In [13] this is further justified by showing thatfor any given A, choosing Σ X randomly from a rotation-invariant prior ensures that (33) is approximately true with highprobability.5 We now show that this implies Postulate 2 if both E X and EY are the manifold of isotropic Gaussians, i.e., thosewhose covariance matrices are multiples of the identity. U X and U Y have the same mean as X and Y and their covariancematrices read τ (Σ X )I and τ (ΣY )I. The relative entropy distance between two Gaussians with equal mean and covariancematrices Σ1 and Σ0 is given by(cid:6)D(P Σ1(cid:10) P Σ0 ) = 12logdet Σ0det Σ1(cid:2)(cid:16)τ+ dΣ−10 Σ1(cid:3)− 1(cid:7)(cid:17).The distances to the manifold of isotropic Gaussians thus read [13]D(P X (cid:10) E X ) = 12D(P Y (cid:10) EY ) = 12,(cid:2)(cid:3)d log τ (Σ X ) − log det(Σ X )(cid:2)(cid:3)d log τ (ΣY ) − log det(ΣY )−→P Y reads τ (Σ X ) A A T . Hence,(cid:18).The covariance matrix of(cid:6)D(−→P Y (cid:10) U Y ) = 12logτ (ΣY )dτ (Σ X )d det( A A T )+ dτ (Σ X )τ ( A A T )τ (ΣY )(34)(35)(cid:19)(cid:7)− 1.4 Ref. [13] also considers the case Y = A X + E, where E is an independent noise term, but we restrict the attention to the deterministic one.5 Ref. [14] extends this framework to the case where the number of dimensions exceeds the number of samples.14D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Due to det(ΣY ) = det(Σ X ) det( A A T ) we haveD(P Y (cid:10) EY ) = D(P X (cid:10) E X ) + D(−→P Y (cid:10) U Y ) + d2(cid:18)1 − τ (Σ X )τ ( A A T )τ (ΣY )(cid:19).Assumption (33) is thus equivalent to condition (V) in Theorem 2. Postulate 2 thus gets an additional justification via aprobabilistic scenario where fis fixed and P X is chosen randomly from a prior that satisfies a certain symmetry condi-tion.For high-dimensional relations that are close to linear, the method above seems more appropriate than the one that usesthe set of all Gaussians (as opposed to the isotropic ones only) as reference manifold. Allowing for all Gaussians, the methodmakes use of the nonlinearities of f , while it removes the information that is contained in Σ X . For relations that are closeto the linear case, one thus looses the essential information, while taking isotropic Gaussians as reference ensures that onlythe information that describes the joint (overall) scaling is lost.Non-uniform reference measure on finite setsThe intuitive explanation of the identifiability of cause and effect used the fact that regions of high density of theeffect correlate with regions of high slope of the inverse function. Remarkably, our method is in principle also applicable−→P Y (cid:10) EY ) > 0 (which is not the case ifto bijections between finite probability spaces, provided that we ensure that D(E X and EY consist of the uniform distribution only). We omit the details but only give a brief sketch of a special casehere.Assume that both X and Y take values in {1, . . . , k} and P X and P Y are probability mass functions with P Y ( y) =P X (g( y)). (Note that in this discrete case, g is invertible but not monotonic.) Let E X and EY be the two-parametric manifoldof distributions of “discrete Gaussians” with− (x−μ)22σ 2,U (x | μ, σ ) ∝ ewhere μ ∈ R and σ ∈ R+. Then the image of the discrete Gaussians will usually not be a discrete Gaussian and ourinference principle becomes non-trivial, yielding preference for one direction. The essential question is, however, underwhich conditions Postulate 2 is still reasonable. The following explanations provide an idea about this. Assume that k islarge and that P X is a distribution that is close to one of the above discrete Gaussian except for a small number of x-values.Let f be a bijection that preserves most of the points {1, . . . , k}, while permuting only some of them. It is then likelythat this permutation increases the distance to the reference manifold rather than decreasing it. This way of reasoningcertainly relies on the assumption that k is large and that the distance of P X to the reference manifold is not too large. Forsmall k, one can easily construct examples with P X deviating so strongly from the Gaussians that a significant fraction ofpermutations decrease the distance to the reference manifold.4.6. Performance in the noisy regimeThe assumption of having a bijective deterministic relation is actually necessary for the IGCI method. Section 4.7, how-ever, will show that the performance on our real data sets was unexpectedly good, even though most of them are obviouslynoisy. We therefore present some explanations for this fact. Although the noisy case is actually out of scope, the develop-ment of future methods could be inspired by understanding the reasonable performance in this regime.On the one hand, we estimate how small the noise needs to be in order not to spoil the method (Section 4.6.1). Onthe other hand we show, that under some conditions noise can even contribute to inferring the correct causal direction(Section 4.6.2).First we discuss a case where IGCI necessarily fails. Let Y be generated from X by a linear model with additive noiseY = X + E with E ⊥⊥ X,hence P Y is obtained by the convolution P Y = P X ∗ P E . For Gaussians as reference manifolds, the projections U X and U Y ofP X and P Y on E X and EY , respectively, are given by the Gaussians with the same mean and variance. If E is Gaussian, wethus have U Y = U X ∗ P E due to the additivity of means and variances under convolution. We haveD(P X (cid:10) E X ) = D(P X (cid:10) U X ) > D(P X ∗ P E (cid:10) U X ∗ P E ) = D(P Y (cid:10) U Y ) = D(P Y (cid:10) EY ),because the convolution with a Gaussian decreases the distance to the set of Gaussians (that it is non-increasing alreadyfollows from monotonicity of relative entropy distance under stochastic maps [20]). Hence, (23) is violated and, after renor-malizing X and Y to unit variance, the entropy of Y will be greater than the entropy of X . The entropy-based estimator forC X→Y will thus converge to a positive number, while our theory makes no statement on the slope-based estimator (notethat the equivalence of both required deterministic models). Similar arguments hold for Y = α X + E, we have restricted thederivation above to α = 1 only for technical convenience. Hence, entropy-based IGCI with Gaussians as reference manifoldfails if the nonlinearity of fis small compared to the width of the (Gaussian) noise. The following subsection provides abound on how relevant small noise can get for the decision made by IGCI.D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31154.6.1. Robustness of entropy-based inference under adding small noiseWe restrict the attention again to real-valued X and Y and recall that the entropy generated by adding independentGaussian noise is related to the Fisher informationJ (Y ) := EP(cid:7)2(cid:6)∂ log P ( y)∂ yby De Bruijn’s identity [20]:∂∂tS(P Y +√t Z ) = 12J (Y +√t Z ),(36)where Z is a Gaussian with variance 1 and Y ⊥⊥ Z . The following Lemma provides a lower bound on the non-Gaussianity ofthe perturbed variable:Lemma 7 (Non-Gaussianity of noisy output). If EY denotes the manifold of Gaussians and E is Gaussian noise with E ⊥⊥ Y then the“decrease of non-Gaussianity” is bounded from above byD(P Y (cid:10) EY ) − D(P Y +E (cid:10) EY ) (cid:2) 12(cid:6)log1 +(cid:16)J (Y )σ 2Y− 1(cid:17)σ 2E+ σ 2Eσ 2Y(cid:7),where σ 2Y and σ 2E denote the variance of the unperturbed output and the noise, respectively.Proof. Set E := σE Z for standard Gaussian Z , then (36) impliesS(P Y +E ) − S(P Y ) =σ 2E(cid:5)0∂∂tS(P Y +√t Z ) dt = 12σ 2E(cid:5)0J (Y +√t Z ) dt (cid:2) 12σ 2E(cid:5)0√J (Y ) J (J (Y ) + J (t Z )√t Z )dt,where the last inequality is due to the Fisher information inequality [21]1J (Y + W )(cid:3) 1J (Y )+ 1J (W ),for arbitrary independent random variables Y and W . Using J (computation), we obtain√t Z ) = 1/t (which can be checked via straightforwardS(P Y +E ) − S(P Y ) (cid:2) 12σ 2E(cid:5)01t + 1/ J (Y )dt = 12(cid:18)(cid:6)logσ 2E+ 1J (Y )(cid:7)(cid:6)− log(cid:7)(cid:19)1J (Y )(cid:2)log= 12J (Y )σ 2E+ 1(cid:3).Recalling from (27) that non-Gaussianity is given by(cid:2)2π eσ 2YD(P Y (cid:10) EY ) = 12− S(P Y ),log(cid:3)because the first term is the entropy of the Gaussian with variance σ 2Y , the non-Gaussianity changes according toD(P Y (cid:10) EY ) − D(P Y +E (cid:10) EY ) (cid:2) 12= 12(cid:6)log(cid:6)σ 2Yσ 2Y+ σ 2E(cid:16)log1 +J (Y )σ 2Y(cid:2)+ logJ (Y )σ 2E(cid:7)(cid:3)+ 1(cid:7)(cid:17)− 1σ 2Yσ 2E+ σ 2E.(cid:2)Note that Gaussians minimize Fisher information for a given variance and thus J (Y )σ 2Y− 1 (cid:3) 0, with equality for Gaus-sians. If Y is Gaussian, convolution with a Gaussian cannot decrease non-Gaussianity any further because it is already zero.For non-Gaussian Y , the amount of decrease not only depends on the “sensitivity term” [ J (Y )σ 2− 1] but also on the ratioYbetween the variance of the noise and the total variance σ 2Y+ σ 2Lemma 7 assumes Gaussian noise. We expect however that non-Gaussian noise will typically decrease non-Gaussianityeven less than Gaussian noise does, except for rare cases of very particularly distributed noise. We therefore propose touse the bound for general noise. To decide whether noise may have reversed the inferred causal arrow, we could proceedas follows. For every hypothetical cause, say, X , we can estimate the density and the function f and thus compute thedistribution of the effect Y without noise. After computing its Fisher information we can estimate the decrease of non-Gaussianity caused by the noise and check whether it is smaller than the difference between D(P Y (cid:7) (cid:10) EY ) and D(P X (cid:10) E X ),where Y(cid:7) := Y + E denotes the noisy effect.E of the noisy output.16D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–314.6.2. Performance of slope-based inference in the noisy regimeWe will now take a closer look at the estimator (31) in the noisy case. The arguments below are partly heuristic, butsimulation studies in Section 4.7 support our claims. Assume that the i.i.d. sample (xi, yi) with i = 1, . . . , m is generated byan additive noise model (1) with strictly monotonic differentiable f . We assume that the xi and hence also the f i := f (xi)are already ordered (xi+1 (cid:3) xi and f i+1 (cid:3) f i ). We have for large m(cid:8)(cid:8)(cid:8) = 1(cid:8)m−1(cid:12)m−1(cid:12)loglog(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)( f i+1 − f i) + (ei+1 − ei)xi+1 − xiyi+1 − yixi+1 − xim − 11m − 1i=1i=1m−1(cid:12)i=1≈ 1m − 1log |ei+1 − ei| − 1m − 1m−1(cid:12)i=1log |xi+1 − xi|.(37)The approximation is based on the observation that the difference | f i+1 − f i| gets negligible compared to |ei+1 − ei| form → ∞ since the latter term remains finite while the other one converges to zero.The second term in (37) is actually the entropy estimator (30) up to the term ψ(m) − ψ(1). Without the noise E, thetwo estimators (30) and (31) coincide with each other, as we have already argued. However, in the noisy regime, the firstterm tends to dominate as m increases, since it diverges as m → ∞.Now we write X as X = ˜f (Y ) + ˜E with an arbitrary function ˜f . To focus on the noise effect, let us assume that Xand Y have the same entropy such that the information contained in the nonlinear functions does not help identifying thecausal direction, i.e., the estimator (30) would give the same value for ˆC X→Y and ˆC Y →X . To investigate the behavior of theestimator (31), denote by A X→Y the first term of (37), i.e.,A X→Y := 1m − 1and letAY → X := 1m − 1m−1(cid:12)i=1m−1(cid:12)i=1log |ei+1 − ei|,log |˜ei+1 − ˜ei|.As the second term of (37) is the same for both directions by assumption, (31) would prefer the direction X → Y (respec-tively, Y → X ) if A X→Y is smaller (respectively, larger) than AY →X .The Jacobian matrix associated with the transformation from ( X, E)T to ( X, Y )T is(cid:18)J =(cid:19),10(cid:7)(X) 1fand hence |J| = 1, where |J| denotes the absolute value of the determinant of J. We then have P X,Y = P X,E /|J| = P X,E . As Xand E are independent we further haveS(X, Y ) = S(X) + S(E).On the other hand, we have(38)S(X, Y ) = S(Y , ˜E).(39)Except for some special cases (for instance, where f is linear and both X and E are Gaussian) ˜E and Y are dependent [9,11],i.e., S(Y ) + S( ˜E) − S(Y , ˜E) > 0. Due to (38) and (39), we thus have S( X) + S(E) < S(Y ) + S( ˜E). As we assumed S( X) = S(Y ),finally we have S(E) < S( ˜E). Furthermore, as E and ˜E approximately have the same variance,6 the above inequality impliesthat ˜E is more Gaussian than E.Let ˜D i := ˜E i+1 − ˜E i and D i := E i+1 − E i . Under the condition that E i are i.i.d., P D iis the convolution of P E and P −E .is a convolution of P ˜E with P − ˜E . Since ˜E is more Gaussian than E, it is quite likely that ˜D i is also moreLikewise, P ˜D iGaussian than D i . We then consider the following three possible cases.1. If E is Gaussian (and so are D i ), ˜D i is also Gaussian (given the above heuristics), and A X→Y = AY →X . Hence, the noisedoes not change the decision.6 Note that E and ˜E have exactly the same variance, if both directions are fitted with linear functions (i.e., both f and ˜f are linear) and X and Y havethe same variance.D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31172. Consider the case where E is super-Gaussian, which, roughly speaking, means that P E has a sharper peak and longertails than the Gaussian variables with the same mean and variance. The Laplacian distribution is an example of suchdistributions. Since ˜E is more Gaussian than E, E is more super-Gaussian than ˜E. Consequently, D i take relatively morevalues that are very close to zero than ˜D i . The function log |D i| is concave on (0, ∞) and symmetric w.r.t. the y-axis; weobtain large negative values for D i that are close to zero. A X→Y thus gets smaller than AY →X . That is, super-Gaussiannoise tends to favor the correct direction, X → Y .3. Suppose E is sub-Gaussian, which is flatter than the Gaussian variable with the same mean and variance. An exampleis the uniform distribution. As ˜D i is more Gaussian (or less sub-Gaussian) than D i , they take more often values that arevery close to zero or very large than D i . Hence AY →X is larger than A X→Y . In other words, sub-Gaussian noise tends tofavor the wrong direction, Y → X .Fortunately, super-Gaussian noise occurs quite often in practice. Although we only analyze the noise effect above, one shouldbear in mind that with the estimator (31), the decision is made based on the joint effect of the properties of the nonlinearfunction and the noise distribution, which correspond to the second and first terms of (37), respectively.7In the analysis above we assume that the data-generating process in the noisy case can be approximated by the additivenoise model. Analyzing the noise effect in more general settings (e.g., in the PNL causal model [11]) is rather complicated,and is not given here. However, in Section 4.7 we also give simulation results on the data with a rather complex datagenerating process and illustrate how the noise influences the performance of IGCI.4.7. ExperimentsIn this section we describe some experiments that illustrate the theory above and show that our method can detectthe true causal direction in many real-world data sets. Complete source code for the experiments is provided online athttp://webdav.tuebingen.mpg.de/causality/ and http://parallel.vub.ac.be/igci. The latter provides an applet showing the dataand the results of IGCI.Simulation studies (I): Cause–effect pairs from a larger causal networkWe investigate the performance of IGCI in the deterministic and the noisy regime.To this end, we simulate a causal relation between n variables X1, . . . , Xn, from which we take different pairs (Y , X) ≡( Xi, X j), where X j is one of the parents of Xi . All causal dependences will be given by structural equations. This ensuresthat in our pairs not only the effects but also the causes are the outcomes of structural equations – reflecting the fact thatcauses in the real world are effects of other variables.The precise form of the data generating process is as follows. We first generate 20 independent variables X1, . . . , X20.Their distribution is randomly chosen from two options with equal probability: either the uniform distribution on [0, 1] ora Gaussian mixture distribution GM with the following density:GM(x) =g(cid:12)i=1w iφ(x|μi, σi),where g ∈ [1, 5], means μi ∈ [0, 1], standard deviations σi ∈ [0, 1/g] and weights w i ∈ [0, 1] withi=1 w i = 1. Each param-eter is randomly chosen from the interval according to a uniform distribution. Then, 50 variables X21, . . . , X70 are definedaccording to the following structural equation:(cid:15)gXi = f i(X j, . . . , X j+k) + λi R i E i,with j, k defined later, where for each i:• The function f i is randomly selected from the following families:LIN Linear functions of the formf (x j, . . . , x j+k) =k(cid:12)j=0c j x j+i,where c j ∈ [−1, 1] and k is a natural number randomly according to the probability 1/2k.POL Polynomials of the formf (x) =n(cid:12)i=1ici xi,7 Rigorously speaking, the noise in the forward direction also changes the best-fitting nonlinear function in the backward direction, which would influencethe estimate of C Y →X as well. As a simple illustration, consider the case where both X and E are uniform. Then the best-fitting function ˜f in the directionY → X is no longer linear, and its shape depends on the noise level. However, we skip the details of this aspect.18D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31with n ∈ [1, 5] and ci ∈ [−1, 1]. The purpose of the factor i is to ensure that the magnitude of each term is similarfor x ∈ [0, 1].MON Monomials of the form f (x) = xn with n ∈ [2, 5].ROOT Root functions f (x) = x1/n with n ∈ [2, 5].MG Cumulative distribution functions of mixtures of Gaussians:f (x) =5(cid:12)i=1αiΦ(x|μi, σi),which is a convex combination of Gaussian cdf’s Φ(x|μi, σi) with αi, μi ∈ [0, 1], σi ∈ [0, 0.2].PROD Product functions of the form f (x j, x j+1) = x j x j+1.QUOT Quotients f (x j, x j+1) = x j/x j+1.• The variables X j, . . . , X j+k are chosen randomly from X1, . . . , Xi−1 (the “causally preceding” variables). Note that k (cid:3) 0for the linear function and k = 1 for the product and division function. The latter results in non-additive noise, sincethe study of the relation between one input variable and the output variable is based on marginalizing the data overthe second input variable. All other functions have only one independent variable.• λi has the probability of 0.5 to be zero, and is otherwise chosen uniformly between [0, 0.2].• R iis the difference of the maximum and the minimum of the function f i after feeding it with the values ofX j, . . . , X j+k. In this way, the noise is proportional to the range of the function values.• The noise term E i is drawn from a Gaussian distribution with mean 0 and variance 1.Note that a deterministic relation is obtained whenever k = 0 (i.e., X j is the only parent of Xi ) and the noise parameter λvanishes. When a deterministic relation is monotonic and decreasing, we make it an increasing function by replacing eachy-value with 1 − y. We repeat the whole procedure of generating the variables X1, . . . , X70 100 times, and each time wegenerate 200 samples such that each causal decision will be based on m = 200 i.i.d. data points.For each data set generated by this procedure, we apply our inference method to the 50 pairs (Y , X) ≡ ( Xi, X j) fori = 21, . . . , 70 (with randomly chosen j as above). We compared the entropy-based and the slope-based method. We alsocompare two different families of reference measures: the uniform family (which amounts to preprocessing both compo-nents of the data by an affine transformation such that the minimum is 0 and the maximum is 1) and the Gaussian family(where each component of the data is preprocessed by an affine transformation such that it has zero mean and standarddeviation 1).Fig. 7 shows some typical examples of input distributions, relations between input and output, and the correspondingoutput distribution. Table 1 lists the values ˆC X→Y and ˆC Y →X of the slope-based estimator (31) and the entropy estimator(30), as well as the corresponding decision. Remarkably, the decision was also correct for the linear noisy case (the fourthcase). For possible explanations see Section 4.6.2.By only taking decisions for | ˆC X→Y − ˆC Y →X | (cid:3) δ for some threshold δ, one can trade off accuracy (percentage of correctdecisions) versus decision rate (percentage of cases in which a decision was taken). Fig. 8 shows the accuracy versus thedecision rate for the deterministic relations, and Fig. 9 shows the same for the probabilistic relations. These results showthat the method works best for deterministic relations, as expected. For deterministic relations, increasing the thresholdincreases the accuracy of the method, coming close to an accuracy of 100% for large threshold values. For deterministicrelations, the Gaussian reference measure performs somewhat better than the uniform reference measure. For probabilisticrelations, however, the picture is rather different. The uniform reference has an increasing accuracy starting at 70% when nothreshold is used and reaching 85% for large thresholds. The Gaussian reference on the other hand fails for small thresholds;the accuracy is close to 50% which is the same as random guessing. Only for large thresholds (decision rates smaller than20%) the accuracy reaches 70%. For both, deterministic and probabilistic relations, the slope-based estimator (31) and theentropy-based one (30) yield similar results.It is also instructive to check to what extent the above procedure generates joint distributions P Y , X that satisfy ourorthogonality assumptions. We therefore compare(cid:3)(cid:2)(cid:2)(cid:3)CovU Xlog f, P Xto CovU Ylog f, P Y,(cid:7)−1(cid:7)because the covariance in condition (II) in Theorem 2 and the corresponding expression for the backward direction take thisform after we use (11) for the uniform reference measure. The first expression is given by the estimator(cid:2)(cid:20)CovP X , log f(cid:3)(cid:7):= 1m − 1(cid:6)m−1(cid:12)i=11 − xi+1 − xi−12(cid:7)(cid:8)(cid:8)(cid:8)(cid:8)log(cid:8)(cid:8)(cid:8)(cid:8),yi+1 − yixi+1 − xi(40)and the second by exchanging the roles of X and Y . By the simulation explained above 1575 examples of deterministicstrictly monotonic relations were generated. The x-axis of Fig. 10 shows the values (40) and the y-axis the analog one forthe backward direction.The figure confirms our postulate in the sense that the covariance in forward direction is usually closer to zero. Most ofthe values for the forward direction are in the interval [−1, 1], while many of the backward values even reach values upto 5. It clearly shows that the backward covariance is biased away from zero and that the spread is higher.D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3119Fig. 7. Typical synthetic cause–effect pairs illustrating various different cases: probabilistic versus deterministic relations, correct versus incorrect result andan indecision. The corresponding quantitative description is given in Table 1.20D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Table 1Quantitative description (only for the uniform reference measures) of the typical examples depicted in Fig. 7.FunctionTypeX46 = f ( X12)X48 = f ( X40)X25 = f ( X20)X22 = f ( X1)X50 = f ( X13)POLPOLMGLINMG(31)ˆC X→Y−0.290.730.035.605.33ˆC Y →X0.29−0.73−0.036.444.71Dec.++−++OK+−−+−(30)ˆS(P X )−2.53−3.81−2.84−2.64−3.14ˆS(P Y )−2.83−3.01−2.8−2.93−2.92Dec.++−++OK+−−+−Fig. 8. Results of four different implementations of IGCI on simulated deterministic causal relations for about 2000 different ( X, Y ) pairs.Fig. 9. Results of four different implementations of IGCI on simulated probabilistic causal relations for about 3000 different ( X, Y ) pairs.D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3121Fig. 10. Empirical violations of orthogonality condition (h3) in forward vs. backward direction (see text).4.7.1. Simulation studies (II): Performance for different shapes of the noiseWe investigate how the performance of slope-based inference (estimator (31)) with uniform reference measure ischanged by the shape of the noise. We generate the data according to Y = f ( X) + E. We use four distributions for P E ,which are the Gaussian, Laplacian (which is super-Gaussian), uniform (which is sub-Gaussian) distributions and a stronglysub-Gaussian distribution (represented by the mixture of Gaussians 0.5N (−2, 1) + 0.5N (2, 1)). Similarly, four distributionsare used for P X ; they are the Gaussian and uniform distributions, a super-Gaussian distribution obtained by passing aGaussian variable through the power-nonlinearity with exponent 1.5 and keeping the original sign, and a sub-Gaussianone represented by the mixture of Gaussians 0.5N (−0.5, 1) + 0.5N (0.5, 1).f ( X) = X 1/3,f ( X) = X 3, and f ( X) = X . Note that in the last case, fis not informative for causal inference at all in the noise-free case.f has three different forms:For each setting, we repeat the simulations 500 times. Figs. 11, 12, and 13 plot the performance as a function of thenoise standard deviation in all possible cases of P E and P X , with the three forms for f , respectively. One can see that inthe second columns of all these figures (corresponding to Laplacian noise) the performance increases along with the noisevariance. In the third and fourth columns (corresponding to the uniform and strongly sub-Gaussian noise), the performancetends to become worse as the noise variance increases. As seen from Fig. 12, the function f ( X) = X 3 is informative forcausal inference: the performance is always good, almost regardless of different choices for P E and P X . When both P E andP X are Gaussian with f ( X) = X 1/3 or f ( X) = X (top-left panels in Figs. 11 and 13), the decision is for high noise level likeis linear and thus not useful for causal inference in the deterministic setting (see Fig. 13),a random guess. Finally, when fin certain combinations of P E and P X , IGCI still infers correctly due to the noise effect.We then consider a fixed signal-to-noise ratio and change the shape of the noise continuously. To this end, we randomlygenerate i.i.d. samples for the noise term E according to the zero-mean generalized exponential distribution (GED)P (e) =v√8Γ (1/v)(cid:21)(cid:8)(cid:8)(cid:8)−(cid:8)exp(cid:22)v(cid:8)(cid:8)(cid:8)(cid:8),e√2σ(41)where v is the mode, Γ (.) the gamma function, and σ the standard deviation. Sub-Gaussian, Gaussian and super-Gaussiannoise are obtained for v > 2, v = 2, and v < 2, respectively. In particular, when v = 1, we get the Laplacian distribution.The uniform distribution is obtained via the limit v → ∞. We use the ratio-of-uniform method [22] to generate the randomnumbers.For various cases of P X and f we vary v from 1 to 5 in (41), while the ratio of the standard deviation of the noise w.r.t.that of f ( X) is fixed to 2. Fig. 14 depicts the performance as a function of v. In all cases of P X and f under consideration,the performance decreases or remains the same as v increases (i.e., as P E becomes less super-Gaussian or more sub-Gaussian), which is consistent with the claims in Section 4.6.2.As a more general setting, we repeat the above simulations with the data generated by Y = X · e E + tanh(E), Y =X · e E + E 35 , and Y = X · e E + E, respectively; here Y is generated by a multiplicative block together with a nonlinear orlinear effect of the noise E. The performance of IGCI as a function of v is given in Fig. 15. Again, as in Fig. 14, one can seethat the performance decreases or remains the same as v increases.22D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Fig. 11. The performance (percentage of correct inferences) at different noise levels under various choices for P E and P X with the function f ( X) =X 1/3. Columns from left to right correspond to Gaussian, Laplacian, uniform, and strongly sub-Gaussian (with P (E) = 0.5N (−2, 1) + 0.5N (2, 1)) noise,respectively. Rows from top to bottom correspond to the Gaussian, uniform, super-Gaussian, and sub-Gaussian distributions for the cause X , respectively.Fig. 12. The performance (percentage of correct inferences) at different noise levels with the function f ( X) = X 3; see the caption of Fig. 11.Real-world data: Cause–effect pairsWe have also evaluated the IGCI method on real-world data, namely on the extended version of the “Cause–effect pairs”dataset described in [1]. This dataset consists of observations of 70 different pairs of variables from various domains, andthe task for each pair is to find which variable is the cause and which variable the effect. For example, one of the pairsconsists of 349 measurements of altitude and temperature taken at different weather stations in Germany. Obviously, thealtitude is the cause and the temperature is the effect. The complete dataset and a more detailed description of each paircan be found at http://webdav.tuebingen.mpg.de/cause-effect. Note that most of the pairs in this data set have high noiselevels, so that we do not necessarily expect our method to work well.In Fig. 16 we show the results for the 70 pairs with the following four variants of IGCI: uniform distribution and Gaus-sians as reference measures, each case combined with the slope-based and the entropy-based estimator. The absolute value| ˆC X→Y | was used as a heuristic confidence criterion. By taking only those decisions with high absolute value, one can tradeoff accuracy versus the amount of decisions taken. Fig. 16 shows the accuracy (i.e., the fraction of correct decisions) asa function of the decision rate (i.e., the fraction of decisions taken out of a total of 70 possible decisions, one for eachcause–effect pair). If the absolute value of ˆC X→Y was indeed a good measure of the confidence, one would expect thatD. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3123Fig. 13. The performance (percentage of correct inferences) at different noise levels with the function f ( X) = X ; see the caption of Fig. 11.Fig. 14. The performance (percentage of correct inferences) with different noise distributions (as indicated by v in (41)) for various P X and f . The ratio ofthe noise standard deviation w.r.t. that of f ( X) is fixed to 2.the accuracy is lowest for decision rate 100% (i.e., if all decisions are taken, regardless of the estimated confidence) andincreases (more or less) monotonically as the decision rate decreases. A complication here is that the amount of data sets(cause–effect pairs) from which the accuracy can be estimated decreases proportionally to the decision rate. This meansthat the accuracies reported for low decision rates have higher uncertainty than the accuracies reported for high decisionrates. For each decision rate, we have therefore indicated the 95% confidence interval that the accuracy is not significantlydifferent from 50% by a gray area.The four variants of IGCI yield comparable results. We also conclude that the majority of the decisions does agree withthe causal ground truth, and that this agreement is statistically significant for high decision rates. However, the accuracydoes not clearly increase with decreasing decision rates. This indicates that the heuristic confidence estimate (the absolutevalue of the estimated ˆC X→Y ) is not functioning properly, although it is difficult to draw any final conclusions about thisbecause of the high uncertainty in the accuracy for low decision rates. Nevertheless, considering the amount of noise that ispresent in many cause–effect pairs, it is surprising that our method works so well: if one always takes a decision, the fourIGCI variants have accuracies of 70 ± 7%, 75 ± 7%, 69 ± 7%, and 70 ± 7%, respectively.24D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Fig. 15. The performance of IGCI on the data generated by rather complex transformations with different noise distributions (as indicated by v in (41)) forvarious P log X and transformations. Note that the y-axis labels correspond to the distribution of log X . The variance of the noise E is fixed to 0.45.Fig. 16. Results of four different implementations of IGCI on 70 cause–effect pairs.Fig. 17 provides comparative results of IGCI (now using only the variant based on (31) with a uniform reference measure)with four other causal inference methods that are suitable for inferring the causal direction between pairs of variables:LINGAM [23], Additive Noise (AN) [9], the Post-NonLinear (PNL) model [11], and a recent non-parametric method (GPI) [24].All methods, except for GPI, employ the HSIC independence test [25] for accepting or rejecting the fitted models and use themaximum of the two HSIC p-values (where each p-value corresponds to a possible causal direction) as confidence estimate.The LINGAM method fits functional relationships of the form Y = α X + E to the data, preferring the causal direction forwhich the noise E is more independent of the hypothetical cause X . The additive noise based method (recall remarksaround (1) and [9]) was implemented using the Gaussian Process regression code in the GPML toolbox [26] to find theD. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3125Fig. 17. Results of various causal inference methods on 70 cause–effect pairs.most likely function f . For post-nonlinear model based inference (2), we employed neural networks to model the functionsf and h.8 Finally, the non-parametric GPI method does not assume a particular class of functional relationships, but usesthe general model Y = f ( X, E) and exploits the smoothness of the function f as one of the criteria for deciding uponthe causal direction. For this method, the confidence value is taken to be the approximated Bayes factor between the twomodels corresponding with the two possible causal directions.In contrast with the experiments reported in Fig. 16, we used at most 500 data points from each cause–effect pair,because most methods need significantly more computation time than IGCI for large sample sizes. Note, however, that theperformance of IGCI in this case is comparable with the performance reported in Fig. 16 where we used all data points. Thiscan be explained because for many pairs, the measured values have been discretized, and therefore, the effective numberof data points used by IGCI is usually lower than the number of available data points. We have repeated the experimentsthree times with different subsamples and plotted the average curves in Fig. 17. We observe that for high decision rates,all methods except LINGAM draw causal conclusions that are significantly correlated with the ground truth. IGCI, PNL andGPI yield comparable performances overall. The performance of the additive noise method seems to deviate from thesethree methods, because its accuracy is somewhat lower if it is forced to always make a decision, but on the other hand,its confidence estimate appears to be more accurate than that of the other methods, because the accuracy increases morequickly (even up to 100%) as the decision rate decreases. Again, it is difficult to draw any definite conclusions about therelative performances of these methods based on only 70 cause–effect pairs.Real-world data: Water levels of the RhineThe data consists of the water levels of the Rhine9 measured at 22 different cities in Germany in 15 minute intervalsfrom 1990 to 2008. It is natural to expect that there is a causal relationship between the water levels at the differentlocations, where “upstream” levels influence “downstream” levels.We tested our method on all 231 pairs of cities. Since the measurements are actually time series, and the causal influenceneeds some time to propagate, we performed the experiments with shifted time series, where for each pair of time series,one series was shifted relatively to the other so as to maximize the correlation between both.Fig. 18 shows for each pair whether the decision is correct or not. It also shows some representative plots of the data. Oneclearly sees that the noise for two nearby cities is relatively low, but it can be quite large for two distant cities. Nevertheless,our method performed quite well in both situations: the overall accuracy, using the uniform reference measure, is 87% (201correct decisions). The results for the Gaussian reference measure are similar (202 correct decisions).8 The large discrepancy between the results for PNL reported here and those reported in [15] is due to the fact that in [15], we applied a hand-tunedpreprocessing method to each pair, whereas here we have treated all pairs equally by using the same preprocessing method for each pair.9 We are grateful to the German office “Wasser- und Schiffahrtsverwaltung des Bundes”, which provides the data upon request.26D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31Fig. 18. Results for the German Rhine data. All pairs out of in total 22 cities have been tested. White means a correct decision, black is a wrong decision,and the gray part can be ignored. On the right, typical data is illustrated for two measurement stations which are near to each other (top) and for twomeasurement stations farther apart (bottom), which shows that the noise increases significantly with the distance.4.8. DiscussionThe assumption that P effect|cause and P cause do not satisfy any “non-generic relation” can be a helpful paradigm for findingnovel causal inference rules. Hence, one of the main important challenges consists in describing what kind of “non-generic”dependences typically occur in backward direction. A general answer to this question could not be given here, but we haveshown that one option for defining dependences in an empirically testable way is given by orthogonality conditions in thesense of information geometry.We have presented a method that is able to infer deterministic causal relations between variables with various domains.The accuracy of the proposed method was shown to be competitive with existing methods. In terms of computation time,this method is orders of magnitude faster (in particular, it is linear in the number of data points). In addition, it can handlethe deterministic case, whereas existing methods only work in the presence of noise.It would be desirable to have a reliable confidence criterion for our inference method. Moreover, we would like to pointout again that in the large noise regime, the present method may completely fail. For a Gaussian reference measure in onedimension, for instance, our entropy-based version necessarily shows the wrong direction when the effect is given by alinear function of the cause plus an independent Gaussian noise. This is because then the effect is more Gaussian than thecause.A generalization of the information-geometric inference method to the case where the relation between cause and ef-fect is not close to a bijective map is not straightforward. In Appendix A we discuss some toy examples showing thatasymmetries between cause and effect can sometimes still be phrased in terms of information geometry.AcknowledgementsPart of this work has been supported by Deutsche Forschungsgemeinschaft, SPP 1395.Appendix A. Outlook: Special cases of non-bijective relationsThe following subsections provide a list of toy models, and explore under which conditions a violation of some of theorthogonality conditions can be shown for the backward direction. The models suggest that there is no straightforwardextension of our IGCI method to the non-bijective case, although orthogonality could also help in identifying the causaldirection.A.1. One way deterministicLet the ranges D X and D Y of X and Y , respectively, be finite and P ( X, Y ) be a distribution for which Y is deterministi-cally determined by X , i.e., Y = f ( X) for some surjective, but not necessarily injective function f (without surjectivity thebackward model would not be defined), as in Fig. A.19. We show that the orthogonality conditions of Theorem 2 get simplefor this case if U X and U Y are the uniform distributions on D X and D Y , respectively. First consider the orthogonalities thatwe expect if X causes Y :Lemma 8 (Orthogonalities for surjective functions, X → Y ). Assume that Y is deterministically given by X . CovU X (hi, P X /U X ) = 0holds trivially for i = 1. For i = 2, 3 it is equivalent toD. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3127Fig. A.19. Left: causal relation given by a deterministic non-injective map from cause to effect. Right: “splitting model”, the cause can deterministically beinferred from the effect.CovU X (log m ◦ f , P X /U X ) = 0,wherem( y) :=(cid:8)(cid:8) f(cid:8)(cid:8)−1( y)denotes the number of pre-images of y.(A.1)Proof. Condition (h1) is trivial since the function x (cid:5)→ D(P Y |x (cid:10) U Y ) is constant (because P ( y|x) = δ y, f (x) and P Y |x thus is apoint measure). To rephrase condition (h2), we first compute−→P ( y) = m( y)|D X |,and thus obtainh2(x) =(cid:12)ylogP ( y|x)−→P ( y)P ( y|x) =(cid:12)ylogδ y, f (x)m( y)δ y, f (x) + c = − log m(cid:2)(cid:3)f (x)+ cwith c := log |D X |. The constant term c is clearly irrelevant for the covariance. Since (h1) is a constant function, uncorrelat-edness between h3 = h1 − h2 and P X /U X is obviously equivalent to uncorrelatedness between h2 and P X /U X . (cid:2)The following lemma describes the relations that we expect for the same condition Y = f ( X) if Y causes X , as in the“splitting model” in Fig. A.19 (right).10 The causal relation is now given by a mechanism that splits every y-value intodifferent x-values in the set A y such that the mapping from x to y is deterministic.Lemma 9 (Orthogonalities for splitting model, Y → X ). Assume again that Y is deterministically given by X (although we now assumeY to be the cause). For the functions y (cid:5)→ hi( y) the equation CovU Y (hi, P Y /U Y ) = 0 is trivial for i = 2 and for i = 1, 3 equivalent to(cid:6)CovU YS(P X| y),(cid:7)P ( y)U ( y)= 0.By slightly abusing notation, S(P X| y) and P ( y)/U ( y) denote the functions y (cid:5)→ S(P X| y) and y (cid:5)→ P ( y)/U ( y), respectively.Proof. We first computeP (x| y) = δ y, f (x)P X (x)P Y ( f (x)).To rephrase condition (h2), we compute←−P (x) = 1|D Y |(cid:12)yP (x| y) =P X (x)|D Y |P Y ( f (x)).We thus obtainh2( y) =(cid:12)xlogP (x| y)←−P (x)P (x| y) = log |D Y |.Therefore, condition (h2) becomes trivial.10 Note that this is the only case in this paper where Y is the cause. The reason is that we want to compare the properties of P X,Y that we expect forthe two possible causal directions.28D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31To reformulate condition (h1), we observe that h1( y) = D(P X| y (cid:10) U X ) is, up to a sign and additive constant, given byS(P X| y). Since h2 is a constant, condition (h3) is equivalent to (h1). (cid:2)These results show that we obtain reasonable conditions for both directions: if X is the cause, we get uncorrelatednessbetween input and the logarithm of the number of pre-images. On the other hand, if Y is the cause, we postulate zerocorrelation between input and conditional entropy. Unfortunately, the orthogonality in one direction does not imply theviolation of orthogonality for the other direction. Moreover, the violation of orthogonality in backward direction can haveboth positive and negative sign. This is shown by the following example.Example 3 (No definite violation in backward direction). Let f be such that the number m of pre-images (see (A.1)) is constant.Then all non-trivial conditions of Lemma 8 are satisfied for the hypothesis X → Y but y (cid:5)→ S(P X| y) can be positively ornegatively correlated or uncorrelated with P Y /U Y . To see this, set D Y = {1, 2} and D X = {1, 2, 3, 4} and P (x| y = 1) = 1/2for each x = 1, 2. On the other hand, P (x| y = 2) = 1 for x = 3. Then S(P X| y=1) = log 2 and S(P X| y=2) = 0. Depending onwhether P ( y = 1) is greater or smaller than P ( y = 2) we can induce positive or negative correlations.Conversely, assume that S(P X| y) is uncorrelated with P ( y)/U ( y) and all our orthogonality conditions would therefore beconsistent with the hypothesis Y → X . To see that then log m( f (x)) can nevertheless be negatively or positively correlatedwith P (x)/U (x), we consider the following example. Set D X = {1, 2, 3, 4, 5} and D Y = {1, 2}. Let P X| y=1 be the uniformdistribution on the set {1, 2} and let P X| y=2 be some distribution on {3, 4, 5} that has also the entropy 1 bit. Then S(P X| y)is constant in y and thus uncorrelated with P ( y)/U ( y) and we can design P ( y) as we like. To check whether log m( f (x))is positively or negatively correlated with P (x)/U (x) we observe(cid:2)(cid:3)(cid:2)log mf (x)(cid:3)P (x) − U (x)= log 2(cid:12)x(cid:6)P Y (1) − 25(cid:7)(cid:6)+ log 3P Y (2) − 35(cid:7)(cid:6)= (log 2 − log 3)(cid:7),P Y (1) − 25which is positive for P Y (1) < 2/5 and negative for P Y (1) > 2/5.This result is a bit disappointing at first glance since it questions the information-geometric method for the non-bijectivecase: if violations of orthogonality for the backward direction occur with both possible signs, decision rules get less simple;preferring the direction for which the violation of orthogonality is smaller with respect to its absolute value seems lessnatural than inference rules that work without absolute value. It could therefore be that notions of independence otherthan our orthogonality conditions are needed. To support this conjecture, we should also mention that in designing P Y andP X|Y in the above example we have in fact adjusted them to each other, we only did it in a way that is not captured by ourorthogonality conditions.There is, however, the following nice result:Lemma 10 (Number of pre-images and input probability). For Y = f ( X), let m( y) be the number of pre-images of y. If log m isuncorrelated with P Y /U Y then log m ◦ f is negatively correlated with P X /U X . On the other hand, if log m ◦ f is uncorrelated withP X /U X , then log m is positively correlated with P Y /U Y :(cid:7)(cid:6)(cid:7)(cid:6)CovU Xlog m ◦ f ,P XU XP YU Y(cid:7)= CovU Y(cid:6)− Dlog m,(cid:23)(cid:23)(cid:23)(cid:23)U Ym|D X |(cid:6)− Dm|D X |(cid:23)(cid:23)(cid:23)(cid:23) U Y(cid:7).Proof.(cid:6)CovU Xlog m ◦ f ,(cid:7)P XU X(cid:12)x(cid:12)y(cid:12)===(cid:2)(cid:3)(cid:2)log mf (x)(cid:3)P (x) − U (x)(cid:7)P ( y) − m( y)|D X |(cid:6)log m( y)(cid:6)log m( y)P ( y) − U ( y) + U ( y) − m( y)|D X |(cid:7)y(cid:6)= CovU Y(cid:6)− Dlog m,(cid:23)(cid:23)(cid:23)(cid:23)U Ym|D X |(cid:7)P YU Y(cid:7)(cid:6)− Dm|D X |(cid:23)(cid:23)(cid:23)(cid:23) U Y(cid:7).(cid:2)(A.2)(A.3)(A.4)(A.5)D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3129The term (A.2) measures to what extent m is non-constant. Since m( y)/|D X | coincides with−→P ( y) this is again our well-known expression (22). Note that the correlations between m and P Y /U Y is positive if Y is the effect, while the correlationbetween m ◦ f and P X /U X is negative if X is the effect. Here, the different sign of the correlation may seem disturbing.However, in the following special case it turns out to be natural:Example 4 (All pre-images are equally likely). For both causal directions X → Y and Y → X assumeP (x| y) =δ y, f (x)m( y).(A.6)If Y → X this is not unlikely to occur, because it only means to divide the probability uniformly over all pre-images x of agiven y. For X → Y it can, for instance, occur if P X is uniform.11We obtainh3(x) =(cid:12)logandh3( y) =y(cid:12)x−→P ( y)U ( y)P ( y|x) = logm( f (x))|D Y ||D X |,←−P (x)U (x)logP (x| y) = log|D X |m( y)|D Y |.Therefore, hs(x) and h3( y) are, up to irrelevant constants, given by m( f (x)) and −m( y), respectively. Hence, Lemma 10implies that h3( y) is negatively correlated with P ( y)/U ( y) if h3(x) is uncorrelated with P (x)/U (x) and vice versa, whichnicely fits into our information-geometric framework.Note, moreover, that log m( y) coincides with S(P X| y) up to a constant (and hence also with D(P X| y(cid:10)U X ) up to a signand a constant). Therefore, uncorrelatedness between log m and P Y /U Y is equivalent to orthogonality condition (h1) inTheorem 1.A.2. Functional relation with small independent noiseIn this subsection we revisit the motivating remarks in Section 2 in a more precise way and describe how they fit intoour information-geometric framework. Consider the so-called additive noise modelY = f (X) + E with E ⊥⊥ X.Let f be a bijection of [0, 1] and E have compact support [0, (cid:18)]. Let P X have support [0, 1], the support of Y is thus givenby [0, 1 + (cid:18)]. By adapting the arguments of Example 1 to the uniform distribution on [0, 1 + (cid:18)] instead of [0, 1] one checkseasily that orthogonality condition (h1) is equivalent to uncorrelatedness between S(P Y |x) and P (x), which holds becauseS(P Y |x) attains the constant value S(E). We now assume that (cid:18) is so small compared to the curvature of f and the scaleof the fluctuations of P (x) that the conditional distribution P X| y is approximately given by the distribution of( y)E,shifted by some y-dependent value. We thus assume−1f(cid:7)S(P X| y) ≈ S(E) + log f(A.7)for all y that are not too close to the boundaries of the interval [0, 1]. For the backward direction condition, the covariance(h1) therefore reads(cid:7)−1( y),(cid:6)CovU YS(P X| y),(cid:7)1+(cid:18)(cid:5)=P ( y)U ( y)(cid:2)S(P X| y)(cid:3)P ( y) − U ( y)dy01(cid:5)≈log f0(cid:7)−1(cid:2)( y)(cid:3)P ( y) − U ( y)dy(A.8)(A.9)where we have not only used the approximation (A.7) but also neglected the fact that S( X| y) actually has to be integratedover [0, 1 + (cid:18)] rather than [0, 1] since the errors are all of order (cid:18). Expression (A.9) is positive for small (cid:18) because in thedeterministic limit (cid:18) → 0, (A.9) can be transformed into1(cid:5)−log f0(cid:7)(cid:2)(x)P (x) − f(cid:7)(cid:3)(x)dx = D(cid:2)P X (cid:10) f(cid:3)(cid:7)(cid:2)+ D(cid:7) (cid:10) P Xf(cid:3)(cid:3) 0,(A.10)11 If P (x) attains many different values, it is, however, unlikely that it always attains the same value within the same A y .30D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–31(cid:7)(cid:7)(x) =as probability density (due to f (1) = 1 and f (0) = 0). Note that in the deterministic invertible casewhere we interpret f←−P (x) and (A.10) is again a symmetrized relative entropy term. This result shows that additive noise modelswe have f(in the low noise regime) induce backward models for which the noise depends on the input in a way that leads to violationof orthogonality condition (h1). It is remarkable that the amount of violation is here described by a term that is similar tothe one that occurred in the bijective as well as in the case of A.1 even though these cases refer to orthogonality (h3). Thissuggests that there is a common principle behind our observations.Appendix B. Justification of Postulate 2For single reference densities instead of manifolds we have justified conditions (h1) to (h3) by the argument that thestructure functions h should not correlate with P X because they only depend on the conditional P Y | X (i.e., the function fin our case). This justification is not completely convincing if we generalize the setting to manifolds: the functions h1and h3 contain the reference density U Y , which is defined by projecting the “output” probability P Y onto EY . U Y thusdepends on both P X and f because P Y is the image of P X under f . We therefore justify Postulate 2 in a slightly differentway.Our justification remains quite informal; to provide a more precise version of the below arguments would go beyond thescope of this paper. We start with the following statement:Observation 1 (Projection of random moves). Let P be the set of probability distributions over some large (or infinite) prob-ability space. Let E ⊂ P be a low-dimensional exponential manifold and P ∈ P be arbitrary. Generate a new point R bymoving into some random direction from P , chosen independently of E . Denoting the projections of P and R on E by P Eand RE , respectively, we obtain for a typical moveP E ≈ RE .The approximate equality means that the error made by replacing one point with the other in any relative entropy expres-sion is small compared to D(P (cid:10) R) and D(R (cid:10) P ).Apart from the approximate equality signs, the statement is also informal by not specifying what a “typical move” means.This would require a probability distribution on the set of possible moves.Assume now that the pair (P X , f ) is generated as follows. Let U X ∈ E X be given and obtain P X by modifying U Xindependently of E X and EY . We can assume that U X is the projection ofaccording to some random move. Generate fP X onto E X without seriously restricting the random moves because this assumption approximates the typical case. Thisis seen by applying Observation 1 to the special case P ∈ E . Let U Y , as usual, be the projection of P Y onto EY and W be−→P Y by a random move.the projection ofThis is justified because it is just the map of the move from U X to P X under f . Since f and this move have been chosenindependently of the manifold EY , Observation 1 states−→P Y onto EY . We now apply Observation 1 and consider P Y as obtained fromW ≈ U Y .Applying f−1 to both sides yieldsW f ≈←−P X ,(B.1)(B.2)where W f denotes the image of W under fis the point in EY that is the closest to the image of U X under f . In the typical case we expectis the point in f−1. Note that W f−1(EY ) that is the closest to U X because WD(P X (cid:10) W f ) ≈ D(P X (cid:10) U X ) + D(U X (cid:10) W f ),because the vector connecting U X with W f does not depend on P X , it only depends on f and the manifolds. The vectorpointing from U X to P X is therefore typically close to orthogonal to the one pointing from U X to W f . Together with (B.2)we thus obtainD(P X (cid:10)←−P X ) ≈ D(P X (cid:10) U X ) + D(U X (cid:10)←−P X ),which is one of the equivalent conditions in Theorem 2.In Section 4.4 we have already mentioned that in the special case of linear relations (32) between high-dimensionalGaussian variables (with isotropic Gaussians as reference manifold), Postulate 2 can be further justified by concentration ofmeasure results. It is instructive to verify that also (B.1) holds for this case. To see this, we recall that U Y has the covariancematrixτ (ΣY )I = τ(cid:2)AΣ X A T(cid:3)I,D. Janzing et al. / Artificial Intelligence 182–183 (2012) 1–3131which is approximately equal to(cid:2)(cid:3)τ (Σ X )τA A TI(see Section 4.4 and [13]). One checks easily that the latter is the covariance matrix of W , i.e., the isotropic Gaussian thatis closest to−→P Y . Using the notations above, this shows (B.1).References[1] J. Mooij, D. Janzing, Distinguishing between cause and effect, Journal of Machine Learning Research W&CP 6 (2010) 147–156.[2] P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction, and Search, Lecture Notes in Statistics, Springer-Verlag, New York, NY, 1993.[3] J. Pearl, Causality, Cambridge University Press, 2000.[4] J. Lemeire, E. Dirkx, Causal models as minimal descriptions of multivariate systems, http://parallel.vub.ac.be/~jan/, 2006.[5] D. Janzing, B. Schölkopf, Causal inference using the algorithmic Markov condition, IEEE Transactions on Information Theory 56 (10) (2010) 5168–5194.[6] R. Solomonoff, A formal theory of inductive inference, Information and Control, Part II 7 (2) (1964) 224–254.[7] G. Chaitin, A theory of program size formally identical to information theory, Journal of the Association for Computing Machinery 22 (1975) 329–340.[8] A. Kolmogorov, Three approaches to the quantitative definition of information, Problems of Information Transmission 1 (1) (1965) 1–7.[9] P. Hoyer, D. Janzing, J. Mooij, J. Peters, B. Schölkopf, Nonlinear causal discovery with additive noise models, in: Proceedings of the Conference NeuralInformation Processing Systems, NIPS 2008, Vancouver, Canada, 2009, MIT Press, 2009.[10] J. Peters, D. Janzing, B. Schölkopf, Causal inference on discrete data using additive noise models, IEEE Transactions on Pattern Analysis and MachineIntelligence 33 (12) (2011) 2436–2450.[11] K. Zhang, A. Hyvärinen, On the identifiability of the post-nonlinear causal model, in: Proceedings of the 25th Conference on Uncertainty in ArtificialIntelligence, Montreal, Canada, 2009.[12] D. Janzing, B. Steudel, Justifying additive-noise-based causal discovery via algorithmic information theory, Open Systems and Information Dynam-ics 17 (2) (2010) 189–212.[13] D. Janzing, P. Hoyer, B. Schölkopf, Telling cause from effect based on high-dimensional observations, in: Proceedings of the 27th International Confer-ence on Machine Learning, ICML 2010, Haifa, Israel, June 2010, pp. 479–486.[14] J. Zscheischler, D. Janzing, K. Zhang, Testing whether linear equations are causal: A free probability approach, in: Proceedings of the 27th InternationalConference on Uncertainty in Artificial Intelligence, UAI 2011, Barcelona, Spain, 2011, pp. 839–847.[15] P. Daniušis, D. Janzing, J. Mooij, J. Zscheischler, B. Steudel, K. Zhang, B. Schölkopf, Inferring deterministic causal relations, in: Proceedings of the 26thConference on Uncertainty in Artificial Intelligence, UAI, July 2010, pp. 1–8.[16] N. Friedman, I. Nachman, Gaussian process networks, in: Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence, UAI, Stanford, CA,USA, 2000, Morgan Kaufmann, 2000, pp. 211–219.[17] S. Amari, H. Nagaoka, Methods of Information Geometry, Oxford University Press, 1993.[18] S. Amari, Information geometry on hierarchy of probability distributions, IEEE Transactions on Information Theory 47 (5) (2001) 1701–1711.[19] A. Kraskov, H. Stoegbauer, P. Grassberger, Estimating mutual information, http://arxiv.org/abs/cond-mat/0305641v1, 2003.[20] T. Cover, J. Thomas, Elements of Information Theory, Wiley’s Series in Telecommunications, Wiley, New York, 1991.[21] A. Dembo, T. Cover, J. Thomas, Information theoretic inequalities, IEEE Transactions on Information Theory 37 (1991) 1501–1518.[22] A.J. Kinderman, J.F. Monahan, Computer generation of random variables using the ratio of uniform deviates, ACM Transactions on Mathematical Soft-ware 3 (3) (1977) 257–260.[23] S. Shimizu, P.O. Hoyer, A. Hyvärinen, A.J. Kerminen, A linear non-Gaussian acyclic model for causal discovery, Journal of Machine Learning Research 7(2006) 2003–2030.[24] Joris M. Mooij, Oliver Stegle, Dominik Janzing, Kun Zhang, Bernhard Schölkopf, Probabilistic latent variable models for distinguishing between causeand effect, in: Advances in Neural Information Processing Systems 23, NIPS 2010, Vancouver, Canada, Curran Associates, 2011, pp. 1687–1695.[25] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, B. Schölkopf, Kernel methods for measuring independence, Journal of Machine Learning Research 6(2005) 2075–2129.[26] C.E. Rasmussen, H. Nickisch, Gaussian Processes for Machine Learning (GPML) Toolbox, Journal of Machine Learning Research 11 (2010), in press.