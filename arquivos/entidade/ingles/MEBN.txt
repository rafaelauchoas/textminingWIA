Artificial Intelligence 172 (2008) 140–178www.elsevier.com/locate/artintMEBN: A language for first-order Bayesian knowledge basesKathryn Blackmond LaskeyDepartment of Systems Engineering and Operations Research, MS4A6, George Mason University, Fairfax, VA 22030, USAReceived 1 March 2006; received in revised form 13 August 2007; accepted 10 September 2007Available online 4 October 2007AbstractAlthough classical first-order logic is the de facto standard logical foundation for artificial intelligence, the lack of a built-in,semantically grounded capability for reasoning under uncertainty renders it inadequate for many important classes of problems.Probability is the best-understood and most widely applied formalism for computational scientific reasoning under uncertainty.Increasingly expressive languages are emerging for which the fundamental logical basis is probability. This paper presents Multi-Entity Bayesian Networks (MEBN), a first-order language for specifying probabilistic knowledge bases as parameterized fragmentsof Bayesian networks. MEBN fragments (MFrags) can be instantiated and combined to form arbitrarily complex graphical prob-ability models. An MFrag represents probabilistic relationships among a conceptually meaningful group of uncertain hypotheses.Thus, MEBN facilitates representation of knowledge at a natural level of granularity. The semantics of MEBN assigns a probabil-ity distribution over interpretations of an associated classical first-order theory on a finite or countably infinite domain. Bayesianinference provides both a proof theory for combining prior knowledge with observations, and a learning theory for refining a repre-sentation as evidence accrues. A proof is given that MEBN can represent a probability distribution on interpretations of any finitelyaxiomatizable first-order theory.© 2007 Elsevier B.V. All rights reserved.Keywords: Bayesian network; Graphical probability models; Knowledge representation; Multi-entity Bayesian network; Probabilistic logic;Uncertainty in artificial intelligence1. IntroductionFirst-order logic is primary among logical systems from both a theoretical and a practical standpoint. It has beenproposed as a unifying logical foundation for defining extended logics and interchanging knowledge among applica-tions written in different languages. However, its applicability has been limited by the lack of a coherent semantics forplausible reasoning. Among the many proposed logics for plausible inference, probability is the strongest contenderas a universal standard of comparison for plausible reasoning systems. Probability has proved its worth in applicationsfrom a wide variety of problem domains, and is a rationally justified calculus for plausible inference under uncertainty(e.g., [18,36,41,69]).Application of probability to complex, open-world problems requires languages based on expressive probabilisticlogics. The development of sufficiently expressive probabilistic logics has been hindered by the lack of modularityof probabilistic reasoning, the intractability of worst-case probabilistic inference, and the difficulty of ensuring thatE-mail address: klaskey@gmu.edu.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.09.006K.B. Laskey / Artificial Intelligence 172 (2008) 140–178141probability assessments give rise to a well-defined and unique probability distribution. The number of probabilitiesrequired to express a fully general probability distribution over truth-values of a collection of assertions is exponen-tial in the number of assertions, making a brute-force approach to specification and inference infeasible for all butthe simplest problems. These difficulties have been addressed by exploiting independence relationships to achieveparsimonious representation and efficient inference [59,61]. Recent years have seen a rapid evolution of increasinglypowerful languages for computational probabilistic reasoning (e.g., [10,16,27,30,31,35,40,43,44,46,48,56,60,64,66,68,71]).This paper presents multi-entity Bayesian networks (MEBN), a language for representing first-order probabilisticknowledge bases. The fundamental unit of representation in MEBN is the MFrag, a parameterized Bayesian networkfragment that represents uncertain relationships among a small collection of related hypotheses. MFrags allow knowl-edge to be specified at a natural level of granularity. Dependence relationships and local distributions are specified forconceptually meaningful clusters of related hypotheses. An MFrag can be instantiated multiple times by binding its ar-guments to different entities. MEBN thus provides a compact language for expressing complex graphical models withrepeated structure. A MEBN theory consists of a set of MFrags that satisfies consistency conditions ensuring existenceof a unique probability distribution over its random variables. MEBN theories can be used to reason consistently aboutcomplex expressions involving nested function application, arbitrary logical formulas, and quantification.The remainder of the paper is organized as follows. Section 2 provides an overview of formalisms for knowledgerepresentation and reasoning under uncertainty. Section 3 defines the MEBN language. Section 4 defines semantics,presents results on expressive power, and discusses inference. Section 5 reviews current research on expressive first-order languages. The final section is a summary and discussion. Proofs and algorithms are given in Appendix A.2. Probability and logicDavis [17] defines a logic as a schema for defining languages to describe and reason about entities in different do-mains of application. Certain key issues in representation and inference arise across a variety of application domains.A logic encodes particular approaches to these issues in a form that can be reused across languages, domains, andtheories.By far the most commonly used, studied, and implemented logical system is first-order logic (FOL), inventedindependently by Frege and Peirce in the late nineteenth century [24,62]. First-order logic is applied by defining a setof axioms, or sentences that make assertions about a domain. The axioms, together with the set of logical consequencesof the axioms, comprise a theory of the domain. Until referents for the symbols are specified, a theory is a syntacticstructure devoid of meaning. An interpretation for a theory specifies a definition of each constant, predicate andfunction symbol in terms of the domain. Each constant symbol denotes a specific entity; each predicate denotes aset containing the entities for which the predicate holds; and each function symbol denotes a function defined on thedomain. The logical consequences of a set of axioms consist of the sentences that are true in all interpretations, alsocalled the valid sentences.Special-purpose logics built on first-order logic give pre-defined meaning to reserved constant, function and/orpredicate symbols. Such logics provide built-in constructs useful in applications. There are logics that provide con-stants, predicates, and functions for reasoning about types, space and time, parts and wholes, actions and plans, etc.When a logic is applied to reason about a particular domain, the modeler assigns meaning to additional domain-specific symbols, and provides axioms to assert important properties of their intended referents. Formal ontologies[33,70] are usually expressed in languages based on first-order logic or one of its subsets.A first-order theory implies truth-values for the valid sentences and their negations, but provides no means toevaluate the plausibility of other sentences. Plausible reasoning is fundamental to intelligence, and plausible reason-ing logics have been an active area of research in artificial intelligence. Because probability is not truth-functional,naïve attempts to generalize the standard logical connectives and quantifiers to create combining rules for probabil-ities encountered many difficulties. Graphical probability models have become popular as a parsimonious languagefor representing knowledge about uncertain phenomena, a formalism for representing probabilistic knowledge in alogically coherent manner, and an architecture to support efficient algorithms for inference, search, optimization, andlearning. A graphical probability model expresses a probability distribution over a collection of related hypotheses asa graph and a collection of local probability distributions. The graph encodes dependencies among the hypotheses.The local probability distributions specify numerical probability information. Together, the graph and the local dis-142K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Fig. 1. Bayesian network for diagnostic task.tributions specify a joint distribution that respects the conditional independence assertions encoded in the graph, andhas marginal distributions consistent with the local distributions [14,42,50,61,75]. A Bayesian network (e.g., [42,59,61] is a graphical probability model in which the dependency graph is an acyclic directed graph. An example of aBayesian network for a diagnostic task is given in Fig. 1. This Bayesian network represents a joint distribution overthe Cartesian product of the possible values of the random variables depicted in the graph.Some authors assume that random variables in a Bayesian network have finitely many possible values. Some requireonly that each random variable have an associated function mapping values of its parents to probability distributionson its set of possible values. In an unconstrained local distribution on finite-cardinality random variables, a separateprobability is specified for each value of a random variable given each combination of values of its parents. Becausethe complexity of specifying local distributions is exponential in the number of parents, constrained families of localdistributions are often used to simplify specification and inference. Examples include context-specific independence[8,26,53,55] and independence of causal influence (ICI) models such as the “noisy or” [42,61]. When a randomvariable and/or its parents have infinitely many possible values, local distributions cannot be listed explicitly, butcan be specified as parameterized functions using local expression languages [15]. When a random variable has anuncountable set of possible values, then the local distributions specify probability density functions with respect to ameasure on the set of possible outcomes (cf., [6,21]).The simple attribute-value representation of standard Bayesian networks is insufficiently expressive for many prob-lems. For example, the Bayesian network of Fig. 1 applies to a single piece of equipment located in a particular roomand owned and maintained by a single organization. We may need to consider problems that involve multiple orga-nizations, each of which owns and maintains multiple pieces of equipment of different types, some of which are inrooms that contain other items of equipment. The room temperature and air conditioner status random variables wouldhave the same value for co-located items, and the maintenance practice random variable would have the same valuefor items with the same owner. Standard Bayesian networks provide no way of compactly representing the correlationbetween failures of co-located and/or commonly owned items of equipment or of properly accounting for these cor-relations when learning from observation. For this reason, extensions to the Bayesian network formalism have beendeveloped to provide greater expressivity.Object-oriented Bayesian networks (OOBNs) and probabilistic relational models (PRMs) provide a natural wayto represent uncertainty about the attributes of instances of different types of objects, where objects of a given typehave attributes drawn from the same distribution (c.f., [63]). That is, they provide a direct way to represent uncertaintyabout the values of unary functions and relations. Representing uncertainty about n-ary functions and relations is morecumbersome. One must reify an n-element argument sequence, define an attribute of the reified entity to represent thedesired function, and then specify a distribution for the attribute.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178143Unlike OOBNs and PRMs, logic-based languages (e.g., [60]) can represent n-ary functions and relations in astraightforward way. Whereas the unit of expression for OOBNs and PRMs is an object type and its attributes, theunit of expression for logic-based languages is the local distribution for an individual term in the language.Like logic-based languages, MEBN can represent uncertainty about the values of n-ary functions and relations ina natural way. An attractive feature of MEBN is that distributions are specified over conceptually meaningful clustersof related hypotheses. This unit of representation facilitates modular specification of MFrag knowledge bases. UnlikePRMs or OOBNs, the hypotheses represented in a given MFrag need not be attributes of a single entity or a singlereified list of entities, but can refer to attributes and relations of different entities. The arguments of random variablesare defined at the level of the MFrag. Therefore, when an MFrag is instantiated, all occurrences of a given argumentmust be bound to the same entity. This constraint is useful in some applications, and is natural to specify in MEBN,but is cumbersome to represent in other languages. In these respects, MEBN is similar to the plate language [30],another language for which the unit of representation is a cluster of related random variables. However, plates cannotexpress nested function application or uncertainty about existence, type, and number. These kinds of uncertainty areeasily expressed in MEBN.The most natural semantics for first-order probabilistic languages is an extension of first-order model theory thatassigns probabilities to sets of interpretations (cf., [67], Section 14.6). All the above formalisms can all be giventhis kind of declarative semantics. That is, a probabilistic knowledge base for any of the above formalisms can betranslated into: (i) a set of first-order axioms defining a set of possible worlds; (ii) statements that allow probabilitiesto be assigned in a consistent way to sets of possible worlds; and optionally (iii) context axioms that index proba-bility distributions. A probabilistic knowledge base then specifies a probability distribution on possible worlds, orin some languages a family of probability distributions indexed by contexts. Different languages have the power toexpress different subsets of first-order logic, with different implications for tractable inference in particular classes ofproblem.Development of probabilistic languages is a vital area of research. A range of solutions is needed to balance expres-siveness against tractability. Among the variety of approaches, some are needed that push the bounds of expressivepower outward. Highly expressive languages are necessary for knowledge interchange, for analyzing the theoreticalproperties of languages with different expressive power, for representing arbitrarily complex knowledge bases andontologies in machine-understandable form, and for defining tractable special cases and approximations to intractableor undecidable problems. Languages and implementations based on subsets of first-order logic vary widely in expres-siveness and tractability for different problem classes. As an example of a very expressive logic, the newly releasedCommon Logic standard [38] is intended for exchange of information among diverse applications. As such, one of itsrequirements is to be at least as expressive as first-order logic. This implies, of course, that it can represent intractableand even undecidable problems. Although this renders Common Logic an undesirable representation medium forsome applications, this level of expressiveness is necessary for other purposes.In a similar vein, there is a need for formalisms capable of specifying arbitrarily expressive probabilistic knowledgebases. For this reason, MEBN was designed to be capable of representing arbitrary first-order sentences. Theorem 5of Section 4.2 proves that MEBN can represent a probability distribution over interpretations of any finitely axioma-tizable first-order theory. This is a non-trivial result. Because sets of first-order axioms may have uncountably manyinterpretations, simply defining a probability distribution over interpretations does not ensure that the set of modelsof an arbitrary sentence will be measurable. In other words, there is no guarantee that an arbitrary probability distri-bution on first-order sentences will assign a probability to every sentence. Furthermore, because first-order logic isundecidable, inference in any probabilistic logic that contains first-order logic must be undecidable in the worst case.Nevertheless, MEBN can implicitly specify answers to arbitrary probabilistic queries about interpretations of finitelyaxiomatizable theories, and the process defined in Section 4.3 converges to the correct answer in the infinite limit.These results are theoretically significant.Theoretical issues aside, practical applications demand tractable solutions. The common semantics shared byMEBN and other first-order probabilistic languages facilitates the definition of translations among representations(cf., [35]). This makes it possible to define restricted subsets of MEBN for which tractable solutions exist tocertain classes of problems. MEBN implementations may make restrictions on expressivity to ensure tractability.Knowledge engineers and inference algorithm designers may make various trade-offs to ensure acceptable perfor-mance in applications. Section 5 provides a brief discussion of the relationship between MEBN and other expres-sive probabilistic languages. The discussion in that section could be extended to establish translations from other144K.B. Laskey / Artificial Intelligence 172 (2008) 140–178expressive first-order languages into MEBN, and from appropriately restricted variants of MEBN into other lan-guages.3. Multi-entity Bayesian networksLike Bayesian networks, MEBN theories use directed graphs to specify joint probability distributions for a col-lection of related random variables. The MEBN language extends ordinary Bayesian networks to provide first-orderexpressive power, and also extends first-order logic (FOL) to provide a means of specifying probability distributionsover interpretations of first-order theories.Knowledge in MEBN theories is expressed via MEBN Fragments (MFrags), each of which represents probabilityinformation about a group of related random variables. Just as first-order logic extends propositional logic to providean inner structure for sentences, MEBN theories extend ordinary Bayesian networks to provide an inner structurefor random variables. Random variables in MEBN theories take arguments that refer to entities in the domain ofapplication. For example, Manager(d, y) might represent the manager of the department designated by the variable dduring the year designated by the variable y. To refer to the manager of the maintenance department in 2003, we wouldfill in values for d and y to obtain an instance Manager(Maintenance, 2003) of the Manager random variable. A givensituation might involve any number of instances of the Manager random variable, referring to different departmentsand/or different years. As shown below, the Boolean connectives and quantifiers of first-order logic are represented aspre-defined MFrags whose meaning is fixed by the semantics. A MEBN theory implicitly expresses a joint probabilitydistribution over truth-values of sets of FOL sentences. Any sentence that can be expressed in first-order logic canbe represented as a random variable in a MEBN theory. The MEBN language is modular and compositional. That is,probability distributions are specified locally over small groups of hypotheses and composed into globally consistentprobability distributions over sets of hypotheses.3.1. Entities and random variablesThe MEBN language treats the world as being comprised of entities that have attributes and are related to other en-tities. Constant and variable symbols are used to refer to entities. There are three logical constants with meaning fixedby the semantics of the logic, an infinite collection of variable symbols, and an infinite collection of domain-specificconstant symbols with no pre-specified referents. Random variables represent features of entities and relationshipsamong entities. There is a collection of logical random variable symbols with meaning fixed by the semantics ofthe logic, and an infinite collection of domain-specific random variable symbols with no pre-specified referents. Thelogical constants and random variables are common to all MEBN theories; the domain-specific constants and randomvariables provide terminology for referring to objects and relationships in a domain of application.Constant and variable symbols:• (Ordinary) variable symbols: As in FOL, variables are used as placeholders to refer to non-specific entities. Vari-ables are written as alphanumeric strings beginning with lowercase letters, e.g., department7. To avoid confusion,the adjective “ordinary” is sometimes used to distinguish ordinary variables from random variables.• Phenomenal (non-logical) constant symbols: Particular named entities are represented using constant symbols. Asin our FOL notation, phenomenal constant symbols are written as alphanumeric strings beginning with uppercaseletters, e.g., Machine37, Fernandez.• Unique Identifier symbols: The same entity may be represented by different phenomenal constant symbols. MEBNavoids ambiguity by assigning a unique identifier symbol to each entity. The unique identifiers are the possiblevalues of random variables. There are two kinds of unique identifier symbols:◦ Truth-value symbols and the undefined symbol: The reserved symbols T, F and ⊥ are logical constants withpre-defined meaning fixed by the semantics. The symbol ⊥ denotes meaningless, undefined or contradictoryhypotheses, i.e., hypotheses to which a truth-value cannot be assigned. The symbols T and F denote truth-valuesof meaningful hypotheses.◦ Entity identifier symbols. There is an infinite set E of entity identifier symbols. An interpretation of the theoryuses entity identifiers as labels for entities. Entity identifiers are written either as numerals or as alphanumericstrings beginning with an exclamation point, e.g., !M3, 48723.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178145Random variable symbols:• Logical connectives and the equality operator: The logical connective symbols ¬, ∧, ∨, ⇒, and ⇔, together withthe equality relation =, are reserved random variable symbols with pre-defined meanings fixed by the semantics.Logical expressions may be written using prefix notation (e.g., ¬(ψ), ∨(ψ, φ), = (ψ, φ)), or in the more familiarinfix notation (e.g., ¬ψ, (ψ ∨ y); (ψ = y)). Different ways of writing the same expression (e.g., = (ψ, φ), (φ =ψ)) are treated as the same random variable.• Quantifiers: The symbols ∀ and ∃ are reserved random variable symbols with pre-defined meaning fixed by thesemantics. They are used to construct MEBN random variables to represent FOL sentences containing quantifiers.• Identity: The reserved random variable symbol (cid:2) denotes the identity random variable. It is the identity functionon T, F, ⊥, and the set of entity identifiers that denote meaningful entities in a domain. It maps meaningless,irrelevant, or contradictory random variable terms to ⊥.• Findings: The finding random variable symbol, denoted Φ, is used to represent observed evidence, and also torepresent constraints assumed to hold among entities in a domain of application.• Domain-specific random variable symbols: The domain-specific random variable symbols are written as alphanu-meric strings beginning with an uppercase letter. With each random variable symbol is associated a positiveinteger indicating the number of arguments it takes. Each random variable also has an associated set of possiblevalues consisting of a recursive subset of the unique identifier symbols. The set of possible values may be infinite,but if so, there must exist an effective procedure that lists all the possible values and an effective procedure fordetermining whether any unique identifier symbol is one of the possible values. If the set of possible values iscontained in {T, F, ⊥}, the random variable is called a logical random variable. For all other random variables,called phenomenal random variables, the set of possible values is contained in E ∪ {⊥}. Logical random variablescorrespond to predicates and phenomenal random variables correspond to functions in FOL. Local distributions(see Definition 3 below) may further restrict the set of possible values as a function of the values of the randomvariable’s parents.Exemplar symbols. There is an infinite set of exemplar symbols used to refer to representative fillers for variablesin the range of quantifiers. An exemplar symbol is denoted by $ followed by an alphanumeric string, e.g., $b32.1Punctuation:• MEBN random variable terms are constructed using the above symbols and the punctuation symbols comma,open parenthesis and close parenthesis.A random variable term is a random variable symbol followed by a parenthesized list of arguments separated bycommas, where the arguments may be variables, constant symbols, or (recursively) random variable terms. When α isa constant or ordinary variable, the random variable term (cid:2)(α) may be denoted simply as α. If ψ is a random variablesymbol, a value assignment term for ψ has the form = (ψ, α), where ψ is a random variable term and α is either anordinary variable symbol or one of the possible values of ψ. The strings = (α, ψ), (α = ψ), and (ψ = α) are treatedas synonyms for = (ψ, α). A random variable term is closed if it contains no ordinary variable symbols and open ifit contains ordinary variable symbols. An open random variable term is also called a random variable class; a closedrandom variable term is called a random variable instance. If a random variable instance is obtained by substitutingconstant terms for the variable terms in a random variable class, then it is called an instance of the class. For example,the value assignment term = (BeltStatus(!B1), !OK), also written (BeltStatus(!B1) = !OK), is an instance of both(BeltStatus(b) = x) and (BeltStatus(!B1) = x), but not of (BeltStatus(b) = !Broken). When no confusion is likely toresult, random variable classes and instances may be referred to as random variables. A random variable term is calledsimple if all its arguments are either unique identifier symbols or variable symbols; otherwise, it is called composite.For example, = (BeltStatus(!B1), !OK) is a composite random variable term containing the simple random variableterm BeltStatus(!B1) as an argument. It is assumed that the sets consisting of ordinary variable symbols, unique1 Exemplar symbols were called Skolem symbols in earlier work (e.g., [46]) because, in analogy to Skolem functions, exemplar symbols replacevariables in the range of quantifiers. However, exemplars are different from Skolem functions, and the terminology was changed to avoid confusion.146K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Fig. 2. MEBN fragments for equipment diagnosis problem.identifier symbols, exemplar random variable symbols, phenomenal constant symbols, and domain-specific randomvariable symbols are all recursive.3.2. MEBN fragmentsIn MEBN theories, multivariate probability distributions are built up from MEBN fragments or MFrags (see Fig. 2).An MFrag defines a probability distribution for a set of resident random variables conditional on the values of contextand input random variables. Random variables are represented as nodes in a fragment graph whose arcs representdependency relationships.Definition 1. An MFragF = (C, I, R, G, D) consists of a finite set C of context value assignment terms;2 a finite setI of input random variable terms; a finite set R of resident random variable terms; a fragment graph G; and a set D oflocal distributions, one for each member of R. The sets C, I, and R are pairwise disjoint. The fragment graph G is anacyclic directed graph whose nodes are in one-to-one correspondence with the random variables in I ∪ R, such thatrandom variables in I correspond to root nodes in G. Local distributions specify conditional probability distributionsfor the resident random variables as described in Definition 3 below.An MFrag is a schema for specifying conditional probability distributions for instances of its resident random vari-ables given the values of instances of their parents in the fragment graph and given the context constraints. A collectionof MFrags that satisfies the global consistency constraints defined in Section 3.3 below represents a joint probabil-ity distribution on an unbounded and possibly infinite number of instances of its random variable terms. The joint2 If φ is a logical random variable, the context constraint φ = T may be abbreviated φ and the context constraint φ = F may be abbreviated ¬φ.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178147distribution is specified via the local distributions, which are defined formally below, together with the conditional in-dependence relationships implied by the fragment graphs. Context terms are used to specify constraints under whichthe local distributions apply.As in ordinary Bayesian networks, a local distribution maps configurations of values of the parents of a randomvariable instance to probability distributions for its possible values. When all ordinary variables in the parents of aresident random variable term also appear in the resident term itself, as for the RoomTemp and TempLight randomvariables of the temperature observability MFrag of Fig. 2, a local distribution can be specified simply by listing aprobability distribution for the child random variable for each combination of values of the parent random variables.The situation is more complicated when ordinary variables in a parent random variable do not appear in the child. Inthis case, there may be an arbitrary, possibly infinite number of instances of a parent for any given instance of the child.For example, in the engine status fragment of Fig. 2, if it is uncertain where a machine is located, the temperaturein any room in which it might be located is relevant to the distribution of the EngineStatus random variable. If amachine has more than one belt, then the status of any of its belts is relevant to the distribution of the EngineStatusrandom variable. Thus, any number of instances of the RoomTemp and BeltStatus random variables might be relevantto the distributions of the EngineStatus random variable. In this case, the local distribution for a random variable mustspecify how to combine influences from all relevant instances of its parents. The standard approaches to this problemare aggregation functions and combining rules [58].MEBN local distributions combine influences of multiple parents through influence counts. In a standard Bayesiannetwork, the probability distribution for a node depends on the configuration of states of its parents. In a MEBN theory,different substitutions for the ordinary variables may yield multiple instantiations of the parents. Each allowablesubstitution defines a parent set, and each parent set has a configuration of states. Influence counts tally the number oftimes each configuration of the parents occurs among these parent sets.Influence counts may seem unintuitive at first, but they extend the causal Markov condition of ordinary Bayesiannetworks in a natural and very general way. According to the causal Markov condition, the distribution of a child maydepend on no information except the values taken on by its parents. For a Bayesian network, each node has a fixednumber of parents that have exactly one configuration in any given possible world. The local distribution in a Bayesiannetwork is a function of this configuration. For an MFrag, when there are multiple instances of the parents of a randomvariable, this gives rise to a set of parent configurations, one for each allowable substitution for the arguments of theparents. The basic idea of influence counts is that when multiple bindings result in the same configuration of parentstates, the only information relevant to the local distribution should be how many cases there are of each configuration,not any other information such as which particular instances participated in each of the bindings. This is exactly whatis represented by influence counts. In the engine status example, if a machine might be located in one of several roomsand might have more than one belt, then there is a configuration of the RoomStatus and RoomTemp variables for eachallowable substitution of rooms and belts. This is explained in detail in the example below.Configurations of the parent random variables that are relevant to the distribution of the child are called influencingconfigurations. The local distribution πψ for a resident random variable ψ in MFrag F specifies, for each instance ofψ: (i) a set of possible values; (ii) a rule for determining the influencing configurations; and (iii) a rule for assigningprobabilities to the possible values given an influencing configuration. This statement is formalized in Definition 3below. Before proceeding to a formal definition of local distributions, a formal definition of influence counts is given,followed by an example of how they are used to define local distributions.Definition 2. Let F be an MFrag containing ordinary variables θ1, . . . , θk, and let ψ(θ ) denote a resident randomvariable in F that may depend on some or all of the θi .2a A binding set B = {(θ1 : ε1), (θ2 : ε2), . . . (θk : εk)} for F is a set of ordered pairs associating a unique identifiersymbol εi with each ordinary variable θi of F . The constant symbol εi is called the binding for variable θidetermined by B. The εi are not required to be distinct.2b Let B = {(θ1 : ε1), (θ2 : ε2), . . . (θk : εk)} be a binding set for F , and let ψ(ε) denote the instance of ψ obtained bysubstituting εi for each occurrence of θi in ψ(θ ). A potential influencing configuration for ψ(ε) and B is a set ofvalue assignment terms {(γ = φ(ε))}, one for each parent of ψ and one for each context random variable of F .Here, φ(ε) denotes the instance of the context or parent random variable φ(θ ) obtained by substituting εi for each148K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Table 1Partial world state for EngineStatus partial worldIsa(Machine, !M1) = TIsa(Belt, !M1) = FIsa(Room, !M1) = FBeltLocation(!M1) = ⊥MachineLocation(!M1) = !R1RoomTemp(!M1) = ⊥BeltStatus(!M1) = ⊥Isa(Machine, !B1) = FIsa(Belt, !B1) = TIsa(Room, !B1) = FBeltLocation(!B1) = !M1MachineLocation(!B1) = ⊥RoomTemp(!B1) = ⊥BeltStatus(!B1) = !OKIsa(Machine, !R1) = FIsa(Belt, !R1) = FIsa(Room, !R1) = TBeltLocation(!R1) = ⊥MachineLocation(!R1) = ⊥RoomTemp(!R1) = !NormalBeltStatus(!R1) = ⊥Isa(Machine, !B2) = FIsa(Belt, !B2) = TIsa(Room, !B2) = FBeltLocation(!B2) = !M1MachineLocation(!B2) = ⊥RoomTemp(!B2) = ⊥BeltStatus(!B2) = !OKIsa(Machine, !R2) = FIsa(Belt, !R2) = FIsa(Room, !R2) = TBeltLocation(!R2) = ⊥MachineLocation(!R2) = ⊥RoomTemp(!R2) = HotBeltStatus(!R2) = ⊥Isa(Machine, !O1) = FIsa(Belt, !O1) = FIsa(Room, !O1) = FBeltLocation(!O1) = ⊥MachineLocation(!O1) = ⊥RoomTemp(!O1) = ⊥BeltStatus(!O1) = ⊥occurrence of θi ;3 and γ denotes one of the possible values of φ(ε) (as specified by the local distribution πφ;see Definition 3 below). An influencing configuration for ψ(ε) and B is a potential influencing configuration inwhich the value assignments match the context constraints of F . Two influencing configurations are equivalentif substituting θi back in for εi yields the same result for both configurations. The equivalence classes for thisequivalence relation correspond to distinct configurations of parents of ψ(θ ) in F .2c Let {ε1, ε2, . . . , εn} be a non-empty, finite set of entity identifier symbols. The partial world W for ψ and{ε1, ε2, . . . , εn} is the set consisting of all instances of the parents of ψ and the context random variables of Fthat can be formed by substituting the εi for ordinary variables of F . A partial world state SW for a partial worldis a set of value assignment terms, one for each random variable in the partial world.2d Let W be a partial world for ψ and {ε1, ε2, . . . , εn}, let SW be a partial world state for W, let B = {(θ1 : εB1),(θ2 : εB2), . . . , (θk : εBk)} be a binding set for F with bindings chosen from {ε1, ε2, . . . , εn}, and let ψ(εB) bethe instance of ψ(θ ) from B. The influence counts #SWψ for ψ(αB) in SW consist of the number of influencingconfigurations SW contains for each equivalence class of influencing configurations (i.e., each configuration ofthe parents of ψ(θ ) in F ).As an example, Table 1 shows a partial world state for the EngineStatus(m) random variable from Fig. 2 withunique identifiers {!M1, !R1, !R2, !B1, !B2, !O1}. In the intended meaning of the partial world of Table 1, !M1denotes a machine, !B1 and !B2 denote belts located in !M1, !R1 denotes the room where !M1 is located, !R2 denotesa room where !M1 is not located, and !O1 denotes an entity that is not a machine, a room, or a belt. The partial worldstate specifies the value of each random variable for each of the entity identifiers. Random variables map meaninglessattributes (e.g., the value of RoomTemp for an entity that is not a room) to the absurd symbol ⊥.To construct the influencing configurations, we first examine Table 1 to find all configurations of context randomvariables that satisfy the context constraints. The first constraint is that the entity bound to m must be a machine. Theonly instance of Isa(Machine, m) with value T binds m to !M1. Therefore, all influencing configurations must include(Isa(Machine, !M1) = T). Next, consider the constraint that the entity bound to r must be a room. There are twoassignments satisfying this constraint: (Isa(Room, !R1) = T) and (Isa(Room, !R2) = T). All influencing configurationsmust contain one of these two value assignments. Now, consider the third context constraint, that the machine boundto m must be located in the room bound to r. This constraint is satisfied by only one binding, of m to !M1 and r to!R1. This eliminates (Isa(Room, !R2) = T) from the influencing configurations. Thus, all influencing configurationsmust contain (Isa(Room, !R1) = T) and (MachineLocation(!M1) = !R1). Next, consider the constraints that the entitybound to b must be a belt located in the machine bound to m. These constraints are satisfied by binding b to either !B1or !B2. Therefore, each influencing configuration must contain either (Isa(Belt, !B1) = T) and (BeltLocation(!B1) =!M1), or (Isa(Belt, !B2) = T) and (BeltLocation(!B2) = !M1). Putting all this information together, the partial worldstate of Table 1 contains two influencing configurations for EngineStatus(!M1):3 If a context value assignment term (γ = φ) has no arguments, then no substitution is needed.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178149IC1: {(Isa(Machine, !M1) = T), (Isa(Belt, !B1) = T), (Isa(Room, !R1) = T),(BeltLocation(!B1) = !M1), (MachineLocation(!M1) = !R1), (RoomTemp(!R1) = !Normal),(BeltStatus(!B1) = !OK)}; andIC2: {Isa(Machine, !M1) = T), (Isa(Belt, !B2) = T), (Isa(Room, !R1) = T),(BeltLocation(!B2) = M1),(MachineLocation(!M1) = !R1), (RoomTemp(!R1) = !Normal),(BeltStatus(!B2) = !OK)}.The partial world state of Table 1 contains no other influencing configurations for EngineStatus(!M1). In both IC1 andIC2, the room temperature is normal and the belt status is OK. Therefore, the influence counts for EngineStatus(!M1)in this possible world state are:RoomTemp =!Normal, BeltStatus =!OKRoomTemp =!Normal, BeltStatus =!BrokenRoomTemp =!Hot,RoomTemp =!Hot,: 2: 0: 0: 0.The local distribution assigned to EngineStatus(M1) in this partial world state would thus be the one for a machinehaving two intact and no broken belts, and located in a room with normal room temperature.BeltStatus =!OKBeltStatus =!BrokenBecause of their generality, influence counts can represent both aggregation functions and combining rules (see dis-cussion in Section 5 below). Implementations of MEBN are free to restrict local distributions for reasons of tractabilityor cognitive naturalness. For example, an implementation might provide a set of standard combining rules such asnoisy Boolean and arithmetic functions, and possibly also provide a language for defining combining functions, butwould not necessarily provide a fully general language for specifying local distributions as a function of influencecounts. That is, influence counts provide for the most general specification of local distributions, but this full general-ity would not necessarily be available in every implementation.Definition 3. The local distribution πψ for resident random variable ψ in MFrag F specifies, for each instance ψ(ε)of ψ : (i) a subset Vψ(ε) of possible values for ψ(ε); and (ii) a function πψ(ε)(α|S) that maps unique identifiers α andpartial world states S to real numbers, such that the following conditions are satisfied:(cid:2)3a For a given partial world state S, πψ(ε)(·|S) is a probability distribution on the unique identifier symbols. That is,πψ(ε)(α|S) (cid:2) 0 andα πψ(ε)(α|S) = 1, where α ranges over the unique identifier symbols.3b For each instance ψ(ε) of ψ, the set Vψ(ε) of possible values of the instance ψ(ε) is a recursive subset of theset of possible values for ψ, and πψ(ε)(Vψ(ε)|S) = 1 for each partial world S. As noted above, there is a set ofpossible values associated with the random variable ψ. This condition states that the set of possible values for aninstance ψ(ε) may be a proper subset of the set of possible values for ψ. If this is the case, then the parents of ψin the fragment graph must include the entity identifiers for one or more of the arguments of ψ, and the possiblevalues of an instance ψ(ε) must be a function of the entity identifiers (cid:2)(εi) of one or more arguments εi . That is,the set of possible values for an instance ψ(ε) may depend only on the random variable class ψ and the identifiersof the entities being substituted for its arguments.3c There is an algorithm such that for any finite subset A of the possible values of ψ(ε) not containing ⊥, and for anypartial world state S for ψ, either the algorithm halts with output πψ(ε)(A|S) or there exists a value N(A, S) suchthat if the algorithm is interrupted after a number of time steps greater than N(A, S), the output is πψ(ε)(A|S).43d πψ(ε) depends on the partial world state only through the influence counts. That is, any two partial world stateshaving the same influence counts map to the same probability distribution.3e Let S1 ⊂ S2 ⊂ · · · be an increasing sequence of partial world states for ψ, and let α be one of the possible valuesfor ψ. There exists an integer N such that if k > N, πψ(ε)(α|Sk) = πψ(ε)(α|SN ).5The probability distribution πψ (ε|∅) is called the default distribution for ψ. It is the probability distribution for ψgiven that no potential influencing configurations satisfy the conditioning constraints of F . If ψ is a root node in anMFrag F containing no context constraints, then the local distribution for ψ is just the default distribution.4 It is required that N(A, S) exists, but there need not be an effective procedure for computing it.5 Again, it is not required that there be an effective procedure for computing N.150K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Table 2Local distribution as function of influence countsContextRoomTemp(r)BeltStatus(b)Belt b locatedin machine m,located inroom r!Normal!HotDefault!OK : k!Broken : n − k!OK : k!Broken : n − kEngineStatus(m)Satisfactoryαk,nβk,n0Overheated1 − αk,n1 − βk,n0⊥001Conditions such as 3c and 3e are needed to ensure that a global joint distribution exists and can be approximatedby a sequence of finite Bayesian networks. The conditions given here are stronger than strictly necessary. Becausethey are satisfied in the MEBN theory for first-order logic presented in Section 4.2 below, they are sufficient todemonstrate the existence of a fully first-order Bayesian logic. Nevertheless, identifying suitable relaxations of theseconditions is an important topic for future research. For example, in some applications it would be useful to define arandom variable as the average of infinitely many instances of its parent. It is clear that such a local distribution wouldnot satisfy condition 3e. Results on convergence of averages to limiting distributions (e.g., [6]) might be applied toidentify suitable relaxations of these conditions. It should be noted in this connection that most papers on expressiveprobabilistic languages explicitly assume the domain is finite (see, for example, [35]). In finite domains, includingfinite domains of uncertain and unbounded cardinality, conditions 3c and 3e are automatically satisfied.Although the sets Vψ(ε) are finite or countably infinite, it is possible to use MEBN to define distributionson arbitrary spaces. We can view the entity identifiers as labels for the elements of a sequence sampled ran-domly from a set that may be uncountably infinite. The characteristics of the sampled elements are specified viathe distributions of features. For example, StdUniform(1), StdUniform(2), . . . , might represent labels for uniformrandom numbers drawn from the unit interval. We might define these labels as StdUniform(1) = !StdUniform1,StdUniform(2) = !StdUniform2, . . . , respectively. The random variable Digit(u, k) might then denote the kth digitof the nth uniform random number. The values Digit(u, k) would be mutually independent with uniform distributionson the set {0, 1}. A uniform random number could be specified to arbitrary precision by drawing a sufficiently longsequence of digits.Table 2 shows an example of a local distribution for the engine status MFrag. The conditioning constraints implythere can be at most one RoomTemp parent that satisfies the context constraint MachineLocation(m) = r. When thisparent has value !Normal, probability αk,n is assigned to !Normal and probability 1 − αk,n is assigned to !Overheated,where k is the number of distinct BeltStatus parents having the value OK, out of a total of n > 0 distinct BeltStatusparents. When the RoomTemp parent corresponding to MachineLocation(m) has value !Hot, the probability of a sat-isfactory engine is βk,n and the probability of an overheated engine is 1 − βk,n, where again k denotes the number ofdistinct belts with value OK and n > 0 denotes the total number of distinct belts. The default distribution applies whenno combination of entities meets the conditioning constraints. It assigns probability 1 to ⊥, meaning that EngineSta-tus(m) is meaningless when the context constraints are not met (i.e., m does not denote a machine, m is not located in aroom, or m has no belt). Default distributions need not assign probability 1 to ⊥. For example, the default distributioncould be used to represent the engine status of beltless machines. Note, however, that the default distribution does notdistinguish situations in which m refers to a machine with no belt from situations in which m is not a machine. Thus,this modeling approach would assign the same EngineStatus distribution to non-machines as to machines with no belt.MFrags may contain recursive influences. Recursive influences allow instances of a random variable to dependdirectly or indirectly on other instances of the same random variable. One common type of recursive graphical modelis a dynamic Bayesian network [29,57]. Recursion is permissible as long as no random variable instance can directly orindirectly influence itself. This requirement is satisfied when the conditioning constraints prevent circular influences.For example, Fig. 3 modifies the belt status MFrag from Fig. 2 so that the status of a belt depends not only on themaintenance practice of the organization, but also on the status of the belt at the previous time. The context constraints = Prev(t) prevents circular influences in instances of this MFrag. The distribution depends on a random variablePrev(t) whose home MFrag is not shown here. Prev maps each positive numeral to the previous numeral, and mapsother entity identifiers to ⊥. If the variable t is bound to 0, there will be no potential influencing configurations thatsatisfy the context constraints (because Prev(0) has value ⊥ and Isa(NatNumber(⊥) = F). Therefore, by Definition 2,K.B. Laskey / Artificial Intelligence 172 (2008) 140–178151Fig. 3. Recursive MFrag.Fig. 4. Indirect reference.there are no influencing configurations for the BeltStatus random variable. Thus, any instance of the BeltStatus randomvariable for which t is bound to zero will have no parents, and its local distribution will be the default distribution.When t is bound to a positive numeral, the only influencing configuration will have s and t bound to consecutivenumerals. Therefore, each BeltStatus instance has exactly one active BeltStatus parent, the one for the same belt at theprevious time.MFrags can represent a rich family of probability distributions over interpretations of first-order theories. Theability of MFrags to represent uncertainty about parameters of local distributions provides a logical foundation for pa-rameter learning in first-order probabilistic theories. Uncertainty about structure can be represented by sets of MFragshaving mutually exclusive context constraints and different fragment graphs, thus providing a logical foundation forstructure learning. Of course, additional work is needed to define and implement learning algorithms for MEBNtheories.MEBN comes equipped with a set of built-in MFrags representing logical operations, function composition, andquantification. All other MFrags are called domain-specific MFrags. The domain-specific MFrags in a MEBN theorymust satisfy constraints that ensure logical consistency of the theory. The built-in MFrags, the constraints on domain-specific MFrag definitions, and the rules for combining MFrags and performing inference provide the logical contentof Bayesian logic. An applied MEBN theory augments the built-in MFrags with a set of domain-specific MFrags thatprovide empirical and/or mathematical content.The built-in MFrags are defined below:• Indirect reference. The rules for instantiating MFrags allow only unique identifier symbols to be substituted forthe ordinary variable symbols. Probability distributions for indirect references are handled with built-in compo-sition MFrags, as illustrated in Fig. 4. These MFrags enforce logical constraints on function composition. Letψ(φ1(α1), . . . , φk(αk)) be a random variable instance, where ψ and φi are random variable symbols and eachαi is a list of arguments. The random variable instance ψ(φ1(α1), . . . , φk(αk)) has a parent φi(αi) for each ofthe arguments and a reference parent ψ(y1, . . . , yk), where the yi denote ordinary variable symbols such that yimay be the same as yj only if φi(αi) and φj (αj ) are logically equivalent expressions.6 The local distribution forψ(φ1(α1), . . . , φk(αk)) assigns it the same value as ψ(y1, . . . , yk) when the value of yi is the same as the valueof φi(αi). Although there are infinitely many possible substitutions for ψ(y1, . . . , yk) and hence infinitely manypotential influencing configurations, in any given world only one of the influences is active. Thus, condition 3e6 It is always permissible to use distinct variables in a composition MFrag, but it is more efficient to use the same variable when the expressionsare known to be logically equivalent.152K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Fig. 5. Logical connective MFrag.Fig. 6. Quantifier MFrags.is satisfied. The default distribution specifies a value for ψ(φ1(α1), . . . , φk(αk)) when there are no influencingconfigurations.• Equality random variable. The resident random variable in the equality MFrag has the form = (u, v), also written(u = v). There are two parents, one for each argument. The equality operator has value ⊥ if either u or v has value⊥, T if φ and ψ have the same value and are not equal to ⊥, and F otherwise. It is assumed that meaningful entityidentifiers are distinct. That is, if ε1 and ε2 are distinct entity identifiers, then (ε1 = ε2) has value ⊥ if (cid:2)(ε1) or(cid:2)(ε2) has value ⊥, and F otherwise.• Logical connectives. The random variable ¬(u) has a single parent, (cid:2)(u); the other logical connectives have twoparents, (cid:2)(u) and (cid:2)(v). The value of ¬(u) is T if its parent has value F, F if its parent has value T, and ⊥ otherwise.The other logical connectives map truth-values according to the usual truth tables and parents other than T or F to⊥ (see Fig. 5).• Quantifiers. Let φ(γ ) be an open logical random variable term containing the ordinary variable γ . A quantifierrandom variable has the form ∀(σ, φ(σ )) or ∃(σ, φ(σ )), where φ(σ ) is obtained by substituting the exemplarterm σ into φ(γ ). A quantifier random variable instance has a single parent φ(γ ). The value of ∀(σ, φ(σ )) is Tby default and F if any instance of φ(γ ) has value F. The value of ∃(σ, φ(σ )) is F by default and T if any instanceof φ(γ ) has value T. It is assumed that a unique exemplar symbol is assigned to each ordinary variable of eachlogical random variable term of the language.7 Fig. 6 shows quantifier MFrags representing the hypothesis thatevery machine has a belt. In FOL, the corresponding sentence is:∀m∃b(Isa(Machine, m) ⇒ Isa(Belt, b) ∧ (m = BeltLocation(b))).An important feature of MEBN is its logically consistent treatment of reference uncertainty. For example, supposethe random variable instance CertificationLevel(Manager(Maintenance, 2003)) is intended to refer to the individualwho managed the maintenance department in 2003. If the possible managers are !Employee37 and !Employee49,7 A countable infinity of exemplar symbols is sufficient for this purpose.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178153Fig. 7. Relating a name to a unique identifier.the distribution for CertificationLevel(Manager(Maintenance, 2003)) will be a weighted average of the probabil-ity distributions for CertificationLevel(!Employee37) and CertificationLevel(!Employee49), where the weights arethe probabilities that Manager(Maintenance, 2003) has value !Employee37 and !Employee49, respectively. If !Em-ployee39 refers to an individual who is also referred to as Carlos, Fernandez, and Father(Miguel), any informationgermane to the certification level of Carlos, Fernandez or Father(Miguel) will propagate consistently to Certification-Level(Manager(Maintenance, 2003)) when Bayesian inference is applied (see Fig. 7). The indirect reference MFragsenforce these logical constraints on function composition.The built-in MFrags defined above provide sufficient expressive power to represent a probability distribution overinterpretations of any finitely axiomatizable FOL theory. Bayesian conditioning can be applied to generate a sequenceof MEBN theories, where each theory in the sequence conditions the preceding theory on new axioms that are consis-tent with all previous axioms. MEBN theories can be used to define special-purpose logics such logics for planningand decision-making.MEBN places no constraints on the distribution of exemplar constants. Implementations may treat them as con-stants that serve no purpose beyond their role as placeholders in quantifier random variables. They can, however, playan important role in modeling. An exemplar constant is intended as a label for a representative filler of its place ina quantifier random variable. Its distribution can be defined in a way that affects the probability that its associatedsentence is satisfied. This is the role played by the exemplar distributions in the construction of Section 4.2. In thisconstruction, exemplar constants for unsatisfiable sentences always have value ⊥, exemplar constants for valid sen-tences never have value ⊥, and exemplar constants for sentences that are neither valid nor unsatisfiable are assignedthe value ⊥ with a probability strictly between 0 and 1. Assigning the value ⊥ to an exemplar constant constrains thegenerative distribution to ensure that the corresponding quantifier random variable cannot have value T. If a sentenceis satisfiable, the exemplar for its negation has non-zero probability of being assigned value ⊥. When this occurs, thedistribution defined in Section 4.2 will prevent its negation from being satisfied. Thus, the original sentence will havevalue T. Even if the joint distribution encoded by the other domain-specific MFrags would assign zero probabilityto T, the exemplars are sampled in a manner that ensures the sentence has positive chance of being satisfied. Detailsof this construction are provided in Section 4.2.There are two kinds of domain-specific MFrags: generative MFrags and finding MFrags. The distinction betweengenerative MFrags and finding MFrags corresponds roughly to the terminological box, or T-box, and the assertionalreasoner, or A-box [9]. The generative domain-specific MFrags specify information about statistical regularitiescharacterizing the class of situations to which a MEBN theory applies. Findings can be used to specify particularinformation about a specific situation in the class defined by the generative theory. Findings can also be used to repre-sent constraints assumed to hold in the domain (cf., [35,42]) although there are both computational and interpretationadvantages to specifying constraints generatively when possible. The two kinds of domain-specific MFrags are definedbelow.Definition 4. A finding MFrag satisfies the following conditions:4a There is a single resident random variable, Φ(ψ), where ψ is a closed value assignment term. For logical randomvariable instances, we may abbreviate Φ(φ = T) as Φ(φ), and Φ(φ = F) as Φ(¬(φ)).4b There are no context random variable terms. There is a single input random variable term ψ, which is a parent ofthe resident random variable Φ(ψ).4c The local distribution for Φ(ψ) is deterministic, assigning value T if ψ has value T and ⊥ if it has value F or ⊥.154K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Definition 5. A generative domain-specific MFrag F must satisfy the following conditions:5a None of the random variable terms in F is a finding random variable term.5b Each resident random variable term in F is either a random variable term that consists of a random variablesymbol followed by a parenthesized list of one or more ordinary variable symbols, or the random variable symbol(cid:2) followed by a constant symbol enclosed in parentheses. (Implementations may treat (cid:2)(ε) and ε as synonyms.)5c The only possible values for the identity random variable (cid:2)(ε) are ε and ⊥. Furthermore, (cid:2)(T) = T; (cid:2)(F) = F;and (cid:2)(⊥) = ⊥.85d For any resident random variable term ψ other than the identity, the local distribution for ψ must assign probabilityzero to any unique identifier ε for which (cid:2)(ε) (cid:13)= ε. One way to ensure this constraint is met is to make (cid:2)(ε) aparent of ψ for any possible value ε for which there is non-zero probability that (cid:2)(ε) (cid:13)= ε, and to specify a localdistribution that assigns probability zero to ε if (cid:2)(ε) (cid:13)= ε.Requirement 5b may seem restrictive at first. For example, if we want to assert that the temperature sensor ismalfunctioning in the machine most recently inspected by Wilson, we cannot simply define an MFrag with residentrandom variable SensorStatus(MostRecentlyInspectedMachine(Wilson)) and give probability 1 to the value !Mal-functioning. The random variable SensorStatus(MostRecentlyInspectedMachine(Wilson)) is defined in its functioncomposition MFrag, and cannot be overridden by the knowledge engineer. We can specify the desired informationeither by making (m = MostRecentlyInspectedMachine(Wilson)) a parent of SensorStatus(m), or by defining a find-ing random variable Φ(SensorStatus(MostRecentlyInspectedMachine(Wilson))). Which choice is best depends on thecircumstances. If Wilson is a saboteur who broke the sensor in the machine he just inspected, the former choice wouldbe appropriate, because whether Wilson inspected the machine has a causal influence on the generative distributionfor the machine’s sensor status. If there is nothing other than Wilson’s recent report of a broken sensor to distinguishWilson or this machine from any other inspector or machine, then our knowledge is evidential rather than causal, andthe latter choice would be appropriate. In either case, the function composition MFrags represent knowledge aboutthe logical properties of function composition. Similar statements apply to the other built-in MFrags. The restrictionson domain-specific MFrags prevent modelers from violating these logical relationships.In summary, MFrags represent influences among clusters of related random variables. Repeated patterns can berepresented using ordinary variables as placeholders into which entity identifiers can be substituted. Probability infor-mation for an MFrag’s resident random variables are specified via local distributions, which map influence counts fora random variable’s parents to probability distributions over its possible values. When ordinary variables appear in aparent but not in a child, the local distribution specifies how to combine influences from multiple copies of the parentrandom variables. Restricting variable bindings to unique identifiers prevents double counting of repeated instances.Multiple ways of referring to an entity are handled through built-in MFrags that enforce logical constraints on functioncomposition. Built-in logical MFrags give MEBN the expressive power of first-order logic. Context constraints permitrecursive relationships to be specified without circular references.3.3. MEBN theoriesA MEBN theory is a collection of MFrags that satisfies consistency constraints ensuring the existence of a uniquejoint probability distribution over the random variables mentioned in the theory. The built-in MFrags provide logicalcontent and the domain-specific MFrags provide empirical content. This section defines a MEBN theory and states themain existence theorem, that a joint distribution exists for the random variable instances of a MEBN theory. A proofis given in Appendix A.A MEBN theory containing only generative domain-specific MFrags is called a generative MEBN theory. Gen-erative MEBN theories can be used to express domain-specific ontologies that capture statistical regularities in aparticular domain of application. MEBN theories with findings can augment statistical information with particularfacts germane to a given reasoning problem. MEBN uses Bayesian learning to refine domain-specific ontologies toincorporate observed evidence.8 A finite domain can be represented by specifying an ordering ε1, ε2, . . . on the unique identifiers, and specifying a probability of 1 that(cid:2)(εi+1) = ⊥ if (cid:2)(εi ) = ⊥. In this case, the cardinality of the domain is the last i for which (cid:2)(εi ) (cid:13)= ⊥. The cardinality may of course be uncertain.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178155The MFrags of Fig. 2 specify a generative MEBN theory for the equipment diagnosis problem. These MFragsspecify local probability distributions for their resident random variables. The conditioning constraints in each MFragspecify type restrictions (e.g., the symbol m must be replaced by an identifier for an entity of type Machine) andfunctional relationships an influencing configuration must satisfy (e.g., the room identifier r must be equal to thevalue of MachineLocation(m)). Each local distribution provides a rule for calculating the distribution of a residentrandom variable given any instance of the MFrag.Reasoning about a particular task proceeds as follows. First, finding MFrags are added to a generative MEBN the-ory to represent task-specific information. Next, random variables are identified to represent queries of interest. Finally,Bayesian inference is applied to compute a response to the queries. Bayesian inference can also be applied to refinethe local distributions and/or MFrag structures given the task-specific data. For example, to assert that the temperaturelight is blinking in the machine denoted by !Machine37, which is located in the room denoted by !Room103A, we couldadd the findings Φ(TempLight(!Machine37) = !Blinking) and Φ(MachineLocation(Machine37) = !Room103A) tothe generative MEBN theory of Fig. 2. To inquire about the likelihood that there are any overheated engines, the FOLsentence ∃m (Isa(Machine,m) ∧ (EngineStatus(m) = !Overheated)) would be translated into the quantifier randomvariable instance ∃($m, Isa(Machine,$m) ∧ (EngineStatus($m) = !Overheated)). A Bayesian inference algorithmwould be applied to evaluate its posterior probability given the evidence.As with ordinary Bayesian networks, global consistency conditions are required to ensure that the local distrib-utions collectively specify a well-defined probability distribution over interpretations. Specifically, the MFrags mustcombine in such a way that no random variable instance can directly or indirectly influence itself, and initial condi-tions must be specified for recursive definitions. Non-circularity is ensured in ordinary Bayesian networks by defininga partial order on random variables and requiring that a random variable’s parents precede it in the partial ordering.In dynamic Bayesian networks, random variables are indexed by time, an unconditional distribution is specified at thefirst time step, and each subsequent distribution may depend on the values of the random variables at the previoustime step. Non-circularity is ensured by prohibiting links from future to past and by requiring that links within a timestep respect the random variable partial ordering. Other kinds of recursive relationships, such as genetic inheritance,have been discussed in the literature [63]. Recursive Bayesian networks [40] can represent a very general class ofrecursively specified probability distributions for logical random variables on finite domains. MEBN provides a verygeneral ability to express recursive relationships on finite or countably infinite domains.Definition 6. Let T = {F1, F2, . . .} be a set of MFrags. The sequence φd (εd ) → φd−1(εd−1) → · · · → φ0(ε0) is calledan ancestor chain for T in partial world state S if there exist B0, . . . , Bd such that:6a Each Bi is a binding set for one of the MFrags Fi ∈ T ;6b The random variable instance φi(εi) is obtained by applying the bindings in Bi to a resident random variable termφi(θi) of Fi ;6c For i<d, either:• φi+1(εi+1) is obtained by applying the bindings in Bi to an input random variable term• φi+1(θi+1) of Fi , and there is an influencing configuration for φi(εi) and Bi that contains φi+1(θi+1), or• φi+1(εi+1) is obtained by applying the bindings in Bi to a context value assignment term φi+1(θi+1) of Fi .The integer d is called the depth of the ancestor chain. The random variable instance φj (εj ) is an ancestor of φ0(ε0)if there exists an ancestor chain φd (εd ) → . . . → φj (εj ) → · · · → φ0(ε0) for T .Definition 7. Let T = {F1, F2, . . .} be a set of built-in, generative, and/or finding MFrags. Let VT denote the set ofrandom variable terms contained in the Fi , and let NT denote the set of random variable instances that can be formedfrom VT by substituting the unique identifiers of {T, F, ⊥} ∪ E for arguments of the random variables in VT . T is asimple MEBN theory if the following conditions hold:7a No cycles. No random variable instance is an ancestor of itself.99 This condition can be relaxed as long as it can be demonstrated that the local distributions are specified non-circularly.156K.B. Laskey / Artificial Intelligence 172 (2008) 140–1787b Bounded causal depth. For any random variable instance φ(ε) ∈ NT containing the (possibly empty) uniqueidentifier symbols ε, there exists an integer Nφ(ε) such that if φd (εd ) → φd−1(εd−1) → · · · → φ(ε) is an ancestorchain for T in S, then d (cid:3) Nφ(ε). The smallest such Nφ(ε) is called the depth dφ(ε) of φ(ε).7c Unique home MFrags. For each φ(ε) ∈ NT , there exists exactly one MFrag Fφ(ε) ∈ T , called the home MFragof φ(ε), such that φ(ε) is an instance of a resident random variable φ(θ ) of Fφ(ε).107d Recursive specification. T may contain infinitely many domain-specific MFrags, but if so, the MFrag specifi-cations must be recursively enumerable. That is, there must be an algorithm that lists a specification (i.e., analgorithm that generates the input, output, context random variables, fragment graph, and local distributions) foreach MFrag in turn, and eventually lists a specification for each MFrag of T .Condition 7c simplifies the theoretical analysis, but there are many circumstances in which it would be useful torelax it. For example, in an independence of causal influence model, it might be convenient to specify influences due todifferent clusters of related causes to be specified in separate MFrags. In a polymorphic version of MEBN, it might beconvenient to specify local distributions for separate subtypes in separate MFrags [12]. Relaxing condition 7c wouldalso allow a more natural treatment of structural learning. It is clear that the main results of this paper would remainvalid under appropriately weakened conditions. Costa [12] defines a typed version of MEBN that relaxes condition 7c.Theorem 1. Let T = {F1, F2, . . .} be a simple MEBN theory. There exists a joint unique probability distribution P genTon the set of instances of the random variables of its MFrags that is consistent with the local distributions assigned bythe MFrags of T . This distribution respects the independence assumptions encoded in the MFrags. That is, a randomvariable instance is conditionally independent of its non-descendants given its full (possibly infinite) set of parentinstances.The proof of Theorem 1 is found in Appendix A.MEBN inference conditions the joint probability distribution implied by Theorem 1 on the proposition that allfindings have value T. This conditional distribution clearly exists if there is a non-zero probability that all findingshave value T. However, when there is an infinite sequence of findings or there are findings on quantifier randomvariables, then any individual sequence of findings may have probability zero even though some such sequence iscertain to occur. For example, each possible realization of an infinite sequence of rolls of a fair die has zero probability,yet some such sequence will occur if tossing continues indefinitely. Although any individual sequence of tosses hasprobability zero, the assumption that the die is fair allows us to draw conclusions about properties of the sequences oftosses that will actually occur. In particular, it is a practical (although not a logical) certainty that if the die is fair, thenthe limiting frequency of rolling a four will be once in every six trials. That is, although a sequence having limitingprobability 1/6 and a sequence having limiting probability 1/3 both have probability zero, the set of worlds in whichthe limit is 1/6 is infinitely more probable than the set of worlds in which the limit is 1/3. Practical certainties aboutstochastic phenomena are formalized as propositions that are true “almost surely” or “except on a set of measure zero”[6]. Almost sure propositions are not true in all possible interpretations of the FOL theory corresponding to a MEBNtheory, but the set of worlds in which they are true has probability 1 under the probability distribution represented bythe MEBN theory. In the above example, the set of worlds in which the limiting frequency is 1/6 has probability 1.The following results pertain to the existence of conditional distributions in a MEBN theory.Definition 8. The distribution P genis called the generative or prior distribution for T . Let {Φ(ψ1 = α1), Φ(ψ2 =α2), . . .} be the finding MFrags for T . A finding alternative for T is a set {Φ(ψ1 = α(cid:16)2), . . .} of valuesfor the finding random variables of T , possibly assigning different values to the finding random variables from thevalues assigned by T . Finding alternatives represent counterfactual worlds for T —that is, worlds that were a prioripossible but are different from the world asserted by the findings to have occurred.1), Φ(ψ2 = α(cid:16)T10 It may be desirable to relax this condition. For example, in an independence of causal influence model, it might be convenient to specifyinfluences due to different clusters of related causes to be specified in separate MFrags. In a polymorphic version of MEBN logic, it might beconvenient to specify local distributions for separate subtypes in separate MFrags. It is clear that the main results would remain valid underappropriately weakened conditions.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178157Corollary 2. Let T be a MEBN theory with findings {Φ(ψ1 = α1), Φ(ψ2 = α2), . . .}. Then a conditional distrib-ution exists for P gengiven {ψ1, ψ2, . . .}. Furthermore, any two such distributions differ at most on a set of findingalternatives assigned probability zero by P genT . The same holds for a conditional distribution given any finite-lengthsubsequence {ψ1, ψ2, . . . , ψn}.TCorollary 2 follows immediately from Theorem 1 and the Radon–Nikodym Theorem [6]. A distributionPT (ξ1, ξ2, . . . | Φ(ψ1 = α1), Φ(ψ2 = α2), . . .) for a set of random variables {ξ1, ξ2, . . .} obtained by conditioningon all findings having value T is abbreviated PT (ξ | Φ(ψ = α)), and is called a posterior distribution for ξ giventhe findings Φ(ψ = α). Any two posterior probabilities are equal except on a set of finding alternatives assignedprobability zero by P genT .When the sequence of finding random variables is infinite, the probability of any particular realization (ψ = α)may be zero. In this case, there is no single well-defined conditional distribution for ξ given (ψ = α). In fact, theconditional distribution given (ψ = α) can be set arbitrarily to any distribution whatsoever. This would seem to implythat Theorem 2 is vacuous. However, much of statistical theory and practice concerns conditioning on probability zeroevents. In many important problems, one particular conditional distribution is singled out as the most natural one, asthe limiting distribution of an appropriate sequence of distributions conditioned on events with non-zero probability(cf., [21], p. 105). Theorem 3 implies that this is the case for most sequences, in the sense that P genassigns probability1 to sequences for which there is a well-defined limiting distribution.TTheorem 3. Suppose {(ξ1 = γ1), (ξ2 = γ2), . . . , (ξn = γr )}, abbreviated (ξ = γ ), is an assignment of values to afinite set of random variables of T . Then assigns probability 1 to the set of finding alternatives {Φ(ψ1 = α(cid:16)1),Φ(ψ2 = α(cid:16)2), . . .} for which(cid:3)PT(cid:3)(ξ = γ ) | Φ(ψ1 = α(cid:16)1), Φ(ψ2 = αexists and is equal to PT ((ξ = γ ) | Φ(ψ = α(cid:16))).limn→∞(cid:16)2), . . . , Φ(ψn = α(cid:16)n)(cid:4)(cid:4)To prove Theorem 3, consider the sequence X1, X2, . . . of random variables defined by:Xn = PT(cid:3)(ξ = γ ) | ψ1, ψ2, . . . , ψn(cid:4).Let Fn be the σ -field generated by the first n finding random variables ψ1, ψ2, . . . , ψn. The random variablesX1, X2, . . . satisfy the condition that E[Xn+1 | Fn] = Xn. A sequence that satisfies this property with respect to aσ -field is called a martingale with respect to that σ -field. The Xn also satisfy supn E[|Xn|] < ∞. The martingaleconvergence theorem [6] implies that there is a random variable X such that Xn → X with probability 1. Furthermore,Theorem 35.5 of Billingsley [6] implies that P genassigns probability 1 to the event thatTX = PT(cid:3)(cid:4)(ξ = γ ) | ψ1, ψ2, . . ..4. Semantics, representation power, and inferenceSection 4.1 defines model theoretic semantics of MEBN. Section 4.2 demonstrates that multi-entity Bayesiannetworks as formalized in Section 3 can express a probability distribution over interpretations of any classical first-order theory, and constructs a MEBN theory in which every satisfiable sentence has non-zero probability. Section 4.3describes an algorithm for performing inference with MEBN theories.4.1. MEBN semanticsIn the standard model theoretic semantics for first-order logic developed by Tarski ([73]; cf. [23]), a FOL theoryis interpreted in a domain by assigning each constant symbol to an element of the domain, each function symbol onk arguments to a function mapping k-tuples of domain elements to domain elements, and each predicate symbol on karguments to a subset of k-tuples of domain elements corresponding to the entities for which the predicate is true (or,equivalently, to a function mapping k-tuples of domain elements to truth-values). If the axioms are consistent, then158K.B. Laskey / Artificial Intelligence 172 (2008) 140–178there exists a domain and an interpretation such that all the axioms of the theory are true assertions about the domain,given the correspondences defined by the interpretation. Such an interpretation is called a model for the axioms.MEBN theories define probability distributions over interpretations of an associated FOL theory. Each k-argumentrandom variable in a MEBN theory represents a function mapping k-tuples of unique identifiers to possible values ofthe random variable. Any function consistent with the logical constraints of the MEBN theory is allowable, and theprobability that the function takes on given values is specified by the joint probability distribution represented by theMEBN theory. For logical random variables, the possible values of the function are T, F, and ⊥; for phenomenal ran-dom variables, the possible values are entity identifiers and ⊥. Through the correspondence between entity identifiersand entities in the domain, a random variable also represents a function mapping k-tuples of domain entities eitherto domain entities (for phenomenal random variables) or to truth-values of assertions about the domain (for logicalrandom variables).MEBN provides a logically coherent means of specifying a global joint distribution by composing local condi-tional distributions involving small sets of random variables. Formerly, this could be achieved only for restricted kindsof distributions. Standard Bayesian networks allow joint distributions on a finite number of random variables to becomposed from locally defined conditional distributions. There are well-known special cases, such as independentand identically distributed sequences or Markov chains, for which joint distributions on infinite sets of random vari-ables can be composed from locally defined conditional distributions. MEBN provides the ability to construct jointdistributions from local elements for a much wider class of distributions on infinite collections of random variables.As demonstrated below, MEBN can represent a joint distribution over first-order sentences that assigns non-zeroprobability to every satisfiable sentence. Thus, through Bayesian conditioning, a probability distribution can be ex-pressed on interpretations of any consistent, finitely axiomatizable first-order theory. This distribution can be updatedthrough Bayesian conditioning when new axioms are added, providing a theoretical framework for analyzing limitingdistributions over interpretations of infinite sequences of first-order sentences.Consider a MEBN theory TM in a language LM having phenomenal random variable symbols X = {ξi}, phe-nomenal constant symbols A = {αi}, domain-specific logical random variable symbols B = {βi}, exemplar symbolsS = {σφi} and entity identifier symbols E = {εi}. It is assumed that the sets X , A, B, and E are pairwise disjoint, areeither finite or countably infinite, and do not contain the symbols T, F, or ⊥. It is assumed that S contains a distinctexemplar symbol σφi /∈ X ∪ A ∪ B ∪ E ∪ {T, F, ⊥} for each pair consisting of an open logical random variable termφ(γ1, . . . , γn) of LM and index i of an ordinary variable γi occurring in φ(γ1, . . . , γn).The following conditions will be assumed in this section because they make it straightforward to define a corre-spondence between a MEBN theory and a counterpart FOL theory with the same logical content. These conditionsare not requirements of MEBN, and need not be assumed by any given application. Even when they are not sat-isfied, a MEBN theory defines a probability distribution on interpretations of a first-order theory, but defining thecorrespondence is less straightforward.FOL1: There are no quantifier random variable terms among the context terms in any of the MFrags of TM , and nosimple random variable term of TM has a quantifier random variable term as a parent.FOL2: Random variables ξ ∈ X or β ∈ B have value ⊥ if any of their arguments belong to {T, F, ⊥};FOL3: If the values of all arguments to a phenomenal random variable ξ belong to E, then the value of ξ belongs toE with probability 1;FOL4: Any constant symbol α ∈ A has value in E with probability 1;FOL5: If the values of all arguments to a logical random variable β belong to E, then the value of β belongs to {T, F}with probability 1.Given these conditions, P genTMgenerates random interpretations of the phenomenal random variable symbols ofLM in the domain {ε ∈ E: (cid:2)(ε) (cid:13)= ⊥)} of meaningful entity identifiers. That is, for each constant symbol, P genTMgenerates a meaningful entity identifier. For each phenomenal random variable symbol, P gengenerates a randomTMfunction mapping k-tuples of meaningful entity identifiers to meaningful entity identifiers. For each logical randomvariable symbol, P gengenerates a random function mapping k-tuples of meaningful entity identifiers to {T, F} (orTMequivalently, the subset of k-tuples for which the randomly generated function has value T).A classical first-order theory TF that represents the logical content of TM is defined as follows:K.B. Laskey / Artificial Intelligence 172 (2008) 140–1781591. The language LF for TF has function symbols X , constant symbols A ∪ E ∪ {⊥}, and predicate symbols B,where the number of arguments for functions and predicates in LF is the same as the number of arguments forthe corresponding random variables in TM .2. For each pair ε1 and ε2 of distinct entity identifiers, TF contains an axiom (ε1 = ε2) ⇒ (ε1 = ⊥) ∧ (ε2 = ⊥).3. For each phenomenal random variable symbol ξ , TF contains axioms asserting that no instance of ξ may take onvalues outside the set of possible values as defined in the home MFrag for ξ .4. If a local distribution in a domain-specific MFrag of TM assigns probability zero to possible value ε of a phenom-enal resident random variable ξ(x) for some set #SWξ(x) of influence counts, there is an axiom of TF specifyingthat the function corresponding to ξ(x) is not equal to ε when the context constraints hold and the parentsof ξ(x) satisfy #SWξ(x). Each such axiom is universally quantified over any ordinary variables appearing in ξand/or its parents and/or the context random variables in the home MFrag of ξ . Formally, TF contains an axiom∀x((κ(x) ∧ #SWξ(x)) ⇒ ¬(ξ(x) = ε)). Here, κ(x) and #SWξ(x) denote formulae in LF asserting that the contextconstraints hold and that the influence counts for the parents of ξ(x) are equal to #SWξ(x); and x denotes anyordinary variables on which ξ , κ, and/or the parents of ξ depend. This applies also to constant random variables,which are treated as functions with no arguments.5. If a local distribution in a domain-specific MFrag of TM assigns probability one to T for a logical random variableβ(x) for some set #SWβ(x) of influence counts, there is an axiom of TF specifying that the predicate β(x) is trueunder these conditions. That is, TF contains an axiom ∀x((κ(x) ∧ #SWβ(x)) ⇒ β(x)). Here, κ(x) and #SWβ(x)denote formulae in LF asserting that the context constraints hold and that the influence counts for the parents ofβ(x) are equal to #SWβ(x), respectively; and x denotes any ordinary variables on which β, κ, and/or the parentsof β depend.6. If a local distribution in a domain-specific MFrag of TM assigns probability one to F for a logical random variableβ(x) for some set #SWβ(x) of influence counts, there is an axiom of TF specifying that the predicate β(x) is falseunder these conditions. That is, TF contains an axiom ∀x((κ(x) ∧ #SWβ(x)) ⇒ ¬β(x)). Here, κ(x) and #SWβ(x)denote formulae in LF asserting that the context constraints hold and that the influence counts for the parents ofβ(x) are equal to #SWβ(x), respectively; and x denotes any ordinary variables on which β, κ, and/or the parentsof β depend.The logical combination MFrags (see Fig. 8) define random variables that explicitly represent the truth-values ofsentences of TF . The assumptions FOL1–FOL5 ensure that these truth-values satisfy the axioms defining TF . That is,P gengenerates random models of the axioms of TF . However, there may be sentences satisfiable under the axioms ofTMTF to which P genassigns probability zero. When a satisfiable sentence of TF is assigned probability zero by P gen,TMTMthere is no assurance that a well-defined conditional distribution exists given that the corresponding logical randomvariable has value T. The following additional condition ensures that a well-defined conditional distribution existsgiven any finite set of satisfiable findings on random variables of TM . Again, this assumption is not required byMEBN, and may not be appropriate for a given application.Suppose φ(γ1, . . . , γn) is an open logical random variable of TM that depends on ordinary variables γ1, . . . , γn.Let θ1(σφ1, θ2(σφ2, . . . , θn(σφn, φ(σφ1, σφ2, . . . , σφn)))) be a quantifier random variable, where σφi is the exem-plar symbol for ordinary variable γi and θi stands for one of the quantifier symbols ∃ or ∀. Suppose thisquantifier random variable θ1(σφ1, θ2(σφ2, . . . , θn(σφn, φ(σφ1, σφ2, . . . , σφn))) corresponds to a satisfiable for-mula of TF . Then P genassigns strictly positive probability to the value T for the quantifier random variableTMθ1(σφ1, θ2(σφ2, . . . , θn(σφn, φ(σφ1, σφ2, . . . , σφn)))).Corollary 4. Suppose TM satisfies FOL1–FOL6, and suppose that TF is the first-order theory, constructed as above,expressing the logical content of TM . Let {Φ(ψ1 = α1), Φ(ψ2 = α2), . . . , Φ(ψn = αn)} be a finite set of findingssuch that the conjunction of the (ψi = αi) is satisfiable as a sentence of TF . Let {α, ξ(θ ), β(θ )} stand for theset of all instances of constant, phenomenal, and logical random variables of TF . Then the posterior distributionPTM ({α, ξ(θ ), β(θ )} | Φ(ψ = α)) exists and is unique.Corollary 4 is a straightforward consequence of Corollary 2. Specifying a generative distribution that satisfiesFOL1–FOL5 is relatively straightforward. A construction is provided in Section 4.2 of a MEBN theory TM ∗ for whichP genTM∗ satisfies FOL6.160K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Fig. 8. Logical MFrags.A MEBN theory is interpreted in a domain of application by associating each entity identifier symbol with anentity in the domain. Through this correspondence between identifiers and the entities they represent, a MEBN theoryinduces a probability distribution on attributes of and relationships among entities in the domain of application. Inparticular, although the generative distribution for a MEBN theory constructs interpretations in the countable domainof entity identifiers, a MEBN theory can be applied to reason about domains of any cardinality. Under the assumptionthat the entities associated with the entity identifiers constitute a representative sample of entities in the domain,statistical conclusions drawn about the domain are valid for domains of any cardinality.Important advantages of MEBN random variable semantics are clarity and modularity. For example, we couldadd a new collection of MFrags to our equipment diagnosis MEBN theory, say for reasoning about the vacationand holiday schedule of maintenance technicians, without affecting the probabilities of any assertions unrelated to thechange. Furthermore, the probability distribution represented by a MEBN theory is a well-defined mathematical objectindependent of its correspondence with actual objects in the world, having a clearly specified semantics as a probabilitydistribution on interpretations. Its adequacy for reasoning about the actual world rests in how well the relationshipsin the model reflect the empirical relationships among the entities to which the symbols refer in a given domain ofapplication. Our approach thus enforces a distinction between logical and empirical aspects of a representation andprovides a clearly defined interface between the two. This supports a principled approach to empirical evaluation andrefinement of domain ontologies.4.2. A generative distribution for first-order logicThis section demonstrates how to define a generative MEBN theory TM ∗ such that P genTM∗ places positive probabilityon value T for any logical random variable φ that corresponds to a satisfiable sentence in first-order logic.Consider a MEBN language LM ∗ and classical FOL language LF ∗ related to each other as described in Section 4.1.We assume there is a total ordering ϕ1, ϕ2, . . . of the phenomenal constant, phenomenal and logical random variableterms ϕi ∈ A ∪ X ∪ B, and a total ordering ε1, ε2, . . . ∈ E of entity identifiers. The domain-specific MFrags of agenerative MEBN theory must define a distribution for each simple open random variable term ϕi(u1, . . . , uni ), wherethe uj are ordinary variables and ni is the number of arguments taken by ϕi . A distribution is also defined for theexemplar constants. The remaining random variables are defined via the logical MFrags of Fig. 8.The joint distribution for simple open random variables and exemplar constants is defined as follows. Letψ1, ψ2, . . . be a total ordering of the quantifier random variables; let π1, π2, . . . be a strictly positive probabilitydistribution on the entity identifiers, and let 0 < θ, ρ < 1 be real numbers. We use the notation ψk to refer a quantifierrandom variable and σψk to refer to the exemplar constant for ψk. That is, ψk denotes a logical random variable ofthe form ∀(σψk , φ(σψk )) or ∃(σψk , φ(σψk )), where φ(u) is an open logical random variable called the body of ψk.We can think of the exemplar constant σψk as denoting a generic filler entity for its place in the quantifier randomK.B. Laskey / Artificial Intelligence 172 (2008) 140–178161variable. Its generative distribution is defined in a way that ensures that if the negation of its quantifier random variableis satisfiable, then σψk has a non-zero probability of having the value ⊥, which constrains the corresponding quantifierrandom variable to have value F.Exemplar constant distributions: The distributions for exemplar constants are defined inductively such that theexemplar term (cid:2)(σψk ) has value ⊥ in models in which ψk is constrained to have value F, and otherwise is sampledrandomly from the entity identifiers that are logically possible values for σψk . Specifically:• The parents of (cid:2)(σψk ) are (cid:2)(σψ1 ), (cid:2)(σψ2 ), . . . , and (cid:2)(σψk−1 ).• By the inductive hypothesis, it is assumed that if (cid:2)(σψi ) = ⊥ for i < k, then ψk has value F. (It will be verifiedbelow that the inductive hypothesis is true for k, then it is true for k + 1.) Conditional on (cid:2)(σψ1), (cid:2)(σψ2 ), . . . ,and (cid:2)(σψk−1 ), the distribution of (cid:2)(σψk ) is defined as follows:◦ If ψk is unsatisfiable as a formula of LF ∗ given the constraints on ψ1, . . . , ψk−1 implied by the values of itsparents, then (cid:2)(σψk ) has value ⊥ with probability 1.◦ If ¬ψk is unsatisfiable as a formula of LF ∗ given the constraints on ψ1, . . . , ψk−1 implied by the values of itsparents, then (cid:2)(σψi ) has value εj with probability πj .◦ Otherwise, (cid:2)(σψi ) has value ⊥ with probability θ and εj with probability (1 − θ )πj .Calculating the above probabilities requires checking for satisfiability of ψk and ¬ψk, which is in general undecid-able. We can define a process that satisfies Definition 3 as follows. First, we assign probability θ to ⊥ and (1 − θ )πjto εj . Then we execute the satisfiability checker. If at any point ψk is proven unsatisfiable, we change the distributionto assign probability 1 to ⊥. If ¬ψk is proven unsatisfiable, we assign probability zero to ⊥ and πj to εj . If either ψkor ¬ψk is unsatisfiable, this algorithm will eventually halt with the correct result. Otherwise, it was initialized withthe correct distribution and this distribution never changes, so if the algorithm is interrupted it will give the correctresult.Domain-specific random variable distributions: The distribution of ϕk(u1, . . . , unk ) is defined as follows.• The parents of ϕk(u1, . . . , unk ) are:◦ ϕi(v1, . . . , vni ) for all i < k, where vj is a different ordinary variable than uj , implying that all instances ofϕi(v1, . . . , vni ) are parents of each instance of ϕk(u1, . . . , unk );◦ Instances of ϕk(v1, . . . , vnk ) such that the entity identifier bound to each uj is equal to or precedes the entityidentifier bound to vj , and strictly precedes it for at least one j (this can be specified by a recursive definitionwith appropriate context constraints);◦ The exemplar constants (cid:2)(σψ1 ), (cid:2)(σψ2 ), . . . ;◦ The identity random variables (cid:2)(e).• If ϕk(u1, . . . , unk ) is a phenomenal random variable, its probability distribution is calculated as follows. For anybinding ε1, . . . , εnk of entity identifiers to the variables u1, . . . , unk , the value ϕk(ε1, . . . , εnk ) = εj is assignedrandomly, with probability πj , from among the entity identifiers whose value is consistent with the satisfiabilityconstraints implied by the assignment of values to the parents of ϕk(ε1, . . . , εnk ). Again, this step requires sat-isfiability checks. Definition 3 is satisfied if it is implemented by initially assigning probability πj to εj , and ifϕk(ε1, . . . , εnk ) = εj is proven unsatisfiable, setting the probability of εj to zero. The probability assigned to ⊥converges to the correct value, but may never stop changing. This is allowed by Definition 3.• If ϕk(u1, . . . , unk ) is a logical random variable, its probability distribution is calculated as follows. For any bindingε1, . . . , εnk of entity identifiers to the variables u1, . . . , unk :◦ ϕk(ε1, . . . , εnk ) has value T if ¬ϕk(ε1, . . . , εnk ) is inconsistent with the satisfiability constraints implied by theassignment of values to the parents of ϕk(ε1, . . . , εnk );◦ ϕk(ε1, . . . , εnk ) has value F if ϕk(ε1, . . . , εnk ) is inconsistent with the satisfiability constraints implied by theassignment of values to the parents of ϕk(ε1, . . . , εnk );◦ Otherwise, ϕk(u1, . . . , unk ) has value T with probability ρ and F with probability (1 − ρ).As before, this calculation is implemented by initially assigning probability ρ to T and probability (1 − ρ) to F,and revising the distribution if one of the satisfiability checks fails.162K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Theorem 5. If ψ is a closed logical random variable corresponding to a satisfiable sentence of LF ∗ , then P gennon-zero probability on the value T for ψ.TM∗ placesProof. The above definition ensures that if ψ corresponds to a satisfiable sentence of TF ∗ , then there is a non-zeroprobability that (cid:2)(σ¬ψ ) has value ⊥. When (cid:2)(σ¬ψ ) has value ⊥, the local distributions for the domain-specificrandom variables are assigned in a way that constrains ψ to have value T. Therefore, there is a non-zero probabilitythat ψ has value T. (cid:3)Theorem 5 shows that P genTM∗ places non-zero probability on the value T for sentences of LF ∗ that are consistentwith the axioms of TF ∗ , which is a first-order theory obtained from P genTM∗ by following the rules of Section 4.1. Thefinal step in our argument is to show how to use Theorem 5 to define a probability distribution that places non-zeroprobability on the models of any satisfiable sentence in first-order logic.Let L be a first-order language with function symbols X , constant symbols A, and predicate symbols B, and letψ be a sentence of L. The correspondences defined in the logical MFrags of Fig. 8 provide a recipe for defininga language LM ∗ that augments L with the special logical constants and random variables common to all MEBNtheories. Following the above definitions, we can define a MEBN theory TM ∗ that has the same domain-specificrandom variable symbols as L. This MEBN theory has a logical random variable ψM ∗ that makes the same assertionas ψ . Section 4.1 defines a corresponding sentence ψF ∗ of TF ∗ . By examining how ψM ∗ is constructed from ψ andhow TF ∗ is constructed from TM ∗ , it is clear that ψF ∗ is satisfiable as a sentence of LF ∗ if and only if ψ is satisfiable asa sentence of L. Thus, given a first-order language with countably many symbols, we can define a MEBN theory thatrepresents a joint probability distribution over logical random variables corresponding to sentences of L. This MEBNtheory assigns non-zero probability to the value T for any logical random variable corresponding to a satisfiablesentence ψ of L. Conditioning on the value T for this random variable results in a joint distribution on models of ψ.Furthermore, the same holds for any finite set of jointly satisfiable sentences, because their conjunction is a satisfiablesentence. It is also clear that this approach fails for infinite sequences of sentences.4.3. Comments on the FOL generative distributionSome comments are in order to clarify what has been accomplished in defining the probability distribution ofSection 4.2. One need not go to such trouble merely to demonstrate the existence of a distribution on interpretationsthat assigns a positive probability to each sentence. This can be accomplished simply by: (1) listing the satisfiablesentences of the language; (2) invoking the axiom of choice to pick an interpretation to satisfy each sentence in the list;and (3) assigning a probability to each of the chosen interpretations. Of course, this is purely a formal demonstrationof existence, and it is not clear whether the resulting distribution has any meaning with respect to the intended domainof application.The distribution defined in Section 4.2 generates possible worlds by assigning random values to functions andpredicates. Because the sampled worlds are required to respect satisfiability constraints that may be undecidable,the generative probabilities are not Turing computable. However, they are computable by an oracle machine with asatisfiability-checking oracle. Oracle machines are often used to prove properties of algorithms. Although the distrib-ution of Section 4.2 cannot be computed exactly, it can be approximated.Furthermore, it is not difficult to modify the process of Section 4.2 to obtain a generative distribution that is nearlythe same as a distribution specified by a domain modeler, but with a “pinch of probability” [20] allocated to eachsatisfiable sentence. This is accomplished by making two changes to the process defined in Section 4.2. First, theexemplar sampling distribution is modified to make probability of imposing an unsatisfiability constraint very small.Second, the domain-specific random variable distributions are sampled according to the domain theory when there areno applicable unsatisfiability constraints, and to localize the changes when there are constraints.The process of Section 4.2 imposes unsatisfiability constraints by assigning the value ⊥ to exemplar constants.When the sampler is about to generate the kth exemplar, it first invokes the oracle to check whether either ψk or¬ψk is unsatisfiable given previously generated constraints. If ψk is unsatisfiable, the value ⊥ is sampled; if ¬ψkis unsatisfiable, a non-⊥ value is sampled. If both ψk and ¬ψk are satisfiable, a random choice is made of whetherto impose an unsatisfiability constraint (assign the value ⊥) or not to impose a constraint (assign a non-⊥ value).At this point, another modification is made to the sampling distribution as follows. An integer k (cid:2) 0 is chosen withK.B. Laskey / Artificial Intelligence 172 (2008) 140–178163probability (1 − θ )kθ . If k (cid:2) 1 and ψk is satisfiable, the sampler sets σψ to the value ⊥, which will constrain samplingto prevent ψk from being satisfied. Otherwise, set σψ is set to the value ⊥ if and only if ψk is unsatisfiable given theconstraints imposed thus far, if any. This procedure imposes at most one unsatisfiability constraint, and the probabilityof imposing a constraint is less than θ .Next, the sampling distributions for the domain-specific random variables are modified to make the perturbeddistribution as close as possible to the original MEBN theory. To do this, sampling of the domain-specific randomvariables is carried out in an order consistent with the fragment graphs of the original MEBN theory. When samplingrandom variable ϕ, if we have not imposed a constraint in our exemplar sampling, or if ϕ is not an ancestor of aquantifier random variable ψ for which an unsatisfiability constraint was imposed, then ϕ is sampled according toits local distribution in the original MEBN theory. Otherwise, a distribution is chosen that prevents ψ from beingsatisfied. To do this, we may need to add new arcs to ϕ from random variables that are not its parents in the originalMEBN theory; but to minimize changes to the original distribution, we do not add parents unless they are ancestorsof ψ.The result of this oracle machine construction is a generative distribution that is close to the original distribution,and approaches it in the limit as θ tends to zero. The unsatisfiability constraints are imposed as local perturbationsthat disturb the original MEBN theory as little as possible. Specifically, when intervening to make ψ unsatisfiable,we modify only the distributions of ancestors of ψ. The generative distributions of all other random variables remainunchanged.There are a few subtle issues that deserve mention. One issue is accounting for the possibility that the originalMEBN theory itself specifies unsatisfiability constraints. This is possible, but details are not presented here. Second,although the probabilities assigned by this “pinch of probability” distribution can be made as close as we like to theoriginal distribution, there remains an arbitrariness to conditional probabilities given sentences assigned probabilityzero by the original MEBN theory. The arbitrariness is localized to the generative distributions of ancestors of themeasure zero sentence. This contrasts with standard probability theory, in which conditional distribution given a setof measure zero is totally arbitrary. Basing a probabilistic logic on graphical models allows us to localize the effectsof conditioning on an event of measure zero. If the original MEBN theory imposes additional mathematical structure,such as a continuous probability density, then there is a natural choice for the conditional distribution. Further analysisof this topic is beyond the scope of this paper.4.4. MEBN inference: Situation-specific Bayesian networksAs noted above, MEBN inference conditions the prior distribution represented by a MEBN theory on its findings.Fig. 9 sketches an inference algorithm that uses knowledge-based model construction [74] to produce a sequenceof approximate situation-specific Bayesian networks. Mahoney and Laskey [54] define a situation-specific Bayesiannetwork (SSBN) as a minimal Bayesian network sufficient to compute the response to a query, where a query consistsof obtaining the posterior distribution for a set of target random variable instances given a set of finding randomvariable instances. This algorithm is a version of the simple bottom-up construction algorithm given in [54], adaptedto the case in which the true SSBN may be infinite. The algorithm begins with a query set consisting of a finiteset of target random variable instances and a finite set of finding random variable instances. These are combined toconstruct an approximate SSBN. The approximate SSBN has an arc between a pair of random variables when onevariable is a parent of the other or a context variable in its home MFrag, and there is an influencing configurationfor the child variable. At each step, the algorithm obtains a new approximate SSBN by adding findings, instantiatingthe home MFrags of the random variables in the query set and their ancestors, adding the resulting random variableinstances to the query set, removing any that are not relevant to the query, and combining the resulting set of randomvariable instances into a new approximate SSBN. This process continues until either there are no changes to theapproximate SSBN, or a stopping criterion is met. If the algorithm is run without a stopping criterion, then if SSBNconstruction terminates, the resulting SSBN provides an exact response to the query or an indication that the findingsare inconsistent. When the algorithm does not terminate, it defines an anytime process that yields a sequence ofapproximate SSBNs converging to the correct query response if one exists. In general, there may be no finite-lengthproof that a set of findings is consistent, but inconsistent findings can be detected in a finite number of steps of SSBNconstruction.164K.B. Laskey / Artificial Intelligence 172 (2008) 140–1781. Initialization: Set the query set Q to the union of the target nodes and the finding nodes. Initialize the RV instances R0 = Q. Set themaximum number of states per random variable N0 equal to a finite integer. Set i = 0.2. SSBN Structure Construction. Set the current SSBN Bi to contain the nodes in Ri and all arcs corresponding to influencing config-urations. Remove from Bi any barren nodes, nodes d-separated from target nodes by finding nodes, and nuisance nodes for whichmarginal distributions do not need to be updated.3. Local Distribution Construction. Set the local distributions in Bi , modifying the local distributions to restrict random variables to nomore than Ni possible values and, to approximate the effect of random variables that have not been enumerated, and compute for nomore than Ki steps.4. Inference. Apply standard Bayesian network inference to compute conditional distributions for the target random variables given thefinding random variables. If findings have probability zero, report that the findings are inconsistent.5. Instance Enumeration and Approximation Parameter Updating. If a stopping criterion is met, output Bi . Else add to Ri additionalparents of random variables for which adding additional parents might change the distribution, increase Ni and Ki and return tostep 2.Fig. 9. SSBN construction algorithm sketch (see Appendix A for details).Fig. 10. Situation-specific Bayesian networks. (a) Two machines in the same room. (b) Two machines that might or might not be in the same room.(a)(b)Fig. 10 shows two SSBNs constructed from the MEBN theory of Fig. 2 for a query on the engine status of twomachines, the first for the case in which the two machines are known to be in the same room, and the second for thecase in which the two machines may be in different rooms. In the first case, learning that the engine in one machineis overheated results in an increase in the probability that the other engine is overheated; in the second case, the sameinformation has almost no effect on the probability distribution for the other machine (there is a small impact becauseof the influence of the evidence on beliefs about the maintenance practices of the owner).When there is uncertainty in the value of a context random variable, it appears explicitly in the SSBN. For example,MachineLocation(m) appears as a node in Fig. 10(b), with possible values !R1 and !R2, the rooms in which !M1 and!M2 might be located. Context random variables may be de facto parents of the other resident random variables in theirMFrag: in this case, EngineStatus(m), SensorStatus(m), and TempLight(m). The distributions for EngineStatus(m) andSensorStatus(m) are multiplexor distributions. That is, the distribution of the child depends on the temperature of theroom in which the machine is actually located and not on the temperatures of the other rooms in which it mighthave been located. The distribution for EngineStatus(m) depends on MachineLocation(m) only through whether ornot its value is ⊥. Because it is known to have non-⊥ value in this situation, no arc is needed in the SSBN fromMachineLocation(m) to EngineStatus(m). When there may be uncertainty in the context random variables, omittingthe implicit arcs from context random variables to resident random variables in the MFrag drawings hides some ofthe complexity in a MEBN theory. Nevertheless, context random variables enable economical representations thatfacilitate knowledge engineering. Furthermore, inference algorithms can exploit context-specific independence toachieve computational efficiency.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178165As noted above, when an ordinary variable appears in a parent but not in its child, the random variable can havean unbounded number of parent instances in the constructed approximate SSBN. Each step of SSBN constructioninstantiates finitely many parents of any random variable. When there are infinitely many computationally relevantparent instances, additional instances are added at each step until a termination condition is reached. Even when afinite-size SSBN exists, constructing it and computing a query response is often intractable. It is typically necessaryto approximate the SSBN by pruning arcs and random variables that have little influence on a query, and/or compilingparts of the SSBN to send to inference engines optimized for special problem types. The process of controlling theaddition and pruning of random variable instances and arcs is called hypothesis management. More generally, execu-tion management controls the inference process to balance accuracy against computational resources. Often, portionsof an inference task can be solved exactly or approximately using efficient special-purpose reasoners. Such reasonersinclude constraint satisfaction systems, deductive theorem provers, differential equation solvers, heuristic search andoptimization algorithms, Markov chain Monte Carlo algorithms, particle filters, etc. Online reasoning systems mayinterleave addition of new findings, refinement of the current approximate SSBN, computation of query responsesgiven the current approximate SSBN, and learning. Laskey, et al. [47,49] treat hypothesis management as a problemof balancing the computational overhead of representing additional random variable instances against accuracy inresponding to queries. Charniak and Goldman [11] and Levitt et al. [7,51] also consider hypothesis management inopen-world computational probabilistic reasoning systems. Hypothesis management is discussed extensively in theliterature on tracking and multi-source fusion (e.g., [72]).5. Probabilistic logics and languagesThere is a growing literature on languages for representing probabilistic knowledge, the semantics of probabilisticrepresentations, and well-foundedness, tractability and decidability of inference in probabilistic theories. The successof graphical models for parsimonious representation and tractable inference has generated strong interest in moreexpressive languages for reasoning with probability. Work in knowledge-based model construction (e.g., [74]) focusedon constructing Bayesian networks from knowledge bases consisting of modular elements representing knowledgeabout small clusters of variables. Early KBMC systems were not built on decision theoretically coherent declarativedomain theories, and relied on heuristic knowledge, typically encoded as procedural rules, for constructing complexmodels from simpler components. As work in knowledge-based model construction progressed, interest grew in thetheoretical foundations of probabilistic representation languages, and in their relationship to classical first-order logic.A number of authors have investigated approaches to integrating classical logic with probability. A common approachhas been to provide language constructs that allow one to express first-order theories not just about objects in a domainof discourse, but also about proportions and/or degrees of belief for statements about these objects. Bacchus et al. [2,3]augment first-order logic with proportion expressions that represent the knowledge that a given proportion of objectsin a domain have a certain property. A principle of indifference is applied to assign degrees of belief to interpretationssatisfying the constraints imposed by ordinary first-order quantification and the proportion expressions. Halpern’s [34]logic can express both proportion expressions and degrees of belief, and provides a semantics relating proportions todegrees of belief. Neither of these logical systems provides a natural way to express theories in terms of modularand composable elements. Unlike Bayesian networks, which have easy to verify conditions ensuring the existence ofa coherent domain theory, it is in general quite difficult in these logical systems to specify complete and consistentprobabilistic domain theories, or to verify that a set of axioms is coherent.Several languages have been developed that represent probabilistic knowledge as modular units with repeated sub-structures that can be composed into complex domain models. These include pattern theory [32], hidden Markovmodels [22], the plates language implemented in BUGS [10,30,71], object-oriented Bayesian networks [5,44,45], andprobabilistic relational models [27,28,63]. There is a great deal of commonality among languages for compactly ex-pressing complex probabilistic domain theories (cf., [35]). Plates in BUGS, object classes in object-oriented Bayesiannetworks, and PRM structures in probabilistic relational models all correspond to MFrag classes.Fig. 11 compares MEBN, PRM and plate representations for a theory fragment in the equipment diagnosis domain.Like Bayesian networks, plates represent a joint distribution as an acyclic directed graph in which nodes representrandom variables, arcs represent direct dependence relationships, and each node is annotated with a specification of aconditional distribution of the random variable given its parents. Repeated structure in a plates model is representedby indexing repeated random variables with subscripts, and enclosing the set of random variables indexed by a given166K.B. Laskey / Artificial Intelligence 172 (2008) 140–178(a)(b)(c)Fig. 11. MFrags, PRM and plates for equipment diagnosis domain. (a) MEBN fragments (findings are not shown). (b) Probabilistic relationalmodel—relational schema & PRM structure (skeleton and instances are not shown). (c) Plates.subscript in a rectangle called a “plate.” These indices play the role of the ordinary variables in an MFrag. As inMEBN, a random variable’s parents may contain indices not mentioned in the random variable, in which case thelocal distribution for the child random variable must specify how to aggregate influences from multiple instancesof the parent random variable. Plate models are restricted to a finite number of instances of each random variable.The number of instances of each random variable is a fixed attribute of the plate model. BUGS has sophisticatedK.B. Laskey / Artificial Intelligence 172 (2008) 140–178167capability for parameter learning, and although there is no built-in mechanism for structure learning, plate modelscan be constructed to represent the problem of reasoning about the presence or absence of conditional dependencyrelationships between random variables.A PRM contains the following elements ([35]; see Fig. 11(b)):• A relational schema that specifies the types of objects and relationships that can exist in the domain;• A PRM structure that represents probabilistic dependencies and numerical probability information;• A skeleton that specifies a unique identifier and a blank template for each individual entity instance;• The data to fill the entries in the blank template.Like a MEBN theory, a PRM represents a probability distribution over possible worlds. Any given PRM can beexpanded into a finite Bayesian network over attributes of and relationships between the individuals explicitly repre-sented in the skeleton. PRMs use aggregation rules to combine influences when multiple instances of a parent randomvariable influence a child random variable (as when multiple reports influence the WatchStatus random variable inFig. 11). In addition to attribute value uncertainty, PRMs have been extended to handle type uncertainty, referenceuncertainty, and identity uncertainty. PRM learning theory provides a formal basis for both parameter and structurelearning. Learning methods have been published (e.g., [27]) for learning both the structure and parameters of PRMsfrom instances in the skeleton. If the probability distribution represented by a PRM is assumed to apply to similarentities not explicitly represented in the skeleton, then PRM learning methods can be extended to allow sequentiallearning as new individuals are added to the skeleton over time, thus providing the logical basis for a form of open-world reasoning. One can also extend the relational schema and PRM structure “by hand” to add new entity types.Heckerman et al. [35] introduce a new language, DAPER, for expressing probabilistic knowledge about struc-tured entities and their relationships. DAPER combines the entity-relation model from database theory with directedgraphical models for expressing probabilistic relationships. DAPER is capable of expressing both PRMs and plates,thus providing a unified syntax and semantics for expressing probabilistic knowledge about structured entities andtheir relationships. As presented in Heckerman et al. [35], DAPER expresses probabilistic models over finite data-bases, and cannot express arbitrary first-order formulas involving quantifiers. That is, DAPER is a macro languagefor compactly expressing finite Bayesian networks with repeated structure, and not a true first-order probabilisticlogic. Because DAPER can represent PRMs and plates, this conclusion applies to these formalisms as well. On theother hand, the random variable semantics described in Section 4.1 could provide a theoretical basis for extendingDAPER, and thus PRMs and plates, into a true first-order logic. Conditions could be identified under which DAPERmodels of unbounded cardinality express well-defined probability distributions over models. If developed more fully,the relationship sketched here between MEBN theories, PRMs and plates would facilitate construction of such anextension.Object-oriented Bayesian networks represent entities as instances of object classes with class-specific attributesand probability distributions. Reference attributes allow representation of function composition. Although OOBNs donot have multi-place relations, these can be handled by defining new object types to represent multi-place relations.Structure and parameter learning methods for OOBNs have been developed (e.g., [4,45]). The current literature onOOBNs does not treat type and reference uncertainty, although clearly it would be possible to extend OOBNs tohandle these kinds of uncertainty. An advantage of OOBNs is the ability to represent encapsulated information, orrandom variables defined internally to an object that are independent of external random variables given the interfacerandom variables that shield an object from its environment. The semantics of encapsulation is based on conditionalindependence relationships. Thus, the concept of encapsulation could be extended to other languages based on graph-ical models, including MEBN theories and DAPER models with encapsulated random variables. As with plates andPRMs, the semantics described in Section 4.1 could provide a theoretical basis for extending OOBNs to achieve fullfirst-order expressive power.A feature of MEBN not present in PRMs, plates or OOBNs is the use of context constraints to specify logicalconditions that determine whether one random variable influences another. A similar effect can be achieved by usingaggregation functions that ignore influences ruled out by the context, but this is more cumbersome. PRMs and OOBNsare founded on a type system. Sophisticated implementations have subtyping, inheritance, and the ability to representtype uncertainty (e.g., [37]). MEBN can be extended to a typed logic that has many of the advantages of typedrelational languages [12].168K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Like MEBN, relational Bayesian networks [39,40] provide formal semantics for probability languages that extendBayesian networks to achieve first-order expressiveness. Random variables in a relational Bayesian network are alllogical. A RBN has a set of pre-defined relations used in defining the local distributions and a set of probabilisticrelational symbols, which represent uncertain relations on the domain. A RBN defines a joint probability distributionon models of the uncertain relations. Probability formulas specify how to combine influences from multiple instancesof the parents of a random variable to obtain a conditional distribution for the random variable given finite setsof instances of its parents. General relational Bayesian networks can represent probability distributions only overfinite domains, although non-recursive RBNs have been extended to represent probability distributions over countablyinfinite domains [39].Bayesian logic programs (e.g., [19,43]) also express uncertainty over interpretations of first-order theories. To en-sure decidability, BLPs have typically been restricted to Horn clause theories. Bayesian logic programs and MEBNtheories represent complementary approaches to specifying first-order probabilistic theories. BLPs represent frag-ments of Bayesian networks in first-order logic; MEBN theories represent first-order logic sentences as MFrags.Although the restriction to Horn clause logic limits the expressiveness of BLP languages, this limitation is balancedby the efficiency of algorithms specialized to Horn clause theories. Research in Bayesian logic programming is ap-plicable to the problem of execution management in SSBN construction. That is, an execution manager can identifyportions of an inference task that involve only Horn clauses, and send these to an inference engine specialized forefficient reasoning with Horn clauses. MEBN semantics could be used to develop extensions to BLP languages thatcould handle knowledge bases not limited to Horn clauses.Other research on integrating logic and probability includes Poole’s [65] parameterized Bayesian networks, Ngoand Haddawy’s [60] work on context-specific probabilistic knowledge bases, PRISM [68], IBAL [64], and BLOG[56]. Parameterized Bayesian networks are designed to provide the ability to reason about individuals not explicitlynamed, an important capability lacking in most probabilistic languages. Poole presents an algorithm for performinginference without grounding the theory. Like MEBN, random variables in a parameterized Bayesian network can takearguments; individuals in a population can be substituted for the parameters to form instances of the random variables.Like MEBN, the population over which the parameters range can be finite or infinite. Poole considers only modelswithout recursion. Thus, a parameterized Bayesian network corresponds to a MEBN theory with no recursive links.For such theories, Poole’s inference algorithm would provide an alternative, possibly more efficient, to the SSBNalgorithm presented here. Ngo and Haddawy represent probabilistic knowledge as universally quantified sentencesthat depend on context. Like MEBN, Ngo and Haddawy exploit context constraints to focus inference on relevantportions of the knowledge base. Unlike MEBN, Ngo and Haddawy separate context, which is non-probabilistic, fromuncertain hypotheses, for which context-specific probability distributions are defined. A context-sensitive knowledgebase corresponds to a partially specified MEBN theory in which there is a reserved subset of logical random variablesthat may appear as context random variables in MFrags, but that have no home MFrags and whose truth-values areassumed to be known at problem solving time. PRISM is a logic programming language in which facts can have pa-rameterized probability distributions. Like a MEBN theory, a PRISM program defines a probability distribution overinterpretations. A PRISM program can be used as a random sampler from the distribution it defines. PRISM also sup-ports abductive reasoning and EM learning. IBAL is a probabilistic programming language that allows users to writefunctional programs with stochastic branches. Given such a program, IBAL uses a variety of inference methods toprovide a probability distribution over outputs of the program. Results may be conditioned on user-specified evidence.IBAL supports parameter learning and utility maximization. BLOG [56] is a new language that enables probabilisticreasoning about unknown entities, and about domains that can contain unknown numbers of entities. Under appropri-ate conditions such as the ones defined in 3.3, BLOG could also express probability distributions over interpretationsof a broad class of first-order theories.Hidden Markov models, dynamic Bayesian networks and partially dynamic Bayesian networks (Bayesian net-works containing both static and dynamic nodes) provide the ability to define simple recursive relationships. Pfeffer[63] also considers recursive probabilistic models, which can express a richer class of recursive relationships. It isstraightforward to express HMMs, DBNs, and recursive probabilistic models as MEBN theories (e.g., Fig. 3).Pattern theory [32] is a graphical modeling language based on undirected graphs. There is an extensive literatureon applications of undirected graphical models to image understanding, geospatial data, and other problems in whichthere is no natural direction of influence. A hybrid language could be defined that extends MEBN to permit bothdirected and undirected arcs. Such an extension is not considered here.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178169A common problem for first-order graphical probabilistic languages is how to specify local distributions when arandom variable has different numbers of parents in different ground Bayesian networks corresponding to a givenfirst-order probabilistic theory. Probabilistic relational models use aggregation functions, in which a summary statisticis computed from the instances of one of the parents, and the local distribution depends on the summary statistic. Forexample, the distribution for WatchStatus in Fig. 11 depends on a summary statistic that aggregates the total numberof problematic reports received about an item. Many knowledge-based Bayesian network construction approaches usecombination rules (e.g., [58,60]. With combination rules, the modeler defines a probability distribution for a singleinstance of each of the parents of a random variable, and a combination rule that specifies how to combine thesedistributions when the ground model contains multiple instances of some or all of the parents. Influence counts canrepresent both combining rules and aggregation functions.To show how influence counts can represent combining rules, consider an extension of our diagnosis example inwhich EngineStatus(m) depends on BeltStatus(b) and GasketStatus(g), and in which the context constraints specifyIsa(Belt, b) and Isa(Gasket, g). To specify a combining rule, the modeler would specify a probability distribution forEngineStatus(m) given each belt/gasket configuration and each room temperature, and define a function to combinethese distributions. Suppose a particular machine has two belts and three gaskets, and is located in one of two rooms.Making all legal substitutions would yield twelve probability distributions: one for each of the six belt/gasket com-binations in each of the two rooms. The combining function would specify how to obtain a single distribution fromthese twelve distributions. To use influence counts to define a combining rule, we would simply specify a probabilitydistribution for each parent configuration, and then use each configuration’s influence count to specify the number ofcopies of the corresponding distribution to combine. To represent aggregation rules with influence counts is a little lessstraightforward. Suppose we want to define an aggregation function that depends on the total number of broken beltsand the total number of broken gaskets. In our machine with two belts and three gaskets, each belt contributes to threeof the six influencing configurations, and each gasket contributes to two of the six influencing combinations. Thus,we would need to divide the total influence counts for broken belt configurations by 3 and the total influence countsfor broken gaskets by two, in order to obtain the needed aggregation function. This simple rule breaks down whenthere are context constraints that rule out some of the combinations. For example, one of the belts might be allowedto combine with only one of the gaskets. This case could be handled by making the entity identifier (cid:2)(e) a parent andcounting the number of unique belt instances and unique gasket instances among the influencing configurations.One concern that may arise with representing combining rules and aggregates is whether conditions 3c and 3eare satisfied. For example, the average of infinitely many terms does not in general satisfy 3c. In fact, a conditionlike 3c is required precisely because in its absence, an average of infinitely many terms may not have a well-defineddistribution. Most other formalism avoid this difficulty by assuming the domain is finite. In BLOG, the cardinalitymay be unbounded, but it is assumed to be finite. In finite domains, conditions 3c and 3e are always satisfied. Thisis because in each interpretation, all but finitely many entity identifiers have value ⊥. Therefore, there are finitelymany influencing configurations for any random variable. The number of influencing configurations may vary frominterpretation to interpretation, but this is allowable under condition 3c. Thus, all random variables, even aggregatesand combined influences, have well-defined distributions, because conditions 3c and 3e are satisfied. Indeed, theseconditions are satisfied in infinite domains under the assumption that in any interpretation, each random variable isactually influenced by only a finite (possibly unbounded) number of its parents. This level of expressiveness is morethan sufficient to represent any problem a knowledge engineer is likely to encounter. These conditions impose realrestrictions only with the attempt to represent completed infinities (e.g., the average of an infinite number of non-zeronumbers). While appropriate relaxations of these conditions are of theoretical interest, identifying conditions to ensurethat MEBN theories can represent completed infinities is not a pressing practical issue.6. Summary and discussionGraphical models were initially limited to problems in which the relevant random variables and relationships couldbe specified in advance. Languages based on graphical models are rapidly reaching the expressive power requiredfor general computing applications. It is becoming possible to base computational inference and learning systems onrationally coherent domain models implicitly encoded as sets of graphical model fragments, and to use such coherentdeep structure models to guide reasoning and knowledge discovery. Probability theory provides a logically coherent170K.B. Laskey / Artificial Intelligence 172 (2008) 140–178calculus for combining prior knowledge with data to evolve an agent’s knowledge as observations accrue. Probabilitytheory also provides a principled approach to knowledge interchange among different reasoners.This paper presents a first-order Bayesian language called Multi-Entity Bayesian Networks (MEBN). The syntacticsimilarity of MEBN to standard first-order logic notation clarifies the relationship between first-order logic and prob-abilistic logic. A MEBN theory (MEBN theory) assigns probabilities to models of an associated FOL theory. MEBNtheories partition FOL theories into equivalence classes of theories with the same logical content but different proba-bilities assigned to models. Provable statements in FOL correspond to statements in the associated MEBN theory forwhich SSBN construction terminates with a probability of 1 assigned to the value T. A MEBN theory correspondingto an inconsistent FOL theory has at least one finding equal to ⊥ with probability 1. If the associated MEBN theoryis inconsistent, SSBN can determine in finitely many steps that it is inconsistent. When SSBN construction does notterminate but the MEBN theory represents a globally consistent joint distribution, the construction process gives riseto an anytime sequence of approximations that converges in the infinite limit to the correct response to the query.MEBN can represent a very large class of probability distributions, and can be used to construct domain theoriesfor a wide variety of application domains. The conditions in Definitions 3 and 5 do impose some constraints onthe distributions MEBN can represent. For example, MEBN cannot represent the average of infinitely many non-zero numerical random variables. It cannot represent random variables with infinitely long ancestor chains, such as aMarkov chain that extends without bound in both the past and the future directions. Nevertheless, MEBN provides arich language for knowledge representation. It can handle n-ary relations, context-specific independence, quantifiers,combining functions, and aggregates.The unit of representation in MEBN is a conceptually meaningful cluster of related random variables. In manyapplications, this representational unit is more natural than a focus on uncertainty about attributes of objects. UnlikeOOBNs and PRMs, MEBN can represent n-ary relationships. Although the specification given in this paper is un-typed, the examples illustrate a simple type system that was formalized in [12]. It is straightforward in typed MEBNto represent probabilistic relational models. Because there presently is no direct MEBN implementation, several pub-lished applications have translated MEBN theories into relational models and used the Quiddity*Suite probabilisticrelational modeling and KBMC toolkit [37] to construct situation-specific Bayesian networks (e.g., [1,13]). Rules aregiven in [12] for translating MEBN theories into Quiddity*Suite frames. There are some features of MEBN (mostnotably context constraints) that cannot be represented declaratively in standard relational languages, but the abilityof Quiddity*Suite to combine Prolog-style rules with a frame-based relational modeling language provides the abilityto specify much more powerful declarative representations (e.g., [25]).Many languages designed for tractable implementation have taken the strategy of restricting expressiveness to en-sure that answers to probabilistic queries are decidable. In an open world, the answer to many queries of interest willbe undecidable, and the best that can be expected is an approximate answer. Languages that provide decidable, closed-form responses to limited classes of queries have an important place both theoretically and practically. Nevertheless,intelligent reasoning in a complex world requires principled methods of coping with intractable and even undecidableproblems. MEBN exploits the language of graphical models to compose consistent domain theories out of modu-lar components connected via clearly defined interfaces, and thus can support efficient implementations of tractabledomain theories. Yet, MEBN can represent highly complex, intractable, and even undecidable domain theories. Al-though the answer to a probabilistic query may be undecidable, and may be intractable even when it is decidable,Bayesian decision theory provides a sound mathematical basis for defining and analyzing the properties of processesthat converge to the correct response to undecidable queries, as well as resource-bounded processes that balance effi-ciency against accuracy. Bayesian theory also provides semantics for the relationship between empirical proportionsand probabilities, and logically justified and theoretically principled way to combine empirical frequencies with priorknowledge to refine theories in the light of observed evidence.MEBN is inherently open. Bayesian learning theory provides an inbuilt capability for MEBN-based systems tolearn better representations as observations accrue. Parameter learning can be expressed as inference in MEBN theo-ries that contain parameter random variables. Structure learning can also be handled by introducing multiple versionsof random variables having home MFrags with different structures. A more natural approach to structure learning,as well as a more flexible type system, requires a polymorphic extension of MEBN. Clearly, a typed MEBN withpolymorphism would be desirable for many applications. We chose in this paper to focus on the basic version of thelogic to highlight its relationship to classical first-order logic and demonstrate that the logic is sufficiently powerful torepresent general first-order theories. Extensions of MEBN are planned to incorporate additional expressivity.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178171AcknowledgementsResearch for this paper was partially supported by DARPA & AFRL contract F33615-98-C-1314, Alphatechsubcontract 98036-7488. Additional support was provided by the Advanced Research and Development Activity(ARDA), under contract NBCHC030059, issued by the Department of the Interior. The views, opinions, and findingscontained in this paper are those of the author and should not be construed as an official position, policy, or decision,of DARPA or ARDA unless so designated by other official documentation. Appreciation is extended to Bruce D’Am-brosio, Suzanne Mahoney, Mike Pool, Bikash Sabata, Masami Takikawa, Dan Upper, and Ed Wright for many helpfuldiscussions. Special thanks are due to Paulo Costa and Tod Levitt for extensive feedback on earlier drafts. The authoris grateful to the anonymous reviewers of earlier drafts for their thorough reviews, insightful comments, and usefulsuggestions. Special thanks are due to an anonymous reviewer of a previous version of this paper for finding a fewerrors.Appendix A. Proofs and algorithmsThis appendix proves that a MEBN theory represents a globally consistent joint distribution over random variableinstances, proves that a MEBN theory constructed as described in Section 4.2 places non-zero probability of valueT on logical random variables corresponding to satisfiable first-order sentences, presents the SSBN constructionalgorithm, shows that SSBN construction identifies an unsatisfiable set of findings in finitely many steps, and provesthat when findings are consistent, SSBN construction converges with probability 1 to the posterior distribution over aMEBN theory’s random variables given that all finding random variables have value T.A.1. Proof of existence theoremTheorem 1. Let T = {F1, F2, . . .} be a simple MEBN theory. There exists a joint unique probability distribution P genTon the set of instances of the random variables of its MFrags that is consistent with the local distributions assigned bythe MFrags of T . This distribution respects the independence assumptions encoded in the MFrags. That is, a randomvariable instance is conditionally independent of its non-descendants given its full (possibly infinite) set of parentinstances.Proof. Let Z = {φ1(α1), . . . , φm(αm)} be a finite subset of NT , and let D = max[dφ(α): φ(α) ∈ {φ1(α1), . . . , φm(αm)}]be the maximum depth of the instances of Z. Suppose D = 0. Let πT 1(φ1(α1), . . . , φm(αm)) be a distribution in whichthe φi(αi) are independent and distributed according to the default distributions πφi (θi )(·|∅) from their home MFragsFφi (αi ). All finite-dimensional distributions constructed in this way from depth 0 elements of NT are consistent witheach other and with the local distributions of T . Therefore, Kolmogorov’s existence theorem11 implies that thesefinite-dimensional distributions can be extended to a joint distribution πT 1 over all instances of depth zero randomvariables, and this joint distribution is consistent with the local distributions of T .Now, suppose T represents a joint distribution πT D over all instances of all random variables of depth less thanD. Let Z = {φ1(α1), . . . , φm(αm)} be a finite subset of NT such that no φi(αi) ∈ Z has depth greater than D. LetA denote the (possibly infinite) subset of NT consisting of the ancestors of depth D elements of Z, together withany elements of Z with depth strictly less than D. Clearly, any instance ϕ(β) ∈ A must have depth less than D.Therefore, the marginal distribution of πT D represents a joint distribution for A consistent with the local distributionsof T .Let S = {ϕ(β) = γ : ϕ(β) ∈ A} be a set of value assignment terms, one for each element of A. Suppose φi(αi) ∈ Z.If φi(αi) has depth less than D, then φi(αi) ∈ A and S assigns a particular value to φi(αi) with probability 1. Other-wise, condition 3e implies that there is a finite subset Sφi (αi ) ⊂ S such that πφi (αi )(·|Sφi (αi )) = πφi (αi )(·|S∗) whenever11 Kolmogorov’s existence theorem (cf., [6]) states that if joint distributions exist for all finite subsets of a collection of random variables, and if allthese finite-dimensional distributions are consistent with each other, then a joint distribution exists for the infinite collection of random variables.172K.B. Laskey / Artificial Intelligence 172 (2008) 140–178Sφi (αi ) ⊂ S∗ ⊂ S.12 Thus, given the value assignments in S, T assigns a well-defined conditional distribution to eachφi(αi) ∈ Z, which is denoted πφi (αi )(·|S). Define a joint conditional distributionπT (D+1)(cid:3)φ1(α1) = γ1, . . . , φm(αm) = γm | S(cid:4)=(cid:3)φi(αi) = γi | S(cid:4)πφi (αi )m(cid:5)i=1in which the φi(αi) are independent and distributed as assigned by the local distributions in their home MFragsconditional on the value assignments in S. Existence of both a joint conditional distribution for the φi(αi) and amarginal distribution for S implies that the marginal joint distribution(cid:6) m(cid:5)πT (D+1)(cid:3)(cid:4)φ1(α1), . . . , φm(αm)=(cid:3)φi(αi) | S(cid:4)πφi (αi )dπT D(S)(A.1)i=1exists and is consistent with the local distributions of T . The marginal distribution (1) is expressed as an integralrather than a sum because there may be uncountably many different ways to choose the value assignments S ={ϕ(β) = γ : ϕ(β) ∈ A}.This construction can be carried out for any finite set of depth D instances, and it is clear that all the distributionsthus defined are consistent with each other and with the local distributions of T . This implies that T represents a jointdistribution over arbitrary finite subsets of NT , and that the distributions constructed in this way are consistent witheach other and with the local distributions of T . A second application of Kolmogorov’s existence theorem implies thatT represents a joint distribution over all instances of random variables in VT . It is clear that this distribution is con-sistent with the local distributions of T . Uniqueness and satisfaction of the causal Markov conditions are immediateconsequences of the construction of the distribution. (cid:3)A.2. SSBN construction algorithmThe situation-specific Bayesian network construction algorithm takes a MEBN theory T , a finite (possibly empty)set of target random variable instances, and a finite (possibly empty) set of finding random variable instances, andcomputes a sequence of Bayesian networks containing the target and finding random variable instances. The algorithmmay be interrupted at any time to obtain an approximate SSBN. If the findings are inconsistent and the algorithm is notinterrupted, it will discover the inconsistency in finitely many steps. If the algorithm terminates without interruptionand the findings are consistent, the last Bayesian network in the sequence can be used to compute the joint distributionof the target random variable instances given that all finding random variable instances have value T. That is, additionalmodel construction would not change the result of the query. For some problems, the algorithm will not terminateunless it is interrupted, but it produces a sequence of approximate SSBNs that converge to the correct query response.We give the SSBN construction for simple MEBN theories only. The modification for mixture MEBN theories isstraightforward. SSBN construction proceeds as follows:SSBNConstruct: The inputs to SSBNConstruct are:• A simple MEBN theory T with partial ordering (cid:19) and modeler-defined MFrags F defined on a set X of randomvariable symbols and a set A of constant symbols;• A finite (possibly empty) set {τi}i(cid:2)T of non-finding random variable instances called the target random variableinstances;• A finite (possibly empty) set {φi}i(cid:2)F of finding random variable instances.The steps in SSBNConstruct are:1. Initialization. Set Q = {τi}i(cid:2)T ∪ {φi}i(cid:2)F , and set R0 = Q. Let N0 and K0 be positive integers. Set the iterationnumber i equal to 0.2. SSBN structure construction. Set the structure of the approximate SSBN Bi as follows:12 Theorem 1 holds under weaker conditions on the local distributions, but condition 3e suffices to show that MEBN can represent classicalfirst-order logic.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178173Fig. 12. Situation-specific Bayesian network.• Set Bi equal to a Bayesian network in which the nodes are the random variables in Ri . Add an arc from randomvariable α to β if α is an instance of a parent of β or is a context random variable in the home MFrag of β.Remove any arcs to β if there are no influencing configurations for β (i.e., there are no configurations of itsparents and context random variables that match the context constraints).• Do until no more changes to Bi occur (see Fig. 12):◦ Remove from Bi all barren nodes, that is, nodes having no descendants in Q;◦ Remove from Bi all nodes that are d-separated by finding nodes from any target nodes;◦ Remove from Bi the parents of any nuisance node for which there is a current cached marginal distribution.A nuisance node [52] is a node that is computationally relevant given the query, but is on no evidential trail13between an evidence and a target node.3. Local distribution construction. Calculate the local distributions in Bi from the local distributions in the MFragsof T , with modifications to restrict random variables to have no more than Ni possible values, to approximate theeffects of random variables that have not been enumerated, and to ensure that computation of local distributionshalts. Specifically:• If ψ is a nuisance node with a current cached marginal distribution (in this case, step 2 above ensures that ψwill be a root node in Bi ), assign it the cached marginal distribution.• For any other node ψ in Bi , let Sψ be a configuration of states of the parents of ψ in Bi (by convention, Sψ = ∅if ψ has no parents in Bi ).◦ If Sψ assigns each parent ψ in Bi to its 1st, 2nd, . . . , or (Ni − 1)st state, then run the algorithm for computingthe probabilities of the first Ni possible values for ψ given Sψ , terminating the computation after Ki steps.Assign the first Ni − 1 states of ψ to the probabilities returned by this algorithm, and assign the Ni th possiblevalue equal to 1 minus the sum of the probabilities for the other values.◦ If Sψ assigns any parent ψ in Bi to its Ni th state, then assign ψ a default distribution that gives non-zeroprobability to all states of ψ (i.e., to all states if there are fewer than Ni or to the first Ni states otherwise).4. Inference. Apply a standard Bayesian network inference algorithm to compute the conditional distribution for thenon-finding random variables in Bi given the finding random variables in Bi . For each node β in Bi , cache itsmarginal distribution and mark it current.13 A node is computationally relevant if it remains after iteratively removing all barren and d-separated nodes. An evidential trail between two setsof nodes is a minimal active undirected path from a node in one set to a node in the other. If a global joint distribution exists, then nuisance nodescan be marginalized out without affecting the result of the query.174K.B. Laskey / Artificial Intelligence 172 (2008) 140–178• If the inference algorithm indicates that the findings are inconsistent, then set the SSBN S equal to Bi , outputS, and stop with an indication that SSBN construction terminated and T is inconsistent.• Else, if all computationally relevant random variables have been added, no random variable in Bi has more thanNi possible values, and no local distribution computation terminated prior to completion, then set the SSBN Sequal to Bi ; return Bi and the joint distribution of the target random variables; and stop with a flag indicatingthat SSBN construction terminated and T is consistent.• Else, go to step 5.5. Instance enumeration and approximation parameter updating. This step enumerates additional instances of ran-dom variables and increases the limits on the number of allowable states per random variable and computationalsteps for local distributions.• If the stopping criterion is met, output Bi and the joint distribution computed in step 4, and stop with anindication that SSBN construction did not terminate.• Else, set Ri+1 = Ri . For each random variable instance β ∈ Bi for which a change in the local distributionmay occur if additional parents are added, add a finite number of instances of parents of β to Ri+1, using aprocess that ensures eventual addition of all instances of parents of β. (Here, a context random variable in arandom variable’s home MFrag counts as a parent.)• Set Ni+1 and Ki+1 to positive integers strictly greater than Ni and Ki , respectively.• For any node in which (i) new parents have been added, or (ii) new states of an ancestor have been added, or(iii) the computation did not halt in computing the local distribution of the node or one of its ancestors, markits marginal distribution as not current.• Increment i, and go to step 2.It is well known that if a set of sentences in FOL is unsatisfiable, then there exists a finite set of ground instancesof a set of logically equivalent sentences that is also unsatisfiable (see, for example, [23,67]). The SSBN constructionalgorithm produces a sequence of Bayesian networks, each of which can be translated into a set of constraints ontruth-values of a finite set of ground instances of FOL sentences implied by the MEBN theory T . Each of theseBayesian networks encodes a probability distribution that assigns non-zero probability to any assignment of truth-values consistent with the constraints it encodes. Each approximate SSBN includes all constraints represented in thepreceding approximate SSBNs, together with additional constraints. If the query set contains only the findings, theneventually all logical constraints implied by the findings and their predecessors in the random variable instance partialorder are enumerated. If the set of all logical constraints is unsatisfiable, then so is a finite subset, and eventually theconstraints encoded in the SSBN will include a finite unsatisfiable subset.The following theorem states that an inconsistent theory can be discovered in a finite number of steps of SSBNconstruction by specifying a query set consisting of only the findings, and setting SSBN construction never to stopunless an inconsistency is found.Theorem 6. If the logical constraints represented by T are unsatisfiable and step 5 of SSBNConstruct is set never tostop, then SSBN construction on a query set consisting only of the findings of T terminates in finitely many steps withan indication that T is inconsistent.Proof. Each approximate SSBN Bi represents a probability distribution over interpretations of a theory for which thelogical axioms form a subset of the logical axioms of T . The domain of this interpretation is a finite set consisting ofall possible assignments of values to the random variables of Bi such that all finding random variables have value T.The approximate SSBN Bi assigns non-zero probability to the hypothesis that all finding random variables have valueT if and only if there is at least one interpretation on this finite domain that satisfies all the logical axioms representedin Bi , which in turn is the case if and only if the logical axioms represented in Bi are simultaneously satisfiable.For k > i, the approximate SSBN Bk includes all logical constraints included in Bi , along with any additional con-straints implied by the local distributions of random variables appearing in Bi+1 but not in Bi . The SSBN constructionprocess eventually adds all computationally relevant random variables, and therefore eventually includes all logicalconstraints represented by the local distributions of any random variable instances that are either findings or ancestorsof findings in the random variable partial ordering (cid:19). Thus, if the findings are unsatisfiable, eventually there will bean approximate SSBN in the sequence that represents an unsatisfiable set of constraints. (cid:3)K.B. Laskey / Artificial Intelligence 172 (2008) 140–178175Note that SSBN construction will never add random variables d-separated from the target random variables byfindings. Therefore, if the query set contains non-finding target random variables, then inconsistencies that wouldbe introduced only by adding d-separated random variables will not be discovered. It is often asserted in logic textsthat an inconsistent theory is “useless” because anything can be proven from a contradiction. In practice, though,inconsistent theories can be quite useful. MEBN can be used to reason with inconsistent theories, as long as queriesare structured so that the target of any given query is d-separated by a subset of the findings from any findings thatcontradict this subset. Thus, MEBN may turn out to be a useful tool for studying conditions under which inconsistenttheories can provide accurate results to probabilistic queries.Condition 3e of Definition 3 implies that in any possible world, each local distribution can be computed fromfinitely many instances of the random variable’s parents and context random variables. However, which instances areneeded can vary from possible world to possible world, and there may be no upper bound on how many instancesare needed. To show that SSBN construction converges to the correct result, it is necessary to show that the correctresponse to a query can be approximated to arbitrary accuracy by explicitly representing only a finite number ofrandom variable instances.Lemma 7. Let Q = {θi}i(cid:2)M be a finite set of random variable instances from a MEBN theory T . Let R denote the setof all random variable instances that are elements of Q or ancestors of elements of Q. Let R0 ⊂ R1 ⊂ R2 ⊂ · · · beRi . Let Bi be thean increasing sequence of finite sets of random variable instances such that Q = R0 and R =Bayesian network constructed from the random variables in Ri . That is: (i) the nodes of Bi are the random variableinstances in Ri ; (ii) there is an arc from θi to θj if θi is either a parent of θj or a context random variable in itshome MFrag, and if there is at least one influencing configuration containing a value assignment for θj ; and (iii) thelocal distribution for each θi is given by its local distribution πθi in its home MFrag. Let S = {θi = αi}i(cid:2)M be anassignment of values to the random variables in Q. Then PBi (S) converges to P genT (S) as i → ∞.(cid:7)iProof. The proof is by induction on the maximum depth of random variable instances in Q. Clearly, the result holdsif all instances in Q are of depth zero. Suppose the result holds for all random variable instances of depth less than D.Suppose the maximum depth of random variables in Q is D.Let R0i be the set obtained by removing from Ri all random variables of depth D; and let Q0 = R00. Let S0 be anassignment of values to random variables in Q0 that agrees with S on the random variables in Q ∩ Q0 (that is, therandom variables in Q of depth strictly less than D). Let B0i be the Bayesian network constructed as described aboveT (S0) as i → ∞.i . By the induction hypothesis, PB0from the random variables in R0(cid:7)Let Ui = Ri\Q. That is, Ui consists of random variables in Bi that are not in Q, and let U∞ =iUi . Let X∞ denotean assignment of values to the random variable instances in U∞, and let Xi denote the subset of value assignmentscorresponding to random variables in Ui . Suppose none of the depth D random variables in Ri has value ⊥. Let θ bea depth D random variable. Condition 3e of Definition 3 implies that there is an integer N such that:(S0) → P geniπθ (α | XN ∪ S0) = πθ (α | XN +1 ∪ S0) = · · · = πθ (α | X∞ ∪ S0).(A.2)Let N∗ denote the smallest N for which (2) holds. The number N∗ is a function of X∞ ∪ S0. Marginalized over X∞,N∗ has a probability distribution P genT (N ∗|S0).We can write:P genT (S) =(cid:8)(cid:8)(cid:9) (cid:5)πθ (θ = α | Xn ∪ S0)P genT (Xn | S0, N(cid:10)∗ = n)nXn× P gen(θ=α)∈Sdepth(θ)=D∗ = n | S0)P genT (NT (S) be an approximation of P gen(cid:5)(cid:8)Let P n∗ables:P n∗T (S) =T (S0).T (S) obtained by enumerating only the finite set Un∗ ∪ Q of random vari-(A.3)πθ (θ = α | Xn∗ ∪ S0)P genT (Xn∗ ∪ S0).(A.4)Xn∗(θ=α)∈Sdepth(θ)=DCombining (3) and (4), and noting that πθ (θ = α | XN ∗ ∪ S0) = πθ (θ = α | Xn∗ ∪ S0) when N∗ (cid:3) n∗, we have:176(cid:11)(cid:11)P n∗T (S) − P gen(cid:11)(cid:11) =T (S)K.B. Laskey / Artificial Intelligence 172 (2008) 140–178(cid:9) (cid:5)(cid:8)(cid:8)πθ (θ = α | Xn ∪ S0)P genT (Xn ∪ S0 | Nn>n∗Xn(θ=α)∈Sdepth(θ)=D(cid:3) P genT (N∗ (cid:2) n∗ | S0)P genT (S0).(cid:10)∗ = n)P genT (N∗ = n, S0)(A.5)Let u be a positive real number, and let n∗ be an integer such thatP genT (S)| < (u/2)P genas i → ∞. We can therefore choose n∗ sufficiently large that:(cid:8)T (S0). By the induction hypothesis, the distributions PB0(cid:5)i(cid:11)(cid:11)PBi (S) − P n∗(cid:11)(cid:11) =T (S)(cid:11)(cid:11)(cid:11)(cid:11)(cid:3)πθ (θ = α | Xn∗ ∪ S0)PB0i(Xn∗ ∪ S0) − P n∗(cid:4)T (Xn∗ ∪ S0)(cid:11)(cid:11)(cid:11)(cid:11)(cid:2)n>n∗ P genT (N ∗ = n) < u/2. Then |P n∗(Xn∗ ∪ S0) converge to P genT (S) −T (Xn∗ ∪ S0)Xn∗(θ=α)∈Sdepth(θ)=DT (S0).< (u/2)P gen(cid:11)(cid:11)PBi (S) − P n∗T (S).Then for i > n∗:(cid:11)(cid:11)PBi (S) − P gen(cid:11)(cid:11) (cid:3)T (S)Therefore, PBi (S) converges to P gen(cid:11)(cid:11) +T (S)(cid:11)(cid:11)P n∗T (S) − P gen(cid:11)(cid:11) < uP genT (S)T (S0).(A.6)Now consider the case in which one or more of the depth D random variables has value ⊥. It is clear that if PBi (S)converges to P genT (S) for all S in which k or fewer of the depth D random variables has value ⊥, then it must alsoconverge when k + 1 of the depth D random variables has value ⊥. This establishes the result for sets Q of depth nogreater than D, and thus concludes the proof. (cid:3)Theorem 8. Suppose the logical constraints represented by T are satisfiable. Furthermore, suppose that the algorithmdescribed in Definition 3c for computing values of πψ(ε)(A|S) returns a zero value only if the exact value πψ(ε)(A|S) isequal to zero. If step 7 of SSBNConstruct is set never to stop, then SSBN construction on query set Q either terminateswith the distribution P geni=i(cid:2)F), or produces a sequence B1, B2, . . . , in which the probability distributionfor Q0 given the findings in Bi converges to the distribution represented by T .T (Q0 | {φi}Proof. Lemma 7 establishes that the distribution on Q can be approximated to arbitrary accuracy by enumerating onlyfinitely many of the random variable instances enumerated during SSBN construction. However, unlike in Lemma 7,SSBN construction also approximates the local distributions by enumerating only finitely many possible values andterminating computation after a finite of steps. Because the maximum number of possible values and the maximumlength of computation increase with the number of SSBN steps, and do not have an upper bound, these additionalapproximations can be added without affecting convergence. (cid:3)References[1] G. Alghamdi, K.B. Laskey, E. Wright, D. Barbara, K.-C. Chang, Modeling insider behavior using multi-entity Bayesian networks, in: 10thAnnual Command and Control Research and Technology Symposium, 2005.[2] F. Bacchus, Representing and Reasoning with Probabilistic Knowledge: A Logical Approach to Probabilities, MIT Press, Boston, MA, 1990.[3] F. Bacchus, A. Grove, J.Y. Halpern, D. Koller, From statistical knowledge bases to degrees of belief, Artificial Intelligence 87 (1997) 75–143.[4] O. Bangsø, H. Langseth, T. Nielsen, Structural learning in object oriented domains, in: FLAIRS, 2001.[5] O. Bangsø, P.H. Wuillemin, Object oriented Bayesian networks: A framework for topdown specification of large Bayesian networks andrepetitive structures, Technical Report CIT-87.2-00-obphw1, Department of Computer Science, Aalborg University, Aalborg, 2000.[6] P. Billingsley, Probability and Measure, Wiley, New York, 1995.[7] T. Binford, T.S. Levitt, Evidential reasoning for object recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence 25 (7)(2003) 837–851.[8] C. Boutilier, N. Friedman, et al., Context specific independence in Bayesian networks, in: Uncertainty in Artificial Intelligence: Proceedingsof the Twelfth Conference, Morgan Kaufmann, San Francisco, CA, 1996.[9] R.J. Brachman, R.E. Fikes, H.J. Levesque, KRYPTON: A functional approach to knowledge representation, IEEE Computer Society 16 (10)(1983) 67–73.[10] W.L. Buntine, Operations for learning with graphical models, Journal of Artificial Intelligence Research 2 (1994) 159–225.[11] E. Charniak, R.P. Goldman, A Bayesian model of plan recognition, Artificial Intelligence 64 (1993) 53–79.K.B. Laskey / Artificial Intelligence 172 (2008) 140–178177[12] P. Costa, Bayesian semantics for the Semantic Web, Doctoral Dissertation, School of Information Technology and Engineering, George MasonUniversity, Fairfax, VA, 2005; http://hdl.handle.net/1920/455.[13] P. Costa, K.B. Laskey, F. Fung, M. Pool, M. Takikawa, E. Wright, MEBN logic: A key enabler for network-centric warfare, in: 10th AnnualCommand and Control Research and Technology Symposium, 2005.[14] R.G. Cowell, Probabilistic Networks and Expert Systems, Springer-Verlag, Berlin, 1999.[15] B. D’Ambrosio, Local expression languages for probabilistic dependency, in: Uncertainty in Artificial Intelligence: Proceedings of the SeventhConference, Morgan Kaufmann, San Mateo, CA, 1991.[16] B. D’Ambrosio, M. Takikawa, J. Fitzgerald, D. Upper, S.M. Mahoney, Security situation assessment and response evaluation (SSARE), in:DARPA Information Survivability Conference & Exposition II, IEEE Computer Society, 2001.[17] E. Davis, Representations of Commonsense Knowledge, Morgan Kaufmann, San Mateo, CA, 1990.[18] B. de Finetti, Theory of Probability: A Critical Introductory Treatment, second English ed., Wiley, New York, 1934/1990; Translated byA. Machi and A.F.M. Smith.[19] L. De Raedt, K. Kersting, Probabilistic logic learning, ACM-SIGKDD Explorations: Special Issue on Multi-Relational Data Mining 5 (1)(2003) 31–48.[20] M.H. DeGroot, Lindley’s paradox: Comment, Journal of the American Statistical Association 77 (378) (1982) 336–339.[21] M.H. DeGroot, M.J. Schervish, Probability and Statistics, Addison Wesley, Boston, MA, 2002.[22] R.J. Elliott, L. Aggoun, J.B. Moore, Hidden Markov Models: Estimation and Control, Springer-Verlag, Berlin, 1995.[23] H.B. Enderton, A Mathematical Introduction to Logic, Harcourt Academic Press, 2001.[24] G. Frege, Begriffsschrift, in: J. Heijenoort (Ed.), From Frege to Gödel, Harvard University Press, Cambridge, MA, 1879/1967; Translated.[25] F. Fung, K.B. Laskey, M. Pool, M. Takikawa, E. Wright, PLASMA: Combining predicate logic and probability for information fusion anddecision support, in: AAAI Spring Symposium on Decision Support in a Changing World, 2005.[26] D. Geiger, D. Heckerman, Advances in probabilistic reasoning, in: Uncertainty in Artificial Intelligence: Proceedings of the Seventh Confer-ence, Morgan Kaufmann Publishers, San Mateo, CA, 1991.[27] L. Getoor, N. Friedman, D. Koller, A. Pfeffer, Learning probabilistic relational models, in: S. Dzeroski, N. Lavrac (Eds.), Relational DataMining, Springer-Verlag, Berlin, 2001.[28] L. Getoor, D. Koller, B. Taskar, N. Friedman, Learning probabilistic relational models with structural uncertainty, in: ICML-2000 Workshopon Attribute-Value and Relational Learning:Crossing the Boundaries, Standford, California, 2000.[29] Z. Ghahramani, Learning dynamic Bayesian networks, in: C.L. Giles, M. Gori (Eds.), Adaptive Processing of Sequences and Data Structures:Lecture Notes in Artificial Intelligence, Springer-Verlag, Berlin, 1998, pp. 168–197.[30] W. Gilks, A. Thomas, D.J. Spiegelhalter, A language and program for complex Bayesian modeling, The Statistician 43 (1994) 169–178.[31] S. Glesner, D. Koller, Constructing flexible dynamic belief networks from first-order probabilistic knowledge bases, in: ECSQARU, 1995,pp. 217–226.[32] U. Grenander, Elements of Pattern Theory, Johns Hopkins University Press, Baltimore, MD, 1996.[33] T.R. Gruber, A translation approach to portable ontology specifications, Knowledge Acquisition 5 (2) (1993) 199–220.[34] J.Y. Halpern, An analysis of first-order logics of probability, Artificial Intelligence 46 (May 1991) 311–350.[35] D. Heckerman, C. Meek, D. Koller, Probabilistic models for relational data, MSR-TR-2004-30. Microsoft Corporation, Redmond, WA, 2004.[36] C. Howson, P. Urbach, Scientific Reasoning: The Bayesian Approach, Open Court, Chicago, IL, 1993.[37] IET, Quiddity*Suite Technical Guide, Information Extraction and Transport, Inc., Arlington, VA, 2004.[38] ISO/IEC, Information technology—Common Logic (CL)—A framework for a family of logic-based languages, ISO/IEC 24707:2007, Inter-national Organisation for Standardisation, Geneva, Switzerland, 2007.[39] M. Jaeger, Reasoning about infinite random structures with relational Bayesian networks, in: Proceedings of the 6th International Conference(KR ’98), 1998.[40] M. Jaeger, Complex probabilistic modeling with recursive relational Bayesian networks, Annals of Mathematics and Artificial Intelligence 32(2001) 179–220.[41] E.T. Jaynes, Probability Theory: The Logic of Science, Cambridge University Press, Cambridge, UK, 2003.[42] F.V. Jensen, Bayesian Networks and Decision Graphs, Springer-Verlag, Berlin, 2001.[43] K. Kersting, L. De Raedt, Adaptive Bayesian logic programs, in: Proceedings of the Eleventh International Conference on Inductive LogicProgramming (ILP 2001), Springer-Verlag, Berlin, 2001.[44] D. Koller, A. Pfeffer, Object-oriented Bayesian networks, in: Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference,Morgan Kaufmann, San Francisco, CA, 1997.[45] H. Langseth, T. Nielsen, Fusion of domain knowledge with data for structured learning in object-oriented domains, Journal of MachineLearning Research 4 (2003) 339–368.[46] K.B. Laskey, P. Costa, Of Klingons and starships: Bayesian logic for the 23rd century, in: Uncertainty in Artificial Intelligence: Proceedingsof the Twenty-First Conference, AUAI Press, Arlington, VA, 2005.[47] K.B. Laskey, B. D’Ambrosio, T.S. Levitt, S.M. Mahoney, Limited rationality in action: Decision support for military situation assessment,Minds and Machines 10 (2000) 53–77.[48] K.B. Laskey, S.M. Mahoney, Network fragments: Representing knowledge for constructing probabilistic models, in: Uncertainty in ArtificialIntelligence: Proceedings of the Thirteenth Conference, Morgan Kaufmann, San Mateo, CA, 1997.[49] K.B. Laskey, S.M. Mahoney, E. Wright, Hypothesis management in situation-specific network construction, in: Uncertainty in ArtificialIntelligence: Proceedings of the Seventeenth Conference, Morgan Kaufman, San Mateo, CA, 2001.[50] S. Lauritzen, Graphical Models, Oxford Science Publications, Oxford, 1996.178K.B. Laskey / Artificial Intelligence 172 (2008) 140–178[51] T.S. Levitt, C.L. Winter, C.J. Turner, R.A. Chestek, G.J. Ettinger, S.M. Sayre, Bayesian inference-based fusion of radar imagery, militaryforces and tactical terrain models in the image exploitation system/balanced technology initiative, International Journal of Human–ComputerStudies 42 (1995).[52] Y. Lin, M.J. Druzdzel, Computational advantages of relevance reasoning in Bayesian belief networks, in: Uncertainty in Artificial Intelligence:Proceedings of the Thirteenth Conference, Morgan Kaufmann, San Francisco, CA, 1997.[53] S.M. Mahoney, Network Fragments, School of Information Technology and Engineering, George Mason University, Fairfax, VA, 1999.[54] S.M. Mahoney, K.B. Laskey, Constructing situation specific networks, in: Uncertainty in Artificial Intelligence: Proceedings of the FourteenthConference, Morgan Kaufmann, San Mateo, CA, 1998.[55] S.M. Mahoney, K.B. Laskey, Representing and combining partially specified conditional probability tables, in: Uncertainty in ArtificialIntelligence: Proceedings of the Fifteenth Conference, Morgan Kaufmann, San Mateo, CA, 1999.[56] B. Milch, B. Marthi, S. Russell, D. Sontag, D.L. Ong, A. Kolobov, BLOG: Probabilistic models with unknown objects, in: Proceedings of theNineteenth Joint Conference on Artificial Intelligence, 2005.[57] K. Murphy, Dynamic Bayesian Networks: Representation, Inference and Learning, Computer Science Division, University of California,Berkeley, CA, 1998.[58] S. Natarajan, P. Tadepalli, E. Altendorf, T.G. Dietterich, A. Fern, A. Restificar, Learning first-order probabilistic models with combining rules,in: Proceedings of the 22nd International Conference on Machine Learning, 2005.[59] R.E. Neapolitan, Learning Bayesian Networks, Prentice Hall, New York, 2003.[60] L. Ngo, P. Haddawy, Answering queries from context-sensitive probabilistic knowledge bases, Theoretical Computer Science 171 (1997)147–177.[61] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo, CA, 1988.[62] C.S. Peirce, On the algebra of logic, American Journal of Mathematics 7 (1885) 180–202.[63] A. Pfeffer, Probabilistic Reasoning for Complex Systems, Stanford University, Stanford, CA, 2000.[64] A. Pfeffer, IBAL: An integrated Bayesian agent language, in: Joint Conference on Artificial Intelligence (IJCAI), 2001.[65] D. Poole, Probabilistic Horn abduction and Bayesian networks, Artificial Intelligence 64 (1) (1993) 81–129.[66] D. Poole, First-order probabilistic inference, in: Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, 2003.[67] S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, Prentice-Hall, Upper Saddle River, NJ, 2002.[68] T. Sato, Modeling scientific theories as PRISM programs, in: ECAI98 Workshop on Machine Discovery, 1998.[69] L.J. Savage, The Foundations of Statistics, Wiley, New York, 1954.[70] J.F. Sowa, Knowledge Representation: Logical, Philosophical and Computational Foundations, Brooks-Cole Publishers, 2000.[71] D.J. Spiegelhalter, A. Thomas, N. Best, Computation on graphical models, Bayesian Statistics 5 (1996) 407–425.[72] L.D. Stone, C.A. Barlow, T.L. Corwin, Bayesian Multiple Target Tracking, Artech House, Boston, MA, 1999.[73] A. Tarski, The semantical concept of truth and the foundations of semantics, Philosophy and Phenomenological Research 4 (1944).[74] M.P. Wellman, J.S. Breese, R.P. Goldman, From knowledge bases to decision models, The Knowledge Engineering Review 7 (1) (1992)35–53.[75] J. Whittaker, Graphical Models in Applied Multivariate Statistics, John Wiley & Sons, Chichester, 1990.