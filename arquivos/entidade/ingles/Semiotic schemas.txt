Artificial Intelligence 167 (2005) 170–205www.elsevier.com/locate/artintSemiotic schemas:A framework for grounding languagein action and perceptionDeb RoyCognitive Machines Group, The Media Laboratory, Massachusetts Institute of Technology, USAReceived 5 August 2004; received in revised form 21 February 2005; accepted 14 April 2005Available online 8 August 2005AbstractA theoretical framework for grounding language is introduced that provides a computational pathfrom sensing and motor action to words and speech acts. The approach combines concepts fromsemiotics and schema theory to develop a holistic approach to linguistic meaning. Schemas serve asstructured beliefs that are grounded in an agent’s physical environment through a causal-predictivecycle of action and perception. Words and basic speech acts are interpreted in terms of groundedschemas. The framework reflects lessons learned from implementations of several language process-ing robots. It provides a basis for the analysis and design of situated, multimodal communicationsystems that straddle symbolic and non-symbolic realms. 2005 Published by Elsevier B.V.Keywords: Grounding; Representation; Language; Situated; Embodied; Semiotic; Schemas; Meaning;Perception; Action; Cross-modal; Multimodal1. Language and meaningThe relationship between words and the physical world, and consequently our ability touse words to refer to entities in the world, provides the foundations for linguistic commu-nication. Current approaches to the design of language processing systems are missing thisE-mail address: dkroy@media.mit.edu (D. Roy).0004-3702/$ – see front matter  2005 Published by Elsevier B.V.doi:10.1016/j.artint.2005.04.007D. Roy / Artificial Intelligence 167 (2005) 170–205171critical connection, which is achieved through a process I refer to as grounding—a term Iwill define in detail. A survey of contemporary textbooks on natural language processingreveals a rich diversity of data structures and algorithms concerned solely with manipula-tion of human-interpretable symbols (in either text or acoustic form) without any seriouseffort to connect semantic representations to the physical world.Is this a problem we should really care about? Web search engines and word processorsseem to work perfectly fine—why worry about distant connections between language andthe physical world? To see why, consider the problem of building natural language process-ing systems which can in principled ways interpret the speaker’s meaning in the followingeveryday scenarios:An elderly woman asks her aide, “Please push that chair over to me”.A man says to his waiter, “This coffee is cold!”.A child asks her father, “What is that place we visited yesterday?”.How might we build a robot that responds appropriately in place of the aide or waiter?How might a web search engine be designed to handle the child’s query? These are ofcourse not questions that are typically considered part of natural language processing, butthese are basic questions that every human language user handles with deceiving ease. Thewords in each of these examples refer to the physical world in very direct ways. The listenercannot do the right thing unless he/she (it?) knows something about the particular physicalsituation to which the words refer, and can assess the speaker’s reasons for choosing thewords as they have. A complete treatment of the meaning of these utterances—involvingboth physical and social dynamics—is beyond the framework presented in this paper.The focus here is on sub-symbolic representations and processes that connect symboliclanguage to the physical world with the ultimate aim of modeling situated language usedemonstrated by these examples.In recent years, several strands of research have emerged that begin to address the prob-lem of connecting language to the world [4,6,12,15,21,22,29,44,51,56,58,62,69,70,74] (seealso the other papers in this volume). Our own efforts have led to several implementedconversational robots and other situated language systems [25,60,61,63–65]. For example,one of our robots is able to translate spoken language into actions for object manipulationguided by visual and haptic perception [64]. Motivated by our previous implementations,and building upon a rich body of schema theory [2,19,34,43,45,50,68] and semiotics [20,42,47,49], I present a theoretical framework for language grounding that provides a com-putational path from embodied, situated, sensory-motor primitives to words and speechacts—from sensing and acting to symbols.The gist of the framework is as follows. Agents translate between speech acts, percep-tual acts, and motor acts. For example, an agent that sees a fly or hears the descriptivespeech act, “There is a fly here” is able to translate either observation into a commonrepresentational form. Upon hearing the directive speech act, “Swat that fly!”, an agentforms a mental representation that guides its sensory-motor planning mechanisms towardsthe intended goal. Signs originate from patterns in the physical world which are sensedand interpreted by agents to stand for entities (objects, properties, relations, actions, situa-tions, and, in the case of certain speech acts, goals). Speech acts, constructed from lexical172D. Roy / Artificial Intelligence 167 (2005) 170–205units, are one class of signs that can be observed by agents. Sensor-grounded perceptionleads to two additional classes of signs which indicate, roughly, the “what” and “where”information regarding an entity. To interpret signs, agents activate structured networks ofbeliefs1 called schemas. Schemas are made of continuous and discrete elements that arelinked through six types of projections. Two of these projection types, sensor and actionprojections, provide links between an agent’s internal representations and the external en-vironment. These links are shaped by the specific physical embodiment of the agent. Thefour remaining projection types are used for internal processes of attention, categorization,inference, and prediction.The primary focus of the framework in its current form is the interface between wordsand physical environments, and how an agent can understand speech acts that are about theenvironment. There are many important issues that are beyond the scope of this paper. I willnot address language generation, conceptual learning, language learning, or the semanticsof social or abstract domains. These topics are clearly of great importance, and will moti-vate future work that takes the framework presented here as a starting point. Learning inparticular deserves further comment. I firmly believe that to scale grounded language sys-tems, statistical machine learning will be required. Without appropriately structured biaseson what is learnable, however, the rich structures underlying situated language use willbe hopelessly out of reach of purely bottom-up data-driven learning systems. The frame-work presented here may provide useful structural constraints for future machine learningsystems.Taxonomic distinctions made in the theory are motivated by recurring distinctions thathave emerged in our implementations—distinctions which in turn were driven by practicalengineering concerns. Although the theory is incomplete and evolving, I believe it will beof value to those interested in designing physically embedded natural language processingsystems. The theory may also be of value from a cognitive modeling perspective althoughthis is not the focus of the paper (see [62]).Connecting language to the world is of both theoretical and practical interest. In practi-cal terms, people routinely use language to talk about concrete stuff that machines cannotmake sense of because machines have no way to jointly represent words and stuff. We talkabout places we are trying to find, about the action and characters of video games, about theweather, about the clothes we plan to buy, the music we like, and on and on. How can webuild machines that can converse about such everyday matters? From a theoretical perspec-tive, I believe that language rests upon deep non-linguistic roots. Any attempt to representnatural language semantics without proper consideration of these roots is fundamentallylimited.1 Although this paper deals with topics generally referred to as knowledge representation in AI, my focus willbe on beliefs. From an agent’s point of view, all that exists are beliefs about the world marked with degrees ofcertainty. Admittedly, as a robot designer, I share the intuition that a robot’s belief that x is true just in the casesfor which the corresponding situation x is the case—a correspondence that I as the designer can verify (Bickhardcalls this “designer semantics” [10]). True beliefs may be called knowledge in cases where the robot can in somesense justify its belief. However, as a starting point I prefer to model beliefs rather than knowledge so that thenotion of correspondence can be explained rather than assumed.D. Roy / Artificial Intelligence 167 (2005) 170–205173Fig. 1. A network of definitions extracted from Webster’s Dictionary containing circularities. To make use of suchsymbolic networks, non-linguistic knowledge is essential to ground basic terms of linguistic definitions.Inherent to current natural language processing (NLP) systems is the practice of con-structing representations of meaning that bottom out in symbolic descriptions of the worldas conceived by human designers. As a result, computers are trapped in sensory deprivationtanks, cut off from direct contact with the physical world. Semantic networks, meaning pos-tulates, and various representations encoded in first order predicate calculus all take objectsand relations as representational primitives that are assigned symbolic names. Without ad-ditional means to unpack the meaning of symbols, the machine is caught in circular chainsof dictionary-like definitions such as those shown in Fig. 1 [27]. Efforts to encode knowl-edge using symbolic forms which resemble natural language and that can be written downby human “knowledge engineers” [37,40] are variations of this theme and suffer from thesame essential limitations. Dictionary definitions are meaningful to humans in spite of cir-cularity because certain basic words (such as the words infants tend to learn first) hook intonon-linguistic experience and non-linguistic innate mental structures. How can we designmachines that do the same? To address this question, let us shift our attention to a verydifferent kind of machine intelligence: robot perception and control.Consider the problem of designing a robot that avoids obstacles and navigates to lightsources in a room. Robot designers have learned that it is a bad idea to simply tell robotswhere obstacles and lights are and expect the robot to work. This is because in practice,with high probability, human mediated descriptions will not quite match the state of theactual environment. No matter how accurately we draw a map and provide navigation in-structions, the robot is still likely to fail if it cannot sense the world for itself and adaptits actions accordingly. These are well known lessons of cybernetics and control theory.Closed-loop control systems robustly achieve goals in the face of uncertain and changingenvironments. Predictive control strategies are far more effective than reactive ones. In-sights into a mathematical basis of teleology derived from developments in control theoryare every bit as relevant today as they were sixty years ago [59]. Cyclic interactions be-tween robots and their environment, when well designed, enable a robot to learn, verify,and use world knowledge to pursue goals. I believe we should extend this design philoso-phy to the domain of language and intentional communication.174D. Roy / Artificial Intelligence 167 (2005) 170–205A comparison between robotics and NLP provides strong motivation for avoidingknowledge representations that bottom out in symbolic, human generated descriptions ofthe world. Language processing systems that rely on human mediated symbolic knowledgehave no way to verify knowledge, nor any principled way to map language to physical ref-erents. An NLP system that is told what the world is like will fail for the same reasons asrobot do.1.1. Language is embedded in the physical worldIn everyday language, it is the rule rather than the exception that speech acts leveragenon-linguistic context to convey meaning. Barwise and Perry [8] call this the efficiency oflanguage—the same words can mean infinitely different things depending on the situationof their use. In a variety of situated language applications, from spoken dialog systems forcars to conversational interfaces for assistive robots, the relationship between language andthe physical world is a basis of efficient language use.We might design ad hoc solutions for specific restricted applications, but I believe aprincipled solution to address in-the-world language processing requires a basic rethink-ing of how machines interpret words. The theory I develop is motivated by such concerns.The framework has emerged through practice. Over the past several years, we have im-plemented a series of systems which learn, generate, and understand simple subsets oflanguage connected to machine perception and action. These engineering activities havebeen guided by the intuition that language needs to be connected to the real world muchthe way that infants learn language by connecting words to real, visceral experience. Whathas been lacking in our work, however, is a coherent way to describe and relate the varioussystems, and provide a theoretical framework for comparing systems and designing newones. This paper is an attempt to address this latter concern. No attempt has been made toprove that the theory is complete or correct in any formal sense given the early stages ofthe work.Although this paper is focused on systems with tight physical embeddings, the underly-ing theoretical framework may be applied to communication tasks in which direct physicalgrounding is not possible or desirable. My underlying assumption is that an approach whichis shaped primarily by concerns of physical grounding will lead to a richer and more ro-bust general theory of semantics since intentional communication in humans presumablyevolved atop layers of sensory-motor control that were shaped by the nature of the physicalworld.1.2. Duality of meaningConsider the coffee scenario, illustrated in Fig. 2. How does the speaker convey meaningby uttering these words in this context? There seems to be a basic duality in the nature oflinguistic meaning. On one hand, the downward pointing arrow suggests that the speechact conveys meaning by virtue of its “aboutness” relationship with the physical situationshared by communication partners. On the other hand, we can interpret speech acts withina larger theory of purposeful action taken by rational agents as indicated by the upwardsarrow.D. Roy / Artificial Intelligence 167 (2005) 170–205175Fig. 2. The duality of meaning of an everyday situated speech act.The everyday common usage of “meaning” also includes an additional sense, roughlythe emotional connotation of something (“My Father gave me that cup—it has great mean-ing for me”). I believe connotative meanings of this kind are more complex and emergefrom more basic aspects of meaning, roughly as a summary statistic of an individual agent’sgoal-directed experiences. I will thus set aside connotations and focus the more basic as-pects of meaning.Referential meaning: Words are used to talk about (refer to) objects, properties, events,and relations in the worldThe sensory-motor associations of taste and temperature conjured by “coffee” and“cold” rely on agents having similar embodied experiences caused by common underlyingaspects of reality (the chemical composition of coffee and the dynamics of heat transfer asthey interact with bodily actions and senses). Furthermore, the speech act in Fig. 2 is anassertion about the state of a specific part of the world: “this” coffee. The word “coffee”has meaning for the listener because, in part, it is directed towards a particular physicalobject as jointly conceived by speaker and listener. The words “this” and “is” connect thespeech act to a region of space-time, in this case a part of the agents’ here-and-now.Functional meaning: Agents use language to pursue goalsSpeech acts can be considered within a broader theory of purposeful action [26]. Beyondthe literal meaning of “this coffee is cold” interpreted as an assertion about the state of theworld, in certain contexts the speaker may also intend an implied meaning to the effect of176D. Roy / Artificial Intelligence 167 (2005) 170–205“I want hot coffee”. Note that even the literal reading of the sentence can be analyzed withrespect to the speaker’s intentions. For example, the speaker might have just been asked,“Do you know if that coffee is hot or cold?”.I believe that developing a computationally precise and tractable theory of languageuse which simultaneously addresses both referential and functional meaning is a grandchallenge for the cognitive sciences. The framework presented here takes steps towardsaddressing central aspects of this challenge, especially with regards to the referential na-ture of words (but of course much more work remains to be done!). My approach will beto identify essential aspects of communicative meaning that are required to build situatedsystems with primitive linguistic abilities. Many details will necessarily be left out in orderto keep the whole in view with the intent of establishing a framework that can later be en-riched and extended. This is indeed the spirit of Wittgenstein’s recommendation in dealingwith phenomena as complex as natural language [78]:If we want to study the problems of truth and falsehood, of the agreement and disagree-ment of propositions with reality, of the nature of assertion, assumption, and question,we shall with great advantage look at primitive forms of language in which these formsof thinking appear without the confusing background of highly complicated processesof thought. When we look at such simple forms of language the mental mist whichseems to enshroud our ordinary use of language disappears. We see activities, reactions,which are clear-cut and transparent. On the other hand we recognize in these simpleprocesses forms of language not separated by a break from our more complicated ones.We see that we can build up the complicated forms from the primitive ones by graduallyadding new forms.2. GroundingThe term grounding will be used to denote the processes by which an agent relatesbeliefs to external physical objects. Agents use grounding processes to construct mod-els of, predict, and react to, their external environment. Language grounding refers toprocesses specialized for relating words and speech acts to a language user’s environmentvia grounded beliefs. Thus, the grounding of language is taken to be derivative of thegrounding of beliefs. We can view the relationship between language, an agent’s beliefs,and the physical world as illustrated in Fig. 3. Schemas are information structures held byan agent that are modified by perceptual input and that guide action (details of the inter-nal structure of schematized beliefs are provided in Sections 4–6). Interactions betweenschemas and the environment are mediated by perception and motor action. Language useis achieved through comprehension and production processes that operate upon schemas.Fig. 3 is reminiscent of the classic semiotic triangle in which all mappings from words toexternal objects are mediated by thoughts [47]. Thus, the framework developed here mightbe called an approach to “computational semiotics” in which the interpretation of signs isperformed through schemas.Agents use schemas to represent beliefs about their environment. Consider an agent thatis situated next to a table that supports a cup. For the agent to hold the grounded belief thatD. Roy / Artificial Intelligence 167 (2005) 170–205177Fig. 3. Grounding is an interactive process of predictive control and causal feedback.that particular cup is on the table, two conditions must hold: (1) that cup must have causedthe belief via the natural physical laws of the universe (the flow of information via photons,physical contact, sensory transduction, etc.), and (2) the belief must support predictions offuture outcomes regarding that cup conditioned on actions which the agent might take. Onthis definition, the grounding process requires both causal and predictive relations betweenreferent and belief. This cyclic process corresponds to an interpretation-control loop thatmust be implemented by an agent that holds grounded beliefs.By virtue of being embedded in a shared physical world, the beliefs of agents are com-pelled to alignment, providing the basis for coordinated action. Communication gets offthe ground because multiple agents can simultaneously hold beliefs grounded in commonexternal entities such as cups of coffee.I take beliefs about the concrete, physical world of objects, properties, spatial relations,and events to be primary. Agents can of course entertain more abstract beliefs, but these arebuilt upon a physically grounded foundation, connected perhaps by processes of analogyand metaphor as suggested by the widespread use of physical metaphor in language [36].An agent’s basic grounding cycle cannot require mediation by another agent. This re-quirement excludes many interesting classes of agents that exist in purely virtual worlds.This exclusion is purposeful since my goal is to develop a theory of physically groundedsemantics. If A tells B that there is a cup on the table, B’s belief about the cup is notdirectly grounded. If B sees a cup on the table but then permanently loses access to thesituation (and can no longer verify the existence of the cup), then B’s belief is not di-rectly grounded. I am not suggesting that an agent must ground all beliefs—that wouldlead to a rather myopic agent that only knows about what it has directly experienced andcan directly verify. I am suggesting that in order to communicate with humans and buildhigher order beliefs from that communication, an agent must have a subset of its beliefsgrounded in the real world without the mediation of other agents. From a practical point ofview, the necessity for real world unmediated grounding is well known to roboticists as wediscussed above. An autonomous robot simply cannot afford to have a human in the loopinterpreting sensory data on its behalf. Furthermore, complex inner representations must becoupled efficiently, perhaps through layering, for operation under real-world uncertainty.For autonomous robots to use language, we have no choice but to deal with internal repre-sentations that facilitate conceiving of the world as objects with properties that participatein events caused by agents. The need for unmediated grounding can also be argued from a178D. Roy / Artificial Intelligence 167 (2005) 170–205cognitive development perspective. Infants don’t learn language in a vacuum—the mean-ings of first words are learned in relation to the infant’s immediate environment. Languageis bootstrapped by non-linguistic experience and non-linguistic innate structures, pavingthe way for comprehension of dictionary definitions and other sources of ungrounded be-liefs. I return to this topic in Section 8.It is worth heading off one possible criticism of the theory which may arise from amisinterpretation of my definition of grounding. Although we have investigated languagelearning in several systems (e.g., [60,61,63,66]), the focus of this paper is on represen-tational issues and many of the implemented structures that this theory is based on havenot yet been learned by any fully automated system. We have instead used a pragmaticapproach in which some aspects of a representation (typically topological structure) aredesigned manually, and machine learning is used to determine settings of parametersonly when standard statistical estimation algorithms are easily applicable. The potentialcriticism arises from the fact that human designers are creating representations for themachines—in essence, it might appear that we are describing the world for the machine—precisely what I said I wanted to avoid. However, there is in fact no contradiction whenwe consider the definition of grounding carefully. The definition places constraints on theprocess by which a particular set of beliefs come to be, are verified, and maintained. Thedefinition does not make any demands on the source of the underlying design of represen-tational elements. These might be evolved, designed, or discovered by the agent. In all ofour robotic implementations, the systems do indeed construct and maintain representationsautonomously, and link language to those belief structures.2.1. Causal sensor grounding is not enoughBased on the definition of grounding I have stated, causality alone is not a sufficientbasis for grounding beliefs.2 Grounding also requires prediction of the future with respectto the agent’s own actions. The requirement for a predictive representation is a signif-icant departure from purely causal theories. For example, Harnad in his 1990 paper onsymbol grounding suggested a causal solution based on categorical perception of sensor-grounded signals [27]. In my own past work [60,63] I have used “grounding” to describelanguage systems with similar bottom-up sensory-grounded word definitions. The prob-lem with ignoring the predictive part of the grounding cycle has sometimes been calledthe “homunculus problem”. If perception is the act of projecting mental images into an“inner mental theater”, who watches the theater?3 How do they represent what they see?A “pictures in the head” theory without an accompanying theory of interpretation passesthe representational buck. The problem of interpretation is simply pushed one layer in-wards, but leaves open the question of how those internal models have meaning for thebeholder. If the inner process constructs a model of the model, we are led to an infiniteregress of nested models which is of course unsatisfactory.2 Sloman and Chappell also point out this limitation of purely bottom-up sensory grounding [71]. They discussthe need for “symbol attachment” which is similar to the expanded definition of grounding developed in thispaper that encompasses perception and action.3 Dennett calls this the Cartesian theater [17].D. Roy / Artificial Intelligence 167 (2005) 170–205179By requiring that the agent be able to translate beliefs into predictions (not necessarilyabout the immediate future) with respect to the agent’s own actions (where not acting atall is considered a kind of action), we have a broad working definition of interpretationthat avoids descriptive regress. Beliefs have meaning for the agent because they have thepotential to predict future outcomes of the world, which the agent can verify for itself bycomparing predictions to actual sensations. As a result of this framing, beliefs that haveno possible impact on the agent’s abilities to make predictions about the outcomes of itsactions are deemed to have no value.43. Desiderata for a theory of language groundingIf a theory of language grounding is to provide the basis for agents to use physicallysituated natural language, I suggest that it must satisfy three criteria:(1) Unification of representational primitives: Objects, properties, events, and situationsshould be constructed from the same set of underlying primitives. This requirement isdesirable if we are to have a way for beliefs about concrete objects and situations to beefficiently translated into expectations with respect to actions (affordances).(2) Cross-modal translatability: Information derived from perception and language shouldbe interpretable into a common representational form since we want to design agentsthat can talk about what they observe and do.(3) Integrated space of actions: Motor acts (e.g., leaning over to resolve a visual ambigu-ity) and speech acts (e.g., asking a question to resolve a visual ambiguity—“is that acup or a can?”) should be expressed in a single integrated space of actions so that anagent may plan jointly with speech and motor acts to pursue goals.The framework that I will now present is motivated by these requirements. In Section 7I will assess to what extent each goal has been satisfied.4. A theory of signs, beliefs, projections, and schemasThe theoretical framework is a product of building systems and represents my attemptto explicate the theoretical elements and structures that underlie these complex engineeredsystems. Rather than separate the description of implementations, I will highlight relevantimplementations in the course of presenting the framework.4.1. SignsThe structured nature of the physical world gives rise to patterns. A collection of dataelements (e.g., pixels) contains a pattern if the data has non-random structure (or equiva-lently, is compressible, or is low in complexity) [18]. Patterns may be interpreted as signs4 This is consistent with Peirce’s pragmatic approach to epistemology [48].180D. Roy / Artificial Intelligence 167 (2005) 170–205by agents. For example, a pattern of photons caused by a fly can serve as a sign of the fly,if appropriately sensed and interpreted by an agent. I take Peirce’s definition of a sign as astarting point [49]:A sign . . . is something which stands to somebody for something in some respect orcapacity.I will interpret Peirce’s definition in the following way. A sign is a physical pattern(first instance of “something” in Peirce’s definition) which only exists as a sign relative toan interpreter (“somebody”). A sign signifies an object, some entity in the world (secondinstance of “something”). Signs may take other signs as their objects, leading to nesting ofsigns. For example, a shadow might be a sign of a cloud. If the shadow leads to a coolerpatch of ground, the temperature of the ground serves as a sign for both the shadow, andchains through to serve as a sign of the cloud. This does not necessarily mean that aninterpreter can make the connection from a sign to its object, only that the physical causallink exists. Signs signify (stand for) only limited aspects of their objects (“some respect orcapacity”) and thus can serve to abstract and reduce information.4.2. Three classes of signs: Natural, indexical, intentionalSigns may be classified as natural, intentional, and indexical.5 This classificationscheme is not mutually exclusive—a physical pattern may be interpreted as both a naturaland an indexical sign. Natural signs are shaped by nomic physical laws (natural flow ofphotons, pull of gravity, etc.) whereas intentional signs are generated by volitional agentsfor some purpose. The configuration of photons signifying the fly is a natural sign. Thespeech act, “there’s a fly!”, is an intentional sign. Of course the surface form of the wordsexist as a physical pattern of vibrating air molecules, as much a part of the sensible worldas photons, but their origins are fundamentally different. The word “fly” signifies the flyby convention and is uttered by a rational agent with some purpose in mind.Indexical signs situate beliefs relative to a spatiotemporal frame of reference. The loca-tion of the fly within an agent’s field of view may lead to an indexical sign of its spatialposition relative to the viewer’s frame of reference. The semantics of indexical signs arisefrom their use as parameters for control. As we shall see, an indexical belief that specifiesthe spatial location of an object may serve as a control parameter in a robot to controlreaching and visual saccade behaviors directed towards a target. An alternative approachwould be to treat the spatiotemporal location of an object as simply another property ofthe object like its color or weight. We have found, however, that in construction of roboticsystems, separation of spatiotemporal information leads to cleaner conceptual designs.65 This classification scheme is related to Peirce’s three-way classification of iconic, indexical, and symbolicsigns. However, I prefer Ruth Millikan’s distinction between natural and intentional signs [42] for reasons Iexplain in the text.6 For example, in order to debug machine vision systems, spatial coordinate frames are very useful from adesigner’s perspective. When designing structured motor control algorithms, conceptualizing action sequencesover time is equally useful. Whether at a formal level these distinctions might be dropped altogether is unclear.D. Roy / Artificial Intelligence 167 (2005) 170–205181I will now focus in some detail on natural signs, and how an agent can create beliefs aboutobjects via natural signs. Indexicals will then be folded in, leading to spatiotemporally sit-uated beliefs about objects. Finally, we will consider the comprehension and generation ofintentional signs (grounded speech acts).4.3. Analog signsSensors transduce physical patterns from the environment into analog, continuouslyvarying signs (for robots, encoded as electrical potentials) which the agent can furthertransform, interpret, store, and use to guide actions. The only way for signs to enter anagent from the environment is through sensors. The embodiment of an agent determinesits sensors and thus directly affects the signs which an agent can pick up.The agent is attuned to specific channels of sensory input and only detects signs thatappear within those channels. For example, an agent can be attuned to high contrast closedforms that are picked out from a visual environment, localized high intensity impulses froma haptic environment, or speech signals from an acoustic environment while ignoring othersigns from those same channels. Attunement may be innate and unalterable, or determinedby the agent’s state of attention. Multiple channels can be derived from a single sensor(e.g., color and shape are different input channels, both of which might be derived fromthe same camera). On the other hand, multiple sensors can contribute to a single inputchannel.7A sensor-derived channel defines a continuous space which I will call the channel’sdomain. Patterns project into domains via sensors. When a pattern is detected within achannel to which the agent is attuned, the detected pattern is called an analog sign.To take a simple example, imagine a robot with camera input that uses an optical re-gion detector based on background/foreground contrast properties to detect closed visualregions (sensed patterns corresponding perhaps to external objects). The robot is designedto measure two features of the region, its maximum height and width. Presented with anobject, an analog sign of the object is thus a pair of continuously varying magnitudes, hand w. The range of possible values of h and w, and thus the domain of incoming signs forthis channel, range from 0 to H and 0 to W , the height and width of the robot’s visual field(measured in pixels). An observed analog sign is a particular pair of (h, w) values resultingfrom an external stimulus.4.4. Analog beliefsAnalog signs are causally tied to the immediate environment. Their indexicality is in-herently limited to the here-and-now. Beliefs, on the other hand, are persistent informationstructures that “stretch” indexicality over space and time. An analog belief is a distribu-tion over all possible observations within a continuous domain. Analog beliefs map analogsigns to scalar magnitudes. An analog belief can serve as both an element of memory which7 The superior colliculus of cats contain neurons which only fire under the conditions of simultaneous auditory,visual, and somatosensory evidence [75]. This is an example in nature of multiple sensors leading to a singlechannel of input.182D. Roy / Artificial Intelligence 167 (2005) 170–205encodes a history of observations within a channel, and may also serve as a prediction ofwhat will be observed within a channel. To be useful in practice, analog beliefs must becontext-dependent. As we shall see, context is defined by the structure of schemas withinwhich beliefs are embedded.Returning to the earlier robot vision example, an analog belief for the shape input chan-nel can be implemented as a probability density function defined over the two-dimensionalH × W domain. One or more analog sign observations may be summarized as an analogbelief, forming the basis for memory.To recap, analog beliefs are continuous distributions that are about signs since they aredefined with respect to domains inhabited by signs. Natural signs, in turn, are about as-pects of their objects by definition—they are causally connected to their objects due tonomic physical conditions of the environment. Due to the nested relationship between be-liefs, signs, and objects, analog beliefs are about objects. Analog beliefs form elements ofschemas which enable an agent to both encode causal histories of signs and make context-dependent predictions about the observation of new signs, satisfying the causal-predictivegrounding cycle defined in Section 2.4.5. Sensor projectionsI now introduce a graphical notation of typed nodes and typed edges that I will useto represent schemas. Fig. 4 shows the notation for analog beliefs as ovals. Analog be-liefs may have names (“A” in Fig. 4) for notational convenience only. The names are notaccessible to the agent. The meaning of an analog belief from the agent’s perspective is de-rived strictly from its function in guiding the agent’s interpretative, predictive, and controlprocesses. Fig. 4 also introduces a representation of the sensory transduction and observa-tion process as a projection. I will define five more projections as we proceed.4.6. Schema types and tokensFig. 4 is our first example of a schema, a structured network of beliefs connected byprojections. We will encounter a series of schema diagrams of this kind as we proceed.The purpose of these diagrams is to show how elements of the theory are combined toimplement various functions such as active sensing, representation of actions and objects,and higher level situational, goal, and linguistic structures. Agents maintain schema typesin a long term memory schema store. An agent interprets its environment by instantiating,modifying, and destroying schema tokens which are instances of structures such as Fig. 4.For example, if an agent is attuned to an input channel represented by the sensor projectionin Fig. 4, then an observation in this channel may be interpreted by instantiating a tokenof the schema, resulting in an instantiation of an analog belief. The decision of whether toFig. 4. Graphical notation for a sensor projection connected to an analog belief.D. Roy / Artificial Intelligence 167 (2005) 170–205183Fig. 5. Graphical notation for a transformer projection which maps a source analog belief, A1, to a target analogbelief, A2.actually instantiate a schema depends on the control strategy employed by the agent. Thestructure of possible interpretations of an observation are determined by the contents of theagent’s schema store. The contents of the store might be innate, designed, learned, or somecombination thereof.4.7. Transformer projectionsA second type of projection is called a transformer. A transformer performs a mappingfrom one analog domain to another. Transformers may be used to pick out features ofinterest from one analog belief to project a new analog belief, or might be used to combinemultiple analog beliefs. An observation from a source domain may be transformed into anobservation in a target domain by a transformer. For example, an observation of a shaperepresented by h and w may be transformed into a one-dimensional domain by taking theproduct of the terms. In this case, the transformer is simply an analog multiplier. An agentmight want to make this transformation in order to ground words such as “large” whichdepend on surface area. A division transformer (i.e., one that computes the ratio w/ h)could be used to ground words which depend on visual aspect ratios such as “long” (foran implementation along these lines, see [61]). The graphical notation for transformers isshown in Fig. 5.4.8. Categorizer projectionsThe need for categorization is directly motivated by the discrete nature of language.Words (or morphemes) are discrete categorical labels. For an agent to use words that referto the physical world it must have the capacity to discretely categorize continuously varyingrepresentations according to linguistic convention.Categorizer projections map analog domains onto discrete domains via pattern catego-rization. Analog signs are projected into a categorical signs. The mapping is many to one.Information about differences between a pair of analog signs is lost if the signs happento map into the same categorical sign. Categorization provides a mechanism for express-ing functional equivalence. Assuming a well-designed agent, categorization provides themeans of establishing kinds of signs that signify the same information to the agent irre-spective of detectable variations in the signs’ analog domains.Let us return one last time to our example of shape representation via two-dimensionalheight and width features. The analog domain may be discretized by creating a pair of ana-log beliefs defined over the domain that are set into competition with each other. A decisionboundary is defined by the points at which the analog beliefs are of equal magnitude. Thisprocess divides the domain into two categories.184D. Roy / Artificial Intelligence 167 (2005) 170–205In general, pattern categorization may be implemented by discriminative or genera-tive means. Discriminative classifiers (e.g., multilayered perceptrons) explicitly model theboundaries between categories defined with respect to a fixed input feature space. Gener-ative classifiers capture the variability in data in general, for instance by modeling proto-typical members of categories, also with respect to a fixed input feature space. Categoryboundaries emerge due to competition between prototypes, or by applying thresholds toprototypes. All of the systems we have implemented to date rely on prototypes to establishcategorical boundaries.4.9. Categorical beliefsThe second elementary form of belief is a categorical belief, which is a belief about theoutput of a categorization process which maps an analog domain to a discrete domain. Cat-egorization is performed by categorizer projections. The output domain of a categorizer isalways a finite discrete set of outcomes. A categorical belief is thus a discrete distribution(typically a discrete probability distribution in our implementations). In contrast to ana-log beliefs, categorical beliefs rely on categorization—they may be thought of as beliefsabout answers to verbal questions one might ask about analog observations (e.g., will thebrightness of this patch of pixels will be greater than 0.5? Is this shape a square?). Fig. 6introduces notation for categorizer projections and categorical beliefs.In cases where all belief is concentrated on a single discrete outcome, the specific out-come can be given a lowercase label and shown explicitly in the graphical notation asillustrated in Fig. 7. The interpretation of this diagram is that the agent believes (remem-bers, predicts) that the outcome of the categorizer will with high likelihood be the indicatedvalue. Residual belief in other outcomes might be maintained—the notation simply makesthe information structure clear for purposes of conceptual design and analysis.4.10. Action projectionsThe specific physical embodiment of an agent gives rise to a natural set of actionprimitives. For example, the robots we have constructed [52,60,63] have separate servoFig. 6. Graphical notation for a categorizer projection which maps a source analog belief, A, to a target categoricalbelief, D.Fig. 7. Graphical notation for a categorizer projection which maps a source analog belief, A, to a target cate-gorical belief with concentrated belief in a single outcome. The label of this outcome (“square”) is a notationalconvenience and is unavailable to the agent.D. Roy / Artificial Intelligence 167 (2005) 170–205185Fig. 8. Graphical notation for action projections.motors dedicated to each degree-of-freedom (DOF) of the robot. Using standard position-derivative control, each motor is associated with a lowest-level action primitive, essentially“move to position x along a specified spatiotemporal path” subject to failure conditions dueto unanticipated collisions or other external conditions which require reactive response.When an agent attempts to execute a primitive action, it either succeeds or fails.Actions provide a new representational element, an action projection, which results ina discrete binary (success/fail) outcome identical in form to the output of categorizer pro-jections. This can be seen in the graphical notation for action projections indicated bydiamonds in Fig. 8. Actions lead to categorical signs that are represented as categoricalbeliefs, either indicated as distributions over binary outcomes (left-most figure) or alterna-tively, specific beliefs about the success or failure of an action (for notational convenience,I write “success” rather than “D = success”).The use of a categorical belief to represent the outcome of an action binds actions intothe theory of signs at a most basic level. Each time the agent executes an action primitive,the result is a categorical sign about the world it has acted upon. Action and sensing arethereby intimately intertwined.4.11. Active perception, perceptive actionSuccess or failure provides only limited information about an action. In general an agentmay want information about the manner in which an action succeeds or fails. An agent canachieve this through active sensing—sensing analog signs while an action is performed.An example of this arose from experiments with one of our robots, Ripley [52,64], which Inow briefly introduce. Only details relevant to the development of the theory are mentionedhere. More technical descriptions of the robot may be found in previous papers.Ripley, pictured in Fig. 9, is a manipulator robot that was designed for grounded lan-guage experiments. Its seven degrees of freedom are driven by back-drivable, compliantactuators instrumented with position and force sensors, providing the robot with a sense ofproprioception. Two miniature video cameras are placed at the gripper which also servesas the robot’s head (when the robot talks with people, it is hard-coded to look up and“make eye contact”, to make spoken interaction more natural). Ripley’s gripper fingers areinstrumented with force-resistive sensors giving it a sense of touch.The visual system of the robot includes several low-level image processing routinesfor segmenting foreground objects from the background based on color, finding closedform connected visual regions, and extracting basic shape and color features from regions.A higher level visual sub-system tracks regions over time and maintains correspondencebetween regions as the robot’s perspective shifts. When a region is detected and trackedover time, an object is instantiated in Ripley’s mental model. The mental model providesRipley with object permanence. Ripley can look away from the table (such that all theobjects on the table are out of sight), and when it looks back to the table, Ripley retains186D. Roy / Artificial Intelligence 167 (2005) 170–205Fig. 9. Ripley is a 7 DOF manipulator robot terminating in a gripper, pictured here handing an apple to its humanpartner. The human speaks into a head-worn microphone to communicate with the robot. Two video cameras andtouch sensors are mounted on the robot’s gripper. Each actuated joint contains both a position and a force sensor,providing proprioceptive sensing.correspondences between objects from before. If a human intervenes and adds, removes,or moves physical objects, Ripley instantiates, destroys, and updates objects in its mentalmodel. Each object in the mental model encodes basic visual attributes of the object (shape,color) and object locations encoded with respect to Ripley’s body configuration (we willreturn to this last point in the discussion on indexical signs in Section 4.12). Ripley’s visualsystem also includes a face tracker to locate the position of its human communicationpartner. It is able to use this information to modulate spatial language to distinguish, forexample, “the cup on my right” from “the cup on your right” [64].The robot’s work space consists of a round table. The robot’s motor control systemallows it to move around above the table and view the contents of the table from a rangeof visual perspectives. A visually-servoed procedure lets the robot move its gripper to thecentroid of visual regions. Several other motion routines enable the robot to retract to ahome position, to lift objects from the table, and to drop them back onto the table.Ripley understands a limited set of spoken requests. Output from a speech recognizer isprocessed by a spatial language interpreter [25] which maps requests onto goals with re-spect to objects in Ripley’s mental model. A limited look-ahead planner chooses actions tosatisfy goals such as looking at, touching, grasping, lifting, weighing, and moving objects.We are now ready to consider how Ripley might represent the meaning underlyingwords such as “soft” or “hard” used in their most literal, physical sense. An obvious ap-proach, one that we implemented, is to sense the degree of resistance which is met in thecourse of gripping. The resistance reading indicates the compliance of the object, providingthe basis for grounding words that describe these properties.Fig. 10 shows how to combine some of the elements introduced earlier into a schemato represent active perception required for touching to gauge compliance, providing thebasis for grounding words such as “soft” and “hard”. The schema may be interpreted asD. Roy / Artificial Intelligence 167 (2005) 170–205187Fig. 10. A schema for active sensing of compliance through grasping.follows. The action primitive closeGrip, when executed, runs a motor controller connectedto the grip motor. The gripper may or may not reach the targeted position (if the robotsuccessfully grasps a large rigid object, the object will block the gripper from closing).The outcome of the action is represented by the categorical belief D. A sensor projection,senseGripResistance, is connected to D and projects an analog belief with the designer-friendly (but invisible to agent!) annotation COMPLIANCE. The connection from D tothe projection is interpreted to mean: run senseGripResistance while the source actionconnected to D is executed.4.12. Indexical signs and schema parametersIndexical signs signify spatiotemporal locations—regions of space-time. These signsgive rise to beliefs about locations, which in turn provide the grounding for language aboutspace and time. I will use the Ripley implementation once again as an example of howbelief structures can be constructed about locations, and then generalize the idea to developthe theoretical framework.To represent a belief about spatial location, consider how Ripley perceives indexicalsigns of objects such as cups. For Ripley to move its gripper to touch a cup, it must setsix joint angles appropriately (the seventh joint is the gripper open-close angle). WhenRipley touches an object, the six-dimensional joint configuration at the moment of con-tact provides an encoding of the object’s location. Similarly, when Ripley looks aroundthe table and detects that same object, again its six joint angles encode position whencombined with the two-dimensional coordinates of the object’s visual region within Rip-ley’s visual field, leading to an eight-dimensional representation of space. To connect thesetwo representations of spatial location, we implemented a coordinate translation algorithmusing principles of forward kinematics and optical projection combined with knowledgeof Ripley’s physical embodiment. All object positions, regardless of which modality de-tected them, are transformed into a two-dimensional space corresponding to positions onthe surface of the robot’s work surface. As currently implemented, the location of an ob-ject is represented deterministically. However, similar to Isla and Blumberg [30], we planto extend the implementation to support probabilistic representation of spatial location byassigning a distribution over possible two-dimensional positions.When an object is detected by Ripley through touch, the configuration of the robot’sbody provides a six-dimensional value which is an observation of the indexical sign orig-inating from the physical object. We can consider body pose to be an input channel, andthe proprioceptive sensor reading to be an observation of an indexical sign. The domain ofthe input channel spans Ripley’s permissible body poses. A transformer projection mapsindexical observations into a two-dimensional domain, which can be transformed again toguide grasping or visual targeting.188D. Roy / Artificial Intelligence 167 (2005) 170–205To generalize, just as in the case of natural signs, an agent may hold indexical beliefsusing the same forms of representation: analog beliefs and categorical beliefs. Indexicalanalog beliefs are distributions over possible locations within a continuous spatiotem-poral domain. Indexical categorical beliefs are distributions over discrete spatiotemporalcategories. Categorical beliefs can be used to represent relative temporal and spatial rela-tionships such as Allen’s temporal relations [1] or topological spatial relations [55].Consider now a search routine used by Ripley called detectHandContact that requires aparameter L, an analog belief defined over a location domain that the robot can map intoarm positions. The routine detectHandContact(L) is not an action primitive, but insteadimplements an iterative search procedure in which the peak value in L is used to selectwhere to reach, and if no hand contact is detected, the region of L around that peak is setto 0, and the next highest peak in L is tried.The same analog belief that guides the hand control routine can also be used to drive avisual routine, detectVisualRegion(L) which performs a similar visual search through thecontrol of visual saccades. As we shall see in Section 5, the use of a shared indexical analogbelief as the control parameter for multimodal action routines provides a basis for derivinga sensory-motor grounded semantics of spatial location which can be extended to representlocation in space and time.4.13. Complex actions and abstractionBuilding on the idea of parameterized actions, we can now construct structured schemasrepresenting complex actions which will provide the basis for grounding concrete actionverbs. The top schema in Fig. 11 gives an example of a schema for lifting. Examiningthe schema from left to right, when interpreted as a control procedure, to lift means tosearch and find the object (using the parameter L1 to guide the haptic search), close thegripper, query the gripper touch sensors, make sure a stable grip is found, and then tomove the gripper to a new location specified by the peak value of another analog beliefparameter, L2. The same schema can be denoted by an abstracted schema (bottom) whichshows a single action projection that carries the designer-friendly label lift and its twoindexical analog belief parameters, the source and destination locations. Note that otherimplementations of lifting which differ from the top schema, but which take the sameinput parameters and lead to the same change in situations can be represented by the singleschema at bottom. The abstraction process suppresses “by means of” details and retainsonly the parametric form of the whole.Fig. 11. Top: Schema for lift; L1 specifies a distribution over possible start locations and L2 specifies a distribu-tion over the target completion locations. Bottom: Abstracted representation of the same schema.D. Roy / Artificial Intelligence 167 (2005) 170–2051895. Schematization of objectsConsider what effects, which might conceivably have practical bearings, we conceivethe object of our conception to have. Then, our conception of these effects is the wholeof our conception of the object. (Charles Sanders Peirce, 1878.)We are now able to construct schemas for physical objects using a combination of nat-ural analog beliefs, natural categorical beliefs, indexical analog beliefs, sensor projections,categorizer projections, and action projections. We have already seen some examples ofschemas for properties (Fig. 10) and complex actions (Fig. 11). Object schemas subsumeaction and property schemas. This is in contrast to many previous computational interpre-tations of schema theory (e.g., [67,68]) which take objects as representational primitivesdistinct from the actions that act upon them. I believe that for an agent to efficiently gener-ate affordances8 of novel situations for dynamically changing goals on the fly, a practicaloption is to represent objects, actions, and goals with a common set of lower level primi-tives.My approach to the construction of objects from sensory-motor grounded primitivesis consistent with Drescher’s approach [19]. Drescher’s schema mechanism represents anobject as a set of expected interactions with the environment. Drescher, however, chose notto allow parameterization and other structuring elements to enter his framework, which ledto difficulties in scaling the representation to higher order concepts of the kind I seek toaddress. Smith’s conception of the “intentional dance” [72] has also directly influenced myapproach to object perception and conception as a dynamic, constructive process.Fig. 12 illustrates a schema for a palpable visible object such as a cup.9 A functionallyequivalent structure has been implemented in Ripley as the object permanence part of therobot’s mental model which coordinates visual perception, motor control for grasping, andreferent binding for speech based understanding of directives [64].Let us walk through the main paths of this schema to see how it works. The handle ofthis schema is the categorical belief labeled O = cup. Symbolic names (e.g., “cup”) will beattached to handles of schema types. The domain of O is a discrete set of possible objectsknown to the agent.10 The label O = cup indicates that this schema encodes beliefs that areheld in the case for which belief within the domain of O is concentrated on the outcomecup. As with all labels, these are provided for us to design and analyze schemas. From theagent’s perspective, O is simply a categorical belief which derives its semantics from itsrelations to other elements of the schema.Two action projections connect to the schema handle O. Following first the topaction projection, detectVisualRegion(L) projects a binary accomplishment categori-8 Affordances is used here as defined by J.J. Gibson to be a function of both the external real situation and thegoals and abilities of the agent [23].9 Arbib, Iberall, and Lyons have also suggested detailed schemas for multimodal integration of vision and grasp-ing of objects such as cup [3], but their choice of representational elements do not lead to a semiotic interpretationas I seek in here.10 Of course the agent may be able to learn new categories of objects and thus increase the span of the domainover time.190D. Roy / Artificial Intelligence 167 (2005) 170–205Fig. 12. Schema for a tangible (touchable, graspable, moveable, visible) object such as a cup.cal belief. Two sensor projections emanate from this categorical belief. The first,senseRegionLocation feeds back the actual location at which a visual region is found toupdate L. An agent can execute this path, for instance, to actively track the location of anobject. The senseRegion sensor is attuned to the output of detectVisualRegion and projectsR, an analog belief with a domain over possible region geometries. Two transformersproject (extract) analog color and shape information about R onto separate analog beliefs.A categorizer projects the shape analog belief onto a specific shape category outcome, s1which corresponds to the shape of cups (if the distribution of belief in O was concentratedon a different object type, say balls, then the distribution over the SHAPE categorical beliefwould shift as well). To specify a cup of a particular color, the distribution of belief wouldsimply be shifted accordingly in the COLOR analog belief.The lower pathway of the schema may look familiar—it is an embedding of the liftschema that we have already seen (Fig. 11). Two feedback loops are used to update L basedon haptic sensation using the senseHandLocation sensory projection. The indexical L canserve as a coordinator between modalities. In Ripley, for example, we have implemented acoarse-grained vision-servoed grasping routine which relies on the fact that a single spatialindexical coherently binds the expected success locations for vision and touch.The object schema is an organizing structure which encodes various causal dependen-cies between different actions that the agent can take and expectations of sensory feedbackgiven that a cup actually exists at L. To believe that a cup is at L, the agent would becommitted to the expectations encoded in this schema. If the agent executed some of theaction projections of the schema and encountered a failure categorical belief, this wouldprovide cause for the agent to decrease its belief that O = cup. Conversely, if the agent isunaware of the presence of a cup, it may inadvertently discover evidence which leads it toinstantiate this schema and thus develop a new belief that there is a cup at L.D. Roy / Artificial Intelligence 167 (2005) 170–205191The object schema serves as a control structure for guiding action. Embedded in thenetwork are instructions for multimodal active perception and manipulation directed to-wards the object. Given a goal with respect to the object (e.g., finding out what its color is,or moving it to a new location), the schema provides predictions of sequences of actionswhich will obtain the desired results.A central aspect of the concept of a cup, that its function is to carry stuff, is not yetcaptured in this schema. To represent this, containment of objects relative to other objectsmust be represented (Section 5.3).5.1. Construction of objects: Individuation and trackingTo perceive an object, the agent must instantiate a schema token that stands for thatobject. A particular internal information structure within the agent serves as an “absorber”for signs from the environment which the agent attributes to an individual object. It is byvirtue of maintaining a particular mental absorber over time that the agent conceptualizesindividuals over time. These internal structures stand for entities in the world and providethe agent with a basis for grounding names and categorical labels that refer to the entities.Partial evidence may cause an agent to instantiate a complex schema token that makesvarious predictions about possible interactions with the object. The schema is groundedin the actual object because (1) physical signs caused by the object are transduced bythe agent and interpreted into schemas, and (2) these schemas in turn generate a clusterof expectations of future interactions with the object as observed through future signs.Of course an agent might make mistakes in the process of interpreting partial evidence,leading to representational errors. Further interaction with the environment may then leadthe agent to revise its beliefs.5.2. Ambiguity and error in interpretationA sign may give rise to multiple possible interpretations. For instance, any tangibleobject may be placed within an agent’s path leading to physical contact. The resultingunanticipated categorical belief (from, say, detectHandContact) might have been causedby any physical object, not just a cup. Prior context-dependent beliefs encoded as a distri-bution over O play an important role in such cases. If the agent has an a priori basis forlimiting expectations to a reduced set of objects, then ambiguity is reduced at the outset. Ifan agent knows it is indoors, the priors on things usually found outdoors can be reduced.Regardless of how low the entropy of an agent’s priors may be, sensory aliasing is a factof life. A circular visual region impinging on an agent’s retina might signal the presenceof a ball, a can viewed from above, a flat disc, and so forth. In response, the agent mightinstantiate multiple schema tokens, one for each significantly likely interpretation.An agent may misinterpret signs in two ways. First, if the agent detects a novel objectfor which it has not matching schema type, it will be impossible for it to instantiate anappropriate schema in response to signs of the objects. If the potential number of schemasis too large, a pragmatic approach for the agent might be to instantiate a likely subset,which can be revised on the basis of future observations. Error may enter in the process ofdeciding on the likely subset. For example, if a robot detects a visual sign of a distant visual192D. Roy / Artificial Intelligence 167 (2005) 170–205region, this might be a sign of virtually any object it has schematized. By applying priorprobabilities on what objects are expected at that time and place, the robot can instantiateonly the most likely subset of schemas. However if the priors do not match that particularcontext, none of the instantiated schemas will predict future interactions with the object,and thus the interpretation is in error.If the agent needs to disambiguate the type of object that caused the ambiguous sign,its course of action lies within the schemas. The instantiated alternative schemas are rep-resented in terms of expected outcomes of actions, and so the agent can choose to executeactions which predict maximally different outcomes for different object classes. For thedisc-ball-can problem, simply leaning over to obtain a view from a side perspective willsuffice.5.3. Situation schemasA situation is represented by linking schemas via their indexical elements. Fig. 13 showsthe schema corresponding to “There is a cup here. Something is touching the cup”. Onlythe handle categorical beliefs of the objects O1 and O2 are shown, along with their as-sociated indexical analog beliefs L1 and L2. I use the notational shortcut of the at linkto summarize object schemas by their handles and associated indexical analog belief. Noexpected outcome of O2 is specified, indicating a high entropy belief state with respect toO2’s type. A pair of categorizers projects beliefs about spatial relationships between L1and L2, and between L1 and L0. The projected categorical belief labeled contact servesas a situational constraint and encodes the belief that a contact relationship exists betweenL1 and L2.L0 is a default spatial indexical analog belief corresponding to “here”. L0’s domainspans the default spatial operating range of the agent which depends on the agent’s em-bodiment. A second spatial relation categorical belief encodes the belief that the cup iscontained within L0. For Ripley, L0 is the surface of a table in front of Ripley which is theonly area that Ripley is able to reach.To represent “the ball is in the cup”, the situational constraint between L1 and L2 ischanged to contain(L1, L2), a topological spatial relation. To reason about embedded in-dexical relationships during goal pursuit, relational constraints must be taken into account.For example, if the agent wishes to find the ball but can only see the cup, belief in acontainment or contact relationship between the ball’s indexical analog belief and the cup’sFig. 13. The situation corresponding to, “There is a cup here. Something is touching the cup.”D. Roy / Artificial Intelligence 167 (2005) 170–205193indexical analog belief support the inference that the ball will be found in the proximity ofthe cup.Clearly there is much more to be modeled with regards to spatial relations. The com-plexities of spatial terms such as “in” are well researched [28] and very detailed geometricmodels are required to capture spatial relations that depend not only relation locations ofobjects, but also their orientations and specific shapes [56,57]. Beyond modeling geometry,functional criteria also play a crucial role [16]. For example, an apple that is at the top ofa heaping bowl of fruit is still said to be “in” the bowl, even though it is not geometricallycontained by the bowl, because the bowl affords control over the location of the apple (ifthe bowl is moved, so is the apple). A promising future direction is to model the interactionof functional and geometric factors (see [11] for preliminary steps). For example, a mobilerobot could ground its understanding of “in the corner” in terms of how the corner restrictsthe robot’s potential motion. Such an approach would introduce constraints from motorcontrol to ground spatial language.5.4. Negation, disjunction, and explicit representationsCertain forms of negation are handled naturally in the proposed framework, others aremore problematic. In Ripley’s world, some objects can be seen but not touched becausethey are flat (e.g., photographs). The distinction between tangible visible objects and in-tangible yet visible objects is handled by replacing the success categorical belief projectedby detectHandContact (L) in Fig. 12 with fail, and by removing all outgoing edges fromthat categorical belief. In effect, the schema encodes the belief that the two kinds of ob-jects are identical except that for photographs, the haptic pathway is expected to fail. Theintangible object’s indexical analog belief L is refreshable only through visual verification.Difficult cases for handling negation arise from possible world semantics. For example,we might want to tell an agent that “there are no cups here”. This sort of negative de-scription is unnatural to represent in the approach I have outlined since the agent explicitlyinstantiates structures to stand for what it believes to be the case. The representation mightbe augmented with other forms of belief, perhaps explicit lists of constraints based on nega-tions and disjunctions which are compared against explicit models to look for conflicts, butthese directions are beyond the scope of this paper.Although the difficulty with existential negation and disjunction might seem to be a se-rious weakness, there is strong evidence that humans suffer from very similar weaknesses.For example, Johnson-Laird has amassed evidence that humans make numerous systematicerrors in dealing with existential logic that are neatly predicted by a theory of mental mod-els according to which humans generate specific representations of situations and reasonwith these explicit models even in cases where they know the models are overly specific[32]. Similar constraints on mental representations of machines may lead to a better “meet-ing of the minds” since systems that conceive of their environment in similar ways can talkabout them in similar ways. From a computational perspective, I believe my approach isclosely related to Levesque’s idea of “vivid representations”, which have difficulty deal-ing with certain classes of existential negation and disjunction for similar reasons [38].Levesque has argued that the choice of vivid representations is defendable when practicalconcerns of computational tractability are taken into account.194D. Roy / Artificial Intelligence 167 (2005) 170–205Fig. 14. The situation corresponding to, “The ball was in the cup. Now it is not”.5.5. Event schemasEvents are partial changes in situations. In Fig. 14, an indexical anchor for time bindsgroups of spatial analog beliefs from two situations at two different points in time (indi-cated by the two large rectangular frames). Temporal analog beliefs (T 1 and T 2) encoderegions along a one-dimensional local timeline exactly analogous to the two-dimensionalspatial domain for spatial indexicals in Ripley. A temporal categorizer temporalCatprojects the categorical belief after(T 2, T 1), specifying that the situation on the right fol-lows in time.In the initial situation at T 1, a ball is believed to be contained in a cup, which is con-tained in the default spatial domain. At some later time T 2, the containment relationbetween the ball and cup becomes invalid—the agent places zero belief in the outcomecontain(L1, L2). Only changes from T 1 are indicated in the situation for T 2—all otheraspects of the original situation are assumed unchanged. Like actions, events may be rep-resented at higher levels of abstraction to suppress “by means of” details, retaining onlyhigher level representations about changes of state. At one level the particular trajectoryof the motion of a cup might be specified, at a higher level only the before-after change inposition and orientation.6. Intentional signsThe representational foundations are finally in place to address the motivation behindthis entire theoretical construction: grounding language. Recall that there are three classesof signs. We have covered natural and indexical signs. The final class of signs, intentionalsigns, are used by agents for goal-driven communication.D. Roy / Artificial Intelligence 167 (2005) 170–205195Fig. 15. The structure of a grounded word.Speech acts are the canonical intentional sign. Viewed from a Gricean perspective [26],speech acts are chosen by rational agents in pursuit of goals. I say “the coffee is cold” toconvince you of that fact, and by Gricean implicature, to issue a directive to you to bringme hot coffee. Intentional signs are emitted by an agent into the environment. Like allsigns, intentional signs are physical patterns that stand for something to someone. In thecase of intentional signs, as opposed to natural and indexical signs, the link from sign tosignified is established by conventions agreed upon by a community of intentional signusers. Gestures such as pointing may also constitute intentional signs but are not addressedhere.Speech acts are assembled from lexical units (words and other elements of the lexicon)using a grammar. Since my focus will be on primitive descriptive and directive speech acts,sophisticated grammars are not needed, only basic rules that map serial order to and fromthematic role assignments. For this reason I will not say much more about parsing andgrammar construction here11 but instead simply assume the requisite primitive grammar isavailable to the agent.6.1. Lexical units (words)Fig. 15 shows the internal representational structure of a word. A fifth type of projectionis introduced in this figure, an intentional projection. This projection is an indicator to theagent that the sign projected by it, in this case the categorical belief labeled “cup”, is aconventional projection, i.e., one that can only be interpreted in the context of communica-tive acts. Intentional projections block interpretative processes used for natural signs sincehearing “cup” is not the same as seeing a cup. Hearing the surface form of the word thatdenotes cups will be interpreted differently depending on the speech act within which it isembedded in (consider “there is a cup here” versus “where is my cup?”).The domain of LEX in Fig. 15 is the discrete set of all lexical units known to the agent.The agent using the schema in Fig. 15 is able to convert discrete lexical units into sur-face forms in order to emit them into the environment through speak actions, hopefully inearshot of other agents attuned to speech through senseSpeech or functionally equivalentsensor projections. The speechCat categorizer has been implemented in our systems usingstandard statistical methods of continuous speech recognition using hidden Markov mod-els. To invert the process, a sixth and final type of projection is introduced. SpeechGen is a11 Elsewhere, we have explored the relationship between grammar acquisition and visual context [60,61] andthe interaction of visual context on parsing of text [25] and speech [65].196D. Roy / Artificial Intelligence 167 (2005) 170–205generator projection which produces an analog sign corresponding to a categorical belief.Since the mapping from analog to categorical signs is one-to-many, the inversion of thisprocess leaves room for variation. If categories are formed on the basis of thresholds on, orcompetition between prototypes, then prototypes are natural choices as outputs of generatorprojections. In previous work on visual-context guided word acquisition, we implementedword structures that are consistent with the schema in Fig. 15 in which speechGen andspeechCat shared acoustic prototypes of word surface forms [60].Word schemas can be connected to various schemas within an agent’s store of schematypes. I have made suggestions of ways by which schemas provide grounding for severalclasses of lexical units. We have seen examples of schemas for properties including visualproperty names (“red”, “round”), affordance terms (“soft”, “heavy”), spatial and temporalrelation labels (“touching”, “before”), verbs (“lift”, “move”), and nouns (“cup”, “thing”).In addition, the very notion of an individual arises from the act of instantiating and main-taining particular schemas, providing the basis for proper nouns and binding of indexicalterms (“that cup”, or more persistent proper names).6.2. Using speech actsAs a basic classification scheme for communicative acts, Millikan has suggested the dis-tinction between descriptive and directive acts [42]. Descriptives are assertions about thestate of the world and are thus akin to natural signs (assuming the speaker can be trusted).Directives are fundamentally different—they are requests for action (including questions,which are requests for information). A speech act may be both descriptive and directive.In the situation depicted in Fig. 2, “This coffee is cold” is a descriptive (it describes thetemperature of a particular volume of liquid) and perhaps also a directive (it may implya request for hot coffee). In systems we have constructed to date, only the more literalinterpretation of speech acts have been addressed, thus I will limit the following discus-sion to this simplest case. I first discuss how the framework handles directives, and thendescriptives.Directives are understood by agents by translating words into goals. The agent’s plan-ning mechanisms must then select actions to pursue those goals in context-appropriateways. This approach suggests a control-theoretic view of language understanding. If weview a goal-directed agent as a homeostasis seeking organism, directive speech acts aretranslated by the agent into partial shifts in goal states which effectively perturb the or-ganisms out of homeostasis. This perturbation causes the agent to act in order to regainhomeostasis.In our schema notation, a goal is represented using a dashed outline for the appropriateanalog or categorical distribution which the agent must satisfy in order to satisfy the goal.These may be called analog goals or categorical goals. A transition ending in a spreadingset of three rays (an iconic reminder that goals are reached for) connects the analog belief asit is currently believed to be to the desired target value. In Fig. 16, the agent has set the goalof changing the cup’s location such that the containment relation holds. This correspondsto the directive, “Put the cup on the plate”.Ripley understands limited forms of directives such as, “touch the bean bag on the left”,or, “pick up the blue one”. To perform the mapping from speech acts to goals, the output ofD. Roy / Artificial Intelligence 167 (2005) 170–205197Fig. 16. “Put the cup on the plate”.a small vocabulary speech recognizer is processed by a parser [25] which is integrated withRipley’s control system and mental model architecture [64]. In some cases, a directive willlead Ripley to collect additional information about its environment before pursuing the goalset by the directive. For example, if the robot is directed to “hand me the heavy one”, butthe weights of the objects in view are unknown, Ripley’s planning system uses the implicitcontrol structure of the schema underlying “heavy”12 to lift and weigh each candidateobject to determine which best fits the bill. Details of Ripley’s planning algorithms areforthcoming. There are of course many other kinds of directives, but in essence, I believetreating the comprehension of directives as a problem of translation into goal schemas is aproductive path forward (for another implementation along these lines see [33]).A higher order verbal behavior, one that we have not yet explored, is the generation ofdirective speech acts. To produce goal-directed directives in a principled way, the agentmust be able to plan with the use of instruments, and treat communication partners asinstruments who can be controlled by influencing their goals through speech acts. Thisin turn requires that the speaker have some degree of “theory of other minds” in order toreason about the goals and plans of other agents. This asymmetry between the cognitiverequirements of language understanding and language generation might in part explainwhy language comprehension always leads production in child language development.Understanding descriptive speech acts is treated in a similar vein as interpreting naturalsigns since both provide information about the state of the world. An interesting challengein understanding descriptive acts is the problem of under-specification in linguistic descrip-tions. “The cup is on the table” tells us nothing about the color, size, orientation, or preciselocation of the cup. Looking at a cup on the table seems to provide all of this information atfirst sight, although change blindness experiments demonstrate that even short term mem-ory encoding is highly goal-dependent (I might recall meeting someone yesterday and thetopic of our conversation, but not the color of her shirt). The framework allows for vari-ous forms of descriptive under-specification. For example, to express uncertainty of spatiallocation, belief can be spread with high entropy across the domain of an indexical analogbelief.Generation of descriptive speech acts, like generation of directives, also requires someability to maintain theories of other minds in order to anticipate effective word choices forcommunicating descriptions. The Describer system [61] uses an anticipation strategy toweed out descriptions of objects which the system predicts will be found ambiguous by12 The words “heavy” and “light” are grounded in active perception schemas similar to those for “soft” and“hard” shown in Fig. 10. Accumulation of joint forces during lifting project the weight of objects.198D. Roy / Artificial Intelligence 167 (2005) 170–205listeners. But this implementation barely scratches the surface of what eventually must bemodeled.There are numerous ideas which we could explore at this point ranging from context-dependency of word meanings (categorizers may receive bias shift signals from othercategorizers, for example, to differentiate heavy feathers from light elephants) to the def-inition of connotative meanings for an agent (as long term summary statistics of objects,properties, and actions in their likelihood to assist or block goals—heavy objects probablyblock more goals of a low powered manipulator whose goal is to move things around, sothe robot would develop a negative connotation towards the concept underlying “heavy”).Given our lack of specific implementations to flesh out such ideas, however, I will notattempt to elaborate further.7. Taking stockA summary of the elements of the theory provides a complete view of the frameworkdeveloped thus far:(1) Three classes of signs, natural, indexical, and intentional, carry different kinds ofinformation for agents.(2) Agents hold beliefs about analog signs (analog beliefs), and beliefs about discretecategories (categorical beliefs).(3) Six types of projections (sensors, actions, transformers, categorizers, intentional pro-jections, and generators) link beliefs to form schemas. Sensor and action projectionsare transducers that link schemas to physical environments.(4) Schemas may use parameters to control actions.(5) Objects are represented by networks of interdependent schemas that encode proper-ties and affordances. Object schemas subsume property and action schemas.(6) Using schemas, an agent is able to interpret, verify, and guide actions towards objects,object properties, spatiotemporal relations, situations, and events.(7) Lexical units are pairs of analog beliefs (encoding surface word forms) and categor-ical beliefs (encoding lexical unit identity) connected to defining schemas throughintentional projections.(8) Speech acts are intentional signs constructed from lexical units.(9) Two kinds of intentional signs, descriptive and directive, are used to communicate.(10) Directive speech acts are interpreted into goal schemas that an agent may choose topursue.(11) Descriptive speech acts are interpreted into existential beliefs represented throughschemas which are compatible with (and thus may be verified and modified by) sens-ing and action.In my introductory remarks I highlighted the referential-functional duality of linguisticmeaning. I defined grounding to be a process of predictive-causal interaction with thephysical environment. Finally, I proposed three requirements for any theory of languagegrounding. Let us briefly review how the theory addresses these points.D. Roy / Artificial Intelligence 167 (2005) 170–205199Both aspects of meaning are addressed to some degree in the framework: (1) Words areabout entities and situations in the world. Words project to schemas which are constructedout of beliefs about signs, and signs are about the world due to causal physical laws. Thechoice of a word’s surface form is arbitrary and conventional, but the underlying mappingof its categorical belief is shaped by causal-predictive interactions with the environment.Language use is situated via indexical beliefs constructed in the process of using language.(2) Agents use language to pursue goals. Since all schemas may serve as guides for con-trolling action, and words are defined through schemas, the very representational fabric ofword meanings may always be viewed from a functional perspective.Schemas are networks of beliefs. Beliefs are both memories of what has transpired,and also predictions of what will transpire (contingent on action). This dual use of beliefstructures satisfies the predictive-causal definition of the grounding process provided inSection 2.Finally, we may assess the framework with respect to the three requirements proposedin Section 3:(1) Unification of representational primitives: Objects, properties, events, and higher levelstructures are all constructed from a unified set of analog beliefs, categorical beliefs,and six types of projections.(2) Cross-modal translatability: Natural signs, indexical signs, and intentional speech actsare interpreted into schemas. Directive speech acts are interpreted as goal schemas.Descriptive speech acts (which are often vague when compared to perceptually de-rived descriptions) are interpreted into compatible schematized belief structures. Inother words, speech acts (intentional signs) are translated into the same representa-tional primitives as natural and indexical signs.(3) Integrated space of actions: Although not explored in this paper, the framework lendsitself to decision theoretic planning in which the costs and expected payoffs of speechacts and motor acts may be fluidly interleaved during goal pursuit.8. Social belief networksIn Section 2 I gave a relatively stringent definition of grounding that requires the be-liever to have direct causal-predictive interaction with the physical subjects of its beliefs.The theoretical framework I have developed does just this—it provides structured repre-sentations of various concepts underlying words and speech acts that are grounded strictlyin sensory-motor primitives. But of course most of what we know does not come from firsthand experience—we learn by reading, being told, asking questions, and in other wayslearning through intentional signs. I argued that to make use of symbolically describedinformation, an agent needs an independent path to verify, acquire, and modify beliefswithout intermediaries. Building on this, social networks may collectively ground knowl-edge that not all members of community can ground. I depict such networks of beliefamongst agents in Fig. 17. Everything we have discussed thus far may be denoted by thegraph on the left. It shows a single agent that holds the belief B(x) about the world. Theworld (denoted as the rectangle with a electrical ground sign at bottom) indeed contains x200D. Roy / Artificial Intelligence 167 (2005) 170–205Fig. 17. Social belief networks.and causally gives rise to B(x) as indicated by the upwards arrow. The downward arrowfrom the agent back to the world denotes that the agent has the ability to verify B(x) byinteracting directly with the physical environment.The right panel of Fig. 17 shows a community of four agents. Only Agent 1 has fulland direct grounded beliefs in x. Agent 2 came to know about x through intentional signstransmitted from Agent 1. Agent 2’s only way to verify x is to ask Agent 3. Agent 3 alsolearned of x from Agent 1, but is able to verify by asking either Agent 1 or Agent 4. Thiskind of graph is reminiscent of Putnam’s linguistic division of labor [54] in which an expertabout x (Agent 1) grounds beliefs about x on behalf of others in the belief network. Theclaim I began with is that there exists some basic set of concepts about the world we allshare which each agent must ground directly for itself, and that language uses these sharedconcepts to bootstrap mediated networks such as the right side of Fig. 17. The ubiquitoususe of physical metaphor in practically all domains of discourse across all world languages[36] is a strong indication that we do in fact rely on physical grounding to as the basis ofconceptual alignment underlying symbolic communication.9. Related ideasThe theory I have presented brings together insights from semiotics (the study of signs)dating back to Peirce with schema theory dating back to Kant. There is a great deal of priorwork on which the theory rests. Rather than attempt a comprehensive survey, I highlightselected work that is most closely related and that I have not already mentioned elsewherein the paper.Perhaps the most well known early work in this area is the SHRDLU system constructedby Winograd [76]. This work demonstrated the power of tight integration of languageprocessing within a planning framework. A key difference in Winograd’s work was theassumption that the language user has a complete, symbolically described world model(blocks on a table top in the case of SHRDLU). The issue of word-to-physical-world con-nectedness was not a concern in Winograd’s work. As a result, his approach does not lead inany obvious way to the construction of physically grounded language systems. Categoriza-tion is not addressed in SHRDLU whereas a central aspect of the approach I have describedD. Roy / Artificial Intelligence 167 (2005) 170–205201here is the processes by which analog signs are transformed into categorical signs, and theresulting distinction between analog and categorical beliefs. Many of Winograd’s insightson the interaction of planning, dialog, and reference, however, remain highly relevant forthe theory I have presented here, and indeed complement the issues I have addressed. Overa decade after SHRDLU, Winograd and Flores [77] wrote a critique of symbolic AI in-cluding the methods employed by SHRDLU. The gist of this critique is to point out theinterpretive “sleight of hand” that tends to underlie symbolic AI systems such as SHRDLU,and the ascriptive errors AI practitioners made in using terms such as “understanding” todescribe such systems (see also [53]). A key reason for this ascription error was that thesystems were unable to link symbols to their physical environment without a human in theloop. In contrast, grounded language systems address this limitation.Minsky’s conception of frames [43] is similar in spirit to my approach. Frames are datastructures that represent stereotyped situations, and are instantiated to interpret experiencedsituations much as I have suggested the role of schemas here. Minsky suggests frames as astructure for interpretation, verification, and control as I have for schemas. Minsky’s papercovered a far wider range of domains, and thus naturally provided less specific details onany one domain. In contrast, the theory I have outlined is focused specifically on questionsof language grounding and reflects specific structures that arose from a concerted effort tobuild language processing systems.Schank and Abelson [68] developed a theory of scripts which are organizing knowledgestructures used to interpret the meaning of sentences. Scripts are highly structured repre-sentations of stereotyped situations such as the typical steps involved in eating a meal at arestaurant. Scripts are constructed from a closed set of 11 action primitives but an open setof state elements. For example, to represent the stereotyped activities in a restaurant script,representational state primitives include hungry, menu, and where-to-sit. In contrast, I havesuggested a theory which avoids open sets of symbolic primitives in favor of a closed setof embodiment-dependent primitives.Several strands of work by cognitive scientists and linguists bear directly on the topics Ihave discussed. Bates and Nelson have proposed constructivist analyses of early languagedevelopment [9,46]. The computational framework presented here is compatible with bothof their approaches. Miller and Johnson-Laird compiled perhaps the most comprehensivesurvey to date of relationships between language and perception [41]. Barsalou’s percep-tual symbol system proposal [7] stresses the importance of binding symbols to sensory-motor representations, as evidenced by recent experiments that probe the embodied natureof cognitive processes [24,73]. Barsalou’s proposal emerged from human behavioral exper-iments as opposed to construction of systems, and as a result provides a more descriptiveaccount in contrast to the computational level of explanation I have attempted here. Jack-endoff [31] presents a compelling view on many aspects of language that have influencedmy approach, particularly his ideas on “pushing the world into the mind”, i.e., treatingsemantics from a subjective perspective.Some noteworthy approaches in the robotics community are closely related to the useof schemas I have proposed. Kuipers’ Semantic Spatial Hierarchy suggests a rich multi-layered representation for spatial navigation [35]. This representation provides a basis forcausal-predictive grounding in spatial domains which I believe might be of great value forgrounding spatial language. Grupen’s work on modeling affordances [13] intermingles ob-202D. Roy / Artificial Intelligence 167 (2005) 170–205ject and action representations and also deserves further study from a language groundingperspective.Bailey [5] and Narayanan [44] propose the use of modified forms of Petri nets (a formal-ism used to model concurrent, asynchronous control flow in networks) to model schema-like structures underlying natural language verbs. Bailey’s representation of manipulationactions is similar to ours (Bailey’s implementations were based on a simulated robot arm).Narayanan used modified Petri nets as a basis for understanding abstract economic newsstories by analogy to underlying physical action metaphors (e.g., “the economy hit rockbottom”). Siskind [70] proposed an approach to modeling perceptually grounded repre-sentations underlying manipulation verbs by combining force dynamics primitives withAllen’s temporal relations [1]. The representation of events proposed by Bailey, Narayanan,and Siskind are all able to model more complex event structures than the approach I havepresented here based on sequences of situation schema. However, my approach providesa holistic account for actions and other ontological categories such as objects, properties,and spatial relations, whereas these other approaches focus only on event structure. An in-teresting direction would be to investigate ways to incorporate the more expressive powerof Petri nets or Siskind’s representation to augment the schema structure while retainingthe holistic nature of the framework I have presented.Littman, Sutton and Singh [39] have proposed the idea of predictive representations ofstate through which states of a dynamical system are represented as “action conditionalpredictions of future observations”. The exact relationship between those ideas and theones I have presented will require detailed study, but it seems to be very similar in spirit ifnot formulation. Also closely related is Cohen’s work with robots that learn “projectionsas concepts” [14] which have been linked to linguistic labels leading to a limited form oflanguage grounding [15].10. Meaning machinesThere are many important questions that this framework raises that I have not begunto address. Where do schemas made of analog beliefs, categorical beliefs, and projectionscome from? How and to what extent can their structure and parameters be learned throughexperience? How might hierarchical structures be used to organize and relate schemas?What kind of cognitive architecture is needed to maintain distinct schematic beliefs anddesires? How does an agent perform efficient inference and planning with them? How areabstract semantic domains handled? How are higher level event, action, and goal structuresorganized to support more sophisticated forms of inference and social interaction? Theseare of course challenging and deep questions that point to the immense number of futuredirections suggested by this work.The framework introduced in this paper emerged from the construction of numerousgrounded language systems that straddle the boundary of symbolic and non-symbolicrealms. In contrast to models that represent word meaning with definitions made of word-like symbols, I have taken a semiotic perspective with the intent of unifying language,perception, and action with a small number of representational primitives. Systems imple-D. Roy / Artificial Intelligence 167 (2005) 170–205203mented according to this framework transduce the physical world of patterns and into aninner “mental world” of beliefs that are structured to support linguistic communication.Although most language in adult conversation does not refer to the concrete physicalworld, is motivated my focus on concrete semantics by two main observations about hu-man communication. First, children bootstrap language acquisition by conversing abouttheir immediate environment—human semantics is physically anchored. Second, a sharedexternal reality, revealed to agents through physical patterns, is the only way to explainwhy conceptual systems are aligned across agents to any degree at all, and thus why wecan communicate with one another. If we are going to bring machines into our conceptualand conversational world as autonomous agents that understand the meaning of words forand by themselves—that truly mean what they say—grounding will play a central role.AcknowledgementsI thank Angelo Cangelosi, Cristiano Castelfranchi, Gary Drescher, Michael Fleischman,Peter Gorniak, Kai-Yuh Hsiao, Nikolaos Mavridis, Andre Ribeiro, Whitman Richards,Aaron Sloman, Stefanie Tellex, Seth Yalcin, and anonymous reviewers for comments onearlier drafts of this paper, and Daniel Dennett, Allen Gorin, Ray Jackendoff, MarvinMinsky, and Rich Sutton for numerous helpful discussions (and arguments!) on the ideaspresented here.References[1] J. Allen, Maintaining knowledge about temporal intervals, Comm. ACM 26 (1983) 832–843.[2] M.A. Arbib, Schema theory, in: M.A. Arbib (Ed.), The Handbook of Brain Theory and Neural Networks,second ed., MIT Press, Cambridge, MA, 2003, pp. 993–998.[3] M.A. Arbib, T. Iberall, D. Lyons, Schemas that integrate vision and touch for hand control, in: Vision, Brain,and Cooperative Computation, MIT Press, Cambridge, MA, 1987, pp. 489–510.[4] D. Bailey, J. Feldman, S. Narayanan, G. Lakoff, Embodied lexical development, in: Proceedings of theNineteenth Annual Meeting of the Cognitive Science Society, Erlbaum, Mahwah, NJ, 1997.[5] D. Bailey, When push comes to shove: A computational model of the role of motor control in the acquisi-tion of action verbs, PhD thesis, Computer science division, EECS Department, University of California atBerkeley, 1997.[6] K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D. Blei, M. Jordan, Matching words and pictures, J. Ma-chine Learning Res. 3 (2003) 1107–1135.[7] L. Barsalou, Perceptual symbol systems, Behavioural and Brain Sciences 22 (1999) 577–609.[8] J. Barwise, J. Perry, Situations and Attitudes, MIT-Bradford, 1983.[9] E. Bates, The Emergence of Symbols, Academic Press, New York, 1979.[10] M.H. Bickhard, Representational content in humans and machines, J. Experimental Theoret. Artificial Intel-ligence 5 (1993) 285–333.[11] A. Cangelosi, K. Coventry, R. Rajapakse, D. Joyce, A. Bacon, L. Richards, S. Newstead, Groundinglanguage in perception: A connectionist model of spatial terms and vague quantifiers, in: Ninth NeuralComputation and Psychology Workshop, Plymouth, UK, 2004.[12] A. Cangelosi, S. Harnad, The adaptive advantage of symbolic theft over sensorimotor toil: Grounding lan-guage in perceptual categories, Evolution of Communication, 2000, 2001.[13] J. Coelho, J. Piaterand, R. Grupen, Developing haptic and visual perceptual categories for reaching andgrasping with a humanoid robot, in: Proceedings of the First IEEE-RAS International Conference on Hu-manoid Robots, IEEE-RAS, 2000.204D. Roy / Artificial Intelligence 167 (2005) 170–205[14] P. Cohen, Projections as concepts, in: Proceedings of the 2nd European Conference on Cognitive Science,1997, pp. 56–60.[15] P. Cohen, T. Oates, C. Beal, and N. Adams, Contentful mental states for robot baby, in: Proceedings of theEighteenth National Conference on Artificial Intelligence, Edmonton, AB, 2002.[16] K. Coventry, S. Garrod, Saying, Seeing and Acting, Psychology Press, 2004.[17] D. Dennett, Consciousness Explained, Little, Brown and Company, 1991.[18] D. Dennett, Real patterns, J. Philos. 88 (1) (1991) 27–51.[19] G. Drescher, Made-up Minds, MIT Press, Cambridge, MA, 1991.[20] F.I. Dretske, Knowledge and the Flow of Information, MIT Press, Cambridge, MA, 1981.[21] J. Feldman, G. Lakoff, D. Bailey, S. Narayanan, T. Regier, A. Stolcke, Lzero: The first five years, ArtificialIntelligence Rev. 10 (1996) 103–129.[22] P. Gärdenfors, Conceptual Spaces: The Geometry of Thought, MIT Press, Cambridge, MA, 2000.[23] J.J. Gibson, The Ecological Approach to Visual Perception, Erlbaum, Mahwah, NJ, 1979.[24] A. Glenberg, M. Kaschak, Grounding language in action, Phychonomic Bull. Rev. 9 (3) (2002) 558–565.[25] P. Gorniak, D. Roy, Grounded semantic composition for visual scenes, J. Artificial Intelligence Res. 21(2004) 429–470.[26] P. Grice, Logic and conversation, in: P. Cole, J. Morgan (Eds.), Syntax and Semantics: vol. 3, Speech Act,Academic Press, New York, 1975, pp. 43–58.[27] S. Harnad, The symbol grounding problem, Physica D 42 (1990) 335–346.[28] A. Herskovits, Language, spatial cognition, and vision, in: Spatial and Temporal Reasoning, Kluwer Acad-emic, Dordrecht, 1997, Chapter 6.[29] G. Herzog, P. Wazinski, VIsual TRAnslator: Linking perceptions and natural language descriptions, Artifi-cial Intelligence Rev. 8 (1994) 175–187.[30] D. Isla and B. Blumberg, Object persistence for synthetic creatures, in: Proceedings of the International JointConference on Autonomous Agents and Multiagent Systems, 2002.[31] R. Jackendoff, Foundations of Language, Oxford University Press, Oxford, 2002.[32] P.N. Johnson-Laird, Mental Models: Towards a Cognitive Science of Language, Inference, and Conscious-ness, Cambridge University Press, Cambridge, 1983.[33] J. Juster, D. Roy, Elvis: Situated speech and gesture understanding for a robotic chandilier, in: Proceedingsof the International Conference on Multimodal Interfaces, 2004.[34] I. Kant, Critique of Pure Reason, Section A137, Cambridge University Press, Cambridge, 1781, 1998.[35] B. Kuipers, The spatial semantic hierarchy, Artificial Intelligence 119 (2000) 191–233.[36] G. Lakoff, M. Johonson, Metaphors We Live By, University of Chicago Press, Chicago, 1980.[37] D. Lenat, Cyc: A large-scale investment in knowledge infrastructure, Comm. ACM 38 (11) (1995) 33–38.[38] H.J. Levesque, Making believers out of computers, Artificial Intelligence 30 (1) (1986) 81–108.[39] M. Littman, R. Sutton, S. Singh, Predictive representations of state, in: Advances in Neural InformationProcessing Systems, MIT Press, Cambridge, MA, 2002.[40] G. Miller, Wordnet: A lexical database for English, Comm. ACM 38 (11) (1995) 39–41.[41] G. Miller, P. Johnson-Laird, Language and Perception, Harvard University Press, Harvard, 1976.[42] R.G. Millikan, Varieties of Meaning, MIT Press, Cambridge, MA, 2004.[43] M. Minsky, A framework for representing knowledge, in: P. Winston (Ed.), The Psychology of ComputerVision, McGraw-Hill, New York, 1975, pp. 211–277.[44] S. Narayanan, Moving right along: A computational model of metaphoric reasoning about events, in: Pro-ceedings of the National Conference on Artificial Intelligence AAAI-99, Orlando, FL, 1999.[45] U. Neisser, Cognition and Reality, Freeman, New York, 1976.[46] K. Nelson, Concept, word, and sentence: Interrelations in acquisition and development, PsychologicalRev. 81 (1974) 267–285.[47] C.K. Odgen, I.A. Richards, The Meaning of Meaning, Harcourt, 1923.[48] C.S. Peirce, How to make our ideas clear, Popular Science Monthly 12 (1878) 286–302.[49] C.S. Peirce, Logic as semiotic: The theory of signs, in: Philosophical Writings of Peirce, Dover, 1897, 1940.[50] J. Piaget, The Construction of Reality in the Child, Ballentine, 1954.[51] K. Plunkett, C. Sinha, M.F. Moller, O. Strandsby, Symbol grounding or the emergence of symbols? vocab-ulary growth in children and a connectionist net, Connection Sci. 4 (3–4) (1992) 293–312.D. Roy / Artificial Intelligence 167 (2005) 170–205205[52] K. Hsiao, N. Mavridis, D. Roy, Coupling perception and simulation: Steps towards conversational robotics,in: Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, Las Vegas, 2003.[53] D. McDermott, Artificial intelligence meets natural stupidity, in: J. Haugeland (Ed.), Mind Design, MITPress, Cambridge, MA, 1981, pp. 143–160, Chapter 5.[54] H. Putnam, The meaning of ‘meaning’, in: Philosophical Papers: vol. 2. Mind, Language and Reality, Cam-bridge University Press, Cambridge, 1975, pp. 215–271.[55] D.A. Randell, Z. Cui, A.G. Cohn, A spatial logic based on regions and connection, in: Proceedings 3rdInternational Conference on Knowledge Representation and Reasoning, Morgan Kaufmann, San Mateo,CA, 1992, pp. 165–176.[56] T. Regier, The Human Semantic Potential, MIT Press, Cambridge, MA, 1996.[57] T. Regier, L. Carlson, Grounding spatial language in perception: An empirical and computational investiga-tion, J. Experimental Psychol. 130 (2) (2001) 273–298.[58] E. Reiter, S. Sripada, Human variation and lexical choice, Computational Linguistics 22 (2002) 545–553.[59] A. Rosenblueth, N. Wiener, J. Bigelow, Behavior, purpose and teleology, Philos. Sci. 10 (1943) 18–24.[60] D. Roy, Learning words from sights and sounds: A computational model, PhD thesis, Massachusetts Instituteof Technology, 1999.[61] D. Roy, Learning visually-grounded words and syntax for a scene description task, Computer Speech andLanguage 16 (3) (2002) 353–385.[62] D. Roy, Grounding words in perception and action: Computational insights, Trends Cognitive Sci. 9 (8)(2005).[63] D. Roy, P. Gorniak, N. Mukherjee, J. Juster, A trainable spoken language understanding system for visualobject selection, in: International Conference of Spoken Language Processing, 2002.[64] D. Roy, K. Hsiao, N. Mavridis, Mental imagery for a conversational robot, IEEE Trans. Systems Man Cy-bernet. B 34 (3) (2004) 1374–1383.[65] D. Roy, N. Mukherjee, Towards situated speech understanding: Visual context priming of language models,Computer Speech and Language 19 (2) (2005) 227–248.[66] D. Roy, A. Pentland, Learning words from sights and sounds: A computational model, Cognitive Sci. 26 (1)(2002) 113–146.[67] D. Rumelhart, A. Ortony, The representation of knowledge in memory, in: R.C. Anderson, R.J. Spiro, W.E.Montague (Eds.), Schooling and the Acquisition of Knowledge, Erlbaum, Hillsdale, NJ, 1977, pp. 99–136.[68] R. Schank, R. Abelson, Scripts, Plans, Goals and Understanding, Erlbaum, Hilsdale, NJ, 1977.[69] J. Siskind, Naive physics, event perception, lexical semantics, and language acquisition, PhD thesis, Massa-chusetts Institute of Technology, 1992.[70] J. Siskind, Grounding the lexical semantics of verbs in visual perception using force dynamics and eventlogic, J. Artificial Intelligence Res. 15 (2001) 31–90.[71] A. Sloman, J. Chappell, The altricial-precocial spectrum for robots, in: Proceedings of IJCAI, 2005.[72] B.C. Smith, On the Origin of Objects, MIT Press, Cambridge, MA, 1996.[73] M. Spivey, M. Tyler, D. Richardson, E. Young, Eye movements during comprehension of spoken scenedescriptions, in: Proceedings of the 22nd Annual Conference of the Cognitive Science Society, Erlbaum,Mahwah, NJ, 2000, pp. 487–492.[74] L. Steels, P. Vogt, Grounding adaptive language games in robotic agents, in: C. Husbands, I. Harvey (Eds.),Proceedings of the 4th European Conference on Artificial Life, MIT Press, Cambridge, MA, 1997.[75] B.E. Stein, M.A. Meredith, The Merging of the Senses, MIT Press, Cambridge, MA, 1993.[76] T. Winograd, A Process Model of Language Understanding, Freeman, New York, 1973, pp. 152–186.[77] T. Winograd, F. Flores, Understanding Computers and Cognition, Addison-Wesley, Reading, MA, 1986.[78] L. Wittgenstein, The Blue and Brown Books, Basil Blackwell, Oxford, 1958.