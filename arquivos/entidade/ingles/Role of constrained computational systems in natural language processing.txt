Artificial Intelligence 103 (199X) 117-132 Artificial Intelligence Role of constrained computational systems in natural language processing * Depurtment of Computer and Information Science and Institute for Research in Cognitive Science, Room 555, Moore School, UniversiQ of Pennsylvania, Philadelphia, PA 19104, USA Aravind K. Joshi ’ Abstract and discourse, The use of constrained among others, has proved formal/computational semantics, pragmatics systems just adequate for modeling various aspects to be an of language-syntax, to both effective research strategy leading to deep understanding of these aspects, with implications machine processing and human processing. This approach enables one to distinguish between the universal and stipulative constraints. This is in contrast to an approach where we start with the most the phenomena by making all constraints powerful in a sense. The use of constrained stipulative for modeling describing locality of structures and brings out the relationship between the complexity of description of primitives and local computations over them. These ideas serve to unify theoretical, computational and statistical aspects of natural languages processing productive in other domains of AI. 0 1998 Elsevier Science B.V. All rights reserved. leads to some novel ways of that this approach will be system and then model formal/computational in AI. It is expected systems Keywords: Abstract character of adjoining; Adjoining; Almost parse; Centering; Center of an utterance; Complexity of inference; Control of inference; Constrained grammars; Local computations on complex structures; Local statistical tree-adjoining grammars; Lexicalized computations; Locality of structures; Locally monadic structure; Monadic predicate; Substitution; Supertags; Supertagging; Universal and stipulative constraints formal systems; Finite state transducers; Lexicalized This paper is a somewhat expanded version of my IJCAI-97 Research Excellence lecture. It is not a survey of the field of natural Award It is not even a survey of all of my own work. I will focus on only a few topics which in a significant way. The illustrate influenced my own work an approach processing. that has language ” This work is partially supported by NSF Grant SBR8920230 ’ Email: joshi@linc.cis.upenn.edu. and AR0 Grant DAAHO404-94-GE-0426 00043702/98/$ PII: SOOO4-3702(98)00065-4 -see front matter 0 1998 Elsevier Science B.V. All rights reserved 118 A.K. Joshi /Arti$icicd Intrlligcwce IO;1 (199X) I17-I.12 various systems set of ideas for describing as the aspects of theses efforts could best be described to starting with the most general and most that characterize formal/computational semantics, pragmatics, discourse, among others. The use of constrained is in sharp contrast systems and then making all constraints, necessary for description, particular use of constrained language-syntax, systems computational powerful computational stipulative between in a sense. The use of such systems allows one to distinguish universal and stipulative constraints. Universal constraints are properties of languages are (or claimed properties which may be languages particular are then stipulative. tries to capture On the other hand constraints a constrained is not achievable at present and perhaps may never be achievable. However, goal of the constrained in computational modeling of language and has given deep insights techniques. languages. Moreover, the that to be) universal across languages and not language particular. All other In this approach one itself. system all Ideally we want this this is the systems approach. This approach has proved to be quite successful that captures all and only the universal properties. Of course, in the approach where in the descriptions it has also led to efficient processing as properties of the constrained are stipulative by definition. the universal properties introduced system into the structure of is unconstrained the underlying system that fall under the characterization of the approach Here are five topics of my research formal/computational using constrained (1) Cascaded finite state transducers (2) Lexicalized grammars-lexicalized (3) Some aspects of bilingual processing. (4) Computation entailments. of certain classes of inferences, systems. for parsing. tree-adjoining grammars (LTAG). for example, presupposition and (5) Local structure of discourse+entering. 1 will only discuss three of these items, will serve to illustrate the main point of my talk-the systems. items 1, 2, and 5. These examples, I hope, role of constrained computational 1. Cascaded finite state transducers My first topic or example is the use of finite state transducers It also happens to be the very first work I did in natural languages processing. As far as I know this is the first use of fst’s for parsing. This work (carried out during the period 1958-1959) was part of a project called Transformations and Discourse Analysis Project (TDAP), directed by Professor Zellig Harris at the University of Pennsylvania. (fst) for parsing. ’ ‘The other participants of this project were Lila Gleitman, Bruria Kauffman, Naomi Sager, and Carol this program has been recently faithfully collaboratively with Phil Hopely. Two papers based on this work have also appeared Chomsky. By a remarkable coincidence documentation, A.K. Joshi, P. Hopely, A parser from antiquity, Natural Language Engineering 2 (4) (1997). An extended version of this paper which for example, Wall Street Journal (WSJ), IBM Computer Manuals, and ATIS (several modern parsers have been evaluated on these corpora also) will appear in: A. Kornai (Ed.), Extended Finite State Automata, Cambridge University Press, 1998. includes an evaluation of this parser on some corpora, from the OrigiIId recently- reconstructed A. K. Joshi /Arti$cial Inrelligence IO3 (1998) 117-132 119 The fst parser consists of a cascade of finite state transducers corresponding to the following computations: . dictionary look-up and computation clusters which behave as a single part-of-speech; of the so-called grammatical idioms, i.e., word l part-of-speech disambiguation; . computation of simple noun phrases, prepositional phrases, and verb clusters; . computation of clauses (strictly not an fst computation). Rather than describing these computations 1 will give an example. which is an actual (of [either (also) output from the original program. [We] (have found} / that [subsequent addition] (of [the second inducer]) (to proceed] + > (for [ 15 minutes]) system]) < after (allowing} {results] Here (in [increased [. .] denotes a simple noun phrase, [single induction] reproduction]) + \ + (of [both enzymes]). (. .) denotes a simple adjunct, and (. . .} denotes a verb cluster. Both < . . > and / . . \ denote clauses, + denotes the end of a verb and part-of- complement. After the dictionary look-up, grammatical speech disambiguation, from phrases by a left to right fst and the verb clusters by right to left, then the prepositional left to right fst. The computation of clauses is done by a pushdown store with a depth first strategy. the simple noun phrases are computed by an fst scanning idioms computation, There are several reasons for mentioning computational this very early work. First fst’s are an example fst’s are once again playing role in natural languages processing and many of the techniques used in system but, more importantly, of a constrained a very significant this early work have close connections of fst technology finite state calculi, determinization weighted fst’s. Some of the key efforts in this area are Koskenniemi (1996), Hobbs et al. (1992) and Mohri et al. (1997). ’ in natural the new techniques and, of course, techniques for handling enormous language processing and minimization, is due to our substantial knowledge of sizes of fst’s and for their stochastic and for handling et al. (1992), Karttunen to some very recent work on fst’s. This resurgence are an example of a constrained Finite state transducers an example of a computational descriptions can be complex. So it is an example (no doubt a simple one) of a computation which I call local computation on complex structures. 1 will return to this theme repeatedly. system but they also serve as system which is finite state (thus local) but where the state 2. Lexicalized tree-adjoining grammars Now I will turn to my second example-lexicalized grammars, which are also examples systems. A lexicalized grammar consists of a finite set of constrained computational ’ K. Koskenniemi, P. Tapanainen, A. Voutilainen. Compiling and using finite-state in: Proc. in Computational Linguistics, COLING-92, Vol. I, 1992, Nantes, France, pp. 156 15th International Conference for Computational 162. I,. Karttunen, Directed Linguistics, ACL-96, Santa Crua, 1996. J.R. Hobbs. D.E. Appelt. J.S. Bear, D. Israel, W.M. Tyson, FAUSTUS: a system for extracting text, Technical Note, SRI International, Menlo Park, 1992. M. Mohri. F. Pereira, M. Riley, Rational power series in text and speech processing, Lecture Notes. AT&T Laboratories, Murray Hill, 1997. in: Proc. 34th Annual Meeting of the Association from a natural language replacement, information syntactic rules, 120 A.K. Joshi/Art$cial Intelligence 103 (199X) 117-132 Fig. 1. LTAG substitution. structures (for example, of elementary trees, or directed acyclic graphs), each strings, structure associated with a lexical anchor. Each anchor may have more than one associated structure. Further i.e., languages in a sense, grammar and lexicon independent. Thus in a lexicalized grammar, are the same thing, in other words Grammar = Lexicon. there is a finite set of composition operations, which are universal, A particular example of a lexicalized grammar 4 is the Lexicalized Tree-Adjoining (LTAG), where each each lexical item is associated with one or more elementary syntactic and semantic trees localize the domain and adjoining. long distance dependencies within operations are substitution associated with the lexical anchor. 5 In LTAG the elementary tree encapsulates the so-called including trees. The two composition is the obvious operation-substituting is a more complex operation-splicing Grammar trees (or directed acyclic graphs). Each elementary information all dependencies of the elementary Substitution Adjoining In Fig. 1 the tree /I is is substituted in the tree y. In Fig. 2 the tree B with root node labeled X is spliced into, or adjoined These two composition operations are language grammatical then consists of a specification of this finite set of elementary is contained information a tree at a frontier node of another tree. tree. a tree into the interior of another at the node X on the frontier of the tree a resulting labeled X and a frontier node also in the tree y. independent. Thus the entire linguistic or theory into, the tree cr at the node X, resulting in the set of elementary trees. The linguistic structures. The two operations-substitution and adjoining, both grow trees. Substitution grows It is the operation of ad- them at the frontier and adjoining grows them in the interior. allows localization of joining and it allows modification of already dependencies built structures, an aspect to which I will return later. Some examples of syntactic depen- dencies are: (1) agreement: person, number, gender, for example, that distinguishes LTAG from all other systems. Adjoining long distance dependencies (2) subcategorization: including 4 Another Grammars Cambridge, 1997. important example of Steedman, Surface Structure is the Categorial Grammars, and Interpretation, in particular: M. Steedman, Combinatory Categorial Inquiry Monograph, MIT Press, Linguistic s My early collaborators in this work were Leon Levy and Masako Takahashi. My later collaborators are K. Vijayshanker, Anthony Kroch, David Weir, Yves Schabes, and Ann Abeille. A.K. Joshi/Art~$cial Intelligence 103 (1998) I1 7-132 121 X B: A X Treep adjoined to treea at the node labeled X in the tree a. Fig. 2. LTAG adjoining. hit requires NP (noun phrase) complement, for example, (3) filler-gap dependen- different verbs take different complements, give requires NP NP, think requires NP S (sentence), cies: who(i) did John ask Bill to invite (i) where there is dependency between who and the complement of invite, which can be at an arbitrary distance, who is the filler of the gap that appears after invite, (4) word-order variation within and across clauses. Some examples of the lexical anchor is treated as a functor semantic dependencies tree, (5) word-clusters: and then all its are arguments are localized within an elementary to; the noncompositional flexible aspects are localized within the elementary semantic aspects-these as the dependencies between a verb and the head nouns of its subject and complements. are, of course, directly related to the statistical dependencies take a walk, give a cold shoulder trees, and (6) word co-occurrences and lexical such are: (I) function-argument: for example, idioms, Fig. 3 shows some highly simplified representations6 tree al of two corresponds like. The the word associated with and the tree a2 corresponds structures construction there will be many other trees associated with like, for example, topicalization, elementary shows more examples of elementary and the associated semantic dependencies have been localized. tree for every ‘minimal” to the object-extraction syntactic construction relative, object subject relative, passive, etc. In fact there will be an in which likes can appear. Fig. 4 trees. It is easy to see that in each tree the syntactic lexically anchored to the transitive construction. Of course, for subject-extraction, In Figs. 5 and 6 a very simple example of a derivation trees that enter the derivation are shown in Fig. 5 and the derivation of who does Bill think Harry likes is shown in Fig. 6. is presented. The elementary We start with the tree a2 corresponding The trees for who and Harry are substituted in a tree corresponding resulting to likes in the object-extraction in a2 as shown (by solid lines with arrows), likes. The trees for Bill and think are construction. to who Harry ’ In the actual grammar each node is decorated with feature structures with possible co-indexing for the values of features across the nodes of the tree. 122 A. K. Joshi /Artijiciul Intelligence 103 (1998) I/ 7-132 S A NP(i) # S A NP’/’ VP /\ ; I likes NP+ I c(i) object extraction likes transitive some other trees for likes: subject extraction, topicalization, subject relative, object relative, passive, etc. Fig. 3. LTAG examples 1. p1: S p2: S p3: VP /\ WJl VP /\ V I think S* S” /\ V I does VP” /\ V I I always p4: S A NPJ( PP NPJ/ A V I from Fig. 4. LTAG examples 2 substituted in the tree /I1 for think as shown (again by solid to Bill think S. Tree 82 for does is adjoined corresponding line with an arrow, resulting the tree for who Hurry likes as shown by a dotted line, resulting in a tree for does Bill think S, which is then adjoined lines), resulting in a tree to this tree as shown by a dotted into in the tree corresponding A. K. Joshi/Arti$ciul Iatelligrrrce IO3 (1998) 117-132 S a2: NPCC S /\ NPJl VP /\ V I likes NPJi e(i) p1: s p2: S r\ ;+ A S” V ( think /Y ; does 123 s*; a3: NP 1 a4: NP# who I Harry clS: NP JI Bill Fig. 5. LTAG examples 3. who does Bill think Harry likes does substitution adjoining -(____---_ I think I 1 Bill a5: NP JI a3: who Harry Fjg. 6. LTAG examples 4. to wh does Bill think Harry lines represent substitution Fig. 7 is very close to the so-called dependency dependencies the lexical items. among likes? This derivation and the dotted lines represent adjoining. The representation represent diagrams, which directly in Fig. 7 where the solid in the is shown 124 A.K. Joshi /Art$cial Intelligence 103 (19%‘) 117-132 who does Bill think Harry likes a2: likes A a.3: who pl: &ink _’ ,-\ a4: Hany p2: does ti: Bill substitution _ _afj_oi_“‘“g_ _ * Compositional * Related to dependency diagrams semantics on this derivation structure Fig. 7. LTAG derivation structure information XTAG system. A very large wide-coverage LTAG grammar the Wall Street Journal Corpus) and also for some applications for English has been built together with It has been used for parsing a wide range of corpora (for such as machine these aspects a parser-the example, retrieval and extraction. translation, computational as this is not the topic of my paper. I am concerned here with constrained I will focus more on the abstract character of adjoining. systems. 7 From this perspective Adjoining, unlike substitution, i.e., it is a kind of higher order operation, a higher order abstraction. This aspect of LTAG together with the fact that it is a constrained computational system has led to many applications of LTAG beyond parsing. Some examples are: changes (modifies) already built structures, I am not going to discuss (SPUD), which an event or a state l The generation work of Becker, Finkler, the work of Stone and Doran and Kilger in the Verbmobil Project (1997) and Dras (1997). 8 The work for the design of a takes in a collection of goals and to describe and and adjoining in a flexible manner. SPUD uses first what operators are the content of a specify and third, how operators achieve pragmatic effects. The operators are as the elementary (1997) and on generation by Stone and Doran uses the LTAG framework sentence planner using descriptions to achieve in describing recursively applies what information allows the possibility of construction a declarative available and how they combine; description; represented them. Meaning for each tree is expressed as a formula to include about them. The operations of substitution ‘descriptions’ of three kinds of information: second, how operators to combine in the so-called “flat” semantics in the world. to determine which entities trees of LTAG and the LTAG operations lexical specifications It incrementally specification 7 Mathematical properties of LTAG have been extensively several of its extensions belong to the class of mildly context-sensitive and other more complex dependencies and they are polynomially studied. In particular, that LTAG and it is known grammars and they capture nested, crossed ‘T. Becker, W. Finkler, A. Kilger, Generation in Verbmobil, Technical Report, DFKI, University of Saarlandes, Saarbrticken, and their 1997. M. Stone, in: Proc. 35th Annual Meeting of for Computational Linguistics, ACL-97, Madrid, 1997. M. Dras, Representing paraphrases using for Computational Linguistics, ACL-97, realization C. Doran, Sentence planning as description using the Association synchronous TAGS, in: Proc. 35th Annual Meeting of the Association Madrid, 1997. tree-adjoining requirements, techniques, in dialog grammar, parsable. translation: A.K. Joshi /Artificial Intelligence 103 (1998) 117-132 12s for example, the possibility of “modifying” of dependencies, in the approaches trees, localization and the pragmatics of the operators nature of LTAG is used for compiling HPSG (head-driven aspects of discourse structure as represented, (1997) and Cristea and Webber representation is modeled by associating with each tree a set of discourse constraints describing when the operator can and should be used. phrase The constrained structure grammar) into LTAG, also in the Verbmobil project (Kasper et al. (1995)) ’ and in the JSPS project at the University of Tokyo headed by Professor Tsujii, where it has been used to compile LTAG into an HPSG, leading to a very efficient parser (Tsujii, Torisawa, Tateisi, Makino and Nishida, 1997 (personal communication). The abstract character of adjoining has also led to the use of LTAG for modeling incremental in the works (1997). lo The key aspects of TAG of Gardent used in these works are the syntactic and semantic encapsulation in the domain of and the operation of adjoining the elementary allowing already built structures. These abstract properties TAG what makes TAG attractive for modeling certain aspects of discourse. However, taken so far the full potential of these abstract properties have not been utilized. Recently, Webber and Joshi (1998) ‘I have explored a “fully” lexicalized TAG for discourse, to examine how the basic insights of LTAG carry over to discourse. Just as lexicalized grammars have shown the value of taking the basic elements of a clause to be not simple words, but “structures” scope, there is value in taking that reflect an item’s role and local syntactic/semantic the basic elements of discourse that reflect the syntactic/semantic In a non-linguistic secondary et al. (1998) and by Abe and Mamitsuka Localization of dependencies us to capture complex dependency nested dependencies schwimmen(k) sah(i) nouns and verbs are in a nested order, as the subscripts clause, Dutch subordinate zag(i) Zuten(j) zwemmen(k) Marie(k) of course, more complex dependencies domain in RNA the abstract character of LTAG has been exploited by Umeura and adjoining allows in a “local” manner. For example, clause Hans(i) Peter(j) Marie(k) (Jan saw Piet make Marie swim). There are, of nested and crossed such as combinations (1994). I2 and the operations of substitution (Hans saw Peter make Marie swim), where to be not simple clauses, but structures are crossed, as in Jan(i) Piet(j) as in a German subordinate complex dependencies these dependencies indicate; however, scope of coherence such as modeling the possibility structures, relations. lussen(j) allowing patterns the in a “R. Kasper, B. Kiefer, K. Netter, K. Vijayshanker, Compilation of HPSG for Computational Linguistics, ACL-95, Cambridge, Meeting of the Association to TAG, in: Proc. 33rd Annual 1995. lo C. Gardent, Discourse TAG, Technical Report, Computerlinguistik. 1996. D. Cristea, B. Webber, Expectations the Association ” B. Webber, A.K. Joshi, Anchoring a lexicalized Discourse Relations, COLING/ACL-1998 Workshop, Montreal, 1998. for Computational Linguistics, ACL-97, Santa Cruz, 1997. in incremental discourse processing, tree-adjoining grammar I2 Y. Umeura, A. Hasegawa, S. Kobayashi, T. Yokomori, Tree-adjoining grammars University of Saarlandes, Saarbriicken, in: Proc. 35th Annual Meeting of for discourse, in: Proc. Workshop on for RNA structure prediction, in 1998. N. Abe. H. Mamitsuka, A new method of predicting protein in: Proc. 1 lth International Conference on Machine tree grammars, Theoretical Computer Science, secondary Learning, 1994. to appear structures based on stochastic 126 A. K. Joshi /Art+&1 Intelligrnce 103 (I 998) I 17-132 Supertags of likes S A NPJl VP /A V likes transitive NP+ NP’b VP I likes I e(i) object extraction some other trees for likes: subject extraction, subject relative, object relative, passive, etc. topicalization, Fig. 8. Supertags dependencies. There is a fascinating psycholinguistic which shows that the crossing dependencies dependencies. Now LTAG predicts correspondence without modeling grammars by a constrained result of Bach et al. (1986) ” than the nested to this psycholinguistic result quite precisely. The relevance of this for the present paper is that such a result would not have been possible it turns out that the automaton formal system such as LTAG. that exactly corresponds are “easier” to process 2. I. Supertugging I will now take a completely different perspective on LTAG. I will treat the elementary item as if they are super part-of-speech trees associated with a lexical supertags) in contrast Now it is well known disambiguation these supertags, which are very rich descriptions of the lexical this will lead to “almost” parsing. The approach is called supertagging. to the standard part-of-speech techniques that local statistical of standard POS. Can we apply (super POS or such as V (verb), N (noun) etc. successful can lead to remarkably for disambiguating techniques items? If we can, then, indeed, I4 In Fig. 8 some elementary trees associated with the lexical item likes are shown. These are the same trees we have seen before. However, now we are going to regard these trees (supertags) associated with likes. Given a corpus parsed by LTAG as super part-of-speech the statistics of supertags, statistics such as unigram, bigram, grammar we can compute these statistics combine not only lexical statistics and trigram Interestingly, frequencies. l3 E. Bach, C. Brown, W. Marslen-Wilson, Crossed and nested dependencies Getman and Dutch: a psycho- linguistic study, Language and Cognitive Processes 1 (1986) 249-262. “This is joint work with B. Srinivas. A.K. Joshi /Artijicial Intelligence 103 (1998) I1 7-132 127 Supertagging al p2 a9 a2 a6 a10 a3 a7 all p1 a4 p4 al2 a5 a8 al3 P3 the purchase price includes two ancillary companies On the average a lexical item has about 8 to IO supertags Fig. 9. Supertagging Supertagging al 0 p2 a3 0 a2 a10 q Ip31 a6 a7 a4 Ip 4 al2 a5 a8 101131 the purchase price includes two ancillary companies - Select the correct supertag for each word -- shown boxed - Correct supertag for a word means the supertag that corresponds to that word in the correct parse of the sentence Fig. 10. Supertagging (as represented by the elementary lexical statistics with the statistics of the environments trees) in which the in but the statistics of constructions items appear, thus combining which the lexical items appear. for example, consider Thus, the string the purchase price includes two ancillaq companies as shown in Fig. 9. The supertags associated with that word appear on top of each word. Some words have only only one supertag associated with them and others have more than one. In the current XTAG system there are about 8-10 supertags per word on the In Fig. 10 the same supertags are average, so there is a very high level of local ambiguity. (in a box). shown for each word; however, for each word one supertag has been identified 128 A.K. Joshi /Artificial intelligence 103 (1998) I1 7-132 This is the “correct” supertag for this word in the sense that this is the supertag associated with this word in the correct parse of this sentence. Suppose we are able to find the correct supertag for each word in this sentence by applying techniques parse because we have not put the supertags then for all practical purposes we have parsed the sentence. together, hence we call it “almost parse”. local statistical disambiguation It is not a complete A supertagging to the standard POS disambiguation experiment was carried out using trigrams of supertags and techniques similar techniques. The corpus used was the Wall Street Journal Corpus (WSJ). With a training corpus of 1 million words and a test corpus the of 47 000 words, the baseline performance was 75% (i.e., 75% of the words received “correct” supertag). The baseline corresponds to the case when the supertag chosen for a word is just the most frequent supertag for this word. We know from the performance of is 90% or better. The disambiguators is low baseline performance very high (about 8-10 on the average) to the local ambiguity of the standard in contrast POS, which is about 1.5. The performance of the trigram supertagger, on the other hand, is 92%. The improvement from 75% to 92% is indeed very remarkable. This means that 92% of the words received the “correct” supertag. for the standard POS that the baseline performance is due to the fact that the local ambiguity for supertagging (lexical systems. computational the performance of the primitives and to make the output to the use of constrained (richer) descriptions of primitives simple and build complex descriptions Of course, more can be said about this supertagging In supertagging we items in our case). to the standard mathematical wisdom or convention, where we keep out of simple (lexical items in our case) are complex because to that primitive. Making i.e., approach. There are techniques to improve look more like a complete parse. I will not discuss these aspects; rather, I will talk about the abstract nature of supertagging and its relevance are working with complex This is quite contrary the descriptions descriptions. The descriptions of primitives we try to associate with each primitive all information descriptions more complex has two consequences: there are many more descriptions of primitives the richer constraints on what other pieces can go with a given piece. Making primitives more complex allows us to compute statistics over these complex descriptions the but, more relevant dependencies and word-to- construction dependencies). Local statistical computations over these complex descriptions lead computation on complex descriptions. (1) local ambiguity for each primitive, however, (2) these richer descriptions locally constrain each other. There is an analogy here to a jigsaw puzzle- that there are stronger of the descriptions to robust and efficient processing. Supertagging these statistics directly is thus an example of a local the description of each piece because dependencies (for example, word-to-word the better, in the sense are more meaningful they capture is increased, importantly, relevant (i.e., regarding The supertag perspective items as complex part-of-speech) lexical the resolution of attachments as attachments). Traditionally ambiguity from parsing where the entire process of parsing consists of resolution of attachments. perspective trees in LTAG associated with the elementary suggests a view of parsing consisting of only in LTAG can both be viewed (substitution (e.g., the attachment the so-called PP attachment problem in I saw the man in the park with a telescope) as a special problem. However, is just a special case of In this (in the traditional sense) but only resolves the parser does not build constituents the supertag perspective the PP attachment and adjoining problem A.K. Joshi /Artt$cial Intelligence 103 (1998) 117-132 129 attachments. This perspective also suggests some novel approaches using unsupervised (currently being pursued by Anoop Sarkar). techniques to corpus based parsing to AI. I can illustrate this by pointing out These considerations relationships are directly to the well-known relevant information algorithm of Waltz (1975) I5 for interpreting interesting of vertices more complex line drawings. What Waltz did was to make the descriptions by adding about the number and types of edges incident on a vertex. Again there is an analogy here to a jigsaw puzzle: the richer the description of a piece the better. the local ambiguity was By making in the taxonomy of increased, local computations junctions) on these complex descriptions these descriptions. So once again we have here an example of a local computation which has been my recurrent for example, an L junction has about 92 physically possible labelings. However, to rapidly disambiguate (a particular kind of junction over complex descriptions, of vertices more complex the descriptions are adequate theme. t6 (richer) 3. Local structure of discourse+entering systems are equal in discourse. computational the use of constrained My last example concerns in the sense that some are “easier” in particular, complexity of inferences is related, at least in part, to the structure of an utterance in the area In order to integrate an of discourse, in the previous discourse a variety of inferences need to be made. However, utterance than others. This not all these inferences in a discourse. If a distinction uniform machinery then, of course, we will be able to model all these inferences but we will not learn much about the language specific that affect the complexity of inferences. One mechanism of a constrained mechanisms inferential known as Centering. I7 In my discussion of centering, of this work that relate to my particular position on constrained computational major theme of this paper. in discourse, which is I will limit myself to certain aspects systems, the is based on the local structure of an utterance is used to subsume all inference mechanisms system that an utterance The basic idea is to start with the observation in a discourse singles out an individual or entity among all those that are denoted by the arguments of the main predicate. This entity is called the backward-looking center of the utterance, which for the purpose of this paper, I will call as the center. The notion of a center is a discourse construct and not a syntactic or semantic construct. Centering an entity is equivalent property a simple example, consider to ascribing a itself may, of course, involve other individuals. As to an individual. The property the utterance John hit Bill In a particular discourse, say D 1, John may be the center, which we may represent as I5 D. Waltz, Understanding line drawings of scenes with shadows, in: PH. Winston (Ed.), The Psychology of Computer Vision, McGraw-Hill, New York, 1975. I6 Waltz (1975) did not use statistical information but this is not relevant to my main point. I7 My work on this system is a collaborative work with Barbara Grosz and Scott Weinstein (and earlier with Steve Kuhn). 130 A. K. Joshi /Art$cial Intelligence 103 (/99X) 117-132 (JOHN x)(HIT x BILL) In another discourse, say D2, Bill may be the center, which we may represent as (BILL y) (HIT JOHN y) in a discourse, where an n-ary predicate The main idea is that the notion of centering allows us to describe the rough logical form is made to look like a monadic of an utterance predicate (predicate of a single argument) by singling out one argument as the center and temporarily hiding the other n - 1 arguments. This leads to a locally monadic structure of in a discourse. And it is this local monadic aspect of the logical form that has an utterance for the complexity of inference. implications In the predicate calculus representation This is not true for an utterance Frege was aware of this distinction and its relevance is an interesting quote from Frege (1879): ‘s in a discourse. all arguments of a predicate have an equal status. to note that as early as 1879 to the ease of certain inferences. Here It is interesting languages the subject in the sequence of words has a distinguished place . . . In ordinary This may, for example, have the purpose of pointing out a certain relation of the given judgement to grasp the entire context.. to others, and thereby making it easier for the listener . . here status construct. the special is assigning In the centering Of course, Frege to the subject, which is a theory the center is a discourse construct and not grammatical a syntactic construct and any entity corresponding to an argument of a predicate can be centered. However, Frege clearly was aware of the (local) focus provided to an entity by the structure of an utterance The notion of (local) focus is central Therefore, role in controlling in a discourse and its influence on the ease of certain inferences. inference. it is not surprising inferences. that local structuring of an utterance strategies for controlling in a discourse plays a to all perceptual involves The actual work on centering (it would be a boring discourse, the study of the transition to utterance because a centered entity does not necessarily utterance all the time in a discourse new entities become centered, previously so on. The transitions of centers have to be described as patterns of continuations center and shifting of the centers (in other words, mechanisms as the discourse proceeds. Centering definite descriptions observation entities centered process). from of centers remain centered to be sure). Centers shift, centered entities become centered again, and of the the centers) the study of how pronouns and these details here but the main for my topic here, is that the more the transitions and the more the (i.e., harder to theory also involves I will not discuss the discourse becomes relate to centering. the more complex in a discourse for tracking , important I will give a simple example two discourses, Dl and D2. In each case assume that John has been established as the center prior to Dl or D2. to illustrate my main point. Consider the following ‘* G. Frege 1879, “Begriffsschrift” 1967. University Press, Cambridge, reproduced in Jean van Heijenroot (Ed.), From Frege to GGdel, Harvard A.K. Joshi /Arti$cial Intelligence 103 (1998) 117-132 131 Dl: (a) John called Bill, (b) He wanted his advice. D2: (a) John called Bill, (b) He was happy to hear from him. In Dl he in Dl(b) the locally monadic refers to John and his to Bill. In D2, on the other hand, he in D2(b) in leads to a that the discourse D2 is harder to process than the discourse D 1. This is because to In D2(b) the center has to be shifted from John to Bill. It is this shifting refers to Bill and him to John. Under a discourse prediction in centering more complexity). of the center in D2(b) that leads to the increased complexity. of a center is preferred theory continuation (the latter leading of an utterance of an utterance characterization in a discourse representation the centering to shifting l9 system as compared system we are able to capture certain aspects of inferential It should be noted that by adopting a constrained of the structure of an utterance the representation The main point here is that a discourse has a locally monadic structure. Monadic calculus to the full predicate calculus. Thus by using is certainly a constrained complexities a constrained system we have actually in discourse. complicated Instead of in a discourse. having all the arguments of a predicate having an equal status, we have given one of the arguments a special status, thus complicating in a discourse, (Section 2. l), in the sense that complexity of the description of the primitives the ease of inference. (utterances to the one in my second example is related to the description of primitives in this case). This situation is similar 4. Conclusion So far I have given three examples of my research, which are relevant to the topic of this in modeling various issues which arise out of I will now briefly discuss some unifying the use of constrained formal/computational systems paper, namely, aspects of language. these (and other related) examples. formal/computational The use of constrained systems for modeling various aspects of language allows us to localize complex dependencies. This is one of the crucial results in the study of these systems. The use of such systems often requires us to make the descriptions of primitives more complex. However, this complexity which become more local, in other words, the greater the complexity of the primitives more local the computations leads to computations over them. Statistics computed over primitives with complex descriptions the sense that they capture the appropriate statistical dependencies of dependencies) and these in turn lead to efficient and robust computations. are more meaningful (due to the localization in the ” A psycholinguistic experiment is suggested here. The prediction would first shift to John in D2(a) and then to BiH in D2(a). An experiment based on the head-mounted technology is under consideration at present. is that in interpreting he D2(b) “attention” eyetracker 132 A.K. Joshi /Artijicial Intelligence 103 (1998) 117-132 Statistics Adopting primitives with complex descriptions requires us to have richly annotated corpora of texts, dialogues, and various interactive situations, either obtained automatically or most likely, semi-automatically. annotated corpora. Statistical of various aspects of language be emphasized because computational modeling of various aspects of language in deciding collect useful statistical information. A significant structures emerging out of the use of constrained very appropriate units for counting then have to be computed over these richly tell us how to count but the computational models tell us what to count. This is an obvious point but needs to is crucial structures we want to deal with and count in order to result of this line of research is that local systems provide the relevant primitive formal/computational techniques in statistical processing. to show that the use of constrained into various aspects of language. formal/computational It leads to appropriate In summary, I have tried the relationship systems gives us deep insights notions of locality my research&-one I discussed primitives-the them. I illustrated statistical aspects of natural make constrained approach will be productive more complex in syntax, semantics, and discourse. from syntax, one from syntax and semantics, and one from discourse. I gave three relevant examples of between locality the description and the more the relevance of locality for unifying languages processing. These are the aspects systems highly relevant to AI. I believe formal/computational the complexity of descriptions local theoretical, the computations computational, of over and that, I believe, that this in other domains of AI also. Acknowledgments I have mentioned my major collaborators in the three topics I have discussed above. I am not giving the entire bibliography which involves due to the lack of space. Instead I want to give a public acknowledgement my collaborators over all these years by listing them below: these and all other collaborators of my debt to Zellig Harris, Lila Gleitman, Naomi Sager, Bruria Kauffman, Carol Chomsky, and Phil Hopely ; Hisao Yamada, S. Kosaraju, Leon Levy, Masako Takahashi, Richard Upton, Johnson Hart, Takeshi Yokomori, K. Vijayshanker, David Weir, Anthony Kroch, Yves Schabes, Anne Abeille, Tilman Becker, Robert Frank, Owen Rambow, B. Srinivas, Christy Doran, Beth Hockey, Seth Kulick, Natasha Kurtonina, Anoop Sarkar, Martha Palmer, Gann Bierner; Ralph Weishedel, Stan Rosenschein, Jerry Kaplan, Eric Mays, Kathy McKeown, Kathy McCoy, Bonnie Webber, Tim Finin, Steve Kuhn, Scott Weinstein, Barbara Grosz, Ellen Prince, Marilyn Walker. 