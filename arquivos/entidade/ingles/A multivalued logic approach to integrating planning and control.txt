ELSEVIER Artificial Intelligence 76 (1995) 481-526 Artificial Intelligence A multivalued logic approach to integrating planning and control Alessandro Saffiotti *, Kurt Konolige, Enrique H. Ruspini ArtQicial Intelligence Centec SRI International, Menlo Park, CA 94025, USA Received 15 July 1993; revised 18 March 1994 Abstract Intelligent agents embedded in a dynamic, uncertain environment should incorporate capabilities for both planned and reactive behavior. Many current solutions to this dual need focus on one aspect, and treat the other one as secondary. We propose an approach for integrating planning and control based on behavior schemas, which link physical movements to abstract action descriptions. Behavior schemas describe behaviors of an agent, expressed as trajectories of control actions in an environment, and goals can be defined as predicates on these trajectories. Goals and behaviors can be combined to produce conjoint goals and complex controls. The ability of multivalued logics to represent graded preferences allows us to formulate tradeoffs in the combination. Two theorems relate complex controls to complex goals, and provide the key to using composition standard knowledge-based deliberation techniques to generate complex controllers. We report experiments in planning and execution on a mobile robot platform, Flakey. 1. Introduction Mobile increasingly sophisticated and autonomous, robots are becoming and we de- mand more intelligent behavior from them in complex environments. To meet these expectations, we must address a set of problems associated with real-world environ- ments: knowledge of the environment is partial and approximate; sensing is partial and noisy; the dynamics of the environment can be only partially predicted; and an agent’s hardware execution is not completely reliable. Classical AI planning approaches to these problems are inadequate, especially in providing realtime decision-making and control for the robot. On the other hand, purely reactive systems of control (e.g., [ 6,9,13,17] ) , * Corresponding author. Current address: IRIDIA, Universitk Libre de Braxelles, 50 av. Roosevelt CP 194/6, B-1050 Brussels, Belgium. E-mail: asaffio@ulb.ac.be. 0004-3702/95/.$09.50 SSDI 0004-3702(94)00088-3 @ 1995 Elsevier Science B.V. All rights reserved 482 A. sajiotti et al./Artijicial Intelligence 76 (1995) 481-526 immediate while providing tally substitute not to carry an oil lantern downstairs response for planned behavior to unpredicted in solving complex environmental situations, cannot to- tasks (for example, by d&ding to look for a gas leak [ 131.) One solution and reactivity to the dual need for planning at the lower level, a complex controller achieves is to adopt a two-level model: at the upper level, a planner decides a sequence of abstract goals to be achieved, these based on the available knowledge; goals while dealing with the environmental ). The (e.g., satisfy strategic goals controller is “complex” because it must be able to simultaneously and low-level “innate” (e.g., going to the end of the corridor), coming job to produce physical goals (e.g., avoiding obstacles on the way). It is the controller’s movements the two main challenges are trading off between multiple goals, and bridging these goals to the highest degree possible. Thus, specified goals and physical movements. that satisfy to developing the gap between abstractly a complex controller from the planner [ 3,10,14,27,30] contingencies approach implements the problem A reasonable is to decompose to complex control [ 2,6 1. In the psychological type of physical movement one specific motor skill, and results into small in a the concept of activity directed at a particular goal has been given the name schema Arbib and for behavior of frogs. Each of these two task-level the frog; the combination the prey and away from by Lyons units of control, each of which certain coordinated and his colleagues have exploited robot control. For example, Arbib and House [ l] describe how schemas can account the prey-seeking goals is represented of the fields describes obstacles. Further extensions of this idea to robot control were investigated and Arbib field attracting or repelling of the frog towards and obstacle-avoidance as a potential to model brain functioning the schema concept the movements literature, [ 241, Overton [ 291, and Arkin Arbib’s motor schemas give an answer [ 31 under the name motor schema. to trading off among multiple goals, by pro- of complex control problems. They do not, however, symbolically above, of bridging the gap between approach viding a modular decomposition try to answer the second challenge specified goals and low-level physical movements. positional directly connects of our approach proach as a mathematically-motivated of multivalued l Behaviors to higher-level planning are very close logic. The advantages to those of Arbib’s. reconstruction to this formalization are described to behavior, based on the mathematics in terms of preferences can be composed mathematically In this paper, we develop a com- that and deliberation processes. The starting point In fact, one can view our ap- the tools of motor schemas, using of multivalued logic, are the following. among control actions. Hence, mul- as a means of trading off between form of coordination than the linear superposition used tiple behaviors their goals -a more powerful in motor schemas. l The concept of a goal as a predicate on the trajectory produced by the controller arises naturally, descriptions of action. and bridges the gap between physical movements and more abstract . Complex goals can be achieved by composing such as goal-regression their respective behaviors. Thus, stan- planning, can be used to generate dard deliberation procedures, complex controllers. A. Sajjiotti et al./Ariifcial Intelligence 76 (1995) 481-526 483 I. 1. Outline of the approach We develop the following the theory of behavior levels of abstraction: in a bottom-up fashion, describing movements at bodily movement + execution =+ goal satisfaction in a specific environment straighten elbow joint put arm out car window signal left turn This way of connecting movements, the work of Israel et al. [ 161. Formally, we use the framework of multivalued [ 23,321, which allows us to express partial preferences logical operators actions and achievements and to combine inspired by logics them by using has been [ 5,351. The basic units of control, control schemas, are descriptions of types of movements. capabilities, schemas define the camera the agent’s basic movement Control aiming to the right. Like classical controllers, states to effector commands. However, control schemas are less committal controllers over the space of all possible commands. The idea here is that different commands generate, are represented by predicates like making a step or control schemas relate internal than classical in that they map each state to a measure of preference, or desirabilityfunction, can functions to a greater or lesser extent, the same type of movement. Desirability in a multivalued We compose control schemas by combining logic. the corresponding desirability functions satisfy the desirability to functions of that will, under some conditions, a corridor, an obstacle avoidance logics. For instance, we can compose control schemas via the operators of multivalued obtain a movement both behaviors. Care must be taken, however, when the control schemas are conflicting, if a robot is facing an obstacle that is, they have no common preferences. For example, blocking corridor following schema with its own context of applicability: corridor following cannot be used, and its preferences contextual truth of these formulae combination, for control schemas. control schema may want to stop, while a schema may want to go forward. In general, we associate each control is blocked, plain should be disregarded. We express logic; and use the degrees of schemas. This weighted called context-depending blending, is the main coordination mechanism by using formulae of a multivalued the preferences of different e.g., when the corridor to weight conditions Control schemas operate with respect to the agent’s internal state, including its sensor rep- these two into the controller, of objects that contain action descriptions in the world, e.g., picking up block “A”. To reconcile readings. Planners, on the other hand, manipulate resentations levels of abstraction, we insert some parts of the planner’s model e.g., the expected size and position of the intended block. These object models, or de- scriptors, are then used as input to control schemas, “lifting” to the level of abstraction used by the planner. To maintain a closed-loop keep track of these objects during execution, by polling perceptual the descriptors anchored to their real-world with a set of object descriptors and a contextual condition, Behaviors play the role of situated actions: formed under which circumstances the effects of the controller response, we to keep together into a behavior. should be per- and with respect to which objects. Behaviors bridge routines correspondent. A control schema, they indicate which movement is packaged 484 A. S@otti et al. /Art@cial Intelligence 76 (1995) 481-526 the gap between abstract action descriptions and physical control. They are inherently movement-oriented, since their control schema induces a preference on control actions; but they also incorporate elements of abstract action: the objects that the action operates on, and the preconditions for the success of the action. Our final step is to link the local notions of sensing and control embodied in behaviors to the global notion of satisfaction of goals. To do this, we adapt some ideas from dynamic logic to our multivalued framework. We define the meaning of a behavior to be the (fuzzy) set of all possible executions that it can produce, expressed as trajectories in the space of states. Then, goals can be defined as predicates on the trajectories. We relate behaviors to goals by defining the central notion of goodness of a behavior for a goal: any execution of the behavior under its contextual condition will satisfy the goal. is a behavior for picking up a block a. We can show that For example, suppose B(a) is true it accomplishes this goal under circumstances C if the predicate Grasping(a) of every trajectory of B(a) in every environment where C is true. In fact, this is a uniform way to describe not only goals of achievement, but also goals of maintenance and prevention. Behaviors can be combined to form complex controllers. If we could prove that the composition of behaviors accomplishes a corresponding composition of their goals, we could use techniques for reasoning about actions and goals to generate complex controllers. In fact, under certain hypotheses of consistency and stability, we are able to prove two theorems that relate composed behaviors to composed goals. First, if two behaviors are individually good for two goals Gr and G2, then their conjunction is good for the conjunctive goal Gr fl G2. Second, if a behavior B is good for G under the circumstances C, and B’ is good for bringing C about whenever in C’, then the chaining of B and B’ is good for G under the more general circumstances C U C’. We now have all the element we need to reason about behaviors and their composition. Behaviors bring with them a specification of their preconditions (the context) and their postconditions (the goals they are good for) ; and they can be combined with classical composition operators, like conjunction and chaining. Thus, we can reasonably expect to be able to use standard planning techniques, and existing planning systems, to generate complex combinations of behaviors that satisfy given goals. This is indeed the case, and we end the paper by showing how we can use use deliberation processes based on goal regression and goal reduction to generate complex controllers. We emphasize that we do not propose here any novel planning technique: our aim is to show that multivalued controllers can be generated by the existing ones. 1.2. Experiments on a mobile robot We have tested the proposed methodology extensively on indoor robot navigation problems, using a mobile robot testbed called Flakey. To make our approach more concrete, we use this testbed throughout the paper, by showing runs and discussing implementations choices. Our experience on Flakey indicates the effectiveness of the discussed methodology, and the ease of incorporating different types of deliberation processes. A. Sajiotti et al. /Artijcial Intelligence 76 (1995) 481-526 485 1.3. Plan of the paper the notions of control schema, context, The next section discusses our multivalued logic approach to defining complex con- trajectory, and context- the perceptual embedding of the agent, blend- operators: conjunction, the notions the- the results of the previous sections and show how we the formal construction by introducing for a goal, and presenting the composition In Section 3, we consider schemas and their three combination trollers, and introduces blending. depending and define behavior ing, and chaining. Section 4 completes of a goal and of goodness of a behavior orems. In Section 5, we exploit can reason about behaviors techniques. Finally, Section 6 compares our method extended version of this paper is available as [ 381. and generate complex controllers using standard planning to other related approaches. An 2. Moving We start our construction like stretching an arm or rolling from the description of the physical movements of an agent. forward, by struc- We describe elementary movements, to the execution of movements, tures called control schemas. Then we turn our attention and study the set of all possible executions, or trajectories, of a control schema. Finally, we show how to compose multiple control schemas and that relate the trajectories of a combined control schema to those of give some theorems the formal definitions by its components. To make examples robot, Flakey. First, some background on the formal tools that we use. things more concrete, we illustrate to describe complex movements, taken from our mobile 2.1. Multivalued logics logics logics provide an ideal framework this paper, we use the framework of multivalued for this choice. The first one is technical: by representing [ 23,321. There are degrees of to merge notions terms, with notions terms. The second logics can be viewed as logics of graded preference Throughout two main reasons truth on a numeric scale, multivalued taken from the world of planning, taken the world of control, reason [ 3 1 ] where we interpret desirability, of being in that world from the point of view of P [ $35,361. Accordingly, we use propositions combine to represent control strategies and goals, and logical connectives the truth value of a proposition P in a world as the utility, or from is semantic. Multivalued typically expressed expressed typically in symbolic in numeric them. to Let L be a propositional language, S a set of states, and TV function extend TV to non-atomic propositions that assigns a truth value in [0, l] to each atomic proposition by the equations : C x S -+ [ 0, 1 ] a in each state. We (1) 0, and @ is 486 A. Sajiotti et al./Arti&ial Intelligence 76 (1995) 481-526 TV(+s) TV(PAQ,S, TV(PVQ,s) = I- TV(P,s) = TV(P,s) @TV(Q,s) = TV(P,s) @TV(Q,s) TV(P + Q, s) = TV(Q, s) 0 TV(R s) triangular norm, or t-norm, with quasi-inverse where 0 is any continuous any continuous t-conorm. the quasi-inverse, T-norms, implication, conjunction, t-norm in each argument, unit. Given a continuous and and disjunction, is any binary operator on [ 0, 1 ] that is commutative, respectively t-conorms are used as a generalization [ 42,45,46] . Mathematically, of logical a associative, non-decreasing and has 1 as unit; a t-conorm has the same properties but has 0 as t-norm 0, its quasi-inverse 0 is defined by x 0 y = sup{w E [O, 11 I OJ c3 y < .x}. Note that, for any 0, y = 0 implies x 0 y = 1 and y = 1 implies x 0 y = x. Example 1. The following are examples of t-norms and t-conorms. (4 min(x, Y) max(x, y> tb) (c) max(x+y- XY x+y-ny 1,O) min(x+y,l) min(x-y+l,l) 1 ifxay, otherwise x mintx/y, 1) The operators truth values choice of 0, $ and 0. in (a) are the ones we use in our robot, Flakey. Notice to be either 0 or 1, the conditions ( 1) reduce to classical that if we restrict logic for any Given a proposition P, we are often interested denoted by [PI. As truth values are numbers to [[PI is a matter of degree. We measure defined by in the set of states of S where P holds, in [ 0, 1 ] , the membership of any state s this degree by the membership function flp] .f~pl ts) = TV(P, ~1. Graded sets of this type are commonly plicity, and when there is no risk of ambiguity, we denote P, and write P(S) sets based on the equations for f[p] (s). We define complement, (1) above as follows: known as fuz,zy sets [ 491. For notational sim- the fuzzy set [Pl] simply by and union of fuzzy intersection P(s) = 1 -P(S) tP nQ><s> = f’(s) @Q(s) (P u Q>(s) = f’(s) CD Q(s) We also define set inclusion by P i Q iff P(s) 6 Q(s) for all s in S. (2) A. Safiotti et al./Artijicial Intelligence 76 (1995) 481-526 487 (a) (b) Fig. 1. Values of D(s, a) for two control schemas: (a) Follow; (b) KeepOff. If we use the operators defined by Zadeh [ 491. (a) in Example 1, these operations correspond to those originally In what follows, we sometimes the interpretation refer to a proposition P as a desirability of P(s) as the desirability of being function on in state s from S to emphasize the viewpoint P. We routinely use the 8, $ and 0 operators functions. The reader should keep in mind conjunction, @ the notion of disjunction, We also use the sup (least upper bound) the notions of existential capture and inf (greatest and universal quantification, that @I is meant to combine desirability to capture the notion of (right to left). to lower bound) operators respectively. and 0 the notion of implication 2.2. Control schemas Consider an agent with a set S of possible internal states, and a set A of (atomic) is generally defined by a function strategy for this agent that can be instantiated need to be less rigid. For concreteness, in each state, a control action. To define generic control actions. A control that produces, movements execution -we “take a step”. We may well have an archetype of the ideal step, but when we take a step that only vaguely meets this archetype; on a muddy in that situation, and still, this inelegant maneuver we want our definition the notion of a in each state, a value of desirability of each possible control. control strategy in several ways depending on the circumstances consider road we may perform a movement of “take a step” to account the best choice available the type of movement types of movements- for it. We extend to produce, is probably of Definition 2. Let S be a set of states, and A a set of control actions. A control schema on S and A is any function D:SxA-+ [O,l]. Intuitively, from the viewpoint of performing that type of movement. the value of D( s, a) measures how much executing a in state s is desirable Example 3. Fig. 1 illustrates schema, called Follow, for proceeding within a given “lane” intended lines); to the Keep-Off control schema, two control schemas. refers (b) In (a), we consider a control (represented by the double to stay away from a given 488 A. Sufiotti et al./Art@cial Intelligence 76 (1995) 481-526 spot. The picture at the small dot. Each vector indicates a possible to the desirability D(s, a). vector is proportional is a representation of the current state s, with the agent being located turning direction a: the length of the that actions the control In the case of our robot Flakey, to be sent control values in A are meant to be directly executable the elements of A are the turning at each control cycle low-level and (100 to the wheel effecters Note commands. velocity msec) . 2.3. Trajectories A control schema is a local notion: it tells us the desirability each state. In order to get a more global view of movements, we introduce of a trajectory. This notion in relating movements is essential to goals in Section 5. of control actions at the concept Definition 4. Let S be a set of states. A trajectory on S is a finite, nonempty sequence of elements of S. The set of all trajectories on S is denoted by I(S). We also denote by 7, (S) the set of trajectories of length at least K to represent We take trajectories on S to represent complete executions of movements, sets of trajectories follow an infinite number of paths when trajectories characterize possible executions, actions and (fuzzy) types of movements. For example, an agent’s hand can its arm, and all these “stretch an arm”. Accordingly, we schema D by the set of its that is, the set of trajectories produced by always executing control are instances of the type of movement the type of movement that are desirable according defined by a control the agent stretches to D. Formally, we proceed as follows. First, we assume that we are given a function M:SxAxS+[O,l] capabilities and constraints, it is that s’ is the state resulting such that M( s, a, s’) measures how possible execution of action a in state s. In general, we expect M to account physical uncertainty For example, the velocity increases be a very complicated to allow us to abstract too much cannot be in an admissible relation, especially for nonholonomic from the details of robot motion. from the for the agent’s features, for the of other agents, and so on. states in which trajectory. Obviously M can its main job here is robots; robots have bounded acceleration, for the known environmental associated with effector failures or interference so any two successive Given a control schema D, we measure the possibility that an execution of D produces a transition from a state s to a state s’ by NextD(s,s’) =sup[D(s,a) @M(s,a,s’)]. &A (3) that (s, s’) is a possible state transition reading of the sup and @ operators, we can interpret (3) as for D to the extent that there exists some our intended Recalling saying action a that is both desirable for D in s. and it leads from s to s’. A. Sajiotti et al./ArtiJcial Intelligence 76 (1995) 481-526 489 Finally, we look at the set of trajectories whose transitions are all possible for D. We call these trajectories desirable for D. Definition 5. Let D be a control schema. The set of desirable trajectories of D is the fuzzy set on ‘T(S) defined by Trajo ( t) = inf N.%tD (S;, si+l ) . Q<i<k where t = (s&St,. . .,sk), k > 0. The set TrajD fully characterizes meaning of D. This is similar taken in dynamic is associated with the set of its possible executions. Segerberg and Tutiya a control schema D, and we take this set to be the logic, where a program [43] and Israel, Perry [ 161 analyze movements to the approach and actions in similar terms. 2.4. Implementation of control schemas A control schema of a control schema D must select, at each state, one control action to send to the effecters. We represent such an implementation is a descriptive device. Any implementation by a control function to choosing and following one of the i.e., to execute one particular movement of type D. Of course, we as is an intractable search problem, to choose the “best” trajectory. This trajectories, like Selecting actions by the Fo function corresponds desired would the size of the state space grows exponential with its dimensionality. occur with potential search methods implementation based on fuzzy control rules of the form that could be employed of control to find candidate field methods techniques [ 34,39,40]. A control schema D is encoded by a set R of fuzzy robot Flakey by using for our mobile Similar problems (see Section 6). There are many different approximate trajectories. We have built an schemas IF Pi THEN Ai, i= l,..., n, in multivalued where each Pi is a proposition actions. From these fuzzy rules, a desirability logic, and each Ai is a fuzzy set of control function DR can be computed by D,y(S,U) = SUP min(Pi(s),Ai(a)). I Qi<n (4) in R that Intuitively, DR says that a is a desirable is rules supports a and whose antecedent in the field of fuzzy control. The process of choosing one action from this customary is called defu@ication. We have used the centroid method, which function desirability computes a desirability-weighted is true in s. This way of interpreting in s if there is some rule average control control control FR(s) = .,-aD(s’a) da ’ JD(s,a)da 490 A. Safiotti et al./ArtQicial Intelligence 76 (1995) 481-526 IF (centerline-on-right A 4ane-angled-left) THEN turn-medium-right IF (centerline-on-left A Jaw-angled-right) THEN turn-medium-left IF (lane-angled-right A xenterline-on-left) THEN turn-smooth-right IF (lane-angled-left A lcenterlineon-right) THEN turn-smooth-left Fig. 2. Fuzzy rules used in Flakey to implement the Follow schema. Fig. 3. A vector field showing the controls generated by the Follow ruleset. to make sense, the rules should not suggest dramatically For averaging opposite actions in the same state. Our coding heuristic has been to make sure that rules with conflicting to use more involved consequents choice functions have disjoint antecedents, Other authors have preferred [ 481) . (e.g., rules for implementing the Follow control schema Example 6. Fig. 2 shows Flakey’s (first two discussed above. These rules keep the robot close to the middle of the lane rules), and approximately lined up with it (last two). Fig. 3 shows the turning controls produced by the corresponding FR. Note that, in each state, FR only outputs one turning control, and does not say anything about this with the desirability the desirability of alternatives-contrast function pictured in Fig. 1 (a). rules fuzzy Flakey’s implement reactive and simple to compute. The fuzzy rule format makes a local “greedy” method, gradient descent, that is highly it easy to write and debug simple control schemas, and it should also make it easier to learn or improve including schemas automatically. We have written rulesets for a dozen control schemas, ones for avoiding an obstacle, on. It is the responsibility gradient descent detect local minima and failure of the schema for crossing a door, for reaching a near location, and so schemas for which to only instantiate to is appropriate based on some global analysis, or to monitor schemas of higher-level methods (see Section 5). 2.5. Combining control schemas is often engaged of several control schemas. For instance, An agent coordination hallway while avoiding obstacles and moving The resulting overall movement in activities requiring its camera to keep track of some landmark. can be characterized by an appropriate combination of the simultaneous and a robot may be going down a activation A. Safiotti et al./Art#cial Intelligence 76 (1995) 481-526 491 control schemas. Two control schemas Dl and D2 can be combined one is the conjunctive for both D1 and D2. Formally, we simply combination: at each state, we consider actions intersect the fuzzy sets D1 and D2: in several ways. The most basic that are desirable (01 n &)(s,a) = Dl(s,a) @&(s,a). (6) The disjunctive combination of D1 and 02 is defined As it may be expected, each conjunct; and those of the disjunction the desirable trajectories of the conjunction in a similar way using U and @. are at most those of are at least those of each disjunct. Theorem 7. Let D1 and 02 be two control schemas. For any trajectory t in ‘T(S), (i) TrajD,no,(t) G Trajo,(t); (ii) Trajo,“o,(t) 3 Trajo, (t). control schemas Conjunctive function would be identically combination works well for combining that is, in every state there is some control that is desirable that are not for both schemas. conflicting; zero in that the combined desirability they conflict, When state. Unfortunately, this is more the normal case than the exception. For example, we may want to combine a control schema for going down the middle of a corridor, and one for staying away from obstacles. the the second one would favor first schema would prefer controls here is that each control controls schema has in general function should be considered only when appropriate. the first control schema can be sensibly applied only in situations where the space in front of the robot is free. When and we should disregard Formally, we restrict the robot, say, right. The key observation its own context of applicability, its desires. the area of competence In the state where the robot is facing an obstacle, In the previous example, that go forward, while its area of competence, and each desirability to a context as of a control the obstacle this schema is detected, is outside that turn schema follows. Definition 8. Let S be a set of states. A context on S is a fuzzy set of S. The notion of context C on S may be extended trajectories 7(S) state. in a natural way, i.e., a trajectory to a corresponding in the set of structure is as desirable as its least desirable C(t) = infC(s). .SEf Definition 9. Let D be a control schema on S and A, and C a context on S. The context restriction of D to C, denoted Dlc, is the control schema defined by the expression Dl’(s,a) = D(s a) 0C(s). , By using our interpretation context C, then the desirability of 0, we read Dlc as saying of a control a is given by D( s, a) ; otherwise, that if a state s is in the any 492 A. SaJiotti et al./ArtQicial Intelligence 76 (1995) 481-526 Fig. 4. The restriction of the Follow control schema to the context C. Trajectories (a) and (b) are both desirable; (c) is not desirable. control is admissible. Mathematically, we have Dlc( S, a) = D(s, a) if C(s) = 1, and = 1 if C(S) = 0. Hence, action must obey D in the states inside C, and is Dlc(s,.) free outside C. Correspondingly, the desirable trajectories for D*’ are those that obey D inside C, and are totally free outside, as shown by the following theorem. Theorem 10. Let D be a control schema and let C be a context on S. If t is a trajectory, then it is true that Trujolc(t) < inf[Traj,(t’) 0C(t’)l. t’ct That is, for t to be desirable for DLc it is necessary that all its segments inside C be desirable for D. Example 11. Fig. 4 shows a lane and three trajectories. Consider the control schema FoIIow~~. Trajectories (a) and (b) are both desirable for Follower, as they run inside the lane whenever in C; trajectory (c) is not desirable, as it fails to follow the lane while in C. Note that trajectory (a) would not be desirable for the unrestricted Follow. We can combine context restriction and conjunctive combination into a general com- bination pattern, called context-dependent blending. Each control schema is associated with a context, meant to identify the states where the schema is competent. Definition 12. Let D1 and D2 be two control schemas, and Cl and C2 two contexts. The context-dependent blending of DI and D2 under Cl and C2 is given by: Dlcl ” DlcZ 1 2 ’ 2.6. Implementation of context-dependent blending Context-dependent blending is implemented in Flakey by meta-level fuzzy rules of the type IFCjTHEN Rj, j=1,2 ,..., m, where each Rj is a ruleset implementing a control schema, and Cj is a context for R,i. Given a current state s, Flakey’s controller computes the function DRY for each Rj A. Safiotti et al./Ar@cial Intelligence 76 (1995) 481-526 493 ..- . . . . . _ -. -. $ -. --._ _: . . _. -- aI Fig. 5. Context-dependent blending of the KeepOff and Follow control schemas. according to equation (4) above. All the DR,‘s functions are then combined modulo the Cj’s contexts, according to Definition 12, and the resulting tradeoff function D,,, is fed to (5) to produce the control value F,,(s). Example 13. Fig. 5 shows an example of context-dependent blending of the Follow and the Keep-Off control schemas. The meta-rules used to encode the contexts are: IF spot-close-in-front THEN Keep-Off IF Ispot-close-in-front THEN Follow The upper part of the picture is a geometrical representation of the internal state of Flakey at one point during execution. Flakey is drawn in top-view heading right. The double lines and the crossed circle represent the locations of the lane to follow and of the spot to avoid. The lower part of the picture plots the evolution over time of the truth value of the contexts, hence the level of activation of the corresponding control schemas in the blending. The trajectory executed by F’lakey is desirable for the blending -in particular, it is desirable for the restriction of Follow to the context -spot-closein-front, as it leaves the center of the lane only when the context is false. It is important to emphasize that an implementation of context-dependent blending should jrst combine the component desirabilty functions, effectively forming a full preference function, and then chose one preferred control action from the combined function, The distinction can be seen in the following example (Fig. 6). In (a), the desirability function DI strongly prefers any direction from -90” to 90”; 02 weakly prefers a narrow angle from 50” to 60”. Their combination is dominated by D2, since DI is indifferent over such a large range. Suppose now that we first summarize each desirability function by choosing one preferred control, and then combine these controls (b) . Notice the result is very different from (a), and does not represent the best choice for movement. The context-dependent blending mechanism constitutes the main technical difference between our control schemas and other methods based on a weighted 494 A. Safiotti et al./Art$cial Intelligence 76 (1995) 481-526 (b) +\ = \ Fig. 6. Combination of full desirability functions (a) and of their representatives (b) . combination of local preferences (e.g., the potential field methods discussed in Section 6 below). 3. Behaving In this section, we study how control schemas can be “lifted” from the level of movements to the level of behaviors in an environment. Behaviors correspond to the concept of situated actions, and are normally associated with the accomplishment of goals. For example, the behavior of “following this wall” can be a way of reaching any one of the doors in that corridor, or a way of following a person. In this section we concentrate on developing the structure of behaviors, and the ways behaviors can be combined, or blended, together. We also touch at issues of relating behaviors to perception. In the next section we relate behaviors to goals. 3. I. Behavior schemas Our approach to lifting a specification of a type of movement to a specification of a behavior in an environment is by associating it with a context of execution and with a set of objects to operate on. For example, consider the movement type “extending the right arm”. If executed in the direction of a cup, and in a situation in which the cup is within the arm’s reach, this movement will result in the production of a behavior of type “hit a cup” (which in its turn may be a way of achieving a goal like “break a cup” or “win a prize”). In general, we characterize a type of behavior by a behavior schema: a specification of what movement should be performed with respect to which objects and under what circumstances. Definition 14. Let S be a set of (internal) states, and A a set of control actions. A behavior schema on S and A is a triple B = (C,D,O) A. Safiotti et al./Arti$ciai Intelligence 76 (1995) 481-526 495 Iprior knowledge i sensing c... . . . . . . . . . . . Environment actions 1 . . . . . . . . . . ..J Fig. 7. Our approach to embedding a controller in the environment. where C is a context on S, D is a control schema on S and A, and 0 is a set of object descriptors. The new element in this definition is the set of object descriptors. The control schema D and the context C operate on formal variables in the internal state, e.g., the direction for moving the arm. If the agent has to act with respect to an external object, these variables must reflect properties of that object, e.g., the position of the cup. We group the variables related to an object into a partial model of the object, called an object ’ Object descriptors are essential to link abstract specifications of actions to descriptor. physical execution (see Fig. 7). Suppose we want to execute the action “pick up cup A”. The identifier “cup A” does not have any meaning to the controller. So, we create a descriptor for this cup based on properties stored in the agent’s long-term memory (e.g., a map), and pass this descriptor to the controller. The controller then operates in an open-loop fashion with respect to the properties in the descriptor. Later, when the actual cup is perceived, we use the output from the perceptual system to continuously update the descriptor, effectively switching to a closed-loop regime. We refer to the process of keeping object descriptors coordinated with physical objects through perception as anchoring [ 371. Example 15. The Follow control schema introduced in Section 2.4 can be used to build a behavior schema for moving down a given corridor, provided that Flakey is in the ’ Note that each descriptor is specific to a behavior: it should include all and only the properties of the object that are relevant to that behavior. 496 A. Sajiotti et al. /Artificial Intelligence 76 (1995) 481-526 -. >._ . _. .. .*-- d al . ..-..a .:.-. (a) ._ - (b) Fig. 8. Anchoring a corridor descriptor to sensor readings for corridor-following. corridor and it is not blocked by an obstacle: ( [at(Corrl) A f 1 acing(OG)], Follow(Corrl), {Corrl, OG} ), where we indicate in parentheses a dependency on (the properties of) an object descrip- tor. The Corrl descriptor includes a lane that approximates the size and position of the actual corridor we want to follow. OG denotes the “Obstacle Grid”, a special descriptor used for obstacle avoidance that matches any object around Flakey. The context C is expressed in a logical form: in any state s, the value of C(S) is computed through simple geometrical reasoning, and from equations (1). Fig. 8 shows the internal state of Flakey while executing this behavior. Note that the state now includes the input from the sensors and the perceptual interpretations built from this input. In the picture, each small dot represents a sonar reading, indicating that something has been detected at that spot, and short segments indicate surfaces reconstructed from these readings. Initially (a) the position of the lane is set accordingly to an internal map. When enough sonar readings are gathered, Flakey’s perceptual routines infer the existence of two parallel walls, marked by “w”, and a corridor, marked by “c”. This information is used to update the Corrl descriptor (b), and Flakey’s motion now follows the actual corridor. The last example illustrates the role of anchoring in going from a type of movement, “go straight between two boundaries”, to a type of behavior, “follow a corridor”, by relating the two boundaries to the perceived walls. Note that executing the same type of movement on a road and anchoring the two boundaries to the perceived white lines would result in a behavior of type “drive in a traffic-lane”. (See [25] for a related approach to encapsulating behavior.) For each descriptor d, we assume we have a function Anchd:S+ [O,l] such that An&(s) measures the extent to which d is anchored in state s. In most practical cases, Anchd will be a binary function that returns either 0 or 1. Anch is extended to sets 0 of descriptors and to trajectories in the obvious way: Ancho(s) = inf Anchd(s) de0 Ancho( t) = infAnchd( s). sEt A. SaJiotti et al./Artijicial Intelligence 76 (199s) 481-526 497 We are now ready to define the meaning of a behavior. As we did for control schemas, we identify the meaning of a behavior schema B with the set of all trajectories that can result from executing B under the correct anchoring. We call these trajectories admissible. Definition 16. Let t be a trajectory in I(S), The degree by which t is an admissible execution of B is given by: and B = (C, D, 0) a behavior schema. .4dme(t) = TrajDlc(t) @An&o(t). That is, a trajectory t is admissible for behavior B if the descriptors 0 are anchored in t, and t is desirable for D whenever in the context C. By virtue of the anchoring of the object descriptors, this trajectory in the formal domain S corresponds to an actual trajectory in the environment. This means that we can analyze the properties of a behavior schema by looking at its admissible trajectories AdmB. 2 We may need to know if a behavior has admissible trajectories that lie in its context. Often, we even require that the behavior has arbitrarily long such trajectories. We measure the satisfiablility of a behavior by the following function (recall that 7n( S) denotes the set of trajectories of length at least n). Sat(B) = inf sup [Adme @C(t)]. n>O EZ#(S) (7) 3.2. Blending behaviors Complex behaviors can be created by composing, or blending, basic behaviors, using the context-dependent blending of desirability functions defined in Section 2. We distin- guish three different flavors of blending, depending on how the contexts are used in the combination. Definition 17. Let B1 = (Cl, Dl ,OI) and B2 = (C2, D2,02) be two behavior schemas. Then the following are behavior schemas: CONJ[B,;&]=( Cl nC2, DI nD2, 01 UO2) BLEND[&;&]=(Cl lJC2, DtC’ nDiC2, O1 U 02) cHAIN[Bl; B2] = ( C1 U C2, D;CL\cz n D;“, 0, u O2 ) . The conjunctive operator CONJ provides the simplest form of combination: it builds a more focused behavior schema that considers two control schemas simultaneously in their common context. The BLEND operator builds a behavior schema by applying ’ The situation is more complex. Anchoring does not link object descriptors with external objects, but with their perceptual images inside the agent. These images reflect the properties of tbe actual objects only as far as the perceptual apparatus is reliable. Accounting for the reliability of perception is a difficult issue that lies beyond the scope of this paper. 498 A. Safiotti et al./Art&ial Intelligence 76 (1995) 481-526 behavior each component special case of blending where the common part of the context. in its own context. And the second behavior the chaining operator CHAIN is a takes priority over the first one in Composite behaviors are implemented dependent blending of control schemas. of desirability expressed in the canonical form in Flakey using for context- In fact, by using min for @, any combination the mechanism functions produced by the CONJ, BLEND and CHAIN operators can be Dlc’ n Dlc2 n . . . n @I, I 2 (8) and then be implemented by meta-rules as shown in Example 13 above. Example 18. The simple corridor that there was no obstacle avoidance behavior built around behavior following behavior defined in Example 15 assumed in front of Flakey. We can blend this behavior with the obstacle to obtain a composite the KeepOfF control schema to go down a corridor while avoiding obstacles on the way: BLEND[ (at(Corr1) A Tfacing(OG), Follow(Corrl), {Corrl, OG}) ; (facing(OG), KeepOff( {OG}) ] where Corrl and OG are as in Example 15. Notice in the context of being at(Corrl), a wider context following two meta-rules: than the Follow behavior alone. This blending that the blended behavior applies that is, in by the is implemented irrespective of there being any obstacles, IF facing(OG) THEN Keep-OfF(OG) IF at(Corr1) A lfacing(OG) THEN Follow(Corr1) internal ( 1) shows Flakey’s in the next subsection.) Fig. 9 shows a run of this composite behavior on Flakey. (The Sense behavior will be discussed (d). state at moment (2) shows an external view of the environment where Flakey moves. (3) plots the level of activation of the three control schemas over time. Spikes in the activation levels are to the two snapshots caused by noisy sonar readings. of Fig. 8 above. Notice taken by Flakey after the corridor has been (b) . In (c) , the obstacle has been detected by Flakey’s sonar-s, and anchored anchored to take over those of Follow. Later, to OG. Hence, toward when (d), and guides Flakey to the one discussed the center of the hallway. The overall pattern of control is similar in Example 13; however, the anchoring of the internal variables used by the control schemas, the preferences the path is clear, Follow resumes this control now produces an effective behavior (a) and (b) correspond of KeepOfF begins in the environment. the new heading full importance through Instants Composition of behaviors plays a central role in our construction, and we will show how we can map plans generated in some standard way into composite behaviors, and hence into embedded controllers. However, not every composition makes sense. In par- ticular, one should be careful functions of two behaviors being com- i.e., they do not conflict in the same context. We measure bined be mutually consistent the mutual consistency of two behaviors Bt and B2 by the value of U(CONJ[ BI; B2] ). That long that are admissible that the desirability - is, Bl and B2 are mutually if we can find trajectories in their common context. for both behaviors arbitrarily consistent A. SaJiotti et al./Art$cial Intelligence 76 (1995) 481-526 499 (1) (2) . . . (a) (b) . . ‘.. (c) El ‘.’ ‘. . - (4 0 Fig. 9. Context-dependent blending of corridor-following and obstacle-avoidance. 3.3. Behavior and perception The object descriptors constitute a model of the real-world objects that are relevant to the control task: the anchoring process keeps them coordinated with the input coming from the perceptual system. In practice, we have found that the introduction of object descriptors greatly simplifies the design of behaviors, by allowing us to decouple the problem of control from the problems of interpreting noisy sensor data. Our behavior- writing methodology in Flakey has been to first write small rulesets for elementary types of movements based on simple descriptors, like follow a line, or reach a location; and then focus on the strategies to keep these descriptors anchored to the right features in the environment. The resulting behaviors often proved to be more robust than purely reactive controllers. For example, our wall following behavior can produce useful movement even when the actual wall is temporarily obscured. Intuitively, the controller follows the generic direction marked by the wall, rather than the actual contour of the wall, and this direction is registered from time to time through perception. Notice that anchoring is normally recomputed many times during action execution (possibly at every control cycle), providing a closed-loop response whenever the relevant sensor data are available. When data are not available, e.g., when starting the behavior, the descriptors act as assumptions originating from the last anchoring, or from prior knowledge. There are several places where the introduction of the object descriptors and of the anchoring mechanism helps us to better understand the points of contacts between perception and action. A first such point of contact is focusing the perceptual system. Gathering a full geometric picture of all the objects in the immediate environment 500 A. Safiotti et al./Artifcial Intelligence 76 (1995) 481-526 is time consuming and impractical given current computational limitations, and leads to slow reactions in dynamic situations. However, particular behaviors may need only part of the full perceptuat information available. As the object descriptors list all the perceptual properties that are relevant to a given behavior, their content can be examined by the perceptual routines to focus attention on those features of the environment that are relevant to the behavior. A similar approach has been adopted by Arkin in his perceptual schemas [2,3]. A second point of contact is the need of some behaviors to keep track of the anchoring of their objects. For example, a behavior for performing some precise manipulation on a workpiece should make sure that the workpiece has been perceived and anchored. This can be done by including the value of An& in the context. More interestingly, behaviors may actively try to achieve and maintain the anchoring. One way to do this is by issuing commands to the perceptual apparatus, e.g., turning a camera or activating an interpretation routine. Another way is by preferring movements that are helpful to the perceptual process. Example 19. We discuss the role of the Sense behavior in Example 18 above. It is easier for perception to find the corridor walls if the robot moves slowly along the corridor in a linear fashion, without turning. Over several seconds, the sonar sensors will approximate a long synthetic aperture, and generate a reliable reading of the wall location. The Follow control schema is designed to move fast along the midline of the corridor: under some conditions (reflective or discontinuous walls), this may make wall recognition more difficult. To remain anchored in these situations, the Follow schema is blended with the following one: (yanchored(Corrl), Sense(Corrl), (Corrl}). When the corridor descriptor is still unanchored, as in Fig. 9(a), the Sense control schema blends in and slows down the robot. Once the corridor is recognized by the perceptual subsystem, as in Fig. 9(b), the context becomes false, and the Follow behavior can proceed freely. Notice that if Corrl later becomes unanchored, e.g., because a wall is occluded by obstacles, the Sense behavior will become active again and help Flakey to recover anchoring. In complex situations, simple strategies like the one above may not work, and higher- level decision-makers must be invoked to figure out how to recover the anchoring, or else form a new plan to achieve the desired goal (see Section 5). The advantage of using sensing behaviors that act to maintain anchoring is that they are simple and can be extremely reactive. 4. Goals We now introduce goals as the final element of our construction, and study the relation between the execution of behaviors and the satisfaction of goals. The fundamental connection between goals and behaviors is formalized by the relation of goodness: a A. Safiotti et al./Artijicial Intelligence 76 (1995) 481-526 501 behavior is good for a goal if, in all trajectories produced by executing the behavior in its context, the goal holds. Additionally, we prove the main formal results of the paper, two composition theorems: the conjunctive composition of two behavior schemas is a complex control that satisfies the conjunct of their goals; and the chaining composition of two behavior schemas is a complex control that satisfies the goal of the second one in a wider context that includes the context of the first behavior. We will see in the next section how these results can be used for means-end reasoning and planning. 4.1. Representing goals Definition 20. Let S be a set of states. A goal on S is a fuzzy subset of the set of trajectories I(S) . Given a goal G and a trajectory t, we read G(t) as the degree by which t is a desirable trajectory for the goal. Specifying goals as sets of satisfactory executions is customary in control theory [ 41, but is a less common in AI approaches to planning. One advantage of this choice is that both goals involving the achievement and goals involving the maintenance (or, for that matter, the avoidance) of some condition can be expressed in the same form. For example, if P and Q are (multivalued) predicates, we can define the following goals: ACHIEVE[P](t) =SUpP(s) set MAINTAIN[P](t) =infP(s) se ACHIEVE![P](t)= SUP 0(&k inf P(Sj) i<j<n SEQUENCE[P,Q](t)= SUP SUP [P(Si) @Qe<Sj)] O<i<n i<j<n (9) (10) (11) (12) where t = (so,.q,... Intuitively, ACHEvE[ P] is the , s,,) is any trajectory in T(S). goal of making P true: any trajectory that at some point makes P (partially) true is a (partially) good trajectory for ACH~EVE[P]. MAINTAIN[ P] is the goal of having P true all the time. ACHIEVE! [ P] requires that P be eventually true, and stay true until the end of the trajectory. And SEQUJZNCE[P,Q] is the goal of making Q true after having made P true. Example 21. Fig. 10 gives examples of trajectories that satisfy different goals. Trajectory (a) satisfies ACHIEvE[at( COrrl) because it leaves the corridor. but not ACHIEVE![at(L)]. 1, but not MAINTALN[ at( Corrl) ] or (b) satisfies ACHlEvE[at( L)] (c) satisfies all of the three ACHIEVE![at( Corrl)], and MArNTAIN[at(Corrl)], goals. It is useful to have a measure of a goal’s satis$abiZity: Sat(G) = sup G(r), ET(S) (13) 502 A. Sagiotti et al./Art$cial Intelligence 76 (1995) 481-526 Cord Ib) - Fig. 10. Trajectories that satisfy different goals meaning that there is some trajectory that can (partially) satisfy the goal. We combine goals by means of the usual set-theoretic operators under the multivalued interpretation wants to reach a certain given in Section 2.1. For example, we can express the goal of a robot that location El while avoiding another location 22 by: ACHIEVE[at(lt)] llMAINTAIN[+(/2)]. of goals is a way to trade off several goals by preferring Conjunction that best satisfy all of them. The characteristics t-norm employed. For example, if we use the min, we require be satisfied at least at the level cr in order for the conjunction a; and if we use the product, we allow for a decrease by an increase the trajectories of this tradeoff depend on the particular that each of the conjuncts to be satisfied at the level to be compensated in one conjunct in the other one. as we did for control Finally, schemas and behaviors, we introduce restriction context avoiding obstacles only matters when there are obstacles nearby. to a certain context-for to focus a goal example, the notion of the goal of Definition 22. Let G be a goal on S, and C a context on S. The context restriction of G to C, denoted GIC, is the goal defined by GIC(t) = inf[G(t’) f’Q 0C(t’)]. In words, the goal GiC is satisfied by any trajectory c (and moves freely outside). that satisfies G whenever it is inside 4.2. Behaviors and goals Executing a behavior may be a good way for satisfying some goals, and not others. a wall- for example, in a corridor. the central notion of goodness to relate goals and behaviors. A behavior the trajectory of the behavior, In general a single behavior may satisfy more following behavior can achieve We introduce is good for a goal if any admissible goal. the goal of being at any given position in context, satisfies than one goal- Definition 23. Let B = (C, D, 0) be a behavior schema, and G a goal. The goodness of B with respect to G is defined by: A. Safiotti et al. /Artificial Intelligence 76 (1995) 481-526 503 Good( B, G) = sup inf [ GLC (t) 0 AdmB (t) I. n>O fEI,(S) Recalling non-multivalued form as follows: the reading suggested in Section 2.1, we can re-write Definition 23 in a Good(B,G) iff 3n > 0 [Vt E %(S) (Adme > Cl”(t))]. for G if there Thus, B is a good behavior is a number n such that any admissible execution of B that is longer than n satisfies G. The reason we disregard short trajectories is that they may not give the behavior enough one-step fail to satisfy the agent, although time to accomplish G. For example, a from the goal to reach a point distant trajectory will obviously in the right direction. it may be going The subtle part of this definition the control schema D whenever lies in the interaction of the contextual conditions of for B if it is anchored that a trajectory 16). When a trajectory is admissible, and the trajectory can vary arbitrarily. B with the goal predicate..Recall and it follows is out of the C context, any control These trajectories if some part of a above requires trajectory enters that this part also satisfy G. Hence, a behavior B is Good for a goal if, for every possible is never entered execution of B, either or formalizes G is satisfied. s The following to the restricted goal G*‘. However, then this part must obey D; the definition (or never entered this reading of Good. is admissible in C (Definition long enough), are indifferent the context, the context theorem Theorem 24. Let B = (C, D, 0) be a behavior schema, and G a goal. Then, Good( B, G) 2 (Y iJ; and only iJ there is a positive integer N such that, for any trajectory t of length greater than N, Adms(t) @C(t) 6 G(t) 8 a. conditions One consequence are unsatisfied. This is reasonable, to work under a given set of assumptions. Unfortunately, that a behavior will reliably to work when its contextual are always built that there is no way in general in any environment: we can always from staying means have behaviors if it goes out of context. We will come back on these issues of our definition of Good is that we should not expect a behavior as behaviors this implies satisfy a goal that keeps an agent the goal. What this to an agent needs to change behavior imagine a malign environment long enough, and hence from achieving in dynamic environments, the ability that work in very general contexts, and/or is that, to be successful in the next section. in the context to guarantee in practice Finally, we note two useful monotonicity properties of Good. Theorem 25. Let B = (C, D, 0) and B’ = (C’, D, 0) be two behavior schemas, and G,G’ be two goals. Then, if C’ C C and G c G’ (i) Good(B,G’) (ii) Good( B’, G) 2 Good( B, G) 2 Good(B,G) 3 Interestingly, this notion is similar to that of goal agent keeps a goal until it is achieved, or until it is impossible. to say whether a good behavior will ever actually achieve the goal. in Cohen and J_evesque’s theory of intention It has the same consequence: [ 71: an it is impossible 504 A. Safloni et al. /Ar@cial Intelligence 76 (1995) 481-526 That is, goodness is preserved by relaxing the goal and by restricting the context. In the rest of this section, we will show how to build behaviors that are good for stronger goals and in larger context. 4.3. Compositionality We know from the last section how to build complex behaviors by composing simpler ones. An important question is whether our composition operators preserve, in some way, the goodness of the component behaviors. Theorem 26 (Conjunction of behaviors). and GI , G2 be two goals. If Good( B1, GI ) 2 LT and Good( B2, G2) > fl, then Let B1 and B2 be two behavior schemas, Good(CONJ[BI; B2], Gl fl G2) b min(cw,P> where n is taken with respect to the mint-norm. Theorem 26 tells us that we can build behaviors that address more goals simulta- neously by conjoining behaviors that are good for the individual goals. As a typical example, a behavior for following a wall can be joined with a behavior for going fast (e.g., one that always prefers high speeds) to obtain a behavior to follow a wall quickly, It is important to note that Theorem 26 only says that the conjoint behavior is good for the conjoint goal: it does not guarantee that either conjunction make sense. In practice, when conjoining behaviors, we should make sure that two consistency conditions are verified. First, that the goals are mutually consistent, that is, the value of Sat(G1 n G2) is “reasonably” close to 1. For instance, suppose G1 and G2 are the goals to reach two far apart locations. Then, G1 n G2 is empty, and promoting this goal will fail to promote each of the two conjuncts individually. The second condition is that the control strategies of the two behaviors should not interfere. Consider a room with two doors, and two behaviors, B1 and B2, for entering through the different doors. Each behavior is individually Good for its own goal, hence, the conjoined behavior CONJ[ Bl; Bz] is Good for the conjunction of the two goals, that is, entering the room. Unfortunately, there is no trajectory that is admissible for both B1 and B2, and the behavior cannot work properly. This situation can be detected by measuring the mutual consistency of the two behaviors, given by Sat( CONJ[ BI; B2] ) ) . Mutual consistency can be analyzed when designing (or planning) a combination; or it can be monitored during execution (see Section 5). The following theorem gives a necessary condition for consistency that can be tested at planning time: if BI falsifies the context of Bz, then B1 and B2 are inconsistent. Theorem 27. Let B1 = (Cl, Dl, 01) and B:! = (CT, D2.02) be two behavior schemas. If Good( BI, ACHIEVE[ X2] ) > (Y then Sat(CONJ[ Bl; Bz] ) Q l/2 0 a. Moreover; if a > 0, then Sat(CONJ[BI;&]) < 1. The conjunction theorem allows us to build behaviors that satisfy stronger goals in a narrower context. We would like to also have a means of satisfying a goal in a wider A. Sajiotti et al. /Artijcial Intelligence 76 (1995) 481-526 505 context. Chaining of behaviors can provide achieves cooperate contexts. to the (9) above) the goal of the second behavior (according to satisfy this: if the first behavior in the combination the context of the second one, the two behaviors in the union of the individual Theorem 28 (Chaining G a goal. Then, if Good( BI , ACHIEVE[ CZ] ) 3 LY and Good( B2, G) 2 p, then Let BI and B2 be two behavior schemas, and of behaviors). Good(CHAIN[B1;B2], G’) b min(a,/?), where G’ = GICZ U SEQUENCE[ C2, Cl 1, C2 is the context of B2, and U is taken with respect to the max t-conorm. the goodness hypotheses, CHAIN[ BI ; B2] for G see below) under the more general context C;! U Ct : intuitively, we use to reach the context C2 of B2; and then use Bz to satisfy G. What for is that behavior chaining can be correctly used as a means is a good behavior in Ct\C2 in practice the effectiveness of the agent’s motor skills beyond their (normally limited) That is, given (actually, G’ - BI whenever this means extending context. There the parts of the execution is a technical caveat here. The chaining theorem says that CHAIN[ BI ; B2] is goal G’. The first component of G’ is the restriction of G to Good for the disjunctive to satisfy the context C2: only G. As B1 makes sure that the C2 context to the goal of eventually satisfying G. For example, suppose a robot that has a behavior B:! to in a room by switching a heater on and off, B2 requires maintain to the room from anywhere the robot to be in the room. If B1 is a behavior in the house, the for eventually fixing temperature then CHAIN [ B, ; Bz] will be a good behavior in the room if the robot is anywhere that are in C2 are requested a fixed temperature this corresponds in the house. is eventually for moving entered, this is to require for many robot navigation behaviors. Making sure that the behavior enters the context C2 is not enough should also remain in C2 long enough to guarantee that (the n parameter that B1 be good for is there out of C2. there are it keeps oscillating between Cl and C2; or it satisfies G. This is trajectories either satisfy G, or it will satisfy G: the behavior in the definition of Good). One way to guarantee the stronger goal ACHIEVE! [ C2] , but this may be unrealistic unrealistic is no way in general If a trajectory only the meaning of the second component of G’: admissible go back to Ct. Oscillatory behavior can be difficult theorem gives a necessary condition each behavior promotes in some domains-it If we require simple achievement, stays in the context Ct U C2 of the chained behavior, however, the context of the other one, their chaining will oscillate. that trajectories will not go systematically to detect and to avoid. The following that can be tested at planning two possibilities: to guarantee for stability time: if Theorem 29. Let Bl, B2 be two behavior schemas, and Cl, C;! their contexts. If Good( Bl , ACHLEVE$ CZ] > > a and Good( B2, ACHIEVE [ Cl ] ) > p, then Goo~(CHAIN[BI;B:!I, SEQLJENCE[C;!,C1]) > min(cY,p). 506 A. Sajjiotri et al./Arl@cial Intelligence 76 (1995) 481-526 ‘;” t: 8 Follow(Corr-1) Cross(Door-5) Fig. 11. A robot in an office environment. The dashed areas indicate the contexts of the Follow and the Cross behaviors. Example 30. The following example ation in Fig. 11, where Hakey, sitting Flakey has a control when the robot behavior schema, called Cross, that results is near to that door. That is, the following behavior (at some level, say 0.8) for the goal ACHIEvE[at(Room-5)]:4 in practice. Consider illustrates chaining the situ- in corridor Corr-1, needs to reach room Room-5. in crossing a door if applied is a good schema BI = ( near( Door-5), Cross( Door-5), {Door-5) ) where the Door-5 object descriptor of B1 is not general enough for Flakey schema seen in Example 15 results corridor. That is, to achieve is kept anchored to Door-5. Unfortunately, to include the present situation. Fortunately, this context. When applied inside a corridor, in Flakey eventually being near each object the context there is a way the Follow behavior in that B2 = (at(Corr-1), Follow(Corr-1), {Corr-1)) theorem (say at the level 0.9) for achieving near( Door-5) is a good behavior The chaining behavior, CHAIN[ &; BI 1, that leads Flakey anywhere along Corr-1. More precisely, and assuming we have in our environment. tells us that we can chain B1 and BP to obtain a composite in the larger context of being that B1 and B2 do not oscillate, into Room-5 Good( CHAIN[ &; B,], AcHIEvE[at(Room-5)] ) > min(0.8,0.9) = 0.8. In the last example, we have reasoned by a form of goal-regression that the CONJ and CHAIN operators can be easily married based the context of applicability. We will see in the to classical plan -although on the less common notion of extending next section generation concepts and techniques. 4 We assume that the information about which basic behaviors are good for which goals, and to which degree, has been given by the designer of the behaviors (see next section). A. Safiotti et al./Artijcial Intelligence 76 (1995) 481-526 507 4.4. Blending reactive and goal-oriented behavior A central problem for autonomous intervene to promptly avoid unforeseen is how to combine purposeful ronments (possibly unforeseen) be prepared robot should reliably gation. Context-dependent good method for combining to write reactive goal-achieving goal-achieving are no obstacles. Then, we write a behavior tion hold when they do not, for instance, a behavior chaining the goal under relaxed assumptions are only convex obstacles. that assumes a somehow behavior events in the environment. For instance, an assembly in uncertain agents operating activity with sensitivity and dynamic envi- to robot should if an assembly piece falls from the table; and a mobile navi- or moving obstacles during goal-oriented to be a and responsiveness in our experience blending of behaviors has proven reactivity and purpuseful action. Our practical methodology behaviors has been as follows. We first write a simple that there that tryes to make (some of) these assump- to go around convex obstacles. By that achieves that there in our example, under the assumption ideal context, for instance, - the latter behavior with the former one, we obtain a new behavior (like most behaviors) the problem seen in Example 18 from the present perspec- for Example 31. We reconsider tive. The Follow behavior going down a corridor provided that there are no physical impediments. In environments with obstacles, hand, is a good behavior a situation where there are no more physical we can obtain a composite behavior there are obstacles or not: it will easily go out of context. The KeepOfF behavior, on the other that is, roughly said, for bringing about these behaviors, that is Good for going down a corridor whether impediments. By chaining to be a good behavior for avoiding obstacles, is written CHAIN[ (facing(OG), Keep-Off(OG), ( at( Corrl) , Follow( Corrl) {OG}); , { Corrl} ) 1. is equivalent to the one in Example 18 (by ignoring This behavior due to the blending mechanism, with decreased weight, during Thus, pass it will choose compatible with the pursuit of the goal of Follow. expressed by Follow is still considered, (see the plot in Fig. 9). the obstacle avoidance maneuvers if the robot has a choice between different ways to avoid an obstacle, e.g., to that is most it on the left or on the right, the Sense). Note that, the desirability the avoidance strategy is widely used form or another, Local combination literature in the of behaviors, of some and reactivity. One ubiquitous problem robotic for mixing goal directedness in the combination. Our approach gives is the emergence of points of local equilibrium in the us some control over these phenomena behaviors that may conflict, but local minima and oscillatory behaviors can still emerge in some cases. Global path-planning methods are better when two conditions hold: (a) the obstacles are complex, and (b) sensing gives an accurate picture of this complexity. are not incompatible with our behavioral approach: an agent Path-plannning can be equipped with a path planning it with for Flakey using other behaviors when needed. We have implemented by allowing us to put disjoint contexts and following behavior, such a behavior and combine techniques 508 A. Safiotti et al./Artificial intelligence 76 (1995) 48I-526 field is generated [30]. A gradient to the robot. At each point, to Payton’s the goal position the robot should travel in the shortest path to the goal, A path-following similar a method grid search from direction schema prefers is useful minima, provided obstacles between Keep-Off behavior at intervals by a the control that are close to those indicated by the field. This behavior local from for coping with convex configurations that enough perceptual to show all the the robot and its goal. It can also be blended with the more reactive to take care of newly sensed obstacles. of obstacles and escaping has been gathered the gradient gives the directions information 5. Planning introduced An intelligent to autonomously the relation between situated agent needs the ability these skills and the achievement develop new strategies, or plans, for solving new tasks. This ability requires that the agent reason about its own of goals motor skills, and about in under certain conditions. We have already sections: we have defined behavior schemas, contexts and goals; we have the previous shown how to compose simpler behaviors into more complex ones; and we have studied the main relation between behaviors and goals: the Good predicate. Moreover, we have level, these ingredients are at the right level claimed this claim by of abstraction to satisfy given goals by showing how we can automatically using customary planning generated behaviors as behavior-a2 plans to emphasize their double nature of planned activity and executable controls. to be used by a reasoning process. In this section, we justify techniques. We refer to the automatically generate complex behaviors that, while grounded all the necessary in the physical ingredients 5. I. Reasoning about behavior to reconcile our formalism with the more standard representations used in In order the planning i.e., one for which we have an implementation template. For example, in Example 30. tradition, we collect all the information -in is the template the following about a basic behavior schema- a data structure called a behavior to the Cross behavior used relative Template Parameters: Precondition: Achieve: ControlSchema: Goodness: : CROSS door( (and at(?p2) Cross(?d) 0.9 connects(?pl,?d,?p2) at (?pl) near(?d) (not obstacle)) What this template says is that if d is bound the descriptors of the two places connected by that door, then the behavior schema to a descriptor of a door and pl and p2 to B = ( (at(p1) A near(d) A Tobstacle), Cross(d), {d,pl}) is such that A. Sajiotti et al. /Artijicial Intelligence 76 (1995) 481-526 509 Template: FOLLOW Parameters: corridor(?c), in(?c, ?p) Precondition: (and at(?c) (not obstacle)) RunCondition: anchored Achieve: near(?p) ControlSchema: Follow(?c) Goodness: 0.9 Template: SENSE Parameters: corridor(?c) Precondition: at(?c) Achieve: anchored ControlSchema: Sense(?c) Goodness: 0.7 Fig. 12. Templates for following and for anchoring a corridor, Good( B, ACHLEVE[ at( p2)] ) > 0.9. This value 0.9 measures the designer’s confidence the appropriate that the behavior will achieve conditions. reflects time it is activated under the properties of the corresponding to make sure that the knowledge encoded It is the responsibility goal every in a template of the basic behavior’s designer behavior schema. We can use tem- correctly of templates, plates conjunction in Section 4.3 give us a lower bound of the goodness of the resulting plan given the goodness of the basic behaviors. and chaining of behaviors. The theorems to build behavioral plans by applying three operations: of behaviors, instantiation its shown the template the environment in Fig. 11, and consider Example 32. Recall Follow and Sense behaviors instantiate situation, we CHAIN an instance of the Follow template a condition namely, of Sense. The resulting plan is given by for the the templates the goal to reach Room-5, we for Cross shown above. As the context is not true in the present in this is running, instance that the corridor be anchored. Hence, we CONJ Follow with a concurrent that should be maitained while template. The RunCondition in Fig. 12. Given the behavior indicates CHAIN[ CONJ[ Follow(Corr1); Sense(Corrl)] ; Cross(Door-5) 1, where we name behaviors after their control schema and object descriptor. illustrates that guide the principles schema B = (C, D, 0) the generation of a behavioral The last example in state s who has a goal G. We know from Theorem 24 that, plan. Suppose an agent if a behavior long execution of B that is in the context C will satisfy G. Hence, the agent will be interested there is an in building admissible in the context C. Formally: ( 1) B is a good behavior through s; and (3) a behavior B such that: t of B passing execution is good for a goal G, then any sufficiently t lies entirely the execution for G; (2) Definition 33. Let B = (C, D, 0) be a behavior degree by which B is an effective plan for G in s is given by: schema, G a goal, and s E S. The It is difficult in general to evaluate the value of Plan[ B; G; s] prior to execution. We know how to compute goodness, but establishing the truth of the other two conditions 510 A. Safiotti et d/Artificial Intelligence 76 (1995) 481-526 in a conjunction trajectories- should not falsify is only necessary. Computing is much harder. Theorem 27 gives us a condition the existence of admissible behavior this condition impossible and just above assumes no obstacles. environment in the picture, but would go out of context if an obstacle appears in front of the robot. In practice, the conditions plan becomes of Definition 33 during execution ineffective, and then modify environments. in dynamic it. for the consistency of B -hence that can be verified on the templates: for each the context of the other one. Unfortunately, the truth of C(t) in general, in Example 32 For example, the is problematic the plan least) 0.7 given (and then become ineffective) the agent will have to monitor in order to detect situations where the It is effective at a degree of (at 5.2. Pre-planned behavior Our first experiment in the automatic generation of behavioral plans makes use a that is good for the given goal, a small planner includes to generate a behavior form of goal regression the starting state. We have implemented simple and whose context that is for Flakey based on a simple strategy: we start from an instance of a behavior that can achieve good for the goal, and then enlarge the context of the combined behavior covers that context. We iterate to have a the current in its pre-computed template, we use a correponding it. As discussed above, care should be taken that the behaviors so conjoined be consistent. We currently delegate the set of situations If a behavior has a conjunctive its context by chaining behaviors in general, available.) the detection of conflicts to a runtime monitoring for which we want (or, response the process until of behaviors precondition conjunction process. 5 to achieve state Fig. 14 shows a part of the office environment this map and the goal to reach Room-5, Flakey’s planner generated used in our experiments. Flakey has annotated with some approximate metric data. the behavioral form in Fig. 13. Each node in the plan represents a behavior schema. to the component the names of the descriptors, and the control is replaced by a pointer a complex blending of the control started by the Still behavior, and extended situation. The Cross behavior the one used above, and requires implemented that the robot in schemas its context until is more in Flakey is (approximately) the context, in a graphical the control schema a topological map of this environment, Given plan shown schema, and lists For composite behaviors, behaviors. The overall the leaves. 6 The planner it covered restrictive facing Cross. Note the use of the Keep-Off behavior situations where there are obstacles around. the door to cross. Correspondingly, the starting than tree represents the planner has chained a Face behavior the applicability to of the plan to to extend Fig. 14 shows an actual execution of this plan, along with a plot of the temporal evolution of the level of activation of the basic behaviors in the plan. Each behavior is 5 We arc also exploring harmful interactions at planning time. the use of SIPE [ 47 ] for building behavioral plans. SIPE uses “plan critics” to detect 6 Recall that, by using min for @, every combination can be written blending mechanism presented implemented by the context-dependent context with truth value of the context is computed by propagating a unitary value down the tree, and ANDing (through min) in that node. Fig. I3 shows the values SO computed in the situation g below. in the canonical form (8). and then in Example 13. The truth value of each it at each node A. Safliotti et al./Artijicial Intelligence 76 (1995) 481-526 Fig. 13. A behavioral plan for reaching Room-5. from together, conditions its contextual that have been chained the activation of the first behavior the conditions that sequencing are verified. For is expected for the activations of the second one, like in (d), and around is not determined by of an internal program counter, but it of the behaviors with the environment. Also notice that in the usual sense: they are deactivated when their context a new 19) ; activated when, and to the extent by which, behaviors to produce (g). Note emerges the interaction behaviors do not “terminate” become corridor and Flakey after (e), Flakey may can move more true. lose a priori Flakey ordering the between Sense behaviors. Finally, note the interaction between purposeful behaviors and reactive (d), obstacle avoidance after (c) and around later on). For example, when entering it by using (but may be reactivated to anchor track of the corridor, and the context of Sense(Corr-1) false (a,d), Flakey needs If, however, a wall becomes occluded then slows down until re-anchoring the executions of the Follow and the walls are anchored, Sense a few feet, (b). could have been established (f). Clearly, no reasonable the Sense behavior gradually becomes is deactivated, (e) and (g). (Example traveling occurs freely 5.3. Monitoring Generating a plan for future action or not this plan will still be adequate when coupled with a monitor whose task is to detect when the plan fails to satisfy the problem of whether it is executed. A behavioral plan can be the three brings about inevitably 512 A. Sapotti et al. /Artijcial intelligence 76 (1995) 481-526 Room-4 Room-5 Door-4 L 0. Door-i\ I : (a- Keep-OffCllg~ Stil1~Goa.l~ Cross~lklor-5) FaceCOoar-51 FollouKorr-13 FollowKorr-2) Ssnse{lkrr-2) Sense(brr-1) Fig. 14. Sample execution of the plan in Fig. 13. conditions problem. using simple computations in Definition 33, and correct it by introducing other behaviors Interestingly, we can compute an upper bound of the effectiveness to remedy the of a plan based on the current state. Theorem 34. Let B = (C, D, 0) be a behavior schema and G a goal. Then, for any state s E S. Plan[B;G;s] < Good(B,G) @min[ infD(s,a),C(S)]. &A A. Sajiotti et al./Art$cial Intelligence 76 (1995) 481-526 513 There are three possible sources of a loss of effectiveness, corresponding to the three conditions in Definition 33. Condition ( I), on the value of Good, can only become false if the goal is changed in some way. Condition (2) may fail if the plan contains an inconsistent conjunction of behaviors. When this happens, the agent gets into a stake state where the desirability function assigns a low value to all possible control actions, causing the value of inf, D (s, a) to drop. Failure of condition (3) is the most common cause of failure of a plan: we have reached a situation for which the plan does not have any suggestion, so the value of C(s) drops and all the behaviors have a low activation level. The solution is to extend the context to cover the current state. Often, this can be done by simply adding some new behaviors that achieve the context C of the original plan, 5.4. Run-time deliberation Monitoring is not the only reason to push at least part of the deliberation process into execution. A generative planner like the one discussed above makes choices about which behavior to invoke for a given goal. Often these choices are better made at execution time, since there is more information available. For example, a robot encountering an obstacle in a hallway might choose to go around to the left or right, or gather more information before making a choice, depending on how close the obstacle is to one side, how much of the obstcle can be seen, and so on. Instead of generating a large, contingent plan to take care of every possible case, the planner could produce an initial partial plan, expecting to fill in behaviors only when it becomes necessary to satisfy an impending goal. One can think of runtime deliberation as tactical planning: limited deliberation using rehearsed procedures for circumscribed tasks. Systems of this sort are called reactive planners. For our experiments, we adopted the Procedural Reasoning System (PRS) of Georgeff and Ingrand [ 151, whose main element is the intention schema, illustrated in Fig. 15. An intention schema is a finite-state machine whose arcs are conditions to test (?P) or to achieve (!P). The FS machine represents a limited strategy for achieving the goal of the schema. A sequence of behaviors is invoked by traversing the branches of the schema. A branch with a condition may be taken if the condition is satisfied. A branch with an achivement predicate causes the PRS interpreter to search for another schema whose goal matches the predicate, and invoke that schema. Behavior templates fit into this process as the lowest-level intention schemas, i.e., those that lead to action. The Achieve slot of the template can be matched to an achievement arc, and the behavior invoked by the interpreter to make the predicate true. Example 35. We implemented two strategies for making decisions during obstacle avoidance in corridors. In the first, we use a simple method for deciding which way to go around the obstacle. If the obstacle seems much further on one side, an immediate decision is made to go around on the other side. This decision may be the wrong one; as it moves, the robot will see more of the obstacle, and if it is blocked the opposite side is chosen. The second strategy is to acquire more information before deciding which way to move. On first contact with an obstacle, the robot “wiggles” to bring more of 514 A. SuJiotti et al./Artijicial Intelligence 76 (1995) 481-526 goal: AVOID-OBSTACLE ?OBSTACLE-LEFT / I ?OBSTACLE-MIDDLE !WIGGLE I \ ?OBSTACLE-RIGHT !KEEP-RIGHT !KEEP-LEFT Fig. 15. A PRS intention schema. to use is made by a higher-level its sonars strategy an obstacle corridor up and down. In (a), more of the obstacle comes into view; (b) shows an extended to bear on the obstacle, and then makes a decision. The decision on which triggered by the presence of a is patrolling then corrected as (Fig. 15). Fig. 16 shows this schema at work while Flakey the wrong decision is made initially, intention schema run. (possibly complex) behaviors based on to choose an can be performed Intention schemas are a way of instantiating to write strategies for different navigation the current situation. A limited amount of deliberation appropriate schemas ing into/out and even a complete plan execution and monitoring tention schemas behaviors. the execution that examine schema when more than one will satisfy a given goal. We have used intention tasks: moving around obstacles, mov- of rooms, deciding when to locate landmarks needed for self-localization, system. For the latter, we write in- state of the plan and its currently-executing 6. Related work The field of planning and control has burgeoned in the last few years, and many in the area of reactive planning. The work we have new ideas have emerged, especially presented owes much to previous work, and we have been influenced by methodologies and specific systems. draw attention controllers. In this section we give an overview of the points of contact, and to complex features of the multivalued logic approach to the distinct A. Sajiotti et al./Artijicial Intelligence 76 (1995) 481-526 515 - ,_,;:,:::::.~.:..:..:;:: “’ *.... IX;;.,i,:.:: “. .’ ., ., :: o : c: ;:; ,,,,,,,,, ;,:, .I. : ;:l; L ‘g r--l (b) T Fig. 16. Two runs of the obstacle-avoidance intention schema. 6. I. Methodologies Both the subsumption architecture and Kaelbling that perform complex ated automata of Rosenschein embedded agents in developing decomposing are in conflict with the spirit of these methodologies, perception troller. the complex behavior methodology, complex behavior and analogical representations into the composition of Brooks and his students [6,9] [ 17,331 are methodologies tasks. In part we have borrowed especially the subsumption of simple behaviors. and the situ- for producing these idea of In part we from in preferring explicit model-based of the world as part of embedding the con- theory the environment i.e., that attempts The theory of situated automata to perform a task by considering is an abstraction fashion, is constructed agents by representing automata Situated automata in that it makes no commitment analogical The key observation contain just enough environment; theory of internal using the techniques design actually accomplishes like to be able to synthesize and this information architecture the world to an internal for constructing is a formal methodology of the agent, away from the traditional planning state that represents embedded its task, and its capabilities. An [ 181. these specifications approach in an to model surfaces, recognize objects, and so forth. is that the state of the agent should tasks of the agent in its form. Situated automata to any kind like to be able to prove, complex controller we Further, we would their goals. it just makes no commitment that provably accomplish to accomplish the specified need not be in an analogical Ideally, we would that any particular its task in the intended environments. complex controllers or representation. of situated automata, of situated automata information is not incompatible with our approach; theory 516 A. SaJiorti et al./Artijicial Intelligence 76 (1995) 481-526 But the current state of situated automata theory is not developed enough to satisfy an ambitious program. Its main practical success has been a suite of development tools: one of them, GAPPS, has been used to generate controllers for mobile robot navigation; we discuss it below. such The subsumption architecture has many points in common with situated automata theory, but without the formal emphasis of the latter. It is a task-oriented methodol- ogy for constructing agents. Each task is accomplished by a behavior, which integrates sensing, computation, and acting. The subsumption architecture is even stronger than situated automata theory in that it rejects the idea of a central, analogical representation of the environment. Each behavior is responsible for extracting needed information from the sensors, processing it in a task-dependent manner, and producing control actions. Behaviors are organized hierarchically, with the lowest level behaviors responsible for maintaining the viability of the agent, and the higher levels pursuing more purposeful goals. This vertical decomposition by behavior or task is contrasted with the horizontal decomposition of the traditional architecture, with its expensive and nonreactive per- ceive/plan/execute cycle. The subsumption architecture has been influential in the mobile robotic community, and its ideas have permeated most of the proposed architectures to some extent. We have incorporated the concept of vertical decomposition into the way in which behav- iors interact with sensing and perception in our multivalued controllers: more reactive behaviors can access raw sensor readings, while more purposeful behaviors use more complex perceptual routines. In fact, the very notion of behaviors themselves is in the spirit of subsumption architecture, since each behavior is oriented towards accomplish- ing a particular goal, But our multivalued controllers differ in several important respects from the subsumption architecture. The most obvious one is a commitment to embed- ding and goal abstraction by means of a perceptual subsystem shared by all behaviors. Without such a subsystem, it is difficult to coordinate reactive and purposeful behavior in a general way, or to abstract the goals of behaviors so deliberation processes can make use of them, or to have behaviors whose purpose is to facilitate perceptual processes of recognition and anchoring.7 The second main difference is in the way behaviors are combined together. Subsumptions architectures generally use by some form of a suppression mechanism. While these mechanisms may be adequate as a programming technique, it is difficult to prove formal properties about them, or to accomplish the kinds of sophisticated tradeoffs among goals that is available using multivalued logic. 4.2. Reactive planning architectures There have been several proposals and implementations of hybrid architectures, ones that combine a low-level reactive control mechanism with one or more deliberative layers. The outline of a typical architecture is shown in Fig. 17. The bottom layer is a controller, a bounded computation function from inputs and perhaps internal state to outputs. This layer usually implements some form of behavior-based control, in ‘We note that there are some impressive robot [ 91 and Mataric’s navigation experiments [ 261. results without such representation, e.g., Connell’s can-retrieving A. Safiotti et al. /Artificial Intelligence 76 (199.5) 481-526 517 Sensing Action Fig. 17. A typical architecture for reactive planners (after Connell’s SSS system). is composed layer behaviors, to an overall goal, or when environmental to warrant different behaviors. The tactical planner must complete initiates and monitors behaviors, such as deciding when they have completed that implement from sub-functions particular taking care of temporal their job, conditions have its layer. In the top takes place, with the results being passed down is invoked and guided by the planner planning layer for execution. Generally, in a timely manner, although not as quickly as the control function the control which behaviors. The second aspects of coordinating or are no longer contributing changed enough computations layer, to the sequencing conditions long-term deliberative in the sequencing There are many different layer, e.g., a task failing or completing. instantiations them with planning [ 141, RAPS [ 13,271, AURA LANTIS these concentrate on the interaction between and integrating architecture, pects of this framework: principled necessary composition to tie the control/sequencing and we have concentrated of this architecture, [ 31, and Pyton’s reactive planners including SSS [lo], AT- [ 301. Most of the top two layers, developing sequencers technology. Our approach fits in the hybrid planning on two important and not fully-developed the relation between the control and sequencing of complex controllers. And we have developed level to the more abstract planning level. as- layers, and the the properties 6.3. Artijkial potential-jields Multivalued logics are one way to provide a trade off between techniques based on the so-called “artificial potential [ 201 and now extensively used in the robotic domain concurrent goals. fields”, first [ 221. In the Another way is using introduced by Khatib potential field approach, a goal is represented by a potential representing the desirability 518 A. Sqfiotti et al. /Artificial Intelligence 76 (1995) 481-526 by a potential of each state from that goal’s viewpoint. For example, represented goal of reaching a given location location. At each point, gradient of the field. is and the is represented by a field having minimum value at that to the vector the goal of avoiding obstacles field having maximum the robot responds to a pseudo-force the obstacles; value around proportional that produced and the combined force is a combination information. Potential that force (e.g., which direction The major technical difference between our control schemas and potential-field meth- ods is how they combine fields are combined by linear superpo- sition: one takes a weighted vector sum of the associated pseudo-forces. Each force is is best to a summary of the preferences avoid an obstacle), In con- trast, when combining two control schemas one takes the t-norm of the two desirability functions, obtaining an assignment seen in Section 2.6 that these two forms of combination second difference in a logical in a logical the ability symbolic planning make contact with the symbolic planning methodologies. to each possible control. We have results. A as formulae language. Complex goals and constraints can often be described more easily field function. Moreover, form integration with classical to specify typically do not try to the context as a logical sentence makes easier. Potential-field is that we express both goals and applicability can produce different form of a potential of the summaries. of utility values in the analytical approaches techniques conditions than in the Introduction. Motor schemas are similar fields and that has many points of contact with our [ 2,3]. Arkin adopts Arbib’s notion of motor to our concept of level, and functions by using pseudo- is integrated is that planning of control activation function, control functions. A related difference A system based on potential the same basic components is AURA, developed by Arkin approach schema, discussed behavior, having perceptual parameters. However, AURA implements forces instead of desirability in a weaker form in AURA. AURA’S planner piece-wise motor schemas time. In our approach, by contrast, behaviors approach to follow this path are dynamically expected is not limited to navigation in principle the agent to orient toward linear path to the goal. This plan is then passed to the execution chosen and instantiated is basically a path planner that generates a layer, where at execution the planner generates the achievement a complex composition of of a given goal. This tasks. 6.4. Situation-action plans A number of authors have claimed real world are better expressed tables developed represent, should be activated. More recently to situations has been clearly spelled out and extensively developed to this category. behavioral plans belong in the that plans to be executed by agents situated in the form of “situation + action” rules. The triangle [ 121 already the idea to incorporated the condition under which that action the idea of plans as sets of rules that specify reactions Our in early robot planning work in the plan, together with each action [ 11,30,41,44]. Schoppers [41] views planning as the task of partitioning situations ronmental viewpoint of achieving possible situation according to the reaction that the agent should produce the given goal. His universal plans specify a reaction that the agent can encounter, and can be thought of as controllers the set of possible envi- from the to each that A. Sajiotfi et al. /Artificial Intelligence 76 (1995) 481-526 519 provide an input-output mapping specific to the achievement of a given goal. Behavioral plans can be seen as a more control-oriented generate behavioral plans, by extending and more situations, to ask how much we should enlarge as soon as the context continue until we cover the entire state space, as proposed by &hoppers. form of universal plans. The way we to cover more It is natural are to stop the current situation, as we did in Section 5.2; and to the context of an initial behavior notion of space partitioning. this context. Two extreme solutions to &hopper’s is similar includes implies loop. This the control of the situation that the evaluation [ 281 has recently extended in bounded this, Nilsson Most of the current agent architectures based on situation-action time, and that the actions should be elementary the idea of triangle for plans, called teleo-reactive trees. A teleo-reactive inside performed considering a new formalism set of situation-action the tree is expected rule. Teleo-reactive In fact, a behavioral plan can be seen as a mathematically motivated generalization TR-tree where contexts can take intermediate multivalued, rules put these rules should be action steps. By tables to develop tree encodes a the activation of each rule in for the activation of its parent that we have built in Section 5. of a are to eventually produce trees resemble the conditions the behavioral plans rules, ordered by a subgoal relation: and we can have concurrent degree of truth, desirability activation of behaviors. functions reduces it using is GAPPS compilation A common it automates for primitive goal-regression into executable the goal-reduction structures. Synthesis control actions. Later development a more abstract way of representing complex controllers by compiling plans feature of the approaches based on situation-action the GAPPS compiler rules rules is that situation- is a convenient prop- the process of producing complex controllers. Another method [ 181. Given a top- into a set of GAPPS [ 191, which gives it to do predictive controllers, goal- and is in the level of on how control schemas to produce complex behavior. GAPPS control actions are simple effector com- the same control action, that GAPPS the same action plans can be compiled erty, because for synthesizing level goal, of condition-action incorporated GAPPS planning. The two synthesis methods we explored for our multivalued regression planning goal-reduction methods, detail of complex action specification. We have concentrated combine mands, and conjoining controls or one of the control actions could be used to program multivalued development using operator descriptions action, and enables and run-time deliberation, respectively. Where is only possible is unspecified. Although are similar the two systems differ as we have presented here. to GAPPS goal-regression logic control schemas, it is conceivable it would require if they produce rules 7. Conclusions Intelligent involves a continued locally, action and sensing happen aimed at, or the undesired consequences and space. By planning, goals and desires. But the result of planning, The work described interplay of local and global factors. Actions they are they can bring about, may lie far away in time to its global the pkzn, has to become physical activity. in this paper focused on the relation between abstract goals of the in the here and now of the agent, but the goals tries to connect the actions it performs an agent 520 A. Safiotti et al./Artificial Intelligence 76 (1995) 481-526 plan and the physical actions that they induce in the agent. from of basic of planning the definition to integrating on grounding Our approach types of movements, the tools of multivalued state to external objects and control has focus& planning in physical action, using the logics. We ingredients started or control schemas, and of the way they can be combined or blended to form complex movements. Then, we have to the level of abstract actions in an environment. Here, we have “lifted” control schemas used two key notions: the the notion of embedding in the environment, and the notion of context, agent’s internal to goals, expressed or circumstances executions. The good behaviors for a goal are those that, when as sets of satisfactory executed the goal. And we in an appropriate that satisfy that composing behaviors creates a new behavior have proven, under certain hypotheses, is the basis that is good for the composition of complex behavior, and we have shown how traditional AI for automatic planning techniques to generate complex controllers of execution. Finally, we have analysis can be readily adapted context, produce executions of the corresponding through perception, goals. This result linked behaviors for given goals. and means-ends for deliberation by anchoring One of the nice properties of the multivalued analysis of complex control from fuzzy control. We have illustrated is that it this with from our mobile robot, Flakey. The ability of Flakey to navigate in unstructured using techniques can be implemented examples real-world environments of SRI during normal office activity. We have reported Section 5. Flakey’s performance including Flakey’s second place at the first robotic competition in [ 81, and a successor, Erratic, at the third competition [ 211. has been tested in innumerable has also been demonstrated some of these experiments runs in the corridors and offices in in a few public events, reported of the AAAI, Our experience in the domain of mobile robot navigation has led us to formulate some principles for achieving should be computationally robust performance inexpensive, in dynamic environments: functions on local being simple them easy to write, debug, and compose (and possibly learn), behaviors should satisfy a single goal over a small range of environments. l Complex behaviors to achieve multiple goals or operate over wider environmental conditions should be composed out of simpler ones. l Since the environment will contain uncertainty, trajectories. rather than relying on precomputed complex behaviors should be reactive, in this paper provides a formal The work presented methodology, and enables haviors. Our methodology now prevalent problems with these architectures: these up with more abstract deliberation processes. the use of classical planning is not a radical departure in the literature. Rather, techniques to build complex be- from the reactive planning methods to two significant and how to link it is a theoretical approach how to form complex movements, framework for this compositional Although the results obtained up to now are extremely promising, our study has uncovered a number of issues that need either a deeper formal analysis, or more experi- the most urgent ones is the question of how the desirability mentation, or both. Among functions used in the control schemas can be constructed in practice. We are currently for synthesizing studying rules from abstract specifications of goals, and for techniques methodological l Behaviors conditions. l To make A. S@otti et al./Art$cial Intelligence 76 (1995) 481-526 521 improving these rules by learning methodologies. A second important issue that we only touched in this paper is the dynamic modification of planned behaviors. We are currently working on the further development of adequate indexes of performance, and on the use of these indexes to patch an existing plan. As for our formal development, two aspects that clearly need a deeper inspection are the notions of consistency and stability. We anticipate that studying these aspects will call for a richer representation of the state, including the dynamics of the environment. Finally, we feel that much more work is needed on the use of anchoring and object descriptors to relate perception and action. Acknowledgments Nicolas Helft, David Israel and Daniela Musto contributed to the development of the ideas presented in this paper. Alessandro Saffiotti has been supported in part by a grant from the National Council of Research of Italy and in part by the Action de Recherches Concertees BELON funded by a grant from the Communaute Francaise de Belgique. Enrique Ruspini was supported by the U.S. Air Force Office of Scientific Research under Contract No. F49620-91-C-0060. Support for Kurt Konolige came partially from ONR Contract No. NOO014-89-C-0095. Additional support was provided by SRI International. Appendix A. Proofs We prove here the main theorems in this paper. The proof of the other results can be found in the technical report version [38]. The following properties of t-norms will be used in the subsequent proofs. Lemma 36. Let @ be any a continuous T-norm with quasi-inverse 0. Then (i> supstf(w> @g(w)1 < supsf(w) @sup,g(w). infJf(w) @g(w)] Z infsf(w> @infsg(w). (ii) (iii) x0y>z ifandonlyifn>z@y. (iv) (x0z) By < (x@yY) 0z. (v) (X0Y) Oz =x0(y@zz). (vi) (~~YY)~(z~w)~(x~z)~(Y~w). Proof of Theorem 10. Looking first at the relation between Nextnlc and NextD, we may notice that, for any two states s, s’ in S, it is Nextolc (s, s’) =;EJ [(D(s,a) 0C(s))@M(s,a,s’)] <,“F; [(D(s,a) 63 M(s,a,s’)) 0C(s)] = Nexto(s,s’) 0 C(s). 522 A. Saflotti et al./Art$cinl Intelligence 76 (1995) 481-526 Let now t = (so, $1,. . . , sk) be any trajectory, and t’ = (q, s~+l, . . . , s,), with 0 6 I < m ,< k, any subtrajectory of t. Then we can see that TrUjD~c(t) 6 inf [Nexto(Si, Si+l) O(i<k 0 C(si>] 6 inf I<i<m [NeXtD(Si, Sifl) 0 C(Si)] < inf NeXtD(Si,Si+l) 0 inf C(Sj) l<i<m l<j<m = TR2&)(t’> 0 C(t’>. As the above inequality holds no matter what subtrajectory t’, we have proved the theorem. Cl Proof of Theorem 24. By definition, Go&( B, G) > a is true iff there is a positive number N such that, for any trajectory t E 7-d S), it is GLC(t) 0AdmB(t) > a. From this, and by applying Lemma 36 (iii), we have Adme @C(t) < (GIC(t) 0 a) @ C(t) <(GLC(t) @C(t)) 0cu <[(G(t) sC(r)) @C(t)1 0a <G(t)0a. 0 Proof of Theorem 26. We let B = CONJ[ BI; B2], For any trajectory t, we have Adme @G(t) = TrdcD,nD2j1C,nC2(t) @Anch~,~~ CZJ G(t) 6 WD~c,.c,(t> @G(t) @Ancho, 1 < [Tr4D~c,(t> sCz(t)l @G(t) @An&, 1 < TrajDLc, (t) @ Amho, =AdmB\(t), where we have used Theorem 10 for the third passage. From Go& Bl , Gt ) > LY by wirtue of Theorem 24, there exists Nt such that AdmB, (t) @ Cl(t) < GI (t) 0 (Y. for any trajectory t of length greater than Nt . A similar property holds for B2 and G2 for, say, N2.Let N=max(Nt,Nz).Then,forany tEIN(S), Adme @Cl(t) c%&(t) <min(Gl(t) 0~ Gz(t) SP) <min(Gt(t),G~(t)) 0min(a,P), and the thesis follows from Theorem 24. Cl A. Sajiotti et al. /Artificial Intelligence 76 (1995) 481-526 523 Proof of Theorem 28. Let B = CHAlN[ BI ; Bz] , and let C = Cl U C2. We first consider the two goodness hypotheses. By Theorem 24, the hypothesis on BI means that Adm~,(t) 0 C,(t) < supC2Cs) 0 (Y set for any trajectory of length greater than some suitable trajectory t, we then have integer, call it Nt. For each such A&Q(~) @C(t) < supC:!(s) set 0a. On the other hand, the hypothesis on B2 translates to (A.l) Adme, 0 C;?(t) < G(t) SP for any trajectory we have, for any such trajectory, t of length greater than a second suitable integer, call it N2. Hence A&J(~) 0 C2(t) 6 TraJoLc, ’ Cl> @A~nchoz(f) @ C2(t) = A&:(r) 0 C2(t) < G(t) 0 P. By monotonicity of goal restriction, the last inequality implies that Adme @ C2(t) < CL”(t) S/3. (A.2) We then turn attention integer N such that, for any trajectory to our thesis. By Theorem 24, we need to prove that there is t longer than N, a positive Adma @C(t) <G’(t) 0min(a,P), or, equivalently, [G~CzUSEQUENCE[C2,Ct]](t) >Adma(t) @C(t) @min(a,P). (A.3) Let N = Nt +max(Nl,N2), (A.1)) than N. From greater and let t = (so,q , . . . , s,) be any trajectory of length there must be some state s E t such that C2(s) b a@AAdmB(t) @C(t). index of such a state, and let t’ = ( sk, sk+l,. . . , s,) be the part of Let k be the smallest t beyond sk. By the goodness hypothesis on Bl, k 6 Nt, and so t’ must have length at least max( Nl , N2). We show that t’ satisfies it does not. Then (A.3) must be false of t’, that is, we must have both the goal G’. For suppose GLC2(t’) <Adrns(t’) @C(t’) @min(cY,/?) and SEQUENCE[Cz,C1](t’) <Adm~(t’) @C(t’) @min(cu,P). Consider (AS). From the definition of SEQUENCE and from (A. 1) we have (A.4) (A.5) 524 A. gafiotti et al./Arttficial Intelligence 76 (1995) 481-526 SEQ~NCE[CZ,C~I(~‘) = SUP SUP [CZ(S~) @C](Sj)] k<i<n i<j(n 2 Sup [C2(si) @ inf Cl(Sj)] k<i<n k<j<n > sup C2CSi) k,<i<n 8 C*(t’> >Adme(t’) @C(i) @C,(t) @a. From this, and by applying (AS), we have Ad&(t’) @C(t’) @C,(i) @a <Adrn~(t’) @C(t) @/3 from which we conclude Cl (t’) < p 0 a. By putting this in (A.4), we obtain GlC2(t’) <Adm~(t’) @C(f) @min(a,P) <Adm(t’) @C2(t’) @C,(t’> @a <Adma @ C2(t’) @ (Ps a) c$ a <Adm~(t’) @ C2(t’) 8 P. But this contradicts (AS) must be rejected, and (A-3) must hold of t’, thus proving the goodness hypothesis on B2 (A.2). Hence one of (A.4) 0 the theorem. and References [ I] M. Arbib and D. House, Depth and detours: an essay on visually guided behavior, Tech. Rept. 85-20 COINS, University of Massachusetts, Amherst, MA (1985). [ 21 R.C. Arkin, Motor schema based navigation for a mobile robot, in: Proceedings of the IEEE International Conference on Robotics and Automation (1987). (31 R.C. Arkin, Integrating behavioral, perceptual and world knowledge in reactive navigation, Robotics I41 151 [61 [71 [81 [91 IlO1 Autonomous Systems 6 ( 1990) 10.5- 122. R. Bellman, Adaptive Control Processes: A Guided Tour (Princeton University Press, Princeton, NJ, 1961). R.E. Bellman and L.A. Zadeh, Decision making in a fuzzy environment, Management Sci. 17 (1970) 141-164. R.A. Brooks, A robust layered control system for a mobile robot, IEEE J. Robotics Automation RA-2( 1) (1982). P.R. Cohen and H.J. Levesque, Persistence, Intention, and Commitment (MIT Press Cambridge, MA, 1990). C. Congdon, M. Huber, D. Kortenkamp, K. Konolige, K. Myers, E.H. Ruspini and A. SaJJiotti, CAFWEL vs. Flakey: A comparison of two winners, AI Mag. 14(J) J. Connell, Minimalist Mobile Robotics: A Colony-style Architecturefor an Artificial Creature (Academic Press, New York, 1990). J. Connell, SSS: A hybrid architecture applied to robot navigation, in: Proceedings of the IEEE Conference on Robotics and Automation ( 1992). (1993) 49-57. A. Sajiotti et al. /Artificial Intelligence 76 (1995) 481-526 525 [ 1 I] M. Drummond, Situated control rules, in: Proceedings First International Conference on Principles of Knowledge Representation and Reasoning ( 1989). [ 121 R.E. Fikes and N.J. Nilsson, STRIPS: a new approach to the application of theorem proving to problem solving, Artif: Infell. 2 ( 1971) 189-208. [ 131 J.R. Fiiy, An investigation into reactive planning in complex domains, in: Proceedings AAAI (1987). [ 141 E. Cat, Integrating planning and reacting in a heterogeneous asynchronous architecture for controlling real-world mobile robots, in: Proceedings AAAI ( 1992). [ 151 M.P. Georgeff and FE Ingrand, Decision-making in an embedded reasoning system, in: Proceedings AAAI, Detroit, MI (1989) 972-978. 1161 D. Israel, J. Perry and S. Tutiya, Actions and movements, in: Proceedings IJCAI Sydney, Australia (1991). [ 171 L. Kaelbling and S. Rosenschein, Action and planning in embedded agents, Robotics Autonomous Systems 6 (1990) 35-48. ] 181 L.P. Kaelbling, Goals as parallel program specifications, in: Proceedings AAAI Minneapolis-St. Paul, MN (1988). [ 191 L.P. Kaelbling, Compiling operator descriptions into reactive strategies using goal regression, Technical Report TR-90-10 Teleos Research, Palo Alto, CA ( 1990). [ZO] 0. Khatib, Real-time obstacle avoidance for manipulators and mobile robots, Interna?. J. Robotics Res. S(1) (1986)90-98. [ 211 K. Konolige, Erratic competes with the big boys, AI Mag. 16 (Summer 1995). [ 221 J. Latombe, Robot Motion Planning (Kluwer Academic, Boston, MA, 199 1) [23] J. tukasiewicz and A. Tarski, Untersuchungen tiber den Aussagenkalktil, Comptes Rendus Sot. Sci. Letfres Varsovie (Cl. III) 23 (1983) 157-168. [24] D. Lyons and M. Arbib, A task-level model of distributed computation for sensory-based control of complex robot systems, Tech. Rept. 85-30 COINS, University of Massachusetts, Amherst, MA ( 1985). [25] C. Malcom and T. Smithers, Symbol grounding via a hybrid architecture in an autonomous assembly system in: P Maes, ed., Designing Autonomous Agents (MIT Press, Cambridge, MA 1990) 123-144. [ 261 M. Mataric, A distributed model for mobile robot environment learning and navigation, Tech. Rept. 1228 MIT AI Laboratory ( 1990). [ 271 D. McDermott, Planning reactive behavior: A progress report, in: Proceedings of the DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control ( 1990). [28] N.J. Nilsson, Toward agent program with circuit semantics, Tech. Rept. STAN-CS-92-1412 Stanford University, Computer Science Dept. Stanford, CA ( 1992). [29] K. Overton, The acquisition, processing, and use of tactile sensor data in robot control, Ph.D. Thesis, University of Massachusetts, Amherst, MA ( 1984). [ 301 D.W. Payton, J.K. Rosenblatt and D.M. Keirsey, Plan guided reaction, IEEE Trans. Systems Man Cybernet. 20(6) (1990). 1311 N. Rescher, Semantic foundations for the logic of preference, in: N. Rescher, ed., The Logic of Decision and Action (Pittsburgh, PA, 1967). [32 ] N. Rescher, Many Valued Logic (McGraw-Hill, New York, 1969). [33 1 S.J. Rosenschein, The synthesis of digital machines with provable epistemic properties, Technical Note, 412 SRI Attificial Intelligence Center, Menlo Park, CA ( 1987). [34] E.H. Ruspini, Fuzzy logic in the Flakey robot, in: Proceedings of the International Conference on Fuuy Logic and Neural Networks (IIZUKA) Japan (1990) 767-770. [351 E.H. Ruspini, On the semantics of fuzzy logic, Internat. J. Approx. Reason. 5 (1991) 45-88. 136 I E.H. Ruspini, Truth as utility: A conceptual synthesis, in: Proceedings Seventh Conference on Uncertainty in Artificial Intelligence, Los Angeles, CA ( 199 1) . [37 I A. Saffiotti, Pick-up what?, in: C. Backstrom and E. Sandewrdl, eds., Current Trends in AI Planning ([OS Press, Amsterdam, Nederlands, 1994) 166-177. [381 A. Saffiotti, K. Konolige and E.H. Ruspini, A multivalued-logic approach to integrating planning and control, Tech. Rept., 533 SRI Artificial Intelligence Center, Menlo Park, CA (1993). [39] A. Safhotti, E.H. Ruspini and K. Konolige, A fuzzy controller for &key, an autonomous mobile robot, Tech. Rept., 529 SRI Artificial Intelligence Center, Menlo Park, CA (1993). 526 A. Sajiotti et al./Artijicial Intelligence 76 (1995) 481-526 (401 A. Saftiotti, E.H. Ruspini and K. Konolige, Integrating reactivity and goal-directedness in a fuzzy controller, in: Proceedings of the Second Fuzzy-IEEE Conference San Francisco, CA ( 1993). [41] M.J. Schoppers, Universal plans for reactive robots in unpredictable environments, in: Proceedings IJCAI (1987). 1421 B. Schweizer and A. Sklar, Probabilistic metric spaces (North-Holland, Amsterdam, 1983). [43] K. Segerberg, Routines, Synthese 65 (1985) 185-210. (441 L. Suchman, Plans and Situated Actions: The Problem of Human Machine Communication (Cambridge University Press, Cambridge, MA, 1987). [45] L. Valverde, On the structure of F-indistinguishability operators, Fuzzy Sets Systems 17 (1985) 313-328. [46] S. Weber, A general concept of fuzzy connectives, negations and implications based on t-norms and t-conorms, Fuzzy Sets Systems 11 ( 1983) 115-134. [47] D.E. Wilkins, Practical Planning (Morgan Kaufmann, San Mateo, CA, 1988). [48] J. Yen and N. Pfluger, A fuzzy logic based robot navigation system, in: Proceedings of the AAAI Fall Symposium on Mobile Robot Navigation, Boston, MA (1992) 195-199. [49] L. Zadeh, Fuzzy sets, Inform. and Control 8 ( 1965) 338-353. 