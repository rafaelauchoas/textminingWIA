Unsupervised visual feature learning withspike-timing-dependent plasticity: How far are we fromtraditional feature learning approaches?Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, PierreBouletTo cite this version:Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, Pierre Boulet. Unsupervisedvisual feature learning with spike-timing-dependent plasticity: How far are we from traditional featurelearning approaches?. Pattern Recognition, 2019, 93, pp.418-429.￿10.1016/j.patcog.2019.04.016￿.￿hal-02146284￿HAL Id: hal-02146284https://hal.science/hal-02146284Submitted on 22 Oct 2021HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.Distributed under a Creative Commons Attribution - NonCommercial| 4.0 InternationalLicenseVersion of Record: https://www.sciencedirect.com/science/article/pii/S0031320319301621Manuscript_e9bbab9223355c7c3fec8d7737d3ac25Unsupervised Visual Feature Learning withSpike-timing-dependent Plasticity: How Far are wefrom Traditional Feature Learning Approaches?Pierre Faleza, Pierre Tirillyb,∗, Ioan Marius Bilascoa, Philippe Deviennea,Pierre BouletaaUniv. Lille, CNRS, Centrale Lille, UMR 9189 - CRIStAL - Centre de Recherche enInformatique Signal et Automatique de Lille, F-59000 Lille, FrancebUniv. Lille, CNRS, Centrale Lille, IMT Lille Douai, UMR 9189 - CRIStAL - Centre deRecherche en Informatique Signal et Automatique de Lille, F-59000 Lille, FranceAbstractSpiking neural networks (SNNs) equipped with latency coding and spike-timingdependent plasticity rules offer an alternative to solve the data and energy bot-tlenecks of standard computer vision approaches: they can learn visual featureswithout supervision and can be implemented by ultra-low power hardware ar-chitectures. However, their performance in image classification has never beenevaluated on recent image datasets. In this paper, we compare SNNs to auto-encoders on three visual recognition datasets, and extend the use of SNNs tocolor images. The analysis of the results helps us identify some bottlenecks ofSNNs: the limits of on-center/off-center coding, especially for color images, andthe ineffectiveness of current inhibition mechanisms. These issues should beaddressed to build effective SNNs for image recognition.Keywords:feature learning, unsupervised learning, spiking neural networks,spike-timing dependent plasticity, auto-encoders, image recognition.1. IntroductionMachine learning algorithms require good data representations to be effec-tive [1]. Good data representations can capture underlying correlations of the∗Corresponding authorPreprint submitted to Pattern RecognitionMarch 30, 2019© 2019 published by Elsevier. This manuscript is made available under the CC BY NC user licensehttps://creativecommons.org/licenses/by-nc/4.0/data, provide invariance properties and help disentangle the data to make itlinearly separable.In computer vision, much effort has been put historicallyinto engineering the right visual features for recognizing, organizing and inter-preting visual contents [2; 3]. More recently, and especially since the rise of deeplearning, those features tend to be learned by algorithms rather than designedby human effort. Learned features have shown their superiority on a numberof tasks, such as image classification [4], image segmentation [5], and actionrecognition [6]. Although effective, feature learning has two major drawbacks:• it is data-consuming, as supervised learning algorithms – especially deeplearning ones – require large amounts of annotated data to be trained;• it is energy-consuming, as training large models, e.g., using gradient descent-based algorithms, has a high computational cost, which increases with theamount of training data. These algorithms are usually run on dedicatedhardware (typically GPU) that are power-intensive.The first issue – data consumption – can be mitigated by the use of unsu-pervised learning models. Unsupervised representation learning is recognized asone of the major challenges in machine learning [1] and is receiving a growinginterest in computer vision applications [7; 8; 9]. A number of unsupervisedmodels have been developed through the years, notably auto-encoders [10] andrestricted Boltzmann machines (RBMs) [11], and their multi-layer counterparts,stacked auto-encoders [12] and deep belief networks (DBN) [13]. Other lines ofwork include sparse coding [14] and the use of semi- or weakly supervised learn-ing algorithms [15]. Moreover, in the case of neural networks, initializing a deepneural network with features learned without supervision before training canyield better generalization capabilities than purely supervised training [12].The second issue – energy consumption – is addressed much less frequently inthe literature, but several authors acknowledge its importance [16; 17; 18] whichis bound to grow more and more as machine learning becomes overwhelminglypresent in a large range of applications: marketing, medicine, finance, education,administration, etc. Most hardware vendors have proposed dedicated machine2learning processor architectures (based on GPU, FPGA, etc.)recently [19].These hardware improvements help reduce the energy consumption by a smallfactor (typically one order of magnitude). Reducing further the energy con-sumption of learning algorithms requires to define new learning models andassociated ultra-low power architectures [18; 20; 21]. One promising model isspiking neural networks (SNNs). In this model, artificial neurons communicateinformation through spikes, as natural neurons do. Initially studied in neuro-science as a model of the brain, SNNs receive constant attention in the fieldsof machine learning and pattern recognition, from both the theoretical [22] andthe applicative [17; 23; 24; 25] perspectives. Dedicated hardware implementingthis model can be very energy-efficient [20]. SNNs have already shown theirability to provide near-state-of-the-art results in image classification, but onlywhen they are trained by transferring parameters from pre-trained deep neuralnetworks [21] or by variants of back-propagation [26]. In terms of energy effi-ciency, the first option is not viable as it still requires to train a standard deepneural network, which is exactly what should be avoided; the second option isnot suited either as back-propagation is a global, centralized algorithm – the er-ror must be propagated from the output to all units –, whereas the efficiency ofSNNs lies in their ability to perform highly decentralized, parallel processing onsparse spike data. The alternative is to use bio-inspired learning rules, such asHebbian rules. Among those, rules based on spike-timing dependent plasticity(STDP) [27] have shown promising results for learning visual features; how-ever, they have only been evaluated on datasets with limited challenges (rigidobjects, limited number of object instances, uncluttered backgrounds. . . ) suchas MNIST, 3D-object, ETH-80 or NORB [28; 29; 30; 31; 32], or on two-classdatasets [31; 32]. How they perform on more complex image datasets, what isthe performance gap between them and standard approaches, and what needsto be done to bridge this gap is yet to be established.Aims and scope. In this paper, we evaluate the ability of SNNs equipped withlatency coding and STDP to learn features for visual recognition on three stan-3dard datasets (CIFAR-10, CIFAR-100, and STL-10). Our goal is to identifysome of the factors that prevent STDP-based SNNs to reach state-of-the-artresults on actual computer vision tasks. First, we compare the performanceof SNNs on grayscale and color images (Section 5.4), then we compare themto one standard unsupervised feature learning algorithm, sparse auto-encoders(Section 5.5). The resulting models are analyzed with respect to different fac-tors (Section 6): input pre-processing, feature sparsity, feature coherence, andobjective functions.It allows us to identify some bottlenecks that should betackled to bridge the gap from SNNs to state-of-the-art models. In the conclu-sion (Section 7), we suggest some solutions to help address these bottlenecks.In this work, we consider only single-layer architectures because multi-layerSNNs with unsupervised STDP are only very recent and difficult to train, dueto the loss of spiking activity across layers [33; 31]. To our knowledge, this is thefirst work that evaluates features learned by unsupervised STDP-based SNNson recent benchmarks for object recognition and on color images, making onestep towards their use for actual vision applications.2. Unsupervised visual feature learningA visual feature extractor can be modeled as a function f : Rh×w → Rnfthat maps an image or image region of size h × w to a real vector of dimensionnf . It defines a dictionary of features of size nf . In the remaining of the paper,f will denote either the feature extractor function or the resulting dictionary,depending on the context. Early visual feature extractors were handcraftedto capture specific types of visual information (e.g., distributions of edges [3]).Recent approaches rather rely on machine learning to produce features thatbetter fit the data and that can be optimized towards a specific application.A typical learning-based feature extractor can be seen as a function fθ whoseparameters θ are optimized towards a specific goal by a learning algorithm. Thegeneral shape of fθ can be specified explicitly (e.g., a linear transform [34]), orimplicitly, based on the learning algorithm to be used (e.g., in a neural network,the possible shapes for fθ are defined by the architecture of the network). The4parameters θ are optimized by the learning algorithm by minimizing an objectivefunction L. In a supervised setting, this optimization step can be expressed as:θ∗ = arg maxL(X, Y ; θ)θ(1)where θ∗ are the parameters returned by the learning algorithm, X = (x1, x2 . . .)denotes the training samples and Y = (y1, y2 . . .) the ground truth attachedto the samples. L is directly set as a performance measure for the specifictask to be solved, e.g. the multinomial logistic regression objective for imageclassification [4], or keypoint matching for image retrieval [34].In an unsupervised setting, labels are not available. The optimization prob-lem becomes:θ∗ = arg maxL(X; θ)θ(2)In this case, L cannot be formulated towards a specific application. Some sur-rogate objective must be defined, that is expected to produce features that willfit the problem, e.g. image reconstruction [10], image denoising [35], and maxi-mum likelihood [13]. Learning rules are sometimes defined directly without anyexplicit objective function, e.g. in k-means clustering [7], but also STDP [27].Some constraints on the parameters or learning algorithm can be added toregularize the training process and reach better solutions. These constraintsreflect assumptions on properties that ”good” features should have, such as:• sparsity: it is often assumed that the extracted features should be sparse,i.e. only a small number of the features can be found in a single imageor image region. Sparsity is especially required when the set of features isover-complete1, to prevent the algorithm from reaching trivial solutions.Sparsity is commonly imposed in sparse coding [14] and auto-encoders [36];• coherence of features [37]: features should be different to span the space ofvisual patterns with limited redundancy2. Coherence measures the possi-1A dictionary of features is over-complete when its dimension (number of features) is largerthan the dimension of the input (size in pixels of the images or image regions processed).2Some authors [38] claim that redundancy should rather be reached to have a good repre-sentation, but with limited evidence.5bility to reconstruct a given feature as a weighted sum of a small numberof other features, i.e. whether features are locally linearly dependent;coherence should be small for the features to be effective.In Section 6, these properties will serve as a basis for the analysis of the featureextractors.3. STDP-based feature learningWhile traditional machine learning algorithms represent data as real val-ues, some models inspired by biology use spikes to carry information. Thesemodels are called spiking neural networks (SNNs). Working with spikes allowsa complete desynchronization of the model since each spike is processed inde-pendently of the others. Thus, the system can be more energy-efficient whenimplemented on dedicated hardware [39] since no master synchronization sys-tem is required [40]. Moreover, in these models, memorization and computationare performed locally by neurons, which saves the data exchange cost and, asopposed to traditional Von-Neumann architectures, bypasses the bus bottleneck.In the following, we detail the preprocessing and learning mechanisms in-volved in SNN architectures. SNNs have two main components: the spikingneurons, discussed in Section 3.1, and the synapses, which connect the neuronsof the network. The synapses are generally responsible for the training of thenetwork. One of the most used learning rule, spike-timing dependent plastic-ity (STDP) [27], is discussed in Section 3.2. As SNNs are fed with spikes, apreprocessing stage is required to transform pixel values into spike trains. Sec-tion 3.3 is focused on the preprocessing required by SNNs to process images.In Section 3.4, we discuss the representation of data as spikes, called the neuralcoding. The last sections describe other mechanisms required to achieve learningin SNNs: Section 3.5 is focused on lateral inhibition, which sets up competitionbetween neurons to force them to learn different patterns; Section 3.6 presentshomeostasis, which prevents certain neurons from taking advantage over theothers, enforcing effective spiking activities throughout the network.63.1. Spiking neuron modelsSpiking neurons (in Figure 1a) are defined as processing units receivingspikes from their input connections (or synapses), and emitting spikes towardstheir output synapses when specific input patterns are received. There are sev-eral spiking neuron models, designed to accomplish different objectives [41].Some aim to faithfully reproduce the behavior of biological neurons (e.g., theHodgkin–Huxley and Morris–Lecar models), while others are designed to be ef-ficiently implemented on hardware (e.g., the integrate-and-fire and Izhikevichmodels); the latter are usually simpler. This paper focuses on the integrate-and-fire (IF) model, to facilitate the understanding of the mechanisms of themodels and to optimize the simulation speed. Most work on SNN-based imagerecognition use this kind of model [28; 29; 31]. IF neurons have a single internalvariable: their voltage V . When a spike is fed to a neuron, it is integrated toV . V remains constant until the next spike, i.e. no leakage is applied. When Vreaches the neuron threshold Vth, the neuron fires: it emits a spike towards itsoutput neurons and resets its potential V to Vrest (see Figure 1b). Formally,V (cid:48) = I, V ← Vrest if V ≥ Vth(3)with V (cid:48) the derivative of V and I the input current. V is also reset to Vrestbetween each input sample, to reset the network activity. This model allows thesystem to perform computations locally, since incoming spikes act on the stateof the neuron, but not on the rest of the network (see Figure 1a).3.2. Spike-timing dependent plasticityNeurons are connected via synapses, which modulate the intensity of thespikes that they transmit via their synaptic weights W . Adapting these weightsallows long-term learning, by reinforcing or weakening connections. To allowenergy-efficient hardware implementations, this learning mechanism should useonly local information like the input and the output spike trains of the neuron.Finally, each synapse propagates the spikes with a random delay d to introducesome noise and make lateral inhibition more active. In this paper, d is sampledfrom a uniform distribution in [dmin; dmax].7I1I2I3I4I5)vm(v3210−100N(a)tO2O2O1tMembrane potentialRefractory periodMembrane potential thresholdIncoming spikesOutgoing spikes2020t (ms)404060608010080100(b)Figure 1: 1a A spiking neuron receives spike trains from incoming synapses, and generatesspikes towards outgoing synapses. 1b Evolution of the membrane potential of an integrate-and-fire neuron.Spike-timing dependent plasticity (STDP, see Figure 2a) [27] is a Hebb-like learning rule observed in biology.It reinforces the connections betweenneurons that have correlated firing patterns (potentiation), and weakens theothers (depression). Multiplicative STDP [28] (see Figure 2b) is used in thispaper. When a neuron fires a spike at time tpost, all incoming synapses areupdated as follows:∆w =α+e−β+−α−e−β−w−WminWmax−WminWmax−wWmax−Wminif tpost ≥ tpreotherwise(4)with ∆w the update applied to synapse weight w, tpre the timestamp of the inputspike and tpost the timestamp of the output spike. α+ and α− are respectivelythe learning rates applied for potentiation and depression. β+ and β− control8the slope of the exponential. Wmin and Wmax are the bounds for synapticweights. When no spike occurs during the presentation of a sample, tpre is setto +∞, so that weight depression occurs.0.2w∆0−0.2LTPLTDLTPLTD0.20.1w∆0−0.1−40 −20020406080100−0.2−100 −80 −60 −40 −2002040∆t (ms)∆t (ms)(a) Biological STDP(b) Simplified STDPFigure 2: Weight variation w.r.t. the difference in spike timestamps.3.3. Image pre-processing and color handlingAs STDP learns correlations between input spikes, images are usually pre-processed to help STDP find meaningful correlations. Typically, edges are ex-tracted from the grayscale images, e.g. through a difference-of-Gaussian (DoG)filter [31; 42] or Gabor filters [30]. In this paper, we use on-center/off-centercoding, which combines DoG filtering with a 2-channel representation necessaryto encode the data as spikes. This coding is inspired from bipolar cells of theretina. The output of the DoG filter at position (x, y) of image I is defined as:DoG(x, y) = I(x, y) ∗ (GDoGsize,DoGcenter − GDoGsize,DoGsurround)where ∗ is the convolution operator and GS,σ is a normalized Gaussian kernelof size S and scale σ defined as:GS,σ(u, v) =gσ(u, v)µ(cid:80)j=−µgσ(i, j)µ(cid:80)i=−µ, u, v ∈ [−µ, µ], µ =S2,with gσ the centered 2D Gaussian function of variance σ. The parameters of thefilter are its size DoGsize and the variances of the Gaussian kernels DoGcenter9and DoGsurround. After DoG filtering, the image is split into two channels c+and c− as follows:c(cid:63)(x, y) = max(0, (cid:63)DoG(x, y)), (cid:63) ∈ {+, −}where DoG(x, y) denotes the output of the DoG filter at position (x, y). c+ isthe positive (”on”) channel, and c− the negative (”off”) channel. This codingmakes it possible to encode negative DoG values as spikes. No thresholds areapplied to c+ and c−, i.e. the output values of the DoG filter are not filtered.Color processing in SNNs is a problem that has not been addressed muchin the literature so far. To apply on-center/off-center coding to color images,we define two strategies. In the first strategy, called RGB color opponent chan-nels, the coding is applied to channels computed as differences of pairs of RGBchannels: red-green, green-blue, and blue-red. The second strategy is inspiredby biological observations: in the lateral geniculate nucleus, which mainly con-nects the retina to the visual cortex, three types of color channels exist: theblack-white opponent channel (which corresponds to the grayscale image), thered-green opponent channel, and the yellow-blue opponent channel [43]. Thesecond strategy applies on-center/off-center coding to the red-green and yellow-blue (computed as 0.5 × R + 0.5 × G − B) channels. This leads to four possibleconfigurations of image coding: grayscale only, RGB opponent channels, biolog-ical color opponent channels (referred to as Bio-color), and the combination ofthe grayscale channel and the Bio-color channels.3.4. Neural codingA step called neural coding is required to transform real or integer valuesfrom datasets spike trains. Several neural coding techniques have been proposedin the literature. The most common ones are frequency coding, which encodesvalues directly as frequencies of spike trains, and temporal coding, which encodesvalues as timestamps of single spikes [44] (see Figure 3). This paper uses latencycoding, a form of temporal coding, since it has several advantages over frequencycoding in visual tasks [45]. It allows to represent a sample with fewer spikes,10but also to simplify the model since at most one spike per connection is emittedduring the presentation of a sample. Each input value x is transformed into aspike timestamp t as follows:t = (1.0 − x) × Tduration(5)with x ∈ [0, 1] the input value and Tduration the duration of the presentation ofa data sample.I3I2I1x3 = 0.9x3 = 0.48x2 = 0.0x2 = 0.1x1 = 0.7x1 = 0.3Sample 1Sample 2tFigure 3: Latency coding.3.5. InhibitionSTDP, or more generally unsupervised and local learning rules, require somecompetition mechanisms in order to prevent all neurons from learning the samepattern. Lateral inhibition is a straightforward method to do so in SNNs [28]:when a neuron fires, it sends inhibitory spikes to others neurons of the samelayer, which reduce their abilities to fire. Only a small number of neurons isactive at each input, which makes it possible to learn different patterns. In thispaper, winner-take-all (WTA) competition is used: only one neuron is allowedto spike at each position per sample.3.6. HomeostasisSeveral authors demonstrated experimentally than SNNs need homeostasisto guarantee an effective learning process [28; 29]. It can be done by adaptingthe thresholds Vth to prevent one neuron from dominating the others. Thethresholds of neurons that fire often are increased, so they will tend to fire less11often later on; inversely, the thresholds of sub-active neurons are decreased.Since latency coding is used, we train neurons to fire at an objective timestamptobj. The choice of this parameter can be tricky. An early tobj trains neuronsto respond to few input spikes, so that neurons learn only local patterns in fewinput channels (blobs or edges – see Figure 4a). A late tobj results in largerpatterns with a combination of multiple input channels (see Figure 4b).(a) tobj = 0.5(b) tobj = 0.7Figure 4: Filters learned with different tobj.Threshold adaptation is performed as follows:∆Vth = −η×(tfire−tobj)+ηif the neuron is the first to fire−ηNneuron−1otherwise(6)with η the learning rate, tfire the timestamp of the spike fired by the neuron,tobj the objective timestamp and Nneuron the number of output neurons. Settinglarge initial values for the thresholds may prevent the neurons from firing. Inthe absence of neuronal activity, no learning nor threshold adaptation can beperformed. It is therefore preferable to initialize the thresholds with small valuesto promote neuronal activity within the network.3.7. Spike to feature conversionFinally, spikes need to be transformed back into feature values that will befed to the classifier. Feature values are computed as follows:fi = 1.0 −t − ToutputminToutputmax − Toutputmin(7)with fi the i-th output feature value, t the spike timestamp (if no spike oc-curs, then t is set to Toutputmax), [Toutputmin , Toutputmax] the range of possibletimestamps. Toutputmin can be computed as Tinputmin + dmin, with dmin, andToutputmax as Tinputmax + dmax.124. Learning visual features with sparse auto-encodersAuto-encoders (AEs) [10] are unsupervised neural networks that learn latentrepresentations that allow to best reconstruct the input data.In this work,among all unsupervised feature learning algorithms, we only consider single-layer AEs, for two reasons. First, they belong to the family of neural networks, asSNNs do, and, within this family, they are one of the most representative modelsfor unsupervised learning (its main competitor being RBMs, which have beenshown to optimize a similar criterion [35] and yield comparable performancefor visual feature learning [7]). Then, we restrict our approach to single-layernetworks, as multi-layer SNNs are only starting to emerge [31]; we believe one-layer SNNs should be well mastered before addressing multi-layer architectures.The typical architecture of an AE is organized in two parts: an encoder encmapping the input X to its latent representation Z = enc(X), and a decoderdec computing a reconstruction ˜X of the input from its latent representation:˜X = dec(Z) = dec(enc(X)). The objective function (Eq. 2) thus becomes:θ∗ = arg minL(X, ˜X; θ)θ(8)where L(., .; θ) is some measure of the dissimilarity between the input X and itsreconstruction ˜X given the model parameterized by θ; in other words, the auto-encoder aims at reconstructing its input with minimal reconstruction error. Inour experiments, the Euclidean distance measures the reconstruction error.The encoder and the decoder can be defined as single-layer or multilayer (inthe case of stacked AEs) neural networks.In the following, we will consideronly single-layer models of this form:Z = enc(X) = σ(WencX + benc)dec(Z) = WdecZ + bdec(9)where Wenc ∈ Rh×w,nf (resp. Wdec ∈ Rnf ,h×w) is the weight matrix of theconnections in the encoder (resp. the decoder), benc ∈ Rnf (resp. bdec ∈ Rh×w)is the bias vector of the encoder (resp. the decoder), and σ(.) is some activation13function3, in our case the sigmoid activation function σ(x) = 11+e−x . The outputof the encoder correspond to the visual features learned by the AE: f = enc(X).To make the AE learn useful representations, the initial approach was toimpose an information bottleneck on the model, by learning representations withdimensionalities lower than the ones of the input data (nf < h × w). However,such representations cannot capture the richness of the visual information, socurrent approaches rather use over-complete (nf > h × w) representations. Inthis case, some additional constraints must be enforced on the model to preventit from learning trivial solutions, e.g., the identity function. These constraintsgenerally take the form of an additional term in the objective function, forinstance: weight regularization, explicit sparsity constraints (sparse AEs [7; 36],k-sparse AEs [37]) or regularization of the Jacobian of the encoder output Z(contractive AEs [46]). Another approach is to change the objective functionfrom reconstruction to another criterion, for instance data denoising [35].In this paper, we will consider sparse AEs as a baseline to assess the perfor-mance of STDP-based feature learning. More recent models (denoising AEs [35],contractive AEs [46], etc.) can reach better performance, but sparse AEs arecloser to current STDP-based SNNs, which also feature explicit sparsity con-straints, usually through lateral inhibition. Also, it allows us to set a minimumbound that SNNs should at least reach to be competitive with regular featurelearning algorithms, and identify some directions to follow to achieve this goal;it constitutes a first step before taking STDP-based SNNs further.In the following, we describe the weight regularization and sparsity con-straint terms that we used in our experiments :• L2 weight regularization: λ2 (||Wenc||22 + ||Wdec||22), where ||.||2 denotes theFrobenius norm and λ is the weight decay parameter;• sparsity term [36]: γ.KL(ˆρ||ρ), where ρ is the desired sparsity level of thesystem, ˆρ is the vector of average activation values of the hidden neurons3We only consider models where no activation function is applied to the decoder outputas we work on continuous (image) data in [0, 1].14over a batch, KL(.||.) the Kullback-Liebler divergence, and γ the weightapplied to the sparsity term in the objective function.This yields the final objective function for the AE:L(X, ˜X; θ) =12||X − ˜X||22 +λ25. Experiments5.1. Experimental protocol(||Wenc||22 + ||Wdec||22) + γ.KL(ˆρ||ρ)(10)The SNN and AE architectures used in our experiments are single-layer net-works with nf hidden units (see Figure 5). We follow the experimental protocolproposed by Coates et al. [7] to compare unsupervised feature extractors. Itis organized in two stages, described below: visual feature learning, and theevaluation of the learned features on image classification benchmarks.wpwpwpnfwpwpnfwp(a) SNN(b) Auto-encoderFigure 5: (a) SNN architecture used in the experiments. Solid arrows denote inhibitoryconnections between hidden units. (b) AE architecture used in the experiments.Feature learning. From the training image dataset I = (I1, I2, . . . , In), we ran-domly sample np patches of size wp × wp. The patches are fed to the featurelearning algorithm for training, to produce a dictionary of nf features.Image recognition. The learned feature dictionary is used to produce imagedescriptors that are fed to a classifier following this process (Figure 6):1. Image patches of size wp × wp are densely sampled from the images withstride s, producing k × k patches per image (Figure 6a).152. Patches are fed to the feature extractor, producing k × k feature vectors ofdimension nf per image, organized into feature maps where each positioncorresponds to one patch of the input image (Figure 6b).3. We apply sum pooling over a grid of size r × r: the feature vectors of thepatches within each grid cell are summed to produce a unique vector ofsize nf per cell. These vectors are then concatenated to produce a singlefeature vector of size r × r × nf for each image (Figure 6c).4. The feature vectors of the images are fed to a linear support vector machine(SVM) for training (training set) or classification (test set).Feature extractionSum poolingwpwpsnf{k(a)k(b)SVMfn×r×r{ nf(c)Figure 6: Experimental protocol. (a) Input image, where k × k patches of size wp × wp areextracted with a stride s. (b) nf feature maps of size k × k produced by the feature extractorfrom its dictionary of nf features. (c) Output vector constructed by sum pooling over r × rregions of the feature maps.5.2. DatasetsWe perform experiments on three datasets commonly used to evaluate un-supervised feature learning algorithms: CIFAR-10, CIFAR-100, and STL-10.Table 1 provides the properties of these datasets. Since previous work evalu-ated SNNs only on grayscale images, we also use grayscale versions of the threedatasets, referred to as CIFAR-10-bw, CIFAR-100-bw, and STL-10-bw.Contrary to MNIST, which is the preferred dataset in the SNN literature [42],these datasets provide color images of actual objects rather than just binary im-ages of digits. It makes it possible to evaluate SNNs in more realistic conditions,in terms of data richness and importance of image pre-processing. Also, unlike16DatasetCIFAR-10 [47]CIFAR-100 [47]STL-10 [7]Resolution # classes Training set size Test set size32 × 3232 × 3296 × 96101001050,00050,0005,00010,00010,0008,000Table 1: Properties of the datasets used in the experiments.DataColorGrayscalenf641024641024ρ0.0050.0050.010.005γ0.50.10.050.1λ10−410−510−510−5Table 2: AE parameters used in the experiments.MNIST, but also other datasets such as NORB, they are not solved or nearly-solved problems (classification accuracy above 95%), so the results can highlightbetter the properties of the algorithms.5.3. Implementation detailsWe use image patches of size 5 × 5 pixels (wp = 5) and a stride s = 1.We evaluate the algorithms with two sizes of feature dictionaries, nf = 64 andnf = 1024. To produce final image descriptors, features are pooled over 2 × 2image regions (r = 2), yielding image descriptors of size 4 × nf .We used a grid search to find the optimal parameters for the AEs and we onlyreport results for the best configuration for each experimental setting. Table 2provides the values of the parameters that we retained. They were consistentlyoptimal over datasets. The AEs are trained for 1,000 epochs on 200,000 randompatches from the training set considered. We use the Adadelta optimizer [48]with an initial learning rate lr = 1.0. AEs are implemented using TensorFlow.Table 3 provides the parameters used to train SNNs. These parametersvalues were obtained using a greedy search: the optimal value of each parameterwas searched while the values of all other parameters were fixed. The largenumber of parameters in this model did not allow us to perform a full grid searchon all parameters. All SNN models are trained on 100,000 random patches fromthe training sets for 100 epochs.17NeuronVth(0)20 mvVrest0 mvTdurationNeural Coding1.0Wmindminα+β+STDP0.0 Wmaxdmax0.00.001 α−β−1.01.00.010.0011.0tobjThreshold Adaptationη0.7Pre-processing1.0 DoGsurround0.0012.0DoGcenterDoGsize7Table 3: SNN parameters used in the experiments.Classification was performed using LibSVM [49] with a linear kernel anddefault parameters. All reported accuracies are averaged over three runs of thefeature learning algorithms.5.4. Color processing with SNNsWe first evaluate the strategies to encode color discussed in Section 3.3: im-ages are first encoded using one of these strategies, then on-center/off-centercoding is applied to each channel. Table 4 shows the classification accuraciesyielded by each strategy on every dataset. Both color coding techniques, biolog-ical channels and RGB opponent channels, provide similar accuracies. However,using grayscale images yields better results than color images. This is counter-intuitive, since color images contain all the information available from grayscaleimages. Since the SNN processes all inputs in the same way, on-center/off-centercoding must cause this information loss. However, this preprocessing step is cur-rently required to extract edges from the images and feed the SNN inputs withspike trains that represent specific visual information. Training an SNN directlyfrom RGB images could be an alternative, but is very challenging, because theactive pixels in the patterns to learn can vary (from zero for a black patternto the size of a patch when all pixels have the maximum intensity); it cannotbe handled by existing homeostasis models. Figure 7 shows examples of filterslearned from raw RGB images; since the network has a single layer, the filterimage for one neuron is obtained by simply interpreting the normalized weightsof its input synapses as RGB values. Many filters converge towards similar oruninformative patterns. It results mostly in dead units and repeated features.18DatasetCIFAR-10CIFAR-100STL-10Color codingRGB opponentBio-colorGrayscaleGrayscale + colorRGB opponentBio-colorGrayscaleGrayscale + colorRGB opponentBio-colorGrayscaleGrayscale + colornf = 6437.66 ± 0.7337.53 ± 0.3345.37 ± 0.1348.27 ± 0.4717.14 ± 0.2217.06 ± 0.0918.43 ± 0.3425.20 ± 0.7644.13 ± 1.3044.23 ± 0.4144.66 ± 0.8749.20 ± 1.04nf = 102445.04 ± 0.0643.54 ± 0.0752.78 ± 0.4156.93 ± 0.5919.87 ± 0.0319.19 ± 0.3522.67 ± 0.3630.44 ± 0.4851.20 ± 0.3050.95 ± 0.0851.40 ± 0.6954.34 ± 0.30Table 4: Classification accuracy (%) w.r.t. to the color coding strategy.Figure 7: Examples of SNN features learned on raw RGB pixels (trained on CIFAR-10). Theyare mostly dead units or simple repeated patterns.Finally, we evaluate the combination of color and grayscale images by train-ing half of the features on each input independently. Results in Table 4 showthat it performs best, showing that DoG-filtered color images still contain in-formation that grayscale DoG-filtered images do not contain. In the remainingof the paper, all the runs performed on color images use this strategy.5.5. SNNs versus AEsThe classification accuracies for each feature learning algorithm and datasetare reported in Table 5. AEs perform consistently better than SNNs4. So, howto bridge the gap between STDP learning and standard neural network ap-proaches? Several elements may explain the performance of STDP. The resultsreported in Table 5 show two trends. First, working with colors always yields4Since we chose to use a simple sparse AE (see Section 4), the actual gap between SNNsand state-of-the-art models should be larger than what these experiments show.19better results than working with grayscale images; a straightforward explana-tion is that color is significant to recognize objects in the datasets considered,either because natural objects (e.g., animals) represented in the datasets havea limited, meaningful set of colors, either because the contexts of the objects(e.g., the sky behind airplanes) have meaningful colors. The second trend isthat the performance gap between SNNs and AEs is larger on color images thanon grayscale images, showing that SNNs cannot handle color well, at least notwith the straightforward color coding techniques that were used in the experi-ments. This result highlights the importance of color in object recognition, andtherefore the need for a more efficient neural coding of color in SNNs.SNNAEDatasetCIFAR-10CIFAR-10-bwCIFAR-100nf = 6448.27±0.4745.37±0.1325.20±0.76CIFAR-100-bw 18.43±0.3449.20±1.0444.66±0.87STL-10STL-10-bwnf = 102456.93±0.5952.77±0.4130.45±0.4822.67±0.3654.34±0.3051.40±0.69nf = 6457.56±0.0853.69±0.3437.71±0.1923.62±0.1852.28±0.4750.63±0.23nf = 102466.98±0.3359.50±0.1736.43±0.2926.56±0.0555.74±0.2552.88±0.29Table 5: Average classification accuracy (%) and its standard deviation w.r.t. to the datasetsand feature learning algorithms.Looking at the filters learned by SNNs and AEs provides additional infor-mation about the properties of features learned by STDP and potential reasonsfor the performance gap. Figures 9 and 10 show samples of filters learned bySNNs and AEs, respectively. The filters are different in nature. Filters learnedby STDP are mostly edges, and some blobs, that are well-defined, with one ortwo dominant colors. By contrast, AEs learn more complex features; edges andblobs can still be observed, but they include a larger range of color or gray levelsand are not as elementary as the ones learned by SNNs. Simple, well-definedfeatures like the ones learned by STDP are conceptually pleasing because theyrepresent elementary object shapes that can easily be understood. They sug-gest better generalization abilities from the feature extractor, and correspond tobiological observations [31]. However, they are not as effective in practice. AEs20can also produce features closer to the ones obtained with SNNs (although withlarger ranges of tones and intensities), but we could observe such features onlyby increasing the weight of L2 regularization, usually at some cost in accuracy.The specific looks of SNN features can be explained in two ways. First,the use of on-center/off-center coding as a preprocessing step biases the modelstowards edge-like filters, as it highlights the edges in the images. Moreover,the learned features contain exclusively black or saturated colors because STDPrules tend towards a saturating regime for weights: once a given unit has learneda pattern, repeated expositions to this pattern will reinforce the sensitivityto this pattern until the weights reach either 1 or 0. This is illustrated inFigure 8a, which shows the distribution of weights in an SNN after training:most weight values are close to 0 or 1. Since AEs perform better and have morestaggered weights, one may believe that saturated weights are detrimental to theperformance of SNNs. To check this, we performed experiments with differentvalues for the parameters β+ and β−: increasing their values allow the weightsto ”escape” more easily from their limit values Wmin and Wmax. Figure 8bshows that the weights are indeed more staggered, but the classification accuracydecreases as β+ and β− get larger (see Table 6). So, the fact that STDP leadsto saturated weights is not the reason for the performance gap with AEs.DatasetCIFAR-10β = 148.27±0.47CIFAR-10-bw 45.37±0.13β = 246.56±0.6844.55±0.57β = 343.18±1.6041.74±1.50β = 441.03±0.2138.90±1.57Table 6: SNN recognition rate according to STDP β parameter (nf = 64).Finally, the filters shown in Figure 9 also show a good property of SNNs:they do not raise any dead units, i.e.features that get stuck in a state withaverage weights that do not correspond to any significant pattern. By contrast,AEs tend to learn a fair amount of such features, especially when the numberof features increases (see Figure 10). This behavior of SNNs can be due to twofactors: lateral inhibition, which prevents neurons from learning similar patterns(here, becoming dead units), and the saturated regime of STDP.21noitalupoP103102101100noitalupoP10310210110000.20.40.60.8100.20.40.60.81W(a) β = 1.0W(b) β = 4.0Figure 8: Distribution of weights (log. scale) in an SNN (nf = 64) after training w.r.t. β.Most weights have values close to 0 or 1 when β decreases.6. Result Analysis and Properties of the Networks6.1. On-center/off-center codingIn this section, we investigate the impact of on-center/off-center coding onclassification accuracy. As mentioned in Section 5.5, this image coding is re-sponsible for the type of visual features learned by STDP, but does it impactthe final accuracy of the system? We compared the accuracy of two systems,each with and without preprocessing images: an AE, under the same protocolas before, and an SVM performing classification directly from image pixels. TheAE parameters for the on-center/off-center coding runs are: ρ = 0.005, γ = 1.0,and λ = 10−4. Results on CIFAR-10 and CIFAR-10-bw are reported in Table 7.Using on-center/off-center coding decreases the accuracy of the classification inboth configurations, which confirms that this coding is one of the causes of thelimited performance of SNNs in image classification. This is due to the fact thatextracting edges with DoG has the effect of selecting only a subrange of frequen-cies. In addition, the accuracies obtained on filtered color images are only onpar with (in the case of AEs) or worse than (with SVM) the results obtainedusing grayscale images; it highlights the fact that on-center/off-center codingcannot handle color effectively. One reason is that edge information is effec-tively represented by grayscale pixels, and the additional information broughtby color is essentially located in uniform image regions. Interestingly, the un-22(a) N=64, Grayscale(b) N=1024, Grayscale(c) N=64, Color(d) N=1024, ColorFigure 9: Grayscale and color filters learned by SNNs on CIFAR-10-bw and CIFAR-10. Fornf = 1024, random samples are shown.supervised SNN models of the literature that are competitive with traditionalapproaches are only evaluated on the MNIST dataset [42], which does not re-quire on-center/off–center coding as the images are only made of edges (whitehandwritten digits on black backgrounds).Therefore, to be effective, SNNs require the design of a suited image codingthat preserves as much visual information as possible. Using alternative methodsto extract edges (such as the image gradient or the image Laplacian) couldcapture slightly different types of edge information, which could be processedwithin a single SNN for improved performance, in a feature fusion approach.However, this would only process edge information, which is insufficient to reachoptimal classification performances. Ideally, SNNs should be able to handle rawRGB pixels; however, this is not straightforward, as we showed in Section 5.4.23(a) N=64, Grayscale(b) N=1024, Grayscale(c) N=64, Color(d) N=1024, ColorFigure 10: Grayscale and color filters learned by AEs on CIFAR-10-bw and CIFAR-10. Fornf = 1024, random samples are shown.6.2. SparsityWe investigate here the sparsity properties of SNNs and AEs. To do so, weuse the following sparseness measure [50]:√sp(f ) =nf −√|fi|(cid:80)nfi(cid:113)(cid:80)nfinf − 1f 2i(11)where f is the vector of activations of hidden units (i.e.the visual featurevector) and nf is the number of hidden units.sp(f ) ∈ [0, 1]; larger valuesindicate sparser activations.Table 8 shows the mean sparseness of features computed on the test setof CIFAR-10. The sparseness is much higher in SNNs than in AEs. Indeed,the specialization of features in SNNs relies mostly on lateral inhibition, whichprevents units from integrating spikes, leading to very sparse activations of the24DatasetCIFAR-10CIFAR-10-dogCIFAR-10-bwCIFAR-10-dog-bwRaw pixels AE features nf = 6437.7921.0728.3825.2957.56±0.0852.65±0.3053.69±0.3452.76±0.08Table 7: Classification accuracy (%) obtained with raw pixels and AE features w.r.t. pre-processing methods. Only one run is performed on raw pixels as SVM training is deterministic.features. Sparsity is often cited as a necessary condition for good represen-tations [1], and has been shown to be correlated to classification accuracy onimage datasets [36]. However, some results in [36] show that maximizing spar-sity does not always lead to improvements in classification accuracy in AEs.Similarly, we observed experimentally that enforcing too much sparsity on theAEs (e.g., by lowering ρ) is detrimental to the classification accuracy. To pushit further, we performed five runs on CIFAR-10 with nf = 64 and differentvalues for parameters λ, γ, and ρ. The AE parameters were set so that thesparseness would be close to the sparseness that we measured in SNNs (i.e., inthe range [0.8;0.9]). In these runs, the classification accuracy varies from 35.53%to 41.03%, much lower than our 57.56% baseline. To check whether high levelsof sparseness are an issue for SNNs too, we ran an experiment where lateralinhibition is deactivated during the feature extraction phase5. As expected, de-activating inhibition decreased the sparseness of the model (from 0.869 to 0.638on CIFAR-10). However, the classification rate decreased too (from 48.27% to47.35%). It shows that, although sparsity is a desirable feature for good rep-resentations, an excessive level of sparseness can be detrimental, and that theright amount of sparsity should be enforced during training. This calls for theuse of other, less restrictive, inhibition strategies than WTA.6.3. CoherenceOne measure of the quality of the learned feature is their incoherence, i.e. thefact that one feature cannot be obtained by a sparse linear combination of other5Inhibition is still maintained during feature training, because SNNs cannot converge ifthere is no competition between neurons, as explained in Section 3.5.25ModelSNNAEnf = 640.869±1.96e−50.352±0.116nf = 10240.967±3.04e−50.112±0.077Table 8: Mean and standard deviation of feature sparseness (test set of CIFAR-10).features in the vocabulary. If the incoherence is low, features are redundant,which is harmful for classification as redundant features will overweight otherfeatures. Inspired by the measure introduced in [37], we measure the coherenceµij of two features fi and fj as their cosine similarity:µij =| < fi, fj > |||fi||2.||fj||2(12)where fi is the i-th feature, < ., . > is the dot product operator, and ||.||2is the L2 vector norm. µ ∈ [0, 1]; 0 corresponds to orthogonal (incoherent)features and 1 to similar (coherent) features. The weights span different rangesof values depending on the feature extractor considered; feature normalizationmakes coherence measures comparable from one extractor to another.Table 9 displays the mean and the standard deviation of coherence measureµ under all experimental settings. Overall, STDP-based SNNs produce morecoherent features, which is one of the factors that can explain their lower perfor-mance. Moreover, the maximum pairwise coherence between two SNN featureis higher (max(µij) = 0.999 in most experimental configurations) than the max-imum coherence between AE-produced features (max(µij) ∈ [0.898, 0.998]), i.e.SNNs can learn almost identical features; in AEs, such features mostly corre-spond to dead units, whereas in SNNs they are significant features that arerepeated. This result shows the limits of WTA inhibition, which should pre-vent features from reacting to the same patterns but fails to do so in practice.This calls for more work on understanding inhibition mechanisms and designinginhibition models that better prevent the co-adaptation of features.6.4. Objective FunctionOne issue with STDP learning is that the objective function optimized bythe system is not explicitly expressed, unlike AEs, which minimize reconstruc-26DatasetCIFAR-10CIFAR-10-bwCIFAR-100nf = 640.252±0.2520.313±0.2710.256±0.230CIFAR-100-bw 0.320±0.2380.263±0.2930.263±0.293STL-10STL-10-bwSNNAutoencodernf = 10240.285±0.2490.340±0.2340.289±0.2380.343±0.2230.293±0.2460.293±0.246nf = 640.154±0.1440.119±0.1380.154±0.1490.121±0.1370.177±0.1640.119±0.132nf = 10240.145±0.1090.225±0.1610.143±0.1990.234±0.1660.151±0.1140.236±0.169Table 9: Mean and standard deviation of feature coherence µ under all experimental settings.tion error. Identifying the criteria that are optimized by STDP rules would helpbetter understand the related learning process and design learning rules for spe-cific tasks. In this section, we check whether STDP rules embed reconstructionas a training criteria, by checking whether features learned through STDP aresuited for image reconstruction, as those learned by AEs do. To do so, wereconstructed the test images from the visual features. First, we reconstructindividual patches: in AEs, the reconstructed patches are directly provided bythe decoder; in SNNs, patches are reconstructed as a linear combination of thefilters weighted by their activations for the current sample, like in an AE withtied weights. Images are reconstructed from patches by averaging the values ofoverlapping patches at each location. Table 10 shows the reconstruction errorof each feature extractor on the test set of CIFAR-10, computed as the sum ofsquared errors between input images and reconstructed images, averaged overthe samples. The reconstruction error is much higher for SNNs than AEs, whichsuggests that STDP does not learn features that allow reconstruction. However,qualitatively, the results look different (see samples in Figure 11): the edges ofthe objects are reconstructed, although with less details than in the originalimages, but the global illumination is degraded. The degradation of pixel in-tensities explains for a large part the increased reconstruction error. This isbest illustrated by the best and worst reconstructions (in the sense of the MSE)that we obtained using SNNs (see Figure 12): edges are reconstructed correctlyin both, but not pixel intensities. The reason for this is that SNNs processDoG-filtered images, in which color intensities are discarded and only edge in-27formation is retained. One could expect the reconstruction error of SNNs tobe much lower if they were able to process raw images directly. Also, the lackof details around the edges could be blamed on the learned features being tooelementary and sparse, which prevents the reconstruction of complex patterns.These results show that, although this is not explicit in the learning rules,STDP learns to reconstruct images, among other potential criteria. However, itis known that minimizing reconstruction error is not sufficient to provide mean-ingful representations [35]. This is why recent AE models include additionalcriteria such has sparsity penalties [36] or Jacobian regularization [46]. Howsuch criteria could be implemented within STDP rules, as well as which othercriteria are already embedded in the STDP rules, are still open questions.(a)(b)(c)(d)(e)Figure 11: Image reconstruction samples from the test sets of CIFAR-10 and CIFAR-10-bw(top: pre-processed input images, bottom: reconstructed images). (a) SNN features, DoG-filtered grayscale image (b) SNN features, DoG-filtered color image (c) SNN features, grayscaleand color DoG-filtered image (d) AE filters, color image (e) AE filters, grayscale image.(a)(b)Figure 12: (a) Best (error: 1.60) and (b) worst (error: 15.38) reconstructions from SNNfeatures from the test set of CIFAR-10 (left: input images, right: reconstructions).28SNNAECIFAR-10CIFAR-10-bwnf = 64 nf = 1024 nf = 64 nf = 10244.94294.97970.08020.004070.07420.004724.41794.4628Table 10: Average reconstruction errors on the test set of CIFAR-10.7. ConclusionIn this paper, we compared spiking neural networks (SNNs) equipped withSTDP to auto-encoders (AEs) for unsupervised visual feature learning. Experi-ments on three image classification datasets showed that STDP cannot currentlycompete with classical neural networks trained with gradient descent, but alsohighlighted a number of properties of SNNs and provided specific directionstowards effective feature learning with SNNs. Specifically, we showed that:• STDP-based SNNs are unable to deal naturally with RGB images;• the common on-center/off-center image coding used in SNNs results in aninformation loss, thus harming the classification accuracy; this informationloss is even more pronounced on color images;• winner-take-all inhibition results in overly sparse features and does notprevent the co-adaptation of features in practice;• STDP-based learning rules produce features that enable to reconstructimages from the learned features, as AEs do, even though the features arenot explicitly optimized for this task.These conclusions suggest two research directions to bridge the gap withstandard neural networks. The first is to address the information loss causedby on-center/off-center coding and the conversion to grayscale images. Onestraightforward solution is to use standard pre-processing methods that haveshown to be effective in computer vision, such as applying DoG filters in ascale-space fashion [2], or replacing them with whitening [7], which highlightsedges while retaining some color information. However, the resulting filters29would still be limited to edge regions; dealing with non-edge information wouldrequire the design of alternatives to DoG filtering or new neural coding schemesthat are not based on contrast only. The second direction is to ensure that thefeatures have adequate amounts of sparsity and redundancy. To do so, one coulddesign more suited inhibition mechanisms. Such mechanisms should be ”soft”,i.e. allow several neurons to fire at the same time, as demonstrated in [37] inthe case of AEs. The methods in [37] could be good candidates but should beadapted to preserve the locality of computations. Another option is to designadaptive inhibition rules that maintain the homeostasis of the system w.r.t tosome sparsity and co-adaptation objectives. Instead of inhibition rules, or inaddition to them, learning rules with homeostatic plasticity [51] can allow toreach the targeted sparsity and co-adaptation levels by acting on the synapsesrather than the states of the neurons.AcknowledgmentsThis work was supported in part by IRCICA (Univ. Lille, CNRS, USR 3380– IRCICA, F-59000 Lille, France) under the Bioinspired Project.References[1] Y. Bengio, A. Courville, P. Vincent, Representation learning: A reviewand new perspectives, IEEE Transactions on Pattern Analysis and MachineIntelligence 35 (8) (2013) 1798–1828.[2] D. G. Lowe, Distinctive Image Features from Scale-Invariant Keypoints,International Journal of Computer Vision 60 (2) (2004) 91–110.[3] K. Mikolajczyk, C. Schmid, A performance evaluation of local descriptors,IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (10)(2005) 1615–1630.[4] G.-S. Xie, X.-Y. Zhang, W. Yang, M. Xu, S. Yan, C.-L. Liu, LG-CNN: Fromlocal parts to global discrimination for fine-grained recognition, PatternRecognition 71 (2017) 118–131.30[5] F. Liu, G. Lin, C. Shen, CRF learning with CNN features for image seg-mentation, Pattern Recognition 48 (10) (2015) 2983–2992.[6] Z. Tu, W. Xie, Q. Qin, R. Poppe, R. C. Veltkamp, B. Li, J. Yuan, Multi-stream CNN: Learning representations based on human-related regions foraction recognition, Pattern Recognition 79 (2018) 32–43.[7] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsuper-vised feature learning, in: Proceedings of the International Conference onArtificial Intelligence and Statistics, 215–223, 2011.[8] D. Wang, X. Tan, Unsupervised feature learning with C-SVDDNet, PatternRecognition 60 (2016) 473–485.[9] Y. Yuan, J. Wan, Q. Wang, Congested scene classification via efficientunsupervised feature learning and density estimation, Pattern Recognition56 (2016) 159–169.[10] H. Bourlard, Y. Kamp, Auto-association by multilayer perceptrons andsingular value decomposition, Biological Cybernetics 59 (4-5) (1988) 291–294.[11] P. Smolensky, Information Processing in Dynamical Systems: Foundationsof Harmony Theory, vol. 1, chap. 6, MIT Press, Cambridge, 194–281, 1986.[12] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, Greedy layer-wise train-ing of deep networks, in: Advances in Neural Information Processing Sys-tems, 153–160, 2007.[13] G. E. Hinton, S. Osindero, Y.-W. Teh, A fast learning algorithm for deepbelief nets, Neural Computation 18 (7) (2006) 1527–1554.[14] S. Zhang, J. Wang, X. Tao, Y. Gong, N. Zheng, Constructing deep sparsecoding network for image classification, Pattern Recognition 64 (2017) 130–140.31[15] P. Tang, X. Wang, Z. Huang, X. Bai, W. Liu, Deep patch learning forweakly supervised object classification and discovery, Pattern Recognition71 (2017) 446–459.[16] R. Bekkerman, M. Bilenko, J. Langford, Scaling Up Machine Learning:Parallel and Distributed Approaches, in: Proceedings of the 17th ACMSIGKDD International Conference Tutorials, 4, 2011.[17] Y. Cao, Y. Chen, D. Khosla, Spiking deep convolutional neural networksfor energy-efficient object recognition, International Journal of ComputerVision 113 (1) (2015) 54–66.[18] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, Y. Bengio, Quantizedneural networks: Training neural networks with low precision weights andactivations, Journal of Machine Learning Research 18 (187) (2018) 1–30.[19] S. Tang, Deep Learning Processor List, URL https://github.com/chethiya/Deep-Learning-Processor-List, Accessed on 11/7/2018.[20] C. D. James, J. B. Aimone, N. E. Miner, C. M. Vineyard, F. H. Rothganger,K. D. Carlson, S. A. Mulder, T. J. Draelos, A. Faust, M. J. Marinella, et al.,A historical survey of algorithms and hardware architectures for neural-inspired and neuromorphic computing applications, Biologically InspiredCognitive Architectures 19 (2017) 49–64.[21] S. K. Esser, P. A. Merolla, J. V. Arthur, A. S. Cassidy, R. Appuswamy,A. Andreopoulos, D. J. Berg, J. L. McKinstry, T. Melano, D. R. Barch,C. di Nolfo, P. Datta, A. Amir, B. Taba, M. D. Flickner, D. S. Modha,Convolutional networks for fast, energy-efficient neuromorphic computing,Proceedings of the National Academy of Sciences 113 (41) (2016) 11441–11446.[22] Y. Bengio, A. Fischer, T. Mesnard, S. Zhang, Y. Wu, From STDP to-wards biologically plausible deep learning, in: Deep Learning Workshop,International Conference on Machine Learning, 2015.32[23] F. Kamaruzaman, A. A. Shafie, Recognizing faces with normalized localGabor features and spiking neuron patterns, Pattern Recognition 53 (2016)102–115.[24] J. Dennis, H. D. Tran, H. Li, Combining robust spike coding with spikingneural networks for sound event classification, in: International Conferenceon Acoustics, Speech and Signal Processing, 176–180, 2015.[25] M.-J. Escobar, G. S. Masson, T. Vieville, P. Kornprobst, Action recognitionusing a bio-inspired feedforward spiking network, International Journal ofComputer Vision 82 (3) (2009) 284.[26] J. H. Lee, T. Delbruck, M. Pfeiffer, Training deep spiking neural networksusing backpropagation, Frontiers in Neuroscience 10 (2016) 508.[27] G.-G. Bi, M.-M. Poo, Synaptic modifications in cultured hippocampal neu-rons: dependence on spike timing, synaptic strength, and postsynaptic celltype, Journal of Neuroscience 18 (24) (1998) 10464–10472.[28] D. Querlioz, O. Bichler, C. Gamrat, Simulation of a memristor-based spik-ing neural network immune to device variations, in: International JointConference on Neural Networks, 1775–1781, 2011.[29] P. U. Diehl, M. Cook, Unsupervised learning of digit recognition usingspike-timing-dependent plasticity, Frontiers in Computational Neuroscience9 (2015) 99.[30] S. R. Kheradpisheh, M. Ganjtabesh, T. Masquelier, Bio-inspired unsuper-vised learning of visual features leads to robust invariant object recognition,Neurocomputing 205 (2016) 382–392.[31] S. R. Kheradpisheh, M. Ganjtabesh, S. J. Thorpe, T. Masquelier, STDP-based spiking deep convolutional neural networks for object recognition,Neural Networks 99 (2018) 56–67.33[32] M. Mozafari, S. R. Kheradpisheh, T. Masquelier, A. Nowzari-Dalini,M. Ganjtabesh, First-spike-based visual categorization using reward-modulated STDP, IEEE Transactions on Neural Networks and LearningSystems 29 (12) (2018) 6178–6190.[33] P. Falez, P. Tirilly, I. M. Bilasco, P. Devienne, P. Boulet, Mastering theOutput Frequency in Spiking Neural Networks, in: International Joint Con-ference on Neural Networks, 1–8, 2018.[34] J. Philbin, M. Isard, J. Sivic, A. Zisserman, Descriptor learning for efficientretrieval, in: European Conference on Computer Vision, 677–691, 2010.[35] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P.-A. Manzagol, Stackeddenoising autoencoders: Learning useful representations in a deep networkwith a local denoising criterion, Journal of Machine Learning Research11 (Dec) (2010) 3371–3408.[36] N. Jiang, W. Rong, B. Peng, Y. Nie, Z. Xiong, An empirical analysis ofdifferent sparse penalties for autoencoder in unsupervised feature learning,in: International Joint Conference on Neural Networks, 1–8, 2015.[37] A. Makhzani, B. Frey, k-Sparse autoencoders, in: International Conferenceon Learning Representations, 2014.[38] K. Gupta, A. Majumdar, Learning autoencoders with low-rank weights, in:International Conference on Image Processing, 3899–3903, 2017.[39] J. L. Krichmar, P. Coussy, N. Dutt, Large-scale spiking neural networks us-ing neuromorphic hardware compatible models, ACM Journal on EmergingTechnologies in Computing Systems 11 (4) (2015) 36.[40] B. Han, A. Sengupta, K. Roy, On the energy benefits of spiking deep neu-ral networks: A case study, in: International Joint Conference on NeuralNetworks, 971–976, 2016.34[41] E. M. Izhikevich, Which model to use for cortical spiking neurons?, IEEETransactions on Neural Networks 15 (5) (2004) 1063–1070.[42] A. Tavanaei, M. Ghodrati, S. R. Kheradpisheh, T. Masquelier, A. S. Maida,Deep learning in spiking neural networks, CoRR abs/1804.08150.[43] M. S. Livingstone, D. H. Hubel, Anatomy and physiology of a color systemin the primate visual cortex, Journal of Neuroscience 4 (1) (1984) 309–356.[44] R. Brette, Philosophy of the spike: rate-based vs. spike-based theories ofthe brain, Frontiers in Systems Neuroscience 9 (2015) 151.[45] R. VanRullen, S. J. Thorpe, Rate coding versus temporal order coding:What the retinal ganglion cells tell the visual cortex, Neural Computation13 (6) (2001) 1255–1283.[46] S. Rifai, P. Vincent, X. Muller, X. Glorot, Y. Bengio, Contracting auto-encoders: Explicit invariance during feature extraction, in: InternationalConference on Machine Learning, 833–840, 2011.[47] A. Kriszhevsky, Learning Multiple Layers of Features from Tiny Images,Tech. Rep., University of Toronto, 2009.[48] M. D. Zeiler, ADADELTA: An adaptive learning rate method, CoRRabs/1212.5701.[49] C.-C. Chang, C.-J. Lin, LIBSVM: a library for support vector machines,Transactions on Intelligent Systems and Technology 2 (3) (2011) 27.[50] P. O. Hoyer, Non-negative matrix factorization with sparseness constraints,Journal of Machine Learning Research 5 (2004) 1457–1469.[51] A. Watt, N. Desai, Homeostatic Plasticity and STDP: Keeping a Neuron’sCool in a Fluctuating World, Frontiers in Synaptic Neuroscience 2 (2010)5.35