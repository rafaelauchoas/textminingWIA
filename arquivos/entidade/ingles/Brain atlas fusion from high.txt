AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptPattern Recognit. Author manuscript; available in PMC 2018 March 01.Published in final edited form as:Pattern Recognit. 2017 March ; 63: 531–541. doi:10.1016/j.patcog.2016.09.019.Brain Atlas Fusion from High-Thickness Diagnostic Magnetic Resonance Images by Learning-Based Super-ResolutionJinpeng Zhang#,a, Lichi Zhang#,a,c, Lei Xianga, Yeqin Shaob, Guorong Wuc, Xiaodong Zhoud, Dinggang Shen*,c,e, and Qian Wang*,aaMed-X Research Institute, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200240, ChinabNantong University, Nantong, Jiangsu 226019, ChinacDepartment of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United StatesdShanghai United Imaging Healthcare Co., Ltd., Shanghai 201815, ChinaeDepartment of Brain and Cognitive Engineering, Korea University, Seoul 02841, Republic of KoreaAbstractIt is fundamentally important to fuse the brain atlas from magnetic resonance (MR) images for many imaging-based studies. Most existing works focus on fusing the atlases from high-quality MR images. However, for low-quality diagnostic images (i.e., with high inter-slice thickness), the problem of atlas fusion has not been addressed yet. In this paper, we intend to fuse the brain atlas from the high-thickness diagnostic MR images that are prevalent for clinical routines. The main idea of our works is to extend the conventional groupwise registration by incorporating a novel super-resolution strategy. The contribution of the proposed super-resolution framework is two-fold. First, each high-thickness subject image is reconstructed to be isotropic by the patch-based sparsity learning. Then, the reconstructed isotropic image is enhanced for better quality through the random-forest-based regression model. In this way, the images obtained by the super-resolution strategy can be fused together by applying the groupwise registration method to construct the required atlas. Our experiments have shown that the proposed framework can effectively solve the problem of atlas fusion from the low-quality brain MR images.KeywordsBrain atlas; super-resolution; image enhancement; sparsity learning; random forest regression; groupwise registrationEmail addresses: jinpengzhangsjtu@gmail.com (Jinpeng Zhang#), lichizhang@sjtu.edu.cn (Lichi Zhang#), dgshen@med.unc.edu (Dinggang Shen*), wang.qian@sjtu.edu.cn (Qian Wang*)Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.1. IntroductionPage 2Medical resonance (MR) imaging has become a pivotally important tool in many brain-related clinical applications and studies. Without introducing hazardous ionizing radiation, the technique allows researchers to observe in-vivo neural structures and functions in a non-invasive way. Large-scale studies are thus enabled for (early) brain development (Thompson et al., 2000; Casey et al., 2000; Lenroot and Giedd, 2006), maturation (Sowell et al., 1999; Paus et al., 2001), and aging (Resnick et al., 2000; Raz et al., 2005). The technique has also provided a unique perspective to investigate disease anomalies (Frisoni et al., 2010; Polman et al., 2011) and to assess the effects of pharmacological interventions (Mulnard et al., 2000; Jack et al., 2004). In general, MR imaging has played a key role in the field of neuroscience as well as translational medicine. Challenged by the studies of larger scales, a lot of efforts have been devoted toward computer-assisted automatic analysis of brain MR images.The brain atlas, which can often be fused from individual brain MR images, has attracted a lot of interest (Mazziotta et al., 1995; Joshi et al., 2004). Given a group of subjects, the atlas encodes the common morphological information within the group. To this end, researchers can compare the atlases of two individual groups (e.g., the diseased group and the normal control group) and then reveal the subtle difference that might be connected with the disease. Meanwhile, the atlas provides a common space where the inter-subject variation within a population can be measured quantitatively. For example, after being registered with the atlas, each subject owns a deformation field that is typically regarded as the pathway between the subject and the atlas. Since the deformation pathways of all images in the group are established upon the same common space defined by the atlas, comparing the estimated deformation fields of all images can capture the inter-subject variation within the group. Obviously, it is important to properly designate a high-quality atlas in advance for many similar studies.However, it is yet rare and difficult to fuse the atlas from the low-quality diagnostic MR images (i.e., with high inter-slice thickness). Currently most aforementioned studies are focusing on high-quality and (nearly) isotropic imaging data, which has identical resolutions in all dimensions. The acquisitions are often conducted on designated MR scanners and may cost high. The resources required for high-quality imaging, however, are not always available. In developing countries such as China, most diagnostic MR images are still scanned with high inter-slice thickness, partially due to concerns on costs of radiation examinations and limited medical resources per capita. For the real clinical data with high inter-slice thickness, the challenge to fuse the brain atlas is yet unresolved. The lack of the atlas fusion method apparently undermines the efforts to incorporate the low-quality diagnostic imaging data into clinical researches.In this work, we intend to apply learning-based super-resolution to real clinical low-quality brain MR images and then fuse the atlas in the groupwise manner (Joshi et al., 2004). Our super-resolution consists of two stages. First, since the subject images are heterogeneous with high inter-slice thickness, we adopt the non-local patch-based strategy and utilize sparsity learning to reconstruct the subject images in the isotropic space. Second, we turn to random forest and learn the regression model for image enhancement, such that the Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 3reconstructed isotropic image (with relatively low quality) is mapped to be of higher quality (i.e., by suppressing incorrect local anatomical patterns). With all subject images processed through super-resolution, we apply groupwise registration and then fuse the atlas. Specifically, for the iteratively updated group mean image, we can apply the aforementioned forest regression model to enhance its quality. The enhanced group mean image provides better guidance in groupwise registration and leads to the atlas with higher-quality essentially.The rest of this paper is organized as follows. In Section 2, we survey the recent development of the relevant works, especially atlas fusion and super-resolution. In Section 3, we provide details of the proposed framework, including the learning-based super-resolution technique and subsequent groupwise registration. In Section 4, we demonstrate the feasibility of our novel framework through experiments. Finally, we provide the discussions with conclusions in Section 5.2. Related Works2.1. Atlas FusionIn the literature, it is popular to select a third-party standard atlas and then conduct group-level statistical analysis. For example, MNI-152 is one of the most widely accepted atlases (Mazziotta et al., 1995). The fusion of the MNI-152 atlas clearly demonstrates two important steps when fusing the brain atlas: (1) 152 individual subjects are registered with a certain template; (2) all registered images are averaged to produce the desired atlas. The MNI-152 atlas was later adopted by International Consortium for Brain Mapping (ICBM) and became part of Statistical Parametric Mapping (SPM), in which it provides automatic parcellation of neural regions-of-interest (ROI) (Tzourio-Mazoyer et al., 2002). The parcellation facilitates numerous studies upon brain morphology and structural/functional connectivity.Though simple and convenient, it is not necessarily proper to select the atlas manually for the atlas-based analysis. The anatomical variation across human brains is typically high, implying that a single and external atlas cannot fully account for individual subjects (Toga and Thompson, 2001). The (pairwise) registration between the subject(s) and the atlas can also introduce systemic bias into subsequent statistical analysis. For example, it is known that the Alzheimer’s Disease (AD) can cause brain tissue atrophy. When examining the impact of AD upon brain morphology, a large-scale group of both patients and normal controls is usually recruited for scanning the anatomical structures. If the atlas was corresponding to normal control, then it would be relatively easier to register the normal control images with the atlas than to register the patient images. In this way, the overall quality of the registered patient data would become less reliable, resulting in the imbalanced signal-to-noise ratios of the patients and the normal controls.To attain increased signal-to-noise ratio and unbiased statistical analysis, the atlas is better to be fused from all subject images in the data-driven way. Groupwise registration has provided such an alternative solution for atlas fusion other than the conventional methods (Joshi et al., 2004). In groupwise registration, all images can deform freely until they become close Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 4enough with each other. Then, the mean of all deformed images is often regarded as the atlas of the subjects. The technique of groupwise registration has attracted a lot of research interest, since it is capable of avoiding the manual selection of the atlas as well as the bias.The task of groupwise registration can be accomplished by learning the intrinsic distribution of the group of images and adapting state-of-the-art pairwise registration technique. In the group mean method (Joshi et al., 2004), for example, the group mean image is firstly built and acts as the template, to which all subject images are registered in the pairwise way. Then, the group mean image is updated by averaging all subject images after they are deformed following their tentative deformation fields estimated previously. The registration-averaging scheme is iteratively performed until convergence. That is, the final group mean image is produced as the atlas, while all subject images are well aligned with the mean image. To be robust with potential outlier subjects, the Riemannian manifold is introduced to embed all subject images (Fletcher et al., 2009). The manifold helps reveal the geometric median of the group, such that possible outliers can be easily suppressed for contributing to estimate the Fréchet mean of the group.The quality of the images, which participate into groupwise registration, is apparently important regarding brain atlas fusion. For the group mean method particularly, the mean image in the initial stage of the groupwise registration is very blur since the subject images are not well aligned yet. The blur mean image can substantially undermine the efforts to register individual subjects with it. That is, the optimization of groupwise registration may be challenged when registering a high-quality subject image with the blur group mean image precisely even from the beginning. There are several solutions in the literature to address the image quality issue.•••Rather than averaging intensities in the group mean method, the Bayesian-based approach estimates the atlas by iteratively deforming an initial guess to the center of the group (Ma et al., 2008). In this way, the final atlas is sharp but at the expense of the bias for choosing an individual subject as the starting point. More recently, the sharp mean image is fused by assigning different weights to the subjects, in accordance to their predicted distances/similarities to the final atlas (Wu et al., 2011).All subject images can be embedded into the manifold that is often described by the tree (Hamm et al., 2010) or the graph (Jia et al., 2010). Each subject image can identify other subjects from the manifold, which are closer to the mean position of the group. By using those identified subjects for guidance and registering with them in the pairwise way, all subjects can approach to the atlas of the group iteratively. Note that no blur mean image is required for the above procedure.It is feasible to minimize the variation across individual subject images by optimizing their deformation fields directly and simultaneously. The variation can be measured in different ways (Learned-Miller, 2006; Wu et al., 2015). For example, we used the Jensen-Shannon divergence to capture the inhomogeneity across all subjects, while each subject’s voxel was signified by multiple attributes Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 5rather than the single intensity only (Wang et al., 2010). All images can converge to the mean of the group gradually by refining their deformation fields and minimizing the overall divergence computed upon the entire group.2.2. Super-resolutionsThe goal of super resolution is to overcome the low quality of the given images due to the limitation of the acquisition. The reconstructed high-resolution output is expected to provide more detailed that is critical to subsequent applications (Park et al., 2003). The super-resolution is widely applied to areas including medical imaging, satellite imaging, and video (Yang et al., 2010).Recently, there are many attempts in the literature to implement the super-resolution methods on the MR images, the resolutions of which are usually insufficient for detailed neuroanatomical analysis. Particularly, the acquired 3D MR images are anisotropic, which have high-thickness in the slice-selection direction as previously mentioned. One way to tackle this problem is to fuse multiple scans of the same subjects to improve the details in the image. A possible solution in this field is to obtain high-resolution MR image by combining multiple orthogonal scans of the same subject (Tamez-Pena et al., 2001; Greenspan et al., 2002). For example, Bai et al. (2004) adopt the maximum a-posteriori (MAP) strategy to combine the two orthogonal MR brain images into one high-resolution result. However, the methods mentioned above cannot be applied here, since the input images in our works are high-thickness in the slice-selection direction, and lack the data from other views.On the other hand, the example-based super-resolution intends to learn the relationship between the low-resolution and the corresponding high-resolution images. The learning can often be conducted by using the patch-based strategy (Freeman et al., 2000, 2002). The trained model is then applied to the input image, such that numerous low-resolution patches are extracted for the determination of the corresponding high-resolution patches. The learning-based strategy can greatly improve the performance of the super-resolution process, and resolve the limitations in the multi-image-based strategies. In this paper, we incorporate the random regression forest (Breiman, 2001) to enhance the quality of the images, which is suitable for our needs due to its robustness and effectiveness.As stated in Section 1, the main goal of this paper is to fuse the brain atlas from the diagnostic MR images with high inter-slice thickness by following the groupwise manner. To address the issues of low-resolution input subject images, the novel strategy of learning-based super-resolution is proposed here to aid the atlas-fusion process, which aims to convert each high-thickness subject image to the low-thickness high-quality isotropic image prior to the process of groupwise registration. The overall pipeline of the proposed framework is presented in Figure 1, which consists of the super-resolution strategy and the groupwise registration method.To develop the super-resolution method we follow the two-stage strategy listed as follows:Pattern Recognit. Author manuscript; available in PMC 2018 March 01.3. Method  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 61.2.We reconstruct the isotropic image data for each subject by the patch-based sparsity learning. For a certain 2D patch in the subject image, we seek for its sparse representation with respect to the collection of 2D patches from a set of high-quality training images. The sparse representation then propagates to the 3D isotropic image space, as the corresponding 3D patch of the subject can also be represented by the 3D patches of the training images.The regression model is learned for the mapping between the corresponding patches in the reconstructed isotropic image, whose quality is relatively low, and its high-quality ground-truth. Given a new subject and its reconstructed isotropic image, the regression model can compute the high-quality enhanced image for the subject in the patch-by-patch way.It is worth noting that we develop the two-stage strategy, in order to ensure the stability of the framework to produce the expected high-resolution images from the input low-resolution ones. If we directly extracted the features from the images with high inter-slice thickness, the obtained information might be too limited to predict the missing parts of the high-resolution outputs in the one-shot fashion. By decomposing the challenging task into two stages, however, the difficulty is reduced for each stage. Therefore, we incorporate the patch-based sparsity learning strategy to firstly construct the low-thickness images, from which the final high-resolution images are later estimated. The detailed flowchart including the two stages in the super-resolution strategy is illustrated in Figure 2.After super-resolution, all subjects are converted to be isotropic and high-quality, while their inter-slice thickness is reduced. Then, we are able to apply groupwise registration upon the processed subject images. Specifically, we adopt the sharp mean method (Wu et al., 2011) for the estimation of individual deformation fields of all subjects. The fused mean image of the group is regarded as the atlas for all subjects in the final.3.1. Sparsity Learning for Isotropic ReconstructionSince the subject images are heterogeneous in terms of their various and high inter-slice thickness, the first step is to reconstruct the isotropic image (e.g., with the voxel spacing of 1 × 1 × 1mm3) for each subject under consideration. Specifically, we employ a set of high-quality isotropic 3D MR images as the training data, and then turn to patch-based sparsity learning to solve the problem. Given a certain voxel x in the input subject image S, a rectangular patch can be extracted as the signature for x. Note that patch, due to the high inter-slice thickness of S. Similarly, for the voxel y in the i-th training is a 2D-only image (Ti), we can define its 2D patch as , as well as that represents the coupled 3D cubic patch centered at y ∈ Ti. Compared with the 2D patch incorporates appearance information from multiple slices in the isotropic training image Ti, which essentially contributes to reconstruct the missing slices around x ∈ S., the 3D cubic patch We utilize sparsity learning to compute the 3D cubic patch around the subject voxel x, based on demonstrated its powerful capability in computer vision and medical image analysis (Wright and the coupled training data. The technique of sparsity learning has recently Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 7et al., 2010). In our early works, we used sparsity learning to establish voxel-wise anatomical correspondences, which essentially lead to high-performance image segmentation (Wu et al., 2015) and registration (Wang et al., 2015a). Specifically, we can solve for the sparse representation of by regarding the collection of the 2D training patches (1)The scalar parameter α in the above regulates the sparsity constraint upon the coefficients {ciy}, such that only a limited number of the training patches are selected for the linear . The non-local strategy is also applied – only the training patches, representation of whose center voxels are within the neighborhood x, can potentially contribute to represent .The coefficient of ciy not only indicates the contribution of the training patch p̃iy to represent the subject patch further presume that the similarity based on 2D rectangular patches can be propagated to 3D , but also encodes their mutual similarity (Wright et al., 2010). We cubic patches. Therefore, the 3D cubic patch centered at x ∈ S, can be estimated from all 3D training patches by in the subject image, which is isotropic and (2)The sparsity-learning-based reconstruction scheme can be performed upon all voxels in the low-quality subject image. For each voxel x, a corresponding 3D patch fill in the nearby cubic area of x in the isotropic image space. To address the overlapping between two cubic patches corresponding to x and its certain neighbor, we impose a sigmoid is computed to decay to modulate the computed 3D patch. In general, the location of x, while its contribution to the isotropic image space is gradually reduced with an increasing offset from x. The contribution drops to zero when reaching the boundary owns the highest confidence at of . In the final, the entire 3D isotropic brain MR image can be reconstructed by incorporating the computed 3D cubic patches corresponding to all voxels in the low-quality subject image and after proper spatial normalization.3.2. Regression Forest for Image EnhancementThe quality of the reconstructed isotropic image for each subject might be limited. As the reconstruction is accomplished by (adaptively) averaging multiple patches of the training Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 8images, the reconstructed subject image may suffer from relatively low sharpness and altered neuroanatomical patterns. Therefore, the subsequent atlas fusion can be undermined if we apply groupwise registration to the reconstructed subject images directly. As a remedy, we propose to further enhance the quality of the reconstructed images through supervised learning. That is, we adopt the patch-based regression forest technique to establish the mapping between the low-quality 3D patches in the reconstructed images and their high-quality ground-truth. The regressor, which is inspired by Dong et al. (2016), can eliminate artifacts within the reconstructed images and improve their quality. In general, with the learned regression model, we can enhance all reconstructed subject images in the patch-by-patch manner.Structured Regression Forest—Recently, regression forest has achieved promising results in medical image analysis due to its efficiency and robustness (Shao et al., 2015; Huynh et al., 2016). The regression forest technique essentially learns the model for the complicated mapping between the input and the output variables. On one hand, the input denotes the 3D cubic patch in the reconstructed image and its abundant features. On the other hand, the output (or the regression target) is the corresponding patch (i.e., sharing the same center voxel with the input patch) in the enhanced image. Note that the input and the output patches are not necessarily of the same size. The output patch is actually smaller than the input patch in our implementation, as information in the input patch is expected to be condensed for better quality of the output patch.The regression forest F consists of b decision trees {T1, T2, …, Tb}. Based on the uniform bagging strategy (Breiman, 2001), each tree is trained by using only a subset of features and samples that are randomly selected. In the training stage, the decision trees grow by recursively splitting the randomly selected input training samples into the left and the right children nodes, based on the bootstrapping of the training samples of the patches in the reconstructed images and their extracted features (Breiman, 2001). To split the training samples at each node, we choose a certain feature and then apply the optimal threshold to it. The feature, as well as the threshold, is determined by exhaustive search within the pool of the random selected features. Note that the process of feature extraction is illustrated later in this section. The best splitting is therefore associated with the maximal reduction of the variation of the training samples. The node splitting stops when reaching the maximal depth of the decision tree or the minimal number of training samples in the leaf node. The regression predictor for each tree Ti is therefore written as g(f(Q, B), Ti), where f(Q, B) is the feature vector, Q is the input patch, and B is the source that the feature vector is extracted from.In the testing stage, for each patch Q′ extracted from the test image I′, we obtain its enhancement estimate by applying the regressor F. For each trained tree Ti, Q′ is first pushed into its root node. Guided by the stored splitting functions, the patch arrives at a certain leaf node. The corresponding estimation result is obtained as g(f(Q′, B), Ti), therefore the overall prediction for the trained forest F can be estimated by averaging the obtained results from all the trees, which is given as:Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 9(3)Furthermore, we adopt the structured strategy to learn the regression forest model (Dollár and Zitnick, 2013). That is, our regression target (or output) is a 3 × 3 × 3mm3 patch, instead of a single voxel as in the conventional methods. The variation of the training samples (or the splitting of each node) is thus determined from the corresponding target patches. Note that the structured learning strategy can help preserve detailed anatomical structures, since all voxels in a certain target patch are enhanced jointly and simultaneously.Instead of measuring the distance/variation of the target patches in the high-dimensional Euclidean space, we switch to the low-dimensional Eigen-space. To this end, we apply principal component analysis (PCA) and represent all target patches with the five major eigenvectors only. The variation of the target patches can thus be computed efficiently. Based on the variation, the optimal feature as well as the threshold can be determined for each node, such that the training samples are split into its two children nodes.Note that the PCA-based representation is only used for computing the variation of the target patches. In each (leaf) node of the regression forest, all original training samples and their corresponding target patches that belong to the specific node are stored. The median (instead of the mean) of the target patches will be used as the output of the node in the testing stage. Similarly, the median output of the entire forest, instead of the mean from individual trees, will be used as the enhanced patch for ensemble learning.Feature Extraction—In this paper, there are two feature information extracted from the patches, one is the center location of the patch, the other is the intensity information of the patch obtained by applying the 3D Haar-like operators. Note that the Haar-like features can be defined in various ways (Wang et al., 2015b). Here the Haar-like operators do not correspond to a complete wavelet band. To compute the Haar-like features for the voxel x with its corresponding patch Q, we randomly determine two cubic areas (namely R1 and R2) within the patch region R. Then, the Haar-like features can be calculated as 1) the mean intensity in R1, or 2) the difference of the mean intensities of R1 and R2. The equation to compute the Haar-like features fHaar is written as follows:(4)where ρ is used to determine the selection of one or two cubic regions. Also note that the sizes of R1 and R2 are chosen from an arbitrary range of {1, 3, 5}. All those parameter settings to compute the Haar-like features are randomly decided in the training stage, and the extracted patches from the input images in the testing stage will follow the same settings when applying the trained forest model.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 10Cascaded Learning—To further improve the performance of super-resolution, we follow the auto-context strategy (Tu and Bai, 2010) that intends to cascade several forest regressors together, and implement image enhancement iteratively. The pipeline is summarized as follows:1.2.3.4.In the first iteration, the regressor is trained by following the structured regression forest strategy described above, which uses only the reconstructed isotropic images as input.When the regressor is trained, it is applied to not only the input testing images, but also the training images to obtain their corresponding estimates.In the other iterations, it also considers the tentatively enhanced image that is output by the preceding regressor. Simply put, the estimates of the training images from the previous iteration also participate in the process of forest training.Each training patch has its Haar-like features obtained from the two sources, which are concatenated together for training the regressor. Note that the priority of feature information from the two sources are treated equally, that the feature numbers from them are identical.5.Iterate from Step 2 until the iteration number set for the works reach.To this end, the enhanced image can be passed through individual regressors in the pipeline for gradual refinement. The output of the last regressor is regarded as the output of the entire pipeline in the final.3.3. Groupwise Registration for Atlas FusionAfter all subject images are processed through super-resolution in their native spaces, we apply groupwise registration to fuse the atlas. The groupwise registration starts with affine alignment of all images, followed by the sharp-mean-based deformable registration (Wu et al., 2011). In every iteration of the sharp-mean-based groupwise registration, the group mean image is produced by dynamically weighting the contributions from all tentatively deformed subjects for high sharpness. Moreover, we apply the aforementioned regression forest (c.f. Section 3.2) to further enhance the quality of the group mean image. The enhanced group mean image then provides guidance, as each subject optimizes its deformation pathway toward the group mean by pairwise registration (i.e., diffeomorphic Demons (Vercauteren et al., 2009)). When completing all iterations of the groupwise registration, we compute the final group mean image and regard it as the atlas of all subjects.•The sharp group mean estimator intends to acquire the mean image of high sharpness by the adaptive weighting strategy. In every iteration, the weights are adaptive across not only the individual subject images but also all spatial locations in the image space. The high weight indicates that the subject under consideration is similar with the atlas at the specific location, and vice-versa. Since all subject images are mostly aligned with the mean image in the final stage of the groupwise registration, the weights become almost equal for Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 11individual subjects/locations as the unbiased groupwise registration approaches to the end.•We further apply the regression model to enhance the quality of the group mean image in every iteration of the groupwise registration. The (tentative) mean image, which guides the registration of individual subjects, is generated by adaptively averaging multiple subject images. Though the process to generate the mean image is different from that to reconstruct the isotropic subject images, both of them may suffer from similar artifacts/patterns caused by intensity averaging. To this end, we argue that the pre-trained regression forest can help improve the quality of the (tentative) mean image in every iteration.4. Experimental ResultsIn this section we evaluate the proposed framework by conducting experiments on both the simulated and the real diagnostic data. By following the pipeline described in Figure 2, we commence by up-scaling the resolution of the input images, and then improving the image quality by applying the regression forest technique. The estimates after the super-resolution strategy are then fused together by the groupwise registration method to construct the required atlas. Note that both datasets have been pre-processed by following the standard protocol (Wang et al., 2015a). Stated succinctly, we applied the N4ITK (Tustison et al., 2010) for bias correction, and then used ITK1-based histogram-matching program to the dataset under study. It is worth noting that we apply the same parameter settings for the pre-processing works on two datasets.4.1. Simulated DataWe generate the simulated data from the NIREP NA0 dataset for quantitative validation of the proposed framework. There are 16 subjects in the NIREP NA0 dataset. For each subject, its high-quality T1-weighted MR acquisition carries expert labeling of 32 anatomical ROIs that are listed in Table 1. All subject images were re-sampled to the isotropic spacing of 1 × 1 × 1mm3 and aligned to the MNI-152 space by affine registration in pre-processing. Then, we re-sampled the subject images to the spacing of 1 × 1 × 8mm3, which were used as the mimics of the diagnostic MR images with high inter-slice thickness.Isotropic Reconstruction—The simulated subject images are firstly processed through the sparsity-learning-based isotropic reconstruction in our framework. We have adopted the leave-one-out validation scheme. That is, after selecting a certain subject (with the spacing of 1 × 1 × 8mm3) as the test, we use the rest 15 images (with the spacing of 1 × 1 × 1mm3) for training. We set the radius of the non-local search neighborhood as 4 empirically. The size of the 2D patch is 5 × 5mm2 in all experiments.In Figure 3, we show the high-thickness input of a certain subject in the first column. The reconstructed isotropic image is provided in the second column accordingly. The quality of the reconstructed subject image apparently becomes much better than the input (especially 1http://www.itk.orgPattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 12in the sagittal and the coronal views), as the missing slices are mostly compensated during the isotropic reconstruction. However, there are artifacts in the reconstructed image (e.g., highlighted by the red arrows), which will be handled in the subsequent enhancement stage.Regression-Based Enhancement—The regression forest is built for enhancing the quality of the reconstructed subject image. Given a certain subject, we use the other 15 pairs of the reconstructed images and their ground-truth for training the regression model. Note that the experiments for those 16 images follow the same parameter settings. There are 20 trees trained in the forest model. The maximum depth of each tree is 19. The minimum sample number for the leaf node is 5. The input sample patch size for the regression forest is 15 × 15 × 15mm3. The number of randomly selected Haar features for the extracted patches is 1000 when splitting a certain node in the tree. The output patch size, as mentioned in Section 3.2, is 3 × 3 × 3mm3. In particular, we have cascaded two regressors into the unified regression pipeline in our implementation for computational efficiency. When more iterations are applied, the performance converges gradually, which can also be observed in other literature (Zhang et al., 2016a,b, 2014).The example of the enhanced subject is shown in the third and the fourth columns, of Figure 3. It is observable that many artifacts and incorrect patterns in the reconstructed subject image are eliminated, while the enhanced image becomes more similar to the ground-truth with less noise. Moreover, we can compute the peak signal-to-noise ratio (PSNR) for each subject regarding its ground-truth. In general, PSNR, which is widely used in computer vision, quantifies the similarity between the (enhanced) subject image and the ground-truth. The overall PSNR for all 16 subjects is 24.28 ± 0.38 after isotropic reconstruction. The measure then increases to 24.81 ± 0.38 after the regression-based enhancement. The p-value in the paired t-test is very low (3.3 × 10−13), indicating that the regression-based enhancement has achieved statistically significant improvement over the quality of the subject images.We also conducted the quantitative evaluation of estimates in different stages of the proposed framework by measuring structural similarity (SSIM) (Wang et al., 2004), which is widely applied to compare images at the same scene in different resolutions. Table 2 presents the comparison between the results with the high-resolution ground-truth for all 16 subjects. The second column presents the performance when simply up-scaling the input low-resolution images by using the trilinear interpolation method, the third column shows when isotropic reconstruction is implemented, and the fourth column is for the random-forest-based enhancement. It can be observed that the performance in each stage is gradually increased, and reaches the peak when all the components in the super-resolution strategy have been implemented. It is also worth noting that the p-values in the two-tailed paired t-tests between any two stages are below 0.05, indicating the statistical significant improvement of the proposed framework when adopting the two components of the super-resolution strategy.We also conduct the experiment of sensitivity study on different inferior-superior resolutions of the input test images, to demonstrate the robustness of super-resolution strategy. The first row in Figure 4 shows the input images with the simulated inferior-superior spacing of 2mm, Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 134mm and 8mm, respectively, and the second row shows their corresponding results after implementing the super-resolution strategy. It can be observed that the differences between the reconstructed images are quite trivial, which shows that the proposed framework can be very robust regarding various inferior-superior resolutions of the input images.Groupwise Registration—The enhanced subject images are further registered in the groupwise manner, which is critical to the quality of the final atlas. Based on the expert labeling of the subjects, we can quantitatively measure the quality of the groupwise registration by the Dice ratio. That is, given two images and their anatomically corresponding ROIs A and B, the Dice ratio is calculated as . The operator | · | here computes the size of the ROI. As all subject images are aligned after groupwise registration, we can warp their ROI labeling to the atlas space. Then, we parcellate the atlas by applying majority voting to all warped subject labels. For each ROI and each subject, the Dice ratio is thus computed in accordance to the atlas and its labeling. The detailed Dice ratios are reported in Figure 5. In particular, the overall Dice ratio for all simulated images (with the spacing of 1 × 1 × 8mm3 and after super-resolution) is 0.749 ± 0.035. The score is lower than applying the groupwise registration to the ground-truth data directly (0.790 ± 0.036, for the spacing of 1 × 1 × 1mm3), as the high inter-slice thickness has severely damaged the quality of the diagnostic images even after sophisticated super-resolution. However, we also compare with the groupwise registration upon the images with the spacing of 2 × 2 × 2mm3. The Dice ratio is 0.732 ± 0.040, indicating that the combination of our framework and the simulated images lead to more accurate registration results. Note that we also conduct the experiments using SSIM measurements to better demonstrate the validity of the registration process. The overall SSIM for all simulated images is 0.9544 ± 0.002, while as for all ground-truth data it is 0.9357 ± 0.003. Note that the SSIM for the simulated images is even higher than the ground-truth. We attribute this phenomenon to the smoothing effect caused by the regression component in the super-resolution strategy. That is, while artifacts are eliminated by the regression forest, the anatomical structure in every subject image can inevitably be smoothed, which results in an increase to the SSIM measure in the final.The finally fused atlases, which are corresponding to the ground-truth data (with the spacing of 1 × 1 × 1mm3) and the simulated diagnostic images (with the spacing of 1 × 1 × 8mm3), are shown in the top and the bottom rows of Figure 6, respectively. Visually there is no major difference between the two atlases, though slightly more abundant anatomical details are available in the ground-truth atlas (e.g., in the occipital lobe). In particular, since both atlases are parcellated by the majority voting of all warped subjects, we can compute the Dice ratios for corresponding ROIs between the two atlases. The overall Dice ratio for all ROIs is 0.896 ± 0.023, and the SSIM value between the two generated atlases is 0.9278, which is high enough to demonstrate the anatomical similarity between the two atlases. In general, the proposed framework can fuse the atlas from the (simulated) low-quality diagnostic brain MR images. Meanwhile, the quality of the atlas is mostly comparable to that fused from the high-quality images.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.4.2. Real Diagnostic DataPage 14A set of 49 healthy subjects is used for fusing the atlas from real diagnostic data. Among them, 16 T1-weighted images are scanned with the spacing of 0.7495 × 0.7495 × 6mm3, while the spacing of the other 33 images is 0.8496 × 0.8496 × 7.2mm3. We applied intra-slice resampling to all subjects and ensure the spacing of each 2D slice to become 1 × 1mm2, as the desired spacing of the atlas is set to 1 × 1 × 1mm3. All subject images are skull-stripped by an expert.The super-resolution of the subjects is based on the NIREP NA0 dataset, which is used for training. Specifically, in the isotropic reconstruction stage, all training images are affinely registered with the subject under consideration first, while the spacing of the warped training images is kept to be 1 × 1 × 1mm3. The isotropic subject image can thus be reconstructed in their own image space, by following the procedure described in Section 3.1. Next, all reconstructed images are affinely registered to the MNI-152 space, where the regression-based image enhancement is conducted. In particular, the regressor(s) acquired in the experiment of the simulated data (c.f. Section 4.1) can be applied directly for the sake of enhancing the reconstructed subject image. Figure 7 presents the estimated results of the real data in different stages of the proposed framework, and the final atlas of all subjects is shown in Figure 8.5. Conclusion and DiscussionIn this work, we intend to fuse the brain atlas from diagnostic MR images, which has mostly been unresolved before. Since the inter-slice thickness of the input MR images is high, it is necessary to perform super-resolution and convert them to be high-quality and isotropic. Therefore, we design the two-stage super-resolution strategy. Each subject is firstly converted to be isotropic following the sparsity-learning-based reconstruction. Then, the reconstructed image is further enhanced by the regression forest, which aims to eliminate artifacts in individual patches. After super-resolution, the subject images are registered in the groupwise manner, such that the atlas of all subjects is fused in the final.Our experiments generally confirm that the proposed framework is effective to solve the problem of atlas fusion regarding the high-thickness diagnostic brain MR images. However, there are several limitations in our current implementation that need future attendance. For example, the isotropic reconstruction is achieved by the non-local patch-based sparsity learning, which often consumes a lot of time (e.g., 6 hours for a typical brain volume in the simulated data, single thread, Intel Xeon E7 CPU). The process can become much faster by performing coupled dictionary learning to the collections of the 2D/3D training patches in advance. Moreover, it might not be optimal to separate the proposed super-resolution into two stages arbitrarily. To this end, we will investigate the future possibility of integrating the reconstruction and the enhancement into a unified model, such that the 2D rectangular patch(s) can predict the high-quality 3D cubic patch directly.The proposed framework can also be challenged by many different types of pathological cases, including atrophy, lesions, and tumors. The reason is that the learning-based super-resolution relies on the training data with similar appearances. For the elderly subjects, the Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 15proposed framework, although reduced in its performance, can still be applied with expected results, since the shape variations caused by the patients’ sympton can still be tolerated in the process. Figure 9 presents the exemplar results obtained in different stages of the framework, which carries atrophy caused by the Alzheimer’s disease. For pathologies that incur a lot of image appearance variation (e.g., lesions and tumors), it is hard to always guarantee the availability of the proper training data and then to reconstruct the high-resolution image by the proposed framework. For example, when reconstructing the 3D patch of a tumor through sparsity learning in our framework, it is required that tumors can be found from the non-local search neighborhoods of the training images. The requirement, however, could not be always satisfied. In the future works we also intend to resolve the robustness issues for the pathological cases.AcknowledgmentsThis work is supported by National Institutes of Health (NIH) grants (EB006733, EB008374, MH100217, MH108914, AG041721, AG049371, AG042599, AG053867, EB022880), National Natural Science Foundation of China (NSFC) grants (61473190, 61401271, 81471733) and Science and Technology Commission of Shanghai Municipality (STCSM) grants (16511101100, 16410722400).ReferencesBai, Y., Han, X., Prince, JL. Super-resolution reconstruction of mr brain images; Proc. of 38th Annual Conference on Information Sciences and Systems (CISS04); 2004. p. 1358-1363.Breiman L. Random forests. Machine learning. 2001; 45:5–32.Casey B, Giedd JN, Thomas KM. Structural and functional brain development and its relation to cognitive development. Biological psychology. 2000; 54:241–257. [PubMed: 11035225] Dollár, P., Zitnick, CL. Computer Vision (ICCV), 2013 IEEE International Conference on. IEEE; 2013. Structured forests for fast edge detection; p. 1841-1848.Dong C, Loy CC, He K, Tang X. Image super-resolution using deep convolutional networks. Pattern Analysis and Machine Intelligence, IEEE Transactions on. 2016; 38:295–307.Fletcher PT, Venkatasubramanian S, Joshi S. The geometric median on riemannian manifolds with application to robust atlas estimation. NeuroImage. 2009; 45:S143–S152. [PubMed: 19056498] Freeman, WT., Jones, TR., Pasztor, EC. Computer Graphics and Applications. Vol. 22. IEEE; 2002. Example-based super-resolution; p. 56-65.Freeman WT, Pasztor EC, Carmichael OT. Learning low-level vision. International journal of computer vision. 2000; 40:25–47.Frisoni GB, Fox NC, Jack CR, Scheltens P, Thompson PM. The clinical use of structural mri in alzheimer disease. Nature Reviews Neurology. 2010; 6:67–77. [PubMed: 20139996] Greenspan H, Oz G, Kiryati N, Peled S. Mri inter-slice reconstruction using super-resolution. Magnetic resonance imaging. 2002; 20:437–446. [PubMed: 12206870] Hamm J, Ye DH, Verma R, Davatzikos C. Gram: A framework for geodesic registration on anatomical manifolds. Medical image analysis. 2010; 14:633–642. [PubMed: 20580597] Huynh T, Gao Y, Kang J, Wang L, Zhang P, Lian J, Shen D. Estimating ct image from mri data using structured random forest and auto-context model. Medical Imaging, IEEE Transactions on. 2016; 35:174–183.Jack C, Shiung M, Gunter J, Obrien P, Weigand S, Knopman D, Boeve B, Ivnik R, Smith G, Cha R, et al. Comparison of different mri brain atrophy rate measures with clinical disease progression in ad. Neurology. 2004; 62:591–600. [PubMed: 14981176] Jia H, Wu G, Wang Q, Shen D. Absorb: Atlas building by self-organized registration and bundling. NeuroImage. 2010; 51:1057–1070. [PubMed: 20226255] Joshi S, Davis B, Jomier M, Gerig G. Unbiased diffeomorphic atlas construction for computational anatomy. NeuroImage. 2004; 23:S151–S160. [PubMed: 15501084] Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 16Learned-Miller EG. Data driven image models through continuous joint alignment. Pattern Analysis and Machine Intelligence, IEEE Transactions on. 2006; 28:236–250.Lenroot RK, Giedd JN. Brain development in children and adolescents: insights from anatomical magnetic resonance imaging. Neuroscience & Biobehavioral Reviews. 2006; 30:718–729. [PubMed: 16887188] Ma J, Miller MI, Trouvé A, Younes L. Bayesian template estimation in computational anatomy. NeuroImage. 2008; 42:252–261. [PubMed: 18514544] Mazziotta JC, Toga AW, Evans A, Fox P, Lancaster J. A probabilistic atlas of the human brain: theory and rationale for its development the international consortium for brain mapping (icbm). Neuroimage. 1995; 2:89–101. [PubMed: 9343592] Mulnard RA, Cotman CW, Kawas C, van Dyck CH, Sano M, Doody R, Koss E, Pfeiffer E, Jin S, Gamst A, et al. Estrogen replacement therapy for treatment of mild to moderate alzheimer disease: a randomized controlled trial. Jama. 2000; 283:1007–1015. [PubMed: 10697060] Park, SC., Park, MK., Kang, MG. Signal Processing Magazine. Vol. 20. IEEE; 2003. Super-resolution image reconstruction: a technical overview; p. 21-36.Paus T, Collins D, Evans A, Leonard G, Pike B, Zijdenbos A. Maturation of white matter in the human brain: a review of magnetic resonance studies. Brain research bulletin. 2001; 54:255–266. [PubMed: 11287130] Polman CH, Reingold SC, Banwell B, Clanet M, Cohen JA, Filippi M, Fujihara K, Havrdova E, Hutchinson M, Kappos L, et al. Diagnostic criteria for multiple sclerosis: 2010 revisions to the mcdonald criteria. Annals of neurology. 2011; 69:292–302. [PubMed: 21387374] Raz N, Lindenberger U, Rodrigue KM, Kennedy KM, Head D, Williamson A, Dahle C, Gerstorf D, Acker JD. Regional brain changes in aging healthy adults: general trends, individual differences and modifiers. Cerebral cortex. 2005; 15:1676–1689. [PubMed: 15703252] Resnick SM, Goldszal AF, Davatzikos C, Golski S, Kraut MA, Metter EJ, Bryan RN, Zonderman AB. One-year age changes in mri brain volumes in older adults. Cerebral cortex. 2000; 10:464–472. [PubMed: 10847596] Shao Y, Gao Y, Wang Q, Yang X, Shen D. Locally-constrained boundary regression for segmentation of prostate and rectum in the planning ct images. Medical image analysis. 2015; 26:345–356. [PubMed: 26439938] Sowell ER, Thompson PM, Holmes CJ, Jernigan TL, Toga AW. In vivo evidence for post-adolescent brain maturation in frontal and striatal regions. Nature neuroscience. 1999; 2:859–861. [PubMed: 10491602] Tamez-Pena JG, Totterman S, Parker KJ. Mri isotropic resolution reconstruction from two orthogonal scans. Medical Imaging 2001, International Society for Optics and Photonics. 2001:87–97.Thompson PM, Giedd JN, Woods RP, MacDonald D, Evans AC, Toga AW. Growth patterns in the developing brain detected by using continuum mechanical tensor maps. Nature. 2000; 404:190–193. [PubMed: 10724172] Toga AW, Thompson PM. The role of image registration in brain mapping. Image and vision computing. 2001; 19:3–24. [PubMed: 19890483] Tu Z, Bai X. Auto-context and its application to high-level vision tasks and 3d brain image segmentation. Pattern Analysis and Machine Intelligence, IEEE Transactions on. 2010; 32:1744–1757.Tustison NJ, Avants BB, Cook PA, Zheng Y, Egan A, Yushkevich PA, Gee JC. N4itk: improved n3 bias correction. Medical Imaging, IEEE Transactions on. 2010; 29:1310–1320.Tzourio-Mazoyer N, Landeau B, Papathanassiou D, Crivello F, Etard O, Delcroix N, Mazoyer B, Joliot M. Automated anatomical labeling of activations in spm using a macroscopic anatomical parcellation of the mni mri single-subject brain. Neuroimage. 2002; 15:273–289. [PubMed: 11771995] Vercauteren T, Pennec X, Perchant A, Ayache N. Diffeomorphic demons: Efficient non-parametric image registration. NeuroImage. 2009; 45:S61–S72. [PubMed: 19041946] Wang Q, Kim M, Shi Y, Wu G, Shen D, Initiative ADN, et al. Predict brain mr image registration via sparse learning of appearance and transformation. Medical image analysis. 2015a; 20:61–75. [PubMed: 25476412] Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 17Wang Q, Lu L, Wu D, El-Zehiry N, Zheng Y, Shen D, Zhou S. Automatic segmentation of spinal canals in ct images via iterative topology refinement. Medical Imaging, IEEE Transactions on. 2015b; 34:1694–1704.Wang Q, Wu G, Yap PT, Shen D. Attribute vector guided groupwise registration. NeuroImage. 2010; 50:1485–1496. [PubMed: 20097291] Wang Z, Bovik AC, Sheikh HR, Simoncelli EP. Image quality assessment: from error visibility to structural similarity. Image Processing, IEEE Transactions on. 2004; 13:600–612.Wright J, Ma Y, Mairal J, Sapiro G, Huang TS, Yan S. Sparse representation for computer vision and pattern recognition. Proceedings of the IEEE. 2010; 98:1031–1044.Wu G, Jia H, Wang Q, Shen D. Sharpmean: groupwise registration guided by sharp mean image and tree-based registration. NeuroImage. 2011; 56:1968–1981. [PubMed: 21440646] Wu G, Kim M, Sanroma G, Wang Q, Munsell BC, Shen D, Initiative ADN, et al. Hierarchical multi-atlas label fusion with multi-scale feature representation and label-specific patch partition. NeuroImage. 2015; 106:34–46. [PubMed: 25463474] Yang J, Wright J, Huang TS, Ma Y. Image super-resolution via sparse representation. Image Processing, IEEE Transactions on. 2010; 19:2861–2873.Zhang, L., Wang, Q., Gao, Y., Wu, G., Shen, D. International Workshop on Machine Learning in Medical Imaging. Springer; 2014. Learning of atlas forest hierarchy for automatic labeling of mr brain images; p. 323-330.Zhang L, Wang Q, Gao Y, Wu G, Shen D. Automatic labeling of mr brain images by hierarchical learning of atlas forests. Medical physics. 2016a; 43:1175–1186. [PubMed: 26936703] Zhang L, Wang Q, Gao Y, Wu G, Shen D. Concatenated spatially-localized random forests for hippocampus labeling in adult and infant mr brain images. Neurocomputing. 2016bPattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 18Figure 1. The overall pipeline of the proposed framework consists of super-resolution (designated by red arrows) and groupwise registration (by blue arrows).Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 19Figure 2. The super-resolution consists of two stages. In the first stage of isotropic reconstruction (red to blue), an intra-slice 2D patch leads to the estimation of the 3D patch in the isotropic image space. In the second stage of regression-based enhancement (blue to green), the regressors enhances the 3D patch in the reconstructed image and predicts the corresponding patch of higher quality. Examples of the real data-flow are shown in the bottom. Note that the thickness of the initial subject images is high (i.e., ≥ 6mm), while the isotropic resolution is typically 1 × 1 × 1mm3.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 20Figure 3. An exemplar subject of the simulated data at different stages of the learning-based super-resolution.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 21Figure 4. The sensitivity study of super-resolution for images with different inferior-superior spacings.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 22Figure 5. The overall Dice ratios and the standard deviations of the groupwise registration upon the simulated data: red - for the down-sampled spacing of 2 × 2 × 2mm3; magenta - for the ground-truth spacing of 1 × 1 × 1mm3; blue - for the diagnostic spacing of 1 × 1 × 8mm3. The ROI indices are corresponding to their names in Table 1.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  Zhang et al.Page 23AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptFigure 6. The atlases fused from the ground-truth images and the simulated diagnostic images.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 24Figure 7. An exemplar subject of real data at different stages of the learning-based super-resolution.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 25Figure 8. The atlas fused from 49 diagnostic MR images with high and heterogeneous inter-slice thickness.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 26Figure 9. An exemplar elder subject (with atrophy caused by the Alzheimer’s disease) at different stages of the learning-based super-resolution.Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  Zhang et al.Page 27List of the ROIs in the NIREP NA0 Dataset.IndexROIIndexROITable 1135791113151719212325272931L Occipital LobeL Cingulate GyrusL Insula GyrusL Temporal GyrusL Superior Temporal GyrusL Infero Temporal RegionL Parahippocampal GyrusL Frontal LobeL Superior Frontal GyrusL Middle Frontal GyrusL Inferior GyrusL Orbital Frontal GyrusL Precentral GyrusL Superior Parietal LobuleL Inferior Parietal LobuleL Postcentral Gyrus2468101214161820222426283032R Occipital LobeR Cingulate GyrusR Insula GyrusR Temporal GyrusR Superior Temporal GyrusR Infero Temporal RegionR Parahippocampal GyrusR Frontal LobeR Superior Frontal GyrusR Middle Frontal GyrusR Inferior GyrusR Orbital Frontal GyrusR Precentral GyrusR Superior Parietal LobuleR Inferior Parietal LobuleR Postcentral GyrusPattern Recognit. Author manuscript; available in PMC 2018 March 01.AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscript  Zhang et al.Page 28SSIM study of the proposed super-resolution strategy in different stages.Table 2Subj.OriginalAft. Isotropic Recon.Aft. Regression1234567891011121314151676.20%75.14%77.77%74.58%75.06%80.44%73.90%77.27%74.78%78.36%76.63%74.24%78.38%75.20%76.49%75.88%82.69%82.52%83.27%82.08%83.30%83.11%82.83%82.92%82.63%83.68%83.86%81.95%84.38%84.10%83.64%83.18%84.60%85.06%85.77%84.20%85.25%86.17%84.28%85.21%84.60%86.07%86.46%83.97%86.81%85.43%85.62%85.05%Overall76.27 ± 1.80%83.16 ± 0.69%85.33 ± 0.84%Pattern Recognit. Author manuscript; available in PMC 2018 March 01.AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscript  