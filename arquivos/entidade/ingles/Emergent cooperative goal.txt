Artificial Intelligence 110 (1999) 1–55Emergent cooperative goal-satisfactionin large-scale automated-agent systems IOnn Shehory a;(cid:3), Sarit Kraus b;c, Osher Yadgar ba The Robotics Institute, Carnegie-Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, USAb Department of Mathematics and Computer Science, Bar Ilan University, Ramat Gan, 52900 Israelc Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USAReceived 12 October 1996; received in revised form 16 April 1998AbstractCooperation among autonomous agents has been discussed in the DAI community for severalyears. Papers about cooperation (Conte et al., 1991; Rosenschein, 1986), negotiation (Kraus andWilkenfeld, 1991), distributed planning (Conry et al., 1988), and coalition formation (Ketchpel,1994; Sandholm and Lesser, 1997), have provided a variety of approaches and several algorithmsand solutions to situations wherein cooperation is possible. However, the case of cooperation inlarge-scale multi-agent systems (MAS) has not been thoroughly examined. Therefore, in this paperwe present a framework for cooperative goal-satisfaction in large-scale environments focusing on alow-complexity physics-oriented approach. The multi-agent systems with which we deal are modeledby a physics-oriented model. According to the model, MAS inherit physical properties, and thereforethe evolution of the computational systems is similar to the evolution of physical systems. To enableimplementation of the model, we provide a detailed algorithm to be used by a single agent within thesystem. The model and the algorithm are appropriate for large-scale, dynamic, Distributed ProblemSolver systems, in which agents try to increase the benefits of the whole system. The complexityis very low, and in some specific cases it is proved to be optimal. The analysis and assessment ofthe algorithm are performed via the well-known behavior and properties of the modeling physicalsystem. (cid:211) 1999 Elsevier Science B.V. All rights reserved.Keywords: Multi-agent systems; Task allocationI This material is based upon work supported by NSF grant No. IRI-9423967, ARPA/Rome Labs contractF30602-93-C-0241 and the Army Research Lab contract No. DAAL0197K0135. Preliminary results of thisresearch appear in the proceedings of ECAI-96 and ATAL-98.(cid:3)Corresponding author. Email: onn_shehory@celis.cimds.ri.cmu.edu.0004-3702/99/$ – see front matter (cid:211)PII: S 0 0 0 4 - 3 7 0 2 ( 9 9 ) 0 0 0 1 8 - 11999 Elsevier Science B.V. All rights reserved.2O. Shehory et al. / Artificial Intelligence 110 (1999) 1–551. IntroductionMulti-agent systems (MAS) have been developed in recent years to address a varietyof computational problems of a highly distributed nature. Although a large body of thisresearch is theoretical, one may find an increasing number of simulated and implementedsystems of agents (e.g., [55]). Usually, either simulations or implementations consistof merely dozens of agents. However, during the course of their development, manyresearchers have realized that, in order to provide solutions to real-world problems, MASshould scale up. Such a scale-up must allow hundreds and thousands of agents to beinvolved in the execution of highly distributed, dynamically changing large numbers oftasks. Several attempts have been made to allow scalability of this type, and theoreticalmodels which have been developed may be applicable (e.g., [61]), however, this is yet tobe investigated.The problems arising in large-scale MAS are myriad, and due to their significanceshould be thoroughly studied. In this paper we cannot address all, hence we concentrateon investigating one facet of this diversity—the issue of task allocation and executionwithin large-scale cooperative MAS. 1 More specifically, we consider cases in whichcooperative autonomous agents allocate themselves to tasks. We provide a model thatallows for the dynamic agent-task allocation and is appropriate for large-scale MAS. Tosupport our theoretical claims, we provide a simulation of a dynamic agent system thatfollows our suggested mechanisms and consists of thousands of agents and tasks. To ourbest knowledge, up to date, this is the largest simulation of a task allocation and executionin a dynamic, open MAS. The model we propose provides a solution to problems whichwere not addressed previously in MAS, and may be the basis for future solutions for alarger class of problem domains.Regardless of their size, MAS are designed to satisfy goals, usually by executing tasksto which these goals may be decomposed. To allow for goal-satisfaction in dynamicsystems of multiple agents and goals and, correspondingly, multiple tasks, MAS shouldbe provided with a mechanism for task-agent allocation. Task-allocation methods in DAI(e.g., [14,51,54]) usually require coordination and communication [15,25]. In very largeagent-communities, direct rapid connection between all of the agents is usually prohibited,as such connection may clog the communication network. 2 Negotiation processes forestablishing cooperation are rather complex. Moreover, coordination-related computationswhich are based on on-line, bilateral communication among all of the agents may betoo complex as well. Therefore, complexities of cooperation methods in MAS becomeunbearable when the number of agents increases.To resolve this problem, we apply methods from classical mechanics to model large-scale MAS [50,52]. We adopt methods used by physicists to study interactions amongmultiple particles. The physics-oriented methods are used to construct a coordinated1 Cooperative MAS are frequently referred to as Distributed Problem Solvers (DPS) agent systems. In DPSagent systems as in cooperative MAS, agents attempt to increase the common outcome of the system. A varietyof algorithms for agent cooperation as a DPS system have been presented, e.g., in [2,13,37].2 For instance, assume that for each task each agent communicates with all other agents. If n is the numberof agents, even O.n/ communication operations per agent, which total to O.n2/ in the network are most likelyoverwhelming when the system consists of thousands of agents.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–553task-allocation algorithm for cooperative goal-satisfaction. This algorithm is to be usedby the single agent within the system, and enable coordination without negotiation andwith limited communication [17]. There are many differences between particles andcomputational systems. Nevertheless, we show that the physics-oriented approach enableslow-complexity coordinated task-allocation and execution in very large MAS.The physics methods, although they may be viewed as restricted to physical domains,allow for a model that is more expressive than other models, e.g., the tileworld model [41],as we discuss in Section 8. In addition, unlike most task-allocation methods for multipleagents, it consists of an inherent interleaving planning and execution. 3 And, as statedpreviously, while several models prove to work successfully for systems that consist of fewagents (usually less than 20), e.g., [16,17], their computational complexity would prohibitscaling up. Even when the model is based on market equilibrium, as in [39], simulationsare limited to less than 20 agents. In contrast, we present a theoretical justification to theability of our model to scale up. Moreover, we further support these claims by simulationsresults that consist of thousands of agents and tasks. These results show no increase incomputation and communication per task and per agent when the size of the system grows.Note that since we use a physics metaphor, our model can more easily be applied toproblems of physical nature (e.g., transportation, as we demonstrate later). We believethat applying our model to other problem domains is possible as well. We illustrate thispossibility by example (Section 5.3), however, do not prove it in this paper. One may beconcerned that using classical mechanics requires continuity of progression of the agentstowards goal-satisfaction. This may be simpler to model, however, continuity is not arequirement. Appropriate modeling of the goals in continuous or semi-continuous termswill suffice. We have performed such modeling in the simulations presented later in thispaper. Cases where continuity modelling is inadequate will be discussed in future work.A simple example where our model can be applied is a system in which agents have toblock holes of various sizes in a planar surface (which has similarities to the tileworld [41]).Each hole to be blocked is a goal, and the filling for blocking holes is the agents’ resource.The purpose of the system is to block as much hole-area as possible. Some holes cannot beblocked by a single agent and thus cooperation is necessary. 41.1. Definitions, assumptions and notationsWe describe the systems with which we deal as a set of agents N and a set of goals G, bothpossibly dynamically changing, located in a goal-space G. An m-dimensional displacementvector is a vector D D hd1; : : : ; dmi. The distance between D1; D2 is defined bysX(cid:0)(cid:1)2:d2i(cid:0) d 1ir 1;2 DiEach agent A 2 N and a each goal g 2 G have a displacement vector D which istheir location in the m-dimensional goal-space G. Since in some domains goals do not3 Note that the weak commitment algorithm [62] does allow for interleaving planning and execution, butrequires that inconsistent plans will be abandoned, then starting from scratch.4 More detailed examples will be presented in Section 5.4O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55have physical properties, the components of a displacement vector D are not necessarilyphysical distances. 5 We refer to such distances as virtual and denote as virtual the goal-space. We assume that the agents with which we deal have the ability to perceive thevirtual displacement in the goal-space, and can perceive the properties of other adjacentagents and goals. This may be done by sensors 6 integrated into the agents. We alsoassume that each agent knows about the types of resources that other agents may have,but may be uncertain of the particular resource-holdings of any other individual. Thesetwo assumptions are necessary since agents who progress within the goal-space need someinformation regarding properties of other agents and goals. We assume that each agent has aperformance capability that can be measured by standard measurement units, which enablequantification of the agents’ task execution. In addition, we assume that there is a scalingmethod which is used to represent the displacement of the agents in the goal-space and toevaluate the mutual distances between goals and agents within this space. This assumptionis necessary since virtual distances (or physical distances) are a significant factor in themodel we present. We assume that goal-satisfaction can be achieved progressively. That is,a goal may be partially satisfied at one instant, and its remaining non-satisfied part may becompleted at another point in time.1.2. Physics notations and backgroundTo present our model, we review several concepts and notations from physics. 7 We startby listing general mathematical notations which we use. A vector is a physical propertywhich has both a direction and a magnitude. Any vector x will be denoted by Ex. Physicalanalysis consists of derivation of functions with respect to time. The first-order time-derivative of x is denoted by Px and the second-order time-derivative is denoted by Rx. Thegradient operation is denoted by Er. This operator is a vector-derivative and (in Cartesiancoordinates) is given by(cid:18)Er D@@xbx;@@yby;@@z(cid:19)bz;(1)where Oj denotes a unit vector in the direction of coordinate j . We continue by listingphysical concepts. The displacement of a particle i is denoted by ri . Usually it is referredto as Eri , the vector of displacement, which is the .xi; yi; zi/ coordinates of the particle.vi denotes the velocity, which is the rate of change of displacement, and ai denotes theacceleration, which is the rate of change of velocity. The kinetic energy of a particle i isrepresented by ki , and the potential is represented by V . The potential is a spatial functionand therefore is sometimes called a field of potential or a potential-well (the latter refersto a specific shape of a potential function). Forces can be derived from the potential. Eachparticle i’s mass is denoted by mi , its displacement is denoted by the displacement vectorEri , its momentum is denoted by Epi and the force that acts on it is denoted by EFi .5 For instance, one can view the number of incorrect letters in a misspelled word as its distance from its correctform.6 The interpretation of sensors is extended in this paper to any information reception device.7 The notations and concepts we present here are described in many introductory physics books, e.g., [34].O. Shehory et al. / Artificial Intelligence 110 (1999) 1–555Classical mechanics provides a formal method for calculating the evolution of thedisplacement and the momentum of particles. Given the initial displacement Eri.0/ andmomentum Epi .0/ of a particle i, its displacement Eri .t/ and momentum Epi.t/, at anyother time t, can be derived via the solution of the equations of motion. These equationsare first- and second-order differential equations. The boundary conditions (that is, thearbitrary constants) of the exact solutions of the equations are the initial displacement andmomentum of the particle. For a particle i, the equations of motion are:EFi D miEpi D miREr i D mi Eai;PEr i D mi Evi :(2)(3)The nature of the motion of a particle depends on the field of potential in which it moves.This dependency is given by:EFi D (cid:0)miErri V(cid:0)(cid:1)Er:(4)For some types of potential V , the solutions of the equations, either exact or approximated,are well known or can easily be derived. In our model we employ only such types ofpotential functions. By relying on the known solutions from physics we may predict thebehavior of the agents who follow our model. Simulations (see Section 7) are performedto further support the validity of the model.1.3. Adapting physics to DAIAs stated previously, we consider large sets of agents and goals. Each agent has a goal-satisfaction capability and should advance toward satisfying goals. We use a physics-oriented model that consists of particles to represent agents and goals and to developa distributed cooperative goal-satisfaction mechanism. The first step in applying thephysics model to DAI is the match between particles and their properties, agents andtheir capabilities, and goals and their properties (see Fig. 1). The next step is to identifythe most appropriate state of matter for modeling an ensemble of agents and goals. Themathematical formulation that is used by physicists, either to describe or to predict theproperties and evolution of particles in these states of matter, will serve as the basis for thedevelopment of algorithms for the agents’ behavior. However, several modifications of thephysics-oriented model are necessary to provide an appropriate algorithm for automatedagents. In the rest of this paper we shall elaborate on these issues.The general idea of our model is that entities of the MAS are modeled by particles.Agents and goals are modeled by dynamic particles and static particles, respectively. Theproperties of a particle i, i.e., its mass mi , its displacement and momentum vectors Eriand Epi , its potential Vi and its kinetic energy ki are abstractions of the properties of themodeled agents and goals as described in Fig. 2. The agent’s goal-satisfaction capabilityis represented by the mass (and the potential energy) of its modeling particle. The mass ofa static particle represents the size of the goal it models. The displacement of a particle inthe physical space models the displacement of the agent in the goal-space.We model goal-satisfaction by a collision of dynamic particles (that model agents)with static particles (that model goals). However, the properties of particle collisions aredifferent from the properties of goal-satisfaction and several adjustments are needed in6O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55DAIPhysicstheenvironments whereIdentifyingphysics-oriented models are appropriate;matching between particle properties andagents/goals propertiesSelecting the states of matter that canbe used for modeling automated agent-systemsDeveloping goal-satisfaction algorithms;adjusting the agent system to the physicssystem for the validity of the algorithmAnalysis of the complexity and propertiesof the algorithmsLocating particle models and theirpropertiesIdentifying states of matter and theparticle properties withinUsing mathematical formulation topredict and describe the propertiesand evolution of the selected stateof matterTheoretical and simulation-basedanalysis of physical particle sys-tems’ behaviorFig. 1. Steps in applying a physics-oriented model to a Distributed AI, DPS problem domain.Automated agentsPhysical modelCommunity of agents satisfying goalsNon-ionic liquid systemAgentGoalAgent’s capabilityDynamic particleStatic particleParticle’s massAgent’s location in agent-goal spaceLocation of particleGoal satisfactionAlgorithm for goals allocationStatic-dynamic collisionFormal method for calculatingthe evolution of displacementFig. 2. The match between the physics model components and the large-scale automated agents’ environments.order to provide the agents with efficient algorithms. These modifications are described indetail in this paper.2. The physics-agent-system (PAS) modelThe model we present entails treating agents, goals and obstacles as if they wereparticles. That is, each agent will have an initial state and its equations of motion. Notethat an agent’s equations of motion do not necessarily entail real physical motion; theymay represent the progress towards the fulfillment of goals. The potential field in whichan agent acts represents the goals, the obstacles and the other agents in the environment.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–557Subject to the potential field, an agent will solve the equations of motion and, according tothe results, progress towards the solution of goals and either cooperate or avoid conflictswith other agents. Note that cooperation and conflict-avoidance are emergent properties ofour physics-oriented model.2.1. State of matter for PASIn order to construct an appropriate potential field to representthe multi-agentenvironment, we shall examine the properties of physical states and locate the mostappropriate one. An appropriate physical state must consist of a potential that, whenlead the agents to beneficial goal allocation andadapted to the agent-model, willsatisfaction. In the solid state particles are localized, i.e., they are bound strongly to theirinitial position; this prevents evolution of the system. Thus, when applied to MAS, dynamicgoal satisfaction by the agents is prevented. In the gas state, interactions between particlesare very weak. In such a case, the system can evolve, but the lack of intensive interactionmeans that cooperation and conflicts will rarely occur. This may be an interesting issue forfuture research, but presently we are interested in cases where there are both cooperationand conflict among the agents. The liquid state lies between these two states. As opposedto the solid state, a liquid evolves more rapidly. However, unlike the gas state, a liquidis dense enough to cause interaction among its particles. 8 Therefore, the liquid model ispreferred as a more appropriate model for the MAS under consideration.Among the liquids, there are two main types: ionic and non-ionic liquids. Ionic liquidsare such that the mutual potential among each pair of particles is a Culombic potential. 9This potential is proportional to 1=r, where r is the distance between the particles. Such apotential diminishes slowly with respect to r, and therefore entails a long-range interactionamong the particles. This type of interaction means that all of the particles in the systemshould be considered when calculating the interactions and the evolution of the state ofeach single particle in the system. Typically used to describe the potential of a particle i ina non-ionic liquid, corresponding to its distance rij from particle j is the Lennard–Jonespotential:V .Er/LJijD 4"(cid:18)(cid:19)1r 12ij(cid:0) 1r 6ij;(5)where " is a mass-dependent coefficient that scales the potential. Because of the shape ofthe curve of function (5) (see Fig. 3), the potential is sometimes called a “potential-well”.Such a potential function entails a potential that vanishes after a short distance r, due tothe high powers of r that are present. The short distance here implies that the interactionsbetween the particles in the system are limited to short distances. That is, a particle interactsonly with particles in a limited neighbourhood, and only these are considered for thecalculations of the evolution in the state of each particle. The properties of the non-ionicliquid, and in particular the short-range potential, make it appropriate for use as a model for8 Note that interactions occur among gas particles too, however, the rate of interactions is extremely smallerthan this rate in liquids.9 Culombic potential is the potential that results from electric charges.8O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Fig. 3. A typical one-dimensional Lennard–Jones potential-well.the large-scale MAS with which we deal. In such systems, communication with all of theagents 10 and computations of all possible interactions may be too complex. Note that thisimplies that the information accessible to an agent regarding its environment is incomplete.2.2. Matching physical properties to agents and goalsPhysical particles may have different masses. As a result of the different masses,particles subject to the same field of potential will have different momentum and kineticenergy. This is because the momentum is expressed by Ep D mEv and the kinetic energyis expressed by k D mv2=2, and both are products of the mass. In the PAS model, theagent’s capability to satisfy goals is represented by the potential energy of the particle thatmodels it. Particles with greater potential energy model agents that can satisfy larger ormore difficult goals and sub-goals. This means that a greater mass of a dynamic particle(that models an agent), other properties remaining constant (and thus causing a greaterpotential energy), entails a larger capability of goal-satisfaction by the agent. The mass ofa fixed particle (which models a goal or an obstacle) represents the size of the goal or theobstacle. This means that in order to satisfy a greater goal, which is modeled by a staticparticle with a greater mass, additional efforts are necessary on the part of the agents.The displacement vector of a particle Eri models the displacement of the agent in thegoal-space. This space can be either physical or abstract, since goals are not necessarily(and are usually not) physical.Example 2.1. An example of an abstract goal-space is the space of queries in a multi-database domain. In such a domain, each database is represented by an agent, and thegoals that the agents must fulfill are the answering of queries that were directed to theirdatabases. The displacement of an agent in the query-space represents the logical proximityof the information stored in its database, either to the information stored in other databases10 The communication required for each agent is O.#agents (cid:2) #tasks/, which is not considered large. However,in large systems this will most probably be overwhelming (e.g., 1000 agents and 1000 tasks, as we have in oursimulation).O. Shehory et al. / Artificial Intelligence 110 (1999) 1–559or to the information necessary for answering a query. Virtual distances in the multi-database domain can be calculated if a pre-defined logical-proximity of a key-words scaleis given. 11 Note that modeling such a space may be rather difficult and require manyadjustments.According to the virtual displacement of an agent, one can calculate its distances fromother agents, goals and obstacles. These distances are then used to calculate the potential.The momentum vector of particle i, Epi , represents its physical velocity and is used forthe calculation of the kinetic energy. In the PAS model, the velocity of a dynamic particle(which models an agent) represents the rate of movement towards the satisfaction of a goalor part of a goal.Example 2.2. In the example in Section 1, a system wherein agents have to block holesin a planar surface is presented. The purpose of the system is to block as much hole-areaas possible. Some holes cannot be blocked by a single agent. According to PAS, each holein this system is modeled by a particle, which is represented by a potential-well, and thesize of the hole is represented by its mass. A greater mass entails a greater hole, and due tothe physical nature of the potential-well, particles surrounding a well with a greater masswill experience stronger attraction to the well. This property of the wells is appropriate forour purposes, because it causes a natural attraction to the holes which is proportional to thesize of the holes. The agents in this system are also modeled by particles, represented bypotential-wells. However, as opposed to the holes which are fixed in their displacements,the agents’ potential-wells are free to move. The agents have masses of various magnitudeswhich represent differences in their abilities to block holes.2.3. Virtual motion towards goal-satisfactionIn the physical world, the motion of particles is caused by the mutual attraction (andrejection) between them. In the agents’ system, the agents calculate the attraction andmove according to the results of these calculations. Since, in the physical world, motionis continuous, the agents’ calculations, which result from our PAS model, must resemblethis continuity. This can be done by performing the calculations repeatedly with a highfrequency.According to the model,the agents shall calculate the potential, subjectto thesurrounding goals and agents. This potential is affected by the virtual distance fromthese neighboring entities. Due to the large size of the systems under consideration,each modeling particle has only a limited effective interaction with the surroundingparticles. Consequently, the modeled agents have limited information about the goal-domain. A particle can only have local reactions to the potential field and, in practice,only its near neighbors will affect its potential. The range of interaction among modelingparticles has a significant effect on the complexity of the calculations that the modeledagents perform. 12 We denote the radius of interaction among the particles by rI . This11 Such proximity scales can be found in various information retrieval systems, e.g., in [46].12 The complexity is analyzed in detail in Section 4.10O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55radius may sometimes be determined by sensors integrated into the agents, or shall bedetermined by the designers of the agents. Assuming a random distribution of the particleswithin the range of the whole system (however, not necessarily uniform), the computationalcomplexity that the modeled agents experience grows linearly with respect to the size of thearea included within the range of interaction. That is, it increases linearly with respect tor 2I , which is disadvantageous. The increment in the complexity, when rI is increased resultsfrom the corresponding increment in the number of agents and goals within the interactionrange. The advantage of the increment of rI is that within the range of rI , there will be moreinteracting entities, which may increase the goal-satisfaction and its efficiency. However,due to the sharp reduction in the magnitude of the potential function beyond a short distancefrom the center of the potential-well, the magnitude of the derived forces is small and theinteraction is negligible and diminishes. Simulations that were performed by physicistshave shown that when these long-distance interactions are neglected, the results of thesimulations still agree with theoretical statistical-mechanics and thermodynamics [42,57].Therefore, it is common to cut off the range of interaction by cutting off the potentialfunction after it diminishes to 1–10% of its maximal value. Our model follows this cut-offstrategy. To illustrate this cutoff within the physical system, we bring forth the followingexample:Example 2.3. Suppose that the potential function of a particle is formulated by V D4".1=r 6 (cid:0)1=r 12/ (a Lennard–Jones potential). The maximal absolute value of this functioncan be calculated by setting its derivative to zero. The result of this is the distance whereinthe potential function is maximal, that is, rmax D 21=6 D 1:112. Substituting rmax into thepotential function, we derive the maximal absolute value of the potential, which is V D ".Simple calculations show that the reduction of V to 10% of the maximal value will causea cut-off distance rcut-off D 1:842 and reduction to 1% will cause rcut-off D 2:714. Bothcut-off distances are not significantly far from the particle under investigation. Such cut-offdistances are adopted for our MAS.The reaction of a particle to the field of potential will yield a change in its coordinatesand energies. The change in the state of the particle is a result of the influence of thepotential (since EFi D (cid:0)m ErV , i.e., the acting force, is derived from the potential). In ourmodel, each agent will calculate the effect of the potential field upon itself by solving a setof differential equations. According to the results of these calculations, it will move to anew state in the goal-domain, as we describe in detail in the formal protocol in Section 2.7.Real motion is not necessary here. In a case of an abstract space, moving to a new statemeans updating the state parameters. For instance, if a state is represented by a vector ofBoolean values (as we illustrate in Section 5.3), such a state change means a change insome of the Boolean values.The equations that an agent must solve during its virtual motion towards goals are theequations of motion of a particle subject to a potential field. Solving these equations maybe complicated, but an approximation by numerical integration (e.g., leap-frog [4]) andVerlet tables (as in [22]) can simplify these calculations while providing results that are, forpractical purposes, of the same quality as the accurate results. This numerical integration,which is done with respect to time, must be iterated frequently (as explained previously inO. Shehory et al. / Artificial Intelligence 110 (1999) 1–5511this section) and performed with small time-steps dt which will be used as the differentialfor the numerical integration.2.4. Setting the size of dtThe size of the time differential dt depends on the properties of the system with whichwe deal. This dependency is due to the effect that dt has on the number of iterations thatmust be performed until an agent reaches a goal. A small dt results in a more accurate nu-merical integration, but the change in the displacement of the modeling particle (and henceof the agent that it models) at each iteration will be very small. This implies that a reductionin the size of dt increases the number of iterations necessary for an agent to reach a goal.This increases the overall time of the goal-satisfaction procedure. A large dt reduces theaccuracy of the numerical integration, but reduces the number of iterations necessary forreaching a goal as well. However, large dt has some deficiencies: as dt grows larger, theprogress towards a goal at each time-step becomes greater. This leads to situations where asingle time-step dt may lead to a large single displacement-translation. Such a translationmay move an agent far from the goal towards which it was moving. Moreover, such a be-havior contradicts the continuous physical properties that are necessary for the success ofour model.From the deficiencies of either large dt or small dt we conclude that dt should liesomewhere in between. The time-step dt shall be chosen subject to the properties of aspecific system. We will use r0 as our unit of measure. Relying on the experience gatheredin physics simulations [42], a typical particle in the model will pass a distance of r0 inabout ten time-steps dt. This requirement implies that the average velocity v of a particle(at its initial displacement) directly affects dt by the relation dt D r0=v. Therefore, theinitial average distance between agents and goals will enable the preliminary setting of dt,as we prove later by simulation results.In the physics model, dt serves mainly as the differential for the numerical integrationand represents real movement-time of the particles. In the PAS model, dt serves as a timeunit where, in each time step dt, agents calculate their parameters and progress accordingto these calculations. dt in the goal-agent system is different from dt of the correspondingparticle system, but both are related to one another by a 1–1 onto function, which isdetermined by the properties of the corresponding models. To determine dt in the goal-agent system, the computation time and the goal-satisfaction time should be considered.dt, which is originally calculated according to the physical properties of the system, willbe dt > max.computation-time; movement-time/ in cases where agents can satisfy goalsand perform calculations in parallel, and dt > computation-time C movement-time in caseswhere agents perform goals and calculations separately.2.5. Collision and goal-satisfactionThe dynamics of the physical system which models the computational system leads tocollisions between particles. In a system that consists of both static and dynamic particles,two types of collisions are possible. One type is a collision between two dynamic particles,12O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55which we denote by DDC. The other type of collision is between dynamic and staticparticles, and we denote it by SDC.2.5.1. Dynamic-dynamic collision (DDC)In our model, the DDC represents the interaction between two agents. 13 We wouldlike the agent-agent interaction to prevent situations in which agents collide, either inthe physical sense of the word (when relevant) or in its abstract sense. 14 This can beachieved by a mutual repulsion among the particles that model the agents. As a result ofthe repulsion, agents do not have to negotiate over goals, since the decision on which agentsshall perform a specific goal will emerge from the repulsion. 15 This repulsion model isespecially necessary in the case of two modeled agents that have a large goal-satisfactioncapability. The reason for this necessity is that, in cases where two such agents or morereach a goal, we would prefer the goal-satisfaction to be performed by only one of them. Insuch cases, one agent can usually perform the goal by itself, and goal-satisfaction by morethan one agent will be a waste of efforts. However, a DDC between particles that modelagents with a small 16 goal-satisfaction capability shall cause a negligibly small repulsion.This is because we would like such agents to cooperate on the goal-satisfaction.To satisfy the repulsion requirements, dynamic particles that model agents shall havea potential that consists of a dominant repulsive component. The potential, including itsrepulsive component, must be proportional to the mass of the particle, since the mass ofthe particle models the goal-satisfaction capability of the agent. Thus, a greater mass of themodeling particle models a greater goal-satisfaction capability of the modeled agent and, asrequired, it also leads to a stronger repulsion. In order to satisfy the requirement of strongrepulsion, the Lennard–Jones potential of a dynamic particle that models an agent shallbe modified from the classical LJ potential. 17 Such a modification will be included in ourmodel and will be done by multiplying the repulsive component of the potential function bya pre-defined factor PDF. As a result of this multiplication, the magnitude of the repulsivecomponent will increase as required. For practical use, we should set the magnitude of thePDF so that it will change the repulsive component by an order of magnitude with respectto the attractive component of the potential. The modification of the LJ potential affectsthe interaction between dynamic and static particles. In order to maintain the magnitude of13 This does not prohibit using a similar model where agent-task interactions are modeled by dynamic-dynamiccollisions as well. We do not pursue this direction here.14 Agents may collide when they attempt to consume the same resources or to perform the same goal, thusinterfering with one another and possibly reducing effectiveness or even prohibiting task execution. For instance,if the agents are robots, then a collision may damage the robots, and therefore should be prevented. In moreabstract cases overlapping locations of agents in the model may be allowed.15 Note that this takes into account cases where agents do not perform goals equally well, as also shown inSection 7.16 When we speak of small and large goal-satisfaction capabilities, we do so with respect to the size of the goalstowards which the agents are moving.17 Note that in molecular dynamics (MD) research, even more significant modifications are applied (e.g.,completely omitting parts of the potential function), yet such modifications do not prohibit a good approximationto the well-known physical behavior. As in MD, the simulations that we have performed support the validity ofour model.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5513attraction in this case, the attractive component of the LJ potential of a static particle mustbe multiplied by the same PDF.2.5.2. Static-dynamic collision (SDC)The SDC represents interaction between an agent and a goal. In such interactions wewould like the static particle that models the goal to attract the dynamic particle that modelsthe agent, in order to lead to goal-satisfaction. The physical motion of dynamic particlestowards static particles is continuous, and the attraction into the potential-well of a staticparticle is a gradual yet continuous process. Therefore, there is no specific point from whichthe particle starts the collision. However, our model requires such a point to let the agentsdecide upon the appropriate actions when reaching a goal, in order to satisfy it. Hence,we must decide artificially upon such a point. Adopting physical concepts, we may use thenotion of typical radius for this purpose. A typical radius of a particle is usually (especiallyin MD) taken to be the distance from its center to the point wherein the force derived fromthe potential is equal to zero. That is, EF D (cid:0)m ErV D 0. We denote this typical radius by (cid:27) ,pand a simple calculation—via the derivation of the potential function—yields (cid:27) D 4" 62.An SDC occurs when a dynamic particle is in the vicinity of a static particle. Vicinityhere means that the distance between them is a few typical radii. Therefore, we arbitrarilydecide upon a distance of 3(cid:27) as the point from which the collision starts 18 and denote itby r0. In two and three dimensions, this point becomes a circle and a sphere of radius r0,respectively. In abstract and multi-dimensional spaces, the interpretation of r0 is of logicaldistance. For instance, in the case of information which is classified by keywords, havingreached r0 means that a database agent is, keyword-wise, very close to the informationnecessary for answering a query, where multi-dimensionality may refer to multiple topicswhich are relevant to this query.Example 2.4. In the hole-blocking system, the holes are physical entities with a definitesize and with accurate boundaries. The model of such a hole must consist of a specificpoint from which the modeled hole begins. We use r0 (as described above) to model thehole boundary. In the hole system, the interaction between a hole and an agent starts whenthe agent physically arrives at the hole. In our model, this will be modeled by the arrivalof a dynamic particle that models the agent to a distance r0 from the center of the staticparticle that models the hole.When a dynamic particle reaches a static particle, i.e., it reaches the distance of r0 fromits center, a collision occurs. Such a collision can have several results, the two extremes ofwhich are the cases of completely-elastic and completely-inelastic collisions. The first typeof collision entails the conservation of all of the kinetic energy of the moving particle, andin our model will represent cases where the resources of the agent are non-expendable.The second type entails the loss of all of the kinetic energy of the dynamic particle,and correspondingly, will model the cases of expendable resources. Between these twoextremes, a variety of combinations may be found and can be adjusted to various cases in18 This is a common choice in MD as well.14O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55the agents’ system. We shall discuss only the two extremes, since their combination is asimple (but time consuming) process.2.5.3. The behavior of agents during an SDCAn important issue in our model is the behavior of the particles during the collisionand the corresponding behavior of the agents. An agent that reaches a goal may eithercompletely or partially satisfy it. In both cases, the model requires a reduction in themagnitude of the goal. Since the size of the goal is modeled by the mass of its modelingparticle, the mass of this particle shall be reduced. It may also require a reduction in themass of the particle that models the agent in the case of depleting resources. 19 However,the reduction of mass is not a physical property of such a collision. Therefore, introducingsuch a reduction into the PAS model may deteriorate its implicit advantages. That is, theexpected physical evolution of the system may lose its validity. In order to avoid this loss,some modification of the model shall be done. We will allow some non-physical parts intothe model, as long as they do not affect the general evolution of the system. This is possibleif the model will consist of a scheme for a temporal partition of the evolution of the system.This means that the evolution of the system will be partitioned into several time segments,and in each temporal segment the physical evolution of the system will not depend on theother segments. 20 After the partitioning into time segments, each time segment can betreated as a separate system. The connection between the time-segmented systems will beestablished via their initial and final states. The initial states of the particles in a new systemare the modified final states of the previous system. A detailed method for performing themass-reduction is presented in Section 3.2.5.4. Time consumption and hindrancesThe nature of the collision in the physics model has several implications for the modeledsystem. Since the modeled goal-satisfaction process requires time, the modeling collisionmust also be time-consuming. Fortunately, physical collisions are not instantaneous.Therefore, our model should only adjust the physical collision time to the goal-satisfactiontime. However, we would like this adjustment to be an implicit property of the physicalbehavior. This is possible if the potential-well which models the goal will cause somekind of hindrance to the particle that models the agent, when the particle has entered theregion of collision r0. Such a hindrance will emerge if the potential well consists of acentral repulsive part. The central repulsive part of the potential-well will cause a gradualrelaxation of the particle that will have reached the well, until the particle stops. Time isrequired for the relaxation process, and this time will model the goal-satisfaction time.Another issue concerning the SDC is the relation between the mass of the particlesand both the goal-size and the agent’s ability to satisfy goals. As previously stated, agreater mass models a greater goal in the case of a fixed potential-well and a greater goal-satisfaction capability in the case of a dynamic particle. The kinetic energy of a dynamic19 Such reductions are required to conserve correct relations between agents and goals in the system. Forinstance, an agent that have used most of its resources should not be attracted to a goal as strongly as a fullyreplenished agent.20 Note that the time segment here is different from dt that is used for calculating the change in the coordinatesof a particle. A typical time segment will be much longer than dt.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5515particle grows as it gets closer to the potential-well by which it is attracted. Consequently,its velocity grows, and its final magnitude depends on the size of the attracting potential-well and its initial distance from this well. As a result of this dependency, two dynamicparticles with the same masses may reach a distance equal to r0 from the center of the samepotential-well with different velocities due to their different initial distances from the well.Therefore, these two particles will have different collision time-periods. If the collisiontime models the goal-satisfaction time, this physical property should be disadvantageousfor our model. This is because it means that among two modeled agents with the samegoal-satisfaction abilities, the one that was initially “farther away” will perform the goalfaster. For clarity of representation, we assume that agents with the same abilities performa goal in the same amount of time. We thus assume goal-satisfaction time to be longerthan collision time. Nevertheless this assumption can be relaxed via a small (though notobvious) modification to the algorithm. This modification is performed by causing anothertype of hindrance; whenever a dynamic particle reaches a static particle, it must completethe collision within a time-period that is equal to the time that it would have taken fora dynamic particle that started moving towards the static particle from a distance of r0(we assume that none of the dynamic particles is initially inside the range r < r0) and hasthe same mass as the colliding particles have. The legitimacy of such a modification to thenature of the physical system can be explained similar to that in the case of mass reduction.Note that this option is not part of the algorithm we present in Section 2.7 (however, maybe added to it). We avoid this addition to keep the algorithm simple.2.6. The potential-wellsAs presented above, we model agents and goals using particles. However, it is commonamong physicists to represent particles by the graphic shape of their potential function. Ascan be seen in Fig. 3, these potential functions have some minimum point, and thereforeare called potential-wells. The reason for using the potential-well notion in addition to theparticle notion is because of the graphic illustration. This illustration may give a betteridea about the attraction and rejection that the potential functions of particles impose.The potential-wells consist of domains of attraction and rejection between particles. Theattraction and the rejection are formally represented by the force, which is derived fromthe potential function. A positive force represents attraction and a negative force representsrejection. The lowest point on the graph is the point where the force nullifies. The forcegrows negative when moving towards the center of the potential-well (i.e., the centralpart causes rejection) and positive when moving away from the center. However, as thepotential curve becomes asymptotic to a horizontal line, the magnitude of the attractiveforce diminishes and finally vanishes. In order to illustrate the rejection and attractionwithin a potential well, the reader may think of a marble that is put in a physical potential-well. It is clear what parts of the well will move the marble towards the center and whatparts will reject from the center. It is also clear where, along the curve, the forces will bestrong and where they will be weak.In the agents’ system, attraction in the case of DDC models cases in which cooperationamong the agents is beneficial, and rejection in these cases models occasions wherecooperation is non-beneficial. It is beneficial for two (or more) agents to jointly perform a16O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55task when each does not have all of the required capability. In such cases the attractionshould (and according to our model would) be the more dominant component of thepotential function. Cooperation would not be beneficial in other cases, hence rejection isapplied. Note that this distinction between beneficial and non-beneficial cooperation doesnot require different types of potential functions, however different scaling of the termsof these functions may be required. In cases of SDC, attraction models the goal-reachingand goal-satisfaction processes, and rejection models the time consumption during the goalsatisfaction, as described in Section 2.5.4.Looking once again at Fig. 3, we can observe that typical potential-wells (the figurerepresents such potential-wells) have a limited range of strong attraction and rejection. Theother parts of the potential may be attractive but the attraction is rather small. This attractionmodels regions wherein goal-satisfaction is non-beneficial or brings about very smallbenefits. The attractive part of the potential is a long-range potential. As can be observed,the magnitude of this potential diminishes after a relatively short distance from the origin.Practically, as simulations that were performed by physicists have proven [32,57], thelong-range interaction can be cut off after a reasonable distance (as we discussed inSection 2.5.2). Such a cutoff will have only a minor effect on the dynamics of the systemof particles.We describe the likelihood of cooperation by potential functions of types that would fitthe properties of the agents. If the benefits of cooperation are functions of more than onevariable, then the potential-wells will be multi-dimensional in the space of the variables(usually these variables are the resources of the agents). For simplicity of representationand calculation, we use one-dimensional continuous functions to represent potential-wells.This will limit us to the case of agents that use only one resource. 21 However, there aremethods for the expansion of the one-dimensional case to the multi-dimensional casein physics, and these methods can be adopted when our approach is used to analyzesystems of computational agents. These methods are appropriate only when the resourcesare independent. This is because the physics-based methods for the analysis of multi-dimensional functions require such independence. Having n resources Rn D fR1; : : : ; Rng,we require that 8i 2 1; : : : ; n, Ri is not a linear combination of S (cid:18) R n Ri , S 6D ;.2.7. A protocol for the single agentHaving described the physical properties of the modeling particles, we may proceedto the protocol according to which the modeled agents shall act. As we have previouslyproposed, each agent and each goal are modeled by a potential-well. Goals are modeled bywells which have a fixed displacement and agents are modeled by dynamic wells. In orderto cause evolution of the system towards goal-satisfaction, each agent uses the informationthat it can gather by observation (e.g., via sensors) about its neighboring agents and goalsand regarding its previous state. According to this information, the agent will construct thelocal field of potential and solve the equations of motion. The results of the equations ofmotion will enable the agent to decide what its next step towards goal-satisfaction will be.The exact detailed algorithm for the single agent i is as follows:21 We discuss the issue of multiple resources and capabilities in Sections 6.1 and 5.3.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5517Loop and perform the goal-reaching and goal-satisfaction processes until one of thefollowing conditions is satisfied:– The resources necessary for the completion of the goal-satisfaction have beendepleted, or,– No more goals within the interaction range rI have been observed for several time-segments. 22The goal-reaching process(i) Advance the time counter t by dt.(ii) Locate all of the agents and goals within the range rI , the predefined interactiondistance. Denote the distance to any neighbouring entity j by rij .(iii) Calculate the mutual Lennard–Jones potential (as described in Eq. (5)) withrespect to each of the agents and goals within the range.(iv) Sum over all of the pairwise potentials VLJ.rij / and calculate the gradient of thesum to derive the force Fi as described in Section 7, in Eq. (4).(v) Using Fi and the previous state Eri .t (cid:0)dt/; Epi .t (cid:0)dt/, solve the equations of motionas described in Section 7, in Eqs. (2) and (3).(vi) The results of the equations of motion will be a new pair Eri.t/; Epi .t/. Move 23 tothe new state that corresponds to the displacement Eri.t/.(vii) At each time-step, after moving to a new state, calculate the new kinetic energyK (see Section 2.2) and the new potential (see Eq. (5)) according to the newcoordinates Eri .t/; Epi .t/.(viii) If, due to the shift to the new displacement, your distance from the center of aparticle that models a goal is greater than r0, return to step (i). Otherwise, you haveentered the region of strong interaction, i.e., you have reached the goal. Therefore,start the goal-satisfaction process.The goal-satisfaction processAfter reaching a goal, the agent must satisfy all or at least parts of it. In order to do soaccording to our model, the following algorithm should be used by the agent. 24(i) Move into the potential-well that models the goal according to the physicalproperties of the entities involved in the process, as described in the goal-reachingprocess.(ii) Perform the goal.(iii) If ma, the mass of the particle that models the agent, is smaller than mg, the massof the particle that models the goal, subtract ma from mg. Else, mg D 0. In a caseof depleting resources, ma is reduced in a similar way.(iv) Return to the goal-reaching process, step (i).22 This requirement allows for a high probability of all goals being satisfied, however, does not guarantee 100%performance. Nevertheless, greater numbers of agents and tasks in the system bring this probability very closeto 1.23 We allow agents to move and perform calculations in parallel, if they are capable of doing so.24 Note that part of the following algorithm is aimed at adjusting the agent’s behavior to the physical model anddoes not directly contribute to goal-satisfaction.18O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Note that the calculations in the algorithm above shall be performed using the physicalproperties that result from the physics model. Therefore, properties of the agents and goalsshall be transformed to physical properties, then used for physics calculations, and then re-transformed into agent-goal properties. An example of such a transformation is presentedin Section 5. The iterative method which we propose leads to a gradual reduction in thenumber and size of the goals to be satisfied, and will lead finally, to completion of the goals.However, the time that such a process consumes may be excessive. We discuss this problemin the next section. In addition, the convergence of the system to a final state wherein eitherall of the goals have been satisfied, or it is not beneficial to satisfy the unsatisfied goals isproven below.3. Scaling and convergenceDue to the evolution of the system, goals will be satisfied gradually. As suggested inSection 2.5.3 and described in Section 2.7, after each static-dynamic collision, the massof the static particle that models the goal diminishes. This implies that the potential-wells which represent the goals will become less attractive, and therefore the rate of goal-satisfaction will decrease. The overall result of such a phenomenon is a gradual decay ofthe goal-satisfaction process. This means that in a system of the type which we propose, asthe goals get closer to full satisfaction, the time for satisfying the rest of the goals decreaseslogarithmically. 25 That is, the time for completion of all of the goals is diminishing, butthe rate of decreasing gradually becomes slower.In order to solve this problem, we employ a scaling method. The purpose of this methodis to amplify the attraction in the system in order to accelerate the goal-satisfaction process,especially when it slows down due to its physical properties. Since the mass of the particlesand the potential-wells directly affect the magnitude of the attraction among them, weshall scale the masses of the entities in the system. However, any change in the physicalproperties in the system, including masses, may change the physical behavior. Therefore,the well-known results of similar physical systems do not necessarily hold, and we may beunable to use them to predict the evolution of goal-satisfaction.However, we shall use the mass-scaling method in instances wherein it preserves theconsistency with the physical results. Actually, the distribution and the size of the systemdisable information about the temporal change in the quantity of goals and sub-goals thathave yet to be satisfied. Therefore, there must either be an on-line synchronization method,or mass reduction will have to be performed according to the initial information that eachagent has about the system and some pre-defined rules. For reasons of simplicity andlow computational complexity, we shall prefer using some pre-defined rules. The decisionregarding these rules is subject to the properties of the system, but can be calculated priorto its implementation. To clarify when mass-scaling instances occur, we shall first presentthe scaling method:25 Such a logarithmic decay is well known in the relaxation of multi-particle physical systems.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5519– During the goal-satisfaction process, after each pre-defined number of iterations (wedenote this number by I ), perform the following:– Multiply the mass of all of the goals by C, a pre-defined factor. The result of thismultiplication will cause the treatment of each mass of a particle that models a goalas mi C(cid:28) , where mi is the mass of goal i without scaling and (cid:28) is the number of timesthat the mass-scaling has been performed. 26Now that the scaling method has been presented, we shall show its validity. Each I stepsof the goal-satisfaction process that are performed without interruptions are similar to aphysical process. The problem arises from the change that may occur after each I stepsdue to the mass-scaling; such an event is not analogous to physical behavior. However, wecan view the evolving system S as a set of systems fSTt0;t1U; : : : ; STtk(cid:0)1;tkUg,[S DiD1;:::;kSTti(cid:0)1;ti Uof I evolutionary steps each. The state of S at time ti , immediately before the ith mass-scaling, is the final state of STtk(cid:0)1;tkU, and immediately after the mass-scaling it is the initialstate of STtk;tkC1U. Since we are interested now only in the sub-systems (which are similar tophysical systems) the gap between them (which is not similar to physical phenomena) canbe ignored.3.1. Local minimaMany dynamic physical systems converge to local minima. That is, instead of reachingthe point of minimum energy, they reach a stable point in which the energy is not minimal.In our case such a phenomenon might cause partial satisfaction of the goals in the system,even when the agents can, potentially, satisfy all of the goals. The question to be askedis whether such a problem exists in our model. Referring to the results from physics, it iscommon for a single particle to be captured in a local-minimum point due to a specificconfiguration of the forces affecting it at this specific point. However, in large particlesystems, it is most unlikely to have all of the particles captured in local minima. Theprobability of the latter increases as the system becomes less energetic. The systems withwhich we deal are highly energetic, and only while satisfying goals do they gradually loseenergy. Therefore, they will reach a low-energy state only when most of the resourceshave become depleted. This will happen only when most (or all) of the goals are alreadysatisfied.From the discussion above we conclude that single agents (only a small number of them)may reach local minima. However, the agent system as a whole will normally avoid localminima, and may be exposed to this risk only after satisfying most of its goals. Therefore,the problem of local minima is of lesser importance in our case. This perception wassupported by the results of the simulations we have conducted.26 The mass mi may also be modified due to partial goal-satisfaction, but the two modifications are independent.In that case, however, the mass of the particle that models the agent may be modified as well.20O. Shehory et al. / Artificial Intelligence 110 (1999) 1–553.2. Goal performance timeAn important property of our model is its ability to provide task-allocation and executionfor a vast majority of the tasks in the system within a bounded time period. 27 This propertystems from two facts: first, the average velocity (v) of particles in the model is a knownconstant; second, as we later prove in Section 4.3, the length of trajectories (l) that particlesin the model traverse is, for most of the cases, bound. Since traversal time (t) relates to thelength of trajectory and to velocity by t D l=v, it follows that t is bound as well. This meansthat one can expect that the time for executing all of the tasks (or at least the vast majorityof them) is bound.4. ComplexityThe complexity of the model that we provide results from two main factors. One fac-tor is the time necessary for reaching a single goal by a single agent during a singletime-segment. The other is the number of time-segments necessary for the completionof the goal-satisfaction procedure, including the time that the agents spend on actual goal-satisfaction. The ratio between the number of goals and the number of agents within thesystem has a major effect on the complexity. These factors are analyzed below. 28 Webegin our analysis in Section 4.1 where we discuss the time consumption during a sin-gle time-segment and only for the virtual motion (and not for the goal-satisfaction) of theagents. Then, in Section 4.2 we analyze the time consumption of the whole process andprove its convergence to a solution. However, in this section we do not yet incorporate thegoal-satisfaction effect on the performance of the process. This analysis is presented inSection 4.3.4.1. Time consumption during a single time-segmentThe time that each agent consumes at each time-segment for calculating its progressionin the goal-domain is equal to the number of time-steps dt that comprise the time-segment,multiplied by the time necessary for the calculations within the time-step. In order toexpress the time-consumption we must first present some concepts and notations andformulate the relationships among them. The number of agents and goals within the rangeof mutual interaction depends on the density of the distribution of agents and goals withinthe domain of the system. That is, it depends upon the average distances between agentsand goals, and not on their total quantities. We denote the number of agents by N.t/, thenumber of goals by G.t/, the total area of the goal-domain by S.t/, the density by n.t/ andthe average virtual distance between agents and goals by d.t/. We express the above as27 This is subject to having sufficient resources and appropriate capabilities available.28 Note that the analysis is constructed from several incremental steps. The reader who is not interested in all ofthe details of the analysis may skip directly to Section 4.3.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5521functions of the time t since they may vary over time. However, in the following analysiswe omit t for ease of representation. Using these notations we can state the following 29n (cid:17) N C GSand S (cid:24) .N C G/d 2:From these basic relations we can conclude thatrd (cid:24)1n:(6)(7)Now that we have formally presented the relation between the average distances and thedensity of the agents and goals in the system, we can formulate an expression for thetime consumption of a single agent depending on the density of the system. The averagenumbers of agents and goals within the range of interaction are given by:and GI D GNI D N(8)n(cid:25)r 2In(cid:25)r 2IN C GN C Gcorrespondingly, and the single-agent time-consumption Ct per time-segment is:(cid:0).NI C GI /tV C 2tint C tT(cid:1)Ct DItr;(9)where tV is the time necessary for calculating the mutual potential and deriving thepotential function, tint is the time necessary for integrating the force and the velocity toyield the velocity and the displacement, respectively, tT is the time for calculating thetranslation from the previous displacement to the new displacement, and Itr is the numberof dt iterations. 30 The derivation and integration that are necessary for calculating thechange in the state of an agent may be complex, but as explained in Section 2.5.4, thiscomplexity can be reduced. In addition, this complexity does not depend on the size ofthe system S or on the number of agents N within it. As a result, the complexity of thesecalculations becomes a small constant.An interesting property of our model arises from the relationships above: the number ofdt time-steps, that is, Itr, increases slowly with respect to the number of agents or even staysconstant, as shown below. In cases where the size S of the goal-domain is fixed, as G and Nare increased, the average numbers of agents and goals within the range of interaction GIand NI correspondingly increase linearly. However, the number of iterations per agent, i.e.,Itr, decreases with respect to the growth in NI and GI . This reduction in Itr results fromthe reduction in d, the average distance between agents and goals, and depends on somephysical properties, too: in large-scale physical systems, the velocity-distribution is a verynarrow normal distribution. This means that most of the velocities are close to the averagevalue. We assume that the velocities do not change due to a change in the number of agentsin the fixed-size system (which is a reasonable assumption). Under such an assumption, andN C G.according to Eq. (7), when N and G are increased, d decreases proportionally top29 We use here the notion of area for reasons of convenience, but we do not restrict ourselves to the two-dimensional case. Rather, we must note that a high dimensionality of the system may reduce the computationalcomplexity. If the dimensionality is denoted by m, then the relations above can be converted to S (cid:24) dm andd (cid:24) n30 Note that we alternate between iterations and time-steps, but they have the same meaning.(cid:0)1=m, and this modification may affect the complexity.22O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55A reduction in the distance entails a linear reduction in the number of time-steps necessaryfor completing the passage of the distance, as can be observed in physics. From this, weconclude that the increment in the numbers of agents and goals leads to a reduction inthe number of time-steps necessary for reaching a goal which is inversely-proportional topN C G. Merging this conclusion with Eq. (9) yields:pCt (cid:24) .NI C GI /Itr (cid:24)N C G:(10)This increment in Ct holds when the size of the system is fixed. However, such a situationdoes not necessarily hold. It may be that both the number of the agents and the size of thesystem have simultaneously been increased. In such cases we shall examine the densityof distribution of agents among the system area. This density n is defined in Eq. (6) as.N C G/=S. In cases that n stays constant, d does not change (see Eq. (7)). Therefore,increasing N leaves NI unchanged and increasing G leaves GI unchanged. The fact thatd does not change implies that Itr remain constant, too. Thus we deriveCt (cid:24) .NI C GI /Itr (cid:24) const:(11)This is a very important property of the model that we propose, since many systemsmay preserve their density when they grow larger, and the result seen above promisescomputational complexity which does not depend on the size of the system.4.2. The general time-consumption for virtual motionAn important issue for the analysis and the assessment of any algorithm is both itscomputational complexity and its convergence to a solution. Since the complexity of asingle time-segment of the algorithm has been calculated above, we have to completethis assessment by proving that the algorithm converges to a solution and to calculate thecomplexity of reaching the solution. In this section we do not yet take the goal-satisfactiontime-consumption into account. This shall be done in Section 4.3.If we assume the worst case, where at each time-segment only one goal is reached by anagent and only part of this goal is satisfied by the agent, then the number of time-segmentsfor satisfying all of the goals will be O.G/. This is because even if each agent completesonly a small fragment f of a goal (and we assume that the size of f does not depend eitheron the number of agents or on the number of goals), the number of time-segments for thecompletion of a single goal will be 1=f and the number of time segments for completionof all of the goals will be G=f . However, the average case is usually better. Since all of theagents are working simultaneously on goal-reaching, Itr—the number of time-steps dt forreaching goals by the agents—decreases. In a case that NI < GI , this reduction is boundedby G=N (note that G=N D GI =NI ). G=N is the optimal number of time-segments forsatisfying all of the goals when N < G. The reason for the reduction in Itr is that whenan agent has reached a goal, the other agents (except for an average of NI =GI that weremoving towards the same goal) have also progressed toward goals. Therefore in the nexttime-segment they will have to move only the remaining distance. We must emphasize thatin the case of GI > NI , this reduction holds only for O.NI / distances, and not necessarilyfor all of the distances.The magnitude of the average distance-reduction during a time-segment is not alwaysknown. We have two major assumptions about the average distance-reduction during aO. Shehory et al. / Artificial Intelligence 110 (1999) 1–5523time-segment. The first assumption is of a constant reduction from the original distance,which does not depend on the number of time-segments that have already taken place. Thesecond assumption is of a reduction that depends on the number of the time-segmentsthat have taken place. The conclusion from the first assumption is that d, the averagedistance, is reduced to a fraction of its original value. This does not improve the order ofcomplexity of the worst case, although the factor of the complexity is smaller. The secondassumption requires that at each time-segment the distance decreases, on average, to 1=xof its size in the previous time-segment (where x > 1 and constant). The conclusion fromthis assumption is that the distance that was d at time-segment t will decrease to x(cid:0)kd attime-segment t C k, where k is an arbitrary integer. Since the time of progression towardgoals is linear with respect to d, the average time will diminish in the same manner asd, the average distance, does. Subject to the second assumption, the total of the distancesduring the whole goal-satisfaction process is the sum over all of the time-segments, and isproportional toGNGXiD11xi ;or, in the case of the first assumptionGGXiD11xi :(12)(13)The sum in the equations above has a constant upper bound which is equal to 1=.x (cid:0) 1/ anddoes not depend either on G or on N . Resulting from this sum are both the convergence ofthe algorithm to a solution and the expected complexity of the general procedure of goal-N C G/ in the worst casereaching. In cases where Eq. (10) holds, this complexity is O.GN C G/ in the average case. In cases where Eq. (11) holds, the complexityand O..G=N/is O.G/ in the worst case and O.G=N/ in the average case. The last result yields fromthe constant time necessary for each time-segment according to Eq. (11) and the constant-bounded number of time-segments. In a case that N > G, the complexity is reduced eitherto O.N C G/ when Eq. (10) holds or to O.1/ when Eq. (11) holds.pppThe last result is disconcerting. In order to resolve this, we must bring forth the rationalefor the case of constant-time complexity. When dealing with very large-scale systems ofagents, the whole system may be observed as an ensemble of sub-systems, all of themrelatively independent, and acting independently in parallel. Thus, the time consumed bya single system for completion of its activities is equal to the time necessary for the wholeensemble. For any system that can be arbitrarily partitioned into many smaller sub-systemssuch that they are large enough to fit our algorithm requirements, this rationale holds.One can claim that the partition into sub-systems does not provide an appropriatedescription of the whole system, because there may be interactions between entities fromdifferent sub-systems. We agree that such interactions are possible. However, we canresolve this as follows. If we allow partial overlap of adjacent sub-systems, then theoverlapping ranges will represent regions of interaction between entities from adjacent sub-systems. These common interaction regions must be large enough to allow for interactionand small enough to cause only minor effects with respect to the whole system. Since the24O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55range of interaction is rI and the size of the whole system is much greater, we find nodifficulty in performing a partition into sub-systems that will satisfy the size requirementsof the overlap region. Thus we reject the claim.4.3. Goal-satisfaction and computation timeWe would like to examine the performance of our algorithm with respect to the optimalagent-goal allocation. 31 This allocation depends on the properties of the goal-agentsystem. Therefore, we shall partition the problem into several sub-problems. We shall dealwith cases where N 6 G. We discuss hereby the effect of the time necessary for goal-satisfaction on the performance of the algorithm. This discussion expands the analysis inSections 4.2 and 4.1, that considers only the computation time, and completes it. We shallpartition the problem subject to the following parameters: size of goals; size of agents; timeconsumed for goal-reaching (denoted by tr ); time consumed for goal-satisfaction (denotedby tg).4.3.1. Equi-size goalsThe first and the simplest will be the case where all of the goals are of the same sizeand all of the agents are such that each agent satisfies a goal with the same efforts and timeconsumption. In such cases, the optimal allocation will cause each agent to perform G=Ngoals, as shown in Section 4.2. We examine the following sub-cases:(i) tg (cid:28) tr :In such a case the goal-satisfaction time is negligibly small. Therefore, thediscussion and the results of Section 4.2 (in which tg was ignored) hold.(ii) tg (cid:24) tr :The fact that the time necessary for goal-satisfaction is not negligible any moremay have a vast effect on the complexity of the algorithm. However, in the currentcase, where agents complete goals by themselves, it has a minor effect, and thecomplexity remains O.G=N/.(iii) tg (cid:29) tr :In this case, since the main factor becomes tg, but the number of goals that eachagent performs is still O.G=N/, the time consumption will be .tg=tr /.G=N/ peragent.The proof for the results above is straight forward. The PAS algorithm allocates agentsto goals mainly according to their capabilities. 32 That is, it is more probable that agentswith more capabilities will be matched to goals that require more capabilities in order to beresolved. In a case that the agents possess of more capabilities than necessary for satisfyingthe goals (as in the case with which we currently deal), the rejection concept of the PASmodel will prevent the allocation of more than one agent to one goal, and therefore allof the agents will be allocated to different goals. Agents that will complete performing a31 This examination is mainly based on the trajectories particles in the model travel, however, it has furtherimplications.32 The allocation depends on the distances of the agents from the goals, too, but for agents with similar distances,the capabilities will affect the allocation.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5525goal will be re-allocated to other goals. Thus, no hindrances may occur and the averageallocation ratio will be O.G=N/ goals per agent, as for the optimal allocation.4.3.2. Goals of various sizesThe second case should have been the case where the goals are still of the same size,but not all of the agents can complete the satisfaction of a goal by themselves. However,several encounters must occur before all of the goals are satisfied. Therefore, after eachencounter some of the goals diminish. As a result, the goals in the system do not remainof the same size. Hence, the case with which we deal will be the case in which both theagents and the goals have a variety of sizes. This may cause agent-goal allocation that willlead to a partial goal satisfaction, and the yielded reduction in the size of the goals willaffect the proceeding allocations. We denote the size of goal gi by Sgi and the size of agentAj by Saj . The average goal size is denoted by Sg and the average agent size is denotedby Sa. In Section 3.2 the average number of goals per agent was expressed by G=N . Thisaverage is insufficient for the analysis of goals and agents with various sizes. Here, theaverage amount of goals-units to be approached (and satisfied) by an agent, with respect toagent-units amount, shall be considered. This average (denoted by (cid:22)) depends on the sizesand not on the number of goals and agents and, for an agent Ai , is given byP(cid:22)i DSaij SgjPk Sak:(14)We consider three cases, all in which Sa < Sg:(i) tg (cid:28) tr :As before, such a case entails a negligibly small goal-satisfaction time. This impliesthat the order of the complexity is not modified and is O.G=N/ in average.However, the sizes of goals and agents affect the factor of the complexity. The timeconsumed for goal-satisfaction is negligible, but the number of goals performed byan agent i is not the only factor that influences the complexity. An important factoris the total virtual-distance that an agent passes in order to reach the goals that itfulfills. That is, it would be better for the performance of the system that agentswill be allocated to tasks in a way that will minimize their virtual trajectories.The system will complete the performance of its goals when the last agent willcomplete its last goal. Therefore, an optimal goal-agent allocation should seek theminimization of the maximal trajectory. We must check if in the case where Sa < Sgour algorithm reaches results close to optimal, and provide a method for measuringthis proximity.The PAS model “prefers” the allocation of closer entities over the allocation ofdistant entities. It also prefers the allocation of greater entities over the allocationof smaller entities. However, the size has only a linear effect on the interaction,where the distance has a high-order polynomial effect. As a result, our algorithmwill give priority to the allocation of closer entities. This will be done with respectto the other entities within the range of interaction. We can view this allocation as26O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55a Traveling-Salesman Problem (TSP). 33 Via this observation, we can assess thecomplexity of our algorithm with the specific settings.The TSP problem is of finding a tour through nodes in a graph where each edgehas a cost, and the tour should visit all of the nodes with a minimal total cost. Itis similar to our problem in its property of finding trajectories between nodes withminimal costs. In our problem the agents seek a trajectory with a minimal length(which is similar to cost). There are several differences between TSP and our caseas we detail below. These differences result in TSP algorithms being inappropriatefor our needs. Yet, the complexity inquired by such algorithms for selecting atrajectory with a minimal cost is useful for comparing with the complexity of ourmodel.TSP differs over the number of agents dealt with (a single agent, whereas wedeal with multiple agents). Multiple agents, in PAS, must avoid conflicts amongthemselves. Therefore, there are occasions where agents prefer longer paths inorder to avoid conflicts. There may also be a decision-problem upon which agentshould perform each task, especially when two agents (or more) with the samecapabilities are approaching the same task.We use a TSP approximation algorithms as a means for the assessment of thequality of the trajectories that result from our algorithm. For this purpose wefind TSP an appropriate reference. The TSP, which is an NP-complete problem,has several polynomial approximation algorithms [44]. Some of them are provedto reach a solution with a cost which is not greater than twice the optimal cost(i.e., the ratio-bound is 2). This ratio-bound is achieved in cases where the costfunction c satisfies the triangle inequality. That is, all of the nodes x; y; z satisfyc.x; z/ 6 c.x; y/ C c.y; z/. In cases where the costs are the distances between thenodes, the Euclidean geometry implies that the triangle inequality holds. The PASmodel consists of such distances between particles, 34 and therefore a particle’sprogression can be viewed as a salesman trajectory. The assessment of the resultsof our algorithm, using comparison to the results of an approximated TSP solution,is provided below.Our algorithm implies that a dynamic particle will most probably (but notalways 35/ reach the closest static particle. This can be viewed as a greedy algorithmin which the shortest distance (cost) is chosen with a probability 1 (cid:0)(cid:14), where (cid:14) (cid:28) 1.This probability depends on the ratio between the number of agents N and thenumber of goals G or on the ratio between the corresponding areas. The reason for(cid:14) (cid:28) 1 arises from the physical model. A dynamic particle will usually experiencethe strongest force from the closest static particle and therefore will move directlytowards this particle. This should happen unless N (cid:29) G. However, even in thelatter case, where many dynamic particles tend to reach the same static particle,33 A comprehensive description and discussion of the TSP can be found in [36].34 When we deal with an abstract space, we define abstract distances. If appropriately defined, these mayconform with the triangle inequality. For instance, in a case of “distance” between keywords, having threedatabases D1; D2; D3 and distances measured by the number of different keywords, one can prove thatd.Di ; Dj / C d.Dj ; Dk / > d.Di ; Dk /, i 6D j 6D k 2 f1; 2; 3g regardless of the order of i; j; k.35 This is because it may be rejected by another dynamic particle that is moving towards the same static particle.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5527PPPPi Sai >i Sai <j Sgj all of the dynamic particles shall have no preclusions whenifmoving toward the same static particle. Therefore, the probability of reaching theclosest static particle is close to 1, and (cid:14) (cid:28) 1. In cases thatj Sgj (cid:14) isnot guaranteed to be small, but (cid:22) will be small.A greedy TSP approximation algorithm in which the shortest distance (cost) ischosen with a probability 1 was proved 36 to have a ratio-bound of 2. The 1 (cid:0) (cid:14)probability we introduce will cause only a small change in the results, and thereforethe resulting trajectories of our algorithm will not be far from the ratio-bound of 2.This depends on the number and the sizes of the modeling static particles withwhich each modeling dynamic particle collides during the iterative goal-satisfactionprocess. The modified ratio-bound will therefore be 2=.1 (cid:0) (cid:14)/(cid:22), where (cid:22) is thenumber of goal-units that an agent has approached during the goal-satisfactionprocess. 37 The average (cid:22), that is denoted by (cid:22) will be;(15)P(cid:22) DSaj SgjPk Sakwhere Sa is the average agent size. However, the worst case that is related to aspecific agent Ai , denote by (cid:22)wj SgjSaii , may be(cid:22)wi(16)PD;where Saiis the size of agent Ai . However, the worst case has a very lowprobability, because it happens only when all of the goals are performed by oneagent, and all of the others perform nothing. According to the analysis above, if (cid:14)and (cid:22) are small, the resulting trajectories of our algorithm are not too far from theoptimal TSP trajectories. Note that this property is not affected by the size of thesystem.To summarize the case where tg (cid:28) tr , the order of the time consumption for goal-reaching and goal-satisfaction is the number of goals per agent multiplied by theratio-bound, i.e., 2(cid:22)=.1 (cid:0) (cid:14)/(cid:22). Note that this may be a large number in cases where(cid:14) and (cid:22) are large, but as explained above, in some cases a big (cid:14) implies a small (cid:22).(ii) tg (cid:24) tr :Here, the influence of the goal-satisfaction time comes into account. In the previouscase we have presented an analysis of the quality of the trajectories that resultfrom the PAS model. This analysis holds for the current case too. As a result ofthe time consumption for goal-satisfaction, the complexity from the previous casewill be multiplied by a constant, but will remain of the order of 2(cid:22)=.1 (cid:0) (cid:14)/(cid:22). Theinfluence of the quality of the trajectories can be omitted only in the case of tg (cid:29) tr ,as described below.36 The algorithm is based on a greedy minimal spanning tree algorithm. The details and the proof are foundin algorithms textbooks such as [7]. This greedy choice is similar to the physical behavior of a particle, which“selects” the direction in which the maximal force is applied. Of course, the reason of this behavior is totallydifferent.37 Note that the number of goals per agent here is different from this number in the equi-size goals case, whichis O.G=N /.28O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55(iii) tg (cid:29) tr :In this case, since the time consumption for goal-satisfaction is much greater thanthe goal-reaching time, we shall ignore the latter. The goal-satisfaction time of anagent is the sum of the time-periods consumed for the satisfaction of goals by thisagent. This sum depends on the goal allocation, which depends on the size of theagent and the sizes of the goals. Therefore, it will be::(17)Pj SgjPk SaktgtrSai5. ExamplesTo illustrate the method in which our algorithm may be implemented, we presentthree examples. In the first (Section 5.1) we recall the hole-filling problem and provide adetailed adjustment of the PAS model to this problem. In the second example (Section 5.2)we present a transportation system and show that the PAS model can be used forthe implementation of cooperative goal-satisfaction in such systems. The third example(Section 5.3) illustrates the applicability of our model to real-world MAS with abstracttasks.5.1. Hole-filling robotsThere is a planar surface with holes in it spread in an arbitrary order. The holes havevarious sizes. On this surface, robots that are designed to fill holes are able to move towardsholes and fill them. The robots have various (limited) quantities of filling. The robots areall members of a system which is aimed at filling as much hole-volume as possible. Theholes and the filling have the same volume units (e.g., cubic centimeters). The robots haveto reach the holes and then fill them. For reaching the holes, the robots are able to move andthey have a velocity. For our algorithm illustration, it will be more convenient to assumethat the velocities of all of the robots are of the same order of magnitude. This assumption isnecessary because otherwise, the assumption that the robots can perceive what happens intheir neighbourhood might be violated. This is because fast robots will move at a velocitythat will cause them to “appear” too close to other agents before they have been detected.Hole-filling and moving are both time consuming. The moving time can be expressedbytmove D distancevelocity:(18)There is a filling rate which does not depend either on the size of the hole or on the amountof filling that the robot holds. This filling rate depends only on the filling capabilities ofthe robot. It will be simpler to assume that all of the robots have the same capabilities, andthey differ only by the quantities of filling they possess. If we denote the filling rate by R,then the time for filling a hole of volume Vhole by a robot (that has at least an amount ofVhole of filling) is given bytfill D VholeR(19):O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55295.1.1. Fitting the model to the problemWe present the fitting properties in Fig. 4. Note that in the table most of the propertieson the left column are the given parameters of the MAS, and only dtr is determined bythe designers when fitting the PAS model to the MAS. The designers may also limit theeffective range of the sensors or let their agents use only part of the information gatheredby these sensors.As can be observed from the table, the average robot velocity and particle velocity areof the same order. Due to the large difference of distance-magnitudes, the time-periodsare different, too. The mass of particles is much smaller than the volume of holes andfilling stuff and has different units. However, this difference should not affect the matchof the physics model to the robot implementation. This is because the Lennard–Jonespotential, which is employed in our model, is calculated with respect to a single mass-unit, and a particle is assumed to have one mass unit. Therefore, one volume unit in therobot case shall be represented by one mass unit in the physics model. Modification in thisrequirement will not cause any significant change in the behavior of the system, becauseany other linear scaling of hole-volume to particle-mass will only cause multiplication ofall of the derived results by a constant. The meaning of such a multiplication is no morethan zooming.In the case of holes, which have both radii and volumes, we must take into considerationthe relation between them. If a hole’s dimension is d and its volume is Vhole, then thepdiameter of the hole will be: dia D dVhole. This implies that the mass of the particle andpits diameter 2r0 must satisfy 2r0 ’ dmass. Such a relation does not necessarily exist, andtherefore when matching the hole/robot system to the PAS model, the mass of the particlesshall be chosen to satisfy this relation. As can be observed from the table (and required byHoles/robotsHole/stuff volume (m3 to m)Diameter of the hole (m2 to m)Sensors’ effective range (m)Movement velocity ((cid:24) m/s)ParticlesMass ((cid:24) 10(cid:0)27 kg)r0 ((cid:24) 10(cid:0)10 m)rI ((cid:24) 10(cid:0)9 m)Movement velocity ((cid:24) m/s)Average path time is (cid:24) 101 sAverage path time (cid:24) 10(cid:0)8 sdtr D 10(cid:0)1 sHole-filling time:tfill D Vhole=Rdtp D 10(cid:0)10 sCollision time:depends on the distance butlimited* by 100 dtptfill D 100 dtr*This limitation is required in the PAS model as described in Section 2.4.tcollision D 100 dtpFig. 4. Holes/robots and particles scaling units.30O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55the model), the radius of interaction should be about ten times greater than the radius ofthe particle. In the robots’ system, it may be that the sensors can perceive information fromdistances that are more than ten times greater than the hole radius. This is not problematicbecause the robots can either ignore distant entities (which is simple but wasteful), or usethis surplus information to make more calculations and elucidate the solution. The latter,of course, requires better computational capabilities, and shall be considered with respectto these. In a case that the designers decide to use the greater sensing capability, the ratiorI =r0 shall be changed correspondingly.In order to implement the robot/hole system using the PAS model, designers shall use thematching that we have presented above. That is, during the activity, each robot will perceivethe properties of the other holes and robots within the range of its sensors, and will processthis information. This processing will include the transformation from the robot/hole unitsto the particles unit, if necessary, and then the calculations that are required according toPAS. The results of the calculations according to PAS will be reversely transformed to therobot/hole units, and the robots will advance according to these results. The perception-transformation-calculation-retransformation-action procedure will be repeated by eachrobot every dtr , as required by our model. Since the transformed dtr is 0.1 s, it will bea sufficient time for any reasonable sensor and processor to perform both the perceptionand the required calculations.5.2. Applying the PAS model to transportationAnother example of MAS in which the PAS model can successfully be implementedis a freight transportation system. Such systems have been discussed in the context ofDAI, e.g., in [19,20,47,60]. In a case where each carrier is controlled by a computationalagent, a coordination mechanism is necessary. If the system is large and communicationis limited, agents that try to increase the common benefits of the system can act accordingto the algorithm that we provide, thus improving the system’s overall performance. Thisimprovement stems from the avoidance of the intensive computation of the schedulingproblem and the circumvention of bottlenecks that may appear in a centralized mechanism.In order to enable such an implementation, we should adjust the PAS algorithm to thetransportation system. This adjustment is described below.The system of freight transportation consists of many carriers. Each carrier has a freight-carrying capability that is given in units of volume and has a given location. The tasks thatthe carriers must fulfill are freight-transportation tasks. We deal here with freights thatshould be moved from various locations to other locations. Therefore, each task can becharacterized by its original location, final location and volume. The adjustment of themodel to the freight system is presented in Fig. 5. As can be seen from the table, eachtask will be modeled by a static particle and each agent by a dynamic particle, as requiredby the PAS model. The volumes will be modeled by particle masses, and the locations byparticle locations.A necessary feature in any agent system and, in particular in the case of transportation,is a communication system. This system must allow for the broadcasting of informationO. Shehory et al. / Artificial Intelligence 110 (1999) 1–5531Carriers and freightsParticlesCarrierFreightDynamic particleStatic particleCarrier/freight volumeParticle massCarrier locationDynamic particle locationFreight locationStatic particle locationFig. 5. Carriers/freights and particles.concerning task execution and locations of agents and tasks. 38 Failing to handle suchinformation, the ability of the system to supply transportation services may deteriorate(see Section 6.2). However, complete and accurate on-line information may not beassumed. Agents are expected to transmit information concerning their location and goal-satisfaction, but this transmission is usually received only by a subset of the other agents.Even within the target subset of agents there may be some which receive an inaccuratemessage or do not receive a message at all (we have examined the effect of such messagetransmition via simulations as described in Section 7).The structure of cities and roadways regulations may prevent movement along theshortest path between two locations, as assumed by the general algorithm. Thus, in thesimulation of the transportation example (Section 7), the distance between two locationsl1 and l2 is calculated as the shortest way that one could drive from l1 to l2. In addition,if the direction for movement 39 bv calculated by the agent in the goal-reaching processalgorithm does not agree with a road direction droad, then the road with the smallest angelwith bv is selected for movement. This selection is not different from a physical behaviorin environments with obstacles, and therefore justified. Another limitation imposed byregulation is a speed limit. We take this to be 50 km/h. A limit on velocities is not partof the physics model, however velocity distribution may allow for values above the limitto be of low probability, thus eliminating them should have a minor effect on the overallbehavior of the system, as our simulations show.Yet, a more detailed adjustment is necessary to enable the implementation of PAS forthe transportation case. Therefore, we summarize the above and expand upon it below. Wedistinguish between two levels of adjustment—the conceptual level and the practical level.The conceptual level consists of the following:(i) The reception of information concerning a freight-transportation task is modeledby the entrance of a particle into the effective radius of interaction rI . Note that inthe transportation case the information is usually gathered via receptors which arenot always affected by the distance from the carrier to the freight. This may imply38 Such communication systems exist in transportation companies, and drivers report their location and taskexecution occasionally. However, although many drivers receive the transmitted information, usually only thecentral coordinator uses the information for planning and allocating tasks.39 The notation bv refers to the direction of a vector Ev.32O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Carriers/freightsCarrier/freight volume (m3)Delivery distance ((cid:24) 104 m)Information range ((cid:24) 103 m)Movement velocity ((cid:24) 10 m/s)Average reaching time (cid:24) 103 sAverage delivery time (cid:24) 103 sdtc D 10 sParticlesMass ((cid:24) 10(cid:0)27 kg)r0 ((cid:24) 10(cid:0)10 m)rI ((cid:24) 10(cid:0)9 m)Movement velocity ((cid:24) 1 m/s)Average path time (cid:24) 10(cid:0)8 sCollision time (cid:24) 10(cid:0)8 sdtp D 10(cid:0)10 sFig. 6. Carriers/freights and particles scaling units.an excess of information. To handle this excess, the information can be filtered torefer only to the most relevant tasks.(ii) The advancement towards a freight is modeled by the movement of a dynamicparticle towards a static particle.(iii) The execution of a freight-transportation task is modeled by the collision betweena static particle, which models the task, and a dynamic particle, which models theagent.In addition to the conceptual level, we present the practical level. As seen previously, this isperformed via matching the properties (see Fig. 6). Most of the properties in the left columnof the table are the typical parameters of the specific transportation system with which wedeal. Only dtc, which is the time-period 40 of the transportation system, is determinedby the designers when fitting the PAS model to the MAS. The radius of interaction rIin the transportation domain is (cid:24) 103 m. As can be observed from the table, the averagecarrier velocity is ten times greater than the particle velocity. This, in addition to the largedifference of distance-magnitudes, results in different time-periods as well. That is, dtc issignificantly different from dtp, which is the time-period of the particle system.Note that the magnitude of dtc is 10 s (which is required to conform with the 10(cid:0)11ratio between times in the physical model and the computerised system). This impliesthat each agent must perform the calculations required according to the PAS modelevery 10 s, and accordingly, move to a new location. 41 Agents must report their locationoccasionally, to enable other agents to use the information about the location for theircalculations. However, it is not required that the agents report their location every 10 s.Such a requirement may be impossible if the reports are performed by human agents.It would, however, be possible if an electronic tracking component is installed in everyagent. Such a device can broadcast its location and an identification code, thus providingthe necessary information with a high frequency and a low cost. Nevertheless, even withoutthe high-frequency reports we have suggested, the system can maintain its stable behaviorand continue satisfying goals. The quality of the calculations will deteriorate in such cases,40 The time period is the typical time-step for the iterative calculation as presented in detail in Section 2.4.41 In the simulations we set it to 1 instead of 10 to speed up the computational process.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5533however, reasonable results and goal performance can still be achieved, as explained inSection 6.2, where imprecise information is discussed.The difference between mass of particles and volume of carriers and freights is of thesame type as in the hole-filling example. Other adjustments are similar to the adjustmentsproposed for the hole-filling domain, and may be similarly justified. To avoid redundantrepetitions, we refer the reader to Section 5.1.The above adjustment allows computational agents to use the PAS algorithm tocoordinate their actions in order to perform the freight-transportation tasks and increase thebenefits of the system. This will be done with very low computation and communicationefforts, as the simulations (Section 7) demonstrate.5.3. Application to abstract tasksAlthough based on physics, the PAS model may be applied to MAS that executenon-physical tasks. To demonstrate this capability, we provide below guidelines to theapplication of PAS in a real-world multi-agent architecture named RETSINA [55,56].In brief, a RETSINA MAS consists of three types of agents—information agents, taskagents and interface agents. Agents have capabilities to perform tasks and capacities (ofthese capabilities) which limit the size of the task or the amount of resources beingused for its execution. In RETSINA, agents do not know about other agents in advanceand may find them via matchmaker- or broker-agents, which are both a specific type ofinformation agents. To date, agents in RETSINA are implemented such that each has asingle capability. 42 As such, since for some tasks several capabilities are necessary, theagents form teams on demand to execute these tasks. Note that an agent A in a RETSINAMAS has a limited view of the rest of the agent society. This view includes mainly agentsand tasks which are relevant to A’s capabilities and tasks. This confined view is similar to,and modeled by, the limitation on interaction range in the PAS model.Agents within RETSINA satisfy goals by interleaving planning and execution. Duringthis process they gradually de-compose a high-level goal to tasks and subtasks. The lower-level tasks consist of executable code with several pre-conditions. Whenever a lower-level task becomes executable (i.e., all of its pre-conditions are satisfied), it is scheduledfor execution and executed when appropriate. The planning mechanism implements anextension of the Hierarchical Task Network (HTN) model [18]. HTNs are hierarchicalnetworks as depicted in Scheme 1. They consist of task-nodes which are connected bytwo types of edges. Reduction link edges describe the de-composition of a high-levelgoal to tasks and subtasks (a tree structure). Provision/outcome link edges represent valuepropagation between task-nodes. Provision/outcome propagation allows one task T1, as itcompletes execution, to propagate an outcome to another task T2, where this outcome is aprovision for the execution of T2. For instance, suppose T is a task of buying a stock. Tmay de-compose to finding the price (T1) and performing the transaction (T2). The latter(T2) requires that T1 be executed, and therefore T1 should propagate a success outcometo T2 when completed successfully. This outcome is a provision for T2. HTNs allow task-nodes to have multiple provisions and outcomes. A task is executable if it cannot be further42 This does not prohibit more complex agents in the future.34O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Scheme 1.reduced and all of its provisions are set (as a result of either outcomes of other tasks or asetting from an outside source). We will use the HTN properties of the RETSINA agents todemonstrate how the PAS model can be used to coordinate task allocation and execution.The main idea of this modeling is that agents gradually allocate themselves to tasks,sometimes through partial execution of other tasks. 43 Each low-level task (that cannotbe further reduced) needs a specific capability for its performance, however usually severalpre-conditions must become true (this happens by setting provisions) before a task becomesexecutable. An agent A would be “attracted” to a task T when it has the required capabilityand capacity for executing T . A would gradually advance toward T by performing theactions which are necessary for satisfying the pre-conditions of T . As the number of thelatter which are yet false decrease, the agent becomes “closer” to the task. Note that thisdoes not prohibit cases where two (or more) agents simultaneously work on setting truepre-conditions of the same task. Only when these two agents have both the same capability(and excess capacity) will “mutual rejection” among them prohibit simultaneous workon the same task. No direct interaction will emerge among agents who have differentcapabilities.To adapt to PAS, we must explain how rejection and attraction, as described above,are practiced in the RETSINA framework. This would be possible if we can measure andnumerically express distances between agents and tasks. For this we define an abstractspace—the provision space—which is a space that consists of all of the provisions allowedin RETSINA. Since provisions are represented by Boolean values, the displacement of atask T would be represented by a vector of provisions PT , where all of the provisionswhich are the specific pre-conditions of this task are set to 1 (true) and all of the othersare 0 (false). Suppose an agent A has the right capabilities and sufficient capacities andattempts to gradually allocate itself to T and perform it. A would have a provisions’ vectorPA where all of the provisions of the approached task T which have already been set are1, and all other provisions 44 are 0. Note that the provisions of a task may have been set by43 Partial execution is possible when a task is comprised from several sub-tasks, and some of the sub-tasks areexecuted.44 In cases where the agent is working on more that one task some of the other provisions may also be true.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5535other agents during partial execution of the task. The distance between A and T is simplythe distance between the provision vectors PA and PT , which is(cid:1)2:sXD(cid:0)dPA;PTP iA(cid:0) P iTWe define an additional distance measureXDPA;PTD(cid:0)P iA(cid:0) P iT(cid:1)2ivuutijP iTD1D 1 refers to the i’s for which P i(normalized to T ), where ijP iT is set true. The normalizedTdistance is required for finding an agent-task distance which ignores all other tasks. Duringthe process of setting the provisions true, the distance between PA and PT decreases, thusD 0, agent A has completed its allocation to T (in PASA gets closer to T . When DPA;PTthis is expressed by r0). At that point the agent will execute the task (by running its code).Note that dPA;PT may be non-zero at this time, indicating that A advanced towards othertasks as well. This also allows the agents to allocate the “closest” one among them, B, to atask T . Other agents are not as close to T as B is since they were approaching other taskssimultaneously. This provides an advantageous behavior, since the latter agents, insteadof engaging in task execution (of T ), will continue advancing towards the other tasksthey were already approaching. Thus a selection mechanism, which up to date was notimplemented in RETSINA, is introduced. In addition to the definition of distance it isnecessary to provide an interpretation of mass in the RETSINA framework. We requirethat the mass represent the capability of task execution, and its magnitude would be thecapacity of this capability. Cases of more than one capability per task (or per agent) are notsupported by the PAS model in its current form. Nevertheless an extension of this modelto handle such cases is suggested in Section 6.1. Moreover, multiple different capabilitiesmay possibly be modeled by several physical charges (e.g., mass and electrical charge).Such an approach is only partially supported by classical mechanics. Extensions may bepossible using quantum mechanics, however this is beyond the scope of this paper.The mechanism presented below discusses the allocation of agents to low-level tasks,however, may be used for agent allocation to higher-level tasks as well. The executable partof the latter is their de-composition to subtasks. Provisions and outcomes of higher-leveltasks are not different from those of low-level tasks. The PAS model requires computationof potential functions and equations of motion. These are used to plan for task allocationand execution. When applied to RETSINA, these computations will be based on the massand distance as expressed above. The results of these computations are new displacementvectors in the provision space. Upon these an agent should decide what provision to handle(and set true) next, thus advance itself to these new displacements. This will result inagents gradually allocating themselves to tasks by means of provision enablement. A multi-directional advancement is achieved by utilizing and manipulating a provision vector whereseveral tasks (and possibly all of the relevant ones) are represented. Note that the physicalmovement which is part of the PAS model allows agents not only to get closer to tasks,but move away from them as well. The interpretation of such movements when appliedto the provision space is the resetting of provisions (i.e., setting them false). Provisions36O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Agents and tasksRepresentationParticlesAgent locationTask locationDynamic Boolean vectorDynamic locationStatic Boolean vectorStatic locationTask/agent capabilityTask/agent capacityParticle massRange of interactionSet of relevant tasks/agentsAction execution thresholdRelevant provisions set truerIr0Task performance rateProvisions set true per timeParticle velocityFig. 7. Agents and tasks in RETSINA vs. particles.may become false as a result of, e.g., limited memory of an agent. This may require thatit maintain only the most relevant provisions accessible, storing the others externally (in asimilar manner of operating systems handling multiple processes).For applying the PAS to RETSINA, it is necessary that agents know about relevanttasks and agents and their provision Boolean vectors. In RETSINA a task is not astand-alone entity. Each task T is always in the possession (and responsibility) of somespecific agent A. This agent is not necessarily the agent that can actually perform T .In order that other relevant agents know about T and its state, A should add to itsadvertisement (at a matchmaker agent) the task and its provision vector. Agents whoare interested in T may, by this contact information, monitor for updates in T ’s state.Moreover, members of teams 45 are (which dynamically form in RETSINA to execute atask cooperatively) should update one another within the team with respect to tasks’ andagents’ provision-state changes. These mechanisms, which are supported by the RETSINAframework, will provide a “range of interaction” for agents and tasks as required for thePAS implementation.The PAS modeling of RETSINA is presented in the table above. Note that the modelingpresented above only provides guidelines for PAS implementation in RETSINA. Furtheradjustment and fine-tuning will be necessary to perform this implementation. We intend topursue this direction in future research.6. Extensions of the modelThe PAS model, which concentrates on limited range of MAS cases, may be extendedto a wider range of situations, relying on its physics properties. We shall demonstrate suchextensions of a system with a single type of capability to a multi-capability system, andof a system wherein information is precise (but not complete) to a system with impreciseinformation.45 The PAS model does not prohibit teams, however, does not provide explicit mechanisms for team formation.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55376.1. Multiple types of tasks and capabilitiesThe model presented may seem restricted to cases where tasks and capabilities are of asingle type. This is not necessarily so, as we discuss below. The capability of an agent toperform tasks and required for a task to be performed by agents is modeled, in the originalPAS, by the mass of a particle. This is very restrictive as each particle has only one mass,and therefore it can model only one capability. However, we can extend the model byreferring to clusters wherein each cluster is comprised from several particles of differenttypes. Within a cluster, each particle shall model a distinct capability. Such clusters willmodel the agents and the tasks. Having done this extension, we may analyze the systemas a superposition of several systems, each of which consists of one type of particles andthus models one type of capability. The compound system can be analyzed as a multi-dimensional space, for which thermodynamics and statistical mechanics provide severalmethods of analysis. Hence, the model can be applied for the multi-capability case.As in the single capability case, the virtual motion of agents will result from the forcesthat are derived from the potential functions of the modeling particles. However, in theextended case the forces will be calculated in each space separately, and the derived virtualmotion will be calculated with respect to the superposition of these forces. Thus, clustersin which all of the particles (or at least most of them) lead to collision will model agentswhich all of their capabilities (or most of them—in a case that partial goal satisfaction isallowed) fit the required capabilities for the satisfaction of a specific task, and their (multi-dimensional) collision will model the goal satisfaction.6.2. Imprecise informationSince we assume that the information gathered about the adjacent entities is a result ofthe agents’ perception, we must take into consideration the noisy nature of informationgathered via sensors. 46 This means that not only is the information incomplete, as weassume in the model (Section 2.1), it is imprecise as well. Therefore, we shall examinethe validity of the model behaviour that is subject to such an imprecision. To studythis issue, we approach the field of thermodynamics and statistical mechanics [43].The thermodynamical behaviour of large-scale particle systems, and the correspondingbehaviour of smaller simulated systems [49], in the case of bounded (closed) systems, 47 isstable. Introducing random low level perturbations into such systems does not significantlyaffect their overall behaviour (as discussed in, e.g., [27, pp. 122–148]). This is becausethe thermodynamic parameters of the systems do not vary significantly. 48 Accordingly,we conclude that our MAS will be stable, since they are expected to be restricted by time,size, resources, etc. Such stability was evident in the simulations we have performed.46 As previously stated, we extend the meaning of sensor to any information reception device.47 Systems with a bounded thermodynamical property are, for instance, systems where the number of particles,the volume, the energy etc. are bounded.48 This stability does not always hold for unbounded or loosely bounded systems, where the result of a smallperturbation may be chaotic.38O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55As the noise to signal ratio increases, 49 even strictly bounded systems may losetheir stability. This loss of stability may occur almost instantaneously, resembling athermodynamical phase transition. However, this may happen only when the ratio issignificantly large (greater than 1 db). Relying on the physics properties, we can concludethat our MAS will maintain its stability when exposed to small perturbations, such as thatwhich noisy information may impose.7. SimulationTo examine our model and show its applicability to real problems, and in particular tothe transportation case presented in Section 5.2, we have performed a set of simulations.Via these we demonstrate effective task allocation and execution in an open, dynamicMAS that consists of thousands of agents and tasks. The problem domain for whichthe simulations where performed is as follows. We simulate freight deliveries within ametropolitan. Such problems in non-computational environments are commonly solved byhaving one or a few dispatch centers to which delivery requests are addressed and theseeach plans and accordingly allocates delivery tasks to delivering agents. 50 This methodmay face bottlenecks and inefficiency when a large, dynamically changing set of agentsand tasks is present. We demonstrate how the PAS model can overcome this limitation.We consider the road-network of a large metropolitan. A snapshot of a part of thisnetwork is depicted in Fig. 8. In this figure squares represent tasks and circles representmessengers (taken from a simulation of 600 messengers and 1200 freights, randomlydistributed). The city map is represented by a lattice-like graph. The boundaries of thecity are 20,000 (cid:2) 30,000 m. The lattice includes vertices located 200 m apart from eachother. An edge may exist between each two neighboring vertices. Each vertex representsa junction and each edge represents a road between two junctions. We designate the map“Full Lattice” when each vertex has edges emanating to all of its neighboring vertices.A more realistic map would have some of the edges missing. To obtain such a map we usesome probability to determine the existence of each edge. As a result disconnected sub-graphs (designated clusters) may occur. In such cases the largest cluster will be selected torepresent the city. We designate the map “X% Lattice” when lattice and cluster generationare performed taking the probability of including an edge in the lattice to X%.In the PAS model, the typical potential of a particle i is the Lennard–Jones potential.When adapting the model to the specific transportation application we experimented withseveral different potential functions, all of which are sums of derivatives of powers of rij .We checked the effect of modified potential functions on the performance, and after severalexperiments we finally concentrated on the following:(cid:1)(cid:0)ErVD (cid:13)ij(cid:0)(cid:0)2(cid:11) ln rij C (cid:12)rij(cid:0)4C (cid:31)rij(cid:1):(20)We sought a potential function which is similar in shape and physical properties to theLennard–Jones potential, and the one above is. It consists of repulsive and attractive49 The ratio between noise and signal is a common method to measure the quality of systems where informationsignals are present. The ratio of 10% or one decibel (db) is considered a threshold value.50 Note that a similar approach was used also in MAS [47].O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5539Fig. 8. A fragment of city map.components and diminishes after a short distance, however not as short as Lennard–Jones(and by far shorter than a Culombic potential), thus implying that the interaction betweenthe particles in the simulated system is similar to the one in the original PAS.We have performed several different types of simulations. These varied over the numbersof tasks and agents involved, the homogeneity of agents and tasks, the reliability ofcommunication, the intensity of the lattice map and the distribution of agents and tasksover the city map. The simulations consist of iterations in which new freights dynamicallyappear at random locations on the map. Messengers (agents) follow our algorithm toperform tasks of reaching freights and delivering them to their destination. Initially,simulations were performed such that agents and tasks are homogeneous in the sensethat they have similar capabilities and capacities. We started with these since they aresimpler to handle and predict. However, it was necessary to examine cases in whichagents and tasks are not homogeneous, which are more realistic. In the homogeneouscase, masses of particles were set to 1 kg, whereas in the heterogeneous case masseswhere set randomly out of a given distribution. We have also examined several latticemaps, starting from a full lattice and moving to 90% and 80% lattice maps. Since wehave seen no significant difference in the performance between the different maps, weconcentrated on the 90% lattice map. We did not test highly disconnected maps, since theyrepresent as class of problems where the solution search space is significantly trimmed,and traditional optimization mechanisms can be exploited instead of the PAS. To learn theeffect of unreliable communication on the performance we have experimented with a casein which messages are passed with arrival probability which is smaller than 1. Additionalparameters of the simulations are as follows. During the simulation no new messengersappear. Parameter values are (cid:13) D 1, (cid:11) D 4000, (cid:12) D (cid:0)15 (cid:2) 105, (cid:31) D 5 (cid:2) 1011 (in Eq. (20)),R0 is 100 m, RI is 2,000 m.40O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Throughout the simulations we have examined various numbers of agents. We allowedagents to appear and disappear dynamically, keeping the average number (in each caseseparately) constant. In the homogeneous case, we considered five settings of agent andtask quantities. From among these, in the first four simulation settings the number ofagents was 300, 400, 600 and 800, and the initial number of tasks was 1200. In the fifthcase, the number of agents was 1200 and the initial number of tasks was 1500. In all fivehomogeneous settings additional tasks where arriving at a rate of 600 tasks per hour. Thedifferent quantities of agents in the first four settings allowed us to study the effect ofthe number of messengers (hence the messengers/freights ratio as well) on the system’sperformance. The first 4 settings were also experimented with in the heterogeneous case.The fifth setting was aimed mainly at studying the effects of up-scaling, and was notexperimented with in the heterogeneous case. The simulations runs were of lengths ofmore the 70 or more. When calculating averages over a simulation run, we omitted theresults of the first 10 h. Since the system always reaches a stable stable within less than10 h, this omission prevents the averages from being affected by the initial conditions. Themain results of the simulations are summarized in the following graphs.In Fig. 9 the ratio between the number of messengers in the system and the numberof agents that are simultaneously involved in movement towards tasks is presented. Theterm Free messenger quantity is the number of messengers which are currently movingtowards freights or searching for them. The other messengers are performing tasks. Fromthe graph one can observe that as the number of messengers involved increases, so doeslinearly increases the number of those that simultaneously move towards tasks. This resultfor itself does not seem of merit, however, it results in reduction in the time required fortask execution (as can be seen in Fig. 12).The term Freight quantity in Fig. 10 is the number of freights currently waiting for amessenger to deliver them. We observe that this number drops sharply as the quantity ofFig. 9. The number of messengers moving towards freights (y axis) as a function of the number of messengers inthe system (x axis). Other agents perform tasks concurrently.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5541Fig. 10. The number of freights waiting for delivery (y axis) decreases sharply as the number of agents in thesystem (x axis) increases.Fig. 11. The time for an agent to reach a task (y axis) increases as the number and density of agents in the system(x axis) increases.messengers goes up. The critical point where transition occurs is around 500 messengers.Given that 1200 tasks are present, this means that for significantly lowering the numberof freights which are simultaneously waiting to be delivered it is enough to have a ratio ofaround 0.4 (500/1200) between messengers’ and tasks’ quantities in the system. Increasingthe ratio over 0.5 (600/1200) does not bring about a significant increase in the performance(with respect to the numbers of freights waiting to be delivered).42O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Fig. 12. The average time freights wait for delivery (y axis) decreases sharply as a function of the number ofagents in the system (x axis).Fig. 11 presents the time 51 it takes a messenger, who successfully delivers a freight toits destination, to reach this freight. One can observe that as the quantity of messengersincreases (and so does their density), the average time required for a messenger to reacha freight increases as well. This results from more messengers per freight. Given thephysics-base behavior of the system, there will be more mutual rejections, so on averagea messenger will need more time to reach a freight. This is a disadvantageous property,although it does not mean that increasing the density is all bad. As we have seen before—itsignificantly reduces the number of freights which simultaneously wait for being delivered.In addition, as shown in Fig. 12, the average waiting time of the freights decreases as well.In Fig. 12 the freight average waiting time, that is, the time that a freight that wassuccessfully delivered to its destination has been waiting before being handled by amessenger is presented. A sharp reduction in the waiting time is observed. We observephase transition around 500 messengers, similar to the phase transition in the case ofFreight quantity (Fig. 10). This further supports the observation that it is not worth whileto increase the agent/task ratio to above some ratio which is, in our simulation settings,around 0.4–0.5.Fig. 13 presents the average freight fulfillment time, which is the time between thefreight initiation and its arrival at its destination. This time subsumes the waiting timeand adds to it the execution time. Less steep than in previous graphs, yet clear, is theimprovement in the performance reached around 500 messengers. It is important to noticethat for 600 messengers and more (and 1200 initial tasks) the overall task execution time isless then 1500 s. For a city of the size with which we deal (20 (cid:2) 30 km) with a speed limitof 50 km/h, this is a desirable fulfillment time.51 Here and in the following graphs time is measured in seconds.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5543Fig. 13. The average time for task allocation and execution (y axis) decreases as the number of agents in thesystem (x axis) increases.Fig. 14. The average time for task allocation and execution (y axis) decreases as the accuracy of message reception(x axis) increases.Fig. 14 presents the results of simulations where the probability of message receptionvaries between 50 and 100%. The simulations were performed for both homogeneous andheterogeneous ensembles of agents and tasks. In these simulations there is a non-negativeprobability of an agent not receiving information regarding neighboring tasks and agents,although this information was transmitted. The number of agents in both of these sets ofsimulations was 600 and the initial number of tasks was 1200. The other parameters wereas in the previous simulations reported above, except for masses in the heterogeneous case.44O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Fig. 15. The average time for task performance (y axis) in the case of agents and tasks normally distributed aroundthe center of the city. The execution time decreased significantly compared to an even distribution (except for thefirst hour of the simulation).In that case, the initial masses of tasks were set randomly, out of a uniform distribution,between 1 and 100 kg, while the masses of the agents were set randomly (uniformlydistributed) between 80 and 180 kg. If the capacity of an agent was smaller than the sizeof the task, it delivered only part of the task at a time. The freight fulfillment time in theheterogeneous case is the time it takes a whole freight to arrive at its destination.From Fig. 14 we can conclude that the freight fulfillment time increases linearly whenthe probability of messages arrival decreases. However, even when only 50% of messagesarriving, the fulfillment time is better than in the case of 400 messengers with 100%message arrival, as seen in Fig. 13 (but, of course, worse than in the case 600 messengersthere). Our results indicate that unreliable communication has a limited effect on thetime required for task performance. Similar observations apply to other measurementsperformed with heterogeneity and unreliable communication.The previous graphs present results of simulations where tasks and agents are randomlydistributed over the city map. In Fig. 15, where task performance time in seconds (y axis)is measured as a function of the simulation time in hours (x axis), we present the resultsof a case where the distribution is uneven. Agents and tasks are located according to atwo-dimensional normal distribution. The center of the distribution function is the centerof the city map (that is, x D 10000 m and y D 15000 m), and the standard deviation is4000 m. Such distribution means that 2=3 of the tasks and agents are concentrated on 1=4of the area of the city. 52 The result, as can be observed from the graph, is a very significantimprovement in the average task fulfillment time. On the one hand, this should be expected,52 Such distributions are common in cities, where most of the activity may be concentrated around their center.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5545Fig. 16. The average time for task performance (y axis) in the case of agents and tasks randomly distributed, withsettings similar to the normal distribution case (for comparison).since the average distance between messengers and freights has decreased too. On theother hand, one may expect localized concentrations of agents and tasks to result in moreconflicts, which in turn may reduce performance. For several local densities we examined,this is not the case. Hence, the PAS is applicable to non-uniform distributions as well. Theresults in Fig. 15 should be compared to those in Fig. 16. There, results of simulation withthe same settings are presented, except for the distribution, which is random instead ofnormal. In both Figs. 15 and 16 variable masses were implemented.In the simulations, we have added a temperature monitoring and control mechanism.Temperature is a parameter that is measured statistically over time, based on velocitiesof particles. A very low temperature usually results in the system converging to localminima, whereas very high temperatures may result in “hyper-active” agents, which mayin turn cause inefficient task allocation and performance. The mechanism we providedperiodically computes the temperature and if necessary slightly corrects it. This correctionis performed by computing the change required in the average velocity (for reaching thedesired temperature) and adding it to or subtracting it from the velocities of all agents. Wefound out that when the parameters were adequately set, corrections were hardly necessary,temperatures in all experiments were moderate and stable, as in Fig. 17. From these results,and from having all tasks performed, we can conclude that local minima, if reached at all,are scarce.From the results presented above as well as myriad additional experiments we concludethe following:– The PAS model can be applied for use in large-scale agent systems to solve realproblems.46O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55Fig. 17. The temperature (values not calibrated to metric scale) of the system (y axis) as a function of simulationtime (x axis) shows to be moderate and stable. This graph demonstrates it for the case of agents and tasksrandomly distributed with variable masses. Similar results were reached in all other cases.– An increase in the number of agents in the system does not increase the amount ofcomputations per agent. Thus, larger systems do not require more computation time.– An increase in the number of agents in the system, holding the number of tasksconstant, is beneficial only to some extent. Beyond some agents/tasks ratio, nosignificant improvement in performance is observed. We believe this phenomenonresults from redundancy in densely populated agent systems.– The results observed are similar for different densities of the lattice map used as wellas for low probabilities of unreliable communication channels.– The PAS is applicable for varying, non-uniform distributions of masses and locationsof particles (and, respectively, agents and tasks).8. Related workA large body of DAI research studies coordination among agents for distributed problemsolving (for example, [13], PGP [16], GPGP [9,17,63]). In [12], Durfee and Lesserstudy their Partial Global Planning (PGP) approach to coordination by implementing itin the Distributed Vehicle Monitoring Testbed (DVMT). The DVMT is a network ofvehicle monitoring nodes. Each node has a planner that plans incrementally. Nodes donot communicate their detailed actions, but do communicate according to a meta-levelorganization. A PGPlanner modifies local plans as required due to incoming messages.In its incremental planning and restricted communication the PGP model is similar to ourmodel. The DVMT task domain (which was used as a testbed for PGP and GPGP) includestraffic monitoring. This is performed by the agents generating tentative maps for vehiclemovements in their areas. Our transportation framework is different: we require that aO. Shehory et al. / Artificial Intelligence 110 (1999) 1–5547transportation task be attached to agents that plan for it and perform it. Therefore, oursimulated transportation system is significantly different from DVMT.The majority of the simulations performed by Lesser and his colleagues were inenvironments that include only few agents (in many cases 4 agents were considered, andthe maximal number of agents which they tested was, to our knowledge, 25 agents). Thiswas reasonable for the problems which they have considered, where there is a “natural”geographical distribution of the agents, and the problems of integration of sub problemsarise between closely related agents. In dynamic environments where different agentsmay have more freedom to move (either physically or abstractly), the problem of scalingup is more significant. Nevertheless, Lesser et al. also considered problems of scale-upand developed methods to reduce the communication needed to reach globally consistentsolutions. For example, the GPGP model [8] extended the PGP ideas by allowing moreagent heterogeneity, the exchange of more truly partially global information at multiplelevels of abstraction and the use of separate scheduling algorithms. However, GPGPprovides a meta-level for coordinated planning and is not a coordination algorithm initself.Transportation problems were discussed in DAI research previously, e.g., in [19,47].Another example is the DVMT as mentioned above. In [47] self-interested agents are dealtwith, whereas we discuss the case of cooperative agents. Another significant difference isthe size of the problem domain. Sandholm [47] provides a solution for only few agents,albeit dozens of carriers that work on their behalf and hundreds of deliveries. Thus, themulti-agent problem they solve is of small magnitude whereas the scheduling problemsassociated with it are rather large. The transportation framework with which they deal iscomprised of few dispatch centers which are the agents and dozens of carriers that movefreights from these centers, possibly sharing tasks. Task allocation is performed by thedispatch centers and not by the carriers.Fischer et al. [19] assume few dispatch centers as well. However, in their solution eachdispatch center has trucks which are each an autonomous agent. Agents are cooperativewithin their company (the dispatch center), however, competitive with regards to otheragents. Yet somewhat centralized, all deliveries can only start from dispatch centers, andall of the information with regards to deliveries if forwarded to truck agents from thesecenters. In our physics-based model no task forwarding is present (or necessary). Note thatFischer et al. assume a dynamic system, however admit to have carried out experimentsonly with respect to a static one. Another difference of their system from ours is the smallsize of the system. They have simulated up to three dispatch centers, each having up to 20truck agents.Distributed problem solving for large-scale problems (where the size is expressed bythe number of variables involved) was presented in [62], where Yokoo presents the weak-commitment algorithm for distributed constraint satisfaction. The presented approach issignificantly different from ours though it permits scaling-up. The algorithm is not appliedto dynamic task allocation and execution among agents as our algorithm does, however, thisseems to be a possible extension of it. However, although it proves to work efficiently forlarge-scale problems (e.g., 1100 Boolean variables), the algorithm has some undesirableproperties, such as abandoning partial solutions which prove inconsistent and (probablyresulting from the latter), an exponential complexity of the worst case. Even in the average48O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55case the amount of computations increases with the size of the problem solved. In themodel we propose the average complexity for large systems is constant. In addition, insteadof erasing them and starting from scratch, partial solutions evolve via frequent corrections,thus they do not reach a state of being completely inconsistent with constraints. This may,however, limit the solution we provide from being implemented in some domains. Notethat the weak-commitment algorithm concentrates on constraint satisfaction, while ourapproach discusses task allocation. Nonetheless, the physics-based approach we presentmay provide solutions to some constraint satisfaction problems, albeit not necessarily tothese solved by Yokoo.The tileworld model [41] was used as a testbed for planning and task allocationand execution in multi-agent systems. The utilization of physics methods allows fora model that is significantly richer than the tileworld model. While the tileworld is achess-board-like grid with a limited, four-directional moves, the physics-based model weprovide allows, in its very basic physical interpretation, for three-dimensional systems withunlimited directional moves. We believe that with some level of abstraction a physics-basedmodel is further more expressive. The tileworld model distinguishes (at least) two differentprocedures—deliberation and path planning—which are usually performed sequentially,whereas in the physics-based model an inherent property is interleaving planning andexecution. And, while the tileworld proves to work successfully for systems of dozensof tasks and agents (15 agents, 80 tasks in [17]), its computational complexity 53 willprobably disable scaling up to thousands of tasks and agents. Such system size is allowedby the physics based model, as simulations prove.Ephrati et al. [17] suggest the multi-agent filtering strategy as a means for coordinationamong agents. They have conducted several experiments that show, that for the tile-world, this strategy improves the performance of the agents. This coordination is achievedwithout explicit negotiation. In our work we do not suggest a strategy, rather we suggesta method for modeling the goal-agent environment. Based upon this model we suggesta detailed algorithm for the single agent for acting efficiently in the environment. Weprovide an explicit analysis of the quality (with respect to communication and computationconsumption) of our algorithm and conditions in which it is most appropriate. In addition,to support the theoretical analysis, we present simulations results. As in the model ofEphrati et al. [17], our model does not require negotiation. In addition, the amount ofrequired communication in our model may be significantly smaller than in the filteringmethods by Ephrati et al., since in our model agents communicate with a small subset ofthe whole agent community, whereas there agents presumably communicate with all otheragents. 54 This may be of lesser significance when few agents satisfy only few dozens ofgoals (as in [17]), however, is most important for systems of hundreds or thousands ofagents and goals, for which we address our solution.Emergent behavior of computational agents has been discussed in several studies thathave been performed in recent years. Glance and Huberman [23] discussed this issue53 As Kinny and Georgeff [31] explicitly say: “to reduce the complexity...we employed a simplified Tileworldwith no tiles.”54 This issue is not explicitly addressed in the paper, but one can conclude it from the information available toan agent about the others.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5549and borrowed methods from statistical thermodynamics in order to study the evolutionof social cooperation. In the context of social dilemmas, they used this methodology tostudy the aggregate behavior of individuals facing social choices. As in our work, theyapplied results from theoretical physics to study the behavior of a system of individuals.However, unlike us, they used theoretical physics specifically for studying the groupproperties and not for studying the properties of the single, individual agent. They alsodid not develop an algorithm for the behavior of such an agent within the group in whichit was a member. The main concern of their research was the collective behavior, whilewe concentrate on the personal behavior, but still discuss the collective behavior whichresults from this behavior. In another paper [24], Glance and Huberman present a detailedphysical formalism of the dynamics of the collective action of a system of individuals.In our work the main issue is the physical behavior of the single agent. We do notuse physics in order to analyze existing systems. Rather, we develop an algorithm thatis based upon the physical properties, and we rely on the known physical 55 behaviorof particles to predict the behavior of the agents that will use the algorithm we havedeveloped.Shoham and Tennenholtz [53] presented results of simulations that were performed inorder to perceive the emergence of conventions in multi-agent systems. They are concernedwith the design of multi-agent systems that converge towards common social laws. In ourresearch, though we discuss emergent cooperation, we do not discuss the emergence of thelaws according to which the cooperation occurs. Rather, we determine the social laws tobe such (physical laws) that they will cause the emergent cooperation of the system whenthis cooperation is necessary.The pursuit problem [1] has been widely used as an example problem for multi-agentcoordination and cooperation. The problem is of several predator agents attempting tocooperatively hunt a moving prey. Its uniqueness is in the necessity of coordinationamong the predators as well as the dynamic change in the goal location. As Ishida andKorf [26] state, off-line algorithms that compute the entire solution will fail to providean appropriate answer due to the dynamism of the problem. They devise a real-timealgorithm, the moving-target search algorithm (MTS), to overcome these limitations.They describe the problem by a connected graph where each setting of the agents onthe graph is a unique state. The complexity of the algorithm MTS is O.N 3/, whereN is the number of states. When considering multiple agents on a large graph, thiscomplexity prohibits feasible solutions. Another algorithm, based on Q-learning, wassuggested in [40]. There, more agents were involved in the solution however their numberwas only 4 .hunters/ C 1 .prey/ agents. The number of trails to reach a solution wasreduced by the learning mechanism, however, is still relatively high. The algorithms usedfor solving the multi-agent pursuit problem are too complex for problems that consist ofthousands of agents and a large-scale problem space. Our research is aimed at such large-scale systems, and may be found inadequate for too small systems. It must be noted that55 A variety of computer science problems, in general and AI, in particular have been described and solved byphysics-oriented models (e.g., [64]). However, we avoid the description of these studies because we do not findthem similar enough to our case.50O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55in our work we do not explicitly discuss cases in which the goals change their locationsdynamically. 56Another approach to describing group behavior is presented by Mataric. In herpaper [38], she proposes the definition of a set of basic interactions that will allow thesimplification of analysis of group behavior. However, Mataric’s approach differs fromours. While she discusses the description and the synthesis of group behavior, she doesnot provide an explicit set of basic interactions. In our work, we concentrate on the natureof the basic interactions and adopt the physical interactions among particles to model theinteractions among agents and goals.Due to the rapid improvement in the micro-mechanics and microprocessor technologies,a real environment of many simple microscopic autonomous robots is becoming possible.As a result of this progress, as described in the paper of Gage [21], a simple emergentcooperation method among the agents is necessary. In his work, Gage defined a numberof specific classes of desired mobility behaviors for use in military scenarios. He alsoprovided a list of topics that should be considered by designers of such systems, andpresented results of simulations which he performed to illustrate the behavior of suchsystems. Gage’s research differs from ours in several aspects: while he concentrates onspecific classes of motion, we do not restrict our model to either specific classes orexclusively to motion. Gage proves the validity of his methods by performing simulations,whereas we rely on theoretical and experimental physics for proving validity.The path-planning and robot-navigation (PPRN) research 57 have used potential fieldsas a means for planning the path, e.g., in [11,29,30,59]. This approach appears to beclosely related to our research, as we, too, use potential functions. However, there areseveral significant differences between the path-planning and robot-navigation problemand our task-allocation and agent-coordination problem, and the techniques that are usedfor solving these problems. The differences are as follows:(i) In PPRN research, the main objective is planning an optimal path for the robots tonavigate from an initial location to its destination. Our main objective, however, isto solve a task allocation problem with aspects of agent-coordination in a multi-agent environment.(ii) While we discuss the multi-agent case, with potentially thousands of agents, thetype of planning research that is involved with potential functions usually discussesthe single robot case. In cases where the more-than-one robot case is discussed, thenumber of robots is considered very small as opposed to the MAS we discuss.(iii) In cases where the PPRN research addresses the multi-robot planning problem,e.g., in [3,21,38,58], the potential field concept is not employed. In such cases thebehavior of groups and formations of robots are discussed, given a set of specificstrategies according to which the robots act. Among these strategies you may findsome in which robots follow a leader or some predefined geometric patterns. In ourmulti-agent model, however, the agents’ strategy is based on the physical potential-well concept.56 Such dynamics, however, are encapsulated in the physical model, and only marginal modifications will berequired to adjust our model the case of moving goals.57 A comprehensive overview of these can be found in [35].O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5551(iv) Although both the PPRN and our research use potential fields for problemrepresentation and resolution, the type of potential functions and the way of usingthem is different, as follows:– The potential functions employed for path-planning are artificial. The onlyexpected result from such potential functions is robot-motion according to thepotential field gradient. In our research, we employ physics potential functions.This results in a physics-like behavior of the agents that act with respect to thesefunctions. In particular, the use of the potential functions of particles in a non-ionic fluid results in a model that provides the single agent with an algorithm forreaching and performing goals within a large community of agents. In addition,the use of such potential functions enables the prediction of the bulk properties(i.e., the behavior) of the MAS as a whole.– In the PPRN research, an attractive potential field (usually quadratic) isemployed to lead the robot to the goal, and various shapes of repulsive potentialfields are used to cause obstacle-avoidance. In our work, all of the entities—agents as well as goals—(and obstacles, if present) are modeled by the sametype of potential function; i.e., by a physical potential-well.– The potential-wells in our model may change dynamically due to the fulfillmentof goals and the expenditure of resources. This leads to a dynamically alternatingpotential field which results in a dynamic update of the agents’ behavior.Dynamics of the potential field in PPRN research, when present, refer only tothe change in the locations of robots and obstacles, and not to a change in thespecific function that models a specific entity.(v) Another important difference of our research, as compared to the path-planningresearch, is that we do not restrict the model to physical trajectories—the modelcan be used for abstract motion. 58(vi) There are cases in which PPRN employ physics-like concepts and analysis methods(e.g., in [11]). Nonetheless, this selection is not based on a model of an existing,large-scale, physical system from which properties can be inferred, as opposedto our our model selection. However, this artificial choice was later proved, withseveral restrictions, to possess of good properties such as polynomial complexityand near-optimal trajectories 59 [10].In summary, the PPRN research with artificial potential functions discusses cases where asingle robot or a small number of robots must navigate and locate their goals. Our approachis very different: we discuss cases of large-scale MAS, with many agents involved; caseswhere a specific agent does not have a specific goal towards which it must navigate; casesof cooperative goal satisfaction. These are not the aim of the PPRN research and thereforeare not discussed in it.The issue of allocating agents to goals has widely been discussed among DAIresearchers. A well-known model is the Contract Net Protocol [54]. The CNP usesnegotiation based on task announcements, bids and contracts for task allocation. The58 The concept of abstract motion shall be explained in the next section.59 Note that the latter is appropriate for a single robot trajectories. Our model is meant for multiple agents taskallocation.52O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55net consists of dynamically alternating worker and manager nodes, and they exchangeinformation about goals to be performed and subgoals that were already processed.The CNP was not designed based on an existing model, as is our model, therefore theperformance of the system is checked only by simulations. In our model, the performanceof the system is predicted from its physical properties and the efficiency is formallycalculated and compared to the optimal results. The model we present allows (but doesnot require) minimization of the transmitted information 60 and thus enables large-scalesystems to be efficient.A study of planning in large-scale MAS has been presented by Wellman [60]. Inthis research, the general-equilibrium approach from economics serves as the theoreticalbasis for the planning mechanism. Mechanisms in which competition is applied areused to construct a market-oriented programming environment, which is employed as ameans for the construction and analysis of distributed planning systems. In his work,Wellman performed simulations to derive system equilibrium, 61 and showed that giventhe appropriate restrictions, the model reaches near-optimal results (small deviationsfrom the optimal results are described in [60]). As done by Wellman, we also discusslarge-scale systems. We, too, apply an analytical model for designing the distributedplanning mechanism. However, the use of the physics-oriented approach allows us topredict the resulting behavior of the system and that of its constituent agents based on theknown behavior of physical systems. Moreover, while Wellman performed simulations thatincluded 3–20 agents [61] (which we view as a comparatively small MAS), we performedsimulations with the number of agents exceeding 1000. Another major difference is thetype of systems for which the models are appropriate. While Wellman’s model is mostappropriate for self-interested agents, our model was designed for DPS systems, where theagents try to increase the overall outcome of the system.9. ConclusionThe design and analysis of large-scale agent systems imposes difficulties that are hardto solve even when the proposed solutions are of low-order polynomial complexity. In thispaper some aspects of this problem are addressed. Namely, we provide a method for taskallocation and execution in several classes of large-scale cooperative MAS. We present aphysics-oriented approach that results in a very low complexity on the part of the singleagent and may even be of order O.1/. Such results are possible since we use a model whosebehavior is already known. Therefore, we are not required to perform the numerous explicitcalculations that would have otherwise been necessary.The model we have presented and the algorithm that enables the single agent to actaccording to the model consist of methods with which the agents allocate themselvesto goals in order to satisfy the goals. The agent-goal matching is an emergent result ofthe physics-oriented behavior of the agents. According to our model, each agent is most60 Note that this minimization refers to the number of recipients of the information and not necessarily to theamount of information transmitted.61 The definition is provided there.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5553strongly attracted to the goal that it will satisfy with the best fit (within a limited range).In addition, in cases where too many agents fit the requirements of the same goal, ourmodel will disenable some of them from reaching the goal, via the property of mutualrejection. As we have shown, the algorithm that we provide leads to agent-goal allocation,it converges to a solution, the computational complexity is low and communication, ifnecessary, is of a small amount. Our method does not lead to the optimal goal-agentallocation, but reaching an optimal allocation requires complete on-line information aboutall of the agents and goals comprising the system and, for a large class of problems, anexponential computation-time.The physics-oriented approach which we present has several advantages. While commonDAI algorithms must be checked for their validity either by a formal proof or bysimulations, our model can rely on theoretical and experimental results that are alreadyknown from physics. According to these results, we can predict the evolution of themodeled MAS, since it will evolve in the same manner as a corresponding physical system.The local interactions, which enable one to derive the global behavior of the system, assurea low computational complexity of the model. In very large-scale MAS, this approachprovides a model that promises emergent cooperative goal-satisfaction activity. As we haveshown, these properties proved to hold in a simulated system. In addition, the properties ofthe system as a whole can be analyzed using concepts from statistical mechanics. 62 Theemployment of such concepts enables the derivation of the bulk properties of a system viathe properties of its components. We leave this analysis for future work.References[1] M. Benda, V. Jagannathan, R. Dodhiawalla, On optimal cooperation of knowledge sources, Technical ReportBCS-G2010-28, Boeing AI Center, 1985.[2] S. Cammarata, D. McArthur, R. Steeb, Strategies of cooperation in distributed problem solving, in: Proc.IJCAI-83, Karlsruhe, Germany, 1983, pp. 767–770.[3] Q. Chen, J.Y.S. Luh, Coordination and control of a group of mobile robots, in: Proc. 1994 IEEE Internat.Conference on Robotics and Automation, San Diego, CA, 1994, pp. 2315–2320.[4] R.F. Churchhouse, Handbook of Applicable Mathematics—Numerical Methods, Wiley, New York, 1981.[5] S.E. Conry, R.A. Meyer, V.R. Lesser, Multistage negotiation in distributed planning, in: A.H. Bond,L. Gasser (Eds.), Readings in Distributed Artificial Intelligence, Morgan Kaufmann, San Mateo, CA, 1988,pp. 367–384.[6] R. Conte, M. Miceli, C. Castelfranchi, Limits and levels of cooperation: Disentangling various types ofprosocial interaction, in: Y. Demazeau, J.P. Muller (Eds.), Decentralized A.I.—2, Elsevier, Amsterdam,1991, pp. 147–157.[7] T.H. Cormen, C.E. Leiserson, R.L. Rivest, Introduction to Algorithms, MIT Press, Cambridge, MA, 1990.[8] K. Decker, V.R. Lesser, Generalizing the partial global planning algorithm, Internat. J. IntelligentCooperative Information Systems 1 (2), 319–346.[9] K. Decker, V.R. Lesser, Designing a family of coordination algorithms, in: V. Lesser (Ed.), Proc. 1st Internat.Conference on Multi-Agent Systems, San Francisco, CA, MIT Press, Cambridge, MA, 1995, pp. 73–80.Longer version available as UMass CS-TR 94–14.62 The analysis of the bulk properties of particle-systems via statistical mechanics methods can be found in manyintroductory books, e.g., [43]. We demonstrate the usefulness of such methods when measuring the stability ofour system in terms of its temperature.54O. Shehory et al. / Artificial Intelligence 110 (1999) 1–55[10] B. Donald, P. Xavier, Provably good approximation algorithms for optimal kinodynamics planning: Robotswith decoupled dynamic bounds, Algorithmica 14 (6) (1995) 443–479.[11] B. Donald, P. Xavier, J. Canny, J. Reif, Kinodynamics motion planning, J. ACM 40 (5) (1993) 1048–1066.[12] E.H. Durfee, V.R. Lesser, Predictability vs. responsiveness: Coordinating problem solvers in dynamicdomains, in: Proc. AAAI-88, St. Paul, MN, 1988, pp. 66–71.[13] E.H. Durfee, Coordination of Distributed Problem Solvers, Kluwer Academic, Boston, MA, 1988.[14] E.H. Durfee, V.R. Lesser, Negotiating task decomposition and allocation using partial global planning,in: L. Gasser, M.N. Huhns (Eds.), Distributed Artificial Intelligence, Vol. II, Pitman, London/MorganKaufmann, San Mateo, CA, 1989, pp. 229–244.[15] E.H. Durfee, V.R. Lesser, D.D. Corkill, Coherent cooperation among communicating problem solvers, IEEETrans. Comput. 36 (1987) 1275–1291.[16] E.H. Durfee, V.R. Lesser, Partial global planning: A coordination framework for distributed hypothesisformation, IEEE Trans. Systems Man and Cybernet. 21 (5) (1991) 1167–1183. (Special Issue on DistributedSensor Networks.)[17] E. Ephrati, M. Pollack, S. Ur, Deriving multi-agent coordination through filtering strategies, in: Proc. IJCAI-95, Montreal, Quebec, 1995, pp. 679–685.[18] K. Erol, Hierarchical task network planning: Formalization, analysis and implementation, Ph.D. Thesis,University of Maryland, College Park, MD, 1995.[19] B.K. Fischer, J.P. Muller, M. Pischel, A model for cooperative transportation scheduling, in: Proc. ICMAS-95, San Francisco, CA, 1995, pp. 109–116.[20] B.K. Fischer, J.P. Muller, A decision-theoretic model for cooperative transportation scheduling, in: W. Vande Velde, J.W. Perram (Eds.), Agents Breaking Away, Lecture Notes in Artificial Intelligence, Vol. 1038,Springer, Berlin, 1996, pp. 177–189.[21] D.W. Gage, Command control for many-robot systems, Unmanned Systems (1992) 28–34.[22] R. Giles, P. Tamayo, A parallel scalable approach to short-range molecular dynamics on the cm-5, TechnicalReport TMC-234, Thinking Machines Corporation, 1992.[23] N.S. Glance, B.A. Huberman, Organizational fluidity and sustainable cooperation, in: Proc. MAAMAW-93,Neuchâtel, 1993.[24] N.S. Glance, B.A. Huberman, The outbreak of cooperation, J. Math. Sociology 17 (4) (1993) 281–302.[25] P. Gmytrasiewicz, E. Durfee, D. Wehe, The utility of communication in coordinating intelligent agents, in:Proc. AAAI-91, Anaheim, CA, 1991, pp. 166–172.[26] T. Ishida, R. Korf, Moving target search: A real-time search for changing goals, IEEE Trans. Pattern Analysisand Machine Intelligence 17 (6) (1995) 609–619.[27] A. Katz, Principles of Statistical Mechanics, W.H. Freeman, San Francisco, CA, 1967.[28] S.P. Ketchpel, Forming coalitions in the face of uncertain rewards, in: Proc. AAAI-94, Seattle, WA, 1994,pp. 414–419.[29] O. Khatib, Real-time obstacle avoidance for manipulators and mobile robots, Internat. J. Robotics Research5 (1) (1986) 90–98.[30] P. Khosla, R. Volpe, Superquadratic artificial potentials for obstacle avoidance and approach, in: Proc. IEEEInternat. Conference on Robotics and Automation, Philadelphia, PA, 1988, pp. 1778–1784.[31] D.N. Kinny, M.P. Georgeff, Commitment and effectiveness of situated agents, in: Proc. IJCAI-91, Sydney,Australia, 1991, pp. 82–88.[32] J. Koplik, J.R. Banavar, J.F. Willemsen, Molecular dynamics of Poiseuille flow and moving contact lines,Physical Review Letters 60 (13) (1988) 1282–1285.[33] S. Kraus, J. Wilkenfeld, The function of time in cooperative negotiations, in: Proc. AAAI-91, Anaheim, CA,1991, pp. 179–184.[34] L.D. Landau, Mechanics and Electrodynamics, Pergamon Press, 1972.[35] J.C. Latombe, Robot Motion Planning, Kluwer Academic, Dordrecht, Netherlands, 1991.[36] E.L. Lawler, J.K. Lenstra, A.H.G. Rinnooy Kan, D.B. Shmoys (Eds.), The Traveling Salesman Problem,Wiley, New York, 1983.[37] V.R. Lesser, A retrospective view of fa/c distributed problem solving, IEEE Trans. Systems Man Cybernet.21 (6) (1991) 1347–1362.[38] M.J. Mataric, Kin recognition, similarity, and group behavior, in: Proc. 15th Annual Conference of theCognitive Science Society, Boulder, CO, 1993, pp. 705–710.O. Shehory et al. / Artificial Intelligence 110 (1999) 1–5555[39] T. Mullen, M. Wellman, A simple computational market for network information services, in: Proc. 1stInternat. Conference on Multiagent Systems, 1995, pp. 283–289.[40] N. Ono, K. Fukumoto, Multi-agent reinforcement learning: A modular approach, in: Proc. ICMAS-96,Kyoto, Japan, 1996, pp. 252–258.[41] M.E. Pollack, M. Ringuette, Introducing the tileworld: Experimentally evaluating agent architectures, in:Proc. AAAI-90, Boston, MA, 1990, pp. 183–189.[42] D.C. Rapaport, Large-scale molecular dynamics simulation using vector and parallel computers, ComputerPhysics Reports 9 (1) (1988) 1–53.[43] F. Reif, Fundamentals of Statistical and Thermal Physics, McGraw-Hill, New York, 1965.[44] D.J. Rosenkrantz, R.E. Stearns, P.M. Lewis, An analysis of several heuristics for traveling salesman problem,SIAM J. Comput. 6 (1977) 563–581.[45] J. Rosenschein, Rationalinteraction: Cooperation among intelligent agents, Ph.D. Thesis, StanfordUniversity, 1986.[46] G. Salton, M.J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, New York, 1983.[47] T.W. Sandholm, An implementation of the contract net protocol based on marginal cost calculations, in:Proc. AAAI-93, Washington, DC, 1993, pp. 256–262.[48] T.W. Sandholm, V.R. Lesser, Coalitions among computationally bounded agents, Artificial Intelligence 94(1997) 99–137.[49] O. Shehory, Polymer fluid flow simulation using molecular dynamics, M.Sc. Thesis, Bar Ilan University,Ramat Gan, Israel, 1992.[50] O. Shehory, S. Kraus, Cooperation and goal-satisfaction without communication in large-scale agent-systems, in: Proc. ECAI-96, Budapest, Hungary, 1996, pp. 544–548.[51] O. Shehory, S. Kraus, Methods for task allocation via agent coalition formation, Artificial Intelligence 101(1–2) (1998) 165–200.[52] O. Shehory, S. Kraus, O. Yadgar, Goal-satisfaction in large-scale agent-systems: A transportation example,in: Proc. ATAL-98, Paris, France, 1998.[53] Y. Shoham, M. Tennenholtz, Emergent conventions in multi-agent systems: Initial experimental results andobservations, in: Proc. Third Internat. Conf. on the Principles of Knowledge Representation and Reasoning(KR-92), Cambridge, MA, 1992, pp. 225–231.[54] R.G. Smith, The contract net protocol: high-level communication and control in a distributed problem solver,IEEE Trans. Comput. 29 (12) (1980) 1104–1113.[55] K. Sycara, K. Decker, A. Pannu, M. Williamson, Designing behaviors for information agents, in: Proc.Agents-97, Los Angeles, CA, 1997, pp. 404–412.[56] K. Sycara, K. Decker, A. Pannu, M. Williamson, D. Zeng, Distributed intelligent agents, IEEE Expert—Intelligent Systems and Their Applications 11 (6) (1996) 36–45.[57] C. Trozzi, G. Ciccotti, Stationary nonequilibrium states by molecular dynamics. II. Newton’s law, PhysicalReview A 29 (2) (1984) 916–925.[58] P. Wang, Navigation strategies for multiple autonomous robots moving in formation, J. Robotic Systems 8(2) (1991) 177–195.[59] C.W. Warren, Global path planning using artificial potential fields, in: Proc. 1989 IEEE Internat. Conferenceon Robotics and Automation, Scottsdale, AZ, 1989, pp. 316–321.[60] M.P. Wellman, A market-oriented programming environment and its application to distributed multicom-modity flow problems, J. Artificial Intelligence Research 1 (1993) 1–23.[61] M.P. Wellman, Market-oriented programming: Some early lessons, in: S. Clearwater (Ed.), Market-BasedControl: A Paradigm for Distributed Resource Allocation, 1995.[62] M. Yokoo, Weak-commitment search fro solving constraint satisfaction problems, in: Proc. AAAI-94,Seattle, WA, 1994, pp. 313–318.[63] M. Yokoo, E.H. Durfee, T. Ishida, K. Kuwabara, Distributed constraint satisfaction for formalizingdistributed problem solving, in: Proc. 12th International Conference on Distributed Computing Systems,1992, pp. 614–621.[64] A.L. Yuille, Generalized deformable models, statistical physics, and matching problems, Neural Computa-tion 2 (1990) 1–24.