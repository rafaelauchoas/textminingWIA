Artificial Intelligence 94 (1997) 57-77 Artificial Intelligence Rationality and intelligence Stuart J. Russell 1 Computer Science Division, University of California, Berkeley, CA 94720, USA Abstract The long-term goal of our field is the creation and understanding of intelligence. Productive in AI, both practical and theoretical, benefits from a notion of intelligence that is precise to allow the cumulative development of robust systems and general results. The concept of to fulfill this role. This paper outlines to our informal that brings theory and practice. Some research enough rational agency has long been considered a leading candidate in the formal conception of rationality a gradual evolution conception of intelligence directions for future research are indicated. @ 1997 Elsevier Science B.V. and simultaneously the gap between it closer reduces Keywords: Philosophical foundations; Intelligence; Rationality; Bounded rationality; Bounded optimality 1. Artificial intelligence AI is a field whose ultimate goal has often been somewhat dispute. Some researchers of intelli,gence without concern useful artifacts without concern to aim to emulate human cognition, others aim at the creation and still others aim to create and subject ill-defined for human characteristics, for abstract notions of intelligence. This variety and provides abhor a definitional feasibility not subscribe. is not necessarily fertilization a bad thing, since each approach uncovers new ideas that, since philosophers to the others. But one can argue vacuum, many of the damaging and ill-informed the of AI to which we as AI researchers do debates about of AI have been about definitions My own motivation for studying AI is to create and understand general property of systems, to be an appropriate goal for the field as a whole, and it certainly of useful artifacts-both as a spin-off and as a focus and driving rather than as a specific attribute of humans. intelligence as a this the creation force for technological I believe includes ’ Email: russell@cs.berkeley.edu. 0004-3702/97/$17.00 @ 1997 Elsevier Science B.V. All rights reserved. PII SOOOG-3702 (97) 00026-X 58 S.J. Russell/Artificial Intelligence 94 (1997) 57-77 The difficulty with this “creation of intelligence” view, however, is that that we have some productive notion of what intelligence can say “Look, my model correctly predicted this experimental but few of us are happy with papers saying “Look, my system can say “Look, my system is. Cognitive observation is saving is cognition”, and artifact developers development. it presupposes scientists of human lives/megabucks”, intelligent”. This difficulty to allow us to design complex of others. “Intelligent” must be given a definition system’s AI subsides vehicle control, into a smorgasbord of fields-intelligence is compounded intelligence systems with confidence and further by the need for theoretical scaffolding to build on the results to the that can be related directly input, structure, and output. Such a definition must also be general. Otherwise, as chess playing, intelligence as as medical diagnosis. the development I shall examine to characterize systems each definition of such definitions over the history as a predicate P that that are intelligent. For each P, I and at least the is P” is interesting development to which the statement “Look, my system true, and the sort of research and technological In this paper, I shall outline supposedly, of AI and related disciplines. can be applied, shall discuss whether sometimes study of P-systems leads. I shall begin with successful behaviour-the definitions of intelligence the idea that intelligence so-called “agent-based” are as follows: is strongly related view of AI. The candidates to the capacity for for formal l PI : Pelfect rationality, or the capacity to generate maximally successful behaviour given the available information. l P2: Calculative rationality, or the in-principle to compute the perfectly rational decision given the initially available capacity information. l P3: Metakvel rationality, or the capacity computation-sequence-plus-action, selected by the computation. under to select the constraint the optimal that combination of the action must be l P4: Bounded optimality, or the capacity successful behaviour given the available information to generate maximally resources. and computational All four definitions will be fleshed out in detail, and I will describe some results have been obtained work under lines. Then I will describe ongoing rationality these the headings of calculative and bounded optimality. so far along that and future results technical a suitable a condition increasingly bounded optimality is always a danger, I shall be arguing that, of these candidates, comes closest to in this sort of claim, characterized the original Is research on bounded the needs of AI research. There the case of AI, the problem of creating can lead to “premature mathematization”, that have meeting that its acceptance by increasingly problem-in optimality bounded optimality, with real and desirable about the nature of intelligence. Some important questions about intelligence formulated thereof. Only time will tell, however, whether bounded optimality additional practical progress it is a real problem intuitions can only be the framework of bounded optimality or some relative research, perhaps with to support significant is more suitable solutions, for research on intelligence? than PI through P3 because little intelligence. it satisfies some essential and answered within can generate enough and also because to do with refinements, scaffolding theoretical that P4, to show stand-in I hope in AI. S.J. Russell/Art$cial Intelligence 94 (1997) 57-77 59 2. Agents recently, it was common fairly faculties” Until “mental systems”, that. This does not provide much guidance. lem of designing systems “right”. or “intelligent study of to define AI as the computational catalogue it at various kinds, and Instead, one can define AI as the prob- for leave that do the right thing. Now we just need a definition This approach involves considering the intelligent and acts upon entity as an agent, it. Formally that is to say is actions to actions instantiates. that senses that the agent its environment speaking, an agent from percept sequences the agent can carry out in the external world a system defined by the mapping Let 0 be the set of percepts set of possible action of doing nothing). Thus the agent function behaves under all circumstances. What counts in the first instance not necessarily what it thinks, or even whether consider reason as occurring “cognitive the right rather systems can do the right thing without such cognitive more subsystems. that the agent can observe at any instant, and A be the the : 0* -+ A defines how an agent is what the agent does, to -further constraints on the internal workings of the agent (such as that it should it allows us to view such in the service of finding that it allows of and reasoning it encompasses than excludes faculties it thinks at all. This to consider various and interconnections in three ways: to do; second, for example) specifications, [ 1,4] ; third, the position as planning boundaries, (including faculties” logically, freedom refusal initial helps thing first, f The agent-based and “embeddedness” Rational agents, of view of the information it was designed). Rationality it does c:onstrain-the emphasized dural rationality make and Thought the fact that reasoning not, be a good cognitive lem. to describe by Simon structures to mainstream view of AI has moved quickly from workshops on “situatedness” in Newsweek. loosely speaking, are agents whose actions make sense from the point possessed by the agent and its goals (or the task for which is a property of actions and does not specify-although [ 10,391 and buzzwords textbooks process by which [ 461, who coined the difference between the question of how to make lecture was titled “Intelligence without Reason” is (perhaps) a derived property of agents implementation scheme that many AI researchers th& question of what decision it. That Rod Brooks’ 1991 Computers the actions are selected. This was a point rationality and proce- the terms substantive to and [5] ) emphasizes that might, or might the Justifying is not an easy prob- rational behaviour. take for granted (see also to achieve One other consequence of the agent-based view of intelligence from other fields that have traditionally looked on the embedded is that it opens AI up agent topic of study. Control theory is foremost among and indeed evolutionary biology itself also have these, but evolutionary to contribute.* ideas to competition as a natural programming ‘1 view lhis as a very positive development. AI is a field defined by its problems, not its methods. insights-among the learning, use, and compilation of explicit knowledge from other fields. This is especially Its in the service of true certainly withstand the influx of new methods principal decision making-can when other, fields are simultaneously them embracing the insights derived within AI. 60 S.J. Russell/Artificial Intelligence 94 (1997) 57-77 Percept history State history \Ir Value Fig. 1. The agent receives percepts environment value of the agent. to generate a state history. The performance measure evaluates the state history from the environment and generates a behaviour which in turn causes the to arrive at the The prevalence problems, fragility of a subsystem and interpreting its outputs. of the agent view has also helped avoiding what Brooks calls the “hallucination” the field move problem towards solving that arises when is masked by having an intelligent human providing input real the to it 3. Perfect rationality to provide Perfect rationality an agent’s actions constrains the information inputs available. We can expand to the definition of success given Fig. 1) . The fundamental the agent is to operate and the performance measure U which evaluates states through which the agent drives the actual environment. Let V( f, E, U) denote expected value according where (for now) we will assume a probability perfectly expectation (see class E in which the sequence of the class E, over elements of E. Then a rational agent is defined by an agent function to U obtained by an agent function this notion as follows are the environment f in environment fopt such that the maximum distribution fopt = argmaxf V( f, E, W . is just a fancy way of saying rational behaviour that the best agent does the best it can. The point I is a well-defined is addressed function of E and U, which this function the tusk environment. The problem of computing This is that perfectly will call below. The theoretical role of perfect [35]. Knowledge-level rationality within AI is well-described by Newell’s relies It can be used to establish an upper bound on rational agent analysis of AI systems of any possible system, by establishing what a perfectly paper on the Knowledge Level on an assumption the performance would do given the same knowledge. of perfect rationality. S.J. Russell/Artijicial Intelligence 94 (1997) 57-77 61 Although the knowledge that a perfectly rational agent has determines the actions that it will take given its goals, the question of where the knowledge comes from is not well understood. That is, we need to understand rational learning as well as rational action. In the logical view of rationality, learning has received almost no attention-indeed, Newell’s analysis precludes learning at the knowledge level. In the decision-theoretic view, Bayesian updating provides a model for rational learning, but this pushes the question back to the prior [6]. The question of rational priors, p~ticul~ly for expressive representation languages, remains unsettled. Another aspect of perfect rationality that is lacking is the development of a suitable body of techniques for the specification of utility functions. In economics, many results have been derived on the decomposition of overall utility into attributes that can be combined in various ways [ 261, yet such methods have made few inroads into AI (but see [2,50] ). We also have little idea how to specify utility over time, and although the question has been raised often, we do not have a satisfactory understanding of the relationship between goals and utility. The good thing about perfectly rational agents is that if you have one handy, you prefer it to any other agent. Furthermore, if you are an economist you can prove nice results about economies populate by them; and if you want to design dis~ibut~ intelligent systems, assuming perfect rationality on the part of each agent makes the design of the interaction mechanisms much easier. The bad thing is that the theory of perfect rationality does not provide for the analysis of the internal design of the agent: one perfectly rational agent is as good as another. The really bad thing, as pointed out by Simon, is that perfectly rational agents do not exist. Physical m~h~isms take time to process information and select actions, hence the behaviour of real agents cannot immediately reflect changes in the environment and will generally be suboptimal. 4. Cakulati7e ration~ity Before discussing calculative rationality, it is necessary to introduce a distinction between the agent function and the agent program. In AI, an agent is implemented as a program, which I shall call 1, running on a machine, which I shall call M. An agent program receives as input the current percept, but also has internal state that reflects, in some form, the previous percepts. It outputs actions when they have been selected. From the outside, the behaviour of the agent consists of the selected actions interspersed with inaction (or whatever default actions the machine generates). Calculative rationality is displayed by programs that, if executed infinitely fast, would result in perfectly rational behaviour. Unlike perfect rationality, calculative rationality is a requirement that can be fulfills by many real programs. Also unlike perfect rationality, calculative rationality is not necessarily a desirable property. For example, a calculatively rational chess program will choose the “right” move, but may take 16’ times too long to do so. The pursuit of calculative rationality has nonetheless been the main activity of theo- to is, capa- retically well-founded research in AI. In the early stages of the field, it was impo~~t concentrate on “epistemological adequacy” before “heuristic adequacy”-that 62 S.J. Russell/Artificial Intelligence 94 (1997) 57-77 satisfy calculus, tradition, traditions. in principle that achieve In the logical the logical and In the decision-theoretic rationality has been the mainstay rather than in practice. 3 Calculative the decision-theoretic the the specified goal in all cases and [ 351 defines rational actions as those that are guaranteed systems, such as theorem-provers rationality under bility of both performance measure accepts behaviours rejects any others. Thus Newell to achieve one of the agent’s goals. Logical planning this defi- using situation rational agents has nition. theory [27]. largely gone on outside AI-for than senten- Representations tial) and solvable problems have been either very small or very specialized. Within AI, networks or belief networks has opened up many new the development in rep- possibilities networks resentational with action and value nodes added) satisfy version of calculative rationality. for agent design, providing complexity. Systems based on influence diagrams the decision-theoretic in stochastic optimal control rather of calculative the design of calculatively have usually been very impoverished the conditions tradition, in many cases an exponential of probabilistic (probabilistic (state-based reduction example, In practice, neither the logical nor the decision-theoretic traditions can avoid the of the decision problems posed by the requirement of calculative is to rule out sources of exponential intractability One response reasoning we ignore the little matter of polynomial-time and Thought in two fascinating Computers [ 30,3 l] and by Henry Kautz in 1989. The accompanying sublanguages issue rather expressiveness was strongly opposed by Doyle and Patil it also restricts under such constraints. 4 are perhaps best seen as indications of the representation than as a solution the applicability complexity in the representations rationality. and least, if computation. This position was expounded lectures given by Hector Levesque in 1985 results on tractable of where complexity may be an research [ 141, who pointed out that services designed and inference tasks addressed, so that calculative and perfect rationality coincide-at to the problem of complexity. The idea of restricting is to constrain In the area of distributed AI, the system designer has control over that part of each that involves negotiations with other agents. Thus, one possible way so that optimal decisions to ensure [ 151. Of in optimal behaviour by the ensemble in interacting with the rest of the agent’s environment to control complexity can be made easily. For example, that the best policy for each agent is simply course, of agents; nor does it solve the problem of complexity environment. the negotiation problem the Clarke Tax mechanism this approach does not necessarily can be used truthfully to state its preferences result The most common and approximations powerful armoury of methods state representations response techniques to complexity has been to use various speedup in the hope of getting reasonable behaviour. AI has developed a very of the decomposition of environment models form; sparse representations for reducing complexity, into sentential including 3 Perhaps not coincidentally, this decision was taken before the question of computational intractability was properly understood in computer science. 4 Doyle and Patil propose instead “should be designed “the costs and benefits based on rational metareasoning, [of computations] to offer a broad mix of services varying the idea of “rational management systems of inference”. Representation in cost and quality” and should take into account as perceived by the system’s user”. That is, they suggest a solution as discussed in Section 5. learning; compilation approximate, (as in STRIPS operators); ning and abstraction; reinforcement application of metalevel control. Although and are effective of optimality is inevitable that intelligent This observation that select suboptimal better theory has been a commonplace to understand actions them. solution decomposition methods such as partial-order parameterized representations (chunking, macro-operators, of value functions EBL, etc.); and plan- for the for moderately agents will be unable some of these methods can retain guarantees large problems that are well structured, it to act rationally in all circumstances. fall outside calculative since the very beginning of AI. Yet systems rationality per se, and we need a 5. Metalrevel rationality Metalevel ration~ity, tradeoff. Metalevel of expected utility this that divides tradeoff between computational taking into account deliberation costs-it that the aim was to take advantage of some sort of ~eta~~e~ architecture to also called Type II rationaIity by I.J. Good [ IS], is based on the costs and decision quality. defines idea of finding an optimal Although Good never made his concept of Type II rationality very precise-he it as “the maximization is clear implement agents out computations results of physical metalevel is a second decision-making the object-level they affect. Met~e~oning 1970s (see and pruning pursuing particular object-level for intelligent the agent into two (or more) notional parts. The object level carries the concerned with the application domain-for example, projecting states, and so on. The domain consists of objects and states that in AI, going back at least to the early search methods of [ 421 for historical details). One can also view selective strategies as embodying metalevel expertise concerning search operations. the utility of certain process whose application and the computational is a design philosophy has a long history the desirability computations architecture themselves computing actions, the intuition computations the decision-theoretic The theory of rationaE metareasoning formalizes the theory of information value [ 23]-the that the metalevel can are actions with (improvements in decision quality). A rational to their expected utility. Rational metareason- “do the right thinking.” The basic idea is that object-level costs (the passage of time) and benefits mctalevel according selects computations ing has as a precursor calculate simulating the information averaged to information-gathering, [ 20,211, Breese and Fehling the idea of value of computation making. that one can by value of acquiring an additional piece of information that would be followed given each possible outcome of in decision quality processes, by analogy [ 321. In AI, Horvitz all showed how could solve the basic problems of real-time decision seems to have originated with Matheson [ 31, and Russell and Wefald lover those outcomes. The application the decision process thereby estimating to computational the expected improvement [ 4 l-431 request, notion The work done with Eric Wefald looked in particular at search algorithms, in which the extend projections of the results of various courses of actions computation in chess programs, each object-level object-level further expands a leaf node of the game computations the future. For example, into tree. The metalevel problem is then to select nodes 64 S.J. Russell/Artificial Intelligence 94 (1997) 57-77 for expansion and to terminate search at the appropriate point. The principal problem with metareasoning in such systems is that the local effects of the computations do not directly translate into improved decisions, because there is also a complex process of propagating the local effects at the leaf back to the root and the move choice. It turns out that a general formula for the value of compu~tion can be found in terms of the “local effects” and the “propagation function”, . such that the formula can be instantiated for any particular object-level system (such as minimax propagation), compiled, and executed efficiently at runtime. This method was implemented for two-player games, two-player games with chance nodes, and single-agent search. In each case, the same general metareasoning scheme resulted in efficiency improvements of roughly an order of magnitude over traditional, highly-engineered algorithms. Another general class of met~~soning problems arises with o~ytime [ 111 or~~i~~e [20] algorithms, which are algorithms designed to return results whose quality varies with the amount of time allocated to computation. The simplest type of metareasoning trades off the expected increase in decision quality for a single algorithm, as measured by a performance projile, against the cost of time [45]. A greedy termination condition is optimal if the second derivative of the performance profile is negative. More complex problems arise if one wishes to build complex real-time systems from anytime compo- nents. First, one has to ensure the inte~rffptibizi~ of the composed system-that is, to ensure that the system as a whole can respond robustly to immediate demands for output. The solution is to interieave the execution of all the components, allocating time to each component so that the total time for each complete iterative improvement cycle of the system doubles at each iteration. In this way, we can construct a complex system that can handle arbitrary and unexpected real-time demands exactly as if it knew the exact time available in advance, with just a small ( < 4) constant factor penalty in speed [ 441. Second, one has to allocate the available computation optimalIy among the components to maximize the total output quality. Although this is NP-hard for the general case, it can be solved in time linear in program size when the call graph of the components is tree-structured [ 521. Although these results are derived in the relatively clean context of anytime algorithms with well-defined performance profiles, there is reason to expect that the general problem of robust real-time decision-making in complex systems can be handled in practice. and met~e~oning. TEBRESIAS Over the last few years, an interesting debate has emerged conceding the nature [ 91 established the idea that explicit, of met~nowl~ge domain-specific metaknowledge was an important aspect of expert system creation. Thus, metaknowledge is a sort of “extra” domain knowledge, over and above the object-level domain knowledge, that one has to add to an AI system to get it to work well. On the other hand, in the work on rational metareasoning described above, it is clear that [ 17,421. the metatheory describing In principle, no additional domain knowledge is needed to assess the benefits of a computation. In practice, met~easoning from first principles can be very expensive, To avoid this, the results of metalevel analysis for particular domains can be compiled into domain-specific metaknowledge, or such knowledge can be learned directly from experience (see [42, Chapter 61 and [ 341). This view of emerging “computational that there is an expertise” leads to a fundamental insight into intelligence-namely, the effects of compartations is domain-independent S.J. Russell/Art@cial Intelligence 94 (1997) 57-77 65 sense imagine interesting one can properties of the environment my mind., this way of thinking AI is incredibly devious and superbly efficient algorithms efforts of computer get there? in which algorithms are not a necessary part of AI systems. Instead, interacting with a general process of rationally guided computation to produce more and more efficient decision making. To for finesses one major puzzle of AI: if what is required the current best ever scientists, how did evolution (and how will machine far surpassing learning) remain sequences search at the metalevel. Obviously, to date have adopted a myopic strategy-a in the area of rational metareasoning. One obvi- open problems is that almost all systems is at least as intractable sequences must be considered because Significant ous difficulty greedy, depth-one of computation lem. Nonetheless, computation may not be apparent as an improvement computations ing could be effective, especially improvement controllers possible more complex systems such as abstraction hierarchy planners, hybrid architectures, so on. the problem of optimal selection object-level prob- in some cases the value of a in decision quality until further learn- from reinforcement is, the to the metalevel post hoc. Other for and as the “reward function” easily available the creation of effective metalevel in decision quality-is include have been done. This suggests for computation-that areas for research as the underlying that techniques seems to be a useful tool in coping with complexity, agents as a formal framework it it- is expensive, rationality has repeated is that, since metareasoning for resource-bounded Although at the metalevel is useless. Therefore, rational me&reasoning rationality the concept of metalevel does not seem to hold water. The reason cannot be carried out optimally. The history of object-level self at the metalevel: perfect rationality rationality at the metalevel made for metalevel computations, tioned above. Within to identify to do so via a metametalevel entirely possible metareasoning suggest micromanaging in bounded optimality. the framework of metalevel that in some environments, that the right approach at all, but will simply is to step outside the appropriate the individual respond results a time/optimality tradeoff has to be as for example with the myopic approximation men- there is no way rationality, however, tradeoff of time for metalevel decision quality. Any attempt in a conceptual it is the most effective agent design will do no regress. Furthermore, simply to circumstances. These considerations the agent, as it were; to refrain decisions made by the agent. This is the approach from taken is unattainable and calculative 6. Bounded optimality The difficulties with perfect rationality and metalevel (actions, computations) that actions or computations be rational sition of constraints on things directly control. Specifying real agems can fulfill notion of feasibility functions This is somewhat analogous that can be implemented the specification. The designer controls for a given machine is introduced by some agent program rationality arise from the impo- that the agent designer does not is of no use if no the In [40], the set of all agent running on that machine. the program. to describe to the idea of computability, but is much stricter because it 66 S.J. R~seli/~rti~c~al Intelligence 94 fI 997) 57-77 relates the operation of a program on a formal machine model with finite speed to the actual temporal behaviour generated by the agent. Given this view, one is led i~ediately to the idea that optimal feasible behaviour is an interesting notion, and to the idea of finding the program that generates it. Suppose we define Agent( 1, M) to be the agent function implemented by the program I running on machine M. Then the bounded optimal program lopt is defined by 1 *Pt = afgmqECM VtAgent(l,~),E,V), where L:M is the finite set of all programs that can be run on M. This is P4, bounded optimality. In AI, the idea of bounded optimality floated around among several discussion groups interested in the general topic of resource-bounded rationality in the late 198Os, par- ticularly those at Rockwell (organized by Michael Fehling) and Stanford (organized by Michael Bratman). The term “bounded optimality” seems to have been originated by Eric Horvitz [21], who defined it informally as “the optimization of computa- tional utility given a set of assumptions about expected problems and constraints on resources”. Similar ideas have also surfaced recently in game theory, where there has been a shift from consideration of optimal decisions in games to a consideration of optimal decision-making programs. This leads to different results because it limits the ability of each agent to do unlimited simulation of the other, who is also doing unlimit~ simulation of the first, and so on. Even the requirement of computability makes a significant difference [33]. Bounds on the complexity of players have also become a topic of intense interest. Papadimitriou and Yannakakis [36] have shown that a collaborative equilib~um exists for the iterated Prisoner’s Dilemma game if each agent is a finite automaton with a number of states that is less than exponential in the number of rounds. This is essentially a bounded optimality result, where the bound is on space rather than speed of computation. Philosophy has also seen a gradual evolution in the definition of rationality. There has been a shift from consideration of act utilitarianism-the rationality of individual acts-to rule utilitun’anism, or the rationality of general policies for acting. The require- ment that policies be feasible for limited agents was discussed extensively by Cherniak [ 81 and Harman [ 191. A philosophical proposal generally consistent with the notion of bounded optimality can be found in the “Moral First Aid Manual” [ 131. Dennett explicitly discusses the idea of reaching an optimum within the space of feasible deci- sion procedures, using as an example the Ph.D. admissions procedure of a philosophy department. He points out that the bounded optimal admissions procedure may be some- fact, the admissions what messy and may have no obvious hallmark of “optimality”-in com~tt~ may continue to tinker with it since bounded optimal systems may have no way to recognize their own bounded optimality. In work with Devika Subramanian, the general idea of bounded optimality has been placed in a formal setting so that one can begin to derive rigorous results on bounded optimal programs. This involves setting up completely specified rel~onships among agents, programs, machines, environments, and time. We found this to be a very valu- able exercise in itself. For example, the “folk AI” notions of “real-time environments” S.J. RusselUArtQicial Intelligence 94 (1997) 57-77 67 and ‘deadlines” ended up with definitions rather different than those we had initially imagined. From this foundation, a very simple machine architecture was investigated in which the program consists of decision procedures of fixed execution time and decision quality. In a “stochastic deadline” environment, it turns out that the utility attained by running several procedures in sequence until interrupted is often higher than that attain- able by any single decision procedure. That is, it is often better first to prepare a “quick and dirty” answer before emb~king on more involved calculations in case the latter do not finish in time. The interesting aspect of these results, beyond their value as a demonstration of non- trivial proofs of bounded optimality, is that they exhibit in a simple way what I believe to be a major feature of bounded optimal agents: the fact that the pressure towards op- timality within a finite machine results in more complex program structures. Intuitively, in a complex environment requires a software architecture that efficient d~ision-~ng offers a wide variety of possible computational options, so that in most situations the agent has at least some computations available that provide a significant increase in decision quality. One possible objection to the basic model of bounded optimality outlined above is that solutions are not robust with respect to small v~iations in the environment or the machine. This in turn would lead to difficulties in analysing complex system designs. Theoretical computer science faced the same problem in describing the running time of algorithms, because counting steps and describing instruction sets exactly gives the same kind of fragile results on optimal algorithms. The 00 notation was developed to deal with this and provides a much more robust way to describe complexity that is indepen- dent of machine speeds and implementation details. This robustness is also essential in allowing complexity results to develop cumulatively. In [ 401, the corresponding notion is asymptotic bounded optimality (ABO) . As with classical complexity, we can define both average-case and worst-case ABO, where “case” here means the environment. For example, worst-case ABO is defined as follows: Worst-case asymptotic bounded optimality. An agent program 1 is timewise (or space- wise) worst-case ABO in E on M iff 3k,no Yl’,n n > no + V*(Agent(l, kM), E,U,n) 2 V*(Agent(l’,M), E, U,n) where kM denotes a version of M speeded up by a factor k (or with k times more memory) and V*( f, E, V, n) is the minimum value of V( f, E, U) for all E in E of complexity n. In English, this means that the program is basically along the right lines if it just needs a faster (larger) machine to have worst-case behaviour as good as that of any other program in all environments. Another possible objection to the idea of bounded optimality is that it simply shifts the intractable computational burden of metalevel rationality from the agent’s metalevel to the designer’s object level. Surely, one might argue, the designer now has to solve offline all the metalevel opti~zation problems that were intractable when online. This it would be surprising if the agent design prob- argument is not without merit-indeed, 68 S.J. Russell/Art$cial Inrelligence 94 (1997) 57-77 is presumably is however, a significant in that the agent designer the putative metalevel agent is working the two difference between lem turns out to be easy. There creating an agent for an entire class problems, of environments, whereas in a specific environ- ment. That this can make the problem easier for the designer can be seen by considering indeed the example of sorting algorithms. trillion elements, but it is relatively easy to design an asymptotically for sorting. would still hold for BO as well as ABO design, but the ABO definitions make it a good deal clearer. to sort a list of a optimal algorithm of the two tasks are unrelated. The unrelatedness It may be very difficult In fact, the difficulties It can be shown easily that worst-case ABO is a generalization a “classical environment” of asymptotically in which classical is a decreasing function of runtime if the output simply by constructing environments may need optimal algorithms, algorithms operate and in which the utility of the algorithm’s behaviour positive more general multiple outputs over abstraction, ABO programs doubling construction discussed below. that are ABO for any time variation time, and so on. As an illustration to trade off output quality in for time, generate is a useful one can construct universal the research are is correct and zero otherwise. Agents [ 441. Further directions for bounded optimality that under certain function, using one can show of how ABO in the utility restrictions from 7. What is to he done? This section describes bounded optimality describe work on calculatively the space of agent programs. into a creative some of the research activities tool for AI system design. First, however, that will, I hope, help to turn I shall that needs to be done in order to enrich rational systems 7.1. Components for calculative rationality for a rational agent depends on the task As mentioned the correct design above, “physical” environment to define some basic properties of task environments and the performance measure on environment that, together requirements on the correspond- [ 39, Chapter 21. The principal properties agent designs is fully observable or partially observable, whether the or it is static (i.e., does not change except when the agent acts) or dy- serve it is discrete or continuous. Although crude, these distinctions in AI. By analysing and solving each subcase and are whether it is deterministic It is possible lead to identifiable environment-the histories. with the complexity of the problem, ing rational environment stochastic, whether namic, and whether to lay out an agenda for basic research producing can produce can build are currently missing. Others are so fragile and non-scalable support impact. their own weight. This presents many opportunities the AI equivalent the equivalent calculatively rational mechanisms with the required properties, theoreticians of bricks, beams, and mortar with which AI architects of cathedrals. Unfortunately, many of the basic components as to be barely able for research of far-reaching to S.J. Russell/Art@cial Intelligence 94 (1997) 57-77 69 tradition of goal-based plans, agent design, based on the creation and execution The logicist in fully observable, static, and of guaranteed discrete tasks are usually specified as logically defined goals rather than general utility functions.) This means that agents need keep no internal state and can even execute plans without is firmly anchored (Furthermore, the use of perception. task environments. deterministic, The theory of optimal action the heading of POMDPs in stochastic, partially observable (Partially Observable Markov Decision Problems), environments goes a first addressed in AI until recently in the work of Sondik under [47] but almost completely class of problems nature unknown has been done in AI on dynamic environments, which require real-time decision making, environments, which have been largely the province of geometry-based or on continuous robotics. are partially observable, nondeterministic, dynamic, !Since most real-world and continuous, [ 71. Similarly, very little work of a fundamental is somewhat surprising. the lack of emphasis applications There are, however, several new bricks under construction. For example, dynamic [ 121 provide a mechanism state of a dynamic, partially observable, nondeterministic forward the effects of actions. Also, the rapid improvement (DPNs) networks ‘of computer probabilistic current project accuracy vision environments more practical. widely used technique objects; DPNs extend Kalman state. Reinforcement learning, function in continuous, nondeterministic others, have had some success Finally, environments. with hidden variables to acquire representations to maintain beliefs about the and to environment, in the speed and physical [24], a filtering systems has made In particular, the application of Kalman interfacing with continuous filtering together with inductive in control theory, allows robust and efficient to allow more general tracking of moving of world representations learning methods such as neural networks, allow learning for continuous rewards [ 371, among to partially observable networks learning methods (i.e., for partially observable environments) may make it possible reinforcement for static and dynamic probabilistic environments. Recently, Parr and Russell from delayed in applying learning the necessary environment models [ 29,381. (a.k.a. BATmobile) project relevant variables lanes to the right) [ 161 is an attempt that are not observable, The Bayesian Automated Taxi this can be viewed as a POMDP because to combine all these new bricks to solve an interesting application problem, namely driving a car on a freeway. Technically, the environment (such as whether or not the Volvo on your left is intending contains the behaviour of other to change In a POMDP, vehicles and the effects of one’s own actions are not exactly predictable. the optimal decision depends on the joint probability over the entire set of of real-time vision algorithms, Kalman state variables. filtering, and dynamic probabilistic the required distribution when a stream of traffic on a freeway. The BATmobile currently uses a hand-coded observing (although decision probably with lookahead methods as well as supervised learning tree far from optimal) on our simulator. We are currently It turns out that a combination to make approximately is a fairly safe driver networks can maintain on this basis, and to make decisions rational decisions, learning methods. and reinforcement experimenting and because distribution As well as extending uncertainty With logical planners, significantly the scope of AI applications, the opportunity increase under to make a difference. a plan either does or does not work; it has proved very difficult for metareasoning for planning new bricks 70 S.J. RusseWArtijicial Intelligence 94 (1997) 57-77 to find heuristics success, or to estimate concrete are likely utility have built-in from hopeless high priority. and abstraction to measure the likelihood that an abstract the “goodness” of a logical plan that does not guarantee logical plan will have a successful steps that and that it is very hard to identify plan elaboration to handle uncertainty to have high value. In contrast, planners designed instance. This means information about the likelihood of success and there is a continuum to perfect plans. Getting metareasoning to work for such systems is a It is also important that have been so effective to apply those methods such as partial-order planning the reach of classical planners. in extending interesting more and the issues 7.2. Directions for bounded optima@ Ongoing research on bounded optimality aims to extend the initial results of [40] to agent designs. In this section, I will sketch some design dimensions in establishing to be followed involved The general scheme bounded optimality results. involves defining a virtual machine M that runs from a class CM. Typically, programs will have a “fixed part” that is shared program. for the any feedfor- and we might be interested topologies, while within each in the weights on the links of the network. Thus, to some extent on the range of programs across some subclass and a “variable part” that is specific Then comparisons same machine. For example, ward neural network. LM consists of all such networks, the subclasses defined by different network in comparing subclass the boundary between machine comparisons and program depends to consider. to the individual in different subclasses suppose M is a machine capable of running that the designer wishes are made between the best programs programs differ individual the methodology level of analysis, At the most general is now quite straightforward: choose a machine, choose a program then dump the resulting agent into a class of environments E. The program with the best performance is bounded optimal for M in E. For example, M is an IBM PC with a C compiler; LM consists of C consists of a population of human chess programs up to a certain size; the environment the bounded optimal opponents; program is the one with the highest rating. is the chess rating achieved; the performance measure that runs on the machine, This rather blunt and unenlightening approach has no doubt occurred of chess programs. As stated, in the construction solve and the solution, once found, would be very domain-specific. define a research agenda for bounded optimality generality. This can be done by exploiting particular designs, particularly In this way, we can prove bounded optimality environments. those that incorporate mechanisms results the orthogonality structure the problem is ridiculously to many engaged hard to is to that provides a little more guidance and in in the definition of the problem, agent The problem for adaptation and optimization. for more general classes of task of time and content, and by using more sophisticated 7.2.1. Mechanisms for optimization Modular design using a hierarchy of components is commonly seen as the only way to build reliable complex systems. The components and interact in well-defined ways. To produce a composite bounded-optimal fulfill certain behavioural specifications design, the S.J. Russell/Art$cial Intelligence 94 (1997) 57-77 71 [40] problem execution the discussion involves allocating time to components that uses an anytime are largely orthogonal of universal ABO algorithms, earlier temporal behaviour [ 521 or arrang- to maximize overall performance. the techniques to the content of the system for example, a com- inference algorithm over a belief network as one of the accuracy of the belief network, optimization ing the order of execution of the components As illustrated in for optimizing components, which can therefore be optimized separately. Consider, posite system its components. the performance reallocation Thus, techniques [52] can be seen as domain-independent optimality class. As a simple example, we might prove that a certain chess program design for all time controls in tools for agent design. They enable bounded results that do not depend on the specific temporal aspects of the environment is AR0 If a learning profile of the inference component will improve, which will result in a to improve overall system performance. and the time allocation algorithm from blitz to full tournament play. such as the doubling construction time that is guaranteed of execution algorithm improves ranging The results obtained so far for optimal process with predictable among components. One can time allocation component imagine components must deal with unexpectedly have assumed a static, of- profiles and fixed in which in processing far more subtle designs slow or fast progress performance for information resources among components, from other components. This might involve ex- and establishing of a computational market, as envisaged by Wellman and would offer a useful additional new interfaces, hierarchies, fline optimization connections individual and changing changing computational so on. This is more reminiscent [ 511, than of the classical level of abstraction needs subroutine in system design. 7.2.2. Mechanisms for adaptation In addition optimization to combinatorial an agent, we can also use learning methods of the structure and temporal behaviour of to improve the design: In [40], l The content of an agent’s knowledge base can of course be improved by inductive bounded optimal designs can in such a way is close to optimal among all components of a given execu- in the agnostic from computational if each component theory, particularly that approximately is learned learning it is shown learning. be guaranteed with high probability that its output quality tion time. Results learning model The k:ey additional carries component step is to analyze through [25], can provide learning methods with the required properties. in each the way in which slight imperfection to slight imperfection in the whole agent. information such as utility l Reinfbrcement learning can be used to learn value results [ 491 provide convergence guarantees for reinforcement functions. Recent learning with a fairly broad class of function learning methods for metalevel [ 42, (Chapter 61, this is shown convergence is needed, however, since current generates control will presumably time. approximators. One can use such the value of computation. information, In e.g., results on technique. Formal to be an effective to optimal control of search would be of great interest. Further work that its search over the agent’s experiences whereas an agent theorems assume a stationary distribution be exploring different populations that is improving of experiences 72 S.J. Russell/Art$cial Intelligence 94 (1997) 57-77 l Compilation methods such as explanation-based including SOAR [28] use compilation learning can be used to transform to allow faster decision making. Several agent archi- to speed up all forms of problem have been obtained by Tadepalli novel only infre- that after a given amount of experience, results on convergence for which no solution has been stored should be encountered representations an agent’s tectures solving. Some nontrivial [ 481, based on the observation problems quently. Presumably, the issues to be faced by bounded optimality when several adaptation “quasistatic” approach, method practical. an agent architecture can incorporate all these learning mechanisms. One of results simultaneously. A the other adequate but not very reaches convergence theoretically and optimization mechanisms is how to prove convergence in which one mechanism its next step, seems are operating is allowed research to take before 7.2.3. Ofline and online mechanisms One can distinguish between offline and online mechanisms bounded- is not itself part of the agent and for constructing optimal agents. An offline construction mechanism is not the subject of bounded optimality designed in a specific environment E-that for a class of environments E. Then a typical is, an environment-specific agent. E E E and returns an agent design constraints. Let C be an offline mechanism theorem will say that C operates for that is ABO (say) to the transient cost of the adaptation or optimization mechanism, In the online case, the mechanism C is considered part of the agent. Then a typical is ABO for all E E E. If the performance measure the if the cost cannot to an agent the analysis for theorem will say that the agent used is indifferent two types of theorems are essentially be ignored-for that reaches becomes more difficult. “experience efficiency” learning It may become necessary robust to obtain in order to define asymptotic results, as is done equivalence in computational the same level of performance if an agent that learns quickly the same. On the other hand, but learns more slowly-then is to be preferred example, that one can easily prove the value of “lifelong learning” theory. It is worth noting ABO framework. An agent that devotes a constant to learning-while-doing learning agent will always be preferred. cannot do worse, after some point. improvement If some fraction of its computational than an agent in the ABO sense, is still possible, the lifelong in the resources that ceases learning 7.2.4. Fixed and variable computation costs Another dimension the cost is fixed. Consider of design space emerges when one considers cost of the “variable part” of the agent design. The design problem siderably when learning, tion mapping Q function fixed, the optimality and to make things concrete from computational in the space have the same execution then all Q functions is to be represented again let the metalevel decision be made by a Q func- that the to value. Suppose by a neural net. If the topology of the neural net is state and action further time. Consequently, criterion used by the standard Q-learning process coincides with bounded the computational is simplified con- the task of metalevel reinforcement S.J. Russell/Arti$cial Intelligence 94 (1997) 57-77 73 is explored, and the equilibrium if the topology of the network reached will be a bounded-optimal optimality, the other hand, space case, the standard Q-learning process will not necessarily converge configuration. A different adaptation mechanism must be found the passage of time and its effect on utility. to alteration is subject time of the different Q-functions On as the design In this varies. to a bounded-optimal that takes into account configuration.5 the execution then Whatever the solution the notion of bounded optimality in good performance result from calculative afford to aim for perfection. to this problem turns out to be, the important point is that that will derived rationality will fail in the more realistic setting where an agent cannot helps to distinguish those that will not. Adaptation mechanisms adaptation mechanisms from 7.2.5. Fully variable architectures so far has been The discussion functional to particular than itself and that exhibits variation over time at all levels of to fairly sedate forms of agent architecture aspects such limited in which the scope for adaptation is circumscribed as metalevel Q functions. However, an agent must in general deal with an environment that is far more complex that almost complete granularity. Limits on the size of the agent’s memory may imply For revision of the agent’s mental example, one can imagine through cycles of winter that a simple rule-based agent living and summer may have to discard all of its summer rules as winter approaches, and then relearn them from scratch the following year. Such situations may engender a rethinking of some of our notions of agent architecture and optimality, and suggest a view of agent programs knowledge as dynamical and internal processes of inductive systems with various amounts of compiled to achieve high performance. forgetting, and compilation. and uncompiled is needed structure learning, 7.2.6. Towards a grammar of AI systems that seems to be emerging The approach research into “architectural for bounded optimality classes directly, perhaps is sufficiently optimization within limited. Then ABO the class or by showing classes” such that in each class results can be obtained that an empirical is to divide the either adaptation in an approximately ABO design. Once this is done, it should be possible of of an that the inclusion in a given capability in the limit of very complex by tool in such work will be fraction can do no harm to an agent’s up the space of agent designs structural variation by analytical process results to compare architecture one class over another. For example, appropriate “macro-operator architecture will result environments-that increasing the use of “no-cost” of computational ABO prospects. in an improvement is, one cannot compensate speed by a constant formation” or “greedy me&reasoning” for the exclusion of the capability to establish asymptotic dominance the allocation of a constant results where, for example, to learning or metareasoning it might be the case factor. A central in behaviour the machine resources 5 A similar observation was made by Horvitz and Brcese 1221 for cases where the object level is so restricted that the metalevel decision problem can be solved in constant time. 74 S.J. RusseWArt~cial intelligence 94 (1997) 57-77 Getting all these architectural devices to work is an important in AI and must be addressed before we can make progress on un- If the classes. then AI may eventu- device” can be made sufficiently concrete, these more complex architectural smoothly together bounded optimality within unsolved problem derstanding notion of “architectural ally develop a grammar for agent designs, describing As the grammar develops, so should the accompanying ABO dominance results. the devices and their interrelations. 8. Summary I have outlined some directions as the desired property of AI systems. This perspective on AI seems from optimization of the inevitable philosophical “move” I have suggested for formally grounded AI research based on bounded to be over over programs. that such an theoretical and practical AI research of a kind it is a satisfactory it is entirely in real intelligent in large process. One can also the needs of those In the same vein, I believe intelligence. as a design optimization the need for complex faced by relatively In particular, tiny minds limitations structure to optimization should allow synergy between optimality a logical consequence actions or computations approach not afforded by other formal frameworks. formal counterpart of the informal goal of creating consistent with our intuitions agents, the importance of the resource worlds, and the operation of evolution argue who wish on computational deviation research intelligence, from perfect rationality that are presumably to emulate human that bounded optimality resources about is likely because the limitations for most of the regrettable to satisfy better it takes into account exhibited by humans. responsible Bounded optimality and its asymptotic that one may want systems cousin are, of course, nothing but formally defined properties to satisfy. It is too early to tell whether ABO will do the same kind of work has done complexity that asymptotic for theoretical computer is still of AI the prerogative researchers. the design process somewhat and and to automate the demands of the environment. The concept of bounded optimality provides a way to make sure the adaptation process the process of adapting a system in design to systematize It may, however be possible to its computational science. Creativity is “correct”. resources for AI the conceptual it will eventually be possible My hope is that with these kinds of investigations, and mathematical develop telligence. For example, why do complex knowledge assumption is still unknown. sumption. What is clear is that it will need something agent design structures over which that distinguishes AI from other disciplines to tools to answer some basic questions about in- to) have declarative intelligent they reason explicitly? This has been a fundamental for agent design, yet the answer the as- like a theory of bounded optimal Indeed, Rod Brooks, Hubert Dreyfus, and others flatly deny to answer systems (appear Most of the agent design that I have discussed here, including this question. features declarative knowledge, have been conceived within build calculatively rational agents and then speed doubt features needed that this methodology will enable intelligence. for general the AI community The reason them up”. Yet one can legitimately to discover all the design computer is that no conceivable the standard methodology the use of of “first S.J. Russell/Artificial Intelligence 94 (1997) 57-77 15 will ever be remotely close to approximating perfect rationality for even moderately complex environments. Perfect ration~ity is, if you like, a “Newtoni~” definition for intelligent agents whereas the real world is a particle accelerator. It may well be the case that agents based on improvements to calculatively rational designs are not even that is potentially achievable given the close to achieving the level of performance underlying computational resources. For this reason, I believe it is imperative not to dismiss ideas for agent designs that do not seem at first glance to fit into the “classical” calculatively rational framework. Instead, one must attempt to understand the potential of the bounded optimal configurations within the corresponding architectural class, and to see if one can design the appropriate adaptation mechanisms that might help in realizing these configurations. As mentioned in the previous section, there is also plenty of work to do in the area of making more general and more robust “bricks” from which to construct AI systems for more realistic environments, and such work will provide added scope for the achievement of bounded optimality. In a sense, under this conception AI research is the same now as it always should have been. Acknowledgements An earlier version of this paper appeared in the Proceedings of the Fourteenth Zn- ternatioml Joint Conference on Artificial Intelligence, published by IJCAII. That paper drew on previous work with Eric Wefald [42] and Devika Subr~anian [40]. The latter work contains a more rigorous analysis of many of the concepts presented here. Thanks allso to Michael Wellman, Michael Fehling, Michael Genesereth, Russ Greiner, Eric Horvitz, Henry Kautz, Daphne Koller, and Bart Selman for many stimulating dis- cussions and useful suggestions on the topic of bounded rationality. The research was supported by NSF grants IRI-8903 146, IRI-921 X512 and IRI-9058427, by a UK SERC Visiting Fellowship. References f 1 ] PE. Agre and D. Chapman, Pengi: an impIementatio~ of a theory of activity, in: ~rocee~i~~~ IJCAI-87, Milan, Italy (Morgan Kaufmann, Los Altos, CA, 1987) X8-272. 121 F. Bacchus and A. Grove, Graphical models for preference and utility, in: Proceedings 11th Conference on Uncertainty in Artijicial Intelligence (UAI-95), Montreal, Que. (Morgan Kaufmann, Los Altos, CA, 1995) 3-10. [ 31 J.S. Breese and M.R. Fehling, Control of problem-solving: principles and architecture, in: R.D. Shachter, T.S. Levitt, L.N. Kanal and J.F. Lemmer, eds., Uncertujn~ in Arf~c~a~ Iafe~~ige~ce, Vol. 4 (North- Holland, Amsterd~, 1990). [4] R.A. Brooks, Engineering approach to building complete, intelligent beings, Proceedings SPIE--The International Societyfor Optical Engineering 1002 (1989) 618-625. [S] R.A. 13rooks, Intelligence without representation, Artificial Intelligence 47 (1991) 139-159. [ 6 J R. Camap, Logical Foundations of Probability (University of Chicago Press,. Chicago, IL, 1950). [7] A.R. Cassandra, L.P. Kaelbhng and M.L. Littman, Acting optimafly in pattially observable stochastic domains, in: Proceedings AAAI-94, Seattle, WA (AAAI Press, 1994) 1023-1028. ]8] C. Chemiak, Minimal Rationality (MIT Press, Cambridge, MA, 1986). 76 S.J. Russell/Artificial Intelligence 94 (1997) 57-77 [9] R. Davis, Meta-rules: [lo] T.L. Dean, J. Aloimonos Cummings, Redwood City, CA, 1995). reasoning about control, Artificial Intelligence 15 (1980) 179-222. and J.F. Allen, Artificial Intelligence: Theory and Practice (Benjamin/ [ 111 T.L. Dean and M. Boddy, An analysis of time-dependent MN (Morgan Kaufmann, Los Altos, CA, 1988) 49-54. planning, in: Proceedings AAAI-88, St. Paul, [ 121 T.L. Dean and K. Kanazawa, A model for reasoning about persistence and causation, Comput. Intell. 5 (1989) 142-150. [ 131 DC. Dennett, The moral first aid manual, Tanner Lectures on Human Values, University of Michigan, Ann Arbor, MI ( 1986). [ 141 J. Doyle and R.S. Patil, Two theses of knowledge representation: Language restrictions, taxonomic classification and the utility of representation services, Art&ial Intelligence 48 (1991) 261-297. [ 151 E. Ephrati and J.S. Rosenschein, The Clarke tax as a consensus mechanism among automated agents, in: Proceedings AAAI-91, Vol. 1, Anaheim, CA (AAAI Press, 1991) 173-178. [ 161 J. Forbes, T. Huang, K. Kanazawa towards a Bayesian taxi, in: Proceedings IJCAI-95, Montreal, Que. (Morgan Kaufmann, Los Altos, CA, 1995). and S.J. Russell, The BATmobile: automated [ 171 M.L. Ginsberg and D.F. Geddis, Is there any need for domain-dependent Proceedings AAAI-91, Vol. 1, Anaheim, CA (AAAI Press, 1991) 452-457. principles of rationality, [ 181 I.J. Good, Twenty-seven in: VP Godambe and D.A. Sprott, eds., Foundations control information?, in: of Statistical Inference (Holt, Rinehart and Winston, Toronto, Ont., 1971) 1088141. [ 191 G.H. Harman, Change in View: Prmciples of Reasoning (MIT Press, Cambridge, MA, 1983). [20] E.J. Horvitz, Problem-solving in: Proceedings 2nd Annual NASA Research Forum, Moffett Field, CA (NASA Ames Research Center, 1987) 26-43. design: reasoning about computational value, trade-offs and resources, 1211 E.J. Horvitz, Reasoning about beliefs and actions under computational resource constraints, in: L.N. Kanal, T.S. Levitt and J.F. Lemmer, eds., Uncertainty in Art$cial Intelligence, Vol. 3 (North-Holland, Amsterdam, 1989) 301-324. [ 221 E.J. Horvitz and J.S. Breese, Ideal partition of resources for metamasoning, Technical Report KSL-90-26, Knowledge Systems Laboratory, Stanford University, Stanford, CA ( 1990). [23] R.A. Howard, [ 241 R.E. Kalman, A new approach Information value theory, IEEE Trans. Systems Sci. Cybernet. 2 ( 1966) 22-26. to linear filtering and prediction problems, J. Basic Engineering (March 1960) 35-46. [25] M. Keams, R. Schapire and L. Sellie, Toward efficient agnostic in: Proceedings 5th Annual ACM Workshop on Computational Learning Theory (COLT-92). Pittsburgh, PA (ACM Press, New York, 1992). learning, [ 261 R.L. Keeney and H. Raiffa, Decisions with Multiple Objectives: Preferences and Value Tradeoffs (Wiley, New York, 1976). [27] P.R. Kumar and l? Varaiya, Stochastic Systems: Estimation, Identification and Adaptive Control (Prentice- Hall, Englewood Cliffs, NJ, 1986). [28] J.E. Laird, P.S. Rosenbloom and A. Newell, Chunking in Soar: the anatomy of a general learning mechanism, Machine Learning 1 ( 1986) 1 l-46. [29] S.L. Lauritzen, The EM algorithm Data Analysis 19 (1995) 191-201. for graphical association models with missing data, Comput. Statist [30] H.J. Levesque, Making believers out of computers, Arttftcial Intelligence 30 (1986) 81-108. [31] H.J. Levesque and R.J. Brachman, Expressiveness in knowledge tractability and representation and reasoning, Comput. Intell. 3 (1987) 78-93. [ 321 J.E. Matheson, The economic value of analysis and computation, IEEE Trans. Sysfems Sci. Cybernet 4 (1968) 325-332. [33] N. Megiddo and A. Wigderson, On play by means of computing machines, ed., Theoretical Aspects of Reasoning about Knowledge: Proceedings 1986 Conference Monterey, CA (Morgan Kaufmann, Los Altos, CA, 1986) 259-274. in: J.Y. Halpem, (TARK-86), [ 341 S. Minton, Is there any need for domain-dependent control information? A reply, in: Proceedings AAAI- 96, Vol. 1, Portland, OR (AAAI Press, 1996) 855-862. [35] A. Newell, The knowledge level, Arti$cial Intelligence 18 (1982) 82-127. S.J. RusseWArtijcial Intelligence 94 (1997) 57-77 71 [36] C.H. Papadimitriou Symposium on Theory of Computation (STOC-94) and M. Yannakakis, On complexity ( 1994). as bounded rationality, in: Proceedings [ 371 R. Parr and S.J. Russell, Approximating optimal policies for partially observable stochastic domains, in: Proceedings IJCAI-95, Montreal, Que. (Morgan Kaufmann, Los Altos, CA, 1995). [38] S.J. Russell, hidden variables, 11466;’ 152. J. Binder, D. Keller and K. Kanazawa, Local networks with in: Proceedings IJCAI-95, Montreal, Que. (Morgan Kaufmann, Los Altos, CA, 1995) in probabilistic learning [ 391 S.J. Russell and P Norvig, Artificial Intelligence: A Modern Approach (Prentice-Hall, Englewood Cliffs, NJ, 19’95). [40] S.J. Russell and D. Subramanian, [ 411 S.J. Russell and E.H. Wefald, On optimal game-tree search using rational meta-reasoning, agents, J. Art$ Intell. Research 3 (1995). in: Proceedings Provably bounded-optimal IJCALS9, Detroit, MI (Morgan Kaufmann, Los Altos, CA, 1989) 334-340. [ 421 S.J. Russell and E.H. Wefald, Do the Right Thing: Studies in Limited Rationality (MIT Press, Cambridge, MA, 1991). [43] S.J. Russell and E.H. Wefald, Principles of me&reasoning, Artijcial Intelligence 49 (1991) 361-395. [44] S. Russell and S. Zilberstein, Composing in: Proceedings IJCAI-9I, Sydney, Australia real-time systems, (Morgan Kaufmann, Los Altos, CA, 1989). [45] H.A. Simon, A behavioral model of rational choice, Quart. J. Economics 69 ( 1955) 99-118. 1461 H.A. Simon, Rational choice and the structure of the environment, in: Models of Rounded Rationality, Vol. 2 (MIT Press, Cambridge, MA, 1958). [47] E.J. Sondik, The optimal control of partially observable Markov decision processes, Ph.D. Thesis, Stanford University, Stanford, CA ( 197 1) [48] P Tadepalli, A formalization of explanation-based macro-operator learning, in: Proceedings IJCAI-91, Sydney, Australia (Morgan Kaufmann, Los Altos, CA, 1991) 616-622. [49] J.N. Tsitsiklis and B. Van Roy, An analysis of temporal-difference Technical Report LIDS-P-2322, MA ( 1996). Laboratory for Information learning with function approximation, and Decision Systems, MIT, Cambridge, [50] M.P. Wellman, Reasoning about preference models, Technical Report MIT/LCS/TR-340, Laboratory for Computer Science, MIT, Cambridge, MA ( 1985). [51] M.P. Wellman, A market-oriented programming environment multicommodity flow problems, J. Artif: Intell. Research 1 (1994) and l-23. its application to distributed [52] S. Zilberstein and S.J. Russell, Optimal composition of real-time systems, Artiftcial Intelligence 82 (1996) 181-213. 