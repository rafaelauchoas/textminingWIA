Artificial Intelligence 172 (2008) 1245–1284www.elsevier.com/locate/artintWhat makes propositional abduction tractableGustav Nordh a,∗,1, Bruno Zanuttini ba LIX, École Polytechnique, Route de Saclay, F-91 128 Palaiseau, Franceb GREYC, UMR CNRS 6072, Université de Caen, Bd. du Maréchal Juin, F-14 032 Caen Cedex, FranceReceived 10 April 2007; received in revised form 28 January 2008; accepted 5 February 2008Available online 12 February 2008AbstractAbduction is a fundamental form of nonmonotonic reasoning that aims at finding explanations for observed manifestations. Thisprocess underlies many applications, from car configuration to medical diagnosis. We study here the computational complexity ofdeciding whether an explanation exists in the case when the application domain is described by a propositional knowledge base.Building on previous results, we classify the complexity for local restrictions on the knowledge base and under various restrictionson hypotheses and manifestations. In comparison to the many previous studies on the complexity of abduction we are able to givea much more detailed picture for the complexity of the basic problem of deciding the existence of an explanation. It turns out thatdepending on the restrictions, the problem in this framework is always polynomial-time solvable, NP-complete, coNP-complete,or (cid:2)PBased on these results, we give an a posteriori justification of what makes propositional abduction hard even for some classesof knowledge bases which allow for efficient satisfiability testing and deduction. This justification is very simple and intuitive, butit reveals that no nontrivial class of abduction problems is tractable. Indeed, tractability essentially requires that the language forknowledge bases is unable to express both causal links and conflicts between hypotheses. This generalizes a similar observation byBylander et al. for set-covering abduction.© 2008 Elsevier B.V. All rights reserved.2 -complete.Keywords: Abduction; Propositional logic; Computational complexity1. IntroductionAbduction is the fundamental reasoning process which consists of explaining observations by plausible causestaken from a given set of hypotheses. For instance, it is an abduction problem to try to derive diseases from ob-served symptoms, according to known rules relating both. This process was extensively studied by Peirce [6], and itsimportance to Artificial Intelligence was first emphasized by Morgan [38] and Pople [41].From the application point of view, abduction has demonstrated its importance. It has been applied in particular toexplanation-based diagnosis (e.g., medical diagnosis [7]), to text interpretation [29], and to planning [28]. It is alsothe fundamental process underlying ATMSs [13].* Corresponding author.E-mail addresses: nordh@lix.polytechnique.fr (G. Nordh), zanutti@info.unicaen.fr (B. Zanuttini).1 Partially supported by the Swedish–French Foundation and the National Graduate School in Computer Science (CUGS), Sweden.0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.02.0011246G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284The formalization and resolution of abduction problems have been studied in numerous formalisms, among whichset-covering [7], default logic [22], logic programming [21,36]. We are interested here in its resolution in classicalpropositional logic.We adopt a complexity-theoretic point of view. More precisely, we are interested in the complexity of decidingwhether an abduction problem has a solution when the underlying knowledge base is propositional. Thus our studyfollows Selman and Levesque’s [48] and Eiter and Gottlob’s [20] seminal papers. We also build on two classificationspreviously obtained in our framework [12,40].Even in the simple setting of propositional logic, deciding whether an abduction problem has a solution is in general(cid:2)P2 -complete. Consequently, like for most hard computational problems, several approaches have been studied forsolving it efficiently: Exhibiting tractable classes obtained by restrictions over the knowledge base [12,16,20,24,51];heuristic approaches, in particular through computation of prime implicates [14,15,37,49] and through reducing theproblem to QBF and using generic QBF solvers [19]; compilation [8,35]; and approximation [31,50].In this paper we adopt the approach consisting of trying to find tractable restrictions over the knowledge base. Ourcontribution is twofold.First, we identify the complexity of abduction, with varying restrictions over the representations of manifestationsand hypotheses, for every constraint language and every clausal or equational language restricting the (propositional)knowledge base, under reasonable assumptions on the representation of the constraints. Concerning manifestations,we study the restrictions where they are expressed as a positive, negative, or unrestricted literal, clause, term, or CNF.Concerning hypotheses, we study the restrictions where they are expressed by a set of literals which is positive, neg-ative, closed under complement, or unrestricted. To that aim, we use the now well-known Schaefer’s framework [46]and Post’s lattice [42]. Precisely, we proceed as follows.• We first prove a relatively small number of tractability and hardness results for particular constraint languages.• Using Post’s classification and these results, we then derive the complexity of abduction for any constraint lan-guage.• In a similar manner, and at the same time, we obtain the complexity of abduction for any clausal or equationallanguage.We exhibit new polynomial and new hard restrictions. We also discover that abduction is always either in P,NP-complete, coNP-complete, or (cid:2)P2 -complete, depending on the restrictions. Such a result could not be taken forgranted, due to Ladner’s result stating that if P (cid:3)= NP, then there exist problems in NP that are neither in P nor NP-complete [33]. Moreover, the fact that some restrictions yield NP-complete, and others coNP-complete problems issurprising at first sight. It reveals in particular that abduction is a very rich problem in terms of completeness resultsin different complexity classes. Thus our results can be used as starting points for establishing complexity results forother problems, in particular in nonmonotonic reasoning.From the application point of view, our tables of complexity allow the designers of knowledge-based agents orexpert systems to choose the appropriate knowledge representation language, according to the tradeoff between theexpressiveness required and the constraints on resolution of abduction problems. Moreover, when a representationlanguage that is hard for abduction must be used, the precise complexity of the corresponding problem allows tochoose heuristic approaches for solving it. For instance, with an appropriate reduction an NP-complete problem canbe solved by a satisfiability solver, while a (cid:2)P2 -complete one cannot; a more generic QBF solver (or a specializedQBF∃,2-solver) must be used.Our second contribution is to identify a simple set of minimal conditions yielding NP-completeness for languageswhich allow for efficient deduction (and are thus good candidates for knowledge representation). For instance, whenterms have to be explained, we discover that abduction is NP-hard exactly when the language for knowledge bases canboth express causal links from hypotheses to individual manifestations and forbid some combinations of hypotheses.This generalizes similar observations by Bylander et al. [7] about set-covering abduction.From this condition it follows that tractability can occur only in very restricted cases, i.e., when there can be nocausal dependency at all or when causes can all be assumed together. For instance, in medical diagnosis this meansthat diseases must not rule each other out for the task to be tractable. We also argue that these conditions give intuitionsabout results beyond Schaefer’s framework of constraint languages, and we revisit some previously known results inthat manner. In this spirit, our observations allow to adopt a unified point of view on the results exhibited with manydifferent restrictions in the literature.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841247The use of Post’s lattice and Schaefer’s framework for studying the complexity of reasoning problems is sometimesconsidered to be overlimitating. For instance, it does not encompass the class of all Horn formulas; this is because Hornclauses can be arbitrarily long. In this study, we adopt these powerful tools coming from complexity-theory and showhow to overcome some of these limitations. Indeed, we use them to show results about infinite constraint languagesas well as about finite ones, and we formulate the former in terms of classes of CNF formulas. These extensions aredirectly motivated by AI-applications where it is common to model the knowledge base using an infinite constraintlanguage represented in terms of classes of CNF formulas.To put our results in context, we briefly discuss the results from the literature on the complexity of abduction thatare most relevant to our present study. Our starting point is Selman and Levesque’s [48], and Eiter and Gottlob’s [20]classical results. Selman and Levesque [48] proved that deciding whether an abduction problem over a Horn knowl-edge base has an explanation is NP-complete, even when the hypotheses are given as a set of positive literals andthe manifestation is a single positive literal. Similarly, Eiter and Gottlob [20] proved that when the knowledge baseis given by a general propositional formula, the problem becomes (cid:2)P2 -complete. Moreover they note that when theknowledge base is given by a definite Horn formula, the hypotheses are positive literals, and the manifestations aregiven by a positive term, then the problem is in P.From these results it is clear that the complexity of the abduction problem depends heavily on the representationlanguage of the knowledge base. This have led researchers to investigate the complexity of abduction for various re-strictions on the representation language of the knowledge base. This line of research culminated with the two recentcomplexity classifications given in [12,40]. In these papers the complexity of deciding if there exists an explanationis classified for a very general class of restrictions on the representation language of the knowledge base, that in-clude many restrictions that have been studied before in the literature. Although the restrictions on the representationlanguage of the knowledge base studied in these papers are identical, the papers differ on the representation of hy-potheses and manifestations. In [12] the hypotheses are given by a set of literals that are closed under complementand the manifestation is given by a single literal, while in [40] the hypotheses are allowed to be any set of literals andthe manifestations are given by a term.When comparing the results of [12] and [40] it becomes clear that even small variations on the representation ofhypotheses and manifestations can have a strong impact on the complexity of the abduction problem. Hence, to un-derstand the complexity of abduction one must also understand how restrictions on the representation of hypothesesand manifestations influence the complexity. This is partially the motivation for the present study where we sys-tematically analyse the complexity of abduction under simultaneous restrictions on the representation of hypotheses,manifestations, and the knowledge base.The paper is organized as follows. We first give some preliminaries about propositional logic, Schaefer’s frameworkand complexity classes (Section 2) and the definition of the abduction problems we are interested in (Section 3).We then survey previous work and give an overview of our results (Section 4). The technical content of the paperfollows: we give several generic reductions between abduction problems (Section 5) and recall several well-knowntools for studying abduction (Section 6); we then give the complexity results (Sections 7 to 12) by distinguishing thevarious families of restrictions over the knowledge base. These results are summarized in Section 13, and some furtherrestrictions on the problems are discussed in Section 14. Finally, we discuss what makes abduction hard in Section 15,and we conclude in Section 16.2. PreliminariesIn the paper we consider restrictions over knowledge bases seen either as propositional formulas or as conjunctionsof constraints taken from a fixed language. We introduce here the corresponding basic definitions and notations, definethe restrictions that we will consider, and briefly recall some complexity notions.2.1. Propositional logicBoolean variables are usually denoted by x, possibly with subscripts and superscripts. A literal is either a variable(positive literal) or the negation of one (negative literal). Literals are usually denoted by (cid:3). The opposite of literal (cid:3) iswritten (cid:3), i.e., (cid:3) = x if (cid:3) = ¬x and vice-versa.1248G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284A clause is a finite disjunction of literals, written C = (cid:3)1 ∨ (cid:3)2 ∨ · · · ∨ (cid:3)n. A formula in Conjunctive Normal Form(a CNF for short) is a finite conjunction of clauses, written KB = C1 ∧ C2 ∧ · · · ∧ Ck. A term is a finite conjunction ofliterals, written T = (cid:3)1 ∧ (cid:3)2 ∧ · · · ∧ (cid:3)n. Variables, literals, clauses and terms are considered special cases of formulas.A (linear) equation is an equation of the form (x1 ⊕ x2 ⊕ · · · ⊕ xn = a) with a ∈ {0, 1} (⊕ denotes addition modulo 2).An affine formula is a finite conjunction of linear equations. Observe that affine formulas are not CNFs.If ϕ is a formula, Vars(ϕ) denotes the set of all variables that occur in ϕ. Given a set of variables V , Lits(V ) denotesthe set of all literals formed upon the variables in V , i.e., Lits(V ) = V ∪ {¬x | x ∈ V }. If L is a set of literals,Ldenotes their conjunction, and N(L) denotes the set of opposite literals {(cid:3) | (cid:3) ∈ L}. A clause, term, or CNF is said tobe positive (resp. negative) if all the literals which occur in it are positive (resp. negative), and a clause or term is saidto be unit if it contains exactly one literal. The empty clause is written ⊥. Observe that Vars(⊥) = ∅ and that ⊥ isalways false. Similarly, (0 = 1) denotes the always false linear equation with no variable.(cid:2)An assignment μ to a set of variables V is a mapping from V to {0, 1}. When the order of the variables is clear,we write assignments as words, e.g., μ = 011 if μ(x1) = 0 and μ(x2) = μ(x3) = 1. If KB is a formula, an assignmentμ to Vars(KB) or to a superset of Vars(KB) is said to satisfy KB, or to be a model of KB, if it makes KB evaluate to 1with the usual rules for the connectives. We write μ |(cid:12) KB.A formula KB is said to be satisfiable if it has at least one model. It is said to entail a formula KB(cid:13) if everyassignment to Vars(KB) ∪ Vars(KB(cid:13)) which satisfies KB also satisfies KB(cid:13), written KB |(cid:12) KB(cid:13). If moreover KB(cid:13) |(cid:12) KB,then KB and KB(cid:13) are said to be (logically) equivalent, written KB ≡ KB(cid:13).Example 1. The formula:KB = (x1 ∨ x2) ∧ (¬x2 ∨ x3 ∨ ¬x4) ∧ (¬x1 ∨ x2 ∨ ¬x3) ∧ (¬x1 ∨ ¬x2)is a CNF containing 4 clauses. We have Vars(KB) = {x1, x2, x3, x4}, and thus Lits(Vars(KB)) = {x1, ¬x1, . . . , x4, ¬x4}.The assignment μ to Vars(KB) defined by μ(x1) = 0, μ(x2) = 1, μ(x3) = 0, and μ(x4) = 0 (μ = 0100 for short) isa model of KB, while 0000 is not. The formula KB(cid:13) = (x1 ⊕ x2 = 1) ∧ (x2 ⊕ x3) = 0 is an affine formula. Its set ofmodels is {011, 100}, and it can be seen that KB(cid:13) |(cid:12) KB, while KB (cid:3)|(cid:12) KB(cid:13) and thus, KB (cid:3)≡ KB(cid:13).2.2. Classes of clauses and equationsWe will first impose restrictions on the knowledge bases of abduction problems in the form of classes of clauses andlinear equations. These classes are reported in Table 1 (page 1251, second and third columns) and described below.The class of all clauses is denoted by CCNF. A clause C is said to be 1-valid if it is satisfied by assigning 1 to allvariables in it. Equivalently, a clause is 1-valid if it contains at least one positive literal. The class of all 1-valid clausesis denoted by C1v. Dually, C0v is the class of all 0-valid clauses.A clause is Horn if it contains at most one positive literal. The class of all Horn clauses is denoted by CHorn.A clause is 1-valid Horn (also called definite Horn) if it contains exactly one positive literal. The class of all 1-validHorn clauses is denoted by C1v−Horn. Analogously, a clause C is said to be dual-Horn (0-valid dual-Horn) if it containsat most one (exactly one) negative literal, and the class of all dual-Horn clauses (0-valid dual-Horn clauses) is denotedby CdHorn (C0v−dHorn). Moreover, the class of all 1-valid dual-Horn clauses is denoted by C1v−dHorn.A clause is said to be bijunctive if it contains at most two literals, and implicative if it contains zero (empty clause)or one literal, or is of the form (¬x1 ∨ x2). The classes of all bijunctive and of all implicative clauses are denotedby Cbij and Cimpl, respectively. A clause is said to be IHS-B− if it is implicative or negative, and the class of allIHS-B− clauses is denoted by CIHSB−. Similarly, a clause is said to be IHS-B+ if it is implicative or positive, and theclass of all IHS-B+ clauses is denoted by CIHSB+. Moreover, a clause is said to be of width k if it contains at most kliterals. The classes of all IHS-B− and IHS-B+ clauses of width k are denoted CIHSB−/k and CIHSB+/k, respectively.A clause is said to be essentially negative if it is negative or unit positive. The class Cneg,= contains all essentiallynegative clauses, as well as the equality relations (x1 = x2). Observe that such equality relations are not clauses, thuswe slightly abuse language. The reason why we allow the equality relation is that it makes the class Cneg,= equivalentin expressiveness to a relational clone, and thus, in particular, stable with respect to the complexity of abduction (seeSection 2.3). Nevertheless, we also discuss the case when this relation is not allowed in the paper. Dually to Cneg,=,the class Cpos,= contains all essentially positive clauses and the equality relations.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841249Recall that a linear equation is an equation of the form (x1 ⊕ · · · ⊕ xn = a), a ∈ {0, 1}. The class of all linearequations is denoted by Eaff . Similarly to the case of clauses we say that a linear equation is 1-valid (resp. 0-valid) ifit is satisfied by assigning 1 (resp. 0) to all variables in it. The classes of all 1-valid and all 0-valid linear equations aredenoted by E1v−aff and E0v−aff , respectively. A linear equation is of width 2 if it contains at most two variables. Theclass of all linear equations of width 2 is denoted by Eaff /2.Finally, the class Cunit,= contains all unit clauses and equality relations (the same remark as for Cneg,= applies). Theclass C1v−unit,= contains the unit positive clauses and the equality relations, and the class C0v−unit,= contains the unitnegative clauses and the equality relations.2.3. Constraint languages and Post’s latticeIn this section we introduce the notions of constraint languages and relational clones. We also describe Post’sclassification of all Boolean relational clones, usually referred to as Post’s lattice. A more detailed introduction toPost’s lattice can be found in the survey articles [2,3].Constraints generalize the notions of clauses and linear equations, and constraint languages generalize the notionof classes of clauses and equations. The set of all n-tuples of elements from {0, 1} is denoted by {0, 1}n. Such tuplesare denoted as sequences or as words, e.g., (0, 1, 1) or 011. Any subset of {0, 1}n is called an n-ary relation on {0, 1}.A (finite) constraint language over {0, 1} is an arbitrary (finite) set of (finitary) relations over {0, 1}.If (cid:5) is a constraint language, then a constraint over (cid:5) is an application of an n-ary relation R ∈ (cid:5) to an n-tupleof variables, written R(x1, . . . , xn) (possibly with repeated variables). A formula over (cid:5) (or (cid:5)-formula) is a finiteconjunction of constraints over (cid:5), written KB = R1(x11, . . . , x1n1 ) ∧ · · · ∧ Rk(xk1, . . . , xknk ). A constraint over (cid:5) isconsidered a special case of (cid:5)-formula. Like for propositional formulas, we write Vars(KB) for the set of all variablesoccurring in KB.If C = R(x1, . . . , xn) is a constraint, an assignment μ to {x1, . . . , xn} or to a superset of {x1, . . . , xn} is said tosatisfy C, or to be a model of C, if the n-tuple (μ(x1), μ(x2), . . . , μ(xn)) is in R. If KB is a (cid:5)-formula, then μ issaid to satisfy KB if it satisfies all its constraints. The notions of satisfiability, entailment and equivalence are definedlike for propositional formulas, and similarly for entailment and equivalence between a propositional formula and a(cid:5)-formula.Example 2 (continued). The set R = {0100, 0110, 0111, 1000, 1001} is a 4-ary relation, and R(cid:13) = {01, 10} and R(cid:13)(cid:13) ={00, 11} are binary relations. Thus (cid:5) = {R, R(cid:13), R(cid:13)(cid:13)} is a constraint language. Then R(x1, x2, x3, x4) ∧ R(cid:13)(cid:13)(x3, x4) isa (cid:5)-formula. It can be seen that this formula entails KB of Example 1. Now C = R(x1, x2, x3, x3) is a (cid:5)-constraint(over three variables) satisfied by exactly 010, 011, 100.The unary relations F = {0} and T = {1} have a special role for abduction problems, as well as the binary relationsR= = {00, 11} and R(cid:3)= = {01, 10}.Relations and formulas are linked to each other by the following definition.Definition 3 (representation). An n-ary relation R is said to be represented by a formula ϕ if Vars(ϕ) = {x1, . . . , xn}and ϕ ≡ R(x1, . . . , xn).Thus, slightly abusing notation, we will identify a clause (or equation) C on variables x1, . . . , xk with the k-aryrelation consisting of its set of satisfying assignments. In particular, we will often identify the literal x (resp. ¬x)and the unary constraint T(x) (resp. F(x)), so that if KB is a (cid:5)-formula and L is a set of literals, then KB ∧L isconsidered a (cid:5) ∪ {T, F}-formula.(cid:2)We also say that a relation is Horn if it can be represented by a Horn CNF. A constraint language (cid:5) is said to beHorn if every relation in (cid:5) is Horn. Thus, slightly abusing notation, we write CHorn both for the class of all Hornclauses and for the (infinite) constraint language containing all the relations represented by Horn clauses. We use thesame shorthands for the other classes of clauses and equations.1250G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Example 4 (continued). Relation R in Example 2 is represented by formula KB in Example 1. Relation R(cid:13) = {01, 10}is represented by the formula (x1 ∨ x2) ∧ (¬x1 ∨ ¬x2), and thus it is bijunctive. Observe that it is also represented bythe formula x1 ⊕ x2 = 1, and thus it is affine (of width 2).Central to our approach is the notion of a relational clone. Intuitively, the relations in the relational clone (cid:15)(cid:5)(cid:16) arethose which can be simulated by existentially quantified conjunctions of constraints over (cid:5) ∪ {R=}. We will show(in Section 5.2) that the complexity of abduction is stable under such simulations, from which it follows that thecomplexity for all (finite) constraint languages is determined by the complexity for one (finite) language in eachrelational clone.Definition 5. Let (cid:5) be a constraint language. The relational clone of (cid:5), written (cid:15)(cid:5)(cid:16), is the set of all relations R suchthat R(x1, . . . , xn) is logically equivalent to ∃V KB (where n is the arity of R) for some set of variables V disjointfrom {x1, . . . , xn} and some (cid:5) ∪ {R=}-formula KB with Vars(KB) = {x1, . . . , xn} ∪ V .Example 6. Let R be the 4-ary relation represented by the CNF (x1 ∨ x2 ∨ x3 ∨ x4) ∧ (¬x1 ∨ ¬x2 ∨ ¬x3 ∨ ¬x4), andrecall that F is the unary relation {0}. Let R(cid:13) be the ternary relation represented by the formula (x1 ∨x2)∧(x2 ⊕x3 = 0).It is easily seen that R(cid:13)(x1, x2, x3) ≡ ∃x4R(x1, x1, x2, x4) ∧ F(x4) ∧ R=(x2, x3), and thus R(cid:13) ∈ (cid:15){R, F}(cid:16).Definition 7. A constraint language (cid:5) is said to be a relational clone if (cid:5) = (cid:15)(cid:5)(cid:16).Emil Post gave a remarkable classification of all classes of Boolean functions which are closed under compositionand projection [42]. Such classes of functions are referred to as clones and, as a result of Post’s classification, theinclusion structure (under set inclusion) among Boolean clones is completely known. Later it was shown that there isa bijection between clones and relational clones [26,43] and that the inclusion structure (under set inclusion) among therelational clones follows from the inclusion structure among the clones. The classification of Boolean clones/relationalclones is called Post’s lattice and is presented in Fig. 1 in terms of relational clones. The lines in the lattice representset inclusion, i.e., a line from a relational clone ICl1 to a relational clone ICl2 lying above ICl1 in the lattice, means thatICl1 ⊆ ICl2. The dotted lines represent infinite chains of relational clones (e.g., IS20, . . . for the rightmostline).0, . . . , ISn0, IS3We give a definition of each relational clone that is relevant to our study in Table 1, taken from [11]. The otherrelational clones are not relevant in the sense that their complexity for abduction is always the same as one of itssuper-clones and one of its sub-clones.As it turns out from the results in [11], most relational clones (cid:5) correspond to a class C of clauses or equations, inthe sense that every relation in (cid:5) can be represented by a conjunction of clauses (equations) from C, and every clause(equation) from C, viewed as a relation, is in (cid:5). In particular, (cid:5) = (cid:15)C(cid:16).Thus we define relational clones in the following manner. For each entry in the table, the relational clone namedin the first column is the set of all relations which can be represented by a conjunction of clauses or equations asin the third column. For instance, the class BR is the relational clone of all relations which can be represented bya conjunction of clauses (that is, the class of all relations); IE2 is the relational clone containing all Horn relations;IL0 is the class of all relations which can be represented by a conjunction of linear equations all with 0 as their rightmember, and so on. When there is a standard name for the relational clone or class of formulas, it is given in the fourthcolumn.Only two relational clones cannot be defined in this manner. A relation is said to be complementive if for everytuple (μ1, . . . , μn) in it, the tuple (μ1 ⊕ 1, . . . , μn ⊕ 1) is also in it. Then IN2 denotes the relational clone containingall complementive relations, and IN denotes that containing all complementive and 0-valid (and thus also 1-valid)relations.Given a relational clone (cid:15)(cid:5)(cid:16) it is easy to locate (cid:15)(cid:5) ∪ {F}(cid:16) and (cid:15)(cid:5) ∪ {T}(cid:16) in Post’s lattice. This will be useful severaltimes, so we state this easy fact for future reference.Proposition 8. (cid:15)(cid:5) ∪ {F}(cid:16) is the least upper bound (in Post’s lattice) of (cid:15)(cid:5)(cid:16) and IR0. (cid:15)(cid:5) ∪ {T}(cid:16) is the least upper boundof (cid:15)(cid:5)(cid:16) and IR1.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841251Table 1Classes of clauses and equationsR. cl.BRIN2INII1II0IE2IE1IE0IEIV2IV1IV0IVID2IM2IMIS10ISk10IS12IS11ISk11ISk1IS00ISk00IS02IS01ISk01ISk0IL2IL0IL1IL3ILID1IDIR2IR1IR0IBFClassCCNF––C1vC0vCHornC1v−Horn––CdHornC1v−dHornC0v−dHorn–CbijCimpl–CIHSB−CIHSB−/kCneg,=–––CIHSB+CIHSB+/kCpos,=–––EaffE0v−affE1v−aff––Eaff /2–Cunit,=C1v−unit,=C0v−unit,=–Clauses/equationsGeneral caseComplementiveall clausessee page 1250see page 12501-valid and 0-validclauses containing at least one positive literalclauses containing at least one negative literalHorn and dual Hornclauses with at most one positive literalclauses with exactly one positive literal(¬x1 ∨ · · · ∨ ¬xn), n (cid:2) 1, (x1 ∨ ¬x2 ∨ · · · ∨ ¬xn), n (cid:2) 2(x1 ∨ ¬x2 ∨ · · · ∨ ¬xn), n (cid:2) 2clauses with at most one negative literal(¬x1 ∨ x2 ∨ · · · ∨ xn), n (cid:2) 2, (x1 ∨ · · · ∨ xn), n (cid:2) 1clauses with exactly one negative literal(¬x1 ∨ x2 ∨ · · · ∨ xn), n (cid:2) 2Bijunctive and IHS-Bclauses containing at most 2 literals(¬x1 ∨ x2), (x1), (¬x1), ⊥(¬x1 ∨ x2)(x1), (¬x1 ∨ x2), (¬x1 ∨ · · · ∨ ¬xn), n (cid:2) 0(x1), (¬x1 ∨ x2), (¬x1 ∨ · · · ∨ ¬xn), k (cid:2) n (cid:2) 0(x1), (¬x1 ∨ · · · ∨ ¬xn), n (cid:2) 0, (x1 = x2)(¬x1 ∨ x2), (¬x1 ∨ · · · ∨ ¬xn), n (cid:2) 0(¬x1 ∨ x2), (¬x1 ∨ · · · ∨ ¬xn), k (cid:2) n (cid:2) 0(x1 = x2), (¬x1 ∨ · · · ∨ ¬xn), n (cid:3) k(¬x1), (¬x1 ∨ x2), (x1 ∨ · · · ∨ xn), n (cid:2) 0(¬x1), (¬x1 ∨ x2), (x1 ∨ · · · ∨ xn), k (cid:2) n (cid:2) 0(¬x1), (x1 ∨ · · · ∨ xn), n (cid:2) 0, (x1 = x2)(¬x1 ∨ x2), (x1 ∨ · · · ∨ xn), n (cid:2) 0(¬x1 ∨ x2), (x1 ∨ · · · ∨ xn), k (cid:2) n (cid:2) 0(x1 = x2), (x1 ∨ · · · ∨ xn), n (cid:3) kAffineall linear equations(x1 ⊕ · · · ⊕ xn = 0), n (cid:2) 0(x1 ⊕ · · · ⊕ xn = a), n (cid:2) 0, a = n (mod 2)(x1 ⊕ · · · ⊕ xn = a), n even, a ∈ {0, 1}(x1 ⊕ · · · ⊕ xn = 0), n even(0 = 1), (x1 = a), (x1 ⊕ x2 = a), a ∈ {0, 1}(0 = 1), (x1 ⊕ x2 = a), a ∈ {0, 1}Unit(x), (¬x), (x1 = x2)(x), (x1 = x2)(¬x), (x1 = x2)(x1 = x2)Name–complementive–1-valid0-validHorndefinite Horn––dual Horn–definite dual Horn–bijunctiveimplicative–IHS-B−IHS-B− of width kessentially negative–––IHS-B+IHS-B+ of width kessentially positive–––affine––––affine of width 2–––––2.4. Complexity notionsWe assume that the reader is familiar with the basic notions of complexity theory, but we briefly recall the follow-ing. P is the class of decision problems solvable in deterministic polynomial time. NP is the class of decision problems1252G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Fig. 1. Post’s lattice of Boolean relational clones in the form of a Hasse diagram.solvable in nondeterministic polynomial time. (cid:2)P2istic polynomial time with access to an NP-oracle. A problem is NP-hard ((cid:2)Ppolynomial-time reducible to it. A problem is NP-complete ((cid:2)P2 -hard). Throughout the paper we assume that P, NP, and (cid:2)P(cid:2)P= NPNP is the class of decision problems solvable in nondetermin-2 ) is2 and2 -hard) if every problem in NP ((cid:2)P2 -complete) if it is in NP and NP-hard (resp. in (cid:2)P2 are pairwise distinct.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841253coNP is the dual complexity class of NP. That is, a problem is in coNP (resp. coNP-complete) if its complement isin NP (resp. NP-complete).If a problem (cid:6) can be reduced to a problem (cid:6)(cid:13) under polynomial-time many-one reductions, then we write(cid:6) (cid:3)P (cid:6)(cid:13). If (cid:6) (cid:3)P (cid:6)(cid:13) and (cid:6)(cid:13) (cid:3)P (cid:6), then we write (cid:6) ≡P (cid:6)(cid:13). We write (cid:6) (cid:3)P (cid:6)1, (cid:6)2 if (cid:6) (cid:3)P (cid:6)1 and (cid:6) (cid:3)P (cid:6)2,and dually for (cid:6)1, (cid:6)2 (cid:3)P (cid:6).3. The abduction problemWe define here the various abduction problems which we study. In order to clarify the presentation, we first definethe general abduction problem, without any restriction on hypotheses and manifestations, and then its restrictions.3.1. General abduction problemThe abduction problem with restrictions on the knowledge base only is defined as follows. Recall that classes ofclauses or equations are identified to constraint languages, so that the following definition encompasses them.Problem 9 (Abd((cid:5))). Let (cid:5) be a constraint language. An instance P of the abduction problem ABD((cid:5)) is a tuple(V , H, M, KB), where• V is a set of variables,• H ⊆ Lits(V ) is the set of hypotheses,• M is a propositional formula (the manifestation), with Vars(M) ⊆ V , and• KB is a (cid:5)-formula, with Vars(KB) ⊆ V .The question is whether there exists an explanation for P , i.e., a set E ⊆ H such that KB ∧KB ∧E entails M.(cid:2)(cid:2)E is satisfiable andWe need to make assumptions on how the relations in the (cid:5)-formula for the KB are represented. When the con-straint language (cid:5) is not specified by giving a class of equations or clauses, then we naturally assume that all therelations in all constraints in the (cid:5)-formula are given in extension. That is, by listing all the tuples belonging toeach relation explicitly. When (cid:5) is specified by giving a class of equations or clauses, then we choose to let the(cid:5)-formula be represented by a system of equations or CNF-formula, respectively. For complexity matters, the size ofP = (V , H, M, KB) is defined to be the total number of occurrences of variables in it, and additionally in the casewhere the (cid:5)-formula is given in extension, the number of tuples in the relations in the (cid:5)-formula.As an illustration, consider, e.g., the language (cid:5) = {R=, R(cid:3)=}, that is, the language containing the binary equalityand difference relations. Then an instance of ABD((cid:5)) is a tuple (V , H, M, KB), where KB is a conjunction of equalityand difference constraints over V , H is a set of literals over V , and M is a propositional formula over V . Similarly,an instance of ABD(CHorn) is a tuple (V , H, M, KB), where KB is a Horn CNF and H and M are as before.Observe that we can assume without loss of generality that KB is satisfiable in an instance. Indeed, if KB isunsatisfiable, then KB ∧E cannot be satisfiable (for any E), and thus there can be no explanation. Nevertheless,we do not enforce this assumption since satisfiable KBs cannot be distinguished from unsatisfiable ones efficiently ingeneral.(cid:2)The notion of explanation is illustrated in the following example.Example 10 (continued). Consider again KB = (x1 ∨ x2) ∧ (¬x2 ∨ x3 ∨ ¬x4) ∧ (¬x1 ∨ x2 ∨ ¬x3) ∧ (¬x1 ∨ ¬x2) ofExample 1. The tuple(cid:3)(cid:4)V = {x1, x2, x3, x4}, H = {x3, ¬x4, x4}, M = x2, KBP =is an instance of ABD(CCNF ). It has exactly three explanations, namely E1 = {x3, ¬x4}, E2 = {x3, x4}, and E3 = {x3}.1254G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284On the contrary, {x1} is not an explanation, because it is not a subset of H , {x4} is not because it does not entail Mtogether with KB, and finally {x1, x3} is not because it is not consistent with KB (and it is not a subset of H ).In the literature it is common to impose a preference relation on the different explanations in order to concentrate onthe most interesting/preferred explanations. One common preference criterion is subset minimality, i.e., an explanationE is said to be subset minimal (⊂-minimal) if there is no other explanation E(cid:13) such that E(cid:13) ⊂ E. We want to emphasizethat in this paper we do not impose any preference relation on explanations. Note that in the example above E3 ⊂E1, E2, which does not matter in our setting but would make E3 the only preferred explanation with respect to thesubset minimality criterion.3.2. Abduction under restrictionsWe now define the abduction problem under restrictions on manifestations and hypotheses. We will use the follow-ing notation for restrictions on manifestations:• POSLITS denotes the class of all positive literals,• NEGLITS denotes the class of all negative literals, and• LITS denotes the class of all literals.• Similarly, POSCLAUSES, NEGCLAUSES and CLAUSES denote classes of clauses,• POSTERMS, NEGTERMS and TERMS denote classes of terms, and• POSCNFS, NEGCNFS and CNFS denote classes of CNFs.Problem 11 (abduction under restrictions). Let (cid:5) be a constraint language, and let M be a class of proposi-tional formulas. An instance P of one of the decision problems P-ABD((cid:5),M), N-ABD((cid:5),M), V-ABD((cid:5),M), orL-ABD((cid:5),M) is an instance (V , H, M, KB) of the problem ABD((cid:5)) such that M ∈ M holds and• H ⊆ V , that is, every h ∈ H is positive, for P-Abd,• H ⊆ N(V ) (every h ∈ H is negative) for N-Abd,• H = VH ∪ N(VH ) for some VH ⊆ V (H is closed under complement) for V-Abd,• H ⊆ Lits(V ) (H is unrestricted) for L-Abd.For all four problems, the question is whether there is an explanation for P .Our motivation for studying various restrictions on the hypotheses and manifestations is to understand how theserestrictions affect the complexity of the problem. We note that as far as no restriction is imposed to the knowledgebase, generic polynomial-time reductions exist between all these restrictions. For instance, moving from a term M toa (fresh) variable m as the manifestation can be done up to adding M ↔ m, or even M → m, to the knowledge base.Similarly, a negative hypothesis of the form ¬h ∈ H can be removed by replacing ¬h with a fresh variable h(cid:13) in H ,and adding ¬h ↔ h(cid:13) to KB.This is the motivation for studying only restricted cases, such as positive hypotheses and single-variable manifesta-tions, in generic heuristic approaches to solving abduction problems (see, e.g., Marquis’ survey [37]). However, whenthe knowledge base is restricted to a particular language, as is the case here, such reductions in general fail to preservethe language. This is why we study various restrictions on H and M, and investigate their impact on the complexityof the problem. Note that in the particular case of Horn knowledge bases and single-literal manifestations, Eiter andMakino were similarly interested in the impact of the polarity imposed to the manifestation [23]. In our terms, amongother results they compared the complexity of L-ABD(CHorn,POSLITS) to that of L-ABD(CHorn,NEGLITS).Further restrictions on the problem, such as assuming that the manifestation is satisfiable or that every hypothesisoccurs in the knowledge base, are of interest for practical purposes. Nevertheless, apart from very restricted cases theydo not affect the complexity of the problem, so we only discuss them briefly in Section 14.To conclude this section, observe that we do not consider DNF manifestations, that is, disjunctions of terms asmanifestations. The reason for that is that already deciding KB |(cid:12) M, where M is a DNF formula, is coNP-completeG. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841255even if KB is empty (tautological), since then it is equivalent to deciding whether M is tautological. Consequently,there is no hope in finding interesting (tractable) classes of knowledge bases for abduction with such manifestations.4. Related work and overview of resultsIn this section we first survey relevant literature on the complexity of abduction, and then give an overview of ourresults. We finally discuss how our results may be used in several contexts.4.1. Related workThe earliest work about assumption-based propositional abduction is Reiter and de Kleer’s, in the ATMS frame-work [45]. As far as this paper is concerned, the main result there is a characterization of explanations by means ofprime implicates, reported in Section 6.2.Subsequently, in the early nineties, several results were given concerning the computational complexity of abduc-tion. Among them, Bylander et al. [7] consider set-covering abduction. That is, they assume that each hypothesiscomes with the set of atomic manifestations which it can explain, and the question is to find a set of hypotheseswhich explains all manifestations. Under various assumptions on the interaction between hypotheses (incompatibility,independence, etc.), the authors investigate the frontier between tractable and intractable such problems. Since thisobjective is similar to ours, we give a detailed comparison of our work with theirs in Section 15.2. The complexityof abduction has also been studied for other representations of the knowledge base, for example when the knowledgebase is represented by ordered binary decision diagrams [30].The first complexity results in the logic-based setting were given by Selman and Levesque [48] for abductionproblems with propositional Horn knowledge bases. Then Eiter and Gottlob performed a more systematic complexityanalysis [20]. The main results, which we will use for our classification purposes, are the following ones.Proposition 12. (See [48].) P-ABD(CHorn,POSLITS) is NP-complete. Hardness holds even if the knowledge base isalso restricted to be acyclic Horn.Proposition 13. (See [20].) The general problem ABD(CCNF ) is (cid:2)P2 -complete. Hardness holds even if all hy-potheses are positive and manifestations are restricted to positive terms, that is, P-ABD(CCNF ,POSTERMS) is also(cid:2)P2 -complete.Proposition 14. (See [20].) P-ABD(C1v−Horn,POSTERMS) is in P.Due to the intractability results of Propositions 12 and 13, several authors investigated polynomial classes of theabduction problem. Eshghi [24] gives a rather technical class based on acyclic Horn formulas (see also del Val’sdiscussion of this result [16]); however, this class is not captured by our framework. Zanuttini [51] also gives severalpolynomial classes using the notion of projection, which is presented in Section 6.3. He also gives new proofs forseveral folklore polynomial classes (discussed by, e.g., Marquis [37]) and for classes of DNF formulas (which are notcaptured by our framework). The results which are relevant here are the following ones.Proposition 15. (See [51].) V-ABD(Eaff ,CLAUSES) is in P.Proposition 16. (See [37,51].) V-ABD(Cpos,=,CLAUSES), V-ABD(Cneg,=,CLAUSES), and V-ABD(Cbij,CLAUSES)are in P.Finally, two classification results have recently been given in Schaefer’s framework, concerning some restrictionswhich we study here. We however wish to emphasize that these results were given for finite constraint languages only,whereas we are interested here in both finite constraint languages and infinite (clausal) languages.Theorem 17 (L-Abd [40]). Let (cid:5) be a finite constraint language. Then L-ABD((cid:5),TERMS) is in P if (cid:5) is in ID1.Otherwise, it is NP-complete if (cid:5) is in IE2, IV 2, ID2, or IL2. Otherwise, it is (cid:2)P2 -complete.1256G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Theorem 18 (V-Abd [12]). Let (cid:5) be a finite constraint language. Then V-ABD((cid:5),POSLITS) is in P if (cid:5) is inIL2, ID2, IS10, IS02, ISk00 (for some k), or IE1. Otherwise, it is NP-complete if (cid:5) is in IE2 or IV 2. Otherwise, it is(cid:2)P2 -complete.V-ABD((cid:5),LITS) is in P if (cid:5) is in IL2, ID2, IS12, IS02, ISkis in IE2 or IV 2. Otherwise, it is (cid:2)P2 -complete.10, or ISk00 (for some k). Otherwise, it is NP-complete if (cid:5)As concerns Theorem 18, observe in particular that there is no finite language which is in IS00 or IS01 but not01 for any k ∈ N. In fact, infinite constraint languages (cid:5) such that (cid:15)(cid:5)(cid:16) = IS01 yield NP-complete problems00 or ISkin ISk(see our Proposition 63).4.2. Overview of resultsIn this paper, building on the aforementioned results, we perform a systematic study of the computationalcomplexity of propositional abduction. This study allows us to give the complexity of problems P-ABD((cid:5),M),N-ABD((cid:5),M), V-ABD((cid:5),M), and L-ABD((cid:5),M) when• M is any of POSLITS, NEGLITS, LITS, and similarly for clauses, terms, and CNFs instead of literals,• (cid:5) is any constraint language or any clausal or equational language.To that aim, we reuse the results given in the literature for specific languages. In particular, we reuse the classifica-tions in [40] and [12]. Observe in particular that the latter concerns some maximally easy problems for us, in the sensethat no other problem which we study can be reduced to them in polynomial time in a generic manner. So it provesvery helpful for deriving hardness results. Dually, the former classification proves helpful for deriving membershipresults, as well as hardness results for CNF manifestations.Nevertheless, these classifications leave large “gaps”. For instance, if (cid:5) is a finite constraint language suchthat (cid:15)(cid:5)(cid:16) = IL2 (unrestricted affine constraints), V-ABD((cid:5),POSLITS) is in P [12] while L-ABD((cid:5),TERMS) is NP-complete [40]. Thus these classifications tell nothing about the tractability frontier between both restrictions.In this paper, for filling several such gaps we strengthen some results given in the literature. Nevertheless, we alsogive a number of brand new results.• We study the complexity of P-ABD and N-ABD for 0-valid and 1-valid languages. We exhibit new trivial andnew (cid:2)P2 -hard problems.• We study the complexity of P-ABD and N-ABD for complementive languages, and identify one minimal coNP-hard and two minimal (cid:2)P2 -hard problems.• We complete the literature on abduction with Horn and dual Horn knowledge bases (mainly Selman andLevesque’s [48] and Eiter and Gottlob’s [20] results). Our main new result is that P-ABD(CdHorn,CNFS) is in P.• We complete the literature on abduction with bijunctive and IHS-B knowledge bases. Abduction over bijunctiveand IHS-B constraint languages are particularly interesting since the borderline between tractability and NP-hardness is mainly situated among problems over such constraint languages. We identify two new polynomialproblems, namely L-ABD(Cimpl,POSCNFS) and L-ABD(Cneg,=,POSCNFS), and many new hardness results witha unified reduction from satisfiability problems.• Similarly, we complete the literature for affine knowledge bases. We give two new tractability results, namelyfor L-ABD(Eaff ,CLAUSES) and V-ABD(Eaff ,TERMS), and several new hardness results using original reductionsfrom satisfiability problems.We also wish to emphasize that as far as we know, CNF manifestations had never been studied before.These results allow us to complete the picture on the complexity of propositional abduction, and in particular toidentify the tractability frontier (between P and NP-hard). This frontier, as we discuss in Section 15, can be char-acterized by very simple conditions. Moreover, it parallels the frontier identified by Bylander et al. for set-coveringabduction [7].G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–128412574.3. ApplicationsAs already evoked, our results are essentially interesting from a complexity-theoretic and from an AI point of view.On the complexity-theoretic side, we give the complexity of abduction for various, fine-grained restrictions, and itturns out that abduction is always either in P, NP-complete, coNP-complete, or (cid:2)P2 -complete. This is interesting sinceLadner’s result states that if P (cid:3)= NP, then there exist problems in NP that are neither in P nor NP-complete [33]. Suchproblems are said to be of intermediate complexity.One way to interpret our results is that the infinite class of abduction problems that we study do not containsuch problems of intermediate complexity. For further discussions on the complexity-theoretic topic of finding largesubclasses of NP which do not contain problems of intermediate complexity, we refer the reader to Feder and Vardi’sseminal paper [25].Finally, since the complexity of abduction spans four classes (P, NP, coNP, and (cid:2)P2 ) and because abduction is acentral problem in nonmonotonic reasoning (see, e.g., [5,37]), our results may serve as starting points for derivingcomplexity results for other problems, by using reductions to or from abduction.From the AI point of view, our results may help the designers of knowledge-based agents or expert systems tochoose the appropriate knowledge representation language. Indeed, depending on the application, and especially onthe constraints on resolution of abduction problems, it might be necessary to ensure that such problems will be solvedefficiently, or it may be acceptable that they are NP-hard.2 Moreover, depending on the application, the sets of hy-potheses and the manifestations may be restricted to particular classes. Then, using our results and the characteristicsand requirements of the application, the designer of a knowledge-based system can choose the appropriate knowl-edge representation language. In particular, she might choose the most expressive one while respecting the tractabilityconstraints.We acknowledge that our restrictions on knowledge bases cannot capture all possible propositional knowledgerepresentation languages. For instance, they cannot capture the class of Horn-renamable CNF formulas, or that ofacyclic Horn formulas, since they impose global restrictions on formulas.Nevertheless, classes defined by local properties are very important for knowledge representation. Indeed, theyare stable under conjunction, that is, if KB1 and KB2 are in one of these classes, then so is KB1 ∧ KB2. This makesthem suitable for merging mutually consistent theories without losing computational properties of each (simply con-junct both) and is important for knowledge approximation purposes, since such classes define a unique least upperbound [47]. Moreover, they have been given complete pictures of complexity for various reasoning tasks (e.g., in-ference under circumscription [39] and several tasks in default reasoning [9]), which allows to choose a knowledgerepresentation language under constraints stemming from several tasks. Last but not least, the relational clone gener-ated by any constraint or clausal/equational language can be recognized in polynomial time [11]. Thus an agent maymake decisions depending on its current knowledge base. It can, for instance, decide not to try to find an exact solutionto a planning problem through abduction because its current knowledge base is not tractable for it, and adopt anotherstrategy, such as approximate planning.5. Reductions between abduction problemsWe have defined restrictions on the abduction problem along three dimensions: 12 different types of manifestations,4 different types of hypotheses, and an infinite number of restrictions over knowledge bases. Since our goal is to givea complexity classification of the problem for each combination of restrictions, this section explains how to restrict toa finite and limited number of cases.5.1. Reductions between restrictions on manifestations and hypothesesThe following reductions are obvious.2 Note that even if the problem at hand is NP-hard, or even (cid:2)Pinstance, using state-of-the-art solvers for Quantified Boolean formulas, which is the approach in [19].2 -hard, most instances may be solved efficiently in practice. This may be done, for1258G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Lemma 19. Let (cid:5) be a constraint language and let M and M(cid:13) be classes of propositional formulas such thatM ⊆ M(cid:13). Then L-ABD((cid:5),M) (cid:3)PL-ABD((cid:5),M(cid:13)). The same result holds for P-ABD, N-ABD, or V-ABD instead ofL-ABD.Lemma 20. Let (cid:5) be a constraint language, and let M be a class of propositional formulas. Then P-ABD((cid:5),M),N-ABD((cid:5),M), V-ABD((cid:5),M), (cid:3)PL-ABD((cid:5),M).5.2. Reductions between restrictions on knowledge basesIn this section we show how Post’s lattice can be used to reduce the number of restrictions on knowledge basesthat need to be considered when classifying the complexity of the abduction problem. This approach via Post’s latticeis crucial for obtaining our complexity classifications. We conclude this section by demonstrating how the approachcan be used to classify the complexity of P-ABD((cid:5),TERMS) for every constraint language (cid:5) and every clausal orequational language C.The key for the approach via Post’s lattice is Lemma 22, which states that ABD((cid:5)(cid:13)) (cid:3)PABD((cid:5)) whenever (cid:5)(cid:13) is afinite and (cid:5)(cid:13) ⊆ (cid:15)(cid:5)(cid:16) (for all restrictions over hypotheses and manifestations considered here). Hence, when studyingthe complexity of the abduction problem for finite constraint languages (cid:5), it is enough to consider one generatingconstraint language per relational clone. In particular, if (cid:5) and (cid:5)(cid:13) are two finite constraint languages and (cid:15)(cid:5)(cid:16) = (cid:15)(cid:5)(cid:13)(cid:16),then ABD((cid:5)) and ABD((cid:5)(cid:13)) are polynomial-time equivalent to each other.We first need the following lemma, which allows to get rid of equality relations in knowledge bases.Lemma 21. Let (cid:5) be a constraint language, and let M be any class of manifestations considered in this paper. ThenL-ABD((cid:5) ∪ {R=},M) (cid:3)PL-ABD((cid:5),M). The same holds for V-ABD, P-ABD, or N-ABD instead of L-ABD.Proof. Let (V , H, M, KB) be an instance of L-ABD((cid:5) ∪ {R=},M). Build a knowledge base over (cid:5) from KB in thefollowing manner. For all constraints R=(xi, xj ) in KB replace every occurrence of xj with xi in KB, H , M, removethe constraint from KB, and remove xj from V . Perform this identification iteratively, until KB does not contain anyequality constraint any more. Clearly, this transformation can be performed in polynomial time and preserves theexistence of an explanation. Finally, it is easily seen that it preserves any restriction on hypotheses and manifestationsin the statement. (cid:2)We can now give the central lemma of our study. The proof checks that additional variables introduced whilereplacing a constraint in (cid:5)(cid:13) with its equivalent expression over (cid:5) do not affect the existence of a solution.Lemma 22. Let (cid:5) be a constraint language, and let (cid:5)(cid:13) ⊆ (cid:15)(cid:5)(cid:16) be a finite constraint language (or a finite clausalor equational language). Let M be a class of propositional formulas. Then L-ABD((cid:5)(cid:13),M) (cid:3)PL-ABD((cid:5),M). Thesame holds for V-ABD, P-ABD, or N-ABD instead of L-ABD, and for a class of clauses or equations C instead of aconstraint language (cid:5).= {x(cid:13)Proof. Let P (cid:13) = (V (cid:13), H (cid:13), M (cid:13), KB(cid:13)) be an instance of L-ABD((cid:5)(cid:13),M). Write KB(cid:13) =) and forall i ∈ I , V (cid:13)}. By the definition of a relational clone we know that for all i ∈ I , the constraintiR(cid:13)i,1, . . . , x(cid:13)i(x(cid:13)= ∅ and KBi is a (cid:5) ∪ {R=}-i,kiformula over Vi ∪ V (cid:13)i . Importantly, for all i ∈ I we assume Vi ∩ V (cid:13) = ∅ and for all i, i(cid:13) ∈ I with i (cid:3)= i(cid:13), we assumeVi ∩ Vi(cid:13) = ∅, i.e., all existentially quantified variables are fresh with respect to V (cid:13) and different from each other. Thisis without loss of generality since the names of these variables are unconstrained.i,1, . . . , x(cid:13)i,ki) of KB(cid:13) is logically equivalent to some formula ∃ViKBi where Vi ∩ V (cid:13)ii,1, . . . , x(cid:13)i,kii∈I R(cid:13)i(x(cid:13)We define an instance P = (V , H, M, KB) of ABD((cid:5) ∪ {R=}) by(cid:2)i∈I Vi ,(cid:5)• V = V (cid:13) ∪• H = H (cid:13),• M = M (cid:13), and(cid:2)• KB =i∈I KBi .G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841259Table 2Minimal set of results for P-ABD((cid:5),TERMS)R. Clone/classCCNF (BR)II0IN2C1v (II1)INCHorn (IE2), Eaff (IL2), Cbij (ID2)IS21IL0IL3C1v−Horn (IE1), E1v−aff (IL1)CdHorn (IV2)Eaff /2 (ID1)Complexityin (cid:2)P2(cid:2)P2 -hard(cid:2)P2 -hardin coNPcoNP-hardin NPNP-hardNP-hardNP-hardin Pin Pin PResultProposition 41Proposition 46Proposition 47Proposition 44Proposition 48Proposition 35Proposition 62 (dual)Proposition 69Proposition 70Proposition 44Proposition 52Theorem 17In other words, we simply replace every constraint in KB(cid:13) with its expression over (cid:5) ∪ {R=}, and we forget exis-tential quantification. Clearly, this can be performed in linear time since languages are fixed and (cid:5)(cid:13) is finite, and thusall expressions of relations in (cid:5)(cid:13) can be stored in a lookup table once and for all.(cid:2)(cid:2)We now claim that P and P (cid:13) have exactly the same explanations. First, let E(cid:13) be an explanation for P (cid:13). Then KB(cid:13) ∧E(cid:13) is satisfiable. Since all existentially quantified variablesE(cid:13) is satisfiable, thus the formulaE(cid:13) is satisfiable. Now assume thatE(cid:13) ∧ ¬M is satisfiable. By the assumption on fresh variables again, itE(cid:13) ∧ ¬M is satisfiable. Since M = M (cid:13), thisare fresh with respect to V (cid:13) and different from each other, it follows that KB ∧KB ∧follows thatcontradicts the fact that E(cid:13) is an explanation for P (cid:13).E(cid:13) ∧ ¬M is satisfiable, i.e., that KB(cid:13) ∧E(cid:13) does not entail M. Then KB ∧∃ViKBi ∧(cid:2)∃ViKBi ∧Thus every explanation E(cid:13) for P (cid:13) is an explanation for P , and the proof is similar for showing the converse. Wei∈Ii∈I(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)conclude by invoking Lemma 21 for getting rid of equality constraints. (cid:2)It is important to note that the reduction in the proof above preserves all the restrictions on hypotheses and manifes-tations considered in this paper. For example, if we reduce from a V-ABD((cid:5)(cid:13),POSLITS) instance, then the resultinginstance will be a V-ABD((cid:5),POSLITS) instance. Thus, using Lemma 22, we will be able to give the complexity of allthe restrictions on the ABD problem for any finite constraint language (Section 13) by considering the complexity ofonly one language per relational clone.We now demonstrate the use of Post’s lattice for classifying the complexity of ABD((cid:5)) problems by giving thecomplete classification of P-ABD((cid:5),TERMS) for every constraint language (cid:5) and class of clauses/equations C. Thereasoning is similar for the other combinations of restrictions on hypotheses and manifestations.Gathering together results for positive hypotheses and terms as manifestations from subsequent sections in thepaper, and applying the (obvious) reductions in Section 5.1, we get the minimal set of results in Table 2. Theseresults are minimal in the sense that they are irredundant with respect to reductions of the form P-ABD((cid:5),TERMS)(cid:3)PP-ABD((cid:5)(cid:13),TERMS) as soon as (cid:5) ⊆ (cid:15)(cid:5)(cid:13)(cid:16). In this table, for each clausal language C, we give inside parentheses thecorresponding relational clone, i.e., the relational clone ICl such that ICl = (cid:15)C(cid:16).The complete picture of complexity can now be obtained as follows.First, as explained in Section 13.2, a upper bound for the complexity of a clausal or equational language C corre-sponding to a relational clone ICl carries over to every language (cid:5) such that (cid:15)(cid:5)(cid:16) ⊆ ICl. Now, since all lower boundsare given for finite languages, the results on Fig. 2 follow from the results in Table 2, where• every result reported from the table is given by a bolded circle, and• every other result follows from these together with reductions of the form P-ABD((cid:5),TERMS) (cid:3)PP-ABD((cid:5)(cid:13),TERMS) as soon as (cid:5) ⊆ (cid:15)(cid:5)(cid:13)(cid:16) (Lemma 22).Consequently, we have the complexity for every constraint language. For more details, we refer the reader toSection 13.2.1260G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Fig. 2. Results for P-ABD((cid:5),TERMS), white is in P, light grey is NP-complete, mid grey is coNP-complete, dark grey is (cid:2)P2 -complete.5.3. Exploiting the symmetry between 0 and 1We now show how to exploit the symmetry between 0 and 1 (or positive and negative) in order to reduce thenumber of cases that need to be considered. We will use the notion of duality.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841261Definition 23 (dual). The dual of a clause C = ((cid:3)1 ∨ · · · ∨ (cid:3)k) is the clause Cd = ((cid:3)1 ∨ · · · ∨ (cid:3)k). The dual of a class ofclauses C is the class Cd = {Cd | C ∈ C}. The dual of an equation Eqn = (x1 ⊕ · · · ⊕ xn = a) is Eqnd = Eqn if n is even,and Eqnd = (x1 ⊕· · ·⊕xn = a ⊕1) otherwise. The dual of an n-ary Boolean relation R is Rd = {(μ1 ⊕1, . . . , μn ⊕1) |(μ1, . . . , μn) ∈ R}. The dual of a constraint language (cid:5) is (cid:5)d = {Rd | R ∈ (cid:5)}.Observe that in the graphical representation of Post’s lattice of Boolean relational clones (Fig. 1), the dual ofa relational clone is simply its mirror image with respect to the vertical line through the center of the lattice [4].Moreover, it is easy to see that if a relation R is represented by a conjunction of clauses (or equations) C1 ∧ · · · ∧ Ck,then Rd is represented by Cd1∧ · · · ∧ Cdk .Example 24 (continued). The dual of clause C = (x1 ∨ x2 ∨ ¬x3) is Cd = (¬x1 ∨ ¬x2 ∨ x3). The dual of equationEqn = (x1 ⊕ x2 = 1) is Eqn itself, and that of Eqn(cid:13) = (x1 ⊕ x2 ⊕ x3 = 0) is (Eqn(cid:13))d = (x1 ⊕ x2 ⊕ x3 = 1). The dual ofrelation R = {000, 010, 111} is Rd = {111, 101, 000}. As for languages, we have, e.g., Cd= Eaff /2and (ISk= CdHorn, E daff /2Horn00)d = ISk10.These definitions allow us to state the following easy equivalences between problems which are in some sensesymmetric to each other. The intuition is simply that switching the polarity of all literals in all components of theinstance and replacing all relations (clauses/equations) by their duals preserve the existence of explanations.Lemma 25. Let (cid:5) be a constraint language or a class of clauses/equations. Then the following equivalences hold:• P-ABD((cid:5),POSLITS) ≡PN-ABD((cid:5)d ,NEGLITS),• N-ABD((cid:5),POSLITS) ≡PP-ABD((cid:5)d ,NEGLITS),• V-ABD((cid:5),POSLITS) ≡PV-ABD((cid:5)d ,NEGLITS), and• L-ABD((cid:5),POSLITS) ≡PL-ABD((cid:5)d ,NEGLITS).They also hold when the manifestations are restricted to positive or negative clauses, terms, or CNFs instead ofliterals.Consequently, in the rest of the paper we will only consider positive and unrestricted manifestations. The complex-ity for negative manifestations can be derived using Lemma 25.Similarly, the lack of positive/negative polarity for manifestations and/or hypotheses allows to reduce the numberof cases to consider. The following lemma is straightforward.Lemma 26. Let (cid:5) be a constraint language or a class of clauses or equations, and let M be any of LITS, CLAUSES,TERMS, or CNFS. Then,• N-ABD((cid:5),M) ≡P P-ABD((cid:5)d ,M),• L-ABD((cid:5),M) ≡P L-ABD((cid:5)d ,M), and• V-ABD((cid:5),M) ≡P V-ABD((cid:5)d ,M).5.4. Other reductionsThe following lemma allows to impose polarities to hypotheses and manifestations when the language contains thedisequality relation R(cid:3)= = {01, 10}.Lemma 27. Let (cid:5) be a constraint language such that R(cid:3)= ∈ (cid:15)(cid:5)(cid:16), and let M be either POSLITS or NEGLITS. ThenL-ABD((cid:5),LITS) (cid:3)P P-ABD((cid:5),M), N-ABD((cid:5),M). The same result holds for clauses, terms, or CNFs instead ofliterals.Proof. Consider an instance (V , H, M, KB) of L-ABD((cid:5),TERMS). The desired instance of P-ABD((cid:5),POSTERMS)is simply obtained by introducing a fresh variable x(cid:13) for any negative literal ¬x in H or M, replacing ¬x with x(cid:13) and1262G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284adding the constraint R(cid:3)=(x, x(cid:13)) to KB. Finally, Lemma 22 allows to assume R(cid:3)= ∈ (cid:15)(cid:5)(cid:16) instead of the stronger R(cid:3)= ∈ (cid:5).The reasoning is similar for the other restrictions. (cid:2)The two following lemmata concern conjunctive manifestations. The proof of the first one follows firstly from thedefinition of the abduction problem.Lemma 28. Let P = (V , H, M, KB) be an instance of an abduction problem, where M = (ϕ1 ∧ · · · ∧ ϕp) is a con-junction of formulas, and let E ⊆ H . Then E is an explanation for P if and only if for all i ∈ {1, . . . , p} it is anexplanation for (V , H, ϕi, KB).Lemma 29. Let P = (V , H, M, KB) be an instance of an abduction problem where M is a term. Let (cid:3) be a literal(cid:3)(cid:13)∈M (cid:3)(cid:13)). Then P has an explanation if andformed upon a fresh variable m /∈ V , and write C for the clause ((cid:3) ∨only if P (cid:13) = (V ∪ {m}, H, (cid:3), KB ∧ C) has one.(cid:6)Proof. Assume first that P (cid:13) has an explanation E(cid:13). We first show that KB ∧E(cid:13) ∧ (contrary that there is a model μ of KB ∧extending μ by μ |(cid:12) (cid:3) yields a model of KB ∧ C ∧(cid:2)E(cid:13) entailsfor P (cid:13). Thus KB ∧satisfiable. Finally, E(cid:13) is an explanation for P .(cid:3)(cid:13)∈M (cid:3)(cid:13). Assume to the(cid:6)(cid:3)(cid:13)∈M (cid:3)(cid:13) ⊂ C,(cid:3)(cid:13)∈M (cid:3)(cid:13)). Then since m occurs only in C and(cid:2)E(cid:13) ∧ (cid:3), which contradicts the fact that E(cid:13) is an explanation(cid:2)E(cid:13) is(cid:3)(cid:13)∈M (cid:3)(cid:13), that is, M. Now since KB ∧ C ∧E(cid:13) is satisfiable, a fortiori KB ∧E(cid:13) entails(cid:2)(cid:2)(cid:6)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)Conversely, assume that P has an explanation E. Then by definition of an explanation, there is a model μ ofE. Now we also haveE; thus μ(cid:13) defined to agree with μ over V and to satisfy (cid:3) is a model of KB ∧ C ∧E |(cid:12) M. It follows that KB ∧ C ∧E |(cid:12) (cid:3), and finally, E is an explanation for P (cid:13). (cid:2)KB ∧KB ∧(cid:2)(cid:2)6. Tools and methods for studying abductionIn this section we present the main generic methods used in the literature for studying abduction problems from acomputational point of view.6.1. Complexity of satisfiability and deductionBy definition, solving an abduction problem involves solving a satisfiability and a deduction problem. We thereforerecall some definitions and well-known complexity results about these two problems.Problem 30 (Sat((cid:5))). Let (cid:5) be a constraint language. An instance P of SAT((cid:5)) is a knowledge base KB over (cid:5), andthe question is whether there exists at least one model of KB.Problem 31 (Deduction((cid:5),M)). Let (cid:5) be a constraint language and let M be a class of formulas. An instance P ofDEDUCTION((cid:5),M) is a tuple (KB, Q), where• KB is a knowledge base over (cid:5), and• Q is a formula in M.The question is whether KB entails Q.Schaefer classified the complexity of SAT((cid:5)) for all possible finite constraint languages (cid:5) [46]. Together withwell-known results about the infinite languages studied here (see in particular [17] and [1]) we have the followingtheorem.Theorem 32 (complexity of Sat). Let (cid:5) be a finite constraint language. Then SAT((cid:5)) is in P if (cid:5) ⊆ II0, (cid:5) ⊆ II1,(cid:5) ⊆ IE2, (cid:5) ⊆ IV 2, (cid:5) ⊆ ID2, or (cid:5) ⊆ IL2. Otherwise, it is NP-complete. Moreover, SAT(C) is in P when C is one ofCHorn, CdHorn, Cbij, Eaff , C0v, and C1v.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841263From the well-known facts that KB entails Q if and only if KB ∧ ¬Q is unsatisfiable, and that KB entails Q1 ∧ Q2if and only if it entails Q1 and it entails Q2, we have the following two corollaries of Theorem 32, which we will usein the paper.Theorem 33 (upper bounds for Deduction). Let C be a class of clauses or equations. Then DEDUCTION(C,CNFS) isin coNP. Moreover, DEDUCTION(C,CNFS) is in P when C is one of CHorn, CdHorn, Cbij, and Eaff .Theorem 34 (lower bound for Deduction). Let (cid:5) be a constraint language with (cid:15)(cid:5)(cid:16) = II1. Then DEDUCTION((cid:5),POS-LITS) is coNP-hard.Proof. It follows from Post’s lattice and Proposition 8 that (cid:15)II1 ∪ {F}(cid:16) = BR. Thus from Theorem 32 it follows thatSAT(II1 ∪ {F}) is NP-complete. Let KBII1∪{F} = KBII1i∈I F(xi) be a knowledge base over II1 ∪ {F}, where KBII1 isa knowledge base over II1 and I is nonempty (clearly, the problem remains NP-complete even with this assumption).Now let i0 ∈ I , and let KB be the knowledge base obtained from KBII1 by replacing every occurrence of xi for somei ∈ I \ {i0} with xi0 . By construction, KB is a knowledge base over II1, and KB ∧ F(xi0) is satisfiable if and only ifKBII1∪{F} is. But KB ∧ F(xi0) is unsatisfiable if and only if KB entails xi0 , which concludes the proof. (cid:2)(cid:2)∧Schaefer languages are precisely those maximal languages (for inclusion) for which deduction of clauses istractable, i.e., CHorn, CdHorn, Cbij and Eaff . From the results above we immediately get the following result.Proposition 35. For any type of restriction on hypotheses and manifestations considered in this paper and for allSchaefer languages C, ABD(C) is in NP.6.2. Prime implicatesThe notion of a prime implicate is widely used for studying various computational problems in propositional logic,especially for problems in nonmonotonic reasoning. The relevance of this notion to abduction has been first pointedout by Reiter and de Kleer [45]. Marquis [37] gives a survey of the various notions of prime implicates, their use fornonmonotonic reasoning (including abduction), and methods for computing them.Definition 36 (prime implicate). Let KB be a propositional formula or a conjunction of constraints. A clause C is saidto be a prime implicate of KB if KB entails C but no proper subclause of it.The following characterization of explanations, first shown by Reiter and de Kleer in the ATMS setting [45], willbe of great use to us.Lemma 37. (See [45].) Let P = (V , H, M, KB) be an instance of an abduction problem, where M = (m1 ∨ · · · ∨ mp)is a nonempty clause, and let E ⊆ H . Then E is an explanation for P if and only if there is a prime implicate of KB of∨ · · · ∨ mjs ) with {(cid:3)1, . . . , (cid:3)r } ⊆ E, {j1, . . . , js} ⊆ {1, . . . , p} and {j1, . . . , js} (cid:3)= ∅ (withthe form ((cid:3)1 ∨ · · · ∨ (cid:3)r ∨ mj1possibly (cid:3)i = mj for some i, j ).Finally, recall from Quine’s result [44] that all the prime implicates of a CNF KB can be generated by resolution,i.e., by repeatedly adding C1 ∨ C2 to KB if there are a variable x and two clauses of the forms (x ∨ C1) and (¬x ∨ C2)in KB, and removing clauses which are tautological or include others.6.3. ProjectionWe will also use the notion of projection as a tool for studying the abduction problem. This notion is similar tothe well-known notion of elimination of middle terms, or existential abstraction, and its use for abduction has beenproposed in [51]. As for the complexity of computing projection in propositional logic, we refer the reader to [34].Intuitively, projecting onto a set of variables V (cid:13) amounts to existentially quantifying every other variable.1264G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Definition 38 (projection of assignments). Let μ be an assignment to a set of variables V , and let V (cid:13) ⊆ V . Theprojection of μ onto V (cid:13), denoted by μ|V (cid:13) , is the assignment to V (cid:13) which agrees with μ.Definition 39 (projection of formulas). Let KB be a propositional formula or a conjunction of constraints, and letV (cid:13) ⊆ Vars(KB). A projection of KB onto V (cid:13) is any knowledge base KB(cid:13) with Vars(KB(cid:13)) ⊆ V (cid:13) and whose set of modelsover V (cid:13) is {μ|V (cid:13) | μ |(cid:12) KB}.Importantly, the projection of a formula is unique only up to logical equivalence. We will mainly use the followingresult. Its proof follows from Lemma 37 when the manifestation is a literal or clause, since it is well-known that aprojection of a knowledge base KB onto a set of variables preserves the prime implicates of KB over this set [34,Proposition 16]. When the manifestation is a term of CNF, the proof follows from the literal or clause case togetherwith Lemma 28.Lemma 40. Let P = (V , H, M, KB) be an instance of any abduction problem. Let V (cid:13) be any set of variables withVars(H ) ∪ Vars(M) ⊆ V (cid:13) ⊆ V , and let KB(cid:13) be a projection of KB onto V (cid:13). Then the explanations for (V (cid:13), H, M, KB(cid:13))are exactly the explanations for P .When computable efficiently, projection used as above allows to circumvent the difficulty of what Selman andLevesque call the support selection task [48]. They argue that this task lies at the core of the computational difficultyof abduction, as witnessed by their study of the Horn case. We come back to this issue in our discussion (Section 15).One case when a projection of a knowledge base can be computed efficiently is when this knowledge base hasa polynomial number of prime implicates, all of which can be enumerated efficiently. This is so, e.g., for bijunctiveknowledge bases, which allows us to derive most results in Section 11. However, this is not the only case, as the affinecase shows (Section 12).7. General caseIn this section we only strengthen Eiter and Gottlob’s statements [20] a little and adapt their proofs to our frame-work.Proposition 41. (Adapted from [20].) L-ABD(CCNF ,CNFS) is in (cid:2)P2 .Proof. Guess an explanation E ⊆ H and check that KB ∧Now, check that KB ∧(cid:2)P(cid:2)2 . (cid:2)E is satisfiable. This verification is in NP by Theorem 32.E |(cid:12) M. This verification is in coNP by Theorem 33. Hence, the problem is in NPNP∪coNP =(cid:2)Proposition 42. (Adapted from [20].) Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = BR. Then P-ABD((cid:5),POSLITS)and N-ABD((cid:5),POSLITS) are (cid:2)P2 -hard.Proof. Eiter and Gottlob [20] show that P-ABD(CCNF ,POSLITS) is (cid:2)P2 -complete. Let (cid:5)3 be the constraint languagecontaining all ternary relations that are the set of models of exactly one clause. It is well known that every CNF islogically equivalent to a 3CNF formula with existentially quantified auxiliary variables. Reasoning as for Lemma 22,we get that P-ABD(CCNF ,POSLITS) ≡PP-ABD((cid:5)3,POSLITS), and thus P-ABD((cid:5)3,POSLITS) is (cid:2)P2 -complete. Now,since (cid:15)(cid:5)3(cid:16) = BR, we get that P-ABD((cid:5),POSLITS) is (cid:2)P2 -complete.The claim for N-ABD((cid:5),POSLITS) follows since any positive hypothesis h can be changed to a negative one ¬h(cid:13),where h(cid:13) is a fresh variable, up to adding (h ∨ h(cid:13)) ∧ (¬h ∨ ¬h(cid:13)), i.e., h ↔ ¬h(cid:13), to KB. (cid:2)Note that, in particular, the hardness result in the preceding proposition holds for any finite constraint language (cid:5)such that (cid:15)(cid:5)(cid:16) = BR. We want to emphasize that, unless explicitly stated otherwise, all the hardness results in the paperfor constraint languages (cid:5) hold for any finite constraint language (cid:5)(cid:13) such that (cid:15)(cid:5)(cid:13)(cid:16) = (cid:15)(cid:5)(cid:16). This is important for ussince we can only use Lemma 22 to derive new hardness results if the original (hard) abduction problem is definedover a finite constraint language.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–128412658. 0-valid and 1-valid languagesThe “easy” abduction problems which we exhibit in this section are of a particular type. Indeed, for them the searchspace can be reduced to only one candidate explanation. The reasoning is similar to that for the definite Horn case(see, e.g., [20, Corollary 5.4]).Lemma 43. Let P = (V , H, M, KB) be an instance of any abduction problem. If KB ∧H |(cid:12) M.an explanation if and only if KB ∧(cid:2)(cid:2)H is satisfiable, then P has(cid:2)H is satisfiable and KB ∧(cid:2)(cid:2)H (cid:3)|(cid:12) M. Then there is a model μ of KB ∧(cid:2)Proof. Obviously, if KB ∧KB ∧μ (cid:3)|(cid:12) M, hence KB ∧(cid:2)E (cid:3)|(cid:12) M and hence, E is not an explanation. (cid:2)H |(cid:12) M, then E = H is an explanation. Conversely, assumeE andH such that μ (cid:3)|(cid:12) M; then, for any E ⊆ H , μ |(cid:12) KB ∧(cid:2)As a direct consequence of Lemma 43, Theorems 32 and 33, we have the following results. The algorithm simply(cid:2)consists of deciding whether KB ∧H entails M.Proposition 44 (1-valid). P-ABD(C1v ,CNFS) is in coNP, and P-ABD(C1v−Horn,CNFS), P-ABD(E1v−aff ,CNFS) arein P.We now give two new results, which give some upper and lower bounds, respectively, for the complexity of P-ABDand N-ABD.Proposition 45. The problem N-ABD(C0v ,POSCNFS) is trivial, in the sense that an instance (V , H, M, KB) has anexplanation if and only if M is empty.Proof. If M is empty, then it is tautological, thus KB entails M. It follows that there is an explanation if and only ifKB is satisfiable, which is necessarily the case since it is 0-valid.Now if M is nonempty, let μ0 be the assignment of 0 to every variable in V . Since KB is 0-valid and H is a set ofE for any E ⊆ H . But since M is positive but not tautological, we also have(cid:2)negative literals, we have μ0 |(cid:12) KB ∧μ0 (cid:3)|(cid:12) M. It follows that for all E ⊆ H we have KB ∧(cid:2)E (cid:3)|(cid:12) M and hence, no E ⊆ H can be an explanation. (cid:2)Proposition 46. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = II0. Then P-ABD((cid:5),POSLITS) is (cid:2)Pif (cid:5) is a constraint language satisfying (cid:15)(cid:5)(cid:16) = II1, then N-ABD((cid:5),POSLITS) is (cid:2)P2 -hard.2 -hard. Similarly,Proof. Let (cid:5) be a constraint language such that (cid:15)(cid:5)(cid:16) = II0. By Post’s lattice and Proposition 8, (cid:15)(cid:5) ∪ {T}(cid:16) = BR andthus, by Proposition 42 P-ABD((cid:5) ∪ {T},POSLITS) is (cid:2)P2 -complete. We give a reduction of this latter problem toP-ABD((cid:5),POSLITS).(cid:2)To this aim, let P (cid:13) = (V (cid:13), H (cid:13), m(cid:13), KB(cid:13)) be an instance of P-ABD((cid:5) ∪ {T},POSLITS). Write KB(cid:13) = KB(cid:5) ∧T(x), where KB(cid:5) is a conjunction of constraints over (cid:5) and VT is a set of variables. We assume VT (cid:3)= ∅ withoutx∈VTloss of generality. Then we define KB to be any conjunction of constraints (possibly with existentially quantified aux-(cid:2)(¬m(cid:13) ∨ x); such a formula exists because (cid:15)(cid:5)(cid:16) = II0 isiliary variables) over (cid:5) and logically equivalent to KB(cid:5) ∧the set of all 0-valid relations and (¬m(cid:13) ∨ x) is 0-valid. We also define H = H (cid:13) ∪ VT, and P = (V (cid:13), H, m(cid:13), KB). Then,clearly P has an explanation if and only if P (cid:13) has one, since KB is logically equivalent to KB(cid:5) ∧ (m(cid:13) →T(x))(the reasoning is similar to that in [12, Lemma 19]). (cid:2)x∈VTx∈VT(cid:2)9. Complementive languagesWe give two new lower bounds.Proposition 47. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IN2. Then P-ABD((cid:5),POSLITS) and N-ABD((cid:5),POSLITS) are (cid:2)P2 -hard.1266G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12842 -complete. Now V-ABD((cid:5),POSLITS) (cid:3)PL-Proof. We know from Theorem 18 that V-ABD((cid:5),POSLITS) is (cid:2)PABD((cid:5),LITS) (Lemmata 19 and 20), and since R(cid:3)= ∈ IN2, we have L-ABD((cid:5),LITS) (cid:3)P P-ABD((cid:5),POSLITS),N-ABD((cid:5),POSLITS) (Lemma 27), which concludes the proof. (cid:2)Proposition 48. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IN. Then P-ABD((cid:5),POSLITS) is coNP-hard.Proof. We know from Post’s lattice and Proposition 8 that (cid:15)(cid:5) ∪ {T}(cid:16) = II1. Thus it follows from Theorem 34 thatDEDUCTION((cid:5) ∪ {T},POSLITS) is coNP-hard. We give a reduction of this latter problem to P-ABD((cid:5),POSLITS).(cid:2)Let (V , KBT, q) be an instance of DEDUCTION((cid:5) ∪ {T},POSLITS), where q ∈ V , and write KBT = KB ∧T(x), where KB is a knowledge base over (cid:5). Now define an instance P of P-ABD((cid:5),POSLITS) by P =(cid:2)x∈VTH is satisfiable, and thus, by Lemma 43, P has an(V , H = VT, q, KB). Since KB is 1-valid we have that KB ∧explanation if and only if KB ∧H entails q, i.e., if and only if KBT entails q. (cid:2)(cid:2)10. Horn and dual Horn languagesIn this section we build over a number of results given in the literature for Horn knowledge bases, mainly bySelman and Levesque [48], Eiter, Gottlob, and Makino [20,23], and Khardon and Roth [32].Lemma 49. (See [32] and [50, Lemma 1].) Let P = (V , H, M, KB) be an instance of ABD(CHorn), where M ∈POSLITS ∪ POSCLAUSES ∪ POSTERMS ∪ POSCNFS. Then P has an explanation if and only if it has a positive one.Corollary 50. Let (cid:5) be a Horn language, and let M be POSLITS, POSCLAUSES, POSTERMS, or POSCNFS. ThenL-ABD((cid:5),M) ≡P V-ABD((cid:5),M) ≡P P-ABD((cid:5),M).Proposition 51. (Adapted from [20, Corollary 5.4].) L-ABD(C1v−Horn,POSCNFS) is in P.Proof. Let P = (V , H, M, KB) be an instance. From Lemma 49 it follows that P has an explanation if and only if(H ∩ V ) is satisfiable. Thus, by Lemma 43 P (cid:13) hasP (cid:13) = (V , H ∩ V , M, KB) has one. Now KB is 1-valid, thus KB ∧an explanation if and only if KB ∧(H ∩ V ) entails M, which can be decided in polynomial time since KB is Horn(Theorem 33). (cid:2)(cid:2)(cid:2)The following result is new and gives quite a broad class of tractable abduction problems. Observe that by duality,it also shows that it is tractable to decide whether a CNF has a negative explanation with respect to a Horn knowledgebase.Proposition 52. P-ABD(CdHorn,CNFS) is in P.Proof. Let P = (V , H, M, KB) be an instance. We assume without loss of generality that KB is satisfiable (see theend of Section 3.1). Let H (cid:13) be the set of all (positive) literals h ∈ H such that KB ∧ h is satisfiable; H (cid:13) can becomputed efficiently by testing every h ∈ H since KB is dual Horn. Then P has an explanation if and only if theinstance (V , H (cid:13), M, KB) has one, since every candidate explanation containing h for some literal h ∈ H \ H (cid:13) wouldbe inconsistent with KB. Now the set of models of a dual Horn knowledge base is closed under componentwise logicalor (this is dual to the well-known closure of Horn theories under logical and, see, e.g., [46]). Hence, since KB ∧ h issatisfiable for every h ∈ H (cid:13), KB ∧H (cid:13) is satisfiable. We now conclude from Lemma 43 that P has an explanation if(cid:2)H (cid:13) entails M, which can be decided in polynomial time since KB is dual Horn (Theorem 33). (cid:2)and only if KB ∧(cid:2)We finally give two hardness results. The proof of the first one follows directly from [12] (V-ABD case, as reportedin Theorem 18) together with Corollary 50.Proposition 53. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IE0. Then P-ABD((cid:5),POSLITS) is NP-hard.Proposition 54. Let (cid:5) be a constraintABD((cid:5),NEGLITS) are NP-hard.language satisfying (cid:15)(cid:5)(cid:16) = IE0. Then P-ABD((cid:5),NEGLITS) and V-G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841267Proof. By Proposition 53, P-ABD((cid:5),POSLITS) is NP-hard. Now the clause (¬x ∨ ¬y) is in IE0, thus Lemma 29gives a reduction from P-ABD((cid:5),POSLITS) to P-ABD((cid:5),NEGLITS) with choosing a negative literal for (cid:3). The proofis similar for V-ABD, using Theorem 18 for hardness of V-ABD((cid:5),POSLITS). (cid:2)11. Bijunctive and IHS-B languagesBijunctive and IHS-B restrictions share an important property, summarized in the next lemma.Lemma 55. Let C be any of Cbij, Cimpl, CIHSB+/k, or CIHSB−/k for some k. Then every prime implicate of a knowledgebase KB over C is in C, and the set of all these prime implicates can be computed in time polynomial in the size of KB.In particular, there are only a polynomial number of them.Proof. It is easily seen that all the prime implicates are in C. Indeed, all of them can be generated by resolution, andas is easily seen from the forms of the clauses, resolution preserves each class in the statement. Thus, starting from aformula over C, only clauses in C can be generated.Clearly, the number of prime implicates of a theory over Cbij, Cimpl, CIHSB+/k, or CIHSB−/k is polynomial in the sizeof the theory, since the size of clauses over C is bounded by 2 or k in all cases. All can be generated in polynomialtime since one can simply generate all clauses of size 2 (resp. k) and for each one, test whether it is entailed by KBand none of its proper subclauses is, in polynomial time in all cases (Theorem 33). (cid:2)Remark 56. The fact that the language is fixed, and thus that k is fixed for languages CIHSB−/k and CIHSB+/k, is crucialin the proof of Lemma 55. Indeed, the statement does not hold for infinite languages CIHSB− and CIHSB+.11.1. Bijunctive and IHS-B languages: Upper boundsWe first give easy consequences of Lemmata 37 and 55, which generalize folklore results based on prime implicategeneration (see in particular Marquis’ survey [37]. The algorithm consists of generating the prime implicates of KBover Vars(H ) ∪ Vars(M) until one as in Lemma 37 is found or all have been tested.Proposition 57. Let C be any of Cbij, Cneg,=, or CIHSB−/k for some k ∈ N. Then L-ABD(C,CLAUSES) is in P.Proof. The only case not handled by Lemma 55 is Cneg,=, because of the equality relation. We show that if a knowl-edge base KB over Cneg,= contains no equality constraint, then it has a polynomial number of prime implicates, all ofwhich can be generated efficiently. We then conclude with Lemma 21.Indeed, since clauses in KB are either unit positive or negative, it can be seen that once resolution has been appliedto each pair of clauses consisting of a positive and a negative one, it cannot be applied any more. Since there can be atmost one positive clause per variable, all prime implicates can be generated in polynomial time. (cid:2)Proposition 58. L-ABD(CIHSB−,POSCLAUSES) is in P.Proof. By a reasoning similar to that in the proof of Lemma 55 we have that every prime implicate of a IHS-B−theory is IHS-B−. Now since the only IHS-B− clauses which contain at least one positive literal are unaryand implicative ones, from Lemma 37 we get that the only minimal candidate explanations of an instance of L-ABD(CIHSB−,POSCLAUSES) are the empty one and those restricted to only one (positive) literal, all of which can betested efficiently. (cid:2)We now give two new tractability results, for which some more work is needed because of conjunctive manifesta-tions.Proposition 59. L-ABD(Cimpl,POSCNFS) is in P.1268G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Proof. First observe that since KB is bijunctive, one can decide in polynomial time whether ∅ is an explanation, bydeciding whether KB is satisfiable and entails M. Thus we assume hereafter that ∅ is not an explanation.Let P = (V , H, M, KB) be an instance of the problem, and assume first that M consists of a single (positive)clause C. In this case, since the only prime implicates of a knowledge base over Cimpl are in Cimpl (Lemma 55), weconclude that a set E ⊆ H is an explanation for P if and only if KB ∧E is satisfiable and there is a literal (cid:3) ∈ Esuch that KB ∧ ((cid:3)) entails C (or, as a subcase, E = ∅ is an explanation); moreover, such an (cid:3) has to be a positiveliteral. Thus the set EC of all such (cid:3)’s can be computed in polynomial time by testing the |H | candidates.Now consider the case when M is a CNF of the form (C1 ∧ · · · ∧ Cp). Then by Lemma 28 and the reasoningabove, a set E ⊆ H is an explanation for P if and only if KB ∧E is satisfiable and for all i ∈ {1, . . . , p}, ∅ isan explanation for Ci or there is some hi ∈ E such that hi ∈ ECi . As is easily seen, this is true if and only if theh) is satisfiable. Since for all i ∈ {1, . . . , p}, ECi contains only positive literals,formula KB ∧this formula is IHSB+ and thus, it can be decided in polynomial time whether it is satisfiable (Theorem 32, sinceCIHSB+ ⊆ CdHorn). (cid:2)(cid:2)pi=1,KB(cid:3)|(cid:12)Ci(cid:6)(h∈ECi(cid:2)(cid:2)Proposition 60. L-ABD(Cneg,=,POSCNFS) is in P.Proof. Let (V , H, M, KB) be an instance. We first invoke Lemma 21 for assuming without loss of generality that KBcontains no equality constraint. Now, reasoning as in Lemma 57 we get that the only prime implicates of KB whichcontain at least one positive literal are unit clauses. Thus, by Lemma 37 there is an explanation for a positive clauseC of M if and only if ∅ or {m}, for some m in C, is an explanation. It then follows from Lemma 28 that there is anexplanation for M if and only if H contains at least one variable in each clause of M which is not entailed by KBalone, which can be decided efficiently. (cid:2)11.2. Bijunctive and IHS-B languages: Lower boundsWe will mainly use the following lemma, which provides a class of reductions from satisfiability problems toabduction problems. The intuition behind the reduction is that we reduce the test for satisfiability of a set of clauses ina formula to a test for explainability of the satisfaction of these clauses, where satisfaction of a clause is explainableby any of the literals in this clause. The clauses which are not transformed by the reduction serve as constraintsover the possible explanations. Importantly, this is exactly the intuition behind our characterization of tractable vs.NP-complete abduction problems, as we shall see in Section 15.(cid:2)(cid:6)i∈I Ci be a CNF formula, where for all i, Ci =Lemma 61. Let ϕ =(cid:3)i,j and every (cid:3)i,j is a literal. LetI (cid:13) ⊆ I be a set of indices. For all i ∈ I (cid:13) let xi be a new variable (xi /∈ Vars(ϕ)), and let si be a literal formed upon xi(intuitively, “clause Ci is satisfied”). Finally, define(cid:2)j ∈Ji(cid:2)(cid:2)i∈I \I (cid:13) Ci ∧• KB =i∈I (cid:13)• V = Vars(ϕ) ∪ {xi | i ∈ I },• H = {(cid:3)i,j | i ∈ I, j ∈ Ji} ∪ {(cid:3)i,j | i ∈ I, j ∈ Ji}, and• M =((cid:3)i,j ∨ si),j ∈Ji(cid:2)i∈I si .Then ϕ is satisfiable if and only if the abduction problem P = (V , H, M, KB) has an explanation. The same resultholds with H = {(cid:3)i,j | i ∈ I, j ∈ Ji}.Proof. Assume first that ϕ has a model μ, and define E to be the set of all literals in H which are satisfied by μ.Then since μ satisfies Ci for all i ∈ I , it satisfies Ci for all i ∈ I \ I (cid:13); now define the assignment μ(cid:13) to V to agree withμ over Vars(ϕ) and to satisfy every si . Then μ(cid:13) satisfies Ci for every i ∈ I \ I (cid:13) and ((cid:3)i,j ∨ si) for every i ∈ I, j ∈ Ji ,thus it satisfies KB. Moreover, clearly μ(cid:13) satisfiesE. Finally, KB ∧We now show that KB ∧i∈I si) is satisfiable. Thenthere is an i ∈ I such that KB ∧E, thus itagrees with μ over Vars(H ); then since μ satisfies ϕ, μ(cid:13) satisfies (cid:3)i,j for at least one j ∈ Ji . Thus μ(cid:13) satisfies (cid:3)i,j ∧ si ,which contradicts the fact that it satisfies KB. Finally, KB ∧E ∧ si is satisfiable. Write μ(cid:13) for one of its models. Then μ(cid:13) satisfiesE entails M. Assume to the contrary that KB ∧(cid:2)E entails M, and E is an explanation for P .E is satisfiable.E ∧ ((cid:2)(cid:2)(cid:2)(cid:6)(cid:2)(cid:2)(cid:2)G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841269Conversely, assume that P has an explanation E. Then there is a model μ of KB ∧E, and we show that μ|Vars(ϕ)is a model of ϕ. First, for all i ∈ I \ I (cid:13), μ satisfies Ci since Ci is in KB. Now assume, towards a contradiction, thatthere is an i ∈ I (cid:13) such that for all j ∈ Ji , μ satisfies (cid:3)i,j . Define the assignment μ(cid:13) to agree with μ over V \ {xi} andto satisfy si . Then μ(cid:13) satisfies KB ∧E but does not satisfy M, which contradicts the fact that E is an explanationfor P . It follows that for all i ∈ I (cid:13), μ satisfies (cid:3)i,j for at least one j ∈ Ji , and thus it satisfies Ci . Finally, μ satisfies ϕ,as desired. (cid:2)(cid:2)(cid:2)Based on this general reduction, we are able to give new hardness results for several abduction problems withbijunctive and IHS-B knowledge bases.Proposition 62. Let (cid:5) be a constraint language. Then,11, V-ABD((cid:5),POSTERMS) and P-ABD((cid:5),POSTERMS) are NP-hard;• if (cid:15)(cid:5)(cid:16) = IS2• if (cid:15)(cid:5)(cid:16) = IM, V-ABD((cid:5),TERMS) is NP-hard;• if (cid:15)(cid:5)(cid:16) = IS20, V-ABD((cid:5),POSTERMS) and N-ABD((cid:5),POSTERMS) are NP-hard.Proof. We first prove the case (cid:15)(cid:5)(cid:16) = IS211. From Theorem 32 it follows that SAT({(x1 ∨ x2 ∨ x3), (¬x1 ∨ ¬x2)}) isNP-complete. Now Lemma 61 gives a reduction from this problem to V-ABD((cid:5),POSTERMS) or to P-ABD((cid:5),POS-TERMS). Indeed, let ϕ =(¬xi,1 ∨ ¬xi,2), where Ip, In are two disjoint sets of indices.Then by Lemma 61 we have the desired reduction by choosing I (cid:13) = Ip and for all i ∈ I (cid:13), si = xi .(xi,1 ∨ xi,2 ∨ xi,3) ∧i∈Ipi∈In(cid:2)(cid:2)The proof is similar for cases (cid:15)(cid:5)(cid:16) = IM and (cid:15)(cid:5)(cid:16) = IS20, up to considering respectively,• SAT({(x1 ∨ x2 ∨ x3), (¬x1 ∨ ¬x2)}), I (cid:13) = Ip ∪ In, for all i ∈ Ip, si = xi , and for all i ∈ In, si = ¬xi , and• SAT({(x1 ∨ x2), (¬x1 ∨ ¬x2 ∨ ¬x3)}), I (cid:13) = In, and for all i ∈ I (cid:13), si = xi . (cid:2)The next proposition is a special case. Indeed, observe that (cid:5) is necessarily infinite, since any finite language(cid:2) IS11).11 for some k ∈ N (yielding (cid:15)(cid:5)(cid:16) ⊆ ISk11included in IS11 must have bounded width and thus, be included in ISkThis special case is discussed in Section 13.2.Proposition 63. Let (cid:5) be an (infinite) language satisfying (cid:15)(cid:5)(cid:16) = IS11. Then V-ABD((cid:5),NEGLITS) and P-ABD((cid:5),NEGLITS) are NP-hard.Proof. Proposition 62 shows that V-ABD((cid:5),POSTERMS) is NP-hard if (cid:15)(cid:5)(cid:16) = IS211. Now Lemma 29 gives a reductionfrom this problem to V-ABD((cid:5)(cid:13),NEGLITS), where (cid:15)(cid:5)(cid:13)(cid:16) = IS11, by choosing a negative literal for (cid:3). The proof issimilar for P-ABD((cid:5),POSTERMS). (cid:2)12. Affine languagesThe main tool which we will use with affine formulas is projection. This will be done through the followinglemma.Lemma 64. (See [51].) Let KB be an affine formula, and let V (cid:13) ⊆ Vars(KB). Then there is a projection of KB onto V (cid:13)which is affine, and such a projection can be computed in polynomial time.Interestingly, contrary to the case of bijunctive and bounded IHS-B knowledge bases, tractability of projection hereis not a consequence of a polynomial number of prime implicates. Indeed, even in the case of a single linear equationof the form (x1 ⊕ · · · ⊕ xn = 0), an affine formula may have an exponential number of prime implicates (2n−1 in theexample, namely all clauses over exactly x1, . . . , xn with an odd number of negative literals).1270G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–128412.1. Affine languages: Upper boundsWe first restate Nordh and Zanuttini’s result [40] about affine formulas of width 2. Indeed, they state it for finitelanguages, but their proof obviously holds for the corresponding infinite language as well.Proposition 65. (Adapted from [40].) L-ABD(Eaff /2,TERMS) is in P.We now give new tractability results. So as to use projection consistently with its definition, observe that in theaffine case we can assume Vars(H ) ∪ Vars(M) ⊆ Vars(KB) without loss of generality. Indeed, for any variable x ∈(Vars(H ) ∪ Vars(M)) \ Vars(KB), a fresh variable newx can be introduced and x ⊕ newx = 0 added to KB withoutchanging the set of explanations.Proposition 66. L-ABD(Eaff ,CLAUSES) is in P.Proof. Consider an instance (V , H, M, KB). Note that since (x (cid:3)= y) ≡ (x ⊕ y = 1) ∈ Eaff we can use Lemma 27 toreduce the instance to an equivalent one (V (cid:13), H (cid:13), M (cid:13), KB(cid:13)) where M (cid:13) is a positive clause and H (cid:13) is a set of negativeliterals. Now, in order to eliminate all variables that are neither in H (cid:13) nor in M (cid:13), project KB(cid:13) onto V (cid:13)(cid:13) = Vars(H (cid:13)) ∪Vars(M (cid:13)), getting a formula KB(cid:13)(cid:13) over the set of variables V (cid:13)(cid:13).Now since (x = y) ≡ (x ⊕ y = 0) ∈ Eaff we can assume Vars(H (cid:13)) ∩ Vars(M (cid:13)) = ∅ without loss of generality, sinceany variable x in the intersection could be duplicated into xH and xM up to adding (xH = xM ) to KB(cid:13)(cid:13).We now have an instance P = (V (cid:13)(cid:13), H (cid:13), M (cid:13), KB(cid:13)(cid:13)) where KB(cid:13)(cid:13) is a set of linear equations, H (cid:13) is the set of negativeliterals {¬x | x ∈ V (cid:13)(cid:13) \ Vars(M (cid:13))}, and M (cid:13) is a positive clause. We can then use exactly the same reduction as in [40,Proposition 11] to show that P has an explanation if and only if the negative term ¬M (cid:13) does not follow from KB(cid:13)(cid:13)when circumscribing all variables. Since this problem is in P if ¬M (cid:13) is a single literal [18, Theorem 7], and the caseof a term is easily seen to be polynomial-time reducible to it, we have the result. (cid:2)The next proposition will use the notion of a full explanation. Given an instance P = (V , H, M, KB) of any ab-duction problem, an explanation E for P is said to be full if Vars(E) = Vars(H ). What we will use is the fact that aninstance of V-ABD, for any restriction on the manifestation, has an explanation if and only if it has a full one. Indeed,E has at least one model μ, it is easily seen that the set E(cid:13) defined to begiven a nonfull explanation E, since KB ∧the set of all literals over H assigned true by μ is a full explanation for P .(cid:2)Proposition 67. V-ABD(Eaff ,TERMS) is in P.Proof. Let P = (V , H, M, KB) be an instance, and write VH for Vars(H ). We first consider the case of a positiveliteral as a manifestation, i.e., M = m for some m ∈ V (the case of a negative literal is dual). Assume m /∈ H , whichis without loss of generality since otherwise P has an explanation if and only if KB ∧ m is satisfiable, which can bedecided efficiently. Write KBH ∪{m} for an affine projection of KB onto VH ∪ {m}. By Lemma 64 such a knowledgebase can be computed in polynomial time. Now define PH ∪{m} = (VH ∪ {m}, H, m, KBH ∪{m}). By Lemma 40, PH ∪{m}has an explanation if and only if P has one. Moreover, obviously, if m does not occur in KBH ∪{m} then PH ∪{m} hasno explanation. Otherwise, let EqnmX x = a) be an equation of KBH ∪{m} containing m.Now let KBH,m=1 be an affine projection of KBH ∪{m} ∧ m onto VH . We claim that the full explanations of PH ∪{m}= (m ⊕(cid:7)are in bijection with the models of KBH,m=1.(cid:2)(cid:2)E ∧ m; thus μ|VH satisfies KBH,m=1 ∧Indeed, let E be a full explanation for PH ∪{m}. Then by definition of an explanation, there is a model μ ofKBH ∪{m} ∧E, and thus KBH,m=1. Conversely, let μ be a model ofKBH,m=1, and write E for the set of all literals over VH and satisfied by μ; we show that E is a full explanation ofPH ∪{m}. First, KBH,m=1 ∧E isE |(cid:12) m. Assume towards a contradiction that there is a modelsatisfiable. We are thus left with proving KBH ∪{m} ∧μ(cid:13) of KBH ∪{m} ∧E ∧ ¬m. In particular, μ(cid:13) satisfies ¬m and agrees with μ over Vars(E) = VH . Moreover, since(cid:7)X x = a ⊕ 1 (i.e., Eqnm with m = 1); it follows that μ(cid:13) satisfiesμ is a model of KBH,m=1, it satisfies the equation(cid:7)X x = a ⊕ 1. Thus it does not satisfy equation Eqnm,X x = a ⊕ 1; since it also satisfies ¬m, it satisfies m ⊕E is satisfiable, thus KBH ∪{m} ∧ m ∧E is satisfiable, and thus KBH ∪{m} ∧(cid:7)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)which contradicts the fact that it satisfies KBH ∪{m}.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841271We conclude that in the case of a single literal m as the manifestation, the full explanations of PH ∪{m} are exactlythe models of KBH,m=1. It follows from Lemma 28 that the full explanations in the case of a manifestation M =m1 ∧ · · · ∧ mk are exactly the models of KBH,m1=1 ∧ · · · ∧ KBH,mk=1. Since every KBH,mi =1 is an affine formula andcan be computed in polynomial time, we finally get a polynomial algorithm. (cid:2)12.2. Affine languages: Lower boundsWe finally give hardness proofs for affine languages, all of which are new. To that aim, we use original reductionsfrom satisfiability problems, inspired by the clausal case (Lemma 61).Proposition 68. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IL. Then L-ABD((cid:5),POSTERMS) is NP-hard.(cid:2)Proof. We give a reduction from SAT({(x1 ⊕ x2 ⊕ x3 ⊕ x4 = 0), (x1), (¬x1 ∨ ¬x2)}), which is NP-complete byTheorem 32. Let ϕ = ϕ0 ∧k∈K (xk = 1), where ϕ0 contains only equations of the form (x1 ⊕x2 ⊕ x3 ⊕ x4 = 0). For all i ∈ I let si /∈ Vars(ϕ) be a fresh variable (intuitively meaning that clause i is satisfied), andfor all i ∈ I let pi,1, ni,1, pi,2, ni,2 /∈ Vars(ϕ) be four fresh variables (“p” stands for “positive” and “n” for “negative”).Write Hp for the set of all pi,j ’s and Hn for that of all ni,j ’s. We define an instance P = (V , H, M, KB) of L-ABD((cid:5),POSTERMS) byi∈I (¬xi,1 ∨ ¬xi2) ∧(cid:2)• V = Vars(ϕ) ∪ {si | i ∈ I } ∪ Hp ∪ Hn,• H = N(Vars(ϕ)) ∪ {xk | k ∈ K} ∪ Hp ∪ N(Hn),• M = {si | i ∈ I } ∪ {xk | k ∈ K}, and• KB = ϕ0 ∧i∈I (xi,1 ⊕ pi,1 ⊕ ni,1 ⊕ si = 0) ∧(cid:2)(cid:2)i∈I (xi,2 ⊕ pi,2 ⊕ ni,2 ⊕ si = 0).The intuition is that the new equations play the role of “implications” ¬xi,1 → si and ¬xi,2 → si .We claim that P has an explanation if and only if ϕ is satisfiable. First, if μ is a model of ϕ, then clearly {¬x |x ∈ Vars(ϕ) and μ(x) = 0} ∪ {xk | k ∈ K} ∪ {pi,j , ¬ni,j | i ∈ I and μ(xi,j ) = 0} is an explanation for P . Conversely,assume E is an explanation for P . Then since for all i ∈ I , si, pi,1, pi,2, ni,1, ni,2 only occur in equations (xi,j ⊕ pi,j ⊕ni,j ⊕ si = 0), E has to contain pi,j or ¬pi,j and ni,j or ¬ni,j for at least one j ∈ {1, 2}. Indeed, otherwise flippingE ∧ ¬si ,the values of si and some pi,j ’s or ni,j ’s in a model of KB ∧contradicting the fact that E is an explanation for P . Thus for all i ∈ I there is a j ∈ {1, 2} such that E contains(cid:2)pi,j and ¬ni,j , since ¬pi,j , ni,j /∈ H . Now by definition of an explanation, there is a model of KB ∧M,and it follows from the reasoning above that this model satisfies ¬xi,1 or ¬xi,2 for all i ∈ I , since otherwise one ofthe equations (xi,j ⊕ pi,j ⊕ ni,j ⊕ si = 0) would not be satisfied. Finally, it satisfies xk for all k ∈ K because theseliterals are in M. Thus, defining μ to be the projection of this model onto Vars(ϕ), we get that μ is a model of ϕ, asdesired. (cid:2)M would yield a model of KB ∧(cid:2)E ∧E ∧(cid:2)(cid:2)(cid:2)Proposition 69. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IL1. Then N-ABD((cid:5),POSTERMS) is NP-hard.Similarly, if (cid:5) is a constraint language satisfying (cid:15)(cid:5)(cid:16) = IL0, then P-ABD((cid:5),POSTERMS) is NP-hard.Proof. As regards IL1, the proof is similar to that of Proposition 68, except that we start from SAT({(x1 ⊕ x2 ⊕ x3 =1), (¬x1 ∨ ¬x2)}), we introduce only one fresh variable ni,j per occurrence of variable in a negative clause, we buildequation (xi,j ⊕ ni,j ⊕ si = 1) instead of (xi,j ⊕ pi,j ⊕ ni,j ⊕ si = 0), and we only add ¬ni,j to H .The proof is dual for IL0, starting from SAT({(x1 ⊕ x2 ⊕ x3 = 0), (x1 ∨ x2)}) and building (xi,j ⊕ pi,j ⊕ p(cid:13)i,j⊕ si =0) with pi,j , p(cid:13)i,j∈ H . (cid:2)Proposition 70. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IL3. Then P-ABD((cid:5),POSTERMS) and N-ABD((cid:5),POSTERMS) are NP-hard.Proof. Since IL ⊂ IL3, by Proposition 68 L-ABD((cid:5),POSTERMS) is NP-hard. Now we know L-ABD((cid:5),POS-TERMS)(cid:3)PL-ABD((cid:5),TERMS) (Lemma 19). Finally, since R(cid:3)=(x, y) is the set of models of x ⊕ y = 1, R(cid:3)= is in IL3.1272G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Thus by Lemma 27 we know that L-ABD((cid:5),TERMS) (cid:3)P P-ABD((cid:5),POSTERMS), N-ABD((cid:5),POSTERMS), whichconcludes the proof. (cid:2)Proposition 71. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = ID. Then V-ABD((cid:5),POSCNFS), P-ABD((cid:5),POS-CNFS), and N-ABD((cid:5),POSCNFS) are NP-hard.Proof. We consider the case of V-ABD. The other cases follow from it using Lemmata 20 and 27.We give a reduction from the satisfiability problem for CNF formulas. Let ϕ =i∈I Ci be a CNF formula. For ev-ery variable x ∈ Vars(ϕ) let px, nx /∈ Vars(ϕ) be two fresh variables (“p” stands for “positive” and “n” for “negative”).Let ϕ(cid:13) be the CNF formula obtained from ϕ by replacing every positive occurrence x of a variable in a clause with thepositive literal px , and every negative occurrence ¬x of a variable with the positive literal nx . By construction, ϕ(cid:13) isa positive CNF.(cid:2)We define an instance P of V-ABD((cid:5),POSCNFS) as follows:(cid:2)x∈Vars(ϕ)(px (cid:3)= nx),• KB =• V = {px | x ∈ Vars(ϕ)} ∪ {nx | x ∈ Vars(ϕ)},• M = ϕ(cid:13), and• H = {px | x ∈ Vars(ϕ)} ∪ {¬px | x ∈ Vars(ϕ)}.Then it is easily seen that if ϕ is satisfiable with a model μ, then E defined to be {px | μ(x) = 1} ∪ {¬px | μ(x) = 0}is an explanation for P . Conversely, if E is an explanation for P , then it is easily seen that any assignment to Vars(ϕ)satisfying μ(x) = 1 (resp. μ(x) = 0) for all px ∈ E (resp. ¬px ∈ E) is a model of ϕ, which concludes the proof. (cid:2)Proposition 72. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IL. Then V-ABD((cid:5),POSCNFS) is NP-hard.(cid:2)Proof. We give a reduction from the satisfiability problem for CNF formulas, similar to the one in the proof ofi∈I Ci be a CNF formula. For every variable x ∈ Vars(ϕ) let px, nx /∈ Vars(ϕ) be two freshProposition 71. Let ϕ =variables (“p” stands for “positive” and “n” for “negative”). Let ϕ(cid:13) be the CNF formula obtained from ϕ by replacingevery positive occurrence x of a variable in a clause with the positive literal px , and every negative occurrence ¬x ofa variable with the positive literal nx . By construction, ϕ(cid:13) is a positive CNF. Moreover, let x1, . . . , x4 /∈ Vars(ϕ).We define an instance P of V-ABD((cid:5),POSCNFS) as follows:(cid:2)• KB = (x1 ⊕ x2 ⊕ x3 ⊕ x4 = 0) ∧• V = {px | x ∈ Vars(ϕ)} ∪ {nx | x ∈ Vars(ϕ)} ∪ {x1, x2, x3, x4},• M = x1 ∧ (x3 ∨ x4) ∧ ϕ(cid:13), and• H = {px, ¬px | x ∈ Vars(ϕ)} ∪ {x1, ¬x1, x2, ¬x2}.x∈Vars(ϕ)(px ⊕ nx ⊕ x3 ⊕ x4 = 0),(cid:2)The idea is that the manifestations x1 ∧ (x3 ∨ x4) together with the equation x1 ⊕ x2 ⊕ x3 ⊕ x4 = 0 in KB force everyE (for any explanation E) to satisfy μ(x3) (cid:3)= μ(x4). Consequently, the equations (px ⊕ nx ⊕ x3 ⊕model μ of KB ∧(cid:2)x4 = 0) force μ(px) (cid:3)= μ(nx) for any model μ of KB ∧E (and any explanation E).It is easily seen that if ϕ is satisfiable with a model μ, then E defined to be {px | μ(x) = 1} ∪ {¬px | μ(x) =0} ∪ {x1, ¬x2} is an explanation for P . Conversely, if E is an explanation for P , then KB ∧E is satisfiable andentails x1 ∧ (x3 ∨ x4) ∧ ϕ(cid:13). Now, assume that there is a model μ of KB ∧E such that μ(x3) = μ(x4) = 1. Notethat x3 and x4 do not occur in H and that they occur together in every equation in KB. So, it is easy to see thatE such that μ(cid:13)(x3) = μ(cid:13)(x4) = 0. This is a contradiction with the fact that E isthere is also a model μ(cid:13) of KB ∧an explanation, since μ(cid:13) does not satisfy (x3 ∨ x4). Hence, any model μ of KB ∧E satisfy μ(x3) (cid:3)= μ(x4) andconsequently, μ(px) (cid:3)= μ(nx) for all x ∈ Vars(ϕ). Hence, the assignment μ(cid:13) to Vars(ϕ) defined by μ(cid:13)(x) = μ(px) forall x ∈ Vars(ϕ) is a model of ϕ. (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)For the next proposition, observe that even the empty language (cid:5) = ∅ satisfies (cid:15)(cid:5)(cid:16) = IBF.Proposition 73. Let (cid:5) be a constraint language satisfying (cid:15)(cid:5)(cid:16) = IBF. Then V-ABD((cid:5),CNFS) is NP-hard.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841273Proof. Recall than IBF only contains the equality relations. We give a reduction from the satisfiability problem forCNFs. Let ϕ be a CNF, we define the following instance P of V-ABD((cid:5),CNFS):• KB is the empty CNF (always satisfied),• V = Vars(ϕ),• M = ϕ, and• H = Lits(Vars(ϕ)).Then it is easily seen that the (full) explanations of P correspond exactly to the models of ϕ. (cid:2)13. Summary and complete classificationWe are now in position to give a complete picture of the complexity of propositional abduction for the 48 restric-tions over hypotheses and manifestations. By a “complete” picture, we mean that our results give the complexity ofabduction for any constraint language and for any class of clauses or equations, as explained in Section 13.2.13.1. Summary of resultsThe complete complexity picture of abduction in given in Table 3. In this table, for each restriction on hypothesesand manifestations and each complexity class, the minimal and maximal languages in this class (with respect tolanguage inclusion) are listed. More precisely, the languages listed on the first line in each cell are the maximallanguages in the complexity class, and the languages on the second line are the minimal languages hard for the class.As an example, consider the cell for NP-complete L-ABD problems where the manifestations are expressed byPOSCNFS. The first row in this cell is Cbij, CdHorn, Eaff , CHorn, and the second row is IS211, ID, IV, IL. This meansthat L-ABD((cid:5),POSCNFS) is NP-complete for any (cid:5) such that (cid:5) is a subset of one of the languages listed in thefirst row, and one of the languages on the second row is a subset of (cid:15)(cid:5)(cid:16). To further exemplify, consider the constraintlanguage (cid:5) = {R}, where R = {001, 010, 100, 111} (i.e., the relation expressed by x1 ⊕ x2 ⊕ x3 = 1). Then, (cid:5) ⊆ Eaffand IL ⊆ (cid:15)(cid:5)(cid:16). Hence, by Proposition 35, L-ABD((cid:5),POSCNFS) is in NP, and by Proposition 72 (together with theobvious reduction from V-ABD to L-ABD) L-ABD((cid:5),POSCNFS) is NP-hard.0, IS2The fact that all the results in Table 3 can be derived from the results reported in the paper (in the manner describedabove) has been checked by a computer program, which is available from the authors.Also note that the results for negative restrictions on manifestations have been omitted from the table; to recoverthem, simply use Lemma 25.Finally, we collapsed the rows concerning (positive) clauses and (positive) literals. Indeed, it turns out that thecomplexity is always the same for both types of manifestations. Section 15.4 gives an explanation for that fact.13.2. On the completeness of the classificationIn this section we motivate our claims that the classifications are complete in the sense that all constraint languagesand classes of equations and clauses are covered.We begin by noting that the upper bounds on the complexity of abduction problems in the paper, which are allgiven in terms of clausal and equational languages C, also hold for any constraint language (cid:5) such that (cid:5) ⊆ (cid:15)C(cid:16). If (cid:5)is a finite constraint language, then this is obvious since we can use a simple lookup table to translate a (cid:5)-formula intoa CNF or system of equations over C. For infinite constraint languages (cid:5) the situation is slightly more involved. Wemake use of a result from [11], stating that we can transform (in polynomial time) any (cid:5)-formula where the relationsare given in extension into an equivalent CNF-formula/system of equations over any clausal/equational language Cfrom Table 1 such that (cid:15)(cid:5)(cid:16) ⊆ (cid:15)C(cid:16). Since all the upper bounds on the complexity of abduction in the paper are given forclausal and equational languages C from Table 1, we get that these upper bounds also apply to constraint languages (cid:5)such that (cid:5) ⊆ (cid:15)C(cid:16).All our hardness results, except for a few special cases discussed below, are proved for finite constraint languages (cid:5).Obviously, using Lemma 22, these hardness results implies hardness for any constraint language (cid:5)(cid:13) such that (cid:5) ⊆ (cid:15)(cid:5)(cid:13)(cid:16).1274G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284Table 3Complexity of propositional abductionMPNP-C.coNP-C.POSLITS orPOSCLAUSESLITS orCLAUSESPOSTERMSCIHSB+/k, Cpos,=, CIHSB−, Cbij, Eaff , C1v−HornCIHSB+/k, Cpos,=, CIHSB−/k, Cneg,=, Cbij, EaffCimpl, Cneg,=, Eaff , C1v−HornV-ABDTERMSEaffPOSCNFSCimpl, Cneg,=, C1v−HornCNFSPOSLITS orPOSCLAUSESLITS orCLAUSESPOSTERMSCIHSB−, Cbij, CdHorn, Eaff , C1v−HornCIHSB−/k, Cneg,=, Cbij, CdHorn, Eaff , C1v−HornCneg,=, Eaff /2, CdHorn, E1v−aff , C1v−HornP-ABDTERMSEaff /2, CdHorn, E1v−aff , C1v−HornPOSCNFSCneg,=, CdHorn, E1v−aff , C1v−HornCNFSCdHorn, E1v−aff , C1v−HornPOSLITS orPOSCLAUSESLITS orCLAUSESPOSTERMSCIHSB+/k, Cpos,=, Cbij, Eaff , CHorn, C0vCIHSB+/k, Cpos,=, Cbij, C0v−dHorn, Eaff , CHornEaff /2, CHorn, C0vN-ABDTERMSEaff /2, C0v−dHorn, E0v−aff , CHornPOSCNFSCHorn, C0vCNFSC0v−dHorn, E0v−aff , CHornPOSLITS orPOSCLAUSESLITS orCLAUSESPOSTERMSCIHSB+/k, Cpos,=, CIHSB−, Cbij, Eaff , C1v−HornCIHSB+/k, Cpos,=, CIHSB−/k, Cneg,=, Cbij, EaffCimpl, Cneg,=, Eaff /2, C1v−HornL-ABDTERMSEaff /2Cimpl, Cneg,=, C1v−HornPOSCNFSCNFS0, IS20, IS2CdHorn, CHornIS01, IV, IE0CdHorn, CHornIS01, IS11, IV, IECbij, CdHorn, CHornIS211, IVCbij, CdHorn, CHornIM, IS20, IS21Cbij, CdHorn, Eaff , CHornIS211, ID, IV, ILCbij, CdHorn, Eaff , CHornIBFCHornIE0CHornIS11Cbij, Eaff , CHornIS211, IL3, IL0Cbij, Eaff , CHornIS21, IL3, IL0Cbij, Eaff , CHornIS211, ID, IL0Cbij, Eaff , CHornIS21, ID, IL0CdHornIS01CdHornIS01Cbij, CdHorn, EaffIS20, IL1, IL3Cbij, CdHorn, EaffIS20, IL1, IL3Cbij, CdHorn, EaffIS20, ID, IL1Cbij, CdHorn, EaffIS20, ID, IL1CdHorn, CHornIS01, IV, IE0CdHorn, CHornIS01, IS11, IV, IECbij, CdHorn, Eaff , CHornIS211, IV, ILCbij, CdHorn, Eaff , CHornIM, IS21, ILCbij, CdHorn, Eaff , CHornIS211, ID, IV, ILCbij, CdHorn, Eaff , CHornIBF0, IS20, IS20, IS2C1vINC1vINC1vINC1vINC1vINC1vINC0vINC0vINC0vIN(cid:2)P2 -C.CCNFINCCNFINCCNFINCCNFINCCNFINCCNFINCCNFIN2, II0CCNFIN2, II0CCNFIN2, II0CCNFIN2, II0CCNFIN2, II0CCNFIN2, II0CCNFIN2, II1CCNFIN2, II1CCNFIN2, II1CCNFIN2, II1CCNFIN2, II1CCNFIN2, II1CCNFINCCNFINCCNFINCCNFINCCNFINCCNFING. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841275The exceptional constraint languages (cid:5) for which we cannot prove hardness by first proving that some finite constraintlanguage (cid:5)(cid:13) ⊆ (cid:15)(cid:5)(cid:16) is hard and then applying Lemma 22, are:• L-ABD((cid:5),M), where (cid:15)(cid:5)(cid:16) ∈ {IS01, IS00}, M ∈ {(POS)LITS, (POS)CLAUSES},• L-ABD((cid:5),M), where (cid:15)(cid:5)(cid:16) ∈ {IS01, IS00, IS11, IS10}, M ∈ {LITS, CLAUSES},• N-ABD((cid:5),M), where (cid:15)(cid:5)(cid:16) ∈ {IS01, IS00}, M ∈ {(POS)LITS, (POS)CLAUSES},• P-ABD((cid:5),M), where (cid:15)(cid:5)(cid:16) ∈ {IS11, IS10}, M ∈ {LITS, CLAUSES},• V-ABD((cid:5),M), where (cid:15)(cid:5)(cid:16) ∈ {IS01, IS00}, M ∈ {(POS)LITS, (POS)CLAUSES}, and• V-ABD((cid:5),M), where (cid:15)(cid:5)(cid:16) ∈ {IS01, IS00, IS11, IS10}, M ∈ {LITS, CLAUSES}.These cases are exceptional since they are all NP-complete (by Proposition 63 and obvious reductions), but if (cid:5) isreplaced by any finite (cid:5)(cid:13) ⊆ (cid:5), then the problems are in P. This is because these languages include relations describedby clauses of arbitrary length, while their finite subsets only allow to express clauses of bounded length. Indeed, theyonly contain binary clauses and clauses with only one polarity, such as (¬x1 ∨ · · · ∨ ¬xk). So the only clauses withcan be resolved against each other to infer a new clause are two binary clauses, or a “long” clause and a binary one,so in no case can a clause of length greater than k be inferred.As a sidenote we want to remark that in the literature on computational problems over restricted constraint lan-guages there are two notions of tractability: local and global. A problem over a constraint language (cid:5) is said to belocally tractable if the problem is tractable over every finite subset of (cid:5), and global tractability coincides with thetractability notion used in this paper. Thus, as first noted by Creignou [10] (and as should be obvious from the discus-sion above), the notions of global and local tractability disagree for the abduction problem. This highlights a differencebetween the complexity of abduction and many other computational problems over constraint language restrictions,such as SAT((cid:5)), for which the notions of local and global tractability coincide.Coming back to the completeness of the classification, it is a tedious but straightforward task (thanks to Post’s clas-sification) to check that, for all constraint languages (cid:5) and restrictions on hypotheses and manifestations consideredin the paper (which are not in one of the special cases already treated above), our results show that either• the abduction problem over (cid:5) is in P as a consequence of (cid:5) ⊆ (cid:15)C(cid:16) for some tractable clausal or equationallanguage C, or• the abduction problem over (cid:5) is NP-complete (coNP-complete, (cid:2)Psome finite NP-hard (coNP-hard, (cid:2)Planguage C which is in NP (coNP, (cid:2)P2 -complete) as a consequence of (cid:5)(cid:13) ⊆ (cid:15)(cid:5)(cid:16) for2 -hard) constraint language (cid:5)(cid:13), and (cid:5) ⊆ (cid:15)C(cid:16) for some clausal or equational2 ).Hence, we have a classification for the complexity of abduction over any constraint language (cid:5) and any combinationof restrictions on hypotheses and manifestations considered in the paper.When it comes to clausal and equational languages C, the reasoning is very similar. First note that all finiteclausal/equational languages C are covered by our discussion about constraint languages above. This is because ab-duction over C has the same complexity as abduction over (cid:5) when (cid:5) and C are both finite and (cid:15)(cid:5)(cid:16) = C. This is becausefinite languages allow us to use a simple lookup table to translate between the different representations in polynomialtime. Hence, we can concentrate on infinite clausal/equational languages C. Now, by Lemma 22 any lower boundon the complexity of abduction over a finite constraint language (cid:5) carries over to any (infinite) clausal or equationallanguage C such that (cid:5) ⊆ (cid:15)C(cid:16). Moreover, the infinite clausal languages C corresponding to the exceptional cases dis-cussed above (where lower bounds cannot be proved by reductions from finite constraint languages) can all be provedto be NP-hard by Proposition 63. For infinite clausal or equational languages C consisting only of either clauses orequations (and not in the exceptional cases discussed above) the results in the paper show that either• the abduction problem over C is in P as a consequence of C ⊆ C(cid:13) for some tractable clausal or equational lan-guage C(cid:13), or• the abduction problem over C is NP-complete (coNP-complete, (cid:2)P2 -complete) as a consequence of (cid:5)(cid:13) ⊆ (cid:15)C(cid:16) for2 -hard) constraint language (cid:5)(cid:13), and C ⊆ C(cid:13) for some clausal or equationalsome finite NP-hard (coNP-hard, (cid:2)Planguage C(cid:13) which is in NP (coNP, (cid:2)P2 ).1276G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284The only remaining problem is upper bounds for infinite clausal and equational languages C expressed by both equa-tions and clauses. For example, C = Cneg,= ∪ {(x1 ⊕ x2 ⊕ x3 = 1)}. Post’s classification yields that all these infiniteclausal/equational languages C satisfy (cid:15)C(cid:16) ∈ {(cid:15)CCNF(cid:16), (cid:15)C0v(cid:16), (cid:15)C1v(cid:16)}. It is very easy to verify that the (trivial) upperbounds that hold for abduction problems over CCNF, C0v, and C1v also hold for the corresponding abduction problemsover C (see the proofs of Propositions 41, 44, and 45). Hence, our results cover any clausal and equational language.As already mentioned, the authors have used a computer program to double-check the completeness of the classi-fication.14. Further restrictionsAs evoked in Section 3.2, several restrictions on the problem were not considered until now but are worth investi-gating. It turns out that most of them do not affect the complexity of the problem.14.1. Unsatisfiable and tautological manifestationsThe first kind of restrictions is about the satisfiability of manifestations. For instance, in typical applications,abduction is the process of explaining a given observation of the world. As a consequence, one may assume thatmanifestations are always satisfiable.We first give a straightforward result.Proposition 74. Let P = (V , H, M, KB) be an instance of any abduction problem. If M is tautological, then P hasan explanation if and only if KB is satisfiable. If M is unsatisfiable, then P has no explanation.Observe that one can efficiently recognize unsatisfiable and tautological literals (vacuously), clauses, or terms.Thus the complexity of abduction is not affected by the extra assumption that such manifestations are nontautologicalor satisfiable, since deciding whether the knowledge base is satisfiable is always at least as hard as abduction. As forCNFs, tautological ones can be efficiently recognized, but not unsatisfiable ones. Nevertheless, it turns out that thecomplexity is not affected either.Proposition 75. L-ABD((cid:5),CNFS) is polynomial-time reducible to L-ABD((cid:5),CNFS) where the manifestation isguaranteed to be satisfiable. The same result holds for V-ABD, P-ABD, or N-ABD instead of L-ABD.Proof. Let (V , H, M, KB) be an instance of L-ABD((cid:5),CNFS). Let new be a fresh variable (new /∈ V ), and let V (cid:13) =V ∪ {new}, M (cid:13) = M ∨ new. Clearly, from the CNF M a CNF M (cid:13)(cid:13) for M ∨ new can be computed efficiently bydistributing ∨new to each clause, and M (cid:13)(cid:13) is satisfiable. We claim that (V (cid:13), H, M (cid:13)(cid:13), KB) has an explanation if andonly if (V , H, M, KB) has one. Indeed, if KB ∧ E is satisfiable and entails M, this is still true for M (cid:13)(cid:13). Conversely, ifKB ∧ E is satisfiable and entails M ∨ new, then we have that KB ∧ E ∧ ¬M ∧ ¬new is unsatisfiable. Since new doesnot occur at all in KB ∧ E ∧ ¬M (and ¬new alone is satisfiable), we have that KB ∧ E ∧ ¬M alone is unsatisfiable,that is, KB ∧ E |(cid:12) M. (cid:2)14.2. Variables in hypotheses and manifestationsAnother interesting kind of restrictions is over the variables allowed to occur in the set of hypotheses and inmanifestations. In particular, it is interesting to consider cases where variables which do not occur in the knowledgebase occur in hypotheses and manifestations. This indeed allows to model situations where the knowledge base tellsnothing about a part of the abduction problem at hand.First of all, it turns out that allowing or not such extra variables in the set of hypotheses does not affect the com-plexity of abduction.Proposition 76. Let (cid:5) be a constraint language, and let P = (V , H, M, KB) be an instance of ABD((cid:5)). Then for anyexplanation E for P there is an explanation E1 for P such that E1 ⊆ E and Vars(E1) ⊆ Vars(KB) ∪ Vars(M).G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841277(cid:2)Proof. Let E be an explanation for P , and let E1 = E ∩ Lits(Vars(KB) ∪ Vars(M)) and E2 = E \ E1. Then clearlyE2 |(cid:12) M.KB ∧Since the left-hand side is satisfiable and Vars(E2) is disjoint from Vars(KB), Vars(E1), and Vars(M), we get that(cid:2)E. Now by definition of an explanation we have KB ∧E1 is satisfiable, since so is KB ∧E1 ∧(cid:2)(cid:2)(cid:2)E2 is irrelevant to the entailment relation. Thus E1 alone is an explanation. (cid:2)As concerns variables occurring in manifestations, the situation is a bit more involved. The following lemma isrelevant to the cases of literals, clauses, or terms, for which the complexity is not affected. Indeed, it shows that forsuch manifestations, one can consider independently the part of the manifestation which is over Vars(KB) and theother part, with almost no computational overhead for the latter.Proposition 77. Let (cid:5) be a constraint language, and let P = (V , H, M, KB) be an instance of ABD((cid:5)). If M =M1 ∧ M2 with Vars(M1) ⊆ Vars(KB) and Vars(M2) ∩ Vars(KB) = ∅, then for any explanation E for P there is apartition of E into {E1, E2} such that E1 is an explanation for (V , H, M1, KB) andE2 |(cid:12) M2 holds.Dually, if M = M1 ∨ M2 with Vars(M1) ⊆ Vars(KB) and Vars(M2) ∩ Vars(KB) = ∅, then for any explanation EE2 |(cid:12) M2 holds.for P there is a partition of E into {E1, E2} such that E1 is an explanation for (V , H, M1, KB) or(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)Proof. Consider M = M1 ∧ M2; the case M = M1 ∨ M2 is similar. Let E be an explanation for P , and let E1 =E ∩ Lits(Vars(KB)) and E2 = E \ E1. Then by definition of an explanation we have KB ∧E2 |(cid:12) M1(cid:2)E1) and Vars(E2) ∩E2 |(cid:12) M2. In the first entailment relation, since Vars(M1) ⊆ Vars(KB ∧and KB ∧(cid:2)Vars(KB ∧E1 aloneentails M1; since moreover KB ∧E2 is satisfiable, E1 is an explanation for (V , H, M1, KB). Similarly, inE2 |(cid:12) M2 holds. (cid:2)the second entailment relation both KB and E1 are irrelevant, thusE1 ∧E1) = ∅ hold, and the left-hand side is satisfiable, we get that E2 is irrelevant, that is, KB ∧E1 ∧E1 ∧(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)We now turn to CNF manifestations. The following lemma shows that for almost all languages, we can assumeVars(M) ⊆ Vars(KB) or not without affecting the complexity of abduction.Proposition 78. Let (cid:5) be any constraint language with (cid:5) (cid:3)⊆ IR2 or R= ∈ (cid:5). Then one can assume up to polynomial-time reductions that an instance (V , H, M, KB) of ABD((cid:5)) satisfies Vars(M) ⊆ Vars(KB).1, . . . , μ(cid:13)Proof. If (cid:5) (cid:3)⊆ IR2 or R= ∈ (cid:5), then (cid:5) contains at least one relation R which is nonempty, n-ary and not equivalent toany term of length n. Then there is at least one place i ∈ {1, . . . , n} and two tuples (μ1, . . . , μi−1, 0, μi+1, . . . , μn),(μ(cid:13)n) in R. Then the assumption Vars(M) ⊆ Vars(KB) can be enforced as follows. Foreach variable x in Vars(M) \ Vars(KB) add n − 1 fresh variables newx,1, . . . , newx,n−1 to V and add the constraintR(newx,1, . . . , newx,i−1, x, newx,i, . . . , newx,n−1) to KB. Write KB(cid:13) and V (cid:13) for the resulting KB and V . By construc-tion x is unconstrained by KB(cid:13), and thus (V (cid:13), H, M, KB(cid:13)) has an explanation if and only if (V , H, M, KB) has one. (cid:2)i+1, . . . , μ(cid:13)i−1, 1, μ(cid:13)The only special case is the following, for the class of CNF formulas containing only unit clauses (and no equalityrelations), when Vars(M) ⊆ Vars(KB).Proposition 79. L-ABD(Cunit,= \ {R=},CNFS) is in P if instances (V , H, M, KB) are required to satisfy Vars(M) ⊆Vars(KB). This holds even if nothing is known about the satisfiability of M.Proof. Obvious since with the assumptions, KB is logically equivalent to a term which defines a complete assignmentto Vars(M). Thus we only have to decide whether this assignment satisfies M. (cid:2)15. DiscussionWe now explain in an intuitive manner what makes propositional abduction hard. We focus on Schaefer languages(Horn, dual Horn, bijunctive and affine), i.e., those languages for which deduction of CNFs is tractable, since they arethe most interesting ones for reasoning and knowledge representation. Recall from Proposition 35 that abduction isin NP for all such languages. Thus we explain what makes it NP-complete. We also compare our observations withBylander et al.’s about set-covering abduction [7].1278G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284We discuss the two most meaningful cases first, namely those of clausal Schaefer languages (Horn, dual Horn andbijunctive) with terms and literals as manifestations. The other cases are only briefly discussed afterwards.15.1. Manifestations expressed by termsGiven restrictions on the abduction problem, we say that a literal is a valid hypothesis (resp. individual manifes-tation) if it can be part of H (resp. of M) for some instance (V , H, M, KB) of the problem. For instance, the validindividual manifestations for N-ABD(CCNF ,POSTERMS) are all positive literals.Fact 80. Let C be a clausal Schaefer language, and assume a restriction on hypotheses and a restriction of manifes-tations to POSTERMS, NEGTERMS, or TERMS. Then propositional abduction is NP-hard for C if and only if it is sowith stronger restrictions or C can express both• implication from hypotheses to individual manifestations, that is, (cid:3)H → (cid:3)M for any valid hypothesis (cid:3)H and validmanifestation (cid:3)M , and• forbidden combinations of hypotheses, that is, (cid:3)H,1 ∨ (cid:3)H,2 for any two valid hypotheses (cid:3)H,1, (cid:3)H,2.For instance, P-ABD(C,POSTERMS) is NP-hard exactly when C can express both (x1 → x2) and (¬x1 ∨¬x2) for any two variables x1, x2. P-ABD(C,TERMS) is NP-hard exactly when P-ABD(C,POSTERMS) or P-ABD(C,NEGTERMS) is NP-hard, or when C can express (x1 → x2), (x1 → ¬x2) (implication) and (¬x1 ∨ ¬x2).Importantly, observe from Fact 80 that if abduction is NP-hard for, e.g., unrestricted terms as manifestations, thenso it is for positive or for negative terms with the same restriction on hypotheses. That is, arbitrary combinations ofpolarities in the manifestation do not add to the complexity of the problem. The same holds for hypotheses, that is,being able to explain with unrestricted hypotheses (or sets of hypotheses closed under complement) is not harder thanbeing able to explain with only positive or with only negative hypotheses.The validity of Fact 80 is proved by the generic reduction given by Lemma 61, applied to each precise problem.More intuitively, the idea is that in the NP-hard cases, each literal m to be explained in a term gives rise to severalpossible explanations, one for each implication h → m expressed by the knowledge base. Thus, each m gives riseto a disjunction of hypotheses, and the whole manifestation M gives rise to a conjunction of disjunctions (CNF) ofhypotheses. Now if the knowledge base can forbid some combinations of hypotheses, we thus have a second CNF,which excludes some models of the first one as explanations. Since the clauses in the first CNF are unbounded, it iseasily seen that this characterization of explanations captures a whole class of hard satisfiability problems.As for the tractable cases, if no implication from hypotheses to individual manifestations can be expressed, thenthere cannot be any explanation, since an explanation for a term is an explanation for each of its literals (Lemma 28).Now if no conjunction of hypotheses can be forbidden, then the search space can be reduced to the conjunction of allhypotheses (Lemma 43).Importantly, Fact 80 gives some intuition about the complexity of abduction for some classes of formulas which arenot captured by Schaefer’s framework of constraint languages. Consider for instance the class of acyclic Horn CNFs,that is, of Horn CNFs which do not contain any cyclic set of clauses of the form {(· · · ∨ ¬x1 ∨ · · · ∨ x2), (· · · ∨ ¬x2 ∨· · · ∨ x3), . . . , (· · · ∨ ¬xk ∨ · · · ∨ x1)}. Clearly, such a formula can contain an arbitrary number of clauses equivalent to(h → m) and (¬h1 ∨ ¬h2) for variables h, m, h1, h2. Thus Fact 80 gives the intuition that explaining positive termswith positive hypotheses is NP-hard when the knowledge base is restricted to be an acyclic Horn CNF. This resulthas indeed been shown by Selman and Levesque [48]. Note however that we cannot formally state Fact 80 for suchgeneral classes of formulas, since the ability to express a given relation would not be properly defined.Another important remark is that in our generic reduction from the satisfiability problem, all variables (or allpositive, all negative literals) not occurring in the query can be hypotheses while the problems remains hard. We comeback to this point when comparing to the case of manifestations expressed by single literals and in the conclusion.15.2. A parallel with set-covering abductionIn their seminal paper [7], Bylander et al. study the complexity of set-covering abduction, and also identify afrontier between some polynomial and some NP-complete abduction problems.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841279In their framework, an abduction problem is given by a set of manifestations M, a set of hypotheses H (hypothesesand manifestations are atoms), and by a map e (“explains”) from subsets of H to subsets of M. An explanation is asubset E of H such that e(E) = M and E is minimal for inclusion. The only assumptions about e is that the size ofits representation is polynomial in the size of H and M, and that it is tractable to compute e(E) for E ⊆ H as well asa kind of inverse of e (we refer to their paper for details).Bylander et al. study the complexity of abduction under various further assumptions about e. Of direct relevanceto our study are the following restrictions. The problem is said to be(cid:5)• independent if ∀E ⊆ H, e(E) =• monotonic if ∀E, E(cid:13) ⊆ H, E ⊆ E(cid:13) ⇒ e(E) ⊆ e(E(cid:13)), and• an incompatibility problem if there is a set I of pairs of elements of H (incompatible hypotheses) such thath∈E e({h}),∀E ⊆ H, (∃{h, h(cid:13)} ∈ I, h, h(cid:13) ∈ H ) ⇒ e(E) = ∅.Observe that all independent problems are also monotonic. The notion of independence is adapted to incompatibility ash∈E e({h}) as soon as there is no {h, h(cid:13)} ∈ Ifollows. An incompatibility problem is said to be independent if e(E) =such that h, h(cid:13) ∈ E.(cid:5)Proposition 81. (See [7].) The problem of deciding whether an explanation exists is in P for independent abductionproblems and for monotonic abduction problems. It is NP-complete for independent incompatibility problems.Although the framework is different from ours, it is clear that the condition for NP-hardness in Proposition 81 isvery close to our Fact 80.Incompatible pairs of hypotheses of Bylander et al.’s framework clearly correspond in our framework to sets ofhypotheses E which are not consistent with the knowledge base (KB ∧E is unsatisfiable).3 Now, since we studyabduction in classical propositional logic, where the consequence relation |(cid:12) is monotonic, our abduction problems aremonotonic. Thus, leaving details out, our abduction problems are incompatibility monotonic problems in the termsof [7], where e is represented in intension by the knowledge base KB. It is also easily seen that any independentincompatibility problem can be transformed into a problem in our framework.(cid:2)With this correspondence in mind, it is clear that Fact 80 confirms Bylander et al.’s results. Indeed, the conditionfor NP-hardness in Proposition 81 states that abduction is hard when some combinations of hypotheses are forbidden(expressiveness of implication from hypotheses to manifestations is implicit in their framework), but becomes tractablewithout this assumption. Thus our observation for terms as manifestations can be seen as generalizing Bylander et al.’sto more complex interactions between hypotheses, and between hypotheses and manifestations.15.3. Manifestations expressed by literalsThe condition for manifestations expressed by literals is a bit more involved than that for terms, but it can intu-itively be seen as the condition allowing to capture the complexity of conjunctive manifestations because it allows toexpress (m1 ∧ · · · ∧ mk → m). Call submanifestation any literal which is neither a hypothesis nor the manifestation(intuitively, the mi ’s in the previous implication). Given a polarity restriction (to positive, to negative, or to any literal),a submanifestation is said to be valid if it satisfies the restriction.Fact 82. Let C be a clausal Schaefer language, and assume a restriction on hypotheses and a restriction of manifes-tations to POSLITS, NEGLITS, or LITS. Then propositional abduction is NP-hard for C if and only if it is so withstronger restrictions or there is a polarity restriction on submanifestations such that C can express• implication from hypotheses to individual submanifestations, that is, (cid:3)H → (cid:3)S for any valid hypothesis (cid:3)H andvalid submanifestation (cid:3)S ,3 As observed in [7], considering incompatible pairs, triples, etc. instead of only pairs does not affect the complexity results.1280G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284• implication from arbitrary conjunctions of submanifestations to individual manifestations, that is, ((cid:3)S,1 ∧ · · · ∧(cid:3)S,k → (cid:3)M ) for any k ∈ N and any k valid submanifestations (cid:3)S,1, . . . , (cid:3)S,k,• forbidden combinations of hypotheses, that is, (cid:3)H,1 ∨ (cid:3)H,2 for any two valid hypotheses (cid:3)H,1, (cid:3)H,2.For instance, N-ABD(C,POSLITS) is NP-hard when, among other cases, C can express implication from hypothe-ses to individual (negative) submanifestations (¬x1 → ¬x2), implication from arbitrary conjunctions of submanifes-tations to manifestations ((¬x1 ∧ · · · ∧ ¬xn → xn+1), for any n), and forbidden conjunctions of hypotheses (x1 ∨ x2).Observe that in general, depending on the restriction on submanifestations, there may be several different expres-siveness conditions for the same restrictions on hypotheses and manifestations. Nevertheless, as in the case of termsas manifestations, it can be seen from Fact 82 that allowing arbitrary combinations of polarities for hypotheses makesthe problem no harder than allowing only positive or negative sets of hypotheses (the corresponding statement formanifestations is obvious).The proof of our observation can be derived from Lemmata 61 and 29. Intuitively, the “intermediate layer” servesto generate conjunctions of (intermediate) hypotheses needed to explain the manifestation, through each implicationy1 ∧ · · · ∧ yn → m where the yi ’s are submanifestations. Thus the manifestation can be explained by a disjunctionof conjunctions of submanifestations, each of which can be seen as a term which has to be explained by hypotheses.Thus, provided arbitrarily long conjunctions of submanifestations can entail the manifestation, the complexity is thesame as in the case of a term.An important difference to manifestations expressed by terms is that in general, NP-hardness arises when some “in-termediate” set of literals can be expressed, where these literals can neither be hypothesized nor observed. For instance,this explains why the complexity of explaining single literals from Horn CNFs falls from NP-complete to polynomialwhen all variables except those in the manifestation are hypotheses, as shown by Selman and Levesque [48]. This alsoconfirms their intuition that selecting the right set of literals is the computational core in abduction. Nevertheless, thisintuition is confirmed only as far as literals are considered as manifestations. Indeed, as we have seen this is not thecase for manifestations expressed by terms.Finally, like for terms our characterization gives intuition about the complexity of abduction for classes of formulaswhich are not captured by Schaefer’s framework. Consider for instance the class of monotone CNF formulas, that is,formulas in which every variable occurs always negated or always unnegated. A variable which always occurs withthe same polarity cannot be a submanifestation for an abduction problem like in Fact 82, since it should occur bothon the right of implications coming “from the hypotheses layer” (and thus, unnegated) and on the left of implications“going to the manifestation layer” (and thus, negated). Thus if all variables are monotone, the intermediate layer hasto be empty and thus, abduction should be tractable. We thus recover (the intuition about) a well-known result (see,e.g., [37, Section 4.2]).15.4. Manifestations expressed by clauses or CNFsWe are now able to explain a posteriori why one can observe that the complexity of the abduction problem isalways the same, may (positive) clauses or (positive) literals be used as manifestations.To this aim, first observe that naturally, when the problem is hard for literals, then so it has to be for clauses.Now concerning tractable cases, we use the condition exhibited above for literals. Indeed, the condition states thatabduction is tractable for literals if either(1) the knowledge bases cannot express a “two-layers” set of implications, from hypotheses to literals, and fromarbitrary combinations of the latter to the manifestation,(2) or no conjunction of hypotheses can be forbidden.It is easily seen that Condition (2) is the same, may literals or clauses (or terms) be considered as manifestations. Nowregarding Condition (1), if it is true with literals, then it has to be true for clauses, since literals are a special case ofclauses. We thus deduce tractability exactly as for literals.Let us mention that, as noted for example by Eiter and Makino [23, Section 5.2], there is a rather genericpolynomial-time reduction from clausal manifestations to single-literal manifestations. The idea is that M = ((cid:3)1 ∨· · · ∨ (cid:3)k) can be replaced with a literal (cid:3), over a fresh variable, up to adding constraints (cid:3)i → (cid:3) (i = 1, . . . , k) to theG. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841281knowledge base. This reduction covers many cases, but fails to explain, for instance, why P-ABD(C1v−Horn,CLAUSES)has the same complexity as P-ABD(C1v−Horn,LITS). Indeed, the added constraints would not preserve C1v−Horn ingeneral.Similarly to the case of clauses vs. literals, we can observe that, as far as clausal, Schaefer language are concerned,the complexity of abduction with manifestations expressed by CNFs is the same as that with manifestations expressedby terms. The case of affine knowledge bases is different, with problems harder for CNFs than for terms. Nevertheless,we do not elaborate on that point here, since most of the hardness for CNF manifestations simply comes from decidingwhether the manifestation alone is satisfiable.15.5. Affine knowledge basesThe case of equational instead of clausal languages is a bit more involved, but follows the same reasoning. We onlysketch the idea, since equational languages are less interesting than CNF ones for knowledge representation purposes.The main difference to the clausal case is that implications cannot be directly expressed. Indeed, if, for instance,h → m is a prime implicate of an affine knowledge base, then so must m → h be. In other words, in this case, theknowledge base entails h ↔ m. It follows that if the knowledge base entails both h1 → m and h2 → m, in fact itentails both h1 ↔ m and h2 ↔ m, and finally, any explanation for m must also satisfy h1 and h2.In general this makes the problem easier. The hard cases arise when implication can be simulated by imposingpolarities on some hypotheses, just as is done, for instance, in the proof of Proposition 68. This also explains why thecomplexity for equational languages sometimes changes from V-ABD to L-ABD, contrary to clausal languages.15.6. A note on NP-complete and coNP-complete casesAs can be seen from the results presented in the previous sections, it turns out that some restrictions on knowledgebases yield NP-complete problems, while others yield coNP-complete problems. This may seem strange at first sight,and deserves some explanation. In particular, we are not aware of any previous results in the literature where coNP-complete propositional abduction problems have been identified.2 is NPNP or, equivalently, NPNP∪coNP.Recall that (cid:2)PEssentially there are two sources of complexity in the general, (cid:2)P2 -complete abduction problem: finding the rightcandidate explanation E (nondeterminism, represented by the “basis” of the “exponential” NPNP∪coNP), and checking(cid:2)that it is indeed a witness, i.e., that KB ∧E |(cid:12) M (represented by the “exponent”).E is satisfiable and KB ∧(cid:2)That various restrictions on the hypotheses, manifestations, and knowledge bases in the abduction problem yieldNP-complete problems is not very surprising. These NP-complete cases occur when both checking a candidate ex-planation is a polynomial problem, which is the case for Schaefer languages (Proposition 35), and the satisfiabilityproblem for a set of clauses can be reduced to finding “the right candidate explanation”, as illustrated by Lemma 61.The perhaps more surprising coNP-complete cases all occur when checking a candidate explanation is coNP-completeand only one candidate explanation needs to be considered, removing nondeterminism as a source of complexity andthus, in some sense, the “basis of the exponential”. Observe that in this paper, when only one candidate explanationneeds to be considered, this is always a result of KB ∧H being trivially satisfiable as is, for example, the case forP-ABD(C1v ,M).(cid:2)16. Conclusion and future workWe have presented a thorough study of the complexity of deciding whether there is an explanation for a propo-sitional abduction problem, under various restrictions over hypotheses and manifestations. We have derived thecomplexity for every possible local restriction on the knowledge bases for the problem, that is, for every Booleanconstraint language and for every clausal or equational language, under the assumptions that the constraints in theknowledge base are given in extension or implicitly as CNFs or systems of equations, respectively. In particular thiscovers all the classical classes of CNF formulas defined by local properties and commonly considered for knowledgerepresentation purposes. Moreover, we have shown that the problem, when not tractable, is complete for one of NP,coNP, or (cid:2)P2 .1282G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284In order to obtain these results, we have used now well-known techniques for obtaining tractability (e.g., primeimplicate generation and projection). We have nevertheless uncovered new tractable cases. But we have also exhibitednew conditions for intractability, some of which turn out to be very weak, as already observed in [40].Importantly, this study allows us to precisely explain what makes propositional abduction hard when the knowledgebase is over a Schaefer language. Several points are very interesting, such as the need for variables which are neitherhypothesized nor observed for making the task of explaining a literal hard, contrary to the case of a term. It alsoturns out that the conditions for intractability of abduction are so weak that almost no interesting class of problemscan be tractable. In fact, only classes where there can be no explanation or there can be no conflicting hypothesesare tractable. This confirms the need for designing algorithms which are efficient in practice. Not surprisingly, ourconditions for intractability confirm the results and observations by Bylander et al. in a different framework [7].We have also argued that our explanation of the hardness of abduction gives hints for the complexity of problemswhich do not fit our framework. We demonstrated that argument for nonlocal (structural) propositional restrictions,but conjecture that it could also apply to completely different frameworks, such as abduction with nonclassical con-sequence relations or abduction in logic programming.Propositional abduction turns out to be a very rich problem from a computational complexity perspective. Byimposing various syntactic restrictions on the problem, no less than four complexity classes are covered. Thus ourresults can serve as a source of results allowing to derive the complexity of other problems. As a matter of fact,it is well-known that under various restrictions, several nonmonotonic reasoning problems reduce to each other. Asan example, our classification for abduction with negative hypotheses and positive terms as manifestations gives thecomplexity of circumscriptive inference of negative clauses, as far as clausal languages restrict the knowledge base.Indeed, it is known (see [40, Proposition 11]) that a negative clause follows from a knowledge base by circumscriptionif and only if its negation cannot be explained with all negative variables as hypotheses (except those in the clause).Thus, the complement of circumscriptive inference of negative clauses is at most as hard as abduction of positiveterms with negative hypotheses. Now, as we discuss in Section 15.1, our classification for Schaefer clausal languagesis preserved by the assumption that all variables except those in the manifestation are abducible. Thus, provided thatKB is in CNF and subject to a restriction over each of its clauses, we get that deciding whether a negative clausefollows from a knowledge base KB under circumscription of all variables is• polynomial-time solvable if KB is restricted to be Horn or 0-valid,• otherwise, coNP-complete if KB is restricted to be in 2CNF or dual Horn,• otherwise, (cid:6)P2 -complete.We believe that our results may be used to derive other results in a similar fashion.We now wish to note that in the paper, we have only studied the decision problem associated to abduction, leavingthe search problem out. That is, we have not considered the complexity of computing an explanation instead of onlydeciding whether there is at least one. However, obviously the search problem is hard as soon as the decision one is,and as is easily seen from our results, all of which are constructive, the converse is also true. Importantly, this is stilltrue when searching for ⊂-minimal explanations, because tractability of abduction entails tractability of satisfiability(considering an unsatisfiable manifestation) and deduction (considering an empty set of hypotheses), and this in turnis enough for minimizing an explanation with a greedy algorithm.To conclude, it would be very interesting to extend this work into several directions. First of all, other problemsrelated to abduction are of importance: for instance, deciding whether a hypothesis is relevant (part of at least onepreferred explanation) or necessary (part of all of them) [20]; enumerating all explanations [23]; counting the numberof (preferred) explanations [27]. Our work provides results which can serve as a basis for studying the complexity ofthese problems, in particular for deriving hard cases. Moreover, as we have done, it would be interesting to understandwhat exactly makes these problems easy or hard.Another important direction is to go further than the propositional setting and Schaefer’s framework. As mentionedabove, we believe that our observations about the reasons for (in)tractability can explain results beyond our framework.In particular, it would be interesting to study the complexity of abduction with nonlocal restrictions over knowledgebases (like, e.g., Eshghi [24] and del Val [15] do) and with higher-cardinality domains.G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–12841283References[1] B. Aspvall, M. Plass, R. Tarjan, A linear-time algorithm for testing the truth of certain quantified Boolean formulas, Information ProcessingLetters 8 (3) (1979) 121–123.[2] E. Böhler, N. Creignou, S. Reith, H. Vollmer, Playing with Boolean blocks, part I: Post’s lattice with applications to complexity theory, ACMSIGACT-Newsletter 34 (4) (2003) 38–52.[3] E. Böhler, N. Creignou, S. Reith, H. Vollmer, Playing with Boolean blocks, part II: Constraint satisfaction problems, ACM SIGACT-Newsletter 35 (1) (2004) 22–35.[4] E. Böhler, H. Schnoor, S. Reith, H. Vollmer, Bases for Boolean co-clones, Information Processing Letters 96 (2) (2005) 59–66.[5] G. Brewka, J. Dix, K. Konolige, Nonmonotonic Reasoning: An Overview, CSLI, 1997.[6] J. Buchler (Ed.), Philosophical Writings of Peirce, Dover Publications, New York, 1955.[7] T. Bylander, D. Allemang, M. Tanner, J. Josephson, The computational complexity of abduction, Artificial Intelligence 49 (1991) 25–60.[8] M. Cadoli, F. Donini, P. Liberatore, M. Schaerf, Preprocessing of intractable problems, Information and Computation 176 (2002) 89–120.[9] P. Chapdelaine, M. Hermann, I. Schnoor, Complexity of default logic on generalized conjunctive queries, in: Proc. 9th International Conferenceon Logic Programming and Nonmonotonic Reasoning (LPNMR’07), 2007, pp. 58–70.[10] N. Creignou, Personal communication, 2006.[11] N. Creignou, P. Kolaitis, B. Zanuttini, Preferred representations of Boolean relations, Technical Report 119, Electronic Colloquium on Com-putational Complexity (ECCC), 2005.[12] N. Creignou, B. Zanuttini, A complete classification of the complexity of propositional abduction, SIAM Journal on Computing 36 (1) (2006)207–229.[13] J. de Kleer, An assumption-based TMS, Artificial Intelligence 28 (1986) 127–162.[14] A. del Val, A new method for consequence finding and compilation for restricted languages, in: Proc. 16th National Conference on ArtificialIntelligence (AAAI’99), AAAI Press, 1999, pp. 259–264.[15] A. del Val, The complexity of restricted consequence finding and abduction, in: Proc. 17th National Conference on Artificial Intelligence(AAAI’00), AAAI Press/MIT Press, 2000, pp. 337–342.[16] A. del Val, On some tractable classes in deduction and abduction, Artificial Intelligence 116 (1–2) (2000) 297–313.[17] W. Dowling, J. Gallier, Linear-time algorithms for testing the satisfiability of propositional Horn formulae, Journal of Logic Programming 3(1984) 267–284.[18] A. Durand, M. Hermann, The inference problem for propositional circumscription of affine formulas is coNP-complete, in: Proc. 20th AnnualSymposium on Theoretical Aspects of Computer Science (STACS’03), in: Lecture Notes in Computer Science, vol. 2607, Springer-Verlag,2003, pp. 451–462.[19] U. Egly, T. Eiter, H. Tompits, S. Woltran, Solving advanced reasoning tasks using quantified boolean formulas, in: Proc. 17th NationalConference on Artificial Intelligence (AAAI’00), AAAI Press, 2000, pp. 417–422.[20] T. Eiter, G. Gottlob, The complexity of logic-based abduction, Journal of the ACM 42 (1) (1995) 3–42.[21] T. Eiter, G. Gottlob, N. Leone, Abduction from logic programs: Semantics and complexity, Theoretical Computer Science 189 (1997) 129–177.[22] T. Eiter, G. Gottlob, N. Leone, Semantics and complexity of abduction from default theories, Artificial Intelligence 90 (1997) 177–223.[23] T. Eiter, K. Makino, On computing all abductive explanations from a propositional Horn theory, Journal of the ACM 54 (5) (2007).[24] K. Eshghi, A tractable class of abduction problems, in: Proc. 13th International Joint Conference on Artificial Intelligence (IJCAI’93), MorganKaufmann, 1993, pp. 3–8.[25] T. Feder, M.Y. Vardi, The computational structure of monotone monadic SNP and constraint satisfaction: A study through datalog and grouptheory, SIAM Journal on Computing 28 (1) (1998) 57–104.[26] D. Geiger, Closed systems of functions and predicates, Pacific Journal of Mathematics 27 (1) (1968) 95–100.[27] M. Hermann, R. Pichler, Counting complexity of propositional abduction, in: Proc. 20th International Joint Conference on Artificial Intelli-gence (IJCAI’07), 2007, pp. 417–422.[28] A. Herzig, J. Lang, P. Marquis, Planning as abduction, in: IJCAI’01 Workshop on Planning under Uncertainty, 2001.[29] J. Hobbs, M. Stickel, D. Appelt, P. Martin, Interpretation as abduction, Artificial Intelligence 63 (1993) 69–142.[30] T. Horiyama, T. Ibaraki, Reasoning with ordered binary decision diagrams, Discrete Applied Mathematics 142 (1–3) (2004) 151–163.[31] H. Kautz, M. Kearns, B. Selman, Horn approximations of empirical data, Artificial Intelligence 74 (1995) 129–145.[32] R. Khardon, D. Roth, Reasoning with models, Artificial Intelligence 87 (1996) 187–213.[33] R. Ladner, On the structure of polynomial time reducibility, Journal of the ACM 22 (1975) 155–171.[34] J. Lang, P. Liberatore, P. Marquis, Propositional independence—formula-variable independence and forgetting, Journal of Artificial Intelli-gence Research 18 (2003) 391–443.[35] P. Liberatore, M. Schaerf, Compilability of propositional abduction, ACM Transactions on Computational Logic 8 (1) (2007).[36] F. Lin, J.-H. You, Abduction in logic programming: A new definition and an abductive procedure based on rewriting, Artificial Intelligence 140(2002) 175–205.[37] P. Marquis, Consequence finding algorithms, in: Handbook of Defeasible Reasoning and Uncertainty Management Systems (DRUMS), vol. 5,Kluwer Academic, 2000, pp. 41–145.[38] C. Morgan, Hypothesis generation by machine, Artificial Intelligence 2 (1971) 179–187.[39] G. Nordh, A trichotomy in the complexity of propositional circumscription, in: Proc. 11th International Conference on Logic for Programming,Artificial Intelligence and Reasoning (LPAR’04), 2005, pp. 257–269.[40] G. Nordh, B. Zanuttini, Propositional abduction is almost always hard, in: Proc. 19th International Joint Conference on Artificial Intelligence(IJCAI’05), 2005, pp. 534–539.1284G. Nordh, B. Zanuttini / Artificial Intelligence 172 (2008) 1245–1284[41] H. Pople, On the mechanization of abductive logic, in: Proc. 3rd International Joint Conference on Artificial Intelligence (IJCAI’73), 1973,pp. 147–152.[42] E. Post, The two-valued iterative systems of mathematical logic, Annals of Mathematical Studies 5 (1941) 1–122.[43] R. Pöschel, L. Kaluznin, Funktionen- und Relationenalgebren, DVW, Berlin, 1979.[44] W.V. Quine, On cores and prime implicants of truth functions, American Mathematical Monthly 66 (1959) 755–760.[45] R. Reiter, J. de Kleer, Foundations of assumption-based truth maintenance systems: preliminary report, in: Proc. 6th National Conference onArtificial Intelligence (AAAI’87), AAAI Press/MIT Press, 1987, pp. 183–188.[46] T. Schaefer, The complexity of satisfiability problems, in: Proc. 10th Annual ACM Symposium on Theory of Computing (STOC’78), ACMPress, 1978, pp. 216–226.[47] B. Selman, H. Kautz, Knowledge compilation and theory approximation, Journal of the ACM 43 (2) (1996) 193–224.[48] B. Selman, H. Levesque, Abductive and default reasoning: a computational core, in: Proc. 8th National Conference on Artificial Intelligence(AAAI’90), AAAI Press, 1990, pp. 343–348.[49] L. Simon, A. del Val, Efficient consequence finding, in: Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI’01), MorganKaufman, 2001, pp. 359–365.[50] B. Zanuttini, Approximation of relations by propositional formulas: complexity and semantics, in: Proc. 5th Symposium on Abstraction,Reformulation and Approximation (SARA’02), in: Lecture Notes in Artificial Intelligence, vol. 2371, Springer-Verlag, 2002, pp. 242–255.[51] B. Zanuttini, New polynomial classes for logic-based abduction, Journal of Artificial Intelligence Research 19 (2003) 1–10.