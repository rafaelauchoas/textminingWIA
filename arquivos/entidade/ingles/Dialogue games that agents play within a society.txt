Artificial Intelligence 173 (2009) 935–981Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDialogue games that agents play within a societyNishan C. Karunatillake a,∗, Nicholas R. Jennings a, Iyad Rahwan b,c, Peter McBurney da School of Electronics and Computer Science, University of Southampton, SO17 1BJ, United Kingdomb (Fellow) School of Informatics, University of Edinburgh, Edinburgh, EH8 9LE, United Kingdomc Faculty of Informatics, The British University in Dubai, PO Box 502216 Dubai, United Arab Emiratesd Department of Computer Science, University of Liverpool, Liverpool, L69 7ZF, United Kingdoma r t i c l ei n f oa b s t r a c tArticle history:Received 21 December 2007Received in revised form 19 January 2009Accepted 3 February 2009Available online 21 February 2009Keywords:Dialogue game protocolsMulti-agent negotiationSocial conflict resolutionArgument schemesHuman societies have long used the capability of argumentation and dialogue to overcomeand resolve conflicts that may arise within their communities. Today, there is an increasinglevel of interest in the application of such dialogue games within artificial agent societies.In particular, within the field of multi-agent systems, this theory of argumentation anddialogue games has become instrumental in designing rich interaction protocols and inproviding agents with a means to manage and resolve conflicts. However, to date, much ofthe existing literature focuses on formulating theoretically sound and complete models formulti-agent systems. Nonetheless, in so doing, it has tended to overlook the computationalimplications of applying such models in agent societies, especially ones with complexsocial structures. Furthermore, the systemic impact of using argumentation in multi-agent societies and its interplay with other forms of social influences (such as those thatemanate from the roles and relationships of a society) within such contexts has alsoreceived comparatively little attention. To this end, this paper presents a significant steptowards bridging these gaps for one of the most important dialogue game types; namelyargumentation-based negotiation (ABN). The contributions are three fold. First, we presenta both theoretically grounded and computationally tractable ABN framework that allowsagents to argue, negotiate, and resolve conflicts relating to their social influences within amulti-agent society. In particular, the model encapsulates four fundamental elements: (i) ascheme that captures the stereotypical pattern of reasoning about rights and obligationsin an agent society, (ii) a mechanism to use this scheme to systematically identify socialarguments to use in such contexts, (iii) a language and a protocol to govern the agentinteractions, and (iv) a set of decision functions to enable agents to participate in suchdialogues. Second, we use this framework to devise a series of concrete algorithms that giveagents a set of ABN strategies to argue and resolve conflicts in a multi-agent task allocationscenario. In so doing, we exemplify the versatility of our framework and its ability tofacilitate complex argumentation dialogues within artificial agent societies. Finally, we carryout a series of experiments to identify how and when argumentation can be useful foragent societies. In particular, our results show: a clear inverse correlation between thebenefit of arguing and the resources available within the context; that when agents operatewith imperfect knowledge, an arguing approach allows them to perform more effectivelythan a non-arguing one; that arguing earlier in an ABN interaction presents a more efficientmethod than arguing later in the interaction; and that allowing agents to negotiate theirsocial influences presents both an effective and an efficient method that enhances theirperformance within a society.© 2009 Elsevier B.V. All rights reserved.* Corresponding author.E-mail addresses: nnc@ecs.soton.ac.uk (N.C. Karunatillake), nrj@ecs.soton.ac.uk (N.R. Jennings), irahwan@acm.org (I. Rahwan), mcburney@liverpool.ac.uk(P. McBurney).0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.02.002936N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9811. IntroductionAutonomous agents usually exist within a multi-agent community, performing actions within a shared social contextto achieve their individual and collective objectives [81]. In such situations, the actions of these individual agents are in-fluenced via two broad forms of motivations. First, the internal influences reflect the intrinsic motivations that drive theindividual agent to achieve its own internal objectives. Second, as agents reside and operate within a social community, thesocial context itself influences their actions. For instance, when agents function within a society that has an organisationalstructure, they may assume certain specific roles or be part of certain relationships. These, in turn, may influence the actionsthat an agent may perform. Here, we categorise such external forms of motivations as social influences.Now, in many cases, both these forms of influence are present and they may give conflicting motivations to the individualagent. For instance, an agent may be internally motivated to perform a specific action. However, at the same time, it mayalso be subject to an external social influence (via the role it is assuming or the relationship that it is part of) not todo so. To illustrate this more clearly, let us consider an example relationship that exists between the two roles supervisorand student.1 Assume that, as a result of this supervisor-student relationship, any agent who assumes the role of studentis socially influenced to produce and hand over his thesis to his supervisor in a timely manner. Therefore, if an agentnamed Andy assumes the role of the student and another named Ben assumes the role of his supervisor, Andy will besocially influenced by Ben to hand over the thesis in time. However, if Andy also has a certain internal motivation to usethat limited time on some other activity (i.e., finish some programming work), a conflict will arise between Andy’s socialinfluence and his internal influence. In such a case, if Andy decides to pursue his internal motivation at the expense ofhis social influence, this may, in turn, manifest itself as a conflict between the two agents since Ben may well have aninterest in Andy abiding by his social influence and hand over his thesis in time. Also an agent may face situations wheredifferent social influences motivate it in a contradictory manner (one to perform a specific action and the other a differentconflicting action). For instance, if Andy is also part of a project, his project manager (Cindy) may socially influence Andyto use his time integrating some software component. Similar to above, in such an event, if the agent decides to abide by acertain social influence and forgo the other, it may also lead to a conflict between that agent and the agent that exerts theneglected social influence.In addition to such disparate motivations, due to the complexity and dynamism usually present within multi-agent sys-tems, in many cases, agents have to carry out their actions with imperfect knowledge about their environment. Specifically,when agents operate within a social context, they may not have complete knowledge about the capabilities, roles, or rela-tionships that they and their counterparts are deemed to assume within the society. Thus, in such instances, an agent maynot be aware of the existence of all the social influences that could or indeed should affect its actions. For instance, Andymay not be aware that Cindy was appointed as the new project manager. Thus, he may not believe that he is required toperform any integration work that Cindy may demand of him. Moreover, agents may also lack the knowledge of certainspecific social and internal influences that motivate other agents’ actions within the community. For instance, Andy may notbe aware of the fact that the university will incur a large penalty if the project integration is not completed in time. Thus,due to the absence of this knowledge, he may chose to write his thesis believing it is more important than the integrationwork. As can be seen, therefore, the lack of knowledge about social influences can also lead to conflicts between agents.From the above discussion, it can be seen that when agents operate in a society with incomplete information and withdiverse and conflicting influences, they may, in certain instances, lack the knowledge, the motivation and/or the capacityto abide by all their social influences. However, to function as a coherent society it is important for these agents to havea means to resolve such conflicts, manage their internal and social influences, and, thus, come to a mutual understandingabout their actions.In searching for a solution to this problem, we observe that when individuals operate within a human society, theyencounter similar forms of conflicts in their day to day life. For instance, when carrying out their actions humans encounterinfluences from different elements within the society, some of which are in conflict with one another. Furthermore, theyalso perform their actions in the presence of incomplete information about their social context. Thus, they also face conflictsdue to their lack of knowledge about certain influences within the society. However, mainly due to their skill in language,dialogue, and debate, human beings have adapted to use different forms of complex interactions to manage and resolve suchconflicts. To this end, researchers and philosophers from different branches of AI, linguistics, dialogue theory, and logic havelong been inspired by this human social ability and have tried to capture and model such behaviour [53,75]. Such studieshave given birth to a number of different dialogue models [80] suited to achieve different objectives (i.e., persuasion [1],negotiation [47,66], inquiry [29], deliberation [45], team formation [14] and decision support [26,82]; refer to Section 2 formore details).Building on these insights, much recent literature has advocated the Argumentation-Based Negotiation (ABN) dialogue typeas a promising way of dealing with the aforementioned conflicts in multi-agent systems (for a detailed review see [6,59]).In essence, the ABN form of a dialogue enhances the ways agents can interact within a negotiation encounter by allowingthem to exchange additional meta-information such as justifications, critics, and other forms of persuasive locutions withintheir interactions. These, in turn, allow agents to gain a wider understanding of the internal and social influences affecting1 We use this example throughout the paper to illustrate certain abstract notions more clearly.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981937their counterparts. Thereby, making it easier to resolve conflicts that arise due to incomplete knowledge. Furthermore, thenegotiation element within ABN also provides a means for the agents to achieve mutually acceptable agreements to theconflicts that they may have in relation to their different influences. Such enhancements lead to richer forms of negotiationwithin multi-agent systems than have hitherto been possible in game-theoretic [64] or heuristic-based [20] models and, byso doing, we believe, ABN provides the desired mechanisms for multi-agent systems to function as a coherent society.To date, however, much of the effort in the use of ABN in multi-agent systems suffer from a common fundamentaldrawback. Specifically, it models and analyses systems within a two-agent context and, thereafter, attempts to extrapolateor generalise the findings into a larger context with more than two-agents. But this reductionist approach largely ignoresthe social context of a multi-agent system. In particular, the systemic impact of ABN in multi-agent systems, its usage as aform of influence within a society, its co-existence with other forms of social influences in such systems, and how both ABNand social influences interplay with each other within a social context has received little attention within the community.Furthermore, most work focuses on the theoretical properties of the various ABN models. Thus, the soundness and com-pleteness of such models have received far greater attention than their computational properties such as the efficiency andeffectiveness of implementing them. This lack of empirical studies is well documented [41] and has led many to observethat there is a significant gap between the theory and the practise in this area.Against this background, the primary motivation of this paper is to model, experiment, and analyse a number of differentways by which agents can use argumentative dialogues to resolve the aforementioned forms of conflicts that may occurbetween agents in a multi-agent society. In particular, this paper builds upon our previous conceptual grounding [37–39]and advances the state of the art in the use of argumentation in MAS in three major ways.First, this paper presents a novel ABN framework that allows agents to detect, manage, and resolve conflicts related totheir social influences in a distributed manner within a structured agent society. The framework is composed of four mainelements; (i) a schema that captures how agents reason about influences within a structured society, (ii) a mechanism touse this stereotypical pattern of reasoning to systematically identify a suitable set of social arguments, (iii) a language and aprotocol to exchange these arguments, and (iv) a decision making functionality to generate such dialogues. One of the mainunique features of this framework is the fact that it explicitly captures the social influences endemic to structured agentsocieties. Moreover, it identifies the different ways agents can use these influences constructively in their dialogues. Thus,the framework leads the way to a thorough experimental analysis on the constructive interplay of ABN and social influ-ences. This interplay has not been sufficiently addressed in the existing literature and, by so doing, this paper presents thefirst application of argumentation-inspired techniques to specify a dialogue-game for arguing about social influences. Fur-thermore, our presumptive scheme for inferring social influences presents a new argumentation scheme [79] for reasoningwithin structured societies, and the way we use our argument scheme to systematically identify arguments within an agentsociety presents a successful attempt to use such schemes in a computational context. In all these different aspects, thispaper presents a strong theoretical contribution to both the argumentation and the multi-agent systems literature.The second major contribution of this paper stems from its experimental analysis. In particular, we present the firstextensive empirical evaluation of argumentation-based strategies within multi-agent systems. The lessons drawn from ourexperiments make the claims about the usefulness of ABN more precise and better empirically backed than they have everbeen. This contrasts with the informal justification of ABN found in most of the literature. More specifically, our results showthat allowing agents to argue during their negotiation interactions significantly enhances their ability to resolve conflicts and,thereby, increases the performance of the society even when functioning with high levels of incomplete information. Wealso show that the benefit of arguing is inversely correlated to the resources available within the system. More precisely, thecomparative advantage of arguing diminishes as the number of social influences (which act as resources) increase within thesociety. Our results also show that arguing earlier in an ABN interaction presents a more efficient method than arguing laterin the interaction. Moreover, we observe that allowing agents to trade social influences during their negotiations, enhancestheir ability to re-allocate these social influences in a more useful manner and, thus, perform more efficiently and effectivelyas a society.The third set of contributions come from our work in bridging the theory to practise divide in argumentation re-search. In particular, the types of social arguments and the strategies designed in this paper identify a number of differentways in which argumentation can be useful in multi-agent systems. In particular, these strategies capture inspiration fromboth the social science and the multi-agent systems literature (i.e., exercising the right to claim compensation, questionnon-performance, negotiating social influence) and represent an array of ways of how agents can manage conflicts in amulti-agent society. Moreover, we use our theoretical ABN framework to formulate concrete algorithms to model such ar-gumentative strategies and, in turn, use them to resolve conflicts in a multi-agent task allocation scenario. In so doing, thepaper starts to bridge the gap between theory and practise and provides a test-bed to evaluate how an ABN model can beused to manage and resolve conflicts in multi-agent societies. Furthermore, in bringing these socially inspired techniquesforward, modelling them within an argumentation context, and encoding such behaviour in a computational environment,this paper also adds significant contributions to both the argumentation and the multi-agent systems community.The remainder of this paper is structured as follows. First, Section 2 reviews the state of the art identifying the differentways the argumentation metaphor has inspired research with AI. It then situates our work within this domain and clearlyidentifies its scope and contributions. Given this context, Section 3 gives a formal representation of our argumentationframework. Next, Section 4 maps this theoretical model to a computational context to evaluate how our argumentationmodel can be used to manage and resolve conflicts within a social context. Subsequently, Section 5 presents our empirical938N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Fig. 1. The shaded areas highlight the scope of this paper.evaluation on how agents can use our ABN framework to argue and negotiate efficiently and effectively in a multi-agentsociety. Finally, Section 6 concludes by highlighting our main findings and future directions.2. Related workAs stated above, this paper centres around two broad areas of AI; namely argumentation-based negotiation and multi-agent systems. Thus, having explained how our work relates to multi-agent systems research (refer to Section 1), in thissection, we situate this paper in the broader context of argumentation in AI.In more detail, argumentation has proven to be a useful metaphor for specifying a variety of computational models andapplications in AI. Depending on their objectives, these computational frameworks have applied this metaphor in quite dis-tinct ways. In particular, we can characterise three major trends (refer to Fig. 1). The first uses the notion of argumentationas a metaphor for defeasible reasoning, in which conflicts within knowledge bases are resolved by analysing the way theyinteract (e.g. through support, attack, conflict, etc.). The second uses individual argument schemes simply as structures forinstantiating rhetorical statements, mainly for natural language generation or advice generation in expert systems. Finally,argumentation has also been used as a metaphor for defining communicative interactions among artificial and/or humanagents. In the following, we briefly survey each of these areas and, subsequently, situate our work in relation to it.2.1. Argumentation as a metaphor for defeasible reasoningOne of the main challenges in specifying autonomous agents in the sort of dynamic and uncertain environments wehave discussed earlier is the maintenance and updating of agent beliefs. In such cases, an agent may receive perceptualinformation that is inconsistent with its view of the world and would need to update its beliefs in order to maintainconsistency. Established ways of mechanising this kind of non-monotonic reasoning include truth maintenance systems [17],default logic [63] and circumscription [48].However, argumentation provides an alternative way to mechanise non-monotonic reasoning. Specifically, argument-based frameworks view this problem as a process in which arguments for and against conclusions are constructed andcompared. Non-monotonicity arises from the fact that new premises may enable the construction of new arguments tosupport new beliefs, or stronger counter-arguments against existing beliefs. For comprehensive surveys on argument-basedapproaches to non-monotonic reasoning, see [12,57]. More recently, argumentation has been also used to perform non-monotonic practical reasoning for situated autonomous agents (e.g. as in work by Pollock [55]).However, this paper is not a contribution to argumentation-based defeasible reasoning. As such, we are not concernedwith the formal analysis of the relationships among arguments and the various semantic or computational characterisationsof argument acceptability. Instead, our focus is on using argumentation as a metaphor for characterising communicationamong agents and in testing this communication empirically rather than analytically (refer to Section 5).2.2. Argument as a metaphor for generating rhetorical statementsIn many intelligent systems (e.g., expert systems or decision-support systems), there is often a need for the system togenerate persuasive statements to the user. In these systems, the argumentation metaphor has been used in a very differentway to the frameworks for symbolic defeasible reasoning presented in the previous subsection. Here, instead of being con-cerned with evaluating (e.g. accepting or rejecting) arguments based on their interaction with other arguments, argumentsare seen as structures (or schema) for generating persuasive utterances for the user. These schemas, which capture stereo-typical (deductive or non-deductive) patterns of reasoning found in everyday discourse, have been a focus of study of manyargumentation theorists (such as Walton [79] and Toulmin [75]). More information on argumentation for natural languagegeneration can be found in [24].N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981939The work presented in this paper does not aim at generating arguments as feed-back to users. However, we do employargument schemes in devising our multi-agent communication protocol for generating arguments among agents because itprovides us a systematic way of extracting arguments within our social context (refer to Section 3.2).2.3. Argumentation as a metaphor for dialogue gamesThe third major use of the argumentation metaphor in AI has been in the specification of rich models of interactionfor resolving conflicts among autonomous agents. To specify such interactions, one needs to define: (i) a communicationprotocol; and (ii) a set of decision mechanisms that enable agents to generate utterances and arguments using the protocol.In terms of communication protocols, the influence of argumentation has manifested itself through the widespread adop-tion of dialogue games. Such dialogue games define interactions between two or more players, where each player makes amove by making some utterance in a common communication language, and according to some pre-defined rules. Dialogue-games have their roots in the philosophy of argumentation and were used as a tool for analysing fallacious arguments [27].Walton and Krabbe [80] have identified various types of dialogues (such as information-seeking, persuasion, negotiation,and deliberation dialogues) and used dialogue games to study the notion of commitment in dialogue. To this end, dialoguegames often employ the notion of a commitment store which tracks participants’ (explicit or implicit) dialogical commitmentsduring a conversation, which can be used to reveal fallacies in conversation.In multi-agent systems, formal dialogue-game protocols have been presented for different atomic dialogue types [44],such as inquiry [29], deliberation [45], team formation [14] and interest-based negotiation [58].Argument schemes also offer a number of useful features for the specification of dialogue game protocols. Their structurehelps reduce the computational cost of argument generation, since only certain types of propositions need to be established.This very feature also reduces the cost of evaluating arguments. To this end, Atkinson et al. [4] use an argumentation schemefor proposing actions to structure their dialogue-game protocol for arguing about action.When it comes to decision mechanisms for generating dialogues, little work exists. Parsons et al. [51] use a set of genericpre-defined attitudes (e.g. confident, careful, cautious) and explore their impact on dialogue outcomes. Ramchurn et al. [60]and Kraus et al. [40] use arguments inspired by work on the psychology of persuasion [33]. Key arguments used in humanpersuasion are given computational representations, which are used to enable agents to generate a variety of arguments ina resource allocation context. Pasquier et al. [52] also present a framework for argument generation and evaluation basedon a computational model of cognitive coherence theory.The work presented in this paper is primarily a contribution to the use of argumentation as a metaphor for specifyingand implementing dialogue games to resolve conflicts about social influences in multi-agent systems (see Section 1). In moredetail, on one hand, this paper extends the state of the art in dialogue game protocols by presenting a new type of dialogueprotocol for arguing about social influences in a structured society. This protocol is presented with full operational semantics(axiomatic semantics are discussed in a companion technical report), and is built on a scheme inspired by recent advancesin social influence in multi-agent systems. On the other hand, this paper also provides a significant advancement to thepragmatic aspects of argumentation in MAS by providing a complete generative model for dialogues, and an extensive set ofexperiments to evaluate a variety of argument generation strategies. To date, no other generative framework has undertakensimilar empirical evaluation.3. The argumentation frameworkHaving explained our motivation and the scope of this work within the argumentation domain, we now proceed toexplain our argumentation framework. In particular, here we present both a formal and computational framework thatallows agents to argue, negotiate, and resolve conflicts in the presence of social influences. In essence, our frameworkconsists of four main elements: (i) a schema that captures how agents reason about social influences, (ii) a set of socialarguments that make use of this schema, (iii) a language and protocol for facilitating dialogue about social influence, and(iv) a set of decision functions that agents may use to generate dialogues within the protocol. In the following sub-sectionswe discuss each of these elements in more detail.3.1. The schemaAs the first step in modelling our argumentation framework, here we formulate a coherent mechanism to capture thenotion of social influences within a multi-agent society. As explained in Section 1, many different forms of external influ-ences affect the actions that an agent performs within a society. Moreover, these social influences emanate from differentelements of the society. In particular, many researchers now perceive a society as a collection of roles inter-connected viaa web of relationships [11,49]. These roles and relationships represent two important aspects of social influence within asociety. Specifically, when an agent operates within such a social context, it may assume certain specific roles, which will,in turn, guide the actions it performs. In a similar manner, the relationships connecting the agents acting their respectiveroles also influence the actions they perform. To date, an array of existing research, both in social science and in multi-agentsystems, attempts to capture the influences of these social factors on the behaviour of the individual (refer to [34]). Never-theless, there is little in the way of consensus at an overarching level. Some tend to be overly prescriptive, advocating that940N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981agents abide by their social influences without any choice or reasoning [22]. While others advocate a detailed deliberativeapproach, analysed at a theoretical level without evaluating its computational costs [16]. Against this background, in thefollowing we progressively introduce what we believe are a minimal set of key notions and explain how we adapt them tobuild a coherent schema that captures the notion of social influence.The notion of social commitment acts as our basic building block for capturing social influence. First introduced throughthe works of Singh [70] and Castelfranchi [10], the notion of social commitment remains simple, yet expressive, and is ar-guably one of the fundamental approaches for modelling social behaviour among agents in multi-agent systems. In essence,a social commitment (SC) is a commitment by one agent to another to perform a stipulated action. More specifically, it isdefined as a four tuple relation:SC = (x, y, θ, w)where x identifies the agent who is socially committed to carry out the action (termed the debtor), y the agent to whomthe commitment is made (termed the creditor), θ the associated action, and w the witness of this social commitment. It isimportant to note that, here, in the desire to maintain simplicity within our schema, we avoid incorporating the witness inour future discussions (as Castelfranchi did in his subsequent expositions). For ease of reference, this allows us to denotea social commitment that exists between a debtor x and a creditor y in relation to an action θ using the abbreviatedform SC.x⇒ yθHaving defined social commitment, Castelfranchi further explains its consequences for both the agents involved. In detail,a social commitment results in the debtor attaining an obligation toward the creditor, to perform the stipulated action. Thecreditor, in turn, attains certain rights. These include the right to demand or require the performance of the action, theright to question the non-performance of the action, and, in certain instances, the right to make good any losses suffereddue to its non-performance. We refer to these as rights to exert influence.2 This notion of social commitment resulting in anobligation and rights to exert influence, allows us a means to capture social influences between two agents. Thus, whena certain agent is socially committed to another to perform a specific action, the first agent subjects itself to the socialinfluences of the other to perform that action. The ensuing obligation, on one hand, allows us to capture how an agent getssubjected to the social influence of another, whereas, the rights to exert influence, on the other hand, model how an agentgains the ability to exert such social influence upon another. Thereby, the notion of social commitment gives an elegantmechanism to capture the social influences resulting between two agents.However, within a society not all social commitments influence the agent to the same degree. Certain social commit-ments may cause a stronger social influence than others. Furthermore, when agents operate in realistic and open multi-agentsocieties, they may face situations where different social influences motivate them in a contradictory manner (as discussedin Section 1). In order to capture such conflicts and conditions, here, we do not strictly adhere to the analysis of Castel-franchi that an honest agent will always gain an internal commitment (resulting in an intention to perform that action) forall its social commitments. On the contrary, in accordance with the work of Cavedon and Sonenberg [11] and Dignum etal. [15,16], we believe that all social commitments encapsulate their own degree of influence that they exert upon the indi-vidual. This will, in turn, result in agents being subjected to obligations with different degrees of influence. This, we believe,is an important characteristic in realistic multi-agent societies, where autonomous agents are subjected to contradictoryexternal influences (which may also conflict with their internal influences). Therefore, if an agent is subjected to obligationsthat either contradict or hinder each other’s performance, the agent will make a choice about which obligation to honour.3In order to facilitate this form of reasoning about conflicting social influences, we associate with each social commitmenta degree of influence f . Thus, when a certain agent attains an obligation due to a specific social commitment, it subjectsitself to its associated degree of influence. To reflect this in our abbreviated notation, we incorporate this degree of influenceparameter finto the social commitment notation as SC.x⇒ yθ, fGiven this basic building block for modelling social influence between specific pairs of agents, we now proceed to explainhow this notion is extended to capture social influences resulting due to factors such as roles and relationships within awider multi-agent society (i.e., those that rely on the structure of the society, rather than the specific individuals whohappen to be committed to one another).Specifically, since most relationships involve the related parties carrying out certain actions for each other, we can viewa relationship as an encapsulation of social commitments between the associated roles. To illustrate this, consider the2 This representation of rights and obligations as correlated pairs (one the dual of the other) conforms to the Hohfeldian analysis of “jural correlatives”where the two concepts are argued to be logically consistent within legal grounds and the existence of one necessarily implies the presence of theother [28]. However, within a distributed multi-agent environment, individual agents may lack perfect knowledge. Thus, they may not be aware of certainrights and obligations they hold within the society. In our work, since we aim to allow agents to argue and resolve such inconsistencies in knowledge (seeSections 4 and 5), we represent both these notions of obligation and rights explicitly within our ABN framework.3 From a deontic logic point of view, this notion of obligation is similar to that of a contrary-to-duty form [56]. A classic example is the moral dilemmaexperienced by Sartre’s soldier [76]; the obligation by duty to kill and the moral obligation not to kill. Within the logic community, a number of differentvariations of deontic logic have been proposed to formalise the semantics of such notions [25,56,65,76]. However, this paper does not attempt to formulatea new form of logic or attempt to forward a logical approach to reason about such decisions. Our primary aim here is to empirically evaluate how agentscan argue, negotiate, and resolve such conflicts that may occur in multi-agent systems. A more detailed discussion on these logical approaches is foundin [34].N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981941Given, an agent ai acting the role riLeads it to be part of the relationship pWith another agent a j acting the role r jA social commitment SCri ⇒r jθ, f• Leads to ai attaining an obligation O toward r j ,Which subjects it to an influence of degree fTo perform the action θassociated with p• And, in turn, leads to a j attaining the right (cid:4) toward riWhich gives it the ability to exert an influence of degree fTo demand, question, and require the performance of action θFig. 2. Schema of social influence.supervisor-student example introduced in Section 1. Now, let us consider the case where this supervisor-student relationshipsocially influences the student to produce and hand over his thesis to the supervisor in a timely manner. This influence wecan perceive as a social commitment that exists between the roles supervisor and student (the student is socially committedto the supervisor to perform the stipulated action). Here, within this social commitment, the student acts as the debtor andthe supervisor acts as the creditor. As a consequence of this social commitment, the student attains an obligation towardthe supervisor to carry out this related action. On the other hand, the supervisor gains the right to exert influence on thestudent by either demanding that he does so or through questioning his non-performance. In a similar manner, in the samesupervisor-student relationship, consider a case where the supervisor is influenced to review and comment on the thesis.This again is another social commitment associated with the relationship. However, in this instance the supervisor is thedebtor and the student the creditor. Thus, this social commitment subjects the supervisor to an obligation to review thethesis while the student gains the right to demand its performance. In this manner, social commitment again provides aneffective means to capture the social influences emanating through roles and relationships of the society (independently ofthe specific agents who take on the roles).This extension to the basic definition of social commitment is inspired primarily by the work of Cavedon and Sonen-berg [11]. However, it is important to note that our extension also broadens the original definition of social commitmentby allowing social commitments to exist between roles and not only between agents. In so doing, we relax the highlyconstraining requirement present within Cavedon and Sonenberg’s model that forces all known roles in a relationship tobe filled if any one is occupied. To explain this, consider the previous example relationship between the roles student andsupervisor. If we define the social commitment between these two roles it captures the general influence within the rela-tionship. Thus, if some particular person (or agent) assumes the role of student, he would still be obligated to produce thethesis to its supervisor even though, at the moment, the school has not appointed a specific supervisor to him. Therefore,this subtle yet important extension allows the agents to maintain a social commitment even though the other party of therelationship is not instantiated. Given this, we can now reflect this extension in our notation by stating either the debtor xcan be either an agent or a role (formally x, y ∈ (R ∪ A) whereor the creditor y in a social commitment denoted as SCR and A denote the set of roles and the set of agents respectively; refer Definitions 1 and 2).x⇒ yθ, fAnother important difference between the model we adopt here and the one proposed by Cavedon and Sonenberg, isthat here we choose to focus on the level of actions and commitments rather at the level of modalities of agents. In moredetail, the work by Cavedon and Sonenberg investigates how different social influences emanating via roles and relationshipsaffect the agent’s internal mental states, in particular, the prioritising of goals. However, here we refrain from going into thelevel of modalities of agents (such as goals, beliefs, and intentions), but rather stay at the level of actions.4 The motivationfor doing so is twofold. First, our primary interest in this work is to use our model to capture arguments that our agentscan use to argue about their actions in an agent society. We aim to do so by implementing this argumentation system andtesting its performance under various arguing strategies (refer to Section 5). To this end, we believe a model that focuseson the level of actions, as opposed to goals, beliefs, and intentions, will reduce the complexity of our effort. Second, anagent adopting a goal, a belief, or an intention can also be perceived as an action that it performs. For instance, when anagent changes a certain belief it has (i.e., the colour of the sky is not red, but blue), it can be perceived as performing twoactions. First, it performs the action of dropping the existing belief (that the sky is red), and, second, it performs the actionof adopting the new belief (that the sky is blue). Therefore, focusing on the level of actions loses little in terms of generality.However, we do acknowledge that focusing at this higher level of actions and not in the more deeper level of modalities,can sometimes limit the level of expressivity of our system. For instance, expressing how social commitments may affectthe internal mental states or the deliberation models of the agents, or modelling agent systems where the internal states ofthe agents (and their updates) are not public and cannot be observed at the multi-agent level. Nonetheless, the advantagesthat we gain by choosing a model that is easily implementable, we believe, are more important for our work.4 Readers interested in extended logical formalisms that capture how individual agent’s mental states such as beliefs, desires, goals, and intentions areaffected via different social influences are referred to [8,49,74].942N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Given this descriptive definition of our model, we can now formulate these notions to capture the social influences withinmulti-agent systems as a schema (refer to Fig. 2). In essence, the social influence schema captures the summary of the socialreasoning model explained above and forwards it as a schematic natural language representation. Such a representation isuseful to systematically identify and extract arguments and is widely used in argumentation literature [79]. Formulae (1)through (6) also present a notational representation of this schema.Definition 1. For n A, nR , n P , nΘ , nSC ∈ N+, let:} denote a finite set of agents,} denote a finite set of roles,} denote a finite set of relationships,• A = {a1, . . . , an A• R = {r1, . . . , rnR• P = {p1, . . . , pn P• SC = {SC1, . . . , SCnSC• Θ = {θ1, . . . , θnΘ• (cid:2) = { f | f ∈ R, 0 (cid:2) f (cid:2) 1} denote the degree of influence.} denote a finite set of actions,} denote a finite set of social commitments,Given these, let:• Act : A × R denote the fact that an agent is acting a role,• RoleOf : R × P denote the fact that a role is related to a relationship, and• DebtorOf : (R ∪ A) × SC denote that a role (or an agent) is the debtor in a social commitment,• CreditorOf : (R ∪ A) × SC denote that a role (or an agent) is the creditor in a social commitment,• ActionOf : Θ × SC denote that an act is associated with a social commitment,• InfluenceOf : (cid:2) × SC denote the degree of influence associated with a social commitment, and• AssocWith : SC × P denote that a social commitment is associated with a relationship.Having specified these definitions, let us consider a relationship p ∈ P that exists between the two roles ri, r j ∈ R and asocial commitment SC ∈ SC that is associated with the relationship p, which commits one of these roles (say ri ) to performto the other (say r j ) an action θ ∈ Θ with a degree of influence f ∈ (cid:2). To denote this social commitment more clearly, we. In particular, by stating the debtor x as role ri and the creditor y as role r j ,can use our general abbreviated notation SCx⇒ yθ, fwe obtain the social commitment SCri ⇒r jθ, f.RoleOf(ri, p) ∧ RoleOf(r j, p) ∧ AssocWith(SC, p) ∧ DebtorOf(ri, SC) ∧ CreditorOf(r j, SC)∧ ActionOf(θ, SC) ∧ InfluenceOf( f , SC) ↔ SCri ⇒r jθ, f.(1)x⇒ yDefinition 2. Let SCθ, f will result in the debtor attainingan obligation toward the creditor to perform a stipulated action and the creditor, in turn, attaining the right to influencethe performance of that action:∈ SC where x, y ∈ (R ∪ A). Thus, as per Castelfranchi [10], SCx⇒ yθ, fSCx⇒ yθ, fwhere:→ Ox⇒ yθ, f − ∧ (cid:4) y⇒xθ, f + ,(2)– Ox⇒ yθ, f − represents the obligation that x attains that subjects it to an influence of a degree f toward y to perform θ (here−the f– (cid:4) y⇒xθ, f + represents the right that y attains which gives it the ability to demand, question, and require x regarding thesign indicates the agent being subjected to the influence) andperformance of θ (here the fsign indicates that the agent attains the right to exert influence).+Definition 3. Now let us consider when a particular agent ai ∈ A assumes the debtor role ri in the above social structure.5This will entail the agent to obtain the social commitment associated with its role:Act(ai, ri) ∧ SCri ⇒r jθ, f→ SCai ⇒r jθ, f.(3)Definition 4. Similarly, if another agent a j ∈ A assumes the creditor role r j , it will also obtain the social commitmentassociated with its role:Act(a j, r j) ∧ SCri ⇒r jθ, f→ SCri ⇒a jθ, f.(4)5 Here, the term social structure is used to refer to the structure generated by the interlink of different roles and relationships. In the above case, thiswould be a simple structure with the two roles ri and r j interlinked via a single relationship p.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981943Combining (2), and (3) we can state that if an agent acts a certain role in a particular relationship and if there exist asocial commitment that is associated with that relationship that commits its role to act as a debtor, then as a result thatagent attains an obligation towards the corresponding creditor role to perform the related action. Thus, we obtain:Act(ai, ri) ∧ SCri ⇒r jθ, f→ Oai ⇒r jθ, f − .(5)Similarly, combining (2) and (4) we can state that if an agent acts a certain role in a particular relationship and if thereexists a social commitment that is associated with that relationship that commits its role to act as a creditor, then as aresult that agent attains a right to influence the corresponding debtor role to perform the related action. Thus, we obtain:Act(a j, r j) ∧ SCri ⇒r jθ, f→ (cid:4)a j ⇒riθ, f + .(6)Having captured the notion of social influences as a schema, we now explain how agents can use this to systematicallyidentify and extract the different types of social arguments to use within a multi-agent society.3.2. The social argumentsWhen agents operate within a society of incomplete information with diverse and conflicting influences, they may, incertain instances, lack the knowledge, the motivation, and the capacity to enact all actions associated with their socialcommitments. However, to function as a coherent society it is important for these agents to have a means to resolve suchconflicts and come to a mutual understanding about their actions. To this end, ABN is argued to provide such a means (seeSection 1). However, to argue in such a society, the agents need to have the capability to first identify the arguments touse. To this end, here we present how agents can use our social influence schema to systematically identify arguments tonegotiate within a society. We term these social arguments, not only to emphasise their ability to resolve conflicts withina society, but also to highlight the fact that they use the social influence present within the system as a core means inchanging decisions and outcomes within the society.More specifically, we have identified two major ways in which social influence can be used to change decisions andoutcomes and thereby resolve conflicts between agents. To explain the intuition behind these two ways more clearly, letus revisit our supervisor-student example. In particular, let us consider a situation where the PhD student Andy has twosocially motivated obligations; one towards his supervisor Ben to write a journal paper and second the towards his project-manager Cindy to help integrate a certain software component. Now, let us assume that due to time restrictions Andy canonly do one of these, and after considering what he believes to be the influences of both of these actions choses to integratethe software. Now, when Ben discovers this decision, he can attempt to follow two main ways to change this decision andconvince/persuade Andy to write the journal paper. The first, is to diagnose Andy’s original decision and try to find outif the facts that he used in his reasoning are correct. For instance, due to lack of perfect knowledge of any one of thepremises in the schema (i.e., his role, his correspondent’s role, about the relationship, about the social commitment, aboutthe degree of influence etc.), Andy might have made his decision in error. So, one way of changing Andy’s decision wouldbe to use argumentation dialogue to convince Andy about this incorrect information, correct his beliefs, and request Andyto consider his decision again with these corrected premises. The second method is to try and negotiate with Andy and,thereby, try to make writing the journal paper the more favourable option for Andy. In this way, Ben can try to introducenew parameters into Andy’s decision. For instance, he can explain why having a journal paper would make it easy for himto defend his thesis, or if he writes the journal paper now the conference paper he is scheduled to write next summerbecomes less important so he might be able to forgo that commitment. In this manner, Ben can use other social influenceshe may have on Andy as leverage to increase the degree of influence related to this action. If by doing so, Ben can convinceAndy that writing the journal paper is more influential than participating in the software integration, then Ben can achievehis objective of changing Andy’s decision. These two methods are depicted in Figs. 3(a) and 3(b) respectively. Now, havingexplained the basic intuition using our specific example, next we will capture these in a more general way and, in turn,systematically use the schema to identify arguments that agents can use in each of these methods.3.2.1. Socially influencing decisionsOne way to affect an agent’s decisions is by arguing about the validity of that agent’s practical reasoning [4,79]. Simi-larly, in a social context (as we have explained above), an agent can affect another agent’s decisions by arguing about thevalidity of the latter’s social reasoning. In more detail, agents’ decisions to (or not to) perform actions are based on theirinternal and/or social influences. Thus, these influences formulate the justification (or the reason) behind their decisions.Therefore, agents can affect each other’s decisions indirectly by affecting the social influences that determine their decisions(see Fig. 3(a)). Specifically, in the case of actions motivated via social influences through the roles and relationships of astructured society, this justification to act (or not to act) flows from the social influence schema (see Section 3.1). Given this,we can further classify the ways that agents can socially influence each other’s decisions into two broad categories:944N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Fig. 3. Interplay of social influence and ABN.(1) Undercut6 the opponent’s existing justification to perform (or not) an action by disputing certain premises within theschema which motivates its opposing decision.(2) Rebut the opposing decision to act (or not) by,(a) Pointing out information about an alternative schema that justifies the decision not to act (or act as the case maybe).(b) Pointing out information about conflicts that could or should prevent the opponent from executing its opposingdecision.Given this, in the following we highlight how agents can systematically use the social influence schema to identifythese possible types of arguments to socially influence each other’s decisions (for a formal notation representation of thesearguments expressed using the language defined in Section 3.3.1 refer to Table A.1 in Appendix A).1. Dispute (Dsp.) existing premises to undercut the opponent’s existing justification.i. Dsp. ai is acting debtor role ri .ii. Dsp. a j is acting creditor role r j .iii. Dsp. ri is related to the relationship p.iv. Dsp. r j is related to the relationship p.v. Dsp. SC is associated with the relationship p.vi. Dsp. fvii. Dsp. θ is the action associated with O.viii. Dsp. θ is the action associated with (cid:4).is the degree of influence associated with O.2. Point out (P-o) new premises about an alternative schema to rebut the opposing decision.i. P-o ai is acting the debtor role ri .ii. P-o a j is acting the creditor role r j .iii. P-o ri is related to the relationship p.iv. P-o r j is related to the relationship p.v. P-o SC is a social commitment associated with the relationship p.vi. P-o fis the degree of influence associated with the obligation O.vii. P-o θ is the action associated with the obligation O.viii. P-o θ is the action associated with the right (cid:4).ix. P-o ai ’s obligation O to perform the action θ .x. P-o a j ’s right to demand, question and require the action θ .3. Point out conflicts that prevent executing the decision to rebut the opposing decision.(a) Conflicts with respect to O.i. P-o a conflict between two different obligations due toward the same role.ii. P-o a conflict between two different obligations due toward different roles.(b) Conflicts with respect to (cid:4).i. P-o a conflict between two different rights to exert influence upon the same role.ii. P-o a conflict between two different rights to exert influence upon different roles.(c) Conflicts with respect to θ and another action θ (cid:10)such that (i) θ (cid:10)either hinders, obstructs, or has negative side effects to θ (see [4]).is an alternative to the same effect as θ ; (ii) θ (cid:10)6 The notion of undercut and rebut we use here is similar to that of [50].N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9819453.2.2. Negotiating social influenceAgents can also use social influences within their negotiations. More specifically, as well as using social argumentationas a tool to affect decisions (as above), agents can also use negotiation as a tool for “trading social influences”. In otherwords, the social influences are incorporated as additional parameters of the negotiation object itself [21] (see Fig. 3(b)). Forinstance, an agent can promise to (or threaten not to) undertake one or many future obligations if the other performs (ornot) a certain action. It can also promise not to (or threaten to) exercise certain rights to influence one or many existingobligations if the other performs (or not) a certain action. In this manner, the agents can use their obligations, rights, andeven the relationship itself as parameters in their negotiations. To this end, the following highlights a number of possibleways that agents can negotiate their social influences (for a formal notation representation of these arguments expressedusing the language defined in Section 3.3.1 refer to Table A.2 in Appendix A).4. Use O as a parameter of negotiation.i. Promise to (or threaten not to) undertake one or many future obligations if the other agent performs (or not) acertain action θ .ii. Promise to (or threaten not to) honour one or many existing obligations if the other agent performs (or not) a certainaction θ .5. Use (cid:4) as a parameter of negotiation.i. Promise not to (or threaten to) exercise the right to influence one or many existing obligations if the other agentperforms (or not) a certain action θ .6. Use third party obligations and rights as a parameter of negotiation.i. Third party obligations(i) Promise to (or threaten not to) undertake one or more future obligations toward ak to perform θ (cid:10), if a j would(or would not) exercise its right to influence a certain agent al to perform θ .(ii) Promise to (or threaten not to) honour one or more existing obligations toward ak to perform θ (cid:10), if a j would (orwould not) exercise its right to influence a certain agent al to perform θ .ii. Third party rights(i) Promise to (or threaten not to) exercise the right to influence one or many existing obligations toward ak toperform θ (cid:10), if a j would honour its existing obligation to perform θ .7. Use P as a parameter of negotiation.i. Threaten to terminate p (its own relationship with a j ) or pagent a j performs (or not) a certain action θ .ii. Threaten to influence another agent (ak) to terminate its relationship paction θ .(cid:10)(a third party relationship that ai has with ak), if the(cid:10)(cid:10)with a j , if a j performs (or not) a certainIn summary, these social arguments allow agents to resolve conflicts in two main ways. The first set of argumentsfacilitate critical discussion about the social influence schema; thus, these allow the agents to critically question, argueabout, and understand the underlying reasons for each others’ action. This form of engagement not only allows the agentsto extend their incomplete knowledge of the society, but also provides a means to convince their counterparts to changedecisions based on such incomplete information and, thereby, resolve conflicts within a society. The second set of argumentsallows the agents to exploit social influences constructively within their negotiations. Thus, providing agents with additionalparameters to influence their counterpart to reach agreements and thereby resolve conflicts via negotiation.3.3. The language and protocolSections 3.1 and 3.2 formulated a schema that captures the notion of social influences and, in turn, we systematicallyused that schema to identify social arguments that allow agents to resolve conflicts within a social context. However,identifying such arguments is merely the first step. Agents also require a means to express such arguments and a mechanismto govern their interactions that would guide them to resolve their conflicts in a multi-agent society. To this end, thefollowing presents the language and the protocol components defined within our ABN framework.3.3.1. The languageThe language plays an important role in an ABN framework. It not only allows agents to express the content and con-struct their arguments, but also provides a means to communicate and exchange them within an argumentative dialogue.Highlighting these two distinct functionalities, we define the language in our framework at two levels; namely the domainlanguage and the communication language. The former allows the agents to specify certain premises about their social con-text and also the conflicts that they may face while executing actions within such a context. The latter provides agents witha means to express these arguments and, thereby, engage in their discourse to resolve conflicts. Inspired by the work ofSierra et al. [69], this two tier definition not only allows us an elegant way of structuring the language, but also provides ameans to easily reuse the communication component within a different context merely by replacing its domain counterpart.We now explain each of these in more detail.946N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Domain Language: This consists of nine language particles. Of these, seven allow the agents to describe their social contextand these flow naturally from our social influence schema (i.e., Act, RoleOf, DebtorOf, CreditorOf, ActionOf, InfluenceOf, andAssocWith). In addition to these, we define two additional predicates that provide a means to express the conflicts that theagents may face while executing their actions. Extending the notation detailed in Section 3.1, we can formally define ourdomain language as follows:Definition 5. Let the domain language L contain the following predicates:– Act : A × R denote the fact that an agent is acting a role.– RoleOf : R × P denote the fact that a role is related to a relationship.– DebtorOf : (R ∪ A) × SC denote that a role (or agent) is the debtor in a social commitment.– CreditorOf : (R ∪ A) × SC denote that a role (or agent) is the creditor in a social commitment.– ActionOf : Θ × (SC ∪ O ∪ (cid:4)) denote that an act is associated with a social commitment, obligation, or right.7– InfluenceOf : (cid:2) × (SC ∪ O ∪ (cid:4)) denote the degree of influence associated with a social commitment, obligation, or right.– AssocWith : SC × P denote that a social commitment is associated with a relationship.– do : A × Θ denote the fact that an agent is performing an action (expressed in the abbreviated form do(θ) when theagent is unambiguous).– Conflict : do( A × Θ) × do( A × Θ) denote the fact that performing the corresponding actions gives rise to a conflict.In addition to these language predicates, two specific forms of actions commonly used within this domain are adoptinga new obligation, right, or relationship and terminating (or dropping) an existing one. To denote these specific actions weuse two special action predicates adopt and drop respectively. Formally,– adopt(z) ∈ Θ where z ∈ (O ∪ (cid:4) ∪ P ) denotes the action of adopting a new obligation, right, or relationship.– drop(z) ∈ Θ where z ∈ (O ∪ (cid:4) ∪ P ) denotes the action of terminating an existing obligation, right, or relationship.Having defined these predicates, we can now give a Backus–Naur Form (BNF) specification of the syntax of the domain(cid:10) ∈ (cid:2). Given these, a sentence l ∈ L can take thelanguage L. Let a ∈ A, r ∈ R, p ∈ P , sc ∈ SC, θ ∈ Θ , o ∈ O , τ ∈ (cid:4), and f , fform,< sentence > ::= < simple_sentence >|< action_sentence >|< conf_sentence >|¬< sentence >|< sentence > ∧ < sentence >< simple_sentence > ::= Act(a, r)|RoleOf(r, p)|AssocWith(sc, p)| f > f(cid:10)|DebtorOf(r, sc)|DebtorOf(a, sc)|CreditorOf(r, sc)|CreditorOf(a, sc)|ActionOf(θ, sc)|ActionOf(θ, o)|ActionOf(θ, τ )|InfluenceOf( f , sc)|InfluenceOf( f , o)|InfluenceOf( f , τ )|< action_sentence > ::= do(a, θ)|do(a, adopt(o))|do(a, adopt(τ ))|do(a, adopt(p))|do(a, drop(o))|do(a, drop(τ ))|do(a, drop(p))< conf_sentence > ::= Conflict(< action_sentence >, < action_sentence >)Communication Language: This consists of seven illocutionary particles; namely Open-Dialogue, Propose, Accept, Reject,Challenge, Assert, and Close-Dialogue. Mainly inspired from the works of Amgoud et al. [2], MacKenzie [42], and McBur-ney et al. [47], these form the building blocks of our dialogue game protocol explained below (refer to Section 3.3.2). To7 Note that within our domain language, the two schema predicates ActionOf and InfluenceOf are extended to rights and obligations as well as socialcommitments. This is to allow agents to directly discuss about the respective parameters such as actions and degrees of influence related to their individualobligations and rights, rather than referring to them indirectly via social commitments. Even though this may allow agents to refer to these parametersin two different ways (i.e., indirectly via social commitments and directly through their obligations and rights), since agents would refer to these quiteregularly when they argue about their social influences, we believe allowing such a direct method of reference is a useful replication.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981947specify these locutions we use a notation similar to that of [47]. In particular, we define the different legal locutions (num-bered L1∼L11) of our communication language as follows.8 Here, ap denotes the proposing agent, ar the responding agent,and ax1 and ax2 represent either agent:Definition 6.• OPEN-DIALOGUE◦ Usage:L1 : Open-Dialogue(ap, ar) orL2 : Open-Dialogue(ar, ap).◦ Informal Meaning: Indicates the willingness to engage in the negotiation dialogue. More specifically, the former isused by the proposing agent to initiate the dialogue, while the latter is used by the responding agent to express itswillingness to join that dialogue.9• PROPOSE◦ Usage:L3 : Propose(ap, ar, do(ar, θr), do(ap, θp)).◦ Informal Meaning: A proposal from ap to ar requesting ar to perform θr and in return for ap performing θp . Thus,the request of this proposal is do(ar, θr) and the reward is do(ap, θp).• ACCEPT◦ Usage:L4 : Accept(ar, ap, do(ar, θr), do(ap, θp)).◦ Informal Meaning: Accept the proposal, thereby agree to perform the requested θr in return for do(ap, θp).• REJECT◦ Usage:• CHALLENGE◦ Usage:L5 : Reject(ar, ap, do(ar, θr), do(ap, θp)).◦ Informal Meaning: Reject the request to perform the requested θr in return for do(ap, θp).L6: Challenge(ap, ar, Reject(ar, ap, do(ar, θr), do(ap, θp)))L7: Challenge(ax2 , ax1 , Assert(ax1 , ax2 , l)).◦ Informal Meaning: Challenge the justification for a certain premise. In particular, this can challenge:– the justification for a reject, or– the justification for a certain assertion where l denotes the asserted premise which can be a well-formed formula(wff) of domain language L.• ASSERT◦ Usage:L8: Assert(ax1 , ax2 , l)L9: Assert(ax1 , ax2 , ¬l).◦ Informal Meaning: Asserts a particular set of premises or their negations. Here, l denotes the asserted premise, whichcan be a wff of domain language L. Asserting the negation would account to disputing that premise.• CLOSE-DIALOGUE◦ Usage:L10: Close-Dialogue(ap, ar) orL11: Close-Dialogue(ar, ap).◦ Informal Meaning: Indicates the termination of the dialogue. In particular the former is used by the proponent toindicate terminating the dialogue whereas the latter is used by the respondent to indicate existing the dialogue.10Both these language components (the domain and the communication) collectively allow the agents to express all thesocial arguments identified in Section 3.2 (i.e., socially influencing decisions and negotiating social influences). These arepresented in Appendix A Tables A.1 and A.2 respectively. Given the language element of our ABN framework, we will nowproceed to describe the protocol.8 Here, we only specify the usage and informal meaning for each of the predicates in our communication language. Due to space restrictions, the detailedformal semantics of the language are presented as a separate technical report [36].9 Please note that even though the two locutions L1 and L2 have a similar syntax, they have different usage, pre-conditions, and effects. These distinctionsare highlighted by the axiomatic semantics (refer to [36]) and the operational semantics (refer to Appendix B).10 Note that, similar to Open-Dialogue locution, locutions L10 and L11 also have different usage, pre-conditions, and effects. These distinctions are high-lighted by the axiomatic semantics (refer to [36]) and the operational semantics (refer to Appendix B).948N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Fig. 4. Dialogue interaction diagram.3.3.2. The protocolIn essence, the protocol governs the agents’ interactions and acts as a guidance for them to resolve their conflicts. Whilethe overall structure of our protocol is inspired from the work on computational conflicts by Tessier et al. [73], the workson pragma-dialectics proposed by van Eemeren and Grootendorst [77], and that on dialogue games conducted by McBurneyet al. [46,47], and Amgoud et al. [2] contributed greatly in defining its operational guidelines.In overview, our protocol consists of six main stages: (i) opening, (ii) conflict recognition, (iii) conflict diagnosis, (iv) conflictmanagement, (v) agreement, and (vi) closing. The opening and closing stages provide the important synchronisation pointsfor the agents involved in the dialogue, the former indicating its commencement and the latter its termination [47]. The fourremaining stages allow agents to recognise, diagnose, and manage their conflicts. In more detail, in the conflict recognitionstage, the initial interaction between the agents brings the conflict to the surface. Subsequently, the diagnosis stage allowsthe agents to establish the root cause of the conflict and also decide on how to address it (i.e., whether to avoid the conflictor attempt to manage and resolve it through argumentation and negotiation [35]). Next, the conflict management stageallows the agents to argue and negotiate, thus, addressing the cause of this conflict. Finally, the agreement stage brings theargument to an end, either with the participants agreeing on a mutually acceptable solution or agreeing to disagree dueto the lack of such a solution. These four stages for arguing to resolve conflicts in a social context map seamlessly to thefour stages in the pragma-dialectics model for critical discussion proposed by van Eemeren and Grootendorst [77]; namelyconfrontation, opening, argumentation, and concluding respectively.In operation, our protocol follows the tradition of dialogue games [46,47] where a dialogue is perceived as a game inwhich each participant make moves (termed dialogue moves) to win or tilt the favour of the game toward itself. Here,the protocol defines the different rules for the game such as locution rules (indicating the moves that are permitted),commitment rules (defining the commitments each participant incurs with each move), and structural rules (that define thetypes of moves available following the previous move).11Against this background, here, the objective of our protocol is to govern the pair-wise interactions between the agents(those that assume the debtor and creditor roles within a society), guiding the two parties to resolve conflicts related totheir social influences. The two parties within the dialogue are referred to as the proponent (the one who initiates thedialogue) and the respondent (the one who responds). The proponent can be either the debtor or creditor agent, while therespondent will be the corresponding other (i.e., in case debtor initiates, the creditor will act as the respondent).Fig. 4 presents an abstract view of our protocol.12 Here, the nodes of the graph represent the various communicationpredicates allowed in our ABN protocol while the edges denote the legal transitions permitted between these distinctdialogue moves. For instance, consider the Reject locution in Fig. 4. An agent can choose to reject a proposal only after itscounterpart has forwarded that proposal. Thus, a Reject dialogue move becomes valid only after a Propose locution, whichis defined as a pre-condition for this locution. On the other hand, if its proposal is rejected, the proponent can respondin one of three possible ways. It may either forward an alternative proposal, try to find the reason for this rejection bychallenging this decision, or end the negotiation dialogue. These three possibilities are represented in Fig. 4 by allowingagents to utter either a Propose, Challenge, or Close-Dialogue move after a Reject.11 Note, this is not intended to be an exhaustive list of rules, but rather the most important ones in our context. For instance, if the aim of the dialoguegoverned by the protocol is persuasion, the win-loss rules specifying what counts as a winning or losing position would become a vital component. For amore detailed discussion refer to [46].12 Note that this diagram only presents an overall abstract view of the protocol. As explained later, detailed axioms of the protocol are given in [36] andits operational semantics are defined in Appendix B.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981949In a more detailed form, we can define these rules as a series of axioms. In particular, for each communicative predicate,we specify the purpose of that dialogue move, its structural rules by way of pre- and post-condition utterances, and anyeffects it may have on both the commitment (CS) and the information stores (IS) of the related agents.13 The followingspecifies these detailed axiomatic rules for the Reject locution.REJECT (Locution L5): If the received proposal failed to satisfy the respondent’s acceptance conditions, it will retort backwith a rejection. In effect both agents would record a dialogical commitment to the fact that the respondent rejectedthe proposal.– Usage:• L5 : Reject(ar, ap, do(ar, θr), do(ap, θp)).– Meaning: By uttering the locution “Reject(ar, ap, do(ar, θr), do(ap, θp))”, agent ar indicates to agent ap that ar rejectsthe proposal made by ap in a prior utterance of the locution “Propose(ar, ap, do(ar, θr), do(ap, θp))”.– Pre-conditions:• For Reject(ar, ap, do(ar, θr), do(ap, θp))Propose(ap, ar, do(ar, θr), do(ap, θp)) ∈ CSi−1(ar).– Valid Responses:• For Reject(ar, ap, do(ar, θr), do(ap, θpi )):Propose(ap, ar, do(ar, θr), do(ap, θp(i+1) ))Challenge(Reject(ar, ap, do(ar, θr), do(ap, θpi )))Close-Dialogue(ap, ar).– IS (information store) updates: none– CS (commitment store) updates:• CSi(ap) ← CSi−1(ap) ∪ Reject(ar, ap, do(ar, θr), do(ap, θp))• CSi(ar) ← CSi−1(ar) ∪ Reject(ar, ap, do(ar, θr), do(ap, θp)).Due to space restrictions, we present the full axiomatic rules governing each language element in our protocol as a sepa-rate technical report. Thus, a reader interested in the comprehensive axiomatic rules of the protocol is referred to [36]. Now,having explained our ABN protocol, we next proceed to detail the final component of our ABN framework; the individualdecision functions.3.4. The decision functionsThe protocol described in the previous sub-section gives agents a number of different options, at various stages, as towhat utterances to make. For instance, after a proposal the receiving agent could either accept or reject it. After a rejection,the agent may choose to challenge this rejection, end the dialogue, or forward an alternative proposal. An agent, therefore,still requires a mechanism for selecting a particular utterance among the available legal options. To this end, in the followingwe define the various decision mechanisms required by both the proponent and the respondent agent to use the definedprotocol to argue, negotiate, and, thereby, resolve conflicts within a multi-agent society. Here, the term proponent is usedto specify the agent that attempts to negotiate the services14 of another to accomplish one of its actions. The respondent,on the other hand, denotes its counterpart participating this negotiation.In specifying these mechanisms, we use a representation similar to that of McBurney et al. [47], which investigates theuse of dialogue game protocols for modelling consumer purchase negotiations. It allows a coherent way of modelling thedecision functions in line with the protocol, which, in turn, help us define the operational semantics (refer to Appendix B)of the protocol in a systematic manner. In this context, we use the same style to define the decision functions (and laterthe operational semantics; see Appendix B) required by individual agents to use ABN to resolve conflicts within the socialcontext of a multi-agent system.3.4.1. Decision mechanisms for the proponentIn essence, the proponent’s decision model has 11 basic decision mechanisms (numbered P1∼P11). These collectivelyallow the proponents to use the above protocol to argue, negotiate, resolve any conflicts, and, thereby, acquire the servicesof their counterparts to achieve actions.P1 Recognise Need: A mechanism that allows the agent to decide whether it requires the services of another to achieve acertain action (θ). This will have two possible outcomes. In case the mechanism recognises that it needs to acquire theservices of another agent, it will forward the outcome needService(θ). Otherwise, it will forward noNeedService(θ).13 Agents participating in dialogue games would establish and maintain their individual commitment (CS) and information stores (IS) to record both thedialogical and action commitments incurred (refer to [80]), as well as any knowledge (or information) gained during the dialogue. Agent’s knowledge-basewould include both commitments and information gained and stored in these CS and IS during their interaction, as well as any other information the agentmay possess about its context.14 Here, the term service refers to an action or sequence of actions performed by one agent at the request of another.950N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9811: Q (θ ) ← ∅2: θp ← getNext(Θ)3: while (θp (cid:15)= ∅) do4:if (Capable(do(ap, θp)) ∧ B5:6:end ifθp ← getNext(Θ)7:8: end while9: return Q (θ )Q (θ ) ← Q (θ ) ∪ Propose(ap, ar , do(ar , θr ), do(ap, θp))apdo(ar ,θr ) > Capdo(ap ,θp )) thenAlgorithm 1. Decision algorithm for generating proposals.P2 Generate Proposals: A mechanism that allows the proponent to generate proposals in order to negotiate the requiredservice from its counterpart. In generating such proposals, each proponent would take two rationality conditions intoconsideration; namely (i) the feasibility of the proposal and (ii) its viability.15 In more detail, given that we assume ouragents do not intentionally attempt to deceive one another, the proponent must have the capability to perform thereward suggested in each proposal. Thus, they will only generate proposals that they believe they have the capabilityto honour. Furthermore, given that our agents are self-interested, each proposal that they generate also needs to beviable on their behalf. Thus, the cost incurred by the proponent in performing the reward (for the generic proposalapPropose(ap, ar , do(ar , θr ), do(ap, θp)) this is denoted as Cdo(ap ,θp )) should not exceed the benefit it gains from its respondentdo(ar ,θr )). This is highlighted in Algorithm 1.16 The outcome of this decisionperforming the requested action (denoted as Bmechanism would be a non-empty set of proposals with the required action θ as the request and an array of bothfeasible and viable rewards. We denote this unordered non-empty finite set as Q (θ).apP3 Rank Proposals: A mechanism that allows the proponent to rank its generated set of proposals. In more detail, theapdo(ap ,θp )) as the ranking parameter. More specifically, a proposalagent would use the cost of performing the reward (Cthat contains a reward that costs less to perform will rank higher than one that costs more. Thus, the outcome of thismechanism is an ordered list of proposals denoted as:S(θ) =(cid:2)(cid:3)S0(θ), S1(θ), . . . , S i(θ), . . . , St(θ)where cost(cid:4)(cid:5)S i(θ)(cid:4)(cid:5)S i+1(θ); t ∈ N.< costP4 Select Proposal: A mechanism that allows the agent to select a proposal to forward to its counterpart. Generally, theagent will take the next highest ranked proposal from its ordered proposal list S(θ). If there is no such proposal (thefinal possible proposal has already been sent) the mechanism will return ∅, in which case the agent will proceed toterminate the dialogue. Thus, there are two possible outcomes. If there is a proposal to forward next, then it will returnthat proposal S i(θ). Otherwise the decision mechanism will return ∅.P5 Find Justification, Continue Negotiation, or Terminate: If a certain proposal is rejected, the proponent needs to decidewhether to find the justification for that rejection, continue negotiation with an alternative proposal, or terminate itsnegotiation. This is a tactical choice for the agent and the decision criteria will depend on its argumentation strat-egy. Corresponding to these three options, this mechanism has three possible outcomes; (i) challengeReject(S i(θ)), (ii)continue(S i(θ)), or terminate(S i(θ)).P6 Evaluate Justifications: A mechanism that allows an agent to compare its own justification (H p) with its counter-part’s (Hr) and analyse any inconsistencies between them. A number of different approaches can be used to designthis mechanism ranging from a simple arbitration heuristic to a more complicated defeasible system that is based onthe strength of justification or even a repeated learning heuristic. In our implementation, we use a simple validationheuristic that has the ability to identify the accuracy of these justifications by examining the validity of each of theirrespective premises (for a more detailed description of this implementation refer to Algorithm 4 in Section 5.1). Irre-spective of how this is implemented, in essence, the decision mechanism will have three possible outcomes. First, if themechanism finds all premises within a certain justification (either the proponent’s or the respondent’s) to be valid, thenit will indicate this through the valid(H) outcome where H = {H p, Hr}. Second, if it finds a certain premise l (wherel ∈ H ) in either the proponent’s or the respondent’s justification to be invalid, it will then indicate this via the invalid(l)outcome. Third, if the mechanism requires more information to accurately identify whether a certain premise is valid orinvalid, then it will indicate this via the outcome needMoreJustification(l).P7 Extract Justification: A mechanism that allows an agent to search within its own knowledge-base to extract jus-tifications for certain premises. Even though our framework has two specific types of challenges, L6 and L7 (see15 This work assumes the agents are self-interested in nature and do not actively attempt to deceive one another. Under these assumptions, we believe,the viability and feasibility are the two most important factors to consider. However, they do not represent the only two factors. For instance, whenagents generate proposals, issues such as trust and reputation of their counterpart may also be important, especially in open multi-agent systems [30]. Byincorporating such elements into the decision criteria of the above algorithm, our model can be easily extended to accommodate these different issues.Nevertheless, such an extension is beyond the scope of this paper.16 Here, we define these algorithms at an abstract level that is independent of any domain. However, by defining how the agents can evaluate these costs,benefits, and feasibility conditions these can be set to reflect a particular context. To aid understanding, Section 4.4 presents one such mapping within ourexperimental context.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981951do(ap ,θp ) > C arAccept(ar , ap, do(ar , θr ), do(ap, θp))1: if (Capable(do(ar , θr )) ∧ Bar2:3: else4:5: end ifReject(ar , ap, do(ar , θr ), do(ap, θp))do(ar ,θr )) thenAlgorithm 2. Decision algorithm for evaluating proposals.Section 3.3.2), only L7 is applicable to the proponent. Reasoning about challenges of type of L6 (i.e., challenge to estab-lish the reason for rejection) is only applicable to the responding agent. In case where the challenge is of type L7 (i.e.,challenge to establish the justification for a particular assertion), the mechanism will forward the reason behind thecorresponding assertion. Thus, this will return a single outcome H as justification.P8 Update Knowledge: A mechanism that allows an agent to update its knowledge with a certain fact. It will trigger asingle outcome knowledgeUpdate(l) where l represents the updated fact.P9 Consider Counter Argument: A mechanism that allows an agent to search within its knowledge to find a valid counterit will indicate this viaargument. This has two possible outcomes. First, if the mechanism finds a counter argument HhasCounterArg(H(cid:10)). Alternatively, if it doesn’t, it will indicate this via noCounterArg().(cid:10)P10 Terminate Challenge: A mechanism that allows an agent to terminate the current challenge. Once complete, it willgenerate a single possible outcome evaluationComplete() indicating this termination.P11 Terminate Interaction: A mechanism that allows the agent to terminate the interaction through exiting the dialogue.Here, the single outcome is exitDialogue(θ) where θ represents the corresponding action under negotiation.3.4.2. Decision mechanisms for the respondentThe corresponding respondent’s decision model has six basic decision mechanisms (R1∼R6). Collectively, they allow theagents to participate as a respondent within our ABN protocol and, thereby, resolve conflicts.R1 Consider Participation: A mechanism that allows the agent to consider whether to participate in the negotiation in-teraction. Here, we assume that all agents are willing to participate. Thus, this mechanism will lead a single outcomeenterDialogue(θ) where θ represents the corresponding action under negotiation.17R2 Evaluate Proposal: A mechanism that allows the respondent agent to evaluate a proposal forwarded by its counterpart.Similar to when generating a proposal, the respondent agent will need to consider two analogous rationality conditionsfor evaluating proposals; namely (i) the feasibility of the proposal and (ii) its viability. More specifically, (i) the respon-dent ar needs to have the capability to perform the requested action and (ii) the benefit of the suggested reward forthe responding agent (denoted as Bardo(ap ,θp )) should outweigh the cost of performing the requested action (denoted asC ardo(ar ,θr )). If both these conditions are satisfied the agent will accept the proposal, otherwise it will reject it. Thus, themechanism has two possible outcomes accept(S i(θ)) or reject(S i(θ)).R3 Extract Justification: A mechanism that allows the respondent agent to search within its own knowledge-base andextract the justification for a certain premise. This is similar to the P7 decision mechanism of the proponent. However,unlike the above, a respondent can receive both (L6 and L7) types of challenges. Thus, the justification would dependon the type of the challenge. More specifically, if the challenge is of type L6 (i.e., challenge to establish the reason forrejection) then the outcome would be the reason for rejecting that proposal. On the other hand, if the challenge is oftype L7 (i.e., challenge to establish the justification for a particular assertion), then the reason behind this assertionis forwarded as the justification. In both cases, the mechanism will return a single outcome H as the correspondingjustification.R4 Consider Premise: A mechanism that allows the agent to consider a particular premise with its current knowledge. Thishas two possible outcomes. If the agent believes it needs further justification to accept this premise (l) it will indicatethis via the needMoreJustification(l) outcome. Alternatively, if the agent chooses to accept this premise, it will update itsknowledge with this premise and will generate a knowledgeUpdate(l) outcome.R5 Consider Counter Argument: A mechanism that allows an agent to search within its knowledge to find a valid counterargument. This is similar to the proponent’s P9 decision mechanism and analogously has two possible outcomes. First,(cid:10)). Alternatively, if it doesn’t, itif the mechanism finds a counter argument Hwill indicate this via noCounterArg().it will indicate this via hasCounterArg(H(cid:10)R6 Terminate Interaction: A mechanism that allows the respondent to react to a dialogue termination initiated by theproponent. Similarly, here the single outcome is exitDialogue(θ) where θ represents the corresponding action undernegotiation.17 As explained in Section 3.4.1, all these decision mechanisms assume the agents are self-interested. Therefore, all the service providers aim to maximisetheir earnings. To this end, even if respondents are already committed to a particular action, they are always willing to listen to other proposals, sincethey have the ability to de-commit if they perceive a more profitable opportunity. Due to this reason, we assume that all responding agents are willing toparticipate in all dialogues.952N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Now, these individual decision functions (explained in Sections 3.4.1 and 3.4.2), the language element (see Section 3.3.1),and the rules of encounter specified by the protocol (refer to Section 3.3.2 and [36]) all combine together to allow ourframework to function as a coherent computing system, which allows agents carry out argumentative dialogues to resolveconflicts. We can formally specify the interaction of these three elements as operational semantics [19,54]. First introducedby van Eijk in [19], operational semantics have now become a widely accepted method of formalising complex dialoguesystems. In particular, the semantics specify the agents utterances and their individual decision-mechanisms as state transi-tion operators, and, thereby, precisely define the interaction between the proponent and the respondent as a state transitionsystem. We present this detailed semantics in Appendix B, and next use an illustrative dialogue to highlight how these dif-ferent elements combine together to allow our agents Ben (the supervisor) and Andy (the student) to form an argumentativedialogue within our supervisor-student example.3.5. Illustrative dialogueAs introduced in Section 1, here we consider a conflict between two agents; Andy (ar), an agent acting the role of a PhDstudent (r p), and Ben (ap), acting as his supervisor (rs). In this context, we assume that Andy has obligations to performtwo distinct actions, both toward Ben: (i) to write a conference paper (θc) and (ii) to write a journal paper (θ j). However,due to time restrictions, Andy can only do one of these actions and has decided to do θc at the expense of θ j . However, thischoice is in conflict with Ben’s own motivation to submit the journal paper in time for an important deadline.In this context, the sample dialogue presented in Table 1 illustrates a particular way Ben can argue, negotiate and,thereby, influence Andy to change his decision. In more detail, first, Table 1 presents the sample dialogue using naturallanguage. Here, Ben acts as the proponent of the dialogue and Andy as the respondent. This natural language representationhighlights how this dialogue systematically flows through each of the five main stages of our protocol. More specifically, itdemonstrates how the two participants open the dialogue, how their interaction allows them to recognise the presence of aconflict, how proponent Ben attempts to diagnose the underlying reason for the conflict, and how they manage it and reachan agreement by using an ABN dialogue.Table 1 also shows how agents can use the different locutions within our ABN framework to encode each of thesedialogue moves. In addition, it also presents the detailed transition steps (specified in Appendix B) taken by each individualagent to automatically generate the different locutions of this dialogue. These transitions combine the agent utterances(both by the proponent and respondent) and their individual decision mechanisms (highlighted in Section 3.4) and, thereby,specify how the ABN system operates to allow the autonomous agents to engage this bilateral dialogue. For instance, togenerate the first Open-Dialogue move M1, the proponent agent would use the transition TR2 (refer to Appendix B). Thisis specified as; [ap, P1, needService(θ)] L1−→[ar, R1, .]. This means that the proponent (Ben) would first use the P1: RecogniseNeed decision mechanism to consider if it requires the services of another (Andy) to achieve the action of writing the journalpaper. Once he realise he does indeed need the services of Andy, he would, in turn, initiate a dialogue with Andy throughthe L1: Open-Dialogue locution. When Andy receives this L1 locution, it will, in turn, initiate his R1: Consider Participationdecision mechanism. Thereafter the system would move to the TR3 transition where the respondent Andy considers hisparticipation and would respond back with a L2: Open-Dialogue locution confirming his willingness to participate in thedialogue. This appears as the next move M2 of the dialogue. In this manner, Table 1 presents the full sequence of transitions,which guides the agents through the series of decision mechanisms and utterances required to generate and progressthrough the sample dialogue within our ABN framework.Given the detailed theoretical definition of our ABN framework, next we map this theory into a computational argumen-tation context in order to empirically justify the performance benefits of our argumentation framework in resolving conflictsin agent societies.4. The experimental argumentation contextTo evaluate how agents can use our argumentation model to manage and resolve conflicts in a multi-agent society, werequire a computational context in which a number of agents interact in the presence of social influences and conflicts ariseas a natural consequence of these interactions. To this end, we now detail how we map our general ABN framework intoa specific multi-agent task allocation scenario.18 In particular, Section 4.1 gives an overview of the task environment of ourscenario followed by Section 4.2 that details its social context. Subsequently, in Section 4.3 we explain how conflicts arisewithin this context. Given this, finally, Section 4.4 details how agents can use our ABN model to interact and manage suchconflicts within it.18 The task/resource allocation problem is one of the most commonly found in distributed computing. For instance, many real world computing envi-ronments such as the grid [23], service-oriented systems [71], sensor networks [43], and supply chain management systems [68] all have this as one oftheir central issues. Thus, in choosing this scenario we aim to illustrate how ABN can be useful and versatile in handling such a fundamental issue. Here,we define the task allocation problem in its most basic form. In so doing, we abstract away any specific issue related to a particular context and, thereby,keep the scenario computationally simple for experimental analysis. We encourage future experimental effort within this domain to explore the value ofargumentation and how it can be usefully applied in different domains and conditions.Table 1A sample dialogue.Dialogue MoveNatural Language RepresentationM1: BenM2: AndyM3: BenM4: AndyM5: BenM6: AndyM7: BenM8: AndyM9: BenM10: AndyM11: BenM12: AndyM13: BenM14: AndyM15: BenOpen-DialogueOpen-DialogueI propose you write the journal paperNo, I can’t.Why not?I am scheduled to write a conference paper and itconflicts with writing the journal paper since Ican’t do two things at once.But, you have an obligation towards yoursupervisor to write this journal paper and I amyour supervisorI also have an obligation towards my supervisor towrite this conference paper.I propose that you write the journal paper andnot write the conference paperNo, I can’tWhy not?The obligation to write the conference paperinfluences me more than the journal paperI disagree. You have misunderstood. The journalpaper should influence you more than theconference paper and I am your supervisor.OK. The influence to write the journal paper ismore important than the conference paper.Now, I propose that you write the journal paperand not write the conference paper.M16: AndyI accept.M17: BenClose-DialogueM18: AndyClose-DialogueNotational RepresentationOpen-Dialogue(ap , ar )Open-Dialogue(ar , ap )Propose(ap , ar , do(ar , θ j ), ∅)Reject(ar , ap , do(ar , θ j ), ∅)Challenge(ap , ar , Reject(ar , ap , do(ar , θ j ), ∅))Assert(ar , ap , Conflict(do(ar , θt ), do(ar , θ j )))Assert(ap , ar , Oar ⇒rsθ j∧ Act(ap , rs))Transitions Leading to the LocutionTR2TR3TR4 → TR5 → TR7TR9TR12TR13TR19Assert(ar , ap , Oar ⇒rsθc)TR23 → TR26Propose(ap , ar , do(ar , θ j ) ∧ ¬do(ar , θc ), ∅)TR17 → TR24 → TR27 → TR29 → TR11 → TR7Reject(ap , ar , do(ar , θ j ) ∧ ¬do(ar , θc ), ∅)Challenge(ap , ar , Reject(ap , ar , do(ar , θ j )∧¬do(ar , θc ), ∅))Assert(ar , ap , InfluenceOf( f c, Oar ⇒rsInfluenceOf( f j , Oar ⇒rsθc)∧) ∧ f c > f j )θ jAssert(ap , ar , ¬( f c > f j ) ∧ ( f j > f c)∧Act(ap , rs)TR9TR12TR13TR18Assert(ar , ap , ( f j > f c ))TR23 → TR28Propose(ap , ar , do(ar , θ j ) ∧ ¬do(ar , θi ), ∅)TR29 → TR11 → TR7Accept(ar , ap , do(ar , θ j ) ∧ ¬do(ar , θi ), ∅)Close-Dialogue(ap, ar )Close-Dialogue(ar , ap )TR8TR30TR31StagesOpeningConflictRecognitionConflictDiagnosisConflictManagementAgreementClosingNC..Karunatillakeetal./ArtificialIntelligence173(2009)935–981953954N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Table 2A sample multi-agent task scenario.Timet0t1t2t3a0c(0,0.8), c(1,0.1)θ0 : [t0, c(0,0.5), 200]θ1 : [t1, c(1,0.3), 900]θ2 : [t2, c(1,0.1), 400]θ3 : [t3, c(0,0.9), 600]a1c(0,0.1), c(1,0.7)θ0 : [t0, c(1,0.2), 500]θ1 : [t1, c(0,0.4), 300]θ2 : [t2, c(0,0.8), 900]a2c(0,0.4), c(1,0.5)θ0 : [t0, c(1,0.5), 700]θ1 : [t1, c(1,0.7), 100]4.1. The task environmentThe task environment consists of two main elements. On one hand, each agent in the system has a list of actions thatit is required to achieve. On the other hand, all agents in the system have different capabilities to perform these actions. Inthis context, agents are allowed to interact and negotiate between one another to find capable counterparts that are willingto sell their services to perform their actions. The following specifies these main elements in more detail:Capability: All agents within the domain have an array of capabilities. Each such capability has two parameters: (i) a typevalue (x) defining the type of that capability and (ii) a capability level (d ∈ [0, 1]) defining the agent’s competence level inthat capability (1 indicates total competence, 0 no competence). Given this, we denote a capability as c(x,d) : [x, d].Action: Each action has four main parameters: (i) the specified time (ti) that the action needs to be performed where i ∈ N,(cid:10)) required to successfully complete(ii) the capability type (x) required to perform it, (iii) the minimum capability level (dthe action, and (iv) the reward (r; distributed normally19 with a mean μ and a standard deviation σ ) that the agent wouldgain if the action is completed. Given this, we denote an action as θi : [ti, c(x,d(cid:10)), r].Each agent within the context is seeded with a specified number of such actions. This number varies randomly betweenagents within a pre-specified range. Table 2 depicts one such sample scenario for a three agent context (a0, a1, and a2) withtheir respective capabilities and actions. For instance, agent a0 has two capability types; c0 with a competence level of 0.8and c1 with a level of 0.1. It also has four actions; θ0, θ1, θ2, θ3; each with their respective capability types, minimum levels,and rewards.In this scenario, the main objective of the agents is to maximise their individual earnings. There are two methods ofdoing so. First, they can find willing and capable counterparts to complete their assigned actions. Once an agent managesto complete a certain action, it will receive the reward associated with that action less any service payments made toacquire the services of its counterpart. This we term the agent’s task earnings. Second, agents can sell their services to otheragents and gain a payment. This we term the agent’s service earnings. Both these components contribute toward the overallindividual earnings of the agent. However, since agents pay for the services of one another, for each service payment anagent makes there would be another corresponding agent obtaining a service earning. Thus, when considering the wholeagent population the service earnings and service payments will cancel each other out and the total population earnings ofthe society will account for the cumulative reward values of the actions achieved by all agents within the society.One important characteristic within this domain is the agents’ ability to renege on agreements after paying a sufficientde-commitment charge. In more detail, since we assume that agents can only perform a single action at any one time, if acertain agent (in the above example a1 in Table 2) agrees to provide its services to a specific agent (a2) for a particular timeslot (t1), a1 will not be able to agree to perform any other action at t1, unless it cancels its current agreement with a2. Forexample, if a0 requests a1 to perform its action, which requires capability c1 at t1, it cannot do so unless it reneges on itscurrent contract with a2. In this context, we allow agents to renege upon their agreements if they perceive a more profitableopportunity. This ability to renege is important because it promotes opportunities for the agents that seek services laterin the scheduling process to achieve agreements if they are willing to pay sufficiently high premiums for these services.Therefore, a1 has the potential to pay a certain compensation value to a2 and de-commit itself out of its current agreementand render its services to another agent (for instance a0), if it receives a more profitable offer from the latter (a0). Here, weuse a simple heuristic to calculate this compensation value. In particular, it is evaluated as the original agreed price plus afixed percentage (10%) of that price as de-commitment penalty (for more details refer to [34]).2019 Here, we use a normal distribution since it gives a more realistic representation of the type of tasks found in many real world applications (i.e., highnumber of medium rewarding tasks and a low number of very high and very low rewarding tasks). However, we do not believe this choice of distributionis not critical to this work.20 Here, any amount lost or gained due to de-commitment penalties are deemed to be embodied within the rewards and the service earning values andsuch payments cancel out one another when we consider the whole society.r001r110r201r0r10r2(a) Rol-Rel mapping.10N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981955r0r1r2r0[0:0]r1[200:0]r2[0:0][400:100][0:0][200:600][0:0][700:200][0:0](b) Social commitment mapping.Fig. 5. Social influence model.r0100r1011r2010a0a1a2(c) Ag-Rol mapping.4.2. The social contextGiven the task environment of our argumentation scenario, we now describe its social context. In essence, here weembody a rich social structure into our multi-agent system. In particular, this structure encapsulates a set of roles intercon-nected via a series of relationships. When agents assume these roles, they will automatically be part of these relationshipswith other agents within the society. This social structure will, in turn, exert social influences upon the agents when theyinteract within the society. The following explains how to model these in more detail.As the first step in mapping this social context into our computational context, we define a specific number of rolesand randomly link them to create a web of relationships. This defines the role-relationship structure. In our experimentswe represent this via a single matrix. Fig. 5(a) shows an example of such a representation between 3 roles: r1, r2, and r3,where 1 indicates that a relationship exists between the two related roles, and 0 indicates no relationship. For instance,consider the three values 0, 1, 0 in first row in the Table 5(a). Since a relationship requires the interlink of two differentroles, the first zero indicates the absence of relationship between the same role r0. Thus, the diagonal of this matrix willalways be zeros. On the other hand, the second value 1 indicates presence of a relationship between the roles ro and r1while the third value 0 indicates that a relationship does not exist between the roles ro and r2. Since a relationship betweenro and r1 essentially means that there exists a relationship between r1 and r0, this matrix will always be symmetrical. Forexample, when we say a relationship exists between the student and supervisor roles, the same relationship also existsbetween supervisor and student.Given this role-relationship structure, we now randomly specify social commitments for each of the active relationshipedges (those that are defined as 1 in the mapping). As per Section 3.1, a social commitment in our context is a commitmentby one role, to another, to provide a certain type of capability when requested. An important component of our notion ofsocial commitment is that not all of them influence the agents in a similar manner and they each have their associateddegree of influence (refer to Section 3.1). Here, we map these different degrees of influence by associating each socialcommitment with a de-commitment penalty. Thus, any agent may violate a certain social commitment at any given time.However, it will be liable to pay the specified de-commitment value for this violation (this is similar to the notion oflevelled commitments introduced by Sandholm and Lesser [67]). Since all our agents are self-interested, they prefer not tolose rewards in the form of penalties, so a higher de-commitment penalty yields a stronger social commitment (thereby,reflecting a higher social influence). Given this, Fig. 5(b) represents such a mapping corresponding to the social structurerepresented in Fig. 5(a). For instance, consider the relationship that exist between roles r0 and r1 (due to the 1 in row1 column 2 in Fig. 5(a), or row 2 column 1 due to its symmetrical nature). Now, as a result, we can randomly generatede-commitment values for each capability type in Fig. 5(b). Note that, the columns in Fig. 5(b) represent the debtor rolesand the rows the creditor roles. Thus, the entry [400:100] in row 2, column 1 indicates that the debtor role r0 is committedto provide capabilities c0 and c1 to a holder of the creditor role r1. If the agent holding the role r0 chooses not to honourthese commitments it will have to pay 400 and 100 (respectively for c0 and c1) if asked. On the other hand, the entry[200:0] in row 1, column 2 indicates that the debtor role r1 is committed to provide capabilities c0 and c1 to a holder ofthe creditor role r0 denoting the different social commitments indebted by the role r1 towards the role r0. This is because,for example, the social commitments from the role student towards supervisor will be different to those from supervisorto student. Therefore, the social commitment matrix is not symmetric allowing us to capture the non-symmetric natureof social commitment between the opposite directions within a given relationship. Finally, if a relationship does not existbetween any two roles (i.e., between roles r0 and r2; note the 0 in row 1 column 3 in Fig. 5(a)) social commitments wouldnot exist between such roles. So these would have zero values in their corresponding entries in Fig. 5(b) (i.e., note the [0:0]in row 1 column 3 in Fig. 5(b)).Having designed this social structure and the associated social commitments, finally we assign these roles to the actualagents operating within our system as shown in Fig. 5(c). For instance, the 1, 0, 0 in the first row in Fig. 5(c) indicates thatthe agent a0 assumes the role r0, but does not assume r1 and r2. The next row 0, 1, 1 indicates that the next agent a1assumes the roles r1 and r2, but not r0.From these three representations, we can easily extract the rights and the obligations of each individual agent within oursystem. For instance, the agent-role mapping (see Fig. 5(c)) shows that agent a0 acts the role r0. Given this, a0’s obligationsand rights can be extracted by following the column and row corresponding that role in Fig. 5(b). In more detail, byfollowing the column 1 corresponding to r0 in Fig. 5(b) (i.e., [0:0], [400:100], [0:0]) we can extract the obligations of therole and by following row 1 in Fig. 5(b) (i.e., [0:0], [200:0], [0:0]) we can extract its rights. Since agent a0 assumes this roler0, the agent will obtain these obligations and rights as its own. If an agent assumes more than one role (such as agenta1 that assumes roles r1 and r2) it will obtain the obligations and rights of all its roles. As an example, the following listsobligations and rights of the agent a0:956N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981• Obligations:– to provide c0 to an agent acting r1; obliged to pay 400 if de-committed.– to provide c1 to an agent acting r1; obliged to pay 100 if de-committed.• Rights:– to demand c0 from an agent acting r1 or to demand 200 if de-committed.Given this global representation of social influence, we will now detail how we seed individual agents with this infor-mation. Since one of the aims in our experiments is to test how agents use argumentation to manage and resolve conflictscreated due to incomplete knowledge about their social influences, we generate a number of settings by varying the level ofknowledge seeded to the agents. More specifically, we give only a subset of the agent-role mapping to each agent. We do soby randomly replacing certain 1s with 0s (in the Matrix 5(c)) and give this partial knowledge to the agents during initiali-sation. Thus, a certain agent may not know all the roles that it or another agent may act. This may, in turn, lead to conflictswithin the society, since certain agents may know certain facts about the society that others are unaware of (see Section 4.3for more details). By controlling this level of change, we generate an array of settings ranging from perfect knowledge (0%missing knowledge) in the society, to the case where agents are completely unaware of their social influences (100% missingknowledge).21Given an overview of the scenario, we now explain how these agent interactions lead to conflicts within this multi-agentcontext.4.3. Computational conflictsAs argued in Section 1, usually within a multi-agent society, we can identify two broad forms of computational conflicts.Namely, the conflicts of interests that may arise due to the disparate motivations of the individual agents and the conflictsof opinions that may occur due to imperfections of information distributed within the context. We can identify both theseforms of conflicts within the above scenario. The following explains these in more detail.First, the self-interested motivations of our agents give rise to conflicts of interests within the system. In more detail, whenan agent attempts to acquire the services of another, it is motivated to pay the lowest amount it possibly can for that service.This is because the lower an agent’s external service payments are, the higher its own task earnings will be. However, on theother hand, when agents sell their services, they are motivated to obtain the highest payment they possibly can to maximisetheir service earnings (refer to Section 4.1). Thus, whenever agents attempt to convince others to sell their services, theinteraction naturally gives rise to conflicts of interest (due to the discrepancy in motivations to pay the minimum whenselling and earn the maximum when buying) between the buyer and seller agents in the system.The dynamics of interaction become more complicated due to the presence of social influences within the society. Forinstance, an agent may be internally motivated (due to its self-interested desire to maximise its earnings) to perform aspecific action. However, at the same time, it may also be subject to an external social influence (via the role it is assumingor the relationship that it is part of) not to perform it. In such a case, the agent is required to make a choice between itsinternal desire and its obligation. If, for instance, the agent decides to pursue its internal motivation at the expense of itssocial influence, this may, in turn, lead to a conflict of interest between it and another of its counterparts who may havean interest in the former abiding by its social influence. Also an agent may face situations where different social influencesmotivate it in a contradictory manner (one to perform a specific action and the other not to). In such situations, the agentis again required to make a choice between which obligation to honour and which to violate. In such an event, if the agentdecides to abide by a certain social influence and forgo the other, this may also lead to conflicts of interest between agents.Second, within a multi-agent society, the information is usually distributed between the individual agents. Thus, a certainindividual may only possess a partial view about the facts of the society. In particular, when agents interact to achieve theirtasks in the above context, they do so with imperfect knowledge about their social influences (refer to Section 4.2). Thus,agents may not be aware of the existence of all the social influences that could or indeed should affect their and theircounterparts’ actions. Due to this lack of knowledge, agents may fail to abide by all their social influences, which, in turn,may lead to conflicts. Since the underlying reason for these forms of conflicts are imperfections in view points betweenagents, these are termed conflicts of opinions [73].For instance, in the above context, a particular agent may not be aware of all the roles that it or another of its counterpartmay act within the society. This may, in turn, lead to conflicts since certain agents may know certain facts about the societythat others are unaware of. To explain this further, consider an instance where agent a0 is not aware that it is acting acertain role r0, which may prescribe it to honour a certain obligation to another agent a1 acting the role r1. Now, when21 Theoretically, it is possible to introduce imperfections to all aspects of the agents’ knowledge (i.e., the task parameters, the capability parameters,and the counterparts known within the society). However, since the objective of these experiments is to explore the concept of how arguments canresolve conflicts, instead of designing an exhaustive implementation with all possible imperfections and arguments, we chose to concentrate on resolvingconflicts that arise due to imperfect knowledge about their social influences. In particular, we concentrate on the imperfections that arise due to the lackof knowledge about the first two premises in the schema Act(ai , ri ) and Act(a j , r j ) (refer to Section 3.1). Thus, conflicts may arise due to the agents’ lack ofknowledge about the role they and their counterparts assume within the society. Increasing the imperfections would most likely increase the reasons whya conflict may occur, thus, bringing more arguments into play. Therefore, we believe, this would have little bearing on the general pattern of the results.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9819571: [p0, p1, . . . , pmax] ← generateProposals()2: p ← p03: isAccepted ← false4:5: {Loop till either the agent agrees or the last proposal fails.}6: while (isAccepted (cid:15)= true (cid:16) p (cid:2) pmax) do7:response ← Propose(p)if (response = “accept”) then8:9:10:11:12:13:isAccepted ← trueelseif (p (cid:15)= pmax) thenp ← getNextViableProposal()end ifend if14:15: end while16: return isAcceptedAlgorithm 3. The negotiate() method.these agents interact within the society, a0 may refuse to honour its obligation to a1 (of which it is unaware) and mayrefuse to pay any penalty for this violation. Thus, such imperfect information may manifest itself as a conflict between thetwo agents. Similarly, in an instance where a0 is aware of its role r0, but is unaware that its counterpart a1 acts role r1, itmay also refuse to honour this obligation. In this instance, the agent’s lack of knowledge about the roles of its counterpartleads to a conflict within the society.Given how different types of conflicts arise within the context, we will now detail a number of different ways agentscan use our ABN framework to manage and resolve them through argumentation. As the first step to this end, we will nextdetail the basic algorithms that agents can use to argue and negotiate in this system.4.4. Agent interactionFirst, we present the negotiation element of the basic ABN algorithm that allows agents to negotiate the services of otherwilling and capable counterparts within this social setting (refer to Algorithm 3). In essence, an agent that requires a certaincapability will generate and forward proposals to another selected agent within the community, requesting that agent to sellits services in exchange for a certain reward. If the receiving agent perceives this proposal to be viable and believes that itis capable of performing the proposal, then the agent will accept. Otherwise it will reject the proposal. In case of a reject,the original proposing agent will attempt to forward a modified proposal. This is done through the getNextViableProposal()method, which essentially implements the P4 decision mechanism explained in Section 3.4.1. The interaction will end eitherwhen one of the proposals is accepted or when all valid proposals that the proposing agent can forward are rejected. If theproposing agent could not reach an agreement with that particular responding agent, then it will choose another potentialservice provider and will initiate negotiations with that agent. In essence, this is a simplified version of the protocol specifiedin Section 3.3.2. Here, the two main decision elements within this negotiation are generating and evaluating proposals. Inthe following we will discuss how our ABN model presented in Section 3.4 is used to design these two decision elements:22Proposal Generation: When generating a proposal, an agent needs to consider two aspects: (i) whether it is capable of car-rying out the reward and (ii) whether the benefit it gains from the request is greater than the cost incurred while performing thereward (refer to Algorithm 1 in Section 3.4.1). To simplify the implementation, we constrain our system to produce proposalswith only monetary rewards. Given this, by slight abuse of notation, we will use m to represent the action “pay monetaryamount m”. Thus, the generic proposal from an agent ai to an agent a j takes the form Propose(ai, a j, do(a j, θ j), do(ai, m))where θ j is the requested action and m the monetary reward. In this context, calculating the benefit and the cost becomesstraight forward. The benefit is the request u j associated with the action θ j and the cost of reward is m the monetaryreward. Using this, the agent can generate an array of proposals with increasing amounts of monetary rewards, the lowestbeing 1 and the highest being (u j − 1).Proposal Evaluation: When the receiving agent evaluates a proposal it also considers two analogous factors: (i) whetherit is capable of performing the request and (ii) if the benefit it gains from the reward is greater than the cost of carrying outthe request (refer to Algorithm 2). To evaluate capability, the agent compares its own level with the minimum required toperform the action. In case of viability, the cost of performing the request is the current opportunity cost. Here, if the agentsare not occupied, the cost is the minimum asking price (set to μ the mean reward value, see Section 4.1), or, if they are, it isthe reward plus the de-commitment cost of the previously agreed action. The benefit, in the simplest case, is the monetary22 It is important to note that this implementation represents but one instantiation of how agents can interact within our framework. We analyse anumber of different variations in Section 5.958N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981{Assert invalid premises of Hr }1: {Challenge for the respondent’s justification}2: Hr ← challengeJustification()3: {Generate personal justification}4: H p ← generateJustification()5:6: if (isValid(Hr ) = false) then7:8: else9:10: end if11: if (isValid(H p) = false) then12:13: else14:15: end if{Assert H p }{Adopt premises of Hr into personal knowledge}{Correct invalid premises of H p within personal knowledge}Algorithm 4. The argue() method.value of the reward m. However, if the agent has a social commitment to provide that capability type to the requestingagent, then the benefit is the monetary reward plus the de-commitment penalty of this social commitment.Given the negotiation interaction, we will now detail how agents argue to resolve conflicts that may arise due to theknowledge imperfections present within their multi-agent society (such as the one highlighted in Section 4.2). In order toresolve such a conflict, agents must first be able to detect it. In this context, they do so by analysing the de-commitmentpenalties paid by their counterparts for violating their social commitments. Specifically, an agent with the right to demanda certain capability would claim the penalty from its counterpart if it believes that the latter has violated its obligation. Toreduce the complexity, here, we assume that agents do not attempt to deceive one another.23 Thus, an agent will eitherhonour its obligation or pay the penalty. However, due to agents having imperfect knowledge about their context (seeSection 4.2), in certain instances a counterpart may not be fully aware of all its obligations and may pay a penalty chargedifferent to what it should have paid. For instance, in the example scenario presented in Fig. 5, since agent a0 acts the roler0 and agent a1 acts the role r1, a0 has the obligation to provide capability c1 to a1 or pay 100 for violating that obligation.However, if agent a0 is unaware that its counterpart a1 is acting r1, it will not pay any penalty charge for refusing toprovide c1. In such an instance, since the actual amount paid (0) in response is different from the amount it expects toreceive (100), the agents would detect the existence of a conflict.Once such a conflict is detected, agents attempt to argue and resolve it by exchanging their respective justifications (referto Algorithm 4). As the first step, the proponent would challenge its respondent’s justification (via the challengeJustification()method) for paying the de-commitment penalty value that the respondent believes it is obligated to pay. These justificationstake the form of the social influence schema (see formulae (5) and (6) in Section 3.1). For instance, an agent may say thatit paid a certain penalty value px because it believes it is acting the role ri and its counterpart acts the role r j , and due tothe relationship between ri and r j it believes that it entails an obligation O x which demands a payment of px in the eventof its violation. Similarly, an agent may say it paid a zero amount as its penalty because it couldn’t find any justification asto why it should pay a certain penalty. Once the proponent receives its counterpart’s justification, it can generate its ownjustification (via the generateJustification() method) as to why the counterpart should pay the penalty value it believes it hasthe right to demand.By analysing these two justifications, agents may uncover certain inconsistencies between the different premises withinthese justifications. As highlighted in Algorithm 4 there can four possible cases. First, the proponent may find that one of thereasons given as support by its respondent may be invalid. In such an event, agents can use the social arguments highlightedin Section 3.2.1 (i.e., 1.i, 1.ii, 1.iii, etc.) to argue about these justifications by disputing those premises which they deeminvalid (see line 7 in Algorithm 4). Second, after close examination (or after further questioning), the proponent may findone his own reasons to be invalid. In such an instance, the agent can correct these invalid premises within its own personalknowledge (see line 12 in Algorithm 4). Even if both the justifications are valid, they can still be inconsistent due to theincomplete knowledge between the two agents. For example, an agent may have paid a certain penalty because it believesthat its counterpart acts a certain role (which in fact is correct). However, the agent may be missing the knowledge that thecounterpart also acts in another role which give its counterpart the right to demand a higher penalty charge. Such missingknowledge can be in both the proponent and the respondent, which gives rise to the final two cases. In such instances,agents can use the social arguments highlighted in Section 3.2.1 (i.e., 2.i, 2.ii, 2.iii, etc.) to assert such missing knowledge bypointing out these alternative justifications and thereby overcoming such imperfections within their knowledge (see lines 9and 14 in Algorithm 4).One important functionality required to achieve these arguments is the ability to determine the validity of thesepremises. This is generally referred to as the defeat-status computation and is an extensively researched area within23 This is an assumption used right through the course of this paper as intentional deception and lying are beyond the scope of this study.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981959argumentation literature (refer to Section 6). The models proposed include arbitration [72], defeasible models [3,18], self-stabilising models [5], and different forms of heuristics [7,40,60]. However, here we do not attempt to re-invent a newdefeat-status computation model. Since we are mainly interested in the systemic impact of ABN in an agent society, in ourimplementation, we abstract away this functionality by using a validation heuristic which simulates a defeasible model suchas [3]. More specifically, the validation heuristic considers a given basic premise and returns true or false depending onits validity, thereby, simulating a defeasible model or an arbitration model. In our experiments, we also vary the accuracylevel of this heuristic and experiment with the effect of having inaccuracies and failures of this defeat-status computationmechanism on the argumentation process (refer to Section 5.2.1).Having successfully mapped our ABN framework to a computation context, next we present a series of ABN strategies andempirically analyse how they would allow agents to both effectively and efficiently resolve conflicts within a multi-agentsociety.5. Empirical evaluationGiven the experimental context, we now present a detailed empirical evaluation on how agents can use our ABNframework (proposed in Section 3) to argue and negotiate efficiently and effectively in a multi-agent society. To this end,Section 5.1 first specifies our experimental settings. Thereafter, we present a series of strategies that agents can use to argueeffectively to resolve conflicts within a social context. For each strategy, we specify detailed algorithms and empirically eval-uate their relative performance benefits to the agent society. In so doing, we empirically identify a set of general conditionsand guidelines on when and how argumentation can enhance the performance of a multi-agent society.5.1. The experimental settingThe experiments are set within an argumentation context with 30 agents, each interacting with one another to negotiatewilling and capable counterparts to achieve their actions (as specified in Section 4). In this task environment, each agent isassigned a number of actions that vary randomly between 20 and 30. Each action is associated with a reward that is setaccording to a normal distribution with a mean 1000 and a standard deviation of 500. In addition, each agent is assignedall three types of capabilities, but their level of competence for each type varies randomly between 0 and 1.To enable us to analyse how agents can use ABN to resolve conflicts within this society, we incorporate a rich socialstructure into our experimental context. In particular, we embody an array of roles, relationships, and social commitmentsinto the agent society. In more detail, first we assign a set of roles to each agent within the context. In order to avoid apredisposition towards any specific specialised form of a social context we assign the roles to agents in a random manner.The maximum number of roles within the society varies between different experiments. These roles are then connected viarelationships which, in turn, contain a series of social commitments associated with them as described in Section 4.2. Thesesocial commitments entail agents with rights to demand, question, and require other agents to perform particular actionsand obligations to do so when requested.In our experiments, we do not assume that agents have perfect knowledge about the social structure within which theyoperate. Therefore, having mapped this social structure, we then vary the level of knowledge about this social structureseeded into our agents. Thereby, we create an array of experimental settings where agents have different levels of imperfec-tions in their knowledge about the structure and its influences. This level of imperfection varies between 0 to 100, where0 indicates perfect knowledge and 100 represents a complete lack of knowledge. Such imperfections, in turn, dictate thenumber of conflicts of opinion present within the society; the greater the lack of knowledge about the society, the greaterthe number of potential conflicts between the agents.Given both the task environment and the social context, we now explain the two metrics used to evaluate the overallperformance of the different ABN strategies in our experiments:24• Effectiveness of the Strategy: We use the total earnings of the population as a measure of effectiveness of ABN strategies.If this value is higher, the strategy has been more effective in handling the conflicts. Therefore, it has allowed agents tofind willing and capable counterparts to perform their actions more effectively within the society. On the other hand, ifthe value is lower, the strategy presents a less effective means of resolving conflicts.• Efficiency of the Strategy: This reflects the computational cost incurred by the agents while using a particular strategyto resolve conflicts within the society. We use the total number of messages exchanged between all agents within thesociety during the interaction as a metric to measure this effect. This provides a good metric because longer interactions,which usually takes a higher number of messages to complete, tend to consume more resources from the agents togenerate, select, and evaluate such messages and also generally consume increased bandwidth within the system. Onthe other hand, shorter interactions, which tend to consume fewer resources, only incur a smaller number of messages.Thus, the number of messages exchanged has a strong correlation to the amount of resources used within the system.More specifically, a strategy that involves fewer messages is said to have performed more efficiently in resolving conflictsthan one that uses a higher number.24 These metrics are not novel to our work, both Jung et al. [31] and Ramchurn et al. [60] used similar measures in their empirical work.960N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Table 3Summary of the simulation parameters.Simulation ParameterValueNumber of agents within the societyNumber of capability typesLevel of capabilityNumber of actions per agentReward value per action (ui )303d ∈ [0, 1]20∼30ui ∼ N(μ, σ 2); μ = £1000; σ = £500Given our experimental settings, we now proceed to detail the different ABN strategies and empirically evaluate theirability to resolve conflicts within a multi-agent society. All reported results are averaged over 30 simulation runs to diminishthe impact of random noise, and all observations emphasised are statistically significant at the 95% confidence level.25 Ineach simulation run, all agents are allowed to iterate through all their actions, trying to negotiate (either successfully orunsuccessfully) the services of others to accomplish those actions.5.2. Strategies, results and observationsHaving described our experimental settings, in the following we analyse a series of ABN strategies that agents may useto argue and resolve conflicts within such a multi-agent context. In designing these different strategies, we draw inspirationfrom our social influence schema and demonstrate a number of different ways that agents can argue to resolve conflictsin a social context. We in turn measure the relative performance benefits (both in terms of efficiency and effectiveness)of using these strategies to derive guidelines on how argumentation can be constructively used within a multi-agent soci-ety.In particular, we analyse three major ways that agents can argue and negotiate to resolve conflicts within our experi-mental multi-agent society. The first and the second methods focus on how agents can socially influence each others’ decisionsby arguing about their social influences and, thereby, effectively and efficiently overcoming conflicts of opinions presentwithin an agent society. The motivation for these two methods stems from our social influence schema (see Section 3.1),which gives the agents different rights in the event where an obligation is violated; namely the right to demand com-pensation (see Section 5.2.1) and the right to challenge non-performance (see Section 5.2.2) of social commitments. Third,we shift our focus to how agents can negotiate their social influences (see Section 5.2.3) and, thereby, attempt to negoti-ate and resolve certain conflicts by way of trading and re-allocating social influences within our experimental multi-agentcontext.In each case, these strategies help us to investigate a number of important hypotheses related to the use of argumenta-tion in a multi-agent society. In the following three sections (Sections 5.2.1, 5.2.2, and 5.2.3) we explain these strategies indetail, highlight the respective hypotheses under investigation, present our experimental results, and analyse the observa-tions.5.2.1. Demanding compensationIf an agent violates a certain social commitment, one of the ways its counterpart can react is by exercising its right todemand compensation. This formulates our baseline strategy. In particular, it extends our negotiation algorithm by allowingthe agents to demand compensation in cases where negotiation fails. Once requested, the agent that violated its social com-mitment will pay its counterpart the related penalty (refer to Algorithm 5). We term this strategy Claim_Penalty_Non_Argue(CPNA). However, in imperfect information settings, a particular agent may violate a social commitment simply because itwas not aware of it (i.e., due to the lack of knowledge of its roles or those of its counterparts, as explained in Section 5.1).In such situations, an agent may pay a de-commitment penalty different to what the other agent believes it should get,which may, in turn, lead to a conflict. In such situations, our second strategy, titled Claim_Penalty_Argue (CPA), allows agentsto use social arguments to argue about their social influences (as per Section 3.2.1) and, thereby, manage their conflicts.Algorithms 5 and 6 define the overall behaviour of both these strategies.Here, our hypothesis is that by allowing agents to argue about their social influences we are providing them with acoherent mechanism to manage and resolve their conflicts and, thereby, allowing them to gain a better outcome as asociety. To this end, the former strategy, CPNA, acts as our control strategy and the latter, CPA, as the test strategy. Figs. 6(a)and 6(b) show our main results from which we make the following observations:√n ). Here, the parameter t increases or decreases the error element (t ∗ (s/25 The statistical significance tests are commonly used in sampling theory to approximately predict the population mean (μ), within a certain error range,using a known sample mean (x) and sample variance (s2). For instance, for a sample size of n, the population mean is stated to range between the limitsμ = x ± t ∗ (s/n )), which, in turn, is said to determine the level of confidencein this approximation. For small samples, this t parameter follows the Student’s t distribution, which, in turn, specifies the certain t value to be used inorder to attain approximations at different levels of confidence. For instance, to attain a 95% confidence level for both upper and lower limits (termedas two-tail) in a population size of 30, it specifies a t value of 2.042. Against this background, all our graphs and results use this notion to calculate thestandard statistical error in the results (for more detail refer to [13]).√N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9819611: isAccepted ← negotiate()2: if (isAccepted = false) then3:4: end ifcompensation ← demandCompensation()Algorithm 5. Claim_Penalty_Non_Argue (CPNA) strategy.1: isAccepted ← negotiate()2: if (isAccepted = false) then3:compensation ← demandCompensation()if (compensation < rightToPenalty) then4:5:argue()end if6:7: end ifAlgorithm 6. Claim_Penalty_Argue (CPA) strategy.Fig. 6. Efficiency and effectiveness of the argue and non-argue strategies.Observation 1. The argumentation strategy allows agents to manage conflicts related to their social influences even at high uncertaintylevels.Fig. 6(a) shows a downward trend in the population earnings as the agents’ knowledge level about their social influencesdecrease (0 on the X-axis indicates perfect information, whereas 100 represents a complete lack of knowledge). This trendis present in both the CPNA and CPA strategies. In essence, the reason for this trend is the agents’ awareness of theirsocial influences. Specifically, if agents are aware of their social influences, they may use these as parameters within theirnegotiations. Thereby, in certain instances, they can use these social influences to endorse their actions which may otherwiseget rejected (see Section 3.2.2). Thus, if agents are aware of their social influences it would, in turn, increase their populationearnings as more actions are accomplished. On the other hand, if the agents are unaware of their social influences, theymay not be able to use these to endorse such actions. Thus, this downward trend depicts this social phenomenon withinour results.In Fig. 6(a), we can also observe that the population earnings when using the non-argue strategy (CPNA) decreases morerapidly than the argue one (CPA). The reason for this is because the argue method within CPA allows agents to manageand resolve certain conflicts of opinion that they may have about their social influences. For instance, if a certain agent isunaware of a role that another acts, it may correct this missing knowledge through arguing with that agent as explained inSection 5.1. Thus, arguing allows agents to correct such gaps in their knowledge and, thereby, resolve any conflicts that mayarise as a result.We can observe this even more clearly in Figs. 7(a) and 7(b), which plot the percentage of information known to theagents during the course of their interactions. For instance, Fig. 7(a) shows how agents start their interaction with only 60%of knowledge (40% missing) about their social influences and, when using the CPA strategy, argue between one anotherand become increasingly aware of their social influences during the course of their interaction (reaching approximately 90%by end of the simulation). On the other hand, since the non-arguing CPNA strategy leaves such conflicts unresolved, thisknowledge remains missing right through the course of the interaction (the 40% missing knowledge remains constant in962N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Fig. 7. Information flow between argue and non-argue strategies with 30 agents and 3 roles.Fig. 7(a)). In this manner, ABN allows the agents to manage their conflicts, become more aware about their social influences,and function more effectively as a society even at high uncertainty levels (e.g., 40% to 80% as seen in Fig. 6(a)).Observation 2. At all knowledge levels, the argumentation strategy exchanges fewer messages than the non-arguing one.Fig. 6(b) shows the number of messages used by both strategies under all knowledge levels. Apart from the two endpoints, where argumentation does not occur (see Observation 3), we can clearly see the non-arguing strategy exchangingmore messages (therefore, performing less efficiently) than the argue one. The reason for this is that even though agentsdo use some number of messages to argue and correct their incomplete knowledge, thereafter they use their correctedknowledge in subsequent interactions. However, if the agents do not argue to correct their knowledge imperfections, theynegotiate more frequently since they cannot use their social influences to endorse their actions. Thus, this one-off increaseof argue messages becomes insignificant when compared to the increase in the propose, accept, and reject messages due tothe increased number of negotiations. For instance, at 50% level of missing knowledge, when agents interact using the CPNAstrategy (which does not allow them to argue) they use on average 335,424 messages for negotiation. However, when usingthe CPA strategy (which allows them to argue) in the same settings, they use on average only 294,322 messages; a 12.5%reduction in negotiation messages in exchange for a 0.2% increase in argumentation messages (see Fig. 8).When taken together, these two observations give support to the hypothesis that allowing agents to argue about theirsocial influences does indeed provide agents a coherent mechanism to resolve conflicts, and thereby, gain a better (moreeffective and efficient) outcome as a society. Given this, we now attempt to qualify this claim by investigating how thisvalue of social argumentation varies under three different conditions. First, we explore two extreme conditions; (i) when thesociety has perfect information and (ii) when there is complete uncertainty about the social context (see Observation 3).Second, we investigate this value of arguing about social influences, when the number of social influences available withinthe society varies (from sparse to abundant; see Observation 4). Third, we experiment with what happens if the agents’arguing mechanism fails to deliver a precise outcome in each and every occasion. In so doing, we explore how such fail-ures in the argumentation mechanism impact the effectiveness of the agent society to perform as a coherent unit (seeObservation 5).Observation 3. In cases of perfect information and complete uncertainty, both strategies perform equally.The reason for both strategies performing equally when there is perfect information (refer to 0% in Fig. 6(a)) is becausethere are no knowledge imperfections. Therefore, in such situations, agents do not need to engage in argumentation tocorrect conflicts of opinions simply because such conflicts do not exist.On the other hand, the reason for both strategies performing equally when there is a complete lack of knowledge ismore interesting (refer to 100% level in Fig. 6(a)). Here, since all the agents within the society are unaware of any socialinfluences (even though they exist), they are not able to detect any conflicts or violations. Consequently, agents do not resortto arguing to manage such conflicts (agents must first recognise a conflict before they can argue and manage it; refer tothe protocol specification in Section 3.3.2). Thus, when there is a complete lack of knowledge, the CPA strategy that allowsarguing performs identically to the non-arguing CPNA one.Observation 4. When there are more social influences within the system, the performance benefit of arguing is only significant at highlevels of knowledge incompleteness.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981963Fig. 8. Number of messages used by negotiate and argue methods at 50% level of missing knowledge.Figs. 9(a) through to 9(f) show the effectiveness of both the strategies as the number of roles increases within thesociety from 3 to 20. One of the key observations here is the declining rate of the non-argue strategy. We can see thatas the number of roles increase, the rate of decline of the non-argue method becomes less pronounced. Furthermore, thecrossover point, where the non-argue method starts to be less effective than the argue strategy, also shifts increasingly tothe right (i.e., higher knowledge imperfections).This again is a very interesting observation. As agents gain a higher number of roles, they acquire an increasing numberof social influences. Now, as explained in Observation 1, the agents use these social influences as a resource to endorsetheir actions. Thus, when an agent has a higher number of social influences, its lack of knowledge about a certain particularinfluence makes little difference. The agent can easily replace it with another influence (which it is aware of) to convince itscounterpart. Therefore, under such conditions, agents arguing about their social influences to correct their lack of knowledgewould have little reward since the non-argue method can more simply replace it with another known influence and stillachieve the same end. In such high resource settings, only when an agent has a near complete lack of knowledge (i.e., 80–90% levels) does the argue strategy yield significant performance gains. This observation complements our previous study onthe worth of argumentation at varying resource levels [35], where we show that the benefit of arguing is more pronouncedat low resource settings and under higher resource conditions is less beneficial.The experiments thus far assume that, if a conflict occurs about the validity of a certain premise (i.e., a particular agentacts a certain role within the society), the related parties have the ability to provide sufficient justification to clearly ascer-tain whether it is indeed valid or invalid (refer to Algorithm 4). Therefore, in such situations, the defeat-status computationmechanism only needs to decide between two possibilities; whether the premise in question is valid or invalid. However,in most realistic societies, agents may fail to provide sufficient justification to precisely determine the outcome of everyargument. Thus, when arguing in such situations, the defeat-status computing algorithm now needs to take into account athird possibility: undetermined, indicating that the given justification is not sufficient and it requires more justification toclearly ascertain its validity (refer to Section 3.4.1). In such situations, the argumentation mechanism will fail, leaving theconflict unresolved. To incorporate such social conditions and to evaluate the performance of ABN under such failures, wenext alter our ABN strategy, CPA, to devise a new ABN strategy CPA-with-n%-Failure. Here, n represents the level of failure,or more precisely, the percentage of times the defeat-status algorithm fails to deliver a clear outcome. We experiment withthis strategy in relation to both CPA and CPNA. The results are presented in Fig. 10 from which we draw the followingobservation.Observation 5. Failure to reach agreements reduces the effectiveness of ABN. However, even with high levels of failure, the ABN strategywill still out perform the non-arguing approach.Figs. 10(a) through to 10(f) clearly show that the CPA-with-n%-Failure strategy deteriorates in performance as the num-ber of failures increase. For instance, the CPA-with-40%-Failure (refer to Fig. 10(c)) allows agents to resolve more conflictsand achieve a higher total earning than the CPA-with-60%-Failure strategy (refer to Fig. 10(d)). Thus, the failure to reachagreements reduces the effectiveness of the ABN strategy. However, we can observe that still, even with 60% or 80% failures,the ABN strategy (CPA-with-n%-Failure) still performs more effectively than the non-arguing CPNA one.5.2.2. Questioning non-performanceIn the event that a particular social commitment is violated, apart from the right to demand compensation, our socialinfluence schema also gives the agents the right to challenge and demand a justification for this non-performance (see964N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981Fig. 9. Total population earnings with 30 agents and a varying number of roles.Section 3.1). It is generally argued in ABN theory that allowing agents to exchange such meta-information in the form ofjustifications gives them the capability to understand each others’ reasons and, thereby, provides a more efficient method ofresolving conflicts under uncertainty [59]. Here we attempt to empirically evaluate this general hypothesis in a multi-agentcontext (i.e., with more than two agents). In particular, we believe that providing the agents with the capability to challengeand demand justifications for violating social commitments allows them to gain a wider understanding of the internal andN.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981965Fig. 10. Total population earnings with 30 agents at varying levels of failure.social influences affecting their counterparts. Thereby, we believe, it will provide a more efficient method for managingsocial influences in the presence of incomplete knowledge.To test this underlying hypothesis we extend our previous best strategy Claim_Penalty_Argue (CPA) to design two addi-tional strategies; Argue_In_First_Rejection (AFR) and Argue_In_Last_Rejection (ALR). Both these strategies allow the agents tochallenge non-performance of social commitment, but at different stages within the negotiation encounter. More specifically,966N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9811: [p0, p1, . . . , pmax] ← generateProposals()2: p ← p03: isAccepted ← false4:5: {Loop till either the agent agrees or the last proposal fails.}6: while (isAccepted (cid:15)= true (cid:16) p (cid:2) pmax) do7:response ← Propose(p)if (response = “accept”) thenisAccepted ← trueelse{Challenge to find reason if the first proposal is rejected.}if (p = p0) thenreasonsToRefuse ← Challenge(p)if (reasonsToRefuse = notCapable) thenrequestedCapability ← reasonsToRefuseupdateMyKnowledge(agent, requestedCapability)else if (reasonsToRefuse = notViable) thenthreasholdPrice ← reasonsToRefuseupdateMyKnowledge(agent, time, threasholdPrice)deemedCompensation ← reasonsToRefuseif (deemedCompensation < rightToPenalty) thenargue()end ifend ifend ifif (p (cid:15)= pmax) thenp ← getNextViableProposal()end if8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:end if31:32: end while33:34: if (isAccepted = false) then35:36: end ifcompensation ← demandCompensation()Algorithm 7. The Argue_In_First_Rejection (AFR) strategy.the former allows agents to challenge after the receipt of the first rejection and the latter after the last rejection. Thus,the two differ on when agents attempt to find the reason (in the first possible instance or after all proposals have beenforwarded and rejected). To formulate these two strategies we extend our CPA algorithm, by incorporating a challenge phaseinto its negotiation element in order to find the reason for rejecting a proposal. In the case of AFR, this challenge is em-bedded after the first proposal is rejected, while in the case of ALR it is embedded after the rejection of the final proposal.Algorithm 7 specifies the AFR strategy. The ALR merely alters when to challenge to find reason; i.e., the test condition inline 13 of the Algorithm 7 is altered to if (p = pmax) then. Given this, Figs. 11(a) and 11(b) show our results and the followinghighlight our key observations:Observation 6. The effectiveness of the various argumentation strategies are broadly similar. However, allowing the agents to challengeearlier in the dialogue, significantly increases the efficiency of managing social influences.Fig. 11(a) shows no significant difference in the effectiveness of the three ABN strategies. This is due to the fact that allthree strategies argue and resolve the conflicts even though they decide to argue at different points within the encounter.Therefore, we do not expect to have any significant differences in the number of conflicts resolved. Thus, the effectivenessstays the same.However, Fig. 11(b) shows a significant difference in the number of messages used by the three strategies at all levels ofknowledge. In particular, the number of messages used by the Argue_In_Last_Rejection (ALR) strategy is significantly lowerthan our original Claim_Penalty_Argue (CPA) one. Moreover, the Argue_In_First_Rejection (AFR) strategy has the lowest numberof messages exchanged.The reason for this behaviour is based on how the agents use these reasons exchanged during the argue phase. In theCPA strategy the main objective of arguing is to resolve the conflict regarding the penalty value that should be paid. How-ever, it does not attempt to find out the actual reason why the counterpart rejected the proposal and failed to honourits social commitment in the first place. For instance, a certain agent may fail to honour a specific social commitmentsimply because it does not possess the necessary capability level to carry out the requested action. It may also be occu-pied at the requested time and may perceive this action to be less viable to de-commit from than its prior agreement.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981967Fig. 11. Efficiency and effectiveness of the various argumentation strategies.By challenging for the reason for the rejection, the latter two strategies allow the requesting agent to gain such meta-information and use them both in their current encounter and any subsequent ones. For instance, if a certain agent refusesto perform a specific action because it does not have the necessary capability level, then the requesting agent can ex-clude that counterpart from any future service requests that may require a capability level the same or greater than therefused action. If its counterpart refused the proposal because it is not viable, then by challenging the reasons for re-fusal, agents can also gain knowledge about their current asking price (the price at which it would become viable). Agentscan then use this information to straight away forward a proposal that meets this asking price, rather than sequentiallyincrementing its offering rewards which would eventually get rejected. In this manner, such reasons give useful meta-information, which the agents can use in their future negotiations. Since the AFR and ALR strategies allow the agents tochallenge, obtain, and exploit such information, they allow the agents to interact more efficiently as a society than whenusing CPA.Moreover, the AFR strategy, which allows agents to argue in the first rejection, provides this information earlier in thenegotiation encounter, which, in turn, gives the agents more potential to exploit such information (even during the presentnegotiation) than getting it in the last encounter (as in ALR). Given this, we can conclude that, in our context, allowingthe agents to challenge non-performance earlier in the negotiation allows them to manage their social influences moreefficiently as a society.Finally, in this line of experiments, we design a strategy that allows agents to reveal information selectively after takinginto consideration the future consequences of such revelation. In more detail, in certain instances, an agent may act certainroles that may entail more obligations than rights. In such instances, it would be to the advantage of that agent not toreveal that information to its counterparts. In this manner, agents may choose to exploit the lack of knowledge of theircounterparts and, thereby, play a more self-interested strategy by choosing to forgo certain rights to obtain a long term gainby not carrying out (or paying violation penalties for) its obligations.To explain this more clearly, consider our simple supervisor student example detailed in Section 1 with two agents Andyand Ben; Andy playing the role of a Ph.D. student and Ben the role of his supervisor. Now, assume that Ben, due to thissupervisory role, gains a single right (i.e., to demand the student to submit the thesis on time) and two obligations (i.e.,to correct the student’s papers and to provide financial aid) towards his student. Due to the imperfect information presentwithin the society, in certain instances, Andy may not be aware of either the fact that Ben assumes the role of supervisoror that he himself assumes the role of student. Due to this missing knowledge, in either case, Andy would not be aware ofthe corresponding obligations and the rights he has towards Ben. In such instances, if the supervisor Ben believes that histwo obligations cost more than the benefit he gains from exercising his right, Ben may play a more self-interested strategyand exploit Andy’s lack of knowledge by choosing not to reveal this information. Thereby, Ben may choose to forgo his lessimportant right in the view of a long term potential to violate his two obligations without any de-commitment penalty, andthus play a more self-interested strategy within the society.Here, our motivation is to explore the broad implication of agents using such a self-interested strategy to manage theirsocial influences within a society. In order to test the impact of this behaviour, here we alter our current best strategy, AFR,and allow agents to evaluate the long term benefits and costs before revealing information about their social influenceswithin the argumentation process. More specifically, we modify our argue function specified in Algorithm 4 and introducean additional test condition before all assertions (refer to Algorithm 8). This test condition (the isAssertViable method)evaluates the long term benefit by calculating the total benefit of the rights that the agent would gain minus the costof obligations it would incur in the event of revealing a certain piece of information to its counterpart. We then use968N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9811: {Challenge for the opponent’s justification}2: Ho ← challenegeJustification()3: {Generate personal justification}4: H p ← generateJustification()5:6: if (isValid(Ho) = false) then7:if (isAssertViable(Ho) = true) then{Assert invalid premises of Ho }8:{Adopt premises of Ho into personal knowledge}end if9:10: else11:12: end if13: if (isValid(H p) = false) then14:{Correct invalid premises of H p within personal knowl-edge}15: else16:if (isAssertViable(H p) = true) then17:{Assert H p }end if18:19: end ifAlgorithm 8. The selectiveArgue() method.Fig. 12. Efficiency and effectiveness of the AFR and the SAFR strategies.this modified selectiveArgue() method in place of the argue() function in line 23 of the AFR Algorithm 7 to formulate ourselective argue strategy. We identify this strategy as Selective_Argue_In_First_Reject (SAFR). Figs. 12(a) and 12(b) plot both theeffectiveness and efficiency of using this SAFR strategy in comparison to AFR from which we make the following observation.Observation 7. Allowing agents to selectively reveal information reduces the performance of the society both in terms of effectivenessand efficiency.In Figs. 12(a) and 12(b) we can clearly observe a slight (yet significant) decrease in the overall performance of the societywhen agents are using SAFR in comparison to AFR. Both in terms of effectiveness and efficiency, it is clear that when usingSAFR the agents as a society tend to achieve a lower overall earnings value (see Fig. 12(b)) and also use a higher numberof messages (see Fig. 12(b)) to accomplish this lower outcome. The difference is more pronounced at settings with higherlevels of missing knowledge (i.e., 70%, 80%, 90% levels).To help us explain the reason for this behaviour, Figs. 13(a) through to 13(b) plot the percentage of information knownto the agents during the course of their interactions while using both these strategies. In these we can observe that whenusing SAFR, because the agents selfishly choose not to reveal information about their social influences in instances where itis to their individual long term disadvantage, certain conflicts within the society remains unresolved. This, in turn, causes thepercentage of information known to the agent to increase at a much slower rate (see Figs. 13(a) through to 13(b)) than whenusing AFR. Moreover, a significant proportion of information still remains missing even at the end of the simulation (seethe 70% and 80% levels in Figs. 13(c) and 13(d)). This missing knowledge leaves the agents unaware of a certain number oftheir social influences. Since the agents cannot use these influences to endorse their actions, the society as a whole achievesa smaller number of actions. Therefore, when individual agents play this self-interested selective argumentation strategy,N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981969Fig. 13. Information flow between the AFR and the SAFR strategies.the agent society as a whole performs less effectively. Furthermore, due to the information about their social influencespropagating slowly and some remaining missing, agents are unable to use them to endorse their actions and, thus, need tonegotiate more with their counterparts to accomplish their actions. These increased negotiations use a significantly highernumber of propose, accept, and reject messages, thereby, increasing the total message count used within the society. Thus,not only does this self-interested selective ABN strategy make the agent society less effective, but it also makes it lessefficient.5.2.3. Negotiating social influenceIn addition to acting as a mechanism for resolving conflicts of opinion in relation to social influences, ABN can also enableagents to augment their negotiation process by way of incorporating threats and promises along with their proposals (referto Section 3.2). More specifically, within a social context, agents can use negotiation as a tool to trade social influences byincorporating these as additional parameters within the negotiation object. Allowing them to do so would, in turn, enhancetheir ability to bargain and, in certain instances, increase their chances of reaching mutually acceptable agreements withina society.This acts as the main underlying hypothesis in our following experiments. In essence, here we use our argumentationmodel to design two extended ABN strategies that allow agents to trade their social influences while arguing within ourexperimental context. In particular, our agents attempt to negotiate for the services of their counterparts. While doingso, agents may, in certain instances, find that they do not have the necessary finances to meet the demands of theircounterparts. In such situations, agents may be able to endorse such actions with additional social influences, by way oftrading away some of their existing rights to influence, which they believe to be either redundant or less important toattaining their overall objectives. Since, within our context, the degree of influence associated with each specific social rightor obligation is reflected by its associated de-commitment penalty, agents have the ability to trade away such rights andobligations in exchange for another by simply negotiating this penalty charge. For example, if an agent desires to increase970N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–9811: isAccepted ← negotiateAFR()2:3: {If the maximum possible proposal for an action is refused.}4: if (isAccepted = false && p = pmax) then5:{Attempt to negotiate social influences from the current time slot that are redundant.}substituteRight ← findSubstituteCurrentRedundent()if (substituteRight (cid:15)= null) then6:7:8:9:10:11:12:negotiateRights(currentRightInNeed, substituteRight)response ← Propose(p)if (response = “accept”) thenisAccepted ← trueend ifend if13:14: end if15:16: if (isAccepted = false) then17:18: end ifcompensation ← demandCompensation()Algorithm 9. Argue_First_Reject-Negotiate_Current_Redundant (AFR-NCR).the influence of a certain social right in exchange for a decrease of another, it can do so by negotiating with its counterpartand agreeing to increase the penalty charge associated with the former right in exchange for a decrease of the latter. In thismanner, these extended strategies allow agents to increase the influence of a certain social right at the expense of another,presumably a less important one, and thereby negotiate social influences to achieve their actions.We implement both these extended strategies by enhancing our current best ABN algorithm, AFR (Algorithm 7). Morespecifically, in these we allow agents to trade their social influences in the event that their basic negotiation interaction(trading with proposals) has been unsuccessful in reaching an agreement. In such instances, both of these strategies al-low agents to trade an existing social right it may have, in exchange for a stronger one with a higher penalty value and,thus, a higher influence. However, they differ in the manner in which they select this replaceable right to influence. Thefirst strategy, AFR-NCR (Argue_First_Reject-Negotiate_Current_Redundant), allows agents to choose a redundant social rightthat they may have upon the same counterpart to demand a different capability type within the same time-slot. Since,within our context, agents have only a single action, which requires only a single capability per time slot, any rights thatmight have demanded another capability type would be redundant towards their overall objectives. Thus, in this strategy,the agents are allowed to trade those redundant capabilities in exchange for increasing the influence of a more requiredright.On the other hand, the second strategy, AFR-NFLI (Argue_First_ Reject-Negotiate_Future_Less_Important), allows agents tofind their substitute right from a future action that they believe to be less important than the current one. In more detail, ifa certain action has a higher reward value, then the agent can afford to spend more to convince another agent to performit (refer to the proposal generation algorithm in Section 5.1 where the maximum monetary offer is defined as the rewardvalue for action r j − 1). Since an agent can afford to spend more on such actions, it can utilise any social influences itmay have on others in order to accomplish its more financially constrained ones (i.e., actions with a lower reward, and,therefore, more financially constrained). Using this as the main intuition, the AFR-NFLI strategy allows agents to trade theseless important social influences in exchange for supplementing actions that fail to even meet the initial asking price of theircounterparts.To this end, Algorithm 9 specifies the operation of our AFR-NCR strategy. In essence, here we first allow the basic AFRalgorithm to negotiate an agreement. However, if it fails to do so, then the extended strategies allow the agents to select asubstitute right and use its social influence to negotiate with their counterparts. In particular, the AFR-NCR uses the func-tion findSubstituteCurrentRedundent() to find this substitute right (see line 6 of Algorithm 9). The AFR-NFLI merely altersthe way that these agents select these substitute rights and uses an alternative function findSubstituteFutureLessImportant()in place of the above line 6. Having specified these extended strategies, Figs. 14(a) and 14(b) plot their performance (bothin terms of effectiveness and efficiency) in comparison to our AFR strategy and the following analyses our main observa-tions.Observation 8. Allowing agents to negotiate social influence enhances the effectiveness of the society.Fig. 14(a) shows a clear increase in the total earnings of the population when the agents are allowed to trade their socialinfluences. In particular, both the extended strategies, AFR-NCR and AFR-NFLI, outperform the original AFR strategy; allowingthe agents a means of performing more effectively within a social context. We can explain the reason for this observationas follows. As explained in Observation 1, social influences act like a resource for the agents to endorse their actions. Insuch a context, when these agents are allowed to trade their social influences, they gain the opportunity to re-allocatethese resources in a more useful manner. In more detail, both strategies allow agents the opportunity to supplement certainactions that require such an endorsement in exchange for foregoing certain social influences that are either redundant orless useful. This, in turn, allows the agents to achieve a higher number of actions.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981971Fig. 14. Efficiency and effectiveness of the AFR, AFR-NCR, and the AFR-NFLI strategies.More specifically, while using AFR in our simulations, agents were capable of completing 61.5% (with a 0.8% standarderror) of their actions on average. However, when they were allowed to trade social influence, both the strategies signifi-cantly increased this completion level allowing agents to reach 69.4% (0.6% standard error) with AFR-NCR and 71.9% (0.7%standard error) with AFR-NFLI. This significant increase in the number of actions completed, allowed the agents to increasetheir earnings, thereby, performing more effectively as a society. When comparing AFR-NCR and AFR-NFLI, the latter allowedagents to perform more effectively as a society. The reason for this depends on how successful the agents are in findinga substitute social influence to trade with. In the former case, agents constrain themselves to only the current time slot,whereas the latter allows them to search through a number of future time-slots. This, in turn, increases the probability ofAFR-NFLI successfully finding a substitute to trade with, thus, significantly enhancing its effectiveness.Observation 9. When agents negotiate social influences they also achieve their tasks more efficiently as society.Fig. 14(b) shows a significant reduction in the number of messages used by the agents when they are allowed to tradetheir social influences within a society. More specifically, agents used a total of 112 164 messages when using the AFRstrategy. However, when using AFR-NCR this number is reduced by 10.1% and with AFR-NFLI by 13.8%. As explained above,when agents are allowed to trade social influences, they are able to re-arrange their influences in a more suitable mannerto endorse their actions. As a result, this increases the probability of reaching an agreement with their counterparts withinthe current encounter. Due to this increased success in their current negotiation encounters, agents are less likely to berequired to iterate through the society finding alternative counterparts and exhaustively negotiating with each other to reachagreements. This, in turn, significantly reduces the negotiation messages (open-dialogue, close-dialogue, propose, reject)used within the society and out numbers the small increase in the messages used by the agents to trade social influences.Furthermore, the AFR-NFLI strategy (in comparison to AFR-NCR) allows agents to perform at a much higher efficiency levelwithin the society. Again this is because the AFR-NFLI strategy is less constrained than the AFR-NCR strategy (i.e., notconstrained only to the current slot, but allows them to search through an array of future time slots) in allowing agents tofind a successful substitute to trade with.6. Conclusions and future workThis paper centres around two broad areas of AI; namely argumentation-based negotiation and multi-agent systems.In particular, we present a novel ABN framework that allows agents within structured societies to argue, negotiate, andresolve conflicts in the presence of social influences. The framework is theoretically grounded, successfully mapped into acomputational context, and empirically evaluated to identify a number of different ways that agents can use ABN to enhancethe performance of an agent society (see Sections 3, 4, and 5 respectively). In so doing, this paper makes a contribution toboth the theory and practise of argumentation in multi-agent systems. The following highlights these main contributions inmore detail.In essence, our ABN framework is composed of four main elements: (i) a schema that captures how agents reason aboutinfluences within a structured society, (ii) a mechanism to use this stereotypical pattern of reasoning to systematically iden-tify a suitable set of social arguments, (iii) a language and a protocol to exchange these arguments, (iv) and a decision makingfunctionality to generate such dialogues. These four elements interact in a coherent and systematic manner (see Section 3).In more detail, the schema that captures agents’ social reasoning is used to extract the social arguments. The language(more specifically the domain language) flows naturally from this schema and, in turn, is used to encode these social ar-guments. In addition, the communication component of the language is strongly linked to the protocol that defines the972N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981rules of encounter to resolve agents’ conflicts. Finally, the protocol is, in turn, used to identify the various individual deci-sion mechanisms to present a coherent and a comprehensive model for agents to argue and negotiate within a structuredsociety.One of the distinguishing features of this framework is that it explicitly takes into consideration the societal elementof a multi-agent system (the social structure and the different influences within it) and, in turn, investigates how thisimpacts the way these agents argue and negotiate within such a community (see Section 3.1). In particular, by using thesocial influence schema, we explicitly capture social influences endemic to structured agent societies and identify a numberof different ways agents can use these influences constructively in their argumentative dialogues. Even though a numberof authors have highlighted the importance of the influences of the society in the argumentation process [59,61], no onehas previously presented a framework to capture this element. Existing work tends to focus on two agent contexts whichlargely ignores the impact of the society. Analysing systems based on such frameworks gives only a partial picture of thesystemic effect of ABN in multi-agent systems (refer to [34] for more details). In contrast, our framework, which explicitlycaptures these influences of a society, leads the way to a thorough analysis on the constructive interplay between ABN andsocial influences. In so doing, this paper extends the state of the art in the application of argumentation in multi-agentsystems.From the argumentation theory point of view, analogous to argumentation schemes for practical reasoning and for expertopinion [79], our social influence schema presents a novel argumentation scheme for reasoning within structured societies.Moreover, the way we used our schema to systematically identify arguments within an agent society (see Section 3.2)also presents a successful attempt to use such schemes in computational contexts. This is a developing area of researchin argumentation literature, where a number of authors have conceptually argued for the potential of such schemes incomputational contexts [62,78]. This work, in line with Atkinson et al. [4], contributes to this field. In particular, whileAtkinson et al. present a model that explores the use of argumentation schemes for practical reasoning, this paper presentsthe use of such schemes for social reasoning in multi-agent systems.In addition, the protocol and the language elements in conjunction with the decision functions present a comprehen-sive dialogical model to automate argumentative dialogues to manage conflicts in multi-agent systems (see Sections 3.3and 3.4). In so doing, it enhances the contribution of this paper to both the argumentation and multi-agent systems com-munities. More specifically, here, we present a protocol for agents to argue, negotiate, and manage conflicts in structuredmulti-agent systems. Similar to the work by McBurney et al. [47], we ground our protocol by specifying its semantics bothin axiomatic and operational terms. Even though grounded in a similar manner, our protocol achieves a different purpose.More specifically, while McBurney et al. present a protocol for consumer purchase negotiations, the language and protocoldefined in this paper allow agents to manage conflicts related to social influences in multi-agent systems. Moreover, wego a step further than McBurney et al. in our domain. In particular, while McBurney et al. explore the completeness oftheir protocol by explaining its operation in a number of case studies, we define concrete algorithms, implement them,and experiment with how an agent society can use our model to resolve conflicts in a multi-agent task allocation sce-nario.The types of social arguments and the strategies designed in this paper identify an array of ways in which argumen-tation can be useful in multi-agent systems (see Section 3.2). More specifically, this paper identifies two major ways ofusing argumentation in multi-agent systems; namely argue about social influences and negotiate social influences. In a broadersense, both these techniques capture inspiration from human societies and signify how humans argue and negotiate toenhance their performance within a social context. In particular, the former allows individuals to correct their misconcep-tions and, thereby, overcome certain inefficiencies due to incomplete information present within the society. The latter, onthe other hand, allows individuals within the society to trade away less useful social influences, and, thus, re-organisetheir influence structure to suit the current task environment. In this manner, both these methods allow a society ofindividuals to achieve a higher level of collective performance. In bringing these socially inspired techniques forward, mod-elling them within an argumentation context, and encoding such behaviour in a computational environment, this paperalso makes contributions not only to the argumentation community, but also to the broader computer science commu-nity.Given these distinct theoretical contributions, the second set of contributions of this paper come from our work inhelping to bridge the theory to practise divide in argumentation research. Most existing argumentation frameworks fail toaddress this divide. They tend to focus more on the theoretical soundness and the completeness of their models and ignorethe computational costs associated with them. Typically, they either present no implementations of their models or, in veryrare instances, present limited experiments in highly constrained two agent contexts. Thus, the gap between the theory andthe practise in argumentation research is well documented [41,59]. In contrast, we use our theoretical model to formulateconcrete algorithms and, in turn, use them to implement the various decision functions connected to our protocol (referto Section 4). In so doing, we successfully map our theory into a computational context and implement an array of ABNstrategies to resolve conflicts in a multi-agent task allocation scenario.In addition to extending the state of the art in forwarding a fully implemented ABN model, we also successfully use thismodel to develop a number of conflict resolution strategies into our argumentation context (see Section 5). In particular,our strategies capture inspiration from both the social science and multi-agent systems literature (i.e., exercising the rightto claim compensation, question non-performance, negotiating social influence) and represent an array of ways in whichagents can manage conflicts in a multi-agent society (refer to Sections 5.2.1, 5.2.2, and 5.2.3). Thus, our experiments areN.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981973neither based on a constrained two agent setting, nor are they limited to one or two carefully chosen ABN methods ded-icated to that context. By mapping these diverse set of strategies within our framework we exemplify its versatility andflexibility.Last, but not least, the results of our experiments also contribute to ABN in multi-agent systems research via a numberof interesting findings (see Section 5.2). In essence, first we allow agents to exercise their right to demand compensationwhen managing conflicts. In particular, here we design two strategies; one that merely demands and collects compensa-tion (non-ABN) and the other that allows agents to resort to argumentation to resolve any discrepancies that may arisewhile negotiating such compensations (ABN). Our results show that allowing agents to use an ABN mechanism to do soenhances their ability to resolve conflicts even at high uncertainty levels. This, in turn, shows ABN to be a more efficientand effective strategy when compared to a non-arguing approach (refer to Observations 1, 2, and 3). However, we alsoshow that this comparative advantage diminishes as the number of social influences (which act as resources) increasewithin the context (refer to Observation 4). This latter observation further justifies our previous experimental result onthe negative correlation of the benefit of arguing and resources available within the context [35]. Given this, next, weexperimentally consider the effectiveness of our ABN strategy in the presence of failures (inability to reach agreementsdue to the lack of sufficient justification). Here, our observations show that failures do indeed reduce the effectivenessof our ABN strategy. However, even with high levels of failure, it still out performs the non-arguing approach (refer toObservation 5). Next in our experiments, we allow agents to exercise their right to question the non-performance in theevent of a conflict and, thereby, allow them to argue about the reason for the conflict. Here, our results show that al-lowing agents to challenge for the reason earlier in their encounter (as opposed to using it as the last resort) enhancestheir efficiency in managing conflicts (refer to Observation 6). Next, in this line of experiments, we design a strategy thatallows agents to selectively reveal information. The results show that allowing agents to do so, reduces the rate of infor-mation propagation within the society, and, therefore, lowers both the efficiency and effectiveness of their performance(refer to Observation 7). Finally, we design a set of strategies that allow agents to negotiate their social influences. Here,we observe that allowing them to do so, enhances their ability to re-allocate these social influences in a more useful man-ner. Thus, this achieves a more efficient and effective way of managing conflicts within a society (refer to Observations 8and 9).This paper also opens the pathway to a number of areas of interesting future exploration. One possible direction is toenhance the framework in order to enable the agents to learn and adapt their argumentation strategies to different individu-als and conditions. In more detail, in our current framework, agents use the social influence schema to extract arguments.Since this schema captures the stereotypical behaviour of the society, these extracted arguments would be effective againsta typical agent that operates within the context. However, if agents have different individual characteristics, certain argu-ments or argumentation techniques may work better with certain individuals (i.e., socially influencing decisions may be abetter way of managing conflicts with understanding individuals since you can reason with them, rather than resorting tothreatening them while negotiating social influences). Furthermore, in certain instances, the settings within the argumenta-tion context may change (i.e., agents may find a better information source, which gives them an increasing level of accessto global knowledge). In such instances as well, certain argumentation strategies may again provide a more effective wayof managing conflicts. In such dynamic situations, if the agents can learn and adapt their strategies to suit the individualor the context, it would provide a more effective way of arguing in such diverse and dynamic environments. This can beachieved by incorporating a learning model into the current ABN framework, thus, allowing agents to adapt their argumen-tation strategies based on their experience on the past encounters. One possibility here would be a re-enforcement learningtechnique [32] that allows agents to profile their counterparts or certain contexts based on their success or failure in theirprevious encounters. Another angle of future research would be to incorporate issues such as trust and reputation into theagents’ argumentation strategy and, thereby, make the framework more applicable within an open agent environment [30].More specifically, the current model considers two issues; viability and feasibility during generating and evaluating pro-posals (see Section 3.4). By extending these decision functions, agents can consider parameters such as trustworthiness orthe reputation level of the other party. In all of these aspects, our framework provides a good point of departure for suchinvestigations within multi-agent systems.Another potential area of future research is to analyse (both in a theoretical and an experimental manner) how agentscan reason about social influences at a cognitive level; especially with the possibility to selectively violate certain obligationsand the normative implications of such violations. One of the main challenges in formalising such a system is to model thenotion of obligation. General deontic logic prescribes that an agent entails an intention to perform its obligations. However,such a model would fail to recognise the agents’ ability to selectively violate such obligations. This is famously known as thecontrary-to-duty reasoning problem in deontic logic [76]. A good example is the moral dilemma experienced by the Sartre’ssoldier; the obligation by duty to kill and the moral obligation not to kill. Logicians have defined two main approachesto handle this problem. The first follows a practical reasoning approach which defines two basic models on obligations: aconflict-tolerant model [9] and prima-facie obligations [65]. The alternative is to follow a more mainstream formal approachsimilar to preference-based dyadic obligations approach suggested by [76]. Even though a number of authors have tried974N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981to use some of these variants (e.g., [16]) their models still remain incomplete and far from an implementable solution.Therefore, this remains a potential area of future research.AcknowledgementsThis research is funded by EPSRC under the Information Exchange project (GR/S03706/01). We like to specially thankCristiano Castelfranchi and Munindar Singh for their insightful comments, advice, and direction given during this study. Wealso like to extend their gratitude to Timothy Norman, Chris Reed, Frank Dignum, Sarvapali Ramchurn, and Pietro Panzarasafor their thoughts, contributions, and discussions. These have been valuable right throughout this study. We also thank thevarious anonymous reviewers for their invaluable comments and suggestions at the various stages of this study. In addition,we also acknowledge AOS Ltd. for their JACK agent framework and support.Appendix A. Notational representation of social argumentHere, we give both the natural language and the notational representation of all social arguments listed in Section 3.2to both socially influence decisions (refer to Table A.1) and negotiate social influences (refer to Table A.2). All argumentsstated are from the point of view of agent ai . Due to space restrictions, here we use an abbreviated form and do not ex-plicitly state the two agents involved in the argument in our notational representation. Therefore, for instance the argumentAssert(ai, a j, ¬Act(ai, ri)) is presented in the abbreviated form as Assert(¬Act(ai, ri)). Also, to save space, in Table A.2 weuse the abbreviated notation ±do() to denote the different combinations of do() and ¬do().Table A.1Social arguments to socially influence decisions.1.i.ii.iii.iv.v.vi.vii.viii.2.i.ii.iii.iv.v.vi.vii.viii.ix.x.3.(a)i.ii.(b)i.ii.(c)Natural Language RepresentationNotational RepresentationDispute (Dsp.) existing premises to undercut the opponent’s existing justification.Dsp. ai is acting debtor role riDsp. a j is acting creditor role r jDsp. ri is related to the relationship pDsp. r j is related to the relationship pDsp. SC is associated with the relationship pDsp. f is the degree of influence associated with ODsp. θ is the action associated with ODsp. θ is the action associated with (cid:4)Assert(¬Act(ai , ri ))Assert(¬Act(a j , r j ))Assert(¬ RoleOf(ri , p))Assert(¬ RoleOf(r j , p))ri ⇒r jAssert(¬AssocWith(SCθAssert(¬InfluenceOf( f , O)Assert(¬ ActionOf(O, θ))Assert(¬ ActionOf((cid:4), θ)), p))Point out new premises about an alternative schema to rebut the opposing decision.P-o ai is acting the debtor role riP-o a j is acting the creditor role r jP-o ri is related to the relationship pP-o r j is related to the relationship pP-o SC is a social commitment associated with therelationship pP-o f is the degree of influence associated with theobligation OP-o θ is the action associated with the obligation OP-o θ is the action associated with the right (cid:4)P-o ai ’s obligation O to performP-o a j ’s right to demand, question and require theaction θAssert(Act(ai , ri ))Assert(Act(a j , r j ))Assert(RoleOf(ri , p))Assert(RoleOf(r j , p))Assert(AssocWith(SCri ⇒r jθ, p))Assert(InfluenceOf( f , O))Assert(ActionOf(O, θ))Assert(ActionOf((cid:4), θ))ai ⇒r jAssert(OθAssert((cid:4)a j ⇒ri))θPoint out conflicts that prevent executing the decision to rebut the opposing decision.Conflicts with respect to OP-o a conflict between two different obligations duetoward the same roleP-o a conflict between two different obligations duetoward different rolesConflicts with respect to (cid:4)P-o a conflict between two different rights to exertinfluence upon the same roleP-o a conflict between two different rights to exertinfluence upon different rolesConflicts with respect to θ and another action θ (cid:10)that (i) θ (cid:10)(ii) θ (cid:10)effects to θis an alternative to the same effect as θ ;either hinders, obstructs, or has negative sidesuchAssert(Oai ⇒r jθAssert(Oai ⇒r jθ∧ Oai ⇒r jθ (cid:10)∧ Oai ⇒rkθ (cid:10)∧ Conflict(do(θ), do(θ (cid:10))))∧ Conflict(do(θ), do(θ (cid:10))))Assert((cid:4)a j ⇒riθAssert((cid:4)a j ⇒riθ∧ (cid:4)a j ⇒riθ (cid:10)∧ (cid:4)a j ⇒rkθ (cid:10)∧ Conflict(do(θ), do(θ (cid:10))))∧ Conflict(do(θ), do(θ (cid:10))))Assert(Conflict(do(θ), do(θ (cid:10))))N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981975Table A.2Social arguments to negotiate social influences.Natural Language RepresentationNotational Representation4.i.ii.5.i.6.(a)i.ii.(b)i.7.i.ii.Use the obligation (O) as a parameter of negotiation.Promise to (or threaten not to) undertake one ormany future obligations if the other agentperforms (or not) a certain action θPromise to (or threaten not to) honour one ormany existing obligations if the other agentperforms (or not) a certain action θUse the right ((cid:4)) as a parameter of negotiation.Promise not to (or threaten to) exercise the rightto influence one or many existing obligations ifthe other agent performs (or not) a certain actionθUse third party obligations and rights as a parameter of negotiation.Third party obligationsPromise to (or threaten not to) undertake one ormore future obligations toward ak to perform θ (cid:10),if a j would (or would not) exercise its right toinfluence a certain agent al to perform θPromise to (or threaten not to) honour one ormore existing obligations toward ak to performθ (cid:10)influence a certain agent al to perform θThird party rights, if a j would (or would not) exercise its right toPromise to (or threaten not to) exercise the rightto influence one or many existing obligationstoward ak to perform θ (cid:10)existing obligation to perform θ, if a j would honour itsUse P as a parameter of negotiation.(cid:10)Threaten to terminate p (its own relationship(a third party relationship that aiwith a j ) or phas with ak), if the agent a j performs (or not) acertain action θThreaten to influence another agent (ak) towith a j , if a jterminate its relationship pperforms (or not) a certain action θ(cid:10)(cid:10)ai ⇒a jPropose(do(a j , θ), do(ai , adopt(Oθ (cid:10)ai ⇒a jPropose(do(a j , θ), ¬do(ai , adopt(Oθ (cid:10)ai ⇒a jPropose(¬do(a j , θ), do(ai , adopt(Oθ (cid:10)Propose(¬do(a j , θ), ¬do(ai , adopt(O)))))))))ai ⇒a jθ (cid:10))))Propose(±do(a j , θ), ±do(ai , drop(Oai ⇒a jθ)))Propose(±do(a j , θ), ±do(ai , drop((cid:4)ai ⇒a jθ (cid:10))))Propose(±do(a j , (cid:4)a j ⇒alθ), ±do(ai , adopt(Oai ⇒akθ (cid:10))))Propose(±do(a j , (cid:4)a j ⇒alθ), ±do(ai , drop(Oai ⇒akθ (cid:10))))ai ⇒a jPropose(do(a j , OθPropose(¬do(a j , Oai ⇒a jθ), ¬do(ai , drop((cid:4)ai ⇒ak), do(ai , drop((cid:4)ai ⇒akθ (cid:10)θ (cid:10)))))))Propose(±do(a j , θ), do(ai , drop(p)))(cid:10))))Propose(±do(a j , θ), do(ai , drop(pPropose(±do(a j , θ), do(ai , (cid:4)ai ⇒akdo(ak ,drop(p(cid:10)(cid:10)))))Appendix B. Operational semanticsHere we present an operational semantics for the multi-agent communications protocol whose syntax is given in Sec-tion 3.3. As explained in Section 3.5, this semantics considers the effects of legal agent utterances as if they were programlanguage commands acting on a virtual computer. In defining this semantics we bring together the protocol, which definesthe rules of the interaction, with the internal decision-making mechanisms of the agents participating in the interaction.In the following paragraphs, we label the thirty-one transition rules of the operational semantics with the symbols “TR1”,“TR2”, etc.We define our semantics using the labelled terminal transition system (LTTS) [54]. In more detail, the LTTS defines theoperation of a system as a series of tuples (cid:18)Γ, A, →, T (cid:19), where Γ represents a set of configurations, A a set of labels,→ : Γ × A × Γ defines a transition relation, and T a set of terminal (or final) configurations; i.e., ∀γ ∈ T , (cid:3)γ (cid:10) ∈ Γ, α ∈ Aα−→ γ2. This method of specifying operationalsuch that (γ , α, γ (cid:10)) →. Conventionally, (γ1, α, γ (cid:10)2) → is sometimes written γ1semantics can be used at different levels of detail, and what counts as one transition for one purpose may be representedthrough many transitions when viewed in more detail [54].In our specification, a configuration γ ∈ Γ is itself a tuple [ai, P , o], where ai is an agent, P is a decision mechanismbeing executed by agent ai , and o is an output of the decision mechanism. Labels denote locutions (general message types)that cause the transition from one configuration to another (possibly in a different agent). Thus, the intuitive meaning ofL−→[a j, P 2, o2] is that if we were in a configuration where agent ai executes mechanisma transition statement [ai, P 1, o1]P 1 leading to output o1, then after sending a message through locution L, the system moves to a configuration whereagent a j executes mechanism P 2 leading to output o2. In certain instances, we also use the above notation to captureinternal transitions where a certain internal decision mechanism leads to another state within an agent. Such transitionsdo not involve communications between different agents, but only changes in the internal state of a single agent. For this976N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981reason, these internal transitions are represented by arrows without labels. It also important to note that in our transitionstatements, we usually refer to output schema as opposed to specific output instances. Moreover, in certain instances weuse the ‘.’ notation to denote any type of output for a given mechanism. Finally, a special state T is used to denote theterminal state of the system. Given this, the following specifies the operational semantics of our ABN system and Fig. B.1captures its operational flow.TR1 If the agent does not require the services of another to accomplish a certain action θ , it will not require any argumen-tation, thus, will move to the terminal state T . To evaluate whether or not the agent requires the services of another,it would use its decision mechanism P1 Recognise Need:(cid:6)(cid:7)ap, P1, noNeedService(θ)→ [ap, P1, T ]TR2 If the agent recognises that it requires the services of another to accomplish a certain action, it will initiate a dialoguewith that agent through the L1: OPEN-DIALOGUE locution. Similar to above, the agent uses the P1: Recognise Needdecision mechanism to evaluate whether or not it requires the services of another. When its counterpart receives thislocution it will initiate its decision mechanism R1: Consider Participation.(cid:6)ap, P1, needService(θ)(cid:7) L1−→[ar, R1, .]TR3 When an agent receives an invitation to enter into a dialogue via the L1: OPEN-DIALOGUE locution, it will indicate itsreadiness via its own L2: OPEN-DIALOGUE locution. Once the proponent receives this reply it will, in turn, initiate thedecision mechanism P2: Generate Proposals attempting to formulate a viable and a feasible set of proposals.(cid:6)ar, R1, enterDialogue(θ)(cid:7) L2−→[ap, P2, .]TR4 Once an agent has generated a feasible and a viable set of proposals, it will initiate its own decision mechanism P3:Rank Proposals in order to obtain an ordered ranking on this set.(cid:7)(cid:6)ap, P2, Q (θ)→ [ap, P3, .]TR5 Once the proposals are ranked, the agent will initiate its own P4: Select Proposal mechanism to a select a proposal toforward to its counterpart.(cid:7)(cid:6)ap, P3, S(θ)→ [ap, P4, .]TR6 If there is no other proposal left to select (i.e., all possible proposals were forwarded and justifiably rejected) andthe P4: Select Proposal mechanism returns null (∅), then the agent will initiate its own P11: Terminate Interactionmechanism to end the dialogue.[ap, P4, ∅] → [ap, P11, .]TR7 If the P4: Select Proposal decision mechanism returns a proposal (i.e., P4 will only return proposals that have not beenpreviously forwarded and justifiably rejected within the encounter), then the agent will forward it to its counterpartvia a L3: PROPOSE locution. Once received, the respondent will initiate the decision mechanism R2: Evaluate Proposalto consider whether to accept or reject this proposal.(cid:6)ap, P4, S i(θ)(cid:7) L3−→[ar, R2, .]TR6 If the respondent decides to accept the current proposal within its R2: Evaluate Proposal mechanism, then it willindicate its decision via the L4: ACCEPT locution. Once a proposal is accepted, the proponent will initiate the decisionmechanism P11: Terminate Interaction to bring the dialogue to an end.(cid:4)(cid:6)ar, R2, accept(cid:5)(cid:7) L4−→[ap, P11, .]S i(θ)TR9 If the respondent decides to reject the current proposal within its R2: Evaluate Proposal mechanism, then it willindicate its decision via the L5: REJECT locution. Once received, this Reject will prompt the proponent to initiate themechanism P5: Find Justification, Continue Negotiation, or Terminate, to decide its next course of action.(cid:4)(cid:6)ar, R2, reject(cid:5)(cid:7) L5−→[ap, P5, .]S i(θ)N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981977TR10 While considering its next course of action (via P5), if the proponent decides to terminate the dialogue, it will initiateits own decision mechanism P11: Terminate Interaction to bring the dialogue to an end.(cid:6)ap, P5, terminate(cid:4)(cid:5)(cid:7)S i(θ)→ [ap, P11, .]TR11 If the proponent decides to continue negotiating with its counterpart (via P5), it will attempt to select and forwardan alternative proposal to that agent. In order to select this alternative, the proponent will initiate its own decisionmechanism P4: Select Proposal.(cid:5)(cid:7)(cid:4)(cid:6)ap, P5, continueS i(θ)→ [ap, P4, .]TR12 The proponent may decide (via P5) to challenge its counterpart to establish the reason for rejecting its current pro-posal. In such cases, the proponent will construct an L6: CHALLENGE locution in order to challenge its counterpartfor its justification to reject the proposal. Once a respondent receives such a challenge, it will, in turn, initiate itsown R3: Extract Justification mechanism that will search within its knowledge-base (or formulate) the reason for thecorresponding rejection.(cid:6)ap, P5, challengeReject(cid:4)(cid:5)(cid:7) L6−→[ar, R3, .]S i(θ)TR13 When the respondent extracts its justification for rejecting the proposal (using its decision mechanism R3), it willassert this via an L8: ASSERT locution to its counterpart. Once received, this will initiate the proponent’s decisionmechanism P6: Evaluate Justifications, which will attempt to compare its own justification with its counterpart’s andanalyse the cause of the conflict.[ar, R3, Hr] L8−→[ap, P6, .]TR14 While evaluating justifications, if the agent still requires more information to evaluate the validity of one of itscounterpart’s premises (lr ∈ Hr), it will attempt to acquire this knowledge via challenging this assertion via the L7:CHALLENGE locution. This will, in turn, restart the opponent’s R3: Extract Justification mechanism.(cid:6)ap, P6, needMoreJustification(lr)(cid:7) L7−→[ar, R3, .]TR15 While evaluating justifications, if the agent still requires more information to evaluate the validity of one of its ownpremises (l p ∈ H p), it will restart its own P7: Extract Justification mechanism to establish the reasoning behind thispremise.(cid:7)(cid:6)ap, P6, needMoreJustification(l p)→ [ap, P7, .]TR16 While evaluating justifications, if the agent finds a premise within its own justification l p to be invalid, then it willinitiate its P8: Update Knowledge mechanism to update its own knowledge-base correcting the invalid premise.(cid:7)(cid:6)ap, P6, invalid(l p)→ [ap, P8, .]TR17 While evaluating justifications, if the agent finds all premises within its counterpart’s justification Hr to be valid, thenit will initiate its P8: Update Knowledge mechanism to update its own knowledge by inserting this valid justificationinto its knowledge-base.(cid:7)(cid:6)ap, P6, valid(Hr)→ [ap, P8, .]TR18 While evaluating justifications, if the agent finds a premise within its counterpart’s justification lr to be invalid, then itwill dispute this premise through an L9: ASSERT locution. Once received, the respondent will initiate its R4: ConsiderPremise mechanism to consider updating the invalid premise within its knowledge-base.(cid:9)(cid:8)ap, P6, invalid(lr)L9−→[ar, R4, .]TR19 While evaluating justifications, if the agent finds all premises within its own justification H p to be valid, then it willassert its justification through an L8: ASSERT locution. Once received, the respondent will initiate its R4: ConsiderPremise mechanism to consider inserting this justification into its knowledge-base.(cid:6)ap, P6, valid(H p)(cid:7) L8−→[ar, R4, .]978N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981TR20 If the P7: Extract Justification decision mechanism is triggered to establish the reason behind a certain premise l p ,(cid:21) l p from its knowledge and pass it back into its P6: Evaluate(cid:10)p where H(cid:10)pthen it will extract this justification HJustifications mechanism.(cid:6)ap, P7, H(cid:10)p(cid:7)→ [ap, P6, .]TR21 While considering a particular premise, if the respondent’s R4: Consider Premise decision mechanism requires morejustification to accept a particular premise, it will challenge the proponent for this further justification. Once received,this L7: CHALLENGE will trigger the proponent’s P7: Extract Justification mechanism to extract further justifications.(cid:6)ar, R4, needMoreJustification(l)(cid:7) L7−→[ap, P7, .]TR22 Once the proponent’s P7: Extract Justification mechanism has extracted further justification in response to a particularvia a L8: ASSERT locution. This will initiate thechallenge by the respondent, it will forward this justification Hrespondent’s R4: Consider Premise mechanism to reconsider the relevant premise with this additional justification.(cid:10)[ap, P7, H(cid:10)] L8−→[ar, R4, .]TR23 While considering a particular premise l, if the respondent’s R4: Consider Premise decision mechanism decides toaccept that premise, it will incorporate (either update or insert) that into its knowledge-base. Once the knowledgeits updated, it will, in turn, trigger the respondent’s own R5: Consider Counter Argument mechanism to search for apossible counter argument within its updated knowledge-base.(cid:7)(cid:6)ar, R4, knowledgeUpdate(l)→ [ar, R5, .]TR24 Once the proponent updates its knowledge with a particular premise l via the P8: Update Knowledge mechanism,it will trigger the proponent’s own P9: Consider Counter Argument mechanism to search for a possible counterargument within its updated knowledge-base.(cid:7)(cid:6)ap, P8, knowledgeUpdate(l)→ [ap, P9, .]TR25 Within the P9: Consider Counter Argument mechanism, if the proponent finds a valid counter argument it will restartits own P6: Evaluate Justification mechanism with this additional argument.(cid:7)(cid:6)ap, P9, hasCounterArg(H p)→ [ap, P6, .]TR26 Within the R5: Consider Counter Argument mechanism, if the respondent finds a valid counter argument, it willforward this argument via a L8: ASSERT locution to the proponent. This will, restart the proponent’s P6: EvaluateJustification mechanism with this additional argument.(cid:6)ar, R5, hasCounterArg(Hr)(cid:7) L8−→[ap, P6, .]TR27 If the proponent, within its P9: Consider Counter Argument mechanism does not find a valid counter argument, itwill initiate its own P10: Terminate Challenge mechanism to terminate this challenge.(cid:7)(cid:6)ap, P9, noCounterArg()→ [ap, P10, .]TR28 If the respondent, within its R5: Consider Counter Argument mechanism does not find a valid counter argument,it will indicate its agreement to the challenge to the proponent via a L8: ASSERT locution. Once, received, this willinitiate the proponent’s P10: Terminate Challenge mechanism.(cid:6)ar, R5, noCounterArg()(cid:7) L8−→[ap, P10, .]TR29 Once initiated, the proponent’s P10: Terminate Challenge mechanism will take steps to terminate the current chal-lenge. Then it will initiate its own decision mechanism P5: Find Justification, Continue Negotiation, or Terminatethus, transferring control again back to the main negotiation strategy selection algorithm.(cid:7)(cid:6)ap, P10, evaluationComplete()→ [ap, P5, .]TR30 If the proponent decides to terminate the dialogue it will indicate this via a L10: CLOSE-DIALOGUE locution. Once therespondent receives this, it will, in turn, initiate its own R6: Terminate Interaction decision mechanism.(cid:6)ap, P11, exitDialogue(θ)(cid:7) L10−→[ar, R6, .]N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981979TR31 When the respondent’s R6: Terminate Interaction is initiated, it will convey its willingness to close the dialoguevia a L11: CLOSE-DIALOGUE locution. Thus, at this time both the proponent and the respondent will terminate theirinteraction. Once completed, the argumentation system would move to the terminal state T .(cid:6)ar, R6, exitDialogue(θ)(cid:7) L11−→[ap, P11, T ]ReferencesFig. B.1. Operational flow.26[1] L. Amgoud, N. Maudet, S. Parsons, Modelling dialogues using argumentation, in: E. Durfee (Ed.), Proc. of the 4th International Conference on Multi-Agent Systems (ICMAS’98), Boston, MA, 2000, pp. 31–38.[2] L. Amgoud, S. Parsons, N. Maudet, Argument, dialogue and negotiation, in: W. Horn (Ed.), Proc. of the 14th European Conference on Artificial Intelligence(ECAI’00), Berlin, 2000, pp. 338–342.[3] L. Amgoud, H. Prade, Reaching agreement through argumentation: A possibilistic approach, in: D. Dubois, C.A. Welty, M.-A. Williams (Eds.), Proc. of the9th International Conference on Knowledge Representation (KR’04), Canada, 2004, pp. 175–182.26 Note that to simplify presentation, we used a single decision mechanism P7 to refer to the process of extracting justification used both (i) internally bythe proponent agent via TR15 followed by TR20; and (ii) in response to a request for justification by another respondent agent via TR21 followed by TR22.The speech act transitions TR21 and TR22 are labelled with the relevant locutions (L7 and L8 respectively) to avoid any ambiguity.980N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981[4] K. Atkinson, T. Bench-Capon, P. McBurney, A dialogue game protocol for multi-agent argument over proposals for action, Journal of Autonomous Agentsand Multi-Agent Systems 11 (2) (2005) 153–171.[5] P. Baroni, M. Giacomin, G. Guida, Self-stabilizing defeat status computation: Dealing with conflict management in multi-agent systems, Artificial Intel-ligence 165 (2) (2005) 187–259.[6] T.J.M. Bench-Capon, P.E. Dunne, Argumentation in artificial intelligence, Artificial Intelligence 171 (10) (2007) 619–641 (Special Issue on Argumentation).[7] J. Bentahar, M. Mbarki, B. Moulin, Strategic and tactic reasoning for communicating agents, in: N. Maudet, S. Parsons, I. Rahwan (Eds.), Proc. of the 3rdInternational Workshop on Argumentation in Multi-Agent Systems (ArgMAS’06), Hakodate, Japan, 2006, pp. 135–150.[8] J. Broersen, M. Dastani, J. Hulstijn, Z. Huang, L. Torre, The BOID architecture: Conflicts between beliefs, obligations, intentions and desires, in: Proceed-ings of the Fifth International Conference on Autonomous Agents, ACM Press, 2001, pp. 9–16.[9] A. Brown, S. Mantha, T. Wakayama, Exploiting the normative aspect of preference: A deontic logic without actions, Annals of Mathematics and ArtificialIntelligence 9 (1993) 167–203.[10] C. Castelfranchi, Commitments: From individual intentions to groups and organizations, in: V. Lesser (Ed.), Proc. of the 1st International conference onMulti-Agent Systems (ICMAS’95), San Francisco, CA, 1995, pp. 41–48.[11] L. Cavedon, L. Sonenberg, On social commitment, roles and preferred goals, in: Proc. of the 3rd International Conference on Multi-Agent Systems(ICMAS’98), Paris, France, 1998, pp. 80–86.[12] C.I. Chesñevar, A. Maguitman, R. Loui, Logical models of argument, ACM Computing Surveys 32 (4) (2000) 337–383.[13] P.R. Cohen, Empirical Methods for Artificial Intelligence, The MIT Press, Cambridge, MA, 1995.[14] F. Dignum, B. Dunin-K ¸eplicz, R. Verbrugge, Agent theory for team formation by dialogue, in: C. Castelfranchi, Y. Lespérance (Eds.), Intelligent AgentsVII: Proc. of the 7th International Workshop on Agent Theories, Architectures, and Languages (ATAL’00), in: LNCS, vol. 1986, Springer Verlag, Berlin,Germany, 2000, pp. 150–166.[15] F. Dignum, D. Morley, E.A. Sonenberg, L. Cavedon, Towards socially sophisticated BDI agents, in: Proc. of the 4th International Conference on Multi-agentSystems, Boston, USA, 2000, pp. 111–118.[16] V. Dignum, D. Kinny, L. Sonenberg, Motivational attitudes of agents: On desires, obligations and norms, in: B. Dunin-Keplicz, E. Nawarecki (Eds.), Proc.of the 2nd International Workshop of Central Eastern Europe on Multi-Agent Systems (CEEMAS’01), vol. 2296, Poland, 2001, pp. 61–70.[17] J. Doyle, A truth maintenance system, Artificial Intelligence 12 (3) (1979) 231–272.[18] P.M. Dung, On the acceptability of arguments and its fundamental role in non-monotonic reasoning, logic programming, and n-persons games, ArtificialIntelligence 77 (2) (1995) 321–358.[19] R. Eijk, Programming languages for agent communications, PhD thesis, Department of Computer Science, Utrecht University, Utrecht, The Netherlands,2000.[20] P. Faratin, C. Sierra, N.R. Jennings, Negotiation decision functions for autonomous agents, International Journal of Robotics and Autonomous Sys-tems 24 (3–4) (1998) 159–182.[21] P. Faratin, C. Sierra, N.R. Jennings, Using similarity criteria to make trade-offs in automated negotiations, Artificial Intelligence 142 (2) (2002) 205–237.[22] M. Fasli, On commitments, roles, and obligations, in: B. Dunin-Keplicz, E. Nawareckiand (Eds.), Proc. of the 2nd International Workshop of CentralEastern Europe on Multi-Agent Systems (CEEMAS’01), vol. 2296, Cracow, Poland, Springer, 2001, pp. 26–29.[23] I. Foster, N.R. Jennings, C. Kesselman, Brain meets brawn: Why grid and agents need each other, in: Proc. of the 3rd International Conference onAutonomous Agents and Multi-Agent Systems (AAMAS’04), New York, USA, 2004, pp. 8–15.[24] M. Gilbert, F. Grasso, L. Groarke, C. Gurr, J.-M. Gerlofs, The persuasion machine: Argumentation and computational linguistics, in: C. Reed, T.J. Norman(Eds.), Argumentation Machines – New Frontiers in Argument and Computation, Kluwer Academic Publishers, Dordrecht, Netherlands, 2004, pp. 121–174.[25] G. Governatori, A. Rotolo, Logic of violations: A gentzen system for reasoning with contrary-to-duty obligations, The Australasian Journal of Logic 4(2006) 193–215.[26] F. Grasso, A. Cawsey, R. Jones, Dialectical argumentation to solve conflicts in advice giving: A case study in the promotion of healthy nutrition,International Journal of Human–Computer Studies 53 (6) (2000) 1077–1115.[27] C.L. Hamblin, Fallacies, Methuen and Co Ltd, London, UK, 1970.[28] W.N. Hohfeld, Fundamental Legal Conceptions as Applied in Judicial Reasoning, Yale University Press, 1919.[29] J. Hulstijn, Dialogue models for enquiry and transaction, PhD thesis, Universiteit Twente, Enschede, The Netherlands, 2000.[30] T.D. Huynh., Trust and reputation in open multi-agent systems, PhD thesis, School of Electronics and Computer Science, University of Southampton,UK, 2006.[31] H. Jung, M. Tambe, S. Kulkarni, Argumentation as distributed constraint satisfaction: Applications and results, in: Proc. of the 5th International Confer-ence on Autonomous Agents (Agents’01), Montreal, Canada, ACM Press, 2001, pp. 324–331.[32] L.P. Kaelbling, M.L. Littman, A.P. Moore, Reinforcement learning: A survey, Journal of Artificial Intelligence Research 4 (1996) 237–285.[33] M. Karlins, H.I. Abelson, Persuasion: How Opinions and Attitudes are Changed, 2nd edition, Lockwood, 1970.[34] N.C. Karunatillake, Argumentation-based negotiation in a social context, PhD thesis, School of Electronics and Computer Science, University ofSouthampton, UK, 2006.[35] N.C. Karunatillake, N.R. Jennings, Is it worth arguing? in: I. Rahwan, P. Moraitis, C. Reed (Eds.), Argumentation in Multi-Agent Systems (Proc. ofArgMAS’04), New York, in: LNCS, vol. 3366, Springer-Verlag, 2004, pp. 234–250.[36] N.C. Karunatillake, N.R. Jennings, I. Rahwan, P. McBurney, Formal semantics of ABN framework, Technical report, School of Electronics and ComputerScience, University of Southampton, http://eprints.ecs.soton.ac.uk/16851/, 2008.[37] N.C. Karunatillake, N.R. Jennings, I. Rahwan, T.J. Norman, Arguing and negotiating in the presence of social influences, in: Proc. of the 4th InternationalCentral and Eastern European Conference on Multi-Agent Systems (CEEMAS’05), Budapest, Hungary, in: LNCS, vol. 3690, Springer-Verlag, 2005, pp. 223–235.[38] N.C. Karunatillake, N.R. Jennings, I. Rahwan, T.J. Norman, Argument-based negotiation in a social context, in: S. Parsons, N. Maudet, P. Moraitis, I.Rahwan (Eds.), Argumentation in Multi-Agent Systems (Proc. of ArgMAS’05), Utrecht, The Netherlands, in: LNCS, vol. 4049, Springer-Verlag, 2005,pp. 104–121.[39] N.C. Karunatillake, N.R. Jennings, I. Rahwan, S.D. Ramchurn, Managing social influences through argumentation-based negotiation, in: Proc. of the 3rdInternational Workshop on Argumentation in Multi-Agent Systems (ArgMAS’06), Hakodate, Japan, 2006, pp. 35–52.[40] S. Kraus, K. Sycara, A. Evenchik, Reaching agreements through argumentation: A logical model and implementation, Artificial Intelligence 104 (1–2)(1998) 1–69.[41] M. Luck, P. McBurney, S. Willmott, O. Shehory, The AgentLink III Agent Technology Roadmap, Technical report, AgentLink III, the European Co-ordinationAction for Agent-Based Computing, Southampton, UK, 2005.[42] J. MacKenzie, Question-begging in non-cumulative systems, Journal of Philosophical Logic 8 (1) (1979) 117–133.[43] G. Mainland, D.C. Parkes, M. Welsh, Decentralized, adaptive resource allocation for sensor networks, in: Proc. of the 2nd USENIX/ACM Symposium onNetworked Systems Design and Implementation (NSDI 2005), Berkeley, CA, 2005, pp. 23–23.[44] N. Maudet, B. Chaib-draa, Commitment-based and dialogue-game based protocols – new trends in agent communication language, Knowledge Engi-neering Review 17 (2) (2003) 157–179.N.C. Karunatillake et al. / Artificial Intelligence 173 (2009) 935–981981[45] P. McBurney, D. Hitchcock, S. Parsons, The eightfold way of deliberation dialogue, International Journal of Intelligent Systems 22 (1) (2007) 95–132.[46] P. McBurney, S. Parsons, Dialogue games in multi-agent systems, Informal Logic 22 (3) (2002) 257–274 (Special Issue on Applications of Argumentationin Computer Science).[47] P. McBurney, R.M. van Eijk, S. Parsons, L. Amgoud, A dialogue-game protocol for agent purchase negotiations, Journal of Autonomous Agents andMulti-Agent Systems 7 (3) (2003) 235–273.[48] J. McCarthy, Circumscription – a form of non-monotonic reasoning, Artificial Intelligence 13 (1–2) (1980) 27–39.[49] P. Panzarasa, N.R. Jennings, T.J. Norman, Social mental shaping: Modelling the impact of sociality on the mental states of autonomous agents, Compu-tational Intelligence 17 (4) (2001) 738–782.[50] S. Parsons, C. Sierra, N.R. Jennings, Agents that reason and negotiate by arguing, Journal of Logic and Computation 8 (3) (1998) 261–292.[51] S. Parsons, M.J. Wooldridge, L. Amgoud, Properties and complexity of formal inter-agent dialogues, Journal of Logic and Computation 13 (3) (2003)347–376.[52] P. Pasquier, I. Rahwan, F. Dignum, L. Sonenberg, Argumentation and persuasion in the cognitive coherence theory, in: P. Dunne, T. Bench-Capon (Eds.),Proc. of the 1st International Conference on Computational Models of Argument (COMMA’06), Amsterdam, Netherlands, IOS Press, 2006, pp. 223–234.[53] C. Perelman, L. Olbrechts-Tyteca, The New Rhetoric: A Treatise on Argumentation, University of Notre Dame Press, Notre Dame/London, 1969.[54] G.D. Plotkin, A structural approach to operational semantics, Technical Report DAIMI FN-19, University of Aarhus, 1981.[55] J.L. Pollock, The logical foundations of goal-regression planning in autonomous agents, Artificial Intelligence 106 (2) (1998) 267–334.[56] H. Prakken, M. Sergot, Contrary-to-duty obligations, Studia Logica 57 (1) (1996) 91–115.[57] H. Prakken, G. Vreeswijk, Logics for defeasible argumentation, in: D. Gabbay, F. Guenthner (Eds.), Handbook of Philosophical Logic, vol. 4, 2nd edition,Kluwer Academic Publishers, Dordrecht, The Netherlands, 2002, pp. 219–318.[58] I. Rahwan, Interest-based negotiation in multi-agent systems, PhD thesis, Dept. of Information Systems, University of Melbourne, Melbourne, Australia,2004.[59] I. Rahwan, S.D. Ramchurn, N.R. Jennings, P. McBurney, S. Parsons, L. Sonenberg, Argumentation-based negotiation, The Knowledge Engineering Re-view 18 (4) (2003) 343–375.[60] S.D. Ramchurn, C. Sierra, L. Godo, N.R. Jennings, Negotiating using rewards, Artificial Intelligence 171 (10) (2007) 805–837 (Special Issue on Argumen-tation).[61] C. Reed, Representing and applying knowledge for argumentation in a social context, AI and Society 11 (3–4) (1997) 138–154.[62] C.A. Reed, D.N. Walton, Towards a formal and implemented model of argumentation schemes in agent communication, in: Argumentation in Multi-Agent Systems (Proc. of ArgMAS 2004), New York, in: LNAI, vol. 3366, Springer-Verlag, 2004, pp. 19–30.[63] R. Reiter, A logic for default reasoning, Artificial Intelligence 13 (1–2) (1980) 81–132.[64] J. Rosenschein, G. Zlotkin, Rules of Encounter: Designing Conventions for Automated Negotiation Among Computers, MIT Press, Cambridge, MA, 1994.[65] A. Ross, Imperatives and logic, Theoria 7 (1941) 53–71.[66] F. Sadri, F. Toni, P. Torroni, Abductive logic programming architecture for negotiating agents, in: Proc. of the 8th European Conference on Logics inArtificial Intelligence (JELIA’02), in: LNCS, vol. 2424, Springer-Verlag, Germany, 2002, pp. 419–431.[67] T.W. Sandholm, V.R. Lesser, Advantages of a leveled commitment contracting protocol, in: Proc. of the 13th National Conference on Artificial Intelligence(AAAI’96), Portland, OR, 1996, pp. 126–133.[68] D.B. Shmoys, E. Tardos, K. Aardal, Approximation algorithms for facility location problems (extended abstract), in: Proc. of the 29th annual ACMsymposium on Theory of computing (STOC’97), El Paso, TX, 1997, pp. 265–274.[69] C. Sierra, N.R. Jennings, P. Noriega, S. Parsons, A framework for argumentation-based negotiation, in: Proc. of 4th International Workshop on AgentTheories Architectures and Languages (ATAL’97), Rhode Island, USA, 1998, pp. 167–182.[70] M.P. Singh, Social and psychological commitments in multiagent systems, in: AAAI Fall Symposium on Knowledge and Action at Social and Organiza-tional Levels, Monterey, CA, 1991, pp. 104–106.[71] M.P. Singh, M.N. Huhns (Eds.), Service-Oriented Computing: Semantics, Processes, Agents, John Wiley & Sons, Ltd., 2005.[72] K. Sycara, Persuasive argumentation in negotiation, Theory and Decision 28 (3) (1990) 203–242.[73] C. Tessier, L. Chaudron, H.-J. Müller (Eds.), Conflicting Agents Conflict Management in Multi-Agent Systems, Kluwer Academic Publishers, Dordrecht,The Netherlands, 2000, pp. 1–30, Chapter: Agents’ conflicts: New issues.[74] R.H. Thomason, Desires and defaults: A framework for planning with inferred goals, in: Principles of Knowledge Representation and Reasoning, 2000,pp. 702–713.[75] S. Toulmin, The Uses of Argument, Cambridge University Press, Cambridge, UK, 1958.[76] L. van der Torre, Y.-H. Tan, Contrary-to-duty reasoning with preference-based dyadic obligations, Annals of Mathematics and ArtificialIntelli-gence 27 (1–4) (1999) 49–78.[77] F.H. van Eemeren, R. Grootendorst, Argumentation, Communication, and Fallacies, Lawrence Erlbaum Associates, Inc., Hillsdale, NJ, 1992.[78] D. Walton, Justification of argument schemes, The Australasian Journal of Logic 3 (1–13) (2005).[79] D.N. Walton, Argumentation Schemes for Presumptive Reasoning, Erlbaum, Mahwah, NJ, 1996.[80] D.N. Walton, E.C.W. Krabbe, Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning, SUNY Press, Albany, NY, 1995.[81] M.J. Wooldridge, An Introduction to MultiAgent Systems, John Wiley & Sons, Chichester, England, 2002.[82] L.R. Ye, P.E. Johnson, The impact of explanation facilities on user acceptance of expert systems advice, MIS Quarterly 19 (2) (1995) 157–172.