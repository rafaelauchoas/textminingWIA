Artificial Intelligence 171 (2007) 429–433www.elsevier.com/locate/artintThe possible and the impossible in multi-agent learningH. Peyton Young a,b,ca Johns Hopkins University, USAb University of Oxford, UKc The Brookings Institution, USAReceived 16 May 2006; received in revised form 20 October 2006; accepted 20 October 2006Available online 7 February 2007AbstractThis paper surveys recent work on learning in games and delineates the boundary between forms of learning that lead to Nashequilibrium, and forms that lead to weaker notions of equilibrium or to none at all.© 2007 Elsevier B.V. All rights reserved.Keywords: Equilibrium; Learning; DynamicsInteractive learning is inherently more complex than single-agent learning, because the act of learning changes thething to be learned. If agent A is trying to learn about agent B, A’s behavior will naturally depend on what she haslearned so far, and also on what she hopes to learn next. But A’s behavior can be observed by B, hence B’s behaviormay change as a result of A’s attempts to learn it. The same holds for B’s attempts to learn about A.This feedback loop is a central and inescapable feature of multi-agent learning situations. It suggests that methodswhich work for single-agent learning problems may fail in multi-agent settings. It even suggests that learning couldfail in general, that is, there may exist situations in which no rules allow players to learn one another’s behavior in acompletely satisfactory sense. This turns out to be the case: in the next section I formulate an uncertainty principlefor strategic interactions which states that if there is enough ex ante uncertainty about the other players’ payoffs (andtherefore their potential behaviors), there is no way that rational players can learn to predict one another’s behavior,even over an infinite number of repetitions of the game ([5]; for earlier results in the same spirit see [1] and [13,14]).Admittedly this and related impossibility theorems rest on very demanding assumptions about agents’ rationality,and what it means for them to “learn” their opponents’ behavior. Under less restrictive conditions more positive resultscan be attained, as we shall see in Section 3. Thus the purpose of this note is not to claim that multi-agent learningis impossibly difficult, but to try to identify the boundary—insofar as we now know it—between the possible and theimpossible in multi-agent learning situations. These issues are discussed in greater depth in [18].1. Model-based learningThe accompanying perspectives paper by Shoham, Powers and Grenager [17], hereafter referred to as SPG, formsmy jumping-off point. They too draw attention to the fact that multi-agent learning is inherently more complex thanE-mail address: pyoung@jhu.edu.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.10.015430H.P. Young / Artificial Intelligence 171 (2007) 429–433single-agent learning. They also make a useful distinction between model-based and model-free forms of learning,which I shall follow here. Using essentially their language, a model-based learning scheme has the following elements:1. Start with a model of the opponent’s strategy.2. Compute and play a best [or almost best] response.3. Observe the opponent’s play and update your model.4. Goto step 2.SPG leave the concept of “model” open, but here I shall suggest a general definition. Namely, a model-based learningmethod is a function that maps any history of play into a prediction about what one’s opponents will do next period,that is, to a probability distribution over the opponents’ actions conditional on the history so far. This definitionencompasses many forms of pattern recognition. The key feature of a model-based learning rule, however, is not whatpatterns it is able to identify in the data, but how it uses these patterns to forecast the opponents’ next moves.Many game-theoretic learning methods fall into this category. Fictitious play is a simple example: each agentpredicts that his opponent will use the distribution next period that he used cumulatively up until now. More gener-ally, Bayesian updating is a model-based learning procedure: each agent updates his beliefs about the repeated-gamestrategy of the opponents (conditional on the observed history), which leads to a prediction of their behavior nextperiod.What exactly do we mean by “learning” in this context? A natural definition is that players “learn” if they eventuallysucceed in predicting their opponents’ behavior with a high degree of accuracy [5]. This idea can be given greaterprecision as follows. Suppose that you are engaged in a two-player game. Given a history ht to time t, let pt beyour prediction of the opponent’s next-period behavior, conditional on ht . Let qt be your opponent’s actual intendedbehavior next period, conditional on ht . Notice that both pt and qt are probability distributions over the opponent’saction space (which we assume is finite). Thus pt and qt lie in an m-dimensional simplex for some nonnegative integerm. The predictive error in period t is (cid:2)pt − qt (cid:2). We could say that you learn to predict if (cid:2)pt − qt (cid:2) → 0 almost surely(cid:2)ps − qs(cid:2)2 → 0as t → ∞. A less demanding definition would be that the mean square error goes to zero: (1/t)almost surely as t → ∞. We shall say that the former is learning to predict in the strong sense and the latter is learningto predict in the weak sense.s(cid:2)t(cid:2)There is a well-known condition in statistics that guarantees that all players will learn to predict in the strong sense.Namely, it suffices that each player’s forecast of the others’ behavior, conditional on his own behavior, never excludeevents that have positive probability under their actual joint behavior. This is the absolute continuity condition [2,15].So far we have said nothing about what determines agents’ behavior, only what it means for them to learn. Ingame theory, a standard assumption is that behavior is rational: at each point in time, given what has happened todate, the players’ behavioral strategies are optimal given their forecasts of what is going to happen at all future dates.If we combine rationality with the absolute continuity condition (which guarantees good prediction), then we getconvergence to Nash equilibrium along the play path [15].Suppose, however, that each player is ignorant of his opponent’s payoff function. If the opponent is rational, hisstrategy will depend—perhaps quite intricately—on what his payoffs are. Hence the first player will have difficultyforecasting the second player’s strategy unless he can gather enough information along the play path to deduce whatthe latter is optimizing. The same holds for the second player trying to forecast the behavior of the first. This turns outto be impossible in principle when there is enough ex ante uncertainty about the payoffs.Theorem 1. (See [5].) Consider an n-person game on a finite joint action space A, where the n|A| possible payoffsdefining G are drawn i.i.d. via a continuous density f that is bounded away from zero on an open interval. G isdetermined once and for all before play begins. Assume the players are forward-looking and rational, with discountfactors less than unity, they know their own realized payoffs, and they use forecasting rules that do not depend on theopponents’ realized payoffs.There is a positive probability that: (i) at least one of the players will not learn to predict even in the weak sense;and (ii) the players’ period-by-period behaviors do not converge to any Nash equilibrium of the repeated game.Furthermore, if the support of f is a sufficiently small interval, then conclusions (i) and (ii) hold with probabilityone.H.P. Young / Artificial Intelligence 171 (2007) 429–433431A consequence of this result is that there exist no general, model-based procedures for multi-agent learning whenplayers are perfectly rational and they have sufficiently incomplete knowledge of their opponents’ payoff functions.A crucial condition for Theorem 1 to hold is that the unknown payoffs are distributed over some interval. If insteadthey were known to lie in a finite set, or even in a countable set, the result can fail. In this case one can tailor theforecasting rules to take account of the restricted set of payoffs that the opponent could be using, and thereby satisfyabsolute continuity. The second crucial condition for Theorem 1 is rationality: agents must optimize exactly. If insteadagents almost optimize, as in smoothed fictitious play [8], the result does not necessarily hold.In my view the first of these conditions (lack of knowledge) is more important than the second (perfect rationality).For one thing the second condition is merely an ideal statement about behavior, there is little or no empirical supportfor the notion that subjects optimize exactly. By contrast the first condition seems quite realistic: a player can hardlybe expected to know the von Neumann Morgenstern payoffs of his opponent with any precision; surely the most thatcan be hoped for is that he knows they lie within some range.I now sketch a model-based, multi-agent learning method that gets around the preceding impossibility result byrelaxing rationality a bit, while maintaining the assumption about complete lack of knowledge. The method is struc-tured along the lines of statistical hypothesis testing [6]. Assume, for the moment, that there are two players, 1 and2, with finite action spaces A1 and A2. Let Δi be the simplex of probability distributions on Ai . At time t, agent 1’smodel is that agent 2 is going to play a fixed distribution p2t ∈ Δ2 in all future periods. Given this model, agent 1chooses a smoothed best response q1t ∈ Δ1. Similarly, agent 2’s model at time t is some p1t ∈ Δ1 and her smoothedbest response is q2t ∈ Δ2. Hypothesis testing takes the following form for each player. Let s be a large positive integer(the sample size) and let τ be a small positive real number (the tolerance level).Hypothesis testing1. Select a model p of the opponent’s strategy uniformly at random.2. Play a smoothed best response q to the current model p.3. Start a test phase with probability 1/s.4. Once a test phase begins, compute the opponent’s empirical frequency distribution p(cid:6) over the next s periods. If(cid:2)p(cid:6) − p(cid:2) exceeds τ go to step 1; if (cid:2)p(cid:6) − p(cid:2) does not exceed τ go to step 2.It can be shown that, given any game G on A1 × A2 and any ε > 0, if s is large enough and τ is small enough theplayers’ behaviors constitute an ε-equilibrium of G in at least 1 − ε of all periods. Further, if the players graduallyincrease s and decrease τ , we obtain the following.Theorem 2. (See [6].) Given any n-person game G on a finite action space A, if the hypothesis testing parametersare annealed sufficiently slowly, the players’ period-by-period behaviors converge in probability to the set of Nashequilibria of G.2. Model-free learningThe second class of learning rules identified by SPG do not rely on prediction of the opponent’s behavior, but onsome form of heuristic adjustment to previous experience. Unfortunately, in this setting it is not so clear what is meantby “learning”. Players are obviously not learning to predict, because they are not predicting. SPG suggest that, inanalogy with single-agent Markov decision problems (MDP’s), we could say that agents “learn” if their adaptive ruleslead to high average payoffs. (In a single-agent context such rules are said to be “effective”.)The difficulty is that, whereas high payoffs are well-defined in single-agent MDP’s, they are usually not well-defined in games. In this setting, agents can only optimize given what the other agents are doing. Of course, wheneveryone optimizes conditional on the others optimizing, the players are in some form of Nash equilibrium, either withrespect to the stage game or the repeated game. This suggests one possible definition of “learning” in a model-freeenvironment, namely, that agents’ average payoffs converge to the payoffs corresponding to some Nash equilibrium.Alternatively, we might say that they learn if their behaviors come into Nash equilibrium more or less by accident:they act as if they were predicting and optimizing, even though they are actually using nonpredictive methods. A third432H.P. Young / Artificial Intelligence 171 (2007) 429–433possibility is that Nash equilibrium is not the appropriate solution concept in this setting. We summarize these possi-bilities as follows.Learning criteria in model-free environmentsI. Payoffs converge to Nash equilibrium payoffs.II. Behaviors converge to Nash equilibrium.III. Behaviors and/or payoffs converge to a subset that has some other normative interpretation, even though it maynot correspond to a Nash equilibrium.Let us consider these in turn. The first criterion is very easy to satisfy if by “Nash equilibrium payoffs” we meanpayoffs in some repeated-game Nash equilibrium. The reason is that, by the Folk Theorem, virtually all payoff com-binations can be realized in a repeated-game equilibrium, provided that each player gets at least his maximin payoff.Many adaptive model-free procedures can achieve the same thing.The second criterion is much more difficult to achieve. Indeed, until recently it was not known whether there existany model-free learning rules that cause behaviors to converge to Nash equilibrium, except in special cases. Here isone example of a model-free rule, called regret testing, that achieves criterion II for any finite two-person game. (Thisis a simplified version of the rule treated in [7].) Let Δd be the set of all probability mixtures on the agent’s actionsthat can be expressed in d or fewer decimal places.Regret testing1. Choose q ∈ Δd uniformly at random.2. Play q for s periods in succession.3. For each action a, compute the regret r(a) from not having played action a over these s periods.4. If maxa r(a) > τ , go to step 1; otherwise retain the current q and go to step 2.Given any two-person game G and any ε > 0, if both players use regret testing with sufficiently large s and d andsufficiently small τ , their behaviors constitute an ε-equilibrium of G in at least 1 − ε of all play periods [7]. Byannealing the parameters sufficiently slowly one can obtain convergence in probability to the set of Nash equilibriafor any finite two-person game G. With some further modifications almost-sure convergence is achievable [9].A crucial feature of both regret testing and hypothesis testing is the random search that occurs whenever a “test”fails. What happens if we drop this aspect of the learning process? To be quite general, let G be an n-person gamewith finite action space A. Let s be a positive integer and let the state of the learning process be the last s plays of thegame. Thus there are |A|s states. A learning rule for i, say fi(z), maps each state z to a probability distribution overi’s actions next period. The rule fi is uncoupled if it does not depend on the opponents’ payoffs [12].Theorem 3. (See [12].) Given a finite action space A and positive integer s, there exist no uncoupled rules fi(z)whose state variable z is the last s plays, such that, for every game G on A, the period-by-period behaviors convergealmost surely to a Nash equilibrium of G, or even to an ε-equilibrium of G, for all sufficiently small ε > 0.Note that regret testing is uncoupled, yet we claimed earlier that an annealed version of it converges almost surelyto the set of Nash equilibria for any finite two-person game G. This does not contradict Theorem 3, however, becausein the annealed version the value of s grows, hence the state variable does not consist of histories of bounded length.Furthermore, even in the non-annealed version (where s is fixed), the state variable consists of more than the lasts plays of the game; it also includes the realization of a random variable, namely, the new choice of q that occurswhenever someone’s regret exceeds his tolerance level τ . Hence it does not satisfy the conditions of Theorem 3.(Hypothesis testing does not satisfy the state variable requirement for the same reason.) This illustrates the rather fineline that separates the possible from the impossible in multi-agent learning theory.We conclude with an example that illustrates why Nash equilibrium is not the only way of evaluating whetheragents are “learning” (criterion III). Consider the following simple adaptive procedure first proposed by Hart andMas-Colell [10,11].H.P. Young / Artificial Intelligence 171 (2007) 429–433433Unconditional regret matching1. Choose an action uniformly at random.2. For each action a ∈ A, compute the regret r(a) from not having played a in all previous periods.3. Among all those actions with positive regrets, choose among them with probabilities proportional to their regrets,then go to step 2; if there are no such actions go to step 1.When a given player uses this rule in a finite game G, his regrets r(a) become nonpositive almost surely no matter whatthe other players do [10]. When all players use the rule, the empirical distribution of their joint behaviors convergesalmost surely to a convex set that contains all of the correlated equilibria of G, and therefore all of the Nash equilibria.(Moreover the average payoffs converge to the set of expected payoffs generated by these distributions.) We shallcall this the coarse correlated equilibrium set [18].1 It has a simple equilibrium interpretation, namely, it is the setof all joint probability distributions φ on A such that each player’s expected payoff (under φ) is at least as high ashis expected payoff if he were to deviate and play an arbitrary action, while the other players adhere to the outcomeprescribed by φ. (In a correlated equilibrium, by contrast, no player wishes to deviate after his prescribed action viaφ is revealed, which is a more restrictive condition.)The significance of this solution concept is that it describes average behavior under a wide variety of adaptiverules, including smoothed fictitious play, calibrated forecasting with best responses [3,4], and a number of variants ofregret matching [10,11]. (Some of these rules actually converge to the set of correlated equilibria.) It seems reason-able to conjecture that this “coarse” notion of correlated equilibrium may prove useful in describing the behavior ofexperimental subjects, though to my knowledge this point has never been investigated systematically.I conclude that there is a fine line between the possible and the impossible in multi-agent learning situations. Itdepends on subtle differences in assumptions about the amount of information that agents have, the extent to whichthey optimize, the desired form of convergence, and the target set. In the preceding I have identified some prominentlandmarks on either side of the dividing line. Its precise course remains to be charted.AcknowledgementI am grateful to Dean Foster and Rakesh Vohra for helpful comments on an earlier version.References[1] K. Binmore, Modelling rational players (part I), Economics and Philosophy 3 (1987) 179–214.[2] D. Blackwell, L. Dubins, Merging of opinions with increasing information, Annals of Mathematical Statistics 38 (1962) 882–886.[3] D.P. Foster, R. Vohra, Calibrated learning and correlated equilibrium, Games and Economic Behavior 21 (1997) 40–55.[4] D.P. Foster, R. Vohra, Regret in the on-line decision problem, Games and Economic Behavior 29 (1999) 7–35.[5] D.P. Foster, H.P. Young, On the impossibility of predicting the behavior of rational agents, Proceedings of the National Academy of Sciencesof the USA 98 (222) (2001) 12848–12853.[6] D.P. Foster, H.P. Young, Learning, hypothesis testing, and Nash equilibrium, Games and Economic Behavior 45 (2003) 73–96.[7] D.P. Foster, H.P. Young, Regret testing: learning to play Nash equilibrium without knowing you have an opponent, Theoretical Economics 1(2006) 341–367.[8] D. Fudenberg, D. Levine, The Theory of Learning in Games, MIT Press, Cambridge, MA, 1998.[9] F. Germano, G. Lugosi, Global convergence of Foster and Young’s regret testing, Games and Economic Behavior (2007), in press.[10] S. Hart, A. Mas-Colell, A simple adaptive procedure leading to correlated equilibrium, Econometrica 68 (2001) 1127–1150.[11] S. Hart, A. Mas-Colell, A general class of adaptive strategies, Journal of Economic Theory 98 (2001) 26–54.[12] S. Hart, A. Mas-Colell, Stochastic uncoupled dynamics and Nash equilibrium, Games and Economic Behavior 57 (2006) 286–303.[13] J.S. Jordan, Bayesian learning in normal form games, Games and Economic Behavior 3 (1991) 60–91.[14] J.S. Jordan, Three problems in learning mixed-strategy equilibria, Games and Economic Behavior 5 (1993) 368–386.[15] E. Kalai, E. Lehrer, Rational learning leads to Nash equilibrium, Econometrica 61 (1993) 1019–1045.[16] H. Moulin, J.P. Vial, Strategically zero-sum games: the class of games whose completely mixed equilibria cannot be improved upon, Interna-tional Journal of Game Theory 7 (1978) 201–221.[17] Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365–377,this issue.[18] H.P. Young, Strategic Learning and Its Limits, Oxford University Press, Oxford, 2004.1 It was first defined, though not named, in an early paper by Moulin and Vial [16]; Hart and Mas-Colell [11] call it the Hannan set.