Artificial Intelligence 148 (2003) 335–383www.elsevier.com/locate/artintPossibilistic instance-based learningEyke HüllermeierDepartment of Mathematics and Computer Science, University of Marburg, Marburg 35032, GermanyReceived 16 July 2001; received in revised form 9 August 2002AbstractA method of instance-based learning is introduced which makes use of possibility theory andfuzzy sets. Particularly, a possibilistic version of the similarity-guided extrapolation principleunderlying the instance-based learning paradigm is proposed. This version is compared to thecommonly used probabilistic approach from a methodological point of view. Moreover, aspects ofknowledge representation such as the modeling of uncertainty are discussed. Taking the possibilisticextrapolation principle as a point of departure, an instance-based learning procedure is outlined whichincludes the handling of incomplete information, methods for reducing storage requirements andthe adaptation of the influence of stored cases according to their typicality. First theoretical andexperimental results showing the efficiency of possibilistic instance-based learning are presented aswell. 2003 Elsevier B.V. All rights reserved.Keywords: Possibility theory; Fuzzy set theory; Machine learning; Instance-based learning; Nearest neighborclassification; Probability1. IntroductionA major theme in machine learning concerns the problem of induction, that is thecreation of general knowledge from particular examples or observed data. In this respect,uncertainty plays a fundamental role. To begin with, the data presented to learningalgorithms is imprecise, incomplete or noisy most of the time, a problem that can badlymislead a learning procedure. But even if observations were perfect, the generalizationbeyond that data would still be afflicted with uncertainty. For example, observed data cangenerally be explained by more than one candidate theory, which means that one can neverE-mail address: eyke@mathematik.uni-marburg.de (E. Hüllermeier).0004-3702/$ – see front matter  2003 Elsevier B.V. All rights reserved.doi:10.1016/S0004-3702(03)00019-5336E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383be sure of the truth of a particular theory. Consequently, inductive reasoning—by its verynature—is inseparably connected with uncertainty [13].In fact, the insight that inductive inference can never produce ultimate truth can betraced back at least as far as Francis Bacon’s epistemology. In his Novum Organum,1 Baconadvocates a gradualist conception of inductive enquiry and proposes to set up degrees ofcertainty. Thus, from experience in the form of given data, one may at best conclude that atheory is likely to be true—not, however, that it is true with certainty. In machine learningand mathematical statistics, uncertainty of this type is generally handled by means ofprobabilistic methods. In Bayesian approaches, for example, an inference result is usuallygiven in the form of a probability distribution over the space of candidate models, that is,each model (theory) is assigned a degree of probability.In this paper, our interest concentrates on possibility theory [29] as an alternativecalculus for modeling and processing uncertainty or, more generally, partial belief.By using possibility theory for handling uncertainty in learning procedures, inductivereasoning becomes possibilistic in the sense that certain generalizations are declared moreor less plausible. In this paper, we shall employ possibility theory in the context of instance-based learning (IBL), a special approach to (supervised) machine learning. IBL relies ona kind of extrapolation principle2 expressing a commonsense rule already suggested byDavid Hume:3 “In reality, all arguments from experience are founded on the similarity,which we discover among natural objects, and by which we are induced to expect effectssimilar to those, which we have found to follow from such objects. . . . From causes,which appear similar, we expect similar effects. This is the sum of all our experimentalconclusions.” Thus, HUME suggests to extrapolate properties of one object to propertiesof similar ones. The idea of possibilistic induction, combined with this extrapolationprinciple, leads to the following inference pattern: The more similar two causes are, themore plausible it is that they have the same effects. Since possibility theory (in conjunctionwith fuzzy set theory) establishes a close connection between the concepts of similarity anduncertainty, it provides an excellent framework for translating this principle into a formalinference procedure.This paper complements recent work on the use of possibility theory and fuzzy sets ininstance-based reasoning [25–27]. The latter is more concerned with extending IBL bymeans of fuzzy set-based modeling techniques, whereas here the focus is on the learningprocess itself. More specifically, we introduce a method of possibilistic IBL, referred toas POSSIBL, which implements the above-mentioned inference pattern. Together, the twoframeworks yield a powerful methodology of instance-based reasoning in which possibilitytheory and fuzzy set-based modeling are used, respectively, for representing gradationof uncertainty and evidential support and for complementing the data-driven inferenceprocedure by means of domain-specific expert knowledge.By way of background, Section 2 recalls some important ideas of possibility theory andSection 3 gives a brief review of instance-based learning and the NEAREST NEIGHBORprinciple upon which it is based. Besides, the aspect of uncertainty in IBL is discussed1 Published in 1620.2 IBL does actually not realize induction proper, as will be discussed later.3 See, e.g., [45, p. 116].E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383337in this section. In Section 4, a possibilistic extrapolation principle is introduced andcompared to other principles commonly used in instance-based learning. Proceeding fromthis extrapolation principle, a method of possibilistic instance-based learning is developedin Section 5. Finally, Section 6 presents experimental studies. The paper concludes with asummary in Section 7.2. Background on possibility theoryIn this section, we recall some basic concepts from possibility theory, as far as requiredfor the current paper. Possibility theory deals with “degrees of possibility”. The term“possibility” is hence employed as a graded notion, much in the same way as theterm “probability”. At first sight, this might strike as odd since “possibility” is usuallyconsidered a two-valued concept in natural language (something is possible or not).Before turning to more technical aspects, let us therefore make some brief remarks onthe semantics underlying the notion of “possibility” as used in possibility theory.Just as the concept of probability, the notion of possibility can have different semanticmeanings. To begin with, it can be used in the (physical) sense of a “degree of ease”. Onemight say, for instance, that it is more possible for Hans to have two eggs for breakfastthan eight eggs, simply because eating two eggs is more easy (feasible, practicable)than eating eight eggs [82]. However, as concerns the use in most applications, andin this paper in particular, possibility theory is considered as a means for representinguncertain knowledge, that means, for characterizing the epistemic state of an agent. Forinstance, given the information that Hans has eaten many eggs, one is clearly uncertainabout the precise number. Still, three eggs appears somewhat more plausible (possible)than two eggs, since three is more compatible with the linguistic quantifier “many” thantwo.It is important to note that a degree of possibility, as opposed to a degree of probability,is not necessarily a number. In fact, for many applications it is sufficient, and often evenmore suitable, to assume a qualitative (ordinal) scale with possibility degrees rangingfrom, e.g., “not at all” and “hardly” to “fairly” and “completely” [33,52]. Still, possibilitydegrees can also be measured on the cardinal scale [0, 1], again with different semanticinterpretations. For example, possibility theory can be related to probability theory, inwhich case a possibility degree can specify, e.g., an upper probability bound [31]. Forconvenience, possibility degrees are often coded by numbers from the unit interval evenwithin the qualitative framework of possibility theory.As a means of representing uncertain knowledge, possibility theory makes a distinctionbetween the concepts of the certainty and the plausibility of an event. As opposed toprobability theory, possibility theory does not claim that the confidence in an eventis determined by the confidence in the complement of that event and, consequently,involves non-additive measures of uncertainty. Taking the existence of two quite oppositebut complementary types of knowledge representation and information processing intoaccount, two different versions of possibility theory will be outlined in the following. Fora closer discussion refer to [34] and [24].338E. Hüllermeier / Artificial Intelligence 148 (2003) 335–3832.1. Possibility distributions as generalized constraintsA key idea of possibility theory as originally introduced by Zadeh [82] is to considera piece of knowledge as a (generalized) constraint that excludes some “world states”(to some extent). Let Ω be a set of worlds conceivable by an agent, including the “trueworld” ω0. With (incomplete) knowledge K about the true world one can then associate apossibility measure ΠK such that ΠK(A) measures the compatibility of K with the event(set of worlds) A ⊆ Ω, i.e., with the proposition that ω0 ∈ A. Particularly, ΠK(A) becomessmall if K excludes each world ω ∈ A and large if at least one of the worlds ω ∈ A iscompatible with K. More specifically, the finding that A is incompatible with K to somedegree corresponds to a statement of the form ΠK(A) (cid:1) p, where p is a possibility degreetaken from an underlying possibility scale P .The basic informational principle underlying the possibilistic approach to knowledgerepresentation and reasoning is stated as a principle of minimal specificity:4 In orderto avoid any unjustified conclusions, one should represent a piece of knowledge K bythe largest possibility measure among those measures compatible with K, which meansthat the inequality above is turned into an equality: ΠK(A) = p. Particularly, completeignorance should be modeled by the measure Π ≡ 1.Knowledge K is usually expressed in terms of a possibility distribution πK, a mappingΩ → P related to the associated measure ΠK through ΠK(A) = supω∈A πK(ω). Thus,πK(ω) is the degree to which world ω is compatible with K.Apart from the boundary conditions ΠK(Ω) = 1 (at least one world is fully possible)and ΠK(∅) = 0, the basic axiom underlying possibility theory after Zadeh involves themaximum-operator:(cid:1)ΠK(A ∪ B) = maxΠK(A), ΠK(B)(cid:2).(1)In plain words, the possibility (or, more precisely, the upper possibility-bound) of the unionof two events A and B is the maximum of the respective possibilities (possibility-bounds)of the individual events.As constraints are naturally combined in a conjunctive way, the possibility measuresassociated with two pieces of knowledge, K1 and K2, are combined by using the minimum-operator:(cid:1)πK1∧K2(A) = minπK1(A), πK2(A)(cid:2)for all A ⊆ Ω. Note that πK1∧K2(Ω) < 1 indicates that K1 and K2 are not fully compatible,i.e., that K1 ∧ K2 is contradictory to some extent.The distinction between possibility and certainty of an event is reflected by the existenceof a so-called necessity measure NK that is dual to the possibility measure ΠK. Moreprecisely, the relation between these two measures is given by NK(A) = 1 − ΠK(Ω \ A)for all A ⊆ Ω:5 An event A is necessary in so far as its complement (logical negation) isnot possible.4 This principle plays a role quite comparable to the maximum entropy principle in probability theory.5 If the possibility scale P is not the unit interval [0, 1], the mapping 1 − (·) has to be replaced by an order-reversing mapping of P .E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383339Worth mentioning is the close relationship between possibility theory and fuzzy sets. Infact, the idea of Zadeh [82] was to induce a possibility distribution from knowledge statedin the form of vague linguistic information and represented by a fuzzy set. Formally, hepostulated that πK(ω) = µF (ω), where µF is the membership function of a fuzzy set F .To emphasize that ω plays different roles on the two sides of the equality, the latter mightbe written more explicitly as πK(ω | F ) = µ(F | ω): Given the knowledge K that ω isan element of the fuzzy set F , the possibility that ω0 = ω is evaluated by the degree towhich the fuzzy concept (modeled by) F is satisfied by ω. To illustrate, suppose that worldstates are just integer numbers. The uncertainty related to the vague statement that “ω0 isa small integer” (ω0 is an element of the fuzzy set F of small integers) might be translatedinto a possibility distribution that lets ω0 = 1 appear fully plausible (µF (1) = 1), whereas,say, 5 is regarded as only more or less plausible (µF (5) = 1/2) and 10 as impossible(µF (10) = 0).2.2. Possibility as evidential supportPossibility theory as outlined above provides the basis of a generalized approach toconstraint propagation, where constraints are expressed in terms of possibility distributions(fuzzy sets) rather than ordinary sets (which correspond to the special case of {0, 1}-valued possibility measures). A constraint usually corresponds to a piece of knowledgethat excludes certain alternatives as being impossible (to some extent). This “knowledge-driven” view of reasoning is complemented by a, say, “data-driven” view that leads to adifferent type of possibilistic calculus. According to this view, the statement that “ω ispossible” is not intended to mean that ω is provisionally accepted in the sense of not beingexcluded by some constraining piece of information, but rather that ω is indeed supportedor, say, confirmed by already observed facts (in the form of examples or data).To distinguish the two meanings of a possibility degree, we shall denote a degree ofevidential support or confirmation of ω by δ(ω),6 whereas π(ω) denotes a degree ofcompatibility.To illustrate, suppose that the values a variable V can assume are a subset of V ={1, 2, . . . , 10} and that we are interested in inferring which values are possible and whichare not. In agreement with the example-based (data-oriented) view, we have δ(v) = 1 assoon as the instantiation V = v has indeed been observed and δ(v) = 0 otherwise. Theknowledge-driven approach can actually not exploit such examples, since an observationV = v does not exclude the possibility that V can also assume any other value v(cid:17) (cid:18)= v. Ascan be seen, the data-driven and the knowledge-driven approach are intended, respectively,for expressing positive and negative evidence. As examples do express positive evidence,they do never change the distribution π ≡ 1. This distribution would only be changed ifwe knew from some other information source, e.g., that V can only take values v (cid:2) 6, inwhich case π(v) = 1 for v (cid:2) 6 and π(v) = 0 for v (cid:1) 5.The distinction between modeling positive and negative evidence becomes especiallyclear when it comes to expressing complete ignorance. As already mentioned above, this6 In [75], this type of distribution is called σ -distribution.340E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383situation is adequately captured by the possibility distribution π ≡ 1: If nothing is known,there is no reason to exclude any of the worlds ω, hence each of them remains completelypossible. At the same time, complete ignorance is modeled by the distribution δ ≡ 0. Thelatter does simply express that none of the worlds ω is actually supported by observed data.Within the context of modeling evidential support, possibilistic reasoning accompaniesa process of data accumulation. Each observed fact, φ, guarantees a certain degree ofpossibility of some world state ω, as expressed by an inequality of the form δφ(ω) (cid:2) d. Thebasic informational principle is now a principle of maximal informativeness that suggestsadopting the smallest distribution among those compatible with the given data and, hence,to turn the above inequality into an equality. The accumulation of observations φ1 and φ2is realized by deriving a distribution that is pointwise defined by(cid:1)δφ1∧φ2 (ω) = max(cid:2)δφ1(ω), δφ2(ω).As can be seen, adding new information has quite an opposite effect in connection withthe two types of possibilistic reasoning: In connection with the knowledge-driven orconstraint-based approach, a new constraint can only reduce possibility degrees, whichmeans turning the current distribution π into a smaller distribution π (cid:17) (cid:1) π . In connectionwith the data-driven or example-based approach, new data can only increase (lower boundsto) degrees of possibility.Closely related to the view of possibility as evidential support is a set-function thatwas introduced in [30], called measure of “guaranteed possibility”: ∆(A) is the degree towhich all worlds ω ∈ A are possible, whereas an event A is possible in the sense of theusual measure of “potential possibility”, namely Π(A) as discussed above, if at least oneω ∈ A is possible.7 For the measure ∆, the characteristic property (1) becomes(cid:1)∆(A ∪ B) = min∆(A), ∆(B)(cid:2).3. Instance-based learningIn recent years, several variants of instance-based approaches to (supervised) machinelearning have been devised, such as, e.g., memory-based learning [70], exemplar-basedlearning [64], or case-based reasoning [50]. Though emphasizing slightly different aspects,all of these approaches are founded on the concept of an instance or a case as a basisfor knowledge representation and reasoning. A case (observation, example, . . . ) can bethought of as a single experience, such as a pattern (along with its classification) in patternrecognition or a problem (along with a solution) in case-based reasoning. To highlight themain characteristics of IBL it is useful to contrast it with model-based learning.8Typically, IBL methods learn by simply storing (some of) the observed examples.They defer the processing of these inputs until a prediction (or some other type ofquery) is actually requested, a property which qualifies them as lazy learning methods [3].7 The latter semantics is clearly in line with the measure-theoretic approach underlying probability theory.8 Needless to say, there is no clear borderline between the two approaches. In fact, several learning techniquesfall in-between (e.g., [22]) or combine concepts of both (e.g., [62]).E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383341Predictions are then derived by combining the information provided by the stored examplesin some way or other. After the query has been answered, the prediction itself andany intermediate results are discarded. As opposed to this, model-based or inductiveapproaches derive predictions in an indirect way: First, the observed data is used in order toinduce a model, say, a decision tree or a regression function. Predictions are then obtainedon the basis of this model (which can also serve other purposes such as explaining). Asopposed to lazy learners, inductive methods are eager in the sense that they greedilycompile their inputs into an intensional description (model) and then discard the inputs.In general, eager (model-based) algorithms have higher computational costs during thetraining phase than lazy (instance-based) methods where learning basically amounts tostoring (selected) examples. On the other hand, lazy methods often have greater storagerequirements, typically linear in the size of the data set, and higher computational costswhen it comes to deriving a prediction.Model-based learning is in line with parametric methods in (classical) statistics,whereas instance-based approaches to machine learning share important features withnon-parametric statistics, such as, e.g., kernel smoothing techniques [74]. It deservesmentioning, however, that instance-based methods are not necessarily non-parametric [77].Besides, the lazy learning paradigm is naturally related to what is called transductiveinference in statistical learning theory [73]. Transductive inference is inference “fromspecific to specific”. Thus, it stands for the problem of estimating some values of a functiondirectly, given a set of empirical data. Instead of transductive inference we shall alsoemploy the less pompous term “extrapolation” to denote this process: The known valuesof a function are extrapolated—in a locally restricted way—in order to estimate unknownvalues. This type of inference represents an alternative to the indirect (model-based)approach which estimates the complete functional relationship in a first step (induction)and evaluates this estimation at the points of interest afterwards (deduction).3.1. Nearest Neighbor classificationThe well-known NEAREST NEIGHBOR (NN) principle originated in the field of patternrecognition [16] and constitutes the core of the family of IBL algorithms. It provides asimple means to realize the aforementioned extrapolation of observed instances.Consider the following setting that will be used throughout the paper: X denotes theinstance space, where an instance corresponds to the description x of an object (usually inattribute-value form). X is endowed with a distance measure DX .9 L is a set of labels, and(cid:19)x, λx (cid:20) is called a labeled instance (or a case). In classification tasks, which are the focusof most IBL implementations, L is a finite (usually small) set {λ1, . . . , λm} comprised ofm classes. S denotes a sample that consists of n labeled instances (cid:19)xı, λxı(cid:20) (1 (cid:1) ı (cid:1) n).Finally, a new instance x0 ∈ X is given, whose label λx0 is to be estimated.In connection with the sample S, note that X × L corresponds to the set of potentialobservations. For each label λ ∈ L, let Cλ ⊆ X denote the set of instances x ∈ X such9 (X , DX ) is often supposed to be a metric space. From a practical point of view, it is usually enough toassume reflexivity and symmetry of DX .342E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383that (cid:19)x, λ(cid:20) can indeed be observed. Cλ is also referred to as a concept. For example, abicycle belongs to the concept “two-wheelers” whereas a car does not. Formally, we canassume an underlying population P of entities such that each element p ∈ P is mappedto a labeled instance (cid:19)x(p), λ(p)(cid:20) in a unique way. Thus, x is an element of Cλ or, say,(cid:19)x, λ(cid:20) is an existing instance if there is at least one p ∈ P such that (cid:19)x, λ(cid:20) = (cid:19)x(p), λ(p)(cid:20).Observe that the mapping p (cid:22)→ x(p) is not assumed to be injective (different elements ofP might have the same description), which means that concepts can overlap (Cλ ∩ Cλ(cid:17) (cid:18)= ∅for λ (cid:18)= λ(cid:17)).The NN principle prescribes to estimate the label of the yet unclassified point x0 bythe label of the closest sample point, i.e., the one which minimizes the distance to x0. Thek-NEAREST NEIGHBOR (kNN) approach is a slight generalization which takes the k > 1nearest neighbors of a new sample point x0 into account. That is, an estimation λestofx0λx0 is derived from the set Nk(x0) of the k nearest neighbors of x0, e.g., by means of themajority vote decision rule:λestx0= arg maxλ∈L(cid:2)(cid:1)x ∈ Nk(x0) | λx = λcard.(2)Not only can the NN principle be used for classification, it is also employable for realizinga (locally weighted) approximation of continuous-valued target functions. To this end, onereasonably computes the (weighted) mean of the k nearest neighbors of a new query pointinstead of returning the most common value.10The inductive bias11 underlying the NN principle corresponds to a representativenessor closeness assumption suggesting that similar (= closely located) instances have similar(or even the same) classification. This hypothesis, which gives rise to the similarity-guided extrapolation principle discussed in the introduction, is clearly of a heuristic nature.Still, theoretical properties of NN classification have been investigated thoroughly from astatistical perspective (e.g., [14]).12 In fact, the origin of the NN approach can be found inwork on non-parametric discriminatory analysis [38,39].Besides, several conceptual modifications and extensions, such as distance weighting,which is discussed below, have been considered. Particularly, (editing) methods forselecting optimal training samples to be stored in the memory have been developed in orderto improve classification performance [78] or to reduce computational complexity [41] orboth. Other extensions aim at supporting the determination of adequate metrics and theoptimal size of the neighborhood. Computational aspects have been addressed as well.For example, fast algorithms for finding nearest neighbors have been devised in order toimprove computational efficiency [40,49,81].10 Shephard’s interpolation method [67] can be considered as a special type of NN estimation.11 Roughly speaking, the inductive bias corresponds to the a priori assumptions on the identity of the model tobe learned. Without a biased angle of view, observed data is actually meaningless and generalization beyond thatdata impossible [56].12 Needless to say, corresponding results can only be derived under certain statistical assumptions on the settingof the problem.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–3833433.2. Uncertainty in NN classificationIn statistical estimation theory, an estimated quantity is always endowed with acharacterization of its reliability, usually in terms of a confidence measure and aconfidence region. Alternatively, an estimation is given directly in the form of a probabilitydistribution. As opposed to this, the NN principle in its basic form merely provides apoint-estimation or, say, a decision rule, but not an estimation in a statistical sense. Theneglecting of uncertainty makes this principle appear questionable in some situations [43].To illustrate, Fig. 1 shows two classification problems. The new instance x0 is representedby a cross, and dark and light circles correspond to instances of two different classes,respectively. In both cases, the kNN rule with k = 5 suggests DARK as a label for x0.As can be seen, however, this classification is everything but reliable: In the above setting,the proportion of dark and light examples is almost balanced (apart from that, the closestpoints are light). This is a situation of ambiguity. The setting below illustrates a problemof ignorance: It is true that all neighbors are dark, but even the closest among them areactually quite distant.A simple (yet drastic) step to handle this type of problem is to apply a reject option inthe form of a distance or frequency threshold. That is, a classification or answer to a queryis simply refused if the nearest neighbors are actually not close enough [15,36,72] or if themost frequent label among these neighbors is still not frequent enough [12,42].A second possibility is to equal statistical methods (especially Bayesian ones) inderiving a probability distribution as an inference result. In fact, this is an obvious ideasince NN techniques have originally been employed in the context of non-parametricdensity estimation [38,53]. Thus, a single decision can be replaced by an estimation inthe form of a probability vector(cid:3)(cid:4),px0(λ1), . . . , px0(λm)(3)where px0(λı ) = Pr(λı | x0) is the probability that λx0= λı , i.e., the conditional probabilityof the label λı given the instance x0. Taking the k nearest neighbors of x0 as a point ofdeparture, an intuitively reasonable approach is to specify the probability px0(λı ) by the.= kı/k,relative frequency of the label λı among the labels of these neighbors: px0(λı )where kı denotes the number of neighbors having label λı . In fact, this approach can alsobe justified theoretically, as will be shown in the following.The NEAREST NEIGHBOR approach to density estimation (not to be confused withthe one to classification) is closely related to kernel-based density estimation. An NNdensity estimator is a kernel estimator with variable kernel width [68]: The size of theFig. 1. Two situations of uncertainty in connection with the basic kNN rule, caused by the existence of more thanone frequent class label among the nearest neighbors (top) and the absence of any close neighbor (bottom).344E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383neighborhood of a point x0 is adapted so as to include exactly k observations. Thus,consider a sample of n observations x1, . . . , xn ∈ Rl which are realizations of an l-dimensional random vector X with probability density φ : Rl → R(cid:1)0. For x0 ∈ Rl let v bethe volume of the smallest sphere V (x0) around x0 that contains k of these observations.The relation(cid:3)X ∈ V (x0)(cid:4)Pr≈ φ(x0) · v(which holds true for small spheres) then suggests the following estimation of φ(x0), thedensity at point x0:φest(x0) = kn · v(4).Coming back to NN classification, consider a sample S that comprises n = n1 + · · · + nmobservations, where nı denotes the number of tuples (cid:19)x, λx (cid:20) ∈ S such that λx = λı . Letx0 be a new observation. Again, we choose an as small as possible hypersphere aroundx0 which contains a set Nk(x0) of k instances from S, where k = k1 + · · · + km withkı = card{x ∈ Nk(x0) | λx = λı }. The conditional probability density of x0 (given the label)can now be estimated byφest(x0 | λı ) = kını · v(5),where v denotes the volume of the hypersphere around x0. Moreover, the unconditionaldensity of x0 and the prior probability of the label λı can be estimated bypest(λı ) = nınφest(x0) = kn · v(6),,respectively. For the probabilities in (3) one thus obtainspx0(λı ) = pest(λı | x0) =φest(x0 | λı ) · pest(λı )φest(x0)=kık.(7)Remark 1. Note that the NN estimation of the conditional probability density (5) isactually given byφest(x0 | λı ) = kını · vı,where vı is the volume of the smallest sphere around x0 that contains all of the kı neighborswith label λı . Then, however, the probabilitiespx0(λı ) = kı · vk · vı(8)do not necessarily add up to 1. This problem is related to a general difficulty of NN densityestimation. Namely, deriving (4) for all x ∈ X leads to a non-normalized density functionφest since each x requires a different hypersphere.1313 Apart from that, an NN density estimation may suffer from very heavy tails and an infinite integral.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383345Of course, (7) might be considered as a formal justification of the original kNN(decision) rule: The label estimated by the (majority vote) kNN rule is just the one ofmaximal (posterior) probability [18]. Still, one should be cautious with the distribution(7). Particularly, it is not clear how reliable the estimated probabilities px0(λı ) = kı/kactually are. It is possible to construct corresponding confidence intervals, but these areonly asymptotically valid [68]. In fact, k is generally small and, hence, (7) not veryreliable.14 Improving the quality of predictions by simply increasing k obviously doesnot work since it also entails an enlarging of the hypersphere around x0.153.3. Weighted NN rulesA straightforward modification of the kNN rule is to weight the influence of aneighboring sample point by its distance. This idea leads to replace (2) byλestx0= arg maxλ∈L(cid:5)x∈Nk(x0): λx=λω(x | x0, S),(9)where ω(x | x0, S) is the weight of the neighbor x. There are different possibilities to definethese weights. For example, let the neighbors Nk(x0) = {x1, . . . , xk} be arranged such thatdı = DX (xı, x0) (cid:1) DX (x , x0) = d for ı (cid:1)  . In [37], the weights are then determinedas16(cid:6)ω(xı | x0, S) =(dk − dı)/(dk − d1)1if dk (cid:18)= d1,if dk = d1.(10)The weighting of neighbors appears reasonable from an intuitive point of view. Forinstance, a weighted kNN rule is likely to yield LIGHT rather than DARK as aclassification in Fig. 1 (top). More general evidence for the usefulness of distance-weighting is provided in [54,58], at least in the practically relevant case of finite samples. Infact, in [5] it was shown that the asymptotic performance of the kNN rule is not improvedby distance-weighting.Note that the original kNN rule corresponds to the weighted rule with(cid:6)ω(x | x0, S) =1 if x ∈ Nk(x0),0 if x /∈ Nk(x0).(11)Thus, the NN rule can be expressed as a global principle involving the complete sample Sof observations without loss of generality:λestx0= arg maxλ∈L(cid:5)(cid:19)x,λx(cid:20)∈S: λx =λω(x | x0, S).(12)14 An estimated probability is always a multiplicity of 1/k. Particularly, px0 (λı ) ∈ {0, 1} in the special casek = 1, i.e., for the 1NN rule.15 Good estimations are obtained for small hyperspheres containing many points. Besides, asymptoticconvergence generally assumes an adaptation of k as a function of n.16 See [54] for a modification that performed better in experimental studies; for other types of weight functionssee, e.g., [79].346E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383Interestingly enough, it is also possible to consider the probabilistic NN prediction (7) inthe context of the weighted NN approach. Namely, (7) can be written as(cid:5)px0(λ) =(cid:19)x,λx(cid:20)∈S: λx =λω(x | x0, S),with the weight function ω now being defined byω(x | x0, S) =(cid:6)1/k0if x ∈ Nk(x0),if x /∈ Nk(x0).(13)(14)Again, (12) then amounts to choosing the label with maximal posterior probability.Of course, in the following situation one would hardly advocate a uniform distributionsuggesting that labels DARK and LIGHT have the same probability:This example reveals a shortcoming of the weight function (14), namely the disregard ofthe arrangement of the neighbors. In fact, the derivation of the probabilistic NN estimation(7) disregards the actual distances and positions in the estimation of probability densities.17This, however, is only justified if the sphere containing the k nearest neighbors is indeedvery small, which is usually not the case in practice. (Note that the label DARK is assigneda higher degree of probability than LIGHT according to (8), cf. Remark 1.)In order to account for this problem, it is possible to combine the idea of weighting andprobabilistic estimation. The use of the uniform weights (14) corresponds to the use ofthe (uniform) Parzen window in kernel-based density estimation [59]. By making use of amore general kernel function K : Rl → R(cid:1)0, a density function which is usually symmetricaround 0, the NN density estimation (4) can be generalized as follows:φest(x0) = 1n·n(cid:5)ı=1Kdk (x0 − xı),(15)where dk is the distance between x0 and its kth nearest neighbor and Kdk is a re-scaling ofa kernel function K (with K(u) = 0 for |u| > 1):Kd : u (cid:22)→ 1/d l · K(u/d).The same reasoning as in Section 3.2 then suggests a weighted counterpart of (7):pest(λ | x0) ∝(cid:5)(cid:19)x,λx(cid:20)∈S: λx =λKdk (x0 − x).(16)17 Taking positions into account becomes very tricky in instance spaces of higher dimension [86].E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383347As can be seen, (16) is nothing else than an estimation derived from the weighted NNrule by means of normalization.18 Thus, proceeding from weights such as (10), one simplydefines a probability distribution px0 such that(cid:5)px0(λ) ∝ω(x | x0, S).(17)(cid:19)x,λx(cid:20)∈S: λx =λRelated to this approach are extensions of NN classification which make use of fuzzysets [6,8,46,47]. By weighting neighbors according to their distance, these methodscompute a “fuzzy” classification(cid:3)λestx0=uλ1(x0), . . . , uλm(x0)(cid:4)(18)for a new instance x0. That is, x0 is not assigned a unique label in an unequivocal way.Rather, a degree of membership, uλ(x0), is specified for each label λ. Consider as anexample the fuzzy kNN algorithm proposed in [47]. The degree to which x0 is assignedthe label λı (is classified into the ıth class) is given byuλı (x0) =(cid:7) =1 uı |x0 − x |−2/(m−1)k(cid:7)|x0 − x |−2/(m−1)k =1,(19)where uı = uλı (x ) is the membership degree of the instance x in the ıth class. Thepossibility of assigning fuzzy membership degrees uı to labeled instances x is seen as adecisive feature. Turning the (non-fuzzy) label λx of an observed instance x into a fuzzylabel allows one to adjust the influence of that instance if it is not considered prototypicalof its class. The constant m in (19) determines the weighting of the distance between x0and its neighbors.Clearly, (19) still has a probabilistic flavor since degrees of membership add up to1.19 However, the use of fuzzy labels makes it more general than (17). In fact, a fuzzyclassification (18) can be written asuλ0(x0) ∝n(cid:5)ı=1uλ0(xı) · ω(xı | x0, S).Formally, the main difference between a probabilistic estimation and a fuzzy classificationis hence the use of fuzzy labels in the latter approach: In the probabilistic case, an observedinstance (cid:19)x, λx (cid:20) supports the label λx only. Depending on the “typicality” of the instance (itmight concern a “boundary case” whose labeling was not unequivocal), it may also supportlabels λ (cid:18)= λx in the case of fuzzy classification.18 Note, however, that (16) actually considers more than k instances if the kth nearest neighbor is not unique.See [58] for an alternative type of distance-weighting in kNN which unifies classification and density estimation.19 Formally, (19) might hence be interpreted as a probability distribution as well. It should be noted, however,that this interpretation might be criticized since the derivation of (19) does not assume an underlying probabilisticmodel.348E. Hüllermeier / Artificial Intelligence 148 (2003) 335–3833.4. IBL algorithmsProceeding from the basic NN approach, a family of instance-based machine learningalgorithms has been proposed in [2,4]. The simplest algorithm, known as IB1, mainlydiffers from the basic NN algorithm in that it normalizes the (numeric) attribute valuesof instances (which are characterized by means of an attribute–value representation) toguarantee that features are equally weighted, processes instances incrementally, and uses asimple method for tolerating missing attribute values. IB2 extends IB1 by using an editingstrategy, i.e., it maintains a memory (case base) of selected cases called prototypes (falselyclassified points are added as references). A further extension, IB3, aims at reducing theinfluence of noisy observations.20 To this end, a classification record is maintained, whichcounts the correct and incorrect votes of the stored references. By weighting attributevalues in the computation of the distance measure, IB4 and IB5 [2] take the relevanceof features into account. The weights are adapted each time a new classification has beenmade.To summarize, IBL algorithms (for concept learning) basically consist of threecomponents [2]: A similarity function computes a numeric similarity between instances.A classification function decides on the membership of a newly presented instance ina concept, given the similarities between the new instance and the stored examples aswell as the labels (and classification performance) of these examples. It yields a completeconcept description when being applied to all (still unclassified) instances. After eachclassification task, a concept description updater derives a modified concept descriptionby maintaining the memory of cases. The decision whether to retain or remove a case isbased on records of the previous classification performance and the information providedby the new classification task.As for the basic NN rule, some efforts have been made to improve the performance ofIBL algorithms. Important points, some of which have already been mentioned above,include conceptual aspects such as the reduction of storage requirements by editingand prototype selection [55], the toleration of noise [4], the definition of similarityfunctions [80], and feature weighting or selection [77], as well as practical issues suchas efficient techniques for indexing training examples [76]. Apart from classification, IBLtechniques can also be employed for function approximation, that is to predict real-valuedattributes [48,86].4. Possibilistic extrapolation of cases4.1. The basic estimation principleThe following type of possibilistic prediction was proposed in [23] and has been furtherdeveloped in [25,27]:.= max1(cid:2)ı(cid:2)nδx0(λ0)(cid:1)σX (x0, xı), σL(λ0, λı)(cid:2),min20 See also [78] for an early work along these lines.(20)E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383349for all λ0 ∈ L, where δx0(λ0) denotes the (estimated) possibility of the label λ0, i.e. the= λ0. Moreover, σX and σL are [0, 1]-valued similarity measures on Xpossibility that λx0and L, respectively.4.1.1. The possibility distribution δx0According to (20), λx0= λ0 is regarded as possible if there is an instance (cid:19)xı, λxı(cid:20) suchthat both, xı is close to x0 and λxı is close to λ0. Or, if we define the joint similarity betweenthe labeled instance (cid:19)xı, λxı(cid:20) and the (hypothetical) case (cid:19)x0, λ0(cid:20) to be the minimum ofthe similarities σX (x0, xı) and σL(λ0, λxı ), this can be expressed by saying that the case(cid:19)x0, λ0(cid:20) is regarded as possible if the existence of a similar case (cid:19)xı , λxı(cid:20) is confirmed byobservation. In other words, a similar case provides evidence for the existence of (cid:19)x0, λ0(cid:20)in the sense of possibility qualification.21Following the notational convention of Section 2, possibility degrees δx0(λ0) denotedegrees of “guaranteed possibility”. Thus, they are actually not considered as degreesof plausibility in the usual sense but rather as degrees of confirmation as introduced inSection 2.2. More specifically, the distribution δx0 : L → [0, 1] is thought of as a lowerrather than an upper bound. Particularly, δx0(λ0) = 0 must not be equated with the= λ0 but merely means that no evidence supporting the label λ0 isimpossibility of λx0available so far! In fact, δx0 is of provisional nature, and the degree of possibility assignedto a label λ0 may increase when gathering further evidence by observing new examples, asreflected by the application of the maximum operator in (20).This is completely in accordance with the use of possibility theory in connection witha special approach to fuzzy rule-based reasoning. Indeed, proceeding from the rule “Thecloser x to x0, the more possible it is that λx is close to λx0 ”, the possibility distribution(20) has originally been derived as the inference result of a related approximate reasoningmethod [32]. The latter concerns an example-based approach to fuzzy rules where asingle rule (case) is considered as a piece of data [84]. This contrasts with the constraint-based approach where a rule is modeled as an implication and several rules are combinedconjunctively (a possibility distribution is then an upper bound, cf. Section 2.1).It is natural to assume a possibility distribution π : Ω → [0, 1] to be normalized (in thesense that supω∈Ω π(ω) = 1) if π(ω) specifies the degree of plausibility that ω correspondsto the “true world” ω0.22 The above remarks make clear that this constraint does not makesense for δx0. In this connection, it should also be noticed that there is not necessarilya unique actual world ω0 in the sense of the possible worlds semantics [9]. Since x0is not assumed to have a unique label, δx0 rather provides information about the set{λ ∈ L | x0 ∈ Cλ} of potential labels. Thus, the state of “complete knowledge” correspondsto the distribution δx0 with δx0(λ) = 1 if x0 ∈ Cλ and δx0(λ) = 0 otherwise.21 The idea of possibility qualification is usually considered in connection with natural language proposi-tions [65,83]. Here, possibility qualification is casuistic rather than linguistic.22 Though generally accepted, this constraint is questioned by some authors. For example, a sub-normalizeddistribution might be allowed in order to express a kind of conflict.350E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383When being applied to all x ∈ X , (20) yields “fuzzy” concept descriptions, that ispossibilistic approximations of the concepts Cλ (λ ∈ L):| x ∈ Xx, δx(λ)(cid:1)(cid:3)=(cid:2)(cid:4),Cestλ(21)where δx(λ) is the degree of membership of x ∈ X in the fuzzy concept Cestλ . Notethat these fuzzy concepts can overlap in the sense that some x has a positive degree ofλ(cid:17) , λ (cid:18)= λ(cid:17).23membership in two concepts Cestλ and Cest4.1.2. The similarity measures σX and σLLet us make some remarks on the similarity measures σX and σL. To begin with,notice that—according to (20)—the similarity of cases is in direct correspondence withthe possibility assigned to a label. Roughly speaking, the principle expressed by (thefuzzy rule underlying) equation (20) gives rise to turn similarity into possibilistic support.Consequently, σX and σL are thought of as, say, support measures rather than similaritymeasures in the usual sense. They do actually serve the same purpose as the weightfunctions in Section 3.3. Particularly, σX (x0, xı) = 0 means that the label λxıis notconsidered as a relevant piece of information since xı is not sufficiently similar to x0. Forcomputation, irrelevant cases in (20) can clearly be left out of account. Thus, it is enoughto consider cases in a certain region around x0. As opposed to the kNN approach, it is thesize of this region rather than the number of neighboring cases which is fixed.We assume σX and σL to be reflexive and symmetric, whereas no special kind oftransitivity is required. In fact, the application of the maximum operator in (20) does evenpermit a purely ordinal approach. In this case, the range of the similarity measures is afinite subset A ⊂ [0, 1] that encodes an ordinal scale such as{completely different, . . . , very similar, identical}.(22)Correspondingly, degrees of possibility are interpreted in a qualitative way [33,52]. Thatis, δx0(λ) < δx0(λ(cid:17)) only means that label λ is less supported than label λ(cid:17); apart from that,the difference between these values has no meaning.Needless to say, a scale such as (22) is more convenient if instances are complexobjects rather than points in a Euclidean space and if similarity (distance) between objectsmust be assessed by human experts (which is common practice in case-based reasoning).Note that an ordinal structure is also sufficient for the original kNN rule. In connectionwith distance-weighting, however, the structures of the involved measures become moreimportant. In any case, one should be aware of the fact that a cardinal interpretation ofsimilarity raises some crucial semantic questions if corresponding measures cannot bedefined in a straightforward way. In the weighted kNN rule, for example, one patient thatdied from a certain medical treatment compensates for two patients that survived if theformer is twice as similar to the current patient. But what exactly does “twice as similar”mean in this context?Looking at (20) from the point of view of observed cases, this estimation principledefines a (possibilistic) extrapolation of each sample (cid:19)x, λx (cid:20). In the original NN approach,23 In practice, fuzzy and/or overlapping concepts seem to be the rule rather than the exception [1].E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383351which does not involve a distance measure DL on L, a case (cid:19)xı, λxıthe label λxı . This corresponds to the special case where σL in (20) is given by(cid:20) ∈ S can only support(cid:6)σL(λ, λ(cid:17)) =10if λ = λ(cid:17),if λ (cid:18)= λ(cid:17),(23)which is reasonable if L is a nominal scale, as, e.g., in concept learning or patternrecognition (classification with |L| = 2).By allowing for graded distances between labels, the possibilistic approach provides(cid:20) to support similar labels as well. This type of extended extrapolationfor a case (cid:19)xı, λxıis reasonable if L is a cardinal or at least ordinal scale. In fact, it should be observed that(20) applies to continuous scales in the same way as to discrete scales and thus unifiesthe performance tasks of classification and function approximation. For example, knowingthat the price (= label) of a certain car is $ 10,500, it is quite plausible that a similar carhas exactly the same price, but it is plausible as well that it costs $10,700. Interestinglyenough, the same principle is employed in kernel-based estimation of probability densityfunctions, where probabilistic support is allocated by kernel functions centered aroundobservations [59,63]. Indeed, (20) can be considered as a possibilistic counterpart ofkernel-based density estimation. Let us finally mention that the consideration of gradeddistances between labels is also related to the idea of class-dependent misclassificationcosts [60,71].4.2. Generalized possibilistic estimationThe possibility distribution δx0, which specifies the fuzzy set of well-supported labels,is a disjunctive combination of the individual support functions(cid:1): λ0 (cid:22)→ minσX (x0, xı), σL(λ0, λxı )(cid:2)δıx0.(24)In fact, the max-operator in (20) is a so-called t(riangular)-conorm and serves as a= λ0 is regarded as possible if (cid:19)x0, λ0(cid:20) is similar togeneralized logical or-operator: λx0(cid:19)x1, λx1(cid:20) or . . . or to (cid:19)xn, λxn(cid:20) or to (cid:19)x2, λx2(cid:20).Now, fuzzy set theory offers t-conorms other than max and, hence, (20) can begeneralized as follows:δx0(λ0).= δ1x0(cid:8)=(λ0) ⊕ · · · ⊕ δnx0(λ0) ⊕ δ2x0(cid:1)σX (x0, xı), σL(λ0, λxı )(λ0)min(cid:2)1(cid:2)ı(cid:2)n(cid:9)= 1 −(cid:1)1 − σX (x0, xı), 1 − σL(λ0, λxı )max(cid:2)1(cid:2)ı(cid:2)nfor all λ0 ∈ L, where ⊗ and ⊕ are a t-norm and a related t-conorm, respectively. Recallthat a t-norm is a binary operator ⊗ : [0, 1]2 → [0, 1] which is commutative, associative,monotone increasing in both arguments and which satisfies the boundary conditionsx ⊗ 0 = 0 and x ⊗ 1 = x. An associated t-conorm is defined by the mapping (α, β) (cid:22)→ 1 −(1 − α) ⊗ (1 − β). The t-norm associated with the t-conorm max is the min-operator. Otherimportant operators are the product ⊗P : (α, β) (cid:22)→ αβ with related t-conorm ⊕P : (α, β) (cid:22)→352E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383α + β − αβ and the Lukasiewicz t-norm ⊗L : (α, β) (cid:22)→ max{0, α + β − 1} with related t-conorm ⊕L : (α, β) (cid:22)→ min{1, α + β}.Observe that the minimum operator employed in the determination of the joint similaritybetween cases can be considered as a logical operator as well, namely as a fuzzyconjunction: Two cases (cid:19)x0, λx0(cid:20) are similar if both, x0 is similar to x1 andλx0 is similar to λx1 . Consequently, this operator might be replaced by a t-norm, too. Bydoing so, (24) and (20) become(cid:20) and (cid:19)x1, λx1δıx0: λ0 (cid:22)→ σX (x0, xı) ⊗ σL(λ0, λxı )andδx0(λ0).=(cid:8)1(cid:2)ı(cid:2)nσX (x0, xı) ⊗ σL(λ0, λxı ),(25)(26)respectively. Note, however, that a (fuzzy) logic-based derivation of the joint similarity isnot compulsory. Particularly, the t-norm ⊗ in (26) need not necessarily be the one relatedto the t-conorm ⊕. For example, one might thoroughly take ⊗ = min and ⊕ = ⊕P , or evencombine the similarity degrees σX (x0, xı) and σL(λ0, λxı ) by means of an operator whichis not a t-norm. In that case, however, the “logical” interpretation of (26) is lost.4.2.1. Control of compensation and accumulation of supportBy choosing an appropriate t-conorm ⊕ in (26) one can control the accumulationof individual degrees of evidential support, especially the extent of compensation.To illustrate, consider the following situation, where σX (x0, x1) = 3/4, σX (x0, x2) =σX (x0, x3) = 1/2, and σX (x0, x4) = 1/4:Should one prefer DARK or LIGHT as a classification of the new point? The use ofthe max-operator as a t-conorm yields δx0(DARK) = 3/4 and δx0(LIGHT) = 1/2 and,hence, the decision DARK. The three moderately similar instances with label LIGHTdo not compensate for the one very similar instance with label DARK. As opposed tothis, the probabilistic sum (α, β) (cid:22)→ α + β − αβ brings about a compensation effect andentails δx0(DARK) = 3/4 and δx0(LIGHT) = 13/16, that is, a slightly larger possibilityfor LIGHT.More generally, different t-conorms can model different accumulation modes, whichtypically entail a kind of saturation effect. In the case of the probabilistic sum ⊕P , forexample, an additional β-similar observation increases the current support α by β(1 − α).Thus, the larger the support already granted is, the smaller the absolute increase due tothe new observation will be. This appears reasonable from an intuitive point of view: Ifthe support of a label is already large, one is not surprised to see another (close) instancehaving the same label. A small support increment then reflects the low information contentrelated to the new observation [44].E. Hüllermeier / Artificial Intelligence 148 (2003) 335–3833534.2.2. Possibilistic support and weighted NN estimationA t-norm ⊗ is called Archimedian if the following holds: For all x, y ∈ ]0, 1[ there is anumber n ∈ N such that ⊗(n)(x) < y (where ⊗(n)(x) = ⊗(n−1)(x) ⊗ x and ⊗(1)(x) = x). Itcan be shown that ⊗ is a continuous Archimedian t-norm iff there is a continuous, strictlydecreasing function g : [0, 1] → [0, ∞] such that g(1) = 0 and(27)for all 0 (cid:1) α, β (cid:1) 1, where the pseudo-inverse g(−1) is defined asα ⊗ β = g(−1)g(α) + g(β)(cid:4)(cid:3)(cid:6)g(−1) : x (cid:22)→g−1(x)0if 0 (cid:1) x (cid:1) g(0),if g(0) < x.The function g is called the additive generator of ⊗. For example, x (cid:22)→ 1 − x andx (cid:22)→ − ln(x) are additive generators of the Lukasiewicz t-norm ⊗L and the product ⊗P ,respectively.Based on the representation (27), one can establish an interesting connection between(26) and the weighted NN rule. To this end, let g be the additive generator of the t-norm24 related to the t-conorm ⊕ used as an aggregation operator in (26). With dı =1 − σX (x0, xı) ⊗ σL(λ0, λxı ) and ωı = g(dı ), we can write (26) asδx0(λ0) = 1 − g(−1)(ω1 + ω2 + · · · + ωn).(28)Since g is decreasing, it can be considered as a weight function that turns a distance dıinto a weight ωı associated with the ıth instance. Then, (28) tells us that the possibilitydegree δx0(λ0) is nothing else than a (monotone increasing) transformation of the sum ofweights ωı . In other words, (26) can be seen as a distance-weighted NN estimation, wherethe weight of a neighbor is determined as a function of its similarity to the new instance.As opposed to (9), however, the weight of a case according to (28) does not depend onother cases stored in memory (cf. Section 4.3.1 below).Consider the Lukasiewicz t-(co)norm as an example, for which we obtain ωı = 1 − dı =σX (x0, xı) ⊗ σL(λ0, λxı ) andδx0(λ0) = min{1, ω1 + ω2 + · · · + ωn}.(29)If, moreover, σL is given by (23), then δx0(λ0) is nothing else than the bounded sum of the= λ0. Thus,similarity degrees σX (xı, x0) between x0 and the instances xı with label λxı(29) is basically equivalent to the global NN method, i.e. the weighted NN approach withk = n,25 apart from the fact that it does not distinguish between labels whose accumulatedsupport exceeds 1 (this is another type of saturation effect). For the probabilistic sum ⊕P ,the mapping between possibility degrees and the sum of weights is one-to-one:(cid:3)δx0(λ0) = 1 − exp−(ω1 + ω2 + · · · + ωn)(cid:4).In connection with the generalized model (26), the t-conorm ⊕ used for combiningindividual degrees of support defines another degree of freedom of the model. It is24 This is not the t-norm used in (26) for defining a joint similarity measure.25 The proper kNN rule cannot be emulated as in (11) since the weights ωı depend on absolute distance (again,see Section 4.3.1 below).354E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383hence interesting to mention the existence of parameterized families of t-(co)norms whichcomprise commonly used operators as special cases. For example, the Frank-family isdefined as⊕ρ : (α, β) (cid:22)→max(α, β)α + β − αβmin{1, α + β}(cid:3)1 + (ρ1−α−1)(ρ1−β−1)1 − lnρρ−1(cid:4)if ρ = 0,if ρ = 1,if ρ = ∞,otherwise.(30)Proceeding from such a family of t-conorms, the degree of freedom of the model reducesto a single parameter, here ρ, which can be adapted in a simple way, e.g., by means ofcross-validation techniques.4.2.3. Upper and lower possibility boundsThe possibility degree (26) represents the support (confirmation) of a label λ0 gatheredfrom similar instances, according to the basic NN principle suggesting that similar(cid:20)instances have similar labels. Now, in the sense of this principle, an observation (cid:19)xı , λxımight not only confirm but also disqualify a label λ0. This happens if xı is close to x0 butλxı is not similar to λ0. A possibility distribution expressing degrees of exclusion ratherthan degrees of support and, hence, complementing (26) in a natural way is given byπx0 : λ0 (cid:22)→(cid:9)(cid:3)1 − σX (x0, xı)(cid:4)⊕ σL(λ0, λxı ).(31)1(cid:2)ı(cid:2)nAccording to (31), an individual observation (cid:19)xı , λxı(cid:20) induces a constraint on the label(cid:20) if both, σX (x0, xı) is large and σL(λ0, λxı )of x0: A label λ0 is disqualified by (cid:19)xı, λxı(cid:20) is completely ignored if σX (x0, xı) = 0, in whichis small. As opposed to this, (cid:19)xı, λxı≡ 1 is an expression ofcase the individual support on the right-hand side of (31) is 1 (πx0complete ignorance: all upper possibility bounds are 1 since there is no reason to discreditany label). This approach is obviously in agreement with the constraint-based view ofpossibilistic reasoning (cf. Section 2.1). Moreover, the distribution (31) is again related toa special type of fuzzy rule [26].The possibility of a label λ0 can now be characterized by means of an extendedestimation, namely as a tuple(cid:14)δ∗x0(λ0) =(cid:15)δx0(λ0), πx0(λ0)with a lower bound δx0(λ0) expressing a degree of confirmation, and an upper boundπx0(λ0) expressing a degree of plausibility. The following cases show that the comple-mentary distribution πx0 can greatly improve the informational content of a possibilisticevaluation:26• δ∗(λ0) = [0, 1]: This is an expression of complete ignorance. Neither is λ0 supportedx0nor is it (partly) excluded by any observation. Thus, λ0 is fully plausible though notconfirmed at all.26 Recall that positive and negative evidence cannot be distinguished in probability theory.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383355(λ0) = [0, 0]: Clear evidence against λ0 has been accumulated in the form of• δ∗x0instances similar to x0 with labels dissimilar to λ0.(λ0) ≈ [1, 1]: The label λ0 is strongly supported through the observation of similar• δ∗x0instances.Notice thatδx0(λ0) > πx0(λ0)(32)indicates a kind of conflict and is closely related to the problem of ambiguity in connectionwith the NN principle (cf. Section 3.2). In fact, (32) can occur if x0 has close neighbors xıand x with quite dissimilar labels λxı and λx (mathematically speaking, x0 is a point ofdiscontinuity). In this case, the evaluation of λ0 is unsteady, and the support δx0(λ0) shouldbe taken with caution. The inequality in (32) might also trigger a revision process that aimsat removing the conflict by means of a model adaptation.4.2.4. Fuzzy logical evaluationThe values δx0(λ0) in (26) can also be considered as membership degrees of a fuzzy set,namely the fuzzy set of “well-supported labels”. In fact, the possibility degree δx0(λ0) canbe seen as the truth degree, (cid:19)P (λ0)(cid:20), of the following (fuzzy) predicate P (λ0): “There isan instance close to x0 with a label similar to λ0.” P (λ0) defines the property that qualifiesλ0 as a well-supported label.Of course, one might easily think of alternative characterizations of well-supportedlabels. Fuzzy set-based modeling techniques allow for translating such characterizationsgiven in linguistic form into logical expressions. By using fuzzy logical connectivesincluding t-norms, fuzzy quantifiers such as “a few” and fuzzy relations such as “closelylocated”, one can specify sophisticated fuzzy decision principles that go beyond the simpleNN rule. Example:“There are at least a few closely located instances, most of these instances have the samelabel, and none of the moderately close instances has a very different label.”The logical expression P (·) associated with such a specification can be used in place of theright-hand side in (26):.= (cid:19)P (λ0)(cid:20).(33)δx0(λ0)The decision rule related to (26) favors the label λestthat meets the requirements specifiedx0by P (·) best. This generalization appears especially interesting since it allows one to adaptthe NN principle so as to take specific characteristics of the application into account.Observe that (33) can also mimic the original kNN rule: Consider the fuzzy proposition“λ0 is supported by many of the k nearest neighbors of x0”, and let the fuzzy quantifier“many (out of k)” be modeled by the mapping ı (cid:22)→ ı/k. Then, δx0(λ0) = ı/k iff ı amongthe k nearest neighbors have label λ0. In this case, possibility degrees (derived from fuzzytruth degrees) formally coincide with probability degrees.356E. Hüllermeier / Artificial Intelligence 148 (2003) 335–3834.3. Comparison of extrapolation principlesSo far, we have discussed two types of NN approaches to estimation and decisionmaking: A probabilistic one, which is in agreement with the original kNN rule, and apossibilistic one introduced in this section. Both approaches can be considered as a two-step procedure. The first step derives a distribution that will subsequently be referred toas the NN estimation. This estimation defines a degree of support for each label λ ∈ L.The second step, the NN decision, chooses one label on the basis of the NN estimation.Usually, the decision is given by the label with maximal support, and ties are broken bycoin flipping. Still, in the case of a continuous (or at least ordinal) scale L, a decision mightalso be obtained by some kind of averaging procedure.In order to facilitate the comparison of the two approaches, we write degrees ofevidential support in the general formν(λ | x0, S) = α{νx (λ | x0, S) | (cid:19)x, λx (cid:20) ∈ S}and thus obtain the (maximal support) decision asλestx0= arg maxλ∈Lν(λ | x0, S).(cid:4)(34)(35)In (34), νx (λ | x0, S) is the support of the hypothesis λx0instance (cid:19)x, λx(cid:20), and α is an aggregation function.= λ provided by the labeledTo reveal the original kNN rule and the probabilistic approach as special cases of (35),note that the probability distribution (7) is obtained by using the arithmetic sum as anaggregation function α and defining the support function asνpx (λ | x0, S) =1/k0if x ∈ Nk(x0) and λ = λx ,otherwise.More generally, a support function can be defined asνpx (λ | x0, S) =Kdk (x0 − x)0if λ = λx ,otherwise,(36)(37)(cid:3)(cid:6)(cid:6)where K is a kernel function. The index dk denotes the distance between x0 and its kthnearest neighbor. It signifies that the kernel function is scaled so as to exclude exactly thoseinstances xı with DX (x0, xı) > dk. Proceeding from (37), the probability distribution px0is obtained by normalizing the supportsνp(λ | x0, S) =which yields(cid:5)(cid:19)x,λx(cid:20)∈Sνpx (λ | x0, S),px0(λ) =(cid:7)νp(λ | x0, S)m =1 νp(λ | x0, S)(38)for all λ ∈ L. That is, the aggregation α is now the normalized rather than the simplearithmetic sum. Of course, since normalization does not change the mode of a distributionit has no effect on decision making and could hence be omitted from this point of view.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383357The possibilistic approach (26) is recovered by α = ⊕ andνδx (λ | x0, S) = σX (x0, x) ⊗ σL(λ, λx ).(39)As can be seen, the main difference between the probabilistic and the possibilistic approachconcerns the definition of the individual support function νx and the aggregation of thecorresponding degrees of support.Apart from that, however, a direct comparison is complicated by the similarity measureover labels, σL, which is used in (39) but not in (37). One possibility to handle this problemis to consider (39) only for the special case (23):(cid:6)νδx (λ | x0, S) =σX (x0, x)0if λ = λx ,otherwise.(40)Eq. (40) reveals that the similarity measure σX now plays the same role as the kernelfunction K in (37).4.3.1. Absolute versus relative supportAn important difference between (37) and (40) is that an example (cid:19)x, λx (cid:20) ∈ S providesrelative support of a label λ in the probabilistic approach but absolute support in thex (λ | x0, S) depends on the absolute similarity between x0 andpossibilistic one. That is, νδx (λ | x0) inx but is independent of further observations. In fact, we can actually write νδx (λ | x0, S) since S does not appear on the right-hand side of (40): The supportplace of νδprovided by observed samples (cid:19)x, λx (cid:20) is bounded to nearby instances, decreases graduallywith distance, and vanishes for completely dissimilar examples.As opposed to this, the support νpx (λ | x0, S) is relative and depends on the relationbetween the distance of x to x0 and the distances of other observations to x0. This isreflected by the scaling of the kernel function in (37). On the one hand, this means thatνpx (λ | x0, S) can be large even though x is quite distant from x0. On the other hand,the extension of the sample S by another instance close enough to x0 might exclude aquite similar observation x from the neighborhood Nk(x0). The corresponding re-scalingof the kernel function will then cancel the support provided by (cid:19)x, λx (cid:20) so far. The inducedthresholding effect appears especially radical (and might be questioned on such grounds)in connection with (36), where νpx (λ | x0, S) is reduced from 1/k to 0, that is from fullsupport to no support at all.The bounding of evidential support, as realized by the possibilistic approach, is oftenadvisable. Consider a simple example: Let X = [0, 1] and λx = I[1/2,1](x)27 and supposeinstances to be chosen at random according to a uniform distribution. Moreover, assumethat a new instance x0 must be labeled, given only one observation, x1. Using the 1NN rule,the probability of a correct decision is obviously 1/2. Now, suppose that the NN rule isapplied only if |x0 − x1| (cid:1) d, whereas a decision is determined by flipping a coin otherwise(this is exactly the procedure that results from the possibilistic approach by defining σX in(20) by σX (x, x(cid:17)) = 1 if |x − x(cid:17)| (cid:1) d and 0 otherwise). A simple calculation shows that theprobability of a correct decision is now 1/2 + d(1 − d). As can be seen, dissimilar instances27 IA is the indicator function: IA(x) = 1 if x ∈ A and 0 otherwise.358E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383are likely to provide misleading information in this example and, hence, the disregard ofsuch instances is indeed advantageous. Loosely speaking, it is better to guess a label atrandom than to rely on observations not similar enough.Of course, the concept of absolute support is actually not reserved to the possibilisticapproach but can be realized for the probabilistic method as well. To this end, one simplyreplaces (37) by(cid:6)νpx (λ | x0, S) =K(x0 − x)0if λ = λx,otherwise,(41)where the kernel function K is now fixed. That is, K is no longer scaled by the size ofthe neighborhood of x0. This is exactly the estimation one derives by the reasoning inSection 3.2 if the generalized NN density estimation (15) is replaced by the simple kernelestimator:φest(x0) = 1n·n(cid:5)ı=1K(x0 − xı).(42)Here, the only problem occurs if νp(λ | x0, S) = 0 for all λ ∈ L. In this situation (ofcomplete ignorance), a probability distribution cannot be derived by normalization.Apart from that, (41) might indeed be preferred to (37) due to the reasons mentionedabove. In fact, one should realize that one of the major reasons for using the NN densityestimator (15) rather than the kernel estimator (42) is to guarantee the continuity ofthe density function φest. In the context of instance-based learning, however, this is notimportant since one is not interested in estimating a complete density function but only asingle value thereof. To the best of our knowledge, (37) and (41) have not been compared ina systematic way in IBL so far. Note that (41) should actually be called a NEAR NEIGHBORestimation since it involves the near rather than the nearest neighbors. The same remarkapplies to the possibilistic approach, of course.Above, it has been argued that the consideration of graded degrees of similarity betweenlabels is often advised (see also our example in Section 4.5 below). It should be mentioned,therefore, that the probabilistic approach might be extended in this direction as well. To thisend, a joint probability density can be estimated based on a kernel function K, which isnow defined over X × L. An estimation for the label λ can then be derived by conditioningon x0:(cid:5)(cid:5)px0(λ) ∝νpx (λ | x0, S) =K(x0 − x, λ − λx ).(cid:19)x,λx(cid:20)∈S(cid:19)x,λx(cid:20)∈SThis is the most general form of a probabilistic estimation. Still, one should keep in mindthat it requires X × L to have a certain mathematical structure, an assumption which is notalways satisfied in applications (again, we refer to our example below).Let us conclude this section with a final remark on related work in a different context.Interestingly enough, a distinction similar to ours between absolute and relative support hasalso been made in connection with cluster analysis. In fuzzy cluster analysis, a point mayhave a positive degree of membership in several classes. Still, in the classical approach [7]the membership degrees add up to 1 and must hence be interpreted as relative numbers.In [51], some difficulties caused by this constraint are discussed, and possibilistic clusteringE. Hüllermeier / Artificial Intelligence 148 (2003) 335–383359is advocated as an alternative. In this approach, a membership degree does indeed reflectthe (absolute) compatibility of a point with the prototype of a cluster.4.3.2. Similarity versus frequencyThe estimation principle underlying the probabilistic approach combines the conceptsof similarity (distance) and frequency: It applies a closeness assumption, typical ofsimilarity-based reasoning, that suggests to focus on the most similar observations (or toweight observations by their distance). From the reduced set of supposedly most relevantinstances, probabilities are then estimated by relative frequencies. This contrasts withthe basic (max–min) possibilistic approach (20) which relies on similarity alone: Theapplication of the maximum operator does not produce any compensation or reinforcementeffect. Thus, possibility depicts the existence of supporting evidence, not its frequency.28The generalized possibilistic approach based on (26) allows for modes of compensationwhich combine both aspects. Especially, the operators mentioned above produce a kindof saturation effect, that is, a limited reinforcement effect: The increase of support due tothe observation of a similar instance is a decreasing function of the support that is alreadyavailable.In this connection, it is important to realize the different nature of the concepts ofpossibility and probability. Particularly, it should be emphasized that the former is notinterpreted in terms of the latter.29 For example, consider the standard probabilistic settingwhere cases are chosen randomly and independently according to a fixed probabilitymeasure over X × L. The possibility degree δx0(λ0) will then converge to 1 with increasingsample size whenever (cid:19)x0, λ0(cid:20) has a non-zero probability of occurrence. In fact, thepossibilistic approach is interested in the existence of a case, not in its probability. Roughlyspeaking, the major concern of this approach is the approximation of the concepts Cλ,whereas the probabilistic approach aims at estimating conditional probability distributions= Pr(· | x0). Of course, this distinction is relevant only if the concepts are overlapping,px0that is, if the query x0 does not have a unique label. Otherwise, a possibilistic and aprobabilistic approach are equivalent in the sense that x0 ∈ Cλ ⇔ Pr(λ | x0) = 1.It is beyond question that the frequency of observations usually provides valuableinformation. Yet, the frequency-based approach does heavily rely on statistical assumptionsconcerning the generation of training (and test) data. Thus, it might be misleading ifthese assumptions are violated. Suppose, e.g., that the probability of observing a positiveexample, while learning a concept C1 ⊆ X , depends on the number of positive examplesobserved so far and hence contradicts an independence assumption (the probability of alabel λx , given the instance x, is not independent of the data). In this case, a probabilisticestimation is clearly biased, whereas the possibility distribution (20) is not affected at all.Indeed, the information expressed by δx0 remains valid even if only negative examplesxı ∈ C0 = X \ C1 have been presented so far: δx0(1) = 0 then simply means that noevidence for x0 ∈ C1 has been gathered as yet. Moreover, the value δx0(0) reflects the28 To a certain extent, this is related to the distinction between an existential and an enumerative analogy factorin models of analogical induction [57].29 Though such a relationship can be established, e.g., by interpreting possibility as upper probability [31] orfuzzy sets as coherent random sets [28].360E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383available support for x0 ∈ C0. This support depends on the distance of x0 to the observednegative examples. Note that δx0(0) = 0 is possible as well. In this case, no evidence isavailable at all, neither for nor against x0 ∈ C1. See Section 6.3 for a simulation experimentwhich concerns the aspect of robustness of NN estimation toward violations of the standardstatistical assumptions.Apart from statistical assumptions, the structure of the application has an importantinfluence. To illustrate, consider two classes in the form of two clusters such that the(known) diameter of both clusters is smaller than the distance between them, that isDX (x1, x2) < DX (x1, x3) whenever λx1(cid:18)= λx3 . The label of an instance can then= λx2be determined with certainty as soon as the distance from its nearest neighbor is known. Inother words, the 1NN rule which does not involve frequency information performs betterthan any kNN rule with k > 1.4.4. NN estimations and NN decisionsIn addition to the extrapolation principles let us compare the induced distributions,referred to as NN estimations, from a knowledge representational point of view, especiallyagainst the background of the two shortcomings of the NN rule illustrated in Fig. 1.A crucial difference between a possibility distribution δ and a probability function pis that the latter obeys a normalization constraint that demands a total probability massof 1, whereas no such constraint exists in possibility theory. Consequently, a possibilitydistribution is more expressive in some situations. Especially, the following points deservementioning:• Possibility reflects ignorance: All possibility degrees δx0(λ) remain rather small if no≡ 0 is ansufficiently similar instances are available. Particularly, the distribution δx0expression of complete ignorance and reflects the absence of any relevant observation(σX (x0, xı) = 0 for all xı). A learning agent using this estimation “knows that it≡ 1/m indicatesdoesn’t know” [70]. As opposed to this, a distribution such as, say, δx0that some (small) evidence is available for each of the m labels λı . These two situationscannot be distinguished in probability theory where they induce the same distribution≡ 1/m (if, as suggested by the principle of insufficient reason, complete ignorancepx0is modeled by the uniform distribution).• Possibility reflects absolute frequency: For example, suppose σX (x0, xı) = 1 − d > 0= λ1 for all n instances xı stored in memory. The probabilistic estimation (7)and λxıthen yields the one-point distribution px0(λ1) = 1 and px0(λ) = 0 for all λ (cid:18)= λ1. Thus,= λ1 is certain, even if n is rather small. With a compensatingit suggests that λx0t-conorm such as the probabilistic sum ⊕P , the extended estimation (26) yieldsδx0(λ1) = 1 − d n and δx0(λ) = 0 for all λ (cid:18)= λ1. Thus, not only does the possibilistic= λ1 reflect the distance but also the actual number ofsupport of the hypothesis λx0voting instances: δx0(λ1) is an increasing function of n and approaches 1 for n → ∞.As can be seen, a probabilistic estimation can represent ambiguity, whereas thepossibilistic approach captures both problems, ambiguity and ignorance: Ambiguity(Fig. 1, top) is present if there are several plausible labels with similar degrees of support,E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383361and ignorance (Fig. 1, bottom) is reflected by the fact that even the most supported labelhas a small degree of possibility. Thus, (26) can be taken as a point of departure fora decision making procedure that goes beyond the guessing of a label. For example, apossible line of action proceeding from (26) might be expressed by the following rules(involving thresholds 0 < dmax < dmin < 1):• If δx0(λ∗) (cid:2) dmin for the most supported label λ∗ and δx0(λ) (cid:1) dmax for all λ (cid:18)= λ∗,then let λestx0= λ∗.• If δx0(λ∗) < dmin, then gather further information.• If δx0(λ∗) (cid:2) δx0(λ) (cid:2) dmin for two labels λ∗, λ ∈ L, then refuse a prediction.The ECHOCARDIOGRAM DATABASE30 is a real-world example that is quite interestingin this respect. One problem that has been addressed by machine learning researchers inconnection with this database is to predict from several attributes whether or not a patientwho suffered from a heart attack will survive at least one year. Since data is rather sparse(132 instances and about 10 attributes), the possibilistic approach often yields estimationswith low support for both alternatives, surviving and not surviving at least one year. Thisis clearly reasonable from a knowledge representational point of view and reveals anadvantage of absolute over relative degrees of support. For example, telling a patient that≡ 0)your experience does not allow any statement concerning his prospect of survival (δx0is very different from telling him that his chance is 1/2 (px0≡ 1/2).Let us mention that a generalization of the kNN rule closely related to our approachhas been developed in [19]. In this method, which is also motivated by the problemsof ambiguity and ignorance in the original kNN rule, an estimation of the label λx0 isgiven in terms of a belief function [66] rather than a possibility distribution. See [27] for acomparison between the two approaches.The discrepancy between a probabilistic and a possibilistic approach (or an approachbased on belief functions) disappears to some extent if one is only interested in a finaldecision, that is if a decision must be made irrespective of the quality and quantity of theinformation at hand. The method in [19], for example, refers to the so-called transferablebelief model [69] and, hence, turns the belief function (at the “credal” level) specifyingthe unknown label into a probability function (at the “pignistic” level) before making adecision. Thus, the support of individual labels is expressed in terms of probability, and anNN estimation can be derived by taking one among the most probable labels, breaking tiesat random.Observe that, as a consequence of applying the maximum operator, a possibilistic NNdecision derived from (20) coincides with the 1NN rule. The generalized version (26),where several moderately similar examples can compensate for one very similar instance,comes closer to the original kNN rule. In fact, for certain special cases, the possibilisticapproach is equivalent—from a decision making point of view—to the probabilisticapproach based on the support function (41). Eq. (28) shows that a possibility degree δx0(λ)is a monotone transformation of the sum of weights ωı , and this relation is one-to-one if30 Available at http://www.ics.uci.edu/~mlearn.362E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383the pseudo-inverse g(−1) is actually the inverse g−1. The similarity function σX can thenbe chosen such thatδx0(λı) (cid:1) δx0(λ ) ⇔ px0(λı) (cid:1) px0(λ ).That is, labels which are better supported in a possibilistic sense are also more probableand vice versa.To illustrate, consider the case where X = Rl and σL(λ, λ(cid:17)) = 1 if λ = λ(cid:17) and 0otherwise. Let K be a kernel function and define σX as (x, y) (cid:22)→ 1 − exp(−K(x, y)).31For the t-conorm ⊕P , the weights in (28) are then given by ωı = K(x0 − xı). Therefore,(cid:16)δx0(λı) = 1 − exp−(cid:17)(cid:5)K(x0 − x)(cid:19)x,λx(cid:20)∈S: λx=λı(cid:3)= 1 − exp−c · px0(λı )(cid:4),where px0(λı ) is the probability degree derived from (41) using the kernel function K andc is the normalization factor(cid:5)c =px0(λ).λ∈L4.5. An illustrative exampleHere, we present a simple example for which the possibilistic approach might beconsidered superior to the probabilistic one. The task shall be to predict a student’s grade inphysics given some information on other grades of that student. Thus, an instance is now asubject, and the label is given by the corresponding grade. We assume that grades are takenfrom the scale L = {0, 1, . . . , 10}, where 10 is the best result. Moreover, we consider twoscenarios S1 and S2:SubjectChemistryFrenchPhilosophySpanishSportsS1––––5S210333–It is clearly not obvious how to define a reasonable similarity measure over the set ofsubjects. In fact, an ordinal measure—sufficient for the possibilistic approach (20)—appears much simpler than a cardinal one. Nevertheless, let us assume the following(cardinal) degrees of similarity:σXChem. French Phil. Span. SportsPhysics3/41/31/31/3031 Formally, one might set K(0).= ∞ to ensure that σX is reflexive.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383363Concerning the set of labels L, graded degrees of similarity are clearly advised in thisexample. Let us define the similarity between two grades a and b to be(cid:1)1 − 1σL(a, b) = max5|a − b|, 0(cid:2).Needless to say, our application does not define a statistical setup par excellence, whichis a main reason why the probabilistic approach does hardly appear suitable. To beginwith, a scenario as defined above cannot be considered as an independent sample (perhapsthe information is censored if it comes from the student himself), not to mention the smallnumber of observations. Moreover, a relative frequency interpretation does not make sense.Finally, the set X endowed with the similarity measure σX (as partly specified above) islikely to lack the mathematical (metric) structure that enables one to define a reasonablekernel function K (either on X or on X × L). Consequently, the derivation of the kNNestimation in Section 3.2 is no longer valid. Clearly, nothing prevents us from still applyingthe formulae and simply interpreting the normalized degrees of additive support as degreesof probability. But one should keep in mind that this approach actually lacks a solidfoundation.The first scenario is a typical example of complete ignorance, for one does not haveany relevant piece of information. It is true that the case base is not empty, but thegrade in sports does not allow one to draw any conclusion on the grade in physics sincethese two subjects are very dissimilar. This is adequately reflected by the possibilistic= δphysics ≡ 0. A probabilistic estimation with relative supportestimation which yields δx0is obviously not appropriate in this example. Since sports is the only neighbor one obtainsa probability distribution that favors grade 5 for physics. Thus, it is clearly advised to useabsolute rather than relative support. Then, however, a probability is actually not definedsince the denominator in (38) is zero. One way out is to take the uniform distribution≡ 1/11 as a default estimation, but this raises the well-known question whether thepx0latter is an adequate expression of complete ignorance (which is definitely denied by mostscholars).Scenario S2 reveals problems of weighting and aggregation. Undoubtedly, a weightedestimation should be preferred in this example. Still, the example shows that the definitionand aggregation of weights can be tricky. What is the most likely grade? Particularly, isgrade 3 for physics more likely than grade 10 or vice versa? The weighted kNN rule favorsgrade 3 since the three subjects which are moderately similar to physics compensate forthe one (chemistry) which is very similar. Of course, this result might be judged critically.Especially, this example reveals a problem of interdependence which is not taken intoaccount by means of a simple summation of weights. Namely, the two subjects Spanishand French are very similar by themselves. Thus, one might wonder whether the grade3 should really count twice. In fact, one might prefer to consider the grades in Frenchand Spanish as only one piece of evidence (suggesting that the student is not good atlanguages) instead of two pieces of distinct information. Formally, the problem is thatthe probabilistic approach makes an assumption of (conditional) independence which isno longer valid when taking structural assumptions about the application into account.Here, such assumptions correspond to the NN inductive bias, namely the hypothesis thatsimilar instances have similar classifications. Given this hypothesis, the instances stored364E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383in the case base are no longer independent (grade 3 in French, in conjunction with thishypothesis, makes grade 3 in Spanish very likely).The problem of interdependence cannot be taken into account as long as an estimationdisregards the similarity between the instances stored in memory, as do all the estimationspresented so far. Still, the aggregation operator ⊕ in the possibilistic approach provides ameans for alleviating the problem. With ⊕ = max, for example, frequency does not countat all and one obtains δx0(3) = 1/3 < 3/4 = δx0(10). The probabilistic sum ⊕P bringsabout a reinforcement effect but still yields δx0(3) = 0.7 < 3/4 = δx0(10), a result thatappears quite reasonable.A second problem related to scenario S2 is that of ambiguity. Particularly, theprobabilistic approach yields a bimodal distribution px0 , and the same is also true for mostaggregation operators in the possibilistic approach. For example, (26) with ⊕ = ⊕P (and⊗ = ⊗P ) yields δx0(3) > δx0(7) < δx0(10). This result is not intuitive, for one might hardlyjudge an intermediate grade less possible than two extreme grades. To solve this problem,δx0 can be replaced by its convex hull(cid:1)maxλ(cid:17)(cid:2)λδx0(λ(cid:17)), maxλ(cid:17)(cid:1)λλ (cid:22)→ minδx0(λ(cid:17))(43)(cid:2).In our example, this leads to the following distribution:λ012345678910δx0 (λ) 00.30.53 0.70.7 0.7 0.70.7 0.70.7 0.75Of course, this prediction is still ambiguous in the sense that is supports several grades bymeans of high degrees of possibility. This is not a defect, however, but rather an adequaterepresentation of the ambiguity which is indeed present in the situation associated withscenario S2.The modification (43) of δx0 should not be considered ad-hoc. Rather, the convexityrequirement can be thought of as a possibility-qualifying rule that complements thesimilarity-based justification of possibility degrees: The more possible two labels are,the more possible is any label in-between. This type of background knowledge and theassociated constraints can be met more easily in the possibilistic approach than in theprobabilistic one. In fact, the incorporation of background information is hardly compatiblewith non-parametric density estimation.In summary, the example has shown the following advantages of the possibilisticapproach: First, the interpretation of aggregated weights in terms of degrees of evidentialsupport is often less critical than the interpretation in terms of degrees of probability.Second, a possibility distribution can represent ignorance. Third, the use of aggregationoperators other than the arithmetic sum can be useful. Fourth, the possibilistic approach ismore flexible and allows for incorporating constraints or background knowledge.4.6. Complexity issuesEven though algorithmic aspects are beyond the scope of this paper, let us have a roughlook at the computational complexity of our possibilistic approach to IBL. A straightfor-E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383365ward implementation of the prediction (25) has a running time which is linear in the size|S| of the sample and the number |L| of labels. In this respect, it is hence completelycomparable to other instance-based learning methods.In order to reduce the computational complexity, IBL approaches take advantage of thefact that a prediction is already determined by the nearest neighbors of the query instance.Thus, the consideration of each sample instance is actually not necessary, and efficiencycan be gained by means of fast algorithms for finding nearest neighbors [40,49,81]. Suchalgorithms employ efficient similarity-based indexing techniques and corresponding datastructures in order to find the relevant instances quickly.The same idea can be applied in connection with our possibilistic approach. In fact, apossibility degree δx0(λ) is completely determined by the neighborhood of the case (cid:19)x0, λ(cid:20),that is the sample instances (cid:19)x, λx (cid:20) satisfying σX (x, x0) > 0 and σL(λx , λ) > 0. As can beseen, apart from minor differences, the possibilistic method is quite comparable to otherIBL methods from a complexity point of view. One such difference concerns the relevantsample instances. In the kNN approach, the number of relevant instances in always k,but the (degree of) relevance of an instance may change when modifying the case base. Asopposed to this, the degree of relevance of a neighboring instance is fixed in the possibilisticapproach, but the number of relevant instances can change.Let us finally mention that efficiency can also be gained if the complete possibilitydistribution δx0 is not needed. In fact, quite often one will only be interested in those labelshaving a high degree of possibility. For example, one might be interested in a fixed numberof maximally supported labels, or in those labels whose support exceeds a given possibilitythreshold. In such cases, the computation of δx0(λ) can be omitted (or broken off) forcertain labels λ.5. Possibilistic instance-based learningProceeding from the NN estimation (26), we have developed a possibilistic method ofinstance-based learning, called POSSIBL. This section presents some extensions of thebasic model which turn POSSIBL into a powerful and practically useful IBL algorithm.5.1. Dealing with incomplete informationThe problem of dealing with incomplete information such as missing attribute values inan important issue in machine learning [20,61]. For example, suppose that the specificationof the new instance x0 is incomplete, and let X0 ⊆ X denote the instances compatiblewith the description of x0. Moreover, recall the lower support-bound semantics of ourpossibilistic approach to IBL. The following generalization of (26) is in accordance withthese semantics:δx0(λ).= infx∈X0δx(λ) = infx∈X0(cid:8)1(cid:2)ı(cid:2)nσX (x, xı) ⊗ σL(λ, λxı ).(44)Indeed, each potential candidate x ∈ X0 gives rise to a lower bound according to (26),and without additional knowledge we can guarantee but the smallest of these bounds to be366E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383valid. This is in agreement with the idea of guaranteed possibility (cf. Section 2.2). Thesimplicity of handling incomplete information in a coherent (namely possibilistic) way isclearly a strong point of POSSIBL. Notice that the computation of the lower bound in(44) is in line with the handling of missing attribute values in IB1, where these valuesare assumed to be maximally different from the comparative value. Yet, the possibilisticsolution appears more appealing since it avoids any default assumption. Indeed, inferringwhat is possible seems to be a reasonable way of dealing with missing attribute values andfor handling incomplete and uncertain information in a coherent way.More generally, imprecise knowledge about x0 can be modeled in the form of apossibility distribution π on X , where π(x) corresponds to the degree of plausibility thatx0 = x. A graded modeling of this kind is useful, e.g., if some attributes are specified in alinguistic way. It suggests the following generalization of (44):δx0(λ)(cid:3).= infx∈Xπ(x) (cid:3) δx(λ)(cid:4),(45)where (cid:3) is a generalized implication operator that is reasonably chosen as the Gödelimplication [35]:(cid:6)α (cid:3) β.=if α (cid:1) β,1β if α > β.From a logical point of view, (45) specifies the extent to which the label λ is supported byall plausible candidates for x0. Notice that the distributions δx and π in (44) have differentsemantics and express degrees of confirmation and plausibility, respectively (cf. Section 2).Particularly, π is assumed to be normalized, i.e., there is at least one instance x withπ(x) = 1. One obviously recovers (44) from (45) for the special case where π is a {0, 1}-valued possibility distribution π = IX0 and hence corresponds to a crisp subset X0 ⊆ X .Similar generalizations can also be realized for coping with incompletely specifiedexamples. Let the ıth case in the memory be characterized by the set Xı × Lı ⊆ X × L.Then, (26) becomes(cid:8).=δx0(λ)inf(cid:19)x,λx(cid:20)∈Xı ×Lı1(cid:2)ı(cid:2)nσX (x0, x) ⊗ σL(λ, λx ),which is in accordance with (44). Moreover, we obtain.=δx0(λ)(cid:8)1(cid:2)ı(cid:2)n(cid:1)maxinf(cid:19)x,λx(cid:20)∈X ×LσX (x0, x) ⊗ σL(λ, λx ), 1 − πı(x, λx )(cid:2)if the ıth case is characterized by means of a possibility distribution πı on X × L ratherthan by a crisp set Xı × Lı . Observe that this expression can be combined with (45) inorder to handle incomplete specifications of both, the sample cases and the new instance.Moreover, notice that the distribution δx0 will generally remain unaffected if an exampleis completely unspecified (πı ≡ 1), which is clearly a reasonable property. See [27] fora more thorough discussion of handling incomplete information and for a more detailedderivation of the above extensions.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–3833675.2. Discounting noisy and atypical instancesIBL is quite sensitive to noisy instances which should hence be discarded [2]. By noiseone generally means incorrect attribute value information, concerning either the descriptivepart x of a case or the label λx (or both). However, the problem of noise is also closelyrelated to the “typicality” of a case. A typical instance is representative of its neighbors,whereas an exceptional (though not incorrect) instance has a label quite different from thelabels of neighboring instances [85].Recall that each case (cid:19)xı , λxı(cid:20) ∈ S is extrapolated by placing the support function or, say,“possibilistic kernel” (25) around the point (cid:19)xı , λxı(cid:20) ∈ X × L, just like a density (kernel)function is centered around each observation in kernel-based density estimation. Of course,the less representative (i.e., noisy or exceptional) an instance is of its neighborhood, thesmaller the extent of extrapolation should be.A simple learning mechanism that adapts the extent of extrapolation of stored cases can(cid:4)(cid:3)δıx0: λ (cid:22)→ mıσX (x0, xı)be realized by means of a slight generalization of the kernel function (25):⊗ σL(λ, λxı ).(46)Here, mı : [0, 1] → [0, 1] is a monotone increasing modifier function with mı(1) = 1. Thisfunction allows for discounting atypical cases. Roughly speaking, mı adapts the similaritybetween the instance xı and its neighbors. For example, xı is made completely dissimilarto all other instances by letting (mı|[0, 1[) ≡ 0. Replacing σX by the modified measuremı ◦ σX is closely related to the idea of local distance measures in NN algorithms.Suppose that a new observation x0 with label λx0 has been made, and consider a stored(cid:20). Should this case be discounted in the light of the new observation? The factcase (cid:19)xı, λxıthat (cid:19)xı, λxı(cid:20) supports a label different from the observed label λx0 need not necessarilydoes not exclude that x0 ∈ Cλ for some λ (cid:18)= λ0. Inbe a flaw. In fact, recall that x0 ∈ Cλx0other words, neither the non-support of the observed nor the support of a different label canactually be punished. However, what can be punished is the disqualification of the label λx0as expressed by the upper possibility model (31). Thus, it is reasonable to require that thedegree of disqualification induced by (cid:19)xı, λxı⊗ σL(λx0, λxı ) (cid:2) β,(cid:3)σX (x0, xı)(cid:20) is bounded:1 − mı(47)(cid:4)where β $ 0 is a constant.The constraint (47) suggests an update scheme in which a stored case (cid:19)xı, λxı(cid:20) is(cid:20) is made: Let F denote a(maybe) discounted every time a new observation (cid:19)x0, λx0parameterized and completely ordered class of functions from which mı is chosen. Anadaptation is then realized by(cid:1)mı ← min(cid:1)mı, sup(cid:3)(cid:4)(cid:2)(cid:2)f ∈ F | 1 − fσX (x0, xı)⊗ σL(λx0, λxı ) (cid:2) β.(48)The discounting of noisy and atypical instances through modifying possibilistic kernelfunctions appears natural and somewhat simpler than the method used in IB3 [2]. Firstly,possibilistic discounting is gradual, whereas an instance is either accepted or rejected (oris temporarily in-between) in IB3. Secondly, the question whether to discount an instanceand to which extent is answered quite naturally in the possibilistic approach, where supportis absolute and graded. In IB3, an instance is either punished or not, and the corresponding368E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383Fig. 2. Left: The large circle corresponds to the support function (possibilistic kernel) centered around xı andmarks the extrapolation of label λxı . Right: The support function is updated after observing a new instance whichhas a different label λx0(cid:18)= λxı and hence must not be supported.decision is based on a rule that appears reasonable but might still be considered ad-hoc(xı is discounted if DX (xı, x0) is smaller than or equal to the distance between x0 and itsclosest accepted neighbor32).The possibilistic adaptation scheme becomes rather simple for the special case X = Rl ,L = {0, 1} and mı = I]γı ,1], where 0 (cid:1) γı < 1. If σX is a strictly decreasing functionof Euclidean distance, then the support function (25) corresponds to a ball around xı :(λ) = 0 otherwise. Theδıx0parameter γı is chosen as large as possible, but such that the support function does not(cid:18)= λxı , that is γı (cid:1) |xı − x | holds true for all ofcover any observed instance x with λxthose x . Fig. 2 gives an illustration for l = 2.(λ) = 1 if λ = λx and x0 is located inside that ball and δıx0This special case is a useful point of departure for investigating theoretical properties ofPOSSIBL. In [4], some convergence properties of IB1 have been shown for a special setupwhich makes statistical assumptions about the generation of training data and geometricalassumptions on a concept C1 to be learned. For POSSIBL, one can prove similar propertiesunder the same assumptions. More specifically, let l = 2, X = [0, 1] × [0, 1] (the resultscan be generalized to any dimension l > 2 and any bounded region X ⊆ Rl) and considera concept C1 ⊆ X . For the special case above, the POSSIBL approximation of C1 is thengiven by(cid:18)Cest1=Bρ(xı )(xı),(cid:19)xı ,1(cid:20)∈Swhere Bd (xı) = {x ∈ X | |x − xı| < d} is the (open) d-ball around xı and(cid:1)ρ(xı) = min(cid:2)|x − xı| | (cid:19)x , λx.Moreover, the approximation of C0 = X \ C1 is given by(cid:20) ∈ S, λx(cid:18)= λxı(cid:18)Cest0=Bρ(xı )(xı).(cid:19)xı ,0(cid:20)∈S= ∅. However, Cest= X does not necessarily∩ CestIt is readily verified that Cest010≡ 0 for some instances x0 ∈ X (which are thenhold true. Thus, one may have δx0classified at random). Consequently, an approximation of concept C1 should actually be∪ Cest132 Auxiliary rules are used if x0 does not have an accepted neighbor.(49)(50)(51)E. Hüllermeier / Artificial Intelligence 148 (2003) 335–3833691 ) which divides instances x0 ∈ X into three groups:represented by the tuple (CestThose which (supposedly) belong to C1 (δx0(0) = 0, δx0(1) = 1), those which do not(δx0(0) = 1, δx0(1) = 0), and those for which no evidence is available so far (δx00 , Cest≡ 0).0 and Cest1Now, a first desirable property is the convergence of the concept approximation, that isthe convergence of Cesttoward C0 and C1, respectively. In this context, however,the property of convergence itself has to be weakened since exact convergence cannot beachieved due to the fact that an NN classifier cannot guarantee the avoidance of wrongdecisions at the boundary of a concept. Moreover, some assumptions on the generation ofsamples and on the geometry of the concept C1 have to be made. Here, we make the sameassumptions as in [4]: Instances are generated randomly and independently according to afixed probability measure µ over X . Furthermore, C1 is a concept having a nice boundary,which is the union of a finite number of closed (hyper-)curves of finite size.We employ the following notation: The ε-neighborhood of C1 is the setC+1 (ε).=(cid:2)(cid:1)x ∈ X | Bε(x) ∩ C1 (cid:18)= ∅,and the ε-core of C1 is defined by(cid:1)x ∈ X | Bε(x) ⊆ C1.=C−1 (ε)(cid:2).(cid:3)A set A ⊆ X is called an (ε, γ )-approximation of C1 if there is a (measurable) set N ⊆ Xwith µ(N) (cid:1) γ and such that−1 (ε) \ NCFinally, let Cest(49) and (51) for |S| = n, i.e., after n observations have been made.0,n denote, respectively, the possibilistic concept approximations1,n and Cest+1 (ε) \ NC⊆ (A \ N) ⊆(cid:4)(cid:3)(cid:4).Lemma 2. The equalities−+1 (ε) = X \ CC0 (ε)hold true for all 0 < ε < 1.and C−0 (ε) = X \ C+1 (ε)Proof. For x ∈ Cx1 ∈ C1. Consequently, there is no x0 ∈ C0 such that |x − x0| < ε and, hence, x /∈ CNow, suppose x ∈ X \ Cmeans that |x − x1| < ε implies x1 ∈ C1 and, hence, x ∈ Cshown in the same way. ✷−1 (ε) we have Bε(x) ⊆ C1, which means that |x − x1| < ε implies+0 (ε).+0 (ε). Thus, there is no x0 ∈ C0 such that |x − x0| < ε, which−1 (ε). The second equality isTheorem 3. Let C1 ⊆ X and 0 < ε, γ , d < 1. There is an integer n0 such that the followingholds true with probability at least 1 − d: The possibilistic concept approximation Cest1,n isa (2ε, γ )-approximation of C1 and Cest0,n is a (2ε, γ )-approximation of C0 for all n > n0.Proof. Let N denote the set of instances x ∈ X for which no sample xı ∈ S exists suchthat |x − xı| < ε. In [4], the following lemma has been shown: µ(N) (cid:1) γ holds true withprobability 1 − d whenevern > &n0 =2/ε(2/γ 2 · ln2/ε(2/d√(cid:3)&(cid:4).√(52)370E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383Subsequently, we ignore the set N , that is we formally replace X by X \ N , C1 by C1 \ Nand C0 by C0 \ N . Thus, the following holds true by definition: For each x ∈ X there is aninstance xı ∈ S such that |x − xı| < ε.Now, consider any instance x ∈ C1,n. Let xı ∈ Sbe an instance such that |x − xı| < ε. For this instance we have xı ∈ Bε(x) ⊆ C1, whichmeans that xı belongs to C1. Furthermore, Bε(xı) ⊆ B2ε(x) ⊆ C1 and, hence, ρ(xı) (cid:2) εfor the value in (50). This implies that x ∈ Bρ(xı )(xı) and, therefore, x ∈ Cest1,n. Thus, wehave shown that C−1 (2ε). We have to show that x ∈ Cest−1 (2ε) ⊆ Cest1,n.Since the same arguments apply to C0, the property C0,n can be shown in an−0 (2ε) ⊆ Cestanalogous way. Thus, using Lemma 2,Cest1,n⊆ X \ Cest0,n⊆ X \ CLikewise, one shows that Cest0,n+1 (2ε).−0 (2ε) = C+0 (2ε). ✷⊆ CRoughly speaking, Theorem 3 guarantees that the 2ε-core of both, C0 and C1 isclassified correctly (with high probability) if the sample S is large enough. In other words,classification errors can only occur in the boundary region. For being able to quantify theprobability of an error, it is necessary to put restrictions on the size of that boundary regionand on the probability distribution µ. Thus, let C denote the class of concepts C1 ⊆ X thatcan be represented as the union of a finite set of regions bounded by closed curves with totallength of at most L [4]. Moreover, let Pβ denote the class of probability distributions µover X such that µ(A) (cid:1) µL(A) · β for all Borel-subsets A ⊆ X , where µL is the Lebesguemeasure and β > 0.Theorem 4. The concept class C is polynomially learnable with respect to Pβ by means of0 , Cestthe possibilistic concept approximation (Cest1 ).+1 (2ε) \ C−Proof. If C1 ∈ C, then the size of the region C1 (2ε) is bounded by 4εL.Consequently, the probability of that area is at most α = 4εLβ. Since a classificationerror can only occur either in this region or in the set N as defined in Theorem 3 andthe probability of N is at most γ , the probability of a classification error is bounded byα + γ . Now, fix the parameters γ and ε as follows: γ = e/2, ε = e/(8Lβ). By substitutingthese parameters into (52) one finds that the required sample size n is polynomial in 1/eand 1/d. In summary, the following holds true for any 0 < e, d < 1, C1 ∈ C, and µ ∈ Pβ :If more than n(1/e, 1/d) examples are presented, where n is a polynomial function of1/e and 1/d, then, with probability 1 − d, the possibilistic concept approximation has aclassification error of at most e. This is precisely the claim of the theorem. ✷5.3. From instances to rulesSelecting appropriate instances to be stored in memory and pruning the training set areimportant issues in IBL that have a strong influence on performance. Especially reducingthe size of the memory is often necessary in order to maintain the efficiency of the system.The basic idea is to remove instances which are actually not necessary to achieve goodE. Hüllermeier / Artificial Intelligence 148 (2003) 335–383371concept descriptions. For example, imagine a concept having the form of a circle in some(two-dimensional) instance space. To classify inner points correctly by means of the kNNrule it might then be sufficient to store positive examples of that concept near the boundary.In connection with POSSIBL, where support is absolute rather than relative, deletinginstances from memory might produce “holes” in the concept description. An interestingalternative, which allows one to reduce the size of the memory and, at the same time, tofill “holes” in the concept description by interpolation, is based on the idea of merginginstances and of generalizing cases into rules. This idea appears particularly reasonablesince the possibilistic estimation principle is closely related to fuzzy rule-based reasoning.More precisely, each observation can be interpreted as a fuzzy rule, namely as an instanceof a fuzzy meta-rule suggesting that similar instances have similar labels.∨ δ2x0To illustrate the one-to-one correspondence between rules and cases in POSSIBL, letX = R, L = {0, 1} and suppose that two instances x1 = 4 and x2 = 6 with label 0 havebeen observed. The possibilistic kernels (25) induced by these cases are shown in Fig. 3.The first case is equivalent to the fuzzy rule “If x0 is approximately 4 then λ = 0” if thefuzzy set “approximately 4” is modeled by the possibility distribution δ1(the individualx0support function (25)). The rules associated with the two cases can be merged into onerule, say, “If x0 is about 5 then λ = 0”, where the fuzzy set “about 5” is modeled by theand δ2pointwise maximum, δ1x0x0(Fig. 3, right)., of δ1x0The above procedure is closely related to several other techniques that have beenproposed in connection with IBL. Viewing cases as maximally specific rules and the ideaof generalizing cases into rules has been put forward in [21,22]. The method proposedin [64] generalizes cases by placing rectangles of different size around them. A newinstance is then labeled by the nearest rectangle rather than by the nearest case. This isvery similar to our approach, where rectangles are replaced by possibility distributions.Relations also exist with the idea of merging nearest neighbors of the same class, therebygenerating new (pseudo-sample) prototypes [11]. In our example, the point 5 may beregarded as a pseudo-instance replacing 4 and 6 (and also endowed with a modified supportfunction).In the example in Fig. 3, the summarizing rule is exactly equivalent to the conjunction ofthe two individual rules. Of course, the merging procedure might also incorporate conceptsof approximation and interpolation. For example, suppose x2 = 8 rather than x2 = 6. The(x), I[5,7]} then goesreplacement of δ1x0beyond a simple combination since δ is larger than the pointwise maximum of δ1and δ2x0x0(6) = 0.5 < 1 = δ(6)). This kind of possibilistic induction can be rea-(e.g., δ1x0sonable and often allows for incorporating background knowledge. Particularly, replacingby its convex hull δ : x (cid:22)→ max{δ1x0(6) = δ2x0(x), δ2x0∨ δ2x0Fig. 3. Possibility distributions induced by two cases (left, middle) and the distribution associated with thesummarizing fuzzy rule (right).372E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383a possibilistic estimation δx0 by its convex hull is advised whenever a multimodal distribu-tion does not make sense (as in our example in Section 4.5) or if the relation of observablecases (cf. Section 3.1) is even known to satisfy a convexity constraint of the formx ∈ Cλ ∩ Cλ(cid:17)(cid:17) ⇒ x ∈ Cλ(cid:17)for all λ < λ(cid:17) < λ(cid:17)(cid:17).As can be seen, the extensions discussed here basically suggest a system that maintainsan optimal rule base rather than an optimal case base, including the combination andadaptation of rules. These extensions are well-suited to the discounting of instancesdiscussed in Section 5.2. Indeed, deriving one rule from several instances (or otherrules) can be accomplished by replacing the latter by a pseudo-instance and definingan appropriate modifier function m for that pseudo-instance. Still, the extensions in thisdirection are premature and have not been implemented in POSSIBL yet.6. Experimental studies6.1. PreliminariesThis section presents some experimental studies providing evidence for POSSIBL’sexcellent performance in practice. We would like to emphasize, however, that it isnot meant as an exhaustive comparative study covering several competing learningalgorithms—and showing that POSSIBL is superior to all of its competitors. Apart fromthe fact that empirical studies are clearly of limited evidence,33 one should realize that theprimary motivation underlying POSSIBL is not another ε-improvement in classificationaccuracy but rather the enrichment of IBL by concepts of possibilistic reasoning (thoughthe latter does clearly not exclude the former). Besides, one should keep the followingpoints in mind. Firstly, POSSIBL has not been developed within a statistical framework.Thus, the type of problems for which POSSIBL is most suitable (see the example inSection 4.5) is perhaps not represented in the best way by standard (public) data setscommonly used for testing performance. Secondly, an important aspect of the possibilisticapproach is the one of knowledge representation. But this aspect is neglected if—as inexperimental studies—only the correctness of the final decision (classification accuracy)counts, not the estimated distribution. Thirdly, a comparison with other IBL algorithmsmight appear dubious since POSSIBL—in its most general form—is an extension of IBLand hence covers specific algorithms such as kNN as special cases.Due to these difficulties, we have decided to apply a basic version of POSSIBL to severaldata sets from the UCI repository and to employ the kNN (resp. IB1) algorithm as areference (we use kNN with k = 1, 3, 5 and the weighted 5NN rule with weight function(10)). Thus, we have refrained from tuning various degrees of freedom in order to optimizethe performance of POSSIBL (an exception is only the experimental study presented inSection 6.4). Instead, we have applied the original max–min version (20), only extended33 It is well known that each algorithm has a selective superiority [10]. Thus, one will always find data sets forwhich a certain algorithm, at least after being tuned appropriately, performs better than others.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383373by the learning scheme presented in Section 5.2. The function mı in (46) was defined ast (cid:22)→ exp(−γı(1 − t)), where γı (cid:2) 0 is the discounting rate of the ıth instance. The constantβ in (47) was taken as 0.8.34 In order to avoid difficulties due to the different handlingof non-nominal class labels and the definition of similarity measures for non-numericattributes, we have restricted ourselves to data sets for which all predictive attributes arenumeric and for which the class label is defined on a nominal scale. The similarity σX isalways defined as 1 minus the normalized Euclidean distance and the similarity σL is givenby (23).6.2. Classification accuracyThe experiments in this section were performed as follows: In a single simulation run,the data set is divided at random into a training set (the case base) and a test set, andthe discounting rates γı are adapted to the training set. A decision is then derived foreach element of the test set by extrapolating the training set (but without adapting thediscounting rates or expanding the case base any further), and the percentage of correctdecisions is determined. Statistics are obtained by means of repeated simulation runs.Results are summarized by means of statistics for the percentage of correct classifica-tions (mean, standard deviation, minimum, maximum, 0.1-fractile, 0.9-fractile) (see Ta-bles 1–5).The experiments show that POSSIBL achieves comparatively good results and is alwaysamong the best algorithms. Thus, it can be said that a basic version of POSSIBL performsTable 1BALANCE SCALE DATABASE (625 observations, 4 predictive attributes, threeclasses, training set of size 300, 1,000 simulation runs)Algorithmmeanstd.minmax0.1-frac.0.9-frac.POSSIBL1NN3NN5NNW5NN0.87760.78370.81170.84920.78640.01480.01610.01650.01550.01640.82150.73230.76300.80300.72940.92300.83690.87070.89230.84280.85840.76300.79070.83070.76550.89840.80300.83380.87070.8067Table 2IRIS PLANT DATABASE (150 observations, 4 predictive attributes, three classes,training set of size 75, 10,000 simulation runs)Algorithmmeanstd.minmax0.1-frac.0.9-frac.POSSIBL1NN3NN5NNW5NN0.95740.94920.95540.95860.95610.02040.01960.01750.01810.01870.84000.84000.86660.85330.84001.00001.00001.00001.00001.00000.93330.92000.93330.93330.93330.97330.97330.97330.98660.973334 Variations of this parameter had no significant influence.374E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383Table 3GLASS IDENTIFICATION DATABASE (214 observations, 9 predictive attributes,seven classes, training set of size 100, 10,000 simulation runs)Algorithmmeanstd.minmax0.1-frac.0.9-frac.POSSIBL1NN3NN5NNW5NN0.68410.68700.64410.62770.67770.04190.04100.04210.04120.04140.53000.52000.48000.48000.50000.84000.82000.81000.78000.83000.63000.63000.59000.57000.62000.74000.74000.70000.68000.7300Table 4PIMA INDIANS DIABETES DATABASE (768 observations, 8 predictive attributes,two classes, training set of size 380, 1,000 simulation runs)Algorithmmeanstd.minmax0.1-frac.0.9-frac.POSSIBL1NN3NN5NNW5NN0.70960.67070.69990.71900.69480.01900.01990.01830.01830.01880.64210.61320.64470.65530.64210.77110.72890.75000.76840.74740.68680.64470.67630.69470.66840.73160.69470.72370.74210.7184Table 5WINE RECOGNITION DATA (178 observations, 13 predictive attributes, threeclasses, training set of size 89, 1,000 simulation runs)Algorithmmeanstd.minmax0.1-frac.0.9-frac.POSSIBL1NN3NN5NNW5NN0.71480.71630.68840.69400.70310.04090.04080.04070.03920.04040.55060.58430.55060.57300.57300.86520.86520.83150.80900.83150.66290.66290.64040.64040.65170.76400.76400.74160.74160.7528at least as well as the basic IBL (NN) algorithms. In other words, possibilistic IBL is inno way inferior to “standard” IBL as a basis for further improvements and sophisticatedlearning algorithms. This is exactly what we wanted to show.Due to the special setting of our experimental studies, especially the choice of max as anaggregation operator and the use of a {0, 1}-valued similarity measure over L, one mightwonder how to explain the different performance of POSSIBL and the NN classifiers. Infact, in Section 4.4 it was argued that the possibilistic NN decision derived from (20) isactually equivalent to the 1NN rule when applying the maximum operator. It should hencebe recalled that POSSIBL, as employed in the above experiments, involves an adaptationof the (absolute) possibilistic support that comes from stored cases, which in essence isresponsible for the differences.A very interesting finding is the following: In the above examples, classificationperformance of the kNN algorithm is generally an increasing or a decreasing functionof k. POSSIBL, on the other hand, performs very well irrespective of the direction of thatE. Hüllermeier / Artificial Intelligence 148 (2003) 335–383375tendency, i.e., regardless of whether a smaller or a larger neighborhood should be calledin. This can be taken as an indication of the robustness of the possibilistic approach.6.3. Statistical assumptions and robustnessLet us elaborate a little more closely on the aspect of robustness. Above, it has beenclaimed that the possibilistic approach is more robust than other methods against violationsof statistical assumptions of independence (see end of Section 4.3.2). This is clearly truefor the possibilistic estimation δx0 the informational content of which remains meaningfuleven if data is not independent. Here, we would like to provide experimental evidence forthe supposition that the possibilistic approach can indeed be advantageous from both, anestimation and a decision making point of view, if the sample is not fully representative ofthe population.The experimental setup is determined as follows: The instance space is defined byX = R, the set of labels is L = {−1, +1}, the class probabilities are 1/2, the conditionalprobability density of x given λx is normal with standard deviation 1 and mean λx .In a single simulation run, a random sample of size n = 20 is generated, using class-probabilities of 1/2 − α and 1/2 + α, respectively (0 < α (cid:1) 1/2). Based on the resultingtraining set, which is not “fully representative” in the sense of [17], predictions arederived for 10 new instances. These instances, however, are generated with the true class-probabilities of 1/2. For a fixed value α and a fixed prediction method, a misclassificationrate r(α) is derived by averaging over 10,000 simulation runs.Fig. 4 shows the misclassification rates for several methods. As was to be expected, r(·)is an increasing function of the sample bias α. The best results are of course obtained ifthe class-probabilities of the training set and the test set coincide, that is for α = 0. Thefigure also reveals that the sensitivity of the kNN classifier increases with k. On the onehand, it is true that a larger k leads to better results for α close to 0. On the other hand,the performance decreases more quickly than for smaller k, and k = 1 is to be preferredfor α close to 1/2. This finding can also be grasped intuitively: The larger k, the more thekNN rule relies on frequency information, and the more it is affected if this information ismisleading.Fig. 4. Misclassification rates of kNN methods (left) and POSSIBL (right, in comparison with 1NN).376E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383Apart from kNN methods, we have tested POSSIBL with ⊕ = ⊕P . The similaritymeasure σX was defined by the triangle (x, y) (cid:22)→ max{0, 1 − |x − y|/0.8}. Interestinglyenough, this approach yields the most satisfactory results. For α close to 0 it is almost asgood as the kNN rules with k > 1, and for α close to 1/2 it equals the 1NN rule. Thus, thecombination mode as realized by the probabilistic sum (α, β) (cid:22)→ α + β − αβ turns out tobe reasonable under the conditions of this experiment. As already explained in Section 4.2,this operator produces a kind of saturation effect: It takes frequency information intoaccount, but only to a limited extent (the larger the current support already is, the smallerthe absolute increase due to a new observation). Thus, it is indeed in-between the 1NN ruleand the kNN rules for k > 1. Intuitively, this explains our findings in the above experiment,especially that POSSIBL is more robust against the sample bias than kNN rules for k > 1.6.4. Variation of the aggregation operatorAn interesting question concerns the dependence of POSSIBL’s performance on thespecification of the aggregation operator ⊕ in (25). To get a first idea of this dependence,we have performed the same experiments as described in Section 6.2 above. Now, however,we have tested POSSIBL with different t-conorms.More precisely, we have specified a t-conorm by means of the parameter ρ in (30),i.e., we have taken different aggregation operators from the Frank-family of t-conorms.POSSIBL was then applied to each data set with different operators ⊕ρ . The simulationresults are presented in Figs. 5–9. Each figure shows the average classification performanceof POSSIBL (over 100 experiments) as a function of the parameter ρ. Please note thedifferent scaling of the axes for the five data sets.Confirming our previous considerations, the results show that in general different t-conorms are optimal for different applications. Still, POSSIBL’s performance is quiterobust toward the variation of the aggregation operator. That is, classification accuracydoes not drop off too much when choosing a suboptimal operator.A very interesting finding is the observation that the parameter ρ = 0 and, hence,the maximum operator is optimal if simultaneously the 1NN classifier performs well incomparison with other kNN classifiers. If this is not the case, as, e.g., for the BALANCEFig. 5. Experimental results for the BALANCE SCALE data.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383377Fig. 6. Experimental results for the IRIS PLANT data.Fig. 7. Experimental results for the GLASS IDENTIFICATION data.Fig. 8. Experimental results for the PIMA INDIAN DIABETES data.378E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383Fig. 9. Experimental results for the WINE RECOGNITION data.SCALE and the PIMA INDIANS DIABETES data, parameters ρ > 0 achieve better results.This finding is not astonishing and can also be grasped intuitively. In fact, it was alreadymentioned that POSSIBL with ⊕ = ⊕0 = max is closely related to the 1NN classifier, asboth methods do fully concentrate on the most relevant information. As opposed to this,aggregation operators ⊕ = ⊕ρ with ρ > 0 combine the information from several neighborsin much the same way as do kNN classifiers with k > 1.6.5. Representation of uncertaintyIt was already mentioned that an important aspect of POSSIBL concerns the representa-tion of uncertainty. The fact that POSSIBL can adequately represent the ignorance relatedto a decision problem is easily understood and does not call for empirical validation. Toget a first idea of POSSIBL’s ability to represent ambiguity we have derived approxima-tions to two characteristic quantities, again using the experimental setup as described inSection 6.1.Let D1 denote the expected difference between the possibility degree of the predictedand the possibility degree of the second best label, given that the prediction islabel λestx0correct:D1.= δx0(λx0) − maxλ∈L, λ(cid:18)=λx0δx0(λ).Moreover, let D0 denote the expected difference between the possibility degree of thepredicted label λestand the possibility degree of the actually true label λx0 , given thatx0λx0:.= δx0− δx0(λx0).(cid:18)= λestx0D0(cid:3)(cid:4)λestx0Ideally, D0 is small and D1 is large: Wrong decisions are accompanied by a large degree ofuncertainty, as reflected by a comparatively large support of the actually correct label. Asopposed to this, correct decisions appear reliable, as reflected by low possibility degreesassigned to all labels λ (cid:18)= λx0 .E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383379Table 6DatabaseD00,094BALANCE SCALE0,194IRIS PLANTGLASS IDENTIFICATION0,181PIMA INDIANS DIABETES 0,2110,226WINE RECOGNITIOND10,5290,6930,4010,4920,721Table 6 shows approximations to the expected values D0 and D1, namely averages over1,000 experiments. As can be seen, the reliability of a prediction is reflected very well bythe possibilistic estimations.7. Summary and future workThe idea underlying the method presented in this paper is to extend instance-basedlearning by concepts and techniques from possibility theory and fuzzy sets. Here, this ideahas been realized in the form of a basic learning procedure called POSSIBL. Apart fromdiscussing methodological aspects, the paper has started the investigation of theoreticalproperties of this approach (under standard statistical assumptions) and the validation ofPOSSIBL by means of experimental studies.The application of possibility theory allows for realizing a graded version of thesimilarity-based extrapolation principle underlying IBL. Not only does this version appearvery natural, it is also intuitively appealing. We have presented a detailed comparisonof the possibilistic extrapolation principle and the commonly used approach which canbe endowed with a probabilistic basis. Even though the two methods are based on quitedifferent semantics, POSSIBL can formally be seen as an extension of the probabilisticapproach. Indeed, it has been shown that the former—at least in its general form—canmimic the latter. Apart from that, the possibilistic approach has the following advantages:• Knowledge representation: A possibilistic (instance-based) prediction is more expres-sive than a probabilistic one. Especially, the former is able to represent the absoluteamount of evidential support as well as partial ignorance, a point which seems to beof major importance in IBL. Furthermore, the interpretation of aggregated degreesof individual support in terms of (guaranteed) possibility (degrees of confirmation) isgenerally less critical than the interpretation in terms of degrees of probability.• Scope for applications: The possibilistic approach is more robust and extends the rangeof applications. Particularly, it makes no statistical assumptions about the generation ofdata and less mathematical assumptions about the structure of the underlying instancespace. In fact, POSSIBL performs at least as well as standard NN techniques fortypical (real-word) data sets. Beyond that, however, it can also be applied to data thatviolates certain statistical assumptions. Finally, the max–min version of POSSIBL caneven be applied within a purely ordinal setting.• Support of extensions: The possibilistic method is more flexible and supports severalextensions of IBL. This includes the adaptation of aggregation modes in the380E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383combination of individual degrees of support, the coherent handling of incompleteinformation, and the graded discounting of atypical cases. Moreover, it allowsone to complement the similarity-based extrapolation principle by other inferenceprocedures.In the paper, we have outlined some extensions of the basic POSSIBL algorithm whichdeserve further investigation. This concerns particularly the ideas to automatically adapta parameterized aggregation operator (Section 4.2.2) and to complement lower possibilitybounds by means of upper bounds (Section 4.2), as well as the combination of instance-based and rule-based inference (Section 5.3). These extensions are important topics ofongoing research, which aims at realizing an efficient framework of plausible instance-based learning on the basis of possibility theory and fuzzy sets. In this regard, let usagain mention the idea of supplementing IBL with fuzzy set-based modeling techniques.In fact, the methods in [27] allow for guiding and extending instance-based learning bymeans of domain knowledge and, thus, for combining knowledge and data in a flexibleway. Parts of the possibilistic IBL framework have already been realized in connectionwith the PRETI project (Platform of Research and Experimentation in the Treatmentof Information) maintained at the INSTITUT DE RECHERCHE EN INFORMATIQUE DETOULOUSE.AcknowledgementsThe author wishes to thank three anonymous referees for useful comments which helpedto improve the paper.References[1] D.W. Aha, Incremental, instance-based learning of independent and graded concept descriptions, in: Proc.6th Internat. Workshop on Machine Learning, Ithaca, NY, Morgan Kaufmann, San Mateo, CA, 1989,pp. 387–391.[2] D.W. Aha, Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms, Internat.J. Man-Machine Stud. 36 (1992) 267–287.[3] D.W. Aha (Ed.), Lazy Learning, Kluwer Academic, Dordrecht, 1997.[4] D.W. Aha, D. Kibler, M.K. Albert, Instance-based learning algorithms, Machine Learning 6 (1) (1991) 37–66.[5] T. Bailey, A.K. Jain, A note on distance-weighted k-nearest neighbor rules, IEEE Trans. Systems ManCybernet. 8 (4) (1978) 311–313.[6] M. Béreau, B. Dubuisson, A fuzzy extended k-nearest neighbors rule, Fuzzy Sets and Systems 44 (1991)17–32.[7] J.C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithm, Plenum Press, New York, 1981.[8] J.C. Bezdek, K. Chuah, D. Leep, Generalized k-nearest neighbor rules, Fuzzy Sets and Systems 18 (1986)237–256.[9] R. Bradley, N. Swartz, Possible Worlds, Basil Blackwell, Oxford, UK, 1979.[10] C.E. Brodley, Addressing the selective superiority problem: Automatic algorithm for model class selection,in: Proc. 10th Machine Learning Conference, 1993, pp. 17–24.[11] C.L. Chang, Finding prototypes for nearest neighbor classifiers, IEEE Trans. Comput. 23 (11) (1974) 1179–1184.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383381[12] C.K. Chow, On optimum recognition error and reject tradeoff, IEEE Trans. Inform. Theory 16 (1970) 41–46.[13] L.J. Cohen, An Introduction to the Philosophy of Induction and Probability, Clarendon Press, Oxford, 1989.[14] T.M. Cover, P.E. Hart, Nearest neighbor pattern classification, IEEE Trans. Inform. Theory 13 (1967) 21–27.[15] B.V. Dasarathy, Nosing around the neighborhood: A new system structure and classification rule forrecognition in partially exposed environments, IEEE Trans. Pattern Analysis and Machine Intelligence 2 (1)(1980) 67–71.[16] B.V. Dasarathy (Ed.), Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, IEEEComputer Society Press, Los Alamitos, CA, 1991.[17] E.R. Davies, Training sets and a priori probabilities with the nearest neighbor method of patternclassification, Pattern Recognition Lett. 8 (1) (1988) 11–13.[18] R. Lopez de Mantaras, E. Armengol, Machine learning from examples: Inductive and lazy methods, DataKnowledge Engrg. 25 (1998) 99–123.[19] T. Denoeux, A k-nearest neighbor classification rule based on Dempster–Shafer Theory, IEEE Trans.Systems Man Cybernet. 25 (5) (1995) 804–813.[20] J.K. Dixon, Pattern recognition with partly missing data, IEEE Trans. Systems Man Cybernet. 9 (10) (1979)617–621.[21] P. Domingos, Rule induction and instance-based learning: A unified approach, in: C.S. Mellish (Ed.), Proc.IJCAI-95, Montreal, Quebec, Morgan Kaufmann, San Mateo, CA, 1995, pp. 1226–1232.[22] P. Domingos, Unifying instance-based and rule-based induction, Machine Learning 24 (1996) 141–168.[23] D. Dubois, F. Esteva, P. Garcia, L. Godo, R. Lopez de Mantaras, H. Prade, Fuzzy set modelling in case-basedreasoning, Internat. J. Intelligent Syst. 13 (1998) 345–373.[24] D. Dubois, P. Hajek, H. Prade, Knowledge driven vs. data driven logics, J. Logic Language Inform. 9 (2000)65–89.[25] D. Dubois, E. Hüllermeier, H. Prade, Flexible control of case-based prediction in the framework ofpossibility theory, in: E. Blanzieri, L. Portinale (Eds.), Advances in Case-Based Reasoning, Proc. EWCBR-2000, 5th European Workshop on Case-Based Reasoning, Trento, Italy, Springer, Berlin, 2000, pp. 61–73.[26] D. Dubois, E. Hüllermeier, H. Prade, Formalizing case-based inference using fuzzy rules, in: S.K. Pal,D.Y. So, T. Dillon (Eds.), Soft Computing in Case-Based Reasoning, Springer, Berlin, 2000, pp. 47–72.[27] D. Dubois, E. Hüllermeier, H. Prade, Fuzzy set-based methods in instance-based reasoning, IEEE Trans.Fuzzy Systems 10 (3) (2002) 322–332.[28] D. Dubois, H. Prade, Fuzzy sets and statistical data, European J. Oper. Res. 25 (1986) 345–356.[29] D. Dubois, H. Prade, Possibility Theory, Plenum Press, New York, 1988.[30] D. Dubois, H. Prade, Possibility theory as a basis for preference propagation in automated reasoning, in:Proc. 1st IEEE Internat. Conference on Fuzzy Systems (FUZZ-IEEE-92), San Diego, CA, 1992, pp. 821–832.[31] D. Dubois, H. Prade, When upper probabilities are possibility measures, Fuzzy Sets and Systems 49 (1992)65–74.[32] D. Dubois, H. Prade, What are fuzzy rules and how to use them, Fuzzy Sets and Systems 84 (1996) 169–185.[33] D. Dubois, H. Prade, Possibility theory: Qualitative and quantitative aspects, in: D.M. Gabbay, P. Smets(Eds.), Handbook of Defeasible Reasoning and Uncertainty Management Systems, Vol. 1, KluwerAcademic, Dordrecht, 1998, pp. 169–226.[34] D. Dubois, H. Prade, P. Smets, Not impossible vs. guaranteed possible in fusion and revision, in: Proc.ESCQARU-2001, Toulouse, France, in: Lecture Notes in Comput. Sci, Vol. 2143, Springer, Berlin, 2001,pp. 522–531.[35] D. Dubois, H. Prade, L. Ughetto, A new perspective on reasoning with fuzzy rules, in: N.R. Pal, M. Sugeno(Eds.), Advances in Soft Computing, Proc. AFSS International Conference on Fuzzy Systems, Calcutta,India, in: Lecture Notes in Artificial Intelligence, Vol. 2275, Springer, Berlin, 2002, pp. 1–11.[36] B. Dubuisson, M. Masson, A statistical decision rule with incomplete knowledge about classes, PatternRecognition 26 (1) (1993) 155–165.[37] S.A. Dudani, The distance-weighted k-nearest-neighbor rule, IEEE Trans. Systems Man Cybernet. 6 (4)(1976) 325–327.[38] E. Fix, J.L. Hodges, Discriminatory analysis: nonparametric discrimination: consistency principles, in:B.V. Dasarathy (Ed.), Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, IEEEComputer Society Press, Los Alamitos, CA, 1991. Reprint of original work from 1951.382E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383[39] E. Fix, J.L. Hodges, Discriminatory analysis: Nonparametric discrimination: Small sample performance,in: B.V. Dasarathy (Ed.), Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, IEEEComputer Society Press, Los Alamitos, CA, 1991. Reprint of original work from 1952.[40] J.H. Friedman, F. Baskett, L.J. Shustek, An algorithm for finding nearest neighbors, IEEE Trans. Comput. 24(1975) 1000–1006.[41] P.E. Hart, The condensed nearest neighbor rule, IEEE Trans. Inform. Theory 14 (1968) 515–516.[42] M.E. Hellman, The nearest neighbor classification rule with a reject option, IEEE Trans. Systems ManCybernet. 6 (1970) 179–185.[43] E. Hüllermeier, Toward a probabilistic formalization of case-based inference, in: T. Dean (Ed.), Proc. IJCAI-99, Stockholm, Sweden, Morgan Kaufmann, San Mateo, CA, 1999, pp. 248–253.[44] E. Hüllermeier, On the representation and combination of evidence in instance-based learning, in: Proc.ECAI-2002, 15th European Conference on Artificial Intelligence, Lyon, France, IOS Press, Amsterdam,2002, pp. 360–364.[45] D. Hume, An Enquiry concerning Human Understanding, Oxford University Press, New York, 1999.[46] A. Józwik, A learning scheme for a fuzzy k-NN rule, Pattern Recognition Lett. 1 (1983) 287–289.[47] J.M. Keller, M.R. Gray, J.A. Givens, A fuzzy k-nearest neighbor algorithm, IEEE Trans. Systems ManCybernet. 15 (4) (1985) 580–585.[48] D. Kibler, D.W. Aha, Instance-based prediction of real-valued attributes, Comput. Intelligence 5 (1989)51–57.[49] B.S. Kim, S.B. Park, A fast k nearest neighbor finding algorithm based on the ordered partition, IEEE Trans.Pattern Analysis and Machine Intelligence 8 (6) (1985) 761–766.[50] J.L. Kolodner, Case-based Reasoning, Morgan Kaufmann, San Mateo, CA, 1993.[51] R. Krishnapuram, J.M. Keller, A possibilistic approach to clustering, IEEE Trans. Fuzzy Systems 1 (2)(1993) 98–110.[52] D.K. Lewis, Counterfactuals and comparative possibility, J. Philos. Logic 2 (1973).[53] D.O. Loftsgaarden, C.P. Quesenberry, A nonparametric estimate of a multivariate density function, Ann.Math. Stat. 36 (1965) 1049–1051.[54] J. Macleod, A. Lik, D. Titterington, A re-examination of the distance-weighted k-nearest neighborclassification rule, IEEE Trans. Systems Man Cybernet. 17 (4) (1987) 689–696.[55] E. McKenna, B. Smyth, Competence-guided edition methods for lazy learning, in: Proc. 14th EuropeanConference on Artificial Intelligence (ECAI-2000), Berlin, 2000, pp. 60–64.[56] T.M. Mitchell, The need for biases in learning generalizations, Technical Report TR CBM-TR-117, RutgersUniversity, New Brunswick, NJ, 1980.[57] I. Niiniluoto, Analogy and similarity in scientific reasoning, in: D.H. Helman (Ed.), Analogical Reasoning,Kluwer Academic, Dordrecht, 1988, pp. 271–298.[58] G. Parthasarathy, B.N. Chatterji, A class of new KNN methods for low sample problems, IEEE Trans.Systems Man Cybernet. 20 (3) (1990) 715–718.[59] E. Parzen, On estimation of a probability density function and mode, Ann. Math. Statist. 33 (1962) 1065–1076.[60] E.A. Patrick, F.P. Fischer, A generalized k-nearest neighbor rule, Inform. and Control 16 (2) (1970) 128–152.[61] J.R. Quinlan, Unknown attribute values in induction, in: Proc. 6th International Workshop on MachineLearning, Morgan Kaufmann, San Mateo, CA, 1989, pp. 164–168.[62] R. Quinlan, Combining instance-based and model-based learning, in: Proc. 10th International Conferenceof Machine Learning, Morgan Kaufmann, San Mateo, CA, 1993, pp. 236–243.[63] M. Rosenblatt, Remarks on some nonparametric estimates of a density function, Ann. Math. Statist. 27(1956) 832–837.[64] S. Salzberg, A nearest hyperrectangle learning method, Machine Learning 6 (1991) 251–276.[65] E. Sanchez, On possibility qualification in natural languages, Inform. Sci. 15 (1978) 45–76.[66] G. Shafer, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ, 1976.[67] D. Shepard, A two-dimensional interpolation function for irregularly spaced data, in: Proc. 23rd NationalConference of the ACM, 1968, pp. 517–523.[68] B.W. Silverman, Density Estimation for Statistics and Data Analysis, Chapman and Hall, London, 1986.[69] P. Smets, R. Kennes, The transferable belief model, Artificial Intelligence 66 (1994) 191–234.[70] C. Stanfill, D. Waltz, Toward memory-based reasoning, Comm. ACM (1986) 1213–1228.E. Hüllermeier / Artificial Intelligence 148 (2003) 335–383383[71] M. Tan, Cost-sensitive learning of classification knowledge and its application to robotics, MachineLearning 13 (7) (1993) 7–34.[72] I. Tomek, A generalization of the k-NN rule, IEEE Trans. Systems Man Cybernet. 6 (1976) 121–126.[73] V.N. Vapnik, Statistical Learning Theory, Wiley, New York, 1998.[74] M.P. Wand, M.C. Jones, Kernel Smoothing, Chapman and Hall, London, 1995.[75] J. Weisbrod, A new approach to fuzzy reasoning, Soft Comput. 2 (1998) 89–99.[76] S. Wess, K.D. Althoff, G. Derwand, Using k-d trees to improve the retrieval step in case-based reasoning,in: S. Wess, K.D. Althoff, M.M. Richter (Eds.), Topics in Case-Based Reasoning, Springer, Berlin, 1994,pp. 167–181.[77] D. Wettschereck, D.W. Aha, T. Mohri, A review and empirical comparison of feature weighting methods fora class of lazy learning algorithms, AI Rev. 11 (1997) 273–314.[78] D.L. Wilson, Asymptotic properties of nearest neighbor rules using edited data, IEEE Trans. Systems ManCybernet. 2 (3) (1972) 408–421.[79] D.R. Wilson, Advances in instance-based learning algorithms, PhD Thesis, Department of ComputerScience, Brigham Young University, Provo, UT, 1997.[80] D.R. Wilson, T.R. Martinez, Improved heterogeneous distance functions, J. Artificial Intelligence Res. 6(1997) 1–34.[81] T.P. Yunck, A technique to identify nearest neighbors, IEEE Trans. Systems Man Cybernet. 6 (10) (1976)678–683.[82] L.A. Zadeh, Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets and Systems 1 (1978) 3–28.[83] L.A. Zadeh, PRUF: A meaning representation language for natural language, Internat. J. Man-MachineStud. 10 (1978) 395–460.[84] L.A. Zadeh, Toward a theory of fuzzy information granulation and its centrality in human reasoning andfuzzy logic, Fuzzy Sets and Systems 90 (2) (1997) 111–127.[85] J. Zhang, Selecting typical instances in instance-based learning, in: Proc. 9th International Conference onMachine Learning (ICML-92), Aberdeen, Scotland, 1992, pp. 470–479.[86] J. Zhang, Y. Yim, J. Yang, Intelligent selection of instances for prediction in lazy learning algorithms,Artificial Intelligence Rev. 11 (1997) 175–191.