NeuroSense: Short-Term Emotion Recognition and Understanding Based on SpikingNeural Network Modelling of Spatio-Temporal EEG PatternsTan, C., Sarlija, M., & Kasabov, N. (2021). NeuroSense: Short-Term Emotion Recognition and UnderstandingBased on Spiking Neural Network Modelling of Spatio-Temporal EEG Patterns. Neurocomputing, 434, 137-148.[23238]. https://doi.org/10.1016/j.neucom.2020.12.098Link to publication record in Ulster University Research PortalPublished in:NeurocomputingPublication Status:Published (in print/issue): 28/04/2021DOI:10.1016/j.neucom.2020.12.098Document VersionVersion created as part of publication process; publisher's layout; not normally made publicly availableGeneral rightsCopyright for the publications made accessible via Ulster University's Research Portal is retained by the author(s) and / or other copyrightowners and it is a condition of accessing these publications that users recognise and abide by the legal requirements associated with theserights.Take down policyThe Research Portal is Ulster University's institutional repository that provides access to Ulster's research outputs. Every effort has beenmade to ensure that content in the Research Portal does not infringe any person's rights, or applicable UK laws. If you discover content inthe Research Portal that you believe breaches copyright or violates any law, please contact pure-support@ulster.ac.uk.Download date: 03/07/2023           Journal Pre-proofsNeuroSense: Short-Term Emotion Recognition and Understanding Based onSpiking Neural Network Modelling of Spatio-Temporal EEG PatternsClarence Tan, Marko arlija, Nikola KasabovPII:DOI:Reference:S0925-2312(20)32010-5https://doi.org/10.1016/j.neucom.2020.12.098NEUCOM 23238To appear in:NeurocomputingReceived Date:Accepted Date:6 June 202019 December 2020Please cite this article as: C. Tan, M. arlija, N. Kasabov, NeuroSense: Short-Term Emotion Recognition andUnderstanding Based on Spiking Neural Network Modelling of Spatio-Temporal EEG Patterns, Neurocomputing(2021), doi: https://doi.org/10.1016/j.neucom.2020.12.098This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a coverpage and metadata, and formatting for readability, but it is not yet the definitive version of record. This versionwill undergo additional copyediting, typesetting and review before it is published in its final form, but we areproviding this version to give early visibility of the article. Please note that, during the production process, errorsmay be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.© 2020 Published by Elsevier B.V.Neurocomputing 00 (2020) 1–17Neuro-computingNeuroSense: Short-Term Emotion Recognition and UnderstandingBased on Spiking Neural Network Modelling of Spatio-TemporalEEG PatternsClarence Tana,∗, Marko ˇSarlijab, Nikola KasabovaaKnowledge Engineering and Discovery Research Institute, Auckland University of Technology, Private Bag 92006, Auckland 1010, New ZealandbFaculty of Electrical Engineering and Computing, University of Zagreb, Unska 3, Zagreb 10000, CroatiaAbstractEmotion recognition still poses a challenge lying at the core of the rapidly growing area of affective computing and is crucial forestablishing a successful human-computer interaction. Identification and understanding of emotions are achieved through variousmeasures, such as subjective self-reports, face-tracking, voice analysis, gaze-tracking, as well as the analysis of autonomic andcentral neurophysiological measurements. Current approaches to emotion recognition based on electroencephalography (EEG)mostly rely on various handcrafted features extracted over relatively long time windows of EEG during participants exposure toappropriate affective stimuli. In this paper, we present a short-term emotion recognition framework based on spiking neural net-work (SNN) modelling of spatio-temporal EEG patterns. Our method relies on EEG signal segmentation based on detection ofshort-term changes in facial landmarks, and as such includes no computation of handcrafted EEG features. Differences betweenparticipants’ EEG properties are taken into account via subject-dependent spike encoding in the formulated subject-independentemotion recognition task. We test our methods on the publicly available DEAP and MAHNOB-HCI databases due to the avail-ability of both EEG and frontal face video data. Through an exhaustive hyperparameter optimisation strategy, we show that theproposed SNN-based representation of EEG spiking patterns provides valuable information for short- term emotion recognition.The obtained accuracies are 78.97% and 79.39% in arousal classification, and 67.76% and 72.12% in valence classification, onthe DEAP and MAHNOB-HCI datasets, respectively. Furthermore, through the application of a brain-inspired SNN model, thisstudy provides novel insight and helps in the understanding of the neural mechanisms involved in emotional processing in thecontext of audiovisual stimuli, such as affective videos. The presented results encourage the use of the proposed EEG processingmethodology as a complement to existing features and methods commonly used for EEG-based emotion recognition, especiallyfor short-term arousal recognition.Keywords: Spiking Neural Networks, Emotion Recognition, Affective Computing, EEG, Event Detection1. IntroductionThe interaction between humans and computational devices is becoming more and more common with the adventof personal digital devices, wearable systems, and other technological interventions. The field of affective computing(AC) combines computer science and emotion research to enable computational systems to identify the emotionalstates of users (affect recognition) and to generate responses that humans are likely to perceive as emotional (affectgeneration). It has also been argued that over time, it may be possible for systems to actually ”feel” emotions [1].∗Corresponding authorEmail addresses: cltan@aut.ac.nz (Clarence Tan), marko.sarlija@fer.hr (Marko ˇSarlija), nkasabov@aut.ac.nz (Nikola Kasabov)1Clarence Tan et al. / Neurocomputing 00 (2020) 1–172The introductory work by Picard [1]. was followed by much research at the intersection of diverse fields such asneuroscience, ethics, psychology, and engineering, among others. To this day, the work on AC has resulted in the im-provement of systems that are capable of interpreting, identifying, and responding to the emotional states of users. Forthis purpose, affective computing makes use of various multimodal inputs such as facial images, i.e. facial expression,voice data, biometric data, e.g. physiological changes, and body language of the user. Computational models termedas ”affect models” are then employed to make sense of these input parameters and identify the emotional state of theuser [2]. The significance of AC is on the rise, given the increased degree of human-computer interactions. Accordingto a recent study1, 95% of the American population now owns a cell phone of some kind among which 77% use asmartphone. To compare, the percentage of smartphone users was 35% in 2011. Instead of the one-sided interactionsthat humans normally expect from machines, utilising AC can make these systems respond in more effective ways,making the whole technology experience more satisfactory to the user [3]. Devices often come with built-in sensorsthat collect user data. The key challenge, lying at the core of AC is recognising the emotional state of a person basedon the available data.Nowadays, AC is finding applications in various fields ranging from gaming [4], education and e-learning [5, 6]to medicine [7, 8], wearable devices [9], robotics [10], etc. For example, research shows that effective computingsystems may aid in the diagnosis of seemingly hidden and unobservable medical conditions such as depression andchronic pain [8]. Similarly, affective systems can provide more empathetic, personalised feedback to students, makingonline learning more efficient [11]. As briefly stated earlier, in order to sense affective states, four types of inputs aretypically used: 1) facial expression recognition [12, 13, 14], 2) voice recognition [15, 16, 17], 3) gesture recognition[18, 19] and 4) biometrics [20, 21, 22]. Various instruments are used to gather the aforementioned types of data,like cameras, microphones, sensors and biometric devices (e.g. heart rate, blood pressure, skin conductance or elec-troencephalography (EEG) measurements). The vast majority of AC research if focused on detailed analysis of thecollected data, in order to determine the emotional state of the user, where various machine learning techniques havebeen employed [17, 23, 24]. Affect models are usually trained on large datasets of relevant data so that they can thenbe employed for emotion recognition and affect generation.Various improvements in model architectures, feature selection methodology [20] and deep-learning-based datarepresentations have led to increases in emotion classification accuracies in the last years. However, explorationof novel approaches and concepts in the analysis of human affect could add new information and thus complementtraditionally used approaches and features. As stated above, EEG is one of the well-established modalities in affectiveresearch [12, 13, 25]. On the other hand, spiking neural networks (SNNs) have recently proven to be successful inmodelling, recognition and understanding of EEG spatio-temporal data in a wide array of domains [26], as describedin section 2. Based on the importance of EEG in AC [25], as well as numerous EEG-based applications using SNNs(described in section 2), in this paper, we adapt and apply the concept of evolving spiking neural networks (eSNN)[27] and propose an SNN classification framework for EEG-based short-term emotion recognition.Main contributions of our paper are:•••••EEG signal segmentation strategy based on detection of changes in facial landmarks for short-term emotionrecognition (section 3.2).SNN-based framework for subject-independent short-term emotion recognition (section 4).Hyperparameter optimisation strategy for spike encoding and the dynamic evolving SNN (deSNN) data repre-sentation (subsections 4.1.1 and 4.2.2).Comparison of emotion classification accuracies obtained by simple EEG spike-based features vs complexSNN-based representation of EEG spiking patterns (section 5).Novel SNN-based insights related to the neural mechanisms involved in short-term emotional processing ofaffective videos.1https://www.pewresearch.org/internet/fact-sheet/mobile/2Clarence Tan et al. / Neurocomputing 00 (2020) 1–1732. Spiking neural networksHuman brains encode information via discrete events known as action potentials or spikes, following an all-or-none principle, where a neuron fires a spike once the accumulated potential reaches a certain threshold, else it remainssilent. Due to this binary nature of information representation, the human brain still outperforms the traditionalartificial neural networks (ANNs) in terms of both energy and efficiency [28, 29]. Compared to the traditional ANNs,SNNs utilise a more biologically plausible model of neurons [30], thus bridging further the gap between neuroscienceand learning algorithms. SNNs have shown the ability to integrate information encoded in time, phase, frequency, aswell as handle large volumes of data in an adaptive and self-organised manner [31], making them particularly suitablefor solving online spatio-temporal pattern recognition problems. SNNs have been shown to be computationally moreefficient than ANNs, both theoretically [32, 33] and in several real-world applications [34]. SNNs have over the pastyears proven to be successful in several real-world learning tasks such as unsupervised classification of non-globularclusters [35], image segmentation and edge detection [36], as well as in various tasks related to modelling, recognitionand understanding of EEG spatio-temporal data [37, 38, 39], such as Alzheimer’s disease classification [40], epilepsyand epileptic seizure detection [41], predicting human behaviour during decision making [42], detection of limbmovement execution and intention for brain-machine interface (BMI) applications [43], classification of activitiesof daily living [44], modelling of peri-perceptual brain processes [45], distinguishing brain states associated withdepression and responsiveness to Mindfulness Training [46], etc.The evolving SNN, i.e. eSNN, is a class of SNN that utilises rank order learning [47] and was first proposed in [48].In addition to the open evolving structure which facilitates the addition of new variables and neuronal connections[49], eSNNs have the advantage of fast learning from large amounts of data and can interact with other systemsactively. eSNNs also allow the integration of various learning rules such as supervised learning, unsupervised learning,fuzzy rule insertion and extraction, to mention a few, and are self-evaluating in terms of system performance. Theseaforementioned properties constitute the evolving connectionist systems (ECOS) principles on which the eSNN isbased [50].In the rank-order learning scheme, the synaptic weights are adjusted only once, making it not veryefficient for spatio-temporal data, where there may be a need to adjust synaptic weights based on the spikes arrivingon a given synapse over time. To overcome this disadvantage, an extension of eSNN known as dynamic eSNN(deSNN) was introduced in [31] that combines rank-order learning with temporal learning rules such as spike-timing-dependent plasticity (STDP), which allows dynamic adjustment of the synaptic weights (more details in subsection4.2.2). However, both eSNN and deSNN do not encapsulate the structural information of the brain in terms of neuronallocations and their connectivity, which may be crucial for modelling of spatio-temporal brain data (STBD), such asEEG. The NeuCube architecture, first proposed in [51], aims at building an eSNN that incorporates structural as wellas functional aspects of the brain along with utilising the unsupervised STDP learning algorithm. Below we give abrief introduction to the NeuCube architecture.Traditional supervised learning methods such as support vector machines (SVM) or multilayer perceptron neuralnetworks (MLP) typically deal with the spatial or temporal aspects of brain data, but cannot handle the dynamic in-teraction between these processes [38]. Furthermore, such models cannot incorporate any prior structural knowledgeof the brain in an unsupervised manner, or handle multimodal brain data, e.g. EEG, functional magnetic resonanceimaging (fMRI), diffusion tensor imaging (DTI), positron emission tomography (PET). NeuCube is a specific imple-mentation of an eSNN, initially proposed to handle pattern recognition problems related to STBD, but has furtherbeen developed and modified to handle various other types of spatio-temporal data such as audiovisual data, climatedata, seismic data and ecological data [26, 52]. The emotion recognition methodology proposed in this paper is basedon the NeuCube framework, which has been adapted and further developed for this specific task, particularly in thedirections of subject-specific spike encoding and hyperparameter optimisation (see section 4).3. Data preparation3.1. DatasetsDue to the high interest in EEG analysis for emotion recognition, several publicly available multimodal databaseshave been established to this day, like DEAP [12], MAHNOB-HCI [13], SEED [53] or DREAMER [54], all of whichinclude EEG. In this paper, we use DEAP and MAHNOB-HCI databases due to their similarity and availability offrontal face video data, which is needed for our event-detection-based signal segmentation.3Clarence Tan et al. / Neurocomputing 00 (2020) 1–174DEAP is a widely used dataset for multimodal emotion analysis, consisting of 32 subjects [13]. 32-channelEEG and peripheral physiological signals, namely the galvanic skin response (GSR), electrooculogram (EOG), elec-tromyogram (EMG), respiration, plethysmograph, electrocardiogram (ECG) and body temperature were recordedduring subjects’ exposure to 40 one- minute long affective music videos. After watching each video, each subjectrated their emotional experience in five dimensions: valence, arousal, dominance, liking and familiarity. The ratingvalues were on a continuous scale of 1-9, except for familiarity, which was rated on a discrete scale of 1-5. For 22 outof the 32 subjects, the frontal video was recorded, so we used data from those 22 subjects only.MAHNOB-HCI is a multimodal dataset consisting of 27 subjects which participated in two experiments [13].In the first experiment, similarly to DEAP, each participant watched 20 emotional videos which were between 34.9and 117 seconds long. Recorded signals included 32-channel EEG, peripheral physiology (ECG, GSR, respiration,skin temperature), eye gaze data, audio data and video from 6 cameras recording facial expressions and head pose.After watching each video, the participants gave an emotional label/tag to the video, as well as rated their emotionalexperience in arousal, valence, dominance and predictability on a 1-9 scale. The second experiment was related toimplicit tagging and was not used in this work.3.2. EEG segmentation based on the analysis of facial landmarksIn this section, we describe the signal segmentation procedure, which was used to detect moments of participants’most intensive emotional engagement, based on detecting events in the time-varying facial landmarks. This step isimportant due to several reasons:••The differences in utterance lengths vary between DEAP and MAHNOB- HCI, as well as within the MAHNOB-HCI database itself. With an event-based approach, we make sure all samples which are to be used for lateremotion recognition are of the same length.The self-assessed emotional experience of the participant is not uniformly distributed across the entire durationof an emotional video but is more likely a result of one or more emotionally intensive events of shorter duration.We aim to capture the occurrences of such events by the analysis of facial landmarks.The facial-video-based event detection algorithm comprises the following steps, which were taken in both datasets:1. In each frame of each video, the participant’s face was tracked. We have employed an implementation2 of theViola-Jones algorithm [55] to detect participant’s faces, noses, eyes, mouth, etc., in the first frame of the video.This step outputs a region of interest (ROI).2. ROI from the previous step is then tracked from frame to frame based on detection and tracking of specificfeatures by using a minimum eigenvalue feature detection algorithm developed by Shi and Tomasi [56] and aKanade–Lucas–Tomasi (KLT) feature tracking algorithm [57].3. Each detected and tracked ROI from the previous step was used as input to a facial landmarks detection algo-rithm3 [58]. Facial landmarks were 68 specific points on the face, such as mouth corners, eyebrow lines, eyelines, etc., as shown in Figure 1.4. Based on the detected and tracked 68 facial landmarks, we compute an array of 20 specific geometrical featuresper frame, related to eyebrow, eye and lip positioning and shape, as described in [13].5. The energy of time-varying geometrical facial features (see Figure 2, top) is calculated disregarding the 10features based on eye landmarks (features f5 to f14 in [13]), as the eye-based features are very sensitive toblinking. The obtained energy signal is shown in Figure 2, bottom.6. A simple liner-slope-based match filter for detecting increases in the facial features energy signal is applied,resulting in a detection signal (see Figure 2). A maximum in the detection signal marks the beginning of aone-second-long event. Figure 3 shows the face of the participant at the beginning and ending of the first eventfrom Figure 2.2The vision.CascadeObjectDetector object from Matlab’s Computer Vision Toolbox.3The pre-trained DLIB model for facial landmarks detection: http://dlib.net/files/shape predictor 6 face landmarks.dat.bz24Clarence Tan et al. / Neurocomputing 00 (2020) 1–175Figure 1: Two examples of facial landmarks detection in the MAHNOB-HCI dataset.7. EEG signal segmentation is finally performed based on the detected events from the previous step. EEG signalswere preprocessed using the TEAP toolbox [59], and all 32 available channels are used.The described procedure resulted with a total of 224 samples for arousal classification and 224 samples for valenceclassification from the DEAP dataset, and a total of 162 samples for arousal classification and 208 samples for valenceclassification from the MAHNOB-HCI dataset. Data from participants with less than 5 samples per participant wereexcluded from the analysis, resulting with a total of 214 samples for arousal classification (125 labelled positively)and 214 samples for valence classification (112 labelled positively) from the DEAP dataset, and a total of 131 samplesfor arousal classification (47 labelled positively) and 191 samples for valence classification (94 labelled positively)from the MAHNOB-HCI dataset.The number of extracted samples, as well as the class distributions, are not equal for valence and arousal dueto our labelling strategy. For arousal, we used utterances which were labelled as either calm or excited/activated,thus excluding medium arousal. Accordingly, for valence, we also selected 2 classes (pleasant and unpleasant), thusexcluding utterances labelled as neutral valence. This resulted with cases for which a participant rated his experienceFigure 2: Detection of increases in facial activity. The top axes are showing the trajectories of facial features during a participant’s exposure to apleasant video. The bottom axes are showing the corresponding facial features energy signal (blue) and the detection signal (red). In this example,three events were detected.5Clarence Tan et al. / Neurocomputing 00 (2020) 1–176Figure 3: An example of a change in the participant’s facial expression from the beginning to the ending of a detected emotional event.as, e.g. pleasant and medium arousal, in which case the data is processed for valence analysis and not processed forarousal analysis.4. Emotion recognition methodologyIn the following section, we describe our emotion recognition methodology based on SNN modelling of spatio-temporal EEG spike patterns. Accord- ing to recent reviews of various studies on emotion recognition from EGG[25, 60], static features extracted on specific slices of EEG data dominate the research landscape. These are mostcommonly related to spectral features, like changes in power over the theta, alpha, beta and gamma frequency bands[12, 61, 62] and spectral power asymmetry measures in pairs of symmetrical electrodes [13, 62]. Besides the frequencydomain features, which are predominant, various time domain and nonlinear features have been investigated as well[63], such as Hjorth features [64], fractal dimensions, entropy features etc. Regardless of the feature extractionmethodology, traditional approaches usually result in a static feature vector, thus usually well capturing the spatialaspect (emerging from the electrode positions), but neglecting the dynamical spatio-temporal nature of EEG patterns.Time-frequency domain features are currently the only tool used to capture these dynamical changes in EEG [65].With our work, we aim to leverage the spatio-temporal nature of EEG in emotion recognition by investigating theconcept of spike encoding and SNN modelling of EEG. Therefore, we focus only on the comparison of the predictivepower of simple spike-based features and complex SNN based spatio-temporal spike-patterns in the task of EEG basedemotion recognition. Accordingly, the possibility of enhancing the predictive power by the integration of EEG-spike-based features with traditional static features described earlier exceeds the scope of our work.Figure 4: Illustration of the proposed SNN computational architecture for EEG-based emotion recognition.The proposed SNN-based framework for subject-independent emotion recognition includes the following process-ing steps (illustrated in Figure 4):1. Subject-specific spike encoding of short-term EEG recordings obtained by the segmentation strategy describedin section 3.2.6Clarence Tan et al. / Neurocomputing 00 (2020) 1–1772. Unsupervised learning of a brain-like 3-D SNN reservoir (SNNr) module, based on STDP learning.3. deSNN representation of spatio-temporal spiking patterns obtained from the trained SNNr, given a specific inputspike sequence. The obtained representation forms a static vector which is suitable as input to any traditionalsupervised learning algorithm (e.g. SVM, ANN, kNN, etc.).4. Feature selection and supervised learning for emotion recognition from the representation obtained in the pre-vious step.4.1. Spike encodingIn an SNN-based architecture, information is processed in the form of binary spiking events. Accordingly, allcontinuous variables first need to be encoded into spike trains, shown as the first step in Figure 4. In this paper, byspike encoding, we namely focus on temporal spike encoding methods where spike timings usually mark changesin the signal value over time [66]. This approach is driven by a biologically plausible view that precise relativespike timing encodes information [67]. Most of the commonly used encoding algorithms [52], such as the Spikemanager tool [68], threshold-based representation (TBR) algorithm, step-forward (SF) encoding, moving-window(MW) encoding, or the Bens Spiker Algorithm (BSA) [69], rely on tracking the temporal changes in the signal, whichare then represented by the exact timing of spikes. Such algorithms usually produce a bipolar spike sequence, wherepositive changes in the signal value (increases) result with positive spikes, and negative changes in the signal value(decreases) result with negative spikes.In this paper, we transform the EEG data into spike trains using the relatively simple version of TBR, known asthe address event representation (AER) method [70, 71]. The method is based on thresholding the rate of change of aninput variable over time and is suitable when the input data is a stream, which is the case with EEG. The algorithm isbased on the variable threshold value that is calculated for each of the 32 input data channels. The variable thresholdarray is calculated for each channel, as the signal dynamics and value ranges can vary between the input channels.For each of the input channels, the variable threshold is calculated based on one scalar input parameter (αT R) in thefollowing way:VT (k) =1NNXi=1(cid:16)µ + σαT R(cid:17)·(1)where N is the number of samples, T is the signal length (number of time points per data sample), and k goes from1 to the number of channels Ninput = 32. αT R is the spike threshold parameter, µ is the sample mean rate of changein the signal, σ is the sample standard deviation of the rate of change in the signal, and VT is the resulting variablethreshold array. Equation (1) represents the threshold value for the k-th input channel. At each time point where thek-th input channel signal difference (rate of change) exceeds the corresponding variable threshold, a positive spike isgenerated. Accordingly, inhibiting, i.e. negative, spikes are generated when the rate of change exceeds the variablethreshold in the negative, i.e. decreasing, direction. Algorithm 1 sums up the spike encoding procedure in a compactalgorithmic form.In affective applications, such as the subject-independent emotion recognition, the encoding algorithm shouldideally provide similar spike trains for similar emotional states, regardless of the high inter-subject variability in thecollected EEG signal properties. Therefore, the encoding procedure described in algorithm 1 is utilised in a per-subjectfashion, with µ and σ being estimated separately for each subject.4.1.1. Optimisation of the spike encoding methodSpike encoding is the first link in the SNN processing chain. Choosing the appropriate spike encoding method,with the optimal hyperparameters, in our case αT R, is extremely important, in order to retain the task-relevant in-formation. The spike encoding can therefore be seen both as a primary information compression as well as a noiseelimination step. Inadequate spike encoding algorithm can result with either loss of useful information on one end(high αT R), or high levels of information noise on the other end (low αT R), as shown in Figure 5. The problem ofselection and optimisation of temporal spike encoding algorithms for SNNs has been most recently tackled in [66]. Anexhaustive approach should evaluate the effectiveness of the encoding within the context of the entire SNN processing7Clarence Tan et al. / Neurocomputing 00 (2020) 1–178NinputT}×1, 0, 1·×NinputNinputVTkNαT R)→ {−VTk + (µ + σchannel k of the i-th sample in Xinδx|mean(x′)st.dev.(x′)Algorithm 1: Spike encoding: fencode : RTRTNinput ,hyperparameters := αT R}Require: Xin ∈×{TEnsure: Xout ∈ {−1, 0, 1×}1: N#(Xin)number of data samples in the dataset}{←2: for k = 1 to Ninput doVTk ←03:for i = 1 to N do4:x5:←x′6:← |µ7:←σ8:←VTk ←9:end for10:VTk ←11:12: end for13: for k = 1 to Ninput dofor i = 1 to N do14:15:16:17:18:19:20:21:22:23:24:25:26:27: end forxchannel k of the i-th sample in Xin←δxx′←0Txout ←for j = 2 to T doif x′j > VTk thenxout( j) ←VTk thenelse if x′j <−xout( j) ← −1end ifend forstore spike train xout in Xout for channel k, sample iend for1×1and classification framework, i.e.the encoding algorithm is optimised according to the output of the whole SNNsystem, e.g. classification accuracy [72]. The computational cost of such an approach would be extremely high, dueto the need for simultaneous optimisation of all SNN system hyperparameters, thus making the approach infeasible.Another approach is to try and optimise the encoding step by itself, via application of the corresponding decodingalgorithm and trying to minimise some error metric between the original and reconstructed signal, as in [66]. Thisapproach neglects the classification task context, thus lacking final validation in terms of the obtained classificationperformance.In this paper, we propose a compromise approach: we optimise the encoding method by itself, due to the alreadystated computational infeasibility of taking into account the rest of the SNN system, while we still take into account thespecific classification problem, i.e. binary emotion classification in terms of valence and arousal. To avoid evaluatingthe performance of the entire SNN system, we calculate an array of simple spike-based features based on statisticaldescriptives and the widely used rate coding scheme [73]. For each of the 32 EEG channels, encoded into spike trainsas described in the previous subsection, we calculate the following 6 features:•••••firing rate of positive spikes,firing rate of negative spikes,median timing difference between consecutive positive spikes,median timing difference between consecutive negative spikes,interquartile range of timing differences between consecutive positive spikes,8Clarence Tan et al. / Neurocomputing 00 (2020) 1–179Figure 5: Spike trains resulting from different AER threshold parameter (αT R) values. Lower αT R yields a dense spike train with a tendency toencode changes which are most likely on the level of noise, while high αT R value yields a sparse spike train with only major changes in signal valuebeing encoded as spike events.interquartile range of timing differences between consecutive negative spikes.·•We use a simple definition of firing rate as the temporal average, i.e. the number of spikes divided by the corre-sponding time interval duration (in our case 1 second for all data samples). Each sample is therefore described by6 = 192 simple rate-coding-based and statistical features. Datasets are formed for an array of different αT R values32(ranging from 0.5 to 5, with a step of 0.5), standardised, and then tested for their task-relevant discriminative power.The used performance metric is the cross-validated classification error obtained with an optimised Inf-FS featureselection algorithm [74, 75] and an RBF-SVM classification algorithm. This metric employs a simple and genericfeature selection and machine learning approach in order to estimate the maximum cross-validated nonlinear classseparation that can be obtained by the calculated features. For each αT R value, a non-convex Bayesian optimisationapproach was employed in order to find the values of the remaining hyperparameter values:αFS : Inf-FS parameter representing the trade-off between feature dispersion and feature correlation, which arethe basis of the Inf-FS feature ranking algorithm (range set to [0, 1]),nFS : number of selected features, based on the obtained feature ranking (range set to [1, 50], integer grid),C: RBF-SVM parameter (range set to [10−3, 103], logarithmic grid),σ: RBF-SVM parameter (range set to [10−3, 103], logarithmic grid),••••which maximize the leave-one-subject-out (LOSO) cross-validation accuracy. Matlab function bayesopt was used,with 500 objective evaluations and a 0.5 exploration ratio. Algorithm 2 sums up the spike encoding optimisationprocedure in a compact algorithmic form.Algorithm 2 has been applied to the tasks of arousal and valence classification, on both DEAP and MAHNOB-HCI, and the results of the spike encoding optimisation are shown in Figure 6. Threshold value αT R = 1.5 yields themost informative spikes for both arousal and valence classification. Such a result can be interpreted in the context ofthe behaviour of the encoding algorithm illustrated in Figure 5.4.2. Spiking neural network processing4.2.1. 3D SNNr moduleOnce the EEG data has been encoded for each subject, the obtained sparse spike trains are used to train a 3D SNNrwith Nr = 1471 leaky integrate and fire (LIF) model neurons. Each neuron has a predefined 3D spatial coordinate,according to the Talairach template coordinates from [76], resulting with a brain-like shape. Accordingly, input spikes9Clarence Tan et al. / Neurocomputing 00 (2020) 1–1710×∈RT}} {Ninput , yRNsamples×1, 2, ..., 10search range for αT R}∈ {from the above: #(αT R,range) = 10}{Algorithm 2: Optimisation of the spike encoding threshold parameter αT R1Require: Xin ∈Ensure: αT R,opt.0.5k, k1: αT R,range ← {2: Nsearch ←#(αT R,range)0Nsearch×13: CVloss ←initialise objective array{}search range for αFS }[0, 1]4: αFS ,range ←{5: nFS ,range ←search range for nFS , integer[1, 50]}{3, 103]6: Crange ←search range for C, logarithmic grid}{3, 103]search range for σ, logarithmic grid7: σrange ←}{8: for i = 1 to Nsearch doX9:calculate X f eatures ∈10:CVloss(i)11:←hyperparameters described in 4.1.1}Bayesopt(X f eatures, y, αFS ,range, nFS ,range, Crange, σrange)fencode(Xin, αT R,range(i))spike encoding, see algorithm 1}{RNsamples×192 based on Xfeatures described in section 4.1.1}[10−[10−←{12: end for13: imin ←14: αT R,opt. ←index of smallest element in CVlossαT R,range(imin)cross-validated classification error, other{are passed over to the SNNr at the neuron locations corresponding to the mapping of 32 EEG channels, as shown inFigure 7.The connections between the neurons in the SNNr are initialised using the small-world connectivity (SWC) ap-proach [37], where a radius is defined as a parameter for connecting neurons within this radius, i.e. small-worldradius (SWR). This results in an SNNr of sparsely connected neurons. After initialisation, the connection weightsW(i j) between the pairs of connected neurons (i j) are determined based on the following expression:W(i j) = sgn (cid:16) rand(1)0.2(cid:17) ·−rand(1)1Ldist(i j),·(2)Where rand(1) generates a pseudorandom from a uniform distribution on the open interval (0, 1), and Ldist(i j) representsthe distance between neurons i and j. The equation above results with an expectation of 80% positive weights and20% negative weights (in matrix W).Figure 6: Results of the spike encoding parameter (αT R) optimisation, for both arousal (left) and valence classification (right), according toAlgorithm 2. For each task, both DEAP and MAHNOB-HCI datasets were used, and the optimal αT R value (indicated by an asterisk) wasdetermined as the one which minimises the used class separation performance metric, i.e. the average cross-validated classification error on bothdatasets.10Clarence Tan et al. / Neurocomputing 00 (2020) 1–1711Figure 7: Brain 3D coordinates are, according to the Talairach template coordinates, used for the allocated spiking neurons in the SNNr - resultingwith 3D brain-like SNNr shape. Yellow neurons (right) are considered input neurons, and correspond to the mapping of 32 EEG channels (left).The STDP learning rule [77] is applied that allows the SNNr to adapt the connection weights based on the spatio-temporal associations between the input-driven spikes. The used STDP algorithm relies on the following hyperpa-rameters:D (potential leak rate): the rate of potential passive degradation through inactivity,R (refractory time): determining a period of resting between spikes,η (STDP rate): learning rate, used for weight updating,β (firing threshold): potential threshold for generating a spike,Niter: number of training iterations,•••••and with the specific steps described in Algorithm 3.4.2.2. deSNN representationAs an output representation of the SNNr-based spike sequences we used the deSNN algorithm [31]. The methodcombines the simple rank-order (RO) learning rule [47] and the STDP-based activation of the neurons in the previouslytrained SNNr (as described in Algorithm 3). In the previous section, we have described how the spike trains can beused to train the SNNr in an unsupervised manner. However, the SNNr now operates as an activation module, meaningthat the new input spike trains are propagated through the trained SNNr, generating a complex spatio-temporal neuronactivation pattern. This pattern is used to generate (evolve) the output neurons, i.e. the deSNN representation. Thealgorithm hyperparameters are:•αdeSNN: main deSNN hyperparameter that determines the output value based on the first spike occurrence of thecorresponding SNNr neuron,d(drift): used for output update on the subsequent spikes of the corresponding SNNr neurons.•From the deSNN procedure, described in Algorithm 4, it can be seen that the static output representation of thespatio-temporal SNNr-based spiking patterns highly depends on the selection of the method’s hyperparameters αdeSNNand d.5. ExperimentsIn [37] it has been suggested to repeat the processing steps described in sections 4.1.1, 4.2, and 4.2.2 for differenthyperparameter values in order to optimise the final classification performance in the supervised learning step. Takinginto account the entire proposed methodology, this includes the following hyperparameters: αT R for spike encoding;S WR, D, R, η, β and Niter for STDP-based unsupervised learning of the SNNr; αdeSNN and d for the deSNN repre-sentation; and finally αFS , nFS , C and σ for the supervised learning step, which includes feature selection and an11Clarence Tan et al. / Neurocomputing 00 (2020) 1–1712Ninput×Nsampleshyperparameters := D, R, η, β, Niter}{××NrRNr0, 1}NrNr , S in ∈ {−Algorithm 3: Unsupervised SNNr weight learning: fS T DPTNr , Winit ∈Require: Cinit ∈ {1, 0, 1}Ensure: Wout : RNr×[1, 2, ..., Nr]1: χall neuron indices←{2: Pk ←k0,initialize neuron potentials{∈∀}k3: Rk ←initialize neuron refractory time counters0,}{∈∀4: find inputneuron indices ι5: for niter = 1 to Niter doη6:√niterχχ⊂χ}×η′for i = 1 to Nsamples do←Ninput spike matrix of the i-th sample in S inTs×for t = 1 to T do←7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33: end forend forend forend forfind f iringneuron indices τ =for all jτ dof iringneurons in ι} ∪ {kχι, Pk > β}\∈{find post synaptic neuron indices γγ and Rk = 0 dofor all k∈Pk + w jk {Pk ←end forupdate potential}ττend for0,kPk ←reset potential∈{}Rk ←R,kreset refractory counter∈}{max(0, Pk −χPk ←Rk ←χmax(0, Rk −τ dofor all jτ\τ\k∀k∀D),1),\ι\∈∈∀∀ι∈∈η(tt fk )find post synaptic neuron indices γfor all kγ do∈w jk −w jk ←end forfind pre synaptic neuron indices γfor all kγ do∈w jk + η(tw jk ←end fort fk )−−RBF-SVM classifier. This makes a total of 13 hyperparameters that should be simultaneously optimised in an exhaus-tive procedure where each iteration includes data preprocessing, STDP-based learning as well as cross-validation ofthe obtained output representation. However, STDP-based training of SNNs is computationally very expensive andaccounts for the most significant proportion of the total execution time.To mitigate the effects of the computationally expensive STDP-based step on the duration of the hyperparameteroptimisation procedure, we optimise the encoding hyperparameter αT R via a compromise approach described in sec-tion 4.1.1, outside of the SNN context, while still taking into account the classification task performance. ParameterαT R = 1.5 is identified as optimal for both the arousal and valence classification tasks. For the STDP-based trainingof the SNNr module, we use a set of previously identified hyperparameters [40]: S WR = 2.5, D = 0.002, R = 6,η = .001, and β = 0.5, in order to avoid training the SNNr at each step of the deSNN and supervised learning hyper-parameter optimisation. Niter was set to 5, due to the relatively small amount of available EEG data. This parameterhas to be taken with caution as high values may cause over-training of the SNNr [37]. The resulting brain-like SNNrconnectivity can be visualised, analysed and interpreted for a better understanding of the EEG data and relative in-volvement of various brain regions. Figure 8 shows the emerging SNNr connectivity, obtained by using data withdifferent emotional labels.12Clarence Tan et al. / Neurocomputing 00 (2020) 1–1713S cube is a sparse Nr ×{T matrix of all neuron firings for the entire××}}}NrNr∈NinputT}1, 0, 1Require: Wfor j = 1 to Nr dosample duration length TAlgorithm 4: Output deSNN representation: fdeS NNRNrNr , sin ∈ {−hyperparameters := α, d}{R1×Nrinitialize representation to 0{Ensure: WdeS NN ∈011: WdeS NN ←×012: Finitialize the neuron firing flags×←}{3: cinitialize the neuron firing order counter0{←4: S cube ←propagate sin through the SNN defined by W5: for i = 1 to T do6:7:8:9:10:11:12:13:14:15:16:17:18:19:20: end forneuron fires for the first time{}αcWdeS NN( j)F( j)1if S cube(i, j) = 1 thenif F( j) = 0 thenend ifcelseend ifend forWdeS NN( j) + dWdeS NN( j)WdeS NN( j)WdeS NN( j)c + 1else←←←←←−dFor each of the 4 available subtasks, i.e. arousal and valence classification with DEAP and MAHNOB-HCI, aBayesian optimisation approach is utilised in order to identify the optimal set of the 6 remaining hyperparameters.2, 2],αFS , nFS , C and σ are optimised using the same ranges as in section 4.1.1. The range for αdeSNN was set to [10−3, 1]. At each iteration both simple spike-based features from section 4.1.1, as well as theand range for d was set to [10−deSNN-based features, were fed to the feature selection and LOSO cross-validated RBF-SVM evaluation, in searchof the maximum accuracy. The idea was to test the added value of deSNN-based features in terms of classificationaccuracy improvement. Results are summed up in Table 1.Table 1: Comparison of the obtained optimized LOSO cross-validation accuracies.Feature setTaskDatasetAccuracySimple spike-bassedfeaturesSimple spike-bassedfeatures + deSNNArousalValenceArousalValenceDEAPMAHNOB-HCIDEAPMAHNOB-HCIDEAPMAHNOB-HCIDEAPMAHNOB-HCI0.63840.75930.64730.72120.78970.79390.67760.7068The upper half of Table 1 is based on the results obtained in section 4.1.1 (Figure 6), while the lower half sumsup the results of deSNN optimisation procedure described in this section. The addition of optimised deSNN repre-sentation to the feature set significantly increased the maximal obtainable accuracy in arousal classification, for bothdatasets. In terms of valence classification, accuracy was slightly improved for DEAP, but not for MAHNOB- HCI.13Clarence Tan et al. / Neurocomputing 00 (2020) 1–1714Figure 8: The obtained connectivity of 4 different SNNr modules. Each SNNr is trained by using the combined data from both DEAP andMAHNOB-HCI, labeled as either low (left) or high (right) in terms of either arousal (top) or valence (bottom). For each SNNr two 3-D plotsfrom different angles are given: the left plot shows frontal and left side of the brain, while the right plot shows the back (posterior) and right sideof the brain. Neurons are plotted with a slight transparency in order to better highlight the 3D nature of the emerged connections. 500 strongestconnections are displayed for each SNNr, with thicker lines denoting stronger connections. Brighter neurons are more active.6. DiscussionIn this work, we expand on the widely used SNN-based NeuCube framework [51] and present an approach forshort-term classification of emotional states, namely arousal and valence, based on EEG data. We employ an EEGsignal segmentation strategy based on detection of changes in facial landmarks for short-term emotion characteri-zation. Using the segmented EEG data, we propose and apply a hyperparameter optimization strategy both for thesubject-specific spike encoding step, as well as for tuning of the deSNN and supervised learning step. The obtainedresults demonstrate how complex SNN-based spatio-temporal modelling of EEG spiking patterns can provide addi-tional information value able for short-term emotion classification. Such SNN-based information was shown to beparticularly useful for short-term arousal classification, and not as much for valence.Main advantage of the described procedure is straight-forward spike-based processing of EEG, as opposed tothe need for extraction of handcrafted features, which is the case with most of the classical methods. The proposedapproach also has the ability of capturing information hidden within the complex spatio-temporal EEG spiking pat-terns on relatively short time frames. Furthermore, the addition of subject-specific spike encoding, which is crucialto successfully develop subject-independent SNN-based decision systems, as well as the proposed hyperparameteroptimisation strategy is an added value to the original NeuCube framework. Hopefully, this will open up additionalpaths and opportunities for further utilisation of brain-like SNN models in various subject-independent classificationtasks.From the utilised ”spike-only-based comparison” approach (Table 1) emerge both the strengths and limitationsof this study. This approach demonstrates that the addition of deSNN features, which are a representation of spatio-temporal SNNr spiking patterns, to simple non-SNN-related spike features improves the classification performance.In contrast to traditional approaches, an SNN-based emotion classification framework presented in this paper can helpin better understanding of short-term emotional processing. From Figure 8, it can be seen that the left side of thebrain dominates in terms of connectivity strength, i.e. activation, in particular regions corresponding to the temporallobe, amygdala and the occipital lobe. This result is not surprising, since both DEAP and MAHNOB-HCI induceemotions by means of emotional videos, which requires visual processing (occipital lobe) as well as the processingof language (temporal lobe). Additionally, it is well known that the amygdala, located in the medial temporal lobe,14Clarence Tan et al. / Neurocomputing 00 (2020) 1–1715plays one of the key roles in general emotional processing [78]. The dominance of the left side most likely emergesfrom the dominance of the left temporal lobe in most people. For example, the lateral sulcus, area considered to behighly involved in language function, is longer on the left than on the right side of the brain [79]. Additionally, theposterior views on the obtained connectivities from Figure 8 suggest that occipital lobe plays the most significant rolein the processing of high arousal. This might indicate that the visual portion of the affective stimuli plays the mostsignificant role in inducing high emotional arousal. Due to the indirect presence of the EEG data from all subjectsin both the training and validation, as a result of a single unsupervised SNNr training for each of the 4 subtasks, thereported accuracies likely overestimate the true predictive power of framework, and as such call for future work, withbigger amounts of data. Given that, as well as the hyperparameter optimisation strategy described in section 5, Table1 should rather serve as a demonstration of the ”added value” of SNN-based features in comparison to the classicalnon-SNN- related spike-based features. Additionally, analysis of the deSNN features in the context of the traditionaltime- or frequency-domain EEG features would be useful but exceeds the scope of our work as such features do notmatch the idea of biologically inspired spike-based processing of EEG. However, the proposed approach should notbe considered only in terms of the obtained classification performance, which is the case in most traditional ”blackbox” emotion recognition models. One of the main advantages of this approach is in the understanding of the inducedand recognised emotions through a brain-inspired SNN model, as shown in Figure 8 and discussed above.7. ConclusionTo conclude, this paper for the first time introduces a brain-inspired SNN architecture to recognise, and mostimportantly, to explain for a better understanding, EEG data measuring two basic dimensions of emotion: arousaland valence. The patterns obtained after deep learning in the SNN architecture are interpreted in terms of brainactivities of subjects. The proposed method can manifest fast, incremental and transfer learning on new data relatedto emotions that make it suitable for further study as well as for real-time emotion recognition systems. Future workwill involve improved SNN hyperparameter optimisation and more comprehensive visualisation and analysis of thebrain-structured SNN during and after learning for a better understanding of brain processes related to emotionalprocessing in humans.References[1] R. W. Picard, Affective computing-mit media laboratory perceptual computing section technical report no. 321, Cambridge, MA 2139 (1995).[2] J. Tao, T. Tan, Affective computing: A review, in: International Conference on Affective computing and intelligent interaction, Springer,2005, pp. 981–995.[3] T. J. Brigham, Merging technology and emotions: Introduction to affective computing, Medical reference services quarterly 36 (4) (2017)399–407.[4] B. Guthier, R. D¨orner, H. P. Martinez, Affective computing in games, in: Entertainment Computing and Serious Games, Springer, 2016, pp.402–441.[5] E. Yadegaridehkordi, N. F. B. M. Noor, M. N. B. Ayub, H. B. Affal, N. B. Hussin, Affective computing in education: A systematic reviewand future research, Computers & Education 142 (2019) 103649.[6] S. Duo, L. X. Song, An e-learning system based on affective computing, physics Procedia 24 (2012) 1893–1898.[7] A. Luneski, E. Konstantinidis, P. Bamidis, Affective medicine, Methods of information in medicine 49 (03) (2010) 207–218.[8] M. S. Aung, S. Kaltwang, B. Romera-Paredes, B. Martinez, A. Singh, M. Cella, M. Valstar, H. Meng, A. Kemp, M. Shafizadeh, et al., Theautomatic detection of chronic pain-related expression: requirements, challenges and the multimodal emopain dataset, IEEE transactions onaffective computing 7 (4) (2015) 435–451.[9] R. W. Picard, J. Scheirer, The galvactivator: A glove that senses and communicates skin conductivity, in: Proceedings 9th Int. Conf. on HCI,2001.[10] F. Cid, J. Moreno, P. Bustos, P. N´unez, Muecas: a multi-sensor robotic head for affective human robot interaction and imitation, Sensors14 (5) (2014) 7711–7737.[11] J. Grafsgaard, J. B. Wiggins, K. E. Boyer, E. N. Wiebe, J. Lester, Automatically recognizing facial expression: Predicting engagement andfrustration, in: Educational Data Mining 2013, 2013.[12] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Nijholt, I. Patras, Deap: A database for emotion analysis;using physiological signals, IEEE Transactions on Affective Computing 3 (1) (2012) 18–31.[13] M. Soleymani, J. Lichtenauer, T. Pun, M. Pantic, A multimodal database for affect recognition and implicit tagging, IEEE Transactions onAffective Computing 3 (1) (2012) 42–55.[14] B. Ko, A brief review of facial emotion recognition based on visual information, sensors 18 (2) (2018) 401.[15] B. Schuller, G. Rigoll, M. Lang, Speech emotion recognition combining acoustic features and linguistic information in a hybrid supportvector machine-belief network architecture, in: 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, Vol. 1,IEEE, 2004, pp. I–577.15Clarence Tan et al. / Neurocomputing 00 (2020) 1–1716[16] I. Miji´c, M. ˇSarlija, D. Petrinovi´c, Mmod-cog: A database for multimodal cognitive load classification, in: 2019 11th International Sympo-sium on Image and Signal Processing and Analysis (ISPA), IEEE, 2019, pp. 15–20.[17] T.-Y. Huang, J.-L. Li, C.-M. Chang, C.-C. Lee, A dual-complementary acoustic embedding network learned from raw waveform for speechemotion recognition, in: 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII), IEEE, 2019, pp.83–88.[18] S. Piana, A. Stagliano, F. Odone, A. Verri, A. Camurri, Real-time automatic emotion recognition from body gestures, arXiv preprintarXiv:1402.5047 (2014).[19] A. Camurri, I. Lagerl¨of, G. Volpe, Recognizing emotion from dance movement: comparison of spectator recognition and automated tech-niques, International journal of human-computer studies 59 (1-2) (2003) 213–225.[20] D. Kukolja, S. Popovi´c, M. Horvat, B. Kovaˇc, K. ´Cosi´c, Comparative analysis of emotion estimation methods based on physiological mea-surements for real-time applications, International journal of human-computer studies 72 (10-11) (2014) 717–727.[21] A. Greco, G. Valenza, L. Citi, E. P. Scilingo, Arousal and valence recognition of affective sounds based on electrodermal activity, IEEESensors Journal 17 (3) (2016) 716–725.[22] X. Zhang, C. Xu, W. Xue, J. Hu, Y. He, M. Gao, Emotion recognition based on multichannel physiological signals with comprehensivenonlinear processing, Sensors 18 (11) (2018) 3886.[23] P. Rani, C. Liu, N. Sarkar, E. Vanman, An empirical study of machine learning techniques for affect recognition in human–robot interaction,Pattern Analysis and Applications 9 (1) (2006) 58–69.[24] O. O. Rudovic, Machine learning for affective computing and its applications to automated measurement of human facial affect, in: 2016International Symposium on Micro-NanoMechatronics and Human Science (MHS), IEEE, 2016, pp. 1–1.[25] S. M. Alarcao, M. J. Fonseca, Emotions recognition using eeg signals: A survey, IEEE Transactions on Affective Computing (2017).[26] C. Tan, M. ˇSarlija, N. Kasabov, Spiking neural networks: Background, recent development and the neucube architecture, Neural ProcessingLetters (2020) 1–27.[27] N. K. Kasabov, Evolving spiking neural networks, in: Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence,Springer, 2018, Ch. 5, pp. 169–199.[28] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, nature 521 (7553) (2015) 436–444.[29] W. Wang, G. Pedretti, V. Milo, R. Carboni, A. Calderoni, N. Ramaswamy, A. S. Spinelli, D. Ielmini, Learning of spatiotemporal patterns ina spiking neural network with resistive switching synapses, Science advances 4 (9) (2018) eaat4752.[30] A. Taherkhani, A. Belatreche, Y. Li, G. Cosma, L. P. Maguire, T. M. McGinnity, A review of learning in biologically plausible spiking neuralnetworks, Neural Networks 122 (2020) 253–272.[31] N. Kasabov, K. Dhoble, N. Nuntalid, G. Indiveri, Dynamic evolving spiking neural networks for on-line spatio-and spectro-temporal patternrecognition, Neural Networks 41 (2013) 188–201.[32] W. Maass, Networks of spiking neurons: the third generation of neural network models, Neural networks 10 (9) (1997) 1659–1671.[33] W. Maass, H. Markram, On the computational power of circuits of spiking neurons, Journal of computer and system sciences 69 (4) (2004)593–616.[34] S. M. Bohte, J. N. Kok, H. La Poutre, Error-backpropagation in temporally encoded networks of spiking neurons, Neurocomputing 48 (1-4)(2002) 17–37.[35] S. M. Bohte, H. La Poutr´e, J. N. Kok, Unsupervised clustering with spiking neurons by sparse temporal coding and multilayer rbf networks,IEEE Transactions on neural networks 13 (2) (2002) 426–435.[36] B. Meftah, O. Lezoray, A. Benyettou, Segmentation and edge detection based on spiking neural network model, Neural Processing Letters32 (2) (2010) 131–146.[37] N. Kasabov, E. Capecci, Spiking neural network methodology for modelling, classification and understanding of eeg spatio-temporal datameasuring cognitive processes, Information Sciences 294 (2015) 565–575.[38] N. Kasabov, J. Hu, Y. Chen, N. Scott, Y. Turkova, Spatio-temporal eeg data classification in the neucube 3d snn environment: methodologyand examples, in: International Conference on Neural Information Processing, Springer, 2013, pp. 63–69.[39] K. Kumarasinghe, N. Kasabov, D. Taylor, Deep learning and deep knowledge representation in spiking neural networks for brain-computerinterfaces, Neural Networks 121 (2020) 169–185.[40] E. Capecci, Z. G. Doborjeh, N. Mammone, F. La Foresta, F. C. Morabito, N. Kasabov, Longitudinal study of alzheimer’s disease degenerationthrough eeg data analysis with a neucube spiking neural network model, in: 2016 International Joint Conference on Neural Networks (IJCNN),IEEE, 2016, pp. 1360–1366.[41] S. Ghosh-Dastidar, H. Adeli, Improved spiking neural networks for eeg classification and epilepsy and seizure detection, Integrated Computer-Aided Engineering 14 (3) (2007) 187–212.[42] Z. G. Doborjeh, M. Doborjeh, N. Kasabov, Eeg pattern recognition using brain-inspired spiking neural networks for modelling human decisionprocesses, in: 2018 International Joint Conference on Neural Networks (IJCNN), IEEE, 2018, pp. 1–7.[43] D. Taylor, N. Scott, N. Kasabov, E. Capecci, E. Tu, N. Saywell, Y. Chen, J. Hu, Z.-G. Hou, Feasibility of neucube snn architecture fordetecting motor execution and motor intention for use in bciapplications, in: Neural Networks (IJCNN), 2014 International Joint Conferenceon, IEEE, 2014, pp. 3221–3225.[44] J. Hu, Z.-G. Hou, Y.-X. Chen, N. Kasabov, N. Scott, Eeg-based classification of upper-limb adl using snn for active robotic rehabilitation, in:Biomedical Robotics and Biomechatronics (2014 5th IEEE RAS & EMBS International Conference on, IEEE, 2014, pp. 409–414.[45] Z. G. Doborjeh, N. Kasabov, M. G. Doborjeh, A. Sumich, Modelling peri-perceptual brain processes in a deep learning spiking neural networkarchitecture, Scientific reports 8 (1) (2018) 1–13.[46] Z. Doborjeh, M. Doborjeh, T. Taylor, N. Kasabov, G. Y. Wang, R. Siegert, A. Sumich, Spiking neural network modelling approach revealshow mindfulness training rewires the brain, Scientific reports 9 (1) (2019) 1–15.[47] S. Thorpe, J. Gautrais, Rank order coding, in: Computational neuroscience, Springer, 1998, pp. 113–118.[48] N. K. Kasabov, Evolving connectionist systems: the knowledge engineering approach, Springer Science & Business Media, 2007.16Clarence Tan et al. / Neurocomputing 00 (2020) 1–1717[49] S. G. Wysoski, L. Benuskova, N. Kasabov, Evolving spiking neural networks for audiovisual information processing, Neural Networks 23 (7)(2010) 819–835.[50] N. K. Kasabov, Time-Space, Spiking Neural Networks and Brain-Inspired Artificial Intelligence, Springer, 2018.[51] N. Kasabov, Neucube evospike architecture for spatio-temporal modelling and pattern recognition of brain signals, in: IAPR Workshop onArtificial Neural Networks in Pattern Recognition, Springer, 2012, pp. 225–243.[52] N. Kasabov, N. M. Scott, E. Tu, S. Marks, N. Sengupta, E. Capecci, M. Othman, M. G. Doborjeh, N. Murli, R. Hartono, et al., Evolving spatio-temporal data machines based on the neucube neuromorphic framework: design methodology and selected applications, Neural Networks 78(2016) 1–14.[53] W.-L. Zheng, B.-L. Lu, Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks,IEEE Transactions on Autonomous Mental Development 7 (3) (2015) 162–175.[54] S. Katsigiannis, N. Ramzan, Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelfdevices, IEEE journal of biomedical and health informatics 22 (1) (2017) 98–107.[55] P. Viola, M. Jones, et al., Rapid object detection using a boosted cascade of simple features, CVPR (1) 1 (511-518) (2001) 3.[56] S. Jianbo, C. Tomasi, Good features to track, in: IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1994, pp.593–600.[57] B. D. Lucas, T. Kanade, et al., An iterative image registration technique with an application to stereo vision, in: International Joint Conferenceon Artificial Intelligence, Vancouver, British Columbia, 1981.[58] V. Kazemi, J. Sullivan, One millisecond face alignment with an ensemble of regression trees, in: Proceedings of the IEEE conference oncomputer vision and pattern recognition, 2014, pp. 1867–1874.[59] M. Soleymani, F. Villaro-Dixon, T. Pun, G. Chanel, Toolbox for emotional feature extraction from physiological signals (teap), Frontiers inICT 4 (2017) 1.[60] W.-L. Zheng, J.-Y. Zhu, B.-L. Lu, Identifying stable patterns over time for emotion recognition from eeg, IEEE Transactions on AffectiveComputing (2017).[61] Y. Huang, J. Yang, S. Liu, J. Pan, Combining facial expressions and electroencephalography to enhance emotion recognition, Future Internet11 (5) (2019) 105.[62] Y.-P. Lin, C.-H. Wang, T.-P. Jung, T.-L. Wu, S.-K. Jeng, J.-R. Duann, J.-H. Chen, Eeg-based emotion recognition in music listening, IEEETransactions on Biomedical Engineering 57 (7) (2010) 1798–1806.[63] R. Jenke, A. Peer, M. Buss, Feature extraction and selection for emotion recognition from eeg, IEEE Transactions on Affective Computing5 (3) (2014) 327–339.[64] B. Hjorth, Eeg analysis based on time domain properties, Electroencephalography and clinical neurophysiology 29 (3) (1970) 306–310.[65] S. K. Hadjidimitriou, L. J. Hadjileontiadis, Toward an eeg-based recognition of music liking using time-frequency analysis, IEEE Transactionson Biomedical Engineering 59 (12) (2012) 3498–3510.[66] B. Petro, N. Kasabov, R. M. Kiss, Selection and optimization of temporal spike encoding methods for spiking neural networks, IEEEtransactions on neural networks and learning systems (2019).[67] S. M. Bohte, The evidence for neural information processing with precise spike-times: A survey, Natural Computing 3 (2) (2004) 195–206.[68] A. Vato, L. Bonzano, M. Chiappalone, S. Cicero, F. Morabito, A. Novellino, G. Stillo, Spike manager: a new tool for spontaneous and evokedneuronal networks activity characterization, Neurocomputing 58 (2004) 1153–1161.[69] N. Nuntalid, K. Dhoble, N. Kasabov, Eeg classification with bsa spike encoding algorithm and evolving probabilistic spiking neural network,in: International Conference on Neural Information Processing, Springer, 2011, pp. 451–460.[70] T. Delbruck, jaer open source project, http://jaer.wiki.sourceforge.net (2007).[71] P. Lichtsteiner, T. Delbruck, A 64x64 aer logarithmic temporal derivative silicon retina, in: Research in Microelectronics and Electronics,2005 PhD, Vol. 2, IEEE, 2005, pp. 202–205.[72] N. Sengupta, N. Kasabov, Spike-time encoding as a data compression technique for pattern recognition of temporal data, Information Sciences406 (2017) 133–145.[73] J. Gautrais, S. Thorpe, Rate coding versus temporal order coding: a theoretical approach, Biosystems 48 (1-3) (1998) 57–65.[74] G. Roffo, S. Melzi, M. Cristani, Infinite feature selection, in: Proceedings of the IEEE International Conference on Computer Vision, 2015,pp. 4202–4210.[75] G. Roffo, Feature selection library (matlab toolbox), arXiv preprint arXiv:1607.01327 (2016).[76] J. Talairach, P. Tournoux, Co-planar stereotaxic atlas of the human brain: 3-dimensional proportional system: an approach to cerebral imaging(1988).[77] S. Song, K. D. Miller, L. F. Abbott, Competitive hebbian learning through spike-timing-dependent synaptic plasticity, Nature neuroscience3 (9) (2000) 919.[78] M. Weymar, L. Schwabe, Amygdala and emotion: the bright side of it, Frontiers in neuroscience 10 (2016) 224.[79] G. H. Yeni-Komshian, D. A. Benson, Anatomical study of cerebral asymmetry in the temporal lobe of humans, chimpanzees, and rhesusmonkeys, Science 192 (4237) (1976) 387–389.17