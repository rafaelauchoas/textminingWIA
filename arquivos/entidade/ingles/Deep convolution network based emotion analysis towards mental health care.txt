Deep Convolution Network Based Emotion Analysis towards Mental Health Care Zixiang Fei1, Erfu Yang*1, David Day-Uei Li2, Stephen Butler3, Winifred Ijomah1, Xia Li4$, Huiyu Zhou5 1 Department of Design, Manufacture and Engineering Management University of Strathclyde, Glasgow G1 1XJ, UK {zixiang.fei, erfu.yang, w.l.ijomah}@strath.ac.uk 2 Strathclyde Institute of Pharmacy & Biomedical Sciences University of Strathclyde, Glasgow G4 0RE, UK david.li@strath.ac.uk 3 School of Psychological Sciences and Health University of Strathclyde, Glasgow G1 1QE, UK stephen.butler@strath.ac.uk 4 Shanghai Jiaotong University, Shanghai lixia11111@alumni.sjtu.edu.cn 5 Department of Informatics University of Leicester, LE1 7RH, Leicester hz143@leicester.ac.uk * Correspondence author: erfu.yang@strath.ac.uk; Tel.: +44-141-574-5279 $ Correspondence author in China: lixia11111@alumni.sjtu.edu.cn; Tel.: +8621-3477-3440 Abstract: Facial expressions play an important role during communications, allowing information regarding the emotional state of an individual to be conveyed and inferred. Research suggests that automatic facial expression recognition is a promising avenue of enquiry in mental healthcare, as facial expressions can also reflect an individual‚Äôs mental state. In order to develop user-friendly, low-cost and effective facial expression analysis systems for mental health care, this paper presents a novel deep convolution network based emotion analysis framework to support mental state detection and diagnosis. The proposed system is able to process facial images and interpret the temporal evolution of emotions through a new solution in which deep features are extracted from the Fully Connected Layer 6 of the AlexNet, with a standard Linear Discriminant Analysis Classifier exploited to obtain the final classification outcome. It is tested against 5 benchmarking databases, including JAFFE, KDEF,CK+, and databases with the images obtained ‚Äòin the wild‚Äô such as FER2013 and AffectNet. Compared with the other state-of-the-art methods, we observe that our method has overall higher accuracy of facial expression recognition. Additionally, when compared to the state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, ResNet and AlexNet, the proposed method demonstrated better efficiency and has less device requirements. The experiments presented in this paper demonstrate that the proposed method outperforms the other methods in terms of accuracy and efficiency which suggests it could act as a smart, low-cost, user-friendly cognitive aid to detect, monitor, and diagnose the mental health of a patient through automatic facial expression analysis. Keywords: Facial Expression Recognition; Deep Convolution Network; Mental Health Care; Emotion Analysis. 1. Introduction Understanding people‚Äôs emotions plays an important role in daily human communications. For advanced human-computer interaction in many emerging applications, recognizing users‚Äô emotions is also vital. Currently, there are many approaches for automated emotional recognition, including the recognition of facial expression and analysis of   voice tone [1], which exist alongside more conventional physiological measures such as measurements of blood pressure, pulse rate or skin conductivity. Automated facial expression recognition has found many practical applications such as in e-learning and health care systems [2,3]. For example, facial expressions are considered as an important feedback mechanism for teachers in terms of monitoring levels of students‚Äô understanding [3]. Within this domain, Mau-Tsuen et al. proposed an automatic system to identify how well students were learning, by means of the analysis of videos taken from learners [3], and demonstrated that such a system could help improving teaching effectiveness and efficiency. Additionally, the user interface developed in such a system could work as a cognitive tool [4] to support and understand the users‚Äô mental state better when and where it was appropriate without external actuation. Further applications of facial expression include its use in the fields of human-computer interaction and interface optimization.  Within these domains, Bahr et al. proposed a novel method to analyze postural and facial expressions to guide interface actuation and actions [4], whilst Pedro et al. employed electromyogram (EMG) sensors to investigate the relationship between users‚Äô facial expressions and adverse-event occurrences [5]. Moreover, health and medical applications of automated facial recognition are also being investigated, for example in the area of diagnostics related to developmental disorders such as autism. Here, promising work has been conducted by means of a system of facial expression analysis during social interactions, where many individuals with such disorders find challenging [6]. Modern techniques in facial expression recognition systems play important roles in these practical applications. These techniques typically involve multiple components, including face localization and facial component alignment, facial feature extraction and facial feature classification. As a simple dichotomy, facial expression analysis systems can be divided into two groups: those using static images and those using continuous video frames. Static algorithms enjoy the advantage of facial expression recognition from a single image or a video frame. In this paper, we describe an algorithm which has been tested for facial expression recognition accuracy from single images mainly acquired through webcams and mobile phone cameras. We would argue that static approaches show promising, but require rigorous and various testing conditions for robust outcomes. Deepak et al, for example, proposed a convolution neural network based algorithm to recognize facial expression with high accuracy [7]. However, their algorithm was tested on only two small facial expression datasets.  Conversely, Jie et al. also proposed three novel convolution neural network (CNN) models with different architectures [8], and tested their algorithms on several datasets such as CK+ and FER2013 datasets. Within the three architectures they presented, the data suggested that the pertained CNN with 5 convolution layers had the best performance. It should be noted however that not all researchers restrict their work to 2D images. For instance, Chenlei et al. very recently proposed a new 3D facial expression modeling method based on facial landmarks [9]. On the other hand, other researchers have moved beyond the analysis of static images and proposed facial expression recognition systems using continuous video frames. For instance, Zia, Lee and other researchers worked on facial expression recognition using temporal dynamics [10,11]. Their proposed system used Fisher Independent Component Analysis as a feature extractor and Hidden Markov Models to learn the features of six different expressions. The experiment results showed that the recognition rate was about 92.85%. Despite advances in the recognition accuracy, there remain significant issues to be addressed in the field of facial expression recognition systems.  The first issue is related to the datasets employed in testing such systems.  Some facial expression recognition systems may have good performance in some image datasets, but perform poorly in the others. For instance, deep CNN approaches often need to determine large amount of weights in the training phase. Consequently it is observed that such approaches suffer from performance decrements when being trained on a small image dataset [8]. A second important issue is ecological validity, in that most of the existing systems use lab-posed facial expressions images.  This is potentially problematic, as it ignores real-world problems such as lighting conditions, image quality and background complexity. It is fatal to address such issues for real-world applications. Facial expression recognition in the wild is a challenging topic due to such issue as well as others such as variance in poses [8].  Thirdly, both traditional approaches and deep learning based approaches have inherent weaknesses. Traditional approaches such as Local Binary Pattern (LBP), Scale Invariant Feature Transform (SIFT) and Support Vector Machine (SVM) employ manually designed features, which can result in poor performance with unseen images. Deniz et al, for instance, observed that traditional approaches performed weakly when being presented with variations in pose, and instead employed a deep CNN based approach to recognize facial expressions, which they argued resulted in improved performance [12]. Approaches such as AlexNet, Vgg, and GoogleNet have long training and testing time as well as an enormous amount of memory resource[13] and require hardware incorporating Graphics Processing Units (GPU‚Äôs), whilst it is also essential in these approaches. Extensive literature provides  a comprehensive review of the relative advantages and weaknesses of the existing facial expression systems, we would recommend those seeking further information to consult one of the several good reviews of the area (see [14‚Äì18]). Indeed, even more recently Byoung, reviewed approaches to facial emotion recognition including conventional facial expression recognition, deep learning based facial expression recognition, well-known facial expression datasets and has also examined some common performance evaluation methods for automated facial expression recognition [19].  Whilst there are many notable developments within the field of automatic face recognition systems, there remain urgent priorities to be addressed to allow these technologies to flourish within the field of mental health care. We would argue that there is vast untapped potential for the field in this area of healthcare. Globally, there are rising numbers of people suffering from cognitive impairment, and consequently there is an urgent need to develop and deploy user-friendly, low-cost and effective facial expression analysis systems for mental health care. Within mental health care, there is enormous potential for automatic facial expression recognition systems to assist clinicians working within mental health settings. Whilst it is uncontroversial to suggest that facial expressions can reflect people‚Äôs mental states [20], it has also been reported that the patients with cognitive impairments may express abnormal facial expressions [21] that are quantitatively different from those of healthy elderly people.  Burton et al, for example, reported abnormal corrugator activity in individuals with cognitive impairment when compared to a control group, when these participants were exposed to image or video stimuli [21]. The forehead muscle is integral to the emitted emotions related to frowning [22]. Work in line with this was conducted by Henry et al, who invited 20 cognitive impaired people and 20 healthy people to watch videos in order to compare their facial reactions when viewing such stimuli, reporting that the group of participants with cognitive impairment demonstrated difficulty both in facial muscle control and in the amplification of expressed emotion [23]. Similarly, Smith et al found cognitively impaired people demonstrated more negative emotions when they were exposed to negative image stimuli, which they argued was indicative of reduced emotional control [24]. Building upon such initial findings, this paper presents a proposed system that has the potential to be applied to the detection and monitoring of cognitive impairments such as dementia through the application of machine learning techniques to the automatic analysis of people‚Äôs facial expressions. Our proposed smart system is able to label and quantify the users‚Äô emotions automatically, through continuous focus upon the evolution of facial expressions over a period of time. In order to develop an efficient and effective framework for recognizing facial expression towards mental health care, this paper presents a facial expression recognition framework which effectively exploits the features extracted from the Fully Connected Layer 6 of AlexNet and the Linear Discriminant Analysis Classifier (LDA). We present the findings from a series of experiments where the performance of our proposed method are compared to that of traditional approaches such as SVM, LDA, the K-nearest Neighbor Classifier (KNN) and made further comparisons to deep learning based approaches such as AlexNet, Vgg, GoogleNet and ResNet. Within our comparisons, we have employed a broad range of well-known facial expression databases including the Karolinska Directed Emotional Faces (KDEF) Database [25], the Japanese Female Facial Expression (JAFFE) Database [26], the extended Cohn-Kanade dataset (CK+) [27,28], the Facial Expression Recognition Challenge (FER2013) [29] and the AffectNet Database [30]. From our experiments it would suggest that exploiting the features extracted from the Fully Connected Layer 6 of AlexNet with the classifier LDA can achieve the best performance in all the five testing databases. The major contributions of this paper are as follows: 1. First, our novel framework for facial expression analysis to support mental health care is presented. Our proposed system extracts deep features from the Fully Connected Layer 6 of the AlexNet, and uses a standard Linear Discriminant Analysis Classifier to train these deep features. As it is known that patients with cognitive impairments may express abnormal facial expressions when exposed to emotional visual stimuli, we would argue that our proposed facial analysis system has excellent potential to detect cognitive impairment at the early stage. 2. We present the findings from the tests of our proposed framework against both across databases with small number of images such as the JAFFE, KDEF and CK+ databases, and databases with images obtained ‚Äòin the wild‚Äô such as the FER2013 and the AffectNet databases. 3. We present the findings from the comparisons of our proposed method with both traditional methods and state-of-the-art methods proposed by other researchers. It is observed that our method has better accuracy facial expression recognition. More importantly, we have also observed that our proposed method has much less computing time and lower device requirements than the other state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, and ResNet. We would argue that these characteristics support our view that the proposed method is both competent and suitable for a facial analysis system that can be employed within mental health care settings. 4. A system which can analyze facial expressions from video stimuli, and subsequently produce an accurate evaluation of the facial expressions detected in an automated system is presented. This paper is organized as follows. Following our general overview in Section 1, we present our proposed deep learning-based framework in Section 2. Section 3 then introduces the experimental set-up for the evaluation of the proposed system to obtain the results. Our findings are then discussed within Section 4. Finally, Section 5 provides our conclusions. 2. Materials and Methods 2.1 Overview of the Whole System Structure In the current research, the inputs to the system are videos of people‚Äôs frontal faces. Image pre-processing is then applied to the video streams. The analysis of the acquired videos is carried out using the deep convolution network AlexNet combined with traditional classifiers such as SVM, LDA and KNN. Finally, the system we present analyzes the emotions acquired in the videos and reports the evolutions of the emotions detected over a period of time. The general structure of the system is shown in Figure 1. In the system, the first stage is the system input.  Here we take video frames of facial expressions from 120 seconds of video with a rate of 30 frames per second. As a result, a 120-second video is thus converted to 3600 images. The image pre-processing technique can then be applied to such input image frames.  This stage has two main operations: removal of unnecessary image parts such as background environmental aspects of the image, and removal of hair; and resizing of the images. Within our approach, firstly a face detector is used to locate the position of the face in the images using the standard Viola-Jones algorithm [31,32]. Next, the appropriate face part is cropped from the images. Finally, the cropped face part is resized to the required size, which is decided by the input of the deep learning network. In our proposed system, we utilize well-established and reliable AlexNet to extract the deep features from the images which have been applied with the aforementioned image pre-processing techniques. Finally, at this stage the image will be also converted into RGB format by concatenating the arrays [33] to meet the requirement of the deep learning network if the video frames are grayscale.   Figure 1. Structure of the whole system [34] In the third stage, facial expression analysis techniques are applied to the pre-processed images. These techniques use a combination of deep learning networks i.e. AlexNet, and traditional classifiers such as SVM, LDA and KNN, which will be introduced in detail in the next section. Facial expressions in the video stimuli are classified into five facial expression groups, namely neutral, happy, sad, angry and surprised. In addition, the probability of each facial expression category for every frame is established. By combining all the facial expression recognition results in the image frames, we obtain the evolution of the probability of each type of facial expression over a period of time. 2.2 Convolution Neural Networks and Proposed Framework 2.2.1 Overview of Convolution Neural Networks Convolution Neural Networks (CNNs) are a type of deep learning network that needs less image pre-processing compared to other traditional image classification algorithms [1]. CNNs have an advantage in that they do not need prior knowledge and manually design the features. They have many applications in various domains including natural language processing and computer vision [36]. A typical CNN has an input layer, an output layer, and hidden layers such as convolution layers, pooling layers and fully connected layers. 2.2.2 AlexNet AlexNet is a type of CNN that was designed by Alex Krizhevsky in the SuperVision group[37]. The AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge in 2012, and achieved a top-five error of 15.3%. It has eight major layers in total including five convolution layers and three fully connected layers. The detailed network structure is shown in Figure 2. The original AlexNet was trained on a subset of the ImageNet database which contains more than one million images, and was able to classify images into 1000 object categories [34]. In the AlexNet, the input receives images and the output includes the label of the images and the probabilities for each of the object categories. In our experiments, the AlexNet is run in Matlab. Moreover, the transfer learning strategy is used to reduce the training time of the network [34]. By using the transfer learning, a much smaller number of training images are needed. There are several steps needed for transfer learning in a pre-trained AlexNet. Figure 2. Structure of the AlexNet [34]. There are 25 layers in the AlexNet, which begins with an image input layer. Next, there are five convolution layers to extract facial features. The output of the activation function forms the neurons of the current layer. As a result, it will form the feature map of the current convolution layer. The calculation can be described in the following function [37‚Äì39]:              ùëãùëóùëô = ùêπ(‚àëùëñ‚ààùëÄùëóùëô‚àí1 ‚àóùëãùëñùë§ùëñùëóùëô +  ùë§ùëè)            Ôºà1Ôºâ    Where, ùëãùëóùëô‚àí1 is the feature map of the output of the l-1 layer, * depicts the convolution operation, ùë§ùëñùëóùëô and ùë§ùëèrepresents weight and the bias, respectively. Each convolution layer is followed by the ReLU (Rectified Linear Units) layer in order to increase the nonlinear properties of the network. The ReLU is a half-wave rectifier function with the advantage of reducing the training time whilst preventing overfitting [40]. In addition, ReLU layers can prevent the gradient vanishing problem and are much faster than other logistic function [41]. The ReLU layers for input x can be described as [37‚Äì39]:                ùêπ(ùë•) = ùëöùëéùë• (0, ùë•)                (2) In addition, convolution layers are also followed by the max pooling layers. The max pooling layers are used for down-sampling. The number of feature maps won‚Äôt be changed by the down-sampling process. On the other hand, the down-sampling process removes unnecessary information and reduces the number of the parameters of the feature map. The down-sampling layer can be described by [37‚Äì39]:               ùëãùëóùëô = ùêπ(ùëëùëúùë§ùëõ(ùëãùëóùëô‚àí1) +  ùë§ùëè)             Ôºà3Ôºâ ùëô‚àí1 represents the j feature map of the pooling layer l, and ùë§ùëè depicts the offset term of the down-sampling where ùëãùëólayer. Finally, in the AlexNet, after the five convolution layers, there are three fully connected layers: FC6, FC7 and FC8. The fully connected layers can be considered as a convolution layer. Also, its convolution kernel size and the input data size should be consistent with those used in the convolution layer. The Fully Connected Layer can be described by [37‚Äì39]:               ùëãùëóùëô = ùêπ(‚àë ùë§ùëñùëóùëô ùëãùëóùëñùëô‚àí1+  ùë§ùëèùëô )              Ôºà4Ôºâ 2.2.3 Deep Feature Extraction In this context, the AlexNet works as a deep feature extractor to extract image features.  We then use these features to train traditional classifiers such as SVM, LDA and KNN. In spite of the possible improvement in recognition accuracy, feature extraction remains fast and relatively simple, using the representational power of the pre-trained deep networks [42]. In addition, as feature extraction only requires a single pass through the data, a CPU is also able to do this work. In this work, a pre-trained CNN works as the deep feature extractor. The advantages of using a pre-trained CNN to extract deep features, compared to the training of a new CNN, are: 1) Less computational power is needed and 2) less data is needed to achieve high recognition accuracy [43]. The work we present used a pre-trained AlexNet, which was trained with more than a million images so that the network model has learned rich feature representations. To demonstrate what features can be learned by the AlexNet, we need to visualize the deep features extracted by the network. In our work, we employed the T-SNE (Stochastic Neighbor Embedding) approach to visualize the features extracted by the AlexNet. The T-SNE - (TSNE) is an algorithm for dimensionality reduction [44,45]. This algorithm allows us to visualize the high-dimensional data of the facial images. The T-SNE function will convert high-dimensional data into low dimensional data. Generally, distant points in high-dimensional space will be converted into distant embedded low-dimensional points and nearby points in the high-dimensional space will be converted into nearby embedded low-dimensional points. As a result, we can visualize the low-dimensional points to find the clusters in the original high-dimensional data. The features extracted by the AlexNet which have transfer-learned facial images from the KDEF dataset are shown in Figure 3.   Figure 3. Using the T-SNE to visualize the features extracted by the AlexNet. 2.2.4 Layer Selection In this study, we aimed to achieve the best performance by selecting the most appropriate layer to extract the features including the Fully Connected Layer 6 (FC6), Fully Connected Layer 7 (FC7) and Fully Connected Layer 8 (FC8) from the AlexNet. While extracting the features from deep CNNs, deeper layers contain higher level features and earlier layers produce lower-level features [42]. In order to obtain the feature representations of the training and testing images, we can use the activations on FC7 or we can obtain lower-level representations of the images from FC6. Meanwhile, the earlier layers may learn features like colors and edges [46]. However, in the deeper layers, the network may learn complicated features such as eyes. We will do experiments to explore, evaluate and compare which layer in the AlexNet is most appropriate to extract the deep features in our study. 2.2.5 Traditional Classifiers The deep features or ‚Äòactivations‚Äô can provide the input of traditional classifiers such as SVM, LDA and KNN in order to further improve the recognition accuracy. Therefore, we also investigated which traditional classifier combined with the AlexNet can achieve the best recognition accuracy. Each of the traditional classifiers has its own characteristics. For example, SVM can use kernels to transform many feature representations into a higher dimensional space in order to classify multiple classes [43]. In addition, SVM has good performance in object classification and face detection applications. On the other hand, the LDA approach is able to find the optimal transformation which can better separate different classes [47]. The LDA has wide applications such as face recognition and image retrieval [48]. In the LDA, when ùëä represents an optimal set of discriminant projection vectors [48], the LDA can be represented as a function of ùëä:                                ùêΩ(ùëä) = |ùëäùëáùëÜùêµùëä||ùëäùëáùëÜùëäùëä|  (5) In (5), ùëÜùêµ and ùëÜùëä are between class scatter matrix and within class scatter matrix, respectively. They are given by    ùëÜùëè = ‚àë ùëÄùëñùëÅùëñ=1(ùëöùëñ ‚àí ùëö)(ùëöùëñ ‚àí ùëö)ùëá    (6)                                ùëÜùëä =  ‚àë ùëÄùëñùëÅùëñ=1     (7)    where                              ùëÄùëñ =  ‚àë(ùë• ‚àí ùúáùëñ)(ùë• ‚àí ùúáùëñ)ùëá  (8) ùëã ‚àà ùë•ùëñThe comparison of these traditional classifiers are presented in the experimental results section. 3. Results In order to evaluate and identify the approach with the best recognition accuracy, different methods were tested in our experiments. To this end, we tested the pre-trained deep CNN AlexNet with the initial learning rate 0.0003, minimum batch size 5 and maximum epochs 10 [34]. The test of the traditional classifiers included multiclass model for SVM, the LDA with linear Discriminant-Type and the KNN with Euclidean Distance and 1 neighbor number. We also conducted some experiments to test the influence of the hyperparameters in the training stage. For the proposed method (AlexNet + FC6 + LDA), we use the LDA classifier to train and classify the deep features extracted from the AlexNet. We have tried to use the ‚ÄòOptimizeHyperparameters‚Äô function in Matlab to find the optimized hyperparameters. We found that the time cost outweighs the small performance improvement. In the LDA, the function will try to optimize the performance by changing hyperparameters Delta and Gamma automatically. By using the OptimizeHyperparameters‚Äô function, we found the operating time is increased by 100 times and there is limited improvement the ‚ÄòOptimizeHyperparameters‚Äô function has the drawback of greatly increasing the operating time, which is not appropriate in our research. As a result, we use the default hyperparameters for the LDA classifier. the LDA classifier. We found in recognition accuracy the JAFFE dataset for in We also tested the combination of the AlexNet with traditional classifiers [49]. As the layer selected to extract the features affects the recognition result, we conducted the experiments using different layers to extract the deep features. The explored layers are the FC6, FC7, and FC8 of the AlexNet. We first tested the combination of the AlexNet and the LDA using FC6, FC7 and FC8 to investigate which layer may have the best recognition accuracy. We subsequently discovered that FC6 demonstrated the highest recognition accuracy. We then used FC6 to extract the deep features and experimentally tested which classifier demonstrated the highest recognition accuracy among SVM, LDA and KNN. We found that the LDA showed the best recognition performance. In summary, we tested the recognition accuracy of facial expressions using the following methods: deep convolution neural networks AlexNet, traditional classifiers SVM, LDA and KNN and the combination of the AlexNet and traditional classifiers using FC8 and LDA, FC7 and LDA, FC6 and LDA, FC6 and SVM, and FC6 and KNN, respectively. Additionally, in order to compare the recognition accuracy among the nine methods, we conducted our experiments on different facial expression databases in this research. Specifically, we used JAFFE, KDEF, CK+, FER2013 and AffectNet Datasets. Moreover, we used the proposed system to quantify the evolution of facial expressions from a video of facial expressions emitted by the first author as the ground truth is known in this case. The detailed experimental outcomes of these comparisons are reported in the following sections. 3.1 Experiment Using the Online Datasets 3.1.1 JAFFE Dataset Experiments To begin with, the JAFFE is a facial expression database with only 213 static images. By using the JAFFE Dataset, we aim to test the influence of small number of images in training the system using different methods. From the JAFFE Dataset, we selected 202 images that were all processed using the image preprocessing techniques mentioned above (it was observed that the JAFFE dataset included some incorrectly labeled facial expressions, which were removed [26]). There are 7 different facial expressions in this dataset: angry, happy, neutral, surprised, sad,   afraid, and disgusted. In each test, 70% of the images were randomly selected as the training images, with the rest of the images serving as testing images. Table 1 compares the 5-fold Cross Validation Error Rate for the facial expressions using the JAFFE Dataset with each different method. The first column in the table shows the method used for facial expressions recognition. The second column shows the 5-fold Cross Validation Error Rate for each method. As the table shows, our proposed method (AlexNet + FC6 + LDA) reaches the lowest error rate of 5.9%. As the number of images in the JAFFE Database is quite small, the deep CNN AlexNet does not demonstrate satisfactory performance. Also, the performance of the AlexNet appears to be quite near to that of the traditional classifiers such as LDA and SVM. Furthermore, Table 2 also shows the recognition accuracy for each emotion and the overall recognition accuracy rates obtained when using each different method for the JAFFE Dataset. The first column in the table shows the method used for facial expressions recognition whilst the second to the eighth column shows the recognition accuracy of each method for angry, disgusted, fearful, happy, neutral, sad and surprised faces, respectively. The ninth column shows the overall recognition accuracy of each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. The method using AlexNet, FC6 and LDA has the highest overall recognition accuracy, but does not demonstrate good performance in recognizing ‚Äòsurprise‚Äô. In general, ‚Äòhappy‚Äô is the most easily recognized emotion by this method, while ‚Äòsurprised‚Äô is the most difficult emotion category to be recognized. Table 1. Comparison of Cross-Validation Error Rate Using Different Methods for the JAFFE Dataset Method AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Error Rate (%) 24.8 26.7 24.8 39.6 18.8 9.4 5.9 10.9 15.4 Table 2. Comparison of Recognition Accuracy of Each Emotion and Overall Recognition Accuracy Using Different Methods for the JAFFE Dataset Method Angry (%) Disgust (%) Happy (%) Neutral (%) Fear (%) 33.3 55.6 66.7 33.3 88.9 100 88.9 88.9 77.8 88.9 88.9 66.7 88.9 77.8 100 37.5 62.5 62.5 62.5 87.5 100 87.5 88.9 100 100 87.5 100 100 88.9 87.5 88.9 100 77.8 87.5 44.4 88.9 77.8 55.6 100 100 88.9 100 100 100 100 AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Sad (%) 87.5 37.5 62.5 75.0 75.0 87.5 100 87.5 87.5 Surprise (%) 87.5 25.0 25.0 37.5 50.0 25.0 75.0 37.5 62.5 Overall Accuracy for Each Method (%) 73.3 56.7 71.7 66.7 83.3 85.0 95.0 85.0 78.3  Average Accuracy for Each Emotion 87.7 73.6 66.7 92.6 91.4 77.8 47.2 77.2 3.1.2 KDEF Dataset Experiments The KDEF Dataset contains 4900 facial expression images. As our system aims to quantify the evolution of emotion from front-view videos of facial expressions, we have only used front-view images. From the KDEF Dataset, we selected all 980 front-view images of facial expressions that were all processed using the images preprocessing techniques outlined above [25]. There are seven different facial expressions in this dataset: angry, happy, neutral, surprised, sad, fearful, and disgusted. In the experiment, 70% of the images were again randomly selected as the training images, with the rest of the images used as the testing images. Table 3 compares the 5-fold Cross Validation Error Rate for the facial expressions using the KDEF Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method. As the table shows, our proposed method has the lowest error rate at 11.6%. However, as the number of images in the KDEF was greatly increased compared to the JAFFE Database, the performance of the AlexNet can be seen to be significantly   improved, with an error rate now 4% higher than the proposed method. On the other hand, the traditional classifiers such as LDA and SVM did not show such a good performance when there were tested with such a large image database. In addition, Table 4 shows the recognition accuracy for each emotion category, and the overall recognition accuracy when using each different method for the KDEF Dataset. The first column in the table shows the method used for facial expressions recognition whilst the second to eighth columns show the recognition accuracy of each method for angry, disgusted, fearful, happy, neutral, sad and surprised faces respectively. The ninth column shows the overall recognition accuracy of each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. The proposed method of using AlexNet, FC6 and LDA clearly demonstrates the highest recognition accuracy. Moreover, we can observe that ‚Äòhappy‚Äô is still the easiest emotion to be recognized, while ‚Äòsad‚Äô and ‚Äòangry‚Äô are the two most difficult emotions to be recognized. Table 3. Comparison Cross-Validation Error Rate Using Different Method for the KDEF Dataset Method AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Error Rate (%) 15.1 36.3 32.1 56.0 17.2 12.0 11.6 12.6 32.6 Table 4. Comparison of Recognition Accuracy of Each Emotion and Overall Recognition Accuracy Using Different Methods for the KDEF Dataset Method Angry (%) Disgust (%) Fear (%) Happy (%) Neutral (%) AlexNet LDA 76.2 47.6 81.0 69.0 92.9 50.0 97.6 83.3 88.1 71.4 Sad (%) 81.0 50.0 Surprise (%) 76.2 71.4 Overall Accuracy for Each Method (%) 84.7 63.3  SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Average Accuracy for Each Emotion 57.1 38.1 83.3 66.7 35.7 69.0 66.7 31.0 83.3 81.0 47.6 97.6 54.8 50.0 88.1 52.4 35.7 73.8 83.3 81.0 85.7 100 85.7 78.6 78.6 85.7 83.3 100 92.9 83.3 78.6 83.3 90.5 97.6 88.1 78.6 40.5 64.3 45.2 88.1 78.6 54.8 76.2 57.1 90.5 85.7 90.5 88.1 76.2 65.0 42.2 83.7 85.7 87.8 86.4 64.0 64.8 70.6 69.8 88.1 77.5 65.4 79.1 73.6 3.1.3 CK+ Dataset Experiments The CK+ Dataset consists of 593 sequences of facial expressions [27,28]. Each video sequences can be regarded as a few continuous video frames. As a result, this is a large database of around 10,000 images of facial expressions taken from 123 models. As these image sequences are continuous, there are many similar images. In our experiment, after removing similar images, 693 images were selected and processed using the image preprocessing techniques described. We selected images with seven different facial expressions in the dataset: angry, happy, neutral, surprised, sad, fearful, and disgusted. We again randomly selected 70% of the images as the training images, with the remainder employed as the testing images. Table 5 shows the 5-fold Cross Validation Error Rate for the facial expressions from the CK+ Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method. As the table shows, our proposed method still obtains the lowest error rate at 3.6%. Generally, in the CK+ Dataset, two images are selected for each emotion for each subject, with one image being the frame when the emotion begins to be expressed whilst the other image is the frame in the image sequence when the emotion reaches the peak of its expression. All the methods appear to have good performance in this dataset. Additionally, Table 6 shows the recognition accuracy for each emotion, and the overall recognition accuracy rate when employing each different method for the CK+ Dataset. The first column in the table shows the method used for facial expressions recognition, with the next seven columns showing the recognition accuracy for each method for angry, disgusted, fearful, happy, neutral, sad and surprised facial images respectively. The ninth column shows the overall recognition accuracy of each method tested. Also, the last row shows the average recognition accuracy using all the 9 methods for each emotion. Within this dataset, ‚Äòsad‚Äô seems to be the most difficult emotion to be recognized, with several methods wrongly classifying the sad emotion as ‚Äòneutral‚Äô. The difficulty of recognition of sad emotion will be discussed in the discussion part. Table 5. Comparison Cross-Validation Error Rate Using Different Methods for the CK Dataset Method AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Error Rate (%) 10.9 10.7 8.4 24.2 8.0 5.2 3.6 6.7 21.6  Method Angry (%) Disgust (%) Fear (%) Happy (%) Neutral (%) Table 6. Comparison of Recognition Accuracy of Each Emotion and Overall Recognition Accuracy Using Different Method for the CK Dataset 95.7 78.3 73.9 65.2 87.0 85.7 88.6 91.4 77.1 77.1 81.8 72.7 81.8 63.6 63.6 97.4 97.4 92.1 65.8 97.4 76.2 88.1 95.2 59.5 95.2 91.3 82.9 72.7 97.4 97.6 33.3 100 91.4 100 100 97.6 58.3 91.3 80.0 81.8 97.4 97.6 41.7 73.9 85.7 63.6 68.4 66.7 33.3 Sad (%) 33.3 33.3 66.7 33.3 33.3 Surprise (%) 97.8 91.1 88.9 64.4 91.1 93.3 95.6 97.8 71.1 Overall Accuracy for Each Method (%) 86.4 85.4 87.9 64.1 85.4 88.3 94.7 89.8 69.9 AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Average Accuracy for Each Emotion 84.1 84.4 75.7 90.4 86.0 40.7 87.9 83.5 3.1.4 FER2013 Dataset Experiments The FER2013 is an online open dataset for facial expressions [29]. This dataset consists of more than 30,000 48*48 pixel grayscale images of faces. There are also seven types of emotions in this dataset: angry, disgusted, fearful, happy, sad, surprised and neutral. However, unlike the JAFFE and KDEF datasets that are lab-posed facial expressions, the FER2013 dataset consists of facial images taken from the internet. In the JAFFE and KDEF databases, facial expression of the same emotional category are similar to each other. However, in the FER2013 dataset, very large variations in facial expression can be observed, within the same category of emotion. In the current experiment, about 30,000 images were selected. Image preprocessing techniques were not applied to this dataset, as the original 48*48 pixel grayscale images are already quite small. We tested with all seven categories of facial emotion available in the database, again randomly selecting 70% of the images as the training set and using the remainder of the dataset as the testing images. Table 7 compares the 5-fold Cross Validation Error Rate for the facial expressions using the FER2013 Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method tested. As the table shows, the proposed method still obtains the lowest error rate at 43.5%. However, as the FER2013 dataset is primarily drawn from the internet, with a broad range of individual variation in terms of expression within the same category of emotion, we can see from the data that all approaches demonstrated difficulty in accurate classification, with no method demonstrating good levels of performance. Table 8 shows the recognition accuracy for each emotion category, and the overall recognition accuracy using each particular method with the FER2013 Dataset. The first column in the table shows the method used for facial expressions recognition, whilst the next seven columns show the recognition accuracy of each method for angry, disgusted, fearful, happy, neutral, sad and surprised faces respectively. The ninth column shows the overall recognition accuracy of each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. In this dataset, happy and surprised facial categories seem to be the simplest emotions to be recognized.  However, even the method using the AlexNet, FC6 and LDA fails to show good performance in recognizing fear emotion. Table 7. Comparison Cross-Validation Error Rate Using Different Methods for the FER2013 Dataset Method AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Error Rate (%) 44.6 68.6 73.4 61.7 50.2 46.5 43.5 48.2 49.6 Table 8. Comparison of Recognition Accuracy of Each Emotion and Overall Recognition Accuracy Using Different Methods for the FER2013 Dataset Method Angry (%) Disgust (%) Fear (%) Happy (%) Neutral (%) 42.0 18.7 14.7 28.0 37.5 43.8 14.6 27.0 54.0 25.5 34.2 18.4 13.0 31.7 24.6 81.3 47.6 38.0 38.0 70.9 58.2 28.9 20.5 38.8 50.1 42.3 32.1 28.5 72.3 55.8 46.4 43.8 36.5 30.7 74.9 59.9 52.1 42.2 41.6 36.9 70.0 45.6 40.3 40.1 59.9 42.1 61.6 43.0 39.5 Sad (%) 43.0 21.1 25.6 30.9 41.5 Surprise (%) 65.6 43.9 40.1 52.4 65.3 67.2 67.5 69.5 66.9 Overall Accuracy for Each Method (%) 56.3 30.8 26.2 36.5 49.8 53.5 56.4 51.7 49.5 34.5 35.8 28.9 62.1 44.3 37.5 60.9 45.8 AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Average Accuracy for Each Emotion 3.1.5 AffectNet Dataset Experiments The AffectNet is also an online dataset which consists of about one million images of facial expressions which were again collected from the Internet, through three major search engines and 1250 emotion related keywords [30]. This dataset provides eleven emotion and non-emotion labels including: Neutral, Surprised, Happy, Sad, Fearful, Disgusted, and Angry, whilst additionally providing the categories of Contemptuous, None, Uncertain and No-Face. In line with the FER2013 Dataset, the AffectNet contains the images of facial expressions which are naturally occurring rather than containing the images posed within a lab. In the current study, about 16,000 images were selected. Image preprocessing techniques were not applied to this dataset. We selected the seven emotional expressions that were in line with the previous four datasets tested and again we randomly selected 70% of the images to serve as the   training images, while the remaining images were again employed as testing images. Table 9 compares the 5-fold Cross Validation Error Rate for the facial expressions using the AffectNet Dataset with each different method. The first column in the table shows the method used for facial expressions recognition, whilst the second column shows the 5-fold Cross Validation Error Rate for each method. As the table shows, the proposed method has the lowest error rate at 39.43%. As the AffectNet dataset is similar to the FER2013 dataset in employing images of facial expressions taken from the Internet, the huge variance within the same class of emotion can again be seen to result in low recognition accuracy rates.  Table 10 shows the recognition accuracy of each emotion category, and the overall recognition accuracy using each different method with the AffectNet Dataset. The first column in the table shows the method used for facial expressions recognition, whilst the second to the eighth columns show the recognition accuracy for each method tested for angry, disgusted, fearful, happy, neutral, sad and surprised emotional expressions respectively. The ninth column shows the overall recognition accuracy for each method. Also, the last row shows the average recognition accuracy using all 9 methods for each emotion. In general, it can be seen that these methods show their best performance in recognizing happy emotions, which is similar to the performance observed with the FER2013 dataset. On the other hand, each method demonstrates its worst performance in recognizing the emotion of surprise. Table 9. Comparison Cross-Validation Error Rate Using Different Methods for the AffectNet Dataset. Method AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN Error Rate (%) 40.81 59.97 60.92 68.54 48.02 43.55 39.43 45.28 59.81 Table 10. Comparison of Recognition Accuracy of Each Emotion and Overall Recognition Accuracy Using Different Methods for the AffectNet Dataset Method Angry (%) Disgust (%) Fear (%) Happy (%) Neutral (%) AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN 58.2 17.8 20.8 14.5 25.1 22.0 4.3 9.1 8.1 5.4 14.3 8.9 12.5 15.6 19.6 85.6 60.8 61.8 43.1 78.8 50.5 38.5 37.3 34.0 53.1 33.1 11.8 22.8 78.6 59.2 25.1 36.2 12.4 25.4 83.2 64.1 32.7 38.0 15.6 31.3 77.2 49.1 32.3 18.8 8.6 19.2 59.4 35.6 21.4 Sad (%) 17.0 14.3 19.4 19.4 17.0 Surprise (%) 36.3 14.4 16.1 7.6 26.9 34.0 32.0 32.0 17.6 Overall Accuracy for Each Method (%) 58.6 38.9 40.3 30.7 52.1 56.0 60.1 54.6 39.2  Average Accuracy for Each Emotion 29.2 10.8 18.8 69.8 46.8 22.1 24.1 47.8 3.2 Experiment Using the Author‚Äôs Facial Expressions In this experiment, our proposed framework was used to recognize facial expression taken from a video of facial expressions emitted by the first author as the ground truth of emotions are perfectly known in this case. Within the video stimulus there were 898 continuous frames, which were preprocessed by the techniques previously outlined. The framework mainly aimed to recognize 5 kinds of emotion in the video stimulus: angry, happy, neutral, surprised, and sad. The training images used 20% of the frames from the video, combined with another dataset emitted by the author containing about 2600 images, whilst the remaining 80% images from the video acted as the testing images in this case. In the subsequent testing, the proposed system successfully classified the images into five facial expression categories: angry, happy, neutral, sad and surprised. In this experiment, as Figure 4 illustrates, the accuracy was about 96.0%. Figure 4. Recognition Results Using the Proposed Framework Figure 5 shows the probability of each type of emotion, which is made up of predicted results for each frame in the video. The graph displays the evolution of facial expressions from the video and changes in facial expressions over time. In the graph, the probability of the five facial expression categories for every frame was predicted by the proposed framework. Each line represents the evolution of one emotion. In Figure 5, the five lines show the evolution of five emotions over a period of time. In addition, the plot was smoothed by calculating the average of the recent frames. As an electrocardiogram can reflect the electrical activity of the heart, Figure 5 reflects the mental state of the patient/ user over a period of time. As shown in Figure 5, around the 300th frame, there are three emotions: happy, neutral and sad. The evolutions of the three emotions at that time are also shown. In addition, this figure illustrates when the user was said to be happy, the duration of the emotion, and how quickly it reached the peak of the emotion. By data analysis, the plot also shows the relative percentage of time for each emotion over the period. As a result, in the area of mental health care, the proposed system clearly has the potential to identify the emotional state of the user. As patients with severe cognitive impairments may express abnormal facial expressions [21], the proposed system may have the potential to be used diagnostically in the future.   1.000.900.800.700.600.500.400.300.200.100.001621567101621151671102622152672103623153673104624154674105625155675106626156676107627157677108628158678AngryHappyNeutralSadSurpriseFigure 5. Facial Expression Analysis for the Video Stimulus of the Author Own Emotional Expressions 4. Discussion Facial expressions are an important component of human social interaction, reflecting people‚Äôs emotions, attitudes, social relations and physiological state. The automated facial expression analysis can play an important role in human-computer interaction in many important applications. On the other hand, deep learning is a dynamic and vibrant topic, encompassing diverse and useful applications, such as the use of A DSAE-based deep learning framework for facial expression recognition and the use of a deep-belief-network-based particle Filter for Analysis of Gold Immunochromatographic Strips[50] [51]  [52]. In this work, we have proposed a deep learning based facial expression recognition framework towards mental health care. Table 11. Comparison Cross-Validation Error Rate Using Different Methods over 5 Databases Method AlexNet LDA SVM KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN JAFFE KDEF 24.8 26.7 24.8 39.6 18.8 9.4 5.9 10.9 15.4 15.1 36.3 32.1 56.0 17.2 12.0 11.6 12.6 32.6 CK+ 10.9 10.7 8.4 24.2 8.0 5.2 3.6 6.7 21.6 FER2013 AffectNet 44.6 68.6 73.4 61.7 50.2 46.5 43.5 48.2 49.6 40.8 60.0 60.9 68.5 48.0 43.6 39.4 45.3 59.8 In the experiments presented here we have tested the facial expression recognition performance of nine methods, including deep convolution neural network AlexNet, the traditional classifiers SVM, LDA and KNN and the combination of the AlexNet and traditional classifiers against five datasets. The overall performance can be seen in Table 11, which shows the 5-fold cross validation error rate of each method for the five datasets employed in our testing. The first column in the table shows the method used for facial expressions recognition, while the second    column to the sixth columns show the error rate of each method for JAFFE, KDEF, CK+, FER2013 and AffectNet respectively. Our experiments demonstrate that in a small dataset like the JAFFE and CK+ datasets, the deep convolution neural network AlexNet and some traditional classifiers like SVM and LDA have similar facial expression recognition accuracy. However, by combining the AlexNet with traditional classifiers like SVM and LDA, the recognition accuracy increases, especially when we extract the deep features from FC6. The experiments show that the method that extracts features from FC6 has better performance than FC7 and FC8 in the five online datasets. We use the AlexNet which is pre-trained with one million natural images from ImageNet to extract features. The features extracted from FC7 are more in line with the classification attribute of the training set of natural images but less in accordance with the dataset of facial expressions [53]. As a result, the methods that extract the features from FC6 have better performance. Additionally, we have observed that using the LDA to classify the deep features results in better performance. When the number of the images in the training dataset increased, such as in the case of the KDEF dataset (which contains about 1000 images), the AlexNet seemed to have better recognition accuracy relatively to its performance when tested with a small image database. On the other hand, we have observed that classifying facial expressions with traditional classifiers did not show good overall performance. As a result, in the KDEF dataset, the recognition accuracy of the combination of the AlexNet and traditional classifiers only demonstrated slightly better performance than that of the AlexNet. In addition, it is noticed that the overall recognition accuracy for the FER2013 and AffectNet databases are lower than that of the JAFFE, KDEF and CK+ databases. We would argue that this is mainly as a result of the FER2013 and AffectNet databases containing the more challenging facial expressions sourced from the Internet, which have huge variance within a given class of emotion for example the difference in image sizes and lighting situations. These facial expressions are more natural and diverse, resulting in stimuli that are more difficult to recognize than the lab-based, controlled and actor-generated facial expressions. Although the recognition performance for the AffectNet is worse than the performance in the JAFFE, KDEF and CK+ databases, the proposed framework has a relatively good performance for the AffectNet database compared to the other state-of-art facial expression recognition algorithms [54,55]. Figure 6 shows the accumulative recognition error rate for the nine methods over the five databases. 300250200150100500AlexNetLDASVMKNN AlexNet +FC8 +LDAAlexNet +FC7 +LDAAlexNet +FC6 +LDAAlexNet +FC6 +SVMAlexNet +FC6 +KNNJAFFEKDEFCK+FER2013AffectNetFigure 6. Comparison of Accumulative Cross-Validation Error Rates Using Different Methods over 5 Databases In general, the method that extracts the deep features using FC6 from the AlexNet and classifies with LDA showed the best overall performance in the five databases tested using these nine methods. Our experiment result   showed a facial expression recognition accuracy of 94.1% on the JAFFE database and 88.4% on the KDEF database, which is a relatively good performance compared to other facial expression recognition algorithms tested on the same databases, as shown in Table 12. The recognition accuracy is calculated from the error rate. Table 12. Recognition Accuracy from published papers on KDEF and JAFFE datasets Accuracy for JAFFE / / / 91.6% 87.1% Accuracy for KDEF 83.0% 74.6% 82.4% / / System DeepPCA[56] AAM+SVM[57] Feature+SVM[58] C+CNN[59] HF[60] Proposed Method (5-fold cross validation recognition accuracy) 88.4% 94.1% In order to further estimate the performance of the proposed method, we compared the proposed method (AlexNet + FC6 + LDA) with some state-of-the-art deep CNN including pure AlexNet, VGG16, GoogleNet and ResNet. We estimated the performance mainly with regard to the operating time of training the network, recognizing the facial expressions and in terms of the recognition accuracy of the facial expression categories. Three facial expression datasets including the JAFFE, KDEF and CK+ Datasets were used in this estimation. Figure 7 shows both the recognition accuracy and the operating time for each method with the JAFFE, KDEF and CK+ Datasets. In Figure 7, the recognition accuracy is shown as clustered columns, with the operating time superimposed as a line chart. We can observe that the proposed method has high recognition accuracy compared to the other deep learning algorithms, but slightly lower than that of the ResNet. However, this should be considered in light of the clear reduction in operating time, as the operating time of the proposed method is around 100 times shorter than that of the ResNet. In addition, it is important to note that deep learning algorithms have high device requirements relating to GPU resources and local dynamic random-access memory requirements.  Indeed in the current assessment, the Vgg16 failed to produce a recognition result in the KDEF and CK+ datasets due to insufficiency in memory resource to complete the task. In general, the proposed method can be seen to have relatively good recognition accuracy, much shorter operating time and low device requirements compared to the state-of-art deep learning algorithms. Figure 7. Comparison of Recognition Accuracy and Operating Time for Different Methods in the JAFFE, KDEF and CK+ Datasets   Another important aspect is the ratio of training images to testing the images employed in assessments. This can enable us to determine the influence of training image ratios for different algorithms on the selected dataset. In the experiments presented, we selected 70% of the images randomly from the whole dataset as the training dataset and used the remaining images as the testing dataset. Here we choose to test the JAFFE Database. Figure 8 shows the different ratios of the training images with the different methods for the JAFFE Database and the resulting recognition accuracy. We observe that when the training images ratios increases, the recognition accuracy for all the methods is increased. The result also suggests that the proposed method using the AlexNet, FC6 and LDA demonstrates the best performance, regardless of the training images ratio examined. Indeed, when we select 90% of the images randomly from the JAFFE Database to act as the training images, and use the remaining images as the testing dataset, the recognition accuracy of the proposed method reaches 97.0%. We can assume that when the training images ratios increase, the recognition performance for other datasets will also be improved. ycaruccAnoitingoceR100.00%90.00%80.00%70.00%60.00%50.00%40.00%30%50%70%90%Training Images RatiosAlexNetSVMLDAAlexNet + FC7 + SVMAlexNet + FC6 + LDAFigure 8. The Influence of the Training Images Ratios on Recognition Accuracy with the Different Algorithms for the JAFFE Database In the experiment, the first phase of the proposed method involves image pre-processing, including aspects such as removing unnecessary environment aspects in the image, identification of the facial region in the image, and some additional essential pre-processing steps. To test the efficiency of this phase we also conducted experiments to compare the 5-fold cross validation error rate using each particular method, for both processed and unprocessed images from the CK+ dataset. The results of these experiments are shown in Table 13 below. The first column in the table shows the method used for facial expressions recognition, with the second and third columns showing the 5-fold Cross-Validation Error Rate of each method for both processed and unprocessed images respectively. As Table 13 shows, the error rate is decreased in all the algorithms for the processed dataset, which proves the efficiency of the image-preprocessing phase. Additionally, we can observe that the image pre-processing phase improves the error rate performance of the traditional classifier to a greater extent, but the combination of the AlexNet and the traditional classifiers seems less dependent on this phase. Table 13. Comparison Cross-Validation Error Rate Using Different Methods for Processed and Unprocessed Images from the CK+ Dataset Method AlexNet LDA SVM Error Rate (%) for Processed Data 10.90 10.74 8.42 Error Rate (%) for Unprocessed Data 12.72 26.42 23.80  KNN AlexNet + FC8 + LDA AlexNet + FC7 + LDA AlexNet + FC6 + LDA AlexNet + FC6 + SVM AlexNet + FC6 + KNN 24.24 7.98 5.22 3.63 6.68 21.63 20.46 11.76 8.56 3.92 8.85 19.74 Figure 9. The Confusion Matrix Using the AlexNet + FC6 LDA for the Datasets of JAFFE, KDEF, CK+, FER2013 and AffectNet are shown in (a), (b), (c), (d) and (e). Finally, we report on the performance relating to the recognition of each kind of facial expression category. Confusion matrices (a), (b), (c), (d) and (e) in Figure 9 show the data relating to the AlexNet + FC6 LDA for the Datasets of JAFFE, KDEF, CK+, FER2013 and AffectNet respectively. Here, it can be observed that the emotional category ‚Äòhappy‚Äô appears to be the easiest emotion to be recognized. On the other hand, some emotional categories like ‚Äòsad‚Äô and ‚Äòdisgust‚Äô seems to be much more difficult to be recognized. We would argue that the sad emotion is currently relatively hard to be recognized for the following two reasons. The first reason is due to the quantity of images depicting the sad emotion within the training dataset. We have observed that in most of the datasets, there are notably fewer images of sad emotions, which consequently increases the recognition difficulty. For example, within the CK+ dataset there are 693 images in total, whilst only 40 images depict sad expressions. Additionally, there are relatively small visual differences between sad emotions and neutral emotions, and consequently sad expressions may be recognized as neutral by the system without using sufficient training data. Furthermore, we notice that in the datasets using the images from the Internet, such as the FER2013 and AffectNet datasets, the number of images for each emotion category is uneven. The predominant emotion type   contained within these two datasets are happy images, which consequently reduces the difficulty of recognition of the happy emotion. However, in the case of the other databases, which contain a more balanced range of emotions, the emotional category with the lowest error rate varies. As the developed facial expression recognition method aims to benefit the mental health care, we further test the proposed method with the datasets that contain the images of facial expressions from the patients with cognitive impairment. We tested the proposed method in the following three datasets: a dataset with facial expressions from the patient with cognitive impairment, a dataset that combines the images from the JAFFE dataset and the images from patients and a dataset that combines the images from the KDEF dataset and the images form patients. We used 70% of images as the training dataset and 30% of images as the testing dataset. Table 14 shows the recognition accuracy using the proposed method in the three datasets. The experimental result shows that the proposed method also has a good performance for the facial expressions from the patients with cognitive impairment. In our examination of the data relating to recognition when employing the first author‚Äôs facial expressions, drawn from a series of continuous video frames, the accuracy rate was approximately 96.0%.We would argue that the system achieved strong performance in recognizing facial expressions taken from videos in this context. It should be noted that one factor that contributed to the high accuracy rate may be that some of the training images and the testing images were selected from a video that had the similar lighting condition and viewpoint. In addition, there was only one participant and one viewpoint. However, the analysis of facial expressions from the videos in our testing appears to be able to evaluate the evolution of the facial expression over a period of time, with changes of the expression emitted, by recognizing facial expressions for each frame in the video. Additionally, the system was also able to quantify the extent of the facial expression. However, there remain some practical issues to be considered. First, it is noticed that the differences in facial expression for happy and neutral, neutral and sad, sad and angry are small. Moreover, for the video of the facial expressions emitted by the first author, the facial expression is currently labelled manually, and there may be potential problems in labelling some frames during a change of expressions. In addition, as stated previously, it was noted that in the JAFFE dataset, some facial expressions appeared to be labelled incorrectly and were removed prior to testing. In the experiment, 202 images of facial expressions were used from the JAFFE, while the original JAFFE dataset has about 213 images. Finally, there are some practical problems for the proposed facial expression recognition system that remain to be addressed in future, including a need for enhanced stability with regard to how the system crops the head area in the images. From the perspective of clinical practice, in order to use the system within elderly people to detect mental health issues like cognitive impairment, the framework needs to be trained with large samples of natural facial expressions taken from elderly people, in order to achieve optimal performance. In recent work, we have collected circa 100, 000 images of facial expressions from elderly people. The recognition of naturally occurring facial expression is more challenging. We should also note that work to complete the processing of these images is ongoing, and we will report in a future work regarding satisfactory verification of these images as a suitable training set for our network. Table 14. The Recognition Accuracy Using the Proposed Algorithm (AlexNet + FC6 + LDA) in the Datasets Containing Facial Expressions from Patient with Cognitive Impairment Dataset Patient with Cognitive Impairment Dataset JAFFE + Patient Dataset KDEF + Patient Dataset Recognition Accuracy (%) 85.13 89.90 89.33 5. Conclusion This paper has presented a novel emotion analysis framework that is able to understand and automatically recognize users‚Äô emotions by analyzing the users‚Äô facial expression from their facial images. The system consists of  three parts: input of the videos of facial expressions, the image pre-processing technique, and automatic facial expression analysis. The system is able to successfully conduct image pre-processing and facial expression analysis. Additionally, after facial expression analysis has been undertaken, it is able to understand the evolution of facial expression over a period of time and can quantify the extent of the emotions detected from a facial video. As facial expressions reflect people‚Äôs mental health state, the proposed framework has great potential to be employed within mental health care. For the facial expression analysis, this paper has also proposed a new solution that extracts the deep features from the FC6 of the AlexNet whilst the standard LDA is exploited to train these deep features. The proposed solution shows promising and has stable performance on all the five tested datasets for the nine methods studied. Additionally, we would argue that the proposed method has relatively good recognition accuracy, much less operating time and lower device requirements compared to the other current state-of-the-art deep learning algorithms. Furthermore, the analysis of facial expressions when taken from the videos demonstrated that our proposed approach is able to report the evolution of facial expression over a period of time, and reliably detect the changes of expression by means of the recognition of facial expression within each frame in the video. Conflicts of Interest: The authors declare no conflict of interest. Acknowledgements: The research work is funded by Strathclyde‚Äôs Strategic Technology Partnership (STP) Programme with CAPITA (2016-2019). We thank Dr Neil Mackin and Miss Angela Anderson for their support. The contents in the paper are those of the authors alone and don‚Äôt stand for the views of CAPITA plc. Huiyu Zhou was partly funded by UK EPSRC under Grant EP/N011074/1, and Royal Society-Newton Advanced Fellowship under Grant NA160342. The authors thank Shanghai Mental Health Center for their help and support. We also thank Dr Fei Gao from Beihang University, China for his kind support and comment. Ethical approval: This article does not contain any studies with human participants or animals performed by any of the authors without the proper ethical approval. References [1] [2] [3] [4] [5] [6] [7] [8] [9] N. Yildirm, A. Varol, A research on estimation of emotion using EEG signals and brain computer interfaces, in: 2nd Int. Conf. Comput. Sci. Eng. UBMK 2017, 2017: pp. 1132‚Äì1136. doi:10.1109/UBMK.2017.8093523. A. Oliveira, C. Pinho, S. Monteiro, A. Marcos, A. Marques, Usability testing of a respiratory interface using computer screen and facial expressions videos, Comput. Biol. Med. 43 (2013) 2205‚Äì2213. doi:10.1016/j.compbiomed.2013.10.010. M.-T. Yang, Y.-J. Cheng, Y.-C. Shih, Facial expression recognition for learning status analysis, 2011. doi:10.1007/978-3-642-21619-0_18. G.S. Bahr, C. Balaban, M. Milanova, H. Choe, Nonverbally smart user interfaces: Postural and facial expression data in human computer interaction, 2007. P. Branco, P. Firth, L.M. Encarna√ß√£o, P. Bonato, Faces of emotion in human-computer interaction, in: Conf. Hum. Factors Comput. Syst. - Proc., 2005: pp. 1236‚Äì1239. doi:10.1145/1056808.1056885. O. Grynszpan, J.-C. Martin, J. Nadel, Using facial expressions depicting emotions in a human-computer interface intended for people with autism, 2005. doi:10.1007/11550617_41. D.K. Jain, P. Shamsolmoali, P. Sehdev, Extended deep neural network for facial emotion recognition, Pattern Recognit. Lett. 120 (2019) 69‚Äì74. doi:10.1016/J.PATREC.2019.01.008. J. Shao, Y. Qian, Three convolutional neural network models for facial expression recognition in the wild, Neurocomputing. 355 (2019) 82‚Äì92. doi:10.1016/j.neucom.2019.05.005. C. Lv, Z. Wu, X. Wang, M. Zhou, 3D facial expression modeling based on facial landmarks in single image, Neurocomputing. 355 (2019) 155‚Äì167. doi:10.1016/J.NEUCOM.2019.04.050. [10] M.Z. Uddin, J.J. Lee, T.-S. Kim, An enhanced independent component-based human facial expression recognition from video, IEEE Trans. Consum. Electron. 55 (2009) 2216‚Äì2224. doi:10.1109/TCE.2009.5373791. J.J. Lee, Z. Uddin, T.-S. Kim, Spatiotemporal human facial expression recognition using fisher independent component analysis and Hidden Markov Model, in: Proc. 30th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS‚Äô08 - "Personalized Healthc. through Technol., 2008: pp. 2546‚Äì2549. D. Engin, C. Ecabert, ‚Ä¶ H.E.-2018 26th E., undefined 2018, Face Frontalization for Cross-Pose Facial Expression [11] [12] Recognition, Ieeexplore.Ieee.Org. (n.d.). https://ieeexplore.ieee.org/abstract/document/8553087/ (accessed August 22, 2019). [13] W. Sun, H. Zhao, Z. Jin, An efficient unconstrained facial expression recognition algorithm based on Stack Binarized Auto-encoders and Binarized Neural Networks, Neurocomputing. 267 (2017) 385‚Äì395. doi:10.1016/J.NEUCOM.2017.06.050. A. Samal, P.A. Iyengar, Automatic recognition and analysis of human faces and facial expressions: a survey, Pattern Recognit. 25 (1992). doi:10.1016/0031-3203(92)90007-6. B. Fasel, J. Luettin, Automatic facial expression analysis: A survey, Pattern Recognit. 36 (2003). doi:10.1016/S0031-3203(02)00052-3. G. Sandbach, S. Zafeiriou, M. Pantic, L. Yin, Static and dynamic 3D facial expression recognition: A comprehensive survey, Image Vis. Comput. 30 (2012). doi:10.1016/j.imavis.2012.06.005. Z. Zeng, M. Pantic, G.I. Roisman, T.S. Huang, A survey of affect recognition methods: Audio, visual, and spontaneous expressions, IEEE Trans. Pattern Anal. Mach. Intell. 31 (2009). doi:10.1109/TPAMI.2008.52. E. Sariyanidi, H. Gunes, A. Cavallaro, Automatic analysis of facial affect: A survey of registration, representation, and recognition, IEEE Trans. Pattern Anal. Mach. Intell. 37 (2015). doi:10.1109/TPAMI.2014.2366127. B. Ko, Ko, B. Chul, A Brief Review of Facial Emotion Recognition Based on Visual Information, Sensors. 18 (2018) 401. doi:10.3390/s18020401. S. Baron-Cohen, A. Riviere, M. Fukushima, D. French, J. Hadwin, P. Cross, C. Bryant, M. Sotillo, Reading the mind in the face: A cross-cultural and developmental study, Vis. Cogn. 3 (1996) 39‚Äì59. K. Burton, A. Kaszniak, Emotional experience and facial expression in Alzheimer‚Äôs disease, Aging, Neuropsychol. Cogn. 13 (2006). doi:10.1080/13825580600735085. S. Passardi, P. Peyk, M. Rufer, T.S.H. Wingenbach, M.C. Pfaltz, Facial mimicry, facial emotion recognition and alexithymia in post-traumatic stress disorder, Behav. Res. Ther. 122 (2019). doi:10.1016/j.brat.2019.103436. J.D. Henry, P.G. Rendell, A. Scicluna, M. Jackson, L.H. Phillips, Emotion Experience, Expression, and Regulation in Alzheimer‚Äôs Disease, Psychol. Aging. 24 (2009). doi:10.1037/a0014001. [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] M.C. Smith, Facial expression in mild dementia of the Alzheimer type, Behav. Neurol. 8 (1995) 149‚Äì156. [25] D. Lundqvist, A. Flykt, A. Ohman, The Karolinska directed emotional faces (KDEF), CD ROM from Dep. Clin. Neurosci. Psychol. Sect. Karolinska Institutet. (1998) 91‚Äì630. doi:10.1017/S0048577299971664. [26] M. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, Coding facial expressions with Gabor wavelets, in: Proc. Third IEEE [27] [28] [29] [30] [31] [32] [33] [34] Int. Conf. Autom. Face Gesture Recognit., IEEE Comput. Soc, n.d.: pp. 200‚Äì205. doi:10.1109/AFGR.1998.670949. T. Kanade, J.F. Cohn, Yingli Tian, Comprehensive database for facial expression analysis, in: Proc. Fourth IEEE Int. Conf. Autom. Face Gesture Recognit. (Cat. No. PR00580), IEEE Comput. Soc, n.d.: pp. 46‚Äì53. doi:10.1109/AFGR.2000.840611. P. Lucey, J.F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews, The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression, in: 2010 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. - Work., IEEE, 2010: pp. 94‚Äì101. doi:10.1109/CVPRW.2010.5543262. Challenges in Representation Learning: Facial Expression Recognition Challenge | Kaggle, (n.d.). https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/overview (accessed April 15, 2019). A. Mollahosseini, B. Hasani, M.H. Mahoor, AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild, IEEE Trans. Affect. Comput. 10 (2019) 18‚Äì31. doi:10.1109/TAFFC.2017.2740923. P. Viola, M. Jones, Rapid object detection using a boosted cascade of simple features, in: Proc. 2001 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognition. CVPR 2001, IEEE Comput. Soc, n.d.: p. I-511-I-518. doi:10.1109/CVPR.2001.990517. Detect objects using the Viola-Jones algorithm - MATLAB - MathWorks United Kingdom, (n.d.). https://uk.mathworks.com/help/vision/ref/vision.cascadeobjectdetector-system-object.html (accessed January 29, 2019). Concatenate arrays - MATLAB cat - MathWorks United Kingdom, (n.d.). https://uk.mathworks.com/help/matlab/ref/cat.html (accessed May 7, 2019). Pretrained AlexNet convolutional neural network - MATLAB alexnet - MathWorks United Kingdom, (n.d.). https://uk.mathworks.com/help/deeplearning/ref/alexnet.html;jsessionid=c4669357f290858d36ed1bcbf8cf (accessed September 21, 2018). [35] M. Matsugu, K. Mori, Y. Mitari, Y. Kaneda, Subject independent facial expression recognition with robust face detection using a convolutional neural network, Neural Networks. 16 (2003) 555‚Äì559. doi:10.1016/S0893-6080(03)00115-1. [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] R. Collobert, J. Weston, A unified architecture for natural language processing, in: Proc. 25th Int. Conf. Mach. Learn. - ICML ‚Äô08, ACM Press, New York, New York, USA, 2008: pp. 160‚Äì167. doi:10.1145/1390156.1390177. A. Krizhevsky, I. Sutskever, G.E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, n.d. http://code.google.com/p/cuda-convnet/ (accessed September 21, 2018). X. Chen, X. Yang, M. Wang, J. Zou, Convolution neural network for automatic facial expression recognition, in: 2017 Int. Conf. Appl. Syst. Innov., IEEE, 2017: pp. 814‚Äì817. doi:10.1109/ICASI.2017.7988558. K. Shan, J. Guo, W. You, D. Lu, R. Bie, Automatic facial expression recognition based on a deep convolutional-neural-network structure, in: 2017 IEEE 15th Int. Conf. Softw. Eng. Res. Manag. Appl., IEEE, 2017: pp. 123‚Äì128. doi:10.1109/SERA.2017.7965717. X. Han, Y. Zhong, L. Cao, L. Zhang, Pre-trained alexnet architecture with pyramid pooling and supervision for high spatial resolution remote sensing image scene classification, Remote Sens. 9 (2017). doi:10.3390/rs9080848. U. Chavan, D. Kulkarni, Optimizing deep convolutional neural network for facial expression recognitions, in: Adv. Intell. Syst. Comput., Springer Verlag, 2019: pp. 185‚Äì196. doi:10.1007/978-981-13-1402-5_14. Feature Extraction Using AlexNet - MATLAB &amp; Simulink - MathWorks United Kingdom, (n.d.). https://uk.mathworks.com/help/deeplearning/examples/feature-extraction-using-alexnet.html;jsessionid=a9dd0dd508fd2b96854cd6f11d5c (accessed January 23, 2019). P. McAllister, H. Zheng, R. Bond, A. Moorhead, Combining deep residual neural network features with supervised machine learning algorithms to classify diverse food image datasets, Comput. Biol. Med. 95 (2018) 217‚Äì233. doi:10.1016/J.COMPBIOMED.2018.02.008. Visualize High-Dimensional Data Using t-SNE - MATLAB &amp; Simulink - MathWorks United Kingdom, (n.d.). https://uk.mathworks.com/help/stats/visualize-high-dimensional-data-using-t-sne.html (accessed April 15, 2019). L. van der Maaten, Barnes-Hut-SNE, (2013). http://arxiv.org/abs/1301.3342 (accessed April 15, 2019). Visualize Activations of a Convolutional Neural Network - MATLAB &amp; Simulink - MathWorks United Kingdom, (n.d.). https://uk.mathworks.com/help/deeplearning/examples/visualize-activations-of-a-convolutional-neural-network.html (accessed January 23, 2019). Q. Ye, N. Ye, T. Yin, Fast orthogonal linear discriminant analysis with application to image classification, Neurocomputing. 158 (2015) 216‚Äì224. doi:10.1016/J.NEUCOM.2015.01.045. N.A.A. Shashoa, N.A. Salem, I.N. Jleta, O. Abusaeeda, Classification depend on linear discriminant analysis using desired outputs, in: 2016 17th Int. Conf. Sci. Tech. Autom. Control Comput. Eng., IEEE, 2016: pp. 328‚Äì332. doi:10.1109/STA.2016.7952041. Compute convolutional neural network layer activations - MATLAB activations - MathWorks United Kingdom, (n.d.). https://uk.mathworks.com/help/deeplearning/ref/activations.html (accessed January 30, 2019). N. Zeng, H. Zhang, B. Song, W. Liu, Y. Li, A.M. Dobaie, Facial expression recognition via learning deep sparse autoencoders, Neurocomputing. 273 (2018) 643‚Äì649. doi:10.1016/j.neucom.2017.08.043. N. Zeng, Z. Wang, H. Zhang, K.-E. Kim, Y. Li, X. Liu, An Improved Particle Filter With a Novel Hybrid Proposal Distribution for Quantitative Analysis of Gold Immunochromatographic Strips, IEEE Trans. Nanotechnol. 18 (2019) 819‚Äì829. doi:10.1109/tnano.2019.2932271. N. Zeng, Z. Wang, H. Zhang, W. Liu, F.E. Alsaadi, Deep Belief Networks for Quantitative Analysis of a Gold Immunochromatographic Strip, Cognit. Comput. 8 (2016) 684‚Äì692. doi:10.1007/s12559-016-9404-x. L. Ding, H. Li, C. Hu, W. Zhang, S. Wang, Alexnet feature extraction and multi-kernel learning for object-oriented classification, in: Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. - ISPRS Arch., International Society for Photogrammetry and Remote Sensing, 2018: pp. 277‚Äì281. doi:10.5194/isprs-archives-XLII-3-277-2018. Y. Li, J. Zeng, S. Shan, X. Chen, Occlusion Aware Facial Expression Recognition Using CNN With Attention Mechanism, IEEE Trans. Image Process. 28 (2019) 2439‚Äì2450. doi:10.1109/TIP.2018.2886767. S. Jyoti, G. Sharma, A. Dhall, A Single Hierarchical Network for Face, Action Unit and Emotion Detection, in: 2018 Digit. Image Comput. Tech. Appl., IEEE, 2018: pp. 1‚Äì8. doi:10.1109/DICTA.2018.8615852. K. Rujirakul, C. So-In, Histogram equalized deep PCA with ELM classification for expressive face recognition, in: 2018 Int. Work. Adv. Image Technol., IEEE, 2018: pp. 1‚Äì4. doi:10.1109/IWAIT.2018.8369725. P. Lucey, J.F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews, The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression, in: 2010 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. - Work. CVPRW 2010, 2010. doi:10.1109/CVPRW.2010.5543262. T. Jabid, M.H. Kabir, O. Chae, Robust facial expression recognition based on local directional pattern, ETRI J. 32 (2010) 784‚Äì794. doi:10.4218/etrij.10.1510.0132. A.T. Lopes, E. de Aguiar, A.F. De Souza, T. Oliveira-Santos, Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order, Pattern Recognit. 61 (2017). doi:10.1016/j.patcog.2016.07.026. G. Fanelli, A. Yao, P.-L. Noel, J. Gall, L. Van Gool, Hough Forest-Based Facial Expression Recognition from Video Sequences, in: Springer, Berlin, Heidelberg, 2012: pp. 195‚Äì206. doi:10.1007/978-3-642-35749-7_15. [60] Zixiang Fei received his Bachelor degree in Liverpool John Moores University and Master Degree in University of York. He is currently a PhD student in University of Strathclyde. His major research interests include computer vision, machine learning, object recognition and deep learning. Erfu Yang is a Lecturer in University of Strathclyde under Strathclyde Chancellor's Fellowship Scheme. In 2008, he received his Ph.D. degree in robotics in the interdisciplinary area of robotics and autonomous Systems from the University of Essex. His main research interests include robotics, autonomous systems, computer vision, image/signal processing, mechatronics, data analytics, etc. David Day-Uei Li received the Ph.D. degree in electrical engineering from the National Taiwan University, Taipei, Taiwan, in 2001. He then joined the Industrial Technology Research Institute, Taiwan, working on CMOS communication chipsets. From 2007 to 2011 he worked at the University of Edinburgh before he took the lectureship in biomedical engineering at the University of Sussex. In 2014 he joined the University of Strathclyde, Glasgow, as a Senior Lecturer. His research interests include mixed signal circuits, CMOS sensors and systems, embedded systems, FLIM systems & analysis, and optical communications. Stephen Butler received an MA and PhD at the University of Glasgow. He is the director of the Strathclyde Oculomotor Lab. His research interests, employing eye tracking technology lie in the cognitive neuroscience of attention and the neuropsychology of eye movements. He has applied his research skills to theoretical work in areas including face processing, addiction, biases in attention and brain injury, and has a broad range of experience in applying his research skills in applied fields from shipping to interface design.            Winifred Ijomah is a Reader in University of Strathclyde. She gained a PhD in Remanufacturing from the University of Plymouth in 2002. Her research focuses on sustainable design and manufacturing, and to date has focused on product end-of-life, particularly on remanufacturing. Xia Li is a psychiatry specialist for older adults in Shanghai Mental Health Center, Shanghai Jiaotong University, school of medicine. She has been engaged in psychogeriatric clinical practice and research for 16 years. Her interest mainly includes Dementia, behavioral problems and Depression in the elderly. Huiyu Zhou received a PhD in Computer Vision from Heriot-Watt University. He is a Reader in University of Leicester. He currently heads the Applied Algorithm and AI (AAAI) Theme and is leading the Biomedical Image Processing Lab at University of Leicester. His research interests includes machine learning, computer vision and artificial intelligence.        