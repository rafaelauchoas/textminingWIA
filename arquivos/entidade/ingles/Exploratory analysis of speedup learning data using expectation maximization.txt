ELSEVIER Artificial Intelligence 85 (1996) 301-319 Artificial Intelligence Exploratory analysis of speedup expectation maximization learning data using Albert0 Maria Segrea**, Geoffrey J. Gordonb .l, Charles P. Elkan’.’ “Department of Management Sciences, University of Iowa, Iowa City, IA 52242, USA bSchool of Computer Science, Carnegie-Mellon University, Pittsburgh, PA 15213. USA ‘Department of Computer Science and Engineering. University of California at San Diego, La Jolla. CA 15213, USA Received January 199.5; revised September 1995 Abstract Experimental evaluations have or not learning performance to data analysis. of speedup to determine whether learning methods into data testing hypothesis to estimate a complementary to obtain deeper insight parametric is used of a problem the comparative approach parametric here how through perimental performance methods, we propose may accelerate leaving unchanged. We show how to apply expectation maximization to fit this kind of multi-component a methodological censored learning. linear model the solution of some problems while solver. To model problem a two-component values data, for the parameters in the past used non- is beneficial. We show of learning methods In solvers this approach ex- of a statistical model of the learning that use speedup that captures how learned knowledge relatively technique, the solution of others a statistical (EM), model. EM allows us to fit the model difficulty to experiments common in the presence involving speedup of 1. Introduction Speedup learning methods, such as subgoal caching [17] or explanation-based intended [14], are generally learning bounded problem-solving mean operating more quickly at a fixed level of competence. Unfortunately, determining whether the performance of a resource- is usually defined to the extent of any performance system. Performance is any improvement at all-is difficult. indeed, detecting improvement-or, improvement to improve there * Corresponding ’ E-mail: ggordon@cs.cmu.edu. ’ E-mail: elkan@cs.ucsd.edu. author. E-mail: segre@cs.uiowa.edu. 0004-3702/96/$15.00 0 1996 Elsevier Science B.V. All rights reserved SSDI 0004-3702(95)00115-8 Since conclusive formal arguments about the performance improvement due to are difficult collected learning method a speedup the only realistic means of detecting in Data these testing. where hypothesis performance methodological experimental ties, non-parametric system performance. studies of speedup with or without the presence methods learning. choices can compromise of censored data,’ are used to construct, experimental studies provide or quantifying performance studies are the null hypothesis typically analyzed using is that there [15] we show how several In the reliability of conclusions improvement some form is no difference [7]. of in learning. One of these methodological in learning addressed that is subsequently the hypothesis to test drawn common from difficul- [6], where improves Hypothesis testing system. provides It simply provides of whether to the question problem population. can be uninteresting learning certainty, sampled differences little into insight an answer. with some degree or not There are or even misleading statistically from a practical learning times where improves of statistical performance on a significant standpoint: the qualitative behavior of a . careful “hiding” even when intuitive a statistical examination result of important characteristics is obtained, the data, of the data it does not substitute test that the checking [6]. for a is not paper presents a rigorous approach to modeling system performance This intended traditional This approach by positing for Exploratory method the model. to expose The contributions this kind of “hiding”. of this paper are three-fold. hypothesis testing with a complementary, data analysis to exploratory a model and using a statistical a deeper understanding analysis of performing of the type advocated the kind of “intuitive is a new mathematical The second contribution the type of exploratory analysis in previous work used a simple support have performance to quantify 171. Here, we propose better captures knowledge may affect some problems more the benefits of certain a more sophisticated of speedup effects linear the First, we show how to augment approach. more exploratory, is parametric technique of system here examination” to estimate operation is a quantitative, in nature: we show how, parameter values can be achieved. reproducible above. learning to just described. We mentioned model of speedup fitting and parameter regression model of problem-solving types of speedup two-component learning, in particular, than others. learning linear model how [13,16, that learned The third contribution is a statistical technique to estimate the two-component model in the presence of censored data. This the parameters technique of is ’ A censored measurement value if we wait itself. For example. up. we have a censored measurement: hours. nontrivial any credible search problems. empirical but we do not know how much more. Resource Thus the censored test of a problem-solving data problem system. is one where we observe a bound on the measured value rather three hours we know for the problem the actual that solver time to solve a problem, is more to solution bounds cannot is fundamental be avoided when and must be addressed than than the then give three solving in A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 303 based on expectation maximization likelihood estimation a special case. We show how to use EM to perform multiple-component regression for maximum in the presence of missing data, of which censored data are linear in the presence of censored data.4 a general method (EM), learning reconstructed a sample dataset In the next section, we introduce in the presence of censored data, and we discuss how information literature. This dataset serves as an example of the paper. We review a non-parametric method the from throughout the machine for hypothesis remainder still testing in the dataset is not revealed by this test. In Section 3, we introduce EM available and show how it can be applied of speedup learning systems in the presence of censored data. In Section 4, we show how EM can fit a simple system from the dataset of Section 2 as an example. We then introduce a performance learning and show how EM can be used to fit two-component model of speedup this new model (again in the presence of censored data), and illustrate the process using learning system performance from censored data, using non-learning from the sample dataset. the performance to investigate linear model 2. An illustrative example The example used throughout in the speedup [ll]. Reports of experiments literature learning experiment times explore here is whether or not an explanation-based combined with a standard backward-chaining performance this paper revisits the classic logic theorist (LT) in this domain have appeared several [10,12,14]. The primary question we learning component, when system, provides a problem-solving improvement The set of LT problems in this domain. is taken from Chapter 2 of Principia Mathematics [19]. to the original 92 problems of Chapter 2, The 87 problems [lo] (a full printed reformulated version of the domain theory and problem set used in this paper can be found in ]141). in the set correspond for use with definite-clause theorem provers The backward-chaining problem solver used is a definite-clause theorem prover in Common Lisp. This is the same theorem prover used in our implemented learning [14]. The previous work on subgoal caching [17] and explanation-based theorem prover unit-increment depth- first iterative deepening. The data described here were collected on a 32MB 90MHz Pentium system running Gnu Common Lisp and the Linux operating system. to perform resource-bounded is configured A resource attempted. along with an annotation whether limit of 5 x lo4 node explorations was imposed on each problem In each trial CPU times and node exploration counts were recorded, indicating whether or not the problem was solved (i.e., the problem was “censored”). ’ After submitting less efficient EM algorithm for this problem [8]. the first version of this paper, we discovered that others have previously described a ?iOJ A.M. Segre et trl. 1 Artificial Intelligence 85 (lYY6) 301-319 In the first trial, bound. In the 34 solved the theorem the second in the first trial and used prover trial, 4 problems were solved 34 of the 87 problems within randomly selected to generate macro-operators prover was tested then on remaining system solved 36 of the the from with the 83 algorithm 83 problems. tested within [14]. The The learning theorem the resource bound. resource among the EBL*DI remaining problems 2.1. Scatter plot inspection The simplest method of analyzing the data collected in trials 1 and 2 is to make a scatter plot of elapsed CPU time for the learning system. Fig. 1 is such a plot, where a logarithmic both axes for clarity. Each datapoint solution without is plotted solution (above) learning. An after the J’ = x line correspond on the vertical on to problems of Fig. 1 seems is plotted represents informal learning learning analysis to system versus transform the horizontal a single problem. CPU axis, while CPU axis. Datapoints that are solved the non-learning has been applied time time to to to falling below (slower) after faster advantageous. system. faster after problem difficult is unchanged). into to factor The In addition, learning system solves 6 more problems solved by both of the 30 problems learning, while only 12 are solved more slowly time indicate that learning is the non-learning indeed than systems. (the censored 17 are solved to solve one are problems Unfortunately. the 47 doubly this kind of informal analysis. A comparison of summary 1OQO~ Learning , , ,, system CPU time vs. non-learning . . ,( . . ., . . ., kzming (seconds) Solved o Censored + Doubly censored o \‘Z,y - 100 IO system CPU time. . ‘, . . ‘/9 0.001 E/(:.,..., 0.001 0.01 0.1 1 10 Non-Learning (seconds) 100 1000 I. Plot of learning shown Fig. datapoints systems. while system. The 47 “square” both learning the system CPU time against the 30 problems correspond non-learning solved both by system CPU the learning to datapoints the six “cross” correspond to those problems solved only by the datapoints and non-learning correspond system exhausted to doubly censored that the 5 x 10J node exploration problems, time. The “diamond” and non-learning learning is. problems where resource bound. A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 305 statistics (e.g., systems) misleading [ 151. total CPU time used by each system on problems solved by both indeed potentially but similarly confusing, is less subjective, and 2.2. Hypothesis testing to account the solution tests to accept can cause these the non-learning in [6] relies on the null hypothesis as Etzioni and Etzioni The analysis advocated system and the solution for censored data. These two non-parametric methods, the one-tailed paired sign test [2] and the one-tailed paired Wilcoxon signed-ranks test [20], suitably extended tests are non- to the more commonly used parametric Student t-test. They parametric analogues test for statistically the significant differences between times for system.’ Unfor- times for learning tunately, because of the maximally conservative way in which these tests handle they are not powerful when the dataset contains many censored censored data, [6] point out, sufficiently many censored observations: regardless of the datapoints strength of the evidence from the uncensored datapoints. For example, when we apply these tests to the data of Fig. 1, in which more than half of the observations are censored, we can reach no useful conclusions.6 If the censored data we test the null hypothesis test strongly fails to reject it (if we reject for small extension of the signed-ranks values of p, then (1 -p) G 10e6). On the other hand, if we test the hypothesis that ((1 -p) 4 lo-“). These two results mean that, according ranks test, at any reasonable system is indistinguishable a few more datapoints might allow us to reach were less extreme, such prospects conclusion, differences we can easily see, and despite differences described The situation it to the extended signed- of the learning system. If the p values some are dim. Thus, despite revealed by the method test cannot detect a difference. in Section 4, the extended signed-ranks is faster, we fail even more strongly that the learning system is faster, from that of the non-learning for the extended sign test. level, the performance is similar, although the non-learning these p values, less extreme, significance but with to reject system is inappropriate here since it requires the underlying solver to be normal. This assumption distribution is unwarranted, of the measured solution since the censoring will to cluster around assumed the resource in this paper limit. is more general than form of censorship every censored variant In this case, all doubly censored the observed exactly arises naturally when a constant points values of the censored datapoint displays the restrictive consumption. form used in [6] The latter, more resource the same resource limit is imposed directly on the parameter fall exactly on the y = x line, and all singly censored the true and coordinate. Because observation support of our more general is treated for these other extensions greater tests, but each of setting, we extend coordinate larger the at its censoring as if it lies either the null hypothesis. We also considered results in a test that can than the value of tests of [6] in the point or at +m, other ways of a false support t-test for each problem ’ The times cause datapoints ’ The where restrictive, of interest. points have both the uncensored natural way: a censored whichever extending conclusion. provides the 3Uh A.M. Segrr er ul. j Artificial Intelligence 85 (1996) 301-31Y 3. Modeling problem-solving performance combines Our approach learning performance to evaluating speedup nature of scatter plot the more inspection with a rigorous mathematical informative is to first posit a model of system foundation. Briefly put, our approach performance, called expectation muxi- and then mization (EM) to estimate values for the parameters of this model. Using the EM technique allows us to estimate parameter values even though some performance data is censored. We can then examine the fitted model in order to identify trends that cannot be directed directly in the raw data (either because of its size or because of censoring), and that cannot be detected by hypothesis to use a statistical technique testing. studied the names “life testing” and “reliability EM has been used to address problems in statistics and operations testing”. The first name research under arises from medical studies in which the object is to estimate the average lifetime of a group of patients when some of the patients are still alive. The second name arises from quality control experiments the mean time to failure for a sample of parts when not all the parts have failed yet. Recently, EM has also shown promise learning tasks such as the [3]. As might be expected discovering patterns tests of [6], can deal with censored given its heritage, EM, like the non-parametric technique. That is, it data. But unlike these weaker methods, EM is a parametric begins from a prespecified model with a prespecified and adjusts these parameters in DNA and protein sequences finite number of parameters, in which the object to fit the data.’ in unsupervised is to estimate 3.1. Using EM to model performance Assume we are given a problem-solving system and wish to evaluate performance with respect to some set standard. We gather data by presenting system with n problems of calibrated measuring Since we generally cannot afford input (as it might take years or even centuries system off before its the later) and (more on (e.g.. elapsed CPU time) on each problem. to let the system run to completion on every to finish), we sometimes cut the it finishes, yielding censored data. its resource consumption *‘difficulty” this thus observations x,,], and Y = [y,, , S,], X = Our the “difficulty” of the ith Lx I”‘., problem. x, is the resource amount consumed on the ith problem, and yi is 0 if the system actually solves the ith input and 1 if the system is cut off before solving if y = 1. the problem. Note that x, is only a lower bound on resource consumption three . _v,,], where 8, measures comprise . J = [a,, . n-vectors, 7 EM could also be used are model assumptions of a hypothesis the focus here to similar (such as deviations test, even while is on the qualitative for parametric required from leaving those hypothesis test to linearity testing. a hypothesis. or normality) However. can seriously since the computations required to fit a violations of parametric affect the significance of a fitted model unchanged. level Thus the qualitative appearance aspects of the analysis. could approximate E&(Z) A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 307 We assume that the observed vectors X and Y are obtained from a “true data vector” Z which we cannot directly observe. Each zi E Z is the resource amount that the the system would have consumed on the ith input if we had ignored resource limit and let it run until it eventually halted. We also assume that the from elements of (A, Z) are some known density with parameters 0 (this is our parametric assumption). We rather are attempting than the parameters of a known distribution, trying to proceed without any information about (A, Z) whatsoever. In principle, for (A, Z), but for this we could assume an arbitrarily complicated distribution paper, we use the linear models described later. Our goal is to obtain a good estimate of 0, since 0 characterizes the relationship between each Si and zi. identically distributed observations independent, to estimate Suppose that, instead of the censored observations X and the censoring flags Y, the true data Z. Then it would be relatively easy to estimate 0 [4]. In fact, we would such as maximum the full true data Z; it would be enough to have sujjicient statistics the data, where which statistics are sufficient depends on the dis- we could observe using a technique not need describing tribution of the (Si, zi). Let this vector of sufficient statistics be denoted T’(Z). likelihood estimation If we knew T,(Z), we could estimate 0. On the other hand, if we knew 0, we values statistics T’(Z) with their expected the sufficient ( 0). This apparent dilemma is the basis of the EM algorithm. likelihood to compute the observed data and statistics TA(Z) based on ?,, = E(T,(Z) 1 C$, X, Y), an initial estimate of We proceed as follows. We begin with an initial estimate &, of 0. First, we use the the guess at the the estimate of 0: we set &r to be the true sufficient improving the estimate is called the expectation muximization algorithm, computing this estimate sufficient parameters. Next, we use f0 to update maximum statistics. We repeat this process over and over, alternately of 0 or of TA(Z). This process it alternates between because maximum likelihood estimate detail in [5]. It is guaranteed, under certain general conditions, maximum (the E step) and a in is described to the likelihood estimate of 0 based on the observables A, X and Y. (the M step).’ The EM algorithm estimate of 0 assuming an expectation to converge f0 are that the 3.2. Measuring problem dijjiculty Our experiment explores the relationship between two variables, A and the true resource consumption Z. Above, we informally explained Sj as the “difficulty” of the ith problem. in more detail what A is, why we need to know it, and how we measure In this section, we describe it for the LT experiment. the machinery of EM, our experiment variable Beneath dependent variable for A is that it be a good predictor of Z. In other words, a problem with low Si should consume fewer resources on average is the is A. So, the basic requirement is a regression analysis. The consumption Z and the true resource independent 8 This is not the most general works whenever the logarithm form of the EM algorithm, of the probability density but it is sufficient function for our purposes. for the true data is linear This form in T,(Z). 30x A.M. Se,gre et al. I) Artificial Intellipwce X5 (1996) 301-319 a problem with a high 6,. It is because requirement of this of the ith problem.’ A second called S, the “difficulty” it should be easily and accurately measurable. In addition, the performance of two different problem solvers. that we have for later we requirement since it is essential than informally A is that want to compare that measurements from a planning is likely this paper, we adopt processor requirements These are drawn problems to a problem solution alternative attractive provide a benchmark usage of the control constitutes A. To avoid censoring system with a high resource multiple experiments For single prover disabled. “Cypress” Of the 87 problems control control learning a slight bias control resource censored only a small effect on the regression version in Data were system with a resource system’s limit for datapoints [18] with collected the experimental far below into our described resource of A be independent several suggest of the problem solvers we are testing. domain, possibilities for 3. For example, of steps consumption. if the in the shortest Another the number its resource to predict for defining A is to use a separate of performance. system Here. a measurable required time it is necessary of these values, the CPU (e.g.. limit. The high cost incurred control problem solver to aspect of the resource the problem) to solve to run the control over can be amortized that use the same problem approach. latter the set. The control of our WAM-based parallel caching a dedicated subgoal using limit of 5 X 10h node explorations and first-order intelligent is a system used logic theorem backtracking 128MB Sun Spare 670MP per problem. in the LT problem the 41 problems system system (Neither system solved any of these problems.) While limit. We exclude below. set. 46 problems were solved within not solved by nor does introduce the learning this omission the the the non- resource from the analyses results, we believe limit was that this bias is negligible: two orders systems, the regression all line, and of magnitude these problems ignoring coefficients. larger since than correspond the the to such points has 4. Using EM to analyze the LT data system on the LT domain with the same system augmented We are now ready to show how to analyze problem-solving using EM. Specifically, we wish to compare problem-solving explanation-based performance compute component an algorithm of parameters the non-learning of the model for the performance to fit this more complex model, component. We linear model learning system, from censored the behavior performance of a backward-chaining data and first posit a linear model for show how EM can be used by an the to data. Next, we posit a two- system and provide learning of censored in the presence of the again experiment. ” For a different exploring might be the quantity response. while a low 8, might predict hetween to the relationship another given the namr quantity ith patient. for 8, might be more appropriate. treatment of a drug and In that case, a high 6, might predict administered effectiveness. a strong patient 8, For example, if we were a weaker response. A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 309 data. Finally, we compare some qualitative conclusions based on these analyses. the performance of the two systems in order to draw 4.1. A linear model Consider relationship the non-learning between A and Z such system data from trial 1. Let us assume there is a that each zi - at5, - b is normally linear distributed with mean 0.” This is the standard linear regression model, and, if it were not for the censored data, we would not need EM. In fact, we could still do regular regression the benefit of EM is that if allows us to keep the censored points in our sample without biasing the line. If we threw out these points, we would be wasting potentially regression thereby at best losing some statistical power, and at worst valuable reaching information, incorrect conclusions. if we threw out the censored datapoints: a datapoint how to use EM to fit the linear model the effect of censored data intuitively. (6, z) can be seen as “attracting” to Before explaining mathematically In censored data, it is useful to understand the regression, an ordinary line towards itself. A point above the line pulls the line upwards, and a regression x is a point below the line pulls it downwards. A censored datapoint it lower bound for the true z value-appearing in the same pulls the line upwards at least as much as an uncensored datapoint is at least as apparent position, since the true position of the censored datapoint high as its apparent position. In contrast, a censored datapoint below the line does not pull the line downwards, since the true datapoint may actually lie on the line or even above it. In fact, a censored datapoint below the line pushes the line upwards, although perhaps only slightly, since a higher line makes it more likely for this datapoint above the line behaves similarly: to be censored. (6, x)-where We do not have to worry about the effects of doubly censored datapoints: this is the performance of one system directly with the because we do not try to compare in our scatter plot comparison of Fig. 1). Instead, other (as we did, for example, standard, S,, and assume that Si is available for each i. we rely on an independent system, which Thus problems solved by neither appear as doubly censored points in a direct comparison, into two singly censored points (one in the learning plot and one in the non-learning plot) the learning nor the non-learning in the indirect comparison.” are transformed below. Without than to measurement the variances subject to achieve (approximately) “I In order as we do in the experiments line might be much smaller that A is not measurement ‘I Of course, we must also deal with solver about experiment, system within to obtain A and how well is a convenient the specified the control the control control system error two the test problem this distribution, we may have to take this transform, the variances at the other end. Also, the log of A, Z, or both, at one end of the regression assumes implicitly lack of regression, the this model error. in standard Just fiction which does not seriously as linear influence the results. the corresponding disadvantage. If we use a control system fails solvers to solve a problem, perform on that problem. then we cannot use problem information Fortunately, learning in or non-learning the LT solves every problem solved by either the system resource limit. 310 z4.M. Sep et ui. /) Arfificiul Intelligence 85 ( 1996) Nl-31 Y 4.1.1. Fitting u linear model to censored data with EM When using EM to fit the linear model just described, the M step involves likelihood estimates for the values of the model parameters a, finding maximum b, and u. Formally, the model is a probability density function: (1) We can find the maximum the result In f and setting likelihood to 0. This process gives estimates for a. b, and (T by differentiating the well-known estimates [4]: (2) These estimates can be expressed in terms of the sufficient statistics after multiplying out the expression for 6 to obtain elements of T,(Z). For the E step, we must find the expected value of T,(Z) in terms of ci, 6, and a. Since and E(z’) for each i. The trick is to do this in the (6) it is sufficient presence to find E(z,) data. of censored then If y, = 0, z, =x,. so E(z,) =x, and E(z~) =x’. However situation the density of zz is is more difficult. Assuming for the moment if y, = 1, the that 2, + 6 = 0 and 6 = 1. Let the probability that z, > z be G(z) = I^ c$(t) dt Conditioning on the fact that I, 3 x, gives the density for z, given z, > x,, (7) A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 fCzi I *i ‘xi) 4(z) = @(xi) ’ The required statistics are then m 4(t) - t @(‘iI dt E(z;)zi>x,)= i XI _ Wi) @(Xi> ’ since J t+(t) dt = -4(t) + C, and qz; Izi>xi)= - XI @(x, 1 I Iy t? 44) dt =&+-b(t)+j#Wf) II3 =&j txi4Cxi) + @(‘ill I Shifting and scaling to handle arbitrary b, 6, and & gives E(Z, I Zi >xi, Eli, ~_) = ~; + ~ kr> xj - Pi @WY and where pi = L%, + d. 4.1.2. Application to the LT non-learning data 311 (9) (W (11) (12) (13) (14) (15) (16) (17) Fig. 2 plots non-learning CPU time versus 8, for the 46 problems for which 8, is to both axes. The line shown in linear regression fit found by EM using the 34 solved and 12 transform applied available, with a logarithmic Fig. 2 is the censored censored problems. 311 A.M. Segre 6’1 al. I Artijiciul Intelligence 85 (1996) 301-319 Non-learning loooo~. I.(. system CPU time vs. control system nodes searched. “J ,,I’ ..I. I.,. ..,. NowLaming (seconds) Solved o 1~) _ Censored + I 10 IO0 lwo loo00 Control (nodes seurched) lOOW.xI 10oW00 Fig. 2. Plot of non-learning datapoints system, while system. The correspond line the 12 “cross” system CPU to the 34 problems time versus control solved both by the control system nodes searched. The “diamond” system and by the non-learning datapoints currespond is the result of using EM to perform tu censored censored problems linear regression. solved only by the control While regression obtained censored incorrect, wards. it is possible on only by EM problems. A regular to obtain the 34 problems exploits the smaller, slope because additional a substantially similar fit using a simple for which both 8, and Z, are available, from linear the fit 12 available line on all 46 problems would have an it down- would pull information datapoints the censored the regression 3.2. A multi-component model In order the learning learning we introduce performance. is a combination model to complete the analysis of the LT experiment, data. We could perform the same analysis systems suggests a different model is more appropriate. a model that The premise is a mixture is that of two submodels the behavior of some speedup we must now analyze again, but experience with In this section, system systems [l] for learning learning of two behavioral modes. We show how EM can be used to these behavioral modes separately in the presence of censored data. 4.21. Justifying u two-component model of speedup leurning generally operate by perturbing system. The exact nature of learned in the case of subgoal from previous caching, learning. problem-solving and Typically learned in the case of explanation-based learning algorithms by a problem-solving on what has been Speedup explored depends e.g.. cache contents heuristics are unaffected “helped” by the learned information. learned previously. while other Speedup by what has been problems learning performance is space the search the perturbation experience, certain rules or search problems are mostly A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 313 thus a good candidate subset of problems remaining problems More precisely, is changed by learned knowledge, is largely unchanged.‘* assume for a two-component model, where performance and performance on a on the that, when augmented with a speedup learning the system displays two distinct linear relationships between difficulty mechanism, A and resource consumption 2: with probability 1 - A, a given problem is mostly unaffected by learning and the point (Si, zi) lies approximately on one line, while with probability A, learned knowledge contributes to the solution of the problem lower, line. The number A is called the and the point (ai, zi) lies along a different, in trial 2 of the LT mixing parameter. Given performance data like that collected experiment, to one of the two subpopulations, as well as simultaneously characterize both model relationships expectation maximization can estimate A and assign each datapoint in the presence of censored data. The next natural step after the above two-component linear model is a model with three or more components. The techniques described below can be extended to analyze to multiple-component models, but we need only two components the LT data. censored 4.2.2. Fitting a two-component linear model to censored data with EM The two-component linear model just described linear model of Section 4.1, given is the natural extension of the simple two subpopulations. Here we show how to estimate the parameters of such a model in the presence of censored data using EM. Others have described an algorithm to fit this model [8]. The algorithm proposed based on two nested EM iterations here is more efficient because it needs only one level of iteration. the hypothesis there are that In the simple censored regression case, the unobserved vector of true resource resource consumptions X and censoring consumptions Z gives rise to observed there are more un- flags Y. In the more complicated in addition to the true resource consumptions Z, we introduce observed variables: each problem data V telling which population an IZ X 2 matrix of unobserved to i belongs belongs population the observed variables X and Y using EM. The estimates for V and Z allow us to infer values for the mixing parameter and the slope and intercept of each population. j, and 0 otherwise.13 As before, we estimate V and Z from to. The element uij of V is defined two-component model, to be 1 if observation of learned not directly helped by learning. This utility problem knowledge may adversely affect the performance of the problem is often associated with of analysis [9]. The method solver on the use of advanced a particular experiment, as explained in techniques affects strongly speedup as well as other how learning the utility problem I2 The addition those problems EBL algorithms here can clarify Section 4.3. I3 In some situations we may have outside knowledge prior distribution macro-operator (e.g., reliable in a solution. However, backtracking failure caching (e.g.. using and intelligent learned for the ur,. For example, is employed knowledge is a necessary it may be possible about u,,. We can encode to determine such outside information such knowledge in a by inspection if any learned is not always available generally leave no trace but not sufficient in the solution generated) condition for speedup). or 314 A.M. Segre rr ul. : Arrrficial Intelligence X.5 (1996) 301-319 To derive unobserved the M step of the EM algorithm. we need data. For convenience. we give the logarithm the density function of this density: for the -ln(f(Z V I ho. k13 n,,, ul. A)) + u,,, Mfl,,) + u,, Ma, 1) + (’ (18) where P,~~ = a,$, + b,, and CL,, = n,6, + h, and C is a constant. To reduce priori lines.” There the two likelihood The resulting namely the variances the number ai are five remaining of parameters and ai of the parameters: to estimate, we assume about two populations that we know a their regression for each of their maximum the slope and A. As before, we find the result In f and setting intercept to zero. is exactly what one might expect. equal lines and estimates the mixing parameter by differentiating estimate for the mixing parameter that belong of datapoints the fraction to the first line: The estimates for the slope and intercept of the first line are ci,, = I c UA, c u,o=, c u,&, - c d,,, ’ c u,,& ; - cc’ > ud, 2 ’ c u,,, c U,G, -i c ud, i r;,, = ’ c u,,, / . (19 (20) (21) These estimates (3)), include replaced except only by C, u,,,. that every those points are similar to the estimates term has an additional for the single-line case (Eqs. factor of u,,) so that that belong to the first line. In particular, (2) and the sums n = C, 1 is to estimate ” Trying these variances models where one line latches onto (almost) variances of variances. If the true variances line. both zero. for each adds many just a few points. are unknown, local maxima fits them as is usually to the EM search exactly, (almost) the case. we recommend the space. These are and thus has variance several trying the fit is to the choice to find the best fit and to determine how sensitive A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 315 The estimates for the slope and intercept of the second line are analogous to the estimates for the first line. We can compute these estimates from the sufficient statistics15 (22) For These formulas constitute the M step for the EM algorithm. the E step, we must find the expected value of each of the sufficient statistics given the current estimates of a,, a,, b,, b,, and A. Since uiO and uil may only take the values 0 and 1, we can compute these expected values as for the single-line case. For example, E(ujozi) = E(uio)E(zj 1 uio = l), and we can calculate E(zi ) uio = 1) using Eq. (16) with pi = pie. The only remaining calculation and the normal density function: is E(uio), which we can derive from Bayes’ rule WiO) = w. yw. 3 11 10 E(U,l) = w, y,. IO ) 11 where in the uncensored case, and (23) (24) (25) (27) in the censored case. rather In practice, than computing directly, we two weighted regressions, one to find ho and b, and one to find ci, and perform 6,. For the first regression, we use weights E(uio) and treat censored points as if they all came from the first line, while for the second we use weights E(uil) and treat censored points as if they all came from the second line. the slopes and intercepts I5 We have defined only need one statistic per parameter, the statistics nine of these nine statistics to make in order to estimate five parameters. Many common distributions but the mixture distribution is not so well behaved. We need all log likelihood linear in T,(Z, V). 316 A.M. Segre et ul. I Artificul Intelligence 85 (1996) .%I-319 Learning system CPU time vs. control system nodes searched. Learning (seconds) . 1~ _ Censored + I IO 100 1000 1oOtXl IOOOW 1ooWoO Control (nodes searched) Fig. 3. Plot of learning system CPU time versus control system nodes searched. The 34 “diamond” to the 34 problems solved hy the control system and the learning system, while datapoints correspond to censored problems solved only by the control system. The the eight “cross” datapoints correspond two solid lines are the result of using EM to fit the two-component linear model, and the dotted line is the censored linear regression from Fig. 7 (included for comparison). 4.2.3. Application Fig. 3 shows to t/w LT lrurrhg the 34 LT problems dutn solved by resource system. The remaining limit and learning mechanism. As for a control system node two solid linear model lines shown using fixed variances The dotted line is the censored the learning the eight censored system within problems the solved 46 - 34 - 8 = 4 problems were used as the 46 the non-learning is available are exploration of fitting the in Fig. 3 are two the from Fig. 2, result of 1.0 and 0.5 for regression system, linear count only the slope of a line represents the relationship discovered between consumption for some population slope corresponds to a faster the dotted the performance (slower) line of the line of Fig. 3 and we conclude of problems that corresponding of the non-learning system on intuition the original that is, that on a portion of the problems speedup to the upper the entire our is applied largely unchanged. underlying learning system. Since imported solid set of problems. learning line is similar system on of datapoints, where a the slopes of from Fig. 2 are the to the is This (see leaving fashion. line, we conclude selectively, In a similar model two-component lower solid line was line has a smaller of problems slope corresponding than the dotted the to helped by learning. for and resource for comparison. the the for which The 5 x 10J node exploration only by the control input problems considered. two-component components. included Recall *‘difficulty” smaller the upper comparable, subpopulation performance consistent with Section 4.2.1), performance since that noticeably the lower solid the subpopulation (larger) solid A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 317 4.3. The benefits of EBL*DI What does to be useful, the performance since datapoints that are solved noticeably the overall analysis say about in the LT domain? First, the macro-operators lying on the of the EBL*DI produced by EBL*DI are algorithm to line correspond lower shown faster after learning. The estimated mixing problems how many problems were helped by parameter A indicates that EBL*DI: the two lines, i.e., that EM is not is the source of the difference between EBL*DI the 27 problems whose solutions finding a spurious distinction, we can examine contain a learned macro-operator. These problems contain all 13 of the points EM assigns to the lower line, showing that the use of a learned rule is a necessary (although not sufficient) condition it is 0.324, or about 3 of every 8 problems. To confirm for being helped by learning.16 approximately here Second, line imported itself as an increase in the slope of the upper solid line with respect there is little evidence here of the utility problem. This problem would to manifest the dotted that, even though one from Fig. 2. This would indicate subpopulation of problems might be helped by learning (the lower solid line), the other subpopulation was hindered by learning. Here, since the two lines have is not an issue. These comparable results [14], where observations and another EBL we compared algorithm drawn from the machine slopes, we can conclude that the utility problem are consistent with our own previously reported algorithm the performance of the EBL*DI literature. learning 5. Discussion This paper has shown how model fitting is valuable in the analysis of speedup testing to provide a deeper provide information fitting-when of experimental like how often learning proposed results. More specifically, model to the multi-component model of speedup learning data. Model fitting goes beyond hypothesis understanding in this applied solves paper-can problems faster (the mixing parameter A), the magnitude of the typical speedup (the ratio of the slope of the lower line in the learning analysis to the slope of the line problems slope of information as the non-learning (the ratio of the slope of the upper line in the learning analysis to the finer-grained the than the typical “it is not the case that the learning system is the same testing. We have illustrated how to fit our multi-component model of speedup analysis), and the effect of learning on “unhelped” system” conclusion provided by hypothesis learning from experiments with a particular EBL algorithm on a to real data obtained in the non-learning in the non-learning analysis). This is much learning system line the are the 13 points whose estimated probability of belonging than “These one half. Note that 13/42 # h^ = 0.324. There is no contradiction here, since the estimate of A is formed from the raw (not the thresholded) probabilities. to the lower line is greater 318 A.M. Segr:rr et al. I Artificial Intelligence 85 (IYY6) 301-31Y that data, literature. performance the prior evidence in our own previous work [14], we claimed In order to fit this model, problem set taken from the machine-learning we have applied the statistical technique of expectation maximization (EM), both to handle censored data and to provide a probabilistic partitioning of datapoints the two submodels. Whenever one suspects that experimental data arise between from distinct subpopulations, it is valuable to eliminate human bias in identifying by using EM to fit a multi-component model. In the case of the subpopulations problem-solving that multiple subpopula- tions exist may be relatively shallow (e.g., based on visual inspection of the data) produced or relatively deep (e.g., based on the selective use of macro-operators by learning). any exploratory data analysis tool is justified only if it is useful in Ultimately, practice. Recall that macro- operators produced by EBL*DI were both more effective and less likely to cause produced by a given other EBL the utility problem (as well as several others). These algorithm when operating conclusions were based on relatively coarse-grained comparisons of summary trials much like the ones described here, statistics collected and were only credible because censoring was never an issue (the EBL*DI learning system solved every problem solved by both control system and the other EBL system within the allotted resource bound). Had this not been the case, we would not have been able to support our claim without access to an analysis technique like the one advocated here. More to the point, is that if one were to remove from Fig. 3 the lines found by EM, simple however, visual examination of the data would reveal little structure. Only through the use linear model is the bimodal structure of the data of EM and our two-component revealed. than the macro-operators in the LT domain from experimental the non-learning The message of this paper is that parametric analysis can give valuable insight In situations into experimental data, even when hypothesis test) are prefer- where non-parametric methods (e.g., the Wilcoxon signed-ranks able for hypothesis testing, one may still gain useful understanding of empirical data by positing a parametric model and using EM to estimate values for the parameters than hypothesis testing, and compared to traditional model-fitting methods, EM lets one use more complicated models. of the model. Modeling gives more testing is inconclusive. information Acknowledgements comments on a draft version of this paper. Support The authors wish to thank Paul Cohen and a second, anonymous, reviewer for for this their constructive research was provided by the Office of Naval Research through grants NOO14-88 K-0123, N00014-90-J-1542. and N0014-94-1178 (AMS), by the Advanced Re- search Project Agency through Rome Laboratory Contract Number F30602-93-C- 0018 via Odyssey Research Associates, (AMS), by a National Science Foundation graduate fellowship (GJG), by the National Science Founda- Incorporated A.M. Segre et al. I Artificial Intelligence 85 (1996) 301-319 319 through grant BES-9402439 tion (CPE). (GJG), and by a Hellman faculty fellowship References [l] M. Aitkin and D.B. Rubin, Estimation and hypothesis testing in finite mixture models, J. Ro_v. Stat. Sot. 47 (1985) 67-75. [2] J. Arbuthnott, An argument for divine providence, taken from the constant regularity observed in the births of both sexes, Philos. Trans. 27 (1710) 186-190. [3] T.L. Bailey and C.P. Elkan, Unsupervised expectation maximization, Mach. Learn. (to appear). learning of multiple motifs in biopolymers using [4] G. Casella and R. Berger, Statistical Inference (Brooks/Cole Publishing Company, Pacific Grove. CA, 1990). [S] A.P. Demptster, N.M. Laird and D.B. Rubin, Maximum likelihood from incomplete data via the EM algorithm, J. Roy. Stat. Sot. B 39 (1977) l-37. [6] 0. Etzioni and R. Etzioni, Technical note: statistical methods for analyzing speedup learning experiments, Mach. Learn. 14 (1994) 333-347. [7] J.N. Hooker, Needed: an empirical science of algorithms, Oper. Res. 42 (1994) 201-212. [8] K. Jedidi, V. Ramaswamy and W. Desarbo, A maximum likelihood method for latent class regression involving a censored dependent variable, Psychometrika 58 (1993) 365-394. [9] S. Minton, Quantitative 42 (1990) 363-392. results concerning the utility of explanation-based learning, Artif. Intell. [lo] R. Mooney, The effect of rule use on the utility of explanation-based learning, in: Proceedings ZJCAI-89, Detroit, MI (1989), 725-730. [ll] A. Newell, J.C. Shaw and H. Simon, Empirical explorations with the logic theory machine: a and J. Feldman. eds., Computers and Thought in: E. Feigenbaum case study in heuristics, (McGraw Hill, New York, 1963). [12] P. O’Rorke. LT revisited: explanation-based learning and the logic of Principia Mathematics. Mach. Learn. 4 (1989) 117-160. [13] A.M. Segre, On combining multiple speedup in: Proceedings Ninth Internafional Conference on Machine Learning, Aberdeen [14] A.M. Segre and C. Elkan, A high-performance explanation-based learning algorithm, Artif. techniques, (1992) 400-405. Zntell. 69 (1994) l-50. [15] A.M. Segre, C. Elkan and A. Russell, Technical note: a critical look at experimental evaluations of EBL, Mach. Learn. 6 (1991) 183-196. [16] A.M. Segre, C.P. Elkan, D. Scharstein, G.J. Gordon and A. Russell, Adaptive eds., Foundations of Knowledge Acquisition Vol. inference, in: A. ,7 (Kluwer Meyrowitz and S. Chapman, Academic Publishers, Boston, MA, 1993) 43-81. [17] A.M. Segre and D. Scharstein, Bounded-overhead caching for definite-clause theorem proving. J. Autom. Reasoning 11 (1993) 83-113. [18] A.M. Segre and D.B. Sturgill, Using hundreds of workstations in: Proceedings AAAI-94 Seattle, WA (1994) 187-192. to solve first-order logic problems. (191 A.N. Whitehead and B. Russell, Principia Mathemafica (Cambridge University Press. Cam- bridge, 1913). [20] F. Wilcoxon. Individual comparisons by ranking methods, Biometrics 1 (1945) 80-83. 