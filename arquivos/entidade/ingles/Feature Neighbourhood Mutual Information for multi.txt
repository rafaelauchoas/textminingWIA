Thi s is a n O p e n Acc e s s d o c u m e n t d o w nlo a d e d fro m ORCA, C a r diff U niv e r si ty' sin s ti t u tio n al r e p o si t o ry: h t t p s://o r c a . c a r diff.a c. u k/id/ e p ri n t/ 7 1 3 7 9/This is t h e a u t h o r’s v e r sio n of a w o r k t h a t w a s s u b mi t t e d t o / a c c e p t e d fo rp u blic a tio n.Cit a tio n fo r fin al p u blis h e d ve r sio n:Ro si n, P a ul L. ORCID: h t t p s://o r ci d.o r g/ 0 0 0 0-0 0 0 2-4 9 6 5-3 8 8 4 2 0 1 5. F e a t u r en ei g h b o u r h o o d m u t u al info r m a tio n fo r m ul ti-m o d al i m a g e r e gi s t r a tio n: Ana p plic a tio n t o e y e fu n d u s i m a gi n g. P a t t e r n R e c o g ni tio n 4 8 (6) , p p . 1 9 3 7-1 9 4 6.1 0 . 1 0 1 6/j. p a t c o g. 2 0 1 4. 1 2. 0 1 4 file P u blis h e r s p a g e : h t t p:// dx. doi.o r g/ 1 0. 1 0 1 6/j.p a t c o g. 2 0 1 4. 1 2. 0 1 4< h t t p:// dx. d oi.o r g/ 1 0. 1 0 1 6/j. p a t c o g. 2 0 1 4. 1 2. 0 1 4 >Pl e a s e n o t e: C h a n g e s m a d e a s a r e s ul t of p u blis hi n g p r o c e s s e s s u c h a s c o py-e di ti n g,fo r m a t ti n g a n d p a g e n u m b e r s m a y n o t b e r efl e c t e d in t his ve r sio n. Fo r t h ed efi nitiv e ve r sio n of t hi s p u blic a tio n, pl e a s e r ef e r t o t h e p u blis h e d s o u r c e. Youa r e a d vis e d t o c o n s ul t t h e p u blis h e r’s v e r sio n if yo u wi s h t o cit e t hi s p a p er.Thi s v e r sio n is b ei n g m a d e a v ail a bl e in a c c o r d a n c e wit h p u blis h e r p olici e s.S e e h t t p://o r c a . cf. a c. u k/ p olici e s. h t ml fo r u s a g e p olici e s. Co py ri g h t a n d m o r al ri g h t sfo r p u blic a tio n s m a d e a v ail a bl e in ORCA a r e r e t ai n e d by t h e c o py ri g h th ol d e r s .*ManuscriptClick here to view linked ReferencesFeature Neighbourhood Mutual Information forMulti-modal Image Registration:An Application to Eye Fundus ImagingP. A. Legg1,2, P. L. Rosin1, D. Marshall1 and J. E. Morgan3phil.legg@cs.ox.ac.uk, paul.rosin@cs.cf.ac.uk,dave.marshall@cs.cf.ac.uk, morganje3@cf.ac.uk1School of Computer Science, Cardiff University, UK.2Department of Computer Science, University of Oxford, UK.3School of Vision Sciences and Optometry, Cardiff University, UK.AbstractMulti-modal image registration is becoming an increasingly powerful toolfor medical diagnosis and treatment. The combination of different imagemodalities facilitates much greater understanding of the underlying condi-tion, resulting in improved patient care. Mutual Information is a popu-lar image similarity measure for performing multi-modal image registration.However, it is recognised that there are limitations with the technique thatcan compromise the accuracy of the registration, such as the lack of spatialinformation that is accounted for by the similarity measure. In this paper,we present a two-stage non-rigid registration process using a novel similaritymeasure, Feature Neighbourhood Mutual Information. The similarity measureefficiently incorporates both spatial and structural image properties that arenot traditionally considered by MI. By incorporating such features, we findthat this method is capable of achieving much greater registration accuracywhen compared to existing methods, whilst also achieving efficient compu-tational runtime. To demonstrate our method, we use a challenging medicalimage dataset consisting of paired retinal fundus photographs and confocalscanning laser ophthalmoscope images. Accurate registration of these imagepairs facilitates improved clinical diagnosis, and can be used for the earlydetection and prevention of glaucoma disease.Keywords: Mutual information; feature derivatives; gauge co-ordinates;non-rigid registration; ophthalmology.Preprint submitted to Pattern RecognitionDecember 1, 20141. IntroductionImage registration is the task of finding the spatial transformation thatgives correct matching correspondence between two images. Registration iswidely used in many application areas, including medical imaging, computervision, and satellite imagery. In particular, registration of images from dif-ferent modalities has become increasingly common to combine signals frommultiple sensors, where the registered images can be used to examine orexplain a particular observation further, for example in patient diagnosis.However, the difficulty is that by their very nature, multi-modal image pairsmay have no clearly defined relation between corresponding image intensi-ties. Mutual Information (MI) has become a popular similarity measure forregistering images of different modalities. The algorithm was simultaneouslyproposed by Viola and Wells [25] and Maes et al. [13]. MI differs from earlierregistration methods as it is derived from information theory and is basedon a statistical comparison of the images. Given two images, A and B, MIcan be defined as:I(A; B) = H(A) + H(B) − H(A, B)where H(A) is the entropy of A, H(B) is the entropy of B and H(A, B) isthe joint entropy of A and B. The transformation that maximises I(A, B)should give the correct registration of the images. Entropy gives a measureof the amount of information that a given signal may contain, and forms thebasis of MI. For a signal X consisting of n elements, Shannon’s entropy [22]is defined as:H(X) = −nXi=0p(i) log2 p(i)where p(i) is the probability of value i occurring within the data set. Theamount of information for a given value is inversely related to its probability,meaning that if the probability of a particular value occurring is low then thisreturns a greater amount of information than if the probability of the value ishigh. It can be thought of that the more rare the occurrence of an event, themore important it is when that event does occur. Despite the wide adoptionof MI, it is recognised that the method is not without limitations [12], nor2can it accurately register all varieties of image modalities, and so alternativemethods have since been proposed ([18, 9, 20, 1, 4, 15, 21, 24, 27, 11]).For our study, we are particularly interested in the registration of a chal-lenging dataset comprised of multi-modal retinal image data, in order toimprove clinician diagnosis and treatment of glaucoma. Glaucoma is thesecond most common cause of blindness in the West and the most commoncause of irreversible blindness worldwide [23]. The affects of glaucoma are ir-reversible, meaning that it is crucial to detect it in the early stages in order toprevent any further progression of the condition [19]. As shown in Figure 1,the image modalities that are to be registered together are colour fundus pho-tographs (shown on the right) and confocal scanning laser ophthalmoscope(SLO) images (shown on the left). Both modalities capture high quality im-ages from the eye of the optic nerve head (ONH), with the fundus photographrecording the clinical appearance and the SLO image providing quantitativeinformation such as the retinal surface reflectivity and topographic struc-ture [14]. From Figure 1, it is apparent to see that there is correspondingstructure present in both modalities, however the two modalities present thisstructure differently due to the acquisition techniques. For example, the sur-face reflectivity of vessels and the ONH result in a different representationcompared to the colour photograph, such as the dark ring of nerve fibres atthe ONH in the SLO, and the hollow appearance of large vessels in the SLO.Currently it is not typical practice to register these two modalities, howeversince the ONH boundary appears much clearer in the fundus photographit seems a logic step to utilise both images effectively. Registration wouldprovide correspondence between topographic and visible ONH damage, andearly detection of glaucoma would result in better prognosis and treatment.Figure 1 shows an example colour fundus photograph and SLO reflectivityimage captured from a patient’s eye, taken from our image data set. The dataset used in this study consists of 135 matching image pairs captured fromthe human eye. The original size of each fundus photograph is 752 × 490pixels, with a resolution of 72 pixels per inch. The SLO images are capturedusing the Heidelberg Retinal Tomograph II (HRT II) [6] device. The field ofview for each SLO image is 15 × 15 degrees and the original size is 384 × 384pixels, with a resolution of 96 pixels per inch. The data set consists of bothleft and right eyes and shows various stages of the glaucoma disease rangingfrom no sign of infection to highly glaucomatous. The data set provides aninteresting challenge for the image processing community, since there maybe regions of non-uniform lighting, regions that lack textural appearance,3Figure 1: A patient’s eye captured by two different image modalities, showing the retinasurface and blood vessels. Left: Confocal SLO image. Right: Fundus colour photograph.physical changes in structure and colour over time due to degradation, andalso distortion introduced by the various changes in curvature from the retinasurface. In addition, whilst the SLO images are of a high clinical standard,there are some cases where slight blurring occurs in the image due to subtlemovement in the eye (known as microsaccades) during acquisition. The dataset comprises of a wide variety of cases that a clinician would encounter whencapturing these two image modalities. All images were taken by an expertclinician, who also provided ground truth data using a manual alignmenttool that was developed specifically for this task. The anonymised data setis available from the authors upon request.This paper provides a extension of our preliminary report given in [11].Here, we now incorporate a two-stage non-rigid registration to provide greaterregistration accuracy, in addition to exploring a much wider set of multivari-ate features that can be utilised by the Feature Neighbourhood Mutual In-formation (FNMI) algorithm. In addition, we also provide a novel analysis ofregistration convergence in the transformation space to examine why differentmethods fail to provide accurate registration. This highlights a key consid-eration, in that a successful similarity measure not only needs to maximisethe correct registration, but would also need to provide a results across thecomplete transformation space that converges towards the correct registra-tion so that transformation optimisation can be performed without failure.The remainder of the paper is as follows: Section 2 provides a literaturereview of Mutual Information and different techniques that have extendedupon the original algorithm. Section 3 describes our proposed algorithm,4Feature Neighbourhood Mutual Information (FNMI). Section 4 presents theapplication of rigid registration. Section 5 discusses the issue of registrationconvergence and explores how this impacts on the different similarity mea-sures. Section 6 extends our method for non-rigid registration and presentsour final results, followed by our conclusion given in Section 7.2. Literature ReviewAs has already been introduced, Mutual Information (MI) is a popularsimilarity measure for registering images of different modalities. The algo-rithm was simultaneously proposed by Viola and Wells [25] and Maes et al.[13]. MI relies on the computation of entropy, which gives a measure of un-certainty for a random variable. It can be observed that by reducing the un-certainty within the joint distribution of the images, we obtain the strongestcorrespondence between them whilst the entropy of each individual imageensure that the image overlap contains meaningful information (rather thanregistering regions of little interest such as background).One recognised issue with MI is that there is little spatial informationincorporated into the measure, which means that if there is a complex cor-respondence between the image modalities then the standard approach canoften fail [12]. Many methods have been proposed to overcome this by in-cluding additional information as part of registration. Pluim et al. [18]suggested integrating gradient information into the MI measure by multi-plying MI by a gradient term. Similarly, Kubecka and Jan [9] suggestedusing gradient-image MI whereby MI is computed for both the original im-ages (after performing illumination correction) and also for the correspondinggradient images. With higher-order MI, Rueckert et al. [20] compute entropyfor intensity pairs rather than just individual intensities to introduce spatialinformation into MI. Beijing et al. [1] modified higher-order MI to use valuessuch as mean and median pixel neighbourhood, neighbouring pixel intensityand intensity gradient. Gan and Chung [4] proposed Maximum DistanceGradient Magnitude (MDGM) for obtaining detailed image structure thatis then also incorporated with higher-order MI. Similar to this, Mellor andBrady [15] proposed using local phase of the image to describe features withinthe registration. The issue with higher-order MI is that the histogram di-mensionality can become very large quite quickly as additional informationis incorporated. This becomes computationally demanding and, also, meansthat the resulting histogram is sparsely populated.5Russakoff et al. [21] and Tomaˇzeviˇc et al. [24] independently proposedsimilar methods that tackle this issue of incorporating additional informa-tion, with methods known as Regional MI (RMI) and Feature MI (FMI) re-spectively. With higher-order MI, to include all neighbouring intensities foreach point would result in a 9D histogram for each image and an 18D jointhistogram. Since such a space is far too large to efficiently compute, RMImodels the data more compactly by its covariance matrix. Russakoff tookthis approach to incorporate neighbourhood intensities whereas Tomaˇzeviˇcused this idea to incorporate gradient features for each point. Yang et al.[27] also furthered this work to include only the mean neighbourhood valuerather than the individual intensity values, leading to an even greater reduc-tion of data. As preliminary reported [11], our approach builds upon thesemethods by incorporating a wide range of derivative based spatial features asdescribed in the next section. More recently, there have been efforts withinthe community to highlight open-source software, such as the Workshop onOpen-Source Medical Image Analysis Software [26]. Other popular softwaretools include the Insight Segmentation and Registration Toolkit (ITK) [7],elastix [3], and NiftyReg [17].3. Feature Neighbourhood MIWhilst there exist many similarity measures for image registration, onemajor weakness for any technique is the presence of local maxima in thetransformation space that the registration is performed within.In manycases, the transformation space can be very large, and so search optimiza-tion techniques are required to reduce the computational load and to achievea result in an acceptable timeframe. Should local maxima be present withinthe transformation space then this can often result in an incorrect registrationresult. It is important therefore to emphasise the need for convergence, sothat the similarity measure being used does not only maximise the true regis-tration, but also converges well towards the maximum result across the wholetransformation space. To achieve this, we introduce Feature NeighbourhoodMutual Information (FNMI), a novel similarity measure that incorporatesboth structural feature derivatives and spatial neighbourhood information formulti-modal image registration. In this section, we shall describe the FNMIsimilarity measure, by first introducing gauge co-ordinate feature derivativesfor extracting useful features from images (Section 3.1), and then presentingthe algorithm to compute similarity that the features are incorporated within6(Section 3.2).3.1. Gauge Co-ordinate Feature DerivativesFeature derivatives essentially perform a mathematical operation on animage, in order to extract useful information such as gradients and ridges.Previously, there has been work that has addressed the use of first-orderfeature derivatives for image registration [1, 4, 18, 24]. Whilst much previouswork only considers first-order derivatives, here we propose to obtain higher-order features that can reveal much more detail on the underlying imagestructure, by the use of gauge co-ordinates [5]. Given intensity L, we candescribe features in a Cartesian co-ordinate frame such as the magnitudey. As more advanced properties are to be definedof the gradientwithin the image, Cartesian notation can soon become cumbersome. Forinstance, isophote curvature would be given as:xLyy − L2y)3/22LxLyLxy − L2x + L2x + L2L2yLxx(L2pIn contrast to Cartesian coordinates, gauge co-ordinates involve changingfrom extrinsic to intrinsic geometry [5] such that a local co-ordinate systemis determined for each individual pixel. The gradient direction of a pixel isan intrinsic property that is used to define the new local co-ordinate frameas the gradient vector ~w and its perpendicular direction ~v:~w =Lx Ly⇥01−1 0  =⇥⇤−Ly Lx⇤~v = ~w ·where Lx is the derivative of L with respect to x and Ly is the derivative ofL with respect to y. Derivatives of the intensity L can now be expressed interms of w and v. For example, Lw is the first derivative of L in the gradientdirection. Similarly, the isophote curvature described earlier can now besimplified significantly to − LvvLw . By using gauge co-ordinates, we can expresshigher-order feature derivatives much more clearly compared to traditionalCartesian notation. In particular, if we can express the feature derivativesclearer then we can simplify the exploration of different features in order tofind a suitable subset for registration.Figure 2 shows a number of different features that can be extracted,expressed using gauge co-ordinates at different scales (defined by  ). Taking7(a) Original(b) (Lv σ = 2)(c) (Lv σ = 4)(d) (Lww σ = 8)(e) (Lvvv σ = 2)(f) (Lwv σ = 4)(g) (Lwvvv σ = 4)(h) (Lwwv σ = 1)Figure 2: Original image and corresponding gauge co-ordinate feature derivatives thathighlight key features from the image.the derivative Lww σ = 8, note that the ridges in the image become welldefined where the isophotes occur, highlighting internal structure of the bloodvessels. Similarly, the derivative Lvvv σ = 2 produces a strong emphasis ofthe outer edges of the blood vessels. Furthermore, when we combine gradientand isophote derivatives we obtain a much richer representation of the imageconsisting of both properties, as can be seen with Lwv σ = 4, Lwvvv σ = 4and Lwwv σ = 1.In our preliminary study [11], we only considered scale-space derivatives,however we now consider a much wider set of features as suggested in [5].Whilst it is possible to extract many higher-order derivatives at many differ-ent scales, this results in a very large number of possible feature combinations.Since it is computationally infeasible to identify exactly what combinationof features would produce the best registration, instead we employ a sub-optimal Sequential Forward Search (SFS) [8] on a subset of 10 image pairsto determine a set of gauge co-ordinate features that perform well for theregistration task. Figure 2 shows the set of gauge co-ordinate features aschosen using SFS, applied to an example SLO image.8It is important to state that the SFS method described here is used onlyto determine a possible subset of features to include for our experimentation,and was not used to configure any other parameters for the testing phaseIn addition, further experimentation could be performed toof our study.identify which features perform best for the entire dataset, using a trainingand testing cross-validation approach. However, for real-world applicationsthis would not be feasible and so this is considered to be outside the scopeof this current work. Having obtained a set of features, the following sectionwill describe the algorithm implementation that the features are incorporatedwithin.3.2. Algorithm ImplementationIn order to calculate FNMI, we assume that we have two images, thefloating template A1 and the corresponding region in the reference imageB1 that we wish to compare against. For each of these images we derive aset of gauge co-ordinate feature images, as described in Section 3.1, denotedas A2...An and B2...Bn (where there are n − 1 gauge co-ordinate features).The collection of images A and B are combined to form a stack made upof 2n images. For each pixel in the template image, we create a vector thatconsists of the pixel and its neighbouring pixels (using a square window ofwidth 2r + 1), for each image in the stack. Supposing the size of the windowis r = 1, then there would be 8 direct neighbouring pixels, however just aswith RMI this may be increased to incorporate further spatial properties ifnecessary. For every pixel where there is an overlap between the content inthe template image and the reference image (of which the count of pixels isgiven by c), a vector consisting of d = 2n(2r + 1)2 elements is produced. Thevectors are then concatenated to give the matrix P (where the size of P isd × c).The matrix P now represents the combination of structural and spatialinformation for the two images being registered. To overcome the problem ofthe high dimensionality of the joint distribution, we adopt a similar approachthat was used by both RMI and FMI. Given P , we calculate its covariancematrix. This reduces the data to a d × d matrix that represents the variancebetween a given point and its neighbouring points from the original matrix.Unlike traditional MI, this approach also eliminates the issue of probabil-ity density estimation since the entropy can be calculated directly from thecovariance matrix.If we assume that the higher-dimensional distributionis approximately normally distributed, then as stated by Shannon [22], the9entropy of a normally distributed set of points in <d with covariance matrixC can be given as:H(C) = log((2πe)d2 det(C)12 ).The joint entropy is computed by H(C), and the marginal entropies arecomputed by H(CA) and H(CB), where CA is the d2 sub-matrix in thetop-left corner of C, and CB is the d2 sub-matrix in the bottom-rightcorner of C. The similarity between the two images A1 and B1 using FNMIis then given as:2 ⇥ d2 ⇥ dF N M I = H(CA) + H(CB)   H(C).4. Rigid RegistrationIn the previous section we have described the process of comparing thesimilarity between two images, using the FNMI algorithm. Since image reg-istration is the task of finding the spatial transformation that gives correctmatching correspondence between two images, we need to incorporate thesimilarity measure with a capability for searching the transformation space.For our study, we developed a MATLAB implementation that is capable ofperforming multi-modal image registration using a variety of different similar-ity measures. The system involves a two-stage registration approach, wherefirst an approximate rigid registration is found based on rotation and trans-lation. We use the built-in MATLAB implementation of the Nelder-Meadsimplex algorithm, combined with a 3-level image pyramid, for efficient searchoptimisation of this transformation space. A important consideration for anysearch optimisation tool is to ensure that the search does not become trappedby false local maxima within the function being optimised. In this experi-ment, the function being optimised is the similarity measure that is computedat each transformation where there is overlap between the template imageand the reference image. We study the effects of registration convergence fur-ther in Section 5. For this work, experiments were conducted on a standarddesktop PC machine configured with a Pentium 2.6GHz dual-core processor,4GB of memory, and Windows 7 operating system. The registration softwaredeveloped and a subset of the image data are both available to download byrequest to the authors.104.1. ResultsTo quantify the results of our experiments, we compare the registrationresults to the ground truth registration results as approved by an expertclinician. We compare the mean and median translation error T (measuredin pixels) and rotation error R (measured in degrees), along with the meanregistration error based on the 4 corner points of the template image, definedas Regerr (measured in pixels). This is calculated by measuring the distancefor each corner point between the registration result and the ground truth.For each registration technique where different parameters are to be tested,the bold values indicate the result that obtains the lowest error, or rather,the best performance. Each experiment is carried out using the complete setof 135 retinal image pairs.Table 1 shows the registration results when using the existing methodsfrom the literature. The methods tested are MI (Viola and Wells [25], Maeset al. [13]), Gradient MI (Pluim et al. [18]), Gradient-Image MI (Kubeckaand Jan [9]), Second-Order MI (Rueckert et al. [20]), Regional MI (Russakoffet al. [21]), Feature MI (Tomaˇzeviˇc et al. [24]) and Neighbourhood Incorpo-rated MI (Yang et al. [27]). For each method, we have also experimentedusing a variety of different parameters to assess the relative performance.We can observe a dramatic difference between using standard MI and meth-ods stated that extend upon MI. From these methods, Regional MI offersthe greatest accuracy improvement, despite requiring the longest runtime.Feature MI also provides fairly good registration accuracy whilst reducingthe runtime. As the two strongest methods, what would be advantageous isto integrate aspects from each of these methods so as to improve registrationeven further. We shall now perform our experimentation using the FNMIalgorithm described in Section 3 that aims to achieve this.4.1.1. Feature Neighbourhood MI using first derivativesIn this first stage, we restrict the possible feature set to the first derivativein the gradient direction, described as Lv in gauge co-ordinate notation, takenat different scales defined by σ.Table 2 shows the registration errors for our proposed similarity measure.The experiments show where a single feature is combined with the intensityimage and also where multiple scale features are combined (with a maximumof 3 additional features being used). FNMI appears to achieves very goodregistration errors for all the tested methods. When one additional featureis used, Lv σ = 2 gives the lowest registration error of Regerr = 5.08. The11MethodMI (256 bins)MI (Scott’s Rule +Skewness)Gradient MIGradient-Image MI2nd-Order MI (left pixel)2nd-Order MI (right pixel)2nd-Order MI (mean)2nd-Order MI (median)2nd-Order MI (gradient)Regional MI (r=1)Regional MI (r=2)Regional MI (r=3)Regional MI (r=4)Regional MI (r=5)Feature MI (Lvσ = 1)Feature MI (Lvσ = 2)Feature MI (Lvσ = 4)Feature MI (Lvσ = 8)Feature MI (Lvσ = 1, 2)Feature MI (Lvσ = 1, 4)Feature MI (Lvσ = 1, 8)Feature MI (Lvσ = 2, 4)Feature MI (Lvσ = 2, 8)Feature MI (Lvσ = 4, 8)Feature MI (Lvσ = 1, 2, 4)Feature MI (Lvσ = 1, 2, 8)Feature MI (Lvσ = 1, 4, 8)Feature MI (Lvσ = 2, 4, 8)NIMI (r=1)NIMI (r=2)NIMI (r=3)NIMI (r=4)NIMI (r=5)MeanT154.541.742.0929.2651.3651.9143.6950.4322.0939.2611.324.694.021.8719.4817.6218.4023.9914.6612.4914.3814.5814.9917.0113.1213.3812.6214.6963.8866.0364.7667.7968.14R2.81.92.242.021.691.881.972.041.131.040.580.510.470.471.892.102.482.651.541.702.061.862.082.501.601.551.621.992.402.442.512.372.71Regerr Runtime154.443.243.1530.8452.2852.6945.0651.5422.8839.5411.945.504.752.6421.0919.6821.2627.1716.1614.4916.8116.5617.4320.1515.1114.9714.4217.3064.7166.7865.5868.5668.992.775.7010.507.342.752.733.113.805.7514.0224.6841.2567.6996.839.2110.6811.1512.4012.3811.9112.7712.0412.8513.3913.6114.5615.1015.346.516.386.356.316.39Table 1: Registration errors for all 135 image pairs using existing MI registration methods.Translation and Regerr are given in pixels, rotation is given in degrees, and runtime isgiven in seconds. The bold values signify the lowest error obtained for each registrationtechnique, based on the given parameters.mean runtime for this method is 84.17 seconds. When two additional featuresare used, the combination of Lv at scales σ = 2 and σ = 4 further reducesregistration error, giving Regerr = 2.63. The inclusion of these two features12FNMILv (σ = 1)Lv (σ = 2)Lv (σ = 4)Lv (σ = 8)Lv (σ = 1, 2)Lv (σ = 1, 4)Lv (σ = 1, 8)Lv (σ = 2, 4)Lv (σ = 2, 8)Lv (σ = 4, 8)Lv (σ = 1, 2, 4)Lv (σ = 1, 2, 8)Lv (σ = 1, 4, 8)Lv (σ = 2, 4, 8)MeanT6.194.146.6521.493.182.016.211.858.219.182.115.975.477.32R0.660.640.821.480.430.490.810.500.611.020.510.500.590.65Regerr Runtime6.995.087.3422.583.912.777.372.639.0410.712.906.726.248.0984.4484.1792.18100.21119.32115.87126.36119.18142.89123.77153.96151.60175.09152.93Table 2: Registration errors for all 135 image pairs using FNMI (with multi-scale gradientfeatures). Translation and Regerr are given in pixels, rotation is given in degrees, andruntime is given in seconds. The bold values signify the lowest error obtained for eachregistration technique, based on the given parametersgives a mean runtime of 119.18 seconds. Whilst this is an increase comparedto using just one additional feature, a runtime of 2 minutes is still seen as anacceptable runtime, especially if the registration proves to be accurate.4.1.2. Feature Neighbourhood MI combining multiple first derivatives andmultiple higher-order gauge derivativesWithin our study we have performed FNMI registration using a numberof different feature combinations derived by gauge co-ordinates and multi-scale derivatives. The complete testing procedure is detailed in [10], howeverfor conciseness, here we shall present only the final testing stage. Similar tobefore, we shall focus on the combinations of first derivative features thatare deemed to provide useful information for the purpose of registration aswas discussed in Section 3.1 (Lv (σ = 1, 2), Lv (σ = 1, 4) and Lv (σ = 2, 4)).Table 3 shows the registration errors when two multi-scale first derivativefeatures are used along with two higher-order gauge derivatives. In compari-son to our testing of MI based methods (see tables 1 and 2) these results showgreater registration accuracy. The lowest registration errors occur when us-ing the features Lv (σ = 2, 4), Lvvv (σ = 2) and Lwvvv (σ = 4), whereRegerr = 2.34. Similarly, there are two other combinations that also improve13FNMILv (σ=1,2) Lww (σ=8) Lvvv (σ=2)Lv (σ=1,2) Lww (σ=8) Lwv (σ=4)Lv (σ=1,2) Lww (σ=8) Lwvvv (σ=4)Lv (σ=1,2) Lww (σ=8) Lwwv (σ=1)Lv (σ=1,2) Lvvv (σ=2) Lwv (σ=4)Lv (σ=1,2) Lvvv (σ=2) Lwvvv (σ=4)Lv (σ=1,2) Lvvv (σ=2) Lwwv (σ=1)Lv (σ=1,2) Lwv (σ=4) Lwvvv (σ=4)Lv (σ=1,2) Lwv (σ=4) Lwwv (σ=1)Lv (σ=1,2) Lwvvv (σ=4) Lwwv (σ=1)Lv (σ=1,4) Lww (σ=8) Lvvv (σ=2)Lv (σ=1,4) Lww (σ=8) Lwv (σ=4)Lv (σ=1,4) Lww (σ=8) Lwvvv (σ=4)Lv (σ=1,4) Lww (σ=8) Lwwv (σ=1)Lv (σ=1,4) Lvvv (σ=2) Lwv (σ=4)Lv (σ=1,4) Lvvv (σ=2) Lwvvv (σ=4)Lv (σ=1,4) Lvvv (σ=2) Lwwv (σ=1)Lv (σ=1,4) Lwv (σ=4) Lwvvv (σ=4)Lv (σ=1,4) Lwv (σ=4) Lwwv (σ=1)Lv (σ=1,4) Lwvvv (σ=4) Lwwv (σ=1)Lv (σ=2,4) Lww (σ=8) Lvvv (σ=2)Lv (σ=2,4) Lww (σ=8) Lwv (σ=4)Lv (σ=2,4) Lww (σ=8) Lwvvv (σ=4)Lv (σ=2,4) Lww (σ=8) Lwwv (σ=1)Lv (σ=2,4) Lvvv (σ=2) Lwv (σ=4)Lv (σ=2,4) Lvvv (σ=2) Lwvvv (σ=4)Lv (σ=2,4) Lvvv (σ=2) Lwwv (σ=1)Lv (σ=2,4) Lwv (σ=4) Lwvvv (σ=4)Lv (σ=2,4) Lwv (σ=4) Lwwv (σ=1)Lv (σ=2,4) Lwvvv (σ=4) Lwwv (σ=1)MeanT5.926.033.905.082.423.554.894.172.673.435.625.702.943.301.821.894.673.081.792.195.543.374.333.083.851.714.153.971.912.46R0.420.550.550.400.480.500.510.490.500.480.480.720.670.520.470.500.490.670.460.510.570.630.570.550.440.430.380.610.470.50Regerr Runtime6.376.754.655.603.274.385.684.893.414.096.226.704.154.012.542.655.314.052.522.926.164.325.213.844.422.344.704.862.633.12222.64211.59208.22213.71200.56211.02213.66209.09199.20216.91200.00210.59223.07211.82203.67224.43209.06217.27220.17216.33205.62210.56227.03208.09207.06223.51206.87223.29222.41219.28Table 3: Registration errors for all 135 image pairs using FNMI (combining multiple firstderivative features with multiple higher-order gauge derivatives). Translation and Regerrare given in pixels, rotation is given in degrees, and runtime is given in seconds. The boldvalues signify the lowest error obtained for each registration technique, based on the givenparameterson our previous results, when using features Lv (σ = 1, 4), Lvvv (σ = 2) andLwv (σ = 4), and features Lv (σ = 1, 4), Lvvv (σ = 2) and Lwvvv (σ = 4). Theresults given for each of these tests all register the images to a satisfactoryclinical standard, showing that FNMI provides an extremely high level ofregistration accuracy compared to existing methods. From our preliminarystudy [11], we have shown here that incorporating more suitable featuresdoes improve the registration accuracy.145. Registration ConvergenceAn important consideration for any similarity measure is how well thecorrect solution can be found within the parameter space. There are two as-pects in particular that should be considered when evaluating the registrationspace; whether the global maximum is the correct registration and how wellthe global maximum can be found in the space. Clearly the most importantrequirement is that the global maximum of the similarity measure occursat the correct registration, otherwise registration will most likely fail. Forour testing, we shall determine from each point in the transformation spacewhether the global maximum can be reached by performing hill climbing.Our approach will assume that the greatest neighbouring value should befollowed, continuing in this fashion until a peak is reached. If this peak is theglobal maximum then it can be said that the starting point converges to theglobal solution. The collection of starting points that converge to the globalmaximum make up the catchment region. Ideally we wish to maximise thecatchment region to improve the likelihood of the search optimization findingthe correct solution.We investigate two possible scenarios for evaluating registration conver-gence; where rotation is fixed and set to the ground truth value, and whererotation lies between ±3  (with an increment of 0.5 ). The first approachallows for better visualization of the registration space since we are exploringonly the translation space, and so this can be displayed clearly by a sur-face plot of the similarity value. The second approach is more difficult tovisualize, but will determine whether the similarity is reliable across the fulltransformation space. The second approach also resembles the true regis-tration problem more accurately, since the transformation space is the sameas that at the coarse level of the image pyramid. In both cases we use thecoarse level of the image pyramid (where the images are 14 of the originalsize, resulting in a fundus image of 188 × 122 pixels and an SLO image of96 × 96 pixels), and we consider the translation space to be the size of thereference image (in this case, the fundus photograph).Figure 3 shows the registration surface plots given by different similaritymeasures for a typical registration of the retinal images. It can be seen thatin each case the global maximum is the same, and is actually the correctpoint of registration. However, the registration surfaces do appear quitedifferent. First, for MI there are many other peaks in the surface which couldlead to incorrect registration. The surfaces obtained using FNMI, FMI and15(a) FNMI (Lv σ = 2)(b) FMI (Lv σ = 2)(c) MI (16 bins)(d) RMI (r = 1)(e) RMI ( r = 3 )(f) RMI ( r = 5 )Figure 3: Registration similarity measure surface plots across the translation space. Themaximum point on each surface plot shows the correct translation for registration.RMI are all relatively smooth and all have a distinctive peak at the globalmaximum.It is evident that the number of local maxima is significantlyreduced compared to MI, leading to a much smoother registration surface.Regional MI (r = 1) shows the point of registration to be very steepwhereas the remainder of the surface is relatively flat. The catchment re-gion of such a point therefore is quite small. As the neighbourhood radiusis increased this catchment region becomes larger, however this also has theeffect of enhancing the catchment region for the local maximum that occursin the top-right of the surface. The surface obtained using FMI is interestingas there are regions with steady slopes to the global maximum. There arehowever also plateau effects present (such as to the right of the global max-imum) that could easily hamper the search algorithm. Finally, when usingFNMI we obtain a distinct peak similar to RMI but also the consistent steadyslope similar to FMI. Also, the problematic local maximum in the top-rightof RMI is reduced significantly in FNMI.16Figure 4: Registration convergence to global optimum for all 135 image pairs, for transla-tion only. (Mean shown by green star, median shown by red horizontal bar, interquartilerange shown by blue box, data range shown by whiskers and outliers shown by red crosses).Figure 4 shows the results for registration convergence when using 6 dif-ferent similarity measures FNMI, FMI, MI and RMI (r = 1, r = 3 andr = 5). The results have been computed for the full set of 135 image pairsfor translation only (where rotation is fixed by the ground truth value). Thegreater the percentage of convergence the more points in the surface thatwill converge to the global maximum value by means of steepest ascent andso we wish to maximize this. As can be seen on the boxplot, FNMI achievesthe greatest mean, median and interquartile range. The mean convergencefor FNMI is 45%, compared with 39% for RMI (r = 5) and 31% for FMI.This is substantially better than just 9% when using MI (16 bins).We perform the same experiment using the full transformation range asis used for the registration task. The challenge of convergence becomes muchmore difficult due to the larger transformation space. Similar to before, wesearch the parameter space which is now a 3-dimensional space (consisting17Figure 5: Registration convergence to global optimum for all 135 image pairs, for bothrotation and translation. (Mean shown by green star, median shown by red horizontalbar, interquartile range shown by blue box, data range shown by whiskers and outliersshown by red crosses).of x-translation, y-translation and rotation).Figure 5 shows the results for registration convergence when consideringthe full transformation parameter space. As was evident in the previous test-ing, it can be seen here also that FNMI provides the best convergence result(given by the largest mean and median results). Since search optimizationschemes are often used for large registration tasks, the convergence of thesimilarity measure becomes critical to ensure the successful outcome of theregistration.6. Non-rigid RegistrationSo far in our study we have performed only rigid registration on the retinalimage pairs. The second stage of registration extends from rigid to non-rigidregistration in order to improve registration accuracy even further.18Figure 6: Example to highlight subtle misalignment in rigid registration (for the imagepair shown in Figure 1).Figure 6 shows an example where global rigid registration appears to haveperformed well, however on closer inspection it can be seen that there aresubtle misalignments, particularly in the two example regions shown. Whilethe centre of the SLO (the ONH) and the main blood vessels register well, itis normally towards the periphery of the image that misalignment becomesapparent. From our test data, this is quite common of these two modalities.This deformation is due to the curvature in the retina surface and the differ-ences between the acquisition of the two images. The fundus image is simplya photograph of the retina whereas the SLO image is generated from 64 in-dividual slices captured at different focal lengths. The deformation could becompared to that of pincushion distortion, however since the distortion isdependent on the curvature of the patient’s retina rather than the cameraoptics this cannot be globally modeled in the same fashion. What is clearlyapparent though is that rigid registration is not sufficient to accurately reg-ister the two modalities successfully.In order to correct the registration, rather than considering the templateimage as a whole (as we have done previously) the image is divided into acollection of subimages. Each subimage is a region from the original thatcan be translated across the reference image. It is possible to restrict thetranslation range of each subimage or apply a weighting term that favourssmall translations so as to keep the regions relatively close together. In thecase of our registration problem, we divide the SLO image into a 4 × 4 setof images. This partitions the SLO image so that the ONH occurs withinthe central 4 windows and the peripheral blood vessels occur in the outer12 windows. Taking the corresponding region from the rigid registration as19the initial starting position, each subimage is individually registered to thereference image. We found that it was sufficient to only consider translationfor each region since the initial registration is already relatively accurate.Also, no image pyramid is required since the size of each subimage is alreadyquite small. The region of translation is restricted for each window sinceit is only subtle misalignment that we expect to be correcting for. For the16 windows used, we limit the 4 central windows to a translation radiusof 3 pixels and we limit the 12 outer windows to a translation radius of 5pixels. Since the translation area is relatively small, the similarity measureis much less likely to suffer dramatically from local maxima unlike the largertransformation search used for the rigid registration.The above process results in 16 individually registered image regions onthe fundus image. If there is deformation then it is most likely that there willbe either overlap or gaps between the individual windows. For each window,we can determine the deformation by considering the centre point of thewindow and measuring the distance between the original window positionand the newly-registered position. The collection of points for the initial andnewly-registered windows allows us to easily model the deformation by usingThin Plate Spline warping [2].6.1. ResultsIn order to test our approach to non-rigid registration, we shall use the135 retinal image pairs described previously. Firstly the images are registeredusing FNMI in order to find an approximate rigid registration. From thiswe perform non-rigid registration using FNMI as the similarity measure ona local window basis and then reconstruct the template image using ThinPlate Spline deformation. The deformation is performed for the SLO imagewhich is mapped to the fundus photograph, since clinically it is the fundusimage that is seen as the ‘gold standard’ [16].Figure 7 gives an example image highlighting the correction that non-rigid registration can offer. In this example, we improve the correspondencebetween the blood vessels in the two highlighted regions whilst preservinggood registration throughout the rest of our image as achieved previously.Using our technique for non-rigid registration, the runtime is very efficientand takes approximately 40 seconds to compute from the initial rigid regis-tration (when using FNMI as the similarity measure).Non-rigid registration results were evaluated by three independent clinicalobservers using a 5 point scale. A system was developed to randomly present20Figure 7: Corrected registration using non-rigid registration (for the image pair shown inFigure 1).the rigid and non-rigid registration images to the clinicians. The clinicianImages werethen had to grade the image before the next was presented.presented as those shown in Figure 8. Each result was presented twice,with the checkerboard overlay being inverted. The mean result was used toeliminate any bias that the display overlay could potentially introduce.Excellent V. Good Good Weak FailRigidNon-rigid116135140401000Table 4: Mean results of rigid and non-rigid registration images as graded by three inde-pendent clinicians by visual assessment.Table 4 shows the mean grades awarded by the clinicians for both rigidand non-rigid registration results. The table clearly indicates the improve-ment that non-rigid registration has, where it can be seen that all non-rigidresults were graded by all three clinicians as excellent.Figure 8 shows an example of the non-rigid registration results. In therigid registration results, whilst the majority of the registration may appearcorrect, typically there is misalignment that occurs towards the right of thetemplate image (particularly noticeable at the top-right of each templateimage). The non-rigid registration manages to correct for this in each casesuccessfully, whilst also preserving the global registration. Clearly, it is im-portant for the rigid registration to provide an accurate approximation of thetrue registration, so that the non-rigid registration process can then refinethis, since to perform non-rigid registration over the complete transformationspace initially would be computationally infeasible.21Figure 8: Rigid registration (left) compared to non-rigid registration (right) results, forfour example SLO and fundus image pairs.227. ConclusionWe have proposed a two-stage non-rigid registration scheme for multi-modal retinal image registration. The proposed method achieves excel-lent accuracy and also maintains efficient runtime. As part of the registra-tion scheme we have proposed Feature Neighbourhood Mutual Information(FNMI) similarity measure, that extends the MI algorithm to incorporate fur-ther image properties such as spatial and structural information. We presentour results using a sub-optimal feature set, which provides a high degree ofaccuracy for the registration task being performed. Due to the flexibilityof information that can be incorporated into the similarity measure in thisfashion, it is likely that this approach could well be used for other multi-modal registration tasks. The features that we present in Figure 2 serve as aset of suitable features that could be widely applicable to other registrationtasks. We also study the registration convergence of FNMI and other simi-larity measures, to assess how well they perform in conjunction with searchoptimization schemes that are often required for large registration tasks. Wefocus on multi-modal retinal images since the registration of these images isa difficult task that cannot be achieved to a satisfactory standard using otherexisting methods. The registration of these two modalities can be used tosignificantly improve demarcation accuracy and monitoring of the ONH forearly detection and prevention of glaucoma disease.References[1] Beijing, C., JunLi, L., Gang, C., 2007. Study of medical image regis-tration based on second-order mutual information. IEEE InternationalConference on Multimedia & Expo , 956–959.[2] Bookstein, F.L., 1989. Principal warps: Thin-plate splines and the de-composition of deformations. IEEE Transactions on Pattern Analysisand Machine Intelligence 11, 567–585.[3] Elastix. http://elastix.isi.uu.nl.[4] Gan, R., Chung, A., 2005. Multi-dimensional mutual information basedrobust image registration using maximum distance-gradient magnitude.,in: IPMI, pp. 210–221.23[5] ter Haar Romeny, B.M., Florack, L.M.J., Salden, A.H., Viergever, M.A.,1993. Higher order differential structure of images, in: IPMI, pp. 77–93.[6] Heidelberg Engineering, Heidelberg, G., 1999. Quantitative Three-dimensional Imaging of the Posterior Segment with the HeidelbergRetina Tomograph. Heidelberg Engineering, Heidelberg, Germany.[7] InsightSegmentationandRegistrationToolkit(ITK).http://www.itk.org.[8] Kittler, J., 1978. Feature set search algorithms. Pattern Recognitionand Signal Processing , 41–60.[9] Kubecka, L., Jan, J., 2004. Registration of bimodal retinal images -improving modifications. Engineering in Medicine and Biology Society,2004. IEMBS ’04. 26th Annual International Conference of the IEEE 1,1695–1698.[10] Legg, P.A., 2010. Multimodal retinal imaging: Improving accuracy andefficiency of image registration using Mutual Information. Ph.D. thesis.School of Computer Science and Informatics, Cardiff University.[11] Legg, P.A., Rosin, P.L., Marshall, D., Morgan, J.E., 2009. A robustsolution to multi-modal image registration by combining mutual infor-mation with multi-scale derivatives, in: MICCAI, pp. 616–623.[12] Legg, P.A., Rosin, P.L., Marshall, D., Morgan, J.E., 2013. Improvingaccuracy and efficiency of mutual information for multi-modal retinalimage registration using adaptive probability density estimation. Com-puterized Medical Imaging and Graphics 37, 597–606.[13] Maes, F., Collignon, A., Vandermeulen, D., Marchal, G., Suetens, P.,1997. Multimodality image registration by maximization of mutual in-formation. IEEE Transactions on Medical Imaging 16, 187–198.[14] Mardin, C.Y., Horn, F.K., Jonas, J.B., Budde, W.M., 1999.Preperimetrix glaucoma diagnosis by confocal scanning laser tomogra-phy of the optic disc. British Journal of Ophthalmology 83, 299–304.[15] Mellor, M., Brady, M., 2005. Phase mutual information as a similaritymeasure for registration. Medical Image Analysis 9, 330–343.24[16] Morgan, J.E., Sheen, N.J.L., North, R.V., Goyal, R., Morgan, S., Ansari,E., Wild, J.M., 2005. Discrimination of glaucomatous optic neuropathyby digital stereoscopic analysis. Ophthalmology 112, 855–862.[17] NiftyReg. http://www.nitrc.org/projects/niftyreg/.[18] Pluim, J.P.W., Maintz, J.B.A., Viergever, M.A., 2000.Image regis-tration by maximization of combined mutual information and gradientinformation. IEEE Transactions on Medical Imaging 19, 809–814.[19] Rosin, P.L., Marshall, D., Morgan, J.E., 2002. Multimodal retinal imag-ing: new strategies for the detection of glaucoma., in: ICIP (3), pp.137–140.[20] Rueckert, D., Clarkson, M.J., Hill, D.L.G., Hawkes, D.J., 2000. Non-rigid registration using higher-order mutual information. Medical Imag-ing: Image Processing , 438–447.[21] Russakoff, D.B., Tomasi, C., Rohlfing, T., Jr., C.R.M., 2004.Imagesimilarity using mutual information of regions, in: ECCV (3), pp. 596–607.[22] Shannon, C.E., 1948. A mathematical theory of communication. BellSystem Technical Journal 27, 379–423, 623–656.[23] Sommer, A., 1996. Doyne lecture. glaucoma: facts and fancies. Eye 10,295–301.[24] Tomaˇzeviˇc, D., Likar, B., Pernuˇs, F., 2004. Multifeature mutual infor-mation, in: Medical Imaging 2004: Image Processing, pp. 143–154.[25] Viola, P.A., Wells, W.M., 1995. Alignment by maximization of mutualinformation, in: ICCV, pp. 16–23.[26] Workshop on Open-Source MedicalImage Analysis Software.http://www0.cs.ucl.ac.uk/opensource mia ws 2012/index.html.[27] Yang, C., Jiang, T., Wang, J., Zheng, L., 2006. A neighborhood incor-porated method in image registration., in: MIAR, pp. 244–251.25