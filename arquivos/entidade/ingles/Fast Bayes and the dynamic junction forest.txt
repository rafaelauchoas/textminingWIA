Artificial Intelligence 107 (1999) 99-124 Artificial Intelligence Fast Bayes and the dynamic junction forest J.Q. Smith a,*, K.N. Papamichail b,l a Department of Statistics, University of Warwick, Coventry, CV4 7AL, UK b School of Informatics, University of Manchester; Manchestel; Ml3 9PL, UK Received 10 Aprit 1997; received in revised form 11 June 1998 Abstract It has been shown propagating probabilities conditional independence examples how processes whose conditional augmented that junction tree algorithms can provide a quick and efficient method for in complex multivariate problems when they can be described by a fixed In this paper we formalise and illustrate with two practical to high dimensional can be applied structure, as well as their underlying distributions, independence propagation algorithms structure. are these probabilistic through the passage of time. 0 1999 Elsevier Science B.V. All rights reserved. Keywords: Dynamic models; Hellinger metric; Influence diagrams; Multivariate state space models Junction trees; Probabilistic expert systems; 1. Introduction that Bayesian in the problem it has been established Over the last ten years relationships between variables are often represented by influence diagrams or causal networks-see expert systems can be made to work quickly and accurately on a wide range of practical problems provided fixed. These that the underlying relationships [2,8,12, 26,3 l] and later in this paper. However, there are many problems where such relationships coded in the graph may be eroded independencies evolve; for example, as information between variables in any learning environment we believe may change through learning at some point that such an evolution of structure an ignored-for needs different exception see Kjaerulff [ 111. Learning this issue. However, until now this has been the structure of dependencies the passage of time. Indeed is gathered. Alternatively, in a dynamic environment is almost inevitable-so is fundamentally the conditional to address remained machine largely * Corresponding ’ Email: nadia.papamich@man.ak.uk. author. Email: j.g.smith@wanvick.ac.uk. OOW3702I99/$ PII: SOOO4-3702(98)00103-9 - see front matter 0 1999 Elsevier Science B.V. All rights reserved. IO0 J.Q. Smith, K. N. Pupumdwil /Artifktrl Intrlli~mw IO7 (IYYY) 99-124 to enable that company that are static in a number of key ways as it can be illustrated from relationships through the following example. Suppose a company produces a number of different brands of products within a large market and knows all her competitors’ brands. There are now marketing to produce a causal network over the vectors techniques available [ 19,201. This causal network codes up of sales made within a given short time interval the sales of different brands at that time and after being the dependency structure between transformed the probability distributions of sales of one product given precise information about collections of others. in a static environment where the causal These quick algorithms are now well understood network remains unchanged over time-for are outlined tree 191 it can be used to update efficiently a helpful introduction in the next section. see Jensen [9]-and into a junction the variables into sales information can induce other dependencies two problems. Firstly. the algorithms in the network. For example, a company will often have much the company’s database will not in the causal network. Instead it will However, in a dynamic environment we encounter of Jensen cannot usually be employed directly because be complete enough to observe directly typically have incomplete noisy data which is not fully disaggregated on the individual brands better information these types of imperfection example of this is when we acquire perfect two brands whose sales S( 1) and S(2) are not dependent, being the market. Having observed S, knowing S(1) tells us precisely the aggregate S induces a dependence possibly also its junction information. Another type of dependence, course a different causal network and its associated junction all possible aggregates, but a dynamic environment processing. It should always be possible even if it is not in the pre-processed the inferential necessary. about its own product sales than those of its competitors. Unfortunately, in the system. The simplest about the sum S of the sales of in different sections of the value of S(2). So between S(1) and S(2). The causal network and in the light of this piece of in [3 11. Of tree might be drawn to include to such pre- as it becomes available to adjust it becomes to absorb new information format. from the influence diagram as and when tree will therefore need to be adjusted induced by missing data, is discussed It is therefore highly desirable is not really conducive framework derived information A second problem generic that components to dynamic environments and the underlying dependence In the application often possible change. competing brand. Alternatively reposition a brand so that it has a different set of competitors. diagram which represents she may choose the dependency for example, above, a competitor may to reprise or re-advertise structure will periodically introduce a new and therefore In this case, the influence like the one above is that it is structure will need to change. In Section 4 of this paper we will formalise of a sequence of graphs called junction experiences in developing forecasting of product sales in a competitive market [2 1,221 and the other the forecasting of the spread of contamination [4,6,29]. The second application after a nuclear accident has been coded up as an operational dynamic junction the description of a dynamic system in terms is based on the authors’ the first, the forests. This formalism fields of application: in two different forest [4]. algorithms In Section 5 we specify an algorithm for automatically adjusting both the graph on which calculations are made and also the objects (the cliques) which form the nodes of that graph. to work when each observation This allows a propagation algorithm+riginally designed J.Q. Smith, K.N. Papamichail /ArtQicial intelligence 107 (1999) 99-124 101 relates only to states in a single clique of the original be valid generally. Such adjustments often reduce the subsequent efficiency of the system because the new junction forest usually has a smaller number of cliques of a larger size. forest-to In Section 6 we introduce several transformations of a junction forest that automatically so that it is a more compact in each of the joint probability distribution changes the representation and efficient form. The method modifies not only the number of states contained clique but also the relationships between discarding techniques a transformation its approximation. This section concludes with an illustration described variables no longer of interest. can also be incorporated within such a system giving an example of such used in conjunction with the Hellinger metric between a density and using the nuclear example them and the existence of the cliques themselves, In Section 7 we outline how approximate in Section 3. We begin the paper with a short review of state space representations of problems and the junction tree propagation algorithm [9,10]. 2. From dynamic influence diagrams to junction graphs For simplicity and only with a slight loss of generality, we shall assume that our dynamic time state space models equation and a system time. One standard class of discrete in terms of an observation [7,34]) is specified in discrete system is defined (see, for example, equation respectively: Y, = Ft0, + ut, 8, = G&e1 + cot. An illustration of the above model is given in Fig. 1. The error distribution of {z/tot: so that an observation Yt at time t = 1,2, . . .} is nearly always chosen t satisfies the conditional independent statement: qJ,,ez 1... I&, (2.1) (2.2) to be mutually independence (2.3) i.e., an observation of the state vector at that time, and at time t only gives direct information about the values of components &+I 1 t&+2, . . . LI f4,...&1 let, (2.4) i.e., states are Markov in time-we vector and then we can determine any probability the future. only need to retain our beliefs about the current state statements we might wish to make about The random vectors ot = 1,2, . . . are called state random vectors. In dynamic systems the state vector is an important object because if we were given its value we could disregard Fig. 1. A discrete time state space model. 102 J.Q. Smith, K.N. Pupamichail /Artificial Intelligence 107 (1999) 99-124 as irrelevant for predicting future observations Y. In this sense the all other information In our simple information we need for forecasting. state vector contains all the relevant marketing example, the state random vectors will be the vectors of sales of each brand at time t, which typically will not be known precisely by the company. The matrix G specifies in time. The simplest evolution would make G how these sales are expected the identity matrix, which would stipulate t were expected to be the same as they were in the time period t - 1. The error vector Wt, captures the likely random drift in this relationship. Each vector Y,, t = 1,2, . . , is the set t where we can read from the rows of of possibly component of Y, the matrix F the aggregate of brands associated with each observational There will usually be some measurement and the vector ut just represents error associated with these observations incomplete data received that sales of each brand in the time period in the interval to develop this. A useful way of coding conditional (XI. . . , x,) is by a causal independence network (graph of an influence diagram) as a set of random vectors [9]. Given a collection of conditional statements of the form: independence statements (2.5) ~ Xk- I } whose union (n-1, X2, i.e., a directed acyclic graph whose nodes are XI, where fi;2k, nk are disjoint subsets of (XI, x2, . X&l } is . . . , x,, with a directed a causal network, edge from x; to xk, 1 6 i < k < n, if and only if Xi lies in the set nk.The elements of flk if and only are called the parents of Xk. A directed acyclic graph is a valid causal network statements of Eq. (2.5) are true. It is easy to show that a if all the conditional valid causal network on the nodes HI, 01, (2.2) has the form of Fig. 2. Deductive diagrams have been well studied-see, . HI associated with the system equation rules on causal networks/influence [I 3,14,17,24]. independence In our context, for example in order to develop networks it will be important vector at any given time. To illustrate to allow nodes to represent the most powerful methods of learning on causal individual components of a state this we return to our marketing example. it is possible In time period 1, there are eight brands in the market (A, B, C. D, E, F, G, H). Suppose to asset that the sales of the three collections of that from expert judgment brands {A, B. C. D}, {E. F} and {G, H) are independent of each other and given the sales learnt about A and of B, the predictions of sales of brand C will not depend on anything the prediction of brand sales of D will not depend on anything learnt about the sales of A can be cited in the causal network and C. It is straightforward of Fig. 2. to show that these judgments Now we can use the system equation to extend structure when we add nodes representing dependency the causal network the sales at future times. For the sake to depict Fig. 2. A causal network between brands sales m the first time period. J.Q. Smith, K.N. Papamichail /Art@ial Intelligence 107 (1999) 99-124 103 ____........._______............... y ____________________-------------1------...........................,................................... I I I , Time Period 1 I 2 I 3 I 4 Fig. 3. A causal network of components of states associated with the levels of sales in the first four time periods when no data is collected and errors in components of it, . , are all independent. t = 1,2, components structure of the market is valid in the second time period, in of simplicity we shall assume a diagonal system matrix G with zeros in the diagonals to the system error terms. Given the third and fourth rows and independent that the same competitive it is easy to check that a valid causal network relating brand sale over the first two time periods time period 3 we is given by the corresponding [E, F} and {G, H] have assumed to compete against one another. They are therefore predicted to become dependent and this is represented in the subgraph associated with the third period. Within time period 4, brand H is withdrawn which gives rise to the adjusted pattern over this period. The full causal network of this process is given in Fig. 3. that several advertising companies have induced brands subgraph of the graph of Fig. 3. During There are various issues highlighted by this example. First, even in small problems increases the one above, the number of variables in time so that the necessary probability like I 04 J.Q. Smith, K.N. Pupumic~huil /Art@ciul Intelligence 107 (1999) 99-124 involved. However, can become more and more if we have a Markov manipulations into the future then the situation can be structure and we only need to make predictions over the current retrieved. This is because, by storing only the probability distributions to date, we have all the information we need from the states given all the data collected if our current data of a past to enable us to estimate subset of the system’s variables can make valid predictions the to remember causal network and its associated probability distribution. A description of how this can be achieved is given later in the paper. into the future. Thus, in this context it is sufficient distribution to calculate it is possible any probability to check directly from the d-separation for sales in the second period given this information Let us return to our example. For time period 1, the causal network of Fig. 2 is valid. By together with the observation and system equations over future events. Now using a distribution over these variables above suppose we observe sales figures about A. B, C, D, E + F, G and H with independent it et al. [3 l] and later in the paper) errors. Using well known results (see Spiegelhalter over these variables while the causal is easy to show that we obtain a new distribution theorem network remains valid. It is also possible follows 19,171 that time distribution and system equations we are able to the same network. Again by using the observation over this network. The predict all probabilities data from the second period consists of observations with independent measurement errors of A. B, C + D. E, F, G and H. It can be shown that since we do not observe C or D is introduced between separately but only obtain a noisy estimate of their sum a dependence C and D and this dependence needs to be carried forward into the time period 3. This is in Section 5 of this paper. The dependence between E, a special case of results discussed is carried through on to the other part of this F. G and H induced by advertising activity error in period 3 is on A, B, marginal network. The data with independent measurement C, D. E + F and G; product H having been withdrawn. A valid network for the variables in time 4 can now be shown to be the one given in Fig. 4. about future events from the distribution Because of the Markov assumption about future sales in our example depends on our data only as it has affected the distribution of current states; not past ones. on current states. Past states will is our distribution It follows only be important them our probability propagation algorithms can be made significantly quicker. that all we need to retain all probabilities if by retaining In this paper we investigate how to efficiently code the joint distribution of the subvector , 0,) which in a particular application, will be sufficient for the predictive this vector &- was just 6,. In the environment @r of (ti) ,02. requirements of the model. In our example example given in Section 3 we will illustrate how it may be more complicated More pIWiSely, than this. $T : (a) will be a high dimensional vector; (b) will be a vector whose length changes with the time index. In practice “effects” are often observed before “causes” in a causal network which makes round the raw causal network an unsuitable framework the system. It is helpful to define a framework for absorption of information which is non- directional. When a causal network is not dynamic probability propagation network as follows. the most common such framework is derived for propagating new information for from a causal tree [I .9,10,13] which is the ,junction J.Q. Smith, K.N. Papamichail /Artijicial Intelligence 107 (1999) 99-124 105 0 A A A B A A B D D C C E E F 8 G H 8 , Time Period F 8 G H 8 2 C D A A B G &” E F 3 4 Fig. 4. Margins of states where the distinction of WY is chosen to preserve the shape of the causal network given all data collected strictly before the time period of brand sales of aggregates given in the text plus some independent errors. index. The data collected is assumed to be measurements in time unconnected that no cycles the set of parents parents of each node are created producing First, new directed edges joining ensure is repeated until in the causal a new causal network. This network in each node of the causal network are process In the causal network joined by an edge. The resulting graph is called decomposable. (B[2], A[3]), of Fig. 3 the directed edges which need in each of the sets 5[i] = (B[3], A[4]), and between all nodes not already connected (E[i], F[i], G[i], H[i], E[i + 11, F[i + 11, G[i + 11, H[i + l]), the set i = 1,2, and Z[3] = (E[3], F[3], G[3], H[3], 1941, F[4], G[4]). Note here that many edges need to be added. On the other hand each of the four causal networks of Fig. 4 used in our predictive dynamic probability and so do not need propagation have all parents already connected any such modification. to be added are: (B[l], A[2]), The cliques of a decomposable graph are defined to be its maximally connected subsets. the causal network of Fig. 2 has 5 cliques (A, B), (I?, C), (B, D), (15, F) and they are (A, B), (B, C, II), (E, F, G, H). The cliques causal network can always be ordered so mat they intersection property [ 14,321, i.e., for all i, 2 < i < m, there is an index For example, (G, H). In the third causal network c[i], 1 < i < m, of a decomposable satisfy, the running r(i), 1 < r(i) < i such that: i-l if d[i] = c[i] f~ U c[j] then d[i] c c[i] ~1 c[~(i)]. j=l 106 J.Q. Smith, K.N. Puprrmichail /Art$cial Intelligence 107 (1999) 99-124 forest of a causal network has as nodes Henceforth, we shall label the vector of variables contained in a clique c[i] by #[iI, the cliques of that 1 < i < rn. A junction network numbered consistently with the running intersection property and with clique c[ j] connected by an undirected edge to clique c[i], j < i, if and only if ,j = r(i) and d[i] is not empty, where r(i) and d[i] are defined above. A junction tree is just a connected subgraph of a junction forest. Here we will call d[i] the separator of the cliques c[i] and c[r(i)] and denote by $1 ]iJ the vector of variables it contains. It is straightforward to check that ~[i]~c[l],....c[nl](d[i]. 2<,i <rn. (2.6) [i])] (q;(@l denote the marginal densities of $[i], (41 [i]} respectively where, Let pi(~$[i]), if d[i] is empty or i = 1, qi = 1, 1 < i < m. Then because of the conditional independence relations given above it can be shown that the joint density p(8) of the original variables can be written where p is the marginal density of the subvector ~$1 i] of c[i] and q is the marginal density of the components of 41 [i] in d[i] defined above (or as equal to 1 if d[i] is empty). Note that two causal networks with the same undirected version will give rise to the same breakdown given above and so in this sense the decomposition the variables. is not dependent on the causal order of (2.7) literature in principle and second the Gaussian In the current into the system. since the propagation for driving algorithms there are computational The fact that the junction tree has a structure which setting although family is not dependent on the causal for the quick it is usually assumed then update the usually in the light of this [9,33]. Here we are in a first that the distribution ordering of the system makes it ideal as a framework absorption of information that a subset of states are observed without error. The algorithms discrete densities of the remaining states of states is typically slightly different setting for two reasons: that we observe only a function of states with error. The first continuous algorithms are equally difference causes no problems involved once we valid in a continuous is much more drift outside above may no longer be invalid. significant because is a in the particular case when errors are independent However, to show that this not so. To function only of states in a single clique it is straightforward that show this we just create one new clique c*[i] for each observation y[i] which contains together with those states of which it is a function. Since c*[ j] only contains observable in our observation states in a single clique c[ j] (say) the conditional for each y[i] in equation tree J, then J* a new junction (e.g., [9]) used on J* instead of J is also valid. The conventional propagation together with now apply directly. Once the propagation has been completed, their connecting edges, in J* and not J can be removed without loss since they contain no unknown quantities not contained a more detailed discussion of this procedure see [5]. tells us that if we connect c*[j] by an undirected edge to c[j] the original (see, e.g., [ 131). The second difference algorithm described tree J* whose edges between states follows in other cliques once the observations and each observation independence the learning are seen-for the cliques algorithms implicit issues J.Q. Smith, K.N. Papamichail /Artificial 0 E,F G,H 1 0 Time Period 0 E,F GH 0 2 Intelligence 107 (1999) 99-124 107 AB x B,CD A,B x B,C,D 0 E,F,G 4 W-SW 0 3 Fig. 5. Marginal junction forests associated with the causal networks of Fig. 4. that an algorithm Henceforth, we will assume to accommodate example all information the sequence of valid junction employed marketing networks tree has been adapted so that an observation of C + D has states which all lie in the same clique. Formal procedures to the tree of this type are given in the body of the paper. above will be In our to the causal that in time period 3 the upper junction like the one described in a given for making efficient adjustments forests corresponding in Fig. 4 are given in Fig. 5. Notice time period. collected (called “absorb” by Jensen) tree J* derived from the original is extremely simple. Thus, consider In many examples both the states and the observation will be Gaussian. In this case in the light of an observation Y and the updating of the marginal densities of the cliques the subsequent adaptation tree J above. The joint density adaptation on the junction (see [16] for the appropriate equations). of the states in c*[j] given Y will be Gaussian in J*. The distribution of cliques These states will form the separator of c*[j] in the junction algorithm the following tree containing sequentially. Once the distribution on all the cliques has been updated the clique c*[j] can tree J as discussed above. be removed without forest will remain The distribution of variables unchanged. in the cliques of other trees in the junction c[j] will then be updated using loss to give us back the original from c[j] junction in c[i] be @[i] = (41 [i], &[i]) into this clique let pj = E(4j[i]), Let the vector of random variables and before Y is j = 1,2, and Cl2 = accommodated is the vector of the newly updated separator. Then ~7 = Cov(r$t[i], &[i]), where &[i] E(q+ [i] 1 Y) is the updated mean of this separator and _ZF, = Var(& [i] ) Y) is its updated covariance matrix which can be read directly from the adjacent updated clique. Since the distribution of &[i] 1 41 [i] remains fixed under the updating, the usual multivariate normal JTjj = Var(#j[i]), 10X J.Q. Smith, K.N. Pupmichail /Artijiciul Intelligence 107 (1999) 99-124 theory (see, e.g., [ 161) now gives us that ,Y; = E(42[i] 1 Y), CLT2 = Var(&]i] I Y) are given by the equations: CT2 = Cov(& [il. &[i] ( Y) and 4=~~2+A(&p,). C;, = AC;,. c;2 = X22 - A[&, - C;,]A’. in detail in 1291. Finally, to new cliques we need to calculate to use these the distribution of the in c[i]. These will be subvectors of c[i]. So in the Gaussian case from the new mean vector continuous to pass information .X21 Cfi’ These results are discussed where A = equations new separators contained this step is trivial: we just need to pick the correct components in non-Gaussian and covariance matrix of ~$[i]. Interestingly, there are appropriate properties on the clique margins-as hyper Wishart families, consuming the discrete case since the analogous operation the updating step analogous multiplying for example, see [3]-this marginalising exists for hyper Dirichlet and step may be very time the calculation of an integral. Of course no such problem exists in is simply a summation. On the other hand the algebra of to the one given above, is often simple-being two functions cases-unless involving together. 3. States and the efficient representation of dynamic graphs structure of a probabilistic There are usually many representations in terms of a graph and it is critical that the graphical representation which guides the probability propagation is most at each time point does so as efficiently as possible. Which graphical representation the predictive computationally and the capability technological method which is currently available In this paper like the ones cited in the last we will assume section to process the data. is determined by many considerations the type timing and quantity of relevant updating that we will use a junction to process this information. tree algorithm information including required, efficient To achieve this efficiency necessary to define the appropriate joint distribution random vector at time statements t contains and also to make a representation more sequence of random vectors, indexed transparent it is in time, whose of a can be represented on a junction all information tree, such that the distribution to calculate required the probability that might be required for future predictions after time. Thus we need: tree which stores the distribution of each state vector at any time t. (a) A sequence of state vectors, indexed by time. (b) A junction (c) Algorithms which adjust this distribution-i.e., associated distributions on clique margins-in point t. One such algorithm was described (d) An algorithm which takes the cliques, junction adjust the cliques junction tree and the light of incoming data at that time in the previous section. clique margins on the state vector at time t - 1 and transforms cliques, a junction t=1,2,3 on these to a class of tree and new associated distributions on clique margins at time t, ,.... This process was illustrated tree and associated distributions in our example given above. J.Q. Smith, K.N. Papamichail /Artijicial Intelligence 107 (1999) 99-124 109 Sometimes exact algorithms give rise to distributions on states whose joint distributions have no compact graphical representation, i.e., they cannot be represented by a sparse graph whose associated cliques have a small number of states. In this case it is often expedient this to substitute for slow exact ones. Before we formalise procedure we give another example, this time an environmental model where the relevant prediction trees evolve in time in a much more structured way, fast approximate algorithms junction Example 3.1. Pentificating Puffs from a steady release. radioactivity it to fragment the spread of gaseous waste after an accident algorithms a mass of dangerous through [28] using (see [29] for more details). Briefly, after a nuclear are emitted from a building. the atmosphere by a wind field and as it is transported larger and the mass it contains becomes more spread out. Once a puff grows the dispersion model sectors of the large puff can be of This is a process for predicting junction tree propagation accident puffs containing Each puff is transported it becomes to a certain size at a time determined by the atmospheric conditions, allows transported fragments will eventually occur and so on. Each fragment but deterministic associated with the release. Readings Y(t, s) of air concentration taken periodically. Physics mass fragments which exist at time t and have part of their puff over the site s-larger contributing error. is determined by a complicated covariates at time t and site s are of puffs is a function of many known environmental into five pieces so that different less of their mass to this reading tells us that these concentrations by the wind field depending together with an independent are a linear combination on where each sector lies. Fragmentation formula which observational is Markov This process, defined by physicists, the of all future of future mass joint distribution observations-can be expressed as a function of the joint distribution of mass fragments which exist at time T. It follows that one valid state space at time T, which will be sufficient to provide any distribution over observations and states at that time, will be the vector of mass fragments/puffs that at time T the distribution in hence fragments-and the sense existing at or before time T. for large T this is typically an extremely Unfortunately long vector. However, we show are linear of puff masses which are fragments of the same “parent” puff, the joint in [29] that, provided puffs are released as a Markov process and all observations combinations distribution on this vector can be stored on a junction forest whose cliques are either: (a) masses of puffs adjacent in time emitted on or before time T (a random vector of length 2); or (b) masses of a “parent” puff and its associated fragments (often either 5 or a random vector of length 6). Cliques are linked by an edge in the junction forest if and only if they have a component is always either a parent fragment or a in common in their random vector. This component this is a very efficient way puff emitted at source. Because cliques are of small dimension, on states, An example of such a tree is given in Fig. 6. If of storing the joint distributions emissions are independent in the state vector and the forest, instead of being a single tree, becomes a forest with one tree for each puff emitted at or before time T. at the source then no puffs of the form (i) need be included 110 J.Q. Smith, K.N. Pupmichail /Artificial Intelligence 107 (1999) 99-124 Fig. 6. A Puff model junction tree. . , c(6) In Fig. 6, for simplicity, puffs are depicted splitting adjacent pairs. Other cliques are cliques associated with the first 7 emissions-containing in the order they appeared. Cliques c(7), c(8) and c(9) are parents and are numbered fragments of previously undivided puffs. Cliques c( 10). c(1 l), c( 12) and ~(13) are parents and fragments of fragments and so on. into 2 rather than 5. c(l), information Now suppose at time T we take an observation which is a linear combination, with error, of puff fragments which are all components of a single clique vector given above. Then the method for accommodating in Section 2 in a Gaussian network outlined tree of the state vector remain unchanged and the joint is valid, the cliques and junction distributions if an for which this is not so then, posterior observation Y is a linear combination of components to observing Y, dependencies forest will be junction induced. In this case the forest will need to be modified before data can be accommodated. In Section 5 we describe a way of doing this. in the cliques are updated as we described. However, implicitly denied by the original on the vectors it is important In this application to note that the system need only determine what of and the source emissions. Therefore, only a subset of states in source masses and the transmit tree can be happened at the source and predict future local contamination-a future mass fragments the process are essential mass fragments terminal information automatically cliques. for determining these distributions-the to the forest. The other states are used only to efficiently speeded up by simplifying or removing completely some of these transmitter around the forest. In later sections we will discuss how a junction linear combination 4. A formalisation A state vector $r at time T is a random vector whose distribution at time T given time T is sufficient, when used together with the data D received up to and including distributional information known a priori, to determine: (a) The expected utility of the optimal policies for any utility function which might be called at time T. J.Q. Smith, K.N. Papamichail /Artificial Intelligence 107 (1999) 99-124 111 (b) The expected utility of the optimal policies for any utility function U which might be called at time T’ > T as a function of covariates whose values are certain to be known at time T1 . We saw in the previous section representations that the appropriateness be answered by the system-i.e., example, for the dynamic the state vector needs @r = ]6, time T then it was sufficient allowed much better computational of a state vector depended heavily on the questions needing that a state vector is not uniquely defined but that some than others. We also saw to the utilities under which policies might be chosen. For . . . ) efficiency linear model of Section 2 in order to estimate { 13, : t = 1,2,3, . . . t Or}. On the other hand, to store past distributions of I!$ in some way-eg., asked only involved to let the state vector at time T be 0~ itself. if questions by setting the future after A state vector @T is not necessarily minimally sufficient for determining in (a) and (b) since some of the components of &- may be included utilities quick propagation using be useful later to write @T = (&- (1)) (PT (2)), where 4~ (l)-called minimal set of components of 4~ which on their own satisfy requirements where the rest @T(2) will be called transmission the L & S algorithm and Spiegelhalter (Lauritzen states. expected to facilitate [14]). It will a (a) and (b) and essential states-are A state process (t - 1, t], which makes &: is a time sequence of state vectors. Note that there is always a model which sets states r3, = (0,, Y,), t = 2,3, . . , where Yt are all the random variables observed t = 1,2, _ . . , a state process although such in the time interval a representation will usually be inefficient. Each state vector at time T in a state process tree, will have associated with it a triple ( TT, CT, ST) consisting of, respectively, a junction the edges a set of cliques (labeling the nodes of the tree) and a set of separators of the tree). It will also have a set of marginal probability distributions a tree (T,, Cl, St, P,; t = 1,2,3,. set PT = {pi(.): c[i] E CT}. Henceforthcall tree state JT at process associated with the state process (&: time T iS Simply &- = (Tr, CT, &-, PT), T = 1, 2, 3, . , . A transmitter clique is a clique it is called an essential clique. whose components . . . then their in Pl lie in a parameterised storage will usually be in terms of the values of these parameters. For example, in the case of a Gaussian process, pi (.) E PI can be stored in terms of their mean vectors pi and covariance matrices Zi . (labeling on cliques-i.e., . .} ajunction When all the densities states, otherwise . . .}. A junction are entirely t = 1,2,3, t = 1,2,3, transmitter family it is convenient specifies interval corresponding tree equation, transformations (1) Transformations Because of the Markov structure of the state process at each time point t = 1,2,3, to specify a junction are a composite of one of three different the junction (t - 1, t]. Usually tree state Jt as a function of Jt_l and any observations . . . i.e., an equation which, for each t, in the time types: In Example 3.1, two state equations. of this kind were described: we will use one of these types of transformation is emitted at for illustration. When a new puff of mass Qr so that Cr = CT-~ U {c}. The time T, it forms a new clique c = (QT, QT-1) in CT-I. Set previous puff mass QT_I tree TT-~ by adding ST = Sr_r U { Qr_1). The tree TT is formed from the junction a node labeled c and an edge joining c to c’ labeled QT-r _ The set of densities PT just adds the density of the clique c to PT. The clique margin of c is calculated by multiplying lies in the clique c’ = (QT-1, Qr-2) from the clique margin c’-by the density of Qr_l-calculated from the chimney to descriptive the I I2 J.Q. Smith, K.N. Prrpamichail /Artificial Intelligence 107 (1999) 99-124 conditional transformation, transmission become completely states at time t. Indeed redundant. density of QT ( Qr_1, states which were essential at time the latter being known a priori. After such a just in Section 6 we will illustrate how they can t - 1 may well become (2) Transformations induced by the arrival of observations. In Section 2 we cited a tree process in the light of an observation which was method of updating a junction a function of states all lying in a single clique. This transformation was particularly simple because to a new set of densities PT in the light of the observation. Again some states in TT _ 1 redundant at time T-for surrogate for data that is now observed. (3) Transformations which redefine a junction it set (TT, CT, ST) = (TT-1, CT-~, &-_I) and changed only PT_~ this can make example, when such states acted as a feasible. Exact versions of these are discussed tree state to make it more compact and so in Section 6 make quick algorithms and approximate methods described in Section 7. Because transformations in this paper we will concentrate our attention on the second two. Examples of the first type are discussed of the type (1) tend to be very specific to an application, in detail by Gargoum and Smith [6] and Gargoum [5]. 5. Transforming junction trees to assimilate data (c[ 11, c[2], In Sections 1 and 2 we gave examples where, because an observation was a function of for In this section we describe an algorithm forest structure algorithms. Such a forest will states in different cliques probability propagation were no longer applicable. which modifies can legitimately contain a clique with all the variables tree state so that the new clique and junction tree propagation in D defined above. . . , c[r]) = D the usual junction use the usual junction tree algorithms the junction Let h[b, c; T] denote a path between nodes b and c in C of (T, C, S, P) such that then h[b, c; T] has no repeated nodes. Note that if h[b, c: T] exists it is unique because forest of the period 2 of Fig. 5, both b and c lie in the same tree of the forest. In the junction we observe the aggregate sales of C + D, with error, and we note that in the junction forest there is no one clique which contains brands C and D. However, there is a path between the two cliques b = (B, C) and c = (B, D} with no repeats-namely the edge joining h to c. c r ., [ 1 in C denote Let T(c[ll,c[21,. . .,c[rl), L.[II, CM,. each of the paths h[c[i],c[j]; the smallest T], 1 < i, j < r. Let Tj(c[l],c[2], trees which comprise subforest . . .c[r]), containing 1 6 j 6 k, (or briefly Tj) denote the forest the k disconnected T(c[l], c[2], . . ., c[r]) and B[j] denote the nodes of Tj, 1 < j < k. Let D[j] = D n B[j] so that D[j] are disjoint, 1 < j < k, and their union is D. We now further partition B[j] into C[i, j], where C[i, j] = (b E B[j]: min,,pj h(h[b, C; Tj]) = i), i = 1,. . . , lj, where h(h) is the number of nodes between b and c in the path h and lj is the maximum distance between a node in B[j] and those in D[j]. Let C[i, _il = iJc(i)C[i, j] c(i), 1 < i 6 lj, 1 6 j < k, and let C#[j] = {C[i. j]: 1 < i < f!j}. c# = u C#[j]. I<j<k J.Q. Smith, K.N. Papamichail /Artijcial Intelligence 107 (1999) 99-124 113 Let T’ be a forest whose nodes are C’ = (C \ D} U C’ U {d}, where d = Uicl c[i] such that: (i) (ii) (iii) (iv) Two nodes b, c in C \ D are connected by an edge in T’ iff they are connected by an edge in T. A node b E C \ D is connected which b was connected in T. The node c[i, j] f C# is connected ori + 1. The node d is connected l<j<k. to a node b E C’ by an edge iff b = c[ 1, j] for some j, to a node c[i, j] E C’ iff C[i, j] contains a node to to the node c[i’, j’] E C# iff j = j’ and i’ = i - 1 It is straightforward to check that T’ is a forest. Since both the cliques C’ and the separators S’ of T’ are defined above, it remains distributions P’. to define the clique marginal probability Each clique in C \ D is given the same margin in C’ as it had in C. The margin of c[i, j] the joint density of states in Tj in (T, C, S, P). from taking the product of the margins c[O, j], this construction appropriately is calculated by marginalising Finally the clique d has margin obtainable 1 < j < k, calculated above. It is straightforward on the junction of this figure. forest T of time period 2 of Fig. 5 gives the junction to check that performing forest T’ of the period 3 Now we have the following theorem. Theorem 5.1. If the junction tree state (T, C, S, P) is valid then so is the junction tree state (T’, C’, S’, P’) dejined above. Proof. This is a direct consequence dix A. q of Pearl’s [17] d-separation Theorem-see Appen- Having transformed T to T’ we note that all the observed variables clique d. It is now valid to use the updating algorithm described the new marginal probabilities on the states of P’. lie in the one in Section 2 to recalculate Note that the joint density p(j) (r#~) of the variables lying in the cliques in B [ j] is given by formula (2.7) so where pi (4) is the margin of c[i], qiij (4) is the density of the separator s[i, i’]-associated with an edge in Tj linking c[i] and c[i’] where Z[ j] is the set of all separators in Tj . (5.1) In the special case when all variables the mean in cliques and covariance matrix of these variables are easily calculated. The mean vector can be read directly from the means of the densities pi (4). The covariance matrix can be obtained by first numbering graph whose undirected version would be Tj with cliques numbered compatibly would be valid. Now the covariance matrix is constructed regularly: i.e., so that a junction inductively using this numbering. in B[j] are jointly Gaussian, the cliques in B[j] Thus let +( 1) denote all variables whose covariance matrix has already been calculated in a single clique) and let 4 (2) = (#l(2), 4 (s)) be a vector of variables (at first the variables 114 J.Q. Smith, K.N. Pupumichail /Artificial Intelligence 107 (IYYY) 99-124 in a clique adjacent where $( 1) = ($1 (l), 4(s)) so that 4(s) r$( 1) and G(2), and $1 (j) is the residual of $(.j), is given by C[j ]: in T/ to those already having their joint covariance matrix calculated, to both ,j = 1,2. If the covariance matrix of 4(j) is the random vector of variables common WI = c Clljl h;(s) hj(.s) C[.s] 1 - where Cl (j) is the covariance matrix of 41 (j) and C[s] is the covariance matrix of 4(s), then the covariance matrix of @*( 1) = (41 (I ). @l(2), @l(3)) is given by Cilll h(1.2) I;I(S) c*[l]= r;T(1,2) Cl[2] h2(s) . i q-w q(s) C(S] 1 and Aj = hj (s)E-’ theory (e.g.. [16]). This completes where A( I, 2) = A 1 C(s)AT (s), j = 1,2, using standard multivariate normal step since we now replace 4(l) by 4*( 1) and repeat the procedure, and keep doing this until the covariance matrix of all It is now a trivial matter to in B[j] have been calculated. variables read the mean and covariance matrices of c[i. j] from the mean and covariance matrix of the vector calculated above-and hence to calculate all the new densities in cliques contained the inductive lying in P’. In the non-Gaussian case the calculation of the density of c[i, j] may need an integration or a summation which will slow the process down dramatically. However, with some loss of efficiency we can replace T’ with a different junction tree having nodes which replace C’ with (c’) where Substituting tion. {c’) for C’ in the constructions (i)-(iv) gives a valid junction tree representa- Finally note that the use of (5.1) without algorithm described the clique margins-but to work quickly and so can be seen as a generalisation in Section 2, the margins qiil(4) can be retrieved that, as for the updating in closed form from the algorithm defined in this section requires no further conditions integration requires of the one given in Section 2. We finish this section with an example. Example 5.1. Deriving a new junction tree for probability propagation. those components You take an observation U, whose distribution depends explicitly on the state vector & of & which lie in cliques D = (~131, c[7], c[12], c[15]. only through lying in c[23], c[24]]. To draw a forest which is both valid and has all these components the trees T1 and T2 which are defined as the union of all a single clique d, first construct paths between in D in T, the tree T given in Fig. 7. These two subtrees are given in Fig. 8. Using Eq. (5.1) we therefore see that the joint densities p(j)(.) of states in Tj . ,j = 1,2, are given by the cliques J.Q. Smith, K.N. Papamichail /ArtiJicial Intelligence 107 (1999) 99-124 115 1 Fig. 7. A junction is observed. c[24]] forest before a function of variables in the set of cliques D = (c[3], c[7], c[lZ], c[15], ~(231, c[W c[16] c[18]c[19]c[20] CL11 d41 d91 4121 Z a1 X g-g-@- CI ci 171 131 0 0 Tl c[131 x 1141 X Z A ~~231 0 m41 i 23 [151 0 subforest in each tree which have the same marking-i.e., [Tl, i-21 with D[l] = {CD], c[7], ~$121, c[15]] and 0[2] = (c[23], c[24]]. We from the set of are the same distance Fig. 8. The constructed now combine cliques observed nodes. and P(2) (.) = P21 (‘)P23 (.)P24(.) q21,23(.)q21,24(.) . From these joint densities the clique margins of the newly created cliques c[O, 11, c[l, 11, c[2, 11, c[O, 21, c[l, 21 (see Fig. 10). The algorithm now reconstructs the tree as depicted it is possible to calculate in Fig. 9. Interestingly it is easy to check that c[2, I] and c[l, 21 can be calculated avoiding an step. Thus, the clique margin of c[ 1,2] is just the clique margin of c[21] and integration the clique margin of c[O, 21 is given by P21 (.)P24(.) q21.23(.)q21,24(.)’ 116 J.Q. Smith, K.N. Papamichail /Artijiciul Intelligence 107 (1999) 99-124 c[ 18~c[19]@0] Fig. 9. The reconstruction of T to 7“. completed. Here d = c[O. 11 U cjO.21 m21 c[1,21 o---o of (TI , r,) ready for the construction of 7”. L.[O, I] = c[3] u C[7] u c[12] u C[l5]. Fig. 10. The transformation c[l, I] =c[21Uc[6] Uc[13]U~[141. ~12. I] =c[l]Uc[41. cfO.21 =c[23]uc[24], C-[1.2] =c[21]. The density of the separator of c[O, 21 and c] 1.21 which is the union of the separators of c[21] and c[23] and c[21] and c[24] in the original junction tree is given by q21,23(.)q21,24(.) r21 (.I ’ lying in both separators, and equal to one if this where r2t (.) is the density of variables set is empty. However, this is not so for B[l], so if states in this tree were not Gaussian it may be necessary to place all these nodes in a single clique. It has been our experience in the nuclear coding that in fact this construction small-indeed or cliques from different combines cliques on trees ?“i which are for many parameter settings we just combine cliques with the same parent trees in the forest. 6. Other exact algorithms 6. I. Splitting up trees in Section 2 tends It will have been noted that if a forest T is split up into many trees then the propagation to work more quickly. The Chop algorithm algorithm described described below time T to another state J* = (T*. C*, S*. P*) whose forest has more trees. It can be applied whenever a separator s E S becomes degenerate-for example, by being observed directly and without error. If a separator ,512 E S between cliques ct and cz E C becomes degenerate tree state J = (T, C, S, P) at a given transforms a junction and c2 are now independent of one another. The following is therefore valid. transformation then clearly et of (T, C, S, P) J.Q. Smith, K.N. Papamichd /Artijicial Intelligence 107 (1999) 99-124 117 The Chop transformation sets S* = S\I&2), c* = (C\kL c21) u (CT, CT), where cr has as its components T* is the forest with tree Tt2 E T containing containing ~2. the new clique and edge set C* and S*, respectively-so the components of ci less the components of ~12, i = 1,2. the cl and the other ~12 is replaced by two trees-one containing inevitably As time passes the number of cliques multiplies the states of the process are defined carefully. This can be done, to some extent, automatically. The next transmitter cliques. It is in fact just a version of Shachter’s transformation for algorithm [23] Barren node reduction junction trees in a dynamic setting. diagrams but stated loses irrelevant for decision influence unless takes the junction The Chop transformation tree states J = (T, C, S, P) to the state J* = (T*, C*, S*, P*) where c* E C* < C, iff c* is an essential clique or if c* labels a node in T which two essential cliques of T. Separator two essential cliques in (T, C, S, P). s* E S* < S iff s* is an edge lying in a path between The forest T* is the subforest of T whose nodes are C* and edges S*, where C* and S* are defined above. For c* E C* < C we set p*(c) = p(c) where p(c) E P. lies on a path in T between Theorem 6.1. If J is a junction above. Proof. See Appendix B. q tree state of a process at time T then so is J* defined If c* E C” then its margin will just be that given is just the margin of the subvector of c in C containing its margin conditional density of ci given ~12, where i = 1 or 2 since ~12 has a degenerate distribution. [30] use this property for discrete problems, calling Implicitly Spiegelhalter it the global for some continuous property. Queen and Smith [21] use it implicitly and Lauritzen independence time series. in P. If c* E C* is not in C then will be the it-which 6.2. Keeping transmitter cliques small It was seen in Section 5 that adjusting from data has a tendency information efficiency. A junction small as possible by replacing the transformation Thin takes the junction state J* = (T*, C*, S*, P*) as follows: tree transformation the transmitter junction to increase tree states to allow accommodation of loss of the clique size with a consequent call Thin keeps the size of transmitter cliques as clique by the union of its separators. Thus tree tree state J = (T, C, S, P) to the junction C* consists of the essential cliques of C and replaces each transmitter clique c E C by c*, where c* is the subvector of c consisting of the components of c which are contained in at least one of its separators+orresponding corresponding c* E C*\C to the node in T to c. Set T* = T and S* = S. If c* E C A C* its density is unchanged and if then the density of c* is the appropriate margin of p(c) where c E C contains to edges in T connecting 118 J.Q. Smith, K.N. Papamichail /Artificial Intelligence 107 (1999) 99-124 clique in C. As the process evolves the c* subvector. Notice then in the Gaussian case, the set of new clique margins is trivial to calculate, being given by the sub mean vector and appropriate sub covariance matrix of the containing it may happen that expediently even if they lie in a path of a junction J = (T, C, S, P) let n(c[j]) E C, called the neighbours of c[j] connected in T and let #s[i, j] denote the dimension of the vector of states s[i. j]. cliques may be removed two essential cliques. In in C, be the set of cliques in T by an edge. Let s[i, j] E S be the edge which connects c[i] to c[j] transmitter tree between to c[j] 6.3. Combining transmitter cliques The final transformation in this section is called Contract. It acts on two mitter cliques c[j] (T*, C*. S*, P*) where: and c[j’] E C which are adjacent in T. It takes C* = (C\{c]jl, ctj’l}) U (c*[j, j’l}. S* = (S\(s[j, j’l}) U (s*[j, j’l}. trans- to (T, C, S, P) to either c[ j] or c[ j’] in where s[ j, j’] denotes the set of all edges in S which are adjacent T and s*[j. j’] is a set of edges from nodes in C* (and C) which are adjacent to c[j] or c[ j’] in C connecting each such node to the new node c*[j, j’]. Let T* be the graph whose nodes and edge set is, respectively, C* and S* defined above. Let the density of c* E C’, to the density of c in P. Define the density of c*[j, j’] by c # c*[,j, j’], in P* be identical 13j(‘)Pjr(‘) q,j,j’(.) . where pj and pjl in P are the densities of c[,j] and c[ j’], respectively, and qj’ is the density of s[j, j’], their separator its mean and covariance matrix can be obtained using the formula (5.1). in T. Note that when the variables in c*[j, j’] are Gaussian, efficient The Contract transformation works with Thin to shorten the length of paths between essential cliques when it is computationally efficiency obviously depends quite critically both on the algorithms used by the system and the machine doing the computing. But larger cliques and trees with more edges will almost lead to a loss of efficiency. There are many candidates which might determine inevitably two cliques. One whether or not the Contract simple criterion, all have the same sample applicable space-as transformation if the state vector components they do in the Gaussian case-is should be enacted between to do this. Computational to Contract iff #(sli, jl) + C 4ilEn(cljl) C <lilEn(c[j’l) #(s[i. j’l) - 2#(s[j. j’l) 6 max C #(s[i, jl) + C #(s[i, j’l) . I clilen(c[jl) cLiJEn(c[j’l) 1 This criterion uses Contract conservatively: it is only employed when, on combining the subsequent use of Thin will make the new clique no bigger. cliques c[j] and c[j’], J.Q. Smith, K.N. Papamichail /Artificial Intelligence 107 (1999) 99-124 119 When all separators are of the same dimension have exactly two edges in T. it is employed iff both c[J and c[j’] each 7. Transformations which approximate quick, forest, are used to simplify of the distribution Even when exact there is to such an approximation must be techniques the forest gradually to keep computational junction If there inefficient. it then becomes necessary a dynamic becomes more and more algorithms that forest with ones which process information more quickly. are times when an imperative approximate an Obviously good approximation is but with the passage of time may give rise to a distribution is likely to arise. The inadequate. There are two ways to investigate whether this problem first, illustrated the efficacy of the approximation with typical data streams on which it might be used against an exact calculation. The second introduce a metric over the distributions on essential states of a given way is analytic-we time and use this to judge the approximation. A good metric to use for this purpose is the Hellinger metric, given by invoked with great care, because states may now look superficially of future states which later in this section, just checks of essential &(p, p’) = 1 (p”* - P”‘*J2T where p and p’ are, respectively, topologically separation measure equivalent a density and its approximating to the variation metric. An alternative, density. This is the Kullbach-Leibler [5]. in a junction The first important point is discussed by Gargoum is that all three measures of separation mentioned to notice it is easy to check that if p and p’ are densities above are “local”. Thus, for example, over variables J which differs only on a clique c, then do (p, p’) = do (p,, p:), where pC, p: are the marginal densities of c under density p and p’, respectively. Also any margin of variables under p or p’ will be no further apart than dH(pc, pi) (see, e.g., Smith [27] for a proof of these results). Thus, in particular, if a margin pC of c E C is approximated by p: but the process is otherwise then left unaltered we can guarantee tree process that where pC and p: are, respectively, induced by this substitution. give rise to only small changes the true and approximating It follows that small approximations in distributions over states. densities on essential states of clique margins will for which A legitimate states. Happily, concern would be that the repeated use of such approximations might, for problems which are truly is a significant true and approximate densities are expected with time, aggregate errors on essential stochastic-i.e., distances between with distributions, time. So in practice, provided observations to decrease geometrically their predictive Smith [27] for more details. to be used with Example 3.1. Here we will discuss just one: the most easy to generalise. The Cut such aggregation Gargoum and Smith [6] and Gargoum [5] discuss several such approximations tends not to be a problem-see are consistent with system error in the state equations- there 120 J.Q. Smith, K.N. Papamichuil /Artificial Intelligence 107 (1999) 99-124 transformation between the two cliques c[i] and c[j] E C as follows: of (T, C. S. P) to (T*. C*, S*, P*) deletes an edge s[i, j] in the forest T set S* = S\(s]i, ,j]). C* = (C\kl.a) u ((~*ljl]. j]=&[j],c*]j]=#2[j]andT*isdefinedbysettingC* wherecl,j]=(~l[j],#zlj]),s[i, and S* defined above as its node and edge set. If c E C* c # c*[j] Finally the density of c*[j] then p*(c) = p(c) E P. is simply the margin of @2[ j] obtained from the density of c[ j]. the density pj of c[j] by the product qjrj where qj is the density of 41 [,jJ and rj the density of &[j]. The Hellinger distance of this approximation will be small if @t [,j] and &[j] each other. are almost independent of all this transformation does is to replace Technically In the Gaussian case rl~ = (1 - det{ l/4(21 + K + Kp’)))“‘. where K= ‘I At I A2 [ 1 and I are the identity maps of the appropriate dimensions, A 1 is the regression matrix of and AZ the regression matrix of Cpz[ j] and Cpl [j]. So here we can clearly 41 [j] and &[j] see that L~H is close to zero when A 1 and A2 are both close to zero-i.e., when learning $1 [j] is expected to hardly change the mean vector of &[j] and vice versa. Note that, whatever the distribution on essential state, the use of the Cut transformation to be conservative-it therefore prohibits ignores certain learning about 41 [j] from &[j] and about in its estimation processes but information that it does this. In particular, its predictions-something to be avoided it is never expected in approximate to inflate its uncertainty learning systems-see is expected &[ j] from #I 1 j]-it acknowledges about Smith [25]. Fig. 11 gives the junction forests obtained by using the Cut transformation to than Forest 2 approximation a forest which was originally a single tree. Forest 1 approximation approximate a smaller Hellinger distance Tree 2 in Forest 1 has split into two degenerate cliques-Tree since it is a whilst clique c[5] has disappeared completely under the Lop transformation transmitter clique separated from the essential cliques by the action of Cut. The forests are modeling and in this context even when the process has run a long time and has many cliques the size of the trees with appropriate settings of the Hellinger distance, under Cut, they are typically of about this size. demands and so has fewer trees. Thus 5 and Tree 7 in Forest 2- the spread of nuclear contamination In this simulation we purposely let observed levels of contamination be much larger than forecasts by the model and so there is no mathematical processes should necessarily levels of contamination which might prompt action, approximate by more than 0.3 of the forecasts associated with the true model-an as far as determining remarkably remain close to the original one. However, for forecasts of forecasts never deviated insignificant deviation seems in many runs at different parameter settings of the model action was concerned. So the methodology reason why the approximating robust. In particular appropriate J.Q. Smith, K.N. Papamichail /Artijicial Intelligence 107 (1999) 99-124 121 Forest 1 Tree1 s[~~~7~ $ E Tree2 Tree 3 Tree 6 Tree7 -__-----____________-- Forest 2 s[S,15] Tree 1 s[4,21] I Tree 3 Tree 4 TIWS Tree 2 Tree 6 Tree 7 Tree 8 Fig. 11. Two forests where cut is administered at different levels of the Hellinger distance. approximation when outlying data is observed. error for forecast contamination does not appear to escalate with time even 8. Conclusions The techniques of dynamic probabilistic in this paper are not just an aid to fast computation. The automated husbandry of belief structures over variables of interest can be fed back to the user to show her how the system is using data to in particular are very useful in this regard. learn about the system. The graphical structures They show how dependencies are created and destroyed by the passage and independencies of time. learning of the type described The use of structural approximation, like the Cut, is particularly it mirrors human model by choosing there is little or no chance of it affecting our beliefs significantly learning. For in practice we always limit to disregard data, which might be observed, because we believe interesting because the scope of our statistical that and assuming variables to date strongly are independent when our evidence Running with an appropriate diagnostic over the prediction model as an albeit complex null hypothesis-as Bayesian modeling-is belief. underpins any inferential a decision support system such dynamic systems are already providing stimuli problem solving space and treating a Bayesian in most current practical and, in our structure which allows for creativity. Certainly as part of to creative entirely consistent with taking such approximations in at least one application. is almost so. it is implicit that this suggests Acknowledgements We are grateful to our colleagues in the RODOS project for many discussions. This work is funded by the CEC (Contract: F14P-CT9S-0007) The views expressed those of the RODOS project. in this paper are those of the authors and do not necessarily reflect and EPSRC (Grant no.: GR/K72254). Appendix A. Proof of Theorem 5.1 Theorem 5.1. !f’ the ,junc‘tiorl .state (T’, C’. S’. P’) defined tree stde ( 7‘. C‘. S. P ) is valid then so is the ,junction tree in Srctim 5. independence Proof. Since T is a forest the running conditional graph It which has a compatible ordering of nodes that introduces then the nodes in B[2]. in C. then the nodes in B[k] and subsequently in T can be equivalently (<‘I) statements intersection coded by a junction first the nodes in B]l], the remaining nodes property (RIP) (321 ensures that the Since by construction T’ is also a forest, the RIP also tells us that its ci statements can be first the nodes in C’[ I], then . then the nodes in C’[k 1 and then the remaining nodes in C’ (which coded by a junction graph 12 which introduces equivalently the nodes in C’(2]. are also in C) in the same order as for I). By the d-separation Theorem diagrams/influence in 12 will be valid iff the CI statements II. There are only two such statements. The first is that [ IS, 17,181 since the junction graphs It and 12 are causal diagrams and are identical on their shared nodes all the CI statements . C’[k] can be deduced from in C’[ 11. implicit and this is implicit from 12 since C’] I 1. in different trees of the forest in T. The second class of statements concern , n. But the CI statements j = 1. in C’[j], cliques in B[j] are independent that sets of variables variables to the sets. This d-separation Theorem. So our result is proved. in cliques adjacent ;I _ . C’[k 1 all contain variates in cliques which lie the dependencies between variables in the in C’[j] are implied by the statement the values of all the of the of each other given is a direct and trivial consequence J.Q. Smith, K.N. Papamichail /ArtiJicial Intelligence 107 (1999) 99-124 123 Appendix B. Proof of Theorem 6.1 Theorem 6.1. If the junction tree .I* = (T*, C*, S*, P*) dejined in Section 6. tree state J = (T, C, S, P) is valid then so is the junction 1 < j;, containing T*(j) using the running T*[l], T*[2], T*[3], . . . , T*[k”], then k’ < k, T*(j*) trees T[ 11, T[2], T[3], . . . , T [k] and T* is a are subtrees to label the nodes in a tree intersection property starting with nodes in the Proof. First check that if T has k disconnected consists of k* disconnectedtrees subtreeof T(j), 1 < j* < k*, 1 < j <k, and no two trees T*(j;) of the same tree T(j), T(j) subtree T * (j) in a way compatible with a directed tree whose undirected version and each node has exactly one parent (except the root node which has none) 1 < j < k. so that the labeling in a way compatible with Continue a directed and each node has exactly one parent is T(j) (except the root node which has none 1 < j < k). in tree T(j) to label the nodes tree whose undirected version j; < k*. It is therefore possible and T*(ji) is T*(j) The junction graph G whose disconnected T[ 11, . . . , T[k] and are compatible with the order above are valid. Since all nodes the corresponding it follows by the d-separation Theorem undirected version clique margins versions in directed subforest T*[ 11, . . . , T*[j*], comprise an ancestor set in G graph of G whose valid. Since C* contains all the states of the process and the in C* agree with those in C, by construction, trees have undirected the result follows. [17] that G*-the subjunction is T*-is directed q References [I] R.G. Cowell, BAIES-A in: J.M. J.O. Berger, A.P. Dawid, A.F.M. Smith (Eds.), Bayesian Statistics 4, Clarendon Press, Oxford, probabilistic expert system shell with qualitative and quantitative learning, Bemardo, 1992, pp. 595600. [2] AI? Dawid, Applications Comput. 2 (1992) 25-36. of a general propagation algorithm for probabilistic expert systems, Statist. [3] A.P. Dawid, S.L. Lauritzen, Hyper Markov laws in the statistical analysis of decomposable graphical models. Ann. Statist. 21 (3) (1993) 1272-1317. [4] S. French, D.C. Ranyard, J.Q. Smith, Uncertainty in RODOS, Research Report 95.10, School of Computer Studies, University of Leeds, UK, 1995. [5] A.S. Gargoum, Issues in Bayesian Department, University of Warwick, UK, 1998. forecasting of dispersal after a nuclear accident, Ph.D. Thesis, Statistics [6] A.S. Gargoum, J.Q. Smith, Approximating dynamic Gaussian junction trees, Research Report 279, Statistics Department, University of Warwick, UK, 1994. forecasting [7] P.J. Harrison, CF. Stevens, Bayesian 205-247. (with discussion), J. Roy. Statist, Sot. Ser. B 38 (1976) [8] F. Jensen, F.V. Jensen, S.L. Dittmer, From influence diagrams to junction trees, in: Proceedings 10th Conference on Uncertainty in Artificial Intelligence, San Francisco, CA, 1994, pp. 367-373. [9] F.V. Jensen, An Introduction to Bayesian Networks, U.C.L. Press, London, 1996. [IO] F.V. Jensen, K.G. Olesen, S.K. Andersen, An algebra of Bayesian belief universes for knowledge-based systems, Networks 20 (1990) 637659. [I l] U. Kjazrulff, A computational scheme for reasoning in Artificial in dynamic probabilistic networks, Intelligence, Stanford, CA, 1992, pp. 121-129. in: Proceedings 8th of probabilities, means and variances in mixed graphical association models, Conference on Uncertainty [ 121 S.L. Lam&en, Propagation J. Amer. Statist. Assoc. 87 (1992) 1098-1108. [ 131 S-I.. LaurnLen. GraphIcal Mud&. Oxtord II-l] S.L. Lauritzen. D.J. Spiegelhalter, Local computations with probabilities on graphtcal structures and their Il~u\er\n> Pm\\. Oxford, 1996. application to expeli systems (with discussion). J. Roy. Statist. Sot. Ser. B SO (1988) 157-224. [ 151 S.I>. Lauritzen. A.P. Dawid, B.N. Larsen. H.G. Leimer, Independence properties of directed Markov tields. Networks 20 (1990) 49 I -SOS. [ Ihj K.V. Mardia. J.T. Kent, J.M. Bibby. Muluvariate Analy\~r. Academic Pre\\. London. 197Y. [ I71 J. Pearl. Probabdistic [ 181 J. Pearl, T.S. Verma. The logic of representing dependencies hy directed graphs. in: Proceedings 6th National in Intelligent Systems. Morgan Kaufmann, San Mateo, CA, 1988. Inference Conference on ArtitiGial Intelligence (AAAI-X7). Seattle. WA. 1987. pp. 374-37’). [ IY] C.M. Queen. Using a multi-regreGon Statistician 43 (I ) (1994) X7-9X dynamic model to forecast salch m a competitive market. The 120) C.M. Queen. Model elicitation m compctiti\e market\. 111: S. French, J.Q. Smith (Eda.). The Practice of Bayesian Analysis. Arnold, London. 1097. pp. 229-243. 121 I C.M. Queen. J.Q. Smith, Multi-regression dynamic models. J. Roy. Statist. Sot. Ser. B 55 (4) ( 1993) X49- X70. 1721 C.M. Queen. J.Q. Smith. D.M. Jame\. BaycGan lorccasts In markets with overlapping structures. Internat. J. Forecasting 10 (lYY4) 209-237. 1231 R.D. Shachter. Evaluating mtlucncc diagram\. m A P. Bau (Ed.). Reliability and Quality Control. Elsevier. North-Holland. Amsterdam. IYXh. pp. 32 I--344. 1241 J.Q. Smith. Influence diagrams for stati\tic;tl modellinp. Ann. Statist. 17 ( 1980) 654-672. 1251 J.Q. Smith, Discussion of”Lcarnmg in probabili%tlc systems” by J. Spiegelhalter and R.G. Coweli, in: J.M. Bernardo, J.O. Berger. A.P. Dawid. A.F.M. Smith (Ed\. \. Bayesian Statistics 4, Clarendon Press. Oxford. 1992. pp. 460363. 1261 J.Q. Smith, Handlmg multiple source\ 01 variation u\m@ intluence diagram\. European J. Oper. Res. X6 ( I995 1 I x9-200. 11-71 J.Q. Smith. BayeGan Appn)x~mat~on~ and the Hcllmgcr metric. J. Roy. Statl\t. Sot. Ser. B. to appear. 12x1 J.Q. Smith. S. French, Bnyesian updating ~,fatmo\phcric dtspcrsion models for u\e after an accidental release of radioactivity. The Statistician 42 (5) C IOO?) 5Ol~-5 I? 1201 J.Q. Smith. S. French. D.C. Rany;ird. An cfticicm graphtcal algorithm of gaseous waste aficr an accidental Belief Network\, Alfred Walker, relea\c. m: A. Gammerman IYYS, pp. 125-140. for updating c\timatcs of the dispersal (Ed.). Probabilistic Reasoning and Bayesian [30] D.J. Spiegelhalter. S.L Lauritzcn. Sequential updating of conditional probabilities trn dlrected graphical structures, Network\ 70 ( 1990) 57Y--60.5. [3 I I D.J. Spiegelhalter. A.P. Dawid. S.L Science X ( 1993) 2 I Y-246. I.auntzen. R.(i. Co\vcll. Bayesian analysis 111 expert systems. Statistical 1321 R.E. Tttrjan. M. Yannakakis, Simple lmcar time alpornhm< IO te\t chordality of graphs. test acyclicity ot hypergraphs and selectively reduce acyclic hypegraph\. SIAM J. Comput. I.3 (19X4) 566S7Y. 1331 A. Thomas. D.J. Spiegelhalter. W.R. Gilks. BUGS: A program to perform Bayesian inference using Gibb\ \ampling. in: J.M. Bernardo. J.O. Berger. A.P. Dawid. A.F.M. Smith (Eds.). Bayesian Statictick 4. Clarendon Pres\. Oxford, 1992. pp. 837-X42. 1.341 M. Weht. P.J. Harrison. Bayesian Forccn\ting and Dynamic Models. Springer. New York. 1906. 