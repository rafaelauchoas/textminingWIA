Artificial Intelligence 202 (2013) 29–51Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOn the conditional independence implication problem:A lattice-theoretic approachMathias Niepert a,∗, Marc Gyssens b, Bassem Sayrafi c, Dirk Van Gucht da Computer Science & Engineering, University of Washington, 185 Stevens Way, Seattle, WA 98195-2350, USAb School for Information Technology, Hasselt University and Transnational University of Limburg, Martelarenlaan 42, B-3500 Hasselt, Belgiumc Computer Science Department, Birzeit University, PO Box 14, Birzeit, West Bank, Palestined Computer Science Department, Indiana University, Lindley Hall 215, 150 S. Woodlawn Ave., Bloomington, IN 47405-7104, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 4 July 2011Received in revised form 4 June 2013Accepted 19 June 2013Available online 28 June 2013Keywords:Conditional independenceProbability and lattice theoryConditional independence is a crucial notion in the development of probabilistic systemswhich are successfully employed in areas such as computer vision, computational biology,and natural language processing. We introduce a lattice-theoretic framework that permitsthe study of the conditional independence (CI) implication problem relative to the class ofdiscrete probability measures. Semi-lattices are associated with CI statements and a finite,sound and complete inference system relative to semi-lattice inclusions is presented. Thissystem is shown to be (1) sound and complete for inferring general from saturated CIstatements and (2) complete for inferring general from general CI statements. We alsoshow that the general probabilistic CI implication problem can be reduced to that forelementary CI statements. The completeness of the inference system together with itslattice-theoretic characterization yields a criterion we can use to falsify instances of theprobabilistic CI implication problem as well as several heuristics that approximate thisfalsification criterion in polynomial time. We also propose a validation criterion based onrepresenting constraints and sets of constraints as sparse 0–1 vectors which encode theirsemi-lattices. The validation algorithm works by finding solutions to a linear programmingproblem involving these vectors and matrices. We provide experimental results for thisalgorithm and show that it is more efficient than related approaches.© 2013 Elsevier B.V. All rights reserved.1. IntroductionConditional independence is an important concept in artificial intelligence and machine learning. It plays a fundamen-tal role in working with probabilistic systems successfully employed in areas such as computer vision, speech recognition,computational biology, and robotics. Numerous real-world systems can be modeled by a probability distribution over aset of random variables. Unfortunately, reasoning over the full joint probability distribution is intractable for all but thesmallest number of cases. It is the very notion of conditional independence that facilitates the decomposition of joint proba-bility distributions into smaller parts which are then processed in sophisticated ways to compute a-posteriori probabilities.Bayesian and Markov networks are among the most commonly used probabilistic graphical models leveraging conditional in-dependencies to answer probabilistic queries and to learn probabilistic parameters more efficiently [1]. A deeper theoretical* Corresponding author.E-mail addresses: mniepert@cs.washington.edu (M. Niepert), marc.gyssens@uhasselt.be (M. Gyssens), bassem.sayrafi@gmail.com (B. Sayrafi),vgucht@cs.indiana.edu (D. Van Gucht).0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2013.06.00530M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51investigation of the mathematical and algorithmic properties of conditional independence is therefore central to the under-standing of probabilistic models [2,3].A deep theoretical understanding of conditional independence and the way it is leveraged in probabilistic graphical mod-els, however, also allows us to understand the shortcomings of said models. Indeed, not every joint probability distributioncan be decomposed according to a graphical structure without loss of information [4,5]. Finding ways of assessing the suit-ability of graphical models for the representation of a given distribution is therefore crucial. In particular, Studený [6] hasbrought this issue to the forefront, leading to an impressive body of work on algebraic representations of conditional inde-pendence structures, providing deep links to algebraic geometry [6,7], supermodular functions on sets, and novel algorithmsfor reasoning with conditional independencies [8,9].More motivation for this research is provided by the problem of knowledge elicitation in the field of reasoning under un-certainty [10,11]. Consider the problem of eliciting knowledge from several domain experts to model a probabilistic system.The resulting incomplete expert feedback might be a combination of some specific subjective probabilities, (conditional)independency and dependency information for the random variables under consideration, and conditional probabilities. Sta-tistical tests on different heterogeneous data sets may provide additional sources of evidence. Each piece of informationcan be interpreted as a constraint on the joint probability distribution to be modeled, and finding a suitable model as aconstraint satisfaction problem (CSP), and the approach to harness CSP solvers for instances of this and related problems iswell-known [12,13]. However, (conditional) independence and dependence statements pose a special problem, because theyoften introduce non-linear constraints resulting in unfeasible CSP instances. Therefore, a remaining challenge is to test forconsistency of the (conditional) independence and dependence information collected from different sources, which requiresan algorithm deciding the implication problem for CI statements [14].A central notion in the realm of reasoning about conditional independence is, therefore, the probabilistic conditionalindependence implication problem, that is, to decide whether a set of CI statements implies a single CI statement relative tosome class of discrete probability measures. While it remains open whether the implication problem for the class of alldiscrete probability measures is decidable, it is known that there exists no finite, sound and complete inference system [15].However, there exist finite sound inference systems that have attracted special interest. The most prominent one is thesemi-graphoid axiom system (Pearl [3]), which we refer to as System G in the present paper. One of the main contributionsof this work is to extend the semi-graphoids to a finite inference system which we refer to as System A which, althoughnot sound, is complete for the general probabilistic implication problem. In the way that the semi-graphoid inference rulesprovide a lower bound on what can be inferred, System A provides an upper bound. We demonstrate that, in the generalcase where the number of variables is not fixed and where no finite axiomatization exists, considering both lower and upperbounds provides deep insights into the implication problem and allows us to develop a novel algorithm for both validatingand rejecting implication problem instances.The techniques we use to obtain these results are made possible through the introduction of a lattice-theoretic framework.We associate semi-lattices of sets of variables with CI statements. Derivability of a single CI statement from a set of CI state-ments in System A is then characterized by the inclusion of the semi-lattice of the former in the union of the semi-latticesof the latter. We also use this framework to show that derivability in System A in the context of arbitrary CI statementscan be reduced to derivability in the context of elementary CI statements, that is, CI statements that express independencebetween two single variables given a third set of variables. This result has important ramifications from a practical point ofview because the use of elementary CI statements allows for a canonical representation of CI statements. We then introducethe additive implication problem for CI statements relative to certain classes of real-valued functions and specify propertiesof these classes that guarantee either soundness or completeness of A . Through the concept of multi-information functionsinduced by probability measures [6], we finally link the additive implication problem for this class of functions to themultiplication-based probabilistic CI implication problem. This allows us to show, for instance, that System A is sound andcomplete for the inference of arbitrary CI statements from sets of saturated CI statements relative to both the class of allbinary probability measures and the class of all discrete probability measures. Saturated CI statements by definition involveall variables under consideration.The combination of the lattice-inclusion techniques and the completeness of System A for the general probabilisticconditional independence problem allows us to derive criteria that can be used to falsify or validate instances of this impli-cation problem. We introduce an approximate logical implication algorithm which combines these falsification and validationcriteria. The validation algorithm is based on our results regarding the reduction of derivability in System A for general CIstatements to derivability for elementary CI statements. It represents a set of such elementary CI statements as a sparse 0–1matrix, and validates instances of the implication problem by solving linear programs with this matrix as constraint ma-trix. Thus, by only requiring the algorithm to decide the majority of the probabilistic conditional independence implicationproblems, we can leverage linear constraint solvers for our purposes. We present an experimental evaluation in which weinvestigate the fraction of instances of the implication problem that can be decided with the novel approach.We report the results of extensive experiments designed to assess the viability of our approach. We relate the exper-imental results for our falsification criteria to those obtained earlier from a racing algorithm introduced by Bouckaert andStudený [8]. The linear programming techniques used in the validation criteria were subsequently also used by Bouckaertet al. [9]. We also compare the experimental results for our validation criteria with their results. The results of our exper-iments show that our approximation algorithm works very effectively and compares favorably to the related work, oftenoutperforming it by several orders of magnitude.M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51312. Related workProbabilistic conditional independence is an important notion in several disciplines [2,3], forming the theoretical basisfor probabilistic graphical models in particular and efficient probabilistic inference in general. Numerous lines of researchhave been devoted to the study of its mathematical, logical, and algorithmic properties. The first axiomatic approach toconditional independence was introduced by Dawid [2]. A closely related inference system for probabilistic CI was developedby Pearl and Paz [16]. In both lines of work, a three-place relation was characterized using similar axiom systems. WhileDawid termed the relation a separoid (cf. Dawid [17]), Pearl and Paz termed it a graphoid. These relations were shownto surface in the context of probabilistic conditional independence and other notions in statistics and artificial intelligence.Shafer [18] provides a comprehensive survey of some of the advances in the use and investigation of probabilistic conditionalindependence up until the year 1996, published in a special issue of Annals of Mathematics and Artificial Intelligence [19–23].A line of work that has stimulated considerable interest in probabilistic conditional independence is Pearl’s book aboutprobabilistic expert systems [3]. A more recent overview of the principles and practice of graphical models can be found in,e.g., Koller and Friedman [1].A considerable amount of related work exists in the area of uncertainty in artificial intelligence. Studený’s work onstructural representations of conditional independence [6], for instance, is highly related to the present work. In fact, there isa close connection between the lattice-theoretic representation and Studený’s theory of imsets [6], an algebraic frameworkfor the study of supermodular functions on sets. Building upon Studený’s work, Hemmecke et al. [7] used, among othertools, integer programming to solve some open problems concerning imsets. Studený discussed the use of the maximizationproblem over the class of l-standardized supermodular functions which can be posed as a linear program [24]. A differentbut related linear programming formulation for a validation algorithm for the CI implication problem was introduced byNiepert et al. [25]. Independently, Bouckaert et al. [9] leveraged the theory of imsets [6] to assemble linear programs toverify various instances of the CI implication problem. Their approach is similar to ours, except that they do not use theseveral orders of magnitude more compact and, therefore, more efficient representation. We will compare the two linearprogramming formulations in the experimental section (Section 9).Similarly influential to the present work is the work of Geiger and Pearl [14] and Malvestuto [26] about the algorithmicand logical properties of conditional independence. In their work, probabilistic conditional independence is approached froma purely logical point of view and syntactic inference systems and their properties with respect to various classes of proba-bility measures are investigated. The authors also discuss possible algorithms for the probabilistic conditional independenceimplication problem and the computational complexity of some of the problems and also pose several open problems.This article is a substantial extension, in terms of both theoretical and experimental results, of previous conference papersby the same authors [25,27]. With this article we extend the theory to fully cover the case of elementary CI statements(Section 5) which serves as the basis for most of the results in Section 9. In addition to additional examples and a refinementof the theoretical results with respect to the additive implication problem (Section 6) and the significant generalization ofthe validation algorithm (Section 7), the experimental section (Section 9) was expanded with several additional experimentsnot included in previous publications. The construction of the constraint matrix of the validation algorithm is also a novelcontribution increasing the efficiency of the validation algorithm. The lattice-theoretic framework that is developed is acontinuation of previous work by three of the present authors and Purdom [28–32]. The theory is applicable in severaldifferent areas of computer science. Sayrafi’s work, for instance, focused primarily on the class of frequency functions overfinite relational databases [31]. The derived inference rules were used in frequent itemset mining algorithms as heuristicsto prune the search space [33].Matúš [34] and de Waal and van der Gaag [35,36] introduce stable independence as a notion to more compactly rep-resent information about conditional independence.1 To this end, conditional independence statements are partitioned ina stable and a non-stable part allowing for more efficient storage and retrieval of probabilistic conditional independencies.Stable independence was also studied in earlier work of some of the present authors [5]. Though not elaborated upon inthis paper, the theory we develop is also applicable to stable sets of CI statements.3. PreliminariesWe define conditional independence (CI) statements as a syntactic notion. Finite inference systems are frequently used todecide logical entailment of such statements at the syntactic level. In the context of CI statements, we consider System G ,the semi-graphoid axioms of Pearl [3] and System S of Malvestuto [26] and Geiger and Pearl [14]. We add to this a novelinference system, referred to as System A , with which we compare the existing ones on a purely syntactic level.With regard to set notation, we often write A B for the union A ∪ B, ab for the set {a, b}, and a for the singleton set {a}if no confusion is possible. Throughout the paper, S denotes a finite universe containing all sets under consideration. For aset A, we write A for S − A, the complement of A with respect to S.We begin by defining conditional independence (CI) statements.1 Two sets of variables are stably independent if they are independent relative to a set of variables C and remain independent relative to each supersetof C .32M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51Fig. 1. The inference rules of the semi-graphoid System G .Fig. 2. The inference rules of System S for saturated CI statements.Fig. 3. The inference rules of System A .Definition 1. A conditional independence (CI) statement is an expression I( A, B|C) where A, B, and C are pairwise disjointsubsets of S. If A BC = S, I( A, B|C) is saturated. If A = ∅, or B = ∅, or both, I( A, B|C) is trivial.In this paper, we consider three inference systems for CI statements. If I is such an inference system, C a set of CIstatements, and c a single CI statement, we denote by C (cid:5)I c that c is derivable from C under the inference rules ofSystem I . Theclosure C +I of C under I is the set {c | C (cid:5)I c}.We first consider the well-known semi-graphoid axiom system [3], denoted here as System G and shown in Fig. 1, andSystem S [14,26], shown in Fig. 2. In contrast to System G , the rules of System S are only applicable to saturated CIstatements and yield saturated CI statements only. In all rules, the sets are assumed pairwise disjoint. Clearly, we have thefollowing.Proposition 2. System S can be inferred from System G ; the weak contraction rule can be inferred from the symmetry, decompo-sition, and contraction inference rules.Finally, we consider the set of inference rules in Fig. 3, denoted as System A . Besides four rules of the semi-graphoidaxiom system, it contains the strong union and strong contraction rules. Clearly, G is subsumed by System A .Proposition 3. System G can be inferred from System A ; the weak union rule can be inferred from the decomposition and strongunion rules.From System A , many other rules can be inferred, such as the composition rule shown in Fig. 4. As System A plays apivotal role in this work, we use the following abbreviations. If C is a set of CI statements and c is a single CI statement,is short for C +then C (cid:5) c is short for C (cid:5)A c; similarly, C +A .We emphasize that in this and the next section we take a purely syntactic viewpoint towards CI statements, inferencerules, and inference systems. In particular, we do not imply that the rules exhibited in Figs. 1, 2, 3, and 4 are necessarilyM. Niepert et al. / Artificial Intelligence 202 (2013) 29–5133Fig. 4. The composition rule.Fig. 5. For S = {a, b, c}, L (a, b|c) = [c, abc] −([ a, abc] ∪ [b, abc]) = {c}.sound and/or complete. Notice, for example, that strong union is not a sound rule for the inference of CI statements relativeto the class of discrete probability measures, as is illustrated by Examples 35 and 38.4. Lattice-theoretic frameworkWe first introduce the lattice-theoretic framework which is at the basis of the theory developed in this work. We as-sociate CI statements with so-called meet semi-lattices, i.e., partially ordered sets in which each pair of elements has aninfimum (a meet) [37]. If the semi-lattice is a class of sets, the infimum of a pair of its elements is their set intersection.In the following, the term “semi-lattice” is used for a meet semi-lattice on a class of sets. Subsequently, we characterizederivability of CI statements in terms of their associated semi-lattices.Given subsets A and B of S, we write [ A, B] for {U | A ⊆ U ⊆ B}. We now associate semi-lattices with conditionalindependence statements as follows.Definition 4. Let I( A, B|C) be a CI statement. The semi-lattice of I( A, B|C), denoted L ( A, B|C), equals [C, S] − ([ A, S] ∪[B, S]) = {U ⊆ S | C ⊆ U , A − U (cid:7)= ∅, B − U (cid:7)= ∅}.Notice that L ( A, B|C) is indeed closed under intersection.Example 5. First, let S = {a, b, c}. Then, L (a, b|c) = {c}, as illustrated in Fig. 5. Next, let S = {a, b, c, d}. Then,L (bc, d|a) = [a, abcd] −L (bc, ∅|a) = [a, abcd] −L (ab, cd|∅) = [∅, abcd] −(cid:2)(cid:3)[bc, abcd] ∪ [d, abcd](cid:2)(cid:3)[bc, abcd] ∪ [∅, abcd](cid:2)= ∅;(cid:3)[ab, abcd] ∪ [cd, abcd]= {a, ab, ac};and= {∅, a, b, c, d, ac, ad, bc, bd}.34M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51For a CI statement c, L (c) denotes the semi-lattice of c, and, for a set of CI statements C , L (C ) =(cid:4)c(cid:8)∈C L (c(cid:8)).2Definition 6. Let I( A, B|C) be a CI statement. The set of witness sets of I( A, B|C), denoted W ( A, B|C), equals {{a, b} | a ∈A & b ∈ B}.Note that, if I( A, B|C) is trivial, then W ( A, B|C) = ∅. Using the previously defined notion of witness sets of a CI statement,we can rewrite the associated semi-lattice as a union of lattices. This is crucial as it opens the door to proofs by inductionon the lattice-elements. The following result is a straightforward consequence of Definition 4.Proposition 7. For a CI statement c = I( A, B|C), L (c) =(cid:4)W ∈W (c)[C, W ].Example 8. Let S = {a, b, c, d} and consider the CI statement I(bc, d|a). Then, L (bc, d|a) = [a, S] − ([bc, S] ∪ [d, S]) ={a, ab, ac}. Alternatively, we may apply Proposition 7 and deduce from W (bc, d|a) = {bd, cd} that L (bc, d|a) = [a, bd] ∪[a, cd] = [a, ac] ∪ [a, ab] = {a, ab, ac}.We now show that, given a set of CI statements C and a single CI statement c, C (cid:5) c if and only if L (c) ⊆ L (C ). Thischaracterization of derivability of CI statements under System A in terms of their associated semi-lattices is at the basisof our work on the CI implication problem. The implication from left to right can be shown by a straightforward structuralinduction. Therefore, we focus on the implication from right to left, for which we need the notion of witness decomposition.Definition 9. The witness decomposition of the CI statement I( A, B|C), denoted wdec( A, B|C), equals {I(a, b|C) | a ∈ A & b ∈B}. For a set of CI statements C , wdec(C ) =(cid:8)).(cid:4)c(cid:8)∈C wdec(cWe now justify the name “decomposition” by showing that, for any CI statement c, we have that (a) the closure of thewitness decomposition under A is equal to the closure of c under A , and (b) the union of semi-lattices associated withthe elements of the witness decomposition is equal to the semi-lattice associated with c.Proposition 10. Let c be a CI statement. Then, (1) {c}+ = wdec(c)+; and (2) L (c) =(cid:4)c(cid:8)∈wdec(c)L (c(cid:8)).Proof. Let c = I( A, B|C). For the first statement, the inclusion from right to left can be derived straightforwardly usingthe decomposition and symmetry rules. To see the reverse inclusion, let a ∈ A. For all b ∈ B, I(a, b|C) ∈ wdec(c). By repeat-edly applying the composition rule, we infer I(a, B|C), and, by symmetry, I(B, a|C). By another repeated application of thecomposition rule, we infer I(B, A|C), and, by symmetry, I( A, B|C). Hence, {c}+ ⊆ wdec(c)+. For the second statement, letI(a, b|C) ∈ wdec(c), and let W = {a, b}. Clearly, W (a, b|C) = {W } and, by Proposition 7, L (a, b|C) = [C, W ]. The statementnow follows from Proposition 7. (cid:2)We are now in the position to prove the fundamental equivalence between derivability under System A and the inclu-sion relationship between the lattice decompositions of the consequent and the antecedents.Theorem 11. Let C be a set of CI statements, and let c be a single CI statement. Then C (cid:5) c if and only if L (c) ⊆ L (C ).Proof. As said before, we focus on the “if”. Let I(a, b|C) ∈ wdec(c), and let W = {a, b}. Since L (c) =L (a, b|C) ⊆ L (C ) (1). By Proposition 10, it suffices to show that I(a, b|C) ∈ wdec(C )+statement ∀V ∈ [C, W ]: I(a, b|V ) ∈ wdec(C )+I(a, b|W ) ∈ wdec(C )+and such that W ∈ L (aand symmetry. For the inductive step, let C ⊆ V ⊂ W . The inductive hypothesis states that, for all VI(a, b|Vsuch that I(a(2). As in the base case, we can show that there exist ausing strong union. Let Wwe distinguish two cases:(cid:8)),. Thereto, we prove the strongerby downward induction on [C, W ]. For the base case, we must show that(cid:8)) ∈ wdec(C )(cid:8) ⊆ W , and we can derive I(a, b|W ) through strong union(cid:8) ⊆ W ,(cid:8)|V ) ∈ wdec(C )+(cid:8)} = {a, b} and we can use symmetry to infer I(a, b|V ). Otherwise,. By statement (1), W ∈ L (C ). By Proposition 10, there exist a(cid:8)} = {a, b} and Cwith V ⊂ V(cid:8), b(cid:8)) ∈ wdec(C )+(cid:8) = W then {a(cid:8)). Hence, {asuch that I(a(cid:8)}. If Wc(cid:8)∈wdec(c)(cid:8) = {aL (cand b(cid:8), b(cid:8), b(cid:8), b(cid:8), b(cid:8), b(cid:8), b(cid:8)|C(cid:8)|C(cid:8)(cid:8)(cid:8)(cid:8)(cid:4)(i) Exactly one of aand b(cid:8)(2)), we derive I(a, b(cid:8)(cid:8)is in W . Assume ab|V ), and, using decomposition, we derive I(a, b|V ).(cid:8) = a ∈ W . Using contraction on I(a, b(cid:8)|V ) and on I(a, b|V b(cid:8)) (in wdec(C )+byis in W . Using strong contraction on I(a(cid:8), b(cid:8)|V ) and on I(a, b|V a(cid:8)) and I(a, b|V b(cid:8)) (both in wdec(C )+by(ii) Neither a(cid:8)(2)), we derive I(a, b|V ). (cid:2)nor b(cid:8)We conclude this section with an illustrative example.2 Observe that the union of meet semi-lattices is not always a meet semi-lattice.M. Niepert et al. / Artificial Intelligence 202 (2013) 29–5135Fig. 6. The inference rules of System E .Example 12. Let S = {a, b, d, e}, let C = {I(a, b|∅), I(d, e|a), I(d, e|b)}, and let c = I(d, e|∅). Each of these four CI statementshas only one witness set, which is {a, b} for the first and {d, e} for the other three. On the one hand, by Proposition 7, wehave that L (C ) = [∅, de] ∪ [a, ab] ∪ [b, ab] = {∅, a, b, d, e, ab, de} and L (c) = [∅, ab] = {∅, a, b, ab}. Therefore, L (c) ⊆ L (C ).On the other hand, we can derive I(d, e|∅) from I(a, b|∅), I(d, e|a), and I(d, e|b) using the strong contraction rule. Therefore,C (cid:5) c, consistent with Theorem 11.5. Elementary CI statementsA witness decomposition of a CI statement (Definition 9) is equivalent to the original statement for derivations underSystem A (Proposition 10). Therefore, the CI statements involved in witness decompositions deserve further study.Definition 13. A CI statement is elementary if it is of the form I(a, b|C), with a and b distinct elements of S and C a subsetof S − {a, b}.Elementary CI statements play an important role in Section 8 and following, where we apply the developed theoryto obtain falsification and validation algorithms for the probabilistic implication problem. Here, we study some of theirproperties.Definition 14. Let c = I( A, B|C) be an arbitrary CI statement. We define Del(c) = {I(a, b|V ) | C ⊆ V ⊆ A BC − {a, b}, a ∈ A,b ∈ B}. Moreover, Del(C ) =(cid:8)).(cid:4)c(cid:8)∈C Del(cGiven a set of arbitrary CI statements C and an arbitrary CI statement c, we consider the sets of elementary CI statements+G and {c}+G = Del(C )and).3 We also present System E , shown in Fig. 6, which involves elementary CI statements only. In Theorem 16,Del(C ) and Del(c). In Theorem 15, we state that C +{c}+ = Del(c)+we show that C (cid:5) c if and only if, for all cel ∈ Del(c), Del(C ) (cid:5)E cel.+G (and hence also C + = Del(C )+G = Del(c)These results yield a normal form for derivations under System A : first, from the given set of CI statements, derive a setof elementary CI statements from the given set of CI statements under System G ; then, from this set, derive a new set ofelementary CI statements under System E ; and, finally, from this last set, derive the targeted CI statement under inferenceSystem G . (Notice that System E can be inferred from System A .)We start with the first and last stage of this normalization, which is straightforward.Theorem 15. Let c be an arbitrary CI statement and let C be a set of arbitrary CI statements. Then, {c}++G , and hence also {c}+ = Del(c)+Del(C )and C + = Del(C )+.+G = Del(c)G and C +G =We now turn to the second stage of the normalization, which involves System E , shown in Fig. 6. System E is con-structed in such a way that only elementary CI statements can be derived. In addition, it has the following property.Theorem 16. Let C be a set of elementary CI statements, and let c be a single elementary CI statement. Then C (cid:5)E c if and only ifL (c) ⊆ L (C ).Proof. System E can be inferred from System A . The “only if” direction then follows from Theorems 11 and 15. The proofof the “if” direction is a straightforward adaptation of the proof of the “if” direction of Theorem 11. (cid:2)Theorems 11, 15, and 16 yield the following corollary.3 We emphasize that the sets of elementary CI statements considered here are not witness decompositions of the corresponding arbitrary CI statements.While a witness decomposition can be obtained using only some rules of System G (namely, the decomposition and symmetry rules), reconstruction of theoriginal CI statement requires the strong union rule.36M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51Corollary 17. Let C be an arbitrary set of CI statements, and let c be a single arbitrary CI statement. Then the following statements areequivalent:• C (cid:5) c;• for all cel ∈ Del(c), Del(C ) (cid:5) cel; and• for all cel ∈ Del(c), Del(C ) (cid:5)E cel.6. The additive implication problemAn important insight into the study of the probabilistic implication problem for CI statements was gained by Studený’sseminal work on conditional independence structures [6]. He showed that, for every CI statement I( A, B|C), a discrete prob-ability measure P satisfies I( A, B|C), meaning that P (C)P ( A BC) = P ( AC)P (BC) for all possible assignments to the variablesin the sets A, B, and C , if and only if the multi-information function M P induced by P , a function defined in Section 7, satis-fies M P (C) + M P ( A BC) = M P ( AC) + M P (BC). This relationship is non-trivial and remarkable because it relates a conditionthat is defined by numerous constraints (one per variable assignment) to a single constraint over an additive set function.Hence, he reduced the multiplication-based probabilistic CI implication problem to an addition-based implication problem.Therefore, we first develop the mathematical machinery for the additive implication problem in this section. This theory isthen directly applicable to analyze the probabilistic implication problem via Studený’s transformation.6.1. Preliminaries and problem statementWe first introduce some terminology. In this work, a real-valued function is a function F : 2S → R, associating a realnumber to each subset of S.Definition 18. Let I( A, B|C) be a CI statement, and F be a real-valued function. We say that F a-satisfies I( A, B|C), denoted|(cid:13)aF I( A, B|C), if F (C) + F ( A BC) = F ( AC) + F (BC).For a real-valued function F and a class of CI statements C , we say that F a-satisfies C , denoted |(cid:13)aFeach CI statement in C .C , if F a-satisfiesDefinition 19. Let C be a set of CI statements, c be a single CI statement, and F be a class of real-valued functions. WeF c, if each function in F that a-satisfies all the CI statements in C alsosay that C a-implies c relative to F , denoted C |(cid:13)aa-satisfies c.Now, given a class of real-valued functions F and classes of CI statements C1 and C2, the additive implication problem(C1, C2)F is to decide whether C |(cid:13)aF c, for C ⊆ C1 and c ∈ C2.We proceed by defining the notion of density of a real-valued function.Definition 20. The density of(cid:5)U ∈[X,S](−1)|U |−| X|F (U ), for each X ⊆ S.the real-valued function Fis the real-valued function (cid:2)F defined by (cid:2)F ( X) =The following well-known relationship between a real-valued function and its density, also known as its Möbius inversion,justifies the name (see, e.g., [38]).Proposition 21. Let F be a real-valued function. Then, for each X ⊆ S, F ( X) =(cid:5)U ∈[X,S] (cid:2)F (U ).The a-satisfaction of a real-valued function for a CI statement can be characterized in terms of an equation involving itsdensity function. This characterization is central in developing our results and follows from a more general result in [31].Proposition 22. Let(cid:5)U ∈L ( A,B|C) (cid:2)F (U ) = 0.I( A, B|C) be a CI statement and F be a real-valued function. Then,|(cid:13)aF I( A, B|C) if and only ifIn the remainder of this section, we study properties of classes of real-valued functions that guarantee soundness and/orcompleteness of inference systems subsumed by System A for additive implication. These results are related to probabilisticconditional independence implication in Section 7.6.2. General soundness propertiesWe first define soundness, and then exhibit several characterizations.M. Niepert et al. / Artificial Intelligence 202 (2013) 29–5137Definition 23. Let I be an inference system for CI statements, F be a class of real-valued functions, and C1 and C2 beclasses of CI statements. Then, I is sound for the additive implication problem (C1, C2)F if, for each set of CI statementsC ⊆ C1, and for each single CI statement c ∈ C2, C (cid:5)I c implies C |(cid:13)aF c.In the remainder of Section 6, we shall consider Call, the class of all CI statements over the universe S, and Csat, theclass of all saturated CI statements over S.In order to characterize soundness for additive implication for several inference systems, we introduce the zero-densityproperty.Definition 24. A class of real-valued functions F has the zero-density property on a set of CI statements C if, for eachreal-valued function F ∈ F and for each CI statement c ∈ C, |(cid:13)aF c implies (cid:2)F (U ) = 0 for each U ∈ L (c).Example 30 lists several real-valued functions illustrating the zero-density property.We now provide characterizations of soundness for System A :Theorem 25. Let I consist of the symmetry, weak union, and strong union rules. Let F be a class of real-valued functions. Thefollowing statements are equivalent:(1) System I is sound for the additive implication problem (Call, Call)F ;(2) F has the zero-density property on Call; and(3) System A is sound for the additive implication problem (Call, Call)F .(cid:4)W ∈W (I( A,B|C))Proof. (1) ⇒ (2). Let I( A, B|C) be a CI statement, and let F ∈ F be a real-valued function such that |(cid:13)aF I( A, B|C). Weshow that (cid:2)F (V ) = 0, for each V ∈ L ( A, B|C). The proof goes by downward induction on L ( A, B|C). By Proposition 7,we have L ( A, B|C) =[C, W ]. Hence, for the base case, we must prove that (cid:2)F (W ) = 0, for each W ∈W ( A, B|C). Let W = {a, b} ∈ W (I( A, B|C)), with a ∈ A and b ∈ B. Now, I(a, b|C ∪ ( A − {a}) ∪ (B − {b})) is derivable fromI( A, B|C) using weak union and symmetry. Furthermore, I(a, b|W ) is derivable from I(a, b|C ∪ ( A − {a}) ∪ (B − {b})) usingstrong union, since C ∪ ( A − {a}) ∪ (B − {b}) ⊆ W . Thus, {I( A, B|C)} (cid:5)I I(a, b|W ), and, hence, by the soundness of I ,F I(a, b|W ). Because L (a, b|W ) = {W }, we can invoke{I( A, B|C)} |(cid:13)aProposition 22 to conclude that (cid:2)F (W ) = 0. For the inductive step, let V ∈ L ( A, B|C). The inductive hypothesis states that(cid:8)|V )(cid:2)F (U ) = 0 for all U ∈ L ( A, B|C) that are strict supersets of V . Similar to the base case, we can infer that |(cid:13)aU ∈L ( A(cid:8),B(cid:8)|V ) (cid:2)F (U ) = 0 (1). Clearly,with A(cid:8)|V ) are supersets of V . Hence, by the inductive hypothesis, (1) reduces(cid:8), BL ( Ato (cid:2)F (V ) = 0.(cid:8) = A − V , B(cid:8)|V ) ⊆ L ( A, B|C). Also, all sets in L ( A(cid:8) = B − V , and V pairwise disjoint. Hence, by Proposition 22,F I( A, B|C), it follows that |(cid:13)aF I(a, b|W ). Since |(cid:13)aF I( AC .(2) ⇒ (3). Let C be a set of CI statements and c a single a CI statement for which C (cid:5) c. Let F ∈ F for which |(cid:13)aFSince F has the zero-density property, we have that, for each U ∈ L (C ), (cid:2)F (U ) = 0. By Theorem 11, C (cid:5) c impliesL (c) ⊆ L (C ). Hence, by Property 22, |(cid:13)a(cid:8), B(cid:8), B(cid:5)(3) ⇒ (1). All rules of System I belong to System A , except forweak union, which can be inferred from it ( Proposi-F c.tion 3). (cid:2)6.3. Soundness properties for saturated CI statementsWe now discuss the additive implication problem (Csat, Call)F .Theorem 26. Let I consist of the symmetry and weak union rules. Let F be a class of real-valued functions. The following statementsare equivalent:(1) System I is sound for the additive implication problem (Csat, Call)F ;(2) F has the zero-density property on Csat;(3) System A is sound for the additive implication problem (Csat, Call)F ; and(4) System G is sound for the additive implication problem (Csat, Call)F .Proof. (1) ⇒ (2). Let I( A, B|C) be a saturated CI statement, and let F ∈ F be a real-valued function such that |(cid:13)aF I( A, B|C).We show that (cid:2)F (V ) = 0, for each V ∈ L ( A, B|C). The proof is similar to the corresponding part of that of Theorem 25,except that we may not use strong union. In the base case of the downward induction, we argued that, for W = {a, b} ∈W (I( A, B|C)), with a ∈ A and b ∈ B, the CI statement I(a, b|C ∪ ( A − {a}) ∪ (B − {b})) can be inferred from I( A, B|C) usingweak union and symmetry. Since S = A BC , that CI statement equals I(a, b|W ). A similar argument applies to the inductivestep.(2) ⇒ (3). Analogous to the corresponding part of the proof of Theorem 25.38M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51Table 1Densities of the real-valued functions considered in Example 30.Density(cid:2)F 10(cid:2)F 11(cid:2)F 12(cid:2)F 13(cid:2)F 2∅0.1000−0.2a0−0.3000.2b00−0.600.6c0000.90.3bc00000ac00000ab00000abc00000(3) ⇒ (4). System G can be inferred from System A (Proposition 3).(4) ⇒ (1). System I is a subset of System G . (cid:2)6.4. General completeness propertiesWe first define completeness, and then study it as in Section 6.2.Definition 27. Let I be an inference system for CI statements, F be a class of real-valued functions, and C1 and C2 beclasses of CI statements. Then, I is complete for the additive implication problem (C1, C2)F if, for each set of CI statementsC ⊆ C1, and for each single CI statement c ∈ C2, C |(cid:13)aF c implies C (cid:5)I c.In order to characterize completeness for additive implication for System A , we introduce theKronecker property .Definition 28. A class of real-valued functions F has the Kronecker property on Ω ⊆ 2S if, for each U ∈ Ω , there existsF U ∈ F such that (cid:2)F U (U ) (cid:7)= 0 and, for all X ∈ Ω , X (cid:7)= U , (cid:2)F U ( X) = 0.The name Kronecker property is inspired by the observation that, on Ω , (cid:2)F U behaves as a Kronecker delta function,which is zero everywhere except in one point.Let Ω (2) = {V ⊂ S | |V | (cid:2) 2}. Relative to a class of real-valued functions, the Kronecker property on Ω (2) implies thecompleteness of System A :Theorem 29. Let F be a class of real-valued functions. If F has the Kronecker property on Ω (2), then System A is complete for theadditive implication problem (Call, Call)F .Proof. Let F have the Kronecker property on Ω (2), and let C |(cid:13)aF c. Let C be a set of CI statements, and let c be a singleCI statement. Assume C (cid:2) c, or, equivalently, L (c) (cid:3) L (C ) (Theorem 11). By Proposition 7, all sets in L (C ) and L (c)are in Ω (2). Now, let U ∈ L (c) − L (C ). Since F has the Kronecker property on Ω (2), we know there exists F U ∈ F suchthat (cid:2)F U (U ) (cid:7)= 0 and, for all X ∈ Ω (2), X (cid:7)= U , (cid:2)F U ( X) = 0. From Proposition 22, it readily follows that |(cid:13)aF c.FHence, C (cid:7)|(cid:13)aF c. By contraposition, C |(cid:13)aF c implies C (cid:5) c. (cid:2)C , while (cid:7)|(cid:13)aThe following example illustrates the zero-density and Kronecker properties.Example 30. Let S = {a, b, c}. Let F1 = {F 10, F 11, F 12, F 13} and F2 = {F 2} as defined by their densities given in Table 1. Now,Ω (2) = {∅, a, b, c}. F1 has the Kronecker property on Ω (2), as F 10, F 11, F 12, and F 13 satisfy Definition 28 for, respectively,∅, a, b, and c. Hence, by Theorem 29, System A is complete for the additive implication problem (Call, Call)F1 . We nextinvestigate if F1 satisfies the zero-density property. First, consider F 10. The zero-density property relative to F 10 can onlybe broken by a constraint I( A, B|C) if ∅ ∈ L ( A, B|C), i.e., if C = ∅, A (cid:7)= ∅, and B (cid:7)= ∅. However, F 10 does not a-satisfythis constraint, as, in the defining condition, F 10(C) + F 10( A BC) = F 10( AC) + F 10(BC), the first term is non-zero, while theothers are zero. Similar arguments can be given for F 11, F 12 and F 13. Hence, F1 also satisfies the zero-density property,and, by Theorems 25 and 29, System A is sound for the additive implication problem (Call, Call)F1 .Finally, consider F2 = {F 2}, which does not satisfy the Kronecker property, as it does not contain functions whosedensities are non-zero on only one member of Ω (2). It also does not have the zero-density property as |(cid:13)aI(b, c|∅) butF 2(cid:2)F 2(∅) (cid:7)= 0. By Theorem 25, System A is not sound for the additive implication problem (Call, Call)F2 . Theorem 29 doesnot allow us to make any statements about the completeness of System A in this case.Theorem 29 goes only in one direction, but has the following weak converse.Theorem 31. Let F be a class of real-valued functions. If A is sound for the additive implication problem (Csat, Call)F and completefor the additive implication problem (Csat, Csat)F , then F has the Kronecker property on Ω (2).M. Niepert et al. / Artificial Intelligence 202 (2013) 29–5139Table 2Example of binary probability distribution over S = { A, B, C}.ABCP0000.250010.000100.250110.001000.251110.001100.001110.25Proof. If |S| (cid:3) 1, then Ω (2) = ∅ and the statement is true. Thus, assume |S| (cid:2) 2. For U ∈ Ω (2), let C be the set of non-trivialsaturated CI statements I( A, B|C) which satisfy either (1) A ⊆ U or (2) C − U (cid:7)= ∅. By Proposition 7, L (C ) ⊆ Ω (2). Byconstruction, U /∈ L (C ). Now, let X ∈ Ω (2), X (cid:7)= U . If X meets both U and U , then let X1 = X ∩ U and X2 = X ∩ U .Otherwise, let X1 and X2 be any two nonempty disjoint sets such that X1 ∪ X2 = X . It is easily verified that, in all cases,I( X1, X2| X) is in C , and, hence X ∈ L (C ). Thus, L (C ) = Ω (2) − {U }. Now, let U 1 and U 2 be any two nonempty disjointsets such that U 1 ∪ U 2 = U , and let c be I(U 1, U 2|U ). Since U ∈ L (c), L (c) − L (C ) (cid:7)= ∅. By Theorem 11, C (cid:2) c, and,by the completeness of A for the additive implication problem (Csat, Csat)F , C (cid:7)|(cid:13)aF c. Hence, there exists F U ∈ F suchc. Since A is also sound for the additive implication problem (Csat, Call)F , F has the zero-densitythat |(cid:13)aF Uproperty on Csat by Theorem 26. Thus, for all X ∈ L (C ) = Ω (2) − {U }, (cid:2)F U ( X) = 0, but, by Proposition 22, (cid:2)F U (U ) (cid:7)= 0,since (cid:7)|(cid:13)aF UC , but (cid:7)|(cid:13)aF Uc. (cid:2)Theorem 31 and Theorem 29 immediately yield the following corollary.Corollary 32. Let F be a class of real-valued functions. If A is sound for the additive implication problem (Csat, Call)F and completefor the additive implication problem (Csat, Csat)F , then A is complete for the additive implication problem (Call, Call)F .7. The probabilistic CI implication problemAlthough the theory developed in Section 6 concerns the additive implication problem for CI statements, it is alsoapplicable to the probabilistic implication problem. The link between both is made with the notion of multi-informationfunctions (Studený [6]) induced by probability measures.47.1. Preliminaries and problem statementWe first introduce some notations and terminology for stating and studying the probabilistic implication problem. A prob-ability model over S = {s1, . . . , sn} is a pair (dom, P ), where dom is a domain mapping associating si to a finite domaindom(si), for i = 1, . . . , n, and P is a probability measure having dom(s1) × · · · × dom(sn) as its sample space. For i1, . . . , ik a} ⊆ S, a is a domain vector of A if a ∈ dom(si1 ) × · · · × dom(sik ). In what follows,subsequence of 1, . . . , n and A = {si1 , . . . , sikwe focus on probability measures, leaving their probability models implicit. If all domains of the underlying probabilitymodel are binary, i.e., consisting of two values, we call the probability measure binary. We denote the class of all probabilitymeasures by Pall and the class of all binary probability measures by Pbin. For a probability measure P and A ⊆ S, wedenote by P A the marginal probability measure of P over A, i.e., for a domain vector a of A, P A(a) =b P (a, b), where branges over all domain vectors of S − A.(cid:5)We now define when a probability measure satisfies a CI statement.Definition 33. Let I( A, B|C) be a CI statement, and P be a probability measure. Then, P m-satisfies I( A, B|C), denotedP I( A, B|C), if, for all domain vectors a, b, and c of A, B, and C , respectively, P C (c)P A BC (a, b, c) = P AC (a, c)P BC (b, c).|(cid:13)mRelative to the multiplicative notion of m-satisfaction, we now define probabilistic implication for CI statements.Definition 34. Let C be a set of CI statements, c be a single CI statement, and P be a class of discrete probability measures.Then, C m-implies c relative to P , denoted C |(cid:13)mP c, if each probability measure P in P that m-satisfies all the CIstatements in C also m-satisfies c.For C , c, and P as above, we denote by C ∗P the set {c | C |(cid:13)mP c}, or C ∗, if the class of discrete probability measuresinvolved is clear from the context.Before continuing, we illustrate some of the concepts above by an example.Example 35. Let S = { A, B, C}, and P be the binary probability distribution shown in Table 2. Clearly, P A B (0, 0) =P A B (0, 1) = P A B (1, 0) = P A B (1, 1) = 0.25 and P A(0) = P A(1) = P B (0) = P B (1) = 0.50. Hence, for each domain vector u4 In this work, we only consider discrete probability measures.40M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51∅(π∅(u))P A B (π A B (u)) = P A(π A(u))P B (πB (u)), where “π X ” denotes projection onto X , always evaluates to true. Weof S, Pmay therefore conclude that P m-satisfies I( A, B|∅). However, P C (0)P A BC (0, 0, 0) = 0.75 × 0.25, while P AC (0, 0)P BC (0, 0) =0.50 × 0.50 yields a different result. Hence, P does not m-satisfy I( A, B|C).Given a class of discrete probability measures P and classes of CI statements C1 and C2, the probabilistic implicationproblem (C1, C2)P is to decide whether C |(cid:13)mP c, for C ⊆ C1 and c ∈ C2.Next, we define the Kullback–Leibler divergence [39].Definition 36. Let P and Q be two probability measures satisfying Q (s) > 0 whenever P (s) > 0, for all domain vectors sof S. Then, the Kullback–Leibler divergence H is defined asH(P |Q ) =(cid:6)sP (s) logP (s)Q (s),with s ranging over all domain vectors of S for which P (s) > 0.For P and Q as above, it is a well-know property [39] that H(P |Q ) (cid:2) 0, with H(P |Q ) = 0 if and only if P = Q .We now define multi-information functions induced by a probability measures [6].Definition 37. Let P be a probability measure. The multi-information function induced by P is the real-valued functionM P : 2S → R defined by⎧⎨⎩M P (∅) = 0;M P ( A) = H(cid:10)P A(cid:11)(cid:11)(cid:11)(cid:11)(cid:12)a∈ A(cid:13)P a,A ⊆ S, A (cid:7)= ∅.A real-valued function F : 2S → R is isotone if, for all disjoint subsets A and B of S, F ( A B) (cid:2) F ( A), and supermodularif, for all pairwise disjoint subsets A, B, and C of S, F ( A) + F ( A BC) (cid:2) F ( A B) + F ( AC). Studený [6] showed that multi-information functions are isotone and supermodular. By a previous remark, multi-information functions are also nonnegative.The class of multi-information functions induced by the class of discrete probability measures P will be denoted by MP .P c. With this result, we can reduce the(multiplicative) probabilistic implication problem for CI statements to the additive implication problem for CI statements,the advantage being that additive problems are usually easier to deal with than multiplicative problems. Moreover, we canthen apply the machinery developed in Section 6.Studený [6] showed, for a CI statement c, that |(cid:13)ac if and only if |(cid:13)mMP7.2. Soundness and completeness propertiesDefinitions 23 and 27 for soundness and completeness of an inference system for the additive implication problem(C1, C2)F (C1 and C2 are classes of CI statements and F is a class of real-valued functions) can be adapted to the settingof probabilistic implication, provided F is replaced by a class of discrete probability measures P , and a-implication (|(cid:13)a)is replaced by m-implication (|(cid:13)m).Example 38. From Example 35, we may conclude that strong union is not sound for the probabilistic implication problem(Call, Call)Pbin . Hence, it is also not sound for the probabilistic implication problem (Call, Call)Pall .It is known (e.g., [40]) that the semi-graphoid axioms are sound for the probabilistic implication problem (Call, Call)Pall .However, System G is not complete. Moreover, there cannot exist a finite set of inference rules that is sound and completefor the implication problem [6]. In addition, it is unknown whether the problem is decidable. For saturated CI statements,the situation is different. Malvestuto [26] and Geiger and Pearl [14] independently showed that System S is sound andcomplete for the probabilistic implication problem (Csat, Csat)Pall (see also [40]). Using the results of Section 6, we canbootstrap these results, as follows.Theorem 39.(1) The class of multi-information functions MPall has the zero-density property on Csat.(2) System A is sound for the probabilistic implication problem (Csat, Call)Pall .Proof. Since System G is sound for the general probabilistic implication problem (Call, Call)Pall ,(Csat, Call)Pall . The result now follows immediately from Theorem 26. (cid:2)it is also sound forM. Niepert et al. / Artificial Intelligence 202 (2013) 29–5141Theorem 40.(1) The class of multi-information functions MPall has the Kronecker property on Ω (2).(2) System A is complete for the probabilistic implication problem (Call, Call)Pall .Proof. By Theorem 39(2), System A is sound for the probabilistic implication problem (Csat, Call)Pall . Since System S isalready complete for the probabilistic implication problem (Csat, Csat)Pall , it follows in particular that System A is. Theresult now follows immediately from Theorem 31 and Corollary 32. (cid:2)It can be shown by direct calculation that the class of multi-information functions induced by the class of binary probabil-ity measures has also the Kronecker property on Ω (2). Therefore, System A is also complete for the probabilistic implicationproblem (Call, Call)Pbin . Please note that completeness of System A with respect to a class of probability functions P doesnot automatically imply completeness with respect to a subclass P (cid:8) ⊆ P .We conclude this section by a sound-and-completeness result.Theorem 41. Both System A and System G are sound and complete for the probabilistic implication problems (Csat, Call)Pall and(Csat, Call)Pbin .Proof. It only remains to show that System G is complete for both probabilistic implication problems. This is becauseSystem G is complete for the probabilistic implication problem (Csat, Call)P , where P is an arbitrary class of probabilitymeasures, if and only if System A is [41]. (cid:2)Notice that, in Theorem 41, we donot assume that the CI statements to be derived are saturated.8. Falsification and validation criteriaDecidability of the CI implication problem over the class of discrete probability measures is open, but it is known ithas no finite sound and complete inference system [15]. Nonetheless, Theorem 40 demonstrates it has a finite completeinference system, namely System A , which is useful since it provides a falsification criterion to decide some non-validinstances (Section 8.2). Of course, for instances which cannot so be falsified, it would be desirable to have validation criteriato decide some valid ones among these. We develop such a validation criterion, based on linear programming techniques(Section 8.3). In preparation, we explore two simplifications (Section 8.1). First, we show that we can reduce the generalprobabilistic implication problem to the probabilistic implication problem for elementary CI statements only. Then, wedemonstrate that it suffices to consider only those variables that occur in the CI statements under consideration. In Section 9,we will give experimental evidence that the combination of the falsification and validation criteria is useful for deciding alarge set of instances of the probabilistic implication problem.8.1. SimplificationsTo reduce the probabilistic CI implication problem to one on elementary CI statements only, we build upon the work inSection 5, where elementary CI statements were considered from a purely syntactical point of view.Proposition 42. Let C be a set of CI statements, and c be a single CI statement. Then,(1) C ∗ = Del(C )∗(2) C |(cid:13)mPall∗ = Del(c)∗and c;c if and only if, for all cel ∈ Del(c), Del(C ) |(cid:13)mPallcel.Proof. It suffices to prove the first statement for a set of CI statements C . By Theorem 15, Del(C ) ⊆ C +sound for the probabilistic implication problem (Call, Call)Pall , we have that Del(C ) ⊆ C ∗+G , from which we derive C ∗ ⊆ Del(C )∗also yields that C ⊆ Del(C ), hence Del(C )∗ ⊆ C ∗. (cid:2)G . Since System G is. Theorem 15We now turn to the second simplification, namely that variables that do not occur in the given CI statements need notbe considered for the implication problem. In particular, this is also the case for hidden variables. For a set of CI statementsover a finite universe S (the set of variables), let var(C (cid:8)) be the set of variables occurring in at least one CI statementC (cid:8)of C (cid:8)P c?” of the probabilistic implication problem relative to S. Intuitively, the variablesof S outside var(C ∪ {c}) play no role. Nevertheless, restricting this instance of the probabilistic implication problem to therelevant variables also means restricting the probability measures considered. Therefore, we need to prove formally that ourintuition is correct.. Now consider an instance “C |(cid:13)m42M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51Lemma 43. Let U and V be disjoint sets of variables. Let P U be a probability measure over U . Then there exists a probability measureP U V over U V such that, for each subset X of U , P XU V= P XU .Proof. Designate by “0” one element in the domain of each variable in U V . Let u be a domain vector of U and v a domainvector of V . Let 0V be the domain vector of V of which each component is 0. We define(cid:14)P U V (u, v) = 0 if v (cid:7)= 0V ;P U V (u, 0V ) = P U (u).Clearly, P U V is a probability measure over U V and, for X ⊆ U , P XU V= P XU . (cid:2)For U ⊆ S, PU denotes class of all probability measures over U .Proposition 44. Let U and V be disjoint sets of variables, C be a set of CI statements over U , and c be a single CI statement over U .Then, C |(cid:13)mc if and only if C |(cid:13)mc.PUPU VU V . Obviously, P U ∈ PU , and, for X ⊆ U , P XProof. We first prove the “only if”. Thereto, let P U V ∈ PU V be such that P U V m-satisfies all CI statements in C . LetP U = P Uc,U V m-satisfies c, and hence that P U V m-satisfies c. We now turn to the “if.” Thereto, let P U ∈ Pit now follows that P U = P Ube such that P U m-satisfies all CI statements in C . By Lemma 43, there exists a probability function P U V ∈ PU V such thatP U = P UU V . The remainder of the proof now goes along the same lines as for the “only if.” (cid:2)U V . Hence, P U m-satisfies all CI statements in C . From C |(cid:13)mPU= P XUAn immediate corollary to Proposition 44 is that, for a set of CI statements C and a single CI statement c, both over S,PSc, i.e., the targeted simplification. We shall, therefore, disregard irrelevant variables.c if and only if C |(cid:13)mPvar(C ∪{c})C |(cid:13)m8.2. Falsification criterionTheorems 11 and 40 yield a falsification criterion, i.e., a sufficient condition which can falsify certain instances of theprobabilistic CI implication problem:Corollary 45. Let C be a set of CI statements. If L (c) (cid:3) L (C ), then C (cid:7)|(cid:13)mPallc.We recall from [5] that testing for lattice inclusion is coNP-complete, that there exists a linear-time reduction to SATfor this problem, and that one can leverage SAT solvers to decide semi-lattice inclusion effectively, even if several hundredsof variables are involved. Now, if the falsified implications were, on average, only a small fraction of all those that arefalsifiable, the result would be disappointing from a practical point of view. Fortunately, we are not only able to show thata large number of implications can be falsified by the “lattice-exclusion” criterion identified in Corollary 45, but also thatpolynomial time heuristics exist that provide good approximations of said criterion. The falsification criterion and the twoheuristics we consider are formally exhibited in Algorithms 1, 2, and 3 (Fig. 7).It follows from Proposition 7 that, if one of the two heuristics returns false, then L (c) (cid:3) L (C ), and hence, by Corol-lary 45, that C (cid:7)|(cid:13)mc.PallExample 46. The inference rule I( A, B|DC) & I( A, D|BC) → I( A, B D|C), isnot sound relative to the class of discrete proba-bility measures. Falsification Heuristic 1 can reject this instance of the probabilistic implication problem.The Falsification Criterion actually leads to a family of polynomial time heuristics. While Falsification Heuristic 1 checksif the greatest lower bound of L (c) is not in L (C ) and Falsification Heuristic 2 checks if the least upper bounds of L (c)are not in L (C ), we may select additional elements in that semi-lattice located between these two extremes to derivemore falsification heuristics. Finally, we observe that there is also an alternative falsification strategy. The principal idea isto construct a discrete probability measure which provides a counterexample using the decidable theory of the reals withaddition. This is almost what Bouckaert and Studený have done with their racing algorithms [8]. Needless to say, this ratherexpensive approach can produce false positives in the sense that it may falsify instances of the probabilistic implicationproblem that are valid. This was not problematic for their application, as they had a different goal in mind.8.3. Validation criterionA validation criterion for the probabilistic implication problem accepts an instance of the problem only if the implicationis valid. If the criterion does not accept the instance, however, this does not imply that the instance is invalid. One of themost prominent validation criteria is the algorithm that computes the closure of the semi-graphoid axioms [2,3], which,M. Niepert et al. / Artificial Intelligence 202 (2013) 29–5143Fig. 7. The different falsification criteria and heuristics derived from the completeness of inference system A .unfortunately, can only validate a small fraction of the set of verifiable instances. Here, we introduce a much more powerfulvalidation criterion, based on the lattice-theoretic framework. We harness our results to represent a set of CI statementsC as a sparse 0–1 matrix AC in which the rows correspond to elements of L (C ) and the columns correspond to certainCI statements of the form I(a, b|C) that are induced from C . Subsequently, for a given CI statement c, we will associatewith the instance C |(cid:13)mc a linear program with equality constraints involving an encoding of c in terms of its associatedsemi-lattice L (c) and the matrix AC . We then show that C |(cid:13)mthis, we introduce relevant elementary CI statements:c is valid if this linear program has a solution. To achievePallPallDefinition 47. An elementary CI statement c is relevant to a set of CI statements C if L (c) ⊆ L (C ). The set of all such CIstatements is denoted R(C ).Since System A is complete but not sound for the probabilistic implication problem (Theorem 40), C (cid:5) c or, equivalently,P c, not all relevant statements associated with C needL (c) ⊆ L (C ) (Theorem 11) is necessary but not sufficient for C |(cid:13)mbe probabilistically implied by C . By Proposition 42, however, there is a subset of R(C ) that is probabilistically equivalentto C .With a CI statement c over the universe S, we associate a vector vc over 2S :(cid:14)vc(U ) =1 if U ∈ L (c);0 if U /∈ L (c).In other words, vc is a representation of the characteristic function of L (c).Apart from being the set of all elementary CI statements equivalent to the given set of CI statements C for derivationsin System A , R(C ) has the following attractive property which justifies calling R(C ) a “basis” for C .Proposition 48. Let C be a set of CI statements, and let c be a single CI statement such that C (cid:5) c. Then, for each CI statement cel inR(C ), there exists a nonnegative integer number kcel such that vc =cel∈R(C ) kcel vcel .(cid:5)Proof. With the semi-graphoid inference rules decomposition and weak union, we can show that L ( A, bD|C) = L ( A, b|C D)∪L ( A, D|C). Moreover, L ( A, b|C D) ∩ L ( A, D|C) = ∅; indeed, the former set contains only supersets of C D, while the lattercannot contain supersets of C . Hence, vI( A,bD|C) = vI( A,b|C D) + vI( A,D|C). By applying this repeatedly, it follows that, for eachelementary CI statement cel in Del(c), there exists a nonnegative integer kcel such that vc =cel∈Del(c) kcel vcel . The finalresult follows from observing that, by Corollary 17 and Theorems 15 and 11, Del(c) ⊆ R(C ). (cid:2)(cid:5)44M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51Observe that I(a, b|C) ∈ R(C ) if and only if I(b, a|C) ∈ R(C ), as both have the same semi-lattice. They also have thesame associated vector. To avoid such duplication, we define R/2(C ) to be R(C ) in which CI statements of the formI(a, b|C) and I(b, a|C) are no longer distinguished. Clearly, Proposition 48 still holds when R(C ) is replaced by R/2(C ).We still need a couple of additional results before we can move on to the construction of the linear program. The firstone involved the multi-information function M P of a discrete probability measure P .Proposition 49. Let A, B, and C be pairwise disjoint subsets of S, and let P be a discrete probability distribution. Then,(cid:5)U ∈L ( A,B|C) (cid:2)M P (U ) (cid:2) 0.Proof. For a real-valued function F , we have F (C) − F ( AC) − F (BC) + F ( A BC) =modular, the left-hand side is nonnegative. Since M P is supermodular [6], Proposition 49 immediately follows. (cid:2)U ∈L ( A,B|C) (cid:2)F (U ) [31]. If F is super-(cid:5)We have already defined the vector associated with a single CI statement. We now define the vector associated with aset of CI statements C as the sum of the vectors associated to the CI statements in C : vC =(cid:5)c∈C vc .Proposition 50. Let C be a set of CI statements, c be a single CI statement, and kc be a positive real number. If, for each CI statementcel ∈ R/2(C ), there exists a nonnegative real number kcel such thatvC = kcvc +(cid:6)kcel vcel ,cel∈R/2(C )then C |(cid:13)mPallc. Moreover, C |(cid:13)mPallcel for all cel ∈ R/2(C ) for which kcel > 0.C . By Proposition 49, we have that, for each CI statement celProof. Let P be a discrete probability measure such that |(cid:13)mPin R,(cid:5)U ∈L (cel) (cid:2)M P (U ) (cid:2) 0. Hence,(cid:6)(cid:6)kcelcel∈R/2(C )U ∈L (cel)(cid:2)M P (U ) (cid:2) 0.(cid:5)(cid:6)(cid:5)Again by Proposition 49, we have thatthat(cid:5)c(cid:8)∈CU ∈L (c(cid:8)) (cid:2)M P (U ) = 0, by Proposition 22. Now, we have thatU ∈L (c) (cid:2)M P (U ) (cid:2) 0. Now, since P satisfies all CI statements in C , we also have(cid:6)(cid:6)0 =(cid:2)M P (U ) = kc(cid:2)M P (U ) +kcel(cid:2)M P (U ),(cid:6)(cid:6)c(cid:8)∈CU ∈L (c(cid:8))U ∈L (c)cel∈R/2(C )U ∈L (cel)where the second equality follows from the precondition in the statement of Proposition 50. Since both terms in theU ∈L (c) (cid:2)M P (U ) = 0. Thus, by Propo-right-hand side of the above equation are nonnegative, and since kc > 0, necessarilysition 22, |(cid:13)mP cel for all cel for which kcel > 0. (cid:2)P c. By the same token, |(cid:13)m(cid:5)Proposition 50 still holds if the sum ranges over all elementary CI statements. There is no point in considering those notrelevant to C , however, as a careful examination of the proof reveals that their coefficients in the sum are necessarily 0,and, therefore, they cannot contribute to the solutions of the above equation.We write the equation in Proposition 50 as a linear program (Schrijver [42])minimize cT x subject to Ax = b, x (cid:2) 0.As we are only interested in satisfying the constraint, that is, we are only interested in the existence of a feasible solution,we set c = 0 (hence, the objective function cT x is zero). As to the parameters of the condition, we put A = AC , the matrixassociated with C , defined over L (C ) × R/2(C ) byAC [U , cel] =(cid:14)1 if U ∈ L (cel);0 if U /∈ L (cel),and b = vC − kcvc . Clearly, if x is a solution of Ax = b, then x is a vector of values kcel , cel ∈ R/2(C ), satisfying the equationin Proposition 50.Example 51. Let S = {a, b, d, e, f }, and(cid:15)C =I(a, b|∅), I(a, b|de), I(d, e|a), I(d, e|b)(cid:16).Based on the results from Section 8.1 concerning the set of relevant CI statements we can exclude the variable f . Then,R/2(C ) equalsM. Niepert et al. / Artificial Intelligence 202 (2013) 29–5145(cid:15)I(a, b|∅), I(a, b|d), I(a, b|e), I(a, b|de), I(d, e|∅), I(d, e|a), I(d, e|b), I(d, e|ab)(cid:16).Fig. 8. The matrix AC of Example 51.We denote these eight relevant elementary CI statements by c1, . . . , c8, respectively. Let c = c5 = I(d, e|∅). Hence,var(C ∪ c) = {a, b, d, e}.The 0–1 matrix AC is shown in Fig. 8. The rows in AC correspond to the elements in L (C ) = {∅, a, b, d, e, ab, de}|S| − |S| − 1, here 26,computed relative to var(C ∪ c) rather than S. While the number of possible rows in the matrix is 2| var(C ∪c)| − | var(C ∪ c)| − 1, here 11, and then further to |L (C )| as computed relative towe have first reduced it to 2var(C ∪ c), here only 7. The columns correspond to the relevant elementary CI statements c1, . . . , c8, respectively. Actually,these columns equal the vectors vc1 , . . . , vc8 , respectively. Notice that, given S, the number of elementary CI statements upto symmetry is(cid:10)(cid:13)|S|2|S|−2,2here 80. We have first reduced this number to(cid:10)(cid:13)| var(C ∪ c)|2| var(C ∪c)|−2,2here 24, and then further to the number of relevant elementary CI statements up to symmetry, here only 8. Compared tothe maximum possible size of 26 × 80 = 2080, our matrix has a size of only 7 × 8 = 56.C = (1, 1, 1, 1, 1, 2, 2). Now, choose kc = 1. Then, vT= (0, 0, 0, 1, 1, 1, 2). This vector is the sumof the 2nd, 3rd, and 8th column of AC . Hence, AC x = vC − kcvc has a nonnegative solution xT = (0, 1, 1, 0, 0, 0, 0, 1). ByProposition 50, C |(cid:13)mc, and also C |(cid:13)mWe have that vTC − kcvTcc3, and C |(cid:13)mc2, C |(cid:13)mc8.PallPallPallPallIt is well-known that a linear program (LP) is solvable in polynomial time in the number of its variables. As calculatedin Example 51, we get in the worst case an LP with an exponential number of variables (matrix columns) and constraints(matrix rows), namely(cid:13)(cid:10)|S|2|S|−22and 2|S| − |S| − 1,respectively. As a rule of thumb, the more columns matrix AC has, the more difficult the corresponding LP problem be-comes. An advantage of our validation strategy over a naïve approach is that AC only consists of the vectors representingthe relevant elementary CI statements in R/2(C ). This means that the actual number of variables of the LP may be verysmall compared to the worst case. We also emphasize that matrix A is always a 0–1 matrix, leading to better numericalstability and the possibility to employ existing sparse matrix data structures.5 We will come back to algorithmic issueswhen we discuss the results of our experiments.Finally, consider again Proposition 50. Unfortunately, we cannot express the condition kc > 0 in a linear program.6 There-fore, we have to make a particular choice for kc (e.g., kc = 1 in Example 51). In the remainder of this section, we shall relatedifferent choices for kc . Thereto, let CI(kc) denote the set of all pairs (C , c) such that, for each elementary CI statement5 In rare cases, solutions to the LPs may be inaccurate due to round-off and truncation errors. Therefore, when we obtain a solution, we convert itscomponents into fractional form (with integer numerator and denominator). For the resulting vector xfrac, we check whether Axfrac = b holds exactly, andwe only accept x as a solution if this is the case.6 Please note that the authors of the paper [9] have developed a method to circumvent this problem. It is potential future work to combine their approachwith the present one.46M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51cel ∈ R/2(C ), there exists a nonnegative real number kcel for which vC = kcvc +monotonicity property.(cid:5)cel∈R/2(C ) kcel vcel . We have the followingProposition 52. If k2 (cid:2) k1 > 0, then CI(k2) ⊆ CI(k1).Proof. Let (C , c) ∈ CI(k2). Then, for each cel ∈ R/2(C ), there exists kcelProposition 50, C |(cid:13)mexists kcel∈R/2(C ) kcel vcel . Byc, and hence, by Theorem 40, C (cid:5) c. By Proposition 48, it follows that, for each cel ∈ R/2(C ), there(cid:2) 0 such that vC = k2vc +vcel . Combining all of this, we obtain(cid:2) 0 such that vc =Pall(cid:5)cel∈R/2(C ) k(cid:8)cel(cid:8)cel(cid:5)vC = k2vc +(cid:6)kcel vcelcel∈R/2(C )= k1vc + (k2 − k1)vc +(cid:6)kcel vcel= k1vc +(cid:6)(cid:2)cel∈R/2(C )cel∈R/2(C )(k2 − k1)k(cid:8)cel+ kcel(cid:3)vcel .Since k2 (cid:2) k1, we have, for each cel ∈ R/2(C ), that (k2 − k1)k(cid:8)cel+ kcel(cid:2) 0. Consequently, (C , c) ∈ C I(k1). (cid:2)We can actually show that some of the inclusions in Proposition 52 are strict.Example 53. This is an adaptation of Example 6.3 in [6]. Let S = {a, b, d, e}, C = {I(a, d|∅), I(d, e|b), I(b, e|d), I(a, e|bd),I(b, d|ae)}, and c = I(a, de|b). It can easily be verified thatvC = 0.5vc + 0.5vI(a,d|∅) + 0.5vI(a,e|∅) + 0.5vI(d,e|b) + 0.5vI(b,e|d)+ 0.5vI(b,d|e) + 0.5vI(d,e|ab) + 0.5vI(b,e|ad) + 0.5vI(b,d|ae),and that the CI statements above are in R/2(C ). Hence, there is a solution to the equation in Proposition 50 with kc = 0.5.One can also verify that this equation has no solution for kc > 0.5.In Section 9, we shall further explore the actual impact of the choice of kc .We conclude this section with an example of an implication that holds and can neither be falsified nor validated by ouralgorithms.Example 54. This is an adaptation of Example 4.1 in [6]. Let S = {a, b, d, e}, C = {I(a, b|∅), I(a, b|d), I(a, b|e), I(d, e|ab)} andc = I(a, b|de). Observe that L (c) ⊆ L (C ), so the lattice-exclusion criterion cannot be used to falsify this instance of theimplication problem. Moreover, Studený [6] showed that C |(cid:13)mc although the equation in Proposition 50 has no solutionfor kc > 0.Pall9. ExperimentsThe theory of the previous sections provides the formal foundation for implementations of practical falsification andvalidation algorithms for the probabilistic implication problem. With the experiments in the present section, we address thefollowing empirical questions:(1) Effectiveness: What fraction of the instances of the probabilistic CI implication problem can we either falsify or validate,i.e., how close do we get to a decision procedure?(2) Efficiency: How fast does the algorithm run? To how many variables does it scale? How much more efficient is thealgorithm compared to the naïve approach7 both in terms of time and space complexity? How does the algorithmcompare to other approaches in the literature?(3) Structural and numerical properties: How large is the constraint matrix A for different instances? What are the numer-ical properties of the validation algorithm? To which extent does the parameter kc of the validation algorithm influencesthe effectiveness and efficiency of the algorithm?Since we may restrict our attention to elementary CI statements, we investigate the probabilistic implication problem(Cel, Cel)Pall , with Cel the class of all elementary CI statements. For each experiment, we first generated instances of this7 Naïve approach refers to the straightforward application of Corollary 45 without using the simplifications developed in Section 8.1 for falsification andwithout restricting the elementary CI statements to the relevant ones for validation.M. Niepert et al. / Artificial Intelligence 202 (2013) 29–5147Fig. 9. Falsification and validation curves of the approximate decision algorithm for 5 variables. For each number of antecedents, 90 000 instances wereconsidered (horizontal line). The number of decidable instances is the sum of the number of falsified instances and the number of validated instances.Table 3Mean and standard deviation of over 10 000 trials of the time to solve a linear program without and with optimizing the constraint matrix A.VariablesWithout optimizing AWith optimizing A78910Mean0.95 ms3.79 ms32.99 ms174.30 msStandard deviation0.39 ms1.83 ms51.28 ms907.53 msMean0.20 ms0.25 ms0.40 ms0.61 msStandard deviation0.33 ms0.49 ms0.77 ms0.54 msimplication problem by randomly selecting n different sets of elementary CI statements C over S as antecedent, and,for each of these, m different elementary CI statements c over S as consequence, one at a time. We first applied thefalsification algorithm to each of these instances “C |(cid:13)mc?”. When the falsification algorithm failed, we applied thevalidation algorithm and took advantage of the results to create the constraint matrix AC and vector b from C and c asdescribed in Section 8. To solve the resulting linear programs we used Gurobi,8 a mixed integer linear programming solverthat employs a version of the simplex algorithm [42,43] and the branch-and-cut method for integer programs. We did notchange the standard optimization settings of the solver. Furthermore, we only accepted a solution if its rational expansionsolved the respective constraints. For our purposes, this is entirely unproblematic, because the objective is to validate asmany instances of the implication problem as possible while ruling out false positives. All experiments were run on adual-core 3.2 GHz Linux PC with 3 GB RAM.PallFor a set of elementary CI statements C , we build the optimized constraint matrix AC bottom-up using System E . First,the closure under the strong union rule is computed. Thereafter, the closure under the elementary contraction and strongcontraction rules is computed. Then, we again compute the closure under the strong union rule. Although this proceduremight involve an exponential number of steps in the size of S, the naïve method of building the constraint matrix by usingthe set of all elementary CI statements over S is at least as complex.Fig. 9 shows the number of instances of the probabilistic implication problem that could be falsified or validated bythe algorithms for 5 variables. For each (cid:5) = 2, . . . , 58 (the number of antecedents), we randomly created n = 4500 differentsets of (cid:5) elementary CI statements, and, for each of those, randomly selected m = 20 different elementary CI statements asconsequences, one at a time, resulting in 90 000 instances for each value of (cid:5). The results show that (1) only a small fractionof the instances could not be decided, and, (2) for larger values of (cid:5) (for 5 variables, (cid:5) > 40), all instances could either befalsified or validated. The behavior of the algorithm was similar over all tested numbers of variables (see below).Table 3 reveals the computational efficiency gained in optimizing the constraint matrix A by using only relevant elemen-tary CI statements, compared to the naïve approach, where all elementary CI statements are used. The times provided arefor 7–10 variables, averaged over 10 000 trials, for n = 100 sets of (cid:5) = 10 antecedents, and m = 100 different consequences,one at a time. Table 4 shows the average time to construct and solve the linear program for sets of (cid:5) = 10 antecedents for5–15 variables, averaged over 100 000 trails (n = 1000, m = 100). The high standard deviations show that construction andsolving times exhibit a high variance depending on the input. Table 5, finally, compares the size of the optimized constraintmatrix to those of the constraint matrix resulting from the naïve approach.8 See http://www.gurobi.com.48M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51Table 4Mean and standard deviation over 100 000 trials of the time needed to construct, respectively, solve the linear program using only relevant elementary CIstatements.VariablesConstruction timeSolving time56789101112131415MeanStandard deviation0.50 ms1.71 ms3.94 ms8.68 ms16.19 ms65.74 ms180.98 ms640.62 ms2150.81 ms6999.10 ms25 968.52 ms0.65 ms2.19 ms0.51 ms1.48 ms3.71 ms9.11 ms26.18 ms54.04 ms39.24 ms471.01 ms497.67 msMean0.18 ms0.16 ms0.13 ms0.10 ms0.26 ms0.53 ms1.00 ms2.23 ms6.09 ms10.19 ms27.12 msStandard deviation0.10 ms0.15 ms0.14 ms0.19 ms0.32 ms0.70 ms1.28 ms2.48 ms5.03 ms9.81 ms21.08 msTable 5Mean and standard deviation over 100 000 trials of the size of the optimized constraint matrix A, compared to the maximal possible values in parenthesis.VariablesNumber of rowsNumber of columns56789101112131415Mean19.53 (26)35.14 (57)60.66 (120)98.46 (247)146.72 (502)244.42 (1013)355.02 (2036)531.19 (4083)823.41 (8178)1154.84 (16 369)1930.60 (32 752)Standard deviationMeanStandard deviation2.025.7311.8523.1241.6668.01102.74176.39347.51672.25960.2638.63 (80)70.12 (240)107.05 (672)152.36 (1792)188.32 (4608)319.61 (11 520)428.29 (28 160)618.84 (67 582)923.39 (159 744)1309.33 (372 736)2089.80 (860 160)8.7618.9927.3844.0958.4995.17128.63205.68388.19774.521046.63To investigate the impact of the choice of kc in Proposition 50, we conducted experiments for sets of (cid:5) = 10, 20, and 30antecedents, for 5–10 variables, averaged over 1 000 000 trials (m = 1000, n = 1000). Each of these was run with kc = 1.0−6. Despite the large difference between both values, at most 1176 additional instances out of a total of 1 000 000and kc = 10(for 20 antecedents and 6 variables) could be validated. Hence, lowering kc does not provide an added benefit, also becausesmall values of kc may lead to numerical instabilities.We also want to empirically verify that (1) the lattice-exclusion criterion can falsify a large fraction of all falsifiable in-stances, and (2) Heuristics 1 and 2 are good approximations of the full-blown lattice-exclusion criterion. To make our resultscomparable to results from other approaches, we adopted the experimental setup for the racing algorithm of Bouckaert andStudený [8] (also using 5 attributes). One thousand sets of antecedents each were generated by randomly selecting 3 up to10 elementary CI statements, resulting in a total of 8000 sets of antecedents. The falsification algorithm and the heuristicswere run on these sets with each of the remaining elementary CI statements as a consequence, one at a time. This resultedin 77 000 instances of the probabilistic implication problem for sets with 3 antecedents, down to 70 000 instances for setswith 10 antecedents.The rejection procedure of the racing algorithm is rooted in the theory of imsets: an instance is rejected if one of theconstructed supermodular functions is a counter-model. It has double-exponential running time and may reject valid in-stances of the probabilistic CI implication problem. This is because the racing algorithm has the purpose of deciding theimplication problem for imsets and, thus, the additive implication problem relative to the class of supermodular functions.The falsification algorithm based on Corollary 45 only rejects invalid instances of the CI implication problem. Fig. 10, left,shows the results. The falsification procedure of the racing algorithm rejects more instances than our approach, but amongthem are possibly also valid instances. Fig. 10, right, shows the rejection curves for the falsification algorithm, Heuristics 1and 2, and the combination of both. The latter compares favorably with the falsification algorithm: 95% of the falsificationsfor 3 antecedents down to 77% for 10 antecedents are found. Also, Heuristic 2 is more effective than Heuristic 1.Subsequent to the conference paper [25] in which we had first introduced the linear programming formulation of thevalidation algorithm based on Proposition 50, Bouckaert et al. [9] leveraged the theory of imsets [6] to also include theconstruction of linear programs to verify various instances of the CI implication problem. Their approach is almost identicalto ours except that they do not optimize the constraint matrix A by considering only the relevant CI statements as columnsand the elements in the semi-lattice of C as rows. In addition, the earlier version of our LP formulation did not take intoaccount the parameter kc of Proposition 50.We adopted the same experimental setup as Bouckaert et al. [9] for an empirical comparison of the two linear pro-gramming formulations. For |S| = 4–10, we randomly generated 1000 sets each consisting of 3 elementary CI statements,M. Niepert et al. / Artificial Intelligence 202 (2013) 29–5149Fig. 10. Left: Rejection and acceptance curves of the racing and falsification algorithms. Right: Falsifications based on the lattice-exclusion criterion and theheuristics.Table 6Comparison of the LP formulation of Bouckaert et al. [9] and the one presented here.|S|Bouckaert et al. [9]LPs/experimentTime/experimentNiepert et al., this paperLPs/experimentTime/experimentTime to construct ANr. rows ANr. columns ATime to solve LPOverall nr. LPs45678910244 ms8.341.03 ms0.2 ms6.068.340.1 ms83428018 ms10.851.74 ms0.44 ms8.8410.850.12 ms10 854240115 ms17.452.68 ms0.93 ms14.5617.450.1 ms17 449672658 ms24.54.06 ms1.36 ms21.3624.50.11 ms24 50117925037 ms4608150.38 s35.457.18 ms3.28 ms32.3135.450.11 ms35 45052.3115.42 ms8.1 ms49.0252.310.14 ms52 30611 5203862.1 s79.7943.01 ms23.06 ms75.1879.790.25 ms79 790then constructed the set of relevant CI statements and the constraint matrix A for each set of antecedents, and finally ranthe validation algorithm with each of the possibly implied elementary CI statements as a consequence. Because we restrictourselves to relevant CI statements, the number of LPs/experiment in Table 6 is much smaller for our approach than forthe approach of Bouckaert et al. The table lists also the other statistics for the two LP formulations. The average time toconstruct the constraint matrix A increases from 0.2 ms for 4 variables to 23 ms for 10 variables. Since the size of theconstraint matrix A remains small (on average about 80 × 80 for 10 variables), the average time to solve a single LP is only0.25 ms for 10 variables which is to be compared with the average time of 3862.1 s/11520 = 300 ms for the constraintmatrix consisting of all elementary CI statements used in Bouckaert et al.’s LP formulation, which is several orders of mag-nitudes slower than ours. Our procedure needs only 43.01 ms for 10 variables compared to the 3862.1 s of Bouckaert etal.’s approach.10. Discussion and future workLogical inference algorithms for probabilistic conditional independence statements have several important applicationsfrom checking consistency during knowledge elicitation to constraint-based structure learning of graphical models [44]. Asit is generally known that there does not exist a finite, sound and complete axiomatization for probabilistic conditionalindependence implication [6], we can try to do the next best thing, which is enclosing the problem between a “lowerbound”, that is, a finite set of inference rules that is sound, and an “upper bound”, a finite set of inference rules that iscomplete. It is well-known that the semi-graphoid axiom system, referred to in this paper as System G is sound (e,g., [40]).One of the main contributions of the present paper is introducing another finite set of inference rules, System A , which isshown to be complete for the probabilistic implication problem. While a sound but not complete finite system of inferencerules can always be transformed into a “tighter lower bound” by adding true logical implications that are not provable, ananalogous strategy is not obvious for transforming a complete but not sound system of inference rules into a “tighter upperbound”. It is therefore justified to ask whether System A is in some sense optimal as an “upper bound”. This is work forfuture research. Of course, the above question could in principle be asked about any complete but not sound axiomatizationsystem. It is therefore important to observe the connection between System A and the lattice-inclusion property, whichplays a pivotal role in the present paper.The theory underlying the present paper is based on a study of the additive implication problem, and, therefore, its scopeof applications is much broader than only the probabilistic implication problem. Other application areas include database50M. Niepert et al. / Artificial Intelligence 202 (2013) 29–51constraints, data mining, and reasoning about uncertainty [28,29,32,33]. Within the context of the probabilistic implicationproblem, the theory is also applicable to other types of (sets of) CI statements, such as stable sets of CI statements [5].An important contribution of this paper are the falsification and validation heuristics for the probabilistic implica-tion problem. While these heuristics are rooted in the lattice-theoretical framework, the supermodularity of the multi-information function also played a pivotal role in the development of the validation criterion. Therefore, a more thoroughstudy of additive implication with respect to the class of supermodular functions is called for.Finally, the lattice-inclusion property and the heuristics developed in this paper can be utilized to store information aboutconditional independencies more efficiently, using non-redundant representations [5]. This is in line with previous work onmore efficient maintenance of conditional independence information [35,36]. Overall, we believe that we have convincinglyargued that the lattice-theoretic framework for reasoning about conditional independence is a novel and powerful tool.AcknowledgementsWe thank Remco Bouckaert for providing us with the source code of the racing algorithm. We also want to thank MilanStudený for his continuous support of the present work and, in particular, the information he provided us concerning thecomplete axiomatization of the implication problem for four attributes. We also thank the anonymous reviewers of severalconferences and workshops (UAI, PGM) and especially those of the Artificial Intelligence journal whose detailed commentsand suggestions have helped to improve the clarity and quality of the article.References[1] D. Koller, N. Friedman, Probabilistic Graphical Models: Principles and Techniques, MIT Press, Cambridge, MA, 2009.[2] A. Dawid, Conditional independence in statistical theory, J. R. Stat. Soc. 41B (1979) 1–31.[3] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann Publishers Inc., 1988.[4] Y. Xiang, S.K.M. Wong, N. Cercone, Critical remarks on single link search in learning belief networks, in: Proceedings of the 12th Conf. on Uncertaintyin Artificial Intelligence, Morgan Kaufmann Publishers, 1996, pp. 564–571.[5] M. Niepert, D. Van Gucht, M. Gyssens, Logical and algorithmic properties of stable conditional independence, Internat. J. Approx. Reason. 51 (5) (2010)531–543.[6] M. Studený, Probabilistic Conditional Independence Structures, Springer-Verlag, 2005.[7] R. Hemmecke, J. Morton, A. Shiu, B. Sturmfels, O. Wienand, Three counterexamples on semigraphoids, Combin. Probab. Comput. 17 (2) (2008) 239–257.[8] R. Bouckaert, M. Studený, Racing algorithms for conditional independence inference, Internat. J. Approx. Reason. 45 (2) (2007) 386–401.[9] R. Bouckaert, R. Hemmecke, S. Lindner, M. Studený, Efficient algorithms for conditional independence inference, J. Mach. Learn. Res. 11 (2010)3453–3479.[10] S.M.M.S. Renooij, H.J.M. Tabachneck-Schijf (Eds.), Proceedings of the 6th UAI Bayesian Modelling Applications Workshop, 2008.[11] J.Y. Halpern, Reasoning about Uncertainty, The MIT Press, 2003.[12] M.J. Druzdzel, L.C. van der Gaag, Elicitation of probabilities for belief networks: Combining qualitative and quantitative information, in: UAI, 1995,pp. 141–148.[13] R. Dechter, Constraint Processing, Morgan Kaufmann Publishers Inc., 2003.[14] D. Geiger, J. Pearl, Logical and algorithmic properties of conditional independence and graphical models, Ann. Statist. 21 (4) (1993) 2001–2021.[15] M. Studený, Conditional independence statements have no complete characterization, in: Transactions of the 11th Prague Conference on InformationTheory, 1992, pp. 377–396.[16] J. Pearl, A. Paz, Graphoids, graph-based logic for reasoning about relevance relations, Adv. Artif. Intell. II (1987) 357–363.[17] A.P. Dawid, Separoids: A mathematical framework for conditional independence and irrelevance, Ann. Math. Artif. Intell. 32 (1–4) (2001) 335–372.[18] G. Shafer, Advances in the understanding and use of conditional independence, Ann. Math. Artif. Intell. 21 (1997) 1–11.[19] J.T.A. Koster, A graphical characterization of lattice conditional independence models, Ann. Math. Artif. Intell. 21 (1997) 13–26.[20] S.A. Andersson, D. Madigan, M.D. Perlman, C.M. Triggs, Gibbs and Markov properties of graphs, Ann. Math. Artif. Intell. 21 (1997) 27–50.[21] S.L. Lauritzen, F.V. Jensen, Local computation with valuations from a commutative semigroup, Ann. Math. Artif. Intell. 21 (1997) 51–69.[22] M. Studený, Semigraphoids and structures of probabilistic conditional independence, Ann. Math. Artif. Intell. 21 (1997) 71–89.[23] F. Matúš, Conditional independence structures examined via minors, Ann. Math. Artif. Intell. 21 (1997) 99–128.[24] M. Studený, Structural imsets, an algebraic method for describing conditional independence structures, in: Proceedings of the Conference on Informa-tion Processing and Management of Uncertainty (IPMU), 2004, pp. 1323–1330.[25] M. Niepert, Logical inference algorithms and matrix representations for probabilistic conditional independence, in: Proceedings of the 25th Conferenceon Uncertainty in Artificial Intelligence, 2009, pp. 428–435.[26] F.M. Malvestuto, A unique formal system for binary decompositions of database relations, probability distributions, and graphs, Inform. Sci. 59 (1992)21–52.[27] M. Niepert, D. Van Gucht, M. Gyssens, On the conditional independence implication problem: A lattice-theoretic approach, in: Proceedings of the 24thConference on Uncertainty in Artificial Intelligence, 2008, pp. 435–443.[28] B. Sayrafi, A measure-theoretic framework for constraints and bounds on measurements of data, Ph.D. thesis, Indiana University, 2005.[29] B. Sayrafi, D. Van Gucht, Inference systems derived from additive measures, in: Workshop on Causality and Causal Discovery, London, Canada, 2004.[30] B. Sayrafi, D. Van Gucht, M. Gyssens, Measures in databases and data mining, Tech. Report TR602, Indiana University Computer Science, 2004.[31] B. Sayrafi, D. Van Gucht, Differential constraints, in: Proceedings of the ACM Symposium on Principles of Database Systems, 2005, pp. 348–357.[32] B. Sayrafi, D. Van Gucht, M. Gyssens, The implication problem for measure-based constraints, Inf. Syst. 33 (2) (2008) 221–239.[33] B. Sayrafi, D. Van Gucht, P.W. Purdom, On the effectiveness and efficiency of computing bounds on the support of item-sets in the frequent item-setsmining problem, in: OSDM ’05: Proceedings of the 1st International Workshop on Open Source Data Mining, 2005, pp. 46–55.[34] F. Matúš, Ascending and descending conditional independence relations, in: Transactions of the 11th Prague Conference on Information Theory, 1992,pp. 189–200.[35] P. de Waal, L.C. van der Gaag, Stable independence and complexity of representation, in: Proceedings of the 20th Conference on Uncertainty in ArtificialIntelligence, 2004, pp. 112–119.[36] P. de Waal, L.C. van der Gaag, Stable independence in perfect maps, in: Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,2005, pp. 161–168.M. Niepert et al. / Artificial Intelligence 202 (2013) 29–5151[37] G.A. Grätzer, General Lattice Theory, second ed., Birkhäuser-Verlag, 1998.[38] J.H. van Lint, R.M. Wilson, A Course in Combinatorics, second ed., Cambridge University Press, 2001.[39] S. Kullback, R.A. Leibler, On information and sufficiency, Ann. Math. Stat. 22 (1951) 79–86.[40] J. Pearl, A. Paz, Graphoids: Graph-based logic for reasoning about relevance relations or when would x tell you more about y if you already know z?,in: Advances in Artificial Intelligence II, Seventh European Conference on Artificial Intelligence, 1986, pp. 357–363.[41] M. Gyssens, M. Niepert, D. Van Gucht, About the completeness of the semigraphoid axioms for the derivation of arbitrary conditional independencestatements from saturated ones, Tech. rep., Indiana University, 2011.[42] A. Schrijver, Theory of Linear and Integer Programming, John Wiley & Sons, Inc., 1986.[43] G. Dantzig, Linear Programming and Extensions, Princeton University Press, 1963.[44] P. Gandhi, F. Bromberg, D. Margaritis, Learning Markov network structure using few independence tests, in: Proceedings of the SIAM InternationalConference on Data Mining, 2008, pp. 680–691.