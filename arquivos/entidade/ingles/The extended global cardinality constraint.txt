Artificial Intelligence 175 (2011) 586–614Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintThe extended global cardinality constraint: An empirical surveyPeter NightingaleSchool of Computer Science, University of St Andrews, St Andrews, Fife KY16 9SX, United Kingdoma r t i c l ei n f oa b s t r a c tArticle history:Received 1 March 2010Received in revised form 18 October 2010Accepted 18 October 2010Available online 21 October 2010Keywords:Global cardinality constraintConstraint programmingGlobal constraintsPropagation algorithmsThe Extended Global Cardinality Constraint (EGCC) is a vital component of constraint solv-ing systems, since it is very widely used to model diverse problems. The literature containsmany different versions of this constraint, which trade strength of inference against compu-tational cost. In this paper, I focus on the highest strength of inference usually considered,enforcing generalized arc consistency (GAC) on the target variables. This work is an exten-sive empirical survey of algorithms and optimizations, considering both GAC on the targetvariables, and tightening the bounds of the cardinality variables. I evaluate a number of keytechniques from the literature, and report important implementation details of those tech-niques, which have often not been described in published papers. Two new optimizationsare proposed for EGCC. One of the novel optimizations (dynamic partitioning, generalizedfrom AllDifferent) was found to speed up search by 5.6 times in the best case and 1.56times on average, while exploring the same search tree. The empirical work represents byfar the most extensive set of experiments on variants of algorithms for EGCC. Overall, thebest combination of optimizations gives a mean speedup of 4.11 times compared to thesame implementation without the optimizations.© 2010 Elsevier B.V. All rights reserved.1. IntroductionConstraint programming is a powerful and flexible means of solving combinatorial problems. Constraint solving of acombinatorial problem proceeds in two phases. First, the problem is modelled as a set of decision variables, and a set ofconstraints on those variables that a solution must satisfy. A decision variable represents a choice that must be made inorder to solve the problem. The domain of potential values associated with each decision variable corresponds to the optionsfor that choice.Consider a sports scheduling problem, where each team plays every other team exactly once in a season. No team canplay two or more matches at the same time. Each team plays in a particular stadium at most twice during the season. Inthis example one might have two decision variables per match, representing the two teams. For a set of matches played inthe same stadium, a global cardinality constraint [24] could be used to ensure no more than two occurrences of each team.The second phase consists of using a constraint solver to search for solutions: assignments of values to decision variablessatisfying all constraints. The simplicity and generality of this approach is fundamental to the successful application ofconstraint solving to a wide variety of disciplines such as scheduling, industrial design and combinatorial mathematics [34,11].The Global Cardinality Constraint (GCC) is a very important global constraint, present in various constraint solving toolk-its, solvers and languages. It restricts the number of occurrences of values assigned to a set of variables. In the originalversion of the constraint [24], each value is given a lower bound and upper bound. In any solution, the number of occur-rences of the value must fall within the bounds. The literature contains many propagation algorithms for this constraint,E-mail address: pn@cs.st-andrews.ac.uk.0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.10.005P. Nightingale / Artificial Intelligence 175 (2011) 586–614587which trade strength of inference against computational cost, for example bound consistency [13,19], range consistency [18],and generalized arc-consistency (GAC) [24,18]. GCC is widely used in a variety of constraint models, for diverse problemssuch as routing and wavelength assignment [30], car sequencing [25], and combinatorial mathematics [11].Returning to the sports scheduling example, GCC can be used to express the stadium constraint (that a team plays in aparticular stadium at most twice during the season). Each value (representing a team) is given the bounds (0, 2), and thevariables are all slots at a particular stadium.GCC has been generalized by replacing the fixed bounds on values with cardinality variables [18], where each cardinalityvariable represents the number of occurrences of a value. To avoid confusion, I refer to this as the Extended Global Cardi-nality Constraint (EGCC). Thus an EGCC constraint has target variables (where the number of occurrences of some values areconstrained) and cardinality variables.In this paper, I focus on the highest strength of inference (enforcing GAC) on the target variables. This allows the study ofvarious methods in great depth, and leads to some surprising conclusions. I also survey methods for pruning the cardinalityvariables in depth. The main contributions of the paper are as follows.• A literature survey of GAC propagation algorithms for the target variables, and their optimizations, in Section 3.• Discussion of important implementation decisions in Section 3 that are frequently omitted from original papers, perhapsdue to lack of space. For example, how to find augmenting paths for Régin’s algorithm [24].• The proposal of two new optimizations in Section 3.4. One of these is based on modifying the flow network of Régin’salgorithm for greater efficiency, and the other is a novel generalization of the dynamic partitioning optimization ofAllDifferent [6].• A careful description of three concrete algorithms for pruning the cardinality variables in Section 4.• Easily the largest empirical study of GAC propagation methods for the target variables of EGCC, in Section 5. Thisinvolves two basic algorithms and seven optimizations.• Experimental conclusions and implementation advice for GAC for the target variables, in Section 6.• An empirical study of pruning the cardinality variables, comparing the three methods, in Section 5.8, leading to experi-mental conclusions in Section 6.• It is shown that an appropriate combination of optimizations is over 4 times faster on average than a careful butunoptimized implementation of Régin’s algorithm (Section 5.10), for our benchmark set.• A fast variant of EGCC is typically orders of magnitude better than a set of occurrence constraints. Even when EGCCpropagation was least effective, it slowed the solver down by only 1.66 times or less in our experiments (Section 5.10).2. Background2.1. PreliminariesFor CSP P = (cid:2)X , D, C(cid:3), a constraint Ck ∈ C consists of a sequence of m > 0 variables Xk = (cid:2)xk1 , . . . , xkmA CSP P = (cid:2)X , D, C(cid:3) is defined as a set of n variables X = (cid:2)x1, . . . , xn(cid:3), a set of domains D = (cid:2)D(x1), . . . , D(xn)(cid:3) whereD(xi) (cid:2) Z, |D(xi)| < ∞ is the finite set of all potential values of xi , and a conjunction C = C1 ∧ C2 ∧ · · · ∧ Ce of constraints.(cid:3) with domainsDk = (cid:2)D(xk1 ), . . . , D(xkm )(cid:3) s.t. Xk is a subsequence1 of X , Dk is a subsequence of D, and each variable xki and domainD(xki ) matches a variable x j and domain D(x j) in P . Ck has an associated set C S⊆ D(xk1 ) × · · · × D(xkm ) of tuples whichspecify allowed combinations of values for the variables in Xk.Although I define a constraint Ck to have scope (cid:2)xk1 , . . . , xkm(cid:3), when discussing a particular constraint I frequently omitkthe k subscript, and refer to the variables as (cid:2)x1, . . . , xm(cid:3), and to the domains as (cid:2)D(x1), . . . , D(xm)(cid:3).A literal is defined as a variable-value pair, xi (cid:8)→ j such that xi ∈ X and j ∈ Z. To prune a literal is to remove the valuek , andj from the domain D(xi). In the context of a constraint Ck, I refer to a tuple τ of values as being acceptable iff τ ∈ C Svalid iff |τ | = m and ∀ j: τ [ j] ∈ D(xk j ) (i.e. each value in the tuple is in its respective domain).A solution to a CSP P = (cid:2)X , D, C(cid:3) is a tuple τ of size | X| where ∀i: τ [i] ∈ D(xi) (τ represents an assignment to allisvariables), and all constraints are satisfied by τ : for each constraint Ck in C with scope (cid:2)xk1 , . . . , xkmk (τ (cid:11)constructed where ∀ j: τ (cid:11)[ j] = τ [k j], and τ (cid:11) ∈ C SGeneralized Arc-Consistency (GAC) for constraint Ck is defined as a function from domains Dk to a set of literals P . Notek is defined in terms of Dk. A literal xi (cid:8)→ j where j ∈ D(xi) is in P iff it is not present in any tuple in C Sk :k : τ [i] = j. Literals in P are not part of any acceptable and valid tuple of the constraint, therefore they can be prunedthat the set C S(cid:3)τ ∈ C Swithout reducing the set of solutions of the CSP P .(cid:3), a new tuple τ (cid:11)is acceptable).2.1.1. Graph theoryRégin’s algorithm [24] and Quimper’s algorithm [18] for pruning EGCC make use of network flow and bipartite matchingtheory [2] as well as strongly connected components [31]. Similarly, Régin’s AllDifferent algorithm [23] makes use of resultsfrom graph theory, in particular maximum bipartite matching [1] and strongly connected components.1 I use subsequence in the sense that (cid:2)1, 3(cid:3) is a subsequence of (cid:2)1, 2, 3, 4(cid:3).588P. Nightingale / Artificial Intelligence 175 (2011) 586–614A bipartite graph G = (cid:2)V , E(cid:3) is defined as a set of vertices V and a set of edges E ⊆ V × V , where the edges areinterpreted as having no direction and the vertices can be partitioned into two sets V 1 and V 2 such that no two elementsin the same set are adjacent.A digraph G = (cid:2)V , E(cid:3) is defined as a set of vertices V and a set of edges E ⊆ V × V , where the edges are interpreted ashaving direction.2.1.2. Propagation and searchPropagation is one of the basic operations of most constraint solvers: it simplifies a CSP by pruning values from thedomains. For example, applying GAC (defined above) to a constraint gives a set of values that may be pruned withoutchanging the solution set. Constraint solvers provide a propagation algorithm (or propagator) for each type of constraint, andthese are applied until the fixpoint is reached for all constraints.Propagation is typically interleaved with splitting. Splitting is the basic operation of search, and a splitting operationtransforms a CSP into two or more simpler CSPs. Hence a depth-first backtracking search is performed, with propagationoccurring at each node in the search tree.A propagator Prop(Ck, Dk) for constraint Ck computes a function from the domains Dk to new domains D(cid:11)k. For example,the propagator may compute the GAC prunings P (defined above), and then prune each literal in P from Dk to construct D(cid:11)k.(cid:11)Propagators only reduce variable domains (they are contracting): ∀ j: D⊆ Dk j . Propagators must also be correct withk jrespect to Ck (the set C Sk is preserved when the propagator is applied) and must not allow assignments that do not satisfythe constraint. These conditions (correctness and weak monotonicity) are defined by Schulte and Tack [28].The propagators considered in this paper are idempotent (assuming that no variable is duplicated in (cid:2)xk1 , . . . , xkm(cid:3)), whichmeans that one application of the propagator will reach a fixpoint for the constraint: Prop(Ck, Dk) = Prop(Ck, Prop(Ck, Dk)).2.2. Extended GCCA traditional Global Cardinality Constraint has just one set of variables (the target variables). Each domain value has fixedlower and upper bounds associated with it. An assignment to the target variables is a solution iff the number of occurrencesof each value is within the bounds for that value.The main focus of this paper is the Extended Global Cardinality Constraint (EGCC). The EGCC has a second set of variables(cardinality variables) representing the number of occurrences of each value. Cardinality variables replace the fixed boundson each value, hence EGCC is much more flexible than GCC. EGCC has the following form.egcc(X, V , C)X is the vector of target variables, V is a vector of domain values of interest, and C is a vector of cardinality variables, onefor each value in V . The constraint is satisfied under an assignment iff for all indices i of V , the number of variables inX set to V i is equal to Ci . There is no restriction on the number of occurrences of any value not in V . (In Régin’s originaldefinition of GCC [24], each value in the target domains has a cardinality interval. In contrast, V might not include allvalues so a default interval of 0 . . . ∞ is used.) Throughout, I use r as the number of target variables | X| for the constraintin question. I use d to represent the number of target variable domain values: d = |D(x1) ∪ · · · ∪ D(xr)| where X = (cid:2)x1 . . . xr(cid:3).Propagation of EGCC would typically be in two phases, to prune the target and cardinality variables respectively. Quimperet al. [18] have shown that enforcing GAC on EGCC is NP-Hard in general. However it is known that when the domains ofthe cardinality variables are an unbroken interval then GAC is tractable [26]. To exploit this tractable case, the algorithmsused in this paper read (and prune) only the bounds of the cardinality variables, and prune the target variables using onlythe bounds of the cardinality variables. The pruning of the target variables is similar to GAC (Section 2.1), however a newdefinition is required.Definition 2.1. For constraint Ck = egcc( X, V , C) with target variables X = (cid:2)x1 . . . xr(cid:3) and cardinality variables C = (cid:2)c1 . . . c|V |(cid:3),(cid:11)(cid:11)) is constructedGAC-On-X is defined as a function from Dk to literal set P as follows. A new constraint C= egcc( X, V , Ck(cid:11)(cid:11)(cid:11) = (cid:2)ci) = {ci . . . ci}. The GAC function is applied to C. Finally P isk to obtain literals Pwith Cthe set of literals in P(cid:11)|V |(cid:3), and domains ∀i: D(cpertaining to X : P = {( yi (cid:8)→ a) ∈ P(cid:11) | yi ∈ X}.(cid:11)1 . . . c(cid:11)(cid:11)GAC-On-X for an EGCC constraint is equivalent to reading the bounds of the cardinality variables, creating a new GCCconstraint with those bounds, and enforcing GAC on the GCC.Samer and Szeider identify other tractable cases, for example when the treewidth of the variable-value graph isbounded [26]. While this work is of theoretical interest, it is not clear that the tractable cases would be found in typi-cal uses of the EGCC constraint.2.3. Basic definitions for EGCCI will refer to the target variables X as x1, . . . , xr and their domains as D(x1), . . . , D(xr). The size of the union of alltarget domains is d. For simplicity, domain elements are assumed to be 1 . . . d.P. Nightingale / Artificial Intelligence 175 (2011) 586–614589(a) Bipartite variable-value graph B.(b) Flow network N(C).Fig. 1. Example of variable-value graph and flow network.First the variable-value graph is defined. The variable-value graph has one set of vertices representing target variables,and a second set representing values. There is an edge between a variable xi and a value a iff a ∈ D(xi). Fig. 1(a) gives anexample of a variable-value graph.Definition 2.2. Given an EGCC K , the bipartite variable-value graph is defined as B(K ) = (cid:2)V , E(cid:3) where V = {x1, . . . , xr,1, . . . , d} and E = {xi ↔ j | j ∈ D(xi)}.Next a flow network N(K ) for an EGCC K is defined. It is derived from the variable-value graph. N(K ) has both acapacity c and lower bound l on each edge. It includes the vertices in the variable-value graph, and also a source vertex sand a sink t. It is defined below and an example is given in Fig. 1(b).Definition 2.3. Given an EGCC K with parameters X = (cid:2)x1 . . . xr(cid:3), V = (cid:2)v 1 . . . vm(cid:3), and C = (cid:2)c1 . . . cm(cid:3),2 the flow graph N(K )is defined as a digraph N(K ) = (cid:2)V , E(cid:3) where V = {x1, . . . , xr, 1, . . . , d, s, t}. E is the union of the following edge sets.• For each edge in B(K ), orient the edge from values to variables. For all edges (v, x) in this set l(v, x) = 0 and c(v, x) = 1.• For all value vertices v i ∈ V , there is an edge (s, v i) with lower bound l(s, v i) = ci and capacity c(s, v i) = ci (i.e. theflow through (s, v i) is within the bounds of the cardinality variable ci ).• For all values a in {1 . . . d} but not in V , there is an edge (s, a) with l(s, a) = 0 and c(s, a) = ∞.• For all variables xi , there is an edge (xi, t) where l(xi, t) = 0 and c(xi, t) = 1.The intuition behind N(K ) is that an integer flow from s to t corresponds to an assignment to the target variables. If theflow uses an edge (xi, a) then in the assignment, xi = a. If a flow in N(K ) covers all the variable vertices, and meets all thelower bounds and capacities, it corresponds to a satisfying assignment to the target variables.2.4. Hall sets and EGCCHall sets are useful for understanding the pruning of the target variables in EGCC. Two types of Hall set are required, forthe upper bounds and lower bounds respectively. The following definition of upper-bound Hall set is equivalent to Quimper’sdefinition (see [17, §5.1]).Definition 2.4. A UB-Hall set H u is a set of variables with corresponding values D(H u) =sum of the upper bounds of D(H u) equals the number of variables: |H u| =(cid:3)v i ∈D(Hu ) ci .(cid:2){D(xi) | xi ∈ H u} such that theIn any solution to the constraint, the variables H u are assigned to values in D(H u) and this assignment meets the upperbound for each value in D(H u). Therefore no other variable x j /∈ H u can be assigned a value in D(H u), and some pruningmay be performed. Variables H u consume the values D(H u).A small example of a UB-Hall set is given in Fig. 2(a). In this case, three variables {x1, x2, x3} are adjacent to only thevalues {1, 2}. The sum of the upper bounds of {1, 2} is three, therefore {x1, x2, x3} is a UB-Hall set.2 For simplicity it is assumed that V ⊆ {1, . . . , d}. If this is not the case, for each value v i not in {1, . . . , d} the corresponding cardinality variable ci is setto 0.590P. Nightingale / Artificial Intelligence 175 (2011) 586–614GCC instances represented as variable-value graphs. Values are labelled with their lower andupper bound as an interval [a, b].(a) Example of a UB-Hall set. Variables{x1, x2, x3} consume values {1, 2}. Thedotted lines will be pruned.(b) Example of an LB-Hall set. Values{1, 2} consume variables{x1, x2, x3}.The dotted lines will be pruned.Fig. 2. Example of a UB-Hall set and an LB-Hall set.For lower bounds, the Hall set is similar but variables and values are swapped. This definition is equivalent to unstablesets as defined by Quimper (see [17, §5.2]).Definition 2.5. An LB-Hall set Hl is a set of values with corresponding variables Vars(Hl) = {xi | Hl ∩ D(xi) (cid:15)= ∅} such thatthe sum of the lower bounds of Hl equals the number of variables: |Vars(Hl)| =(cid:3)ci .v i ∈HlIn this case, in any solution to the constraint, the variables Vars(Hl) must be assigned to values in Hl exclusively, tomeet the lower bounds of Hl. Therefore other values may be pruned. The values Hl consume the variables Vars(Hl).A small example of an LB-Hall set is shown in Fig. 2(b). The sum of the lower bounds for values {1, 2} is three, and thetwo values are adjacent to three variables {x1, x2, x3}, therefore {1, 2} is an LB-Hall set. This leads to the pruning of twovalues.The definition of LB-Hall set captures the reason for prunings but not failure. The constraint fails (C Ska set of values Hl where the sum of the lower bounds is greater than the number of variables: |Vars(Hl)| <(cid:3)= ∅) if there existsv i ∈Hlci .UB and LB-Hall sets are closely related to enforcing GAC-On-X (Definition 2.1). Quimper [17] proved that lower boundsand upper bounds can be considered separately without losing GAC-On-X (thus decomposing EGCC into an upper-boundconstraint (ubc) and a lower-bound constraint (lbc)). He also showed the correspondence between Hall’s marriage theoremand the satisfiability of the ubc. It follows that finding all UB-Hall sets H u and pruning values in D(H u) from other variablesis sufficient to enforce GAC-On-X on the ubc. For lbc, Quimper shows directly that finding all LB-Hall sets is sufficient toenforce GAC-On-X.The algorithms presented in the next section make use of UB- and LB-Hall sets to prune the target variables.2.5. Experimental contextExperiments were performed with Minion [4,5] version 0.9. The solver was modified only to add variants of EGCC. Inthis section I give an overview of Minion.Constraint solvers provide a propagation loop that calls propagators until the global fixpoint is reached. Propagatorssubscribe to variable events and are scheduled to be executed when one of the events occurs. Subscription to an event isreferred to as placing a trigger, where a trigger is an object and it is placed into a list related to the event. A propagatoris triggered when it is called because an event occurred. Minion provides the following variable event types: max(D(xi))changed; min(D(xi)) changed; value a removed from D(xi); D(xi) changed in any way; xi is assigned.Triggers are identified by a number which is passed to the propagator. Therefore a propagator can identify the exactevent that caused it to be called. Notification of the events is important for several of the EGCC propagators; without thisfacility the propagator would scan the variable domains, adding a linear or quadratic cost. The exact use of variable eventsis described in Section 3.5.2.Minion is a variable-centric solver with an additional constraint-centric queue. The solver has two queues for efficiencyreasons: the variable queue is very fast, because adding a variable event to the queue is an O (1) operation (whereas withthe constraint queue, each trigger is copied to the queue). However, the variable queue does not allow constraints to begiven different priorities. Having the additional constraint queue overcomes this limitation.The variable queue contains the variable events listed above. The constraint queue contains pointers to constraints. Con-straints are responsible for adding themselves to the constraint queue as necessary. It has a lower priority than the variablequeue: the variable queue is emptied before each item is processed from the constraint queue. In all the experimentspresented below, only EGCC and AllDifferent constraints use the constraint queue.Propagators may require internal state for efficiency. Minion provides both backtracked memory (that is restored assearch backtracks) and non-backtracked memory. The backtracked memory must be allocated before search begins, and isblocked together. It is backtracked by copying the block. The consequences of this memory architecture are discussed inSection 5.2.1.P. Nightingale / Artificial Intelligence 175 (2011) 586–6145913. Pruning the target variables of EGCCIn this section I discuss pruning the target variables, beginning with a survey of the relevant literature. There are twopublished algorithms to enforce GAC-On-X, given lower and upper bounds for the occurrence of each value. Régin [24]presented an algorithm based on network flow. It makes use of the Ford–Fulkerson algorithm [2] to compute a flow whichrepresents an assignment to the target variables. The assignment satisfies the lower and upper bounds for each value. ThenTarjan’s algorithm is used to compute the set of edges that cannot belong to any maximum flow. These edges correspond todomain values to be pruned. The time complexity of one call to the algorithm is O (r2d), dominated by the Ford–Fulkersonalgorithm.An alternative algorithm was presented by Quimper et al. [18,17].3 The approach here is to split the GCC into twoconstraints, such that enforcing GAC on both is equivalent to enforcing GAC on the GCC. In this way they obtain a bettertime bound than Régin’s algorithm.3.1. Régin’s algorithmThe first stage of Régin’s algorithm computes a flow that is both feasible (it meets all lower bounds) and maximum,without exceeding capacities. First a feasible flow is computed, then it is extended to a maximum flow.3.1.1. Computing a feasible flowTo compute a feasible flow, a second flow network LB(K ) is used that is identical to N(K ) with one additional edge.There is an edge (t, s) with l(t, s) = 0 and c(t, s) = ∞.In order to use the Ford–Fulkerson algorithm [2], Régin defines the residual graph for a flow network and flow. A flowis a function mapping all edges to the quantity of material passing through them (a non-negative integer). The intuitionbehind the residual graph is that there is an edge from vertex a to vertex b iff it is possible to increase the flow from a to bwithout violating the capacity c(a, b), or to reduce the flow from b to a without violating the lower bound l(b, a). (The firstcase applies when (a, b) is an edge in N(K ), and the second case applies when (b, a) is an edge in N(K ).)Definition 3.1. The residual graph Res(G, f ) is derived from a flow network G and a flow f . It is a digraph with the same setof vertices as G. For each edge (a, b) in G, if f (a, b) > l(a, b) then the edge (b, a) is present in Res(G, f ). If f (a, b) < c(a, b)then the edge (a, b) is present in Res(G, f ). No other edges are present in Res(G, f ).The algorithm to compute a feasible flow is as follows. Suppose fis an infeasible flow. Pick an edge (a, b) from LB(K )such that f (a, b) < l(a, b). Find a simple path from b to a in Res(LB(K ), f ). This is named an augmenting path, and (by thedefinition of Res(LB(K ), f )) the flow can be increased along this path and through (a, b) by 1 unit. This is denoted applyingthe augmenting path. For each edge (x, y) in the path, either f (x, y) is increased or (if the edge is oriented ( y, x) in LB(K ))(cid:11)(a, b) > f (a, b). In this context, the increase in the flow throughf ( y, x) is decreased. This creates a new flow f(a, b) is always 1. If there is no augmenting path from b to a, then it is impossible to satisfy the lower bound and the EGCCfails.where f(cid:11)Fig. 3 shows two examples of augmenting paths in the residual graph Res(LB(K ), f ). The existing flow f passes through(s, 3), (3, x3), (x3, t), and (t, s). In CSP terms, this flow represents the assignment x3 = 3.For EGCC, the only edges where the lower bound is non-zero are those from s to a domain value, (s, v i). An augmentingpath has one of two forms. It passes through the edge (t, s) as shown in Fig. 3(a), or it does not pass through (t, s) asshown in Fig. 3(b). In the first case, applying the augmenting path increases the overall flow from s to t (by assigning x2to 1 in this example). In the second case, applying the augmenting path does not affect the overall flow from s to t. In thisexample, the flow through (s, 2) is increased (by setting x3 to 2) and the flow through (s, 3) is decreased.For this paper the implementation iterates through the values v i where f (s, v i) < l(s, v i) and meets the lower boundfor v i if possible. An augmenting path is sought starting at the vertex v i . The search succeeds when it discovers s or t orfails when all reachable vertices have been explored. Terminating at s corresponds to Fig. 3(b). When terminating at t, theedge (t, s) is appended to the augmenting path and this corresponds to Fig. 3(a).3.1.2. Computing a maximum flow from a feasible flowGiven a feasible flow f 0, the Ford–Fulkerson algorithm is used again to compute a maximum feasible flow. An augment-ing path is sought from s to t in Res(N(K ), f 0). This is applied to create flow f 1. The process is repeated for f 1 to create f 2,etc. The algorithm terminates when no augmenting path exists from s to t in Res(N(K ), fk). If the maximum feasible flow fkdoes not cover all variable vertices, then the constraint fails. An example is given in Fig. 4(a). In this example, the feasibleflow f 0 uses edges (1, x1), (1, x2) and (2, x3), therefore these edges are reversed in Res(N(K ), f 0). The augmenting pathuses edge (4, x4) and completes the maximum flow.3 The algorithm was described in Claude-Guy Quimper’s PhD thesis [17] therefore I refer to it as Quimper’s algorithm throughout.592P. Nightingale / Artificial Intelligence 175 (2011) 586–614(a) Augmenting path for edge (s, 1) that passes through t.(b) Augmenting path for edge (s, 2) that does not pass through t.Fig. 3. Examples of augmenting paths in the residual graph to compute a feasible flow.(a) Augmenting path from s to t.(b) SCCs of the residual graph.Fig. 4. Examples of computing a maximum feasible flow and the SCCs of the residual graph.3.1.3. Finding augmenting paths in Ford–FulkersonThe two main options here are depth-first search (FF–DFS) and breadth-first search (FF–BFS). The problem is very similarto maximum bipartite matching: an augmenting path alternates between variables and values (ignoring s and t). ThereforeI refer to the bipartite matching literature.Setubal empirically compared ABMP, FF–BFS, FF–DFS and Goldberg’s algorithm [29]. He generated bipartite graphs with2p vertices in each partition, where p ∈ {8 . . . 17}. With an estimate of 29 vertices or fewer in each partition,4 an examinationof Setubal’s results on sequential computers (taking the size closest to 29 and all smaller sizes) shows that FF–BFS iscompetitive for all classes and is most efficient (or equal) in 8/11 classes of graphs, and 10/13 sets of a particular size.Setubal recommends using FF–BFS for graphs up to thousands of vertices. Given these results I used FF–BFS throughout.3.1.4. Pruning the domainsThe second stage of Régin’s algorithm makes use of strongly connected components (SCCs). An SCC is a maximal set ofvertices of a digraph with the property that there is a path from any vertex to any other in the set. It follows that thereare cycles within the SCCs, and no cycles with edges between SCCs. The set of SCCs forms a partition of the vertices of thedigraph. Tarjan’s algorithm can be used to efficiently compute the SCCs of any digraph in O (|V | + |E|) time [31].An edge of the form (v i, x j) from N(K ) that cannot be in any maximum feasible flow corresponds to a value to bepruned (i.e. v i is pruned from x j ). Given the maximum feasible flow f that covers all variable vertices, the residual graphRes(N(K ), f ) is partitioned into its SCCs. If an edge (v i, x j) goes between two SCCs and is not used in the flow f , then thealgorithm prunes v i from D(x j).4 The largest EGCC constraint in the benchmark instances has 200 variables and fewer values, so they are all smaller than 29.P. Nightingale / Artificial Intelligence 175 (2011) 586–614593The intuition behind this result is that for any edge (v i, x j) in the residual graph, if v i and x j are in the same SCC thenthere is a simple path from x j to v i (by the definition of SCCs). This path may be used as an augmenting path to increasethe flow through (v i, x j). Hence (v i, x j) can take part in a maximum feasible flow. However, if v i and x j are in differentSCCs, there is no path from x j to v i . (v i, x j) cannot take part in any maximum feasible flow, unless it is in f . In this casethe algorithm prunes v i from x j .Another understanding of Régin’s algorithm comes from Hall sets. Every pruning is justified by a UB-Hall set (Defini-tion 2.4) or an LB-Hall set (Definition 2.5); see Section 2.4. For a deletion of a from D(xi), either a is consumed by aUB-Hall set that does not contain xi , or xi is consumed by an LB-Hall set that does not contain a. In both cases, the Hallset corresponds directly to an SCC of the residual graph: in the first case, the SCC containing a; in the second the SCCcontaining xi .Fig. 4(b) shows an example where f flows through edges (1, x1), (1, x2), (2, x3) and (4, x4). SCCs of Res(N(K ), f ) aremarked with thick dotted lines. The edges (4, x3), (3, x3) and (3, x1) cross between SCCs, so the corresponding values arepruned from the target domains. Tarjan’s algorithm and the pruning of domains is implemented exactly as described in [6].3.1.5. Time complexity of Régin’s algorithmIf δ is the number of edges in B(K ) (i.e. the sum of the sizes of target variable domains), and r is the number oftarget variables, the time to find a maximum feasible flow with Ford–Fulkerson is O (rδ). (No more than r augmentingpaths are found and applied.) The complexity of Tarjan’s algorithm is Θ(δ) (i.e. run time is bounded above and below by δasymptotically), because Tarjan’s algorithm uses every edge in the graph.Régin suggests that Dinic’s algorithm [32] should be faster in practice than Ford–Fulkerson [24]. However Dinic’s al-gorithm with the Sleator–Tarjan method of finding a blocking flow (as described by Tarjan [32]) has an upper bound ofO (rδ log(r + d)). (This bound may not be tight for our problem.) In this paper I do not consider Dinic’s algorithm becauseof its greater complication and worse time bound.3.2. Quimper’s algorithm in detailThe approach taken by Quimper et al. [18,17] is to split the GCC into a lower bound constraint (lbc) and an upper boundconstraint (ubc). The lbc ensures that the lower bound for each value is respected, and similarly the ubc enforces the upperbound. Enforcing GAC on the lbc and ubc independently prunes the same values as GAC on the GCC [19]. For both the lbcand ubc, a two-stage algorithm similar to Régin’s is used.For the first stage (of both lbc and ubc), the variable-value graph B(K ) is used, and a structure similar to a maximummatching is computed. A conventional maximum matching M is a maximum-cardinality set of edges of B(K ) such that novertex occurs more than once in M. This is generalized by allowing some vertices to occur more than once: value verticesmay occur multiple times up to a capacity cap(a) for a value a. Variable vertices occur at most once.A generalized maximum matching is computed using a modified Hopcroft–Karp algorithm [10]. The modification is verysimple and does not affect the worst-case execution time. In the lbc cap(a) is set to the lower bound of a for each value a,and for ubc the upper bound is used. At this point, the lbc fails if the matching does not meet all the lower bounds. The lbcmatching is completed (to cover all variable vertices) by matching each unmatched variable vertex with an arbitrary value.The ubc fails if the generalized matching does not cover all variables.For the second stage of both algorithms, the matchings are translated to flows in N(K ). For a matching M and cor-responding flow f M , each edge (x, y) ∈ M carries a unit of flow in f M . Each edge from B(K ) not in M carries no flowin f M .The second stage of Régin’s algorithm is used with changes to the bounds: for the ubc 0 is used as the lower bound forall values; and for the lbc ∞ is used as the upper bound for all values.Finally, it is not necessary to run the two propagators alternately to a fixpoint to enforce GAC on the GCC. It is sufficientto run one then the other. The implementation used in this paper runs the lbc propagator then the ubc propagator.The use of Hopcroft–Karp in place of Ford–Fulkerson produces a tighter time bound of O (r1.5d) (or O (r0.5δ)) for onecall to the propagator. Although Quimper’s algorithm has a tighter upper bound, it may not be better in practice because itmaintains two maximal matchings rather than one in Régin’s algorithm, and makes two calls to Tarjan’s algorithm ratherthan one. The two algorithms are compared experimentally in Section 5.3.3.3. Review of optimizations of the basic algorithmsThe algorithms described above are similar to each other and to Régin’s AllDifferent algorithm [23]. A number of opti-mizations to this collection of algorithms have been proposed by various authors. They are surveyed in this section.3.3.1. Incremental matchingThe maximum flow M (or matchings Ml and Mu in the case of Quimper’s algorithm) may be maintained incrementallyduring search [24]. This is done by storing M between calls to the propagator. When the propagator is called, M may nolonger be maximum because of domain removals, so the flow or matching algorithm is used to repair it. For AllDifferent,incremental matching has been shown to improve efficiency [6].594P. Nightingale / Artificial Intelligence 175 (2011) 586–6143.3.2. Incremental graph maintenanceThe original GAC AllDifferent algorithm [23] stores its graph between calls, maintaining the graph incrementally asvariable domains change. One parameter of the algorithm is the set of values deleted from variable domains, and thefirst step of the algorithm is to update the graph. This idea has two costs: updating the graph by removing edges; andbacktracking the graph as search backtracks. Whether the benefit outweighs the cost is an empirical question which isanswered below. The implementation of incremental graph maintenance is discussed in Section 3.5.1.An algorithm without incremental graph maintenance can discover the graph as it is traversed, by querying variabledomains and the maximum flow. This is the approach used for AllDifferent by Gent et al. [6].3.3.3. Priority queueMany constraint solvers have a priority queue for constraints (e.g. Choco [14], Gecode [27]), such that the prioritiesdetermine the order in which constraint propagators are executed. It is standard practice for the EGCC to have a lowpriority. Schulte and Stuckey demonstrate the importance of priority queueing [27], and it is evaluated in the experimentshere.3.3.4. Staged propagationSchulte and Stuckey proposed multiple or staged propagation for AllDifferent [27], where a cheap propagator with a highpriority is combined with a more expensive, low priority propagator.I do not experiment with staged propagation for EGCC in this paper, however it would be an interesting area for futurework.3.3.5. Dynamic partitioningGent et al. [6] proposed an algorithm which partitions an AllDifferent constraint during search. Suppose for examplewe have AllDifferent(x1 . . . x6) and have x1 . . . x3 ∈ {1 . . . 3}, x4 . . . x6 ∈ {4 . . . 6}. This can be partitioned into two independentcells: AllDifferent(x1 . . . x3) and AllDifferent(x4 . . . x6). The main benefit is that if some variable xi has changed, the propaga-tor need only be executed on the cell containing xi , not the original constraint. This saves time in Tarjan’s algorithm.A cheap way of obtaining the partition is to use the SCCs of the residual flow network, which are computed as partof Régin’s AllDifferent algorithm. In some cases it is possible to find a finer partition than the SCCs. However, experimentsshowed that using SCCs as the partition is effective in practice [6].In this paper I generalize dynamic partitioning to the EGCC constraint. This is described in Section 3.4.2.3.3.6. Assigned variable removalThe implementation of EGCC in Gecode [27] updates its array of target variables each time it is called, removing assignedvariables.5 This promises to be a lightweight and effective optimization. It is evaluated in Section 5.6.Dynamic partitioning subsumes assigned variable optimization, because any assigned variable is a singleton SCC andtherefore cannot be in any active cell of the constraint. However, dynamic partitioning is likely to be more expensive.3.3.7. Domain countingRecall that Quimper’s algorithm divides the constraint into the upper-bound constraint (ubc) and lower-bound constraint(lbc). Quimper and Walsh observed that the ubc need not be propagated when domains are large [20]. They proposed analgorithm that constructs a sorted list of the sizes of all target variable domains. It iterates through the list and determineswhether the ubc propagator is run. I suspect this algorithm would be too expensive for general use, although on someproblem classes it may prove valuable. Quimper and Walsh do not give a domain counting algorithm for the lbc.A simpler form of domain counting has been used for AllDifferent. Lagerkvist and Schulte used the following scheme:when triggered by a target variable xi the propagator only runs if |D(xi)| (cid:2) r (where r is the arity of the constraint) [15].Gent et al. improved the threshold to |D(xi)| (cid:2) r − 1 [6], but did not find domain counting to be useful in experiments.It is possible to derive a similar domain size threshold for the ubc, using the definition of a UB-Hall set (Definition 2.4).However it is not possible for the lbc. Consider the definition of an LB-Hall set (Definition 2.5). The size of the domains ofthe variables in the LB-Hall set is not restricted by the definition. Since it is not possible for the lbc, it is not possible forthe EGCC. I do not experiment with domain counting in any form.3.3.8. Important edgesKatriel observed that many value removals affecting a GCC constraint result in no other value removals, and so workprocessing them is wasted [12]. She introduces the concept of an important edge of the residual graph. An important edgeis one whose removal causes the pruning of some variable-value pair. Therefore, when an unimportant edge is removed, itis not necessary to run the propagator.Where r is the number of target variables, Katriel gave an upper bound of 3r on the number of important edges thatcorrespond to domain values (i.e. edges between variable vertices and value vertices).5 Guido Tack, personal communication.P. Nightingale / Artificial Intelligence 175 (2011) 586–614595Katriel shows that if there are many allowed values per variable, the expected cost of propagation can be reduced. Sheproposes to keep a count of pruned values, and run the propagator only when the counter reaches a threshold value. Thethreshold is set so that the propagator is likely to prune a value when executed. This algorithm does not always enforce GACon the GCC. Katriel does not report an implementation, and observes that the risks of failing to propagate may outweighthe reduced cost of propagation.While an implementation of Katriel’s probabilistic algorithm would be interesting, the fact that it does not maintain GACon the GCC puts it outside the scope of this paper.Gent et al. [6] gave an algorithm for AllDifferent to identify a small set of edges containing the important edges andpossibly others. The identified edges correspond to important domain values. The propagator is only executed when animportant domain value has been removed, thus maintaining GAC with fewer calls to the propagator. This approach isadapted for EGCC in Section 3.5.3.3.3.9. EntailmentQuimper et al. give the conditions under which the GCC constraint is entailed (i.e. there are no unacceptable tuples inthe relation of the constraint, under the current domains) [18]. If the constraint is entailed, it need not be propagated at thecurrent search node or its descendents. For the lower bound constraint, the condition is that for each value v with lowerbound LB(v), LB(v) variables are assigned to v. Similarly, for the upper bound constraint, for each value v, at most UB(v)domains contain v. However, EGCC cannot be entailed until all variables are assigned. If some variable is not assigned, anyacceptable tuple may be turned into an unacceptable tuple by changing the value of that variable.I did not experiment with entailment of GCC because the conditions are quite tight, and are likely to occur only whena large number of variables are assigned, therefore the benefit appears to be limited. Also the architecture of Minion is notwell suited to entailment (as discussed in Section 5.2.1).3.4. Novel optimizations for pruning the target variablesIn this section I describe two optimizations. The first is a change to Régin’s algorithm intended to improve the computa-tion of a maximum flow. The second generalizes dynamic partitioning (described in Section 3.3.5) to EGCC.3.4.1. Transpose graph for computing the maximum flowTo compute a maximum feasible flow from a feasible flow, Régin’s algorithm uses the graph N(K ), and seeks paths froms to t in N(K ). An alternative would be to use the transpose of N(K ) denoted N(K )T . The transpose is N(K ) with thedirection of every edge reversed. A path from t to s in N(K )T is equivalent to a path from s to t in N(K ).The direction of the flow f is reversed to form f T , and the algorithm searches for paths from t to s in the residual graphRes(N(K )T , f T ). The algorithm works as follows. Iterate through edges (t, xi) that carry no flow. For each edge, search fora path p from xi to s. If there is such a path, augment the flow along p and through (t, xi). If there is no path p, it is notpossible to construct a flow that covers all variables so the algorithm fails immediately.The conventional Régin’s algorithm completes the maximum flow before testing if it covers all variables. When using thetranspose graph, the algorithm can potentially stop much earlier, when it discovers a variable that cannot take part in amaximum flow. Also, each search for an augmenting path is more focused since it starts with a specific variable. Quimper’salgorithm uses the transpose graph, however (like Régin’s algorithm) it completes the maximum flow before testing if itcovers all variables [18].In Section 5.4 this approach is evaluated compared to Régin’s original algorithm. For both algorithms, a breadth-firstsearch is used to find augmenting paths.3.4.2. Dynamic partitioningDynamic partitioning essentially re-writes the EGCC constraint into multiple independent constraints as domains arenarrowed. As described in Section 3.3.5, Gent et al. gave an algorithm for dynamic partitioning of AllDifferent [6]. TheAllDifferent algorithm maintains a partition of the set of variables. I generalize the algorithm to EGCC. Consider the followingEGCC constraint.x4 . . . x6 ∈ {2, 3, 4}x1 . . . x3 ∈ {1, 3},c1, c2 ∈ {0, 1},(cid:4)EGCCc3, c4 ∈ {0, 1, 2}(cid:5)[x1 . . . x6], [1, 2, 3, 4], [c1, c2, c3, c4]GAC-On-X propagation removes value 3 from variables x4 . . . x6. Following this, the domains of x1 . . . x3 and x4 . . . x6 aredisjoint, and the constraint can be re-written into two constraints as shown below.(cid:4)EGCC(cid:4)EGCC(cid:5)[x1 . . . x3], [1, 3], [c1, c3](cid:5)[x4 . . . x6], [2, 4], [c2, c4]596P. Nightingale / Artificial Intelligence 175 (2011) 586–614Fig. 5. The refinement of the partition as an EGCC constraint is re-written as multiple constraints.Suppose x3 were assigned to 3. The first of the two constraints can be re-written again as follows.(cid:4)EGCC(cid:4)EGCC(cid:4)EGCC(cid:6)c1, (c3 − 1)(cid:7)(cid:5)[x1, x2], [1, 3],(cid:5)[x3], [3], [1](cid:5)[x4 . . . x6], [2, 4], [c2, c4]In this case, the domains of [x1, x2] and x3 are not disjoint, they share the value 3. One occurrence of 3 resides with x3,and c3 − 1 occurrences of 3 reside with [x1, x2]. Suppose x1 were also assigned 3. Now the occurrences of 3 have reachedits upper bound, so after propagation and further re-writing we have this situation.(cid:4)EGCC(cid:4)EGCC(cid:4)EGCC(cid:4)EGCC(cid:5)[x2], [1], [1](cid:5)[x1], [3], [1](cid:5)[x3], [3], [1](cid:5)[x4 . . . x6], [2, 4], [c2, c4]The EGCC algorithm maintains a partition of the set containing target variables and values. The major changes fromAllDifferent are that values are included in the partition, and corner cases of EGCC (involving singleton variables and values)are accounted for. Initially the partition has one cell, consisting of all target variables and values. The partition is refined aspropagation and search progresses, and restored as search backtracks. In the example above, the final refined partition wouldbe {{x1}, {x2}, {x3}, {1}, {3}, {x4, x5, x6, 2, 4}}. Assigned variables are singleton sets, and so are values where the number ofoccurrences has reached the upper bound.The partition I use corresponds to the SCCs of the residual graph (Section 3.1.4), and these are stored in the partitiondata structure described in [6]. (Target variables are represented using integers 0 . . . r − 1, and values using r . . . r + d − 1 ifthere are d values.) The data structure allows an item to be located in O (1) time, and its cell to be iterated in linear time.Splitting a cell also takes linear time, and undoing the split operation on backtracking is O (1).Fig. 5 gives an example of how the partition data structure of [6] works on EGCC. Each cell is stored in setElementsin a contiguous block in no particular order. The array splitPoint marks where a cell ends. Only splitPoint is backtrackedas search backtracks, hence cells join back together but the elements may be in a different order. (A third array maps avariable or value to its index in setElements, hence allowing it to be located in O (1) time.)An assigned target variable forms a singleton SCC, therefore the assigned variables are removed from the active cells ofthe constraint and cause almost no overhead.Triggering with dynamic partitioning. The constraint maintains a set τ of target variables and values to be processed. Whenthe constraint is notified of a domain change event, it adds the variable changed (for target variables) or the correspondingvalue (for cardinality variables) to τ . τ is cleared after the propagator executes and whenever search backtracks.P. Nightingale / Artificial Intelligence 175 (2011) 586–614597When the propagator is called, it iterates through τ and constructs a set of the cells to be propagated. A cell is prop-agated iff the cell contains a variable or value in τ . Propagation is performed on each cell in this set independently. Cellsthat are not propagated are almost cost-free. This scheme relies on the solver notifying the propagator of changed variables.If this information were not available, the propagator could discover the changed variables by iterating through each targetvariable domain, however this would add a quadratic cost and may outweigh any speed-up caused by the optimization.Dynamic partitioning affects the worst-case analysis of Tarjan’s algorithm. Without dynamic partitioning, the bound isΘ(δ), where δ is the number of edges in the residual graph. With dynamic partitioning, the bound is O (δ) because it onlyruns Tarjan’s algorithm on triggered cells of the constraint, in effect ignoring parts of the residual graph.3.5. Implementation of optimizations from the literatureIn this section I describe the implementation details of optimizations found in the literature, when these are not specifiedin the original papers.3.5.1. Incremental graph maintenanceIn this optimization, the variable-value graph is stored between calls and updated incrementally. This was first used byRégin [23] and is described in Section 3.3.2. For each vertex in the variable-value graph, an iterable list of adjacent verticesis required. The order of iteration is not important, but obtaining the next element should be O (1). Similarly removing anelement and testing its presence in the list should be O (1) operations. Restoring the list on backtracking should be as cheapas possible.The following representation is used, where each vertex is represented by a unique integer from a small range.List An array of vertices (integers), not backtracked.ListSize A single integer representing the size of the adjacency list. This must be backtracked.InvList An array mapping vertices to their positions in List. Not backtracked.This representation has the advantages of minimizing the backtracking memory and being directly iterable. The removaloperation for a vertex a is to swap it with the item at the end of the list (i.e. place it in position ListSize-1), and thento reduce ListSize by one, thus a disappears from the list. On backtracking, ListSize is restored and a reappears in the list,at the end. InvList allows a to be found in constant time, and is updated when the swap is performed. Thus the removaloperation is O (1). An item a is in the list iff InvList[a] < ListSize. This data structure is used by the solver Mistral [8].ListSize is backtracked by copying, as described in Section 2.5.The constraint is notified of each pruned value for all target variables. These events are used to maintain the adjacencylists and queue the constraint for propagation if necessary.Fixpoint reasoning.It may be helpful to perform fixpoint reasoning [27]. The EGCC propagator is idempotent if there are norepeated variables. When it prunes a value from a target variable, it will be notified later of the pruning but there is no needto run the propagator again. When using adjacency lists, the two relevant lists are updated immediately when the pruningoccurs. When the constraint is notified of a pruning, it tests whether the lists need to be updated. If not, the constraint isnot queued for propagation. Hence when using adjacency lists the propagator does some limited fixpoint reasoning.3.5.2. Priority queueing and triggeringEGCC places triggers on the upper and lower bounds of all cardinality variables. If it is using incremental graph mainte-nance, it is notified individually of each value that is removed from a target variable. Otherwise, it is notified of changes totarget variables, specifying the variable affected but not the value(s) removed.The EGCC is triggered in one of three ways depending on configuration:• Normal priority: The propagator is executed whenever it is notified of any event.• Low priority: The propagator is queued (added to the constraint queue if not already present) for any event.• Low priority with incremental graph: The propagator is queued for any event from a cardinality variable. For the targetvariables, the propagator is queued whenever it is notified of a value removal that is not already reflected in theadjacency lists.3.5.3. Important edges and dynamic triggersThe edges of the residual graph can be partitioned into important and unimportant (as discussed in Section 3.3.8). Onlythe removal of an important edge can cause pruning of the target variables. The number of calls to Régin’s algorithm canbe reduced by ignoring the removal of unimportant edges.The algorithm presented by Gent et al. [6] records the edges T that Tarjan’s algorithm uses in its internal proof that eachSCC is strongly connected. Tarjan’s algorithm performs a depth-first search (DFS) in the residual graph R. The edges of Rwhich are traversed by the DFS are included in T . The algorithm also maintains an integer named lowLink for each vertex.During the DFS, the lowLink values are updated using edges in the graph, and the criterion for identifying an SCC is based598P. Nightingale / Artificial Intelligence 175 (2011) 586–614Fig. 6. Finding important edges (T ) in Res(N(K ), f ). The flow f uses edges (x1, 1), (x2, 2) and (x3, 3). The DFS tree of Tarjan’s algorithm is shown inred dotted thick lines. The three other edges in T are shown in thick black lines. Three edges corresponding to domain values are unimportant. (Forinterpretation of the references to color, the reader is referred to the web version of this article.)on the lowLink value. For each vertex, the lowLink value may be changed several times, but only its final value is used inidentifying SCCs, therefore the edge used to obtain its final value is included in T . All other edges in R are not includedin T . This algorithm is also correct for EGCC; Fig. 6 shows an example of finding T for an EGCC constraint.While the edges in T remain in the residual graph, each component will remain strongly connected and therefore nopruning is possible. This method yields at most 2r + d edges that correspond to domain values. Compared to Katriel’stheoretical bound of 3r [12], there are d − r spurious edges. However the method is simple and fast, with only minimalinstrumentation of Tarjan’s algorithm and no change to the time bound.Two variants of AllDifferent were implemented by Gent et al. [6] based on important edges. The first variant useddynamic triggers (movable triggers that are restored on backtracking), moving at most 2r + d value triggers each timeTarjan’s algorithm is executed. Dynamic triggers are substantially more expensive than static triggers, and in experimentsthe cost of dynamic triggers outweighed the benefit in most cases.The second variant records the important domain values in backtracking arrays. When the propagator is triggered, itreturns immediately if no important value has been removed. The main cost is backtracking the arrays by block copying. Inexperiments this was a minor improvement with an average 6% speed-up. This approach is referred to as internal dynamictriggers because it simulates dynamic triggers within the constraint.The internal dynamic triggers method of Gent et al. [6] can be trivially adapted for EGCC. The algorithm for constructingset T is used unchanged. For each variable, a list of values is stored corresponding to edges in T . The lists are linked lists,stored in a block of backtracking memory with size O (3r + d). This allows O (1) append, quick iteration, and linear-timeclear.The propagator is changed in only two places. Tarjan’s algorithm is changed to record the T values into the backtrackingarray. When a cell with target variables Xcell is triggered, each changed variable xi ∈ Xcell is checked against its list of Tvalues. If no T values have been deleted, then the target variables are not pruned for that cell.6 Cardinality variables arepruned regardless. This approach is evaluated in Section 5.7.4. Pruning the cardinality variablesIn this section I describe three algorithms for pruning the cardinality variables. The first is a simple approach that countsvalues in the target domains, it does not make use of the flow network. The second approach is to use the simple algorithmand add an implied sum constraint. The third approach computes a maximum and minimum flow for a particular value. Inall cases, the algorithm described is run after pruning the target variables.These three methods are compared empirically in Section 5.8.4.1. A simple algorithmFor a domain value a, a simple upper bound is the number of target variables that have a in their domain. A lowerbound is the number of target variables that are assigned to a.For each value a, when not using incremental graph maintenance, the algorithm iterates through all target variables andcomputes the upper and lower bound. When using incremental graph maintenance, the upper bound is already known: itis the length of the adjacency list for a. The algorithm finds the lower bound by iterating through the adjacency list of aand counting assigned variables.6 If dynamic partitioning is not used, consider the constraint to have one cell containing all target variables.P. Nightingale / Artificial Intelligence 175 (2011) 586–614599If dynamic partitioning is used, the algorithm only processes values in those cells of the constraint that have beentriggered. Assigned variables are removed from active cells of the constraint, therefore values that only occur in assignedvariables will not be processed.The algorithm described above is stateless (i.e. requires no backtracking state) and quadratic (O (rd)), and it behaves wellcombined with dynamic partitioning and incremental graph maintenance. A stateless O (r + d) algorithm is possible butpreliminary experiments did not show any benefit, and this algorithm does not partition dynamically, therefore I disregardedit. It is also possible to construct a stateful O (d) algorithm, by maintaining the number of variables assigned to each valuein a set of backtracking integers. I avoided this because it requires backtracking memory.4.2. An implied sum constraintA second approach is to use the simple algorithm and add an implied sum constraint over the cardinality variables. Thetotal occurrences of all values must equal the number of target variables.egcc(X, V , C) ∧(cid:8)C = rThis implied constraint is sound iff all values in the domains of target variables are in V , and therefore have a corre-sponding cardinality variable.This is the approach used by Gecode [27].7 However, in Gecode the definition of EGCC is slightly different: the variablesin X are only allowed to take values in V . Therefore the sum constraint is always sound in Gecode.4.3. A flow network algorithmQuimper et al. [18] proposed an algorithm based on the flow network N(K ). For a value a and cardinality variable ca, thealgorithm finds a maximum flow containing the minimum occurrences of a. This is used to prune the lower bound of ca.Similarly, it finds a maximum flow containing the maximum occurrences of a to prune the upper bound of ca.This is an expensive method, but it provides the maximum possible pruning under the assumption that domains ofcardinality variables are an unbroken interval.4.3.1. Pruning lower boundsThe algorithm described by Quimper et al. is as follows. Take an existing maximum flow f that respects the upperbounds for all values. Remove all units of flow that pass through value-vertex a, to form the reduced flow fa. Similarly,remove vertex a and all incident edges from N(K ) to form a new network N(K )a. Using the Ford–Fulkerson algorithm on(cid:11)network N(K )a, augment fa to find a maximum flow fa from s to t.(cid:11)a represents a maximum assignment to the target variables X such that a is not used and all values are within their| occurrences of a, thereforeupper cardinality bounds. Therefore to complete the assignment to X , there must be r − | fca (cid:3) r − | f|.(cid:11)af(cid:11)aThe implementation makes use of the transpose graph and is almost identical to that described in Section 3.4.1, withthree changes: the algorithm does not stop when it encounters a variable-vertex with no augmenting path; the graph N(K )Tais used in place of N(K )T ; and the algorithm halts if the size of the flow reaches r − ca, because in this case it is not possibleto prune ca.The time required to prune all cardinality lower bounds is O (r2d) [18], or O (rδ) where δ is the number of edges inN(K ). This is because the algorithm seeks at most r augmenting paths.4.3.2. Pruning upper boundsTo find a new upper bound for value a, the flow through edge (s, a) in N(K ) is maximized while observing the lowercardinality bound for all other values. Quimper et al. prove that this can be done in O (r2.66) time. To find the upper boundof a, they start with a non-maximal flow fl with exactly cb occurrences of each value b (therefore the maximum numberof free variables). The goal is to maximize the flow using paths from a to a free variable. If a value-vertex is not used in flthen it cannot be part of a path (excluding s) from a to a free variable in Res(N(K ), fl). The number of reachable verticesis no more than 2r + 1. The authors identify the network as a special case and cite a proof that the maximum flow can befound in O (r2.66) time.The implementation used here is simpler. It begins with a maximum flow f that respects the lower bounds for all values.To find the upper bound for a, the algorithm maximizes the flow through (s, a) by finding augmenting paths with a BFS inN(K ) (excluding s) starting at a and ending at a value-vertex b where f (s, b) > cb (i.e. the flow through b is greater thanits lower bound). The path is applied to increase the flow through a and decrease b. The algorithm halts if the size of theflow through a reaches ca, because in this case it is not possible to prune ca. The final flow through edge (s, a) is the newupper bound for ca.7 Guido Tack, personal communication.600P. Nightingale / Artificial Intelligence 175 (2011) 586–614In common with Quimper’s algorithm, the number of reachable vertices is 2r + 1 or fewer. To prune all capacity vari-ables C , the time bound is O (|C|r3), a factor of r1/3 less efficient.5. Experimental evaluationIn this section I describe the context of the experimental evaluation. Then I present two groups of experiments. FirstI compare various algorithms and optimizations for pruning the target variables, in experiments one to five. Secondly,I compare algorithms for pruning the cardinality variables in experiment six. Finally, in Section 5.10, the best propagationmethod for the target variables is compared against a careful but unoptimized implementation of Régin’s algorithm. Also, thebest EGCC propagator is compared against the decomposition of EGCC into a set of occurrence constraints, demonstratingthe utility of EGCC as a global constraint.5.1. Benchmark setIn this section I describe the problem classes and instances used to compare propagation algorithms.5.1.1. Car sequencingThe car sequencing problem [9] (prob001) is to sequence cars on a conveyor through a factory. There are a number ofoptional parts that may be fitted to the cars, and each optional part has a corresponding machine which fits the part. Foran option i, the machine cannot accept more than pi cars in every qi . Therefore, in every contiguous subsequence of lengthqi there must be no more than pi cars requiring the option.There are a number l of different types of car, where each type has a set of options that it requires. A fixed number ofeach type is required in the sequence. Finally, the length n of the sequence is given.Three models for this problem are presented below. They all share a common core. There is an explicit representationseq of the sequence. This is an array of length n of variables with domain {0 . . . l − 1} (representing the type of car). AnEGCC constraint is placed on seq, to enforce the required number of each type of car.All models also contain a two-dimensional array optused of Boolean variables. For each sequence index j and option k,optused[ j, k] indicates whether the car at position j requires option k. Each element optused[ j, k] is connected to seq[ j] bya binary table constraint.Model A. Régin and Puget presented an encoding of the capacity constraints for the machines as a set of EGCC constraints[25]. It is a very complex model and I do not reproduce it in full.For each option i, there are n + qi − 1 subsequences of seq to consider. For each subsequence, we need to state that nomore than pi of the cars require option i. This is done using n × qi extra variables, and qi EGCC constraints (each with ntarget variables).There is also a cardinality variable for each subsequence, giving the number of cars in the subsequence that do notrequire option i. Two consecutive subsequences of the same length overlap by qi − 1 cars, therefore the correspondingcardinality variables cannot differ by more than 1, and the difference is easily determined. A set of logic and arithmeticconstraints are added to capture this fact.The key advantage of Régin and Puget’s model is that the EGCC constraints combine subsequence capacities with con-straints on the whole sequence. Each car type that requires option i is represented in the auxiliary variables, and therequired number of that car type is enforced by all the EGCC constraints.Régin and Puget do not report the level of consistency that was used for the EGCC cardinality variables. However, oninstance 2, Régin and Puget report 9355 fails in ILOG Solver [25]. Minion performs 9452 left branches (using the simplecardinality pruning algorithm in Section 4.1). Instance 1 is also similar (0 fails in ILOG Solver, 113 left branches in Minion).This suggests that the model, propagation and variable ordering may be equivalent.Model B.In car sequencing, a single sequence constraint [33] may be used to represent the capacity constraints for oneoption. The sequence constraint for option i is given parameters pi and qi , and is posted on variables optused[i, ∗]. Itrestricts the number and position of occurrences of value 1 in optused[i, ∗].Van Hoeve et al. [33] proposed an encoding of the sequence constraint as a regular constraint (i.e. a constraint thatrecognizes a regular language). In this case the language is the set of all assignments to optused[i, ∗] that satisfy the capacityconstraints for option i. The corresponding deterministic finite automaton (DFA) has O (2qi ) states. A cost parameter is addedto fix the total number of cars with option i (i.e. to fix the number of 1’s in optused[i, ∗]), and the cost-regular propagator [3]is used to enforce GAC on the resulting constraint.Minion does not have cost-regular or regular propagators, therefore the constraint is encoded into table constraints. First,the DFA is augmented with a counter that counts the number of 1’s in the sequence. Only sequences with the correctnumber of 1’s are accepted by the augmented DFA. The number of states is increased to O (n2qi ). The augmented DFA isencoded into a set of ternary table constraints as described by Quimper and Walsh [21], and GAC is enforced on these tableconstraints. This is equivalent to enforcing GAC on the original cost-regular constraint.P. Nightingale / Artificial Intelligence 175 (2011) 586–614601Table 1EFPA example with v = 5, q = 3, λ = 2, d = 4.c1c2c3c4c5000000112210212120212120122110Model AB. Model AB is the combination of models A and B. This is very similar to model ‘(C) A + REG with cost’ in vanHoeve et al. [33].Variable and value ordering. All three models use the variable and value ordering by Régin and Puget [25]. First, the optionsare ordered according to a measure of how tightly constrained they are. For each option i, the measure uses the demandof i, denoted ki , which is the number of cars in the sequence that require it. The slack of option i is n − qi(ki/pi) (wherelow slack indicates the option is tightly constrained8).The optused variables are searched. First, the options are ordered by slack, with the least slack first. For each option, thevariables are branched from the middle out (i.e. at each step the unassigned variable closest to the middle of the sequenceis selected). Finally, the value order is 1 then 0.80 instances were used. Instances 0 to 4 are from Régin and Puget,9 and 5 to 79 are the other instances given on CSPLib(prob001) [9].5.1.2. Magic sequenceThe magic sequence problem [9] (prob019) is to find a sequence of length n such that element i in the sequence is thenumber of occurrences of i in the sequence. It is modelled as a list X of n variables with domain {0 . . . n − 1}. There is oneconstraint: EGCC( X, (cid:2)0 . . . n − 1(cid:3), X). The variables are searched in index order, and values are explored in ascending order.Instances were generated for n ∈ {20, 30, 40, 50, 100, 150, 200, 300}.5.1.3. Equidistant frequency permutation arrays (EFPAs)The EFPA problem [11] is to find a set (often of maximal size) of codewords, such that any pair of codewords areHamming distance d apart. Each codeword (which may be considered as a sequence) is made up of symbols from thealphabet {1, . . . , q}, with each symbol occurring a fixed number λ of times per codeword. A fourth parameter v is thenumber of codewords in the set. Typically v would be maximized. Table 1 shows an example of an EFPA.The problem is modelled as a two-dimensional array of variables where each row represents a codeword. The model isgiven by Huczynska et al. [11] (the non-Boolean model with the implied constraint set). The variable and value orderingdescribed there is used. The 24 instances of EFPA used in the experiments in [11] are also used here, with two added: d = 4,q = λ = 5, v ∈ {11, 12}. This provides a mixture of 13 satisfiable instances and 13 unsatisfiable or unknown instances.Each row of the model has an EGCC constraint with qλ target variables to enforce λ occurrences of each symbol. Thereare also EGCC constraints with λ target variables used in the implied constraints.5.1.4. Round-robin sports schedulingThe round-robin sports scheduling problem [9] (prob026) is to schedule games among n teams on n/2 pitches overn − 1 weeks. The model and variable and value ordering is given in [6], and EGCC constraints are added to enforce therequirement that each team plays on each pitch at most twice. This gives n/2 EGCCs with 2(n − 1) target variables andcapacities 0 . . . 2 for all values. Instances were generated with n ∈ {10, 12, 14, 16}.5.2. Experimental setupFor all experiments I use Minion as described in Section 2.5.10 The instances are not preprocessed. I used a timeout of1800 s. The experiments were run on a Linux (Ubuntu 9.10) server with two Intel Xeon quad-core E5520 CPUs clocked at2.27 GHz and 12 GB of RAM.Minion performs a binary search where the left branch assigns a variable. It counts left branches. The speed of search ismeasured by dividing the number of left branches by time taken, this is referred to as branch rate.In this setup, timings (and branch rates) exhibit some variation. To measure the variation, I used the PriorityQ-IncMatch-IncGraph propagator (Section 5.5), and measured the branch rate twice for each benchmark (with a timeout of 1800 s).8 Régin and Puget claim that negative slack means the capacity constraint for the option cannot be satisfied [25]. This is not true because the ends ofthe sequence are a special case. Consider a problem where n = 8 and an option i has parameters pi = 2, qi = 3 and demand ki = 6. The slack for option iis −1, and the capacity constraint can be satisfied with the optused sequence (cid:2)1, 1, 0, 1, 1, 0, 1, 1(cid:3).9 For instances 0 and 3 the option ordering derived from slack is not the same as that reported by Régin and Puget [25], which may have been adjustedby hand.10 Source code for the solver is available at http://www.cs.st-andrews.ac.uk/~pn/egcc/.602P. Nightingale / Artificial Intelligence 175 (2011) 586–614For each instance, the absolute difference between the two rates was divided by the smaller rate to obtain a proportionaldifference. The mean of these values is 0.03 (i.e. the larger branch rate is 3% larger than the smaller one) and the maximumis 0.50. Those instances that completed in less than 0.1 s showed the most variation; excluding those, the mean is 0.03 andmaximum is 0.17.In all experiments below, the median of three runs is used. For each experiment comparing algorithms A and B, todetermine statistical significance I used the Wilcoxon paired signed-rank test implemented in R [22]. The branch ratesof A and B are measured for all benchmark instances. The null hypothesis is that the branch rates are drawn from thesame distribution (i.e. A and B run at the same speed). The difference between A and B is deemed to be significant if theprobability of the null hypothesis is less than 0.01.The various implementations of EGCC were extensively tested and de-bugged, and all variants report the same branchcount on instances that complete within the time-out.5.2.1. Solver architectureThe EGCC propagators make use of some Minion features that may not be available in all solvers. Perhaps the mostimportant is notification of which variables have been changed, and (when using the incremental graph) which valueshave been pruned. This granularity of events is widely available however (for example in Gecode via advisors [15] andChoco [14]).Another important consideration is memory architecture. Minion allows propagators to have both backtracked and non-backtracked memory. The backtracked memory for all propagators is blocked together, and cannot be allocated or freedduring search. Memory is backtracked by copying the block, which is very efficient when the amount of backtracked memoryis small and static. Memory architecture affects most of the optimizations for EGCC, therefore experimental results thatare marginal could be reversed with a different architecture. Different data structures may be required with a differentarchitecture.In contrast to Minion, Gecode backtracks all state, and it does so by copying. When search branches Gecode traversesa tree of objects (including constraints and variables) and copies each individually. This architecture has very differentproperties to Minion. Entailment (Section 3.3.9) is a case in point: in Gecode, if a constraint is entailed, it is removed fromthe tree (and its triggers are removed) thus the cost of copying the constraint and its triggers is removed. In Minion triggersare not copied when search branches, and any backtracking state the constraint has cannot be de-allocated. Therefore thepotential gain from detecting entailment is much less in Minion. Gecode employs entailment for GCC.115.3. Experiment one: comparing Quimper’s and Régin’s algorithmsIn this section I compare the two basic algorithms for pruning the target variables. To do this, various other choicesmust be made. These choices are mainly based on the current state-of-the-art from the literature. I use a priority queuewhere EGCC has a low priority (Section 3.5.2),incremental matching (Section 3.3.1), and incremental graph mainte-nance (Section 3.5.1), including fixpoint reasoning. I do not use dynamic partitioning (Section 3.4.2), assigned variableremoval (Section 3.3.6), dynamic triggers (Section 3.5.3), or the transpose graph (Section 3.4.1). The weakest algorithmis used to prune cardinality variables (Section 4.1). The two algorithms are referred to as Baseline-Régin and Baseline-Quimper.Fig. 7 shows the experimental results comparing Régin’s algorithm to Quimper’s. The results are strongly in favour ofRégin’s algorithm, despite the better worst-case bound of Quimper’s algorithm. The results are statistically significant, witha mean speed-up of 1.62 times. Recall that the speed of the whole solver is measured, so 1.62 is a lower bound on the truespeed-up of the EGCC propagator.The performance of the two algorithms is closest on car sequencing model B. These instances contain only one EGCCconstraint and a large set of table constraints (typically over 1000) and other constraints. Therefore the potential to speedthem up by improving the EGCC algorithm is limited.Quimper’s algorithm intends to speed up the first stage of the process — computing a maximum flow or matching — byusing a more sophisticated algorithm. The second stage is almost identical to Régin’s, except that it must be performed twicein Quimper’s algorithm. To investigate further, I profiled Baseline-Régin using Callgrind [35]. Table 2 shows the proportion ofCPU instructions spent in the flow algorithm and Tarjan’s algorithm. The solver was profiled on one easy satisfiable instancefrom each problem class. For all six instances, the algorithm spends over 60% of its CPU instructions in Tarjan’s algorithm,and less than 15% in the flow algorithm. This is consistent with the empirical results: if Tarjan’s algorithm is the moreexpensive stage, it would be counterproductive to run it twice in order to speed up the first stage.In all six cases, the proportion of instructions spent in the maximum flow algorithm is surprisingly low, with the bulkof instructions spent in Tarjan’s algorithm. While the flow algorithm has a worse upper bound, Tarjan’s algorithm alwaysreaches its upper bound (Section 3.1.5).Based on these results, only Régin’s algorithm is used for the rest of the experiments.11 Guido Tack, personal communication.P. Nightingale / Artificial Intelligence 175 (2011) 586–614603Fig. 7. Speedup of Baseline-Régin compared to Baseline-Quimper. The graph is a scatterplot, with each point comparing results on a single instance. Thex-axis represents the run time of Quimper’s algorithm to solve the instance. The y-axis gives the speedup obtained by using Régin’s algorithm instead ofQuimper’s algorithm. A ratio of 1 indicates that the two methods run at the same speed, with ratios higher than 1 indicating that Régin’s algorithm isfaster, and ratios less than 1 indicating that Quimper’s algorithm is faster. The ratio is calculated by dividing the branch rate with Régin’s algorithm bythat with Quimper’s algorithm. In this graph we can see that Régin’s algorithm almost always performs substantially better than Quimper’s algorithm. Allsubsequent graphs labelled ‘Speedup of X compared to Y’ follow the same conventions, where in this case X = Baseline-Régin and Y = Baseline-Quimper.Table 2Instructions spent in the flow algorithm and in Tarjan’s algorithm, as a proportion of the propagator Baseline-Régin. Toavoid inlining, the solver was re-compiled without optimizations.InstanceEFPA-4-4-4-8Magic sequence 40Car seq A instance 1Car seq B instance 1Car seq AB instance 1Sports scheduling 10Searchnodes27,10014511311111136,926Calls86,82440,15764001166064324,860Proportionin flowalgorithm14%1.4%5.7%7.3%5.8%7.4%Proportionin Tarjan’salgorithm71%67%65%92%64%76%5.4. Experiment two: making use of the transpose graphIn Section 3.4.1 I described a change to Régin’s algorithm intended to speed up the computation of a maximum flow.To evaluate this I use the same experimental set-up as in the previous experiment, and simply compare Régin’s originalalgorithm with the variant. Fig. 8 is a plot of the results. It appears that measurement noise hides the difference betweenthe two algorithms. The difference is not statistically significant.To obtain more exact data, I profiled the solver using Callgrind [35]. The profiler provides the total number of in-structions spent in a function and other functions it called. Table 3 shows instruction counts for finding or repairingthe maximum flow with and without the transpose graph. The proportion compared to the whole propagator is alsogiven.Using the transpose graph does give substantial gains in some cases. For example on the car sequencing A instance,it is 33% better. However, on that instance, the overall gain is very low because the propagator only spends 5.7% of itsinstructions in the maximum flow algorithm.Based on the results of this experiment, from here on I use only Régin’s algorithm with the transpose graph.5.5. Experiment three: standard optimizations of Régin’s algorithmIn this section I experiment with optimizations found in the literature, and investigate whether they are worthwhile. Thefollowing variants of Régin’s algorithm are used.604P. Nightingale / Artificial Intelligence 175 (2011) 586–614Fig. 8. Speedup of transpose compared to Régin’s original algorithm.Table 3Instruction counts for finding or repairing a maximum flow, with and without the transpose graph. To avoid inlining, thesolver was re-compiled without optimizations.InstanceEFPA-4-4-4-8Magic seq 40Car seq A 1Car seq B 1Car seq AB 1Sports sched 10Searchnodes27,10014511311111136,926Calls86,82440,15764001166064324,860StandardRégin’salgorithm267 m315 m290 m4.63 m284 m1724 mTranspose247 m297 m193 m4.62 m188 m1201 mInstructionsin max flowfor standard14%1.4%5.7%7.3%5.8%7.4%Simple The algorithm described in Section 3.1 with the transpose graph optimization (Section 3.4.1), run at normal priority.The weakest algorithm is used to prune cardinality variables (Section 4.1).PriorityQ The Simple algorithm run at low priority as described in Section 3.5.2.PriorityQ-IncMatch PriorityQ plus incremental matching as described in Section 3.3.1.PriorityQ-IncMatch-IncGraph PriorityQ-IncMatch plus incremental graph maintenance as described in Section 3.5.1.Fig. 9 shows that it is worthwhile to use the priority queue. All instances are faster with PriorityQ compared to Simple.Even Magic Sequence instances (with one constraint) benefit from PriorityQ because the propagator is called once formultiple variable events.Fig. 10 shows that it is worthwhile to use incremental matching. Almost all instances are faster with PriorityQ-Incmatchcompared to PriorityQ, with very substantial speedups in some cases.Finally, Fig. 11 shows that in most cases it is worthwhile to use PriorityQ-IncMatch-IncGraph compared to PriorityQ-IncMatch. The main exception is the magic sequence problem, where all eight instances are slower with PriorityQ-IncMatch-IncGraph.For all three comparisons, the difference is statistically significant. Based on these results, I use PriorityQ-IncMatch-IncGraph as a baseline for all subsequent experiments.The results for PriorityQ and IncMatch are broadly similar for AllDifferent [6]. However, for AllDifferent the speedup forIncGraph is less substantial [16]. Also, when using dynamic partitioning, IncGraph is detrimental for most instances [16].5.6. Experiment four: assigned variable removal and dynamic partitioningIn this experiment I evaluate assigned variable removal (AVR) and dynamic partitioning. These two optimizations areclosely related: dynamic partitioning subsumes AVR, because it partitions assigned variables into a singleton cell. I comparethe following three variants experimentally.P. Nightingale / Artificial Intelligence 175 (2011) 586–614605Fig. 9. Speedup of PriorityQ compared to simple.Fig. 10. Speedup of PriorityQ-IncMatch compared to PriorityQ.Baseline The same as PriorityQ-IncMatch-IncGraph in the previous section.Baseline-AVR Baseline with assigned variable removal (Section 3.3.6).Baseline-Cell Baseline with dynamic partitioning as described in Section 3.4.2.The second stage of Régin’s algorithm (i.e. Tarjan’s algorithm) is often more expensive than the first stage (Table 2). Asdiscussed in Section 3.4.2, dynamic partitioning improves the time bound of Tarjan’s algorithm from Θ(δ) to O (δ). AVRdoes not have this effect.Fig. 12 shows that it is worthwhile to remove assigned variables in most cases, and for some of the most difficultinstances. In the best case, it sped up the solver by 2.5 times, and in the worst case slowed it down by about 35%.The comparison of Baseline-Cell against Baseline is plotted in Fig. 13. In this case the results are much more pronouncedthan AVR, with a speed up of 5.6 times in the best case. Car sequencing models A and AB and the magic sequence problembenefit substantially from dynamic partitioning. EFPA and sports scheduling show a less substantial benefit. Car sequencing606P. Nightingale / Artificial Intelligence 175 (2011) 586–614Fig. 11. Speedup of PriorityQ-IncMatch-IncGraph compared to PriorityQ-IncMatch.Fig. 12. Speedup of Baseline-AVR compared to Baseline.model B also shows benefit, with 67/80 instances running faster with dynamic partitioning, even though there is only oneEGCC constraint. The mean average speedup is 1.56.For both comparisons, the difference is statistically significant.For AllDifferent, dynamic partitioning is very effective [6], yielding a mean speedup of 2.98 times (with the assignmentoptimization). Dynamic partitioning superficially appears to be more effective for AllDifferent, however the two benchmarksets are entirely different.5.7. Experiment five: internal dynamic triggersAll previous optimizations were intended to speed up some part of the propagator. In contrast, internal dynamic triggers(IDT, Section 3.5.3) is intended to reduce the number of times that Régin’s algorithm is called.P. Nightingale / Artificial Intelligence 175 (2011) 586–614607Fig. 13. Speedup of Baseline-Cell compared to Baseline.Table 4Calls to Régin’s algorithm comparing Baseline to Baseline-IDT.InstanceEFPA-4-4-4-8Magic sequence 40Car seq A instance 1Car seq B instance 1Car seq AB instance 1Sports scheduling 10Searchnodes27,10014511311111136,926Calls withBaseline86,82440,15764001166064324,860Calls withBaseline-IDT57,260811618561141662196,472Dynamic partitioning reduces the cost of pruning target variables, therefore it reduces the potential for internal dynamictriggers to save time. Therefore, I evaluate internal dynamic triggers both with and without dynamic partitioning. Fourvariants are used.Baseline The same as Baseline in the previous section.Baseline-IDT Baseline with internal dynamic triggers (Section 3.5.3).Baseline-Cell Baseline with dynamic partitioning.Baseline-Cell-IDT Baseline-Cell with internal dynamic triggers.Table 4 shows the number of calls to Régin’s algorithm with Baseline and Baseline-IDT for the six easy problems usedpreviously. It shows that the dynamic triggers approach can substantially reduce the number of calls. The most encouragingis magic sequence 40 where the number of calls is reduced by 80%.Fig. 14 shows the empirical results comparing Baseline-IDT to Baseline. The magic sequence problem benefits the mostfrom IDT, but this could be a red herring because of its very unusual structure. For other problems, the difference rangesfrom 0.5 to 1.6 times faster. Overall the mean speedup is 1.15. This indicates that the overhead of maintaining and back-tracking the internal dynamic triggers cancels out the benefit in most cases. Although the two algorithms are similar, thedifference is statistically significant.As expected, dynamic partitioning reduces the benefit of dynamic triggers. Baseline-Cell-IDT is 7% slower on averagethan Baseline-Cell on the benchmarks. Baseline-Cell-IDT was faster for 69 of 278 instances. The maximum speed-up wasjust 7%. The difference is statistically significant.As discussed in Section 3.5.3, the cost of collecting the trigger values is negligible, so it seemed likely that IDT wouldhelp, particularly with long constraints. However, this is not what was observed. Dynamic triggers were also unsuccessful inAllDifferent when applied with dynamic partitioning [6].608P. Nightingale / Artificial Intelligence 175 (2011) 586–6145.8. Experiment six: pruning the cardinality variablesFig. 14. Speedup of Baseline-IDT compared to Baseline.In this experiment I compare the three methods of pruning the cardinality variables described in Section 4. Dynamicpartitioning (Baseline-Cell) has been found to be a substantial improvement over Baseline, therefore I use Baseline-Cell andcombine it with the three methods as follows.Baseline-Cell The same as Baseline-Cell in the previous section. This employs the simple cardinality algorithm described inSection 4.1.Baseline-Cell-Sum Baseline-Cell with the additional sum constraint as described in Section 4.2. For all benchmarks, the sumconstraint is correct.Baseline-Cell-Flow Baseline-Cell with the flow cardinality algorithm described in Section 4.3.The three variants perform different levels of propagation (and are ordered from least to most powerful). In this exper-iment I compare run times rather than branch rates. I also do not evaluate on those instances where the cardinalities areconstants. This leaves car sequencing models A and AB, magic sequence, and EFPA. The Wilcoxon paired signed-rank testwas applied to run time rather than branch rate, with the result that each pair of methods are significantly different.Since Baseline-Cell-Sum is an improvement of Baseline-Cell, I will compare these first. Fig. 15 shows that the usefulnessof the sum constraint depends very much on problem class. On magic sequence, it is consistently very useful. It is alsouseful on the majority of car sequencing problems where neither variant timed out.As shown in Table 5, Baseline-Cell-Sum is able to solve one additional instance (magic sequence 300) within the timelimit. For 20 instances of 109 it reduced the number of search nodes. These 20 instances consist of all eight magic sequences,the three unsatisfiable EFPA instances where d = 3, and nine of car sequencing model A.There are a number of car sequencing model AB instances where Baseline-Cell-Sum reached the same fixed point fasterat the root node. However, only one of these instances is also faster during search.In conclusion, adding the sum constraint is low-risk and is sometimes very helpful, and would be a good default choicein place of Baseline-Cell.Next Baseline-Cell-Flow is compared to Baseline-Cell-Sum. Table 5 shows that Baseline-Cell-Flow is more robust, solvingtwo extra instances within the time limit (car sequencing 12 with models A and AB). Baseline-Cell-Flow explores fewernodes for 21% of 111 benchmarks. The two instances that are solved only by Baseline-Cell-Flow appear in the upper rightcorner of Fig. 16.Baseline-Cell-Flow can be inefficient, as shown by Fig. 16: in the worst case it is 48 times slower than Baseline-Cell-Sumon easy car sequencing instances. The bulk of this slow-down is at the root node: surprisingly the solver takes over 45 sto reach the fixed point for most of the car sequencing benchmarks. In contrast Baseline-Cell-Sum never takes over 1.31 sat the root node. For the first propagation of the EGCC, dynamic partitioning has no effect so all cardinality variables arepruned. For car sequencing this is very costly, but not for EFPA and magic sequence.If the root node is excluded, in the worst case Baseline-Cell-Flow takes 5.73 times longer than Baseline-Cell-Sum (thiswould be 0.17 on the y-axis of Fig. 16).P. Nightingale / Artificial Intelligence 175 (2011) 586–614609Fig. 15. Time comparison between Baseline-Cell-Sum and Baseline-Cell. The x-axis is the time taken by Baseline-Cell and the y-axis is the proportion oftotal times, Baseline-Cell divided by Baseline-Cell-Sum. The timeout was 1800 s.Table 5For each cardinality algorithm: the number of instances solved; and the number of instances with a reduced nodecount vs the weaker algorithms.Instancessolved (of 194)Saved nodesvs SimpleSaved nodesvs SumBaseline-CellBaseline-Cell-SumBaseline-Cell-Flow108109111–2033––23Both Sum and Flow are hugely more efficient than Baseline-Cell on the magic sequence problem. In both cases, this ismainly not because they explore fewer branches. Taking magic sequence 100 as an example, Baseline-Cell solves it in 385branches, with 675,192 executions of the EGCC propagator (average 1754 calls per branch). This is extremely pathologicalbehaviour for an instance with only one constraint, and is caused by the cardinality variables being the same as the targetvariables. Baseline-Cell-Sum reduces this pathological behaviour, solving it in 242 branches and 41,428 calls to the propaga-tor (average 171 calls per branch). The speed-up of 47 times is much greater than the 1.59 times reduction in the numberof branches, and greater than the 16 times reduction in the number of calls. Therefore adding the sum constraint reducesthe average time taken per call to EGCC, as well as reducing the number of calls.In conclusion, Baseline-Cell-Flow is risky, frequently slowing the solver down substantially and does not appear to be agood default choice. However, it is able to solve more instances within the half-hour time limit.5.9. Experiment seven: comparing EGCC to AllDifferentGiven an efficient implementation of EGCC, is it worthwhile implementing GAC AllDifferent? The Baseline-Cell propagatorwas adapted by removing the cardinality variables and using {0, 1} as the cardinality for all values. The adapted Baseline-Cell is compared to the SCC-AssignOpt variant of AllDifferent described by Gent et al. [6], using the benchmarks from thatpaper. The difference is statistically significant, AllDifferent is 1.31 times faster on average.5.10. Evaluating all optimizations combinedIn the previous sections, I individually evaluated many efficiency measures for pruning the target variables of EGCC.In this section, I consider the effect of them all together. When using the simple cardinality algorithm, the most efficientvariant is Baseline-Cell. Baseline-Cell is 51.5 times faster than Simple on average, with a maximum speedup of 237 times.However, Simple does not include the priority queue optimization, which is ubiquitous and external to the propagator.Fig. 17 compares Baseline-Cell with PriorityQ. The mean speed up is 4.11 times, and the maximum is 20.9. Baseline-Cell isa substantial improvement over PriorityQ, and this underlines the importance of implementing EGCC well.Fig. 18 is a plot of the nodes explored per second by Baseline-Cell. This gives an idea of the speed of the propagatoron different classes of instances. The EFPA instances are very fast, exceeding 20,000 nodes per second in all cases, which is610P. Nightingale / Artificial Intelligence 175 (2011) 586–614Fig. 16. Time comparison between Baseline-Cell-Flow and Baseline-Cell-Sum. The x-axis is the time taken by Baseline-Cell-Sum and the y-axis is theproportion of total times, Sum divided by Flow. The timeout was 1800 s.Fig. 17. Speedup of Baseline-Cell compared to PriorityQ.perhaps remarkable when maintaining GAC-On-X. In this case the constraints are quite short. For example on the instance(cid:2)4, 4, 4, 9(cid:3) the longest EGCC has 16 target variables.Magic sequence is by far the slowest class, with a single EGCC constraint whose arity is the length of the sequence. Thisclass has extremely pathological behaviour with Baseline-Cell, making a very large number of calls to the propagator toreach a fixed point after each branch (e.g. on instance 100, average 1754 calls per branch). This is caused by the target andcardinality variables being the same, therefore the constraint triggers itself many times before reaching a fixed point.Finally, I compare one of the best EGCC variants (Baseline-Cell-Sum) against the decomposition of the EGCC constraintinto a set of occurrence constraints. The decomposition is as follows. For each constraint egcc( X, V , C), for each value a ∈ Vand corresponding cardinality variable ca ∈ C , an occurrence constraint occurrence( X, a, ca) is created, stating that ca is thenumber of occurrences of a in X . The decomposition is referred to as Occurrence.Fig. 19 compares Baseline-Cell-Sum against Occurrence. Many instances were solved by Baseline-Cell-Sum and not byOccurrence, these are at the right-hand side of the plot. The speed-up can be many orders of magnitude, with the mostextreme point being magic sequence 20, where Occurrence times out and Baseline-Cell-Sum takes 0.01 s. Baseline-Cell-SumP. Nightingale / Artificial Intelligence 175 (2011) 586–614611Fig. 18. Plot of Baseline-Cell branches per second. The x-axis is Baseline-Cell runtime, and the y-axis the number of branches searched per second.Fig. 19. Time comparison between Baseline-Cell-Sum and Occurrence. The x-axis is the time taken by Occurrence, and the y-axis is the proportion of totaltimes, Occurrence divided by Sum. The timeout was 1800 s.appears to be faster for all problem classes except EFPA. Occurrence is faster for 15 instances (all from the class EFPA) andsolves 21 in total, whereas Baseline-Cell-Sum solves 109. The proportion of run times ranges from 0.6 to 180,000.For most (14/15) cases where Occurrence wins, the number of branches is within 10% of Baseline-Cell-Sum: the EGCCpropagation is ineffective. However, even with almost the same size of search tree, EGCC slows down the solver by only1.66 times or less.6. Experimental conclusionsIn this section I summarize the most important outcomes of the experiments.612P. Nightingale / Artificial Intelligence 175 (2011) 586–6146.1. The basic algorithm for pruning target variablesFirst of all, there are two published algorithms for enforcing GAC-On-X: Régin’s algorithm and Quimper’s algorithm.Quimper’s algorithm has a tighter worst-case bound, and therefore seems to be a more attractive choice. However, I foundRégin’s algorithm to be more efficient by a substantial margin (1.62 times faster on average).The second stage of Régin’s algorithm (i.e. Tarjan’s algorithm) appears to be much more expensive than the first, based onsolver profiling. This counterintuitive observation may inform further optimizations, and explains why Quimper’s algorithmis less efficient than Régin’s.Despite the experimental findings it is possible that Quimper’s algorithm will out-perform Régin’s on very large con-straints. However given the size of problems that can be solved in real life, and the small asymptotic difference betweenthe two algorithms, the constants are more important than asymptotic behaviour.Also, it is not clear which maximum flow algorithm should be used with Régin’s algorithm. Only one was experimentedwith: Ford–Fulkerson with breadth-first search. Depth-first search and Dinic’s algorithm are other possibilities that cannotbe ruled out.6.2. Optimizations to the basic algorithmThe results show there is huge benefit from using the following optimizations: using a priority queue and running EGCCat a low priority; incremental matching; incremental graph maintenance; and dynamic partitioning. The results on theseoptimizations are substantial enough that they are unlikely to be reversed by different implementation choices or the studyof different instances, and these optimizations remain effective when combined.It is not possible to give a definitive order of importance of these optimizations, because the experiments were cu-mulative. However, it seems likely that the priority queue is by far the most important, and among the others dynamicpartitioning is particularly important because it showed the most benefit for the largest and hardest instances.Using the transpose graph to compute the maximum flow was not measurably faster, although it was shown to be usingfewer CPU instructions by profiling.Some of the results depend on the context in which EGCC is used. In particular, internal dynamic triggers (IDT) werenot of much benefit on the benchmarks (and when combined with dynamic partitioning, actually slow the solver down).In our benchmarks, the target domains are small: in all cases, smaller than or equal the number of target variables. IDT isexpected to work best when target domains are large, and hence the proportion of important values is small. Therefore, IDTcould be considered if an EGCC constraint will be used with very large domains, to ameliorate the overheads in this case.6.3. Algorithms for pruning the cardinality variablesThe findings for pruning cardinality variables are straightforward. Adding an implied sum constraint over the cardinalityvariables has a low overhead and is frequently helpful in reducing the number of branches or the time to reach the fixpoint.Using Quimper’s flow-based algorithm is expensive, slowing down a substantial number of the benchmarks, therefore Icannot recommend it as a default choice. However it is more powerful than the simple algorithm with the sum constraint,solving two extra instances within the half-hour time limit.6.4. Comparing against OccurrenceBaseline-Cell-Sum never takes more than twice as long as Occurrence to solve any of the benchmarks, and is typicallyorders of magnitude faster. This is encouraging, and suggests that Baseline-Cell-Sum could perhaps be used as a defaultchoice by an automated modelling assistant.6.5. Other levels of consistencyIn this paper I have focused exclusively on GAC-On-X, and this allowed an extensive study of algorithms for that case.However, I have not compared GAC-On-X against bounds or range consistency, so can offer no conclusions on the relativemerits of different levels of consistency. This would be a very interesting avenue of further work.7. ConclusionsI have presented an extensive survey of propagation methods for the EGCC constraint, studying the pruning of bothtarget variables and cardinality variables, surveying many methods from the literature and presenting some methods thathave not been previously reported.I focused on generalized arc-consistency for the target variables (GAC-On-X) and evaluated two basic algorithms from theliterature along with five optimizations found in the literature, and two novel optimizations. In each case I have reported ontheir implementation and given an empirical analysis of their behaviour. While it was impossible to experiment with everypossible combination of optimizations, I took care to compare each optimization against an appropriate baseline method,P. Nightingale / Artificial Intelligence 175 (2011) 586–614613and to avoid straw men. Particular attention was paid to evaluating combinations of optimizations, which is (naturally)not usually a feature of papers that propose optimizations. The experiments presented here comprise easily the deepestexperimental analysis of GAC-On-X algorithms. Based on them, I was able to conclude that some optimizations are key andothers are less generally useful.I would like to draw particular attention to the results with dynamic partitioning, a novel generalization of an optimiza-tion for AllDifferent [6]. With EGCC dynamic partitioning was 1.56 times faster on average, with a maximum of 5.6 times.The largest gains were seen on the most difficult instances where the solver timed out. The gain for EGCC is less pronouncedthan for AllDifferent [6], albeit on entirely different benchmarks, and with a different combination of other optimizations.For the best combination of optimizations, I found a mean improvement of more than 4 times in runtime over a carefulbut unoptimized implementation of Régin’s algorithm. This confirms that optimizations are an essential part of a practicalimplementation of EGCC.Regarding the cardinality variables, I was able to confirm that the implied sum constraint used by Gecode is indeedvaluable, and also that the stronger flow-based pruning algorithm given by Quimper et al. [18] can also be valuable, sinceit solves more instances within a time limit than either other method.Finally, a fast variant of EGCC is typically orders of magnitude better than a set of occurrence constraints. Even whenEGCC propagation was not effective, it slowed the solver down by only 1.66 times or less in the experiments.AcknowledgementsI owe a debt to many people for helpful discussions, in particular Ian Gent, Ian Miguel, and Guido Tack. I would also liketo thank Ian Gent and Ian Miguel for comments on a draft of this paper. I thank Chris Jefferson for helpful discussions andfor pointing out that EGCC cannot be entailed, and Andrea Rendl for use of Tailor [7] for generating benchmarks. This workwas supported by EPSRC grants EP/E030394/1 and EP/H004092/1.References[1] Claude Berge, Graphs and Hypergraphs, North-Holland Publishing Company, 1973.[2] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Introduction to Algorithms, MIT Press, 1990.[3] Sophie Demassey, Gilles Pesant, Louis-Martin Rousseau, A cost-regular based hybrid column generation approach, Constraints 11 (2006) 315–333.[4] Ian P. Gent, Chris Jefferson, Ian Miguel, Minion: A fast, scalable, constraint solver, in: Proceedings of the 17th European Conference on ArtificialIntelligence (ECAI 2006), 2006, pp. 98–102.[5] Ian P. Gent, Chris Jefferson, Ian Miguel, Watched literals for constraint propagation in minion, in: Proceedings of the 12th International Conference onthe Principles and Practice of Constraint Programming (CP 2006), 2006.[6] Ian P. Gent, Ian Miguel, Peter Nightingale, Generalised arc consistency for the alldifferent constraint: An empirical survey, Artificial Intelligence 172 (18)(2008) 1973–2000.[7] Ian P. Gent, Ian Miguel, Andrea Rendl, Tailoring solver-independent constraint models: a case study with Essence’ and Minion, in: Proceedings of SARA2007, 2007, pp. 184–199.[8] Emmanuel Hebrard, Mistral, a constraint satisfaction library, in: Proceedings of the Third International CSP Solvers Competition (CPAI08), 2008, pp. 31–40.[9] Brahim Hnich, Ian Miguel, Ian P. Gent, Toby Walsh, CSPLib: a problem library for constraints, http://csplib.org/.[10] J.E. Hopcroft, R.M. Karp, An n2.5 algorithm for maximum matchings in bipartite graphs, SIAM Journal on Computing 2 (4) (1973) 225–231.[11] Sophie Huczynska, Paul McKay, Ian Miguel, Peter Nightingale, Modelling equidistant frequency permutation arrays: an application of constraints tomathematics, in: Proceedings of CP-2009, 2009, pp. 50–64.[12] Irit Katriel, Expected-case analysis for delayed filtering, in: J. Christopher Beck, Barbara M. Smith (Eds.), CPAIOR, in: Lecture Notes in Computer Science,vol. 3990, Springer, 2006, pp. 119–125.[13] Irit Katriel, Sven Thiel, Complete bound consistency for the global cardinality constraint, Constraints 10 (3) (2005) 191–217.[14] Francoise Laburthe, Choco: a constraint programming kernel for solving combinatorial optimization problems, http://choco.sourceforge.net/.[15] Mikael Z. Lagerkvist, Christian Schulte, Advisors for incremental propagation, in: Proceedings of the 13th Principles and Practice of Constraint Program-ming (CP 2007), 2007, pp. 409–422.[16] Peter Nightingale, Are adjacency lists worthwhile in alldifferent? Technical Report CIRCA preprint 2009/20, University of St Andrews, 2009.[17] Claude-Guy Quimper, Efficient propagators for global constraints, PhD thesis, University of Waterloo, 2006.[18] Claude-Guy Quimper, Alejandro López-Ortiz, Peter van Beek, Alexander Golynski, Improved algorithms for the global cardinality constraint, in: Pro-ceedings of the 10th Principles and Practice of Constraint Programming (CP 2004), 2004, pp. 542–556.[19] Claude-Guy Quimper, Peter van Beek, Alejandro López-Ortiz, Alexander Golynski, Sayyed Bashir Sadjad, An efficient bounds consistency algorithm forthe global cardinality constraint, in: Proceedings of the 9th Principles and Practice of Constraint Programming (CP 2003), 2003, pp. 600–614.[20] Claude-Guy Quimper, Toby Walsh, The all different and global cardinality constraints on set, multiset and tuple variables, in: Recent Advances inConstraints, in: Lecture Notes in Artificial Intelligence, vol. 3419, 2006.[21] Claude-Guy Quimper, Toby Walsh, Global grammar constraints, in: Proceedings of CP-2006, 2006, pp. 751–755.[22] R Development Core Team, R: A Language and Environment for Statistical Computing, ISBN 3-900051-07-0, R Foundation for Statistical Computing,Vienna, Austria, 2008.[23] Jean-Charles Régin, A filtering algorithm for constraints of difference in CSPs, in: Proceedings of the 12th National Conference on Artificial Intelligence(AAAI 94), 1994, pp. 362–367.[24] Jean-Charles Régin, Generalized arc consistency for global cardinality constraint, in: Proceedings of the 13th National Conference on Artificial Intelli-gence (AAAI 96), 1996, pp. 209–215.[25] Jean-Charles Régin, Jean-François Puget, A filtering algorithm for global sequencing constraints, in: Proceedings of the 3rd Constraint Programming (CP97), 1997, pp. 32–46.[26] Marko Samer, Stefan Szeider, Tractable cases of the extended global cardinality constraint, in: Proceedings of the Fourteenth Computing: The Aus-tralasian Theory Symposium (CATS 2008), 2008, pp. 67–74.614P. Nightingale / Artificial Intelligence 175 (2011) 586–614[27] Christian Schulte, Peter J. Stuckey, Efficient constraint propagation engines, ACM Transactions on Programming Languages and Systems (TOPLAS) 31 (1)(2008).[28] Christian Schulte, Guido Tack, Weakly monotonic propagators, in: Principles and Practice of Constraint Programming (CP 2009), 2009, pp. 723–730.[29] João C. Setubal, Sequential and parallel experimental results with bipartite matching algorithms, Technical Report IC-96-09, Institute of Computing,University of Campinas, Brazil, 1996.[30] Helmut Simonis, A hybrid constraint model for the routing and wavelength assignment problem, in: Proceedings of CP-2009, 2009, pp. 104–118.[31] Robert Endre Tarjan, Depth-first search and linear graph algorithms, SIAM Journal on Computing 1 (2) (1972) 146–160.[32] Robert Endre Tarjan, Data Structures and Network Algorithms, SIAM, 1983.[33] Willem-Jan van Hoeve, Gilles Pesant, Louis-Martin Rousseau, Ashish Sabharwal, Revisiting the sequence constraint, in: Proceedings of the 12th Princi-ples and Practice of Constraint Programming (CP 2006), 2006, pp. 620–634.[34] Mark Wallace, Practical applications of constraint programming, Constraints 1 (1/2) (1996) 139–168.[35] Josef Weidendorfer, Markus Kowarschik, Carsten Trinitis, A tool suite for simulation based analysis of memory access behavior, in: Proceedings of the4th International Conference on Computational Science (ICCS 2004), 2004.