ELSEVIER Artificial Intelligence 8 1 ( 1996) 11 I- 125 Artificial Intelligence Some pitfalls for experimenters with random SAT David G. Mitchell *, Hector J. Levesque 2 Department of Computer Science, University of Toronto, Toronto, Ontario M5S IA4, Canada Received July 1994; revised April 1995 Abstract We consider the use of random CNF formulas in evaluating the performance of SAT testing algorithms, and in particular the role that the phase transition phenomenon plays in this use. the properties of formula Examples in distributions prior to designing an experiment. We expect the field. the importance of understanding this to be of increasing from the literature importance illustrate Keywords: Satisfiability; Random problems; Phase transitions: Experimental design 1. Introduction tasks, problems reasoning to various Satisfiability the performance this is especially generated CNF formulas testing relationship lies at the core of many computational of SAT testing programs. Not surprisingly, of its close intelligence. Randomly for evaluating of formula distribution In formulas. sources of test material region” associated with the satisfiable-to-unsatisfiable the number of clauses examples to the the literature of experiments where sufficient consideration was not given from properties of the formulas used. In most cases, the test formulas were implicitly assumed and because so in artificial are a popular class of test problems the choice random investigation families of distributions were more useful the “hard phase transition which occurs as is increased. Here we make this concrete, by presenting than others, and suggested choosing to the validity of any [26], we argued that some is crucial formulas using from ’ Corresponding author. E-mail: mitchell@cs.toronto.edu. Work carried out while at Simon Fraser University, Bumaby, Canada, supported by the Institute of Robotics and Intelligent Systems and an EBCO/EPIC Graduate Scholarship. *Fellow of the Canadian Institute for Advanced Research. Supported in part by a grant from the Natural Sciences and Engineering Research Council of Canada. 0004-3702/96/$15.00 @ 1996 Elsevier Science B.V. All rights reserved XSDlOOO4-3702( 95)00049-6 I12 D.G. Mitchell. H.J. L.evesque/Art@cial Intelligence 81 (1996) I II-125 to be challenging in some way, or at least to have hardness dependent on a particular parameter, which we show in further experimentation not to be the case. Our examples are based on currently popular “unstructured” random formulas. The be value of these as test material-even when sampled from the “hard region”-can questioned on the grounds that they may not be much like real problems [ 5,17,20], but some of these distributions appear challenging for a variety of methods, and we expect their use to continue. Moreover, most available alternatives are either puzzle-type problems (such as cross-word puzzles) or other distributions with no a priori greater validity. Nonetheless, some classes of “structured” random formulas are likely to become more popular, and in Section 8 we discuss the implications of our examples for such distributions. In Section 2 we describe our SAT testing algorithm and the formula distributions under consideration, and in Section 3 we survey some properties of these formulas. Sections 4-7 examine individual experiments, in light of what we now know about the formulas used. Additional data is presented as needed. In Section 8 we summa- rize. 2. Materials and methods We assume the reader is familiar with the problem SAT as defined in [ 121. The data for our analysis was produced by testing randomly generated SAT instances with a simple version of the Davis-Putnam Procedure [ 81. Our procedure, which we refer to as “DP”, is shown in Fig. 1. In the figure, w denotes a CNF formula and w\p denotes the formula that results when we simplify w by setting the variable p true. We assume the variables are ordered, and let uars(w) denote the set of variables mentioned in w, and min{vars(w)) the least variable mentioned in w. DP is essentially the splitting variant of the Davis-Putnam Procedure as described in [7], but without the pure literal rule. It uses no heuristics other than unit propagation. This is perhaps the simplest complete SAT testing procedure that performs well enough to be useful, and so provides a kind of baseline for performance of many related methods. We count each recursive DP call (or equivalently, each simplification) as a step. We procedure DP( w) if w is empty then return satisfiable else if w contains an empty clause then return unsatis$able else if w contains a unit clause (1) then return DP(w\l). else let p := min(vars(w)} if DP( o\p) = satisfiable then return satisfiable else return DP(w\7p) end DP Fig. 1. The procedure DP. D.G. Mitchell, H.J. L.mesque/Artijcial Intelligence 81 (1996) 111-125 113 report the median number of DP steps to find a satisfying assignment, if there is one, or report failure otherwise. (For most of the distributions used here, the median number of steps is highly correlated with mean, and also with mean and median run times.) We also report the proportion of formulas which are satisfiable. Each point in a graph represents a sample statistic based on 5000 formulas. We examine three families of random CNF formulas. Each family has three parame- ters; the number of variables n, the number of clauses M and for each n, a distribution of clauses over n variables. We will see that the constructed parameter c = m/n, the ratio of clauses to variables, is often more useful than m. The families are as fol- lows. l The widely studied fixed clause length family, which we call “random k-SAT”. Clauses are selected uniformly at random, with replacement, from the set of all 2k(‘$ nontrivial3 clauses of length k defined over n variables. l The “constant density” distributions. In the simplest version, a clause is constructed by including each of the 2n literals with some probability p (which may be a function of n) . Clause lengths are binomially distributed, with expected length 2np. It is often useful to study these formulas in terms of expected clause length k, in which case we set p = k/2n. Since empty clauses, unit clauses and trivial clauses are easy to simplify out,4 experimenters began using variants in which certain clause types were disallowed (in which case expected clause length is altered slightly by rejection of forbidden clause types). In Section 4 we consider the case where unit clauses are permitted, but trivial and empty clauses are forbidden. In Sections 3 and 5 we consider the case where empty, unit and trivial clauses are forbidden. We call this version random &k-SAT (using E to denote that k is an expectation). l Distributions with clause lengths distributed uniformly over some fixed range [k, Z] . We will call such distributions “[ k,l] -SAT”. 3. Patterns in random SAT In this section we will survey some basic (empirical) properties of random formulas for reference in following sections. 3.1. Random k-SAT The upper graph of Fig. 2 shows the proportion of random k-SAT formulas with n = 25 variables which are satisfiable. There is one curve for each clause length k E {2,3,4,5}, and for each curve the ratio of clauses to variables c is varied from l/5 to 30. As has been shown before, at small ratios almost all formulas are satisfiable and at high ratios almost all are unsatisfiable. For each k there is some range of c values over which the proportion of satisfiable formulas changes from almost 1 to almost 0. The location of is trivial if it contains both a variable and its negation. instances of this family with probability [27] extending work by Franc0 [9] and others presented an algorithm which not less than 1 - E, for any constant E > 0, in polynomial 3 A clause 4 Moreover, Wu and Tang solves expected time. See also [ 2,221. 114 D.C. Mitchell. H.J. L.evesyue/Artijicial Intelligence 81 (1996) 11 l-125 Pr[SAT] 1:: -71 DP Steps 0.0 10000 1000 100 10 0 5 10 15 20 25 30 Ratio of Clauses to Variables Fig. 2. Median DP steps for random k-SAT: varying c for selected values of k. this “transition ratio for all values of n is analytically c = 4.758 bounded region” varies with k, but for each k it occurs at approximately The asymptotic the same region above and below: For k = 3, the currently known bounds are location of the transition [ 4,14,24]. [lo] transition respectively. (to distinguish k. As expected, y-axis, necessitated by the dramatic the median number of DP steps required from the regions is not to say there are no hard formulas [ 191 and c = 3.003 to test The lower graph of Fig. 2 shows increase the same formulas, Note the logarithmic the peak at each k lines up with the of peak difficulty with increasing region, We call the part of the curve near the peak the “hard corresponding far from the transition, which it qualitatively region” in the for small n are relatively easier. This “easy region”, nor that the “hard region” sense). Moving there is a range of ratios, for each curve, over little as the ratio is varied, and which we will refer to as the which difficulty changes the number of DP steps is just less than 25 (the number “plateau” of variables) speaking, DP just assigns values to variables by guessing and using unit propagation. We refer to the region below region. c = 1, where difficulty drops off quickly with decreasing When c is increased beyond region, difficulty gradually decreases. This general pattern is also found for larger k than is shown here (although we do not know if it holds for very large k). to the left from the hard region, and almost no backtracking region. In this region, hard in any rigorous c, as the “shoulder” occurs. Roughly is necessarily the transition Random 2-SAT Observe the difficulty of random 2-SAT increases only to the level of the plateau region and then begins to decrease again. Random 2-SAT does in Fig. 2 that as c is increased D.G. Mitchell, H.J. L.evesque/Art$cial Intelligence 81 (I 996) 11 l-125 115 Table 1 DP performance on random 2-SAT when c = I ?I DP steps steps/n 25 19 0.76 50 31 0.74 15 55 0.73 100 74 0.74 125 150 175 200 92 0.74 110 0.73 128 0.13 146 0.73 225 I64 0.73 250 182 0.73 DP Steps 10000 1000 100 10 2 3 4 5 6 7 8 Ratio of Clauses to Variables Fig. 3. DP steps for random 3-SAT: selected values of n. in the sense a hard region not exhibit that is seen when k is larger, 5 which is perhaps not surprising since 2-SAT is solvable in time O(n + m) [ 11. Table 1 shows median DP steps, and the ratio of median DP steps to n, to test random 2-SAT with c = 1 and n up to 250. At least over this range the median number of steps is smaller than n, and grows no faster than n. DP requires time O(2”) in the worst case for random 2-SAT, but this data suggests it may be linear on average. Location of the peak We are usually interested in performance for increasing n, rather than just at a particular n. Fig. 3 shows the effect of increasing n on the curves for random 3- SAT. The transition region becomes narrower (that is, occurs over a smaller range of c) as n is increased. The peak hardness for DP occurs at approximately the point where 40% of the formulas are satisfiable, and shifts slightly to the left as n is increased, in correspondence with the shift of that point. Since empirical tests measure difficulty for a particular algorithm on a given set of problems, we expect the location of peak hardness to be somewhat procedure-dependent. ’ Cheeseman et al. [ 31 hypothesized that phase transitions might not occur for problems in l? Random 2-SAT is one of several counterexamples, but the present observation suggests a revised version of the hypothesis. I16 DC. Mitcheli, H.J. L.evesque/Art@cial intelligence 81 (19%) 111-125 1.0 Pr[SAT] 0.5 DP Steps 0.0 80 60 40 20 0 0 2 4 6 8 10 12 Ratio of Clauses to Variables Fig. 4. Median DP steps for random CR-SAT varying c for selected values of k. to DP which tends to do better on satisfiable For example, adding a heuristic but poorer on unsatisfiable the average performance subsets, and this weighting satisfiable hardness of random 3-SAT for the Davis-Putnam about half the instances were satisfiable, but the algorithms to peak at slightly different instances instances would result in a peak further to the right, because on satisfiable and un- that peak Procedure was near the ratio where appear in each of [4,21,26] changes with c. In [26] we suggested is a weighted average of performance ratios. 3.2. Random Zk-SAT Fig. 4 shows the proportion of satisfiable formulas and the median DP steps for testing to l/5 restrictions, random EZSAT for k E {2,3,4,5} and c varied from random Ek-SAT with n = 2.5 variables, and random 2-SAT are identical 12. (Because of length distributions.) As with random k-SAT, the four curves all converge to the same shoulder shape at small c, in this case when c < 2, and for each k > 3 there is an easy plateau region, a hard region, and a trailing off of difficulty on the right. As with the fixed clause the peaks and transition points shift right with increased k, and the peak length formulas, hardness also increases with k. The most significant difference between these formulas is that these are very much easier. The and random k-SAT, as has been noted before, difference n. lower ratios for Another difference have a few very long clauses, the same average clause most clauses are shorter the transition lengths of 5, and increases with increasing regions are shifted to somewhat lengths. Since these distributions the average clause is a factor of 100 for clause length, which apparently is that the transition region also. lowers than D.G. Mitchell, H.J. L,evesque/Artificial Intelligence 81 (1996) 11 l-125 117 0.0 - 10000 r I I .\ I. I I I - Hooker - s-SAT--- - 1000 r _ 2-SAT---- DP Steps \ \ l-- 1 I I *-- _ . . / , I 1 . . --. _ -_ ---_ __ I I I I I ,’ / - _ _ _ 100 F 10 0 1 2 3 4 5 6 7 Ratio of Clauses to Variables . 8 Fig. 5. Median DP steps for formulas used by Hooker. 3.3. [k,Z]-SAT The general patterns shown above also occur for [ k,l] -SAT, albeit with the added complexity of an additional parameter (random k-SAT is the special case of [ k,l] -SAT when k = I). We content ourselves in this case with presenting only the particular data needed. in Sections 4 and 6. 4. No hard region In Section 3.1 we noted that random 2-SAT did not exhibit a hard region, in the usual sense, and that it can be solved very efficiently even in the worst case. In this section, we will consider experiments using two distributions over NP-complete sets of formulas which also do not appear to have hard regions, and which our data suggests are as easy on average as random 2-SAT. As a consequence, we conclude that the data from the experiments described below provides no evidence that the procedures are effective on nontrivial problems. Hooker [ 171 compared the performance of a resolution-based procedure with that of one based on cutting planes, using constant density formulas with unit clauses allowed, but empty and trivial clauses prohibited. In this case, p was set so that the expected clause length was 5, and formulas were generated with c = 2, and n E { 10,20,30,50}, and also with n = 20 and c E { 1,2,3,4,6,32). We generated formulas from the same distribution and tested them with DP. Fig. 5 shows the median DP steps for these 118 D.G. Mitchell, H.J. L.evesque/Artijcial Intelligence RI (I 996) 1 II-125 1.0 Pr[SAT] 0.5 nn 1000 DP Steps [2,7]-SAT S-SAT Z-SAT - - - - - - 0 1 2 3 4 5 6 7 8 Ratio of Clauses to Variables Fig. 6. Median DP steps for [ 1,7] -SAT and [2,7]-SAT. formulas (labeled “Hooker” in the figure), with n = 50 and c being varied from l/5 to 8. Corresponding curves for random 3-SAT and random 2-SAT, also with n = 50, are included for comparison. There is no evidence of a hard region for Hooker’s formulas, and the median number of DP steps to test these formulas was less than 50 at every ratio (we tested up to c = 50). We repeated the experiment with n = 100, and found exactly the same pattern. Hooker noted that the cutting plane algorithm required only a very small number of iterations to solve these problems, while his resolution procedure performed so poorly that he was unable to report completion times for most conditions. Yet DP-which can be expressed as a resolution strategy [ 151 -required almost no backtracking (less than 3 backtracks on average) to test them, and consistently solves them in a small fraction of a second. Gallo and Urbani [ 1 l] compared the performance of several enhancements to the Davis-Putnam Procedure on [ 1,7]-SAT. We again generated a range of these formulas and tested them with DP. Fig. 6 shows the median DP steps and proportion satisfiable for [ 1,7] -SAT formulas with n = 50 variables (and corresponding curves for [ 2,7]-SAT, random 2-SAT and random 3-SAT). The [ 1,7] -SAT formulas exhibit essentially the same behavior as random 2-SAT, with the usual “shoulder” pattern at the left, but no evidence of a hard region. We confirmed this behavior up to n = 1000, where the peak remains smaller than that of random 2-SAT. Virtually no backtracking is required to test these formulas. We conclude that formula distributions such as [ 1,7]-SAT and those used by Hooker in [ 171 are no harder on average than random 2-SAT, and thus of little use in SAT algorithm evaluation. D.G. Mitchell, H.J. L.evesque/Art$icial Intelligence 81 (1996) 111-125 119 5. Missing the hard region We observed orders of magnitude will see that experimenters below the hard region, which are trivial in Section 3 that random &k-SAT does exhibit a hard region, although In this section we but from have often used formulas random k-SAT formulas. easier than comparable from this distribution, in spite of their large size. Gu [ 161 and Kamath et al. [ 181 evaluated their procedures (based on local search by testing large random &3-SAT formulas, respectively) and tested random E3-SAT with n E (50,500, and interior point programming, Gu with n E {50,500,1000} generated in the upper graph of Fig. 7. Also shown, 3-SAT with n = 50. The experiments that testing To confirm our intuition we plotted number of backtracks large n, these formulas are extremely easy. 6 and Kamath with n = 1000, in all cases with c = 2. We lOOO}, and show the results is the curve for random at c = 2 clearly fall to the left of the hard region. these requires only guessing and unit propagation, in the lower graph of Fig. 7. The median at c = 2 was 1 for II = 500 and 0 for n = 1000: even with fairly the median number of backtracks for comparison, the curve is essentially that peak difficulty Both Gu and Kamath et al. also used formulas the same shape as the left one-fourth of a curve in the hundreds. We generated and tested random ElO-SAT from random ElO-SAT. Gu tested formulas with various values of n up to 5000 and c up to 10, and Kamath et al. used formulas with IZ = 1000 and c up to 32. We expected for these but also that the hard region should occur at quite a high value of would be substantial, formulas with c, probably n = 50 and c varied from 1 to 50, with 1000 samples per point. We do not include a graph: in the region appears up to about c = 8, and from c = 8 upper graph of Fig. 7. The shoulder the to c = 50 the curve no backtracking hard region: all 1000 formulas the mean number was required of backtracks was about 1. We also tested random ElO-SAT formulas with n = 1000 variables and c ranging up to 10. At c = 10, the curve of median DP steps for these formulas has only just passed the shoulder region. About 90% of the formulas at c = 10 required zero backtracks to test, and only one of the 1,000 formulas we tested required as many as three. At lower ratios even fewer backtracks were needed. These formulas, despite being very large, are trivial flat. At c = 50 there is no evidence of approaching tested were satisfiable, and essentially at any ratio below 35, where to test on average. to test these is nearly formulas 6. Interaction of parameters In [ 61 d’Anjou and his colleagues the performance demonstrate report tests using a variety of random formulas of a Boltzmann Machine method for SAT testing to (which h Truth in advertising: Although the vast majority of these formulas is in the thousands due to a very few formulas which ate harder (but still trivial compared number backtracks sized hard random S-SAT formulas). The harder formulas occur so rarely we would not expect with similarly [ 131 to see them have studied apparently harder, but rarer still, formulas in themselves, but we believe they occur even more rarely as n gets large, and do not make the distribution on the whole useful. in studies with small sample sizes such as the examples in this region. These are interesting in this section. Gent and Walsh required 0, 1 or 2 backtracks, the mean 120 D.C. Mitchell, H.J. Levesque/Art@cial Intelligence 8I (1996) 111-125 I I I n I _.---__ : 100000 r 10000 1000 100 in DP Steps DP Back- tracks 0 1 2 3 4 5 Ratio of Clauses to Variables Fig. 7. Median DP steps and backtracks for random &3-SAT. we will refer to as BM). Their data is presented as evidence average, or number of clauses. We will see that, in spite of the authors’ methodical manipulation the data does not support of all distribution that BM solves SAT, on length in the number of variables, regardless of clause in time “nearly this conclusion. parameters, linear” for k was k E {2,5,6,7}, and the experiment was repeated To show run time of BM is independent of clause size d’Anjou et al. tested random the effect on run time of varying k-SAT formulas with fixed n and m, and observed for each k. The range In all cases the formulas had m = 25 clauses. For each value of n, n E { 10,15,20,25}. BM took essentially ratio for the same time for all choices of k. The clause-to-variable all of these test sets is in the range 1 < c < 2.5. They all fall in the plateau region for DP (cf. Fig. 2)) so if we performed this same experiment with DP instead of BM, we would also find no effect of k. But we know that in the hard region, k has a dramatic if the same experiment design had been effect on hardness for m so that some tests were in the transition used, but with a larger value chosen if we picked n = 25 and region, m = 250 (so c = lo), we would find that increasing in DP steps! the results would be just as suspect. For example, k led to a dramatic decrease for DP. Just as importantly, To show run d’Anjou et al. tested formulas 25 to 55. The average time of BM is independent of the number of clauses from [2,7]-SAT with n = 20 variables and m varied from time for BM increased only very slightly as m was varied from 25 in a formula, D.G. Mitchell, H.J. Levesque/Artijcial Intelligence 81 (1996) 111-125 121 to 55 (although the variance did increase substantially). The range of clause-to-variable ratios for this experiment is from 1.25 to 2.75. As Fig. 6 shows, [2,7]-SAT does exhibit the easy-hard-easy pattern, but the number of steps for DP to test these formulas does not change significantly across the range of ratios tested by d’Anjou et al. Again, if we performed this experiment with DP we would get the same results. We do know, however, that the performance of DP is affected by the number of clauses. (To a large extent, and in a more general form, this is what this volume is all about.) Manipulation of m will detect the easy-hard-easy pattern, but only if the range spans the transition region. Anomalous results will occur with other uninformed choices too. For example, varying m from 130 to 160 in this experiment would produce a decrease in difficulty with increasing m. Believing BM to be unaffected by clause length and number of clauses, d’Anjou et al. concluded that they could demonstrate the scaling of run time by varying n, while keeping clause length distribution and m constant. Thus, to support their main claim they tested formulas from [ 2,7]-SAT with n E { 10,15,20,25,30,35,40, lOO}, in every case with m = 25 clauses. The times for BM to solve these problems, as a function of n, fit very closely to a linear curve. Once again the range of ratios tested (from 2.5 down to 0.25 as the number of variables was increased from 10 to 100) falls within the easy region ’ (cf. Fig. 6). If m is fixed, then increasing n decreases c, so picking a larger value for m would not have helped this experiment. For example, if we repeated this experiment using DP, and with m = 65, then at n = 10 (so c = 6.5) we would be testing in the hard region, but at large n we would be in the easy region (eg., with n = 40 we have c z 1.6). Our data suggests that none of the formula sets used in [6] are harder than random 2-SAT, and so all experiments reported there are examples of “missing the hard region”, as discussed in Section 5. However, there are additional issues involved here. We don’t know how BM would perform on hard formulas, but the results in [6] seem to be an artifact of the easy formulas, and unfortunate parameter choices, rather than indicative of positive performance characteristics of BM. Increasing n while m is fixed leads to small values of c, and thus easy instances. Increasing m alone leads to large values of c, and thus easy (relative to the peak) problems. Moreover, increasing k alone shifts the hard area to higher values of c, thus producing easy problems. To generate hard formulas of varying size requires that n and m be varied at the same time, since hard formulas result from an interaction of parameters. Similarly, if we want to change k but obtain formulas with the same relative hardness, or the same proportion satisfiable, we must also change m at the same time. More generally, we see that varying all parameters does not necessarily make an experiment informative about performance of the test procedure, if the effects of those manipulations on the formulas are not to some extent understood beforehand. ‘The IOO-variable point was suspect a priori. There are 200 literals to choose from, but we only expect about 115 literals in a 25 clause formula, so almost half the literak won’t even be mentioned. 122 D.G. Mitchell. H.J. Levesque/Artijicial Intelligence RI (1996) I II-125 Table 2 Locations of tests in Gallo & Urbani [ 111 Test Variables Clauses Ratio 50% point Satisfiable Time (s) 1 10 50 5.0 4.86 7 0.3 2 20 100 5.0 4.55 5 1.2 3 30 140 4.67 4.45 2 5.1 4 40 170 4.25 4.40 6 13.5 5 50 200 4.0 4.36 8 32.7 7. Picking ratios When n), but with a fair understanding the chosen distribution it seems a simple heuristic region, might keep us on safe ground. (typically experimentally simple as knowing the approximate when k = 3). In this section we consider whether common peak hardness. has a hard region to the transition that corresponds like “test where about half the formulas are satisfiable” In most experiments we will vary at least one parameter interactions, we can often region fairly easily. For random L-SAT it may be as ratio for each k (e.g., setting m = 4.25n in the the location of cannot precisely determine in which the experimenter track the transition this approach of parameter is adequate transition situation To begin, we consider an experiment by Gallo and Urbani [ 111, in which performance of several SAT procedures were compared on five sets of random 3-SAT formulas, which partial results are shown the number of test formulas which were satisfiable for Gallo and Urbani’s our own experimental places) of the ratio at which 50% of the formulas will be satisfiable of n and m. for in Table 2. The table gives the parameter values used, (out of lo), and the time in seconds Procedure. Also shown are to two decimal for the given values (based on large samples and accurate implementation estimates of the Davis-Putnam the peak Gallo and Urbani chose m based on estimates of where about one half of the formulas would be satisfiable, but comparison with the known 50% ratios in Table 2 shows they from small sample sizes. We do not were somewhat off, probably due to estimating know exactly where but the in this experiment, the peaks for the procedure identical for c two procedures will be very close. If we assume in Table 2 shift from somewhat above the peak, to somewhat below the peak, crossing it near the third data point. Thus, looking at the first three points gives an overestimate of the growth in time with n, and looking at the last three points we gives an underestimate of growth. suggests this to be true, then the choices is for the procedure used to our DP, and our experience is almost Fig. 8, shows median DP steps to test random 3-SAT with various the hard region precisely, have not defined increasing the transition the ratios used in [ 111 are well within we test exactly at the peak? If we are reasonably n (even though in some sense it. Although we to get wider with it seems region gets narrower). We would claim that all if this hard region. So can it matter that much close, and if we are only interested LX. Mitchell, H.J. L.evesque/Artificial Intelligence 81 (1996) 111-125 123 n=25 - - n=50 : : DP Steps 16000 14000 12000 10000 8000 6000 4000 2000 0 Ratio of Clauses to Variables fig. 8. Median DP steps for random 3-SAT selected values of n. perhaps not. But this broadening the left slope of the hard region becomes of the hard improvements, in dramatic performance so we cannot count region may not occur with other procedures, or other distributions, rapidly much steeper as n is on it. Further, increased. With n = 75, a shift in c of only four per cent can result in a factor of two lines in Fig. 8, difference and this effect seems to become more pronounced with increasing n. Moreover, since the transition estimate of a particular point on the transition curve to within a few per cent can be a very expensive in the number of steps taken, as is shown by the two vertical region becomes narrower as n is increased, finding an accurate experimental task for large n. formulas The steep slope on the left of the hard region does seem rather far from the transition region to be much of a problem, but again for procedures other than DP this may not be the case. Likely candidates here are some of the local-search-based methods. These have been used with considerable for the primary satisfiable test sets used in this work, and success has been reported with formulas near the hard region and having [23]. Although performance of local search is very good where just over half of the formulas are satisfiable, as of this writing we through know of no rigorous study of how these procedures perform as c is increased the transition they do that if there are formulas on which likely poorly, [ 16,23,25]. Random 3-SAT formulas have been among they may be in the right-hand part of the transition thousands of variables region, and it seems for finding satisfying truth assignments success region. A major difficulty in evaluating these procedures SAT testing program available search procedures are sound but incomplete, has solved all the satisfiable correct. Since failure formulas given, and thus is more likely as n increases, measured is that there is no sound and complete the local there is no way to know if such a procedure times are run times can be expected if reported average capable of testing such hard formulas. Since 124 D.G. Mitchell, H.J. Levesque/Art@cial Intelligence 81 (1996) 111-125 to underestimate the actual increase in solution time with n. The narrowing of the transition region with increasing n further exacerbates the problem, because for large II an error of even one or two per cent in the choice of c could make a substantial difference in both the proportion of formulas which are satisfiable, and in the difficulty of these formulas. 8. Discussion Random formulas have been used by many researchers to empirically evaluate the performance of SAT testing programs. The value of such studies depends upon careful selection of formula distribution and parameter values, and we presented a number of examples from the literature to illustrate this. The lesson here may be summarized as follows. When using random formulas, an extensive enough study of the distribution’s parameter space must be carried out before an experiment is designed, if the results are to be meaningful. In the case of a previously unused distribution family, this may require extensive testing, and even for known distributions considerable care may be needed. Simple heuristics for picking parameter settings may not always be adequate, and a range of parameter values should be tested at each IZ of interest, to establish where peak difficulty is. This is especially true for local search methods, since their incompleteness means we have less knowledge about behavior near the transition region. While it might be argued that it is too easy to point the finger at previous work in light of our current state of knowledge, there is an important lesson for future work, especially regarding “more structured” random distributions, such as those used in [20], or those used by workers in scheduling or constraint satisfaction. Similar pitfalls should be expected with any nontrivial parameterized distribution of random problems, and in fact will likely become even more troublesome. Suggestions for “re- alistically structured” problem distributions generally have more parameters than the currently popular “unstructured” families, so that effects of parameter manipulation on problem difficulty may be much more complex. A full understanding requires exami- nation of the effects of every parameter, and also of every combination of parameters. Understanding all interactions of even five or six parameters is not a task to be taken lightly. Watching for implicit assumptions in experimental designs is essential. We would like to stress two examples of particular importance with random SAT. The first is that size alone (or number of variables) is not a useful measure of hardness (cf. Section 5). This is important because in evaluating algorithm performance we are most interested in what happens as problem size increases. Formulas with 5000 variables, 50,000 clauses and 500,000 literals [ 161 sound impressively large, but when they are from a distribution like the one we called random &lo-SAT, they can be solved very easily by a simple procedure like DP, with less than one backtrack on average. The second is that manipulating a single parameter while holding others constant does not necessarily give us a good indication of how that parameter affects instance difficulty (cf. Section 6). This is important because many distributions of interest have multiple parameters with interaction effects which are far from intuitively obvious. D.G. Mitchell, H.J. Levesque/Artijicial Intelligence 81 (1996) 111-125 125 References 1 I 1 B. AspvaIl, M.F. Plass and R.E. Tarjan, A linear-time algorithm for testing the truth of certain quantified boolean formulas, In5 Process. L.&t. 8 (3) (1979) 121-123. 12 1 M. Chao and J. France. Probabilistic analysis of a generalization of the unit-clause literal selection heuristics for the k satisfiability problem, Inform. Sci. 51 (1990) 289-3 14. 13 1 P. Cheeseman, B. Kanefsky and W.M. Taylor, Where the really hard problems are, in: Proceedings IJCAl-91, Sydney, Australia (1991). 14 I J.M. Crawford and L.D. Auton, Experimental results on the cross-over point in satisfiability problems, in: Proceedings AAAl-93, Washington, DC ( 1993) 21-29. 15 I J.M. Crawford and A.B. Baker, Experimental results on the application of satisfiability algorithms to scheduling problems, in: Proceedings AAAl-94, Seattle, WA ( 1994). 16 I A. d’Anjou M. Gratis, F.J. Torrealdea and M.C. Hemandez, Solving satisfiability via Boltzmann Machines, IEEE Trans. Pattern Anal. Mach. lntell. 15 (5) (1993). 17 1 M. Davis, G. Logemann and D. Loveland, A machine program for theorem-proving, Commun. ACM 5 ( 1962) 394-397. 18 1 M. Davis and H. Putnam, A computing procedure for quantification theory, J. ACM 7 (1960) 201-215. 19 I J. France, On the probabilistic performance of algorithms for the satisfiability problem, Inf: Process. Letf. 23 (1986) 103-106. [ 101 A. Frieze and S. Suen, Analysis of three simple heuristics on a random instance of k-SAT, Manuscript (1992). [ I I] G. Gallo and G. Urbani, Algorithms for testing the satisfiability of propositional formulae, J. Logic Program. 7 ( 1989) 45-6 I. I 12 I M.R. Garey and D.S. Johnson. Computers and Intractability: A Guide to the Theory ojNP-Completeness (Freeman, New York, 1979). I 13 I 1.P. Gent and T. Walsh, The hardest random SAT problems, in: Proceedings K/-94 ( 1994). ( 14 I I.P. Gent and T. Walsh, The SAT phase transition, in: Proceedings ECAI-94, Amsterdam (1994). 1 I5 1 A. Goldberg, On the complexity of the satisfiability problem, Courant Computer Science Report No. 16, New York University ( 1979). I 16 I J. Gu, Efficient local search for very large-scale satisfiability problems, SIGART Bull. 3 (I) ( 1992) 8-12. I I7 I J.N. Hooker, Resolution vs. cutting plane solution of inference problems: some computational experience, 0per. Res. Lett. 7 (I) (1988) l-7. I I8 I A. Kamath, N.K. Karmarkar, K.G. Ramakrishnan and M.G.C. Resende, Computational experience with an interior point algorithm on the satisfiability problem, in: Integer Programming and Combinatorial Optimization. (Mathematical Programming Society, 1990) 333-349. I 191 A. Kamath, R. Motwani, K. Palem and P Spin&is, Tail bounds for occupancy and the satistiability threshold conjecture, in: Proceedings FOCS-94 (1994). I20 I K. Konolige, E&y to be hard: difficult problems for greedy algorithms, in: Proceedings KR-94, Bonn, Germany ( 1994) 374-378. I 2 I I T. Larrabee and Y. Tsuji, Evidence for a satisfiability threshold for random 3CNF formulas, Tech. Repott UCSC-CRL-92-42 CRL. University of California, Santa Cmz, CA ( 1992). I 22 I I? Purdom, A survey of average time analyses of satisfiability aIgorithms, 1 fn$ Process. I3 (4) ( 1990). I23 I B. Selman, H. Kautz and B. Cohen, Noise strategies for improving local search, in: Proceedings AAAI-94, Seattle, WA ( 1994) 337-343. [ 24 I B. Selman and S. Kirkpatrick, Critical behavior in the computational cost of satisfiability testing, Artif fnfell. 81 ( 1996) 273-295 (this volume). I 25 I B. Selman, H.J. levesque and D.G. Mitchell, A new method for solving hard satistiability problems, in: Proceedings AAAI-92, San Jose, CA ( 1992) . I26 I B. Selman, D.G. Mitchell and H.J. Levesque, Generating hard satisfiability problems, Artif: Intell. 81 ( 1996) 17-29 (this volume). 127 I L. Wu and C.Y. Tang, Solving the satistiability problem by using randomized approach, Inf. Process. Letf. 41 (1992) 187-190. 