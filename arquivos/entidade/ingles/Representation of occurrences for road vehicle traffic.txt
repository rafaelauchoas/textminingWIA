Artificial Intelligence 172 (2008) 351–391www.elsevier.com/locate/artintRepresentation of occurrences for road vehicle trafficR. Gerber, H.-H. Nagel ∗Institut für Algorithmen und Kognitive Systeme, Universität Karlsruhe (TH), 76128 Karlsruhe, GermanyReceived 28 June 2004; received in revised form 17 July 2007; accepted 19 July 2007Available online 27 July 2007AbstractOur 3D-model-based Computer Vision subsystem extracts vehicle trajectories from monocular digitized videos recording roadvehicles in inner-city traffic. Steps are documented which import these quantitative geometrical results into a conceptual repre-sentation based on a Fuzzy Metric-Temporal Horn Logic (FMTHL, see [K.H. Schäfer, Unscharfe zeitlogische Modellierung vonSituationen und Handlungen in Bildfolgenauswertung und Robotik, Dissertation, 1996]). The facts created by this import stepcan be understood as verb phrases which describe elementary actions of vehicles in image sequences of road traffic scenes. Thecurrent contribution suggests a complete conceptual representation of elementary vehicle actions and reports results obtained byan implementation of this approach from real-world traffic videos.© 2007 Elsevier B.V. All rights reserved.Keywords: Computer vision; Knowledge representation; Temporal reasoning; Reasoning about actions and change; Fuzzy metric-temporal logic1. IntroductionAn adult is expected to be able to write down—not necessarily with style and precision—what he sees. Concedingsimilar, but appropriately adapted reservations, what is required to have a computer perform an analogous task?Obviously, an analogue to human seeing could be Computer Vision. The notion of an automatic report generator,too, is no longer considered as science fiction. It most likely turns into a challenge, however, to imagine the detailedcommunication between a computer vision (sub)system and an algorithmic report generator. What looks like the meredefinition of an interface will turn out to require the design of a system-internal logic-based conceptual representationof a text in combination with the design of an entire set of processes operating on this representation.Investigations to be discussed in the sequel address an important step towards the algorithmic transformation ofvideo signals into a natural language text which describes the recorded scene, in particular its temporal development.The presentation will first sketch an overall system concept in order to provide a framework for the subsequentdiscussion which will then concentrate on the conversion of geometric tracking results into elementary conceptualrepresentations of relevant aspects of the (short-term) development in the recorded scene. A preliminary version ofthis approach has been partially outlined in [10].* Corresponding author.E-mail address: nagel@iaks.uni-karlsruhe.de (H.-H. Nagel).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.07.001352R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 1. In the upper left panel, the image plane projection of a polyhedral model for a fastback has been overlaid to frame number 340 from an imagesequence recorded at a gas station. In addition, one can see the trajectory segment obtained by automatic model-based tracking of this vehicle whichwill be referred to as object_1. Frames 635, 1180, and 2131 show snapshots of various maneuvers of another vehicle (object_4), with analogousoverlays of a projected polyhedral model and the trajectory for this vehicle (see Section 1 for more explanations). The sketch in the bottom rowillustrates the maneuvers of object_4 in this sequence.Fig. 1 illustrates a coherent source of examples for different stages of such a transformation. The first frame1 #340in the upper left panel shows a snapshot where a fastback has already stopped at the second petrol pump on thefilling lane (see Fig. 2) closer to the observer (subsequently referred to as the ‘lower filling lane’). A second fastback(subsequently referred to as ‘object_1’) had just entered the gas station and selected the filling lane on the other side1 Based on special derivative operators which suitably interpolate between digitizations in even and odd scanlines of interlaced video (see, e.g.,[32]), actually each half-frame—or field in video-coding terminology—is evaluated in its own right, resulting in a temporal sampling rate of20 msec. This aspect reduces the approximation errors by the Extended Kalman-Filter used and thus improves the tracking quality. Beyond thisfact, however, it does not influence the conversion of geometric results to natural language concepts. In order to simplify the presentation, we shalluse the term frame henceforth without further qualifications.R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391353Fig. 2. Groundplan sketch of the gas station.of the petrol pumps (‘upper filling lane’) in order to stop next to the second petrol pump, too. About 300 frames—i.e. 6 seconds—later, a third vehicle, a sedan (‘object_4’), entered the gas station and headed towards a passing lanebetween the upper filling lane and the gas station building. It passed object_1, changed back to the upper filling lane,stopped there and backed up slowly until it eventually stood next to the third petrol pump, immediately in front ofobject_1, around frame-time 1180. About 1000 frames (20 secs) later, after the fastback on the lower filling lanehad already left the gas station, object_1 started to move backwards to gain space in order to change to the passinglane. Object_1 then passed object_4 and headed towards the exit of the gas station. About three quarters of a minutelater around frame-time 4600, object_4 started to move forward and headed towards the exit, too. Examples will refermostly to the sequence of maneuvers performed by object_4 and object_1 during the period while this image sequencehad been recorded.The derivation of conceptual representations and textual descriptions of agent behavior from visual input hasbecome a research topic of constantly growing interest in the last few years. Such research has to deal with theuncertainties related to the geometric results estimated from video sequences and with bridging the semantic gapbetween (mainly geometric) computer vision results and (mainly conceptual) action descriptions. This contributionaddresses a basic topic related to the second aspect, namely isolatable agent activities or occurrences for non-humanagents, in particular rigid vehicles in videos recorded from road traffic. A discussion of relevant prior publications willbe postponed to the concluding sections because similarities and differences can be stated there more succinctly withrespect to what will be reported in the sequel.2. System outlineAlgorithmic text generation based on a recorded video sequence has to be concerned with at least two disciplines,namely computer vision and computational linguistics. Each of these two disciplines already covers several subdis-354R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 3. Coarse layer structure of the overall system (from [28]; ©2000 IEEE, by permission). The layers with light gray background constitute thecore Computer Vision subsystem for the extraction of a (mostly geometric) 3-D scene representation. The Conceptual Representation subsystemhas a medium gray background, the text generation is incorporated into the Natural Language Level with background in dark gray (see, too, thetext or [29]).ciplines. Any system for video-to-text transformation will thus be complex and, therefore, difficult to present and toanalyse. The following subsection provides an overview of our entire system approach, thereby setting the frame for amore detailed outline of steps in video-based text generation proper. More information about the development of thissystem concept during past decades can be found in [29], with recent developments being discussed in [2].2.1. Overall system structureThe transformation of video signals into a text describing the recorded temporal development within the depictedscene can be subdivided into three groups of processes—see Fig. 3:(1) The subsystem which controls the video recording and the subsequent processing steps up to and including theextraction of 3-D time-dependent geometric descriptions of the scene and, in particular, of visibly moving bodies.This subsystem comprises the layers devoted to the following subtasks:(a) Control of the recording equipment including actuators required, for example, to change pan and tilt of videocamera heads, zoom of camera lenses, etc.—the Sensor-Actuator-Level (SAL).(b) The Image-Signal-Level (ISL) devoted to image processing operations on the recorded video signal.(c) The Picture-Domain-Level (PDL) where information extracted from the image signal is aggregated intoPicture-Domain-Descriptors in the 2-D image plane.(d) The Scene-Domain-Level (SDL) which combines Picture-Domain-Descriptors with knowledge about thecamera and about the scene in order to obtain a three-dimensional representation of (at least) the geometryof temporal developments in the recorded scene.In the particular example illustrated by Fig. 1, this information comprises the 3-D vehicle status together withthe 3-D model of those vehicles which have been detected, initialized, and tracked. The vehicle status comprisesthe scene ground plane coordinates (x, y) of the model reference point, the vehicle orientation θ , speed v, andR. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391355steering angle2 ψ. The vehicle status is updated at each frame time point, i.e. every 20 msec, by a Kalman-Filterincorporated into a model-based tracking process—see [14,19,23].3(2) The quantitative 3-D spatio-temporal information provided by the model-based vehicle tracking subsystem isconverted into an elementary conceptual representation at the interface between the Scene-Domain-Level and theConceptual-Primitives-Level (CPL). Information about the spatio-temporal developments in the scene representedin form of conceptual primitives is aggregated by abstraction processes into information about, for example, thebehavior of agents in the depicted scene at the Behavior-Representation-Level (BRL). These two layers constitutethe interface between the core Computer Vision subsystem (comprising the SAL, the ISL, the PDL, and the SDL;light gray background in Fig. 3) on the one hand and the text generation subsystem incorporated into the remainingsubsystem, namely(3) the Natural-Language-Level (NLL) (dark gray background in the same figure). This latter layer could comprisein principle—in addition to a natural language text generation component—also a natural language question-answering component although such a component has not been studied yet in this context.The remainder of this exposition will concentrate on the subsystem for the representation and use of ConceptualPrimitives.2.2. Principal steps for text generationThe system outlined in Fig. 4 still constitutes an exploratory stage of text generation from video recordings. Designand implementation of this system version were based on the following considerations:• It appeared more important at this stage of the overall investigation to conceive and implement an entire sys-tem which attempts to cover a carefully delimited discourse domain completely rather than to construct isolatedsubsystems devoted to an in-depth study of special problems.• In particular, the design and exploitation of conceptual representations should be emphasized, based on the hy-pothesis that the interface between the extraction of geometric information and the formulation of this informationas a natural language text constitutes the real challenge at this stage of our investigations.• In order to base such a system on a reliable methodology, treatment of intermediate results at the conceptual levelshould use formal logic to the extent possible.• The Computer Vision subsystem Xtrack evaluates monocular image sequences of road traffic scenes recordedby a stationary video camera. The detection and tracking of road vehicles by Xtrack exploits their motionrelative to the (assumed) static background and foreground. Apart from this more technical aspect, vehicularmotion is of prime interest because it provides the basis for– a (short-time) prediction of vehicle appearance in the next image frame and– some (longer-time) prediction during entire image subsequences.The latter aspect should allow to characterize vehicle behavior conditioned on a representation of the currentstatus of the depicted scene.As will be explained in more detail below, geometric results of the core Computer Vision subsystem are importedinto the conceptual representation subsystem—see Fig. 4—together with knowledge about the geometry of the staticpart of the depicted scene. A two-step approach converts this input from a quantitative, numerical representation intoa qualitative, conceptual one: the first step converts the input into discrete values compatible with predefined attributeschemes, the second step then combines the resulting attributes to assert occurrences, i.e. conceptual representationsfor a recognizable significant change (or its absence!) of a vehicle’s status, in particular its motion status. The time2 Some of the experimental results discussed in the sequel have been obtained with an older version of the road vehicle tracking system Xtrack(see [14,19,21]) where the angular velocity around the vertical axis of the vehicle (here identical with the ground plane normal) constituted the fifthcomponent of the vehicle state vector.3 A framework called MOTRIS (Model-based Tracking in Image Sequences) comprising the Xtrack model-based tracking approach has beenre-designed and re-implemented in Java. It has been released under GNU GPL, see [26].356R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 4. A feed-forward system version designed to convert geometric results obtained by a model-based tracking (core Computer Vision) subsysteminto a coherent natural language text. At each layer, an entry into the left column corresponds to the representation of intermediate results atthat layer. Entries into the right column explicate the kind of knowledge exploited by the transformation subprocess at that particular layer. Theleft entry at the bottom layer refers to results obtained by the Computer Vision subsystem. The entry into the right column of the bottom layer(Geometric Lane Model) refers to a-priori knowledge about the geometry of the depicted scene. The layers with dark gray background use a FuzzyMetric-Temporal Horn Logic (FMTHL) in order to represent and manipulate a-priori knowledge together with the results provided by the nextlower layer. The topmost two layers rely on Discourse Representation Structures in order to convert this conceptual representation into a coherentnatural language text.scale relating to such primitive conceptual representations of vehicle motion extends from a fraction of a secondupwards to several seconds.The next step (Situation Analysis) combines primitive conceptual representations with knowledge about conditionsin the scene which may influence the switch between particular occurrences as being the most appropriate descriptionR. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391357Table 1Resulting Degree of Validity (DoV) of a fuzzy logic operation δ combining two formulae F1, F2 with related DoV ℘1, ℘2, respectively, for weak,medium, and strong semantics (from [35])δ←∧∨weakmin(1, 1 − ℘2 + ℘1)min(℘1, ℘2)min(1, ℘1 + ℘2)medium1 − ℘2 + ℘1 ∗ ℘2℘1 ∗ ℘2℘1 + ℘2 − ℘1 ∗ ℘2strongmax(℘1, 1 − ℘2)max(0, ℘1 + ℘2 − 1)max(℘1, ℘2)Table 2General structure of facts, rules, and queries in F-Limette and FMTHL (from [35])FactRuleQueryExpression in F-Limetteλ | t1 : t2 ! Rel.λ | t1 : t2 ! (Rel1:-Rel2).λ | t1 : t2 ? Rel.Corresponding FMTHL-formula↓λ ◦[t1,t2] Rel↓λ ◦[t1,t2] (Rel1 ← Rel2)↓λ (cid:8)[t1,t2] Relof (short-term) vehicular behavior. The dominant scale for temporal intervals of interest thus changes by 1–2 ordersof magnitude, i.e. to between several seconds and a minute or more.The Conceptual Scene Description obtained at this level is then prepared for presentation to a human user who isanticipated in our case so far as a reader of the textual descriptions to be generated. The conceptual representationof preferences expressed by this reader—his perspective on the temporal development of the recorded scene—isexploited in order to create a Perspectivated Conceptual Scene Description PCSD.This PCDS is transferred to processes at the Natural Language Level in order to be converted into natural languagetext. As a first step, the Perspectivated Conceptual Scene Description obtained from a particular image (sub)sequencewill be converted into a Discourse Representation Structure (see [18]) which in turn is converted into the output text,see [11]. In the remainder of this contribution, we shall concentrate on the interface—see Fig. 3—between the coreComputer Vision subsystem and the Conceptual Primitives Level, i.e. the bottom two levels indicated in Fig. 4.3. NotationIn order to facilitate a compact presentation, a short introduction of the Logical Vocabulary of the Fuzzy MetricTemporal Horn Logic notation is provided which will be used in the sequel.Fuzzy Metric Temporal Logic (FMTL) extends the First Order Predicate Calculus (FOPC) with metric temporaland fuzzy reasoning (see [35]). Temporal Logic extends FOPC by adding sets of time instants and their ordering.As a consequence, the Degree of Validity (DoV) of formulae can vary with time. Metric temporal logic is based ona linear, discrete time structure corresponding to integer values, here representing the frame number of video imageframes. FMTHL comprises two temporal logic operations (◦s,e and (cid:8)s,e) for denotation of universal and existentialvalidity of formulae, respectively, within given sets of time instants. Fuzzy Logic generalizes FOPC by real numbertruth values μ ∈ [0, 1]. Fuzzy degrees of validity are generated for logic subjunction, conjunction, disjunction, andnegation. Table 1 comprises three possible semantics—weak (w), medium (m) and strong (s)—related to the first threelogic operations. The two monadic logic operations ↓λ and ↑λ represent weakening and intensivation, respectively, ofvalidity.FMTHL is the Horn-Logic fragment of FMTL. FMTHL-formulae can be subdivided into rules, facts, and queries.The head of a rule consists of a certain predicate p, whereas its body comprises specific conditions whose validitiesdetermine the validity of p. Stating a query to F-Limette4 involves finding a substitution on the basis of given ruleswhich verifies the validity of the query: for each predicate p currently required, the bodies of all rules are evaluatedrecursively whose head comprises p.Table 2 conveys the general structure of expressions in F-Limette and corresponding FMTHL-formulae. Facts andrules are, in general, universally quantified whereas queries are quantified existentially.4 This system has been made available as a package under GNU Public License, in connection with the Situation Graph Editor of M. Arens, seehttp://cogvisys.iaks.uni-karlsruhe.de/Vid-Text/.358R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391F-Limette follows widespread conventions for formal logic in that identifier symbols for variables start with capitalletters whereas function and predicate symbols start with a lower case letter. Constants are considered as 0-ary functionsymbols and thus begin, too, with a lower case letter. Temporal constants −i and +i allow to use infinity as a boundaryvalue for the temporal validity of formulae. The term always in front of a formula stands for ‘1 | -i : +i !’ whichmeans unrestricted validity of the formula (expressed by the number 1 for true in front of the vertical separation bar)from minus infinity until plus infinity, i.e. for all time instants. The term next within a formula stands for ‘+1 !’which means that the predicate following this term has to be valid at the next time instant.The two-letter symbol :- denotes a (re-)implication operator. Predicates in the body of an FMTHL-expressionwhich are separated by a comma are joined conjunctively. The semicolon denotes the disjunction operator. The oper-ator is assigns the result for the expression on its right-hand side to the variable on its left-hand side, whereas >=,=, <, +, -, *, / correspond to conforming relational and arithmetic operators, respectively.4. Results and a-priori knowledge imported from the computer vision subsystemThe Computer Vision subsystem provides two different types of information to the conceptual representation sub-system: tracking results—i.e. estimates for the time-dependent state of vehicles detected and tracked in an input videosequence by a model-based approach—and (time-independent) geometric lane data exploited in this context. The lat-ter data have to be converted into a conceptual representation of the lane structure in the recorded scene for furtheruse by the FMTHL inference engine F-Limette activated by the conceptual representation subsystem.4.1. Import of tracking resultsTracking results provided by the Computer Vision subsystem comprise geometric values for the (x, y)-positionof vehicles in the recorded scene, their orientation θ , their velocity v, and their steering angle ψ for each agent(vehicle) and frame time point. Each result, after conversion to an FMTHL fact in the form of a predicate namedhas_status connected with individuals for the variable Agent (for example, ‘object_4’), is imported into theconceptual representation subsystem. In the following, a small part of the trajectory data derived for ‘object_4’ (seeFig. 1) is shown below:time [frame #] !614615616617618619620!!!!!!!has_status(has_status(has_status(has_status(has_status(has_status(has_status(agentobject_4,object_4,object_4,object_4,object_4,object_4,object_4,x [m]8.941049,8.884917,8.830486,8.780960,8.733459,8.686386,8.639212,y [m]1.849823,1.873256,1.899629,1.926439,1.954635,1.986568,2.014085,◦θ []146.9672,147.0714,147.1823,147.2313,147.2106,147.1799,147.2301,v [m/s]2.934614,2.873042,2.809821,2.803807,2.793914,2.737727,2.675781,◦ψ []−2.064247 ).−2.462217 ).−2.830728 ).−3.210003 ).−3.581725 ).−3.541704 ).−3.131224 ).Since the Computer Vision subsystem currently does not associate yet a Degree of Validity (DoV) with its geomet-ric results, each imported fact is treated as absolutely valid (i.e. μ = 1). For details concerning the notation of FMTHLfacts see Section 3, Table 2.4.2. Lane geometryThe lane geometry of a traffic scene needs to be known to our Computer Vision subsystem Xtrack, for example,in order to present the lane structure of selected image frames for interactive inspection of intermediate trackingresults.Fig. 5 shows a bird’s eye view of the lane model corresponding to the gas station where the gas station sequenceillustrated in Fig. 1 has been recorded.The geometric lane model consists of points, lines and lane segments (for example, points p347–p350 in Fig. 6).This data is used in the conceptual representation subsystem in order to associate agent positions with lanes and toderive conceptual descriptions of vehicle behavior. Analogously to the trajectory data in Section 4.1, these geometricR. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391359Fig. 5. Bird’s eye view of the quantitatively known lane model for the gas station. The alphanumeric identifiers shown here for lane segmentsestablish a correspondence between the geometric description given by the line segments on the one hand and the conceptual representation of thelane structure required for text generation.Fig. 6. Overlay of a suitable projection of the lane model from Fig. 5 onto one image of the gas station sequence illustrated in Fig. 1. Here:Enlargement of the area around the last filling place on the lower filling lane of the gas station (lane segment ‘1’ in Fig. 5). In addition, the pointsp347–p350 and the scene coordinate system are shown.lane data are imported into the conceptual representation subsystem by a set of facts and by FMTHL-rules makingadditional elementary properties about the lane geometry available in conceptual rather quantitative geometric terms.Points are referenced by their x/y-position in the scene and an unique identifier:always point( 1.030, -3.352, p347).always point(-1.870, -3.353, p348).always point( 1.031, -0.852, p349).always point(-1.871, -0.853, p350).Again, all these data are assumed by the interpretation process to be absolutely valid (i.e. μ = 1). Lines are de-scribed by the identifiers of the corresponding endpoints and their own identifier:always line(p347, p348, l420).always line(p349, p350, l421).Lines are implicitly oriented from their first endpoint (e.g., ‘p347’) to their second (‘p348’). In consequence, linesare enlisted as directed edges in Fig. 5.360R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Neighboring (more or less parallel) lines are combined into lane segments (e.g., lane segment ‘1’ in Figs. 5 and 6):always segment_of_lane(l421, l420, lseg_1).Thus, each lane segment can be conceived as a convex polygon connecting the overall four endpoints of their twoexplicitly implemented lines. Neighboring lane segments can be combined to lane objects or, for short, lanes. Forinstance, lane segments ‘11, 3, 102, 4, 101, 1, and 15’ are combined to a lane named ‘lobj_230’ by the part_of -Relation:always lane_segment(lseg_11).always lane_segment(lseg_3).always lane_segment(lseg_102).always lane_segment(lseg_4).always lane_segment(lseg_101).always lane_segment(lseg_1).always lane_segment(lseg_15).always part_of(lseg_11, lobj_230).lobj_230).always part_of(lseg_3,always part_of(lseg_102, lobj_230).always part_of(lseg_4,lobj_230).always part_of(lseg_101, lobj_230).always part_of(lseg_1,lobj_230).always part_of(lseg_15, lobj_230).always lane(lobj_230).5. Generation of a primitive conceptual representation for time-dependent agent propertiesThe facts obtained from the data provided by the Computer Vision subsystem constitute the link between theComputer Vision and the Conceptual Representation subsystems. Some of these facts comprise quantitative numericalvalues for, e.g., velocity and positions of agents as in614 ! has_status(object_4, 8.941049, 1.849823, 146.9672, 2.934614, −2.064247).(see Section 4.1). This status information about the tracked vehicle ‘object_4’ provides the value range of an interpre-tation function for the following logical formulahas_speed(Agent, small)which is a logical predicate relating the speed of an agent (e.g., ‘object_4’) to a discrete conceptual value small. Todo so, one has to derive the Degree of Validity (DoV) with which the geometric value of the agent’s velocity V ismapped to the discrete value small.degreeOfValidity 5-ary predicate symbol. Its DoV corresponds to the function value of the trapezoidal function de-scribed by the last four arguments related to the first argument. The meta-predicate sp overwrites the DoVby the arithmetic expression of its argument, see [35].always (degreeOfValidity(X,P1,P2,P3,P4) :-X >= P1 , X < P2 , Wert is (X - P1) / (P2 - P1) , sp(Wert) ;X >= P2 , X < P3 , sp(1.0) ;X >= P3 , X < P4 , Wert is (P4 - X) / (P4 - P3) , sp(Wert)).Fig. 7 shows one example relating the geometric value for V to the discrete value small (and to others). Thetrapezoidal function μsmall can be conceived as logical interpretation functionI{associate_speed(V, small)} = μsmall(V ).R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391361Fig. 7. Discretization of continuous speed values into a set of intervals. The upper part shows the fuzzy membership functions μspeed_value for thesubset { zero, small, normal, high, very_high } of discrete conceptual speed values.To be more general, μDISCRETE_VALUE(V ) can be written as μA,B,C,D(V ) where A < B < C < D represent thearguments for which the slope of the trapezoidal function exhibits a discontinuity. According to Fig. 7, one obtains,for example,μsmall(V ) = μ0.28,0.83,2.78,5.56(V )orμnormal(V ) = μ2.78,5.56,12.5,16.67(V ).In the following, one possible implementation of this predicate in terms of an FMTHL-rule during the import ofinformation from the Computer Vision subsystem into the Conceptual Representation subsystem is given:always (has_speed(Agent, small) :- has_status(Agent,X,Y,Theta,V,Psi),associate_speed(V, small)).always (associate_speed(V, small) :- degreeOfValidity(V,0.28,0.83,2.78,5.56)).This corresponds to the following logical interpretation:I{ has_speed(Agent, small) }== I{ has_status(Agent,X,Y,Theta,V,Psi) ∧ associate_speed(V,small) }== I{ has_status(Agent,X,Y,Theta,V,Psi) ∧ degreeOfValidity(V,0.28,0.83,2.78,5.56) }== min( I{has_status(Agent,X,Y,Theta,V,Psi)},I{degreeOfValidity(V,0.28,0.83,2.78,5.56)} )where the symbol == has been used to indicate that this operation refers to a numeric equality, i.e. neither to a logicalequality operator nor to an assignment operator as it is used in many programming languages. The minimum functionhas been used as the fuzzy version of the conjunction as it is usually done.Since has_status is not a fuzzy predicate (see Section 4.1), one obtainsI{has_speed(Agent, small)} == 0providedI{has_status(Agent,X,Y,Theta,V,Psi)} == 0,which will occur at time points where no trajectory data for the agent is available (for example because the agent iscurrently not in the field of view of the recording camera). As a consequence, no geometric velocity value V for theagent will be available and thus no association to a discrete value can be performed. Otherwise, one obtains362R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391I{ has_speed(Agent, small) }== min(1, I{ degreeOfValidity(V,0.28,0.83,2.78,5.56) } )== I{ degreeOfValidity(V,0.28,0.83,2.78,5.56) }== μ0.28,0.83,2.78,5.56(V ).As a result, geometric input data can be associated with discrete conceptual values by defining FMTHL predicatessuch as has_speed(Agent, small). These predicates constitute the head of simple FMTHL rules whose bodyconsists of the non-fuzzy has_status-predicate in order to access the geometric value and of the degreeOfVa-lidity-predicate. The latter represents the compatibility of the value imported from the Computer Vision subsystemwith the vagueness of the discrete concept small as expressed numerically by the trapezoidal function.6. OccurrencesIn general, already a short observation of a road vehicle allows a fairly good prediction regarding its subsequentmotion during a few seconds. Such a prediction will become even more reliable if the movements of other trafficparticipants in its environment, in particular, of other road vehicles, and properties of the road can be taken intoaccount. Since a reliable prediction of the behavior of other traffic participants is important for safe and smooth roadtraffic, it is no surprise that a highly specific vocabulary has been developed in order to communicate such behavior.Already a short introspection will reveal that such communication can be characterized as either pertaining to theshort-term movement of a single vehicle—possibly supplemented by reference to the road or to some other immedi-ately relevant object—or to some abstraction referring to an entire, usually goal-directed, sequence of such short-termmovements. We consider the first type as elementary and denote a recognizable movement primitive as an occurrence:this notion appears as sufficiently neutral to allow its potential extension to other types of movement primitives beyondthose relating to road vehicle motion.A systematic search for all verbs in a standard dictionary of the German language yielded a subset of about sixtyverbs (from among about 9200) which relate to road vehicle movements (see [20,27]). Occurrences related to thissubset can be categorized(1) as perpetuative if they tend to retain the dominant aspect of a movement without major change,(2) as mutative if they characterize the systematic change of some aspect, or(3) as terminative if they relate to the beginning or ending of a dominant movement characteristic.The algorithmic recognition of a particular occurrence will be presented here as based on logical inference al-though the approach exhibits obvious analogies to pattern recognition—provided the essential time-dependencies areneglected. Each occurrence can be characterized uniquely by a conjunction of predicates. These in turn consist of aconjunction of up to three (sub)predicates, namely(1) a PRE-Condition (PREC) which has to be satisfied before the occurrence in question could be considered torepresent a valid description of the temporal development in which the agent is involved;(2) a MONotonicity-Condition (MONC or MC) indicating the type of admissible monotonous change which may takeplace while the occurrence represents a valid description;(3) a POST-Condition (POSTC) which becomes true once the occurrence in question will no longer constitute anadequate description of the temporal development in which the agent is involved.As will be seen shortly, evaluation of temporal relations between the validity of these three (sub)predicates are essentialfor a proper characterization of occurrences. In some cases, the monotonicity condition will be irrelevant and can beomitted, in other cases only the monotonicity condition will be relevant such that pre- and post-conditions could beomitted. In a fourth variant, the pre-condition remains true while an occurrence constitutes a valid description oftemporal developments although attribute values may vary during this period, thereby preventing the satisfiability ofa monotonicity condition. If the pre-condition is identical with the post-condition, pre- and post-condition are unifiedinto a CONDition.R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391363Table 3Time-dependent (!) predicates defining occurrences which refer only to the agent. The definition of the predicate has_speed can be found inAppendix A.1.1, that of the predicate has_direction in Appendix A.1.3, and correspondingly in Appendix A.1.2 for has_modeOccurrenceTypehas_speedhas_directionhas_modeacceleratebrakedrive at constant speeddrive at regular speeddrive fastdrive forwarddrive offdrive slowlydrive straight aheaddrive very fastreversestandstopturn leftturn rightmutmutmutperpperpperptermperpmutperpperpperptermmutmutPRECmovingmovingmovingnormalhigh–zerosmallmovingvery_high–zerosmallmovingmovingMONChighersmallerconstant–––higher–––––smaller––POSTCmovingmovingmovingnormalhigh–smallsmallmovingvery_high–zerozeromovingmovingMONC––––––––straight––––leftrightCOND–––––forward––––backwards––––Fig. 8. Transductor for the recognition of perpetuative occurrences.6.1. Occurrences which refer only to the agentThese are the simplest occurrences and will be treated first. They can be characterized by (conjunctions of) threetime-dependent predicates which evaluate whether an attribute function takes on a particular (discrete) value or not.One such predicate relates to the agent’s speed, one to the agent’s direction, and one to the mode of agent motion—see Table 3. It has been discussed in Section 5 how predicates will be evaluated with respect to the validity of anattribute-value binding in case of a particular agent at a particular point in time: based on results provided by the coreComputer Vision subsystem, logic formulae—which express schematic (a-priori) knowledge about what may happenin a road traffic scene—are interpreted.6.2. Taking temporal dependencies into account: transductors for occurrence recognitionThe a-priori knowledge about temporal relations between the satisfaction of equality attributes has been codedinto the type of an occurrence (perpetuative, mutative, terminative) and into the way attribute values are requiredfor the PREC, the MONC, and the POSTC. A finite state acceptance automaton (transductor) has been designedfor each type. These transductors determine whether or not the required conditions are satisfied in the prescribedtemporal order. Supplementary predicates which appear in FMTHL formulae specifying a transductor—like, e.g.,increasing_condition(Agent,Verb)—are given in Appendix A.2.6.2.1. Transductor for perpetuative occurrencesThe transition diagram for this transductor is given in Fig. 8. This transductor is realized by the following fuzzymetric-temporal logic inferences (see Section 3 for an explanation of the logical vocabulary of FMTHL):364R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 9. Transductor for the recognition of mutative occurrences.R1: always (perpetuative(Agent,Verb) :- waiting_perp(Agent,Verb)).R2: always (waiting_perp(Agent,Verb) :- condition(Agent,Verb) ,R3: always (waiting_perp(Agent,Verb) :- has_status(Agent,X,Y,Theta,V,Psi) ,R4: always (active_perp(Agent,Verb):- condition(Agent,Verb) , ! ,1 ? waiting_perp(Agent,Verb)).increasing_condition(Agent,Verb), ! ,active_perp(Agent,Verb)).R5: always (active_perp(Agent,Verb):- has_status(Agent,X,Y,Theta,V,Psi) ,1 ? waiting_perp(Agent,Verb)).output(DoV, Verb, Agent) ,1 ? active_perp(Agent,Verb)).The heads of the rules (implications) correspond to the states of the automaton. Their bodies comprise—in additionto conditions according to each state—the name of the (possible) successor state. Evaluation starts in rule R1. Rule R2tests (according to the state waiting of the automaton), whether a certain condition is currently valid and whetherits Degree of Validity (DoV) increases during five consecutive time points. In this case, the automaton switchesinto the active state (R4–R5). Otherwise, provided additional trajectory data are available, the automaton remainswaiting (R3).Perpetuative occurrences which refer only to the agent can be evaluated just by using attributes with agent refer-ence (speed, direction, and mode). The entry for drive_slowly in Table 3 can be transformed into an FMTHLimplication according to the following example:always (condition(Agent,drive_slowly) :- has_speed(Agent,small)).In this case, the formulation of a single condition can be extracted directly from Table 3 in analogy to explanations inSection 5. Evaluation is started by a simple logical query, for example, in the case of drive_slowly:?- perpetuative(Agent, drive_slowly).6.2.2. Transductor for mutative occurrencesSee Section 3 for an explanation of the logical vocabulary of FMTHL.R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391365always (mutative(Agent,Verb)always (waiting_mut(Agent,Verb) :- precondition(Agent,Verb) ,:- waiting_mut(Agent,Verb)).mon_condition(Agent,Verb) ,postcondition(Agent,Verb) , ! ,terminating_mut(Agent,Verb)).always (waiting_mut(Agent,Verb) :- precondition(Agent,Verb) ,mon_condition(Agent,Verb) ,active_mut(Agent,Verb)).always (waiting_mut(Agent,Verb) :- has_status(Agent,X,Y,Theta,V,Psi) ,always (active_mut(Agent,Verb) :- mon_condition(Agent,Verb) ,postcondition(Agent,Verb) ,terminating_mut(Agent,Verb)).always (active_mut(Agent,Verb) :- mon_condition(Agent,Verb) ,1 ? waiting_mut(Agent,Verb)).1 ? active_mut(Agent,Verb)).always (active_mut(Agent,Verb) :- has_status(Agent,X,Y,Theta,V,Psi) ,1 ? waiting_mut(Agent,Verb)).always (terminating_mut(Agent,Verb):- postcondition(Agent,Verb) ,mon_condition(Agent,Verb) ,output(DoV, Verb, Agent) ,1 ? terminating_mut(Agent,Verb)).always (terminating_mut(Agent,Verb):- has_status(Agent,X,Y,Theta,V,Psi) ,1 ? waiting_mut(Agent,Verb)).Pre-, monotonicity- and postcondition have to be evaluated in order to determine the validity of drivingstraight ahead. These conditions can be derived directly from Table 3:always (precondition(Agent,drive_straight_ahead) :-has_speed(Agent,moving)).always (mon_condition(Agent,drive_straight_ahead) :-has_direction(Agent,straight)).always (postcondition(Agent,drive_straight_ahead) :-has_speed(Agent,moving)).6.2.3. Transductor for terminative occurrencesSee Section 3 for an explanation of the logical vocabulary of FMTHL.always (terminative(Agent,Verb) :- waiting_term(Agent,Verb)).always (waiting_term(Agent,Verb) :- precondition(Agent,Verb) , ! ,ready_term(Agent,Verb)).always (waiting_term(Agent,Verb) :- has_status(Agent,X,Y,Theta,V,Psi) , 1 ?always (ready_term(Agent,Verb):- mon_condition(Agent,Verb) ,waiting_term(Agent,Verb)).always (ready_term(Agent,Verb)1 ? active_term(Agent,Verb)).:- precondition(Agent,Verb) , ! ,1 ? ready_term(Agent,Verb)).always (ready_term(Agent,Verb):- has_status(Agent,X,Y,Theta,V,Psi) ,always (active_term(Agent,Verb) :- postcondition(Agent,Verb) , ! ,1 ? waiting_term(Agent,Verb)).1 ? terminating_term(Agent,Verb)).always (active_term(Agent,Verb) :- mon_condition(Agent,Verb) , ! ,has_status(Agent,X,Y,Theta,V,Psi) ,1 ? active_term(Agent,Verb)).always (active_term(Agent,Verb) :- waiting_term(Agent,Verb)).always (terminating_term(Agent,Verb)366R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 10. Transductor for the recognition of terminative occurrences.:- postcondition(Agent,Verb) ,mon_condition(Agent,Verb) , ! ,output(DoV, Verb, Agent) ,1 ? terminating_term(Agent,Verb)).always (terminating_term(Agent,Verb):- has_status(Agent,X,Y,Theta,V,Psi) , 1 ?waiting_term(Agent,Verb)).6.3. Occurrences which refer to agent and locationTable 4 presents—in analogy to Table 3—the definition of predicates which together characterize occurrencesreferencing both the agent and a specified location in the road traffic scene.In order to compute occurrences with reference to a location, the particular supplementary argument has to beintegrated into the inference rules for an occurrence. For example, in the case of a perpetuative occurrence withlocation reference, the rule R2 mentioned in Section 6.2.1R2: always (waiting_perp(Agent,Verb) :- condition(Agent,Verb) ,increasing_condition(Agent,Verb), ! ,active_perp(Agent,Verb)).has to be changed intoR2_L: always (waiting_perp(Agent,Location,Verb):- condition(Agent,Location,Verb) ,increasing_condition(Agent,Location,Verb), ! ,active_perp(Agent,Location,Verb)).6.4. Occurrences which refer to the agent and an additional objectTable 5 presents—in analogy to Table 3—the definition of predicates which together characterize occurrencesreferencing both the agent and a stationary object in the road traffic scene. The definition of predicates for the case ofan additional moving object can be found in Table 6.R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391367Table 4Time-dependent (!) predicates defining occurrences which refer to both the agent and a location. The symbol ‘>’ indicates a decreasingslope for the value subject to the monotonicity condition MONC, the symbol ‘<’ correspondingly an increasing slope. has_course denotesthe abbreviation of the predicate has_course_towards_loc, see Appendix A.1.4. Similarly, has_distance stands for the predicatehas_distance_to_loc, see Appendix A.1.5OccurrenceTypehas_speed(Agent)has_course(Agent,Location)has_distance(Agent,Location)arrive at locdepart from locdrive to locleave locleave loc behindpark at locpass locrun over locstop at loctermtermmuttermmutperptermperptermPRECmovingmovingmovingzeromovingzeromovingmovingmovingMONCPOSTCPRECMONCPOSTCPRECMONCPOSTC–––<––––>movingmovingmovingmovingmovingzeromovingmovingzero––appr.–leaving–appr.––––––––changing––––appr.–leaving–leaving––smallzeronot_zerozerosmallzeronot_zerozero–><>–<––––zerosmallsmall–not_zerozeronot_zerozerozeroTable 5Time-dependent (!) predicates defining occurrences which refer to both the agent and another stationary object. The predicate referred to by thecolumn heading have_course is treated in Appendix A.1.7. In addition to the predicates enumerated in this table, has_speed(Object,zero) always has to be true. See caption of Table 4 regarding the symbols ‘<’ and ‘>’OccurrencesTypehas_speed(Agent)have_course(Agent,Object)have_distance(Agent,Object)be standing nearcollide withdrive pastmerge in front ofmove away frommove towardspull out behindstart in front ofstop behindperptermmuttermmutmuttermtermtermPRECzeromovingmovingmovingmovingmovingmovingzerosmallMCPOSTCPRECMCPOSTCPRECMCPOSTC–––––––<>zerozeromovingmovingmovingmovingmovingmovingzero––passingpassingleavingappr.appr.––––constantchanging––changing––––passingleavingleavingappr.pass.––smallsmallsmallsmallsmallsmallsmallsmallsmall–>––<>–––smallzerosmallsmall––smallsmallsmallIn order to compute occurrences with additional reference to an object, the particular supplementary argument hasto be integrated into the inference rules for an occurrence. For example, in the case of a perpetuative occurrence withobject reference, the rule R2 mentioned in Section 6.2.1R2: always (waiting_perp(Agent,Verb) :- condition(Agent,Verb) ,increasing_condition(Agent,Verb), ! ,active_perp(Agent,Verb)).has to be changed intoR2_O: always (waiting_perp(Agent,Object,Verb):- condition(Agent,Object,Verb) ,increasing_condition(Agent,Object,Verb), ! ,active_perp(Agent,Object,Verb)).In this case, the characterization of an occurrence depends on whether the additional object remains stationary ormoves itself.6.5. Occurrences which refer to agent and laneTable 7 presents—in analogy to Table 3—the definition of predicates which together characterize occurrencesreferencing both the agent and a lane.368R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391(!) predicates defining occurrences which referTable 6to both the agent and another moving object. The predicateTime-dependenthave_difference_in_orientation is treated in Appendix A.1.8 and the predicate have_distance in Appendix A.1.6. The spatialrelation predicate appearing in the last two columns headed by ‘relative_position’ and ‘configuration’ are treated in Appendix A.1.9. In addition tothe predicates enumerated in this table, has_speed(Agent, moving) and has_speed(Object, moving) always have to be trueOccurrenceTypehave_difference_in_orientationhave_distancerelative_positionConfigurationPRECMCPOSTCPRECMCPOSTCPRECPOSTCPRECapproach crossingapproach oncomingcatch up withclose up tocut in front ofdraw ahead ofdrive in front offall behindflankfollowget out of the way ofleave crossingleave oncominglet run intolose a lead onmake waymerge in front ofmove pastpasspull up torun intoslip in front ofswing outmutmutmutmutmutmutperpmutmutperpmutmutmutmutmutmutmutmutmutmutmutmutmutcrossingoppositeequalcrossingcrossingequalequalequalequalequalequalcrossingoppositeequalequaloppositecrossingequalequalequalequalequalequal–constantconstant––constant–constantconstant–constant–constantconstantconstantconstantchangingconstantconstantconstantconstantconstantconstantcrossingoppositeequalequal–equalequalequalequalequalequalcrossingoppositeequalequaloppositeequalequalequalequalequalequalequalnormalnormalnormal–small––small––smallsmallsmallsmallnormalsmallsmall––smallsmallsmallsmall>>><><–<–––<<>>–––<–><>smallsmallsmall–zero––normal–––––zerosmall––––smallzero––––behind–frontfrontfrontbehindbesidebehindbehind––frontfrontbehind–besidefrontbesidebehind–behind––behindbehindfrontfrontfrontbehindbesidebehindbeside––frontfrontbesidefrontfrontfrontbesidebehindfront–––straight–––straightstraight–straight–––straightstraight–––straighthalf-leftstraighthalf-leftstraightPOSTC––straight–––straightstraight–straight–––straightstraight–straight–straightleftstraightstraighthalf-leftIn order to compute occurrences with additional reference to a lane, the particular supplementary argument has tobe integrated into the inference rules for an occurrence. For example, in the case of a perpetuative occurrence withlane reference, the rule R2 mentioned in Section 6.2.1R2: always (waiting_perp(Agent,Verb) :- condition(Agent,Verb) ,increasing_condition(Agent,Verb), ! ,active_perp(Agent,Verb)).has to be changed intoR2_L: always (waiting_perp(Agent,Lane,Verb):- condition(Agent,Lane,Verb) ,increasing_condition(Agent,Lane,Verb), ! ,active_perp(Agent,Lane,Verb)).These occurrences—see Table 7—are specified in analogy to the ones treated in preceding subsections.7. ResultsResults will be illustrated for two different kinds of road traffic, namely for vehicles crossing an inner-city inter-section and for vehicles maneuvering at a gas station. The results will be presented as graphs of Degree of Validity(DoV) (see Section 3) versus frame-number, i.e. (discretized) time, for selected occurrences.Please note that the DoV of an occurrence at a particular frame-number can imply a small temporal non-localitydue to temporal derivatives which may have been incorporated into the definition of an occurrence. In any case, theDoV-value obtained for each frame-number should be looked at as an individual experiment which can be assessedR. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391369Table 7Time-dependent (!) predicates defining occurrences which refer to both agent and lane. The predicates agent_residence, l_element andlane_ref are treated in Appendix A.1.10OccurrenceTypehas_speedhas_directionl_elementagent_residencelane_refchange lanecross a lanedrive on lanetravel on laneturnmutperpperpperpmutPRECmovingmovingmovingmovingmovingPOSTCmovingmovingmovingmovingmovingMCstraight–––not_straightMCchangingequalequal–changingPRECPOSTCon–––onon–––onMC–acrossalongalong–Fig. 11. Representative image frames from the sequence dtneu05 (left: frame 20; right: frame 400). The trajectory including a model projection ofvehicle1 is superimposed to the image frame in the left panel. The right panel comprises trajectories and model projections for vehicle2 throughvehicle5 (from [22]).as being acceptable or not. In this manner, the reader may form his own assessment rather than being confronted withpotentially highly aggregated assessments derived by test persons. As a byproduct of this way of presentation, thesensitivity of DoV-results for a selected occurrence and correlations between consecutive DoV-values in a subsequenceof frames can be taken into account. In addition, the relation between certain problems in vehicle tracking (like partialocclusion of a vehicle) and the resulting DoV-results can be detected and assessed.7.1. Selected occurrences for vehicles crossing an intersectionTwo representative frames from the image sequence dtneu05 recorded at the Durlacher-Tor-Platz in Karlsruhe areshown in Fig. 11.The left panel of Fig. 12 plots the DoV for two potential occurrence associations with the visible trajectorysegment of vehicle1 in Fig. 11(left). Most of the time, the DoV is practically 1 for the occurrence drive_at_constant_speed, i.e. the system considers this occurrence as an appropriate characterization of the behaviorof this vehicle. The right panel of Fig. 12 shows plots of the DoV for the occurrences drive_straight_ahead,turn_left and turn_right associated with the same trajectory segment. Apart from the initial phase whenmodel-based vehicle tracking had not yet stabilized sufficiently, the correct alternative drive_straight_aheadhas the highest DoV, justifying the use of this occurrence in order to describe the behavior of this vehicle during thetracked period. Either one or both of these high-DoV occurrences could be chosen, depending on the aspect of vehiclebehavior to be emphasized.The analogous plots in Fig. 13 for vehicle2 from Fig. 11(right) cover a larger temporal interval during whichthis vehicle turns left before continuing straight ahead again. The characterization drive_at_constant_speeddominates most of the time although the alternatives occasionally exhibit a higher DoV during the turning phase. Thedirectional aspect of the maneuver performed by this vehicle during the tracking period is shown in the right panel of370R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 12. (Left panel) The Degree of Validity (DoV) for the occurrences accelerate and drive_at_constant_speed plotted as afunction of frame-number for vehicle1 in Fig. 11(left). (Right panel) Analogously for the occurrences drive_straight_ahead, turn_leftand turn_right.Fig. 13. (Left panel) The Degree of Validity (DoV) for the occurrences accelerate, drive_at_constant_speed, and brakeplotted as a function of frame-number for vehicle2 in Fig. 11(right). (Right panel) Analogously for the occurrences drive_straight_ahead,turn_left and turn_right.Fig. 13. Following some initial oscillations, turn_left dominates between frame-numbers 280 and 400 after whichdrive_straight_ahead clearly takes over as the most appropriate characterization of directional behavior forthis vehicle.Results mostly analogous to those obtained for vehicle2 are shown in Fig. 14 for vehicle3. It should be noted thatthis vehicle is occluded quite severely by the advertisement column during the first part of its trajectory. This factshows up by noticeable oscillations of the DoV associated with alternative occurrence associations during this periodalthough the DoV-value stabilizes again once the vehicle can be tracked without significant occlusions.Results for vehicle5 are similar to those for vehicle1, with occasional oscillations due to occlusion by a mast, theadvertisement column, and another mast in the center of the image frame. Tracking of vehicle5 had been initializedalready prior to its (short-time partial) occlusion by the mast in the right upper quadrant of Fig. 11(right). It drovefaster than the turning vehicle3 and thus its partial occlusion by the top of the advertisement column did not affect theoccurrence associations to the same degree as in the case of vehicle3. Results for vehicle4 are similar to those shownfor vehicle5 and thus have been omitted.R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391371Fig. 14. Analogous to Fig. 13, but for vehicle3. Note that the occurrence turn_right never acquired a DoV greater than zero and thus has notbeen plotted in the right panel.Fig. 15. Analogous to Fig. 12, but for vehicle5 (without DoV-values for turn_right).7.2. Graphical illustrations of selected occurrences for the gas station sequenceFig. 1 shows four images of a scene recorded at a gas station. The whole video sequence comprises about 8.000frames or about 160 seconds (this corresponds to a scan rate of 50 frames per second). Seven vehicles were trackedsuccessfully which perform complex maneuvers during the recording period. Here, we concentrate on the maneuversperformed by ‘object_4’ during this sequence (see Fig. 1). A sample of geometric results obtained by the computervision system has already been presented in Section 4.1.The names of lanes and filling places in the gas station sequence are explained in the groundplan map of the gasstation (see Fig. 2).The two bottom panels of Fig. 1 sketch the maneuvers of ‘object_4’ during the recording period illustrated byrepresentative image frames in the top four panels. Trajectory data obtained by the model-based tracking systemXtrack have been associated with occurrences as described in the preceding sections. Fig. 16 plots the Degree ofValidity DoV for the occurrences1. be standing near,2. drive past,3. move away from, and4. move towards372R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 16. The (green) curve starting at frame 600 with DoV = 1.0 represents the degree of validity for the occurrence ‘object_4 drive pastobject_1’. The (violet) curve in the lower left corner of this graph indicates the diminishing DoV for the occurrence ‘object_4 move towardsobject_1’. Between frames 800 and about 970, the occurrence ‘object_4 move away from object_1’ (represented by the blue curve) reaches alocal maximum with DoV≈ 0.4 which nicely supplements the two other occurrences for a detailed description of ‘object_4 overtakes object_1’.The backup-maneuver of object_4 between frames 970 and about 1200 is indicated by the subsequent (light-green) curve. The subsequentoccurrence ‘object_4 be standing near object_1’ (red curve) shows up rather clearly between frames 1200 and about 1850. Then, object_1starts to move backwards (see Fig. 17). Therefore, be standing near is no longer valid (since be standing near is only defined whenboth vehicles are standing) and its DoV decreases to zero. Later, around frame 1950, object_1 stops which means that from now on be standingnear becomes valid again, but now with a lower DoV, since now the distance between object_4 and object_1 is significantly higher.(see Table 5) as a function of frame number, i.e. of time. The initial maneuvers where object_4 passes object_1 can berecognized by the complementary variations of the DoV for the occurrences move towards, drive past, andmove away from between frames 600 and about 970. After a short backward motion the subsequent occurrencebe standing near shows up unmistakably through the approximately constant value of ≈ 0.6 for the associatedDoV between frames 1200 and about 1850.Analogous results obtained for object_1 are illustrated in Figs. 18 and 19. The maneuvers of object_1 are sketchedin Fig. 17.8. Related publications and concluding discussionsA systems approach has been outlined for the algorithmic transformation of video signals into a natural languagetext describing recorded vehicle maneuvers in road traffic scenes. Elaborating a part of this framework, the discus-sion focuses on a detailed presentation of steps which transform the geometric results for 3D-model-based vehicletracking obtained by a computer vision subsystem into conceptual representations for movements of a single road ve-hicle (occurrences). An occurrence representation is associated with the ‘Conceptual Primitives Level’ of Fig. 3. Thesystem-internal representation of about sixty occurrences for the description of vehicular road traffic has been speci-fied. This is an attempt to cover all occurrences relevant for the description of motor vehicles on roads explicitly, thusfacilitating the treatment of a broad range of road traffic image sequences. In addition, it should ease the discriminationbetween cases where a performance insufficiency may be attributed to the lack of a suitable occurrence definition, toR. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391373Fig. 17. Maneuver of object_1 from the gas station sequence.Fig. 18. While driving towards the second pump of the gas station (see Fig. 17), object_1 permanently reduces its velocity. Starting at frame 300,the DoV of the occurrence drive at regular speed thus continuously decreases from approximately 0.3 to zero (red curve) whereas theDoV of drive slowly increases from approximately 0.7 to 1.0 (green curve). Then, object_1 remains standing (blue curve) until frame 1780.Between frames 1800 and 1950 the vehicle starts to back up which is indicated by the decrease of the DoV of stand and the increase of driveslowly. Between frames 1950 and 2050, the opposite performance can be observed. Here, the vehicle stands again in order to change mode.Then it drives slowly again, now in forward direction. It permanently accelerates whereby drives at regular speed becomes validagain. See Table 3 for the definition of the occurrences displayed here.an inappropriate parameterization, or to an implementation error. The price to be paid for such advantages consists inthe attention and space to be devoted to the required details.A fuzzy metric-temporal logic has been chosen as the basis for the creation and manipulation of a system-internalrepresentation of the temporal developments within the recorded scene. This choice obviates the necessity to learn,approximate, or postulate a large number of probability distributions which are required for a probabilistic inferenceapproach providing the same coverage. So far, we did not encounter experimental evidence that the fuzzy membershipfunctions explicated here caused difficulties. These membership functions are intuitively acceptable and can be easilyadapted to special requirements if that should become necessary. In addition, the same fuzzy metric-temporal inference374R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 19. This figure illustrates object_1 overtaking object_4 (see, as well, Fig. 17). The illustration starts at frame 1200 where object_1 is standingnear object_4 (red curve) until frame 1800. Then object_1 starts to back up and drive past object_4 (green curve). Then, object_1 comes to astop, which means that the DoV of drive past is reduced to zero and standing near is becoming valid again, with a smaller value for DoVbecause of the now greater distance between both vehicles. Object_1 then changes mode and begins to move towards object_4 (violet curve).The (light green) curve starting at frame 2150 represents the degree of validity for the occurrence ‘object_1 pulls out behind object_4’which decreases at frame 2200, where object_1 starts to move away from object_4 (blue curve). For a detailed description of the occurrencesmentioned here, see Table 5.mechanism has been used uniformly throughout the Conceptual Representation Level. This fact together with theoverall system approach facilitates to trace down insufficiencies in system performance and to provide remedies in themost appropriate component.Parameters have been separated from the algorithmic steps proper by definition and instantiation of occurrencesbased on formal logic. This should facilitate an analysis of the approach, in particular regarding its potential extensionto various natural languages. So far, it can be used equally well for occurrences formulated in English and German.The required processing steps have been illustrated by results obtained for vehicles crossing an inner-city intersec-tion and for vehicle maneuvers at a gas station. Altogether, thousands of frames have been analyzed in this mannerwith the conclusion that our algorithmic approach yields results generally compatible with our judgment. So far, re-maining discrepancies can be attributed to difficulties (detection, initialization, and tracking of vehicle images) in theComputer Vision subsystem.It may have been noted that an occurrence can be mapped to a verb phrase in the linguistic sense. Normally, thesubject for such verb phrases will be the vehicle of interest which we denote as agent. It thus is possible to formulate asimple ‘single-sentence text’ which comprises the agent as subject and a suitably conjugated verb phrase derived froman occurrence representation which characterizes a movement primitive in isolation. The analogy of such an approachto case-based single sentence text understanding [8] should be obvious. The ability to generate simple one-sentencetexts which describe particular aspects of a vehicle’s motion is expected to help tracking down system shortcomingsin a focused manner.We refrain from claims that this formalism can be extended to cover other occurrences because we have not yetaccumulated sufficient experience in this direction. It appears more important for us at the moment to improve theR. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391375detection and tracking of road vehicles. Such tracking results can then be exploited to subject this approach to moreencompassing tests.Given the preceding expositions, several remarks concerning earlier publications by other authors may clarify sim-ilarities of and differences between approaches. Publications which appeared prior to the year 2000 will be consideredto be accessible via introductory sections of more recent publications, in particular of recent surveys or special issues[3,4,12,34]. Such earlier publications thus will not be discussed generally in what follows. Similarly, we shall refrainin general from discussing the treatment of human movements.8.1. Publications linking traffic videos to conceptual representationsResearch by Caelli and coworkers [5,6] should be mentioned here as an early combination of information extractionfrom real-world intersection traffic images and knowledge-based approaches. A rectangular representation for vehicleimages was extracted by background subtraction and clustering of neighboring change pixels. Feature-based matching(centroid coordinates and orientation of enclosing rectangles) were used to establish correspondences between threeimage frames recorded at 400 msec time intervals in order to obtain estimates for velocity and orientation changes. Thevehicle image descriptors were then exploited to interpret a-priori knowledge about typical movements and behavior ofvehicles at inner-city road intersections. This knowledge relied on a frame-based representation in a kind of semanticnetwork in combination with rule-based evaluation of predicates.Remagnino et al. [33] report on a hybrid approach towards the generation of textual descriptions based on theevaluation of videos recorded at parking lots. A data-driven technique is used to track (isolated) persons in the imageplane. Knowledge about the pose of the recording stationary camera with respect to the ground plane then enables theauthors to estimate the 3D-scene position of the tracked person in order to relate this to vehicle positions in the scene.No details are given about how vehicle positions in the scene have been determined. A Bayesian Belief Networkrepresentation of a small number of behaviors associates pedestrian position and movements to textual formulationsof a kind to be expected from an automated video-based parking-lot surveillance system.Howarth and Buxton [17] derive a small set of both primitive and abstracted events recorded at a roundabout in thestreet traffic domain by means of a model-based tracking approach. Fernyhough et al. [7] study the derivation of eventmodels from object movements using a data-driven learning approach—see, too, [9]. Simple events like overtakingprocesses can be identified by evaluation of neighborhood relations between image segments.Liu et al. [24] derive textual descriptions from recorded traffic scenes. Fuzzy membership functions are used toderive conceptual primitives from the trajectories which describe type, speed, or association of moving objects withparticular lanes. The primitives are combined into a small set of complex movements comprising four situations ofgiving way to other objects and to an alarm situation.The change detection approach reported by Stauffer and Grimson [36] has been extended in [25] to extract and trackimage regions corresponding to moving bodies even from image sequences recorded by a non-stationary camera (asopposed to the case of a stationary camera [16]). The resulting trajectories are used in turn to infer the behavior of roadvehicles or humans in the recorded scene. For this purpose, a hierarchy of entities is proposed consisting of imagefeatures, mobile object properties, and scenarios. This representation explicates links between (schematic) high-levelevent descriptions and low-level image features. In addition, temporal relations occurring more frequently in thisapplication domain are explicated as well.8.2. On conceptualizations of temporal changesThe discussion in the preceding section did not differentiate the treatment of publications according to whether theyaddressed events or isolatable activities, let alone the problem how to concatenate (principally isolatable) activitiesinto a representation of agent behavior(s). Space limitations do not permit to enter into an in-depth discussion ofsuch terminological differences. This is the reason, too, why we do not discuss [15] which exploits instantiations ofoccurrence schemata (rather similar to the ones discussed here) for a study of more complex vehicle behavior.Neumann reported on a similar systems approach [30]: it, too, uses a system-internal representation for eventswhich is based on formal logic. This publication introduces the notion of a (time-dependent) Geometric SceneDescription (GSD) which is supposed to be provided by what we call here a core Computer Vision subsystem. Time-stamped components of this GSD provide individuals in the sense used for the interpretation of predicate sets in376R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391formal logic—see, e.g., [31]. These predicates represent the schematic (a-priori) system-knowledge required to un-derstand and describe temporal developments in the recorded video at a conceptual level. Details about the relationsbetween this approach reported in [30] and the historical development of the approach reported here can be foundin [27,29]. When Neumann formulated his approach, computing power available at current costs was smaller by abouta factor of 10 000 compared to 2004/2005 with the consequence that the extraction of a suitable GSD was simply notyet feasible at that time. In addition to now having an operational 3D-model-based vehicle tracking system available,our system differs from Neumann’s approach by relying on a fuzzy logic which provides much more flexibility toaccommodate to stochastic errors as well as to the vagueness of the conceptual terms introduced. In addition, we use ametric-temporal logic which allows to quantify temporal relations if desirable. As illustrated in the preceding sections,the entire systems approach has been implemented and tested to the extent that non-trivial experiments on real-worldvideos are feasible. It remains a question for further study, though, to investigate the conceptual differences impliedby the concepts ‘event’ vs. ‘occurrence’, the latter being oriented towards an activity aspect of temporal developmentsin the scene as opposed to the result character of such a development implicitly given by the former conceptualization.Similar questions are raised by the more recent research reported in [7]: these authors study boundary conditionsunder which system-internal representations for events in road traffic need not be designed—as it has been done bothby [30] and in this contribution—but can be learned from observations. Many of the research problems related tothese different approaches have been mentioned in the concluding sections of the publication by [7]. The reader mayprofit from comparing their conclusions to the results reported in the preceding sections. In particular, the problemwhether the set of occurrences attributed to Cahn von Seelen in [7] does indeed exhaust the conceptual space of roadvehicle motions now becomes amenable to an experimental test along the lines suggested in [7]: the ‘Diplomarbeit’by Cahn von Seelen mentioned there had been stimulated by considerations which occurred during the preparation ofan invited talk at the Alvey Vision Conference 1987 [27]. The set of elementary road vehicle movements mentionedin [7] is thus a direct predecessor of the set of occurrences documented in this contribution.8.3. Concluding remarksLooked at from a different point of view, published literature related to our subject can be subdivided roughlyinto two categories. On the one hand, more theoretically oriented approaches investigate action and event modellingusing various representation formalisms (see, e.g., [1] and [37]). These approaches are not yet applied to real data. Onthe other hand, there are approaches pursued by groups from the vision community which rely on signal-near imageprocessing in order to detect and describe observations at a higher level of abstraction. Most publications mentionedabove can be assigned to this second category. Due to their data-driven origin, these approaches are often limitedby peculiarities of the application domain or by the capabilities of the vision system. In comparison with the moretheoretically oriented approaches, examples from the latter category appear somewhat brittle at the conceptual layer.Approaches which try to connect a robust vision subsystem operating in the 3D scene domain with a comprehensiveand theoretically well-founded representation formalism on a conceptual layer (as we prefer to see our approach) arestill hard to find.Anticipated improvements could comprise system modifications which estimate any temporal derivatives requiredby occurrence definitions already in the Computer Vision subsystem, for example by extending the status vector forthe Kalman-Filter-based vehicle tracking with appropriate additional components. A suitable choice of the systemcovariance for these derivative components should provide smoother estimates than the local 5-point derivatives usednow. It remains to be seen, however, how the Kalman-Filter will react if the dimension of the status vector is increasedsubstantially, quite apart from the increase in computing time associated with such a modification. This considerationnicely illustrates the options which become available if the entire transformation from video to natural languageconcepts takes place within a single integrated systems approach.Another path for future research consists in changing the discourse domain in order to study how this would enforcemodifications to the techniques used so far. Although many other interesting topics are likely to come to one’s mindupon additional reflection, it might be worth to consider the effort required to turn the presently available experimentalsystem approach into a reliable tool. Experience suggests that efforts along this direction should be given priority, forexample by enlarging the size and variety of video sequences to be evaluated. Such experiments will facilitate todetermine the really important system parameters and to tune the overall performance by systematically varying theseparameters until the system exhibits a generally satisfactory performance. Only then will one be able to distinguishR. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391377between insufficiencies due to suboptimal parameter choices and more serious shortcomings of the entire approach.Such insights should form the basis for more fundamental research into alternatives which promise to retain theadvantages of the current approach and nevertheless allow to remove some of the insufficiencies which are likely tobecome more evident with an increasing number of experiments.It appears, though, that the system in its current form already allows to formulate much more specific researchtopics than it was possible at a time where no system with the capabilities described here was available (see [29]).In addition, the fuzzy metric-temporal logic formalism used here for the import of geometric results from the coreComputer Vision subsystem into the Conceptual Primitives Level can be used without modifications to aggregate suchconceptual primitives into higher levels of conceptual abstractions and to instantiate related schematic representations.This representational homogeneity facilitates the transfer of the methodological approach towards other applicationdomains as illustrated by treatment of human behavior in [13].AcknowledgementThe authors gratefully acknowledge the carefully prepared recommendations by all anonymous reviewers.The investigations reported here have been partial supported by the European Union FP5-project CogViSys (IST-2000-29404).Appendix A. Non-logic vocabularyThe following list comprises the definition of all relevant predicates, including the predicates which are conceivedas facts provided by the core Computer Vision subsystem (see Section 4).A.1. Attributes for occurrence analysisAs discussed in Sections 5 and 6, the notion occurrence refers to an elementary or primitive movement. Eachoccurrence is characterized by the requirement that specific spatio-temporal relations between particular attributevalues have to be satisfied.The following subsections enumerate the attributes which have been defined for the purpose of characterizingoccurrences related to elementary or primitive road vehicle movements. Forward references to Appendix A.2 regardingsupplementary predicates like derivative can be found repeatedly since it is anticipated that the subsections ofthis appendix need not be studied serially.A.1.1. Attributes related to ‘speed’See Fig. 7.always (has_speed(Agent,Value) :-has_status(Agent,X,Y,Theta,V,Psi) ,associate_speed(V,Value)).always (associate_speed(V,Value) :-degreeOfValidity(V,0,0,0.28,0.83) , Value = zero ;degreeOfValidity(V,0.28,0.83,2.78,5.56) , Value = small ;degreeOfValidity(V,2.78,5.56,12.5,16.67) , Value = normal ;degreeOfValidity(V,12.5,13.89,100.0,100.0) , Value = high ;degreeOfValidity(V,16.67,20.0,100.0,100.0) , Value = very_high ;degreeOfValidity(V,0.28,0.83,100.0,100.0) , Value = moving).always (has_speed_change(Agent,Value) :--2 ! has_status(Agent,X_2,Y_2,Theta_2,V_2,Psi_2) ,-1 ! has_status(Agent,X_1,Y_1,Theta_1,V_1,Psi_1) ,has_status(Agent,X,Y,Theta,V,Psi) ,1 ! has_status(Agent,X1,Y1,Theta1,V1,Psi1) ,378R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 20. Discretization of signed relative velocity values into the ‘modes’ backwards and forward.Fig. 21. Mapping of steering angle estimates to conceptual direction values.2 ! has_status(Agent,X2,Y2,Theta2,V2,Psi2) ,derivative(V_2,V_1,V,V1,V2,Deriv) ,associate_speed_change(Deriv,Value)).always (associate_speed_change(Deriv,Value) :-degreeOfValidity(Deriv,-100,-100,-1.11,-0.56) , Value = smaller ;degreeOfValidity(Deriv,-1.11,-0.56,0.56,1.11) , Value = constant ;degreeOfValidity(Deriv,0.56,1.11,100,100) , Value = higher).A.1.2. Attributes related to ‘mode’This attribute characterizes a movement as standing, backwards, or forward, depending on the sign andmagnitude of the velocity estimate V obtained by model-based tracking—see Fig. 20. The attribute value standingcorresponds essentially to the velocity interval where the degree of validity for backwards and forward dropsbelow 1.always (has_mode(Agent,Value) :-has_status(Agent,X,Y,Theta,V,Psi) ,associate_mode(V,Value)).always (associate_mode(V,Value) :-degreeOfValidity(V,-100,-100,-0.5,0) , Value = backwards ;degreeOfValidity(V,0,0.5,100,100) , Value = forward ;degreeOfValidity(V,-0.83,-0.28,0.28,0.83) , Value = standing).A.1.3. Attributes related to ‘direction’always (has_direction(Agent,Value) :-has_status(Agent,X,Y,Theta,V,Psi) ,has_speed(Agent,moving) ,associate_direction(Psi,Value)).R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391379Fig. 22. Mapping the angular difference between the vehicle orientation and the line connecting the current vehicle position with a specified locationin the environment into the conceptual ‘course descriptions’ approaching, passing, and leaving the location.always (associate_direction(Psi,Value) :-degreeOfValidity(Psi,-100,-100,-5,-2.5) , Value = right ;degreeOfValidity(Psi,-5,-2.5,2.5,5) , Value = straight ;degreeOfValidity(Psi,2.5,5,100,100) , Value = left ;(degreeOfValidity(Psi,-100,-100,-5,-2.5) ;degreeOfValidity(Psi,2.5,5,100,100)) , Value = not_straight).A.1.4. Attributes related to ‘course_towards_location’The supplementary predicates ang_direction, ang_diff, ang_norm, and derivative are treated inAppendix A.2.always (has_course_towards_loc(Agent,Loc,Value) :-course_of_agent_towards_loc(Agent,Loc,Course) ,associate_course_towards_loc(Course,Value)).always (course_of_agent_towards_loc(Agent,Loc,Course) :-has_status(Agent,X,Y,Theta,V,Psi) , location(Loc,XO,YO) ,ang_direction(XO-X,YO-Y,R) , ang_diff(Theta,R,Diff) ,ang_norm(Diff,Course)).always (associate_course_towards_loc(Course,Value) :-degreeOfValidity(Course,-50,-30,30,50) , Value = approaching ;degreeOfValidity(Course,30,50,130,150) , Value = passing ;degreeOfValidity(Course,130,150,180,200) , Value = leaving ;degreeOfValidity(Course,-150,-130,30,50) , Value = passing ;degreeOfValidity(Course,-200,-200,-150,-130) , Value = leaving).always (has_course_change_towards_loc(Agent,Loc,Value) :--2 ! course_of_agent_towards_loc(Agent,Loc,Course_2) ,-1 ! course_of_agent_towards_loc(Agent,Loc,Course_1) ,course_of_agent_towards_loc(Agent,Loc,Course) ,1 ! course_of_agent_towards_loc(Agent,Loc,Course1) ,2 ! course_of_agent_towards_loc(Agent,Loc,Course2) ,derivative(Course_2,Course_1,Course,Course1,Course2,Deriv) ,associate_course_change_towards_loc(Deriv,Value)).always (associate_course_change_towards_loc(Deriv,Value) :-degreeOfValidity(Deriv,-15,-5,5,15) , Value = constant ;degreeOfValidity(Deriv,-100,-100,-15,-5) , Value = changing ;degreeOfValidity(Deriv,5,15,100,100) , Value = changing).380R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 23. Mapping the distance between the current estimate for the vehicle position and a specified location in the environment into a set ofconceptual values. Note that the conceptual values for the attribute distance_to_location depends, too, on the estimated velocity: thethreshold for the assignment of, e.g., the conceptual value normal increases with increasing speed.A.1.5. Attributes related to ‘distance_to_location’An explanation of the supplementary predicates length, maximum, and derivative can be found in Appen-dix A.2.always (has_distance_to_loc(Agent,Loc,Value) :-distance_of_agent_to_loc(Agent,Loc,Distance,Offset) ,associate_distance_to_loc(Distance,Offset,Value)).always (distance_of_agent_to_loc(Agent,Loc,Distance,Offset) :-has_status(Agent,X,Y,Theta,V,Psi) , location(Loc,XO,YO) ,length(X-XO,Y-YO,Distance) , maximum(V,10,Offset)).always (associate_distance_to_loc(Distance,OS,Value) :-degreeOfValidity(Distance,-5,-5,0.5,2.5) , Value = zero ;degreeOfValidity(Distance,0.5,2.5,0.25*OS,0.75*OS) , Value = small ;degreeOfValidity(Distance,0.25*OS,0.75*OS,1.25*OS,1.75*OS) ,degreeOfValidity(Distance,1.25*OS,1.75*OS,100,100) , Value = high ;degreeOfValidity(Distance,1.5,3.5,100,100) , Value = not_zero).Value = normal ;always (has_distance_change_to_loc(Agent,Loc,Value) :--2 ! distance_of_agent_to_loc(Agent,Loc,Distance_2,Offset_2) ,-1 ! distance_of_agent_to_loc(Agent,Loc,Distance_1,Offset_1) ,distance_of_agent_to_loc(Agent,Loc,Distance,Offset) ,1 ! distance_of_agent_to_loc(Agent,Loc,Distance1,Offset1) ,2 ! distance_of_agent_to_loc(Agent,Loc,Distance2,Offset2) ,derivative(Distance_2,Distance_1,Distance,Distance1,Distance2,Deriv) ,associate_distance_change_to_loc(Deriv,Value)).R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391381Fig. 24. Mapping the distance between the current estimates for the position of the agent vehicle and that of another (patient) vehicle into a set ofconceptual values. Note that the conceptual values for the attribute distance depends, too, on the estimated relative velocity between agent andpatient, in analogy to the case of distance_to_location treated in Section A.1.5.always (associate_distance_change_to_loc(Deriv,Value) :-degreeOfValidity(Deriv,-1,-0.1,0.1,1) , Value = constant ;degreeOfValidity(Deriv,-100,-100,-1,-0.1) , Value = smaller ;degreeOfValidity(Deriv,0.1,1,100,100) , Value = higher).A.1.6. Attributes related to the ‘distance between the agent and another moving object’always (have_distance(Agent,Patiens,Value) :-distance_is(Agent,Patiens,Distance,Offset) ,associate_distance(Distance,Offset,Value)).always (distance_is(Agent,Patiens,Distance,Offset) :-has_status(Agent,X,Y,Theta,V,Psi) ,has_status(Patiens,XO,YO,ThetaO,VO,PsiO) ,length(X-XO,Y-YO,Distance) , maximum(V,10,Offset)).always (associate_distance(Distance,OS,Value) :-degreeOfValidity(Distance,-5,-5,0.5,2.5), Value = zero ;degreeOfValidity(Distance,0.5,2.5,0.25*OS,0.75*OS) , Value = small ;degreeOfValidity(Distance,0.25*OS,0.75*OS,1.25*OS,1.75*OS) ,Value = normal ;degreeOfValidity(Distance,1.25*OS,1.75*OS,100,100) , Value = high ;degreeOfValidity(Distance,1.5,3.5,100,100) , Value = not_zero).always (have_distance_change(Agent,Patiens,Value) :--2 ! distance_is(Agent,Patiens,Distance_2,Offset_2) ,-1 ! distance_is(Agent,Patiens,Distance_1,Offset_1) ,distance_is(Agent,Patiens,Distance,Offset) ,1 ! distance_is(Agent,Patiens,Distance1,Offset1) ,2 ! distance_is(Agent,Patiens,Distance2,Offset2) ,derivative(Distance_2,Distance_1,Distance,Distance1,Distance2,Deriv) ,associate_distance_change(Deriv,Value)).382R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 25. Mapping the angular difference between the vehicle orientation and the line connecting the agent and patient vehicle positions into theconceptual ‘course descriptions’ approaching, passing, and leaving the patient vehicle.always (associate_distance_change(Deriv,Value) :-degreeOfValidity(Deriv,-1,-0.1,0.1,1) , Value = constant ;degreeOfValidity(Deriv,-100,-100,-1,-0.1) , Value = smaller ;degreeOfValidity(Deriv,0.1,1,100,100) , Value = higher).A.1.7. Attributes related to ‘course’This attribute evaluates the ‘course’ of an agent vehicle with respect to another—standing—patient vehicle in anal-ogy to course_towards_location (see Appendix A.1.4). The supplementary predicates ang_direction,ang_diff, ang_norm, and derivative are treated in Appendix A.2.always (have_course(Agent,Patiens,Value) :-course_is(Agent,Patiens,Course) ,associate_course(Course,Value)).always (course_is(Agent,Patiens,Course) :-has_status(Agent,X,Y,Theta,V,Psi) ,has_status(Patiens,XO,YO,ThetaO,VO,PsiO) ,has_speed(Agent,moving) ,has_speed(Patiens,zero) ,ang_direction(XO-X,YO-Y,R) , ang_diff(Theta,R,Diff) ,ang_norm(Diff,Course)).always (associate_course(Course,Value) :-degreeOfValidity(Course,-50,-30,30,50) , Value = approaching ;degreeOfValidity(Course,30,50,130,150) , Value = passing ;degreeOfValidity(Course,130,150,180,200) , Value = leaving ;degreeOfValidity(Course,-150,-130,30,50) , Value = passing ;degreeOfValidity(Course,-200,-200,-150,-130) , Value = leaving).always (have_course_change(Agent,Patiens,Value) :--2 ! course_is(Agent,Patiens,Course_2) ,-1 ! course_is(Agent,Patiens,Course_1) ,course_is(Agent,Patiens,Course) ,1 ! course_is(Agent,Patiens,Course1) ,2 ! course_is(Agent,Patiens,Course2) ,derivative(Course_2,Course_1,Course,Course1,Course2,Deriv) ,associate_course_change(Deriv,Value)).always (associate_course_change(Deriv,Value) :-degreeOfValidity(Deriv,-15,-5,5,15) , Value = constant ;degreeOfValidity(Deriv,-100,-100,-15,-5) , Value = changing ;degreeOfValidity(Deriv,5,15,100,100) , Value = changing).R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391383Fig. 26. Mapping the angular difference between the orientation of the agent and the patient vehicle into the conceptual values equal, crossing,and opposite.A.1.8. Attributes related to the ‘difference_in_orientation’This attribute evaluates the orientation difference between the agent and a patient vehicle. The supplementarypredicates ang_diff, ang_norm, and derivative are treated in Appendix A.2.always (have_difference_in_orientation(Agent,Patiens,Value) :-difference_is(Agent,Patiens,Difference) ,associate_difference_in_orientation(Difference,Value)).always (difference_is(Agent,Patiens,Difference) :-has_status(Agent,X,Y,Theta,V,Psi) ,has_status(Patiens,XO,YO,ThetaO,VO,PsiO) ,ang_diff(ThetaO,Theta,Diff) ,ang_norm(Diff,Difference)).always (associate_difference_in_orientation(Difference,Value) :-degreeOfValidity(Difference,30,50,130,150) , Value = crossing ;degreeOfValidity(Difference,-50,-30,30,50) , Value = equal ;degreeOfValidity(Difference,130,150,400,400) , Value = opposite).always (have_change_in_difference(Agent,Patiens,Value) :--2 ! difference_is(Agent,Patiens,Diff_2) ,-1 ! difference_is(Agent,Patiens,Diff_1) ,difference_is(Agent,Patiens,Diff) ,1 ! difference_is(Agent,Patiens,Diff1) ,2 ! difference_is(Agent,Patiens,Diff2) ,derivative(Diff_2,Diff_1,Diff,Diff1,Diff2,Deriv) ,associate_change_in_difference(Deriv,Value)).always (associate_change_in_difference(Deriv,Value) :-degreeOfValidity(Deriv,-15,-5,5,15) , Value = constant ;degreeOfValidity(Deriv,-100,-100,-15,-5) , Value = changing ;degreeOfValidity(Deriv,5,15,100,100) , Value = changing).A.1.9. Attributes related to the position of agent relative to another vehicleThe agent-patient-position characterizes the position of an agent vehicle either with respect to anothervehicle alone or with respect to another vehicle and the (same or neighboring) lanes on which both vehicles cur-rently drive. In the former case, the conceptual descriptions left_of, half_left_of, half_right_of, orright_of will be used, see Fig. 27. In the latter case, the conceptual descriptions in_front_of, beside_of,or behind will be used—see Fig. 28—in order to emphasize that the position between agent and patient vehicle isgiven with respect to the road spine, i.e. by evaluation of the position difference along the lane structure.384R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391Fig. 27. The position of an agent vehicle relative to another (patient) vehicle is mapped into the conceptual values left_of, half_left_of,half_right_of, or right_of.Fig. 28. The conceptual descriptions in_front_of, beside_of, or behind emphasize that the position between agent and patient vehicle isgiven with respect to the road spine, i.e. by evaluation of the position difference along the lane structure.The representation of a lane has been presented in Appendix 4.2. The supplementary predicates ang_direction,ang_diff, and ang_norm are treated in Appendix A.2.always (relative_position(Agent,Patiens,Value) :-left_of(Agent,Patiens) , Value = left ;half_left_of(Agent,Patiens) , Value = half_left ;straight_of(Agent,Patiens) , Value = straight ;right_of(Agent,Patiens) , Value = right ;half_right_of(Agent,Patiens) , Value = half_right).always (left_of(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,degreeOfValidity(Angle,40,70,110,140)).always (half_left_of(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,(degreeOfValidity(Angle,10,30,60,80) ;degreeOfValidity(Angle,100,120,150,170))).always (straight_of(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,(degreeOfValidity(Angle,-400,-400,-175,-160) ;degreeOfValidity(Angle,-20,-5,5,20) ;degreeOfValidity(Angle,160,175,400,400))).always (half_right_of(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,(degreeOfValidity(Angle,-170,-150,-120,-100) ;R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391385Fig. 29. Mapping the angular difference between the orientation of an agent vehicle and the orientation of the lane on which it drives into theconceptual values along and across.degreeOfValidity(Angle,-80,-60,-30,-10))).always (right_of(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,degreeOfValidity(Angle,-140,-110,-70,-40)).always (have_relative_position(Agent,Patiens,Angle) :-has_status(Agent,X,Y,Theta,V,Psi) ,has_status(Patiens,XO,YO,ThetaO,VO,PsiO) ,Agent <> Patiens ,ang_direction(XO-X,YO-Y,R) , ang_norm(Theta,Theta2) ,ang_diff(R,Theta2,Diff) , ang_norm(Diff,Angle)).always (configuration(Agent,Patiens,Value) :-in_front_of(Agent,Patiens) , Value = front ;beside_of(Agent,Patiens) , Value = beside ;behind(Agent,Patiens) , Value = behind).always (behind(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,degreeOfValidity(Angle,-90,-50,50,90)).always (in_front_of(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,(degreeOfValidity(Angle,90,130,200,200) ;degreeOfValidity(Angle,-200,-200,-130,-90))).always (beside_of(Agent,Patiens) :-have_relative_position(Agent,Patiens,Angle) ,(degreeOfValidity(Angle,-160,-130,-50,-20) ;degreeOfValidity(Angle,20,50,130,160))).A.1.10. Attributes related to agent orientation relative to laneThe orientation of an agent vehicle is described by the conceptual values along or across relative to the lanesegment on which it drives—see Fig. 29. The representation of a lane has been discussed in Section 4.2. The supple-mentary predicates ang_direction, ang_diff, and ang_norm are treated in Appendix A.2.always (agent_residence(Agent,Lane,on) :- on(Agent,Lane)).always (on(Agent,Lane) :- on_lobj(Agent,Lane)).386R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391always (lane_ref(Agent,Lane,Value) :-along(Agent,Lane) , Value = along ;across(Agent,Lane) , Value = across).always (along(Agent,Lane) :-have_relative_orientation(Agent,Lane,Value) ,(degreeOfValidity(Value,-400,-400,-150,-120) ;degreeOfValidity(Value,-60,-30,30,60) ;degreeOfValidity(Value,120,150,400,400))).always (across(Agent,Lane) :-have_relative_orientation(Agent,Lane,Value) ,(degreeOfValidity(Value,-150,-120,-60,-30) ;degreeOfValidity(Value,30,60,120,150))).always (have_relative_orientation(Agent,Lane,Value) :-has_status(Agent,X,Y,Theta,V,Psi) ,on_lseg(Agent,Lseg) ,part_of(Lseg,Lane) ,segment_of_lane(L1,L2,Lseg) ,line(P1,P2,L1) , line(P3,P4,L2) ,point(P11,P12,P1) , point(P21,P22,P2) ,point(P31,P32,P3) , point(P41,P42,P4) ,ang_direction(P21-P11,P22-P12,Ang) ,ang_diff(Theta,Ang,Angdiff) ,ang_norm(Angdiff,Value)).always (l_element(Agent,Value) :-along(Agent,Lane1) , next along(Agent,Lane2) ,(Lane1 <> Lane2 , Value = changing) ;Lane1 == Lane2 , Value = equal)).A.2. Supplementary rulesang_diff Ternary predicate symbol. Returns a substitution for variable Diff , where Diff = R − T becomes true.always (ang_diff(R,T,Diff) :- Diff is R - T).ang_direction Ternary predicate symbol. Returns a substitution for variable Ang (representing the value of an anglebetween a straight line (0, 0)(X, Y ) and the positive x-coordinate of the 2D standard coordinate system).always (ang_direction(X,Y,Ang) :-X==0 , Y < 0 , Ang is -90 ;X==0 , Y >= 0 , Ang is 90 ;X>0 , Y>=0 , Ang is atan(Y/X) * 360 / 6.2831853 ;X>0 , Y<0 , Ang is atan(Y/X) * 360 / 6.2831853 + 360 ;X<0 , Ang is atan(Y/X) * 360 / 6.2831853 + 180).ang_norm Binary predicate symbol. Returns a substitution for variable Norm where the given mathematical con-straint becomes true.always (ang_norm(Ang,Norm) :-Ang >= 180 , Norm is 360 - Ang ;Ang < -180 , Norm is 360 + Ang ;Ang >= -180 , Ang < 180 , Norm is 1 * Ang).R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391387degreeOfValidity 5-ary predicate symbol. Its DoV corresponds to the function value of the trapezoidal function de-scribed by the last four arguments related to the first argument, see Section 5. The meta-predicate sp—seeAppendix B—overwrites the DoV by the arithmetic expression of its argument, see [35].always (degreeOfValidity(X,P1,P2,P3,P4) :-X >= P1 , X < P2 , Wert is (X - P1) / (P2 - P1) , sp(Wert) ;X >= P2 , X < P3 , sp(1.0) ;X >= P3 , X < P4 , Wert is (P4 - X) / (P4 - P3) , sp(Wert)).derivative 6-ary predicate symbol. Returns a substitution for variable Deriv, where the given mathematical constraintbecomes true. Used for 5-point-derivation.always (derivative(A,B,C,D,E,Deriv) :- Deriv is (-2.0*A)-B+D+(2.0*E)).increasing_condition Binary predicate symbol. True, if the DoV of condition(Agent,Verb) is increasing at five con-secutive time instants; analogously for increasing_moncondition and increasing_postcondition.always (increasing_condition(Agent,Verb) :--2 ? (A1 {condition(Agent,Verb)} B1) ,-1 ? (A2 {condition(Agent,Verb)} B2) ,(A3 {condition(Agent,Verb)} B3) ,1 ? (A4 {condition(Agent,Verb)} B4) ,2 ? (A5 {condition(Agent,Verb)} B5) ,positive_derivative(B1,B2,B3,B4,B5)).inside_segment Ternary predicate symbol. True, if 2D-point (X,Y) is inside lane segment L_Seg.always (inside_segment(X,Y,L_Seg) :- (lane_segment(L_Seg),segment_of_lane(L1,L2,L_Seg), line(P1,P2,L1), line(P3,P4,L2),test_l(X,Y,P2,P1) , test_l(X,Y,P1,P3),test_l(X,Y,P3,P4) , test_l(X,Y,P4,P2))).length Ternary predicate symbol. Returns a substitution for variable L, where the given mathematical constraintrepresenting the euclidean distance of two points in 2D-space becomes true.always (length(A,B,L) :- L is sqrt((A * A) + (B * B))).maximum Ternary predicate symbol. Returns a substitution for variable M, where the given mathematical constraintbecomes true.always (maximum(A,B,M) :- A < B , M is B ; A >= B , M is A).negative_derivative 5-ary predicate symbol. True, if the derivation of 5 points given is negative.always (negative_derivative(A,B,C,D,E) :-Derivative is (-2.0*A)-B+D+(2.0*E) ,Derivative < 0).on_lobj Binary predicate symbol. Derives a DoV for an Agent being on a lane object Lobj.always (on_lobj(Agens,Lobj) :- on_lseg(Agens,Lseg) , part_of(Lseg,Lobj)).on_lseg Binary predicate symbol. Derives a DoV for an Agent being on a lane segment Lseg.388R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391In order to relate vehicles to lanes, spatial reasoning is needed which, however, is not yet provided byFMTHL. Nevertheless, a binary non-fuzzy predicate on_lane(Agent,Lane) has been implementedwhich becomes absolutely true if the vehicle Agent is on lane Lane. Otherwise this predicate becomesabsolutely false. In order to compute the DoV ∈ {0, 1} for this predicate, the current position of the agent(obtained from corresponding results imported via the has_status predicate) is related to the endpointsof each segment of the lane (given by the geometric lane model, see Section 4).always (on_lseg(Agens,Lseg) :- has_status(Agens,X,Y,_,_,_),inside_segment(X,Y,Lseg)).output3-ary and 4-ary predicate symbol. Generates output of its arguments in the notation of metric temporal logicfacts.always (output(DoV, Verb, Agens) :- write(DoV) , write(’ | ’) ,showtime, write(Verb), write(’(’) ,write(Agens) , writeln(’).’)).always (output(DoV, Verb, Agens, Object) :- write(DoV) , write(’ | ’) ,showtime, write(Verb), write(’(’) ,write(Agens) , write(’,’) ,write(Object) , writeln(’).’)).positive_derivative 5-ary predicate symbol. True, if the derivation of 5 points given is positive.always (positive_derivative(A,B,C,D,E) :-Derivative is (-2.0*A)-B+D+(2.0*E) ,Derivative > 0).showtime predicate symbol for time output.always (showtime :-ci(Low,High),write({Low:High}),write(’ ! ’)).The meta-predicate ci returns the current time interval, see [35].test_l6-ary predicate symbols. Derives a non-fuzzy DoV whether a location (X,Y) is on the left side of a directedline from location (X1,Y1) to location (X2,Y2).always (test_l(X,Y,P1,P2) :- point(X1,Y1,P1), point(X2,Y2,P2), ! ,(S is (Y2-Y1) * (X-X1) + (X1-X2) * (Y-Y1)), (S >= 0.0)).Appendix B. Alphabetical list of constants, variables, and predicatesacrossAgentagent_residencealongalwaysang_diffang_directionang_normapproachingassociate_speedUsed as Constant and Predicate. See Appendix A.1.10.Variable.Predicate. See Appendix A.1.10.Used as Constant and Predicate. See Appendix A.1.10.Shortform of ‘1 | -i : +i !’. See Section 3.Predicate. See Appendix A.2.Predicate. See Appendix A.2.Predicate. See Appendix A.2.Constant. See Appendices A.1.4, A.1.7.Predicate. See Appendix A.1.1.R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391389atanbackwardsbehindbesidechangingciconfigurationconstantcrossingdegreeOfValidityderivativeequalforwardfronthalf_lefthalf_righthas_course_towards_lochas_distance_to_lochas_modehas_speedhas_statushave_coursehave_difference_in_orientationhave_distancehighhigher-i, +iincreasing_conditioninside_segmentisl1, l2, . . .Lane, Lane1, . . .lane_reflane_segmentleavingleftl_elementlengthlinelobj_1, lobj_2, . . .maximummovingnegative_derivativenextnormalnot_straightnot_zeroobj_1, obj_2, . . .onon_lobjon_lsegoppositeFunction (Arcustangens). See Appendix A.2.Constant. See Appendix A.1.2.Constant. See Appendix A.1.9.Constant. See Appendix A.1.9.Constant. See Appendices A.1.4, A.1.7, A.1.8, A.1.10.Meta-Predicate. Returns current time interval; see [35].Predicate. See Appendix A.1.9.Constant. App. A.1.1, A.1.4, A.1.5, A.1.6, A.1.7, A.1.8.Constant. See Appendix A.1.8.Predicate. See Section 5 and Appendix A.2.Predicate. See Appendix A.2.Constant. See Appendices A.1.8, A.1.10.Constant. See Appendix A.1.2.Constant. See Appendix A.1.9.Constant. See Appendix A.1.9.Constant. See Appendix A.1.9.Predicate. See Appendix A.1.4.Predicate. See Appendix A.1.5.Predicate. See Appendix A.1.2.Predicate. See Appendix A.1.1.Predicate. See Section 4.1.Predicate. See Appendix A.1.7.Predicate. See Appendix A.1.8.Predicate. See Appendix A.1.6.Constant. See Appendices A.1.1, A.1.5, A.1.6.Constant. See Appendices A.1.1, A.1.5, A.1.6.Temporal constants. See Section 3.Predicate. See Appendix A.2.Predicate. See Appendix A.2.Operator. See Section 3.Proper names for lines.Variables (for a lane)Predicate. See Appendix A.1.10.Predicate. See Section 4.2.Constant. See Appendices A.1.4, A.1.7.Constant. See Appendices A.1.3, A.1.9.Predicate. See Appendix A.1.10.Predicate. See Appendix A.2.Predicate. See Section 4.2.Proper names for lanes and lane segments.Predicate. See Appendix A.2.Constant. See Appendix A.1.1.Predicate. See Appendix A.2.Shortform of ‘+1 !’. See Section 3.Constant. See Appendices A.1.1, A.1.5, A.1.6.Constant. See Appendix A.1.3.Constant. See Appendices A.1.5, A.1.6.Proper names for Vehicles.Predicate. See Appendix A.1.10.Predicate. See Appendix A.2.Predicate. See Appendix A.2.Constant. See Appendix A.1.8.390R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391outputP1, P2, ...p1, p2, . . .part_ofpassingpointpositive_derivativePsirelative_positionrightsegment_of_laneshowtimesmallsmallerspstandingstraighttest_lThetaVValuevery_highXYzero+, -, *, /, <, =, >=ReferencesPredicate. See Appendix A.2.Variables (for point coordinates).Proper names for Points.Predicate. See Section 4.2.Constant. See Appendices A.1.4, A.1.7.Predicate. See Section 4.2.Predicate. See Appendix A.2.Variable (for steering angle of a vehicle). See Section 4.1.Predicate. See Appendix A.1.9.Constant. See Appendices A.1.3, A.1.9.Predicate. See Section 4.2.Predicate. See Appendix A.2.Constant. See Appendices A.1.1, A.1.5, A.1.6.Constant. See Appendices A.1.1, A.1.5, A.1.6.Meta-Predicate. Overwrites the Degree of Validity (DoV) by the arithmeticexpression of its argument; see [35].Constant. See Appendix A.1.2.Constant. See Appendices A.1.3, A.1.9.Predicate. See Appendix A.2.Variable (for orientation of a vehicle). See Section 4.1.Variable (for velocity).Variable (for conceptual or arithmetic constants).Constant. See Appendix A.1.1.Variable (for x-coordinate of points).Variable (for y-coordinate of points).Constant. See Appendices A.1.1, A.1.5, A.1.6.Arithmetic and Relational Operators. See Section 3.[1] V. Akman, S.T. Erdogan, J. Lee, V. Lifschitz, H. Turner, Representing the zoo world and the traffic world in the language of the causalcalculator, Artificial Intelligence Journal 153 (2004) 105–140.[2] M. Arens, R. Gerber, and H.-H. Nagel, Conceptual representations between video signals and natural language descriptions, Institut fürAlgorithmen und Kognitive Systeme, Universität Karlsruhe (TH), 76128 Karlsruhe, Germany, January 2004.[3] H. Buxton, Learning and understanding dynamic scene activity: A review, Image and Vision Computing 21 (1) (2003) 125–136.[4] H. Buxton, A. Mukerjee, Conceptualizing images, Image and Vision Computing 18 (2) (2000) 79.[5] T. Caelli, W.F. Bischof, Machine Learning and Image Interpretation, Plenum Press, New York, London, 1997.[6] S. Dance, T. Caelli, Z.-Q. Liu, Picture Interpretation—A Symbolic Approach, World Scientific Publishing Co., Singapore, 1995.[7] J. Fernyhough, A.G. Cohn, D.C. Hogg, Constructing qualitative event models automatically from video input, Image and Vision Comput-ing 18 (2) (2000) 81–103.[8] C.J. Fillmore, The case for case, in: E. Bach, R.T. Harms (Eds.), Universals in Linguistic Theory, Holt, Rinehart & Winston, New York, 1968,pp. 1–90.[9] A. Galata, A. Cohn, D. Magee, D. Hogg, Modeling interaction using learnt qualitative spatio-temporal relations and variable length Markovmodels, in: F. van Harmelen (Ed.), Proc. 15th European Conference on Artificial Intelligence (ECAI-2002), 21–26 July 2002, Lyon, France,IOS Press, Amsterdam, 2002, pp. 741–745.[10] R. Gerber, H.-H. Nagel, ‘Occurrence’ extraction from image sequences of road traffic scenes, in: L. van Gool, B. Schiele (Eds.), Pro-ceedings Workshop on Cognitive Vision, 19–20 September 2002, ETH, Zurich, Switzerland, 2002, pp. 1–8, http://www.vision.ethz.ch/cogvis02/finalpapers/gerber.pdf.[11] R. Gerber, H.-H. Nagel, Algorithmic conversion of road traffic videos into natural language descriptions, Technical Report, Institut für Algo-rithmen und Kognitive Systeme, Universität Karlsruhe (TH), 76128 Karlsruhe, Germany, January 2005.[12] S. Gong, H. Buxton, Understanding visual behaviour, Image and Vision Computing 20 (12) (2002) 825–826.[13] J. Gonzàlez i Sabató, Human sequence evaluation: the key-frame approach, Doctoral Dissertation, Universitat Autònoma de Barcelona, Bel-laterra, Spain, May 2004. Ediciones Gráficas Rey, S.L., 2004, ISBN 84-933652-2-X. http://www.cvc.uab.es/poal/hse/hse.htm.[14] M. Haag, H.-H. Nagel, Combination of edge element and optical flow estimates for 3D-model-based vehicle tracking in traffic image se-quences, International Journal of Computer Vision 35 (3) (1999) 295–319.[15] M. Haag, H.-H. Nagel, Incremental recognition of traffic situations from video image sequences, Image and Vision Computing 18 (2) (2000)137–153.R. Gerber, H.-H. Nagel / Artificial Intelligence 172 (2008) 351–391391[16] S. Hongeng, R. Nevatia, Multi-agent event recognition, in: Proc. 8th Int. Conference on Computer Vision (ICCV 2001), vol. II, 9–12 July2001, Vancouver, BC, Canada, IEEE Computer Society, Los Alamitos, CA, 2001, pp. 82–91.[17] R.J. Howarth, H. Buxton, Conceptual descriptions from monitoring and watching image sequences, Image and Vision Computing 18 (2)(2000) 105–135.[18] H. Kamp, U. Reyle, From Discourse to Logic, Kluwer Academic Publishers, Dordrecht, Boston, London, 1993.[19] D. Koller, K. Daniilidis, H.-H. Nagel, Model-based object tracking in monocular image sequences of road traffic scenes, International Journalof Computer Vision 10 (1993) 257–281.[20] H. Kollnig, H.-H. Nagel, Ermittlung von begrifflichen Beschreibungen von Geschehen in Straßenverkehrsszenen mit Hilfe unscharfer Mengen,Informatik—Forschung und Entwicklung 8 (1993) 186–196 (in German).[21] H. Kollnig, H.-H. Nagel, 3D pose estimation by directly matching polyhedral models to gray value gradients, International Journal of ComputerVision 23 (3) (1997) 283–302.[22] S. Krieger, M. Arens, R. Gerber, H.-H. Nagel, Estimation of vehicle color and its use for text generation from traffic videos, CogViSys–FinalReport, Institut für Algorithmen und Kognitive Systeme, Universität Karlsruhe (TH), Karlsruhe, Germany, February 2005.[23] H. Leuck, H.-H. Nagel, Automatic differentiation facilitates OF-integration into steering-angle-based road vehicle tracking, in: Proc. IEEEConference on Computer Vision and Pattern Recognition CVPR’99, vol. 2, 23–25 June 1999, Fort Collins, CO, pp. 360–365.[24] Z.-Q. Liu, L.T. Bruton, J.C. Bezdek, J.M. Keller, S. Dance, N.R. Bartley, C. Zhang, Dynamic image sequence analysis using fuzzy measures,IEEE Transactions on Systems, Man, and Cybernetics—Part B: Cybernetics 31 (4) (2001) 557–572.[25] G. Medioni, I. Cohen, F. Brémond, S. Hongeng, R. Nevatia, Event detection and analysis from video streams, IEEE Transactions on PatternAnalysis and Machine Intelligence (PAMI) 23 (8) (2001) 873–889.[26] http://kogs.iaks.uni-karlsruhe.de/motris/.[27] H.-H. Nagel, From image sequences towards conceptual descriptions, Image and Vision Computing 6 (2) (1988) 59–74, Invited Lecturepresented at the Third Alvey Vision Conference, 15–17 September 1987, Cambridge, UK.[28] H.-H. Nagel, Image sequence evaluation: 30 years and still going strong, in: A. Sanfeliu, J.J. Villanueva, M. Vanrell, R. Alquézar, J.-O. Ek-lundh, Y. Aloimonos (Eds.), Proc. 15th International Conference on Pattern Recognition ICPR-2000, vol. 1, 3–7 September 2000, Barcelona,Spain, IEEE Computer Society, Los Alamitos, CA, 2000, pp. 149–158.[29] H.-H. Nagel, Steps toward a cognitive vision system, AI-Magazine 25 (2) (2004) 31–50.[30] B. Neumann, Natural language description of time-varying scenes, in: D. Waltz (Ed.), Semantic Structures—Advances in Natural LanguageProcessing, Lawrence Erlbaum Associates, Publ., Hillsdale, NJ, 1989, pp. 167–206 (Chapter 5, Report FBI-HH-B-105/84, FachbereichInformatik der Universität Hamburg, Hamburg, Germany).[31] B. Neumann, Th. Weiss, Navigating through logic-based scene-models for high-level scene interpretations, in: J.L. Crowley, J.H. Piater,M. Vincze, K. Paletta (Eds.), Proc. 3rd International Conference on Computer Vision Systems (ICVS 2003), 1–3 April 2003, Graz, Austria,in: Lecture Notes in Computer Science, vol. 2626, Springer-Verlag, Berlin, Heidelberg, New York, 2003, pp. 212–222.[32] M. Otte, H.-H. Nagel, Estimation of optical flow based on higher order spatiotemporal derivatives in interlaced and non-interlaced imagesequences, Artificial Intelligence Journal 78 (1995) 5–43.[33] P. Remagnino, T. Tan, K. Baker, Agent oriented annotation in model based visual surveillance, in: Proc. Sixth International Conference onComputer Vision (ICCV 1998), 4–7 January 1998, Bombay, India, Narosa Publishing House, New Delhi, India, 1998, pp. 857–862.[34] M. Shah, Guest introduction: the changing shape of computer vision in the twenty-first century, International Journal of Computer Vision 50 (2)(2002) 103–110.[35] K.H. Schäfer, Unscharfe zeitlogische Modellierung von Situationen und Handlungen in Bildfolgenauswertung und Robotik, Dissertation,Fakultät für Informatik der Universität Karlsruhe (TH), Juli 1996 (in German). Erschienen in der Reihe ‘Dissertationen zur Künstlichen Intel-ligenz (DISKI)’, Band 135, Sankt Augustin: infix 1996, ISBN 3-89838-135-8. This book is available from Akademische VerlagsgesellschaftGmbH, Berlin, Germany; it can be ordered via their internet website http://www.aka-verlag.de.[36] C. Stauffer, W.E.L. Grimson, Learning patterns of activity using real-time tracking, IEEE Transactions on Pattern Analysis and MachineIntelligence (PAMI) 22 (8) (2000) 747–757.[37] M. Ying, H. Wang, Lattice-theoretic models of conjectures, hypothesis and consequences, Artificial Intelligence 139 (2002) 253–267.