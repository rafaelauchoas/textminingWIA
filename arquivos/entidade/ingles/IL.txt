2202nuJ7]VC.sc[1v86330.6022:viXraIL-MCAM: An Interactive Learning and Multi-channel AttentionMechanism-based Weakly Supervised Colorectal Histopathology ImageClassification ApproachHaoyuan Chena, Chen Lia,‚àó, Xiaoyan Lib,‚àó, Md Mamunur Rahamana, Weiming Hua, Yixin Lia, Wanli Liua,Changhao Suna,c, Hongzan Sund, Xinyu Huange, Marcin GrzegorzekeaMicroscopic Image and Medical Image Analysis Group, College of Medicine and Biological Information Engineering,Northeastern University, ChinabDepartment of Pathology, Cancer Hospital of China Medical University, Liaoning Cancer Hospital and Institute, ChinacShenyang Institute of Automation, Chinese Academy of Sciences, ChinadDepartment of Radiology, Shengjing Hospital of China Medical University, ChinaeInstitute of Medical Informatics, University of Luebeck, GermanyAbstractIn recent years, colorectal cancer has become one of the most significant diseases that endanger humanhealth. Deep learning methods are increasingly important for the classification of colorectal histopathologyimages. However, existing approaches focus more on end-to-end automatic classification using computersrather than human-computer interaction. In this paper, we propose an IL-MCAM framework. It is basedon attention mechanisms and interactive learning. The proposed IL-MCAM framework includes two stages:automatic learning (AL) and interactivity learning (IL). In the AL stage, a multi-channel attention mech-anism model containing three different attention mechanism channels and convolutional neural networks isused to extract multi-channel features for classification. In the IL stage, the proposed IL-MCAM frame-work continuously adds misclassified images to the training set in an interactive approach, which improvesthe classification ability of the MCAM model. We carried out a comparison experiment on our datasetand an extended experiment on the HE-NCT-CRC-100K dataset to verify the performance of the proposedIL-MCAM framework, achieving classification accuracies of 98.98% and 99.77%, respectively. In addition,we conducted an ablation experiment and an interchangeability experiment to verify the ability and inter-changeability of the three channels. The experimental results show that the proposed IL-MCAM frameworkhas excellent performance in the colorectal histopathological image classification tasks.Keywords: Colorectal cancer histopathology image, Attention mechanism, Interactivity learning, Imageclassification1. IntroductionCancer is a life-threatening disease caused by theover-proliferation of cells in the human body. Be-cause cancer growth is uncontrollable and irregu-lar, cancer can invade the surrounding tissues andrapidly metastasise to other body parts through thecirculatory or lymphatic systems. Colorectal cancer(CRC) is a common type of intestinal cancer. CRC‚àóCorresponding author:‚àó‚àóCorresponding author:Email addresses: lichen201096@hotmail.com (ChenLi), lixiaoyan@cancerhosp-ln-cmu.com (Xiaoyan Li)Preprint submitted to Journal of LATEX Templatesis initially a polyp, and with time it transforms intocancerous cells. CRC has a high incidence and mor-tality rate. Among cancers, it has the third highestincidence and second highest mortality rate in theworld [1, 2]. In China, the age-standardized inci-dence of CRC in 2018 is 28.1 per 100,000 and 19.4per 100,000 for men and women, respectively, andthis is increasing annually [3]. Therefore, it is im-perative for doctors to diagnose colorectal cancerquickly and accurately.The traditional approach for diagnosing CRC isfor histopathological examination. The patholo-gist stains the tissue specimen with haematoxylinJune 8, 2022   and eosin (H&E), and then determines the area ofmalignancy by observing changes in cell morphol-ogy and tissue composition under a microscope [4].However, the results obtained by pathologists areoften time-consuming and highly subjective, whichmakes histopathological evaluation by pathologistsalone inadequate [5]. Therefore, the emergence ofrapid and efficient computer-aided diagnosis (CAD)technology is crucial. CAD assists doctors in im-proving the quality and efficiency of diagnosis inmedical images through image processing, patternrecognition and machine learning [6].Traditional CAD approaches usually use classicalmachine learning methods, which work as follows:First, the image features, such as shape, colourand texture, are extracted manually. Then theextracted features are classified by a classifier [7].With the advent of deep learning, the subjectiveextraction of features in machine learning has beenreplaced by automatic feature learning in comput-ers using convolutional neural network (CNN) mod-els, thereby dramatically increasing the accuracyand efficiency of CAD [8]. However, CNN mod-els have a disadvantage in that they do not appro-priately extract valid information from small-scaledatasets. This disadvantage makes it especially im-portant to combine CNN models with an attentionmechanism (AM). AM is an approach that assignthe computational resources in favour of the mostinformative component of the signal [9]. AM ap-proaches mainly represent the automatic selectionof attention regions in computer vision tasks. Dif-ferent AM approaches can be separated into spa-tial, channel and mixed domains depending on thedifferent priorities of computational resource allo-cation, leading to different AM approaches havingdifferent attention regions within the same task.In medical image datasets, complex componentsand limited differentiation between different stagesmake identification of attention regions difficult us-ing a single AM. Consequently, we propose the IL-MCAM framework: a weakly supervised learningapproach based on multi-channel attention mecha-nism (MCAM) and interactive learning (IL) to im-prove accuracy in colorectal histopathological im-age classification (CHIC) tasks. The whole processof this approach is shown in Fig. 1.The training process is separated into two stages:automatic learning (AL) stage and IL stage. TheAL stage is performed with the MCAM model, con-sisting of three channels: spatial information chan-nel (SIC), multi-scale global information channel2(MGIC) and multi-scale spatial information chan-nel (MSIC). The training images are input into theMCAM model to obtain the model parameters ofthe AL stage using the weighted voting approachafter several epochs. In the IL stage, the validationimages are input into the previously trained modelfor classification. Misclassified images are manuallylabelled with attention regions and then added tothe training set for retraining. This process is iter-ated several times until no new errors are generatedin the IL stage. Finally, the model parameters ofthe final iteration are preserved, and the test imagesare input to obtain the CHIC task results.The contributions of this paper are as follows:‚Ä¢ First, the MCAM model can identify the at-tention regions as accurately as possible in thechannel and spatial dimensions by integratingdifferent attention mechanisms, thereby im-proving the accuracy of the CHIC task in theAL stage.‚Ä¢ Second, the IL approach manually labels at-tention regions, enabling modification of theerrors caused by the MCAM model in the ALstage, which further improves the accuracy ofthe CHIC task in the IL stage.This paper is organised as follow: Section 2 pro-vides a review of the status of CHIC tasks in thepast few years, Section 3 details the approaches inthis paper, Section 4 presents experimental results,Section 5 analyses the reasons based on the exper-imental results. Finally, Section 6 concludes thepaper with a brief conclusion.2. Related Work2.1. Classification tasks in Colorectal Histopathol-ogy ResearchIn CHIC tasks, there are several examples of ma-chine learning approaches for classification by man-ually extracting image features into classifiers. In[10], local binary pattern (LBP) texture featureswith an integrated contrast measured through asupport vector machine (SVM) classifier obtained a99.5% accuracy in normal-abnormal binary classifi-cation of 643 patient-level images. In [11], 60 nor-mal and abnormal images are extracted with meanand variance features and grey-level co-occurrencematrix (GLCM) features. The extraction featuresare used for classification by the SVM classifier,Figure 1: The whole process of IL-MCAM framework.and an 89.5% F1-score in binary classification isobtained after 3-fold cross-validation. In [12], theexperimental process is as follows: The texture fea-tures,including LBP, Haraclick features and lo-cal intensity order patterns, are dimensionally re-duced dimensional using principal component anal-ysis, and then different classifiers are used to clas-sify the reduced dimensional features. The experi-ment achieves 91.3% accuracy for 464 images us-ing the SVM classifier.In [13], the integrationfeatures, including three different texture features,Laplacian-of-Gaussian filter, discrete wavelets andGLCM, are used for classification in a linear dis-criminant analysis classifier. An accuracy of 98.2%is obtained for 480 colorectal histopathology im-ages.In [14], the histogram-low features of 5000patch-level images using the rbf-SVM classifier ob-tained 98.6% accuracy in the binary classificationproblem.In recent years, with the eminence of deep learn-ing in regular image classification tasks, an in-creasing amount of research has been conducted onCHIC tasks.In [15], a semi-supervised classifica-tion approach based on restricted Boltzmann ma-chines (RBMs) is proposed. This approach uses thefeatures of the sub-regions in the image for learn-ing. A deep belief network of consecutive RBMsis constructed to extract pixels. The activationvalues of the hidden unit nodes of the RBM areused as the final features. The extracted featuresare learned using an unsupervised clustering ap-proach. Two datasets containing 3236 and 1644images is used in multi-classification and obtainedaccuracies of 96.11% and 78.99%, respectively. In[16], a new adaptive CNN implementation modelthat performs well even in low-resolution and con-strained images is proposed. Using this approach,an accuracy of 94.5% is achieved for 3200 patch-3level images. In [17], a CNN based on a modifiedVGG model is proposed for CRC classification. Anaccuracy of 82% is achieved for 10280 images. In[18], a dynamic ensemble learning method is pro-posed for a multiclass CHIC task. This approachfirst uses transfer learning to train each model andthen a particle swarm optimisation algorithm to se-lect and integrate the models. It obtains an accu-racy of 94.52% for 5000 patch-level histopathologyimages using the ResNet-121 architecture. In [19],a multi-classification accuracy of 95.3% is achievedfor 410 patient-level images using a combination ofIn [20], anclassical CNN and CapsNet models.ensemble model based on Xception, DenseNet-121and InceptionResNet-V2 achieves 92.83%, 96.16%and 99.13% accuracy for CRC-5000, NCT-CRC-HE-100K and the merged datasets, respectively.Similarly,in [21], the ResNet model using fine-tuning achieves 96.77%, 99.76% and 99.98% accu-racy for CRC-5000, NCT-CRC-HE-100K and themerged datasets, respectively.In [22], a 92.083%accuracy is obtained by evaluating 108 differentcombinations of features and classifiers on the CRC-5000 dataset. In [23], an encoder unit of an autoen-coder module and a modified DenseNet-121 archi-tecture are used for the purpose approach. Thisapproach, has an accuracy of 97.2% for the Zenodo-100K colorectal histopathology dataset. In [24], theResNet-50 model is used on private datasets andobtains an overall accuracy higher than 80%.2.2. Overview of Deep Learning MethodsIn computer vision tasks, CNN models are themost used deep learning methods. The continuousimprovement of transformer models and multilayerperceptron (MLP) models has made them popu-lar. Especially, deep learning methods are widelyused in many biomedical image analysis tasks, suchModel parameters for the i-thiterationnew error image?NoYesFinal model parametersEvaluationPrecision  RecallF1-Score  AccuracyTraining DataNormalAbnormalValidationDataNormalAbnormal(a)Data Pre-processingRotation and mirroring‚Ä¶(b) Training ProcessTestDataNormalAbnormal(c) Test ProcessSpatial Information Channel (SIC) Saving the model weights of each channel by weighted votingManually labeled attention region for retraining (IL Stage)Multi-scale Global Information Channel(MGIC)Multi-scale Spatial Information Channel(MSIC)Multi-channel Attention Mechanism (MCAM) Model (AL Stage)as COVID-19 idetificaion [25, 26], microorganismimage analysis [27, 28, 29], histopatholgical imageanalysis [30, 31, 32, 33, 34] , cytopathological im-age analysis [35, 36, 37] and sperm image analy-sis [38, 39].The first application of the CNN model wasLeNet, proposed by LeCun et al.in 1989 [40]. In2012, Krizhevsky et al. proposed AlexNet, whichthat uses the powerful parallel computing abilityof the graphics processing unit to process severalmatrix operations during training [41]. Since then,deep learning methods have formally replaced tradi-tional machine learning methods. The subsequentimprovements for the CNN models focus on threeaspects: network depth, network width and hy-brid network depth and width. The VGG [42],ResNet [43] and DenseNet [44] models increase thenetwork depth by using small convolutional layers,residual mechanisms and dense layers to improvemodel performance. The Inception-V3 [45] andXception [46] models enhance the network widthby using multi-scale inception blocks and separa-ble convolutional blocks. Some models such as In-ceptionResNet [47] and ResNeXt [48] enhance thenetwork‚Äôs depth and width by combining inceptionblocks and residual mechanisms in the feature ex-traction layer of the network, thereby improving theclassification task performance.Transformer models were initially proposed byin 2017 for natural language pro-Vaswani et al.cessing tasks [49]. In recent years they have movedto computer vision tasks. Transformer modelsare divided into two main categories, pure trans-former models and transformer models combinedwith CNN [50]. Pure transformer models includeViT [51], DeiT [52], CaiT [53] and T2T-ViT [54]models. These models directly input the imageinto the transformer encoder after position encod-ing. Transformer models combining with CNNs arethe BoTNet [55], CoaT [56] and LeViT [57] models,which input the feature maps obtained by convolu-tion of the images into the transformer encoder.The currently implemented MLP models are theimproved versions of transformer models. TheMLP-mixer [58] model is improved by replacingthe self-attention layers of the ViT model with sev-eral perceptrons. The gMLP [59] and ResMLP [60]models add a gate mechanism and a residual mech-anism to the MLP-mixer model to improve the per-formance.43. IL-MCAM frameworkThe whole process of the IL-MCAM frameworkis shown in Fig. 2.Step 1:In Fig. 2-(a), the original images areproportionally divided into training, validation andtest sets.Step 2: In Fig. 2-(b), the training set images aredata augmented using the rotation and expansionapproach.Step 3: In Fig. 2-(c.1), the AL stage is imple-mented by inputting the training set images intothe MCAM model for training. The MCAM com-prises of three parallel channels, and each channelis composed of CNN models with integrated atten-tion mechanisms, namely SIC, MGIC, and MSIC.The differences between the three different atten-tion mechanism channels and their correspondingCNN models are listed in Table 1. SIC consists ofthe VGG-16 [42] model-integrated mixed-domainSimAM [61] to extract spatial image information,and each SimAM block is added to the VGG-16 model after each convolutional layer. MGICconsists of the Inception-V3 [62] model integratedchannel domain SE [63] to extract image multi-scale global channel information, and each SE blockis added to the Inception-V3 model after each in-ception block. MSIC consists of the Xception [64]model integrated low-cost channel domain ECA [65]to extract image multi-scale local channel informa-tion, and each ECA block is added to the Xcep-tion model after each flow block. The three trainedchannels are distributed using the weighted votingapproach for each channel component weight.Step 4: In Fig. 2-(c.2), the IL stage uses a human-machine interaction. The misclassified images inthe validation set are labelled with attention re-gions and then input into the training set for re-training with a transfer learning approach. Thisprocess is repeated for several iterations until thereare no new misclassified images in the validationset. The model parameters of the last iteration arereserved as the final output.Step 5:In Fig. 2-(d), the test set images aretested using the last reserved model parameters toobtain the final classification results.This section is composed as follows: CNNs areexplained in Section 3.1, the transfer learning ap-proach is described in Section 3.2, building aMCAM model is detailed in Section 3.3, and inter-active learning strategy is explained in Section 3.4.3.1. Convolutional Neural Network (CNN)A CNN is a feedforward neural network thatincludes the computation of the convolution anddepth structure. A CNN consists of several lay-ers,including a convolution layer, pooling layer,and fully connected layer. The convolutional layer,which is the core of the CNN extracts image fea-tures using a convolution kernel. The pooling layeris used to compress the input feature map and ex-tract the main features. The fully connected layerconnects all features and classifies the output fea-tures using a classifier.In a CNN, the informa-tion extracted by the convolution layers of differ-ent networks is separated into two main categories:global and local. Global information refers to themacroscopic representation of an image in its classand is usually extracted by large convolution ker-nels and positional coding. Local information, alsodescribed as spatial information, represents the fea-tures of a restricted region of the image in its classand is usually extracted by a small convolution ker-nel.The Visual Geometry Group (VGG) proposedthe VGG-16 [42] model at the University of Ox-ford in 2014. Its novelty contribution is raising thedepths of networks from eight to 16, and convertinglarge convolution kernels such as 7√ó7 and 5√ó5 intotwo or three 3√ó3 small convolution kernels. It is an-other milestones in deep learning after AlexNet [41]and the baseline for comparing new methods in thefield of deep learning. The VGG model has sig-nificant advantages. It uses the small convolutionkernel to enhance the extraction of spatial informa-tion better [66].The Inception-V3 [62] model is another method[41] and is based onthat modifies AlexNetinGoogLeNet [67], proposed by Szegedy et al.2015.Instead of using the conventional methodto increase the number of network layers, theInception-V3 model uses a novel convolutionmethod to decompose large filter sizes by using par-allel convolution and factorised convolution. Theentire decomposition module is called the inceptionstructure. Moreover, this model has five differentinception structures, each with its own set of com-ponents. The Inception-V3 model uses an inceptionmodule instead of a large convolution kernel and aglobal average pooling layer instead of a fully con-nected layer, to substantially reduce the number ofparameters compared with other models. AmongFigure 2: The structure of the proposed IL-MCAM framework.5(a) Data DistributionOriginal DataNormalAbnormal(c.1) MCAM Model in AL Stage‚Ä¶‚Ä¶Training SetValidation SetTest Set‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶Training Set‚Ä¶‚Ä¶‚Ä¶Data Augment(b) Data Pre-processingConv 3√ó3SimAM BlockSIC√ó2Max PoolingInputMGICMSICConv 3√ó3Conv 3√ó3(c) Training Process(c.2) IL StageEntry flowReLUSeparable ConvReLUSeparable ConvMax Pooling+Conv 3√ó3Middle flowReLUSeparable ConvReLUSeparable Conv+ReLUSeparable ConvConv 3√ó3SimAM Block√ó2Max PoolingConv 3√ó3SimAM Block√ó3Max PoolingConv 3√ó3SimAM Block√ó3Max PoolingConv 3√ó3SimAM Block√ó3Max PoolingFC LayerSoftMaxOutPut1Outputùúîùúî1FC LayerSoftMaxOutPut2FC LayerSoftMaxOutPut3ùúîùúî2ùúîùúî3Validation Set‚Ä¶‚Ä¶Model parameters for the i-thiterationThis iteration generates new error image?(d) Test ProcessLabels the attention region of the newly misclassified images in the validation set and adds them to the next iteration of transfer learning Test Set‚Ä¶‚Ä¶Model parameters for the final interactivity learningRetain the model parameters of the last iterationFTPrecisionRecallF1-ScoreAccuracyEvaluation√ó3Max PoolingConv 3√ó3√ó3Max PoolingMax PoolingInception ESE Block√ó2Inception DSE BlockInception CSE Block√ó4Inception BSE BlockInception ASE Block√ó3ReLUConv 3√ó3Entry flowECA Block√ó3Middle flowECA Block√ó8Entry flowECA BlockMax PoolingSeparable ConvReLUSeparable ConvReLU‚Ä¶NormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalReLUTable 1: The differences between three different attention mechanism channels and their corresponding CNN models. Thebottom row shows classification layers and other rows are feature extraction layers.VGG-16ConvConvMaxpoolConvConvMaxpoolConvConvConvMaxpoolConvConvConvMaxpoolConvConvConvSICConvSimAM BlockConvSimAM BlockMaxpoolConvSimAM BlockConvSimAM BlockMaxpoolConvSimAM BlockConvSimAM BlockConvSimAM BlockMaxpoolConvSimAM BlockConvSimAM BlockConvSimAM BlockMaxpoolConvSimAM BlockConvSimAM BlockConvSimAM BlockInception-V3ConvConvMGICConvConvMaxpoolMaxpoolConvConvConvConvXceptionConvReLUConvReLUEntry flowMaxpoolMaxpoolEntry flowInception AInception AInception AInception BInception CInception CInception CInception CInception DInception EInception EIncecption ASE BlockInception ASE BlockInception ASE BlockInception BSE BlockInception CSE BlockInception CSE BlockInception CSE BlockInception CSE BlockInception DSE BlockInception ESE BlockInception ESE BlockEntry flowMiddle flowMiddle flowMiddle flowMiddle flowMiddle flowMiddle flowMiddle flowMiddle flowEntry flowSeparable ConvMSICConvReLUConvReLUEntry flowECA BlockEntry flowECA BlockEntry flowECA BlockMiddle flowECA BlockMiddle flowECA BlockMiddle flowECA BlockMiddle flowECA BlockMiddle flowECA BlockMiddle flowECA BlockMiddle flowECA BlockMiddle flowECA BlockEntry flowECA BlockSeparable ConvReLUReLUSeparable ConvSeparable ConvReLUReLUMaxpoolFC LayerSoftmaxMaxpoolFC LayerSoftmaxMaxpoolFC LayerSoftmaxMaxpoolFC LayerSoftmaxMaxpoolFC LayerSoftmaxMaxpoolFC LayerSoftmaxCNN models, Inception-V3 has an exceptional abil-ity to extract global multiscale information owingto its parallel convolution structure and partiallylarge convolution kernels.The Xception [64] model improves the Inception-V3 [62] model by combining the depth-separableconvolution [68] and residual mechanism [43]. Un-like the standard convolution approach, depth-separable convolution is performed separately foreach channel in the feature map [68]. The advan-tage of Xception is its combination of the resid-ual structure and depth-separable convolution. Thedepth-separable convolution effectively extracts themulti-scale features of the image, and the resid-ual mechanism makes the network model convergeeasily. In contrast to the Inception-V3 model, thesmall convolutional kernel in depth-separable con-volution gives the Xception model a good localmulti-scale information extraction ability.3.2. Transfer Learning (TL)Training CNN models from scratch requires alarge amount of data and high computationalpower, resulting in a longer training time. More-over, the small size and vague labels of medicaldatasets make TL critical for CHIC tasks [69]. TL isa machine learning approach wherein a pre-trainedmodel is reused in another task [70]. The TL pro-cess has two steps: the first is selecting an originaldataset and pre-training on the original dataset.The second is to fine-tune the pre-trained modelusing the dataset of the target task.In this paper, we use the ImageNet dataset as theoriginal dataset to pre-train the model. Because low6workstation computing power makes it challengingto pre-train MCAM models directly using the TLapproach, we make changes based on traditionaltransfer learning. The pre-training parameters ofthe traditional VGG-16, Inception-V3, and Xcep-tion models in the Pytorchvision package are loadedlayer by layer according to the same parts of SIC,MGIC and MSIC as in Table 1, which are frozenin training. Only AM layers and fully connectedlayers are used for fine-tuning, and the weights ofeach channel are assigned using weighted voting.3.3. Multi-Channel Attention MechanismsAMs are inspired by human biological systems.They tend to focus on specific parts when process-ing large sets of information [9]. The AM approachhas become one of the most imperative conceptsin the field of deep learning [71]. However, tradi-tional AMs have several disadvantages. For exam-ple, a single AM can identify redundant informa-tion, leading to mistakes. To overcome some reducethe disadvantages of AMs, we propose an MCAMmodel. This model extracts features from multipleperspectives through three channels: SIC, MGICand MSIC. These three complementary channelsimprove the precision of identifying attention re-gions and the accuracy of classification tasks.SIC: SIC is expectation because its capabilityof extracting spatial information is excellent. TheSimAM attention mechanism has an outstandingability to distribute weights to the features of spa-tial dimensions [61]. The structure of the SimAMattention mechanism is illustrated in Fig. 3-(a).In visual neuroscience, the most informative neu-rones show different firing patterns in the surround-ing neurons and keep the activity of the surround-ing neurons, a phenomenon known as spatial sup-pression [72]. The easiest way to find these spa-tially suppressed neurones is to measure the lin-ear separability between the target and other neu-rones. In computer vision tasks, the edge featuresof images often play a crucial role in classificationtasks. Furthermore, the edge features of images arethe same as those of spatial suppression neurones,which often exhibit incredibly high contrast withthe surrounding colours and textures. Therefore,the SimAM attention mechanism works by using anenergy function (EF) from neuroscience to assignweights to different spatial locations. The energyfunction perceives each pixel of the feature map asa neuron and the minimal energy of neurones canbe expressed as following:e‚àót =4(œÉ2 + Œª)(t ‚àí ¬µ)2 + 2œÉ2 + 2Œª(1)(cid:80)M(cid:80)Mi=1 xi and œÉ2 = 1Mwhere t is target neuron, i is index over spatial di-mension, ¬µ = 1i=1(xi ‚àíM¬µ)2 are mean and variance calculated over all neu-rons except t in that channel, xi is other neurons ina same channel, M = H √ó W is number of neuronson the channel and Œª is a coefficient and is set to1e ‚àí 4 according to the experiments on the CIFARdatasets [61]. Spatially suppression neurons are lesssimilar to other neurons and exhibit a high linearseparability, thus showing a significant deviation int and u leading to a low e‚àót . Meanwhile, in neuro-science, it is considered that lower energy indicatesneurons that are more differentiated from surround-ing neurons. Therefore, the weights of each neuroncan be calculated from e‚àót . The optimization phaseof the whole SimAM attention mechanism is ob-tained by scaling operator:(cid:101)X = sigmoid(1E) ¬∑ X(2)where X and (cid:101)X are input feature map and out-put feature map, E is all e‚àót are grouped in spatialand channel dimensions. Finally, the confidence ofeach neuron at each place is obtained by sigmoidactivation function.In Section 3.1, it is shown that the VGG-16 modelis capable of extracting spatial information [42].Above all, the SimAM attention mechanism afterevery convolutional layer in VGG-16 model is de-signed to extract spatial information in the SIC.MGIC: In the MGIC, the model is expected tobe capable of extracting multi-scale global infor-mation.In Section 3.1, it explained that ‚Äî theInception-V3 model is the best CNN model thatcan extract globalinformation [62]. Therefore,the Inception-V3 model is selected to extract fea-tures in MGIC. The extraction of multi-scale infor-mation in the Inception-V3 model is implementedby concatenating different sized receptive fields, sothe multi-scale ability of the Inception-V3 modelis represented in the channel domain of each fea-ture map. The SE attention mechanism, whichpossesses a good distribution of channel weights,is selected to strengthen the importance betweenthe channel features in the MGIC [63]. The struc-ture of the SE attention mechanism is shown inFig. 3-(b). The SE attention mechanism con-sists of two phases: squeeze phase and excitation7phase. The SE attention mechanism consists oftwo phases: squeeze and excitation. The squeezephase encodes the entire spatial features into aglobal feature by global average pooling to generatechannel-wise statistics. The excitation phase ob-tains the channel-wise importance using two fullyconnected layers, a dimensionality-reduction layerand a dimensionality-increasing layer, and the finalchannel-wise weights are obtained by the sigmoidactivation function.MSIC: This channel is implemented using depth-separable convolution of the Xception [64] model.Depth-separable convolution causes information ex-tracted from each channel of the feature map to bediversified so that multi-scale spatial informationcan be extracted appropriately. The ECA atten-tion mechanism is used after each flow of the Xcep-tion model to strengthen its ability to extract multi-scale information. The ECA attention mechanismuses a low time consumption to assign weights tothe importance of the channel information of eachfeature map [65]. The structure of the ECA at-tention mechanism is illustrated in Fig. 3-(c). TheECA attention mechanism first uses global aver-age pooling (GAP) to obtain channel-wise infor-mation, then uses 1D convolutional that capturescross-channel interaction information with a convo-lutional kernel of size k, and finally obtains channel-wide weight information using a sigmoid activationfunction.Multi-channel fusion approach: This approach isan integrated classifier that relies on the classifica-tion decision values of different channels and theweights of each channel to improve the classifica-tion performance [36]. In this experiment, the lastfeature maps of SIC, MGIC and MSIC are usedto obtain the classification decision values for eachchannel using pooling, fully connected and softmaxlayers. Then, the classification decision values ofeach channel are weighted and evaluated using gridweighted voting to obtain the classification decisionvalues of the MCAM model. Finally, the categorythat belongs to the maximum classification deci-sion values of the MCAM model is used as the finalclassification result. The calculation formula is de-scribed as follows:C = max1<x<n3(cid:88)i=1œâiDi(3)where C is classification category, n is the numberof categories, i is the number of channels of theMCAM model and Di = {d1, d2, ..., dn} is n clas-sification decision values of ith channel of MCAMmodel.Figure 3: The structure of three different attentionmechanism. (a) is SimAM Blocks after each convolutionlayer in SIC. (b) is SE Blocks after each Inception block inMGIC. (c) is ECA blocks after each flow in MSIC.3.4. Interactive Learning (IL)The substance of the proposed IL-MACM frame-work is limited frequency incrementallearning.Incremental learning means that a learning sys-tem can continuously learn from new samples andcan preserve most of previously acquired knowl-edge [73]. The implementation ofincrementallearning is achieved through an IL strategy. TheIL-MACM framework process is shown in Fig. 4.First, the misclassified images in the validation setare sent to the pathologist after one training itera-tion. The attention regions are then discreetly andmeticulously labelled by pathologists. Finally, thelabelled images are input into the training set forthe next training iteration until no new errors ap-pear in the validation set.8Feature MapCHWGAP√ók=3œÉ(c) ECA BlocksFeature MapCHWGAP√ó(b) SE Blocks......Feature MapCHWEF√ó(a) SimAM BlocksCHWCHWCHWFigure 4: The process of IL-MCAM framework.4. Experiment Results and Analysis4.1. Experimental Settings4.1.1. DatasetIn this study, an haematoxylin and eosin (H&E)stained colorectal cancer (CRC) histopathologydataset (HE-CRC-DS) is used in the experiment toevaluate the classification performance of the pro-posed IL-MCAM approach. This dataset is col-lected and labelled by two pathologists from theCancer Hospital of China Medical University andfour biomedical researchers from Northeastern Uni-versity. Pathologists provide electron microscopyimages of histopathological sections of CRC en-teroscopic biopsies using an ‚ÄúOlympus‚Äù microscopeand the ‚ÄúNewUsbCamera‚Äù software and also pro-vide image-level annotations of weakly supervisedleaning processes. Biomedical researchers organiseand create datasets. Details of the acquisition ofHE-CRC-DS are shown in Fig. 5, and the image-level labels are given as follows: First, when thepathologist finds only the differentiation stage in a40√ó image, it is magnified to 200√ó for preserva-tion, and this differentiation stage is then used asthe image-level label. Then, if the physician findsmultiple differentiation stages or similar differentia-tion stages in a 40√ó image, the most severe stage ismagnified to 200√ó and saved, and the most severestage is used as the image level label. In summary,the image-level label is the same as the patient-levellabel in HE-CRC-DS.HE-CRC-DS includes 4005 images of 2048√ó1536pixels in the ‚Äú.png‚Äù file format. The overall magni-fication of all images in the HE-CRC-DS is 200√ó.Most pathologists classify CRC into five categories:normal, polyp, low grade, high grade, and cancer.Due to the unbalanced data distribution in the ini-tial dataset, this experiment classified the normalcategory including normal, polyp and low gradewith 2031 images and the abnormal category in-cluding high grade and cancer with 1974 images.Figure 5: Details of the acquisition of HE-CRC-DS. (a) is a40√ó image obtained by enteroscopy biopsy. (b) is a 200√óimage containing in HE-CRC-DS. Pathologists first assessthe most severely differentiation stage in the 40√ó images.Then the dataset images are obtained by adjusting themagnification to 200√ó and giving image-level labelsaccording to the most severe differentiation stage.Examples of the HE-CRC-DS are shown in Fig. 6.The normal category is shown in Fig. 6-(a). All ofthem have intact oval glands with neatly arrangednuclei. The abnormal category is shown in Fig. 6-(b). The boundaries of the glandular structures arenot clear and the nuclei are drastically enlarged.4.1.2. Data SettingsAll the images in the HE-CRC-DS, including thenormal and abnormal categories, are randomly par-titioned into training data and test data at a ra-tio of 1:1.In the training data, the training andvalidation sets are randomly assigned three timesat a ratio of 1:1 and used to perform three ran-domised experiments. All of these are resized to 224√ó 224 pixels using bilinear interpolation. Becausethe small size of the medical image dataset leads toa large amount of error information in training, thetraining set is enlarged to six times by rotating it90‚ó¶, 180‚ó¶, and 270‚ó¶ and horizontal and vertical mir-roring. The biomedical researcher uses the ‚ÄúMAT-LAB R2020a‚Äù software to perform resizing, rota-tion, and mirroring operations in the pre-processingstage. The pathologist uses the ‚ÄúPhotoshop‚Äù soft-ware to label the attention areas in the IL stage.The normal and abnormal categories of the epithe-lial tissues are labelled to minimise the impact ofmesenchymal misclassification. The data settingsare listed in Table 2.4.1.3. Hyper-parameter SettingThe IL-MCAM framework consists of two stages.In the AL stage, MCAM model uses 100 epochsand 16 batch sizes trained by the HE-CRC-DS. Inthe AL and IL stages, the model parameters pre-served in each iteration are those with the highest9Output misclassification imagesLabeled normal/abnormal regionsAL StageWorkstations IL StagePathologists(a) 40√óenteroscopebiopsy image(b) 200√óenteroscopebiopsy imageCancerFigure 6: Some examples in the HE-CRC-DS.Table 2: Data setting of HE-CRC-DS for training,validation and test sets.Image Type Training Validation Test1015Normal987Abnormal2002Sum3048296460125084931001validation set accuracy in this iteration. It uses amodified transfer learning approach in Section 3.2for the CHIC task. In the IL stage, one iteration isset to 50 epochs, and only the last fully connectedlayer is trained using a fine-tuning approach. TheAdamW optimiser [74] is used for optimization, andits parameters are set to 2e ‚àí 3 learning rate, 1e ‚àí 8eps, [0.9, 0.999] betas and 1e ‚àí 2 weight decay.4.1.4. Evaluation CriteriaTo overcome the bias between different algo-it is crucial to choose the appropriaterithms,evaluation criteria.Specificity (Spec.), sensitiv-ity (Sens.), F1-score (F1) and average accuracy(Avg.Acc.) are the most standard metrics for eval-uating classification performance. True positive(TP), true negative (TN), false positive (FP) andfalse negative (FN) are used to define these crite-ria in Table 3. Spec. represents to the ratio of allnegative samples predicted to be correct to all ac-tual negative samples. Sens. represents the ratioof correctly classified positive samples to all actualpositive samples. F1 is a comprehensive considera-tion of precision and recall, and it is a critical eval-uation criterion for evaluating a model. Avg.Acc isthe most typical and fundamental evaluation crite-rion.Table 3: Criteria and corresponding definitions for imageclassification evaluation.Criteria DefinitionCriteriaDefinitionSpec.F1TNTN+FP2√óTP+FP+FN Avg.Acc.Sens.2√óTPTPTP+FNTP+TNTP+TN+FP+FP4.2. Classification Evaluation4.2.1. Experimental ResultsTo analyse the experimental results, we show theconfusion matrix obtained from three randomisedexperiments of the proposed MCAM model and IL-MCAM framework in Fig. 7. Three randomised ex-periments and the average evaluation results of theproposed MCAM model and IL-MCAM frameworkare shown in Table 4.In the 1st experiment, the MCAM model is usedin the validation and test sets for classification, andthe confusion matrix is shown in Figs. 7-(a) and (g).For the validation set, five abnormal category im-ages are misclassified as normal, and five normalcategory images are misclassified as abnormal. Forthe test set, 12 abnormal category images are mis-classified as normal, and 23 normal category images10(a) Normal(b) AbnormalNormalPolypLow GradeHigh GradeCancerare misclassified as abnormal. Additionally, the IL-MCAM framework is also used in the validation andtest sets for classification, and the confusion matrixis shown in Figs. 7-(d) and (j). For the validationset, one abnormal category image is misclassifiedas normal, and three normal category images aremisclassified as abnormal. For the test set, eightabnormal category images are misclassified as nor-mal, and 14 normal category images are misclassi-fied as abnormal. In summary, compared with theMCAM model, the IL-MCAM framework identifiestwo more correct abnormal images and four morenormal images in the validation set and identifiesnine more correct abnormal images and four morenormal images in the test set.In the 2nd experiment, the MCAM model is usedin the validation and test sets for classification, andthe confusion matrix is shown in Figs. 7-(b) and (h).For the validation set, nine abnormal category im-ages are misclassified as normal, and four normalcategory images are misclassified as abnormal. Forthe test set, 16 abnormal category images are mis-classified as normal, and 14 normal category imagesare misclassified as abnormal. Additionally, the IL-MCAM framework is also used in the validation andtest sets for classification. The confusion matrix isshown in Figs. 7-(e) and (k), and the results are thesame as those for the MCAM model. These resultssuggest that the addition of the IL stage did notoccur in this randomised experiment.In the 3rd experiment, the MCAM model is usedin the validation and test sets for classification, andthe confusion matrix is shown in Figs. 7-(c) and(i). For the validation set, five abnormal categoryimages are misclassified as normal, and six normalcategory images are misclassified as abnormal. Forthe test set, 18 abnormal category images are mis-classified as normal, and 14 normal category imagesare misclassified as abnormal. Additionally, the IL-MCAM framework is also used in the validationand test sets for classification, and the confusionmatrix is shown in Figs. 7-(d) and (j). For the val-idation set, no abnormal category images are mis-classified as normal, and six normal category im-ages are misclassified as abnormal. For the test set,five abnormal category images are misclassified asnormal, and 12 normal category images are misclas-sified as abnormal. In summary, compared with theMCAM model, the IL-MCAM framework identifiesfive more correct abnormal images and no more nor-mal images in the validation set and 13 more cor-rect abnormal images and two more normal images11in the test set.The following results are obtained from Table 4.First, it can be observed that the accuracy of classi-fication results of the first and third randomised ex-periments using the IL-MCAM framework is higherthan that of accuracy using MCAM model, ex-cept for the second randomized experiment. In thethree randomised experiments, the MCAM modelachieves 99.02%, 98.72% and 98.85% on averagein the abnormal category of the validation set and98.32%, 98.45% and 98.37% on average in the ab-normal category of the test set for Spec., Sens. andF1, respectively. Based on the MCAM model, us-ing the IL-MCAM framework improve the Spec.,Sens., and F1 by 0.13%, 0.60% and 0.37% on av-erage for the abnormal category of the validationset and 0.59%, 0.35% and 0.47% for the abnor-mal category of the test set, respectively. TheAvg.Acc. of the three randomised experiments im-proves by 0.36% and 0.47% in the validation andtest sets, respectively. It is observed that using IL-MCAM framework can improve the classification ef-fect of the MCAM model. Furthermore, in the threerandomised experiments, regardless of whether theMCAM model or the IL-MCAM framework is usedfor classification, the deviations between the accu-racies of the validation and test sets are not morethan 1.00%. This indicates that the IL-MCAMframework has good extensibility and robustness.Finally, the standard deviations of the three ran-domized experiments obtained using the MCAMmodel are 0.12% and 0.10% for the validation andtest sets, respectively. The standard deviations ofthe three randomised experiments using the IL-MCAM framework are 0.39% and 0.27% for thevalidation and test sets, respectively. The slightfluctuation of the standard deviation indicates thatthe MCAM model and IL-MCAM framework havegood stability.4.2.2. Contrast Experiment of CHICThere are three contrast experiments as follows:The first compares the proposed IL-MCAM frame-work with other traditional deep learning mod-els, the second compares the proposed IL-MCAMframework with models that do not use TL, and thethird compares the proposed IL-MCAM frameworkwith models that do not use AM.Comparison with other deep learning mod-els: To validate the excellent performance ofthe MCAM model and IL-MCAM framework inthe CHIC task, we compare 18 different basicFigure 7: Confusion matrix for three randomised experiments in the CHIC task. 1st to 4th rows are used to represent theresults of using MCAM model in validation set, IL-MCAM framework in validation set, MCAM model in test set andIL-MCAM framework in test set, respectively. Each column represents each randomized experiment.12Predicted ClassAbnormalNormalAbnormalNormalActual Class48848.75%50.46%98.99%1.01%50.46%50350.25%99.02%0.98%98.99%1.01%99.02%0.98%99.00%1.00%1stexperimentPredicted ClassAbnormalNormalAbnormalNormalActual Class48448.35%40.40%99.18%0.82%90.90%50450.35%98.24%1.76%98.17%1.83%99.21%1.01%98.70%1.30%2ndexperimentPredicted ClassAbnormalNormalAbnormalNormalActual Class48848.75%60.60%98.79%1.21%50.46%50250.15%99.01%0.91%98.99%1.01%98.82%1.18%98.90%1.10%3rdexperimentAbnormalNormalAbnormalNormal49249.15%30.30%99.39%0.61%10.10%50550.45%99.80%0.20%99.80%0.20%99.41%0.59%99.60%0.40%AbnormalNormalAbnormalNormal48448.35%40.40%99.18%0.82%90.90%50450.35%98.24%1.76%98.17%1.83%99.21%1.01%98.70%1.30%AbnormalNormalAbnormalNormal49549.45%60.60%98.80%0.20%00.00%50250.15%100.00%0.00%100.00%0.00%98.82%0.18%99.40%0.60%AbnormalNormalAbnormalNormal97548.70%231.15%97.70%2.30%120.60%99249.55%98.80%1.20%98.78%1.22%97.73%2.27%98.25%1.75%AbnormalNormalAbnormalNormal97148.50%140.70%98.58%1.42%160.80%100150.00%98.38%1.62%98.38%1.62%99.62%0.38%98.50%1.50%AbnormalNormalAbnormalNormal96948.40%140.70%98.58%1.42%180.90%100150.00%98.23%1.77%98.18%1.82%98.62%1.38%98.40%1.60%AbnormalNormalAbnormalNormal97948.90%140.70%98.59%1.41%80.25%100150.00%99.21%0.79%99.19%0.81%98.62%1.38%98.90%1.10%AbnormalNormalAbnormalNormal97148.50%140.70%98.58%1.42%160.80%100150.00%98.38%1.62%98.38%1.62%99.62%0.38%98.50%1.50%AbnormalNormalAbnormalNormal98249.05%120.60%98.79%1.21%50.25%100350.10%99.50%0.50%99.49%0.51%98.82%1.18%99.15%0.85%MCAM model in Validation data (b)(c)OverallOverallOverallOverallOverallOverall(a)Predicted ClassActual ClassPredicted ClassActual ClassPredicted ClassActual ClassIL-MCAM framework in Validation data (e)(f)OverallOverallOverallOverallOverall(d)Predicted ClassActual ClassPredicted ClassActual ClassPredicted ClassActual ClassMCAM model in Test data (h)(i)OverallOverallOverallOverallOverallOverall(g)OverallPredicted ClassActual ClassPredicted ClassActual ClassPredicted ClassActual ClassIL-MCAM framework in Test data(k)(l)OverallOverallOverallOverallOverall(j)OverallTable 4: Performance analysis of the proposed MCAM model and IL-MCAM framework on validation and test sets amongthree randomised experiments. ([In %].)Test Name1stExperimentModel/FrameworkMCAMIL-MCAM2ndExperimentMCAMIL-MCAM3rdExperimentAverageMCAMIL-MCAMMCAMIL-MCAMCategoryAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalValidation SetSens.Spec.98.9999.0299.0298.9999.8099.4199.4199.8098.1799.2199.2198.1798.1799.2199.2198.1798.9998.8298.8298.99100.0098.8298.82100.0098.7299.0299.0298.7299.3299.1599.1599.32F198.9999.0299.6099.6098.6798.7398.6798.7398.8998.9299.3999.4198.8598.8999.2299.25Avg.Acc99.0099.6098.7098.7098.9099.4098.87 ¬± 0.1299.23 ¬± 0.39Test SetSpec.97.7398.7898.6299.1998.6298.3898.6298.3898.6298.1899.4998.8298.3298.4598.9198.80Sens.98.7897.7399.1998.6298.3898.6298.3898.6298.1898.6298.8299.4998.4598.3298.8098.91F198.2498.2798.8998.9198.4898.5298.4898.5298.3898.4399.1499.1698.3798.4198.8498.86Avg.Acc98.2598.9098.5098.5098.4099.1598.38 ¬± 0.1098.85 ¬± 0.27deep learning models, including CNN models, vi-sion transformer (VT) models and MLP models,which are AlexNet [41], VGG-16 [42], Inception-V3 [45], ResNet-50 [43], Xception [46], ResNeXt-50 [48],InceptionResNet-V1 [47], DenseNet-121 [44], ViT [51], DeiT [52], BoTNet-50 [55],CoaT [56], CaiT [53], T2T-ViT [54], LeViT [57],MLP-Mixer [58], gMLP [59] and ResMLP [60].The results of the contrast experiment betweenthe IL-MCAM framework and other deep learningmodels are shown in Table 5. the evaluation crite-ria are obtained by averaging the results of threethe randomised experiments. The following resultsare obtained and are listed in Table 5 by analysingthe performance of traditional deep learning mod-els on the test set.First, Inception-V3, VGG-16 andInception-V3 have the best Spec., Sens., and F1,respectively,in the abnormal category with val-ues of 98.13%, 98.65% and 98.23%, respectively.Meanwhile, VGG-16, Inception-V3, and Inception-V3 have the best Spec., Sens. and F1, respectively,in the normal with values of 98.65%, 98.13% and98.23%, respectively. Finally, Inception-V3 has thebest Avg.Acc. of 98.25%.Compared with traditional deep learning models,the proposed MCAM model and IL-MCAM frame-work can improve the performance. Although Sens.obtained by the MCAM model in the abnormal cat-egory is 0.20% lower than the optimal Sens. ofthe traditional deep learning models, it is higherthan that of all other deep learning models exceptthe VGG-16. Meanwhile, Spec. and F1 obtainedby the MCAM model in the abnormal categoryare 0.19% and 0.12%, respectively, which are bet-ter than those of traditional deep learning models.Similarly, the MCAM model improves Spec. andF1 compared to traditional deep learning modelsin the normal category, except for a slight decreasein Sens. Most importantly, the Avg.Acc. of theMCAM model is 0.13% higher than the optimalresult of traditional deep models, indicating thatthe MCAM model improves the classification per-formance compared with traditional deep learningmodels.Compared with the optimal results obtained bytraditional deep learning models, Spec., Sens. andF1 obtained by the IL-MCAM framework in theabnormal category improved by 0.78%, 0.15%, and0.61%, respectively. In addition, Spec., Sens. andF1 obtained by the IL-MCAM framework in thenormal category improved by 0.15%, 0.78%, and0.60%, respectively, compared with traditional deeplearning models. Finally, the Avg.Acc. of the IL-MCAM framework is 0.60% higher than that of tra-ditional deep learning models. The above compari-son results indicate that the IL-MCAM frameworkperforms better than traditional deep learning mod-els in the CHIC task.The contrast experiment result between the IL-MCAM framework and traditional deep learningmethod shows that the proposed MCAM model hasimproved significant improvement compared to the13traditional deep learning model in the CHIC task.Furthermore, interactive learning of the IL-MCAMframework can further improve the classificationperformance of the MCAM model.Comparison with IL-MCAM Frameworkwithout TL: To validate the effectiveness of TLin the experiment, we conduct a comparison ex-periment between a model using TL and a modelwithout TL during the re-training process, and theexperimental results from three randomised experi-ments are shown in Fig. reffig:compare-freeze. Themodel without TL has 98.00%, 98.48%, and 98.21%of Spec., Sens. and F1, respectively, in the ab-normal category. The Spec., Sens.and F1 ofthe model with TL in the abnormal category are98.91%, 98.80% and 98.84%, respectively, which isan improvement of 0.91%, 0.32% and 0.63%, re-spectively, compared to the model without TL. TheSpec., Sens. and F1 of the model without TL are98.48%, 98.00%, and 98.25%, respectively, in thenormal category. The Spec., Sens. and F1 of themodel with TL in the normal category are 98.80%,98.91% and 98.86%, respectively, which are 0.32%,0.91% and 0.59% higher than those of the modelwithout TL. The Avg.Acc. for the model withoutTL is 98.23%, whereas the Avg.Acc. of the modelusing TL is 98.85%, which is 0.62% higher than themodel without TL. In summary, the comparison ex-periment illustrates that the proposed IL-MCAMframework using TL is better than the model with-out TL.Figure 8: Performance analysis about whether to freeze thenetwork layer in AL stage on test set. (a) is theperformance in abnormal category. (b) is the performancein normal category. ([In %].)and F1, respectively,randomised experiments, as shown in Fig. 9. Theensemble model has 98.29%, 99.02% and 98.64%of Spec., Sens. and F1 in the abnormal category.The IL-MCAM framework has 98.91%, 98.80% and98.84% of Spec., Sens.inis lowerthe abnormal category. Although Sens.the IL-MCAM framework than the ensemble model,the most critical evaluation criterion, F1, is 0.20%higher. Similar results are obtained for the nor-In addition, the Avg.Acc. of IL-mal category.MCAM framework is 98.85%, which is 0.20% higherthan that of ensemble model. In summary, the IL-MCAM framework with added AM modules is moreeffective than the ensemble model composed of tra-ditional deep learning models.Figure 9: Performance analysis about whether to use AMin MCAM model on test set. (a) is the performance inabnormal category. (b) is the performance in normalcategory. ([In %].)4.3. Extended ExperimentsIn this section, we describe the three conductedexperiments. In Section 4.3.1, we describe the ab-lation experiments to verify the roles of the SIC,MGIC and MSIC modules in the IL-MCAM frame-In Section 4.3.2, we describe the experi-work.ment that used deep learning models combined withother AMs to implement the function of SIC, MGICand MSIC to verify the interchangeability of theIn Section 4.3.3, we de-IL-MACM framework.scribe the experiment that used the NCT-CRC-HE-100K dataset for multi-classification experiments toverify good generalisation ability of the IL-MACMframework.Comparison with ensemble model withoutAM: To validate the effectiveness of the AM mod-ule in the experiment, we replaced SIC, MGICand MSIC channels with traditional the VGG-16,Inception-V3 and Xception to obtain an ensemblemodel. The results of the ensemble model and IL-MCAM framework obtains from averages of three4.3.1. Ablation ExperimentTo verify the role of the three channels in theIL-MCAM framework, we conduct ablation exper-iments according to the experimental setting de-scribed in Section 4.1.3. We list the results of thethree randomised experiments in Table 6, and theimportance of each channel as follows.1498.9198.8098.8498.8598.0098.4898.2198.23SpecSensF1Avg.Acc.Unfreeze LayerFreeze Layer (Ours)98.4898.0098.2598.2398.8098.9198.8698.85SpecSensF1Avg.Acc.Unfreeze LayerFreeze Layer (Ours)(a) Result in abnormal category(b) Result in normal category98.2999.0298.6498.6598.9198.8098.8498.85SpecSensF1Avg.Acc.Without AMAM (Ours)98.8098.9198.8698.8599.0298.2998.6698.65SpecSensF1Avg.Acc.Without AMFreeze Layer (Ours)(a) Result in abnormal category(b) Result in normal categoryTable 5: Performance analysis of the proposed MCAM model and IL-MCAM approach along with the traditional models ontest set. ([In %].)Type Model/FrameworkAlexNet [41]VGG-16 [42]Inception-V3 [45]CNNResNet-50 [43]Xception [46]ResNeXt-50 [48]InceptionResNet-V1 [47]DenseNet-121 [44]ViT [51]DeiT [52]BoTNet-50 [55]VTCaiT [53]CoaT [56]T2T-ViT [54]LeViT [57]MLP-Mixer [58]MLPgMLP [59]ResMLP [60]OursMCAMIL-MCAMSpec.93.5096.3596.7598.6598.1398.3895.7693.2197.9098.4593.4093.8295.5796.4596.1697.6777.9074.9094.7792.3094.4895.5475.6672.4473.8987.9490.1592.0079.3182.0673.1072.1488.0788.9672.1277.4198.3298.4598.9198.53Sens.96.3593.5098.6596.7598.3898.1393.2195.7698.4597.9093.8293.4096.4595.5797.6796.1674.9077.9092.3094.7795.5494.4872.4475.6687.9473.8992.0090.1582.0679.3172.1473.1088.9688.0777.4172.1298.4598.3298.8098.91F194.9094.9097.6897.6998.2398.2694.3694.6598.1598.1993.5493.6895.9696.0496.9096.9075.8077.0293.3993.7294.9695.0473.3774.7481.8979.6291.0391.0980.7180.6272.2073.0088.4288.6075.1274.3298.3798.4198.8498.73Avg.Acc.94.9097.6898.2594.5198.1793.6096.0096.9076.4293.5695.0174.0880.8291.0580.6672.6388.5174.7398.3898.85CategoryAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormal15Table 6: Results of the ablation experiments on the three channels obtained on test set. ((cid:88)indicates that this channel is used.[In %].)ChannelSIC MGICMSIC(cid:88)(cid:88)(cid:88) (cid:88)(cid:88)(cid:88)(cid:88) (cid:88)(cid:88)(cid:88)(cid:88)(cid:88)CategoryAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormal1st ExperimentSepc.97.9398.3898.7299.0997.3498.4899.2199.8098.2398.6899.4199.8099.4199.80Sens.98.3897.9399.0998.7298.4897.3499.8099.2198.6898.2399.8099.4199.8099.41F198.1398.1798.8998.9198.0898.1299.5099.5098.4398.4699.6099.6099.6099.602nd ExperimentSepc.97.1497.6798.2298.6898.6297.7798.4898.2398.6298.2898.6298.2898.6298.38Sens.97.6797.1498.6898.2297.7798.6298.2398.4898.2898.6298.2898.6298.3898.62F197.3797.4398.4398.4798.1798.2398.3398.3798.4398.4898.4398.4898.4898.523rd ExperimentSepc.98.7298.4899.0998.7298.8998.8298.8299.0999.1998.8299.0998.8299.4998.82Sens.98.4898.7298.7299.0998.8298.8999.0998.8298.8299.1998.8299.0998.8299.49F198.5898.6298.8998.9198.8498.8798.9498.9698.9999.0198.9398.9699.1499.16Avg.acc.98.05 ¬± 0.4998.75 ¬± 0.2198.39 ¬± 0.3398.77 ¬± 0.3098.63 ¬± 0.2598.77 ¬± 0.2298.85 ¬± 0.26First, through the ablation experiment in the sec-ond row, it can be observed that the Avg.Acc. us-ing only MSIC is only 0.10% lower than that of IL-MCAM, and even in the second randomised experi-ment on the abnormal category Sens. is higher thanthe results obtained from the IL-MCAM frame-work. Meanwhile,it can be observed that theAvg.Acc. decreases more by 0.22% when the MSICis removed from the ablation experiment in the fifthrow.Importantly, these results indicate that theMSIC plays an irreplaceable role in the entire IL-MCAM framework.Second, through the third row of ablation exper-iments, the Avg.Acc. using only MGIC is 0.46%lower than that of the IL-MCAM framework, andfor the abnormal class in the third ran-Sens.domised experiment is equal to the results obtainedfrom the IL-MCAM framework. Moreover, throughthe fourth row of the ablation experiment, the sameis obtained with the MGIC removed asAvg.Acc.is obtained with the MSIC removed. Importantly,these results indicate that the MGIC plays a crucialrole in the entire IL-MCAM framework.Finally, it can be observed in the ablation ex-periment in the first row that the Avg.Acc. usingonly SIC is 0.80% lower than that of the IL-MCAMframework. Through the sixth row of the ablationexperiment, there is a slight decrease of 0.08% inthe Avg.Acc. after the SIC is removed. There is aslight decrease in the second and third randomisedexperiments and no decrease in the first randomisedexperiment, which indicates that the SIC plays rolein the overall IL-MCAM framework, but the effectis limited.4.3.2. Interchangeability ExperimentTo verify that the three modules in the IL-MCAM framework are interchangeable, we conductthe following an extended experiment using the ex-perimental setting described in Section 4.1.3.In SIC, CBAM [75] is used instead of SimAM [61]because CBAM [75] is similar to SimAM [61] in as-signing weights to the spatial information of theVGG-16 model. In MGIC, ECA [65] and SRM [76]are similar to SE [63] and used to assign weightsto the channel information to improve the abilityof Inception-V3 model to extract multi-scale globalinformation; therefore, ECA [65] and SRM [76] areIn MSIC, SE [63] andused instead of SE [63].SRM [76] are similar to ECA [65] and can assignweights to the channel information to improve theability of Xception model to extract multi-scaleglobal information; therefore, SE [63] and SRM [76]are used instead of ECA [65]. The results of the ex-tended experiments to verify the interchangeabilityarelisted in Table 7. The first to fourth rows are thereplaced AM models, and the fifth row is the pro-posed IL-MCAM framework. We can observe thatthe classification accuracies of the four replacedmodels are 98.38% at the highest and 98.08% atthe lowest level, which is a variation not more than0.90% from the IL-MCAM framework and is a tol-erable gap. Furthermore, in the second and thirdrandomised experiments, the Spec. and Sens. ofsome replaced models in the abnormal category are16Table 7: Performance analysis of interchangeable experiments using different AMs in each of the three channels. ([In %].)ChannelSICMGIC MSICCBAM ECASRMCBAM ECASECBAM SRMSRMCBAM SRMSESimAM SEECACategoryAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormalAbnormalNormal2nd ExperimentSepc. Sens. F13rd ExperimentSepc. Sens. F11st ExperimentSepc. Sens. F196.65 98.38 97.49 98.28 98.82 98.53 98.72 99.49 99.0998.38 96.65 97.52 98.82 98.28 98.58 99.49 98.72 99.1196.94 98.89 97.89 98.81 98.38 98.58 98.72 99.29 98.9998.89 96.94 97.91 98.38 98.81 98.62 99.29 98.72 97.5297.04 98.28 97.64 98.92 98.08 98.47 97.93 99.39 98.6498.28 97.04 97.67 98.08 98.92 98.53 99.39 97.93 98.6696.55 98.78 97.65 97.64 98.18 97.88 98.62 98.78 98.6898.78 96.55 97.66 98.18 97.64 97.93 98.78 98.62 98.7298.62 99.19 98.89 98.62 98.38 98.48 99.49 98.82 99.1499.19 98.62 98.91 98.38 98.62 98.52 98.82 99.49 99.16Avg.Acc.98.38 ¬± 0.6698.50 ¬± 0.4598.27 ¬± 0.4498.08 ¬± 0.4498.85 ¬± 0.26even higher than that of the IL-MCAM framework.In summary, the three channels of the IL-MCAMframework are interchangeable.4.3.3. NCT-CRC-HE-100K Image ClassificationTo verify that the IL-MCAM framework hasgood generalisation ability, we carry out experi-ments on the publicly available CRC dataset NCT-CRC-HE-100K, composed of 100,000 patch-levelimages of nine different tissue categories, all ofwhich are 224√ó224 pixels containing 0.5 micronsper pixel. All images are colour-normalized us-ing Macenko‚Äôs method [77, 78]. The nine differentcategories of tissue are adipose (ADI), background(BACK), debris (DEB), lymphocytes (LYM), mu-cus (MUC), smooth muscle (MUS), normal colonmucosa (NORM), cancer-associated stroma (STR)and colorectal adenocarcinoma epithelium (TUM),some of which are shown in Fig. 10. We divide thedata in the ratio of 6:2:2 for the training, valida-tion and test sets as shown in Table 8, and con-duct extended experiments using the experimentalparameters listed in Section 4.1.3.The confusion matrix obtained using the MCAMmodel and the IL-MCAM framework on the test setof the NCT-CRC-100K dataset is shown in Fig. 11.When using the MCAM model for classification,19933 images are correctly classified, 64 images areisincorrectly classified, and 99.68% of Avg.Acc.obtained. When using the IL-MCAM model forclassification, 19952 images are correctly classified,45 images are incorrectly classified, and 99.78% ofAvg.Acc. is obtained. Compared with the classifi-cation results obtained by the MCAM model, IL-MCAM framework improves the classification accu-racy in every category except for two fewer imagescorrectly classified in the DEB category.Figure 10: Some example of NCT-CRC-HE-100K.Finally, we compare our method with previousexperimental results obtained in NCT-CRC-HE-100K, and the comparison results are shown in Ta-ble 9. The best results in recent years are obtainedby the method proposed by Ghosh et al. using theTL and ResNet methods, which obtained 99.76%Avg.Acc. Our proposed IL-MCAM framework ob-tained 99.78% Avg.Acc. which is 0.02% higher thanthe previous best result. This result indicates thatthe proposed IL-MCAM framework exhibits goodclassification performance and generalisation abil-ity.17(a) ADI(b) BACK(c) DEB(d) LYM(e) MUC(f) MUS(g) NORM(h) STR(i) TUMFigure 11: Confusion matrix obtained using MCAM model and IL-MCAM framework on test set of NCT-CRC-100K dataset.Table 8: Data setting of HE-CRC-DS for training,validation and test sets.Table 9: Comparison of the average accuracy of theproposed method with other methods inNCT-CRC-HE-100K. ([In %].)Image Type Training Validation Test2081ADI2113BACK2302DEB2311LYM1779MUC2707MUS1752NORM2089STR2863TUM19997Sum62456340690869355338812252586268859160005208121132302231117792707175320892863199984.4. Experimental Environment and Computa-tional TimeIn our experiments, the proposed IL-MCAMframework have an AL stage and an IL stage. Inthe AL stage, it took 1.23h to train the MCAMIn IL stage, it took 5 min tomodel in parallel.label each misclassified image and 40 min to fine-tune the model. This experiment is carried out ona workstation. The running memory of the work-station is 32GB. It uses the Windows 10 Profes-sional operating system and is equipped with an8GB NVIDIA GeForce RTX 4000 GPU. Python3.6, Pytorch 1.7.0, and Torchvision 0.9.0 are con-figured on the workstation.18ReferencesKather et al. [79]Ghosh et al. [20]Hamida et al. [21] TL+ResNetProposedProposedMethodsAvg.Acc.98.70TL+VGG-16Ensemble CNN 96.1699.7699.6899.78MCAMIL-MCAM5. DiscussionThis year, the rapid development of deep learn-ing models has played a crucial role in the fieldof medical diagnosis. Classification of colorectalhistopathological images plays a crucial role in theearly prevention of diseases. In this paper, the pro-posed IL-MCAM framework is used for the classifi-cation of HE-CRC-DS and achieves good results.Compared with regular images, medical imagestend to be larger in size and the distribution of fo-cused attention regions of the same class in med-ical images is not uniform in shape. TraditionalCNN models using convolutional kernels tend tooverconcentrate computational power on extract-ing edge information; therefore, we consider us-ing a multi-channel approach combined with an at-tention mechanism to extract multi-scale informa-tion. VGG-16, Inception-V3 and Xception models208110.41%00.00%00.00%00.00%00.00%10.01%00.00%00.00%00.00%99.95%0.05%00.00%211210.56%00.00%00.00%00.00%00.00%00.00%00.00%00.00%100.00%0.00%00.00%00.00%229211.46%00.00%00.00%10.01%10.01%10.01%30.02%99.74%0.26%00.00%00.00%10.01%231111.56%00.00%00.00%00.00%00.00%00.00%99.96%0.04%00.00%10.01%20.02%00.00%17778.89%00.00%00.00%00.00%70.04%99.22%0.78%00.00%00.00%00.00%00.00%00.00%270413.52%40.02%20.02%00.00%99.93%0.07%00.00%00.00%00.00%00.00%00.00%00.00%17448.72%00.00%60.03%99.66%0.34%00.00%00.00%20.02%00.00%00.00%10.01%00.00%208510.43%10.01%99.81%0.19%00.00%00.00%50.03%00.00%20.02%00.00%30.02%10.01%284614.23%99.62%0.38%100.00%0.00%99.95%0.05%99.57%0.43%100.00%0.00%99.89%0.11%99.89%0.11%99.54%0.46%99.81%0.19%99.41%0.59%99.78%0.22%ADIPredicted ClassActual ClassOverallOverallADIBACKBACKDEBLYMMUCMUSNORMSTRTUMDEBLYMMUCMUSNORMSTRTUM(b) IL-MCAM Framework208010.40%00.00%00.00%00.00%00.00%10.01%00.00%00.00%00.00%99.95%0.05%00.00%211210.56%00.00%00.00%00.00%00.00%00.00%00.00%00.00%100.00%0.00%00.00%00.00%229411.47%00.00%10.01%10.01%00.00%10.01%30.02%99.74%0.26%00.00%00.00%10.01%231111.56%00.00%00.00%10.01%00.00%00.00%99.91%0.09%00.00%10.01%00.00%00.00%17718.86%00.00%90.05%00.00%70.04%99.04%0.96%00.00%00.00%00.00%00.00%00.00%270213.51%10.01%20.02%00.00%99.88%0.12%00.00%00.00%00.00%00.00%10.01%00.00%17358.68%00.00%70.04%99.54%0.46%10.01%00.00%40.02%00.00%10.01%30.02%00.00%208510.43%10.01%99.43%0.57%00.00%00.00%30.02%00.00%50.03%00.00%60.03%10.01%284314.22%99.48%0.52%99.95%0.05%99.95%0.05%99.65%0.35%100.00%0.00%99.55%0.45%99.82%0.18%99.03%0.97%99.81%0.19%99.30%0.70%99.68%0.32%ADIPredicted ClassActual ClassOverallOverallADIBACKBACKDEBLYMMUCMUSNORMSTRTUMDEBLYMMUCMUSNORMSTRTUM(a) MCAM Modelare generally considered to have a good ability toextract spatial information, multi-scale global in-formation and multi-scale local information, andthe combination of SimAM, SE and ECA attentionmechanisms further improves the recognition accu-racy. The IL-MCAM framework uses three chan-nels, SIC, MSIC and MGIC, to enhance the widthand ensure the complementarity of the extractedinformation. Meanwhile, three AMs are used toenhance the depth of the model to ensure the accu-racy of the extracted information in each channel.The IL-MCAM framework enhances the classifica-tion performance in terms of width and depth. Insummary, we select the models mentioned above toform the MCAM model.Table 10 shows the model parameters and train-ing time for comparing the proposed approacheswith other traditional deep learning models. First,we can observe that the proposed MCAM model hasvery good results and has a significant improvementin classification results compared to traditional au-tomatic methods using interactions.In addition,although the VT and MLP models are more effec-tive than CNN models for routine tasks and havebeen shown to have a good ability to extract globalinformation, these models do not work well in thisexperiment because of overfitting. The small med-ical training set leads to overfitting when trainedon a complex or large model, and the experimen-tal results validate this conclusion. In VT models,ViT and CaiT have large model parameters, butthe experimental results are not satisfactory, andthere are good classification results obtained by thelightweight DeiT and T2T-ViT. The same resultsare also obtained by the MLP models. Finally,owing to the complexity of the computational pro-cess caused by the complexity of the network, somesmall-scale models also require considerable com-putation time. In contrast, three channels of SIC,MGIC and MSIC only use simple convolutional andAM blocks, and using parallel training techniquescan significantly reduce computation the time ofthe three channels, so the IL-MCAM frameworkdoes not consume a lot of time for training althoughthere are large model parameters.Table 10: Model parameters and training time ofcomparing between the proposed approaches and othertraditional deep learning models.Model/FrameworkAlexNet [41]VGG-16 [42]Inception-V3 [45]ResNet-50 [43]Xception [46]ResNeXt-50 [48]InceptionResNet-V1 [47]DenseNet-121 [44]ViT [51]DeiT [52]BoTNet-50 [55]CaiT [53]CoaT [56]T2T-ViT [54]LeViT [57]MLP-Mixer [58]gMLP [59]ResMLP [60]MCAMIL-MCAMSize (MB) Time (s)21751283.49079.68830.827.131.221.172.146020.615.565.822573.2169639639133170605340477240154564326028601502256647726956307328522943112846396894370607060The confusion matrix for the three randomisedexperiments is shown in Fig. 7. To further anal-yse the causes of misclassification, we consultedthe pathologists in detail and concluded the fol-Figure 12: Examples of misclassified images from HE-CRC-DS.19A: AbnormalP: NormalScore: 99.99%A: AbnormalP: NormalScore: 99.91%A: NormalP: AbnormalScore: 70.51%A: NormalP: AbnormalScore: 97.88%lowing. Examples of misclassified images from thethree randomised experiments are shown in Fig. 12.These examples can explain the three main rea-sons for the misclassification of HE-CRC-DS in theCHIC task using the proposed IL-MCAM frame-work. First, most of the lumen structure in Figs. 12-(a) and (b) is regular, and the cancer part occu-pies a small portion at the edge of the image, sothe IL-MCAM framework classifies this image asnormal during the testing phase. Second, Fig. 12-(c) is an image in the low grade category, wherethe nuclei of some of the luminal structures havestarted to enlarge, leading the IL-MCAM frame-work to classify the image as abnormal. Finally,Fig. 12-(d) shows an image from the normal cate-gory with blebs, which is misclassified owing to thepresence of blebs.6. Conclusion and Future WorkIn this paper, we propose an IL-MCAM frame-work based on attention mechanisms and interac-tive learning for CHICs. The proposed IL-MCAMframework uses an MCAM model that combineddifferent attention mechanisms for automatic learn-ing. After automatic learning, the misclassified im-ages are iteratively trained by manually labellingthe attention regions to achieve the interactive pro-cess. Finally, evaluation metrics are obtained bytesting.In the CHIC task, a significant perfor-mance improvement is observed in the proposed IL-MCAM approach compared with traditional deepIn addition, we conduct threelearning models.extended experiments: ablation experiments illus-trate the role of each channel in the IL-MCAMframework; interchangeability experiments demon-strate the feasibility of designing three channels,and illustrate the interchangeability of the threechannels, and extended experiments on the NCT-CRC-HE-100K dataset illustrate the generalisationability of the IL-MCAM framework.In the future, to accommodate different CHICtasks, we plan to find the most suitable model forthe current task from attention mechanisms anddeep learning models using permutation and com-bination. We also plan to add attention mecha-nisms at different locations of deep learning mod-els to analyse the impact of convolutional layers onclassification performance in CHIC tasks.Declaration of competing interestThe authors declare that they have no conflict ofinterest.AcknowledgmentsThis work is supported by the ‚ÄúNational NaturalScience Foundation of China‚Äù (No.61806047) andthe ‚ÄúFundamental Research Funds for the CentralUniversities‚Äù (No. N2019003). We thank Miss Zix-ian Li and Mr. Guoxian Li for their important dis-cussion.References[1] M. Ganz, X. Yang, G. Slabaugh, Automatic segmen-tation of polyps in colonoscopic narrow-band imagingdata, IEEE Transactions on Biomedical Engineering59 (8) (2012) 2144‚Äì2151.doi:https://doi.org/10.1109/TBME.2012.2195314.[2] F. Bray, J. Ferlay, I. Soerjomataram, R. L. Siegel,L. A. Torre, A. Jemal, Global cancer statistics 2018:Globocan estimates of incidence and mortality world-wide for 36 cancers in 185 countries, CA: a cancer jour-nal for clinicians 68 (6) (2018) 394‚Äì424. doi:https://doi.org/10.3322/caac.21492.[3] J. Zhou, R. Zheng, S. Zhang, H. Zeng, S. Wang,R. Chen, K. Sun, M. Li, J. Gu, G. Zhuang, et al., Col-orectal cancer burden and trends: Comparison betweenchina and major burden countries in the world, ChineseJournal of Cancer Research 33 (1) (2021) 1. doi:https://doi.org/10.21147/j.issn.1000-9604.2021.01.01.[4] M. A. Iftikhar, M. Hassan, H. Alquhayz, A colon can-cer grade prediction model using texture and statisticalfeatures, smote and mrmr, in: 2016 19th InternationalMulti-Topic Conference (INMIC), IEEE, 2016, pp. 1‚Äì7.doi:https://doi.org/10.1109/INMIC.2016.7840161.[5] F. Aeffner, K. Wilson, N. T. Martin, J. C. Black,C. L. L. Hendriks, B. Bolon, D. G. Rudmann, R. Gi-anani, S. R. Koegler, J. Krueger, et al., The gold stan-dard paradox in digital image analysis: manual versusautomated scoring as ground truth, Archives of pathol-ogy & laboratory medicine 141 (9) (2017) 1267‚Äì1275.doi:https://doi.org/10.5858/arpa.2016-0386-RA.[6] S. Ai, C. Li, X. Li, T. Jiang, M. Grzegorzek, C. Sun,M. M. Rahaman, J. Zhang, Y. Yao, H. Li, A state-of-the-art review for gastric histopathology image anal-ysis approaches and future development, BioMed Re-search International 2021. doi:https://doi.org/10.1155/2021/6671417.[7] M.I. Jordan, T. M. Mitchell, Machinelearn-ing: Trends, perspectives, and prospects, Science349 (6245) (2015) 255‚Äì260. doi:https://doi.org/10.1126/science.aaa8415.[8] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, nature521 (7553) (2015) 436‚Äì444. doi:https://doi.org/10.1038/nature14539.[9] L. Itti, C. Koch, E. Niebur, A model of saliency-basedvisual attention for rapid scene analysis, IEEE Trans-actions on pattern analysis and machine intelligence2020 (11) (1998) 1254‚Äì1259. doi:https://doi.org/10.1109/34.730558.[10] N. Linder, J. Konsti, R. Turkki, E. Rahtu, M. Lundin,S. Nordling, C. Haglund, T. Ahonen, M. Pietik¬®ainen,J. Lundin,Identification of tumor epithelium andstroma in tissue microarrays using texture analysis,Diagnostic pathology 7 (1) (2012) 1‚Äì11. doi:https://doi.org/10.1186/1746-1596-7-22.[11] L. Jiao, Q. Chen, S. Li, Y. Xu, Colon cancer de-tection using whole slide histopathological images, in:World Congress on Medical Physics and Biomedical En-gineering May 26-31, 2012, Beijing, China, Springer,2013, pp. 1283‚Äì1286. doi:https://doi.org/10.1007/978-3-642-29305-4_336.[12] R. Peyret, A. Bouridane, S. A. Al-Maadeed, S. Kun-hoth, F. Khelifi, Texture analysis for colorectal tumourbiopsies using multispectral imagery, in: 2015 37th An-nual International Conference of the IEEE Engineer-ing in Medicine and Biology Society (EMBC), 2015,pp. 7218‚Äì7221. doi:https://doi.org/10.1109/EMBC.2015.7320057.[13] A. Chaddad, C. Desrosiers, A. Bouridane, M. Toews,L. Hassan, C. Tanougast, Multi texture analysis ofim-colorectal cancer continuum using multispectralagery, PloS one 11 (2) (2016) e0149893. doi:https://doi.org/10.1371/journal.pone.0149893.[14] J. N. Kather, C.-A. Weis, F. Bianconi, S. M. Melch-ers, L. R. Schad, T. Gaiser, A. Marx, F. G. Z¬®ollner,Multi-class texture analysis in colorectal cancer histol-ogy, Scientific reports 6 (1) (2016) 1‚Äì11. doi:https://doi.org/10.1038/srep27988.[15] C. T. Sari, C. Gunduz-Demir, Unsupervised feature ex-traction via deep learning for histopathological classi-fication of colon tissue images, IEEE Transactions onMedical Imaging 38 (5) (2019) 1139‚Äì1149. doi:https://doi.org/10.1109/TMI.2018.2879369.[16] J. Malik, S. Kiranyaz, S. Kunhoth, T. Ince, S. Al-Maadeed, R. Hamila, M. Gabbouj, Colorectal cancerdiagnosis from histology images: A comparative study,arXiv preprint arXiv:1903.11210.URL https://arxiv.org/abs/1903.11210[17] H. Yoon, J. Lee, J. E. Oh, H. R. Kim, S. Lee, H. J.Chang, D. K. Sohn, Tumor identification in colorectalhistology images using a convolutional neural network,Journal of digital imaging 32 (1) (2019) 131‚Äì140. doi:https://doi.org/10.1007/s10278-018-0112-9.[18] N. Dif, Z. Elberrichi, A new deep learning model selec-tion method for colorectal cancer classification, Interna-tional Journal of Swarm Intelligence Research (IJSIR)11 (3) (2020) 72‚Äì88. doi:https://doi.org/10.4018/IJSIR.2020070105.[19] H.-G. Nguyen, A. Blank, A. Lugli,I. Zlobec, Aneffective deep learning architecture combination fortissue microarray spots classification of h amp;estained colorectal images, in: 2020 IEEE 17th Inter-national Symposium on Biomedical Imaging (ISBI),2020, pp. 1271‚Äì1274. doi:https://doi.org/10.1109/ISBI45749.2020.9098636.[20] S. Ghosh, A. Bandyopadhyay, S. Sahay, R. Ghosh,I. Kundu, K. Santosh, Colorectal histology tumor de-tection using ensemble deep neural network, Engineer-ing Applications of Artificial Intelligence 100 (2021)104202.doi:https://doi.org/10.1016/j.engappai.2021.104202.V. Derang`ere, F. Ghiringhelli, G. Forestier, C. Wem-mert, Deep learning for colon cancer histopathologicalimages analysis, Computers in Biology and Medicine136 (2021) 104730. doi:https://doi.org/10.1016/j.compbiomed.2021.104730.[22] E. F. Ohata, J. V. S. das Chagas, G. M. Bezerra, M. M.Hassan, V. H. C. de Albuquerque, P. P. Reboucas Filho,A novel transfer learning approach for the classificationof histological images of colorectal cancer, The Journalof Supercomputing (2021) 1‚Äì26doi:https://doi.org/10.1007/s11227-020-03575-6.[23] T. Sarkar, A. Hazra, N. Das, Classification of colorec-tal cancer histology images using image reconstructionand modified densenet,International Conferencein:on Computational Intelligence in Communications andBusiness Analytics, Springer, 2021, pp. 259‚Äì271.URL https://link.springer.com/chapter/10.1007/978-3-030-75529-4_20[24] D. Sarwinda, R. H. Paradisa, A. Bustamam, P. Anggia,Deep learning in image classification using residual net-work (resnet) variants for detection of colorectal can-cer, Procedia Computer Science 179 (2021) 423‚Äì431.doi:https://doi.org/10.1016/j.procs.2021.01.025.[25] M. M. Rahaman, C. Li, Y. Yao, F. Kulwa, M. A. Rah-man, Q. Wang, S. Qi, F. Kong, X. Zhu, X. Zhao, Iden-tification of covid-19 samples from chest x-ray imagesusing deep learning: A comparison of transfer learningapproaches, Journal of X-ray Science and Technology28 (5) (2020) 821‚Äì839. doi:https://doi.org/10.3233/XST-200715.[26] C. Li, J. Zhang, F. Kulwa, S. Qi, Z. Qi, A sars-cov-2 microscopic image dataset with ground truth im-ages and visual features,in: Chinese Conference onPattern Recognition and Computer Vision (PRCV),Springer, 2020, pp. 244‚Äì255. doi:https://doi.org/10.1007/978-3-030-60633-6_20.[27] S. Kosov, K. Shirahama, C. Li, M. Grzegorzek, En-vironmental microorganism classification using condi-tional random fields and deep convolutional neural net-works, Pattern recognition 77 (2018) 248‚Äì261. doi:https://doi.org/10.1016/j.patcog.2017.12.021.[28] C. Li, K. Wang, N. Xu, A survey for the applica-tions of content-based microscopic image analysis inmicroorganism classification domains, Artificial Intel-ligence Review 51 (4) (2019) 577‚Äì646.doi:https://doi.org/10.1007/s10462-017-9572-4.[29] J. Zhang, C. Li, S. Kosov, M. Grzegorzek, K. Shi-rahama, T. Jiang, C. Sun, Z. Li, H. Li, Lcu-net: A novellow-cost u-net for environmental mi-croorganism image segmentation, Pattern Recognition115 (2021) 107885. doi:https://doi.org/10.1016/j.patcog.2021.107885.[30] X. Zhou, C. Li, M. M. Rahaman, Y. Yao, S. Ai, C. Sun,Q. Wang, Y. Zhang, M. Li, X. Li, et al., A compre-hensive review for breast histopathology image analysisusing classical and deep neural networks, IEEE Access8 (2020) 90931‚Äì90956. doi:https://doi.org/10.1109/ACCESS.2020.2993788.[31] D. Xue, X. Zhou, C. Li, Y. Yao, M. M. Rahaman,J. Zhang, H. Chen, J. Zhang, S. Qi, H. Sun, An ap-plication of transfer learning and ensemble learningtechniques for cervical histopathology image classifica-tion, IEEE Access 8 (2020) 104603‚Äì104618. doi:https://doi.org/10.1109/ACCESS.2020.2999816.[21] A. B. Hamida, M. Devanne, J. Weber, C. Truntzer,[32] H. Chen, C. Li, X. Li, G. Wang, W. Hu, Y. Li,21W. Liu, C. Sun, Y. Yao, Y. Teng, et al., Gashis-transformer: A multi-scale visual transformer approachfor gastric histopathology image classification, arXivpreprint arXiv:2104.14528.URL https://arxiv.org/abs/2104.14528[33] Y. Li, X. Wu, C. Li, X. Li, H. Chen, C. Sun, M. M.Rahaman, Y. Yao, Y. Zhang, T. Jiang, A hierarchi-cal conditional random field-based attention mecha-nism approach for gastric histopathology image clas-sification, Applied Intelligence (2022) 1‚Äì22doi:https://doi.org/10.1007/s10489-021-02886-2.[34] W. Hu, C. Li, X. Li, M. M. Rahaman, J. Ma, Y. Zhang,H. Chen, W. Liu, C. Sun, Y. Yao, et al., Gashissdb: Anew gastric histopathology image dataset for computeraided diagnosis of gastric cancer, Computers in biologyand medicine (2022) 105207doi:https://doi.org/10.1016/j.compbiomed.2021.105207.[35] M. M. Rahaman, C. Li, X. Wu, Y. Yao, Z. Hu, T. Jiang,X. Li, S. Qi, A survey for cervical cytopathology im-age analysis using deep learning, IEEE Access 8 (2020)61687‚Äì61710. doi:https://doi.org/10.1109/ACCESS.2020.2983186.[36] M. Mamunur Rahaman, C. Li, Y. Yao, F. Kulwa,X. Wu, X. Li, Q. Wang, Deepcervix: A deep learning-based framework for the classification of cervical cellsusing hybrid deep feature fusion techniques, Computersin Biology and Medicine 136 (2021) 104649.URL https://doi.org/10.1016/j.compbiomed.2021.104649[37] W. Liu, C. Li, M. M. Rahaman, T. Jiang, H. Sun,X. Wu, W. Hu, H. Chen, C. Sun, Y. Yao, et al., Is the as-pect ratio of cells important in deep learning? a robustcomparison of deep learning methods for multi-scalecytopathology cell image classification: From convolu-tional neural networks to visual transformers, Comput-ers in biology and medicine (2021) 105026doi:https://doi.org/10.1016/j.compbiomed.2021.105026.[38] X. Li, C. Li, F. Kulwa, M. M. Rahaman, W. Zhao,X. Wang, D. Xue, Y. Yao, Y. Cheng, J. Li,for dynamic object be-et al., Foldoverhaviour description in microscopic videos, IEEE Ac-cess 8 (2020) 114519‚Äì114540. doi:https://doi.org/10.1109/ACCESS.2020.3003993.features[39] A. Chen, C. Li, S. Zou, M. M. Rahaman, Y. Yao,H. Chen, H. Yang, P. Zhao, W. Hu, W. Liu, et al., Sviadataset: A new dataset of microscopic videos and im-ages for computer-aided sperm analysis, Biocyberneticsand Biomedical Engineeringdoi:https://doi.org/10.1016/j.bbe.2021.12.010.[40] Y. LeCun, B. Boser, J. S. Denker, D. Henderson,R. E. Howard, W. Hubbard, L. D. Jackel, Backprop-agation applied to handwritten zip code recognition,Neural Computation 1 (4) (1989) 541‚Äì551. doi:https://doi.org/10.1162/neco.1989.1.4.541.[41] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenetclassification with deep convolutional neural networks,Advances in neuralinformation processing systems25 (2012) 1097‚Äì1105. doi:https://doi.org/10.1145/3065386.[42] K. Simonyan, A. Zisserman, Very deep convolu-tional networks for large-scale image recognition, arXivpreprint arXiv:1409.1556.URL https://arxiv.org/abs/1409.1556[43] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learn-ing for image recognition, in: Proceedings of the IEEEconference on computer vision and pattern recognition,2016, pp. 770‚Äì778.doi:https://doi.org/10.1109/CVPR.2016.90.[44] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Wein-berger, Densely connected convolutional networks, in:Proceedings of the IEEE conference on computer vi-sion and pattern recognition, 2017, pp. 4700‚Äì4708. doi:https://doi.org/10.1109/CVPR.2017.243.[45] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wo-jna, Rethinking the inception architecture for computervision, in: Proceedings of the IEEE conference on com-puter vision and pattern recognition, 2016, pp. 2818‚Äì2826. doi:https://doi.org/10.1109/CVPR.2016.308.[46] F. Chollet, Xception: Deep learning with depthwiseseparable convolutions, in: 2017 IEEE Conference onComputer Vision and Pattern Recognition (CVPR),2017, pp. 1800‚Äì1807. doi:https://doi.org/10.1109/CVPR.2017.195.[47] C. Szegedy, S. Ioffe, V. Vanhoucke, A. A. Alemi,inception-resnet and the impact ofInception-v4,residual connections on learning,in: Thirty-firstAAAI conference on artificial intelligence, 2017, pp.4278‚Äî-4284. doi:https://dl.acm.org/doi/10.5555/3298023.3298188.[48] S. Xie, R. Girshick, P. Doll¬¥ar, Z. Tu, K. He, Ag-gregated residual transformations for deep neural net-works, in: Proceedings of the IEEE conference on com-puter vision and pattern recognition, 2017, pp. 1492‚Äì1500. doi:https://doi.org/10.1109/CVPR.2017.634.[49] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,L. Jones, A. N. Gomez, (cid:32)L. Kaiser, I. Polosukhin, Atten-tion is all you need, in: Advances in neural informationprocessing systems, 2017, pp. 5998‚Äì6008.URL https://arxiv.org/abs/1706.03762[50] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S.Khan, M. Shah, Transformers in vision: A survey,arXiv preprint arXiv:2101.01169.URL https://arxiv.org/abs/2101.01169[51] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-senborn, X. Zhai, T. Unterthiner, M. Dehghani,M. Minderer, G. Heigold, S. Gelly, et al., An imageis worth 16x16 words: Transformers for image recogni-tion at scale, arXiv preprint arXiv:2010.11929.URL https://arxiv.org/abs/2010.11929[52] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-rolles, H. J¬¥egou, Training data-efficient image trans-formers & distillation through attention, in: Interna-tional Conference on Machine Learning, PMLR, 2021,pp. 10347‚Äì10357.URLtouvron21a.htmlhttps://proceedings.mlr.press/v139/[53] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve,H. J¬¥egou, Going deeper with image transformers, arXivpreprint arXiv:2103.17239.URL https://arxiv.org/abs/2103.17239[54] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang,F. E. Tay, J. Feng, S. Yan, Tokens-to-token vit: Train-ing vision transformers from scratch on imagenet, arXivpreprint arXiv:2101.11986.URL https://arxiv.org/abs/2101.11986[55] A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel,A. Vaswani, Bottleneck transformers for visual recog-nition,2021 IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR), 2021,pp. 16514‚Äì16524.doi:https://doi.org/10.1109/in:22[69] M. Raghu, C. Zhang, J. Kleinberg, S. Bengio, Trans-fusion: Understanding transfer learning for medicalimaging, arXiv preprint arXiv:1902.07208doi:https://arxiv.org/abs/1902.07208.[70] S. J. Pan, Q. Yang, A survey on transfer learning,IEEE Transactions on knowledge and data engineering22 (10) (2009) 1345‚Äì1359. doi:https://doi.org/10.1109/TKDE.2009.191.[71] Z. Niu, G. Zhong, H. Yu, A review on the atten-tion mechanism of deep learning, Neurocomputing452 (2021) 48‚Äì62. doi:https://doi.org/10.1016/j.neucom.2021.03.091.[72] B. S. Webb, N. T. Dhruv, S. G. Solomon, C. Tailby,P. Lennie, Early and late mechanisms of surround sup-pression in striate cortex of macaque, Journal of Neuro-science 25 (50) (2005) 11666‚Äì11675. doi:https://doi.org/10.1523/JNEUROSCI.3414-05.2005.[73] F. M. Castro, M. J. Mar¬¥ƒ±n-Jim¬¥enez, N. Guil, C. Schmid,K. Alahari, End-to-end incremental learning, in: Pro-ceedings of the European conference on computer vision(ECCV), 2018, pp. 233‚Äì248. doi:https://doi.org/10.1007/978-3-030-01258-8_15.[74] I. Loshchilov, F. Hutter, Fixing weight decay regular-ization in adam (2018).URL https://openreview.net/forum?id=rk6qdGgCZ[75] S. Woo, J. Park, J.-Y. Lee,I. S. Kweon, Cbam:Convolutional block attention module,in: Proceed-ings of the European conference on computer vision(ECCV), 2018, pp. 3‚Äì19. doi:https://doi.org/10.1007/978-3-030-01234-2_1.[76] H. Lee, H.-E. Kim, H. Nam, Srm: A style-based re-calibration module for convolutional neural networks,in: Proceedings of the IEEE/CVF International Con-ference on Computer Vision, 2019, pp. 1854‚Äì1862. doi:https://doi.org/10.1109/ICCV.2019.00194.[77] M. Macenko, M. Niethammer, J. S. Marron, D. Bor-land, J. T. Woosley, X. Guan, C. Schmitt, N. E.Thomas, A method for normalizing histology slides forquantitative analysis, in: 2009 IEEE International Sym-posium on Biomedical Imaging: From Nano to Macro,IEEE, 2009, pp. 1107‚Äì1110. doi:https://doi.org/10.1109/ISBI.2009.5193250.[78] J. N. Kather, N. Halama, A. Marx, 100,000 histologicalimages of human colorectal cancer and healthy tissue(Apr. 2018). doi:https://doi.org/10.5281/zenodo.1214456.[79] J. N. Kather, J. Krisam, P. Charoentong, T. Luedde,E. Herpel, C.-A. Weis, T. Gaiser, A. Marx, N. A.Valous, D. Ferber, et al., Predicting survivalfromcolorectal cancer histology slides using deep learn-ing: A retrospective multicenter study, PLoS medicine16 (1) (2019) e1002730.doi:https://doi.org/10.1371/journal.pmed.1002730.CVPR46437.2021.01625.[56] W. Xu, Y. Xu, T. Chang, Z. Tu, Co-scaleimage transformers, arXiv preprintconv-attentionalarXiv:2104.06399.URL https://arxiv.org/abs/2104.06399[57] B. Graham, A. El-Nouby, H. Touvron, P. Stock,A. Joulin, H. J¬¥egou, M. Douze, Levit: a vision trans-former in convnet‚Äôs clothing for faster inference, arXivpreprint arXiv:2104.01136.URL https://arxiv.org/abs/2104.01136[58] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer,X. Zhai, T. Unterthiner, J. Yung, A. P. Steiner, D. Key-sers, J. Uszkoreit, et al., Mlp-mixer: An all-mlp archi-tecture for vision, in: Thirty-Fifth Conference on Neu-ral Information Processing Systems, 2021, pp. 1‚Äì12.URL https://arxiv.org/abs/2105.01601[59] H. Liu, Z. Dai, D. R. So, Q. V. Le, Pay attention tomlps, arXiv preprint arXiv:2105.08050.URL https://arxiv.org/abs/2105.08050[60] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, A. Joulin, G. Synnaeve, J. Verbeek,H. J¬¥egou, Resmlp: Feedforward networks for imageclassification with data-efficient training, arXiv preprintarXiv:2105.03404.URL https://arxiv.org/abs/2105.03404[61] L. Yang, R. Zhang, L. Li, X. Xie, Simam: A simple,parameter-free attention module for convolutional neu-ral networks, in: International Conference on MachineLearning, PMLR, 2021, pp. 11863‚Äì11874.URL https://proceedings.mlr.press/v139/yang21o.html[62] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wo-jna, Rethinking the inception architecture for computervision, in: 2016 IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2016, pp. 2818‚Äì2826.doi:https://doi.org/10.1109/CVPR.2016.308.[63] J. Hu, L. Shen, S. Albanie, G. Sun, E. Wu,Squeeze-and-excitation networks, IEEE Transactionson Pattern Analysis and Machine Intelligence 42 (8)(2020) 2011‚Äì2023.doi:https://doi.org/10.1109/TPAMI.2019.2913372.[64] F. Chollet, Xception: Deep learning with depthwiseseparable convolutions, in: 2017 IEEE Conference onComputer Vision and Pattern Recognition (CVPR),2017, pp. 1800‚Äì1807. doi:https://doi.org/10.1109/CVPR.2017.195.[65] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, Q. Hu,Eca-net: Efficient channel attention for deep convo-lutional neural networks,in: 2020 IEEE/CVF Con-ference on Computer Vision and Pattern Recognition(CVPR), 2020, pp. 11531‚Äì11539. doi:https://doi.org/10.1109/CVPR42600.2020.01155.[66] W. Rawat, Z. Wang, Deep convolutional neural net-works for image classification: A comprehensive re-view, Neural computation 29 (9) (2017) 2352‚Äì2449.doi:https://doi.org/10.1162/neco_a_00990.[67] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich,Going deeper with convolutions, in: Proceedings of theIEEE conference on computer vision and pattern recog-nition, 2015, pp. 1‚Äì9. doi:https://doi.org/10.1109/CVPR.2015.7298594.[68] L. Sifre, S. Mallat, Rigid-motion scattering for tex-ture classification, arXiv preprint arXiv:1403.1687doi:https://arxiv.org/abs/1403.1687.23