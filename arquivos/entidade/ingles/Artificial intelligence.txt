ELSEVIER Artificial Intelligence 77 (1995) 95-127 Artificial Intelligence Artificial intelligence: an empirical science Herbert A. Simon* Department of Psychology, Carnegie Mellon University, Pittsburgh, PA 15213-3890, USA Received August 1993; revised May 1995 Abstract My initial tasks in this paper are, first, to delimit the boundaries of artificial intelligence, then, to justify calling it a science: is AI science, or is it engineering, or some combination of these? After arguing that it is (at least) a science, I will consider how it is best pursued: in particular, and theory in developing AI. the respective roles for experiment I will rely more on history than on speculation, the field has much to tell us about how we can continue and accelerate of my examples will be drawn from work with which I have been associated, speak with greater confidence about what motivated its defects) than I can about the work of others. My goal, however, through history, but to make definite proposals where relevant, in advancing that advance. Many for I can that work and its methods (and about is not to give you a trip for our future priorities, using history, for our actual experience for my views. as evidence 1. Artificial intelligence as science AI deals with some of the phenomena surrounding computers, hence is a part science [27]. It is also a part of psychology and cognitive science. It that appear when computers perform of computer deals, tasks that, if performed by people, would be regarded as requiring thinking. in particular, with the phenomena intelligence- in the 1950s as an inquiry the nature of intelligence began It used computers as a revolutionary indeed exhibit, thereby providing a means for examining it in utmost detail. “B.C.“, Artificial intelligence. intelligence, before computers, the only observable examples of intelligence were the minds of living organisms, especially human beings. Now the family of intelligent systems had been joined by a new genus, intelligent computer programs. into tool to simulate, * E-mail: has@a.gp.cs.cmu.edu. 0004-3702/95/$09.50 SSDZ 0004-3702(95)00039-9 0 1995 Elsevier Science B.V. All rights reserved 96 H.A. Simon I Artificial intelligence 77 (1995) 95-127 1.1. The multiple goals of AI from the AI thrust, As the papers reprinted reveal, in Feigenbaum and Feldman’s classic, Computers and three- Thought, pronged. One goal was to construct computer programs (e.g., the Logic Theorist) capable of exhibiting intelligent “complex “artificial friends and foes of the activity.) systems. information intelligence”, which had become the alternative the established usage among both to begin building a theory of name for the endeavor was and thereby, (The original Carnegie-Rand its very beginnings, was at least later accepted processing”; intelligence, our group A second goal was to construct programs (e.g., GPS) that exhibited intelligence by using processes like those used by humans in the same tasks. Here the aim was intelligently. The third to achieve a theory of how the human mind can behave intelligent programs (e.g., Tonge’s assembly line balancing goal was to construct program) in performing some of the world’s work. (In the body of this paper, I will refer to systems in this the usual denotation of third category as “expert systems”, enlarging somewhat in Sections 2 and 3 of Part 1 of Computers and that name.) The systems described Thought focus upon the first of these three objectives; in Part 2, those described upon the second; and those in Sections 4-6 of Part 1 upon the third. that could supplement or complement human intelligence of the systems intelligence. Surrounding Almost from its very birth, then, AI was a multicelled organism. Its foundation intelligence, either as pure was the capability for building systems that exhibited into the nature of intelligence, explorations of the theory of human explorations tasks or explorations intelligence, requiring there gradually grew up corresponding bodies of theory. But we should not think of the programs the Logic Theorist as isolated from the theories. Quite the contrary. For example, in solving theory embodies problems likewise, is a powerful heuristic GPS embodies commonly employed by people for problem-solving a theory: requires a physical symbol system capable of heuristic search. that could perform practical these operative AI systems that achievement that means-ends intelligence analysis theory the the of of the We can extract is no more separable from such programs verbal theoretical principles, as Allen Newell and I did in our 1975 Turing Award address, but the the basic operational definitions of what the principles mean. programs provide The theory is from of the verbal statements are different theories, which exhibit different properties when the programs are run. I will have more to say later about the relation of programs I to theories and how runs of programs are used to test theories. For the moment, will simply remind you that: the mathematics of the laws of motion. Different than classical mechanics from the program implementations search. statements The moment of truth is a running program. 1.2. Social fragmentation of AZ Side by side with the growth of programs aimed at the triple objectives of the human mind and building and understanding intelligence, understanding H.A. Simon ! Artificial Intelligence 77 (1995) 95-127 97 rather researchers focus upon expert systems, these social structures in detail, except to mention there have grown up communities of researchers understanding concerned with these objectives. Some of the researchers are interested in more than one of the three goals, a few with all three; but social structures have formed than that emphasize and enforce their common concerns. I will not describe the separateness of the three endeavors that, in there are four, not three, principal groups; for at least two rather distinct intelligence. One the “pure” is often associated with and/or science”; some within AI groups some found mainly in computer science departments, in program verification identifies with “cognitive some of its in computer and fact, groups of subgroup, colleagues who are complexity. Another members are to be found science, miscellaneous other areas. the years, all four AI groups has gradually the distance ranging from the Ameri- increased. They attend different professional meetings, to can Psychological Society, ACM and engineering societies like IEEE. They limit their reading and citations more and more their training in different academic disciplines and subdisciplines, each passing on to the next generation to the journals published by their groups. They receive its own specialized version of the enterprise. in psychology, some the Cognitive Science Society, interested subgroup in anthropology in philosophy, in linguistics, computational the AAAI, separating through theory some Over of that, In assigning a broad definition in spite of diversity of goals, there to AI, covering all of these groups, I reveal my that makes belief I believe that each one continuing communication its goals by drawing on the of these groups can get substantial help in advancing the serious work of the others, and that disadvantages the over have been of fragmentation) whole history of AI. I believe that AAAI and the Cognitive Science Society share primary for opposing and turning back the forces of dissociation. the advantages of interaction among them highly desirable. (and demonstrated is a common core responsibility frequently response intelligence The distinction My reasons for believing in complementarity of the several goals will emerge as I would just like to state two of the reasons now, in a preliminary way. I proceed. the way in which humans achieve (I will call it “heuristic First, search”) is quite different from the way in which computers performing numerical analysis and similar tasks typically do it. (We might call the latter method, “brute is not black and white, but force disciplined by mathematics.“) obvious none the less. The human methods, I believe, are absolutely essential for ill-structured problems, hence we must under- intelligent the human mind or stand them regardless of whether our aim is to understand intelligence Programs in general. for solving inverting matrices or equations depend heavily on the size and speed of solving partial differential computers, but use algorithms based on the known, and usually rich, mathemati- the amount of search required. cal structures of their problem spaces to reduce search, but almost These always preserve to find the to any desired degree of approximation. Because of the mathematical solution reduction principles do not always optimize the property of completeness-they linear programming are guaranteed to relatively problems, search 9x H. A. Simor~ ” Artificiul Intelligence 77 (1995) 9.5-127 solving regularity sufficiency Human of the spaces searched, and sometimes problem humans, whose computational super-computers tion, problems having of strong shrewd use of heuristics of search and optimality What I am calling “disciplined goals, poorly ill-defined and regular mathematical that it is often possible to prove theorems about the the efficiency, seldom of algorithms. shares any of these properties. Nevertheless, abilities or even PCs. are sometimes able are puny compared with to solve, with those of modern little computa- are very difficult even by computer and bounded People standards-problems spaces, problem solve such problems characterized structure. lack by the and at the expense of giving up guaranteed attained. of the solutions completeness brute force” had its origins in numerical the theory of computation, which has had some extension systems. Many of us do not believe the same that range of application force can achieve its successor, to symbolic brute and numerical disciplined attain. Unless that humans to explore reason every for of source underlying mechanisms brute understanding problems, force, even disciplined how to constructing ideas and Artificial intelligence and until it is demonstrated heuristic that search vigorously intelligent systems, equal reason the human and techniques that give these brute force, human intelligence, their power fails. These concerns or not, can be applied for solving expert has always systems had a special analysis from of the methods and flexibility they can, we have as a techniques the to understand in situations where are central both to to ill-structured area, where programs rely on heuristic and often use satisficing criteria of success. Given this interest, reason which to keep is gradually in close touch with the progress of research demystifying the nature of human “shrewdness”, such problems. in search, without interest this important guarantees of they in human “residual” completeness, have every intelligence, “intuition” A second and even “creativity”. reason the principal means for close communication for their performance, of progress and in our then these tasks. Over the years, of tasks has disclosed a continually among all of the sub-strands tasks to field find is of AI is requiring to see what kinds of processes exploration are of a steadily richer array of mechanisms programs computer even illustrates, behavior. And as the great diversity of existing for a single programs behavior task there may be many ways to skin can cast much provides characteristics range of difficult are light on the underlying of alternative Human the peculiar intelligence a valuable of human tasks where of intelligence. to perform domain that intelligence sufficient widening for intelligent that play chess the cat. Comparison principles ill-structured regularly exhibited. 2. Artificial and natural objects Artificial were designed objects, including computer to be that way. This fact has led some programs, are what to claim they are because they that there can be no H.A. Simon I Artificial Intelligence 77 (1995) 95-127 99 science of artificial objects, but only an engineering the most extreme to form of mathematical science of AI, and denigrate engineering”. look theorems about intelligent systems as the only genuine route technology. Those who hold and proof of to a the role of system building and experiment as “only the discovery this view lend themselves they do not But the claim that artificial objects are divorced that fallacious. An artificial object natural object. Automobiles conservation objects, natural or artificial. No object, artificial or natural, these laws-satisfy from empirical science, and is to natural-science methods of research is as fully bound by the laws of nature as any the law of gravity and the are as subject laws limit the set of possible that does not obey to of energy as glaciers are. Scientific exist [36, Chapter 11. these constraint-an 2.1. Znternal constraints The natural laws that determine and behavior of an object, natural or artificial, are its internal constraints. An artificial system, like a natural that can be studied by the methods of one, produces to all science. observation empirical phenomena common and experiment the structure It might be objected ignorance of natural that a system designed deliberately law and of the effects produced by natural way can produce no surprise or new information. This objection enormous operating on complex systems. The world of artificial (and natural) objects of unanticipated and of computational of systems empirically many species of organisms. portion of the richness and complexity of the real world by attempting it from first principles. to behave in a desired shrugs off our laws is full consequences, because of the limits both of empirical knowledge power. The case, in AI, for studying many different kinds to the case, in biology, for studying is essentially In neither case can we capture more than a miniscule to deduce identical Often the most efficient way to predict and understand the system and observe complex system is to construct are also computational models, we can use the programs models, an advantage theory not only models but simultaneously ena under study. The “natural” for the field of AI that is unique the behavior of a novel it. Because AI programs themselves as their own In AI, the exhibits the behavior of the phenom- in science. sciences also depend, their behavior, scientist constructs systems and studying method. The natural certain natural produced change with changes in the system parameters. So Galileo rolls balls down inclined planes or over the edges of tables, and measures the time of the roll or the length of the flight as a function of the angle of the plane. for their progress, on building artificial for this is the essence of the experimental in which then observes these phenomena to be prominent; by the system, and especially how laws is thought the phenomena the operation a system of To experiment is to use the artificial to study the natural. To design an AI 100 H.A. Simon I Artificial Intelligence 77 (1995) 95-127 system and observe how its behavior changes with changes in the design is to perform an experiment. Most of what we know about artificial intelligence has been thereby making AI a thoroughly experimental learned by carrying out experiments of this kind, science. 2.2. External constraints AI, a science of design-is A system, artificial or natural, must conform not only to the internal constraints imposed by natural law, but to two sets of external constraints as well. The system that are defined by natural can only come into existence under conditions law; and it can only survive and operate effectively in suitable environments. These initial and boundary conditions are the system’s external constraints. Synthetic chemistry-like (by actual synthesis) that operate upon chemical molecules. constraints, we can often determine what its various to survive, and how functions components to Darwin, generates systems or modifies existing ones; then tests their ability to survive in the ambient environment. The artificer does exactly the same thing, except that the generator the designer (the design process) is more purposeful, and the tests (the purposes has in mind) may go beyond biological fitness. the external constraints the external carry out these functions. Nature, according a system must perform By manipulating to determining in order devoted As the functional teleological component purposes do, the difference between vanishes. The constraints same natural laws as those that face design. requirements into all systems imposed by the environment in the same way that a introduce the designer’s the natural and the artificial fades and then imposed by nature on living organisms derive from the 2.3. Science and engineering there is moderated is also a large element of chance We see that, far from striving to separate science from engineering, we need not distinguish them at all. But if we insist upon a distinction, we can think of engineering as science for people who are impatient. The Darwinian processes of biology depend on the chance of mutations and crossover to produce new designs. in human design processes, Although is already chance by heuristics in a very known about the systems of interest, selective way, greatly increasing the scientist the While is interested also in creating systems that achieve desired goals. Apart engineer there is no need to distinguish between computer from this difference scientists and computer engineers, or AI scientists and engineers. We can stop debating whether AI is science or engineering; that use prior knowledge, what to generate and combine elements the odds that the product will be functional. in creating new knowledge, is interested in motives, specifically it is both. H.A. Simon I Artificial Intelligence 77 (1995) 95-127 101 3. Research by system synthesis It is time to relate these generalities Regardless of our reasons for pursuing AI, its main research method is to: and study systems that exhibit intelligence. The basic paradigm to the discipline of artificial intelligence. is to build a feature of intelligence Select a task incorporating practical importance or that exhibits features and complexities yet been simulated by AI systems. Build a system exhibiting intelligence. Examine the behavior of the system in different ments and with different initial conditions. that is of substantial that have not this feature of task environ- incorporated The Logic Theorist some simple methods of heuristic analysis, which was tested [41] had means search, which were tested on the task of discovering proofs for theorems. GPS incorpo- in a variety of simple problem- rated means-ends language solving domains. SHRDLU for processing natural strings and extracting in a blocks and learning to world. EPAM [ll] has mechanisms for recognizing, settings drawn from discriminate, which were tested the research for determining the position of a vehicle and steering it, which are tested by driving it on roads. In what sense are these kinds of design projects experiments? their semantic meanings, which were in a range of experimental [30] has mechanisms literature on verbal learning. NAVLAB remembering tested 3.1. System design as experimentation the An experiment manipulates system. What are the independent variables of a particular in an AI system? The dependent variables clearly are measures of the performance of the system: how intelligently it behaves both in terms of the range of tasks it can handle and its skill and efficiency and dependent variables and dependent independent in handling them. Defining variables for means-ends the independent takes a little more care. Suppose that we are studying a particular version of GPS. First, there is the core of the system: in the case of GPS, principally its it also contains mechanism strategies for heuristic search; it may incorporate best-first search, for example, or it contains information about particular depth-first search. Still more peripherally, task domains, situations including productions that select move operators (current situation and goal situation) and productions that are noticed. relevant its basic symbol-processing analysis. A little more peripherally, that notice differences between the differences to reducing capabilities and Changes in any or all of these components can be regarded as changes in the internal constraints on GPS. Or, we can think of the symbol-processing capa- bilities and the means-ends mechanism as internal constraints, and the remaining as initial conditions. Both internal constraints and initial conditions components In addition, can be treated as independent variables for experimental purposes. 102 H.A. Simon I Artificial Intelligence 77 (1995) 95-127 the task environments with which we confront GPS define the external constraints on its behavior, constituting another set of independent variables. In the earliest experiments in AI, with systems like LT, GPS, STUDENT and task domain and domain the others that are reported knowledge were held constant, while the principal independent variables were the to be answered core of the system itself, and often was: “What basic symbolic capabilities and heuristics will enable a system to exhibit in a task domain that is difficult for humans?” in Computers and Thought, its strategies. The question intelligence Procedures the problems with moderate comparison with a brute-force (as compared, operate for evaluating outcomes were not elaborate. Did the system solve computing in search? At what level of problem difficulty could it effort? Did selectively, it behave say, with human skills)? Today, when an artificial intelligence project is aimed at extending AI to a new class of task domains, matters remain much the same. The BACON system [21] takes a set of data from an experiment. BACON contains for scientific discovery capabilities and a small set of heuristics for inducing basic symbol-manipulating consists laws from data and inventing new theoretical concepts. Experimentation in exploring the range of tasks over which it can and can’t discover the regularities in data, the reasons for its successes and failures (i.e., capabilities and the characteristics of the corresponding the degree of selectivity of its search. the relation between task environments), its and independent variable in the system will improve In such a line of experimentation, whether with GPS or BACON, initially, the is the core of the system itself and its strategies. in a task, and what improves, principal What changes changes are required emphasis may shift from manipulating fixed system over a range of task environments. How flexible and general system? the characteristics of the system to testing a is the to handle new tasks ? As system performance its performance In research within the so-called expert-novice is, the system’s domain knowledge, that variable. The main initial conditions, dependent organized how in memory, generality, the task domain core of mechanisms interest is in learning how much knowledge, In research on Is there a small that can carry the main part of the load over all the tasks? for expert performance. independent variable. is needed is the central paradigm, on the other hand, the in- is the central Extendability is generality over Two themes are visible in much AI experimentation. One is extendability; the tried out on “toy that are better structured and less difficult than the real-life tasks we is a toy task; medical diagnosis is a other tasks”-tasks would like to handle. The Tower of Hanoi real-life tasks. New ideas in AL are often task. Learning a language example, Siklossy’s ZBIE simple syntax and semantics, for one reason or another task; but we may build an AI system, for its ability to acquire is not yet ready to handle [34], that, while demonstrating is a real-life H.A. Simon I Artificial Intelligence 77 (1995) 95-127 103 the full scope of a natural language. We illuminate a real-life task by simulating a toy subtask. language. to extend its extendability theory of language We should remember to acquiring a complete natural We may wish to consider ZBIE a candidate in its veridicality will depend on the prospect of extending If ZBIE’s limits appear learning, but its our confidence capabilities to be due to the short time it has had for learning or to physical limits on memory size, we will be more sanguine about than if we can see that for the extension. Of course additional or different mechanisms will be needed it. the final test of its strength as a theory will be the actual attempt in physics are only tested in relatively that most theories simple laboratory situations, and in fact, many important phenomena can only be observed clearly under highly controlled conditions. So we must be careful not to those impose imposed that we would never reach beliefs about anything significant. One sound reason for caution about that handle toy subtasks successfully is that in AI we are always faced with the specter in our of combinatorial in programs ability to build and use large knowledge bases to increase selectivity in building [31], and as we have succeeded like DENDRAL levels of increasing numbers of systems at (human) professional performance, less menacing. And as I will discuss later, such warnings as NP-completeness do not threaten combinatorial explosion for most of the problems we actually seek to solve. explosion of search. But as we have gained confidence in other sciences. That would surely guarantee the upward scalability of programs theory verification much stronger [23] and INTERNIST the specter becomes in AI requirements that operate than for Generality over tasks is that component of the program. interested, that apply to a wide range of tasks. GPS was designed to separate as it should be, in discovering from the task-dependent an extensive those mechanisms of the components of the program, and Ernst that it in a dozen or more environments without alteration of the component of the program. The theory of problem solving that research activity to demonstrate AI is most intelligence task-independent and Newell [8] undertook could solve problems task-independent GPS represents Similarly, the EPAM program (or be given) an appropriate [ll] contains a core of mechanisms for recogni- it must as initial in its memory, as well as strategies derived from the task instructions. tion, memory and learning. To perform any task within its capabilities, acquire conditions It is primarily these mechanisms tasks. that we regard as the EPAM theory; and it is to new body of knowledge, the core mechanisms invariant as EPAM that should remain is extended stored it should not be supposed their cores. Knowing how much knowledge, is required by a program is However, and what kind of to limited tasks is also an knowledge, to know important part of AI theory. today to play at what knowledge, how organized, would be required for a chess program that the theoretical content of programs it to real-life interest It would be of enormous to extend 104 H.A. Simon 1 Artificial Intelligence 77 (1995) 95-127 grandmaster grandmaster. level without needing to search more (100 branches?) than a human Research within the expert-novice the knowledge bases required is concerned with understanding determining intelligence cesses (and other bases for intelligence) those more specialized processes reached in particular domains. paradigm has focused specifically on for high level performance. Artificial those general heuristic pro- to many domains and to be both that are applicable that permit high levels of performance 3.2. The physical symbol system hypothesis The first task of AI research was to determine whether at all with symbolic intelligent behavior systems. The repeated could be obtained led Al Newell and successes that the field achieved me, this strong common foundation of the whole gamut of intelligent devices and programs. We called it The Physical Symbol System Hypothesis: in a variety of task domains to offer a hypothesis in our 1976 Turing Lecture, list-processing to explain A physical symbol system (PSS) has the necessary and sufficient means for general intelligent action. As the hypothesis such symbols and symbol structures, in detail the defining is a familiar one, I need not recount characteristics of a physical symbol system. A PSS is simply a system capable of and inputting, outputting, organizing storing symbols (patterns with denotations), and for reorganizing and acting conditionally on the outcomes of the tests of identity or difference, identity. Digital computers are demonstrably PSSs, and a solid body of evidence that brains are also. The physical materials of which PSSs are has accumulated made, and the physical laws governing these materials are irrelevant as long as they support symbolic storage and rapid execution of the symbolic processes mentioned comparing above. them The PSS Hypothesis asserts that the external constraints intelligence can be satisfied by, and only by, a PSS. Since different imposed by any task tasks requiring impose quite different constraints, the claim that being a PSS is necessary and sufficient for intelligence may seem surprising. Its truth depends essentially on the generality and adaptability of PSSs like computers and brains. Of course we are but of the speaking here not of mathematical empirical can exhibit intelligence over a wide range of tasks, employing only acceptable amounts of computation to do so. truth (e.g., Turing computability) instructed, fact that computers and brains, appropriately There is some dispute today about hinging on the definition of the term “symbol”. in connectionist so that advocated by Brooks are not regarded as symbols, then the hypothesis wrong, for systems of these sorts exhibit intelligence. have, above) as patterns the Physical Symbol System Hypothesis, If we define “symbol” narrowly, systems or robots of the sort is clearly If we define symbols (as I systems and Brooks’ [2] the basic components then connectionist that denote, H.A. Simon I Artificial Intelligence 77 (1995) 95-127 105 robots qualify as physical symbol systems. empirical one, whose fate will continue the mechanisms where we draw the definitional boundary of “symbol”. employed by systems is an to be decided by empirical evidence about regardless of intelligence, the hypothesis In any case, that exhibit 4. Theories of intelligence Putting aside now the design task of developing specific intelligent systems, we turn to the task of developing the theories of those systems together with theories of the design process. I must preface what I am going to say with a discussion of what the term “theory” means, or should mean. 4.1. What is a theory? There is an unfortunate the three theories confusion, encouraged by the similarity between in getting much from little. From a few basic premises, all sorts of important laws of motion, the in an empirical science, on the between theories, on the other. This confusion probably the great success of by, in are words “theory” and “theorem”, one hand, and formal deductive originated with, and certainly has been encouraged Newtonian mechanics particular derived mathematically In many minds, branch of mathematics and has created strong urges in the other sciences to emulate empirical truth by reasoning. Economics provides perhaps examples of the use of logic unfettered conclusions about sciences, Very the illusion that physics is nearly a textbooks on Rational Mechanics); this royal road to the most flagrant to reach unwarranted by observation the real world, but examples are not absent from the other little of the physics of complex systems about how matter behaves this success has created (recall the innumerable including computer in the real world. consequences science. (the atmosphere, flavor. Mathematics in empirical observation. by boundary conditions It is sometimes theory was motivated by anomalies of observation-espe- it is surrounded portions, that are grounded condensed matter) has this highly deductive generous but conditions that special relativity cially the incompatibility anics and the Lorentz black-body intensities of spectral obtained when bolometers were extended observed phenomena between the Galilean invariance of Maxwell’s laws. Similarly, Planck’s radiation was motivated by the failure of Wien’s law to explain invariance of the laws of mech- law of the radiation range. Carefully that spectral lines in the face of new observations of infra-red into are still the starting point for theory in physics. the ocean, is, in there initial and forgotten When we turn from physics to sciences like biology and geology, and even the priority of observed phenomena over conclusions reached via long from general axioms becomes even more evident. Not only do chemistry, chains of inference most of the known regularities and experimentation, in these sciences derive from extensive observation but many of the regularities, especially the most important, 106 H.A. Simon I Arrificial Intelligence 77 (1995) 95-127 such generalizations are not quantitative, called theory of disease, we observed, theory, are any equations The germ and for that matter. of disease theory but qualitative. In our Turing Address, Al Newell laws of qualitative structure is such a qualitative the theory of evolution (QS and inexact by natural and I laws). The germ law, as is the cell If there selection. in “The Origin of Species”, they are exceedingly inconspicuous. like: “Most organisms says something like: “If you diagnose a disease, (of course, you won’t always find one).” The cell theory (perhaps many similar are made up of one or more structures that are ‘cells’ structure, all having called for example, remarkably nuclei (actually, approximate, of have central them).” Both theories soon theories are qualitative, surrounded become and generality, are QS laws; by crowds of laws of are few describing relatively of vary degrees process. Most of precision of these species only eukarytes look for a microorganism says something more!) membrane-bounded across in basic course, even particulars, organization quantitative. vague. These and When we are dealing with complex this kind of complexity all have systems, whatever and messiness. the science, To some extent portions of them can if their application cases. For even smaller, simpler be solved in closed subsystems, form. More often, the mathematical the behavior systems has to be studied by computer modelling and simulation, to simple can sometimes almost still be modelled mathematically-or limited formulations complex little or no help from today, problems solved numerically largest users of supercomputers. are seldom theorems. Even in the more in closed symbolic solved theoretical portions form; more often with many hours of computing; and physicists theories they can is of with of physics they are are the world’s properties of complex systems, all the Al systems of interest, we find that two forms, which at first view seem diametrically in the form of computer and fuzzier of these Because essentially take theories Allen Newell and I dubbed in their application of these programs, “laws of qualitative to Al. a in Al opposed: theories term that the principal describes theories there are precise that of the form each structure”. Let us examine 4.2. Computer programs as theories in Al In the physical sciences, systems of differential theories of system behavior. equations must be supplemented and initial computer and boundary programs, which conditions. systems of difference equations, perform do in physics. The only distinction is that the former treat precise for expressing situation, of system parameters the differential In cognitive science, simply of differential and difference while computation set to any value, the latter equations equations treat cycle. Since it as a discrete variable-the interval the basic time this is a distinction without equations provide For predictions the major tool in any given estimates by empirical from a formal the same exactly standpoint are role as systems time as a continuous system changes between differential variable state with each by the system can be fact, difference. (In represented an important H.A. Simon I Artificial Intelligence 77 (1995) 95-127 107 when we make numerical computations routinely approximate continuous by discrete on differential time.) equation systems, we form in closed and difference equations to yield general in real or complex Simple systems of differential of system theorems can be solved for any values of the parameters. As we have seen, when matters get a numbers behavior in the equations are not numerical, little more complex, or when the symbols solutions in closed form can no longer be obtained, and the system is studied by carrying out simulations for various values of the parameters. As the results apply only to the particular parameter values used in the simulations, we return again to the kinds of qualitative generalizations all complex systems. that characterize as theories, we must In interpreting programs characteristics of the programs represent regarded as irrelevant “notation”, and questions arise in natural science theories, but perhaps AI that is worth examining. to define what the theory, what characteristics are to be and what parts constitute boundary conditions theory. These same form in application of the initial conditions take a particular for a particular take care processes. is a theory of problem In the earlier discussion of experimentation, we saw that a running AI program contains a definition of the goal and knowledge about the task domain as well as some of which may be problem-solving It also contains strategies, task-specific. When we say that GPS solving, we are including at least some of the more general, and speaking of the core program, task-independent For example, is a theory of human perceptual and memory processes it must be given the stimuli plus relevant knowledge assumed to be in memory already at the time the task. the task is performed How strategies this information were adopted are also appropriate targets of scientific inquiry, but they are not part of the core EPAM theory. and the strategies used by the subject to perform got into memory and why and how particular the EPAM program [ll]. To test its predictions in any given task situation, strategies. content If we strip all of the domain-specific simple mechanisms. Likewise, BACON from EPAM, or a medical diagnosis program or a chess-playing program, what remains is usually a small set is capable of of fairly discovering for a wide range of scientific physical and chemical phenomena, heuristics for generating hypotheses heuristic. consists of a half dozen domain-independent and a simple search-control for consideration, laws and new theoretical [21], which concepts Theory versus programming In the natural sciences, details in old-fashioned is usually relatively there the in which it is expressed. Maxwell’s equations can be little confusion between theory and the notation written coordinate notation or in more modern vector notation. Everyone agrees that in either form it is the same theory. Even in a more complex and subtle case, everyone agrees that Heisenberg’s matrices, Schrbdinger’s wave equations, the same theory: quantum mechanics. and Dirac’s abstract algebraic all represent formulation 108 H.A. Simon I Artificial intelligence 77 (1995) 95-127 In theories there is still some ambiguity as formalisms implemented by running programs, to how far down in the hierarchy of programming the theory extends. Pretty clearly, the fact that a program is written in Common Lisp is not part of the it expresses. But how about the fact that it is written in some form of Lisp? theory Or that it is written language? in judging the degree Since speed of execution is not wholly of intelligence independent that of its programming languages of some kind, and that most AI programs are written in list-processing in most of them are implemented the processes is not an irrelevant consideration the theory implementation. language and not an algebraic Surely it is not irrelevant in an AI program in a list-processing in a performance, as productions. from the primitives of the programming is not always easily made. the pragmatic sentiments One way in which we can make clearer the substantive content of our programs is to indicate as definitely as we can the domain primitives, which can then be this distinguished distinction Beyond the relations between intelligence, on the one hand, and list processing and productions, on the other, have perhaps not been adequately elucidated. One might even want to strengthen the definition of a physical symbol system processing capabilities, the ability associations as well as the ability ductions). Almost all intelligent programs make essential use of these abilities. to include a requirement of list- to form associations and labelled (pro- to act on recognition I have just expressed, language. However, (descriptions), especially In describing intelligent programs and those of their components interest, we are usually able to characterize theoretical if inexact, fashion. This brings us back to laws of qualitative structure qualitative, form, in AI. Much of our communication preferably backed up by the harder currency of running programs. The shorthand allows us to ignore program details that are irrelevant it does not provide guarantees that the informally described system will perform as adver- tised . takes this shorthand about our theories these components to the theory; that are of in a I will have more to say later about methods programs. But before we go into questions of interpretation turn to the other principal form of theory: laws of qualitative structure. for describing and evaluating let us and evaluation, 4.3. Laws of qualitative structure in AI As we seek to develop theory in computer science, and specifically in AI, we that characterize should be looking for laws of qualitative structure and regularities of organization and process primarily empirical and experimental. out by designing complex systems particular parameter values, of course) and operating conditions, using a variety of observations and measurements behavior. them. Our search for them will of necessity be In the case of AI the search will be carried laws (quantified with that embody them under a wide range of their to characterize these In our Turing Address, Newell and I proposed, in addition to the Physical H.A. Simon I Artificial Intelligence 77 (1995) 95-127 109 Symbol System Hypothesis, (HS): a second QS law, the Heuristic Search Hypothesis Problems are solved (when intelligence selectively sentation). (heuristically) through a problem is required for solution) by searching repre- space (i.e., a problem The Heuristic Search Hypothesis The HS Hypothesis is nearly as broad as the PSS Hypothesis, qualitative. It is nevertheless quite powerful problems are generally solved by exhaustive search through or without knowledge the help of knowledge of the structure of the problem is used by converting it into search control heuristics. in what it excludes. It denies and equally that large problem spaces, space. This The HS Hypothesis the conditions under which heuristics that such processes as hill climbing and means-ends characterize principles of effective search control. With respect learned powerful bases for selectivity about Sometimes is augmented by a number of more specific QS laws that some relatively general and useful search heuristics, and by some to search heuristics, we have analysis provide in many task domains, and we know a good deal like these are or aren’t effective. formally. these conditions For example, hill climbing is a reliable method only when local maxima are also by other criteria when analysis works only if the problem space is global maxima, and hill climbing must be supplemented this condition factorable in a certain sense (when operators can be ordered so that differences removed by operators of high priority are not reinstated by those of low priority [7,191). (e.g., [9]), we are even able to characterize is not met; means-ends for example, classes of task domains: With respect to search control, special-purpose heuristics have been devised for for game particular environments. The contrasts and best-first strategies are fairly well understood. Most of this knowledge also takes the form of QS laws, although a few mathematical through the efficiency of the A* search algorithm, the literature measured by length of solution path; about efficiency, measured by expected computing effort, about criteria for optimal best-first search [38]). among depth-first, breadth-first, theorems are scattered theorems about in performance alpha-beta search (e.g., Twenty or more years of research on expert systems has produced a third very general QS law, problem solution by recognition (REC): Expert systems solve frequently occurring problems recognition. largely by the process of That stored system is to say, an expert (computer or human) possesses a set of productions capable of noticing cues in everyday problems and thereupon evoking for dealing with the situations the knowledge that is relevant in medical role, for example, marked by the cues. Recognition plays a central system, human or automated, diagnosis, whether in and DENDRAL, to elucidate interprets mass spectrograph chemical structure the early expert in order in memory which [23]. data 110 H.A. Simon ! Artificial Intelligence 77 (1995) 95-127 Recognition processes are implemented, of EPAM and the Rete nets of production processes another closely related QS law, the Knowledge Principle: in the discrimination net system languages. Using recognition large bodies of data, suggesting to draw upon for example, the expert enables intelligent understanding A system exhibits competence primarily because of the specific knowledge bear: the concepts, heuristics about its domain of endeavor. facts, representations, methods, models, metaphors, [22]) and action at a high level of that it can bring to and (Lenat and Feigenbaum These examples, and especially the QS laws, PSS, HS and REC, show in what large measure our general knowledge about problem solving in AI is embedded in laws of qualitative structure which have been induced from specific expert systems as computer programs. All of these programs are PSSs, and their modelled identified. components In comparison with the QS laws, the mathematical the field has created implement HS and REC heuristics are easily that theorems that to date fade into insignificance. The same picture emerges when we turn for example. A number of systems have been constructed to other subdisciplines within AI: that learn efforts, or from the successful problem-solving learning, from their own problem-solving in the form of worked-out examples of problem solutions. The efforts of others to systems of Waterman, Neves and others [24,40], belong adaptive production this line of work, as do the explanation-based learning systems of Mitchell, and the chunking procedures of Soar [25]. What we have learned about learning from the construction of such systems is perhaps best summed up by the QS law of Learning from Examples (LE): system examples of problem If a production solutions, showing the intermediate analysis or some related method of causal attribution can be used to create automatically new productions capable of solving problems of the same general kind. is provided with detailed steps, then means-end In yet another AI domain, more than three decades of experience systems for automatic body of knowledge about the requisites of such systems, translation of natural in building language has produced a substantial including the QS law: Satisfactory lexicon and syntax, but also a substantial body of semantic knowledge provide context for resolving ambiguities. translation of natural language requires not only knowledge of a to respect AI resembles most other Similar examples of QS laws can be extracted from other domains within AI. In for this empirical knowledge the example, and processes of specific species of organisms, combined with more structures processes general QS laws describing general mechanisms connecting DNA with proteins, these mecha- nisms are being modelled symbolically with computer programs; and computer simulations are being compared with the findings of experimental manipulations. (e.g., metabolism, Increasingly, In biology, of is typically embodied scientific disciplines. immune reactions). in descriptions H.A. Simon I Artificial Intelligence 77 (1995) 95-127 111 4.4. Dealing with complexity dynamic long-term Currently, systems whose is more promise for theories the topic of complexity attracts a great deal of interest, but there in full about complexity remains a question of what can be said meaningfully that deal with particular aspects or generality. There theory of chaos treats the complexity of forms of complexity. The mathematical nonlinear The theory of systems that possess many interacting components deals with another complexity deal with a third form. form of complexity. Theories of computational too broad to support theories with any considerable to be), one can be optimistic about the possibility of building theories and discovering QS structure these that characterize the possibilities just for hierarchical structure of many-component in complex systems. Thus, while bare “complexity” may be a category content various kinds of complex rather systems; and seriality and parallelism (as “general systems” proved aspects of complexity: systems. Let me is unpredictable. illustrate behavior closely related two Hierarchy It has been observed empirically for a long time that most many-component in nature and those observed those devised by man-have a systems-both [36]. That is to say, viewed from the top down, they can hierarchical architecture be divided into subsystems that are divided in turn into subsubsystems, and so on, until we reach a level of primitives which we do not wish to, or cannot, decompose is to to the bottom. The Second respect Commandment (No level of the hierarchy GOTOs!). The Third Commandment insensitive levels interact only through further. The First Commandment the of structured programming top is to minimize the interaction between different substructures to the structure of the levels below, so that adjoining this hierarchy by working inputs and outputs. is to make each from It would seem that structured programming was already for these commandments invented many eons ago by Nature, are observed pretty well in a vast majority of natural systems. Organisms are made of systems (digestive, respirato- ry, circulatory and so on); systems are made of organs, organs of tissues, tissues of cells, cells of organelles, organelles of proteins, proteins of amino acids, amino acids of atoms and so on. At a still more minute scale, we run all the way down through atomic nuclei to elementary particles, quarks, and possibly strings. At the other end of the scale, the universe contains galaxies, which contain stars, which may have planetary systems. Hierarchy yields several laws of qualitative structure, systems will evolve more rapidly generalizations. An example of mathematical hierarchical law that the long-run dynamics of such systems depends (approximately) higher termined nearly independently within each subsystem. level structure, while than non-hierarchical high frequency, the short-run, the former is a QS and even some precise law that ones, and a only on is de- dynamics One of the mathematical generalizations is a formalization of the latter QS law, II? H.A. Simon I Artificial Intelligence 77 (1995) 9.5-127 and provides and quantitative, ming mentioned algorithms for the computations [S, 361. These are closely above. related to the commandments laws, both qualitative program- of structured Hierarchy may be viewed as a powerful of computation required system can be expected antidote to determine to increase only to computational the (approximate) complexity. behavior linearly with the number and if the subsystems at any given cells in tissues of an organism), to increase the possibilities for parallel computation level are and remain only logarithmically. in a system are inversely identical The amount a hierarchical primitives; identical Since of precedence constraints, and the latter are related of interaction would be conducive of parts, we would to parallelism, provided expect to guide the boundaries of the parallel intensity frequency organization and strength and to the number the hierarchical lines of hierarchy were used That components These claims would have is to say, we would expect that do not have high-frequency to gain from a capability interaction. for parallelism to be made much more precise before literally, but they illustrate one way in which one might approach systems. It is interesting that principles organizations, which are almost like always hierarchical these are taken of parallel most human not ments), infrequent As with to hierarchy and with occasion the other role theory to play a major mathematical real phenomena ments. To a major through building and departmental of authority but the hierarchy arranged to boxes-within-boxes so to interact with each other. topics we have discussed, we see that empirical in the study of complexity, that will at least handle that there simplified models larger units that but and give guidance to the conduct and interpretation extent, we will reach an understanding of complex testing them. of of (e.g., related to that the that subsystems. between they could be the design in instantiated (I am referring arrange- relatively have has for research is also room of the complex of experi- systems 4.5. The role of formal theory I have already made a number observing intelligence, artificial only a modest future. As in most other empirical impact have been laws of qualitative and simulations. On the other side of with “formal synonymous dards of precision What distinguishes problems simpler solutions principal priate in closed technique task environments the matter, theories”. as do the symbolic from the physical form (i.e., for drawing them in role in AI, and are unlikely to play a central of comments that precise mathematical about sciences, structure, the theories supported the role of formal theories in have played theorems role in the foreseeable of greatest by detailed import experiments and “precise mathematical Computer expressions some mathematics sciences is that theorems” is not stan- programs meet of other parts of mathematics. the same that has been applied they do not usually to the admit seen, the in appro- theorems). inferences Consequently, from their behavior. them and evaluate as we have them is to run H.A. Simon I Artijkial Intelligence 77 (1995) 95-127 113 If we think of theorems and simulation simply as two kinds of formal treatment, the former when we simplify and abstract tools we have available. We obtain we obtain mathematical account more of the world’s complexity. AI in particular, we are usually operating those in which theorems can be proved. This is not a virtue; life. We should treasure and relevance can be proved. to fit the the latter when we take into In computer science in general, and in than it is simply a fact of the occasions when theorems of some generality, power in areas of greater complexity the real world Initial and boundary conditions in programs Some feel uncomfortable because programs seem so much more complex than laws of motion, or Maxwell’s equations, or even the laws of quantum Newton’s mechanics. Some comfort can be gained from in AI programs arises because they incorporate substantial part of this complexity their tasks, but also a great not only basic general mechanisms and deal of knowledge pertaining heuristics specific to those domains. The domain-specific knowledge and strategies in other sciences. correspond to the initial and boundary conditions of theories task domains, and strategies the point made earlier, for performing to particular that a Theory primitives versus programming details I have also observed that even the computer code representing the AI theory that boundary leaves off and pure programming is a substantive, lies this kernel does convenience i.e., experimental, not signal where takes over. Where question. Specification languages for theories formal relatively languages in programs claims embedded that are not as precisely A different way to clarify the theoretical is to as define languages, but that describe the theory in a form that allows anyone programming skilled in the art to program it. Examples of this method of generalizing while retaining a good deal of precision will be found in the languages used in our book, to describe the BACON program and the other discovery Scientific Discovery, programs discussed there; and in several formalisms used in Human Problem Solving to describe GPS and other problem-solving programs. Some efforts are now under way [4] to define standard specification implemented languages in a formal way, short of full implementa- that could be used to define theories tion; but it remains whether a variety of languages will be needed representations different to be seen whether a single language can do the job or radically different tasks, or the same tasks with to accommodate used in handling different cognitive strategies. And, finally, the mechanisms that programs incorporate can usually be stated even more succinctly, if less precisely, as laws of qualitative structure. I14 H.A. Simon ! Artificial Intelligence 77 (1995) 95-127 5. Evaluating intelligent systems Evaluating the success of an artificial intelligence research simple or it can be complex. When a rather primitive heuristic the Logic Theorist search, with a modest that relatively strated selectivity, logic, that the result depended the fact that amounts human simple, made could fact alone find proofs for many theorems told us a great deal about in Principia, intelligence. for humans. on the task being nontrivial required modest amounts the program by today’s standards), but amounts comparable of computation (almost to what we might brain could provide. It depended its search highly selective on the fact that LT’s heuristics, as compared with brute-force search. can be effort (LT) demon- capability The significance It depended for a basic work on of also on trivial think a though Similar statements can be made about molecule or scientific discovery centrally interesting require systems professional a combination DENDRAL, Winograd’s INTERNIST, these humans, so, exhibit sufficient dog: “The marvel Demonstrating for computers AI enterprise. When we speak should be on understanding and describing is not the range of tasks them. that SHRDLU, medical diagnosis systems for AI levels of intelligence of knowledge identification systems systems like MYCIN like and like AM and BACON. What makes that, in is that in doing and heuristics the dancing at all.” they perform and knowledge, power about is that it dances that can be programmed tasks and the marvel computing statement base, Johnson’s it dances well; requiring intelligence the nature of these programs are major goals of the focus our main of evaluating such programs, for the task. We can echo Samuel 5.1. The purposes of evaluation How complex and difficult a matter it is to evaluate just seen, our goals. As we have our first aim will be to construct intelligence, of different forms what we expected to do may be relatively occasions when evaluation must be more elaborate in different them task environments. if our purpose a system will depend upon theory of the pure is to advance systems that exemplify intelligence Evaluating simple. But and principled. whether there they do are other Simulating human intelligence If we wish interest When our can do tasks the programs to claim step. scientists make discoveries, data, scientists. lies in understanding human mental processes, that human professionals that BACON teaches us something are capable of is only then we must also compare BACON’s showing that the first about how human processes with actually used by from field or history or laboratory, describing the processes Designing When expert systems the interest that can complement, lies in creating supplement systems, expert or replace like DENDRAL of human activities or MYCIN, experts, the H.A. Simon / Artificial Intelligence 77 (1995) 95-127 115 employed and the measures of success change again. In building to human the quality of the programs’ performances mechanisms expert systems the processes used by our systems will not be limited processes; but we will need to compare of other expert systems in and with the performances with human performance, the same domain, along all dimensions of concern: e.g., quality of solutions, error rate, cost, user-friendliness, and so on. Extending theory there is more them to perform their tasks and perform In both cases just mentioned to system synthesis designing and evaluating specific systems for particular uses. There in improving designs (or simulations). The research characteristics to enable Thus, systems must have and what general principles than merely is an interest includes determining what they must embody it when needed, and theories of problem-solving in the domain of artificial intelligence, we need theories of the charac- in systems, systems for learning, systems for navigating and in the external world (robots), systems for understanding human speech the more we can anticipate what intelligence to exhibit teristics and underlying principles of systems capable of holding memory and retrieving systems for inducing concepts, operating so on. The more powerful properties (humanoid or other), and the better theories, these domains must possess the systems we can design. them effectively. information systems these in Improving the design process In addition, research may aim at improving evaluation processes cesses, hence come within Hypothesis, mentioned. of the theory of solving problems. systems for automatic design. this kind of research Indeed themselves. As design and evaluation the scope of artificial the efficiency of the design and are intelligent pro- the PSS and the research previously the theory of designing can best be regarded as a special part It can be studied by creating and studying is not distinct from intelligence 5.2. Evaluating expert system designs of the 1940s performed the new system perform better and/or more efficiently the superiority, In the case of expert systems, evaluation of a particular design is often very than the more easily it can be although statistics were reported about how rapidly the most to it has been with steam engines, pragmatic: Does systems already available? The greater demonstrated. Analogously, the early computers important news was that they performed support automobiles, them. This was enough So radios and all other major engineering that evaluation of technology; but it b to argue that at the frontiers of a new technology, very crude qualitative evaluation may be enough to point the way. The design process, with its constant modification of the emerging system to meet difficulties and failures in airplanes, This is not to argue certain computations, for the advancement their continued is unimportant development. innovations. information llh H.A. Simon I Artificial Intelligence 77 (1995) 95-127 incorporates in itself a severe performance, considerations of extendability, visible to the designers who are familiar with the in the performance of early designs, may be more design details, but not revealed than statistics of for R&D fruitful directions toward important performance. regime of evaluation. Moreover, in pointing We are faced with the celebrated recipe for rabbit stew, which begins: “First catch the rabbit.” First design a system that has the desired general capability, at least at a minimal of the design and final evaluation may be very difficult, but at least it has a foundation on which to proceed. level. Having accomplished improvement that, The immediate research goal is sometimes to build a system that will be of practical use. More often, the goal is to use design and evaluation as a basis for building AI theories of the kinds suggested in the previous sections. The tasks are selected for feasibility and for the light they can throw on the general principles of organization and operation of intelligent systems, paving the way for construction at a later date of systems having real-world utility. In the beginning, tasks were selected that were relatively simple and well structured, and that called for little real-world knowledge. Standard environments like chess, the Tower of Hanoi and the Blocks World provided us with situations within which we could experiment and reach understanding of the properties and operation of fundamental problem-solving mechanisms. in memory and processed the research gradually extended taught us, among other in order With growing success in designing such systems (and growing size and speed of the computers available for simulation), to tasks calling for large amounts of real-world knowledge and tasks where the initial goals and constraints were less well defined: interpreting mass spectrograms, diagnosing disease. They have things, how knowledge must be organized to to permit knowledge-rich is, tasks in which a system must deal with an actual Robotics for they real-world that can be compel attention finessed is not limited to tasks calling for sensing and physical response; a scheduling system that handles about an actual flow of factory orders and responds completion, machine down-times, cancellations, data errors, etcetera, is also a “robot” to kinds and levels of complexity and uncertainty test beds. (What I am calling “robotics” tasks, that environment, for these purposes.) task environments. to AI research, are of growing in laboratory to genuine information importance intelligent response By now, a very wide range of tasks has been explored, when performed learning, and even for those qualities we call “intuition”, ty”. by human beings, call for professional-level including many that, for expertise, “insight” or “creativi- Which of these kinds of benchmark the greatest I can only answer, “All of the above”. A recent promise for future AI research? article in the AZ Magazine [15] provides a thoughtful discussion of the issues, and, along with their points of especially as the authors of different sorts of test beds agreement, illustrates vividly the complementarity tasks and test beds offer their disagreements reveal H.A. Simon I Artificial Intelligence 77 (1995) 95-127 117 If I had to express a preference, for the AI enterprise. Steve Hanks’ remarks about “the dangers of experimentation because I think such experimentation ty and cleanliness sometimes complexity, I would generally endorse in the small”, not is unnecessary, but because its manageabili- seduces us into neglecting domains of real-world the issues of extendability. to face squarely and refusing What is a success, and what a failure, in expert system design? Three kinds of criteria have been evoked: comparison with human performance, measurement of performance and upper bound of performance. But comparison with a theoretically before we take up these possible solutions to the evaluation problem, we need to say something about what we mean by a “good” or “effective” the domain of on a standard expert system. tasks from determined interest, set of Dimensions of effectiveness come immediately of effectiveness Three dimensions range and flexibility, and computational to mind: quality of efficiency. We can judge a for example, along the first and third dimensions by its strength of there will often be a between to range and flexibility, a to play chess is of no use for other tasks, whereas the General Problem and table of “Quality of include such performance, chess program, play and by the time it takes to make a move. Of course trade-off program Solver can attempt any task for which a suitable connections performance” components representation can be devised. criterion, which may as reliability, graceful degradation, user-friendliness is itself a multi-dimensional two criteria. With respect between operators and differences and others. these Comparisons with human performance As soon as it was shown that one can invert a large matrix more rapidly with a than wit a desk calculator, people began to do just that. Within a digital computer very short even demonstrate systems having more of an AI flavor than those used for matrix inversion. to the superiority. The same may be said, in general, about expert time the computer was so much more powerful evaluation was not than the calculator, that sophisticated in economic required terms, Levels of human performance provide useful benchmarks quality of expert systems, as long as system performance range. Human performance also calibrates tasks. Such measures can be used whether or not the expert system human processes. the breadth and flexibility of system performance not only provides a metric through for measuring the lies within the human that range, but over diverse imitates Comparison on standard tasks It is convenient to have available a set of benchmark standard some way from a domain’s population of tasks [12,33]. Standard used to evaluate a system at various stages of its development, compare appropriate the power of competing systems. The difficulty standard. tasks sampled in tasks can be to and especially lies in defining an Suppose we wish to evaluate a medical diagnosis system. (For an excellent 11x H.A. SIrnon / Artificial Intelligence 77 (199.5) 95-127 of the QMR system by N.B. Giuse, that signal the presence see an evaluation to list the diseases for which we want of these diseases recent example, We might be able the symptoms sample of tasks needs to be a sample not only over diseases, several symptoms, Should we weight our sample by frequencies Should we weight population? symptoms Some by using a decision-theoretic principle, seriousness questions might approach patterns where of symptom including ailments signal? it by these the of of But perhaps we are making the problem more difficult et al. [13].) the system to work. But are highly variable. Our of but over patterns are present simultaneously. patterns the diseases in the human the that in least be answered, at problem. to the sampling than purposes we can assume that any relatively expert in roughly systems this will only be approximately at such a ranking. the same order as any other broad true and is helpful only if our main If we wish to use the test tasks as a guide then to specific their inclusiveness, and the possibility system components may be more it really is. For broad sample will rank sample. At is interest to system their their of important relating than many practical different best, in arriving improvement, components representativeness. With all of these qualifications, for evaluating systems. A good example of speech for the evaluation standard the performance sets of tasks can generally systems of expert is the set of tests that ARPA recognition systems [26]. Different provide and for set on tested on the same corpus of speech, and evaluated with respect to speed and accuracy. Such tests can be designed to evaluate over specified different benchmarks useful comparing several occasions systems were both ranges of vocabulary Similarly, chess programs programs using and with human that the same scale is used and subject matter. are players, routinely and evaluated their strength by competition specified by EL0 between ratings, chess players. this kind of “horse to rate human raised to scientific for to win means provide principles sometimes from underlying been lack force. Objections have attention diverting ning”. But the objections system, is stronger, the very kinds competitions easier to find discourage things-however but also about of empirical the specific strengths that searching information from irrelevant projects and the tests of effectiveness rich information and weaknesses lead under to demonstrate racing” as to mere ways of “win- a powerful not only about who of the designs, These to understanding. street the discoveries may be to the goal. lamps where it is Theoretical bounds on performance Theories of computational complexity have given us some vated bounds on system performance. that most of them have theorems to evaluate and our systems. Another about which about complexity, The criterion focused on criteria One disadvantage that facilitate of the existing theoretically-moti- theories proving mathematical is these may not be the criteria we would be using case of the street it has been easiest lamp. to prove theorems size, N, grows is worst-case indefinitely performance in the limit, as some measure of problem H.A. Simon I Artificial Intelligence 77 (1995) 95-127 119 a system whose expected worst-case computation time to one whose time grows only polynomially large. By this criterion, grows exponentially with N is inferior with N. Moreover, solution any system that does not guarantee completeness for every problem Surely we would prefer than worst-case (reaching a test. times rather the former, we have to define a probability measure over the population of problems. For this, and other reasons of mathematical to estimate expected times, and most of the existing theorems use a worst-case criterion. computation in finite time) automatically to measure quality by expected computation it has seldom proved possible times, but to determine fails the worst-case feasibility, algorithms and offered in complexity theory, of One of the pioneers theory, Michael Rabin, as early as 1974 [32] described the dilemma I have just presented, expressed his dissatisfaction with the limits of existing remedial (including suggestions and allowing computation with applicability defining occasional errors). What he did not suggest, and what I would offer as solution for is the idea of using empirical criteria, based on actual computing the dilemma, experience, of “what works”, in those usual cases where theorems of the desired kind are unavailable. That is, in fact, what both AI and numerical computation have done from the beginning. limited some to use computational For many purposes, we will prefer systems that, though failing to solve problems, usually solve them in a short time and solve sometimes (if not always short) a large fraction of the problems presented time. Fig. 1 illustrates hypothetically some of the alternative ways in which we might want to evaluate systems. System A in the figure solves 60 per cent of the to reach asymptote after an problems presented to it in 1 minute, but appears in an acceptable Fig. 1. Problem solving power of three algorithms. 120 H.A. Simon I Artificial Intelligence 77 (1995) 9S-127 hour with only 7.5 per cent of the total solved. System B reaches 60 per cent only after 30 minutes, but solves 75 per cent with an hour, and appears to reach asymptote above 90 per cent. System C is guaranteed to solve all of the problems sooner or later, but solves only 10 per cent in 30 minutes, and 20 per cent in two hours. It is not evident which of these systems is to be preferred, and preference will depend on computing costs and on how serious the consequences are of failing to solve a problem within a given computing to be solved resources, we will seldom want to are very large compared with our computing rank alternative to how long it will take them to solve every problem. time. When the problems systems according Because of the relatively poor match between tractability of criteria for evaluating programs, on the one hand, and the practical significance of the criteria, on the other hand, mathematical theories have not taken us very far in system evaluation, and we have to rely principally on empirical evaluation methods the mathematical to guide design. interesting interesting complexity Closely related to know whether It is not particularly that are exponential the practical question integer programming, for goodness of solution. to issues of computational Systems can also be evaluated the standard measures of computational is the question of the scalability of a design. Designs that work well in a small scale do not always scale up well. Here, because algorithms in problem size explode rapidly, complexity may be of some is not how systems scale in the limit, but value. However, they require when used on problems of the sizes that occur in what computation practice. to know whether a program for natural language processing could handle a vocabulary of ten billion words; it is usually it can handle a vocabulary of one much more hundred thousand words (or, for some applications, even 100 words). In operations in real time, for instance, computation ordinarily there the efficiency of a program by the times it requires research is a tradition of (linear programming, to reach optimal evaluating that would usually solutions, but in many situations we might prefer a system in a short time to one that reach a very good (not necessarily optimal) solution In systems that have would find the optimum, but only after much computation. the solution to respond returns solution as that is “best so far” when the time limit is reached, or a “satisfactory” soon as one is obtained. to wring out the last drop of approximation There has also been a tradition It may be either impossible, or simply wasteful, to the optimum solution. to the A* search algorithm) of seeking to minimize the number of steps to solution; whereas in many domains, is of little interest; what is wanted is to conserve the number of steps to solution on computation [38]. If we are proving for finding the solution time required difficult theorems, we more often set the goal of finding a proof than of finding the shortest proof. Shortest path to solution and shortest expected computation time to solution are completely different criteria, and it is usually the latter, not the former, (e.g., in relation that is relevant. etcetera) H.A. Simon I Artificial Intelligence 77 (1995) 95-127 121 5.3. Use of statistical tests in evolution or fortunate. The AI literature has made rather testing hypotheses. One can argue both sides of the question of whether unfortunate matter formed a user and observer of typical usage and, at times, as a contributor can only provide published little use of standard statistical methods for this is I must confess to some rather strong attitudes on the testing theory, as to the theory. I to the these views in brief summary here, with some pointers in the course of long experience with hypothesis literature. Tests of statistical significance are widely used in psychology and biology, very little in physics. (The classical “probable errors” of physics are usually estimates of the accuracy of instruments, that an observed not tests of the probability could have occurred by chance.) Tests of significance are legitimate- phenomenon ly used to test whether a variable produces any effect (as compared with the null that the observed “effect” was produced by chance). The presence or hypothesis absence of statistical significance says nothing whatsoever of whether the effect is important significance and importance are unrelated quantities. in magnitude: to reporting As mathematical statisticians agree unanimously, tests of statistical significance cannot be used to test whether a model fits data. (For a brief explanation of why there.) One appropriate cited they can’t, see [14] or [35] and the references such tests is to report percentage of variance explained alternative showing (R*). In addition, how much a dependent one, variable changes with change provide measures of the importance of the independent variable. Even the latter holds only if the equations being analyzed are structural equations, statement the underlying causal structure of the phenomena reflecting coefficients), in an independent regression coefficients (not correlation [17]. As AI is primarily concerned with evaluating systems (i.e., models), statistical testing is unlikely to play a very useful role in the enterprise, although the magnitude, hence importance, of the effects in particular kinds of environments in systems operating to measuring hypothesis a greater attention produced by changes would be very desirable. 6. Theories of human intelligence Throughout this paper I have emphasized that is theory of intelligence, and have not had much to say the direction of AI research concerned with the general about models of human cognitive processes: psychological models. In spite of the shared method of research, the design and evaluation of systems, there is no immediately obvious reason why there should be any close connection research directed at designing intelligent systems and simulating human between cognition, or between It could be that, because of the radical differences between electronic devices and brains, pro- theories of intelligence. the corresponding 112 H. A. Simon ! Artificial Intelllgmce 77 (1W5) Y-5-127 designed to bc efficient systems would be expert from systems designed to simulate totally human different thinking. in is the this as expert Their problem-solving the searches systems) case. The strongest do not play chess searches of human chessmasters chess-playing programs are far more extensive in the same way as and a few [39]. However, kind have been written (e.g., the NSS program, PARADISE) with the aim of understanding human play by imitating of the programs level: designed the most powerful as expert of are modest amateurs. searches. well beyond The programs human it. systems play the programs designed capabilities, as and generally make use of only a moderate explore of the game only a few tree simulations and make (perhaps store of hun- use of of the branches to select the branches to be explored. to the matter. While internal different expert systems and cognitive constraints of (the physics the same tasks task demands. To the constraints- are numerous the means they are performing the same and heavy, between of implementation requirements these the programs that lowest at the in fundamental similarities the biology of brains). when grams architecture and process extent, specifically grandmasters. some To (designed human far less selective chess programs MATER, than of a different As of 1993. the most powerful conduct enormous simulations grandmaster their moves, The cognitive at a formidable as cognitive but not millions) chess knowledge is another are subject chess designed expert systems before making chess knowledge. dreds, heuristic But simulations computers to the same external they are subject requirements task the extent may themselves perform hardware side to quite that reveal dissimilar however level-the versus them, there Programs constructed programs constructed inner constraints. to simulate simply from telligence. of human tested whose domain. There human to perform efficiently tasks behavior are evaluated differently The former beings in function the same are tested by comparing task environments-exactly and explain their behavior with as any phenomena empirical is to describe in- requiring the behavior theories are in some about the methodology in any empirical the empirical the observations is nothing the methodology from the theory to predict the predictions with agreement of symbolic models of cognition (e.g., [6,25]). that distinguishes science. The general observe the theory it at a general paradigm the phenomena, level is to use compare to bring about better on substantive literature on methodology ’ ns well as a literature here. further phenomena, and revise there [ 1.281) topic (e.g., 1 will not discuss the theory with data. As is a large 7. The future of artificial intelligence At the outset of my remarks, of AI on the basis of what of the past should briefly. 1 said that I was going the past has taught us. A number plain, I need and now be reasonably of important summarize for the future lessons them only to prescribe H.A. Simon I Artificial Intelligence 77 (1995) 95-127 123 7.1. The fundamental strategy: pushing the frontier for whose performance Our main task in AI continues to be to explore a wider and wider range of activities to identify is essential. We need intelligence in handling, and attack that we have not yet succeeded aspects of intelligence each in turn as soon as we have any ideas about how to proceed. The attack must consist, as in the past, of building systems that actually perform the tasks and produce the phenomena associated with them. We need to evaluate our programs for their efficiency, and for scope and scalability. It is not hard to identify some task domains that should be high on the priority list, although no claims can be made for the completeness of such a list. My own and candidates would be two that are already receiving considerable are: another machine (including change of representa- tion). is still pretty much on the frontier. These attention, three candidates robotics and representation that learning, Machine learning Machine learning the specialization will delay specialists within AI. We might better say “specialists, alas”, for there that the impact of new discoveries about mechanisms on mainstream AI. Both serial and connectionist are being explored, each is effective. is in a vigorous state of activity with its own journal and is danger learning learning techniques that will teach us in which domains a healthy competition theorems about in theory-notably what I have said about (personal communication) in the body of my paper is as applicable There have been some achievements of learning algorithms-but the theorems and convergence theories to learning as to other topics in AI. Running programs and their evaluation, and QS laws are the key to progress. is today a reasonable Pat Langley learning. A count of balance between empirical work and theory papers presented that of 44 papers, 39 contained experimental evaluations of specific programs with explicit measures of performance. On the other hand, the conferences on computational learning theory than by specialists in artificial intelligence, present mainly theoretical papers. theory, probably populated more by computer that there in machine at a recent machine learning conference scientists believes showed from Robotics Robotics is also a vigorous field, and also in danger of being too far separated from mainstream AI. I viewed with mixed feelings the establishment of a separate in robotics in my own university. One of the common criticisms graduate program of much mainstream AI research in modelling the actual, real-world, situation and the model of the problem situations, between this luxury of situation in its systems confusing feedback channels to reflect reality more accurately. Of course, this distinction can be attained in AI modelling, by keeping in computer memory. As robotics cannot afford reality, the model with external the models periodically is that no distinction that can correct incorporate is made, it must stored 124 H.A. Simon 1 Artificial Intelligence 77 (1995) 95-127 in memory of robotics continually the one outside the computer. both an abstracted model and a simulated is that reminds the system builder of the complexity the distinction a necessity it makes “real world”, but the virtue and instead of an option, of the real real world- Representation It is still typically the case that before a computer can exhibit intelligence any task it must be provided with a representation space the kinds of objects that specifies and phenomena of the task domain: the kinds of operators that are available for changing in a in the problem one problem into another. Some work has been done. but only a modest amount, to show handling problem states, state how problem and representations can be generated from external The UNDERSTAND program [16] for example, can generate as inputs (simple to the General The puzzles). for specific physics problems Problem ISAAC from natural Solver program language of Newell, Laird and Rosenbloom its representations as it moves to new problem some procedures for representation change and Simon in representation [18] have proposed needed to solve information. problem repre- from verbal descrip- [29] can generate descriptions in [25], has some spaces. class that would the Mutilated in a restricted a method This sample problem. of what has been done reminds be like that could as the appropriate us of how much invent representation to be or even one for dealing with a the calculus, remains Through what sequence for quantum mechanics obtained, of steps was Heisenberg’s or Schrodinger’s wave matrix repre- suitable of problems sentations tions representations textbooks. capability Korf [19] specified of abstract problems. Kaplan change the critical produce Checkerboard The Soar program for modifying done. What would a program the calculus that could select particular representation sentation? problem? At the most general differences between (logic, mathematics) properties of these the whole domain 7.2. Methodology level, reasoning and reasoning representations of human-computer the topic of representation in formal in words or leads us to consider the linguistic representations from pictures or diagrams. Understanding is a basic the issue not only within AI, but also in interaction. in Al came largely from constructing ever intelligent programs to perform I should not like to leave the impression there is not room from our models increasingly that the for great the and faultless and that can be extracted the past progress and complex tasks, ill-structured we have used has been theory in it. Wherever In arguing that sophisticated more difficult and methodology improvement phenomena sciences. But perhaps make experiments carefully to system our they produce, we should extract the greatest cumulative opportunity it-just as is done for improvement evaluation, and (1) attending by (2) paying more a great deal more attention in other empirical in method systematically is to and to H.A. Simon I Artificial Intelligence 77 (1995) 95-127 125 comparison between systems as a basis for understanding their interactions, will address mainly psychological level of sophistication. the mechanisms, and I system side of AI and not comment on the reached a somewhat higher that account for outcomes. Again, in discussing these matters, side, where methodology has perhaps the expert Standard tasks for evaluation To evaluate systems in various earlier that DARPA has used to evaluate progress task domains, we need standard sets of tasks studying different systems. I in that will be used repeatedly by many investigators mentioned the standards speech understanding. association Not only would this create a consensus on benchmarks would also lead to a valuable dialogue on what constitutes good performance: how the criteria depend on the presence or absence of real-time constraints on computation, of computational It could be a very appropriate activity for our professional such standards for a range of areas. for investigators, but it the complexity, and so on. through committees, to establish, performance standards between theories relation and Extraction of general mechanisms and principles interesting such contests in the literature The aim in evaluating systems is not simply to establish which is better at any (as among chess computers) given moment, however trademark, our systems, what we usually may be. Because we name, and thereby report large complex systems, each consisting of a substantial number of interacting mechanisms. After we have than another, or solved them learned faster, we do not yet know the reasons for the superiority. We do not even know in what to accomplish that one system has solved more problems similar or quite different mechanisms are comparisons between they use quite their results. respects The comparison of systems does not end when we have determined which is just the beginning of a performs better on particular kinds of tasks. That that each system employs and the contributions of comparison of the mechanisms these mechanisms to system performance on different kinds of tasks. What we are seeking, as always, are QS laws that can guide the design of the next system, and that can advance theory of intelligence and intelligent systems. the general If, in my account of the past and future of AI research, empirical methods over formal theory, and in the pages of our journals sometimes sacrifices attention to over-simple problems Some distinguished members demeaned in recent years, a kind of theory envy I have emphasized it is because I have sensed, in our meetings that in favor of attention treatment. to exact mathematical to complex but real problems that are amenable of our profession have even challenged and the very concept of experimental computer science. My own scientific record is strewn with examples of mathematical work, some of it relevant to replace mathematics with experiments, or vice versa; it is to secure and maintain a to our deep tolerance to real problems, some of it perhaps not. The issue is not whether for a plurality of approaches our discipline throughout 126 H. A Simon I Artificiul Intelligence 77 (199.5) 95-127 and a dedication to improving each of these approaches in its scientific problems; own terms. Acknowledgments This research was supported Grant No. and by the Defense Advanced Research Projects Agency, Depart- the Air Force Avionics Science Foundation, 3597, monitored by the National ARPA Order by under contract F33615-81-K-1539. Reproduction for any purpose of the United States Government. in whole or in part Approved for public is of Defense, DBS-9121027; ment Laboratory permitted release; distribution unlimited. References Ill I21 131 141 161 I71 (81 MA, (Harvard University representation, for Advanced Press, Cambridge, Intelligence without and J. Fox. Towards and J. von Neumann. a systematic methodology J. Farringdon manuscript. Study, Princeton. NJ, 1946). The Archrrecture of (‘ogmtion Artif. Intell. 47 (19’31) 139-159. Preliminary Discussion of the Logical Design J .R. Anderson, 1083). R.A. Brooks, A.W. Burks, H.H. Goldstine of an Electronic Computing Instrument (Institute R. Cooper. Unpublished P.J. Courtois. Decomposability: Queuing and C‘omputer System Applications New York, 1977). K.A. Ericsson G.W. Ernst. Sufficiency for the success of GPS, G.W. Ernst and A. Newell, GPS: A Case Study in Generality and Problem Solving (Academic Press, New York. 1960). 0. Etzioni, Why PRODIGYiEBL Y22. E.A. Feigenbaum E.A. Feigenbaum and J.A. Feldman, Computers und Thought (McGraw-Hill, and H.A. Simon, EPAM-like models of recognition and H.A. Simon. Protocol Analysis (MIT Press. Cambridge, MA, 1993). in: Proceedings AAAI-90. Boston, MA New York, 1963). learning, Cognitive .I. ACM 16 (1969) 517-533. for cognitive modelling, (1990) 916- (Academic conditions works. Press. and Simon, consensus and H.A. Process models among physicians Performance of Computer J. Math. Psychol. 4 (1967) 246-276. measurement and analysis of certain Science, Carnegie Mellon University, SC;. 8 (1984) 305-336. J. Gaschnig. Department N.B. Giuse et al., Evaluating tion, Methods Inf. Med. 32 (1993) 137-145. L.W. Gregg formation, S. Hanks, M.E. Pollack and J.R. Hayes and H.A. Simon, Understanding Knowledge and Cognition (Erlbaum, W.C. Hood and T.C. Koopmans, and H.A. Simon, C.A. Kaplan R.E. Korf, Toward P. Langley Report. NASA Ames Research Center, Moffett Field, CA (1991). a model of representational and D. Kibler, The experimental Al Mag. 14 (4) (1993) 17-42. written problem the design of agent architectures. and P.R. Cohen, Benchmarks, Potomac. MD, 1974). study of machine stochastic and search algorithms. Ph.D. Thesis, Pittsburgh, in medical knowledge PA (lY79). base construc- theories of simple concept test beds, controlled experimentation. instructions, in: L.W. Gregg, ed., eds., Studies in Econometric Method (Wiley, New York, 1953). In search of insight, Cognitive Psychol. 22 (1990) 374-419. changes, Artif. Intell. 14 (1980) 41-78. learning, Unpublished Tech. 1101 [111 1121 11-11 1141 1151 1161 1171 1181 1191 1201 H.A. Simon I Artificial Intelligence 77 (1995) 95-127 127 [21] P. Langley, H.A. Simon, G.L. Bradshaw and J.M. Zytkow, Scientific Discovery (MIT Press, Cambridge, MA, 1987). [22] D.B. Lenat Italy Milan, and E.A. Feigenbaum, (1987) 1173-1182; [23] R.K. Lindsay, B.G. Buchanan, the thresholds of knowledge, also Artif. InteN. 47 (1991) 185-250. E.A. Feigenbaum and J. Lederberg, On in: Proceedings IJCAI-87, DENDRAL: a case study of the first expert system for scientific hypothesis formation, Artif. Intell. 61 (1993) 209-261. examples by examining (241 D.M. Neves, A computer working Society for Computational Studies of Intelligence, Toronto, Ont. problems (1978) 191-195. program in a textbook, learns algebraic that and in: Proceedings 2nd National Conference of the Canadian procedures [25] A. Newell, Unified Theories of Cognition (Harvard University final report [26] A. Newell systems: Press, Cambridge, MA, 1990). of a study group, Department of Computer et al., Speech understanding Science, Carnegie Mellon University (1971). 1271 A. Newell, A.J. Perlis and H.A. Simon, What is computer science?. Science 157 (1967) 1373-1374. [28] A. Newell and H.A. Simon, Computer science as empirical inquiry: symbols and search, Comm. ACM 19 (1976) 113-126. [29] G.S. Novak, Representation of knowledge in a program for solving physics aproblems, in: Proceedings IJCAI-77, Cambridge, MA (1977) 286-291. [30] D.A. Pomerleau, J. Gowdy and C.E. Thorpe, processing [31] H. Pople, for autonomous Problem solving: robot guidance, an exercise Combining artificial J. Eng. Appl. Artif. Intell. 4 (1991) 961-967. in synthetic in: Proceedings reasoning, networks and IJCAI-77. symbolic Cambridge, MA (1977). [32] M.O. Rabin, Theoretical (1974). impediments to artificial intelligence, in: Proceedings IFIPS Conference [33] A.M. Segre, C. Elkan and A. Russell, Technical of EBL, Mach. Learning 6 (2) (1991) 183-196. note: a critical look at experimental evaluations [34] L. Siklossy, Natural language learning by computer, Representation and Meaning (Prentice-Hall, judging the plausibility [35] H.A. Simon, On Englewood in: H.A. Cliffs, NJ, 1972). Simon and L. Siklossy, eds., of theories, in: Van Roostelaar and Staal, eds., Logic, Methodology and Philosophy of Sciences III (North-Holland, Amsterdam, 1968). [36] H.A. Simon, The Sciences of the Artificial (MIT Press, Cambridge, MA, 2nd ed., 1981). in: K. vanlehn, [37] H.A. Simon, Cognitive architectures rational and ed., comment, analysis: Erlbaum, Hillsdale, NJ, 1991). Architectures Simon [38] H.A. for Intelligence (Lawrence J.B. Kadane, and Optimal problem-solving search: all-or-none solutions, Artif. Intell. 6 (1975) 235-248. [39] H.A. Simon and J. Schaeffer, The game of chess, in: R.J. Aumann and S. Hart, eds., Handbook of Game Theory (Elsevier, Amsterdam, 1992) 1-17. [40] D. Waterman, Generalization learning techniques for automating the learning of heuristics, Artif. Intell. 1 (1970) 120-170. [41] T. Winograd, Understanding Natural Language (Academic Press, New York, 1972). 