Artificial Intelligence 175 (2011) 1290–1307Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDemocratic approximation of lexicographic preference modelsFusun Yaman a,∗, Thomas J. Walsh b, Michael L. Littman c, Marie desJardins da BBN Technologies, 10 Moulton St., Cambridge, MA 02138, USAb University of Arizona, Department of Computer Science, Tucson, AZ 85721, USAc Rutgers University, Department of Computer Science, Piscataway, NJ 08854, USAd University of Maryland Baltimore County, Computer Science and Electrical Engineering Department, Baltimore, MD 21250, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 27 February 2009Received in revised form 5 August 2010Accepted 5 August 2010Available online 2 December 2010Keywords:Lexicographic modelsPreference learningBayesian methods1. IntroductionLexicographic preference models (LPMs) are an intuitive representation that corresponds tomany real-world preferences exhibited by human decision makers. Previous algorithms forlearning LPMs produce a “best guess” LPM that is consistent with the observations. Ourapproach is more democratic: we do not commit to a single LPM. Instead, we approximatethe target using the votes of a collection of consistent LPMs. We present two variations ofthis method—variable voting and model voting—and empirically show that these democraticalgorithms outperform the existing methods. Versions of these democratic algorithms arepresented in both the case where the preferred values of attributes are known and the casewhere they are unknown. We also introduce an intuitive yet powerful form of backgroundknowledge to prune some of the possible LPMs. We demonstrate how this backgroundknowledge can be incorporated into variable and model voting and show that doing soimproves performance significantly, especially when the number of observations is small.© 2010 Elsevier B.V. All rights reserved.Lexicographic preference models (LPMs) are one of the simplest yet most intuitive preference representations. An LPMdefines an order of importance on the variables that describe the objects in a domain and uses this order to make preferencedecisions. For example, the meal preference of a vegetarian with a weak stomach could be represented by an LPM such thata vegetarian dish is always preferred over a non-vegetarian dish, and among vegetarian or non-vegetarian items, mild dishesare preferred to spicy ones.Despite the simplicity of lexicographic LPMs, several studies on human decision making [4,20,9] experimentally demon-strate that humans often make decisions using lexicographic reasoning instead of mathematically more sophisticated meth-ods such as linear additive value maximization [6].Previous work on learning LPMs from a set of preference observations has been limited to autocratic approaches: oneof many possible consistent LPMs is picked heuristically and used for future decisions. However, it is highly likely thatautocratic methods will produce poor approximations of the target when there are few observations.In this paper, we present a democratic approach to LPM learning, which does not commit to a single LPM. Instead,we approximate a target preference using the votes of a collection of consistent LPMs. We present two variations of thismethod: variable voting and model voting. Variable voting operates at the variable level and samples the consistent LPMsimplicitly. The learning algorithm based on variable voting learns a weak order on the variables, such that each linearizationcorresponds to an LPM that is consistent with the observations. Model voting explicitly samples the consistent LPMs and* Corresponding author.E-mail addresses: fusun@bbn.com (F. Yaman), twalsh@cs.arizona.edu (T.J. Walsh), mlittman@cs.rutgers.edu (M.L. Littman), mariedj@cs.umbc.edu(M. desJardins).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.012F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071291employs a weighted vote, where the weights are computed using Bayesian priors. The additional complexity of voting-basedalgorithms (compared to autocratic methods) is tolerable: both algorithms have low-order polynomial time complexity. Ourexperiments show that these democratic algorithms outperform both the average and worst-case performance of the state-of-the-art autocratic algorithm.We also investigate the effect of imperfect data on the learning algorithms. We consider two kinds of imperfections:faulty observations (noise) and hidden ties (ties that are broken arbitrarily). Our empirical evaluation demonstrates that all ofthe algorithms we consider are robust in the presence of hidden ties. However, even a small number of faulty observationssignificantly reduce the performance of the voting algorithms. On the other hand, the greedy algorithm is resilient: that is,the performance decline is proportional to the amount of noise in the data. We take a lesson from this, and adapting thevoting methods to consider the amount of noise in an environment, we empirically show the resulting heuristic is on parwith the greedy approach in the case of noisy observations.To further improve the performance of the learning algorithms when the number of observations is small, we introducean intuitive yet powerful form of background knowledge. The background knowledge defines equivalence classes on thevariables, indicating the most important set of variables, the second most important set, and so on. This representationpermits a user or designer to provide partial information about an LPM (or a class of LPMs) that can be used by the learnerto reduce the search space. We demonstrate how this background knowledge can be used with variable and model votingand show that doing so improves performance significantly, especially when the number of observations is small.In the rest of the paper, we give some background on LPMs (Section 2), then describe our voting-based methods (Sec-tion 3). After introducing these methods in the case where the preferred values of all attributes are known, we presentextensions of these algorithms to the case where preferred values are not known a priori (Section 4). We then introduce ourbackground knowledge representation, show how we can generalize the voting methods to exploit this background knowl-edge (Section 5), present an approach for handling noisy data (Section 6), and present experimental results of this work(Section 7). Finally, we present related work (Section 8) and discuss our future work and conclusions (Section 9).2. Lexicographic preference modelsIn this section, we briefly introduce the lexicographic preference model (LPM) and summarize previous results on learn-ing LPMs. In this work, we only consider binary variables whose domain is {0, 1}.1 For clarity in the introduction of ouralgorithms, we assume for now that the preferred value of each variable is known. This assumption will be removed inSection 4. Without loss of generality, we will assume that 1 is always preferred to 0.Given a set of variables, X = { X1, . . . , Xn}, an object A over X is a vector of the form [x1, . . . , xn]. We use the notationA( Xi) to refer the value of Xi in the object A. A lexicographic preference model L on X is a total order on a subset R of X .We denote this total order with (cid:2)L. Any variable in R is relevant with respect to L; similarly, any variable in I = X − Ris irrelevant with respect to L. If a variable A appears earlier in this total order than B ( A < B), then A is said to be moreimportant or to have a smaller rank than B.If A and B are two objects, then the preferred object given L is determined as follows:• Find the smallest (most important) variable X∗in (cid:2)L such that X∗has different values in A and B. The object that hasthe value 1 for Xis the most preferred.∗• If all relevant variables in L have the same value in A and B, then the objects are equally preferred (a tie).Example 1. Suppose X1 < X2 < X3 is the total order defined by an LPM L, and consider objects A = [1, 0, 1, 1], B =[0, 1, 0, 0], C = [0, 0, 1, 1], and D = [0, 0, 1, 0]. A is preferred over B because A( X1) = 1, and X1 is the most importantvariable in L. B is preferred over C because B( X2) = 1 and both objects have the same value for X1. Finally, C and D areequally preferred because they have the same values for the relevant variables.An observation o = ( A, B) is an ordered pair of objects, connoting that A is preferred to B. In many practical applications,however, preference observations are gathered from demonstration of an expert who breaks ties arbitrarily. That is, whenpresented with a situation in which a decision or choice must be made, if the expert judges the two alternatives to beequally good, the expert will in fact be indifferent, and will therefore be equally likely to choose either alternative. Thus, forsome observations, A and B may actually be tied in the preference order, although we cannot determine this directly fromthe observations. Therefore, an LPM L is said to be consistent with an observation ( A, B) iff L implies that A is preferredto B or that A and B are equally preferred.The problem of learning an LPM is defined as follows. Given a set of observations, find an LPM L that is consistent withthe observations. Previous work on learning LPMs was limited to the case where all variables are relevant. This assumptionentails that, in every observation ( A, B), A is strictly preferred to B, since ties can only happen when there are irrelevantattributes.1 The representation can easily be generalized to monotonic preferences with ordinal variables, such that 1 corresponds to a preference on the values inincreasing order, and 0 to a decreasing order, as shown by Yaman and desJardins [21] for conditional preference networks (CP-nets).1292F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–1307Algorithm 1 greedyPermutationRequire: A set of variables X and a set of observations O .Ensure: An LPM that is consistent with O , if one exists.1: for i = 1, . . . , n do2:Arbitrarily pick one of X j ∈ X such thatMISS( X j, O ) = min Xk∈ X MISS( Xk, O )Rank( X j) := i, assign the rank i to X jRemove X j from XRemove all observations ( A, B) from O such that A( X j) (cid:4)= B( X j)5:6: Return the total order (cid:2) on X such that Xi < X j iff Rank( Xi) < Rank( X j)4:3:The best published algorithm for learning LPMs from observations was presented by Schmitt and Martignon [16],who proposed a greedy variable-permutation algorithm that is guaranteed to find one of the LPMs that is consistentwith the observations,if one exists. They have also shown that for the noisy data case, finding an LPM that doesnot violate more than a constant number of the observations is NP-complete. We use this greedy algorithm, which isshown in Algorithm 1, as a performance baseline. The algorithm refers to a function MISS( Xi, O ), which is defined as|{( A, B) ∈ O : B( Xi) is preferred to A( Xi)}|; that is, the number of observations violated in O if the most important variableis selected as Xi . Basically, the algorithm greedily constructs a total order by choosing the variable at each step that causesthe minimum number of inconsistencies with the observations. If multiple variables have the same minimum, then one ofthem is chosen arbitrarily. The algorithm runs in polynomial time, specifically O (n2m), where n is the number of variablesand m is the number of observations.Dombi et al. [7] have shown that if there are n variables, all of which are relevant, then O (n log n) queries to an oraclesuffice to learn an LPM. Furthermore, it is possible to learn any LPM with O (n2) observations if all pairs differ in only twovariables. They proposed an algorithm that can find the unique LPM induced by the observations. In case of noise due toirrelevant attributes (with ties reported arbitrarily), the algorithm does not return an answer.In the net section, we investigate the following problem: Given a set of observations with no noise, but possibly witharbitrarily broken ties, find a rule for predicting preferences that agrees with the target LPM that produced the observations.Later in the paper, we will relax this assumption to permit noisy data (Section 6).3. Voting algorithmsWe propose a democratic approach for approximating the target LPM that produced a set of observations. Instead offinding just one of the consistent LPMs, it reasons with a collection of LPMs that are consistent with the observations. Giventwo objects, such an approach prefers the one that a majority of its models prefer. A naive implementation of a votingalgorithm would enumerate all LPMs that are consistent with a set of observations. However, since the number of modelsthat are consistent with a set of observations can be exponential, the naive implementation is infeasible.In this section, we describe two methods—variable voting and model voting—that sample the set of consistent LPMs anduse voting to predict the preferred object. Unlike existing algorithms that learn LPMs, these methods do not require allvariables to be relevant or observations to be tie-free. The following subsections explain the variable-voting and model-voting methods and summarize our theoretical results.3.1. Variable votingVariable voting uses a generalization of the LPM representation. Instead of a total order on the variables, variable votingreasons with a weak order2 ((cid:2)) to find the preferred object in a given pair. Among the variables that differ in the twoobjects, the ones that have the smallest rank (and are hence the most salient) in the weak order vote to choose thepreferred object. The object that has the most “1” values for the voting variables is declared to be the preferred one. If thevotes are equal, then the objects are equally preferred.Definition 1 (Variable voting). Suppose X is a set of variables and (cid:2) is a weak order on X . Given two objects, A and B, thevariable-voting process with respect to (cid:2) for determining which of the two objects is preferred is:• Define D to be the set of variables that differ in A and B.• Define D• Define N A to be the number of variables in D∗∗number of variables in Dthat favor B.∗to be the set of variables in D that have the smallest rank among D with respect to (cid:2).• If N A > N B , then A is preferred. If N A < N B , then B is preferred. Otherwise, they are equally preferred.that favor A (i.e., that have value 1 in A and 0 in B) and N B to be the2 A weak order is an asymmetric, reflexive, and transitive order. In other words, a weak order defines an ordering over sets of objects; within each set,the objects are unordered with respect to each other.F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071293Algorithm 2 learnVariableRankRequire: A set of variables X , and a set of observations O .Ensure: A weak order on X .1: Π (x) = 1, ∀x ∈ X2: while Π has changed on the last iteration dofor Every observation ( A, B) ∈ O do3:4:5:6:7:8:9:∗ = {x ∈ D | ∀ y ∈ D, Π (x) (cid:2) Π ( y)}∗ | A(x) = 1}∗ | B(x) = 1}D = {x | A(x) (cid:4)= B(x)}DV A = {x ∈ DV B = {x ∈ DVariableVote predicts a preferred object based on V A > V B .for x ∈ V B such that Π (x) < |X| doΠ (x) = Π (x) + 1;10:11: Return weak order (cid:3) on X such that x (cid:3) y iff Π (x) < Π ( y).Table 1The rank of the variables after each iteration of the for-loop in line 3 of the algorithm learnVariableRank.ObservationsInitially[0, 1, 1, 0, 0], [1, 1, 0, 1, 1][0, 1, 1, 0, 1], [1, 0, 0, 1, 0][1, 0, 1, 0, 0], [0, 0, 1, 1, 1]X11222X21111X31111X41223X51223Example 2. Suppose (cid:2) is the weak order { X2, X3} < { X1} < { X4, X5}. Consider objects A = [0, 1, 1, 0, 0] and B =[0, 0, 1, 0, 1]. D is { X2, X5}. Dis { X2} because X2 is the smallest ranking variable in D with respect to (cid:2). X2 favors Abecause A( X2) = 1. Thus, variable voting with (cid:2) prefers A over B.∗Algorithm 2 presents the algorithm learnVariableRank, which learns a weak order (cid:2) on the variables from a set ofobservations such that variable voting with respect to (cid:2) will correctly predict the preferred objects in the observations.Specifically, it finds weak orders that define equivalence classes on the set of variables. The algorithm maintains the min-imum possible rank for every variable that does not violate an observation with respect to variable voting. Initially, allvariables are considered equally important (rank of 1). The algorithm loops over the set of observations until the ranksconverge. At every iteration and for every pair, variable voting predicts a winner, which allows us to use this algorithm inthe online-learning setting where examples (O ) need to be classified during the learning process. Regardless of this pre-diction, the ranks of the variables that voted for the wrong object are incremented, thus reducing their importance. Finally,the algorithm builds a weak order (cid:2) based on the ranks such that x (cid:2) y if and only if x has a lower rank than y. In theoffline-learning setting (where O is a set of training examples), this weak order can then be given directly to variable votingto classify examples in the test set.Example 3. Suppose X = { X1, X2, X3, X4, X5} and O consists of ([0, 1, 1, 0, 0], [1, 1, 0, 1, 1]), ([0, 1, 1, 0, 1], [1, 0, 0, 1, 0])and ([1, 0, 1, 0, 0], [0, 0, 1, 1, 1]). Table 1 illustrates the ranks of every variable in X after each iteration of the for-loop inline 3 of the algorithm learnVariableRank. The ranks of the variables stay the same during the second iteration of the while-loop; therefore, the loop terminates. The weak order (cid:2) based on ranks of the variables is the same as the order given inExample 2.We now summarize our theoretical results about the algorithm learnVariableRank.Correctness Suppose (cid:2) is a weak order returned by learnVariableRank( X, O ). Any LPM L based on a corresponding topo-logical sort (cid:2)L of (cid:2) will be consistent with the observation set O . This can be proven simply by contradiction. Supposean observation oi existed such that a majority of the variables within an existing class led to an incorrect classificationof oi based on the returned weak order (cid:2). Since the learning algorithm loops over all of the observations, this processwould have resulted in an increment of a value and the algorithm would not have completed with the current (cid:2), thus allLPM consistent with (cid:2) are consistent with O . Furthermore, learnVariableRank never increments the ranks of the relevantvariables beyond their actual rank in the target LPM. This can be seen by considering the cases of both relevant and ir-relevant variables. First, for relevant variables, the number of times a variable is in the set of variables that actually vote(and are therefore potentially incremented) and votes incorrectly, is simply its true rank t. Once it reaches this true rank, bydefinition it cannot vote incorrectly because this variable must vote correctly when all other values are tied (otherwise thiswould not be its true rank). Second, the ranks of the irrelevant variables can be incremented only as far as the number ofvariables, thus the algorithm is guaranteed to terminate, even in the presence of irrelevant variables.1294F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–1307In either the online-learning setting (O being incrementally fed to the learner) or the offline setting (O pro-Convergencevided as a batch), learnVariableRank has a mistake-bound of O (n2), where n is the number of variables. To see this, weconsider two cases for any given observation ot at time t, assuming for ease of exposition that A is preferred to B. First,if V A > V B , then a mistake will not be made, though some variables may have their rank increased anyway (because theywould have voted the wrong way). The second case is where V A < V B , in which case a mistake is made, and thereforewe need to limit the number of such cases. Ignoring any increments in the first case, we see that because each mistakeincreases the sum of the potential ranks by at least 1 and the sum of the ranks the target LPM induces is O (n2), thesecond case can occur no more than O (n2) times. This bound guarantees that given enough observations (as described inthe background section), learnVariableRank will converge to a weak order (cid:2) that, when used in conjunction with variablevoting, consistently classifies all preferred objects with respect to the target LPM. Furthermore, the incrementing of ranks incase 1 gives us the stronger result (mentioned above) that every topological sort of (cid:2) has the same prefix as the total orderinduced by the target LPM. If all variables are relevant, then (cid:2) will converge to the total order induced by the target LPM.Computational complexity We consider the computational complexity of learnVariableRank in the offline-learning settingwhere O is provided as a training set. A loose upper bound on the time complexity of learnVariableRank is O (n3m), wheren is the number of variables and m is the number of observations. This bound holds because the while-loop on line 2 runsat most O (n2) times (because this is the max sum of the maximum possible ranks) and the for-loop in line 3 runs for mobservations (by definition). The time complexity of one iteration of the for-loop is O (n) (since all variables need to beconsidered in the worst case); therefore, the overall complexity is O (n3m). We leave the investigation of tighter bounds,improved data structures, and the average case analysis for future work.3.2. Model votingThe second method we present employs a Bayesian approach. This method randomly generates a sample set, S, ofdistinct LPMs that are consistent with the observations. When a pair of objects is presented, the preferred one is predictedusing weighted voting. That is, each L ∈ S casts a vote for the object it prefers, and this vote is weighted according to itsposterior probability P (L|S).Definition 2 (Model voting). Let U be the set of all LPMs, O be a set of observations, and S ⊂ U be a set of LPMs that areconsistent with O . Given two objects A and B, model voting prefers A over B with respect to S if(cid:2)(cid:2)P (L|S)VL( A>B) >P (L|S)VL(B> A),(1)L∈UL( A>B) is 1 if A is preferred with respect to L, and 0 otherwise. Vwhere Vposterior probability of L being the target LPM given S, calculated as discussed below.L∈UL(B> A) is defined analogously. P (L|S) is theWe first assume that all LPMs are equally likely a priori. In this case, given a sample of LPMs S of size k, the posteriorprobability of an LPM L will be 1/k if and only if L ∈ S, and 0 otherwise. Note that when S is maximal, this case degener-ates into the naive voting algorithm. However, it is generally not feasible to enumerate all consistent LPMs—in practice, thesample has to be small enough to be feasible and large enough to be representative.In constructing S, we exploit the fact that many consistent LPMs share prefixes in the total order that they define on thevariables. We wish to discover and compactly represent such LPMs. To this end, we introduce the idea of aggregated LPMs. Anaggregated LPM, ( X1, X2, . . . , Xk, ∗), represents a set of LPMs that define a total order with the prefix X1 < X2 < · · · < Xk.Intuitively, an aggregated LPM states that any possible completion of the prefix is consistent with the observations. Thealgorithm sampleModels in Algorithm 3 implements a “smart sampling” approach by constructing an LPM that is consistentwith the given observations, returning an aggregated LPM when possible. We start with an arbitrary consistent LPM (such asthe empty set, which is always consistent) and add more variable orderings extending the input LPM. We first identify thevariables that can be used in extending the prefix—that is, all variables Xi such that in every observation, either Xi is 1 inthe preferred object or Xi is the same in both objects. We then select one of those variables randomly and extend the prefix.Finally, we remove the observations that are explained with this selection and continue with the rest of the observations. Ifat any point, no observations remain, then we return the aggregated form of the prefix, since every completion of the prefixwill be consistent with the null observation. Running sampleModels several times and eliminating duplicates will produce aset of (possibly aggregated) LPMs.Example 4. Consider the same set of observations O as in Example 3. Then, the aggregated LPMs that are consistentwith O are as follows: (), ( X2), ( X2, X3), ( X2, X3, X1, ∗), ( X3), ( X3, X1, ∗), ( X3, X2) and ( X3, X2, X1, ∗). To illustrate theset of LPMs that an aggregate LPM represents, consider ( X2, X3, X1, ∗), which has a total of 5 extensions: ( X2, X3, X1),( X2, X3, X1, X4), ( X2, X3, X1, X5), ( X2, X3, X1, X4, X5), ( X2, X3, X1, X5, X4). Every time the algorithm sampleModels runs onthe set of observations O from Example 3, it will randomly generate one of the aggregated LPMs: ( X2, X3, X1, ∗), ( X3, X1, ∗),or ( X3, X2, X1, ∗). Note that the shorter models that are not produced by sampleModels are all sub-prefixes of the aggregatedLPMs and it is easy to modify sampleModels to return those models as well.F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071295Algorithm 3 sampleModelsRequire: A set of variables X , a set of observations O , and rulePrefix, an LPM to be extended.Ensure: An LPM (possibly aggregated) consistent with O .1: candidates is the set of variables {Y : Y /∈ rulePrefix | ∀( A, B) ∈ O , A(Y ) = 1 or A(Y ) = B(Y )}.2: while candidates (cid:4)= ∅ do3:if O = ∅ thenreturn (rulePrefix, ∗).4:5:6:7:Randomly remove a variable Z from candidates.Remove any observation (C, D) from O such that C( Z ) (cid:4)= D( Z ).Extend rulePrefix: rulePrefix = (rulePrefix, Z ).Recompute candidates.8:9: return rulePrefixTable 2The posterior probabilities and number of votes of all LPMs in Example 5.LPMs( )( X2)( X2, X3)( X2, X3, X1, ∗)( X3)( X3, X1, ∗)( X3, X2)( X3, X2, X1, ∗)P (L|S 1)1/311/311/315/311/3116/311/315/31P (L|S 2)0005/26016/2605/26NLA>B01150715NLB> A00000700An aggregate LPM in a sample saves us from having to enumerate all possible extensions of a prefix, but it also introducescomplications in computing the weights (posteriors) of the LPMs, as well as their votes. For example, when comparing twoobjects A and B, some extensions of an aggregate LPM might vote for A and some for B. Thus, we need to find the totalnumber of LPMs that an aggregate LPM represents and determine what proportion of them favor A over B (or vice versa),without enumerating all extensions. Suppose there are n variables and L is an aggregated LPM with a prefix of length k.Then the number of extensions of L is denoted by FL and is equal to fn−k, where fm is defined to befm =m(cid:2)i=0(cid:3)(cid:4)mi× i! =m(cid:2)i=0(m)!(m − i)!.(2)Intuitively,fm counts every possible permutation with at most m items. Note that fm can be computed efficiently andthat the number of all possible LPMs when there are n variables is given by fn.While the above formula calculates the total number of extensions, we still need to determine how many votes anaggregate LPM L = ( X1, X2, . . . , Xk, ∗) will allocate to each of two compared objects A and B. We will call the variablesX1, . . . , Xk the prefix variables. If A and B have different values for at least one prefix variable, then all extensions willvote in accordance with the smallest such variable. Suppose all prefix variables are tied and m is the set of all non-prefixvariables. Then m is composed of three disjoint sets a, b, and w, such that a is the set of variables that favor A, b is the setof variables that favor B, and w is the set of variables that are neutral (that is, that have the same value in A and B).An extension L(cid:8)of L will produce a tie iff all variables in a and b are irrelevant in L(cid:8). The number of such extensionsis f|w|. The number of extensions that favor A over B is directly proportional to |a|/(|a| + |b|). Therefore, the number ofLextensions of L that will vote for A over B (denoted by NA>B ) isNLA>B=|a||b| + |a|× ( fm − f|w|).The number of extensions of L that will vote for B over A is computed similarly. Note that the computation of Nand FL can be done in linear time by caching the recurring values.(3)LA>B , NLB> A ,Example 5. Suppose X and O are as defined in Example 3. The first column of Table 2 lists all LPMs that are con-sistent with O . The second column gives the posterior probabilities of these models given the sample S 1, which isthe set of all consistent LPMs. The third column is the posterior probability of the models given the sample S 2 ={( X2, X3, X1, ∗), ( X3, X1, ∗), ( X3, X2, X1, ∗)}. Given two objects A = [0, 1, 1, 0, 0] and B = [0, 0, 1, 0, 1], the number of votesfor each object based on each LPM is given in the last two columns. Note that the total number of votes for A and B doesnot add up to the total number of extensions of ( X3, X1, ∗) because two of its extensions—( X3, X1) and ( X3, X1, X4)—preferA and B equally.Algorithm 4 describes modelVote, which takes a sample of consistent LPMs (produced, for instance, by sampleModels) anda pair of objects as input, and predicts the preferred object using the weighted votes of the LPMs in the sample.1296F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–1307Algorithm 4 modelVoteRequire: A set of LPMs, S, and two objects, A and B.Ensure: Returns either one of A or B or tie.1: Initialize sampleSize to the number of non-aggregated LPMs in S.2: for every aggregated LPM L ∈ S dosampleSize+ = F L.3:4: Vote( A) = 0; Vote(B) = 0;5: for every LPM L ∈ S do6:if L is not an aggregate rule then7:8:9:10:11:12:13:14:15:winner is the object that L prefers among A and B.Increment Vote(winner) by 1/sampleSize.elseif A and B differ in at least one prefix variable of L thenprefers among A and BL∗is any extension of Lwinner is the object that L∗Vote(winner)+ = F L/sampleSize.elseVote( A)+ = N LVote(B)+ = N LA>B /sampleSize.B> A/sampleSize.16:17: if Vote( A) = Vote(B) then18:19: else20:Return a tieReturn the object obj with the highest Vote(obj).Returning to Example 5, the reader can verify that model voting will prefer A over B. Next, we present our theoreticalresults on the sampleModels and modelVote algorithms.Complexity The time complexity of sampleModels is bounded by O (n2m), where n is the number of variables and m is thenumber of observations: the while-loop in line 2 runs at most n times (the worst case is that each variable needs to beremoved one at a time from candidates). At each iteration, we have to process every observation, each time performingcomputations in O (n) time. If we call sampleModels s times (to generate a sample of size s) then the total complexity ofsampling is O (sn2m). For constant s, or with s bounded by a polynomial function of the other relevant quantities (n and m),this bound is still polynomial. Similarly, the complexity of modelVote is O (sn) because it considers each of the s rules in thesample, counting the votes of each rule, which can be done in O (n) time.Comparison to variable voting The set of LPMs that is sampled via learnVariableRank is a subset of the LPMs that sampleMod-els can produce and there are cases where this relationship is strict (models(learnVariableRank) ⊂ models(sampleModels)).For inclusion, we see that sampleModels without aggregates considers all possible models (since every variable is consideredin every location in the LPM recursively). Thus, the LPMs consistent with the weak order returned by learnVariableRankmust be a subset of these models. The strictness can be shown with the running example in the paper demonstrates thatsampleModels can generate the LPM ( X3, X1, ∗); however, none of its extensions is consistent with the weak order returnedby learnVariableRank.4. Learning preferred attribute valuesIn the previous sections, we assumed that the preferred value for each binary variable was known, so only the order ofimportance on the variables needed to be learned. In this section, we generalize the definition of an LPM to explicitly statethe preferred value for each relevant variable. The motivation for this generalization is that the preferred value of a variableis not always known a priori. For example, in a meal preference learning situation, different groups of people might preferdifferent values of the “spicy” variable.To represent this larger model space, we will use a pair of literals to represent each variable, similar to the trick usedin learning Boolean formulae over binary variables. There are two literals l based on a variable X : the variable X (positiveliteral) and its negation ¬ X (negative literal). Given a set of variables V , let L(V ) be the set of all literals based on variablesin V . A generalized LPM L on L(V ) is a total order on a subset R of L(V ) such that R does not contain both a positive anda negative literal based on the same variable. If A and B are two objects, then the preferred object given L is determinedas follows:• Find the smallest literal l in L such that if X is the variable l is based on, then A( X) and B( X) have different values. Ifl is a positive literal, then the object that has the value 1 for X is preferred; otherwise, the object that has the value 0for X is preferred.• If all relevant variables in L have the same value in A and B, then the objects are equally preferred (a tie).F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071297Algorithm 5 genLearnVariableRankRequire: A set of variables X , and a set of observations O .Ensure: A weak order on literals based on variables in X .1: Π (x) = 1 and Π (¬x) = 1, ∀x ∈ X2: while Π can change do3:for every observation ( A, B) ∈ O do4:5:6:7:8:∗ = {x ∈ D | ∀ y ∈ D, Π (x) (cid:2) Π ( y)}D is the set of literals based on variables that differ in A and BDV A is the set of positive (negative) literals in DV B is the set of positive (negative) literals in Dfor x ∈ V B such that Π (x) < |X| + 1 dothat are 1 (0) in A.that are 1 (0) in B.∗∗Π (x) = Π (x) + 1;9:10: Return weak order (cid:3) on X such that x (cid:3) y iff Π (x) < Π ( y).Example 6. Suppose ¬ X1 < X2 < X3 is the total order defined by a generalized LPM L, and consider objects A = [1, 0, 1, 1],B = [0, 1, 0, 0] and C = [0, 0, 1, 1]. B is preferred over A because B( X1) = 0, and ¬ X1 is the most important literal in L.B is preferred over C because B( X2) = 1 and both objects have the same value for X1.Next we will adapt the voting algorithms described in previous sections to learn generalized LPMs.4.1. Generalized variable votingWe can adapt the definition of variable voting (Definition 1) in a similar way to the LPM generalization above. Essentially,we need to define the weak order (cid:2) over a set of literals instead of a set of variables. We also need to modify the way wecount votes (N A and N B ), such that among the voting literals, positive (negative) literals vote for the object that has 1 (0)for the variable the literal is based on. To avoid repetition, we will not formally define generalized variable voting, but thefollowing example demonstrates the new vote-counting procedure.Example 7. Suppose that (cid:2) is the weak order { X2, X3} (cid:2) { X1, ¬ X2, ¬ X3, X4, ¬ X5} (cid:2) {¬ X1, ¬ X4, X5}. Consider objects A =[1, 0, 1, 1, 0] and B = [0, 0, 1, 0, 1]. The literals based on variables that are different in A and B are D = { X1, ¬ X1, X4,¬ X4, X5, ¬ X5}. The literals that get to vote are X1, X4, and ¬ X5 since they are the smallest ranking variables in D withrespect to (cid:2). X1 votes for A because X1 is a positive literal and A( X1) = 1. Similarly, X4 votes for A. ¬ X5 votes for Bbecause it is a negative literal and B( X5) = 0. Therefore, variable voting with (cid:2) prefers A over B.As the previous example demonstrates, a literal votes only if its complement does not have a smaller rank. Furthermore,if both a literal and its complement have the same ranking, then their votes will cancel each other out and will not affectthe preference decision. We note that in either case, since both the positive and negative literals appear in the variableranking, special care must be taken when constructing an LPM from this weak ordering, a topic we return to at the end ofthis section.Algorithm 5 presents the algorithm genLearnVariableRank. Given a set of observations, this algorithm learns the rankingof each literal and returns a weak order (cid:2) on a subset of the literals. Generalized variable voting (as outlined above)with respect to (cid:2) will correctly predict the preferred objects in the observations. The genLearnVariableRank algorithm isvery similar to learnVariableRank; the major difference is that in genLearnVariableRank, the ranking function Π is over allpossible literals and is updated when the prediction was wrong or correct but not unanimous. The rank of a literal is notincremented beyond one more than the number of variables.Example 8. Suppose X = { X1, X2, X3, X4, X5} and O consists of ([0, 1, 1, 0, 0], [1, 1, 0, 1, 1]), ([0, 1, 1, 0, 1], [1, 0, 0, 1, 0]) and([1, 0, 1, 1, 0], [0, 0, 1, 0, 1]). Table 3 illustrates the ranks of every literal based on variables in X after each iteration of thefor-loop in line 3 of the algorithm genLearnVariableRank. The while-loop in line 2 of the algorithm terminates after two iter-ations. The algorithm genLearnVariableRank returns the weak order { X2, X3} (cid:2) { X1, ¬ X2, ¬ X3, X4, ¬ X5} (cid:2) {¬ X1, ¬ X4, X5}.The asymptotic bounds for the complexity and convergence of learnVariableRank also hold for genLearnVariableRank.However, for correctness, the relationship between the weak order (cid:2) returned by genLearnVariableRank and the generalizedLPMs consistent with the observations needs to be revised. Specifically, a topological sort of (cid:2) will not be a valid generalizedLPM because it will contain literals that are negations of each other. To correct this problem, we can simply discard anyliteral that has a higher rank than its opposite; if a pair of such literals appears in the same bin, we can discard them both.A topological sort can then be performed on the resulting weak order to produce a generalized LPM L. Any LPM produced insuch a manner is consistent with O because the discarded variables could never have been used by the learning algorithmto make, or even influence, a prediction.1298F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–1307Table 3The rank of the literals after each iteration of the for-loop in line 3 of the algorithm genLearnVariableRank.ObservationsX1X2X3X4X5¬ X1¬ X2¬ X3¬ X4¬ X5Initially[0, 1, 1, 0, 0], [1, 1, 0, 1, 1][0, 1, 1, 0, 1], [1, 0, 0, 1, 0][1, 0, 1, 1, 0], [0, 0, 1, 0, 1][0, 1, 1, 0, 0], [1, 1, 0, 1, 1][0, 1, 1, 0, 1], [1, 0, 0, 1, 0][1, 0, 1, 1, 0], [0, 0, 1, 0, 1]1222222111111111111111222222122222311122231122222122222211122231122222Algorithm 6 genSampleModelsRequire: A set of variables X , a set of observations O , and rulePrefix, an LPM to be extended.Ensure: An LPM (possibly aggregated) consistent with O .is the set of positive literals {Y : Y , ¬Y /∈ rulePrefix | ∀( A, B) ∈ O , A(Y ) = 1 or A(Y ) = B(Y )}.is the set of negative literals {¬Y : Y , ¬Y /∈ rulePrefix | ∀( A, B) ∈ O , A(Y ) = 0 or A(Y ) = B(Y )}.+−1: candidates2: candidates3: candidates = candidates4: while candidates (cid:4)= ∅ do5:if O = ∅ thenreturn (rulePrefix, ∗).+ ∪ candidates−6:7:8:9:Randomly remove a variable Z from candidates.Remove any observation (C, D) from O such that C( Z ) (cid:4)= D( Z ).Extend rulePrefix: rulePrefix = (rulePrefix, Z ).Recompute candidates.10:11: return rulePrefix4.2. Generalized model votingThe modifications to model voting can be considered in two parts. First, we need to extend the algorithm sampleModelsto produce generalized LPMs. Algorithm 6 presents the algorithm genSampleModels, which operates on the level of literalsand returns (possibly aggregated) generalized LPMs. Similar to sampleModels, a positive literal X is considered as a candidatefor rule extension only if in every observation, either X is 1 in the preferred object or is the same in both objects. A negativeliteral ¬ X is a candidate when in every observation, either X is 0 in the preferred object or is the same in both objects.Note that to produce a valid LPM, we also need to ensure that the prefix contains at most one literal based on the samevariable.Second, we need to generalize the counting of model extensions for aggregate LPMs and the distribution of votes whencomparing two objects. The number of extensions of L is denoted by FL and is equal to fn−k, where n is the set of variablesthe literals are based on, k is the length of the prefix in L, and fm is redefined asfm =m(cid:2)(cid:4)(cid:3)mii=0× i! × 2i =m(cid:2)i=0m! × 2i(m − i)!.(4)The new definition ofevery combination of literals (positive or negative) for each variable.fm has an extra 2i term inside the summation because the extensions of i fixed variables includeNow consider a pair of objects, A and B, and an aggregated generalized LPM L. As before, if A and B have differentvalues for at least one prefix literal in L, then all extensions will vote in accordance with the smallest such literal. However,if all prefix variables are tied, then in generalized model voting, the votes will be divided equally because there is anequal number of extensions with positive and negative literals based on the rest of the variables. Thus, the algorithm forgeneralized model voting will be the same as modelVote, except that the first line will call the algorithm genSampleModelsand lines 14 to 16 (which compute the distribution of votes for aggregate LPMs when the prefix variables are tied) will bedeleted.5. Introducing background knowledgeIn general, when there are not many training examples for a learning algorithm, the space of consistent LPMs is large. Inthis case, it is not possible to find a good approximation of the target model. To overcome this problem, we can introducebackground knowledge, indicating that certain solutions should be favored over the others. In this section, we propose aform of background knowledge consisting of equivalence classes over the set of attributes. These equivalence classes indicatethe set of most important attributes, second most important attributes, and so on. For example, when buying a used car,most people consider the most important attributes of a car to be the mileage, the year, and the make of the car. Thesecond most important set of attributes is the color, number of doors, and body type. Finally, perhaps the least importantproperties are the interior color and the wheel covers. Throughout this section, we assume that the preferred value of aF. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071299variable is known or is given in the background knowledge. We now formally define our representation for backgroundknowledge and what it means for an LPM to be consistent with the background knowledge.Definition 3 (Background knowledge). The background knowledge B for learning a lexicographic preference model on a set ofvariables X is a weak order: that is, a total order on a partition of X . B has the form E 1 < E2 < · · · < Ek, wherei E i = X .B defines a weak order on X such that for any two variables x ∈ E i and y ∈ E j , x < y iff E i < E j . We denote this weakorder by (cid:2)B .(cid:5)Definition 4. Suppose that X = { X1, . . . , Xn} is a set of variables, B the background knowledge, and L an LPM. L is consis-tent with B iff the total order (cid:2)L is consistent with the weak order (cid:2)B.Intuitively, an LPM that is consistent with background knowledge B respects the variable orderings induced by B. Thebackground knowledge prunes the space of possible LPMs. The size of the partition determines the strength of B; forexample, if there is a single variable per set, then B defines a specific LPM. In general, the number of LPMs that is consistentwith background knowledge of the form E 1 < E2 < · · · < Ek can be computed with the following recursive formula:(cid:7)(cid:6)(cid:6)(cid:6)(cid:7)[e1, . . . , ek]G= f e1+ e1! ×G(cid:7)[e2, . . . , ek]− 1,(5)where ei = |E i| and the base case for the recursion is G([ ]) = 1. The first term in the formula counts the number of possibleLPMs using only the variables in E1, which are the most important variables. The definition of consistency entails that avariable can appear in (cid:2)L iff all of the more important variables are already in (cid:2)L, hence the term e1!. Note that therecursion on G is limited to the number of sets in the partition, which is bounded by the number of variables; therefore, itcan also be computed in linear time by caching precomputed values of f .To illustrate the potential power of background knowledge, consider a learning problem with nine variables. Withoutbackground knowledge, the total number of LPMs is 905,970. If the background knowledge B partitions the variables intothree sets, each with three elements, then the number of LPMs consistent with B is only 646. If B has four sets, where thefirst set has three variables and the rest have two, limits the number to 190.We can easily generalize the learnVariableRank algorithm to utilize background knowledge, by changing only the first lineof learnVariableRank, which initializes the ranks of the variables. Given background knowledge of the form S 1 < · · · < Sk, thegeneralized algorithm assigns the rank 1 (most important rank) to the variables in S 1, rank |S1| + 1 to those in S2, and soforth. This initialization ensures that an observation ( A, B) is used for learning the order of variables in a class S i only whenA and B have the same values for all variables in classes S1, . . . , S i−1 and have different values for at least one variable inS i .The algorithm modelVote can also be generalized to use background knowledge B. In the sample generation phase, weuse sampleModels as presented earlier, and then eliminate all rules whose prefixes are not consistent with B. Note thateven if the prefix of an aggregated LPM L is consistent with B, this may not be the case for every extension of L. Thus,L,Bin the algorithm modelVote, we need to change any references to FL and NB< A ),respectively, where:LB< A ) with FL,BA<B (or NLA<B (or NBL and N• F• NBL is the number of extensions of L that are consistent with B, andL,BA<B is the number of extensions of L that are consistent with B and prefer A. (NL,BB< A is analogous.)Suppose that B is given as E1 < · · · < Em. Let Y denote the prefix variables of an aggregate LPM L and let Ek be thefirst set such that at least one variable in Ek is not in Y . Then, FBL = G([|Ek − Y |, |Ek+1 − Y |, . . . , |Em − Y |]).When counting the number of extensions of L that are consistent with B and prefer A, we again need to examinethe case where the prefix variables equally prefer the objects. Suppose Y is as defined as above and D i denotes the setdifference between E i and Y . Let D j be the first non-empty set and Dk be the first set such that at least one variable inDk has different values in the two objects. Obviously, only the variables in Dk will influence the prediction of the preferredobject. If• di = |D i|, the cardinality of D i , and• a is the set of variables in Dk that favor A, b is the set of variables in Dk that favor B, and w is the set of variables inDk that are neutral,then NL,BA>B , the number of extensions of L that are consistent with B and prefer A, can be computed as follows:NL,BA>B=|a||a| + |b|(cid:6)×BL − GF(cid:6)(cid:8)d j . . . dk−1, |w|(cid:9)(cid:7)(cid:7).(6)1300F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13076. Handling noisy dataAlthough inferring an LPM from noisy data is NP-complete [16], the moderate empirical success of the greedy algorithm(as seen in Section 7.7) give us an intuition as to how a heuristic solution with voting can be developed. Specifically, thegreedy approach iteratively constructed an LPM where each added attribute violated the fewest number of observations. Wecan borrow this intuition to build heuristic extensions of learnVariableRank and modelVote. If the expected number of noisyobservations (cid:3) in a data set is provided, then the new algorithm, Noise-Aware Model Vote (NAMV), changes sampleModelsto only consider a variable as a candidate if adding it will not violate more (in total with the other variables) than (cid:3)observations. Notice that this remains a stochastic LPM construction and can still consider many more LPMs than thegreedy approach. Following a similar path, we can develop a noise-aware version of learnVariableRank in which the ranks ofthe variables are not updated unless at least (cid:3) observations are mispredicted.If we do not know the expected amount of noise in our observations, then we can employ a hybrid approach betweengreedyPermutation and modelVote, which we call greedyVote, by simply replacing the sampling algorithm sampleModels withgreedyPermutation. In doing so, we will be losing the advantage of aggregate LPMs (since greedyPermutation produces asingle LPM) and we will be confining our samples to the ones that can only be generated by the “make minimum mistakes”heuristic. We will investigate the resilience of both extensions empirically in Section 7.7.7. ExperimentsIn this section, we explain our experimental methodology and discuss the results of our empirical evaluations. We definethe prediction performance of an algorithm P with respect to a set of test observations T asperformance(P , T ) = Correct(P , T ) + 0.5 × Tie(P , T )|T |,(7)where Correct(P , T ) is the number of observations in T that are predicted correctly by P (including any prediction for t ∈ Twhere t is actually a tie) and Tie(P , T ) is the number of observations in T that P predicted as a tie when one object shouldactually have been preferred over the other. Note that an LPM returned by greedyPermutation never returns a tie. In contrast,variable voting with respect to a weak order in which every variable is equally important will only return ties, so the overallperformance will be 0.5, which is no better than randomly selecting the preferred objects. We will use MV, VV, and G todenote the model voting, variable voting, and the greedy approximations of an LPM. Similarly we will use NAMV and GV todenote Noise-Aware Model Vote and greedyVote.Given sets of training and test observations, (O , T ), we measure the average and worst performances of VV, MV and G.When combined with learnVariableRank, VV is a deterministic algorithm, so the average and worst performances of VV arethe same. However, this is not the case for MV with sampling, because sampleModels is randomized. Even for the sametraining and test data (O , T ), the performance of MV can vary. To mitigate this effect, we ran MV 10 times for each (O , T )pair, and called sampleModels S times on each run (thus the sample size is at most S), recording the average and worst ofits performance. The greedy algorithm G is also randomized (in line 2, one variable is picked arbitrarily), so we ran G 200times for every (O , T ), recording its average and worst performance. In all of the figures below, the data points are averagesover 20 runs with different (O , T ). We employed two-tailed T-test with 95% confidence interval to test significance. In ourdiscussion of the results, when applicable, we note the statistically significant differences.For our experiments, the control variables are R, the number of relevant variables in the target LPM; I , the number ofirrelevant variables; N O , the number of training observations; and N T , the number of test observations. For MV experiments,the sample size (S) is also a control parameter. For fixed values of R and I , an LPM L is randomly generated. (If backgroundknowledge B is given, then L is also consistent with B.) Unless otherwise noted (as in Section 7.5), the preferred value ofeach attribute is 1; otherwise, the preferred value of each attribute is chosen randomly. We randomly generated N O and N Tpairs of objects, each with I + R variables. Finally, we labeled the preferred objects according to L. In order to allow otherresearchers replicate our results, we posted the data files and the scripts that we used for data generation on the web athttp://maple.cs.umbc.edu/LPM.7.1. Comparison of MV, VV and GFig. 1(a) shows the average performance of G; MV with sample size S = 200; and VV for R = 15, I = 0, and N T = 20,as N O ranges from 2 to 20. Fig. 1(b) shows the worst performance for each algorithm. In these figures, the data points areaverages over 20 different pairs of training and test sets (O , T ). The average performance of VV and MV is better than theaverage performance of G, and the difference is significant at every data point. Also, note that the worst-case performanceof G after seeing two observations is around 0.3, which suggests a very poor approximation of the target. VV and MV’sworst-case performance are much better than the worst-case performance of G, justifying the additional complexity of thealgorithms MV and VV.F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071301Fig. 1. (a) Average prediction performance and (b) worst prediction performance of the greedy algorithm, variable voting, and model voting.Fig. 2. The average and worst case prediction performance of greedyVote compared to model voting, variable voting, and greedy approaches for noise-freedata.7.2. Greedy votingEven though we proposed greedyVote as a noise-aware adaptation of model vote we also investigated its performance forthe noise-free case. For this experiment, we used the same data set and control variables explained in 7.1. In addition, thesample size for GV is set to 200 as it is for MV. Fig. 2 contains the average performance of MV, VV and G and as well as theworst performance of MV which were already reported in Fig. 1(a) and (b). Fig. 2 demonstrates the average and worst caseperformance of GV. Just like MV, GV is a randomized algorithm, thus for each data set we ran GV ten times and used theaverage of ten runs as the prediction performance. An interesting result is GV’s average performance is very close to VV’sperformance and its worst case performance is almost same as the average performance of G. Therefore, by virtue of beinga voting-based algorithm, GV demonstrates a much better worst case performance than the greedy algorithm; however, theworst case performance is still significantly worse than MV and VV. We believe that this behavior occurs because GV usesthe greedy approach for sampling the space of consistent LPMs, as reflected in the tight overlap between worst GV andaverage G performances.1302F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–1307Fig. 3. (Left) The average prediction performance and (right) the worst prediction performance of model voting for different sample sizes.7.3. Effect of sample size on MV performanceFig. 3 shows the worst and average prediction performance of MV with sample sizes S = 10, S = 50, S = 200 andS = 1200 for problems with 10 relevant variables (R = 10) and no irrelevant variables (I = 0). The number of observations(No) increases from 2 to 20 along the x-axis. In general, as the sample size increases, the prediction performance increases.The effect of sample size on worst-case performance is more evident than the average performance.7.4. Irrelevant variablesIrrelevant variables hamper the pruning of the space of possible LPMs. Of the two voting-based algorithms, MV has theability to ignore some of the variables early on if sampleModels produces LPMs that do not use all of the variables. VV, onthe other hand, operates with the entire set of variables, although given enough observations, it eventually discovers theirrelevant ones. In our experiments, we compared the average performance of MV and VV for cases where the total numberof attributes were constant and the number of irrelevant attributes varied. Our results showed that both algorithms arerobust in the presence of irrelevant variables. Furthermore, for several test cases where the number of irrelevant variablesdominated the number of relevant variables, or when the total number of attributes was small, the performance of bothalgorithms improved over the case where only relevant variables appeared. That is, all things being equal, the algorithmsfound it easier to learn to ignore an irrelevant variable than to use a relevant one.Fig. 4 depicts two such cases. The graph on the left shows the average prediction performance of MV and VV for R = 2,I = 3 and R = 5, I = 0. The data sets with fewer relevant variables are learned more easily. The right shows the results ofthe same comparison and the same pattern of results with more attributes: R = 3, I = 12 and R = 15, I = 0. We have seenthis pattern in several other similar situations. Note that the graphs show model voting outperforming variable voting, butwe have not observed this difference to be statistically significant. Our experiments indicate that both algorithms are robustin the presence of irrelevant variables, still achieving high accuracy values with relatively few samples.7.5. Learning preferred variable valuesWe implemented the generalized versions of MV and VV, which we will refer as gMV and gVV, respectively. We testedthe performance of gMV and gVV using two different data sets. The first data set is the same as the one used for comparingMV and VV where R = 15 and I = 0 and for all variables 1 is always preferred over 0. Fig. 5(a) shows the average andworst performance of MV and VV. A quick comparison to Fig. 1 reveals the decreased prediction performance (which isstatistically significant) in all three. This was an expected result, given the increase in the search space. The second data sethas R = 5 and I = 10 and the preferred value of variables are chosen randomly. Fig. 5(b) demonstrates the performance ofMV and VV in a very similar pattern to Fig. 5(a).7.6. Effect of hidden tiesFig. 6 shows the prediction performance of VV and G (average and worst case) for problems with 10 relevant variables(R = 10) and five irrelevant variables (I = 5). The number of observations (No) is always 50, but the number of theseobservations that are hidden ties varies from 0 to 45 (x-axis).F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071303Fig. 4. Average MV performance and VV performance for different numbers of relevant and irrelevant variables.Fig. 5. (a) Comparison of gMV and gVV for a data set with 15 relevant variables and no irrelevant variables, where 1 is always the preferred value.(b) Comparison of gMV and gVV for a data set with 5 relevant and 10 irrelevant variables, where the preferred value of each attribute is chosen randomly.In general, as the number of hidden ties increase in the observations, the prediction performance degrades. This resultwas expected because the number of useful observations that can help the algorithms learn the ranking on the relevantattributes decreases as the number of hidden ties increases. The performance of MV on the same data sets was very similarto VV and has thus been omitted for readability.A more interesting result, however, is that the existence of hidden ties in the data actually improves the performance ofboth algorithms, compared to a smaller dataset with the hidden tie observations omitted (the “filtered” versions in Fig. 6).Our explanation for this phenomenon is that although hidden ties do not provide useful information for learning the orderon the relevant attributes, their existence helps the algorithms identify the irrelevant attributes (because hidden ties canincrease the number of mistakes that would be caused only by the irrelevant attributes) and push them further up in theranking (decreasing their importance), thus allowing the other observations to clarify the ordering of the identified relevantattributes.1304F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–1307Fig. 6. The prediction performance of VV and G for varying number of hidden ties in 50 observations. The results for filtered VV and G are obtained byeliminating the hidden ties from the observations.7.7. Effect of noiseFig. 7 shows the average prediction performance of MV and G for problems with 10 relevant variables (R = 10) and fiveirrelevant variables (I = 5). The total number of observations (No) is always 50 but the number of these observations thatare faulty varies from 0 to 45 (x-axis). Fig. 8 shows the worst performance for the same setting.The results show that both the average and the worst performance of MV (which operates under the assumption ofnoise-free data) are significantly compromised by even small amounts of noise, which it interprets as a refutation of thecorrect model (as well as many of the “almost correct” models). The asymptotic performance at 0.5 reflects the fact thatthis noise causes MV to eliminate all of the models from its version space, causing it to predict a tie for every testingobservation (essentially making it a random selection algorithm). We omitted the results for VV from the figure since VV’sbehavior closely resembled that of MV. The performance of G decays far more gracefully than MV or VV because G allowsfor some of the observations to be discarded. In Figs. 7 and 8, we compare the average and worst performance of NAMVand GV to the other algorithms presented in this paper, including “filtered” versions where the noisy data was omitted(which provided as baselines). Notice that unlike the original modelVote, which was confounded by even a small amount ofnoise, the gentle decays of NAMV and GV mirror the greedy approach’s robustness to noise and perform comparably on thisdata set. Asymptotically, if all of the observations are noisy, NAMV still performs better than modelVote in the average case(a statistically significant result), because it does not eliminate all of the possible models, so instead of defaulting to randomselection, it favors the test observation with the most 1s. Among the two noise adaptations of MV, GV demonstrates thebest worst-case performance (for noise levels 10 to 25 the difference between GV and NAMV is statistically significant).7.8. Effect of background knowledge on performanceFig. 9 shows the positive effect of incorporating background knowledge on the performance of voting algorithms forI = 0, and N T = 20, as N O ranges from 2 to 20. In addition, this experiment aims to show that backgroundR = 10,knowledge does not undermine the advantage that voting algorithms held over the greedy algorithm in the knowledge-free case. To this end, we have trivially generalized G to produce LPMs that are consistent with given backgroundknowledge B. The data points are averages over 20 different pairs of training and test sets (O , T ). We have arbitrar-ily picked two weak orderings to use as background knowledge: B1: { X1, X2, X3, X4, X5} < { X6, X7, X8, X9, X10} andB2: { X1, X2, X3} < { X4, X5} < { X6, X7, X8} < { X9, X10}. The performance of VV improved greatly with the introduction ofeach ordering as background knowledge. B2 is stronger than B1, and therefore prunes the space of consistent LPMs morethan B1. As a result, the performance gain due to B2 is greater than that due to B1. The difference between the perfor-mance with background knowledge and without background knowledge is statistically significant except at the last point.Note that using background knowledge is particularly effective when the number of training observations is small. Theworst-case performance of G with background knowledge B1 and B2 are also shown in Fig. 9. In both cases, the worst-caseperformance of G is significantly lower than the performance of VV with the corresponding background knowledge.Using the same experimental scenario, we obtained very similar results with MV, as seen in Fig. 10. In summary, theworst case performance of greedy algorithm with background knowledge B2 outperforms the average performance of MVwithout any background knowledge. However, even with weaker knowledge, such as B1, the average performance of MV isbetter than G with B2.F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071305Fig. 7. The average prediction performance of model voting, greedy, NAMV, and GV as the number of noisy observations increases.Fig. 8. The worst prediction performance of model voting, greedy, NAMV, and GV as the number of noisy observations increases.8. Related workThe concept of lexicographic comparisons is commonplace in everyday life. Expressions such as “safety first,” “aboveall else do no harm,” or “quality is job one” all evoke lexicographic preferences. Lexicographic rankings are often used insporting events. For example, countries competing in the Olympics are typically ranked by total medals, then gold medals,then silver medals [17]. The winner of the Netflix prize was chosen by ranking submissions first by minimum test error,then by earliest submission [13].Lexicographic utilities have been applied to understanding human preferences [19,11]. They have an extensive mathemat-ical foundation that has been studied in the economics, psychological, and management science literature [8]. Lexicographicorders and other preference models have been utilized in several research areas, including multicriteria optimization [1],linear programming [5], and game theory [14].The most relevant existing work for learning and/or approximating LPMs is by Schmitt and Martignon [16] and Dombiet al. [7], which were summarized in Section 2.1306F. Yaman et al. / Artificial Intelligence 175 (2011) 1290–1307Fig. 9. The effect of background knowledge on VV and G, using two arbitrarily selected weak orderings as background knowledge, where B2 is stronger(more constraining) than B1.Fig. 10. The effect of background knowledge on average MV performance, using two arbitrarily selected weak orderings as background knowledge, whereB2 is stronger than B1.In general, preferences and ranking are similar. The ranking problem, as described by Cohen et al. [3], is similar tothe problem of learning an LPM. However, that line of work poses learning as an optimization problem, with the goal offinding the single ranking that maximally agrees with the given preference function. In particular, their approach constructsa collection of domain-specific “ranking experts” whose predictions are combined using a model voting scheme. The votingconcept is similar in spirit to our approach, but the underlying representations are quite different.Torrey et al. [18] employ an inductive logic programming approach to learn multi-attribute ranking rules. In principle,these rules can represent lexicographic preference models.Fürnkranz and Hüllermeier [10] investigate the pairwise preference learning and ranking problem. The observations area set of partially ranked objects and the goal is to learn how to rank a new set of objects. Their approach is to reduce theoriginal problem to a number of binary classification problems, one for each pair of labels. Hence, they make no assumptionsabout the underlying preference model.Boutilier et al. [2] consider a preference learning algorithm and representation (CP-nets) for modeling preferences undera ceteris paribus (all else being equal) assumption. However, this representation will not necessarily capture lexicographicpreference models, and is therefore not directly applicable to the problem we have considered.Another analogy, described by Schmitt and Martignon [16], is between LPMs and decision lists [15]. Specifically, it wasshown that LPMs are a special case of 2-decision lists, but that the algorithms for learning these two classes of models arenot directly applicable to each other.9. Conclusions and future workIn this paper, we presented democratic approximation methods for learning lexicographic preference models (LPMs)given a set of preference observations. Instead of committing to just one of the consistent LPMs, we maintain a set ofmodels and predict based on the majority of votes. We described two such methods: variable voting and model voting.We showed that both methods can be implemented in polynomial time and exhibit much better worst- and average-caseF. Yaman et al. / Artificial Intelligence 175 (2011) 1290–13071307performance than the existing methods. Finally, we have defined a form of background knowledge that can be used toimprove performance when the number of observations is small; we incorporated this background knowledge into thevoting-based methods, significantly improving their empirical performance.Future directions of this work allow for a number of extensions and further theoretical investigations. Many of thetheoretical bounds presented in this paper could be tightened (such as the complexity bound of learnVariableRank) orpotentially defined in terms of simpler parameters (such as the sample size parameter for model vote), while still maintain-ing performance guarantees. We have recently extended the basic LPM representation and learning techniques to supportcontext-dependent preferences in the form of branching LPMs, including methods for learning branching LPMs from noisydata [12]. We are also continuing to investigate heuristics like NAMV and greedyVote that make them more robust againstnoise. While the problem of learning LPMs from noisy data is NP-complete, the superior performance of the voting algo-rithms over the greedy method in the noise-free case indicates that it may be possible to identify and characterize otherrestricted problem settings in which heuristic extensions such as NAMV and greedyVote would significantly outperform thestate-of-the-art greedy approach.AcknowledgementsThis work was supported by the Defense Advanced Research Projects Agency and the US Air Force through BBN Tech-nologies Corp. under contract number FA8650-06-C-7606.References[1] D. Bertsekas, J. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods, Athena Scientific, 1997.[2] C. Boutilier, R.I. Brafman, C. Domshlak, H.H. Hoos, D. Poole, CP-nets: A tool for representing and reasoning with conditional ceteris paribus preferencestatements, Journal of Artificial Intelligence Research 21 (2004) 135–191.[3] W. Cohen, R. Schapire, Y. Singer, Learning to order things, Journal of Artificial Intelligence Research 10 (1999) 243–270.[4] A.M. Colman, J.A. Stirk, Singleton bias and lexicographic preferences among equally valued alternatives, Journal of Economic Behavior and Organiza-tion 40 (4) (December 1999) 337–351.[5] G. Dantzig, A. Orden, P. Wolfe, The generalized simplex method for minimizing a linear form under linear inequality restraints, Pacific Journal ofMathematics 5 (1955) 183–195.[6] R.M. Dawes, The robust beauty of improper linear models in decision making, American Psychologist 34 (1979) 571–582.[7] J. Dombi, C. Imreh, N. Vincze, Learning lexicographic orders, European Journal of Operational Research 183 (2) (2007) 748–756.[8] P. Fishburn, Lexicographic orders, utilities and decision rules: A survey, Management Science 20 (11) (1974) 1442–1471.[9] J.K. Ford, N. Schmitt, S.L. Schechtman, B.M. Hults, M. Doherty, Process tracing methods: contributions, problems and neglected research issues, Organi-zational Behavior and Human Decision Processes 43 (1989) 75–117.[10] J. Fürnkranz, E. Hüllermeier, Pairwise preference learning and ranking, in: Proc. ECML-03, Cavtat–Dubrovnik, Springer-Verlag, 2003, pp. 145–156.[11] G. Gigerenzer, D.G. Goldstein, Reasoning the fast and frugal way: Models of bounded rationality, Psychological Review 103 (4) (1996) 650–669.[12] J. Jones, M. desJardins, Learning of branching lexicographic preference models. Tech. Rep. TR-CS-10-05, UMBC Dept. of CS&EE, June 2010.[13] Netflix, The Netflix prize rules, http://www.netflixprize.com/rules, 2009.[14] A. Quesada, Negative results in the theory of games with lexicographic utilities, Economics Bulletin 3 (20) (2003) 1–7.[15] R. Rivest, Learning decision lists, Machine Learning 2 (3) (1987) 229–246.[16] M. Schmitt, L. Martignon, On the complexity of learning lexicographic strategies, Journal of Machine Learning Research 7 (2006) 55–83.[17] Sports illustrated, http://sportsillustrated.cnn.com/olympics/2008/medals/tracker/, 2008.[18] L. Torrey, J. Shavlik, T. Walker, R. Maclin, Relational macros for transfer in reinforcement learning, in: Proceedings of the Seventeenth Conference onInductive Logic Programming, Corvallis, Oregon, 2007.[19] A. Tversky, Intransitivity of preferences, Psychological Review 76 (1969) 31–48.[20] M.R. Westenberg, P. Koele, Multi-attribute evaluation processes: methodological and conceptual issues, Acta Psychologica 87 (1994) 65–84.[21] F. Yaman, M. desJardins, More-or-less CP-networks, in: Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence, 2007.