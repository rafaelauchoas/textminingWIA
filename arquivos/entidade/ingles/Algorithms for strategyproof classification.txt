Artificial Intelligence 186 (2012) 123–156Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAlgorithms for strategyproof classificationReshef Meir a,∗, Ariel D. Procaccia b, Jeffrey S. Rosenschein a,1a School of Engineering and Computer Science, Hebrew University, Jerusalem 91904, Israelb Computer Science Department, Carnegie Mellon University, 5000 Forbes, Pittsburgh, PA 15213, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 25 September 2011Received in revised form 12 March 2012Accepted 26 March 2012Available online 27 March 2012Keywords:Mechanism designClassificationGame theoryApproximationThe strategyproof classification problem deals with a setting where a decision maker mustclassify a set of input points with binary labels, while minimizing the expected error. Thelabels of the input points are reported by self-interested agents, who might lie in order toobtain a classifier that more closely matches their own labels, thereby creating a bias inthe data; this motivates the design of truthful mechanisms that discourage false reports.In this paper we give strategyproof mechanisms for the classification problem in tworestricted settings: (i) there are only two classifiers, and (ii) all agents are interested in ashared set of input points. We show that these plausible assumptions lead to strong positiveresults. In particular, we demonstrate that variations of a random dictator mechanism, thatare truthful, can guarantee approximately optimal outcomes with respect to any family ofclassifiers. Moreover, these results are tight in the sense that they match the best possibleapproximation ratio that can be guaranteed by any truthful mechanism.We further show how our mechanisms can be used for learning classifiers from sampleddata, and provide PAC-style generalization bounds on their expected error. Interestingly,our results can be applied to problems in the context of various fields beyond classification,including facility location and judgment aggregation.© 2012 Elsevier B.V. All rights reserved.1. IntroductionConsider a learning algorithm, which takes a labeled set of samples (“training data”) as input, and outputs a binary clas-sifier. The training data, typically hand-constructed by human experts, is supposed to reflect the knowledge of the expertson the current domain. The basic requirement from such an algorithm is to guarantee that the output classifier minimizesthe number of classification errors with respect to the ‘truth’ (according to the domain experts). Standard machine-learningliterature studies the performance of such algorithms given various distributions and concept classes (e.g., linear classifiers),sparse or noisy data, etc.However in many real-life situations, the experts have a personal interest in the outcome of the algorithm, and thereforethey cannot be assumed to be truthful. If an expert can bias the learned classifier in her favor by lying, then the reportedtraining data will no longer reflect the properties of the domain (or even the properties of the real training data). Optimizinga classifier based on such corrupted data may result in a very poor classifier, regardless of the guarantees supplied bylearning theory (which assumes truthfulness).We consider two interrelated settings. The first setting is decision-theoretic; a decision must be made based on datareported by multiple self-interested agents. The agents are concerned with the binary labels of a set of input points. Put* Corresponding author. Tel.: +972 2 6585188.E-mail addresses: reshef.meir@mail.huji.ac.il (R. Meir), arielpro@cs.cmu.edu (A.D. Procaccia), jeff@cs.huji.ac.il (J.S. Rosenschein).1 Tel.: +972 2 6585353.0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.03.008124R. Meir et al. / Artificial Intelligence 186 (2012) 123–156another way, the agents may disagree on the labels of the points of the input space, and we do not assume any underlyingdistribution. The utility of an agent with respect to a given decision (i.e., a given classifier) is the number of points onwhich the label provided by the classifier agrees with the agent’s own label. The goal of the decision maker is to choose aclassifier that maximizes the social welfare—the sum of utilities. As we will see, results in this setting can also be appliedto problems in the context of various other fields, including facility location and judgment aggregation.The second setting is learning-theoretic, a variation of the standard Supervised Classification problem. Samples are drawnfrom some distribution over the input space, and are then labeled by experts. A classification mechanism receives thesampled data as input, and outputs a classifier. Unlike the standard setting in machine learning (but similarly to our firstsetting), the experts are assumed to be self-interested agents, and may lie in order to increase their utility. This settingmay seem far more involved than the first, as it deals with generalization from partial data (the dataset) to the underlyingdistribution. However, we show that under the standard assumptions of learning theory, the learning problem effectivelyreduces to finding a classifier that best fits the available data (i.e., to the first setting, above).In both settings the decision maker (or mechanism, or learning algorithm) aims to find a classifier that classifies theavailable data as well as possible. However, the agents may misreport their labels in an attempt to influence the finaldecision in their favor. The result of a decision making process based on such biased data may be completely unexpectedand difficult to analyze. A truthful learning mechanism eliminates any such bias and allows the decision maker to select aclassifier that best fits the reported data, without having to take into account the hidden interests of the agents. In otherwords, once we guarantee that agents are telling the truth, we may concentrate on the more standard goal of minimizingthe error. In order to obtain truthfulness, however, we may need to trade off optimality. Our goal is to provide mechanismsthat are both truthful and approximately optimal in terms of social welfare.1.1. Restrictions on the domainIn recent work [29] we showed that in an unrestricted domain, it is effectively impossible to design truthful mechanismsthat are close to optimal. This motivates the investigation of restricted domains. In this paper we consider several suchrestrictions, described below.1.1.1. Restricting the concept class: two functionsA seemingly simple case is when the concept class contains only two functions. This is equivalent to a (binary) decisionthat has to be made based on data points that are controlled by multiple (possibly) selfish agents, where the decisionaffects all the agents. The decision maker would like to make a decision which is consistent, as much as possible, with allthe available data. However, in our strategic setting the agents might misreport their data in an attempt to influence thefinal decision in their favor.As a motivating example, consider a decision that has to be made by the Workers’ committee of the TAs in the HebrewUniversity, regarding an ongoing strike. Each member of the committee (who represents one department) announces howmany TAs in his/her department support the strike, and how many oppose it. A final decision is made based on totalsupport for the strike. Suppose that 60% of the economics department opposes the strike. However, the representative ofthe economics department majors in game theory. She therefore knows that for the benefit of the majority of TAs in herdepartment, it would be better to state that everybody objects to the strike.21.1.2. Restricting the dataset: shared inputsOur main conceptual contribution in this paper, which leads to strong positive results, is the assumption of shared inputs.In the decision-theoretic setting, this means that the agents share the same set of input points, and only disagree on thelabels of these points. In the learning-theoretic setting, the shared inputs assumption implies that the agents are interestedin a common distribution over the input space, but, once again, differ with respect to the labels.The first restriction we described did not address the issue of shared inputs. However, as the two possible classifiersare constant, the identity of the input points (i.e., their location) is irrelevant—only their labels matter. Hence, the firstrestriction is in fact a very special case of the latter (see also footnote 17).As the shared inputs assumption is a weaker restriction than assuming two functions, the guarantees are also somewhatweaker. Nevertheless, they hold with respect to any concept class. We believe that in many environments the requirement ofshared inputs is satisfied. As an example, consider a large organization that is trying to fight congestion in an internal emailsystem by designing a smart spam filter. In order to train the system, managers are asked to review the last 1000 emailssent to the “all employees” mailing list (hence, shared inputs) and classify them as either “work-related” (positive label) or“spam” (negative label). Whereas the managers will likely agree on the classification of some of the messages (e.g., “BuyViagra now!!!” or “Christmas Bonus for all employees”), it is likely that others (e.g., “Joe from the Sales department goes ona lunch break”) would not be unanimously classified. Moreover, as each manager is interested in filtering most of what hesees as spam, a manager might try to compensate for the “mistakes” of his colleagues by misreporting his real opinion with2 In an attempt to avoid such misrepresentation, major decisions usually require a gathering of all TAs, and the use of a standard voting procedure.However, most decisions are taken with a much narrower quorum.R. Meir et al. / Artificial Intelligence 186 (2012) 123–156125respect to some cases. For example, the manager of the R&D department, believing that about 90% of the Sales messages areutterly unimportant, might classify all of them as spam in order to reduce the congestion. The manager of Sales, suspectingthe general opinion on her department, might do the exact opposite to prevent her e-mails from being filtered. The factthat some users may not have a full understanding of the learning algorithm, does not necessarily prevent them from tryingto bias it anyway. Even if their strategy is not optimal for them, it still contaminates the data.Interestingly, our model for binary classification with shared inputs is equivalent to models that have been suggested inthe literature for problems in seemingly unrelated domains, including judgment aggregation, partition aggregation, facilitylocation, and voting (for a more detailed comparison, see Section 1.3 and discussion).One such common classification/partition problem is deciding on the operating hours of a shared resource. As a concreteexample, consider a building with a central heating system (such buildings are common in Jerusalem and in many cities ofEurope). Every tenant has certain hours in which he wants the heat to be on (e.g., always on when he is home, and offotherwise, since the cost is shared by all tenants). The household fee is the same for all tenants, and thus there is no transferof payoffs. A “classifier” is a partition of the day (or week) to on and off intervals. Furthermore, there are constraints on thefinal partition; for example, on intervals must be at least 3 hours long to achieve better efficiency.1.1.3. Realizable datasetsIn some cases, learning is facilitated if we know that there is at least one “perfect” classifier in our concept class (that is,a classifier that separates all positive data points in the dataset from the negative ones). Such datasets are called realizable.It is therefore possible that the labels of each agent will be realizable, even if there is no single classifier that is perfect forall agents. We study how realizability, which can be seen as another restriction on the dataset, affects the optimality of theproposed mechanisms in the context of shared input.1.2. Overview of our resultsWe wish to design classification mechanisms that achieve a good outcome in the face of strategic behavior. By “goodoutcome” we mean that the output of the mechanism provides an approximation of the optimal solution.3 We would alsolike our mechanisms to be strategyproof (SP), that is, the agents must not be able to benefit from lying. These two keyrequirements are formalized and demonstrated with examples in Section 2.We begin by presenting mechanisms for the two-function problem in Section 3. The results of this section serve twopurposes. First, the tight worst-case analysis of SP mechanisms provides a full picture of their power and limitations in thebinary decision-making setting. Second, the focus on a simple setting allows us to explain in detail subtle issues that arealso important for the next, more general, setting.We put forward a simple deterministic decision-making mechanism which is group strategyproof (i.e., even coalitionsof agents do not gain from lying) and gives a 3-approximation of the optimal global risk; in other words, the numberof mislabeled points is at most 3 times the minimum number. Moreover, we show that no deterministic strategyproofmechanism can do better. Interestingly, we circumvent this result by designing a strategyproof randomized mechanism thatgives a 2-approximation, and further demonstrate that this is as far as randomization can take us.In Section 4, we turn to study the more general case, under the shared inputs assumption. We first show that SP de-terministic mechanisms cannot guarantee a sublinear approximation ratio. We show that choosing a dictator at randomprovides an approximation ratio of 3 in expectation, even if agents have weights, i.e., the decision mechanism values someagents more than others (in that case we randomly select a dictator according to the weights). We then drive the approx-imation even lower by using a non-trivial selection of the dictator, matching it with the known lower bound of 3 − 2n ; itis quite striking that these results hold with respect to any concept class. In addition, we show that when datasets arerealizable, an even better approximation ratio (of 2 − 2n ) can be guaranteed.In each section we further show how the suggested mechanisms for the decision-theoretic setting can be further ex-ploited to attain similar approximation results in the learning-theoretic setting. We observe that in the learning-theoreticsetting, designing strategyproof mechanisms is virtually impossible, since there is an additional element of randomness in-troduced by sampling the input space. We therefore relax the strategyproof requirements, and instead investigate each oftwo incomparable strategic assumptions: that agents do not lie if they cannot gain more than (cid:2); and that agents always usea dominant strategy if one exists with respect to a specific sample. We show that under either assumption our randomizedmechanisms can be run directly on sampled data while maintaining a bounded expected error. Our theorems give a connec-tion between the number of samples and the expected error of the mechanism in each case, in the spirit of PAC-learningalgorithms [40].1.2.1. Mechanisms with paymentsAn important remark is that in the strategyproof classification setting, standard economic money-based mechanisms suchas the Vickrey–Clarke–Groves (VCG) mechanism (see, e.g., [32]) can be used to obtain good results. However, our setting3 Approximation algorithms are frequently used in various domains in computer science in order to overcome computational barriers. While we largelyignore issues of computational complexity, optimal algorithms are typically not strategyproof; hence, the need for approximation.126R. Meir et al. / Artificial Intelligence 186 (2012) 123–156admits strategyproof mechanisms that do well even without assuming that money is available. Achieving our goals withoutresorting to payments is highly desirable, since often payments cannot be made due to legal or ethical considerations.Moreover, in Internet environments VCG style payments are notoriously difficult to implement, due to banking and securityissues. Hence, we follow the example set by previous work on strategyproof learning models (e.g., [10], see below) byconsidering approximation mechanisms that do not require payments.1.3. Related workThis paper lies at the intersection of several areas, including mechanism design, judgment aggregation, and learning. Wecluster the related work by areas.1.3.1. Approximate mechanism design without moneyMechanisms that deal with strategic behavior of agents have been proposed recently for a large range of applications.While certain restrictions may allow the design of optimal SP mechanisms [39], often this is not the case, and approximationis required. This observation gave rise to the agenda of approximate mechanism design without money (AMDw/oM).Below, we overview some SP mechanisms for machine learning problems in detail, and compare them to our work.These, however, constitute just one facet of the large variety of problems to which AMDw/oM can be applied. Approximatemechanisms without payments have been proposed for facility location, matching [3,15], resource allocation [18,19,33],scheduling [23], and even auctions [20].1.3.2. Strategyproof learning algorithmsThe work most closely related to ours is a paper by Dekel et al. [10]. Their work focused on regression learning, wherethe labels are real numbers and one is interested in the distances between the mechanism’s outputs and the labels. Exceptfor this very significant difference, the settings that we study and our goals are very similar to theirs. Dekel et al. providedupper and lower bounds on the approximation ratio achieved by supervised regression mechanisms in this model. Notably,some of our bounds resemble the bounds in their regression setting. Moreover, similar intuitions sometimes apply to bothsettings, although it seems the results of one setting cannot be analytically mapped to the other. Dekel et al. also concentrateon mechanisms without payments, but their results hold only with respect to very specific function classes (as they do notassume shared inputs; see, e.g., Theorems 4.1 and 4.2 of [10]). We also demand weaker assumptions for some of ourgeneralization theorems, thereby allowing for stronger results.Strategyproof regression has also been studied by Perote-Peña and Perote [34]. They suggested several mechanisms andcompared them to naive learning algorithms in a strategic setting. Unlike Dekel et al., they evaluated their mechanismsempirically rather than analytically, with respect to some specific assumptions on the strategic behavior of the agents.Another rather closely related work by the same authors has results of a negative flavor. Perote and Perote-Peña [35]put forward a model of unsupervised clustering, where each agent controls a single point in R2 (i.e., its reported location).A clustering mechanism aggregates these locations and outputs a partition and a set of centroids. They show that if everyagent wants to be close to some centroid, then under very weak restrictions on the clustering mechanism there alwaysexists a beneficial manipulation, that is, there are no reasonable (deterministic) clustering mechanisms that are SP.1.3.3. Judgment and partition aggregationWhile the motivation for our model stems from the binary classification problem in machine learning, very similar modelshave been used to describe various problems of judgment aggregation. In particular, a list of binary issues that must bedecided upon is essentially equivalent to a dataset with binary labels. Similarly, a suggestion to split a finite set into twoparts can also be replaced with labels for each element in the set.Properties of mechanisms for judgment/partition aggregation have been discussed extensively in the literature sincethe 1970s [42,30,24,5,16]. A recent paper that deals explicitly with manipulations is by Dokow and Holzman [14], whichcharacterizes strategyproof aggregation rules (that can also be interpreted as classification mechanisms in our framework).Our current work differs in two important ways from the literature on judgment aggregation. First, we explicitly measurethe quality of proposed mechanisms (in the spirit of AMDw/oM), which enables us to compare SP mechanisms to oneanother. Second, we study not only deterministic mechanisms, but also randomized ones. We believe that the notion ofapproximation, and the use of randomization (both a common practice in computer science) can also contribute to thestudy of more “standard” judgment aggregation settings. The current paper is a demonstration of this approach.1.3.4. Facility locationIn the facility location problem, agents report their location (usually in some metric space), and the mechanism outputsa location for a facility that is close, on average, to all agents. SP location mechanisms for various topologies have beensuggested and studied (see, e.g., [1,26,36], which also provides a clear overview of the field).Consider a dataset labeled by several agents, and a binary cube whose dimensions correspond to the samples in thedataset. It is not hard to verify that classification with shared inputs is equivalent to facility location on the binary cube,where the label vector of each agent corresponds directly to a specific vertex of this cube. Similarly, any concept class (whichR. Meir et al. / Artificial Intelligence 186 (2012) 123–156127defines the allowed labellings) corresponds to a set of vertices which constitutes the allowed locations. A classificationmechanism then seeks the optimal classification (i.e., the optimal vertex) within this restricted set.Although our main focus in the context of binary classification is the binary cube, all of our mechanisms in this papercan be directly applied to facility location problems in any metric space.An important note is that it is typically assumed that the set of allowed locations for the facility coincides with thepossible locations of the agents. This is equivalent to the assumption of realizability in our classification model. We study SPmechanisms both with and without this assumption.1.3.5. VotingA finite set of classifiers can also be thought of as a class of candidates in a voting scenario, where the experts are castingthe votes. While such a perspective is sometimes useful (see, for example, [29]), the preferences in voting are typically muchmore expressive.We can, however, model any preference profile with a proper input space. Suppose that we have a set of candidates;consider the binary cube from the last section, where every dimension (i.e., a sample in the dataset) corresponds to apair of candidates. The allowed set of vertices (i.e., the concept class) restricts the outcome to vertices that correspond to alinear order over the candidates. The assumption of realizability in this setting is interpreted as rationality of the voters. Theoptimal classification mechanism, which minimizes the average distance to all voters, is equivalent to the Kemeny–Youngvoting rule [22]. Therefore, SP classification mechanisms can be interpreted in this setting as strategyproof approximationsof the Kemeny–Young rule. It is important to note, however, that strategyproofness in our model does not coincide with thesimilar requirement in voting (as in the typical voting setting only the identity of the winner is considered).1.3.6. Other related workThere is a significant body of work on learning in the face of noise, where the noise can be either random or adversarial(see, e.g., [6,25]). Dalvi [9], and Dekel and Shamir [11] study settings more similar to ours, where the learning process ismodeled as a game between a classifier and an adversary. However, in these papers the goal is to do well in the face ofnoisy or biased data, rather than provide incentives in a way that prevents the dataset from being manipulated in the firstplace.Further afield, it is worth mentioning several examples from the literature that apply machine learning techniques inorder to resolve problems in economics or game theory. Balcan et al. [4] apply SP machine learning algorithms to learnbidders’ valuations in auctions. However, the authors achieve truthfulness by learning from agents that are not directlyinfluenced by the outcome that relies on their reported data. This is not possible in our setting, as all agents are affected bythe selected classifier. Other papers such as Procaccia et al. [37] suggest learning algorithms that enable better preferenceaggregation, but do not consider strategic behavior of the society. Finally, there has been some recent work on automatedmechanism design using techniques from machine learning [7,8]. Although the designed mechanisms are required to betruthful, the learning algorithm itself does not handle private information, and thus truthfulness is irrelevant.2. Model and notationsWe start by introducing our model and notations for the decision-theoretic setting; additional definitions for thelearning-theoretic setting are given subsequently.2.1. Binary classification with multiple expertsLet X be an input space, which we assume to be either a finite set or some subset of Rd. A classifier or concept c is afunction c : X → {+, −} from the input space to the labels {+, −}. A concept class C is a set of such concepts. For example,the class of linear separators over Rd is the set of concepts that are defined by the parameters a ∈ Rd and b ∈ R, and mapa point x ∈ Rd to + if and only if a · x + b (cid:2) 0.Denote the set of agents by I = {1, . . . , n}, n (cid:2) 2. The agents are interested in a (finite) set of k data points X ∈ X k. Inthis paper we assume that X is shared among the agents, that is, all the agents are equally interested in each data pointin X . This plausible assumption, as we shall see, allows us to obtain surprisingly strong results. Naturally, the points in Xare common knowledge.Each agent has a private type: its labels for the points in X . Specifically, agent i ∈ I holds a function Y i : X → {+, −},which maps every point x ∈ X to the label Y i(x) that i attributes to x. Each agent i ∈ I is also assigned a weight w i , whichreflects its relative importance; by normalizing the weights we can assume that(cid:2)i∈I w i = 1. Let(cid:3)(cid:4)(cid:5)x, Y i(x)S i =: x ∈ X(cid:6)be the partial dataset of agent i, and let S = (cid:5)S1, . . . , Sn(cid:6) denote the complete dataset. S i is said to be realizable w.r.t. aconcept class C if there is c ∈ C which perfectly separates the positive samples from the negative ones. If S i is realizable forall i ∈ I , then S is said to be individually realizable. Fig. 1 shows an example of a dataset with a shared set of points X .128R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Fig. 1. An instance with shared inputs. Here, X = R2, C is the class of linear separators over R2, and n = 3. The data points X of all three agents areidentical, but the labels, i.e., their types, are different. The best classifier from C with respect to each S iis also shown (the arrow marks the positivehalfspace of the separator). Only the rightmost dataset is realizable.We use the common 0–1 loss function to measure the error. The risk,4 or negative utility, of agent i ∈ I with respect toa concept c is simply the relative number of errors that c makes on its dataset. Formally,(cid:7)(cid:7)Ri(c, S) = 1k(cid:2)c(x) (cid:7)= y(cid:3)= 1kx∈ X(cid:2)(cid:3)c(x) (cid:7)= Y i(x)(cid:5)x, y(cid:6)∈S i,(1)where [[ A]] denotes the indicator function of the boolean expression A. Note that S iis realizable if and only ifminc∈C Ri(c, S) = 0. In contrast to most standard learning scenarios, in our model there is no “ground truth”, and theobjective is to classify in a way that will be most satisfactory to the agents. Thus the global risk is defined asRI (c, S) =(cid:7)i∈Iw i · Ri(c, S) = 1k(cid:7)(cid:7)i∈Ix∈ X(cid:3)(cid:2)c(x) (cid:7)= Y i(x).w i ·2.2. Mechanism properties(2)A deterministic mechanism M receives as input a dataset S,5 and outputs a classifier c ∈ C. Note that since S is finite, thereare only finitely many different ways to classify the data; thus, Ri(M(S), S) for all i ∈ I and RI (M(S), S) are well-defined.This will no longer be the case in the learning-theoretic setting, where we will need to slightly modify our definitions.A randomized mechanism is identified with a probability distribution pM over S × C. We restrict our attention to proba-bilities with a finite support. That is, for every dataset S, the mechanism M returns c ∈ C, with a probability of pM(c | S).When measuring the risk, we are interested in the expected number of errors that the mechanism makes on the givendataset. Formally,(cid:8)(cid:9)RiM(S), S(cid:10)Ri(c, S)(cid:12)(cid:11)(cid:11) S== EpM(cid:7)c∈CpM(c | S) · Ri(c, S),(3)and the global risk is defined analogously.For any (complete or partial) dataset S(cid:8) ⊆ S, the best available classifier with respect to the dataset S(cid:8)is referred to asthe empirical risk minimizer (erm)—a common term in the machine learning literature. Formally,.(4)(cid:8)erm(cid:8)S(cid:9)(cid:7)(cid:2)(cid:3)c(x) (cid:7)= y= argminc∈C(cid:5)x, y(cid:6)∈S(cid:8)For the complete dataset, we denote the best classifier by ccontext). That is,∗(S), and its risk by r∗(S) (or simply c∗, r∗if S is clear from the∗c(S) = erm(S) = argminc∈CRI (c, S)and r∗(S) = RI (c∗(S), S).The simple mechanism that always computes and returns erm(S) is referred to as the ERM mechanism (with blockletters).6 If there is more than one optimal classifier, we assume that ERM returns one of them arbitrarily. Similarly, a mech-anism which returns the best classifier with respect to a partial dataset of a specific agent (e.g., erm(S 1)) is called a dictatormechanism.If ris said to be perfect. Note that the existence of a perfect classifier in C implies that all partial datasets∗ = 0 then c∗are realizable, but the converse does not hold.We measure the quality of the outcome of a mechanism using the standard notion of multiplicative approximation.4 When the dataset S consists of sampled data, the appropriate term is empirical risk. This distinction will become significant in Sections 3.3 and 4.3.5 We implicitly assume that information regarding the weights of the agents is contained in the dataset.6 Actual algorithms to compute the erm may raise various practical problems that depend on the domain, such as computational complexity. However,such problems are not within the scope of this paper. Since an erm always exists and the number of data points is finite, there is an algorithm thatcomputes an erm in finite time.Definition 2.1. A mechanism M is an α-approximation mechanism if for any dataset S it holds that RI (M(S), S) (cid:3) α · rR. Meir et al. / Artificial Intelligence 186 (2012) 123–156129∗(S).Note that randomized mechanisms are only required to attain approximation in expectation, and not necessarily with highprobability.We emphasize that the real labels of the input points are private information, and an agent may report different labelsthan the ones indicated by Y i . We denote by Y i : X → {+, −} the reported labels of agent i. We also denote by S i ={(cid:5)x, Y i(x)(cid:6): x ∈ X} the reported partial dataset of agent i, and by S = (cid:5)S 1, . . . , Sn(cid:6) the reported dataset.Strategyproofness implies that reporting the truthful types is a dominant strategy for all agents. For a dataset S and i ∈ I ,let S−i be the complete dataset without the partial dataset of agent i.Definition 2.2. A (deterministic or randomized) mechanism M is strategyproof (SP) if for every dataset S, for every i ∈ I , andfor every S i ,(cid:8)(cid:9)(cid:8)RiM(S), S(cid:3) Ri(cid:9)M(S i, S−i), S.(5)Our goal is to design mechanisms that are both SP and guarantee a low worst-case approximation ratio.There is an inherent tradeoff between strategyproofness and good approximation. The ERM mechanism (which alwaysreturns erm(S)), for example, is a 1-approximation mechanism, but is not SP (as we show in the next section). On the otherhand, a mechanism that selects agent 1 as a dictator, and returns erm(S 1), is clearly SP but in general may give a very badapproximation (e.g., if all other agents disagree with agent 1).We remark that for randomized mechanisms, some make a distinction between strategyproofness in expectation (asDefinition 2.2 implies), and universal strategyproofness. The latter, stronger definition requires that an agent cannot gain fromlying even after the randomization takes place. Interestingly, the first, weaker notion of strategyproofness is sufficient forour lower bounds, but our upper bounds satisfy universal strategyproofness.3. Choosing from two classifiersIn this section we consider a very simple concept class, containing only two classifiers. For ease of exposition we assumethat there is a positive classifier c+ and a negative classifier c−, such that c+(x) = “ + ”, c−(x) = “ − ” for any x ∈ X . Ourconcept class C = {c+, c−} can be thought of as choosing between a global positive decision and negative decision, respectively.Remark 1. Although we define our concept class C as containing two specific classifiers, our results easily extend to everyconcept class of size 2 (provided that there is at least one data point x ∈ X on which the two concepts disagree). Indeed,the part of the dataset on which the concepts agree can only improve the approximation ratio, and on the other hand wecan always give examples where all data points are in conflict. Thus both upper and lower bounds still hold.We start with some observations that will allow us to simplify our model in this setting. Note that the identity of eachdata point is not important, only the fraction of positive and negative labels that each agent attributes to the dataset. Wecan also think of this setting as if each agent controls a different set of points Xi , where the size of each such partial datasetis proportional to the agent’s weight. With this interpretation our model becomes even simpler, as both the weight and thetype of each agent are completely defined by the number of “positive points” and “negative points” it controls.Consider our TA committee example from the introduction. We can count each TA as a single data point (which ispositive if it supports the strike), and the representative of each department reports the opinions of all TAs. The weight ofdepartment in this case would be proportional to the number of workers.(cid:2)We denote the number of points controlled by agent i by mi = | Xi| = |S i|, and the size of the full dataset by m = |S| =i∈I mi . This notation will be used in this section instead of k. We further denote the number of positive and negative datai∈I P i ,points by P i = |{(cid:5)x, y(cid:6) ∈ S i: y = +}|, and Ni = mi − P i = |{(cid:5)x, y(cid:6) ∈ S i: y = −}|. For convenience we also let P =N =i∈I Ni . We emphasize that {P i, Ni}i∈I contains all the information relevant to our problem and can thus replace S.With these alternative notations, the private risk of concept c for agent i is the same as in Eq. (1), only replacing k(cid:2)(cid:2)with mi . The risk is further simplified in the two-function case:if c = c−,if c = c+.Ri(c, S) = 1miP i/mi,Ni/mi,c(x) (cid:7)= y(cid:7)=(cid:13)(cid:3)(cid:2)(cid:5)x, y(cid:6)∈S iWe update the definition of the global risk as follows:RI (c, S) =(cid:7)i∈ImimRi(c, S) = 1m(cid:7)(cid:2)c(x) (cid:7)= y(cid:3).(cid:5)x, y(cid:6)∈S(6)(7)Similarly to the private risk, RI (c, S) is either P /m (for c−) or N/m (for c+). Note that by taking w i = micase of Eq. (2).m , this is a special130R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Fig. 2. ERM is not strategyproof. Agent 1 changes one of its points from negative to positive, thus changing the risk minimizer from c− to c+, to agent 1’sadvantage. In this illustration, X = R2.Unfortunately, if we choose ERM as our mechanism, then even in this simple setting the agents may lie in order todecrease their subjective risk.Example 3.1 (Illustrated in Fig. 2). Agent 1 controls 3 examples: 2 positive and 1 negative. Agent 2 controls 2 examples, bothnegative. Since there is a majority of negative examples, ERM would return c−; agent 1 would suffer a subjective risk of 2/3.On the other hand, if agent 1 reported his negative example to be positive as well, ERM would return c+, with a subjectiverisk of only 1/3 for agent 1. Indeed, note that an agent’s utility is measured with respect to its real labels, rather than withrespect to the reported labels.It is easy to see, however, that an agent cannot gain by lying when it only controls one point. For instance, if an agenthas a positive point and ERM returns c−, falsely reporting a negative label will only reinforce the mechanism’s decision.This is in striking contrast to the regression learning setting considered in Dekel et al. [10], where the deepest technicalresults concern the single-point-per-agent scenario.Despite the fact that ERM is not SP, we would still like to use the optimal concept in order to evaluate other conceptsand mechanisms. From the definition of the erm, we have that∗ = RIr(cid:8)c∗, S(cid:9)(cid:3)= min(cid:6)RI (c+, S), RI (c−, S)3.1. Deterministic mechanisms= min(cid:13)(cid:14).Nm,PmDenote by ci the erm on S i , i.e., ci = c+ if P i (cid:2) Ni and c− otherwise. Clearly ci is the best classifier agent i can hope for.Consider the mechanism given as Mechanism 1.Mechanism 1 the Projected Majority mechanism (PM)(cid:2)(cid:8) =Based on the labels of each agent P i , Ni , calculate ci . Define each agent as a negative agent if ci = c−, and as a positive agent if ci = c+.Denote by P(cid:8) (cid:2) Nif Pelse return c−.end ifi:ci =c+ mi the number of examples that belong to positive agents, and similarly Ni:ci =c− mi = m − Pthen return c+.(cid:8) =(cid:2).(cid:8)(cid:8)Remark 2. Informally we state that in our current setting, we can obtain similar approximation results even under mech-anisms that are not SP, assuming agents lie only when this is beneficial to them. Nevertheless, strategyproofness gives usa very clean framework to analyze mechanisms in the face of strategic behavior. When we discuss our learning theoreticframework, where obtaining strategyproofness is next to impossible, we shall apply the former, less elegant, type of analysis.We will show that this mechanism has the excellent game-theoretic property of being group strategyproof : no coalitionof players can gain by lying. In other words, if some agent in the coalition strictly gains from the joint lie, some other agentin the coalition must strictly lose. While technically simple, this first result demonstrates the key principles of strategyproofmechanisms.Theorem 3.2. Mechanism 1 is a 3-approximation group-SP mechanism.Proof. We first show group strategyproofness. Let B ⊆ I . We can assume without loss of generality that either all agents inB are positive or all of them are negative, since a positive (resp., negative) agent cannot gain from lying if the mechanismR. Meir et al. / Artificial Intelligence 186 (2012) 123–156131Fig. 3. The examples of each agent in the three datasets are shown (for t = 2). Agent 1 can make dataset II look like dataset III and vice versa by reportingfalse labels. The same goes for agent 2 regarding datasets I and II.returns c+ (resp., c−). Again without loss of generality, the agents are all positive. Therefore, if some agent is to benefit fromlying, the mechanism has to return c− on the truthful dataset. However, since the mechanism considers all agents in B tobe positive agents when the truthful dataset is given, an agent in B can only hope to influence the outcome by reporting a(cid:8)majority of negative examples. However, this only increases N, reinforcing the mechanism’s decision to return c−.It remains to demonstrate that the approximation ratio is as claimed. We assume without loss of generality that the. We first prove that if the mechanism returned the positive concept, at least 1/4 of(cid:8) (cid:2) N(cid:8)otherwise we would get c = c−. Now, if an agent is positive (ci = c+), at least half of itsmechanism returned c+, i.e., Pthe examples are indeed positive, that is, P (cid:2) 1(cid:8) (cid:2) m2Indeed, clearly P(cid:2) N(cid:8)4 m.examples are also positive. ThusP =(cid:7)i∈IP i (cid:2)(cid:7)(cid:7)P i (cid:2)i:ci=c+i:ci=c+mi2(cid:8)= P2,and hence P (cid:2) P2(cid:2) m4 .(cid:8)P (cid:2) m/2, thenNow, we know that P + N = m, so N = m − P (cid:3) m − ( m(cid:3) 3P . Clearly if the mechanism decided “correctly”, i.e.,4 ) = 3m4RI (c, S) = RI (c+, S) = Nm= r∗.Otherwise, if P < m/2, thenRI (c, S) = RI (c+, S) = Nmm∗In any case we have that RI (c, S) (cid:3) 3r(cid:3) 3P= 3RI (c−, S) = 3r∗., proving that Mechanism 1 is indeed a 3-approximation mechanism. (cid:2)As 3-approximation is achieved by such a trivial mechanism, we would naturally like to know whether it is possible toget a better approximation ratio, without waiving the SP property. We show that this is not the case by proving a matchinglower bound on the best possible approximation ratio achievable by an SP mechanism. Note that the lower bound onlyrequires strategyproofness, not group strategyproofness.Theorem 3.3. Let (cid:2) > 0. There is no (3 − (cid:2))-approximation strategyproof mechanism.Proof. To prove the bound, we present 3 different datasets. We show that any SP mechanism must return the same resulton all of them, while neither concept in C yields an approximation ratio of (3 − (cid:2)) in all three.Let (cid:2) > 0. We will use I = {1, 2}, and an integer t = t((cid:2)) to be defined later. Note that in all three datasets m1 = m2 =2t + 1. We define the three datasets as follows (see Fig. 3 for an illustration):• S I : P 1 = 2t + 1, N1 = 0; P 2 = t, N2 = t + 1,• S II: P 1 = 2t + 1, N1 = 0; P 2 = 0, N2 = 2t + 1,• S III: P 1 = t + 1, N1 = t; P 2 = 0, N2 = 2t + 1.132R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Let M be some strategyproof mechanism. Then it must hold that M(S I ) = M(S II). Indeed, otherwise assume first thatM(S I ) = c+ and M(S II) = c−. Notice that the only difference between the two settings is agent 2’s labels. If agent 2’struthful labels are as in S I , his subjective erm is c−. Therefore, he can report his labels to be as in S II (i.e., all negative) andobtain c−. Now, if M(S I ) = c− and M(S II) = c+, agent 2 can gain by deviating from S II to S I . A symmetric argument, withrespect to agent 1 (that in all settings prefers c+) shows that M(S II) = M(S III).So, without loss of generality assume that c = M(S I ) = M(S II) = M(S III) = c+ (otherwise, symmetric arguments yield thesame result). Therefore:(cid:9)(cid:8)c, S IIIRI= RI(cid:8)c+, S III(cid:9)= N1 + N2m= 3t + 14t + 2.On the other hand, the negative concept is much better:∗ = RIr(cid:8)c−, S III(cid:9)= t + 14t + 2.By combining the last two equations:RI (c, S III)r∗=3t+14t+2t+14t+2= 3t + 1t + 1.(8)Let us set t > 3SP mechanism cannot have an approximation ratio of 3 − (cid:2). (cid:2)(cid:2) ; then the last expression is strictly greater than 3 − (cid:2), and thus RI (c, S III) > (3 − (cid:2))r∗. We conclude that any3.2. Randomized mechanismsWhat if we let our mechanism flip coins? Can we find an SP randomized mechanism that beats (in expectation) the3-approximation deterministic lower bound? To answer the question we first recall the definition of the risk of such amechanism given in (3).For our simple concept class C = {c+, c−}, a randomized mechanism is defined only by the probability of returning apositive or negative concept, given S. Accordingly, the risk (both private and global) is(cid:8)R(cid:9)M(S), S= p+ · R(c+, S) + p− · R(c−, S),where p+, p− stand for pM(c+ | S) and pM(c− | S).We start our investigation of SP randomized mechanisms by establishing a lower bound of 2 on their approximationratio.Theorem 3.4. Let (cid:2) > 0. There is no (2 − (cid:2))-approximation strategyproof randomized mechanism.The proof, along with all the remaining proofs of this section, appears in Appendix A.(cid:8)We presently put forward a randomized SP 2-approximation mechanism, thereby matching the lower bound with anupper bound. However we first propose a simpler mechanism and analyze where it fails: The natural thing to do would beas in our deterministic Projected Majority Mechanism and then simply to select c+ with probabilityto calculate P(cid:8)/m. We refer to this simple mechanism as the weighted random dictator mechanism (WRD),Pfor reasons that will become apparent in Section 4.1.7 Unfortunately, this simple randomization (which is clearly SP) cannoteven beat the deterministic bound of 3 − (cid:2), as demonstrated by the following example.(cid:8)/m and c− with probability Nand N(cid:8)Example 3.5. Consider the dataset S of n agents with the following examples: one agent with P 1 = t + 1, N1 = t, and(cid:8) =n − 1 additional agents each holding 2t + 1 negative examples. Thus P = t + 1; N = (n − 1)(2t + 1) but P(n − 1)(2t + 1). The optimal classifier makes |P | = t + 1 mistakes, thus rm . On the other hand, the expected numberof mistakes made by the mechanism is(cid:8) = 2t + 1; N∗ = t+1(cid:8)m · RIWRD(S), S(cid:9)(cid:8)· (t + 1) + Pm= p− · |P | + p+ · |N| = Nm(t + 1) + 2t + 1= (n − 1)(2t + 1)n(2t + 1)n(2t + 1)+ 2nt + n − t − 1= (n − 1)(t + 1)nn= nt + n − t − 1 + 2nt + n − t − 1n(cid:8)(cid:8)·(n − 1)(2t + 1) + t(cid:9)(2nt + n − t − 1)= 3nt + 2n − 2t − 2n.7 This procedure is equivalent to randomly selecting an agent with probability proportional to its weight, and using its preferred classifier to classify theentire dataset—hence Weighted Random Dictator.R. Meir et al. / Artificial Intelligence 186 (2012) 123–156We have that the approximation ratio of this mechanism is at leastRI (WRD(S), S)r∗= 3nt + 2n − 2t − 2n(t + 1)t→∞→ 3 − 2n.133(9)Thus, for every (cid:2) > 0, there is a large-enough t such that the approximation ratio is worse than 3 − 2n− (cid:2).Note that in this example all agents control datasets of the same size (2t + 1). A similar example can be crafted with twoweighted agents, by merging the datasets of agents 2, . . . , n to a single, heavier, agent. This example will provide us with alower bound of 3 − 2w 1, where w 1 is the weight of the lighter agent.Crucially, an adjusted, less intuitive randomization can do the trick.Mechanism 2 The Square Weighted Dictator Mechanism (SRD)(cid:8)(cid:8)Compute PReturn c+ or c− with probability proportional to (Pas in Mechanism 1.and N(cid:8))2, (N(cid:8))2, respectively.Theorem 3.6. Mechanism 2 is a group-SP 2-approximation randomized mechanism.There are, in fact, multiple ways to achieve a 2-approximation using different randomizations on N. In a previousversion of this paper we suggested one such alternative randomization [28]. A third procedure follows as a special case fromthe CRD mechanism described in Section 4.1.and P(cid:8)(cid:8)3.3. Binary decision in a learning theoretic settingIn this section we extend our simple setting to a more general machine learning framework. Our previous results will beleveraged to obtain powerful learning theoretic results.Instead of looking at a fixed set of examples and selecting the concept that fits them best, we now turn to look atsampled datasets. That is, we assume that there is some fixed and known distribution D X ∈ (cid:4)(X ) (where (cid:4)( A) is the set ofprobability distributions over a set A), which represents the interest that agents have in different parts of the input space.According to our shared input assumption, the distribution of interest is the same for all agents.In addition, each agent i ∈ I now has a private function Y i : X → {+, −}, which assigns a label to every point in theinput space. Observe that Y i , along with the distribution D X , induces a (private) distribution Di over inputs and labels, i.e.,Di ∈ (cid:4)(X × {+, −}). This distribution determines the type of agent i.The new definition of the subjective risk naturally extends the previous setting by expressing the errors a concept makeswith respect to the distribution Di :c(x) (cid:7)= yRi(c) = E(x, y)∼Di(cid:10)(cid:2)(cid:3)(cid:12)(cid:10)(cid:2)= Ex∼D Xc(x) (cid:7)= Y i(x)(cid:3)(cid:12).The global risk is calculated similarly to how it was previously defined, as the weighted average of the private risk, i.e.,RI (c) =w i · Ri(c).(cid:7)i∈IFor ease of exposition, we will assume in this section that all agents have equal weight. Thus, RI (c) = 1nSection 4.3, when discussing the more general problem, we will not use this assumption.8Similarly, we can no longer compare the outcome of our mechanism to r∗(S), as this notion of the optimal risk assumesa fixed dataset, whereas an instance of the learning-theoretic setting consists of a set of distributions. We therefore definethe minimal risk asrmin = infc∈CRI (c).(12)Although in the general case C might be an open set, in our simple two-function setting C is finite, and rmin =min{RI (c−), RI (c+)}.Note that we cannot directly evaluate the risk in this learning theoretic framework; we may only sample points from theagents’ distributions and ask the agents to label them. We then try to minimize the real global risk, using the empirical riskas a proxy.9 The empirical risk is the risk on the sampled dataset, as defined in the previous section.8 The results in this section can also be generalized to varying weights by sampling for each agent a number of points proportional to its weight, yet stilllarge enough.9 This is similar to an oracle model, where we have no direct access to the distribution, but we can ask yes/no questions about it. The major differenceis that in our model the “oracle” may lie! (perhaps the Sphinx model would be a better name).(10)(11)(cid:2)i∈I Ri(c). In134R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Remark 3. A subtle point is that the mechanism we present is not strategyproof, and in fact no mechanism that gets sampleddata points as input is strategyproof. Indeed, even if there is only a single agent, which gives greater weight to negativepoints (according to D1), it might be the case that, by miserable chance, the agent’s sampled dataset only contains positivepoints. Thus there is some non-zero probability that the agent will have an incentive to “lie” by reporting negative labels.We note that even allowing payments would not guarantee strategyproofness in our example, as it contains only oneagent. This fact may seem contradictory to the revelation principle (see, e.g., [32]), but not if we recall that truthful mech-anisms are only guaranteed to exist under direct revelation. In our domain, direct revelation means that the agents must beasked to explicitly select the classifier they prefer. However in the learning-theoretic setting the agents only reveal theirpreferences indirectly, by submitting their preferred labels on the sampled data points.3.3.1. Three game-theoretic assumptionsWhile full strategyproofness is too much to ask for, we can still make assumptions on the behavior of agents that willallow us to formally analyze the outcome of our mechanisms. We exploit this very simple setting to clarify the distinctionbetween three alternative game-theoretic assumptions on agents’ behavior.The (cid:2)-truthfulness assumption The first assumption is that agents will not lie unless their expected gain from this lie is atleast (cid:2). This assumption is stronger than the rationality assumption in the decision-making setting, where we demandedthis only for (cid:2) = 0. In Section 4.3 we refer to this assumption as the “Truthful Approach”. This is the approach taken forexample by Dekel et al. [10].The pure rationality assumption A second assumption is that agents will always play a dominant strategy, if one is availableto them. The existence of dominant strategies depends on the mechanism, as well as on the dataset, and we allow arbitrarybehavior when such a strategy does not exist. This assumption is also stronger than the standard rationality assumption(which does not assume anything about agents’ behavior when truth-telling is suboptimal), but it is incomparable withthe first assumption. In Section 4.3 we refer to this assumption as the “rational approach”. It is important to note that therational approach entails that agents must have complete knowledge of their own distribution. This implicit assumption isnot necessary under the truthful approach.The weak truthfulness assumption The third assumption, which is also the weakest, requires that an agent is truthful if thisis a weakly dominant strategy, i.e., if it cannot gain by lying.An agent that always obeys the first, second or third assumption is called (cid:2)-truthful, purely rational, or weakly truthful,respectively. Note that both (cid:2)-truthful agents (for any (cid:2) (cid:2) 0) and purely rational agents are always weakly truthful, whichmeans that the third assumption is indeed the weakest.In this section we employ the third assumption as it supplies us with the strongest results. Thus the results in this sectionare “stronger” in a way than the results of Dekel et al. [10] (regression) and the results in Section 4 (classification).10Remark 4. We offer a simple scenario that will highlight the substantial difference between the different assumptions.Suppose we employ the pure rationality assumption, and consider the following simple mechanism: sample one pointfrom D X , and let all agents label this single point. If an agent labels the point positively, the agent is positive; otherwise itis negative. Now apply either Mechanism 1 or Mechanism 2. This clearly gives us approximation upper bounds of 3 and 2respectively, using only one sampled data point. In contrast, the (cid:2)-truthfulness assumption will not guarantee anything inthis case. This suggests that the difference between the assumptions is non-trivial. Compare also with the analysis of thetwo first approaches in Section 4.3.Mechanism 3 The Binary Learning Mechanism ( (cid:15)SRD)for each agent i ∈ I do(cid:8) = mi points i.i.d. from D X .Sample mDenote i’s set of data points as Xi = {xi,1, . . . , xi,m(cid:8) }.Ask agent i to label Xi .Denote S i = {(cid:5)xi, j , Y i (xi, j )(cid:6)}m(cid:8)j=1.end forUse Mechanism 2 on S = {S 1, . . . , Sn}, return SRD(S).10 In fact, a simple variant of the proofs in Section 4.3 could be directly applied to the binary decision problem (as it is a special case of shared inputs,and has a bounded VC dimension), yielding an approximation ratio that is close to 2. However, this bound would only hold under either of the first twostrategic assumptions.R. Meir et al. / Artificial Intelligence 186 (2012) 123–156135The risk of the mechanism is computed as the expectation of the risk of the outcome classifier, where the expectation istaken over both randomizations: the sampling of the data points, and the randomization performed by SRD. Formally (forboth private and global risk),(cid:10)R( (cid:15)SRD) = E X∼(D X )m(cid:8)RSRD(S)(cid:9)(cid:12),(13)where the labels of X in S are set according to our strategic assumptions.We presently establish a theorem that explicitly states the number of examples we need to sample in order to properlyestimate the real risk. We will get that, in expectation (taken over the randomness of the sampling procedure and Mecha-nism 2’s randomization), Mechanism 3 yields close to a 2-approximation with relatively few examples, even in the face ofstrategic behavior.Theorem 3.7. Given sampled datasets, assume weak truthfulness. For any (cid:2) > 0, there is msampling mpoints for each agent, it holds that(cid:8)(cid:8)(polynomial in ln(n) and 1(cid:2) ) such that byRI ( (cid:15)SRD) (cid:3) 2rmin + (cid:2).(cid:8) > 50 1(cid:2)2 ln( 10nSpecifically, sampling mWhile the proof is quite technical, it can be sketched as follows. Mechanism 2 is SP with respect to the (already sampled)dataset S. Thus if an agent’s sampled dataset faithfully represents its true distribution, and the agent is strongly inclinedtowards c+ or c−, the agent still cannot benefit by lying (by the weak truthfulness assumption). If an agent is almostindifferent between c+ and c−, it might wish to lie—but crucially, such an agent contributes little to the global risk.(cid:2) ) will suffice.4. Classification with shared inputsWe begin with an analysis of the decision-theoretic setting. As in Section 3, these results will later be applied to thelearning-theoretic setting.In this section, we assume that all agents control the same set of data points. The size of this dataset is denoted by k.The total number of labeled data points from all agents is thus m = n · k. However, as our mechanisms in this section useonly a single agent, k is effectively the size of the input being used.4.1. Deterministic mechanismsWe start by examining an extremely simple deterministic mechanism. Recall that erm(S(cid:8)) is the concept c ∈ C that(cid:8) ⊆ S (see Eq. (4)). Our mechanism simply lets the heaviest agent dictate which concept is chosen.minimizes the risk w.r.t. SMechanism 4 The Heaviest Dictator Mechanism (HD)h ← argmaxi∈I w i . // (Let h ∈ I be an agent with maximal weight)return erm(Sh).If more than one erm exists, return one of them arbitrarily. The mechanism is clearly SP: the heaviest dictator h has nointerest to lie, since its best concept is selected; all other agents are simply ignored, and therefore have no reason to lieeither. We have the following result.Theorem 4.1. Let |I| = n. For every concept class C and any dataset S, Mechanism 4 is an SP (2n − 1)-approximation mechanism.Recall the central negative result regarding deterministic mechanisms with non-restricted input.Theorem 4.2 (Meir, Procaccia, and Rosenschein [29]). There exist concept classes for which any deterministic SP mechanism has anapproximation ratio of at least Ω(m), where m is the total size of the full dataset.We therefore see that the restriction to shared inputs helps by removing the dependency on the size of the dataset, butnevertheless an approximation ratio that increases linearly with the number of agents is not very appealing. However, itturns out that using deterministic mechanisms we cannot do better with respect to every concept class. Indeed, a slightvariation of Theorem 4.2 gives us the following result.Theorem 4.3. Suppose there are n agents with shared inputs. There exist concept classes for which any deterministic SP mechanismhas an approximation ratio of at least Ω(n), even if all the weights are equal.136R. Meir et al. / Artificial Intelligence 186 (2012) 123–156The proof of the theorem is a minor variation of the proof of Theorem 4.2, which applies the Gibbard–Satterthwaiteimpossibility theorem [17,38].Theorem 4.3 implies that Mechanism 4 is optimal, up to a constant, as a generic mechanism that applies to any conceptclass. Of course, for specific concept classes one can do much better, as shown in Section 3. One could hope that imposingfurther restrictions on the dataset, such as realizability, would enable the design of better SP mechanisms. However, recentresults show that the Ω(n) bound remains even if all datasets are realizable [13].4.2. Randomized mechanismsIn order to break the lower bound given by Theorem 4.3, we employ a simple randomization. We will see that thisrandomization yields a constant approximation ratio with respect to any concept class (under our assumption of shared inputs,of course). Moreover, if the agents have uniform weights, then this mechanism cannot be further improved.Mechanism 5 The Weighted Random Dictator (WRD) mechanismselect agent i with probability w i .return erm(S i ).Consider Mechanism 5, which is clearly SP. The following theorem bounds its approximation ratio for different cases.Theorem 4.4. For every concept class C and for any dataset S, Mechanism 5 is an SP (3 − 2w min)-approximation mechanism, wherew min = mini∈I w i . Moreover, if S is individually realizable, then (2 − 2w min)-approximation is guaranteed.When all agents have the same weight, we have that w min = 1n . We therefore have the following corollary which followsdirectly from Theorem 4.4.Corollary 4.5. Let |I| = n, and assume all agents have equal weights. For every concept class C and for any dataset S, Mechanism 5 isan SP (3 − 2n when S is individually realizable).n )-approximation mechanism (2 − 2The last corollary also follows as a special case from results we will see in Section 4.2.2.It is possible to show that the analysis of Mechanism 5 is tight. Indeed, consider the outcome of the mechanism for theconcept class {c−, c+}. In this case, the mechanism is essentially equivalent to the naive randomized mechanism presentedin Section 3.2, and yields the same outcome. Therefore, Example 3.5 gives a tight lower bound on the approximation ratioof the mechanism, matching the upper bound given in Theorems 4.4 and 4.5. A similar example can be easily constructedfor every concept class of size at least two.4.2.1. Is the WRD mechanism optimal?It is natural to ask whether better (randomized) SP mechanisms exist. For specific concept classes, the answer to thisquestion is positive, as demonstrated by Theorem 3.6. For general concept classes, the following lower bound is known.Theorem 4.6 (Meir, Almagor, Michaely and Rosenschein [27]). Suppose there are n agents with shared inputs. There exist conceptclasses for which any randomized SP mechanism has an approximation ratio of at least 3 − 2n , even if all the weights are equal.Theorem 4.6 shows that when weights are uniform, the WRD mechanism (i.e., selecting a dictator uniformly at random)is in fact optimal. That is, no SP mechanism can do better. However, the mechanism is suboptimal for weighted datasets, asit only guarantees a 3 approximation in this case.We next turn to close this gap, presenting new mechanisms that beat the WRD mechanism on weighted datasets,matching the lower bound given in Theorem 4.6.4.2.2. Improving the upper bound for weighted agentsTheorem 4.6 in fact tells us that we must pick a dictator at random to have an SP mechanism. However we are still freeto define the probabilities of selecting different agents, and we may take agents’ weights into account. The WRD mechanismis an example of such a randomization, but we can design others.Recall that in the two-function scenario, we performed an optimal randomization by using the SRD mechanism. As afirst attempt to improve the upper bound, we translate the SRD mechanism to the current setting.11 That is, the mech-anism would select every dictator i ∈ I with probability proportional to w 2i . Unfortunately, while SRD does attain someimprovement over the WRD mechanism, it is still suboptimal, even for n = 3.11 We slightly abuse notation here and use the name SRD, although it is no longer equivalent to Mechanism 2.R. Meir et al. / Artificial Intelligence 186 (2012) 123–156137Proposition 4.7. There is a dataset S with three agents, such that(cid:8)RISRD(S), S(cid:9)> 2.4 · r∗>(cid:16)3 − 2n(cid:17)∗.rA similar counterexample exists for individually realizable datasets, where the approximation ratio of SRD is above 1.39n for n = 3). We therefore must take a somewhat different approach in the selection of the dictator.(i.e., strictly above 2 − 2Consider the mechanisms CRD and RRD, where the latter is a small variation of the former.Mechanism 6 The Convex-weight Random Dictator Mechanism (CRD)(cid:8)i.2−2w i= w ifor each i ∈ I , set pcompute αw = 1(cid:2)i∈I pselect agent i with probability pi = αw preturn erm(S i ).(cid:8)i.(cid:8)i .Mechanism 7 The Realizable-weight Random Dictator Mechanism (RRD)h ← argmaxi∈I w i .if wh (cid:2) 12 thenreturn erm(Sh).(cid:8)i= w iend iffor each i ∈ I , set pcompute βw = 1(cid:2)i∈I pselect agent i with probability pi = βw preturn erm(S i ).1−2w i(cid:8)i..(cid:8)i .The CRD and RRD mechanisms are clearly SP, as the probabilities are unaffected by the reported labels.Theorem 4.8. The following hold for Mechanism 6:• αw (cid:3) 2 − 2n .• CRD has an approximation ratio of 1 + αw, i.e., at most 3 − 2n .• if S is individually realizable, then the approximation ratio is αw2+ 1, i.e., at most 2 − 1n .By Theorem 4.6, no SP mechanism can do better on a general dataset in the worst case, thus CRD is optimal. However,if the dataset is known to be individually realizable, CRD is suboptimal, and RRD is strictly better (in the worst case).Theorem 4.9. The following hold for Mechanism 7:• βw (cid:3) 1 − 2n .• RRD has an approximation ratio of at most 4, and at least 3 (in the worst case).• if S is individually realizable, then the approximation ratio is 1 + βw, i.e., at most 2 − 2n .Observe that for two agents the RRD simply selects the heavier dictator. Thus if the dataset is not realizable, the approx-imation ratio can be as high as 3, which accounts for the lower bound in the non-realizable case.The CRD mechanism matches the lower bounds for any set of weighted agents, thereby showing that the uniform weightcase is, in fact, the hardest. The situation with the RRD mechanism is similar—no randomization of dictators can do better.However, it is still an open question whether there are better, more sophisticated, randomized mechanisms for the realizablecase. The natural conjecture would be that there are none, as Dokow et al. proved for deterministic mechanisms [13].Note that when weights are uniform, then the CRD, RRD, SRD and WRD mechanisms all coincide.12 Thus Theorem 4.5also follows as a special case from Theorems 4.8, 4.9.Curiously, RRD is better than CRD when the dataset in known to be realizable, whereas in the general case the converseis true. Therefore, a different mechanism should be used, depending on our assumptions on the dataset. However, themechanism must be decided on a-priori—we cannot select between CRD and RRD after observing the labels, as this wouldnot be strategyproof!12 There is a tiny exception here: when n = 2, w 1 = w 2 = 1outcome is a 1-approximation.2 , then RRD returns an arbitrary dictator, rather than random. However in this case any138R. Meir et al. / Artificial Intelligence 186 (2012) 123–1564.2.3. Applying the mechanisms to the two-function settingSuppose that C = {+, −}. We can join together all positive agents, and all negative agents, and construct an instance(cid:8), Nwith two meta-agents, whose weights are proportional to P(as defined in Section 3.1). The RRD mechanism thensimply selects the heavier meta-agent (equivalently to the PM mechanism), and thus guarantees an approximation ratioof 3. The CRD mechanism, applied to this setting, guarantees an approximation ratio of 3 − 2= 2. It thereforensupplies us with an alternative 2-approximation SP mechanism for the two-function setting.= 3 − 22(cid:8)4.3. The learning-theoretic settingIn this section we leverage the upper bounds which were attained in the decision-theoretic setting to obtain results ina machine-learning framework. That is, we present a learning mechanism that guarantees a constant approximation of theoptimal risk in expectation, even in the face of strategic behavior.We use the notations and definitions introduced in Section 3.3, where the preferences of each agent are represented bya function Y i : X → {+, −}.13 Reinterpreting our shared input assumption in the learning-theoretic setting, we assume thatall agents have the same probability distribution D X over X , which reflects the relative importance that the agents attributeto different input points; the distribution D X is common knowledge.The private risk of a classifier c ∈ C is computed according to Eq. (10):(cid:3)(cid:12)(cid:10)(cid:2)Ri(c) = Ex∼D Xc(x) (cid:7)= Y i(x).That is, according to the expected number of errors that c makes w.r.t. the distribution D X . As for the global risk, it iscomputed according to Eq. (11), i.e.RI (c) =(cid:7)i∈Iw iRi(c).The goal of our mechanisms is to find classifiers with low risk. We therefore compare them to the best risk that isattainable by concepts in C, and thus rmin = infc∈C RI (c). Eq. (12) is a special case of this definition for C = {c−, c+}.Our goal is, once again, to design mechanisms with risk close to optimal. However, constructing an SP mechanismthat learns from sampled data is nearly impossible (as explained in Remark 3). Hence, we weaken the strategyproofnessrequirement, and analyze the performance of our mechanisms under each of the first two strategic assumptions describedin Section 3.3: the (cid:2)-truthfulness assumption, which states that agents do not lie unless they gain at least (cid:2); and the purerationality assumption, under which agents always play a weakly dominant strategy if one exists.4.3.1. The (cid:2)-truthfulness assumptionAn (cid:2)-strategyproof mechanism is one where agents cannot gain more than (cid:2) by lying. We show below that, similarly toDekel et al. [10], the results of Section 4.2 can be employed to obtain a mechanism that is “usually” (cid:2)-strategyproof. Wefocus on the following mechanism.Mechanism 8 The Generic Learning Mechanism ( (cid:15)CRD)Sample k data points i.i.d. from D X (denote the sampled points by X ).for each agent i ∈ I doAsk agent i to label Xi .Denote S i = {(cid:5)x j , Y i (x j )(cid:6)}kj=1.end forUse Mechanism 5 on S = {S 1, . . . , Sn}, return CRD(S).We denote by RI ( (cid:15)CRD) the expected risk of Mechanism 8, where the expectation is taken over the randomness of thesampling and the randomness of Mechanism 5, just as in Eq. (13) in the two-function setting:R( (cid:15)CRD) = EX∼(D X )k(cid:8)(cid:10)RCRD(S)(cid:9)(cid:12),where the labels of X in S are set according to our varying strategic assumptions.We wish to formulate a theorem that asserts that, given enough samples, the expected risk of Mechanism 8 is relativelysmall under the (cid:2)-truthfulness assumption. The exact number of samples needed depends on the combinatorial richnessof the function class; this is usually measured using some notion of class complexity, such as the VC dimension (see, e.g.,[21]). For instance, the VC dimension of the class of linear separators over Rd is d + 1. We do not dwell on this point toomuch, and instead assume that the dimension is bounded.13 As with the theorems in Section 4.1, our results in this section will follow as a special case from the more general model, where agents have distribu-tions over the labels.R. Meir et al. / Artificial Intelligence 186 (2012) 123–156139Theorem 4.10. Assume all agents are (cid:2)-truthful, and let C be any concept class with a bounded dimension. For any (cid:2) > 0, there is kn ) · rmin + (cid:2).(polynomial in 1(cid:2) and ln(n)) s.t. if at least k data points are sampled, then the expected risk of Mechanism 8 is at most (3 − 2The proof sketch is as follows:(a) There is a high probability that the random sample is “good”, i.e., close to the actual interest of the agents.(b) Whenever the sample is good for some agent, this agent will report truthfully (under the (cid:2)-truthfulness assumption).(c) When the sample is good for all agents, the risk of Mechanism 8 is close to the risk of Mechanism 5, and thus we havealmost a (3 − 2n )-approximation.(d) Otherwise the risk can be high, but this has a small effect on the total expected risk, as it occurs with low probability.We prove Theorem 4.10 along these lines in Appendix B.2, and supply an exact upper bound on the number of samplesrequired for the theorem to hold.4.3.2. The pure rationality assumptionRecall that under the pure rationality assumption, an agent will always use a dominant strategy, when one exists. Weonce again consider the performance of Mechanism 8. Note that since our mechanism uses a dictator, each agent i has aweakly dominant strategy. In order to see that, observe that there is some classifier ˆci that minimizes the risk w.r.t. thewhole distribution Di .14 The dominant strategy of agent i is to label the sampled dataset X according to ˆci . Note that thisdoes not mean that i is being truthful, as it is possible that ˆci(x) (cid:7)= Y i(x) (see Remark 3).Theorem 4.11. Assume all agents are purely rational, and let C be any concept class with a bounded dimension. For any (cid:2) > 0, there isn ) · rmin + (cid:2).k (polynomial only in 1(cid:2) ) s.t. if at least k data points are sampled, then the expected risk of Mechanism 8 is at most (3 − 2Interestingly, the alternative assumption improved the sample complexity: the number of required samples no longerdepends on n, only on 1(cid:2) . In a somewhat counter-intuitive way, the rationality assumption provides us with better boundswithout using the notion of truthfulness at all. This can be explained by the fact that a rational (i.e., self-interested) labelingof the dataset is a better proxy to an agent’s real type than a truthful labeling. Indeed, this strange claim is true since thesampling process might produce a set of points X that represents the agent’s distribution in an inaccurate way.155. DiscussionWe first review our results in the decision making setting, then in the learning theoretic setting, and finally present somedirections for future research.5.1. Decision making settingWe started by studying the simple case where there are only two possible decisions. In this setting there is an almosttrivial mechanism that is group strategyproof, and guarantees a 3-approximation ratio. While there are no better determinis-tic mechanisms, we showed how a specific randomization can be used to achieve a 2-approximation ratio, while maintainingthe group-SP property.For the more general case, we showed that a simple randomization of the dictator (the WRD mechanism) achievesthe best possible approximation ratio when agents have uniform weights, but falls short in the weighted case. We thenpresented a new mechanism that closes this gap and obtains optimal approximation results in the general case (CRD).In the weighted realizable case, we presented a mechanism that matches the best known results with uniform weights.However it is still an open question whether this bound is tight, as no non-trivial lower bounds are known.We showed that these approximation results stand in sharp contrast to the deterministic case, where no deterministicmechanism can guarantee a constant approximation ratio. The trivial selection of the heaviest agent as a dictator is the bestdeterministic SP mechanism at hand. Results also highlight the power of the shared inputs assumption, as they allow us tobreak the lower bounds that hold in the general case [29].All these results (summarized in Tables 1 and 2) may help decision makers—both human and automated—in reaching adecision that approximately maximizes social welfare, when data might be biased by conflicting interests.5.1.1. Implications for facility locationAs we hinted in the introduction, our classification model can be seen as facility location in metric spaces, where then bound in Theorem 4.5 follows directly from a folk result inparticular space that we use is the binary cube. In fact, the 2 − 214 There is a fine issue here regarding the finiteness of the concept class, that we deal with in the proof.15 As we explained in Remark 3, the revelation principle does not apply here, since the agents do not report their full preferences.140R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Table 1Summary of results (deterministic mechanisms). The corresponding theorem for each result appears in parentheses.HDPMLower boundAll classes (shared inputs)General datasetsO (n) (Th. 4.1)–Ω(n) (Th. 4.3)Realizable datasets⇒ O (n)–Ω(n) [13]Binary decision⇒ O (n)3 (Th. 3.2)3 (Th. 3.3)Table 2Summary of results (randomized mechanisms). We conjecture that the upper bound for realizable datasets is tight, but this remains an open question.WRDSRDCRDRRDBest upper boundLower boundAll classes (shared inputs)General datasets3 (Th. 4.4)> 2.4 (Prop. 4.7)3 − 2n (Th. 4.8)(cid:2) 3 (Th. 4.9)3 − 23 − 2n (CRD)n (Th. 4.6 [27])Realizable datasets2 (Th. 4.4)> 1.392 − 12 − 2n (Th. 4.8)n (Th. 4.9)2 − 2n (RRD)?Binary decision⇒ 32 (Th. 3.6)232 (SRD, CRD)2 (Th. 3.4)facility location, and has been employed, for example, by Alon et al. [2]. We will next describe our results in the decision-theoretic setting in the wider context of metric spaces, thereby extending and generalizing the mentioned folk theorem.Let (cid:5)F , d(cid:6) be a metric space.16 Let F = { f 1, . . . , fn} be a finite set of points in F , where each point f i has an attached∗ ∈ F be theweight w i reflecting its importance. Define d( f , F ) as the (weighted) average distance from f to F , and let fpoint that minimizes this distance, i.e.,f∗ = argminf ∈Fd( f , F ) = argminf ∈F(cid:7)w id( f , f i).i(cid:3)nWe are interested in selecting one of the points in F , that will be as close as possible to all other points. The restrictionis that this selection must be “blind”. That is, we must select without knowing the actual distances. All we know are theweights of the n points. Clearly, if weights are uniform, one can do no better than simply picking a random point in F . Thefollowing inequality, which is a folk theorem, bounds the expected distance achieved in this process.(cid:8)df∗(cid:9), F.(14)1n(cid:7)i(cid:3)nd( f i, F ) (cid:3)(cid:17)(cid:16)2 − 2nAs we informally explained before, the upper bounds on the approximation ratio of the WRD mechanism (e.g., therealizable part of Theorem 4.5) can be derived from Eq. (14) by defining a metric over classifiers, reflecting the fractionof the data space on which they disagree. In the uniform-weight, realizable case, the WRD mechanism picks an agent atrandom, and thus its risk is exactly the average distance between each agent’s optimal classifier and the other agents. Theformal details appear in Appendix B, where we also supply an analog for the non-realizable case, and extend our bounds toweighted agents.Moreover, the full proofs show that all mechanisms of Section 4 attain the specified approximation ratios in a moregeneral model, where the private labels are non-deterministic, and datasets are given in the form of a (private) distributionover X × {+, −}.17 The theorems in Section 4, under the standard model we presented (with deterministic labels), followas a special case.5.1.2. Implications for partition and judgment aggregationGiven a subset X of Rd (and in particular an interval), partitions of X just form another metric space. Informally, thedistance between two partitions is exactly the volume they disagree on. The set of all partitions that are allowed constitutesthe concept class C.A similar approach to the Judgment aggregation problem requires some additional assumptions, since issues on theagenda cannot always be directly compared and quantified. We will clarify this using the following simple example (theDoctrinal paradox, see e.g. [14]): The agenda contains the three logical expressions X = (a, b, a ∧ b). Legal assignments arethose that are also logically consistent (e.g., (1, 1, 1) is legal, but (1, 1, 0) is not). We can therefore naturally define C as the16 It is in fact sufficient to assume that d is a pseudo-metric, i.e., it is possible that d( f , f17 The datasets in Section 3 can be viewed as a single data point with non-deterministic labels. The probabilities of a positive/negative label for agent iare proportional to P i and Ni , respectively..(cid:8)) = 0 for f (cid:7)= f(cid:8)R. Meir et al. / Artificial Intelligence 186 (2012) 123–156141set of all legal assignments (|C| = 4 in this case). The “dataset” S then contains the opinion of every judge over the correctassignment. Consistency of the judges’ opinions coincides with the requirement that S is individually realizable. The subtleissue is that a-priori, there is no reason to say that, for example, (1, 1, 0) is closer to (1, 1, 1) than to (0, 0, 0). Howeverif we assign a fixed weight to every issue on the agenda (that all judges can agree on) then we have a natural metric,and we are back at the “shared input” setting of Section 4. Our suggested mechanisms can therefore be used to randomizea legal assignment that is close—on average—to the opinions of the judges. It is important to note however that if thejudges disagree on the importance of certain issues, then approximation is not well-defined, and even strategyproofness isno longer guaranteed.Dokow and Holzman [14] characterized those agendas for which (deterministic) non-dictatorial aggregation rules exist.18Our randomizations guarantee a constant bound on the social welfare under any agenda, but it is likely that under somefamilies of agendas (such as those characterized by Dokow and Holzman), an even better outcome can be guaranteed.We should mention in this context a recent paper by Nehama [31], which studies approximate judgment aggregationrules from a different angle, without considering incentives or welfare at all. Rather, the paper characterizes rules whoseproperties (e.g., consistency) only approximately hold. We hope to explore the applicability of similar relaxations to otherdomains in our future work.5.2. Learning-theoretic settingIn all cases where a constant upper bound on the approximation ratio was available, we showed how to use the SPdecision mechanism to implement learning mechanisms with a bounded expected risk. More precisely, our mechanismssample a finite number of data points from a given distribution, which are thereafter labeled by self-interested agents. Theexpected risk of the mechanism (where expectation is taken over both sampling procedure and internal randomization) iscompared to the expected risk (over the given distribution) of the best classifier in the concept class. This allows us toachieve an approximation ratio that is arbitrarily close to the approximation guaranteed in the decision theoretic setting:2 when there are only two classifiers, and 3 − 2n when there are more (provided that all agents sample from the samedistribution). When the optimal risk itself is high (say, above 5–10%) then such results are not very useful. With lowoptimal risk, a constant approximation ratio of 2 or 3 is quite good, especially since it applies across all concept classes andall distributions.We made a distinction between alternative game-theoretic assumptions on agents’ behavior, showing how the differentassumptions affect the mechanism and the number of required samples.Our results in the learning theoretic setting contribute to the design of algorithms that can function well in non-cooperative environments. We also promote understanding of the underlying assumptions on agents’ behavior in suchenvironments, and how these may affect the learning process.5.3. Future workFuture research may provide answers to some of the questions we left open, and expand this young hybrid field in newdirections. More efficient SP mechanisms may be crafted to handle specific concept classes. Further extensions of the SPclassification model we presented may be considered: formalizations other than the PAC-like one we suggested; differentloss functions; alternative game-theoretic assumptions as well as restrictions on the structure of the dataset. It is alsopossible to alter the model by allowing different types of strategic behavior, such as misreporting the location of the datapoints rather than their labels.All of these directions may reveal new parts of the overall picture and promote a better understanding of the conditionsunder which SP learning can take place effectively. This, in turn, might supply us with new insights regarding our resultsand regarding their relationship to other areas.AcknowledgementsThis work was partially supported by Israel Science Foundation grant #898/05, the Israel Ministry of Science and Tech-nology grant #3-6797, and the Google Inter-University Center for Electronic Markets and Auctions. The authors thank OmriAbend, Shaull Almagor, Assaf Michaely and Ilan Nehama for their enlightening comments on drafts of this paper.Appendix A. Proofs of Section 3Theorem 3.4. Let (cid:2) > 0. There is no (2 − (cid:2))-approximation strategyproof randomized mechanism.Proof. We will use the same datasets used in the proof of Theorem 3.3, and illustrated in Fig. 3. Let M be an SP randomizedmechanism, and denote by pM(c | S) its probability of outputting c given S.We first show that the mechanism chooses the positive hypothesis with the same probability in all three datasets.18 Dokow and Holzman [14] did not require strategyproofness, but different properties that are closely related.142R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Lemma A.1. pM(c+ | S I ) = pM(c+ | S II) = pM(c+ | S III).Proof. As in the proof of Theorem 3.3, the agents can make one dataset look like another dataset. If pM(c+ | S I ) (cid:7)= pM(c+ |S II) then agent 2 will report its labels in a way that guarantees a higher probability of c−. Similarly, pM(c+ | S II) (cid:7)= pM(c+ |S III) implies that agent 1 can increase the probability of c+ by lying. (cid:2)Denotep+ = pM(cid:8)c+(cid:11)(cid:11) S I(cid:9)(cid:8)c+(cid:9)(cid:11)(cid:11) S II(cid:8)c+(cid:9)(cid:11)(cid:11) S III,= pM= pMandp− = pM(cid:8)c−(cid:11)(cid:11) S I(cid:9)(cid:8)c−(cid:9)(cid:11)(cid:11) S II(cid:8)c−(cid:9)(cid:11)(cid:11) S III.= pM= pMWithout loss of generality p+ (cid:2) 12(cid:8)(cid:8)M(cid:9)(cid:9)S III, S III(cid:8)= p+RIc+, S III= p+ · 3t + 14t + 2(cid:9)(cid:2) p−. Then:(cid:8)(cid:9)c−, S III(cid:2) 12+ p−RI+ p− · t + 14t + 2RIwhereas· 3t + 14t + 2+ 12· t + 14t + 2= 12,∗ = RIr(cid:8)c−, S III(cid:9)= t + 14t + 2.For t > 1(cid:2) it holds thatRI (M(S III), S III)r∗= 2 − 1t + 1As before, if p− > p+, a symmetric argument shows that RI (M(S I ), S I ) > (2 − (cid:2))r> 2 − (cid:2).= 4t + 22(t + 1)achieve a (2 − (cid:2))-approximation, even through randomization. (cid:2)∗. Therefore no SP mechanism canTheorem 3.6. Mechanism 2 is a group strategyproof 2-approximation randomized mechanism.Proof. Similarly to Mechanism 1, Mechanism 2 is clearly group SP, since declaring a false label may only increase theprobability of obtaining a classifier that labels correctly less than half of the agent’s examples, thus increasing the subjectiveexpected risk.Assume without loss of generality that N (cid:2) P , i.e., that the negative classifier c− is better. Denote by w = N(cid:8)m the totalweight of all agents that support c−.Lemma A.2. 1 − r∗ (cid:3) 1+w1−w r∗.Proof. The largest possible number of negative examples is achieved when all the negative agents control only negativeexamples, and all the positive agents control only a slight majority of positive labels. Formally, N (cid:3) N(cid:8) + P(cid:8)2 , and thus:1 − r(cid:3) N∗ = RI (c+) = Nmm∗ = 1 − (1 − r(cid:8)(cid:8)+ P2m∗) (cid:2) 1−w= w + 1 − w= 1 + w22 . By dividing the two inequalities, 1−r2.∗It must follow that rr∗ (cid:3) 1+w1−w ; thus the lemma is proved. (cid:2)(cid:8)(cid:9)RISRD(S), S= w 2r= w 2RI (c−, S) + (1 − w)2RI (c+, S)w 2 + (1 − w)2∗ + (1 − w)2(1 − r∗)w 2 + (1 − w)2∗ + (1 − w)(1 + w)rw 2 + (1 − w)2∗∗ = 2r= w 2rw 2r=(cid:3)r∗,(cid:3) 11/2∗ + (1 − w)2 1+w1−w rw 2 + (1 − w)2∗(from Lemma A.2)12w 2 − 2w + 1∗rwhere the last inequality holds since 2w 2 − 2w + 1 has a minimum in w = 12 . (cid:2)R. Meir et al. / Artificial Intelligence 186 (2012) 123–156143Theorem 3.7. Given sampled datasets, assume weak truthfulness. For any (cid:2) > 0, there is msampling mpoints for each agent, it holds that(cid:8)(cid:8)(polynomial in ln(n) and 1(cid:2) ) such that byRI ( (cid:15)SRD) (cid:3) 2rmin + (cid:2).Specifically, sampling m(cid:8) > 50 1(cid:2)2 ln( 10n(cid:2) ) will suffice.Proof. In this proof we will differentiate the real risk, as defined for the learning-theoretic setting, from the empirical riskon a given sample, as defined in the simple setting. The empirical risk will be denoted byˆRI (c, S) = 1m(cid:7)(cid:2)c(x) (cid:7)= y(cid:3).(cid:5)x, y(cid:6)∈SAlso, to simplify notation we replace (cid:15)SRD with just M throughout the proof. Note that M can equally stand for any othergroup strategyproof 2-approximation mechanism (including CRD, and the mechanism presented in [28]).Without loss of generality we assume that r∗ = RI (c−) < RI (c+). Notice that if rconcept our mechanism returns will trivially attain a risk of at most 12rest of this proof that+ 3(cid:2) (cid:3) rRI (c−) + 3(cid:2) (cid:3) 12(cid:3) RI (c+) − 3(cid:2).∗ = RI (c− 3(cid:2) then any∗ + 6(cid:2). Therefore, we can assume for the∗) = RI (c−) > 12(15)Let us introduce some new notations and definitions. Denote the data set with the real labels by S i = {(cid:5)xi, j, Y i(xi, j)(cid:6)} j(cid:3)m(cid:8) ;S = {S1, . . . , Sn}. Note that the mechanism has no direct access to S, but only to the reported labels as they appear in S.Define G as the event “the empirical and real risk differ by at most (cid:2) for all agents”; formally:(cid:11)(cid:11)(cid:11) < (cid:2).(cid:11) ˆRi(c, S i) − Ri(c)∀c ∈ {c+, c−}, ∀i ∈ I,(16)Lemma A.3. Let δ > 0. If m(cid:8) > 12(cid:2)2 ln( 2nδ ), then with probability of at least 1 − δ, G occurs.Proof. Fix i ∈ I . Consider the event Y i(x) = +, and its indicator random variable [[Y i(x) = +]]. We can rewrite the empiricaland real risk as the sum and the expectation of this variable:(cid:10)(cid:12)[[ y = +]](cid:7)Y i(x) = +(cid:7)(cid:2)(cid:10)(cid:2)(cid:3)(cid:12),Ri(c−) = Ex∼D XˆRi(c−, S i) = 1m(cid:8)(cid:3)Y i(x) = += E(x, y)∼Di= 1m(cid:8)(x, y)∈S i(x, y)∈S i[[ y = +]].Since S i is sampled i.i.d. from Di , the empirical risk is the sum of independent Bernoulli random variables with expectationRi(c−). We derive from the Chernoff bound that for any data set of size |S i| = m:(cid:8)Pr(cid:10)(cid:11)(cid:11)(cid:11) ˆRi(c−, S i) − Ri(c−)(cid:11) > (cid:2)(cid:8) > 1Taking m(cid:12)< 2e−2(cid:2)2m(cid:8).2(cid:2)2 ln( 2nδ ), we get:(cid:11)(cid:11)(cid:10)(cid:11) > (cid:2)(cid:11) ˆRi(c−, S i) − Ri(c−)∃i ∈ I,(cid:10)(cid:11)(cid:11)(cid:12)(cid:11) ˆRi(c−, S i) − Ri(c−)(cid:11) > (cid:2)Pr(cid:3)(cid:12)Pr[¬G] = Pr(cid:7)i∈Iwhere the first inequality is due to the union bound. (cid:2)Note that since(cid:11)(cid:11)(cid:11) =(cid:11) ˆRi(c−, S i) − Ri(c−)(cid:11)(cid:11)(cid:11),(cid:11) ˆRi(c+, S i) − Ri(c+)(cid:3) |I|2e−2(cid:2)2m(cid:8)< nδn= δ,it is enough to show the above for c−.If G occurs, then from (16) and the triangle inequality it holds that for all c ∈ {c+, c−} and i ∈ I ,(cid:11)(cid:11)(cid:11)RI (c) − ˆRI (c, S)(cid:11) (cid:3)(cid:7)i∈I1n(cid:11)(cid:11)(cid:11)Ri(c) − ˆRi(c, S)(cid:11) (cid:3) (cid:2).(17)Using (17) we could have bounded the risk of M(S), but unfortunately this would not do as the mechanism may onlyaccess S and not S. In order to bound RI (M(S)), we need to know, or estimate, how the agents label their examples. To144R. Meir et al. / Artificial Intelligence 186 (2012) 123–156handle this problem, we will first analyze which agents may gain by lying, and then define a new data set ˜S with thefollowing two properties: no agent has motivation to lie (thus we can assess the result of running M on ˜S), and ˜S, S arevery similar.We now divide I into two types of agents: IP i, Ni the number of positive/negative examples the agent controls in S i . Note that P i = mwe may assume without loss of generality that all agents i ∈ Iour mechanism). Agents in IMechanism 2 (which is used by Mechanism 3 in step 3) is SP.For each agent i define a new set of examples ˜S i as follows:. For each agent i ∈ I , we denote by(cid:8) ˆRi(c−, S i). Since RI (c−) < RI (c+)prefer c+ (otherwise lying only lowers the expected risk of, on the other hand, cannot benefit by lying, since S i must reflect i’s truthful preferences, and| < (cid:2)}, and I(cid:8) = {i ∈ I: |Ri(c−) − 1(cid:8)(cid:8) = I \ I(cid:8)(cid:8)2(cid:8)(cid:8)• If i ∈ I• If i ∈ I, ˜S i = S i .(cid:8)(cid:8), define ˜P i = P i + (cid:17)(cid:2)m(cid:8)(cid:8)(cid:18) and let ˜S i contain ˜P i positive examples and m(cid:8) − ˜P i negative ones.Lemma A.4. If G occurs, then for all agents in I˜Ni (cid:3) ˜P i ⇐⇒ Ri(c−) (cid:2) Ri(c+).Proof. If i ∈ I(cid:8)(cid:8)then w.l.o.g. Ri(c−) (cid:3) Ri(c+) − 2(cid:2), thus from (16)˜P i = P i = m(cid:8) ˆRi(c−, S i) (cid:3) m(cid:8)(cid:8)Ri(c−) + (cid:2)(cid:9)(cid:8)(cid:8)(cid:3) mRi(c+) − (cid:2)(cid:9)(cid:3) m(cid:8) ˆRi(c+, S i) = Ni = ˜Ni.If i ∈ I(cid:8)then according to our assumptionRi(c+) (cid:3) Ri(c−) (cid:3) Ri(c+) + 2(cid:2).Moreover, by the definition of ˜P i ,˜P i (cid:2) P i + m(cid:8)(cid:2),˜Ni (cid:3) Ni − m(cid:8)(cid:2).Thus˜P i (cid:2) P i + m(cid:8)(cid:2) = m(cid:8) ˆRi(c−, S i) + m(cid:8)(cid:2) (cid:2) m(cid:8)Ri(c−) (cid:2) m(cid:8)Ri(c+) (cid:2) m(cid:8)(cid:8)ˆRi(c+, S i) − (cid:2)(cid:9)(cid:2) Ni − m(cid:8)(cid:2) (cid:2) ˜Ni.(cid:2)Lemma A.4 implies that, if G occurs, agents cannot do better than report ˜S under Mechanism 3, since ˜S i reflects the realpreferences of agent i. Now, if agent i reports truthfully, then P i = P i . If i decides to lie, it may report more positive labels,but cannot gain from reporting more than ˜P i such labels, and, crucially, the mechanism’s outcome will not change in thiscase. The immediate result is that we can assume:P (cid:3) P =(cid:7)i∈I1nP i (cid:3)(cid:7)i∈I1n˜P i = ˜P ,and, since the expected risk of M only increases with the number of positive examples (the probability of Mechanism 3choosing the positive classifier increases),(cid:8)(cid:9)M( ˜S)(cid:9)M(S)(cid:9)M(S)(18)(cid:8)(cid:8).(cid:3) RI(cid:3) RIRIWe can now concentrate on bounding the empirical risk on ˜S.Lemma A.5. If G occurs,∀c ∈ {c+, c−},(cid:11)(cid:11)(cid:11) (cid:3) 3(cid:2).(cid:11)RI (c) − ˆRI (c, ˜S)As in Lemma A.3, it will suffice to show this only for c−.(19)(cid:8) > 1(cid:2) ,= P i + (cid:17)mm(cid:8)(cid:8)(cid:2)(cid:18)(cid:8)(cid:2) + 1(cid:3) P i + mm(cid:8)Proof. From (16), for mˆRI (c−, ˜S) =˜P im(cid:8)(cid:3) P im(cid:8)+ 2(cid:2) = ˆRI (c−, S) + 2(cid:2) (cid:3) RI (c−) + (cid:2) + 2(cid:2) = RI (c−) + 3(cid:2).(cid:2)R. Meir et al. / Artificial Intelligence 186 (2012) 123–156From (15) and (19)ˆRI (c−, ˜S) (cid:3) RI (c−) + 3(cid:2) (cid:3) RI (c+) − 3(cid:2) (cid:3) ˆRI (c+, ˜S).So c− is also empirically the best concept for ˜S; Mechanism 2 guarantees:(cid:8)ˆRIM( ˜S), ˜S(cid:9)(cid:3) 2 ˆRI (c−, ˜S).Furthermore, since the risk of Mechanism 3 is a convex combination of the risk of c+, c−, we get from (19),(cid:8)(cid:9)M( ˜S)RI(cid:3) ˆRI(cid:8)M( ˜S), ˜S(cid:9)+ 3(cid:2).145(20)(21)(22)Finally, by using (18), (22), (21) and (20) in this order, we get that if G occurs:(cid:9)(cid:8)(cid:8)(cid:8)(cid:9)M(S)RI(cid:9)(cid:3) ˆRIM( ˜S)(cid:3) RI(cid:8)RI (c−) + 3(cid:2)(cid:3) 2(cid:9)M( ˜S), ˜S+ 3(cid:2) = 2r+ 3(cid:2) (cid:3) 2 ˆRI (c−, ˜S) + 3(cid:2)∗ + 9(cid:2).If G does not occur, the risk cannot exceed 1. Thus by applying Lemma A.3 with δ = (cid:2) = (cid:2)(cid:8)(cid:2)(cid:8)2 ln( 10n(cid:2)(cid:8) ):50 1(cid:8)RI ( (cid:15)SRD) (cid:3) Pr[G](cid:9)∗ + 9(cid:2)2r+ Pr[¬G]1 (cid:3) 2r∗ + 9(cid:2) + (cid:2) (cid:3) 2r∗ + (cid:2)(cid:8),10 we find that for m(cid:8) >as required. (cid:2)Appendix B. Proofs of Section 4B.1. Proofs of upper bounds under shared inputs (Sections 4.1, 4.2)We formulate and prove our results in a somewhat more general model, in which the preferences of each agent areencoded by a distribution, rather than a deterministic function. The new model extends the one presented in Section 4with two components: (a) some data points may receive more attention than others; (b) the preferences of each agent canreflect uncertainty, or indeterminism, regarding the label of a specific data point. The theorems in Section 4 follow easily asa special case. In addition, the use of distributions makes the proofs in the generalization section (Section 4.3) easier andmore natural.For that purpose we replace the profile of finite datasets S = (cid:5)S1, . . . , Sn(cid:6) with a profile of distributions F = (cid:5)F 1, . . . , Fn(cid:6)over X × {−, +}. The marginal of all distributions over X is the same. We denote this marginal by FX , and take it as ameasure of the interest that the agents have in different parts of the input space. Let H be the set of all deterministicfunctions h : X → {−, +}. In particular, C ⊆ H.We adjust the definition of the private and global risk to handle distributions.The private risk of h ∈ H to agent i w.r.t. the profile F is thus defined as(cid:3)(cid:12).Ri(h, F ) = E(cid:5)x, y(cid:6)∼F ih(x) (cid:7)= y(cid:10)(cid:2)As usual, the global risk is defined as(cid:7)RI (h, F ) =w iRi(h, F ).i∈IAs with discrete datasets, F i is said to be realizable w.r.t. a concept class C ⊆ H if there is a concept c ∈ C such thatEvery distribution p on X × {−, +} induces a non-deterministic function f p from X to labels. Formally, Pr( f p(x) =+|x) = E(cid:5)x, y(cid:6)∼p[[[ y = +]]|x], and for convenience we denote this probability by f p(x) ∈ [0, 1]. Similarly,= E(cid:5)x, y(cid:6)∼p(cid:12)(cid:10)[[ y = −]] | x.We denote by F the set of all such non-deterministic functions. Note that H ⊂ F , and thus every concept class C is also(cid:8)f p(x) = 1 − f p(x) = Pr(cid:9)f p(x) = −|xRi(c, F i) = 0.a subset of F .A special case is when p = F i , in which case f i ≡ f p conveys the preferences of agent i. We assume that agents’ prefer-ences are independent; thus for every two agents i (cid:7)= j, for every x ∈ X and every y, y(cid:8) ∈ {−, +},(cid:8)Prf i(x) = y, f j(x) = y(cid:8)= Prf i(x) = y(cid:11)(cid:9)(cid:11) x(cid:8)(cid:11)(cid:9)(cid:11) x(cid:8)Prf j(x) = y(cid:8)(cid:11)(cid:9)(cid:11) x.(23)Definition B.1. We define the distance between two classifiers (w.r.t. a fixed distribution F X ∈ (cid:4)(X )), as the part of spacethey label differently. Formally:(cid:8)df , f(cid:9)(cid:8)(cid:8)(cid:9)(cid:8)f , f= d F X= Ex∼F X(cid:10)(cid:8)Prf (x) (cid:7)= f(cid:8)(x)(cid:11)(cid:11) x(cid:9)(cid:12).(24)146R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Let C ⊆ H any concept class, then the following holds.∀c ∈ C, ∀ j ∈ I,d( f j, c) = R j(c, F ).The proof of Eq. (25) is as follows.R j(c, F ) ≡ E(cid:5)x, y(cid:6)∼F j(cid:10)(cid:2)c(x) (cid:7)= y(cid:3)(cid:12)= Ex∼F X(cid:18) (cid:7)(cid:2)( y | x)c(x) (cid:7)= y(cid:19)(cid:3)y∈{−,+}Pry∼F j(cid:3)(cid:12)(cid:10)(cid:2)(cid:3)= EF X= EF X= EF X= EF X(cid:9)(cid:2)f j(x)(cid:8)(cid:10)Pr(cid:8)(cid:10)Pr(cid:8)(cid:10)Prc(x) (cid:7)= −(cid:11)(cid:11) xf j(x) = −f j(x) = −, c(x) = +f j(x) (cid:7)= c(x)(cid:11)(cid:11) xc(x) (cid:7)= +(cid:8)(cid:3)+ Pr(cid:8)+ Pr(cid:2)+ f j(x)c(x) (cid:7)= −(cid:11)(cid:9)(cid:11) x= d(c, f j)∗ = argminc∈C RI (c, F ).(cid:9)(cid:12)(from (24)).f j(x) = +(cid:9)(cid:2)(cid:11)(cid:11) xf j(x) = +, c(x) = −(cid:3)(cid:12)c(x) (cid:7)= +(cid:11)(cid:9)(cid:12)(cid:11) xRecall that ci = argminc∈C Ri(c, F ) and cAs a special case of Eq. (25), we get that∀i, j∀i ∈ I(cid:9)(cid:8)d(ci, f j) = R j(ci, F )(cid:20)ci = argmin,(cid:21)d(c, f i)c∈C.(25)(26)(27)The following lemma can be seen as a formalization of the statement that our decision-making setting is equivalent tofacility location in some metric space (the binary cube).Lemma B.2. d is reflexive, non-negative, symmetric and satisfies the triangle inequality.f , fd( f , f ) = Ex∼F X(cid:8), fProof. Non-negativity and symmetry are trivial.[Pr( f (x) (cid:7)= f (x) | x)] = Ex∼F X(cid:8)(cid:8) ∈ F . Note that disagreement of f and f(cid:8)Prf (x) (cid:7)= ff (x) (cid:7)= f(cid:11)(cid:9)(cid:11) x= Pr(x)(cid:8)(cid:8)(cid:8)(cid:8)(cid:3) Prand therefore[0] = 0, thus it is reflexive as well. We prove the triangle inequality. Let(cid:8)(cid:8)requires that at least one of them disagrees with f; thus for all x ∈ X(cid:8)(cid:8)(x) = f(x), f(cid:11)(cid:8)(cid:9)(cid:11) x+ Pr(x)f(cid:8)(cid:8)(cid:11)(cid:9)(cid:11) x(x) (cid:7)= f(x)(cid:8)(cid:8)+ Pr(cid:8)(cid:8)(x)f (x) = f(cid:11)(cid:9)(cid:11) x,(cid:8)(x), f(cid:8)(x) (cid:7)= f(cid:8)(cid:8)(x)(cid:11)(cid:9)(cid:11) x(cid:8)f (x) (cid:7)= f(cid:8)(cid:8)(cid:9)(cid:8)(cid:8)df , f= Ex∼F X= Ex∼F XThus the triangle inequality holds. (cid:2)f (x) (cid:7)= ff (x) (cid:7)= f(cid:8)(cid:8)(cid:10)Pr(cid:8)(cid:10)Pr(cid:8)(cid:8)(x)(x)(cid:9)(cid:12)(cid:11)(cid:11) x(cid:11)(cid:9)(cid:12)(cid:11) x(cid:8)(cid:10)(cid:3) Ex∼F XPr(cid:8)(cid:10)+ Ex∼F XPrf (x) (cid:7)= f(cid:8)(x) (cid:7)= ff(cid:11)(cid:9)(cid:11) x(cid:11)(cid:11) x(cid:8)(x)(cid:8)(cid:8)(x)(cid:8)+ Pr(cid:9)(cid:12)f(cid:8)(cid:8)(cid:8)(cid:8)(x) (cid:7)= f(cid:9)(cid:8)= df , f+ d(cid:9)(cid:12)(cid:11)(cid:11) x(cid:8)(cid:8), f(cid:9).(x)(cid:8)(cid:8)fLemma B.3.(cid:7)i∈I(cid:7)i∈IProof.w iRI (ci, F ) =(cid:7)(cid:7)ijw i w jd(ci, f j).w iRI (ci, F ) =(cid:7)iw iRI (ci, F ) =(cid:7)(cid:16)(cid:7)w iij(cid:17)w jR j(ci, F )=(cid:7)(cid:7)ijw i w jd(ci, f j).(cid:2)Lemma B.4.(cid:7)(cid:7)ijw i w jd( f i, f j) (cid:3) (2 − 2wmin)r∗.R. Meir et al. / Artificial Intelligence 186 (2012) 123–156147Proof.(cid:7)(cid:7)w i w jd( f i, f j) =(cid:7)(cid:7)w i w jd( f i, f j)(since d( f i, f i) = 0)j(cid:7)(cid:7)(cid:8)(cid:8)df i, ci(cid:9)∗j(cid:7)=i(cid:8)c+ d(cid:9)(cid:9)∗, f jw i w j(triangle inequality)j(cid:7)=ii(cid:7)(cid:8)w idf i, c(cid:9)(cid:7)∗j(cid:7)=i(cid:7)(cid:7)(cid:8)w iw jdf j, c(cid:9)∗w j +(cid:8)w idf i, c(cid:9)∗(1 − w i) +i(cid:7)j(cid:7)=i(cid:16)(cid:7)(cid:8)w iw jdf j, c(cid:9)∗− w id(cid:8)f i, c(cid:17)(cid:9)∗(cid:8)(cid:8)d(cid:8)(cid:8)dw iw i(cid:9)∗f i, ci(1 − w i) + rj(cid:8)∗ − w id(cid:9)(cid:9)∗f i, c(cid:9)∗f i, c(1 − w min) + r∗ − w mind(cid:9)(cid:9)∗(cid:8)f i, ci(cid:3)===(cid:3)i(cid:7)i(cid:7)i(cid:7)i= (1 − wmin)(cid:7)(cid:16)(cid:8)w idf i, c(cid:7)(cid:9)∗∗+ rw i − w min(cid:7)(cid:8)w idf i, c(cid:17)(cid:9)∗= (1 − wmin)ri∗ + r∗ − w minr∗ = (2 − 2wmin)r∗.ii(cid:2)Note that Eq. (14) is derived as a special case of the lemma when weights are uniform.We can now use these lemmas to bound the approximation ratio of our mechanism in this extended setting. We beginwith the simpler, deterministic mechanism.Theorem 4.1(cid:2). Let |I| = n. For every concept class C and any profile F , Mechanism 4 is an SP (2n − 1)-approximation mechanism.Proof. We first find a lower bound on r(cid:9)(cid:9)(cid:7)∗ = RIr(cid:8)c∗, F=(cid:8)c∗, Fw iRi(cid:2) w jd(cid:9)(cid:8)c∗, f ji∈I(cid:2) 1n(cid:9)(cid:8)c∗d, f j∗:=(cid:7)i∈I(cid:9)(cid:8)c∗, f iw id(since j is heaviest).(28)Then we upper bound the risk of c j :(cid:8)(cid:9)RIHD(F ), F= RI (c j, F ) =(cid:7)w id(c j, f i) = w jd(c j, f j) +(cid:3) w jd(cid:8)c∗, f j(cid:9)i∈I+(cid:7)(cid:8)d(cid:8)c j, c(cid:9)∗(cid:8)c∗+ dw i, f i(cid:9)(cid:9)(cid:8)c j, c∗= d(cid:9)(cid:7)i(cid:7)= jw i +(cid:7)(cid:8)c∗(cid:9)(cid:8)c j, c∗= d, f iw id(cid:7)i(cid:7)= jw id(c j, f i)(from the triangle inequality)(cid:9)(cid:7)i(cid:7)= j∗w i + r∗(cid:16)i∈I+ ri(cid:7)= j(cid:9) n − 1n(cid:8)d(c j, f j) + d(cid:8)c2d, f j(cid:9)∗n∗(cid:8)c j, c∗ + n − 1∗ + n − 1n(cid:17)(cid:9)(cid:9)w j (cid:2) 1n(cid:8)∗f j, c(from (27))∗ + n − 12n · rn∗ + (n − 1)2r∗(from (28))∗ = (2n − 1)r∗.(cid:2)(cid:3) d(cid:3) r(cid:3) r(cid:3) r= r(triangle inequality)(cid:2)Theorem 4.4w min = mini∈I w i . Moreover, if S is individually realizable, then (2 − 2w min)-approximation is guaranteed.. For every concept class C and for any dataset S, Mechanism 5 is an SP (3 − 2w min)-approximation mechanism, where148R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Proof. Using the lemmas above,(cid:8)RIWRD(F ), F(cid:7)(cid:9)=w iRI (ci, F ) =(cid:7)(cid:7)w i w jd( f i, c j)i∈I(cid:7)(cid:7)i(cid:7)j(cid:7)i(cid:7)j(cid:7)(cid:3)(cid:3)=ij(cid:8)(cid:9)d( f i, f j) + d( f j, c j)w i w j(triangle inequality)(cid:8)d( f i, f j) + d(cid:8)f j, c(cid:9)(cid:9)∗w i w j(from (27))w i w jd( f i, f j) +(cid:7)(cid:8)w jdf j, c(cid:9)(cid:7)∗w iiij(cid:3) (2 − 2wmin)r∗ += (2 − 2wmin)r∗ +(cid:7)j(cid:7)j(cid:8)w jdf j, c(cid:9)∗(cid:9)(cid:8)c∗, Fw jR j(from Lemma B.4)(from (25))= (2 − 2wmin)r∗ + RIj(cid:9)(cid:8)c∗, F= (3 − 2wmin)r∗.Further, if we have an individually realizable profile F(cid:8), then for any agent j, d( f j, c j) = R j(c j, F(cid:8)) = 0 (from (25)), inwhich case(cid:8)(cid:8)WRDRI(cid:9)(cid:8)F, F(cid:9)(cid:8)=(cid:7)i∈I(cid:8)ci, F(cid:9)(cid:8)(cid:3)w iRI(cid:7)(cid:7)ijw i w jd( f i, f j) (cid:3) (2 − 2wmin)r∗.Thus the proof of Theorem 4.4’ (and Theorem 4.4 as a special case) is complete. (cid:2)Proposition 4.7. There is a dataset S with three agents, such that(cid:8)RISRD(S), S(cid:9)> 2.4 · r∗>(cid:16)3 − 2n(cid:17)∗.rExample B.5. We set our concept class to C = {c−, c+}. Assume w.l.o.g. that an agent that is indifferent between the conceptsdictates the c− concept. Let S1, S2 be all positive. S3 contains exactly half negative samples. We set agents’ weights asfollows: w 1 = w 2 = 0.29, and w 3 = 0.42.∗ = RI (c+, S) = 0.21. However, the SRD mechanism selects agent 3 (andObserve first that RI (c−, S) = 0.79, whereas r0.4220.292+0.292+0.422thus the concept c−) with probability of(cid:9)(cid:8)∼= 0.511. Therefore,RISRD(S), S> 0.51 · 0.79 + 0.49 · 0.21 = 0.5058 > 2.4 · 0.21 = 2.4 · r∗,which proves the lower bound.Proposition B.6. There is an individually realizable dataset S with three agents, such that(cid:8)(cid:9)RISRD(S), S> 1.39 · r∗>(cid:17)(cid:16)2 − 2n∗.rExample B.7. We keep C = {c−, c+}. Let S1, S2 be all positive, and S3 be all negative. We set agents’ weights as follows:w 1 = w 2 = 0.363, and w 3 = 0.274.We have that RI (c−, S) = 0.763, and r∗ = RI (c+, S) = 0.274. The SRD mechanism selects agent 3 with probability of0.27420.3632+0.3632+0.2742(cid:8)∼= 0.222. Therefore,(cid:9)RISRD(S), S> 0.222 · 0.763 + 0.778 · 0.274 > 0.382 > 1.39 · 0.274 = 1.39 · r∗,which proves the lower bound for the realizable case.Theorem 4.8(cid:2). The following hold for Mechanism 6, w.r.t. any profile F :• αw (cid:3) 2 − 2n .• CRD has an approximation ratio of αw + 1, i.e., at most 3 − 2n .• if S is individually realizable, then the approximation ratio is αw2+ 1, i.e., at most 2 − 1n .R. Meir et al. / Artificial Intelligence 186 (2012) 123–156Proof. We first prove that αw (cid:3) 2 − 2n .Let g(x) = 12−2x . Note that g is convex. Also, since(cid:2)i∈I w i = 1, we have that(cid:3)1n(cid:7)i∈Iw 2i(cid:3) 1.(αw)−1 =(cid:7)(cid:8)ip=i∈I(cid:16)(cid:7)(cid:2) gi∈I12 − 2(1/n)(cid:2)(cid:7)i∈Iw i(cid:17)12 − 2w i=(cid:7)i∈Iw i g(w i)w i · w i=1(cid:2)2 − 2i∈I w 2i(from Jensen’s inequality)(from (29)),149(29)thus αw (cid:3) 2 − 2n .We denote by d( f , f(cid:8)) the number of disagreements between f and fclassifier in C that is the closest to them (i.e., c ∈ C that minimizes d(c, f i)). For any c, it holds that.(cid:8)f i, ci denote the labels of agent i, and theRI (c, F ) =w iRi(c, F ) =w id(c, f i).(cid:7)i∈I(cid:7)i∈I∗) (cid:3) 2d( f i, cpiRI (ci, F ) =Note that for all i, d(ci, c(cid:7)(cid:8)(cid:9)RICRD(F ), Fi∈I(cid:7)(cid:16)(cid:7)i∈I(cid:7)j(cid:7)=i(cid:16)(cid:7)==(cid:3)=∗is closer to f i than ci .∗), since otherwise c(cid:7)(cid:7)piw jd(ci, f j)i∈Ij∈I(cid:17)pi w jd(ci, f j) + pi w id(ci, f i)(cid:8)d(cid:8)ci, c(cid:9)∗(cid:8)c∗+ d, f j(cid:9)(cid:9)pi w j+ pi w id(cid:8)c∗, f i(cid:17)(cid:9)i∈I(cid:7)j(cid:7)=i(cid:8)ci, cpid∗(cid:9)(cid:7)w j +(cid:7)(cid:7)(cid:9)(cid:8)c∗, f jpi w jdi∈I= αw(cid:3) αw(cid:7)i∈I(cid:7)i∈Iw i2(1 − w i)(cid:8)2dw i2(cid:7)j(cid:7)=i(cid:8)ci, cd∗j∈Ii∈I(cid:9)(1 − w i) +(cid:7)(cid:9)∗+(cid:8)c∗, f jw jdf i, c(cid:8)c∗, f jw jdj∈I(cid:9)= (αw + 1)RI(cid:9)(cid:8)c∗, F(cid:8)c∗, f jw jd(cid:9)(cid:7)pii∈I(cid:7)j∈I(cid:9)= (αw + 1)(cid:16)(cid:3)3 − 2nj∈I(cid:17)∗.rNow, in the realizable case, f i = ci for all i.(cid:7)(cid:7)(cid:8)RICRD(F ), FpiRI (ci, F ) =(cid:9)=(cid:7)w jd( f i, f j) =pi(cid:7)(cid:7)piw jd( f i, f j)i∈Ij(cid:7)=ii∈I(cid:7)(cid:7)i∈I∗f i, c(cid:9)j∈I(cid:8)+ d(cid:8)(cid:8)d(cid:9)(cid:9)∗f j, cpi w j(T.I.)j(cid:7)=ii∈I(cid:7)(cid:8)i∈I(cid:7)pidf i, c(cid:8)pidf i, c∗∗(cid:9)(cid:7)(cid:7)(cid:7)(cid:8)piw jdf j, c(cid:9)∗w j +j(cid:7)=i(cid:9)(1 − w i) +i∈I(cid:7)j(cid:7)=i(cid:8)rpi∗(F ) − w id(cid:8)(cid:3)==i∈I= αw(cid:7)i∈Iw i2(1 − w i)(cid:8)df i, c(cid:9)∗i∈I(1 − w i) + r∗(F ) −(cid:9)(cid:9)∗(cid:8)f i, c(cid:7)pi w idf i, ci∈I(cid:9)∗150R. Meir et al. / Artificial Intelligence 186 (2012) 123–156(cid:7)(cid:8)w idf i, c(cid:9)∗∗+ r(F ) −(cid:8)pi w idf i, c(cid:9)∗(cid:7)i∈I(cid:8)= αw2= αw2(cid:16)(cid:3)i∈I∗r(F ) + r∗(F ) −(cid:7)(cid:9)∗pi w idf i, c(cid:17)+ 1∗r(F ) (cid:3)i∈I(cid:16)2 − 1n(cid:17)∗r(F ),αw2which completes the proof. (cid:2)Theorem 4.9. The following hold for Mechanism 7:• βw (cid:3) 1 − 2n .• RRD has an approximation ratio of at most 4, and at least 3 (in the worst case).• if S is individually realizable, then the approximation ratio is 1 + βw, i.e., at most 2 − 2n .Proof. Let q(x) = 11−2x . Note that q is convex.(cid:7)(cid:7)(cid:7)−1 =(βw)(cid:8)ip=11 − 2w i=w iq(w i)i∈Iw i(cid:17)i∈Ii∈I(cid:16)(cid:7)(cid:2) qi∈Iw i · w i=1(cid:2)1 − 2i∈I w 2i(from Jensen’s inequality)(cid:2)11 − 2(1/n)(from (29)),thus βw (cid:3) 1 − 2n .For the upper bound, we will need the following.Lemma B.8. For all i ∈ I , pi (cid:3) 2w i .Proof. Let h(x) = x1−2x . Note that h is convex. Thus by Jensen’s inequality(cid:16)1n − 1(cid:7)j(cid:7)=ih(w j) (cid:2) h1n − 1(cid:7)j(cid:7)=i(cid:17)(cid:16)(cid:17)w j= h1 − w in − 1Next,(cid:7)j∈Iw j1 − 2w j= w i1 − 2w i+(cid:7)j(cid:7)=iw j1 − 2w j(cid:16)= w i1 − 2w i(cid:17).(30)+(cid:7)j(cid:7)=ih(w j)(cid:2) w i1 − 2w i= w i1 − 2w i(cid:2) w i1 − 2w i+ (n − 1)h+ (n − 1)1 − w in − 11−w in−11 − 2 1−w in−1= w i(by Eq. (30))= w i1 − 2w i+ 1 − w i1 − 2 1−w in−1+ 1/21 − 2 1/2n−11 − 2w i+ 12 n−2n−1>w i1 − 2w i+ 12.Therefore,pi = βw p=(cid:8)i(cid:16)(cid:7)j∈Iw j1 − 2w j(cid:17)−1 w i1 − 2w i<1w i1−2w i+ 12·w i1 − 2w i=w iw i + 1−2w i2=w iw i − w i + 12= 2w i.(cid:2)R. Meir et al. / Artificial Intelligence 186 (2012) 123–156151We now bound the risk of RRD. We skip some steps that are detailed in the upper bound proof of the CRD mechanism.(cid:8)RIRRD(S), S(cid:9)=(cid:7)piRI (ci, S) =(cid:7)(cid:8)ci, c∗pid(cid:9)(cid:7)w j +(cid:7)(cid:7)(cid:9)(cid:8)c∗, f jpi w jdi∈I= βw(cid:3) βw= βw= βw(cid:7)i∈I(cid:7)i∈I(cid:7)(cid:16)i∈I(cid:7)dw i1 − 2w i2w i(1 − w i)1 − 2w iw i(1 − 2w i)1 − 2w i(cid:9)(cid:8)dw idf i, c∗+ βwi∈I∗(S) + βw(cid:8)w idf i, c(cid:7)i∈I(cid:9)∗= βwr(cid:7)(cid:3) 2i∈I(cid:8)ci, c(cid:9)∗j(cid:7)=i(1 − w i) +i∈I(cid:8)cw jdj∈I∗, f j(cid:7)j∈I(cid:9)(cid:7)pii∈I(cid:8)(cid:9)∗f i, c∗+ r(S)(cid:8)df i, c(cid:9)∗+ w i(cid:17)(cid:9)∗(cid:8)df i, c∗+ r(S)1 − 2w i(cid:8)(cid:9)∗∗+ r(S)df i, c(cid:7)i∈Iw i1 − 2w i(cid:9)(cid:8)∗df i, cw i1 − 2w i+ 2r∗(S) = 2r∗(S) + 2ri∈I(S) = 4r∗∗(S).∗(S) (cid:3)(cid:7)(cid:8)pidf i, c(cid:9)∗∗+ 2r(S)+ ri∈IIn the realizable case, recall that f i = ci for all i.(cid:7)(cid:7)(cid:7)(cid:8)RIRRD(S), SpiRI (ci, S) =pi(cid:9)=w jd( f i, f j) =(cid:7)(cid:7)piw jd( f i, f j)i∈I(cid:7)(cid:7)i∈If i, c(cid:9)∗j∈I(cid:8)+ d(cid:8)(cid:8)d(cid:9)(cid:9)∗f j, cpi w j(T.I.)i∈Ij(cid:7)=ij(cid:7)=ii∈I(cid:7)(cid:8)pidf i, c(cid:9)(cid:7)∗(cid:7)(cid:7)(cid:8)piw jdf j, c(cid:9)∗w j +i∈I(cid:7)(cid:8)pidf i, c(cid:9)∗j(cid:7)=i(1 − w i) +i∈I(cid:7)j(cid:7)=i(cid:8)rpi∗(S) − w id(cid:9)(cid:9)∗(cid:8)f i, c(cid:3)==(cid:7)i∈Iw i1 − 2w i(cid:8)w idf i, c(cid:9)∗∗+ r(S)i∈I= βw= βw= βw(cid:7)i∈I(cid:7)i∈I(cid:7)i∈I(cid:9)∗i∈I(1 − w i) − βwf i, c(cid:8)df i, c(cid:9)∗∗+ r(S)(cid:8)dw i1 − 2w iw i(1 − 2w i)1 − 2w i(cid:9)(cid:8)w idf i, c∗∗+ r(S) = βwr(cid:17)∗r(S),(cid:16)2 − 2n= (1 + βw)r∗(S) (cid:3)∗(S) + r∗(S)which proves the upper bound. (cid:2)B.2. Proofs of generalization results (Section 4.3)As in Section 3.3, we distinguish the notation R(c) (the risk w.r.t. the fixed input distribution) from ˆR(c, S) (the empiricalrisk, w.r.t. the sampled dataset S). For the proofs in this section, we will also need the following fundamental result frommachine learning theory.Theorem B.9 (Vapnik and Chervonenkis [41]). Let m be s.t.m >V C(cid:2)2log(cid:16)(cid:17).V C(cid:2)2δLet S be a dataset that contains m data points sampled i.i.d. from a distribution D ∈ (cid:4)(X × Y). Then with probability of at least 1 − δ,∀c ∈ C(cid:8)(cid:11)(cid:11)(cid:11)R(c) − ˆR(c, S)(cid:11) < (cid:2)(cid:9)(31)where V C is a constant which depends only on the concept class C, and not on the distribution D or on any other property of theproblem.152R. Meir et al. / Artificial Intelligence 186 (2012) 123–156V C is known as the VC-dimension of C, introduced in [41]. We do not give a formal definition of V C here. However,detailed and accessible overviews of both VC theory and PAC learning are abundant (for example, [12]). While V C may bevery large, or even infinite in some cases, it is known to be finite for many commonly used concept classes (e.g., linearclassifiers).Theorem 4.10. Assume all agents are (cid:2)-truthful, and let C be any concept class with a bounded dimension. For any (cid:2) > 0, there is kn ) · rmin + (cid:2).(polynomial in 1(cid:2) and ln(n)) s.t. if at least k data points are sampled, then the expected risk of Mechanism 8 is at most (3 − 2Proof. Let S i = (cid:5) X, Y i( X)(cid:6) be the partial dataset of agent i, with its true private labels. Denote by Q i = Q i((cid:2)) the event that∀c ∈ C(cid:8)(cid:11)(cid:11)(cid:11)Ri(c) − ˆRi(c, S)(cid:11) < (cid:2)(cid:9).(32)We emphasize that Q i is a property of S, i.e., for some random samples S the event Q i holds, whereas for others it doesnot hold. Our proof sketch can now be reformulated as follows:(a) Q i happens for all i simultaneously with high probability.(b) Whenever Q i occurs, agent i will report truthfully (under the (cid:2)-truthfulness assumption).(c) When all Q i occur, the risk of Mechanism 8 is bounded by (3 − 2(d) Otherwise the risk can be high, but this has a small effect on the total expected risk.n ) · rmin + (cid:2).Let δ > 0. As S i is an i.i.d. random sample from Di , then from Theorem B.9 every Q i occurs with probability of at least1 − δ (provided that there are enough samples). Also, from the union bound the probability of the event ∀ j Q j is at least1 − δ(cid:8), where δ = δ(cid:8)n .Lemma B.10. If Q i occurs, then agent i can gain at most 2(cid:2) by lying.Proof. Assume agent i is selected by the mechanism, otherwise it is trivially true.We denote by ˆci ∈ C the concept returned by the mechanism when i reports truthfully, i.e., ˆci = argminc∈CLet any cˆRi(c, S i).(cid:8) ∈ C,(cid:8)cRi(ˆci) − Ri(cid:9)(cid:8)= Ri(ˆci) − ˆRi(ˆci, S i) + ˆRi(ˆci, S i) − Ri(cid:3)(cid:11)(cid:11)(cid:11) +(cid:11)Ri(ˆci) − ˆRi(ˆci, S i)(cid:11)(cid:11) ˆRi< (cid:2) + (cid:2) = 2(cid:2) (from (32))., S i(cid:2)(cid:8)c(cid:9)(cid:8)(cid:8)c− Ri(cid:9)(cid:8)(cid:8)c(cid:9)(cid:11)(cid:11)(cid:8)(since ˆci is empirically optimal)By Lemma B.10, i cannot gain more than 2(cid:2) by reporting c(cid:8). By taking (cid:2) < (cid:2)(cid:8)2 , we complete the proof of parts (a) and (b)Now, for part (c), we assume ∀i Q i . Thus, from Lemma B.10 and the (cid:2)-truthfulness assumption, all agents are truthfulfrom the proof sketch.(i.e., S = S).Lemma B.11. If S holds that Q i occurs for all i ∈ I , then(cid:9)(cid:8)c∗(S), SˆRI∗(S) = argminc∈C(cid:3) rmin + (cid:2),ˆRI (c, S).where cProof. For any c ∈ C, |Ri(c) − ˆRi(c, S i)| < (cid:2), from Eq. (32). Therefore(cid:7)ˆRi(c, S i) <(cid:3) ˆRI (c, S) =ˆRi(c, S) =(S), S(cid:8)c(cid:7)ˆRIpipi(cid:9)∗(cid:7)(cid:8)piRi(c) + (cid:2)(cid:9)= RI (c) + (cid:2),and in particular ˆRI (ci∈I∗(S), S) < rmin + (cid:2). (cid:2)i∈Ii∈IWe now bound the expected risk of the mechanism. We denote by cM(S) the (random) classifier that is returned byMechanism 6 on the input S. For any random variable A, EM[ A | S] is the expectation of A over the random dictatorselection for a fixed dataset S. Similarly, ES [ A | i] is the expectation of A over the random sampling, given that i is theselected dictator.(cid:10)RI= ES(cid:9) (cid:11)(cid:8)(cid:11) ∀ j Q jcM(S)(cid:8)(cid:10)(cid:10)EMcM(S)RI(changing the order of randomizations)(cid:9) (cid:11)(cid:11) i, ∀ j Q j(cid:12) (cid:11)(cid:11) ∀ j Q j(cid:8)cM(S)(cid:9) (cid:11)(cid:11) S= EM(cid:10)RIES(cid:12)(cid:12)E(cid:10)(cid:12)(cid:12)R. Meir et al. / Artificial Intelligence 186 (2012) 123–156153(cid:10)RI(cid:8)ˆci(S)(cid:9) (cid:11)(cid:11) i, ∀ j Q j(cid:12)piES(cid:10)ˆRI(cid:8)ˆci(S), S i(cid:9)+ (cid:2)(cid:11)(cid:11) i, ∀ j Q j(cid:12)piES(from (32))(cid:7)i∈I(cid:7)i∈I(cid:7)=(cid:3)=(cid:10)ˆRI(cid:8)ˆci(S), S ipiES(cid:8)cM(S), S3 − 2n3 − 2n(cid:17)ˆRI(cid:17)(cid:12)(cid:9) (cid:11)(cid:11) i, ∀ j Q j(cid:12)(cid:12)+ (cid:2)+ (cid:2)(cid:9) (cid:11)(cid:11) i, ∀ j Q j(cid:9) (cid:11)(cid:8)(cid:11) i, ∀ j Q jc(cid:19)(cid:19)(S), S∗(cid:19)(cid:19)(rmin + (cid:2))(cid:16)(cid:11)(cid:11) i, ∀ j Q j(cid:17)3 − 2ni∈I= EM(cid:10)ES(cid:18)(cid:10)ˆRI(cid:18)(cid:16)(cid:3) EMES(cid:18)(cid:16)(cid:18)ES(cid:17)(cid:3) EM(cid:16)=3 − 2n(rmin + (cid:2)) + (cid:2) (cid:3)· rmin + 4(cid:2) =+ (cid:2)(from Theorem 4.8)+ (cid:2) (from Lemma B.11)(cid:16)3 − 2n(cid:17)rmin + (cid:2)(cid:8),which proves part (c) of the proof sketch.Finally, we bound the total risk of the mechanism, taking part (d) into account.(cid:10)RIRI ( (cid:15)CRD) = E(cid:9)(cid:12)(cid:12)(cid:12)(cid:12)(cid:8)(cid:10)cM(S)RI= Pr(∀ j Q j)ES= ES(cid:10)(cid:10)EMRI(cid:10)EM+ Pr(¬∀ j Q j)ES(cid:9) (cid:11)(cid:10)(cid:11) SRI(cid:9) (cid:11)(cid:10)(cid:11) SRI(cid:17)(cid:8)cM(S)(cid:8)cM(S)(cid:10)EM(cid:3) ES(cid:10)EM= ES(cid:16)3 − 2n(cid:9) (cid:11)(cid:10)(cid:8)(cid:11) SEMcM(S)(cid:12) (cid:11)(cid:9) (cid:11)(cid:8)(cid:11) ∀ j Q j(cid:11) ScM(S)(cid:12) (cid:11)(cid:9) (cid:11)(cid:8)(cid:10)(cid:11) ¬∀ j Q j(cid:11) ScM(S)RI(cid:12) (cid:11)(cid:12)(cid:11) ∀ j Q j(cid:8) · 1+ δ(cid:12) (cid:11)(cid:11) ∀ j Q j(cid:8)+ δ(cid:17)(cid:16)3 − 2rmin + (cid:2)(cid:8)(cid:8)n(cid:8) + (cid:2)(cid:8) =rmin + δ(cid:3)(cid:12),(cid:12)(since all agents are truthful in this case)as required. (cid:2)We conclude by computing the exact number of samples needed by Mechanism 8 under the (cid:2)-truthfulness assumption.Lemma B.12. If k > 64 V C(cid:2)2 log(256 V C ·n(cid:2)3 ), thenRI ( (cid:15)CRD) (cid:3)rmin + (cid:2).(cid:17)(cid:16)3 − 2nProof. From Theorem B.9, if |S j| > V CV C((cid:2)∗)2δ∗ ), then Pr(¬Q j((cid:2)∗)) < δ∗and from the union bound it holds that4n , and unfolding all the residues we used in the proof, we get that((cid:2)∗)2 log((cid:9)(cid:8)(cid:2)∗¬Q j< nδ∗.(cid:8)Pr∃ j ∈ I, ¬Q j(cid:9)(cid:9)(cid:8)(cid:2)∗(cid:3)(cid:7)j∈ITaking (cid:2)∗ < (cid:2)8 and δ∗ < (cid:2)(cid:17)(cid:16)3 − 2n(cid:16)3 − 2nrmin (cid:3)(cid:17)rmin + 4(cid:16)3 − 2n(cid:17)rmin + 4(cid:2)∗ + 2nδ∗(cid:2)8+ 2n(cid:16)3 − 2n(cid:2)4n=(cid:17)rmin + (cid:2),RI ( (cid:15)CRD) (cid:3)<(cid:16)logV C((cid:2)∗)2while(cid:17)V C((cid:2)∗)2δ∗= V C((cid:2)/8)2log(cid:16)V C((cid:2)/8)2((cid:2)/4n)(cid:17)= 64V C(cid:2)2(cid:16)log256(cid:17).V C · n(cid:2)3(cid:2)Theorem 4.11. Assume all agents are purely rational, and let C be any concept class with a bounded dimension. For any (cid:2) > 0, there isn )rmin + (cid:2).a k (polynomial only in 1(cid:2) ) s.t. if at least k data points are sampled, then the expected risk of Mechanism 8 is at most (3 − 2154R. Meir et al. / Artificial Intelligence 186 (2012) 123–156(cid:2)nProof. Note that the private distributions D1, D2, . . . , Dn induce a global joint distribution on the input space, defined asD =i=1 w iDi . We can alternatively define rmin as the minimal risk of any concept w.r.t. the distribution D, i.e., rmin =infc∈C E(x, y)∼D[[[c(x) (cid:7)= y]]]. We would like to analyze the outcome of Mechanism 8 and compare the empirical risk to theactual risk. However, we have a technical problem with doing so directly, since S i (as defined in the proof of Theorem 4.10)is sampled i.i.d. from Di , but not from D.In order to prove the theorem, we introduce a virtual mechanism (see Mechanism 9). This mechanism generates a truthfuldataset S, which can be used as an i.i.d. sample from the joint distribution D.Mechanism 9 The Virtual Learning MechanismSample k data points i.i.d. from D X (assume we get the same dataset X as in Mechanism 8).for each point x ∈ X doSelect agent i with probability w i .Add (cid:5)x j , Y i (x)(cid:6) to S.end forreturn c∗(S) = erm(S).The output of Mechanism 9, c∗(S), is the best concept (in C) for the real dataset S. Note that S is an i.i.d. sample from D,but an actual mechanism such as Mechanism 8 cannot have access to the real labels Y i —hence the term virtual mechanism.We denote by T = T ((cid:2)) the event(cid:8)c∗(cid:9)(S)RI< rmin + 2(cid:2).(33)Similarly to Q j in the previous proof, T is a property of S, i.e., its occurrence depends only on the sampling.Lemma B.13. If k = k(δ, (cid:2)) is large enough thenPr(¬T ) < δ.Proof. This is an immediate corollary of Theorem B.9. As csampled i.i.d. from D, then for any c ∈ C< ˆRI+ (cid:2) (cid:3) ˆRI (c, S) + (cid:2) < RI (c) + (cid:2) + (cid:2)(cid:9)(S)(S), S(cid:8)c(cid:8)cRI(cid:9)∗∗∗ = argminc∈CˆRI (c, S), C is of a bounded dimension and S isholds with probability of at least 1 − δ, for a large enough k. In particular,(cid:8)Pr(T ) = PrRI(cid:8)c∗(cid:9)(S)(cid:9)< rmin + 2(cid:2)> 1 − δ.(cid:2)It is still not clear how to approximate c∗(S), as our mechanism only has access to S. For that purpose, we define anew concept class C X ⊆ C as the projection of C on X . Formally, let H X ⊆ H be the class of all dichotomies of X , i.e., all hs.t. h : X → {−, +},19 then C X = C ∩ H X . In other words, C X contains all dichotomies of X that are also allowed by C.Denote by S i the dataset with the reported labels of agent i, and by ˆci the best concept w.r.t. to this dataset. That is,∗(S) ∈ C X and ˆc j ∈ C X for all agents. This is the case sinceS i = {(cid:5)x, Y i(x)(cid:6)}x∈ X and ˆci = argminc∈Cboth S, S j are labeled versions of the set X . Thus any classifier that is computed w.r.t. S or S j is a dichotomy of X (which∗(S)minimizes some function that depends on the labels). We define ˜c = argminc∈C X RI (c). Clearly RI (˜c) (cid:3) RI (cis also a member of C X . Thus when T occurs, the inequalityˆRi(c, S i). Observe that c∗(S)), since cRI (˜c) < rmin + 2(cid:2)(34)also holds, directly as a special case of (33).We next show how to approximate ˜c using the generalized variant of Theorem 4.8, as it appears in the appendix.Consider a profile F = (cid:5)D1, . . . , Dn(cid:6). This is a valid profile with shared inputs; thus for any concept c ∈ C, R(c) = R(c, F ) forprivate and global risk alike.Lemma B.14. Let j be the selected dictator, thenˆc j = argminc∈CXR j(c) = argminc∈CXR j(c, F ).ˆR j(c, S j). Since we assumed j is purely rational, he will always label all examples in X inProof. Recall that ˆc j ≡ argminc∈Ca way that will minimize his private risk. From the way Mechanism 8 works, only concepts in C X may be returned, and forany c ∈ C X , there is a labeling of X s.t. c is returned. This labeling Y (c) is simply ∀x ∈ X ( y(x) = c(x)). Thus argminc∈C X R j(c)is the best that agent j can hope for, and he can also achieve it by reporting the appropriate labels Y j . (cid:2)19 Put differently, H X is a partition of H to equivalence classes, according to their outcome on X ⊆ X .R. Meir et al. / Artificial Intelligence 186 (2012) 123–156We now apply Theorem 4.8’ on F , using the class C X , getting(cid:17)(cid:17)p jRI (ˆc j, F ) (cid:3)∗r(F ) =RI (˜c).(cid:16)3 − 2n(cid:16)3 − 2n(cid:7)j∈I155(35)To see why this holds, observe that the left term is the expected risk of Mechanism 6 when the input is the profile Fand the concept class C X ; and ˜c is the globally optimal classifier for this input. We emphasize that Eq. (35) always holds,independently of the sampling or selection.Finally, we bound the risk of the result concept:RI ( (cid:15)CRD) = ES(cid:12)(cid:12)(cid:11)(cid:11) S(cid:10)EM= Pr(T )ES(cid:10)(cid:10)EM(cid:3) ESRI (cM)(cid:18)(cid:7)(cid:8)ˆc j(S)(cid:10)RI (cM)(cid:11)(cid:10)(cid:10)(cid:11) SEMRI (cM)(cid:12) (cid:11)(cid:11)(cid:12)(cid:11) T(cid:11) S(cid:9) (cid:11)(cid:11) T= ESw jRI(cid:12)+ Pr(¬T )ES(cid:12) (cid:11)(cid:11) T+ δ · 1 (from Lemma B.13)(cid:19)(cid:10)EM(cid:10)RI (cM)(cid:11)(cid:11) S(cid:12) (cid:11)(cid:11) ¬T(cid:12)(cid:18)(cid:16)j∈I3 − 2n(cid:17)ES(cid:17)<(cid:3) ES(cid:16)3 − 2n(cid:16)3 − 2n=+ δ(cid:19)+ δ(from (35))(cid:17)RI(cid:8)˜c(S)(cid:9) (cid:11)(cid:11) T(cid:10)(rmin + 2(cid:2))(cid:12)(cid:11)(cid:11) T+ δ(rmin + 2(cid:2)) + δ =(from (34))(cid:17)rmin + 6(cid:2) + δ.(cid:16)3 − 2nBy taking δ = (cid:2) = (cid:2)(cid:8)7 , the proof is complete.Similarly to Lemma B.12, it follows from Theorem B.9 that taking(cid:16)(cid:17)k > 49V C(cid:2)2log343V C(cid:2)3is sufficient for Mechanism 8 to work well under the pure rationality assumption. (cid:2)References[1] N. Alon, M. Feldman, A.D. Procaccia, M. Tennenholtz, Strategyproof approximation of the minimax on networks, Mathematics of Operations Re-search 35 (3) (2010) 513–526.[2] N. Alon, M. Feldman, A.D. Procaccia, M. Tennenholtz, Walking in circles, Discrete Mathematics 310 (23) (2010) 3432–3435.[3] I. Ashlagi, F. Fischer, I. Kash, A.D. Procaccia, Mix and match, in: Proceedings of the 11th ACM Conference on Electronic Commerce (ACM-EC), 2010,pp. 305–314.[4] M.-F. Balcan, A. Blum, J.D. Hartline, Y. Mansour, Mechanism design via machine learning, in: Proceedings of the 46th Symposium on Foundations ofComputer Science (FOCS), 2005, pp. 605–614.[5] J.-P. Barthèlemy, B. Leclerc, B. Monjardet, On the use of ordered sets in problems of comparison and consensus of classifications, Journal of Classifica-tion 3 (1986) 187–224.[6] N.H. Bshouty, N. Eiron, E. Kushilevitz, PAC learning with nasty noise, Theoretical Computer Science 288 (2) (2002) 255–275.[7] V. Conitzer, T. Sandholm, Complexity of mechanism design, in: Proceedings of the 18th Annual Conference on Uncertainty in Artificial Intelligence(UAI), 2002, pp. 103–110.[8] V. Conitzer, T. Sandholm, An algorithm for automatically designing deterministic mechanisms without payments, in: Proceedings of the 3rd Interna-tional Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2004, pp. 128–135.[9] N. Dalvi, P. Domingos, Mausam, S. Sanghai, D. Verma, Adversarial classification, in: Proceedings of the 10th International Conference on KnowledgeDiscovery and Data Mining (KDD), 2004, pp. 99–108.[10] O. Dekel, F. Fischer, A.D. Procaccia, Incentive compatible regression learning, Journal of Computer and System Sciences 76 (2010) 759–777.[11] O. Dekel, O. Shamir, Good learners for evil teachers, in: Proceedings of the 26th International Conference on Machine Learning (ICML), 2009, pp. 216–223.[12] L. Devroye, L. Györfi, G. Lugosi, A Probabilistic Theory of Pattern Recognition, Springer-Verlag, New York, 1997.[13] E. Dokow, M. Feldman, R. Meir, I. Nehama, Mechanism design on discrete lines and cycles, in: Proceedings of the 13th ACM Conference on ElectronicCommerce (ACM-EC), 2012, forthcoming.[14] E. Dokow, R. Holzman, Aggregation of binary evaluations, Journal of Economic Theory 145 (2010) 495–511.[15] S. Dughmi, A. Ghosh, Truthful assignment without money, in: Proceedings of the 11th ACM Conference on Electronic Commerce (ACM-EC), 2010,pp. 325–334.[16] P. Fishburn, A. Rubinstein, Aggregation of equivalence relations, Journal of Classification 3 (1986) 61–65.[17] A. Gibbard, Manipulation of voting schemes, Econometrica 41 (1973) 587–602.[18] M. Guo, V. Conitzer, Strategy-proof allocation of multiple items between two agents without payments or priors, in: Proceedings of the 9th Interna-tional Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2010, pp. 881–888.[19] M. Guo, V. Conitzer, D. Reeves, Competitive repeated allocation without payments, in: Proceedings of the 5th International Workshop on Internet andNetwork Economics (WINE), 2009, pp. 244–255.156R. Meir et al. / Artificial Intelligence 186 (2012) 123–156[20] P. Harrenstein, M.M. de Weerdt, V. Conitzer, A qualitative Vickrey auction, in: Proceedings of the 10th ACM Conference on Electronic Commerce(ACM-EC), 2009, pp. 197–206.[21] M.J. Kearns, U.V. Vazirani, An Introduction to Computational Learning Theory, MIT Press, 1994.[22] J. Kemeny, Mathematics without numbers, Daedalus 88 (1959) 577–591.[23] E. Koutsoupias, Scheduling without payments, in: Proceedings of the 4th Symposium on Algorithmic Game Theory (SAGT), 2011, in press.[24] B. Leclerc, Efficient and binary consensus functions on transitively valued relations, Mathematical Social Sciences 8 (1984) 45–61.[25] D. Lowd, C. Meek, P. Domingos, Foundations of adversarial machine learning, Manuscript, 2007.[26] P. Lu, X. Sun, Y. Wang, Z.A. Zhu, Asymptotically optimal strategy-proof mechanisms for two-facility games, in: Proceedings of the 11th ACM Conferenceon Electronic Commerce (ACM-EC), 2010, pp. 315–324.[27] R. Meir, S. Almagor, A. Michaely, J.S. Rosenschein, Tight bounds for strategyproof classification, in: Proceedings of the 10th International Joint Conferenceon Autonomous Agents and Multi-Agent Systems (AAMAS), Taipei, Taiwan, 2011, pp. 319–326.[28] R. Meir, A.D. Procaccia, J.S. Rosenschein, Strategyproof classification under constant hypotheses: A tale of two functions, in: Proceedings of the 23rdAAAI Conference on Artificial Intelligence (AAAI), 2008, pp. 126–131.[29] R. Meir, A.D. Procaccia, J.S. Rosenschein, On the limits of dictatorial classification, in: Proceedings of the 9th International Joint Conference on Au-tonomous Agents and Multi-Agent Systems (AAMAS), 2010, pp. 609–616.[30] B. Mirkin, On the problem of reconciling partitions, in: H. Blalock (Ed.), Quantitative Sociology: International Perspectives on Mathematical and Statis-tical Modeling, Academic Press, New York, 1975.[31] I. Nehama, Approximate judgement aggregation, in: Proceedings of the 7th International Workshop on Internet and Network Economics (WINE), 2011,pp. 302–313.[32] N. Nisan, Introduction to mechanism design (for computer scientists), in: N. Nisan, T. Roughgarden, E. Tardos, V. Vazirani (Eds.), Algorithmic GameTheory, Cambridge University Press, 2007, Chapter 9.[33] A. Othman, E. Budish, T. Sandholm, Finding approximate competitive equilibria: Efficient and fair course allocation, in: Proceedings of the 9th Interna-tional Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2010, pp. 873–880.[34] J. Perote, J. Perote-Peña, Strategy-proof estimators for simple regression, Mathematical Social Sciences 47 (2004) 153–176.[35] J. Perote-Peña, J. Perote, The impossibility of strategy-proof clustering, Economics Bulletin 4 (23) (2003) 1–9.[36] A.D. Procaccia, M. Tennenholtz, Approximate mechanism design without money, in: Proceedings of the 10th ACM Conference on Electronic Commerce(ACM-EC), 2009, pp. 177–186.[37] A.D. Procaccia, A. Zohar, Y. Peleg, J.S. Rosenschein, The learnability of voting rules, Artificial Intelligence 173 (12–13) (2009) 1133–1149.[38] M. Satterthwaite, Strategy-proofness and Arrow’s conditions: Existence and correspondence theorems for voting procedures and social welfare func-tions, Journal of Economic Theory 10 (1975) 187–217.[39] J. Schummer, R.V. Vohra, Mechanism design without money, in: N. Nisan, T. Roughgarden, E. Tardos, V. Vazirani (Eds.), Algorithmic Game Theory,Cambridge University Press, 2007, Chapter 10.[40] L. Valiant, A theory of the learnable, Communications of the ACM 27 (1984).[41] V.N. Vapnik, A.Y. Chervonenkis, On the uniform convergence of relative frequencies of events to their probabilities, Theory of Probability and itsApplications 16 (2) (1971) 264–280.[42] R. Wilson, On the theory of aggregation, Journal of Economic Theory 10 (1975) 89–99.