ArticleDeep forecasting of translational impact in medicalresearchHighlightsd Deep learning models of biomedical paper content canaccurately predict translationd Deep content models substantially outperform traditionalcitation metricsd Models trained on patent inclusion transfer to predictingNobel Prize-preceding papersAuthorsAmy P.K. Nelson, Robert J. Gray,James K. Ruffle, ..., Bryan Williams,Geraint E. Rees, Parashkev NachevCorrespondenceamy.nelson@ucl.ac.uk (A.P.K.N.),p.nachev@ucl.ac.uk (P.N.)d Science policy is potentially better informed by deep contentmodels than by citationsIn briefAnalyzing 43.3 million biomedical papersfrom 1990–2019, we show that deeplearning models of publication title andabstract content can predict inclusion in apatent, guideline, or policy documentwith far greater fidelity than citationmetrics alone. If judgments of thetranslational potential of science are to bebased on objective metrics, then complexmodels of paper content should bepreferred over citations.Nelson et al., 2022, Patterns 3, 100483May 13, 2022 ª 2022 The Authors.https://doi.org/10.1016/j.patter.2022.100483llllOPEN ACCESSArticleDeep forecasting of translationalimpact in medical researchAmy P.K. Nelson,1,7,* Robert J. Gray,1 James K. Ruffle,1 Henry C. Watkins,1 Daniel Herron,2 Nick Sorros,3 Danil Mikhailov,3M. Jorge Cardoso,4 Sebastien Ourselin,4 Nick McNally,2 Bryan Williams,2,5 Geraint E. Rees,1,6 and Parashkev Nachev1,*1High Dimensional Neurology Group, UCL Queen Square Institute of Neurology, University College London, Russell Square House,Bloomsbury, London WC1B 5EH, UK2Research & Development, NIHR University College London Hospitals Biomedical Research Centre, London WC1E 6BT, UK3Wellcome Data Labs, Wellcome Trust, London NW1 2BE, UK4School of Biomedical Engineering & Imaging Sciences, King’s College London, London WC2R 2LS, UK5UCL Institute of Cardiovascular Sciences, University College London, London WC1E 6BT, UK6Faculty of Life Sciences, University College London, Gower Street, London WC1E 6BT, UK7Lead contact*Correspondence: amy.nelson@ucl.ac.uk (A.P.K.N.), p.nachev@ucl.ac.uk (P.N.)https://doi.org/10.1016/j.patter.2022.100483THE BIGGER PICTURE The relationship of scientific activity to real-world impact is hard to describe andeven harder to quantify. Analyzing 43.3 million biomedical papers from 1990–2019, we show that deeplearning models of publication, title, and abstract content can predict inclusion of a scientific paper in a pat-ent, guideline, or policy document. We show that the best of these models, incorporating the richest infor-mation, substantially outperforms traditional metrics of paper success—citations per year—and transfersto the task of predicting Nobel Prize-preceding papers. If judgments of the translational potential of scienceare to be based on objective metrics, then complex models of paper content should be preferred over ci-tations. Our approach is naturally extensible to richer scientific content and diverse measures of impact. Itswider application could maximize the real-world benefits of scientific activity in the biomedical realm andbeyond.Development/Pre-production: Data science output has beenrolled out/validated across multiple domains/problemsSUMMARYThe value of biomedical research—a $1.7 trillion annual investment—is ultimately determined by its down-stream, real-world impact, whose predictability from simple citation metrics remains unquantified. Here wesought to determine the comparative predictability of future real-world translation—as indexed by inclusion inpatents, guidelines, or policy documents—from complex models of title/abstract-level content versus citationsand metadata alone. We quantify predictive performance out of sample, ahead of time, across major domains,using the entire corpus of biomedical research captured by Microsoft Academic Graph from 1990–2019, encom-passing 43.3 million papers. We show that citations are only moderately predictive of translational impact. Incontrast, high-dimensional models of titles, abstracts, and metadata exhibit high fidelity (area under the receiveroperating curve [AUROC] > 0.9), generalize across time and domain, and transfer to recognizing papers of Nobellaureates. We argue that content-based impact models are superior to conventional, citation-based measuresand sustain a stronger evidence-based claim to the objective measurement of translational potential.INTRODUCTIONScientometrics has existed for only a small fraction of the historyof science itself, sparked by the logical empiricists of the ViennaCircle in their philosophical quest to construct a unified languageof science.1 Developed into the familiar, citation-centered formthrough arduous manual extraction in the mid-20th century,2,3its indicators have proliferated in the Internet age. They nowdominate the research landscape, routinely informing majorfunding decisions and academic staff recruitment worldwide.4–8Patterns 3, 100483, May 13, 2022 ª 2022 The Authors. 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).llOPEN ACCESSThe importance of the original goal has become magnifiedover time: to measure scientific progress regardless of fundingor ideology, uncolored by the reputations of individuals or insti-tutions. But the fundamental focus of its current solution—thevolume and density of discussion in print—is detached fromthe ultimate, real-world objective and subject to familiar distor-tions, such as the popularity of papers notable only for beingspectacularly wrong.9–11These concerns are amplified in medical science, whose pri-mary focus is not merely knowledge but impact on patient health:necessarily a consequence rather than a constitutive character-istic of research activity, neither easily benchmarked nor directlyoptimized. And there is no doubt that optimization is needed;over the past 60 years, the number of new drug approvals perunit R&D spend has consistently halved every 9 years, whereaspublished medical research has doubled with the same period-icity,12 and only 0.004% of basic research findings ultimatelylead to clinically useful treatments.13 The critical pre-requisitefor all research—funding—shows substantial randomness in itsdistribution,14 enough for at least one major healthcare funderto award grants by lottery.15 Any decision function based onrandom chance, or a process demonstrably not much betterthan random chance, leaves room for improvement, particularlywhen commanding approximately $1.7 trillion global annual in-vestment across the United States, Japan, South Korea, andthe European Union.16Is this state of affairs partially caused by over-reliance onmisleading scientometrics, have we simply not found the rightmetrics yet, or is the relation between scientific activity andconsequent impact opaque to objective analysis? To addressthese crucial questions, we need a fully inclusive survey ofpublished medical research that relates its characteristics toan independently measured translational outcome as close toreal-world impact as can be quantified. This relationshipmust be explored with models of sufficient expressivity todetect complex relations between many candidate predictivefactors beyond paper-to-paper citations. The extant literatureis largely limited to modeling keywords or simple representa-tions of semantic content,17–21 within specific subdomains, orcomparatively restricted bibliographic databases,22–26 andwithout exploration of the impact of data dimensionality andmodel flexibility.Here we provide the first comprehensive, field-wide analysisof translational impact measured by its most widely acceptedproximalindices—patents, guidelines, or policies—based on29 years of data from the medical field encompassing 43.3million published papers. We quantify the ability to predict inclu-sion in future patents, guidelines, or policies from conventionalage-normalized citation counts and compare this with the pre-dictive fidelity of deep learning models incorporating more com-plex features extracted from metadata, titles, and abstracts. Weevaluate the performance of the best model across time and the-matic domain and in transfer to the task of recognizing papers ofNobel laureates. We derive succinct, surveyable representationsof paper title and abstract content with deep autoencoding oftransformer-based text embeddings and of publication meta-data with stochastic block models. The breadth and depth ofanalysis allow us to draw strong conclusions about the compar-ative fidelity of conventional bibliographic and novel semantic2 Patterns 3, 100483, May 13, 2022Articlepredictors of translational impact, with substantial implicationsfor research policy.RESULTSCitationsOver the period from January 1990 to March 2019, only 17.1million of the 43.3 million published papers categorized as med-ical by Microsoft Academic Graph were cited at least once. Ofthese, 964,403 were included in a patent and 16,752 in a guide-line or a policy document. Included papers were more frequentlycited, but the numbers of citations and inclusions were weaklycorrelated (Pearson’s r = 0.094 for guidelines or policies, r =0.248 for patents; Figure 1). The mean time delay from paperpublication to first patent inclusion was 4.73 years (SD 4.54;Figure S1).Predictive performanceA series of models was developed to investigate the relativecontribution of three data modalities—annual paper citations,metadata only, and the combination of metadata and abstract/ti-tle embeddings—in predicting two translational outcomes: apaper’s inclusion in a patent or policy/guideline reference list. At-tempting to predict inclusion in a guideline or policy documentfrom the traditional measure of impact—annual paper cita-tions—yielded a mean cross-validated area under the receiveroperating curve (AUROC) of 0.766 with univariable logisticregression (Citations-LogisticRegression) and 0.767 with anoptimized univariable multilayer perceptron (MLP) model (Cita-tions-MLP).In contrast, a high-dimensional model trained on metadataand title and abstract embeddings, based on a hybrid MLPand convolutional neural network (CNN), Full-MLP-CNN,achieved an AUROC of 0.915 and average precision (AP) of0.919 on unseen test data (Figure 2A). The MLP trained on onlymetadata, withouttitle or abstract embeddings (Metadata-MLP), achieved a lower mean cross-validated AUROC of0.882, significantly so, as judged by cross-validation confidenceintervals.For the task of predicting patent inclusions, annual paper cita-tions yielded a mean cross-validated AUROC of 0.756 with uni-variable logistic regression (Citations-LogisticRegression) andoptimized univariable MLP (Citations-MLP).A high-dimensional model trained on metadata and title andabstract embeddings (Full-MLP-CNN) achieved a much higherAUROC of 0.918 and AP of 0.859 on unseen test data (Figure 2B).The MLP trained only on metadata, without title or abstract em-beddings (Metadata-MLP), achieved a lower mean cross-vali-dated AUROC of 0.876.Across both tasks, a high-dimensional neural network modeltrained on metadata and content embeddings substantially out-performs more commonly used citation based metrics when pre-dicting future translational impact.Performance over time and across research domainTo test the generalizability of the models, we must examinesustained performance over time and across domains. Forguideline or policy documents,the high-dimensional Full-MLP-CNN model trained only on data from 1990–2013 andArticlellOPEN ACCESSFigure 1. Relationship between paper citations and translational document inclusionsShown are citation histograms of papers included (red) or not included (blue), plotted with semi-transparency on the same log axes, in guideline or policydocuments (top left) or patents (top right). The area of overlap is shown in purple, and the contrasting papers are all other identically filtered biomedical paperswith at least one citation.. The relationship between citation and inclusion counts for included papers is shown in binned scatterplots for guideline or policyinclusions (bottom left) and patent inclusions (bottom right), also plotted on log axes.tested on out-of-sample papers published over all succeeding4 years achieved an AUROC of 0.920 and an AP of 0.911 (non-averaged). Crucially, there was no appreciable diminution in fi-delity over time for individual years (Figures 3A, 3B, and S2A).Performance was consistently good to excellent within eachof the top 8 most common domains of medicine (Figures 3Cand 3D).An identical analysis of patent inclusions produced a similarpicture, yielding an AUROC of 0.902 and an AP of 0.606 forout-of-sample papers published over all succeeding 4 years(non-averaged), with no diminution over time for individual yearson AUROC but some diminution on AP (Figures 3E and 3F), likelyreflecting the correspondingly shorter time frames for realizationof any patent inclusion; papers in later years would have to beincluded within fewer years, leading to an artificially deflated pro-portion of included papers and penalized specificity. Indeed, theAUROC improved with time (Figure S2B), likely reflecting geo-metric growth in publication numbers (doubling every 9 years),changes in publication and citation patterns, and an increaseover time of patents in which papers may be included. Futureperformance was consistently good to excellent within each ofthe top 8 most common domains (Figures 3G and 3H).Patterns 3, 100483, May 13, 2022 3llOPEN ACCESSABFigure 2. Model predictive performance(A and B) Shown are cross-validated receiver operating curves (ROCs) for theFull-NLP-CNN model trained on metadata and title and abstract embeddings(orange for validation, purple for held-out test) and the Metadata-MLP model(red) and the Citation-LogisticRegression model trained on citation count peryear (blue) for guideline or policy inclusions (A) and patent inclusions (B). Theconfidence intervals are ±2 SD on cross-validation.Data ablation studiesRebuilding the Full-MLP-CNN models without paper-level met-rics—paper citations, paper rank, and paper mentions—andseparately, also without attributes influenced by factors of meritextrinsic to the paper itself—affiliation, authors, journal, and4 Patterns 3, 100483, May 13, 2022Articlefield—yielded slightly diminished fidelity. For guideline and pol-icy inclusions, the paper-level metrics-ablated model achievedan AUROC of 0.905, and the model ablated of paper-level met-rics and extrinsic factors achieved an AUROC of 0.896. The cor-responding values for models based on metadata only were0.832 and 0.816. A model trained only on title-abstract embed-dings, without any metadata at all, achieved an AUROC of0.892 (Figure S3A). For patent inclusions, identically constrainedmodels yielded AUCROCs of 0.881, 0.866, 0.847, 0.813, and0.822, respectively (Figure S3B).Transfer to predicting papers preceding a Nobel PrizeIf the high-dimensional models are capable of capturing funda-mental features of translational impact, they may identify pa-pers whose impact is judged by other criteria. To test forsuch transfer learning, we applied our best patent model(Full-MLP-CNN)—trained on data with Nobel Prize-precedingpapers removed and without retraining on new targets—tothe task of identifying the papers, published before the prizewas awarded, of Nobellaureates in physiology or medicinefrom 1990–2019.We identified 166 papers, 60 of which were included in pat-ents. Strikingly, the Full-MLP-CNN model retrieved a substan-tially higher proportion of Nobellaureate papers (103 of 166)than Metadata-MLP (86 of 166) or Citations-LogisticRegression(23 of 166) while retaining superior fidelity for detecting patent in-clusions (AUROC 0.79 versus 0.73 and 0.73, respectively).Predictors of inclusionA complex, high-dimensional model cannot easily yield intelli-gible weightings of predictive importance because its decisionis a highly non-linear function of a large set of input features. Acoarse indication of relative feature importance can nonethe-less be derived from alternative architectures oflesserflexibility. Here a boosted trees model (Metadata-AdaBoost)was used, trained on the metadata and optimized by gridsearch to similar performance as the MLP (AUROC 0.878,guideline or policy inclusions; AUROC 0.877, patent inclu-sions) (Tables S1A and S1B).For guideline or policy inclusions, the rank of the paper, ametric provided by Microsoft Academic Graph (MAG),27 reflect-ing the eigencentrality-based ‘‘influence’’ of a paper, had thehighest feature importance, followed by the paper count, cita-tion count, and rank of the journal in which the paper was pub-lished. For patentthe top three features wererelated to journal productivity-related metrics. The top 10feature importances of models restricted to data before 2014were very similar to those trained on the full time period,although the ordering was different in the patent model, withgreater weight on citations, year, and field productivity(Table S1B).inclusions,Deep semantic structure of titles and abstractsTextual analysis of title or abstract content cannot easily yield anintelligible set of predictive features as in the foregoing models.But we can visualize the sentence-level embeddings of the titleand abstract encoded by BioBERT,28 a rich, context-aware rep-resentation of naturallanguage concepts tuned on biologicaltext, through a succinct representation generated by a deepArticleACEGBDFHllOPEN ACCESSFigure 3. Predictive performance in futureyears and most common fields(A–H) ROC curves (A and C) and precision-recallcurves (B and D) for the Full-MLP-CNN model trainedon data from 1990–2013 and tested on papers pub-lished in the subsequent 4 years, plotted by year,for guideline or policy inclusions and patent inclu-sions, respectively. Also shown are ROC curves (Eand G), and precision-recall curves (F and H) for theFull-MLP-CNN model trained on data from 1990–2013 and tested on data from 2014–2019, plottedby each of the top 8 most common fields, for guide-line or policy inclusions and patentinclusions,respectively.Patterns 3, 100483, May 13, 2022 5llOPEN ACCESSautoencoder. Represented in a two-dimensional space throughnon-linear dimensionality reduction, the embeddings showed adegree of disentanglement of clusters rich in guideline or policyinclusions versus none (Figure 4A). This reveals intrinsic struc-ture in the data exploited by the hybrid model to achieve thehigh classification performance observed. An identical analysisof the structure of the patent inclusion embeddings revealed asimilar intrinsic structure (Figure 4B).Graph community structureThe similarity and dissimilarity between papers can be modeledas a graph whose edges index the dependencies between indi-vidual features. Hierarchically arranged distinct patterns of sim-ilarity, the graph’s community structure, can then be revealed bystochastic block modeling,29 here performed separately forguideline or policy-included papers and patent-included papers,each compared against all other papers.institutional,Distinct communities of author,journal, anddomain features emerged across both groups (Figures 5 and6). Overall, the community structures of papers not includedin guideline, policy or patent documents were most similar, asindexed by pairwise comparisons of the log-normalized mutualinformation of the inferred model parameters, and the commu-nity structure of guideline- or policy-included papers was mostdistinctive (Figure S4). This observation cohered with the struc-ture of an undirected features graph, weighted by the absolutecorrelation coefficient between features, that showed patent in-clusions to be more centrally embedded within the widernetwork of metadata than guideline or policy inclusions(Figure S5).Contrasting the effect of inclusion in the guideline or policygroup, indices related to the first author and journal were moredecisive in the included papers, whereas indices related to theinstitution and journal were more decisive in the others (Fig-ure S6A). The domains of virology, endocrinology, alternativemedicine, psychiatry, nursing, and environmental health werealso more prominent in the former and surgery, radiology,traditional medicine, and rehabilitation in the latter. The effectof inclusion in the patent group was most strongly manifestedin institutional indices for the included group and field indicesfor the others. The contrast between domains was more strikingthan in the guideline or policy model, with pharmacology espe-cially dominant in the included group and general medical spe-cialties in the others (Figure S6B).The translational impact of journalsJournal impact factors—indices of the annual citation return ofan average paper—exclude patent, guideline, or policy inclu-sions. So ranked, the top 10 journals in the medical domainbased on cited papers published between 1990 and 2019are listed in Table 1. This corresponds to a medical domain‘‘impact factor’’ over three decades rather than the commonlyreported annual. The equivalent ranking for guideline and pat-ent inclusions, identically filtered, are listed in Tables 2 and 3,respectively. In the absence of plausibly objective weighting ofpolicies or guidelines, this metric will be sensitive to the nu-merosity of distinct policy documents within any givendomain, reflecting its political, regulatory, or administrativecomplexity.6 Patterns 3, 100483, May 13, 2022ArticleDISCUSSIONWe provide the first comprehensive framework for forecastingthe translation of published medical research in the form of pat-ent, guideline or policy inclusions, reveal the community struc-ture of translational inclusions, and compute the top translation-ally relevant journals across biomedicine over the past threedecades.Simple citation versus complex content metricsWe show that standard citation metrics are markedly inferior tothose derived from complex models based on more detailed de-scriptions of published research. If objective metrics are to beused in translational assessment, then the use of conventionalmetrics is here shown to be insupportable. Our analysis sug-gests that the problem rests not with citations but with the ex-pressivity of any simple metric of something as constitutionallycomplex as research translation. It is clear that the translationalsignal is distributed through the combinatorial fabric of papercitation networks, metadata, and content captured in titles andabstracts. No easily interpretable scalar value could capture it.Conversely, that surprisingly economical information about a pa-per—its metadata, title, and abstract—can be exploited by theright modeling architecture to yield high predictive fidelity meansthat no one could argue that no objective alternative is available.Even without full text information, we can confidently identifylarge swathes of research activity unlikely to inform guidelinesor policy or to become the substrate of patents across timeand diverse subdomains. The choice now is not between subjec-tive, qualitative assessment and simple quantitative metrics, butincludes machine learning models that are no less objective,reproducible, and generalizable for being complex.We cannot and do not argue that machine learning models re-move the need for qualitative assessment, but only that thequantitative metrics in current use could be far better. Contentparameterization of entire scientific fields, limited in existing liter-ature to keyword analysis or word-level or simple document-level embeddings,17–21 can be usefully extended using deeplearning models, such as those applied here, to capture a greaterdepth of meaning from abstracts. Indeed, the clearly observedrelation between model complexity and achieved fidelity sug-gests that modeling of the body of a paper—currently infeasiblefor copyright reasons—is likely to yield still higher fidelity. Theanalysis of metadata may additionally be expanded with inclu-sion of wider dissemination scores, such as those captured inAltmetric, which has already been examined at single-journalscale,25 and more widely across the full Scopus database,22for predicting paper-paper citations. This will inevitably usheran examination of policies on the right trade-off between perfor-mance and intelligibility that must be settled politically, notempirically.Possibilities and limitations of complex translationalforecastingOur models are of direct, first-order inclusions, indifferent to theupstream published sources a given paper itself cites. They maybe more likely to predict the translational potential of a meta-analysis, for example, than that of any of the preceding studiesinforming it. But the proposed framework can be naturallyArticlellOPEN ACCESSFigure 4. t-distributed stochastic neighbor embed-ding (t-SNE) projection in 2 dimensions of title and ab-stract BioBERT autoencoder embeddings(A and B) Labeled by the presence (orange) or absence (blue)of a guideline or policy (A) or patent (B) inclusion. Note thediscernible data structure that enables accurate prediction ofinclusion but is too complex to be reduced to any small set ofcharacteristic features.Patterns 3, 100483, May 13, 2022 7llOPEN ACCESSArticleFigure 5. Nested stochastic block models (SBMs) showing the community structure of the metadata of papers included in guidelines or pol-icy versus those not includedNode size in these models corresponds to the eigencentrality of each feature, edge weight corresponds to the pairwise absolute value of the correlation co-efficient between features, and the colours indicate community membership at the lowest hierarchy. The included class is the bottom hemifield.extended to second- or higher-order inclusions earlier in the cita-tion path, weighting the cascade of information down the fulltranslational pathway in a principled way. For example, the cita-tion nexus has been modeled as a graph,30 with publication-based metrics as the predictive target, in evolution of establishedapproaches for predictive modeling of bibliographically definedimpact.31,32 The constraint on inclusion depth, among other con-siderations, prevents naive use of our models to determine thecausal sufficiency of translation, but no one would claim thatany metric within so complex a system could plausibly indexcausality on its own. A complex model can also be used to distin-guish empirical from meta-analytical papers with potentiallygreater accuracy than bibliographic ‘‘article type’’ tags, weight-ing inclusions by their empirical content.Equally, although unethical biases can corrupt carelessly de-signed or interpreted complex models, they can also be revealedby them, where the neglected subpopulation is defined by thecomplex interaction of several variables of ethical concern8 Patterns 3, 100483, May 13, 2022ArticlellOPEN ACCESSFigure 6. Nested SBMs showing the community structure of the metadata of papers included in patents versus those not includedNode size in these models corresponds to the eigencentrality of each feature, edge weight corresponds to the pairwise absolute value of the correlation co-efficient between features, and the colours indicate community membership at the lowest hierarchy. The included class is the top hemifield.simple models are too crude to illuminate. Insisting on simple,low-dimensional decision boundaries does not remove biasbut merely conceals it from view; complex models, correctly de-signed and used, are not the problem here but an essential partof the solution. A sharp distinction must be drawn betweensimplicity and explainability; where a system is inherently, irre-ducibly complex, a simple metric cannot be explanatory. The un-precedented scale of analyzed data, drawn from the largestopen bibliographic repository in the world, limits potential distor-tion from sampling bias; use of out-of-sample, ahead-of-timemeasures of performance further strengthens generalizability.Use of content and metadata overcomes the limits of eitherused alone: highly discussed incorrect research or falsely in-flated citation counts and major theoretic advances withoutany secondary spread can be handled by a model incorporatingboth inputs.Our demonstration that a purely content-based model, shornof author and institutional features, is highly predictive of trans-lational impact shows that the predictive signal does not merelyreflect institutional productivity or prestige and can be used toPatterns 3, 100483, May 13, 2022 9llOPEN ACCESSArticleTable 1. Top 10 journals by paper citations per paperJournalPaper citations/total papersPaper citationsTotal papersAnnual Review of ImmunologyPhysiological ReviewsAnnual Review of NeurosciencePsychological BulletinPharmacological ReviewsCellAnnual Review of PsychologyCA: A Cancer Journal for CliniciansPsychological ReviewClinical Microbiology Reviews480.835407.457383.877333.302318.500295.994283.465263.673262.322250.334issues associated with reliance on metadata,address ethicalsuch as weighting of institutions on purely historical perfor-mance. Any individual or institution can submit test data to themodel and independently validate predictions over time orretrain with further, prospectively acquired data to ensureadequate handling of future time-varying trends or extension toother data types within MAG, such as preprints. The fidelity ofany prediction is inevitably constrained by the quality of thedata used to train the model from which it is derived; as biblio-graphic databases improve, so should the models built onthem. Further development might also helpfully include semanticanalysis to contextualize the high-dimensional content embed-dings, allowing further insights into emerging patterns of transla-tional impact.Our work builds on existing research on patent, guideline, andpolicy inclusions. A ‘‘patent-paper citation index’’ has been pro-posed to formalize science-to-technology linkages,33 and patentinclusions have been systematically evaluated to quantify valuereturn on public research investments34 and used as a markerof the technological importance of scientific papers.23 Althoughit may seem that patents should precede published research, alarge study of United States patent and paper linkage foundthat 60% referenced prior research.35 Patent inclusions havetherefore been explored as indicators of papers whose recogni-tion has been delayed35 and, therefore, are an establishedindicator for translational merit, especially of basic science. Simi-larly, a focus on impact assessments has prompted analyses ofreferencing patterns within cancer guidelines,36 small hand-curated groups of guidelines,37 and, separately, policy inclu-sions extracted by hand,24 systematic analysis of coronavirusdisease 2019 ( COVID-19) policy,26 or from Altmetric,38 althoughdifficulties with comprehensive data acquisition have hamperedthe latter. Although one study has recently attempted to predictcombined guideline and clinical trial citations of basic researchusing a small set of Medical Subject Headings term-derived fea-tures,19 no comprehensive predictive framework for the tangibleproduct of scientific research, rather than trials, has beendescribed previously. The critique of paper citation metrics formeasuring impact is not new and has been described at lengthelsewhere, but the argument can now be rigorously testedagainst objective markers of translation.Application of highly expressive language models to search-able, comprehensive, fully digitized repositories of scientificpublications has the power to derive compact but rich represen-10 Patterns 3, 100483, May 13, 2022376,494392,789240,691409,295210,8473.229.298161,575240,733247,107239,3197839646271,22866210,910570913942956tations of research activity on which high-fidelity predictivemodels can be founded. Here focused on the task of predictingtranslational signals, the approach can be used to forecast manyaspects of scientific activity upstream of real-world impact. Ourwork argues for a radical shift toward adoption of novel methodsfor evaluation of medical research, a shift for which observedlevels of translational productivity—declining for more than halfa century—demand urgent and decisive action.EXPERIMENTAL PROCEDURESResource availabilityLead contactFurther information and requests for resources should be directed to the leadcontact, A.P.K.N. (amy.nelson@ucl.ac.uk).Materials availabilityThis study did not generate new unique reagents.Data and code availabilityThis paper analyzes existing, publicly available data, available by applicationto MAG(https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/). Guideline and policy data are available from the Reachproject at Wellcome Data Labs (https://reach.wellcomedatalabs.org/). Codefor extracting guideline and policy references is available at https://github.com/wellcometrust/reach. Analytic code will be made available upon reason-able request. Any additional information required to reanalyze the data re-ported in this paper is available from the lead contact upon reasonablerequest.DataThe dataset was downloaded from MAG, the largest and widest citationcoverage open publications database,27,39 in March 2019. It was filtered toinclude medical papers, as labeled by MAG, published from January 1990 toMarch 2019, with at least one paper-to-paper citation. Papers were extractedby filtering for ‘‘Doc-Type’’ attribute ‘‘journal’’; medical papers were further iso-lated by filtering on the ‘‘Field’’ code specific to ‘‘Medicine.’’ To extract a patentinclusion count, papers were matched by ID to the reference list on patent en-tries, in turn provided within MAG through the Lens database; a detaileddescription of these data sources is available elsewhere.40 To extract guidelineor policy inclusion counts, papers were matched by title to a dataset kindlyprovided by the Wellcome Trust, containing reference lists scraped from doc-uments on the World Health Organization, National Institute of Clinical Excel-lence, UNICEF, Me´ decins Sans Frontie` res, United Kingdom government, andUnited Kingdom Parliament websites. A free web-based tool for guideline andpolicy inclusion detection is available from Wellcome Data Labs (https://reach.wellcomedatalabs.org/), and associated code is available (https://github.com/wellcometrust/reach). Title matching was by a combination of fuzzy matchingand cosine similarity of term frequency-inverse document frequency vectors,with manual cleaning of the resulting matches focused on titles with low fuzzymatching and cosine similarity scores and shorter word counts.ArticleTable 2. Top 10 journals by guideline or policy inclusions per paperllOPEN ACCESSJournalTobacco ControlEastern Mediterranean Health JournalNoise & HealthHuman Resources for HealthHealth Policy and PlanningInfluenza and Other Respiratory VirusesGlobalization and HealthPLOS MedicineBulletin of The World Health OrganizationTrauma, Violence, & AbuseGuideline or policyinclusions/total papersGuideline or policy inclusionsTotal papers0.0940.0910.0850.0840.0790.0610.0430.0400.0390.0383,12028255691576329152247223,3253,09064982419771,0366813,8096,278574The full feature list extracted from MAG is included in Table S2 and summar-ily comprised publication year; paper citation count; paper rank; author count;reference count; and rank, paper count, and paper citation information for thefirst and last author, the first and last authors’ affiliations, the journal, and thefield. First and last authors were isolated from ordered author lists suppliedin MAG and used in place of the full author list to avoid variably sized or sparseauthor feature sets, with the rationale that these authors tend to have thelargest effect on a paper. The first level of medical domain fields were ex-tracted, 43 in total, and added as features using multiple one hot encoding.Field names from hierarchical topic modeling were supplied in MAG,41 andrank, a reinforcement learning estimation of dynamic eigencentrality, reflectinga paper’s connectedness to other influential entries in the graph,42 was alsosupplied in MAG. In addition to a simple paper citation count, the number oftimes a paper was referenced in the text body of another paper was summedto create a ‘‘paper mentions’’ count.Predictive analysisNatural language processingMedical papers were further filtered to include those with titles and abstracts.Sentence-level embeddings were generated for each title using BioBERT,28 astate-of-the-art BERT language model pre-trained on biomedical corporacomprising PubMed abstracts and PubMed Central full-text articles, in addi-tion to general corpora comprising English Wikipedia and BooksCorpus.BERT is a highly influential Transformer encoder, released in 2018, that isable to learn the context of words by joint conditioning on the full sentencerather than creating a sequential representation where context is lost withincreasing distance between words.43 The sentence-level embeddings werederived from the output of the first (classification) token.To create a fixed-length abstract-level embedding, we truncated the ab-stracts to 20 sentences or zero padded where the abstract was shorter, re-placing each sentence with its BioBERT embedding and concatenating thearray to create a 15,360-dimensional vector. The truncation threshold wasmotivated by empirical investigation of abstract sentence count distributionwithin training data; 92% of papers had 20 sentences or less (Figure S7).This was further concatenated with the title vector, creating a 16,128-dimen-sional representation of the title and abstract taken together.PreprocessingTo rebalance the proportions of positive and negative target labels, the major-ity negative class was randomly sampled without replacement; this rebalanc-ing strategy was motivated by the abundance of data, a preference towardfewer assumptions at the cost of poorer fit, and the desire to avoid linear over-sampling techniques, such as synthetic minority oversampling, which havebeen shown to underperform in higher dimensions.44 Papers without a titleor abstract were then removed. This led to a 1.1:1 balance of positive to nega-tive labels in the patent group and the guideline or policy group. Data wererandomly split into label-stratified training and test sets with a 9:1 ratio. Missingvalues in the metadata were imputed with medians derived from the trainingsplit, and values were transformed into Z scores.ModelingTo address the primary objective of detecting signals of translation, we traineda series of models to predict a binary outcome of inclusion in a patent versusnone and, separately, a binary outcome of inclusion in a guideline or policydocument versus none. This was motivated by two considerations: first, thateach outcome was an independent measure of translation rewarding predom-inantly fundamental, basic science or applied, clinical (and meta-analytical)science in patent and guideline or policy classes, respectively, and second,that patent inclusions were around 50 times more prevalent than guidelineor policy inclusions and might unfairly dwarf the predictive signal of thelatter class.We first modeled a single variable—paper citations per year—using logisticregression to provide a baseline prediction reflecting current citations-basedpractice (Citations-LogisticRegression). The hyperparameters of this modelwere optimized using a parallelized, cross-validated grid search. Logisticregression was selected for its simplicity over hyperparameter-optimizedTable 3. Top 10 journals by patent inclusions per paperJournalPatent inclusions/total papersPatent inclusionsTotal papersAnnual Review of ImmunologyNature BiotechnologyProtein EngineeringPharmacological ReviewsTrends in BiotechnologyCellJournal of Experimental MedicineAdvanced Drug Delivery ReviewsChemical ReviewsTransfusion Science5.7333.8623.1533.0452.9902.7252.2932.1671.9251.8454,48917,0422,5572,0164,73329,72924,1056,8621,3051,5527834,4138116621,58310,91010,5133,166678841Patterns 3, 100483, May 13, 2022 11llOPEN ACCESSMLPs given statistically equivalent 10-fold cross-validation performance. Sec-ond, we modeled the metadata—all features extracted from MAG pertaining tothe paper and its research environment, excluding title and abstract embed-dings—using a 6-layer perceptron with categorial variables one-hot encoded(Metadata-MLP). Third, motivated by the tiled structure of the numerical ab-stract and title representations, we trained a 1-dimensional CNN for classifica-tion, using an initial kernel length and stride of 768 to match the length of eachsentence vector, yielding a compact text representation upstream of the fullyconnected layers.These two models were tuned by cross-validation within the training set, andthe best models were combined into a final model that took the metadata andtitle-abstract embeddings as inputs, as specified in Figure S1 (Full-MLP-CNN).The differing tensor sizes of metadata and title-abstract embeddings at theconcatenation layer of the final model matched the optimal architectures ofthe individual models; the need for higher relative compression of the title/ab-stract embeddings likely reflects the higher density of information in themetadata.InterpretabilityDeep neural networks do not explicitly provide quantification of the importanceof individual features to prediction. We therefore trained and grid-search-opti-mized an AdaBoost model45 on metadata features (Metadata-AdaBoost), cho-sen for its explainability balanced with good sensitivity to linear and non-lineareffects, and extracted Gini-importance from the best-performing model onvalidation data.To illuminate the title-abstract embeddings, we trained a fully connected au-toencoder on the 16,128 BioBERT dimensions of each title and abstract,deriving a 50-dimensional representation compressed to two dimensionswith t-distributed stochastic neighbor embedding (t-SNE).46 The resultingplot was colored by the presence or absence of a translation inclusion.Model evaluationThe predictive performance of all models on the training set was evaluated bystratified 10-fold cross-validation using AUROC and AP, a measure of the areaunder the precision recall curve. The former is a common metric for assessingpredictive performance that balances sensitivity against specificity across arange of classification thresholds, and the latter is more resistant to imbal-anced data bias and balances sensitivity against precision, the purity of pre-dicted positive results. The final, tuned, highest-performing model was testedon the unseen test data and assessed by AUROC and AP. All AUROCs andAPs were macro averaged.To assess the performance of the final model on future papers, the same ar-chitecture was trained from scratch on data from January 1990 to December2013 and tested on data from January 2014 to December 2017. Papers pub-lished from January 2018 to March 2019 were not used for testing because ofthe short latency of conversion to first patent, policy, or guideline inclusion(Table S3). We measured the performance in the full set of future papers toobtain summary metrics, in addition to individually across each of the 4 years,and within the top 8 fields to investigate the calibration to these groups. Anypapers with multiple field membership were considered in each appropriatefield. To quantify any reliance on time-dependent citation patterns for a givenpaper, we assessed the performance of the full model whose training set had‘‘paper citation count,’’ ‘‘paper rank,’’ and ‘‘paper mentions count’’ variablesremoved; similarly, to quantify any reliance on features denoting merit extrinsicto the paper, author-, institution-, journal-, and field-level ranks and countswere removed.As further validation, an external, publicly available dataset containing thepublication output of Nobel Prize laureates in physiology or medicine47 wasdownloaded, matched to MAG, and processed identically to the test data.All papers from 1990 to 2019 published up to and including each prize-winningpaper were tested on Citations-MLP, Metadata-MLP, and Full-MLP-CNN pat-ent inclusion models retrained on the entire corpus with the tested papersremoved. The AUROCs and numbers assigned to positive and negative labelswere recorded. Nobel prizes were counted from 1991–2019 to allow analysisof at least 1 year of papers preceding the first award.Articleture of patent included versus non-included groups and guideline or policyincluded versus not included groups.Toward the former aim, we plotted paper citations against translation inclu-sions and examined their correlation by fitting a linear regression model with1,0003 bootstrapped confidence intervals. We ranked journals by paper cita-tion counts normalized by the journal’s total paper count within our dataset,filtered as described for medical papers from 1990–2019 with at least one cita-tion. This roughly corresponds to a canonical ‘‘impact factor,’’ although the in-terval is widened from yearly to three decades. We repeated this for a journal’spatent inclusions count and guideline or policy inclusion count. Journalsanalyzed in this manner were filtered to include only those with 500 or more to-tal papers in the dataset.Toward the latter aim, we fit Bayesian weighted, non-parametric, nestedstochastic block models29 on all papers with patent inclusions and all paperswithout them and then again on all papers with guideline or policy inclusionsand all papers without them, degree corrected and weighted exponentiallyby the absolute value of the pairwise correlations of features extracted fromMAG (excluding titles and abstracts). Stochastic block models are generativerandom graph models that display community structures, subsets of nodesconnected by larger edge densities than those outside of the subset. Themodels were strengthened by sampling from the posterior distribution andequilibrated with Markov chain Monte Carlo over 100,000 iterations to ensureconvergence. Scalable force-directed placement48 was used for visualizationof the combined feature graph, with node size proportional to eigencentralityand edge weight and color proportional to the absolute value of the correlationcoefficient between two features.Analytic environmentAll analyses were written in Python 3.5. Preprocessing was performed usingPandas,49 NumPy,50 and Scikit-Learn51 and visualization using Matplotlib,52Seaborn,53 and Graph-tool.54 Neural networks were built in Keras55 with Ten-sorflow backend and PyTorch;56 other models were built in Scikit-Learn. t-SNEwas performed using Multicore-TSNE,57 and BioBERT models were down-loaded and implemented locally. The hardware specification used was as fol-lows: 96 gigabyte random access memory, Intel Xeon(R) central processingunit E5-2620 v.4 at 2.10 GHz 3 32 processor, and GeForce GTX 1080/PCIe/SSE2 graphics.SUPPLEMENTAL INFORMATIONSupplementalpatter.2022.100483.information can be found online at https://doi.org/10.1016/j.ACKNOWLEDGMENTSA.P.K.N., R.J.G., P.N., D.H., N.M., and B.W. are supported by the NIHR UCLHBiomedical Research Centre. P.N. is supported by the Wellcome Trust.AUTHOR CONTRIBUTIONSConceptualization, A.P.K.N., P.N., D.H., N.M., G.E.R., and B.W.; methodol-ogy, A.P.K.N., P.N., R.J.G., J.K.R., and H.C.W.; software, A.P.K.N., R.J.G.,J.K.R., and H.C.W.; validation, A.P.K.N. and P.N.; formal analysis, A.P.K.N.and P.N.; resources, N.S. and D.M.; data curation, A.P.K.N., N.S., and D.M.;writing – original draft, A.P.K.N. and P.N.; writing – review & editing,A.P.K.N., P.N., R.J.G., J.K.R., M.J.C., D.H., N.M., G.E.R., and B.W.; visualiza-tion, A.P.K.N. and P.N.; funding acquisition, P.N., G.E.R., D.H., N.M., B.W.,M.J.C., and S.O.DECLARATION OF INTERESTSThe authors declare no competing interests.Descriptive analysisAs a secondary objective, we sought to understand the correspondence ofpatent and guideline or policy citations to the far more widely measured andacknowledged paper citations as well as to understand the community struc-Received: December 7, 2021Revised: January 10, 2022Accepted: March 4, 2022Published: April 8, 202212 Patterns 3, 100483, May 13, 2022ArticleREFERENCES1. Bellis, N.D. (2009). Bibliometrics and Citation Analysis: From the ScienceCitation Index to Cybermetrics (Scarecrow Press).2. Garfield, E. (1968). A unified index to science. Proc. Int. Conf. Sci. Inf. 1,461–474.3. Price, D.J. (1986). Little Science, Big Science– and beyond (ColumbiaUniversity Press).4. Nicholson, J.M., and Ioannidis, J.P.A. (2012). Conform and be funded.Nature 492, 34–36. https://doi.org/10.1038/492034a.5. Lewison, G., Cottrell, R., and Dixon, D. (1999). Bibliometric indicators toassist the peer review process in grant decisions. Res. Eval. 8, 47–52.https://doi.org/10.3152/147154499781777621.6. Patel, V.M., Ashrafian, H., Ahmed, K., Arora, S., Jiwan, S., Nicholson, J.K.,Darzi, A., and Athanasiou, T. (2011). How has healthcare research perfor-mance been assessed?: a systematic review. J. R. Soc. Med. 104,251–261. https://doi.org/10.1258/jrsm.2011.110005.7. El Emam, K., Arbuckle, L., Jonker, E., and Anderson, K. (2012). Two h-in-dex benchmarks for evaluating the publication performance of medicalinformatics researchers. J. Med. Internet Res. 14, e144. https://doi.org/10.2196/jmir.2177.8. Haak, L.L., Ferriss, W., Wright, K., Pollard, M.E., Barden, K., Probus, M.A.,Tartakovsky, M., and Hackett, C.J.(2012). The electronic ScientificPortfolio Assistant: integrating scientific knowledge databases to supportprogram impact assessment. Sci. Public Pol. 39, 464–475. https://doi.org/10.1093/scipol/scs030.9. Angeli, A., Mencacci, N.E., Duran, R., Aviles-Olmos, I., Kefalopoulou, Z.,Candelario, J., Rusbridge, S., Foley, J., Pradhan, P., Jahanshahi, M.,et al. (2013). Genotype and phenotype in Parkinson’s disease: lessons inheterogeneity from deep brain stimulation. Mov. Disord. 28, 1370–1375.10. McNutt, M. (2014). The measure of research merit. Science 346, 1155.https://doi.org/10.1126/science.aaa3796.11. Hirsch, J.E. (2020). Superconductivity, what the H? The emperor has noclothes. Preprint at arXiv, 2001.09496.12. Nachev, P., Herron, D., McNally, N., Rees, G., and Williams, B. (2019).Redefining the research hospital. NPJ Digit. Med. 2, 1–5. https://doi.org/10.1038/s41746-019-0201-2.13. Contopoulos-Ioannidis, D.G., Ntzani, E., and Ioannidis, J.P.A. (2003).Translation of highly promising basic science research into clinical appli-cations. Am. J. Med. 114, 477–484. https://doi.org/10.1016/s0002-9343(03)00013-5.14. Graves, N., Barnett, A.G., and Clarke, P. (2011). Cutting random fundingdecisions. Nature 469, 299. https://doi.org/10.1038/469299c.15. Liu, M., Choy, V., Clarke, P., Barnett, A., Blakely, T., and Pomeroy, L.(2020). The acceptability of using a lottery to allocate research funding:a survey of applicants. Res. Integr. Peer Rev. 5, 3. https://doi.org/10.1186/s41073-019-0089-z.16. Avin, S. (2019). Mavericks and lotteries. Stud. Hist. Philos. Sci. A 76,13–23. https://doi.org/10.1016/j.shpsa.2018.11.006.llOPEN ACCESS21. Peng, H., Ke, Q., Budak, C., Romero, D.M., and Ahn, Y.Y. (2021). Neuralembeddings of scholarly periodicals reveal complex disciplinary organiza-tions. Sci. Adv. 7, eabb9004. https://doi.org/10.1126/sciadv.abb9004.22. Hassan, S.U., Imran, M., Gillani, U., Aljohani, N.R., Bowman, T.D., andDidegah, F. (2017). Measuring social media activity of scientific literature:an exhaustive comparison of scopus and novel altmetrics big data.Scientometrics 113, 1037–1057. https://doi.org/10.1007/s11192-017-2512-x.23. van Raan, A.F.J. (2017). Patent citations analysis and its value in researchevaluation: a review and a new approach to map technology-relevantresearch. J. Data Inf. Sci. 2, 13–50. https://doi.org/10.1515/jdis-2017-0002.24. Newson, R., Rychetnik, L., King, L., Milat, A., and Bauman, A. (2018). Doescitation matter? Research citation in policy documents as an indicator ofresearch impact – an Australian obesity policy case-study. Health Res.Pol. Syst. 16, 55. https://doi.org/10.1186/s12961-018-0326-9.25. van der Zwaard, S., de Leeuw, A.W., Meerhoff, L.A., Bodine, S.C., andKnobbe, A. (2020). Articles with impact: insights into 10 years of researchwith machine learning. J. Appl. Physiol. 129, 967–979. https://doi.org/10.1152/japplphysiol.00489.2020.26. Yin, Y., Gao, J., Jones, B.F., and Wang, D. (2021). Coevolution of policyand science during the pandemic. Science 371, 128–130. https://doi.org/10.1126/science.abe3084.27. Sinha, A., Shen, Z., Song, Y., Ma, H., Eide, D., Hsu, B.J., and Wang, K.(2015). An overview of Microsoft academic service (MAS) and applica-tions. In Proceedings of the 24th International Conference on WorldWide Web, pp. 243–246. WWW ’15 Companion. Association forComputing Machinery. https://doi.org/10.1145/2740908.2742839.28. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., and Kang, J. (2020).BioBERT: a pre-trained biomedicallanguage representation model forbiomedical text mining. Bioinformatics 36, 1234–1240. https://doi.org/10.1093/bioinformatics/btz682.29. Peixoto, T.P. (2014). Hierarchical block structures and high-resolutionmodel selection in large networks. Phys. Rev. X 4, 011047. https://doi.org/10.1103/PhysRevX.4.011047.30. Weis, J.W., and Jacobson, J.M. (2021). Learning on knowledge graph dy-namics provides an early warning of impactful research. Nat. Biotechnol.39, 1300–1307. https://doi.org/10.1038/s41587-021-00907-6.31. Acuna, D.E., Allesina, S., and Kording, K.P. (2012). Predicting scientificsuccess. Nature 489, 201–202. https://doi.org/10.1038/489201a.32. Fu, L., and Aliferis, C. (2010). Using content-based and bibliometric fea-tures for machine learning models to predict citation counts in the biomed-literature. Scientometrics 85, 257–270. https://doi.org/10.1007/icals11192-010-0160-5.33. Yamashita, Y. (2018). Exploring characteristics of patent-paper citationsand development of new indicators. Scientometrics, IntechOpen, 151.https://doi.org/10.5772/intechopen.77130.34. Li, D., Azoulay, P., and Sampat, B.N. (2017). The applied value of publicinvestments in biomedical research. Science 356, 78–81. https://doi.org/10.1126/science.aal0010.17. Kawamura, T., Watanabe, K., and Egami, N.M. (2018). Mapping scienceIntechOpen,based on research content similarity. Scientometrics,175–194. https://doi.org/10.5772/intechopen.77067.35. Ahmadpoor, M., and Jones, B.F. (2017). The dual Frontier: patented inven-tions and prior scientific advance. Science 357, 583–587. https://doi.org/10.1126/science.aam9527.18. Tshitoyan, V., Dagdelen, J., Weston, L., Dunn, A., Rong, Z., Kononova, O.,Persson, K.A., Ceder, G., and Jain, A. (2019). Unsupervised word embed-dings capture latent knowledge from materials science literature. Nature571, 95–98. https://doi.org/10.1038/s41586-019-1335-8.19. Hutchins, B.I., Davis, M.T., Meseroll, R.A., and Santangelo, G.M. (2019).Predicting translational progress in biomedical research. J. Kimmelman,ed. 17, e3000416. https://doi.org/10.1371/journal.pbio.3000416.36. Lewison, G., and Sullivan, R. (2008). The impact of cancer research: howpublications influence UK cancer clinical guidelines. Br. J. Cancer 98,1944–1950. https://doi.org/10.1038/sj.bjc.6604405.37. Grant, J., Cottrell, R., Cluzeau, F., and Fawcett, G. (2000). Evaluating‘‘payback’’ on biomedical research from papers cited in clinical guidelines:applied bibliometric study. BMJ 320, 1107–1111. https://doi.org/10.1136/bmj.320.7242.1107.20. Ebadi, A., Xi, P., Tremblay, S., Spencer, B., Pall, R., and Wong, A. (2021).Understanding the temporal evolution of COVID-19 research through ma-language processing. Scientometrics 126,chine learning and natural725–739. https://doi.org/10.1007/s11192-020-03744-7.38. Haunschild, R., and Bornmann, L. (2017). How many scientific papers arementioned in policy-related documents? An empirical investigation usingWeb of Science and Altmetric data. Scientometrics 110, 1209–1216.https://doi.org/10.1007/s11192-016-2237-2.Patterns 3, 100483, May 13, 2022 13llOPEN ACCESSArticle39. Martı´n-Martı´n, A., Thelwall, M., Orduna-Malea, E., and Delgado Lo´ pez-Co´ zar, E. (2021). Google scholar, Microsoft academic, scopus, dimen-sions, web of science, and OpenCitations’ COCI: a multidisciplinarycomparison of coverage via citations. Scientometrics 126, 871–906.https://doi.org/10.1007/s11192-020-03690-4.40. Jefferson, O.A., Jaffe, A., Ashton, D., Warren, B., Koellhofer, D., Dulleck,U., Ballagh, A., Moe, J., DiCuccio, M., Ward, K., et al. (2018). Mappinginfluence of published research on industry and innovation.the globalNat. Biotechnol. 36, 31–39. https://doi.org/10.1038/nbt.4049.41. Shen, Z., Ma, H., and Wang, K. (2018). A Web-scale system for scientificknowledge exploration. Preprint at arXiv, 180512216.42. Wang, K., Shen, Z., Huang, C., Wu, C.H., Eide, D., Dong, Y., Qian, J.,Kanakia, A., Chen, A., and Rogahn, R. (2019). A review of Microsoft aca-demic services for science of science studies. Front. Big Data 2, 45.https://doi.org/10.3389/fdata.2019.00045.43. Devlin, J., Chang, M.W., Lee, K., and Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding.Preprint at arXiv, 181004805 [cs].44. Elreedy, D., and Atiya, A.F. (2019). A comprehensive analysis of syntheticminority oversampling technique (SMOTE) for handling class imbalance.Inf. Sci. 505, 32–64. https://doi.org/10.1016/j.ins.2019.07.070.45. Freund, Y., and Schapire, R.E. (1997). A decision-theoretic generalizationof on-line learning and an application to boosting. J. Comput. Syst. Sci. 55,119–139. https://doi.org/10.1006/jcss.1997.1504.46. van der Maaten, L., and Hinton, G. (2008). Visualizing Data using t-SNE.J. Mach. Learn. Res. 9, 2579–2605.47. Li, J., Yin, Y., Fortunato, S., and Wang, D. (2019). A dataset of publicationlaureates. Sci. Data 6, 33. https://doi.org/10.1038/records for Nobels41597-019-0033-6.48. Hu, Y. (2006). Efficient, high-quality force-directed graph drawing. Math. J.10, 37–71.49. McKinney, W. (2010). Data structures for statistical computing in Python.Proc. 9th Python Sci. Conf. 445, 51–56.50. Oliphant, T.E. (2007). Python for scientific computing. Comput. Sci. Eng. 9,10–20. https://doi.org/10.1109/MCSE.2007.58.51. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011).Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12,2825–2830.52. Hunter, J.D. (2007). Matplotlib: a 2D graphics environment. Comput. Sci.Eng. 9, 90–95. https://doi.org/10.1109/MCSE.2007.55.53. Waskom, M., Botvinnik, O., Hobson, P., Cole, J.B., Halchenko, Y., Hoyer,S., Miles, A., Augspurger, T., Yarkoni, T., Megies, T., et al. (2014). Seaborn:V0.5.0 (Zenodo). https://doi.org/10.5281/zenodo.12710.54. Peixoto, T.P. (2014). The Graph-Tool Python Library. https://doi.org/10.6084/m9.figshare.1164194.v9.55. Chollet, F.; others (2015). Keras. https://keras.io.56. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation inPyTorch. https://openreview.net/forum?id=BJJsrmfCZ.57. Ulyanov, D. (2016). Multicore-tsne (GitHub Repos GitHub).14 Patterns 3, 100483, May 13, 2022