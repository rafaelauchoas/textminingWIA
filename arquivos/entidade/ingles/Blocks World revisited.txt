Artificial Intelligence 125 (2001) 119–153Blocks World revisitedJohn Slaney a;(cid:3);1, Sylvie Thiébaux b;2a Computer Sciences Lab, Australian National University, Canberra, Australiab CSIRO Mathematical & Information Sciences, PO Box 664, Canberra, AustraliaReceived 15 August 1999; received in revised form 14 June 2000AbstractContemporary AI shows a healthy trend away from artificial problems towards real-worldapplications. Less healthy, however, is the fashionable disparagement of “toy” domains: whenproperly approached, these domains can at the very least support meaningful systematic experiments,and allow features relevant to many kinds of reasoning to be abstracted and studied. A major reasonwhy they have fallen into disrepute is that superficial understanding of them has resulted in poorexperimental methodology and consequent failure to extract useful information. This paper presents asustained investigation of one such toy: the (in)famous Blocks World planning problem, and providesthe level of understanding required for its effective use as a benchmark. Our results include methodsfor generating random problems for systematic experimentation, the best domain-specific planningalgorithms against which AI planners can be compared, and observations establishing the averageplan quality of near-optimal methods. We also study the distribution of hard/easy instances, andidentify the structure that AI planners must be able to exploit in order to approach Blocks Worldsuccessfully. (cid:211) 2001 Elsevier Science B.V. All rights reserved.Keywords: Blocks World; Planning benchmarks; Random/hard problems; Approximation algorithms1. Introduction1.1. Blocks WorldThe Blocks World (BW) consists of a finite number of blocks stacked into towers on atable large enough to hold them all. The positioning of the towers on the table is irrelevant.* Corresponding author.E-mail addresses: John.Slaney@arp.anu.edu.au (J. Slaney), Sylvie.Thiebaux@cmis.csiro.au (S. Thiébaux).1 Some of this work was done while the author was visiting IMAG (Grenoble, France).2 This work was partly done while the author was at IRISA (Rennes, France).0004-3702/01/$ – see front matter (cid:211)PII: S 0 0 0 4 - 3 7 0 2 ( 0 0 ) 0 0 0 7 9 - 52001 Elsevier Science B.V. All rights reserved.120J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153Fig. 1. BW planning problem and optimal plan.The BW planning problem is to turn an initial state of the blocks into a goal state, by movingone block at a time from the top of a tower onto another tower or to the table (see Fig. 1).The optimal BW planning problem is to do so in a minimal number of moves.BW planning turns out to be tractable and optimal BW planning NP-hard [6,13,14].Further, optimal BW planning is Max-SNP hard, meaning that there exists some fixedperformance ratio under which it cannot be approximated tractably [26]. Near-optimalplanning within a ratio of 2 is tractable [14], but the question of whether a better ratiois achievable in polynomial time remains open [26]. For a number of extensions of thebasic BW above (table with a limited capacity, blocks with identical names, . . . ) similarcomplexity results can be shown [14], except for approximation within a constant ratio,which is not always tractable [26].1.2. Motivations behind this paperArtificial domains such as the Blocks World, the Traveling Salesman and the Queens,are in themselves of little practical interest. Despite this, they have remained staples of theAI literature over 30 years, because they are hard for general purpose AI systems and, atleast in principle, can support meaningful, systematic and affordable experiments. Morerecently, BW seems to have fallen into disrepute. In part, this is due to the efforts of theAI planning community to address domains of more practical relevance. More importantlyhowever, it is due to superficial understanding of the domain, leading to poor experimentalmethodology and consequent failure to extract useful results from it.Three major gaps in existing knowledge of BW prevent it from being effectively usedas a benchmark. The first is the lack of knowledge about how to construct probleminstances that are suitable for systematic experimentation: for example hard instances,or uniformly distributed random ones. This seriously lowers the value of experiments.Results are commonly obtained on isolated BW instances, often unspecified except for thenumber of blocks involved, and presumably hand-crafted to show off the good points of aparticular system (here we refrain from pointing the finger at any particular example in theliterature). Matters have started to improve with attempts to create some benchmarks forplanning. 3 In particular Kautz and Selman have introduced a series of four BW problemscalled ‘bw_large.a’ to ‘bw_large.d’, which have now attained that status [17,18]. This is3 The AIPS planning competition (http://www.cs.yale.edu/(cid:24)mcdermott.html and http://www.cs.toronto.edu/aips2000) is a symptom of the perceived need for standards.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153121good at least in that everyone’s system is solving the same problem, but as we shall show,these problems are atypical in several respects, not least in that they have abnormallyshort plans, and are easy to solve optimally. The alternative to experimenting on particularinstances is to use random problems, but when this has been done, it has usually beendone poorly. BW optimisation problems of a given size vary so much in difficulty that“five random problems” are hardly a statistically adequate base for strong conclusions.Moreover, no attention has been paid to the distribution of the random instances, althoughit greatly affects the results. Even in the AIPS 2000 competition, perhaps the most carefullyconstructed testbed for planners, the largest BW problems are again atypical and easy tosolve optimally. We believe that problem selection will improve only when the featuresmaking instances hard, easy or average are identified, and when software for generatingrandom instances with meaningful distributions is used.The second important gap lies in the lack of knowledge about the performance in timeand solution quality of BW-specific planning methods. It was not until the 1990s that theworst-case complexity of optimal BW planning was studied by Chenoweth [6], Gupta andNau [13,14] and Selman [26], and much remains to be done. Very little is known about theaverage time complexity of optimal BW planning. No results are available about averageperformance ratios for near-optimal planning. Also, the exact time complexity of near-optimal BW planning has not been carefully analysed, [14] and [1,6] reporting respectivelycubic-time and quadratic-time upper bounds in the number of blocks. As a consequence,no proper BW “gold standard” exists, and this makes it hard to assess the effectivenessof approaches to planning on the basis of results reported in the literature. For instance,[1,2,7,18] show how specific methods for near-optimal BW planning can be fruitfullyencoded in a general system, while [17] exhibits domain-independent techniques thatdramatically improve performance for BW planning. These facts should not be interpretedas showing that these systems are really effective for domains like BW unless theymatch the best domain-specific ones, both in time complexity and in solution quality. Aslong as little is known about the behaviour of BW-specific methods, misinterpretation iseasy.Only recently have planning systems been able to deal with more than a few blocks—a fact which has hardly enhanced the reputation of the field within the wider researchcommunity. This was largely due to a third gap: ignorance of BW-specific featuresthat general systems should be able to exploit if they are to do well with the domain.The papers by Bacchus and Kabanza [1,2] clearly show that attempting to bridge thisgap greatly improves not only planner performance but also the ability to interpret theexperimental results. Indeed, without a precise understanding of which information isrelevant for a domain and which is not, the reasons why a given general techniqueperforms well on this domain cannot be adequately analysed, nor is it possible to knowwhether the domain is a suitable testbed for analysing the merits of that technique.Recent improvements notwithstanding, the performance of general planners, especiallyon optimal BW problems, remains far from comparable with domain-specific ones. Thisindicates that a more detailed examination of the structure of BW problems is likelyto reveal features which will further improve planner performance and experimentalmethodology.122J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–1531.3. Contributions of the paperIn this paper we undertake a sustained investigation of BW, with a view to filling allthree gaps. Much of the material presented is compiled from a series of conference papers[31,32] and technical reports [27–30] written over a number of years.In Section 2 we look at BW itself, giving expressions for the number of BW states asa function of the number of blocks and showing how to use this function to generaterandom states with uniform distribution. A supply of such states is essential to experimentson average-case behaviour, yet the problem of generating them is not trivial and has notpreviously been addressed.In Section 3 we consider algorithms both for optimal and for near-optimal BW planning.We detail several methods for finding near-optimal plans in linear time, thus closing thecomplexity question for this problem. We also outline an optimal solver capable of dealingwith arbitrary problems of up to 150 blocks. The resulting programs are offered as areference point for assessing the effectiveness of planning systems on this domain.In Section 4 we first examine the average performance of the algorithms in termsof speed and, more importantly, of solution quality. Having linear-time implementationsmeans that we can find near-optimal BW plans for problems on the order of a million blocksin a matter of seconds, enabling us to consider much larger problems than was previouslypossible. We present evidence that the average performance ratios of these algorithms areeverywhere much better than the worst case of 2, and may even approach 1 in the limit.Since the difference between plans produced by the crudest algorithm and optimal plansis so small, claims that a system produces suboptimal plans of “high quality” must beinterpreted carefully.Using our optimal algorithm, we then turn to discovering which optimisation problemsare hard and which are easy. It emerges that the hard problems are those in a criticallyconstrained range for which the number of towers in the initial and goal states is areasonably good predictor. We conclude Section 4 by reconsidering the structure of BWplanning problems in the light of these experiments, with a view to determining how theyshould be approached by domain-independent planning systems.Finally, we discuss the future of BW as a testbed for comparing planners and as an objectof study of relevance to general-purpose planning. Naturally, it would not do to study onlyartificial domains such as the blocks, but we present this paper as evidence that there is stillmuch to learn from them.2. The states of Blocks World2.1. DefinitionsWe shall first enter some definitions. We assume a finite set B of blocks, with TABLE asa special member of B which is not on anything. Our favourite presentation of BW takesas primitive not the binary relation ON but the unary “support” function S which picksout, for block x, the block which x is on. Thus S is a partial function from B nfTABLEg toB, injective except possibly at TABLE and such that its transitive closure is irreflexive. WeJ. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153123refer to the pair hB; Si as a part-state, and identify a state of BW with such a part-state inwhich S is a total function. Clearly, in the latter case, a specification of the support functioncompletely determines the disposition of the blocks.For a part-state (cid:27) D hB; Si and for any a and b in B, we define: ON(cid:27) .a; b/ iff S.a/ D b,CLEAR(cid:27) .a/ iff either a D TABLE or :9b .ON(cid:27) .b; a//, ABOVE(cid:27) as the transitive closure ofON(cid:27) , and finally POSITION(cid:27) .a/ as the sequence ha VV POSITION(cid:27) .S.a//i if S.a/ exists andhai otherwise. That is, the position of a block is the sequence of blocks at or below it. Werefer to the position of a clear block as a tower. A tower is grounded iff it ends with thetable. Note that in a state (as opposed to a mere part-state) every tower is grounded.2.2. Number of Blocks World statesOne of our first questions on encountering BW was how many different states it has, asa function of the number of blocks. Answering this question turns out to be more usefulthan it may appear at first glance. For instance, Schoppers uses an estimate of the numberof states to argue about the compactness of universal BW plans [25]. As another example,we shall see later how such a derivation provides important insight into the generationof random BW states. At least to judge by the number of people who have asked usfor our magic formula, there does not seem to be any analytical definition published inthe literature. To our knowledge, only algorithmic descriptions of the computation of thenumber of states as well as exact or approximate figures for small number of blocks havebeen reported, see, e.g., [18,25].Instead of merely counting the number of BW states of a given size, we shall in factconsider a slightly more general problem. We will count the number of states that extend apart-state in which there are k grounded towers and n ungrounded ones. Let this be g.n; k/.As a special case, the number of BW states of size n is f .n/ D g.n; 0/. We will give twodefinitions of g: a recursive one, and an iterative one, each of which has its uses.The recursive definition of g is quite simple and gives insight into the generation ofrandom states. First g.0; k/ D 1 for all k, since if n D 0 then every tower is alreadygrounded and the part-state is already a state. Now consider g.n C 1; k/. To extend thepart-state, the first ungrounded tower must go on something: either on the table or on oneof the n C k other towers. If it goes on the table, that gives a part-state with n ungroundedtowers and k C1 grounded ones, which has then g.n; k C1/ possible extensions. Otherwise,there are n C k ways of placing it on one of the n C k other towers, each of which leaves apart-state with n ungrounded towers and k grounded ones, which has then g.n; k/ possibleextensions. In sum:g.0; k/ D 1;g.n C 1; k/ D g.n; k C 1/ C .n C k/g.n; k/:Table 1 gives the value of g.n; k/ for small n and k. Since the value of g.n; k/ is producedby the values of the g.x; y/ for hx; yi in the triangle bounded by the vertical column abovehn; ki and by the diagonal going up and right from hn; ki, Table 1 is reminiscent of Pascal’striangle, but with multiplication thrown in. It is worth noting that these numbers rapidlybecome big. For instance f .30/ D 197987401295571718915006598239796851.124J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153Table 1The function g and the triangle for g.3; 1/kn0123456789011313735014051112734209154613327213137350140513142113610459276415312291961190815164335833933604637633932892077754240513763313092239435310473762501801547015839435314417294596553129755613298996976751233459655317572114589410911757211404726308611163391958The iterative definition of g is less intuitive, but forms a better basis from which toderive average-case figures for BW states. Let us decide that we are going to extend thepart-state by putting i of the n ungrounded towers on the table and the remaining n (cid:0) i onways of choosing the i towers. For the first of the remainingother towers. There aren (cid:0) i towers, there are n C k (cid:0) 1 ways of choosing the tower it is going to be on. For thesecond, there are n C k (cid:0) 2 ways, and for the last one, i C k ways. So altogether, there are.n C k (cid:0) 1/W=.i C k (cid:0) 1/W ways of placing the n (cid:0) i towers on others. Summing for all iyields:(cid:0)ni(cid:1)g.n; k/ DnXiD0(cid:18)(cid:19)ni.n C k (cid:0) 1/W.i C k (cid:0) 1/W:Naturally, the two definitions of g can be proven equivalent (see [28, pp. 5–6]). Theproof resembles the derivation of the combinatorial interpretation of binomial coefficientsfrom the binomial addition formula. In fact, it is legitimate to ask whether the values ofg correspond to any well-known sequences of numbers, such as binomial coefficients,Stirling numbers, and the like. The answer is that they nearly do. There exists a simpleexpression of g in terms of a Laguerre polynomial: 4g.n; k/ D nWLk(cid:0)1n.(cid:0)1/the advantage being that we can help ourselves to the known mathematics of thesepolynomials. In [29, pp. 23–26] we do so to show that the average number of towers ina BW state of size n tends topn.4 For an overview of these polynomials, see [21, pp. 76–97]. The Laguerre polynomial LkPniD0.nCk/WnW.iCk/W .(cid:0)x/i .ni(cid:0)(cid:1)n.x/ is defined asJ. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153125The iterative version of g is also well suited to derive a simple expression for the numberof states with exactly t towers, or more generally for the number of states with exactly ttowers extending a part state with k grounded towers and n ungrounded ones. Let us callthis h.n; k; t/. If k towers are already grounded, we must put exactly i D t (cid:0) k towers onthe table, and the rest onto other towers. So it suffices to omit the sum from the iterativedefinition of g, and to replace i with t (cid:0) k:h.n; k; t/ D(cid:19)(cid:18)nt (cid:0) k.n C k (cid:0) 1/W.t (cid:0) 1/Wif t > k and 0 otherwise:We are indebted to an anonymous reviewer of this paper for yet another definition of thenumber of states f , which does not require a generalisation to the binary function g, andwhich lends itself to more efficient algorithms for generating states. Let c.n/ be the numberof states of n blocks in which a given block (say, the nth) is clear. Obviously c.1/ D 1. Nowfor the recursive definition, suppose we know f .n/ and c.n/ and consider adding one moreblock. The states of n C 1 blocks comprise those in which the extra block is under one ofthe blocks in a state of n blocks (n (cid:1) f .n/ of those), those in which the extra block is clearand on the table (f .n/ of those) and those in which the extra block is placed on a clearblock in a state of n blocks. In the last case, note that there are n blocks on which the extraone could be, and each is clear in c.n/ states. The extra block is clear in all states exceptfor the first n (cid:1) f .n/. In sum:f .n C 1/ D f .n/ C n (cid:1) c.n/ C n (cid:1) f .n/;c.n C 1/ D f .n/ C n (cid:1) c.n/:Again it is not difficult to generalise the definition from that of all states to that of stateswith exactly t towers, providing an alternative to h, but we refrain from going into detailshere because that generalisation will not be used in what follows.2.3. Random Blocks World statesArmed with the g function, or with f and c, we can now generate uniformly distributedrandom BW states and problems (pairs of random states), and with h we can generaterandom states with a given number of towers. The latter is particularly useful, because aswe shall see in Section 4, the difficulty of optimal BW planning varies with the number oftowers.Surprisingly few papers mention experiments on random BW problems, and in thosewhich do, the underlying distribution is left obscure (see, e.g., [22, p. 180], [1,9,15,23]).Perhaps the reason is that generating uniformly distributed random BW problems is notas trivial as it may appear. Naïvely incorporating a random number generator into analgorithm for producing the states will not work: typically, it makes some states moreprobable than others by a factor exponential in the number of blocks. As a result, the sampleis skewed and experiments on the average case may be biased. We invite the skepticalreader to try by hand the case n D 2 with a view of generating the three possible states with126J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153a probability of 1=3. Most methods, including the random BW states generator included inthe distribution of UCPOP, 5 give probabilities 1=2, 1=4 and 1=4.The UCPOP generator, for instance, uses the following method to produce a BW state ofsize n:(1) start with an empty table and n blocks to be placed,(2) repeat until all n blocks have been placed:(2a) select with probability 1=(cid:30) one of the (cid:30) blocks that have not yet been placed,(2b) select with probability 1=.(cid:28) C 1/ the table or one of the (cid:28) towers beinggrounded at this stage, and place the selected block on it.So with two blocks for example, the probability of both blocks being in a single tower inascending order is 1=4, and that of both blocks being on the table is 1=2. With n blocks,the probability of all blocks being on the table is 1=nW, while that of all blocks being in onetower in ascending order is 1=.2.n(cid:0)1/nW/, that is, exponentially smaller. It can be shownn.that the average number of towers in a state generated this way tends toSo since the number of towers affects the difficulty of problems, average-case experimentson the performance of algorithms will be biased.2n and notppThe skew in the above method comes from the fact that placement on the table shouldnot have the same probability as placement on another block. That is, among the statesextending a given part-state, the proportion in which the selected block is on the table isnot the same as the proportion in which it is on a given block. Therefore, the key to gettingthe probabilities right is to use the g function for counting the possible extensions of thepart-state. We build a BW state of size n as follows:(1) start with an empty table and n ungrounded towers each consisting of a single block,(2) repeat until all towers are grounded:(2a) arbitrarily select one of the (cid:30) yet ungrounded towers,(2b) select the table with probability g.(cid:30) (cid:0) 1; (cid:28) C 1/=g.(cid:30); (cid:28) / or one of the othertowers (grounded or not) each with probability g.(cid:30) (cid:0) 1; (cid:28) /=g.(cid:30); (cid:28) /, and placethe selected ungrounded tower onto it.In fact, it is hard to work with g directly, as the numbers rapidly become too large.It is better to work with the ratio R.(cid:30); (cid:28) / D g.(cid:30) (cid:0) 1; (cid:28) C 1/=g.(cid:30) (cid:0) 1; (cid:28) / between theprobabilities of the two types of placements. This is always a fairly small real number(cid:30)U. 6 Elementary calculation shows that R.1; (cid:28) / D 1 for alllying roughly in the range T1;(cid:28) and that:pR.(cid:30) C 1; (cid:28) / D R.(cid:30); (cid:28) /.(cid:30) C (cid:28) C R.(cid:30); (cid:28) C 1//(cid:30) (cid:0) 1 C (cid:28) C R.(cid:30); (cid:28) /so no value of g need ever be computed.This method is implemented in the BWSTATES program 7 [27]. BWSTATES firstcalculates in quadratic time all the values of R relevant to the size of the problems beinggenerated and stores them using quadratic space. Once this is done, generating a randomstate is a linear-time problem. However, the space requirements impose a limit of the order5 http://www.cs.washington.edu/research/projects/ai/www/ucpop.html.6 In the limit, as (cid:30) becomes much larger than (cid:28) it converges top(cid:30). On the other hand, where (cid:28) is much largerthan (cid:30) the limiting value is 1 [29, pp. 23–25].7 BWSTATES is available from http://arp.anu.edu.au/(cid:24)jks/bwstates.html.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153127Begin with n ungrounded blocks (i.e. (cid:30) D n) and an empty tableRepeat until all towers are groundedArbitrarily select an ungrounded towerWith probability c.(cid:30)/=f .(cid:30)/ do:/* the tower constitutes the top of a grounded tower */Repeat until the given tower is grounded:Select the table with probability f .(cid:30)/=.f .(cid:30)/ C (cid:30)c.(cid:30)//or one of the other ungrounded towers each with equalprobability, and put the given tower on it.Decrement (cid:30)else (that is, with probability 1 (cid:0) c.(cid:30)/=f .(cid:30)/) do:/* the tower goes below another tower */Select one of the other ungrounded towers, each withequal probability, and put it on the given tower.Decrement (cid:30)Fig. 2. Algorithm to generate random states using the c and f functions.of 10000 blocks. The alternative, i.e., generating states without precomputing and storingR values, would be even more limiting since it would require a time cubic in the numberof blocks.The same principles may be applied to generate uniformly distributed states with exactlyt towers, except that the h function should be used in place of g. So step (2b) reads: selectthe table with probability h.(cid:30) (cid:0) 1; (cid:28) C 1; t/= h.(cid:30); (cid:28); t/ or one of the other towers eachwith probability h.(cid:30) (cid:0) 1; (cid:28); t/= h.(cid:30); (cid:28); t/, and place the selected ungrounded tower ontoit. Here calculations are made much easier, because the probability of a placement on thetable simplifies to .t (cid:0) (cid:28) /=(cid:30), and that of the other placements to .(cid:30) C (cid:28) (cid:0) t/=.(cid:30).(cid:30) C (cid:28) (cid:0) 1//.So these may be computed directly, without going through the ratio R, and the generationof a state takes linear time in the number of blocks.Although the g function makes it easy to explain the skew in other random generatorssuch as that in UCPOP and although it is used in the 1995 release of BWSTATES, it is not themost efficient basis for producing random states with uniform distribution. For certain ofthe experiments reported below, we required a supply of uniformly distributed states withup to a million blocks, which are beyond what can be done using g. Fortunately, the c andf functions lead naturally to a linear-time algorithm as shown in Fig. 2.As in the case of the algorithm based on the g function, it pays to pre-compute and storethe ratios required, in this case P .(cid:30)/ D c.(cid:30)/=f .(cid:30)/, for all (cid:30) < n. In fact, P .(cid:30)/ is close top(cid:30), so again these are convenient-sized real numbers and it is much easier to work with1=them than with f and c directly. The recursive calculation of P is very easy: P .1/ D 1 andP .(cid:30) C 1/ D (cid:30)P .(cid:30)/ C 1(cid:30).P .(cid:30)/ C 1/ C 1:What makes this algorithm more efficient than that based on g is just that computing andstoring P takes linear resources instead of the quadratic ones required for R.128J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153We now leave the topic of random BW states. We shall return to it in Section 4 whenmaking empirical observations on the average-case behaviour of planning algorithms, butfor now we shall focus on BW planning itself.3. Blocks World planning3.1. DefinitionsWe first define the classical notions of problems, 8 moves, and plans. A BW planningproblem over B is a pair of states hhB; S1i; hB; S2ii. In problem hI; Gi, I is the initial stateand G is the goal state. Here we consider only problems with completely specified goalstates. A move in state (cid:27) D hB; Si is a pair of blocks m D ha; bi with a 2 BnfTABLEg andb 2 B nfag, such that CLEAR(cid:27) .a/, CLEAR(cid:27) .b/ and :ON(cid:27) .a; b/. The result of m in (cid:27) isthe state RES.m; (cid:27) / D hB; S0i where S0.a/ D b and S0.x/ D S.x/ for x 2 B nfTABLE; ag.A plan for BW problem hI; Gi is a finite sequence hm1; : : : ; mpi of moves such that eitherI D G and p D 0 or else m1 is a move in I and hm2; : : : ; mpi is a plan for h RES.m1; I /; G iin virtue of this definition. A plan for a given problem is optimal iff there is no shorter planfor that problem.In order to discuss particular BW planning algorithms in the remainder of this paper weneed a few further notions. We say that a block whose position in I (as defined earlier) isdifferent from its position in G is misplaced in hI; Gi, and that one which is not misplacedis in position. Only misplaced blocks have to be moved in order to solve a problem. Next,we say that a move ha; bi is constructive in hI; Gi iff a is in position in hRES.ha; bi; I /; Gi.That is, iff a is put into position by being moved to b. Once a block has been movedconstructively, it need not move again in the course of solving the problem.If no constructive move is possible in a given problem we say that the problem isdeadlocked. In that case, for any misplaced block b1 there is some block b2 which must bemoved before a constructive move with b1 is possible. Since the number of blocks is finitethe sequence hb1; b2 : : :i must eventually loop. The concept of a deadlock, adapted fromthat given in [14], makes this idea precise. A deadlock for BW problem hI; Gi over B is anonempty subset of B that can be ordered hd1; : : : ; dki in such a way that:(cid:26)for all i, 1 6 i < k, NhI;Gi.di; diC1/NhI;Gi.dk; d1/,and alsowhereNhI;Gi.a; b/ (cid:17) POSITIONI .a/ 6D POSITIONG.a/ ^ POSITIONI .b/ 6D POSITIONG.b/ ^9x 6D TABLE .ABOVEI .b; x/ ^ ABOVEG.a; x//:Note that it is possible for a single block to constitute a deadlock. For instance, the problemin Fig. 1 is deadlocked, the deadlocks being fag and fa; dg. To see this, note that NhI;Gi.a; d/taking the third block in the definition to be x D e, that NhI;Gi.d; a/ taking x D c, and thatNhI;Gi.a; a/ taking x D b.8 We follow [14] in using the expression problem for what some authors call a problem instance. This shouldnot cause any confusion.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153129It is easy to see that if NhI;Gi.a; b/ then in any plan for hI; Gi, the first time b is movedmust precede the last time a is moved. 9 A deadlock being a loop of the NhI;Gi relation, atleast one block in each deadlock must be moved twice. The first move of this block shouldbe to the table, so as to break deadlocks without risking introducing new ones. This alreadysets constraints on the sort of plans we need to consider. They will consist of exactly oneconstructive move and at most one non-constructive move to the table per misplaced block.There must be enough of the latter type to break all the deadlocks. In other terms, the setof blocks moved this way must be a hitting set for the deadlocks (i.e., intersect all thedeadlocks). 10 In order to produce an optimal plan, this hitting set must be of minimal size,and as it turns out, finding a minimal-size hitting set is notoriously NP-hard [10, p. 222].This is the sub-problem which makes optimal BW planning difficult [14].3.2. Strategies for near-optimal BW planningOn the other hand, merely finding some plan or other is easy. Unless we start movingblocks that are already in position or choose deliberately to introduce new deadlocks,we can even easily produce plans that are at most twice as long as the optimal. Variousstrategies for near-optimal BW planning within a factor of 2 have been described in theliterature, for which we shall take the papers by Gupta and Nau as sources [13,14].However, we find that these methods need to be clarified, and that their time complexityhas not carefully been analysed. We shall therefore reformulate some of them and showthat they can all be implemented to run in time linear in the number of blocks.US. The first and simplest strategy we shall consider is one we have dubbed US (Unstack-Stack) [13, p. 630]. It amounts to putting all misplaced blocks on the table (the ‘unstack’phase) and then building the goal state by constructive moves (the ‘stack’ phase). No blockis moved by US unless it is misplaced, and no block is moved more than twice. Everymisplaced block must be moved at least once even in an optimal plan. Hence the totalnumber of moves in a US plan is at worst twice the optimal.GN1. Another algorithm which is usually better in terms of plan length than US (and neverworse) is given on pp. 229–230 of [14]. We call it GN1 for Gupta and Nau. It amounts to aloop, executed until broken by entering case (1):(1) If all blocks are in position, stop.(2) Else if a constructive move ha; bi exists, move a to b.(3) Else arbitrarily choose a misplaced clear block not yet on the table and move it tothe table.9 The case in which a D b is trivial. If a 6D b, then in the state immediately following its last move, a is clearand at the top of a tower containing x. If b has not yet moved, since it was above x initially, it is still above x andhence below a, so a must be moved again in order to clear b, contradicting the supposition that it had made itslast move.10 For our problem example in Fig. 1, all blocks except g are misplaced and have to be moved constructively. Inaddition, the two deadlocks fa; dg and fag have to be broken, which is achieved by moving a to the table.130J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153Compared to US, GN1 never (non-constructively) moves a block to the table if it can bemoved constructively. This avoids moving twice when once is enough.GN2. Some of the remarks in [13,14] suggest yet another reading, though with no detailsof how it may be achieved. This one uses the concept of the deadlock. We call it GN2 andit is the same as GN1 except that the misplaced clear block to be moved to the table ischosen not completely arbitrarily but in such a way as to break at least one deadlock. Thatis, clause (3) reads:(3) Else arbitrarily choose a clear block which is in a deadlock and move it to the table.Gupta and Nau [14, p. 229, step 7 of their algorithm] say that in a deadlocked problemevery misplaced clear block is in at least one deadlock. If this were true, GN1 and GN2would be identical. It is false, however, as may be seen from the example in Fig. 1. Blockf is not in any deadlock, and so can be chosen for a move to the table by GN1 but notby GN2. It is possible for GN1 to produce a shorter plan than GN2 (indeed to produce anoptimal plan) in any given case, though on average GN2 performs better because it nevercompletely wastes a non-constructive move by failing to break a deadlock.In order for GN2 to be complete, in every deadlocked problem there must be at least oneclear block which is in a deadlock. In fact, we can prove the stronger result that in everydeadlocked problem there exists a deadlock consisting entirely of clear blocks. We nowsketch the proof, since it makes use of the notion of a (cid:1) sequence which will be needed inimplementing GN2.Let (cid:27) D hB; Si be a state that occurs during the attempt to solve a problem with goalstate G D hB; SGi and suppose the problem (cid:25) D h(cid:27); Gi is deadlocked. Let b be misplacedand clear in (cid:27) . Consider POSITIONG(b), the sequence of blocks which in G will lead fromb down to the table. Let c be the first (highest) block in this sequence which is already inposition in (cid:27) (c may be the table, or SG.b/, or somewhere in between). In the goal state,either b or some block below b will be on c. Let us call this block d. What we need todo, in order to advance towards a constructive move with b, is to put d on c. This is notimmediately possible because there is no constructive move in (cid:27) , so either c is not clear orelse d is not clear. If c is not clear, we must move the blocks above it, starting with the oneat the top of the tower which contains c in (cid:27) . If c is clear, we should next move the clearblock above d in (cid:27) . This is how the function (cid:14)(cid:25) is defined: (cid:14)(cid:25) .b/ D x such that(cid:26)CLEAR(cid:27) .x/andABOVE(cid:27) .x; d/ABOVE(cid:27) .x; c/if CLEAR(cid:27) .c/,if :CLEAR(cid:27) .c/.If b is in position or not clear or if SG.b/ is in position and clear, let (cid:14)(cid:25) .b/ be undefined.Now let (cid:1)(cid:25) .b/ be the sequence of blocks obtained from b by chasing the function (cid:14)(cid:25) :(cid:26)(cid:1)(cid:25) .b/ Dhb VV (cid:1)(cid:25) .(cid:14)(cid:25) .b//ihbiif (cid:14)(cid:25) .b/ exists,otherwise.The point of the construction is that if (cid:14)(cid:25) .b/ exists, then CLEAR(cid:27) .(cid:14)(cid:25) .b// and N(cid:25) .b; (cid:14)(cid:25) .b//.To see why the latter holds, note that either c or d is below (cid:14)(cid:25) .b/ in (cid:27) and below b in thegoal. Now for any misplaced clear b, if (cid:1)(cid:25) .b/ is finite then there is a constructive movein (cid:27) using the last block in (cid:1)(cid:25) .b/, while if it is infinite then it loops and the loop is aJ. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153131deadlock consisting of clear blocks. This loop need not contain b of course: again Fig. 1shows an example, where (cid:1)(cid:25) .d/ is hd; a; a; : : :i.Our suggestion for a way of implementing GN2, therefore, is to replace the originalclause (3) with:(3) Else arbitrarily choose a misplaced clear block not on the table; compute its (cid:1)sequence until this loops; detect the loop when for some x in (cid:1)(cid:25) , (cid:14)(cid:25) .x/ occursearlier in (cid:1)(cid:25) ; move x to the table.It is worth noting that our GN2 is not quite the same as the pure GN2 given above, becausewe always break a (cid:1) sequence deadlock rather than an arbitrary deadlock. So there may bedeadlocked blocks (e.g., d in Fig. 1) which cannot be chosen for a move to the table becausethey are not part of a loop in any (cid:1) sequence. This appears to improve the average-casebehaviour, but we have not investigated the matter in detail.We now show how to implement all of US, GN1 and GN2 to run in time linear in thenumber n of blocks. This improves on the known complexity of these algorithms. Theoriginal [14] did not mention any bound better than O.n3/ for near-optimal BW planningthough O.n2/ implementations have been described by other authors [1,6].3.3. Linear-time algorithm for USThe key to making US a linear-time algorithm is to find a way to compute whichblocks are in position in O.n/, and to execute this computation only once in the courseof the problem solution. We do this by means of a combination of recursion and iteration,Fig. 3. The US algorithm.132J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153as shown in Fig. 3. The algorithm makes use of several variables associated with eachblock b:ClearbInPositionbExaminedbSibSgbTrue iff b is clear in the current state.True iff b is already in position.True iff InPositionb has been determined.Block currently below b.Block below b in the goal.INPOS can be called at most twice with any particular blockDuring initialization,b as parameter: once as part of the iteration through the blocks and at most oncerecursively from a call with the block above b. Hence the number of calls to INPOSis bounded above by 2n. Similar considerations apply to the recursive STACK andUNSTACK procedures. The stored information is updated in constant time by MOVE.3.4. Linear-time algorithm for GN1For GN1, we need to recognise constructive moves from tower to tower as well as thosefrom the table. To achieve this, we add further structure to the problem representation. Atany given time, each block has a status. It may be:(1) ready to move constructively. That is, it is misplaced but clear and its target is inposition and clear.(2) stuck on a tower. That is, it is misplaced, clear and not on the table, but unable tomove constructively because its target is either misplaced or not clear.(3) neither of the above.More variables are now associated with each block b: Statusb records the status (READY,STUCK, or OTHER) of this block, while Pib and Pgb denote the blocks (if any) which areon this one currently and in the goal. Evidently, initialising, setting and updating these willnot upset the O.n/ running time. To make it possible to select moves in constant time, theblocks of status READY and STUCK are organised into doubly linked lists, ReadyList andStuckList, one such list containing the blocks of each status. Inserting a block in a list anddeleting it from a list are constant-time operations as is familiar. The next block to moveis that at the head of ReadyList unless that list is empty, in which case it is the block at thehead of StuckList. If both lists are empty, the goal is reached.When a block a moves, it changes its status as well as its position. Certain other blocksmay also change status as a result of the move. When they exist, these are: the blockcurrently below a (Sia ) if any, the block that will be on a in the goal (Pga) if any, andthe block which in the goal will be on the block currently below a (PgSia ) if any. Those,however, are all of the possible changes: a constant number (4) for each move. Thereforethe number of delete-insert list operations on blocks is at worst linear in the number ofmoves in the plan, Nothing else stands in the way of the linear-time implementation ofGN1 shown in Fig. 4. 1111 The function INPOS is the same as in the US implementation and is therefore not repeated in Fig. 4.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153133Fig. 4. The GN1 algorithm.3.5. Linear-time algorithm for GN2GN2, however, is a different matter. To implement GN2 via (cid:1) sequences, it is necessaryto compute (cid:14)(cid:25) .b/ for various blocks b, and to achieve linear time there must be both a wayto do this in constant time and a way to limit the number of (cid:14) calculations to a constantnumber per block. On the face of it, neither of these is easy. To find (cid:14)(cid:25) .b/ it is necessary toknow which is the highest block in position in the goal tower of b and to know which is theclear block above a given one. These items of information change as moves are made, andeach time such an item changes for one block it changes for all the O.n/ blocks in a tower,so how can those changes be propagated in constant time? Moreover, when a deadlock is134J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153Fig. 5. Asking the concierge ‘Who lives at the top of the tower?’to be broken, a new (cid:1) sequence has to be computed, as many blocks may have movedsince the last one was computed, thus changing (cid:14). Computing a (cid:1) sequence appears to beirreducibly an O.n/ problem, and since O.n/ such sequences may be needed, this appearsto require GN2 to be of O.n2/ even if (cid:14)(cid:25) .b/ can somehow be found in constant time.The first trick that begins to address these difficulties is to note that whatever changes ina tower of blocks, one thing does not change: the block on the table at the bottom of thetower. If that block moves, the tower no longer exists. We call the bottom block in a towerthe concierge for that tower. Now if we want to know who lives at the top of the tower,we can ask the concierge. When a block comes or goes at the top of the tower, only theconcierge need be informed (in constant time) and then since every block knows which isits concierge, there is a constant-time route from any given block to the information as towhich block above it is clear (see Fig. 5). Not only the towers in the initial (or current)state have concierges, but so do the towers in the goal state. These keep track of whichblock in their tower is the highest already in position. Additional variables associated witheach block b denote its initial and goal concierges, Cib and Cgb. In case b is a concierge,there are more variables denoting the clear block Topb above it, and the highest block Hinbalready in position in its goal tower. Through the concierges, there is a a constant-time routefrom b to the c and d required to define (cid:14)(cid:25) .b/. The procedure for initialising the additionalvariables is closely analogous to that for determining which blocks are in position and canbe executed in linear time for the same reason. Updating them when a move is made takesconstant time.Next, the key to managing the (cid:1) sequences is that although (cid:14) may change the N(cid:25)relation is indestructible except by moving the blocks involved. That is, if Nh(cid:27);Gi.x; y/Fig. 6. How [not] to break a deadlock.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153135Fig. 7. The GN2 algorithm.136J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153then that relationship persists in the sense that in all future states (cid:18) , Nh(cid:18);Gi.x; y/ unless xor y has moved in getting from (cid:27) to (cid:18) . Moreover, as noted above, if N(cid:25) .x; y/ then x cannotmove constructively until y has moved at least once. Now, let (cid:12) D hb1; : : : ; bki be a non-looping sequence of clear blocks which are stuck on towers, each except the last linked toits successor by the relation N(cid:25) . At some point, bk may cease to be stuck and become readyto move, but no other block in (cid:12) can change its status until bk actually moves. Thus the (cid:12)sequence may dwindle, and even become null, as moves are made, but it always remains asingle sequence—it never falls into two pieces as would happen if a block from the middleof it changed status—and because N(cid:25) is indestructible (cid:12) remains linked by N(cid:25) . For thealgorithm, then, we maintain such a sequence (cid:12), constructed from parts of (cid:1) sequences asfollows. Initially, (cid:12) is null. If the problem becomes deadlocked, first if (cid:12) is null then it isset to consist just of the block at the head of the StuckList, and then it is extended by adding(cid:14)(cid:25) .bk/ to the end of it. This is done repeatedly until the sequence threatens to loop because(cid:14)(cid:25) of the last block bm is already in (cid:12). At that point bm is chosen to break the deadlock.It is important not to choose (cid:14)(cid:25) .bm/ for this purpose, since that could result in breaking(cid:12) into two pieces (see Fig. 6). Each addition to (cid:12) takes only constant time, and any givenblock can be added to the sequence at most once. Therefore maintenance of the (cid:12) sequencerequires only linear time. This completes the description of the principles underlying thealgorithm, which is shown in Fig. 7. 123.6. Algorithm for optimal Blocks-World planningSome of the experiments in Section 4 below require the ability to generate optimalBW plans. For instance, in order to gather experimental results concerning the averageperformance ratios of the various near-optimal algorithms, we need to know the optimalplan lengths. A good BW-specific optimal planning method is also needed to determinehow hard optimal BW planning really is, and get an idea of the size of (average) BWproblems that should be regarded as truly challenging. Moreover, it is a prerequisite tothe study of the distribution of hard and easy instances among problems of a given size, tothe identification of a pattern in this distribution, and to the investigation of the relationshipbetween hard problems and average ones.Unfortunately, the literature seems to lack any algorithm suitable for these purposes.The non-deterministic algorithm given in [14] is useful for complexity analyses but notas a practical method, while most papers reporting experiments on BW use a domain-independent planner for generating optimal plans and drastically limit the size of theproblems considered. We shall therefore describe the method we used.Recall from Section 3.1 above that the key to optimal BW planning is finding a hittingset of minimal cardinality for the set of deadlocks in the problem. At the heart of ouralgorithm, therefore, is a backtracking search for such a hitting set. One complication isthat we do not know at the outset which sets of blocks constitute deadlocks; nor do weknow of an efficient method of enumerating the deadlocks; nor in fact do we ever knowthat we have hit them all until we can finally produce a plan.12 The functions INPOS, STATUS and STAT are the same as in the implementation of GN1, and are not repeatedin Fig. 7. The (cid:12) sequence is implemented as a stack.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153137Our algorithm PERFECT overcomes this problem as follows. It constructs a set K ofknown deadlocks. Initially, only the singleton deadlocks are known; other deadlocks arecomputed as needed. At each iteration it finds a minimal size hitting set H for the deadlockscurrently in K and tests this to discover whether it hits all deadlocks in the problem. If itdoes, it constitutes a solution and the algorithm halts. If not, a deadlock disjoint from H isfound and added to K:/* initialisation */K  ffbg V fbg is a deadlockg/* main loop */Repeat until plan returned:Generate H a minimal size hitting set for KTEST(H )if H solves the problem thenelse find a deadlock D such that D \ H D ;return PLAN(H )K  K [ fDgThis requires five procedures to solve sub-problems:(1) The set of singleton deadlocks must be found to initialise K.(2) A minimal size hitting set H for K must be generated.(3) H must be tested to see whether it hits all deadlocks in the problem.(4) A deadlock D disjoint from H must be found.(5) H must be used to produce a plan.Of these, (2) is the NP-complete sub-problem which will not be detailed further hereexcept to note that it is solved by a backtracking search, requiring exponential time inthe worst case. It may be speeded up somewhat by incorporating various search heuristicswhich, however, are not especially germane to the present paper. The other four sub-problems are all tractable and the corresponding procedures all make use of our linear-timeimplementation of the near-optimal BW planner GN1.It is easy to modify GN1 by giving it a set H of blocks as a parameter and requiring (inclause (3) of the GN1 algorithm) that only blocks in H be used to break deadlocks. Thisresults in a plan if H is a hitting set for the deadlocks in the problem, and in failure to finda plan otherwise. That is, it yields a (linear-time) decision procedure for whether H is ahitting set or not. We call GN1 thus modified GN1H. Evidently, GN1H suffices for both (3)and (5) above.GN1H may also be used to achieve (1). It is clear that GN1H(B n fbg) results in a plan ifand only if fbg is not a (singleton) deadlock. Therefore, by running GN1H(B n fbg) for eachblock b 2 B in turn, we obtain in quadratic time the set of all singleton deadlocks. 1313 This is not the most efficient way of detecting singleton deadlocks—the experiments on singleton deadlocksreported in Appendix A below use a better one—but for small problems such as those for which optimal planningis feasible, it is adequate.138J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153A similar technique suffices for (4). Suppose that H is not a hitting set for all deadlocksin the problem. Then proceed as follows:for each b 2 B n TABLE doTEST(H [ fbg/if H [ fbg is not a hitting set thenH  H [ fbgObviously, at the end of this (quadratic-time) procedure, H is a non-hitting set and ismaximal in the sense that every proper superset of it is a hitting set. Let D be itscomplement with respect to B n fTABLEg. D is a deadlock: it contains a deadlock, sinceotherwise H would be a hitting set, and it contains nothing else, since otherwise H wouldnot be maximal.Our linear-time implementation of GN1 therefore serves both as a decision procedure forpotential hitting sets and as a plan generator once a hitting set has been found. It enablesour optimal procedure to cope with well over 100 blocks in reasonable time, as may beseen in Fig. 9 below.4. Experimental observationsUsing our random BW problem generator and the above algorithms, we are able to makea number of interesting observations on the structure of large BW problems and on theperformance in time and average solution quality of BW-specific planning methods.Unsurprisingly, our near-optimal planners are fast. They all solve problems of a millionblocks in a matter of seconds. More interestingly, we find that the plans they produce arecloser to the optimal than we had expected. On the basis of the experimental results, it maybe conjectured that their average performance ratios tend in the limit to 1, in which case,despite the NP-hardness of optimal planning, it is possible in linear time to find plans inwhich, on average, the proportion of unnecessary moves is vanishingly small.By close examination of random BW problems, we identify the structural features whichplanning systems should use if they are to do well with the domain. Proving the optimalityof plans, of course, is more complicated. Our experiments show a clear easy-hard-easydistribution of optimisation problems, using the numbers of towers in the initial and goalstates as the order parameter. This is related to a tradeoff between the number of deadlocksand their size.In the interest of readability, details of the data used for these experiments have beenrelegated to Appendix A.4.1. Time performancesWe start with the average runtimes of the near-optimal algorithms as a function of thenumber n of blocks. As will be clear from the experiments in Section 4.2 below, there is nosignificant difference between the average and worst cases runtimes for these algorithms.As shown in Fig. 8, the time for all three programs is linear in the number of blocks. Thereis very little difference between GN1 and US. Since some aspects of the computation areclosely related to the plan length, GN1 sometimes even outperforms US because it producesJ. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153139Fig. 8. Average runtimes (in secs) as a function of n for the near-optimal algorithms.Fig. 9. Median runtime (in secs) as a function of n for the optimal algorithm.shorter plans. Naturally, since nobody really wants to convert one BW state into another,the program speeds are not important in themselves. The point of this experiment was justto confirm empirically the theoretical claims that linear execution time may be attained,and that domain-independent planners are orders of magnitude away from BW-specificmethods. 14In Fig. 9, we also measured the runtime of our optimal algorithm. As will becomeapparent from the experiments in Subsection 4.3.2, the average case is again similar tothe worst case, namely here exponential in n. The main purpose of the experiment was todecide what number of blocks we could expect to use for other experiments if these wereto be completed within the time available without sacrificing the information which can begleaned from relatively large examples. We find that it is reasonable to go up to n D 150, at14 To the best of our knowledge, the current state of the art is represented by TALPLANNER [7,19], a variant ofthe TLPLAN system [2] which solves BW problems near-optimally in quadratic time. 500 blocks problems takeabout a second.140J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153Fig. 10. Problem leading to the worst optimal plan length.Fig. 11. Average plan length as a function of n: (number of moves)=n.which size the average problem is several orders of magnitude harder than those that havebeen considered in the domain-independent setting. 154.2. Plan length and solution qualityAfter the performance in time of the algorithms, we look at their performance in solutionquality. The length of the plan produced by all near-optimal algorithms, as well as theoptimal plan length is 2n (cid:0) 2 in the worst case. As can be seen from Fig. 10, it is possiblefor every block but one on the table to constitute a singleton deadlock and to have to movetwice. The average length of the plan produced by the near-optimal algorithms approachesthis worst case quite closely, as may be observed from the graph in Fig. 11. As expected,US gives the longest plans on average and GN2 the shortest, but for large numbers of blocksit makes little difference which algorithm is used. In particular, it is evident from the graphsthat the algorithms will give very close average plan lengths in the limit.The immediate questions raised by the present results are whether all of the algorithmsconverge to the same average plan length (as a proportion of n) and if so whether15 Neither TLPLAN [2] nor SATPLAN [18] copes with random optimisation problems of more than 20 blocks.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153141this limiting figure is 2n. Positive answers to both questions are strongly suggestedby the data—indeed, they seem obvious—but obviousness is not proof. The theoreticalthe average length of theinvestigation does not appear easy: even to prove thatpplans produced by the ‘baby’ algorithm converges to 2.n (cid:0)n/ we needed nontrivialmathematics involving complex analysis and the theory of Laguerre polynomials [29, pp.23–26]. This algorithm, which is not near-optimal, simply puts all blocks (misplaced ornot) on the table before building the goal position.The absolute performance ratio of the near-optimal algorithms is 2 in the limit. Theworst-case problem is shown in Fig. 12. The deadlocks are the pairs fa; cig for 1 6 i 6 k,so the optimal plan, of length k C 2, consists in breaking all the deadlocks by moving a tothe table and then moving the ci and a constructively. US will first move all of the ci (anda) to the table. In the worst case, GN1 and GN2 may also move all of the ci to the tablebefore moving a constructively, giving a plan of length 2k C 1 and therefore a performanceratio of .2k C 1/=.k C 2/ which tends quickly to 2.Fig. 13 shows the average performance ratios. This graph contains a real surprise: theaverage performance of US does not degrade monotonically but turns around at aboutn D 50 and begins to improve thereafter. The explanation is the plan lengths for US quicklyapproach the ceiling of 2n, at which point the optimal plan lengths are increasing moreFig. 12. Problem leading to performance ratios of 2.Fig. 13. Average performance ratio as a function of n: (plan length)=(optimal plan length).142J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153quickly than the US ones because the latter are nearly as bad as they can get. It is clearfrom this figure and Fig. 11 that the other near-optimal algorithms would exhibit similarcurves if it were possible to observe the length of optimal plans for high enough valuesof n.One result readily available from the graphs is an upper bound around 1.23 for averageperformance ratios. However, Figs. 11 and 13 together suggest that the limiting valuewill be well below this rough upper bound. Our open question following this experimentis whether the optimal plans tend to a length of 2n in the limit. If they do, then notonly the near-optimal algorithms but even the ‘baby’ algorithm have the perfect averageperformance ratio of 1 in the limit. A positive answer would be implied if the number ofsingleton deadlocks tended to n, but on investigating this figure experimentally we foundthat it appears to be only around 0:4n (see experiments in Section 4.3.1).4.3. The structure of Blocks World planning problemsAccording to the above experiments, the gap in time performance between domain-specific and domain-independent methods is huge, in both the optimal and near-optimalcases. Moreover, in the latter, domain-specific methods achieve higher solution quality.Why is this? What structural properties of the domain are the specific solvers using whichthe generic planners are missing?4.3.1. Composition of the average BW problemAs a preliminary step, we examine the composition of the average BW problem. Someof the blocks are misplaced and have to move, and some do not. Then, of the misplacedblocks, some occur in deadlocks, and some do not. Finally, of the deadlocked blocks, someconstitute singleton deadlocks and so must obviously move twice, while others do not.We call the blocks in the last category live blocks. The non-trivial part of a BW planningproblem is to decide which of the live blocks should move twice. This is demonstratedin Fig. 14, which represents the average hardness of optimal BW planning problems ofn D 100 blocks as a function of the number of live blocks in these problems. The positivecorrelation is obvious.Quantitatively, what is the proportion of blocks in each category? The first observationis that almost all of the n blocks are misplaced. Fig. 15 shows the distribution of problemswith a given number of misplaced blocks. This has been obtained with random problemsof n D 1000 blocks, but we find that the distribution does not vary significantly with nand similar results are obtained with as few as 20 blocks. In particular the probabilitythat all blocks are misplaced seems to converge quickly to e(cid:0)1. 16 Instances with morethan about 5 blocks in position are extremely rare, even for problems with 10000 blocks.Except in pathological cases, this already imposes a lower bound of roughly n on the planlength.Clearly any block which starts or finishes on the table cannot be involved in anyn blocks are on the table in the average state, so theren deadlock-free blocks on average. This gives us an upper bounddeadlock. As already noted,should be at least 2pp16 An analytical expression for this probability as a function of n is given in [28].J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153143Fig. 14. Median hardness as a function of the number of live blocks.Fig. 15. Probability of exactly m blocks being misplaced for m D n; : : : ; n (cid:0) 6.paround 2.n (cid:0)n/ on the average plan length. Experimentally (see Fig. 16) we find thatthe blocks on the table are almost the only deadlock-free ones: on average less than 2others are deadlock-free. It follows that the upper bound cannot be significantly improvedby reasoning about which blocks are in deadlocks, and that there is no need for a plannerto spend a lot of time in such reasoning.On the other hand, reasoning about special kinds of deadlocks may be valuable (forinstance GN2 gains by concentrating on (cid:1) sequence deadlocks). Most importantly, weobserve in Fig. 17 that on average nearly 40% of the blocks are singleton deadlocks. Thislimiting figure is approached quite closely for problems around n D 100 blocks. In sum,the optimal plan length is almost always between 1:4n and 2.n (cid:0)n/, and so on averageless than 30% of the moves are non-trivial.p144J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153Fig. 16. Average number of deadlock-free blocks not on the table as a function of n.Fig. 17. Proportion of blocks constituting singleton deadlocks as a function of n.4.3.2. Hard and easy BW problemsThe next question is under what conditions is it difficult to plan these non-trivial movesin an optimal way—that is, when is it hard to find a minimal size hitting set for thedeadlocks? Well, it is easy if there are few deadlocks, and it is easy if all the deadlocksare small. Unfortunately, these two conditions tend to work against each other. For fewdeadlocks to exist, the N relation should be sparse, which typically occurs where there aremany short towers. Small deadlocks occur where the N relation loops very easily, whichhappens when most of the blocks are found in a very few tall towers. The hard problems aretherefore squeezed between the “under-constrained” area in which there are many towersand the “over-constrained” area in which there are few.This analysis is confirmed by Fig. 18, which represents the median hardness (as thenumber of times PERFECT backtracks) of random problems of 100 blocks against theparameters of the number of towers initially and in the goal. It can be seen that optimalJ. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153145Fig. 18. Median hardness by number of towers in the initial and goal states.planning is easy for a wide range of these parameters: for problems of this size the hardregion is confined between 8 and 18 towers. However, it is important to note that mostrandom problems fall in the hard region: random problems of this size cluster strongly100 D 10 towers. So on average, BW problems are reasonably hard. Randomaroundproblems are however not the hardest. For instance, problems of this size produced by theUCPOP generator cluster around200 D 14 towers and are harder.pp4.3.3. What are the relevant features?In the light of the above, we may begin to answer the questions at the head of this section.At least we may observe:(1) As noted for example in [1], it is crucial that planners classify blocks as in positionor misplaced rather than attempting to carry out long chains of reasoning purely interms of ON and CLEAR: Being able to recognise constructive moves and make themwhenever the opportunity exists already suffices for more than half of the averageplan.(2) More than half of the remainder are moves which break singleton deadlocks. Beingable to recognise and make these moves is therefore sufficient to allow a planner tocome close to completing plans without any search or backtracking.(3) The fact that all non-constructive moves may be to the table is easily coded (as noted,e.g., by Kautz and Selman in [18]) and greatly reduces the set of possible plans andtherefore the search. In Kautz and Selman’s terminology, it is a safe simplifyingassumption based on a completeness theorem which could in principle be foundautomatically.146J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153Once the constructive moves have been identified, and the blocks in singleton deadlocksremoved from consideration, there remains only the problem of finding a hitting set forthe remaining deadlocks. For near-optimal BW planning, no more reasoning is required:any choice whatever, even the worst, yields a plan very close to the optimal on average.For optimal BW planning, it is necessary to generate a minimal size hitting set, but thisproblem is not specific to the field of planning and is best approached using techniquesavailable off the shelf. In either case, therefore, the above features constitute the structurewhich we expect domain-independent planners to exploit if they are to deal adequatelywith the domain.4.3.4. Existing benchmark instancesAnother question that can be addressed following the above observations is that of therepresentativity of existing benchmark instances, such as the ‘bw_large.a’ to ‘bw_large.d’commonly used in discussing the effectiveness of SAT-based planners (see, e.g., [8,11,12,16–18]).As can be seen from Fig. 19, the first obvious feature of these four problems is that thelarger ones are built up from the smallest by progressively adding blocks. In bw_large.athere are no deadlocks at all, so even GN1 solves it optimally. In the others, there aretwo deadlocks (the same two in every case) and it is obvious that putting block 11 onthe table is sufficient to break them both, after which again constructive moves suffice.Hence the reasoning in all three cases is essentially the same and is hardly challenging.Fig. 19. Problems bw_large.a . . . bw_large.d from [17].J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153147Blocks inNumber ofSingletonOptimalTime to solveProblempositiontowersdeadlocksplanlengthoptimally (sec)100.0100.1200.0200.1300.0300.1400.0400.1500.0500.111121212113 C 52 C 510 C 99 C 65 C 25 C 64 C 48 C 55 C 64 C 579791291352782643683254624611821823483465775677737409639690.040.052.920.590.210.470.945.210.831.84Fig. 20. Problems probblocks-100 . . . probblocks-500 from AIPS-2000.A less obvious but important feature is that they are quite unlike the average problem oftheir size in that they have many non-deadlocked blocks and no singleton deadlocks at all.This gives them all unusually short plans: the average plan length for 19-block problems is24.5, yet bw_large.d requires only 18 moves. Comparisons based on these problems tendto favour planners which prefer short plans, SAT-based planners for instance. This featureis therefore highly relevant to their use as a benchmark.Another benchmark set calling for comment is that from the planning competition ofAIPS-2000, and in particular the “additional” problems from the “hand tailored systemstrack” (see Fig. 20). These have from 100 to 500 blocks. For their size, they all havevery small numbers of towers in both initial and goal states, leading to abnormally largeproportions of self-deadlocked blocks (over 90% in some cases). As a consequence, theiroptimal plans are long but extremely easy to find. PERFECT, which finds random problemsof 160 blocks challenging, solves them almost instantly, as shown in the last column ofFig. 20. Problem 300.0 is even solved without backtracking. In the AIPS competition,optimality was not required, so the extreme skew in the sample does not invalidate theresults. However, it should be noted that because they are so easy to solve optimally andbecause their optimal plans are so long, these problems reveal little or nothing about thequality of solutions produced by competing suboptimal planners.5. Conclusion: What future for Blocks World?BW has traditionally been the “Hello World” of planning: often, a trivial instance ofit is used to illustrate what an action is, how some notation represents postconditions,how execution failures can be handled or whatever. To this practice we have no objection,save that the example becomes boring eventually. However, as we have been at pains topoint out, the use of it as an benchmark has been less satisfactory. We believe that its148J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153future can be a great improvement on its past. Following the investigations reported in thispaper, we conclude with our expectations as to what this future might be and a few lastrecommendations about how it should be achieved.5.1. Comparing plannersIt seems clear that planning systems worth comparing on the domain will soon allachieve quadratic-time performance—some have already done so [1,7]—or even lineartime, after which all that can be compared on the basis of speed is the efficiency of thecoding. So we do not think that comparing time performances in solving BW problemsnear-optimally will be found particularly useful. This is probably why, according to reportsof the AIPS 2000 competition, “one comment that was heard at the conference . . . was thatnow the Blocks World is no longer an interesting problem even as a benchmark.” 17We expect that much more worthwhile comparisons will be made on the basis of planlength, and for anytime planners, on the basis of their rate of improvement. It will beimportant to bear in mind that the difference between poor plans such as those producedby US and reasonably good ones such as those of GN2 is not a large proportion of the moves,so apparently small improvements in solution quality may represent significant advances inplanning technology. Hence, while we agree that (non-optimal) BW planning is becominga less interesting benchmark as regards solution time, we strongly disagree with the viewthat it is no longer a worthwhile benchmark at all.We believe that another important facet of planning for which BW will prove a suitabletestbed is the ease with which formalisms and systems allow domain-specific controlknowledge to be specified or automatically learned. For example, the effect of TLPLAN’sgoal control rule 3 [2, p. 11] is to disallow any plan that is not at least as good as a USplan (i.e., the moves produced are a subset of US moves). This together with the “trigger”rule noted in [1, p. 165] yields an implementation of GN1. The obvious next step is to seehow easily more restrictive conditions could be encoded, for instance to disallow non-GN2moves and recognise singleton deadlocks.5.2. Using the right problem instancesIn making comparisons, care will have to be taken over the choice of problem instancesused. For experiments in which optimal plans are sought, it is best to use instances likely tobe hard on average, from the region around the peak in Fig. 18 for example. However, it isimportant to be careful as to what is being tested. The difficulty of the hardest instances isdominated by that of finding minimal cardinality hitting sets, and a planner’s ability to dothis may well not be seen as the most important thing to assess. Hence, for example, if 100-block problems are used to compare optimal planners, it may be valuable to concentrateon moderately hard instances whose initial and goal states have 10 towers, or 20, ratherthan on the hardest ones which have 14. This will give more weight to other aspects ofreasoning.17 http://www.ida.liu.sc/ext/witas/achiev/aipscom/page.html.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153149For experiments in which optimality is not an issue, it is clearly good to use uniformlydistributed random instances in order to obtain meaningful results on average behaviour.In any case, there is no point in seeking especially hard instances for that purpose, sincethere is no significant difference in difficulty or plan length between the average problemsand the hardest.5.3. Beyond experimentsBW’s place as an experimentation benchmark need not exhaust its future contribution.Like the Traveling Salesman [20], it may constitute a basis for investigating other problemsof more practical interest. The abstract problem of which BW is a instance is the following:sets of actions producing the goal conditions (constructive actions) cannot be consistentlyordered so as to meet all of their preconditions (that is, deadlocks occur). To resolve eachdeadlock, a number of additional actions have to be introduced (non-constructive actions).Since deadlocks are not independent, there is a need to reason about how to resolve themall using as few additional actions as possible. This core problem is present in other morerealistic situations. For instance, moving blocks is not very different from moving moreexciting objects such as packages, trucks and planes. Again, the version of BW in whichthe table has a limited capacity captures the essence of the container loading problem,a problem that is crucial to efficiency of freight operations [24,33]. 18 Therefore, findingthe right generalisations of strategies that are effective for BW appears promising as anapproach to more sophisticated problems.There is more. Existing classes of tractable planning problems do not seem to captureC-US [4] or the restriction of STRIPS to groundthe properties of domains like BW. In SASliterals and operators with positive preconditions and one postcondition [5], planning istractable while optimal planning and even near-optimal planning are not [26]. 19 Yet, near-optimal planning is tractable for certain domains like BW which are too sophisticated tobe encoded within such classes. 20 The fact that the identification of tractable subclasses ofC planning originated in the careful examination of a toy problem in sequential controlSAS[3, p. 29] suggests that BW is a good candidate for identifying in a similar way a new classof planning problems for which near-optimal planning is tractable.5.4. SummaryWe take the present paper to have cleared the way to the effective use of BW asa benchmark. Firstly, our methods for generating random instances with meaningfuldistributions enable systematic experiments to be performed. Secondly, by presenting18 At the minimum, the “classical” version considered in this paper, in which the table capacity is unlimited, isa useful relaxation of the limited-capacity version. An optimal plan for the former yields an underestimate of theplan length for the latter and thus an admissible heuristic.C19 The intractability of near-optimal planning for SASity result for the mentioned subclass of STRIPS [26] and from the inclusion of this latter subclass in SAS20 BW planning is approximable within a constant because (1) a constant number of non-constructive actionssuffices to resolve all the deadlocks involving a given constructive action, and (2) the non-constructive actionscan be introduced in such a way as not to create new deadlocks.-US follows directly from the corresponding intractabil--US.C150J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153linear-time algorithms for near-optimal BW planning within a factor of 2, we have closedthe question of its time complexity. These algorithms and our optimal solver providea reference point for future experiments. Thirdly, we have empirically established theaverage plan quality of near-optimal solution methods, thus setting the parameters forevaluation of suboptimal BW planners. Finally, by careful experimentation, we haveidentified the distribution of hard and easy instances, and isolated the features most relevantto efficient planning for the domain.While most of our observations and results are aimed at enhancing the understanding ofBW needed for assessment purposes, the same understanding is a prerequisite for the largerproject of extending insights gained from BW to a wider class of domains. We expect thatour investigations will contribute to that project too, and hope that this paper will not beseen merely as a report on how to stack blocks fast.AcknowledgementsThis paper has benefited from discussions over a number of years with many people,including Philippe Chartier, Joachim Herztberg, Eric Jacopin, René Quiniou, Bart Selman,Kerry Taylor and Toby Walsh. We are also thankful to the anonymous reviewers of ourpapers on the topic, and especially to a reviewer of the present paper for the alternativedefinition of the number of states in Section 2.2.Appendix A. Experimental dataThis appendix details the setup for each of the experiments in Section 4. Note that the2-dimensional graphs in that section are series of lines joining all the data points.Time Performances (Figs. 8 and 9). The programs were written in C and compiled withgcc using optimisation to level O2. Times were obtained using the C library functiontimes(). The experiments were run on a Sun Enterprise 450 under Solaris. The systemhas four processors and 2Gb of memory. For the near-optimal algorithms, 100 randomproblems were generated for each size n a multiple of 10000, up to n = 1 million. Forthe optimal algorithm, 100 random problems were used for each size n a multiple of 5 upto n D 150. In the optimal case, the median rather than the mean is plotted, because thedistribution of times is expected to be heavy-tailed, so that the observed mean depends onthe sample size.Plan Length and Plan Quality (Figs. 11 and 13). For the experiments on plan length,between 100 and 10000 random BW problems were used for each of the 55 sizes weconsidered, up to n = 1 million blocks, forming a test set of some 250000 problems.More specifically, we generated 10000 problems for each size up to n D 10 and each sizea multiple of 10 up to n D 100, 5000 problems for each size n a multiple of 100 fromn D 200 to n D 1000, 1000 problems for each size n a multiple of 1000 from n D 2000to n D 10000, 500 problems for each size n a multiple of 10000 from n D 20000 toJ. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153151n D 100000, and 100 problems for each size n a multiple of 100000 from n D 200000to n D 1000000. For the experiments on plan quality, we used 10000 problems for eachsize up to n D 50 and 3000 problems for each size n a multiple of 10 from n D 60 ton D 140.Hardness and Live Blocks (Fig. 14). The figure has been produced by solving optimally50000 random problems of n D 100 blocks, and recording for each of them the numberof times PERFECT backtracks, as well as the number of live blocks. The curve shows themedian of the hardness cases for each number of live blocks. The live blocks are thosewhich are involved in deadlocks but not as singletons. A block x is involved in a deadlockiff N (cid:3).x; x/, where N (cid:3) is the transitive closure of the N relation. So the set of deadlockedblocks can be computed in O.n3/ at worst, which is the time taken to calculate both N andits transitive closure. The singleton deadlocks are computed in O.n2/ using the proceduredescribed in Section 3.6, and are removed from the set.Misplaced Blocks (Fig. 15). The figure has been produced by analysing 10000 randomproblems of size n D 1000 blocks. We also conducted the same experiment on 1000problems of sizes n D20, 100 and 10000, respectively, and obtained similar figures.Deadlock-Free Blocks (Fig. 16). The figure has been produced using 10000 randomproblems for each size up to n D 50. We also conducted the same experiment on 100problems for each size n a multiple of 50 from n D 50 up to n D 400. This confirmed thatthe average number of deadlock-free blocks not on the table does not exceed 2 and eventends to decrease slowly with n (1.87 for n D 400). The deadlock-free blocks are computedin O.n3/ as indicated above (see live blocks).Singleton Deadlocks (Fig. 17). The data set for this experiment consisted of 1000 randomproblems with n blocks for each n 6 10 and for each of the following values of n: 15, 20,25, 30, 35, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 350, 400, 500, 600, 700, 800,900, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 5000, 6000, 7000, 8000, 9000, 10000,15000, 20000, 25000, 30000, 35000, 40000, 50000, 60000, 70000, 80000, 90000, 100000.For each problem in the set, if n < 2000, all blocks were selected, otherwise 1000 blockswere selected at random. Each selected block was tested to see whether any block belowit initially was also below it in the goal. For each block selected, this computation takestime linear in the number of blocks below it initially, so we could only afford to considerproblems of size up to n D 105 blocks instead of n D 106 as in the above experiment onplan length.Hardness and Towers (Fig. 18). For each configuration (number of towers initially andin the goal from 1 to 50), 1000 random problems of n D 100 blocks were used. Since thegraph is symmetric about the diagonal, only half of the cases needed to be considered. Evenso, altogether more than a million problems with 100 blocks were solved optimally usingPERFECT, and the number of backtracks recorded. Experiments for smaller n showed thesame easy-hard-easy pattern, the location of the peak increasing with n (see [30]).152J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153References[1] F. Bacchus, F. Kabanza, Using temporal logic to control search in a forward chaining planner, in: Proc.EWSP-95, Assisi, Italy, 1995, pp. 157–169.[2] F. Bacchus, F. Kabanza, Using temporal logic to express search control knowledge for planning, ArtificialIntelligence 116 (1–2) (2000) 123–191.[3] C. Bäckström, Five years of tractable planning, in: Proc. EWSP-95, Assisi, Italy, 1995, pp. 19–33.[4] C. Bäckström, B. Nebel, Complexity results for SASplanning, Computational Intelligence 11 (4) (1995).[5] T. Bylander, The computational complexity of propositional STRIPS planning, Artificial Intelligence 69C(1994) 165–204.[6] S.V. Chenoweth, On the NP-hardness of Blocks World, in: Proc. AAAI-91, Anaheim, CA, 1991, pp. 623–628.[7] P. Doherty, J. Kvarnström, TALplanner: An empirical investigation of a temporal logic-based forwardchaining planner, in: Proc. 6th International Workshop on Temporal Representation and Reasoning (TIME-99), 1999.[8] M.D. Ernst, T.D. Millstein, D.S. Weld, Automatic SAT-compilation of planning problems, in: Proc. IJCAI-99, Stockholm, Sweden, 1999, pp. 1169–1176.[9] T.A. Estlin, R.J. Mooney, Hybrid learning of search control rules for plan-space planners, in: Proc. EWSP-95, Assisi, Italy, 1995, pp. 145–156.[10] M.R. Garey, D.S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness,Freeman, San Francisco, CA, 1979.[11] A. Gerevini, L. Schubert, Inferring state constraints for domain-independent planning, in: Proc. AAAI-98,Madison, WI, 1998, pp. 905–912.[12] E. Giunchiglia, A. Massarotto, R. Sebastiani, Act, and the rest will follow: Exploiting determinism inplanning as satisfiability, in: Proc. AAAI-98, Madison, WI, 1998, pp. 948–953.[13] N. Gupta, D.S. Nau, Complexity results for Blocks World planning, in: Proc. AAAI-91, Anaheim, CA,1991, pp. 629–633.[14] N. Gupta, D.S. Nau, On the complexity of Blocks World planning, Artificial Intelligence 56 (1992) 223–254.[15] S. Kambhampati, B. Srivastava, Universal classical planner: An algorithm for unifying state-space and plan-space planning, in: Proc. EWSP-95, Assisi, Italy, 1995, pp. 81–94.[16] H. Kautz, B. Selman, Planning as satisfiability, in: Proc. ECAI-92, Vienna, Austria, 1992, pp. 359–363.[17] H. Kautz, B. Selman, Pushing the envelope: Planning, propositional logic, and stochastic search, in: Proc.AAAI-96, Portland, OR, 1996, pp. 1194–1201.[18] H. Kautz, B. Selman, The role of domain-specific knowledge in the planning as satisfiability framework, in:Proc. AIPS-98, 1998, pp. 181–189.[19] J. Kvarnström, P. Doherty, P. Haslum, Extending TALplanner with concurrency and resources, in: Proc.ECAI-2000, 2000, pp. 501–505.[20] E.L. Lawler, J.K. Lenstra, A.H.G. Rinnooy Kan, D.B. Shmoys, The Traveling Salesman Problem, Wiley,Chichester, 1992.[21] N.N. Lebedev, Special Functions and Their Applications, Dover Publications, New York, 1972.[22] S. Minton, Learning effective search control knowledge: An explanation-based approach, Ph.D. Thesis,Carnegie-Mellon University, Pittsburgh, PA, 1988.[23] Y. Qu, S. Kambhampati, Learning search control rules for plan-space planners: Factors affecting theperformance, in: Proc. EWSP-95, Assisi, Italy, 1995, pp. 133–144.[24] A.E. Rizzoli, L.M. Gambardella, M. Zaffalon, M. Mastrolilli, Simulation for the evaluation of optimisedoperations policies in a container terminal, in: Proc. HMS99, Maritime and Industrial Logistics Modellingand Simulation, Genoa, Italy, 1999.[25] M.J. Schoppers, Estimating reaction plan size, in: Proc. AAAI-94, Seattle, WA, 1994, pp. 1238–1244.[26] B. Selman, Near-optimal plans, tractability, and reactivity, in: Proc. Internat. Conference on the Principlesof Knowledge Representation and Reasoning (KR-94), Bonn, Germany, 1994, pp. 521–529.[27] J. Slaney, Generating random states of blocks world, Technical Report TR-ARP-18-95, AutomatedReasoning Project, Australian National University, Canberra, 1995.[28] J. Slaney, S. Thiébaux, Adventures in blocks world, Technical Report TR-ARP-7-94, Australian NationalUniversity, Canberra, 1994.J. Slaney, S. Thiébaux / Artificial Intelligence 125 (2001) 119–153153[29] J. Slaney, S. Thiébaux, Blocks World tamed: Ten thousand blocks in under a second, Technical ReportTR-ARP-17-95, Automated Reasoning Project, Australian National University, Canberra, 1995.[30] J. Slaney, S. Thiébaux, How best to put things on top of other things, Technical Report TR-ARP-6-96,Automated Reasoning Project, Australian National University, Canberra, 1996.[31] J. Slaney, S. Thiébaux, Linear time near-optimal planning in the Blocks World, in: Proc. AAAI-96, Portland,OR, 1996, pp. 1208–1214.[32] J. Slaney, S. Thiébaux, On the hardness of decision and optimisation problems, in: Proc. ECAI-98, Brighton,England, 1998, pp. 244–248.[33] T. Slavin, Virtual port of call, in: New Scientist, June 1996, pp. 40–43.