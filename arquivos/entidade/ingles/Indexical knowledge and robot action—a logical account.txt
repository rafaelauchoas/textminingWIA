ELSEVlER Artificial Intelligence 73 ( 1995) 69-115 Artificial Intelligence Indexical knowledge and robot action-a account logical Yves Lesphnce *, Hector J. Levesque ’ Department of Computer Science, University of Toronto, Toronto, Ont., Canada MSS IA4 Received October 1992; revised May 1993 Abstract The knowledge required for action is generally indexical rather than objective. For example, a robot that knows the relative position of an object is generally able to go and pick it up; he need not know its absolute position. Agents may have very incomplete knowledge of their situation in terms of what objective facts hold and still be able to achieve their goals. This paper presents a formal theory of knowledge and action, embodied in a modal logic, that handles the distinction between indexical and objective knowledge and allows a proper specification of the knowledge prerequisites and effects of action. Several kinds of robotics situations involving indexical knowledge are formalized within the framework; these examples show how actions can be specified so as to avoid making excessive requirements upon the knowledge of agents. 1. Introduction 1.1. Motivation Designing autonomous robots or other kinds of agents that interact in sophisticated is hard; you need good tools to do it. Designs should ways with their environment theories of the interactions that agents have with their be based on well-developed environment. There has been a widespread perception that logic-based formalisms are unsuitable for this task because classical logic can only represent objective knowledge, it cannot capture the context-relativity, the “situatedness” of action and of the knowledge required for action. In this paper, we will show that this view is inaccurate. We will present a logical theory of knowledge and action that does accommodate indexicality * Corresponding author. E-rnaii: lesperan@ai.torunto.edu. 1 Fellow of the Canadian Institute for Advanced Research. 0004-3702/95/$09.50 @ 1995 Elsevier Science B.V. All rights reserved SSDIOOO4-3702(94)00010-X 70 Y Lespkrunce, H.J. Levesque/Artijiciul Intelligence 73 (1995) 69-115 framework for modeling many kinds of interactions and show that it provides a suitable that typically occur between a robot and its environment. Logic-based ours can help clarify many notions doubt such theories do not capture all aspects of situatedness be done to show that they can be successfully after having examined to the theoretical no matter what kind of architecture behavior. sophisticated theories such as that are used in informal analyses of interaction. No to to robot design. But we think that this work, one will agree that it makes an important contribution for applicability, of the field and has significant potential turns out to be best for achieving and much work remains reactive yet foundations applied is incomplete knowledge: agents A key feature of agent-environment typi- interaction cally know very little about their environment. Many theories of action and most existing the need to deal with what agents know and need to planners have completely that agents always have perfect knowledge of the do- know by unrealistically [ 42,431, main under consideration. But in the past decade, Moore 140,411, Morgenstern and other researchers have proposed that do address this need. ignored assuming theories of knowledge and action inserting is working, to withdraw for someone a sequence of keys a bank card and pressing For example, it is physically possible funds at a teller the there is money available, and so on). But to it is physically possible funds, we machine by machine the card is not defective, this a very weak notion; by the same token, write a sonnet at a typewriter. To say that an agent is truly able to withdraw would want to insist that the agent know among many other things, his bank identification this stronger notion of ability code. The reason sense of actions that otherwise to explain an agent who always it carefully before pushing is that it allows us to make seem to have no relevant effect. For example, how else takes a small piece of paper out his wallet and looks at for a monkey is so useful (assuming the keys on the teller machine? actions such as these, we need to consider agents that have knowledge To understand about l the effects various actions would have upon their knowledge; l the knowledge prerequisites of the actions they might do-the conditions under which they are able to achieve goals by doing actions. do provide such as Moore’s and Morgenstern’s Theories reasonable answers to the of the issues question of what the relationship between knowledge and action is and have considerably advanced our understanding work deals with one aspect of the relationship been neglected and is inadequately handled by these theories: required (or relative) knowledge l if a robot knows and action the fact that the knowledge it is often indexicul involved. But many problems between knowledge the relative position of an object he can go and pick rather than objective knowledge. For example, to the agent’s perspective-that remain. This that has is often relative for action it up- knowing might not know where he is) ; the absolute position of the object is neither necessary nor sufficient (he l one can soft-boil an egg if one has a timer-knowing at what time it will be done is neither necessary nor sufficient (one may not know what time it is). In these cases, the agent has very incomplete knowledge of his situation, at least as far to achieve as objective but this does not interfere with his ability facts are concerned, b Lespbance, H.J. Levesque/Artijicial Intelligence 73 (1995) 69-115 71 his goals by doing actions. More generally, an agent may have sufficient knowledge to be able to achieve his goals even if he does not know l where he is; l what time it is; l which objects are around him; l where these objects are located (in absolute terms) ; l who he is. This is the case because the knowledge required for physical interactions with the en- vironment is indexical knowledge, knowledge about how one is related to things in the environment or to events in one’s history. This should not come as a surprise, since agents act upon their environment from a particular perspective, a particular place and moment in time. The same action done at different places and times has dif- ferent effects. So it makes sense that the prerequisites of an action should involve knowledge that is relative to this perspective. Similarly and fortunately, the knowl- edge supplied by perception is indexical knowledge. For example, by using his sonar, a robot comes to know how far from him an object is (at the current time) ; he does not learn anything about the object’s absolute position. Indexicality has of course been a central theme of the “situated action” paradigm [ 5,5 1,531. Several researchers have argued that indexical knowledge plays a major role in the operation of reactive agents [ 2,521. Existing theories of the relationship between knowledge and action cannot handle these cases properly, as they do not accommodate the distinction between indexical knowledge and objective knowledge. They impose unnecessarily strong knowledge requirements upon agents before sanctioning their ability; they tend to require objective knowledge when indexical knowledge would be sufficient. They also cannot represent accurately the effects of action upon knowledge. We need an account of ability that squares with the fact that an agent may be missing very basic facts about his objective situation and that of the things around him and still be capable of achieving his goals. The reason we care about this is that this form of incomplete knowledge is the norm rather than the exception for agents in the world. This is not to say that objective knowledge is never necessary for action. Because it is independent of the context, it can be stored or communicated without the need for adjustments to compensate for changes in the context. It is clear that human agents have all sorts of objective knowledge about the world and draw upon a variety of external sources of objective knowledge such as textbooks, databases, maps, timetables, etc. In hierarchical planning, it is likely that top-levels plans involve actions that are specified in a relatively objective way (e.g., in planning a car trip across the province, one first decides on what roads to follow and only later on whether to turn right or left). What this means is that a theory of knowledge and action must accommodate both the indexical knowledge involved in interaction with the physical environment and the objective knowledge involved in high-level planning and social interactions. As well, it must allow each kind of knowledge to be mapped into the other provided that knowledge of the context is available, so that objective knowledge can be exploited in physical interactions and indexical knowledge is available for communication, long-term storage, etc. 72 E Lespkrance, H.J. Levesque/Artijkial Intelligence 73 (1995) 69-115 1.2. The approach support between it distinct and action indexicality; that handles latter notion (i.e., defined the distinction the representation is used to capture We want a theory of knowledge in a wide range of circumstances. of indexical knowledge prerequisites for indexical knowledge prerequisites that formalize notions of knowledge, in- and taken is to develop a quantified modal logic that embod- time, the physical possi- is formalized as a derived in terms of the notions mentioned above). The epistemic and tem- of indexical knowledge, dexical and objective knowledge and accounts effects of action. The approach ies the theory. The logic includes primitives action, and historical necessity-the bility of a course of action for an agent. The notion of ability operator poral parts of the logic adequately keeping knowledge operator standard possible-world plete system permits an adequate characterization and effects of actions The specification for the it is a simple and natural extension of the [ 18,201. The com- from objective knowledge. We provide a formal semantics that handles semantic scheme for the logic of knowledge of the theory as a logical system has several advantages. The prop- to provably valid sentences of the to agents correspond is precisely that the theory attributes this ensures erties since attributions logic; be objects of be- they can of properties lief/knowledge by agents; so for example, not only can you state that if an agent does S he will know whether p, but also that the agent knows that if he does 6 he will know whether 9. In fact, our theory models not only interactions but also agents’ knowledge In it, whenever an agent is able to achieve a goal by doing an action, about interactions. he knows like “if agent a knows rp then a can achieve 9 by doing s” is valid in the logic, then this must be known by all agents. the theory’s claims and the ontological The provision of a formal semantics formally about knowl- commitments edge, action, and ability-the that the kind of reasoning a designer might do in ensuring is able to achieve a goal. Our methodology differs little from that agent being designed [ 42,431, and others who have previously worked used by Moore on theories of knowledge [ 40,4 11, Morgenstern and action. the logic can be used to reason to agents emerge as sentences, that this is the case. Whenever specified. Moreover, involved. Finally, that the theory a statement themselves clarifies it uses logic, it does not take any representational Note that our theory is at the “knowledge level” is represented by sentence-like to the position that knowledge about agent architecture. While (i.e., assume is similar work [ 24,481. But there is an emerging consensus for agents will need to be based on an adequate Our work attempts A substantial taken by Rosenschein to provide this kind of foundation. [ 451, and as such, does not say much stance this inside agents); in their situated automata that satisfactory architectural designs interaction. entities theory of agent-environment and Kaelbling part of our efforts has been devoted is truly useful and that it handles adequately application domains within the theory designed crucial effect upon what knowledge theory; The applications to the formalization the logical system proposed. This is essential of various that that it is that have a from an agent, but are not settled by the the kind of parameters an action should take. show how actions can be specified so as to avoid making involves many decisions the kind of situations for example, decisions concerning the formalization for. Moreover, is required of actions developed to ensure E Lespkance, H.J. L.evesque/Artijicial Intelligence 73 (1995) 69-115 73 excessive requirements upon the knowledge of agents, and how such specifications can be used to prove that an agent is able to achieve a goal by doing an action if he knows certain facts. Much of our formalization efforts have been directed at a robotics domain, since this kind of application provides the most intuitive examples of situations where indexical knowledge is sufficient for ability. We will describe in detail our formalization of this domain, where a simple robot is involved in various types of interaction with its en- vironment. In our formalization, perception does yield indexical knowledge and ability to act upon an object does not require knowing which object is involved or what its absolute position is. We also show how indexical knowledge and objective knowledge can be and must be related in our framework to deal with the use of maps for navigation. We discuss the representational issues that arise, which have general relevance to the formalization of actions with indexical knowledge prerequisites or effects. In the next section, we briefly discuss related work. Then in Section 3, we give an overview of the logical system that forms the basis of our framework. The core of the paper is Section 4, where we describe our formalization of various types of interactions a simple robot has with its environment. Following that, Section 5 briefly describes other application domains that have been formalized, in particular cases where temporal knowledge is required for ability. Then in Section 6, we argue that the applications examined, in particular the ones involving temporal knowledge, provide convincing evidence that the distinction between indexical and objective knowledge supported by our framework has substantial practical value and that it cannot be done justice within existing accounts of ability. We conclude in Section 7 by discussing the contributions and limitations of this work, and suggesting various directions for further research; we discuss how this work might be applied to the design of autonomous robots and other types of agents. 2. Related work This survey is selective and often sketchy; for a more complete discussion, see [ 291. 2.1. Theories of knowledge and action As argued in the introduction, a theory that explains how agents manage to achieve their goals by doing actions, and in particular why they perform knowledge acquisition actions, must account for the effects of action upon knowledge and for the knowledge prerequisites of action; it must include an account of ability as opposed to mere “‘physical possibility”. The most influential work in this area is Moore’s theory of knowledge and action [ 40,4 11. His framework can be described as a combination of first-order dynamic logic (a modal logic of action) [ 131 with an S4 modal logic of knowledge [ 18,191. 2 2 Strictly speaking, the framework is not the modal logic just described, but the encoding in first-order logic of the semantics of this modal logic; thus the part of the logic dealing with action is closely related to the situation calculus [ 381. 74 r Lespt!rctnce. H.J. Levesque/Art@cul Intelligence 73 (1995) 69-f 15 of action, notice On the matter of knowledge prerequisites action would achieve a goal without knowing how to execute one may know that cooking beef bourguignon would impress knowing how amounts how to do an action action. that one may know that an that action; for example, the party’s guests without For Moore, knowing how to do an action stands for; knowing to de ditto knowledge of the to knowing what primitive actions that action (description) requires having de re as opposed to cook beef bourguignon. if John Let us review the philosophical relation between that the new manager is sufficiently precise to pick up a particular (whoever he is) must be happy-a to be the new manager, but is not told whom, he may then come to de ditto rather than de is Paul, he will then come to have a de lore on this distinction. De re knowledge attributions the knower are said to assert the existence of some kind of epistemic is that they apply to cases where and some entity. The intuition behind such attributions the agent’s knowledge entity, as opposed to being about whatever satisfies a description. The latter cases are said to involve mere is told that someone among his co-workers de ditto knowledge. For example, has been selected believe re belief. If he is later told that the new manager [ 191, knowing who/what re belief of Paul that he must be happy. Following Hintikka to knowing of some x that it is 0 (de re). The question 0 is usually for an agent to have de re knowledge has been the subject of what precisely of much philosophical In AI, the common answer has been that having de re knowledge of some entity requires knowing a standard name a view shared by Moore as well as the present work. Since for that entity what standard names that they must and thus, that de re knowledge must in some sense always be objective be objective, knowledge. But this can be a bit misleading, as knowing what the relative position of an object is or knowing what primitive actions an action stands for hardly qualify as having objective knowledge. The domain over which one quantifies matters. In such cases, one can at best say that one has “objective” knowledge of a relational property. refer to must be common knowledge; to amount is required [ 281 for a discussion). this means [26,35], debate taken (see is, and knows of himself the action next necessarily So Moore uses this distinction know who he is. Complex actions are handled is said to be able to achieve a goal by doing a sequence of two actions of ability. For atomic actions, his in his formalization account goes as follows: an agent is able to achieve a goal by doing an action if and only that it is physically possible if he knows what the given action for him to do the action next and that his doing in to have de re knowledge of the goal being achieved. Note that the agent for instance, an recursively; himself-to agent if and only the goal of being able to achieve the main goal by doing the second action. The agent need not initially know what all that make up his plan are; he needs only know that he will know what to do the actions next at each step of the plan. Note that in most cases, the specification of an action need these fall out of the theory’s not say anything explicit about its knowledge prerequisites; general principles under which one knows what and the specification the action the first action, he is able to achieve of the conditions if by doing is required results is. The requirement that the agent know what the action is has interesting results when the action is an instance of a parametrized procedure in many such cases, agents know what the procedure (e.g. DIAL( COMBINATION( SAFES) ) ) ; is and one wants to say that an Y Lespe’rance, H.J. Levesque/Art$cial Intelligence 73 (1995) 69-115 I.5 formalism (technically, rigid function). But note that assuming is if he knows what the arguments one states are; it is easy to state agent knows what the action is an epistemi- this in Moore’s tally is known whenever one knows what the parameters are is often wrong. For example, whether one to do with whether one knows a standard knows how to PICKUP(BLOCK~) name for BLOCKI; the relative position of BLOCKI, or more generally where BLOCK1 is relative to oneself. One has to be very careful as to how one parametrizes the actions, and in many cases, the parameters must be indexical descriptions. that the procedure that an instance of a procedure it has more to do with whether one knows has nothing But Moore’s to the agent; that he himself the distinction between (e.g., an agent’s knowing framework does not really accommodate this concept requires clearly objective) indexi- that is indexical with cal and objective knowledge. His logic does not handle knowledge it does not capture an agent’s concept of himself, and knowledge respect is holding a block). that involves the agent to know who he is (know a standard name for himself, So the account something in order to be judged able to achieve a goal by doing an action. This is clearly unnecessary. What need would a very simple agent, say a robot insect, have for knowing who he is? Because of this, the formalism cannot really model what knowing to be an important (see Sec- tion 6 for more discussion of this). Moore’s logic also does not handle knowledge about to say that after looking at a clock, absolute together with the way an agent knows what time of various domains the account in Moore’s and not enough indexical knowledge. feature of how reactive agents deal with their environment treats parametrized framework procedures mean that formalizations too much objective knowledge under an indexical description it is.3 These expressive times. So for instance, to. Yet this seems it is not possible to require limitations something amounts tend of Moore’s account of ability. Firstly, Let us point out a few other limitations the notion: one is taken the action absolutely guarantees is a highly idealized version of the commonsense is not just a matter of knowing what actions likely to achieve the goal is that deals with uncertainty, notion formalized that doing as able to achieve a goal only if one knows that the goal will be achieved; that it is highly just knowing It would be great to have a version of the account not enough. it may be argued that but this is not something we attempt ability to take. For example, does knowing how to play a sonata only require knowing which notes to play and does knowing how to make? On the other hand, to ride a bicycle only require knowing which movements flaw in the account; perhaps it is not clear that these examples point to some essential it is just a matter of making explicit associated from playing a note, as well as keep with the action discourse track of what note in terms of their effects rather than in terms of what body where we individuate the temporal and memory constraints (e.g., one needs to recover quickly in the present work. Secondly, to play next). Finally, in commonsense there are cases actions 3 Technically speaking, Moore models knowledge with an accessibility relation that is indexical with respect to the agent because that ranges over (instanta- them. is to characterize who the agent thinks he might be in these states. He also cannot represent knowledge time is associated with them. This should become times because his states have no absolute neous) world states. He cannot represent knowledge nothing about absolute clearer after our accounts of knowledge and ability have been introduced. 76 Y Lesphance, H.J. Levesyue/Art@cial Intelligence 73 (1995) 69-115 or effector commands (e.g., one may say that at the end of movements every working day, a sales representative does the same action no matter where he might from the one we be, he goes home). But this is a different sense of “doing an action” are concerned with; we are not trying language discourse about action. to produce a semantic account of natural get executed Let us briefly discuss other theories of knowledge and action. One of the unattractive extends included is essentially is that knowledge in Moore’s framework a recasting of Moore’s A theory that significantly to be closed under logical consequence. Konolige is features of the logic of knowledge [ 25 ] has developed a theory assumed of knowledge and action, based on an account of knowledge as a predicate on sentences, that avoids this defect. 4 His account of ability into his framework; only the same restricted class of actions is handled. this coverage has been developed by Morgenstern It handles both concurrent actions and plans involving multiple agents. Simpler is also syntactic, but differs to the paradoxes in the hierarchy). Note that in her account. Her argumen- closure the weak knowledge prerequisites [ 42,431. cases are treated as in Moore’s. Her account of knowledge significantly and there is a single knowledge tation against classical problem, but on the claim involved faucet, but knows the faucet by asking John to do it for him. However, our recent work [ 341 undermines this argument; we show that the notion of “somehow being able to achieve a goal” can framework. in fact be modeled if Paul does not know how to fix a leaky that his friend John is somehow able to do it, then he is able to fix truth predicate is, in fact, closed under for all the languages logical consequence (she does not use the Tarskian approach in multi-agent planning. For example, is not based on the consequential that they cannot express logics of knowledge in a possible-world from Konolige’s Neither Konolige nor Morgenstern have the same recognize in action; in this respect. One researcher it is Haas. In [ 161, he sketches how indexical knowledge might be the role of indexical knowledge as Moore’s limitations in a specification of ability; but he does not formalize his proposals. their formalisms who did recognize handled 2.2. Theories of indexical knowledge There has been a lot of work on indexical knowledge in philosophy, but we can is to oneself is an from Lewis’s view that having a belief only mention a few references here (see [29] ). Our account of indexical knowledge inspired and the current essential In [461, Perry argues convincingly involves ascribing a property feature of propositional that indexicality attitudes. [37]. time Let us say a bit more about some recent work by Grove [ 151, where account of knowledge the same as ours. However, for action and not account they propose a logic of knowledge to ours; is quite similar their work is more narrowly to knowledge, its relationship [ 141 and Grove and Halpern indexicality. Their is essentially in fact, their semantics that does handle focussed and only handle than ours; they do in a time 4 Note however that such “syntactic” accounts have been claimed to have the opposite defect, that is, to individuate knowledge states ton finery [361; for instance, it is far from clear that a belief that 9 and C,O’ is any different from a belief that ‘p’ und rp. K Lesphance, H.J. L.evesque/Artifcial Intelligence 73 (1995) 69-115 77 limited way. On the other hand, their work is technically very thorough; they discuss several logical systems with varying degrees of expressiveness, both propositional and first-order, and they provide complete axiomatizations and complexity results5 They also develop an appealing approach to quantification into epistemic contexts that permits a resolution of the ambiguity of de re reports with respect to the way the knower is referring to the object. 2.3. Reactive behavior and situated action It has been argued that indexicality plays an important role in how agents manage to react in a timely manner to changing environmental conditions (e.g., avoid collisions with moving objects). Let us look at some recent work on reactive agents where this comes out. This is a promising application area for our work. The classical AI paradigm with respect to the production of intelligent behavior involves a smart planner that searches for a plan that achieves the agent’s goal and a dumb executor that carries out the plan in a mechanical way. In recent years, there has been a definite movement towards exploring alternatives to this paradigm. It is felt that because of the emphasis it places on search for complete plans, classical deliberative planning is too slow for producing reactive behavior. Moreover, it is not sufficiently grounded in perception; much of the effort expended on constructing a plan may be wasted if environmental conditions change in the meantime. The alternative architectures proposed achieve reactivity by emphasizing environment monitoring and the selection of actions appropriate to conditions; the focus is on developing a sophisticated executor. Agre and Chapman [ 2,3] have been among the most radical in their reevaluation of the classical paradigm. Their views have been influenced by anthropological theories of action [ 1,531. They emphasize the complexity of the real situations in which action occurs, the uncertainty of the information the agent may have about them, and the need for reactivity. This leads them to argue that the production of most activity does not involve the construction and manipulation of explicit representations of the world; the associated computational costs are just too prohibitive. They say that: Rather than relying on reasoning to intervene between perception and action, we believe activity mostly derives from very simple sorts of machinery interacting with the immediate situation. This machinery exploits regularities in its interac- tion with the world to engage in complex, apparently planful activity without requiring explicit models of the world. [2, p. 2681 The architecture they propose involves peripheral modules for perception and effector control and a central system that mediates between the two. They argue that combina- tional networks, that is, circuits computing logic functions, can form an adequate central system for most activities; thus in such cases, the central system has no state. They 5 Interestingly, their results seem to show that the conryiexity of deciding whether a sentence is valid is no worse in systems that accommodate the distinction between objective and indexical knowledge than in comparable systems that do not. 78 E Lespe’mnce, H.J. Levesque/Artijkiul Intelligence 73 (1995) 69-115 systems representations involving objective have built various application systems, called Pengi, plays a videogame where one controls a penguin in a maze, pushes away ice blocks, and confronts malicious “killer bees” to provide support for their analysis. One of these that navigates [ 21. From our point of view, the most interesting elements of Agre and Chapman’s are their notions of indexical-functional domain are inappropriate They see the machinery such as “the block I’m pushing” intend to clobber indexical because they depend on his purposes. Clearly, Agre and Chapman information they do not propose any forma1 version of this modeling scheme entities and aspects. 6 They claim that traditional facts such as “(AT BLOCK-213 427 991)” to the agent’s situation or goals. indexical-functional entities aspects such as “the bee I than I am”. These entities and aspects are they are also functional because find the notion of indexical their robots and explaining how they behave. However, is closer to the projectile they depend on the agent’s situation; they do not make reference in their networks as registering and indexical-functional in designing scheme. because useful analysis includes to refer complexity and the role that attempts an interesting representations representations. a computational agents, includes Subramanian and Woodfill terms. The are introduced [52] have recently proposed indexical they use a version of the situation indexical account of play their in them; to trace the source To model calcu- logical constant Now that ev- to refer to the are also that in to de- reactive agent architectures work of the efficiency gains associated with the use of indexical the world as viewed by reactive that [38] with a vocabulary lus situation. Constraints the current is used ery situation has at most one predecessor to a situation’s predecessor most recent action). Domain-dependent used. Subramanian determine what a reactive agent will do next (one-step planning) this framework. This duce what should be the case after an action has been performed. Their complexity analysis of this kind of setting representations indexical without tities of the relevant instead, their words: “the power of using quantification.” theories can be propositionalized they do not quantify over all en- In indexically. it gives us implicit indexical explosion because they refer these terms, such as This-Block, control the efficiency gains associated with the use of (e.g., Before(Now) indexical to the entities terms show how the kind of indexical and the function Before to ensure is used the usual combinatorial rules, which are used to the situation prior for plan-monitoring can be specified and Woodfill is also done to the fact indexical involved is that traces refers type; rules that in particular, of whether framework Independently and Woodfill’s calculus, to distinguish fails the semantics and the simple world-dependence account for the distinctive the past is determined while the future its inability this analysis inherits most of the limitations is correct, to handle knowledge between of non-rigid logical behavior of indexical acquisition the context dependence constants. Finally, it must be noted that Subramanian situation of the ordinary actions. Moreover, terms of indexical the theory does not the fact that terms, for example, is not. h This is Agre and Chapman’s entities; but there are indexical termmology. There arc of course no such things as indexical (or agent-relative) concepts or notions of things. (or agent-relative) E Lespdrance, H.J. L.evesque/Art@cial Intelligence 73 (1995) 69-115 79 Another conception of how reactive behavior [ 24,481. Their approach and Kaelbling is used to specify tools based on the logic are also developed; agents and allow high-level reactive agents and the environments is of particular can be produced has been proposed to us they the interest in which these tools facilitate specifications to be “compiled” by Rosenschein because a logic operate. Design specification into circuit-level of complex ones. The architecture states. They propose an alternative way of doing they propose for reactive agents It involves a perceptual to machine representations representations. does not mean both of which may have state of explicit component, of explicit content automata view. It involves viewing state contains A machine that sp) if given the machine that a certain wire in a robot carries a “1” would mean a certain only be carrying content of a machine conditions. radius of his position, a “1” if there if given is an object within is in that state implies the state transition state is defined the information component that one loses the ability [ 231 also avoids formal manipulation and an action selection (registers). They also argue that the lack to ascribe semantic situated as coupled automata. that cp (i.e., in that state, the machine knows the fact that the fact that there is an object within the wire can the state transition and world, functions, this-the that 5p must be the case. For example, the information to environmental in terms of how it is correlated the agent and its environment functions of the machine that radius. Thus temporally to indexicality; do not discuss it does not handle Rosenschein and Kaelbling it does not handle knowledge to the agent, and while it accommodates of reactive behavior. Their as Moore’s with respect in the the role of indexical knowledge logic suffers from the same kind of expressive that indexical production limitations is indexical with respect objective knowledge. This may come as a knowledge, a robot surprise because in [481 uses that keeps propositions. But their logic does not really model all of that many symbols are used, it’s easy to indexicality. When their work strongly suggests lose track of what the formalism that a theory of knowledge indexical between and objective knowledge would be very useful for producing better accounts of reactive behavior and better tools for the design of reactive agents. domain-dependent really handles. However, that handles such as the one is within shouting distance track of whether a moving object indexical-sounding indexical-sounding the distinction of examples formalization and action temporally involving their 3. A logic of indexical knowledge, action, and ability In this section, we briefly review a formal ability, and some of its properties. A much more detailed examination including simply present two sections. Our theory theory of indexical knowledge, action and of the theory, issues involved can be found in [ 29,331. Here, we of the next a discussion of the general in sufficient detail first-order modal in a many-sorted the applications to underwrite is formulated the theory familiarity with conventional assume acquaintance with the standard apparatus of modal first-order logic as in logics, as described logic with equality. We [39], and at least some in [ 211 or [ 81. 80 Y Lespe’rance, H.J. Levesyue/Artijicial Intelligence 73 (1995) 69-115 3.1. syntax We want to be able to express attributions of indexical knowledge in our logic, for that he himself was holding a cup five minutes ago. In such that is relative. It may be relative to the knower, this, the current agent, and now, to other aspects of the context. To handle two special terms: self, which denotes time; these terms are called primitive indexiculs. Non-logical includes the current that Rob knows is a “proposition” example, cases, what is known or to the time of the knowing, or perhaps our language which denotes (domain-dependent) interpretation, is currently holding something-we Our semantics handles which consist of a possible-world, modeling and time, modeling counterparts words (more about this shortly). for example, UPHOLDING this by interpreting say that such symbols are non-primitive symbols may also depend on the current agent and time for their may express the fact that the current agent indexicals. to indices, and an agent to be formal from any such terms and formulas with respect the objective circumstances, the context. Note that self and now are not intended of similar sounding English words and behave differently logical symbols temporal language, individuals (as usual), function the sort: individual, symbols, and terms respectively, with superscripts The language we use is called LIKA and as any first-order it divides into terms and formulas. ’ The terms here, however, are of four different syntactically terms, agent terms, and action sorts: terms for ordinary terms. For each of these four sorts, there are both variables and function (that is, functions whose values will be of the proper sort) and as usual, constants are taken symbols. We will use the metavariables V, F, and 8 to range over to be 0-ary function variables, i, t, a, and d used temporal, agent, and action respectively. So, for example, to indicate terms ut stands for a temporal variable, and od stands for an action temporal are formed function terms cannot appear as arguments The atomic formulas using predicate symbols and terms, written ,19,), which are used to assert that 81, . . . ,8, stand in static relation R at the R(8,,... (01 = 02)) current between (0: < terms of the same sort, as well as expressions of temporal precedence 0:). Finally, Does( ed, 0’) is used to assert that the current agent does action ed starting from the current For non-atomic time and for the current agent. We also have equality expressions to function include predications term. Syntactically, firstly, only in the obvious way, with the following formulas, we have negations, time and ending at time 8’. symbols of any sort. and secondly, action terms as arguments, symbols may take temporal restrictions: implications, (such as disjunctions; and universal quantifica- see below). Finally, if CP then so are At( 8’, p), By(P, p), Ihow( tions, and all the standard abbreviations is a formula, that p holds at time et, that is, when 8’ is taken to be the current that q holds when P the current agent knows at the current should be taken as attributing himself and the current to be the current agent. Know(p) time that q. If Q contains indexical knowledge-knowledge time. For example, KIIOW(~XHOLDING(X)) is taken and 0~. At(h)‘, p) means time. By( P, p) means is used to say that indexicals, Know(~) the agent has about could mean that ’ LIKA stands for “Language of Indexical Knowledge and Action”. In [29], we used the name &de, instead of LIKA. L L.esp&ance, H.J. Levesque/Artijicial Intelligence 73 (1995) 69-115 81 the agent knows that he himself is currently holding something. Finally, q qp is used to say that (p is historically necessary, that is, that it must hold, given everything that has happened up to now. This completes the syntactic specification of the language. The notions of free variable, bound variable, and term that is free for a variable in a formula are assumed to have their usual definition. We use (p{v H 6) to stand for the result of substituting 6 for all free occurrences of v in (p, provided that u and B belong to the same sort. All symbols of the language other than the function symbols and the predicate symbols are considered to be logical symbols, and will have a fixed interpretation. In the examples of the next section, we will introduce domain-dependent function or predicate symbols (with names like POS or HOLDING) as well as axioms governing their interpretation. 3.2. Semantic structures The terms and formulas of LZKA are understood in terms of the following semantic W,@, -c, M, A, and K. Instead of a single domain of discourse, components: d,I,I,D, we have non-empty domains for each sort: a set A of agents, a set 7 > A of individuals, a set 7 of time points, and a set 2) of primitive actions. The terms of LZKA are taken to refer to elements of these domains, and quantification is handled in the usual way. W is a set of temporally extended possible worlds. As explained earlier, we handle indexicals by interpreting terms and formulas with respect to indices, which are triples consisting of a world, an agent, and a time; so & = W x A x 7 is the set of indices. We take a, i, t, d, w, and e (possibly subscripted, primed, etc.), as ranging over arbitrary elements of A, 1, 7, D, W, and E respectively. The By and At operators are used to change the agent and time component of the index respectively. The denotations of terms and the truth of predications are handled in the obvious way using a function Q, which, at each index, maps function symbols to functions over the appropriate domains and predicate symbols to relations over the appropriate domains. for each time point-that Turning now to the temporal aspects, formulas containing < are understood in terms of -x, which is a relation over 7 whose intended interpretation is “is earlier than”. M is a family of accessibility relations-one is used to interpret the historical necessity operator 0. Intuitively, w zt w* if and only if w and w* differ only in what happens after time t. Formulas containing Does are interpreted in terms of A C 2) x I x 7, which determines which actions are done by which agents in which worlds over which time intervals: (d, (w,a, ts), te) E A if and only if action d is done by agent a from time ts to time b in world w. Finally, our semantics for knowledge the stan- relation K is dard possible-world taken to hold over indices rather than plain possible worlds: K 5 &*. Informally, ((w,a, t), (~‘,a’, t’)) E K if and only if as far as agent a at time t in world w knows, it may be the case that w’ is the way the world actually is and he is a’ and the current time is t’. Thus, we model the knowledge state of an agent at a time in a world by a set of indices, which characterizes not only which worlds are compatible with what the agent knows, but also which points of view upon these worlds are compatible with what he knows. In other words, we allow an agent to be uncertain not only about what world [20,27 1. The knowledge accessibility is a simple generalization of scheme 82 Y. L.espPrunce, H.J. Levesque/Artijicral Intelligence 73 (1995) 69-115 he is in, but also about who he is and what time it is. 3.3. Denotation and satisfaction More formally, we have the following: a semantic structure M is a tuple is a function that maps variables is the assignment that is identical into elements of the domain appropriate to g except that it maps variable An assignment to them. g{u w x} u into the entity x. The denotation of a term 13 in a structure M at an index e = (w, a, t) under an assignment under consideration when it is clear from context): [f9jJFg 1s defined as follows g, written (from now on, we omit the structure . uUJle,$ = g(u). 0 [selfjle,, = a, l [nowj,,g = t, 0 [F(B,,... We can now define what it means ,~n,=.~ =~(~,e)(ue~n,,,...,us,n,,,). for a formula +CJ to be satisjed by a structure M, an index e = (w,a, t), and an assignment g, which we write M,e,g /= p: 0 e,g + R(#,. . .,e;) iff (uel;n,,,, . . , uebn,,,) E @(Re), l e, g + Does(Bd, 0’) iff A( pd],,,, e, uetn,,,), . e,g t= 4 = 02 iff [[h n,,, = ue2n,,,, /= lye iff it is not the case that e,g l e, g + 0; < 0: iff ue; De,, 4 ue:n,,,, l e,g l e, g k (91 > ~2) iff either it is not the case that e,g k cp~, or e,g l e, g k Vusp iff for every entity x in the domain appropriate /= 9, /= ~2, to u, e, g{u H x} + (D, l e, g + At(Bf, P) iff (w, a, [et],,,), g I= 50, l e, g t= By(e”, 40) iff (w, ueq,,,, t), g k 50, l e, g + Know(q) l e,g k 0~ A formula q is satis$able iff for all e’, such that (e, e’) E K, e’, g b q, iff for all w* such that w Et w*, (~*,a, t),g /= C,D. that M, e, g + 9. A formula p is valid (written if there exists a structure M, index e, and assignment g, k 4p) if it is satisfied by all such structures, indices, and assignments. language, As mentioned to be a formalization there are true indexicals interact with their environment. of the behavior earlier, our logic is not intended a tool for modeling how indexicals; we see it as a specification of English agents (I, you, now, In English, here, etc.), which refer to aspects of the utterance context no matter where they appear, [ 71 (I myself, you yourself, he himself, and there are quasi-indexicals/quasi-indicators etc.), which are used to report that an agent has an indexical mental state. The behavior indexicals self and now displays characteristics of both categories. When of our primitive indexical “I” self occurs outside like the English and when now occurs outside indexical like in English, but one can imagine quasi-indexicals-there the scope of Know or At, it behaves In the scope of Know on the other hand, self and now behave the scope of Know or By, it behaves are no temporal quasi-indexicals like the English “now”. E L.espt+ance, H.J. Levesque/Art@cial Intelligence 73 (1995) 69-115 83 how a temporal analog of “he himself’ would work. Finally, when self occurs in the scope of By and when now occurs in the scope of At, they behave like pronouns that are bound by the operator; this is similar to the way in which an expression like “the day before” depends on surrounding temporal operators as in “tomorrow, it will be the case that the day before, I wrote this line”. We wanted to keep the logic simple and allow typical assertions about knowledge and action to be expressed concisely. This led to the chosen design. These features come at the cost of a simple relationship to English. But we feel that the logical behavior of our primitive indexicals is easily understood once one realizes it is different from that of any English analogs. This is not to say that formalizing the behavior of English indexicals is uninteresting. In fact, we think that build a model that relates the context our framework can be used as a foundation to and action. We return to this topic in sensitivity of language to that of mental states Section 7.2. 3.4. Constraints To ensure that the semantics adequately models the notions that we are trying to cap- ture (knowledge, historical necessity, etc.), we impose various constraints on semantic structures. When we speak of a valid or satisfiable formula we always mean relative to semantic structures that satisfy these constraints. Here we will simply list the constraints without justification (and minimal explanation) : (1) (2) (3) (4) (5) (6) (7) E A, then ts 5 te. S4 epistemic logic. K must be reflexive and transitive. Linear time logic. 4 must be a strict total order, that is, transitive, connected, and irreflexive. * Start time is before end time. If (d, (w,a,t,),b) Historical necessity is an X5 operator. For all t E 7, zt must be an equivalence relation. Possibilities do not increase over time. If w at2 w* and ti 3 t2, then w zt, w*. Equivalent worlds support the same basic facts and knowledge. If w zt w*, then, letting e = (w,a, t) and e* = (~*,a, t), it must be the case that (a) for any predicate R, @(R, e*) = @(R, e), (b) for any function symbol F, @(F, e*) = @(F, e), (c) for any e’, (e*, e’) E K iff (e, e’) E K. Equivalent worlds support the same actions. (a) If w Et, w*, then A(& (w*,a,t,),t,) if w zt w* and ts -: t, then (b) iff A(d, (w,a,t,),t,), there exists te such that t + t, and A(d, (w,artS),te) if and only if there exists t: such that t 4 t: and A(d, (w*,a,t,), t:). * A relation R is connected if and only if for any x1 and x2. either x1 Rx2 or XI = x1 or xzRxl ; a relation R is irreflexive if and only if for no x is it the case that xRx. 84 Y Lespbance, H.J. Levesque/ArtiJicial Intelligence 73 (1995) 69-115 (8) Persistent memory and awareness exists t, there then t, 3 of actions. time a and ((w,a, ((w,a,t), ti If ti, where t) (~‘,a’, t’, 3 t’)) E K that such tr), (w’,a’, tb)) E K and if dfd, (w,a,t,), then d(d, (w’,a’, tb), t’). See [ 331 for discussion of these constraints. 3.5. Abbreviations To simplify writing conventions. notational formulas in i&X, it is convenient Firstly, we assume the usual definitions def a dual to 0: Osp = -O~cp. Next, using to use certain abbreviations or for V, A, c, 3, #, >, 6, that specifies which agent knows for Does and other operators the operator By, we the given that have yet and 2. We also introduce define a more common version of Know similar definitions proposition; to be introduced (Res, Can, etc.): are assumed Know(B”,q) ~fBy(8”,Know((p)), and similarly for Does, Res, Can, etc. We have also developed a set of definitions that make it easy to state the occurrence of a large class of complex actions. We first define a new syntactic category, that of action expressions. We use the syntactic variable S to represent members of this category. The category is defined by the following BNF rule: terms, which represent It includes action no time and changes nothing, the actions St and 192, and if( rp, 61, &), which represents doing action St if the condition simple actions, (St ; 8,)) which represents the noOp action which the sequential the action composition that consists takes of in rp holds, and in doing action 82 otherwise. We inductively extend Does to take action expression arguments as follows: l Does(noOp, 0’) dzf (8’ = now); l Does((&;&),t9’) dzf 3ui(Does(S,,v:) provided that ui and vz are distinct and do not occur anywhere A 3uk(u: = 19’ A At(vi,Does(&,uk)))), in f?‘, 81, or &; l Does(if(~,SI,&),@) “zf (rpADoes(G1,6*)) V (-9~Does(&,@)). We also introduce the following abbreviations for action expressions: . ak d;f noOp, (~5;6~-‘), if k = 0, if k > 0; l iffhen( q, 8) “zf if(p, 6, noOp); l whi’ek(v’6) def = noop, iffhen(qp, (8; whilek-1 (~,a))), if k = 0, if k > 0. Note that the numbers-k be quantified over. Also the last abbreviation properly capturing an unbounded (E IV) in the above are not part of the language, and so cannot loop”; above is a bounded form of “while form is problematic in the language as it stands. I! Lesptrance, H.J. L.evesque/Art$icial Intelligence 73 (1995) 69-115 85 Let US also define some dynamic-logic-style operators that will be used in our for- malization of ability.9 AfterNee(S, 4p), which is intended to mean “+T must hold after s”, is defined inductively as follows: l Af%erNee( Bd, p) dgf EM~~(Does(~~,v’) > At(u’,p)), where ut is a temporal vari- able that does not occur free in (p; l AfterNec( noOp, p) dzf 40; l AfterNec((&;&),gp) ef AfterNec( 81, AfterNec( 62, cp) ) ; l AfterNec(if(pc,61,&),q) def = (4~~ IJ AfterNM&,qP)) A C-G 3 AfterNM&,pP)). In terms of this, we define PhyPoss( 6)) which is intended to mean that it is “physically possible” for self to do action S next (even though he may not be able to it because he does not know what primitive actions S stands for), and Res( 8, cp), read as “8 results in cp”, which means that S is physically possible and 4p must hold after S: l PhyPoss( 6) dzf yAfterNec( S, False) ; . Res(&p) %if PhyPoss(S) A AfterNec(S,4p); Trne (False) stands for some tautology (contradiction). Let us also define a few additional operators: l Kwhether ( q p) Ef Know(q) v Know; l DoneWhen( 6, sp) dzf 3u~3u:( ut - e - now A At(u~,Does(S,u~) A 4p)), provided that ut and ui are distinct and do not occur anywhere in 5p or S; l SomePast( VP) dzf 3u’(u’ < now A At(u’, cp)), where ut does not occur free in 9; Kwhether( a) means that self knows whether (p holds; DoneWhen( 8, sp) means that means that self has just done S and that cp was true when he started; and SomePast q~ held at some point in the past. 3.6. Ability We base our formalization of ability on that of Moore [40], which in spite of its relative simplicity, does get at the essential connection between the ability of agents to achieve goals and the knowledge they have about relevant actions. It is simpler than his because we do not attempt to handle indefinite iteration (while-loop actions). Moore’s formalization of this case is actually defective because it does not require the agent to know that the action will eventually terminate. We leave this case for future research. Since we are not treating indefinite iteration, we can simply define ability in terms of the other constructs of the logic as follows: l Can(&q) l CanDo( ed) Ef 3ud Know ( ud = ed) A Know (PhyPoss ( fId) ) , where action variable A Know(AfterNec(S,qp)); zf CanDo ud does not occur free in Bd; ’ These definitions are a bit different from the ones given in [ 29,311. The ones given here make the operators behave exactly as their dynamic logic [ 131 analogs. The differences are discussed in [ 331. 86 X Lesptrance, H.J. L.evesque/Artificial Intelligence 73 (1995) 69-1 I5 CanDo( noOp) “Lf True; CanDo(&;&) Ef CanDo AKnow(AfterNec(S1,CanDo(S2))); . 0 . CanDo(if(cp,&,&)) definition says zf (Know(~)ACanDo(&))V(Know(~~)ACanDo(&)). that is able is defined the agent to achieve if he knows what that action that are represented by action The formally Can( S, rp), if and only goal p must hold. CanDo actions-actions action Bd if and only possible of primitive actions descriptions. The second case handles sequentially case handles do 61 and knows of conditional condition po,. holds and can do 81, or knows the goal p by doing action 6, if he can do action 6 and knows that after doing 6, the inductively. The first case takes care of simple terms. It says that self can do a simple that it is physically over the class to arbitrary action the noOp action, which trivially holds. The third self can do (Si ; 82) if and only if he can it he will be able to do 62. The final case takes care that the if and only if he either knows that it does not hold and can do 82. lo is and knows involves quantifying-in (e.g. “send grasping signal to hand”), as opposed for him to do it. Note that the definition actions: self can do if( qc, 61,&) composed actions: that after doing Note that we eliminate Moore’s requirement indexical knowledge. Thus, we require agent know what the action to do it, and that he know afterwards. Mere de re knowledge achieve a goal; we discuss Also, as discussed expressive prerequisites is. or effects temporal is, that he know that if he himself does is neither necessary nor sufficient this further and give a concrete example it, the goal will necessarily that it is physically possible in the simple action case we require that the agent know who he is; instead, that the for himself hold for being able to in Section 4.2. is based on a more advantages when dealing with actions whose times and knowing what time it logic has important involve knowledge of absolute in Section 5, the fact that our account of ability 3.7. Properties of the logic In this subsection, we list some properties of the logic of LIKA that are used in the indeed of the next section. We show that the logic proofs of the robotics applications these properties satisfies in [ 29,331, where we also discuss their significance. The basis of our logic, that is, the part concerned with first-order is standard specialization contexts. This yields (the axiomatization is restricted to prevent non-rigid the following proposition: in [ 391 can be used) with one exception: terms from being substituted logic with equality, the “axiom” of into modal Proposition 3.1 (Specialization). cp, no occurrence of a function symbol gets substituted or By operatol; no occurrence of self gets substituted and no occurrence of now gets substituted k Vv(o 3 fp{ v ++ 19}, provided that 8 is free for v in into the scope of a Know, At, into the scope of Know or By, into the scope of Know or At. is preferable to the one in [ 29.3 I ] as it separates the knowledge prerequisites I” This way of defining Can involving the goal from the rest; this makes it easier to prove results involving complex actions; see 1331 for further discussion. K Lespkance, H.J. Levesque/Art@cial Intelligence 73 (1995) 69-1 I5 87 Knowledge obeys the following principles in our logic: Proposition 3.2. + Know(q+ > ~2) > (Know(qp1) > Know(qp2)). Proposition 3.3. If /= 4p. then + Know ( cp) . Proposition 3.4. + Know( q~) 2 rp. Proposition 3.5. t= Know(q0) > Know(Know(q)). Proposition 3.6. k Vu Know( q~) 3 Know( By). Note that our handling of indexical knowledge affects how the above statements should be read. For instance, Proposition 3.5 says that if the agent knows that Q, then he knows that he himself currently knows that 40; Va( Know( a, p) > Know( a, Know( a, 40) ) ) is not valid. We say that a formula (p is future-blind At operator or the Does predicate outside that can be represented that a future-blind necessary if it is true: if and only formula if and only if 5p contains no occurrences of the the scope of a Know operator except in forms says as SomePast and DoneWhen. The following proposition (i.e., one that does not talk about the future) is historically Proposition 3.7. b q q = q, provided that rp is future-blind. Finally, we will need the following properties of Can: Proposition 3.8. rfb pi > Can(&,(p,), then b Can(Sl,pi) > Can((&;&).rp,). Proposition 3.9. k Can( 8, cp) > Can( S, Know( (p) ). Proposition 3.10. + Can(if((oc,4,~22),rPg) = (Kn~w(rp,) ACan(Sl,(p,)) - V (Know(~q+) ACan(a29P8Dg)). Formalizing a simple robotics domain In the previous (described in detail for the formalization section, we briefly reviewed a theory of indexical knowledge, action, that this theory forms an adequate indexical knowledge prerequisites a robotics domain the theory and prove that a robot is able to achieve certain goals by doing certain facts. We will argue that our framework allows that ignore indexicality. and ability framework or effects. Let us now substantiate within actions provided a much more accurate modeling of these cases than frameworks in [ 331). We claim involving of actions this claim. We will formalize that he knows various 88 E Lesphnce, H.J. Levesque/Artifcial Intelligence 73 (1995) 69-115 As will become clear, the framework we provide does not turn the formalization indexical knowledge prerequisites in developing from an agent; that may have or effects a formalization into a trivial of actions that have a task. Many decisions must still be made from the point of view of how much and what kind of crucial bearing on its adequacy a it requires knowledge take. What we provide on this front is some general advice on what procedure should that we formalize. Our to watch for, as well as the exemplary value of the situations central admonition is that one should be careful not to impose excessive knowledge requirements upon agents in formalizing actions; in particulal; one should merely require indexical knowledge, as opposed to objective knowledge, when that is s@cient. Together with our formalization of the examples, we provide a discussion of how this guideline has been put into practice. for example, decisions as to which parameters in this respect 4.1. The domain Our domain involves a robot, call him ROB, that moves about on a two-dimensional agents in the (primitives interact, but is not to model situations where multiple is the only source of activity repertory of basic actions grid. Since our purpose to present and justify our account of indexical knowledge and action, our formalization will be based on the assumption that the robot domain. We take our robot to have the following he may move forward by one square, he may turn right or left 90”, of his architecture): and if he may sense whether an object is on the square where he is currently positioned there is one, what shape it has, and he may pick up an object from the current square the object he is holding on the current square. It should be clear that in or put down spite of the simplicity of this domain, to a large number of problems faced by real robotic agents. For instance, one can view objects of particular landmarks that there are no physical obstacles being on a square does not prevent robot as standing over the object). Finally, note that the formalization the uncertainty sensing; to real robotics problems. shapes as such landmarks. We assume an object the does not model the effects of actions and acquiring knowledge by in predicting this limitation would have to be addressed the robot from being on it too (one can imagine and the robot can then navigate by recognizing to the robot’s movements; it contains analogs for the framework to be applicable in particular, involved 4.2. Ability to manipulate an object The indexicality of action manifests is that a robot can act upon relative to himself, he need not know either First consider a simple actually positioned where that object by making various assumptions, (manipulate) itself in many ways in this domain. One key way is if he knows where the object’s absolute position or his own. instance of this where the robot wants to pick up an object and is that object an object is. Relevant aspects of the domain are formalized ” most of which have to do with the types of action arc essentially non-logical (they are not part of the ” Assumptions logic proper). In reasoning within a theory, we only deal with semantic structures where the assumptions come out true. Note that due to this, assumptions not only hold at time now, but at all times, and it is common knowl- edge I 181 that this is the case (i.e., everyone knows it, and so on). in a theory of a particular domain that everyone knows it, everyone knows axioms Y Lesptfrance, H.J. L.evesque/ArtiBcial Intelligence 73 (1995) 69-115 89 involved. The following assumption specifies the effects of the action PICKUP: Assumption 4.1 (Effects of PICKUP). ~VJX(~BJECT(X) APOS(X) =~~~~A+HoLDING(Y) 3 R~s ( PICKUP, HOLDING(X) ) ) . Definition 4.2. here Ef Pos( self) It says that if some object x is positioned where the agent currently is and he is not currently holding anything, then his doing the action PICKUP next will result in his holding X. ** This means that under these conditions, it is both physically possible for him to do PICKUP, and his doing so necessarily results in his holding the object. In fact, we assume that all basic actions are always possible. The view adopted is that such actions characterize essentially internal events which may have various external effects depending on the circumstances. l3 We also assume that agents always know how to do basic actions, that is, know what primitive actions they denote. This is formalized as follows: Assumption 4.3 (Basic actions are known). + zldKnow(d = IP), where ed E {PICKUP, PUTDOWN, FORWARD, RIGHT, LEFT, SENSE}. We also make various frame assumptions for PICKUP (i.e., assumptions about what does not change as a result of the action). The following says that when the agent does PICKUP, the positions of all things must remain unchanged: Assumption 4.4 (PICKUP does not affect ~0s). + VXVJI(POS(X) = p XI A&~N~IZ(PICKUP,POS(X) =p)). We also assume that PICKUP does not affect the orientation of anything and that unheld objects that are not where the agent is remain unheld after the action; these assumptions are specified analogously to the one above and we omit them here. Now clearly, just having de re knowledge of some object is insufficient for being able to pick it up; something must be known about the object’s position. Perhaps there are domains where as soon as an agent knows which object is involved, he would know I2 Even though we are specifically talking about the agent and time of the context in the above, the attribution in fact applies to all agents and times, since it is assumed that the assertion is valid (i.e., satisfied at all (p, then b VaVtBy(a, At( t, (p) )). If several agents were indices), and it is a property of our logic that if k involved, we might have to formalize the domain differently. I3 For instance, Assumption 4.1 only specifies what happens when PICKUP is done under the conditions stated; what its effects are in other circumstances is not addressed. 90 I! Lespe’runce, H.J. Levesyue/Artijicial Intelligence 73 (1995) 69-115 the agent determines thus one might develop a formalization where to it, and 4.5 turns out to be valid. However, how to get to it (or how to find out) ; in such a case, one might want all aspects of the process by which and navigates in Proposition agents often fail to know where objects are. Our formalization the situation about holds: to suppress relative position the formula this is clearly not the usual case; this; it models to know So the following proposition to account to manipulate. at a level of detail sufficient the position of objects for what agents need the object’s they want reflects Proposition 4.5. p EIXKIIOW( OBJECT( X) ) 3 Can( PICKUP, 3x HOLDING(X) ). Proof. Consider a structure where e, g + A POS(X) KIIOW( ~X(OBJECT(X) some object, but does not know that any object e’, g sumptions Know(AfterNec(PICKUP, 3x HOLDING(X))) = here)), that there be an /= Does(PICKUP, 4.1 and 4.3. is here. Let is easy to see It ive), i.e., the agent does not know i.e., K~OW(~BJECT(X)) the agent is here, this object index e’ be such but e, g 1 the agent knows of in fact, does not know (e,e’) E K and is consistent with As- that e, g h implies that t) A lAt( t, 3x HOLDING(X)). This that the above that zt must be reflex- that after he does PICKUP, he must be holding the fact (using something. So the agent is not able to achieve the goal by doing PICKUP and the structure falsifies the formula. 0 actions involving grasping, arm motions is not sufficient in terms of lower-level for being able to act upon In a discussion of the robot action of “putting a block on another block”, Moore [ 411 that knowing what blocks are involved may not be enough and suggests to the and ungrasping. Now, knowledge of an object’s absolute it (and nor is it necessary). One is and therefore may not reflects this to the simple at some position is p and that he is not holding some object by that he may not know that he is recognizes that the action be defined objects’ positions, position may not know what one’s absolute position and orientation be able to deduce where the object fact. For instance, one can prove situation discussed p and knows anything, doing at p. to oneself. Our formalization the following proposition with respect the action PICKUP. The reason for this is simply that the absolute position of some object he still might not be able the goal of holding It says that even if the agent is currently to achieve is relative earlier. Proposition 4.6. ~33p(here=pAKnow(3x(O~~~~~(x) APOS(X) =p) ~\+HoLDING(Y))) 3 Can(PICKUP, 3x HOLDING(X) ) The proof is similar On the other hand, we can also prove to that of the previous proposition; it appears in [ 291. is and that he is not holding anything, that if the agent knows that some object then he must be able is to where he currently achieve the goal of holding some object by doing PICKUP: E L.esp&ance. H.J. L.evesque/Art#cial Intelligence 73 (1995) 69-115 91 Proposition 4.7. b K~ow( 3x( OBJECT( X) A POS( X) = here) A 73~ HOLDING(Y) ) 1 CaII( PICKUP, 3x HOLDING(X) ) . Proof. Suppose e,g + 3dKnow(d reasoning, Assumption 4.1 implies that that the antecedent holds at e and g. By Assumption = PICKUP), i.e., the agent knows what the action 4.3, we have that is. By quantifier k %(OBJECT(X) A POS(X) = here) A 13~ HOLDING(Y) 1 PhyPoss (PICKUP) A 3x AfterNec( PmwP, HOLDING(X) ) . to produce ), i.e., the agent knows 3.3 and 3.2 and the supposition, So by Propositions Know(PhyPoss(PICKUP) as well as that e, g b Know( 3xAfterNec(PICKUP, HOLDING(X))). ward (provided be the case knows after doing Can( PICKUP, &HOLDING(X) it must be the case that e, g b is physically possible, It is straightfor- that k 3vAfterNec( ed, 9) > AfterNec( Bd, 3vsp) 3.3 and 3.2, it must the agent e,g + that e, g k Know(AfterNec(PICKUP, the goal must hold. Thus by in ed). Thus by propositions that v does not occur a semantic proof &HOLDING(X) that the action the definition, the action, ) ), i.e., ) . 0 The agent can be totally and still be able to achieve ignorant of what his (and the goal. the object’s) absolute position is Note that proposition 4.7 makes no requirement too weak and an easy fix would is involved. But it is possible such de re knowledge. For example, up holding be the same as the one may appear which object requiring that the agent knows where he was before doing some object then he can by doing action PICKUP achieve at his own position before at his position is currently that was at his position before to strengthen that the object involve assuming that the agent ends the action. This that the agent knows the above proposition without the fact that was that and that he is not currently holding anything, it says that if the agent knows the following proposition some object captures the goal of holding some object that was that after the action, he would be holding the PICKUP he has just done. the action. Specifically, Proposition 4.8. k KIIOW(~X(~BJECT(X) A POS(X) = here) A -3y HOLDING(~)) 2 C~~(PICKUP,~X(HOLDING(X) A DoneWhen( PICKUP,OBJECT(X) A POS(X) = here))). The proof is similar to that of proposition 4.7; it appears further to require uniqueness. But it should be clear involved in the initial and goal situations, without in [29]. This result can be the that it be known that identifying requiring strengthened objects what objects they are, is not a trivial matter. Before moving on, let’s examine another variant of this situation. By the necessitation rule for By and universal generalization, is that any agent who knows an immediate consequence that there is an object where he himself of Proposition 4.7 is and that he is 92 Y LuspPrance, H.J. L.evesque/Artijiciul Intelligence 73 (1995) 69-l 15 the goal of holding something by doing not holding anything must be able to achieve PICKUP, that is, k VuBy(a, qp), where spp is the formula of proposition 4.7. However, if instead of this, an agent a merely knows that there is an object where a is, it no longer follows that he is able to achieve the goal. Proposition 4.9. ~~aBy(a,Know(3x(O~~~~~(x) APOS(X) =POS(U)) ~\~~~HoLDING(Y)) 1 Can(PICKUP, 3x HOLDING(X))). The reason why this is not valid is simply that a may not know that he is a. This shows the goal will necessarily for ability. (in some that knowing of oneself hold afterwards, as Moore’s formalization One can similarly models of Proposition 4.7, the agent does not have such knowledge). is not sufficient either that if one does the action, that such de re knowledge of ability requires, is not necessary (de re) show 4.3. Going to a relative position Let us now look at navigation. Since seems that given any relative position, formally it the robot should be able to go there. Let us show that this is the case. The effects of action FORWARD are specified as follows: there are no obstacles in our robot’s world, Assumption 4.10 (Effects of FORWARD). b ‘dp Vo( here = p A selfori = o 3 Res(FORWARD,here =p + (1,0) x ROT(O))). Definition 4.11. selfori %’ ON ( self). Definition 4.12. ROT ( eb ) “zf sin 0: cos e; _ sine:, coseb ’ > Assumption 4.13. + V&O < ORI < 25-J. Assumption 4.14. b VX(HOLDING(X) 1 POS(X) = here). Assumption 4.10 says that as a result of doing FORWARD, the direction he is facing; further along to the absolute square the agent with respect matrix associated with angle 0;. Assumption are where he is. We also need the following FORWARD, must Appendix A.l), unchanged; leave his orientation that it must leave selfori the agent moves one of the orientation represents frame of reference and ROT( t9:) is the rotation 4.14 says that objects held by the agent that the agent’s doing frame assumptions: in that are not held by him (a formal version of this appears unchanged the position of objects and that it has no effect on whether or not an object is being held. L Lesphance, H.J. Levesque/Artijcial Intelligence 73 (1995) 69-115 93 Notice that we use expressions from the language of arithmetic and analytic geometry in formalizing this domain. We make the following assumptions regarding these forms: Assumption 4.15. All the constants and function symbols from arithmetic and analytic geometry used are rigid (i.e., have the same denotation at every index). Assumption 4.16. All facts from arithmetic and analytic geometry are valid. I4 A consequence of Assumption 4.15 is that the restrictions on specialization do not apply for arithmetic or analytic geometry expressions: Proposition 4.17. + Vusp 3 p{ v H 19}, provided that 8 is free for v in q and all constants and function symbols 0 contains are arithmetic or from analytic geometry. Finally, let us add a few more definitions: Definition 4.18. RPOSTOAPOS( 6;) !Ef (here + e; x ROT( Sdf0I-i) ) . Definition 4.19. RORITOAORI( 0;) %f MODz,( Sdfori + 6;). Definition 4.20. MovEDToBY($,, @,, S) Ef 3ub3vL (DuneWhen( 6, va = RROSTOAPOS( 8;) A U: = ROR~I’OAORQ 6:) ) Ahere=vpAselfori=vb), where t..$ and d, do not occur in t9$ @,, or S. RROSTOAPOS is a function that converts a relative position into an absolute one; similarly, ROR~TOAORI converts a relative orientation to an absolute one. MOVEDTOBY ($, @,, S) means that the agent has just done action 6 and that by doing so, he has moved to the square that was at relative position 0; (before the action) and has turned towards the direction that was relative orientation 0: (before the action). Now, let us discuss what these assumptions imply with respect to the ability of the robot to get to a given relative position. We start with a simple case: that the robot is able to go to the square directly in front of him (i.e., to relative position (1,O)) by doing FORWARD. In fact, the following proposition says something slightly stronger: that by doing FORWARD, one is able to achieve the goal of being on the square that was at relative position (1,O) before the action and of having one’s orientation remain unchanged. I4 This is essentially a placeholder for an axiomatization of arithmetic and analytic geometry; there should be no problem coming up with one that gives us all the facts we need even though it would necessarily be incomplete. 94 I! laspkmnce, H.J. L.evesyue/Artijiciul Intelligence 73 (1995) 69-l 15 Proposition 4.21. t= Can( FORWARD, MOVEDTOBY ( ( 1 , 0) , 0, FORWARD) ). A proof of this proposition This result can then be extended is in Appendix A. 1. to deal with squares arbitrarily (i.e., relative positions in sequence, one is able to achieve far away along the (n, 0), where n E N). We can show the goal of being the action and of having one’s that was at relative position (n,O) before is facing row that the agent that by doing FORWARD II times on the square orientation remain unchanged: Proposition 4.22. For all n E IV, + Can( FORWARD”, MOVEDTOBY ( (n, 0)) 0, FORWARD”) ) . The proof is in Appendix A.2. Now, let us look at the general case. The action of going goes as follows: to go to relative position to go to relative position that is, to a position behind oneself, one turns left twice (i.e., defined below. The definition II E N, one simply goes forward n times; negative then goes return one first goes to (n,O), 90”, then one goes to (m, 0), and then finally, one turns left three times (i.e., 270”) return to relative position to one’s original orientation; that is, to the right position on the x axis, then one turns finally, to go to an arbitrary to the original orientation. and then one turns relative position to a relative position is (n,O), where (m,O), where m is a IgO”), left twice again, so as to (n, m), left to integer, (-m,O), Definition 4.23. GORPOS ( (n, 0) ) d:f FORWARD”, where II E N; GGRPGS((m,O)) ~fLE~2;GGRPGS((-m,0));LEFT”, where m E Z, m < 0; GORPOS((n,m)) dzf GORPOS((n,O));LE~;GORPOS((m,O));LEF+, where n,m E Z, m # 0. We also need provide a specification for the action of turning left. Its effects are formalized as follows: Assumption 4.24 (Effects of LEFT). This says that as a result of doing LEFT, the agent’s orientation is rotated by 90”. that doing LEFT does not affect the position of We also make the frame assumptions Y Lespdrance. H.J. kvesque/Artijicial Intelligence 73 (1995) 69-115 9s anything nor whether any object is being held. Given this, it is straightforward to prove ability results for LEFT similar to the ones previously obtained for FORWARD. Let us only state our main result: Proposition 4.25. For all n E N, i= C~II(LE~,MOVEDTOBY((O,O), MOD~?,($~?~),LEFT”)). This says that by doing LEFT n times in sequence, the agent is able to achieve the goal of having his orientation rotated by n times 90” to the left of what it was before the action and of having his position remain unchanged. Then more generally, one can prove the following proposition about GORPOS; it says that one can, by doing GORPOS( (n, m)), achieve the goal of being on the square that was at relative position (n, m) before the action and of having one’s orientation remain unchanged: Proposition 4.26. For all n, m E Z, ~C~~(GORPOS((~,~)),MOVEDTOBY((~,~),O,GORPOS((~,~)))). A proof of this proposition is in Appendix A.3. Let us also point out that our formalization captures the fact that an agent need not be able to go to a given absolute position, for example, in a situation where he does not know where he is. The following proposition formalizes a simple instance of this; it states that the agent need not be able to go to absolute position p by doing FORWARD, even if p happens to be the position of the square directly in front of him. Proposition 4.27. F Vp(p = RPOsTOAPOS( (1,0)) 3 Can(FORWARD, here = p)). A common reason for going somewhere is that one wants to get hold of or manipulate something that is there. It is straightforward to extend the results obtained so far and show that knowing the relative position of an object is sufficient for being able to go and manipulate it. Let RPOS(@) stand for the position of 8’ relative to self: Definition 4.28. RPOS( 6’) dzf ( (POS( 0’) - here) x ROT( -selfori) ) . In [29], we prove the following result: Proposition 4.29. For all n E N, i= I~ow( 3x( OBJECT( X) A EPOS( X) = (n, 0)) A -3~ HOLDING(Y) ) 1 Can( (FORWARD”; PICKUP), 3x HOLDING(X) ) . This says that if the robot knows that there is an object at position (n,O) relative to himself, that is, on the n th square directly in front of him, and knows that he is not holding anything, then he is able to achieve the goal of holding some object by doing 96 K Lespbrance, H.J. L.evesque/Artifcial Intelligence 73 (1995) 69-115 that whether an entity FORWARD n times frame assumption omit details as no new techniques are required. and show that by doing GORPOS( (R m)) hold something, provided and that he is not holding anything. followed by PICKUP. Note that this result requires is an object the additional It should be easy to generalize is not affected by any action. We this result followed by PICKUP, the agent can come to (n, m) that he knows that there is an object at relative position 4.4. Perception We will come back to this issue of what one must know in order to be able to go and it too a limited an object, but now let’s have a look at perception. As observed earlier, the action SENSE constitutes I5 In our domain, manipulate yields indexical knowledge. form of perception. We formalize the effects of SENSE as follows: Assumption 4.30. b b’~Ra( SENSE, Kwhether ( 3x( OBJECT( X) A POS ( X) = here A -HOLDING(X) AOFSHAPE(X,S)))). says This assumption that doing SENSE results in the agent knowing whether an unheld object of a given shape is present at his current position that there can only be a single object resting on a square and that the robot’s sensors are this focussed on this object and not on any object assumption trivially that by doing SENSE, the agent can find out whether there is an unheld object where he is and, if there is one, what its shape is. and the fact that basic actions are assumed that he might be holding). From (we are assuming to be known, it follows We also make the following frame assumptions: and the positions of everything unchanged; that the agent’s doing SENSE leaves that it does not change whether is held or not; and finally that no action affects the shape of objects. his orientation an object 4.5. Ability to go where an object is in order Let’s now go back to the issue of what one must know and act upon an object. We said that knowing sufficient of objects are. More typically, area until they find the object. The following proposition it says that by sensing and then scanning an unheld object is present, is an unheld object where he is, or knowing k squares behind him involves to be able to go the relative position of the object was for this. But in real life, agents rarely know exactly what the relative positions they know roughly where objects are and scan the general formalizes an instance of this; that that there that there are no unheld objects on the the one he is currently on). The SCAN~( cp) action true. forward, up to k squares, until he senses (up to k times) until (p becomes the goal of either knowing the robot can achieve forward and sensing repetitively moving (including [lo-121 has done I5 Davis however, he does not address interesting work on the topic of reasoning the fact that the knowledge obtained about knowledge and perception; from perception is indexical knowledge. I: L.esp&ance, H.J. Levesque/Am@ial Intelligence 73 (1995) 69-115 97 Definition 4.31. SCANk ( p) !Zf whilek ( T(P, FORWARD; SENSE). Proposition 4.32. For all k E N, k Can((SENSE;SCAN&(3x(OBJUJT(x) KIIOW(~X(~BJECJ(X) A POS(x) = here A -HOLDING(X)))), A POS(X) = here A -HOLDING(X))) v Know(Gln(-k < n < Or\ ~X(OBJECT(X) A RPOS(X) = (n,O) A ~HoLDING(x))))). Appendix A.4 contains a proof of this proposition. So it is quite clear that ability to act upon an object does not require knowing its relative position. But then what is required? It seems that the best we can say is that the agent must know of some procedure that will take him to where the object is. This creates problems in formalizing the knowledge prerequisites of certain high-level parametrized actions, for example, the action of “going to the position of an object 6” GOWHERE( It would be inappropriate to treat this action as a primitive because we want to model how knowledge enables action at a more detailed level. The other way of dealing with such an action within our (and Moore’s) framework involves defining it in terms of lower-level actions that are parametrized with the information that must actually be known in order to be able to do the high-level action. This allows knowledge prerequisites to be enforced by the requirement that one know which primitive action to do next. But for actions like GOWHERE( it is not clear how this can be put into practice. However, notice that GOWHERE is a strange kind of action, in that it appears to refer to anything that would achieve the goal that the agent be where B is; it behaves as much as a goal as like an action. Perhaps we should rule out the intro- duction of such actions, but instead provide an action-less version of the Can operator: CanAch(q) would mean that self is somehow able to achieve 9. Then, we may use CanAch(pos( 8) = here A rp) instead of something like Can( c+oWHERE( 0) ,5p). I6 A coarse “syntactic” way of formalizing CanAch goes as follows: e, g + CanA&( (p) iff there exists an action expression S such that e, g b Can(G,4p). A more general and robust approach is proposed in [ 341. 4.6. Map navigation A general account of ability must be based on a formalism that handles both indexical knowledge and objective knowledge, as well as knowledge that relates the agent’s perspective to the objective frame of reference, what we might call orienting knowledge. To see why this is the case, let us look at one last area of robotics, that of navigation with the help of a map. Maps (of the usual kind) contain objective information. To use this information, say to figure out how to get to a destination, an agent must first orient himself-find out where he is on the map, that is, what his absolute position is, and find out what his absolute orientation is. If he does not already have this information, I6 This assumes that it is known that B refers to the same entity before and after the action; the assumption can be dispensed with by referring to the denotation of 8 prior to the action as illustrated in Section 4.2. 98 K Lespdrance, H.J. Levesyue/Artijcial intelligence 73 (I 995) 69-l 15 he might try to obtain map. it by searching his environment for landmarks represented on the the map) ; then by scanning Our simple domain provides instances of this if we treat objects of various shapes as landmarks. Consider a variant of the scanning example of the previous section, where the robot knows what the absolute position of the unique object of shape s is (from the robot should be able to get having examined is not in the into a state where he either knows where he is, or knows region scanned. The following proposition correct. It says that the unique object with shape s is at absolute position p and that if the robot knows forward, up to k squares, until he that it is not held, then by sensing and then scanning the goal of either senses that knowing the one the unheld object of shape s is on none of the k squares behind him (including he is currently on). that he is at absolute position p (i.e., knowing where he is), or knowing that an unheld object with shape s is present, he can achieve shows that this is essentially that the object forward, Proposition 4.33. For all k E N, k VpVs( K~ow(~~(~BJEcT(~)~\Pos(~)= pA -HOLDING( VJJ(OFSHAPE(V,S) -y=x)))> Can( (SENSE;SCAN~(~X(OBJECT(X)APOS(X)=~~~~A~HOLDING(~)AOFSHAPE(.~,~)))), Know( here = p) V Know(ySn(-k < II < OA ~~(OBJECT(X) A RPOS(X)= (n,O)A ~HOLDING(X) A OFSHAPE(X,S)))))). closely The proof is omitted as it is similar in [ 291. find out what its absolute orientation related result appears an agent might Similarly, to that of Proposition 4.32; a compete proof of a for a single on the map. Once a pair of landmarks whose orientation or perhaps by searching are represented orientation he can then use the information Orienting knowledge, objective notions, allows objective knowledge and vice versa. knowledge to be represented. is, knowledge that relative to one another landmark whose faces have distinctive for is by searching is indicated on the map, shapes that the agent has found out where he is and what his to any absolute position; in the map to figure out how to go where he wants. I7 to are related notions about how indexical to be mapped into indexical knowledge It is a key feature of our formalism that it allows all these kinds of is, he will know which relative position corresponds 5. Other applications The notion of indexical knowledge and the important by looking at robotics examples involve actions taking place in physical space; easily understood These relative bound to his point of view to that of physical in physical space. But this space. This might suggest is not the case; as we did in the previous the agent has knowledge role it plays in action are most section. that is is rather that the notion is really the notion I7 Many of these characteristics of the map navigation problem were pointed out by Israel I22 I. E Lmpdrance, H.J. Levesque/Art@cial Intelligence 73 (1995) 69-115 99 abstract. It is useful as long as the domain involves agents that operate in some kind of space, from some kind of point of view into that space. In [29], two non-robotics domains that involve very abstract notions of space are examined. The first involves an agent making a phone call. The phone system is naturally viewed as a kind of space structured into various kinds of regions and sub-regions, An agent can be quite ignorant of what characterizes his absolute position in that space, things like area code, country code, etc. This domain was formalized and it was proven that an agent is able to establish a connection to some phone if he knows what its number is and either knows that it is in his own area, or knows that it is not and knows what the phone’s area code is (international and same-area long distance calls were ignored). The other domain examined is that of data structure search and manipulation. At first, it might seem strange to look at this in terms of an agent located in some kind of space and having knowledge that is relative to his point of view into that space, yet this seems very much the outlook taken by many algorithms; one talks about following pointers and searching graphs. An example involving the heapsort algorithm [4] was partly formalized. We sketched a proof of the fact that an agent is able to heapify a tree consisting of a new node installed as root of two existing heaps by sifting the new node down the tree until its key is at least as large as that if its sons. l8 Note that the “space” involved in this example has a very different topology from that of physical space. There is as much of a need for keeping track of one’s place in time as of one’s place in space. In the spatial examples of Section 4, we described cases where the agent is required to know his absolute position, other cases where he need only know where something is relative to himself, that is, know its relative position, and still other cases where neither kind of knowledge is needed-the agent needs only know a navigation procedure that gets him where he wants. The same kind of distinctions apply to situations where temporal knowledge is required for ability. Depending on the case, the agent might need to: ( 1) know what time it is, e.g., an agent that needs to lock up a room at a specific time, say 5 p.m., and does so by checking a clock until the time comes, (2) know how much time some process needs to go on, e.g., an agent who soft-boils an egg (i.e., cooks it without having the yolk solidify) using a timer, (3) monitor the environment for a condition indicating that a process has gone on long enough, e.g., an agent who wants to fry a fish filet without overcooking it and who achieves this by frying it until its flesh is no longer translucent. In [29], a formalization of the first two examples is sketched. The third example does not require time to be explicitly represented; it can be formalized much like the scanning example of Section 4.5. In [ 321, we also formalize an example of a common temporal reasoning/planning problem that can only be handled in a formalism that includes both indexical and non-indexical concepts and supports reasoning using both. The example involves an agent that does not initially know what time it is; he must keep track of time in relative terms (using a timer), but later convert this indexical knowledge into absolute I8 A heap is a binary tree that satisfies the “heap property”, that is, where the key stored at any node is greater or equal to the ones stored at its left and right sons (if they exist). 100 E Lesptrance, H.J. Levesque/Artifcial Intelligence 73 (1995) 69-115 knowledge was found satisfactory (by finding out what time it is) for communication that the framework proposed provides formalization in every one of these cases. the necessary to another agent. I9 It a tools for producing 6. The need for distinguishing between indexical and objective knowledge To someone indexical knowledge depends on the context, may appear as little more some facts as holding “now”, accustomed (context-sensitivity) for the passage of time would be high, if implemented to the objective point of view of science or mathematics, than an artifact of natural is often convenient, indexicality language. One may thus claim that while using indexical descriptions in practice, wishing representations indexical have to adjust the representations if an agent’s knowledge base describes time step, it should describe [ 171 points out that the cost of adjusting a knowledge base that contains references He proposes its representations. Let us discuss can always be understood objectively. One reason for that this claim were true has to do with the fact that the semantic content of so if the context changes you may to keep the semantic content unchanged. For instance, then at the next these facts as holding “one time step before now”. *’ Haas time indexical in the obvious way. in edge. relative to knowing that if one knows who one is and knows what time it is (remember then anything de re knowledge an indexical way is also known that an agent always knows who he is and what time it is? [ 321 for a more detailed (see that they can use in the way a to objective knowl- are treated as is here amounts time. Given this, it is clear that we are taking that one knows in to assume the claim that indexical knowledge can be reduced terms and formulas that something to an agent and a time; for example, knowing that a robot use its internal clock to eliminate in an objective way. *’ But is it reasonable to require knowing a standard name), the temporal part of this question is at one’s position at the current all occurrences of “now” In our semantics for knowledge, Let’s consider that something indexical First, humans do not have internal clocks discussion). robot can, and they do not always know what time it is. A system with humans will need starting). Even the internal clocks of robots could be guaranteed to simple robotics contexts, to be accurate. if we limit ourselves this (e.g. to model that is interacting is just it seems unlikely that In such cases, Haas’s to remind a user that his meeting roasting, that announces in the oven, setting track of the roasting it is ready. But the agent the agent has to set a turkey so that someone else can take it out when the time at least every 30 minutes. The plan we consider to roast in the oven and leave a message saying when the turkey (who sets the turkey timer on the stove and the involves putting to the radio until the time is announced while the time the turkey started roasting, I9 Specifically, started to roast) does not initially know what time it is and only has access a radio station turkey keeping and leaving a message 2o Subramanian situation calculus *t E.g., if at 10 a.m. Dec. 8, 1991, Rob the robot knows that there is currently a pop can one meter in front of him, and also knows who he is and what time it is, then he must know of Rob and of 10 a.m. Dec. 8, 1991 that there is a pop can one meter in front of that person at that time (de re). [ 521 prove that such a transformation time with the timer, and finally calculating the timer to one hour, then listening and Woodfill framework. is truth-preserving within to the one-hour their indexical to that effect. IT L.espt+ance, H.J. Levesque/Artijicial Intelligence 73 (1995) 69-115 101 scheme leads to indexical information being misrepresented. Moreover, Haas’s robot cannot even represent the fact that his internal clock is incorrect; it could not monitor for this and plan to get its clock reset when appropriate. Also, for very simple (e.g. insect-like) robots, the cost of fitting them with internal clocks and setting them may be too high. Finally, as Haas recognizes, it is not at all clear that the cost of updating a temporal knowledge base that contains indexicals need be prohibitive; for example, if all occurrences of “now” are replaced by a new constant and the fact that this new constant is equal to “now” is added, then only this single assertion need be updated as time passes. Work on reactive agent architectures supplies other reasons for wanting a formalism that can represent indexical knowledge. As pointed out by Agre and Chapman [ 21, the world can change in unexpected ways and reasoning about change can be very costly; in some cases it is better to rely on perception to get fresh information at every time step rather than try to update a representation of the world; in such cases, the problem of updating indexical representations does not arise. And as Rosenschein and Kaelbling [48] have shown, it is legitimate to ascribe knowledge to agents even when they have no explicit representation of this knowledge. In such cases, one needs a formalism that distinguishes between indexical and objective knowledge just to accurately model the agent’s thinking. The output of the agent’s perception module says nothing about time, and even if the agent has a correct internal clock, he may have no need to time-stamp his knowledge, We want a formalism that makes the distinctions required to model this. Let us briefly discuss the other half of the above question, that is, whether agents need always know who they are (know a standard name for themselves). It is tempting to dismiss the usual amnesia examples as mere philosophical curiosities. But if we think of very simple agents, say insects; it would not seem all that unusual to have them not know who they are; in fact, one is hard pressed to come up with good reasons for them to need to know who they are. One can also imagine mass produced computers or robots that do not have their name etched in memory at the factory or even entered at boot-time. One might also look at processes running in a computer as agents. Grove [ 141 and Grove and Halpem [ 151 describe cases in the area of distributed systems and communication protocols where one does not want to assume that agents know who they are. One of their examples involves a set of processes that are running a leader-election protocol (to select one of them to play some special role later on) ; the processes are anonymous, they do not have any unique identifier as part of their state and they are all running the same program; they do not know who they are. 7. Conclusion Agents act upon and perceive the world from a particular perspective. It is important to recognize this relativity to perspective if one is not to be overly demanding in specifying what they need to know in order to be able to achieve goals through action. In many they need not cases (especially simple interactions with their physical environment) know much about their objective situation; what they need is indexid knowledge, knowledge about how they are related to things in their environment or to events in 102 YI LespPrance, H.J. Levesque/Artijiciai Intelligence 73 (1995) 69-115 their history. And perception yields just the kind of indexical knowledge Previous formal accounts of the ability of agents they fail to properly specify [ 40,411 and Morgenstern requirements such as that of Moore end up imposing knowledge ability; In this work, we have developed a formal solution model the indexical knowledge prerequisites how the theory can be used to formalize a robot and its environment. that are neither necessary nor sufficient [ 42,431, have ignored that is needed. to achieve goals by doing actions, this, and thus for and effects of actions. to the problem of how one should and effects of action. We have also shown types of interaction between several common the knowledge prerequisites 7.1. Contributions Let us review the ways between in which our theory improves over previous in an indexical way without knowing indexical and objective knowledge: and action. Our account of knowledge accounts of formally it allows an agent it in any kind of objective to indexical into indexical temporally Its simple modal syntax provides succinct ways of expressing most to be mapped time, in particular, about how objective notions are related thus allowing objective knowledge fully handles in particular, attributions of indexical knowledge. is a simple and natural extension of standard possible-world It retains what is perhaps the most attractive Its model-theoretic semantic schemes feature of the distinction between knowledge the relationship captures to know something way and vice-versa. Knowledge notions can be expressed, knowledge and vice-versa. Also, the account indexical knowledge. matters of interest, semantics for the logic of knowledge. possible-world relation and important properties of the notion of the possible-world inherits omniscience” their knowledge, of the requirement indexical and temporally limitations problem (i.e., agents are taken absolute knowledge. the correspondence that knowledge be persistent semantics: some between constraints on the accessibility formalized. On the other hand, approach, in particular, it also the “logical of to know all the logical consequences includes a formalization that works well with both temporally as well as all validities). Finally, our account over The temporal subset of our logic it with the formula KIIOW( allows quantification times between indexically into epistemic of temporal knowledge; can be made. Relations is simple and very expressive. Both objective and specified times can also be expressed. The fact that terms denoting so one contexts, for example, one can ex- that Paul knows what time it is without John himself now = t) ) The occurrence several agents can these cases). in- of actions (unbounded while-loops). More generally, our account of (even in that di- JOHN, 3tKnow(PAur_, and actions though our account of ability does not deal with one cannot express is an important the occurrence processes, limitation: involving [29,33] temporal specified assertions indexical and objectively times are included can make very weak ascriptions press the claim that John knows knowing of concurrent be expressed There volving the logic though rection) enumerable. actions, continuous (even is one significant indefinite iteration the set of properties ; in fact, we do not know whether identified is limited by the fact that we have not identified a full axiomatization the set of valid formulas is recursively step in E Lesphance, H.J. Levesque/Art$icial Intelligence 73 (1995) 69-115 103 Our formalization of ability improves over previous accounts in several ways. Firstly, it does not require an agent to know who he is in order to be able to achieve a goal by doing an action; all references to the agent within the knowledge required are indexical references. For instance, in the simple action case we require the agent to know that it is physically possible for himself to do the action and that if he himself does the action, the goal will necessarily hold afterwards (as well as knowing what the action is). De re knowledge of oneself is neither necessary nor sufficient for the agent to be able to achieve the goal (a concrete example was given in Section 4.2). Situations where an agent does not know who he is (in the sense of not knowing an objective standard name for himself) are perhaps unusual, but our theory could not claim to reflect a real understanding of indexicality if it did not deal with such cases. Secondly, our account of ability is based on a very expressive temporal logic. We can thus handle prerequisites or effects of actions that involve knowledge of absolute times and knowing what time it is, as well as many cases of actions that refer to times (e.g., the action of “locking up at 5 p.m.“, discussed in Section 5). This would also make it easier to extend the account to handle more complex actions than are currently treated, for instance concurrent actions, actions involving several agents, and actions that refer to time in very general ways. Finally, the logic on which the account of ability is based includes an adequate treatment of indexical knowledge in general; as the applications in Sections 4 and 5 show, this allows a more accurate specification of the knowledge prerequisites and effects of actions. On the other hand, our account of ability suffers from various limitations. The class of actions handled is quite restricted. Moreover, the notion modeled is extremely idealized: high it requires that the action absolutely guarantee that the goal will be achieved-a probability of success is not enough. We have shown how the framework can be used to model various common types of interaction between a robot and its environment. These examples show how actions can be formalized so as to avoid making excessive requirements upon the knowl- edge of agents-so that only indexical knowledge, as opposed to objective knowl- edge, is required when that is sufficient. Our formalization accounts for the facts that perception yields indexical knowledge, and that ability to act upon an object does not require knowing which object is involved or what its absolute position is. It was also shown how indexical knowledge and objective knowledge can be related in the framework, to deal with the use of maps for navigation. Many representa- tional issues that have general relevance to the formalization of actions with indexi- cal knowledge prerequisites or effects were discussed. Applications of the theory in other domains, in particular, ones that involve temporal knowledge, were also briefly discussed. These applications provide evidence to that effect that the distinction be- tween indexical and objective knowledge supported by our framework has substan- tial practical value and that it cannot be done justice within existing accounts of ability. 104 K LespPrance. H.J. L.evesque/Art@ial Intelligence 73 (1995) 69-115 7.2. Directions for future research that are not affected by them. Recently, Reiter A major difficulty in producing that is, providing a usable framework a way of specifying about action for reasoning is that does not require [47] has the situation calculus and Scherl and actions to the frame problem within all properties the frame problem, enumerating proposed a solution Levesque producing reasoning initial situation. We are currently of the situation calculus so as to incorporate [49] have extended actions. The approach about what facts hold in a situation reformulating this solution As mentioned earlier, our formalization to deal with knowledge allows a form of regression to reasoning our framework this approach of ability has many to be used to the frame problem and knowledge- to reduce about what facts held in an into an extended version [ 501. It should be indefinite involving multiple agents. [ 341 handle some of these cases, agents in limitations. [42,43] imagine involving interesting the indexicality and concurrency, as well as plans types of actions, point of view, since that we are cooperating such as those involving to handle more complex as well as our recent work interacting the difference for if they are to have indexical knowledge extended iteration, nondeterminism, Morgenstern but these accounts do not deal with indexicality. Situations from are especially perspective between agents must be accounted of a of the same facts. For example, in the dismantling motor, and are facing each other; suppose we need to jointly use a certain wrench in the next step; I might have to come to know that the wrench we are to use is the one on my that it is the one on your right. However, multiple agents left while you need to realize settings give rise to new difficulties about change. One issue is whether that could affect their situation and if not, what agents know about all event occurrences assumptions [44] has proposed an approach is the question of how agents to deal with this. Another refer to each other. In many cases, agents know of each other only under some indexical to assume description that agents always know who all the agents involved are. The formalization in [30] deals with this. they make about such events. Morgenstem issue connected in front of me now”) ; it would be inappropriate (e.g., “the person to indexicality for reasoning proposed It would also be desirable to untangle various aspects of the notion of ability- the agent between “being able etc. We are currently working on a formalization distinguish action”, between cases where not, between cases where the agent acts as a dumb where he does some optimization, deals with these distinctions, compatible with the approach the formalization absolutely guarantees cannot hope to attain such a degree of certainty. an account looked at. to achieve a goal” and “knowing how to execute an is aware of his ability and cases where he is interpreter of a program and cases that and is [34]. As well, the action that the goal will be achieved. Yet, in most real situations, agents to come up with should handles to the frame problem mentioned that can cope with this. Both default and probabilistic earlier that performing iteration and non-determinism, of Section 3.6 requires It would be desirable that it be known approaches indefinite It would also be desirable logic-one that is complete, of some aspects of the logic. to have a mathematically if this is achievable. This might require a re-examination satisfactory axiomatization of the I! Lesptrance, H.J. L.evesque/Arti$cial Intelligence 73 (1995) 69-115 105 Another issue that should be investigated is the interaction between indexicality and hierarchical plans. It seems that the top level actions in such a plan would usually be specified in a relatively objective way (e.g., go to City Hall), while the lower level actions would be indexically specified (e.g., turn right at the next light). How do the different levels in such a plan relate to each other? Route planning should be a good test domain. Indexicality is but one aspect of situatedness. One should look at how other aspects can be modeled. For instance, Barwise [ 51 talks about how “situated inference” is often relative to background conditions; as an example, he describes how one might infer that an object will fall from the fact that it has been released in mid-air, an inference that is relative to the presence of gravity. In many cases, such background conditions play a role in ensuring that an agent’s doing some action will achieve his goal, but the agent is totally unaware of it, or he simply ignores it because these conditions normally hold in his usual environment. It should be possible to extend our account to model this notion of “ability relative to background conditions”. More challenging would be developing a propositional attitude model of the kind of opportunistic acting/planning that Agre and Chapman describe in [ 31, but not obviously beyond current techniques. The theory holds great potential for applications in the modeling of communication (both natural-language and that involving artificial agents exchanging messages in de- signed languages), especially in conjunction with the extensions mentioned earlier. We have seen that the knowledge required for many types of action is indexical. Thus while many uses of indexical expressions in language are only communicative shortcuts, ways of succinctly referring to the relevant entities, it should not be surprising that there are also cases where the information that needs to be communicated is intrinsically index- ical. For example, I can help you get to my place by telling you where it is relative position. To model this, one needs a formal account of communication to your currenf that relates the context sensitivity of language to that of mental states and action.** In [ 301, a preliminary version of such an account is developed, using our theory of indexical knowledge and action as a foundation. Some recent work [54] has focussed on providing agents that interact with the world with the ability to understand natural language instructions; this would be an ideal setting for exploring concrete applications of such an account. Last but not least, the theory would appear to hold much promise for applications in the design of reactive agents. Rosenschein and Kaelbling [ 24,481 use a logical formalism as a design notation and as a specification language in their robot design framework. Since indexical knowledge appears to be centrally involved in the production of reactive behavior [ 2,521, it seems that elements of our logic could prove useful in language for a planner or these roles. It might even be useful as a representation 22 No existing model of natural language use does this. Some, like Cohen and Levesque’s [9], include sophisticated accounts of how the knowledge and intentions of agents am involved in speech acts, but ignore in both utterances and mental attitudes. Others, such as Barwise and Perry’s [ 61, include elaborate indexicality, treatments of indexicality, but do not provide any kind of computational account of how an agent can draw inferences from his prior knowledge and the indexical representations that result from interpreting an utterance. 106 Y Leslkrance, H.J. Levesque/Artificial Intelligence 73 (I 995) 69-l 15 sophisticated indexicality executor. This kind of application might also yield back some insights into and suggest refinements to our theory. Appendix A. Additional proofs A.I. Proof of Proposition 4.21 First, let us formally state one of the frame assumptions for FORWARD mentioned in Section 4.3: Assumption A.1 (FORWARD does not affect selfori) . + v/0( SelfOri = 0 3 AfterNec( FORWARD, selfori = o) ) . We prove Proposition 4.21 by first showing a more general result, Lemma A.3 below. The following lemma is used in its proof. Lemma A.2. k b’pV~( MOVEDTOBY (p, o, 6) 3 AfterNec( FORWARD, MOVEDTOBY (p + ( 1,O) x ROT(O) , 0, (8; FORWARD) ) ) ) . Proof. ( 1) Take arbitrary e = (w, a, t) and g. Assume (2) Take arbitrary w* and te such that w zt w* and t 5 t,. Let e* = (w*, a, t) and that e, g b MOVEDTOBY(~, o, 6). e: = (w*, a, te). Assume (3) By Proposition (4) By this and (2), that A( [IFoRwARDT],,,~, e*, tel. 3.7, ( 1) implies that e*, g k MOVEDTOBY(~, o, 6). it follows that there exists t, such that (w*,a,t,),g{t, ---) te} ~Does((&RORwARD),t,). Let e: = (~*,a, ts). We must now show that [selforiJ& = [ROR~TOAORI(O)~,:,~ and uhe~n,~ ,g = [RPOSTOAP~~(~ + (1,0) x ROT(O))I],~,~. (5) By Assumption A.1 and the reflexivity of q (constraint we must have that [IselforiJ&, = [selforin,,,,. By (3), we have (4) in Section 3.4)) that [selfori],, g = [RORITOAORI( o)],. (6) By Assumption ,s. SO it must be the case that [selfor& g = [RORtTOAORt(o)],_ 4. IO and the reflexivity of q, we have that uhe4ed,g = [heren,,,, + (I, 0) x ROT( [selfori],,,g). By (3), we have that [here],,,, = [IRPOSTOAPOS(p)],:,,, which implies that uhe4kr,g = [RPOSTOAPOS ( p ) 1,: ,g + ( ( 1 (0) x ROT( [[selforiJj,* ,g ) ) I! Lespe’rance. H.J. L.evesque/Artijicial Intelligence 73 (1995) 69-115 107 By (5) we then get that [Ihe&: ,g = [RPOSTOAPOS(~)~,~,~ + ((1,O) X RoT([[RoRIToAoRI(o)n,:,p)). By the definitions of RPOSTOAPOS and RORITOAORI, this means that Uhemll,: .g = [[(hem + p x ROT(seifori)) + (1,0) X ROT(MOD&?lfOri+ O))]_. Thus [heE],c,g= [here + (p + (1,O) x ROT(~)) x RoT(selfori)],~,g = [RPOSTOAPOS(~ + (i,o) X ROT(o))],z,B. (7) From (4), (5) and (6), it follows that e,*,g /== MOVEDTOBY(~ + (1,0) x ROT(O),O, (&FORWARD)). Since w* and & are arbitrary, this together with (2) implies that e , g + AfterNec ( FORWARD, MOVEDTOBY(P + (1,0) x ROT(o),o,(S;FORWARD))). Cl Lemma A.3. + ‘@VO( KIIOW( MOVEDTOBY (p, o, 8) ) 2 Can(FORWARD,MOVEDTOBY(p + (1,0) x ROT(O), 0, (S;F~RWARD)))). Proof. Let 4p Ef MOVEDTOBY (p, o, 8) and P’%~MOVEDTOBY(P + (1,O) x ROT(O),O, (&FORWARD)). that e, g i= Know(p). We must show that Take arbitrary e and g. Suppose e, g b Can(FORWARD, 9’). By Assumption 4.3, we must have that e, g k 3d Know(d = FORWARD). It is easy to show that Assumption 4.10 implies that b PhyPoss(FORwARD). Thus by Proposition 3.3, b ~ow(PhyPoss(FORWARD)). > By Propositions Know(AfterNec(FORWARD, cp’)). By the above supposition, this implies that e, g b Know ( After&c ( FORWARD, cp’ ) ) . 0 3.3 and 3.2, Lemma A.2 that k Know(p) implies Let us now proceed with the proof of Proposition 4.21 proper. It is easy to show that + MOVEDTOBY ( (0, 0), 0, noOp) . By Proposition 3.3, we must then also have that k KXIOW( MOVEDTOBY ( (0,O) , 0, noOp) ) . Thus, by Lemma A.3, we have that k Can( FORWARD, MOVJZDTOBY ( (0,O) + (1,O) x ROT( 0) , 0, (noOp; FORWARD) ) ) . Since + Does( (noOp; 8) , 0’) E Does( S, et), the desired result follows by Proposi- tion 4.17 and Assumption 4.16. 0 108 K Lmptfrance, H.J. L.evesque/Artijicial Intelligence 73 (1995) 69-115 A.2. Proof of Proposition 4.22 As for Proposition 4.21, we proceed by first showing a more general result: Lemma A.4 For all n E N, k V~VO(KIIOW(MOVEDTOBY(~,O,S)) 1 Can( FORWARD”, MOVEDTOBY (p + (n, 0) x ROT( 0) , 0, (&~oRw&)))). Proof. By induction over n. Base case: n = 0. In this case we only need to show that k V@‘O(~~OW(MOVEDTOBY(~,O,@) 1 K~OW(MOVEDTOBY(~ + (0,O) x ~~~(o),o,(fi;noOp)))). It is easy to show that b Does( (8; noOp), 0’) s Does(S, 0’). This together with Proposition 4.17 and Assumption 4.16 implies that /= MOVEDTOBY(~ + (0,O) x ROT(O),O, (6;noOp)) E MovEDToBY(~,o,@. From this, the desired result follows easily by Propositions Induction step: Assume that the formula 3.3 and 3.2. is valid for n and show that it must then be valid for n + 1. By the induction hypothesis and Proposition 4.17, we have that k KIIOW( MOVEDTOBY (p + (1,O) x ROT( 0) , o, (8; FORWARD) ) ) 1 Can(FoRwARD”,MOvEDTOBY(p + (1,0) x ROT(o) + (n,O) x ROT(o), o,((S;F~RWARD);F~RWARD”))). It is easy to show that /= Does((G1;&);&) E Does(&; (&;83)). By this, Proposi- tion 4.17, and Assumption 4.16, the above simplifies to k KIIOW(MOVEDTOBY(~ + (1,O) x ROT(O),O, 1 Can(FoRwARDn,MOvJSDTOBY(p (&FORWARD))) + (n+ 1,0) x ROT(o), 3 0 (&FORWARD”+‘))). , By Propositions 3.8 and 3.9, it follows from this and Lemma A.3 that the formula is validforn+ 1. Cl Given this sition 4.21 was proved Lemma A.3). 0 lemma, we can prove Proposition 4.22 in the same way in the previous subsection (Lemma A.4 is used that Propo- instead of A.3. Proof of Proposition 4.26 In this case too, we proceed by first showing a more general result, Lemma A.6 below. The proof uses the following strengthened version of Proposition 4.25: Lemma A.5 For all n E N, 1 LespCrance, H.J. L.evesque/Artijcial Intelligence 73 (1995) 69-115 109 b Vp’dl~( KIIOW( MOVEDTOBY (p, o, S) > 1 Can(LEFT”, MovEDToBY(~, MOD~&U i- n?r/2), (& LEPL”)))). We omit its proof, which goes along the same lines as that of Lemma A.4. Lemma A.6 For all n, m E Z, k V$V'O(KIIOW(MOVEDTOBY(~,~,~)) 3 C~~(~~RPOS((~,~)),MO~EDTOBY(P + (n,m) x ROT(O), o,(s;GORPoS((n,m)))))). Pmf. Case 1. m = 0 and n E N: Then GORPOS ( (n, m) ) = FORWARD” and the desired result follows immediately from Lemma A.4. Case 2. m = 0 and n < 0: Then GoRPoS( (n, m)) = LEF?; GORPOS( (-?Z,O)); LEF?. By Lemma A.5, proposition 4.17, and Assumption 4.16, we have that b KIIOW(MOVEDTOBY(~,~,S)) > Can(LE~,MOVEDTOBY(p,MOD2,(o + r), (&LEti>>>. By Case 1 and proposition 4.17, we have that i= ~ow(MOVEDTOBY(p,MOD2,,(o+ r), @;LEl+))) > Can(GORPos((-n,O)), MOVEDTOBYQ + (-n,O) x ROT(MOD~,(O + r)), MOD2r(O+7r),(S;LEFl-$GORPOS((--n,0))))). By propositions 3.8 and 3.9, one can chain these results to obtain: k KIIOW(MOVEDTOBY(~,~,~)) > Can( (LE~;GORPOS( (-n,O))), MOVEDTOBY(~ + (-n.0) x ROT(MOD2,(0+ ?r)), MOD2,(0+7r),(8;LEF+;GORPOS((-n,(l))))). By chaining this again with an appropriate instance of Lemma A.5, one obtains that k KIIOW(MOVEDTOBY(~,~,S)) > Can(GORPos( (n,O)), MOVEDTOBY(~ + (-n,O) x ROT(MOD2,(0 + r)), MODZ,~(MOD~,(O+~~) +~),(&GORPOS((n,O))))). Now it is easy to check that (-n,O) x ROT(MO&,(o+7i-))=(-ncos(o+7r),--nsin(o+~)) =(ncos(o),nsin(o)) = (n,O) x ROT(O). 110 Y Lespdrance, H.J. Levesyue/Arti~ciul tntelligence 73 (I 995) 69-1 I5 It is also the case that RORITOAORI(MOD2,(MOD2,(0 + ?r) + 7~)) = MOD2, (SelfOh + MOD2, (MOD2, ( o + %-) + 71) ) = MoD2, (selfori + 0) = RORITOAORI (0). So by the definition of MOVEDTOBY and proposition 4.16, we must have that k MOVEDTOBY(JJ + (-n,O) x ROT(MOD~,(~ + n-)), MODzp(MOD2,(0 + rr) + V), (6; GORPW (F 0)) ) ) = MOVEDTOBY (p + (n. 0) x ROT(O) , o, (8; GORPOS( (n, 0)) ) ). The desired result follows. Case 3. m # 0: Then GORPOS ( (II, m) ) = GOBPOS ( (II, 0) ) ; LEFT; GOBPOS ( (m , 0) ) ; LEl+ . The proof instances of Case 2 with instances of Lemma AS, we show that to Case 2. By chaining (Propositions is similar 3.8 and 3.9) appropriate + KIIOW( MOVEDTOBY (p, o, 6) ) > Can(GORPOS( (n,m)), MOVEDTOBY(~ + (n,O) x ROT(O) + (m,O) x ROT(MOD2,(o+ z-/2)), MODz,(MOD2,( 0 + 7r/2) + 3~/2), (s;GORPOs((n,m))))). One then shows that + MOVEDTOBYQ + (n,O) x ROT(O) + (m,O) x ROT(MOD~,(~+ 7r/2)), MODZ,,(MODZ,(O+~~/~) +3~/2),(6;GORPOS((n,m)))) f MOVEDTOBY (p + ( n,m) X ROT(~),O,(&GORPOS((~,~)))) to obtain the desired result. 0 Given tion 4.21 was proved this lemma, we can then prove Proposition 4.26 in the same way that Proposi- 0 in Section A.1 (Lemma A.6 is used instead of Lemma A.3). A.4. Proof of Proposition 4.32 Let us first introduce some shorthands: l UOAT( 0:) dzf 3vi( OBJECT( vi) A RPOS( u’ ) = (et, 0) A ~HOLDING( vi) ), provided ui does not occur free in 0:; . UOH d:f UOAT( 0) ; l UOBTW( 0:, 0:) dzf 3u’( 0; 6 ui < 0: A UOAT( vi)), provided vi does not occur free in 0: and 81; E Lmphance, H.J. L.evesque/Arti$cial Intelligence 73 (199.5) 69-I 15 111 the agent if 0; is negative) in front of the agent UOAT( 13:) says that there is an unheld object 6: squares directly ; UOH says that there is an unheld object here; and (behind finally, UOBTW( @, 0;) says that there is an unheld object between 0; and 6: squares directly in front of or behind the agent. We prove Proposition its scanning com- ponent, Lemma A.9 below. This result depends on two sublemmas. The first concerns the FORWARD 4.32 by first establishing a result concerning action: Lemma A.7. For all n E N, + Know( +OBTW( -n, 0) ) 3 Can(FORwARD,+OBTW(-n- 1,-l)). lemma can be proven from Assumptions for FORWARD mentioned 4.3 and 4.10, as well as the frame in Section 4.3; a detailed proof of a closely the SENSE lemma concerns result appears in [29]. The second preliminary This assumptions related action: Lemma A.8 For all n E N, +Know(+JOBTW(-n,-1)) > Can( SENSE, Know( UOH) V Know( ~UOBTW( -n, 0) ) ) . It can be proven SENSE mentioned from Assumptions in Sections 4.4 and 4.3. 4.3 and 4.30, and the frame assumptions for Let us now proceed with the main lemma concerned with scanning: Lemma A.9. For all k, n E N, k Know(UOH) VKnow(~UOBTW(-n,O)) > Can( scmk( UOH) , Know( UOH) V Know( -UOBTW( -n - k, 0) ) ) . Proof. By induction over k. Base case: SCANO(UOH) dzf noOp and b Can(noOp, cp) 3 Know( 4p), so all we need to show is that for all n E IV, k Know(UOH) V Know(+IOBTW( -n,O)) > ti~w(Know(UOH) This is easily done using Propositions V Know(TUOBTW(-n,O))). 3.5, 3.3, and 3.2. Induction step: Assume that the formula is valid for k and show that it must then be valid for k + 1. SCANk+i (UOH) dzf if( +OH, FORWARD; SENSE; SCANT (UOH) , noOp) , and so by Proposition 3.10, we have that p Can(ScAN~++I(UOH),Know(UOH) VKnow(+JOBW(-n- (k+ l),O))) E (Know(UOH)ACan(noOp,Know(UOH)VKnow(~UOBTW(-n-(k+1),0)))) 112 E Lespbance, H.J. Levesque/Artijicial Intelligence 73 (1995) 69-115 V (Know( 7UOH) A Can( FORWARD; SENSE; SCAN~( UOH), Know(UOH) VKnow(-3JOBTW(-n - (k+ l),O)))). By Propositions 3.5, 3.3, and 3.2, it is easy to show that for all n E N, /= Know(UOH) 3 Know(UOH) A Can(noOp, Know(UOH) V K~ow(TUOBTW(-n - (k + l),O))). What remains to be proved is that for all n E N, /= Know(-JJOBTW( --n, 0)) 3 (Know( -dJOH) A Can( FORWARD; SENSE; SCANT (UOH), Know( UOH) V KIIOW( ~UOBTW( -n - (k + I), 0) ) ) ) Clearly k ~UOBTW( -n, 0) > 4JOH for n E N; so by Propositions f= Know( 7UOBTW( hypothesis, it follows by Proposition 3.3 and 3.2, -n, 0)) > Know( 1UOH). From 3.8 that for all we have Lemma A.8 and n E N, that for all n E N, the induction /=Know(TUOBTW(-(n+ 1),-l)) > Can( SENSE; SCANk (UOH) , Know(UOH) VKnow(7UOBTW(-(n + 1) - k,O))). By Propositions 3.8 and 3.9, this and Lemma A.7 imply that for all n E W, + Know( TUOBTW( 3 Can( FORWARD; -n, 0) ) Know(UOH) SENSE; SCANk (UOH) , V&ow(lUOBTW(-n- (kf l),O))). 0 We can now complete the proof of Proposition 3.8, we must have that for all n E N, Proposition 4.32. By Lemmas A.9 and A.8 and + Know( 4JOBTW( - 1) ) > Can( (SENSE; SCANk (UOH) ) , -n, Know( UOH) V Know( 7UOBTW( -n - k, 0) ) ) . Clearly /= 4JOBTW( 0, - 1 ), and thus by Proposition 3.3 + Know(4JOBTW(O, -1)); the desired result follows. 0 Acknowledgements This paper describes a revised version of work of Computer Science, University that was part of the first author’s Ph.D. Thesis at the Department of Toronto, which was supervised by the second author (jointly with Graeme Hirst). Revisions were done Japan while and the School of Computing Science, Simon Fraser University, Burnaby, Canada. This work was funded by the Institute for Robotics and Intelligent Systems and the Natural Science and Engineering Research Council of Canada, and the Information Technology the first author was at NTT Basic Research Laboratories in Musashino, X Lesperance, H.J. L.evesque/ArtiJicial Intelligence 73 (1995) 69-115 113 Research Centre of Ontario. David Israel, Graeme Hirst, and the referees have provided many valuable suggestions. Discussions with Phil Agre, Adam Grove, Joe Halpem, and Joe Nunes have also been helpful. References [ I ] PE. Agre, Review of L.A. Suchman, Plans and Situated Action: The Problem of Human-Machine (1990) (Cambridge University Press, Cambridge, UK, 1987). Artif Intell. 43 (3) Communication 369-384. [ 21 PE. Agre and D. Chapman, Pengi: an implementation of a theory of activity, in: Proceedings AAAI-87, Seattle, WA (1987) 268-272. [ 31 P.E. Agre and D. Chapman, What are plans for?, Rob. Autonom. Sysr. 6 (1990) 17-34. [ 41 A.V. Aho, J.E. Hopcroft and J.D. Ullman, The Design and Analysis of Computer Algorithms (Addison- Wesley, Reading, MA, 1974). [5] J. Barwise, Information and circumstance, Notre Dame J. Formal Z.ogic 27 (3) (1986) 324-338. [6] J. Barwise and J. Perry, Situations and Attitudes (MIT Press/Bradford Books, Cambridge, MA, 1983). [7] H.-N. Castageda, On the logic of attributions of self-knowledge to others, J. Philos. 65 (15) (1968) 439-456. [ 81 B.F. Chellas, Modal Logic: an Introduction (Cambridge University Press, Cambridge, UK, 1980). [9] P.R. Cohen and H.J. Levesque, Rational interaction as the basis for communication, in: P.R. Cohen, J. Morgan and M.E. Pollack, eds., Intentions in Communication (MIT Press, Cambridge, MA, 1990) 221-255. [ lo] E. Davis, Inferring ignorance from the locality of visual perception, in: Proceedings AAAI-88, St. Paul, MN (1988) 786-791. [ 111 E. Davis, Reasoning about hand-eye coordination, in: Working Notes, IJCAI Workshop on Knowledge, Perception, and Planning, Detroit, MI (1989) l-6. [ 121 E. Davis, Solutions to a paradox of perception with limited acuity, in: Proceedings First International Conference on Principles of Knowledge Representation and Reasoning, Toronto, Gnt. (1989) 79-82. [ 13 1 R. Goldblatt, Logics of Bme and Computation, CSLI Lecture Notes 7 (Center for the Study of Language and Information, Stanford University, Stanford, CA, 1987). [ 141 A.J. Grove, Topics in multi-agent epistemic logic, Ph.D. Thesis, Department of Computer Science, Stanford University, Stanford, CA ( 1992). [ 151 A.J. Grove and J.Y. Halpem, Naming and identity in a multi-agent epistemic logic, in: J. Allen, R. Fikes and E. Sandewall, eds., Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference, Cambridge, MA (1991) 301-312. [ 161 A.R. Haas, A syntactic theory of belief and action, Artif Intell. 28 (1986) 245-292. 1171 A.R. Haas, Indexical expressions and planning, unpublished manuscript, Department of Computer Science, State University of New York, Albany, NY ( 1991). [ 181 J.Y. Halpem and Y. Moses, A guide to the modal logics of knowledge and belief: preliminary draft, in: Proceedings IJCAI-85, Los Angeles, CA ( 1985) 480-490. [ 191 J. Hintikka, Knowledge and Belief (Cornell University Press, Ithaca, NY, 1962). [20] J. Hintikka, Semantics for the propositional attitudes, in: J.W. Davis, D.J. Hackney and K.W. Wilson, eds., Philosophical Logic (Reidel, Dordrecht, Netherlands, 1969) 21-45. [ 211 G.E. Hughes and M.J. Cresswell, An Introduction to Modal Logic (Methuen, London, UK, 1968). [22] D.J. Israel, The role of propositional objects of belief in action, Tech. Report CSLI-87-72 CSLI, Stanford University, Stanford, CA (1987). [ 231 L.P. Kaelbling, An architecture for intelligent systems, in: Reasoning about Actions and Plans: Proceedings of the 1986 Workshop, Timberline, OR (1987) 395-410. [24] L.P. Kaelbhng and S.J. Rosenschein, Action and planning in embedded agents, Rob. Autonom. Syst. 6 (1990) 35-48. [25] K. Konolige, A first order formalization of knowledge and action for a multi-agent planning system, in: J. Hays and D. Michie, eds., Machine Intelligence 10 (Ellis Horwood, Chichester, UK, 1982). I14 E LespPrance, H.J. L.evesque/Artificial Intelligence 73 (1995) 69-115 I 26 1 K. Konolige, A Deduction Model of Belief (Pitman, London, 1986). I27 I S.A. Kripke, Semantical considerations I28 I A. Kronfeld, Reference and Computation: An Essay in Applied Philosophy of Language on modal logic, Acta Philos. Fennica 16 ( 1963) 83-94. (Cambridge University Press, New York, 1990). I29 I Y. Lespgrance, Computer CSRI-248. A formal Science, University theory of index&d knowledge of Toronto, Toronto, Ont. and action, (1991); Ph.D. Thesis, Department of as Tech. Report also published [ 30 ] Y. LespCrance, An approach to modeling in: .I. Horty and Y. Shoham, eds., Reasoning about Mental States: Formal Theories and Applications, Papers from the I993 AAAI Spring Symposium, Stanford, CA (1993) 79-85; also Tech. Report SS-93-05, AAAI Press; also in: Proceedings IJCAI-93 Workshop on “Using Knowledge in its Conrext”, Chambery, France (1993). in action and communication, indexicality I 3 I 1 Y. LespCrance and H.J. Levesque, lndexical knowledge in robot plans, in: Proceedings AAAI-90, Boston, MA ( 1990) 868-874. ( 32 1 Y. LespCrance and H.J. Levesque, An argument in: R. Elio, ed., Proceedings Tenth Biennial Conference of the Canadian Society for Computational Studies of Intelligence, Banff, Alta. ( 1994) 27 l-277. representations for indexical in temporal reasoning, I33 I Y. LespCrance and H.J. Levesque, On the logic of indexical knowledge and action, Tech. Report, Department of Computer Science, University of Toronto, Toronto, Ont. (1995), to appear, I34 I Y. LespCrance, H.J. Levesque and F. Lin, A formalization that avoids in: Proceedings Fourth International Conference on Principles of Knowledge of ability and knowing how the frame problem, Representation and Reasoning (KR’94), Bonn, Germany ( 1994). submitted. 1351 H.J. Levesque, Foundations of a functional approach to knowledge representation, Art@ Intell. 23 (1984) 155-212. I36 I H.J. Levesque, A logic of implicit and explicit belief, in: Proceedings AAAI-84, Austin, TX ( 1984) 198-202. I37 1 D. Lewis. Attitudes de ditto and de se, Philos. Rev. 88 (4) [ 38 I J. McCarthy and P. Hayes, Some philosophical intelligence, in: B. Meltzer and D. Michie. eds., Machine Intelligence 4 (Edinburgh University Press, Edinburgh, UK, 1979)463-502. from the standpoint of artificial problems ( 1979) 5 13-543. I 39 I E. Mend&on, I40 I R.C. Moore, Reasoning Introduction to Mathematical Logic (Van Nostrand, New York, 2nd ed., 1979). about knowledge and action, Tech. Report 191, AI Center, SRI International, Menlo Park, CA ( 1980). 141 I R.C. Moore, A formal theory of knowledge and action, in: J.R. Hobbs and R.C. Moore, eds., Formal Theories of the Common Sense World (Ablex, Norwood, NJ, 1985) 319-358. I42 I L. Morgenstem, Knowledge preconditions for actions and plans, in: Proceedings IJCAI-87, Milan, Italy ( 1987) 867-874. I43 ] L. Morgenstem, Foundations of a logic of knowledge, action, and communication, Ph.D. Thesis, Department of Computer Science, New York University, New York ( 1988). I44 I L. Morgenstem, Knowledge and the frame problem, in: K. Ford and P Hayes, eds., Reasoning Agents in a Dynamic World: The Frame Pro&em (JAI Press, Greenwich, 1991). I45 I A. Newell, The knowledge I46 I J. Perry, The problem of the essential I47 I R. Reiter, The frame problem for goal regression, level, Art$ Infell. 18 ( 1) ( 1982) 87-127. indexical, N&s 13 ( 1979) 3-2 I. in the situation calculus: a simple solution (sometimes) in: V. Lifschitz, result Computation: Papers in Honor of John McCarthy (Academic Press, San Diego, CA, I99 1) 359-380. ed., Artijicial Intelligence and Mathematical Theory of 148 I S.J. Rosenschein properties, c>f the 1986 Conference, Monterey, CA ( 1986) 83-98. and L.P. Kaelbling, in: J.Y. Halpem, epistemic ed., Theoretical Aspects of Reasoning about Knowledge: Proceedings of digital machines with provable The synthesis 149 I R.B. Scherl and H.J. Levesque, The frame problem and knowledge-producing actions, in: Proceedings AAAI-93, Washington, DC ( 1993) 689-69.5. I50 I R.B. Scherl, H.J. Levesque and Y. LespCrance, IS1 I B.C. Smith, The owl and the electric encyclopedia, Artif Inrell. 47 ( 1991) 251-288. in the situation calculus, Indexicals in preparation and a completeness I! Lesptrance, H.J. L.evesque/Artificial Intelligence 73 (1995) 69-115 115 [52] D. Subramanian and J. Woodfill, Making the situation calculus indexical, in: Proceedings First International Conference on Principles of Knowledge Representation and Reasoning, Toronto, Ont. ( 1989) 467-474. [53] L.A. Suchman, Plans and Situated Action: The Problem of Human-Machine Communication (Cambridge University Press, Cambridge, UK, 1987). [54] B. Webber, N. Badler, B. di Eugenio, C. Geib, L. Levison and M. Moore, Instructions, intentions and expectations, Art$ Intell. 73 (1995) 253-269. 