Artificial Intelligence 171 (2007) 805–837www.elsevier.com/locate/artintNegotiating using rewardsSarvapali D. Ramchurn a,∗, Carles Sierra b, Lluís Godo b,Nicholas R. Jennings aa IAM Group, School of Electronics and Computer Science, University of Southampton, UKb IIIA—Artificial Intelligence Research Institute, CSIC, Bellaterra, SpainReceived 8 November 2006; received in revised form 2 April 2007; accepted 2 April 2007Available online 6 May 2007AbstractNegotiation is a fundamental interaction mechanism in multi-agent systems because it allows self-interested agents to cometo mutually beneficial agreements and partition resources efficiently and effectively. Now, in many situations, the agents need tonegotiate with one another many times and so developing strategies that are effective over repeated interactions is an importantchallenge. Against this background, a growing body of work has examined the use of Persuasive Negotiation (PN), which involvesnegotiating using rhetorical arguments (such as threats, rewards, or appeals), in trying to convince an opponent to accept a givenoffer. Such mechanisms are especially suited to repeated encounters because they allow agents to influence the outcomes of futurenegotiations, while negotiating a deal in the present one, with the aim of producing results that are beneficial to both parties. Tothis end, in this paper, we develop a comprehensive PN mechanism for repeated interactions that makes use of rewards that can beasked for or given to. Our mechanism consists of two parts. First, a novel protocol that structures the interaction by capturing thecommitments that agents incur when using rewards. Second, a new reward generation algorithm that constructs promises of rewardsin future interactions as a means of permitting agents to reach better agreements, in a shorter time, in the present encounter. We thengo on to develop a specific negotiation tactic, based on this reward generation algorithm, and show that it can achieve significantlybetter outcomes than existing benchmark tactics that do not use such inducements. Specifically, we show, via empirical evaluationin a Multi-Move Prisoners’ Dilemma setting, that our tactic can lead to a 26% improvement in the utility of deals that are madeand that 21 times fewer messages need to be exchanged in order to achieve this.© 2007 Elsevier B.V. All rights reserved.Keywords: Persuasive negotiation; Repeated negotiations; Negotiation tactics; Bargaining; Bilateral negotiation1. IntroductionNegotiation is a fundamental concept in multi-agent systems (MAS) because it enables self-interested agents tofind agreements and partition resources efficiently and effectively. In most cases, such negotiation proceeds as a seriesof offers and counter-offers [20]. These offers generally indicate the preferred outcome for the proponent and theopponent may either accept them, counter-offer a more beneficial outcome, or reject them. Now, in many cases, the* Corresponding author.E-mail addresses: sdr@ecs.soton.ac.uk (S.D. Ramchurn), sierra@iiia.csic.es (C. Sierra), godo@iiia.csic.es (L. Godo), nrj@ecs.soton.ac.uk(N.R. Jennings).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.014806S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837agents involved need to negotiate with one another many times. However, such repeated encounters have rarely beendealt with in the multi-agent systems literature (see Section 7 for more details). One of the main reasons for this isthat repeated encounters require additional mechanisms and structures, over and above those required for single shotencounters, to fully take into account the repeated nature of the interaction. In particular, offers that are generatedshould not only influence the present encounter, but also future ones, so that better deals can be found in the long run[9,25]. To this end, argument-based negotiation (ABN), in which arguments are used to support offers and persuadean opponent to accept them, has been advocated as an effective means to achieve this [30,36] and, therefore, this isthe approach we explore in this paper.In more detail, ABN techniques aim to enable agents to achieve better agreements faster by allowing them toexplore a larger space of possible solutions and/or to express, update, or evolve their preferences in single or multipleshot interactions [21]. They do this by providing additional explanations that justify the offer [1], identifying othergoals satisfied by the offer that the opponent might not be aware of [31], or offering additional incentives conditionalupon the acceptance of the offer [2,22,39]. While all these approaches capture, in one way or another, the notionof persuasiveness, a number of them have focused specifically on the use of rhetorical arguments such as threats,rewards, and appeals [3,28,41,44]. To be clear, here, we categorise such argument acts as persuasive elements thataim to force, entice, or convince an opponent to accept a given offer (see Section 7 for more details). In particular,we categorise such approaches under the general term of Persuasive Negotiation (PN) to denote the fact that these tryto find additional incentives (as opposed to justifying or elaborating on the goals of an offer) to move an opponent toaccept a given offer [30,36].In order to implement a PN mechanism, it is critical that the exchanges between the negotiating agents follow agiven pattern (i.e. ensuring that agents are seen to execute what they propose and that the negotiation terminates) andthat the agents are endowed with appropriate techniques to generate such exchanges (i.e. they can evaluate offers andcounter-offers during the negotiation process). These requirements can be met through the specification of a protocolthat dictates what agents are allowed to offer or commit to execute and a reasoning mechanism that allows agentsto make sense of the offers exchanged and accordingly determine their best response [30]. Given this, we present anovel protocol and reasoning mechanism for pairs of agents to engage in PN in the context of repeated games, inwhich the participating agents have to negotiate over a number of issues many times. In particular, we focus on theexchange of rewards (as opposed to threats or appeals). We do so because rewards have a clear benefit for the agentreceiving it, and entail a direct commitment by the agent giving it, to continue a long term relationship which is likelyto be beneficial to both participating agents.1 In addition to the standard use of rewards as something that is offeredas a prize or gift, our model also allows agents to ‘ask’ for rewards in an attempt to secure better outcomes in thefuture, while conceding in the current encounter and therefore closing the deal more quickly. This latter perspective iscommon in human-to-human negotiations where one of the participants may ask for a subsequent favour in return foragreeing to concede in the current round [17,33].Being more specific still, our PN mechanism constructs possible rewards in terms of constraints on issues to benegotiated in future encounters and our protocol extends Rubinstein’s [37] alternating offers protocol to allow agentsto negotiate by exchanging arguments along with their offers (in the form of promises of future rewards or requestsfor such promises in future encounters).Example. A car seller may reward a buyer who prefers red cars with a promise (or the buyer might ask for the reward)of a discount of at least 10% (i.e. a constraint on the price the seller can propose next time) on the price of her yearlycar servicing if she agrees to buy a blue one instead at the demanded price (as the buyer’s asking price for the red caris too low for the seller). Now, if the buyer accepts, it is a better outcome for both parties; the buyer benefits becauseshe is able to make savings in future that match her preference for the red car and the seller benefits in that he reduceshis stock and obtains immediate profit.1 The use of appeals and threats poses a number of problems. For example, the use of appeals usually assumes agents implement the samedeductive mechanism (an overly constraining assumption in most cases) because appeals impact directly on an agent’s beliefs or goals whichmeans that such appeals need to adopt a commonly understood belief and goal representation [1,3,22]. Threats, in turn, tend to break relationshipsdown and are not guaranteed to be enforced, which makes them harder to assess in a negotiation encounter [19].S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837807We believe such promises are important in repeated interactions for a number of reasons. First, agents may beable to reach an agreement faster in the present game by providing some guarantees over the outcome of subsequentgames. Thus, agents may find the current offer and the reward worth more than a counter-offer (which only delaysthe agreement and future games). Second, by involving issues from future negotiations in the present game (as inthe cost of servicing in the example above), we effectively expand the negotiation space considered and, therefore,provide more possibilities for finding (better) agreements in the long run [20]. For example, agents that value futureoutcomes more (because of their lower discount factors) than their opponent are able to obtain a higher utility infuture games, while the opponent who values immediate rewards can take them more quickly. Thirdly, if the rewardguarantees the range of possible outcomes in the next game, the corresponding negotiation space is constrained bythe reward, which should reduce the number of offers exchanged to search the space and hence the time elapsedbefore an agreement is reached. Continuing the above example, the buyer starts off with an advantage next time shewants to negotiate the price to service her car and she may then not need to negotiate for long to get a reasonableagreement.Against this background, this work advances the state of the art in the following ways. First, we provide a newalternating offers protocol that extends the alternating offers protocol and builds upon Bentahar et al. [6] to specifycommitments that agents make to each other when engaging in persuasive negotiations using rewards. Specifically,the protocol details, using dynamic logic, how commitments arise or get retracted as a result of agents promisingrewards or making offers. Thus, by using our protocol, it is possible to keep track of the commitments made andtherefore ensure that they do enact the rewards or offers they commit to. The protocol also standardises what an agentis allowed to say or what it can expect to receive from its opponent which, in turn, allows it to focus on making theimportant negotiation decisions. Second, as part of an agent’s reasoning mechanism, we develop a Reward GenerationAlgorithm (RGA) that calculates constraints (which act as rewards) on resources that are to be negotiated in futuregames. The RGA thus provides the first heuristic to compute and select rewards to be given and asked for. Third, wedevelop a specific Reward Based Tactic (RBT) that uses the RGA to generate combinations of offers and rewards. Inso doing, we provide the first PN tactic that considers the repeated nature of interactions when generating offers andrewards. We then go on to show that RBT can reach better agreements (up to 26% more utility) in less time (using 21times fewer messages) than standard non-persuasive negotiation tactics.The remainder of this paper is structured as follows. Section 2 describes the basic definitions of repeated negotia-tion games and the properties of the agents. Section 3 details our PN protocol and Section 4 presents the RGA and thefunctions used by the agents to evaluate incoming offers and rewards. Given this, Section 5 describes the RBT algo-rithm. In Section 6, we empirically evaluate the RBT and benchmark it against other standard negotiation algorithms.Section 7 details related work and Section 8 concludes.2. Repeated negotiation gamesIn this section we formalise the repeated negotiation games within which we apply PN. Thus, let Ag be the setof agents and X be the set of negotiable issues. Agents negotiate about issues x1, . . . , xn ∈ X where each one hasa value vi in its domain D1, . . . , Dn. Then, a contract O ∈ O is a set of issue-value pairs, noted as O = {(x1 =v1), . . . , (xm = vm)}, where O is the set of all such contracts.2 We will also note the set of issues involved in a contractO as X(O) ⊆ X. During negotiation, an agent can limit the range of values it can accept for each issue, termed itsnegotiation range and noted as [vximax]. Without loss of generality, we require that each variable xi in a contractoccurs at most once and that the number of variables and the values taken by them is finite.min, vxiGiven these basic definitions, a negotiation game is one in which an agent starts by making an offer O = {(x1 =v1), . . . , (xm = vm)} (with or without rewards) over a set of issues {x1, . . . , xm} ⊆ X and the opponent may thencounter-offer or accept. The agents may then go on counter-offering until an agreement is reached or the deadlinetdead is reached (we superscript it with the agent identifier where needed).3 While it is possible to consider infinitely2 Other operators (cid:2), (cid:3) can also be used. This means agents can specify a range of values to enact rather than a specific value. This will beimportant when we need to specify rewards in Section 4.2.3 If an agreement is reached, the agents are committed to enacting the deal settled on according to the protocol defined in Section 3. Note, ifthey cannot be forced to enact a deal, a trust model such as [34,43] can be used to check for this and the behaviour of the agent can be alteredaccordingly. However, the latter case is beyond the scope of this work.808S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837Table 1Summary of notation usedximax]ximin, vAgOU (O)OαX[vtθτ(cid:7)αe−(cid:7)α (θ +t)e−(cid:7)α (τ +t)t αdeadLα|.| (cid:2) |δU βxithe set of agents (usually α and β).the set of contracts (a contract is O ∈ O).the utility of a contract.a contract in which ∀xi ∈ X(Oα), |δU αxithe set of negotiated issues x1, x2, . . . .the negotiation range of a given issue xi .time since first negotiation game started.the delay between two negotiation games.the time between two offers.the discount factor of agent α.the discount between games for agent α.the discount between offers for agent α.the deadline of the negotiation game for α.the target utility of agent α.or finitely repeated games, we focus on the base case of one repetition in this work because we aim to understandat a foundational level the impact that promises of future rewards may have on such encounters. We also constrainthe games, and further differentiate them from the case where agents play one game each time independently of thefirst one, by allowing the second game to happen if and only if the current game has a successful outcome (i.e. anagreement is reached within the agents’ deadlines). In so doing, there is no possibility for agents to negotiate bothoutcomes in one negotiation round. The agents may also come to an agreement in the first game but fail to reachone in the second game, in which case they only obtain utility from the outcome of the first game. This, we believe,more closely models realistic applications where agents will engage in long-term relationships only if they can findsome benefit in so doing given the result of their previous agreement (i.e. reach some agreements prior to continuingtheir relationship). Such approaches are common in long-term contracting or relationships as defined in the economicliterature [9,25]. Negotiation games are played in sequence and there may be a delay θ between the end of the firstgame and the beginning of the second one. Moreover, during a game, the time between each transmitted offer is notedas τ .In each negotiation game, agents can assess the value of offers exchanged using their utility function. Each agent→ [0, 1] and the utility over a contract U : O → [0, 1]has a (privately known) utility function over each issue Uxi : Dxiis defined as:(1)(cid:2)U (O) =wiUxi (vi)i=1,...,m(cid:3)where O = {(x1 = v1), . . . , (xm = vm)}, wi is the weight given to issue xi andwi = 1. We consider two agentsα, β ∈ Ag having utility functions designed as per the Multi-Move Prisoners’ Dilemma (MMPD) (this game is chosenbecause of its canonical and ubiquitous nature—see Appendix A for more details) [5,7,46]. According to this game,α’s marginal utility δU is higher (on an absolute scale) than β’s for some issues, which we note as Oα, and less forothers, noted as Oβ , where Oα ∪ Oβ = O.4 Moreover, given the delays that exist between and during games, agents’utilities will be discounted as follows. In between games, the discount is computed as e−(cid:7)(θ+t) and between offers itis e−(cid:7)(τ +t) where t is the time since the negotiation started (note that we expect θ (cid:8) τ generally) and (cid:7) is known asthe discount factor of the agent.5 The value of (cid:7) scales the impact of these delays, where a higher value means a moresignificant discounting of an offer and a lower value means a lower discounting effect. Finally, each agent is assumedto have a target utility to achieve over the two games (noted as L ∈ [0, 2]). This target can be regarded as the agent’s4 By establishing such a relationship between the agents’ utility functions, we aim to make our model applicable to more realistic settings. Also,we believe it is not unreasonable to assume that agents could estimate which issues are more important (i.e. have a higher |δU |) to them or to theiropponent. In any case, our mechanism also applies to the case where agents’ marginal utilities sum to zero (in which case the agents play a commonzero-sum game [25]).5 The exponential decay function is commonly used in bargaining theory to capture the cumulative discounting effect of delays between offers.Other functions could also be used according to the particular application context chosen.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837809aspiration level for the combined outcomes of the two games [13]. This target must, therefore, be less than or equalto the sum of the maximum achievable utility over the two games (2 in the case an agent has a (cid:7) = 0 and exploitsboth games completely); that is L (cid:3) 1 + e−(cid:7)(θ+t), where 1 is the maximum achievable utility in an undiscountedgame.Having defined the basic constructs of repeated negotiation games, we summarise the notation used in Table 1. Inthe next section, we describe the negotiation protocol. To this end, we build upon the notation presented in this sectionin order to clearly specify the semantics of the interaction.3. The negotiation protocolAs discussed earlier, negotiation proceeds via an exchange of offers and counter-offers [37]. In general, the pro-tocol specification of this interaction is rather simple in that there is only one type of commitment upheld by eachagent at any one time (that is enacting the proposal if its offer is accepted). However, extending the protocol to en-capsulate persuasive elements such as rewards means that other commitments (pertaining to the enactment of thecontent of rewards) must be specified for the agents issuing these rewards [6,23,47]. We term these commitmentssocial commitments since they are pledges made by agents by virtue of their publicly visible actions or utterances.These commitments can then be checked by an institution or arbitrator to make sure that the agents are doing whatthey are supposed to and thus provide guarantees of proper behaviour [30].There are a number of representations that can be used to specify how these commitments can be made or retractedby the illocutions (what the agents say) and the actions (what the agents do) [30]. However, given that rewards arelikely to result in a large number of states and state transitions and that the enactment of rewards requires clearsemantics of actions to be performed, we specify our protocol using Harel’s dynamic logic (DL) [18]. This type ofaction-based logic is particularly suitable for specifying programs or sets of actions which have start and terminationconditions and constructs similar to a negotiation encounter. Specifically, we build upon the work of [6] to cater forrewards. To this end, we first provide a brief overview of the constructs of dynamic logic and then specify the syntaxand semantics of the language used to describe the protocol. Finally, we detail the axioms that capture the impact ofillocutions and other actions taken by agents in a negotiation encounter.3.1. PreliminariesDynamic logic has been proposed as a multimodal logical system to give semantics to programs. A program canbe conceived as a combination of actions that change the state of the world. The main components of DL are thus aset of atomic programs a0, a1, . . . ∈ Π0 and a set of modal formulae Φ to describe the world states (see [18] for moredetails). The atomic actions are basic, indivisible, and execute in a single step. Given this, a program Π is generatedby composing actions using a number of operators such that if a, b ∈ Π then:• a; b ∈ Π signifies that b is performed after a (i.e. sequential composition).• a∗ ∈ Π represents an iteration of a an indeterminate number of times.• ϕ? ∈ Π tests whether the formula ϕ ∈ Φ is satisfied in the current state.• a ∪ b ∈ Π specifies a non-deterministic execution of either a or b.Moreover, [a]ϕ denotes that after program a ∈ Π is executed, it is necessary that ϕ is true. (cid:9)a(cid:10)ϕ denotes that afterprogram a ∈ Π is executed, it is possible that ϕ is true. The propositional operators ∧, ∨, ¬, ↔, and 1 can be definedfrom → and 0 in the usual way.DL semantics are based on Kripke-style structures M = (S, τ, ρ) where S represent the set of states, τ : Φ → 2Sgives the states where a formula is true, and ρ : Π → 2S×S is a function taking a program as argument and giving thecorresponding set of pairs of starting and end states that the program connects.In the following subsections we define a particular theory called PN (for persuasive negotiation) over DL to modela persuasive negotiation dialogue. To do so, we first describe the language, that is the set Π0 of illocutionary (orother) actions that agents interchange, and the set of formulae Φ that will describe the state of a negotiation encounter.Given these, we provide a set of axioms that express the constraints which apply within our persuasive negotiationprotocol.810S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–8373.2. The PN languageIn this section we describe the main components of the language. We first formalise the notion of contracts as anaction that agents can execute. Second, we describe the illocutions that can be exchanged during the dialogue and,third, we detail the predicates that are used to represent the state of the world.3.2.1. ContractsThe central element of PN is the contract that agents negotiate upon. We extend the notion of a contract givenin Section 2 to capture the fact that agents execute elements of a contract. To this end, we note the set of formulaeASG ⊂ Φ as consisting of atomic assignments of the form xi = vi and conjunctions of atomic assignments (x1 =v1) ∧ (x2 = v2) ∧ · · · ∧ (xn = vn).6 We also introduce the operator Do to represent contracts as atomic actions tobe more consistent with the logical language representation used in this section. Thus, what we define as a contract{(x1 = v1, . . . , (xm = vm)} is equivalent to Do((x1 = v1) ∧ · · · ∧ (xm = vm)). Moreover, a union of contracts {x1 =v1), (x2 = v2)} and {(x3 = v3), (x4 = v4)} to {x1 = v1), (x2 = v2), (x3 = v3), (x4 = v4)} is equivalent to a conjunctionof the contents of the two contracts, that is, Do((x1 = v1) ∧ (x2 = v2) ∧ (x3 = v3) ∧ (x4 = v4)).7Given the above definitions, a contract Do(ϕ) ∈ O, with ϕ ∈ ASG, represents the action of making the assignmentϕ true.83.2.2. IllocutionsAgents negotiate by sending illocutions which represent offers and counter-offers. These illocutions are consideredto be actions in our setting as per speech-act theory [4,38]. Illocutions generally talk about other illocutions (to be sentat a later time) or about contracts that can be made between the pair of negotiating agents. Here our set of illocutionsI ⊂ Π0 consists of two general classes. The first consists of the proper negotiation illocutions Ineg, while the secondcontains those illocutions Ipers that are added to form the persuasive part of negotiation. We will denote by Iα and Iβthe set of all illocutions that α and β can send respectively.First, negotiation illocutions from Ineg have the general form:• propose(α, β, p)—denotes that α sends a proposal to β to accept the deal given in p ∈ O.• accept(α, β, p)—denotes that α accepts to enact the contract p ∈ O.Second, persuasive illocutions from Ipers have the general form:• reward(α, β, p, q)—denotes that α will reward β with q ∈ O ∪ Iα if β accepts the contract p ∈ O and p isenacted. As can be seen, q can either be a deal that is favourable to β or an illocution that will help β in future(e.g. enhance the reputation of β or an unconditional accept of a deal to be presented at a later time).• askreward(α, β, p, q)—denotes that α asks for a reward q ∈ O ∪ Iβ from β if β accepts the offer presented inp ∈ O and p is enacted.3.2.3. World descriptionAs discussed in Section 3.1, the actions or programs performed by agents result in changes in the state of the world.In our model, programs consist of a number of illocutions or contract executions. To represent the consequences oftheses actions we exploit the theory presented by [6]. In their model, the authors prescribe commitments that hold indifferent states of the world and agents are able to navigate between different states through the actions they perform.In short, these actions lead to some commitments becoming true or false. We therefore extend the work of Bentahar etal. to incorporate the notion of commitment in the framework of persuasive negotiation. To this end, we first conceive6 Other mathematical operations such as (cid:3), =, (cid:2) can also be used in contracts as discussed in Section 2.7 Actually, when committing to the execution of a contract an agent α commits to make true those variable bindings of issues that are under theagent’s control (that is, issues in Xα ). However to simplify notation we’ll just represent that the agent is socially committed to the whole contract.8 Whenever we apply an operator to a formula or action, like in Do(ϕ) or later with propose, reward, SC, etc., we actually mean the application ofthe operator over a term representing the formula. This is sometimes represented with the Gödel quotes: Do((cid:15)ϕ(cid:16)). We will, however, abuse notationand omit the quotes.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837811of the set of social commitments that can be made in a dialogue as a result of illocutions being uttered and that canbe retracted as other illocutions are uttered or other actions are executed. At the beginning of a negotiation dialogue(i.e. before any agent says anything), all the commitments are false. As the negotiation proceeds, some will becometrue (active) or false (inactive) according to the illocutions sent. Some commitments might also become false whensome actions are performed after negotiation. In order to represent commitments in the negotiation state we need tointroduce special operators to describe them:• SC(α, β, ϕ, q) ∈ Φ denotes a commitment from α to β to enact q given ϕ is satisfied. Here, q ∈ O ∪ Iα, ϕ =Done(a1) ∧ · · · ∧ Done(an) ∈ Φ to denote that the commitment is conditional upon the enactment of a number ofactions (a1 to an) or ϕ = true to denote that the commitment is unconditional.• Done(a) ∈ Φ where a ∈ Π to denote that action a has been performed.For instance, SC(α, β, Done(propose(α, β, p); accept(β, α, p)), p) means that in case β accepts contract p pro-posed by α then α is also committed to β over the same contract. Moreover, arbitrary compound formulae in Φ canbe constructed from these atomic formulae and formulae in ASG using the standard connectives ∧, ∨, ¬. For exampleSC(α, β, Done(accept(β, α, p)) ∧ Done(p), q) means that α is committed to doing q if β has accepted an offer p andp has been done.Building on these basic elements, the set of states S of the DL framework will be determined, in our setting, bythe truth values of three types of formulae; (i) assignments of values to issues (e.g. (x = v)), (ii) instances of Donepredicates (e.g. Done(p)), and (iii) instances of SC predicates (e.g. SC(α, β, ϕ, p)). Thus, each state of the world canbe described by a (possibly partial) assignment of the values to some issues, actions that have been already performed,and social commitments that are active.Given the definition of the semantics of the PN language, we next describe the axioms that support the basic rulesof our persuasive negotiation protocol.3.3. The PN axiomsWe first explain the three basic axioms regarding the meaning of the operators Do and Done:• [Do(ϕ)]ϕ—after the execution of Do(ϕ), necessarily ϕ is true.• [a]Done(a)—after executing action a, necessarily the formula Done(a) is true.• Done(a; b) → Done(a) ∧ Done(b)—the execution of the action sequence a; b implies that a and b have beenperformed.Next, we capture the relationship between illocutions and social commitments. We avoid the rules depicting theturn-taking procedure that normally happens in negotiation in order to focus on the essential features of the commit-ments with respect to the enactment of proposals and rewards:9• [propose(α, β, p)]SC(α, β, Done(accept(β, α, p)), p).This means that after propose(α, β, p) is uttered, α commits to enact p if β accepts the proposal.• [reward(α, β, p, q)](SC(α, β, Done(accept(β, α, p)), p) ∧ SC(α, β, Done(accept(β, α, p)) ∧ Done(p), q)).This means that after reward(α, β, p, q) is uttered, α commits to its part of the deal p if β accepts the deal p.Moreover, α commits to make the reward q ∈ O ∪ Iα happen once the contract p is made true.• [askreward(α, β, p, q)](SC(α, β, Done(accept(β, α, p)), p) ∧ SC(β, α, Done(accept(β, α, p)) ∧ Done(p), q)).This means that after askreward(α, β, p, q) is uttered, α commits to its part of the deal p if β accepts the con-tract p. Moreover, β commits to make the reward q ∈ O ∪ Iβ happen once the contract p is made true.9 The rules of encounter we use are the ones described in Section 2. The logical representation of these rules could be further formalised usingDL to finer levels of granularity so as to describe turn-taking, deadlines to send new proposals or rewards, and withdrawal from the negotiation.Examples of negotiation protocols that cater for some of these rules can be found in [23,27]. However, here we choose to focus on what we believeto be the bare essentials of a protocol with respect to persuasive negotiation.812S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837We next outline the axioms that specify the dynamics of the social commitments when actions are performed:(cid:4)(cid:4)• Unconditionally committing to enacting a contract or reward:SCα, β, Done(a), p(cid:5)(cid:4)¬SC→ [a](cid:4)α, β, Done(a), p(cid:5)(cid:5)∧ SC(α, β, true, p)In this case, once the action a has been done, α is committed to enacting p (which could be a contract or a reward)without any conditions. This is usually the case when a is an accept of the offer to do p or when a is a contractthat had to be executed before a reward p were to be given.• Conditionally committing to enacting a contract or reward:SCα, β, Done(a) ∧ ϕ, p(cid:5)(cid:4)¬SC→ [a](cid:4)α, β, Done(a) ∧ ϕ, p(cid:5)(cid:5)∧ SC(α, β, ϕ, p)In this case, once action a has been done, α only commits to do p if ϕ is true. This can happen, for example, ifa reward p has been offered and ϕ represents the enactment of the offer (accepted through action a) conditionalupon which the reward p was to be enacted.• Enacting a contract or reward:SC(α, β, true, p) → [p]¬SC(α, β, true, p)This simply means that a commitment to enact a contract or reward is revoked once the contract or reward isenacted.We finally describe the basic axioms that ensure that agents commit to the most up-to-date contract or rewards:• Committing to only one contract at a time:– [propose(α, β, p)]¬SC(α, β, Done(accept(β, α, p(cid:17))), p(cid:17)), for p(cid:17) (cid:18)= p.– [reward(α, β, p, q)]¬SC(α, β, Done(accept(β, α, p(cid:17))), p(cid:17)), for p(cid:17) (cid:18)= p.– [askreward(α, β, p, q)]¬SC(α, β, Done(accept(β, α, p(cid:17))), p(cid:17)), for p(cid:17) (cid:18)= p.These mean that a commitment to a previous offer is retracted when a new contract is offered, or a reward is givenor asked for with a new offer.• Committing to only one reward at a time:– [reward(α, β, p, q)]¬SC(α, β, Done(accept(β, α, p)) ∧ Done(p), q(cid:17)), for q(cid:17) (cid:18)= q.– [askreward(α, β, p, q)]¬SC(β, α, Done(accept(β, α, p)) ∧ Done(p), q(cid:17)), for q(cid:17) (cid:18)= q.These mean that a commitment to a previous reward is retracted when a new reward is given or asked for.Using all the above axioms, it is possible to automatically check what agents are allowed to say or do at any pointduring the negotiation dialogue and after the negotiation has ended. This can be achieved by storing each commitmentincurred during the dialogue in a commitment store and, as new illocutions are issued, these are checked againstthe commitment store to see if they can be accepted and then used to make certain existing commitments active orinactive. Such a mechanism can easily be built into an electronic institution for automated checking (e.g. [10,11,30]).4. The persuasive negotiation strategyThe protocol we have described in the previous section structures interactions between agents as it allows themto understand the messages exchanged and the commitments they make while negotiating. However, protocols, suchas ours, do not give any indication about the content of offers or rewards that agents need to devise in order to reachgood agreements, nor do they indicate when and how to send such offers and rewards (which determine the agents’strategy). Therefore, to complement the protocol, it is important to devise mechanisms to generate and evaluate offersand rewards that they may be committed to enact. In particular, we do so with respect to the following requirements[21]:(1) Techniques must exist for generating proposals and for providing the supporting arguments—this demands thatagents be endowed with strategies to generate offers. Here we will assume no prior information about the oppo-nent (except that of the knowledge of a conflict of preferences and the domain of discourse as per many otherS.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837813models in this area [13,15]). In such situations, the heuristic-based approach has a proven track record of elicitinggood outcomes and so this is the approach adopted here. Generally, these mechanisms assume no knowledgeof the opponent and decide on offers and counter-offers according to the behaviour of the opponent (behaviour-dependent tactics), the deadline of the agent (time-dependent tactics), and the amount of resources available(resource-dependent tactics) [12]. In this section (and later ones) we develop a heuristic that is tailored to theproblem of repeated negotiations.(2) Techniques must exist for assessing proposals and their associated supporting arguments—this means that agentsneed to be able to evaluate the benefit of proposals and rewards to them. This is normally captured by evaluatingthe incoming offers against the agent’s preference structure or utility function. However, as we will see, in repeatedencounters, agents do not know the outcome of future games a priori; that is, there exists some uncertainty aboutsuch outcomes. This uncertainty needs to be taken into account in the decision making of the agents in priorgames. Currently, however, there is no negotiation technique that deals with strategies specifically tailored forsuch repeated encounters, but here we aim to use persuasive negotiation to do so in order to reduce the uncertaintyof future outcomes through the use of rewards.(3) Techniques must exist for responding to proposals and their associated supporting arguments—here again theheuristic-based models have been shown to provide good responses to offers and counter-offers. In particular, wewill give special attention to those heuristic-based models that try to achieve Pareto-efficiency in the bargainingencounter because such models have been shown to take less time to come to better agreements overall [13]. Inso doing, we also aim to develop a bargaining mechanism that seeks the most efficient partitioning of resources.In general, through persuasive negotiation, we give agents a means of influencing future negotiations through re-wards, rather than just exchanging offers and counter-offers that only impact the outcome of the present encounter.Given that negotiation normally occurs over the partitioning of some resource, the rewards, in our case, aim to con-strain this partition by imposing bounds on agreements that could be achieved in future negotiations. Thus, promisesof rewards (asked for or given) partially determine the partitioning of resources to be negotiated at a later time (seeexample in Section 1).To this end, in this section, we develop a Reward Generation Algorithm (RGA) that generates rewards based onoffers calculated by other techniques (such as resource or behaviour-based tactics). Moreover, in Section 5, we developa specific persuasive negotiation strategy that builds upon the RGA to generate both offers and rewards.From this section onwards, we will focus on the specific features of repeated negotiation games described inSection 2 and abuse the notation slightly to denote the set of outcomes in the first game by O1 and those in the secondby O2 (On in the more general case). Thus, in the specific setting we consider, the proposal p and reward q specifiedby persuasive illocutions such as reward(α, β, p, q) and askreward(α, β, p, q) are such that p ∈ O1 and q ∈ O2.10 Inso doing, what we represented as a reward in Section 3, for example q ∈ O for a reward given by α, is now translated toa set of constraints (using operators (cid:3), =, (cid:2)) that α will abide by in a contract O2 ∈ O2. Similarly, normal negotiationillocutions such as propose(α, β, p) and accept(α, β, p) only consider offers from the first game, that is, p ∈ O1.Given this, we first discuss when rewards can justifiably be used to persuade an opponent and then move on todescribe how such rewards are generated by combining the different components of the RGA. Finally, we deviseevaluation functions to assess the utility that can be obtained from rewards and the offers they support. In so doing,we describe how agents decide whether to counter-offer or accept a given offer.4.1. When to use rewardsIn PN, agents try to give rewards or ask for rewards in order to get their opponent to accept a particular offer.Rewards are about giving a higher utility outcome to an opponent (when given) or a higher utility to the agent askingfor it in the second game. Given this, rewards are specified in the second game in terms of a range of values for eachissue. Thus, giving a reward equates to specifying a range such as vx > 0.5 for issue x in O2 ∈ O2 to an agent whoseutility increases for increasing values of x. Conversely, asking for a reward means specifying a range such as vx < 0.4in O2 for the asking agent (whose utility increases for decreasing values of x). Now, agents may find it advantageous10 Here we do not consider rewards which could be illocutions as suggested in Section 3, but these could easily be implemented by extending theproposed solution. Such an extension will be considered in future work.814S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837Fig. 1. Determining the outcome of the second game according to the offer made in the first game.to accept such rewards if it costs them more to counter-offer (due to their discount factor) or if they risk passing theirdeadline (or their opponent’s). Here, we do not deal with the issues related to whether the agents keep to their promisesor how to tackle the uncertainty underlying this (we simply assume they do), but rather we focus on the reasoningmechanism that the agents require in order to negotiate using rewards. In more detail, a reward can be given or askedfor in the following contexts:• A reward is proposed when the agent can still manage to achieve its target L after reaching an agreement andgiving the reward. This may happen if agent α is asking β to concede in the first game, giving α more utilityin the first game. Agent α may then afford to forsake some utility in the second game (which it values less dueto discounting effects). It may do so by conceding in the second game and this acts as a reward. Note here thatthe reward may cost the sender something as well and it therefore needs to estimate the cost of this reward withrespect to Lα properly before committing to giving the reward.• A reward can be asked for by an agent if it is able to concede in the first game so as to catch up in the secondone. In this case, the agent asking for the reward has some costs in conceding in the first game and entices theopponent to pledge to something in return (a concession in the second game) for the concession in the first game.The agent asking for the reward also needs to ask for a reward that is commensurate with its target and the levelof concession it is making.The above reasoning is captured in Fig. 1. As can be seen, given a contract O1 offered by α, a reward from α toβ would be to propose a negotiation range that is more favourable to β (i.e. make offers with high utility for β) inthe second game. The agreement reached in the first game would then be of higher utility for α. The converse applieswhen agent α asks β for a reward. These procedures can be seen as a trade-off mechanism often used in negotiationwhereby agents trade-off gains in the present (or the future) in return for gains in the future (or in the present) [33]. Ingeneral, there are three main ways agents stand to gain from using rewards in this manner:(1) Agents may be able to reach an agreement faster in the first game by providing some guarantees over the outcomeof the second game. For example, when α specifies that it will negotiate for only a third of the pie in the secondgame, β might prefer to accept this offer instead of delaying the negotiation as it would result in both the first andsecond pie being worth a significant amount less than its target Lβ . This, in turn, reduces negotiation time andhence the less discounted is the outcome in the first and second games.(2) The negotiation mechanism can be more efficient in that it allows agents to explore a larger negotiation space overwhich they may have different preferences. This may happen particularly if α has a lower discount factor than β.For example, β can trade-off a third of the second pie, which its opponent values more, against higher profits inthe first game.(3) Agents may be able to reach an agreement faster in the second game, since not much of the negotiation space isleft to be searched if a reward has been given or asked for. For example, α and β only have to negotiate over athird of the pie in the second game rather than the whole pie.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837815Require: O1 ∈ O1, L1: Compute concessions in Oα1 and Oβ1 . % Here the agent determines how much both agents concede on theissues for which they have a higher and lower |δU | than their opponent.2: Select O2 ∈ O2 that matches the level of concession in O13: Check whether the combination of O1 and O2 satisfies L, adjust [vmin, vmax] for second game according to values in O2 and send offer andreward.4.2. The reward generation algorithmAlgorithm 1. Main steps of the RGA.Building on the reasoning mechanism presented in Section 4.1, we now develop our reward generation algorithm(RGA). Its role is to determine the level of concession made in the first game, and hence set the value of the corre-sponding reward, and to decide whether to send it or not. First, we assume that an agent has some means of generatingoffers O1 which comply with its negotiation ranges for each issue. These can be generated using what is termed anegotiation tactic [12]. In line with much work on negotiating in the presence of deadlines, we assume the agent’snegotiation tactic concedes to some extent until an agreement is reached or the deadline is passed. Then, at each stepof the negotiation, based on the concessions made in an offer O1 ∈ O1, RGA computes the reward O2 ∈ O2 anddecides if it is to be asked for or given. In more detail, Algorithm 1 outlines the main steps of RGA which are thendetailed in the following subsections.4.2.1. Step 1: Compute the concession degreesIn this context, the degree to which an agent concedes in any game is equivalent to the value it loses on some issues1 ) ∈ O1 is the value of anmin] is its negotiation range, then we define:to its opponent relative to what the opponent loses to it on other issues. Assuming (x = vxissue x, and [vxmax, vx(cid:5)(cid:4)vx1(cid:4)(cid:4)(cid:6)vxvx= max, UxUxmaxmin(cid:4)(cid:5)(cid:4)(cid:6)vxvx= min, UxUxmaxminU x1U xU x= Uxmax(cid:5)(cid:7)(cid:5)(cid:7)min(cid:5)From these, we can compute the maximum an agent could get as:(cid:2)Umax =wxU xmaxx∈X(O1)the minimum as:Umin =(cid:2)wxU xminx∈X(O1)and the actual utility as:(cid:2)U1 =wxU x1x∈X(O1)(cid:3)wx = 1. These weights can be ascribed the same values given to thewhere wx is α’s relative weight of issue x andweight the issue has in the utility function (see Eq. (1)) and can be normalised for the number of issues consideredhere. Then, the concession degree on the offer O1 is computed as:con(O1) = Umax − U1Umax − Umin(2)It is then possible to calculate concessions on issues with higher and lower |δU | for α using conα(Oα1 ) and 1 − conα(Oβrespectively. Then, the complement of conα(Oαmuch β concedes to α from α’s perspective (or how much α exploits β).1 ) (i.e. 1 − conα(Oα1 ) or conα(Oβ1 ) and conα(Oβ1 )1 )) represents how816S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–8374.2.2. Step 2: Determine the rewardsTo determine which agent concedes more in the game (given that they play a MMPD), α needs to compare itsdegree of concession on the issues with higher |δU | than β (i.e. Oβ1 ) (ina zero sum game this is calculated for all issues). This means determining what are the different conditions whenconα(Oβ ) is compared with the concession (1 − conα(Oα)) of β (as perceived by α). To this end, we define threeconditions which refer to the case where α concedes as much as β (COOP) (i.e. it cooperates), concedes more to β(CONC) (i.e. it concedes), and concedes less than β (EXPL) (i.e. it exploits) respectively as follows:1 ) and those with lower |δU | than β (i.e. Oα• COOP = true if conα(Oα• CONC = true if conα(Oα• EXPL = true if conα(Oα1 ) + conα(Oβ1 ) + conα(Oβ1 ) + conα(Oβ1 ) = 1 (i.e. α has no grounds to give or ask for a reward).1 ) > 1 (i.e. α should ask for a reward).1 ) < 1 (i.e. α should give a reward).The above conditions capture the fact that an agent can only ask for a reward if it is conceding in the first game andcan only give one if it is exploiting in the first game. It is possible to envisage variations on the above rules as agentsmay not always want to give a reward to their opponent if they are exploiting in the first game or they may want toask for one even if they are not conceding. However, these behaviours could be modelled in more complex strategies(which we will consider in future work). But, in so doing, an agent may also risk a failed negotiation. Here, therefore,we focus on the basic rules that ensure agents try to maximise their chances of reaching a profitable outcome.Now, having determined whether an argument is to be sent or not and whether a reward is to be asked for or given,we can determine the value of the reward. Given that an agent α aims to achieve its target Lα, the value chosen for a1 ), conα(Oβreward will depend on L and on (conα(Oα1 )) (i.e. the degrees of concession of the agent). We will considereach of these points in turn (and ignore the agent identifier where it is clear from the context).Given O1, the first game standing offer, the minimum utility α needs to get in the second game is l2 = L − U (O1).We then need to consider the following two cases (remember e−(cid:7)(θ+t) is the maximum that can be obtained in thesecond game with discounts):(1) If l2 (cid:3) e−(cid:7)(θ+τ +t) it is still possible for α to reach its target in the second game (provided the agents reach anagreement in the first one) and, therefore, give (or ask for) rewards as well. The larger l2 is, the less likely thatrewards will be given (since less can be conceded in the second game and still achieve L). Note that τ is added tothe discounting effect to denote that an agent will take some time to send the next illocution.(2) If l2 > e−(cid:7)(θ+τ +t), it is not possible to give a reward, but an agent may well ask for one in an attempt to achieve avalue as close as possible to l2.l2For now, assuming we know l2 (cid:3) e−(cid:7)(θ+τ +t), it is possible to determine how much it is necessary to adjust thenegotiation ranges for all or some issues in O2 in order to achieve l2. Specifically, the agent calculates the undiscountede(cid:7)(θ+τ +t) it needs to get in the second game. Then, it needs to decide how it is going to adjust the utilityminimum utilityit needs on each issue, hence the equivalent bound vout for each issue, in order to achieve at leaste(cid:7)(θ+τ +t) . Here, wechoose to distribute the utility to be obtained evenly on all issues. Other approaches may involve assigning a highervout (hence a higher utility) on those issues which have a higher weight in the utility function. In so doing, voutmay constrain the agent’s ranges so much for such issues that its negotiation ranges may not overlap with that of itsopponent and result in no possible agreement between them. Our approach tries to reduce this risk. Thus, the requiredoutcome vout of an issue in the second game can be computed as:(cid:9)(cid:8)l2(3)vout = U−1xl2e−(cid:7)(θ+τ +t)Having computed the constraint vout, the agent also needs to determine how much it should reward or ask for. Tothis end, the agent computes the contract ¯O which satisfies the following properties:(cid:4)¯Oβ2and conα(cid:4)¯Oα2(cid:4)Oα1= conα= conαconαOβ1(cid:5)(cid:5)(cid:5)(cid:4)(cid:5)This is equivalent to our heuristic described in Section 4.1 where the level of concession or exploitation in the offer in1 ) is mapped to the reward asked for or given in the second one (i.e. here ¯O2 =the first game (i.e. here O1 = Oα1∪ OβS.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837817∪ ¯Oβ¯Oα2 ). Then, assuming linear utility functions and finite domains of values for the issues, the above procedure2is equivalent to reflecting the level of concession on issues with higher |δU | by α onto those with higher |δU | for β.This is the same as inverting Eq. (2) given a known Umax and Umin (as defined in step 1), and finding vx1 by assigningU x1 for each issue (a procedure linear in time with respect to the number of issues considered).1Let us assume that for an issue x this results in a bound vr (a maximum or minimum according to the type of argumentto be sent). Thus, from ¯O2, α obtains bounds for all issues in the rewards it can ask from or give to β. Given this, wewill now consider whether to send a reward based on how vr and vout compare.= U1 and inverting U x4.2.3. Step 3: Decide whether to send the offers and the rewardsAssume that α prefers high values for x and β prefers low ones and that it has been determined that a rewardshould be offered (the procedure for asking for the reward is broadly similar and we will highlight differences wherenecessary). Now, α can determine whether a reward will actually be given and what its value should be according tothe following constraints:(1) vr (cid:2) vout: α can promise a reward defining an upper bound vr on the second game implying that α will not askfor more than vr . This is because the target vout is less than vr and α can, therefore, negotiate with a revised upperbound of v(cid:17)= vout. When asking for a reward, α will ask for a lower bound vrmax(i.e. v(cid:17)= vr ) and negotiate with its normal upper bound vmax in order to achieve a utility that is well above itstarget.= vr and a lower bound of v(cid:17)minmin(2) vr < vout: α cannot achieve its target if it offers a reward commensurate with the amount it asks β to concede inthe first game. In this case, α revises its negotiation ranges to v(cid:17)= vout (with vmax remaining the same). Thus,the agent does not send a reward but simply modifies its own negotiation ranges. Now, if it were supposed to askfor a reward, α cannot achieve its target with the deserved reward. However, it can still ask β for the reward vr (asa lower bound) and privately bound its future negotiation to v(cid:17)= vout while keeping its upper bound at vmax. Inso doing, it tries to gain as much utility as possible.11minminNow, coming back to the case where l2 > e−(cid:7)(θ+τ +t) (implying vout > vr as well), the agent that intends to ask fora reward will not be able to constrain its negotiation range to achieve its target (as in point (2) above). In such cases,the negotiation range is not modified and the reward may still be asked for (if CONC = true).Given the above final conditions, we can summarise the rules that dictate when particular illocutions are used andnegotiation ranges adjusted, assuming an offer O1 has been calculated and O2 represents the associated reward asshown in Algorithm 2. With all this in place, the next section describes how the recipient of the above illocutionsreasons about their contents.4.3. Evaluating offers and rewardsHaving discussed how agents would generate rewards, we now describe how an agent evaluates the offers andrewards it receives. Generally, when agents negotiate through the standard alternating offers protocol, the proponentaccepts an offer from its opponent only when the next offer the proponent might put forward has a lower (discounteddue to time) utility for itself than the offer presented to it by their opponent. This is expressed as in Rule 1.However, agents using persuasive negotiation also have to evaluate the incoming offer together with the rewardthey are being asked for or are being given. From the previous section, we can generally infer that a reward impliesa value vr that defines either a lower or an upper bound for a given issue in the next negotiation game. For example,a reward to be given by a seller might be a guaranteed discount (i.e. a lower limit price) on the next purchase by thecurrent buyer which could also have been a reward requested by the buyer. Therefore, given this bound, the agent mayinfer that the outcome of any given issue will lie in [v(cid:17)min, v(cid:17)] which might be equivalent to or different from theagent’s normal negotiation ranges [vmin, vmax] and may take into account the agent’s target vout (given its target l2) orthe value vr itself.max11 The difference between the constraint applied by the reward and by the target is that the reward applies the constraint to both agents, while thelatter only applies separately to each agent according to their individual targets.818S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837if COOP or (EXPL and vout > vr ) for all x ∈ X(O2) thenpropose(α, β, O1).end ifif CONC and l2 (cid:3) e−(cid:7)(θ +τ +t) thenaskreward(α, β, O1, O2) and modify [vmin, vmax] for second game.end ifif CONC and l2 > e−(cid:7)(θ +τ +t) thenaskreward(α, β, O1, O2).end ifif EXPL and vout (cid:3) vr for all x ∈ X(O2) thenreward(α, β, O1, O2) and modify [vmin, vmax] for second game.end ifAlgorithm 2. Step 3 of RGA.if U (Onext) · e(cid:7)β (τ +t) (cid:3) U (Ogiven) · eby α and Onext is β’s possible next offer−(cid:7)β t then % U (Ogiven) is the offer givenaccept(β, α, Ogiven)end ifRule 1. Accepting an offer in the usual case.maxmin, v(cid:17)Generally, we can assume that given a negotiation range [v(cid:17)], an agent may be able to define an expectedoutcome of that range using a probability distribution (e.g. uniform, normal, gamma) or some reasoning based on itsnegotiation strategy (e.g. a conciliatory strategy would expect a lower utility gain in the second game as compared toa non-conciliatory one when faced with a non-conciliatory opponent). This probability distribution may be estimatedfrom previous interactions with the opponent or knowing the behaviour of the opponent’s bargaining strategy and itsrelationship with the agent’s own bargaining position [17,33]. Given this expected outcome for any issue, the agentmay then calculate the expected utility (determined according to the bounds set by the reward) of that reward alongwith the utility of the offer to which it is tagged. Moreover, using the same procedure it can calculate the expectedutility of any reward or offer that it might want to send next. By comparing the two sets of utilities, it can then make adecision as to whether to accept or counter-offer in the next step. We detail such a procedure as follows.Assume β is the agent that is the recipient of a reward (given or asked for) and that β prefers small values for the2 in theissue x being considered. Then, let β’s negotiable range be [vmin, vmax] for the issue x and β’s target be lβsecond game (which implies that it needs at least vβout for the issue in the second game).Now, if β receives reward(α, β, O, Oa) (or askreward(α, β, O, O(cid:17)r is the upper bound proposed by α for each issue x in Oa (vαa). In the meantime, β has calculated another offer Onew with a reward Ob in which a bound vβa)), meaning that Oa is its reward for the secondgame, then Oa implies that vαr would be a lower boundin O(cid:17)r is to be givento each issue x in Ob. Then, for each issue x, β calculates the negotiable ranges for the second game given vαr asa is asked for) while it calculates [vβ[vmin, vαr . We assumer] (orβ can then calculate (using a probabilistic technique) the expected outcome of each range as evα[vαout, vmax}]. Given each of these expected outcomesfor each issue, the overall expected outcomes, EOa and EOb, of the second game can be calculated for each type ofreward respectively as:(cid:2)r , min{vout, vmax}] in the case of O(cid:17)r , min{vout, vmax}] if O(cid:17)out, vmax}] given vβx for [vmin, vαra) and evβr , min{vβr , min{vβx for [vβ] (or [vαU (EOa) =wx · U(cid:5)(cid:4)evαxU (EOb) =x∈X(EOa )(cid:2)x∈X(EOb)wx · U(cid:5)(cid:4)evβx(4)(5)where EOa is the expected outcome of the reward given by α, EOb is the expected outcome of the reward given by β,(cid:3)wx = 1 and wx is the weight given to each issue in the utility function (as per Eq. (1)). These weights for thesecond game may be different from those used in evaluating offers in the first game and if this is known in advance,S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837819if U (Onew) · e−(cid:7)β (τ +t) + U (EOb) · e−(cid:7)β (θ +τ +t) (cid:3) U (O) · e−(cid:7)β t + U (EOa ) · e−(cid:7)β (θ +t) thenaccept(β, α, O)elsereward(β, α, Onew, Ob) or askreward(β, α, Onew, Ob)end ifRule 2. Evaluating a received reward when about to give or ask for a reward.if U (O(cid:17)new) · eaccept(β, α, O)−(cid:7)β (τ +t) + U (EO(cid:17)b) · e−(cid:7)β (θ +τ +t) (cid:3) U (O) · e−(cid:7)β t ) + U (EOa ) · e−(cid:7)β (θ +t) thenelsepropose(β, α, O(cid:17)new)end ifRule 3. Evaluating a received reward when about to send a normal offer.the agent will have to compute the value of expected outcomes in the second game with the future weights in order tobe consistent.Given that the expected outcomes have been calculated, then the agent decides to accept or counter-offer usingRule 2. This evaluates the offer generated against the offer received to decide whether to accept the offer received orsend the reward illocution (note the addition of discount factors to reflect the time till the next game and in sendingthe counter-offer). Note that the same principle applies if the agent were about to send an askreward instead.Finally, we consider the case where agent β has received a persuasive offer and can only reply with another offerwithout any argument. In this case, β calculates the expected outcome of the second game without any constraints(i.e. using its negotiation range [vmin, vmax] to elicit EO(cid:17)b). Rule 3 therefore compares the utility of the offer receivedagainst the utility of the offer generated and the outcome expected in the next game to decide whether to propose orto accept. Note here that the second game is left more uncertain in this case since the bounds have not been changedby any reward. This means that the agent cannot guarantee that it will meet its target and can also result in the agentstaking more time to reach an agreement in the second game (as in the case of non-persuasive tactics as we showin Section 6). As we have seen in this section, the generation of rewards and evaluation of offers assume that thereis an offer based upon which rewards can be computed. Given this, in the next section, we discuss and remove thisassumption by developing a novel tactic that uses the RGA to generate offers and rewards.5. The reward-based tacticAs described in the previous section, RGA requires an offer generated by some negotiation tactic in order togenerate the accompanying reward. In this vein, the most common heuristic-based tactics can be classified as:(i) behaviour-based (BB)—using some form of tit-for-tat or (ii) time-based—using Boulware (BW) (concedes littlein the beginning before conceding significantly towards the deadline) or Conceder (CO) (starts by a high concessionand then concedes little towards the deadline) [12].12 Now, many of these tactics engage in positional bargaining [17]by starting from a high utility offer for the proponent (here α) and gradually conceding to lower utility ones. In turn,this procedure automatically causes RGA to start by promising rewards and then gradually move towards asking forrewards. This is because these tactics generate offers that are exploitative at the beginning of the negotiation. As theagent gradually concedes on its initial offer during the negotiation, the reward generation mechanism would ask forrewards instead. Thus, it is not possible for these tactics to ask for rewards at the beginning of the negotiation. This cansignificantly reduce the efficiency (in terms of the sum of utilities of the agents) of the negotiation encounter since oneof the agents may be better off conceding the second game if it has a low discount factor (cid:7) and, in return, exploit thefirst game (as discussed earlier in Section 1). This would mean that the more patient agent (i.e. the one with a lower12 Other negotiation tactics might also be resource-based or dependent on other factors. The tactics we select here have been chosen because theyare among the most common studied in the literature [12,33].820S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837discount factor (cid:7)) could ask for a reward in the second game or the other agent could offer a reward in the secondgame.To ground our work, we present a novel reward-based tactic (RBT) (based on Faratin’s trade-off tactic [13]) thateither asks for or gives a reward at any point in the negotiation in order to reach an agreement. To do so, however, theagent needs to know how to evaluate incoming offers and generate counter-offers accordingly. We will consider threemain cases in calculating the best response to an offer and a reward. These are:(1) An offer and a reward have been received and it is possible to counter-offer with a reward.(2) It is not possible to counter-offer with a reward and the last offer involved rewards.(3) It is not possible to counter-offer with a reward and the last offer did not involve rewards.We show how the algorithm deals with each of these cases in turn.5.1. Case 1: Counter-offering with a rewardIn this case, an offer and a reward have been received and it is possible to counter-offer with a reward (according tothe RGA). Thus, an agent α needs to calculate combinations of rewards asked for or given with offers and choose thecombination which it deems most appropriate to send to β. To calculate these combinations, α first needs to determinethe overall utility each combination should have. To achieve this, we use a hill climbing method similar to Faratin etal.’s [13] model. In this method, the agent tries to find an offer that it believes is most favourable to its opponent,while not necessarily conceding too much. In our case (particularly for utility functions based on the MMPD), thisprocedure equates to the agent trying to gain more utility on the issues on which it has a higher |δU | and less onthose for which it has a lower |δU | than β.13 In so doing, the strategy tries to maximise joint gains in the repeatednegotiation encounter.Therefore, to calculate the best combination of offer and reward for an agent α to send in the hill-climbing approach,α first calculates the utility of the next offer it intends to send and then finds the offer and reward that optimally matchthis utility value. By optimality, in this case, we mean that either the offer or the reward should also be the mostfavourable one to β. Thus, the utility of the next offer is calculated according to the difference that exists betweenα’s previous offer and the last one sent by β and the step in utility α wishes to make from its previous offer. The sizeof this utility step can be arbitrarily set. Given a step of size f ∈ [1, ∞], the utility step is calculated by the functionSu : O1 × O2 × O1 × O2 × [1, ∞] → [0, 2] as follows:Su(O1, O2, O(cid:17)1, O(cid:17)2, f )e−(cid:7)t (U (O1)e−2(cid:7)τ + U (EO2)e−(cid:7)(θ+2τ ) − U (O(cid:17)1)e−(cid:7)τ − U (EO(cid:17)2)e−(cid:7)(θ+τ ))=f(6)2 are the current offer and the expected outcome of β’s reward O(cid:17)where O1 and EO2 are α’s previous offer and expected outcome in the second game from α’s reward O2 respectively,1 and EO(cid:17)O(cid:17)2 respectively. In case Su returns zero or anegative value, α would accept the offer and reward (after applying the evaluation rules defined in Section 4.3). Whena reward is not specified by the agents, the utility calculated by the function only considers the offers made by eachagent (i.e. remove U (EO(cid:17)) and U (EO(cid:17)2) from its calculation).Given the utility step Su, it is then possible to calculate the utility Nu of the combination of the next offer andreward using the following equation:Nu = U (O1)e−(cid:7)(2τ +t) + U (EO2)e−(cid:7)(θ+2τ +t) − Su(O1, O2, O(cid:17)1, O(cid:17)2, f )(7)Given that rewards specify bounds on the negotiation in the second game, each combination that can be offered ina step represents a space of possible agreements in the second game given an offer in the first one. Therefore, findinga combination that more closely matches the opponent’s offer and reward equates to finding another space of offersthat is close to the opponent’s space that covers its latest offer and reward. This procedure is pictured in Fig. 2.13 Note this is different from the point discussed in Section 4.2.2 since here we do not constrain the negotiation ranges, but rather search for offersthat may be profitable to both parties.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837821Fig. 2. The hill climbing performed by RBT for an agent α to find an appropriate reward and offer in response to the offer and reward by agent β.The shaded semi circles represent spaces over which different offers and rewards have the same utility for α. Each new offer by α is made closer toagent β’s previous offer.Given previously received and proposed offers and rewards, find (O1, O2) such that:– maximise conα(Oα– maximise conα(Oβ2 ) to give a reward to β.2 ) to ask for a reward from β.subject to:– U α(O1, O2) = Nu– ∀(x = v) ∈ O1, O2, vmin (cid:3) v (cid:3) vmax % i.e. all values need to be within the negotiation range.Optimisation Model 1. Computing the best counter-offer and reward.As can be seen in this figure, in our tactic, α calculates the most favourable combination of offer and reward foragent β that achieves the utility Nu. In so doing, our tactic aims to make offers that are closest to those preferredby β in a few steps without losing much utility. In calculating a reward to be given we take into account the factthat in the MMPD the opponent likes some issues more than others and by maximising the opponent’s gain on theseissues we ensure that the reward is more attractive to the opponent. In the same way, when a reward is asked for,the associated offer is calculated such that the values of the issues in the offer are more favourable to the opponenton those issues it prefers most according to the MMPD. To calculate these offers and rewards, we solve the problemdefined by Optimisation Model 1 using Linear Programming techniques in order to calculate the reward that is eithermost favourable to β or to α. Algorithm 1 therefore runs through the RGA to find the best possible rewards and theassociated offers whose combined utility are equal to Nu. However, Algorithm 1 can also fail to find an optimal output(as a result of the constraints being too strong (e.g. the target L being too high) or the optimiser not being able to findthe solution in the specified number of steps) and, in these cases, we resort to the procedure described in case 2.5.2. Case 2: Counter-offering without rewards given previous rewardsIn this case, the agent cannot find a combination of an offer and a reward whose utility matches Nu. Therefore,the agent calculates an offer using one of the standard heuristic-based tactics outlined at the beginning of this section.In this case, BB tactics would not be appropriate to generate an offer given previous offers by the opponent sincethese offers may also be associated to rewards. This means that the offers by themselves (which would be used inBB to calculate the next offer) do not exactly depict the concessions that the agent has made leading to BB tacticsmisunderstanding the behaviour of the opponent. This, in turn, could lead to an offer by a BB agent where it concedesmore than it should. Therefore, either BW or CO are used to generate the offer since these are independent of theprevious offers made by the opponent.822S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837Given previously received and proposed offers, find (O1) such that:– maximise conα(Oα1 ) % i.e. maximise α’s concessions on issues β has a high |δU |.subject to:– U α(O1) = Nu– ∀(x = v) ∈ O1, vmin (cid:3) v (cid:3) vmax % i.e. all values need to be within the negotiation range.Optimisation Model 2. Computing the best counter-offer.5.3. Case 3: Counter-offering without rewards given no previous rewardsIn the event that β only proposes an offer without any rewards, our tactic needs to be able to respond by a similarprocedure (as in case 1) in order to continue the same step-wise search for an agreement. In this case, our tacticcalculates the offer whose utility is equal to Nu (without U (EO(cid:17)2) in Eq. (7)). Moreover, the offer calculated is such thatit is the one that is most similar to the offer by β. This is achieved by solving the problem defined in the OptimisationModel 2. This calculates an offer O1 such that O1 maximises the level of concession the opponent likes most as inthe previous case while still achieving Nu. In case the issues being negotiated are qualitative in nature, the similaritybased algorithm by [13] may be used.6. Experimental evaluationIn this section, we describe a series of experiments that aim to evaluate the effectiveness and efficiency of ourPN reasoning mechanism. To this end, we pitch agents using the RGA and RBT against a number of non-persuasivenegotiation tactics using standard benchmark metrics. We first detail the experimental settings and describe the typesof agents we benchmark our algorithm against as well as the metrics used in our tests. Given this, we provide theresults of these tests and go on to analyse the performance of the RBT under different parameter settings.6.1. Experimental settingsThe scenario we consider involves agents playing two negotiation games as per the rules discussed in Section 2.The general settings that apply to the two negotiation games are as follows:• The pair of negotiating agents have their utility functions shaped by the MMPD (as discussed in Appendix A).The actual utility the opponent obtains for particular values of the issues are not known since utilities are private.Thus agents α and β negotiate over 4 issues x1, . . . , x4 where x1 and x2 (e.g. price or bandwidth) are more valuedby α than β, while x3 and x4 (e.g. usage of service or time of payment), are more valued by β than α.• The agents have their utility functions U α and U β specified over each issue as per Table 2. As can be noted, theweights and gradients of the utility functions are chosen such that they respect the conditions of the MMPD (asdetailed in Appendix A).• The maximum time for a negotiation game to take place (tmax) is set to 2 seconds, which allows around 300dead andillocutions to be exchanged between the two agents.14 Unless stated otherwise, the agents’ deadlines, t αt βdead, are then defined according to a uniform distribution between 0 and 2 seconds.• (cid:7)α and (cid:7)β —the discount factors are set to a value between 0 and 1 drawn from a uniform distribution (unlessstated otherwise).• Lα and Lβ —the targets of the agents are drawn from a uniform distribution between 0 and 2 (unless statedotherwise).• θ and τ —θ is set to 0.5 seconds (meaning that the second game is discounted by e−0.5(cid:7) ) for each agent while τ isset to 0.0001 (meaning that the utility of each offer is discounted by e−0.0001(cid:7) ) to simulate instantaneous replies(unless stated otherwise).14 Experiments were run using MATLAB 7.1 on a 2 GHz Intel PC with 1 GB of RAM. Preliminary experiments with the negotiation tacticssuggest that if the agents do not come to an agreement within this time period, they never achieve any agreement even if the maximum negotiationtime is extended.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837823Table 2Utility functions and weights of issues for each agentAgentαβUtility function and weight of each issueUx1 , wx10.4x1, 0.51 − 0.2x1, 0.4Ux2 , wx20.9x2, 0.21 − 0.6x2, 0.1Ux3 , wx31 − 0.2x1, 0.20.9x2, 0.3Ux4 , wx41 − 0.6x2, 0.10.4x1, 0.2• [vmin, vmax]—the negotiation range for each issue and each agent are defined (and privately known) using λ,the degree of alignment of the negotiation ranges. For example, if λ = 1, the two negotiation ranges overlapcompletely, while if the degree of alignment is 0, the negotiation ranges do not overlap at all. The degree ofalignment is arbitrarily set to 0.8 to represent the fact that agents have a reasonably large set of possible agreementsthat they could reach and still achieve their target.We will further assume that the first offer an agent makes in any negotiation is selected at random (but having a highutility for the agent). Also, the first agent to start the negotiation is chosen at random. This random choice reduces anypossible first-mover advantage a strategy may have over another (i.e. which loses less utility due to discount factors).Moreover, in order to calculate the expected outcome of the second game (as discussed in Section 4.3), agents drawthe outcome for each issue from a normal distribution with its mean centred in the middle of the agent’s negotiationrange for the second game with a variance equal to 0.5. Finally, in all our experiments we use ANOVA (ANalysis OfVAriance) to test for the statistical significance of the results obtained.6.2. Populations of negotiating agentsIn order to benchmark the RBT against standard negotiation tactics, we create three groups of agents. First, wecreate agents which use RBT to negotiate in the first game. These agents then use any of the standard tactics (discussedin Section 5) in the second game. Second, we create a group of agents, called PNT (for Persuasive Negotiation Tactics),which use the RGA rewards. They do so by generating offers using standard tactics (BB, BW, or CO as defined inSection 5) and plug in such offers in the RGA to obtained the compatible rewards. In the second game, PNT agentssimply use the same standard tactics to generate offers. Third, we create a group of agents, called NT (for NegotiationTactics), which only use standard negotiation tactics to generate offers in both games (see [12] on how to implementstandard tactics in more detail).In the following experiments, we use homogeneous populations of 80 agents for each of NT, PNT, and RBT andalso create a heterogeneous population of equal numbers of RBT and PNT agents (40 each) which we refer to asPNT&RBT to study how RBT and PNT agents perform against each other.6.3. Efficiency metricsAs argued in Section 1, one of goals of PN is to achieve better agreements faster than standard negotiation mech-anisms. To test whether our PN model achieves this, we use the following metrics:• Average number of offers—this is the average number of offers that agents need to exchange before coming toan agreement. To calculate this, we record the number of offers made each time an agreement is reached andcalculate the average of these over the total number of negotiations. Note that each time an offer is made a shorttime τ elapses. A lower average equates to a shorter time before agents come to an agreement (mutatis mutandisif the average is high). Moreover, the lower this average, the lower is the loss in utility as a result of the discountfactors (cid:7). Thus we can define a time-efficient tactic as one that takes a relatively small number of offers to reachan agreement.• Success rate—this is the ratio of agreements reached over all pairs of games to the number of times agents meetto negotiate. The larger this success rate, the better the negotiation tactic is at finding an attractive offer for theopponent.• Average utility per agreement—this is the sum of the utilities of both negotiating agents over all agreementsdivided by the number of agreements reached. The higher this value, the better is the strategy at finding an824S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837outcome that brings a high utility to both participating agents. Thus we define a socially efficient negotiationtactic as one which brings a high sum of utility in the outcome.• Expected utility—this is equal to the average utility weighted by the probability that an agreement is reached.The probability is calculated by dividing the total number of agreements by the number of encounters agentshave. Thus, if the agents find an agreement on all encounters, there is a probability of 1 that they will come to anagreement in a future encounter. A strategy with a high expected utility is one which is most likely to reach highutility agreements every time it meets other strategies.Having defined our evaluation metrics, we next detail the results of our experiments.6.4. Comparing persuasive and non-persuasive strategiesWhen agents play two negotiation games, in the first one, NT (without the reward generation mechanism) is onlyable to make offers and evaluate offers, while PNT is able to both generate and evaluate offers and rewards. Giventhat persuasive strategies like PNT and RBT can constrain their rewards according to their target L (as shown inSection 4.2.2), we also need to allow other non-persuasive tactics to constrain their ranges accordingly to ensure a faircomparison. Thus, we allow all tactics to constrain the ranges of the issues in the second game according to their targetwhenever they reach agreements without the use of any rewards (i.e. using only a propose illocution). The procedureto do so is similar to that described in Section 4.2.2 where vout, as calculated in Eq. (3), is used as the bound on thenegotiation range of the second game but without the use of rewards.Given this, we postulate a number of hypotheses regarding the performance of RGA and RBT and describe theresults which validate them.Hypothesis 1. Negotiation tactics that use the RGA are more time efficient than those that do not.deaddead= t βThis hypothesis follows from the fact that we expect rewards to help agents find an agreement faster. We impose the= 1 s, (cid:7)α = (cid:7)β = 0.1, θ = 1 s, and λ = 0.8.following basic settings on the interactions: Lα = Lβ = 0.8, t αThese settings are chosen to represent symmetric conditions for both agents and impose relatively few constraints onthe two negotiation games that agents play. The symmetric nature of the interaction ensures that no tactic is in a moreadvantageous position to its opponent. Here we recorded the average number of offers (the lower this number themore time efficient the agents are) an agent makes in order to reach an agreement. For all populations of tactics, eachagent meets another agent 50 times and this is repeated 15 times and the results are averaged. We recorded the resultsin Table 3. Thus, it was found that NT takes an average of 547 offers to reach an agreement, while PNT agents take58 and the combined PNT and RBT population takes around 56 offers per agreement. The performance of only RBTagents is significantly better than the other populations since they reach agreements within only 26 offers (which isless than NT by a factor of 21).15 These results validate Hypothesis 1. Now, the reason for the superior performanceof persuasive tactics in general is that the rewards make offers more attractive and, as we expected, the shrinkage ofnegotiation ranges in the second game (following from the application of the rewards) further reduces the negotiationspace to be searched for an agreement. The additional improvement by RBT can be attributed to the fact that everyRBT agent calculates rewards and offers (through the hill-climbing algorithm) that give more utility to its opponentson issues for which they have a higher marginal utility (as explained in Section 5). Hence, this is faster than forPNT&RBT in which only one party (the RBT) performs the hill-climbing.These results suggest the outcomes of RBT and PNT populations should be less discounted and should also reachmore agreements (since they take less time to reach an agreement and hence do not go over the agents’ deadlines).However, it is not clear whether the utility of the agreements reached will be significantly higher than for NT agents.This leads to the following hypothesis.Hypothesis 2. Negotiation tactics that use the RGA achieve a higher success rate, expected utility, and average utilitythan those that do not.15 Using ANOVA, it was found that, using a sample size of 15 for each population, and α = 0.05, that F = 2210 > Fcrit and p = 8 × 10hence that the results are statistically significant (i.e. the difference between the means of the distribution are not the same).−74,S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837825Table 3Benchmark resultsTacticNo. of offersSuccess rateAverage utilityExpected utilityRBTPNT&RBTPNTNT2656585471.01.00.990.872.021.951.91.842.021.951.881.6To test this hypothesis, we run the same experiments as in the previous case and record the average utility peragreement and the number of agreements reached. Thus, it is possible to calculate the expected utility, average utilityper encounter, and the success rate per game as explained earlier. These are recorded in Table 3.Thus it was found that the success rate of persuasive strategies is generally much higher than NT strategies (0.87/en-counter for non-persuasive strategies, 0.99/encounter for PNT strategies only, 1.0/encounter for RBT and PNT, and1.0/encounter for RBT only).16 This result clearly shows that the use of RGA increases the probability of reachingan agreement. The similar performance of RBT and PNT&RBT and the difference between PNT&RBT and PNTshows that RBT agents, as well as being able to find agreements readily with their similar counterparts, are also ableto persuade PNT agents with more attractive offers. This is confirmed by the fact that the average utility of persuasivestrategies is generally higher (i.e. 1.9/encounter for PNT, 1.95/encounter for PNT&RBT, and 2.02/encounter for RBT)than NT (i.e. 1.84/encounter). Note that the difference in utility between NT and other tactics would be much greaterif discount factors (cid:7)α and (cid:7)β were bigger (given the high average number of offers NT uses (i.e. 547)).Given the trend in success rate and average utility, the expected utility follows a similar trend with NT agentsobtaining 1.6/encounter, PNT 1.88/encounter, RBT and PNT 1.95/encounter, and 2.02/encounter for RBT agentsonly.17 Generally speaking, from the above results, we can conclude that RGA, used together with basic tactics,allows agents to reach better agreements much faster and more often.These results also suggest that PNT agents reach broadly similar agreements (in terms of their utility) to NT agents(if we discount the fact that rewards significantly reduce the time to reach agreements and increase the probability ofreaching an agreement). Now, as discussed in Section 5, PNT agents usually generate offers first (starting from highutility ones as for the NT agents) and then calculate the rewards accordingly. Given this, the agents tend to start bygiving rewards and end up asking for rewards. As the negotiation proceeds (if the offers are not accepted), the offersgenerally converge to a point where agents concede nearly equally on all issues (irrespective of the marginal utilitiesof the agents) and the rewards converge to a similar point. This, in turn, results in a lower overall utility over the twogames than if each agent exploits the other one in each game in turn. Now, if rewards are selected in a more intelligentfashion, as in RBT, the agents reach much higher overall utility in general. This is because agents exploit each othermore on the issues for which they have a higher marginal utility than their opponent. This is further demonstrated bythe results of the RBT agents which suggest they reach agreements that have high utility for both participating agents.It can also be noticed that the performance of mixed populations of RBT and PNT agents perform less well thanRBT agents and slightly better than a pure PNT population (see results above). This suggests that the RBT agents canfind agreements that convince their PNT opponent more quickly as they are able to propose better rewards and offersthan PNT agents. However, it is not apparent whether RBT agents are able to avoid being exploited by their PNTcounterparts in such agreements which RBT tries to make more favourable to PNT agents (as described in Section 5).Given this, we postulate the following hypothesis.Hypothesis 3. Agents using RBT are able to avoid exploitation by standard tactics connected to RGA (i.e. PNT).16 Using ANOVA, it was found that for a sample size of 15 for each population of PNT, PNT and RBT, and PNT only, with α = 0.05, F = 8.8 >−4. These results confirm that there is a significant difference between the means of PNT and the other strategies.Fcrit = 3.15 and p = 4.41 × 10The success rate of NT agents were always lower than the other populations in all elements of the sample.17 These results were validated statistically using ANOVA, where it was found that F = 3971 > Fcrit = 2.73, and p = 7.36 × 10size of 15 per population and α = 0.05. These results mean that there is a significant difference between the means of the populations.−80, for a sample826S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837In order to determine which tactic is exploited, we recorded PNT’s and RBT’s average utility separately.18 Thus,it was found that on average, both RBT and PNT agents obtained about the same average utility per agreement (i.e.0.96/encounter). This result validates the above hypothesis and suggests that the hill-climbing mechanism of RBTagents calculates offers that can convince the opponent without reducing the utility of both RBT and PNT agentssignificantly (i.e. in small steps) and also that it maximises joint gains through Algorithm 1.In general, through the above experiments we have empirically demonstrated the usefulness of rewards in bar-gaining. Thus, we have achieved our initial aim of using PN to enable agents to achieve better agreements faster. Inthe following section, we further study RBT to see how it is affected by different conditions in the environment tounderstand what are the important factors that affect the efficiency of our persuasive negotiation strategy.6.5. Evaluating the reward based tacticIn this section we further explore the properties of RBT by studying its behaviour when key attributes of the agentsare varied. As can be deduced from Section 4, there are a large number of attributes that can affect the behaviourof RBT, but here we will focus on the following main ones which we believe have a significant impact on both ourreward generation component and the behaviour of RBT. These attributes are:(1) L—the target determines the size of the reward that can be given to or asked for as determined by vout in Eq. (3)and the procedure described in Section 4.2.2. Given this, varying L allows us to study the effectiveness of PN ingeneral as the possibility of asking for or giving a reward changes. Moreover, we aim to study the effect of oneagent having a lower or higher target than its opponent on the outcomes of negotiations.(2) (cid:7)—the discount factor dictates the utility of offers, as well as rewards. In particular, we aim to see how RBT andour reward generation mechanism can help agents that have different discount factors find good agreements.(3) θ —the delay before the second game is played determines the value of the reward. Increasing this value cansignificantly reduce the value of a reward to an agent. By varying θ we aim to see how it impacts on the use ofrewards during negotiation and how this affects the outcome of each game.In all of these experiments we compute the 95% confidence interval of each result and plot these as error bars on theappropriate graphs in order to show the statistical significance of the results.19First we investigate the impact of the negotiation target L on the outcome of negotiations. In this context, L is usedto decide whether a reward should be sent or not and what the negotiation ranges of an agent should be in the secondgame (see Section 4.2.2). The higher the value of L, the less agents are likely to be able to construct rewards. This isbecause an agent may have to shrink the negotiation range in the second game more in order to achieve a higher Lover the two games. Therefore, we expect the agents to achieve fewer deals and have a corresponding lower overallexpected utility. Moreover, in the case where only one agent has a high L, then the opponent’s rewards are less likelyto be accepted because these rewards are less likely to allow the agent to achieve its target, and hence the agents areless likely to come to agreements or take more offers to come to any agreement. In this case we would also expectthe agent with the higher L to negotiate more strongly and constrain the second game more such that it should get ahigher utility than its opponent. To investigate these intuitions, we will consider a pair of agents α and β that use RBTand postulate the following experimental hypothesis.Hypothesis 4. The higher the value of Lα relative to Lβ , the higher is the average utility of α compared to that of β.To test Hypothesis 4 we ran an experiment where the agents were made to negotiate using similar settings as in theprevious section, except for the fact that Lα was varied between 0 and 1.5 while Lβ was kept fixed at 0.5. The resultsof the experiment are shown in Fig. 3.18 We validated this result using ANOVA with a sample of size 15 per strategy and α = 0.05. Thus it was found that the null hypothesis (i.e. equalmeans for the two samples) was validated with F = 0.13 < Fcrit = 4.10 and p = 0.71 > 0.05.19 If the error bars overlap any two points, it indicates that there is no significant difference between these points. Otherwise there is a significantdifference with a 95% confidence level.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837827(a)(b)Fig. 3. Expected utility, average number of offers, and average utility of agents when Lα is varied. (a) Expected utility of α and β when Lβ = 0.5and Lα are varied. (b) Average number of offers between α and β when Lα is varied. (c) Average utility of α and β when Lβ = 0.5 and Lα isvaried.(c)As can be seen from Fig. 3(a), the overall expected utility of both agents rises sharply at Lα = 0.7 and there is asharp rise in the number of offers exchanged between the two agents (in Fig. 3(b)). Moreover it was found that thesuccess rate of the agents did not drop. The main cause for the jump in expected utility and rise in the average numberof offers can be explained by the results shown in Fig. 3(c). As can be seen, from Lα = 0.7, α’s utility gradually riseswhile β’s utility sharply falls. This means that α exploits β on all the issues that are negotiated.In more detail, in order to obtain Lα = 0.7 and above, α would need to exploit β in the first game on all the issuesit prefers more than β or exploit β on all issues (which it likes less or more than β) in the second game. This can bededuced from the weights used in the utility functions shown in Table 2. Therefore, at this point, α and β are likelyto exploit each other maximally on the issues they prefer in each game. This results in a high point in utility since itrepresents the cooperate–cooperate point in the MMPD (hence the peak in Fig. 3(a)). When Lα < 0.7, the agents canstill find agreements without completely exploiting their opponent on any issue and therefore agree to proposals andrewards that result in a lower overall utility since the outcome then lies further away from the cooperate-cooperatepoint of the MMPD.Beyond Lα = 0.7, it becomes harder for α to give or ask for any rewards. This is because as Lα increases, the useof rewards decreases as α’s ability to concede in either game decreases (since it needs to achieve a high target) and αcan only constrain its negotiation ranges more and more in the second game in trying to achieve its target (as discussedin Section 4.2.3). However, given that Lβ = 0.5 < Lα, β can still afford to be exploited by α and still manage to reachits target over the two games. Hence the success rate of the two agents does not decrease. However, given the morestringent demands of α, the agents are likely to exchange a larger number of offers (i.e. β conceding a significantnumber of times) until an agreement is reached.828S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837In general, these results validate Hypothesis 4 and also confirm our intuition that α’s bargaining power shouldincrease with respect to its target. Given these results, it can be expected that if the second game were less discounted,α could have started exploiting β at a higher value than 0.75. We will therefore explore such discounting effects on thenegotiation and investigate the effect of increasing both agents’ targets at the same time to see the general behaviourof the system as the discounts and targets are varied.Before doing so, however, we next study the effect of the discount factor (cid:7)α on the outcome of the negotiation(keeping (cid:7)β = 0.5). In this case, a low value of (cid:7)α equates to a low discounting effect on the outcome of the twogames and conversely for a high value of (cid:7)α. Therefore we can expect that as (cid:7)α gets higher the agreements reachedin the two games would be much more discounted and hence result in a lower overall expected utility. Moreover, withhigher (cid:7) values, agents will find it harder to achieve their target L as they will value both offers (and counter-offers)and rewards less. Agents are then likely to take more offers to reach an agreement and reach fewer agreements aswell. In the case where only (cid:7)α is varied, we would expect that the agent with the higher discount factor would bemore likely to accept any offer by its opponent since counter-offering might take up time that discounts its own offermore than the one offered by the opponent. This means that the more patient agent is likely to get its offers moreeasily accepted (i.e. take fewer numbers of offers on average) and exploit its opponent more. Hence, as predicted bygame theoretic models of bargaining [25], the more patient agent gets an increasingly higher average utility than itsless patient opponent as the difference between their discount factors increases. We therefore postulate the followinghypothesis.Hypothesis 5. The higher the value of (cid:7)α relative to (cid:7)β , the less agents are likely to reach agreements and the moreoffers they will take to reach an agreement.To test this hypothesis, we ran a similar experiment as above apart from the fact that we kept the target for bothagents at Lα = Lβ = 0.5 and we varied (cid:7)α between 0 and 4 (while keeping (cid:7)β = 0.5). In this context, it is obvious thatthe overall expected utility of the agents will decrease when (cid:7)α increases (and the utility α gets decreases as a resultof the discounting effect). Given this we recorded the average utility of each agent and the number of offers they taketo reach an agreement. The results are shown in Fig. 4.As can be seen from Fig. 4(a), β’s utility slightly decreases as (cid:7)α rises. The number of offers used by the agents alsorises significantly as (cid:7)α increases beyond 1.44. This is because, beyond (cid:7)α = 1.44, the discounting of the second gameis such that it is worth less than 0.5 (assuming α exploits all issues in the second game). Thus, it becomes impossiblefor α to ask for rewards and it can only rely on giving rewards. Moreover, as the discounting effect increases, it alsobecomes harder for β to convince α with them. Eventually, as time passes, the agents can only rely on offers and αconstrains its negotiation ranges in the next game so as to achieve its target. Given this, negotiations take even moretime in the second game (as in the previous experiment). Therefore, the target slightly reduces the advantage of β’spatience (i.e. in having a lower discount factor) in this type of game. It was also found that the success rate of the(a)(b)Fig. 4. Average utility and average number of offers made as (cid:7)α is varied. (a) Average utility of α and β when (cid:7)α is varied. (b) Average number ofoffers made by α and β as (cid:7)α is varied.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837829(a)(b)Fig. 5. Varying the target and discount factor of α and β and the resulting expected utility and number of agreements reached. (a) Expected utility.(b) Success rate.agents does not significantly decrease (from 1 to 0.999) after (cid:7)α = 1.44. This suggests that the agents sometimes runout of time trying to convince each other. This may happen when a poor agreement is reached in the first game andα constrains its negotiation ranges in the second game so much that no agreement is possible. These results thereforevalidate Hypothesis 5.Given the above results, we can expect that the combined effect of an increasing target and an increasing discountfactor should significantly reduce the expected utility of both agents and increase the number of offers they need tomake to come to an agreement. We therefore postulate the following hypotheses.Hypothesis 6. The higher the value of Lα and Lβ , the lower the expected utility of both agents.Hypothesis 7. The higher the value of (cid:7)α and (cid:7)β , the less agents are likely to reach agreements and the more offersthey will take to reach an agreement.Therefore, we varied both agents’ discount factors and targets to see which had a stronger effect on the negotiationoutcomes. The plot of the expected utility and the success rate is shown in Fig. 5.As can be seen from Fig. 5(a), the expected utility is more significantly affected by Lα and Lβ .20 The resultsconfirm Hypotheses 6 and 7. A jump in utility (as in the experiment for Hypothesis 4) is noticed at particular valuesin the agents’ target, corresponding to points where the agents need to try and exploit each other maximally andconstrain their negotiation ranges in the second game so as to achieve this. However, beyond a certain point, agentsare not able to exploit each other maximally any more and cannot use rewards to achieve their target. This results ina decrease in the number of agreements reached as shown in Fig. 5(b). Moreover, we notice that the point at whichthe expected utility drops relative to target values decreases in (cid:7). This confirms our initial intuition that the discountfactor influences to some extent the effect of the target on the expected utility.We also recorded the average number of offers made by the agents to see the impact of the target and discountfactors on it. The results are shown in Fig. 6. As can be seen, the drop in expected utility is reflected by the jumpin the number of offers made. The region where the peak occurs corresponds to values of the targets and discountfactors where the agents are still able to use rewards to persuade each other and significantly shrink their negotiationranges in the second game to reach their target. Beyond this peak (i.e. for higher values of the targets in particular), theagents can only find agreements in the first game and they do so according to the hill- climbing mechanism of RBT(which guarantees that they meet in a few number of steps). Note that the plateau at low values of L is at a lower valuethan that at high values of L, suggesting that rewards can significantly reduce the number of offers made to reach anagreement compared to those that only make offers using the hill climbing method.20 Note that jumps above a success rate of 1 (similarly for jumps of expected utility above 2) are only due to curve fitting rather than actual results.830S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837Fig. 6. Impact of L and (cid:7) on the average number of offers.(a)(b)Fig. 7. Impact on offers and rewards when varying θ . (a) Average number of offers per encounter as θ is increased. (b) Percentage of agreementsmade without rewards as θ is varied.Finally, given that higher values of (cid:7) decrease the probability that agents reach an agreement and increase thenumber of offers exchanged, we expect a similar effect for higher values of the delay. This is because a longer delaydecreases the value of rewards to both agents, and hence reduces the probability of reaching each agent’s target L.Therefore, we expect that the longer the delay θ , the lower the success rate of the agents and the higher the averagenumber of offers needed to reach an agreement. Given this, we postulate the following hypothesis.Hypothesis 8. The higher the value of θ , the less likely it is that agents will use rewards and the more offers they taketo reach an agreement.As for the above hypotheses, we ran a similar experiment keeping Lα = Lβ = 0.5 and (cid:7)α = (cid:7)β = 0.5, varied θbetween 0 and 5 seconds, and recorded the expected utility of the agents. The success rate of the agents did notdecrease significantly, while the number of offers significantly increased when θ increased beyond 3 seconds asshown on Fig. 7(a). These results confirm Hypothesis 8. The reason for the jump in the number of offers at θ = 3has a similar explanation to that in the previous experiment for (cid:7)α = 1.44. Indeed at θ = 3, the total value of thesecond game decreases below 0.5 and decreases the value of rewards that can be given or asked for. This resultsin the agents only being able to make offers without rewards and hence they increase the constraints on the secondnegotiation, which, in turn, increases the number of offers needed to reach an agreement. To confirm these results,we also recorded the number of agreements reached without the use of rewards. As shown in Fig. 7(b), it was indeedfound that the number of agreements reached through without the use of rewards increases as θ increases.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–8378317. Related workIn this paper we have dealt with both repeated negotiations and PN. We previously presented a preliminary versionof our PN strategy in [36]. In this paper, we have elaborated on the protocol, discuss the evaluation functions in moredetail, and thoroughly evaluate the associated reasoning mechanism. In the following subsections we survey the mainwork that has been carried out in both areas and distinguish ours from it.7.1. Repeated negotiationsRepeated negotiations or repeated games have long been studied in game theory [26]. In particular, the closestwork to ours in this area is that of Muthoo [24,25] who analysed the equilibrium offers that arise when agents bargainrepeatedly over a number of issues. In a similar vein, Busch and Hortsmann [9] have analysed the equilibrium offersthat arise when agents need to decide whether to negotiate all the terms of a long term relationship in one go or settlethe agreement incrementally at different points in time. Their results imply that it might be better in some cases togo for short term deals rather than long term ones since the former imply lower negotiation costs. In our case, theheuristics we employ in the RGA follow a similar line of thought in that the outcome of the second game is notcompletely negotiated in the first one. This, in turn, reduces the time to come to an agreement and hence agents do notlose a significant amount of utility due to discounting effects.In the multi-agent agent systems area, repeated negotiations have mostly been considered in terms of repeated(sequential) auctions [8,14,16]. These works have looked at equilibrium strategies that agents should use in suchauctions under settings of complete information. Our work differs from this, and the game theoretic approaches ingeneral, in that we look at a decentralised bargaining interaction where agents do not have any knowledge about theiropponent and need to find the best agreements possible. This is, we believe, a more realistic situation although itrequires us to turn to heuristic methods and empirical evaluations rather than analytical solutions and proofs.7.2. Persuasive negotiationA number of approaches to PN have considered various aspects of the problem over the last few years since theseminal work of Sycara [40–42] and the challenges identified by Tohmé [45] and Jennings et al. [21]. First, we notethe work on the language to describe the domain (hence the content of rewards), as well as to communicate persuasivearguments [22,27,39]. In our work, we mainly build upon [39] in order to construct the domain and communicationlanguages for the use of rewards. However, our work differs in that we additionally consider rewards that can be askedfor and we also specify social commitments that are entailed by illocutions exchanged during negotiations. Moreover,we additionally specify a reasoning mechanism and a tactic for PN.Second, in terms of reasoning mechanisms for PN, we note the work of [22] which specifies arguments such asthreats, rewards, or appeals, in terms of logic statements. However, the semantics of such arguments are not completelyspecified and the choice over which argument to send is made according to a number of ad hoc rules. Building uponthis, [35] proposed a reasoning mechanism that also considered threats, rewards, and appeals. In their case, argumentswere abstract elements that gave some utility to the agents. The choice of the arguments to send was then determinedaccording to how trustworthy an opponent is using a number of fuzzy rules [34]. More recently, [2,3] provided aformal model of arguments (such as threats, rewards, and explanatory arguments) along with the logic to determinethe force of an argument. They also specify a mechanism to identify conflicts between threats, rewards, or appeals.Their conception of rewards is similar to ours in that they capture the gains from the reward in terms of the gainsfrom the goals that the reward achieves. However, they do not specify any negotiation protocol, nor any negotiationalgorithm that determines when and with which offers to send rewards or threats. Moreover, they do not study howthreats and rewards bring about better negotiation outcomes.In general, none of the above approaches have ever concretely instantiated arguments in terms of a standard negoti-ation scenario as we do. Moreover, none of the above algorithms have been benchmarked against standard negotiationalgorithms, and hence, the gains they claim to generate have never been properly quantified. In contrast, we haveshown that our approach can generate significant gains over standard negotiation tactics in various respects.832S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–8378. ConclusionsIn this paper we have presented a comprehensive model of persuasive negotiation that enables agents to achievebetter deals in repeated encounters than was previously possible using standard negotiation tactics. In particular, wefocus on the use of rewards, as rhetorical arguments, that can either be given or asked for. Specifically, these rewardsdefine the constraints that can be imposed on the set of possible agreements in future negotiation games, contingentupon the opponent agreeing to the offer they support in the current encounter.The model consists of two parts: a protocol and a reasoning mechanism. In terms of the protocol, we have useddynamic logic to specify the commitments that arise in persuasive negotiation based on the exchange of rewards. Inso doing, we ensure that the negotiation dialogue between agents can be checked for consistency and that the ensuingcommitments are stored. Our PN protocol is the first to consider the commitments that result from asking for or givingout rewards in a negotiation encounter.In terms of the reasoning mechanism, we define how an agent can generate, select, and evaluate rewards andoffers. This decision making model is composed of the Reward Generation Algorithm that computes rewards that canbe asked from or given to an opponent and a set of functions that permit the evaluation of incoming and outgoingoffers and rewards. The RGA is based on the simple principle that concessions made in previous games need to becompensated for by future rewards. We have also shown how the RGA can easily be connected to non-persuasivenegotiation tactics in order to generate rewards in repeated encounters. Building upon this decision making model, wedeveloped a new Reward Based Tactic that permits the generation of rewards to be asked for or given to an opponentat any point in the negotiation. The RBT strives to achieve Pareto-efficient deals by ensuring that the most preferredoutcomes are selected for the negotiating agents. In so doing, it has been shown to reduce the number of offers thatagents need to make to come to an agreement, and also to enable agents to achieve higher utility deals than standardbenchmark tactics in the MMPD domain.In particular, our results show that RGA can enable agents using standard negotiation tactics to make a 17% gainin utility in repeated encounters. More importantly, RBT has been shown to generate agreements that are 26% betterthan these standard tactics using 21 times fewer messages. Note that these results are only indicative of the possibleimprovement that PN could bring since our agents are made to interact under the specific setting of an MMPD.Other settings could be envisaged, but we expect similarly positive results since the MMPD is generally consideredto capture the canonical properties of the interactions we aim to apply PN to. Moreover, we have analysed the RBT’sproperties and shown that the most important factor that impacts on the number of offers exchanged and the averageutility achieved is the target that the agents set themselves to achieve. An agent’s target determines how aggressivelyit will try to come to an agreement and when it can offer or ask for rewards. Thus, the higher the target, the less likelyit will be able to give rewards and the more likely it will be to ask for rewards. In the extreme case, given the principlewe apply in the RGA, agents may not be able to claim or give rewards at all since they may have to avoid making anyconcession in order to achieve their target.In general, our work raises a number of theoretical and practical issues. First, in allowing for rewards in repeatedencounters, we extend the bargaining problem initially posed by Rubinstein [37]. Now, such problems are usuallystudied to deduce their equilibrium properties using bargaining theory [25]. This is important in order to understandthe interplay of such factors as the agents’ targets and discount factors and their impact on the negotiation outcome.However, we believe that PN mechanisms like ours will undoubtedly generate more complex interaction scenarios.These scenarios will therefore raise a number of more complex theoretical issues that will need to be addressed.Second, the fact that an agent’s reasoning mechanism is much more sophisticated than that for standard negoti-ation tactics, indicates that the design of such agents is likely to become more challenging as the complexity of thearguments they can exchange increases. This means more structured approaches in terms of methodologies and frame-works, will be needed for designing PN agents [32]. Such approaches should help define and standardise the reasoningmechanism of agents in such a way that different types of arguments, protocols, or decision making functionalitiescan be interconnected and adapted to fit particular application contexts.Third, while we have shown that PN can be beneficial to the constituent agents, it is also important to studywhich system-wide properties emerge when PN mechanisms are used. In this vein, it is usually expected that thedecentralised, bilateral negotiations based on the standard negotiation tactics we presented can rarely achieve the levelof efficiency guaranteed by centralised auction-based approaches. However, given that PN techniques can supportmuch richer interactions than existing automated bilateral negotiation mechanisms, it is possible to exchange moreS.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837833meaningful information which could lead agents to achieve better deals (as in our case). This could, in turn, lead tobetter efficiency at the system level. Hence, it is important to study how beneficial such PN mechanisms could berelative to auction-based approaches and identify the trade-offs that result from their use.Finally, while RBT has been shown to be better than the standard tactics in MMPD-based repeated encounters,it is but one of many other tactics that could be envisaged in the future to be used in different or similar contexts.Given this, it would be interesting to use techniques such as evolutionary game theory or genetic algorithms to seehow these strategies change the performance of agents when pitted against other different strategies [48]. This wouldhelp determine which strategy to choose when an agent is placed in any given population.AcknowledgementsCarles Sierra and Lluís Godo are partially supported by the OpenKnowledge European STREP project and theSpanish project EIA, TIN2006-15662-C02-01. We wish to thank Pilar Dellunde for valuable comments on the logicalaspects of the paper, Prof. Abhinay Muthoo, Dr. Pablo Noriega, and Prof. dr. John Jules Ch. Meyer for their construc-tive comments and advice on the negotiation model and related work. We also thank the anonymous reviewers fortheir very valuable comments.Appendix A. Devising utility functionsThe prisoner’s dilemma (PD) is well known for its applicability to very general forms of interactions [5]. In devisingutility functions according to the PD, we aim to build more realistic and interesting interaction scenarios than zero-sum games [25,37]. In particular, the characterisation of the agents’ utility functions in terms of a PD is done soas to model general interactions where each agent (in a pair) prefers some issues more than his counterpart. This iscommonly the case where, for example, high-volume traders are able to enjoy economies of scale such that they valuethe price of the goods they sell less than what individual customers probably would. Another example would be a carseller who has high costs in getting a car with a special colour while the buyer may not have such strong feelings forsuch a colour.With respect to the PD, in the case of a bargain, cooperation means that the agent agrees to concede while adefection means that the agent exploits its opponent. In order to devise utility functions that are appropriate for thiswork (which assumes that more than two values may be enacted for any issue) we require that there be more thanjust two moves (i.e. Cooperate or Defect) that are present in the standard version of the PD. In particular, we needa continuous scale of cooperation between these two extremes. To this end, we extend the prisoner’s dilemma tothe multi-move prisoner’s dilemma (MMPD) [7,29,46]. In the MMPD, actions (or moves) are considered to be theenactment of the contents of a contract (e.g. paying for goods, delivering goods). Both the interaction partners havetheir own actions dictated by the part of the contract that they have to enact (e.g. seller delivers goods and buyer paysfor the goods at a given time). Agents may also have more than one issue to take care of (e.g delivery of goods andensuring they are of a certain quality) and for each issue a discrete number of possible values can be given (e.g. payingafter 3 days, 4 days, . . . or delivering after 1 month, 2 months).In the following section, we first define the action set (possible moves) of the agents which will interact via theMMPD. Then, we provide a formal definition of the MMPD (with respect to multi-issue contracts). The last subsectionshows how we can devise the utility functions of the agents so that they can engage in a MMPD. These utility functionsare then used by the agents in experiments we describe in Section 6.A.1. The action setWhenever a contract is signed, each agent is given its part of the contract to enact. In order to simplify notation, wewill note as Oα those issues that α enacts in a contract and Oβ as those that β enacts (which is a slight modificationto the formalism we introduced in Section 2). In effect, the achievement of the issue-value pairs (xi = vi ) in an agent’spart of the contract is its ‘action’ or ‘move’ in the game. Thus, an agent α can generate its action set O(Oα) for theMMPD by defining all the possible assignments of the values of the issues that it controls. This is expressed as:(cid:4)Oα(cid:6)Oα = {x1 = v1, . . . , xn = vn} | xi ∈ X(cid:5), vi ∈ Dxi(cid:4)Oα(A.1)O=(cid:7)(cid:5)834S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837Table A.1Multi-move prisoner’s dilemmaα’s part/β’s partOβkOβlOαiU β (OαiU β (Oαi∪ Oβ∪ Oβk ), U α(Oαil ), U α(Oαi∪ Oβk )∪ Oβl )OαjU β (OαjU β (Oαj∪ Oβ∪ Oβk ), U α(Oαjl ), U α(Oαj∪ Oβk )∪ Oβl )Each agent thus has all its possible actions defined and these actions result in a payoff for each agent similar to aprisoner’s dilemma with a discrete multi-action set (as opposed to a binary action set).A.2. The gameis α’s action and OβThe MMPD is represented as a matrix where each row (and column) corresponds to a particular degree of co-operation from one of the agents. Therefore, a contract O between agents α and β can be represented as a point ink is β’s action such that O = Oαthe matrix where Oαk . The sub-indexes of the differentiicontracts correspond to a row i and a column k respectively in the matrix. We assume that a total order applies overall the possible contracts (in the matrix) according to the utility of each contract to the agent concerned when movingalong a single row or column. This means that for an agent α, Oαj , where j > i, are two possible executionsbut Oαj is a defection (or exploitation) by α (or a cooperative move i.e. a concession by β) resulting in greater utilityfor α and utility loss for β, if β agrees on Oβk (i.e. staying on the same column). Let Oα be the set of contracts handledby α and Oβ similarly for β.i and Oα∪ OβWe can then define the multi-move prisoner’s dilemma as follows for Oαj representing a defection from Oαi by αand Oβl representing a defection from Oβk by β:Definition 9. Two agents α and β engage in a multiple-move prisoner’s dilemma (MMPD) over the contracts they can∈ Oβ wherechoose iff, for any four points in the matrix: ∀OαU β (Oβi , Oαjl ), the following rules are respected:∈ Oα, where U α(Oαk ) < U β (Oβj ) and ∀Oβi ) < U α(Oαk , Oβl(1) Defection Rules (an agent can exploit another’s cooperation by defecting (i.e. exploiting), but ends up with alower payoff if the other side also defects):(cid:5)(cid:5)(cid:4)(cid:4)Oαi(cid:4)OαiU αU β∪ Oβl∪ Oβl(cid:5)< U α> U β(cid:4)OαjOαj∪ Oβl∪ Oβl(cid:5)< U α> U β(cid:4)Oαi(cid:4)Oαi(cid:5)(cid:5)∪ Oβk∪ Oβk< U α> U β(cid:4)Oαj(cid:4)Oαj(cid:5)(cid:5)∪ Oβk∪ Oβk(2) Pareto Efficiency Rules (the sum of the rewards when both cooperate (i.e. concede) is higher than the sum obtainedif either or both of the agents defect (i.e. exploit)):(cid:4)Oαi(cid:4)OαjU αU α(cid:5)(cid:5)∪ Oβk∪ Oβk+ U β+ U β(cid:4)Oαi(cid:4)Oαj(cid:5)(cid:5)∪ Oβk∪ Oβk> U α> U α(cid:4)Oαj(cid:4)Oαj(cid:5)(cid:5)∪ Oβk∪ Oβl+ U β+ U β(cid:4)Oαj(cid:4)Oαj(cid:5)(cid:5)∪ Oβk∪ OβlFrom the above rules it is then possible to derive the following payoff matrix for any pair of possible contracts tobe chosen by both agents:We next define the utility functions that do respect the payoff structure of the MMPD. To this end, we propose thefollowing theorem:Theorem 10. Let X be a given set of issues, α and β be two agents, with Xα being issues under α’s control andXβ being issues under β’s control (with X = Xα ∪ Xβ ). Assume that the utility for α of a contract O = (x1 =v1, . . . , xn = vn) over issues X(O) ⊆ X is of the form U α(O) =(vi) and analogously for agent β,U β (O) =xi are the utility functions for α and β of the individual issue xi .xi ∈X(O) ωαxxi (vi), where U αxixi ∈X(O) ωβand U βx · U β· U αxi(cid:3)(cid:3)S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837835Moreover we assume that U αy ∈ Xβ (O) respectively, and differentiable (strictly) decreasing otherwise.x (v) and U βy (u) are differentiable (strictly) increasing functions for any x ∈ Xα(O) andThen, U α and U β respect the aforementioned defection and Pareto-efficiency rules of a multi-move prisoner’sdilemma if the following conditions are satisfied:(i)(ii)(cid:9)(cid:8)− dU βxdxωβx·> ωαx· dU αxdxfor all issues x ∈ Xα(O).(cid:8)(cid:9)ωαy·−> ωβy·dU αydydU βydyfor all issues y ∈ Xβ (O)(A.2)(A.3)where the inequalities are point-wise.Proof. Without loss of generality, we may assume X(O) = {x, y}, Xα = {x} and Xβ = {y}. Let O = (x = v, y = u)be the agreed contract. We begin by considering a defection by agent α in an issue x from the value v to a value v(cid:17) suchthat U α(v(cid:17)) > U α(v) (given that everything else remains the same). For an easier notation we will write U α(v, u) todenote the utility of agent α on a contract (x = v, y = u), similarly for agent β, and U (v, u) for U α(v, u) + U β (v, u).From the defection and Pareto-efficiency rules of the MMPD we have the conditionU (v, u) > U (v(cid:17), u),and using our assumptions on the utilities U α and U β (from Eqs. (A.2) and (A.3)), this means:x U βx (v) + ωβ) + ωβx U β(cid:17)(cid:17)x (vx U αx U αωαx (v) > ωαthat is, we have the equivalent condition to be required:(cid:5))x (vNow, under general assumptions, we have:(cid:4)x (v) − U βU β(cid:5)x (v)) − U α(cid:4)U α> ωαxx (vx (vωβx)(cid:17)(cid:17)and(cid:17)U αx (v) − U αx (v) =v(cid:17)(cid:10)vdU αxdxdxx (v) − U βU βx (v(cid:17)) = −v(cid:17)(cid:10)vdU βxdxdx(A.4)(A.5)(A.6)(A.7)Hence, applying the condition expressed in Eq. (A.2) of the theorem to Eqs. (A.6) and (A.7) we have Eq. (A.5)satisfied, and hence U (v, u) > U (v(cid:17), u) as well (where u(cid:17) is a defection by α from u). Similarly, the same procedurecan be applied to Eqs. (A.6) and (A.7) above using Eq. (A.3) such that a defection by agent β changing the agreedvalue y = u to any new value y = u(cid:17), with U β (u(cid:17)) > U β (u) (given the opponent does not defect in each case), yieldsU (v, u) > U (v, u(cid:17)).Finally, if both agents defect to say x = v(cid:17) and y = u(cid:17), with U α(v(cid:17)) > U α(v) and U β (u(cid:17)) > U β (u) (given all elsestays the same), then we obviously have the desired inequalities which actually express the Pareto-efficiency rules:U (v, u) > max(cid:4)U (v(cid:17)(cid:17), u), U (v, u(cid:5))(cid:4)(cid:2) minU (v(cid:17)(cid:17), u), U (v, u(cid:5))while still having the following defection rules satisfied: U αx (v(cid:17)) (given all else stays the same). (cid:2)x (v) > U βU βx (v) < U α(cid:17)(cid:17))> U (v, ux (v(cid:17)), U βy (u) < U βy (u(cid:17)) and U αy (u) > U α(A.8)y (u(cid:17)),If the utility function of an agent α for each issue in a contract satisfies the conditions expressed in Eqs. (A.2)and (A.3) with respect to its opponent β, then the two agents follow a prisoner’s dilemma. These utility functions836S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837generally mean that α has a higher marginal utility than β on some issues (e.g. issues y in Theorem 10) and a lowermarginal utility on other issues (e.g. issues x in Theorem 10). Then, each agreement that they could reach represents adifferent degree of exploitation or concession by one of the parties concerned. The degree of concession is determinedby the difference that exists between the maximum value that an agent could obtain (if it exploited its opponent onall issues) and the value of the agreement chosen (see Eq. (2)). The higher the exploitation, the higher utility loss isexpected from a particular contract for the opponent.References[1] L. Amgoud, S. Kaci, On the generation of bipolar goals in argumentation-based negotiation, in: I. Rahwan, P. Moraitis, C. Reed (Eds.),Argumentation in Multi-Agent Systems: State of the Art Survey, in: Lecture Notes in Artificial Intelligence, vol. 3366, Springer, 2004,pp. 192–207.[2] L. Amgoud, H. Prade, Formal handling of threats and rewards in a negotiation dialogue, in: Proceedings of the Fourth International JointConference on Autonomous Agents and Multi-Agent Systems, ACM Press, 2005, pp. 529–536.[3] L. Amgoud, H. Prade, Handling threats, rewards and explanatory arguments in a unified setting, International Journal of Intelligent Sys-tems 20 (12) (2005) 1195–1218.[4] J.L. Austin, How to Do Things with Words, Harvard University Press, 1975.[5] R. Axelrod, The Evolution of Cooperation, Basic Books, New York, 1984.[6] J. Bentahar, B. Moulin, J.C. Meyer, B. Chaib-draa, A logical model for commitment and argument network for agent communication, in:C. Sierra, L. Sonenberg, N.R. Jennings, M. Tambe (Eds.), Proceedings of the Third International Conference on Autonomous Agents andMulti-Agent Systems, 2004, pp. 792–799.[7] A. Birk, Boosting cooperation by evolving trust, Applied Artificial Intelligence 14 (8) (2000) 769–784.[8] F. Brandt, G. Weiss, Vicious strategies for Vickrey auctions, in: AGENTS ’01: Proceedings of the Fifth International Conference on Au-tonomous Agents, ACM Press, 2001, pp. 71–72.[9] L. Busch, I.J. Hortsmann, Endogenous incomplete contracts: A bargaining approach, Canadian Journal of Economics 32 (4) (1999) 956–975.[10] M. Esteva, J.A. Rodríguez, B. Rosell, J.L. Arcos, Ameli: An agent-based middleware for electronic institutions, in: Third International JointConference on Autonomous Agents and Multi-Agent Systems, 2004, pp. 236–243.[11] M. Esteva, J.A. Rodríguez-Aguilar, C. Sierra, P. García, J.L. Arcos, On the formal specification of electronic institutions, in: F. Dignum,C. Sierra (Eds.), Agent Mediated Electronic Commerce, in: Lecture Notes in Artificial Intelligence, vol. 1991, Springer, 2001, pp. 126–147.[12] P. Faratin, C. Sierra, N.R. Jennings, Negotiation decision functions for autonomous agents, International Journal of Robotics and AutonomousSystems 24 (3–4) (1998) 159–182.[13] P. Faratin, C. Sierra, N.R. Jennings, Using similarity criteria to make trade-offs in automated negotiations, Artificial Intelligence 142 (2) (2002)205–237.[14] S. Fatima, M. Wooldridge, N.R. Jennings, Optimal negotiation strategies for agents with incomplete information, in: J.-J. Meyer, M. Tambe(Eds.), Intelligent Agent Series VIII: Proceedings of the 8th International Workshop on Agent Theories, Architectures, and Languages (ATAL2001), in: Lecture Notes in Computer Science, vol. 2333, Springer, 2001, pp. 53–68.[15] S. Fatima, M. Wooldridge, N.R. Jennings, An agenda-based framework for multi-issue negotiation, Artificial Intelligence 152 (1) (2004) 1–45.[16] S. Fatima, M. Wooldridge, N.R. Jennings, Sequential auctions for objects with common and private values, in: Proceedings of the FourthInternational Joint Conference on Autonomous Agents and Multi-Agent Systems, 2005, pp. 635–642.[17] R. Fisher, W. Ury, Getting to Yes: Negotiating Agreement Without Giving In, Penguin Books, New York, 1983.[18] D. Harel, Dynamic logic, in: D. Gabbay, F. Guenther (Eds.), Handbook of Philosophical Logic Volume II, D. Reidel Publishing Company,1984, pp. 497–604.[19] J. Hovi, Games, Threats, and Treaties—Understanding Commitments in International Relations, Pinter, 1998.[20] N.R. Jennings, P. Faratin, A.R. Lomuscio, S. Parsons, C. Sierra, M. Wooldridge, Automated negotiation: Prospects, methods and challenges,International Journal of Group Decision and Negotiation 10 (2) (2001) 199–215.[21] N.R. Jennings, S. Parsons, P. Noriega, C. Sierra, On argumentation-based negotiation, in: Proceedings of the International Workshop onMulti-Agent Systems, Boston, USA, 1998.[22] S. Kraus, K. Sycara, A. Evenchik, Reaching agreements through argumentation: A logical model and implementation, Artificial Intelli-gence 104 (1–2) (1998) 1–69.[23] P. McBurney, R.M. van Eijk, S. Parsons, L. Amgoud, A dialogue-game protocol for agent purchase negotiations, Journal of AutonomousAgents and Multi-Agent Systems 7 (3) (2003) 235–273.[24] A. Muthoo, Bargaining in a long-term relationship with endogenous termination, Journal of Economic Theory 66 (1995) 590–598.[25] A. Muthoo, Bargaining Theory with Applications, Cambridge University Press, 1999.[26] M.J. Osborne, A. Rubinstein, Bargaining and Markets, Academic Press, 1990.[27] S. Parsons, C. Sierra, N.R. Jennings, Agents that reason and negotiate by arguing, Journal of Logic and Computation 8 (3) (1998) 261–292.[28] C. Perelman, The Realm of Rhetoric, first ed., University of Notre Dame Press, 1982.[29] L. Prechelt, INCA: A multi-choice model of cooperation under restricted communication, BioSystems 37 (1–2) (1996) 127–134.[30] I. Rahwan, S.D. Ramchurn, N.R. Jennings, P. McBurney, S.D. Parsons, L. Sonenberg, Argumentation-based negotiation, The KnowledgeEngineering Review 18 (4) (1996) 343–375.[31] I. Rahwan, L. Sonenberg, F. Dignum, Towards interest-based negotiation, in: J.S. Rosenschein, T. Sandholm, M. Wooldridge, M. Yokoo(Eds.), Proceedings of the 2nd International Joint Conference on Autonomous Agents and Multi-Agent Systems, Melbourne, Australia, 2003,pp. 773–780.S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837837[32] I. Rahwan, L. Sonenberg, N.R. Jennings, P. McBurney, Stratum: A methodology for designing agent negotiation strategies, Applied ArtificialIntelligence 21 (10) (2007).[33] H. Raiffa, The Art and Science of Negotiation, Belknapp, 1982.[34] S.D. Ramchurn, D. Huynh, N.R. Jennings, Trust in multi-agent systems, The Knowledge Engineering Review 19 (1) (2004) 1–25.[35] S.D. Ramchurn, N.R. Jennings, C. Sierra, Persuasive negotiation for autonomous agents: A rhetorical approach, in: C. Reed (Ed.), Workshopon the Computational Models of Natural Argument, IJCAI, 2003, pp. 9–18.[36] S.D. Ramchurn, C. Sierra, L. Godo, N.R. Jennings, Negotiating using rewards, in: Proceedings of the Fifth International Joint Conference onAutonomous Agents and Multi-Agent Systems, ACM Press, 2006, pp. 400–407.[37] A. Rubinstein, Perfect equilibrium in a bargaining model, Econometrica 50 (1982) 97–109.[38] J. Searle, Speech Acts: An Essay in the Philosophy of Language, Cambridge University Press, New York, 1969.[39] C. Sierra, N.R. Jennings, P. Noriega, S. Parsons, A framework for argumentation-based negotiation, in: M. Singh, A. Rao, M. Wooldridge(Eds.), Intelligent Agent IV: 4th International Workshop on Agent Theories, Architectures and Languages (ATAL 1997), in: Lecture Notes inComputer Science, vol. 1365, Springer, 1998, pp. 177–192.[40] K. Sycara, Arguments of persuasion in labour mediation, in: Proceedings of the Ninth International Joint Conference on Artificial Intelligence,1985, pp. 294–296.[41] K. Sycara, Persuasive argumentation in negotiation, Theory and Decision 18 (3) (1990) 203–242.[42] K. Sycara, The PERSUADER, in: D. Shapiro (Ed.), The Encyclopedia of Artificial Intelligence, John Wiley and Sons, 1992.[43] W.T.L. Teacy, J. Patel, N.R. Jennings, M. Luck, Travos: Trust and reputation in the context of inaccurate information sources, AutonomousAgents and Multi-Agent Systems 12 (2) (2006) 183–198.[44] C. Tindale, Acts of Arguing, a Rhetorical Model of Argument, State University Press of New York, Albany, NY, 1999.[45] F. Tohmé, Negotiation and defeasible reasons for choice, in: Proceedings of the Stanford Spring Symposium on Qualitative Preferences inDeliberation and Practical Reasoning, 1997, pp. 95–102.[46] G. Tsebelis, Are sanctions effective? A game theoretic analysis, Journal of Conflict Resolution 34 (1990) 3–28.[47] D.N. Walton, E.C.W. Krabbe, Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning, SUNY Press, Albany, NY, 1995.[48] J. Weibull, Evolutionary Game Theory, The MIT Press, 1995.