ELSEVIER Artificial Intelligence 79 (1995) 1-38 Artificial Intelligence Pat-learning non-recursive Prolog clauses William W. Cohen * AT&T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974, USA Received August 1993; revised April 1994 Abstract (ILP) Recently logic programming there has been an increasing amount of research on learning concepts expressed in subsets of Prolog; the term inductive has been used to describe this growing body of research. This paper seeks to expand the theoretical foundations of ILP by investigating the pat-learnability of logic programs. We focus on programs consisting of a single function-free non-recursive clause, and focus on generalizations of a language known to be pat-learnable: namely, the language of determinate function-free clauses of constant depth. We demonstrate that a number of syntactic generalizations of this language are hard to learn, but that the language can be generalized to clauses of constant locality while still allowing pat-learnability. More specifically, we first show that determinate clauses of log depth are not regardless of the language used to represent hypotheses. We then investigate the pat-learnable, effect of allowing indeterminacy in a clause, and show that clauses with k indeterminate variables are as hard to learn as DNP. We next show that a more restricted language of clauses with bounded indeterminacy is learnable using &CNF to represent hypotheses, and that restricting the “locality” of a clause to a constant allows pat-learnability even if an arbitrary amount of indeterminacy is allowed. This last result is also shown to be a strict generalization of the previous result for determinate function-free clauses of constant depth. Finally, we present some extensions of these results to logic programs with multiple clauses. Keywords: Machine learning; inductive logic programming; pat-learning 1. Introduction Recently there has been an increasing amount of research on learning concepts ex- in first-order such as description pressed logics [ <5,29,5 I] most researchers logics. While some researchers have considered for concepts as a representation logics special-purpose and examples logic as a representation have used standard first-order * E-mail: wcohen@research.att.com. 0004-3702/95/$09.50 SSDIOOO4-3702(94)00034-X @ 1995 Elsevier Science B.V. All rights reserved in particular, most have used restricted subsets of Prolog [ 7,37,39,40,46]. The term inductive logic programming to represent con- (ILP) has been used to language; cepts describe this growing body of research. learning systems on Prolog is that its semantics One advantage of basing in particular, a number of previous and com- plexity are mathematically well understood. This offers some hope that learning systems based on it can also be rigorously analyzed. A number of formal results have in fact been obtained; results expand of logic programs. the representational other words, we wish to determine logic programs, This paper seeks to the pat-learnability the degree to which in for investigating In particular, our goal is to investigate carefully researchers have derived [ 16.21,23,27]. imposed by certain practical systems are necessary; in Valiant’s the theoretical rather than to analyze existing ]50] mode1 of pat-learnability the representational of ILP by further learning systems. of learnability” “boundaries foundations learnability restrictions In this paper WC will consider primarily logic programs consisting of a single function- the results that are obtain- extensions of clauses here because programs are straightforward clause. We focus on single clauses because of multiple-clause [ 10. 161. We consider only non-recursive requires somewhat different free non-recursive able on the learnability results for single clauses analysis of recursive programs we also note that recursion has not been important ods to real-world problems 1 17,30.35,38 1. A final restriction knowledge will be allowed ground unit clauses been made by several practical in our analysis, we will allow only background theories of (also known as a database or a model). This restriction has also [ 37,40,46]. learning systems [ 8, IO] ; forma1 machinery in several applications of ILP meth- is that while background In this paper, we will first define pat-learnability learnability the most important of these (for the purpose of this paper) and review previous results for logic programs: shows that a single determinate [ 161. We then investigate the language of determinate We show represent hypotheses. that this language function-free clause of constant depth ’ is pat-learnable a number of generalizations clauses of logarithmic depth (rather is not pat-learnable, regardless of the language used to of this language, beginning with than constant depth). We then investigate indeterminacy for indeterminate clauses. We show the effect of allowing in a clause, and obtain a that clauses with even a single language but is predictable, using to represent hypotheses. Both of these results are negative, as they demonstrate series of results “free” variable are as hard to learn as DNF. and that a slightly more restricted of clauses with “bounded k-CNF that apparently show that bounding amount of indeterminacy Our last result on hard to learn. However. we next even if an arbitrary the “locality” of a clause allows pat-learnability, is not pat-learnable, clauses concerns are surprisingly indeterminacy” indeterminate is allowed. reasonable languages clauses and clauses with bounded clause can be rewritten as a clause with locality no greater the relative expressive power of locality: we show that for fixed i and than sense, a is, in a very reasonable the language of clauses of bounded locality of the language of ii-determinate clauses. ij-determinate .j, every ij-determinate “‘I Thus .I. strict generalization 3 ’ These restrictions are precisely defined 111 Sedan 2 5 N!W Cohen/Ar@cial Intelligence 79 (1995) 1-38 3 To summarize these results, we show that although the obvious syntactic generaliza- tions of ij-determinacy language of clauses bounded all fail to produce pat-learnable locality does yield a pat-learnable results on the learnability languages, generalizing language. of multiple-clause to the non- Finally, we state some additional recursive programs, discuss A number of the results related work, and conclude. in this paper have been previously presented form elsewhere formal issue, [ 8,9,12]. The pat-learnability of recursive programs, another is considered in depth in two companion papers [ 13,141. in a preliminary interesting 2. Preliminaries 2. I. Logic programming In this section, we will give an overview of logic programming. As we are considering in particular, below only coincide with the usual ones for the case of non-recursive logic programs, our overview has been simplified accordingly; single-clause the reader Prolog programs. For a more complete description of logic is referred to one of the standard texts (e.g., [ 341) . very simple the definitions function-free programming Logic programs represent with letters represent with upper-case symbol and Xl,. the arity of the literal. A fact are written over an alphabet of constant symbols, which we will represent with letters like tl , t2, . . . ; an alphabet of predicate symbols, which we like p, q or r; and an alphabet of variables, which letters. A literal is written p( X1, . . . , Xk) . . , X, are variables. The number of arguments is written p( tl , . . . , tk) where p the arity of a fact is to letters 0 them as sets 0 = {Xl = tl, X;? = t2,. . . ,X, = t,,} and A the result of replacing each variable X in A with if 81 usually will usually we will always where p is a predicate is called k in a literal symbol and tl, . . . , tk are constant is a predicate the number of arguments constant and g and (when necessary) write where is a literal, we will use A0 to denote the constant and 02 are both substitutions, we will use A&& to which X is mapped by 8; extending to denote symbols or variables; we will represent symbol onto which Xi is mapped. (A&)&. A fact f is an instance of a literal A if there is some substitution substitutions with the Greek function mapping variables If 8 is a substitution k. A substitution ti is the constant symbols; again, this notation is a partial slightly, symbol If 01 and 82 are substitutions than 02. Notice that if 81 is more general instances of A01 is a superset of the set of instances of A&. 8 such that A0 = F. such that 81 C 02, then we say that 81 is more general than 02, then for any literal A, the set of Finally, a de$nite clause is written A&B, A . . . A Bl where A and Bl, . . . , Bl are literals. A is called the head of the clause, and the conjunction B1 A. . . A B1 is called the body of the clause. the extension of a clause A+Bl A . . . A BI with respect all facts f such that either we will also call a database-then to the database DB is the set of If DB is a set of facts-which l fEDB,or l there exists a substitution clause, B# E DB. 0 so that AB = f, and for every Bi from the body of the In the latter case, we will say that the substitution the clause. For brevity, we will let ert(C.DB) to the database DB. 8 proves f to be in the extension 01 the extension of C with respect denote For technical reasons, an equali@ predicate-that for every constant assumption IO any database with only a polynomial can be made without f; appearing it will be convenient is. a predicate to assume that every database DB contains symbol equal such that equal( ti, ti) E DB ti # t,i. This since such a predicate can be added in DB. and eqqual(t,, tj) $ DB for any loss of generality, increase Readers familiar with logic programming will notice that this definition of ‘*extension” for semantics of Prolog programs in size. coincides with the usual the programs considered Prolog programs over a ground background define tixpoint or minimal-model in this paper (i.e.. single-clause non-recursive function-free theory). Hence one might more succinctly the extension of a clause C with respect to DB as (,f: C A DB t f}. Again for those familiar with lust-order logic. a clause A + B) A t. A B[ can also be thought of as a logical statement VX,. , X,,( TB, V c’ ~lBl ;i A I . , X,, are the variables where XI. a clause with respect statement above and the conjunction to DB is simply of the facts in DB. that appear in the clause. Then the set of facts e that follow the extension of from the logical Example. If DB is the set DB = {mother( ann,bob), father( bobjulie). father( bobchris)} then the extension of the clause grandmother( X, Y) +mother( X, Z ) , father( Z. Y) with respect to DB is the set DB U (grandmother( annjulie). grandmother( annchris)}. (Notice predicate that prove the additional that we have adopted f( X, Y) where that a function .f( X, Y) is true iff Y = ,f( X) .) The most general substitutions facts the convention is represented f by a ,f 1 = grandmother( annjulie). ,fz = grandmother( annchris ) in the extension are 6) = {X = ann, Y = julie, Z = bob}. 01 = {X = ann. Y = Chris, Z = bob} WM! Cohen/Art$cial Intelligence 79 (1995) 1-38 5 2.2. Models of learnability formal analysis of learnability, in this paper on the case of function-free of course, Our goal is to determine by formal analysis which subsets of Prolog are efficiently Prolog. it In this section, we will describe of the models of pac- by learnable; we focus Any means our basic models of learnability; learnability, Pitt and Warmuth requires an explicit model of what these are slight modifications [50], and polynomial to be “efficiently for a language predictability, non-recursive by Valiant learnable”. introduced introduced the domain. Define a concept C over X to be a representation [ 441. Let X be a set, called is polynomially of some subset of X, and a language LANG to be a set of concepts. Associated with X and LANG are two size complexity measures. We will write the size complexity of e E X as ]IC 11 or Ilel], and we will assume that some concept C E LANG or instance this measure to represent C or e. We use the notation X,, (respectively LANG,) to stand for the set of all elements of X (respectively LANG) of size complexity no greater than n. In this paper, we will be between a concept and the set it represents; when rather casual about there is a risk of confusion we will refer to the set represented by a concept C as the extension of C. to the number of bits needed the distinction related form. One might measure Example. For example, to boolean variables, normal of the vector, and measure in C. Thus ((xl AZ) V (FAxg)) for the instance and let DNF be the language of boolean let X be the domain of binary vectors, interpreted as assignments in disjunctive of a vector e E X as the length of a formula C by the number of literals l/e\] = 5, and for the concept C = e = 00110 we have the complexity the complexity formulae we have l/Cl/ =4. An example of C is a pair (e, b) where b = 1 if e E C and b = 0 otherwise. distribution is a probability pair of multisets S+, S- draw n f rom the domain X according positive examples of C, and S- containing basic learning models. function, If D a sample of C from X drawn according to D is a only only negative ones. We can now define our to D, Sf containing Definition 1 (Polynomially predictable). A language LANG is polynomially predictable function m( i, $, n,, n,) so that iff there is an algorithm PACPREDICT and a polynomial for every n, > 0, every n, > 0, every C E LANG,,, every E: 0 < e < 1, every 6: 0 < 6 < 1, and every probability function D, PACPREDICT has the following behavior: distribution (1) given a sample S+,S- of C f rom X,< drawn according least m( d, i, n,, n,) examples, PACPREDICT outputs a hypothesis H such that to D and containing at Prob(D(H- C) + D(C - H) > E) < 6 the probability is a where PACPREDICT PACPREDICT; is taken over the possible samples Sf and S- and randomized algorithm) over any coin (if flips made by (2) PACPREDIC~ ples; and runs in time polynomial 111 i. h, II,, n,, and the number of exam- (3) H can be evaluated in polynomial The algorithm PACPREDIC’~ is called a prediction algorithm time. for LANG, and the function m( i, f , II,, n, ) is called the .ramp/e complexity of PACPREDICT. predictability” as “predictability”. We will sometimes The first condition abbreviate “polynomial in the definition merely states be low, as measured against must (usually) the training examples were drawn. The second condition, that the sample size is polynomial. is polynomial. The final condition very weak sense that it can be used to make predictions this is a worst-case the inputs of the learner. that the error of the hypothesis the probability distribution D from which together with the stipulation time of the learner in the time. Notice that learning model. as the definition allows an adversarial choice of all that the hypothesis be usable that the total running in polynomial requires ensures simply The model of polynomial predictability has been well studied [44], and is a weaker version of Valiant’s [ 501 criterion of put-lrarnabilit~: Definition 2 (Pa-leurnable) rithm PACLEARN so that A language LANCE i.s pa-learnable iff‘ there is an algo- ( I ) PACLEARN satisfies all the requirements in the definition of polynomial pre- dictability, and (2) on inputs S’ and Sm. PACLJXRN always outputs a hypothesis H E LANG. Thus if a language Predictability is pat-learnable also has an important is not predictable, language words, one cannot make a non-predictable language, only by adding additional that the language indicates hence is a strong negative result. then no supersct of that language property not shared by pat-learnability: it IS predictable, but the converse need not be true. if a In other the language predictable by generalizing is predictable. restrictions. Showing a language is, in some sense. too expressive is not predictable and to learn efficiently, On the other hand. it is often considered desirable in ILP contexts. that are logic programs; hence a polynomial to output hy- algorithm, which may potheses than a pat-learning output hypotheses algorithm. Thus ideally one would like all positive results to be given in the pat-learning model, and all negative paper we will (whenever possible) use predictability prediction model. In this in the pat-learning model, and in the polynomial results format, may be much less desirable give positive results. in an arbitrary to be given in negative prediction primarily results 2.3. Background knowledge: extending the .rtundard models So far, our formalization is standard. However, provides both a set of examples and a “background that may be useful in constructing a hypothesis: in a typical the user theory” defining a set of predicates ILP system, the task of the learner is then to find a WW Cohen/Artificial Intelligence 79 (199.5) 1-38 7 logic program P such that P, together with the background the data. theory, is a good model of To account for the background learnability. One way of doing by the target concept [ 231 and Dieroski models knowledge, the model of that are entailed in this paper, we will follow Haussler et al. [ 161 in using a closely related formalism which more directly this is to allow examples it is necessary to be clauses [ 20,27,45]. to extend However, the typical use of background knowledge in ILP systems. the set of pairs of the form If LANG is some set of definite clauses and DB is a database, denotes represents the extension of C with respect to DB, as defined of all facts e such that C A DB k e. If DB is some set of databases, denotes will be called a language family. The set of definite clauses LANG will be called a clause language. then LANG[ DB ] (C, DB) such that C E LANG. Each such pair the set then LANG[ VB ] the set of all languages LANG[DB] where DB E VB. Such a set of languages in Section 2.1-i.e., In this paper, we will consider primarily the learnability of language families, using to the usual set of and the notions of pat-learnability in addition algorithms learning that accept a database training examples. The following definitions to this new setting. polynomial predictability as input extend Definition 3. A language DB E DB there is a prediction family LANG [ DB] is polynomially predictable iff for every algorithm PACPREDICTDB for LANG[DB] . A language nomially predictable in time polynomial fixed to be DB, is a prediction family LANG[VB] is uniformly polynomially predictable iff it is poly- and there is an algorithm PACPREDICT(DB, Sf, S-), which runs in all of its inputs, such that PACPREDICT, with its first argument algorithm for LANG[DB] . The (uniform) pat-learnability of a language family is defined analogously. Intuitively, a language family is predictable if it can be predicted regardless of the family is uniformly predictable if there is a single prediction database DB, and a language algorithm Notice that works for all databases. that PACPFEDICT(DB, S+, S-) must the size of the database DB. Thus uniform predictability requires the prediction (or learning) algorithm run in time polynomial in all of its (and pac- to scale well with the size including inputs, learnability) of the background database DB. Finally, or less. Most of the results let us define a-DB to be the set of databases containing only facts of arity a in this paper will be in one of the following is uniformly pat-learnable. l For any fixed constant a, LANG[~-DB] forms: in LANG[DB] are pat-learnable, Such a result means clauses that works for any database DB, and furthermore only polynomial exponential positive that even if one allows an adversary choice of the database DB, learning algorithm time requires time it may require is a strong in the size of the database DB. (However, arity a of facts in the database.) This that there is a known that the algorithm in the maximum the learnability result about in LANG. of clauses l For every a 2 a0 (where a0 is some small fixed constant, say a0 = 3) LANG[ a-DB] is not predictable. Such a result means LANG[DB] representation of clauses that for at least some databases DB E a-DB, are not predictable (and hence not pat-learnable clauses in regardless of the used for hypotheses). This is a negative result about the learnability in LANG. The notions of uniform pat-learnability is an additional and predictability it may seem odd to allow an adversarial however, because extend the standard models input to the learner. The standard and all target concepts; we have simply the learning model worst case also over all possible choices of a database. At is if the database DB is such that the target concept cannot in DB. or is only expressible by an extremely then the learning to find an accurate hypothesis time and sample complexity may grow with the size of the target concept is actually worst case over all databases DB that of the target concept can be to the ILP setting, where the database models are worst case over all distributions made lirst glance, reasonable, be expressed using predicates defined large concept, (since quickly C E LANG[DB] are “appropriate” found using We will ). Thus the model in the sense that a concise the size of a database DB. The parameters choice of the database. This typically use II/, to denote the predicates defined is not required representation in DB. system IZ<,, 11~ and II), all measure, requiring value in keeping to consider the learner the results in some sense. to be polynomial the size of the learning problem, and we are in all of these size measures; while there is some the casual reader may find it easier these different measures separate, in terms of a single-size measure II = lip + nb + n,. Example. As an example muternal-grandmother, the user might provide the database of an ILP learning problem, to learn the predicate DB = { father( charlie,william ). mother(charlie,susan) . father( susandan father( william,maurice), ). mother( susanruth), mother( william,caroline), father( rachel,maurice) , mother( rachel,caroline) , father( elizabeth,warren ), mother(elizabeth,rachel)} and the examples S’ = { maternal_grandmother( charlieruth ). maternal_grandmother( elizabeth,caroline)}, S = { maternal_grandmother( charlie,dan) , maternal_grandmother( william,caroline), maternal_grandmother( ruth,dan). maternal_grandmother( mauricesusan) ). In this problem, and the size of the examples I-DEPTHDETERM the user’s database DB is in 2-DB, the size of the database is nr = 2. An ILP learning system for the clause (see below for definition) might produce the hypothesis is Q = IO, language H = maternaI_grandmother( X, Y) t--mother( X. Z) A mother( Z, Y). WW Cohen/Ar@cial Intelligence 79 (1995) 1-38 9 If the learning system were a pat-learning (if S+ and S- were sufficiently then could make some guarantees about the user provides only the inputs S+, S-, and the database DB. large, and drawn system with a known sample complexity, one that from a fixed distribution) the error rate E of the learner. Note, however, 2.4. Sample complexity of learning logic programs In typical ILP problems the examples will all have the same predicate symbol p and arity n,; thus, in effect, the predicate and arity of the head of the target clause are given. One important fact to note is the following. Theorem 4. Let DATALoC~)'" be the language of all function-free clauses that have a head with predicate symbol p and arity ne. Then for any jixed con- stant a and any DB E a-DB, of DATALOC$“~ [ DB] in n,, nt and nb (where nb is the size of DB). the Vapnik-Chervonenkis is polynomial non-recursive dimension [4] Proof. We will establish an upper bound on the number of semantically different clauses. A DATALOG clause of size n, can contain at most n, +a~ distinct variables, as at most in the head, and at most an, variables can appear in the body; n, variables can appear (n, + ant>“c possible clause heads. Since thus there are at most there are at most nb in the database, and each literal consists of one such predicate predicates that appear that can appear symbol and a or fewer variables, in the body of a clause these two bounds together, that succeeds with the database DB. Putting the total number of semantically there are at most nb(n, +a~)~ different clauses literals is (n, + an,)“’ . (nb(np + ant)a)n’. The VC dimension is bounded by the logarithm of this quantity n, log, (% + a&) + nY log, (nb(& + ah)“> which is polynomial in nb, n,, and nt. 0 Blumer et al. [4] show that if a concept class has polynomial VC dimension, then sample size, any consistent hypothesis H of minimal or near- size will with high confidence have low error. More specifically, any algorithm of the size of the for a certain polynomial minimal A that outputs a consistent smallest consistent hypothesis will satisfy all the requirements perhaps, Thus, of pat-learning-except, time. the requirement the following that the learner runs in polynomial that is within a polynomial hypothesis for DATALOG, except simple procedure will satisfy all of the requirements the requirement of uniform that the learning program be in increasing order of that is consistent with the sample. Since this paper con- all non-vacuous DATALOG clauses that are restrictions of DATALOG, this means that if computational in this paper are pat-learnable. considered is ignored, all of the languages then, is when polynomial-time learning is possible. time: enumerate pat-learnability polynomial size, and return siders only languages complexity The central question we address, the first clause 2.5. Constant-depth determinacy arid previous results then the input variables of the literal B, are those variables appearing several useful restrictions If A+-~ RI A A B, is an (ordered) on definite definite in Bi that in Bi are called if for every possible ‘A B,_t : all other variables appearing (with respect to DB) introduced Muggleton I37 1 have and Feng clauses, which we will now describe. clause, also appear in the clause A++Bl A’ output variables. A literal B, is determinate substitution and B,_,g E DB there is at most one substitution a literal and the binding of the input variables. is determinate if all of its liter& is determinate A clause g that unifies A with some fact P such that Bla E DB, B~(T E DB,. . . , 13 so that B,& E DB. Less formally, if its output variables have only one possible binding, given DB are determinate. Informally, determinate Next, define the depth of a variable appearing clauses are those that can be evaluated without backtracking in a clause A+-Bl A. in the head of a clause have depth zero. Otherwise, interpreter. .A B, as follows. let B; be the first the variable V, and let d be the maximal depth of the input variables is the maxima1 depth of any Variables appearing literal containing of Bi; then the depth of V is d + I. The depth of a clause variable in the clause. by a Prolog Example. The clause maternal_grandmother( C, G) ---mother( C. M ) ,? mother( M, G) (assuming mother is determinate one. for the variable M, and hence the clause has depth one. Assuming enclosed-paper and length are determinate, is functional the clause ). The maximum depth of a variable is that the predicates unwelcome-mail ( E) +--envelope( E) I enclosed_paper( E, P ) A must-review(P) length( /? L) 1, gt50( L) A is determinate variable L has depth two. and of depth two. The variable P I‘rom this clause has depth one, and the An interesting class of logic programs is the following. Definition 5 (ij-determinate) over a database DB E j-VB A determinate clause of depth bounded by a constant i is called ij-determinate. learning program GOLEM, which has been applied ij-determinate programs. Closely inductive logic programming to a number of practical also related restrictions systems, including The problems have been adopted by several other FOIL [ 17,30,38 1, learns [ 471 and LINUS [ 321. of non-recursive The learnability ij-determinate clauses has also been formally ied [ 161. For notation, of depth i-DEPTHDETERM[ i or less; the language j-DB 1. One important family of ij-determinate clauses result is the following. let i-DEPTHDETERM be the language of determinate stud- clauses is thus denoted W W CohedArtijicial Intelligence 79 (1995) 1-38 I1 Theorem 6 (Dieroski, Muggleton guage family i-DEFTHDETErw[ j-Z)B] is uniformly pat-learnable. and Russell [ 161). For any $xed i and j, the fan- that neither (or worse) _ it has been shown is nor pat-learnable that a single clause is NP-hard these negative condition does not hold; specifically, Other previous work has established clauses of fixed depth nor the language of determinate if the the ij-determinacy clauses language of indeterminate [ 271. The proof of these facts is based on showing of arbitrary depth is pat-learnable that there are sets of examples such that finding a single clause in the language consistent with the examples Unfortunately, only show learning consistent with all of the examples. Most ILP learning clauses, not a single clause, and the results do nor show that learning using expressive times called representation-dependent. 2 One of the goals of this paper the positive representation-independent first, however, we will describe orem 6. These analytic importance, because they to output a single clause learn a set of this more results are some- is to develop result of The- the results are of limited practical is required that complement shortly; results will be developed tool used to obtain systems, however, to be hard when Such negative is intractable. representation the learner the results. learnability learning results 2.6. Reducibility among prediction problems Pitt and Warmuth problems, analogous used method of showing [44] have introduced to the notion of reducibility a notion of reducibility for decision problems between prediction that is commonly to prove a problem NP-hard. Prediction-preserving reducibility is essentially a that one language is no harder to predict than another. reducibility). Let LANGE be a language over do- Definition 7 (Prediction-preserving main Xi and LANG2 be a language over domain X2. We say that predicting LANGi reduces to predicting LANG2, denoted LANG, 9 LANG2, fi : Xi --f X2, henceforth henceforth the instance mapping, and a function : LANGE + LANG2, if there is a function called called ( 1) x E C if and only if fi( x) E fc( C)- the concept mapping, so that the following all hold: i.e., concept membership is preserved by fc the mappings; the size complexity the size of concept fi (x) can be computed (2) (3) of fc(C) is polynomial in the size complexity of C-i.e. representations is preserved within a polynomial factor; in polynomial time. Note that fc need not be computable; also, since fi can be computed in polynomial time, fi(x) must also preserve size within a polynomial factor. Intuitively, fc( Ci ) returns a concept C2 f LANG2 the same decisions about concept membership-on that will “emulate” Cl-i.e., make that have been “prepro- examples * The prototypical in a broader setting if the hypotheses of the learning the richer language of k-CNF [ 421. example of a learning problem is learning k-term DNE Assuming that is hard in a representation-dependent setting but not that RP {NP, pat-learning k-term DNF is intractable system must be k-term DNF, but tractable if hypotheses can be expressed in scheme then one possible for LANG? exists, to predicting LANG2 and a fi. If predicting LANG, reduces cessed” with the function learning algorithm for learning concepts from LANG, would be the following. First. convert any examples of the unknown con- cept Cl from the domain Xl to examples over the domain X2 using the instance mapping .f’,. If the conditions of the definition hold, then since Cl is consistent with the original the concept ,fc (Cl ) will be consistent with their image under f;; thus running examples, for LANG: should produce some hypothesis H that is a good the learning approximation to map H back into the .f, ~’ may be difficult or impossible. However, original in Cl: given an example x from the original H can still be used to predict membership domain Xl, one can simply predict language LANG,. as computing of ,f(.( Cl ). Of course, \ E Cl to be true whenever it may not be possible algorithm Pitt and Warmuth algorithm prediction [43] give a more rigorous argument to the following for LANGI. leading f;(x) E H. that this approach theorem. leads to a Theorem 8 (Pitt and Warmuth ). A.s.rur?re rhar LANG, 9 LANG?. Therz the following hold: l Jf’ LANG? is polyonliall~ l !f’ LANC,, is rrot polytmnial!\ predictuble, rhrr~ LANG 1 is poly~omially predictuble. predictable. tlzetl LANGE is not polynomially pre- dictable. The second case of the theorem allows one to another; this is useful because language prediction secure. The first case of the theorem gives a means of obtaining for LANG,, given a prediction is as hard as breakin g cryptographic for LANG?. schemes for a number of languages, to transfer hardness results it is known from one that to be a prediction algorithm that are widely assumed If ,f; is one-to-one and ible”; pat-learnable. preserving in this case it can be shown For example, reduction between then the reduction that if LANG? is pat-learnable is said to bc “invert- is also then LANG, the proof of Theorem 6 is based on an invertible prediction- Ij-determinate clauses and monotone monomials. algorithm ,f(. _’ is computable, 3. Log-depth clauses are hard to learn In the next two sections, we will investigate the models described above: pat-learnability point will be Theorem Theorem 6 by generalizing if the corresponding languages 6-in are learnable. and polynomial particular, we will consider generalizing the learnability in of definite clauses predictability. Our starting the result of in various ways, and seeing the definition of i,j-determinacy relaxing the restriction in the list of clauses We will first consider [ 371 argue that many practically useful programs are limited that clauses have constant depth. Mug- in depth; it the case that the more complex clauses have greater depth. It might be that clause depth d is some slowly size (as measured by either clause size q, database size gleton and Feng however, is frequently plausibly argued that it is more reasonable growing fq,, or example size II,,). they provide as examples function of problem their argument, to support to assume WW Cohen/Artificial Intelligence 79 (1995) 1-38 13 output I I I Y2 I Y5 I OR Y4 I OR I Yl NOT AND I’ Xl J x2 L x3 Y3 OR J L x4 X5 circuit( Xl ,X2,X3,X4,X5) + A not(Xl,Yl) and( X2,X3,Y2) A or( X4,X5,Y3) A A or(Yl,Y2,Y4) or(Y2,Y3,Y5) A and( Y4,YS,Output) A true( Output) Fig. 1. Constructing a determinate clause equivalent to a circuit. The key result of this section is that increasing the depth bound to be even logarithmic in the size of examples makes determinate clauses hard to predict. Theorem 9. For any constant a > 3, the language family (log n,) -DEPTHDETERM[ a-VB] is not polynomially predictable, under cryptographic assumptions. 3 from boolean circuits of Proof. The proof is based on a prediction-preserving depth d to determinate be the language of depth-d boolean circuits over n binary variables containing only AND, OR and NOT gates with two, with the usual semantics and complexity measures. 4 We will show that there fan-in exists a database DBcrn E 3-VB, containing only eleven atomic facts, such that clauses of depth d. Let d-CIRcurr reduction d-CIRCUIT a d-DEPTHDETERM[ D&tB 1. The expressive power of depth-bounded [ 51; in particular has been well studied predict under cryptographic assumptions immediately from this reduction and Theorem 8. boolean circuits, as well as their learnability, it is known that log-depth circuits are hard to follows [25, Theorem 41. Thus the theorem The construction language used for the is a binary vector bl . . . b,,, which is converted by the instance mapping in Fig. 1. An example in the reduction is illustrated circuit s More precisely, the prediction problem is intractable the quadratic problems are widely conjectured residue problem, inverting the RSA encryption to be intractable. if one or more of the following are intractable: function, or factoring Blum integers solving [ 251. These 4 Specifically, the complexity assignment the extension of a circuit C is the set of all variable assignments of an instance is the number of inputs (i.e., such that C outputs a “l”, an the length of the binary vector representing to those inputs) and the complexity of a circuit is the number of gates in the circuit. ,j’, to an atom of the form circuit( 111. . I>,, 1. For example, converted functions und, or. and not, as well as a definition of the unary predicate is ;I “I”: succeeds whenever the vector 1001 I would be to circuit( I, O,O, I, I ). The database DB CIR contains definitions of the boolean true, which its argument DB~~R = { utld( 0, O,(J). CZH~( 0. I. 0). or( 0.0, O), or(0, I, 1) , t~ot( 0, I ), ~~t~d~1,0,O),c7ttrl~1.1.I~.or1I,0.1),~~r(1,1.1), tzot(l,O). true( I ) }. Finally, the concept mapping gate C, in the circuit as there is a single ,f, 14 a:, indicated for each literal L, with a single output variable F, defined in the figure. To be precise, ut7d( Zil , z;2. ,: ), if‘ (;, ib an AND gate, L, E or(Z,i,Z,z,K). if G, is an OR gate. tzot(Z,l. K). i if G, ib an NOT gate. where Assume without inputs in each cast the Z,,, are the variables that correspond loss of generality to a gate G, before G, in the ordering: that the numbering then the clause fc(C’) is simply to Gi. to the input(s) for the G, always puts all the ,f; ( c‘ ) s citr.uif( XI. X,, 1 Notice that the construction preserves depth. The algorithm presented by Muggleton and Feng for learning a single ij-determinate in the depth of the clause. The result above shows that no this bound much, e.g., that in depth. The result holds even for learning systems clauses exists that improves representation for their hypotheses (e.g., systems that approximate for determinate is doubly exponential clause learning algorithm that is even singly exponential USC an alternative one clause with several ). Recent work has shown are drawn from a uniform distribution tions about the distribution predictable. that log-depth circuits arc hard to predict even if examples fairly strong assump- clauses of examples will not make log-depth determinate 126 ] ’ Thus even making 4. Hard-to-learn indeterminate clauses The results of Section 3 indicate of ii-determinate clauses by increasing that one is not likely to be able to generalize the depth bound. We will now consider the class relaxing ’ ‘This exe requires the additional cryptographic assumptum that solving the II x II Itc subset sum is hard. W. W Cohen/Artificial Intelligence 79 (I 995) l-38 15 the second key aspect of the ij-determinacy determinate. While for many problems determinacy real-world problems using only determinate able to relax this restriction. literals is an appropriate for which some of the background knowledge restriction: the condition that clauses be restriction, there are cannot be accessed to be [ 111; thus for practical reasons, it would be useful several plausible ways lead to languages In particular, we first consider bounding to relax the determinacy that are hard to pat-learn it in any other way, and show that this leads In this section, we will consider restriction, and show that these relaxations and (in most cases) also hard to predict. depth of a clause, but not restricting a language variables predict as DNF. We then consider an alternative of indeterminacy pat-learnable. in an indeterminate is also bounded, clause, and show that this language set of restrictions and show that that language that is hard to predict. We then consider bounding the to the number of “free” is exactly as hard to in which the degree but not is predictable, 4.1. Constant-depth indeterminate clauses The most obvious way of relaxing that are not determinate. Unfortunately, clauses hard to learn. Letting ~-DEPTH denote we have the following result: ij-determinacy would be to consider constant-depth this leads to a language that is the language of all clauses of depth k or less, family Theorem 10. For a > 3 and k 3 1, the language predictable, unless NP C P/Poly. family k-DEPTH[a-DB] is not [47] shows that if a language LANG is polynomially Proof. Schapire every C E LANG can be emulated by a polynomial-sized therefore, for some C E I-DEPTH[DB] to construct a polynomial testing membership then circuit. To prove the theorem, sized database DB E 3-VB such that in C is NP-hard. it is sufficient predictable, the following predicates: Let DB contain l The predicate boolean(X) l For k = l,... , n, the predicate is true if X = 0 or X = 1. linkk(M,vX) is true if M E {-n, . . , n}, V E (0, l}, X E (0, 1}, and one of the following conditions . . . , - 1) or also M E {l,. holds: - M = k and X = V, - M= - M # k and M $ -k -k and X = TV, (and X and V have any values). l Finally, the predicate sat(V1, V2, V3) is true if each K E (0, 1) and if one of VI, V2, V, is equal to 1. Now, consider a 3-sat formula 4 = Aa, ( li, V li, V li, ) over the n variables x1, . . . , .rn We will encode this formula as the following arity-3n atom where mi, = k if Ii, = xk and mi, = -k if li, = q. Now consider the clause CS,Q below: W W Cohen/Arfific~ul lntelligencr 79 (I 995) I-3X SUt(MI,,Mlz3Ml, ,.... M,,,M,,,,M,,)- A boolean( Xk ) A L=I to possible values for the variables xr , that can be assigned to values introduce The first two sets of literals XL’S correspond and the x,‘s correspond 4. The third set of literals ensures then V, and Xk are complements; consistent with some assignment ensures that 4 is satisfied. variables: two sets of depth-l indeterminate the ,x, over which 4 is defined, in to the literals that if I,, = x1; then v, = Xk, and that if l;, = z that the V,,‘s have values of literals i.e., the last conjunction in 4 succeeds: l;, that appear thus ensures to the s,‘s. Finally, this conjunction that the values given to the \i,‘s are such that every clause Thus we conclude that 4 is satisfiable iff the clause Cs~r succeeds on the instance e++. and hence that determining whether CS/\T succeeds must be NP-hard. Finally, notice that the boolean predicate requires seven facts to define; and (since each linkk predicate 8n facts to define) /IDBJI is bounded by a polynomial in II. This completes the link predicates the proof. requires two facts to define; the sat predicate requires at most 2n. 2 ’ 2 = together require only 8n2 facts to define. Hence in II. It is also clear that CS,U is of size polynomial 9 Therefore, in the remainder of this section, we will consider the learnability guages strictly more restrictive clauses with a bounded number of “free variables”. than k-DEPTH. We will first consider the learnability of lan- of 4.2. Clauses with k ,free variables Let the free variables of’ a clause be those variables clause but not in the head. One reasonable with only a small number of free variables. This restriction by Haussler restriction [ 231. that appear to impose in the body of the is to consider clauses to that imposed is analogous We will consider now the learnability recursive clauses containing necessarily of depth at most k; also restricting clauses can be evaluated be quite simple, notice in polynomial of the language k-FREE, defined that clauses to be all non- in ~-FREE are that time. While at first glance this language seems to as true the number of free variables ensures that a clause p(X) +q( X. Y) classifies an example p(a) at most k free variables. Notice WE! Cohen/Art@cial Intelligence 79 (1995) l-38 17 Database: fori= l,...,k truei(b,y) falsei( b, y) for all b, y : b = 0 or y E 1, . . . , r but y # i forallb,y:b=loryEl,...,rbuty#i DNF formula: (qAUjAu4) v (EATg v (U] Auq) Equivalent clause: dnfC%,X2,X3&) + A falsel(X3,Y) A truel(X4,Y) A truel(Xl,Y) false:! (X2,Y) A false2 (X3,Y) A trues(Xl,Y) A false3QLY). Fig. 2. Constructing an indeterminate clause equivalent to a DNF formula. exactly when q(a, 61) E DB V . . . V q(a, b,) E DB, where bl, . . . , b, are the possible variables allow some bindings “disjunctive” to be expressed by a single clause. of the (indeterminate) variable Y. Thus, indeterminate encode any boolean expression (over a suitable database). This leads to the following the expressive power of indeterminate to in disjunctive normal form 6 using a single k-free clause theorem. free variables concepts As it turns out, we can exploit Theorem 11. For any constants k-FRm[ a-VB] is predictable, then DNF is predictable. a 2 2 and k 3 1, if the language family Proof. As in Theorem 9, the statement of the theorem reduction: follows directly from a single Lemma 12. Let r-Tl?ItMDNF database DB, of size polynomial denote the language of r-term DNFformulae. There is a in r such that r-B_NF a 1-FREE[DB,I. (To see that the theorem that a DNF formula of n can have at most n terms, and that any DNF formula with fewer than n thus any DNF to exactly n terms by adding terms of the form ulK; the lemma, notice follows from complexity terms can be padded of size n, or less could be predicted using l-free clauses over the database DB,,.) Proof. The construction DB, contain sufficient atomic facts to define the binary predicates false, that behave as follows: this reduction on which is based is illustrated in Fig. 2. Let true1 ,falsel , . . . , true,, l truei(X,Y)succeedsifX=l,orifYE{l,..., l falsei(X,Y) succeedsifX=O,orifYE{l,..., i-l,i+l,..., i-l,i+l,..., r}, r}. 6 Recall that boolean formulae of the form Vi Aj lij are said to be in disjunctive normal form. We denote the length it (i.e., the number of literais the number of variables and the size measure for a formula being for examples being the size measure an assignment) as DNF, with this language of a bit vector encoding contains. Since 2r - 1 facts are required is O(G). to detine each of these predicates, the total size of DB, We now define the instance mapping f; to map an assignment 01 to map a formula of the form b,, to the atom dr~fc 0, , , O,,). The concept mapping .f’, is defined to the clause .f;(4) =dr!f‘(X ,..... x,,)--/\A& !=I ,=I / \ where Lit,., is dclined as true, ( x,, Y) fulse,( x,, Y) il’ l,, = L’, . it‘ I,, = I’, Lir,, Z { f;(e) Clearly polynomial size, this reduction is polynomial. and f<(d) are of the size a:, (J and 4 respectively; since DB,. is also of Next, notice that in fc(d) (if the clause fc (4) if d is true for an assignment is to succeed ) it can be bound there is only one variable Y not appearing in the head, and to only the r values 1,. . . , r. Thus 111 b,,. then some term T, = r\T=, I,, must be true; in this (with Y bound i’ # i case r\ia, Liti,, succeeds also succeeds with Y bound then each T, fails, and hence for every possible hinding of Y some conjunction A’;=, Liti, will fail. Thus concept membership to the value i) and r\;L, L&f,; for every to i. On the other hand, if 4 is false for an assignment, is preserved by the mapping. q It also should be noted an r-term DNF expression; pat-learning k-term DNF that every clause 111 I-FREE[DB,] thus Lemma 12, together with existing hardness can be translated into results for [ 241. leads to the following result. Observation 13. For a > 2 the language,fLmily I-FREE[ a-DB] is not pat-learnable. It is straightforward to obtain a number of other similar representation-dependent clauses results for pat-learning [ 271. However, hardness rem 1 of Kietz then these are of limited results here. We turn instead that are harder to learn than DNF. The answer in ~-FREE, somewhat along the lines of Theo- that learning DNF is hard, interest, given Theorem 1 I; hence we will not develop such in ~-FREE to another question: whether there are languages to this question is no: if one accepts the conjecture Theorem k-FREE[ U-DB] 14. v DNF is predictable then ,fijr ull constants a and k, the language family is uniformly predictable. It suffices Proof. DB E N-DB to show that for all constants ~1 and k and every background theory KM? Cohen/Art@ial Intelligence 79 (1995) 1-38 19 k-FEEE[DB] _a DNF if this reduction holds, one could use the hypothesized prediction Below we will give such a reduction for algorithm for an arbitrary since DNF to predict k-FEEE[DB]. database DB . Let C be a clause can be determined contains an equality predicate, one can also assume of C are distinct.7 Thus the head of C can be determined from any of the positive examples, and because we assume in k-FEEE[ DB] . The predicate symbol and arity of the head of C that DB in the head that all of the variables from the examples. Notice also that each clause has at most n, + k variables, and hence of variables that could serve as arguments database DB be of size nb. Since the database DB contains there are at most nb . (n, + k)O possible to a literal. Let there are only the at most nb literals Bi, . . . , Bnb.(,,,+kp (n, + k)” a-tuples background predicate that can appear symbols, Now, let C = AtB,, in the body of a k-FREE clause. A . . . A B,, be a clause in k-FEEE[ DB] . Recall that C covers an example e iff there exists some substitution 8 such that B,,(T~~EDBA...AB,,~B,EDB ( 1:) theory DB is of size nb and all predicates where 0, is the most general substitution ground most unb constants to the k free variables. Thus, let us introduce in DB, and hence only such that AB, = e. However, since the back- are of arity a or less, there are at . . , (+(amF (anb) k possible substitutions 01,. the boolean variables Uij where i ranges from one to nb.( n,+k)’ a literal, and j ranges from one to (anb)k and represents a substitution. and represents Notice that the size of this set of variables fi(e) instance mapping as follows: uij will be true in Q above. Finally, the DNF formula let the concept mapping of an example e to return an assignment Q is polynomial in n, and ne. We will define the to these variables if BigjO, E DB, where dc is as defined fc( C) map a clause C = A+B,, A . * . A B,, to if and only (anL2 1 fc(C> E V /1\uc,j. j=l i=l Since both polynomial and hence and the proof. 0 and 1 are polynomial ( unb)k size. It also can be verified this mapping preserves concept membership. This completes is of that fc( C) is true exactly when Eq. ( 1) is true, the reduction (in n,, nb, and nt) the formula fc(C) The predictability theory for several years. Thus, while this result does not actually settle the question of whether of DNF has been an open problem in computational learning ‘More precisely, for every is an equivalent clause C’ in which constraints larger are represented by conditions than C’. target clause C in which the variables there in the head of C’ are distinct, and the necessary equality in the body of C’. It is easy to see that C’ need be only polynomially in the head are are not distinct, the variables 20 W W Cohen/Arf@%d Intellrgmce 79 (1995) l-38 indeterminate require a substantial theoretical advance. clauses are predictable, it does show that answering the question will 4.3. Clauses with bounded indeterminaq If one believes that DNF is hard to predict, suggested by this result of a clause it does suggest some possible first restriction indeterminacy” that is needed to emulate it may be that bounding will lead to a predictable that predictability show introduced to a language. restrictions that might lead to learnable is based on the observation then the result above is negative; however, languages. The that the “degree of is closely related to the number of terms in the DNF formula for any fixed k. Hence, associated with a clause this would is it, and that k-term DNF is predictable the number of possible language. Such a result would be useful: decreases gradually (if not learnability) as indeterminacy substitutions intuitively, investigate in particular, In this section, we will It turns out that this intuition language to a certain is correct: in result learnability of clauses with bounded the weaker model of predictability. We will first present a fairly general version of this result, and then consider some concrete the result of Theorem 6 can be extended This gives us a positive of the general result. such a restriction. indeterminacy. instantiations 4.3. I. Bounding the indeterminacy of u clause We will want to talk about clauses that are almost, but not quite, deterministic; hence the following definition. Definition 15 (Effectively tively k-indeterminate dure SUBST( e, DB) having k-indeterminate). (with respect to X) A language LANG[ DB] iff there is a polytime that, given any e E X, computes a set of substitutions computable is called eflec- proce- (01, . . , 0,) the following properties: I is bounded above by k, l the number of substitutions l for every C E LANG [ DB] . if e is in the extension of C, then every most general in the set of 0’ that proves e to be in the extension of C is included substitution 0,‘s generated by SUBST. Note that since duplications are allowed among the 8i (i.e., it might be that 13; = @.; loss of generality that I= k. for some i $ j) we can assume without a language Informally, is k-indeterminate substitutions small set of candidate be necessary. As one example of such a language, l-indeterminate: prover Some additional here, SUBST can be implemented examples are given in Section 4.3.2. the single substitution to generate that suffice for all the theorem proving ij-determinate if given an instance e, one can produce a that might clauses are effectively theorem by using a Prolog style that proves e to be in the extension of C. The following property will also be important: Definition 16 (Polynomial nomial literal support literal support). A language iff for every X,,,, and every DB E VB family LANG[Z)B] has poly- there is a set of literals LIT WW CohedArtijicial Inrelligence 79 (1995) l-38 21 and a partial order + on LIT such that of LIT is polynomial l the cardinality l LANG[DB] in n, and IIDBII; is exactly those clauses AtBl A . . . A B,, where A is fixed, all the Bi are members of LIT, and the body of the clause satisfies if Bi+Bj and Bj is in the body of the clause, clause and appears to the left of Bj. the following restriction: then Bi also is in the body of the One example of a language with polynomial in this case, the polynomial determinate clause can be obtained by a simple counting argument is the relationship clauses: literal support bound on the number of literals [ 371, and the ordering is the language of ij- in a function Bi5B.i iff the input variables of Bj are bound by Bi. a key property in fact generalizes clauses pat-learnable. in this case, the ordering This definition ij-determinate literal support; be used to ensure The example of k-free clauses shows that polynomial ensure that clauses are “linked” learnability. that, together with determinism, makes The language of k-free clauses also has polynomial function, or might function might be a constant in such a way as to reduce indeterminacy. literal support is not sufficient to The principle result of this section these two restrictions cult to extend further in Section 4.3.3. yields a predictable this predictability result is the theorem below, which shows that imposing it is diffi- result. This issue is discussed language of clauses. Unfortunately, to a pat-learning Theorem 17. Let k-INDETE-LS support family k-INDETERMPLS [ a-VB] literal that is also effectively k-indeterminate. Then for any jixed a, the language language with polynomial be any clause is uniformly predictable. Proof. The proof is analogous learning will show that for any DB E a-DB in k-INDETERh@LS a clause to the proof of Theorem 14, except that we will reduce i.e., we a k-term DNF expression: to learning k-INDETERMPLS [ a-DB] a k-=RMDNF. this, since k-term DNF is predictable using theorem The follows k-CNF as a hypothesis the language Since from immediately space k-INDETERh@LS has polynomial [ 421. is some set of literals B1, . . . , B, such that each clause C in the language can be written C = /I... A B,,. As before we will introduce a set of variables Uc,j where ci ranges A+B,, from 1 to n and encodes a literal B,, and j ranges from 1 to k and encodes a substitution. instance mapping will map an example e to an assignment Q over literal support, these kn there The as follows. First, variables of effective k-indeterminacy ordering of these substitutions Q is then constructed the procedure SUBST( e, DB) guaranteed is used to generate a set of k substitutions by the definition 81, . . . , ok. The can be arbitrary (we will see why shortly). An assignment in which Uc;j is true if and only if B#j E DB for 0,. 22 W W C‘oherr/Art~fic~ictI Itttelligewr 79 (I 995) I -3X Finally, define the concept mapping ,f; to map the clause C = AtB,, A . A B,, to the k-term DNF formula Note that when a clause C covers an example e. then it must be that some 0, makes the clause true, and hence one of the terms of j’.(C) will be true; conversely, when C doesn’t cover e, no terms of .f; (C) are true. So these mappings preserve concept membership. Notice also and can even be different that for different examples. the ordering of the 0; is irrelevant, E 4.3.2. Languages satisjying the restrictions Although the result above is stated quite generally, restrictions that enforce support, and devise natural syntactic have polynomial language k-FREE[DB] clauses over databases of constant of databases of size less than or equal to I: is suggested by the proof of Theorem d t jlDBjlk-’ In e erminate. is effectively it is nonetheless these two key restrictions: 14, which shows Thus the language rather difficult to that clauses One possible language family of k-free the set that any size I is predictable. Thus letting VB, denote that they be effectively k-indeterminate. Observation 18. For jixed k and 1 tlze language predictuble. jbmily k-FREE[VBl] is uniformly Note however that the time complexity of the most natural prediction algorithm (where one predicts k-term DNF using k-CNF) a practical algorithm. Also, severe restriction. restricting is 0( 11,“)) which seems rather high for is a rather the size of the background database Another possibly more useful way of defining a language meeting the restrictions above is as follows. l First, specify a tuple of II,, variables. The head of every clause in the language will have as its arguments For instance, fix the arguments in learning the tuple T. l Next, specify some small set of output -+ such that each output function when used in a clause Continuing head of the clause), one might specify the example given above family relationships like grandfather or nephew, one might to be the two variables X and Y, in that order. literals S = {L,, , L,} and an ordering literal can have at most d possible bindings, in a manner consistent with -+. (in which X and Y are the arguments to the the following set of c = 6 output literals S= { LI =parent(X,A),L2 =parent(yB), L3 = parent( A, C), Ld = pnrent( B, D) , L5 = spouse( X, E), L6 = spouse( P F)} together with the following ordering -<,s: WW Cohen/Artificial Intelligence 79 (1995) 1-38 23 L2 -w4. this ordering Given bindings (assuming can have at most one binding for this set S, we have d = 2. constraint, that a person has at most two parents) literals Ll, L2, L3 and L4 can have at most two and literals L5 and L.5 (assuming each person has only one spouse). Thus, l Finally, define the language S-OUTPUT list T and bodies with the argument set S, and used in an order consistent with -+. to be the set of clauses that contain output literals selected that have heads from the in S-OUTPUT for generating It is easy to show that a clause procedure for the free variables substitutions arity a the language S-OUTPUT has polynomial free variables. Hence: substitutions is simply is effectively (ISI . d)-indeterminate: the to generate all possible in the literal set S. Also, for databases with a fixed literal support, since it has at most alS[ to backtrack Observation 19. For S-OUTPUT[ a-DB] in S can have at most d bindings. every is uniformly predictable, provided constants a, c, and d, and every that ISI < c, and every literal set S, literal The time complexity of the k-CNF-based It should be noted that there is no a priori way to choose the literal set S and ordering a language S-OUTPUT requires additional user the user function 4s. Thus in practice, specifying input. For example, had to specify learning problem given above, to the examples and the background database DB) in the family (in addition is O(necd). relationship prediction algorithm l the pair of variables X, Y that must appear l the set of indeterminate literals S = {parent( A, X) , . . .} that can appear in a hy- in the head of the hypothesis clause; pothesis clause; l the ordering this In respect ~-FREE, which require function 4s. the clause language S-OUTPUT differs from i-DEPTHDETERM and little user input to specify. This result also can be generalized somewhat. One generalization clauses also have polynomial literal support. that ij-determinate combine a new predictable the language of ij-determinate clauses with the language S-OUTPUT language, of clauses of the form It is thus possible is based on the fact to to obtain AtBl A... AB,ADkA...AD, A. . .A B, is ij-determinate where AtBl provides one way of introducing of ij-determinate clauses without making prediction intractable. and A+Dl A. . .A D, is S-OUTPUT. This result into the language a small amount of non-determinism 4.3.3. Further discussion It should be emphasized of reasons why the result that although this is a positive result, is rather weak. First, we have not shown there are a number to be the language first, because k-term DNF is hard to pat-learn, the result of learning with a reasoning the result appears to be predictable; to extend to be difficult is a disadvantage the target clause. This thus there is no way of obtaining that a clause if the ultimate goal system based on logic programs. for two the concept and second, because to k-term DNF cannot be easily reversed. The for k-term DNF yields [ 31, which that learns k-term DNF with general DNF) or even for classes of it (see, for example, (see, for example, to logic programs to pat-learnability, if the prediction algorithm used learnable learning only approximates pat-learnable, accurately is to integrate Furthermore, reasons: mapping used to reduce clause latter hypotheses describes an algorithm distributions may still be impossible with polynomial literal support. fact means that even to pat-learn clauses from an effectively k-indeterminate under which k-term DNF is directly that can be easily converted [ 331) language A second problem is that all known algorithms time exponential tolerated without the next section a different in k. This suggests imposing additional restriction on indeterminate clauses. that only a small amount of indeterminism restrictions. For these reasons, we will consider for predicting k-term DNF require can be in 5. Learnable indeterminate clauses 5. I. Highly local clauses are leut-ruble We will now consider an alternative to find a language of indeterminate being pat-learnable. restriction on indeterminate the aim that is not only predictable, but also clauses, clauses The construction a natural question makes indeterminate in general;* idea behind develop to ask is if limiting in Lemma 12 requires a free variable clauses easier to learn. This restriction, unfortunately, that appears in every literal; the number of occurrences of each free variable does not help related restriction does make learning easier. The basic is to limit the length of a “chain” of “linked” variables; we however a closely the restriction this notion more formally below. Definition 20 (Locale). Let Vi and Vz be two free variables in a clause A+Bl A.. A B,-. We say that V, touches V2 if they appear in the same literal, and that V, in$uences V2 if it either touches V?, or if it touches some variables V; that influences t$. The locale of a variable V is the set of literals {Bi,, . . , Bi,} that contain either V, or some variable influenced by V. appearing Thus injbences and touches are both symmetric and reflexive relations, and injuences is the transitive closure of touches. Informally, variable VI influences variable V, if the for V2 (when choice of a binding is the testing to see if a ground fact e is in the extension of C). The locality of a clause the possible choices of bindings for VI can affect ’ The problem is that if a database contains an equality predicate, then variables can be “copied” an arbitrary number of times. W W Cohen/Artificial Intelligence 79 (I 995) I-38 25 size of the largest set of literals illustrate locality. influenced by a free variable. The following examples Example. each free variable is underlined. In the following clauses, the free variables are highlighted, and the locale of father( E S) +-son( S, F) A husband( F, W) . no_payment_due( S) tenlist( S, PC) A peace_corps(PC), draftable(S) ccitizen( S, C) A unitedstates( C) A age(S,A) A (A 2 18) A (A < 26). thus in the third clause that the influence the variable S is not influenced by C, and hence age( S, A) is not in the locale relation applies only to free variables; Notice above, of c. Finally, let the locality of a clause be the cardinality of the largest locale of any free the language of clauses with locality k in that clause, and let ~-LOCAL denote variable or less. The principle result of this section is the following. Theorem 21. For anyfied k and a, the languagefamily k-LOCti[ a-VB] pat-learnable. is uniformly at most a new variables, to A are n, distinct variables. As every new literal labeled by the k-local clause A+-Bl A. . . A Bl. As in the that predicate symbol and arity of A are known, in the body any size k locale can contain at most n, + ak in the database in a locality has one predicate symbol and at most a arguments, there are only nb(n, + ak)” different that could appear in a locality, and hence at most p = (nb( n, + ak)O) k different 9 these localities as LOCI, . . . , LOC,. Note that for Proof. Let Sf, S- be a sample proof of Theorem 17, one can assume and that the arguments can introduce distinct variables. Also note that there are at most nb distinct predicates DB. Since each literal each of which literals localities of length k. Let us denote constant a and k, the number of distinct is one of the n, + ak variables, localities p is polynomial in n, and nb. Now, notice that every clause C of locality k can be written in the form A+LOCi, 3 s . .) LOCi, is one of the p possible than one of the LOC,. Since no free variables locales do not interact, and hence e E exr(C,DB) locales, and no free variable appears are shared between in locales, exactly when e E In other words, C can be decom- thus use of the form A+LOCij. One can where each Loci, more the different ext(A+LOCi,,DB),..., posed Valiant’s [50] into a conjunction technique e E ext(A+LOCi,, DB). of components for monomials to learn C. 9 Up to renaming of variables. In a bit more detail, the following algorithm will pat-learn k-local clauses. The learner initially hypothesizes the most specific k-local clause, namely A-LOC,. (LOC,,. The learner esis all LOC; such that e $ exf( A+-BLOC,, DB). when 3~: DB E LOC;Oru where 8, is the most general substitution can be checked To see that this condition at most ak free variables, ( an/, ) Ok substitutions used for Valiant’s procedure, then examines each positive example e In turn, and deletes from its hypoth- (Note that e is in this extension exactly such that AB, = e. time, recall that u can contain and DB can contain at most a& constants; hence at most the argument 0 Following the target concept. g need be checked, which is polynomial.) this algorithm will pat-learn in polynomial Again, algorithm this result can bc extended somewhat; for the language of clauses of the form for example, there is pat-learning where A+Bl A ,\ B, is ij-determinate and A+D, A A D,, is k-local. 5.2. The expressive power of’ locul clauses Theorem 21 is a positive result; it shows that k-local clauses can be efficiently learned formal model. The importance of this result, however, depends a great that clauses, do not seem to correspond very well to and other the usefulness of locality of k-local clauses as a representation ij-determinate typically used tasks. In this section. we will attempt to evaluate for list manipulation language; we note in logic programs in a reasonable deal on the usefulness k-local clauses, unlike the sorts of clauses programming as a bias. 5.2.1. Experimental results clause system is empirically, each of which language. Among by applying a learning [ 1 I 1. In these experiments, One way to evaluate a bias several different versions of the experimental that uses experiments of this sort are reported ILP learned programs made up of clauses ij- the clause and k-local clauses. These different versions of Grendel were then compared the literature. These experiments problems from simple is useful on many problems, notably list. However, on two of the eight benchmarks, that bias to benchmark problems. Some preliminary elsewhere system Grendel were constructed, from a different determinate on a set of eight benchmark confirmed that ij-determinacy recursive programs significantly imposing important some cases, a useful way of doing so. and restriction that it is sometimes the results suggest and indicate better results were obtained by discarding instead a locality to relax restriction. Thus, restriction, like append and considered were the determinacy the determinacy is, at least in that locality in learning languages taken WW Cohen/Art@cial Intelligence 79 (1995) 1-38 21 5.2.2. Locality generalizes ij-determinacy of locality the usefulness is to formally analyze languages. For instance, A second way to evaluate the ex- pressive power of k-local clauses. The easiest way to do this is by comparing ~-LOCAL to other k clearly must have depth k + 1 or less; thus k-LOCAL is also a restriction of the language of clauses of constant to the language of clauses with depth. However, a bounded in that the construction Lemma 12 is a length-n that has locality n, while similarly number of free variables. To see this, note the language k-LOCAL is incomparable any clause with locality clause with a single free variable the clause used has locality one, but n free variables. To summarize, for all k’, k-LOCAL g k/-FREE and k-FREE g k’-LOCAL. k-LocAt_ c (k + l)-DEPTH, but A more interesting question clauses. Clearly, clauses are not ij-determinate. depth can have unbounded since k-local clauses can include is the relationship of k-local clauses indeterminate It is also the case that determinate to ij-determinate literals, some k-local clauses with bounded locality. As an example, consider the clause p(X)+-successor(X,Y) Aql(Y) A...Aq,(Y). there is a surprising However, ij-determinate every the bound on the locality the language of clauses of constant determinate More precisely, clauses of constant depth. the following relationship the two languages: between clause can be rewritten as a clause with bounded relationship is a function only of i and j. Thus, in a very reasonable locality is a strict generalization it turns out that locality, where sense, of the language of holds between these languages. For a-‘DB, Theorem 22. d-DEE’rHDETERM[ DB] , there is clause C’ E k-LOCAE[ DB] such that C’ is equivalent to C and IlC’ll < kllCl[, where k = ad+‘. every DB clause C E d, and every every E Proof. Let C = AcBl A. .A B, be a clause. We will say that literal Bi directly supports literal Bj iff some output variable of Bi is an input variable of Bj, and that literal Bi indirectly supports Bj iff Bi directly supports Bj, or if Bi directly supports some Bk that indirectly is the transitive closure of “directly supports”.) supports Bj. (Thus “indirectly supports” Now, for each Bi in the body of C, let LOCi be the conjunction LOCi = Bj, A ’ . . A Bjk, A Bi the Bj are all of the literals of C that support Bi, either directly or indirectly, in the same order that they appeared in C. Next, let us introduce for i = where appearing 1 ,..., r a substitution vi = {Y = x: Y is a variable occurring in LOCi but not in A}. 28 W! W! C’ohen/Artijicid lntelltgence 79 (1995) 1-38 The determinate clause C: Below learned on an actual problem clause t more_active( DrugA,DrugB) is a function-free [ 301. version of a depth-2 determinate structure( DrugA,X,Y,Z) A not_equal_toh( X) A polarity (Y,P> A equal_to2( P) A structure( DrugB,T,U,V) A equal_toh( V). % B, % B2 % Bx % B4 % B5 % Bb The support relationships: BI is not directly supported by any literals; B2 is directly supported by BI; B3 is directly supported by BI; B4 is directly supported by B3, and indirectly supported by BI ; B5 is not directly supported by any literals; and B6 is directly supported by Bs. The first phase of the construction: LOCI = structure( DrugA,X,Y,Z) LOCz = structure( DrugA,X,Y,Z) Anottequal_to_h( X) LOC3 = structure(DrugA,X,Y,Z) LOC4 = structure( DrugA,X,Y,Z) Apolarity( Y,P) Aequal_to2( P) LOCs = structure( DrugB,T,U,V) LO& = structure( DrugB,T,U,V) Aequal_toh( V) Apolarity( Y,P) The constructed clause C’: After renaming all free variables appear clause, we obtain the following clause C’. in only a single conjunction the variables in these conjunctions so that them into a single +- ,ZI 1 P more_active( DrugA,DrugB) structure(DrugA,XI,Y~ structure( DrugA,Xz,Yz,Zz structure(DrugA,Xs,YJ,ZI structure(DrugA,X4,Y4,&)Apolarity( structure( DrugB,TS,Us,VS 1 A ) Anotequal_toh( )Apolarity(Y3,Py) and collecting X2) A A Y4,P4)Aequal_to_2(P4) A StIUCtUl-e( DrugB,T6,Us,Vg) kqldtOh(V6). Fig. 3. Constructing a local clause equivalent to a determinate clause. We can then define LOC: = LOCiai; are copies of LOCI,. . . , LOC, variables of LOC:, are different clause the effect of this last step is that LOC’, , in which variables have been renamed from the free variables of LOC:?. Finally, , LOC: . so that the free let C’ be the AtLOCi A A LOC; An example of this construction the example at this point. is given in Fig. 3. We suggest that the reader refer to that C’ is k-local, We claim and furthermore that if C is determinate, remainder of the proof, we will establish for k = udi’, that C’ is at most k times then C’ has the same extension these claims. the size of C, as C. In the W. U? Cohen/Arti&ial Intelligence 79 (1995) 1-38 29 the first two claims it is sufficient To establish C for k = ad+‘) equivalently, the maximum number of literals at depth d or less. Clearly function N(d) every LOCi) is an upper bound on k. (that C’ is k-local and at most k times to show that the number of literals this, let us define N(d) the size of in every LOC; (or to be to a Bi with input variables the is bounded by k. To establish in any LOCi corresponding for any DB E a-DB and C E d-DEP’rHDETERM[DB], The function N(d) is bounded by the following lemma. Lemma 23. For any DB E a-VB, N(d) < c& a’( < ad+‘) Proof. By induction on d. For d = 0, no literals will support Bi, and hence each locality LOCi will contain only the literal Bi, and N( 0) = 1. Now assume depth d. Notice that the lemma holds for d - 1 and consider a literal Bi with inputs at that LOCi can be no larger than the conjunction A ,i:L3j directly supports B; LOCj A Bi. literal Bj there are no more sup ort Bi. Also, any since that directly N(d - 1) < cj=l dpl a’, we see that that directly supports Bi must be at depth d - 1 or less, and than a input variables of Bi, there are at most a different Bj that Putting this together, and using the inductive hypothesis N(d)<aN(d-l)+l<a By induction, the lemma holds. 0 Now we consider the second claim that for any determinate C, the C’ constructed above has the same extension. The first direction of this equivalence any clause C: actually holds for Lemma 24. If a fact extension of C’ with respect to DB. f is in the extension of C with respect to DB, then f is in the Proof. We wish to show that if f E ext( C, DB), ing literals that if f E ext(C,DB), in the body of a clause does not change then f E ext(C’,DB), where then f E ext( C’, DB). Since duplicat- to show it is sufficient its extension, C = ( A+-LOCl A . ’ . A LOC,). Consider variable since the substitutions (+i introduced in LOCi is given a distinct name of C’. Since each in the construction in LOC;, ui is a one-to-one mapping, the free variables in the LOG are distinct, the substitution u = Ub, ai’ free and is well defined. As an example, for the clause C’ from Fig. 3, we would have u = {X, = x, x2 = x, x3 = x, x‘j = x, Y, =xY2=x,Y3=yY4=X z, = z, z, = z, z, = z. Z‘J = z, P3 = p, P4 = P, i-5 = T. T6 = T. lJ~=iJ,iJ~=iJ,y~=~v~=v}. It is easy to see that applying renaming the variables-i.e. that C’u = C. this substitution to C’ will simply “undo” the effect of Now, assume all literals all literals f E ext(C,DB); some substitution in the body of the clause CH are in DB. Clearly for the substitution in the body of the clause C’B’ are in DB, and hence f E ext( C’, DB). then there is by definition 8 so that 8’ = (TO 0 0 We must finally establish the converse of Lemma 24. This direction of the equivalence requires that C be determinate. Lemma 25. If u j&t rmte. theta f is in the extension of C n)ith respe1.t to DB. f is in the extetlsion (?f’C’ with respect to DB, arid C is determi- if C is determinate then C’ is determinate, is most easily proved by picking Proof. If f E ext( C’, DB), then there must be some 0’ that proves this. Let us define a variable Y in C’ to be a “copy” of Y E C if Y is a renaming of Y (i.e., if Y$,’ = Y for the rrl defined above). Certainly and it is easy to show that for a determinate C’. 0’ must map every copy of Y to the same constant ty. (This two copies Y and 5 of Y and then using induction over the depth of Y to show that they must be bound to the same constant. true, since there are no copies For variables of depth d = 0, the statement the literals B, and B., that of depth-O variables. For variables of depth d :> 0, consider to show that contain Y and I’, as output variables, and apply their input variables must have the same bindings; of C, this shows that Y and Yi must be bound together with the determinism the inductive hypothesis to the same constant.) is vacuously Hence, let us define the substitution H = {Y = ty: copies of Y in C’ are bound to rr by 0’) for all i: I < i < r, LOC;H = LOC,H’; hence Clearly, then 0 proves that f E ext( C, DB) El if 0’ proves that f’ t ext(C’,DB) Proof of Theorem 22 (continued). We have now established bounded size, and is equivalent to C. This concludes the proof of the theorem. 0 that C’ is k-local, of We note that the proof technique used in Theorem 22 is similar to that used by Dieroski, Muggleton and Russell in particular, Dieroski, Muggleton be rewritten as a conjunction LOC{ , to the conjunctions [ 161 to show that ij-determinate and Russell showed that a ij-determinate clauses are learnable; clause can each of which corresponds closely of boolean propositions, . LOC:. introduced in the proof above. Finally, although we have shown that ij-determinate by a constant, it should be observed that the constant clauses have locality bounded for example, is fairly large: NW. Cohen/Art@cial Intelligence 79 (1995) l-38 31 for i = j = 3, the bound on locality would be k = 34 = 81. Hence ij-determinate Theorem 21 need not be the best algorithm for learning the algorithm of clauses. 6. Extensions to multiple-clause programs So far, all of our results have been for programs containing now consider extending one clause. This is an important containing multiple would clearly be useful. clauses; the results presented above to programs topic, because many practical further understanding of the limitations a single clause. We will that contain more than learn programs systems of such systems Still considering programs over a fixed database, an immediate non-recursive learning an arbitrary that even with severe restrictions, hard. (We will use, in this proof, the usual semantics logic program for logic programs is cryptographically [ 341.) result is Theorem 26. For a > 1, the language of non-recursive multiple-clause taining clauses from under cryptographic the following assumptions: language families are not polynomially predictable, programs con- l 0-DEPTHDETERM[ a-DB], l 0-FREE[a-Z)B], 0 O-LOCAL [ a-DB] . This is true even if the background database is restricted to contain only a single fact. is a straightforward Proof. The proof again reduce predicting learning a multiple-clause and OR gates. lo The instance mapping a circuit to a program P as follows. a boolean circuit program). We will assume to an ILP adaptation of the proof of Theorem 9; we will this case, learning problem that the circuit contains only AND is as in Theorem 9. The concept mapping maps (in l For every AND gate Gi the program P will contain a clause pi(Xlt.-. ,Xn>+h A L2 where Lit and Liz are defined as follows: true(&), if the jth input to gate Gi is the variable Xk, &j = pk(Xl,...,Xn), { if the jth input to gate Gi is the output of gate Gk. l For every OR gate Gi the program P will contain two clauses .&(X1,... ,&)+h, Pi(XI,-..*Xn)+b2, where Lit and Liz are defined analogously. lo This is possible because of another force all NOT’s to be negations of input variables, and by constructing new variables equivalent , Z we can also eliminate to Yi, reduction; by repeatedly applying LkMorgan’s these NOT gates. an instance mapping laws to a circuit we can that introduces n W W C’oherf/Arr!ficid lntell;~enw 79 (1995) I-38 output AND I gs I I I g3 OR I OR g4 gl AND L t- OR g2 J L circuit (kll ,X2,X&4,X5 ps (Xl ,X2,X3,X4,X5 ) + /3- /)s ( i:,X2,Xi,i4,XS). p?(Xl,X2,X3,X4,XS) A pJ(X1,X2,X3,X4,X5). p4 (X 1 ,X2,X3,X4,X5) pj( X1,X2,X3.X4,X5) 1)~ (X 1 ,X2,X3,X4,X5 p.i( Xl ,X2,X3,X4,X5) /-‘x (Xl ,X2,X3,X4,X5 p2 (Xl ,X2,X3,X4,X5) pr (X 1 ,X2,X3,X4,X5) + f- ) + _p ) + -p r -- pj (XI .X2,X3,X4,X5). pr (X 1 .X2,X3,X4,X5). true(X1). 1’1 (X I ,X2,X3,X4,X5 truc( X4). true( X5) true( X2) A true( X3). ). Fig. 4. Constructing a logtc program equivalent to a circuit. l Finally, P contains a single clause ckGr( XI , . , x,, ) -_I’,, ( x, . , .X,) ) where pn is the gate whose output is the output of the circuit. An example of this construction is shown in Fig. 4. It is easy to verify that this construction clause programs of any of the types named {trUe( I ,}. 0 reduces learning circuits to learning multiple- in the theorem over the database DB = logic programs are hard to lcarn, we will henceforth the heads of all the clauses this restriction restrict ourselves in the program have the same predicate learning [ 6.37,40,46]. We will call such programs multiple-clause predicate &Jinitions. Since arbitrary to cases in which symbol and arity; systems For this case, predicate definition . , Ck}, and the extension of P with respect to a database DB is the union of the extensions of the C, (again with respect to DB). Again, has been made by a number of practical this coincides with the usual semantics is simply a set of clauses P = {Cl,. simple: a multiple-clause of our representation for non-recursive the semantics remains Many of the preceding results extend immediately to multiple-clause nitions; for completeness, we will state these extensions below. programs. predicate defi- W U? CohedArtijicial Intelligence 79 (1995) l-38 33 Observation 27. For a Z 3, the language of multiple-clause predicate definitions con- taining clauses from (log n,) -DEPTHDETERM[ a-DB] are not polynomiallypredictable, under cryptographic assumptions. This follows directly from the non-predictability of single clauses. Observation 28. For any a b 2 and any k 2 1, the language of multiple-clause predicate dejnitions containing clauses from k-FREE[ a-DB] is uniformly predictable if and only if DNF is predictable. This follows directly of a set of DNF formula is still in DNF. from Theorems 11 and 14, and from the fact that the disjunction Observation 29. Let k-INDE’IERh@IS be any clause language with polynomial literal support containing only effectively k-indeterminate clauses. Then for any Ned a, k and 1, the language of multiple-clause predicate definitions containing at most 1 clauses from k-INDETERMPIS [ a-‘DB] is uniformly predictable. This follows directly from Theorem 17 and the fact that the union of 1 distinct k-term DNF formulas, where 1 is constant, is a (k . l)-term DNF formula. extensions of Theorems clauses and k-local clauses respectively. Again, It remains to consider of ij-determinate reduction an appropriate based on previous to boolean monomials: set of boolean learnability sions are straightforward, an invertible by constructing over those features, and then converting While Theorem 21 was proved directly, reduction monomial, as hard to learn as general DNF, one can easily obtain and it is known to monomials. ‘t Since show 6 and 21, which the pac- these exten- results. The proof of Theorem 6 is based on clause can be learned learning a monotone monomial clause. this monomial back to a ij-determinate it could have also been proved via an invertible to a monotone any ij-determinate features, that in a distribution-independent setting, monotone DNF is in both cases a single clause reduces this result: Observation 30. For any fixed i and j, the language of multiple-clause predicate def- iff DNF is initions containing clauses from i-DEFIMDETl3thJ[ j-Z)B] pat-learnable. is pat-learnable For anyfied k and a, the language of multiple-clause predicate definitions containing clauses from k-LOCAL[ a-VB] is pat-learnable iff DNF is pat-learnable. Some positive results are also obtainable. l-term DNF is learnable Russell have observed simple distribution against simple distributions that the learning algorithm can be used to learn an l-clause It is known that for any constant 1, monotone and l-term DNF against a predicate definition [ 331. I2 Dieroski, Muggleton for monotone ij-determinate one could construct p variables ~1,. I1 Specifically, which each ui is true iff e E exr( A+LOCi, DB). I* Simple distributions are a broad class of probability distributions that include all computable distributions. , up. and map an example e to an assignment Q in 34 W W. CdwdArtijiciul Intrlli~ence 79 (I 995) I-38 by first, constructing DNF over those features, and finally converting predicate definition. Thus: the appropriate set of boolean features, then learning an l-term this formula back to a ij-determinate Theorem 31 (Dieroski, Muggleton and Russell [ 161). For any$xed i und j, the /an- g14uge of multiple-clause predicate definitions containing at most 1 clauses from i-DEPTHDETERM[ j-DB] is un+fi,rmly pa-learnable against .simple distributions. The same proof technique can be applied to predicate definitions containing ~-LOCAL clauses; thus we have the following corollary of Theorems 21 and 3 1. Observation 32. For uny fixed k und u, the language of multiple-clause predicate dej- nitions containing at most 1 clauses from k-LOCAL[ u-DB] against simple distributions. is uniformly put-learnable One problem with applying these results in practice is that the proofs of learnability simple distributions must sample against a certain “universal distribution”, which Implemented systems have thus used heuristic methods are not completely constructive: in particular, is not computable to learn multiple clauses. for the learning algorithm [ 331. 7. Concluding remarks Most implemented first-order learning systems use restricted are mathematically well understood; resent concepts. An obvious advantage of this representation complexity this suggests such logics can also be mathematically theoretical foundations of this subfield, tigating model of polynomial predictability analyzing encourages power. the learnability the learnability of restricted analyzed. This paper has sought inductive logic programming, logic programs. Most of our analysis introduced by Pitt and Warmuth of a language by characterizing logic programs is that its semantics to rep- and that learning systems using the inves- the is using [ 441. This model its expressive by formally to expand In this paper we have characterized several extensions of the language of determinate clauses of constant depth [ 16,371. These results will now be summarized. log-depth First, via a reduction circuits, we showed that a single log-depth determinate clause from is not pat-learnable. Next, we relaxed indeterminate the condition of determinacy, clauses of constant depth can be shown and obtained Since we considered free variables of indeterminacy clause several restrictions of this language. We showed is as hard to learn as DNF. We also showed that restricting language with “polynomial of a clause leads to predictability literal support”. (but not pat-learnability) to be hard a number of results. to predict, that a clause with k the degree for any W W Cohen/Artificial Intelligence 79 (1995) 1-38 35 n-DEPTHDETERM k-DEPTH log n-DEPTHDETERM k-DEPTHDETERM S-OUTPUT Fig. 5. Summary of results for single clauses. Above the heavy line are languages below the heavy line are languages also pat-learnable. Predicting the boxed that are predictable. All predictable that are hard to predict, and languages except k-INDETERMPU are language k-FREE is equivalent to predicting DNF, an open problem. Finally, we showed that restricting is especially learnability. This result to be a strict generalization and j, every ij-determinate than j’+‘. the locality of a clause to a constant k leads to pac- interesting because k-local clauses can be shown of ij-determinate sense: for fixed i clause can be rewritten as a clause with locality no greater in the following clauses indeterminate [ 16,271; note however These results are summarized that the previous clauses make in Fig. 5, which shows the languages considered in this paper, partially ordered by their expressive power. l3 Some results are from previous work determinate and constant-depth have relaxed. The analysis languages below the heavy polynomially cryptographic shown boxed question that we is original. The or (in the case of k-INDETEFUv@LS) above the heavy line are hard to predict, under as are all supersets of these languages. The language k-free, this is an open line, learning these results, several previous predictable. The languages assumptions, in a dotted associated with line are pat-learnable results representational from the literature have been ex- is predictable theory. for arbitrary-depth in computational is predictable; the remaining assumptions languages iff DNF results [23] raises In obtaining tended. Haussler concepts with k variables is easy to show that every existential indeterminate clause; in general as hard to predict as DNF. thus an immediate the question of the learnability of existential conjunctive in a representation-independent conjunctive (i.e., predictability) setting. It concept can be expressed by a single result of Theorem 17 is that these concepts are More recently, Kietz [27] has shown and that constant-depth hard to pat-learn, learn. These results have both been strengthened particular, we have presented representation-independent that arbitrary-depth indeterminate determinate clauses are clauses are also hard to pac- in in a number of ways in this paper: hardness results for determinate I3 Strictly speaking, k-INDETERMPLS is a set of languages, not a single in the Set need not be of depth k. However, every k-INDETERMPU language that we have considered is of bounded language, and languages depth. related is to find, in .16 W.W. Cohen/Artijicial Intelligence 79 (1995) 1-38 clauses of only log depth, rather than arbitrary depth, and also for indeterminate depth clauses. constant- We have also further investigated nate clauses. One result of this in-depth subclass of indeterminate and that is a strict generalization clauses the learnability of various subclasses of indetermi- investigation has been isolation of an interesting that is pat-learnable, (the class of k-local clauses) of the class of ij-determinate clauses. Kietz and Dieroski [28] have also investigated the complexity of a closely from LANGE, a hypothesis for (t, VB, LANGE, LANGE) theory DB E DB and a set of examples results and the ILP problem it is possible task called the ZLP problem. The “ILP problem” given a background LANGE that is “consistent” with the examples with respect to the provability k. One corollary of Theorem 21 is that the ILP problem for k-local clauses The connection between our negative complex. As formalized by Kietz and Dieroski, polynomial time using an algorithm the number of examples-for expressive, one might hypothesize Thus it is possible even and Theorem 4, if a language LANG is not polynomially solves or (ii) of examples. corresponding hypothesis. relationship is tractable. is somewhat more in that grows quickly with language LANGE is sufficiently all the positive examples. for a language might be solvable, is hard to predict. However, by the results of Blumer et al. [4] and algorithm A time, in the number I4 Thus that the imply ILP problems cannot be tractably solved in any way that yields a concise predictable (i) A does not run in polynomial the ILP problem the size of the hypotheses linearly results of this paper returned by A grows nearly that generates a hypothesis the negative predictability to solve the ILP problem for LANG, then either that the ILP problem if the hypothesis table containing if the language in principle a lookup example, in this area appear in [ 10,13,14,19]. suggest themselves. The learnability of recursive logic A number of further questions is a challenging problem: some results is difficult, continued [ 18,22,3 programs The learnability of multiple-clause analysis DNF is encouraging programs, or genera1 logic programs the settings considered by Angluin, Frazier and Pitt [2]) much work remains to be done in relating to each other, as well as to other learnable predicate definitions progress on the learnability I 1. The learnability the learnable first-order is largely an open issue; although of fairly general classes of of restricted classes of genera1 logic to is also an open area. Finally, languages of first-order clauses (perhaps analogous languages (e.g., [ 15,411) . in more restricted settings Acknowledgments The author would like to thank Haym Hirsh and Rob Schapire presentation; Mike Kearns, J&g-Uwe Kietz, and Rob Schapire discussions; technical content. and the reviewers, for their many helpful comments on the presentation for comments on the for a number of helpful and I4 More precisely, the hypothesis size for a sample of m examples is not always less than ma for any a < 1. W W Cohen/Artificial Intelligence 79 (1995) I-38 31 References [ 11 A.V. Aho, J.E. Hopcroft and J.D. Ullman, The Design and Analysis of Computer Algorithms (Addison- Wesley, Reading, MA, 1974). 121 D. Angluin, M. Frazier and L. Pitt, Learning conjunctions of horn clauses, Mach. Learn. 9 (2/3) (1992). 131 A. Blum and M. Singh, Learning functions of k terms, Computational Learning Theory, Rochester, NY (Morgan Kaufmann, Los Altos, CA, 1990). in: Proceedings Third Annual Workshop on I41 A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth, Classifying learnable concepts with the Vapnik-Chervonenkis dimension, J. ACM 36 (1989) 929-965. IS] R. Boppana and M. Sipser, The complexity of finite functions, in: J. van Leeuwen, ed., Handbook of Theoretical Computer Science (North-Holland, Amsterdam, 1990) 758-804. I61 W.W. Cohen, Grammatically biased language, Artif Intell. 68 ( 1994) 303-366. learning: learning description logic programs using an explicit antecedent I71 W.W. Cohen, Compiling knowledge in: Proceedings of the Ninth International Conference on Machine Learning, Aberdeen into an explicit bias, ( 1992). I81 W.W. Cohen, Cryptographic limitations on learning one-clause logic programs, in: Proceedings Tenth National Conference on Artijcial Intelligence, Washington, DC ( 1993). 191 W.W. Cohen, Learnability of restricted logic programs, in: Proceedings Third International Workshop on Inductive Logic Programming, Bled ( 1993). I IO] W.W. Cohen, A pat-learning algorithm for a restricted class of recursive logic programs, in: Proceedings Tenth National Conference on Artacial Intelligence, Washington, DC (1993). [ 111 W.W. Cohen, Rapid prototyping of ILP systems using explicit bias, in: Proceedings 1993 IJCAI Workshop ( 1993). on Inductive Logic Programming, Chambery [ 121 W.W. Cohen, Pat-learning nondeterminate Artificial Intelligence, Seattle, WA ( 1994). clauses, in: Proceedings Eleventh National Conference on I 131 W.W. Cohen, Pat-leaming I 141 W.W. Cohen, Pat-learning [ 151 W.W. Cohen and H. Hirsh, Learnability recursive recursive logic programs: efficient algorithms, logic programs: negative results, J. Artif: Intell. Res. 2, 541-573. J. Artif: Intell. Res. 2, 500-539. of description logics, in: Proceedings Fourth Annual Workshop on Computational Learning Theory, Pittsburgh, PA (ACM, New York, 1992). [ 161 S. Dieroski, S. Muggleton and S. Russell, Pat-learnability of determinate logic programs, in: Proceedings [ 171 C. Feng, 1992 Workshop on Computational Learning Theory, Pittsburgh, PA (1992). fault diagnosic Press, New York, 1992). Programming (Academic temporal Inducing rules from a qualitative model, in: Inductive Logic I 181 M. Flammini, A. Marchetti-Spaccamela distributions, probability Pittsburgh, PA (ACM, New York, 1992). under classes of in: Proceedings Fourth Annual Workshop on Computational Learning Theory, and L. K&era, Learning DNF formulae I 191 M. Frazier and techniques, (1993). and C.D. Page, Learnability results in: Proceedings Third International Workshop on Inductive Logic Programming, Bled theories: Some basic non-determinate of recursive, [ 201 M. Frazier and L. Pitt, Learning from entailment: An application to propositional horn sentences, in: Proceedings Tenth International Conference on Machine Learning, Amherst, MA (Morgan Kaufmann, Los Altos, CA, 1993). 12 I] A. Frisch and C.D. Page, Learning constrained atoms, in: Proceedings Eighth International Workshop on Machine Learning, Ithaca, NY (Morgan Kaufmann, Los Altos, CA, 1991). I 22 ] T. Hancock, Learning kp decision trees on the uniform distribution, in: Proceedings Sixth Annual ACM Conference on Computational Learning Theory, Santa Cruz, CA (ACM, New York, 1993). [ 231 D. Haussler, Learning conjunctive [ 241 M. Kearns, M. Li, L. Pitt and L. Valiant, On the learnability of boolean concepts in structural domains, Mach. Learn. 4 (1) (1989). formulae, in: Proceedings 19th ACM Symposium on Theory of Computing, New York ( 1987). [ 251 M. Kearns and L. Valiant, Cryptographic limitations on learning Boolean formulae and finite automata, in: Proceedings 21th ACM Symposium on Theory of Computing, New York ( 1989). 126 / M. Kharitonov. Cryptographic lower bounds on the learnability of boolean in: Proceedings Fourth Annuul Workshop on Compufutionul functions on the uniform Learning Theory, Pittsburgh, distribution, PA (ACM, New York, 1992) 127 1 J.-U. Kietz, Some computational programming, in: Proceedings lower bounds of inductive 1993 Europeun Conference on Machine Leurning, Vienna ( 1993). for the computational complexity logic of inductive binding j 28 1 J.-U. Kietz and Dieroski. 129 I J.-U. Kietz and K. Morik, Constructive Inductive logic programming induction of background and learnability, SIGART Bull. 5 ( 1994) 22-31. in: Proceedings Workshop knowledge, OII Evuluating und Chunging Representution in Muc,hine Leurning (ut IJCAI-91). Sydney ( I991 ), I30 1 R.D. King, S. Muggleton, R.A. Lewis and M.J.E. Stemberg, Drug design by machine learning: the use logic programming to model the structure-activity relationships of trimethoprim analogues to dihydrofolate reductase. Proc. Nut. Acud. Sci. 89 ( 1992). 1 3 I 1 E. Kushilevitz and D. Roth, On learning visual concepts and DNF formulae, in: Proceedings Sixth Annuul :tCM Conference OH Cornpututionul I 32 I N. Lavra? and S. Dieroski. Background fpurning Theor!, Santa Cruz. CA (ACM, New York, 1993). knowledge and declarative bias in inductive concept learning, Internutionul Workshop A11’92, Lecture Notes in: K.P. Jantke, ed., Anulo#cul in Artificial Intelligence 642 (Springer, Berlin, 1992). ctrrd Inductive Inferem?: [ 33 I M. Li and P. Vitanyi, Learning simple concepts under simple distributions, SIAM J. Comput. 20 (5) (1991). [ 31 1 J.W. Lloyd, Fimndut~ons of Lr~g~ Pro,qrunrminji 13.5 1 B.D.S. Muggleton. The application of inductive Ir~ductive Logic, Progrutnmin~ (Springer, Berlin, 2nd ed., 1987). logic programming (Academic Press. New York, 1992). to finite-element mesh design. in: in: Inductive Logic Programming (Academic Press, New 1371 Inductive logic programming, I36 ) S. Muggleton, York. 1992). S. Muggleton and C. Feng, Eticient ( Academic Press, New York, 1992). S. Muggleton. R.D. King and M.J.E. Sternberg, Protein secondary machine S.H. Muggleton, ed., Inductive Logic, Progrummin~ M. Pazzani and D. Kibler, The utility of knowledge L. Pitt and M. Frazier, Classic learning, Protein En,qr,q. 5 ( 1992) 647-657 1391 1401 I41 I 1381 ( Academic Press, New York, 1992). in inductive learning, Much. Leurn. 9 ( I ) ( 1992). on Sevenfh Anmud ACM Conference in: Proceedings Theory, New Brunswick. NJ (ACM, New York, 1994). learning. limitations on learning from examples, J. ACM 35 ( 1988) 965-984. among prediction problems: On the difficulty of predicting in Complexity Theory, Washington, /EEE Conjtirence on Structure burning in: Proceedin,ys 3rd Annual Cornpututionul L. Pitt and L. Valiant, Computational L. Pitt and M.K. Warmuth, Reductions automata, DC (IEEE Computer Society Press. Silver Spring, MD, 1988). L. Pitt and M. Warmuth, Prediction-preserving G.D. Plotkin, A note on inductive generalization. Much. J.R. Quinlan, Learning J.R. Quinlan, Determinate logical definitions literals from relations. Much. burn. logic programming, reducibility, J. Compu/. Syst. Sci. 41 ( 1990) 430-467. Intell. 5 (1969) 153-163. 5 (3) ( 1990). in inductive in: Proceedings Eighth Ithaca. NY (Morgan Kaufmann, Los Altos, CA, 1991). lnternutionul Workshop on Muchine Leurning, R.E. Schapire, The strength of weak learnability, Much. Leurn. 5 (2) E. Shapiro, Algorithmic L.C. Valiant, A theory of the learnable, Commurl. ACM 27 (11) M. Vilain, P. Koton and M. Chase, On analytical Progrum Debugging ( 1984). (MIT Press, Cambridge, MA, 1982). ( 1990). Eighth Nutionul Conference on Art$chl Intellipncr, and similarity-based in: Proceedings classification, Boston. MA (MIT Press, Cambridge, MA, 1990). WI I-23 I I441 1451 I461 1471 1481 1491 ISOl 151 I induction of logic programs, in: Inductive Logic Prugrurnrning structure prediction using logic-based 