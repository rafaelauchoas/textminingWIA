Artificial Intelligence 204 (2013) 30–55Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintType Extension Trees for feature construction and learningin relational domainsManfred Jaeger a,∗, Marco Lippi b, Andrea Passerini c, Paolo Frasconi da Institut for Datalogi, Aalborg Universitet, Denmarkb Dipartimento di Ingegneria dell’Informazione e Scienze Matematiche, Università degli Studi di Siena, Italyc Dipartimento di Ingegneria e Scienza dell’Informazione, Università degli Studi di Trento, Italyd Dipartimento di Ingegneria dell’Informazione, Università degli Studi di Firenze, Italya r t i c l ei n f oa b s t r a c tArticle history:Received 23 October 2012Received in revised form 31 July 2013Accepted 8 August 2013Available online 20 August 2013Keywords:Statistical relational learningInductive logic programmingFeature discovery1. IntroductionType Extension Trees are a powerful representation language for “count-of-count” featurescharacterizing the combinatorial structure of neighborhoods of entities in relationaldomains. In this paper we present a learning algorithm for Type Extension Trees (TET)that discovers informative count-of-count features in the supervised learning setting.Experiments on bibliographic data show that TET-learning is able to discover the count-of-count feature underlying the definition of the h-index, and the inverse document frequencyfeature commonly used in information retrieval. We also introduce a metric on TET featurevalues. This metric is defined as a recursive application of the Wasserstein–Kantorovichmetric. Experiments with a k-NN classifier show that exploiting the recursive count-of-count statistics encoded in TET values improves classification accuracy over alternativemethods based on simple count statistics.© 2013 Elsevier B.V. All rights reserved.Probabilistic logical (or relational) models provide models for properties and relationships of entities in domains witha relational structure, such as graphs, networks, or, generally, any kind of structure found in a relational database. Theprevalence of this type of structured data, and the challenges posed by it for traditional machine learning methods basedon simple attribute-value data models has led to an increasing interest over the past 10 years in probabilistic logical models,and associated statistical-relational learning techniques [10,6].When modeling entities embedded in a relational domain a key question is what features of the entities are relevantto model and predict properties of interest. Apart from using attributes of the given entities themselves, one has in rela-tional learning the ability to construct new features by considering the relational neighborhood of an entity. Taking intoconsideration related entities and their attributes, one obtains a basically unlimited supply of potential features.A word on terminology here may be in order: by an attribute we mean a formal representation in a dataset of a propertyof individual entities by a data column. The color property of a flower, for example, could be formalized by attributessuch as color ∈ {red, green, blue, orange, . . .}, or three distinct attributes RGB_red, RGB_green, RGB_blue ∈ {0, . . . , 255}. Thevalue space of an attribute will typically be a simple data type like Boolean, enumeration, or numeric. In classic attribute-value data, feature is often a synonym for attribute. By contrast, we use feature to denote formalized properties in a muchbroader sense. First, a feature may only be implicit in the data as a function of explicit data. For example, brightnessas a function of RGB_red, RGB_green, and RGB_blue is a feature for an attribute-value dataset containing the attributes* Corresponding author.E-mail addresses: jaeger@cs.aau.dk (M. Jaeger), lippi@diism.unisi.it (M. Lippi), passerini@disi.unitn.it (A. Passerini), paolo.frasconi@unifi.it (P. Frasconi).0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2013.08.002M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5531Fig. 1. Bibliographic data fragment and count-of-count feature.RGB_red, RGB_green, RGB_blue; the “number of friends older than 22” is a feature (of a person entity) in a relational social-network dataset. Second, for relational data, a feature can also represent a property relating multiple entities. Thus, “titleshaving more than 3 words in common” would be a feature for a pair of paper entities in a bibliographic database. Third,unlike other common frameworks in statistical learning (like for example kernel methods), in this paper we are not in-terested in simple numerical features but will focus on features whose values are complex combinatorial data structuresrepresenting what we loosely will call counts-of-counts. In all cases, however, we require that a feature has a well-definedformal specification and value space. The language available for the formal specifications defines the feature space.Relational learning frameworks differ widely to what extent they are linked to a clearly defined feature space, and towhat extent feature selection or feature construction is integrated into the model learning process. On the one hand, thereare techniques that only require the availability of features of a simple data type. The features construction is not part of thelearning framework, and usually requires an application-dependent data pre-processing [42]. Propositionalization approachesalso maintain a strict separation between feature construction and learning, but specific frameworks and representationlanguages for feature specification are a crucial ingredient [22].On the other extreme there are approaches in which feature construction and model learning are tightly integrated,and, in fact, the learned model essentially consists of a list of features represented in a formal specification language. Tothis category belong most frameworks that are based on predicate logic as the feature representation language [3,20,21]. Inbetween, there are approaches where feature construction is an integral part of the learning process, but the exact featurespace accessible to the learner is less clearly delimited [1,18].A key component in the design of relational features is given by the tools that are available for constructing features fromproperties of entities that are related to the entity of interest by chains of one or several relations. Since the number ofentities that are reached by such “slotchains” [8] varies from instance to instance, this feature construction usually involvesa form of combination or aggregation of the properties of multiple related entities.In non-probabilistic inductive logic programming approaches, such an aggregation is usually based purely on existentialquantification, i.e., a feature only determines whether or not a related entity with certain attributes exists. So, for example,for an “author” entity in a bibliographic database one could define a Boolean feature saying whether there exists a paperciting a paper of this author. A number of frameworks that are more closely linked to relational databases [31,13] constructfeatures based on aggregation operators. Here, it would be possible, for example, to construct a feature that represents theaverage count of citations that papers of an author have received, or a feature that represents the average price of itemspurchased by a customer. Recently approaches that define probability distributions over entire structures based on purecount features have become quite popular [36,43]. Here the probability of a relational structure (over a given domain ofentities) is determined by the count of entity tuples that satisfy some relational constraints, typically expressed as a logicalclause.All these approaches are based on features that only represent relatively simple summary statistics about quantitativeproperties of an entity’s relational neighborhood. However, for many prediction tasks, a more detailed picture of combi-natorial count-of-count features may be relevant. Consider the tiny bibliographic dataset shown in Fig. 1, for instance. Itrepresents 5 different authors, 10 different papers by these authors, and citation links between the papers. Simple summaryfeatures for an author a could be the number of a’s papers, or his/her total or average citation count. However, a currentlyimportant attribute for an author is the h-index [14]. To predict the h-index (or another attribute closely linked to theh-index – like receiving a professional award, or a large research grant) one may need to consider the more detailed featureof the count of papers with given counts of citations. In Fig. 1, the values for the 5 authors of this count-of-count featureare shown on the right (an expression k : l meaning that there are l papers with k citations).As another example for count of count features consider the Internet Movie Database (IMDB), a quite popular objectof investigation in relational machine learning. Here one may be interested in predicting some attribute of a movie, e.g.,whether it will be a box-office success (e.g. [38,31]). For this prediction one may consider the cast of the movie, for examplein terms of its size, the count of actors in the cast who have previously received an award nomination, the total number ofaward nominations shared by the actors, etc. Again, a more detailed count-of-count feature can be more informative thanonly flat counts: it will make a difference, perhaps, whether there is a single actor in the cast with many award nominations(perhaps a single box office draw actor, but maybe beyond the peak of his/her career?), or whether there are many actorswith one or two nominations each (perhaps a young all-star cast?).In information retrieval, relevance measures for a document d given a query q are often based on counting terms ap-pearing both in d and q. These counts will usually be weighted by a term weight such as inverse document frequency, whichis given by the number of documents in the collection containing the term. Thus, the relevance measure is computed32M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55from a count-of-count feature. Similarly to Fig. 1, for example, a query-document pair (d, q) could have a feature value[3 : 1, 10 : 2, 7 : 1] expressing the fact that d and q have 1 term in common that appears in a total of 3 documents, 2 termsin common that each appear in 10 documents, and 1 term in common that appears in 7 documents.Finally, consider the relational domain consisting of Web-pages and the links-to relation. An important attribute of aweb-page is its pagerank [5], and we may want to estimate the pagerank of a web-page based on information of its localrelational neighborhood. Unlike the h-index in the bibliographic example, which is precisely determined by a relational(cid:4), P ), the pagerank1 is fully determined only by theneighborhood of radius 2 defined by the chain authorOf ( A, P ), cites(Pstructure of the whole relational domain. However a useful approximation might be obtained already from local information.Clearly relevant for the pagerank of page P is the number of its incoming links. Also important is the pagerank of the pages(cid:4)linking to P , and hence the number of their incoming links. Furthermore, it is important to know for the pages PPlinking to P the number of outgoing links of P(pointing to pages other than P ), because this determines how much ofis “inherited” by P . Again, the full relevant information is only given by a comprehensive count-of-countthe pagerank of Pfeature.(cid:4)(cid:4)(cid:4)The purpose of this paper is to develop a framework for the representation of rich combinatorial count-of-count featuresin the context of relational learning. The methodological contribution of the paper has three main components:(C1) The definition of Type-Extension Trees (TETs) [7] as a formal representation language for count-of-count features.(C2) A method for learning Type-Extension Trees in conjunction with a simple predictive model based on TET-features.This gives us a method for relational feature discovery, as well as a baseline supervised learning framework. In [7]TET learning employed the classic notion of information gain, while in this paper we take advantage of the recentlyintroduced relational information gain [26].(C3) The definition of a novel metric on TET feature values, which enables distance-based learning techniques that makeuse of count-of-count features in a substantial manner.To illustrate the relationship and significance of these three components, consider an analogy with learning from stan-dard numerical attribute-value data, where each instance is fully characterized by a tuple of numeric attributes, and aclass variable. Fig. 2 on the left shows a small dataset with numeric attributes A1, A2, N1, . . . , N4 and a binary class labelwith values p(ositive) and n(egative). In this dataset the class is only correlated with the attributes A1 and A2, whereasN1, . . . , N4 are random noise. The plot in the left part of the figure represents the values of A1, A2 and C . The relevantfeature subset for predicting the class label then is { A1, A2}, out of the space of all possible feature subsets (Fig. 2A). Theset { A1, A2} may also be called a sufficient or model-independent feature for predicting C . No concrete type of machinelearning model will use all the information represented by this feature. A decision tree model, for instance, will only usefinitely many Boolean features defined by lower- or upper-bounds on A1, A2-values. A linear classifier will only use a linearfunction of A1, A2. We may call the spaces of such featuresreduced or model-specific feature spaces (Fig. 2B). A proper dis-tinction between the sufficient and reduced feature spaces is important when we interpret the result of learning a specificmodel also in terms of feature discovery: the decision tree learned from our example dataset (Fig. 2C left) uses four Booleanfeatures A1 > 0.79, . . . , A2 > 0.75, and, strictly speaking, has “discovered” exactly these four features. However, one willtypically want to generalize, take { A1, A2} as the discovered feature in the sufficient feature space, and assume that suitablereductions of { A1, A2} in other model-specific feature spaces will also lead to good performance of other types of models.Fitting a logistic regression model to our data (Fig. 2C right) directly leads only to the construction of the linear function12.8 − 22.1 A1 · · · + 3.4N4 as a predictive feature. Again, one can abstract from this reduced feature, and try to identify the“discovered” model-independent feature. Here this abstraction is not as clear-cut as in the decision tree case. Consideringthe attributes whose coefficients in the linear function have the highest absolute values for inclusion in the feature subset,one here might take any of the subsets { A1}, { A1, A2}, { A1, A2, N3, N4} as the discovered model-independent feature.Our objective (C1) aims at defining for relational data a rich, model-independent feature space that corresponds tothe space of attribute subsets (Fig. 2A), and that includes complex count-of-count features. This definition is developed inSection 2 via syntax and semantics of Type Extension Trees (see Section 2.1). The construction of a sufficient feature spacefor relational data not only faces the challenge of the basically unlimited supply of possible features, but also the challengeof the diversity of relational learning tasks: attribute prediction for individual entities, link prediction, and classification ofwhole relational structures (e.g. molecules) require the specification not only of features for single entities, but also for pairsof entities, tuples of entities, and global features of a whole relational dataset. TET features provide a uniform and coherentframework for all these cases.Component (C2) corresponds to the feature discovery process via the learning of a lightweight model as illustrated inFig. 2. This is achieved by first defining directly on TET-feature values a discriminant function that turns a TET-feature intoa predictive model (Section 3). Based on this discriminant function a TET structure learning algorithm is developed that,strictly speaking, will discover a feature (T , f ) (T a TET, f a discriminant function defined on T ), but from which in a trivialabstraction step we can extract the model-independent TET feature T (Section 5).1 In fact pagerank can be applied beyond Web searching, e.g. in bibliometrics [2] or collaborative filtering [27].M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5533Fig. 2. Analogy: numerical data. A: Sufficient feature space, B: Model-specific feature spaces, C: “lightweight” learned models.Given a TET – either learned or manually constructed based on expert knowledge – we finally in component (C3) definea predictive model based on TET features that makes use of the TET’s count-of-count values in a more substantial andsophisticated manner than the discriminant function. For this we define a metric on TET values that then enables nearestneighbor classification (Section 4).2. Feature representation with Type Extension Trees2.1. TET syntax and semanticsIn this section, we review the basic syntax and semantics definitions of Type Extension Trees [17]. To simplify thedefinitions, we will assume that all attributes and relations are Boolean, which means that a multi-valued attribute like’color’ that would give rise to atomic propositions such as color(hat, red) is assumed to be encoded using Boolean attributeslike color_red(hat). Relational data can then be viewed as a model in the sense of the following definition.Definition 2.1. Let R be a relational signature, i.e. a set of relation symbols of different arities. A (finite) model for R,M = (M, I) consists of a finite domain M, and an interpretation function I : r(a) → {t, f } defined for all ground atoms r(a)constructible from relations r ∈ R and arguments a ∈ Marity(r).Throughout the paper we use f and t as shorthands for false and true. Furthermore, we denote objects (entities) froma domain with lowercase letters, and logical variables with uppercase letters. Only first-order formulas are acceptable,i.e. variables always stand for objects. Bold symbols always denote tuples of the corresponding non-bold symbols, e.g.,in the foregoing definition: a = (a1, . . . , aarity(r)). In logic programming terminology, M is a Herbrand interpretation forthe signature consisting of R and constant symbols for the elements of M. For convenience we may assume that thedomain M is partitioned into objects of different types, that arguments of relations are typed, and that I is only defined forground atoms with arguments of appropriate types. For the sake of simplicity we do not introduce any special notation forspecifying types. Rather, in our examples below we will typically use generic capital letters ( X , Y , Z , U , V , W ) to indicatevariables which may range over all domain objects, and specific letters (like A for author or P for paper) to implicitly meanthat the corresponding variables are restricted to a subset of objects. With a slight abuse of notation, if τ (a) is a complexground sentence, we denote by I(τ (a)) its truth value under interpretation I .Definition 2.2. An R-literal is a (negated) atom r(V ) (r ∈ R ∪ {=}, V a tuple of variable symbols). We also allow the specialliteral (cid:7)(V ), which always evaluates to t. An R-type is a conjunction of R-literals.A type extension tree (TET) over R is a tree whose nodes are labeled with R-types, and whose edges are labeled with(possibly empty) sets of variables.In the following we will usually omit the reference to the underlying signature R, and talk about literals and types,rather than R-literals and R-types.Note that according to Definition 2.2 a literal cannot contain any constant symbols as arguments. Since R is assumed toonly contain relation and no constant symbols, this is consistent with the usual definition of an R-literal. The term “type”for a conjunction of literals is motivated by two distinct (yet compatible), existing uses of “type”: on the one hand, the typeof an entity is commonly understood as a property expressed by a single unary predicate, e.g., movie(V ) or person(V ). Ourdefinition generalizes this to types of tuples of objects, and to types which are expressed via a conjunction of literals. On the34M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55other hand, “type” is used in mathematical model theory for consistent sets of formulas using free variables V 1, . . . , V n thatdescribe properties of n-tuples of domain elements [15]. Our definition is a special case of type in this sense by limiting itto a single, quantifier-free conjunction of literals.Example 2.3. The following is a TET for a signature containing relations author, authorOf and cites:author( A)P 1−→ authorOf ( A, P 1)P 2−→ cites(P 2, P 1).(1)According to the semantics given below, this TET represents a feature that is sufficient for computing the h-index of anauthor, for example.Example 2.4. A TET that will be sufficient for representing relevance features based on inverse-document frequency weightsis(cid:7)(D, Q )Tterm_in_document(T , D), term_in_query(T , Q )(cid:4)D(cid:2)term_in_documentT , D(cid:3)(cid:4)(2)Labeled edges in a TET are related to quantifiers in predicate logic: like a quantifier, a labeled edge binds all occurrencesof the variables associated with the edge in the subtree rooted at this edge. The free variables of a TET are all variables notbound by an edge label. We call a TET propositional if all edge labels are empty. The TET in (1) has the single free variable A,the one in (2) the two free variables D and Q . In both cases, the root node essentially serves to introduce the free variables,and, in case of (1), to explicitly establish that the variable ranges over entities of type author. If such type constraints onvariables are assumed to be implicit in the variable names, then the root will usually be a vacuous (cid:7)() atom, as in (2).We write T (V ) to denote a TET whose free variables are among the variables V (but does not necessarily contain all ofthem). We write(cid:4)τ (V ),T (V ) =(cid:2)(cid:3)W 1, T 1(V , W 1)(cid:2)(cid:3)(cid:5), . . . ,W m, Tm(V , W m)(3)to denote a TET with a root labeled with τ (V ), and m sub-trees T 1(V , W i, ) reached by edges labeled with variables W i(possibly empty).A TET T (V ) with free variables V = V 1, . . . , V k will define a feature for k-tuples of domain entities: for any model M,and any a ∈ Mk the TET defines a feature value V (T (a)). Fig. 1 on the right shows (in a somewhat simplified form) thevalues V (T (a1)), . . . , V (T (a5)) for the TET T ( A) in (1). We give the general definition of TET semantics in two steps: firstwe define the value space of nested counts associated with a given TET T (V ), and then the actual mapping a (cid:8)→ V (T (a)).Definition 2.5. For any set A we denote with multisets( A) the set of all multisets over A. We denote with {a1 : k1, . . . , an : kn}a multiset that contains ki copies of ai . The value space V(T ) of a TET T is inductively defined as follows:Base: If T = [τ ] consists of a single node, then V(T ) = {t, f }.Induction: If T = [τ , (W 1, T 1), . . . , (W m, Tm)], thenm×V(T ) = { f } ∪ {t} ×(cid:2)(cid:3)V(T i)multisets.i=1We note that according to this definition the structure of V(T ) only depends on the tree structure of T , but not on thelabeling of the edges of T , or the types at the nodes of T .Example 2.6. (1) is a graphical representation of a TET that following (3) can be written asT ( A) =(cid:4)author( A),(cid:2)(cid:4)(cid:2)(cid:4)(cid:5)(cid:3)(cid:5)(cid:3)(cid:5)P 1,cites(P 2, P 1)(4)The recursive definition of V(T ) is grounded in V([cites(P 2, P 1)]) = {t, f }. In other words, the single node TET [cites(P 2, P 1)](cid:4)( A, P 1)),represents a Boolean feature for pairs of papers. The inductive construction proceeds with the definition of V(TwhereauthorOf ( A, P 1),P 2,.(cid:4)T( A, P 1) =(cid:4)authorOf ( A, P 1),(cid:2)(cid:4)(cid:5)(cid:3)(cid:5)P 2,cites(P 2, P 1)(5)represents a feature of an author-paper pair A, P 1. This value space is constructed according to the inductive case of Def-inition 2.5 as the union of { f } and pairs of the form (t, A), where A is a multiset of t, f values. Thus, examples are: fM. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5535(according to Definition 2.7 below, this is the feature value of an author-paper pair (a, p), where a is not the author of p),(t, { f : 9, t : 1}) (this will be the feature value, e.g., of the pairs (a1, p1) and (a2, p4) in Fig. 1), or (t, { f : 8, t : 2}) (the featurevalue of (a1, p2) in Fig. 1).Finally, values of the full TET (4) are either f , or (t, A), where A is a multiset of values from V(T(cid:4)( A, P 1)). Examples are⎞⎧⎨⎩⎧⎨⎩⎧⎨⎩⎧⎨⎩(cid:17)⎛γ1:γ2:γ3:γ4:γ5:⎝t,⎛⎝t,⎛⎝t,⎛⎝t,(cid:16)t,f : 8(t, { f : 9, t : 1}) : 1(t, { f : 8, t : 2}) : 1f : 7(t, { f : 9, t : 1}) : 2(t, { f : 10}) : 1f : 7(t, { f : 8, t : 2}) : 1(t, { f : 10}) : 2f : 8(t, { f : 8, t : 2}) : 1(t, { f : 10}) : 1f : 7(t, { f : 10}) : 3(cid:18)(cid:19).⎫⎬⎭⎫⎬⎭⎫⎬⎭⎫⎬⎭⎠⎞⎠⎞⎠⎞⎠(6)Here, for better readability, the outer multisets are written as column vectors, rather than in comma-separated linearform (used for the inner multisets). These five values are just the feature values of the five authors a1, . . . , a5 in Fig. 1.We will usually use γ to denote TET values. We note that the t component of a value of the form (t, A), A a multiset, isredundant: since every occurrence of a multiset is prefixed by such a t one could just write A instead of (t, A). However,adding the explicit t as an embellishment to the value lets us maintain a clearer match between the structure of a TETT and its values γ ∈ V(T ): otherwise, for example, the value (t, { f : 1}) of a two-level TET would become just { f : 1} andeasily confused with a value f .In the preceding example we have already introduced the values of the TET feature (1) for the entities a1, . . . , a5 in Fig. 1.These values refine the informal count-of-counts shown in Fig. 1 by representing in a more principled way the recursivenature of count-of-counts, and by also including f counts (in this case, the number of papers not written by a given author,and the number of papers not citing a given paper). In the following we give the general definition of the feature valueV (T (a)) ∈ V(T (V )) for a specific tuple a.Definition 2.7. Let M = (M, I) be a model, T (V 1, . . . , V k) a TET, and a ∈ Mk. The value V (T (a)) ∈ V(T ) is defined as follows:Base: If T (V ) = [τ (V )] consists of a single node, then V (T (a)) := I(τ (a)).Induction: If T (V ) = [τ (V ), (W 1, T 1(V , W 1)), . . . , (W m, Tm(V , W m))]:(a) If I(τ (a)) = f then V (T (a)) := f .(b) If I(τ (a)) = t then(cid:2)(cid:3)T (a)V:=(cid:3)(cid:2)t, μ(a, W 1, T 1), . . . , μ(a, W m, Tm),with μ(a, W i, T i) ∈ multisets(V(T i)) given by(cid:22)(cid:21)(cid:22)(cid:21)(cid:21)(cid:20)(cid:21)(cid:20)(cid:2)γ :b ∈ Mki s.t. V(cid:3)T i(a, b)= γ,(7)where γ ranges over all values in V(T i), and ki is the number of variables in W i .We remark that the original definitions given in [17] treated the cases I(τ (a)) = f and I(τ (a)) = t symmetrically, whichis why values according to Definition 2.5 were called “pruned” in [7] (because here the recursive evaluation of counts is cutoff once a node evaluating to fis encountered on a TET branch).We next consider in some detail the special case of unlabeled edges in a TET, and, in particular, purely propositionalTETs. Consider a TET with a single unlabeled branch attached to the root: T = [τ (V ), (∅, T 1(V ))]. The multiset (7) thencontains the single value γ = V (T 1(a)). Generally, an unlabeled edge in a TET induces in the recursive structure of V(T )a single value of the sub-TET Treached by this edge, whereas an edge labeled with one or several variables induces amultiset of such values. The following example serves to illustrate the nature of propositional TETs. At the same time theexample shows how different tree structures of a TET can be used to represent different logical properties.(cid:4)36M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55Table 1Propositional TETs and their values.e(v 1, v 2), ¬e(v 2, v 1)e(v 1, v 2)e(v 2, v 1)(a)(b)e(v 1, v 2)(cid:7)(v 1, v 2)(cid:7)(v 1, v 2)e(v 2, v 1)(c)a1• a2•a1• →a2•a1• ←a2•a1• ↔a2•(a)ftff(b)f(t, { f })f(t, {t})(c)(t, { f }, { f })(t, {t}, { f })(t, { f }, {t})(t, {t}, {t})e(v 1, v 2), e(v 2, v 1)e(v 1, v 2), ¬e(v 2, v 1)¬e(v 1, v 2), e(v 2, v 1)¬e(v 1, v 2), ¬e(v 2, v 1)(d)(d)(t, { f }, { f }, { f }, {t})(t, { f }, {t}, { f }, { f })(t, { f }, { f }, {t}, { f })(t, {t}, { f }, { f }, { f })Example 2.8. Consider a propositional TET T (V ) with V = v 1, . . . , vn. Since no further variables are introduced via edgelabels, then all nodes only contain literals in the variables V , and therefore V (T (a1, . . . , an)) only depends on the relationalsub-structure induced by a1, . . . , an in M. By varying the types at the nodes, as well as the tree structure of T (V ), thevalues V (T (a)) can represent a variety of different structural properties.Consider a relational signature that contains a single binary (“edge”) relation symbol e(·,·). The upper part of Table 1shows 4 different propositional TETs with the two free variables v 1, v 2. In all cases, the TET value T (a1, a2) is determinedby the relational sub-structure induced by a1, a2.The leftmost column in Table 1 lists the four different possible sub-structures, and the remaining columns the valuesreturned by TETs (a)–(d) for these sub-structures.TET (a) represents a feature that only tests whether a1, a2 define the sub-structurea1• →a2• . TET (b) is doing a two-stagetest for the two possible edge relations. If the first test fails, i.e. there is no edge e(a1, a2) then the value is false, regardlessof the presence or absence of the converse edge.Both (c) and (d) use a vacuous root-type (cid:7)(v 1, v 2) to connect sub-TETs that are evaluated separately. The values incolumns (c) and (d) in the table list the values of the different sub-TETs according to the top-to-bottom order on thebranches defined by the graphical representation above. Both TETs discriminate between all four a1, a2-structures in thesense that each structure has a unique value. In this sense, (d) could be seen as a redundant version of (c). However, as wewill see in Section 4, (c) and (d) exhibit quite distinct behavior with respect to the metric we will define on TET values.2.2. TET definable featuresIn this section we illustrate that TETs provide a representation language for a rich class of fundamental features that areusable in a variety of learning frameworks, and that, thus, TET-features can play the role of the sufficient feature space inanalogy to Fig. 2A.Example 2.9. Inductive Logic Programming (ILP) is probably the oldest approach to relational learning. In ILP one learns clas-sification rules for a target predicate, usually in the form of logical clauses. A classic example for ILP are Bongard problems,where scenes consisting of geometric objects have to be classified as positive or negative examples. Here, any such scene isa relational model, whose domain consists of geometric objects on which attributes like triangle( X ) and in( X, Y ) are defined.Then a rule for the positive scene examples can bepositive ← circle(X) ∧ triangle(Y ) ∧ in(Y , X),which says that a scene is positive, if it contains a triangle inside a circle (cf. [35, Chapter 4]). The feature used by thisclassification rule, thus, is a reduction of the TET feature(cid:7) X,Y−−→ circle(X) ∧ triangle(Y ) ∧ in(Y , X).Values of this simple TET are of the form (t, { f : k, t : l}), giving the counts of object pairs that satisfy, respectively do notsatisfy, the body of the rule. The reduced ILP feature is just the Boolean that tests whether l > 0.The original limitation to Boolean features defined by existential quantification has in some ILP based approaches beenlifted via the introduction of special literals that contain aggregators for quantitative information. For example, [44] intro-duce aggregate conditions, which now would allow for a rule like(cid:20)positive ← circle(X) ∧ count(cid:22)Y | triangle(Y ) ∧ in(Y , X)(cid:2) 3,M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5537which says that a scene is positive, if it contains a circle with at least three triangles inside. The Boolean feature used bythis rule is a reduction of the TET feature(cid:7) X−→ circle(X) Y−→ triangle(Y ) ∧ in(Y , X).(8)While not explicitly explored in [44], nested aggregate conditions also could be possible:(cid:20)(cid:20)(cid:22)(cid:21)(cid:21) circle(X), count(cid:21)(cid:22)(cid:21) triangle(Y ) ∧ in(Y , X)Ypositive ← countX(cid:2) 3(cid:2) 2would now classify a scene as positive if it contains at least two circles, each with at least three triangles inside. ThisBoolean feature still is a reduction of the underlying count-of-count feature represented by (8).Several other frameworks have been proposed for constructing numeric or Boolean features by iterated aggregation overchains of relational dependencies. Some of these also are in the ILP tradition [22], others arise out of a combination ofrelational extensions of probabilistic graphical models or decision trees [8,19,1,31].While in this previous work nested aggregation was considered, all these frameworks require an immediate aggregationat each step along a chain of relations. Thus, considering bibliographic data for example, one could define an author featurethat represents whether the author has at least seven publications with at least seven citations each. However, a complexcount-of-count feature that would be sufficient for computing the h-index is outside the scope of these feature constructionmethods.Most of the approaches here mentioned also consider (or, in some cases, focus on) numeric predicates, and aggregationof numeric values using functions like mean and max. For the purely categorical (especially Boolean) data that we considerin this paper, the only common aggregation function is count (with “exists” as the special case “count (cid:2) 1”). In spite of ourcurrent restriction to Boolean data, the same basic TET architecture could also be extended to numeric data, and therebyalso be used to represent the underlying sufficient combinatorial and numerical information needed to compute moremodel-specific aggregated features.Example 2.10. In this example we focus on Markov-Logic Networks (MLNs) [36] in the generative setting. In this case MLNsdefine a distribution over all models M for a given signature R, and a fixed domain M. This distribution is defined by aknowledge base KB containing first-order logic formulas φi with attached numeric weights w i :KB:φ1(X 1) w 1. . .. . .φn(X n) wn(9)We refer to [36] for the details of MLN syntax and semantics. Relevant in the current context is the fact that the distributionis defined as a function of count features, which, adapting the notation of the previous example, can be written in the form(cid:20)countX(cid:21)(cid:22)(cid:21) φ(X)(10)where now φ(X) can be any first-order formula with free variables X . While, thus, very similar in appearance tothe aggregate features of the preceding example, there are some essential differences: first, MLNs depend on the ac-tual integer-valued count feature, not only on derived Boolean features of the form count{. . .} (cid:2) k. Second, (10) takesthe count of all substitutions of tuples of domain elements for all the free variables X in φ(X). As a result, whereascount{Y | triangle(Y ) ∧ in(Y , X)} (cid:2) 3 was a feature of the object X , now count{X | φ(X)} is a feature of entire models M.Concrete MLN implementations will often allow only a restricted class of formulas φ(X) in the model specification(e.g. quantifier free formulas). However, the use of arbitrary first-order formulas poses no problem at the level of semanticdefinitions, and so we here consider models with no restrictions on the φ(X). For any such count feature one can constructa TET T ( ) such that the integer-valued feature (10) is obtained as a reduction of the values V (T ( )). We construct T ( ) inthe form (cid:7) X−→ T φ(X), where now T φ(X) is a TET with free variables X , such that the truth value of φ(a) can be read offthe TET value V (T φ(a)).The construction of T φ is by induction on the structure of φ. In the atomic case φ(X) ≡ r(X) T φ contains the singlenode r(X). For a conjunction φ(X) = φ1(X 1) ∧ φ2(X 2) one lets T φ(X) = [(cid:7), (∅, T φ1 (X 1)), (∅, T φ2 (X 2))]. At this point inthe construction TET values V (T φ(a)) already encode more information than the mere Boolean value φ(a), since it alsocontains the individual truth values for the two conjuncts φ1, φ2 (cf. (c) in Table 1). For the negation case φ(X) ≡ ¬ψ(X)one can simply let T φ = T ψ : if the truth value of ψ(a) is retrievable from V (T ψ (a)), then so is the truth value of ¬ψ(a). Inthe quantifier case φ(X) ≡ ∃Y ψ(Z , Y ) ( X = Z ∪ Y ) one defines T φ(X) = [(cid:7), (Y , T ψ (Z , Y ))]. Again, the TET constructed inthis step represents a counting-refinement of the Boolean feature actually required (note that while (10) has an outermostcounting semantics for the variables X , there is a standard Boolean semantics for any quantifiers appearing internally in φ).Example 2.11. Graphs are a special kind of relational models in which there is a single binary (edge) relation e( X, Y ), and,in the case of labeled graphs, multiple unary relations li( X) representing the different node labels. An important feature38M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55considered in graph mining problems, and used, for example, to define kernel functions on graphs [9], is the number ofsubgraphs of a specific structure: if H is any (finite) graph, thenisomorphic to H that are embedded in G. The (unlabeled) graph H with nodes {1, . . . , k}counts the number of subgraphs Hcan be described by a type that contains one variable Xi for each node i of H , and the literal e( Xi, X j) for each edge i → jin H . Then(cid:21)(cid:20)(cid:21)φH (G) :=(cid:4) | H(cid:4) ⊆ G, HH(cid:22)(cid:21)(cid:21)(cid:4) (cid:16) H(cid:4)(cid:7) X1,..., Xk−−−−−→(cid:23)e(Xi, X j)(i, j):e(i, j)is just the TET feature that corresponds to φH (G). The case of labeled graphs, or the case where Hsubgraph relationship (i.e., there are no edges in G between any nodes matched to nodes of Hpresent in H) are treated in a similar manner.(cid:4)(cid:4)(cid:4) ⊆ G is the induced, other than the edges alsoThe preceding examples illustrate the ability of the TET language to represent in a coherent manner a wide range offeatures used in a variety of different relational learning frameworks. There are some limitations to the TET language,though: TETs are essentially rooted in first-order logic, with a refined counting semantics replacing Boolean existential oruniversal quantification. They cannot represent features that are not first-order in nature, such as features that are definedin terms of the transitive closure of a relation, which can only be defined in suitable extensions of first-order logic, such asleast fixed-point logic, or transitive closure logic [11]. For example, “paper p1 can be found by tracing a chain of citationsstarting with paper p2” is not TET-expressible. Also, the integer-valued feature “Erdös number of author a” cannot becaptured by a TET, since it depends on a chain of co-authorship relations of undetermined length.3. TET discriminant functionA TET alone only defines a feature of objects in a relational domain. TET-defined features can be incorporated in manyways into existing types of predictive or descriptive models. For example, one can define distance or kernel functions onTET value spaces V(T ), thereby making TET features usable for standard clustering techniques, or SVM classifiers. We willintroduce a metric on V(T ) in Section 4. In this section we first describe how to build a predictive model on a TET featureusing simple discriminant functions on TET values, i.e. functions of the form [7]d : V(T ) → R.Such discriminant functions directly lead to binary classification models. We use {+, −} to denote binary class labels. Thenone can learn two discriminant functions d+, d−, and a threshold value t, and assign class label + to a tuple a iff(cid:2)(cid:2)(cid:3)(cid:3)(cid:2)(cid:2)(cid:3)(cid:3)d+VT (a)/d−VT (a)> t.3.1. Defining the simple TET discriminant(11)We now introduce one simple type of TET-discriminant function. The motivation for the particular form of discriminantfunction we propose is twofold: first, for a given TET, these discriminants are efficient to learn and evaluate. Since thediscriminant function is also used in TET learning, within a wrapper evaluation routine for candidate TETs (see Section 5),efficiency is an important issue. Second, as we will show below, our discriminant function definition can be motivated as auniform generalization of the classic decision tree and Naive Bayes models.Definition 3.1. Let T be a TET. A weight assignment β for T assigns a non-negative real number to all nodes of T . A weightassignment can be written as (βr, β 1, . . . ,β m), where βr is the weight assigned to the root, and β i is the weight assignmentto the ith sub-tree.For a TET T with node weights β we define the discriminant function dβ as follows. Let γ ∈ V(T ):• If γ = f define dβ (γ ) := 0.• If γ = t then T = [τ (V )] consists of a single node, and β = (βr). Define dβ (γ ) := βr .• If γ = (t, μ1, . . . , μm), μi ∈ multisets(V(T i)), definedβ (γ ) := βr ·m(cid:24)(cid:24)i=1γ (cid:4)∈μi ,γ (cid:4)(cid:17)= f(cid:2)(cid:3).γ (cid:4)dβ i1βrIn the following we illustrate the nature of this discriminant function. We begin by investigating TETs in a propositionalsetting, where it turns out that our simple discriminant is closely related to the standard decision tree and Naive Bayesmodels.M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5539Table 2A propositional dataset.b( X)c( X)ttfftftfa( X)t1;150;105;34;2f10;69;315;96;2Fig. 3. Decision tree and TET discriminant d+.Fig. 4. Naive Bayes model and TET discriminant d+.Table 2 shows a hypothetical dataset with observations of 100 cases for which three Boolean attributes a, b, c and abinary class label class with values +, − are recorded. To be consistent with our relational notation, we view the Boolean−attributes as unary relations a( X), b( X), c( X) defined on the observations X . The entries in the table represent pairs nof counts for the positive and negative class with the given attribute value combination.; n++−Fig. 3 on the left shows a decision tree that could be constructed for this data. The nodes are labeled with the countsof positive/negative examples that reach the nodes. An example e with attribute values a(e) = t, b(e) = f , c(e) = t,; nnfor instance, would be classified as positive, or, more precisely, would be estimated as being positive with probability 9/14.The right side of the Fig. 3 shows a propositional TET for a, b, c. This TET is labeled with a weight assignment, where−) of the positive class among the examplesthe weight at each node corresponds to the empirical frequency nthat satisfy all the conditions on the path from the root down to the node. For the example a(e) = t, b(e) = f , c(e) = t theTET in Fig. 3 evaluates to+ + n+/(n(cid:2)(cid:3)T (e)V(cid:2)t,=(cid:20)(cid:2)t, { f }, {t}(cid:3)(cid:22), { f }(cid:3).For this value, the discriminant function evaluates as(cid:2)(cid:2)(cid:3)(cid:3)VT (e)β+d= 50/10010/4050/1009/1410/40= 9/14 = P(cid:2)+(cid:21)(cid:21) a(e) = t, ¬b(e) = t(cid:3).(12)A Naive Bayes model learned from the data of Table 2 is shown in the top of Fig. 4, with a corresponding TET below.Again the TET nodes are labeled with weight assignments corresponding to empirical frequencies nNow the example a(e) = t, b(e) = f , c(e) = t evaluates to the TET value(cid:2)(cid:3)T (e)V(cid:2)t, {t}, { f }, { f }, {t}, {t}, { f }(cid:3),=which gives a discriminant function value equal to+/(n+ + n−).40M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55(cid:2)(cid:2)(cid:3)(cid:3)VT (e)β+d= 50/10010/4050/10030/4650/100= P (+)= P (+)P (+ | a(e) = t)P (+)P (a(e) = t | +)P (a(e) = t)31/6450/100P (+ | b(e) = t)P (+)P (¬b(e) = t | +)P (¬b(e) = t)P (+ | c(e) = t)P (+)P (c(e) = t | +)P (c(e) = t).(13)While (13) is not quite equivalent to the posterior probability P (+ | a(e) = t, ¬b(e) = t, c(e) = t) that one would obtain fromββ+(V (T (e)))/d+, only usingthe Naive Bayes model, one still obtains that the ratio d−) as weights) is equal to the odds ratio P (+ | a(e), b(e), c(e))/P (− | a(e), b(e), c(e))the negative class frequencies nin the Naive Bayes model.β− similarly defined as dβ−(V (T (e))) (with d+ +n−/(nThe special cases (12) and (13) for TETs emulating decision trees and Naive Bayes models, respectively, generalize toarbitrary propositional TETs as follows. To simplify matters, consider a TET T with a single free variable X , and vacuous root(cid:7)( X). Assume that T has n nodes besides the root, and that the ith node is labeled with a type τi( X). We denote withφ pi ( X) the Boolean feature that is the conjunction of all types on the path from the root to node i (not including τi( X)itself). Let node i be labeled with the weight P (+ |φ piNow consider an example e. It defines a “prefix” of T consisting of all nodes i for which φ pi (e) ∧ τi(e) evaluates to t.Without loss of generality, assume that the nodes in this prefix are just i = 1, . . . , l for some l (cid:3) n. Then the discriminantfunction value for e is= t, τi = t).(cid:2)(cid:2)(cid:3)(cid:3)VT (e)β+d= P (+)l(cid:24)P (+ |φ piP (+ |φ pi= t, τi = t)= t)l(cid:24)= P (+)P (τi = t | +, φ piP (τi = t | φ p= t)i= t).i=1i=1Let ki (cid:2) 0 be the number of children of node i (cid:3) l within the prefix {1, . . . , l} (ki will usually be less than the actuali be the Boolean feature that is the conjunction of alli depends on e, which defines the relevant∧ τi and class = +,number of children of τi in the full TET T ). For j = 1, . . . , ki let φ jprefix nodes contained in the subtree rooted at the jth child of i (note that φ jprefix). If we now assume that for each i (cid:3) l the child features φ jthen one obtainsi ( j = 1 . . . ki ) are independent given φ piP (+)l(cid:24)i=1(cid:2)τi = tP(cid:21)(cid:21) +, φ pi(cid:3)= t(cid:25)(cid:26)= P+,τi = t,l(cid:23)i=1and from thatβ+(V (T (e)))β−(V (T (e)))dd= P (+,P (−,(cid:27)l(cid:27)li=1 τi = t)i=1 τi = t).(14)β+/dβThus, in this way d− can be interpreted as an odds ratio. The independence assumption we made to arrive at thisinterpretation is appropriate for the decision tree and Naive Bayes emulating TETs: in the first case, it actually becomesvacuous, because here the prefix defined by an example always consists of a single branch. For the Naive Bayes TETs, itis just the regular Naive Bayes assumption. For TETs that do not have a pure decision tree or Naive Bayes structure, theindependence assumption can still be reasonable, and lead to a coherent probabilistic interpretation of the d+/d− ratio.Note, however, that one also easily can construct TETs in which our independence assumption is infeasible, due to logicaldependencies between different child features φ ji . Thus, while our analysis here leads to a general understanding of thenature of the discriminant function, it does not necessarily endow it in all cases with a coherent probabilistic semantics.(cid:4)(V , W ) with the set of branches −→ TSo far, we have considered propositional TETs only. However, the analysis of the discriminant function for this case di-rectly carries over to non-propositional TETs. For this we only have to observe that for any given concrete domain M onecan transform a TET T into an equivalent propositional one by grounding all variable-introducing edges, i.e., by replacing(cid:4)(V , a) (a ∈ Ma branch W−−→ T). If all the groundings of the original branch arelabeled with copies of the original weight assignment, then the discriminant function defined by the grounded TET is thesame as the discriminant function defined by the original TET. Thus, the interpretation of the discriminant function onpropositional TETs also explains the discriminant function on general TETs, with two additional assumptions, or observa-(cid:4)) defined by two different substitutionstions: our general independence assumption implies that features T(cid:4)(V , W ) are assumed as independent, and the d+/d− ratio one obtains is only an approximationof constants in a sub-TET Tof (14), since the weights defined by the initial TET are not exact class frequencies for the ground features, but only sharedapproximations obtained from aggregating statistics from all groundings (see Section 5).(cid:4)(V , a), T(cid:4)(V , a|W |Example 3.2. Consider the following weight assignment for the TET from Example 2.3:1.0author( A) P 1−→1.5authorOf ( A, P 1) P 2−→2.0cites(P 2, P 1).M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5541We compute the discriminant function value for γ2 and γ3 from Example 2.6:⎞⎞⎛⎛f : 7(t, { f : 9, t : 1}) : 2(t, { f : 10}) : 1⎫⎬⎭⎠⎧⎨⎩γ3 :⎝t,f : 7(t, { f : 8, t : 2}) : 1(t, { f : 10}) : 2⎫⎬⎭⎠ .⎧⎨⎩γ2 :⎝t,Since the underlying TET consists of a single branch, here m = 1 at all levels in the recursive value definition. At the firstlevel of recursion we obtain:dβ (γ2) = 1.0 ·(cid:2)dβ(cid:2)(cid:2)t, { f : 9, t : 1}(cid:3)(cid:3)(cid:3)2 · dβ(cid:2)(cid:2)t, { f : 10}(cid:3)(cid:3).To proceed, we compute for values of the form (t, { f : k, t : l}) for the sub-TET T(cid:19)l(cid:16)(cid:2)(cid:2)(cid:3)(cid:3)dβt, { f : k, t : l}= 1.5 ·2.01.5.Plugging this into (15) gives(cid:16)(cid:16)dβ (γ2) = 1.0 ·1.5 ·(cid:19)1(cid:19)22.01.5· 1.5 = (1.5)3 ·(cid:16)(cid:19)2.2.01.5(cid:4)( A, P 1) (cf. (5)):(15)(16)The last re-arrangement of the terms in (16) can be read as follows: each paper P 1 written by author A contributes a factor1.5 to the discriminant function value, and each citation to a paper by A contributes a factor of 2.0/1.5. Since in γ3 the totalnumber of authored papers, and citations to these papers also is 3, respectively 2, the same discriminant function value isobtained:dβ (γ3) = 1.0 · 1.5 ·(cid:19)2(cid:16)2.01.5· 1.52 = (1.5)3 ·(cid:16)(cid:19)2.2.01.5(17)The preceding example points to a limitation of the discriminant function: the function value d(γ ) depends only oncertain “flat” counts contained in γ , not on the more detailed count-of-count structure. On the other hand, being a productof factors determined by simple counts, it turns out that TETs with the discriminant function can emulate Markov LogicNetworks, as the following example illustrates.Example 3.3. Consider a MLN knowledge base (9) in which all φi are conjunctions of literals. The MLN then defines theweight of a model M as(cid:28)ni=1 count{X i |φi (X i )}[M]w i =en(cid:24)(cid:2)e w i(cid:3)count{X i |φi (X i )}[M],i=1where count{X i | φi(X i)}[M] is the value of the count feature (10) in M.The same weight function on models is defined by the discriminant function on the TET(cid:4)(cid:7)( ),(cid:2)(cid:3)X 1, φ1(X 1)(cid:2)(cid:3)(cid:5), . . . ,X n, φ1(X n)with the weight assignment β = (1, e w1 , . . . , e wn ).MLNs whose formulas φi are arbitrary quantifier-free formulas also can be emulated by a TET discriminant function. Forthis one may write φi in a disjunctive normal formk li jkare mutually exclusive. Then the same construction as above applied to all formulas φi j with associated weights w i yields adiscriminant function representation of the MLN weight function. This representation, however, may now be of a size thatis exponential in the length of the original formulas φi .k li jk with literals li jk, such that the individual disjuncts φi j :=(cid:29)(cid:27)(cid:27)j4. TET metricAfter the simple discriminant function of the previous section, we now introduce our second tool to build predictive anddescriptive models directly on TET-defined features. This consists of the definition of a metric on the value space V(T ) of aTET T .The metric is defined by induction on the structure of V(T ). Following Definition 2.5, the base case is:• If V(T ) = {t, f }, define dtet(t, f ) = 1, dtet(t, t) = dtet( f , f ) = 0.42M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55The core of the induction step for dtet consists of the specification of the distance between two multisets of valuesμ = (γ1 : k1, . . . , γm : km), μ(cid:4) =(cid:2)γ (cid:4)1: k(cid:4)1, . . . , γ (cid:4)l(cid:4) : k(cid:4)l(cid:3)where the γi , γ (cid:4)j all come from a value space V( ˜T ) of some sub-TET ˜T of T , and dtet is already defined on V( ˜T ).After normalizing the counts to probability values pi := ki/(k1 + · · · + km), p(cid:4)l) we can view the twovalues as probability distributions on the metric space (V( ˜T ), dtet). A standard way to define a metric on such distributionsis the well-known Wasserstein–Kantorovich or Earth-Mover’s Distance:+ · · · + k(cid:4)j/(k:= k(cid:4)1(cid:4)jDefinition 4.1. Let μ = (γ1 : p1, . . . , γm : pm), μ(cid:4) = (γ (cid:4)to probability distributions p, p1.(cid:4): p(cid:4)1, . . . , γ (cid:4)l(cid:4) : p(cid:4)l) be two multisets over V( ˜T ) with counts normalizedLet dtet be a metric on V( ˜T ). The Wasserstein–Kantorovich distance between μ and μ(cid:4)(cid:3)(cid:30)(cid:2)(cid:2)(cid:3)(cid:2)isdWKμ, μ(cid:4)= infqdteti, j(cid:3)qγi, γ (cid:4)jγi, γ (cid:4)jwhere the infimum is taken over all probability distributions q on V( ˜T ) × V( ˜T ) whose marginal on the first component isequal to p, and whose marginal on the second component is equal to p. Note that dWK is a valid metric if the “grounddistance” dtet is a valid metric [4].(cid:4)We now define dtet(γ , γ (cid:4)) for γ , γ (cid:4) ∈ V(T ):Definition 4.2. Let V(T ) = { f } ∪ {t} ××mγ , γ (cid:4) ∈ V(T ). Depending on whether one or both of γ , γ (cid:4)i=1 multisets(V(T i)), and assume that dtet is defined on V(T i) (i = 1, . . . , m). Letare f , we define dtet(γ , γ (cid:4)) as:(cid:17)= fγγ (cid:4)f0 1f(cid:17)= f 1 Eq. (19)In the non-trivial case γ , γ (cid:4) (cid:17)= f , we have γ = (t, μ1, . . . , μm), γ (cid:4) = (t, μ(cid:4)1, . . . , μ(cid:4)m) with(μ1, . . . , μm),(cid:2)1, . . . , μ(cid:4)μ(cid:4)m(cid:3)∈m×multisets(cid:2)(cid:3)V(T i).i=1Then(cid:2)γ , γ (cid:4)(cid:3):=dtetωidWK(cid:2)(cid:3),μi, μ(cid:4)im(cid:30)i=1where ω0, . . . , ωm > 0 are adjustable weight parameters with(cid:28)i ωi = 1.Proposition 4.3. For all T , dtet is a metric on V(T ) with values in [0, 1].(18)(19)Proof. The statement is clearly true for the base case V(T ) = {t, f }.For the case V(T ) = { f } ∪ {t} ××m[0, 1] on V(T i) (i = 1, . . . , m). Then dWK defined on multisets (μi, μ(cid:4)the convex combination in (19) defines a metric on V(T ) \ { f }. By the condition[0, 1].i=1 multisets(V(T i)) we have the induction hypothesis that dtet is a metric with values ini) over V(T i) is a metric for all i = 1, . . . , m, and hencei ωi = 1 its values lie in the interval(cid:28)It remains to show that the extension via (18) to include the f case still satisfies the properties of a metric.dtet(γ , γ (cid:4)) (cid:2) 0 with equality only for γ = γ (cid:4), and dtet(γ , γ (cid:4)) (cid:3) 1, as well as symmetry dtet(γ , γ (cid:4)) = dtet(γ (cid:4), γ ) are clearlysatisfied. For the triangle inequality, consider dtet(γ , γ (cid:4)) + dtet(γ (cid:4), γ (cid:4)(cid:4)). If γ = γ (cid:4) = γ (cid:4)(cid:4) = f , then the sum is zero, and equalto dtet(γ , γ (cid:4)(cid:4)). If one of γ , γ (cid:4), γ (cid:4)(cid:4)is not f , then the sum is greater or equal 1, and thus greater or equal dtet(γ , γ (cid:4)(cid:4)). (cid:2)The TET metric defined above can be computed using the transportation simplex algorithm, a specialized linear pro-gramming algorithm for solving the transportation problem [37]. Ling and Okada [25] have introduced a faster algorithmfor computing the Earth Mover’s Distance between histogram. However that algorithm assumes fixed-size histograms andcannot deal with signatures of distributions, as required by the recursive definition of dtet.In our experiments, we found the CPU time required for computing the distances to be negligible compared to CPU timefor computing the TET-values. A simple theoretical analysis justifies this finding. Assume we want to calculate the distancebetween two TET values having (for simplicity) the same shape, uniform branching factor m, and height h. Let n = mh thenumber of nodes in the TET value and assume the transportation simplex (which needs to be computed on each TET-valueM. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5543Table 3Distance matrices for propositional TETs.a1• a2•a1• →a2•a1• ←a2•a1• ↔a2•a1• a2•a1• →a2•a1• ←a2•a1• ↔a2•(cid:4)a2•(cid:4)a1•(cid:4)(cid:4)aa2•1• →(cid:4)(cid:4)aa2•1• ←(cid:4)(cid:4)aa2•1• ↔(cid:4)a2•(cid:4)a1•(cid:4)(cid:4)aa2•1• →(cid:4)(cid:4)aa2•1• ←(cid:4)(cid:4)aa2•1• ↔0100a1• a2•01/21/211011(a)a1• →a2•1/2011/2(c)01000100a1• ←a2•a1• ↔a2•1/2101/211/21/20(cid:4)a2•(cid:4)a1•(cid:4)(cid:4)aa2•1• →(cid:4)(cid:4)aa2•1• ←(cid:4)(cid:4)aa2•1• ↔(cid:4)a2•(cid:4)a1•(cid:4)(cid:4)aa2•1• →(cid:4)(cid:4)aa2•1• ←(cid:4)(cid:4)aa2•1• ↔0101a1• a2•01/21/21/21011(b)a1• →a2•1/201/21/2(d)01011110a1• ←a2•a1• ↔a2•1/21/201/21/21/21/20Table 4Author distance matrix: (a): no normalization (b): false counts normalized.a1a2a3a4a5a10a20.110a30.110.020(a)a40.010.110.10a50.130.020.020.120a1a2a3a4a5a10a20.260a30.370.220(b)a40.230.190.140a50.590.320.220.360node) takes a polynomial time O (mk) for some k (in [37] k was empirically found to be between 3 and 4). We thereforehave the recurrence for the running time T (n) of the TET-distance calculation:T (n) = mT(cid:19)(cid:16)nm(cid:3)(cid:2)mk+ O= mT(cid:19)(cid:16)nm(cid:2)nkh(cid:3).+ OBy the master theorem, if h > k then T (n) = O (n) and if h < k then T (n) = O (nkh ).We now illustrate some of the properties of the dtet metric. Our first example uses simple propositional TETs to illustratethe flexibility of the dtet metric that derives from varying TET structures.Example 4.4. Consider the four TETs (a)–(d) from Example 2.8. Table 1 gave for the four possible configurations of entitypairs a1, a2 the associated TET values. Table 3 now shows the distance matrices obtained from evaluating dtet on these val-ues. For (c) and (d) uniform weights for the different branches have been used, i.e., ωi = 1/2 in (c), and ωi = 1/4 in (d). Forbetter readability and ease of comparison, the rows and columns in these matrices are indexed by the a1, a2-substructures,even though the entries in the table are, of course, a function of their values.We obtain the following characteristics of the distance function defined by the four TETs:(a) This is 0/1-distance to the “reference structure” a1 → a2: any pair a(cid:4)1, a(cid:4)2 that has a different structure has distance 1to a1 → a2, and distance 0 to any other pair that also does not have the reference structure.(b) This metric identifies two structures (a1, a2), (a(cid:4)1, a(cid:4)2) (i.e., assigns zero distance between them), if neither contains theedge →. Otherwise, structures have distance 0 iff they are equal, and distance 1 else.(c) Here the TET metric becomes the (normalized) edit distance relative to the two primitive edit operations edge insertionand edge deletion.(d) This is a scaled 0/1 distance: two structures (a1, a2), (a(cid:4)2) have a constant distance > 0 iff they are different. Notethat two distinct structures have distance 1/2 rather than 1, because their values agree on the two out of four TETfor both of them. The two other branches each return a distance of 1, which with thebranches that evaluate to fωi = 1/4 weights gives a total distance 1/2.(cid:4)1, aExample 4.5. Table 4(a) gives the distances between the values γi shown in (6), i.e. the distances between the authors ai inFig. 1 defined by the TET (1).44M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55The matrix shows that according to dtet there are two clusters {a1, a4} and {a2, a3, a5} of authors: the distances betweenauthors within each of these groups is about one order of magnitude smaller than the distance between authors fromdifferent groups.Comparing with Fig. 1 one finds that the clusters are defined by the number of papers written by an author: two for thefirst cluster, and three for the second. Given the difference in the number of authored papers, the citation distribution hasa secondary influence on the distance value: thus, for example, d(a1, a2) < d(a1, a5), because the citation pattern of the twopapers of a1 is more similar to the one of the three papers of a2, than the three papers of a5.The preceding example highlights a potential problem with the definition of dtet: differences in top-level counts appearto have a dominating influence on the distance values. While it often will be reasonable that primary counts have a largerimpact than counts in the lower levels of the TET, it may be desirable to control the extent to which this is happening.In the following we first analyze in a more general manner the distances obtained between certain TET values, and thenintroduce a method for adjusting the metric so that its behavior can be adapted to fit more closely the needs in specificapplications.We consider the generic two-level count-of-count TET(cid:7)(V ) W−→ r(V , W ) U−→ s(W , U ).Assume that the variables W , U are typed, such that W ranges over a sub-domain of size K , and U ranges over a sub-domain of size N. We now consider values of the form(cid:16)(cid:17)t,f : K − k(t, { f : N − n, t : n}) : k(cid:18)(cid:19).These values are symmetric in the sense that all W with r(V , W ) have the same number n of s()-successors U (cf. also thevalues for a2, a4, a5 in (6)). Assuming K , N to be fixed, these values are fully characterized by the two parameters n, k, andone can derive a closed-form expression for pairs of values of this form:(cid:2)dWK(n, k),(cid:3)(cid:3)(cid:2)n(cid:4)(cid:4), k= min(k, kK(cid:4))(cid:4)||n − nN+(cid:4)|.|k − kK(20)(cid:4)|(cid:4) (cid:18) N and k, k|k−kKThis expression explains several potential problems that were already visible in the example of Table 4(a): first, since(cid:4) (cid:18) K , the distances will tend to be very small numbers. Second, the distance is dominatedtypically n, nin counts at the first level of the TET. Furthermore, the distance is sensitive to the sizes of theby the differencedepends on K , N,domains over which the variables range: the behavior of (20) as a function of the actual counts n, k, nin particular on differences in order of magnitude (K (cid:18) N or N (cid:18) K ). Note that the third issue is akin to the situationin standard attribute-value data, where certain numeric features may dominate a distance measure due to the order ofmagnitude of their measuring scale. We can address all these (potential) problems by introducing a normalization operationon TET values., k(cid:4)(cid:4)4.1. Value normalizationIn analogy to standard normalization procedures for numeric data, we introduce a normalization operation for TET values.A standard normalization procedure for numeric data would be a linear transformation x (cid:8)→ ax + b, where the coefficientsa, b are such that the empirical distribution in the transformed dataset has mean 0 and variance 1. Note that here theconcrete coefficients a, b depend on the dataset (the original empirical mean and variance of x), but that the normalizationprocedure in general is defined by the two “hyper-parameters” 0 and 1. Instead of standardizing all numeric attributes to 0mean and variance 1, one could also assign different such hyper-parameters to different attributes, and thereby adjust theimpact different attributes have on the overall distance function.For TET values, we will perform normalization by scaling f counts, i.e., replacing occurrences of f : k by f : ak for some a.The normalization is guided by data-independent hyper-parameters that can be adjusted to optimize the behavior of theTET metric dtet for specific purposes. The concrete multiplicative factors a will then depend on the hyper-parameters, andthe empirical distribution of TET values for which the normalization is performed. The hyper-parameters are defined by anormalization labeling in the sense of the following definition.Definition 4.6. A normalization labeling for a TET(cid:2)(cid:2)(cid:4)τ (V ),(cid:3)W 1, T 1(V , W 1)T (V ) =, . . . ,W m, Tm(V , W m)(cid:3)(cid:5)is given by a vector ( y1, . . . , ym) of non-negative real numbers, and a normalization labeling for each of the sub-TETs T i(i = 1, . . . , m).In the following, we assume for notational convenience m = 1, i.e., T = [τ (V ), (W , T(cid:4)(V , W ))].M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55Definition 4.7. Let T = [τ (V ), (W , TLet Γ = {γ1, . . . , γn} ⊆ V(T ) \ { f }.(cid:4)(V , W ))], and ( y, y(cid:4)) be a normalization labeling ( y(cid:4)a normalization labeling for T45(cid:4)).We write the γi as(cid:2)t,f : kγi =(cid:20)fi , γi,1 : ki,1, . . . , γi,li(cid:22)(cid:3): ki,liwith γi, j (cid:17)= f .Definefavg = 1/nkn(cid:30)i=1kfi ,(cid:17)= favg = 1/nkn(cid:30)li(cid:30)i=1j=1ki, j.The normalization of γi in Γ with hyper-parameters ( y, y(cid:4)) is now given by:• replacing k• replacing each γi, j by the normalization of γi, j in Γ (cid:4) := {γi, j | i = 1, . . . , n; j = 1, . . . , li} with hyper-parameters yfi by y(kfavg)kfi .(cid:17)= favg/k(cid:4).A normalization parameter y specifies the ratio of totalf to non- f counts in all the values in the given dataset corre-sponding to the branch labeled with y.Example 4.8. A normalization labeling for our basic bibliographic TET isauthor( A) P 1,0.1−−−−→ authorOf ( A, P 1) P 2,1.0−−−−→ cites(P 2, P 1).The hyper-parameters (0.1, 1.0) were found in our experiments on predicting the h-index in the DBLP dataset (cf. Sec-tion 6.1). Normalizing the dataset consisting of the 5 values in (6) gives the normalized values:⎧⎨⎩⎧⎨⎩⎧⎨⎩⎧⎨⎩(cid:17)⎛γ ∗1 :γ ∗2 :γ ∗3 :γ ∗4 :γ ∗5 :⎝t,⎛⎝t,⎛⎝t,⎛⎝t,(cid:16)t,f : 0.281(t, { f : 0.878, t : 1}) : 1(t, { f : 0.78, t : 2}) : 1f : 0.246(t, { f : 0.878, t : 1}) : 2(t, { f : 0.976}) : 1f : 0.246(t, { f : 0.78, t : 2}) : 1(t, { f : 0.976}) : 2f : 0.281(t, { f : 0.78, t : 2}) : 1(t, { f : 0.976}) : 1f : 0.246(t, { f : 0.976}) : 3(cid:18)(cid:19).⎫⎬⎭⎫⎬⎞⎠⎞⎠⎭⎞⎫⎬⎭⎫⎬⎭⎠⎞⎠Observe that 0.1 = (0.281 + 0.246 + 0.246 + 0.281 + 0.246)/(2 + 3 + 3 + 2 + 3), and 1 = (2 · 0.878 + 3 · 0.78 + 4 · 0.976)/(1 +2 + 1 + 2 + 2).The distance matrix obtained for these normalized values is shown in Table 4(b). One immediately sees that now therange of distance values is more spread out in the interval [0, 1], and that the clustering according to paper count hasdisappeared. The most dis-similar authors now (as before) are a1 and a5 (few papers with many citations vs. many paperswithout citations). However, the most similar authors now are a3 and a4, who previously even belonged to different clusters.Seeing that a3 differs from a4 only by the addition of one paper without citations, it makes intuitive sense that the distancemeasure optimized for predicting the h-index sees them as nearest neighbors.5. TET learningWe first describe in more detail the learning problem we want to solve. Our data consists of a model M in the senseof Definition 2.1. In our implementation, M is given as a relational database containing one table for each r ∈ R, wherethe table for r contains all tuples a ∈ Marity(r) for which I(r(a)) = true. Furthermore, we are given an initial target table, i.e.a table consisting of a set of examples with +/− class labels. For example, a learning problem given by the data depictedin Fig. 1, with a1, a3, a4 as positive and a2, a5 as negative examples would be given by the 4 leftmost tables in Table 5.Columns in the data tables are headed by synthetic identifiers Argi . Columns in the target table (other than the class label46M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55Table 5Input data tables.Arg1a1a2...a5Arg1p1p2...p10Arg1Arg2Arg1Arg2a1a1...a5p1p2...p10p3p5...p9p1p2...p6Aa1a2a3a4a5Label+−++−authorpaperauthorOfcitestargetTable 6Local target table.Aa1a1a2...a5a5a5Label++−...−−−Pp1p2p3...p8p9p10targetcolumn) are headed by variable names, which will then become the names of the free variables in the TET we construct.Thus, given the input we will want to construct a TET T ( A) over the signature R = {author, paper, authorOf , cites} thatcaptures features of A that are predictive for the class label given in target.Our general approach to TET learning is a recursive top-down construction that associates with each node a local dis-crimination task represented by a local target table. In our running example, starting with the input data in Table 5 wewould initialize the TET construction with the vacuous TET (cid:7)( A). If the first extension was (cid:7)( A) P−→ authorOf ( A, P ), thenwe would associate with the node authorOf ( A, P ) the local target table shown in Table 6. This construction of local targettables is essentially the same as the construction of local training sets in FOIL [34]. The construction of this new target tableamounts to a problem transformation: the problem of predicting the label of an author is transformed into predicting thelabel of author/paper pairs in the new target table, which may be effected by taking into consideration attributes of bothauthors and papers, as well as additional relations between authors and papers (if any such exist in the data).The exact specification of the construction of local target tables is as follows. Let n be a TET node associated with abe a child of n labeled with type σ (V , W ), andlocal target table ttn(V , L) with columns for variables V and label L. Let nreached by an edge labeled with variables W (we include the possibility that |W | = 0, i.e. the edge is really unlabeled).Then nis associated with a target table ttn(cid:4) with columns for V , W and L, defined as:(cid:22)(cid:20)(cid:2)(cid:4)(cid:4)(cid:3)σ (a, b)= true.(21)ttn(cid:4) (V , W , L) =(a, b, l) ∈ M|V |+|W | × {+, −} : (a, l) ∈ ttn; IIn the case |W | = 0, ttn(cid:4) is just the subset of ttn containing the elements for which σ (a) is true.When building the TET, candidate tree-extensions . . . W−−→ σ (V , W ) are scored based on the relational information gain(RIG) measure proposed in [26]. RIG values represent both direct and potential informativeness of the extension: directinformativeness is provided by extensions that in one step increase the class-purity of the local target table. Potentialinformativeness is provided by extensions that introduce new entities into the target table which in subsequent steps mightenable discrimination between positive and negative examples via additional features related to the new entities.High RIG values (unlike information gain in decision tree learning, for example), thus, do not give bounds on a guaranteedimprovement of classification accuracy in a single construction step, but may only indicate a potential improvement thatcould be obtained by further construction steps. After termination of a recursive sub-tree construction, therefore, the finalpredictive accuracy gain of the sub-tree is evaluated, and the sub-tree is pruned if this gain does not exceed a giventhreshold. The evaluation of the current TET’s accuracy has to be based on a concrete classification model built on theTET feature. It is here that we crucially use the discriminant function model of Section 3: a weight assignment definingthe discriminant function is very fast to learn, and the resulting discriminant function values on the validation set are fastto compute. The overall wrapper-evaluation with the discriminant function, thus, is computationally very efficient. If thefinal TET is to be used in conjunction with a different classification model than the discriminant function, then it couldbe beneficial to already use that classification model in the wrapper-evaluation during TET learning. However, for complexmodels this can be computationally very expensive. Furthermore, it does not appear to be the case that TET features learnedusing the discriminant function are highly biased towards this particular classification model. The discriminant function,thus, can play the role of a lightweight classification model that may be used to discover features which are also useful formore complex model types, similarly as in the propositional case decision tree or logistic regression model learning can actas a feature selector also for subsequent use in more complex models, like support vector machines (cf. Fig. 2).M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5547Table 7TET learning.for all σ (V , W ) ∈ CANDbuild_TET(Data M, Labeled table tt, TET_node parent, TET_node root)1. parent.weight = positive_class_frequency(tt)current_score = predictive_score(M, root)2.3. EXT:=possible_extensions(parent, θvars, θdepth)for all σ (V , W ) ∈ EXT compute RIG(tt, σ (V , W ))4.5. CAND:= candidate_extensions(EXT,RIG-values, θRIG)6.7.8.9.10.11.12.13.14.(cid:4) = construct_tt(M, tt, σ (V , W ))ttnextChild = new TET_node(σ (V , W ))add nextChild as child to parent(cid:4), nextChild, root)build_TET(M, ttnew_score == predictive_score(M, root)if new_score − current_score < θscoreremove nextChild from parentelse current_score=new_scoreTable 7 outlines the TET learning algorithm. It is implemented as a procedure that recursively expands an initial TET. Itreceives as arguments the data, a local target table tt to be classified, a pointer parent to the current node to be expandedand a pointer to the root of the TET being constructed. The initial call isbuild_TET(M, tt, T , T )where T = new TET_Node((cid:7)(V )) is a pointer to an initial TET with vacuous root type (cid:7)(V ). The construction works asfollows: Line 1 sets the weight for the discriminant function d+ for the input node. It is just the relative frequency ofpositive examples in the target table associated with this node (the weight for d− being one minus this weight). Thus, thediscriminant function here is learned (at little extra cost) in parallel with the TET construction.The function predictive_score(M, root) called in lines 2 and 11 performs the global evaluation of the current TET basedon its predictive performance in conjunction with the chosen classification model. If a model other than the discriminantfunction here is used, then calls to predictive_score(M, root) may require computationally expensive model training for thecurrent TET.Lines 3–5 are crucial: here a subset of all the possible extensions of the current node defined by types σ (V , W ) of childnodes is constructed for further exploration. This operation is analogous to refinement operators in ILP. Our construction isin two steps: in the first step the set of possible extensions for the current node is constructed by the function possible_ex-tensions. This function can implement various constraints and a language bias. In our implementation and experiments, werestrict possible extensions in terms of the number of literals and the number of new variables in σ (V , W ) (mostly limitingboth numbers to at most one). The function can also take TILDE-style user-defined rmode declarations [3], that can forcecertain arguments of the new literal to be filled with variables already present in the parent node (input variable), or witha new variable introduced by this extension (output variable). As common in ILP algorithms, type predicates can be usedto specify the type of arguments of the literals. In this case the learner expects a unary predicate for each variable type,being true for all and only the objects of that type. In addition to this user-defined bias, we force candidate extensionsto use at least one of the latest introduced variables along their path to the root. The rationale for this constraint is thatextensions introducing new variables were selected based on their RIG score and thus likely on their potential rather thandirect informativeness. By focusing the search toward further refinements of the new variables, we force the algorithm totry making this potential informativeness explicit. When introducing a new variable, the algorithm automatically adds in-equality constraints guaranteeing that they cannot be bound to the same value as that of any of the root variables, i.e. thoseidentifying the entity the TET will represent features of. In case of a typed language bias, inequality constraints are addedfor variables of the same type only. In order to control the computational cost of adding new variables, we constraint thenumber of variables in each path from the root to a leaf to be within a user-defined maximum value (θvars). Finally, possi-ble_extensions is used to implement a termination condition: if the depth of the current parent node in the TET has reacheda (user specified) maximum depth (θdepth), then possible_extensions will return an empty set. In the next step, the relationalinformation gain is computed for all possible extensions; the function candidate_extensions then performs a selection basedon RIG values. Our current implementation of candidate_extensions selects all extensions whose RIG value exceeds a userdefined threshold (θRIG).A child node is then created for each candidate extension. The function construct_tt constructs the local target table forthe child according to (21). Lines 8–9 add a new child labeled with the current candidate extension σ (V , W ) to parent. Line10 continues the recursive construction at the new child, which then becomes the root of a whole new subtree. Lines 11–14then evaluate the extension of the old TET with this new subtree, and either accept or reject it based on a user definedthreshold for the required global score improvement (θscore ).48M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55Fig. 5. The TET learned on the DBLP data set for the task of h-index prediction. The first branch on the left represents the relational features which arenecessary and sufficient to compute h-indices.6. Experiments6.1. DBLPAs already stated in the introduction, bibliometrics represents an ideal domain to test the capability of TETs of learningcount-of-counts features: in particular, we focus on the task of predicting h-indices. The h-index of an author A is definedas the maximum number h such that A authored h papers having at least h citations each [14].The data set used in our experiments is taken from the DBLP Computer Science Bibliography [24]2 enhanced with citationdata [41]. We extracted a set of facts and relations in the form of a MySQL database from the original data set availableat http://www.arnetminer.org/citation. For the sake of reproducibility, we also provide a package with the scripts used tobuild the data set used in our experiments (see Supplementary Materials). From the original database we extracted thefollowing tables:• author( A) providing the id of each author;• paper( P ) providing the id of each paper;• author_of ( A, P ), true if A is an author of P ;• cites( P 1, P 2), true if P 1 cites P 2.Our first goal was to learn a TET able to discriminate between authors with high and low h-index (i.e., above/belowa certain threshold). To this end, we extracted a sub-graph of the whole DBLP network (see Supplementary Materials)consisting of 8726 authors and 244,265 papers, and used the learning algorithm described in Table 7, employing relationalinformation gain [26] as the scoring function guiding the search, and the discriminant function of Definition 3.1 to evaluatethe TET score after the introduction of each new literal.3 We chose an h-index threshold h = 7 to define positive andnegative examples. We fixed θRIG = 0, so that each candidate extension with non-zero RIG was considered (in the ordergiven by the RIG score), θscore = 1e − 4 in order to prune away branches with low improvement, θdepth = 2 and θvars = 3.Fig. 5 shows the learned TET: it consists of three different branches, the first corresponding to the count-of-counts featurewhich can be used to exactly compute the h-index, and the two other branches describing features which are also correlatedto the h-index of author A: the number of papers Pcited by each paper P written by A (second branch), and the numberof co-authors Afor each paper P of A (third). One may wonder why the learner did not return a TET consisting only of thefirst branch, since this feature would be sufficient to predict the h-index in the training examples exactly. Note however thatthe classification accuracy on the training examples is also constrained by the prediction model that maps the TET featurevalue into a binary classification. The discriminant function used by the learner (as well as virtually any other conceivableprediction model) is not able to exactly map the feature value obtained from the first branch into the binary thresholdfunction h > 7, and therefore additional features represented by the additional branches can still be useful for obtaining abetter fit to this threshold.(cid:4)(cid:4)The TET shown in Fig. 5 in combination with the discriminant function guiding the learning phase achieves an F1of 61.3% for the binary classification task of predicting authors with h > 7. As a comparison, we employed TILDE [3] toinductively learn a logical decision tree in the same setting. The task turned out to be quite difficult for TILDE: being theauthor_of ( A, P ) predicate only potentially informative but not directly informative [26], with the same language bias used byour TET learner, TILDE ended up the search with an empty tree. We therefore tried to modify the language bias, by allowingthe joint introduction of the predicates author_of ( A, P ) and cites( P 1, P 2): the result was a quite complex tree learned byTILDE, which anyhow contained only three positive leaves covering a few examples, heading to an F1 of 1.8 %. We also triedto define some aggregates4 in the language bias, counting the number of papers of an author and the number of citations ofa paper. We used the TILDE option which allows the introduction of multiple aggregates within the same tree branch,5 butin this case the search could not be completed due to memory requirements. We finally tried to use exhaustive lookahead,which turned out to be computationally very expensive: with only one level of lookahead, TILDE ran for over 20 dayswithout terminating. As a comparison, the TET learning algorithm ran for about 7 minutes. Finally, in order to assess the2 http://dblp.uni-trier.de.3 No particular rmode declaration was specified.4 Aggregates are specific rmodes for TILDE defining predicates that represent aggregating functions, such as count, average, min, max.5 This option is called aggregate_refinement.M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5549Fig. 6. Results on the task of h-index prediction over years: we plot the RMSE of several competitors as a function of the prediction year.potential of the TET metric introduced in Section 4, we also tested our TET with a k-NN classifier in a leave-one-out setting,using the TET metric between TET values: in this case, the F1 achieved by the classifier, even without value normalization,was 88.5% with a single neighbor, and up to 91.2 with k = 5.The second experiment is about h-index forecasting: given data up to a given year Y 0, we predict the h-index of anauthor in the forthcoming years. In this case, our aim was to measure the discriminative power of the metric defined onTET values and described in Section 4, with respect to several baselines and other prediction models using plain counts.The experimental framework is constructed as follows: first, we extracted from the whole DBLP the set of 8441 authorshaving h-index > 3 in Y 0 = 2000, and then split this set into 2/3 for the development set, and 1/3 for the test set (thedevelopment set was again split in 2/3 for training and 1/3 for validation). Our predictor was built as follows: using just thesimple TET shown in the first branch of Fig. 5, which describes all the sufficient features to calculate h-index, we computedTET values for all the authors, and then run a simple k-NN algorithm employing the TET metric as distance between suchvalues. This predictor was compared against several different competitors:1. predict future h-index as equal to current h-index (SAME);2. predict future h-index of a test author a as the average of future h-indices of training authors having current h-indexequal to a (AVFUT);3. predict future h-index using a k-NN algorithm, using as distance a linear combination of plain counts features, that isthe number of papers npap and the number of citations ncit (hpred = w pap × npap + w cit × ncit) (k-NN counts);4. predict future h-index as a non-linear Support Vector Regressor taking plain counts features npap and ncit as input (SVR);5. predict future h-index using a k-NN algorithm with the TET metric, but using the whole learned TET (represented inFig. 5) to compute TET values (complete TET k-NN).Note that albeit very simple, the SAME and AVFUT predictors have a significant advantage over the other methods,since they make direct use of the h-index as a materialized feature in the data, whereas the other methods only are givenunderlying paper and citation counts.The validation set was used to perform model selection over the parameters of each model: regularization parameter C−4, . . . , 1); w pap, w citand Gaussian kernel width γ were tuned for SVR (C ranges in 10−2, . . . , 102) and the number of neighbors k for the counts-based k-NN; the normalization coefficients(both ranging in 10y and the number of neighbors k for k-NN with TET metric.6 Root Mean Squared Error (RMSE) was used to measure theperformance of each predictor.−2, . . . , 102, while γ ranges in 10Fig. 6 shows the results obtained as a function of the prediction horizon H over years, starting from 2000 – the year forwhich TET values were computed, and therefore corresponding to H = 0 – up to 2009 (H = 9).The SAME and AVFUT predictors, by construction, have 0 error at H = 0, and remain very accurate for short predictionhorizons, as the dynamics of h-indices change slowly over time. The TET based predictor has a lower average predictionerror starting from H > 5, and always outperforms methods based on plain counts of features. It should also be noticed that6 Model selection for the normalization coefficients was performed on the simple TET, and resulted in the normalization labeling shown in Example 4.8.50M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55Table 8F1 results on the CORA dataset. Horizontal lines separate different learning parameter settings (allowing for increasingly complex TETs to be learned),plus a manually curated TET T (hand). Rows within each learning setting indicate a different predictive model, either discriminant function (DF) or k-NNwith the TET metric, for different values of k > 1 (only values producing non-negligible differences are reported). Columns report results for each fold andmacro-averaged over the five folds.θdepthθvars1122333322334444handhandhandhandhandLearnDFk-NN>1DFk-NN>1DFk-NN5k-NN10k-NN100DFk-NN5k-NN10k-NN100k-NN500189.289.289.289.279.288.089.486.689.293.694.294.797.6290.690.690.690.690.690.695.795.790.699.298.999.799.7390.190.190.890.358.970.978.690.890.896.897.198.098.0495.295.295.295.289.688.894.295.895.296.697.799.399.3588.088.088.088.088.088.081.181.188.091.394.595.395.3avg90.690.690.890.781.385.387.890.090.895.596.597.498.0Table 9Area under the recall-precision curve (AURPC) results on the CORA dataset. Horizontal lines separate different learning parameter settings (allowing forincreasingly complex TETs to be learned), plus a manually curated TET (hand). Rows within each learning setting indicate a different predictive model,either discriminant function (DF) or k-NN with the TET metric, for different values of k > 1 (only values producing non-negligible differences are reported).Columns report results for each fold and macro-averaged over the five folds.θdepthθvars1122333322334444handhandhandhandhandLearnDFk-NN>1DFk-NN>1DFk-NN5k-NN10k-NN100DFk-NN5k-NN10k-NN100k-NN50010.9260.9660.9260.9660.8720.9060.9320.9450.9260.9920.9920.9940.99520.9930.9930.9930.9930.9930.9930.9930.9930.9931.001.001.001.0030.9400.9400.9460.9560.9860.8800.9280.9600.9460.9860.9950.9960.99240.9800.9800.9800.9800.9600.9250.9690.9820.9800.9980.9991.001.0050.9570.9570.9570.9570.9570.9570.9570.9570.9570.9620.9620.9530.971avg0.9590.9670.9600.9700.9540.9320.9560.9670.9600.9880.9900.9890.992the k-NN algorithm based on the complete learned TET performs slightly better than the simple TET for longer predictionhorizons. This happens because the TET metric, although taking into account count-of-counts features (and thus performingconsistently better than plain counts-based metrics), is not able to exactly compute the h-index starting from these features.6.2. CoraCORA is a dataset of research papers and their citations, originally collected by Andrew McCallum and used for differentpredictive tasks including hierarchical classification, information extraction, and citation matching. Here we focus on thislatter task, namely predicting whether two bibliographic records refer to the same paper. We rely on the relational datarepresentation and experimental setting defined by Singla and Domingos [40].The domain consists of entities of types title, author, venue (given by a string value), bibrec (a bibliographic record givenby its author, title and venue fields), as well as title_word, author_word, venue_word, which are the constituent words appear-ing in title, author, and venue. Only the first author of each record is considered in this setting. The data set contains 1295bibliographic records, referring to 132 different research papers, 50 authors, and 103 venues. Relations are all represent-ing part-of relationships: title_of, author_of, and venue_of link bibliographic records to their constituent fields; word_in_title,word_in_author, word_in_venue link complete field strings to their constituent words. Note that relations in the first groupare one-to-one, whereas the second group is one-to-many.The experimental setting by Singla and Domingos [40] consists of a five fold cross validation procedure over plausiblecandidate pairs as identified using McCallum et al.’s canopy approach [28], with TF-IDF cosine as the similarity measure.This results in 52,923 overall candidate pairs, with 30,971 positive and 21,952 negative pairs respectively. The compileddataset is available at alchemy.cs.washington.edu.M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5551Fig. 7. Features in the CORA TETs for entity resolution on bibliographic records.We fixed θRIG = 0, as in the DBLP experiments, and θscore = 1e − 2. We ran the TET learner on each of the five differenttraining sets with increasing values of the main parameters controlling the size of the search space, θdepth and θvars. Tables 8and 9 report F1 and Area under the recall-precision curve (AURPC) values for each fold and macro-averaged on the fivefolds. The simple discriminant function guiding the TET learning phase (DF) is compared to k-NN with the TET metric, fordifferent values of k > 1 (only values producing non-negligible differences are reported). We did not perform any fine tuningon the TET metric, leaving all hyper-parameters to one (see Section 4.1).The first apparent finding is that small TETs seem to perform quite well on this dataset, and increasing their complexitydoes not pay much. Fig. 7(a) shows the TET learned in four (1, 2, 4, 5) out of five folds for both (θdepth = 2, θvars = 1) and(θdepth = 3, θvars = 2) parameter settings and in two (2, 5) folds for the (θdepth = 4, θvars = 3) setting. This represents thevery basic feature of the two records B 0, B1 having identical title and/or venue fields. It is noteworthy that neither here,nor in any of the other TETs, features involving the author field were constructed. This may be due to the fact that theCORA dataset contains relatively many different publications by a relatively small number of (first) authors, so that theauthor field becomes a quite poor predictor for the identity of papers. Note that simple pairs with clearly different firstauthors were preliminarily excluded by the canopy construction [28] and are not part of the plausible candidates. The onlydifference between the two simpler learning settings is in fold three, where the (θdepth = 3, θvars = 2) setting learns the TETin Fig. 7(b), while the TET learned by the simplest setting (θdepth = 2, θvars = 1) lacks the T W−−−→ word_in_title(T , T W ) branch.The former achieves slightly better results, possibly because of a correlation between the number of title words and thelikelihood that two entries refer to the same paper.Learning more complex TETs does not seem to provide improvements in this setting. However, an inspection on thelearned TETs gives interesting insights on the potential of the mined features. Figs. 7(c) and (d) show linear branches thatconstitute the main features in two of the complex TETs. (c), at first, appears to be a fairly complex feature, which becauseof its three variable introductions would represent a three-level hierarchical count. However, due to the one-to-one nature ofthe relations in the extensions V−→ venue_of (B0, V ) and T−→ title_of (B, T ), only the extension B−→ venue_of (B, V ) introducesreal counts other than 0 and 1. Roughly speaking, the feature (c) counts the number of records B that have the same venueas B0, and the same title as B1. In both TETs containing the branch (c), also the dual branch with the roles of B 0 andB1 interchanged was constructed. Intuitively, this feature uses the transitivity of the same_paper relation by considering“interpolating” records B for which there is evidence that they are equal to B 0 because of agreement in the venue field,and equal to B1 because of agreement in the title field (or vice-versa). Note that the dataset includes 103 distinct venuesfor the 132 papers, which makes venue almost as discriminative as title.52M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–55Fig. 8. Handcrafted CORA TET representing idf-like counts-of-counts features. The dashed triangle indicates a copy of the title subtree, with title replaced byvenue.(d) is a refinement of the left branch of (a): in addition to testing equality of the title field, (d) also counts the number ofadditional records B that have the same title. It appears, however, that the introduction of the variable T 1 in the extensionT 1−−→ title_of (B, T 1) here is redundant, since T 0 at this place is already established as the unique title of B, and so thesimpler extension −→ title_of (B 0, T 0) would express the same logical feature. The reason for the roundabout way (d) endsup taking for defining this feature lies in the fact that our learning algorithm includes a constraint that variables introducedat a previous extension (B in this case) should be used in the next child node (see Section 5).With the exception of the T W−−−→ word_in_title(T , T W ) branch of TET (b), all features discussed so far do not use theword-related relations. Comparisons are based on identity of whole title and venue strings. This is different in (e), whichshows the complete TET learned for the third fold in the (θdepth = 4, θvars = 3) learning setting. It again is a refinement ofthe same-title feature, but this time in its right sub-branch also introducing counts of title words, and by means of theT 1−−→ word_in_title(T 1, T W ) → title_of (B0, T 1) a count-of-count feature that incorporates an inverse-subsequent extensiondocument frequency feature (cf. TET (2) in Example 2.4). To understand the meaning of this branch, consider how thesub-TET rooted at word_in_title(T 0, T W ) evaluates for a title word w: if w does not occur in the title of B1, then the valueis f . If w is in the title of B1, but not in the title of B 0, then the value is (t, {(t, { f }) : k1, f : k2}), where k1 and k2 arethe number of titles in the domain that contain, respectively do not contain, w. If, finally, w occurs both in the title of B 1and B0, then the value is (t, {(t, {t}) : k1, f : k2}), with the same k1, k2 as before. The branch T W−−−→ · · · then provides foreach of these possible values the count of words w with that value. In this manner, the values of the right branch pro-vide all the count-of-count statistics required for the inverse-document-frequency feature. Note that the more parsimoniousrepresentation (2) is not learnable by our current TET learner, since we only allow single-literal nodes.We see, thus, that the learned complex TETs represent very reasonable features. In order to better understand whetherthe lack of performance gain over the simpler TETs is possibly due to lack of predictive relevance of these features, or dueto a difficulty for the TET metric to utilize the feature information in the way it is presented by these TETs, we manuallydesigned a TET that encodes the idf-like feature represented by (e) in a way that is optimized for the TET metric (Fig. 8).It provides separate branches for words in each of the titles and each of the venues (the dashed triangle indicates a copyof the title subtree, with title replaced by venue). As shown in the last rows of Tables 8 and 9, the TET achieves quitehigh performance when paired with k-NN employing the TET-metric, with almost perfect AURPC in most folds, while thediscriminant function again fails to exploit the full potential of counts-of-counts features.We can conclude that the TET learner is able to discover complex features with high discriminative value. The fact thatsome post-processing in the representation of the features was needed to obtain the best performance results with k-NNprediction may not be very surprising, since the learner is not optimizing with regard to the TET metric. In future workthis kind of post-processing may be automated by implementing efficient techniques for optimizing the parameters of theTET metric (weight parameters ω and normalization labels y), and optimizing the TET structure using post-pruning and-balancing operations.The results of the learned TETs are comparable with those achieved with a Markov Logic Network (MLN) that (like ourTET) is language independent, i.e. does not contain rules referring to specific strings occurring in the data, which achievesM. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5553an AURPC of 0.971 [40]. Note that the MLN based approach in [40] – as well as more recent approaches achieving stillhigher accuracy [33,39] – perform collective classification, and therefore can exploit the fact that the binary relation onbibliographic records that one predicts is an equivalence relation. The two classification models we have used both performindependent predictions for each pair of bibliographic records, and therefore cannot be expected to achieve results that arecompetitive with state-of-the-art collective approaches. It should be emphasized, though, that in [33,39] the MLN structure(i.e. the set of logical formulas) was carefully designed by hand, while in our experiments the TET structure is learned fromdata. A carefully crafted TET as described in Fig. 8 indeed achieves a macro-averaged AURPC of around 0.99. Additionally, TETfeatures could equally well be used in connection with collective classification techniques. We also compared TET resultswith those achievable by TILDE, with and without aggregates. As for the DBLP case, the search procedure of TILDE suffersfrom a lack of direct informativeness of single predicates, and plain TILDE returns an empty tree for all folds. However,exhaustive lookahead allows us to overcome the problem and recover the same rules of the simple TET in Fig. 7(a), thusachieving substantially equivalent results (macro averaged F1 = 91.0%, slightly better than those of the simple TET as Tildelearns these rules for the third fold too). More complex features, like the idf-like ones in Figs. 7(e) and 8 cannot be recoveredby plain Tilde, and adding aggregates in the language bias concerning counts of author, title and venue words dramaticallyincreases learning time: the search did not finish after a week of CPU time.As for inductive logic programming and relational rule learning approaches, learning time strongly depends on the con-straints imposed on the search space, which is otherwise exponential in the number of candidate predicates. Learning TETstakes roughly three minutes, one hour and 20 hours respectively, on average over the five folds, for the three increasinglycomplex learning settings (θdepth = 2, θvars = 1), (θdepth = 3, θvars = 2), (θdepth = 4, θvars = 3). As a matter of comparison, Tildelearns the tree resembling TET in Fig. 7(a) in 30 seconds or 11 hours, depending on the number of exhaustive lookaheadsallowed (one and two, respectively).7. ConclusionProperties of entities in a relational domain can depend on complex combinatorial count-of-count features characterizingthe entities’ relational neighborhood. Examples of properties that are directly defined in terms of count-of-count featuresare the h-index of an author, and certain relevance measures widely used in information retrieval. Type Extension Treesare a simple, but highly expressive representation language for count-of-count features. In this article we have presented amethod for learning Type Extension Trees in supervised learning settings as a means of discovering count-of-count featuresthat are informative for the prediction of a class label.Most existing frameworks for statistical relational learning either are only based on simpler, “flat”, count features, ortheir use of count-of-count features is only implicit in the specification of conditional probability distributions, and does notinclude an interpretable representation of the underlying features. Examples of frameworks of the first kind are Markov LogicNetworks [36] (cf. Example 2.10), and systems providing simple aggregation operators [1,13,18]. Examples of frameworks ofthe second kind are probabilistic relational models that allow the specification of conditional probability distribution usingnested combination functions [16,30].Kernel methods can be also applied to (implicitly) extract features from relational data. The general framework of con-volution kernels [12] has originated a wealth of different approaches for defining the similarity between structured objects(see e.g. [45] and references therein). Features defined by these kernels essentially count fragments or substructures, butnot counts of counts. In most cases, these methods aim to develop a suitable representation of structured data for subse-quent learning, not to discover features. There are previous works, however, where the feature space itself is learned fromrelational data [23,29] and is interpretable in terms of definite clauses.In most of these previous works relational feature construction is an integral part of a particular learning paradigm.Relational features in their own right have previously been investigated in [32]. Here a systematic view of aggregation-basedfeatures at different levels of complexity is developed. However, the focus still is on aggregation over a single level ofrelational dependencies.Discovered TET features can be used in a variety of classification models, and could be integrated into existing modelssuch as relational probability trees [31], or inductive logic programming systems, for which simpler types of count featureshave already been used [1]. In this paper we have considered two approaches for directly augmenting TET features intofull prediction models. The simple discriminant function is fast to learn and evaluate, but only makes limited use of thecount-of-count information provided by a TET feature value. We have therefore also introduced a metric on TET valuesdefined by a recursive application of the Wasserstein–Kantorovich metric. With this metric, distance-based methods forsupervised or unsupervised learning become directly applicable.Our experiments have shown that our TET learning algorithm is able to discover non-trivial and interpretable count-of-count features. A comparison of the classification accuracies achieved with the discriminant function model and k-nearestneighbor classification based on the TET metric indicates that TET features learned using the discriminant function can alsosupport other classification models, and that a model that exploits the complex count-of-count information outperformsmodels only using flat counts.54M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–558. Supplementary materialThe software for TET learning and for the computation of Wasserstein–Kantorovich metric between TET values, togetherwith the data used in the experiments presented in this paper, can be downloaded at http://www3.diism.unisi.it/~lippi/research/TET.html.AcknowledgementsAP, ML and PF were partially supported by PRIN grant 2009LNP494_002.References[1] A. Van Assche, C. Vens, H. Blockeel, S. Dzeroski, First order random forests: Learning relational classifiers with complex aggregates, Mach. Learn. 64(2006) 149–182.[2] Carl Bergstrom, Measuring the value and prestige of scholarly journals, Coll. Res. Libr. News 68 (5) (2007) 314–316.[3] H. Blockeel, L. De Raedt, Top-down induction of first-order logical decision trees, Artif. Intell. 101 (1–2) (1998) 285–297.[4] Vladimir I. Bogachev, Aleksandr V. Kolesnikov, The Monge–Kantorovich problem: achievements, connections, and perspectives, Russ. Math. Surv. 67 (5)(2012) 785.[5] Sergey Brin, Lawrence Page, The anatomy of a large-scale hypertextual web search engine, Comput. Netw. 30 (1–7) (1998) 107–117.[6] Luc De Raedt, Paolo Frasconi, Kristian Kersting, Stephen Muggleton (Eds.), Probabilistic Inductive Logic Programming: Theory and Applications, LectureNotes in Computer Science, vol. 4911, Springer, Berlin, 2008.[7] P. Frasconi, M. Jaeger, A. Passerini, Feature discovery with type extension trees, in: Proceedings of the 18th Int. Conf. on Inductive Logic Programming(ILP), in: Lecture Notes in Artificial Intelligence, vol. 5194, 2008, pp. 122–139.[8] N. Friedman, Lise Getoor, D. Koller, A. Pfeffer, Learning probabilistic relational models, in: Proceedings of the 16th International Joint Conference onArtificial Intelligence (IJCAI-99), 1999.[9] T. Gärtner, P. Flach, S. Wrobel, On graph kernels: Hardness results and efficient alternatives, in: Proceedings of the 6th Annual Conference on Compu-tational Learning Theory and the 7th Kernel Workshop, in: LNAI, vol. 2777, 2003, pp. 129–143.[10] Lise Getoor, Ben Taskar (Eds.), Introduction to Statistical Relational Learning, MIT Press, Cambridge, MA, 2007.[11] E. Grädel, Finite model theory and descriptive complexity, in: Finite Model Theory and Its Applications, in: Texts in Theoretical Computer Science,Springer, 2007, Chapter 3.[12] D. Haussler, Convolution kernels on discrete structures, Technical report 99-10, UCSC-CRL, 1999.[13] D. Heckerman, C. Meek, D. Koller, Probabilistic entity-relationship models, PRMs, and plate models, in: L. Getoor, B. Taskar (Eds.), Introduction toStatistical Relational Learning, MIT Press, 2007.[14] J.E. Hirsch, An index to quantify an individual’s scientific research output, Proc. Natl. Acad. Sci. 102 (46) (2005) 16569–16572.[15] Wilfrid Hodges, Model Theory, Cambridge University Press, 1993.[16] M. Jaeger, Relational Bayesian networks, in: Proceedings of the 13th Conference of Uncertainty in Artificial Intelligence (UAI-13), Providence, USA,Morgan Kaufmann, 1997, pp. 266–273.[17] M. Jaeger, Type extension trees: A unified framework for relational feature construction, in: Proceedings of Mining and Learning with Graphs (MLG-06),2006.[18] Y. Kavurucu, P. Senkul, I.H. Toroslu, Concept discovery on relational databases: New techniques for search space pruning and rule quality improvement,Knowl.-Based Syst. 23 (8) (2010) 743–756.[19] A. Knobbe, M. de Haas, A. Siebes, Propositionalisation and aggregates, in: Proceedings of PKDD 2001, 2001, pp. 277–288.[20] A.J. Knobbe, A. Siebes, D. van der Wallen, Multi-relational decision tree induction, in: Proceedings of PKDD-99, 1999, pp. 378–383.[21] Stanley Kok, Pedro Domingos, Learning the structure of Markov logic networks, in: Luc De Raedt, Stefan Wrobel (Eds.), ICML, in: ACM InternationalConference Proceeding Series, ACM, 2005, pp. 441–448.[22] M.-A. Krogel, S. Wrobel, Transformation-based learning using multirelational aggregation, in: Proceedings of ILP 2001, in: LNAI, vol. 2157, 2001,pp. 142–155.[23] N. Landwehr, A. Passerini, L. De Raedt, P. Frasconi, Fast learning of relational kernels, Mach. Learn. 78 (3) (2010) 305–342.[24] Michael Ley, The dblp computer science bibliography: Evolution, research issues, perspectives, in: Alberto H.F. Laender, Arlindo L. Oliveira (Eds.), SPIRE,in: Lecture Notes in Computer Science, vol. 2476, Springer, 2002, pp. 1–10.[25] Haibin Ling, Kazunori Okada, An efficient earth mover’s distance algorithm for robust histogram comparison, IEEE Trans. Pattern Anal. Mach. In-tell. 29 (5) (2007) 840–853.[26] Marco Lippi, Manfred Jaeger, Paolo Frasconi, Andrea Passerini, Relational information gain, Mach. Learn. 83 (2) (2011) 219–239.[27] Nathan N. Liu, Qiang Yang, Eigenrank: a ranking-oriented approach to collaborative filtering, in: Proceedings of the 31st Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, 2008, pp. 83–90.[28] Andrew McCallum, Kamal Nigam, Lyle H. Ungar, Efficient clustering of high-dimensional data sets with application to reference matching, in: Pro-ceedings of the sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’00, New York, NY, USA, ACM, 2000,pp. 169–178.[29] S. Muggleton, H. Lodhi, A. Amini, M. Sternberg, Support vector inductive logic programming, Innov. Mach. Learn. (2006) 113–135.[30] S. Natarajan, P. Tadepalli, E. Altendorf, T.G. Dietterich, A. Fern, A. Restificar, Learning first-order probabilistic models with combining rules, in: Proceed-ings of the 22nd International Conference on Machine Learning (ICML-05), 2005, pp. 609–616.[31] Jennifer Neville, David Jensen, Lisa Friedland, Michael Hay, Learning relational probability trees, in: Lise Getoor, Ted E. Senator, Pedro Domingos, ChristosFaloutsos (Eds.), KDD, ACM, 2003, pp. 625–630.[32] C. Perlich, F. Provost, Aggregation-based feature invention and relational concept classes, in: Proceedings of the 9th ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining (KDD-2003), 2003.[33] Hoifung Poon, Pedro Domingos, Joint inference in information extraction, in: Proceedings of AAAI-07, 2007, pp. 913–918.[34] J.R. Quinlan, Learning logical definitions from relations, Mach. Learn. 5 (1990) 239–266.[35] L. De Raedt, Logical and Relational Learning, Springer, 2008.[36] M. Richardson, P. Domingos, Markov logic networks, Mach. Learn. 62 (1–2) (2006) 107–136.[37] Yossi Rubner, Carlo Tomasi, Leonidas J. Guibas, The earth mover’s distance as a metric for image retrieval, Int. J. Comput. Vis. 40 (2) (2000) 99–121.[38] Ajit Paul Singh, Geoffrey J. Gordon, Relational learning via collective matrix factorization, in: Proceedings of KDD-08, 2008, pp. 650–658.M. Jaeger et al. / Artificial Intelligence 204 (2013) 30–5555[39] Sameer Singh, Karl Schultz, Andrew Mccallum, Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factorgraphs, in: Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD ’09, Springer-Verlag, Berlin, Heidelberg, 2009, pp. 414–429.[40] P. Singla, P. Domingos, Entity resolution with Markov logic, in: Proceedings of ICDM-06, 2006.[41] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, Zhong Su, Arnetminer: extraction and mining of academic social networks, in: Proceedings ofKDD-08, 2008, pp. 990–998.[42] Ben Taskar, Carlos Guestrin, Daphne Koller, Max-margin Markov networks, in: Sebastian Thrun, Lawrence Saul, Bernhard Schölkopf (Eds.), Advances inNeural Information Processing Systems 16, MIT Press, Cambridge, MA, 2004.[43] G. Van den Broeck, N. Taghipour, W. Meert, J. Davis, L. De Raedt, Lifted probabilistic inference by first-order knowledge compilation, in: Proceedings ofIJCAI 2011, 2011.[44] C. Vens, J. Ramon, H. Blockeel, Refining aggregate conditions in relationallearning,in: Proceedings of PKDD 2006,in: LNAI, vol. 4213, 2006,pp. 383–394.[45] S.V.N. Vishwanathan, N.N. Schraudolph, R. Kondor, K.M. Borgwardt, Graph kernels, J. Mach. Learn. Res. 99 (2010) 1201–1242.