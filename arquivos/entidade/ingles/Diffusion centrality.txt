Artificial Intelligence 239 (2016) 70–96Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDiffusion centrality: A paradigm to maximize spread in social networksChanhyun Kang a, Sarit Kraus b, Cristian Molinaro c,∗V.S. Subrahmanian aa Department of Computer Science, University of Maryland, USAb Department of Computer Science, Bar-Ilan University, Israelc DIMES, Università della Calabria, Italyd Department of Computer Science, Boise State University, USA, Francesca Spezzano d, a r t i c l e i n f oa b s t r a c tArticle history:Received 7 February 2015Received in revised form 21 May 2016Accepted 30 June 2016Available online 7 July 2016Keywords:Social networksDiffusion modelLogic programmingQuantitative logicWe propose Diffusion Centrality (DC) in which semantic aspects of a social network are used to characterize vertices that are influential in diffusing a property p. In contrast to classical centrality measures, diffusion centrality of vertices varies with the property p, and depends on the diffusion model describing how p spreads. We show that DC applies to most known diffusion models including tipping, cascade, and homophilic models. We present a hypergraph-based algorithm (HyperDC) with many optimizations to exactly compute DC. However, HyperDC does not scale well to huge social networks (millions of vertices, tens of millions of edges). For scaling, we develop methods to coarsen a network and propose a heuristic algorithm called “Coarsened Back and Forth” (CBAF) to compute the top-k vertices (having the highest diffusion centrality). We report on experiments comparing DC with classical centrality measures in terms of runtime and the “spread” achieved by the k most central vertices (using 7 real-world social networks and 3 different diffusion models). Our experiments show that DC produces higher quality results and is comparable to several centrality measures in terms of runtime.© 2016 Elsevier B.V. All rights reserved.1. IntroductionAn increasingly important problem in social networks (SNs) is that of assigning a “centrality” value to vertices which will reflect their importance within the SN. Well-known measures such as degree centrality [21,46], betweenness centrality [8,20], PageRank [9], closeness centrality [49,5], and eigenvector centrality [7] only take the structure of the network into account—they do not differentiate between vertices that are central w.r.t. spreading one topic or meme or sentiment vs. spreading another. A vertex that is important in spreading awareness of a mobile phone program may be very unimportant in spread-ing information about a restaurant. Likewise, most past work assumes that there is no information about properties of the vertices/edges or edge weights, but in modern social networks, at least some self-declared properties exist and, in many cases, analysis of tweets and posts can provide further information. These omissions can cause different problems as shown in the following toy example.* Corresponding author.E-mail address: cmolinaro@dimes.unical.it (C. Molinaro).http://dx.doi.org/10.1016/j.artint.2016.06.0080004-3702/© 2016 Elsevier B.V. All rights reserved.C. Kang et al. / Artificial Intelligence 239 (2016) 70–9671Fig. 1. A small HIV social network. Shaded vertices denote people with HIV.Example 1 (HIV). Fig. 1 shows four people a, b, c, d, where b has HIV. Solid edges denote sexual relationships, while dashed edges denote friend relationships. Both “friend” and “sexual partner” relationships can play a role in the diffusion of HIV (as friends may, unbeknownst to us, also be sexual partners). Edge weights denote the intensity of the relationships.The table below shows the centrality of a, b, c, d w.r.t. various centrality measures. Notice that the nature of the rela-tionships (i.e., friend and sexual partner) are not taken into account by these centrality measures, as they consider only the topological structure of the network.Centrality measureDegreeBetweennessPageRankClosenessEigenvectora120.3670.330.375b0.3300.1410.20.125c0.6600.2460.250.25d0.6600.2460.250.25The only person in this network capable of spreading HIV is b. However, b has the lowest centrality according to all five centrality measures mentioned above.Example 2. Consider again the same network shown in Fig. 1 and suppose the vertices denoted users on Twitter. A solid edge (u, v) denotes the fact that u and v have both retweeted at least one of the other’s tweets, while a dashed edge indicates that they are friends (i.e., u follows v and vice versa). Suppose b was the only person to have tweeted a positive opinion about a political candidate while none of the others have done so. Then, by the same reasoning as in the previous example, and given that Fig. 1 is the entire network, we can again infer that any other user (i.e., a, c, d) who tweets positively about the same candidate was either influenced by b or was influenced by some exogenous process. b should clearly get more credit for the other user’s positive tweet than anyone else, but has the lowest centrality according to classical centrality measures.Past centrality measures do not take into account (i) the property of interest w.r.t. which a vertex’s “importance” is measured, (ii) how properties (e.g., HIV in the example above) diffuse through the SN, and (iii) any semantic aspect of the network (properties of vertices and edges), solely focusing on the topological structure. Taking all of these aspects into account is crucial in determining the most central vertices. We can readily think of networks (e.g., Twitter) where person A has the highest centrality in terms of spread of support for Republicans, while person B is the central player in terms of spread of support for conserving gorillas. The network in both cases is the same (Twitter), but the most “central” person depends on the diffusive property with respect to which a vertex is considered “influential” or “central”. Taking into account diffusion models (e.g., how one person influences another) is another crucial aspect. Different ways of spreading a property (e.g., a disease) may lead to different central vertices. Furthermore, intrinsic properties of vertices (customers, patients) and the nature and strength of the relationships (edges) are important too. For instance, [45] talks about the role of nine different demographic factors in influencing online purchases across 14 product categories, showing that some demographic factors are relevant for some product types, while others are relevant for others. This paper proposes the novel notion of diffusion centrality that takes an SN, a diffusive property p, and a previously learned diffusion model (cid:2) for p, and defines centrality of vertices based on this input. We do not provide algorithms to automatically learn diffusion models—interested readers may find one such algorithm in [10].The paper’s goal is to show how diffusion centrality can be used to achieve higher spread of a diffusive property p by using diffusion models for p rather than classical centrality measures. We further show that this can be done for most diffusion models we have seen in the literature. Last but not least, our methods are shown to scale to social networks with over 2M vertices and 20M edges.Real-world diffusion models fall into three diverse categories. In cascade models, there is some probability that a vertex will spread a diffusive property p to one of its neighbors [34,36]—these include popular disease spread models such as the SIR model of disease spread [26]. Tipping models use other mathematical calculations such as cost-benefit analysis, often involving no probabilities, in order to decide whether a vertex will adopt a certain behavior. Tipping models were introduced by Nobel laureate Tom Schelling [50] to model segregation of neighborhoods, and by Granovetter [24]. In homophilic models, similarities between two vertices are considered in order to decide if the two vertices will adopt a similar behavior [42,12,3]. 72C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Homophilic models use various types of distance measures between attributes of a vertex (e.g., age, occupation, gender) and combine them via non-probabilistic measures to achieve a degree of similarity between two vertices.Because diffusion models vary dramatically, a paradigm to express them must be capable of: (i) expressing semantic properties of vertices and connections between them, (ii) representing probabilistic inferences, and (iii) expressing inferences based on non-probabilistic, quantitative reasoning. The suite of knowledge representation paradigms offers several starting points. Bayesian nets and causal inference [47] offer an obvious place to start as they can be used to express cascade models. However, it is not clear if they can be used to express generic quantitative information, which is needed to express many other real diffusion models, such as tipping and homophilic models. In contrast, the language of Generalized Annotated (logic) Programs (GAPs) [35] has been well-studied in knowledge representation and is rich enough to capture a wide variety of different forms of reasoning. It can represent both the structure of social networks (with semantics and weight annotations) as well as these diverse types of diffusion models. Indeed, as shown in [51], many existing diffusion models from all three aforementioned categories can be expressed as GAPs, including: the Susceptible–Infectious–Removed (SIR) [2]and Susceptible–Infectious–Susceptible (SIS) [26] models of disease spread; the Big Seed marketing approach [57], which is a strategy of advertising to a large group of individuals who are likely to spread the advertisement further through network effects; models of diffusion of “favorited” pictures in Flickr [13]; tipping models like the Jackson–Yariv model [28]. We also considered richer versions of GAPs, such as hybrid knowledge bases [40], but decided they were far more expressive than needed for reasoning about diffusive processes.The paper is organized as follows. Related work is discussed in Section 2. Section 3 introduces basic notions used in the rest of the paper. We define Diffusion Centrality (DC) in Section 4. Section 5 proposes a general “hypergraph fixed point al-gorithm” to efficiently compute the likelihood that an arbitrary vertex has certain properties according to the GAP diffusion model. We also define novel classes of GAPs, together with novel optimization methods to develop the HyperDC algorithm for computing DC. In Section 6, we propose the CBAF algorithm for finding the vertices with the top-k highest diffusion cen-tralities in an approximate way. Section 7 describes extensive experiments comparing DC with classical centrality measures in terms of both runtime and the “spread” generated by central vertices.2. Related workSeveral centrality measures have been proposed in graph theory and social network analysis: degree centrality [21,46], betweenness centrality [8,20], PageRank [9], closeness centrality [49,5], and eigenvector centrality [7]. Variants of such centrality measures have been proposed in [18,1,25], while game-theoretic centrality measures have been proposed in [27,52]. These centrality measures take into account only the network topology for determining vertex centrality, while DC considers additional information: the “semantics” embedded in the network, the diffusive property with respect to which a vertex is considered “influential” or “central”, and the model describing how such a property propagates.There has been extensive work in reasoning about diffusion in social networks. One well-known problem is influence maximization, that is, the problem of identifying a subset of vertices in a social network that maximizes the spread of influ-ence. The problem was formulated as an optimization problem in the seminal paper [33], which focuses on two propagation models: the independent cascade and the linear threshold models. Subsequently, several approaches have been proposed to efficiently solve the problem [39,15,29,16,23,56,37]. All the aforementioned approaches consider restricted diffusion models and no vertex/edge properties are taken into account. In contrast, our approach deals with very general diffusion mod-els and takes social network semantic properties into account. Being able to accurately model the diffusion processes and incorporate the network semantics is fundamental for correct analysis of real-world diffusion phenomena.Another well-studied related problem is the target set selection problem [14,17], which assumes a deterministic tipping model and seeks to find a set of vertices of a certain size that optimizes the final number of adopters. These approaches focus on specific diffusion models and neglect networks’ semantics.The notion of diffusion centrality was first proposed in [31]. This paper extends [31] in different respects. In [31], diffu-sion models are expressed using simple conditional probability rules, while in this paper we use the more general language of GAPs. The algorithms introduced in this paper are more general and efficient, and exploit several new optimizations in-troduced in this paper. Moreover, we have addressed the new problem of finding an approximate set of top-k vertices with the highest diffusion centrality and proposed efficient algorithms to solve it.Recently, after our paper on diffusion centrality [31], a few pieces of work have observed that no individual can be a universal influencer, and influential members of the network tend to be influential only in one or some specific domains of knowledge [4,11,53]. This has led to the extension of the classic independent cascade and linear threshold models to be “topic-aware” [4,11], thus considering propagation with respect to a particular topic. Still, the diffusion models considered by these works are limited, as well as the characteristics of vertices and edges.Recently, [48] has addressed the problem of coarsening a social network (that is, finding a more succinct representation of it by grouping vertices together) while preserving its propagation characteristics as much as possible. It works well for the independent cascade model and considers only the structure of the network for merging. The idea of coarsening a network has been extensively used in community detection techniques [32]. However, they use different metrics for coarsening, such as cut-based, flow-based, or heavy-edge matching-based conditions. Methods to compress weighted graphs into smaller ones have been proposed in [54,55], where nodes and edges are grouped into “supernodes” and “superedges”. No semantic properties of vertices/edges or diffusion models are considered. In our CBAF algorithm, when coarsening networks, we C. Kang et al. / Artificial Intelligence 239 (2016) 70–9673explicitly consider a given diffusion model and merge vertices that have the same role in the process, considering the semantic aspects of the SN. Another related problem is graph sparsification, which relies on the notion of “spanners” [22,41]. The main difference is that graph sparsification removes edges (so the nodes stay the same), while we contract the graph trying to preserve the behavior w.r.t. a given diffusion model. [43] uses probabilistic soft logic to develop graph summarization techniques for grouping similar entities (vertices) and relations (edges) by considering the semantic aspects of networks. The approach does not consider any diffusion process explicitly.3. PreliminariesIn this section, we define social networks (SNs), illustrate generalized annotated programs (GAPs) from [35], and show how diffusion models can be expressed with GAPs. We refer the reader to [35] for more details on GAPs.We model SNs as weighted directed graphs where properties can be assigned to vertices and edges. More specifically, properties of vertices and edges are taken from two (disjoint) sets VP and EP, respectively. For instance, a property in VPmight be hiv (meaning that a vertex has HIV), while properties in EP might be fr and sp, representing friend and sexual relationships, respectively—e.g., if an edge is assigned property fr, then its endpoints are friends.A social network (SN) is a tuple (V , E, VL, ω) where:1. V is a finite set of vertices;2. E ⊆ V × V × EP is a finite set of (labeled) edges;3. VL : V → 2VP assigns a set of properties to each vertex;4. ω : E → (0, 1] assigns a weight to each edge.Thus, an SN is a directed graph where VL assigns a set of properties to each vertex, and there can be multiple labeled edges between a given pair of vertices, each of which is associated with a weight and a unique edge property. An SN is depicted in Fig. 1, where property hiv is assigned to vertex b, sp is assigned to solid edges, and fr is assigned to dashed edges.Below we briefly recall GAPs. We start with an example that illustrates a GAP modeling the spread of HIV in SNs like the one in Fig. 1.Example 3. A GAP (cid:2)hi v for the SN of Example 1 might be:(cid:6)) : Y ∧ hiv(V(cid:6)[r1] hiv(V ) : 0.9 × X × Y ← sp(V , V[r2] hiv(V ) : 0.4 × X × Y × Y[r3] hiv(V ) : 0.6 × X × Y × Y(cid:6) ← fr(V , V(cid:6) ← sp(V , VThe first rule says that the confidence that a vertex V has HIV, given that a partner Vhas HIV with confidence X , is 0.9 × X × Y , where Y is the weight of the sexual relationship between the two vertices. The other rules can be similarly read.) : Y ∧ sp(V(cid:6)) : Y ∧ sp(V(cid:6) ∧ hiv(V(cid:6) ∧ hiv(V) : X(cid:6)(cid:6)) : X) : Y(cid:6)(cid:6)) : Y, V(cid:6), V(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)) : X(cid:6)We will treat properties in VP as unary predicate symbols, called vertex predicate symbols, and properties in EP as binary predicate symbols, called edge predicate symbols. Given a vertex (resp. edge) predicate symbol p, then p(t) (resp. p(t1, t2)) is a vertex atom (resp. edge atom), where t, t1, t2 are variables or constants representing vertices. For instance, in Example 3above, hiv(V ) and sp(V , V(cid:6)) are a vertex atom and an edge atom, respectively.An (edge or vertex) annotated atom is of the form A : μ, where A is an (edge or vertex) atom, and μ is an annotation term, that is, an expression built from functions, variables (distinct from those used inside atoms), and real numbers in [0, 1]. For instance, in Example 3, hiv(V ) : 0.9 × X × Y is an annotated atom.A GAP-rule (or simply rule) is of the form A0 : μ0 ← A1 : μ1 ∧ . . . ∧ An : μn where n ≥ 0 and every Ai : μi is an annotated atom. A0 : μ0 is the head of the rule, while A1 : μ1 ∧ . . . ∧ An : μn is the body of the rule. A generalized annotated program(GAP) is a finite set of rules.We assume that VP contains a distinguished vertex predicate symbol vertex that represents the presence of a vertex in an SN. Every SN S = (V , E, VL, ω) can be represented by a GAP, denoted (cid:2)S , as follows:(cid:2)S = {vertex(v) : 1 | v ∈ V } ∪ {p(v) : 1 | v ∈ V ∧ p ∈ VL(v)} ∪ {ep(v 1, v 2) : ω((cid:11)v 1, v 2, ep(cid:12)) | (cid:11)v 1, v 2, ep(cid:12) ∈ E}When we augment (cid:2)S with rules describing how certain properties diffuse through the social network, we get a GAP (cid:2) ⊇ (cid:2)S that captures both the structure of the SN and the diffusion principles. In this paper we consider a restricted class of GAPs: every rule with a non-empty body has a vertex annotated atom in the head ([35] allows any annotated atom in the head of a rule). Thus, edge atoms can appear only in rule bodies or rules with an empty body. This restriction results from the set of diffusion models we consider in this paper: neither edge weights nor edge labels change as the result of the diffusion. However, most of the techniques developed in this paper can be directly applied to or easily generalized for unrestricted GAPs.74C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Table 1Iterations of T(cid:2).Iteration of T(cid:2)01234hiv(a)000.090.090.09hiv(b)01111hiv(c)000.0060.00810.0081hiv(d)000.0320.0320.032An (annotated) atom (resp. rule, GAP) is ground iff it contains no variables (neither inside atoms nor in annotation terms). We use A to denote the set of all ground atoms. Moreover, grd(r) denotes the ground instances of a rule r, i.e. the set of all rules obtained from r by replacing every occurrence of a variable in an annotation term with a real number in [0, 1], and every occurrence of a variable inside an atom with a vertex. Given a GAP (cid:2), we define grd((cid:2)) =We now briefly recall the semantics of GAPs. An interpretation I is a mapping from the set of all ground atoms A to [0, 1]. We say that I satisfies a ground annotated atom A : μ, denoted I |= A : μ, iff I( A) ≥ μ. [35] associates an operator T(cid:2)that maps interpretations to interpretations with any GAP (cid:2). Suppose I is an interpretation. Then,r∈(cid:2) grd(r).(cid:2)T(cid:2)(I)( A) = max({I( A)} ∪ {μ | A : μ ← A A1 ∧ . . . ∧ A An is in grd((cid:2)) and for all 1 ≤ i ≤ n, I |= A Ai})Roughly speaking, the semantics of GAPs requires that when there are multiple ground instances of GAP-rules with the same head that “fires”, the highest annotation in any of these ground rules is assigned to the head atom. [35] shows that if we start from the interpretation that assigns 0 to every ground atom and iteratively apply T(cid:2), then we reach a least fixed point, denoted lfp((cid:2)), which captures the ground atomic logical consequences of (cid:2).Example 4. Consider the social network S of Example 1 and the GAP (cid:2)hi v of Example 3. Let (cid:2) be the GAP modeling both the SN and the diffusion model, i.e., (cid:2) = (cid:2)S ∪ (cid:2)hi v . Then, T(cid:2) reaches a least fixed point at the third iteration, as shown in Table 1. Edge atoms are not reported in the table, as their annotations and weights do not change by applying T(cid:2) (because edge atoms do not appear in rule heads). Specifically, 0.1 is assigned to sp(a, b), sp(b, a), sp(a, c), and sp(c, a), 0.8 is assigned to fr(a, d) and fr(d, a), and 0.7 is assigned to fr(c, d) and fr(d, c).Initially, 0 is assigned to all ground atoms (iteration 0). Next, 1 is assigned to the ground atom hiv(b), because b has HIV in the original SN (iteration 1). From now on, rules in (cid:2)hi v can be used to assign higher values to (vertex) ground atoms. Specifically, at iteration 2, the following rules can “fire”:[r[r[r(cid:6)1(cid:6)2(cid:6)3] hiv(a) : 0.9 × 1 × 0.1 ← sp(a, b) : 0.1 ∧ hiv(b) : 1] hiv(d) : 0.4 × 1 × 0.8 × 0.1 ← fr(d, a) : 0.8 ∧ sp(a, b) : 0.1 ∧ hiv(b) : 1] hiv(c) : 0.6 × 1 × 0.1 × 0.1 ← sp(c, a) : 0.1 ∧ sp(a, b) : 0.1 ∧ hiv(b) : 1and are used to assign 0.09 to hiv(a), 0.032 to hiv(d), and 0.006 to hiv(c).At the subsequent iteration (iteration 3), the following rule can fire and is used to assign a higher value to hiv(c), namely 0.0081:[r(cid:6)(cid:6)1] hiv(c) : 0.9 × 0.09 × 0.1 ← sp(c, a) : 0.1 ∧ hiv(a) : 0.09No higher values can be derived from a further application of T(cid:2), and thus the least fixed point is reached, assigning 0.09 to hiv(a), 1 to hiv(b), 0.0081 to hiv(c), and 0.032 to hiv(d).Example 5. Suppose the SN in Fig. 1 represents Cell phone users, all edges have property fr representing friendship relations, and vertices have properties like male, female, young, old, and adopter, telling us if the user adopted a cell phone plan. The phone company wants to identify important users. Suppose d is male and everyone else is female; initially nobody is an adopter. A cell phone provider may have a diffusion rule learned from past promotions:adopter(V(cid:6)) : 0.6 × X × Y ← adopter(V ) : X ∧ male(V ) : Y ∧ fr(V , V(cid:6)) : 0.1The vertex which has the greatest influence, if given a free mobile phone plan and if the above diffusion model is used, is clearly d (because this is the only vertex that can “influence” others to adopt the plan). However, we see from the table in Example 1 that d is not the most relevant vertex w.r.t. to all centrality measures.4. Diffusion centralityDiffusion centrality tries to measure how well a vertex v can diffuse a property p (e.g., the hiv property). Given an SN S = (V , E, VL, ω), a vertex predicate symbol p, and a vertex v ∈ V , the insertion of p(v) into S, denoted S ⊕ p(v), is (cid:6)(v) = VL(v) ∪ {p}. In other words, inserting p(v) into the SN (V , E, VLis exactly like VL except that VL(cid:6)(cid:6), ω) where VLC. Kang et al. / Artificial Intelligence 239 (2016) 70–9675a social network merely says that vertex v has property p and that everything else about the network stays the same. (cid:6)(cid:6), ω) which is just like S except Likewise, the removal of p(v) from S, denoted S (cid:16) p(v), is the social network (V , E, VLthat VL(cid:6)(cid:6)(v) = VL(v) \ {p}.Definition 1 (Diffusion centrality). Let S = (V , E, VL, ω) be an SN, (cid:2) a GAP, and p a property. The diffusion centrality (DC for short) of a vertex v ∈ V w.r.t. (cid:2), p, and S, denoted dc(cid:2),p,S (v), is defined as follows:(cid:3)v(cid:6)∈V \{v}lfp((cid:2) ∪ (cid:2)S⊕p(v))(p(v(cid:6))) −(cid:3)v(cid:6)(cid:6)∈V \{v}lfp((cid:2) ∪ (cid:2)S(cid:16)p(v))(p(v(cid:6)(cid:6)))Whenever (cid:2), p, and S are clear from the context, we denote the diffusion centrality of a vertex v simply as dc(v).This definition says that computing the diffusion centrality of vertex v involves two steps. First, we assume that vertex vhas property p and see how much diffusion occurs. This is done by computing the least fixed point of the diffusion model and the SN S ⊕ p(v) (i.e., the original SN where p is assigned to v). Notice that the overall diffusion is quantified by (cid:6) (cid:17)= v of S. Then, summing up the values that the least fixed point assigns to atoms of the form p(vwe assume that vertex v does not have property p and see how much diffusion occurs. This is done by computing the least fixed point of the diffusion model and the SN S (cid:16) p(v) (i.e., the original SN where p is not assigned to v). The diffusion centrality of v is the difference between the above two numbers and captures the “impact” that would occur in terms of diffusion of property p if vertex v had property p.1(cid:6)) across all vertices vNotice that the least fixed point of (cid:2) and S ⊕ p(v) (or S (cid:16) p(v)) takes into account the initial assignment of p to the vertices of S. Thus, DC depends also on the initial assignment of p in S. For instance, consider two SNs that are identical except that in the first one every vertex has property p, while in the second one no vertex has property p. In the first SN, dc(v) = 0 for every vertex v, because whether p is given to v or not has no impact, since everyone already has p. In contrast, in the second SN, dc(v) reflects how much overall spread of p we achieve in the SN by assigning p to v.Example 6. Consider again the HIV SN of Example 1 and the GAP of Example 3. Recall that the only vertex with property hiv is b. It can be easily verified that the values for the positive and negative summands of Definition 1 for all vertices are as reported in the following table.Positive summandNegative summandDiffusion centralitya1.1221.04010.0819b0.1300.13c1.1221.1220d1.09811.09810For instance, consider vertex a. If we assume that hiv is given to a in the original SN, then we get an overall spread of hiv of 1.122 (positive summand of Definition 1). If we assume that hiv is not given to a in the original SN, then we get an overall spread of hiv of 1.0401 (negative summand of Definition 1). Then, the diffusion centrality of a is the difference of these two values. The same argument can be applied to the remaining vertices. It is worth noting that if hiv is not assigned to b in the original SN, then no spread of hiv occurs (the negative summand for b is 0), because nobody else has HIV in the SN.Thus, b has the highest centrality w.r.t. hiv and (cid:2)hi v —classical centrality measures (Example 1) do not capture this because b is not a “central” vertex from a purely topological perspective. However, b should have the highest centrality because it is the only one with HIV. Vertices c and d do not increase the confidence of any vertex to have HIV. So their diffusion centrality is zero.Example 7. If we return to the cell phone case (Example 5), we see that the DC of d is 1.2, while all other vertices have 0 as their DC. Furthermore, as opposed to classical centrality metrics, c and d do not have the same centrality, because their properties and the diffusion of interest make them important to a different extent.Diffusion Centrality Problem (DCP). Given an SN S = (V , E, VL, ω), a GAP (cid:2), and a property p, the diffusion centrality problem consists of finding the DC (w.r.t. (cid:2), p, and S) of every vertex of S.Top-k Diffusion Centrality Problem (kDCP). Given a 0 < k < |V |, the top-k diffusion centrality problem consists of finding a set T of k vertices of S having the highest DC, that is, the DC of every vertex in T is greater than or equal to the DC of every vertex in V \ T .1 Considering just the first summation of Definition 1 is wrong. Suppose we have an SN and a vertex v s.t. the first summation of dc(v) is a high number N (i.e., N is the expected number of vertices with property p assuming that v has property p). Suppose that when we assume that v does not have property p, the same value N is determined (i.e., the second summation equals N). Then, intuitively, v should not have a high diffusion centrality as the expected number of vertices with property p is the same regardless of whether v has property p or not (thus, v does not play a central role in diffusing p). In contrast, considering just the first summation would give v a high centrality.76C. Kang et al. / Artificial Intelligence 239 (2016) 70–965. The HyperDC algorithm for exact diffusion centrality computationIn this section, we present the HyperDC algorithm (Section 5.3), which solves DCP exactly using the HyperLFP algorithm (Section 5.2) to compute the least fixed point of a GAP. We start with a set of optimizations that can speed up HyperDC.5.1. Optimization stepsWe first present optimizations that can be applied to arbitrary GAPs, and then identify two subclasses of GAPs for which DC can be computed even more efficiently. Before that, we introduce notation and terminology used in the following.The dependency graph of a GAP (cid:2) is a directed graph dep((cid:2)) whose vertices are the predicate symbols in (cid:2). There is an edge from a predicate symbol q to a predicate symbol p iff there is a rule in (cid:2) where p occurs in the head and q occurs in the body. We say that p depends on q if there exists a path from q to p in dep((cid:2)). If p depends on q and vice versa, then we say that p and q are mutually recursive. We use M(cid:2),p to denote the set of all predicate symbols that are mutually recursive with p. We define R(cid:2),p as the set of predicate symbols q such that (i) p depends on q, and (ii) q appears in the head of some rule of (cid:2). Note that M(cid:2),p ⊆ R(cid:2),p . Given a GAP (cid:2) and a property p, we define(cid:2)p = {r ∈ (cid:2) | p is the head predicate symbol of r, or p depends on the head predicate symbol of r}.We are now ready to introduce our first optimization.Caching lfp When computing dc(v), we note that S, S ⊕ p(v), and S (cid:16) p(v) differ only in whether or not vertex v has property p. One way to leverage this is to first compute and cache lfp((cid:2) ∪ (cid:2)S ) independent of v. We then only need to calculate one summation in order to compute dc(v).Proposition 1. Consider a social network S = (V , E, VL, ω), a GAP (cid:2), and a property p. Let φ = lfp((cid:2) ∪ (cid:2)S ) and v be a vertex in V . Then,dc(v) =⎧⎪⎪⎨⎪⎪⎩(cid:8)v(cid:6)∈V \{v}(cid:8)v(cid:6)∈V \{v}φ(p(v(cid:6))) −(cid:8)v(cid:6)(cid:6)∈V \{v}lfp((cid:2) ∪ (cid:2)S⊕p(v))(p(v(cid:6))) −lfp((cid:2) ∪ (cid:2)S(cid:16)p(v))(p(v(cid:8)φ(p(vv(cid:6)(cid:6)∈V \{v}(cid:6)(cid:6)))(cid:6)(cid:6)))if p ∈ VL(v)if p /∈ VL(v)Proof. Consider a vertex v ∈ V . If v has property p, then, by definition of S ⊕ p(v), we have that φ = lfp((cid:2) ∪ (cid:2)S⊕p(v))and the first equation holds. Likewise, if v does not have property p, then, by definition of S (cid:16) p(v), we have that φ =lfp((cid:2) ∪ (cid:2)S(cid:16)p(v)), and thus the second equation follows too. (cid:2)Network filtering We now present an optimization, called network filtering, which consists of reducing the given SN by removing vertices (and the relative incoming and outgoing edges) that do not play any role in the diffusion process, i.e., those vertices that can never receive or transmit diffusive property p from/to other vertices in the SN via any rule in the considered diffusion model. As shown in the following, network filtering is a sound optimization technique, that is, its aim is to reduce the size of the SN (so as to make the DC computation faster) without altering the DC values. Specifically, removed vertices have zero DC in the original SN, while the DC of the remaining vertices in the reduced SN is the same as in the original one (cf. Proposition 2).Example 8. Consider the diffusion model (cid:2)hi v from Example 3:hiv(V ) : 0.9 × X × W ← sp(V , Vhiv(V ) : 0.4 × X × W × Whiv(V ) : 0.6 × X × W × W(cid:6) ← fr(V , V(cid:6) ← sp(V , V(cid:6)) : X(cid:6)) : W ∧ sp(V(cid:6)) : W ∧ sp(V) : W ∧ hiv(V(cid:6)(cid:6)(cid:6)(cid:6), V(cid:6), V(cid:6)(cid:6)) : W(cid:6)(cid:6)) : W(cid:6) ∧ hiv(V(cid:6) ∧ hiv(V) : X(cid:6)(cid:6)) : XSuppose v is a vertex with no sp relations (in a given SN). Moreover, suppose none of v’s friends have sp relations. Then, v is an “unnecessary” vertex (for the purpose of computing DC) because there is no rule r ∈ (cid:2)hi v by which vertex v can receive property hiv or transmit hiv to other vertices. In fact, it is easy to see from the rules above that a vertex cannot transmit hiv if it does not have sp relations. Moreover, a vertex cannot get hiv if it does not have sp relations (the first and third rules cannot be applied) and its friends have no sp relations (the second rule cannot be applied).Thus, roughly speaking, network filtering matches diffusion rules against vertices to identify vertices that do not partici-pate in the activation of any rule, regardless of how many properties are involved in the diffusion rules and in the SN.C. Kang et al. / Artificial Intelligence 239 (2016) 70–9677Definition 2 (Rule activation). Given a GAP (cid:2), a property p, and a rule r ∈ (cid:2), we define relbody(r) = { A A | A A is an annotated atom in the body of r and its predicate symbol is not in R(cid:2),p}. Vertex v of an SN S activates a rule r ∈ (cid:2) iff there exists a ground rule r(cid:6) ∈ grd(r) such that:1. for every A A ∈ relbody(r(cid:6)2. if A : μ is the head of r3. v appears in an annotated atom of the body of r(cid:6)), (cid:2)S |= A A;, then μ > 0; and(cid:6).Definition 3 (Necessary and unnecessary vertices). Let S be an SN, (cid:2) be a GAP, and p a property. A vertex of S is necessary if it activates a rule of (cid:2)p , otherwise it is unnecessary.Roughly speaking, a necessary vertex is one that might “trigger” some rule during the least fixed point computation, while unnecessary vertices have no chance of being involved in the least fixed point computation. Identifying necessary vertices is similar in spirt to the identification of relevant rules in probabilistic logic programs, where the aim is to find ground rules that are relevant for the given query [19]. In our case, we are interested in the values assigned to ground atoms of the form p(v), which are in a sense the query atoms—here p is the diffusive property. However, rather than getting rid of ground rules, we get rid of vertices (i.e., constants) that are “irrelevant”. This is important in our applications where the SN may have millions of vertices and disregarding the irrelevant ones can yield significantly better performances. Of course, disregarding some vertices lead to disregarding some ground rules as well (i.e., those ground rules containing at least one irrelevant vertex). The filtering of a social network eliminates all unnecessary vertices (along with their incoming/outgoing edges).Definition 4 (Network filtering). Let S = (V , E, VL, ω) be an SN, (cid:2) a GAP, p a property, and U the set of unnecessary vertices. The filtering of S (w.r.t. (cid:2) and p) is the SN S (cid:6) = (V(cid:6), ω(cid:6)) where:(cid:6), VL(cid:6), E1. V2. E3. VL4. ω(cid:6)(e) = ω(e) for all e ∈ E(cid:6) = V \ U ;(cid:6) = E \ {(cid:11)u, v, q(cid:12) ∈ E | u ∈ U ∨ v ∈ U };(cid:6)(v) = VL(v) for all v ∈ V.;(cid:6)(cid:6)The filtering S(cid:6)of an SN S is useful because unnecessary vertices have a DC of zero in S, while the DC of necessary vertices can be computed on the (smaller) SN S (cid:6)in a sound way, that is, their DC in S (cid:6)is the same as in S.Proposition 2. Let S be an SN, (cid:2) be a GAP, p a property, and S (cid:6)dc(cid:2),p,S (v) = 0, otherwise (v is necessary) dc(cid:2),p,S (v) = dc(cid:2),p,S(cid:6) (v).the filtering of S. For every vertex v of S, if v is unnecessary, then Proof. If a vertex v is unnecessary, it cannot activate any rule in (cid:2), thus, independent of whether or not it has the diffusion property p, it does contribute to diffusing p to other vertices (see item 2 of Definition 2). It follows that(cid:8)(cid:8)v(cid:6)∈V \{v} lfp((cid:2) ∪ (cid:2)S⊕p(v))(p(vv(cid:6)∈V \{v} lfp((cid:2) ∪ (cid:2)S )(p(v(cid:8)(cid:6))) =v(cid:6)∈V \{v} lfp((cid:2) ∪ (cid:2)S(cid:16)p(v))(p(v(cid:6))) =(cid:6))), and then dc(cid:2),p,S (v) = 0.If a vertex v is necessary, all the rules it activates contain necessary vertices, and then we have that(cid:8)(cid:8)v(cid:6)∈V \{v} lfp((cid:2) ∪ (cid:2)S⊕p(v))(p(vv(cid:6)∈V \{v} lfp((cid:2) ∪ (cid:2)S(cid:16)p(v))(p(v(cid:6))) =(cid:6))) =(cid:8)(cid:8)v(cid:6)∈V \{v} lfp((cid:2) ∪ (cid:2)S(cid:6)⊕p(v))(p(vv(cid:6)∈V \{v} lfp((cid:2) ∪ (cid:2)S(cid:6)(cid:16)p(v))(p(v(cid:6))),(cid:6))).It follows that dc(cid:2),p,S (v) = dc(cid:2),p,S(cid:6) (v). (cid:2)Subclasses of GAPs We now introduce p-monotonic GAPs, a class of GAPs to which we can apply further optimizations (in addition to those discussed so far).Definition 5 (p-monotonic GAP). We say that a rule A0 : μ0 ← A1 : μ1 ∧ . . . ∧ An : μn is monotonic iff μ0 is a monotonic function.2 Given a GAP (cid:2) and a property p, we say that (cid:2) is p-monotonic iff, for every rule r in (cid:2), when the head predicate symbol is in R(cid:2),p then r is monotonic.2 If μ0 is a constant, it is considered to be monotonic. Moreover, if μ0 = f (. . .), then μ0 is monotonic if fthen f (x1, . . . , xn) ≤ f ( y1, . . . , yn), where n is the number of arguments of f .is monotonic, i.e., if xi ≤ yi for all 1 ≤ i ≤ n, 78C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Example 9. Consider the GAP (cid:2) of Example 3, for which R(cid:2),hiv = {hiv}. To see if (cid:2) is hiv-monotonic we need to check if for every rule in (cid:2) having an annotated atom of the form hiv(V ) : μ in the head, we have that μ is a monotonic function. Since this is the case, then (cid:2) is hiv-monotonic. The following GAPp(V ) : 0.5 × X × W ← fr(V , Vq(V ) : (1 − X) × W ← fr(V , Vp(V ) : 0.9 × X × W ← fr(V , V(cid:6)(cid:6)) : W ∧ p(V) : W ∧ p(V(cid:6)) : W ∧ q(V(cid:6)) : X) : X) : X(cid:6)(cid:6)is not p-monotonic because R(cid:2),p = {p, q} and the annotation of the head atom of the second rule is a non-monotonic function.Given a GAP (cid:2) and a property p, we define the p-interfered predicate set as follows:(cid:9)I(cid:2),p =M(cid:2),pR(cid:2),pif (cid:2) is p-monotonicif (cid:2) is not p-monotonicThus, the p-interfered predicate set contains all the predicate symbols that are mutually recursive with p if the GAP (cid:2)is p-monotonic, while, if (cid:2) is not p-monotonic, it is the set of predicate symbols q such that p depends on q and q appears in the head of some rule in (cid:2). For instance, I(cid:2),hiv = {hiv} for the GAP of Example 3, while I(cid:2),p = {p, q} for the GAP of Example 9. Notice that when I(cid:2),p = R(cid:2),p , then I(cid:2),p is bigger as M(cid:2),p ⊆ R(cid:2),p .Recall that, given a GAP (cid:2) and a property p, then (cid:2)p is the set of rules r in (cid:2) such that either p depends on the head predicate symbol of r or p is the head predicate symbol. We also define∗(cid:2)p= {r ∈ (cid:2)p | every predicate symbol q in the body of r is s.t. q /∈ I(cid:2),p}.p over the SN, as it does not depend on the remaining rules in (cid:2)p . Then, (cid:2)p \ (cid:2)∗Roughly speaking, rules in (cid:2)p are the only ones that may affect the values assigned to ground atoms of the form p(v)in the least fixed point—so, we can ignore rules in (cid:2) \ (cid:2)p . Moreover, (cid:2)p can be partitioned into two sets: (cid:2)∗p and (cid:2)p \ (cid:2)∗p . We can first evaluate (cid:2)∗p can be evaluated. This somehow reminds of the stratification of logic programs, where a program is partitioned into different “strata”, which are evaluated one at a time according to an order dictated by their dependencies. In our case, we first evaluate a “base stratum” (namely (cid:2)∗p ), and then use its result as a starting point for the evaluation of a stratum consisting of “mutually recursive” rules (namely (cid:2)p \ (cid:2)∗p ). Thus, in our setting, we have two strata, as we are interested in just one “special” predicate p modeling the diffusive property. The following proposition states precisely how (cid:2)p and (cid:2)∗p are exploited (recall that A denotes the set of all ground atoms).Proposition 3. Consider an SN S, a property p, and a GAP (cid:2). Let ψ be the interpretation lfp((cid:2)S ∪ (cid:2)∗lfp(((cid:2)p \ (cid:2)∗p) ∪ { A : ψ( A) | A ∈ A})(p(v)) for every vertex v of S.p). Then, lfp((cid:2) ∪ (cid:2)S )(p(v)) =While the previous proposition can be applied to arbitrary GAPs, it becomes more effective for p-monotonic ones, be-p (which is “precomputed” at an initial stage) is larger. In general, the smaller I(cid:2),p is, the cause I(cid:2),p is smaller and thus (cid:2)∗more effective the proposition above will be.Example 10. Consider the following GAP (cid:2):[r1]s(V ) : 0.5 × X ← p(V ) : X[r2] p(V ) : W × X × Y ← fr(V , V[r3]s(V ) : 0.6 × X × W ← fr(V , V[r4] q(V ) : 0.9 × X × W ← fr(V , V(cid:6)(cid:6)) : Y) : W ∧ p(V ) : X ∧ s(V(cid:6)) : W ∧ q(V) : X(cid:6)(cid:6)) : W ∧ male(V(cid:6)) : XSuppose p is the diffusive property. Clearly, (cid:2) is p-monotonic (all rule heads contain monotonic functions), (cid:2)p = (cid:2), and (cid:2)∗= {r3, r4}, as all predicate symbols in the body of r3 (resp. r4) are not mutually recursive with p. Proposition 3 says pthat, for every SN S, we can first compute the interpretation ψ defined as lfp((cid:2)S ∪ (cid:2)∗p). Then, starting from ψ , the GAP consisting only of r1 and r2 is evaluated.Below we present another class of GAPs, called p-dwindling. As discussed in the next section, for p-dwindling GAPs our HyperLFP algorithm has a faster convergence to the least fixed point.Definition 6 (p-dwindling GAP). Suppose p is a property. A GAP (cid:2) is p-dwindling iff for every ground rule A0 : μ0 ← A1 :μ1 ∧ . . . ∧ An : μn in grd((cid:2)) s.t. q is the predicate symbol of A0 and q ∈ I(cid:2),p , it is the case that μ0 ≤ μi for every 1 ≤ i ≤ ns.t. the predicate symbol of Ai is in I(cid:2),p .C. Kang et al. / Artificial Intelligence 239 (2016) 70–9679For the Flickr, Jackson–Yariv, and SIR models (see Appendix A) we consider in our experimental evaluation, we can state the following properties.Proposition 4. The Flickr model is p-monotonic and p-dwindling. The Jackson–Yariv model is p-monotonic but not p-dwindling. The SIR model is neither p-monotonic nor p-dwindling.5.2. The HyperLFP algorithmIn this section, we propose an efficient hypergraph-based algorithm, HyperLFP, to compute the least fixed point used for diffusion centrality computation.A directed hypergraph is a pair (cid:11)V , H(cid:12) where V is a finite set of vertices and H is a finite set of directed hyperedges. A hyperedge is a pair (cid:11)S, t(cid:12) where S is a set of vertices, called source set, and t is a vertex, called target vertex. Given a hyperedge h ∈ H , S(h) denotes its source set and t(h) denotes its target vertex.We now define a hypergraph that captures how a property p diffuses through an SN S according to a GAP (cid:2). The hypergraph does not depend on which vertices have property p in the original SN, but depends only on (cid:2) and the structure of S in terms of edges and vertex properties other than p. Therefore, given a GAP (cid:2) and an SN S, the diffusion hypergraph has to be computed only once and can be used with different assignments of a property p to the vertices of S. This allows us to save time in computing diffusion centrality which requires computing the least fixed point for different initial assignments of p. However, if the SN changes, the diffusion hypergraph needs to be recomputed. In addition, the hypergraph allows us to eliminate diffusion rules that are useless for computing the least fixed point.Definition 7 (Enabled rule). Consider an SN S, a GAP (cid:2), and a property p. Let ϕ = lfp((cid:2)S ∪ (cid:2)∗enabled iff ϕ( A) ≥ μ for every annotated atom A : μ in the body of r whose predicate symbol is not in I(cid:2),p .p). A rule r ∈ grd((cid:2) − (cid:2)∗p) is Intuitively, enabled rules are the ground rules that can affect the diffusion of p (directly or indirectly) in the least fixed point computation.Example 11. Consider the GAP (cid:2)hi v of Example 3 and the SN of Example 1 (cf. Fig. 1). In this case, we have (cid:2)∗ϕ = lfp((cid:2)S ), and the following ground instance of the second rule belongs to grd((cid:2) − (cid:2)∗pp):= ∅, [r] hiv(d) : 0.4 × 0.8 × 0.1 × 1 ← fr(d, a) : 0.8 ∧ sp(a, c) : 0.1 ∧ hiv(c) : 1The rule above is enabled as ϕ(fr(d, a)) = 0.8 and ϕ(sp(a, c)) = 0.1. Notice that the atom hiv(c) does not play any role in determining whether or not the rule is enabled because hiv ∈ I(cid:2),hiv (recall that I(cid:2),hiv = {hiv}).Definition 8 (Diffusion hypergraph). Consider an SN S = (V , E, VL, ω), a GAP (cid:2), and a property p. The hyperedge associ-(cid:6)(cid:6)(v i) :ated with a ground rule r ∈ grd((cid:2)) whose head annotated atom is of the form p(cid:6)(v)(cid:12) and is denoted by hedge(r). The diffusion hypergraph H(S, (cid:2), p) is a triple μi is in the body of r and p(cid:11)N, H, W (cid:12) such that:(cid:6)(v) : μ is defined as (cid:11){p(cid:6)(cid:6) ∈ I(cid:2),p}, p(cid:6)(cid:6)(v i) | p1. (cid:11)N, H(cid:12) is a directed hypergraph with N = {q(v) | q ∈ I(cid:2),p and v ∈ V }, and H = {hedge(r) | r is an enabled ground rule of(cid:2) whose head predicate symbol is in I(cid:2),p},2. W is a function such that for each h ∈ H and matrix M[1...|I(cid:2),p|][1..|V |] of real values in [0, 1], W (h, M) is the head annotation of the ground rule r satisfying the following two properties: (i) hedge(r) = h, and (ii) for every atom q(v)appearing in S(h), M[q][v] is equal to the annotation of q(v) in r.Example 12. Fig. 2 shows the diffusion hypergraph for the GAP (cid:2)hiv of Example 3 and the social network of Example 1. Since I(cid:2),hiv = {hiv} and V = {a, b, c, d}, the nodes of the diffusion hypergraph are N = {hiv(a), hiv(b), hiv(c), hiv(d)}. The hyperedges are derived from enabled ground rules. Consider for instance the enabled ground rule r from Example 11. This rule corresponds to the hyperedge hedge(r) = (cid:11){hiv(c)}, hiv(d)(cid:12) because {hiv(c)} is the set of atoms in the body of r s.t. the predicate symbols belongs to I(cid:2),hiv and hiv(d) is the atom in the head of the rule. The hyperedge labels represent the function W , i.e. the label on a hyperedge is the function in the annotation of the head atom of the corresponding rule. For instance, for the rule r from Example 11, the label on the hyperedge hedge(r) is 0.4 × (0.8 × 0.1) × hiv(c) (where 0.4 is a constant in the function and the values 0.8 and 0.1 are precomputed) and represents how we have to update the value for hiv(d) once that the value of hiv(c) has changed.The rough idea of the HyperLFP algorithm (Algorithm 1) is that hyperedges that propagate a value greater than zero are kept in a max-heap and those propagating higher values are visited first; the max-heap is updated as propagation unfolds.For all q ∈ I(cid:2),p and v ∈ V , M[q][v] is the initial value of the ground atom q(v), U [q][v] keeps track of the hyperedges (cid:6)[q][v] is initially set to M(cid:6)[q][v] is the current assignment to q(v). Specifically, Mhaving q(v) in their source set, and M80C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Fig. 2. A diffusion hypergraph.Algorithm 1 HyperLFP.Input: For a social network S = (V , E, VL, ω), a GAP (cid:2), and a property p,two matrices U [1...|I(cid:2),p|][1..|V |] and M[1...|I(cid:2),p|][1..|V |],a max-heap Heap, the diffusion hypergraph H(S, (cid:2), p) = (cid:11)N, H, W (cid:12)(cid:6) ← ∅;Heapwhile Heap (cid:17)= ∅ do(cid:11)h, w(cid:12) ← deleteMax(Heap);Let q(v) = t(h);(cid:6)[q][v] < w thenif M(cid:6)[q][v] ← w;Mfor each hOutput: lfp((cid:2) ∪ (cid:2)S )(q(v)) for all v ∈ V and q ∈ I(cid:2),p1: C ← copy of M;(cid:6) ← copy of M;2: M3: while Heap (cid:17)= ∅ do4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20: return M(cid:6) ∈ U [q][v] do(cid:6));(cid:6));(cid:6)] then(cid:6)C[q;if (cid:2) is p-monotonic then(cid:6) ← W (h(cid:6)(v(cid:6) > C[q(cid:6)][v(cid:6), M(cid:6)) = t(h(cid:6)][v(cid:6)] ← wHeap ← Heap(cid:6)Add (cid:11)h(cid:6);wLet qif w(cid:6)(cid:12) to Heap;(cid:6)(cid:12) to HeapAdd (cid:11)h(cid:6), w(cid:6), welse;;(cid:6)and then iteratively updated by the algorithm. C keeps track of the highest values propagated by hyperedges that were added to Heap. At each iteration of the while loop on lines 5–18, a pair (cid:11)h, w(cid:12) with maximum w is retrieved from Heap. (cid:6)[q][v], (cid:6)[q][v] is less than w, it is set to w, otherwise another hyperedge is retrieved from Heap. If w is assigned to MIf Mthen hyperedges that can be affected by this are inspected (for each loop on lines 10–18). Only the hyperedges having t(h)in the source set are inspected. For each of them, if no hyperedge added to Heap propagated a higher value (line 13), then if . The (cid:2) is p-monotonic the hyperedge is added to Heap (along with the value it propagates), otherwise it is added to Heapreason for this is that when (cid:2) is p-monotonic we can retrieve hyperedges in descending order of their weights even if their values were derived from different iterations of T(cid:2). However, if (cid:2) is not p-monotonic, the iterations of T(cid:2) must be executed one after the other without mixing the values derived at each of them. So, when (cid:2) is not p-monotonic, hyperedges are is retrieved in descending order of their weight for each iteration of T(cid:2). When Heap is empty, Heapreturned if both heaps are empty.is assigned to Heap. M(cid:6)(cid:6)(cid:6)If a GAP (cid:2) is p-dwindling and p-monotonic, then HyperLFP ensures that when a value w is assigned to a ground atom q(v), w is the final value for q(v) in the least fixed point; hence the hyperedge that propagated w as well as any other hyperedge having q(v) as a target atom no longer needs to be considered in order to see if a new higher value can be assigned to q(v).Proposition 5. The worst-case time complexity of Algorithm HyperLFP is O (|N| + 1αwhere Umax = maxv∈V ,q∈I(cid:2),p(cid:6)[q][v]).for (w − M· |H| · (log |H| + Umax · (Smax + log |H|))), {|{h | h ∈ H ∧ q(v) ∈ S(h)}|}, Smax = maxh∈H {|S(h)|}, and α is the minimum value obtained at line 9 C. Kang et al. / Artificial Intelligence 239 (2016) 70–9681Algorithm 3 Init.Input: A social network S = (V , E, VL, ω),a p-interfered predicate set I(cid:2),p ,a diffusion hypergraph H = (cid:11)N, H, W (cid:12).Output: M[1..|I(cid:2),p|][1..|V |],U [1..|I(cid:2),p|][1..|V |], Heap1: n = |I(cid:2),p|; m = |V |;2: M[1..n][1..m]; U [1..n][1..m];3: Heap ← ∅;4: for each v ∈ V , q ∈ I(cid:2),p doM[q][v] ← 0, U [q][v] ← ∅;5:6: for each v ∈ V , q ∈ VL(v) ∩ I(cid:2),p do7:8: for each h ∈ H do9:10:11:12: return (cid:11)M, U , Heap(cid:12);Add (cid:11)h, W (h, M)(cid:12) to Heap;for each q(v) ∈ S(h) doU [q][v] ← U [q][v] ∪ {h};M[q][v] ← 1;Algorithm 2 HyperDC.Input: An SN S = (V , E, VL, ω), a GAP (cid:2),a property p, the diffusion hypergraphD = H(S, (cid:2), p) = (cid:11)N, H, W (cid:12)elsev∈V F [p][v];Min[p][v] ← 1;(cid:12) ← Init(S, I(cid:2),p, D);for each h ∈ U [p][v] doHeap ← copy of Heapin ;if p ∈ VL(v) thenMin[p][v] ← 0;Output: {(cid:11)v, dc(v)(cid:12) | v ∈ V }1: (cid:11)Min, U , Heapin2: Heap ← copy of Heapin ;3: F ← HyperLFP(U , Min, Heap, D) ;(cid:8)4: sump ←5: Re sult ← ∅;6: for each v ∈ V do7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26: return Result;remove (cid:11)h, w(cid:12) from Heap;(cid:6)) = t(h);Let pif W (h, Min) > Min[p(cid:6)] thenAdd (cid:11)h, W (h, Min)(cid:12) to Heap;M ← HyperLFP(U , Min, Heap, D);(cid:6)←sumpif p ∈ VL(v) thendc(v) ← (sump − F [p][v]) − sumMin[p][v] ← 1;dc(v) ← sumMin[p][v] ← 0;Add (cid:11)v, dc(v)(cid:12) to Result;v(cid:6)∈V ,v(cid:6)(cid:17)=v M[p][v(cid:6)][v(cid:6)(velse(cid:6)];(cid:8)(cid:6)p− (sump − F [p][v]);(cid:6)p ;5.3. The HyperDC algorithmThe HyperDC algorithm (Algorithm 2) initializes Min, U , and Heapin by calling Algorithm 3 (line 1), and then uses them to compute lfp((cid:2) ∪ (cid:2)S ) (lines 2–4). After that, the diffusion centrality of each vertex is computed (lines 6–45). Specifically, the value of atom p(v) is incorporated into Min, and Heap is updated accordingly (lines 7–16). Then, the least fixed point is computed using the updated Min and Heap (lines 17–18). Finally, the diffusion centrality of vertex v is computed and the (cid:6)), the positive value of p(v) in Min is restored (lines 19–25). In this last step, Proposition 1 is leveraged. In fact, if p ∈ VL(vsummand of the definition of DC has already been computed in lines 2–4 and what is being computed is the negative (cid:6)summand. In this case, the positive summand is equal to sump − F [p][v] and the negative summand is equal to sump , so (cid:6)), the negative summand has been computed (cid:6)the diffusion centrality of vertex v is (sump − F [p][v]) − sump . If p /∈ VL(v(cid:6)− (sump − F [p][v]).already in lines 2–4. In this case, the diffusion centrality of vertex v is sumpProposition 6. The worst-case time complexity of Algorithm HyperDC is O (|V | · (|N| + 1αwhere Umax = maxv∈V {|{h | h ∈ H ∧ v ∈ S(h)}|}, Smax = maxh∈H {|S(h)|}, and α is defined as in Proposition 5.· |H| · (log |H| + Umax · (Smax + log |H|)))), A brief note is in order about how the techniques in this section yield better scalability. Compared to our past work [31], HyperDC is approximately 100 times faster (this is the average speedup obtained in our experimental evaluation). In particular, the U and Heapinit structures used in HyperDC are built just once. This yields a speedup of approximately 5×. In addition, the filtering based on Proposition 3 yields a further speedup of 20×, leading to a total speedup of 100×.6. CBAF algorithm for approximating kDCPThe goal of this section is to develop our Coarsened Back and Forth (CBAF) algorithm to approximately compute the top-k diffusion centrality vertices in huge social networks where it is not feasible to compute DC for all vertices. The basic idea is to coarsen the original social network S into a smaller network S (cid:6)which tries to preserve the “diffusive behavior” of S. The top-k vertices are then computed over S (cid:6)and the solution is mapped back to a subgraph of S on which we again compute the top-k vertices. Given a social network S, a GAP (cid:2), and a property p, CBAF performs the following steps: (1) Compute the filtering S (cid:6)by merging its vertices so as to obtain a new social network SC and a mapping from the vertices of S (cid:6)to the vertices of SC ; (3) Compute a set T C of top-k vertices of SC ; (4) Use T C to compute an approximate set of top-k vertices of S. The first step has already been described in Section 5.1, while the other steps will be detailed in the rest of this section.of S; (2) Coarsen S(cid:6)82C. Kang et al. / Artificial Intelligence 239 (2016) 70–966.1. Social network coarseningThis section proposes a new semantical coarsening technique that reduces network size by merging together vertices that are similar, while trying to preserve the structural and semantic properties of the network w.r.t. a property p. The coarsening process involves the following issues: (i) how to select “similar” vertices to be merged together, (ii) how to assign properties to a merged vertex and to its edges after merging, and (iii) how to compute edge weights between merged vertices. These issues are addressed in the following.Vertex similarity. CBAF can work with any function to determine whether two vertices are similar. Throughout this paper, we assume that given any vertex v, there is a set SIM(v) ⊆ V of vertices that are similar to it. Function SIM can obviously be defined in many ways. We provide one such way that takes the diffusion model for p into account (as well as the social network structure).Consider an SN S, a GAP (cid:2), and a property p. Given two vertices u and v of S, we write u ∼ v iff u and v activate the same set of rules of (cid:2) (cf. Definition 2 for the notion of “activation”). Using this equivalence relation, we define SIM(cid:2)(v) ={u ∈ V | u ∼ v}. Obviously, many other definitions are also possible, but this is the one used in our experiments.Vertex merging. We now define how to merge similar vertices. When a set of vertices is merged into a new vertex v, we have to specify vertex properties of v, as well as associated edge properties/weights.Definition 9 (Vertex properties merging). Let S be an SN and {v 1, . . . , vn} be a subset of the vertices of S (to be merged). For each p ∈ VP, let g p : {0, 1}n → {0, 1} be any associative and commutative function. Then, we define:mergeVP({v 1, . . . , vn}, VL) = {p ∈ VP | g p(ϑ(v 1, p), . . . , ϑ(vn, p)) = 1}where ϑ(v i, p) = 1 if p ∈ VL(v i), otherwise ϑ(v i, p) = 0.The above definition assumes the existence of a function g p that takes the values (0 or 1) of the p-property of the vertices being merged and combines them into a single 0 or 1 value denoting whether the merged vertex has property por not.Some examples for the function g p can be computing the property intersection or union (i.e., taking the minimum or maximum value across the xi ’s). We can also use a “majority” function where the new vertex has the property p if the majority of the vertices being merged have the property p.We now address the problem of assigning edges to the new merged vertex. Let S = (V , E, VL, ω) be an SN and V(cid:6)(cid:6). For each edge (v, u, ep) ∈ E such that either v ∈ V{v 1, . . . , vn} be a subset of V to be merged into a new vertex vu ∈ V \ Vedge (v, v(cid:6)or u ∈ V(cid:6), ep) if u ∈ V, the new vertex vand v ∈ V \ V.(cid:6)(cid:6)(cid:6)(cid:6)(cid:6) =and , or the incoming will have the outgoing edge (v(cid:6), u, ep) if v ∈ V(cid:6)Edge weighting. We now define how to assign a weight to an arbitrary set of edges (having the same label) in S =(V , E, VL, ω).Definition 10 (Edge weighting). Let S = (V , E, VL, ω) be an SN and ep ∈ EP. Moreover, let {e1, . . . , em} ⊆ (V × V × {ep}) be an arbitrary set of edges between vertices in V labeled with ep, and gep : (0, 1]m → (0, 1] be any associative and commutative function. Then, we defineweight({e1, . . . , em}) = gep(ω∗(e1), . . . , ω∗where ω∗(e) = ω(e) if e ∈ E, otherwise ω∗(e) = 0.(em))(cid:6)Given two sets of vertices Vand Vof all possible edges having property ep that can exist from vertices in V(cid:6)(cid:6)}. Finally, if v{(vis a new incoming edge, then its weight is computed as weight(possEdges({v}, Voutgoing edge, then its weight is computed as weight(possEdges(Vto vertices in V(cid:6)is a new vertex obtained by merging a set V(cid:6), ep)), while, if eand an edge property ep ∈ EP, we define the set possEdges(V, i.e. possEdges(Vof vertices and e(cid:6)(cid:6) = (v(cid:6), {v}, ep)).(cid:6)(cid:6), ep) | v(cid:6)(cid:6) ∈ V(cid:6) ∈ V(cid:6) ∧ v(cid:6), v(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6), ep) as the set (cid:6)(cid:6), ep) =(cid:6), V(cid:6) = (v, v(cid:6), ep)(cid:6), v, ep) is a new (cid:6), V(cid:6)(cid:6)Social network coarsening. We are now ready to give the definition of social network coarsening.Definition 11 (Social network coarsening). Let S = (V , E, VL, ω) be an SN and θ be a real number in (0, 1] called the contrac-tion factor. A coarsening of S is an SN S (cid:6) = (V(cid:6), ω(cid:6)) together with an onto mapping π : V → V(cid:6), VL(cid:6), Es.t.:(cid:6)(cid:6)| ≤ θ · |V |;• |V(cid:6) = {(cid:11)π (v 1), π (v 2), ep(cid:12) | (cid:11)v 1, v 2, ep(cid:12) ∈ E ∧ π (v 1) (cid:17)= π (v 2)};• E(cid:6)(v• VL• ω(cid:6)(e(cid:6)) = mergeVP({v ∈ V | π (v) = v(cid:6);(cid:6)) = weight(possEdges(π −1(v 1), π −1(v 2), ep)), for e(cid:6)}, VL), for v(cid:6) ∈ V(cid:6) = (cid:11)v 1, v 2, ep(cid:12) ∈ E(cid:6).C. Kang et al. / Artificial Intelligence 239 (2016) 70–9683Algorithm 4 CoarsenSN.Input: A social network S = (V , E, VL, ω), a GAP (cid:2), a contraction factor θ ∈ (0, 1],a similarity function SIM, a neighbors threshold ρ ∈ (0, 1],a vertex properties merging function mergeVP, and an edge weight function.(cid:6), VL(cid:6), ω(cid:6)) and π : V → V(cid:6)of S.(cid:6), ω(cid:6)) = (V , E, VL, ω);(cid:6)(v) ← mergeVP({v} ∪ M, VL(cid:6));(cid:6) ∈ π −1(v);(cid:6) ∈ M;Randomly select a vertex v ∈ R;M ← getMergingSet(v, S(cid:6), (cid:2), SIM, ρ);R ← R \ (M ∪ {v});if (M (cid:17)= ∅) then(cid:6)| > θ · |V |) ∧ cont) do(cid:6)(cid:6), E(cid:6), VL, cont ← true;Output: A coarsening S(cid:6) = (V1: S(cid:6) = (V(cid:6), E2: π (v) ← v for all v ∈ V ;3: R ← V4: while ((|V5:6:7:8:9:10:11:12:13:14:15:16: return (cid:11)S(cid:6), π (cid:12);17: getMergingSet(v, S(cid:6), (cid:2), SIM, ρ)18:19:20:21:(cid:6) ← V(cid:6), ω(cid:6)(cid:12) ← UpdateEdges(E(cid:6)) ← v for all v(cid:6)) ← v for all v(cid:6) \ M;if (R = ∅) thencont ← f alse;VLπ (vπ (vV(cid:11)E(cid:6) = U ∩ SIM(v);U = all neighbors of vertex v in S(cid:6)URandomly select a set M ⊆ Ureturn M;(cid:6);(cid:6), ω(cid:6), v, M, weight, π );of size |M| = ρ · |U(cid:6)| ;22: updateEdges(E23:24:25:26:(cid:6), ω, v, M, weight, π )(cid:6)(cid:6) = {(cid:11)π (v 1), π (v 2), ep(cid:12) | (cid:11)v 1, v 2, ep(cid:12) ∈ EEfor each edoω(cid:6)(ereturn (cid:11)E(cid:6) = (cid:11)π (v 1), π (v 2), ep(cid:12) ∈ E(cid:6)) ← weight(possEdges(π −1(v 1), π −1(v 2), ep)) ;(cid:6)(cid:6), ω(cid:6)(cid:12);(cid:6) ∧ π (v 1) (cid:17)= π (v 2)};(cid:6)(cid:6)Coarsening an SN S yields a new SN S (cid:6)that (i) the number of vertices of S (cid:6)u and v in S, and u has been merged into a new vertex uthen there is an edge between ufunction mergeVP; and (iv) the weights of the edges of S (cid:6)we denote by π −1(v(cid:6)) the set {v ∈ V | π (v) = vand v(cid:6)}.(cid:6)(cid:6)together with a mapping π from the vertices of S to the vertices of S (cid:6)such is smaller than that of S by a factor θ ; (ii) if there was an edge between two vertices in S(cid:6)(cid:6) (cid:17)= u, are assigned using , ; (iii) the vertex properties of each merged vertex of S (cid:6)are assigned by function weight. In the following, given vwhile v has been merged into a new vertex vin S(cid:6)in S(cid:6)(cid:6) ∈ V(cid:6)(cid:6)(cid:6)Algorithm 4 is a general algorithm for coarsening a social network. It starts by initializing the mapping function π as the identity function. In each iteration, a vertex v in the current SN is randomly selected, and the set of vertices to be merged of all v’s neighbors that are similar with it is computed by the function getMergingSet. This function computes the set Uto v according to the similarity function SIM received as input, and returns a subset M of Uwhose size is a percentage ρ(cid:6)|. If the set of vertices M is not empty, vertex v is merged with M, otherwise a new vertex v is randomly selected. If of |Uv is merged with M, the new vertex properties of v are computed using the mergeVP function, the mapping π is updated, and the set of edges is updated by the function UpdateEdges. This function first computes the new set of edges so that if there was an edge between a vertex in M ∪ {v} and another vertex u, now there is an edge between v and u, while the new edge weight is computed by using the function weight received in input. The algorithm’s iterations stop when either (cid:6)|, where θ is the contraction factor establishing the number |Vthe desired size of the coarsened network, or it is not possible to merge any other vertex. Algorithm 4 can be used with our similarity function SIM(cid:2).(cid:6)| of vertices in the current SN is less than or equal to θ · |V(cid:6)(cid:6)6.2. Adapting the DC definition to the coarsened networkWe now adapt diffusion centrality to the case of coarsened networks. This intermediate step will later be used in the computation of diffusion centrality for vertices of the original network. When a set of vertices M in the original network Srepresents the network Sv(cid:6) consisting of all vertices in M and all is merged into a single vertex vedges among them from the original network. Thus, even if two vertices vhave the same diffusion centrality on the coarsened network, the actual diffusion of the property of interest in the original network among the vertices belonging to Sv(cid:6) and Sv(cid:6)(cid:6) may be different and depends on the properties of the two subnetworks Sv(cid:6) and Sv(cid:6)(cid:6) (number of vertices, in the coarsened network representing the edges, etc.). To take this into account, we assign a weight to each vertex vimportance of vw.r.t. to the original network.in the coarsened one, vand v(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)Definition 12 (Diffusion centrality on a coarsened network). Let S be a social network, SC = (V C , E C , VLC , ωC ) be a coarsening of S with vertex mapping π , and mv w be a function assigning a weight to each vertex in V C . Then, the diffusion centrality of a vertex v in the coarsened SN is defined as84C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Algorithm 5 CBAF (Approximate Top-k).Input: An SN S = (V , E, VL, ω), a GAP (cid:2), a property p, an integer k,a set of options opts as described in Table 2.(cid:6), E(cid:6), VL(cid:6), ω(cid:6)) ← net workF iltering(S, (cid:2), p)Output: An approximate set of top-k vertices.1: S(cid:6) = (V2: (cid:11)SC = (V C , E C , VLC , ωC ), π (cid:12) ← CoarsenS N(S(cid:6), (cid:2), opts)3: T C ← computeT op K (SC , k, mv w) % Use HyperDC4: T C = T C ∪ nbrs(T C , dC , SC )5: V I = π −1(T C ) ∪ nbrs(π −1(T C ), dI , S)6: SI = (V I , E I , VLI , ωI ) ← SN induced from S by the vertices in V I7: return computeT op K (SI , k, mv w 0)Table 2Input options opts for Algorithm 5.θ ∈ (0, 1]SIMρ ∈ (0, 1]mergeVPweightcontraction factorsimilarity functionneighbors thresholdvertex properties merging functionedge weight functionmv wdCdImerged vertex weight functionneighbors distance for extendingthe set T Cneighbors distance for extendingthe set π −1(T C )(cid:6)dc(cid:2),p,SC(v) = (cid:12)v(cid:6)∈V C \{v}(cid:6)(cid:10)mv w(v(cid:10)mv w(v) · lfp((cid:2) ∪ (cid:2)SC ⊕p(v))(p(v(cid:6)(cid:6)) · lfp((cid:2) ∪ (cid:2)SC (cid:16)p(v))(p(v(cid:6)(cid:11)))−(cid:11)))(cid:6)(cid:6)(cid:12)v(cid:6)(cid:6)∈V C \{v}Function mv w, which we call merged vertex weight function, can be defined in several ways. Below we provide three alternative definitions. Consider an SN S = (V , E, VL, ω), and let SC and π be a coarsening of S. Given a vertex v of SC ,• mv w 0(v) = 1. In this case, the original definition of DC is used.• mv w 1(v) = |V v | × |E v ||V v |−1 , where V v = π −1(v) and E v = {(cid:11)v 1, v 2, ep(cid:12) | v 1, v 2 ∈ V v ∧ (cid:11)v 1, v 2, ep(cid:12) ∈ E}. Thus, the weight of v is given by the number of vertices in the original SN that were merged into v multiplied by the density of Sv . The idea is that if Sv has high density and many vertices, the weight of v should be high.• mv w 2(v) = ln(mv w 1(v)) + 1. Here, we consider the fact that the diffusion of the property p can rapidly decrease |V v |2−|V v |= |E v |according to the distance of vertices in V v from the diffusion source vertex v.6.3. CBAF: approximately solving the kDCP problemIn this section, we show how to approximately compute the top-k diffusion centrality vertices in a social network. First, we introduce some definitions.Definition 13 (Induced social network). Given an SN S = (V , E, VL, ω), the SN induced from S by a set of vertices V I ⊆ V is SI = (V I , E I , VLI , ωI ), where E I = {(cid:11)v 1, v 2, ep(cid:12) | (cid:11)v 1, v 2, ep(cid:12) ∈ E ∧ v 1, v 2 ∈ V I }, VLI (v) = VL(v) for all v ∈ V I , and ωI (e) =ω(e) for all e ∈ E I .Given an SN S = (V , E, VL, ω), a set of vertices T ⊆ V , and a positive integer d, we denote by nbrs(T , d, S) the set of the neighbors of the vertices in T at a distance no greater than d. If d = 0, then nbrs(T , d, S) = ∅.(line 1). Then S(cid:6)We are now ready to present our CBAF algorithm shown in Algorithm 5. CBAF takes as input a social network S, a GAP (cid:2), a property p, an integer k, and a set of options opts as described in Table 2. It returns an (approximate) set of top-k diffusion centrality vertices over S. The first step of the algorithm filters out the original SN S by removing all unnecessary vertices, obtaining the network S (cid:6)is coarsened into a smaller social network SC (line 2), and the exact set T C of top-k vertices over SC is computed (line 3) by running HyperDC to find the diffusion centrality of all vertices in S(cid:6)and choosing the top-k. At this point the set T C is extended with its neighbors in SC at a distance no greater than dC , in order to limit the bias of the vertices in T C in the following computation (line 4). Next, the above set of vertices is mapped back to the vertices of the original SN S and is extended with its neighbors on S at a distance no greater than dI obtaining the set of vertices V I (line 5)—with a slight abuse of notation, we use π −1(T C ) to denote ∪v∈T C π −1(v). After that, the social network SI induced from the vertices in V I on S is computed (line 6), and the algorithm returns the approximate set of top-k vertices over S as the exact set of top-k diffusion centrality vertices computed over SI (line 7). Note that CBAF first evaluates diffusion centrality of all vertices in the coarsened network SC (which is typically small) and then diffusion centrality of all vertices in a subgraph of the original network corresponding to the neighborhood of vertices associated with the solution found in line 3 of the CBAF algorithm. This is typically a small fraction of the vertices in the original social network S.C. Kang et al. / Artificial Intelligence 239 (2016) 70–9685Table 3Non-game social networks used in the experiments.NetworkBlogCataloge-mail EnronDoubanwiki-Votesoc-Epinionsemail-EuAllDescriptionFriendshipe-mail communicationsFriendshipWikipedia vote relationshipsTrust relationshipse-mail communicationsTypeUndirectedUndirectedUndirectedDirectedDirectedDirected# Vertices10,31236,692154,9077,11575,879265,214# Edges333,983183,831654,188103,689508,837420,045Table 4STEAM networks used in the experiments.Social networkGAME8690GAME50510GAME6850GAME1500GAME24420GAME11450GAME17300GAME420GAME220# Vertices4,08328,87252,87966,57182,37789,942122,4671,307,3352,030,579# EdgesAvg. degree21,447115,254164,702303,240437,554327,258441,65712,431,56420,596,3315.253.993.114.565.313.643.619.5110.14Density6.28E−032.73E−045.45E−052.05E−038.84E−055.97E−06Avg. degree64.7810.028.4514.576.711.58Density1.29E−031.38E−045.89E−056.84E−056.45E−054.05E−052.94E−057.27E−065.00E−067. Experimental evaluationThis section contains a detailed report on our experiments.1) Exact Computation with HyperDC. We compared the runtime and spread generated by diffusion centrality against clas-sical centrality measures (Section 7.2), using networks with up to around 265K vertices and 440K edges.2) Approximate Computation with CBAF. We tested scalability, runtime, and spread of CBAF with networks of up to 2M vertices and 20M edges (Section 7.3).3) Comparing spread of memes by high DC vertices against low DC vertices. In order to test whether diffusion centrality captured real spread, we ran a test with MemeTracker data where we knew who had initiated a meme. We tested the hypothesis that vertices with high centrality would be more influential than those with low centrality according to diffusion centrality as well as classical centrality measures. Kolmogorov–Smirnov tests validate both findings: (i) high DC vertices diffuse memes better than low DC vertices, and (ii) diffusion centrality does a better job explaining real meme diffusion than classical centrality measures.We implemented HyperLFP (Algorithm 1), HyperDC (Algorithm 2), and CBAF (Algorithm 5) in Java. To compute degree, eigenvector, PageRank, closeness, and betweenness centrality we used the Java Universal Network/Graph Framework (JUNG).3All experiments were run on an Intel Xeon @ 2.40 GHz, 24 GB RAM.7.1. Experimental setupSocial networks. Our experiments used several real-world social networks summarized in Table 3. The networks are taken from the Stanford Large Network Dataset Collection [38]. We also considered an additional online game dataset called STEAM [6], which contains friendship relations (represented as directed edges) between players (vertices). Each player has several vertex properties and we selected country, group(s), games played, and total time played per game. We extracted 10 subnetworks from the whole STEAM dataset by choosing different games and selecting, for each game, all players who played and all edges between the players. The features of the extracted networks are reported in Table 4.Diffusion models. We ran experiments with a conditional probability model (hereafter referred to as the “Flickr model” [13]), the Jackson–Yariv tipping model [28], and the SIR model of disease spread [2]. We generalized these diffusion models by adding an additional condition q(u) : μ in rule bodies. When all vertices have the property q, then we have the original diffusion models. The q-condition determines when the diffusion process can happen. More specifically, for the Flickr model, only vertices satisfying property q can spread the diffusive property, while for the Jackson–Yariv and SIR models, only vertices satisfying property q can get the diffusive property. In the Flickr model, q is used to represent the “willingness” of a person to share her/his information (and thus influence other people). In the Jackson–Yariv and SIR models, q is used to represent a precondition for a vertex to adopt a behavior or get a disease. Thus, property q is useful when expressing the fact that only some vertices with some characteristics (modeled by q) can infect or be infected by the diffusive property. Note that edges originating from nodes that do not have property q can still play a role in the diffusion depending on the diffusion model. In our experiments, we compared DC with other classical centrality measures by varying the percentage of vertices in the network with property q. The diffusion models are reported in Appendix A.3 jung.sourceforge.net.86C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Fig. 3. STEAM data: Runtimes (ms) averaged per vertex when δq ∈ {1%, 2%, 3%, 4%, 5%, 10%, 15%, 20%, 25%, 30%}.7.2. Diffusion centrality vs. classical centrality measuresAs described earlier, the Flickr, Jackson–Yariv, and SIR models assume that some set of vertices in the network have property p and that only vertices satisfying some property q can spread (in the case of the Flickr model) or receive (in the case of Jackson–Yariv and SIR models) p. For STEAM data, we were able to use known properties of the vertices for q but we were not able to do this for the other datasets.Before getting into the details of our experimental evaluation, we summarize the high level conclusion of these com-parative experiments (experiments on scalability using the CBAF algorithm are in the next subsection): (i) the runtime of HyperDC is better than betweenness and closeness centrality and comparable with the others, and (ii) the spread achieved by diffusion centrality is almost always better than those achieved by classical centrality measures.7.2.1. STEAM dataWe used all but the last two (very large) STEAM data sets (Table 4) for the comparative analysis as some classical centrality measures could not finish the computation even on the first seven STEAM data sets. Consistent with the data, we set the percentage δp of vertices that initially have property p to 0% and varied the percentage δq of vertices having property q by using real properties of vertices in the STEAM data. In the STEAM data, q was taken to be the playing time of users in the game and was assigned as follows: we sorted the players in descending order according to playing time and assigned q to the first δq users. We varied δq ∈ {1%, 2%, 3%, 4%, 5%, 10%, 15%, 20%, 25%, 30%}. The last two STEAM subnetworks, which are substantially larger, were used for the evaluation of the CBAF algorithm (cf. Section 7.3).Runtime. We compared the time to compute DC w.r.t. the three diffusion models against the time to compute classical centrality measures. Fig. 3 shows how the runtimes vary w.r.t. δq for two representative STEAM games.Of the 7 STEAM games we tested in this experiment, closeness centrality computation finished only in the smallest case (GAME8690). HyperDC is much faster than betweenness and closeness centrality for all the STEAM networks and for all values of δq. Even when δq = 30%, the runtime of HyperDC is still low.HyperDC is faster than PageRank over networks GAME8690 and GAME50510 (the two smallest STEAM networks) for the Flickr and Jackson–Yariv models, and also faster than degree over network GAME50510 when δq is low.4 However, when the number of vertices increases (i.e., for the STEAM datasets other than GAME8690 and GAME50510), HyperDC becomes slower than PageRank and degree for all three diffusion models. For instance, in GAME1500, a crossover occurs when δq = 3% for the SIR model but not for the Jackson–Yariv model. There are two main reasons for that: (i) the size of I(cid:2),p is larger for the SIR model (recall that the smaller I(cid:2),p is, the more effective the optimization of Proposition 3 will be), and (ii) the SIR model is neither p-monotonic nor p-dwindling (and thus optimizations are less effective, cf. Section 5.1).Runtime of HyperDC tends to be linear. We note from Proposition 6 that in the worst case, the complexity of HyperDC is nonlinear. In order to assess actual runtime characteristics of HyperDC, we ran an experiment on the STEAM data when the number of vertices increases and as δq varies. We used the networks from GAME 50510, 6850, 11450, and 17300 which all have a similar average degree (in the 3–4 range). Fig. 4 shows the number of vertices in the network on the x-axis and the average runtime per vertex on the y-axis. Different curves show varying values of δq in the range 5–30%. We see that irrespective of the diffusion model used and the value of δq , HyperDC runs in linear time in practice.4 Though it may seem surprising that HyperDC sometimes beats degree centrality, the reason for this is that when δq is low (below 3%), HyperDC only needs to compute diffusion centrality for a small number of vertices. However, when δq is larger, this is not the case.C. Kang et al. / Artificial Intelligence 239 (2016) 70–9687Fig. 4. STEAM data: Average runtime (per vertex) of HyperDC as the number of vertices is increased and for different δq using Flickr, Jackson–Yariv, and SIR models.Table 5Spread over STEAM networks for different δq values.ModelNetworkSTEAM data: average ratio of the spread generated by diffusion centrality to the best spread generated by any of the classical centrality measures for different δq1%2%3%5%4%FKModelJYModelSIRModelGAME8690GAME50510GAME6850GAME1500GAME24420GAME11450GAME17300AverageGAME8690GAME50510GAME6850GAME1500GAME24420GAME11450GAME17300AverageGAME8690GAME50510GAME6850GAME1500GAME24420GAME11450GAME17300Average19.418.1436.6677.6125.6512.0288.7296.92.811.08.18.85.410.99.08.01.210.14.95.72.06.74.35.02.422.5654.289.96.5585.5363.5246.41.75.05.86.73.37.74.55.00.82.63.63.31.44.42.12.62.023.811.648.36.1804.36.6129.01.45.14.94.62.86.22.43.90.83.02.82.21.52.71.02.01.826.510.449.96.38.96.715.81.24.34.53.72.65.12.23.40.82.22.62.41.51.71.11.81.919.510.746.65.79.76.814.41.34.03.63.62.24.22.23.00.82.22.52.11.41.81.21.710%1.95.08.148.84.96.95.415%1.63.66.450.03.15.55.120%1.53.05.656.22.35.14.425%1.42.74.155.51.94.93.730%1.32.43.23.11.73.72.811.610.811.210.61.22.72.52.31.62.91.62.10.81.41.81.71.11.40.71.31.11.92.12.01.42.21.21.70.91.11.71.20.81.10.61.11.11.51.81.91.31.91.21.50.91.01.41.10.81.10.61.01.11.11.71.71.31.71.21.40.80.91.31.00.81.00.60.92.61.21.01.60.91.31.61.21.30.80.91.20.80.80.90.60.9Higher values of δq lead to higher running times, because when more vertices have property q, diffusion unfolds more and thus HyperDC takes more time to converge to the least fixed point. In several real scenarios, information spreading is limited to individuals who are within close proximity (e.g., see [13]).Spread. We now compare the diffusion of property p when we choose the top-k central vertices according to DC vs. using classical centrality measures on the STEAM data. In each case, the top-k vertices are called seeds. We vary k from 10 to 100in steps of 10. The spread w.r.t. a given set of seeds is the expected number of vertices with property p (after diffusion) assuming the seeds have property p minus the expected number of vertices with property p (after diffusion) in the original social network. This difference is normalized. By the expected number of vertices with property p for an SN S after diffusion v∈V lfp((cid:2) ∪ (cid:2)S )(p(v)), where V is the set of vertices of S. Our spread experiments are presented in two of (cid:2) we mean ways. In the first, we show how spread varies by averaging over the number of seeds for fixed δq values. In the second, we do the opposite.Spread experiments averaged over varying k values for specific δq values. We varied δq over the set {1%, 2%, 3%, 4%, 5%, 10%, 15%,20%, 25%, 30%}. For each selection of δq, we considered every value of k from the set {10, 20, . . . , 100}. Then for a fixed δq, k, Diffusion–Model triple, we computed the ratio of the spread using DC to the best spread achieved by any of the classical centrality measures. Table 5 reports the average of these ratios. Thus, any ratio greater than 1 shows that DC achieves a higher spread than all of the classical centrality measures.(cid:8)For the Flickr and Jackson–Yariv models, DC always achieved a better spread than all classical centrality measures. For the SIR model, on average, DC achieves better spreads than classical centrality measures as long as δq ≤ 15% — at 20%, they are even, and at 25–30%, classical centrality measures achieve a better spread.88C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Table 6STEAM data: Average ratio of the spread generated by diffusion centrality to the best spread generated by any of the classical centrality measures for different k values.ModelNetworkAverage ratio of the DC spread to the best spread of other centrality measures for different k values905010604070803020FlickrModelJYModelSIRModelGAME8690GAME50510GAME6850GAME1500GAME24420GAME11450GAME17300AverageGAME8690GAME50510GAME6850GAME1500GAME24420GAME11450GAME17300AverageGAME8690GAME50510GAME6850GAME1500GAME24420GAME11450GAME17300Average16.548.141.4364.048.270.963.393.21.93.32.63.12.23.02.12.61.22.12.22.51.42.11.31.82.16.867.394.372.2116.4101.065.71.53.63.33.32.24.31.92.90.82.02.42.41.42.81.21.92.29.086.158.44.4155.1134.864.31.33.93.73.92.24.72.53.20.82.42.52.61.42.41.31.92.110.5101.267.85.2187.7164.076.91.23.73.73.92.34.52.83.20.82.42.42.21.32.31.41.82.17.7114.974.25.5216.8187.887.01.34.13.84.02.44.72.83.30.82.82.42.21.32.21.31.92.08.6127.981.05.7241.58.067.81.43.73.53.72.44.52.83.10.82.52.22.11.12.21.21.72.09.2138.588.35.5265.28.073.81.43.93.83.72.54.63.03.30.82.72.32.11.12.11.31.82.19.8149.693.55.7287.88.579.61.44.04.03.62.44.83.03.30.83.02.51.91.12.21.31.82.110.3157.899.96.0310.08.985.01.43.74.23.52.44.82.93.30.92.72.51.81.02.21.31.81002.17.2166.2104.75.795.39.455.81.43.84.03.52.34.63.03.20.92.72.41.81.02.21.41.8Not surprisingly, this spread ratio decreases as δq increases because when δq is large, more vertices get infected regardless of how the seeds were chosen and thus the difference between DC and other centrality measures decreases.Interestingly, spread ratios for the Flickr model are very large compared to those for the Jackson–Yariv and SIR models. This is because the expected number of vertices which have property p after diffusion is much higher in the case of the Flickr model than in the other two cases.Spread experiments averaged over varying δq values for specific k values. Here we selected values of k as before and averaged over different possible values of δq drawn from the set {1%, 2%, 3%, 4%, 5%, 10%, 15%, 20%, 25%, 30%}. Table 6 summarizes the results. As in the previous case, a ratio exceeding 1 implies that DC outperforms all classical centrality measures. On average, for all values of k and for all three diffusion models, DC achieves a better spread than all the classical centrality measures, with only a few exceptions in the SIR model.Interestingly, the spread ratio for GAME8690 is consistently the lowest according to each of three diffusion models and in each setting of both Tables 5 and 6. This game has just 4083 vertices (so it is very small and dense). Our choice of q is based on the amount of playing time of a player and there is strong correlation between that and the number of friends. As a consequence, having multiple seeds does not greatly increase diffusion of property p because of overlaps between the vertices that may be influenced by the seeds.7.2.2. Non-game social network dataIn this section, we perform experiments similar to those reported above on the networks in Table 3. For these networks, the data did not have associated vertex properties. We looked at two cases.Case 1. We randomly selected δp = 0.1% of the vertices to have a synthetic property p (in 5 runs). In each run, we varied δq ∈ {1%, 2%, 3%, 4%, 5%, 10%, 15%, 20%, 25%, 30%} and selected δq% of the vertices to have a synthetic property q.Case 2. We randomly selected δq = 3% of the vertices and varied δp ∈ {0.1%, 0.2%, 0.3%, 0.4%, 0.5%}, associating synthetic properties p and q with vertices as above.Recall that, as mentioned before, vertices that do not have the q-property can still play a role in spreading p depending on the diffusion model.5Runtime. Case 1. Fig. 5 shows how runtime varies w.r.t. δq for Case 1 above. The networks are sorted from left to right according to the number of vertices, with directed networks in the first row and undirected networks in the second row. As 5 For instance, consider an SN with vertices v 1, v 2, v 3, v 4, and edges (v 1, v 2), (v 2, v 3), (v 3, v 4), where only v 4 has property q. Consider a diffusion model saying that if X has property p, X is connected with Y , Y is connected with Z , and Z is connected to W , then if W has property q he gets property p. If, for instance, v 1 has property p, then v 4 gets p too. Notice that each of the four vertices and their connections play a role in that. So, if for instance we delete v 2 and the edge (v 2, v 3) (e.g., just because v 2 does not have property q), we lose the real behavior discussed above.C. Kang et al. / Artificial Intelligence 239 (2016) 70–9689Fig. 5. Non-game SNs: Runtime per vertex when varying δq from 1% to 30% and δp = 0.1%.in the case of our experiments with the STEAM data, closeness and betweenness centrality are time consuming compared to the other algorithms (including HyperDC).HyperDC with the Flickr model is faster than PageRank in the majority of cases considered, and sometimes faster than eigenvector centrality. HyperDC with the SIR model is faster than PageRank for directed networks when δq ≤ 4%. As in the case of the STEAM data described earlier, this is because the network filtering step can eliminate many vertices. For the Flickr and SIR models, the runtimes increase slightly as δq increases because of the higher diffusion that takes place.However, as in the case of the STEAM data, HyperDC with the Jackson–Yariv model often takes the most time. By in-creasing the percentage of vertices having property q, computing diffusion centrality for the Jackson–Yariv model becomes slower than for the other diffusion models, because the diffusion process is determined by the sum of neighbors’ diffu-sion probability and now we have that 0.1% of the vertices initially have the property p. Thus, every time that a vertex’s probability is updated, all of its neighbors who have q are updated in the next step.Case 2. Fig. 6 shows how runtime varies as δp varies. The runtime for closeness and betweenness centrality are very high (worse by 1 to 3 orders of magnitude) and hence we do not report runtimes for them.HyperDC’s runtime for the Flickr and SIR models do not vary much with δp . HyperDC with the Flickr model is faster than PageRank in almost all networks and settings. Compared to degree centrality, HyperDC with Flickr exhibits competitive runtimes, being faster in about half the settings considered and comparable or slightly worse in the others. HyperDC with the SIR model is faster than PageRank in all data sets except the Douban data set.As in the case of the STEAM data, HyperDC with the Jackson–Yariv is the worst performer w.r.t. runtime (excluding betweenness and closeness centrality which we eliminated earlier due to their high running times) because the computation of diffusion centrality for the Jackson–Yariv model becomes slower when many vertices have the diffusion property p.Spread. In both Cases 1 and 2, on average, experimental results showed that the ratio of spread generated by HyperDC to the spread generated by the best classical measure exceeds 1. As in the case of the STEAM data, the best ratios are for the Flickr model.7.3. CBAF algorithm: performance experimentsIn this section, we describe experiments we performed to compare CBAF with HyperDC in terms of both runtime and spread. Simply put, these experiments show that CBAF almost always achieves the same spread as HyperDC with a runtime that is always lower (with the correct choice of settings) than HyperDC—moreover, sometimes it takes less than half the runtime of HyperDC.Input options for CBAF. There are a number of input options for CBAF that influence its performance (cf. Table 2). In order to find the best possible input options, we ran extensive experiments in which we considered the 972 different candidate option sets determined by the candidate values reported in Table 7. Each option set was tested 5 times (because of the 90C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Fig. 6. Non-Game SNs: Runtime per vertex when varying δp from 0.1% to 0.5% with δq = 3%.Table 7Candidate and best values of the input options for CBAF.Possible valuesOptiondCdIweightmergeVPmv wρ0, 1, 20, 1, 2min, max, averageunion, intersect, majoritymv w 0, mv w 1, mv w 2One vertex, 10%, 50%, 100%Best input optionsO P 110maxmajoritymv w 1100%O P 210maxintersectmv w 1100%O P 310averagemajoritymv w 0100%random component of the algorithm) over the GAME17300 network. Candidate option sets were compared on the basis of their running time and the quality of their results, measured in terms of spread, recall, Kendall and Spearman’s rank correlation coefficients. The three option sets that achieved a good balance between runtime and spread are reported in Table 7—these were the options we used in our experiments with CBAF. Specifically, dC = 1 and dI = 0 showed better running times than the other values with little difference in terms of quality; weight = min was disregarded as its computing time was slightly better than max and average, while the quality was much worse; running times for mergeVP = union and mv w = mv w 2 were worse than the other values while the qualities were similar; ρ = 100% showed slightly higher running times, but much better quality than the other values. In all experiments, we used the vertex similarity function SIM(cid:2) defined earlier. The contraction factor θ was chosen from the set {0.2, 0.3, 0.4}.CBAF evaluation measures. We used two measures to evaluate CBAF. The time ratio (denoted ratiotime) is simply the ratio of time taken by CBAF vs. HyperDC. The spread ratio (denoted ratiospread) is the ratio of spread according to CBAF (assuming the top-k vertices have property p) vs. that according to HyperDC.We compared HyperDC and CBAF over the two largest STEAM networks (GAME420 with 1.3M vertices and 12.43M edges, and GAME220 with 2.03M vertices and 20.59M edges) and the 2 largest non-game networks (Email-Eu and Douban), together with the Flickr, Jackson–Yariv, and SIR models, using the three best option sets of Table 7. In all the experiments we set δp = 0 and δq = 5%. For the Email-Eu and Douban networks, we set k = 100 and θ ∈ {0.4, 0.3, 0.2}. For the two STEAM networks, we ran the experiments with θ = 0.4, and k ∈ {130, 650, 1300} for GAME420 and k ∈ {200, 1000, 2000} for GAME220.Runtime. Option O P 3 is the one that consistently yielded the fastest runtimes for CBAF and its results are reported in Table 8 (we present the results only for the network GAME220, which is the largest one, because the results for GAME420 are quite similar to the ones for GAME220). The time ratios are always less than one in all networks and for all diffusion models with the exception of the Jackson–Yariv model in the huge network GAME220 with k = 2000. In particular, on the C. Kang et al. / Artificial Intelligence 239 (2016) 70–9691Table 8Results over the Douban, Email-Eu, and GAME220 networks (Option set O P 3).Douban networkModelθEmail-Eu networkGAME220 networkFlickrJYSIR0.40.30.20.40.30.20.40.30.2ratiotime0.630.620.640.930.910.910.460.480.47ratiospread0.960.970.970.930.940.940.990.990.99ratiotime0.830.820.830.520.550.660.720.760.76ratiospread0.610.670.620.820.820.830.900.890.91k200100020002001000200020010002000ratiotime0.520.450.600.950.951.040.660.720.64ratiospread0.990.990.940.971.001.000.970.981.00Douban network with the SIR model the time ratio is less than 50%, while delivering an almost perfect spread ratio of 0.99. On the Email-Eu network using the Jackson–Yariv model, it runs in just over 50% of the time taken by HyperDC. On the huge GAME220 network using the Flickr model it runs in under 60% of the time taken by HyperDC. CBAF does not work well for the Jackson–Yariv model in the two huge networks because of the high average degree of these networks.Spread. In all cases, all three options yield approximately the same spread. In the huge networks and the Douban network, the spread ratio is always close to one. On the Email-Eu network, the spreads range from 0.8–0.9 for the Jackson–Yariv and SIR models. However, for the Flickr model, the spread is lower, mostly in the 0.6–0.7 range. The reason for this is that the Email-Eu network has a very low average degree which leads to merged vertices (in the coarsened networks) representing only small sets of vertices of the original network so that the induced network is small and not representative enough to compute approximate top-k vertices well.The performance of CBAF w.r.t. runtime and spread depends on the input options. In general, we have observed that the more aggressive is the coarsening step, the faster is the algorithm, but the quality (spread) gets worse. In the step computing the induced SN, if we add more neighbors the quality gets better, but running times become higher.7.4. Testing the quality of DC in MemeTracker dataWe also tested the quality of diffusion centrality in the real context of memes diffusion through the Web. We used the MemeTracker data6 consisting of a set of 172M news articles and blog posts from 1M online sources collected from September 1 2008 till August 31 2009. For each article/post the dataset contains timestamp, phrases contained in the document and hyper-links. In addition, phrases have been clustered together and this information is available in the data, too. We considered all phrases in the same cluster as the same meme. We built a network from the raw phrase data where vertices are online websites and edges are hyperlinks. More specifically, we selected as vertices the top 10,000 sites w.r.t. number of hyperlinks, and inserted a direct edge (u, v) between two sites u and v if there is a webpage on site u having a hyperlink to a webpage on site v.Our aim was to show that diffusion centrality correlates well with the spread generated by vertices: vertices with high diffusion centrality spread more than vertices with low diffusion centrality. To show that, we restricted our attention to source vertices, i.e. online websites that firstly showed a meme m, and for each of them we computed the actual spread (number of websites infected by m). We assumed that a site u infected a site v with meme m if there is an edge from u to v, m appeared on u at time t1 and on v at time t2, and t1 < t2. When many webpages belonging to the same website u are infected by the same meme m, we consider the smaller webpage timestamp as timestamp for the infection of u. Moreover, our analysis focused only on the top 5000 most spread memes.We assumed that the memes diffusion through the websites is described by a (cascade) diffusion model for which we estimated the parameters by using the actual spread of memes (the identification of the actual model for memes diffusion is out of the scope of this paper).By using the MemeTracker data and the above diffusion model, we performed a set of experiments as follows. We do not use a k-fold cross validation since we have temporal data, but we considered a time window (t1, t2) to determine training/testing data that we moved in steps of one day, whose size (in days) assumed values in the set {30, 60, 90}. For each time window, we considered the data whose timestamp is in the first 80% of the days as a training set to estimate the parameter of the diffusion model, and the last part as a test set. For each meme m in the test set we computed the tuple (m, v, c, s), where v is the source of m, c is its centrality value (we computed diffusion centrality, PageRank, degree, betweenness and closeness centrality), and s is the actual spread of m.Fig. 7 (left) shows the distribution of centrality values for DC, PageRank, closeness, betweenness, and degree centrality as well as the distribution of the actual spread over all the memes in the dataset and by considering a sliding time window of size 30 days. The plot in Fig. 7 (right) shows the value of the distance (measured by using the Kolmogorov–Smirnov 6 http :/ /www.memetracker.org /data .html.92C. Kang et al. / Artificial Intelligence 239 (2016) 70–96Fig. 7. (Left) Actual spread and centrality measures’ distributions. (Right) Kolmogorov–Smirnov statistic between actual spread distribution and centrality measures’ distributions from the left plot.statistic) between the actual spread distribution and the distribution of PageRank, closeness, diffusion, betweenness, and degree centrality. The figure shows that the distribution of DC values is the closest to the one of the actual spread and its distance is the smallest. This result intuitively suggests that an initiator with a high value of diffusion centrality is more likely to reach a high value of spread than one with a low value of diffusion centrality, while this does not hold for the other centrality measures.7.5. Summary of experimentsIn this section, we conducted three experiments. The first compares HyperDC with classical centrality measures. The experiment shows that: The runtime of HyperDC is better than betweenness and closeness centralities and comparable with other centrality measures. The spread achieved by diffusion centrality is almost always better than those achieved by classical centrality measures.The second experiment compares HyperDC with CBAF. We show that CBAF almost always achieves the same spread as HyperDC with a runtime that is always lower (with the correct choice of settings) than HyperDC.Recall that in our experimental setup, we considered an initial distribution of a property q, which models a characteristic that a vertex should have to infect or be infected by the diffusive property. As also discussed before, running times increase as the percentage δq of vertices having property q increases. This is because diffusion unfolds more with more vertices having property q, which means that the time to compute DC increases. Nevertheless, HyperDC showed good performances with values of δq up to 30%. Lower percentages of vertices having property q essentially mean that propagation unfolds less, and thus the fixpoint of HyperDC is reached sooner. However, it is worth mentioning that this is the case in several real scenarios. For instance, [13] analyzed propagation in the Flickr social network and found out that even the more popular photos have substantially limited popularity outside the immediate network neighborhood of the uploader.The runtime of DC is expected to be higher with more complex diffusion models. A structural analysis of the corre-sponding GAPs can provide insights on what to expect from the diffusion model in terms of running time to evaluate it. In this regard, the subclasses of GAPs and the optimizations introduced in Section 5 are useful tools. For instance, if a GAP is p-monotonic and/or p-dwindling, we can lower running times, as we can apply further optimizations and because the HyperLFP converges more quickly. Another useful parameter to analyze is the size of I(cid:2),p , which roughly speaking consists of the predicate symbols “interfering” with p; we can expect better performances when I(cid:2),p is small. These analyses are useful in practice to “tune” the diffusion model and find a good balance between accurateness of the model and time to evaluate it. For instance, one might want to simplify the considered diffusion model to make it p-monotonic, sacrificing its accurateness in describing the diffusion process, but obtaining a more efficient evaluation.To strengthen our claims about the advantage of DC in predicting the spread initiated by given vertices, we used the MemeTracker data where diffusion occurred naturally without our intervention. The parameters of the diffusion model were estimated from the data (making sure to separate learning and testing data). The results of this experiment showed that there is a correlation between the DC values and the actual diffusion of memes: high diffusion centrality vertices diffuse memes better than low diffusion centrality vertices. It also showed that diffusion centrality is a better predictor of real meme diffusion than classical centrality measures.8. ConclusionCentrality and importance in social networks are closely interlinked concepts. Central vertices are assumed to be impor-tant and vice versa. However, in real-world online social networks, people who are considered important or authoritative on some topics may be considered very unimportant on others. For instance, an influential sports commentator is not likely to be an influential movie critic. Moreover, the importance of an individual should be measured by an influence related fac-tor. A person who can influence 1000 people about movies should be considered more important (with regard to movies) than one who can influence just 500. Diffusion models are a mechanism to capture the spread of concepts or phenomena C. Kang et al. / Artificial Intelligence 239 (2016) 70–9693through a network. Many diffusion models have been developed to successfully predict the extent of spread of memes and concepts through networks.In this paper, we propose the novel concept of Diffusion Centrality and show how it can be computed with respect to a wide array of diffusion models. [51] shows that the framework of generalized annotated programs (GAPs) can express most known diffusion models.In this paper, we increase the breadth of knowledge about GAPs by introducing novel specific classes of GAPs and present the HyperDC algorithm that exactly computes the top-k diffusion centrality vertices w.r.t. any GAP-expressible diffusion models. In addition, we present a novel (but approximate) Coarsening Back and Forth (CBAF) algorithm that allows us to take a huge social network, and reduce it to a manageable size to efficiently solve (in an approximate way) the problem of finding the top-k vertices.We conduct a very detailed experimental study on several real-world social networks. A first set of experiments compares the runtime and spread generated via the HyperDC algorithm with classical centrality measures. Our results show that HyperDC is efficient and produces better spread than current centrality measures. A second set of experiments looks at the scalability of CBAF, showing that it almost always has a lower runtime than HyperDC, while achieving high spreads. In particular, CBAF was tested on networks with over 2M vertices and over 20M edges and achieved acceptable runtime. Using MemeTracker data, we show that diffusion centrality captures the importance of people who are truly responsible for the spread of a meme more effectively than past centrality measures.This work opens up a dramatic new set of possible diffusion models that can be automatically learned from data that take into account the rich semantics that can be associated with vertices and edges in modern social networks like Twit-ter, Facebook, and LinkedIn. For instance, in the case of predicting election outcomes using Twitter data [30], we might learn rules that identify the likelihood that person P will vote for a candidate C by taking their gender, demographic fac-tors, tweets, and the tendencies of their neighbors to vote for C. Alternatively, we might look at the diffusion of banking crises throughout the world’s nations by considering network flows of exposures—the nodes in such a network would be countries and the edges would be labeled with the exposure a country has to another [44]. High exposure of country A to country B would suggest that a systemic banking crisis in country B could lead to default, triggering a systemic bank-ing crisis in country A. Clearly, there are many other factors to be considered. For instance, political factors (represented as semantic properties of vertices) might capture the likelihood of country A taking intelligent steps to forestall a crisis. Mutual trust (a property of the edge) might determine whether A and B can work together to address the problem. Such a diffusion model will use the rich semantic opportunities offered by research in knowledge representation via GAPs, as well as other paradigms, to learn more fine-grained diffusion models than the relatively coarse grained diffusion models that were developed in the past. We believe this would form a rich line of inquiry for the future.AcknowledgementsSome of the authors were partly supported by ARO grants W911NF11103, W911NF1410358, and W911NF09102, by Maf'at, and by Israel Science Foundation (grant #1488/14).Appendix A. Diffusion modelsBelow we provide details on the diffusion models used in the experimental evaluation. The Flickr model consists of the following rule (see [51]):p(v) : μv(cid:6),v × μp × μq × dp F ← e(v(cid:6), v) : μv(cid:6),v ∧ p(v(cid:6)) : μp ∧ q(v(cid:6)) : μq(cid:6)saying that if vertex vhas properties q and p then it can diffuse property p to its neighbor v. The value dp F is a constant representing the probability that the vertex v will receive property p. This model falls into the category of cascade models.The Jackson–Yariv model is a diffusion model stating that a vertex will receive (adopt) a property p according to the cumulative effect of its neighbors and the ratio of the benefit to the cost of the vertex for adopting p. Suppose that v i is an agent having a default behavior that can be changed in the new behavior p, and that v i has specific cost ci and benefit bifor adopting the behavior p. Then, the Jackson–Yariv model can be expressed by the rule (see [51]):p(v i) : bici(cid:8)× r(j E j) ×(cid:8)j w j(cid:8)j E j× wq × dp J Y ←(cid:12)(cid:10)e(v j, v i) : E j ∧ p(v j) : w j(cid:11)∧ q(v i) : wqv j |(v j ,v i )∈E(cid:8)where (1) r(j E j) is a function describing how the number of neighbors of v i affects the benefits to v i for adopting (cid:8)j w j(cid:8)is the fraction of the neighbors of v i having property p, and (3) dp J Y is a constant representing the behavior p, (2)j E jj E j) to be a logarithmic function probability that the vertex v will adopt the property p. In our experiments we set r(max of the network, and having values within the interval [0.1, 2], normalized by the logarithm of the maximum in-degree din+ 0.1. When the annotation μ of an atom p(v), with v ∈ V , becomes greater than 1, then we i.e., r(set μ = 1. Moreover, observe that the vertex v i can adopt property p only if it also has property q. For the STEAM data, we j E j) = 1.9 × ln(j E j )max)ln(din(cid:8)(cid:8)(cid:8)94C. Kang et al. / Artificial Intelligence 239 (2016) 70–96= 1 for all vertices, while for the Non-Game networks we randomly assigned bicito the vertices according to a normal set bicidistribution with 0.5 ≤ bici≤ 1.5.The SIR model is a classic disease model which labels each vertex with susceptible if it has not had the disease but can receive it from one of its neighbors, infectious if it has caught the disease and t units of time have not expired, and recoveredwhen the vertex can no longer catch or transmit the disease. The diffusion rules are following (see [51]):p(v) :(1 − R) × μeri(v) :μrvr1(v) :μpv× μpv(cid:6),v← ri−1(v) : μr← p(v) : μpvv(cid:6) × (1 − Rv , i ∈ [2, t](cid:6)) × μqv× dp S I R ← rt (v) : R ∧ e(v(cid:6), v) : μev(cid:6),v∧ p(v(cid:6)) : μpv(cid:6) ∧ rt (v(cid:6)) : R(cid:6) ∧ q(v) : μqvHere, only the vertices having property q can be susceptible and the diffusion property p represents that a vertex is infected. Properties ri (i (cid:17)= t) express that a vertex is in the infectious state at time t − 1 and rt means that a vertex is recovered. In our experiments, we set t = 2. The constant dp S I R is the probability that the vertex v will be infected. The SIR model falls into the category of cascade models.We conclude this section by showing how the well-known linear threshold model can be expressed with GAPs. Recall that in the linear threshold model, a vertex v is influenced by each neighbor w according to a weight b w,v such that bw,v ≤ 1. Furthermore, each vertex v has a threshold θv ∈ [0, 1], which is the weighted fraction of v’s neighbors (cid:8)w neighbor of vthat must become active in order for v to become active, that is, v becomes active if (cid:8)bw,v ≥ θv . The GAP w active neighbor of vbelow captures the behavior of the linear threshold model. For each vertex v, the GAP has a rule of the following form:⎡⎛⎞⎤p(v) :⎝⎢⎢⎢(cid:3)w neighbor of vB w,v × X w⎠ − θv←⎥⎥⎥(cid:12)(cid:10)e(w, v) : B w,v ∧ p(w) : X w(cid:11)w neighbor of vNotice that the SN is assumed to be modeled with facts of the form e(w, v) : bv,w , meaning that w is a neighbor of v and bw,v is the weight according to which w influences v. If a vertex v is active, then p(v) : 1 holds, otherwise p(v) : 0 holds.Appendix B. ProofsProposition 3. Consider an SN S, a property p, and a GAP (cid:2). Let ψ be the interpretation lfp((cid:2)S ∪ (cid:2)∗lfp(((cid:2)p \ (cid:2)∗p) ∪ { A : ψ( A) | A ∈ A})(p(v)) for every vertex v of S.p). Then, lfp((cid:2) ∪ (cid:2)S )(p(v)) =Proof. All the rules in (cid:2) \ (cid:2)p have a predicate in the head atom that cannot reach predicate p, and then these rules do not affect the value of lfp((cid:2) ∪ (cid:2)S )(p(v)). As we are interested in computing the diffusion centrality for property p, we can ignore these rules in the computation. Moreover, rules in (cid:2)p can be partitioned into two sets of rules, namely p , we have that rules in ((cid:2)p \ (cid:2)∗(cid:2)∗p because an atom having predicate symbol q ∈ I(cid:2),p may appear in the head of a rule in (cid:2)∗p), but not vice versa. Thus, the rules in ((cid:2)p \ (cid:2)∗p and then the value of ψ can be pre-computed. (cid:2)p) do not contribute to the least fixed point of (cid:2)∗p and in the body of a rule in ((cid:2)p \ (cid:2)∗p) “depend on” rules in (cid:2)∗p), and by definition of (cid:2)∗p and ((cid:2)p \ (cid:2)∗Proposition 4. The Flickr model is p-monotonic and p-dwindling. The Jackson–Yariv model is p-monotonic but not p-dwindling. The SIR model is neither p-monotonic nor p-dwindling.Proof. The GAP describing the Flickr model is p-monotonic because the function μv(cid:6),v × μp × μq × dp F in the head of its unique rule is clearly monotonic. The GAP is also p-dwindling because the value of the function μv(cid:6),v × μp × μq × dp F is less than or equal to any value that μp can assume, as the annotations μv(cid:6),v , μq and the constant dp F can assume only values between 0 and 1.The GAP describing the Jackson–Yariv model is p-monotonic too. In fact, the function in the head of its unique rule is × dp J Y is a constant. However, the GAP describing the Jackson–Yariv model is × r((cid:8)monotonic as the term bicinot p-dwindling because of the term j E j) × 1(cid:8)(cid:8)j E jj ω j in the head atom function.The GAP describing the SIR model is not p-monotonic because of the terms (1 − R) and (1 − R(cid:6)) in the head atom × dp S I R ). Moreover, the GAP is not annotation function of the first rule (which is (1 − R) × μe(cid:6)) in the head p-dwindling. To see this, it is sufficient to note that, because of the presence of the terms (1 − R) and (1 − Ratom annotation function of the first rule, we cannot say that the value assumed by this function is always less than or equal to R or R(remember that I(cid:2)S I R ,p = {p, r1, r2}). (cid:2)v(cid:6) × (1 − R(cid:6)) × μqv× μpv(cid:6),v(cid:6)Proposition 5. The worst-case time complexity of Algorithm HyperLFP is O (|N| + 1αUmax = maxv∈V ,q∈I(cid:2),p(cid:6)[q][v]).(w − M· |H| · (log |H| + Umax · (Smax + log |H|))), where {|{h | h ∈ H ∧ q(v) ∈ S(h)}|}, Smax = maxh∈H {|S(h)|}, and α is the minimum value obtained at line 9 for C. Kang et al. / Artificial Intelligence 239 (2016) 70–9695Proof. The cost of making two copies of the matrix M at Lines 1–2 is O (|N|). The loop at Line 5 is executed |H| times, as at each iteration we remove an hyper-edge from Heap (Line 6). Within this loop, the predominant costs are the cost of deleting the maximum from Heap (Line 6), which is O (log |H|), and the cost of the loop at Line 10. This loop is executed (cid:6) ∈ U [q][v] whose number is Umax in the worst-case and at each iteration the cost of computing the for each hyper-edge h(cid:6)(cid:6)(cid:12) either to Heap (Line 16) or Heapfunction W at Line 11 is O (Smax) in the worst-case, while the cost of adding (cid:11)h(Line 18) is O (log |H|). Thus, the cost of executing Lines 5–19 is O (|N| + |H| · (log|H| + Umax · Smax)). Finally, the loop at Line 3 is executed 1α times in the worst-case as 1 is the maximum growth the annotation of an atom can have and α is the minimum increment step. (cid:2)(cid:6), wProposition 6. The worst-case time complexity of Algorithm HyperDC is O (|V | · (|N| + 1αwhere Umax = maxv∈V {|{h | h ∈ H ∧ v ∈ S(h)}|}, Smax = maxh∈H {|S(h)|}, and α is defined as in Proposition 5.· |H| · (log |H| + Umax · (Smax + log |H|)))), Proof. By leveraging Proposition 1, the HyperDC algorithm computes one least fix point for each vertex, so its worst-case time complexity is given by the number of vertices (|V |) times the worst-case time complexity of the HyperLFP algorithm (which is O (|N| + 1α· |H| · (log |H| + Umax · (Smax + log |H|)))) called at line 17. (cid:2)References[1] S. Adali, X. Lu, M. Magdon-Ismail, Local, community and global centrality methods for analyzing networks, Soc. Netw. Anal. Min. 4 (1) (2014) 210.[2] R.M. Anderson, R.M. May, Population biology of infectious diseases: part I, Nature 280 (5721) (1979) 361–367.[3] S. Aral, L. Muchnik, A. Sundararajan, Distinguishing influence-based contagion from homophily-driven diffusion in dynamic networks, Proc. Natl. Acad. [4] N. Barbieri, F. Bonchi, G. Manco, Topic-aware social influence propagation models, Knowl. Inf. Syst. 37 (3) (2013) 555–584.[5] M.A. Beauchamp, An improved index of centrality, Behav. Sci. 10 (2) (1965) 161–163.[6] R. Becker, Y. Chernihov, Y. Shavitt, N. Zilberman, An analysis of the steam community network evolution, in: Proc. Convention of Electrical & Electronics Sci. 106 (51) (2009) 21544–21549.Engineers in Israel, 2012, pp. 1–5.[7] P. Bonacich, Factoring and weighting approaches to status scores and clique identification, J. Math. Sociol. 2 (1) (1972) 113–120.[8] U. Brandes, A graph-theoretic perspective on centrality, Soc. Netw. 30 (2) (2008) 136–145.[9] S. Brin, L. Page, The anatomy of a large-scale hypertextual web search engine, Comput. Netw. 30 (1–7) (1998) 107–117.[10] M. Broecheler, P. Shakarian, V.S. Subrahmanian, A scalable framework for modeling competitive diffusion in social networks, in: Proc. International Conference on Social Computing, 2010, pp. 295–302.ing Database Technology, 2014, pp. 295–306.[11] Çigdem Aslay, N. Barbieri, F. Bonchi, R.A. Baeza-Yates, Online topic-aware influence maximization queries, in: Proc. International Conference on Extend-[12] D. Centola, The spread of behavior in an online social network experiment, Science 329 (5996) (2010) 1194–1197.[13] M. Cha, A. Mislove, P.K. Gummadi, A measurement-driven analysis of information propagation in the Flickr social network, in: Proc. International World Wide Web Conference, 2009, pp. 721–730.[14] N. Chen, On the approximability of influence in social networks, SIAM J. Discrete Math. 23 (3) (2009) 1400–1415.[15] W. Chen, C. Wang, Y. Wang, Scalable influence maximization for prevalent viral marketing in large-scale social networks, in: Proc. International Con-ference on Knowledge Discovery and Data Mining, 2010, pp. 1029–1038.[16] Y. Chen, W. Peng, S. Lee, Efficient algorithms for influence maximization in social networks, Knowl. Inf. Syst. 33 (3) (2012) 577–601.[17] C.-Y. Chiang, L.-H. Huang, B.-J. Li, J. Wu, H.-G. Yeh, Some results on the target set selection problem, J. Comb. Optim. 25 (4) (2013) 702–715.[18] S. Dolev, Y. Elovici, R. Puzis, Routing betweenness centrality, J. ACM 57 (4) (2010).[19] D. Fierens, G.V. den Broeck, J. Renkens, D.S. Shterionov, B. Gutmann, I. Thon, G. Janssens, L.D. Raedt, Inference and learning in probabilistic logic programs using weighted boolean formulas, Theory Pract. Log. Program. 15 (3) (2015) 358–401.[20] L.C. Freeman, A set of measures of centrality based on betweenness, Sociometry 40 (1) (1977) 35–41.[21] L.C. Freeman, Centrality in social networks conceptual clarification, Soc. Netw. 1 (3) (1979) 215–239.[22] W.S. Fung, R. Hariharan, N.J.A. Harvey, D. Panigrahi, A general framework for graph sparsification, in: Proc. ACM Symposium on Theory of Computing, [23] A. Goyal, W. Lu, L.V.S. Lakshmanan, SIMPATH: an efficient algorithm for influence maximization under the linear threshold model, in: Proc. International 2011, pp. 71–80.Conference on Data Mining, 2011, pp. 211–220.[24] M. Granovetter, Threshold models of collective behavior, Am. J. Sociol. 83 (6) (1978) 1420–1443.[25] A. Graves, S. Adali, J. Hendler, A method to rank nodes in an RDF graph, in: Proc. International Semantic Web Conference, 2008.[26] H.W. Hethcote, Qualitative analyses of communicable disease models, Math. Biosci. 28 (3–4) (1976) 335–356.[27] M.T. Irfan, L.E. Ortiz, On influence, stable behavior, and the most influential individuals in networks: a game-theoretic approach, Artif. Intell. 215 (2014) [28] M. Jackson, L. Yariv, Diffusion on social networks, Econ. Publ. 16 (1) (2005) 69–82.[29] Q. Jiang, G. Song, G. Cong, Y. Wang, W. Si, K. Xie, Simulated annealing based influence maximization in social networks, in: Proc. AAAI Conference on [30] V. Kagan, A. Stevens, V.S. Subrahmanian, Using Twitter sentiment to forecast the 2013 Pakistani election and the 2014 Indian election, IEEE Intell. Syst. [31] C. Kang, C. Molinaro, S. Kraus, Y. Shavitt, V.S. Subrahmanian, Diffusion centrality in social networks, in: Proc. International Conference on Advances in Social Networks Analysis and Mining, 2012, pp. 558–564.[32] G. Karypis, METIS and parmetis, in: Encyclopedia of Parallel Computing, 2011, pp. 1117–1124.[33] D. Kempe, J.M. Kleinberg, É. Tardos, Maximizing the spread of influence through a social network, in: Proc. International Conference on Knowledge 79–119.Artificial Intelligence, 2011.30 (1) (2015) 2–5.[34] D. Kempe, J.M. Kleinberg, É. Tardos, Influential nodes in a diffusion model for social networks, in: Proc. International Colloquium on Automata, Lan-[35] M. Kifer, V.S. Subrahmanian, Theory of generalized annotated logic programming and its applications, J. Log. Program. 12 (3 & 4) (1992) 335–367.[36] M. Kimura, K. Saito, H. Motoda, Efficient estimation of influence functions for SIS model on social networks, in: Proc. International Joint Conference on Discovery and Data Mining, 2003, pp. 137–146.guages and Programming, 2005, pp. 1127–1138.Artificial Intelligence, 2009, pp. 2046–2051.96C. Kang et al. / Artificial Intelligence 239 (2016) 70–96[37] M. Kimura, K. Saito, R. Nakano, Extracting influential nodes for information diffusion on a social network, in: Proc. AAAI Conference on Artificial Intelligence, 2007, pp. 1371–1376.[38] J. Leskovec, The Stanford Large Network Dataset Collection, http://snap.stanford.edu/data/index.html, 2015.[39] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J.M. VanBriesen, N.S. Glance, Cost-effective outbreak detection in networks, in: Proc. International Conference on Knowledge Discovery and Data Mining, 2007, pp. 420–429.[40] J.J. Lu, A. Nerode, V.S. Subrahmanian, Hybrid knowledge bases, IEEE Trans. Knowl. Data Eng. 8 (5) (1996) 773–785.[41] M. Mathioudakis, F. Bonchi, C. Castillo, A. Gionis, A. Ukkonen, Sparsification of influence networks, in: Proc. International Conference on Knowledge Discovery and Data Mining, 2011, pp. 529–537.[42] M. McPherson, L. Smith-Lovin, J.M. Cook, Birds of a feather: homophily in social networks, Annu. Rev. Sociol. 27 (2001) 415–444.[43] A. Memory, A. Kimmig, S.H. Bach, L. Raschid, L. Getoor, Graph summarization in annotated data using probabilistic soft logic, in: Proc. International Workshop on Uncertainty Reasoning for the Semantic Web, 2012, pp. 75–86.[44] C. Minoiu, C. Kang, V.S. Subrahmanian, A. Berea, Does financial connectedness predict crises?, Quant. Finance 15 (4) (2015) 607–624.[45] M.B. Naseri, G. Elliott, Role of demographics, social connectedness and prior internet experience in adoption of online shopping: applications for direct marketing, J. Target. Meas. Anal. Mark. 19 (2) (2011) 69–84.[46] J. Nieminen, On the centrality in a graph, Scand. J. Psychol. 15 (1) (1974) 332–336.[47] J. Pearl, S. Russell, Bayesian Networks, Computer Science Department, University of California, 1998.[48] M. Purohit, B.A. Prakash, C. Kang, Y. Zhang, V.S. Subrahmanian, Fast influence-based coarsening for large networks, in: Proc. International Conference on Knowledge Discovery and Data Mining, 2014.[49] G. Sabidussi, The centrality index of a graph, Psychometrika 31 (1966) 581–603.[50] T. Schelling, Micromotives and Macrobehavior, W.W. Norton and Co., 1978.[51] P. Shakarian, M. Broecheler, V.S. Subrahmanian, C. Molinaro, Using generalized annotated programs to solve social network diffusion optimization problems, ACM Trans. Comput. Log. 14 (2) (2013) 10.[52] P.L. Szczepanski, T.P. Michalak, M. Wooldridge, A centrality measure for networks with community structure based on a generalization of the Owen value, in: Proc. European Conference on Artificial Intelligence, 2014, pp. 867–872.[53] J. Tang, J. Sun, C. Wang, Z. Yang, Social influence analysis in large-scale networks, in: Proc. International Conference on Knowledge Discovery and Data Mining, 2009, pp. 807–816.Mining, 2011, pp. 965–973.[54] H. Toivonen, F. Zhou, A. Hartikainen, A. Hinkka, Compression of weighted graphs, in: Proc. International Conference on Knowledge Discovery and Data [55] H. Toivonen, F. Zhou, A. Hartikainen, A. Hinkka, Network compression by node and edge mergers, in: Bisociative Knowledge Discovery – An Introduction to Concept, Algorithms, Tools, and Applications, 2012, pp. 199–217.[56] C. Wang, W. Chen, Y. Wang, Scalable influence maximization for independent cascade model in large-scale social networks, Data Min. Knowl. Discov. 25 (3) (2012) 545–576.[57] D. Watts, J. Peretti, Viral marketing for the real world, Harv. Bus. Rev. (2007).