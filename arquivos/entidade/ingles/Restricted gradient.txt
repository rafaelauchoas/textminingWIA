Artificial Intelligence 172 (2008) 454–482www.elsevier.com/locate/artintRestricted gradient-descent algorithm for value-functionapproximation in reinforcement learningAndré da Motta Salles Barreto a,∗, Charles W. Anderson ba Programa de Engenharia Civil/COPPE, Universidade Federal do Rio de Janeiro, Rio de Janeiro, RJ, Brazilb Department of Computer Science, Colorado State University, Fort Collins, CO 80523, USAReceived 22 May 2006; received in revised form 22 August 2007; accepted 23 August 2007Available online 6 September 2007AbstractThis work presents the restricted gradient-descent (RGD) algorithm, a training method for local radial-basis function networksspecifically developed to be used in the context of reinforcement learning. The RGD algorithm can be seen as a way to extractrelevant features from the state space to feed a linear model computing an approximation of the value function. Its basic idea is torestrict the way the standard gradient-descent algorithm changes the hidden units of the approximator, which results in conservativemodifications that make the learning process less prone to divergence. The algorithm is also able to configure the topology of thenetwork, an important characteristic in the context of reinforcement learning, where the changing policy may result in differentrequirements on the approximator structure. Computational experiments are presented showing that the RGD algorithm consistentlygenerates better value-function approximations than the standard gradient-descent method, and that the latter is more susceptible todivergence. In the pole-balancing and Acrobot tasks, RGD combined with SARSA presents competitive results with other methodsfound in the literature, including evolutionary and recent reinforcement-learning algorithms.© 2007 Elsevier B.V. All rights reserved.Keywords: Reinforcement learning; Neuro-dynamic programming; Value-function approximation; Radial-basis-function networks1. IntroductionSutton and Barto [77] and Kaelbling et al. [37] describe reinforcement learning as a class of problems, rather thanas a set of techniques. In this paradigm, an agent must learn how to act through direct interaction with an environment.The only information available to the agent is a reinforcement signal providing evaluative feedback for the decisionsmade.This rather informal definition is sufficient to explain the increasing interest in the field from the artificial intelli-gence and machine learning communities. First of all, this framework is a crude but appealing model of what actuallyhappens in nature, and the analogy with animal behavior is almost irresistible [64,76]. From an engineering perspec-tive, the reinforcement-learning paradigm is also tempting, since it transfers to the learning system the burden offiguring out how to accomplish a task. This way, instead of providing a set of examples with the desired behavior, as* Corresponding author.E-mail addresses: andremsb@lncc.br (A. da Motta Salles Barreto), anderson@cs.colostate.edu (C.W. Anderson).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.08.001A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482455in supervised learning, the designer is left with the much simpler task of representing a problem in terms of rewardand punishment signals only. If, for example, one wanted to have a mobile robot travel from one point to another, itcould be done by simply giving the robot a reward when the goal was reached.In order for the above scenario to be true, however, several obstacles other than those in the robot’s trajectorymust be overcome. One of the major difficulties arises from the combination of reinforcement learning and functionapproximation. If one wants to solve real-world reinforcement learning tasks—where the number of possible statesof the system is usually too large to allow an exhaustive exploration—it is necessary to provide the agent with theability to generalize. Nevertheless, as is now well known (though not completely understood), the combination ofreinforcement learning algorithms with function approximators can easily become unstable, and finding a feasibleway to merge both paradigms is today an active area of research in machine learning.This paper presents an attempt in this direction, namely a training method for local radial-basis function networksespecially designed to be used in the context of reinforcement learning. The restricted gradient-descent (RGD) algo-rithm is essentially a modified version of the standard gradient-descent method, and as such it inherits both its qualitiesand drawbacks. However, the restrictions imposed by RGD on the application of gradient-descent’s delta rule make itless prone to divergence. In the RGD algorithm the center of the radial functions are always moved toward the currentstate, which tends to confine them to the convex-hull formed by the training data. Also, the widths of the radial-basisfunctions are only allowed to shrink, and thus they can not possibly diverge to infinity. Obviously, these restrictionson the delta rule limit the trajectory of the approximator’s parameter vector in the parameter space. This lost of flexi-bility is compensated by the on-line allocation of new hidden units, which results in a monotonically increasing of theapproximation granularity.This work is organized as follows. Section 2 presents a brief review of reinforcement learning, focusing on methodsthat rely on the concept of a value function to address this problem. As mentioned, the stability of such methods canbe affected by the use of function approximators. This issue is discussed in more detail in Section 3. One way toalleviate the instability caused by the use of approximators is to adopt linear models operating on features extractedfrom the original state space. Section 4 discusses the concept of “features” used in this paper, as well as a way toselect them using local radial-basis functions. Section 5 presents the RGD algorithm, which can be seen as a strategyto extract relevant features from the state space. This algorithm is applied in Section 6 to a series of computationalexperiments. Particularly, the performance of RGD is compared to that of the standard gradient-descent method andto several traditional reinforcement-learning algorithms using other techniques to approximate the value function.Section 7 presents an overall analysis of the experiments and a hypothesis explaining the stable behavior of therestricted gradient-descent algorithm. Finally, in Section 8 the main conclusions about this research are presented, andsome possible directions for future work are discussed.2. Reinforcement learningThe goal of the agent in reinforcement learning is to find a policy π —a mapping from states to actions—thatmaximizes the expected return. The return Rt is the total reward the agent receives in the long run from time t on:Rt = rt+1 + γ rt+2 + γ 2rt+3 + · · · =∞(cid:2)i=0γ irt+i+1,where γ ∈ [0, 1] is the discount factor. This parameter determines the relative importance of the individual rewards,depending on how far in the future they are.The rewards r ∈ (cid:5) are given by the environment to the agent each time it performs an action. Usually, this inter-action happens in discrete steps: at each time step t, the agent selects an action a ∈ A(st ) as a function of the currentstate st ∈ S. The sets S and A(st ) are part of the environment and represent the possible states of the system and theavailable actions in each state st . As a response to the action a, the agent receives from the environment a rewardrt+1 and a new state st+1. This loop is repeated indefinitely or until the agent reaches a terminal state. The interactionbetween agent and environment can be formalized as a Markov decision process [10,14,54].One way to address the reinforcement-learning problem is to search for a good solution in the policy space directly.This could be done, for example, by parametrizing π and then have an evolutionary algorithm search for good policies[35,88]. Another approach is to compute an approximation of the gradient of the average reward with respect to theparametrized policy, which can be used to perform gradient ascent [7,8,73].456A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482Another way to deal with the reinforcement learning problem is to use methods derived from dynamic program-ming [9,14,54]. One advantage of this approach is the fact that dynamic programming has been studied for a longtime, and is now supported by a strong and well understood theoretical basis. Central to the dynamic programmingapproach is the concept of a value function. The action-value function of a given policy π associates each state–actionpair (s, a) with the expected return when performing action a in state s and following π thereafter:Qπ (s, a) ≡ Eπ {Rt | st = s, at = a},where Eπ { } denotes the expected value when following policy π . Since the notation above is widely adopted, theaction-value function is usually referred to as the Q-function. Once the Q-function of a particular policy πk is known,we can derive a new policy, πk+1, which is greedy with respect to Qπk (s, a):πk+1(s) = arg maxa∈A(s)Qπk (s, a).(1)The policy πk+1 is guaranteed to be at least as good as (if not better than) the policy πk. This is the fundamentalidea of the reinforcement learning algorithms based on dynamic programming: given an initial arbitrary policy π0,compute its value function Qπ0 (s, a) and then generate a better policy π1 which is greedy with respect to this function.The next step is to compute Qπ1 (s, a), use it to generate a new policy π2, and so on. Under certain assumptions, thesuccessive alternation of these two steps—policy evaluation and policy improvement—can be shown to converge tothe optimal policy π ∗, which maximizes the expected return on every state.Of course, the above process can be executed in different levels of granularity. It is not necessary, for example, tohave an exact Q-function Qπk (s, a) to perform the policy improvement step. One can compute a rough approximationof this function and then use it to generate a new policy πk+1. The improvement step itself can be performed indifferent levels. Eq. (1) could be used to update the policy πk on a single state, a set of them, or on the entire state spaceS. The exact way the policy evaluation and policy improvement steps are performed defines the different reinforcementlearning algorithms.2.1. Computing the value functionA complete control problem can be broken into two stages: policy evaluation and policy improvement. The policyimprovement step is usually easy to compute. This is especially true if A(st ) is finite and small for every st , in whichcase it is reduced to the computation of the “max” operator in (1). Therefore, much of the effort in reinforcementlearning research is devoted to the evaluation problem, that is, given a policy π how to compute its value function.Suppose the state space is a finite set. One way to do policy evaluation is to use sample trajectories to compute theaverage return associated with each state–action pair [5,45]:k (s, a) = 1Qπkk(cid:2)i=1Rit , with st = s and at = a,(2)t are actual returns following the visits to (s, a). It can be shown that the sequence Qπwhere Ri∞ asymp-totically approaches the true Q-function of π [77]. One drawback of this approach is the fact that it can be naturallyapplied only to tasks where the interaction between agent and environment can be broken into subsequences or “trials”,as in a maze, for example, where a new trial starts every time the agent reaches the goal. We call each subsequence anepisode [77].2 , . . . , Qπ1 , QπEven if the task can be subdivided into episodes, when using (2) the agent does not learn anything during eachinteraction, but only between them, when the collected data can be used to update Qπk . It is possible for the agentto learn while interacting with the environment. This is because of a recursive relation between states known as theBellman equation [9]:(cid:3)rt+1 + γ Qπ(cid:7)ss(cid:7) + γ QπRaP ass(cid:7)(cid:4)(cid:5)st+1, π(st+1)(cid:5)(cid:8)(cid:7),)Qπ (s, a) = Eπ(cid:2)| st = s, at = a, π(s(cid:4)s(3)=(cid:6)(cid:7)s(cid:7)where P areward of this transition.ss(cid:7) is the probability of reaching state s(cid:7) when in state s and performing action a and Rass(cid:7) is the expectedA. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482457Eq. (3) is one of the most important results in dynamic programming, and is the core of most value-function basedreinforcement-learning methods. First of all, Qπ is the unique solution for its Bellman equation, and thus solving (3)for every state–action pair corresponds to finding the true Q-function of π . Furthermore, the Bellman equation makesit possible to update the Q-values of a state s based on the estimated value of its successors. This is what Sutton andBarto [77] call bootstrapping, and is the basic mechanism through which an agent can learn while interacting with itsenvironment.If the environment’s dynamics are completely known—that is, if P ass(cid:7) are given—the Bellman equationsof all state–action pairs can be written as a system of linear equations whose solution is the true value function of π .One way to solve this system is to use iterative methods in which the approximation of Qπ is successively refinedbased on its past estimates [14,54]:ss(cid:7) and RaQπk+1(s, a) =P ass(cid:7)ss(cid:7) + γ QπRak(cid:4)s(cid:7), π(s(cid:7)(cid:5)(cid:8).)(cid:2)(cid:7)s(cid:7)(4)ss(cid:7) and RaThe above algorithm is known as iterative policy evaluation. It can be applied synchronously, where the old valuesQπk (s, a) are kept during one iteration, or in a Gauss–Seidel style, in which the most recent available values are usedto make up the targets for the updates. In both cases, however, a model of the system is required. This restriction canbe removed if P ass(cid:7) are estimated from sample transitions from the environment or a generative model. Thisis the basic idea of the temporal-difference learning methods [74], whose update rule can be written as:(cid:7)− Qπ(5)where r is the reward received in one transition from state s to s(cid:7) and α ∈ [0, 1] is the learning rate. Temporal-difference (TD) learning is one of the central ideas of reinforcement learning. It makes it possible for the agent tolearn while interacting with the environment without knowledge of its dynamics. With the use of eligibility traces,the TD method can update more than one Q-value at each iteration. This generates a family of algorithms known asTD(λ) [67].(cid:7)r + γ Qπkk+1(s, a) = Qπ(cid:8)k (s, a)k (s, a) + α, π(s(cid:4)sQπ(cid:5)),(cid:7)Eq. (5) can be modified to deal with the complete control problem, that is, besides the evaluation step, also toperform the policy improvement step. This is done by incorporating the “max” operator into the update rule, and theresulting algorithm is called Q-learning [83]. If the policy improvement step is applied after each temporal-differenceupdate—that is, if (1) and (5) are applied alternately—one has the SARSA algorithm [57,75].The algorithms represented by (4) and (5) and their control counterparts are guaranteed to converge to the truevalue function Qπ if certain assumptions are respected [14,24,36,66]. One of these assumptions is that Qπk is storedin a lookup-table, with one entry for each state–action pair. This makes the application of the standard version of thesemethods to large or continuous state spaces infeasible.3. Reinforcement learning and function approximationIn real-world reinforcement learning tasks, it is usually not possible to visit every state–action pair (s, a), andthus the agent must be able to generalize from its limited experience. In the context of dynamic-programming basedalgorithms, this means the value function must be represented by a function approximator.Generalization from examples has been studied in artificial intelligence for many years, and there is now a greatbody of research on function approximation that can be borrowed from supervised learning. However, the combinationof reinforcement learning and function approximators is not a straightforward task, for several reasons:(1) In reinforcement learning, it is important that learning occurs on-line, while the agent interacts with the environ-ment. This requires methods that are able to deal with incrementally acquired data, instead of a static trainingset [77,85].(2) Also, if a bootstrapping method is adopted, the target values to be used as training examples—the right-hand sideof (4) and (5), for example—are highly non-stationary. If the policy changes during the learning process, thisproblem can get even worse, since the distribution from which the training examples are picked may also changeover time [55].(3) As observed by Boyan and Moore [19] and Gordon [32], even if the function approximator is able to approximatethe final value function Qπ , it might not be able to represent the intermediary versions Qπk of this function.458A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482(4) In the complete control problem, even if a good approximation of the optimal value function Q∗ is found, theapproximation error may cause the resulting policy π to perform very poorly [13,58,68,89].Despite all these difficulties, reinforcement learning with function approximators has been successfully appliedin various domains. Classical examples in game playing are the Samuel’s checker player [60,61] and Tesauro’s TD-Gammon, a program based on reinforcement learning which is able to play backgammon near the level of the world’sstrongest grandmasters [79]. The reinforcement learning paradigm has also been applied to different sequential de-cision tasks, such as shop-schedule strategies for NASA space shuttle missions [90], elevator dispatch control [23],dynamic channel allocation in cellular telephone networks [65] and regulation of an irrigation network [34]. In ro-botics, reinforcement learning algorithms combined with function approximation have been used, for example, fornavigation control [25,41] and to help robots to walk [12] and to play soccer [72].The contrast between the theoretical obstacles and the practical successes in applying reinforcement learning withfunction approximators resulted in great interest in the area. Some of the earlier works presented discouraging re-sults, with simple counterexamples in which the most popular methods would fail dramatically [4,19,32,81]. Later,Sutton [75] showed that some of these counterexamples could be solved when using a linear approximator and on-line state sampling (that is, sampling the transitions according to the current policy π ). Based on this observation,Tsitsiklis and Roy [82] proved that TD(λ) with this configuration converges with probability 1. This result createda strong tendency toward the use of linear models in reinforcement learning, for which several theoretical analysesexist [53,56,63,78]. The strongest results in the current literature are those regarding the convergence of control algo-rithms with linear function approximators [33,39,48,49]. Among these, kernel-based reinforcement learning [48] andthe least-squares policy iteration algorithm (LSPI) [39] deserve special attention, and will be discussed further in thetext.4. Features as local basis functionsAs mentioned, recent theoretical and practical results in the reinforcement-learning literature consider the use oflinear approximators. One way to represent the Q-function in the case of discrete actions is to have one linear modelfor each possible action:˜Qπa (s) =m(cid:2)i=1wai θi(s),for all a ∈ A(s)(6)where wato all (cid:8)wa).i are the linear weights associated with action a and θi(s) are the m features representing state s (commonThe concept of feature used here is the same used by Tsitsiklis and Roy [82], and corresponds to what kind ofinformation the agent extracts from the environment. The definition of a suitable feature space is a fundamentalstep for any reinforcement learning method, and is completely task-dependent. To illustrate this statement, one cancompare two well known card games, blackjack and poker. While in the first one the values of the card ranks areenough to derive a good strategy, an agent unable to distinguish the suits on the cards would not perform very wellplaying poker.Usually, the identification of which features should be used is not a trivial task. If one has enough knowledge aboutthe domain of interest, it might be a good strategy to handcraft the structure of the linear approximators and to adhereas tightly as possible to the frameworks for which guarantees exist. If this is not the case, it might be desirable toautomate this process, since the alternative would be an expensive trial-and-error procedure.4.1. Local radial-basis functionsOne way to extract features from the state space is to use basis functions [81]. In this way, each feature θi(s) isa function mapping S into (cid:5). It has been claimed in the literature that the functions θi should be local, that is, theyshould present a significant activation in only a limited region of the state space [3,69,80]. The main motivation forthis is to avoid a phenomenon known as interference [26]. Interference happens when the update of one state–actionpair changes the Q-values of other pairs, possibly in the wrong direction.A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482459Interference is naturally associated with generalization, and also happens in conventional supervised learning.Nevertheless, in the reinforcement learning paradigm its effects tend to be much more harmful. The reason for thisis twofold. First, the combination of interference and bootstrapping can easily become unstable, since the updatesare no longer strictly local. The convergence proofs for the algorithms derived from (4) and (5) are based on the factthat these operators are contraction mappings, that is, their successive application results in a sequence convergingto a fixed point which is the solution for the Bellman equation [14,36]. When using approximators, however, thisasymptotic convergence is lost, since the update of one state–action pair (s, a) can change the Q-values of other pairs(si, aj ), with si (cid:9)= s or aj (cid:9)= a. In this case, the approximation errors on the points (si, aj ) may increase, and if ˜Qπ(si)ajare subsequently used as the target values for other updates (as in (5)), the errors are passed on, easily spreading outover the entire domain.Another source of instability is a consequence of the fact that in on-line reinforcement learning the distributionof the incoming data depends on the current policy. Depending on the dynamics of the system, the agent can remainfor some time in a region of the state space which is not representative of the entire domain. In this situation, thelearning algorithm may allocate excessive resources of the function approximator to represent that region, possibly“forgetting” the previous stored information [85].One way to alleviate the interference problem is to use a local function approximator. The more independent eachbasis function is from each other, the less severe this problem is (in the limit, one has one basis function for eachstate, which corresponds to the lookup-table case) [86]. A class of local functions that have been widely used forapproximation is the radial basis functions (RBFs) [52]. The characteristic feature of local RBFs is the fact that theirvalue decreases monotonically with the distance from a central point, called the center (cid:8)c of the radial function. Thewidth σ of the RBF determines how fast its value drops. As first noticed by Broomhead and Lowe [21] and Poggioand Girosi [51], the radial basis function paradigm for approximation can be structured as an artificial neural network.In this case, the RBFs are the activation functions of the network’s hidden units. It has been shown that, if there isno limit on the number of radial functions available and their centers and widths are adapted during training, an RBFnetwork is a universal approximator [29].5. Restricted gradient-descent algorithmIt is possible to use an RBF network to approximate the reinforcement-learning value function. In this case, definingthe hidden layer of the network corresponds to determining the structure of the feature space wherein the linear modelwill operate. This is not a trivial task: the configuration of an RBF-network hidden layer requires the definition of thenumber of radial functions, their centers and widths.This section presents the restricted gradient-descent algorithm, a training method for local RBF networks developedto be used in the context of reinforcement learning. This means it can operate on-line, deal with non-stationary dataand adapt the network topology according to the complexity of the value function. To achieve this the RGD algorithmrelies on two basic mechanisms. The first of them, discussed in Section 5.1, is the strategy adopted to allocate newunits for the hidden layer. Section 5.2 presents the second ingredient of RGD, namely the mechanism through whichfeatures to be used in the value-function approximation are determined. Finally, in Section 5.3 the complete RGDalgorithm is presented, and a pseudo-code for the case in which it is combined with TD learning is given.The use of local radial functions in reinforcement learning is by no means a new idea. See, for example, [3,44,48,58,59], just to cite a few. Instead of going over the details of each previous attempt to use RBF networks toapproximate the value function, we simply point out the similarities and differences of each approach with respect toour algorithm while RGD’s characteristics are presented. For a throughout review, the reader is redirected to Ratitch’sPhD thesis [55].5.1. Dynamic allocation of resourcesOne decision that has a strong impact on the performance of an RBF network is the number of units in its hiddenlayer. If there are too few units (or features), the model might not be able to represent the value function with thenecessary accuracy to generate a good policy. On the other hand, an excessive number of hidden nodes makes thelearning process much slower, besides potentially harming the generalization capability of the network.460A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482It is desirable to be able to adapt the topology of the approximating model according to the complexity of the valuefunction being approximated. This is especially true in the complete control problem, since the value function changesevery time a policy-improvement step is performed. When using local RBF networks, it is possible to allocate newhidden units “on-demand”, based on the pattern of activity of the network. One can, for example, place new RBFs inthe regions of the state space where the approximation error is unusually large [27,50].The allocation of new units based on the approximation error requires the definition of either a schedule or athreshold to trigger the process [59]. Usually, both values are very domain specific. Another idea is to try to “cover”the relevant areas of the state space equally, assuring that each state visited by the agent results in a minimum activationlevel of the RBF network. If after the presentation of a state (cid:8)s the activation of the network is below a specific thresholdτ , a new RBF is added to the model, with its center coincident with (cid:8)s [46,55].1 One advantage of using such a strategyis that the threshold τ can be defined independently of the domain, and can be seen as a way to control the overlap—and thus the interference—between RBFs.The latter strategy has still another advantage: it can be used to guarantee a regular coverage of the state space evenif the granularity of the approximator dynamically increases (that is, if the widths of the RBFs decrease over time).This is an essential characteristic that makes the adaptive process of the RGD algorithm possible, as described in thenext section.5.2. Defining features based on the approximation errorAs noted by Sutton and Barto [77], in reinforcement learning the structure of the approximator should be relatedto the complexity of the value function. Different regions in the domain of this function may require different levelsof granularity, and ideally the approximator’s structure would reflect this requirement. A strong indication of theneed for a finer granularity is a large approximation error. However, using this information to determine the networktopology is not straightforward, since any error function is discontinuous with respect to the number of RBFs in thenetwork’s hidden layer. Since the gradient of the error function with respect to the number of hidden units is notcomputable, one can try to detect the need for a finer grain indirectly. Particularly, if the conventional gradient-descentmethod successively reduces the RBFs’ widths, new radial functions can be added in the areas of the state space leftuncovered.Given a target value Qπ (s, a) (which can be obtained based on (2), (4) or (5)), suppose the objective is to minimizethe following weighted euclidean-norm:(cid:2)(cid:2)ξ1 =ρε2 =(cid:4)ρ(s, a)Qπ (s, a) − ˜Qπ(cid:5)a (s)2,(7)(s,a)i . If εwawhere ρ(s, a) is a distribution weighting the errors of different state–action pairs. In this case, the incremental gradient-descent algorithm will change the parameters of a local RBF in two distinct ways, depending on the relationshipbetween the approximation error ε and the RBF’s output weights wai > 0, the width σi of the radial basisfunction will be enlarged and its center (cid:8)ci will be moved toward the current state (cid:8)s, as shown in Fig. 1. If εwai < 0, theopposite changes will be performed: the RBF’s width will be reduced and its center moved away from the current state(Fig. 2). A more formal presentation with the update equations for the Gaussian function can be found in Appendix A.The indiscriminate application of the updates described above can easily lead to divergence. In bootstrappingreinforcement-learning the target values used to update ˜Qπ (·) depend on the current approximation of the Q-function.Notice, for example, how in (4) and (5) the computation of a new estimate ˜Qπk (·).So, if there is a source of error in the updates, this error will be amplified at each iteration, and the cumulative effectof this will be an unbounded growth of the approximation error. There is some evidence in the literature that one sucha source of error is an exaggeration of the Q-values [32,48,56,80]. The underlying idea is that the combination of the“max” operator in (1) with the inevitable imprecision on the estimate ˜Qπk+1(·) upward. In thecase of local RBFs, this systematic overestimation will result in a never-stopping increase of the functions’ widths(see Fig. 1(b)). In fact, this phenomenon was observed in our preliminary experiments with reinforcement learningand RBF networks, and was one of the motivations for the development of the RGD algorithm.k+1(·) depends on the previous ˜Qπk (·) is likely to bias ˜Qπ1 We use ‘(cid:8)s ’ instead of ‘s’ whenever we want to emphasize that the state s is a vector, that is, S ⊂ (cid:5)n.A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482461(a) Original RBF(b) Width is enlarged(c) Center is moved toward the current stateFig. 1. Changes made when εwai > 0.(a) Original RBF(b) Width is reduced(c) Center is moved away from the current stateFig. 2. Changes made when εwai < 0.The idea of RGD is to restrict the modifications performed by the standard gradient-descent algorithm to theRBFs’ parameters. If the widths of the radial functions are only allowed to “shrink”, one has an approximator whosegranularity is steadily increasing. Obviously, this process will leave some areas of the state space uncovered, whichcan be compensated by the allocation of new hidden units. As the reduction of the RBFs’ widths is error driven, theresulting procedure is a training algorithm which increases the number of features used by the linear model basedindirectly on the approximation error.5.3. AlgorithmAlgorithm 1 shows a pseudo-code of RGD combined with TD(0). Notice that this is essentially the TD-learningalgorithm using a linear approximator, except for the block of code detached between dotted lines. The extension forthe case where λ > 0 as well as for the control algorithms is straightforward.As shown in Algorithm 1, the first step of the RGD method is to check whether the most activated unit is belowthe threshold τ . If so, a new RBF θm+1 is added to the network, coincident with the current state (cid:8)s. The width σm+1of the new RBF determines the overlap between this function and its neighbors, and its definition together with τallows for some control on the level of interference between the functions. If σm+1 and τ are sufficiently small, thechanges performed by the RGD algorithm on the hidden layer may be restricted to the most activated unit θi , sincethe activation level of the others will usually not be very high. The changes on the RBF’s parameters depend on εwai ,as discussed before. If εwai < 0, the width of theradial function is reduced, and no changes are made to its center.i > 0, (cid:8)ci is moved towards (cid:8)s, and its width is left unaltered; if εwaReducing the RBFs’ widths has two desirable effects. First, it decreases the overlap between functions, which helpsto maintain the locality of the model. Also, since the reduction is proportional to the approximation error, the size ofthe radial functions tends to reflect the shape of the value function, with narrower RBFs in the areas where it is more462A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482loopinitialize (cid:8)sa ← π((cid:8)s)repeat{action given by π for (cid:8)s}((cid:8)s2) − ˜Qπperform action a and observe next state (cid:8)s2 and reward ra2 ← π((cid:8)s2)ε ← r + γ ˜Qπ{compute TD-error}a2(cid:8)wa ← (cid:8)wa + αε (cid:8)θ{update the linear weights}. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .{find the most activated unit}θi ← maxj θj ((cid:8)s)if θi < τ thena ((cid:8)s)elseallocate new hidden unit θm+1i > 0 then (cid:8)ci ← (cid:8)ci − β ∂ε2∂ (cid:8)ciif εwaelse σi ← σi − β ∂ε2∂σi{move (cid:8)ci towards (cid:8)s}{decrease σi}end if. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .(cid:8)s ← (cid:8)s2a ← a2until state (cid:8)s is terminalend loopAlgorithm 1. RGD-TD(0).complex. If the learning rate β is small, moving the centers of the radial functions always in the direction of thecurrent state tends to confine the RBFs in the convex-hull defined by the training data-points. This might also be adesirable property, since the approximator’s resources will be concentrated in the regions of the state space where thedata lies [69].As mentioned before, the widths of the RBFs should be defined in order to guarantee some overlap between thefunctions. One way to do that is to define σm+1 so that the activation of the new RBF θm+1 will equal τ at the centerof the most activated unit θi ; that is:σm+1 = σθm+1((cid:8)ci; (cid:8)cm+1, σ ) = τ,such that(8)where the dependency of θm+1 on the parameters (cid:8)cm+1 and σ has been emphasized. Notice that when using thisstrategy the width of a new RBF will depend on the widths of the previous ones: the larger the width of its neighborθi , the larger σm+1 will be, since the threshold τ will be “triggered” further from (cid:8)ci . Suppose we start with a singleRBF in the network. How, then, to define the width σ1 of this function? Clearly, an underestimated σ1 will result in anexcessive number of hidden units in the final network generated by RGD. As the RBFs’ widths are constantly reducing,however, an overestimated value for this parameter should not harm the approximation. This issue is discussed in moredetail in Section 6.1.Another question that arises naturally is when to stop shrinking the RBFs: as the minimization is being made inthe least-squares sense, the otherwise local minima will generate approximation residuals in both directions, and thealternate signs of εwai will result in a never-stopping decrease of the RBFs’ widths. The decision on when to stopthe shrinking process is left to the user, who should define an appropriate stop criterion depending on the task. Thisis where specific domain knowledge should be incorporated to the process such as, for example, an upper boundon the number of hidden units or a threshold for the approximation error. The next section will show computationalexperiments using different stop criteria.6. Computational experimentsIn this section we present an empirical analysis of RGD. Four tasks were used to evaluate different aspects of thisalgorithm. In Section 6.1 we use a simple maze to study the general behavior of RGD. Particularly, we focus on theA. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482463(a) Maze with optimal policy. Thearrows represent the sum of theunit-length vectors correspondingto the optimal actions. The sym-bol (cid:2) is used to denote states forwhich all actions have the samevalue-function.(b) Value function’s gray map. Thebrighter a state, the higher its valuefunction. Walls are represent asblack squares.Fig. 3. Maze task.(c) Value function’s landscape. Thevalue of the internal walls are consid-ered to be min(s,a) Qπ(s, a) − 1.∗policy evaluation problem to analyse how the two main parameters of RGD affect its performance. In Section 6.2the results of RGD on the mountain-car task are contrasted with those of its direct “ancestor”, namely the standardgradient-descent method. The latter is applied to both linear and non-linear models of different sizes, which allowsus to evaluate the quality of the features selected by RGD. Finally, in Sections 6.3 and 6.4 RGD is compared withother algorithms found in the literature in more challenging control problems. We use the pole-balancing and theAcrobot tasks to check the performance of RGD against evolutionary and recent reinforcement-learning algorithms.The configuration used by RGD in the experiments is discussed in Appendix B.6.1. MazeSection 5 presented several statements regarding the expected behavior of the RGD algorithm. In this section avery simple task will be used to verify these assertions. The task is a 25 × 25 maze presented by Menache et al. [44]and shown in Fig. 3(a). The objective in this domain is to find one of the goal states (marked with “G”) as quickly aspossible. The actions available at every state are north, south, east and west, corresponding to the four possibledirections. The arrows in Fig. 3(a) represent the optimal policy π ∗ for the following reward function: each time theagent performs an action, it receives a reward of −0.5, except if it ends up in one of the two goal states, in which caseit gets a reward of +8 and is repositioned at a random state. If the agent runs into a wall (the limits of the maze or thedark gray squares in Fig. 3(a)), it remains in the same state, but still gets a “reward” of −0.5. Figs. 3(b) and 3(c) showthe corresponding state-value function2 for the case of a discount factor γ = 0.9.The task in this first experiment is to use the RGD algorithm to perform the policy evaluation step, that is, givenπ ∗ use Algorithm 1 to compute the corresponding value function Qπ ∗(s, a). In order to assess the quality of theapproximation ˜Qπ ∗(s, a) we report the measure ξ1 defined in (7). However, it is known that even small approximationerrors can result in large deviations of the approximated policy ˜π with respect to the true π , as discussed in Section 3.Since the final goal of reinforcement learning is to find a good policy (and not a good value-function approximation),we defined another metric, which can be seen as a measure of how well one can “restore” π from its approximatevalue function ˜Qπ (s, a). The metric ξ2 is defined as follows:(cid:9)(cid:5)(cid:4)1π(s), ˜π (s)0if π(s) (cid:9)= ˜π(s),otherwise.(cid:5)π(s), ˜π (s)ξ2 = 1|S|, with δ(cid:2)=δ(cid:4)sWe defined a simple strategy to control the complexity of the models generated by the RGD algorithm: when thenumber m of units in the RBF network reaches a limit mmax, no more changes are made to its hidden layer (that2 The value function of a state s is defined as V π (s) = maxa Qπ (s, a).464A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482Fig. 4. Results on the maze task with σ1 = 30. The points correspond to the results achieved after 1,000,000 transitions. Averaged over 50 runs.(a) ξ1(b) ξ2Fig. 5. Results on the maze task with τ = 0.5. The points correspond to the results achieved after 1,000,000 transitions. Averaged over 50 runs.(a) ξ1(b) ξ2is, neither new RBFs are added nor the ones already in the model have their parameters changed). Fig. 4 showsthe performance of the RGD algorithm when using different values for mmax and τ (the initial width used in thisexperiment was σ1 = 30, which leads to an activation level of the first RBF greater than 0.5 over the entire statespace). The first thing to notice when analysing Fig. 4 is that the metrics ξ1 and ξ2 are in fact not as correlated as onewould expect. Even so, note how an intermediary value of τ = 0.5 generates the best results for both measures. Thedifference is particularly relevant when one considers ξ2: for τ = 0.1 and τ = 0.9, the policies ˜π found by RGD varybetween an error rate of 30% and 40% (regardless of the number of RBFs), while for τ = 0.5 the quality of the policyincreases monotonically with the number of hidden units. Apparently, a high value for τ increases the interferencebetween the radial functions, while too low a value decreases the smoothness of the function computed by the RBFnetwork.We now use the value τ = 0.5 found in the last experiment to check the performance of RGD under different valuesfor σ1 and mmax. If there were no limit on the number of hidden units, smaller values for the initial width σ1 wouldprobably generate better results, since the number of RBFs in the final network’s hidden layer would increase. If theresources are limited, though, using larger widths might be a better choice. Fig. 5 makes it clear that using an initialwidth σ1 = 1—which generates an activation level greater than τ = 0.5 over only one state s—degrades the results ofRGD almost under all values of mmax.Besides influencing the widths of the new RBFs, a large σ1 also increases the number of adaptation steps of thefirst hidden unit, which provides an initial approximation of Qπ (s, a). But how large should σ1 be? Fig. 5 shows thatA. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482465(a) ξ1—minimum at (7.5,0.5)(b) ξ2—minimum at (15,0.7)Fig. 6. Gray map representing the results on the maze task with mmax = 15. The brighter the square, the worse the performance of the RGDalgorithm. The points correspond to the results achieved after 1,000,000 transitions. Averaged over 50 runs.(a) Policy ˜π generated by theagent. The light gray areas rep-resent states for which ˜π (s) (cid:9)=π ∗(s).(b) Gray map of the value functiongenerated by the agent.(c) Landscape of the value functiongenerated by the agent. The valueof the internal walls are considered∗˜Qπto be min(s,a)(s, a) − 1.Fig. 7. Results on the maze task after 1,000,000 transitions using τ = 0.5 and σ1 = 15.the difference on the results when the initial width changes from 15 to 30 is not very significant. Intuitively, one cansay that, if the number of transitions is large enough, the performance of the RGD algorithm will not change verymuch when σ1 is above a threshold σ ∗, since the RBFs will shrink down to their “right” size. The closer σ1 is to σ ∗,the faster RGD will find a reasonable solution. The exact value of σ ∗ is domain-dependent, and might vary with thenumber of transitions (obviously, if the number of transitions is fixed, too large a σ1 will result in poor performance,since the RBFs’ widths will not have enough time to adjust).The value of the “ideal” initial width σ ∗ may also depend on the parameter τ , since it changes the way σ1 affects thewidths of the RBFs that will be created (see (8)). To illustrate this point, we performed several experiments in whichthe maximum number of RBFs was fixed at mmax = 15 while τ and σ1 changed. The results after 1,000,000 transitionsare shown in Fig. 6. Notice that extreme values for both τ and σ1 result in bad performance of the RGD algorithm.In general, the configurations in the center of Figs. 6(a) and 6(b) perform better than those in the edges. Even thoughneither of the two metrics has a minimum at the exact center, both of them present their best performances with aconfiguration close to σ1 = 15 and τ = 0.5.Fig. 7 shows the result of a single execution of the RGD algorithm using τ = 0.5, σ1 = 15 and mmax = 15. Asshown in Fig. 7(a), after 1,000,000 transitions the agent’s policy ˜π selects the optimal action in more than 80% ofthe state space. Notice that the wrong choices are concentrated in the areas where the concavity of the value functionchanges, particularly around the internal walls. The reason for this is clear when one observes Fig. 7(b), which showshow the high values of the states close to the goal go across the limits of the internal walls, causing states on theopposite side of the walls to have high Q-values. Even though, Fig. 7(c) shows that the approximation ˜Qπ ∗(s, a)466A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482Fig. 8. Configuration of the RBFs on the maze task after 1,000,000 transitions. The parameters used were τ = 0.5, σ1 = 15 and mmax = 15. Thecontours were drawn at θi (s) = τ = 0.5.performed by the resulting RBF network was able to assimilate the overall characteristic of the true value functionQπ ∗(s, a).An interesting issue is how exactly the approximation shown in Fig. 7(c) is constructed. It is expected that the RBFnetwork’s structure reflects the landscape of the value function, with a larger number of narrower RBFs in the areaswhere its value changes more abruptly. This expectation is confirmed in Fig. 8, which shows the distribution of theradial functions in the two-dimensional state space after the run described above terminated. Notice how the RGDalgorithm managed to concentrate more resources of the approximator in the areas surrounding the two goals and theinternal walls, where the value function is more complex.Another point that comes up naturally regards the susceptibility of RGD to the presence of noise. In principle,one might suspect that RGD would not perform very well under stochastic environments, since the noise could causethe RBFs to ratchet down to zero width. We believe, however, that noise has the same effect on RGD as the residualerrors intrinsic to the least-squares approximation. If no stop criteria are used to interrupt the shrinking of the RBFs,the widths of these functions will never stop decreasing, even if the environment is deterministic. Therefore, anadequate stop criterion would interrupt the decreasing of the radial function’s widths caused by noise, just like inthe deterministic case. If this is true, the sensitivity of RGD to noise is more related to the strategy used to stop theshrinking of the RBFs than to the algorithm’s mechanism itself.To test our hypothesis, we re-ran the experiment with the maze task, but now with some noise added to the envi-ronment’s dynamics. Specifically, when the agent selected one of the four possible directions north, south, eastor west, it was moved in the right direction with probability 1 − η; with probability η (the “noise”) it was randomlypositioned in one of the 4 neighbor cells. Fig. 9 shows the effect of different levels of noise on the performance ofRGD. Notice that, when using mmax as the stop criterion, the presence of noise does not affect the quality of theapproximation. Actually, one can easily see that ξ1 drops slightly when η = 0.4 and η = 0.5, probably because of thenew shape of the value function (when the level of noise η is increased, the value function of neighbor states get closerto each other, since all the actions can lead to any of the neighbor cells).6.2. Mountain carWhen a stop criterion is used to interrupt the adaptation of the RBFs, the RGD algorithm operates in two distinctstages. In the first one it configures the hidden units of the RBF network, which corresponds to determining whichfeatures will be used by the output layer. After the stop criterion has been satisfied, the network’s hidden layer is keptfixed, and RGD is reduced to the standard gradient-descent method applied to a linear model. An interesting questionis whether the features selected by RGD in the first stage are really helpful for the value-function approximation inthe second.To investigate this issue we chose the mountain-car task [67]. In this domain the goal is to drive a car out of avalley. The challenge relies on the fact that the car’s engine is not strong enough to pull it up the slope it is facing: theonly way to escape is to first move away from the goal and up the other slope until enough inertia has been built upto carry the car out of the valley. This task has a continuous state space, with each state s represented by the positionand velocity of the car. There are three possible actions in every state: full throttle forward, full throttle reverse andA. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482467Fig. 9. Box-and-Whisker plots of the RGD results on the maze task under different noise levels. The values correspond to the approximation errorachieved by the agent after 1,000,000 transitions using τ = 0.5, σ1 = 15 and mmax = 15. The boxes in the graphics represent the median and thelower and upper quartiles. The dotted lines extend up to the extreme values. Statistics computed over 50 runs.zero throttle. The agent receives a reward of −1 at every time step until it moves past its goal position at the topof the mountain, which ends the episode with a reward of 0. An episode may also terminate if the agent does notreach the goal within 1000 steps. Regardless of how the previous one has been terminated, a new episode is alwaysstarted at a random position. The equations governing the system were implemented exactly as described by Suttonand Barto [77], with a discount factor γ = 1 and the input variables normalized to lie in the interval [0, 1].The mountain car is a complete control problem. To deal with this task we adopted the already mentionedSARSA(λ) algorithm using no eligibility traces (that is, λ = 0; see Appendix B for details). In order to analysethe quality of the features selected by RGD we combined SARSA with this algorithm and also with the standardgradient-descent method applied to linear RBF networks of two different sizes. The linear models had 4 and 9 fixedRBFs evenly distributed over the two-dimensional state-space. To guarantee that the comparison would be made be-tween models of about the same size, the parameter mmax of the RGD algorithm was assigned the same values (recallthat mmax is only an upper limit on the number of hidden units). The widths of the fixed radial functions were de-termined using (8) with an activation level of 0.5. The initial widths σ1 used by RGD were determined as being 1.5the widths of the fixed RBFs in the linear model of the same size. Based on the experiments of the last section, theminimum-activation threshold used by RGD to allocate new units was set as τ = 0.5.To assess the quality of the approximation constructed by both methods, we counted after each episode the numberof steps taken by the agent to escape from 100 randomly-selected states (the same set of states was used for all theexperiments). Fig. 10 shows these numbers for RGD and the standard gradient-descent algorithm applied to RBFnetworks with fixed hidden units. Notice in Fig. 10(a) how the performance of the networks with 4 fixed hidden unitsdegenerates after episode 20,000 (observe that the shadowed regions representing the 95% confidence interval of bothmethods do not overlap after around episode 30,000). With the 9 fixed-units networks a different phenomenon occurs,as shown in Fig. 10(b): their performance degrades from episode 10,000 up to around episode 30,000, when it startsto improve again. The results of the RGD algorithm make it clear that, at least in this problem, its strategy to positionthe RBFs leads to a stable behavior. Notice how the policy generated by SARSA-RGD consistently keeps the numberof steps to escape under 100, even when using only 4 RBFs in the hidden layer.In Section 4 we mentioned that the indiscriminate application of the delta rule on the on-line gradient-descentalgorithm may lead to divergence in the context of reinforcement learning. To verify this statement we repeated theexperiment above with the standard gradient-descent algorithm applied to an RBF network with a tunable hiddenlayer. The networks were initialized exactly as before, but now the centers and widths of the RBFs were allowed toadapt.Table 1 shows the average results achieved by RGD and the standard gradient-descent method applied to RBFnetworks with fixed and adjustable RBFs. The values correspond to the performance of the agent after 50,000 episodes.Notice that when using 4 units in the hidden layer the RBF network with adjustable units is unable to learn anything468A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482(a) 4 RBFs(b) 9 RBFsFig. 10. Average number of steps taken to escape from 100 randomly-selected initial states of the mountain-car task. The shadowed gray regionscorrespond to the 95% confidence interval computed for 50 runs.Table 1Average number of steps taken to escape from 100 randomly-selected initial states of the mountain-car task. The values correspond to the resultsachieved after 50,000 episodes using SARSA(0) algorithm with the models shown. The numbers on the left refer to the number of hidden unitsused by the RBF networks (for the RGD algorithm this is the value used for mmax). Averaged over 50 independent runs. “SD” stands for standarddeviation and “95% CI” is the confidence interval at the 95% probability level49Fixed RBFsAdjustable RBFsRGD (σ1 = 0.85)Fixed RBFsAdjustable RBFsRGD (σ1 = 0.58)Mean172.91890.6976.6573.85205.9652.04Best93.58890.6949.3363.8046.1147.04Worst269.30890.69222.3497.48890.6975.77SD63.520.0034.309.45305.699.7695% CI[155.30,190.52][890.69,890.69][67.14,86.16][71.23,76.47][121.23,290.69][49.33,54.75]about the task, performing a number of steps near the maximum possible in all final episodes of all runs (theoretically,a failure to escape from all 100 initial states would result in an average of 1000 steps. The number 890.69 probablyreflects the presence of trivial states from which the car is able to escape the valley no matter what actions are selectedby the agent). When the number of hidden units is increased to 9 the performance of the model with adjustableRBFs improves, but is still worse than the other two. For both 4 and 9 units in the hidden layer the RGD algorithmpresents results substantially better than those obtained by the standard gradient-descent algorithm. This difference isstatistically significant at the 95% probability level.3 Notice that the worst result of RGD corresponds to an average of222.34 steps to escape from the 100 initial states. This is much smaller than the maximum number of steps possible,which indicates convergence to a reasonable solution in all cases.The experiments with the mountain-car task make it clear that RGD is able to generate better results than thestandard gradient-descent algorithm. Why exactly does it happen? Besides the on-demand allocation of new units,RGD has three characteristics that distinguish it from the standard gradient-descent method: the widths of the radialfunctions are only allowed to shrink, its centers only move towards the current state, and only the most activated unithas its parameters changed. An interesting question is whether all these features are really necessary, and how theyinteract with each other. To answer that, we performed a series of experiments in which each one of RGD’s featureswas “turned on” and “off”. The results are shown in Table 2. The first row corresponds to the case where RGD behavesmore similarly to the standard gradient-descent algorithm, while the last one corresponds to the actual RGD.3 For the statistical tests, we used Student’s “t-test” or the “u-test” from Mann and Whitney [42], depending on whether the data were normallydistributed or not.A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482469Table 2Average number of steps taken to escape from 100 randomly selected initial states of the mountain-car task. The values correspond to the resultsachieved after 5000 episodes using SARSA(0) algorithm combined with RGD using different configurations. The parameters of the RGD algorithmwere set as τ = 0.5, σ1 = 0.58 and mmax = 9. Averaged over 50 independent runs. The columns “W”, “C” and “U” indicate the restrictions used byeach configuration. “W” and “C” refer to the changes performed by the algorithm to the widths and center of the RBFs, respectively. The column“U” indicates whether all or only the most activated unit is changed at each iteration. An “–” means all the changes are made indiscriminately (asin the standard-gradient algorithm); a “(cid:3)” indicates only restricted changes are made, as in RGDRestrictionsMeanBestWorstSD95% CIW––––(cid:3)(cid:3)(cid:3)(cid:3)C––(cid:3)(cid:3)––(cid:3)(cid:3)U–(cid:3)–(cid:3)–(cid:3)–(cid:3)12345678547.88695.66548.24742.2879.5772.5262.8861.7150.4855.8148.8050.1149.1248.1448.3347.35890.69890.69890.69890.69161.48156.16479.43137.94392.76350.72408.81314.2828.7922.6060.4819.13[439.01,656.75][598.45,792.87][434.92,661.56][655.17,829.39][71.59,87.55][66.26,78.78][46.12,79.64][56.4,67.01]The first thing that stands out when observing Table 2 is the big difference between the values shown in the firstfour rows and those in the last four ones. This makes it clear that the restriction on the changes made to the RBFs’widths is the main mechanism through which RGD achieves good performance. When considering the cases in whichthe RBFs are only allowed to shrink (rows 5 through 8), moving the centers of the radial functions always towards thecurrent point seems to be a good strategy: notice how the means of rows 7 and 8 are smaller than the correspondingrows 5 and 6. Finally, moving only the most activated unit seems to increase the stability of the algorithm, as shownby the standard deviations. Notice how this value drops from row 5 to 6 and 7 to 8 (and also from row 1 to 2 and 3to 4).As a final observation, it should be mentioned that RGD achieved its best performance when using all three featuresthat distinguish it from the standard-gradient algorithm; notice how the last row of Table 2 presents the best valuesfor all the statistics shown. The differences between the mean shown in the last row and all the others are statisticallysignificant at the 95% level, except for the 7th row, whose average number of steps is statistically identical to that ofthe standard RGD. This indicates that moving all the units or only the most activated one does not have a strong effecton the final results. It is important to notice, however, that the latter requires a much smaller number of operations,and thus might be a better choice.6.3. Pole balancingThe pole-balancing problem is a classic reinforcement-learning task studied by many authors [1,6,45]. The ob-jective here is to apply forces to a wheeled cart moving along a limited track in order to keep a pole hinged to thecart from falling over. At every time step the agent receives information regarding the position and velocity of thecart, the angle between the cart and the pole, and the angular velocity of the pole, which constitutes a 4-dimensionalcontinuous state space. There are two actions available at every state: push the cart either left or right with a force ofconstant magnitude. If the pole falls past a 12-degree angle or the cart reaches the boundaries of the track the agentgets a reward of −1 and a new episode is started. At all other steps the agent receives a reward of 0.Here we use the pole-balancing task to compare the performance of the RGD algorithm with that of other methodsfound in the literature. As a basis for our comparison we chose the work of Moriarty and Miikkulainen [47], in whichthe authors evaluate the performance of several techniques on the pole-balancing problem, including evolutionary andmore traditional reinforcement-learning algorithms. More specifically, their “Symbiotic, Adaptive Neuro-Evolution”algorithm (SANE) is compared to the GENITOR system of Whitley et al. [88], the Adaptive Heuristic Critic (AHC)with both single-layer [6] and two-layer networks [2] and a lookup-table version of the Q-learning method of Watkinsand Dayan [84]. The discretization of the state-space used by the single-layer AHC and the Q-learning algorithms wasbased on the work of Barto et al. [6], in which prior knowledge about the domain was used to partition the space into162 non-overlapping regions. The other methods received the continuous values of the state variables. For our exper-iments we adopted the SARSA(λ) algorithm combined with RGD as in Section 6.2, but now with eligibility traces470A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482Table 3Results averaged over 50 runs on the pole-balancing task. The episodes refer to the number of attempts (or “starts”) necessary to balance the polefor 120,000 steps starting with the pole straight up and the car centered in the track1-layer AHC2-layer AHCQ-learningGENITORSANESARSA-RGDEpisodes to learnFailuresMean232897619751846535411Best323963366272705Worst5003413081016470521910924SD709757319191396329237040000Freeparameters162 × 235 × 2162354018.6 ± 2.9with a decay rate of λ = 0.5. The initial width σ1 = 1 was computed so that the first RBF would have an activationlevel greater than τ = 0.5 over the entire state space. In order to have a fair comparison, we ran our simulations on aversion of the pole-balancing task implemented exactly as described by Moriarty and Miikkulainen [47].The task in the first experiment was to find a network able to balance the pole for 120,000 time steps starting withthe pole straight up and the cart centered in the track. Neither the cart nor the pole had any initial velocity. A failurewas said to occur if the agent was not able to achieve the goal within 50,000 episodes. Notice that with this task itis not straightforward to use a fixed exploration strategy with SARSA (we adopted (cid:16)-greedy exploration with a fixed(cid:16), as detailed in Appendix B). Since this algorithm does not have an explicit actor—but instead derives it from thecurrent approximation of Qπ (s, a)—the only way to balance the pole for a reasonable amount of time is to reducethe exploration rate, since exploratory moves can easily end an episode. In this context, though, defining a decreasingschedule for (cid:16) could make the analysis of RGD’s performance more difficult, since the success of this algorithm woulddepend on one more parameter.4 Therefore, instead of gradually decreasing the exploration rate we simply tested theperformance of the current RBF network after each episode. This test was performed without exploring or learning,very much like a “validation” step in supervised learning [62]. If the network was able to balance the pole for 120,000steps, the run was terminated. If not, the learning process continued with a fixed (cid:16)-greedy exploration. Notice thatwith such an “external” criterion to interrupt the training process no special care had to be taken to stop the shrinkingprocess of the RGD algorithm.Table 3 shows several statistics regarding the number of episodes taken by each method to balance the pole. Noticehow these numbers are extremely favorable to RGD: besides learning the task faster, this algorithm also presented astable behavior, as shown by the lower standard deviation. These results are even more impressive when one considersthat the other methods do not configure the topology of the approximators, using instead models whose complexityare known to be sufficient for this problem. Particularly, the 1-layer AHC—the only algorithm to learn faster thanRGD on average—used a set of features constructed based on knowledge about “useful regions” of the state space [6],which in theory makes the task much easier. Notice also that the models generated by RGD were on average simplerthan those used by the other algorithms, as shown by the number of free parameters associated with each method (theAHCs adopted two networks of the same size, one for the actor and one for the critic).The success of RGD in this experiment might be a consequence of the stop criterion used by this algorithm: as thetask is always initialized at the same start position, checking the current solution against the initial state corresponds toverifying if the policy derived from ˜Q(s, a) is able to accomplish the task. This is the same as having a validation set insupervised learning that is coincident with the final test set. Indeed, this stop criterion may generate highly specializedsolutions, able to perform the task only in the restricted set of states tested. Even though, this might be a valid strategyin tasks that are initialized always in the same way, as is the case of this version of the pole-balancing problem. Noticealso that this is the same stop criterion adopted by the evolutionary methods, both of them outperformed by RGD. Inthe next experiment we will verify how this strategy to stop the learning process extends to domains with a larger setof initial states.The agents trained in the last experiment were able to balance the pole from one specific start position, namely:the pole straight up, the car centered and both velocities equal zero. A more interesting challenge is to learn the task4 The strategy adopted by Moriarty and Miikkulainen [47] to stop the Q-learning algorithm is not clear.A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482471over a range of initial states and then try to generalize over the entire state space. Our second experiment was setup in order to reproduce this scenario: it was configured exactly as the previous one, but instead of always startingthe learning process at the same state, the four input variables were randomly selected from the range of possiblevalues. In the work of Moriarty and Miikkulainen [47] all the algorithms were interrupted when the pole had beenbalanced for 120,000 time steps from any initial random position. To be coherent with our decision of not decreasingthe exploration rate, we generalized the stop criterion used in the last experiment: now, the process was stopped whenthe network being constructed by RGD was able to balance the pole for 1000 steps from n consecutive start positions,with no failure. This test was made with the model fixed and no exploratory moves.To verify the quality of the solutions generated by each algorithm we measured their generalization ability against100 random initial states. These values are shown in Table 4, along with the number of episodes taken by each methodto achieve the goal. The first thing to note is that using only 1 start position as a criterion to stop RGD’s learningprocess does indeed result in very specialized solutions, as discussed before. Notice that, when using this strategy, theRGD algorithm presents a reasonable learning rate, but very bad generalization. When the number of start positionsused as the stop criterion is increased, the expected phenomenon occurs: the number of episodes to accomplish thetask also increases, and the resulting RBF networks present much better generalization. Notice that the results ofSARSA-RGD when using n = 5 and n = 7 are quite good, specially considering that many of the initial states usedto assess the networks’ generalization represent irrecoverable situations, that is, states from which it is impossible tobalance the pole [88].The values shown in Table 4 make it clear that the RGD algorithm takes a larger number of episodes to learn thepole-balancing task than the other algorithms. This is true even when only 1 initial state is used to stop RGD. Why doesit happen? The algorithms being compared in this section can be divided in three categories, according to the featurespace they work in. Q-learning and 1-layer AHC operate on a set of features pre-selected based on human knowledgeabout the problem. The 2-layer AHC, GENITOR and SANE extract the features used in the approximation through amapping from the original input space to a higher dimensional hidden space of fixed size. The RGD algorithm belongsto a third category, in which the dimension of the hidden space is also learned. We conjecture that the difference onthe number of episodes to learn the task is related to the amount of a priori information given to each algorithm aboutthe feature space.Even using less information about the feature space RGD presented the best generalization performance amongthe tested algorithms (except for the SARSA-RGD-1 case, in which the algorithm clearly stopped prematurely). Atthe 95% probability level, the difference on the generalization of the other algorithms is not statistically significant,except between 1-layer AHC (the best) and Q-learning, which presented the worst generalization among all. Whencomparing RGD with the others, the difference on the mean generalization is significant for n (cid:4) 5. Notice, however,that the stop criterion used by RGD was specially designed to improve generalization, and in principle it is possibleto come up with similar strategies for the other algorithms.The values shown in Table 4 clearly indicate that RGD is a stable algorithm, at least in the pole-balancing task.Notice that for n (cid:4) 3 this algorithm presents very low standard deviations of the generalization metric, and the worstTable 4Results averaged over 50 runs on the pole-balancing task. The episodes refer to the number of attempts necessary to balance the pole from a randominitial position. The generalization was measured as the number of initial states out of 100 randomly-selected ones from which the trained agentswere able to balance the pole for 1000 time steps. The “n” in the SARSA-RGD-n labels represent the number of states used as stop criteria (thatis, the number of consecutive start positions from which the agent was supposed to balance the pole to terminate an episode)1-layer AHC2-layer AHCQ-learningGENITORSANESARSA-RGD-1SARSA-RGD-3SARSA-RGD-5SARSA-RGD-7Episodes to learnMean4301251324022578169128446618845511387Best803458426415461102234022492976Worst737345922100561296444615993148462366626851SD10719338190320929841367267042505534GeneralizationMeanBestWorst5044414848155056587676618181616874802513211161740SD16201123252112128472A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482network found in all 50 runs of all 3 configurations was able to balance the pole in 16% of the states tested, whichis not bad when compared to the other methods. Another indication of RGD’s stability is the fact that it has notfailed to balance the pole in any of the 50 runs, in contrast with 1-layer AHC (3 failures) and 2-layer AHC, with 14failures.5 Finally, it is interesting to note that the size of the RBF networks generated by RGD are compatible withthe complexity of the models used by the other methods (shown in Table 3), spanning from an average of 50 freeparameters for n = 3 to 86 when n = 7.The conclusion is that the RGD algorithm is a viable alternative in problems similar to the pole-balancing task. Ifone has enough information about the state-space to partition it or even to define the right structure of the approximator,any of the methods shown in the upper half of Table 4 would be expected to perform similarly. If this information isnot available, RGD might be a good choice.A final point should be mentioned regarding the performance of the evolutionary methods (GENITOR and SANE)on the experiments with the pole-balancing task. Evolutionary algorithms have always been particularly successful onthis task, since the early works [88] until more recently (see [31] and references therein). So much so that the pole-balancing has become a benchmark problem in the field, and since the publication of Moriarty and Miikkulainen’swork more difficult versions of the task have been proposed and successfully addressed [31,35,71]. In addition, com-parisons between recent evolutionary methods and traditional value-based reinforcement learning algorithms seem toindicate a clear advantage of the first over the second [31].6However, we believe part of this success is due the fact that the pole-balancing belongs to a class of controlproblems particularly suitable for optimization techniques. As well known, the search performed by evolutionaryalgorithms is based on information gathered between episodes, but not within them. This type of search is feasiblein tasks like balancing a pole, where the learning process spans a large number of short episodes. However, it doesnot seem practical to use evolutionary methods in tasks where the episodes themselves are long, as for example inthe game of chess. The methods based on temporal-difference learning, on the other hand, tend to be less sensitiveto this aspect, since the learning occur both intra and inter episodes. Perhaps more importantly, in the pole-balancingproblem it is possible to assess the quality of a candidate solution even if it has failed to accomplish the task (here, thequality was measured by the number of steps a solution could balance the pole for). Although many problems have asimilar formulation, this is not always true. In shortest-path problems very little information can be gathered from afailure other than the reinforcement signal, and ranking the potential solutions is no longer a trivial task. In the nextsection we present a control problem with such a characteristic, and make the present discussion more concrete.6.4. AcrobotUnderactuated mechanical systems have more degrees of freedom than actuators. Examples of such systems in-clude manipulator arms on diving vessels or spacecrafts, non-rigid body systems, and balancing systems such asunicycles or dynamically stable legged robots [17]. In this section we study the Acrobot, an underactuated non-linearsystem that has been studied by both control [70] and machine-learning [16,75] researchers. The Acrobot is an in-teresting task because its dynamics are complex enough to yield challenging control problems, yet simple enough topermit a complete mathematical analysis and modeling. In our experiments we used a simulator whose equations ofmotion are given in [17,70,77].7The Acrobot is a two-link robot arm powered at the elbow but free-swinging at the shoulder. It resembles a gymnastswinging on a bar, in which case only the joint corresponding to the gymnast’s waist can exert torque (thus the name).The goal is to swing the gymnast’s “feet” above the bar by an amount equal to one of the links as fast as possible (areward of −1 is given to the agent on all time steps, with no discounting). The choice to be made at every time step isthe torque applied at the second joint. Following Sutton [75], we restricted the options to three choices: positive torqueof +1 Nm, negative torque of −1 Nm, or no torque at all. The state-space is continuous and 4-dimensional, with two5 Following Moriarty and Miikkulainen [47], the failure cases were not included in the statistics of Table 4.6 Gomez et al. focus on the problem of solving a sequence of increasingly difficult versions of the pole-balancing task, some of them not solvedby value-function based methods [31]. Unfortunately, they do not report a measure of the generalization capability of their evolved networks, whichseems to be a dimension in which traditional reinforcement-learning algorithms produce competitive results.7 We used 4th order Runge–Kutta integration with a time step of 0.005 seconds and actions chosen after every 10 time steps (the reason for suchchoices is given further in the text). The constants were set as in [17,75], and we did not restrict the velocities of the links.A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482473variables representing the joint positions and two representing the joint angular velocities. All episodes are started atthe stable position (cid:8)s = [0, 0, 0, 0], and terminate when the agent reaches the goal or a maximum of 1000 steps.In our simulations actions were applied at a frequency of 20 Hz, contrasting with the usual choice of 5 Hz. Thismodification makes the task considerably harder, and also explains the larger number of steps taken by the agents toreach the goal in our experiments when compared to previous results [16,75]. Notice that the Acrobot task formulatedin this way is a relatively hard shortest-path problem, and as such it presents the characteristics discussed in the lastsection. In particular, a “bad” agent/controller will often perform a sequence of actions that will never lead to thegoal, which results in the episode being truncated at an arbitrary point. Therefore, the only information returned by anagent that has failed to accomplish the task is a “failure signal”. Any optimization technique that relies on the conceptof an objective function—evolutionary methods included—will have problems ranking the unsuccessful candidatesolutions. In fact, unless a successful solution luckily emerges in the process (which is highly unlikely in this case),the optimization will be reduced to a random search.In order to check the last statement, we implemented four evolutionary methods and tested them on the Acro-bot task. We tried a conventional real-coded genetic algorithm [30], its steady-state version (which is very similarto the GENITOR algorithm of the last section [88]), and two evolution strategies: (1, 10)-ES and (1 + 10)-ES [15],which have recently shown excellent performance on the pole-balancing task when combined with genetic program-ming [87]. In all of them an individual encoded the linear weights of an RBF network with 81 Gaussian units evenlydistributed over the state space (we tried 10 different levels of overlap between the functions, as detailed below).Besides the RBFs, we also adopted a constant term. As in (6), we used one linear model for each action, thus eachcandidate solution was a real-vector of length 3 × 82 = 246. The fitness of a solution was defined as 1000 − ns, wherens is the number of steps taken by the corresponding policy to accomplish the goal. The genetic algorithms used apopulation of 100 individuals, and all four methods were interrupted after 10,000 evaluations had been carried out.8All other choices were standard [38]. Each algorithm was executed 10 times for each configuration of the hidden layer,resulting in 100 independent runs; none of them was able to find a single individual capable of achieving the goal inless than 1000 steps.9Given the bad results achieved by the evolutionary methods, we proceeded to try a value-function based method,namely the least-squares policy iteration algorithm (LSPI) [39]. LSPI is an approximate policy-iteration algorithm. Itextends the benefits of least-squares temporal difference (LSTD) [18,20] to the control problem. Like the second, theformer makes efficient use of data and eliminates learning parameters. Unlike LSTD, LSPI does not need a model ofthe system to perform the policy update and can be used with data collected from any reasonable sample distribution.The LSPI algorithm enjoys good convergence properties [39] and has been applied to several problems [40].The first step when applying LSPI to any task is to collect data in the form of transitions (s, a, r). Usually, sampletrajectories are generated by exploratory policies [39]. However, using random policies in the Acrobot task tends toconcentrate the data around the equilibrium state (cid:8)s = [0, 0, 0, 0]. Sampling transitions from a random distribution isnot a trivial task with the Acrobot, either. First, the variables representing the links’ velocities are not bounded, andwe have to define an interval from which to pick the samples. More importantly, we want the data to be concentratedin the relevant regions of the state space, which is hard to define a priori. In order to overcome these difficulties, wedecided to implement a graphical interface and let some people “play” with the Acrobot. Based on the movements ofthe Acrobot on the screen, the person could choose at every time step the torque to be applied on the system, like avery simple videogame. The idea was to figure out what parts of the state space are really visited during a reasonableepisode, and also to set up a baseline against which to compare the algorithms’ results. Each person was asked tointeract with our simulator until he/she got familiarized with the system’s dynamics. After that, each person played8 This number was set in order to make the number of operations compatible with those performed by the other methods applied to this task. Inparticular, since each evaluation takes approximately 1000 × |A| × m operations, where m = 82 is the number of features used, the total cost ofeach run was of the order of magnitude of 109 operations.9 Notice that this formulation of the Acrobot task was deliberately designed to illustrate the difficulty of applying conventional optimizationtechniques to hard shortest-path problems. The “standard” version of the problem—in which actions are applied at a frequency of 5 Hz—is easilysolved by evolutionary methods [43]. Notice also that the only information used to define the fitness of an individual was ns, the number of stepstaken by it to reach the goal. It is, of course, possible to design more elaborate fitness functions [22,91], but this requires domain-specific knowledgenot used by the value-function based methods.474A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482Table 5Results obtained by 5 different people with our Acrobot simulator. Thevalues refer to the number of steps taken to swing the Acrobot’s tipabove the bar. Averaged over 10 episodesPerson12345Mean402.0290.6365.6453.4612.6Best302232299245391Worst472630673651996SD49.16121.22110.62106.28217.68for 10 episodes and we recorded the data generated. We were helped by 5 people, which resulted in 21,242 transitions.The detailed information regarding this experiment is given in Table 5.Our first attempt was to use the data generated by the humans’ interaction with the system directly, but LSPI wasnot able to find any policy capable of performing the task when using this dataset. Thus, we generated a larger datasetin the following way. First, we bounded the four variables based on our human-generated dataset (that is, the limits ofthe intervals were defined as the minimum and maximum values present in the data for the corresponding variable).Then, we picked 10,000 states (cid:8)si evenly distributed over the hypercube defined by these intervals, and in each (cid:8)si weapplied each one of the 3 actions. This resulted in a dataset with 30,000 transitions. All the results of LSPI reportedhere were generated using this dataset.10We began our experiments with LSPI using a Gaussian RBF network with 16 hidden units located in a 2 × 2 ×2 × 2 grid, plus a constant term. Unfortunately, none of our experiments with this architecture were successful, andthus we increased the granularity of the grid to 81 RBFs. We used the same width σ for all hidden units, and trieddifferent values for this parameter. In particular, we used (8) to define several levels of overlap τ between neighborradial functions. We started with the intuitive values τ ∈ {0.9, 0.7, 0.5, 0.3, 0.1}, but since we noticed smaller valuesgenerated better results, we also tried τ ∈ {0.09, 0.07, 0.05, 0.03, 0.01}. This resulted in 10 different configurationsof the hidden layer (these were the configurations used in the experiments with the evolutionary methods). LSPI wasexecuted for 10 iterations, and since each iteration takes around m2 × 30,000 + (m × |A|)3 operations, the total numberof operations performed on each run was of the order of 109.Table 6 presents preliminary results of LSPI on the Acrobot task when using different levels of overlap betweenthe RBFs. Notice that for τ = 0.9, τ = 0.7 and τ = 0.5 none of the 10 runs was able to find a policy that could swingup the Acrobot in less than 1000 steps. This may not come as a surprise for the first two values, but we expected betterresults for τ = 0.5. Anyway, the results improve significantly for smaller levels of interference between the RBFs, andfor τ ∈ [0.05, 0.1] the average results obtained by LSPI are better than those achieved by 4 out of 5 people who triedour simulator. The LSPI algorithm reaches its best performance at τ = 0.05, with the smallest average and standarddeviation among all. We used this configuration to run extra experiments with LSPI, as will be described further inthe text.The experiments with the RGD algorithm were much simpler to do, since we did not have to define a dataset. Wesimply combined RGD with SARSA(0) and executed it on-line. We used (cid:16)-greedy exploration with a fixed (cid:16) = 0.5.As in the previous experiment, the agent was allowed to explore during learning only, which explains such a largeexploration rate. Every time we wanted to check the performance of the current RBF network we fixed (cid:16) = 0, whichcorresponds to using the greedy policy with respect to the current value-function approximation. All the results ofRGD refer to this setting. We adopted a decaying learning rate starting at α = 10−3 and going down to α = 10−6. Thenumber of episodes performed during learning was defined in order to make the computational cost of SARSA-RGDcompatible with LSPI’s. Since in our current RGD implementation each step takes approximately 8m operations, andconsidering each episode takes at most 1000 steps, we set the number of episodes as 600 for the 16-RBF case and as10 We tried several other datasets as, for example, sampling a larger number of transitions as described above or merging human-generated datawith transitions uniformly sampled. None of them resulted in any improvement on LSPI’s results.A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482475Table 6Results of LSPI on the Acrobot task. Each run consisted of 10 iterations. Aver-aged over 10 runsτ0.900.700.500.300.100.090.070.050.030.01Mean1000.01000.01000.0927.5353.3364.9384.8335.0422.4562.9Best100010001000855257288313315374431Worst10001000100010001000100010003438001000SD0.000.000.0076.42227.60223.44216.1812.89133.22170.14Table 7Results obtained by SARSA-RGD on the Acrobot task with several parameter configurations. The value of the hidden layer’s learning rate β isgiven relative to α, the learning rate of the linear parameters. Results averaged over 10 runsmmax = 16 RBFsBestMeanmmax = 81 RBFsBestMeanParametersWorstWorstSDSDβτσ110105050101050500.50.50.50.50.70.70.70.70.1αα0.1αα0.1αα0.1αα471.2633.7327.9307.9647.2438.1502.6431.8295297247224275293273274100010004113581000100010001000280.54318.2856.2535.02371.96296.16343.43299.80264.2261.1300.4272.3256.6567.6254.5253.3245242247248243260244230343283459297280100028528229.6214.6973.3016.7312.47372.3811.5715.043000 for the experiments with 81 hidden units.11 We experienced with different values for τ , σ1 and β (the hiddenlayer’s learning rate). The results are shown in Table 7.The first thing that stands out in Table 7 is the fact that SARSA-RGD was able to find successful policies evenwhen using a maximum of only 16 hidden units to approximate the Q-function. We believe the strategy used by RGDto configure the hidden layer is playing an important role here, specially considering LSPI was unable to find a singlesuccessful solution using the same network architecture. Notice that for τ = 0.5 and σ1 = 50 the results obtained bySARSA-RGD with a 16-RBF network are competitive with those achieved by LSPI with 81 RBFs, and really closeto the best results found by human experience. When mmax = 81 hidden units, SARSA-RGD consistently overcomesLSPI and often the best results of Table 5. Perhaps more importantly, RGD manages to find a solution for the problemin all 10 runs of 7 out of 8 configurations.In order to get more reliable results, we reran the experiments with the best configuration of each algorithm, nowaveraging over 50 executions. LSPI was applied to an RBF network with 81 hidden units and an overlap of τ = 0.05between neighbor functions. RGD used the parameters on the last row of Table 7, namely mmax = 81, τ = 0.7,σ1 = 50, and β = α. The results are shown in Fig. 11 and Table 8.Fig. 11 shows the performance of both LSPI and SARSA-RGD over time. Notice how LSPI makes little progressafter the first iteration, and after the 5th one all the 50 runs converge to about the same solution, which remainsunaltered until the last iteration. At the 95% confidence level, we can say LSPI will find a policy that needs at least332 steps to accomplish the task, as shown by the last column of Table 8. Contrasting with LSPI, RGD’s resultsdecrease monotonically until around episode 1500, and after this point the results seem to bounce around 300 steps.The confidence interval of RGD’s final result is about 8.8 times wider than LSPI’s, which suggests more variationon the algorithm’s behavior from one run to the other. Even so, there is a clear advantage of the first algorithm over11 The resulting numbers of operations are of the same order of magnitude as those performed in 10 iterations of LSPI with models of the samesize, namely of the order of 107 and 109, respectively.476A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482Fig. 11. Results achieved by the best configurations of LSPI and SARSA-RGD on the Acrobot task (see text for details). The shadowed gray regionscorrespond to the 95% confidence interval computed for 50 runs.Table 8Results achieved by the best configurations of LSPI and SARSA-RGD on the Acrobot task (see text fordetails). LSPI’s numbers correspond to the results found after 10 iterations; SARSA-RGD was allowed torun for 3000 episodes (these values result in both algorithms performing a number of operations of the orderof magnitude of 109). Averaged over 50 runsLSPISARSA-RGDMean335.90276.56Best315238Worst3431000SD12.11106.6295% CI[332.54,339.26][247.01,306.11]the second. Notice in Fig. 11 how RGD’s curve crosses LSPI’s between episodes 600 and 900 (which correspondsto the second and third iterations of LSPI), and after episode 1200 the confidence intervals do not overlap anymore.Note also that the final result of RGD represents a reduction of almost 60 steps over that of LSPI, on average. As afinal point, we should mention that RGD was able to find policies able to control the Acrobot at the same level ofproficiency of the most skilled people who tried our simulator.In our experiments with the Acrobot the SARSA-RGD algorithm was able to achieve better results than LSPIperforming roughly the same number of operations. We believe this happened for two main reasons. First, the waydata was collected and used by both algorithms was quite different. When merged with SARSA, RGD is an on-linealgorithm, and as such it actively gathers data according to its exploration strategy. Normally, the greedy action withrespect to the current Q-function will be chosen more often than the others. This results in some regions of the statespace being visited more often than others, which is equivalent to saying the corresponding states are assigned higherweights in the Q-function approximation (see (7)). In shortest-path problems this characteristic may be crucial, sinceregions of the state space “far” from the path being explored by the agent will simply be ignored. With LSPI, it is hardto define beforehand which states are important and which are not, and the usual choice is a uniform coverage of thestate space. Thus, a lot of computational effort is wasted in the approximation of the Q-function in regions of the statespace that will never be visited by the agent in practice.12However, this fact alone might not be sufficient to explain the superior performance of SARSA-RGD when com-pared to LSPI. To check this out, we reran the experiments with SARSA, but now using the standard gradient-descentalgorithm instead of RGD. We used an RBF network with 81 fixed hidden-units evenly distributed over the statespace and tried two levels of overlap between the functions: τ = 0.7 and τ = 0.5. All the parameter values used in the12 One way to remedy this with LSPI is to resample the transitions (s, a, r) at each iteration according to the current greedy policy [39].A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482477experiments with RGD were kept, resulting in an on-line algorithm with exactly the same exploration strategy. Foreach level of interference between RBFs we executed SARSA for 10 times. None of them resulted in a policy able toaccomplish the task.Another characteristic of RGD that might have helped on the Acrobot task is its strategy to define the feature space,that is, the way it configures the number of basis functions in the model, as well as its centers and widths. The resultswith the 16-RBF networks supports this hypothesis, since LSPI was not able to find a single solution for the problemusing the same number of hidden units equally spaced over the state space. An interesting question here is: howwould another non-linear method do on the Acrobot task under the same circumstances of RGD? To answer that, weperformed yet another test: we reran the last experiment, but now we let the gradient descent method also configurethe RBFs’ centers and widths. The functions were initially positioned as in the last experiment, and again we triedtwo values for τ : 0.7 and 0.5. As in the experiments with RGD, we tested two values for the hidden layer’s learningrate, β = α and β = 0.1α. All the remaining parameters were set with the same values as before. We executed thealgorithm for 10 runs with each configuration, which resulted in 40 independent runs. Again, none of the runs resultedin a successful solution.7. DiscussionIt is probably possible to improve the results of both RGD and the other algorithms on the tasks studied by usingmore specific configurations. We prefer instead to concentrate on the behavior of the algorithms without too muchtweaking or use of domain knowledge, which in our opinion better reproduces a real-world scenario. The experimentsperformed in this way have shown that the RGD algorithm shares with the standard gradient-descent method one ofits most desirable characteristics: generality. In all domains it was tested, RGD presented a stable behavior and wasable to find reasonable solutions using several parameter configurations.The main difference between RGD and the standard gradient-descent algorithm is in the application of the deltarule. In particular, in the RGD algorithm the widths of the RBFs are only allowed to shrink, and the centers alwaysmove towards the current state. These modifications are conservative, in the sense that they can not lead to divergenceof the parameters to infinity. As long as a sufficiently small learning rate is adopted, the widths of the radial functionswill asymptotically approach zero, and its centers will be confined to the convex hull defined in the state space by thedata.Another difference between RGD and the gradient-descent method is the possibility of allocating new hidden units.The strategy used by RGD to add and position the radial functions follows the basic philosophy of the so-called “self-organizing networks” [27,28]: 1) determine the hidden unit that is closest to the current input vector, 2) move it (andoptionally its k-nearest neighbors) towards the input vector and 3) add new units on-demand, according to a pre-defined insertion criterion. One objective of such unsupervised-learning methods is dimensionality reduction [28]. Inprinciple, if the states that come up in the agent interaction with a reinforcement-learning environment are concentratedin a low-dimensional subspace of the original state space, one can expect that RGD will restrict the RBFs to such asubspace. Notice, however, that generally this will not be the case, and therefore RGD is subjected to Bellman’s “curseof dimensionality” [11], in the sense that the number of RBFs in the approximator will grow exponentially with thenumber of dimensions of the state space.7.1. Kernel-based reinforcement learningWe believe that increasing the number of RBFs while decreasing their widths is the main mechanism through whichRGD achieves its good results (see Table 2). Although derived from intuitive ideas, this strategy has surprisinglymany similarities with the work of Ormoneit and Sen on “kernel-based reinforcement learning” [48], which has amore theoretical perspective. In their work, the authors show how the Q-function can be approximated by a sum ofweighting kernels, which resembles a local RBF network whose functions are centered at the states (cid:8)si . Ormoneit andSen argue that this schema can be used to derive an iterative update rule similar to (4), and prove that the resulting˜Qπ (s, a) converges in probability to the true Qπ (s, a) as the number of sample transitions used in the approximationincreases. This is a very desirable property, which the authors call consistency: additional training data always improvethe quality of the approximation ˜Qπ (s, a), and eventually leads to optimal performance. In the reinforcement learning478A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482context, this property is very hard to establish for conventional parametric approximators such as neural networkswith a fixed architecture trained by the standard gradient-descent algorithm [48].Besides some simple assumptions on the reward, transition and kernel functions, the only requirement for theabove convergence to be true is that the weighting kernels “shrink” with increasing sample size at an “admissible”rate. Since in kernel-based reinforcement learning there is one kernel function for each sample transition, the laststatement is equivalent to saying that the widths of the functions should decrease to zero as their number increases.Also, this reduction should not happen too fast, in order to guarantee a certain degree of overlap between neighborfunctions. This is very similar to the reasoning behind RGD, though in the opposite direction: while in kernel-basedreinforcement learning the number of functions determines their widths, in the RGD algorithm the opposite happens.Notice, however, that having one kernel for each sample transition is not feasible in practice (especially in on-linelearning), and therefore RGD’s strategy to configure the hidden layer can be regarded as a practical approach fordefining the kernel functions.It is somewhat surprising that two works following so different lines of thought have come to so similar conclusions,and in our opinion this strengthens our empirical arguments. More importantly, it opens up new interesting possibilitiesfor future research. We believe it might be possible to fit RGD (or a slightly modified version of it) within the kernel-based reinforcement-learning framework.8. ConclusionsThe restricted gradient-descent algorithm is essentially a strategy to extract important features from the state space.If one has enough information about the problem at hand to handcraft the feature space, it is certainly a better choiceto do so and use a linear model to perform the value-function approximation. On the other hand, if not much is knownabout the domain, the RGD algorithm may be an appealing alternative.RGD is basically a modified version of the standard gradient-descent algorithm, and as such it inherits both itsqualities and drawbacks. In particular, it is very simple and general, that is, it has wide applicability and requiresminimal use of domain knowledge, as shown by the experiments. Our algorithm presents still some advantages whencompared to its unrestricted form: since the changes performed by RGD are conservative, the non-linear parameters ofthe approximator can not diverge. Besides, RGD is able to configure the topology of the RBF networks, which may bean important feature in some situations. On the downside, we can mention the facts that RGD makes inefficient use ofdata when compared to least-squares methods like LSTD, and that its performance depends on the right definition ofits parameters. Also, it is sensitive to the dimensionality of the state space, meaning that the size of the RBF networkwill usually grow exponentially with the dimension of the input space.The study presented in this work is fundamentally an empirical analysis, intended to show the feasibility of applyingthe RGD algorithm to reinforcement-learning benchmark tasks. Specifically, it has been shown that this algorithmcombined with SARSA presents competitive results with other methods found in the literature, including evolutionaryand traditional reinforcement-learning algorithms. When merged with SARSA, RGD becomes an on-line and “on-policy” algorithm, that is, learning takes place while the agent interacts with the environment and data are collectedaccording to the agent’s actual experience. It is not difficult to think of other combinations that would give rise toalgorithms with different characteristics (RGDQ-learning, for example, would be an on-line and off-policy algorithm).It is also conceivable to use a model of the environment to aid in the learning process.A more theoretical analysis of RGD is desirable, especially regarding its connections with Ormoneit and Sen’skernel-based reinforcement learning. Like RGD, kernel-based learning relies on a local approximator with an openarchitecture, and in both cases an increase on the number of basis functions results in a decrease of their widths. As atheoretical framework the kernel-based approach enjoys much stronger convergence properties, but is not practical. Onthe other hand, RGD is practical, but lacks theoretical performance guarantees. We believe bridging the gap betweenthem would be beneficial for both.AcknowledgementsThe first author would like to thank the support provided by the Brazilian agency “Coordenação de Aperfeiçoa-mento de Pessoal de Nível Superior” (CAPES), which made the current research possible. We also would like to thankA. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482479Helena, Tati, Bruninho and Carol for playing with our Acrobot simulator. Finally, we thank the reviewers from theArtificial Intelligence Journal, whose suggestions were of great value for the preparation of the final manuscript.Appendix A. Update equations for the Gaussian functionThe Gaussian function is given by:(cid:10)θi((cid:8)s) = exp−(cid:11)(cid:12)(cid:8)s − (cid:8)ci(cid:12)2σ 2i(A.1)where (cid:12).(cid:12) denotes the Euclidean norm and σi > 0. The value function computed by an RBF network using m Gaussianunits has the following form for a given action a:˜Qπa ((cid:8)s) =m(cid:2)i=1wai θi((cid:8)s).When using the on-line gradient-descent algorithm to minimize (7), the update rule for the RBFs’ widths is:(cid:17)σi= − ∂ε2∂σi= εwai 4θi((cid:8)s)(cid:12)(cid:12)(cid:8)s − (cid:8)ci(cid:12)2σ 3i(cid:15).(cid:13)(cid:14)always (cid:2)0It is easy to note that the nature of the change performed by the delta rule will depend on the sign of εwaother terms are non-negative. If εwahappens with the centers (cid:8)ci := − ∂ε2∂ (cid:8)cii > 0, the width will be enlarged; if εwai , since alli < 0, it will be reduced. Similar situation((cid:8)s − (cid:8)ci).= εwai(A.2)(cid:17) (cid:8)ci4θi((cid:8)s)σ 2i(cid:12) (cid:13)(cid:14) (cid:15)always (cid:2)0In this case, if εwafrom (cid:8)s.i > 0 the center (cid:8)ci will be moved toward the current state (cid:8)s; if εwai < 0 it will be moved awayAppendix B. RGD configuration on the experimentsThe RBF networks generated by RGD to approximate the value function Qπ (s, a) had the structure shown in (6),that is, one output layer for each possible action a. The Gaussian function given by (A.1) was used as the hidden units’activation in all the experiments. We always started the learning process with a single unit in the network’s hiddenlayer, and the widths of new RBFs were defined using (8). The hidden layer’s learning rate β was set as 0.1α, whereα is the learning rate used for the output layer (empirically, we found out that using β < α results in a more stablebehavior of RGD). On the experiments with the Acrobot we also tested α = β, as discussed in the text. The value ofα varied among experiments. As in the maze and mountain-car tasks the number of steps to learn was not the focus ofthe analysis, we used a small learning rate α = 10−4. The same learning rates were used for the RBF networks withfixed and tunable hidden units in Section 6.2. In Section 6.3, on the other hand, the learning performance of RGDwas compared with that of other algorithms, and thus a larger learning rate α = 10−1 was adopted. On the Acrobottask we used a decaying learning rate, as discussed in Section 6.4. No eligibility traces were used in the experiments,except in the pole-balancing task, where a decay rate of λ = 0.5 was adopted. With the SARSA algorithm an (cid:16)-greedyexploration strategy was used, and unless otherwise noted (cid:16) was set as 0.15 (which means the action presenting thelargest value function was selected 85% of the time and in the remaining an action was picked uniformly at random).The value for τ and σ1 are discussed in the text. All the parameters were determined empirically, based on a small setof preliminary experiments.480A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482References[1] C.W. Anderson, Learning and problem solving with multilayer connectionist systems, PhD thesis, Computer and Information Science, Uni-versity of Massachusetts, 1986.[2] C.W. Anderson, Learning to control an inverted pendulum using neural networks, IEEE Control Systems Magazine 9 (1989) 31–37.[3] C.W. Anderson, Q-learning with hidden-unit restarting, in: Advances in Neural Information Processing Systems, 1993, pp. 81–88.[4] L.C. Baird, Residual algorithms: Reinforcement learning with function approximation, in: International Conference on Machine Learning,1995, pp. 30–37.[5] A.G. Barto, M. Duff, Monte Carlo matrix inversion and reinforcement learning, in: Advances in Neural Information Processing Systems,vol. 6, Morgan Kaufmann, 1994, pp. 687–694.[6] A.G. Barto, R.S. Sutton, C.W. Anderson, Neuronlike adaptive elements that can solve difficult learning control problems, IEEE Transactionson Systems, Man, and Cybernetics 13 (1983) 834–846.[7] J. Baxter, P. Bartlett, Direct gradient-based reinforcement learning: I. Gradient estimation algorithms, Technical report, Research School ofInformation Sciences and Engineering, Australian National University, July 1999.[8] J. Baxter, L. Weaver, P. Bartlett, Direct gradient-based reinforcement learning: II. Gradient ascent algorithms and experiments, Technicalreport, Research School of Information Sciences and Engineering, Australian National University, July 1999.[9] R.E. Bellman, Dynamic Programming, Princeton University Press, 1957.[10] R.E. Bellman, A Markov decision process, Journal of Mathematical Mechanics 6 (1957) 679–684.[11] R.E. Bellman, Adaptive Control Processes, Princeton University Press, 1961.[12] H. Benbrahim, J. Franklin, Biped dynamic walking using reinforcement learning, Robotics and Autonomous Systems Journal (December1997).[13] D.P. Bertsekas, Dynamic Programming: Deterministic and Stochastic Models, Prentice-Hall, Inc., Upper Saddle River, NJ, 1987.[14] D.P. Bertsekas, J.N. Tsitsiklis, Neuro-Dynamic Programming, Athena Scientific, Belmont, MA, 1996.[15] H.-G. Beyer, H.-P. Schwefel, Evolution strategies: A comprehensive introduction, Natural Computing 1 (1) (2002) 3–52.[16] G. Boone, Efficient reinforcement learning: Model-based Acrobot control, in: International Conference on Robotics and Automation, vol. 1,IEEE Robotics and Automation Society, Albuquerque, NM, 1997, pp. 229–234.[17] G. Boone, Minimum-time control of the Acrobot, in: International Conference on Robotics and Automation, vol. 1, IEEE Robotics andAutomation Society, Albuquerque, NM, 1997, pp. 3281–3287.[18] J.A. Boyan, Technical update: Least-squares temporal difference learning, Machine Learning 49 (2002) 233–246.[19] J.A. Boyan, A.W. Moore, Generalization in reinforcement learning: Safely approximating the value function, in: Advances in Neural Infor-mation Processing Systems, MIT Press, Cambridge, MA, 1995, pp. 369–376.[20] S.J. Bradtke, A.G. Barto, Linear least-squares algorithms for temporal difference learning, Machine Learning 22 (1/2/3) (1996) 33–57.[21] D.S. Broomhead, D. Lowe, Multivariable functional interpolation and adaptive networks, Complex Systems 2 (1988) 321–355.[22] S.C. Brown, K.M. Passino, Intelligent control for an Acrobot, J. Intell. Robotics Syst. 18 (3) (1997) 209–248.[23] R.H. Crites, A.G. Barto, Improving elevator performance using reinforcement learning, in: Advances in Neural Information Processing Sys-tems, vol. 8, MIT Press, Cambridge, MA, 1996, pp. 1017–1023.[24] P. Dayan, T. Sejnowski, TD(λ) converges with probability 1, Machine Learning 14 (1994) 295–301.[25] M. Dorigo, M. Colombetti, Robot shaping: Developing autonomous agents through learning, Artificial Intelligence 71 (1994) 321–370.[26] J. Farrel, T. Berger, On the effects of the training sample density in passive learning control, in: American Control Conference, 1995, pp. 872–876.[27] B. Fritzke, Growing cell structures—a self-organizing network for unsupervised and supervised learning, Neural Networks 7 (9) (1994) 1441–1460.[28] B. Fritzke, A growing neural gas network learns topologies, in: G. Tesauro, D.S. Touretzky, T.K. Leen (Eds.), Advances in Neural InformationProcessing Systems, vol. 7, MIT Press, Cambridge, MA, 1995, pp. 625–632.[29] F. Girosi, T. Poggio, Networks and the best approximation property, Technical Report AIM-1164, Massachusetts Institute of TechnologyArtificial Intelligence Laboratory and Center for Biological Information Processing Whitaker College, 1989.[30] D. Goldberg, Real-coded genetic algorithms, virtual alphabets, and blocking, Technical Report IlliGAL Report 90001, Illinois Genetic Algo-rithms Laboratory, Dept. of General Engineering—University of Illinois, Urbana, IL, USA, 1990.[31] F. Gomez, J. Schmidhuber, R. Miikkulainen, Efficient non-linear control through neuroevolution, in: ECML 2006: 17th European Conferenceon Machine Learning, Springer, Berlin, 2006.[32] G.J. Gordon, Stable function approximation in dynamic programming, in: International Conference on Machine Learning, Morgan Kaufmann,San Francisco, CA, 1995, pp. 261–268.[33] G.J. Gordon, Reinforcement learning with function approximation converges to a region, in: Advances in Neural Information ProcessingSystems, 2000, pp. 1040–1046.[34] C. Guestrin, M. Hauskrecht, B. Kveton, Solving factored MDPs with continuous and discrete variables, in: 20th Conference on Uncertaintyin Artificial Intelligence, 2004.[35] C. Igel, Neuroevolution for reinforcement learning using evolution strategies, in: Congress on Evolutionary Computation (CEC 2003), vol. 4,IEEE Press, 2003, pp. 2588–2595.[36] T. Jaakkola, M.I. Jordan, S.P. Singh, On the convergence of stochastic iterative dynamic programming algorithms, Neural Computation 6(1994).[37] L.P. Kaelbling, M.L. Littman, A.P. Moore, Reinforcement learning: A survey, Journal of Artificial Intelligence Research 4 (1996) 237–285.A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482481[38] M. Keijzer, J.J. Merelo, G. Romero, M.G. Schoenauer, Evolving objects: A general purpose evolutionary computation library, ArtificialEvolution 2310 (2002) 231–242.[39] M.G. Lagoudakis, R. Parr, Least-squares policy iteration, Journal of Machine Learning Research 4 (2003) 1107–1149.[40] M.G. Lagoudakis, R. Parr, M.L. Littman, Least-squares methods in reinforcement learning for control, in: SETN, 2002, pp. 249–260.[41] L.-J. Lin, Self-improving reactive agents based on reinforcement learning, planning and teaching, Machine Learning 8 (1992) 293–321.[42] H.B. Mann, D.R. Whitney, On a test of whether one of 2 random variables is stochastically larger than the other, Annals of MathematicalStatistics 18 (1947) 50–60.[43] P.H. McQuesten, Cultural enhancement of neuroevolution, PhD thesis, The University of Texas at Austin, 2002.[44] I. Menache, S. Mannor, N. Shimkin, Basis function adaptation in temporal difference reinforcement learning, Annals of Operations Research—Special Issue on the Cross Entropy Method 134 (2005) 215–238.[45] D. Michie, R. Chambers, BOXES: An experiment on adaptivecontrol, Machine Intelligence 2 (1968) 125–133.[46] J.D.R. Millán, D. Posenato, E. Dedieu, Continuous-action Q-learning, Machine Learning 49 (2002) 247–265.[47] D.E. Moriarty, R. Miikkulainen, Efficient reinforcement learning through symbiotic evolution, Machine Learning 22 (1–3) (1996) 11–32.[48] D. Ormoneit, S. Sen, Kernel-based reinforcement learning, Machine Learning 49 (2002) 161–178.[49] T.J. Perkins, D. Precup, A convergent form of approximate policy iteration, in: Advances in Neural Information Processing Systems, vol. 15,MIT Press, Cambridge, MA, 2003, pp. 1595–1602.[50] J.C. Platt, A resource-allocating network for function interpolation, Neural Computation 3 (2) (1991) 213–225.[51] T. Poggio, F. Girosi, Network for approximation and learning, Proceedings of the IEEE 78 (9) (September 1990) 1481–1497.[52] M.J.D. Powell, Radial basis functions for multivariable interpolation: A review, in: J.C. Mason, M.G. Cox (Eds.), Algorithms for Approxima-tion, Clarendon Press, Oxford, 1987, pp. 143–167.[53] D. Precup, R.S. Sutton, S. Dasgupta, Off-policy temporal-difference learning with function approximation, in: 18th International Conferenceon Machine Learning, Morgan Kaufmann, San Francisco, CA, 2001, pp. 417–424.[54] M.L. Puterman, Markov Decision Processes—Discrete Stochastic Dynamic Programming, Wiley-Interscience, 1994.[55] B. Ratitch, On characteristics of Markov decision processes and reinforcement learning in large domains, PhD thesis, School of ComputerScience, McGill University, Montréal, 2004.[56] S.I. Reynolds, The stability of general discounted reinforcement learning with linear function approximation, in: UK Workshop on Computa-tional Intelligence, 2002.[57] G. Rummery, M. Niranjan, On-line q-learning using connectionist systems, Technical Report CUED/F-INFENG/TR 166, CambridgeUniversity—Engineering Department, 1994.[58] P.N. Sabes, Approximating Q-values with basis function representations, in: 1993 Connectionist Models Summer School, Lawrence ErlbaumAssoc. Inc., Hillsdale, NJ, 1993.[59] K. Samejima, T. Omori, Adaptive internal state space construction method for reinforcement learning of a real-world agent, Neural Net-works 12 (1999) 1143–1155.[60] A.L. Samuel, Some studies in machine learning using the game of checkers, IBM Journal on Research and Development 3 (1959) 211–229.[61] A.L. Samuel, Some studies in machine learning using the game of checkers. ii—recent advances, IBM Journal on Research and Develop-ment 11 (1967) 601–617.[62] W. Sarle, Stopped training and other remedies for overfitting, in: Proceedings of the 27th Symposium on Interface, 1995.[63] R. Schoknecht, A. Merke, Convergent combinations of reinforcement learning with linear function approximation, in: Advances in NeuralInformation Processing Systems, vol. 15, MIT Press, Cambridge, MA, 2003, pp. 1579–1586.[64] W. Schultz, P. Dayan, P.R. Montague, A neural substrate of prediction and reward, Science 275 (1997) 1593–1599.[65] S.P. Singh, D. Bertsekas, Reinforcement learning for dynamic channel allocation in cellular telephone systems, in: Advances in Neural Infor-mation Processing Systems, vol. 9, MIT Press, Cambridge, MA, 1997, p. 974.[66] S.P. Singh, T. Jaakkola, M.L. Littman, C. Szepesvari, Convergence results for single-step on-policy reinforcement-learning algorithms, Ma-chine Learning 38 (3) (2000) 287–308.[67] S.P. Singh, R.S. Sutton, Reinforcement learning with replacing eligibility traces, Machine Learning 22 (1–3) (1996) 123–158.[68] S.P. Singh, R.C. Yee, An upper bound on the loss from approximate optimal-value functions, Machine Learning 16 (3) (1994) 227–233.[69] W.D. Smart, L.P. Kaelbling, Practical reinforcement learning in continuous spaces, in: International Conference on Machine Learning, 2000,pp. 903–910.[70] M.W. Spong, The swing up control problem for the Acrobot, IEEE Control Systems Magazine 15 (1995) 49–55. Reprinted in Neurocomputing:Foundation of Research.[71] K.O. Stanley, R. Miikkulainen, Efficient reinforcement learning through evolving neural network topologies, in: GECCO 2002: Genetic andEvolutionary Computation Conference, Morgan Kaufmann, New York, 2002, pp. 569–577.[72] P. Stone, R.S. Sutton, Scaling reinforcement learning toward RoboCup soccer, in: 18th International Conference on Machine Learning, MorganKaufmann, San Francisco, CA, 2001, pp. 537–544.[73] R. Sutton, D. McAllester, S. Singh, Y. Mansour, Policy gradient methods for reinforcement learning with function approximation, in: Advancesin Neural Information Processing Systems, 2000, pp. 1057–1063.[74] R.S. Sutton, Learning to predict by the methods of temporal differences, Machine Learning 3 (1988) 9–44.[75] R.S. Sutton, Generalization in reinforcement learning: Successful examples using sparse coarse coding, in: Advances in Neural InformationProcessing Systems, vol. 8, MIT Press, Cambridge, MA, 1996, pp. 1038–1044.[76] R.S. Sutton, A.G. Barto, Time-derivative models of Pavlovian reinforcement, in: Learning and Computational Neuroscience: Foundations ofAdaptive Networks, MIT Press, Cambridge, MA, 1990, pp. 497–537.[77] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge, MA, 1998. A Bradford Book.482A. da Motta Salles Barreto, C.W. Anderson / Artificial Intelligence 172 (2008) 454–482[78] V. Tadi´c, On the convergence of temporal-difference learning with linear function approximation, Machine Learning 42 (3) (2001) 241–267.[79] G.J. Tesauro, TD-Gammon, a self-teaching backgammon program achieves master-level play, Neural Computation 6 (2) (1994) 215–219.[80] S. Thrun, A. Schwartz, Issues in using function approximation for reinforcement learning, in: Fourth Connectionist Models Summer School,Lawrence Erlbaum Associates, Hillsdale, NJ, 1993.[81] J.N. Tsitsiklis, B. Van Roy, Feature-based methods for large scale dynamic programming, Machine Learning 22 (1996) 59–94.[82] J.N. Tsitsiklis, B. Van Roy, An analysis of temporal-difference learning with function approximation, IEEE Transactions on Automatic Con-trol 42 (May 1997) 674–690.[83] C. Watkins, Learning from delayed rewards, PhD thesis, University of Cambridge, England, 1989.[84] C. Watkins, P.D. Dayan, Q-learning, Machine Learning 8 (1992) 279–292.[85] S.E. Weaver, L.C. Baird, M.M. Polycarpou, Preventing unlearning during on-line training of feedforward networks, in: International Sympo-sium of Intelligent Control, Gaithersburg, 1998, pp. 359–364.[86] D.A. White, D.A. Sofge, Handbook of Intelligence Control, Neural, Fuzzy and Adaptive Approaches, Van Nostrand Reinhold, New York,1992. Chapter: Applied learning: Optimal control for manufacturing.[87] D. Whitley, M. Richards, R. Beveridge, A. da Motta Salles Barreto, Alternative evolutionary algorithms for evolving programs: Evolutionstrategies and steady-state GP, in: Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation (GECCO 2006),vol. 1, ACM Press, Seattle, Washington, 2006, pp. 919–926. Winner best GP paper.[88] D. Whitley, S. Dominic, R. Das, C.W. Anderson, Genetic reinforcement learning for neurocontrol problems, Machine Learning 13 (2–3)(1993) 259–284.[89] R.J. Williams, L.C. Baird, Tight performance bounds on greedy policies based on imperfect value functions, Technical Report NU-CCS-93-14,Northeastern University, November 1993.[90] W. Zhang, T.G. Dietterich, A reinforcement learning approach to job-shop scheduling, in: International Joint Conference on Artificial Intelli-gence, 1995.[91] D.Z. Zhao, J. Yi, GA-based control to swing up an Acrobot with limited torque, Transactions of the Institute of Measurement and Control 28 (1)(2006) 3–13.