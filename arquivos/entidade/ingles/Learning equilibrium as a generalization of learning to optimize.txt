Artificial Intelligence 171 (2007) 448–452www.elsevier.com/locate/artintLearning equilibrium as a generalization of learning to optimize ✩Dov Monderer ∗, Moshe TennenholtzFaculty of Industrial Engineering and Management, Technion – Israel Institute of Technology, Haifa 32000, IsraelReceived 15 May 2006; received in revised form 20 October 2006; accepted 21 December 2006Available online 26 January 2007AbstractWe argue that learning equilibrium is an appropriate generalization to multi-agent systems of the concept of learning to optimizein single-agent setting. We further define and discuss the concept of weak learning equilibrium.© 2007 Elsevier B.V. All rights reserved.Keywords: Learning; Machine learning; Learning equilibrium1. PrefaceIn [16], Shoham, Powers, and Grenager (SPG) present five distinct agendas in multi-agent learning. In this man-uscript we discuss their third agenda—the normative approach. SPG mention that the requirement that learningalgorithms would be an equilibrium may serve as a synonyms to this approach. We claim that the equilibrium ap-proach is indeed the right one if the question is: what should a mediator who makes recommendations to all players,recommend. The equilibrium property seems to be a necessary ingredient in such recommendations. As economistsdo not tend to consider mediators (competing firms do not go to a central mediator to determine their, say, pricingpolicy)1 it is of no surprise that the equilibrium agenda is not a major issue in the learning literature in economics.However, when, say eBay provides the participants a proxy service, then it actually plays the role of a mediator (andnot only of an organizer).The theory of learning in multi-agent systems inherits all the conceptual and practical difficulties of learning insingle-agent settings, as well as all difficulties of analyzing behavior in multi-agent settings. Therefore, in order todefine and understand the equilibrium approach to learning in multi-agent systems we phrase it as an extension ofwork on learning in single-agent systems.22. Learning in single-agent systemsRoughly speaking, one could partition work on learning in single-agent systems into two major but not necessarilyindependent categories:✩ Both authors thank the Israeli Science Foundation and the Fund for the Promotion of Research at the Technion for the support of their research.* Corresponding author.E-mail address: dov@ie.technion.ac.il (D. Monderer).1 Some well-known exceptions to this statement can be found in the literature on correlated equilibrium [4], and on communication equilibrium[8,15]. However, the general theme in economics is that there is no mediator in the system that recommends behavior to the agents.2 For the sake of exposition we introduce all notations in this paper with pure strategies.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.01.002D. Monderer, M. Tennenholtz / Artificial Intelligence 171 (2007) 448–452449• Descriptive theory-prediction: Given examples of past behavior of a system, and some background information,we would like to predict the future behavior of the system. Classical work in statistics, inductive inference andsupervised learning in AI fit into this category, as well as much work in data mining and the classification ofsubjects in psychology.• Normative theory-optimization: Given partial information about a system, our aim is to devise algorithms foroptimizing an agent’s behavior in the system. Typically, the interleaving of exploration and exploitation is needed.Work on reinforcement learning in AI, as well bandits problems fit into this category.We only discuss our view about the extension to multi-agent systems of the normative approach. That is, we usethe word learning in a single-agent setting as a synonym for optimization in dynamic situations with incompleteinformation.Consider a single agent facing a dynamic decision problem with incomplete information, D, defined by a set ofdynamic decision problems Dω with a parameter ω ∈ Ω. Each ω ∈ Ω is called a state of nature. Nature chooses ω, butthe agent does not know ω. However, he possesses some initial information about the state of nature, and he acquiresadditional partial information after every stage. For example, every Dω can be a Markov decision problem. If we havea prior probability distribution over the set Ω we call the problem a Bayesian dynamic decision problem. If we wantto stress the fact that such a prior probability does not exist we call the problem a Pre-Bayesian dynamic decisionproblem.3A strategy of the decision maker at each Dω is called a policy. A strategy of the agent in the decision problem withincomplete information, D, is called in this paper an algorithm. We assume that had the agent known ω he would havechosen an optimal policy, which would have given him the long-run value, v(ω), of Dω. More precisely, let Uω be thelong-run reward function of the problem Dω, and let S(ω) be the set of possible policies for this problem. A policyf ω ∈ S(ω) is an optimal policy at Dω ifmaxgω∈S(ω)Uω(gω) = Uω(f ω).The value of Dω is the real-valued function v defined on Ω as follows:v(ω) = maxgω∈S(ω)Uω(gω) ∀ω ∈ Ω.(1)(2)Ideally, in a dynamic decision problem with incomplete information an optimizing agent would use an algorithmthat guarantees v(ω) for every ω. This approach is mainly taken in machine learning. We accept this view; in ourview the right notion for a learning-to-optimize algorithm for a decision problem with incomplete information is thefollowing: It is an algorithm that yields an optimal policy at every ω.4 More precisely:Let f be an algorithm for D. For every ω we denote by fω, the policy induced by f on Dω.f is a learning-to-optimize algorithmin D if for every w, fω is an optimal algorithm in Dω, that ismaxgω∈S(ω)Uω(gω) = Uω(fω) ∀ω ∈ Ω.(3)An equivalent definition will be useful in the sequel.The set of all algorithms for D is denoted by S. For every f ∈ S and for every ω ∈ Ω define U (ω, f ) = Uω(fω).Obviously, f is a learning-to-optimize algorithm in D if and only ifU (ω, g) = U (ω, f ) ∀ω ∈ Ω.maxg∈S(4)Unfortunately, there exist dynamic decision problems for which a learning-to-optimize algorithms do not exist.In Bayesian dynamic decision problems it is customary to look for algorithms that maximize the long-run expectedreward of the agent. Such algorithms generally exist. We call such an algorithm an optimal Bayesian algorithm.3 In economics, it is customary to relate to a Bayesian model as a model with incomplete information. Until recently, a model without priors wasnot given a special name. Recently, such games have received several titles in various papers. In this paper we follow the terminology of [11], andwe refer to such games as pre-Bayesian.4 Practically, the definition would be more elaborate, and would refer to various accuracy parameters. Notice that we refer here to the long-runvalue mentioned above.450D. Monderer, M. Tennenholtz / Artificial Intelligence 171 (2007) 448–452That is, f is an optimal Bayesian algorithm if(cid:2)(cid:2)maxg∈SΩU (ω, g) dμ(ω) =U (ω, f ) dμ(ω),Ωwhere μ is the prior probability on Ω.5(5)It is important to note that in a Bayesian dynamic decision problem, every learning-to-optimize algorithm is alsoan optimal Bayesian algorithm, but the converse does not necessarily hold.63. Learning in multi-agent settingsWe take the position of a mediator who is about to assign algorithms to a set of selfish agents, N = {1, 2, . . . , n} whoare engaged in a multistage game with incomplete information, G.7 This game, G is defined by a set of multi-stagegames with complete information, Gω with a parameter ω ∈ Ω. Nature chooses a state of nature ω, but the agents donot know ω. However, each agent possesses some initial private information about the state of nature, and he acquiresadditional partial information after every stage. For example, every Gω can be a repeated game. If we have a priorprobability distribution over the set Ω we say that G is a Bayesian multi-stage game. If we want to stress the factthat such a prior probability does not exist we call G a pre-Bayesian multi-stage game. In the complete informationcase, when dealing with the multi-agent setting, the term policy used in the single agent setting is replaced by the termstrategy. As in the single agent setting, a strategy of an agent in G is called an algorithm.The first question is what is the analogous concept of an “optimal policy” in the single-agent setup in the game Gω,in which the agents know ω. This is one of the most important conceptual issues dealt with in game theory. We take theposition that in the presence of a mediator, optimality means equilibrium. That is, the strategy given to every agent i isoptimal if all other agents are using their strategies in the profile.8 Hence, the analogous definition to an optimal policyin the single agent decision problem with complete information is: A profile of strategies, f ω = (f ωn ) isan equilibrium profile in Gω ifi (gωU ωi (f ω) = maxU ω∈Si (ω)−i) ∀i ∈ N.2 , . . . , f ω1 , f ωi , f ω(6)gωiNote, however, that except for two-person zero-sum games, the concept of a value function does not have a well-defined meaning in the multi-agent model.It is important to stress again the existence of a mediator in order to understand our approach. We do not claimthat economic agents play in equilibrium. We do not claim that an agent who is facing a multi-agent decision problemshould use an equilibrium strategy; This is because we cannot be sure that other agents would use an equilibriumstrategy, and even if they do they may stick to another equilibrium. However, a reliable mediator who provides allagents with algorithms can expect players to use the algorithms only if the profile of algorithms is in equilibrium.Hence, we assume that had the agents known ω they would have chosen an optimal profile of strategies, i.e. theywould have behaved according to an equilibrium profile of Gω.Extending upon the single-agent perspective, in our opinion the right notion for a learning-to-optimize algorithmprofile in a pre-Bayesian multi-stage game is the following notion of learning equilibrium: It is a profile of algo-rithms that yield an equilibrium profile of strategies at the multi-stage game, Gω, for every ω. That is, f is a learningequilibrium if(cid:3)(cid:4)max∈Si (ω)gωii (gωU ωi , f ω−i) = U ωi (f ω) ∀i ∈ N∀ω ∈ Ω.(7)5 In models in which Uω is defined as the limit of averages, it is some times customary to take the limit before the integral in (5).6 Some of the literature in single-agent learning deals with the issue of when a given optimal Bayesian algorithm is also a learning-to-optimizealgorithm.7 Recall that this approach is heavily based on the existence of a mediator. The reader may consult [1] in order to see one of the authors’ approachto learning to optimize in the absence of such a mediator.8 In particular models one can focus on particular refinements of Nash equilibrium like sub-game perfect equilibrium, sequential equilibrium ordominated strategy equilibrium.D. Monderer, M. Tennenholtz / Artificial Intelligence 171 (2007) 448–452Alternatively, one can use the analogous definition to (5). That is, f is a learning equilibrium if and only if∀ω ∈ Ω.Ui(ω, gi, f−i) = Ui(ω, f ω) ∀i ∈ N(cid:3)(cid:4)maxgi ∈Si451(8)Notice that condition (7) has a local flavor in the sense that in order to show that f is not a learning equilibrium onehas to find for some player i, a state ω and a strategy for i in the game Gω that violates (7). On the other hand, therequirement in (8) is a global one in the sense that in order to show that f is not a learning equilibrium, one has to findfor some player i, an algorithm in G that violates (8) for some ω. The global definition is just the classical definitionof ex-post equilibrium. Hence, the terms learning equilibrium and ex post equilibrium coincide for multi-stage gameswith incomplete information.9In the context of Bayesian multi-stage game, the equilibrium approach implies the use of a profile of algorithmsthat form a Bayesian equilibrium. Recall that f is a Bayesian equilibrium if(cid:2)(cid:2)maxgi ∈SiΩUi(ω, gi, f−i) dμ(ω) =Ui(ω, f ω) dμ(ω) ∀i ∈ N.(9)ΩLike in the single agent setting, in a Bayesian multi-stage game, every learning equilibrium is also a Bayesianequilibrium, but the converse does not necessarily hold.While in general Bayesian equilibrium exists (with mixed strategies), and learning equilibrium may not exist, recentwork have shown that (surprisingly) learning equilibrium does exist in several rich settings [3,5,6].When a learning equilibrium does not exist one may use other notions of equilibrium in pre-Bayesian games inorder to define learning. For example, minimax-regret equilibrium [12] and safety level equilibrium [2] can be used.These other notions have the advantage of existence, but they will not be state-wise optimal in the sense of yielding aNash equilibrium at every state of nature.4. Weak learning equilibriumFor ease of exposition we discuss in this section only a simple type of multi-stage game with incompleteinformation—repeated games with incomplete information. Hence, for every ω there is a one-shot game G(ω) suchthat Gω is the dynamic game, in which G(ω) is played in every stage. In G(ω) the players choose among possibleactions. Hence, every strategy profile f ω in Gω generates an infinite path of action profiles in G(ω). Moreover, weonly discuss the long-run utility function defined by the limit of averages of payoffs in the one stage games.As mentioned, in the Bayesian setting, if a learning equilibrium does not exist, it is natural to require that thealgorithm profile satisfies the global optimality required by a Bayesian equilibrium. In such an equilibrium an agentwho considers his expected long-run payoff would not deviate from the algorithm designed for him. However, a profileof algorithms, which forms a Bayesian equilibrium might not satisfy the state-wise optimality requirement of beingin equilibrium in every state of nature.Kalai and Lehrer [13] defined a notion of “weak” state-wise optimality-like property, which may be satisfied bya profile of algorithms. Their notion inspires the following definition. An algorithm profile f is a weak learningequilibrium if it is a Bayesian equilibrium that satisfies the following property: the path of actions generated at everystate of nature is an equilibrium path from a certain stage on. That is, for every ω there exists an equilibrium strategyprofile gω at G(ω) and an integer T (ω) such that the profile of actions generated by f ω and gω at stage T coincide forevery T (cid:2) T (ω).For example, if each G(ω) is a Prisoner’s Dilemma game (where different ω’s determine different payoff functionsof the prisoners dilemma), then a Bayesian equilibrium algorithm profile that for every ω generates the cooperativeoutcome at every stage would be a weak learning equilibrium, because by the Folk theorem, for each ω there exists arepeated game equilibrium strategy profile that generates the path “cooperation at every stage”.Kalai and Lehrer [13] proved the existence of a mixed-strategy version of weak learning equilibrium for a Bayesianrepeated game with a countable set of states of nature in which every player knows his own payoff, and only his ownpayoffs.9 The original definition of learning equilibrium in [5,6], and its generalizations, in particular to the concept of robust learning equilibrium in [3],used the global approach.452D. Monderer, M. Tennenholtz / Artificial Intelligence 171 (2007) 448–4525. Some related literature from economicsThe literature on learning in economics suggests a weakening of the notion of Bayesian equilibrium, by requir-ing self-confirming equilibrium. This notion has been implicitly defined by Fudenberg and Kreps [9] and furtherdeveloped by Dekel, Fudenberg and Levine [7,10]. A related variant, titled subjective equilibrium, was defined andanalyzed by Kalai and Lehrer [13,14]. In a self confirming equilibrium the beliefs of the players may not coincideoff equilibrium path. This definition makes a lot of sense in economics environments, where the algorithms of theplayers are independently chosen , but makes less sense when the algorithms are suggested to the players by a media-tor. Dekel, Fudenberg, and Levine [7] discussed situations under which self-confirming equilibrium satisfies the weaknotion of local optimality suggested by [13], yielding in our terminology a mixed-strategy version of weak learningequilibrium.6. SummaryWe consider multi-stage games with incomplete information, in which a mediator provides the agents with algo-rithms. We focus on generalizing learning-to-optimize algorithms in the single-agent setting to learning equilibriumin a multi-agent setting. The existence of such a mediator is a major issue in our setting. Our generalization refers totwo requirements that turn out to imply each other: the algorithms should be state-wise optimal, as well as globallyoptimal, where optimality in the multi-agent setting is captured by the notion of equilibrium. Hence, in a learningequilibrium, an agent does not attempt to learn the other players’ algorithms but rather she takes these algorithmsas given, and she tries to optimize. This optimization may yield gradual learning of the true state through stages ofexploration and exploitation.References[1] A. Altman, A. Boden-Bercovici, M. Tennenholtz. Learning in one-shot strategic form games, in: Proceedings of ECML-06, 2006.[2] I. Ashlagi, D. Monderer, M. Tennenholtz, Resource selection games with unknown number of players, in: Proceedings of the 5th InternationalJoint Conference on Autonomous Agents and Multiagent Systems (AAMAS-06), 2006.[3] I. Ashlagi, D. Monderer, M. Tennenholtz, Robust learning equilibrium, in: Proceedings of the 22nd Conference on Uncertainty in ArtificialIntelligence (UAI 2006), 2006.[4] R.J. Aumann, Subjectivity and correlation in randomized strategies, Journal of Mathematical Economics 1 (1974) 67–96.[5] R. Brafman, M. Tennenholtz, Optimal efficient learning equilibrium: Imperfect monitoring in symmetric games, in: Proceedings of AAAI-2005, AUAI Press, 2005.[6] R.I. Brafman, M. Tennenholtz, Efficient learning equilibrium, Artificial Intelligence 159 (2004) 27–47.[7] E. Dekel, D. Fudenberg, D.K. Levine, Payoff information and self-confirming equilibrium, Journal of Economic Theory 89 (2) (1999) 165–185.[8] F. Forges, An approach to communication equilibrium, Econometrica 54 (6) (1986) 1375–1385.[9] D. Fudenberg, D.M. Kreps, Learning in extensive games, I: Self-confirming equilibrium, Games and Economic Behavior 8 (1995) 20–55.[10] D. Fudenberg, D.K. Levine, Self-confirming equilibrium, Econometrica 61 (1993) 523–546.[11] R. Holzman, N. Kfir-Dahav, D. Monderer, M. Tennenholtz, Bundling equilibrium in combinatorial auctions, Games and Economic Behav-ior 47 (2004) 104–123.[12] N. Hyafil, C. Boutilier, Regret minimizing equilibria and mechanisms for games with strict type uncertainty, in: Proceedings of the 20thAnnual Conference on Uncertainty in Artificial Intelligence (UAI-04), Arlington, VA, AUAI Press, 2004, pp. 268–277.[13] E. Kalai, E. Lehrer, Rational learning leads to Nash equilibrium, Econometrica 61 (5) (1993) 1019–1045.[14] E. Kalai, E. Lehrer, Subjective equilibrium in repeated games, Econometrica 61 (5) (1993) 1231–1240.[15] R.B. Myerson, Multistage games with communication, Econometrica 54 (2) (1986) 323–358.[16] Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Stanford University Discussion Paper, 2006.