ELSEVIER Artificial Intelligence 73 ( 1995) 271-306 Artificial Intelligence Reinforcement learning of non-Markov decision processes Steven D.Whitehead a**, Long-Ji Lin b,* a GTE Laboratories Incorporated, 40 Sylvan Road, Waltham, MA 02254, USA h School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA Received September 1992; revised April 1993 Abstract Techniques based on reinforcement learning (RL) have been used to build systems that learn to perform nontrivial sequential decision tasks. To date, most of this work has focused on learning tasks that can be described as Markov decision processes. While this formalism is useful for modeling a wide range of control problems, there are important tasks that are inherently non- Markov. We refer to these as hidden state tasks since they arise when information relevant to from the agent’s immediate identifying sensation. Two important types of control problems that resist Markov modeling are those in which ( 1) the system has a high degree of control over the information collected by its sensors (e.g., as in active vision), or (2) the system has a limited set of sensors that do not always provide adequate information about the current state of the environment. Existing RL algorithms perform unreliably on hidden state tasks. the state of the environment is hidden (or missing) This article examines two general approaches to extending reinforcement learning to hidden state tasks. The Consistent Representation (CR) Method unifies recent approaches such as the Lion algorithm, the G-algorithm, and CS-QL. The method is useful for learning tasks that require the agent to control its sensory inputs. However, it assumes that, by appropriate control of perception, the external states can be identified at each point in time from the immediate sensory inputs. A second, more general set of algorithms in which the agent maintains internal state over time is also considered. These stored-state algorithms, though quite different in detail, share the common feature that each derives its internal representation by combining immediate sensory inputs with internal state which is maintained over time. The relative merits of these methods are considered and conditions for their useful application are discussed. * Correponding ’ Currently at Siemens Corporate Research author. Fax: (617) 890-9320. E-mail: swhitehead@gte.com. 755 College Road East, Princeton, NJ 08540, USA. Fax: (609) 734-6565. E-mail: Incorporated, Ijl@learning.scr.siemens.com. 0004-3702/95/$09.50 SSDI 0004-3702( @ 1995 Elsevier Science B.V. All rights reserved 94)00012-P 272 S.D. Whitehead. L.-J. Lin/Artijiciul lnfelligence 73 (1995) 271-306 1. Introduction is necessary robots cannot be achieved theories of agent-environment interaction need to include an account of to develop and maintain intelligent agents. Sophisticated too complex, idiosyncratic, are too rigid and through meticulous programming and uncertain inexpressive alone. The real to know ahead of time, and to be for programming alone agents must bear at least some of the burden of skill acquisition the world does not stand still, to maintain a high agents must learn new skills and adapt old ones to changes level in the is much Computational learning. Learning real-world world programming feasible. Intelligent themselves. Also, because of performance, environment. Though languages there are many kinds of learning, and many things in the end, all learning boils down to learning control. The value of anything only be measured in terms of its ability in terms of its effect on the agent’s the environment interaction with its environment, This article to control focuses on reinforcement that is well suited to learning, environments to a desired end. a paradigm [ 7,461. that an agent might learn, learned can is modeled as a controller In reinforcement learning to a finite state are unknown). At each time step, the controller to change state and generate a payoff. a that maximizes control policy coupled control (whose in highly interactive interaction transition probabilities learning the agent-environment machine performs an action which causes The agent’s objective measure of the total payoff received over time. that make reinforcement the environment Some of the features ( 1) RL is a weak method is to learn a state-dependent in that through (a) (b) trial-and-error learning occurs ment; the feedback used for learning teacher, who offers the “correct answer” learning (RL) appealing are: experimentation with the environ- takes the form of a scalar payoff-no explicit is required; (c) on sequential decision making tasks, payoffs may be sparse and considerably delayed; and little or no prior knowledge (d) is required. (2) RL is incremental (3) RL can be used to learn direct sensory-motor mappings, and is, thus, appropriate to unexpected tasks in which the agent must respond quickly and can be used online. for highly reactive events in the environment. (4) RL is valid in nondeterministic (5) When used in conjunction with temporal difference environments. (TD) methods has proven checkers to be effective on difficult sequential decision making [ 381, pole balancing [ 8,301, and backgammon [ 491) . (6) RL architectures incorporate supervised are extensible. Recently, RL systems have been extended aspects of planning [ 15,24,56] learning [ 25,45,5X], and hierarchical intelligent exploration Traditionally, research in RL has focused on Markov decision processes [ 19,441, RL (e.g., tasks to [ 21,23,5 11, control [ 16,26,4 1,621. intuitively the agent directly observes the (MDPs). corresponds Described to a control formally in Section 2, a Markov decision process task in which at each point in time (a) S.D. Whitehead, L.-J. Lin/Artijcial Intelligence 73 (1995) 271-306 273 (b) and to predicting in its internal in time, encodes all the information In most applications, the effects of actions for two reasons. First, the agent does not observe relevant It is important the effects of actions depend only upon in the sense of being given a label which names the action the state of the state. repre- is known as important focusing on Markov decision processes of stochastic processes and learning methods, [ 441, rely on the Markov property during credit assignment, to apply the classical mathematicals [ 7,44,54,55]. state. directly, that the agent, at each point state of the environment and the current the environment This assumption sentation the Murkov assumption. to the theoretical development has allowed dynamic programming which use TD-methods and may perform badly when are important as Markov decision processes. These non-Markov hidden information situation. they occur whenever (or missing) to as for a relevant piece of of the current [ 18,593. Nevertheless, (or easily) is violated that are not naturally tusks, since to be hidden there formulated tasks are commonly control problems Second, existing of RL, because the assumption it is possible representation reinforcement it has been the agent’s researchers referred learning from state learning to distinguish then guessing into bins according than guess a classification. in the context of autonomous Hidden state tasks arise naturally to their color, say Bin-l red from blue, robots. The the agent’s sensors for the task at hand. Suppose a robot is charged with the task of sorting for red, Bin-2 for blue. If the robot’s it can do then for any given block If there are an equal number of blocks of each if the robot to achieve 100% performance. The former case is missing simplest example of a hidden state task is one which occurs when are inadequate blocks sensors are unable no better color, can detect color, corresponds from the agent’s available In general, readings, to uniquely needed information task, then the decision problem is always available. sensor the sensors do not provide all the to the internal if there are circumstances identify is non-Markov. The latter case is Markov since once a color sense the state of the environment with respect the information if a robot’s than chance. On the other hand, is defined only by its immediate to achieve optimal performance can do no better decision problem, to a non-Markov since relevant representation. it can easily representation information in which needed learn and is state resources perception of integrating [ 601. In active perception, tasks are also a natural consequence Hidden tive/selective over the allocation of its sensory is used to sense visual processing modules) an efficient, is not properly maintained data generated by the sensors may fail to code a relevant piece of information, resultant the environment. be periods of time when decision learning with ac- the agent has a degree of control (e.g., controlling visual attention or selecting in then the and the to the current state of there will its sensors, the representation may be ambiguous with respect that if the agent must learn to control It follows [ 3,5,6]. This control if control representation will be inadequate. Therefore task-specific way. However, the environment the internal internal task will be non-Markov. for applying Techniques reinforcement learning to non-Markov the central technique focus of this article. We describe a generalized sistent Representation (CR) Method that can be used to learn control the CR-method active perception [ 571. The principal idea underlying is decision processes called the Con- in systems with is to split con- 214 S.D. Whitehead, L.-J. Lin/Artificial Intelligence 73 (I 995) 271-306 phase and an overt phase. During (or sensor configuration) actions the perceptual in order to gen- of the current external environment. is used to select overt action; that is, an into representation that changes (i.e., Markov) the overt stage, this representation the system performs two phases, a perceptual sensing trol phase, erate an adequate During action representation method also the perceptual of the environment. Although, Lion algorithm in which proper control of its sensors. It is not appropriate state information state. the agent can always the CR-method learn not only actions needed [60], CS-QL identify the state of the external environment. Systems using the consistent the overt actions needed to perform a task, but to construct an adequate, task-specific representation [48], and the G-algorithm the current unifies such recent algorithms [ 131, it is restricted state of the environment as the to tasks through for tasks in which the agent must store in order to infer the current over time, or remember previous events speech sensory immediate To overcome are considered this restriction, to achieve a crude several, more general approaches is one which augments form of short recognition that retain state information over time. We refer to these as stored-state methods. The simplest of these approaches inputs with a delay [25]. This approach has been line the successful in certain the system method of predictive distinctions learns a predictive model of the sensory and to drive action selection. A third approach the internal then uses uses a recurrent neural network learning methods these methods considered. Following (i.e., environmental [28]. Each of are term memory tasks [ 4, 14,25,39]. inputs in combination with existing for their useful application [53]. Another alternative in detail and conditions control policy directly to learn a recurrent state of this model state-dependent this approach, reinforcement is described observables) is called from reinforcement The remainder of the article is organized as follows. Section 2 provides a basic review learning. Section 3 discusses non-Markov decision pro- the they cause for learning control. Section 4 presents to control perception and three stored-state for each. Section 6 discusses all of these are drawn and conclusions of concepts cesses and considers Consistent Representation Method as a technique reviews examples of this technique. Section 5 describes and compares methods and specifies preference methods conditions in the broader context of scalability in Section 7. the difficulties for learning 2. Review of reinforcement learning In this section the basic concepts from reinforcement learning are reviewed. We begin the interaction. Next, we review a simple model of agent-environment by describing principles of Markov decision processes and Q-learning learning algorithm. A thorough learning, however, is beyond focus primarily on Q-learning processes. Other algorithms review of Markov decision processes and Q-learning, [ IO] and [ 541. For a review of reinforcement [ 8,19,44,63] learning suffer a similar reinforcement review of Markov decision processes and reinforcement [ 541, a popular the scope of this article. Throughout and the difficulties caused for it by non-Markov the article, we shall decision fate. For a more complete the reader may wish to consult in general, see [ 71. S.D. Whitehead, L.-J. Lin/Art$cial Intelligence 73 (1995) 271-306 275 Environment States: S Payoffs Actions: A Agent i Fig. 1. A simple model of agent-environment interaction. 2.1. Modeling agent-environment interaction Fig. 1 illustrates a model of agent-environment interaction reinforcement two synchronized At each time point, learning. In this model the agent and the environment finite state automatons interacting in a discrete the following sequence of events occurs. that is widely used in are represented by time cyclical process. ( 1) The agent senses the state of the environment. (2) Based on this current state, the agent chooses an action to perform. (3) Based on the current state and the action selected by the agent, the environment to a new state and generates a payoff. makes a transition (4) The payoff is passed back to the agent. 2.1.1. The environment The environment is modeled is described by the tuple as a Markov decision process. Formally, decision process states, A is the set of possible actions, T is the state transition reward function. At each time, the environment accepts one action from A. S and A are usually assumed transitions new states typically are modeled by the transition (T : S x A -+ S). The transition a Markov (S, A, T, R), where S is the set of possible and R is the function, occupies exactly one state from S, and to be discrete and finite. State pairs into and in terms of a set of transition probabilities, Px,?. (a), where function, T, which maps state-action is in general probabilistic, specified function Px,?(a) = Prob(T(x,a) = y). (1) Payoffs generated by the environment maps state-action may also be probabilistic. pairs into scalar-valued are determined by a reward function, R, which rewards ( R : S x A + II%). The reward function Notice that in a Markov decision process reward generated) state and the immediate models of this type are said to be memoryless is fundamental Markov property knowledge of the current state is always sufficient the reward received over time) action-selection (MDP), (i.e., the next depend only upon the current state. Process the Markov property. The that it implies (i.e., to maximize to devise (e.g., a strategies whose decisions depend upon additional to this model of the environment [ lo]. Thus, even though the effects of actions it may be possible for optimal control and to satisfy information because 216 S.D. Whiteheud, L.-J. Lin/Art@cial Intelligence 73 (1995) 271-306 trace), history that depend only upon knowledge of the current state. these strategies cannot possibly outperform the best decision strategies 2.1.2. The agent The agent is responsible for generating the current state, selects an action, and observes Rewards are used as feedback for learning. One way to specify an agent’s behavior control actions. At each time step it senses that result. the new state and reward for each state, an action to perform. Formally, scribes, states to actions (,f : S + A), where f(x) is in terms of a control policy, which pre- from in state X. denotes the action to be performed is a function a policy f learning, In reinforcement the agent’s objective is to learn a control policy that maxi- mizes some measure of the total reward accumulated over time. In principle, any number is one based on of reward measures can be used, however, a discounted sum of the reward received over time. This sum is called the return and is defined for time t as the most prevalent measure return(t) = ~yr’r,+,l, n=o where y, called the temporal discount factor, is the reward received at time t + n. Because objective is to find a policy that maximizes return, given agent’s objective That is, find f*, such that that the process begins is to find a policy, * V, (x) =mfgyf(x) V’x t S. (2) is a constant between 0 and 1, and r,+” the agent’s the process may be stochastic, the expected return. in state x and follows policy f*, that is uniformly f, to be the expected thereafter. The states. f best for all possible For a fixed policy f, define VJ (x) , the value function for policy (3) An important property of MDPs is that f* is well defined and guaranteed the Optima& Theorem from dynamic programming time, discrete state Markov decision process [ 91 guarantees there always exists a deterministic to exist. In that for a that is optimal. Furthermore, a policy f is optimal if and only if it satisfies the particular, discrete policy following relationship: Q.r(x, f (x) 1 = ~~;(Q~(x.a)) b’x E S (4) the action-value to be the expected in state x, applies action a once, and follows policy is defined function, Intuitively, Eq. (4) states that a policy is optimal if and only where Qf( x, a), that the agent starts [9, lo]. the policy specifies an action that maximizes the local “action-value”. That is, return given thereafter f if in each state, f*(x) =a such that Qf*(x,a) =sga;[Qf.(.x,b)] ‘dx E S, and VP(X) =mgy[Qf*(x,a)] b’x E S. (5) (6) S.D. Whitehead, L.-J. Lin/Artijicial Intelligence 73 (1995) 271-306 277 function (e.g., uniformly zero) Q +-- a set of initial values for the action-value Repeat forever: 1) 2) that is usually consistent with f(x) x + the current state. Select an action a to execute but occasionally an alternate. Execute action a, and let y be the next state and r be the reward received. Update Q(x,a): Q<L~> t (1 -a)Q(x.a) +dr+yU(y)l 3) 4) where U(Y) = Q(Y, f(y) ). Here for each x E S: f(x) +-- a such that Q( X, a) = max&A Q( x, 6). Fig. 2. A simple version of the one-step Q-learning algorithm. For a given MDP, the set of action-values values are said to define the optimal action-value for which Fq. (4) holds is unique. These function Q* for the MDP If an MDP is completely (including then the optimal policy can be computed directly using known probabilities the transition and reward from in many cases, the structure and dynamics the agent cannot compute learn an effective its environment techniques and these circumstances distributions), dynamic programming of the environment the optimal policy directly, but must explore control policy by trial-and-error. are not known. Under [ 9,10,36]. However, 2.2. Q-learning Q-learning [54] is an incremental decision problems. Also, because other reinforcement reinforcement learning method. It is a good repre- it is simple, mathematically well founded, the difficulties learning [44] ), an (namely TD-methods for illustrating is useful techniques is useful for understanding weaknesses for reinforcement learning because sentative and widely used. For our purposes Q-learning caused by non-Markov algorithms understanding of other algorithms. In Q-learning use similar credit assignment of the difficulties for Q-learning by Fq. directly function is to initialize (5). A simple Q-learning the agent estimates and uses algorithm the agent’s action-value the optimal action-value it to derive a control policy using is shown Q-function) mandated step of the algorithm agent’s estimate of the optimal action-value is available, values can be arbitrary main control/learning agent selects an action Q to execute. Most of the time, specified by its policy the agent will occasionally (also called the the greedy strategy in Fig. 2. The first function, Q. Q is the about the task the initial the the this action will be the action as defined by Eq. (5), but for the purposes of exploration choose an action that appears sub-optimal. For example, one (e.g., uniformly loop. The first step is to sense the agent enters state, X. Next, that information may be encoded in the initial values, otherwise zero). After initialization, If prior knowledge the current function. f(x) 278 S.D. Whitehead, L.-J. Lin/Artijicial Intelligence 73 (1995) 271-306 to follow J’ with probability might choose The agent executes Finally, estimate estimate p and choose a random action otherwise. 2 the reward r and state y that result. pair (x, a) is updated. In particular, an the action-value for Q*( X, a) reward r with a utility the immediate for the next state, (I(y) = max&A [ Q(y, b)]. The sum for state-action is obtained by combining the selected action and notes estimate y + YU(Y), called a one-step corrected estimator, Q*? since, by definition is an unbiased estimator for Q*(.x,n) when Q = (7) Q*(x,u) =ElR(x,a) +yV*(T(x,u))l, (8) where V*(X) = max&A Q*(x, a). The one-step estimate for Q ( X, a) using a weighted sum: estimate is combined with the old Q(x.a) + (1 -a)Q(.r,a) +alv+yU(.v)l, (9) rate. Q-learning where @ is the learning for any finite Markov decision process infinitely often and if the learning is guaranteed if, in the limit, every state-action to converge to an optimal policy pair is tried [ 5.51. rate decreases according to a proper schedule 3. Non-Markov tasks To see how non-Markov decision the actual decision problem need only make a distinction between an abstract a task and these as the external and the internal decision problems, is agent-independent the agent, while agent-specific. and exists outside tasks arise in agent-environment (agent-independent) faced by a learning agent. We shall refer interactions, specification one for to since the former and is endogenous respectively, the latter 3.1. An example consider To illustrate the distinction, the task of inspecting in a packaging apples to whether or not they are ripe. Ripe plant. Suppose apples are to be sorted according to the supermarket, green apples are crushed for their juice. If apples apples are shipped then a Markov decision process model of the come down a conveyor one at a time, as follows. We could define a state variable called ripeness task might be formulated state variable would to characterize induce a state space of size 2. The model could have two actions, which would be to either “accept” or “reject” an apple. A reward function could be defined to give a unit of reward whenever a ripe apple was accepted and a green apple was rejected, and a unit of penalty otherwise. A transition the current apple as either “ripe” or “green’‘-this function could be defined to model the dynamics *Occasionally choosing ronment. Exploration examples of more sophisticated is necessary an action at random to guarantee is a particularly that the agent will eventually simple mechanism for exploring the envi- learn an optimal policy. For exploration strategies. see I 2 I, 45,511. S.D. Whitehead. L.-J. Lin/Artifcial Intelligence 73 (1995) 271-306 279 0.1 0.9 Fig. 3. The apple sorting example. task facing a robot with no sensors. b) reject 3 6 =ept (a) An abstract (external) model of the task. (b) The actual (internal) then nonuniform into the hopper transition probabilities as might be the case if apples were loaded of the apple sequence. dependent, different orchards, dependence. For example, we might assume be the same type as the last one with probability 0.9, and different with probability If the types of apples coming down the conveyor are temporally in crates from this that the next apple down the conveyor will 0.1. to a specification of the task. exist in the and that might It is a mathematical mind of the modeler, and are intended the task requirements. perform abstraction. The states, actions, to capture rewards, and transitions the essence of the environment the external decision problem. in Fig. 3(a), corresponds could be used to model to the physical agent It makes no explicit This process model, illustrated reference the task. It defines consider Conversely, the decision problem agent, situated consider its environment, a robot’s head. For simplicity, that it uses to represent to affect action. In a totally basic structure of the internal decision problem: actions: A 1 = 0, A 1 = 1. The transition are determined physics of the sensory-motor detect a lever that can be used to select apples match On the other hand, the sensor detects bruises), external one. the color of the apple on the conveyor task (assuming the external the sensors and affecters determine facing a control system embedded that has a single binary a robot and a single binary actuator Al inside sensor Sl that it uses the two states: Sl = 0, Sl = 1; and two and reward dynamics of this internal process but also by the to is then the internal decision process may closely red apples are “ripe” and green apples are “green”). is able if the robot’s sensor (say, red or green), and if its affector if the agent’s sensor or affector is not closely matched then the internal problem may bear little resemblance (e.g., say to the not only by the dynamics of the external environment, interface. For example, 3.2. Accounting for perceptual-motor processes The distinction between external and internal decision our model of agent-environment for the mappings performed by the agent’s perceptual-motor is shown into account processes. The new model in Fig. 4. On the sensory side the agent’s perceptual processes map states in interaction by augmenting Fig. 1 to explicitly tasks can be incorporated 280 S.D. Whiieheud, L.-J. Lin/Artifcml Intelligence 73 (199.5) 271-306 States: S Actions: A I t Percaphrd Pr- ,“‘_i Internal Representation: s I I Motor PmmsssS Embedded ’ Controller ,‘d ’ Motor Commands: A’ Agent Fig. 4. A model of agent-environment processes. Perceptual processes map states from the external world model onto internal representations. Motor processes map internal motor commands onto actions for the agent’s perceptual-motor in the external world model. that explicitly accounts interaction involve internal to actions could be of considerable and a motor mapping might immediate precepts, attentional mechanisms, representation. On the motor side, in the external task complexity. A perceptual and stored history involve complex motor sequences or parallel to the external world onto states in the agent’s the agent’s motor processes map internal motor commands model. s In general, these mappings mapping might information; actions. For the purposes of the present discussion we shall restrict consideration simple mappings for the remainder only on sensory mappings motor commands the embedded generate a reward in the external model, controller. of the article, we will concentrate is a one-to-one mapping between to that would that reward is correctly passed to the embedded actions. We also assume is, whenever a situation and external controller. That is passed directly in the world and assume that there Indeed, reward arises only. 3.3. Non-Mar&v internal tasks Formally, a decision above and beyond knowledge of the current state can be used to better predict the dynamics of the process and improve control. 4 task is non-Markov if information an agent’s In general, if there are internal states that can represent multiple external states. We call this phenomenon per- ceptual aliasing since it occurs when two or more external states, through the perceptual internal decision problem will be non-Markov ‘Notice that these mappings are conceptual more than physical, in that they run through (abstract) task model. For example, a state in the agent’s internal in the physical universe. which (via the modeling process) get mapped the world and representation may into states then into the external represent certain situations task model. in the external 4 Strictly if information prediction additional speaking, about this definition the history of the process is slightly overstated. Traditionally a process (in addition to its current and control. However, because we are dealing with systems sensing we have chosen information to adopt a definition through is said to be non-Markov to improve to collect state) can be used the potential that is slightly more general. that have SD. Whitehead, L.-J. Lin/Artij’icial Intelligence 73 (1995) 271-306 281 mapping, get superimposed onto a single internal state. Intuitively, perceptual aliasing occurs when the agent is uncertain about the state of the external world. For example, in a situated agent, it occurs when the agent’s sensors fail to code a relevant piece of information. Perceptual aliasing results in non-Markov decision tasks because, by defi- nition, there are states in the internal representation that do not code all the information needed to characterize the future dynamics of the task. Returning to our apple sorting robot, suppose the robot’s color sensor is temporarily disconnected (i.e., always made to read zero). In this case, the robot is unable to distinguish apples by their color and the internal representation collapses to a single state, (see Fig. 3(b) > . Clearly, this decision problem is non-Markov since ( 1) knowledge of the current apple’s color could be used to improve the systems performance-without its sensor, the robot is reduced to guessing; and (2) knowledge of the recent history of the process could be used to improve performance. For example, knowledge that the last action resulted in a positive reward could be exploited to yield a control policy that performs better than chance. This follows since according to the external model, 90 percent of the time the current apple is the same type as the previous one. 3.3.1. The ubiquity of non-Markov tasks Markov decision tasks are an ideal. Non-Markov tasks are the norm. They are as ubiquitous as uncertainty itself. An agent that can be uncertain about the state of the external task necessarily faces an internal decision problem that is non-Markov. And sources of uncertainty abound. Sensors have physical limitations. Rarely are they perfectly matched to the task. Sensor data are typically noisy, unreliable, and full of spurious information. Sensors have limited range, and relevant objects are often occluded. Also, information can be hidden in time. The spoken word is transient and lost unless it is actively processed and stored. Short term memory too is limited and subject to deterioration. Lin [ 281 provides a good example: Consider a packing task which involves 4 steps: open a box, put a gift into it, close it, and seal it. An agent driven only by its current visual precepts cannot accomplish this task, because when facing a closed box the agent does not know if the gift is already in the box and therefore cannot decide whether to seal or open the box. In this case occlusion of the gift by the lid prevents immediate perception of a vital piece of information. Such hidden state tasks also arise when temporal features (such as velocity and acceleration) are important for optimal control, but not included in the system’s primitive sensor set. Even if perfect sensors were available, many control problems are too ambiguous or ill-posed to specify a state space in advance. Indeed, part of the agent’s task may be to discover a useful state space for the problem. For example, integrating learning and active perception invariably leads to non-Markov decision tasks [ 571. Active perception refers to the idea that an intelligent agent should actively control its sensors in order to sense and represent only information that is relevant to its immediate ongoing activity If an agent must, as part of the task, learn to control its sensors, [ 1,3,5,6,12,52]. 282 S.D. Whitehead. L.-J. Lin/Artificial lnrelligence 73 (I 995) 271-306 there will be periods of time when the agent will improperly control its internal control problem will necessarily learning fail to attend state of the external to a relevant piece of information, task. be non-Markov. This follows since during its sensors, the identify and fail to unambiguously 3.4. Eflects on control The level of performance inferior assumes the problem is non-Markov the robot with its color sensor that can be obtained by an agent whose is generally decision problem usually problem is, a non-Markov mistakenly where robot without that the degree of performance 95% of the apples are ripe, then a blind policy only 5% of the time. On the other hand, perceptual decision (and no memory) degradation to sub-optimal is Markov. This can be seen in the apple sorting intact can achieve perfect classification, internal decision to that of its Markov counterpart. That if the agent task, but the can perform no better than chance. Notice if depends on the problem. For example, that “accepted” every apple would fail tasks whose best fixed policies are arbitrarily bad.5 aliasing can lead to non-Markov the sensor control leads 3.5. Dijj’iculties for reinforcement learning learning methods to non- control and in some cases to inability the agent’s for the underlying there are states decision in the agent’s states in the external world The straightforward application of traditional reinforcement in many cases yields sub-optimal stem from that represent These difficulties In particular, because of perceptual representation Markov decision problems severely degraded performance. obtain accurate estimates of the utility and action-values process. internal model. The utility and action-value internal they represent. Because of this averaging, and action-value may result agent may inaccurately optimal action, or the agent may incorrectly degrade in the selection of sub-optimal estimates will be erroneous perceive a sub-optimal it is inevitable aliasing, states estimates tend to reflect a mix (or average) of the values two or more distinct learned by the agent for these non-Markov states for the external that the agent’s internal utility for some situations. These errors, in turn, states, as the actions action to be of higher value than the true for the non-Markov Unfortunately these difficulties are compounded [44] which cause errors to propagate even for non-aliased employ utility (action-value) rewards with utility estimates estimators for subsequent throughout thus states. In particular, most reinforcement the state space, the value of the optimal action. by use of temporal difference meth- infecting learning for recently one-step Q-learning, that combine values states-in estimates are obtained by adding reward with a utility es- for the next state (cf. Eq. (7) ). Thus, for a given state if the agent constructs states, those estimators will likely that use utility estimates from non-Markov the immediate ods action selection algorithms observed action-value timate estimators 5 This is significant because most reinforcement learning algorithms aim to learn fixed policies. Opening the door to probabilistic policies could improve performance under these circumstances. S.D. Whitehead, L.-J. Lin/Artijicial Intelligence 73 (1995) 271-306 283 and the error will be propagated to that state. Once infected a state may be erroneous propagate its error to other states in a similar manner. 4. Consistent representation methods In the last few years several RL algorithms have been developed [60] learns the CS-QL algorithm to control visual attention [ 481 learns efficient, to extract task-relevant and the G-algorithm input vector. 6 In this section, we review the Consistent Represenration Method, a computational [ 131 learns these algorithms system; perception. The Lion algorithm deictic sensory-motor sensing operations; a large present these different algorithms [ 621. to deal with selective in a primitive task-specific bits from in turn. We then that unifies framework 4.1. The Lion algorithm The Lion algorithm was perhaps to address an adaptive perception the first reinforcement [59]. learning algorithm specifically to learn a simple It was used designed feature of this task is manipulation it with only that the agent its to focus partial access visual attention on relevant objects and select appropriate motor commands. The details of the task are as follows. task in a modified Blocks World. The distinguishing is equipped with a controllable To learn to the environment. that provides learn the task, the agent must sensory system task 4.1. I. The block-stacking task The learning task is organized into a sequence of trails. On each trial, the agent is in arbitrary stacks. Blocks are distinguishable to pick up the green block as quickly as possible. (ranging presented with a pile of blocks. A pile consists of a random number of blocks from 1 to 50) arranged only by color; they may be red, green, or blue. Each pile contains a single green block. The agent’s goal is simply the goal before reward, otherwise it receives no reward. The dynamics of the environment are such that a block can be and the agent’s hand is empty. Thus in some cases it is uncovered grasped only when the goal. In this task the effects of block to reach to unstack blocks it is necessary manipulating actions are completely deterministic. it receives a fixed positive If the robot achieves time limit expires, the trial’s is the agent’s sensory-motor What differentiates tasks) this task from other Block World problems ment learning system the system is equipped with a deictic sensory-motor with an ability time that provides a complete and objective description of every object system which provides about is implemented (and other reinforce- system. Instead of assuming a sensory in the scene, the controller the scene at a using to flexibly access a limited amount of information system, selective perception [ 1 ] _ In a deictic sensory-motor h Note that these methods differ from supervised feature selection methods [ 351 that rely on the presentation of preclassified an embedded samples. The algorithms presented here operate without explicit supervision in the context of reinforcement learning task. 284 S.D. Whiiehead, L.-J. Lin/Art$cial Intelligence 73 (1995) 271-306 Sensory Inputs Internal Motor Commands Peripher iaaturas red-in-scene green-in-scene bluein-scene object-in-hand action-framecolor: (00 - red. 01 -green. . ..) action-frameshape:(O - block, 1 -table) action-framestack-height: (00-O.Ol-l....) Local features _ attn-framer: (00 - red. 01 - green, . ..) attn-framestack-hafght: (00 - 0,Ol - 1, . ..) attn-frametabbbaiow Action Frame Commands: moveactbn-frameto-green moveaotiin-frameto4ue moveactbn-frameto-at&-top eaoeon-frameto-stWk+ottom ~~e~Mrameto-table m$: mweattn-frameto-rad moveattn-frameto-green mweattn-trameto-Uue mweattn-frameto-staok-tcq move&n-tiS-!xttom mweattn-frametetat4e Relational properties d attn-frame-in-hand frames-vertically-aligned frames-hotizontally-aligned Fig. 5. A specification for the deictic sensory-motor system used by Meliora-11 (Whitehead [60] ). The system has two markers, an action-frame marker and an attention-frame marker. The system has a 20-bit input vector, 8 overt actions, and 6 perceptual actions. The values registered in the input vector and the effects of internal action commands depend upon the bindings between markers in the sensory-motor system and objects in the environment. information into the internal for both perception to a focus of attention. input vector at each point In practice, and action. On in the environment representation). targets for overt manipulation. A specification a marker corresponds frames of reference into view (i.e., is used to select system used by the agent [ 1,521. Conceptually, markers markers are used to establish the sensory side, placing a marker on an object about that object marker placement the sensory-motor two markers: the sensory side, the system generates a 20-bit of these bits represent local, marker-specific of a marker’s bound object. Other bits detect relational properties horizontal presence or absence of red in the scene. By moving markers agent can multiplex brings On the motor side, for in Fig. 5. This system employs is given the action-frame marker and the attention-frame marker, respectively. On in time. Most such as the color and shape such as vertical and such as the the to object small input register. supported those to the attention-frame marker. Both index objects top-of-stack). by the sensory-motor related groups contain commands by their primitive The action-frame marker has additional commands The “grasp-object-at-action-frame” the object marked by the action-frame marker. Similarly, for controlling marker placement. These actions (e.g., alignment, while others detect spatially non-specific blocks. to grasp (if possible) side of Fig. 5 are the internal motor commands to the action-frame marker and those related (e.g., color) or by spatial relationship that are used for manipulating properties from object a wide range of information system. These commands the “place-object-at-action- Listed on the right-hand onto its relatively into two groups: are partitioned information, the system command features causes SD. Whitehead. L.-J. Lin/Artijcial Intelligence 73 (199.5) 271-306 285 World State 1: World State 2: Internal Rep: \ / 11100000000100000000 Fig. 6. An example of perceptual abasing in the block-stacking domain. In this case, two world states with different utilities and optimal actions generate the same internal representation. The “*” indicates the location of the attention-frame marker; the “+” the location of the action-frame. frame” command causes the system to place a held object at the location marked by the action-frame. The decision problem facing the agent’s embedded controller is non-Markov since improper placement of the system’s markers fails to multiplex relevant information onto the agent’s internal representation. This point is illustrated in Fig. 6 which shows two different external world states (each corresponding to a different state in a Markov model of the task) that, because of an improper placement of markers, generate the same internal representation. 4.1.2. Control To tackle this non-Markov decision problem, the Lion algorithm adopts an approach which attempts to select overt (manipulative) actions based only on the action-values of internal states that are unaliased (Markov). To accomplish this, the Lion algorithm breaks control into two stages. At the beginning of each control cycle a perceptual stage is performed. During the perceptual stage, a sequence of commands for moving the attention-frame marker is executed. These so-called “perceptual actions” cause a sequence of input vectors to appear in the input register. These values are temporarily buffered in a short term memory. Since perceptual actions do not change the state of the external environment, each buffered input corresponds to a representation of the current external state. If the perceptual actions are selected with care one of these internal states will be Markov (i.e., will encode all information relevant to determining the optimal action). Once the perceptual stage is completed, the overt stage begins. During the overt stage an action for changing the state of the external environment is selected. These so- called “overt actions” correspond to commands for the action-frame marker.’ To guide ’ Notice that moving the action-frame marker from one object to another changes the state of the external environment since it changes the set of objects that can be effected by the grasp and place commands. 286 S.D. Whitehead. L.-J. Lin/Art@ial Intelligence 73 (I 995) 271-306 l- World states 7 Internal states Perceptual actions Overt action Consistent internal states large nodes correspond Fig. 7. A graphical depiction of the Lion algorithm. The large (super) graph depicts where large node depict perceptual within each current world state and arcs corresponding to world states and arcs correspond cycles, with nodes corresponding to perceptual actions. to internal the overt control cycle. to overt actions. The subgraphs embedded representations of the the Lion algorithm maintains a special action-value overt-action pairs. This overt action-value for perceptually aliased states are suppressed they are equal to zero), whereas the action-values this action-value is defined over internal-state, in that the action-values selection of an overt action, which is special ideally to take on their nominal values. Given during of each buffered state and choosing Since aliased states tend to have suppressed correspond to the optimal action two-stage control cycle graphically. internal from a unaliased internal function function (i.e., for unaliased states are allowed the Lion algorithm, the action-values action-value. function, the selected action state. Fig. 7 illustrates tends to this the action with the maximum action-values, the overt stage, selects an overt action by simply examining 4.1.3. Learning A special learning function. The learning rule is used to learn the overt action-value procedure operates as follows. First, the internal state with the maximal action-value identified as the Lion. The action-value rule for one-step Q-learning for the Lion state is used to update This further changes is tested zero. is to the standard rule for the remaining buffered states. state, is learned for aliased states cease. Finally, each buffered state to for this state is updated according (i.e., Eq. (9)). Next, the error term in the updating the action-values is done so that once an accurate action-value to see if it is aliased. in the action-values for an unaliased its action-value tests positive, If a state is reset the sign of the error A very simple procedure is used to identify potentially term in the one-step Q-learning examines of the difference between constructed task is deterministic, overestimate other hand, positive error only). Therefore, aliased states can be detected by monitoring aliased states. The rule simply is, the sign estimate the then aliased states tend to regularly states, on the (i.e., the sign in a state’s current action-value If all action-values their action-values tend to monotonically and the action-value are initially (i.e., show a negative error). Unaliased and rewards are nonnegative, after a one-step delay). the optimal action-value set to zero, from below rule (that approach SD. Whitehead, L.-J. Lin/Art$cial Intelligence 73 (1995) 271-306 287 the estimation error. * The learning rule for the perceptual stage is much simpler. For perceptual control function is estimated over internal-state, perceptual-action that maximizes action-value the perceptual a perceptual pairs. During action The perceptual standard one-step Q-learning also accounted actions perceptual than those that do not. (See action-value stage, actions are selected by choosing input bit vector the perceptual for the current is updated within the action-value function (internal stage, using the perceptual state). the is for. Since aliased that lead to unaliased rule except states that the overt utility of the internal tend to have suppressed overt action-values, internal states tend to have higher action-values state [573 for further details.) 4.1.4. Discussion The Lion algorithm is able to learn learns a perceptual block, and learns an overt control policy to unstack covering blocks. Detailed experimental control strategy that focuses that moves the block manipulation task described above. It the attention-frame marker on the green the action-frame marker as needed results can be found in [ 571. to deal with non-Markov exploits The Lion algorithm decision problems. These several assumptions are: ( 1) The effects of actions must be deterministic. (2) Only nonnegative (3) For each external state, there must exist at least one configuration rewards are allowed. limiting assumptions in order of the sensory system that generates an internal state that is unaliased. 4.2. CS-QL for its cost [ 2,351. Tan recognized for the cost of sensing. learning is much work in machine but fails to account to explicitly account algorithms There information, is necessary sensitive CS-QL, which stands for Cost-Sensitive Q-Learning, for cost-sensitive agent not only efficient procedure learning with reinforcement the overt actions needed for classification learning learns for identifying CS-QL and the Lion algorithm that focuses upon tasks: CSID3 the predictive power of it that to be efficient two cost- respectively. ideas the learning In CS-QL, to perform a task, but also learns an In [ 481, he develops and CS-IBL, resulted when he combined [47]. learning is decomposed into a two-stage process of sensing QL control and action different. model specific piece of information perceptual control policy, as in the Lion algorithm, CS-QL constructs a classification the sensing model used in CS-QL sensory-motor the agent has a set of primitive a about the external environment. Also, instead of learning a tree, the same basic control cycle. That is, in CS- control) is considerably system, CS-QL adopts a sensing (overt control). However, Instead of using a deictic test provides tests. Each (perceptual in which sensing the current state of the environment. share 8 Subtle interactions to suppression their action-values. This sometimes tend to bounce back from such suppressions leads and eventually stabilize. For a detailed discussion of this technique for detecting aliased states, see [ 571 and 1481. cause unaliased states. However states these states sometimes of unaliased to overestimate 288 S.D. Whitehead, L.-J. Lin/Artijicial Intelligence 73 (1995) 271-306 where each internal node corresponds to a test result, and each leaf corresponds (for example, tree when every leaf in the tree is unaliased; state in a Markov model of the task. see Fig. 8). In CS-QL, to a sensing operation, to a state in the agent’s internal each branch corresponds representation the agent has learned an adequate classification that is, when each leaf represents a unique incrementally. tree is learned The classification the tree consists of a single root node. In other words, the entire external state space is collapsed onto a single internal to internal nodes) state. As aliased introduce new by attaching distinctions is achieved. The tree is expanded until a Markov representation to them. The new leaf nodes that result into the representations. leaves are detected, they are expanded sensing operations (converted Initially, to attach inexpensive that remain, When expanding those among tests tends to explore the most efficient accounts for both cost and the discriminatory algorithm below), more efficient classification CS-QL uses the same technique the same limitations a node, CS-QL simply selects the least expensive sensing operation, low-cost to the target leaf. This heuristic first, but may not always generate that test (see the G- trees should result. To detect aliased leafs, employed by the Lion algorithm. Thus, CS-QL shares power of each sensing trees. By incorporating as the Lion algorithm. a more sophisticated sensing procedures selection method favoring learn to deduce in a simulated from CS-QL has been successfully demonstrated its position the robot must sensors. One very simple which from its selective along with a classification the robot’s sensing operations nearby cells in the maze. The cost of sensing a cell is proportional the robot. By accumulating features efficiently) allow it to detect properties tree learned by CS-QL, its position within robot navigation information task in gathered task, is shown in Fig. 8. In this example, (e.g., empty, barrier, cup) of from (and to its distance from nearby cells the robot can successfully instance of this type of navigation the maze. identify limited 4.3. The G-algorithm Instead, [ 131 tried to minimize The G-algorithm to apply Q-learning the G-algorithm was developed in the context of a more general video-game domain technique developed the Lion algorithm and CS-QL, to address a kind of adaptive per- its development was not the cost of sensing or by the need to to mitigate In particular, when Chap- is a third ception task. However, unlike specifically motivated by the desire system. control an active sensory problems caused by the availability of too much information. man and Kaelbling subtask found formation generated by the sensors. The subtask the agent with a target, orienting agent’s sensory system generated 100 bits of input. Using all this information in an internal relevant introducing the bits that are specifically in time, the resulted 2’O” states. Most of these bits, however, were ir- interfere with learning by representation. On the other hand, known ahead of time and at they (called Amazon), by the sheer volume of in- involved aligning to it, and firing a weapon. At each point to learn a simple align-and-shoot system was being overwhelmed they were studying and just relevant are not necessarily to the specific subtask state space containing they were studying in the internal unnecessary distinctions that their learning S.D. Whitehead, L.-J. Lin/Arti$cial Intelligence 73 (1995) 271-306 289 b) CUP 4T nothing up-state t-1, 0) 3 Cl q obstacle \ 4 1 7 2 6 Fig. 8. A simple example of CS-QL: (a) a 3 x 3 grid world, (b) a learned mapping between state descrip- tions and states, (c) a learned optimal decision policy, and (d) a learned cost-sensitive classification tree. (Reproduced with permission.) other stages of the game, the bits that were irrelevant “now” were vitally important “then”. The G-algorithm was developed to learn a control policy which could generalize over irrelevant information in the input. The G-algorithm works by identifying bits in the input vector that are important for control. It is very similar to CS-QL in that both grow classification trees incrementally. That is, both start with a single root node (i.e., assuming no information is relevant), then construct a tree-structured classification circuit by recursively splitting nodes based on the values of sensory inputs. In CS-QL, the information used to split nodes in the tree corresponds to the results of sensing acts (or tests), in the G-algorithm nodes are split based on the values of bits in the input. As in CS-QL, the leaves of the G-algorithm’s tree define the agent’s internal state space. Unlike CS-QL, the G-algorithm does not associate a cost with sensing/reading a bit. What sets the G-algorithm apart from both CS-QL and the Lion algorithm, is the internal states. CS-QL and the Lion method it uses to detect non-Markov (aliased) algorithm both monitor the sign in estimation error to detect non-Markov states; a method that is limited to deterministic tasks only. The G-algorithm uses a much more general statistical test. In general, a leaf in the classification tree is non-Markov if it can be shown that there are bits in the input vector (that have not already been tested in traversing the tree from root-to-leaf) that are statistically relevant to predicting future rewards. To detect if a leaf is non-Markov, the G-algorithm uses the Student’s T-test 1421 to find statistically significant bits. That is, over time as the agent experiences a 290 S.D. Whiteheud, L.-J. Lin/Artijiciul Intelligence 73 (1995) 271-306 this estimate leaf is separated is above a threshold, it is that distinct distributions relevant and the leaf is split. into one of two bins. One bin corresponds insight provided by (and consequently states, reward data is collected and stored for each leaf, target-bit to these two sets of variety of external pair. Data for a given situations when the target bit is on; the other when the bit is off. Given data, a Student’s T-test is used to determine how probable gave rise to them. If after sufficient sampling, is deemed The relevance in that the T-test assumes is clearly not the case in general, since reward distributions this problem can be mitigated by comparing distributions the G-algorithm A bit’s relevance must be apparent is required limitations domains. to test bit is limited that the reward distributions being compared are Gaussian. This can be arbitrary. However, rewards. Also, in higher order pairings. in isolation. Finally, additional memory and sensing and to stochastic to gather statistics seem the G-algorithm detect non-Markov states). The specific algorithm is to use statistical methods to detect bits that are relevant to pay for a method to be a minor price testing. Nevertheless is not guaranteed these difficulties of cumulative for relevance that extends the bit The G-algorithm was successfully demonstrated on the orient-and-shoot task. It was found to significantly in a neural network. See [ 131 for details and a discussion of some difficulties encounter. outperform an alternative approach that used error backpropagation they did 4.4. The Consistent Representation Method While the algorithms described above vary considerably same basic approach. We refer to this common tation in their detail, they share the framework as the Consistent Represen- two stages: a perceptual stage (1) (2) (3) (4) (5) into are: representation is partitioned time step, control (or overt) stage. stage aims to generate an internal (CR) Method. 9 The key features of the CR-method At each followed by an action The perceptual The action stage aims to generate optimal overt actions. Learning occurs in both control stages. For the action stage, traditional ment learning on the internal perceptual resentation modified It is assumed sensory stage in that the perceptual for non-Markov it. that the external state can always be identified state space. This constraint, states. When one is found, are used. These techniques stage constantly monitors to eliminate techniques inputs. from reinforce- impose a Markov constraint in turn, drives adaptation the internal the perceptual process in the rep- is that is Markov. immediate to be Markov. for the ‘) The term Consistent Representation is derived to internal state space in Fig. 8, two different external predicting states are classified in a sub- optimal policy. This slightly weaker concept of being “partially Markov” or “Markov with respect lo reward” has been associated with the term “consistent”. See [ 57 1 for a futther discussion of this distinction. it is not absolutely necessary for each state to be Markov with respect into the same state, state 2. This “mis-classification”, future states). For instance, however, does not result (but not necessarily it is sufficient the fact that In particular, rewards future from S.D. Whitehead, L.-J. Lin/Artijicial Intelligence 73 (1995) 271-306 291 Motor System f The Agent Fig. 9. The basic architecture of a system using the Consistent Representation Method. Control is accomplished in two stages: a perceptual stage, followed by an overt stage. The goal of the perceptual stage is to generate a Markov, task-dependent internal state space. The goal of overt controJ is to maximize future discounted reward. Both control stages are adaptive. Standard reinforcement Learning algorithms can be used for overt learning, while perceptual learning is driven by feedback generated by a representation analysis module, which monitors the internal state space for non-Markov states. Fig. 9 illustrates an architectural ponents include: an overt control, trol to the selective The line perceptual ceived by both tion monitor detects non-Markov trol . sensory the overt control and overt controllers from the overt controller a selective a motor and a representation monitor. The sensory embodiment system, line system, a perceptual of the CR-method. The major com- control, con- acts. the is re- the perceptuai (or selection) represents overt acts. Both from control the environment from system represents perceptual to the motor system are adaptive. Reward and the representation monitor. The representa- con- to the perceptual feedback states and provides The correspondence between the components of this architecture and each of the pre- is as follows. The Lion algorithm assumes a deictic sensory-motor includes assumes assumes commands a sensory-motor vious algorithms system which CS-QL ing acts; and the G-algorithm bits are selected as relevant. The identification tual control takes form of a binary classification internal representation input bit vectors; while of a classification learning algorithm the form of a “perceptual tree. The Lion, CS-QL, use an overestimation in CS-QL and generated by for moving perceptual interface that consists of a set of discrete (or attentional) markers; sens- individual in the percep- the from which and procedure implemented policy” in the Lion algorithm, a binary input vector tree in CS-QL and the G-algorithm. The task-specific the Lion algorithms the G-algorithm and G-algorithm to a subset of corresponds it is defined by the leaves alI use a form of Q- both the Lion and CS-QL relies on a more technique, while the G-algorithm for overt control. For representation monitoring, 292 S.D. Whitehead, L.-J. Lin/Arti&ial Intelligence 73 (1995) 271-306 general statistical method. ‘” Relating the Lion, CS-QL and G-algorithm in the common the statistical methods used by the G-algorithm limitations, to yield algorithms into Lion and CS-QL is useful for two reasons. First, it promotes cross-fertilization the structure provided by the CR-method highlights them. extensions method specific algorithms. For instance, be incorporated domains. Second, tions and fundamental identified at each point these techniques track of information These more general hidden state tasks and several stored-state the subject of the next section. it suggests assumption made by all these algorithms in time from immediate for many interesting that for one reason or another has become perceptually approaches that function to overcome inappropriate sensor and a is that all external states can be inputs. This assumption makes to keep tasks that require memory framework of the CR- of ideas between can in stochastic shared assump- In particular, inaccessible. to them are 5. Stored-state methods One obvious approach to dealing with inadequate perception and non-Markov decision is to allow the agent to have a memory of its past. This memory can help the problems agent identify hidden states, since it can use differences that based on immediate perception situations huge volume of information to remember, how to encode problem sliding window of its history, predictive model of environmental two approaches, control policy directly in the other approach observables that have been discussed from reinforcement. in the literature. in memory appear identical. The problem traces to distinguish is: given a available about the past, how should the agent decide what to this the agent keeps a it, and how to use it. There are two approaches In one approach the agent builds a state-dependent this section describes a third approach, which learns a history-sensitive [ 4,14,28,39,.50]. In addition to these 5.1. Three stored-state architectures Fig. 10 depicts Markov domains. trained using temporal difference methods (Q-function). three stored-state In our experiments with all three, a neural network for reinforcement architectures learning to incrementally learn an action-value in non- (Q-net) was function In the window-Q architecture, instead of relying only upon immediate to define the sensations (or sensations) tions, to represent access to the information size. The window-Q its current its internal representation, for the N most recent state. In other words, inputs sensory sensa- the agent uses its immediate time steps, and the N most recent actions allows direct the window to use this the window-Q architecture in the past through a sliding window. N is called is simple and straightforward. However, architecture I0 A version of the Lion algorithm has also been developed where feedback used to detect non-Markov [ 57 I. learning states. This external supervision dramatically is from an external supervisor improves both perceptual and overt S.D. Whitehead, L.-I Lin/Arrificial Intelligence 73 (1995) 271-306 293 action-value a&Xl-Value current sensation & recent N sensations & recent N actions sensation action contextual features Fig. 10. Three stored-state (b) recurrent-Q, and (c) recurrent-model. architectures for reinforcement leaming in non-Markov domains: (a) window-Q, architecture one must choose a window size, which may be difficult to do in advance. On the one hand, if the selected window size is too small, the internal representation may not be sufficient to define a state space that is Markov. On the other hand, an input problem may arise if the window size is chosen to be too large, or if generalization the window must necessarily be large to capture relevant information that is sparsely distributed in time. Under these circumstances excessive amounts of training may be required before the neural network can accurately learn the action-value function and generalize over the irrelevant inputs. In spite of these problems, the window-Q architec- ture is worthy of study, since ( 1) this kind of time-delay neural network has been found to be useful in speech recognition tasks [53], and (2) the architecture can be used to establish a baseline for comparing other methods. The window-Q architecture is sort of a brute-force approach to using memory. An alternative is to distill a (small) set of contextual feutures out of the large volume of information about the past. This historical context together with the agent’s current sensory inputs can then be used to define its internal representation. If the contextual features are constructed correctly then the resultant internal state space will be Markov and standard RL methods may be used to learn an optimal control policy. The recurrent- Q and recurrent-model architectures illustrated in Fig. 10 are based on this basic idea. However, they differ in the way they construct their contextual features. Unlike the window-Q architecture, both of these architectures can in principle discover and utilize historical information that depends on sensations arbitrarily deep in the past, although in practice this has been difficult to achieve. Recurrent neural networks, such as Elman networks [ 171, provide one approach to constructing relevant contextual features. As illustrated in Fig. 11, the input units of an Elman network are divided into two groups: the (immediate) sensory input units and the context units. The context units are used to encode a compressed representation of relevant information from the past. Since these units function as a kind of memory and 294 S.D. Whiteheud, L-J. Lin/Art@cia/ Intelligence 73 (1995) 271-306 Fig. I I. An Elman network encode an aggregate of previous network states, the output of the network depends upon past as well as current The recurrent-Q uses a recurrent network inputs. architecture to estimate (Fig. 10(b) ) To predict action-values recurrent Q-net) must learn contextual correctly, features which enable between different external states that generate the same immediate the action-value the recurrent network to sensory the network function directly (called distinguish inputs. architecture for learning (Fig. 10(c)) is responsible an action. Because The recurrent-model consists of two concurrent to predict the immediate sensory from performing code the state of the external environment, features learning a one-step prediction module or action model, and a Q-learning module. inputs immediate the model must the effects of an action on the that an accurate predictive model can be learned, and then a Markov state space can be to be features. This can be completely components: The prediction module (and payoffs) that result inputs do not completely learn and use contextual the environment. that the model’s contextual generated for the Q-learning the conjunction follows since, at any given determined by this new state representation and recurrent-model least-mean-square method features can be extracted, component by defining its inputs (internal input and the contextual and the action taken. architectures the next state of the environment (e.g., error back-propagation), of the agent’s immediate to accurately predict Both the recurrent-Q learn contextual If we assume state space) the agent’s sensory time, in an important way. In learning the predictive model, the goal a gradient descent, differ errors between actual and predicted the environment time as long as the environment is to minimize Eq. In (9)). from the environment value function. Since information weak, noisy, and even inconsistent the recurrent-Q errors between architecture this case, during is uncertain. provides all the needed training information, which sensory inputs and immediate does not change. For recurrent Q-learning, temporally successive predictions of action-values the error signals are computed based partly on information and partly on the agent’s current estimate of the optimal action- little or no useful this latter term changes over time and carries these error signals may be in general over time. Because of this the practical viability of the early stages of learning, features using but they is to minimize rewards. In this case, is consistent over the goal (see Having these architectures, are also possible. For example, we can combine introduced two of these approaches to the recurrent Q-net could include not just the current sensory architectures: input but also recent two architectures, For instance, one approach would be to share the context units between inputs and recent actions. We can also combine that combinations the first it is worthwhile the inputs the last to note S.D. Whitehead, L.-J. Lin/Art$cial Intelligence 73 (1995) 271-306 295 2 possible initial states: a W c”‘“^“’ 3 a$Xions: walk left, walk right &pick up 4 blnaty inputs: left cup. right cup, left collision & right collision reward: 1 when the last cup is picked up 0 otherwise Fig. 12. Task 1: a two-cup collection task. The cup locations are denoted by filled circles. the model network and the Q-network based on prediction this article is needed versions. is only concerned with errors from both networks. Although such that the contextual features learned would be the three basic architectures. Further there are many possibilities, investigation than the basic to see if other combinations will result in better performance 5.2. Network training The (nonrecurrent) Q-nets of the window-Q and recurrent-model of temporal difference methods architectures can be [ 441 and the [ 37 1. This combination has been successfully trained using a straightforward connectionist back-propagation to solve several nontrivial applied combination algorithm Training the model of the recurrent-model architecture reinforcement learning problems [ 24,25,49]. is slightly more complicated. Recurrent networks can be trained by a recurrent version of the back-propagation algo- rithm called back-propagation through time (BF’TT) or unfolding of time [ 371. BFTI’ is that any recurrent network spanning T steps can be converted based on the observation the network T times. Once a re- into an equivalent current network can be directly applied. The Q-net of the recurrent-Q For detailed network structures and implementation, can also be trained by BF’TT together with temporal difference. feed-forward network by duplicating is unfolded, back-propagation architecture see [ 281. 5.3. Simulation results This subsection presents simulation results of a study in which the three history-based architectures were applied we have gained insight of the relative merits of each and the conditions descriptions to a series of non-Markov decision into the behavior of these architectures, of the simulation and results can be found in [ 281.) this study, tasks. Through and a better understanding (Detailed for their useful application. 5.3. I. Task 1: two-cup collection We begin with a simple two-cup collection to pick up two cups located learning agent walking pick-up action, current cell. The agent’s sensory right one cell, walking it will pick up a cup if and only input includes task requires task (Fig. 12). This the in a 1-D space. The agent has three actions: the if the cup is located at the agent’s if four binary bits: two bits indicating the agent executes left one cell, and pick-up. When 296 S.D. Whitehead, L.-J. Lin/Artificial Intelligence 73 (1995) 271-306 there is a cup in the immediate action results agent out of the space will cause a collision. is restricted in a collision problem left or right cell, and two bits indicating from the left or the right. An action attempting if the previous to move the two possible The cup collection so that there are only initial states (Fig. 12). In each trial, the agent starts with one of the two initial states. Because of each trial, the location the agent can see only one of the two cups at the beginning of the other cup can only be learned from previous the cups optimally, the agent must use contextual decide which way to go after picking up the first cup. Note that the reason for restricting the possible contextual This initial states is to avoid perceptual aliasing at the onset of a trial when no such as which initial state it starts with, to information task is nontrivial (1) The agent cannot trials. To collect is available. information, for several reasons: sense its current cell, (2) operates with no cup in sight especially after picking up the first cup. architectures were tested on this cup collection it gets no reward until both cups are picked up, and (3) The three memory iment was repeated 5 times, and every optimal control policy within 500 trials. observation, however, was the following: The recurrent-model a perfect model within 500 trials. For instance, steps or more, the model normally this imperfect model did not prevent Q-learning is not able to predict The two main results of this experiment were: time each architecture (The window size N was 5.) One interesting architecture never learned successfully task. The exper- learned an if the agent has not seen a cup for 10 the appearance of the cup. But from learning an optimal policy. the cup in it often l To demonstrate that each of the architectures simple hidden state task, and in particular, architectures are able to develop and use memory-based is capable of learning to demonstrate this that the two recurrent to perform l To notice that for the recurrent-model architecture, partially correct This is good news, since a perfect model it may provide sufficient contextual is often difficult to obtain. features. contextual is only even when the model features for optimal control. 5.3.2. Task 2: Task I with random features Task 2 is simply Task 1 with two random bits in the agent’s sensory two difficult-to-predict and irrelevant there are often many features accessible features which are difficult input. The to the to agent. In the real world, random bits simulate learning predict but fortunately whether is to pick up cups inside. The ability is important for a learning not relevant it is going system to rain outside might be difficult, but it does not matter to the task to be solved. For example, predicting if the task features to handle difficult-to-predict but irrelevant to be practical. The simulation results are summarized of the window-Q architecture or the recurrent-Q as follows: The two random features had little architecture, impact on the perfotmance but had a noticeable negative The system using formance during on the optimal policy; policies. impact on the recurrent-model architecture. the recurrent-model architecture the course of 300 trials. However, streaks of optimal per- could not stabilize the optimal policy and several sub-optimal exhibited it apparently it oscillated between It was also observed that the model tried in vain to reduce the prediction errors S.D. Whitehead, L.-J. Lin/Artijcial Intelligence 73 (1995) 271-306 297 fail to learn the contextual for the poorer performance on the two random bits. There are two possible explanations compared with that obtained when there are no random sensation bits. First, the model the task, because much of features needed might the effort was wasted on trying the to learn to predict the model network and the Q-net, activations of the context units were shared between features on the model part could simply a change destabilize is times. To test the second ruled out, since to explanation, we fixed the model at some point of learning and allowed only changes the Q-net. to solve the random bits. Second, because if the change was significant. The first explanation indeed was found many a well-trained Q-net, to the representation the optimal policy the agent found the optimal policy and indeed stuck to it. two lessons: In such a setup, revealed of contextual architecture is more economic that the former will not try to learn a contextual than the recurrent-model feature architecture if it does not This experiment l The recurrent-Q in the sense appear to be relevant to predicting action-values. l A potential problem with the recurrent-model of contextual to the features on the model part may cause instability on the is that changes architecture representation Q-net part. 5.3.3. Task 3: Task 1 with control errors prevail Noise and uncertainty in the real world. To study to handle noise, we added 15% control errors to the agent’s actuators, architectures that 15% of the time the executed action would not have any effect on the environment. (The two random bits were removed.) the capability of these so In three out of the five runs, the window-Q architecture successfully policy, while recurrent-Q architecture in the other two runs, it only found sub-optimal the optimal policy (with little instability). The recurrent-model learned always architecture found the optimal the In contrast, policies. always found the optimal one and some sub-optimal the optimal policy after 500 trials, ones due in Task 2. If features, much as happened of contextual rate to 0 at the end), we should be able the model (for example, by gradually decreasing to obtain a stable and optimal its policy oscillated between representation but again to the changing we can find some way to stablize the learning policy. Two lessons have been learned l All of the three architectures l Among errors. the architectures, from this experiment: can handle small control errors to some degree. recurrent-Q seems to scale best in the presence of control 5.3.4. Task 4: pole balancing In the pole balancing problem, cart in order the system’s objective of a movable (Fig. 13). This problem has been studied widely in the reinforcement interest because of its resemblance It is of practical to problems missile guidance) (e.g., biped balance and locomotion). and robotics interest because of the difficult credit assignment to the base to the cart via a hinge literature. (e.g., It is of theoretical problem which arises due to sparse learning in aerospace to balance a pole is to apply forces that is attached Fig. 13. The pole balancing problem inputs completely signals. In particular, task, the system’s sensory In the traditional pole balancing in most formulations of the problem, characterizes In our experiments, the system only reinforcement receives nonzero in our simulations reinforcement when the pole falls over. For instance the system receives a penalty of -1 when the pole tilt exceeds 12 degrees from vertical. the position and velocity of the cart and the angular position and velocity of the pole (431. This the state of the system and yields control problem information that is Markov. and pole angle are given. only decision problem, and in order to learn an adequate control This yields a non-Markov for the cart policy and pole. In this experiment, the pole could be balanced test trials where the pole f2, or *3 degrees. In the training phase, pole angles and starts with an angle of 0, fl, randomly. The initial cart velocity and pole velocity were cart positions were generated always set to 0. N = 1 was used here. for over 5000 steps in each of the seven the system must construct a policy was considered satisfactory whenever features resembling the cart position contextual velocities include The input representation used here was straightforward: each of the pole angles and cart positions. The following trials taken by each architecture before a satisfactory policy was learned. These numbers are the average of the results (A satisfactory policy was not always found within 1000 trials.). from the best five out of six runs. one real-valued table shows input unit for the number of method window-0 recurrent-Q recurrent-model # of trials 20h 552 247 While collection task. the recurrent-Q tasks, it was outperformed by the other two architectures architecture was the most suitable architecture for the cup for the pole balancing 5.4. Comparisons The above experiments provide some insight into the performance of the three memory in architectures. This subsection determining when one architecture may be preferred over another. considers problem characteristics that may be useful SD. Whitehead, L.-J. Lin/ArtiJcial Intelligence 73 (1995) 271-306 299 5.4.1. Problem parameters Some of the features (or parameters) architectures are: of a task that affect the applicability of these l Payo#delay. l Memory depth. One important problem parameter is the length of time over which represen- task is is important because In cases where the payoff in order to generate an internal inputs the memory depth for the pole balancing the agent must remember previous tation that is Markov. For example, 1, as evidenced by the fact that the window-Q agent was able to obtain satisfactory an optimal policy control based only on a window of size 1. Note the policy. For in- may require a larger memory depth than that needed stance, for Task 1 (cup collection) (and the optimal policy based on a window of size as small as 2, occasionally learn) learn the optimal control when using a window of size 5. but it could only reliably is zero except for the goal state, we define leading the overall difficulty of learning an accurate Q-function becomes the payoff delay of a problem to the goal. This parameter Q-learning. As the payoff delay increases, increasingly to be the length of the optimal action sequence the window-Q agent was able to represent that learning to represent difficult due to the increasing difficulty of credit assignment. to be learned. In general, than predicting action-values features the more perceptual aliasing the agent has to discover, and the more an agent faces, the more contextual requires difficult the task becomes. (i.e., a Q-net), which in turn features more contextual than representing optimal policies. Consider Task requires more contextual 1 for example. Only the optimal actions: “is there a cup in front?’ and “is the second cup on the right-hand requires more features such as side or left-hand side?‘. But a perfect Q-function “how many cups have been picked up so far?” and “how far is the second cup from here?“. A perfect model the same features as the perfect Q- function. But a perfect model for Task 2 requires even more features such as “what is the current state of the random number generator?“, while a perfect Q-function for Task 2 requires no extra features. two binary contextual for this task requires In general, predicting features are required (i.e., a model) to determine it influences sensations features l Number of contextualfeatures to note It is important in order to actions so that their relative values are in the correct order. Similarly, that we do not need a perfect Q-function needs nor a perfect to only assign a to obtain an optimal policy. A Q-function model values model needs to only provide sufficient features for constructing a good Q-function. 5.4.2. Architecture characteristics the above problem parameters, we would like to understand which of the three the key types of problems. Here we consider to particular of each architecture, along with the problem parameters Given architectures advantages which influence l Recurrent-model is best suited and disadvantages architecture. The key difference between and the is driven by learn- recurrent-Q ing an action model is that the agent can obtain better training data for the action model than it can for the than the Q-function. One strength of this approach is that its learning of contextual this architecture architecture features rather the importance of these characteristics. 300 S.D. Whiteheud. L.-J. Lin/Artijicial Intelligence 73 (1995) 271-306 this learning more reliable and effective. Q-function, making examples of the action model ples) are directly observable with each step the agent In contrast, training examples of the Q-function triples) are not directly observable values based on its own changing approximation (<sensation, (<sensation, since the agent must estimate action, next-sensation, takes In particular, training payoff> quadru- in its environment. action, action-value> to the true action-value the training action- function. features are dependent (even though the action these reward functions, or goals, to predict rewards as well as sensations). As a result, if the agent has several different The second strength of this approach is that the learned and independent of the reward function on the environment model may be trained features can be reused to learn to achieve. suffers training examples, the relative disadvantage it has the offsetting that are relevant to the features to represent the optimal action the optimal Q-function. This is l Recurrent-Q architecture. While this architecture learn those contextual features needed to represent that it need only that it must learn from indirectly observable advantage control problem. The contextual model are a superset of those needed easily seen by noticing from the action model a few features to predict completely be learned by the recurrent-Q needed by the recurrent-model architecture architecture. are necessary look ahead search). Thus, that the optimal control action can in principle be computed in cases where only (by using but many are needed that must than the number the next state, the number of contextual can be much smaller for predicting action-values features recursively architectures). l Window-Q architecture. The primary advantage of this architecture networks networks. This advantage not have to learn the state representation network than nonrecurrent history fixed window which captures only a bounded history. In contrast, network approaches sensations represent contextual that are arbitrarily deep in the agent’s history. longer typically is offset by the disadvantage to those features directly observable is that it does (as do the other two recurrent to train that the in its the two recurrent that depend on it can use is limited can in principle take much information Recurrent features Given these competing advantages each will be the preferred architecture l One would expect the advantage of the window-Q tasks where the memory depths are the smallest task). l One would expect the recurrent-model for different for the three architectures, one would imagine types of problems: architecture (for example, to be greatest the pole balancing that in to be most important (for example, the pole balancing architecture’s advantage of directly available in tasks for which the payoff delay is the that the It is in these situations task). of training Q-values is most problematic for the recurrent-Q l One would expect the advantage of the recurrent-Q relevant to control-to be most pronounced relevant and irrelevant contextual task with two random features features). Although architecture-that it need only in tasks where the is the lowest (for example, the recurrent-model can acquire the optimal policy as long as just the relevant features are training examples longest indirect estimation architecture. learn those features ratio between the cup collection architecture S.D. Whitehead, L.-J. Lit/Artificial Intelligence 73 (1995) 271-306 301 learned, the drive to learning the irrelevant features may cause problems. First of all, representing the irrelevant features may use up many of the limited context units at the sacrifice of learning good relevant features. Secondly, as we have seen in the experiments, the recurrent-model architecture is also subject to instability due to change which improves the changing representation of the contextual features-a model is also likely to deteriorate the Q-function, which then needs to be re-learned. The tapped-delay line scheme, which the window-Q architecture uses, has been widely applied to speech recognition [53] and turned out to be quite a useful technique. However, we do not expect it to work as well for control tasks as it does for speech recognition, because of an important difference between these tasks. A major task of speech recognition is to find the temporal patterns that already exist in a given sequence of speech phonemes. Whereas in reinforcement learning, the agent must look for the temporal patterns generated by its own actions. If the actions are generated randomly as it is often the case during early learning, it is unlikely to find sensible temporal patterns within the action sequence so as to improve its action selection policy. On the other hand, we may improve the performance of the window-Q architecture . The TDNN used by using more sophisticated time-delay neural networks (TDNN) here is quite primitive; it only has a fixed number of delays in the input layer. We can have delays in the hidden layer as well [ 20,531. Bodenhausen and Waibel [ 111 describe a TDNN with adaptive time delays. Using their TDNN, window-Q may be able to determine a proper window size automatically. 6. Discussion This paper has described learning techniques that were developed to handle tasks that involve either selective perception or state hidden in time. However, many tasks of interest in fact involve both selective perception and memory, and solutions to these tasks may require integration of both Consistent Representation Methods and stored-state methods. One simple approach to extending the CR-method would be to extend the agent’s selective sensory system to include remembered sensory-motor events. That is, instead of selecting bits of information from the current sensory input only, the system could also select bits from a memory trace of previous inputs and actions. This approach is similar to the window-Q architecture in that a memory trace is maintained, however it differs in that only a relatively small amount of information would be selected at each point in time. Moreover, under this scheme it might be possible to devise reference-based rules in a way that would preserve relevant memories while for updating the history-trace dropping irrelevant ones. Other architectures that combine features from both the CR-method and history-based architectures may also be very useful. For example, one problem with the CR-method as it currently stands is that it uses no information about the previous state of the environment when trying to identify the current state. In a sense the system re-identifies the state of the environment starting from “scratch” after each action. Knowledge of the last state and the most recent action could considerably reduce the effort required to 302 S.D. Whiteheutl. I..-J. Lirz/Arfjficd Inrellipnc~e 73 (1995) 271-306 the current state, since in most environments identify be local and predictable. Thus instead of “rediscovering” agent could merely verify from a limited number of possibilities. transitions between states tend to the the state after each action, the outcome the current state, or in the worst case, identify In addition to further exploring variations on the above architectures, must also assess the scalability to extend a desire problems that involve selective perception and/or we have been successful. However, compared to the scale of problems A few of the issues future work to these algorithms. These algorithms were derived from to in time. To some extent simple remain painfully intelligent behavior. learning beyond Markov decision problems state hidden the tasks we have explored required that must be addressed for truly autonomous, to achieve scalability reinforcement include: and l Learning bias: Reinforcement can proceed much more quickly learning can be viewed as a kind of search through If that search can be biased in an appropriate learning it might otherwise. One to introducing bias into a learning agent is to allow it to interact with other tasks. Other agents can serve as role models, and in general can strongly bias in the space of possible control policies. direction, approach intelligent advice givers, an agent’s the context of reinforcement time in learning learning. Simple versions of these methods have been demonstrated [ 15,25,61]. Nevertheless, more work is needed. agents performing instructors, learning and have produced significant critics, and supervisors, improvements similar than given reward l Fast/efJicient credit assignment: Credit assignment is the fundamental in the environment, which actions were to learning algorithms solve this problem by the system be changed from that payoff and how should learning: for generating reinforcement responsible improve performance. Most reinforcement making knowledge model) developed. For example, see [ 25,3 I, 33,34,45,64]. incremental about to the system over a long period of time. If additional (for example, an action can be made available, more efficient credit assignment methods can be the causal structure of the environment problem changes l Generalization: A reinforcement is impractical, In particular, when ences. optimal control on experience with similar tions (i.e., Q-function) with look-up tors that promote useful generalization [ 13,27,29,32,47-491. situations. the state space learning from that exhaustive agent must generalize is so large its experi- for search the agent must guess about new situations based func- tables, we must develop function approxima- Instead of representing action-value across states and actions. For example, see l Hierarchical in reinforcement learning: To date much of the work that are small compared learning has to those facing real robotic systems. from robot may require precise focused on problems For example, a walking dozens of sensors, and may need to control dozens of effecters. The combinatorics associated with such problems quickly overwhelm other source of complexity learning complex fectively. Then control policies can be integrated tractable ones: First, a into tasks that can be solved ef- tasks the elementary to the simplest RL methods. An- arises when agents pursue multiple goals. Hierarchical problems into multiple elementary that are learned is approach task is decomposed task. Once the agent has learned to solve the original complex (continuous) for solving information to turning intractable S.D. Whitehead, L.-J. Lin/Art@cial Intelligence 73 (1995) 271-306 303 solve one complex same elementary task, solving a new one task may be easier if the two share the subtasks. For work in this direction, that stand between current see [ 22,26,27,40,62]. technology there are many other issues of intelligent autonomous Of course development However, to play an important role. the autonomy afforded by reinforcement learning methods makes agents, and reinforcement learning and the is no panacea. them likely 7. Conclusions control systems must deal with information inadequate Intelligent sensors. When agent must actively control decision problem be very difficult. information its sensors it faces is necessarily limitations is available in order to select relevant non-Markov. Learning imposed by their from the agent’s sensors or when the the internal tasks can features, these control In this article we have presented several approaches to dealing with non-Markov into control the current state of the environment; to dealing with tasks that involve control/selection is partitioned decision problems. The Consistent Representation approach In the CR-method, which aims to identify reward. Three which aims [ 601, the G-algorithm [ 131, and CS-QL uses presented. The major assumption made by the CR-method the environment the sensory relevant state information at each point prevents is temporarily hidden from view. (CR) Method was proposed as an in an active sensory system. control phase, and an overt control phase, the Lion algorithm and examples of their the state of controlling instances of this method, [47], were described is that in time by appropriately two phases: a perceptual system. This assumption it from being applied to tasks in which can be identified to maximize Stored-state methods are more appropriate for tasks in which information is hidden a predictive model of the external environment, whose own internal state is architectures were described: window-Q, line uses a tapped-delay events. The recurrent-model architecture The window-Q in time. Three stored-state and recurrent-Q. fixed length history of recent sensory-motor constructs used, in conjunction with sensory uses a recurrent neural network task directly. Because steps, its own internal input. These three architectures were demonstrated conditions inputs, to learn for their useful application were discussed. to drive control. The recurrent-Q the action-value the recurrent network can encode state information state is used to resolve ambiguities function time sensory on a series of hidden state tasks, and caused by inadequate across recurrent-model, to maintain a architecture architecture for the non-Markov The methods described on relatively in very complicated demonstrated compared nificant advance over traditional non-Markov stones lems of hidden state. to more sophisticated tasks at all. Perhaps in this article are preliminary simple domains. Nevertheless, reinforcement tasks and they have not been extensively in that they have only been tested or represent a sig- learning algorithms, which do not address these rather modest algorithms will serve as stepping for dealing with the ubiquitous prob- and capable methods these algorithms 304 S.D. Whiteheud, L,.-J. Lm/Artifciul Intelligence 73 (1995) 271-306 Acknowledgements acknowledge We gratefully to this article by Dana Ballard, the contributions made Tom Mitchell, Rich Sutton, Lonnie Chrisman, Ming Tan, Sebastian Thrun, and Rich Caruana. Thank you for sharing your thoughts and ideas, your comments and criticisms, and most of all your time and energy. References [ I 1 P.E. Agrc. The dynamic structure of everyday life. Ph.D. Thesis, Tech. Report No. 1085, MIT Artificial intelligence Lab., Cambridge, MA ( 1988). 12 1 D.W. Aha and D. Kibler, Noise-tolerant instance-based learning algorithms, in: Proceedings IJCAI-89, Detroit, MI (1989) 794-799. 13 1 J. Aloimonons. 333-356. 1. Weiss and A. Bandyopadhyay, Active vision, fnt. .I. Cornput. Vision 1 (4) (1988) 14 1 J.R. Bachrach, Connectionist modeling and control of finite state environments, Ph.D. Thesis, University of Massachusetts. Department of Computer and Information Sciences, Amherst, MA ( 1992). I 5 1 R. Bajcsy and P Allen, Sensing strategies, in: Proceedings U.S.-France Robotics Workshop, Philadelphia, PA (1984). 161 D.H. Ballard, Animate vision, Technical Report 329, Department of Computer Science, University of Rockester, Rochester, NY ( 1990). 17 1 A.B. Barto, S.J. Bradtke and S.P. Singh, Real-time learning and control using asynchronous dynamic programming, Technical Report 9 l-57, University of Massachusetts, Amherst, MA ( 1991) 18 1 A.G. Barto, R.S. Sutton and C.W. Anderson, Neuron-like elements that can solve difficult learning control problems, IEEE Truns. Syst. Man Cybern. 13 (5) (1983) 834-846. I 9 I R.E. Bellman, Dynamic Programming I IO I D.P. Bertsekas, Dynamic Progrummin~: Deterministic (Princeton University Press, Princeton, NJ, 1957). and Stochastic Models (Prentice-Hall, Englewood Cliffs, NJ. 1987). I I I I U. Bodenhausen and A. Waibel, The Tempo 2 algorithm: adjusting in: D.S. Touretzky, ed., Advances Mateo. CA, 1991). in Neural In&rmafion time-delays by supervised learning, Processing Systems 3 (Morgan Kaufmann, San I I2 1 D. Chapman, Vision, I I3 I D. Chapman Proceedings and L.P. Kaelbling, Learning IJCAI-91, Sydney, Australia Instntction. and Action I 14 I L. Chrisman, Reinforcement learning with perceptual (MIT Press, Cambridge, MA, 1993). from delayed reinforcement ( 199 I ) ; also: Teleos Technical Report TR-90-l the predictive distinctions aliasing: in a complex domain, I ( 1990). approach, in: in: Proceedings AAAI-92, San Jose, CA ( 1992) 183-l 88. I IS I J. Clouse and P.E. Utgoff. A teaching method for reinforcement Conference on Machine Learning. Aberdeen, Scotland International learning, in: Proceedings Ninth ( 1992). [ I6 I P. Dayan and G. Hinton, Feudal reinforcement in: J.E. Moody, S.J. Hanson and R.P. Lippmann, in NeuruL fnfi~rmation Processing Sysrems 5 (Morgan Kaufmann, San Mateo, CA, 1993). learning, eds., Advances I I7 1 J.L. Elman, Finding structure 1 I8 I J.J. Grefenstette, Credit assignment 3 (1988) 225-245. 1 19 1 J.H. Holland, Escaping brittleness: rule-based parallel Kaufmann, San Mateo, CA, 1986). systems, in time, Cogn. Sci. 14 ( 1990) 179-21 I. in rule discovery systems based on genetic algorithms, Much. Learn. the possibilities of general-purpose learning algortihms applied to in: Muchine Lenrnin~: An Artificial Intelligence Approach II (Morgan I20 I A.N. Jain, A connectionist Report CMU-CS-9 12 I I L.P. Kaelbling, Learning I22 I L.P. Kaelbling, Hierarchical learning architecture for parsing spoken language, Ph.D. Thesis, Technical I-208, Carnegie Mellon University, School of Computer Science ( 199 1). in embedded learning systems, Ph.D. Thesis, Stanford University, Stanford, CA ( 1990). in: Proceedings Tenth in stochastic domains: preliminary results, bternutional Corzference on Muchine Lxcwning ( 1993). S.D. Whitehead, L.-J. Lin/Art$cial Intelligence 73 (1995) 271-306 305 I231 S. Koenig and R. Simmons, Complexity analysis of real-time reinforcement learning applied to finding shortest paths in deterministic domains, Technical Report CMU-CS-93-106. School of Computer Science, Carnegie Mellon University, Pittsburgh, PA ( 1992). [ 241 Long-Ji Lin, Programming robots using reinforcement learning and teaching, in: Proceedings AAAI-91, Anaheim, CA (1991) 781-786. [25] Long-Ji Lin, Self-improving reactive agents based on reinforcement learning, planning and teaching, Mach. Learn. 8 ( 1992) 293-32 I. [261 Long-Ji Lin, Hierarchical learning of robot skills by reinforcement, in: Proceedings 1993 IEEE International Conference on Neural Networks (1993). [271 Long-Ji Lin, Reinforcement learning for robots using neural networks, Ph.D. Thesis, Technical Report CMU-CS-93-103, Carnegie Mellon University, School of Computer Science, Pittsburgh, PA ( 1993). [28] Long-Ji Lin and T.M. Mitchell, Reinforcement learning with hidden states. in: Proceedings Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats (MIT Press, Cambridge, MA, 1993 ) I 29 1 S. Mahadevan and J.H. Connell, Scaling reinforcement learning to robotics by exploiting the subsumption architecture, in: Proceedings Eighth International Workshop on Machine Learning, Evanston, IL ( 1991). [ 301 D. Michie and R.A. Chambers, ‘Boxes’ as a model of pattern-formation, in: C.H. Waddington, ed., Toward a Theoretical Biology 1, Prolegomena (Edinburgh University Press, Edinburgh, 1968) 206215. [31] T.M. Mitchell and S.B. Thrun, Explanation-based neural network learning for robot control, in: J.E. Moody, S.J. Hanson and R.F! Lippmann, eds., Advances in Neural Information Processing Systems 5 (Morgan Kaufmann, San Mateo, CA, 1993). 132) A. Moore, Variable resolution dynamic programming: efficiently learning action maps in multivariate in: Proceedings Eighth International Conference on Machine Learning, real-values state spaces, Evanston, IL (1991) 333-337. 1331 A.W. Moore and C.G. Atkeson, Prioritized sweeping: reinforcement learning with less data and less real time, Mach. Learn. 13 (1) (1993) 103-130. 1341 Jing Peng and R.J. Williams, Efficient learning and planning within the Dyna framework, in: Proceedings Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats (MIT Press, Cambridge, MA, 1993). [ 351 J.R. Quinlan, Induction of decision trees, Mach. Learn. 1 (1986) 81-106. [ 361 S. Ross, Introduction to Stochastic Dynamic Programming (Academic Press, New York, 1983). [ 371 D.E. Rumelhart, GE. Hinton and R.J. Williams, Learning internal representations by error propagation, in: D.E. Rumelhart and J.L. McClelland and the PDP Research Group, eds., Parallel Distributed Processing: Explorations in the Microstructure of Cognition 1 (MIT Press, Cambridge, MA, 1986) Chapter 8. [ 38 1 A.L. Samuel, Some studies in machine learning using the game of checkers, in: E.A. Feigenbaum and J. Feldman, eds., Computers and Thought (Krieger, Malabar, FL, 1963) 71-105. [39] J. Schmidhuber, Reinforcement in: D.S. Touretzky, ed., Advances in Neural Information Processing Systems 3 (Morgan Kaufmann, San Mateo, CA, 1991) 500-506. learning in Markovian and non-Markovian environments, [40] S.P Singh, Transfer of learning across compositions of sequential tasks, in: Proceedings Eighth International Workshop on Machine Learning, Evanston, IL (1991) 348-352. 1411 S.P. Singh, Transfer of learning by composing solutions of elemental sequential tasks, Mach. Learn. 8 (1992) 323-339. 1421 G.W. Snedecor and W.G. Cochran, Statistical Methods (Iowa State University Press, Ames, Iowa, 1989). learning, Ph.D. Thesis, University of 1431 R.S. Sutton, Temporal credit assignment in reinforcement Massachusetts at Amherst ( 1984); also: COINS Tech. Report 84-02. 1441 R.S. Sutton, Learning to predict by the method of temporal differences, Mach. Learn. 3 (1) (1988) 9-44. 1451 R.S. Sutton, Integrating architectures for learning, planning, and reacting based on approximating dynamic programming, in: Proceedings Seventh International Conference on Machine Learning, Austin, TX ( 1990). 1461 R.S. Sutton, ed., Reinforcement Learning (Kluwer, Boston, MA, 1992). I49 1 G. Tesauro, Practical I50 I SThrun 306 S.D. Whitrhrtrd L.-J. la /Art@irrl Itltulligenc~e 73 (1995) 271-306 147 1 Ming Tan, Cost sensitive I./CA/-Y/. Sydney, Australia reinforcement ( 199 I ) learning for adaptive classification and control. in: Proceedings 148 1 Ming Tan, Cost sensitive robot learning. Ph.D. Thesis. Carnegie Mellon University, Pittsburgh, PA (1991). issues in temporal differcncc learning. Mtrch. Ileum. 8 ( 1992) 2577277. and K. Moller, Planning with an adaptive world model, in: D.S. Touretzky, ed.. Adwnces 1n Neural I@mtation [ 5 I 1 S. Thrun, Efficient exploration Processing: .‘+wm 3 (Morgan Kaufmann, San Mateo, CA, I99 I ), in reinforcement learning, Technical Report CMU-CS-92.102, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA ( 1992). 1 52 1 S. Ullman, Visual routines. Cqynition 18 ( 1984) 97- 159; also in: S. Pinker. ed., visuctl Cognition (MIT Press, Cambridge, MA, 198s). IS3 I A. Waibel, Modular construction of time-delay neural networks for speech recognition, Neunll Cornput. 1 ( 1989) 39-46. I S4 I C.J.C.H. Watkins. Learning (1989). from delayed rewards, Ph.D. Thesis, University of Cambridge, England IS5 I C.J.C.H. Watkins and P. Dayan, Technical note: Q-learning, Mu& 1 Sh 1 SD. Whitehead, Complexity learning, and cooperation Anaheim, CA ( I99 I ) ; a similar version also appears in: Proceedings Eighth Mtrchrne LetrmiuK. Evanston, IS7 1 SD. Whitehead, Reinforcement IL ( I99 I ). learning in reinforcement Lam. 82, ( 1992) 39-46. in: ProcedinRs AAAI-91. International Workshop on for the adaptive control of perception and action, Ph.D. Thesis, Department of Computer Science. University of Rochester, Rochester, NY ( 1991). IS8 I S.D. Whitehead and D.H. Ballard. A role for anticipation in reactive systems that learn, in: Proceedings Sixth Internatiorurl Workshop OH Machirw Lecmin~, Ithaca, NY ( 1989). IS9 I S.D. Whitehead and D.H. Ballard. Active perception and reinforcement learning, Neurul Conq~~~f. 2 (4) ( 1990); also in: Proceedings Se\mth Interncrtionul Cmzference on Machine Leurning, Austin, TX (1990). 1601 S.D. Whitehead and D.H. Ballard. Learning to perceive and act by trial and error, Mach. Lmrrr. 7 ( I ) ( I99 I ) ; also: Technical Report 33 I, Department of Computer Science, University Rochester, NY ( 1990). of Rochester, 16 I I S.D. Whitehead and D.H. Ballard, A study of cooperative mechanisms learning, Technical Report 365, Computer Science Department, University of Rochester, Rochester. NY ( I99 I ). for faster reinforcement I62 I S.D. Whitehead. J. Karlsson and J. Tenenberg, Learning multiple goal behavior via task decomposition and dynamic policy merging, in: J.H. Connell and S. Mahadevan, eds., Robot Learning (MIT Press, Cambridge, MA, 1993). If.3 I R.J. Williams, Reinforcement learning in connectionist networks, Technical Report ICS 8605, Institute for Cognitive Science, University of California at San Diego ( 1986). 1641 R.C. Yee. S. Saxena. PE. Utgoff and A.G. Barto, Explaining temporal-differences to create useful concepts for evaluating states, in: Prowedings AAAI-90, Boston, MA ( 1990). 