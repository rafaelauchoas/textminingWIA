Artificial Intelligence 175 (2011) 487–511Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOnline planning for multi-agent systems with bounded communicationFeng Wu a,b,∗, Shlomo Zilberstein b, Xiaoping Chen aa School of Computer Science, University of Science and Technology of China, Jinzhai Road 96, Hefei, Anhui 230026, Chinab Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 2 February 2010Received in revised form 22 September2010Accepted 26 September 2010Available online 29 September 2010Keywords:Decentralized POMDPsCooperation and collaborationPlanning under uncertaintyCommunication in multi-agent systemsWe propose an online algorithm for planning under uncertainty in multi-agent settingsmodeled as DEC-POMDPs. The algorithm helps overcome the high computationalcomplexity of solving such problems offline. The key challenges in decentralized operationare to maintain coordinated behavior with little or no communication and, whencommunication is allowed, to optimize value with minimal communication. The algorithmaddresses these challenges by generating identical conditional plans based on commonknowledge and communicating only when history inconsistency is detected, allowingcommunication to be postponed when necessary. To be suitable for online operation,the algorithm computes good local policies using a new and fast local search methodimplemented using linear programming. Moreover, it bounds the amount of memory usedat each step and can be applied to problems with arbitrary horizons. The experimentalresults confirm that the algorithm can solve problems that are too large for the bestexisting offline planning algorithms and it outperforms the best online method, producingmuch higher value with much less communication in most cases. The algorithm also provesto be effective when the communication channel is imperfect (periodically unavailable).These results contribute to the scalability of decision-theoretic planning in multi-agentsettings.© 2010 Elsevier B.V. All rights reserved.1. IntroductionA multi-agent system (MAS) consists of multiple independent agents that interact in a domain. Each agent is a decisionmaker that is situated in the environment and acts autonomously, based on its own observations and domain knowledge, toaccomplish a certain goal. A multi-agent system design can be beneficial in many AI domains, particularly when a systemis composed of multiple entities that are distributed functionally or spatially. Examples include multiple mobile robots(such as space exploration rovers) or sensor networks (such as weather tracking radars). Collaboration enables the differentagents to work more efficiently and to complete activities they are not able to accomplish individually. Even in domains inwhich agents can be centrally controlled, a MAS can improve performance, robustness and scalability by selecting actions inparallel. In principle, the agents in a MAS can have different, even conflicting, goals. We are interested in fully-cooperativeMAS, in which all the agents share a common goal.In a cooperative setting, each agent selects actions individually, but it is the resulting joint action that produces theoutcome. Coordination is therefore a key aspect in such systems. The goal of coordination is to ensure that the individualdecisions of the agents result in (near-)optimal decisions for the group as a whole. This is extremely challenging especially* Corresponding author at: Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USA.Tel.: +1 413 545 1985; fax: +1 413 545 1249.E-mail addresses: wufeng@mail.ustc.edu.cn (F. Wu), shlomo@cs.umass.edu (S. Zilberstein), xpchen@ustc.edu.cn (X. Chen).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.09.008488F. Wu et al. / Artificial Intelligence 175 (2011) 487–511when the agents operate under high-level uncertainty. For example, in the domain of robot soccer, each robot operatesautonomously, but is also part of a team and must cooperate with the other members of the team to play successfully.The sensors and actuators used in such systems introduce considerable uncertainty. What makes such problems particularlychallenging is that each agent gets a different stream of observations at runtime and has a different partial view of thesituation. And while the agents may be able to communicate with each other, sharing all their information all the time isnot possible. Besides, agents in such domains may need to perform a long sequence of actions in order to reach the goal.Different mathematical models exist to specify sequential decision-making problems. Among them, decision-theoreticmodels for planning under uncertainty have been studied extensively in artificial intelligence and operations research sincethe 1950’s. Decision-theoretic planning problems can be formalized as Markov decision processes (MDPs), in which a singleagent repeatedly interacts with a stochastically changing environment and tries to optimize a performance measure basedon rewards or costs. Partially-observable Markov decision processes (POMDPs) extend the MDP model to handle sensor un-certainty by incorporating observations and a probabilistic model of their occurrence. In a MAS, however, each individualagent may have different partial information about the other agents and about the state of the world. Over the last decade,different formal models for this problem have been proposed. We adopt decentralized partially-observable Markov deci-sion processes (DEC-POMDPs) to model a team of cooperative agents that interact within a stochastic, partially-observableenvironment.It has been proved that decentralized control of multiple agents is significantly harder than single agent control andprovably intractable. In particular, the complexity of solving a two-agent finite-horizon DEC-POMDP is NEXP-complete [12].In the last few years, several promising approximation techniques have been developed [3,11,17,19,46,47]. The vast majorityof these algorithms work offline and compute, prior to the execution, the best action to execute for all possible situations.While these offline algorithms can achieve very good performance, they often take a very long time due to the doubleexponential policy space that they explore. For example, PBIP-IPG – the state-of-the-art MBDP-based offline algorithm –takes 3.85 hours to solve a small problem such as Meeting in a 3×3 grid that involves 81 states, 5 actions and 9 observations[3]. Online algorithms, on the other hand, plan only one step at a time and they do so given all the currently availableinformation. The potential for achieving good scalability is more promising with online algorithms. But it is extremelychallenging to keep agents coordinated over a long period of time with no offline planning. Recent developments in onlinealgorithms suggest that combining online techniques with selective communication – when communication is possible –may be the most efficient way to tackle large DEC-POMDP problems. The main goal of this paper is to present, analyze,and evaluate online methods with bounded communication, and show that they present an attractive alternative to offlinetechniques for solving large DEC-POMDPs.The main contributions of this paper include: (1) a fast method for searching policies online, (2) an innovative way foragents to remain coordinated by maintaining a shared pool of histories, (3) an efficient way for bounding the number ofpossible histories agents need to consider, and (4) a new communication strategy that can cope with bounded or unreliablecommunication channels. In the presence of multiple agents, each agent must cope with limited knowledge about theenvironment and the other agents, and must reason about all the possible beliefs of the other agents and how that affectstheir decisions. Therefore, there are still many possible situations to consider even for selecting just one action given thecurrent knowledge. We present a new linear program formulation to search the space of policies very quickly. Anotherchallenge is that the number of possible histories (situations) grows very rapidly over time steps, and agents could runout of memory very quickly. We introduce a new approach to merging histories and thus bound the size of the pool ofhistories, while preserving solution quality. Finally, it is known that appropriate amounts of communication can improvethe tractability and performance of multi-agent systems. When communication is bounded, which is true in many real-world applications, it is difficult to decide how to utilize the limited communication resource efficiently. In our work,agents communicate when history inconsistency is detected. This presents a new effective way to initiate communicationdynamically at runtime.The rest of the paper is organized as follows. In Section 2, we provide the background by introducing the formal modeland discussing the offline and online algorithms as well as the communication methods in the framework of decentralizedPOMDPs. In Section 3, we present the multi-agent online planning with communication algorithms including the generalframework, policy search, history merging, communication strategy and implementation issues. In Section 4, we report theexperimental results on several common benchmark problems and a more challenging problem named grid soccer. We alsoreport the results for the cooperative box pushing domain with imperfect communication settings. In Section 5, we surveythe various existing online approaches with communications that have been applied to decentralized POMDPs, and discusstheir strengths and drawbacks. Finally, we summarize the contributions and discuss the limitations and open questions inthis work.2. BackgroundIn this section we provide a formal description of the problem and some essential background. We consider settings inwhich a group of agents coordinate with each other at discrete time steps. The agents operate over some finite numberof steps, T , referred to as the horizon. At each time step, each agent first receives its own local observation from theenvironment and then takes an action. The combination of all agents’ actions causes a stochastic change in the state ofthe environment, and produces some joint reward, after which a new decision cycle starts. In such cooperative sequentialF. Wu et al. / Artificial Intelligence 175 (2011) 487–511489decision-making settings, the agents have to come up with a plan that maximizes the expected long term reward of theteam. Planning can take place either offline or online. In offline planning, the computed plans are distributed to each agentand executed by each agent based on its local information. Hence the planning phase can be centralized as long as theexecution is decentralized. Online planning algorithms often interleave planning with execution. Thus they only need tofind actions for the current step, instead of generating the whole plan at once as offline algorithms do. Agents can usuallyshare information with each other by communication. But communication is not a free resource in many applications. Wewill discuss the communication model in detail later in this section.2.1. Formal model of decentralized POMDPsThe Markov Decision Process (MDP) and its partially observable counterpart (POMDP) have proved very useful forplanning and learning under uncertainty. The Decentralized POMDP offers a natural extension of these frameworks for co-operative multi-agent settings. We adopt the DEC-POMDP framework to model multi-agent systems, however our approachand results apply to equivalent models such as MTDP [41] and POIPSG [40].Definition 1 (DEC-POMDP). A decentralized partially observable Markov decision process is a tuple (cid:3)I, S, { Ai}, {Ωi}, P , O ,R, b0(cid:4) where• I is a finite set of agents indexed 1, . . . , n. Notice that when n = 1, a DEC-POMDP is equivalent to a single-agent POMDP.• S is a finite set of system states. A state summarizes all the relevant features of the dynamical system and satisfies theMarkov property. That is, the probability of the next state depends only on the current state and the joint action, noton the previous states and joint actions: P (st+1|s0, (cid:5)a0, . . . , st−1, (cid:5)at−1, st, (cid:5)at) = P (st+1|st, (cid:5)at).• Ai is a finite set of actions available to agent i and (cid:5)A =×i∈I Ai is the set of joint actions, where (cid:5)a = (cid:3)a1, . . . , an(cid:4) denotesis the set of joint observations, where (cid:5)o =• Ωi(cid:3)o1, . . . , on(cid:4) denotes a joint observation. Every time step the environment emits one joint observation, but each agentonly observes its own component.is a finite set of observations available to agent i and (cid:5)Ω =×i∈I Ωia joint action. We assume that agents do not observe which actions are taken by others at each time step.• P is a state transition probability table. P (s(cid:7)(cid:7)|s, (cid:5)a) denotes the probability that taking joint action (cid:5)a in state s results in a. P describes the stochastic influence of actions on the environment. We assume that the transitiontransition to state sprobabilities are stationary, which means that they are independent of the time step.• O is a table of observation probabilities. O ((cid:5)o|sjoint action (cid:5)a and reaching state sprobability is also assumed to be stationary.(cid:7), (cid:5)a) denotes the probability of observing joint observation (cid:5)o after taking. O describes how the agents perceive the state of the environment. The observation(cid:7)• R : S × (cid:5)A → (cid:9) is a reward function. R(s, (cid:5)a) denotes the reward obtained from taking joint action (cid:5)a in state s. R spec-ifies the agents’ goal or task. It is an immediate reward for agents taking a joint action in some state. The agents donot generally observe the immediate reward at each time step, although their local observation may determine thatinformation.• b0 ∈ (cid:3)(S) is the initial belief state distribution. It is a vector that specifies a discrete probability distribution over S thatcaptures the agents’ common knowledge about the starting state.Formally, we define the history for agent i, hi , as the sequence of actions taken and observations received by agent i. Atany time step t,(cid:2)i , o1a0i , a1i , . . . , ot−1i(cid:3), at−1i, oti=htiis the history of agent i, and ht = (cid:3)ht(cid:4) is the joint history. The term joint belief b(·|h) ∈ (cid:3)(S) denotes the probabilitydistribution over states induced by joint history h. Given a set of joint histories of the previous step, computing a set ofjoint belief states of current step is straightforward using Bayes’ rule:1, . . . , htn(cid:3)(cid:2)∀s(cid:7)∈S , btO ((cid:5)ot|ss(cid:7)(cid:7)∈S O ((cid:5)ot|s(cid:7)(cid:7), (cid:5)at−1)Throughout this paper, we use b(h) as a shorthand of the joint belief b(·|h).(cid:7)|s, (cid:5)at−1)bt−1(s|ht−1)s∈S P (s(cid:7)(cid:7)|s, (cid:5)at−1)bt−1(s|ht−1)s∈S P (s(cid:4)(cid:7), (cid:5)at−1)(cid:7)|ht(cid:4)=s(cid:4)(1)A local deterministic policy δiis a mapping from local histories to actions in Ai , i.e. δi(hi) = ai . Andfor agent ia joint deterministic policy, δ = (cid:3)δ1, . . . , δn(cid:4),i.e. δ(h) =is a tuple of local deterministic policies, one for each agent,(cid:3)δ1(h1), . . . , δn(hn)(cid:4) = (cid:5)a. A deterministic policy can be represented as a policy tree with nodes representing actions andedges labeled with observations. A joint deterministic policy is a set of policy trees. Similarly, a local stochastic policy foragent i, πi(ai|hi), is a mapping from a local history hi to a distribution over Ai . A joint stochastic policy, π = (cid:3)π1, . . . , πn(cid:4),is tuple of local stochastic policies.490F. Wu et al. / Artificial Intelligence 175 (2011) 487–511Solving a DEC-POMDP for a given horizon T and start state s0 can be seen as finding a policy δ that maximizes theexpected cumulative reward(cid:3)(cid:2)Vδ, s0= E(cid:8)(cid:2)Rst, (cid:5)a t(cid:3)(cid:7)(cid:7)s0(cid:5)T −1(cid:6)t=0Because of the recursive nature of DEC-POMDPs, it is more intuitive to specify the value function recursively:(cid:2)Vδ, ht(cid:3)=(cid:6)(cid:3)(cid:2)pst|ht(cid:3)(cid:2)Rst , (cid:5)a+(cid:6)(cid:2)Pst+1|st , (cid:5)a(cid:3)(cid:2)(cid:5)o|st+1, (cid:5)a(cid:3)V(cid:2)Oδ, ht+1(cid:3)(cid:9)(cid:10)stst+1,(cid:5)owhere (cid:5)a = δ(ht), ht+1 = ht ◦ (cid:5)a ◦ (cid:5)o and the state distribution p(st|ht) given history ht is computed recursively as follow:(cid:3)(cid:2)pst|ht(cid:2)(cid:5)o|st, (cid:5)a= O(cid:3)(cid:6)(cid:2)Pst|st−1, (cid:5)a(cid:3)(cid:3)(cid:2)(cid:5)a|ht−1pp(cid:2)st−1|ht−1(cid:3)(2)(3)(4)st−1where ht = ht−1 ◦ (cid:5)a ◦ (cid:5)o and p(s|h0) = b0(s), ∀s ∈ S. A survey of DEC-POMDP models and algorithms is available in [48].Previous studies have identified different categories of DEC-POMDPs characterized by different levels of observabilityand interaction. The computational complexity of solving these problems ranges between NEXP and P [26]. When eachindividual observation of an agent identifies the true state uniquely, a DEC-POMDP reduces to a multi-agent MDP (MMDP)[15]. When the joint observation identifies the true state, a DEC-POMDP is referred to as a DEC-MDP. A DEC-MDP is calledtransition-independent DEC-MDP (TI-DEC-MDP) when the transition models of the agents are independent of each other [9].Other special cases that have been considered are, for instance, goal-oriented DEC-POMDPs [26], event-driven DEC-MDPs [8],network distributed POMDPs (ND-POMDPs) [33], DEC-MDPs with time and resource constraints [13,14,29], DEC-MDPs withlocal interactions [51] and factored DEC-POMDPs with additive rewards [36].In this work we consider the general finite-horizon DEC-POMDPs, without any simplifying assumptions on the obser-vations, transitions, or reward functions. The complexity of general finite-horizon DEC-POMDPs has been shown to beNEXP-complete. Due to these complexity results, optimal algorithms have mostly theoretical significance. Current researchefforts in this area focus mostly on finding scalable approximation techniques [3,11,17,19,46,47].2.2. Offline algorithms versus online algorithmsDeveloping algorithms for solving DEC-POMDPs approximately has become a thriving research area. Most existing algo-rithms operate offline, generating some type of a complete policy before execution begins. The policy specifies what actionto take in any possible runtime situation. While good performance can be achieved using these algorithms, they often takesignificant time (e.g. more than a day) to solve modest problems. The reason is that they need to consider all possiblepolicies of the other agents – or a sufficiently large set of policies – in order to preserve solution quality. In domains suchas robot soccer, it is not feasible to consider all possible strategies all through to the end. Besides, small changes in theenvironment’s dynamics require recomputing the full policy. In contrast, online algorithms only need to plan the currentaction and thus can be much faster. This has long been recognized in competitive game playing such as chess. A typicalalgorithm often performs a limited amount of lookahead and plans only for the current step, and then repeats this processonline after observing the opponent’s response. Furthermore, online planning can better handle emergencies and unforeseensituations, which allows online approaches to be applicable in many domains for which offline approaches are not adequate.Implementing online algorithms for decentralized multi-agent systems is very challenging. Since the underlying systemstate as well as the observations of the other agents are not available during execution time, each agent must reason aboutall possible histories that could be observed by the other agents and how that may affect its own action selection. Incooperative multi-agent domains, agents must ensure coordination. Consider, for example, a robot soccer problem in whichtwo defenders (D1 and D2) are trying to mark two attackers ( A1 and A2). Each defender has different observations ofthe environment and thus may compute a different best joint plan online based on its own local knowledge. For example,the best joint plan based on D1’s knowledge is (cid:3)D1 → A1, D2 → A2(cid:4) and the best joint plan based on D2’s knowledge is(cid:3)D1 → A2, D2 → A1(cid:4), where D i → A j denotes defender D i marking attacker A j . However, the actual joint action executedis (cid:3)D1 → A1, D2 → A1(cid:4), which is an example of miscoordination. The outcome of miscoordination could be arbitrarily badand cause severe failures in teamwork [57]. In the example mentioned above, miscoordination leads to undesired behavior,namely two defenders mark A1 while A2 is left unmarked. In practice, if each agent computes policies with different privateinformation, there is a risk that the resulting policies will fail to achieve the intended effects. On the other hand, if agentsignore their private information, their policies become open-loop controllers, without considering their local observations.Thus, we define coordination as follows.Definition 2 (coordination). When agents maintain a single shared plan (joint policy) and always execute an action that ispart of that plan (policy), we say that the MAS exhibits coordination. Otherwise, it exhibits miscoordination.F. Wu et al. / Artificial Intelligence 175 (2011) 487–511491It is very difficult to guarantee coordination when planning is performed online and agents have different local informa-tion. Another major difficulty is that online algorithms must often meet real-time constraints, greatly reducing the availableplanning time, compared with offline methods. Due to these difficulties, work on online planning for DEC-POMDPs has beensparse. In this paper, we address these key challenges of multi-agent online planning and answer the following questions:(1) How to guarantee coordination when agents operate based on local observations, each having different private infor-mation? (2) How to meet planning time constraints and bound memory usage when each agent must reason about a vastnumber of possible outcomes and action choices of other agents?2.3. Communication in decentralized POMDPsAgents with limited source of information about their teammates must reason about the possible histories of teammembers and how these histories affect their own behaviors. Communication can alleviate this problem by sharing pri-vate information such as sensory data. Hence, online algorithms often incorporate communication to improve performance.However, communication is often limited by bandwidth and sometimes can be costly or unreliable. For example, robotsthat work underground or on another planet may need to move to certain locations to initiate communication. Even whencommunication is readily available and cheap – for instance, in the case of indoor mobile robots – limited bandwidth andunreliability often lead to latency and robots may need to wait for a period or resend messages several times until thecritical information is fully received. In all these situations, too much communication will affect negatively the performanceof the system. Thus, an interesting and challenging question is how to integrate online planning with communication anduse communication effectively.Bounded communication is recognized as an important characteristic of multi-agent systems due to several factors:(1) Agents may move into regions that have no coverage of the communication signal. For example, search and rescue robotsworking in subterranean tunnels such as subways and mine caves may have no wireless communications because highfrequency waves cannot penetrate rock, limiting radio communication to areas with line of sight between the transceivers[30]. Another example is two Mars rovers with one rover located behind some blocking obstacle [6]. In both examples,robots must move to certain spots where the wireless connection is available and then try to communicate with eachother. Obviously, communication is costly if the working sites are far from the communication spots. Agents must boundthe frequency of communication and maintain coordination when communication is unavailable. (2) Agents may not beable to share all the information with other agents all the time due to the limitations of the communication channel andcomputational device. Consider, for example, the AIBO soccer robot [42]. Communication latency of the wireless network ishigh. On average, messages takes 0.5 seconds to be received by all teammate robots, and in some cases, latency is observedto be as high as 5 seconds. This latency makes it difficult for the robots to communicate all the most recent informationall the time. The robots also have limited computational power. They receive and process images at 20 frames per second.Because each image may contain only a few (or none) of the relevant state features, it takes time to build a world modeland compute what should be shared with others. Besides, receiving messages from others at 20 Hz overloads the robotoperating system’s message buffers. After a few minutes of attempting to communicate their private information as they areupdated, the robots’ motion controllers are affected, causing the robots to slow down and occasionally crash. (3) Anotherimportant factor is communication failures, which are very common in wireless sensor networks [1]. Failures may requiremultiple attempts to communicate before the information is transmitted successfully. In all the scenarios mentioned above,bounded communication is preferable. Additionally, it is known that free communication reduces a DEC-POMDP to a largesingle agent POMDP. This is done by having each agent broadcast its local observation to the other agents at each timestep. When all of the local observations are known to every agent, they can be treated as a single joint observation, givingthe system the same complexity as a single-agent POMDP, PSPACE-complete. When communication is not free, finding theoptimal communication policy is as hard as the general DEC-POMDP, which is NEXP-complete [41]. In this work, we adoptbounded communication which is a more suitable assumption for the domains we are interested in.There are three possible ways in which agents can share information and coordinate their actions: indirect communi-cation, direct communication, and using common uncontrollable observed features. In indirect communication, one agent’sactions can affect the observations made by another agent. Hence these observations can serve as messages transmittedbetween the agents. Generally, when the observations of the agents are dependent on non-local information, each obser-vation provides some form of indirect communication. Thus a general DEC-POMDP already includes this form of indirectcommunication, and the policy determines what to communicate and when. With direct communication, information canbe shared by the agents by sending messages directly to each other. When there are components of the global system statethat are observed by both agents but are not affected by any of these agents’ actions, the agents can then act upon thecommon knowledge and coordinate their actions without exchanging messages directly.In this work we consider direct communication and use the sync communication model [61] where each agent broadcastsits local information to all the others. The communication language simply allows transmission of the agents’ action-observation histories. The tell and query models [61] are more complex, allowing one way communication between onepair of agents. In designing a general communication strategy, one must determine when to communicate, with whom tocommunicate, and what to send. However, when the sync model is used, the main question is when to initiate communica-tion. Once communication is initiated by any agent, all the agents share their local information. Communication is generally492F. Wu et al. / Artificial Intelligence 175 (2011) 487–511Fig. 1. Online planning framework for two agents.assumed to be instantaneous – a message is received without delay as soon as it is sent. But in this work we consider theimpact of possible stochastic delays.The sync communication model is essential to simplify the already high complexity of planning with different sets ofpartial information. Nevertheless, this does not necessarily mean that agents must transmit all their local information to theother agents. Agents need only transmit the information which is relevant to establishing coordination. For example, in themulti-access broadcast channel problem [11], the actual information communicated may be only several bytes to indicatethe status of the buffer, but the messages in the buffer themselves may be several mega-bytes or giga-bytes large. Similarly,it is common for a file sharing system to broadcast the file names in a directory to all the peers instead of transmittingall the files. In DEC-POMDPs, the messages exchanged among agents are sequences of high-level observations, not thesensor readings that are the basis for each observation. Additionally, it is possible to incorporate selective communicationmethods [43] within our model. Generally, this can be done by choosing observations with larger impact on the beliefs –an interesting direction that is left for future work. In practice, the usefulness of selective communication often depends onhow the observation model is defined. Even with selective communication, the question of when to communicate remainsopen. Another challenge simplified by the sync model is the choice of whom to communicate with. This is particularlyrelevant when each agent interacts directly with a subset of the group, rather than the entire team. In this paper, we focuson general DEC-POMDP settings without assuming any special interaction structure. The domains we investigate requirecoordination among all the agents, which means that the action of one agent affects the optimal choices of all the others.The sync model is thus suitable for these settings, and the question of when to communicate that it presents is still verychallenging.Additionally, we use and-communication [20], assuming separate communication and action phases in each time step.Thus communication facilitates better domain-level action selection by conditioning domain actions on the specific infor-mation received, rather than replacing domain-level actions as in or-communication [20]. We did not factor an explicit costfor communication in this work because any such cost would have been arbitrary and not particularly relevant to the appli-cations we are interested in. In the experimental results, we show that our approach is effective in the sense that it achievesbetter value with less communication compared with existing online techniques with communication. This guarantees that,if the cost of communication is specified, our approach will be beneficial for any such cost.3. Multi-agent online planning with communicationIn this section we introduce a new algorithm, Multi-Agent Online Planning with Communication (MAOP-COMM), forfinding approximate solutions of general DEC-POMDPs. This algorithm is executed in parallel by all the agents in the team,interleaving planning and execution. More precisely, our online algorithm is divided into a planning phase, an executingphase and an updating phase, which are applied consecutively at each time step. An example involving two agents is shownin Fig. 1.Definition 3 (belief pool). A belief pool at time-step t is defined by a tuple (cid:3){H tagent i and Bt is a set of joint belief states, Bt = {b(ht)|ht ∈ Ht} where Ht =×i∈I Htii .|i ∈ I}, Bt(cid:4), where Hti is a set of histories forTo illustrate this concept, consider the robot soccer problem mentioned in Section 2.2. Suppose that the defense strategyis that each defender should mark the nearest attacker. Then, the belief pool will contain knowledge about who is thenearest defender for each attacker. As long as all the defenders maintain the same belief pool and use the same tie-breakingrule, the outcome plans they compute (determining which defender marks each attacker) will be the same. This guaranteesthat the strategies of the defenders are coordinated.F. Wu et al. / Artificial Intelligence 175 (2011) 487–511493Algorithm 1: Expand Histories and Update BeliefsInput: H t , Bt , δtH t+1 ← ∅; Bt+1 ← ∅for ∀ht ∈ H t , ∀(cid:5)o ∈ (cid:5)Ω do(cid:5)a ← δt (ht )// append (cid:5)a, (cid:5)o to the end of ht .ht+1 ← ht ◦ (cid:5)a ◦ (cid:5)o// calculate the distribution of ht+1.p(ht+1) ← p((cid:5)o, (cid:5)a|ht )p(ht )// test if ht+1 is a reachable joint history.if p(ht+1) > 0 thenH t+1 ← H t+1 ∪ {ht+1}// compute the belief state of ht+1.bt+1(·|ht+1) ← Update belief with bt (·|ht ), (cid:5)a, (cid:5)o// add bt+1 into a hash table indexed by ht+1.Bt+1 ← Bt+1 ∪ {bt+1(ht+1)}return H t+1, Bt+1Definition 4 (local and joint policies). A local policy for agent i is a mapping from a set of histories to a set of actions,δi : H i → Ai , and δi(hi) denotes the action assigned to history hi . A joint policy is a set of local policies, δ = (cid:3)δ1, δ2, . . . , δn(cid:4),one for each agent, and δ(h) denotes the joint action assigned to joint history h.In the planning phase, each agent computes a joint policy δt for every possible history in the belief pool. During the← ht−1i (hti )i , and appends the action to the end of the◦ ai . After that, each agent updates its own belief pool based on the plan δt , as shown in Algorithm 1,executing phase, agent i adds its new observation to its own local history, htiaccording to its component in the joint policy δt and its current local history htlocal history, htiand continues to the next step.i , executes an action ai = δt← hti◦ otiAs mentioned, coordination is an important issue in multi-agent planning since the outcome of uncoordinated policiescan be arbitrarily bad. In offline planning, the coordination is guaranteed by distributing and executing the same pre-computed joint policy. It is much more difficult to achieve coordination in online planning because the policy is computedonline and each agent receives different (or partial) information from the environment.In our online algorithm, each agent maintains the same joint histories for the team. This ensures that all the agentsfind the same joint policy and thus remain coordinated. While the algorithm is randomized, it nevertheless ensures thateach agent finds the same set of joint policies by using the same pseudo-random number generator with an identicalseed. It is important to emphasize that we only use common knowledge for planning. With the same belief pool andrandomization scheme, each agent can generate exactly the same joint policy. Each agent’s local observation is used onlyfor policy execution.Definition 5 (belief inconsistency). When an agent believes that p must be true and the agent’s observation implies ¬p, wesay that the belief of that agent is inconsistent.Intuitively, belief inconsistency occurs when an agent’s beliefs contradict the observations it obtains from the environ-ment. For example, in the robot soccer problem mentioned in Section 2.2, suppose that D1 observes that its nearest attackeris A2, not A1 as indicated by its (inconsistent) belief. Then D1 should communicate with all the other defenders, inform-ing them that the nearest attacker to D1 is actually A2 so that they can update their belief pools with more accurateinformation. Hence, the team benefits from communication by finding better plans based on consistent beliefs.This is why communication is triggered when our algorithm detects belief inconsistency. The agent then initiates com-munication as soon as the communication resource is available. When communication occurs, each agent broadcasts itsown local observation sequence to the other agents (the sync model). Consequently, each agent can construct the actualjoint history and calculate the actual joint belief state. The best joint action is then selected based on the new joint beliefstate. And the belief pool is emptied and replaced with the actual history.Notice that communication is not essential to ensure coordination in our framework, but it offers an optional mecha-nism to improve performance. This makes the use of communication more flexible, particularly in domains with a boundedcommunication resource. Communication can be easily integrated into our framework by considering it before the plan-ning phase. Coordination is guaranteed in any case because communication just updates the common knowledge of theagents. More precisely, the belief pools of the agents are the same without communication, and they remain the sameafter communication occurs. The sync model used by our work guarantees that each agent has the same knowledge aftercommunication.494F. Wu et al. / Artificial Intelligence 175 (2011) 487–511Algorithm 2: Multi-Agent Online Planning with CommunicationInput: b0, seed[1..T − 1]foreach i ∈ I (parallel) do(cid:5)a0 ← arg max(cid:5)a Q ((cid:5)a, b0)Execute the action a0i and initialize h0iH 0 ← {(cid:5)a0}; B0 ← {b0}; τcomm ← falsefor t = 1 to T − 1 doSet the same random seed by seed[t]H t , Bt ← Expand histories and beliefs in H t−1, Bt−1← Get the observation from the environmentoti← Update agent i’s own local history with othtiiif H t is inconsistent with oti thenτcomm ← trueif τcomm = true and communication available thenSynch htτcomm ← falsei with other agentsif agents communicated thenht ← Construct the communicated joint historybt (ht ) ← Calculate the joint belief state for ht(cid:5)at ← arg max(cid:5)a Q ((cid:5)a, bt (ht ))H t ← {ht }; Bt ← {bt (ht )}elseπ t ← Search the stochastic policy for H t , Bt← Select an action according to π t (ai|htati )iH t , Bt ← Merge histories based on π t← Update agent i’s own local history with atihtiExecute the action atiTheorem 1. The multi-agent online planning algorithm (MAOP-COMM) always guarantees coordination among agents with or withoutcommunication.Proof (sketch). We present several mechanisms to ensure that each agent maintains the same belief pool and finds thesame joint policy for the team, so they can still coordinate with each other online. As shown in Algorithm 1, only commonknowledge arising from the model is used to update the belief pool. Agents run the algorithm in lockstep and a set ofpredetermined random seeds are used to ensure that all the agents come up with the same randomized behavior. Theonly private information is the local history of each agent, which is tracked by remembering the actions executed andthe observations received in previous steps. This local history is only used to execute the plan and has no effect on thecoordination mechanisms. In Fig. 1, the inputs and outputs of the plan and update modules are the same for all agents. The“execute” module is the only part that considers the local information of each agent, but this does not make any change tothe belief pool as well as the joint policy and cannot lead to miscoordination. We use the sync communication model toreset the belief pools with the same histories and joint belief state. Thus, each agent also maintains the same belief poolafter communication. As long as the belief pools of each agent are identical, the joint policies computed based on the beliefpools are still the same. Therefore, agents remain coordinated. (cid:2)Our online algorithm starts by first calculating and executing the best joint action for the initial belief state using aheuristic value function. Then, the main planning loop shown in Algorithm 2 is executed. Generally, there are three majorchallenges in implementing this framework: (1) It is an NP-hard problem to find the decentralized policies for every possiblehistory of every agent [58]. In Section 3.1, we provide an approximate solution by solving a series of linear programs. (2) Thebelief pool itself can be extremely large, making it impossible be managed online. More precisely, the number of possiblehistories grows exponentially with the time step. In Section 3.2, we introduce policy-based techniques to bound the memoryusage of the belief pool. (3) Detecting inconsistency in the belief pool is nontrivial given only the agent’s local information.In Section 3.3, we describe a method to address that problem efficiently. Finally, in Section 3.4, we discuss some datastructures used in our implementation to store belief pools.3.1. Searching stochastic policies using linear programmingIn decentralized multi-agent systems, agents without knowledge of the observations of the other agents must reasonabout all the possible belief states that could be held by others and how that affects their own action selection. In order tofind agent i’s policy qi for history hi , agents need to reason about all the possible histories h−i held by the others as wellF. Wu et al. / Artificial Intelligence 175 (2011) 487–511495Fig. 2. Illustration of similarity between the one-step online planning and the one-step offline policy tree construction.as all the possible policies associated with them. In other words, we need to find a joint policy δ that maximizes the valuefunction below:(cid:6)(cid:6)(cid:2)(cid:3)V (δ) =p(s|h)Vδ(h), s(5)h∈Hs∈Swhere p(s|h) is the state distribution given a joint history h.It is important to point out that the joint policy created by our approach is a truly decentralized policy, which dependson the private information of each agent. That is, the resulting policy of each agent depends only on its individual obser-vation history, i.e. δ(h) = (cid:3)δ1(h1), . . . , δn(hn)(cid:4). The goal of our multi-agent online planning is that each agent independentlycalculates the same plan δ(h) for the team and then executes its share of the plan based on its own local history. Forexample, agent i will execute the action ai = δi(hi). The advantage of a decentralized policy is that each agent can executeits part of the plan independently based on the local information it acquired so far.Finding decentralized policies online is analogous to the one-step policy tree construction used in offline DEC-POMDPplanning algorithms. As shown in Fig. 2, the histories (h1, h2, . . . , h8) are paths of the tree from the root down to thecurrent branches. The target of both online and offline planning is to associate the histories with the “right” sub-policiesthat satisfy the optimality criterion. In offline planning histories are often represented as trees and the goal is to constructthe best complete policy. In contrast, online algorithms store histories in a sequence form and evaluate the policy onlyfor the current step. Obviously, the number of histories represented in the sequence form is much smaller, which is a keyadvantages of online planning.The straightforward way of finding the best joint policy is to enumerate all possible mappings from histories to sub-policies and choose the best one. However, the size of the joint space is exponential over the number of the possiblehistories. The number of histories itself grows exponentially with the problem horizon. In fact, this problem is equivalent tothe decentralized decision problem studied by [58], which has been proved to be NP-hard [58]. In our algorithm, we find anapproximate solution using stochastic policies, by solving the problem as a linear program. The value of a joint stochasticpolicy, π , is as follows:πi(qi|hi)Q(cid:2)(cid:3)(cid:5)q, b(h)(6)V (π ) =(cid:6)(cid:6)(cid:11)p(h)h∈H(cid:5)qi∈Iwhere p(h) is the probability distribution of history h, b(h) is the belief state induced by h, and Q ((cid:5)q, b(h)) is the value ofpolicy (cid:5)q at b(h). Note that (cid:5)q is a policy from the current time to the end of the problem, so Q ((cid:5)q, b(h)) is the value thatagents will achieve in future steps when starting with a state distribution b(h). Unfortunately, the optimal value of Q is notavailable online. In fact, this is a subproblem of the original DEC-POMDP with a new horizon T − t where t is the currenttime step. So trying to find the optimal value is equivalent to solving the entire problem offline by simply setting t = 0. Butthe optimal value can be estimated by certain heuristics. Usually, the heuristic close to the optimal value may take moretime to compute but yield better performance and vice versa.We use one-step lookahead to estimate the value of future steps. It means that we consider a policy with only one actionnode. In this case, any approach which provides a set of value functions V (s) can be used to define the heuristic. Ideally, theheuristic should represent not only the immediate value of a joint action but also its expected future value. As mentioned,finding the optimal value is intractable because it requires us to do as much work as solving the entire DEC-POMDP. Oneapproach we used is the solution of the underlying MDP. For the one-step lookahead case, qi, (cid:5)q can be simplified to ai, (cid:5)a.The QMDP heuristic [28] can then be written as follows:Q ((cid:5)a, b) =(cid:9)b(s)R(s, (cid:5)a) +(cid:6)s∈S(cid:6)s(cid:7)∈S(cid:2)P(cid:7)|s, (cid:5)as(cid:3)V MDP(cid:10)(cid:3)(cid:2)(cid:7)s(7)496F. Wu et al. / Artificial Intelligence 175 (2011) 487–511Table 1Improving the policy using linear programming.Variables: ε, πi (qi |hi )Objective: maximize εImprovement constraint:(cid:4)V (π ) + ε (cid:2)h∈H p(h)(cid:4)(cid:5)q πi (qi |hi )(cid:12)k(cid:15)=i πk(qk|hk)Q ((cid:5)q, b(h))Probability constraints:(cid:4)πi (qi |hi ) = 1∀hi ∈ H i ,qi∀hi ∈ H i , qi , πi (qi |hi ) (cid:3) 0where V MDP is the value function of the underlying MDP. The QMDP heuristic is an upper bound of the optimal value sinceit is based on the assumption that agents fully observe the underlying system state at each step. A tighter bound would bethe QPOMDP heuristic [44]:(cid:9)(cid:10)Q ((cid:5)a, b) =b(s)R(s, (cid:5)a) +(cid:6)s∈S(cid:6)(cid:2)Ps(cid:7)∈S(cid:7)|s, (cid:5)as(cid:3) (cid:6)O(cid:5)o∈ (cid:5)Ω(cid:3)(cid:2)(cid:5)o|s(cid:7), (cid:5)aV POMDP(cid:3)(cid:2)b(cid:5)o(cid:5)a(8)(cid:5)o(cid:5)ais the successor belief state of b with (cid:5)a, (cid:5)o and V POMDP is the value function of the underlying POMDP. Intuitively,where bthis means the agents will share their observations at each future step. When a concrete problem is considered, domain-specific knowledge can be used to better estimate the heuristic value. In our implementation, we use the QMDP heuristicbecause the underlying MDPs of the tested domains can be solved quickly and optimally. The one-step lookahead could beextended into a multi-step lookahead, but this is much more complex in our settings and beyond the scope of this article.To start the search procedure, each local stochastic policy πi is initialized to be deterministic, by selecting a randomaction with a uniform distribution. Then, each agent is selected in turn and its policy is improved while keeping the otheragents’ policies fixed. This is done for agent i by finding the best parameters πi(qi|hi) satisfying the following inequality:(cid:10)(cid:2)(cid:3)(cid:5)q, b(h)πi(qi|hi)π−i(q−i|h−i)QV (π ) (cid:2)(cid:9)(cid:6)p(h)(cid:6)(9)h∈H(cid:5)q(cid:12)where π−i(q−i|h−i) =k(cid:15)=i πk(qk|hk). The linear program shown in Table 1 is used to find the new parameters of πi . Theimprovement procedure terminates and returns π when ε becomes sufficiently small for all agents. Although the algorithmwill terminate after a finite number of iterations, convergence to optimality is not guaranteed. It is possible to get stuck in asuboptimal Nash equilibrium in which the policy of each agent is optimal with respect to the others. In fact, the suboptimalsolution may achieve a value which is arbitrarily far from the globally optimal one. Hence, algorithms which start with anarbitrary policy and make iterative improvements by alternating among the agents cannot produce polices which are withina guaranteed bound of the optimal value.One simple technique is to use random restarts to move out of local maxima. The observation is that different startingpoints may converge to different locally optimal values. Hopefully, with several random restarts, one of the values may beglobally optimal or close to it. This process is shown in Algorithm 3. The number of restarts is determined by the onlineruntime constraint. If the time per planning step is long, more restarts can be used and there is a better chance to get closeto the globally optimal solution. This simple technique works very well in the test domains we experimented with.Note that each agent shares the same random seed so that all agents have the same randomized behavior. It is easy to(cid:7)construct situations in which two policies qi and qi have the same value. In order to guarantee coordination, each agentwill choose the same policy according to a predetermined tie-breaking rule based on a canonical ordering of the policies(e.g. qi ≺ q(cid:7)i ).3.2. Bounding joint histories using policy-based mergingNote that the underlying system state as well as the observations of other agents are not available during the executiontime of DEC-POMDPs. Each agent must reason about all the possible histories that could be observed by the other agents andhow that may affect its own action selection. However, the number of possible joint histories increases exponentially withthe horizon, which is (|Ωi|T )|I|assuming that each agent coordinates at each step and knows the joint policy. For a smallproblem with 2 agents and 2 observations, the number of possible joint histories with 100 steps is 2100×2≈1.6 × 1060,which is infeasible for any planning algorithms. Even storing them in the memory is impossible. This presents a majorchallenge for developing online algorithms for DEC-POMDPs.Optimal history merging. A more detailed analysis of the planning process shows that most of the joint histories kept inmemory are useless. One reason is that the goal of reasoning about others’ histories is to find out what others do and see.Clearly, each time only one sequence corresponds to this for each agent. Because they do not share private informationwith each other, what agents can do is to maintain a distribution over the histories based on the information they have. AsF. Wu et al. / Artificial Intelligence 175 (2011) 487–511497Algorithm 3: Search Stochastic Policies with Random RestartsInput: H, Bfor several restarts do// select the start point randomly.π ← Initialize the parameters to be deterministic with random actionsrepeatε ← 0foreach i ∈ I do// optimize the policy alternatively.πi, ε(cid:7) ← Solve the linear program in Table 1 with H, B, π−iε ← ε + ε(cid:7)until ε is sufficiently smallif π is the current best policy thenπ ∗ ← πreturn π ∗long as the distribution is sufficiently accurate, eliminating histories early on may have no effect on the decision. Anotherreason is that the history segments of the early stage may be useless. For example, in the multi-agent tiger problem [32],two agents are facing two doors (left and right): behind one lies a tiger and behind the other lies untold riches. The agentscan independently open either door or listen for the position of the tiger. The reward function is designed to encouragecoordination in that opening a door together will sustain less injury if the tiger is present or receive a greater amount ofwealth if the riches are present. The listen action incurs a small cost. The position of the tiger is reset randomly after adoor is opened. The resulting policy with horizon 6 may be that an agent listens twice, opens a door if it receives the sameobservation of the tiger’s position, listens twice again and then again opens a door if it received the same two observationsof the tiger’s position. For all histories in which an agent opens a door on the third step, the best action for it to take by thetime of the last step will not be affected by any observations received in steps 1 to 3. Those observations provide no usefulinformation since the tiger’s position is reset if a door is opened. From the perspective of the team, the only importantthing for an agent is the probability that others opened a door in step 3. This quantity can be found by grouping similarhistories together.Definition 6. Two histories hi, hand ∀h−i, s, b(s|h) = b(s|h(cid:7)i for agent i are probabilistically equivalent (PE) when the following holds: ∀h−i, p(h) = p(h(cid:7))(cid:7)) where h = (cid:3)hi, h−i(cid:4), h(cid:7) = (cid:3)h(cid:7)i, h−i(cid:4) and h−i is a joint history without agent i’s.The PE condition means that two histories of an agent have the same distribution and also the same resulting beliefstate, but they may differ in action-observation sequences. The following property has been established for histories thatare probabilistically equivalent.Lemma 1. (See [38].) When two histories are PE, then they are best-response equivalent and can be clustered together as one historywithout any loss of value.Although the PE condition has very nice theoretical properties, it is not very applicable in practice because it requires toknow h−i , every possible history combination of other agents. As mentioned earlier, the range of values of h−i will be verylarge and it is intractable to test every possibility. Considering every possible h−i is important because every single valuemay affect the policy of agent i. Hence it is worth analyzing how these values affect agent i’s policy. First, agent i couldform a belief about the current state by reasoning about the history of the other agents, h−i . Together with its own historyhi , agent i can calculate a state distribution b((cid:3)hi, h−i(cid:4)) based on the joint history. Second, agent i needs to know whatpolicies others take because it should choose a policy which complements the others. In this context, the goal of reasoning∗about the histories of other agents is to compute a policy. If the optimal policy qi of agent i with hi is given, agent i canfollow the policy without considering its own history as well as the others’. For example, in the multi-agent tiger problem,if an agent knows that the optimal policy from now on is to open a door whenever it hears the tiger’s roar behind thatdoor twice, remembering what happened before is not necessary.Definition 7 (policy equivalent). Two histories hi, hidentical optimal policy.(cid:7)i of agent i are policy equivalent (POE) when hi, h(cid:7)i , have at least oneHere, a policy means a complete conditional plan from the current step to the last step. Usually, a policy is representedas a tree with nodes corresponding to actions and branches corresponding to observations. The optimal policy is a fixedpoint in policy space, which produces the best team performance given that other agents follow their optimal policies.Therefore, the optimal policy of an agent is part of the optimal joint policy of the team. In DEC-POMDPs, there exists at498F. Wu et al. / Artificial Intelligence 175 (2011) 487–511least one optimal joint policy for the team. The optimal joint policy of small problems can be computed by optimal offlinealgorithms [5,27,55,56].Theorem 2. When two histories hi, heither of them.(cid:7)i of agent i are POE, then they can be merged without loss of value in policy generation by keepingProof. At step 0, if the optimal policies for steps 0 to T are given, agents can select the optimal joint policy for b0. At step t,i . At any future step t + k, forassume agent i merges two histories htany k-step history hk. If not, the optimal policies for(cid:7)t(cid:7)thti are different because they have different sub-trees, contradicting the assumption that hti and hi have the sameoptimal policy. Due to the assumption of optimality, qt+ki . At step t + k, for any given k,agent i can still find the optimal policy after merging ht(cid:7)ti because they share the same optimal policy qti must still share the same optimal policy qt+ki must be a sub-policy tree of qti and h(cid:7)ti . The theorem thus holds for all steps by induction. (cid:2)i and h(cid:7)t◦ hkii , history hti and hi and h◦ hkiiTheorem 3. When two histories of agent i are PE, they are also POE, meaning that at least one of their optimal policies is the same.Proof. The proof is based on the theorems in [38]. Suppose that histories htidentical extensions ht+kare also PE. Since ht+k, hoptimal action for ht+k, hboth for hti , hhave at least one equal optimal policy so they are also POE. (cid:2)(cid:7)t+k, hi(cid:7)t+ki(cid:7)t+ki(cid:7)ti are PE at step t. At any step t + k, the(cid:7)t, created by appending the same length-k action-observation sequence to the end of hti ,are PE, they have equal optimal Q-value function. And agent i will always select the samebased on the optimal Q-value function. Hence it is very easy to build a policy which is optimal(cid:7)ti by considering every possible extension with optimal actions. When two histories of agent i are PE, theyi , hi , hiiiHowever, when two histories of agent i are POE, they are not necessarily PE. For example, in the multi-agent tigerdomain, suppose that agent i opens the left door (OL) when the history is hi but it opens the right door (OR) if the history(cid:7)i . After the door is opened, the tiger’s position is reset. So agent i will follow the same optimal policy. Obviously, hi ◦{O L}is h◦ {O R}, but they are POE. Therefore, the POE condition is more general than the PE condition, P E ⊂ P O E.is not PE with hIn single-agent POMDP, the policy tree can be evaluated given a history, thus an agent can either compare the policy orvalue. Since multiple optimal policies may exist but have the same value, the value equivalent (VE) condition may be moregeneral, P O E ⊂ V E. However, when multiple agents are involved, only a joint policy with a joint history can be evaluated.It is not clear how to calculate the exact value of a local policy given the history of a single agent in DEC-POMDPs.(cid:7)iApproximate history merging.In DEC-POMDP offline planning, the POE condition facilitates sub-policy reuse. Reusing a policyfor several histories is the same as mapping several histories to a single policy. When handling a policy, this is referred toas reuse. It is called merging when managing histories. In the optimal bottom-up dynamic programming algorithm, the bestpolicies from the current to the last step are available. The number of policies is much lower than the number of possiblehistories because a small set of policies can be reused to build any complex policies in the next iteration. When consideringapproximate solutions, the condition of optimal policies can be relaxed. For example, consider a robot sent out to clean10 rooms in a building. There are three type of rooms: 5 office rooms, 3 meeting rooms and 2 restrooms. The robot onlyneeds to know how to clean these three types of rooms. In other words, the optimal policy for one type of room can bereused several times in the policy of cleaning the 10 rooms. In this scenario, the policy of cleaning one type of room is notnecessarily optimal. In fact, the basic idea of MBDP is to keep a fixed number of policies and reuse them approximatelywhen building the policies of the next iteration. The same observation can be applied to online planning as a way to mergehistories.Unfortunately, the optimal policy for each agent is not available during execution time. Finding the optimal policy isas hard as solving the entire problem. But we can approximate future polices using limited lookahead. A k-step lookaheadpolicy is a set of policy trees of depth-k, one for each agent. The k-step lookahead policy therefore can be evaluated bydecomposing the value function into an exact evaluation of the k-steps and a heuristic estimate of the remaining part. Then,we can define similarity by comparing the structure of depth-k trees. The k-step lookahead policy is generated by pre-computed heuristics. The POE condition is approximated by the similarity of the k-step lookahead policies. More precisely,two histories of agent i are POE when the k-step lookahead policies of them have similar structure. In this paper, we requirethat the depth-k policy trees be identical for them to be considered similar, but that can be generalized to other measuresof similarity.We present a way (Algorithm 4) to maintain a bounded size belief pool online and use it to coordinate the strategy ofthe team. Bounding the size of histories is important especially for online planning, where the planning time of each stepis limited. Clustering methods often have no guarantee to reach a desired size of belief pool. If the size is too large, thealgorithm may exceed the planning time and miss the action cycle. Even worse, without a policy there is no way to maintainthe belief pool. In real applications, this may cause damage to the system if no special recovery process is implemented.In our algorithm, we merge histories whenever they are POE and then randomly choose just one history per policy, so thenumber of histories retained is bounded by the number of the policies generated and the definition of similarity. At eachstep, heuristics are used to perform the k-step lookahead and create a fixed number of policies.F. Wu et al. / Artificial Intelligence 175 (2011) 487–511499Algorithm 4: Policy-Based History MergingInput: H t , Q t , π tforeach i ∈ I do← ∅i˜H t// Hi is a hash table indexed by qi .Hi(qi) ← ∅, ∀qi ∈ Q i// group histories based on the policy.foreach hi ∈ H ti do// get the policy of hi according to π tqi ← Select a policy according to π t// add hi to the hash table with key qi .Hi(qi) ← Hi(qi) ∪ {hi}i (qi|hi)i .// generate a new set of histories.foreach qi ∈ Q ti do// keep one history per policy.if Hi(qi) is not empty thenhi ← Select a history from Hi(qi) randomly˜H t← ˜H t∪ {hi}iiii// fill up the history set.while | ˜H t| doqi ← Select a policy from Q tif Hi(qi) is not empty then| < |Q tiii randomlyhi ← Select a history from Hi(qi) randomly˜H t← ˜H t∪ {hi}return ˜H tFig. 3. Communication model for two agents.3.3. Communicating when inconsistency arisesIn our online planning framework, communication will be initiated before the planning phase, as shown in Fig. 3. First,each agent will decide if communication is necessary then check if the resource is available. If communication is not needed,agents will continue to plan without communication. If communication is needed, but the resources are unavailable, theagents will postpone communication to the next step. Otherwise, they will sync their local information and do planningbased on the information received from teammates. The local information communicated is just each agent’s local observa-tion sequence between the last communication step and the current step. The resources include the local communicationdevice of the agents and the communication channel. All communication failures require re-communication at the next step.The decision to communicate is thus made whenever other agents initiate communication, communication is postponed inthe previous step, or inconsistency of the belief pool is detected.To verify the inconsistency of a belief pool, it will be straightforward if agents know the current state of the system.However, in the DEC-POMDP model, the state is unavailable online and each agent can receive only its own local observationat the execution time. Fortunately, agents’ local observation often provides partial information of the system state. Hencewe can detect the problem by examining any inconsistency between the belief pool and the local observation of the agent500F. Wu et al. / Artificial Intelligence 175 (2011) 487–511from the environment. Intuitively, when agent i tries some action and observes oi , the probability of which is less than (cid:8)according to the belief pool, it is likely that there is something wrong with the belief pool.Let hti denote agent i’s local history at step t, and oti be the local observation agent i receives from the environmentis the local history we maintain at step t, not the local action-observation sequence. We denoteat step t. Note that htiwith B(hti ) a set of joint beliefs with the history component hti in the pool.Definition 8 ((cid:8)-inconsistency). At time step t, the maintained belief pool Bt is said to be (cid:8)-inconsistent with agent i’s localobservation oti if(cid:13)(cid:6)max∀b,∀ot−is(cid:7)∈S(cid:2)(cid:5)o t|s(cid:7), (cid:5)aO(cid:3)(cid:6)(cid:2)Ps∈S(cid:14)(cid:3)b(s)(cid:7)|s, (cid:5)as< (cid:8)(10)∪ ot−i , ot−i∈×k(cid:15)=i Ωk andwhere (cid:5)a is a joint action based on (cid:5)o t and the joint policy computed at the previous step, (cid:5)o t = otib ∈ B(hti ).This definition is used to monitor inconsistency between agent i’s local history and observation, which provides anindication of history inconsistency in the pool. The threshold (cid:8) is determined by the structure of the observation function.If the uncertainty regarding the observation is small, (cid:8) should be small too. Note however that this rule cannot detect everyform of inconsistency in the belief pool and is only based on the current local observations. And when inconsistency isdetected, it only means that there is a high likelihood of a problem in the belief pool.The amount of communication is determined by both the observation structure and the heuristic. Intuitively, agents canmake the right decision as long as the belief pool contains a joint history that is close to the real joint history. However,agents cannot obtain the observations of the other agents at execution time, so it is impossible to know the real jointhistory. But they can check for inconsistency of the history pool based on their local information. If the pool is inconsistent,the agent can refresh the belief pool by communicating with the other agents and synchronizing the observation sequence.After synchronization, the belief pool contains only the real joint history and is consistent.Unlike most approaches which require instantaneous communication, our approach allows agents to postpone commu-nication when the resource is unavailable. They can sacrifice some value and make decisions without communication. Therole of communication is therefore to improve performance when it is possible. When communication fails for a long time,state-of-the-art approaches continue to make decisions using either open-loop methods which totally ignore the local ob-servations [44,52] or local-greedy methods which lead to miscoordination due to the different local information of eachagent. Obviously, the open-loop or local-greedy policies can become arbitrarily poor given a problem with sufficiently longhorizons. Our approach computes a joint policy conditioned on the local observation of each agent. The common joint policyensures that each agent will take coordinated actions while utilizing its local information. Hence agents can coordinate theirbehavior even without communication.It is worthwhile to point out that our concept of belief inconsistency is fundamentally different from the idea of beliefdivergence [60]. The belief divergence approach first establishes a reference point, which is the belief of agents when theylast synchronized their knowledge. Then it compares the current belief state with that reference point. The distance betweenthe belief points is measured by KL-divergence. If the divergence reaches some threshold, the agents communicate with eachother to synchronize their observation sequences. This approach assumes that the other agents do not receive independentlyany new observations and that their beliefs remain static after the last communication step [60]. In contrast, our approachkeeps updating the joint beliefs given the possible observations of the other agents. Belief inconsistency is detected whenthe agent’s local observation does not match the projected joint belief.To summarize, our approach has the unique ability to postpone communication and at the same time take into accountnew local observations and maintain coordination. While other approaches also allow (or could be easily modified to al-low) postponement of communication, the result is a significant degradation in performance. Specifically, in Dec-Comm,this leads to open-loop operation discarding local observations. In approaches that rely on belief divergence, postponingcommunication leads to a greater divergence of beliefs and increases miscoordination.3.4. Implementation considerationsAs mentioned earlier, we only try one-step lookahead in our implementation. So the policy for each agent is just aone-node policy tree associated with a certain action. When storing the history, we do not need to save the whole action-observation sequence in the belief pool. We only need to assign an index to a history and use a hash table ht = (cid:3) (cid:5)θ , bt(cid:4) torepresent the joint history, where (cid:5)θ = (cid:3)θ1, . . . , θn(cid:4), θi is the history index for agent i and b is the joint belief induced for thejoint history by Eq. (1). Since we keep only one history for one policy, the history index for agent i can be represented as atuple θi = (cid:3)qt−1i is the observation of the current step.is the policy tree index of the previous step and ot(cid:4) where qt−1, otiiiF. Wu et al. / Artificial Intelligence 175 (2011) 487–511501Fig. 4. Example of history expansion and updating. The joint history (cid:3) (cid:5)θ0, b0(cid:4) with two components (cid:3)q1, ∗(cid:4) and (cid:3)q2, ∗(cid:4) in the pool is expanded to(cid:3) (cid:5)θ1, b1(cid:4), (cid:3) (cid:5)θ2, b2(cid:4), (cid:3) (cid:5)θ3, b3(cid:4), (cid:3) (cid:5)θ4, b4(cid:4) by assigning all possible joint observations. Agent 1 gets the observation o1 from the environment and updates its lo-cal history from (cid:3)q1, ∗(cid:4) to (cid:3)q1, o1(cid:4). Agent 2 gets the observation o2 and updates its local history from (cid:3)q2, ∗(cid:4) to (cid:3)q2, o2(cid:4).Fig. 5. Example of history merging and policy execution. The histories (cid:3)q2, o1(cid:4) and (cid:3)q2, o2(cid:4) in the pool map to the same policy q5, so only one history israndomly selected (e.g. (cid:3)q2, o1(cid:4)) for the next step and its index is updated to (cid:3)q5, ∗(cid:4). (cid:3)q1, o1(cid:4) maps to q3 and its updated index becomes (cid:3)q3, ∗(cid:4). (cid:3)q1, o2(cid:4)maps to q4 and its updated index is (cid:3)q4, ∗(cid:4). The policy for the local history of agent 1 (cid:3)q1, o2(cid:4) is q3, so agent 1 executes q3 and updates its local historyto (cid:3)q3, ∗(cid:4). Agent 2 executes q5 and updates its local history to (cid:3)q5, ∗(cid:4) since (cid:3)q2, o2(cid:4) maps to q5.For the one-step lookahead case, this index can be further simplified as θi = (cid:3)at−1to represent each element of the belief pool ht ∈ Ht isi, oti(cid:4). Therefore, the data structure we useht =(cid:15)(cid:15)(cid:15)qt−1, ot(cid:16) (cid:17)(cid:18) (cid:19)11θ1(cid:16)(cid:20), . . . ,(cid:20), qt−1, ot(cid:16) (cid:17)(cid:18) (cid:19)22θ2(cid:17)(cid:18)(cid:5)θ(cid:20)(cid:20)(cid:20), bt(cid:15)qt−1, ot(cid:16) (cid:17)(cid:18) (cid:19)nnθn(cid:19)At every step, we update each index as well as the joint belief state. Fig. 4 shows how to expand histories and updatethe index of the agent’s local history with its observation. Fig. 5 shows how to merge histories and update the index ofthe agent’s local history after executing a policy. Note that the new history is created by appending first a new action andthen a new observation to the end of a previous history. The history index without observation is represented by (cid:3)q, ∗(cid:4) inthe figures for the intermediate histories during the update process, hi ◦ ai , that are missing an observation. It is possible todesign different types of indices and keep more than one history for each policy if needed.i(cid:7)ii , oi , a1i , o2i , . . . , at−1i , oi(cid:4), where qtis the policy agent i executes and oi(cid:4) in the pool, we will change agent i’s local history index to (cid:3)qtEach agent’s own local history used for execution is also represented by an index. At each step, we will update theindex of the agent’s own local history using (cid:3)qtis the observationreceived from the environment at the current step t. If the history indexed by (cid:3)qti , oi(cid:4) is merged and represented by another(cid:7)(cid:4). Hence we always map the agent’s localhistory (cid:3)qti , oihistory to a history in the pool. For the purpose of communication, agent i also stores an action-observation sequence(cid:3)a0i , o1To summarize, we developed new data structures to implement the belief pool as well as each agent’s local history.Instead of storing all the observation-action sequences, we use indices to represent histories. Each index of a history (cid:3)qi, oi(cid:4)contains two parts: a pointer to agent i’s policy, qi and the current local observation oi . In Fig. 4, we show how to expandthe belief pool by every joint observation and how to update each agent’s local history by its current observation receivedfrom the environment. In Fig. 5, we show how the expanded histories are merged and how each agent’s local historyis updated after executing a new policy. These two figures illustrate the key operations using the new representation ofindices in our implementation.(cid:4) that includes the actions executed and observation received., otii502F. Wu et al. / Artificial Intelligence 175 (2011) 487–5114. Experimental resultsWe have implemented and tested MAOP-COMM using three standard benchmark problems and a more challenging prob-lem called grid soccer. In each of these environments, we first solved the underlying (centralized) MDP and provided theresulting value function as a heuristic to our algorithm. The reported results are averages over 20 runs of the algorithm oneach of the problems. We present the average accumulated reward (Reward), average online runtime per step (Time(s)), andaverage percentage of communication steps (Comm(%)) with different horizons (Horizon). While communication is limitedand minimizing it is an important goal, we did not add an explicit cost for communication because any such cost wouldhave been arbitrary and not particularly relevant to these applications. The main purpose of the experiments is to testwhether high-valued plans can be computed quickly on-line, while using little communication. Specifically, our goal wasto achieve significantly better value with significantly less communication compared to the state-of-the-art. MAOP-COMMwas implemented in Java and ran on a 2.4 GHz Intel Core 2 Duo processor with 2 GB of RAM. Linear programs were solvedusing lp_solve 5.5 with Java wrapper. All timing results are CPU times with a resolution of 0.01 second.We did try to compare MAOP-COMM with the two existing online planners that use communication (i.e., BaGA-Command Dec-Comm), but only Dec-Comm with particle filtering (Dec-Comm-PF) can solve the benchmark problems we used.As mentioned earlier, the main reason for this limitation is that BaGA-Comm and the exact version of Dec-Comm do notbound the size of histories (or beliefs). In our experiments, we observed that agents often kept silent for 10 steps ormore. Consequently, the number of possible joint histories becomes very large (e.g., 52×10 for 2 agent problem with 5observations after 10 steps without communication). Even the BaGA-Cluster approach could not reduce such pool of historiesto a manageable size in the test domains. BaGA-Comm and the exact version of Dec-Comm ran out of memory and timevery quickly. Therefore, we compared MAOP-COMM with Dec-Comm-PF, the only existing algorithm that bounds the amountof memory.In fact, BaGA-Comm and Dec-Comm yield similar performance (average values and amount of communication) in mostdomains which are tractable for both of them. Another fact pointed out by [44] is that Dec-Comm using an exact treerepresentation of joint beliefs and Dec-Comm-PF that approximates beliefs using sufficiently large particle filters provide nosubstantial difference in performance. Therefore, comparing our algorithms to Dec-Comm-PF – the leading communicativeonline algorithm which is applicable to all the test problems – presents the best way to assess the benefits of our approach.To put these results in perspective and better understand the role of communication, we also include the results forFULL-COMM – the case of full communication (communicating observations at each step, (cid:8) = +∞), and MAOP – our ownonline approach with no communication (no inconsistency monitoring, (cid:8) = 0). The monitoring threshold for MAOP-COMMwas set to (cid:8) = 0.01 and the number of particles for Dec-Comm-PF was 100. According to our experiments, using more than100 particles resulted in no substantial difference in value but an obvious increase in runtime for Dec-Comm-PF. We didnot use a discount factor in these experiments because all the tested problems involve a finite horizon.It is important to emphasize that the FULL-COMM strategy is expected to outperform approaches that use partial com-munication. Nonetheless, we provide the results of FULL-COMM to establish an upper bound for our online algorithm.Although perfectly reliable and instantaneous communication is generally unrealistic, it is interesting to show how well theonline communication approach performs when it uses the same MDP heuristic. In our implementation, FULL-COMM runsas a POMDP with joint actions and observations. The joint action selected at each step is based on a one-step lookaheadusing the MDP heuristic. It takes very little time because the main computation is the Bayesian update of only one jointbelief state. We do not consider the time for transmitting observations among agents. As discussed earlier, the FULL-COMMstrategy simply reduces a DEC-POMDP to a POMDP, which is much easier to solve. Our goal in these experiments is toshow that our proposed approach for online planning with bounded communication is effective, when compared with theFULL-COMM upper bound.4.1. Perfectly reliable communication channelOur initial set of experiments involves a communication channel that is perfectly reliable; it is assumed to be alwaysavailable and without noise. We relax these assumptions in the following section.Standard benchmark problems. The first set of experiments involves four standard benchmark problems: Broadcast Chan-nel [11], Meeting in a Grid [11], Cooperative Box Pushing [46] and Stochastic Mars Rover [4]. These benchmarks have beenwidely used to evaluate cooperative multi-agent planning algorithms modeled as DEC-POMDPs.1 Because the number ofpossible histories is extremely large and bounding the size of histories is one of our key contributions, we used benchmarkproblems with larger observation sets. Other well-known benchmark problems such as Multi-Agent Tiger [32], RecyclingRobots [2] and Fire Fighting [35] have only 2 observations.The Broadcast Channel problem [11] is a simplified two agent networking problem. At each time step, each agent mustchoose whether or not to send a message. If both agents send messages, there is a collision and neither gets through. Thisproblem has 4 states, 2 actions and 5 observations. The results in Table 2 show that in this problem all the methods achieved1 Original domain descriptions are available for download from the DEC-POMDP repository: http://users.isr.ist.utl.pt/~mtjspaan/decpomdp/index_en.html.F. Wu et al. / Artificial Intelligence 175 (2011) 487–511503Table 2Benchmark results (20 trials).HorizonAlgorithmBroadcast channel: |S| = 4, | Ai | = 2, |Ωi | = 520100MAOPMAOP-COMMDec-Comm-PFFULL-COMMMAOPMAOP-COMMDec-Comm-PFFULL-COMMMeeting in a 3×3 Grid: |S| = 81, | Ai | = 5, |Ωi | = 720100MAOPMAOP-COMMDec-Comm-PFFULL-COMMMAOPMAOP-COMMDec-Comm-PFFULL-COMMCooperative box pushing: |S| = 100, | Ai | = 4, |Ωi | = 520100MAOPMAOP-COMMDec-Comm-PFFULL-COMMMAOPMAOP-COMMDec-Comm-PFFULL-COMMStochastic Mars Rover: |S| = 256, | Ai | = 6, |Ωi | = 820100MAOPMAOP-COMMDec-Comm-PFFULL-COMMMAOPMAOP-COMMDec-Comm-PFFULL-COMMRewardTime (s)Comm (%)18.2518.3518.4518.9089.9590.3590.090.603.103.352.904.7515.3017.1014.9024.707.5099.30136.75222.50−16.0441.95296.50880.5018.1946.1945.0261.4151.15222.86133.04325.04< 0.01< 0.01< 0.01< 0.01< 0.01< 0.01< 0.01< 0.010.150.260.22< 0.010.190.300.24< 0.010.140.160.35< 0.010.130.130.36< 0.010.970.052.46< 0.012.200.092.39< 0.010.00.00.0100.00.00.00.0100.00.011.070.50100.00.012.1078.20100.00.011.5083.50100.00.012.2659.87100.00.017.022.0100.00.018.0035.00100.0similar values with runtime less than 0.01 seconds. Both MAOP-COMM and Dec-Comm-PF initiated no communication. Theperformance without communication was almost the same as the case of full communication. These results have a simpleintuitive explanation. The probability that an agent’s buffer will fill up on the next step is 0.9 for one agent and 0.1 for theother. Therefore, it is easy to coordinate in this case by simply giving one agent a higher priority.In the Meeting in a Grid problem [11], two robots navigate on a grid with no obstacles. The goal is for the robots tospend as much time as possible in the same location. In order to make the problem more challenging, we used larger3×3 grid and simulated a noisy sensor with a 0.9 chance to perceiving the right observation. This problem has 81 states,since each robot can be in any of 9 squares at any time. Each robot has 5 actions and 7 legal observations for sensinga combination of walls around. The results in Table 2 show that MAOP – the online algorithm without communication –performed surprisedly well in this case. The one-step lookahead provided a good heuristic for this problem because agentscan meet anywhere and the problem resets after that. The results for FULL-COMM show that agents do not benefit muchfrom communication. MAOP-COMM achieved a higher value than Dec-Comm-PF, but with much less communication. Theruntimes of MAOP-COMM, MAOP and Dec-Comm-PF were short and quite close to each other.In the Cooperative Box Pushing domain [46], two agents located on a 3×4 grid are required to push boxes (two small andone large box) into a goal area. The agents benefit from cooperation because when they cooperatively push the large box intothe goal area they get a very high reward. In order to make the problem more challenging, we have the agents transition to arandom state when the problem resets itself. We also included uncertain observations in this domain with a 0.9 probabilityfor the right observation and a 0.025 probability for the others. This domain has 100 states with 4 goal states and 96 non-goal states. Each agent has 4 actions and 5 observations. The results in Table 2 show that in this domain communicationdid improve performance significantly. MAOP without communication performed poorly. The one-step lookahead was nolonger a good heuristic because agents in this domain have multiple goals (large box or small box). For this domain withlonger horizons such as 100, MAOP-COMM outperformed Dec-Comm-PF, again with much less communication. MAOP andMAOP-COMM ran a little faster than Dec-Comm-PF.504F. Wu et al. / Artificial Intelligence 175 (2011) 487–511Fig. 6. The 3×3 grid soccer domain with 1 opponent and 2 teammates.Table 3Grid soccer results (20 trials).HorizonGrid soccer 2×3: |S| = 3843, | Ai | = 6, |Ωi | = 11Algorithm20100MAOPMAOP-COMMDec-Comm-PFFULL-COMMMAOPMAOP-COMMDec-Comm-PFFULL-COMMGrid soccer 3×3: |S| = 16,131, | Ai | = 6, |Ωi | = 1120100MAOPMAOP-COMMDec-Comm-PFFULL-COMMMAOPMAOP-COMMDec-Comm-PFFULL-COMMReward180.50290.6129.50373.901157.801933.901441.601933.60190.70296.00271.40356.0803.601679.501044.401808.20Time (s)Comm (%)0.250.281.36< 0.010.140.161.28< 0.011.902.3015.26< 0.011.962.509.48< 0.010.014.8033.5100.00.015.4030.80100.00.027.059.0100.00.026.9070.70100.0The Stochastic Mars Rover [4] is a larger problem with 256 states and 2 agents. Each agent has 5 actions and 8 ob-servations. The original problem has deterministic observations. We made it more challenging by introducing uncertaintyinto the observations. Each agent observes with 0.9 probability what is really in front of it, and with 0.025 probability anyother observation. The results in Table 2 show that communication is critical in this domain as well. Without communica-tion, MAOP gets less value than MAOP-COMM – the communicating version. Again, for the longer horizon, MAOP-COMMproduces much higher value than Dec-Comm-PF with much less communications.To summarize, MAOP-COMM performed very well in all the benchmark problems using much less communication thanDec-Comm-PF. In some domains such as Broadcast Channel and Meeting in a Grid, MAOP could also achieve very high valuewithout any communication. Although the tested horizon was only up to 100, our approach can solve problems with muchlarger horizons since we bound the size of histories at each step. The experimental results varied a little with the horizonbecause in problems with longer horizons there is a greater chance for miscommunication and error accumulation. Theparameter (cid:8) presents a good way to tradeoff between the amount of communication and overall value. In domains such asCooperative Box Pushing, a larger (cid:8) would allow more communication and consequently improve performance.The grid soccer domain. To demonstrate scalability, we also tested our algorithm on a more challenging problem called gridsoccer. Shown in Fig. 6, the domain includes two agents for one team and one opponent for the other team. Each agent has4 possible orientations (up, down, left or right). The opponent – with full observation and reliable actions – always executesa fixed policy and tries to approach the ball as fast as possible. If the opponent bumps into an agent with the ball, it willget the ball and the game terminates with a reward of −50. If the agent with the ball enters the goal grid, the game alsoterminates with a reward of 100. Each agent has 6 actions: north, south, east, west, stay and pass. Each action has a 0.9probability of success and 0.1 probability of having no impact on the current state. When an agent executes a pass action,the ball is transferred to the other agent on the next step, if and only if the other agent executes the stay action at thesame time. Otherwise, the ball goes out of the field and the game terminates with a reward of −20. For each step resultingin a non-terminal state, there is a penalty of 2. After reaching a terminal state the problem is reset. Each agent gets one outof 5 possible observations describing the situation in front of it (free, wall, teammate, opponent, goal) and 2 observationsindicating who controls the ball. Thus, the total number of observations is 11. The observation is noisy with a 0.9 chanceto perceive the correct observation and a 0.01 chance to perceive each of the other observations. We tested our algorithmon two grid soccer problems: one is a 2×3 grid with 3843 states, and the other is a 3×3 grid with 16,131 states. Theseproblems are the largest tackled so far by decision-theoretic algorithms for multi-agent planning.F. Wu et al. / Artificial Intelligence 175 (2011) 487–511505Fig. 7. Result of accumulated rewards with different thresholds.The results are shown in Table 3. MAOP-COMM achieved higher value than Dec-Comm-PF, while the performance ofMAOP is competitive, indicating that MAOP-COMM with a smaller (cid:8) would use even less communication and produce goodvalue. The runtimes of MAOP-COMM and MAOP were almost ten times faster than Dec-Comm-PF. One reason is that theoperators in MAOP-COMM and MAOP are much cheaper than the particle filtering used in Dec-Comm-PF. Another reason isthat the number of histories kept by MAOP-COMM and MAOP is much smaller than the number of particles used by Dec-Comm-PF. MAOP-COMM and MAOP scaled well in the 3×3 instance. The most state-sensitive operator was the Bayesianupdate. For a problem with 16,131 states, the update loop takes hundreds of seconds. Fortunately, the transition functionsare sparse in many real applications including grid soccer, allowing us to optimize the Bayesian update in all the algorithmsincluding Dec-Comm-PF. Incidentally, in problems with larger state spaces, the runtime advantage of keeping less historiesbecame more significant.4.2. Imperfect communication channelIn many real-world applications, communication could be unreliable due to noise or poor reception. For example, whena wireless communication network is used, connectivity could be intermittent and messages may have to be retransmittedseveral times. Our work allows communication to be postponed when the communication channel is not available. Whilecommunication could be postponed in other approaches, the downside is more severe. Dec-Comm [44], for example, willsimply ignore the agents’ local observations and run open-loop policies. The belief divergence approach [60] will makeincorrect assumptions about the other agents’ beliefs and run greedy policies, which are likely to be uncoordinated. Incontrast, when communication is postponed, our approach still computes a conditional plan that takes into considerationall the possible observations. As discussed in Section 3, this conditional plan remains coordinated for all the agents.In this section, we present experimental results for the cooperative box-pushing domain with an imperfect commu-nication channel. We simulated intermittent communication by drawing a random variable from a uniform distributioneach time communication is initiated. If the value is greater than some threshold, the communication channel is available;otherwise, it is unavailable. We varied the threshold from 0.0 to 1.0 and measured the accumulated rewards and percent-age of communication averaged over 20 runs. Communication is always available when the threshold is 0.0 and there isno communication allowed when the threshold is 1.0. We show the results of two MAOP-COMM variants: MAOP-COMM-POSTPONE, which postpones communication until the channel is available, and MAOP-COMM-DROP, which simply drops thecommunication attempt when the channel is not available.We show the results of accumulated rewards in Fig. 7 and the results of percentage of communication in Fig. 8 withdifferent thresholds. As expected, the reward and amount of communication goes down in MAOP-COMM-DROP when theprobability of unavailable communication is high. It means that these communication steps are truly critical for the co-operative box pushing domain and simply dropping them will gradually decrease the rewards. Interestingly, the rewardsand amount of communication grow for a certain range of threshold values when we postpone communication until it isavailable.Interestingly, as the threshold starts growing (from 0 to 0.4), the agents implemented by MAOP-COMM-POSTPONE com-municate a little bit more but also get better value. The value then declines as the threshold continue to grow. We havetried to analyze and explain this phenomenon, which seems counterintuitive. There are several possible reasons contributingto this. When communication is postponed, agents still need to make decisions without communication. The inconsistentbelief pool becomes more and more uncertain as time goes on. The actions computed based on these uncertain beliefs willintroduce some randomness. And because the value function is based on a heuristic that overestimates future performance,506F. Wu et al. / Artificial Intelligence 175 (2011) 487–511Fig. 8. Result of percentage of communication with different thresholds.this randomness can have a positive effect on performance. Another possible explanation for problems with multiple goals isthat agents may change their current goals after the communication. Once an agent initiates communication, all the agentsin the team will be forced to refresh their belief pool. Since the heuristic is only an estimate of future value, changinggoals too frequently based on the heuristic may not be desirable. Therefore agents may lose value if they communicate andchange their undergoing goal too often. Consider, for example, the cooperative box pushing domain. Suppose that Agent 1is close to the small box and decides to push it by itself based on the current belief pool. At the same time, Agent 2 is closeto the large box and initiates communication, which forces Agent 1 to refresh its belief pool. Based on the new belief pooland the heuristic, the two agents establish a new goal and decide to push the large box together. However, the new goalcomputed by the overestimating heuristic may not be achievable while the old goal of Agent 1 could be achieved withinthe limited horizon. Thus, the agents could lose value by incorrectly estimating their abilities and abandoning current goals.This is also the reason that when to communicate, not the frequency of communication is more important [20].We also compared our algorithms with Dec-Comm when the communication channel is imperfect. As the results show,MAOP-COMM-POSTPONE performs better than Dec-Comm with higher value and less communication. Interestingly, thevalue of Dec-Comm also grows when a certain amount of uncertainty about the reliability of the communication channelis introduced. Overall, these experiments confirm the advantage of MAOP-COMM-POSTPONE in domains with imperfectcommunication, which is an important factor in real-world applications.5. Related workThe literature on planning and communication in DEC-POMDPs or equivalent models can be generally divided into worksthat compute full offline policies and those that do not. In this section, we only describe related work where explicitcommunication is involved. More general surveys of DEC-POMDP solution methods have been recently published by Seukenand Zilberstein [48], Oliehoek et al. [35] and Bernstein et al. [10].5.1. Offline planning with communicationOne group of solution methods determines the entire plan and communication strategy offline, before plan executionstarts. The stored plan is then used at runtime. The COM-MTDP model [41] and DEC-POMDP-COM model [25], which haveequivalent complexity [48], provide theoretical frameworks for reasoning about communication offline. The COM-MTDPmodel offers a framework for analyzing the optimality of team performance and the computational complexity of theagents’ decision problem. In terms of optimality analysis, it is able to encode existing teamwork theories and models, basedon joint intentions and STEAM [57], and provide a novel algorithm which outperforms these earlier coordination strategies.The DEC-POMDP-COM model formalizes the problem for a given communication language and semantics, and examines thevalue of an optimal policy of action and communication with different cost models.Pynadath and Tambe’s work focuses on the optimality and complexity analysis of various communication models. Theoptimal solution they provide is not particularly useful in practice due to the complexity result – NEXP-complete in the gen-eral communication case. Besides, the COM-MTDP communication policies used in the joint intention instantiations requirethe designer to specify a joint persistent goal and allow an agent to send a message when the goal has been achieved [41].The DEC-POMDP-COM model applies to the more general problem where the agents are allowed to communicate more thanonce and optimize the timing and frequency of communication. Both models require a priori semantics for the communi-cated messages. The semantics define the type of messages as well as the situations in which agents will communicate.F. Wu et al. / Artificial Intelligence 175 (2011) 487–511507Assuming a message set Σ with user-defined, fixed semantics simplifies the optimization problem, but makes the modeldesign more difficult and domain-dependent.Spaan et al. [50] established a new model where messages are sent as part of agents’ action vectors and received in thenext time step as part of the recipients’ observation vectors. In contrast to the COM-MTDP and DEC-POMDP-COM models,it does not require an explicit communication language, but instead treats the semantics of communication as part of theoptimization problem. They also present an iterative method for computing a joint policy for the team in a decentralizedfashion, treating communication as an integral part of the reasoning process. Given a set of fixed policies for all agentsbut agent i, the policy computation process converts the DEC-POMDP into a POMDP from i’s perspective, which factors theexpected contribution to the joint team reward of the policies of the other agents. The communication policy is based ona heuristic method in which information entropy is defined over policies and states. However, the model is restricted totransition independent agents where each agent’s observation only depends on an agent’s local state.Generally, reasoning about communication offline requires the enumeration of all possible messages and their effecton the team. Unfortunately, the number of these messages grows exponentially and is as large as the set of all possibleobservation histories. The COMMUNICATIVE DP-JESP technique integrates a communication strategy into K -step componentsof the JESP algorithm and finds a Nash equilibrium of policies for multiple agents [31]. The JESP (Joint Equilibrium-BasedSearch for Policies) approach tries to find the policy that maximizes the joint expected reward for one agent at a time,keeping the policies of the other agents fixed. the process is repeated until an equilibrium is reached [32]. DP-JESP isa dynamic programming approach to reason about an agent’s policy in conjunction with its teammates. The multi-agentbelief state defined in this approach is a distribution over the current state as well as the observation history of the otheragents. Since the agent does not know exactly what observations the other agents have received at runtime, the algorithmallows the agents to periodically synchronize their own observation histories to reduce the likelihood of undesired behavior.In order to keep the algorithm tractable, it uses a fixed communication decision, which enforces a rule that communicationmust occur at least every K steps. Thus no policy can be indexed by an observation history of length greater than K . It alsouses the sync model but does not assume a separate communication phase. That is, in each decision cycle, an agent caneither choose to communication or act.Using information value theory, the value of communication can be defined as the net gain from communicating, whichis the difference between the expected improvement in the agents’ performance due to communication and the costsassociated with communication. When the value of communication is greater than certain threshold, agents can benefit fromthe communication by sharing their local information. However, computing the exact value of communication is intractableespecially in multi-agent systems where communication is constrained and each agent has different partial informationabout the overall situation. Under the myopic assumption – that communication is only possible at the present time –it is possible to estimate efficiently the value of communication. Becker et al. [7] studied the implications of the myopicassumptions and developed a myopic communication strategy for transition-independent DEC-MDPs. Carlin and Zilberstein[18] extended the work to more general DEC-POMDP models and further improved the performance with non-myopicreasoning. Both works use the sync model of communication and determine the communication strategy offline.5.2. Online planning with communicationOnline approaches provide an important alternative in which the decision when and what to communicate occurs atexecution time. The approaches most similar to ours are Bayesian Game Approximation with Communication (BaGA-Comm)[20] and Avoids Coordination Errors by reasoning over Possible Joint Beliefs with Communication (ACE-PJB-Comm) [42], alsoknown as Dec-Comm [44].In the BaGA-Comm framework, a series of smaller Bayesian games are constructed and solved using BaGA [21] to gener-ate policies and joint-type spaces at each time-step. An alternating-maximization algorithm is used to find locally optimalsolutions for each Bayesian game. To guarantee that each agent has sufficient information to independently construct thesame game, a large type space, which corresponds to possible joint histories, is maintained by each agent. In order to exertsome control over the size of the type space, a followup method, BaGA-Clustering [22], introduced two types of clustering:Low Probability Clustering (LPC) and Minimum Distance Clustering (MDC). LPC removes the clusters with low probability bymerging each one with its nearest remaining neighbor. MDC repeatedly finds the most similar pair of clusters and mergesthem. Both LPC and MDC use the worst-case expected loss as the similarity measure. Although clustering methods mayalleviate the exponential growth of histories in some domains, they do not generally bound the number of histories keptin memory. In the worst case, the size of histories still grows exponentially when limited clustering is possible. Besides,clustering methods are often time-consuming. To further improve the performance, several communication strategies areused to share information among agents. The authors present three types of communication strategies: a fixed policy, anExpected Value Difference (EVD) policy and an approach based on Policy Difference (PD). The experimental evaluation ofthese method showed that EVD and PD result in similar performance and number of communication acts, and are muchbetter than the fixed policy approach. This work has demonstrated that it is important to decide when – not just howoften – an agent should communicate to achieve good performance [20].Unlike BaGA-Cluster, which merges histories based on their similarity in terms of the worst-case expected loss, wemerge histories based on the similarity of the future policy structures. Moreover, our approach bounds the size of thehistories in memory at each step while BaGA-Cluster does not. Thus, our algorithm can solve problems in which agents508F. Wu et al. / Artificial Intelligence 175 (2011) 487–511may not communicate for a long period of time, while BaGA-Comm becomes intractable very quickly when the state andobservation spaces are large. For communication, Emery-Montemerlo also uses the sync model. But rather than transmittinglocal observations, agents broadcast their current type to the team. However, the type information may not be sufficientlyaccurate after the lossy clustering procedure is applied.In the Dec-Comm framework, each agent maintains a distribution of possible joint beliefs and chooses to communicateonly when integrating its own observation history into the joint belief causes a change in the joint action selected by theQPOMDP heuristic function. This work emphasizes the challenge of avoiding coordination error by reasoning over possiblejoint beliefs. However, the number of possible joint beliefs often grows rapidly beyond what is feasible to store in memory.To address this, Roth et al. utilize a fixed-size method for modeling the distribution of possible joint beliefs using particlefiltering. The approach requires only a fixed amount of memory due to the use of sampling. Nevertheless, in many domains,the number of particles needed to accurately model joint beliefs may be large. With the particle representation, agentsmay initiate too many communications when the real joint belief is not sampled. Our approach can better address thesesituations because it initiates communication when history inconsistency is detected. Basically, communication in Dec-Commis based on the PD policy, which initiates communication when the policies before and after communication are different.Additionally, Roth et al. use the tell model for communication, which allows some subset of agents to broadcast their mes-sages to others. Subsequent work has also addressed the question of what to communicate, which selects the most valuablesubset of observations from an agent’s observation history, instead of using the entire set as the broadcast message [43].5.3. Other relevant work on communicationSeveral different aspects of communication within special cases of DEC-POMDPs have been studied in recent years. Rothet al. [45] proposed an algorithm to generate decentralized policies with minimal communication for factored DEC-MDPs.It employs techniques for solving factored MDPs to generate the centralized, free-communication plan for the team offline.At the runtime, each agent executes a factored policy by traversing the policy tree, choosing branches according to thevalues of the state variables that it encounters, until it reaches an action at a leaf. Communication is needed to facilitate theexecution of those portions of the policy without context-specific independence. It utilizes the query communication model,where each agent asks its teammates for information when needed.Spaan and Melo [51] introduced interaction-driven Markov games (IDMGs), assuming communication only occurs atinteraction states. It explicitly distinguishes between situations in which agents should interact and situations in whichthey can act independently. In non-interaction states, each agent chooses its individual actions based on a simple heuristicapproach that completely disregards the existence of other agents. In interaction states, each agent communicates its currentindividual state to the other agents. Then each agent computes a specific Nash equilibrium of the corresponding matrix-game and chooses actions accordingly. Communication is assumed to be unlimited and noise-free.Williamson et al. [59] introduced the dec_POMDP_Valued_Com model, which adds a special communication reward func-tion to DEC-POMDPs. The impact of any communication is measured using KL Divergence – the difference in information inan agent’s belief state with and without communication. They extended an online approach called Real Time Belief SpaceSearch (RTBSS) to generate the policy. Early work on RTBSS relied on extensive domain knowledge to encode explicitly howthe agents should coordinate, assuming that communication is some parallel activity to other actions [39]. The online ap-proach of Williamson et al. [59] is quite similar to the Dec-Comm algorithm, except that the former algorithm only uses thelocal observations to update the joint belief states. This work relies on a hand-tuned parameter to value communication asa weighted sum of the communication reward and the original reward. To overcome this, the RS_dec_POMDP model approx-imates this valuation by shaping the reward based on belief divergence [60]. Like the dec_POMDP_Valued_Com model, theRS_dec_POMDP model still requires a domain-dependent communication reward function, provided by the designer, as anadd-on to the general DEC-POMDP model. Most importantly, the belief update in their work is domain-dependent. In gen-eral DEC-POMDPs, agents must consider the joint belief space, which takes into account the behaviors of the other agents.This joint belief space blows up exponentially because there are so many choices and outcomes of policies for the otheragents, and each agent can only obtain its own observation. Hence we use a policy-based history merging technique tobound memory usage. But for specific domains such as RoboCup rescue, it is possible to design an ad-hoc belief monitoringmechanism so that the algorithm only maintains a single belief state at each step of agents’ online decision-making [59].Oliehoek et al. [34] use a QBG value function [37] to find communication policies for domains where communicationshave a one-step delay. They model the problem using Bayesian Games and adapt Persues, an approximate POMDP solver,to compute QBG value functions. More recent work extends this approach to handle stochastic delays [52]. They present amodel which allows communication to be delayed by one or more time steps and explicitly considers future probabilitiesof successful communication. The Q value function is exact when the communication delays are at most one time step.In situations with delays longer than one time step, agents take coordinated decisions based on some open-loop method.Unlike our work, this approach does not consider when to communicate.There are many other ways to use communication in multi-agent systems. Stone and Veloso [53] utilize the low-bandwidth communication to do real-time task decomposition and dynamic role assignment. Xuan [61] consider com-munication in DEC-MDPs whenever an agent notices ambiguity in what it should plan next. Shen et al. [49] formulate theDEC-MDP with a two-layer Bayesian network and find the near-optimal communication strategy for problems with a givenstructure. Goldman et al. [24] address the problem of potential misbehavior resulting from misinterpretation of messagesF. Wu et al. / Artificial Intelligence 175 (2011) 487–511509exchanged, and establish a formal framework to identify a collection of properties that allow agents to interpret what oth-ers are communicating. Some researchers have managed to learn communication policies using reinforcement learning insettings that do not require a complete model to be known [23,54]. Most of these approaches focus on fully-observablemulti-agent domains and are based on reinforcement learning, which relies on many trials [16]. The major benefit of com-munication is to improve coordination since each independent learner may have different models online.6. ConclusionsWe present a new online algorithm for planning under uncertainty in multi-agent settings with bounded communication.The algorithm addresses the key challenge of keeping the team of agents coordinated by maintaining a shared pool of histo-ries that allows agents to choose local actions and detect inconsistency when it arises. The algorithm has several importantadvantages. First, it can use communication very selectively to recover from inconsistency. It can also delay communicationwhen the resource is not available and – if needed – avoid communication for a long period of time. A second advantageis scalability. The algorithm can solve existing benchmark problems much faster than the best offline algorithms and it cansolve larger problems that are beyond the scope of offline DEC-POMDP planners. Finally, the algorithm performs very wellin practice, outperforming the best existing online method by producing better value with less communication.The performance of online planning is highly dependent on the quality of the value function which guides the behaviorof agents when interacting with the environment. The optimal value function is not available at execution time becausecalculating it may take as much computation time as solving the entire problem. But it can be approximated by someheuristic functions such as the Q-value function of the underlying MDP. The effectiveness of such heuristics is, however,domain dependent. In multi-agent settings, it is challenging to find heuristics that take into account the interdependenceamong the agents. That is, the actions of one agent may affect the observations of other agents. This is an implicit form ofcommunication, which may be critical for coordination. How to capture this type of information by the heuristic function isan important and interesting research direction for multi-agent online planning.As discussed earlier, it is intractable to keep all the possible histories of the team. Therefore we introduce a technique forbounding the usage of memory when maintaining the belief pool. Obviously, any such bounding of memory will introduceerror into the pool, thereby making the pool inconsistent with the real situation. To the best of our knowledge, we arethe first to address this type of inconsistency by explicit communication among agents. The algorithm monitors the beliefpool at each step and refreshes the pool by sharing the private information of agents when inconsistency is detected byany of the agents. It is worth mentioning that the effectiveness of our detection method depends on the precision of theagents’ observations. We verify the belief pool using each agent’s local observation at each step. If the observation is verynoisy, it is hard to tell whether the detected inconsistency is due to problems with the belief pool or merely uncertaintiesin the local observation. In many applications, the sensor data should be accurate enough to detect inconsistencies of thebelief pool. If not, the agents may keep silent without exceeding the usage of communication resources. In this paper, weuse communication to share local information among agents. Another interesting type of communication is to negotiate theteam’s policy among the agents. This type of negotiation is nontrivial and beyond the scope of this paper.More broadly, this paper tries to draw the attention of the AI community to online methods as a viable alternative formulti-agent decision-theoretical planning. We show that online planning with limited communication can perform quitewell, while taking much less time. Surprisingly, work on this topic has been sparse [21,22,43,44]. Our work contributes tothe literature by presenting a complete framework of multi-agent online planning with two new methods for bounding theusage of both memory and communication resources; it explores several promising research directions for planning andlearning in multi-agent systems.AcknowledgementsSpecial thanks to Maayan Roth for sharing her source code of Dec-Comm and to the reviewers for their helpful feedbackand suggestions. This work was supported in part by the China Scholarship Council, the Air Force Office of Scientific Researchunder Grant No. FA9550-08-1-0181, the National Science Foundation under Grant No. IIS-0812149, the National ScienceFoundation of China under Grant No. 60745002, and the National High-tech Project of China under Grant No. 2008AA01Z150.References[1] I. Akyildiz, W. Su, Y. Sankarasubramaniam, E. Cayirci, Wireless sensor networks: a survey, Computer Networks 38 (4) (2002) 393–422.[2] C. Amato, D.S. Bernstein, S. Zilberstein, Optimizing memory-bounded controllers for decentralized POMDPs, in: Proceedings of the 23rd Conference onUncertainty in Artificial Intelligence, 2007, pp. 1–8.[3] C. Amato, J.S. Dibangoye, S. Zilberstein, Incremental policy generation for finite-horizon DEC-POMDPs, in: Proceedings of the 19th International Con-ference on Automated Planning and Scheduling, 2009, pp. 2–9.[4] C. Amato, S. Zilberstein, Achieving goals in decentralized POMDPs, in: Proceedings of the 8th International Joint Conference on Autonomous Agentsand Multi-Agent Systems, 2009, pp. 593–600.[5] R. Aras, A. Dutech, F. Charpillet, Mixed integer linear programming for exact finite-horizon planning in decentralized POMDPs, in: Proceedings of the17th International Conference on Automated Planning and Scheduling, 2007, pp. 18–25.[6] R. Becker, A. Carlin, V. Lesser, S. Zilberstein, Analyzing myopic approaches for multi-agent communication, Computational Intelligence 25 (1) (2009)31–50.510F. Wu et al. / Artificial Intelligence 175 (2011) 487–511[7] R. Becker, V.R. Lesser, S. Zilberstein, Analyzing myopic approaches for multi-agent communication, in: Proceedings of the 2005 IEEE/WIC/ACM Interna-tional Conference on Intelligent Agent Technology, 2005, pp. 550–557.[8] R. Becker, S. Zilberstein, V.R. Lesser, Decentralized Markov decision processes with event-driven interactions, in: Proceedings of the 3rd InternationalJoint Conference on Autonomous Agents and Multi-Agent Systems, 2004, pp. 302–309.[9] R. Becker, S. Zilberstein, V.R. Lesser, C.V. Goldman, Solving transition independent decentralized Markov decision processes, Journal of Artificial Intelli-gence Research 22 (2004) 423–455.[10] D.S. Bernstein, C. Amato, E.A. Hansen, S. Zilberstein, Policy iteration for decentralized control of Markov decision processes, Journal of Artificial Intelli-gence Research 34 (2009) 89–132.[11] D.S. Bernstein, E.A. Hansen, S. Zilberstein, Bounded policy iteration for decentralized POMDPs, in: Proceedings of the 19th International Joint Conferenceon Artificial Intelligence, 2005, pp. 1287–1292.[12] D.S. Bernstein, S. Zilberstein, N. Immerman, The complexity of decentralized control of Markov decision processes, in: Proceedings of the 16th Confer-ence on Uncertainty in Artificial Intelligence, 2000, pp. 32–37.[13] A. Beynier, A. Mouaddib, A polynomial algorithm for decentralized Markov decision processes with temporal constraints, in: Proceedings of the 4thInternational Joint Conference on Autonomous Agents and Multiagent Systems, 2005, pp. 963–969.[14] A. Beynier, A. Mouaddib, An iterative algorithm for solving constrained decentralized Markov decision processes, in: Proceedings of the 21st NationalConference on Artificial Intelligence, 2006, pp. 1089–1094.[15] C. Boutilier, Planning, learning and coordination in multiagent decision processes, in: Proceedings of the 6th Conference on Theoretical Aspects ofRationality and Knowledge, 1996, pp. 195–210.[16] L. Busoniu, R. Babuska, B.D. Schutter, A comprehensive survey of multiagent reinforcement learning, IEEE Transactions on Systems, Man, and Cybernet-ics, Part C: Applications and Reviews 38 (2) (2008) 156–172.[17] A. Carlin, S. Zilberstein, Value-based observation compression for DEC-POMDPs, in: Proceedings of the 7th International Joint Conference on Au-tonomous Agents and Multi-Agent Systems, 2008, pp. 501–508.[18] A. Carlin, S. Zilberstein, Myopic and non-myopic communication under partial observability, in: Proceedings of the 2009 IEEE/WIC/ACM InternationalConference on Intelligent Agent Technology, 2009, pp. 331–338.[19] J.S. Dibangoye, A. Mouaddib, B. Chaib-draa, Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs, in: Proceedings of the8th International Joint Conference on Autonomous Agents and Multi-Agent Systems, 2009, pp. 569–576.[20] R. Emery-Montemerlo, Game-theoretic control for robot teams, Doctoral Dissertation, Robotics Institute, Carnegie Mellon University, August 2005.[21] R. Emery-Montemerlo, G.J. Gordon, J.G. Schneider, S. Thrun, Approximate solutions for partially observable stochastic games with common payoffs, in:Proceedings of the 3rd International Joint Conference on Autonomous Agents and Multi-Agent Systems, 2004, pp. 136–143.[22] R. Emery-Montemerlo, G.J. Gordon, J.G. Schneider, S. Thrun, Game theoretic control for robot teams, in: Proceedings of the 2005 IEEE InternationalConference on Robotics and Automation, 2005, pp. 1163–1169.[23] M. Ghavamzadeh, S. Mahadevan, Learning to communicate and act using hierarchical reinforcement learning, in: Proceedings of the 3rd InternationalJoint Conference on Autonomous Agents and Multi-Agent Systems, 2004, pp. 1114–1121.[24] C.V. Goldman, M. Allen, S. Zilberstein, Learning to communicate in a decentralized environment, Autonomous Agents and Multi-Agent Systems 15 (1)(2007) 47–90.[25] C.V. Goldman, S. Zilberstein, Optimizing information exchange in cooperative multi-agent systems, in: Proceedings of the 2nd International JointConference on Autonomous Agents and Multi-Agent Systems, 2003, pp. 137–144.[26] C.V. Goldman, S. Zilberstein, Decentralized control of cooperative systems: Categorization and complexity analysis, Journal of Artificial IntelligenceResearch 22 (2004) 143–174.[27] E.A. Hansen, D.S. Bernstein, S. Zilberstein, Dynamic programming for partially observable stochastic games, in: Proceedings of the 19th National Con-ference on Artificial Intelligence, 2004, pp. 709–715.[28] M.L. Littman, A.R. Cassandra, L.P. Kaelbling, Learning policies for partially observable environments: Scaling up, in: Proceedings of the 12th InternationalConference on Machine Learning, 1995, pp. 362–370.[29] J. Marecki, M. Tambe, On opportunistic techniques for solving decentralized Markov decision processes with temporal constraints, in: Proceedings ofthe 6th International Joint Conference on Autonomous Agents and Multiagent Systems, 2007, pp. 825–832.[30] A.C. Morris, D. Ferguson, Z. Omohundro, D. Bradley, D. Silver, C. Baker, S. Thayer, W. Whittaker, W.R.L. Whittaker, Recent developments in subterraneanrobotics, Journal of Field Robotics 23 (1) (2006) 35–57.[31] R. Nair, M. Tambe, M. Roth, M. Yokoo, Communication for improving policy computation in distributed POMDPs, in: Proceedings of the 3rd InternationalJoint Conference on Autonomous Agents and Multiagent Systems, 2004, pp. 1098–1105.[32] R. Nair, M. Tambe, M. Yokoo, D.V. Pynadath, S. Marsella, Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings,in: Proceedings of the 18th International Joint Conference on Artificial Intelligence, 2003, pp. 705–711.[33] R. Nair, P. Varakantham, M. Tambe, M. Yokoo, Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs, in:Proceedings of the 20th National Conference on Artificial Intelligence, 2005, pp. 133–139.[34] F.A. Oliehoek, M.T.J. Spaan, N. Vlassis, Dec-POMDPs with delayed communication, in: The 2nd Workshop on Multi-agent Sequential Decision-Makingin Uncertain Domains, 2007.[35] F.A. Oliehoek, M.T.J. Spaan, N. Vlassis, Optimal and approximate q-value functions for decentralized POMDPs, Journal of Artificial Intelligence Re-search 32 (2008) 289–353.[36] F.A. Oliehoek, M.T.J. Spaan, S. Whiteson, N.A. Vlassis, Exploiting locality of interaction in factored Dec-POMDPs, in: Proceedings of the 7th InternationalJoint Conference on Autonomous Agents and Multiagent Systems, 2008, pp. 517–524.[37] F.A. Oliehoek, N. Vlassis, Q-value functions for decentralized POMDPs, in: Proceedings of the 6th International Joint Conference on Autonomous Agentsand Multiagent Systems, 2007, pp. 833–840.[38] F.A. Oliehoek, S. Whiteson, M.T.J. Spaan, Lossless clustering of histories in decentralized POMDPs, in: Proceedings of the 8th International Joint Confer-ence on Autonomous Agents and Multi-Agent Systems, 2009, pp. 577–584.[39] S. Paquet, L. Tobin, B. Chaib-draa, An online POMDP algorithm for complex multiagent environments, in: Proceedings of the 4th International JointConference on Autonomous Agents and Multi-Agent Systems, 2005, pp. 970–977.[40] L. Peshkin, K.-E. Kim, N. Meuleau, L.P. Kaelbling, Learning to cooperate via policy search, in: Proceedings of the 16th Conference on Uncertainty inArtificial Intelligence, 2000, pp. 489–496.[41] D.V. Pynadath, M. Tambe, The communicative multiagent team decision problem: Analyzing teamwork theories and models, Journal of Artificial Intel-ligence Research 16 (2002) 389–423.[42] M. Roth, Execution-time communication decisions for coordination of multi-agent teams, Ph.D. thesis, The Robotics Institute, Carnegie Mellon Univer-sity, 2007.[43] M. Roth, R. Simmons, M. Veloso, What to communicate? Execution-time decision in multi-agent POMDPs, in: Proceedings of the 8th InternationalSymposium on Distributed Autonomous Robotic Systems, 2006.[44] M. Roth, R.G. Simmons, M.M. Veloso, Reasoning about joint beliefs for execution-time communication decisions, in: Proceedings of the 4th InternationalJoint Conference on Autonomous Agents and Multiagent Systems, 2005, pp. 786–793.F. Wu et al. / Artificial Intelligence 175 (2011) 487–511511[45] M. Roth, R.G. Simmons, M.M. Veloso, Exploiting factored representations for decentralized execution in multiagent teams, in: Proceedings of the 6thInternational Joint Conference on Autonomous Agents and Multiagent Systems, 2007, pp. 457–463.[46] S. Seuken, S. Zilberstein, Improved memory-bounded dynamic programming for decentralized POMDPs, in: Proceedings of the 23rd Conference inUncertainty in Artificial Intelligence, 2007, pp. 344–351.[47] S. Seuken, S. Zilberstein, Memory-bounded dynamic programming for DEC-POMDPs, in: Proceedings of the 20th International Joint Conference onArtificial Intelligence, 2007, pp. 2009–2015.[48] S. Seuken, S. Zilberstein, Formal models and algorithms for decentralized decision making under uncertainty, Journal of Autonomous Agents andMulti-Agent Systems 17 (2) (2008) 190–250.[49] J. Shen, V.R. Lesser, N. Carver, Minimizing communication cost in a distributed bayesian network using a decentralized mdp, in: Proceedings of the2nd International Joint Conference on Autonomous Agents and Multi-Agent Systems, 2003, pp. 678–685.[50] M.T.J. Spaan, G.J. Gordon, N. Vlassis, Decentralized planning under uncertainty for teams of communicating agents, in: Proceedings of the 5th Interna-tional Joint Conference on Autonomous Agents and Multiagent Systems, 2006, pp. 249–256.[51] M.T.J. Spaan, F.S. Melo, Interaction-driven Markov games for decentralized multiagent planning under uncertainty, in: Proceedings of the 7th Interna-tional Joint Conference on Autonomous Agents and Multiagent Systems, 2008, pp. 525–532.[52] M.T.J. Spaan, F.A. Oliehoek, N. Vlassis, Multiagent planning under uncertainty with stochastic communication delays, in: Proceedings of the 18thInternational Conference on Automated Planning and Scheduling, 2008, pp. 338–345.[53] P. Stone, M.M. Veloso, Task decomposition, dynamic role assignment, and low-bandwidth communication for real-time strategic teamwork, ArtificialIntelligence 110 (2) (1999) 241–273.[54] D. Szer, F. Charpillet, Improving coordination with communication in multi-agent reinforcement learning, in: Proceedings of the 6th IEEE InternationalConference on Tools with Artificial Intelligence, 2004, pp. 436–440.[55] D. Szer, F. Charpillet, Point-based dynamic programming for DEC-POMDPs, in: Proceedings of the 21st National Conference on Artificial Intelligence,2006, pp. 1233–1238.[56] D. Szer, F. Charpillet, S. Zilberstein, Maa*: A heuristic search algorithm for solving decentralized POMDPs, in: Proceedings of the 21st Conference onUncertainty in Artificial Intelligence, 2005, pp. 576–590.[57] M. Tambe, Towards flexible teamwork, Journal of Artificial Intelligence Research 7 (1997) 83–124.[58] J. Tsitsiklis, M. Athans, On the complexity of decentralized decision making and detection problems, IEEE Transaction on Automatic Control 30 (1985)440–446.[59] S.A. Williamson, E.H. Gerding, N.R. Jennings, A principled information valuation for communication during multi-agent coordination, in: The 3rd Work-shop on Multi-agent Sequential Decision-Making in Uncertain Domains, 2008.[60] S.A. Williamson, E.H. Gerding, N.R. Jennings, Reward shaping for valuing communications during multi-agent coordination, in: Proceedings of the 8thInternational Joint Conference on Autonomous Agents and Multiagent Systems, 2009, pp. 641–648.[61] P. Xuan, V. Lesser, S. Zilberstein, Communication decisions in multi-agent cooperation: Model and experiments, in: Proceedings of the 5th InternationalConference on Autonomous Agents, 2001, pp. 616–623.