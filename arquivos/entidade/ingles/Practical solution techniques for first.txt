Artificial Intelligence 173 (2009) 748–788Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintPractical solution techniques for first-order MDPs ✩Scott Sanner a,∗, Craig Boutilier ba Statistical Machine Learning Group, National ICT Australia, Canberra, ACT, 0200, Australiab Department of Computer Science, University of Toronto, Toronto, ON M5S 3H5, Canadaa r t i c l ei n f oa b s t r a c tArticle history:Received 21 October 2007Received in revised form 4 November 2008Accepted 9 November 2008Available online 24 November 2008Keywords:MDPsFirst-order logicPlanningin the number of domain objects and exponentialMany traditional solution approaches to relationally specified decision-theoretic planningproblems (e.g., those stated in the probabilistic planning domain description language,or PPDDL) ground the specification with respect to a specific instantiation of domainobjects and apply a solution approach directly to the resulting ground Markov decisionprocess (MDP). Unfortunately, the space and time complexity of these grounded solutionapproaches are polynomialin thepredicate arity and the number of nested quantifiers in the relational problem specification.An alternative to grounding a relational planning problem is to tackle the problem directlyat the relational level. In this article, we propose one such approach that translates anexpressive subset of the PPDDL representation to a first-order MDP (FOMDP) specificationand then derives a domain-independent policy without grounding at any intermediate step.However, such generality does not come without its own set of challenges—the purpose ofthis article is to explore practical solution techniques for solving FOMDPs. To demonstratethe applicability of our techniques, we present proof-of-concept results of our first-orderapproximate linear programming (FOALP) planner on problems from the probabilistic trackof the ICAPS 2004 and 2006 International Planning Competitions.Crown Copyright © 2008 Published by Elsevier B.V. All rights reserved.1. IntroductionThere has been an extensive line of research over the years aimed at exploiting structure in order to compactly repre-sent and efficiently solve decision-theoretic planning problems modeled as Markov decision processes (MDPs) [12]. Whiletraditional approaches from operations research typically use enumerated state and action models [62], these have provedimpractical for large-scale AI planning tasks where the number of distinct states in a model can easily exceed the limits ofprimary and secondary storage on modern computers.Fortunately, many MDPs can be compactly described by using a factored state and action representation and exploitingvarious independences in the reward and transition functions [12]. The independencies and regularities laid bare by suchrepresentations can often be exploited in exact and approximate solution methods as well. Such techniques have permittedthe practical solution of MDPs that would not have been possible using enumerated state and action models [22,36,38,75].However, factored representations are only one type of structure that can be exploited in the representation of MDPs.Many MDPs can be described abstractly in terms of classes of domain objects and relations between those domain objectsthat may change over time. For example, a logistics problem specified in the probabilistic planning domain description✩Parts of this article appeared in preliminary form in [S. Sanner, C. Boutilier, Approximate linear programming for first-order MDPs, in: Uncertainty inArtificial Intelligence (UAI-05), Edinburgh, Scotland, 2005, pp. 509–517; S. Sanner, C. Boutilier, Practical linear evaluation techniques for first-order MDPs,in: Uncertainty in Artificial Intelligence (UAI-06), Boston, MA, 2006].* Corresponding author.E-mail address: ssanner@nicta.com.au (S. Sanner).0004-3702/$ – see front matter Crown Copyright © 2008 Published by Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.003S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788749language (PPDDL) [89] may refer to domain objects such as boxes, trucks, and cities. If the objective is to deliver all boxesto their assigned destination cities then the locations of these boxes and trucks may change as a result of actions takenin pursuit of this objective. Since action templates such as loading or unloading a box are likely to apply generically todomain objects and can be specified independently of any ground domain instantiation (e.g., 4 trucks, 5 boxes, and 9 cities),this permits compact MDP descriptions by exploiting the existence of domain objects, relations over these objects, and theability to express objectives and action effects using quantification.Unfortunately, while relational specifications such as PPDDL permit very compact, domain-independent descriptions ofa variety of MDPs, this compactness does not translate directly to effective solutions of the underlying planning problems.For example, one approach to solving a relational decision-theoretic planning problem might first construct sets of statevariables and actions for all possible ground instantiations of each relation and action with respect to a specific domain(e.g., 4 trucks, 5 boxes, and 9 cities). Then this approach might apply known solution techniques to this ground factoredrepresentation of an MDP. Unfortunately, such an approach is domain-specific; and the size of the ground MDP growspolynomially in the number of domain objects, and exponentially in the predicate arity and the number of nested quantifiersin the problem specification. For sufficiently large domains and complex relational MDP specifications, grounding may notbe a viable option.An alternative approach to grounding is to apply a solution approach directly at the relational level. In this article,we discuss one such technique that translates an expressive subset of the relational PPDDL representation to a first-orderMDP (FOMDP) [14] specification. A symbolic policy may then be derived with respect to this FOMDP, resulting in a domain-independent solution that exploits a purely lifted version of the Bellman equations and avoids grounding at any intermediatestep. This stands in contrast to alternate first-order approaches discussed in Section 6.2 that induce symbolic representationsof the solution from samples of the Bellman equation in ground problem instances.Unfortunately, the use of first-order logical languages to describe our FOMDP specification and solution introduces theneed for computationally expensive logical simplification and theorem proving. While this means that exact solutions arenot tractable for many FOMDPs, there is often a high degree of regularity and structure present in many FOMDPs that canbe exploited by the approximate (heuristic) solution techniques proposed in this article. To this end, this article continuesthe tradition of exploiting structure to find effective solutions for large MDPs.After providing a review of MDPs and relevant solution techniques in Section 2 and the FOMDP formalism and itssolution via symbolic dynamic programming [14] in Section 3, we make the following contributions to the practical solutionof FOMDPs:(1) Section 3.2.2: We show how to translate a subset of PPDDL problems including universal and conditional effects toFOMDPs.(2) Section 4.1: We show how to exploit the logical structure of reward, value, and transition functions using first-orderextensions of algebraic decision diagrams (ADDs) [4] for use in both exact and approximate FOMDP solutions.(3) Section 4.2: We apply additive decomposition techniques to universal reward specifications in a manner that leads toefficient solutions for our FOMDP representation and reasonable empirical performance on example problems.(4) Section 5.3: We show how to generalize the approximate linear programming technique for MDPs [19,36,72] to the caseof FOMDPs by casting the optimization problem in terms of a first-order linear program.(5) Section 5.4: We define a linear program (LP) with first-order constraints and provide a constraint generation algorithmthat utilizes a relational generalization of variable elimination [91] to exploit constraint structure in the efficient solutionof this first-order LP (FOLP).To demonstrate the efficacy of our techniques, we present proof-of-concept results of our first-order approximate linearprogramming (FOALP) planner on problems from the probabilistic track of the ICAPS 2004 and 2006 International Plan-ning Competitions in Section 5.6. Following this, we discuss a number of related first-order decision-theoretic planningapproaches and discuss the relative advantages and disadvantages of each in Section 6. We conclude with a discussion ofpossible extensions to our techniques in Section 7.2. Markov decision processesMarkov decision processes (MDPs) were first introduced and developed in the fields of operations research and eco-nomics [6,41,73]. The MDP has since been adopted as a model for decision-theoretic planning with fully observable state inthe field of artificial intelligence [7,8,12] and as such provides the formal underpinning for the framework that we describein this article. In this section, we describe various algorithmic approaches for making optimal sequential decisions in MDPsthat we later generalize to the case of first-order MDPs. The following presentation derives from Puterman [62].2.1. The MDP model and optimality criteriaFormally, a finite state and action MDP is specified by a tuple (cid:3)S, A, T , R, h, γ (cid:4). S is a set of distinct states. An agentin an MDP can effect changes to its state by executing actions from the set A. We base our initial presentation in thissection on finite state and action MDPs; but in much of what follows, we will assume an infinite, discrete state and action750S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788space. The standard techniques for MDPs discussed here can be generalized to countable or continuous state and actionspaces [62].The transition function T is a family of probability distributions T (s, a, s(cid:5)|a, s), which denotes the probability(cid:5)) = P (s(cid:5) ∈ S when action a ∈ A was executed. This representation enforces the Markovthat the world transitions from s ∈ S to sproperty: the distribution over states st+1 at time t + 1 is independent of any previous state st−i and action at−i , i (cid:2) 1,given st and at .The preferences of the agent are encoded in a reward function R : S × A → R. In addition to specifying single-steppreferences, the agent must also specify how it trades off reward over the horizon h of remaining decision stages. In thisarticle, we focus on the expected sum of discounted accumulated reward over an infinite horizon (h = ∞) since this ismost compatible with the (approximate) linear programming approach that we adopt later. In the calculation of discountedaccumulated reward, we discount rewards t time steps into the future by a discount factor γ t where γ ∈ [0, 1]. Throughoutthis article, we assume γ < 1. The use of γ < 1 allows one to model the notion that an immediate reward r is worth morethan the equivalent reward delayed one or more time steps in the future. Practically, γ < 1 is required to ensure that thetotal expected reward is bounded in the case of infinite horizon MDPs.A stationary policy takes the form π : S → A, with π (s) denoting the action to be executed in state s. The value ofpolicy π is the expected sum of discounted future rewards over horizon h given that π is executed. Its value function isgiven by:(1)V π (s) = Eπ(cid:2)(cid:4)h(cid:3)t=0γ t · rt | s0 = s(cid:5)πV (s) = arg maxa∈AR(s, a) + γ(cid:3)s(cid:5)∈S(cid:6)(cid:6)where rt is a reward obtained at time t, γ is a discount factor as defined above, and s0 is the initial starting state.A greedy policy πV with respect to a value function V is simply any policy that takes an action in each state thatmaximizes expected value with respect to V , defined as follows:P (s(cid:5)|s, a)V (s(cid:5))(2)Thus, from any value function, we can derive a corresponding greedy policy that represents the best action choice withrespect to that value estimation.An optimal policy π ∗any greedy policy with respect to the optimal value function Voptimal policy, V π ∗ (s) = V∗(s). We note that V∗satisfies the following fixed-point equality:in an infinite horizon MDP maximizes the value function for all states. An optimal policy π ∗isand likewise the optimal value function is the value of an∗T (s, a, s(cid:5)) · V∗(cid:5))(s(3)(cid:5)∗V(s) = maxaR(s, a) + γ(cid:3)s(cid:5)∈S∗Finding Vdenote some attempt at approximating V∗constitutes finding an exact solution to an MDP. Throughout the article, we use the term solution more loosely to, whether the approximation guarantees error bounds or is simply heuristic.2.2. MDP solution algorithmsIn this section we describe several exact and approximate solution techniques for MDPs that we later extend to thefirst-order case.2.2.1. Value iterationWe begin our discussion of MDP solutions by providing two equations that form the basis of the stochastic dynamicprogramming algorithms used to solve MDPs.We define V 0π (s) = R(s, π (s)) and then inductively define the t-stage-to-go value function for a policy π as follows:π (s) = R(s, π (s)) + γV tT (s, π (s), s(cid:5)) · V t−1π (s(cid:5))(4)Based on this definition, Bellman’s principle of optimality [6] establishes the following relationship between the optimal valuefunction at stage t and the optimal value function at the previous stage t − 1:(cid:5)V t(s) = maxa∈AR(s, a) + γ(cid:6)T (s, π (s), s(cid:5)) · V t−1(s(cid:5))(5)The computation of V t from V t−1 via this relationship is referred to as a Bellman backup. The value iteration algorithmconsists of repeatedly performing Bellman backups to compute these t-stage-to-go value functions.We note that the Bellman backup is often rewritten in the following two steps to separate out the backup of a valuefunction through a single action and the maximization of this value over all actions:(cid:3)s(cid:5)∈S(cid:3)s(cid:5)∈SS. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Q t(s, a) = R(s, a) + γ ·V t(s) = maxa∈A(cid:7)(cid:8)Q t(s, a)(cid:3)s(cid:5)∈ST (s, a, s(cid:5)) · V t−1(s(cid:5))Puterman [62] shows that terminating once the following Bellman error condition is met(cid:9)(cid:9)(cid:9) <(cid:9)V t(s) − V t−1(s)maxs(cid:4)(1 − γ )2γ751(6)(7)(8)guarantees that the estimated value function V t is (cid:4)-optimal over an infinite horizon, that is, its value is within (cid:4) of theoptimal value: maxs |V t(s) − V∗(s)| < (cid:4).We note that the value iteration approach requires time polynomial in the backup depth d and the number of states andactions, i.e., O (|S|2 · |A| · d). Puterman [62] provides a proof that value iteration converges linearly.2.2.2. Linear programmingAn MDP can also be solved using the following linear program (LP):Variables:V (s), ∀s ∈ S(cid:3)Minimize:V (s)s∈SSubject to: 0 (cid:2) R(s, a) + γ(cid:3)s(cid:5)∈SP (s(cid:5)|s, a)V (s(cid:5)) − V (s); ∀s ∈ S, ∀a ∈ A(9)Puterman [62] provides a proof that the solution to this LP is the optimal value function for an MDP.2.2.3. Approximate linear programmingOne general and popular approximate solution technique for MDPs is that of linear-value function approximation [36,46,47,71,72,78]. Representing value functions as a linear combination of basis functions has many convenient computationalproperties, many of which will become evident as we incorporate relational structure in our MDP model. However, perhapsone of the most useful properties is that linear value function representations lead to MDP solutions requiring optimizationwith respect to linear objectives and linear constraints—that can be formulated as LPs.In an n-state MDP, the exact value function can be specified as a vector in Rn. This vector can be approximated by avalue function ˜V (cid:10)w that is a linear combination of k fixed basis functions (or n-vectors), denoted bi(s):˜V (cid:10)w (s) =k(cid:3)i=1w i · bi(s)(10)The linear subspace spanned by the basis set will generally not include the true value function, but one can use pro-jection methods to minimize some error measure between the true value function and the linear combination of basisfunctions. The basis functions themselves can be specified by domain experts, constructed or learned in an automated fash-ion (e.g., [61]; [51]). We will consider first-order methods for automated basis function construction in Section 5 and relatedwork in Section 6.Approximate linear programming (ALP) is simply an extension of the linear programming solution of MDPs to the casewhere the value function is approximated. In a linear value function representation, the objective and constraints will belinear in the weights being optimized, leading to a direct LP formulation. Consequently, we arrive at the following variantof the previous exact LP solution:Variables:Minimize:(cid:10)w(cid:3)s∈S˜V (cid:10)w (s)Subject to: 0 (cid:2) R(s) + γ(cid:3)s(cid:5)∈SP (s(cid:5)|s, a) ˜V (cid:10)w (s(cid:5)) − ˜V (cid:10)w (s); ∀s ∈ S, ∀a ∈ A(11)2.3. Selecting an MDP solution approachThe choice of whether to use a linear programming or dynamic programming solution to MDPs is not always clear. Linearprogramming offers a simple one-shot solution, but it relies on efficient LP solvers. Dynamic programming is straightforwardto implement, but may require a large number of iterations to converge. However, the choice of exact vs. approximateis almost invariably determined by the size of the state space. For sufficiently large state spaces, approximate solutiontechniques are the only viable option. But this last statement depends critically on how one measures the size of the statespace.752S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Fig. 1. An example BoxWorld problem. Trucks may drive along solid lines and planes may fly along dashed lines. The goal in this instance is to get allboxes in Paris (indicated by the star).Despite their promise, the exact and approximate solution techniques discussed above must represent the value function(and policy, if required) as vectors or functions over an explicitly enumerated state (and action) space. This is simply notfeasible for large-scale AI planning problems. Fortunately, there are many representations (e.g., factored or relational) wellsuited to decision-theoretic planning that do not require explicit state or action enumeration in either the problem repre-sentation or the solution. To this end, we will be concerned with the exploitation of relational planning structure for theremainder of this article.3. First-order MDPsGiven that relational representations seem natural for planning problems, it makes sense to attempt to exploit thisrelational structure at a first-order level without resorting to grounding. This is precisely the idea behind the first-order MDPmodel (FOMDP) and its symbolic dynamic programming solution [14], which we review in this section. For the remainderof this article, when we refer to a FOMDP without further qualification, we refer to the specific formalization presentedin [14], although there are other possible first-order MDP formalizations and associated solution approaches (we discussthese alternatives in Section 6). The reader already familiar with the motivations for FOMDPs and the presentation andnotation in [14] may wish to skip this section and proceed directly to the main contributions of this article in Sections 4and 5.3.1. MotivationBefore we introduce FOMDPs and their solution, we begin with the basics of relational planning problem specificationsand motivate the need for exploiting this structure at a lifted first-order level rather than at a ground propositional level.3.1.1. Relational planning specificationsWe assume basic familiarity with unsorted first-order logic with equality. While we use a sorted notation for specifyingobject types of variables and predicate slots, we assume this sort information is compiled into an unsorted logical formwhere ∀Sort : c φ(c) is rewritten as ∀c. Sort(c) ⊃ φ(c) and likewise ∃Sort : c φ(c) is rewritten as ∃c. Sort(c) ∧ φ(c). Assumingthese transformations, we draw on the logical notation and semantics for unsorted first-order logic given in [16]. Specifically:• Predicate Symbols: We assume a set of predicates P i of each arity 0 (cid:3) i (cid:3) m for some finite maximum m. We assume“=”∈ P 2 with its usual interpretation.• Function Symbols: We assume a set of function symbols f j of each arity 0 (cid:3) j (cid:3) n for some finite maximum n.In addition, we use a few notational conventions. All predicates (including unary predicates denoting domain object classes)are capitalized and all variables and constants are lowercased. We denote the types of predicate arguments using thenotation φ(Sort1, . . . , Sortk) for some predicate of arity k.11 Logically, this requires a background theory axiom ∀x1, . . . , xk φ(x1, . . . , xk) ⊃(cid:10)k1=1 Sorti (xi ) for each predicate φ(Sort1, . . . , Sortk).S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788753• Domain Object Types: Box, Truck, City• Relational (S)tate Descriptors (with parameter sorts):BoxIn(Box, City), TruckIn(Truck, City), BoxOn(Box, Truck)• (R)eward: if [∃Box : b.BoxIn(b, paris)] then 10 else 0• (A)ctions (with parameter sorts) and (T )ransition Function:◦ load(Box : b, Truck : t):– Effects (probability 0.9):∗ when [∃City : c. BoxIn(b, c) ∧ TruckIn(t, c)] then [BoxOn(b, t)]∗ ∀City : c. when [BoxIn(b, c) ∧ TruckIn(t, c)] then [¬BoxIn(b, c)]◦ unload(Box : b, Truck : t):– Effects (probability 0.9):∗ ∀City : c. when [BoxOn(b, t) ∧ TruckIn(t, c)] then [BoxIn(b, c)]∗ when [∃City : c. BoxOn(b, t) ∧ TruckIn(t, c)] then [¬BoxOn(b, t)]◦ drive(Truck : t, City : c):– Effects (probability 1.0)∗ when [∃City : c1. TruckIn(t, c1)] then [TruckIn(t, c)]∗ ∀City : c1. when [TruckIn(t, c1)] then [¬TruckIn(t, c1)]◦ noop– No effects.Fig. 2. A PPDDL-style representation of a simple variant of the BoxWorld problem. The deterministic PDDL subset would exclude the probabilistic annota-tions of effects assuming that all effects occur with probability 1.0.We can view many decision-theoretic planning problems as consisting of classes of domain objects and the changingrelations that hold between those objects at different points in time. For example, in the BoxWorld logistics problem [79]illustrated in Fig. 1, we have four classes of domain objects: Box, City, Truck, and Plane. For the relations that hold betweenthem, we have BoxIn(Box, City), BoxOnTruck(Box, Truck), TruckIn(Truck, City), PlaneIn(Plane, City), BoxOnPlane(Box, Plane)). Inthis framework, generic action templates such as loading or unloading a box from a truck or plane or driving trucks andflying planes between cities are likely to apply generically to domain objects and thus the planning problem can be specifiedindependently of any ground domain instantiation.One recent language for representing relational probabilistic planning problems is PPDDL [89]. At its core, PPDDL isa probabilistic extension of a subset of PDDL conforming to the deterministic ADL planning language [58]; ADL, in turn,introduced universal and conditional effects into the STRIPS representation [29]. To see the compactness of a relationalrepresentation, we provide a (P)PDDL representation of the BoxWorld problem in Fig. 2 where for simplicity, we omit thePlane class of objects and associated actions and relations and abbreviate BoxOnTruck(Box : b, Truck : t) as BoxOn(Box : b,Truck : t).General PPDDL specifications can be more compact for some problems than the PPDDL subset we refer to in this article.For example, in general PPDDL, universal and conditional effects and probabilities can be arbitrarily nested, thus allowingfor exponentially more compact representations of probabilistic action effects than can be achieved with probabilities onlyat the top-level of effects [66]. In addition, there are some general PPDDL specifications that cannot be translated to thePPDDL subset described here. If the general PPDDL specification uses probabilistic effects nested under universal effects (e.g.,each box falls off a truck with some independent probability), it is generally impossible to translate such a problem to therestricted PPDDL subset used here because it requires an indefinitely factored transition probability model that cannot beexpressed with finite probability specifications restricted to the top level of effects. While we do not discuss such model-expressivity here, we refer the reader to Sanner and Boutilier [70] and Chapter 6 of Sanner [67] for a treatment of suchissues in first-order MDPs.While the meaning of the PPDDL representation in Fig. 2 is intended to be relatively straightforward, there are a fewimportant points that should be explained. First, we assume that actions can be executed in all states so we do not encodeexplicit preconditions. While this assumption is not necessary, it does not have any effect on the value of an optimalpolicy in a domain that already has a noop action and it helps simplify our later notation. When an action executes, eachprobabilistic effect is realized independently according to the specified probability. For example, the unload action realizesits effects only 90% of the time, whereas the drive action deterministically realizes its effects on each execution.Probabilistic effects at the top-level of the effect specification consist of conjunctions of effects. Each individual effect canbe universal and conditional. Universal effects denoted by universally quantified variables in the then clause permit the effectto apply to an arbitrary number of objects not explicitly named in the action’s parameter list. Conditional effects denoted bywhen can be arbitrary first-order formulae specifying that the effects listed in the then clause hold in the post-action stateif the when conditions hold in the pre-action state. When universally quantified variables are shared between the when/thenclause pair, we refer to such effects as universal conditional. We note that each individual effect is only allowed to mentionone positive or negative relation in the then portion of the clause. A conjunction of then effects can be easily specifiedas multiple effects with the same when condition. Disjunctive (i.e., non-deterministic) effects are prohibited in PPDDL. Forexample, when the load(b, t) action is executed, its effects are realized with probability 0.9. When these effects are realized,then for any city c that satisfies BoxIn(b, c) ∧ TruckIn(t, c) in the pre-action state, BoxOn(b, t) ∧ ¬BoxIn(b, c) will hold in754S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788• Domain Object Instantiation:◦ Box = {box1, box2, box3},{paris, berlin, rome}• (S)tate-variable Atoms (i.e., binary state variables):Truck = {truck1, truck2},City =◦ BoxIn:{BoxIn(box1, paris), BoxIn(box2, paris), BoxIn(box3, paris),BoxIn(box1, berlin), BoxIn(box2, berlin), BoxIn(box3, berlin),BoxIn(box1, rome), BoxIn(box2, rome), BoxIn(box3, rome)}◦ TruckIn:{TruckIn(truck1, paris), TruckIn(truck1, berlin), TruckIn(truck1, rome),TruckIn(truck2, paris), TruckIn(truck2, berlin), TruckIn(truck2, rome)}◦ BoxOn:{BoxOn(box1, truck1), BoxOn(box2, truck1), BoxOn(box3, truck1),BoxOn(box1, truck2), BoxOn(box2, truck2), BoxOn(box3, truck2)}• (A)ctions:◦ load:{load(box1, truck1), load(box2, truck1), load(box3, truck1)load(box1, truck2), load(box2, truck2)}, load(box3, truck2)}◦ unload:{unload(box1, truck1), unload(box2, truck1), unload(box3, truck1),unload(box1, truck2), unload(box2, truck2)}, unload(box3, truck2)}◦ drive:{drive(truck1, paris), drive(truck1, berlin), drive(truck1, rome)drive(truck2, paris), drive(truck2, berlin), drive(truck2, rome)• (T )ransition Function:Follows directly from ground instantiation of PPDDL actions in Fig. 2.• (R)eward:if [BoxIn(box1, paris) ∨ BoxIn(box2, paris) ∨ BoxIn(box3, paris)] then 10 else 0Fig. 3. One possible ground MDP instantiation of the BoxWorld FOMDP.the post-action state since both effects have equivalent when conditions. When these effects are not realized on 10% of theload(b, t) executions, no state changes occur and it is equivalent to a noop action.One can easily see that this relationally specified domain-independent specification allows very compact MDP specifi-cations when compared to a corresponding ground factored MDP representation. For example, consider instantiating thePPDDL problem in Fig. 2 to the ground factored MDP representation in Fig. 3 where we assume a problem instance with adomain instantiation of three boxes, three cities, and two trucks. While this is a trivially small domain instantiation, we notethat its factored MDP representation requires 21 propositional atoms corresponding to over two million distinct states and18 distinct actions that can be executed in each state. And the reward, which uses existential quantification in the relationalPPDDL specification must be grounded to obtain the corresponding factored MDP representation. Clearly, for n objects, thegrounded factor for the formula ∃Box : b. BoxIn(b, paris) will contain |Box| state variables, but if the reward were changedto ∀City : c ∃Box : b. BoxIn(b, c), the ground reward representation would contain |Box| · |City| state variables—thus implying acombinatorial growth in the number of nested quantifiers.In general, the number of ground atoms for a factored MDP representation will scale linearly in the number of relations,exponentially in the arity of each relation (assuming more than one domain object), and polynomially in the number ofdomain objects that fill each relation argument. To see this, let us assume for simplicity that all object class instantiationshave k instances. Then a single unary relation would be represented by k ground atoms, a binary relation by k2 atoms, andan n-ary relation by kn atoms. Similarly, the size of the grounding of any quantified formula is exponential in the numberof nested quantifiers, linear in the number of relations, and exponential in the size of the domain object classes beingquantified. Assuming k instances for all object classes and q nested (non-vacuous) quantifiers over formulae containing rrelations, the resulting unsimplified ground representation of the formula would require rkq ground atoms.For sufficiently small predicate arities and levels of quantifier nesting (assuming these remain constant for a problemas the domain size varies), the space requirements for representing a ground MDP may be acceptable. Thus, if we haveadequate space to permit the grounding of a relational MDP to obtain a factored MDP and we have the time to find anoptimal solution to this factored MDP, then grounding gives us one approach to representing and solving relational MDPsfor specific domain instances. However we note that while solving MDPs exactly is known to be polynomial in the numberof states (see Section 2.2.2), the number of states is exponential in the number of ground atoms in a factored representation.This is Bellman’s [6] well-known curse of dimensionality and since the number of ground atoms is at least linear in domainsize, it implies that the exact solution methods discussed previously require time at least exponential in the domain size.This precludes the general possibility of exact solutions to grounded relational MDPs for all but the smallest domain sizes.While this suggests the use of approximation methods for solving grounded MDPs, there are useful lifted alternatives torepresenting and solving relational MDPs that we discuss next.S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788755• if (∃b.BoxIn(b, paris))∗, paris) ∧ BoxOn(b∗, t∗))then do noop (value = 100.00)• else if (∃b∗, tthen do unload(b• else if (∃b• else if (∃b, c, tthen do drive(t∗, c, tthen do load(b∗1, tthen do drive(t• else if (∃b, c∗) (value = 89.0)∗.TruckIn(t∗, t∗.BoxOn(b, t∗, paris) (value = 80.0)∗.BoxIn(b∗, t∗, c2.BoxIn(b, c∗, c∗) (value = 72.0)∗1) (value = 64.7)• else do noop (value = 0.0)∗) ∧ TruckIn(t, c)∗, c) ∧ TruckIn(t∗, c))∗1) ∧ TruckIn(t∗, c2))Fig. 4. A decision-list representation of the expected discounted reward value for an exhaustive partitioning of the state space in the BoxWorld problem.The optimal action is also shown for each partition where the optimal bindings of the action variables (denoted by a *) correspond to any binding satisfyingthose variable names in the state formula.3.1.2. Grounded vs. lifted solutionsIn contrast to the grounded approach to representing relational MDPs as factored MDPs, it is important to point out thatno matter how many domain objects there may be in an actual problem instance, the size of the PPDDL relational planningproblem specification in Fig. 2 remains constant. Consequently, this invites the following question: if we can avoid a domain-dependent blowup in the representation of a relational MDP as in PPDDL, can we avoid a domain-dependent blowup in itssolution too? Although we have yet to discuss the specifics of how we might find a domain independent solution to thisPPDDL representation, in Fig. 4 we provide an optimal domain-independent value function and its corresponding policy forthe relational PPDDL specification of the BoxWorld problem in Fig. 2 (using discount factor γ = 0.9).The key features to note here are the state and action abstraction in the value and policy representation that are affordedby the first-order specification and solution of the problem. That is, this solution does not refer to any specific set of domainobjects, say just C it y = {paris, berlin, rome}, but rather it provides a solution for all possible domain object instantiations. Andwhile the BoxWorld problem could not be represented as a grounded factored MDP for sufficiently large domain instanti-ations, much less solved, a domain-independent solution to this particular problem is quite simple and applies to domaininstances of any size due to the power of state and action abstraction afforded by the first-order logical representation.Thus, an alternative idea to grounding a relational MDP specification and solving it for a particular domain instance isto translate the PPDDL relational specification to a first-order MDP representation that is directly amenable to solutionsvia lifted symbolic dynamic programming. This approach obtains a solution that applies universally to all possible domaininstantiations and has a time complexity that is independent of domain size. As we will see, the power of this lifted style ofsolution is that it exploits the existence of domain objects, relations over these objects, and the ability to express objectivesand action effects using quantification.3.2. Situation calculus backgroundBefore we present the first-order MDP (FOMDP) formalism, we discuss the basics of the situation calculus, which in turnprovides the logical foundations for our FOMDP representation. We begin by describing the necessary background materialfrom the situation calculus and Reiter’s default solution to the frame problem [64] required to understand FOMDPs. Thisincludes a discussion of the basic ingredients of the situation calculus formulation: actions, situations, and fluents along withrelevant axioms (e.g., unique names for actions and domain-specific axioms). Next we introduce effect axioms and explainhow these can be derived from a PDDL specification. Then we show how effect axioms can be compiled into the successor-state axioms that underly the default solution to the frame problem of the situation calculus. We conclude by introducingthe regression operator Regr that will prove crucial to our symbolic dynamic programming solution to first-order MDPs.3.2.1. Basic ingredientsThe situation calculus is a first-order language for axiomatizing dynamic worlds [52]. Its basic language elements consistof actions, situations and fluents:• Actions: Actions are first-order terms consisting of an action function symbol and arguments. For example, an action forloading box b on truck t in the running BoxWorld example is represented by load(b, t).• Situations: A situation is a first-order term denoting a specific state. The initial situation is usually denoted by s0 andsubsequent situations resulting from action executions are obtained by applying the do function, do(a, s) representingthe situation resulting from executing action a in situation s. For example, the situation resulting from loading box b ontruck t in the initial situation s0 and then driving truck t to city c is given by the term do(drive(t, c), do(load(b, t), s0)).• Fluents: A fluent is a relation whose truth value varies from situation to situation. A fluent is simply a relation whoselast argument is a situation term. For example, imagine an initial state s0 in which fluent BoxOn(b, t, s0) is false, butfluents TruckIn(t, c, s0) and BoxIn(b, c, s0) are true. Then under the semantics of a deterministic version of the load(b, t)756S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788action (which we formally define in a moment), BoxOn(b, t, do(load(b, t), s0)) holds. We do not consider functionalfluents in this exposition, but they are easily added to the language without adverse computational side effects [64].3.2.2. From PDDL to a first-order logic domain theoryTo axiomatize a PDDL domain theory in first-order logic, we must first consider how to describe the effects and non-effects of actions. We can begin by describing positive and negative effect axioms that characterize how fluents change as aresult of actions. Note that in the following presentation, all relations that can change between states in PPDDL have beenrewritten as fluents with an extra situation term. In addition, we assume all axioms are implicitly universally quantified.• Positive Effect Axioms: positive effect axioms state which actions can explicitly make each fluent true; for example:[∃c. a = load(b, t) ∧ BoxIn(b, c, s) ∧ TruckIn(t, c, s)] ⊃ BoxOn(b, t, do(a, s))[∃t. a = unload(b, t) ∧ BoxOn(b, t, s) ∧ TruckIn(t, c, s)] ⊃ BoxIn(b, c, do(a, s))[∃c1. a = drive(t, c) ∧ TruckIn(t, c1, s)] ⊃ TruckIn(t, c, do(a, s))• Negative Effect Axioms: negative effect axioms state which actions can explicitly make each fluent false; for example:[∃c. a = unload(b, t) ∧ BoxOn(b, t, s) ∧ TruckIn(t, c, s)] ⊃ ¬BoxOn(b, t, do(a, s))][∃t. a = load(b, t) ∧ BoxIn(b, c, s) ∧ TruckIn(t, c, s)] ⊃ ¬BoxIn(b, c, do(a, s))][∃c. a = drive(t, c) ∧ TruckIn(t, c1, s)] ⊃ ¬TruckIn(t, c1, do(a, s))In general, positive and negative effect axioms can be specified by considering all of the ways in which each action canaffect each fluent. Fortunately, these axioms are easy to derive directly from the PDDL representation given in Fig. 2. In fact,one can verify that these effect axioms are simply syntactic rewrites of the PDDL effects where we have made the followingtransformations:(1) The action name from the PDDL effect is placed in an equality on the LHS of the ⊃.(2) All universal quantifiers for universal effects are dropped as all unquantified variables are assumed to be universallyquantified in the effect axioms.(3) The when conditions of the PDDL effect are conjoined on the LHS of the ⊃ with all fluents specified in terms of thesituation s.(4) The then portion of the effect (which should be a single literal) is placed on the RHS of the ⊃ and is parameterized bythe post-action situation do(a, s). Whether the literal is negated or non-negated respectively determines whether theresulting axiom should be negative or positive.(5) Any free variables appearing only on the LHS of the ⊃ and not appearing free in the action term are explicitly existen-tially quantified in the LHS.This takes care of specifying what changes, however we have not provided any axioms for specifying what does not change,i.e., the so-called frame axioms. Obviously, if we want to prove anything useful in our theory, we have to specify frameaxioms. Otherwise, we would never be able to infer the properties of a successor or predecessor state for an action assimple as a noop. However, specifying exactly what does not change in a compact manner has been an extremely difficultproblem to solve for the situation calculus—this is, of course, the infamous frame problem.An especially elegant solution to the frame problem is that proposed by [63]. In this solution, we specify all positiveand negative effects for a fluent, which conveniently, we have just done in our translation from PDDL above. We use thefollowing normal form for positive effect axioms where F is a fluent and γ +F ((cid:10)x, a, s) represents a first-order formula that, iftrue in s, results in F ((cid:10)x, do(a, s)) being true after action a((cid:10)x) is executed in situation s:γ +F ((cid:10)x, a, s) ⊃ F ((cid:10)x, do(a, s))Likewise, we use the following normal form for negative effect axioms where γ −that if true in s, results in F ((cid:10)x, do(a, s)) being false after action a((cid:10)x) is executed in situation s:(12)F ((cid:10)x, a, s) represents a first-order formulaγ −F ((cid:10)x, a, s) ⊃ ¬F ((cid:10)x, do(a, s))(13)We note that the potential difference between our previous presentation of positive and negative effect axioms and thisnormal form is that there is exactly one positive effect axiom for each positive fluent and one negative effect axiom for eachnegative fluent. This just happens to be the case in our example, but if it were otherwise, we could use the simple logicalequivalence[(C1 ⊃ F ) ∧ (C2 ⊃ F )] ≡ [(C1 ∨ C2) ⊃ F ]to rewrite any set of effect axioms derived from the PDDL subset of PPDDL into this normal form.Next, we need to add in unique name axioms for all pairs of distinct action names A and B stating thatA((cid:10)x) (cid:17)= B((cid:10)y)(14)(15)S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788and also that identical actions have identical arguments:A(x1, . . . , xk) = A( y1, . . . , yk) ⊃ x1 = y1 ∧ · · · ∧ xk = yk757(16)From this normal form, unique names axioms, and explanation closure axioms that state these are the only effects that holdin our world model, Reiter showed that we can build successor state axioms (SSAs) that compactly encode both the effect andframe axioms for a fluent. The format of the successor state axiom for a fluent F is as follows:F ((cid:10)x, do(a, s)) ≡ ΦF ((cid:10)x, a, s)≡ γ +F ((cid:10)x, a, s) ∨ F ((cid:10)x, s) ∧ ¬γ −F ((cid:10)x, a, s)(17)For our running BoxWorld example, we obtain the following SSAs:BoxOn(b, t, do(a, s)) ≡ ΦBoxOn(b, t, a, s)≡ [∃c. a = load(b, t) ∧ BoxIn(b, c, s) ∧ TruckIn(t, c, s)]∨ BoxOn(b, t, s) ∧ ¬[∃c. a = unload(b, t) ∧ BoxOn(b, t, s) ∧ TruckIn(t, c, s)]BoxIn(b, c, do(a, s)) ≡ ΦBoxIn(b, c, a, s)≡ [∃t. a = unload(b, t) ∧ BoxOn(b, t, s) ∧ TruckIn(t, c, s)]∨ BoxIn(b, c, s) ∧ ¬[∃t. a = load(b, t) ∧ BoxIn(b, c, s) ∧ TruckIn(t, c, s)]TruckIn(t, c, do(a, s)) ≡ ΦTruckIn(t, c, a, s)≡ [∃c1. a = drive(t, c) ∧ TruckIn(t, c1, s)]∨ TruckIn(t, c, s) ∧ ¬[∃c1. a = drive(t, c) ∧ TruckIn(t, c1, s)]While the notation might seem a bit cumbersome, the meaning of the axioms is quite intuitive. For example, the suc-cessor state axiom for BoxOn(b, t, ·) states that a box b is on a truck t after an action iff the action loaded box b on truck tor box b was already on truck t to begin with and the action did not unload it.3.2.3. RegressionAn important tool in the development of first-order MDPs is the ability to take a first-order state description ψ and“backproject” it through a deterministic action to see what conditions must have held prior to executing the action if ψholds after executing the action. This is precisely the definition of regression. Fortunately, the SSAs lend themselves to avery natural specification definition of regression: if we want to regress a fluent F ((cid:10)x, do(a, s)) through an action a, we needonly replace the fluent with its equivalent pre-action formula ΦF ((cid:10)x, a, s). In general, we can inductively define a regressionoperator Regr(·) for all first-order formulae as follows [64]:• Regr(F ((cid:10)x, do(a, s))) = ΦF ((cid:10)x, a, s)• Regr(¬ψ) = ¬Regr(ψ)• Regr(ψ1 ∧ ψ2) = Regr(ψ1) ∧ Regr(ψ2)• Regr((∃x)ψ) = (∃x)Regr(ψ)Using the unique names assumption for actions and these regression rules, we can perform regression on any first-orderlogic formula. For example, if∃b. BoxIn(b, paris, do(unload(b∗∗, t), s))holds then we can use the regression operator to determine what must have held in the pre-action situation s. Following isa derivation using the above rules:∗∗Regr(∃b. BoxIn(b, paris, do(unload(b, t= ∃b. Regr(BoxIn(b, paris, do(unload(b= ∃b. ΦBoxIn(b, paris, unload(b), s)= ∃b. [[∃t. unload(b, t∗∗∗∗), s)))∗∗, t), s))), t∨ BoxIn(b, paris, s)∗∧ ¬[∃t. unload(b) = unload(b, t) ∧ BoxOn(b, t, s) ∧ TruckIn(t, paris, s)]∗) = load(b, t) ∧ BoxIn(b, paris, s) ∧ TruckIn(t, paris, s)]], tAt this point, we can use the unique names axioms for actions to simplify, and exploit rules for distributing quantifiersand renaming variables with respect to equality to obtain the following equivalent representation:= [∃b, t. b = b∗ ∧ t = t∗ ∧ BoxOn(b, t, s) ∧ TruckIn(t, paris, s)]= [(∃b.b = b∨ ∃b. BoxIn(b, paris, s)∗) ∧ (∃t.t = t∨ ∃b. BoxIn(b, paris, s)∗) ∧ BoxOn(b∗∗, t, s) ∧ TruckIn(t∗, paris, s)]758S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788We will assume throughout the rest of this article that all object domains are non-empty.2 This leads to the following fullysimplified form of the regression:Regr(∃b. BoxIn(b, paris, do(unload(b∗, s) ∧ TruckIn(t= [BoxOn(b, t∗), s))), t∗, paris, s)] ∨ ∃b. BoxIn(b, paris, s)∗∗This final result is very intuitive: it states that if there exists a box b in paris after unloading some box b∗twas in paris, or a box was in paris to begin with., then either the truck t∗(18)∗from some truck3.3. FOMDP representationHaving defined the deterministic situation calculus translation of a simple PDDL model, we use this as a building blockto obtain a first-order MDP (FOMDP) [14] from the restricted PPDDL syntax for relational MDPs that we introduced earlier.A FOMDP can be thought of as a universal MDP that abstractly defines the state, action, transition, and reward tuple(cid:3)S, A, T , R(cid:4) for all possible domain instantiations (i.e., an infinite number of ground MDPs). In this subsection we formalizethe building blocks of FOMDPs. We begin by introducing the case notation and operations and discuss the representationof the reward and value function as case statements. Then we describe how stochastic actions are represented by buildingon our previous situation calculus formalization. Once all of these components are defined, we will have everything neededto generalize the dynamic programming solution of MDPs from the ground case to the lifted case of symbolic dynamicprogramming for FOMDPs.3.3.1. Case representation of rewards, values, and probabilitiesWe introduce two useful variants of a case notation along with its logical definition to allow first-order specifications ofthe rewards, probabilities, and values required for FOMDPs:(t = case[φ1, t1; · · · ; φn, tn]) ≡⎝t =⎛φ1 : t1:: :φn : tn⎞⎠(cid:17)≡(cid:15) (cid:16)i(cid:2)n{φi ∧ t = ti}(19)Here the φi are state formulae where fluents in these formulae do not contain the term do and the ti are terms. We notethat in contrast to states, situations reflect the entire history of action occurrences. However, the specification of our FOMDPdynamics is Markovian and allows recovery of state properties from situation terms. For this reason, we can always representthe situation term using the free variable s without loss of generality. Often the ti will be numerical constants and the φiwill partition state space.We emphasize that the case notation for a logical formula (whether in the syntactic form t = case[φ1, t1; · · · ; φn, tn] orin the tabular form above) is simply a meta-logical notation used as a compact representation of the logical formula itself.In the meta-logical notation of cases, all formulae φi , terms ti and parameters of the case statement such as the situationterm s refer to symbols of the underlying logical language. At a meta-logical level, a case statement may be viewed as arelation since the case “partition” formulae may overlap and may not be exhaustive. Case statements may be compared with(in)equalities and manipulated with arithmetic operations to produce other case statements (all at a meta-logical level).To illustrate this notation concretely, we represent our BoxWorld FOMDP reward function R(s) from our PPDDL repre-sentation in Fig. 2 as the following rCase(s) statement that reflects the immediate reward obtained in situation s:rCase(s) =∃b.BoxIn(b, paris, s): 10¬∃b.BoxIn(b, paris, s) : 0(20)For simplicity of presentation, we will assume the reward is not action dependent, but such dependencies can be introducedwithout difficulty. Throughout the text, R(s) will be used to represent a generic FOMDP reward case statement and rCase(s)will refer to the specific reward function. Thus, for BoxWorld, we write R(s) = rCase(s) and wherever R(s) occurs, we cansubstitute the logical formula for rCase(s).Here we see that the first-order formulae in the case statement divide all possible ground states into two regions ofconstant-value: when there exists a box in paris, a reward of 10 is achieved, otherwise a reward of 0 is achieved. Likewisethe value function V (s) that we derive through symbolic dynamic programming can be represented in exactly the samecase format. Indeed, V 0(s) = R(s) in the first-order version of value iteration.The case representation can also be used to specify transition probabilities (as we will see below). We first discuss theoperations that can be performed on case statements.2 Logically, this requires a background theorem axiom for every object type Sort that states ∃o. Sort(o). With this, we can use the simplification (∃Sort :o. o = o∗) ⊃ (cid:18).S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–7887593.3.2. Case operationsIn this subsection we introduce various operations that can be applied to case statements providing both a formal logicaldefinition and a graphical example that intuitively demonstrates the application of the case operation.We begin by formally introducing the following binary ⊗, ⊕, and (cid:21) operators on case statements [14]:case[φi, ti : i (cid:3) n] ⊗ case[ψ j, v j : j (cid:3) m] = case[φi ∧ ψ j, ti · v j : i (cid:3) n, j (cid:3) m]case[φi, ti : i (cid:3) n] ⊕ case[ψ j, v j : j (cid:3) m] = case[φi ∧ ψ j, ti + v j : i (cid:3) n, j (cid:3) m]case[φi, ti : i (cid:3) n] (cid:21) case[ψ j, v j : j (cid:3) m] = case[φi ∧ ψ j, ti − v j : i (cid:3) n, j (cid:3) m](21)(22)(23)Intuitively, to perform an operation on case statements, we simply perform the corresponding operation on the cross-product of all case partitions of the operands. Letting each φi and ψ j denote generic first-order formulae, we can performthe “cross-sum” ⊕ of case statements in the following manner:φ1 : 10φ2 : 20⊕ψ1 : 1ψ2 : 2=φ1 ∧ ψ1 : 11φ1 ∧ ψ2 : 12φ2 ∧ ψ1 : 21φ2 ∧ ψ2 : 22Likewise, we can perform (cid:21), ⊗, and max operations by, respectively, subtracting, multiplying, or taking the max of partitionvalues. Note that for a binary operation involving a scalar and a case statement, a scalar value C may be viewed as case[(cid:18), C]where (cid:18) is a tautology. We use theoperators to, respectively, denote summations and products of multiple caseoperands.and(cid:18)(cid:19)It is important to note that some partitions resulting from the application of the ⊕, (cid:21), and ⊗ operators may be incon-sistent; if we can identify such inconsistency, we simply discard such partitions. When the case partitions contain generalfirst-order logic formulae, inconsistency detection is undecidable. However, for the symbolic dynamic programming algo-rithm discussed in this section, it is not required that all inconsistent partitions be discarded; failing to do so simply resultsin a non-minimal case representation that contains partitions not corresponding to any world state. In practice, we rely ontime-limited incomplete theorem proving for inconsistency pruning.We define a few additional operations on case statements, the first being the binary ∪ operation:case[φi, ti : i (cid:3) n] ∪ case[ψ j, v j : j (cid:3) m] = case[φ1, t1; . . . ; φn, tn; ψ1, v 1; . . . ; ψm, vm](24)In this operation we simply construct the union of the partitions from each of the case statements; for example:φ1 : 10φ2 : 20∪ψ1 : 1ψ2 : 2=φ1 : 10φ2 : 20ψ1 : 1ψ2 : 2Next we define two unary operations. The ∃(cid:10)x. case((cid:10)x) operation simply existentially quantifies the case((cid:10)x) statement.Since case((cid:10)x) is defined logically with a disjunction, we can distribute the ∃(cid:10)x inside the disjunction:⎛∃(cid:10)x.⎝t =φ1((cid:10)x) : t1:: :φn((cid:10)x) : tn⎞(cid:16)⎠ ≡ ∃(cid:10)x.{φi((cid:10)x) ∧ t = ti}(cid:16)i(cid:2)n{∃(cid:10)x. φi((cid:10)x) ∧ t = ti}≡i(cid:2)n⎛≡⎝ t =⎞⎠∃(cid:10)x. φ1((cid:10)x) : t1:: :∃(cid:10)x. φn((cid:10)x) : tn(25)Normally we assume an implicit “t =” for a case statement but show it above for logical clarity.The second unary operation is denoted “casemax” (and not “max”) since it produces a case statement as opposed to asingle numerical value. The result of casemax is a case statement where the maximal possible value of its case argumentis assigned to each region of state space in the resulting case statement. Assuming that the case partitions are pre-sortedsuch that ti > ti+1 and all partitions of equal value have been disjunctively merged we can formally define this operation asfollows:(cid:20)casemax case[φ1, t1; . . . ; φn, tn] = caseφi ∧(cid:22)¬φ j, ti : i (cid:3) n(cid:21)j<i(26)760S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Following is a more intuitive graphical exposition of the same casemax operation:casemaxφ1 : t1φ2 : t2......:φn : tn=φ1: t1φ2 ∧ ¬φ1: t2......φn ∧ ¬φ1 ∧ ¬φ2 ∧ · · · ∧ ¬φn−1 : tn:One can easily verify that if the partitions are sorted from the highest value t1 to the lowest tn, then the highest valueconsistent with any state formula in the input case statement is assigned to the unique partition consistent with that stateformulae in the resulting case statement. (If the φi in the input are mutually exclusive, then the casemax results in a casestatement logically equivalent to the original.) The application of casemax requires constructing new partition formulae, upto n times the length of the original formulae for a case statement with n partitions. Fortunately, the use of inconsistencydetection discussed previously and first-order ADDs (FOADD) that we introduce in the next section will mitigate the impactof this blowup by respectively pruning inconsistent case partitions and simplifying the representation of case formulae.It is important to point out that all of the case operators are purely symbolic in that the ti case partition values arenot necessarily restricted to constant numerical values, but can be arbitrary symbolic (possibly state-dependent) terms [14].However, the casemax operator (as defined here) implicitly requires an ordering on the ti . We assume for the rest of thissection that the case values are numeric rather than symbolic, and apply the natural < operator for our ordering.3.3.3. Stochastic actions and transition probabilitiesTo state the FOMDP transition function for an action, stochastic “agent” actions are decomposed into a collection ofdeterministic actions, each corresponding to a possible outcome of the stochastic action. We then use a case statement tospecify a distribution according to which “Nature” may choose a deterministic action from this set whenever the stochasticaction is executed. As a consequence we need only formulate SSAs using the deterministic Nature’s choices [2,15,59,64], thusobviating the need for a special treatment of stochastic actions in SSAs.Letting A((cid:10)x) be a stochastic action with Nature’s choices (i.e., deterministic actions) n1((cid:10)x), · · · , nk((cid:10)x), we represent theprobability of ni((cid:10)x) given A((cid:10)x) is executed in s by P (n j((cid:10)x), A((cid:10)x), s). Continuing with the translation of our simple PPDDLexample, we note that the load(b, t) action has one set of effects that occurs with probability 0.9. We use the deterministicaction loadS(b, t) to denote the successful occurrence of these effects, and we let the deterministic action loadF(b, t) denotethe failure of these effects to execute. To do this, we must redefine our SSAs from the previous PDDL case: now load(b, t) isa stochastic action executed by the agent with loadS(b, t) and loadF(b, t) being possible outcomes (i.e., deterministic actionschosen by Nature). Similarly, we interpret the other two actions using unloadS(b, t)/unloadF(b, t) as the two deterministicoutcomes for unload(b, t), and driveS(t, c)/driveF(t, c) as the two deterministic outcomes for drive(t, c). For completeness andcorrectness, we redefine our SSAs for BoxWorld in terms of these new deterministic actions for the BoxWorld FOMDP:BoxOn(b, t, do(a, s)) ≡ ΦBoxOn(b, t, a, s)≡ [∃c. a = loadS(b, t) ∧ BoxIn(b, c, s) ∧ TruckIn(t, c, s)]∨ BoxOn(b, t, s) ∧ ¬[∃c. a = unloadS(b, t) ∧ BoxOn(b, t, s) ∧ TruckIn(t, c, s)]BoxIn(b, c, do(a, s)) ≡ ΦBoxIn(b, c, a, s)≡ [∃t. a = unloadS(b, t) ∧ BoxOn(b, t, s) ∧ TruckIn(t, c, s)]∨ BoxIn(b, c, s) ∧ ¬[∃t. a = loadS(b, t) ∧ BoxIn(b, c, s) ∧ TruckIn(t, c, s)]TruckIn(t, c, do(a, s)) ≡ ΦTruckIn(t, c, a, s)≡ [∃c1. a = driveS(t, c) ∧ TruckIn(t, c1, s)]∨ TruckIn(t, c, s) ∧ ¬[∃c1. a = driveS(t, c) ∧ TruckIn(t, c1, s)]Here, we have simply replaced our previous deterministic action names from the PDDL version with the deterministicsuccess versions of Nature’s choice actions that we will use in our FOMDP. Note that since the “failure” versions of theactions correspond to the “no effects” case, they obviously do not play any role in the SSAs. The frame assumption presentin the SSAs ensures that what was not explicitly changed remains the same.We can now specify a distribution P (n j((cid:10)x), A((cid:10)x), s) over Nature’s choice deterministic outcome using case statements tospecify families of distributions, where the partitions in the case statements correspond to different classes of states andstochastic action parameters on which the distributions are conditioned. We denote specific instances of P (n j((cid:10)x), A((cid:10)x), s)with the case statement pCase(n j((cid:10)x), A((cid:10)x), s) where (cid:18) is a tautology, for example:pCase(loadS(b, t), load(b, t), s) = (cid:18) : 0.9pCase(loadF(b, t), load(b, t), s) = (cid:18) : 0.1S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788pCase(unloadS(b, t), unload(b, t), s) = (cid:18) : 0.9pCase(unloadF(b, t), unload(b, t), s) = (cid:18) : 0.1pCase(driveS(b, t), drive(b, t), s) = (cid:18) : 1.0pCase(driveF(b, t), drive(b, t), s) = (cid:18) : 0.0761(27)(28)The above axiomatization does not fully illustrate the power of the FOMDP representation in that the probabilities arenot state or action dependent, so we briefly digress to demonstrate a slightly more interesting variant. Suppose that thesuccess of driving a truck to a city depends on whether the truck contains a box b with volatile material denoted bythe predicate Volatile(b). Then we can specify the family of distributions over Nature’s choices for this stochastic action asfollows:pCase(driveS(t, c), drive(t, c), s ) =pCase(driveF(t, c), drive(t, c), s ) =∃b.BoxOn(b, t, s) ∧ Volatile(b): 0.9¬(∃b.BoxOn(b, t, s) ∧ Volatile(b)) : 1.0∃b.BoxOn(b, t, s) ∧ Volatile(b): 0.1¬(∃b.BoxOn(b, t, s) ∧ Volatile(b)) : 0.0Here we see the transition probability of drive(t, c) can be easily conditioned on state properties of s and action param-eters t and c.It is important to note that the probabilities over all deterministic Nature’s choices for a stochastic action sum to one:k(cid:23)j=1P (n j((cid:10)x), A((cid:10)x), s) = (cid:18) : 1 ; ∀(cid:10)x, sIn addition, each P (n j((cid:10)x), A((cid:10)x), s) should be a disjoint partitioning of state space such that no two case partitions ambigu-ously assign multiple probabilities to the same state. These two properties are crucial to having a well-defined probabilitydistribution over all possible deterministic action outcomes for every possible state.For this last example, the second property can be easily verified:pCase( driveS(t, c), drive(t, c), s ) ⊕ pCase( driveF(t, c), drive(t, c), s ) = (cid:18) : 13.4. Symbolic dynamic programming (SDP)Symbolic dynamic programming (SDP) [14] is a dynamic programming solution to FOMDPs that produces a logical casedescription of the optimal value function. This is achieved through the symbolic operations of first-order decision-theoreticregression and maximization that perform the traditional dynamic programming Bellman backup at an abstract level withoutexplicit enumeration of either the state or action spaces of the FOMDP. Among many possible applications, the use of SDPleads directly to a domain-independent value iteration solution to FOMDPs.We will assume a constant numerical representation of values in order to explicitly perform the casemax during SDPin this article. However, we note that an appropriate generalization of casemax (cf., Chapter 6 of [67]) along with Regr offunctional fluents [64] allows the definitions covered here to apply to general symbolic value representations using generalterms rather than constants, hence the original use of “symbolic” in the name of the SDP algorithm.3.4.1. First-order decision-theoretic regressionSuppose we are given a value function V (s). The first-order decision-theoretic regression (FODTR) [14] of this valuefunction through an action A((cid:10)x) yields a case statement containing the logical description of states and values that wouldgive rise to V (s) after doing action A((cid:10)x). This is analogous to classical goal regression, the key difference being that actionA((cid:10)x) is stochastic. In MDP terms, the result of FODTR roughly corresponds to a Q-function (albeit one with free variablesfor the action parameters), which corresponds to the first half of a Bellman backup operation given in Eq. (6).3We define the first-order decision theoretic regression (FODTR) as the situation calculus analog of Eq. (6) where we notethat different successor states only arise through different Nature’s choice deterministic actions:FODTR[V (s), A((cid:10)x)] = R(s) ⊕ γ ·(cid:2)k(cid:23)j=1{P (n j((cid:10)x), A((cid:10)x), s) ⊗ V (do(n j((cid:10)x), s))}(29)(cid:4)3 We do not use an action dependent reward R(s, A((cid:10)x)), but could substitute it if needed.762S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788FODTR uses a meta-logical notation that takes as arguments V (s) representing the logical case statement for a value functionwith situation variable s and a parameterized stochastic action term A((cid:10)x) with free variables (cid:10)x. All subsequently definedoperations on case statements in this article will be defined analogously.The only problem with the FODTR[V (s), A((cid:10)x)] operation as currently defined is that the formula V (do(n j((cid:10)x), s)) refersnot to the current situation s, but to the future situation do(n j((cid:10)x), s), but this is easily remedied with regression:FODTR[V (s), A((cid:10)x)] = R(s) ⊕ γ ·(cid:2)k(cid:23)j=1{P (n j((cid:10)x), A((cid:10)x), s) ⊗ Regr(V (do(n j((cid:10)x), s)))}(30)(cid:4)This is equivalent to the FODTR operation in Eq. (29) since the Regr operation preserves equivalence (by definition). Also onaccount of the equivalence preserving properties of Regr, we note that if V (s) partitions the state space then so must theresulting case statement for FODTR[V (s), A((cid:10)x)]. Thus, from a logical description of V (s) we can derive one for its decision-theoretic regression FODTR[V (s), A((cid:10)x)]. This is key to avoiding state and action enumeration in dynamic programming.We denote an instance of the value function V (s) by the case statement vCase(s). As defined previously, we also assumethat the reward function R(s) and instances of Nature’s choice probabilities P (n j((cid:10)x), A((cid:10)x), s) are denoted by rCase(s) andpCase(n j((cid:10)x), A((cid:10)x), s), respectively.∗)As an example, let us compute the FODTR for vCase(s) = rCase(s) through the stochastic action A((cid:10)x) = unload(bwhere rCase(s) is the BoxWorld reward as previously defined in Eq. (20). Since vCase(s) is logically defined, we can pushthe Regr operator into the individual vCase(s) partitions as follows:∗∗, t∗FODTR[vCase(s), unload(b(cid:24)k(cid:23)(cid:2), t)]= rCase(s) ⊕ γpCase(n j((cid:10)x), unload(bj=1Now, since the stochastic action A((cid:10)x) = unload(bover unloadS(band unloadF(b∗, t∗, t∗, t∗) and unloadF(b∗) from Eqs. (27) and (28), respectively, obtaining:)] = rCase(s) ⊕(cid:20)(cid:5), t∗∗FODTR[vCase(s), unload(b∗∗, t), s) ⊗ Regr(∃b.BoxIn(b, paris, do(n j((cid:10)x), s))): 10Regr(¬∃b.BoxIn(b, paris, do(n j((cid:10)x), s))) : 0(cid:25)(cid:4)∗). We now substitute the pCase definitions for the deterministic actions unloadS(b∗), we know that Nature’s deterministic action choices n j((cid:10)x) range∗)∗, t∗, tγ(cid:5)⊕∗, t(cid:18) : 0.9 ⊗ Regr(∃b.BoxIn(b, paris, do(unloadS(bRegr(¬∃b.BoxIn(b, paris, do(unloadS(b∗, t(cid:18) : 0.1 ⊗ Regr(∃b.BoxIn(b, paris, do(unloadF(bRegr(¬∃b.BoxIn(b, paris, do(unloadF(b∗), s)))∗, t∗))))∗, t: 10∗)))) : 0: 10∗), s))) : 0(cid:6)(cid:22)(cid:6)∗)We have already computed Regr(∃b.BoxIn(b, paris, do(unloadS(b∗). And by the properties of Regr, we know that Regr(¬φ) = ¬Regr(φ)from the PDDL case has been renamed to unloadS(bso we can easily obtain the regression of the negated partition in rCase(s). It is important to note that if rCase(s) partitionedthe post-action state space, the Regr operator preserves this partitioning in the pre-action state space. We note that∗)))) from Eq. (18) where the deterministic unload(b∗, t∗, t∗, tRegr(φ((cid:10)x, do(unloadF(b∗∗, t)))) = φ((cid:10)x, s)∗, t∗) has no effects and is thus equivalent to a noop action. Making these substitutions,can be easily derived since unloadF(bexplicitly multiplying in the action probabilities and discount factor γ = 0.9, and explicitly writing out rCase(s), we obtainthe following (where, for readability, we use ¬“ to denote the conjunction of the negation of all partitions above the givenpartition in the case statement):FODTR[vCase(s), unload(b∗∗)] =, t∃b.BoxIn(b, paris, s) : 10¬“: 0∗, s) ∧ TruckIn(t∗, t[BoxOn(b∨∃b. BoxIn(b, paris, s)¬“∃b.BoxIn(b, paris, s) : 0.9¬“: 0⊕⊕∗, paris, s)]: 8.1: 0Finally, explicitly carrying out the ⊕’s and simplifying yields the final result:FODTR [vCase(s), unload(b∗∗)] =, t∃b. BoxIn(b, paris, s)¬“ ∧ [BoxOn(b¬“∗, t∗, s) ∧ TruckIn(t: 19.0∗, paris, s)] : 8.10:(31)∗)The case statement resulting from FODTR contains free variables for the action parameters; in this case A((cid:10)x) = unload(bso the free parameters are b. This result is intuitive: it states that if a box was already in paris then we get reward19 (10 for the current reward and 9 for the discounted 1-step reward). Otherwise, if a box is not in paris in the currentand t∗, t∗∗S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788763∗∗was on truck tin paris and the action was specifically unload(b∗), then we get an expected futurestate, but box breward of 8.1 taking into account the success probability of unloading the box and the discount factor. Finally, if no box isin paris in the current state and we do not unload a box then we get 0 total reward.∗, tThis case statement represents the value of taking stochastic action unload(b∗) and acting so as to obtain the valuegiven by rCase(s) thereafter. However, what we really need for symbolic dynamic programming is a logical description of∗a Q-function (recall Eq. (6)) that tells us the possible values that can be achieved for any action instantiation of b.This leads us to the following definition Q ( A, s) of a first-order Q-function that makes use of the previously defined ∃(cid:10)xunary case operator:and t∗, t∗Q t( A, s) = ∃(cid:10)x. FODTR[V t−1(s), A((cid:10)x)](32)We denote a specific instance of Q t( A, s) by the case statement qCaset(s, A). We can think of qCaset(s, A) as a logicaldescription of the Q-function for action A((cid:10)x) indicating the values that could be achieved by any instantiation of A((cid:10)x). Byusing the first-order case representation of states as well as action quantification via the ∃(cid:10)x operation, FODTR effectivelyachieves both action and state abstraction.Letting vCase0(s) = rCase(s), we can continue our running example to obtain a Q-function description for action unloadwhere we have removed vacuous quantifiers. Technically, qCase1(unload, s) would not be an exhaustive partitioning of thestate space in that the 0 value partition from Eq. (32) is not the same one implied here from the ¬“ because the partitionformulae above it have been quantified. However, throughout this article, we can exploit our assumption that all FOMDPshave a noop action to assume that the minimum value for any state is 0 (as opposed to being undefined). Thus we canalways show the final 0 partition as ¬“ to indicate that any partitions not explicitly assigned a value by the above partitionsare assigned a default value 0. Thus, we arrive at the following intuitive result:qCase1(unload, s) = ∃b∗∗, t. FODTR[vCase0(s), unload(b∗∗)], t=∃b. BoxIn(b, paris, s)∃b¬“∗. [¬“ ∧ BoxOn(b∗, t∗, t∗, s) ∧ TruckIn(t: 19.0∗, paris, s)] : 8.10:In words, this states if the box was already in paris then we get a discounted reward of 19. Otherwise, if a box is notin paris in the current state, but there exists some box on a truck in paris, then we could unload it to get an expecteddiscounted reward of 8.1. Finally, if there is no box on a truck to unload in paris and there is no box already in paris thenwe get 0 expected discounted reward. It is instructive to compare this description to the prior description of FODTR withoutexistential action quantification—the difference is subtle, but important for action abstraction.3.4.2. Symbolic maximizationAt this point, we can decision-theoretically regress the value function through a single stochastic action to obtain arepresentation of its Q-function, but to complete the dynamic programming (Bellman backup) step in the spirit of Eq. (7)from Section 2, we need to know the maximum value that can be achieved by any action. For example, in the BoxWorldFOMDP, our possible action choices are unload(b, t), load(b, t), and drive(t, c) and our Q-function computations using Eq. (32)give us qCase1(unload, s), qCase1(load, s), and qCase1(drive, s). In general, we will assume that we have m stochastic actions{ A1((cid:10)x1), . . . , Am((cid:10)xm)} and a corresponding set of Q-functions {qCaset( A1, s), . . . , qCaset( Am, s)} derived from a common valuefunction vCaset−1(s).We might try to obtain a case description of the value function vCaset(s) by simply applying the case ∪ operator tomerge all partitions of the Q-functions, i.e., qCaset(s, A1) ∪ · · · ∪ qCaset(s, Am). While this provides us with a description ofpossible values, it is not a value function because the state spaces of each Q-function may overlap, thus potentially assigningmultiple values to the same underlying state. What we really want instead is to assign the highest possible value to eachportion of state space. Fortunately, this is quite easy with the casemax operator. Thus we get the following equation for thesymbolic maximization of Q-functions:V t(s) = casemax [Q t( A1, s) ∪ · · · ∪ Q t( Am, s)](33)Recalling the way in which the casemax operation is computed from Eq. (26), every resulting instance vCaset(s) of the valuefunction V t(s) will have the following case statement format where value case partition ψ j corresponds to value v j andv i > v i+1:vCaset(s) =ψ1: v 1ψ2 ∧ ¬ψ1: v 2......ψn ∧ ¬ψ1 ∧ ¬ψ2 ∧ · · · ∧ ¬ψn−1 : vn:This approach effectively gives us a decision-list representation of our value function (recall the optimal value functionrepresentation from Fig. 4). Thus, to determine the value for a state, we can simply traverse the list from highest to lowestvalue and take the value for the first case partition that is satisfied. The casemax operation guarantees that this valuefunction will be a disjoint partitioning of the state space and our previous assumption that all actions are executable in allstates ensures that this value function exhaustively assigns a value to all possible states (assuming vCaset−1 was exhaustive).764S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–7883.4.3. First-order value iterationOne should note that the SDP equations given here are exactly the lifted versions of the classical dynamic programmingsolution to MDPs given previously in Eqs. (6) and (7) from Section 2. Since those equations were used in part to define avalue iteration algorithm, we can use the lifted versions to define a first-order value iteration algorithm where (cid:4) is our errortolerance:(1) Initialize V 0(s) = R(s), t = 1.(2) Compute V t(s) given V t−1(s) using Eqs. (32) and (33).(3) If the following Bellman error inequality holds(cid:26)(cid:26)(cid:26)V t(s) (cid:21) V t−1(s)(cid:26)(cid:3) (cid:4)(1 − γ )2γ∞then terminate and return V t(s), otherwise go to step 2.(34)Here, we define (cid:23)V t(s) (cid:21) V t−1(s)(cid:23)∞ as the maximal absolute value of any consistent partition in the case statement result-ing from V t(s) (cid:21) V t−1(s).For example, applying first-order value iteration to the 0-stage-to-go value function (i.e., vCase0(s) = rCase(s), given pre-viously in Eq. (20)) yields the following simplified 1- and 2-stage-to-go value functions in the BoxWorld problem domain:vCase1(s) =vCase2(s) =∃b.BoxIn(b, paris, s): 19.0¬“ ∧ ∃b, t.TruckIn(t, paris, s) ∧ BoxOn(b, t, s) : 8.1¬“: 0.0∃b.BoxIn(b, paris, s): 26.1¬“ ∧ ∃b, t.TruckIn(t, paris, s) ∧ BoxOn(b, t, s) : 15.4¬“ ∧ ∃b, c, t.BoxOn(b, t, s) ∧ TruckIn(t, c, s): 7.3¬“: 0.0After sufficient iterations of first-order value iteration, the t-stage-to-go value function converges, giving the optimal valuefunction (and as we derive in a moment, an optimal policy) from Fig. 4.Boutilier et al. [14] provide a proof that SDP and thus every step of value iteration produces a correct logical descriptionof the value function. From this, we can lift the error bounds from the ground MDP case in Eq. (8) to show domain-independent error bounds on the first-order abstracted value estimate:Corollary 3.4.1. Let Vvalue iteration guarantees (cid:23)V t(s) − V∗(s) be the optimal value function for a FOMDP. Terminating according to the criteria given in Step 3 of first-order∗(s)(cid:23)∞ < (cid:4) for any domain instantiation (even infinite) of the FOMDP.More generally, as a direct result of this corollary, we can derive domain-independent error bounds for the first-orderrepresentation of the value function produced by any first-order MDP algorithm (see Section 6 for other first-order algo-rithms).Corollary 3.4.2. Let ˆV (s) be an arbitrary first-order case representation of a value function. Let ˆVand (33) to ˆV (s) for a FOMDP. Let (cid:4) = 21−γ(cid:5)(s) (cid:21) ˆV (s)(cid:23)∞. Then (cid:23) ˆV (s) − V(cid:5)(s) be the result of applying Eqs. (32)∗(s)(cid:23)∞ < (cid:4) for any domain instantiation of the FOMDP.(cid:23) ˆVThe difference of γ between the bounds of Corollaries 3.4.1 and 3.4.2 occurs because the former refers to a bound onV t(s), while the latter refers to a bound on ˆV (s) = V t−1(s) and value iteration is known to contract the error by γ on eachiteration.3.4.4. Policy representationGiven a value function, it is important to be able to derive a first-order greedy policy representation from it, just as wedid in the ground case in Section 2. This policy can then be used to directly determine actions to apply when acting in aground instantiation of the FOMDP, or it can be used to define first-order versions of (approximate) policy iteration [69].Fortunately, given a value function V (s), it is easy to derive a greedy policy from it. Assuming we have m parameterizedactions { A1((cid:10)x), . . . , Am((cid:10)x)}, we can formally derive the policy π (s)[·] using the · to denote the value representation fromwhich the policy is derived as follows (taking into account a few modifications to the case operators that we discuss in amoment):(cid:15) (cid:27)(cid:17)π (s)[V (s)] = casemax∃(cid:10)x. FODTR[V (s), Ai((cid:10)x)](35)i=1...mWe often refer to a specific instance of π (s) with the case statement π Case(s). For bookkeeping, we require that eachpartition (cid:3)φ, t(cid:4) in ∃(cid:10)x FODTR[V (s), Ai((cid:10)x)] maintain a mapping to the action Ai that generated it, which we denote as (cid:3)φ, t(cid:4) →S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788765Ai . Then, given a particular world state s, we can evaluate π Case(s) to determine which maximal policy partition (cid:3)φ, t(cid:4) → Aiis satisfied by s and thus, which action Ai should be applied. If we retrieve the bindings of the existentially quantified actionvariables ∃(cid:10)x in that satisfying policy partition, we can easily determine the parameterization of action Ai that should applyaccording to the policy.To make this concrete, we derive a simple greedy policy for the BoxWorld FOMDP assuming the value function∗) and noop. Noting that we have already computed∗)] in Eq. (31) and that FODTR[rCase(s), noop] will just be rCase(s) with 10 replaced by 19, we∗, tV (s) = rCase(s) and that we only have two actions unload(b∗, tFODTR[rCase(s), unload(bobtain the following policy:π Case[rCase(s)]= casemax({∃b∗∗, t. FODTR[rCase(s), unload(b∗∗, t=∃b. BoxIn(b, paris, s)∗, t¬“ ∧ [∃b¬“∗. BoxOn(b∗, t∗, s) ∧ TruckIn(t)]} ∪ {FODTR[rCase(s), noop]}): 19.0 −→ noop∗, paris, s)] : 8.1 −→ unload(b∗, t∗):0 −→ noopFor a more interesting policy, we refer the reader back to the optimal value function and policy for BoxWorld given inFig. 4.Technically, we note that there may be an infinite number of actions that can be applied since there are an infinite∗) depending on the domain instantiation. Thus, this policy representation∗, tnumber of ground instantiations of unload(bmanages to compactly represent the selection of an optimal action amongst an infinite set.4. Practical FOMDP solution techniquesThe last section reviewed a symbolic dynamic programming (SDP) algorithm theoretically capable of producing an (cid:4)-optimal value function for a FOMDP that does not require theorem proving to detect inconsistent case partitions or logicalsimplification to maintain compact representations of case partition formulae. However, in practice, both theorem provingand simplification are needed to control the representational blowup of the value function occurring at each step of valueiteration.To this end, the first half of this section introduces a practical first-order extension of the algebraic decision diagram(ADD) [4] data structure, the first-order ADD (FOADD), for maintaining case statements in a simplified, non-redundant formatthat facilitates theorem proving for inconsistency detection. We show how FOADDs can be used to exploit structure inSDP for FOMDPs in much the same manner that ADDs have been used to exploit structure in dynamic programming forMDPs [38]. We conclude with an illustrative empirical results demonstrating that FOADDs enable an automated solutionto basic FOMDPs. We will discuss related work on first-order decision diagrams (FODDs) [81], also applied to FOMDPs, inSection 6.In the second half, we introduce an additive decomposition approach for approximately solving FOMDPs with universalreward specifications. This approach is motivated in part by previous decomposition methods and enables the applicationof FOMDP solution techniques to a reward specification that otherwise renders SDP solution approaches intractable.4.1. Representation and solution with first-order ADDsAn algebraic decision diagram (ADD) [4] is a data structure for compactly representing a function from Bn → R usinga directed acyclic graph. ADDs have been used to compactly model transition functions, rewards, and value functions infactored MDPs [12]. Moreover, value iteration defined in terms of ADD operations has yielded substantial improvements intime and space complexity over enumerated state representations [38].To extend these ideas to the first-order framework, we define methods for breaking down first-order case partitionformulae into their boolean propositional components and create a compact first-order ADD (FOADD) representation of casestatements. Then we can apply known ADD algorithms to perform the ⊗, ⊕, and (cid:21) case operations. Once we have shownhow to do this, we end with a discussion of the practical use of FOADDs and a small example of a FOADD application toSDP.4.1.1. FOADD construction and operationsThe first aspect of FOADDs concerns how to construct them automatically from a case representation. Since ADDs arepropositional, we need some method of finding propositional structure in first-order formulae. We can do this by permut-ing quantifiers at the same level of nesting (e.g., [∃x, y.φ] ≡ [∃ y, x.φ]) and by distributing quantifiers as deeply into caseformulae as possible using the following rewrite rule templates ((cid:24) indicates variables other than those explicitly quantified):[∃x. A(x, (cid:24)) ∨ B(x, (cid:24))] −→ [(∃x. A(x, (cid:24))) ∨ (∃x. B(x, (cid:24)))][∀x. A(x, (cid:24)) ∧ B(x, (cid:24))] −→ [(∀x. A(x, (cid:24))) ∧ (∀x. B(x, (cid:24)))][∃x. A(x, (cid:24)) ∧ B((cid:24))] −→ [(∃x. A(x, (cid:24))) ∧ (B((cid:24)))][∀x. A(x, (cid:24)) ∨ B((cid:24))] −→ [(∀x. A(x, (cid:24))) ∨ (B((cid:24)))](36)(37)(38)(39)766S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Fig. 5. An example conversion from a case statement to a compact FOADD representation demonstrating first-order CSI.We also perform equality simplification using the non-empty domain assumption with the following two rules:[∃x. x = y ∧ A(x, (cid:24))] −→ A( y, (cid:24))[∀x. x (cid:17)= y ∨ A(x, (cid:24))] −→ A( y, (cid:24))(40)(41)The first rule is fairly straightforward while the second rule follows simply from the negation of the first rule with renaming.In practice, we iteratively apply simplification rules (40), (41) followed by rewrite rules (36)–(39) working from the inner-most to the outermost quantifiers until no more rewrites can be applied. While other orders may give different (potentiallysmaller) results, we find that this deterministic approach is generally sufficient to expose most propositional structure infirst-order formulae.We provide the following example application of these rewrite and simplification rules to demonstrate their power:∃x, z. [x = y ∧ A(x, (cid:24)) ∧ B( y, z)]≡ ∃x. [x = y ∧ A(x, (cid:24)) ∧ (∃z. B( y, z))]≡ (∃x. x = y ∧ A(x, (cid:24))) ∧ (∃z. B( y, z))≡ A( y, (cid:24)) ∧ (∃z. B( y, z))[Apply rewrite rule (38) for z][Apply rewrite rule (38) for x][Apply simplification rule (40) for x]To build a FOADD, we first apply these rules to expose the propositional structure of a first-order formula. Consider theexample in Fig. 5(a,b); we start with∃x.[ A(x) ∨ ∀ y. A(x) ∧ B(x) ∧ ¬ A( y)]and apply rewrite rule (39) for y followed by (36) for x to obtain[∃x. A(x)] ∨ ([∃x. A(x) ∧ B(x)] ∧ [∀ y.¬ A( y)]).(42)(43)Once we have pushed quantifiers as far down as possible, we extract the propositional structure of the formula byconsidering propositional connectives over quantified formulae as follows:(cid:15)(cid:17)∃x. A(x) ∨[∃x. A(x) ∧ B(x)] ∧ ∀ y.¬ A( y)(44)Each of these boxes represents a formula that we cannot further decompose into propositional components. Consequently,we treat each of these boxed formulae as propositions. To do this, we maintain a table of mappings from propositionalvariables p, naming each first-order formula, to first-order formulae ψ : {p → ψ}. To convert a new formula φ in a casestatement to a propositional variable, we examine each formula-to-proposition mapping in our table. If φ ≡ ψ for some ψin the table, we return its corresponding proposition p; if φ ≡ ¬ψ , we return ¬p; otherwise, we add a new propositionlabel q and add the mapping q → φ to our table and return q. In our example, having built the table shown in Fig. 5(b), wecan convert the formula to its propositional counterpart:a ∨ (b ∧ ¬a)(45)S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788767At this point, we can build an ADD from a case statement whose formulae are purely propositional. What makes thisADD first-order is the additional proposition to first-order formula mapping that gives each proposition a first-order defi-nition. Standard ADDs can exploit context-specific independence (CSI) [13] (i.e., where the value of a function is independentof an input variable given the assignment to other variables). There is, however, an additional form of CSI that we canexploit in FOADDs—first-order CSI. This first-order CSI follows from the structured and potentially overlapping nature of thepropositional variables. For instance, in our example, ¬a ⊃ ¬b, so as we traverse its FOADD representation, we can force thedecision node for b in the context of a. This is shown in Fig. 5(c).The options for detecting first-order CSI include:(a) Do not perform any first-order CSI detection at all.(b) Maintain information about all pairwise implications in the propositional mapping table and detect just this pairwisefirst-order CSI during the application of FOADD operations.(c) Perform full simplification for all decision nodes in the context of the conjunction of all decisions made for parentnodes during all operations on the FOADD.Obviously (a) requires no additional computation, but can give rise to FOADDs with potentially dead paths. In contrast,(c) requires substantial computation in return for extensive simplication. In practice, we find (b) to offer the most reasonabletradeoff between computation and simplification; time-limited theorem proving, although incomplete, suffices to identifymany pairwise node implications that lead to substantial first-order CSI pruning. It is trivial to extend the ADD algorithmto do this additional consistency check in the presence of parent decisions when performing the standard ADD Apply andReduce operations. However, if (b) or (c) are used, it is not sound to reorder the ADD nodes since the first-order context ofthese prunings may change and thus may no longer be valid after node reordering.Once we convert a case statement to an FOADD, we can apply the ⊗, ⊕, and (cid:21) case operations to FOADDs by makingdirect use of the ADD Apply operations of multiplication, addition, and subtraction [4]. We can reuse standard ADD oper-ations for FOADDs since they are just ADDs with augmented variable definitions in the propositional mapping table. Thus,the only practical difference between ADD and FOADD operations is that these augmented variable definitions may lead toadditional pruning of structure due to first-order CSI.In general, FOADDs may be treated as ADDs, except for the requirement to consult the propositional mapping table inthe following circumstances:1. when constructing a FOADD;2. when converting a FOADD back to a case representation or evaluating a ground state; or3. when exploiting first-order CSI using method (b) or (c) above, we may consult this table during the ADD Reduce andApply procedures.4.1.2. Practical considerationsReplacing case statements with FOADDs in the representation and solution of FOMDPs has the potential to exploit agreat deal of structure that naturally occurs in these representations. First, the disjunctive nature of positive effects inthe regression of FOMDP formulae introduces a number of disjunctions during the application of algorithms such as SDP.Second, the existential quantification of the action variables in these formulae introduce existential quantifiers that canbe distributed through the disjunctions introduced by Regr. Consequently, every SDP step introduces structure that can bedirectly exploited by the previously described methods for exposing propositional structure of first-order formulae. As such,our approach to representing FOADDs is well-suited to FOMDPs as we demonstrate below with a small example.However, if we were to define a complete SDP algorithm for FOMDPs that only uses FOADDs, we would need to definespecial unary FOADD operations such as Regr, casemax, and ∃(cid:10)x used in the SDP algorithm. While Regr can be easily defined(note that a FOADD is just a compact representation of a case statement and thus Regr can still be applied), it changes thelogical meaning of the FOADD nodes since they have a first-order definition. In general, maintaining a canonical represen-tation after performing Regr on a FOADD requires expensive node reordering operations. The application of ∃(cid:10)x and casemaxalso generally require expensive node reordering operations. For these reasons, we do not apply Regr, casemax, or ∃(cid:10)x toFOADDs in practice, instead opting for a pragmatic use of FOADDs that exploits their strengths.4The primary advantage of FOADDs is the provision of efficient binary operations and formula simplification through thebreakdown of propositional structure and the elimination of redundancy that occurs during their construction. In doing thissimplification, FOADDs remove a lot of burden from the theorem prover, which must otherwise detect inconsistency withhighly redundant representations. Thus, in our SDP algorithms, we use FOADDs where they are most useful and efficient—binary operations and logical simplification—and revert to the case representation to perform the unary operations of Regr,casemax, and ∃(cid:10)x that can be expensive due to the need for internal node rotations. This approach leads to a viable SDPalgorithm, to which we now turn.4 While we do not discuss Regr, casemax, and ∃(cid:10)x for FOADDs further here, the reader is referred to [67] for additional information on how one mightperform these operations efficiently.768S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Fig. 6. An example FOADD representation of the reward in BoxWorld and the FOADD representation of the optimal value function and policy for thisdomain.4.1.3. Symbolic dynamic programming with FOADDsThe use of FOADDs in the somewhat hybrid manner discussed above allows the development of a practical SDP algorithm.We have implemented a fully automated first-order value iteration algorithm and tested it on several examples to de-velop a sense of its effectiveness. One problem tested is the running BoxWorld FOMDP example. The FOADDs for thereward, optimal value function and policy are given in Fig. 6. For the variable ordering, we simply maintain the order offormulae as they were added to the variable mapping table in the FOADD during the SDP algorithm. We use the Vampiretheorem prover [65] for detecting equivalence and inconsistency. The total running time for this solution until convergencewithin tolerance 1e–4 was 15.7 s on a 2 Ghz Pentium with 2 Gb of RAM. Unsurprisingly, the final FOADD for this problemgives exactly the decision list structure that we would expect for the BoxWorld problem as shown in Fig. 4.We have also used our FOADD value iteration algorithm to solve other variants of the BoxWorld problem, including theversion given in [14] with an extra fluent for Rain(s) and action probabilities conditioned on this fluent. We also used aBoxWorld reward with the following structure:R(s) =∃b.BoxIn(b, paris, s) ∧ TypeA(b): 10¬“ ∧ ∃b.BoxIn(b, paris, s) ∧ ¬TypeA(b) : 5¬“: 0(46)Here in addition to the Rain(s) fluent, we have also added a non-fluent predicate TypeA(b) to distinguish types of boxesand varying rewards for each type of box. The FOADDs for these solutions are too large to display, but we note that aftera small number of steps of value iteration, the value function FOADD stopped growing indicating that all relevant statepartitions had been identified. Value iteration continued with this quiesced FOADD until all values at the leaves converged.The respective solution times to convergence within tolerance 1e–4 for these more complex problems were 70.4 s and 489 son a 2 Ghz Pentium with 2 Gb of RAM. For comparison, the ReBel algorithm [43] produced the same solution for the firstFOMDP variant with the Rain(s) fluent in <6 s on a 3.1 Ghz machine. ReBel’s specialization for a less expressive subsetof FOMDPs (still capturing BoxWorld, however) results in a substantial performance edge. We discuss differences betweenReBel and the work in this article in Section 6.There appear to be at least two general criteria for problem domains to demonstrate finitely-sized optimal value func-tions with the current case representation as occurred in these examples: (1) the non-zero reward case partitions must beexistentially quantified and (2) the FOMDP dynamics must not introduce transitive structure that cannot be finitely boundedby domain axioms. As this last requirement is vague, we provide an example. In the BoxWorld problem covered in thissection, we implicitly assume that all cities are accessible from each other via the drive action. If instead we had someunderlying road topology indicated by Conn(City : c1, City : c2) that restricted the drive action and we did not know thistopology in terms of prior knowledge specified as domain axioms, then the SDP algorithm would likely need to generaterepresentations for all possible topologies, thus likely leading to a value function of infinite “size.” Infinite-sized value func-tions can also occur when condition (1) is violated as we discuss in the next subsection. We discuss potential researchdirections to mitigate these observed deficiencies of the case representation in Section 7.1.S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788769Unfortunately, the FOADD solution approach has failed to scale to more complex problems used in the planning com-munity (particularly problems from the ICAPS 2004 and 2006 International Planning Competitions) since they typically usemore complex rewards, including those with universal quantifiers. Whereas problems with existentially quantified rewardsmay exhibit a finite-size optimal value function, this is rarely the case with universal rewards. Thus additional techniquesare required to handle this problem, as we discuss next.4.2. Decomposing universal rewardsIn first-order domains, we are often faced with universal reward expressions that assign some positive value to the worldstates satisfying a formula of the general form ∀ y φ( y, s), and 0 otherwise. For instance, in our BoxWorld problem, we maydefine a reward as having all boxes b at their assigned destination city c given by Dst(b, c):R(s) =∀b, c. Dst(b, c) ⊃ BoxIn(b, c, s) : 1¬“: 0(47)One difficulty with such rewards is that our case statements provide a piecewise-constant representation of the valuefunction. However, with universal rewards, the value function typically depends on the number of domain objects of interest.In our example, value at a state depends on the number of boxes not at their proper destination (since this can impact theminimum number of steps it will take to obtain the reward). So a t-stage-to-go value function in this case would have thefollowing characteristic structure (where we use English in place of first-order logic for readability):V t(s) = =∀b, c. Dst(b, c) ⊃ BoxIn(b, c, s) :1γ:One box not at destinationTwo boxes not at destination : γ 2......t − 1 boxes not at destination : γ t−1:Obviously, since there are t distinct values in an optimal t-stage-to-go value function, the piecewise-constant case repre-sentation requires a minimum of t case partitions to represent this value function. And when we combine these countingdynamics with other interacting processes in the FOMDP, we often see an uncontrollable combinatorial blowup in the num-ber of case partitions of value functions for FOMDPs with universally defined rewards. As noted by [33], effectively handlinguniversally quantified rewards is one of the most pressing issues in the practical solution of FOMDPs.To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards inMDPs [11,53,61,74]. We divide our solution into off-line and on-line components where the on-line component requires afinite-domain assumption in order to execute the policy.4.2.1. Offline generic goal solutionIntuitively, given a goal-oriented reward that assigns positive reward if ∀(cid:10)y G((cid:10)y, s) is satisfied, and zero otherwise, we candecompose it into a set of ground goals {G( (cid:10)y1), . . . , G( (cid:10)yn)} for all possible (cid:10)y j in a ground domain of interest. If we reach astate where all ground goals are true, then we have satisfied ∀ y G( y, s).Of course, our methods solve FOMDPs without knowledge of the specific domain, so the set of ground goals that will be∗) for a “generic”faced at run-time is unknown. Thus, in the offline FOMDP solution, we assume a generic ground goal G((cid:10)yobject vector (cid:10)y. Assuming that our universal reward takes an implicative form as it does in our reworked BoxWorldexample, the conditions in the antecedent (Dst(b, c)) indicate the goal objects of interest (all pairs (cid:3)b, c(cid:4) satisfying Dst(b, c))and the consequent of the implication indicates the specific goal G((cid:10)y, s) to be achieved for these objects (BoxIn(b, c, s)).∗It is easy to construct a generic instance of a reward function R G((cid:10)y∗)(s) given a single goal. In our BoxWorld example∗∗and cto denote our goal objects of interest G(b∗, c∗):we would introduce the distinguished constants b∗, crCaseG(b∗,c∗)(s) = BoxIn(b¬BoxIn(b∗, s)∗, c: 1∗, s) : 0(48)∗Given this simple reward, it is easy to derive a value function V G((cid:10)y∗)(s) for this FOMDP using SDP or the approximateFOMDP solution algorithms that we introduce in subsequent sections. V G((cid:10)y∗)(s) and its corresponding policy assume that(cid:10)yis the only object vector of interest satisfying relevant type constraints and goal preconditions in the domain. In ourrunning BoxWorld example, the optimal vCaseG(b∗,c∗)(s) would look very similar to Fig. 4 (or 6) with some differencesowing to the fact that our reward is defined in terms of constants brather than existentially quantified variables band c.and c∗∗We next derive Q-functions for each action Ai((cid:10)x) from the value function V G((cid:10)y∗)(s) for the “generic” domain:Q G((cid:10)y∗)( Ai, s) = ∃(cid:10)x. FODTR[V G((cid:10)y∗)(s), Ai((cid:10)x)](49)For our running BoxWorld example, we would derive qCaseG(b∗,c∗)( Ai, s) for Ai ∈ {unload, load, drive}.770S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Algorithm 1. EvalPolicy({qCaseG((cid:10)y∗)(·, ·)}, {G( (cid:10)y1), . . . , G( (cid:10)yn)}, s) → Ai ((cid:10)c).4.2.2. Online policy evaluationWith the offline solution (i.e., Q-function for each action) of a generic goal FOMDP in hand, we address the on-line problem of action selection for a specific domain instantiation given at run-time. We assume a set of ground goals{G( (cid:10)y1), . . . , G( (cid:10)yn)} corresponding to a specific finite domain given at run-time. If we assume that (typed) domain objectsare treated uniformly in the uninstantiated FOMDP, as is the case in many logistics and planning problems, then we obtainthe Q-function for any goal G( (cid:10)y j) by replacing all ground terms (cid:10)yin qCaseG((cid:10)y∗)( Ai, s) with the respective terms (cid:10)y j toobtain qCaseG( (cid:10)y j )( Ai, s).∗Returning to our running example, from the value function vCaseG(b∗,c∗)(s) we derived a Q-function qCaseG((cid:10)y∗)( Ai, s) foreach action Ai . If at run-time, we are given the three goals Dst(b1, paris), Dst(b2, berlin), and Dst(b3, rome), then we wouldsubstitute these goals into our Q-functions to obtain three goal-specific Q-functions for each action Ai :{qCaseG(b1,paris)( Ai, s), qCaseG(b2,berlin)( Ai, s), qCaseG(b3,rome)( Ai, s)}(50)Action selection requires finding an action that maximizes value with respect to the original universal reward. Follow-ing [11,53], we do this by treating the sum of the Q-values of any action in the subgoal MDPs as a measure of its Q-valuein the joint (original) MDP. Specifically, we assume that each goal contributes uniformly and additively to the reward,so the Q-function for an entire set of ground goals {G( (cid:10)y1), . . . , G( (cid:10)yn)} determined by our domain instantiation is just(cid:28)n1n qCaseG( (cid:10)y j )( Ai, s). Action selection (at run-time) in any ground state is realized by choosing the action with max-j=1imum additive Q-value. Naturally, we do not want to explicitly create the joint Q-function, but instead use an efficient“scoring” technique that evaluates potentially useful actions by iterating through the individual Q-functions as described inAlgorithm 1.While this additive and uniform decomposition may not be appropriate for all domains with goal-oriented universalrewards (and certainly offers no performance guarantees on account of its heuristic nature), we have found it to providereasonable results for domains such as BoxWorld as we empirically demonstrate in the next section. While our approachonly currently handles rewards with universal quantifiers, this reflects the form of many planning problems. Nonetheless,this technique could be extended for more complex universal rewards, the general open question being how to assign creditamong the constituents of such a reward.5. Linear-value approximation for FOMDPsPerhaps the greatest difficulty with the symbolic dynamic programming (SDP) approach and practical extensions dis-cussed in the last section is that the size of the value function case representation grows polynomially on each iteration andS. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788771thus exponentially in terms of the number of iterations.5 Similar growth can occur for the first-order formulae representingthe state partitions themselves. Once these formulae become too large to practically detect equivalence or inconsistency, allhope of obtaining a compact representation of the value function is lost as the number of partitions in the case represen-tation grow unboundedly with no practical means for simplification or pruning. Indeed, the SDP approaches above, usingboth FOADDs and universal reward decomposition, are incapable of producing value functions and policies competitive withother planners from the ICAPS 2004 and 2006 International Probabilistic Planning Competitions [32,50].Given that approximate solution techniques such as linear value approximation [19,36,71] have allowed MDP solutionsto scale far beyond the limits of exact algorithms, at the same time offering reasonable error guarantees, this suggestsgeneralizing linear value approximation techniques to FOMDPs. In this section, we generalize the LP methods for groundMDPs, discussed in Section 2, to the first-order case. This reduces the task of solving an FOMDP to that of obtaining goodweights for a set of basis functions that approximates the optimal value function. This requires the generalization of linearprograms to handle first-order constraints and further requires efficient extensions of solution methods such as constraintgeneration and variable elimination in cost networks to exploit the first-order structure of these constraints.To develop a completely automated linear-value approximation approach to FOMDPs we must address the issue of au-tomatic basis function construction; to do this, we adapt techniques proposed by [33]. With appropriate domain axiomsdefining legal states, our techniques provide fully first-order, non-grounded solutions to FOMDPs derived from PPDDL andcan compete with planners from the ICAPS 2004 and ICAPS 2006 International Probabilistic Planning Competitions.5.1. Benefits of linear-value approximationLinear-value approximation for FOMDPs is attractive for several reasons:• Given that much of the computation in linear value approximation reduces to solving LPs, this reduces the algorithmdesign space to the setup and solution of linear programs.• Since the size of linear-value approximations is fixed, it can be used to moderate the complexity of the resulting solutionalgorithm. This leads to a flexible solution approach that trades off approximation accuracy and computation.• Linear value approximation does not require extensive logical simplification in practice, just weight projections thatmake use of a theorem prover. This is a tremendous advantage over exact techniques that require substantial simplifi-cation in order to maintain a compact representation.• Linear value approximation have yielded reasonable empirical performance for ground and factored MDPs, suggestingpromise for its application to FOMDPs.• If we do not use additive reward decomposition techniques of Section 4.2 (which approximate the FOMDP model), thenwe can derive domain-independent error bounds on our resulting value function using Corollary 3.4.2.5.2. First-order linear-value representationWe represent a value function as a weighted sum of k first-order basis functions, denoted bi(s), each ideally containing asmall number of formulae that provide a first-order abstraction of state space:V (s) =k(cid:23)i=1w i · bi(s)(51)Throughout this section, we assume that each individual basis function bi(s) is represented by a case statement that is anexhaustive and disjoint partitioning of state space. This property will be useful when we define the backup operators next.However, two basis functions may assign non-zero values to overlapping regions of state space; in fact this can be quiteuseful for representing additively decomposable values.Such a linear value function representation can often provide a reasonable approximation of the exact value function,especially given the additive structure inherent in many real-world problems. For example, as argued in previous sections,many planning problems have additive reward functions or multiple goals, both of which lend themselves to approximationvia linearly additive basis functions. Unlike exact solution methods where value functions can grow exponentially in sizeduring the solution process and must be logically simplified, here we maintain the value function in a compact form thatrequires no simplification, just discovery of good weights.As an example, consider approximation of the value function for our BoxWorld FOMDP from the last section, using thefollowing basis functions (we refer to specific instances of bi(s) as bCasei(s)):bCase1(s) =∃b. BoxIn(b, paris, s) : 1¬“: 05 In the worst case, a single case operation can yield a quadratic blowup in the number of case partitions in terms of the maximum number of casepartitions in its operands.772S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788bCase2(s) =bCase3(s) =∃b, t. BoxOn(b, t, s) : 1¬“: 0∃b, t. TruckIn(t, paris, s) ∧ BoxOn(b, t, s) : 1¬“: 0Then each instance of V (s) (denoted by vCase(s)) has the form:vCase(s) = [w 1 · bCase1(s)] ⊕ [w 2 · bCase2(s)] ⊕ [w 3 · bCase3(s)](52)(53)Each basis function is relatively small and represents a portion of state space to which we would expect to assign somepositive value in order to approximate the BoxWorld value function.5.2.1. Backup operatorsSuppose we are given a value function V (s). Backing up this value function through an action A((cid:10)x) yields a case state-ment containing the logical description of states that would give rise to V (s) after doing action A((cid:10)x), as well as the valuesthus obtained.However, due to the free variables in action A((cid:10)x), there are in fact two types of backups that we can perform. Thefirst, B A((cid:10)x)[·], regresses a value function through an action and produces a case statement with free variables for the actionparameters. The second, B A[·], existentially quantifies over the free variables (cid:10)x in B A((cid:10)x)[·]. Thus, the application of B A[·]results in a case description of the regressed value function indicating the values that could be achieved by any instantiationof A((cid:10)x) in the pre-action state.The definition of B A((cid:10)x)[·] is almost the same as the first-order decision theoretic regression (FODTR) operator fromEq. (30), except that we do not explicitly add in the reward. Slightly modifying our definitions from Section 3.3.3, welet n1((cid:10)x), . . . , nq((cid:10)x) be the set of Nature’s deterministic actions for stochastic action A((cid:10)x). Then we define B A((cid:10)x)[·] as follows:B A((cid:10)x)[V (s)] = γ(cid:2)q(cid:23)j=1{P (n j((cid:10)x), A((cid:10)x), s) ⊗ Regr(V (do(n j((cid:10)x), s)))}(54)(cid:4)Defining B A((cid:10)x)[·] in this way without the reward makes it a linear operator. Thus, if we apply this operator to our linear-value function representation, it distributes to each first-order basis function:B A((cid:10)x)[V (s)] = B A((cid:10)x)(cid:4)w i · bi(s)=(cid:2)k(cid:23)i=1k(cid:23)i=1w i · B A((cid:10)x)[bi(s)]Having defined B A((cid:10)x)[·], we now use it to define B A[·]:6B A((cid:10)x)[V (s)]B A[V (s)] = ∃(cid:10)x.(cid:7)(cid:8)Unfortunately, if we apply B A[·] to our linear-value function representation, we see that B A[·] is not necessarily linear:B A[V (s)] = B A(cid:2)k(cid:23)i=1(cid:4)(cid:24)w i · bi(s)= ∃(cid:10)x.k(cid:23)i=1(cid:25)w i · B A((cid:10)x)[bi(s)](55)(56)(57)The difficulty is that the existential quantification of B A[·] jointly constrains the backup of all basis functions that containthe existentially quantified variable as a free variable.These problems can be mitigated, however. We begin with a few definitions.Definition 5.2.1. We say that a deterministic action n j((cid:10)x) affects a fluent F if there is a positive or negative effect axiomthat contains a = n j((cid:10)x) in the body of the axiom and F in the head (cf., Section 3.2.2). We say that a stochastic action A((cid:10)x)affects a fluent F if at least one of Nature’s choices n j((cid:10)x) for A((cid:10)x) affects F . Finally, a formula φ is affected by a stochasticaction A((cid:10)x) iff φ contains a fluent affected by A((cid:10)x). Since a case statement is defined as a logical formula, this definitionextends to case statements in the obvious way.Property 5.2.2. When a basis function case statement bi(s) is affected by a stochastic action A((cid:10)x), B A((cid:10)x)[bi(s)] will contain the actionarguments (cid:10)x as free variables. The inverse of this property is also true: if a stochastic action A((cid:10)x) does not affect a basis function bi(s),B A((cid:10)x)[bi(s)] will not contain the action arguments as free variables.6 For simplicity, we assume that the reward is independent of the action arguments (cid:10)x, allowing us to exclude the reward from the ∃(cid:10)x operation of B A . Ifrequired, such dependencies could be added with appropriate adjustments to our definitions.S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788773To exploit this property, we let I++A denote the set of indices i for basis functions bi(s) that are affected by an action A((cid:10)x)−(so that for all i ∈ IA denote the set of indicesA , B A((cid:10)x)[bi(s)] contains none of the free variables (cid:10)x).of basis functions bi(s) not affected by an action (so that for all i ∈ IWe can exploit the fact that the ∃(cid:10)x is vacuous for case statements not containing free variables (cid:10)x and remove these termsfrom the scope of the ∃(cid:10)x quantification. This yields the following form for B A :A , B A((cid:10)x)[bi(s)] contains at least one of the free variables (cid:10)x). Likewise, we let I−(cid:20)(cid:23)B Ai(cid:22)w ibi(s)=(cid:15) (cid:23)i∈I−A(cid:17)w i B A((cid:10)x)[bi(s)]⊕ ∃(cid:10)x.(cid:17)w i B A((cid:10)x)[bi(s)](cid:15) (cid:23)i∈I+A(58)Consequently, if no fluent occurs in more than a few basis functions and no action affects more than a few fluents then wecan reasonably expect the result of applying B A to retain some additive structure. The first property can be controlled bythe appropriate design of basis functions. The second is true of typical planning domains.As a concrete example to demonstrate the backup operators and the exploitation of additive structure, let us computeBdrive[·] for our previously specified linear-value function from Eq. (53):Bdrive[vCase(s)] = ∃t= ∃t= ∃t= ∃t∗∗∗∗, c, c, c∗∗,c∗∗,cBdrive(tBdrive(t∗ {w 1 · Bdrive(t(cid:5)∗, cw 1 ·∗)[vCase(s)]∗)[w 1 · bCase1(s) ⊕ w 2 · bCase2(s) ⊕ w 3 · bCase3(s)]∗,c∗)[bCase1(s)] ⊕ w 2 · Bdrive(t∗,c∃b. BoxIn(b, paris, s) : 0.9¬“: 0∃b, t. [t = t∨TruckIn(t, paris, s)] ∧ BoxOn(b, t, s)¬“∗ ∧ c⊕ w 2 ·∗ = paris ∧ ∃c1TruckIn(t, c1, s)]∗)[bCase2(s)] ⊕ w 3 · Bdrive(t∃b, t. BoxOn(b, t, s) : 0.9¬“: 0: 0.9: 0∗,c∗)[bCase3(s)]}⎫⎬⎭(59)⊕ w 3 ·∗) action and thus their backupHere, we note that the first and second basis functions are not affected by the drive(tthrough this action is equivalent to a backup through a noop. Since the third basis function is affected by the actiondrive(tand cinto the result of its backup, we can push the quantifiersin to just this third case statement:∗) and this introduces the action parameters t∗, c∗, c∗∗Bdrive[vCase(s)] = ∃t∗∗, c= w 1 ·∗,c∗)[vCase(s)]. Bdrive(t∃b. BoxIn(b, paris, s) : 0.9¬“: 0⊕ w 2 ·∃b, t. BoxOn(b, t, s) : 0.9¬“: 0⊕ w 3 · ∃t∗∗, c⎧⎨⎩∗ ∧ c∗ = paris ∧ ∃c1TruckIn(t, c1, s)]∃b, t. [t = t∨TruckIn(t, paris, s)] ∧ BoxOn(b, t, s)¬“⎫⎬⎭: 0.9: 0Finally, we carry out the explicit ∃tthe case partitions and simplify. This allows us to remove the ∃tdomain assumption:∗∗, coperation on the third case statement where we distribute the quantifiers insideby rewriting equalities and exploiting the non-empty∗, c∗Bdrive[vCase(s)] = ∃t∗∗, c= w 1 ·∗,c∗)[vCase(s)]. Bdrive(t∃b. BoxIn(b, paris, s) : 0.9¬“: 0∃b, t. BoxOn(b, t, s) : 0.9¬“: 0∃b, t. [(∃c1. TruckIn(t, c1, s)) ∨ TruckIn(t, paris, s)] ∧ BoxOn(b, t, s) : 0.9¬“: 0⊕ w 2 ·⊕ w 3 ·(60)This example demonstrates best case performance for B A[·], where an action only affects one basis function thus allowingthe other basis functions to be removed from the scope of the ∃(cid:10)x operator. Then the ∃(cid:10)x operator can be easily applied to asingle case statement without incurring a representational blowup that would otherwise occur if the ∃(cid:10)x ranged over a sumof case statements and the explicit “cross-sum” ⊕ was required.Of course, in many cases, more than one basis function will be affected by an action. For example, if we had computedBunload[vCase(s)], all three basis functions would have been affected by the action and we would have had to explicitlycompute the “cross-sum” ⊕ of the backups of all three basis functions. While this effectively counteracts many of thebenefits of linear-value approximation since additive structure can no longer be exploited, we will see that by generatingour basis functions in a restricted manner, we can often manage to avoid computing the explicit ⊕, even when all basisfunctions are affected by an action. We will discuss this further when we discuss basis function generation.774S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–7885.3. First-order approximate linear programmingWe now generalize the approximate linear programming (ALP) approach for MDPs (see Eq. (11)) to first-order MDPs. Ifwe simply substitute appropriate notation, we arrive at the following formulation of first-order ALP (FOALP) [68]:Variables: w i; ∀i (cid:3) kMinimize:(cid:3)k(cid:23)si=1w i · bi(s)(cid:2)Subject to: 0 (cid:2) R(s) ⊕ B A(cid:4)w i · bi(s)(cid:21)k(cid:23)i=1w i · bi(s); ∀ A, s(61)k(cid:23)i=1As with ALP, our variables are the weights of our basis functions and our objective is to minimize the sum of values over allstates s. We have one constraint for each stochastic action A (e.g., in BoxWorld, A ∈ {unload, load, drive}) and each state s.One advantage of FOALP over SDP is that it does not require a casemax, thus avoiding the representational blowup incurredby this step in SDP.7 Unfortunately, while the objective and constraints in ALP for a ground MDP range over a finite numberof states, this direct generalization to the FOALP approach for FOMDPs requires dealing with infinitely (or indefinitely) manystates s.Since we are summing over infinitely many states in the FOALP objective, it is ill-defined. Thus, we redefine the FOALPobjective in a manner that preserves the intention of the original approximate linear programming solution for MDPs. InALP (see Eq. (11)), the objective equally weights each state and minimizes the sum of the value function over all states.However, if we look at the case partitions (cid:3)φi(s), ti(cid:4) of each basis function bi(s) case statement, each case partition servesas an aggregate representation of ground states assigned equal value. Consequently, rather than count ground states inour FOALP objective—of which there will generally be an infinite number per partition—we suppose that each basis functionpartition is chosen because it represented a potentially useful partitioning of state space, and thus weight each case partitionequally. Consequently, we rewrite the FOALP objective as follows:(cid:3)k(cid:23)si=1w i · bi(s) =k(cid:23)(cid:3)w ii=1sbi(s) ∼k(cid:23)(cid:3)w ii=1(cid:3)φ j ,t j (cid:4)∈bit j|bi|We use |bi| to indicate the number of partitions in the ith basis function. This approach can be seen as aggregating stateswithin a basis function partition into one abstract state and then weighting each abstract state uniformly in importance. For(cid:28)ki=1 w i . Of course, this solutionthe case of 0–1 indicator basis functions as in Eq. (52), this yields a simple objective ofrequires approximating the original objective and thus FOALP does not represent an exact generalization of the groundALP approach to the first-order case. Nonetheless, we show that this approximation still leads to reasonable results in ourempirical evaluation.With the issue of the infinite objective resolved, this leaves us with one final problem—the infinite number of constraints(i.e., one for every state s). Fortunately, we can work around this since case statements are finite. Since the value ti foreach case partition (cid:3)φi(s), ti(cid:4) is constant over all situations satisfying the φi(s), we can explicitly sum over the casei(s)statements in each constraint to yield a single case statement representation of the constraints. The key observation here isthat the finite number of constraints represented in the single “flattened” case statement hold iff the original infinite set ofconstraints in Eq. (61) hold.To understand this, consider the constraints for the drive action in FOALP, substituting our previously defined basisfunctions bCasei(s) from Eq. (52) for bi(s), the results of the Bdrive operator for these basis functions from Eq. (60), and thereward definition for BoxWorld given by rCase(s) in Eq. (20) for R(s). We substitute all of these directly into the constraintof the form in Eq. (61) above to obtain:0 (cid:2)∃b.BoxIn(b, paris, s) : 10¬“: 0⊕ w 1 ·∃b. BoxIn(b, paris, s) : 0.9¬“: 0⊕ w 2 ·⊕ w 3 ·(cid:21) w 1 ·(cid:21) w 3 ·∃b, t. BoxOn(b, t, s) : 0.9¬“: 0∃b, t. [(∃c1. TruckIn(t, c1, s)) ∨ TruckIn(t, paris, s)] ∧ BoxOn(b, t, s) : 0.9¬“: 0∃b. BoxIn(b, paris, s) : 1¬“: 0∃b, t. TruckIn(t, paris, s) ∧ BoxOn(b, t, s) : 1¬“: 0∃b, t. BoxOn(b, t, s) : 1¬“: 0(cid:21) w 2 ·; ∀s7 The reasons for this are the same as for the lack of a max in the ground case as discussed in Section 2.2.3.(62)S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788775Next we perform an explicit ⊕ and (cid:21) for some of the case statements, simplify the resulting partitions, and distribute theweights into the partition values:0 (cid:2)∃b. BoxIn(b, paris, s) : 10 − 0.1 · w 1¬“0:⊕∃b, t. BoxOn(b, t, s) : −0.1 · w 2¬“0:⊕∃b, t. TruckIn(t, paris, s) ∧ BoxOn(b, t, s): −0.1 · w 3¬“ ∧ ∃b, t, c1. TruckIn(t, c1, s) ∧ BoxOn(b, t, s) : 0.9 · w 3¬“0:; ∀s(63)To maintain our representation in a compact and perspicuous form, we define the following propositional renamings for thefirst-order formulae in these case statements:8φ1(s) ≡ ∃b. BoxIn(b, paris, s)φ2(s) ≡ ∃b, t. BoxOn(b, t, s)φ3(s) ≡ ∃b, t. TruckIn(t, paris, s) ∧ BoxOn(b, t, s)φ4(s) ≡ ∃b, t, c1. TruckIn(t, c1, s) ∧ BoxOn(b, t, s)Finally, we fully expand the ⊕ to obtain an explicit representation of all FOALP constraints for the drive action in ourBoxWorld example:φ1(s) ∧ φ2(s) ∧ φ3(s): 0 (cid:2) 10 − 0.1 · w 1 + −0.1 · w 2 + −0.1 · w 3φ1(s) ∧ φ2(s) ∧ ¬φ3(s) ∧ φ4(s): 0 (cid:2) 10 − 0.1 · w 1 + −0.1 · w 2 + 0.9 · w 3φ1(s) ∧ φ2(s) ∧ ¬φ3(s) ∧ ¬φ4(s)0 (cid:2) 10 − 0.1 · w 1 + −0.1 · w 2:φ1(s) ∧ ¬φ2(s) ∧ φ3(s)0 (cid:2) 10 − 0.1 · w 1 + −0.1 · w 3:φ1(s) ∧ ¬φ2(s) ∧ ¬φ3(s) ∧ φ4(s)0 (cid:2) 10 − 0.1 · w 1 + 0.9 · w 3:φ1(s) ∧ ¬φ2(s) ∧ ¬φ3(s) ∧ ¬φ4(s)0 (cid:2) 10 − 0.1 · w 1 + −0.1 · w 2:¬φ1(s) ∧ φ2(s) ∧ φ3(s)0 (cid:2) −0.1 · w 2 + −0.1 · w 3:¬φ1(s) ∧ φ2(s) ∧ ¬φ3(s) ∧ φ4(s)0 (cid:2) −0.1 · w 2 + 0.9 · w 3:¬φ1(s) ∧ φ2(s) ∧ ¬φ3(s) ∧ ¬φ4(s)0 (cid:2) −0.1 · w 2:0 (cid:2) −0.1 · w 3¬φ1(s) ∧ ¬φ2(s) ∧ φ3(s):0 (cid:2) 0.9 · w 3¬φ1(s) ∧ ¬φ2(s) ∧ ¬φ3(s) ∧ φ4(s):0 (cid:2) 0¬φ1(s) ∧ ¬φ2(s) ∧ ¬φ3(s) ∧ ¬φ4(s) :; ∀s(64)Here, if we had detected that any partition formula had been inconsistent, we would have removed it and the correspondingconstraint.While we note that technically there are an infinite number of constraints (one for every possible state s), there are onlya finite number of distinct constraints. In fact, the case representation conveniently partitions the state space into regionswith the same constraint. Thus, to solve the FOALP problem, we could enumerate all consistent constraints for every actionand then directly solve the resulting LP. In addition to the above constraints for the drive action in BoxWorld, this approachwould require us to carry out a similar procedure for the unload, load, and noop actions; however, once we did this, wewould have all of the constraints necessary for solving the FOALP first-order linear program specification.However, as the number of basis functions increases, the number of constraints can grow exponentially in the numberof case statements in the constraint. To tackle this problem, we examine the underlying optimization problem in the nextsection.5.4. First-order linear programsWe can restate the FOALP problem as the optimal solution to a general first-order linear program (FOLP) for which weprovide a generic solution. A FOLP is nothing more than a standard linear program where the constraints are written interms of a sum of case statements whose case partition values may be specified as linear combinations of the weights.Efficiently solving FOLPs poses a number of difficulties—and we tackle these difficulties next.5.4.1. General formulationA FOLP is specified as follows:8 One will note that the renaming of first-order formulae with “propositional” variables is in the same spirit as FOADDs. Consequently, we note thatFOADDs prove to be an efficient method for representing and performing operations on the constraints that occur in FOALP.776S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Algorithm 2. FOMax(C, (cid:3)R1 . . . Rn(cid:4)) −→ (cid:3)S, v(cid:4).Variables: w 1, . . . , wk ;Minimize:k(cid:3)i=1ci w iSubject to: 0 (cid:2) case1,1( (cid:10)w, s) ⊕ · · · ⊕ case1,l(1)( (cid:10)w, s); ∀s:0 (cid:2) casem,1( (cid:10)w, s) ⊕ · · · ⊕ casem,l(n)( (cid:10)w, s); ∀s(65)The k variables (cid:10)w = (cid:3)w 1, . . . , wk(cid:4) and objective weights (cid:10)c = (cid:3)c1, . . . , ck(cid:4) are defined as in a typical LP, the main differencebeing the form of the constraints. Here we have m different constraints of varying length l( j) (i.e., the number of casestatements in constraint j, 1 (cid:3) j (cid:3) n). We allow the ti in each partition (cid:3)φi, ti(cid:4) of case( (cid:10)w, s) to be linearly dependent on theweights (cid:10)w (e.g., ti = 3w 1 + 2w 2). We note that the first-order LP for FOALP can be cast in this general form. As previouslydiscussed in our FOALP example, we could simply compute the explicit “cross-sum” ⊕ to flatten out each constraint j into asingle case statement as in Eq. (64). However, this could be inefficient as it scales exponentially in the number of summedcase statements. Fortunately, we can extend constraint generation methods used in factored MDPs [71] to the first-ordercase as we show next.S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–7887775.4.2. First-order cost network maximizationIn the constraint generation approach to solving a FOLP, the most important operation is to find a most-violated con-straint given a current solution (i.e., setting of weights (cid:10)w). In this section, we formulate this problem as maximization overa first-order generalization of a cost network [23] represented as follows:0 (cid:2) maxs[case1( (cid:10)w, s) ⊕ · · · ⊕ casen( (cid:10)w, s)](66)The use of maxs indicates that we are only interested in the single value (and corresponding case partitions contributing tothis value) that maximizes the RHS. casemax would be less efficient here since it would exhaustively enumerate all valuesand constraints when we only require the single maximal value and constraint.To determine the maxs with this form of the constraints, we define the FOMax algorithm (see Algorithm 2) to carry outthis computation. It is similar to variable elimination [90] or bucket elimination [23] (which makes a stronger connection toresolution), except that we use a simple ordered version of first-order resolution in place of propositional ordered resolution.Thus, we term this generalized variable elimination technique used by FOMax to be relation elimination.Ostensibly, relation elimination and the technique of first-order variable elimination (FOVE) [20,21,60] appear similar sincethey both deal with lifted versions of variable elimination. However, they fundamentally apply to different problems: FOVEdoes not permit quantified formulae in its representation, while relational elimination permits full first-order logic in itsrepresentation; furthermore, FOVE permits the representation of indefinite products and sums whereas relation eliminationonly permits finite products and sums. Here we require full first-order logic, but not indefinite products or sums. Whileit is beyond the scope of this article to delve into a detailed discussion, we note that both relation elimination and FOVEcan be combined when required; this occurs, for example, in FOALP approaches to factored FOMDP solutions (cf., [70] andChapter 6 of [67]).We provide a concrete example of FOMax and relation elimination in Fig. 7. Relation elimination proceeds analogouslyto variable elimination, except that we choose a relation R to eliminate at every step rather than a propositional variable.Elimination order can affect the time and space requirements of FOMax since eliminating R requires the “cross-sum” ⊕of all case statements containing R, incurring a polynomial blowup in the number of case statements being summed. Inpractice, we greedily eliminate the relation R at each step that minimizes this representational blowup, although this is notguaranteed to provide an optimal order.On any elimination step of FOMax, once all of the case statements containing R have been explicitly “cross-summed,”the next step is to determine whether any case partitions are inconsistent (via resolution) or θ -subsumed and dominatedin value (using the generalized θ subsumption operator (cid:27)θ [18] with respect to our background theory, similar to theapproach used by ReBeL [43]); in both cases, these partitions may be removed since they will never contribute to themaximally consistent partition. Once all relations have been eliminated, maximal case partitions and their values extractedFig. 7. An example use of FOMAx to find the maximally violated constraint during first-order constraint generation.778S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788from the remaining sum of case statements are used to generate the maximal value (and case partitions contributing to thisvalue).We note that the ordered resolution strategy we use in FOMax is not refutation-complete: it may loop indefinitely atan intermediate relation elimination step before finding a latter relation with which to resolve a contradiction. This isan unavoidable consequence of the fact that refutation resolution for general first-order theories is semi-decidable. Froma practical standpoint, it is necessary to bound the number of resolutions performed at each relation elimination step(100 clauses per elimination step in our experiments) to prevent non-termination of FOMax due to an infinite number ofresolutions. This incomplete theorem proving approach may generate unnecessary constraints corresponding to unsatisfiableregions of state space; while these constraints serve to overconstrain the set of feasible solutions, this has not led toinfeasibility problems in practice. Furthermore, we often omit the generalized θ -subsumption test (cid:27)θ since the savingsfrom this simplification does not outweigh its computational cost. This does not affect completeness since simplification isnot required for inconsistency detection.Finally, we remark that if the resolution procedure does finitely terminate before the inference limit is reached onevery step of FOMax, then the conjunction of case partition formulae returned by FOMax is guaranteed to be satisfiableas a consequence of the completeness of refutation resolution. Research on decidable resolution procedures for expressivesubsets of first-order logic [54] may pave the way for stronger completeness guarantees for FOMax in future work.5.4.3. First-order constraint generationWe can use the FOMax algorithm to find the maximal constraint violation when we have constraints of the form inEq. (66). This allows us to define the following first-order constraint generation algorithm where we have specified somesolution tolerance (cid:4):(1) Initialize LP with i = 0, (cid:10)w i = (cid:10)0, and empty constraint set.(2) For each constraint in the cost-network form of Eq. (66), find the maximally violated constraint C (if one exists) usingthe FOMax algorithm applied to the constraint instantiated with (cid:10)w i .(3) If C ’s constraint violation is larger than (cid:4), add C to the LP constraint set, otherwise return (cid:10)w i as solution.(5) Solve LP with new constraints to obtain (cid:10)w i+1, goto step 2.In first-order constraint generation, we initialize our LP with an initial setting of weights, but no constraints. Note thatthe initial weights (cid:10)w 0 = (cid:10)0 will violate at least one constraint in a FOMDP with non-zero reward. Then we alternate betweengenerating constraints based on maximal constraint violations at the current solution and re-solving the LP with theseadditional constraints. This process repeats until no constraints are violated and we have found the optimal solution. Inpractice, this approach typically generates far fewer constraints than the full exhaustive enumeration approach given byEq. (64). To provide intuitions for this, we refer back to the example of finding the most violated constraint in Fig. 7.Using first-order constraint generation, we now have a solution to the first-order LP from Eq. (65), thus providing ageneral solution for FOALP. At this point, the only step for FOALP that we have not automated is the generation of basisfunctions, which we discuss next.5.5. Automatic generation of basis functionsThe effective use of linear approximations requires a “good” set of basis functions, one that spans a space containinga good approximation to the true value function. Previous work has addressed the issue of basis function generation inground MDPs [51,57], while other work has addressed the inductive generation of first-order features or basis functionsfrom sampled experience [83,86]. Here we consider a deductive first-order basis function generation method that draws onthe work of [33]. Specifically, they use regressions of the reward as candidate basis functions for learning a value function.This technique has allowed them to generate fully or t-stage-to-go optimal policies for a range of BlocksWorld problems.We leverage a similar approach for generating candidate basis functions using regression, except that rather than usethese candidate basis functions to learn a value function, we fit their weights without sampling or grounding by usingFOALP. Algorithm 3 provides an overview of our basis function generation algorithm. The motivation for this approach is asfollows: if some portion of state space φ has value v > τ in an existing approximate value function for some non-trivialthreshold τ , then this suggests that states that can reach this region (i.e., found by Regr(φ) through some deterministicaction) should also have reasonable value. However, since we have already assigned value to φ, we want the new basisfunction to focus on the area of state space not covered by φ; thus we negate φ and conjoin it with Regr(φ).As a small example, given the initial weighted basis function bCase1(s) = w 1 · rCase(s) from BoxWorld,bCase1(s) = w 1 ·∃b. BoxIn(b, paris, s) : 10¬“: 0,we derive the following weighted basis function from bCase1(s) when considering deterministic action Ai = unloadS(bduring basis function generation:bCase2(s) = w 2 ·¬[∃b. BoxIn(b, paris, s)] ∧ [∃c. BoxOn(b¬“∗, t∗, s) ∧ TruckIn(t∗, paris, s)] : 1: 0(67)∗, t∗)(68)S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788779Algorithm 3. BasisGen(FOMDP, τ , n) −→ B.If one examines the form of these two basis functions, the inherent “orthogonality” between the new basis functionsand the ones from which they were derived allows for significant computational optimizations. For example, since the toppartition of bCase1(s) takes the form φ1 and the top partition of bCase2(s) takes the form ¬φ1 ∧ φ2, these two partitions aremutually exclusive and could never jointly contribute to the value of a state. Thus, when two basis functions are orthogonalin this manner, we can efficiently perform an explicit “cross-sum” ⊕ on them to obtain a single compact case statementrepresenting both weighted basis functions:bCase1,2(s) = bCase1(s) ⊕ bCase2(s)∃b. BoxIn(b, paris, s)¬[∃b. BoxIn(b, paris, s)] ∧ [∃c. BoxOn(b¬“=∗, t∗, s) ∧ TruckIn(t∗, paris, s)] ::: w 1 · 10w 20(69)This style of basis function generation also has many computational advantages for FOALP. To see this, we return to ouroriginal discussion concerning the fact that the B A[·] operator as defined in Eq. (58) will not be able to preserve additivestructure when all basis functions in the linear-value function representation are affected by the stochastic action A((cid:10)x).Recalling Property 5.2.2, if all basis functions are affected by A((cid:10)x), then the backup B A[·] of a sum of basis functions willrequire their explicit “cross-sum” since they will all have free variables (cid:10)x causing them to be summed with ∃(cid:10)x is applied.However, in the best case, if the explicit “cross-sum” was already pre-computed for orthogonal basis functions by mergingthem, then this blowup will not occur.Of course, since different actions generate different non-orthogonal basis functions from the same “parent” basis function,it will not generally hold that all basis functions are pairwise orthogonal to each other. Nonetheless, if we can exploit themutual orthogonality of subsets of the basis functions to efficiently carry-out their explicit “cross-sum”, then we can stillachieve an exponential time speedup relative to the worst-case of the B A[·] operator that requires the explicit computationof the “cross-sum”. To see how subsets of basis functions can be efficiently summed, we refer back to Eq. (69), whichprovides an example sum of two orthogonal basis functions. In general, any mutually orthogonal subset of basis functionscan be merged in this way.As a consequence, we can exploit properties of orthogonal basis function generation in FOALP to mitigate exponentialspace and time scaling in the number of basis functions, where worst-case exponential scaling arises at various points dueto the need to explicitly compute the “cross-sum” of the linear-value representation. While we do not claim this methodof basis function generation will be appropriate for all domains, we will demonstrate that it works reasonably well for thestochastic planning problems evaluated in the next section.780S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–7885.6. Empirical resultsWe evaluated FOALP on PPDDL planning problems from the ICAPS 2004 [50] and ICAPS 2006 [32] International Proba-bilistic Planning Competitions (IPPC). We divide the discussion of results according to each competition in order to reflectthe differences in the competition setup, the data collected, and the specific planners that entered each competition.We used the Vampire theorem prover and the CPLEX 9.0 LP solver9 in our FOALP implementation and applied BasisGen(Algorithm 3) to our FOMDP translation of these PPDDL domains, generated as described in Section 3.2.2. We additively de-composed universal rewards using the technique described in Section 4.2; we note that doing so prevents us from obtainingany approximation guarantees on the solution generated by FOALP.We provided FOALP with additional background theory axioms that were not encoded in the PPDDL source: if a fluentwas intended to have functional arguments in PPDDL (PPDDL does not make provisions for specifying this property explic-itly), we provide a background axiom stating this. So, for example, in our running BoxWorld example, we would providethe following functional constraint axioms:∀b, c1, c2, s. BoxIn(b, c1, s) ∧ BoxIn(b, c2, s) ⊃ c1 = c2∀t, c1, c2, s. TruckIn(t, c1, s) ∧ TruckIn(t, c2, s) ⊃ c1 = c2∀b, t1, t2, s. BoxOn(b, t1, s) ∧ BoxOn(b, t2, s) ⊃ t1 = t2In words, these axioms state that a box can only be in one city, a truck can only be in one city, and a box can only be on onetruck. Any search-based or inductive planner that is given an initial state respecting these constraints (which was always thecase in the competition instances) would never have to consider such erroneous states violating these constraints since theyare unreachable from non-erroneous states satisfying these constraints. However, FOALP has no initial state knowledge in itsoffline solution phase and will produce extremely poorly approximated value functions if it cannot rule out such erroneousstates as being inconsistent.The need for these constraints may be viewed as a major drawback of the FOALP approach and was the reason that,although FOALP entered the ICAPS 2006 Probabilistic Planning Competition, it did not compete on 6 of the 10 problemdomains (since these 6 problem domains were released at the start of the competition and rules prevented the plannersfrom being modified beyond this point). On the other hand, we note that functional constraints on fluents represent aminimal type of problem knowledge often easily encoded by the person specifying a PPDDL problem; the constraints forBoxWorld are a good example. As an aid to future non-grounding planners, we recommend that the capability to specifyfunctional constraints on fluents be incorporated in future versions of the PPDDL specification. If such constraints are knownto hold on all initial states, automated techniques based on reachability analysis could also be used to prove such constraintshold as well.In the following sections, we present proof-of-concept results comparing FOALP to other planners across a sampling ofproblems where FOALP has been able to generate policies for IPPC problems.5.6.1. ICAPS 2004 probabilistic planning competition problemsWe applied FOALP to the BoxWorld logistics and BlocksWorld probabilistic planning problems from the ICAPS 2004IPPC [50]. In the BoxWorld logistics problem, the domain objects consist of trucks, planes, boxes, and cities. The numberof boxes and cities varied in each problem instance, but there were always 5 trucks and 5 planes. Trucks and planes arerestricted to particular routes between cities in a problem instance-specific manner. The goal in BoxWorld was to deliverall boxes to their destination cities and there were costs associated with each action. The transition functions allowed fortrucks and planes to stochastically end up in destinations other than that intended by the execution of their respective driveand fly actions. BlocksWorld is just a stochastic version of the standard domain where blocks are moved between the tableand other stacks of blocks to form a goal configuration. In this version, a block may be dropped with some probability whilepicking it up or placing it on a stack.We stopped our offline basis function generation algorithm after iteration 7 in BasisGen (Algorithm 3) taking less than 2hours for both problems on a 2 Ghz Pentium with 2 Gb of RAM; iteration 8 could not complete due to memory constraints.We note that if we were not using the “orthogonal” basis function generation described in Section 5.5, we would not getpast iteration 2 of basis function generation (the system does not terminate within 10 hours at iteration 3); thus, theseoptimizations have substantially increased the number of basis functions for which FOALP is a viable solution option.We compared FOALP to the three other top-performing planners on these problems: NMRDPP is a temporal logic plannerwith human-coded control knowledge [77]; mGPT is an RTDP-based planner [10]; (Purdue-)Humans is a human-coded plan-ner, Classy is an inductive first-order policy iteration planner, and FF-Replan [85] (2004 version) is a deterministic replannerbased on FF [39]. Results for all of these planners are given in Fig. 8.Since FOALP was only able to complete 7 iterations of basis function generation, this effectively limits the lookaheadhorizon of our basis functions to 7 steps. A lookahead of 8 would be required to properly plan in the final BoxWorld prob-lem instance and thus FOALP failed on this instance. It is important to note that in comparing FOALP to the other planners,9 http://www.ilog.com/products/cplex/.S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788ProblemCompeting Probabilistic PlannersNMRDPPmGPTHumansClassyFF-Replanbx c10 b5bx c10 b10bx c10 b15bw b5bw b11bw b15bw b18bw b2143837604954794683522861840–494466397––41931712949448046946245637600495480468045542534627949448100459781FOALP4333660494480470464456Fig. 8. Cumulative reward of 5 planning systems and FOALP (100 run avg.) on the BoxWorld and BlocksWorld probabilistic planning problems from theICAPS 2004 IPPC (– indicates no data). BoxWorld problems are indicated by a prefix of bx and followed by the number of cities c and boxes b used in thedomain. BlocksWorld problems are indicated by a prefix of bw and followed by the number of blocks b used in the domain.NMRDPP and Humans used hand-coded control knowledge. FF-Replan was a very efficient search-based deterministic plan-ner that had a significant advantage because near-optimal policies in these specific goal-oriented problems can be obtainedby assuming that the highest probability action effects occur deterministically and making use of classical search-basedplanning techniques. The only autonomous fully stochastic planners were mGPT and Classy (itself an inductive first-orderplanning approach), and FOALP performs comparably to both of these planners and outperforms them by a considerablemargin on some problem instances.5.6.2. ICAPS 2006 probabilistic planning competition problemsWe now present results for FOALP on three problem domains from the ICAPS 2006 IPPC [32]: BlocksWorld, TireWorld,and Elevators.10 In BlocksWorld, there are blocks and a table and the goal is to stack and unstack blocks from eachother in an effort to achieve a goal configuration of the blocks with respect to the table. TireWorld is a relatively simpleproblem where the goal is to drive from a goal city to a destination city, while being able to pick up a spare tire in somecities. One stochastic outcome of driving between cities is that a tire may go flat and can only be fixed when a spare tireis present. Thus, routes with cities that contain spare tires are preferred to other routes that do not. Finally, Elevators isa problem with a grid-like state space. The horizontal dimension of the grid corresponds to positions on a floor and thevertical dimension corresponds to different floors. There may be elevators at each position that can move vertically betweenfloors. An agent can occupy one position on one floor and can move left or right between positions or can move into or outof an elevator if it is at the appropriate floor or position. Any elevator can be moved up or down independently of whetherthe agent resides in it. There can be gates at certain positions, which probabilistically teleport the agent back to the startposition of floor 1, position 1. Finally, there are a number of coins at different known positions and the goal is for the agentto retrieve them all.In all of the following results, BasisGen (Algorithm 3) was run for a four-hour fixed time limit on a 2 Ghz Pentium with2 Gb of RAM to generate solutions for successively larger sets of basis functions. At the four-hour mark, we halted thesolution process and used the largest (most recent) set of basis functions and weights for which FOALP had successfullyterminated. Since the offline solution time of 4 hours can be amortized over an indefinite number of instances for a givenproblem, we do not report this in the online policy evaluation times in the following results.In Figs. 9, 10, and 11, we provide data for FOALP and competing planners that specifies the number of problem instancessolved, the online solution generation time, and the average number of actions required to reach the goal in each successfulproblem. We compare to the following planners that entered the competition11: (1) FPG [17], which uses policy gradientsearch in a factored representation of the Q-functions; (2) sfDP [76], which uses ADD-based dynamic programming [38]with reachability constraints based on initial state knowledge; (3) Paragraph [49], which uses a probabilistic extension ofGraphplan [9] for probabilistic planning; (4) FF-Replan [88] (2006 version) is a deterministic replanner based on FF [39].We note that all planners in this competition aside from FOALP are ground planners in that they use a propositionalrepresentation of a PPDDL problem for a specific domain instantiation.The results vary by problem, so we explain each in turn. In TireWorld, FOALP’s policy allowed it to solve most problemsalthough its policy was suboptimal in the number of actions and % problems solved in comparison to FF-Replan. In this case,it appears that the approximation inherent in the FOALP approach fared poorly in comparison to a deterministic replannerlike FF-Replan that could perform nearly optimally on this problem. FOALP’s slow policy evaluation on this problem is dueto the transitive nature of the road connection topology and the lack of optimization in FOALP’s logical policy evaluator. InElevators, the top three planners including FOALP all performed comparably with the deterministic replanner performing10 In the ICAPS 2006 IPPC, FOALP ran on the three problems reported here as well as Exploding-BlocksWorld (not reported here). We do not report theExploding-BlocksWorld results since the competition version of the FOALP planner was restricted to use only the BlocksWorld subset of the Exploding-BlocksWorld problem description. In this section, we only show results for problems where FOALP was able to generate a policy for the full problemdescription.11 Not all planners ran on all of the problems in the competition. Furthermore, some planners did not provide results on all problem instances, this isnoted for each result plot.782S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788Fig. 9. A boxplot of performance of four planners on 15 instances of the TireWorld problem domain from the probabilistic track of the ICAPS 2006 IPPC.sfDP did not produce results for this problem; all other planners reported results for all instances.Fig. 10. A boxplot of performance of three planners on 15 instances of the Elevators problem domain from the probabilistic track of the ICAPS 2006 IPPC.sfDP and Paragraph did not produce results for these problems; FF-Replan and FPG did not report results for 2 and 3 problem instances, respectively.consistently faster than the others, again due to the suitability of this domain for deterministic replanning and the relativespeed of that approach. The goals in this domain are highly decomposable and FOALP thus benefited substantially fromits additive goal decomposition approach. In BlocksWorld, FOALP shows the best performance, solving more problems,S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788783Fig. 11. A boxplot of performance of four planners on 15 instances of the BlocksWorld problem domain from the probabilistic track of the ICAPS 2006IPPC. Paragraph did not produce results for these problems; FF-Replan, FPG, and sfDP did not report results for 1, 5, and 10 problem instances, respectively.taking less time on the hard instances (FPG did not report results for the 5 hardest instances, thus skewing its results),and reaching the goal with the fewer actions (sfDP did not report results for the 10 hardest instances, thus skewing itsresults). In this case, FOALP’s performance owes to two advantages: (1) first-order abstraction in BlocksWorld considerablyhelps the system avoid much of the combinatorial complexity that the ground planners face, and (2) the additive goaldecomposition, although not optimal for all BlocksWorld problems, performed very well on these problem instances.5.6.3. Summary of resultsIn summary, the first-order representation of FOALP seems to offer robust performance across a range of domain instancesizes and problems. However, as discussed at the end of Section 4.1.3, the case representation used by FOALP is a limitingfactor in its performance due to its inability to exploit value structure in problems requiring reasoning about universalrewards (for which suboptimal additive reward decomposition techniques were used) or transitive reachability (for whichthe deficiency is quite clear from the TireWorld results). We discuss potential research directions to mitigate these observeddeficiencies in Section 7.1.6. Related workIn this section, we review work related to that presented in this article across two important dimensions: deductive first-order decision-theoretic planners based on symbolic dynamic programming (SDP), and inductive lifted decision-theoreticplanners based on learning first-order representations of value functions, control knowledge, or policies from groundeddomain instantiations.6.1. Variants of symbolic dynamic programmingThere have been a variety of alternative exact approaches to solving relationally specified MDPs without grounding inthe spirit of SDP. Each of these approaches apply an SDP-like algorithm to their own first-order MDP representation. LikeSDP, these algorithms all have guarantees on domain-independent error bounds for the value functions they produce andcan produce exact domain-independent value functions when they exist. However, all of these approaches are restricted tosolve less expressive variants of relational MDPs than SDP as we describe below.First-order value iteration (FOVIA) [40,42] and the Relational Bellman algorithm (ReBel) [43] are value iteration algo-rithms that solve a restricted subclass of relational MDPs, most notably disallowing combined universal conditional effects(as defined in Section 3.1.1). Since universal conditional effects are a powerful planning formalism underlying the ADL ex-tension to STRIPS, it can be argued that this is a significant limitation of these alternate SDP approaches. Both have provided784S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788fully automated proof-of-concept results; we were able to directly compare SDP with FOADDs and ReBel on the BoxWorldproblem in Section 4.1.3. ReBel’s specialization for a less expressive subset of FOMDPs (still capturing BoxWorld, however)results in a substantial performance edge for this problem although both produce the same, exact solution. Results for ReBeland FOVIA are not available for the specific versions of the planning competition domains that we examined in Section 5.First-order decision diagrams (FODDs) [81] have been introduced to compactly represent case statements and to permitefficient application of symbolic dynamic programming operations to another restricted class of relational MDPs via valueiteration [80] and policy iteration [82].Since FODDs are very similar in spirit to the FOADDs we defined in Section 4.1, we enumerate some of the majordifferences between these two formalisms:(1) FODDs disallow explicit universal quantification. This prevents FODDs from being applied to relational MDPs withuniversal preconditions or alternating quantifiers in their effects, although importantly, they can handle universal con-ditional effects.(2) Unlike FOADDs, which are maintained in a canonical form, FODDs are maintained in a sorted format, but are notguaranteed to be in a canonical form. As such, they rely on a range of simplification rules to maintain compact rep-resentations. This approach has the advantage that some diagrams without a strict order can be exponentially morecompact than diagrams with a strict order [80]. However, rather than having a well-defined simplification algorithmleading to a canonical form, simplification in FODDs is somewhat open-ended and heuristic.(3) There is no need to reorder internal decision nodes after Regr in FODDs in order to maintain a canonical form. In thisway, Regr is more efficient in FODDs than in FOADDs. This results in value and policy iteration algorithms that can beperformed completely in terms of FODDs, unlike the current FOADD representation.(4) FODDs assume an implicit semantics where the maximal value is assumed for all instantiations of the free variables,thus precluding the need to perform explicit ∃x and casemax. In FOADDs, such operations would need to be performedexplicitly. As such, the use of FODDs can lead to very compact representations for decision-theoretic planning, but thissemantics may interfere with extensions of FODDs to handle universally quantified formulae.Consequently, FODDs represent an interesting alternative in the design space of data structures for the compact represen-tation of case statements. Nonetheless, the major limitation with respect to the work we present in this article is theirlimitations w.r.t. representing some forms of universal quantification. Ideally the best approach would be to combine theadvantages of FOADDs with those of FODDs. This is a non-trivial problem, however, and an interesting future researchdirection.6.2. Alternative lifted approaches to decision-theoretic planningThere are many alternative approaches to first-order decision-theoretic planning that reason inductively about sampledomain instances and sample trajectories to produce lifted value functions or policies. This stands as an alternative toreasoning symbolically about actions and rewards directly at a first-order level without grounding as done in this article.In one class of approaches, sampled experience from grounded domain instantiations is used to directly induce relationalrepresentations of value or Q-functions in a reinforcement learning approach. This can be done with pure reinforcementlearning using relational decision or regression trees to learn a value or Q-function [25], combining this with supervisedguidance [24], or using Gaussian processes and graph kernels over relational structures to learn a value or Q-function [31].A second approach uses experience sampled from ground domain instantiations to induce first-order policy repre-sentations. In one version, policies can be learned directly from sampled experience trajectories generated using otherplanners [84]. In a different vein, policies can be learned in an approximate policy iteration framework [87] that combinestrajectory sampling with policy updates derived from these trajectories. In this approach, sample experience trajectories canbe generated using planning heuristics [26] and/or random walks on problem sizes that are adaptively scaled as plannerperformance improves [27].A third inductive approach (that could also be used in conjunction with FOALP) allows first-order features to be learnedfrom experience rather than symbolically deriving them directly from the relational MDP specification as described in Sec-tion 5.5. In one approach, heuristic control knowledge represented in a first-order taxonomic syntax can be learned fromsolution trajectories on a given problem [86]. In another recent approach, relational basis functions can be learned fromsampled trajectories and then used in an approximate value iteration framework [83].Since the approaches in this subsection also produce first-order value functions or policies, it is important to compareand contrast them with the symbolic deductive approach we adopt. In this approach, our ideal objectives are threefold:(1) Obtaining domain-independent exact or bounded approximate solutions where possible while exploiting natural rela-tional and first-order planning structure.(2) Avoiding potential pitfalls of value functions and policies specific to biases from (small) sampled domain instantiations.(3) Avoiding an intractable representational blowup by grounding in the solution algorithm.S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788785In practice, the approaches advocated in this article are unable to effectively achieve objective (1): the heuristics (suchas universal reward decomposition from Section 4.2) required to apply our techniques to planning competition problemsprevent the derivation of bounds. Objective (2) may be met in practice, although the approximations required for practicalapplications introduce their own representational biases. Finally, objective (3) may also be satisfied in practice, although thedomain-independent approach introduces its own representational blowup by effectively planning for every possible domaininstantiation.In comparison, inductive first-order approaches outlined above share a goal similar to (1) in exploiting natural relationalplanning structure in a domain-independent manner, but cannot claim to support (2) since they must sample. Theoreticalcomplexity results by [44,45] indicate that (3) is indeed possible to achieve for inductive approaches in some settings. Wefurther note that in practice, the bias and computational complexity inherent in sampling a small set of possible grounddomain instantiations of an MDP is not generally problematic since policies that work on one domain instantiation oftengeneralize to similar or larger domains given an appropriate representation language [87].So we may then ask: which first-order approach is better, inductive or deductive?12 Empirically, recent results [83] showthat inductive first-order approaches outperform FOALP. Is this the final answer? Hopefully not; but clearly there is still agreat deal of work to be done in order to make first-order deductive approaches fully competitive with recent state-of-the-art first-order inductive approaches. Perhaps even more promising though is the potential to combine advances among bothapproaches; [33] do this in work that combines inductive logic programming with first-order decision-theoretic regression,showing that optimal policies can be induced from few training samples if using deductive methods to generate candidatepolicy structure. Such approaches offer the hope of combining the best of both worlds while sharing the goal of exploitingfirst-order structure in relational decision-theoretic planning problems.7. Future directions and concluding remarksIn this article, we have motivated the need to exploit relational structure in decision-theoretic planning problems. To thisend, we have provided a thorough review of the FOMDP representation of [14] and showed how to translate an expressivesubset of PPDDL to this particular FOMDP representation. We reviewed the solution of FOMDPs via symbolic dynamic pro-gramming and contributed additional practical solution techniques based on the use of first-order ADDs (FOADDs), additivevalue decomposition of universal rewards, and first-order approximate linear programming (FOALP). Combining all of theseideas, we have provided proof-of-concept results from the probabilistic track of the ICAPS 2004 and 2006 InternationalPlanning Competitions.We outline some interesting directions for future work, and offer some concluding remarks on decision-theoretic plan-ning in the framework of FOMDPs.7.1. Future directionsThere are a number of open issues raised by our work that merit further exploration. We enumerate a few of them:(1) An interesting approach for the practical application of FOMDPs to decision-theoretic planning is to combine theirapproximate offline solution with online methods for enhancing their performance. We need only look at the rangeof successful planners used in planning competitions for ideas. Perhaps one of the most useful approaches would beto use offline methods for solving FOMDPs to generate a first-order approximated value function. Then we could usesuch a value function as a heuristic seed for online search methods such as RTDP [5,22]. Another approach wouldbe to consider domain-specific control knowledge encoded as temporal logic constraints as in TLPlan [3], programconstraints as in Golog [48] (both TLPlan and Golog are deterministic planners) or decision-theoretic extensions such asDT-Golog [15]. We discuss the use of program constraints further in a moment.(2) We did not explore approximate extensions of value iteration for FOMDPs. Given the success of the APRICODD plan-ner [75] that performs approximate value iteration using ADDs, this approach is quite appealing for first-order approxi-mate value iteration using FOADDs. When the FOADD representing the value function becomes too large, we can simplyprune out nodes in the FOADD in an effort to reduce the size of the value function while minimizing the approximationerror.(3) One promising use of FOMDPs is at the highest level of an abstraction hierarchy for agent-based decision-theoreticplanning. [22] demonstrate that an MDP model can be approximated to a structure that is efficiently solvable and thaterror bounds can be obtained on the resulting optimal policy in the abstracted model with respect to the optimal policyin the non-abstracted version. If we lift such results to FOMDPs, then this offers a very appealing paradigm for theiruse: we can approximate a general FOMDP model to a level that we know we can solve efficiently while obtaining errorbounds on the performance of the optimal policy in this approximated model. Or, further afield, we can use a solution12 To clarify, we use the term inductive to refer to any algorithm with an inductive component. However, it should be noted that all of the inductiveapproaches mentioned above incorporate some form of deduction by sampling from the Bellman equations then using induction to obtain a symbolicrepresentation from these samples. In contrast the SDP and FOALP approaches advocated in this article can be viewed as pure symbolic deduction sincethey deduce their value representations from a lifted version of the Bellman equation.786S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788to this approximated model as guidance for other more computationally expensive algorithms like ground heuristicsearch or as seed values [22] or shaped rewards [55] for value iteration in the non-abstracted MDP model.In addition to these immediate open problems posed by our techniques, we have only touched on the surface of FOMDPsand the vast array of stochastic decision processes and symbolic solution methods that are possible. There remain a numberof promising directions for the exploitation of structure in relationally-specified decision-theoretic planning problems thatwe briefly describe here:(1) One of the original goals in the FOMDP and symbolic dynamic programming frameworks [14] was to allow for verygeneral symbolic representations. While most current FOMDP research has assumed a constant numerical representationof the values in case statement partitions, there are many situations where we might obtain non-constant values in ourcase statements, e.g., compactly representing value functions in FOMDPs with universal rewards that depend on thecount of objects satisfying a property in a given situation, or in the context of modeling continuous state properties,perhaps combined with discrete state properties in a first-order generalization of hybrid MDPs [34,37]. However, as thecase statement is generalized to handle non-constant numerical representations, case operators like the casemax mustbe appropriately generalized to efficiently handle such value representations (see Section 6.2.3 of [67] for one exampleof such a casemax generalization). Furthermore, theorem provers must also be capable of reasoning about countingproperties or (constrained) continuous variables in such symbolic case statement enhancements in order to detect theinconsistency of state partitions.(2) In many FOMDPs there is an element of underlying topological graph structure. For example, in logistics planning,this graph structure may involve the accessibility of different cities via roads and flight routes. Currently, this graphstructure is not exploited by our solution methods. Yet its regularity, if known a priori, could likely be exploitable bysolution methods that could “compile” out this graph structure. This approach would be far more advantageous thanrelying on the first-order case representation to extract relevant graph properties using the cumbersome specificationof transitively composed relations (i.e., ∃c1, c2. Road(c1, c2) ∧ ∃c3. Road(c2, c3) ∧ ∃c4.Road(c3, c4) ∧ . . .).(3) We often have a predefined set of constraints on the behavior of an agent and we need to optimize the agent’s policywith respect to those constraints. If we can specify the program constraints in the form of a Golog program [48],then we can generalize the hierarchy of abstract machines (HAM) architecture [1,56] to the case of solving FOMDPswith respect to Golog program constraints. Such a solution would permit the (approximately) optimal execution of anincompletely specified program over all possible domain-instantiations. Various approaches in the decision-theoreticDT-Golog framework [15,28] have provided an initial investigation into these ideas.The above suggestions are but a few of the many possible extensions to the work presented in this article and first-orderdecision-theoretic planning in general.7.2. Concluding remarksFor a few years immediately succeeding the publication of the symbolic dynamic programming solution [14] to rela-tionally specified MDPs, this domain-independent non-grounding approach was disparaged as being unrealistic for practicalapplications due to the complexity of value functions or due to the need for logical simplification and theorem proving [30,35,84]. While these are all in fact significant obstacles to be overcome in the practical application of first-order MDPs todecision-theoretic planning, this article has aimed to show that these obstacles are not insurmountable. It has provideda substantial step in the direction of demonstrating that with careful attention paid to the first-order representation andalgorithms specifically designed to exploit that representation, non-grounded lifted solutions are viable in practice as wedemonstrated with our proof-of-concept results from the ICAPS 2004 and 2006 International Planning Competitions. Ourhope is that this article lays the foundations for further exploration of these non-grounding approaches and permits theintegration of these ideas with other lines of research in decision-theoretic planning.AcknowledgementsWe are grateful to the three anonymous reviewers for their extensive comments and suggestions: these have vastlyimproved the presentation and discussion of our work. We are also grateful to Kee Siong Ng who provided many suggestionsand corrections. This work was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada. Thisresearch was conducted while the first author (now with NICTA) was at the Department of Computer Science, University ofToronto. NICTA is funded by the Australian Government’s Backing Australia’s Ability and the Centre of Excellence programs.References[1] D. Andre, S. Russell, Programmable reinforcement learning agents, in: Advances in Neural Information Processing Systems (NIPS-01), vol. 13, 2001, pp.78–85.[2] F. Bacchus, J.Y. Halpern, H.J. Levesque, Reasoning about noisy sensors in the situation calculus, in: International Joint Conference on Artificial Intelligence(IJCAI-95), Montreal, 1995, pp. 1933–1940.S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788787[3] F. Bacchus, F. Kabanza, Using temporal logics to express search control knowledge for planning, Artificial Intelligence 116 (1–2) (2000) 123–191.[4] R.I. Bahar, E. Frohm, C. Gaona, G. Hachtel, E. Macii, A. Pardo, F. Somenzi, Algebraic decision diagrams and their applications, in: IEEE/ACM InternationalConference on CAD, 1993, pp. 428–432.[5] A.G. Barto, S.J. Bradtke, S.P. Singh, Learning to act using real-time dynamic programming, Tech. Rep. UM-CS-1993-002, U. Mass. Amherst, 1993.[6] R.E. Bellman, Dynamic Programming, Princeton University Press, Princeton, NJ, 1957.[7] D.P. Bertsekas, Dynamic Programming, Prentice Hall, Englewood Cliffs, NJ, 1987.[8] D.P. Bertsekas, J.N. Tsitsiklis, Neuro-Dynamic Programming, Athena Scientific, Belmont, MA, 1996.[9] A.L. Blum, M.L. Furst, Fast planning through graph analysis, in: IJCAI 95, Montreal, 1995, pp. 1636–1642.[10] B. Bonet, H. Geffner, mGPT: A probabilistic planner based on heuristic search, in: Online Proceedings for The Probabilistic Planning Track of IPC-04:http://www.cs.rutgers.edu/~mlittman/topics/ipc04-pt/proceedings/, 2004.[11] C. Boutilier, R.I. Brafman, C. Geib, Prioritized goal decomposition of Markov decision processes: Toward a synthesis of classical and decision theoreticplanning, in: International Joint Conference on Artificial Intelligence (IJCAI-97). Nagoya, 1997, pp. 1156–1162.[12] C. Boutilier, T. Dean, S. Hanks, Decision-theoretic planning: Structural assumptions and computational leverage, Journal of Artificial Intelligence Re-search (JAIR) 11 (1999) 1–94.[13] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller, Context-specific independence in Bayesian networks, in: Uncertainty in Artificial Intelligence (UAI-96), Portland, OR, 1996, pp. 115–123.[14] C. Boutilier, R. Reiter, B. Price, Symbolic dynamic programming for first-order MDPs, in: International Joint Conference on Artificial Intelligence (IJCAI-01), Seattle, 2001, pp. 690–697.[15] C. Boutilier, R. Reiter, M. Soutchanski, S. Thrun, Decision-theoretic, high-level agent programming in the situation calculus, in: AAAI-00, Austin, TX,2000, pp. 355–362.[16] R. Brachman, H. Levesque, Knowledge Representation and Reasoning, Morgan Kaufmann Publishers Inc., San Francisco, CA, 2004.[17] O. Buffet, D. Aberdeen, The factored policy gradient planner (ipc-06 version), in: Proceedings of the Fifth International Planning Competition, 2006.[18] W. Buntine, Generalized subsumption and its application to induction and redundancy, Artificial Intelligence 36 (1988) 375–399.[19] D. de Farias, B.V. Roy, The linear programming approach to approximate dynamic programming, Operations Research 51 (6) (2003) 850–865.[20] R. de Salvo Braz, E. Amir, D. Roth, Lifted first-order probabilistic inference, in: 19th International Joint Conference on Artificial Intelligence (IJCAI-2005),Edinburgh, UK, 2005, pp. 1319–1325.[21] R. de Salvo Braz, E. Amir, D. Roth, MPE and partial inversion in lifted probabilistic variable elimination, in: National Conference on Artificial Intelligence(AAAI-06), Boston, USA, 2006.[22] R. Dearden, C. Boutilier, Abstraction and approximate decision-theoretic planning, Artificial Intelligence 89 (12) (1997) 219–283.[23] R. Dechter, Bucket elimination: A unifying framework for reasoning, Artificial Intelligence 113 (1999) 41–85.[24] K. Driessens, S. Dzeroski, Integrating experimentation and guidance in relational reinforcement learning, in: International Conference on MachineLearning (ICML), 2002, pp. 115–122.[25] S. Dzeroski, L. DeRaedt, K. Driessens, Relational reinforcement learning, Machine Learning Journal (MLJ) 43 (2001) 7–52.[26] A. Fern, S. Yoon, R. Givan, Approximate policy iteration with a policy language bias, in: Advances in Neural Information Processing Systems 16 (NIPS-03), December 2003.[27] A. Fern, S. Yoon, R. Givan, Learning domain-specific control knowledge from random walks, in: International Conference on Planning and Scheduling(ICAPS-04), June 2004, pp. 191–199.[28] A. Ferrein, C. Fritz, G. Lakemeyer, Extending DTGolog with options, in: 18th International Joint Conference on Artificial Intelligence (IJCAI-2003), Aca-pulco, Mexico, 2003, pp. 144–151.[29] R.E. Fikes, N.J. Nilsson, STRIPS: A new approach to the application of theorem proving to problem solving, AI Journal 2 (1971) 189–208.[30] N.H. Gardiol, L.P. Kaelbling, Envelope-based planning in relational MDPs, in: Advances in Neural Information Processing Systems 16 (NIPS-03), Vancou-ver, CA, 2004, pp. 1040–1046.[31] T. Gartner, K. Driessens, J. Ramon, Graph kernels and Gaussian processes for relational reinforcement learning, Machine Learning Journal (MLJ) 64(2006) 91–119.[32] A. Gerevini, B. Bonet, B. Givan (Eds.), Online Proceedings for The Fifth International Planning Competition IPC-05: http://www.ldc.usb.ve/bonet/ipc5/docs/ipc-2006-booklet.pdf.gz, Lake District, UK, 2006.[33] C. Gretton, S. Thiebaux, Exploiting first-order regression in inductive policy selection, in: Uncertainty in Artificial Intelligence (UAI-04), Banff, Canada,2004, pp. 217–225.[34] C. Guestrin, M. Hauskrecht, B. Kveton, Solving factored MDPs with continuous and discrete variables, in: 20th Conference on Uncertainty in ArtificialIntelligence, 2004, pp. 235–242.[35] C. Guestrin, D. Koller, C. Gearhart, N. Kanodia, Generalizing plans to new environments in relational MDPs, in: 18th International Joint Conference onArtificial Intelligence (IJCAI-2003), Acapulco, Mexico, 2003, pp. 1003–1010.[36] C. Guestrin, D. Koller, R. Parr, S. Venktaraman, Efficient solution methods for factored MDPs, Journal of Artificial Intelligence Research (JAIR) 19 (2002)399–468.[37] M. Hauskrecht, B. Kveton, Linear program approximations for factored continuous-state Markov decision processes, in: Advances in Neural InformationProcessing Systems 16, 2004, pp. 895–902.[38] J. Hoey, R. St-Aubin, A. Hu, C. Boutilier, SPUDD: Stochastic planning using decision diagrams, in: Uncertainty in Artificial Intelligence (UAI-99), Stock-holm, 1999, pp. 279–288.[39] J. Hoffmann, B. Nebel, The FF planning system: Fast plan generation through heuristic search, Journal of Artificial Intelligence Research (JAIR) 14 (2001)253–302.[40] S. Hölldobler, E. Karabaev, O. Skvortsova, FluCaP: A heuristic search planner for first-order mdps, Journal of Artificial Intelligence Research (JAIR) 27(2006) 419–439.[41] R.A. Howard, Dynamic Programming and Markov Processes, MIT Press, 1960.[42] E. Karabaev, O. Skvortsova, A heuristic search algorithm for solving first-order MDPs, in: Uncertainty in Artificial Intelligence (UAI-05), Edinburgh,Scotland, 2005, pp. 292–299.[43] K. Kersting, M. van Otterlo, L. de Raedt, Bellman goes relational, in: International Conference on Machine Learning (ICML-04), ACM Press, 2004, pp.465–472.[44] R. Khardon, Learning action strategies for planning domains, Artificial Intelligence 113 (1–2) (1999) 125–148.[45] R. Khardon, Learning to take actions, Machine Learning 35 (1) (1999) 57–90.[46] D. Koller, R. Parr, Computing factored value functions for policies in structured MDPs, in: International Joint Conference on Artificial Intelligence (IJCAI-99), Stockholm, 1999, pp. 1332–1339.[47] D. Koller, R. Parr, Policy iteration for factored MDPs, in: Uncertainty in Artificial Intelligence (UAI-00), Stockholm, 2000, pp. 326–334.[48] H.J. Levesque, R. Reiter, Y. Lespérance, F. Lin, R. Scherl, GOLOG: A logic programming language for dynamic domains, Journal of Logic Program-ming 31 (1–3) (1997) 59–83.788S. Sanner, C. Boutilier / Artificial Intelligence 173 (2009) 748–788[49] I. Little, Paragraph: A Graphplan-based probabilistic planner, in: Proceedings of the Fifth International Planning Competition, 2006.[50] M.L. Littman, H.L.S. Younes (Eds.), Online Proceedings for The Probabilistic Planning Track of IPC-04: http://www.cs.rutgers.edu/mlittman/topics/ipc04-pt/proceedings/. Vancouver, Canada, 2004.[51] S. Mahadevan, Samuel meets Amarel: Automating value function approximation using global state space analysis, in: National Conference on ArtificialIntelligence (AAAI-05), Pittsburgh, 2005, pp. 1000–1005.[52] J. McCarthy, Situations, actions and causal laws, Tech. rep., Stanford University, 1963, reprinted, in: M. Minsky (Ed.), Semantic Information Processing,MIT Press, Cambridge, MA, 1968, pp. 410–417.[53] N. Meuleau, M. Hauskrecht, K.-E. Kim, L. Peshkin, L.P. Kaelbling, T. Dean, C. Boutilier, Solving very large weakly coupled Markov decision processes, in:National Conference on Artificial Intelligence (AAAI-98), Madison, WI, 1998, pp. 165–172.[54] B. Motik, Reasoning in description logics using resolution and deductive databases, Ph.D. thesis, Univesität Karlsruhe (TH), Karlsruhe, Germany, January2006.[55] A.Y. Ng, D. Harada, S. Russell, Policy invariance under reward transformations: theory and application to reward shaping, in: Proc. 16th InternationalConf. on Machine Learning, Morgan Kaufmann, San Francisco, CA, 1999, pp. 278–287.[56] R. Parr, S. Russell, Reinforcement learning with hierarchies of machines, in: M.M.K. Jordan, S. Solla (Eds.), Advances in Neural Information ProcessingSystems 10, MIT Press, Cambridge, MA, 1998, pp. 1043–1049.[57] R. Patrascu, P. Poupart, D. Schuurmans, C. Boutilier, C. Guestrin, Greedy linear value-approximation for factored Markov decision processes, in: NationalConference on Artificial Intelligence (AAAI-02), Edmonton, 2002, pp. 285–291.[58] E.P.D. Pednault, ADL: Exploring the middle ground between STRIPS and the situation calculus, in: KR, 1989, pp. 324–332.[59] D. Poole, The independent choice logic for modelling multiple agents under uncertainty, Artificial Intelligence 94 (1–2) (1997) 7–56.[60] D. Poole, First-order probabilistic inference, in: IJCAI, 2003, pp. 985–991.[61] P. Poupart, C. Boutilier, R. Patrascu, D. Schuurmans, Piecewise linear value function approximation for factored MDPs, in: National Conference onArtificial Intelligence (AAAI-02), Edmonton, 2002, pp. 292–299.[62] M.L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, Wiley, New York, 1994.[63] R. Reiter, The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression, in: V. Lifschitz (Ed.),Artificial Intelligence and Mathematical Theory of Computation (Papers in Honor of John McCarthy), Academic Press, San Diego, CA, 1991, pp. 359–380.[64] R. Reiter, Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems, MIT Press, 2001.[65] A. Riazanov, A. Voronkov, The design and implementation of vampire, AI Communications 15 (2) (2002) 91–110.[66] J. Rintanen, Expressive equivalence of formalisms for planning with sensing, in: 13th International Conference on Automated Planning and Scheduling,2003, pp. 185–194.[67] S. Sanner, First-order decision-theoretic planning in structured relational environments, Ph.D. thesis, University of Toronto, Toronto, ON, Canada, March2008.[68] S. Sanner, C. Boutilier, Approximate linear programming for first-order MDPs, in: Uncertainty in Artificial Intelligence (UAI-05), Edinburgh, Scotland,2005, pp. 509–517.[69] S. Sanner, C. Boutilier, Practical linear evaluation techniques for first-order MDPs, in: Uncertainty in Artificial Intelligence (UAI-06), Boston, MA, 2006.[70] S. Sanner, C. Boutilier, Approximate solution techniques for factored first-order MDPs, in: 17th International Conference on Automated Planning andScheduling (ICAPS-07), 2007, pp. 288–295.[71] D. Schuurmans, R. Patrascu, Direct value approximation for factored MDPs, in: Advances in Neural Information Processing 14 (NIPS-01), Vancouver,2001, pp. 1579–1586.[72] P. Schweitzer, A. Seidmann, Generalized polynomial approximations in Markovian decision processes, Journal of Mathematical Analysis and Applica-tions 110 (1985) 568–582.[73] L.S. Shapley, Stochastic games, Proceedings of the National Academy of Sciences 39 (1953) 327–332.[74] S.P. Singh, D. Cohn, How to dynamically merge Markov decision processes, in: Advances in Neural Information Processing Systems (NIPS-98), MIT Press,Cambridge, MA, 1998, pp. 1057–1063.[75] R. St-Aubin, J. Hoey, C. Boutilier, APRICODD: Approximate policy construction using decision diagrams, in: Advances in Neural Information Processing13 (NIPS-00), Denver, 2000, pp. 1089–1095.[76] F. Teichteil, P. Fabiani, Symbolic stochastic focused dynamic programming with decision diagrams, in: Proceedings of the Fifth International PlanningCompetition, 2006.[77] S. Thiebaux, C. Gretton, J. Slaney, D. Price, F. Kabanza, Decision-theoretic planning with non-Markovian rewards, Journal of Artificial Intelligence Re-search 25 (January 2006) 17–74.[78] J.N. Tsitsiklis, B. Van Roy, Feature-based methods for large scale dynamic programming, Machine Learning 22 (1996) 59–94.[79] M. Veloso, Learning by analogical reasoning in general problem solving, Ph.D. thesis, Carnegie Mellon University, August 1992.[80] C. Wang, S. Joshi, R. Khardon, First order decision diagrams for relational MDPs, in: Twentieth International Joint Conference on Artificial Intelligence(IJCAI-07), Hyderabad, India, 2007, pp. 1095–1100.[81] C. Wang, S. Joshi, R. Khardon, First order decision diagrams for relational MDPs, Journal of Artificial Intelligence Research (JAIR) 31 (2008) 431–472.[82] C. Wang, R. Khardon, Policy iteration for relational MDPs, in: Uncertainty in Artificial Intelligence (UAI-07), Vancouver, Canada, 2007.[83] J. Wu, R. Givan, Discovering relational domain features for probabilistic planning, in: 17th International Conference on Automated Planning andScheduling (ICAPS 2007), 2007, pp. 344–351.[84] S. Yoon, A. Fern, R. Givan, Inductive policy selection for first-order Markov decision processes, in: Uncertainty in Artificial Intelligence (UAI-02), Ed-monton, 2002, pp. 569–576.[85] S. Yoon, A. Fern, R. Givan, Learning reactive policies for probabilistic planning domains, in: Online Proceedings for The Probabilistic Planning Track ofIPC-04: http://www.cs.rutgers.edu/mlittman/topics/ipc04-pt/proceedings/, 2004.[86] S. Yoon, A. Fern, R. Givan, Learning measures of progress for planning domains, in: 20th National Conference on Artificial Intelligence, July 2005, pp.1217–1222.[87] S. Yoon, A. Fern, R. Givan, Approximate policy iteration with a policy language bias: Learning to solve relational Markov decision processes, Journal ofArtificial Intelligence Research (JAIR) 25 (2006) 85–118.[88] S. Yoon, A. Fern, R. Givan, FF-Replan: A baseline for probabilistic planning, in: 17th International Conference on Automated Planning and Scheduling(ICAPS-07), 2007, pp. 352–359.[89] H.L.S. Younes, M.L. Littman, D. Weissman, J. Asmuth, The first probabilistic track of the international planning competition, Journal of Artificial Intelli-gence Research (JAIR) 24 (2005) 851–887.[90] N.L. Zhang, D. Poole, A simple approach to Bayesian network computations, in: Proc. of the Tenth Canadian Conference on Artificial Intelligence, 1994,pp. 171–178.[91] N.L. Zhang, D. Poole, Exploiting causal independence in Bayesian network inference, Journal of Artificial Intelligence Research (JAIR) 5 (1996) 301–328.