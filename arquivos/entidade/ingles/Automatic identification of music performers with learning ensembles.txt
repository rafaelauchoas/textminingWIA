Artificial Intelligence 165 (2005) 37–56www.elsevier.com/locate/artintAutomatic identification of music performers withlearning ensemblesEfstathios Stamatatos a, Gerhard Widmer b,c,∗a Department of Information and Communication Systems Engineering,University of the Aegean, Samos, Greeceb Department of Computational Perception, Johannes Kepler University, Linz, Austriac Austrian Research Institute for Artificial Intelligence, Vienna, AustriaReceived 29 April 2004Available online 14 March 2005AbstractThis article addresses the problem of identifying the most likely music performer, given a set ofperformances of the same piece by a number of skilled candidate pianists. We propose a set of verysimple features for representing stylistic characteristics of a music performer, introducing ‘norm-based’ features that relate to a kind of ‘average’ performance. A database of piano performances of22 pianists playing two pieces by Frédéric Chopin is used in the presented experiments. Due to thelimitations of the training set size and the characteristics of the input features we propose an ensembleof simple classifiers derived by both subsampling the training set and subsampling the input features.Experiments show that the proposed features are able to quantify the differences between music per-formers. The proposed ensemble can efficiently cope with multi-class music performer recognitionunder inter-piece conditions, a difficult musical task, displaying a level of accuracy unlikely to bematched by human listeners (under similar conditions). 2005 Elsevier B.V. All rights reserved.Keywords: Machine learning; Classification; Ensemble learning; Music* Corresponding author.E-mail addresses: stamatatos@aegean.gr (E. Stamatatos), gerhard.widmer@jku.at (G. Widmer).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.01.00738E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–561. IntroductionThe representation of music as given in the printed score is not able to capture everymusical nuance. Hence, a piece played exactly as notated in the printed score would soundmechanical and highly unmusical. Expressive music performance is the act of ‘shaping’a piece of music according to the artist’s understanding of the structure (or ‘meaning’) ofthe piece. Every skilled performer continuously modifies important parameters, such astempo and loudness, in order to stress certain notes or ‘shape’ certain passages. Expressiveperformance is what makes music come alive and what distinguishes one performer fromanother (and what makes some performers famous).Because of its central role in our musical culture, expressive performance is a centralresearch topic in contemporary musicology. One main direction in empirical performanceresearch aims at formulating rules or principles of expressive performance either with thehelp of human experts [7] or by processing large volumes of data using machine learningtechniques [25,26]. Another direction of research is based on implicit knowledge extractedfrom recordings of human performers using case-based reasoning techniques [13]. Obvi-ously, these directions attempt to explore the similarities between skilled performers insimilar musical contexts. On the other hand, the differences between performers have notbeen studied thoroughly. Repp [17] presented a statistical analysis of temporal common-alities and differences among distinguished pianists’ interpretations of a well-known pieceand demonstrated the individuality of some famous pianists. However, the differences inmusic performance are still expressed generally with aesthetic criteria rather than quanti-tatively.In this paper, we use AI (specifically: machine learning) techniques in an attempt toexpress the individuality of music performers (pianists) in machine-interpretable terms byquantifying the main parameters of expressive performance. In order to avoid any sub-jective evaluation of our approach, we apply it to a well-defined problem: the automaticidentification of music performers, given a set of piano performances of the same pieceof music by a number of skilled candidate pianists. From this perspective, our task can beviewed as a typical classification problem, where the classes are the candidate pianists.1A set of features that represent the stylistic properties of a performer is proposed, introduc-ing the ‘norm performance’ as a reference point, while ideas taken from machine learningresearch are applied to the construction of the classifier. The dimensions of expressive vari-ation that will be taken into account are the three main expressive parameters available toa pianist: timing (variations in tempo), dynamics (variations in loudness), and articulation(the use of overlaps and pauses between successive notes).Experimental results show that it is indeed possible for a machine to distinguish musicperformers (pianists) on the basis of their performance style. We will show that successfullearning from extremely limited training data can be achieved by maximally exploiting thegiven information via ensemble learning (based on subsampling both the data and the inputfeatures). From the point of view of machine learning, this constitutes another supporting1 In a sense, this is also related to currently ongoing efforts in the area of Music Information Retrieval (MIR),where much work is devoted to the automatic classification of music recordings according to style or artist (see,e.g., [22]).E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–5639case for the utility of ensemble learning methods, specifically, the combination of a largenumber of independent simple ‘experts’ [3]. The contribution of this work to musicologyis the identification (via machine learning methodology) of a set of global characteristicsof performance style that seem to be relevant to distinguishing different artists.On the other hand, it must be stressed that the presented results are rather limitedbecause of the limited empirical data available for this investigation. Obtaining precisemeasurements, in terms of timing deviations, dynamics, and articulation, of performancesof highly skilled artists is a difficult task. We are currently investing a large amount of ef-fort into developing new methods for extracting expressive details from given recordingsand hope to be able to report on much more extensive experiments in the future.This paper is organized as follows: the next section contains a brief description of thedata and terminology used in this study. Section 3 describes the proposed features forthe quantification of the music performance style. Section 4 explains how preliminary ex-perimentation was performed in order to establish the main parameters and strategy forlearning. Section 5 then presents the ensemble learning experiments performed and theresults achieved. Finally, Section 6 discusses the major conclusions drawn from this studyand future work directions.2. Data and terminologyThe data used in this study consists of performances played and recorded on a Boe-sendorfer SE290 computer-monitored concert grand piano, which is able to measure everykey and pedal movement of the artist with very high precision. 22 skilled performers,including professional pianists, graduate students and professors of the Vienna Music Uni-versity, played two pieces by Frédéric Chopin: the Etude op.10 no.3 (first 21 bars) and theBallade op.38 (initial section, bars 1 to 45). The digital recordings were then transcribedinto symbolic form (MIDI) and matched against the printed score [4]. Thus, for each notein a piece we have precise information about how it was notated in the score, and how itwas actually played in a performance. The parameters of interest are the exact time whena note was played (vs. when it ‘should have been played’ according to the score)—thisrelates to tempo and timing—the dynamic level or loudness of a played note (dynamics),and the exact duration of a played note, and how the note is connected to the following one(articulation). All this can be readily computed from our data.2In the following, the term Inter-Onset Interval (IOI) will be used to denote the timeinterval between the onsets of two successive notes of the same voice. We define Off-Time Duration (OTD) as the time interval between the offset time of one note and theonset time of the next note of the same voice, and Dynamic Level (DL) as the ‘loudness’2 Note that this is only incomplete performance information. For example, we currently do not use informationabout the pedalling behaviour. Also other expressive information related to the produced sound, such as the‘attack’ or ‘touch’ of the notes (if there is such a thing—see [10]), cannot be measured in our data, because ourdata is derived from symbolic MIDI events.40E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56Fig. 1. The three main parameters used to characterize note-level performance details: Dynamic Level (DL),Inter-Onset Interval (IOI), and Off-Time Duration (OTD).of an individual note (in terms of the MIDI velocity parameter).3 The parameters are il-lustrated graphically in Fig. 1. The 22 pianists are referred to by their code names (#01,#02, etc.).3. Quantifying music performance style3.1. Score and normIf we define (somewhat simplistically) expressive performance as ‘intended deviationfrom the score’, then different performances differ in the way and extent the artist ‘deviates’from the score, i.e., from a purely mechanical (‘flat’) rendition of the piece, in terms oftiming, dynamics, and articulation. In order to be able to compare performances of piecesor sections of different length, we need to define features that characterize and quantifythese deviations at a global level, i.e., without reference to individual notes and how thesewere played.Fig. 2 shows the performances of the first 30 soprano notes of the Ballade by the pianists#01–#05 in terms of timing, expressed as the real duration (played inter-onset intervals) ofthe melody’s sixteenth notes,4 and dynamics. The default tempo and dynamic levels ofa non-expressive, purely mechanical interpretation of the score (with an arbitrarily fixedtempo and loudness level) would correspond to straight lines. As can be seen, the musicperformers tend to deviate from the default interpretation in a similar way in certain notesor passages. In the timing dimension, the last note of the first bar is considerably lengthened3 Generally, loudness is a rather complex concept. In our study, we are only interested in the relative loudnessat the time of onset of a note (not, e.g., in how the acoustic tone changes over time), which is essentially whatthe pianist can control, and what computer-controlled pianos measure. Onset loudness is measured in MIDI interms of the velocity with which the key was depressed—hence the name ‘velocity’ for the corresponding MIDIparameter.4 The sixteenth note is the shortest duration category appearing in the piece. IOIs longer than an eighth notewere divided into the appropriate number of virtual sixteenth notes for the figure. For instance, a played quarternote is divided into 4 sixteenths of equal duration, and that duration is then plotted in Fig. 2.E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–5641Fig. 2. Timing and dynamics variations for the first 30 soprano notes of Chopin’s Ballade op. 38 (score above) asperformed by pianists #01–#05. Default tempo and dynamic level, and performance norm derived from pianists#06–#10 are depicted as well.(last note of the introductory part) while in the dynamics dimension the first two bars areplayed with increasing intensity (introductory part), and the second soprano note of thefifth bar is played rather softly (a phrase boundary).These similarities in the performances remain when we take a more global look at curvesthat are smoothed over longer melodic passages. Fig. 3 (top) shows the timing deviationsof five pianists (#01–#05) from the printed score of the Chopin Etude (measured as themoving average of the difference between performed IOIs and the IOIs that would resultfrom a mechanical performance of the piece at a pre-specified fixed tempo). It is obviousthat all the pianists tend to deviate from the score in a similar way. That is not surpris-ing. It is well known that to a certain extent, expressive variation is correlated with thestructure of the piece of music (e.g., phrase structure, harmonic structure, etc.); indeed, ex-pressive performance is a means for the performer to communicate structural information42E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56Fig. 3. Smoothed timing deviation of the pianists #01–#05 from the printed score (above) and from the norm ofpianists #06–#10 (below) for the soprano notes of Chopin’s Etude op.10, no.3.to the listener. The peaks and dips of the resulting performance curves tend to correlate,more or less strongly, with phrase boundaries and phrase centers. Thus, if we decide (aswe will in the following sections) to take entire segments of a piece as training examplesand use as features global summarizations of a pianist’s tempo and dynamics deviationsacross those segments (rather than looking at single notes and detailed aspects of the mu-sic played, such as its phrase structure, harmonic structure, etc., which would have to beproduced by a labour-intensive ‘manual’ musical structure analysis), these global featureswill strongly depend on and vary with the training set. Sampling the training set fromslightly different segments of the same piece may substantially change the values of someattributes.This problem can be avoided by the use of what we call norm deviation features. Inaddition to the comparison of the performance of a certain pianist with the printed score,we propose the averaged performance of a different set of performers as a reference point.Fig. 2 shows such a norm performance, in terms of timing and dynamics on the single-notelevel, calculated from the performances of pianists #06–#10. As can be seen, the normfollows the basic form of the individual performances. Similarly, Fig. 3 (bottom) showsthe timing deviation of pianists #01–#05 from the average performance (i.e., norm) of thepianists #06–#10 for the Etude, in terms of smoothed differences over multiple-note pas-sages. The timing deviations of the first set of pianists from the norm of the second set aremore stable across the piece. This is a strong indication that the norm deviation featuresshould not be affected by slight changes to the training set. Given a set of reference perfor-mances, the norm deviation can be easily calculated for timing, dynamics, and articulation.E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56433.2. Melody leadAnother valuable source of information comes from the exploitation of the so-calledmelody lead phenomenon [9]. Notes that should be played simultaneously according tothe printed score (i.e., chords) are usually slightly spread out over time. A voice that is tobe emphasized tends to precede the other voices and is usually played louder. Studies ofthis phenomenon [15] showed that melody lead generally increases with expressivenessand skill level. Therefore, deviations between the notes of the same chord in terms oftiming and dynamics can provide useful features that capture an aspect of the stylisticcharacteristics of the music performer.3.3. The proposed featuresAs mentioned above, the training examples will be segments of a piece (more precisely,the melody) of a certain length, i.e., sequences of played notes. We propose the followingtypes of global features for characterizing such performed segments, given the printedscore and a performance norm derived from a given set of different performers:– Score deviation features:5D(IOIs, IOIm)D(IOIs, OTDm)D(DLs, DLm)timing,articulation,dynamics,– Norm deviation featuresD(IOIn, IOIm)D(OTDn, OTDm)D(DLn, DLm)timing,articulation,dynamics,– Melody lead features:D(ONxy, ONzy)D(DLxy, DLzy)timing,dynamics,where D(x, y) (a scalar) denotes the deviation of a vector of numeric values y from areference vector x, IOIs and DLs are the vectors of the nominal inter-onset intervals anddynamic-levels, respectively, according to the printed score, IOIn, OTDn, and DLn arethe inter-onset interval, the off-time duration, and the dynamic level, respectively, of theperformance norm, IOIm, OTDm, and DLm are the inter-onset interval, the off-time dura-tion, and the dynamic-level, respectively, of the actual performance, and ONxy , and DLxyare the on-time and the dynamic level, respectively, of a note of the xth voice within thechord y. The same score-based features have been used in previous work for successfullydiscriminating two skilled performers playing the same piano pieces [18].5 Note that articulation deviation from the printed score is calculated based on the score IOIs because the scoreOTDs would always be zero.44E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–564. Preliminary experimentsApplied machine learning is rarely confined to the simple application of an inductionalgorithm to a given data set. Many alternative learning algorithms are available, the datacan be filtered and represented in various ways, and even the question of what should beregarded as an individual training example is often a non-trivial one. The present project isa case in point. This section gives a brief account of a variety of preliminary experimentsthat had to be performed in order to establish some of the basic parameters of the learningtask—definition of features and training examples—and to provide an initial impressionof the difficulty of the problem. This is mainly to give the reader an appreciation of thekinds of ‘mundane’ processing and analysis steps that are often essential to the success ofempirical machine learning projects. It is the experiences gathered in these experiments thatprompted us to opt for the ensemble learning approach that will be described in Section 5.The reader interested only in the final results can safely skip Sections 4.2–4.4.4.1. Data and base-level classification algorithmIn the following experiments, Pianists #01–#12 will be used as the set of reference pi-anists to compute the norm performance. The task will be to learn to distinguish pianists#13–#22, which gives a 10-class classification problem. The ‘real’ experiments to be re-ported on in Section 5 will test learning and prediction in inter-piece conditions. There, theperformances of Chopin’s Ballade op. 38 will be used as the training material, and the per-formances of the Etude op.10/3 as the test cases. The preliminary ‘parameter determinationexperiments’ reported in the present section use only the training piece (the Ballade), sothat the optimizations performed will not in any way be influenced by knowledge of thedata eventually used for testing.The classification method used in the following experiments is discriminant analysis,a standard technique of multivariate statistics. The mathematical objective of this methodis to weight and linearly combine the input variables in such a way so that the classes areas statistically distinct as possible [6]. A set of linear functions (based on individual inputvariables and ordered according to their importance) is extracted on the basis of maximiz-ing between-class variance while minimizing within-class variance using a training set.Then, class membership of unseen cases can be predicted according to the Mahalanobisdistance from the classes’ centroids (the points that represent the means of all the trainingexamples of each class). The Mahalanobis distance d of a vector x from a mean vector mis defined as follows:(cid:2)d 2 = (x − m)(1)C−1x (x − m)where Cx is the covariance matrix of x. This classification method also supports the calcu-lation of posterior probabilities (the probability that an unseen case belongs to a particulargroup), which are proportional to the Mahalanobis distance from the classes centroids.A recent study [12] compared discriminant analysis to several more complex classificationmethods (from statistics, decision trees, and neural networks) and showed that discriminantanalysis is absolutely competitive when considering the compromise between classificationaccuracy and training time cost.E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56454.2. Selecting the appropriate distance typeFor measuring the deviation in each of the features defined in the previous section, dif-ferent types of distance could be applied. We decided to choose the appropriate type ofdistance for each feature category according to its statistical significance in a training set.For determining the best type of distance measure for each type of feature, the trainingpiece (the Ballade) was divided into four non-overlapping segments, each including 40 so-prano notes. For each segment of the performance of the piece by the pianists #13–#22,the values of the features were calculated for the following different types of distance (or,more correctly, deviation measures, as some of them are not distances, mathematicallyspeaking):Simple: Ds(x, y) = 1nn(cid:1)(xi − yi),Relative: Dr (x, y) = 1n(xi − yi)xi,i=1n(cid:1)i=1Simple absolute: Dsa(x, y) = 1nn(cid:1)(cid:2)(cid:3)|xi − yi|,Relative absolute: Dra(x, y) = 1n(|xi − yi|)xi.i=1n(cid:1)i=1(2)(3)(4)(5)Then, analysis of variance (ANOVA) was applied to these values for extracting conclu-sions about the statistical significance of the different types of distance and features. Themost significant features proved to be the deviation from the norm in terms of timing andarticulation, the timing deviation between the first and the third voice as well as betweenthe first and the fourth voice (the bass line), and the deviation from the score in terms oftiming and articulation. As regards the different types of distances, Dr gave the best re-sults for the score deviation features. This type of distance has been used previously forcomparing different performances [8]. Ds seems to be the appropriate choice for the normdeviation features. Dsa is the best distance type for the melody lead features, which indi-cates that information on whether a voice precedes or follows the first voice in a chord isnot as important as the degree to which the voices are separated in time and dynamics.64.3. Determining appropriate training examplesSince only two musical pieces are available in our data (one of which should serveas independent test piece), the training examples of the music performer classifier shouldconsist of piece segments rather than entire musical pieces. To determine the best mode of6 Interestingly, this observation is partly corroborated, from a different angle, by a very recent experimentallistening study with human subjects [11].46E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56Table 1Comparison of score and norm deviation measures for different types of distance and different methods of formingtraining examples (accuracies computed by leave-one-out cross-validation on the training set)ScoreNormDistanceDsDrDsaDraDsDrDsaDraAccuracy (%)Equal-lengthPhrase-based52.5604052.582.557.545205052.53042.577.5454520segmentation—equal length segments or segments based on the piece’s phrase structure—a simple experiment was performed. A number of simple classifiers, based on differenttypes of features and distance definitions, were trained via discriminant analysis on theperformances by pianists #13–#22 of the Ballade op. 38, with different methods of seg-menting the piece into training examples: in one case, the piece was segmented into fourparts of equal length (40 soprano notes each), in the other, it was cut into four parts ofdifferent length, according to phrase boundaries that were identified manually by a humanexpert. Table 1 shows the classification accuracies achieved (computed via leave-one-outcross-evaluation on the training data). As can be seen, in all cases the classifiers based ontraining examples of equal length gave better or equal accuracy results in comparison withthe phrase-based classifiers. This is a strong indication that the proposed features eithercannot benefit from or are independent of traditional musicological definitions of musi-cal structure (such as phrase structure). In addition, the norm deviation features generallyoutperformed the score deviation features.Another experiment was concerned with the length (in terms of soprano notes) of thetraining examples. Fig. 4 shows the relation of the length of the training examples with theclassification accuracy that can be obtained, again using Ballade op. 38 as testing groundand the norm deviation features. In this intra-piece condition, it turns out that the longer thesegments that constitute the training examples, the more accurate the classifier. The sameholds for the score deviation or the melody lead features. This means that for constructingreliable classifiers it is necessary to have training examples as long as possible, whichmakes for a rather small number of examples and in turn means that the number of inputfeatures per example (segment) should be rather small, in order to avoid overfitting ofthe training data (most learning methods—including discriminant analysis—are not ableto generalize well when given too many input features in comparison to the number oftraining examples per class).4.4. Performance of the simple classification modelIn order to provide an initial impression of the difficulty of the problem and to revealthe most important similarities and differences between the performers, a simple classifi-E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–5647Fig. 4. Classification accuracy vs. training example length (in soprano notes).cation model is first described. The performances of Ballade were segmented into 8 partsof equal length (20 soprano notes). These segments were then separated into two data sets,henceforth called Ballade-1 and Ballade-2, comprising the first four segments and the lastfour segments of each performance, respectively. Additionally, the performances of Etudewere segmented into 4 parts of equal length (20 soprano notes). Thus, three data sets eachone comprising four examples per class became available. This enabled us to perform bothintra-piece (training and test sets taken from the same piece) and inter-piece (training andtest sets taken from different pieces) experiments.Due to the restricted number of features that should be used concurrently in a classifier(because of the danger of over-fitting), distinct classifiers for norm-based and score-basedfeatures were developed. Hence, three score-based classifiers and three norm-based classi-fiers were constructed based on the training sets of Ballade-1, Ballade-2, and Etude. Thisalso enables the objective comparison between norm deviation and score deviation fea-tures. Fig. 5 depicts the class centroids in the space of the first two discriminant functions(which account for the greatest part of the total variation) derived from Ballade-1 andEtude, respectively, for both the norm-based and score-based features. Note that only therelative positions of the centroids can be compared, not the exact values of discriminantfunctions. As can be seen, in both cases the positions of the class centroids derived fromthe norm-based and the score-based features have many similarities.However, a closer look reveals that by using the norm-based features, the centroids aredistributed more widely along the first discriminant function (which by far accounts for thegreatest part of the total variation). Specifically, in the case of Ballade-1, the first discrim-inant function values of the centroids lie between −6.8 and 3.7 for norm-based featuresand between −3.9 and 2.1 for score-based features. The corresponding spans in the caseof Etude are between −3.8 and 5.7 for norm-based features and between −2.9 and 3.5 forscore-based features. Similar observations can be made for the second discriminant func-tion’s spans. This fact means that the norm-based features are better able to produce robust48E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56Fig. 5. The centroids of the pianists #13–#22 in the space of the first two discriminant functions for Ballade-1(above) and Etude (below). Norm-based models are shown on the left side and score-based models on the rightside. The numbers inside parentheses indicate the amount of variance explained by the corresponding function.and reliable classifiers since the classes are more widely spread within the classificationspace.The examination of the relative positions of centroids between Ballade-1 and Etudeindicates that many similarities and differences between performers remain constant ininter-piece conditions. For instance, in both data sets the classification models reveal aproximity between pianists #13 and #19, #16 and #18, #14 and #22, etc. Naturally, theserelations are much stronger between classification models extracted from segments of thesame piece (i.e., between Ballade-1 and Ballade-2).The results of applying the norm-based and score-based classifiers to new unseen mu-sical parts taken either from the same piece or from a different one are given in Table 2.Each classification model derived from a training set was applied to the other two datasets. In this experiment each test set consisted of a single case per class. To illustrate thisfurther, for instance, the classifier trained on the performances of Ballade-1 (the first halfof the piece) was applied to the performances of Ballade-2 (the second half of the samepiece) and the performances of Etude (a different piece), attempting to predict the mostlikely performer. To imitate this procedure, a human expert should first hear 10 differentperformances of the first half of a piece and then try to guess the performer of the secondhalf or of another piece. The results for all possible combinations of training and test setare given in Table 2.E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–5649Table 2Prediction results for norm-based and score-based classifiers; number of correct predictions (maximum possibleis 10)Training setNormScoreBallade-1Test setBallade-2Ballade-1Ballade-2EtudeBallade-1Ballade-2Etude–93–539–47–4Etude44–15–As can be seen, the results in intra-piece conditions (training set: Ballade-1, test set: Bal-lade-2, and vice versa) of the norm-based classifiers are quasi perfect (9 out of 10 correctpredictions), significantly better than the performance of the score-based classifiers.7On the other hand, in inter-piece conditions, the performance of the norm-based andscore-based classifiers is comparable. However, the norm-based classifiers are more robustor stable (3–4 correct predictions) in comparison to the score-based ones (correct predic-tions ranging from 1 to 5).5. Ensemble learningAs shown in the previous section, the efficiency of the individual classifiers, based ononly one feature source, is limited under inter-piece conditions. The characteristics of theproblem suggest the use of an ensemble of classifiers rather than a unique classifier. Recentresearch in machine learning [2,5,21] has studied thoroughly the construction of meta-classifiers. In this study, we take advantage of these techniques, constructing an ensembleof classifiers using two basic strategies:Subsampling the input features. This technique is usually applied when multiple redun-dant features are available. In our case, the input features should not be usedconcurrently due to the limited size of the training set (i.e., only a few trainingexamples per class are available) and the consequent danger of overfitting thetraining set.Subsampling the training data set. This technique is known to produce best results when‘unstable’ learning algorithms (i.e., algorithms that tend to produce strongly dif-ferent models from slightly different data) are used for constructing the baseclassifiers. In our case, a subset of the input features (i.e., the score deviation7 This should not come as a surprise. Obviously, the norm-based features along with the simple distance Dscan easily capture consistent differences in tempo and loudness between pianists, and these are much more likelyin intra-piece conditions: a pianist A playing the first half of a piece faster than pianist B will also tend to playthe second half faster than B (although that also need not be true for each passage of the piece). Things are morecomplex in inter-piece conditions, and with performance parameters such as articulation and melody lead.50E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56measures) is unstable—their values can change drastically given a slight changein the selected training segments.5.1. The proposed ensembleGiven the scarcity of training data and the multitude of possible features, we chose touse a relatively large number of rather simple individual base classifiers or ‘experts’, inthe terminology of [3]. Each expert is trained using a different set of features and/or partsof the training data. The features and sections of the training performances used for theindividual experts are listed in Table 3. C11 is based on the deviation of the performerfrom the norm. C21, C22, C23, and C24 are based on the deviation of the performer fromthe score and are trained using slightly changed training sets (because the score featuresare known to be unstable relative to changes in the data). The training set was divided intofour disjoint subsets and then four different overlapping training sets were constructed bydropping one of these four subsets (i.e., cross-validated committees [16]). Finally, C31,C32, C33, C34, and C35 are based on melody lead features. They differ in whether theyconsider inter-chord differences in timing and/or dynamics, and between which voices ofa chord. The last column in Table 3 shows the cross-validated accuracy of each individualexpert on the training data (i.e., in an intra-piece condition). As can be seen, the classifierbased on norm deviation features is by far the most accurate.The combination of the resulting simple classifiers or experts is realized via a weightedmajority scheme. The prediction of each individual classifier is weighted according to itsaccuracy on the training set [14]. Both the first and the second choice of a classifier aretaken into account. Specifically, the weight wij of classifier Cij is as follows:wij =aij(cid:4)xy axy(6)Table 3Description of the proposed simple classifiers. The third column indicates the number of training examples (andtheir length in terms of soprano notes) per classCodeInput featuresC11C21C22C23C24C31C32C33C34C35Ds (IOIn, IOIm), Ds (OTDn, OTDm), Ds (DLn, DLm)Dr (IOIs , IOIm), Dr (IOIs , OTDm), Dr (DLs , DLm)Dr (IOIs , IOIm), Dr (IOIs , OTDm), Dr (DLs , DLm)Dr (IOIs , IOIm), Dr (IOIs , OTDm), Dr (DLs , DLm)Dr (IOIs , IOIm), Dr (IOIs , OTDm), Dr (DLs , DLm)Dsa(ON1m, ON2m), Dsa(ON1m, ON3m), Dsa(ON1m, ON4m)Dsa(DL1m, DL2m), Dsa(DL1m, DL3m), Dsa(DL1m, DL4m)Dsa(ON1m, ON2m), Dsa(DL1m, DL2m)Dsa(ON1m, ON3m), Dsa(DL1m, DL3m)Dsa(ON1m, ON4m), Dsa(DL1m, DL4m)Trainingexamples4 × 4012 × 1012 × 1012 × 1012 × 104 × 404 × 404 × 404 × 404 × 40Acc. (%)82.550.844.846.748.357.542.525.035.047.5E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–5651where aij is the accuracy of the classifier Cij on the training set (see Table 3). aij /2 is usedto compute the weight for the second choice of a classifier. The class receiving the highestvotes is the final class prediction. Specifically, if cij (x) is the prediction of the classifierCij for the case x and P is the set of possible classes (i.e., pianists) then the final predictionˆc(x) isˆc(x) = arg maxp∈P(cid:1)ijwij × eq(cid:2)cij (x), p(cid:3)(7)where eq(a, b) = 1 if a is equal to b and 0 otherwise.5.2. Classification resultsFor testing the proposed ensemble under inter-piece conditions, the Ballade op. 38 wasused as the training material, and the Etude op.10/3 as the test piece. This is because theformer is the longer piece and therefore the acquisition of long training examples is pos-sible. The training piece (the Ballade) was divided into four non-overlapping segments of40 soprano notes each, which gives (only) four training cases per pianist, for each of theten target pianists #13–#22. The individual base classifiers as defined above were trainedon the performances of the Ballade by pianists #13–#22; pianists #01–#12 were used todefine the performance norm. Both the individual base classifiers and the combined en-semble classifier were then tested on an independent test piece, the Etude, which was usedin its entirety as one segment. Table 4 shows the classification results for the individualbase classifiers. The classification accuracies of the individual classifiers range between 30and 50%. The errors of norm deviation and score deviation classifiers are partially cor-related (i.e., common misclassifications: #16–#18, #19–#13, #20–#14, #21–#14). On theother hand, the errors of the melody lead classifiers seem quite independent of the others.Note that uncorrelated errors are crucial for the success of ensembles of classifiers [5].Table 5 shows the classification results of the ensemble classifier. The ensemble cor-rectly identified the pianist in 7 out of 10 cases, which gives an accuracy of 70%. OneTable 4Predictions of the individual simple classifiers on performances of the unseen test set (Etude op. 10/3). The firstcolumn indicates the code of the actual performer. Correct predictions are in boldface. Last row summarizescorrect guessesActualC11C21C22C23C24C31C32C33C34C35#13#14#15#16#17#18#19#20#21#22Correct:#13#14#21#18#17#13#13#14#14#224#13#21#21#18#17#13#19#21#14#173#16#14#14#16#17#16#19#14#14#194#13#22#21#18#17#18#13#14#14#193#18#22#14#18#17#18#13#14#14#223#13#21#15#16#15#17#16#20#17#164#13#21#13#16#17#17#19#20#17#165#13#13#15#19#16#22#19#14#13#153#13#21#17#16#16#18#16#14#21#164#13#15#13#16#21#14#19#20#14#16452E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56Table 5Predictions (first and second choice) of the ensemble of the simple classifiers on performances of the unseentest set (Etude op. 10/3). The first column indicates the code of the actual performer. Correct predictions are inboldface. Last row summarizes correct guessesActual1st choiceScore2nd choiceScore#13#14#15#16#17#18#19#20#21#22Correct:#13#14#21#16#17#18#19#14#14#2270.560.310.340.460.470.300.400.420.510.29#18#21#14#18#15#13#13#20#22#1610.230.290.250.340.160.260.270.220.150.25additional pianist (#20) is correctly recognised if we also admit the learner’s second choice.The ensemble thus performs substantially better than any of the constituent classifiers.Note that 70% is indeed a high success rate in a 10-way classification task with uni-formly distributed classes, where the ‘baseline’—the accuracy that can be achieved byintelligent guessing, i.e., by always predicting the most frequent class—is only 10%. Notealso that this would be a very difficult task for a human: imagine you first hear 10 differ-ent pianists performing one particular piece (and that is all you know about the pianists),and then you have to identify each of the 10 pianists in a recording of another (and quitedifferent) piece.8The score assigned to each prediction can be used as an indication of the classifier’scertainty. Thus, the classification of the performances by pianists #14, #18, and #22 arethe most difficult cases since the distance of the first choice from the second choice isless than 0.05, and the right decision was taken by the meta-classifier by a very narrowmargin. Looking at Table 4, we notice that none of the individual base classifiers managedto correctly predict all these three pianists. Obviously, it is only by combining the expertiseof the individual classifiers via a meta-classifier that this high success rate can be achieved.More detailed insight into the stability of the ensemble learner is provided by the fol-lowing investigation where, instead of just looking at prediction accuracy based on crisppredictions, we also consider the probability score (posterior probabilities) assigned toeach class for a particular test case. More specifically, the Mean Reciprocal Rank (MRR) isbased on the ordered list of classes (from most likely to least likely pianists) predicted bya classifier. For n test cases, the MRR of the ith classifier is defined asMRRi =n(cid:1)j =11Rij,(8)8 The interested reader can attempt to follow this procedure. The digital recordings used in this study can beaccessed at http://www.oefai.at/~wernerg/mp3.htm.E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–5653Table 6Performance of the base classifiers and the ensemble on the test set. FR and FA are calculated for threshold = 0.2ClassifierC11C21C22C23C24C31C32C33C34C35EnsembleAccuracy0.4000.3000.4000.3000.3000.4000.5000.3000.4000.4000.700MRR0.6080.5110.5550.5430.5270.6370.6180.4620.5890.5470.800FR0.6000.3000.4000.4000.3000.5000.5000.6000.4000.6000.200FA0.0670.1330.1220.1670.1670.1000.1110.1670.1220.1560.156where Rij is the rank of the true class of the j th case in the prediction produced by the ithclassifier. For 10 classes the MRR would range from 0.1 to 1.0. The higher the MRR, thebetter the ranking of the true classes in the ordered list of classifier answers.Alternatively, it is also possible to examine the types of errors committed by a classifier.Given a fixed confidence threshold, any prediction with an associated probability scoreabove the threshold is accepted. False Rejection (FR) and False Acceptance (FA) ratesthen provide useful information about the classifier. FR is defined as the ratio of rejectedperformances by the true pianists to the total performances of the true pianists (i.e., theratio of positive test examples missed). FA is the ratio of performances by other (‘wrong’)pianists accepted, to the total number of performances by other pianists. FR and FA arecalculated for a given confidence threshold. There is an obvious trade-off: when the con-fidence threshold is too low, FR tends to 0 and FA tends to 1 (all the performances areaccepted). On the other hand, when the confidence threshold is too high, FR tends to 1 andFA tends to 0 (only the performances with high score are accepted).Table 6 shows the performance of the base classifiers and the ensemble on the test set interms of accuracy of crisp predictions, MRR, FR, and FA (for threshold = 0.2). As can beseen, the MRR of the ensemble (consisting of 7 correct guesses, 1 second place guess, and2 fourth place guesses) is far better than the MRR of the base classifiers (C11, C31, andC32 are the most competent ones). Moreover, the ensemble has the lowest FR value (onlytwo of the correct choices are rejected) but many base classifiers have a better FA value(because of the trade-off between FR and FA). These results strengthen the credibility ofthe proposed ensemble since it is shown that the misses it produces are near-misses ratherthan random ones.6. DiscussionThe article has presented a computational approach to the problem of discriminatingbetween music performers playing the same piece of music, and introduced a set of simpleglobal features that capture some aspects of the individual style of each performer.54E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56Due to the limited data available and to certain characteristics of the discriminatingfeatures, we proposed a classification model that takes advantage of various techniquesof constructing meta-classifiers based on an ensemble of very simple classifiers, each onecapturing a nuance of the style of the music performer. In particular, by subsampling theinput features we manage to exploit all the different features that cannot be used concur-rently (to avoid overfitting of the training set), and by subsampling the training data, wetake advantage of the instability of a feature subset. Hence, it is demonstrated that a com-bination of AI techniques is able to deal with a very difficult problem, in effect displayinga level of accuracy unlikely to be matched by human listeners (under similar conditions).9The proposed features can be easily computed and do not make use of any piece-specificinformation (to be extracted by structural or harmonic analysis). They are global mea-sures extracted from multiple-note passages. An analysis of the induced classifiers andalso statistical analyses [19] provide insights into the relative importance of various typesof features. In particular, it turns out that features related to articulation (staccato vs. legato)and melody lead are the most informative, followed by aspects of tempo and timing and,finally, dynamics.This result relates in interesting ways to a very recent study by a musicologist [20],which investigated, via experiments with human subjects, the subjective perception ofsimilarity between expressive performances. The study showed quite clearly that humansubjects paid most attention to global characteristics of the performances. The most im-portant factors, according to the participants, were global tempo, rubato, and articulation,followed by loudness. In essence, the human listeners attend to the same parameters thatturned out to be most informative in our experiments. The study also showed that the sameparameters were important for both musicians and non-musicians. Statistical modellingof subjects’ similarity ratings revealed that global measures were more often included inoptimal models than local measures, and tempo features were more useful than dynamicsfeatures to explain the subjects’ similarity ratings; again, this is very much in agreementwith our machine learning results.The work described here was performed in the context of a large project whose goal isto study and characterize fundamental principles of expressive music performance with AImethods [23,24]. The current study can be seen as another attempt at quantifying featuresthat are crucial to understanding and modeling this complex phenomenon. However, froma musicological point of view, our current set of global, segment-based features, whileeasy to compute, do not give direct insights into the individual performance strategies ofmusicians. One would like to have features that explicitly describe the artist’s expressiveactions for every note (as, e.g., in [1]). That would require measurements at the note level,associated with particular local musical contexts and piece-specific information, as, e.g.,in [26]. Especially characterizing the musical contexts is a demanding task.9 Actually, a comparison with human listeners is not at all straightforward. It is very difficult to define what these‘similar conditions’ would be. How many times would a person be allowed to listen to each of the training/testrecordings in order to reach a level similar to that of feeding the preprocessed data to a classifier? What wouldbe the level of expertise of the listener? How could one account for the effects of previous knowledge of musicand musical style (which the computer does not have)? For these reasons, we avoided to provide a direct human-machine comparison for this task.E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–5655The reliability of our current results is still severely compromised by the very small setof empirical data that were available. It is planned to invest substantial effort into collectingand precisely measuring a larger and more diverse set of performances by a set of differentpianists (on a computer-controlled piano). Studying famous concert pianists with this ap-proach would require us to be able to precisely measure timing, dynamics, and articulationfrom sound recordings, which unfortunately still is an unsolved signal processing problem.AcknowledgementsThis work was supported by the EU project HPRN-CT-2000-00115 (MOSART), theSTART program of the Austrian Federal Ministry for Education, Science, and Culture(Grant no. Y99-INF), and the project P12645-INF, sponsored by the Austrian Fonds zurFörderung der wissenschaftlichen Forschung (FWF). The Austrian Research Institute forArtificial Intelligence acknowledges basic financial support from the Austrian FederalMinistry for Education, Science, and Culture. We would like to thank Werner Goebl forproviding the data used in the experiments. We are indebted to the anonymous reviewersfor very helpful comments and, in particular, the suggestion to perform the MRR experi-ment reported in Table 6.References[1] J.L. Arcos, M. Grachten, R. Lopez de Mantaras, Extracting performers’ behaviors to annotate cases in aCBR system for musical tempo transformations, in: Proceedings of the 5th International Conference onCase-Based Reasoning (ICCBR 2003), Trondheim, Norway, 2003, pp. 20–34.[2] E. Bauer, R. Kohavi, An empirical comparison of voting classification algorithms: bagging, boosting, andvariants, Machine Learning 39 (1/2) (1999) 105–139.[3] A. Blum, Empirical support for winnow and weighted-majority based algorithms: results on a calendarscheduling domain, Machine Learning 26 (1) (1997) 5–23.[4] E. Cambouropoulos, From MIDI to traditional music notation, in: Proc. of the AAAI’2000 Workshop onArtificial Intelligence and Music, 17th National Conf. on Artificial Intelligence, 2000, pp. 19–23.[5] T. Dietterich, Ensemble methods in machine learning, in: First Int. Workshop on Multiple Classifier Systems,2000, pp. 1–15.[6] R. Eisenbeis, R. Avery, Discriminant Analysis and Classification Procedures: Theory and Applications, D.C.Health and Co, Lexington, MA, 1972.[7] A. Friberg, Generative rules for music performance: a formal description of a rule system, Computer MusicJ. 15 (2) (1991) 56–71.[8] A. Friberg, Matching the rule parameters of phrase arch to performances of ‘Träumerei’: a preliminary study,in: Proc. of the KTH Symposium on Grammars for Music Performance, 1995, pp. 37–44.[9] W. Goebl, Melody lead in piano performance: expressive device or artifact?, J. Acoustical Soc. Amer. 110 (1)(2000) 563–572.[10] W. Goebl, R. Bresin, A. Galembo, Once again: the perception of piano touch and tone. Can touch audiblychange piano sound independently of intensity?, in: Proceedings of the 2004 International Symposium onMusic Acoustics (ISMA’04), Nara, Japan, 2004, pp. 332–335.[11] W. Goebl, R. Parncutt, Asynchrony versus intensity as cues for melody perception in chords and real music,in: Proc. 5th ESCOM Conference, Hannover, Germany, 2003, pp. 376–380.[12] T. Lim, W. Loh, Y. Shih, A comparison of prediction accuracy, complexity and training time of thirty-threeold and new classification accuracy, Machine Learning 40 (3) (2000) 203–228.56E. Stamatatos, G. Widmer / Artificial Intelligence 165 (2005) 37–56[13] R. López de Mántaras, J.L. Arcos, AI and music: from composition to expressive performances, AI Maga-zine 23 (3) (2002) 43–57.[14] D. Opitz, J. Shavlik, Generating accurate and diverse members of a neural network ensemble, in: D. Touret-zky, M. Mozer, M. Hasselmo (Eds.), Adv. Neural Inform. Process. Syst. 8 (1996) 535–541.[15] C. Palmer, On the assignment of structure in music performance, Music Perception 14 (1996) 23–56.[16] B. Parmanto, P.W. Munro, H.R. Doyle, Improving committee diagnosis with resampling techniques, in:D. Touretzky, M. Mozer, M. Hasselmo (Eds.), Adv. Neural Inform. Process. Syst. 8 (1996) 882–888.[17] B. Repp, Diversity and commonality in music performance: an analysis of timing microstructure in Schu-mann’s ‘Träumerei’, J. Acoustical Soc. Amer. 92 (5) (1992) 2546–2568.[18] E. Stamatatos, A computational model for discriminating music performers, in: Proceedings of theMOSART Workshop on Current Research Directions in Computer Music, 2001, pp. 65–69.[19] E. Stamatatos, Quantifying the differences between music performers, in: Proceedings of the InternationalComputer Music Conference (ICMC’2002), Göteborg, Sweden, 2002, pp. 376–382.[20] R. Timmers, Predicting the subjective similarity between expressive performances of music from objec-tive measurements of tempo and dynamics, Submitted for publication. Available as Report TR-2003-25,Austrian Research Institute for Artificial Intelligence, Vienna, 2003, http://www.ai.univie.ac.at/cgi-bin/tr-online?number+2003-25.[21] L. Todorovski, S. Dzeroski, Combining classifiers with meta decision trees, Machine Learning 50 (3) (2003)223–249.[22] G. Tzanetakis, P. Cook, Musical genre classification of audio signals, IEEE Trans. Speech AudioProcess. 10 (5) (2002) 293–302.[23] G. Widmer, Using AI and machine learning to study expressive music performance: project survey and firstreport, AI Comm. 14 (2001) 149–162.[24] G. Widmer, S. Dixon, E. Goebl, W. Pampalk, A. Tobudic, In search of the Horowitz factor, AI Maga-zine 24 (3) (2003) 111–130.[25] G. Widmer, Machine discoveries: a few simple, robust local expression principles, J. New Music Res. 31 (1)(2002) 37–50.[26] G. Widmer, Discovering simple rules in complex data: a meta-learning algorithm and some surprising mu-sical discoveries, Artificial Intelligence 146 (2) (2003) 129–148.