Reinforcement Learning in a Spiking Neural Model of Striatum Plasticity´Alvaro Gonz´alez-Redondoa,∗, Jes´us Garridoa, Francisco Naveros Arrabala,Jeanette Hellgren Kotaleskib, Sten Grillnerb, Eduardo RosaaResearch Centre for Information and Communications Technologies (CITIC-UGR). Calle Periodista RafaelG´omez Montero 2, E18071 Granada, Spain.bKungliga Tekniska H¨ogskolan, SE-100 44, Stockholm, SwedenAbstractThe basal ganglia (BG), and more specifically the striatum, have long been proposed to playan essential role in action-selection based on a reinforcement learning (RL) paradigm. However,some recent findings, such as striatal spike-timing-dependent plasticity (STDP) or striatal lateralconnectivity, require further research and modelling as their respective roles are still not well under-stood. Theoretical models of spiking neurons with homeostatic mechanisms, lateral connectivity,and reward-modulated STDP have demonstrated a remarkable capability to learn sensorial pat-terns that statistically correlate with a rewarding signal. In this article, we implement a functionaland biologically inspired network model of the striatum, where learning is based on a previouslyproposed learning rule called spike-timing-dependent eligibility (STDE), which captures importantexperimental features in the striatum. The proposed computational model can recognize com-plex input patterns and consistently choose rewarded actions to respond to such sensorial inputs.Moreover, we assess the role different neuronal and network features, such as homeostatic mech-anisms and lateral inhibitory connections, play in action-selection with the proposed model. Thehomeostatic mechanisms make learning more robust (in terms of suitable parameters) and facili-tate recovery after rewarding policy swapping, while lateral inhibitory connections are importantwhen multiple input patterns are associated with the same rewarded action. Finally, according toour simulations, the optimal delay between the action and the dopaminergic feedback is obtainedaround 300ms, as demonstrated in previous studies of RL and in biological studies.Keywords: Striatum, Reinforcement learning, Spiking neural network, Dopamine, Eligibilitytrace, Spike-timing-dependent plasticity1. IntroductionAnimals learn to choose actions among manyoptions by trial and error, thanks to the feed-back provided by sparse and delayed rewards.Reinforcement learning (RL) serves as a theo-retical framework for an agent, a system that5∗Corresponding authorEmail address: alvarogr@ugr.es( ´Alvaro Gonz´alez-Redondo)Preprint submitted to Neurocomputingacts based on received feedback, to learn tomap situations to actions. This state-actionmapping aims to maximize the performance ofactions, mainly (but not exclusively) consid-ering how rewarding or punishing the conse-quences of the actions are (Sutton et al., 1992).The basal ganglia (BG), a group of forebrainnuclei, are posited to play a critical role inaction-selection based on RL (Grillner et al.,2005; Graybiel, 1998; Hikosaka et al., 2000;1015June 2, 2023Gurney et al., 2001). However, the roles ofrecent findings, such as striatal spike-timing-dependent plasticity (STDP) models and stri-atal asymmetrical lateral connectivity, remainunclear. Investigating these interactions couldimprove our comprehension of the BG’s role inRL, potentially leading to the development ofmore efficient bio-inspired reinforcement learn-ing agents.This study aims to explore the impact ofhomeostatic mechanisms and asymmetric lat-eral inhibitory connections on action-selectionin the striatum. We use the RL framework togain insights into the neural basis of decision-making and contribute to more biologicallyplausible basal ganglia models. Our modelstands out from previous models in severalways: it does not require a critic or extra cir-cuitry for a temporal difference signal, therebysimplifying the model and reducing computa-tional complexity; additionally, it employs aspiking neural network with spike-time patternrepresentation that adapts well to varying pat-tern complexities in the pattern classificationlayer.thatWe propose a functional, biologically in-in-spired striatum network modelcorporates dopamine-modulated spike-timing-dependent eligibility (STDE, Gurney et al.(2015)) and asymmetric lateral connectivityimproves(Burke et al., 2017). This modelupon existing striatum models by integratinghomeostatic mechanisms, asymmetric lateralinhibitory connections, and the STDE learningrule, capturing essential experimental featuresfound in the striatum.In this article, we present a model that ef-fectively processes complex input patterns inthe context of reinforcement learning. We con-duct multiple analyses to assess the interactionbetween the learning rule, homeostatic mecha-nisms, and lateral inhibitory connectivity pat-terns. By incorporating these elements, westrive to develop a comprehensive and biologi-cally plausible striatum model that offers valu-202530354045505560657075808590951002able insights. Our study examines the indi-vidual and combined effects of these factors,shedding light on the unique topology of thestriatum network and its role in reinforcementlearning tasks.The main contributions and findings of thiswork are:• A functional and biologically inspired net-work model of the striatum that integratesdopamine-modulated STDE, homeostaticmechanisms, and asymmetric lateral in-hibitory connectivity, providing a morecomprehensive and biologically plausiblerepresentation of the striatum’s function.• Analysis of the role of homeostatic mecha-nisms in making learning more robust andfacilitating recovery after rewarding policyswapping.• Investigation of the importance of lateralinhibitory connections when multiple in-put patterns are associated with the samerewarded action.• The use of a spiking neural networkwith spike-time pattern representationthat scales well with different pattern com-plexity, making the model suitable for awide range of reinforcement learning tasks.• Demonstration that the optimal delay be-tween action and dopaminergic feedbackoccurs around 300 ms, which is consistentwith previous reinforcement learning andbiological studies.• A model that does not require a critic, sim-plifying the learning process and reducingthe need for additional circuitry.1.1. Basal Ganglia Circuitry and Striatal Con-nectivity in Decision MakingThe BG network is composed of severalstructures, grouped in inputs [being the stria-tum the best known, and populated by mediumspiny neurons (MSN)], intermediate layers [the105110115120125130135140145external segment of the globus pallidus (GPe),and the substantia nigra pars compacta (SNc)]and output [substantia nigra pars reticulata(SNr)].The information flows segregatedthrough the BG circuits (DeLong et al., 1985;Parent and Hazrati, 1995).It has been pro-posed that the BG process a large number ofcognitive streams or channels in parallel (Gur-ney et al., 2001), each of them representing afeasible action to be performed (Suryanarayanaet al., 2019). According to recent research,this segregation through the entire cortico-BG-thalamic loop shows a very high specificity,down to almost neuron-to-neuron level (Hun-nicutt et al., 2016; Foster et al., 2021). Thus,it seems feasible to impact behavior at differ-ent levels of detail. However, with the currentbiological evidence it is not exactly known howthe activation of a channel maps to the cor-responding behavior and we just assume herethat these channels involve a decision makingprocess.The striatum, as the primary input of thebasal ganglia, connects to the SNr via directand indirect pathways, which are traditionallythought to promote and inhibit behavior, re-spectively. Each pathway crosses the striatumthrough different subpopulations of MSNs, ex-pressing dopamine receptors D1 for the directpathway and D2 for the indirect pathway. Re-cent genetic and optical studies on striatal cir-cuits have allowed for testing classical ideasabout the functioning of this system, but newmodels are needed to better understand therole of the striatum in learning and decision-making (Cox and Witten, 2019).1501551601651701751801.2. Spiking Neural Networks: Learning, Re-ward Modulation, and Striatal Connectiv-ity185In recent decades, the use of biologicallyplausible computational models composed ofspiking neurons able to learn a target functionhas demonstrated being increasingly success-ful (Taherkhani et al., 2020; Tavanaei et al.,2019). These models use discrete-time events1903(spikes) to compute and transmit information.As the specific timing of spikes carry relevantinformation in many biological contexts, thesemodels are useful to understand how the braincomputes at the neuronal description level.Combined with the use of local learning rules,these models can be implemented in highly effi-cient, low-power, neuromorphic hardware (Ra-jendran et al., 2019). Within this framework,learning from past experiences can be achievedusing the STDP learning rule, a synaptic modelfeaturing weight adaptation that has been ob-served in both biological systems (Levy andSteward, 1983) and the BG (Fino and Venance,2010). The STDP also was demonstratedto be competitive in unsupervised learning ofcomplex pattern recognition tasks (Masquelieret al., 2009; Garrido et al., 2016). The com-plexity of the patterns comes from their statis-tically equivalent activity level and from beingimmersed within a noisy stream of hundredsor thousands of inputs. These studies shownthat an oscillatory stream of inputs reachinga population of spiking neurons enables a tar-get post-synaptic neuron equipped with STDPto detect and recognize the presence of repeti-tive current patterns (Masquelier et al., 2009).The added oscillatory drive performs a current-to-phase conversion: the neurons that receivethe most potent static current will fire the firstduring the oscillation cycle. This mechanismlocks the phase of the spike time, facilitatingthe recognition of the previously presented pat-terns.However, STDP-based learning systems tendto use statistical correlations to strengthensynaptic connections, resulting in the selectionof the most frequent patterns at the expenseof the most rewarding (Garrido et al., 2016).Thus, the STDP rule can be modified to drivethe learning of patterns that statistically cor-relate with a reward signal (Izhikevich, 2007;Legenstein et al., 2008). In biological systems,unexpected rewards signal relevant stimuli dur-ing learning by releasing dopamine (DA). More195200205210215220225230235specifically, the reward signal is linked to thephasic modulation of dopaminergic neurons inthe SNc and ventral tegmental area (Schultz,2010), that sends reinforcement signals to thestriatal neurons. These rewards do not needto happen instantly after the relevant stim-ulus; they can be delayed seconds, resultingin the distal reward and temporal credit as-signment problems. In Izhikevich (2007); Leg-enstein et al. (2008), the authors suggest areward-modulated STDP rule that enables aneuron to detect rewarded input patterns last-ing milliseconds, even if the reward is delayedby seconds, by using the so-called eligibilitytrace. Also, based on the eligibility trace, Gur-ney et al. (2015) developed a synaptic learn-ing rule called Spike-Timing-Dependent Eligi-bility (STDE) based on physiological data thatcaptures many features found in the biologicalMSN of the basal ganglia. This model is moreflexible than the previous STDP-like rules asdifferent learning kernels can be used depend-ing on the amount and type (reward or pun-ishment) of reinforcement received. Althoughthe authors did not include some importantBG features like the GPe nucleus or a cortico-striatal loop, their model successfully learnedto select an action channel driven by strongercortical input, based only on the timing of theinput and the reward signal.Another relevant feature of the striatum isits connectivity. Burke et al. (2017) proposeda model of asymmetric lateral connectivity inthe striatum that tries to explain how differ-ent clusters of striatal neurons interact andwhich role they play in information process-ing. This model accounts for the in vivo phe-nomenon of co-activation of sub-populationsof D1 or D2 MSNs, which seems paradoxicalas each subpopulation projects to behaviorallyopposite pathways (direct and indirect, respec-tively). This structured connectivity pattern isdetermined by lateral inhibition between neu-rons that belong to the same channel and be-tween neurons within different channels but ac-2402452502552602652702752804counting for the same receptor type (D1 or D2).The authors also include asymmetrical connec-tions with more intensive intra-channel inhibi-tion from D2 to D1 neurons than in the op-posite direction. This pattern resulted in syn-chronized phase-dependent activation betweenMSN D1 and D2 neuron groups that belong todifferent channels.1.3. ContributionAll the previous ideas are important piecesof the process of goal-oriented learning but fur-ther research is required as their respectiveroles and how they complement each other arestill not well understood. The combination ofthe STDE rule within a network with asym-metrically structured lateral inhibition has notbeen studied before, and some relevant con-clusions emerge from this specific study.Inthis article, we design and study a functionaland biologically inspired model of the striatum.Our approach is based on spike time represen-tation of complex input patterns and integratesdopamine modulated STDE and asymmetriclateral connectivity, among other mechanisms.This model learns to select the most rewardingaction to complex input stimuli through RL.The proposed model has been demonstrated tobe capable of recognizing input patterns rele-vant for the task and consistently choosing re-warded actions in response to that input. Weperformed numerous analyses to measure andbetter understand the interaction between thelearning rule with homeostatic mechanisms andthe lateral inhibitory connectivity patterns. Bymeasuring the single and combined effects ofthese factors in the learning process, we wantto shed light on how the particular topology ofthe striatum network facilitates the resolutionof RL tasks.2. MethodsAiming to implement a RL framework in a bio-logically plausible striatum model, we started de-signing a task where the agent has to learn howto map different input patterns into actions basedon the reward signal delivered by the environment.We implemented a network model of the striatumcapable of learning this task. This system behaveslike a RL agent and can solve action-selection tasks.The methods section is structured as follows: wefirst define the neuron and synapse models, inputpattern generation, and networks structures used inour experiments. Then we describe the experimen-tal design used with the network model and how wemeasure its learning capability. In SupplementaryMaterials we also explain both a previous exper-iment and a simpler model we made to test theviability of the combination of oscillatory inputs,STDE and homeostatic rules that we employed inthe final network model.2.1. Computational models2.1.1. Neuron modelsWe used conductance-based versions oftheLeaky-Integrate and Fire (LIF) neuron model (Ger-stner and Kistler, 2002) as it is computationally ef-ficient and captures certain biological plausibility.We use this model in every layer of the network,but with different parameters. We classify the neu-ron types according to the layer they belong to:cortical neurons for the input, striatal neurons (di-vided in two subpopulations according to which DAreceptor express, D1 or D2) for the learning layer,and action neurons for the output. There is also adopaminergic neuron that receives the rewards andpunishments. The parameters used for each typewere manually tuned to obtain reasonable firingrates. For the cortical neurons we used a numberof spikes per input cycle (with 8 cycles per second)close to Masquelier et al. (2009) and Garrido et al.(2016) (see details about the input protocol in sec-tion 2.1.2). For the striatal neurons, we tuned theparameters to obtain a mean firing rate of aroundone spike per second to be within biological ranges(Miller et al., 2008) but with activity peaks of twoor three spikes per input cycle (16-24 spikes persecond). The action neurons (an integrative popu-lation that outputs the agent’s behavior) are tunedto fire every input cycle if they receive enough stim-ulation from its channel (at least two more spikesfrom D1 neurons than D2 neurons each cycle). Thedopamine neuron was tuned to have a firing rangefrom 50 to 350 spikes per second, with these un-realistic values chosen for performance (instead ofsimulating a bigger dopaminergic population). The2852902953003053103153203253303353403453503553603653703755parameters used for each neuron type are shown atsupplementary table 1.2.1.2. Input and oscillatory driveIn the input generation procedure (Masquelieret al., 2009; Garrido et al., 2016) we consider atrial as a segment of time of the simulation wherewe present some input stimuli to the network. Thelength of each trial is taken from a uniform ran-dom distribution between 100 and 500ms. An in-put stimulus represents a combination of 2000 in-put current values conveyed one-to-one to a set ofcortical neurons of the same size (Fig. 8A). Aninput pattern is a combination of current valueswhich target precisely the same cortical neuronsevery time the input pattern is presented for theentire simulation. For every time bin, one or nopattern is presented. Only half of the cortical neu-rons (1000) are pattern-specific when presenting aspecific pattern, while the other half receives ran-dom current values. The cortical neurons specificfor each pattern are selected at the initialization.When no pattern is presented, all the cortical neu-rons receive random current values. Two thousandcurrent-based LIF cortical neurons transform theinput current levels into spike activity. These neu-rons have a firing rate between 8 to 40 spikes persecond due to the sum of the input current values(ranged from 87% to 110% of the cortical neuronrheobase currents) and an oscillatory drive at 8Hzfeeding these neurons (with an amplitude of 15% ofthe rheobase current of the cortical neurons). Thisoscillatory drive turns the input encoding from ana-logical signal to phase-of-firing coding (Masquelieret al., 2009) by locking the phase of the corticalspikes within the oscillatory drive, as shown in Fig.8B. By using these parameters, the cortical neuronsfire between 1 and 5 spikes per cycle.2.1.3. Spike-Timing-DependentEligibility(STDE) learning ruleWe implemented a version of the STDE learn-ing rule (Gurney et al. (2015)), a phenomenologicalmodel of synaptic plasticity. This rule is similar toSTDP, but the kernel constants are DA-dependant(that is, different values are defined for low DA andhigh DA values, and interpolated for DA values in-between, as shown in Fig. 1 and SupplementaryFig. 9Ai and Aii). STDE is derived from in vitrodata and predicts changes in direct and indirectpathways during the learning and extinction of sin-380385390395400405410415lo, k−hi, k+gle actions. Throughout, we used the following pa-rameters and procedures unless we specified other-wise. The kernel shape is defined by the parameterskSP KDA with SP K ∈ {+, −} being the spike orderpre-post for applying k+DA and post-pre for apply-ing k−DA, respectively, and DA ∈ {hi, lo} being thehigh- or low-DA cases, resulting in four parametersin total: k+hi and k−lo. We obtained theselearning kernel constant values by hand-tuning forboth MSN D1 and D2 cases (see SupplementaryFig. 9 and supplementary table 2). As in the classicSTDP learning rule, the weight variation in STDEis calculated for every pair of pre- and post-synapticspikes and decays exponentially with the time dif-ference between the spikes (Fig. 1). We use timeconstants τ = 32 ms and the weights values areclipped to [0, 0.075].Our implementation of STDE uses elegibilitytraces that decay exponentially to store the poten-tial weight changes, similarly to Izhikevich (2007).Following Gurney et al. (2015) we have two differenteligibility traces per synapse, c+ and c− for spikepairs with positive and negative timing respectively,updated for every pair of pre- and post-synapticspikes at times tj and ti as in equations (1) and(2):δc+ = (cid:0)αk+hi + αk+lo(cid:1) · etj −tiτeliif tj ≤ ti(1)δc− = (cid:0)αk−hi + αk−lo(cid:1) · e−tj −tiτeliif tj > ti(2)with α = 1−α, α been a value dependent of DA thatwe define in equation 3, and τeli been the eligibilitytrace time constant with a value twice the length ofthe mean reward delay. Overall plastic change at asingle synapse is then the sum of contributions fromboth c+ and c−, scaled by a learning rate factorη = 0.002.The level of DA in the system is determined byone neuron that fires at high (and unrealistic) ratesfor computational simplicity, representing a popu-lation of neurons from the SNc. This neuron firesspontaneously at a baseline frequency of 200Hz.The environment (i.e., the application of rewardingpolicies during the experiment) injects positive (ornegative) current in the dopaminergic neuron whenrewards (or punishments) are applied to the model,resulting in the firing rate of this neuron ranging be-tween 50Hz and 350Hz. All plastic synapses sharea global DA level d that decays exponentially withtemporal constant τda = 20ms. For each spike emit-6Figure 1: Kernels used for STDE synapses of MSN D1(top) and D2 (bottom), showing the weight change de-pending on the time difference between pre- and post-synaptic spikes and dopamine. Thick lines representkernels at dopamine minimum, normal, and maximumvalues (red, black, and green, respectively). Thin linesare interpolations of these values.ted by the dopaminergic neuron, d is increased by1τdawith 200-ms delay.Our implementation of STDE uses the linearmixing function α in equation (3), clipped to [0, 1],to smoothly morph between kernels with low andhigh DA:460α =d − dmindmax − dmin(3)465420425430435440445450455where dmin and dmax are the minimum and maxi-mum values of DA considered. We use this equationfor computational efficiency instead of the Naka-Rushton function used in Gurney et al. (2015) (theauthors also noted that this is not a requirement, aslong as the mixing function was increasingly mono-tonic and saturating). The function is bounded tovalues of DA firing rate between 50 and 350Hz, withthe baseline at 200Hz.2.1.4. Homeostatic mechanismsDuring learning, in some cases, the neurons canstop firing indefinitely due to a learning historyleading to the wrong parameters. Neuron activitycan also die by sudden changes in the reward policy,leaving the state of the synaptic weights ill (not rep-resenting any stimuli and not getting enough inputto fire by chance). To recover neurons from thisstate, we added two different homeostatic mech-anisms, one at the synaptic level and one at theneuron level. Although one or the other is enoughto avoid the ill-states, we saw in our tests that werecovered faster and more reliably by using both.The synapses implementing the STDE includeda non-Hebbian strengthening in response to everypre-synaptic spike. For each arriving spike, thesynaptic weight increases by Cpre = η · 4 · 10−4.This non-Hebbian strengthening is added to enablethe recovery of low-bounded synapses (e.g., after arewarding policy switch). Although the rewardingpolicy does not change in the network experiment,this homeostatic mechanism also benefits the com-plete network model learning (more details in sec-tion 5.2.2 and Supplementary Fig. 14).In order to avoid neurons to become permanentlysilent during learning, we include adaptive thresh-old to our neuron models based on Galindo et al.(2020) according to the following equation:dVthdt= −Vth − Eleakτth(4)where Vth represents the firing threshold at thecurrent time, Eleak is the resting potential of the4704754804854904955005057neuron, and τth is the adaptive threshold time con-stant. According to equation 4, in the absence ofaction potentials, the threshold progressively de-creases towards the resting potential, facilitatingneuron firing. When the neuron spikes, the firingthreshold increases a fixed step proportional to theconstant Cth as indicated in equation 5, makingneuron firing more sparse.δVth =Cthτth(5)2.1.5. Striatum network modelThe network model of the striatum (Fig. 3A)contains two channels (channel A and channel B,each one representing a possible action). Everychannel contains two same-sized subpopulations(D1 and D2 neurons, respectively) of striatal-likeneurons (in total, 16 neurons per channel) and oneso-called action neuron that integrates excitatoryactivity from D1 neurons and inhibitory activityfrom D2 neurons. This design simplifies the bio-logical substrate in which all MSN are inhibitory,but we implemented the network computation byconsidering the net effect of each neuron type onbehavior. Biological MSN D1 neurons inhibit SNr,which promotes behavior, and MSN D2 neurons in-hibit GPe, which, in turn, inhibit SNr with the totaleffect of decreasing behavior (Fig. 3A).Our striatum model implements lateral inhibi-tion within each MSN D1 population, within eachMSN D2 population, between MSN D1 and MSND2 populations within the same channel, and be-tween the MSN populations associated with differ-ent action channels. Inspired by Burke et al. (2017),we used an asymmetrical structured pattern of con-nectivity (Fig.5E in (Burke et al., 2017), andadapted here in Fig. 2). Following this connec-tivity pattern, we added lateral inhibition betweenneurons that belong to the same channel and be-tween those that belong to different channels butuse the same dopaminergic receptor D1 or D2 (withstronger inhibition from D2 to D1 neurons than inthe opposite direction). Since the small size of thenetwork under study and the small weight of the D1to D2 MSN connections, the overall contribution ofthese connections was neglectable, so we decidednot to include them in our simulations as we see nosignificant impact on previous simulations.The environment generates the reinforcementsignal based on comparing the chosen and the ex-pected action and then delivers it to the dopamin-policy applies no punishment or reward to the agentduring noisy inputs, whatever the action taken is.In case of spiking both action neurons during a re-inforced input, the network is punished.5452.3. Performance measurementFigure 2: Connectivity pattern used for the lateral in-hibition, inspired on Burke et al. (2017). Two channels(action A and action B) are shown, each with two pop-ulations of D1 and D2 MSN.550510ergic neuron. Rewards are excitatory, and punish-ments are inhibitory inputs to this neuron. Thedopaminergic modulatory signal is global and deliv-ered to every STDE connection from cortical layerto striatal neurons (Fig. 3A). It is important tonote that this model does not implement a critic(commonly used in actor-critic frameworks (Suttonet al., 1992)), so there is no reward prediction errorsignal.5152.2. Experimental designWe first validated the proposed learning mech-anisms with a simpler network model of only oneneuron and a easier experimental task, as can beseen in Supplementary Methods 5.1 and Supple-mentary Results 5.2.The action-selection task used to test the model(Fig. 3B) works as follows: the agent has two possi-ble actions to choose, A or B. An action is selectedif the activity balance of its D1 and D2 neuronsis biased to D1 in two spikes at least in one cy-cle (making the corresponding action neuron spike).The agent can do none, both, or any of them at atime. The input stream contains five different non-overlapping input patterns, each one presented 16%of the time (80% in total). The policy used to giverewards (excitation) and punishments (inhibition)to the agent (dopaminergic neuron) is the follow-ing. When pattern 1 or 2 is present, the agent isrewarded if action A is selected (action A neuronfires during the pattern presentation and action Bneuron does not fire) but punished if action B isselected. When pattern 3 or 4 is present, the agentis rewarded if action B is selected but punished ifaction A is selected. When pattern 5 is present, theagent is punished if it selects action A or B. This5205255305355405555605655705755808In the action-selection task we measure the per-formance of the models by calculating the percent-age of correct action choices (i.e. the learning accu-racy). This measure is widely used in classificationproblems when the objective is to describe the ac-curacy of a final map process (Stehman, 1997). Todo so, for each pattern presentation we store therewarded (expected) action in response to the pre-sented pattern, and the finally selected (chosen) oneduring that pattern presentation. We only considerin the calculation those trials in which some rewardor punishment can be delivered, ignoring those in-tervals with no repeating patterns conveyed to theinputs (only noisy inputs). We consider that anaction has been taken if the corresponding actionneuron has spiked at least once during the patternpresentation. Conversely, we consider that no ac-tion has been taken if none of the action neuronsspikes during the same duration. In order to obtainan estimation of the temporal evolution of the ac-curacy we use a rolling mean of the last 100 values.3. Results and discussionWe did extensive testing of the learningmechanisms we proposed. Some of these re-sults demonstrate that the combination ofSTDE learning rule and homeostatic mecha-nisms allow learning (and re-learning) of re-warded patterns, or that there is no effect ofthe reward delay and the frequency of the inputpattern on the learning process, among others.However, as they are not the main concern forthis article, they are placed in the Supplemen-tary Results 5.2 section for further examina-tion.The main results and discussion are struc-tured as follows: we first show the general be-havior of the network. Then we study the effect585590595600605610615620625of the lateral connectivity pattern on the per-formance and the way neurons are processinginformation. Finally, we put our results in con-text by comparing our model with previouslyproposed models in the literature.3.1. General network behaviorDuring the simulation of the action-selectiontask, each action group neuron becomes over-all active in response to the presentation of theassociated patterns as shown in the raster plots(Fig. 3C and D) and the activity balance forthe action neuron groups (Fig. 3E), produc-ing mainly dopaminergic rewarding (Fig. 3F).The action accuracy reveals steady-state per-formance after 200 seconds of simulations (Fig.3G). According to these results, our combi-nation of STDE learning rule (Gurney et al.,2015) with homeostatic mechanisms and an os-cillatory input signal in a cortico-striatal modellearns to accurately select the most rewardingaction.The way our network learns to associatethe corresponding input stimulus with sub-populations of D1 and D2 neurons in channel Aor channel B is the following: If the agent takesthe right action for a specific input pattern, theenvironment delivers a reward with some delay(high DA level in Fig. 3F). This reward poten-tiates the synapses between the cortical layerand the action-associated D1 sub-population,resulting in more frequent firing. On the otherhand, if the agent takes a wrong action, then itreceives a punishment sometime later (low DAlevel in Fig. 3F). This punishment weakens thesynapses from the cortical layer to the action-associated D1 sub-population while strength-ening the corresponding synapses to the D2(inhibitory) sub-population of the same chan-nel. This learning process makes the agentstick to the rewarded action and switch to adifferent one when punished. For the specificcase when the environment punishes any actionduring a stimulus presentation, both D2 sub-populations increase their activity, and bothaction neurons remain silent.630635640645650655The proposed model shows how combin-ing two complementary dopamine-based STDElearning rules (Fig. 1) can facilitate the associ-ation between sensorial cortical inputs and re-warded actions with arbitrary rewarding poli-cies. Previously, the STDE rule had beenshown to be capable of learning to select anaction channel driven by stronger cortical in-put (Gurney et al., 2015), and here we showthat this rule can also be used to learn inputsdefined by the specific timing of their spikes(as all the inputs have the same average fir-ing rate). This represents a higher complex-ity task and illustrates how STDE can be effi-ciently used for spike time pattern representa-tion.The model also is completely bioplausible,as all the mechanisms used have been de-scribed in biological systems: DA induces bidi-rectional, timing-dependent plasticity at MSNsglutamatergic synapses (Shen et al., 2008), invitro pyramidal neural recordings are consis-tent with simulations of adaptive spike thresh-old neurons, and they lead to better stimulusdiscrimination than would be achieved other-wise (Huang et al., 2016), and rat hippocam-pal pyramidal neurons in vitro can use rate-to-phase transform (McLelland and Paulsen,2009). Detailed discussion on the role of thehomeostatic mechanisms can be found in Sup-plementary Materials.3.2. Effect of lateral inhibition patterns and660task complexityOnce we have demonstrated how the stri-atal network can support RL, we wondered towhat extent the connectivity pattern of the lat-eral inhibition in the striatum could impact thelearning capabilities. So that we extensivelyexplored different versions of connectivity.We first study if there is any relationshipbetween the connectivity pattern and diffi-culty of the task. We organized the lateralinhibitory connections in two groups:intra-channel (inhibitory connections from D2 MSNsto D1 MSNs within the same channel) and6656709Figure 3: Cortico-striatal network solving a RL task. A. Structure of the network. See section 2.1.5 for a detailedexplanation. B-F. The activity of the network during the last 5 seconds of simulation. Background color indicatesthe reward policy (yellowish colors, action A is rewarded and B is punished; bluish colors, action B is rewarded andA is punished; grey, any action is punished). B. Input pattern conveyed to the cortical layer. C. Raster plot of thechannel-A action neurons. Yellow dots represent MSN D1 spikes, and orange dots are MSN D2 spikes. D. Raster plotof channel B. Cyan dots represent MSN D1 spikes, and dark blue dots are MSN D2 spikes. E. Action neuron firingrates. The middle horizontal line represents 0 Hz. Action A and B activity are represented in opposites directionsfor clarity. Action A neuronal activity increases in yellow zones while action B neuronal activity in cyan intervals.F. Firing rate of the dopaminergic neuron (black line). Dotted horizontal lines indicate the range of DA activityconsidered: black is the baseline, green is the maximum reward, and red represents the maximum punishment. Dotsindicate rewards (green) and punishment (red) events delivered to the agent. G. Evolution of the learning accuracyof the agent, see section 2.3 for further details. The dotted line marks the accuracy level by chance.10Figure 4: Effect of the lateral inhibitory connectivityon the performance during a simpler version of the RLtask. The horizontal dotted line represents the accu-racy obtained by a random agent. The curves representthe mean and the standard error of the mean of theevolution of each agent during the task (n=5).675680685690inter-channel (inhibitory connections betweenD1 MSNs of different channels, and betweenD2 MSNs of different channels). We obtainedfour possible subsets of connectivity patternsby keeping or removing each connection type(Fig. 2). We used three difficulty levels for thetask: easy, normal and hard. The easy taskuses only one stimulus associated with each ac-tion (stimulus 1 to action A, stimulus 2 to ac-tion B, stimulus 3 to no action). The normaltask uses two stimulus per action, and one no-go stimulus. The hard task uses four stimuliper action, and two no-go stimuli.The results of the easy version of the exper-iments are shown in the Fig. 4. The modelswithout inter-channel inhibition work worse,as they stabilize with lower values of accuracy.The models with inter-channel inhibition seemto reach a similar level of accuracy but theintra-channel inhibition seems to reduce thelearning rate.700705710In the normal version of the task, we againobtained the best learning performance when71569511Figure 5: Effect of the lateral inhibitory connectivity onthe performance during the normal RL task. The curvesrepresent the mean accuracy and the shaded areas rep-resent the standard error (n=30). Four different config-urations are tested, depending on the presence of twointra- and inter-channeltypes of lateral connectivity:inhibition. The horizontal dotted line represents theaccuracy obtained by a random agent with no learningmechanisms.using the inter-channel lateral inhibition withasymmetrical structured connection pattern,and the difference increased. In this case, thereis no apparent effect in of the intra-channel lat-eral inhibition in this task (Fig. 5). Accordingto our simulations, lateral inter-channel inhi-bition facilitates the emergence of one action-related channel over the other one in a winner-take-all manner, as expected.lateralWe saw in previous experiments that theinter-channelinhibition is always in-creases accuracy, so we will use it always in thefollowing tests. In the hard task we obtainedsmall but significant differences: The accuracyof the network improves faster with the intra-channel lateral inhibition (see Fig. 6). Also,apparently the network with the intra-channelinhibition settled in a more stable regime asit maintains its performance, compared withthe network without this intra-channel inhi-bition which slowly degrades (Supplementaryfrom the original resulted in reduced learningperformance (Supplementary Fig. 15). In thisFigure, the curve #5 represents the networkwith both lateral inhibition in D1 layer and D2layer, as well as intra- and inter-channel lateralinhibition. This structure (similar to the oneproposed by Burke et al. (2017)) obtains thebest accuracy.3.3. Effect of intra-channellateral inhibitionon neuronal specializationIntra-channel inhibition seems to facilitatelearning in more complex tasks, possibly be-cause it enhances neuron specialization. Wesaw a strong reduction of correlation at timedifference δt = 0 between action A and B D1sub-populations caused by intra-channel inhi-bition (data not shown), but this does not seemto justify the improved accuracy for more com-plex tasks.Then, we hypothesized that intra-channel in-hibition could encourage neuron specializationto specific cortical patterns. We tested thisidea by analyzing the preferred stimuli for eachneuron after the learning process (Fig. 7), andobtained the opposite result: the intra-channellateral inhibition affects D1 neurons by forcingthem to share more evenly their activity overseveral stimuli, in addition to reducing their av-erage activity. This is in contrast with the net-work without intra-channel lateral inhibition,where the activity is more focused on the fa-vorite stimuli and has higher mean activity.According to these results, although individ-ual neurons of the network with intra-channelinhibition have less precise representation of in-dividual sensorial stimuli, these models havehigher precision to associate rewarding actions.This can be explained assuming some sparserepresentation of the stimuli, where the simul-taneous firing of several (but not many) neu-rons are needed to indicate the presence of aninput stimuli. This more sparse representationemerges due to the combination of stronger in-hibition and the homeostatic mechanisms: aneuron avoids firing when it is inhibited, so74074575075576076577077578012Figure 6: Effect of the intra-channel lateral inhibitoryconnectivity on the performance during a harder versionof the RL task. The horizontal dotted line representsthe accuracy obtained by a random agent. The curvesand the filling color represent the mean, the standarderror of the mean, respectively, of the evolution of eachagent during the task (n=150), simulated for 500 sec-onds.720725730735Fig. 13). The results so far suggest that bothconnectivity patterns contribute to a reliableaction-selection paradigm.Taking these results together, it seems thatwhen we use several stimuli associated witheach action, intra-channel inhibition improvesthe RL action selection task. However, whenonly one stimulus is associated with each ac-tion, this intra-channel inhibition does not im-pact learning performance. When comparedwith the results in Fig. 5 and 6, it seems thatthe intra-channelinhibition improveslateralthe learning capabilities only with a hardertask, but when the task is too simple then theintra-channel connection increases the learningtime.We also explored the effect of connectivitypatterns of lateral inhibition different from theproposed by Burke et al. (2017)), by adding orremoving lateral connections within a channel,within each subpopulation, and between sub-populations of the same channel. All variationsusing multiple mechanisms proposed in thethe STDE learning rule that im-literature:plements synaptic modification in cortex-MSNconnections (Gurney et al., 2015), combinedwith homeostatic mechanisms (Galindo et al.,2020) and an oscillatory input signal (Masque-lier et al., 2009; Garrido et al., 2016) in a net-work with asymmetrical structured lateral in-hibition (Burke et al., 2017) can rapidly andconsistently learn to detect the presence of re-warded input patterns. These processes havebeen described in biological systems and hereproved to be robust.Simpler STDP-like rules have been used forRL tasks (Izhikevich, 2007; Legenstein et al.,2008), but they were employed in simpler net-works, single neurons, and simple tasks. Be-yond the state-action mapping role proposedin this article for the striatum, other theoriesexist about the action decision process. How-ever, computational models of BG in the lit-erature have considerably evolved during thelast two decades (Rubin et al., 2021), andthere is still no consensus about how to achievegoal-oriented learning in a BG model. Pre-vious models ranged from those with action-selection features but no learning (Beiser et al.,1997; Gillies and Arbuthnott, 2000; Humphrieset al., 2006; Lo and Wang, 2006a; Berns andSejnowski, 1998; Gurney et al., 2001; Sen-Bhattacharya et al., 2018; Frank, 2006; Rat-cliff and Frank, 2012; Bogacz, 2007) (but see(Frank, 2005)) to simple forms of learning,with RL (Bogacz and Larsen, 2011), rate-basedlearning rules (Hong and Hikosaka, 2011), orbased on modulated STDP with eligibilitytraces (Humphries et al., 2009; Gurney et al.,2015; Baladron et al., 2019). These modelsconsidered direct and indirect pathways (as”selection” and ”control” routes, respectively),composed of MSN D1 and D2 striatal neu-rons controlling GPe and SNr. Many modelsassume that the BG work as an actor-criticmodel (Bogacz and Larsen, 2011; O’Dohertyet al., 2004), and actor-critic frameworks have81081582082583083584084585013Figure 7: Effect of the intra-channel lateral inhibitoryconnectivity on the firing rate pattern on their preferredstimuli of D1 neurons. Higher and more specialized fir-ing patterns occur in networks without intra-channellateral inhibition, while more sparse representations oc-cur in networks with it. Lines and shaded areas rep-resent mean and 95% confidence intervals of the mean(n = 150), respectively.785790795800the homeostatic mechanisms tend to compen-sate for this activity reduction by increasingits chances to fire in response to several stim-uli. This sparse representation has been sug-gested to facilitate sensorial pattern recogni-tion in other brain areas, such as the cerebellarcortex, the mushroom body, and the dentategyrus of the hippocampus (Cayco-Gajic andSilver, 2019).In the context of our model, the sparse repre-sentation due to intra-channel inhibition playsa role in the action selection process, which canbe seen as a form of classification. Here, thegoal is not to classify stimuli per se, but to as-sign stimuli to appropriate actions. The sparsecoding helps to achieve more efficient and ro-bust action selection by reducing the overlap-ping between representations of different sen-sorial states, minimizing interference, and en-abling more reliable decision-making.3.4. Comparison with previous models of rein-forcement learning and basal ganglia805We presented a point-neuron model of theBG that can solve complex action-selectiontasks using a RL paradigm. We do so by855860865870875880885890895been used for RL tasks like maze navigation(Fr´emaux et al., 2013; Potjans et al., 2009;Vasilaki et al., 2009) and cartpole (Fr´emauxet al., 2013). More biologically-constrainedmodels of the BG have been proposed to ex-plain the origin of diseases like Parkinson’s dis-ease (Lindahl and Kotaleski, 2016) and the roleof specific interneurons (Goenner et al., 2021)or pathways (Girard et al., 2021) during action-selection. Recent accumulation-to-bound mod-els describe the decision process as an accumu-lation of evidence for each alternative actionuntil a decision threshold is exceeded in oneof these actions (Mulder, 2014).It would beinteresting to explore how these models couldbe incorporated with the proposed model, po-Intentially requiring additional brain areas.this regard, some models incorporate recurrentactivity loops with the cortex through the tha-lamus (Lo and Wang, 2006b).Moreover, we acknowledge that similar mod-els can already deal with more complex action-selection tasks than the one used in this work,such as cart-pole, inverted pendulum, or sim-ple mazes (Fr´emaux et al., 2013). However,there exist some important differences betweentheir model and the one proposed in this arti-cle. First, our network does not include a critic.Second, their learning rule requires a temporaldifference (TD) signal that would need addi-tional circuitry. Third, their model requiresan additional place-cell layer with unsupervisedlearning to represent complex input patterns.However, it remains as a future work to em-bed the network model into a closed-loop ex-perimental setup requiring continuously gradedoutput (instead of selecting an action in a dis-crete set of possibilities). This way, the modelcould deal with a larger set of RL tasks.Inour case, we have integrated a spiking neu-ral network with spike-time pattern represen-tation that scales well with different patternscomplexity at the pattern classification layer.Future work will explore how our model couldbe extended for such complex action controlframeworks.9004. ConclusionIn this article we tested the respective rolesin learning of the different mechanisms usedduring our simulations: homeostatic mecha-nisms make the neurons change their responseto compensate for long-lasting changes in theinput level, making learning faster and morerobust to the configuration. The asymmetri-cal lateral inhibition consistently outperformedother connectivity configurations. By addingintra-channel lateral inhibition to the networkmodel, we induced the channels to generate asparse representation of each stimulus relevantfor the task. This made the network less proneto errors as the model had to recruit moreneurons to take an action. Lastly, by segre-gating striatal and action neurons in indepen-dent channels for each action and incorporatingMSN D1 (Go neurons) and MSN D2 (No-Go)sub-populations with different learning kernels,the model effectively learned arbitrary map-pings from sensorial input states to action out-put in a two-choice action-selection task. MSND1 neurons and MSN D2 neurons coopera-tively facilitated action selection with contraryeffects; MSN D1 neurons learned to potenti-ate preferred actions while MSN D2 neuronslearned to inhibit non-preferred actions.AcknowledgementsRegionalThis research is supported by the Span-INTSENSO (MICINN-FEDER-ish GrantPID2019-109991GB-I00),grantsJunta Andaluc´ıa-FEDER (CEREBIO P18-ThisFR-2378 and A-TIC-276-UGR18).research has also received funding from the EUHorizon 2020 Framework Program under theSpecific Grant Agreement No. 945539 (HumanBrain Project SGA3) and the EU Horizon2020 research and innovation program underthe Marie Sk(cid:32)lodowska-Curie grant agreement891774 (NEUSEQBOT). Additionally,No.90591091592092593093594014the main author has been funded with a na-tional research training grant (FPU17/04432).Finally, this research was also supported bythe Vetenskapsr˚adet (VR-M-2017-02806, VR-M-2020-01652); Swedish e-science ResearchCenter (SeRC); KTH Digital Futures.ReferencesBaladron J, Nambu A, Hamker FH. The subthala-mic nucleus-external globus pallidus loop biases ex-ploratory decisions towards known alternatives: aneuro-computational study. European Journal ofNeuroscience 2019;49(6):754–67.99510001005Beiser DG, Hua SE, Houk JC. Network models ofthe basal ganglia. Current opinion in neurobiology1997;7(2):185–90.1010Berns GS, Sejnowski TJ. A computational model ofhow the basal ganglia produce sequences. Journal ofcognitive neuroscience 1998;10(1):108–21.Bogacz R. Optimal decision-making theories:linkingneurobiology with behaviour. Trends in cognitive sci-ences 2007;11(3):118–25.Bogacz R, Larsen T. Integration of reinforcement learn-ing and optimal decision-making theories of the basalganglia. Neural computation 2011;23(4):817–51.Burke DA, Rotstein HG, Alvarez VA. Striatal local cir-cuitry: a new framework for lateral inhibition. Neu-ron 2017;96(2):267–84.Cayco-Gajic NA, Silver RA. Re-evaluating circuitmechanisms underlying pattern separation. Neuron2019;101(4):584–602.Cox J, Witten IB.re-ward learning and decision-making. Nature Re-views Neuroscience 2019;20(8):482–94. doi:10.1038/s41583-019-0189-2.StriatalcircuitsforDeLong MR, Crutcher MD, Georgopoulos AP. Pri-mate globus pallidus and subthalamic nucleus: func-tional organization.Journal of neurophysiology1985;53(2):530–43.Fino E, Venance L. Spike-timing dependent plasticityin the striatum. Frontiers in synaptic neuroscience2010;2:6.Foster NN, Barry J, Korobkova L, Garcia L, Gao L,Becerra M, Sherafat Y, Peng B, Li X, Choi JH, GouL, Zingg B, Azam S, Lo D, Khanjani N, Zhang B,Stanis J, Bowman I, Cotter K, Cao C, Yamashita S,Tugangui A, Li A, Jiang T, Jia X, Feng Z, Aquino S,Mun HS, Zhu M, Santarelli A, Benavidez NL, SongM, Dan G, Fayzullina M, Ustrell S, Boesen T, John-son DL, Xu H, Bienkowski MS, Yang XW, Gong H,Levine MS, Wickersham I, Luo Q, Hahn JD, LimBK, Zhang LI, Cepeda C, Hintiryan H, Dong HW.The mouse cortico–basal ganglia–thalamic network.101510201025103010351040104515945950955960965970975980985990Nature 2021;598(78797879):188–194. doi:10.1038/s41586-021-03993-3.Frank MJ. Dynamic dopamine modulation in the basalganglia: a neurocomputational account of cognitivedeficits in medicated and nonmedicated parkinson-ism. Journal of cognitive neuroscience 2005;17(1):51–72.Frank MJ. Hold your horses: a dynamic computationalrole for the subthalamic nucleus in decision making.Neural networks 2006;19(8):1120–36.Fr´emaux N, Sprekeler H, Gerstner W. Reinforcementlearning using a continuous time actor-critic frame-work with spiking neurons. PLoS computational bi-ology 2013;9(4):e1003024.Galindo SE, Toharia P, Robles ´OD, Ros E, Pas-tor L, Garrido JA. Simulation, visualization andanalysis tools for pattern recognition assessmentwith spiking neuronal networks. Neurocomputing2020;400:309–21. doi:10.1016/j.neucom.2020.02.114. arXiv:2003.06343.Garrido JA, Luque NR, Tolu S, D’Angelo E.Oscillation-Driven Spike-Timing Dependent Plastic-ity Allows Multiple Overlapping Pattern Recognitionin Inhibitory Interneuron Networks.InternationalJournal of Neural Systems 2016;26(05):1650020.doi:10.1142/S0129065716500209.Gerstner W, Kistler WM. Spiking neuron models: Sin-gle neurons, populations, plasticity. Cambridge uni-versity press, 2002.Gillies A, Arbuthnott G. Computational models of thebasal ganglia. Movement Disorders 2000;15(5):762–70.Girard B, Lienard J, Gutierrez CE, Delord B, DoyaK. A biologically constrained spiking neural networkmodel of the primate basal ganglia with overlappingpathways exhibits action selection. European Jour-nal of Neuroscience 2021;53(7):2254–77.Goenner L, Maith O, Koulouri I, Baladron J, HamkerFH. A spiking model of basal ganglia dynam-ics in stopping behavior supported by arkypalli-dal neurons. European Journal of Neuroscience2021;53(7):2296–321.Graybiel AM. The basal ganglia and chunking of actionrepertoires. Neurobiology of Learning and Memory1998;70(1):119–36. doi:https://doi.org/10.1006/nlme.1998.3843.Grillner S, Hellgren J, M´enard A, Saitoh K, Wik-str¨om MA. Mechanisms for selection of basicmotor programs – roles for the striatum and pal-lidum. Trends in Neurosciences 2005;28(7):364–70.URL: https://www.sciencedirect.com/science/doi:https:article/pii/S0166223605001293.//doi.org/10.1016/j.tins.2005.05.004.Gurney K, Prescott TJ, Redgrave P. A computationalmodel of action selection in the basal ganglia.i.a new functional anatomy. Biological cybernetics105010551060106510701075108010851090109511002001;84(6):401–10.Gurney KN, Humphries MD, Redgrave P. A NewFramework for Cortico-Striatal Plasticity:Be-havioural Theory Meets In Vitro Data at theReinforcement-Action Interface.PLoS Biology2015;13(1):e1002034.doi:10.1371/journal.pbio.1002034.Hikosaka O, Takikawa Y, Kawagoe R. Role of the basalganglia in the control of purposive saccadic eye move-ments. Physiological reviews 2000;80(3):953–78.Hong S, Hikosaka O. Dopamine-mediated learning andswitching in cortico-striatal circuit explain behav-ioral changes in reinforcement learning. Frontiers inbehavioral neuroscience 2011;5:15.Huang C, Resnik A, Celikel T, Englitz B. Adaptivespike threshold enables robust and temporally pre-cise neuronal encoding. PLoS computational biology2016;12(6):e1004984.Humphries MD, Lepora N, Wood R, Gurney K. Cap-turing dopaminergic modulation and bimodal mem-brane behaviour of striatal medium spiny neuronsin accurate, reduced models. Frontiers in computa-tional neuroscience 2009;3:26.Humphries MD, Stewart RD, Gurney KN. A physiolog-ically plausible model of action selection and oscilla-tory activity in the basal ganglia. Journal of Neuro-science 2006;26(50):12921–42.Hunnicutt BJ, Jongbloets BC, Birdsong WT, Gertz KJ,Zhong H, Mao T. A comprehensive excitatory inputmap of the striatum reveals novel functional orga-nization. eLife 2016;5:e19103. doi:10.7554/eLife.19103.Izhikevich EM.Solving the distal reward problemthrough linkage of STDP and dopamine signaling.Cerebral Cortex 2007;17(10):2443–52. doi:10.1093/cercor/bhl152.Legenstein R, Pecevski D, Maass W. A learning theoryfor reward-modulated spike-timing-dependent plas-ticity with application to biofeedback. PLOS Com-putational Biology 2008;4:1–27. URL: https://doi.org/10.1371/journal.pcbi.1000180. doi:10.1371/journal.pcbi.1000180.Levy W, Steward O. Temporal contiguity requirementsfor long-term associative potentiation/depression inthe hippocampus. Neuroscience 1983;8(4):791–7.Lindahl M, Kotaleski JH. Untangling basal ganglia net-work dynamics and function: role of dopamine deple-tion and inhibition investigated in a spiking networkmodel. eneuro 2016;3(6).1105111011151120112511301135114011451150Lo CC, Wang XJ. Cortico–basal ganglia circuit mecha-nism for a decision threshold in reaction time tasks.Nature neuroscience 2006a;9(7):956–63.1155Lo CC, Wang XJ. Cortico–basal ganglia circuit mecha-nism for a decision threshold in reaction time tasks.Nature neuroscience 2006b;9(7):956–63.Masquelier T, Hugues E, Deco G, Thorpe SJ.116016Oscillations, Phase-of-Firing Coding, and SpikeTiming-Dependent Plasticity: An Efficient LearningScheme. Journal of Neuroscience 2009;29(43):13484–93. doi:10.1523/JNEUROSCI.2207-09.2009.McGill R, Tukey JW, Larsen WA. Variations of BoxPlots. The American Statistician 1978;32(1):12–6.doi:10.2307/2683468.McLelland D, Paulsen O. Neuronal oscillations andthe rate-to-phase transform: mechanism, model andmutualinformation. The Journal of physiology2009;587(4):769–85.Miller BR, Walker AG, Shah AS, Barton SJ, Re-bec GV. Dysregulated information processing bymedium spiny neurons in striatum of freely behav-ing mouse models of huntington’s disease. Journal ofneurophysiology 2008;100(4):2205–16.Thedence accumulation in the brain.Neuroscience 2014;34(42):13870–1.JNEUROSCI.3251-14.2014.evi-Journal ofdoi:10.1523/temporal dynamics ofMulder MJ.O’Doherty J, Dayan P, Schultz J, Deichmann R, Fris-ton K, Dolan RJ. Dissociable roles of ventral anddorsal striatum in instrumental conditioning. science2004;304(5669):452–4.Parent A, Hazrati LN. Functional anatomy of the basalganglia. ii. the place of subthalamic nucleus and ex-ternal pallidium in basal ganglia circuitry. Brain re-search reviews 1995;20(1):128–54.Potjans W, Morrison A, Diesmann M. A spiking neu-ral network model of an actor-critic learning agent.Neural computation 2009;21(2):301–39.Rajendran B, Sebastian A, Schmuker M, Srinivasa N,Eleftheriou E. Low-power neuromorphic hardwarefor signal processing applications: A review of archi-tectural and system-level design approaches. IEEESignal Processing Magazine 2019;36(6):97–110.Ratcliff R, Frank MJ. Reinforcement-based decisionmaking in corticostriatal circuits: mutual constraintsby neurocomputational and diffusion models. Neuralcomputation 2012;24(5):1186–229.Rubin JE, Vich C, Clapp M, Noneman K, VerstynenT. The credit assignment problem in cortico-basalganglia-thalamic networks: A review, a problem anda possible solution. European Journal of Neuro-science 2021;53(7):2234–53.Schultz W. Dopamine signals for reward value and risk:basic and recent data. Behavioral and brain functions2010;6(1):1–9.Sen-Bhattacharya B, James S, Rhodes O, Sugiarto I,Rowley A, Stokes AB, Gurney K, Furber SB. Build-ing a spiking neural network model of the basal gan-glia on spinnaker. IEEE Transactions on Cognitiveand Developmental Systems 2018;10(3):823–36.Shen W, Flajolet M, Greengard P, Surmeier DJ. Di-chotomous dopaminergic control of striatal synapticplasticity. Science 2008;321(5890):848–51.thematic classification accuracy.Selecting and interpreting mea-Stehman SV.Re-sures ofmote Sensing of Environment 1997;62(1):77–89.URL: https://www.sciencedirect.com/science/article/pii/S0034425797000837.doi:https://doi.org/10.1016/S0034-4257(97)00083-7.Suryanarayana SM, Kotaleski JH, Grillner S, GurneyKN. Roles for globus pallidus externa revealed in acomputational model of action selection in the basalganglia. Neural Networks 2019;109:113–36. doi:10.1016/j.neunet.2018.10.003.Sutton RS, Barto AG, Williams RJ. ReinforcementIEEElearning is direct adaptive optimal control.Control Systems Magazine 1992;12(2):19–22.Taherkhani A, Belatreche A, Li Y, Cosma G, MaguireLP, McGinnity TM. A review of learning in bio-logically plausible spiking neural networks. NeuralNetworks 2020;122:253–72.Tavanaei A, Ghodrati M, Kheradpisheh SR, Masque-lier T, Maida A. Deep learning in spiking neuralnetworks. Neural Networks 2019;111:47–63.Vasilaki E, Fr´emaux N, Urbanczik R, Senn W, Gerst-ner W. Spike-based reinforcement learning in con-tinuous state and action space: when policy gra-dient methods fail. PLoS computational biology2009;5(12):e1000586.Yagishita S, Hayashi-Takagi A, Ellis-Davies GC,Urakubo H, Ishii S, Kasai H. A critical time windowfor dopamine actions on the structural plasticity ofdendritic spines. Science 2014;345(6204):1616–20.116511701175118011851190175. Supplementary materials5.1. Supplementary methods5.1.1. Single-striatal-neuron model and experimentsFigure 8: Pattern detection experiments with reinforcement learning and a single striatal neuron. A. Single-striatal-neuron model setting, with serial and oscillatory input currents feeding to a cortical layer. In this simulation, twodifferent input patterns are used and colored in green and red. The cortical layer feeds the striatal neuron with plasticsynapses with STDE, where learning occurs. A reward or punishment signal is delivered to a global dopaminergicneuron that modulates the plastic synapses. B. Raster plot of the cortical neurons (blue dots), with input patternscontaining only half of the cortical neurons, oscillatory driving current (solid red line), and the striatal neuron(bottom, red dots). C. Evolution of the striatal neuron’s response to each input pattern through time measuredusing a uncertainty coefficient (see details in the methods section). Insets show the distribution of synaptic weightsat the beginning and the end of the learning procedure.In order to assess the learning capabilities of the proposed model, we define two types of experi-ments: pattern detection and action-selection. The latter one is already explained in the main text.During pattern detection experiments (Supplementary Fig. 8B), we train a simple model to detectone specific pattern within a noisy input stream. Two (the so-called selected and non-selected)non-temporally-overlapping repeating patterns are presented 20% of the time each (40% in total).We test the STDE learning rule in a RL setting, where a reward (excitation to the dopaminergicneuron) is given if the striatal neuron spikes sometime after the selected pattern is presented. Oth-erwise, if the striatal neuron fires in response to the non-selected pattern, punishment (inhibitionto the dopaminergic neuron) is given to the striatal neuron. Finally, as a stress test, we added apolicy swapping procedure for switching the rewarded pattern every 200 seconds (SupplementaryFig. 9). This way, we can test how robust is our combination of synaptic and homeostatic rulesduring learning.For this first set of experiments, we used a model with only one striatal neuron that learns to solvea simple RL task. This model allows the validation of the proposed learning mechanisms. It usesthe input protocol explained in the oscillatory drive section 2.1.2. A dopaminergic signal modulatesthe synapses that connect from the cortical neurons to the striatal neuron (Supplementary Fig.8A), implementing the STDE learning rule as well as the homeostatic mechanisms. Rewards(punishments) delivered by the environment alter the dopaminergic modulatory signal by exciting(inhibiting) the dopaminergic neuron every time the striatal neuron spikes when the input patternis correct (incorrect). The environment delivers rewards and punishments with some delay (fixedto 300 ms by default). If the striatal neuron does not fire, the environment delivers no reward norpunishment to the DA neuron.1195120012051210121518122012255.1.2. Mutual informationIn order to measure how good the detection is in the pattern detection experiments, we calculatedthe mutual information (MI) between the presentation of each input pattern and the striatal neuronactivity, as previously done in Garrido et al. (2016). We consider that the striatal neuron respondedto the pattern if it fires at least once during the stimulus presentation, lasting from 100 to 500ms following a uniformly distributed random distribution. For each stimulus used in the patterndetection experiments, we consider the possible states S of the pattern (present or absent) and thepossible response R of the striatal (neuron fired or not). The MI is then defined in equation (6).M I = H(S) + H(R) − H(S, R)(6)where H(S) is the entropy of the stimuli patterns, H(R) is the entropy of the responses, andH(S, R) the joint entropy of the stimuli patterns and the responses. These values are defined as inGarrido et al. (2016). The upper bound of the MI for a perfect detector would be M Imax = H(S),so we can obtain a normalized measurement of performance called uncertainty coefficient (UC)defined in equation (7). The UC is calculated independently for both the rewarded and the non-rewarded patterns during pattern detection experiments.U C =M IM Imax=H(S) + H(R) − H(S, R)H(S)(7)12305.1.3. Parameters usedParametereexc (mV)einh (mV)τAM P A (ms)τGABA (ms)τref (ms)Cm (pF)gleak (nS)Vthr (mV)eleak (mV)Cortical0.0−85.05.010.01.0250.025.0−40.0−65.0Striatal0.0−85.05.030.015.050.010.0−50.0−65.0Action0.0−85.05.060.015.0100.025.0−40.0−65.0Dopaminergic0.0−85.05.010.01.0250.025.0−65.0−40.0Table 1: Neuron parameters used in the model.MSN D1MSN D2Parameterk−lok+lok−hik+hiValue0.0−1.0−1.01.0Parameterk−lok+lok−hik+hiValue−1.01.00.0−1.0Table 2: STDE parameters used in the model.195.2. Supplementary results5.2.1. Single-striatal-neuron experiments1235124012451250125512601265In a previous article by Masquelier et al. (2009), an oscillatory driving signal greatly facilitates therecognition of complex patterns over noise with STDP-like rules. We have extended this learningrule to account for a rewarding signal in a RL paradigm. During a whole learning task (lasting200 seconds), two different repeating and non-overlapping input patterns are presented. Onlyone of them produces a rewarding signal if, and only if, the striatal neuron fires simultaneouslyto the pattern presentation, providing reward modulation to the learning rule. Using this RLframework, the striatal neuron becomes selective to the presentation of the rewarded pattern only(Supplementary Fig. 8B). It usually takes less than 100 seconds of simulated time to consistentlygenerate spikes with the presentation of the rewarded pattern (Fig. 8B). The detection capabilitiesof this network are also evidenced by the evolution of the uncertainty coefficient (green line inSupplementary Fig. 8C), which remains stable between 0.6 and 0.8 after 80 seconds of discontinuouspattern presentation (Supplementary Fig. 8C), while the punished pattern receives no considerableresponse (red line in Supplementary Fig. 8C). It can also be observed how the initial uniformweight distribution (insets in Supplementary Fig. 8C) turns into a binomial distribution with asmall number of synapses with near-maximum weights and most of the synapses near the minimumweight.Once demonstrated the effectiveness of the STDE learning rule, we aim to assess if it allowsdetection of rewarded patterns with policy swapping (i.e., the pattern that offers rewarding signals isswapped every 400 seconds of simulation). Every time that the rewarding policy swaps, the neurontemporarily reduces its average firing rate (cyan line in Supplementary Fig. 9Di), and consequently,the adaptive firing threshold approaches the resting potential (pink line in Supplementary Fig.9Di). Once the threshold is low enough, the neuron starts learning the new rewarded pattern,increasing the activity of the dopaminergic neuron as a consequence (Supplementary Fig. 9Ei).This is an important feature because neurons can recover from silent states caused by suddenchanges in the reward policy.Inspired by the different types of neurons existing in the striatum, we adapted the synapticmodel parameters to reproduce the differential operation of the learning rule for the MSN D1 andthe MSN D2 neurons (MSN D1 and D2 parameters for STDE in Supplementary Table 2). Thus,we adjusted different kernel shapes for low and high DA (Supplementary Fig. 9Bi and Bii, left andright, respectively). According to our simulations, the neuron equipped with a D1 kernel learnsto detect only the rewarded pattern (Supplementary Fig. 9Ci). In contrast, the striatal neuronequipped with MSN D2 kernel parameters (a reversed version of MSN D1) learns to detect thenon-rewarded pattern (Supplementary Figs. 9Cii, 9Dii and 9Eii). These results point out that, ina network of MSNs with D1 and D2 subpopulations, the D1 subpopulation learns to respond torewarded patterns while the D2 neurons learn to fire in response to the punished (or non-rewarded)patterns. In this way, the output layer makes simple decisions by just weighting the activity ofthese subpopulations.12705.2.2. Homeostatic mechanisms: non-Hebbian strengthening and adaptive thresholdAiming to check the influence of the homeostatic mechanisms, we have replicated the samepolicy-swapping learning framework with a more complex task (five different input patterns) anddifferent configurations of the homeostatic rules. In the absence of non-Hebbian strengthening,successful learning requires fine-tuning of the learning rule parameters and maximum weight for20Figure 9: Pattern detection experiments with two different STDE sets of parameters. Xi column shows the trainingresulting from using a learning kernel adapted to learn rewarded patterns, as used in MSN D1 synapses. Xii columnshows the same training results for the MSN D2 kernel used. Note that this kernel is learning the opposite (punished)pattern. A row shows the kernel functions used with different levels of DA. B row shows the response of the striatalneuron. The background color indicates which pattern is being rewarded at that specific time frame, and the verticaldotted lines indicate when the rewarding policy swaps. C row shows the evolution of the adaptive threshold andthe firing rate of the striatal neuron. D row shows the firing rate of the dopaminergic neuron, which represents theamount of reward obtained by the striatal neuron through the task. The horizontal green, black and red dotted linesindicate the maximum, baseline, and minimum dopaminergic activity.1275each simulation seed (data not shown). Thus, we barely managed to find a set of parameters suitablefor multiple seeds without this homeostatic mechanism. For this reason, in all the simulations shownin this article we employ the non-Hebbian strengthening mechanism.On the other hand, the adaptive threshold is not strictly necessary for successful learning. How-ever, the learning performance (in terms of UC) with adaptive threshold increases faster and morereliably than without adaptive threshold (Supplementary Fig. 10). It is important to highlightthat lack of homeostatic mechanisms often resulted in the more frequent inability of detecting cor-tical patterns, as demonstrated by lower MI values for the 25-percentile of the simulations (lowerboundary of blue shadow in Supplementary Fig. 10, right). In the absence of these mechanisms,the striatal neuron activity extinguishes when the reinforcement policy swaps and, in many cases,remain silent for the rest of the simulation. We tested different learning rates and time constantsof dopamine and, in every case, learning was faster with adaptive threshold, as shown in Supple-mentary Fig. 14. Thus, these homeostatic rules provide the STDE rule with the ability to re-learndifferent patterns reliably. Moreover, using both of these mechanisms also makes learning robustwithin a broader parameter space and makes it unnecessary to fine-tune the parameters for eachexperiment. Although only one of these homeostatic mechanisms would be enough to avoid silent12801285129021neurons, we saw in our tests that the system recovered faster and more reliably by using both.Figure 10: Effect of the adaptive threshold in the learning performance of the single-striatal-neuron model. In thisexperiment we used a more complex version of the policy-swap task with 5 (one rewarded, the rest punished) differentpatterns instead of 2. The left curves and the filling represent the evolution of the mean uncertainty coefficient andstandard error during a repeated 400-s learning protocol (n=300). The asterisks marked intervals indicate where themeans are statistically different with 95% confidence level. The right plot shows the percentiles 5-25-50-75-95, withdashed lines (5 and 95 percentiles), fillings (25 and 75 percentiles) and solid lines (50 percentiles).5.2.3. Effect of reward delay and input pattern1295130013051310We wondered how the delay between the action decision (in response to cortical stimulus) andthe rewarding signal affects the learning capabilities of our system. In order to evaluate the impactof this parameter, we carried out network simulations with different reward delays (we did nothave to adjust any other parameter due to the robustness of the model). We found the bestperformance when the rewarding signal was provided 300 ms after the sensorial presentation (blueline in Supplementary Fig. 11). Longer or shorter delays resulted in decaying learning accuracy.This result is similar to what can be found in biology ((Yagishita et al., 2014)).Since our implementation of the DA-modulated learning rule is based on eligibility traces, wewondered if this optimal delay was somehow related to the duration of the stimulation patterns.Then, we evaluated the reward delay effect on learning when sensorial patterns were longer (300-700 ms and 500-900 ms) than in the control case (100-500 ms). However, our simulations showsimilar learning accuracy with longer cortical patterns (orange and green lines in SupplementaryFig. 11) as in control conditions (blue line in Supplementary Fig. 11). So that it seems unlikelythat the pattern generation algorithm influenced the preferred delay.Finally, we also studied how the frequency of pattern presentation influences the accuracyachieved at the end of the simulation. We compared the results obtained presenting the pat-terns 80 percent of the time (as in the rest of the experiments made) with the results obtained bypresenting the patterns 40 percent. In order to compensate for the lower exposure of the striatalneurons to input patterns (since in the latter, the network will only see the patterns half the time),we simulated twice as long (up to 1000 seconds). According to our simulations, the proposed net-work similarly managed to successfully associate cortical inputs to associated actions independently22of how often the patterns are presented, as long as it experiences enough trials (SupplementaryFig. 12).131523Figure 11: Effect of the delay of the rewarding feedback in the learning accuracy. A. Simulations with differentpattern lengths: within 100 to 500 ms (blue), within 300 to 700 ms (in orange), and within 500 to 900 ms (in green).Every point represents the mean accuracy level obtained in the last 100 seconds of simulation with different delayvalues, and the shaded area shows the standard error of the mean (n=10). B. Notched box plot of all the values.Notice that ”if the notches about two median do not overlap, the medians are, roughly, significantly different atabout a 95% confidence level” (see McGill et al. (1978) for details).24Figure 12: Effect of the delay of the reward in the learning performance with input pattern proportion of 0.8 (inblue), and with input pattern proportion of 0.4 (in orange). Every notched box (McGill et al., 1978) represents themedian (n=10) performance level obtained in the last 100 seconds of simulation for different delay values.255.2.4. Effect of lateral inhibition in harder experimental settingFigure 13: Same as in Fig. 6, but simulated for 1000 seconds.265.2.5. Effect of DA time constant, learning rate and adaptive thresholdFigure 14: Learning performance for different values of DA time constant, learning rate and adaptive threshold. Atthe top, mean and standard error are shown for each condition. At the bottom, boxplots of the last 200 seconds ofsimulation (n=80).275.2.6. Lateral connectivity patterns effectFigure 15: Learning performance for different connectivity patterns of lateral inhibition. Left: Connectivity topologiestested in these experiments. Note that all these tests assume inter-channel inhibition, as they clearly outperformedother models. Right: evolution of the learning accuracy during 500s of simulation with the medium-complexity task.Every line is marked with the same color of the topology under test. Each line represents the average value withn = 10 seeds28