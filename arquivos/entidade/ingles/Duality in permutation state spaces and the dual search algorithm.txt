Artificial Intelligence 172 (2008) 514–540www.elsevier.com/locate/artintDuality in permutation state spaces and the dual search algorithmUzi Zahavi a, Ariel Felner b,∗, Robert C. Holte c, Jonathan Schaeffer ca Computer Science Department, Bar-Ilan University, Ramat-Gan, Israelb Department of Information Systems Engineering, Ben-Gurion University, Israelc Computing Science Department, University of Alberta, Edmonton, Alberta, CanadaReceived 2 August 2006; received in revised form 27 June 2007; accepted 22 October 2007Available online 6 November 2007AbstractGeometrical symmetries are commonly exploited to improve the efficiency of search algorithms. A new type of symmetry inpermutation state spaces, duality, is introduced. Each state has a dual state. Both states share important attributes such as theirdistance to the goal. Given a state S, it is shown that an admissible heuristic of the dual state of S is an admissible heuristicfor S. This provides opportunities for additional heuristic evaluations. An exact definition of the class of problems where dualityexists is provided. A new search algorithm, dual search, is presented which switches between the original state and the dual statewhen it seems likely that the switch will improve the chance of reaching the goal faster. The decision of when to switch is veryimportant and several policies for doing this are investigated. Experimental results show significant improvements for a number ofapplications, for using the dual state’s heuristic evaluation and/or dual search.© 2007 Elsevier B.V. All rights reserved.Keywords: Heuristics; Search; Admissibility; Duality1. Introduction and overviewThe states of many combinatorial problems (e.g., Rubik’s cube, 15-puzzle) are defined as placements of a set ofm objects into a set of n locations (where n (cid:2) m). All the different ways to put the objects into the locations with atmost one object per location defines a state space which is called a permutation state space in this paper.1 Given twostates in a permutation state space, start and goal, and a set of operators that transform one state into another, searchalgorithms such as A∗ [8] and IDA∗ [12] can be used to find the shortest sequence of operators that transform startinto goal. These algorithms use a cost function f (n) = g(n) + h(n), where g(n) is the cost to reach state n from startand h(n) is an admissible (i.e. is always a lower bound) heuristic function estimating the cost from n to goal.* Corresponding author.E-mail addresses: zahaviu@cs.biu.ac.il (U. Zahavi), felner@bgu.ac.il (A. Felner), holte@cs.ualberta.ca (R.C. Holte), jonathan@cs.ualberta.ca(J. Schaeffer).1 Strictly speaking, a permutation would require n, the number of locations, to be exactly the same as m, the number of objects. We have relaxedthis requirement and only demand that n (cid:2) m. We use the term strict permutation state space to refer to state spaces in which the states arepermutations in the strict sense (m = n).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.10.019U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540515The effectiveness of the search is greatly influenced by the accuracy of h(n). When h(n) is more accurate, thenumber of nodes generated in a search decreases and the goal state is reached sooner [16]. There is a tradeoff for thisreduction, however. More accurate heuristics usually consume a larger time overhead per node generated and thereforethe percentage reduction in the actual time needed to solve a problem is smaller in practice than the percentagereduction in the total number of generated nodes. Usually, the reduction in the number of generated nodes dominatesthe constant time per node and therefore a time reduction is seen as well [4,17].In this paper a new type of symmetry is discussed—duality. It is based on the observation that in strict2 permutationstate spaces, i.e., when m = n, the roles played by objects and locations are interchangeable. By reversing these roles,a state, S, can be mapped to its dual representation, Sd . Given an admissible heuristic, h, the value h(Sd ) is a lowerbound on the distance from S to the goal. Taking the maximum of h(S) and h(Sd ) can result in a better heuristicvalue for S and, hence, less search. Further, if h(Sd ) > h(S), this can be exploited by using a search algorithm thatswitches representations when it appears likely to be beneficial. The dual search algorithm searches in the original ordual search space, switching representations to whichever has a higher heuristic value.The contributions of this paper are as follows:• A formal definition of duality is given, along with precise conditions for it to be applicable. The dual of a state, S,is another state, Sd , that is easily computed from S and shares key search-related properties with S, such as beingthe same distance from the goal. Therefore any admissible heuristic for Sd can be used as an admissible heuristicfor S.• A new type of search algorithm, dual search, is introduced. It has the unusual feature that it does not necessarilyvisit all the states on the solution path that it returns. Instead, it constructs its solution path from path segmentsthat it finds in disparate regions of the state space. The jumping from region to region is effected by choosing toexpand Sd instead of S whenever doing so improves the chances of achieving a cutoff in the search.• Using the heuristic evaluation of the dual state (h(Sd )) in the search shows a significant performance improvementfor a number of domains. Adding the dual search algorithm further improves the results. For all the domainsstudied, the results represent the best in the published literature.The idea of duality is also used in the constraint satisfaction problems (CSP) literature, where flipping the rolesof variables and constraints produces a dual version of the problem. Independent of our work, Hnich et al. discussmethods to use duality in CSP applications [9]. For example, they exploit duality by choosing to solve the variationof the problem that appears to be faster to solve. By contrast, in this paper we introduce duality ideas in the context ofheuristic state-space search.The paper is organized as follows. Sections 2 and 3 present background material. In Section 4, the notion of simpleduality is defined. Simple duality is a special case of duality that only applies to strict permutation states spaces.Section 5 discusses the properties of the dual heuristic. Section 6 presents a new search algorithm based on duality,DIDA∗ (Dual IDA∗). Section 7 provides experimental evidence for the benefits of using the heuristic evaluation of thedual state and for the dual search algorithm. Section 8 provides generalization of the duality notion to a wider varietyof permutation state spaces that are not necessarily strict. Experimental results for the general case are then providedin Section 9. A summary and suggestions for future work are provided in Section 10. Preliminary versions of thispaper appeared in [7,20].2. Problem domains and permutation state spacesThis section introduces the three application domains used in this paper and gives a formal definition of permutationstate spaces. Pattern databases, used as the heuristic evaluation function for our application domains, are described.2.1. The sliding-tile puzzlesOne of the classic examples in the AI literature of a single-agent path-finding problem is the sliding-tile puzzle.Three versions of this puzzle are the 3 × 3 8-puzzle, the 4 × 4 15-puzzle and the 5 × 5 24-puzzle. They consist of a2 We also provide generalization of this idea to permutation state spaces that are not necessarily strict.516U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540Fig. 1. The 8-, 15- and 24-puzzle goal states.Fig. 2. 3 × 3 × 3 Rubik’s cube.square frame containing a set of numbered square tiles, and an empty position called the blank. The legal operatorsare to slide any tile that is horizontally or vertically adjacent to the blank into the blank position. The problem isto rearrange the tiles from some random initial configuration into a particular desired goal configuration. The statespace grows exponentially in size as the number of tiles increases, and it has been shown [19] that finding optimalsolutions to the sliding tile problem is NP-complete. The 8-puzzle contains 9!/2 (181,440) reachable state, the 15-puzzle contains about 1013 reachable states, and the 24-puzzle contains almost 1025 states. The goal states of thesepuzzles are shown in Fig. 1.The classic heuristic function for the sliding-tile puzzles is called Manhattan distance. It is computed by countingthe number of grid units that each tile is displaced from its goal position, and summing these values over all tiles,excluding the blank. Since each tile must move at least its Manhattan distance to its goal position, and a legal moveonly moves one tile, the Manhattan distance is a lower bound on the minimum number of moves needed to solve aproblem instance.2.2. Rubik’s cubeRubik’s cube was invented in 1975 by Erno Rubik of Hungary. It is one of the most famous combinatorial puzzleof our time. The standard version consists of a 3 × 3 × 3 cube (Fig. 2), with different colored stickers on each of theexposed squares of the sub-cubes, or cubies. Any 3 × 3 × 1 edge plane of the cube can be rotated 90, 180, or 270degrees relative to the rest of the cube. In the goal state, all the squares on each side of the cube are the same color. Thepuzzle is scrambled by making a number of random moves, and the task is to restore the cube to its original goal state.There are about 4 × 1019 different reachable states. There are 20 movable cubies and 6 stable cubies in the center ofeach face. The movable cubies can be divided into eight corner cubies, with three faces each, and twelve edge cubies,with two faces each. Corner cubies can only move among corner positions, and edge cubies can only move amongedge positions.2.3. The pancake puzzleThe pancake puzzle is analogous to a waiter navigating a busy restaurant with a stack of n pancakes [2]. To avoiddisaster, the waiter wants to sort the pancakes ordered by size. Having only one free hand, the only available operationis to lift a top portion of the stack and reverse it. In this domain, a state is a permutation of the values 0 . . . (N − 1).A state has N − 1 successors, with the kth successor formed by reversing the order of the first k + 1 elements of theU. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540517Fig. 3. The 5-pancake puzzle.permutation (1 (cid:3) k < N ). For example, if N = 5 the successors of state (cid:3)0, 1, 2, 3, 4(cid:4) are (cid:3)1, 0, 2, 3, 4(cid:4), (cid:3)2, 1, 0, 3, 4(cid:4),(cid:3)3, 2, 1, 0, 4(cid:4) and (cid:3)4, 3, 2, 1, 0(cid:4), as shown in Fig. 3. From any state it is possible to reach any other permutation, so thesize of the state space is N!. In this domain, every operator is applicable to every state. Hence its branching factor isN − 1.2.4. Permutation state spacesA state space is a set of states, and a set of operators that map states to states. A specific problem instance is a statespace together with a particular initial state and goal state. The task is to find an optimal path from the initial state tothe goal state.Permutation state spaces are a special type of combinatorial problems, consisting of a set of m objects and nlocations (n (cid:2) m). The states in such problems are all the different ways of placing the objects in the locations with atmost one object per location.Strict permutation state spaces are a special case of permutation state spaces where the number of objects is exactlythe same as the number of locations, i.e., n = m.An operator in permutation state spaces changes the locations of some of the objects. This is a very rich andinteresting class of problems, including, for example, all finite mathematical groups. The puzzles defined above areall permutation state spaces, as are many of the classic benchmark problems for planning, such as the Blocks World.Note that general permutation problems even if solved by heuristic search do not necessarily span a permutationstate space. For example in the Travailing Salesman Problem (TSP) the task is to find the permutation of cities withthe optimal cost. However, this problem does not span a permutation state space as defined here. In TSP, there is nopredefined goal state and there is no meaning of finding a path from a given initial state to the goal state via otherpermutation states.3. HeuristicsThe efficiency of a single-agent search algorithm is usually dominated by the quality of the heuristic used. Thebest known heuristics for the application domains in this paper all take the form of a pattern database (defined below).Pattern databases are therefore used in all the experimental studies, and the purpose of this section is to give thebackground details on pattern databases. However, it is important to note that none of this paper’s key ideas (duality,dual heuristic evaluations, and dual search) depend on the heuristic being a pattern database, these ideas apply toheuristics of all forms.3.1. Pattern databasesA powerful approach for obtaining admissible heuristics is the use of pattern databases (PDBs) [1]. The state spaceof a permutation state space problem is all the different ways to placing the given set of objects into the locations.A subproblem is an abstraction of the original problem defined by only considering some of these objects whiletreating the others as “don’t care”. A pattern (abstract state) is a specific assignment of locations to the objects of thesubproblem. The pattern space or abstract space is the set of all the different reachable patterns of a given abstractproblem.518U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540Fig. 4. Example of regular lookups.Each state in the original state space is abstracted to a pattern of the pattern space by only considering the patternobjects, and ignoring the others. The goal pattern is the abstraction of the goal state.There is an edge between two different patterns p1 and p2 in the pattern space if there exist two states s1 and s2 ofthe original problem, such that p1 is the abstraction of s1, p2 is the abstraction of s2, and there is an operator of theoriginal problem space that connects s1 to s2.A pattern database (PDB) is a lookup table that stores the distance of each pattern to the goal pattern in the patternspace. A PDB is built by running a breadth-first search3 backwards from the goal pattern until the whole pattern spaceis spanned. A state S in the original space is mapped to a pattern S(cid:5) by ignoring details in the state description that arenot preserved in the subproblem. The value stored in the PDB for S(cid:5) is a lower bound (and thus serves as an admissibleheuristic) on the distance of S to the goal state in the original space since the pattern space is an abstraction of theoriginal space.Pattern databases have proven very useful for finding lower bounds for combinatorial puzzles [1,5,6,14,15].Furthermore, they have proved useful for other search problems (e.g., multiple sequence alignment [18,22] and plan-ning [3]).3.1.1. Pattern databases examplePDBs can be built for the sliding-tile puzzles, as illustrated in Figs. 4(a) and (b). Assume that the subproblem onlyincludes tiles 2, 3, 6 and 7. Patterns are created by ignoring all the tiles except for 2, 3, 6 and 7. Each pattern containstiles 2, 3, 6 and 7 in a unique combination of positions. The resulting {2–3–6–7}-PDB has an entry for each patterncontaining the distance from that pattern to the goal pattern (shown in the lower part of Fig. 4(b)). Fig. 4(b) depictsthe PDB lookup in this PDB for estimating a distance from a given state S to the goal (Fig. 4(a)). State S is mapped toa 2–3–6–7 pattern by ignoring all the tiles other than 2, 3, 6 and 7. Then this pattern’s distance to the goal pattern islooked up in the PDB. To be specific, if the PDB is represented as a 4-dimensional array, PDB, with the array indexesbeing the locations of tiles 2, 3, 6, and 7 respectively, the lookup for state S is PDB[8][12][13][14] (tile 2 is in location8, tile 3 is in location 12, etc.). The value retrieved by a PDB lookup for state S is a lower bound (and thus serves asan admissible heuristic) for the distance from S to the goal state in the original space. In this paper, accessing the PDBfor a state S will be referred to as a regular lookup, and the heuristic value will be referred to as a regular heuristic.3.1.2. Additive pattern databasesAdditive pattern databases provide the current best admissible heuristic for the sliding-tile puzzles [5,15]. The tilesare partitioned into disjoint sets (patterns) of tiles and a PDB is built for each set. The PDB stores the cost of moving3 This description assumes all operators have the same cost. The techniques easily extend to the case when operators have different costs.U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540519Fig. 5. Partitionings and reflections of the tile puzzles.Fig. 6. Both S and G are reflected about the main diagonal to get another possible PDB lookup.the tiles in the given subproblem from any given arrangement to their goal positions. If for each set of tiles only themoves of tiles from the given set are counted, then values from different disjoint PDBs can be added and the resultis still admissible. An x–y–z partitioning is a partition of the tiles into disjoint sets with cardinalities of x, y and z.Fig. 5 presents the two 7–8 partitionings for the 15-puzzle and the two 6–6–6–6 partitionings for the 24-puzzle whichwere first used in [5,15].3.2. Geometric symmetriesIt is common practice to exploit special properties of a state space to enable additional lookups to be done in aPDB. In [1] several alternative lookups that can be made in the same PDB based on the physical symmetries of the15-puzzle are described. For example, because of the symmetry about the main diagonal, the PDB built for the goalpattern in Fig. 4(b) can also be used to estimate the number of moves required to get tiles 8, 9, 12 and 13 from theircurrent positions in state S to their goal locations. As shown in Fig. 6, both S and G are reflected about the maindiagonal yielding Sr and Gr . The 2–3–6–7 PDB can be used to get a lower bound on the number of moves required toget tiles 8, 9, 12, and 13 from their current positions in state Sr to their locations in Gr . This is identical to the numberof moves required to move them from their current positions in state S to their goal locations in G.This idea of reflecting the domain about the main diagonal for having another set of PDBs was also used to solvethe 15-puzzle and 24-puzzle with additive PDBs [5,15]. It is easy to see that the two 7–8 partitionings for the 15-puzzle(and similarly those of the 24 puzzle) in Fig. 5 are reflections of each other about the main diagonal and only one PDBis needed in practice. For another example of geometric symmetry, consider Rubik’s cube and assume there is a PDBfor the blue face which gives values for all cubies with blue colors. Reflecting and rotating this puzzle will enablesimilar lookups for any other face with a different color (e.g., yellow, red, etc.) since any two faces are symmetric.Because all valid, alternative PDB lookups provide lower bounds on the distance from state S to G, their maximumcan be taken as the value for h(S). Of course, there is a tradeoff for doing this—each PDB lookup increases the timeit takes to compute h(S). Because additional lookups provide diminishing returns in terms of the reduction in thenumber of nodes generated, it is not always best to use all possible PDB lookups [1]. A number of methods exist forreducing the time needed to compute h(S) by making inferences about some of the values without actually lookingthem up in a PDB [10].520U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–5404. Simple dualityIn this paper, two types of duality are presented. This section introduces simple duality, which applies to strictpermutation state spaces (the number of objects and locations is the same, as in Rubik’s cube) in which the operatorshave no preconditions (every operator is applicable to every state). Section 8 generalizes the notion of duality to statespaces in which there may be more locations than objects and operators may have preconditions.4.1. Assumptions for simple dualityBefore defining the dual state, we first present four assumptions which are preconditions for simple duality:(1) Every state is an arrangement of m objects into n locations, with m = n and exactly one object per location. Inother words, for simple duality we require that the state space is a strict permutation state space. For example,the most natural representation of the 8-puzzle has 9 objects, eight representing the individual tiles and onerepresenting the blank. This assumption will be relaxed in Section 8 to allow n (cid:2) m and at most one object perlocation.(2) A set of operators is given that change the locations of some of the objects. We assume that the operators’ actionsare location-based permutations, meaning that an operator re-arranges the contents of a certain set of locationswithout any reference to specific objects. For example, an operator could swap the contents of locations A and B.(3) The operators are invertible, and an operator and its inverse cost the same. Consequently, if operator sequence Ocan be applied to state S1 and transform it into S2, then its inverse, O−1, can be applied to state S2 and transformit into S1 at the same cost as O.(4) Operators have no preconditions. That is, every operator is applicable to every state. This assumption is onlyassumed for simple duality. In Section 8 assumption 4 is dropped, resulting in the notion of general duality wherethe operators are not necessarily applicable to every state but have internal preconditions.Example domains where assumption 4 is violated are the sliding tile puzzle and the Towers of Hanoi. In the formerthere is a precondition which refers to the location of the blank. In the latter, there is a precondition which refers tothe topmost discs on the operator’s source and destination pegs.Two definitions for the dual state follow, and a proof that they are equivalent.4.2. Simple duality: Definition 1For any given pair of states, S1 and S2, there is a unique location-based permutation, π , that transforms S1 to S2.For example, π in Fig. 7(a) describes how the objects move from their locations in the 4-pancake state S to their goallocations in G. The letters a, b, c and d denote the locations. π maps a to c in Fig. 7(a) because the object (3) that isin location a in S is in location c in G. The permutation π is entirely determined by the state descriptions of S and G,it does not depend on the operators that define the state space. In particular, π is defined whether or not there exists asequence of operators that transforms S into G.Dual state (Definition 1). For state S and goal state G, let π be the location-based permutation such that π(S) = G.Then Sd , the simple dual of S for goal G, is defined to be π(G).Fig. 7. Location-based permutation π that maps S to G (a) and G to Sd (b).U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540521Fig. 8. Simple duality, G = π(S) and Sd = π(G).let G be the goal statefor each location xlet o be the object located in x in Slet y be the location of o in Gdefine πx = yDual(State S)122.12.22.33.3 endfor34Sd = π(G)return SdAlgorithm 1. Dual-Calculating the dual state according to Definition 1.This definition is illustrated in Fig. 8. As will be shown below, with the assumptions, the cost of reaching G from Sand from Sd is the same, and therefore max(h(S), h(Sd )) is an admissible heuristic for S for any admissible heuristich. For example, if PDBs are being used and h(S) = PDB[S], for some given PDB for G, then h(Sd ) = PDB[Sd ] isalso admissible for S.In practice Sd is calculated by constructing π from the descriptions of S and G and then applying π to G as shownin Algorithm 1. Therefore, Sd can be calculated from the description of S without the need to know any actual pathfrom S to G as illustrated in Fig. 7(b).4.3. Simple duality: Definition 2The advantage of our first definition is that it properly motivates the location of the dual state in the search space.We now provide an alternative definition for the dual state analogous to the dual concept in the constraint satisfactionfield (e.g., [9]). We then prove that the two definitions are equivalent.In strict permutation state spaces, the roles played by objects and locations in representing a state are interchange-able. Usually in the vector containing the state description, the locations are the variables and the objects are thevalues. This is called the regular representation of the state. Flipping the roles of objects and locations in the vectorthat describes S yields the dual representation. Here, objects are the variables and locations are the values. Given avector of size K that represents a state, the regular representation treats it as the objects that occupy locations {1 . . . K}.The dual representation will treat them as the locations that are occupied by objects {1 . . . K}. For example, if the rep-resentative vector is (cid:3)3, 1, 4, 2(cid:4) then the regular representation refers to a state S = (cid:3)3, 1, 4, 2(cid:4) (object 3 in location 1,object 1 in location 2, etc.). The dual representation of this vector corresponds to the dual state Sd where object 1 isin location 3, object 2 is in location 1, etc. The dual state in its regular representation is Sd = (cid:3)2, 4, 1, 3(cid:4).In this section, we assume a canonical definition of the goal state G. That is, given an enumeration of both the ob-jects and the locations, then in G object i is located in location i. For the goal state, the regular and dual representationare identical.Dual state (Definition 2). Given a vector representation V of a state S, the dual state, Sd , is defined to be the statewhich is described by V in the dual representation.Algorithm 2 is based on Definition 2 and calculates the regular representation of the dual state Sd from the regularrepresentation of S.522U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540Dual(State S)1. For each object x ∈ S1.1. Let y be the location of object x in S1.2. In Sd place object y in location x2. Return SdAlgorithm 2. Dual-Calculating the dual state based on Definition 2.Fig. 9. The relation between a state S and its dual state Sd . Given a representative vector (cid:3)3, 1, 4, 2(cid:4), then its regular representation corresponds toS while its dual representation corresponds to Sd .4.4. AnalysisA number of theorems concerning duality are now given. The following terminology is used. LocS(x) = y indicatesthat object y is located at location x in state S. Therefore, in the goal state G, ∀i LocG(i) = i. Copt(X, Y ) denotes thecost of an optimal path from state X to state Y .Theorem 4.1. Definitions 1 and 2 are equivalent.Proof. Let G be the canonical goal state, S be a state, π be the permutation such that π(S) = G, and Sd be the dualof state S obtained by Definition 1. It needs to be shown that if in S an arbitrary object j is located in location i, thenin Sd object i will be located in location j . Assume that π moves the content of location i to location j . Applying πfor the first time (on S) will move object j from location i to location j (its home location in G). Applying π for thesecond time (on G) will move object i from its home location to location j . (cid:2)Fig. 9 shows the relation between state vector S (Fig. 9(a)) and its dual Sd (Fig. 9(d)) according to both definitions.Fig. 9(a,b) shows S being mapped to G by the permutation π , with the definition of π written beneath the arrow((1, 3) means that the object in location 1 in S is mapped to location 3 in G, etc.). In the lower part of the figure, π isapplied to G to produce Sd . The vector that describes S, (cid:3)3, 1, 4, 2(cid:4), means that location 1 is occupied by object 3, 2by 1, etc. In the dual representation (where objects are the variables and locations are the values) this vector meansthat object 1 is in location 3, object 2 is in location 1, etc. The state that corresponds to the dual representation is Sdin Fig. 9(d).Theorem 4.2. (Sd )d = S.Proof. Show that these states have the same objects in the same location. We will show that ∀Location y: (LocS(y) =x) (cid:8)⇒ (Loc(Sd )d (y) = x).And indeed,∀Location y: (LocS(y) = x)def 2(cid:8)⇒ (LocSd (x) = y)def 2(cid:8)⇒ (Loc(Sd )d (y) = x).(cid:2)Theorem 4.3. If O = {o1, o2, . . . , on} is a legal path from S to G then O−1 = {o−1Sd to G and has the same cost as O.n , . . . , o−12 , o−11} is a legal path fromProof. If O = {o1, o2, . . . , on} is a legal path from S to G then O is also a legal path from G to Sd (Definition 1).Because all operators can be reversed (Assumption 3) the sequence of operators O−1 = {o−1} is a legaln , . . . , o−12 , o−11U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540523path from Sd to G. Since operators and their inverses cost the same (Assumption 3) the cost of O and O−1 is thesame. (cid:2)Theorem 4.4. Copt(S, G) = Copt(Sd , G).Proof. There are two cases to consider. If there does not exist an operator sequence transforming S into G, thenthere cannot exist an operator sequence transforming Sd into G and therefore Copt(S, G) = Copt(Sd , G) = ∞. Al-ternatively, if there does exist an operator sequence transforming S into G, let O be a minimum-cost sequence ofn operators that transforms S into G. According to Theorem 4.3, O−1 is a legal path from Sd to G of the samecost and therefore Copt(S, G) (cid:2) Copt(Sd , G). Applying the same reasoning to any minimum-cost path from Sd toG implies Copt(Sd , G) (cid:2) Copt((Sd )d , G) Theorem 4.2(cid:8)⇒ Copt(Sd , G) (cid:2) Copt(S, G). These two inequalities together implyCopt(S, G) = Copt(Sd , G). (cid:2)As a result, any admissible heuristic for state Sd is also admissible for state S, and vice versa. Therefore, givena heuristic h for each state S, its dual heuristic hd (S) can also be calculated (the regular heuristic of the dual state,h(Sd )). If PDBs are being used, then a PDB lookup for the dual state Sd is used as a heuristic bound for S. Such aPDB lookup for Sd , is called the dual PDB lookup for S.45. Attributes of the dual heuristicIn this section different attributes of the dual heuristic are discussed.5.1. When the regular and dual heuristics provide different valuesAdmissible heuristics for permutation problems can be divided into two types. Let π be the permutation thattransforms S to G. The first type of heuristic calculates its value by considering the effect of π (the current location)for all the objects of the state. For example, Manhattan distance provides a heuristic bound for moving each of thetiles. The second type of heuristic considers the effect of π for only a subset of the objects. PDBs, for example, providefull solutions to the relaxed problem that contains only a subset of the objects.For the first type of heuristic (e.g., Manhattan distance), the dual and regular heuristics are equal. This is becauseboth states are reached from the goal state by applying the same permutation π (in reverse directions) and the heuristicconsiders a similar effect of π for all the objects of the state for both S and Sd . Consider state S and Sd as provided inFig. 10. For each tile in S a unique tile can be found in Sd with the same Manhattan distance (the same effect of π ).For example, in S, tile 1 (located in location 5) has to move one up—the Manhattan distance of this tile is 1. Similarly,in Sd tile 5 (located in location 1) has to move one step down. Note that this is only true when all the objects in thestate are considered.Fig. 10. 15-puzzle duality and Manhattan distance.4 In [7], the same idea of flipping the roles of objects and locations is used to produce dual patterns and dual PDB lookups. The dual representationpresented in this paper generalizes this principle to the entire state and allows any heuristic of the dual state Sd (not just PDBs) to be used for S.The definition of dual PDB lookups of [7] is different but is equivalent to the regular lookup of the dual state (i.e., to PDB[Sd ]) which is definedhere.524U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540For the second type, the regular and dual heuristics are not necessarily equal. Using only a subset of objects mightcause a different heuristic result. For example, assume that only tile 1 is being considered by Manhattan distance. InFig. 10 tile 1’s Manhattan distance heuristic in S is 1 while in Sd it is 4. For PDBs, only a subset of the objects isconsidered and therefore the heuristic of the dual state can be different. For example, assume that in Fig. 10, the PDBbased on tiles {2, 3, 6, 7} is used. The PDB lookup for S has these tiles in locations {8, 12, 13, 14}. The PDB lookupfor Sd has these tiles in locations {9, 5, 15, 12}. These tiles now have to travel different paths to get to the goal stateand therefore S and Sd have completely different entries in the PDB with completely different values.5.2. Inconsistency of the dual heuristicA heuristic h is consistent if for any two states, x and y, |h(x) − h(y)| (cid:3) dist(x, y) where dist(x, y) is the optimaldistance between them. In other words, the difference between the heuristic values of two states is never greater thenthe cost of a path between them. When moving from a parent node to a child node, using the heuristic of the dualstate might produce inconsistent [21] values even if the heuristic itself (in its regular form) is consistent. In a standardsearch, a parent state, P , and any of its children, S, are neighbors by definition. Thus a consistent heuristic mustreturn consistent values when applied to P and S. However, the heuristic values obtained for P d and Sd might not beconsistent because P d and Sd are not necessarily neighbors. This is a consequence of the following corollary.Corollary 5.1. Let P and S be two states and let c be the actual distance between them. The distance between P dand Sd is not necessarily c. In particular it might be larger.Proof. An example is sufficient. Consider the 9-pancake puzzle states shown in Fig. 11. State G is the goal state ofthis puzzle. State S1 is the neighbor of G obtained by reversing the tokens at locations 1–3 (shown in the bold frame),and state S2 obtained by further reversing the tokens in locations 1–6. States Gd , Sd2 are the dual states of G,S1 and S2 respectively. Observe that while states S1 and S2 are neighboring states, Sd2 (their duals) are not1 will not arrive at node Sdneighbors. Reversing any consecutive k first tokens of state Sd1 and Sd2 .52 which differ by more than 1. Using these values for S1and S2 would be inconsistent since they are neighbors. This can be shown by the following PDB example. Supposepatterns for the 9-pancake puzzle are defined by only considering tokens 4–6 while ignoring the rest of the tokens. TheA consistent heuristic might return values for Sd1 and Sd1 and SdFig. 11. 9-pancake states.5 Note that in this particular example S1 and Sdand state S1 is a single move away from the goal.1 are identical. In this domain applying a single operator twice in a row will reach the same stateU. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540525Fig. 12. Geometrical and dual symmetries.Fig. 13. Propagation of values with inconsistent heuristics.resulting PDB provides distances to the goal pattern from all reachable patterns. The right column of Fig. 11 showsthe corresponding pattern for each state obtained by using the ∗ symbol to represent a “don’t care”.Regular PDB lookups produce consistent heuristic values during search [11]. Indeed, since states S1 and S2 areneighbors, their PDB heuristic values differ by at most 1. In state S1, tokens 4–6 are in their goal locations andtherefore h(S1) = 0. In state S2 tokens 4–6 are not in their goal locations and we need to apply one operator to reachthe goal pattern and thus h(S2) = 1. Dual PDB lookups are admissible, but not necessarily consistent. The dual PDBlookup for state S1, i.e., the PDB lookup for state Sd1 returns 0 since tokens 4–6 are in their goal location for stateSd1 . However, the pattern projected from state Sd2 is two moves away from the goal pattern. Thus, performing the duallookup for states S1 and S2 (i.e., PDB lookups for states Sd2 ) will produce heuristics that are inconsistent (0and 2). Thus when moving from S1 to S2 (or vice versa), even though g was changed by 1, h was changed by 2. (cid:2)1 and Sd5.3. Geometrical symmetries versus dual symmetriesThere is a major difference between states obtained by a dual symmetry and states obtained by a geometricalsymmetry. This difference is illustrated in Fig. 12. As derived from Corollary 5.1 above, given a state S and its dualSd the neighbors of Sd are not necessarily the dual states of the neighbors of S. This is shown in Fig. 12b. Forgeometrically reflected states (such as Sg of Fig. 12a), however, exactly the opposite of Corollary 5.1 is true. That isif the distance between states P and S is c then the distance between P g and Sg (the geometrical reflected state of Pand S) is exactly c. Geometrical symmetries only transform the domain without changing its internal structure. As aresult, neighbors of a reflected state Sg are also reflections of the neighbors of S. Therefore, using heuristics of thereflected states (that is, using the PDB lookup towards the reflection of the goal as described above in Section 3.2)also produce consistent heuristics.5.4. Bidirectional pathmaxIn [7,21], the bidirectional pathmax (BPMX) method for propagating inconsistent heuristic values during searchwas introduced, and experiments showed that it can be effective in pruning subtrees that would otherwise be explored.The bidirectional pathmax method is illustrated in Fig. 13. The left side of the figure shows the (inconsistent)heuristic values for a node and its two children. When the left child is generated, its heuristic (h = 5) can propagateup to the parent and then down again to the right child. To preserve admissibility, each propagation reduces h by thecost of traversing that path (1 in this example). This results in h = 4 for the root and h = 3 for the right child. Whenusing IDA∗, this bidirectional propagation can cause many nodes to be pruned that would otherwise be expanded. For526U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540example, suppose the current IDA∗ threshold is 2. Without the propagation of h from the left child, both the root node(f = g + h = 0 + 2 = 2) and the right child (f = g + h = 1 + 1 = 2) would be expanded. Using the propagation justdescribed, the left child will increase the parent’s h value to 4, resulting in a cutoff without even generating the rightchild. BPMX should be regarded as an integral part of any search algorithm when the heuristic is inconsistent and theoperators are invertible, and it is used in all the experiments reported in this paper.6. Dual searchTraditionally, heuristic search algorithms find optimal solutions by starting at the initial state and traversing thestate space until the goal state is found. The various traditional search algorithms differ in their decision as to whichstate to expand next, but in all of them a solution path is found only after all the states on the path have been traversed.Dual search has the remarkable property of not necessarily visiting all the states on the solution path. Instead, itconstructs its solution path from path segments that it finds in disparate regions of the state space. In this paper, thefocus is on DIDA∗, the dual version of IDA∗. Dual versions for other algorithms can be similarly constructed.6.1. Dual IDA∗ (DIDA∗)Recall that the distance to the goal G from both S and Sd is identical and therefore the inverse, O−1, of any optimalpath, O, from Sd to G is an optimal path from S to G. This fact presents a choice, which DIDA∗ exploits, for howto continue searching from S. For each state S, DIDA∗ computes h(S) and h(Sd ). Suppose that max(h(S), h(Sd ))does not exceed the current threshold. DIDA∗ can either continue from this point using S, as IDA∗ does, or it canswitch and continue its search from Sd . Switching from S to Sd is called jumping. A simple policy for making thisdecision is to jump if Sd has a larger heuristic value than S—larger heuristic values suggest that the dual side has abetter chance of achieving a cutoff sooner (due to the locality of the heuristic values). This is referred to as the jump iflarger (JIL) policy. Deciding when to jump is an important part of the algorithm, and alternatives to JIL are discussedlater. Of course, later on in the search, DIDA∗ might decide to jump back to the regular side (e.g., when that heuristicvalue is better). Once the goal state is reached an optimal solution path can be reconstructed, as described below, fromthe sequence of dual and regular path segments that led to the goal from the start.Fig. 14 illustrates the difference between IDA∗ and DIDA∗. In Fig. 14(a), IDA∗ finds a path from S0 to G. InFig. 14(b), the DIDA∗ search starts the same: starting at regular state S0 moves 1 and 2 are made, leading to state1 . No further switches occur, and DIDA∗S1. Then, because of its jumping policy, DIDA∗ switches to the dual state Sdcontinues on the dual side until the goal G is reached. In Fig. 14(c), the DIDA∗ search starts out the same as inFig. 14(b) but at state Sd2 a jump is made back to the regular side and DIDA∗ continues from S2 to G.6.2. Constructing the solution pathThe correctness of DIDA∗ is best seen by considering how the path segments it finds are joined together to createa path from start to goal. IDA∗ constructs its solution path by backtracking from the goal state to the start state,recovering the path in reverse order. This will not work in DIDA∗ since some of the moves are on the regular side(i.e., the forward search) while some are on the dual side (i.e., the backward search). The solution is to maintain anFig. 14. Dual IDA∗search (DIDA∗).U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540527(initial_state S) (returns an optimal solution)let threshold = max(h(S), h(Sd ))let Path = NULLrepeat{∗1 DIDA2344.14.24.3 } until GoalFound5return PathGoalFound = DDFS(S,NULL,NULL,0,REGULAR,Path,threshold)threshold = next thresholdAlgorithm 3. DIDA∗.additional bit per state during the search, the side bit, indicating whether the search at that point is on the regular orthe dual side. At the start of the search, the side bit is set to REGULAR. A child inherits the bit value of its parent, butif a jump occurs, the value of the side bit is flipped. To construct the solution path, DIDA∗ backtracks up the searchtree to recover the moves made to reach the goal. If the side bit for the current move, o, has the value REGULAR,then o is added to the f ront of the partially built path as usual. However, if the side bit indicates that o is on the dualside, then its inverse, o−1, is added at the end of the partially built path.It is important to note that (based on Theorem 4.3) only the operators are taken from the dual side to the regularside. The exact states visited by the path in the dual side are not necessarily the duals of the states of the actual solutionpath.In Fig. 14(a), when IDA∗ backtracks, the solution path is reconstructed by adding the moves to the front of thepartially built path, resulting in the path being built in the order {6}, {5, 6}, . . . , {1, 2, 3, 4, 5, 6}. Fig. 14(b) illustrateshow this works in DIDA∗. Backtracking from G will lead to the following pairs of values (corresponding to the moveand the side bit) in the order {(3−1, D), (4−1, D), (5−1, D), (6−1, D), (2, R), (1, R)}. Since the side bit of the firstfour moves indicates that they belong to the dual side, the inverses of those moves are added to the end of the partiallybuilt path, yielding the partially built paths of {3}, {3, 4}, {3, 4, 5}, {3, 4, 5, 6}. Now the side bit indicates that the searchoccurred in the regular side. Hence the next two moves are inserted at the front of the path, obtaining {2, 3, 4, 5, 6} and{1, 2, 3, 4, 5, 6}. The dashed line in Fig. 14(b) shows how to concatenate the solution path from Sd1 to G in its correctplace.Algorithm 3 presents the pseudocode for DIDA∗. DIDA∗ mirrors IDA∗ by iteratively increasing a solution costthreshold until a solution is found. Each iteration calls DDFS (dual depth-first search) which is presented in Algo-rithm 4. DDFS recurses until a solution is found or the cost threshold is exceeded. DIDA∗ differs from a standardIDA∗ search in several respects. First, each call to DDFS includes extra parameters: a side_bit (indicating if the searchis currently on the REGULAR or DUAL side) and the last move made on the regular and dual sides (used for operatorpruning, as explained in Section 6.4). Second, a jump decision is included in DDFS, possibly resulting in a jump(lines 5–5.4). Finally, when the goal has been found, the reconstruction of the solution path distinguishes between theregular and dual sides (lines 6.3.1–6.3.2).6.3. The benefit of jumpingThe regular and dual states are different and, hence, there can be large differences in the (admissible) heuristicvalues between states S and Sd .6 By using the side that has the highest heuristic value (for the current context), oneis increasing the chances of moving into a region of the search space with values high enough to create a cutoff.Of course, the decision to switch sides is a heuristic and not guaranteed to improve the search every time a jump ismade.Consulting the heuristic of the dual state introduces diversity into the heuristic values obtained during the search;information is obtained from a different area of the search space. DIDA∗ introduces a stronger diversity since it is notonly peeking but is physically jumping into that area.6 For example, experiments on the 17-pancake problem with a heuristic which has a maximum value of 14, the difference observed was up to 8.528U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–5406.4. The penalty for jumpingUsually, depth-first search algorithms avoid generating duplicate nodes by disallowing operators that can be shownto be irrelevant based on the previous sequence of operators. The simplest example of this is disallowing the inverseof the previous operator. More sophisticated techniques enforce an ordering on the operators, disallowing redundantsequences. Such mechanisms are referred to as operator pruning in this paper. Operator pruning can significantlyreduce the branching factor. For example, the branching factor of Rubik’s cube at the root node is 18, but the averagebranching factor below the root can be reduced by operator pruning to 13.34 [14].There can be no operator pruning at the start state, because there is no search history. Let its branching factor be b.Subsequent nodes in a normal search have a smaller branching factor, at most b − 1, because of operator pruning. Bycontrast, DIDA∗ sometimes pays a branching-factor penalty for jumping to a dual state. As before, the start state hasa branching factor of b, and subsequent nodes on the regular side have a lower branching factor. However, on everybranch of the search tree, when a jump is made to the dual side for the first time only, the dual state has no searchhistory and will have a branching factor of b. On subsequent jumps on a branch, the history on that side can be usedto do operator pruning. In Algorithm 4, the previous moves from the regular and dual sides are passed as parameters,allowing DIDA∗ to prune the inverse of the previously applied operator on a given side.2 is reached. DIDA∗ then jumps to the dual state of SdTo illustrate this, consider Fig. 14(c). DIDA∗ has to consider all operators at the start state, S0. Moves 1 and 2 aremade on the regular side, reaching S1. Here DIDA∗ decides to jump to Sd1 ; a completely new state with no history.Thus, operator pruning is not possible here and all the operators must be considered. DIDA∗ makes moves 6−1, 5−1and 4−1 on the dual side until state Sd2 , S2, back on the regularside. Because it is returning to the regular side, a history of the previous moves is known and operator pruning canbe used in expanding S2. For example, the previous operator on this side is operator 2, so its inverse, 2−1, can beignored. To understand why operator pruning can be applied, even though S1 bears no apparent relation to S2, recallhow DIDA∗ constructs its final solution path. If a path is found leading from S2 to the goal, the first operator on thispath will be placed immediately after the operator that leads to S1 in the final solution path. Since this path is optimal,it cannot possibly contain an operator followed immediately by its inverse. The same reasoning justifies the use ofmore sophisticated operator pruning techniques as well. In IDA∗, operator pruning can be used at all nodes except theroot. In DIDA∗, the first time a jump is made, on any given branch, no history is available and operator pruning isunavailable. For example, in Rubik’s cube, when performing the first jump, on any branch, DIDA∗ has 18 children toconsider, as opposed to the average of 13.34 that would be seen by IDA∗.1 boolean DDFS(state S, previous_move pmr ,previous_dual_move pmd , depth g, bool side_bit, List Path,int threshold)let h = max(h(S), h(Sd ))if (h + g) > threshold return falseif S = goal_state return trueif should_j ump(S, Sd ){S = Sdswap(pmr , pmd )side_bit = ¬side_bitfor each legal_move m {23455.15.25.35.4 } endif66.16.26.36.3.16.3.26.3.36.3.46.47} endif} endforreturn false−1 continueif m = pmrgenerate child C by applying m to Sif DDFS(C, m, pmd , g + 1, side_bit, Path, threshold) = true{/*operator pruning*/if (side_bit = REGULAR) then Path = m :: Pathelse Path = Path :: m−1return trueAlgorithm 4. DDFS “::” adds an element to a list.U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540529To avoid the penalty of jumping, a degenerate jumping policy, which only allows a jump at the root node, can beused (JOR). If h(root) > h(rootd ) then the search is conducted on the regular side, otherwise it is conducted on thedual side. No further jumps are allowed for JOR.7. Experimental results for simple dualityThis section provides experimental results that show the benefit of performing dual lookups and using the dualsearch algorithm. Pattern databases are used in all the experimental domains because they represent the state-of-the-art heuristics.7.1. Rubik’s cubeKorf first solved the 3 × 3 × 3 Rubik’s cube with PDBs [14]. As discussed above, the cubies of Rubik’s cube canbe divided to corner cubies and edge cubies. As a first experiment to test the duality ideas, a 7-edge-cubies PDB wasbuilt, the largest that can be stored in 1 GB of memory. There are 510,935,040 possible permutations of the sevenedge cubies. At four bits per entry, 255 MB are needed for this PDB. The heuristics used in this set of experimentswere based on this 7-edges PDB.Table 1 presents results for this set of experiments. The experiments above were on 100 instances with length (cid:3) 14.The table columns are as follows:Heuristic: Which heuristic was used for the PDB lookups: r stands for the regular state and d for its dual state.Similarly, 4r (4d) means that we took the maximum of 4 regular (dual) heuristics.OP—operator pruning: “+” means that the operator leading to a node’s parent is pruned; “−” means no operatorsare pruned.Search: Search algorithm (IDA∗ or DIDA∗).Policy: The jumping policy used by DIDA∗.Nodes and Time: Average number of generated nodes and the average time needed to solve a problem with 3.0 GHzPentium 4 machine with 2 GB of memory.Jumps: Average number of times that DIDA∗ jumped between the regular and dual sides.The first line of this table shows the results of IDA∗ using a regular PDB lookup. These searches generated anaverage of 90 million nodes. Intuitively, one might think that performing only a dual lookup should produce the sameresults since the exact same PDB is being queried. Surprisingly, however, line 2 shows that when using the dualheuristic the number of generated nodes decreases to only 8 million nodes, an improvement factor of 11. The reasonfor this dramatic improvement is as follows. While values in a PDB are locally correlated, the dual lookup frequentlylooks in (“jumps” to) different areas of the PDB. Thus, the general flow of the search benefits from a large diversityof areas in a PDB and a “bad” area can be quickly escaped from. Since the dual heuristic is inconsistent, BPMX wasTable 1Rubik’s cube (7-edges PDB) results#123456789Heuristicrdmax(r, d)max(r, d)max(r, d)max(r, d)max(r, d)max(4r, 4d)max(24r, 24d)OP+++++−−++Search∗∗∗IDAIDAIDA∗∗∗DIDADIDA∗IDADIDA∗∗IDAIDAPolicyNodes–––JILJOR–JIL––90,930,6628,315,1162,997,5392,697,0872,464,68529,583,45219,022,292615,563362,927Time28.183.241.341.161.0230.2720.440.510.90Jumps–––15,0130.23–3,627,504––530U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540Table 2Rubik’s cube resultsHeuristicsmax(8, 6, 6)max(8, 6, 6, 6d, 6d)max(8, 7, 7, 7d, 7d)Nodes352,656,042,894253,863,153,49354,979,821,557Time102,36291,29544,201Memory130,757130,757299,757also used.7 In line 3, the maximum of both the regular and the dual heuristics is used. This further reduced the numberof generated nodes to roughly 3 million, an improvement of a factor of 30 over the benchmark of line 1.Lines 4–5 shows the results for DIDA∗ using different jumping policies. DIDA∗ with JIL (line 4) yields a modestimprovement over line 3. Applying the JOR policy (line 5) further improves the results by a modest amount. TheJump value reveals that in 23 of the cases the dual heuristic at the start state was better and the search was performedin the dual side; the other 77 cases had ties or a better regular heuristic.To better understand the penalty incurred by the first jump of DIDA∗, operator pruning was disabled and the resultsfor IDA∗ and DIDA∗ compared. Results are provided in lines 6–7. Here, the operator that leads to a node’s parent is notpruned. In both cases, the maximum of the PDB lookups for the regular and dual states was used. Disabling operatorpruning increases the search effort by a factor of 10 when compared to line 3 where operator pruning was enabled.Results show that in this setting DIDA∗ with JIL reduced the number of generated nodes by one third compared toIDA∗. The improvement factor of DIDA∗ over IDA∗ was more significant than those reported in lines 4 and 5 sincenow the penalty of the first DIDA∗ jump was minor because the operator pruning was disabled.Due to geometrical symmetries in this domain there are multiple possible regular and dual lookups. Many com-binations of geometrical reflected regular lookups and geometrical reflected dual lookups were tried. Since DIDA∗does not seem to produce significant improvements for this domain, only IDA∗ was used here (no jumping). The bestresults achieved reduced the number of nodes generated (when taking 24r + 24d) by a factor of 250, and the time(4r + 4d) by a factor of 55. All this was possible with just one 7-edge-cubies PDB stored in memory.The results obtained for this set of experiments yield the following insights. First they show that dual lookups areeffective and using them reduces the search effort by an order of magnitude. Second, these results show that operatorpruning is important and using it reduced the search effort by an order of magnitude. Third, it shows that in thisdomain, the penalty of DIDA∗ almost offsets the benefits and using DIDA∗ only improves IDA∗ by a modest amount.Korf’s original 1997 Rubik’s cube experiments on 10 random instances were repeated [14]. Again, since DIDA∗does not seem to produce significant improvement in this domain, only IDA∗ was used with dual PDB lookups forthis set of experiments. Korf used three PDBs for this domain: one PDB for the eight corner cubies and two PDBsfor two sets of six edge cubies. Since a legal move in this domain moves eight cubies, the only way to combine thesethree PDBs is by taking their maximum. Note that there are eight corner cubies and all eight are used by the 8-cornerPDB. Thus, performing a dual lookup for this particular PDB is irrelevant. Here, the entire space of corner cubies isin the database and both lookups give the same result.8Results for the same set of 10 random instances used in [14] were obtained and are provided in Table 2. Theresults for Korf’s set of 8 + 6 + 6 PDBs were improved by a modest amount by adding the dual lookups for both6-edge PDBs (from 353 billion nodes to 253 billion). Increasing the edges PDB from six to seven cubies and usinga 8 + 7r + 7r + 7d + 7d setting reduced the search to 54 billion nodes—an improvement of a factor of 6.4 overKorf’s initial setting. The improvements of adding dual lookups for the 6- and 7-edges PDBs are modest since mostof the time the 8-corner PDB has the maximum value; this PDB is larger and contains more cubies than the 6- and7-edge PDBs. This can be seen in the following rates, which were measured over 10 million random instances. Forthe 8 + 6r + 6r + 6d + 6d setting, the 8-corner PDB had the maximum value for 73.5% of the instances while one ofthe lookups in the 6-edges cubies was the maximum for only 7.3% of the instances (the rest of the instances were atie). These numbers changed to 40.8% and 21.3% respectively for the 8 + 7r + 7r + 7d + 7d setting.7 In fact, we observed that a significant part of the 11-fold improvement (a 2.3-fold improvement) is due to activating BPMX. See [7,21] for adeeper treatment of BPMX.8 Since corner cubies can switch locations only with corner cubies and the entire space corner cubies is in the database then the heuristic alwaysreturns the optimal cost. The regular state and the dual state share the same optimal distance to the goal state thus both evaluations will return theexact value (which is the real cost).U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–5405317.2. Pancake puzzleUnlike the other puzzles discussed in this paper, the pancake puzzle does not have geometrical symmetries [2].This is a consequence of the special structure of the problem; each location has different attributes (such as how manyoperators are applicable to a location and where the location can be permuted to). Therefore, the dual heuristic isimportant because it provides the only additional possibility for obtaining another heuristic “for free”.Table 3 presents results averaged over 10 random instances of the 17-pancake puzzle. The heuristic used was aPDB based on the rightmost tokens 10, 11, . . . , 16 (which gives slightly better average heuristic values than a PDBbased on tokens 0, 1, . . . , 6). Here again, the phenomenon is observed that the dual heuristic is much better than theregular heuristic and the improvement factor is 23.8 in terms of generated nodes.9 When taking the maximum of theregular and dual heuristic, an improvement was obtained over the simple case of the regular heuristic: a factor of 138in nodes generated and a factor of 92 in time.The last line of the table shows the results when DIDA∗ was used. DIDA∗ with the JIL policy produces a roughly10-fold performance improvement over IDA∗ (from 2,478 million to 260 million nodes) when using the same heuris-tic. In this domain there are no obvious redundant operator sequences, so a depth-first search cannot prune any of theoperators based on the previous operators. Only the trivial pruning of the parent is possible, making the branchingfactor below the root N − 2. When performing the first jump to the dual side, on any particular branch, the branchingfactor increases by only one, from N − 2 to N − 1.Note that, while using the exact same PDB, the total improvement of DIDA∗ over the simple case is by three ordersof magnitude and the time to solve a problem was reduced from more than three days to only six minutes.Table 4 compare results averaged over 100 random instances of the pancake puzzle for sizes 11 to 15. For the lastlines of the 16-pancake and the 17-pancake problems (which demand days of computations) only results over 50 and10 instances were compared respectively. The heuristic used was a PDB based on the seven rightmost tokens. The firstcolumn indicates the size of the pancake puzzle. The second column indicates the average optimal solution cost foreach set of random instances. The following columns presents the average number of generated nodes using differentheuristics and different search methods. The table shows that the larger the problem space (i.e., the bigger the IDA∗search needed), the larger the improvement for the various methods of using duality.Fig. 15 shows the improvement factor of the different variations over the basic regular lookup (column 3 of Table 4).The figure shows that the improvement factor of DIDA∗ steadily increases with the size of the problem. For theproblem of size 11, it is a factor of 59. An improvement of 1314-fold is seen for a problems of size 17. For the smallerTable 317-pancake puzzle results over 10 random instancesHeuristicrdmax(r, d)max(r, d)Algorithm∗∗∗IDAIDAIDA∗DIDA(JIL)Nodes342,308,368,71714,387,002,1212,478,269,076260,506,693Time284,05412,4853086362Table 4Pancake puzzle results for different sizes of problemsSize11121314151617Ave.9.8310.5011.8812.6713.7814.7215.60IDA∗ h = r16,407148,3804,268,70066,213,088864,968,14018,184,871,249342,308,368,717IDA∗ h = d867641498,6052,143,32838,953,014608,590,92814,387,002,121IDA∗ h = max(r, d)404253829,423474,0826,259,06195,124,4952,478,269,076∗DIDA(JIL) h = max(r, d)275159715,291229,3482,306,74518,469,496260,506,6939 Since the dual heuristic is inconsistent, BPMX was also used. Again, part of the 23.8-fold improvement is due to activating BPMX.532U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540Fig. 15. Improvement on IDA∗with regular lookup.Fig. 16. General duality.problems, the regular lookup in the 7-token PDB is more accurate and provides tighter bounds on the solution, limitingthe opportunities for large performance improvements. When the problem is large the PDB is less accurate, enablingother search and heuristic methods to find large performance improvements. Note that the other variations are alsoalways better than the simple version (regular lookup only) but the improvement of these systems does not seem toincrease as dramatically as DIDA∗.8. General dualityThe simple definition of duality used so far assumes that any operator sequence that can be applied to any givenstate S can also be applied to the goal G. This only applies to search spaces where operators have no preconditions(assumption 4 in Section 4.1). In the sliding-tile puzzles, for example, operators have preconditions (the blank mustbe adjacent to the tile that moves) and an operator sequence that applies to S will not be applicable to G if the blank isin different locations in S and G. A more general definition of duality, allowing preconditions on operators, will nowbe given. Assumption 4 is dropped but assumptions 1–3 are still needed, although assumption 1 is relaxed to allown, the number of locations, to be greater than m, the number of objects. With this general definition, dual heuristicevaluations and dual search are possible for a much wider range of state spaces, including the sliding-tile puzzles, theBlocks World, and the Towers of Hanoi.Duality (general definition). The dual of a given state, S, for goal state G, can be defined with respect to any state Xsuch that any sequence of operators that can be applied to state S can also be applied to state X and vice versa. If π isthe location-based permutation such that π(S) = G, then SdX, the dual of S with respect to X, is defined to be π(X).This idea is illustrated in Fig. 16. The same path that transforms S to G also transforms X to SdX. As a special case, ifU. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540533Fig. 17. General duality SdX= π(β(S)) = β(π(S)).let o be the object located in p in Slet y be the location of o in Gdefine πp = yDual(State S, Goal G, State X)for each location p {11.11.21.31.423} endforSd= π(X)Xreturn (SdX )Algorithm 5. Calculation of the general dual state of S with respect to X.X = G (this is possible if any operator sequence applicable to S is also applicable to G) then this definition becomesthe simple definition given earlier.The 8-puzzle state S and the goal state G of Fig. 17 do not have the same applicable operators. For example, theoperator “move up the tile in the middle” is applicable to S but not to G. A state X needs to be found such that alloperator sequences applicable to S will be applicable to X. This is done with the mapping β, which renames the tilesto transform S into X. For the given S this X could be any state having the blank in the same position as S. SdX can bederived in two ways, either by applying π to X (as shown in Algorithm 5) or by renaming the tiles in G according toβ. π (shown in Fig. 17), for example, maps the tile in the upper left location in S, or in X, to the lower left location inG, or SdX, respectively. By contrast, β renames object 6 in S, or in G, to object 1 in X, or SdX, respectively.By definition, any legal sequence of operators that produces SdX when applied to X can be legally applied to S toproduce G, and vice versa. Because an operator and its inverse cost the same, duality provides an alternative way toestimate the distance from S to G: any admissible estimate of the distance from SdX to X is also an admissible estimateof the distance from S to G. If PDBs are being used, general duality suggests using a PDB, PDBX (with X as thegoal state), in addition to the usual PDB, PDBG (with G as the goal). Given a state S, in addition to the standardheuristic value, PDBG[S], a heuristic value for the dual state can be used by computing π for S and then looking upPDBX[π(X)].It is possible to have multiple states, {Xi}, each playing the role of X in the definition. In this case, a state S couldhave more than one dual—it will have a dual with respect to each Xi that has the all-important property that anysequence of operators applicable to S is also applicable to Xi and vice versa. A PDB, PDBXi would be built for eachXi (with Xi as the goal). Lookups for the dual state of S could be made in PDBXi for each Xi for which a dual of Sis defined.For the sliding-tile puzzles, we define Xi to be a state in which the blank is in position i, and build a PDB foreach Xi . Then, given a state S with the blank in position i, the dual of S with respect to Xi is calculated and its valueis looked-up in PDBXi . For example, in the 8-puzzle there are nine different locations that the blank could occupy.Define nine different states, X0 . . . X8, with Xi having the blank in position i, and compute nine PDBs, one for eachXi . Of course, geometric symmetries can be used to reduce the number of distinct PDBs that must actually be createdand stored. For example, below only four 7-tile PDBs are needed to cover all possible blank locations in the 15-puzzle.534U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–5408.1. Other domainsTo convey the generality of general duality, we will briefly describe how it applies to two additional domains, theBlocks World, and the Towers of Hanoi.In the Blocks World, there are B objects (blocks), each of which may be placed on a “table” or on anotherblock.There may be at most one block on a block, so towers are formed of various heights when blocks are stackedupon each other.10 The size of the state space is approximately p(B) ∗ (B!), where p(B) is the number of ways ofpartitioning the integer B. The standard Block World operators allow any block to be picked up (by a “hand” that thenholds the picked-up block) if it has no block on top of it, and to put the block being held down onto the table or ontoany block that has nothing on top of it. Two states can have the same set of operator sequences applied to them onlyif they have the same “structure”, i.e. they partition B the same way (e.g. into two towers, one of height 3, the otherof size B − 3). In order for each state to have a non-trivial dual, we need one Xi for each different structure – in otherwords we need p(B) different Xi states. This number grows fairly quickly as B increases but it is not excessivelylarge for the values of B typically used in experiments. For example, for B = 12 blocks the number of structures,p(12), is 77. If practical considerations force only some of the 77 Xi PDBs to be computed, duals will exist for thestates that have the same structure as one of Xi ’s for which a PDB was built.The Towers of Hanoi is the same as the Blocks World except for these differences: (1) there are a limited number oflocations on the table (called “pegs”), typically three or four, and each peg has an identity; and (2) each block (calleda “disc”) has a distinct “size”, and a larger disc cannot be placed on top of a smaller one. The latter constraint causesthe number of distinct structures to explode exponentially—two states can have the same set of operator sequencesapplied to them only if they contain exactly the same towers of discs, which means they can differ only in which pegsthe various towers are on. General duality applies in this case, but it is of no benefit.8.2. Dual search for the general caseSuppose dual search is proceeding on the “regular side” (with G as the goal) and decides at state S to jump to Sdi ,the dual of S with respect to Xi . Search now proceeds with Xi playing the role of the goal in all respects. In particular:(1) if Xi is reached, the search is finished and the final solution path can be reconstructed; and (2) the permutation πis calculated using Xi instead of G. The latter point has an important implication for the sliding-tile puzzles: the dualof any state generated when the search goal is Xi will have the blank in location i.9. Experimental results for the sliding-tile puzzles (general duality)General duality has been implemented for the 15-puzzle and 24-puzzle. In this section, results for both using dualheuristics and using the dual search algorithm are given for these domains.9.1. 15-puzzleFor the 15-puzzle, the same 7–8 PDB partitioning from [15] was used (as shown in Fig. 5). As explained inSection 8, for each possible blank location a unique PDB has to be built to be able to perform a lookup for the dualstate and calculate the dual heuristic. However, the number of unique PDBs that must be built can be reduced. Giventhe location of the blank, then a horizontal line (or a symmetric vertical line) across the middle of the puzzle dividesit into two regions of eight locations. One region (call it A) has 8 locations which are occupied by eight real tiles, andanother region of 8 locations (B) which are occupied by seven real tiles and the blank. The 8-tile group is not affectedby the blank since it has exactly 8 location with exactly eight tiles in it and therefore, the regular 8-tile PDB can beused for the dual state.This is not the case for the 7-tile PDB which is affected by the location of the blank. However, as shown in Fig. 18there are only four different unique blank locations for the 7–8 partitioning. A unique 7-tile PDB should be built for10 Note that there could be B different stacks, each with up to B objects. A location is any possible location in any of these stacks. Of course,many of the locations are empty. But, notionally, the number of distinct locations is O(B2).U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540535Fig. 18. Four different (dual) 7-tile pattern databases.Table 5Results for the 15-puzzle#12345678HeuristicAlgorithmJumpAv. HNodesrdmax(r, r∗)max(r, d)max(r, d)max(r, d)max(r, r∗, d, d∗)max(r, r∗, d, d∗)∗∗IDAIDA∗∗IDAIDADIDADIDA∗∗∗IDADIDA∗One PDB lookup––Two PDB lookups--JILJ15Four PDB lookups-J1544.7544.3945.6344.4044.4044.4046.1246.12136,289247,29936,71065,34951,63336,93718,60113,687Time0.0810.1390.0340.0690.0660.0470.0220.019each of these cases. Any state S of the 15-puzzle can be mapped into one of these cases in order to calculate therelevant PDB for its dual state Sd . The frame on the left of Fig. 18 indicates the relevant PDB for the dual lookup ofeach possible blank location. In those locations where two PDBs are given, then the right label indicates the PDB touse for a horizontal partition while the left corresponds to a vertical partition.The amount of memory needed is 519 KB for the 8-tile PDB and 57.5 KB for a 7-tile PDB. Thus the total memoryneeds (the 8-tile and 4 7-tile PDBs) is 749 KB. The three extra PDBs needed to handle all the dual cases correctly,represent a small increase of memory.Table 5 presents results of the different heuristics averaged over the same 1000 instances used in [15]. The averagesolution for this set of instances is 52.52. The first column indicates the heuristic used, with ‘r ∗’ and ‘d ∗’ representingthe reflected regular and dual PDB lookups.Line 1 presents the results when only the regular PDB is used, while line 2 presents the results when only the dualheuristic is used. An interesting phenomenon is that unlike the other domains, in the 15-puzzle the pure dual PDBlookup was worse than the pure regular lookup (it generated almost twice as many nodes). The reason for this is thelocation of the blank. Note that while the regular PDB lookup always consults the 8-tile PDB and the 7-tile PDBlabeled a in Fig. 18, the dual state might also consult one of the other 7-tile PDBs (labeled b, c and d). The currentstate S always aims for a region B configuration such that the blank is located in a corner (the goal state) while thedual state Sd needs to consider other possibilities for region B. It turns out that getting the blank to the corner is aharder task and needs more moves. While the average value over all the entries of the PDB labeled a in Fig. 18 is20.91, the average values of the PDBs labeled b, c and d are 20.81, 20.31 and 20.53 respectively. Thus, the valuesobtained by the PDBs that correspond to b, c and d will be smaller than those obtained by the PDB of a.Historically, the goal location of the blank is in the corner. However, if a goal state is set such that the blank is inlocation 4 (as in Fig. 18(c)) then the regular heuristic will always look in the weakest PDB while the dual heuristic willconsult the other PDBs as well. Such experiments have been made and, indeed, the pure dual PDB lookup generatednearly 40% regular PDB.536U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540Given one PDB lookup, one either performs a lookup on the regular or the dual PDB state. When two lookupsare allowed many other combinations are possible. Line 3 of Table 5 used the maximum of the regular and reflectedPDBs. Note that lines 1 and 3 are the same results obtained by [15], but on different (faster) hardware. Line 3 presentsthe best published results for this puzzle [15]. Line 4 uses the maximum of the regular and the dual heuristic. For thesame reason as line 2, line 4 was worse than line 3 since it used all four PDBs and not just the best one.DIDA∗ experiments included two jumping policies: JIL and J15. J15 works as follows. The sliding-tile puzzlehas two important attributes that did not arise in the previous domains, but should be taken into account in DIDA∗’sjumping policy. First, the branching factor is not uniform. It varies from two to four depending on the location ofthe blank, and will often be different for S and Sd . Second, as explained above, there will be several different PDBs,each based on an Xi having the blank in a different position. The Xi are chosen to maximally exploit the geometricalsymmetries of the puzzle, so that although there are 16 positions the blank could be in, only four PDBs are needed.The average heuristic value for each of these PDBs is different. Note that a small difference in average PDB valuecan have a dramatic effect on the PDB’s pruning power. Because S and Sdi will often have the blank in a differentlocation, and therefore draw their heuristic values from different PDBs, it is important for the jumping policy to takethe average value of the PDBs into account.J15, considers both these attributes. It is a three-part decision process. First, the effective branching factor of theregular and dual states is compared. This is done by considering the blank location and the history of the previousmoves, choosing to prefer the state with the smaller effective branching factor.11 Second, if there is a tie, then thequality (average value) of the relevant PDB is considered. Preference is given to the PDB with the higher average. Theaverage values of the four PDBs were given above. Third, if there is still a tie, then the JIL policy is used.The results for DIDA∗ with JIL and with J15 are presented in lines 5 and 6. DIDA∗ with J15 is almost twiceas efficient as IDA∗ using max(r, d) (line 4). However, as explained earlier the dual heuristic was inferior in thisparticular domain because the regular heuristic used a PDB with higher average values. Thus, max(r, d) was almosttwo times slower than the benchmark results from [15] (line 3). Using DIDA∗ with J15 can overcome this problem.DIDA∗ with J15 (line 6) generated roughly the same number of nodes as the benchmark results when using only twoPDB lookups.Finally, the lines 7 and 8 perform all four possible PDB lookups on this domain. This is achieved using bothregular and dual lookups and their reflections about the main diagonal. Line 7 presents the maximum over the fourPDB combinations. Using all four lookups reduces the number of generated nodes by more than a factor of twoand eliminated one third of the execution time compared to the best results of [15] (line 3 of Table 5). The timeimprovement is smaller because in the new setting four PDB lookups are performed, as opposed to only two PDBlookups for the previous benchmark.To the best of our knowledge using the four regular/dual normal/reflected PDB lookups gives the best existingheuristic for this puzzle.It is important to note that the dual lookups for the sliding-tile puzzles are of great importance as there is onlyone geometrical symmetry available for the state-of-the-art additive heuristic—the reflection about the main diagonal.Thus, the dual idea doubles the number of possible lookups and achieved a speedup of a factor of two over the previousbenchmarks.When performing all four possible lookups with DIDA∗ and J15, the result is a new state-of-the-art solver. Thenumber of nodes is now reduced to only 13,687 and the average time per problem is now 0.019. Of historical noteis that the number of generated nodes is now nearly 30,000 times smaller than when IDA∗ first solved the 15-puzzleusing only Manhattan distance [12].Note from the table that the constant time per node is not significantly increased when moving from IDA∗ toDIDA∗. The reason is that the most time consuming stage of these algorithms is the overhead of the PDB lookups11 The reliance of J15 on the branching factor causes a subtle problem. Suppose the start state, S, has the blank in location 5. It will have abranching factor of four but its dual, Sd5 , calculated with respect to X5, will have a branching factor of 2, because, it will have the blank in the samelocation as goal state G (the upper lefthand corner). Dual search with J15 will therefore jump to Sd5 and proceed searching from there with X5 asthe goal. The states generated during this search will have branching factors of at most 3, but their duals will all have a branching factor of 4. Theywill have the blank in the same location as the current search goal, X5, but without having any history on the other side (because the jump wasmade at the root state). J15 will therefore never make another jump. To avoid this problem, jumps from states with the blank at interior locationsthat are within a few moves of the start state are not permitted.U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540537which slow down the calculations because they perform queries into main memory. This overhead depends on thenumber of PDB lookups and is similar in both IDA∗ and DIDA∗. The only additional overhead in DIDA∗ is activatingthe jumping policy which is relatively very small.9.2. 24-puzzleSimilar experiments were performed using the 24-puzzle. The original 6–6–6–6 partitioning from [15] (Fig. 5)needed storage for only two 6-tile PDBs since all the 3 × 2 rectangles are symmetric. As before, additional PDBs areneeded to handle the blank. Eight 6-tile PDBs are used: one for all the 3 × 2 rectangles and their duals, and seven 6-tilePDBs for the irregular shape in the top left corner (see Fig. 5). They are numbered 0, 1, 5, 6, 10, 11, 12 in Fig. 19, withthe number reflecting the location of the blank in the Xi that defined the PDB. Fig. 19(a) indicates which PDB is to beused for each possible position of the blank, and Fig. 19(b) shows two different 6–6–6–6 additive PDB partitionings,PDB12 and PDB0 (the PDB used in [15]). Fig. 19(c) shows the average heuristic value for each of the PDBs. Each6-tile PDB needs 122 MB and the new system needs eight times as much memory. When using DIDA∗, the JIL andJ24 heuristics were used (J24 works exactly the same way as J15 but for the 24-puzzle).In [15] 50 random instances were optimally solved. This data set was sorted in increasing order of their optimal so-lutions. Table 6 presents the average results over the first 25 random instances for all the different variations. Table A.3in Appendix A gives further results for the entire set of 50 instance for the best variations of DIDA∗. The first linepresents the benchmark results from [15] where the maximum between the regular PDB (r) and its reflection aboutthe main diagonal (r ∗) were taken. The second line is IDA∗ with regular and dual PDB lookups. Line 3, is DIDA∗with JIL. Finally, line 4 shows that DIDA∗ with J24 outperforms the benchmark results by a factor of 5.3. Detailedresults for each of the 50 instances from [15] with variations on the PDB lookups is provided in Appendix A.The last two lines (5 and 6) present the case where all possible four PDB lookups were used. IDA∗ with all fourlookups improved the benchmark results by a factor of 3.2. DIDA∗ with all four lookups further improved this to a totalof improvement factor of 11.0 over the benchmark. Furthermore, note that DIDA with two lookups (r, d) outperformIDA∗ with the entire set of four lookups by a factor of 1.65.Fig. 19. 24-puzzle heuristic.Table 6∗DIDAresults on the 24-puzzle on the first 25 random instances#123456Heuristicmax(r, r∗)max(r, d)max(r, d)max(r, d)max(r, r∗, d, d∗)max(r, r∗, d, d∗)Search∗∗IDAIDADIDADIDA∗∗∗IDADIDA∗PolicyNodesJumpsTwo PDB lookups––JILJ24Four PDB lookups–J2443,454,810,04531,103,112,89516,302,942,6808,248,769,71313,549,943,8683,948,614,947––176,075,34323,851,828–13,083,286538U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540The versions with two PDB lookups ran at around 300,000 nodes per second while the versions with 4 PDB lookupsran at around 220,000 nodes per second. Thus, as observed in the other domains the improvement in the total runningtime was a little smaller.10. Conclusions and future workDuality is a new form of symmetry between states in permutation state spaces and it allows the usage of multipleheuristics for a given state. DIDA∗ is a novel search algorithm which exploits this symmetry. DIDA∗ can switchbetween state representations to maximize the overall quality of the heuristic values seen in the search. The algorithmhas several surprising properties, including no need for a search frontier data structure and solution path constructionfrom disparate regions of the search space. Using the dual heuristic significantly improves the heuristic value. Addingthe dual search algorithm provides additional performance gains (up to an order of magnitude) in several applicationdomains using a state-of-the-art heuristic search algorithm.Future work can continue in the following directions:Table A.1DIDA∗results on the 24-puzzle on the 50 random instances#135Heuristicmax(r, r∗)max(r, d)max(r, r∗, d, d∗)Search∗IDADIDA∗∗DIDAPolicyNodesJumpsTwo PDB lookups–J24360,892,479,67075,201,250,617–147,733,547Four PDB lookupsJ2437,674,826,64978,134,424Table A.224-puzzle first 25 instances. DIDA∗uses the J24 jumping policyNo1 (25)2 (40)3 (29)4 (36)5 (20)6 (30)7 (47)8 (44)9 (1)10 (22)11 (2)12 (16)13 (38)14 (3)15 (32)16 (4)17 (28)18 (35)19 (27)20 (31)21 (5)22 (37)23 (46)24 (49)25 (6)AverageSol818288909292929395959696969797989898999910010010010010195)Benchmark (r, r∗292,174,44465,099,5784,787,505,6372,582,008,940312,016,177,6841,634,941,42030,443,173,162867,106,2382,031,102,6353,592,980,531211,884,984,5253,803,445,93438,173,50721,148,144,928428,222,50710,991,471,9662,258,006,870116,131,234,74353,444,360,03326,200,330,6862,899,007,6251,496,759,94465,675,717,510108,197,305,702103,460,814,368∗IDA(r, d)547,754,44678,265,28929,093,280,8763,128,723,82450,287,497,9841,950,647,38918,915,533,169373,833,955815,220,8744,006,328,755149,827,435,3251,829,307,20445,976,05549,550,582,5471,707,750,97414,523,612,6512,106,454,88674,794,747,60434,150,080,39119,627,677,4143,785,640,3111,471,627,38257,066,411,68756,056,805,705201,836,625,690∗DIDA-J24 (r, d)DIDA∗-J24 (r, r∗, d, d∗)152,941,19013,720,4242,811,214,6231,209,506,40229,036,511,6492,484,991,6416,421,296,55537,432,750114,270,7401,762,446,93522,818,488,960174,895,0128,435,47129,121,290,6621,685,362,6222,480,394,914543,149,0596,553,916,2433,447,475,09520,768,799,210484,549,8761,563,432,72632,733,564,0729,106,414,74430,684,741,23886,623,7388,027,1341,052,360,568569,488,3564,464,625,8731,225,111,0004,151,834,90027,828,31060,208,978814,538,59114,893,883,061125,947,8565,298,2599,394,905,290623,772,0781,500,144,838360,755,0983,763,906,8551,999,173,10914,456,575,816178,355,2441,014,271,80818,191,427,1815,362,475,53714,383,834,20343,454,810,04531,103,112,8958,248,769,7133,948,614,947U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540539• Obtaining a better understanding of the jumping policies. Given an application, how does one go about determin-ing the best policy?• Analysis to see if the duality concept can be generalized from permutation state spaces to encompass a wider setof application domains and perhaps other forms of permutation problems or even general search problems.• Integrating the idea of duality into other search algorithms (e.g., A∗ [8], RBFS [13], breadth-first heuristicsearch [23]). Initial results with Dual-A∗ on the 15-pancake puzzle reduce search time by roughly 20%.AcknowledgementsThis research was supported by the Israel Science Foundation (ISF) under grant number 728/06 to Ariel Felner,by the Natural Sciences and Engineering Research Council of Canada (NSERC), and Alberta’s Informatics Circle ofResearch Excellence (iCORE).Appendix A. Experimental results of the 24-puzzleIn this section, results for the entire set 50 random instances of the 24-puzzle are given. They are compared to theresults in [15]—IDA∗ with max(r, r ∗)—and are referred to as the “Benchmark” in the following tables. Table A.1summarizes the results on the entire set of 50 instances. The best version outperforms the benchmark by an order ofmagnitude.The 50 instances have been sorted by increasing order of length of the optimal solution. In Tables A.2 and A.3 theinstances are given according to this order. The number in the parenthesizes is the instance number given in [15]. TheSol column gives the length of the optimal solution path. The next three columns provide the number of generatednodes for the four different algorithms. The Benchmark column corresponds to the r + r ∗ system from [15]. The nextcolumn, IDA∗(r, d), uses IDA∗ but takes the maximum between the regular and dual heuristic. Finally the last twocolumns present results obtained by DIDA∗ with the J24 jumping policy and using two and four PDB lookups. For allTable A.324-puzzle the rest of the 50 instancesNo26 (13)27 (45)28 (34)29 (15)30 (21)31 (7)32 (23)33 (39)34 (43)35 (26)36 (11)37 (19)38 (33)39 (41)40 (24)41 (48)42 (8)43 (42)44 (12)45 (17)46 (18)47 (14)48 (9)49 (50)50 (10)AverageSol101101102103103104104104104105106106106106107107108108109109110111113113114107)Benchmark (r + r∗1,959,833,48779,148,491,306481,039,271,661173,999,717,809724,024,589,335106,321,592,792171,498,441,076161,211,472,63355,147,320,20412,397,787,3911,654,042,891,186218,284,544,2331,062,250,612,55826,998,190,480357,290,691,483555,085,543,507116,202,273,788245,852,754,920624,413,663,951367,150,048,758987,725,030,4331,283,051,362,3851,818,005,616,6064,156,099,168,5061,519,052,821,943678,330,149,297∗DIDA-J24 (r + d)DIDA∗-J24 (r + r∗ + d + d∗)2,196,890,32716,455,892,50759,384,485,258104,581,763,68047,574,279,914112,115,069,81625,019,468,32534,094,740,37719,521,199,9954,710,801,259223,800,028,89671,328,672,853697,848,426,0651,831,465,73058,794,690,620102,741,654,32646,087,884,50670,605,794,60933,355,872,84264,989,490,579560,055,473,298531,467,600,97836,575,158,063285,616,821,863343,089,661,403142,153,731,5241,525,086,3368,903,606,54533,346,319,76148,205,749,58423,105,133,31541,489,057,09615,308,110,75221,449,225,37712,031,249,9382,293,128,38081,918,451,41729,200,386,532410,610,357,344866,811,66122,991,124,35945,159,149,71532,266,488,30232,982,573,37814,264,946,73533,540,174,776314,071,218,585220,384,669,29621,812,286,743180,090,018,836137,210,634,02871,401,038,351540U. Zahavi et al. / Artificial Intelligence 172 (2008) 514–540instances one can see the improved performance of the new methods. Table A.3 further gives results for DIDA∗ withJ24 and two and four PDB lookups for the rest of the 50 cases.References[1] J.C. Culberson, J. Schaeffer, Pattern databases, Computational Intelligence 14 (3) (1998) 318–334.[2] H. Dweighter, Problem e2569, American Mathematical Monthly 82 (1975) 1010.[3] S. Edelkamp, Planning with pattern databases, in: Proceedings of the 6th European Conference on Planning (ECP-01), 2001, pp. 13–34.[4] A. Felner, Solving the graph-partitioning problem with heuristic search, Annals of Mathematics and Artificial Intelligence 67 (2006) 19–39.[5] A. Felner, R.E. Korf, S. Hanan, Additive pattern database heuristics, Journal of Artificial Intelligence Research (JAIR) 22 (2004) 279–318.[6] A. Felner, R. Meshulam, R.C. Holte, R.E. Korf, Compressing pattern databases, in: Proceedings of the National Conference on ArtificialIntelligence (AAAI-04), July 2004, pp. 638–643.[7] A. Felner, U. Zahavi, R.C. Holte, J. Schaeffer, Dual lookups in pattern databases, in: Proceedings of the International Joint Conference onArtificial Intelligence (IJCAI-05), 2005, pp. 103–108.[8] P.E. Hart, N.J. Nilsson, B. Raphael, A formal basis for the heuristic determination of minimum cost paths, IEEE Transactions on SystemsScience and Cybernetics SCC-4 (2) (1968) 100–107.[9] B. Hnich, B. Smith, T. Walsh, Dual modelling of permutation and injection problems, Journal of Artificial Intelligence Research 21 (2004)357–391.[10] R.C. Holte, J. Newton, A. Felner, R. Meshulam, D. Furcy, Multiple pattern databases, in: ICAPS, 2004, pp. 122–131.[11] R.C. Holte, M.B. Perez, R.M. Zimmer, A.J. MacDonald, Hierarchical A*: Searching abstraction hierarchies efficiently, in: Proceedings of theNational Conference on Artificial Intelligence (AAAI-96), 1996, pp. 530–535.[12] R.E. Korf, Depth-first iterative-deepening: An optimal admissible tree search, Artificial Intelligence 27 (1985) 97–109.[13] R.E. Korf, Linear-space best-first search, Artificial Intelligence 62 (1) (1993) 41–78.[14] R.E. Korf, Finding optimal solutions to Rubik’s Cube using pattern databases, in: Proceedings of the National Conference on ArtificialIntelligence (AAAI-97), 1997, pp. 700–705.[15] R.E. Korf, A. Felner, Disjoint pattern database heuristics, Artificial Intelligence 134 (2002) 9–22.[16] R.E. Korf, M. Reid, S. Edelkamp, Time complexity of Iterative-Deepening-A*, Artificial Intelligence 129 (1–2) (2001) 199–218.[17] R.E. Korf, L. Taylor, Finding optimal solutions to the twenty-four puzzle, in: Proceedings of the National Conference on Artificial Intelligence(AAAI-96), 1996, pp. 1202–1207.[18] M. McNaughton, P. Lu, J. Schaeffer, D. Szafron, Memory efficient A* heuristics for multiple sequence alignment, in: Proceedings of theNational Conference on Artificial Intelligence (AAAI-02), 2002, pp. 737–743.[19] D. Ratner, M. Warmuth, Finding a shortest solution for the n × n extension of the 15-puzzle is intractable, in: Proceedings of the NationalConference on Artificial Intelligence (AAAI-86), Philadelphia, PA, 1986, pp. 168–172.[20] U. Zahavi, A. Felner, R.C. Holte, J. Schaeffer, Dual search in permutation state spaces, in: Proceedings of the National Conference on ArtificialIntelligence (AAAI-06), 2006, pp. 1076–1081.[21] U. Zahavi, A. Felner, J. Schaeffer, N. Sturtevant, Inconsistent heuristics, in: Proceedings of the National Conference on Artificial Intelligence(AAAI-07), 2007, pp. 1211–1216.[22] R. Zhou, E. Hansen, Space-efficient memory-based heuristics, in: Proceedings of the National Conference on Artificial Intelligence (AAAI-07), 2004, pp. 677–682.[23] R. Zhou, E. Hansen, Breadth-first heuristic search, Artificial Intelligence 170 (4–5) (2006) 385–408.