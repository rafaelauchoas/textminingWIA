2202nuJ32]VC.sc[2v91970.7012:viXraA survey on bias in visual datasetsSimone Fabbrizzi∗1,3, Symeon Papadopoulos1, Eirini Ntoutsi2, and IoannisKompatsiaris11CERTH-ITI, Thessaloniki, Greece2Freie Universit¨at, Berlin, Germany3Leibniz Universit¨at, Hannover, GermanyJune 24, 2022Abstract1IntroductionComputer Vision (CV) has achieved remarkableresults, outperforming humans in several tasks.Nonetheless, it may result in significant discrim-ination if not handled properly as CV systemshighly depend on the data they are fed with andcan learn and amplify biases within such data.Thus, the problems of understanding and dis-covering biases are of utmost importance. Yet,there is no comprehensive survey on bias in vi-i) de-sual datasets. Hence, this work aims to:scribe the biases that might manifest in visualdatasets;ii) review the literature on methodsfor bias discovery and quantification in visualdatasets; iii) discuss existing attempts to collectbias-aware visual datasets. A key conclusion ofour study is that the problem of bias discoveryand quantification in visual datasets is still open,and there is room for improvement in terms ofboth methods and the range of biases that canbe addressed. Moreover, there is no such thingas a bias-free dataset, so scientists and practi-tioners must become aware of the biases in theirdatasets and make them explicit. To this end,we propose a checklist to spot different types ofbias during visual dataset collection.∗Corresponding author: simone.fabbrizzi@iti.grThis work is currently under review at Computer Vi-sion and Image Understanding.In the fields of Artificial Intelligence (AI), al-gorithmic fairness, and (big) data ethics, theterm bias usually refers to the case in whichAI-powered decisions show prejudice against in-dividuals or groups of people defined based onprotected attributes like gender or race [Ntoutsiet al., 2020].Instances of this prejudice havecaused discrimination in many fields, includingrecidivism scoring [Angwin et al., 2016], onlineadvertisement [Sweeney, 2013], facial recognition[Cook et al., 2019], and credit scoring [Bartlettet al., 2019].Defining the concepts of bias and fairness inmathematical terms is not a trivial task. Vermaand Rubin [2018] provided a survey on morethan 20 different measures of algorithmic fair-ness, many of which are incompatible with eachother. This incompatibility - the so-called im-possibility theorem [Chouldechova, 2017, Klein-berg et al., 2017] - forces scientists and practi-tioners to choose the measures they use based ontheir personal beliefs or other constraints (e.g.,business models) on what has to be consideredfair for the particular problem/domain.While algorithms may also be responsible forthe amplification of pre-existing biases in thetraining data [Bolukbasi et al., 2016], the qual-ity of the data itself contributes significantly to1   the development of discriminatory AI applica-tions. Ntoutsi et al. [2020] identified two ways inwhich bias is encoded in the data: correlationsand causal influences among the protected at-tributes and other features; and the lack of repre-sentation of protected groups in the data. Theyalso noted that biases manifest in ways that arespecific to the data type.In this work, we focused on how biases canbe encoded in the data (e.g., via spurious cor-relations, causal relationship among the vari-ables, and unrepresentative data samples) and,in particular,images andin visual data (i.e.,videos), which comprises one of the most popu-lar and complex data types. Visual data encap-sulates many features that require human expe-rience and context to interpret. These includethe human subjects, how they are depicted andtheir reciprocal position in the image frame, im-plicit references to culture-specific notions andbackground knowledge, etc. Even the colour-ing scheme can convey different messages. Thus,making sense of visual content remains a verycomplex task, and understanding bias in visualdata is even harder.Computer vision (CV), the primary domainthat enables computers to gain high-level under-standing from visual data, is heavily dominatednowadays by deep learning (DL) methods [Le-Cun et al., 2015, Baraniuk et al., 2020] that al-lowed for outstanding performance in tasks likeobject detection, image classification and imagesegmentation. DL methods, however, rely heav-ily on data, and the results are as good and“fair” as the data used for their training. CV hasrecently drawn attention for its ethical implica-tions when deployed in several settings, rangingfrom targeted advertising to law enforcement.There has been mounting evidence that deploy-ing CV systems without a comprehensive ethi-cal assessment may result in major discrimina-tion against protected groups. For instance, fa-cial recognition technologies [Cook et al., 2019,Robinson et al., 2020], gender classification algo-rithms [Buolamwini and Gebru, 2018], and au-tonomous driving systems [Wilson et al., 2019]have been all shown to exhibit discriminatorybehaviour.While bias in AI systems is a well-studied field,the research in biased CV is more limited despitethe abundance of visual data produced nowa-days and their widespread use in the ML com-munity. Moreover, to the best of our knowledge,there is no comprehensive survey on bias in vi-sual datasets ([Torralba and Efros, 2011] repre-sents a seminal work in the field, but it is limitedto object detection datasets). Hence, the contri-i) to explorebutions of the present work are:and discuss the different types of bias that arisefrom the collection of visual data; ii) to system-atically review the works that aim at address-ing and measuring bias in visual datasets; iii)to discuss some attempts to compile bias-awaredatasets. We believe this work to be a useful toolfor helping scientists and practitioners to bothdevelop new bias-discovery methods and collectdata in ways as less biased as possible. To thelatter end, we propose a checklist that can beused to spot the different types of bias that mightenter the data during the collection process (Ta-ble 6).The structure of the survey is as follows. First,we describe in detail the different types of biasthat might affect visual datasets (Section 2), pro-vide concrete examples of CV applications thatare affected by those biases, and a description ofhow they manifest in the life cycle of visual con-tent. Second, we systematically review the meth-ods for bias discovery in visual content proposedin the literature (Section 3). Third, in Section 4,we discuss the weaknesses and strengths of somebias-aware visual benchmark datasets. Finally,in Section 5, we conclude and outline some pos-sible future research direction.2 Manifestation of Bias inVisual DataIn this section, we describe in detail the typesof bias that pertain to the capture and collectionof visual data (Figure 1), namely: selection bias2Figure 1: Examples of selection, framing and label bias. On the right, a list of applications thatcan be affected by each type of bias.3(Section 2.1), framing bias (Section 2.2) and la-bel bias (Section 2.3). Furthermore, we describe(Section 2.4) how they manifest within the lifecycle of visual content, from capture to the de-ployment of CV algorithms. Note that a compre-hensive analysis of historical discrimination andalgorithmic bias is beyond the scope of this work.The interested reader can refer to Bandy [2021]for a survey on methods for auditing algorithmicbias both in CV and other AI-related areas.Our categorisation builds on the scheme byTorralba and Efros [2011] who organised thetypes of visual bias into four different categories:selection bias, capture bias (which we collapseinto the more general concept of framing bias),label bias, and negative set bias. The latter ariseswhen the labelling does not reflect entirely thepopulation of the negative class (say non-whitein a binary feature [white people/non-white peo-ple]). We consider negative class bias as an in-stance of selection and label bias.Even though our categorisation appears on thesurface to be similar to the one by Torralba andEfros [2011], their analysis focused on datasetsfor object detection.Instead, we contextualisebias in a more general setting and we also fo-cus on discrimination against protected groups1.Since selection, framing and label bias manifestin many different ways, we also go further by de-scribing a sub-categorisation of these three typesof bias (Table 1) including several biases com-monly encountered in Statistics, Health studies,or Psychology and adapting them to the contextof visual data. While in the following we de-scribe selection, framing, and label bias in gen-eral terms, we also provide references in Table1 for the interested reader who might want todelve further into their different manifestations.2.1 Selection BiasDefinition Selection bias is the type of biasthat “occurs when individuals or groups in a1Note that, while this is the focus of our survey, wealso take into account cases in which bias does not nec-essarily affect people, e.g., in object detection.study differ systematically from the populationof interest leading to a systematic error in anassociation or outcome”2. More generally,itrefers to any “association created as a result ofthe process by which individuals are selectedinto the analysis” ([Hern´an and Robins, 2020,Chapter 8, pg. 99]). In visual datasets, using thefirst definition would be tricky as, for instance,in the case of facial recognition, respecting theethnic composition of the population is generallynot enough to ensure good performance acrossevery subgroup, as we will see in the following.Hence, we adopt a slight modification of Hern´anand Robins [2020] definition:We call selection bias any disparities or as-sociations created as a result of the processby which subjects are included in a visualdataset.Description Torralba and Efros [2011] showedthat certain kinds of imagery are more likely tobe selected during the collection of large-scaleleading to selection biasbenchmark datasets,(see sampling bias, Table 1). For example, inCaltech101 [Li Fei-Fei et al., 2004] pictures la-belled as cars are usually taken from the side,while ImageNet [Deng et al., 2009] contains moreracing cars. Furthermore, a strong selection biascan also be present within datasets.Indeed,Salakhutdinov et al. [2011] showed that, unlessa great effort is made to keep the distributionuniform during the data collection process, cat-egories in large image datasets follow a long-taildistribution. Having such a distribution meansthat, for example, people and windows are waymore common than ziggurats and coffins [Zhuet al., 2014].Selection bias becomes particularly worrisomewhen it concerns humans. Buolamwini and Ge-bru [2018] pointed out that under-representationof people from different genders and ethnic2Catalogueof Bias Collaboration, Nunan D,Bankhead C, Aronson JK. Selection bias.Cata-logue Of Bias 2017: http://www.catalogofbias.org/biases/selection-bias/. Last visited 18.05.2022.4groups may result in a systematic misclassifi-cation of these groups (their work concentrateson gender classification algorithms). They alsoshowed that some popular datasets were biasedtowards lighter-skinned male subjects. For ex-ample, Adience [Eidinger et al., 2014] (resp.IJB-A [Klare et al., 2015]) have 7.4% (resp.4.4%) of darker-skinned females, 6.4% (resp.16.0%) of darker-skinned males, 44.6% (resp.20.2%) oflighter-skinned females and 41.6%(resp. 59.4%) of lighter-skinned males. Such im-balances greatly affect the performance of CVtools. For instance, Buolamwini and Gebru[2018] showed that the error rate for dark skin in-dividuals could be 18 times higher than for lightskin ones in some commercial gender classifica-tion algorithms.Affected applicationsIn summer 2020, theNew York Times published the story of a BlackAmerican individual wrongfully arrested due toan error made by a facial recognition algorithm[Hill, 2020]. While we do not know whetherbias in the data caused this exact case, we knowthat selection bias can lead to different errorrates in face recognition. Hence, such technol-ogy would require much more care, especially inhigh-impact applications.Autonomous driving systems are also likely af-fected by selection bias as it is very challengingto collect a dataset that describes every possi-ble scene and situation a car might face. TheBerkeley Driving Dataset [Yu et al., 2020] forexample, contains driving scenes from only fourcities in the US; an autonomous car trained onsuch a dataset will likely under-perform in othercities with different visual characteristics. Theeffect of selection bias on autonomous drivingbecomes particularly risky when it affects pedes-trian recognition algorithms. Wilson et al. [2019]studied the impact of under-representation ofdarker-skinned people on the predictive inequityof pedestrian recognition systems. They foundevidence that the effect of this selection bias istwo-fold: first, such imbalances “beget less sta-tistical certainty” making the process of recogni-tion more difficult; second, standard loss func-tions tend to prioritise the more representedgroups and hence some kind of mitigation mea-sures are needed in the training procedure [Wil-son et al., 2019].Moreover, Buolamwini and Gebru [2018] ex-plained that part of the collection of benchmarkface datasets is often done using facial detectionalgorithms. Therefore, every systematic bias inthe training of those tools propagates to otherdatasets. This is a clear example of algorith-mic bias turning into selection bias (see automa-tion bias, Table 1), as described at the end ofSection 2.4. Furthermore, image search enginescan contribute to the creation of selection biases(see availability bias, Table 1) as well due to sys-tematic biases in their retrieval algorithms; see,[Kay et al., 2015] for a study on gender bias inGoogle’s image search engine.Remarks Finally, Klare et al. [2012] pointedout that, while class imbalances have undoubt-edly significant impact on some facial recognitionalgorithms, they do not explain every disparityin the performance of algorithms. For instance,they suggested that a group of subjects mightbe more challenging to recognise, even with bal-anced training data, if it is associated with highervariance (for example, due to hairstyle or make-up). The reader can also refer to Terh¨orst et al.[2021] for an analysis of the effect of these kindsof attributes on facial recognition technology.2.2 Framing BiasDefinition According to the seminal work ofEntman [1993] on framing of (textual) commu-nication, “To frame is to select some aspects ofa perceived reality and make them more salientin a communicating text, in such a way as topromote a particular problem definition, causalinterpretation, moral evaluation and/or treat-ment recommendation for the item described”.Coleman [2010] adopted the same definition forthe framing of visual content and added that“In visual studies, framing refers to the selection5of one view, scene, or angle when making theimage, cropping, editing or selecting it”. Thesetwo definitions highlight how framing bias hastwo different aspects. First, the medium aspect:visual content is, in all respect, a medium, andtherefore the way they are composed conveysdifferent messages. Second, the technical aspect:framing bias also derives from how an image iscaptured or edited. Hence, in the followingWe refer to framing bias as any associationsor disparities that can be used to convey dif-ferent messages and/or that can be tracedback to the way in which the visual contenthas been composed.Note that, while the selection process is a pow-erful tool for framing visual content, we keep theconcepts of selection and framing bias distinct,as they present their own peculiarities.Description An example of visualframingbias (to be more specific, capture bias, Table 1)has been studied by Heuer et al. [2011]. Theyanalysed images attached to obesity-related ar-ticles that appeared on major US online newswebsites in the period 2002-2009. They con-cluded that there was a substantial differencein how such images depicted obese people withrespect to non-overweight ones. For example,59% of obese people were headless (6% of non-overweight), and 52% had only the abdomen por-trayed (0% of non-overweight). This portrayal as“headless stomachs” [Heuer et al., 2011] (see Fig-ure 1 b) for an example) may have a stigmatis-ing and de-humanising effect on the viewer. Onthe contrary, positive characteristics were morecommonly portrayed among non-overweight peo-ple. Corradi [2012], while analysing the use offemale bodies in advertisements, talked about“semiotic mutilation” when parts of a woman’sbody are used for advertising a product with ade-humanising effect, similar to what Heuer etal. have described in their work about obesity.The relationship between bodies and faces inimage framing is a well-known problem. Indeed,Archer et al. [1983] used a ratio between theheight of a subject’s body and the length of theirface to determine whether men’s faces were moreprominent than those of women. They foundout that this was true in three different set-tings: contemporary American news media pho-tographs, contemporary international news me-dia photographs, and portraits and self-portraitsfrom the 17-th century to the 20-th century (in-terestingly, there was no disparity in earlier art-works). Furthermore, they found some evidencethat people tend to draw men with higher facialprominence and thus that this bias does not onlyoccur in mass media or art.Affected applications An application thatcan suffer greatly from framing bias (see stereo-typing, Table 1) is that of image search en-gines. For example, Kay et al. [2015] found outthat while searching for construction workers onGoogle Image Search, women were more likelyto be depicted in an unprofessional or hyper-sexualised way3. However, it is unclear whetherthe retrieval algorithms are responsible for theframing or they just index popular pages asso-ciated with the queries. Nonetheless, the prob-lem remains because the incorrect framing in asearch engine can contribute to the spread of bi-ased messages. We recall, for instance, the caseof the photo of a woman “ostensibly about toremove a man’s head” [Paquette, 2015] that wasretrieved after searching “feminism” on a famousstock photos website.2.3 Label BiasDefinition For supervised learning,labelleddata are required. The quality of the labels4is of paramount importance for learning and3This specific problem seems to be solved, butthe queries female construction worker and maleconstruction worker still return several pictures inwhich the subjects are hyper-sexualised.4Note that by label we mean any tabular informationattached to the image data (object classes, measures, pro-tected attributes, etc.)6comprises a tedious task due to the complex-ity and volume of today’s datasets. Jiang andNachum [2020] define label bias as “the biasthat arises when the labels differ systematicallyfrom the ground truth”. For instance, Terh¨orstet al. [2021] found some evidence that the facerecognition dataset Labelled Faces in the Wild(LFW, see [Huang et al., 2008] for the original re-lease and [Kumar et al., 2009] for the attributes)presents a very low label accuracy against humanannotation.Furthermore, Torralba and Efros[2011]highlight bias as a result of the labelling processitself for reasons like “semantic categories areoften poorly defined, and different annotatorsmay assign different labels to the same typeof object”. Torralba and Efros’ work mainlyfocused on object detection tasks, and henceby different label assignment they refer, e.g.,to “grass” labelled as “lawn” or “picture” as“painting”. Nevertheless, these problems ariseespecially when dealing with human-relatedfeatures such as race or gender.We define label bias as any errors in the la-belling of visual data, with respect to someground truth, or the use of poorly defined orinappropriate semantic categories.Description As already mentioned, a signifi-cant source of labelling bias is the poor defini-tion of semantic categories. Race is a particu-larly clear example of this: according to Barbu-jani and Colonna [2011] “The obvious biologicaldifferences among humans allow one to make ed-ucated guesses about an unknown person’s an-cestry, but agreeing on a catalogue of humanraces has so far proved impossible”. Given suchan impossibility, racial categorisation in visualdatasets must come at best from subjects’ ownrace perception or, even worse, from the stereo-typical bias of annotators. From a CV stand-point then, it would be probably more accurateto use actual visual attributes, if strictly neces-sary, such as Fitzpatrick skin type [Buolamwiniand Gebru, 2018] or skin reflectance [Cook et al.,2019] rather than a fuzzy category such as race.Note that, while skin tone can be a more objec-tive trait, it still does not entirely reflect humandiversity. Similarly, the binary categorisation ofgender has been criticised. The reader can referto [Hanna et al., 2020, Kasirzadeh and Smart,2021] to explore the challenges that the use ofill-defined categories poses to algorithmic fair-ness from both the ontological and operationalpoints of view.Moreover, Torralba and Efros [2011] arguedthat different annotators can come up with dif-ferent labels for the same object. While thismainly applies to the labelling of object detec-tion datasets rather than face datasets, wherelabels are usually binary or discrete, it gives usan important input about bias in CV in gen-eral: annotators’ biases and preconceptions arereflected in the datasets.We can view what has been described sofar also as a problem of operationalisation ofwhat Jacobs and Wallach [2021] called unob-servable theoretical constructs (see measurementbias, Table 1). They proposed a framework thatserves as guideline for the mindful use of fuzzysemantic categories and answers the followingquestions on the validity of operationalisations(or measurements) of a construct: “Does the op-erationalization capture all relevant aspects ofthe construct purported to be measured? Dothe measurements look plausible? Do they cor-relate with other measurements of the same con-struct? Or do they vary in ways that suggestthat the operationalization may be inadvertentlycapturing aspects of other constructs? Are themeasurements predictive of measurements of anyrelevant observable properties (and other unob-servable theoretical constructs) thought to be re-lated to the construct, but not incorporated intothe operationalization? Do the measurementssupport known hypotheses about the construct?What are the consequences of using the measure-ments[?]”A concrete case of the use of a fuzzy semanticcategory is provided in [Liang et al., 2018], where7the creation of a face dataset for facial beauty isdescribed. Since beauty and attractiveness areprototypical examples of subjective characteris-tics, it is obvious that attempts of constructingsuch datasets will be filled with the personal pre-conceptions of the participants who labelled theimages (see observer bias, Table 1).2.4 Media Bias Life CycleFigure 2 gives an overview of the life cycle ofvisual content and depicts how different types ofbias can enter at any step of this cycle and canbe amplified in consecutive interactions. Belowwe describe each of these components in moredetail.Affected applications Since deep learningboosted the popularity of CV, a modern formof physiognomy has gained a certain momentum.Recently, several studies appeared claiming to beable to classify images according to the criminalattitude of the subjects5 or sexual orientation6.A commercial tool has also been released to de-tect terrorists and paedophiles. While “doomedto fail” for a series of technical reasons well ex-plained by Bowyer et al. [2020], these applica-tions rely on a precise ideology that Goldenfein[2019] called computational empiricism: an epis-temological paradigm that claims, despite anyscientific evidence, that the true nature of hu-mans can be measured and unveiled by algo-rithms. The reader can also refer to the famousblog post “Physiognomy’s New Clothes”7 for anintroduction to the problem.Remarks Just as selection bias, label bias canlead to a vicious cycle: a classification algorithmtrained on biased labels will most likely reinforcethe original bias when used to label newly col-lected data (see automation bias, Table 1).5See, for instance, the retracted article: Hashemi andhttps://journalofbigdata.springeropen.com/Last visitedHallarticles/10.1186/s40537-019-0282-4.30.03.2022.6For example, Kosinski and Wang https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual. Last visited 30.03.2022.7B. Ag¨ueray Arcas, M. Mitchelland A.Todorov, Physiognomy’s New Clothes, May 2017.https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a. Last visited 30.03.2022.1. Real world The journey of visual con-tent alongside bias starts even before the contentis generated. Inequalities undeniably shape ourworld, and this is reflected in the generation ofdata in general and of visual content in partic-ular. For example, Zhao et al. [2017] found outthat the dataset MS-COCO [Lin et al., 2014], alarge-scale object detection, segmentation, andcaptioning dataset which is used as a benchmarkin CV competitions, was more likely to associatekitchen objects with women. While both imagecapturing and dataset collection come at a laterstage in the life cycle described in Figure 2, it isclear that in this instance, such bias has roots inthe gender division between productive and re-productive/care labour. Nevertheless, as shownin the following paragraphs, each step of the lifecycle of visual content can reproduce or amplifyhistorical discrimination as well as insert new bi-ases.2. Capture The actual life of visual contentstarts with its capture. Here the first types ofbias can be introduced, selection bias and fram-ing bias. While selection bias is usually ob-served in datasets, where entire groups can beunder-represented or non-represented at all, theselection begins with the choices of the pho-tographer/video maker8. Moreover, the way aphoto is composed (arrangement, camera’s set-ting, lighting conditions, etc.) is a powerful wayof conveying different messages and thus a pos-sible source of framing bias. Note that the se-lection of the subjects and the framing of the8We do not necessarily mean professional photogra-phers/video makers.8NameSampling bias∗Negative set bias [Torralba and Efros, 2011]Availability bias†Platform biasVolunteer bias†Crawling biasSpurious correlationExclusion bias∗Chronological bias†Geographical bias [Shankar et al., 2017]Capture bias [Torralba and Efros, 2011]Apprehension bias†Contextual bias [Singh et al., 2020]Stereotyping§DescriptionBias that arises from the sampling of the visual data.It includes class imbalance.(say non-white in aclassWhen a negativewhite/non-white categorisation) is not representa-tive enough.Distortion arising from the use of the most readilyavailable data (e.g., using search engines).Bias that arises as a result of a data collection beingcarried out on a specific digital platform (e.g., Twit-ter, Instagram, etc.).When data is collected in a controlled setting insteadof being collected in-the-wild, volunteers that partici-pate in the data collection procedure may differ fromthe general population.Bias that arises as a result of the crawling algo-rithm/system used to collect images from the Webor with the use of an API (e.g., the keywords used toquery an API, the seed websites used in a crawler).Presence of spurious correlations in the dataset thatfalsely associate a certain group of subjects with anyother features.Bias that arise when the data collection excludespartly or completely a certain group of people.Distortion due to temporal changes in the visualworld the data is supposed to represent.Bias due to the geographic provenance of the visualcontent or of the photographer/video maker (e.g.,brides and grooms depicted only in western clothes).Bias that arise from the way a picture or video iscaptured (e.g., objects always in the centre ,exposure,etc.).Different behaviour of the subjects when they areaware of being photographed/filmed (e.g., smiling).Association between a group of subjects and a specificvisual context (e.g., women and men respectively inhousehold and working contexts)When a group is depicted according to stereotypes(e.g., female nurses vs. male surgeons).Observer bias†Perception bias†Measurement bias [Jacobs and Wallach, 2021] Every distortion generated by the operationalisationof an unobservable theoretical construct (e.g., raceoperationalised as a measure of skin colour).Bias due to the way a annotator records the informa-tion.When data is labelled according to the possibly flawedperception of a annotator (e.g., perceived gender orrace) or when the annotation protocol is not specificenough or is misinterpreted.Bias that arises when the labelling/data selection pro-cess relies excessively on (biased) automated systems.Automation bias§9Label•SelectionFraming•••••••••••••••••••••••••Table 1: A finer-grained categorisation of biases described in Section 2. The bullets representwhich types of bias they are a subcategory of (Note that there are overlaps as different kind of biascan co-occur). The definitions are adapted from: (†) https://catalogofbias.org/biases/; (*)https://en.wikipedia.org/wiki/Sampling_bias; (§) https://en.wikipedia.org/wiki/List_of_cognitive_biases; and [Torralba and Efros, 2011, Shankar et al., 2017, Singh et al., 2020,Jacobs and Wallach, 2021].Figure 2: Simplified illustration of visual content life cycle and associated sources of bias.10Real World1.2. Capture3. Editing4. Dissemination5. Data collection6. AlgorithmsActors/structures involved:SocietyPhotographers/video makersMainstream/social mediaScientists/businessesTypes of Bias:Historical discriminationSelection bias; framing biasFraming biasSelection bias; framing biasSelection bias; label biasAlgorithmic biasVisual Content Life CycleDiscriminationGeneration of newbiased visual dataphoto/video are both active choices of the pho-tographer/video maker. Nevertheless, historicaldiscrimination can turn into selection and fram-ing bias if not actively countered.3. Editing With the advent of digital pho-tography, image editing and post-processing arenow a key step in the content life cycle. Post-processing has become a fundamental skill for ev-ery photographer/video maker, along with skillssuch as camera configuration, lighting, shooting,etc. Since photo editing tools are extremely pow-erful, they could give rise to many ethical is-sues: to what extent and in which contexts is itright to modify the visual content of an image orvideo digitally? What harms could such modifi-cations potentially cause? The discussion aroundthe award-winning Paul Hansen’s photo “GazaBurial” [Krug and Niggemeier, 2013] represents apractical example of how these questions are im-portant to photojournalism. In particular, whatis the trade-off between effective storytelling andadherence to reality? Nonetheless, photo edit-ing does not concern journalism only. Actually,it affects people, and especially women, in sev-eral different contexts, from the fashion industry[Jamil, 2018] to advertising and high-school year-books [Cramer and Levenson, 2021].4. Dissemination The next step in the life ofvisual content is dissemination. Nowadays, im-ages and videos are shared via social and main-stream media in volumes that nobody can possi-bly inspect. For instance, more than 500 hours ofvideos are uploaded to YouTube every minute9.Dissemination of visual content suffers from bothselection and framing bias. The images thatare selected, the medium and channels throughwhich they are disseminated, the caption or textthat are attached to them are all elements thatcould give rise to bias (see [Peng, 2018] for acase study of framing bias in the media coverageof 2016 US presidential election).9https://blog.youtube/press/.Lastvisited23.03.2022.5. Data collection The part of the life cy-cle of visual content that is more relevant toour discussion is the collection of visual datasets.Here we encounter selection bias once again, asthe data collection process can exclude or under-represent certain groups from appearing in thedataset, as well as label bias as great effort isusually expended to collect data along with addi-tional information in the form of annotations. Aswe discussed in Section 2.3, this process is proneto errors, mislabelling, and explicit discrimina-tion (see [Miceli et al., 2020] for an analysis ofpower dynamics in the labelling process of visualdata). Furthermore, benchmark datasets havegained incredible importance in the CV field. Onthe one hand, they allowed significant measur-able progress in the field. On the other hand,they represent only a small portion of the visualworld (see [Prabhu and Birhane, 2020] for a cri-tique of the large-scale collection of visual datain the wild ).6. Algorithms Finally, there is the actualstep of model training. Fairness and accountabil-ity of algorithms is a pressing issue as algorithm-powered systems are used pervasively in appli-cations and services impacting a growing num-ber of citizens [Drozdowski et al., 2020, Bandy,2021]. Important questions arise for the AI andCV communities on how to implement fairnessin algorithms such as: What legal frameworksshould governments put into place? What arethe strategies to mitigate bias or make algo-rithms explainable? Nevertheless, the journey ofvisual content does not end with the training ofmodels. Indeed, there are several ways in whichbiased algorithms can generate vicious feedbackloops as described in the remarks of Section 2.3.Moreover, the recent explosion in popularity ofgenerative models, namely Generative Adversar-ial Networks (GANs, [Goodfellow et al., 2014]),has made the process of media creation very easyand fast (see [Mirsky and Lee, 2021] for a sur-vey on AI-generated visual content). Such AI-generated media can then be reinserted in thecontent life cycle via the Web and present their11own ethical issues (see [Berendt et al., 2021] fora discussion on the socio-technical aspects andambivalence of the Web of humans and AIs).3 BiasandDiscoveryQuantification in VisualDatasetsThis section aims to understand how researchershave tackled the problem of discovery and quan-tification of bias in visual datasets since high-quality visual datasets are a critical ingredienttowards fair and trustworthy CV systems [Huet al., 2020]. To this end, we performed a sys-tematic survey of papers addressing the followingproblem: Given a visual dataset D, is it possibleto discover/quantify what types of bias it exhibits(of those described in Section 2)? In particu-lar, we focus on the methodologies and measuresused in the bias discovery process, of which weare going to outline the pros and cons. Further-more, we will try to define open issues and pos-sible future directions for research in this field.Note that this problem is critically different fromboth the problem of finding out whether an al-gorithm discriminates against a protected groupand the problem of mitigating such bias.In order to systematise this review, we pro-ceed in the following way to collect the relevantmaterial: first, we outlined a set of keywordsto be used in three different scientific databases(DBLP, arXiv and Google Scholar) and we se-lected only the material relevant to our researchquestion following a protocol described in the fol-lowing paragraph; second, we summarised theresults of our review and outlined the pros andcons of different methods in Section 3.5. Our re-view methodology was inspired by the works ofMerli et al. [2018], and Kofod-Petersen [2012].Our protocol also resembles the one described in[Kitchenham, 2004, Section 5.1.1].Given our problem, we identify the followingrelevant keywords: bias, image, dataset, fair-ness. For each of them, we also defined a setof synonyms and antonyms (see Table 3). Notethat we include the words “face” and “facial”among the synonyms of the word image. Thisis mainly motivated by the fact that in the ti-tle and the abstract of the influential work ofBuolamwini and Gebru [2018] there are no oc-currences of the word “image”. Instead, we findmany occurrences of the expression “facial anal-ysis dataset”. Since facial analysis is an impor-tant case study for detecting bias in visual con-tent, it makes sense to include it explicitly in oursearch. The search queries have been composedof all possible combinations of the different syn-onyms (antonyms) of the above keywords (forexample, “image dataset bias”, “visual datasetdiscrimination”, or “image collection fairness”).This process resulted, after manual filtering10,in 17 relevant papers. We expanded the listwith 6 articles by looking at papers citing theretrieved works (via Google Scholar) and their“related work” sections. We also added to thelist the work of Lopez-Paz et al. [2017] on causalsignals in images that was not retrieved by theprotocol described above.In the following, wereview all these 24 papers, dividing them intofour categories according to the strategies theyuse to discover bias:• Reduction to tabular data: these relyon the attributes and labels attached to orextracted from the visual data and try tomeasure bias as if it were a tabular dataset.• Biased image representations:theserely on lower-dimensional representations ofthe data to discover bias.• Cross-dataset bias detection: these as-sess bias by comparing different datasets,trying to discover some sort of “signature”due to the data collection process.• Other methods: Different methods thatcould not fit any of the above categories.10The manual filtering consisted in keeping only thosepapers that describe a method or a measure for discover-ing and quantifying bias in visual datasets. In some cases,we kept also works developing methods for algorithmicbias mitigation but that could be used to discover bias inthe dataset as well (see Balakrishnan et al. [2020]).12Cursive capital letters denotes visual datasetsA model f trained on the dataset DBold letters denotes vectorsAverage Precision-Recall (AP) scoreSymbol DescriptionDfD(·)wAPAPB(fD) AP score of the model fD when tested on the dataset B|| · ||2|D|σ(·)1[·]P(·)P(·|·)H(·)H(·|·)I(·, ·)D(·)ln(·)mean(·)L2-normThe number of elements in the dataset DSigmoid functionIndicator functionProbabilityConditional probabilityShannon entropyConditional Shannon entropyMutual informationSimpson D scoreNatural logarithmArithmetic meanTable 2: Brief summary of the notation used in Section 3.biasimagedatasetSynonymsdiscriminationvisual, face, facialcollectionAntonymsfair, unbiasedTable 3: Set of keywords and relative synonyms and antonyms.133.1 Reduction to Tabular Dataimage captions,Methods in this category transform visual datainto tabular format and leverage the multitudeof bias detection techniques developed for tab-ular datasets [Ntoutsi et al., 2020]. The fea-tures for the tabular description can be ex-tracted either directly from the images (using,for example, some image recognition tools) orindirectly from some accompanying image de-scription/annotation (e.g.,la-bels/hashtags) or both. In the majority of cases,feature extraction relies on automatic processesand is therefore prone to errors and biases. Thus,biases that exist in the original images (selec-tion, framing, or label bias) might be reflectedand even amplified in the tabular representationdue to the bias-prone feature extraction process.Below we review different approaches underthis category that extract protected attributesfrom visual datasets and evaluate whether biasexists. Existing approaches can be broadly cat-egorized into parity-based, information theoreti-cal and others. As already explained, the sourcesof bias in such a case are not limited to bias inthe original images, but bias may also exist dueto the labelling process and the automatic fea-ture extraction. The impact of such additionalsources on the results is typically omitted.Count/demographic parity Dulhanty andWong [2019] proposed a simple method for au-diting ImageNet [Deng et al., 2009] with respectto gender and age biases. They applied a face de-tection algorithm to two different subsets of Ima-geNet, the training set of ILSVRC [Russakovskyet al., 2015] and the person category of ImageNet[Deng et al., 2009]. After that, they applied anage recognition model and a gender recognitionmodel to them. They then computed the datasetdistribution across the age and gender categories,finding out a prevalence of men (58.48%) and aminimal amount of people (1.71%) in the > 60age group. Computing the percentage of menand women across every category allowed themto identify the highest class imbalances in thetwo subsets of ImageNet. For example, in theILSVRC subset, the 89.33% of the images incategory bulletproof vest were labelled as menand the 82.81% of the images in category lip-stick were labelled as women. Therefore, thismethod does not only give information on theselection bias but also on the framing of the pro-tected attributes, given a suitable labelling of thedataset. The authors noted that this method re-lies on the assumption that the gender and agerecognition models involved are not biased. Asthe authors pointed out, such an assumption isviolated by the gender recognition model, andtherefore, analysis cannot be totally reliable.Yang et al. [2020] performed another analy-sis of the person category of ImageNet [Denget al., 2009], trying to address both selectionand label bias. They addressed label bias byasking annotators to find out, first, whetherthe labels could be offensive or sensitive (e.g.,sexual/racial slur), and, second, to point outwhether some of the labels were not referringto visual categories (e.g., it is difficult to un-derstand whether an image depicts a philan-thropist). They removed such categories andcontinued their analysis by asking annotatorsto further label images according to some cat-egories of interest (gender, age, and skin colour)to understand whether the remaining data werebalanced with respect to those categories andthen to address selection bias. This demographicanalysis showed that women and dark-skinnedpeople were both under-represented in the re-maining non-offensive/sensitive and visual cat-egories. Moreover, despite the overall under-representation, some categories were found toalign with stereotypes (e.g., the 66.4% of peo-ple in the category rapper were dark-skinned).Hence, they also potentially addressed someframing bias. The annotation process was val-idated by measuring the annotators’ agreementon a small controlled set of images.Zhao et al. [2017] measured the correlationbetween a protected attribute and the occur-rences of various objects/actions. They assumedto have a dataset labelled with a protected at-14laciteroehtnoitamrofnI;desab-yportnEytirapcihpargomeD;tnuoCytirapcihpargomeD;tnuoC•ytirapcihpargomeDtnuoCtnuoClaciteroehtnoitamrofnIdesab-yportnEegakaeltesataDytilasuaCserusaemtnereffid4serusaemtnereffid31•desab-ecnatsiDdesab-ecnatsiDsnoitnevretnIecapstnetalaniruobhgientseraeNnoitasilarenegtesatad-ssorCnoitasilarenegtesatad-ssorCsaibgnilledoMsaibgnilledoMdesab-ledoMdesab-ledoMytilasuaCgnicruos-dworC•••••••••••••••••••••••••••••••••••••sdohtem/serusaemfoepyTLabelFramingSelectionraeY910202027102710281029102810291029102120291020202120212020202110251022102910251029102020271020202]9102[gnoWdnaytnahluD]7102[.lateraknahS]0202[.la]7102[.latetegnaYoahZ]8102[urbeGdnainiwmalouB]1202[ooJdnaneni¨akkr¨aK]1202[naksilaCdnadeetS]0202[.latenanhsirkalaB]1102[sorfEdnaablarroT]9102[.latezep´oL-zep´oL]5102[rimahSdnaledoM]5102[.lateisammoT]2102[.latealsohK]1202[.lateregnihcaW]0202[.lategnaW]9102[.lategnaJ]9102[akhsavoKdnasamohT]7102[.latezaP-zepoL]0202[.latekralC]0202[.lateuH]9102[.laterelreM]8102[.lateadnaP]9102[.lategnaW]9102[.latemKi123456789011121314151617181910212223242repaP.oNatadralubatotnoitcudeRnoitatneserperegamidesaiBnoitcetedsaibtesatad-ssorCrehtO15Table 4: Summary of the collected material.tribute G with values {g1, ..., gn} and an at-tribute O = o describing the occurrences of saidobjects or actions in the images (for instance,G could be the gender and O a cooking toolor an outdoor sport scene). The bias score ofthe dataset with respect to a protected attributevalue G = g and the scene/object O = o is de-fined as the percentage of g co-occurrences witho:b(o, g) =c(o, g)x∈{g1,...,gn} c(o, x)(cid:80)(1)where c(o, x) counts the co-occurrences of the ob-ject/scene o and the protected attribute’s valuex. If b(o, gi) > 1n , where n is the number of val-ues that the protected attribute can assume, itmeans that attribute G = g is positively corre-lated with the object/action O = o.Shankar et al. [2017] used instead a simplecount to assess geographic bias in ImageNet[Russakovsky et al., 2015] and Open Images[Krasin et al., 2016]. They found out, for ex-ample, that the great majority of images ofwhich the geographic provenance is known comesfrom the USA or Western European countries,resulting in highly imbalanced data.Such ageography-based analysis has been expanded byWang et al. [2020]. While having balanced datais important for many applications, a mere countis usually not enough to assess every type of bias,as a balanced dataset could still contain spuriouscorrelations, framing bias, label bias, etc.Buolamwini and Gebru [2018] constructed abenchmark dataset for gender classification. Fortesting discrimination in gender classificationmodels, their dataset is balanced according tothe distribution of both gender and Fitzpatrickskin type as they noticed that the error ratesof classification models tended to be higher atthe intersection of those categories (e.g., blackwomen) because of the use of imbalanced train-ing data. Hence, while they quantify bias by sim-ply counting the instances with certain protectedattributes, the novelty of their work is that theytook into account multiple protected attributesat a time.1i=1i=1Information theoretical Merler et al. [2019]introduced four measures to construct a bal-anced dataset of faces. Two measures of diver-sity: Shannon entropy H(X) = − (cid:80)nP(X =xi) · ln(P(X = xi)) and Simpson index D(X) =P(X=xi)2 where P(X = xi) is the probabil-(cid:80)nity of an image having value xi for the attributeX ∈ {xi}ni=1; and two measures of evenness:(EShannon = H(X)n ). Suchmeasures have been applied to a set of facial at-tributes, ranging from craniofacial distances togender and skin colour, computed both via au-tomated systems and with the help of humanannotators.ln(n) and ESimpson = D(X)Panda et al. [2018] also proposed to use (condi-tional) Shannon entropy for discovering framingbias in emotion recognition datasets. Using apre-trained model, they computed the dataset’stop occurring objects/scenes. They computedthe conditional entropy of each object acrossthe positive and negative set of the emotionsto see whether some objects/scenes were morelikely to be related to a certain emotion. Forexample, they found out that objects like bal-loons or candy stores are only present in thenegative-set of sadness in Deep Emotion [Youet al., 2016]. Given an object c and an emo-tion E = e ∈ {0, 1} (where e = 1 represents,for instance, sadness and e = 0 represents thenegative-set non-sadness) they computed:H(E|X = c) = −(cid:88)e∈{0,1}pec · ln(pec)(2)wherepec = P(E = e|X = c)When the conditional entropy of an object iszero, it means that such an object is associatedonly with the emotion E or, on the contrary, isnever associated with it (it only appears in thenegative set). This may be considered a type offraming bias.Kim et al. [2019] introduced another defini-tion of bias inspired by information theory. Theywanted to develop a method for training classifi-cation models that do not overfit due to dataset16biases. In doing so, they give a precise definitionof bias: a dataset contains bias when I(X, Y ) (cid:29)0 where X is the protected attribute, Y is theoutput variable and I(X, Y ) := H(X)−H(X|Y )is the mutual information between those ran-dom variables. Kim et al. proposed to min-imise such mutual information during training sothat the model forgets the biases and generaliseswell. Note that if Y is any feature in the dataset,this measure can be used to quantify bias in thedataset instead of the output of a model.Other Wang et al. [2019] defined dataset leak-age to measure the possibility of a protected at-tribute to be retrieved using only the informa-tion about non-protected ones. Given a datasetD = {(xi, yi, gi)} where xiis an image, yi anon-protected attribute and gi is a protected at-tribute, the attribute yi is said to leak informa-tion about gi if there exists a function f (·), calledattacker, such that f (yi) ≈ gi. Operationally,the attacker f (·) is a classifier trained on {yi, gi}.The dataset leakage is measured as follows:λD =1|D||D|(cid:88)i=11[f (yi) = gi](3)Wachinger et al. [2021] explicitly used causal-ity for studying spurious correlations in neu-roimaging datasets. Given variables X and Ythey wanted to test whether X causes Y or thereexists a confounder variable Z instead. Sincethose two hypotheses imply two different factori-sations of the distribution P(X|Y ), the factori-sation with a lower Kolmogorov complexity isthe one that identifies the true causal structure.Kolmogorov complexity is approximated by Min-imum Description Length (MDL).Jang et al. [2019] proposed 8 different mea-sures for identifying gender framing bias inmovies. The following measures are computedfor a movie for each gender, therefore the meansare computed across every frame in which an ac-tor of a certain gender appears. The measuresare the following: Emotional Diversity H(X) =− (cid:80)sP(X = xi) · ln(P(X = xi)) where P(X =i=1√xi) is the probability that a character expressesa certain emotion xi and the sum is computedon the range of the different emotions shown bycharacters of a certain gender (the list of emo-tions was: anger, disgust, contempt, fear, hap-piness, neutral, sadness, and surprise); SpatialStaticity exploits ideas from time-series analysisto measure how static a character is during themovie, it is defined as mean(P SD(p(t)) wherePSD is the power spectral density of the time-series of the position on the x-axis (resp. y-axis)of the character (the higher the value the lessanimated is the character); Spatial occupancyA) where A is the area of the face ofmean(the character; Temporal occupancy NwhereN is the number of frames in which the char-acter appears and Ntotal is the total number offrames; Mean age computed over each frame andcharacter; Intellectual Image mean(G) where Gis the presence of glasses (this seems a debat-able choice as it might itself suffer from somelabel bias); Emphasis on appearance mean(E)where E is the light exposure of faces again cal-culated for each frame; finally, the type and fre-quency of surrounding objects is analysed. Theattributes involved in the computation of thesemeasures are mainly extracted using MicrosoftFace API, which Buolamwini and Gebru [2018]demonstrated to be biased against black women.NtotalWang et al. [2020] proposed REVISE, a com-prehensive toolfor bias discovery in imagedatasets. The authors defined three sets of met-rics. The first set contains metrics based solelyon the use of bounding boxes of objects (Notethat a person is considered an object as it canbe classified by an objected detection model);the second set contains metrics that exploit alsoprotected attribute labels; the third set uses ad-ditional unstructured information such as textwhen the protected attribute is not provided ex-plicitly. REVISE implements 13 different met-rics, some of which are very similar to what wedescribed in the previous paragraphs. We aregoing to describe two of the most relevant:i)scene diversity H(S) where S is the scene at-tribute computed applying a pre-trained scene17recognition algorithm [Zhou et al., 2018]; and ii)appearance differences, computed by extractingfeatures using a feature extractor of images witha same object/scene but a different protected at-tribute value and then training an SVM classifieron the extracted features to see whether it couldlearn different representations of subjects withdifferent protected attribute values in the samecontext.3.2 Biased Image RepresentationsThe following methods analyse the distancesand geometric relations among images exploit-ing their representation in a lower-dimensionalspace to discover the presence of bias.Distance-based K¨arkk¨ainen and Joo [2021]proposed a simple measure of diversity for test-ing their face dataset. They studied the distribu-tion of pairwise L1-distances calculated after em-bedding the images in a lower-dimensional spaceusing a neural network pre-trained on a differentbenchmark face dataset. If such distribution isskewed towards high pairwise distances, it meansthat the data show high diversity. Nonethe-less, such an analysis is heavily influenced bythe embedding. For instance, faces in a “white-oriented” dataset were also well separated in theembedding space, probably because the neuralnetwork used for the embedding had also beentrained using a similarly biased dataset.Steed and Caliskan [2021] developed a methodfor addressing human-like biases in the latentimage representation of unsupervised generativemodels inspired by bias detection methods inNatural Language Processing [Caliskan et al.,2017]. They discovered that the biases foundin the latent space of two big models trainedon ImageNet [Russakovsky et al., 2015] matchwith human-like biases. The authors measuredbias by looking at associations between semanticconcepts (for example, man-career and woman-family) by measuring the cosine similarity amongvectors in the latent space computed by apply-ing the models to controlled samples of imagesthat resemble those visual concepts. More pre-cisely, given a model f that maps images intoa vector space Rd, two sets of images J and K(e.g., photos of men and women, respectively),and two sets A and B of images representingthe concepts we want to measure the associationwith (e.g., photos of people at work and photosof people in familiar settings), we can measurethe association of J with A and K with B in thefollowing way:s(J, K, A, B) =s(j, A, B) −(cid:88)j∈J(cid:88)k∈Ks(k, A, B)wheres(w, A, B) =meana∈Acos(f (w), f (a))−− meanb∈Bcos(f (w), f (b))(4)(5)Alternatively, we can measure the size of theassociation via the Cohen’s d:d =meanj∈J s(j, A, B) − meank∈Ks(k, A, B)stdw∈J∪Ks(w, A, B)(e.g.,(6)The authors found out that the represen-tationsthey analysed contained several bi-ased associations that resemble human cog-the association betweennitive biasesflower/insects with pleasant/unpleasant respec-tively, male/female with career/family orwhite/black with tools/weapons). Nevertheless,it is not clear how many of those associationswere present in the original training data and towhat degree this is the responsibility of the CVmodels used for computing the representations.While this method for measuring biased associa-tions is model agnostic, in the sense that it can beapplied to any possible model, its results heavilydepend on the learned representations and theemployed models.Other Balakrishnan et al. [2020] developed amethod for assessing algorithmic biases in imageclassifiers via computing the causal relationshipbetween the protected attribute and the outputIn doing so, they developed aof a classifier.18method for intervening on the image attributes:they assumed to have a generator, such as a Gen-erative Adversarial Network [Goodfellow et al.,2014], that generates images from latent vectors.They also assumed to have learned hyper-planesin the latent space that separate different at-tributes. Sampling points along the directionorthogonal to an attribute’s hyper-plane gives amodified version of the original image with re-spect to that attribute. The authors noted thatsuch interventions are not entirely disentangled:for example, adding long hair to the images ofwhite males also adds the beard, and chang-ing the gender attributes to the images of blackmales adds earrings. This is probably due todatasets bias which is then detected as a side ef-fect of the transformation described above. Notethat, since such transformation is the result ofgeometric operations, it means that the bias isencoded in the geometry of the latent space. Itwould be interesting to study to what extentthese manipulations of the latent space can beused as a bias exploration tool.3.3 Cross-dataset Bias DetectionMethods in this category derive from the real-isation that the issue of generalisation in CVmight be due to bias. Researchers in the fieldof object detection are usually able to tell withfair accuracy which famous benchmark datasetan image comes from [Torralba and Efros, 2011].This means that each dataset carries some sortof “signature” (bias) that makes the provenanceof its images easily distinguishable and affectsthe ability of the models to generalise well. Themethods described in the following aim to detectsuch signatures by comparing different datasets.Note that the works of Tommasi et al. [2015],L´opez-L´opez et al. [2019] could also fit Section3.2.Cross-dataset generalisation Torralba andEfros [2011] tested bias in object detectiondatasets by answering the following question:“How well does a typical object detector trainedon one dataset generalize when tested on a rep-resentative set of other datasets, compared withits performance on the native test set?” Theassumption here is that if the performance onthe native test set is much higher it means thedatasets exhibit some bias that is learned bythe object detector fD. Hence, let us considera dataset of interest D and other n benchmarks{Bi}. They compare performance by computing:1nn(cid:88)i=1APBi(fD)APD(fD)(7)The closer to 1 this score is, the more fD gen-eralises well. Note that if this score is low, wecan say that the fD does not generalise well andthus the dataset D is probably biased, while ifthe score is close to 1 we can say that the datasetsshare a similar representation of the visual world.Furthermore, the authors also proposed a test,which they presented as a toy experiment butwhich has been utilised in many other studies,called Name the dataset. They trained a modelto recognise the source dataset of a particularimage: the greater the accuracy of this model,the more distinguishable and the more biased thedatasets are. The methods described above havealso been used by Tommasi et al. [2015], Pandaet al. [2018], K¨arkk¨ainen and Joo [2021].Tommasi et al. [2015] investigated the possibil-ity of using CNN feature descriptors to addressdataset bias. In particular, they replicated theexperiments by Torralba and Efros [2011] andKhosla et al. [2012] (see next paragraph) usingDeCAF features [Donahue et al., 2014]. Further-more, they slightly changed the measure used byTorralba and Efros for the evaluation of cross-generalisation:CD = σ(1100(APD(fD)−1nn(cid:88)i=1APBi(fD))) (8)where σ() is the sigmoid function.Other Khosla et al. [2012] proposed a methodfor both modelling and mitigating dataset bias.19j, yij)}sij=1, where |Di| = si, yiIn particular, they train an SVM binary classi-fier on the union of a set of n datasets Di ={(xij ∈ {−1, 1} isa common class among all the n datasets, andwhere xij are feature vectors extracted via somefeature extraction algorithms. The problem isframed as a multi-task classification [Evgeniouand Pontil, 2004] where the algorithm learns ndistinct hyper-planes wi · x where wi = w + ∆i.The vector w, which is common to each dataset,models the common representation of the visualworld while the specific biases of each of the Didatasets are modelled by the vectors ∆i. This isachieved via the following minimisation problemminw,∆i12||w||2 +n(cid:88)i=1(cid:20) λ2(cid:21)||∆i||2 + L(w, ∆i)(9)whereL(w, ∆i) =si(cid:88)(C1 min(1, yijw · xij)+j=1+ C2 min(1, yij(w + ∆i) · xij)(10)and λ, C1 and C2 are hyper-parameters. Theauthors proved that studying the vectors ∆igives useful semantic information about eachdataset bias (e.g., they discovered that Cal-tech101 [Li Fei-Fei et al., 2004] has a strong pref-erence on the side view of cars).L´opez-L´opez et al. [2019] also investigated theuse of feature descriptors to discover biases. Inparticular, they wanted to understand how theimages in different datasets are distributed afterembedding them in the same lower-dimensionalspace. Given n datasets, D1,...,Dn they sam-pled two sets of images for each of them G1,...,Gnand P1,...,Pn, called respectively gallery sets andprobe sets. Then, they computed the latent spaceapplying a pre-trained feature descriptor f to thegallery sets. After that, they computed the fol-lowing probability:P(x∗ ∈ Di|x ∈ Di)(11)among the feature vectors of the images in theunion of the gallery sets. If such probability isnot 1n , it means that the nearest neighbours arenot equally distributed among the n datasets.Hence, there must be some selection bias.3.4 Other MethodsOther visual dataset bias discovery methods donot fit any of the three categories describedabove, and range from crowdsourcing frame-works to ad-hoc trained classification models.Model-based Model and Shamir [2015] pro-posed a simple method for addressing bias in ob-ject recognition datasets. They cropped a smallcentral sub-image from each image in the orig-inal dataset. These cropped pictures were sosmall that humans could not recognise the ob-jects in the pictures. Hence, if the attained per-formance of a model trained on such images wasbetter than pure chance, the data contained dis-tinguishable features spuriously correlated withthe object categories. Then the data showedsome kind of selection or framing (capture) bias.Thomas and Kovashka [2019] studied the po-litical framing bias of images by scraping imagesfrom online news outlets. Their idea was to traina semi-supervised tool for classifying images ac-cording to their political orientation. They la-belled images according to the political orienta-tion of the source. They also used informationregarding the articles hosting the images by feed-ing the network with both the image and the doc-ument embedding of the article. Note that theproposed architecture incorporates textual infor-mation at training time but allows them to clas-sify images without any additional informationat testing time. Thus, this model can be usedto understand if a specific image or a collectionof images is biased towards a particular politicalfaction. Moreover, the visual explanation of sucha model could give semantic information on thepolitical framing of the dataset.where x is the feature vector of an image inthe probe set Pi and x∗ is its nearest neighbourClark et al. [2020] proposed to use an ensem-ble classification algorithm for mitigating bias.20The idea is to train a low-capacity network (i.e.,with a low number of parameters) together witha high-capacity one (i.e., with more than doublethe parameters) so that the former learns spu-rious correlations while the latter learns to clas-sify the data in an unbiased way. While thisdifference in capacity and the ensemble train-ing encourages the two models to learn differ-ent patterns, there is still the possibility for thehigh-capacity model to learn simpler patternsand hence bias. To avoid this, the two networksare trained to result in conditionally indepen-dent outputs. This will be an incentive for theensemble to isolate simple and complex patterns.While the authors use this method with the onlypurpose of mitigating algorithmic bias, studyingthe lower-capacity model can give informationon spurious correlations in the training dataset,highlighting selection/framing bias.Lopez-Paz et al. [2017] proposed a method todiscover “causal signals” among objects appear-ing in image datasets.In particular, given aset of images D = {di}, they score the pres-ence of certain objects/attributes A, B by us-ing an object detector or a feature extractor,respectively. Then they apply a Neural Causa-tion Coefficient (NCC, [Lopez-Paz et al., 2015])network, a neural network that takes a bag ofsamples {(ai, bi)mi=1} as input and returns a scores ∈ [0, 1] where s = 0 means that A causes B ands = 1 means instead that B causes A. Note that,while not every causal relationship is a source ofbias, some might be. Moreover, such causal re-lationships can be thought of as a by-product ofthe selection of the subject. Hence this methodcan detect selection bias.Human-based Hu et al.[2020] proposed anon-automated approach for bias discovery.Their method consists of a three-step crowd-sourcing workflow for bias detection (selectionand framing, according to our categorisation).In order to avoid the complexity of free text de-scription, in the first step, workers are presentedwith a batch of images and asked to describe thesimilarities among the images of the batch viaa question-answer pair (e.g., if every image ofthe batch shows only white airplanes, and thenthere is some selection bias, the worker would la-bel the batch with the following question-answerpair: What colour are the airplanes in the im-ages? White). In a second step, each worker isasked to answer some of the questions collectedin the first phase based on different batches toconfirm the presence of such biases. Finally, theworkers are asked to evaluate whether the state-ments are true in the real visual world or, on thecontrary, constitute some biases (e.g., is it truethat every airplane is white? If the answer isyes, this is not considered an instance of bias).Note that since this last step is based on “com-mon sense knowledge and subjective belief” [Huet al., 2020], it heavily relies upon workers’ back-grounds and biases.3.5 DiscussionThe reduction to tabular data is a reasonable andeffective way of discovering bias. These methodscould leverage the great amount of work that hasbeen already done in the field of Fair MachineLearning for tabular data. Nevertheless, it seemsthat for what concerns visual data, the meth-ods used are relatively simplistic. Most of theworks just look for balance in the protected at-tribute or compare the distribution with respectto other features. Furthermore, these methodsheavily rely on labels that are either attached tothe data or automatically extracted. Hence, anylabelling bias affects such discovery methods andshould be taken into account.In a complementary way, the image represen-tation methods reduce the problem to bias de-tection in a lower-dimensional space instead ofreducing it to tabular data. While this couldbetter capture the complexity of visual content,such methods are necessarily influenced by boththe models used to compute the representationand the metric used to compute distances in it.Moreover, they are usually harder to apply sincethey need some kind of supervision for comput-ing the space.21Despite their historicalimportance, cross-dataset detection methods suffer from major is-sues. First, they are only applicable when sev-eral comparable datasets are available, which isnot often practical. Second, they might help tounveil the presence of bias, but without furtherinspections, they cannot reveal what kind of biasit is. Note that, while these methods might beuseful to get an idea of the existence of some bi-ases in a visual dataset, they are of little or nouse if we want to discover bias within the dataset,for example, if we want to understand whetherthere are discriminatory differences between menand women in the same dataset.Regarding those methods that do not fit anyof the above-mentioned categories, we cannotoutline common pros and cons as they are veryproblem/domain-specific.4 Bias-aware Visual DataCollectionIn the following, we are going to describe someattempts to construct datasets where the ex-istence of bias was taken into account duringthe dataset creation process. These datasetswere constructed for specific purposes and wereprobably not thought of as universally bias-freedatasets. Nonetheless, analysing which biaseshave been removed and which have been notmight be helpful to understand the general chal-lenge of bias in visual datasets. We summarisethe content of this section in Table 5. Further-more, we propose a checklist for helping the col-lection of bias-aware visual datasets (Table 6).Note that, while the previous section systemat-ically reviewed the literature on bias discoveryand quantification in visual data, this section ismeant as a case study. Therefore, it is more lim-ited in its scope.4.1 Datasetsa face dataset constructed by collecting the pho-tos of members of six different national parlia-ments. The authors aimed to collect balanceddata regarding the gender of the subjects andtheir skin colour. To do so, they selected threenations from African countries (Rwanda, Sene-gal, and South Africa) and three from Euro-pean countries (Iceland, Finland, and Sweden)according to the gender parity rank11 amongtheir Members of Parliament (MP). The datahave been labelled by three annotators (includ-ing the authors) according to (binary) gender ap-pearance and the Fitzpatrick skin type (rangingfrom I to IV, these labels are used by dermatolo-gist as a gold standard for skin classification).The definitive skin labels were provided by aboard-certified dermatologist, while the defini-tive gender labels were also determined based onthe title, prefix, or the name of the parliamentar-ians (note that using names as a proxy of gendercan cause label bias [Karimi et al., 2016]). Whilethe data collection process described above re-sulted in a much more balanced dataset com-pared to other famous benchmarks (Adience [Ei-dinger et al., 2014]; IJB-A [Klare et al., 2015]),it is still not free from possible biases. For exam-ple, the selection process targets a small numberof African and northern European countries toensure gender and skin tone balance. Neverthe-less, it completely excludes, for instance, Asianand South American countries. Moreover, MPsare likely to be middle-aged people, which couldalso exclude young and older people from the se-lection. On the framing bias side, different coun-tries might have different standards/dress codesfor their MPs’ official portraits, which could turninto some biases as well.K¨arkk¨ainen and Joo [2021] collected a facedataset emphasising, in particular, the balancein terms of age, gender, and race. They reliedon a crowdsourcing workflow for annotating theimages. In particular, they asked three differentworkers to label the images according to gender,age group, and race. They kept the label if thereBuolamwini and Gebru [2018] released the PilotParliaments Benchmark (PPB) dataset. PPB is11https://data.ipu.org/women-ranking?month=6&year=2021. Last visited 23.03.2022.22Label•••FramingSelection•••••••••••-xe;srotatonnanamuH-rofnilanoitidda;strepnoitamseYniks;rednegyraniBsegamiK2.18102urbeGdnainiwmalouBenot]8102[;srotatonnaenihcaMseY;rednegyraniB;egAsegamiK0799102]9102[.laterelreMsrekrowdworCseY;rednegyranib;egAsegamiK5.8011202ooJdnaneni¨akkr¨aKecar]1202[srekrowdworcenotniks-am;srotatonnanamuHseYrednegyranib;egA;segamiK140202.latesoluopogroeGnoitamrofnilanoitiddAsrotatonnaenihc-oNseYecar;rednegyraniB-soedivK44segamiK05segamiK2191020202-ulcnI(]0202[.lateuW]9102[.lateubraB)kramhcneBevis]0202[noitamrofnilanoitiddAseY;rednegyranib-noNsegamiK20202-noN(]0202[.lateuWssecorPgnillebaL-erP-tagnitsixetnetnoCdetcetorPsetubirteziSraeYrepaPslebaldedivorpenotniks;redneg-fles;srotatonnanamuHoNyranib-noN;egAsoedivK1.541202]1202[.latesabrizaHecar-hcneBredneGyranib)kramTable 5: Summary of the datasets described in Section 4.23was a 2/3 accordance. Otherwise, they wouldhave proposed the image to other three workersand discarded it if again it resulted in three dif-ferent judgements. We can spot two sources oflabel and selection bias, respectively: first, wecannot be sure that the workers are able to de-termine the three labels homogeneously acrossevery sub-group 12; second, discarding the pho-tos the workers cannot agree on might result inthe missed selection of a certain group of peo-ple whose characteristics are difficult to deter-mine for the workers. Finally, the taxonomy ofraces used by the authors (White, Black, Indian,East Asian, Southeast Asian, Middle East, andLatino) already introduces a form of label bias.While it is derived from the taxonomy commonlyused by the US Census Bureau and might bedescriptive of the composition of the US popula-tion, it hardly captures the complexity of humandiversity.Diversity in Face (DiF) [Merler et al., 2019]and KANFaces [Georgopoulos et al., 2020] aretwo face datasets that try to address the issueof bias by ensuring as much diversity as pos-sible using the diversity measures proposed byMerler et al. [2019] that we reviewed in Section3. The attributes they control the diversity forare: age, gender, skin tone, a set of craniofa-cial ratios, and pose. The authors also took intoaccount a metric of illumination. By collectingdiverse data, the authors try to avoid selectionbias (in the case of age, gender, and skin tone)and some framing bias (in the case of pose andillumination).Barbu et al.[2019] made an attempt toavoid framing bias in large-scale object detectiondatasets. In particular, they added controls forobject rotations, viewpoints and background byasking crowd workers to photograph objects intheir homes in a natural setting according to theinstructions given by the authors. While thisresulted in a much more diverse dataset (the au-12see the interesting TwitterCostanza-Chockschock/status/134647883125578956923.03.2022) for an on-point discussion about this issue(@schock),thread from Sachahttps://twitter.com/visited(Lastthors used ImageNet [Russakovsky et al., 2015]as a comparison), because of the above controls,the objects appear only in indoor context, arerarely occluded, and are often centre-aligned.Thus, it seems that specific framing biases havebeen avoided, while the collection procedure hasintroduced others. Also, the authors removedsome classes from the dataset for reasons thatrange from privacy concerns (e.g., “people”) orbecause they were not easy to move and pho-tograph in different settings (e.g., “beds”).Inprinciple, this might generate some selection bias(more specifically, negative class bias) since theabsence of those objects could make the negativeclasses less representative.Wu et al.[2020] collected two benchmarkdatasets:the Inclusive Benchmark Database(IBD), and Non-binary Gender BenchmarkDatabase (NGBD)13. IBD contains 12,000 im-ages of 168 different subjects, 21 of whom iden-tify as LGBTQ. The geographic provenance ofthe subjects in the dataset is balanced. NGBDcontains 2,000 images of 67 unique subjects. Thesubjects are public figures whose gender identityis known. Thus, the database contains multiplegender identities (namely: non-binary, gender-fluid, genderqueer, gender non-conforming, agen-der, gender neutral, gender-less, third gender,and queer ). The authors themselves identify twomajor risks of label bias: first, “Gender identityhas its multifaceted aspects that a simple labelcould not categorize” [Wu et al., 2020] (the au-thors identify the problem of modelling genderas a continuum as a direction for future work);and, second, “Gender is a complex socio-culturalconstruct and an internal identity that is not nec-essarily tied to physical appearances.” [Wu et al.,2020].Hazirbas et al.[2021] proposed the CasualConversations Dataset for evaluating the per-formance of CV models across different demo-13While collecting inclusive data is a noble purpose, theuse the authors made of their dataset (extending genderclassification algorithms to non-binary genders) should bestrongly discouraged as it might be used to target alreadymarginalised groups of people.24graphic categories. Their dataset is composed of3,011 subjects and contains over 45,000 videos,with an average of 15 videos per person. Thevideos were recorded in multiple US states witha diverse set of adults in various age, gender andapparent skin tone groups. This work representsprobably the greatest effort to build a balanceddataset addressing both selection and framingbias (in the form of the illumination of videos).Nevertheless, some forms of imbalance remain.For example, most videos present bright light-ing conditions; and most subjects are labelledas either male or female (with just the 0.1% ofthe participants that identify as “Others” and2.1% whose gender is unknown). The label biasthat the use of categories such as gender, age,and race could create is overcome by asking theparticipants their age and gender and using theFitzpatrick Skin Type instead of race. Nonethe-less, the authors declare that there are “videos inwhich two subjects are present simultaneously”but that they provide only one set of labels whichmight also be a form of label bias. Last, we notethat the subjects are all from the US, which rep-resents a serious selection bias as the US popula-tion hardly represents the whole of humankind.4.2 DiscussionAs highlighted by the cases analysed in the previ-ous section, dealing with bias in visual data is nota trivial task. In particular, collecting bias-awarevisual datasets might be incredibly challenging.Thus, we propose a checklist (Table 6) to helpscientists and practitioners spot and make ex-plicit possible biases in the data they collect oruse. Furthermore, we add a brief discussion onpre-processing mitigation strategies. Note thatbias mitigation is an area on its own and wouldrequire an entire review to be adequately dis-cussed. Therefore, we will just provide the inter-ested reader with some hints and references.Our checklist is inspired by previous work ondocumentation and reflexive data practices [Ge-bru et al., 2018, Miceli et al., 2021], but addsseveral questions specific to selection, framing,and label bias because they have their own pe-culiarities and must be analysed separately. Westart with a general set of questions on thedataset purpose and the collection procedure.Then, we continue with questions specific to se-lection, framing, and label bias.In particular,we ask whether the selection of subjects gener-ates any imbalance or lack of diversity, whetherthere are spurious correlations or harmful fram-ing, whether fuzzy categories are used for la-belling, and whether the labelling process con-tributes to inserting biases.We want to stress that careful data collectionand curation are probably the most effective mit-igation strategies (see, for example, the mitiga-tion insights described in [Wang et al., 2020])for all the three types of bias presented in 2.However, selection bias seems to be the easiestto mitigate using standard pre-processing tech-niques such as re-sampling or re-weighting. Notethat several image datasets have a long-tail dis-tribution of objects, and hence pre-processingmitigation techniques have to take that into ac-count (see [Zhang et al., 2021]), especially whenmany objects can co-occur in the same image.When the label bias is generated by poor op-erationalisation of unobservable theoretical con-struct [Jacobs and Wallach, 2021], the entire la-belling must be reconsidered. Otherwise, whenbias is caused by the presence of synonyms or in-correct labelling, an effective data cleaning couldhelp the mitigation. When it appears in the formof capture bias, framing bias can be mitigated byeither re-sampling techniques that ensure diver-sity (in pose and lighting, for example) or viadata augmentation. When the framing bias re-lates to the messages carried by the images, auto-matic pre-processing strategies are probably lesseffective as the way an image is interpreted andthe message it conveys are not universal, whichgives rise to the need for careful human inspec-tion and analysis.25GeneralSelection biasFraming biasLabel biasDescriptionWhat are the purposes the data is collected for? [Gebru et al., 2018]Are there uses of the data that should be discouraged because of pos-sible biases? [Gebru et al., 2018]What kind of bias can be inserted by the way the collection process isdesigned? [Gebru et al., 2018]Do we need balanced data or statistically representative data?Are the negative sets representative enough?Is the dataset representative enough?Is there any group of subjects that is systematically excluded from thedata?Do the data come from or depict a specific geographical area?Does the selection of the subjects create any spurious associations?Will the data remain representative for a long time?Are there any spurious correlation that can contribute to framing dif-ferent subjects in different ways?Is there any biases due to the way images/videos are captured?Did the capture induce some behaviour in the subjects (e.g. smilingwhen photographed)?Are there any images that can possibly convey different messages de-pending on the viewer?Are subjects of a certain group depicted in a particular context moreoften than others?Do the data agree with harmful stereotypes?If the labelling process relies on machines: have their biases been takeninto account?If the labelling process relies on human annotators: is there an adequateand diverse pool of annotators? Have their possible biases been takeninto account?If the labelling process relies on crowd sourcing: are there any biasesdue to the workers’ access to crowd sourcing platforms?Do we use fuzzy labels? (e.g., race or gender)Do we operationalise any unobservable theoretical constructs/use proxyvariables? [Jacobs and Wallach, 2021]Table 6: Checklist for bias-aware visual data collection5 Conclusions and ResearchOutlookThe aim of this survey was threefold: first, toprovide a description of different types of biasand to illustrate the processes through whichthey affect CV applications and visual datasets;second, to perform a systematic review of meth-ods for bias discovery in visual datasets; and,third, to describe existing attempts to collect vi-sual datasets in a bias-aware manner. We showedhow the problem of bias is pervasive in CV; It ac-companies the whole life cycle of visual content,involves several actors, and re-enters the life cy-cle through biased CV algorithms. One of ourmajor contributions has been to provide a de-tailed description of the different manifestations(selection, framing, and label) of bias in visualdata, along with several examples. We also wentfurther by providing a sub-categorisation (Ta-ble 1) which also includes several categories ofbias that are commonly described in Statistics,Health studies, or Psychology adapting them to26the context of visual data.should be studied more deeply.The systematic review in Section 3 allowedus to draw some consideration on the state ofthe art in bias discovery methods for visual dataand to outline some possible future streams ofresearch. The vast majority of them used thestrategy of reducing the problem to tabular data(Section 3.1). While it seems a natural optionas it allows leveraging the wealth of techniquesfrom the Fair Accountable and Transparent Ma-chine Learning (FATML) literature, using differ-ent representations of visual data would certainlyopen new unexplored research directions. Forexample, representing an image as a scene graph[Johnson et al., 2015], would allow the use ofbias detection techniques used for rankings andgraphical data [Krasanakis et al., 2020, Pitouraet al., 2021]. Moreover, since scene graphs can bethought of as small-scale knowledge graphs, thisrepresentation would allow both the integrationof additional knowledge (e.g., from Wikipedia)and the use of methods for bias detection inknowledge graph embeddings [Bourli and Pi-toura, 2020].Balakrishnan et al. [2020] showed that a latentrepresentation of data encapsulates bias in its ge-ometry. This was also noted by Bolukbasi et al.[2016] in their work on bias in word embeddings.Nonetheless, the study of this relationship be-tween the geometry of latent spaces and bias isusually limited to variations of cosine similarity.Hence, a promising research direction would beto exploit more sophisticated tools for studyingsuch relationships, e.g., Topological Data Anal-ysis [Chazal and Michel, 2021].Some of the most influential works on biasdetection in images, e.g., [Torralba and Efros,2011], focused on object detection datasets. Insuch datasets, the long-tail distribution of ob-jects is very common and evaluating bias in thesedatasets is difficult because the long tail dis-tracts from the focal points. Moreover, the co-occurrence of different objects has obvious impli-cations for the effectiveness of mitigation meth-ods [Zhang et al., 2021] for imbalanced data (e.g.,oversampling, under-sampling), and therefore itMost of the reviewed papers focus on images.Only few of them study bias in videos. Thisopens the road to many possibilities (for exam-ple, applying methods from time-series and mul-timodal analysis to the bias detection domain).Furthermore, we noted that selection and fram-ing bias are the two types of bias that are morecommonly taken into account. Nevertheless, la-bel bias is often very concerning (Section 2.3)and hence more research is needed to fill thisgap.Similarly, for obvious simplicity reasons, mostrelated works concentrate on a single protectedattribute (an exception to this is the work ofBuolamwini and Gebru [2018]). Hence, multi-attribute fairness studies would enrich the liter-ature in this respect. We also noted that mostworks we reviewed in Section 4 focus on facialdata. This is probably due to the concerningapplications of face detection/recognition algo-rithms. Therefore, there is room for improvingdatasets and establishing data collection prac-tices in other fields, including medical imaging,self-driving cars, etc.We finally reviewed several attempts to collectbias-aware data in Section 4 and concluded thatthere is no such thing as bias-free data. Hence,it is of utmost importance, along with the devel-opment of reliable bias discovery tools, that re-searchers and practitioners become aware of thebiases of the datasets they collect and make themexplicit in a standardised way (see, for example,[Gebru et al., 2018, Miceli et al., 2021]). In Ta-ble 6, we outline a checklist for the collection ofvisual data. We believe that having such a guidewill help practitioners and scientists spot pos-sible causes of bias, collect data that is as lessbiased as possible, and be aware of such biasesduring their analysis.Acknowledgements We would like to thankAlaa Elobaid, Miriam Fahimi and GiorgosKordopatis-Zilos for their feedback and fruitfuldiscussions. This work has received funding fromthe European Union’s Horizon 2020 research and27innovation programme under Marie Sklodowska-Curie Actions (grant agreement number 860630)for the project “NoBIAS - Artificial Intelligencewithout Bias” and under grant agreement num-ber 951911 for the project “AI4Media - A Euro-pean Excellence Centre for Media, Society andDemocracy”. This work reflects only the au-thors’ views and the European Research Exec-utive Agency (REA) is not responsible for anyuse that may be made of the information it con-tains.ReferencesJulia Angwin, Jeff Larson, Surya Mattu, andLauren Kirchner. Machine bias: There’ssoftware used across the country to predictfuture criminals. and it’s biased againstblacks.URL https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.ProPublica, 2016.Dane Archer, Bonita Iritani, Debra D. Kimes,and Micheal Barrios. Face-ism: Five studies ofsex differences in facial prominence. Journalof Personality and Social Psychology, 45:725–735, 1983.Guha Balakrishnan, Yuanjun Xiong, Wei Xia,and Pietro Perona. Towards causal bench-marking of bias in face analysis algorithms. InAndrea Vedaldi, Horst Bischof, Thomas Brox,and Jan-Michael Frahm, editors, ComputerVision - ECCV 2020 - 16th European Confer-ence, Glasgow, UK, August 23-28, 2020, Pro-ceedings, Part XVIII, volume 12363 of Lec-ture Notes in Computer Science, pages 547–563. Springer, 2020.10.1007/978-3-030-58523-5\ 32. URL https://doi.org/10.1007/978-3-030-58523-5_32.doi:Jack Bandy. Problematic machine behavior: Asystematic literature review of algorithm au-dits. Proc. ACM Hum.-Comput. Interact., 5(CSCW1), apr 2021. doi: 10.1145/3449148.URL https://doi.org/10.1145/3449148.Richard Baraniuk, David L. Donoho, and MatanGavish. The science of deep learning. Proceed-ings of the National Academy of Sciences, 117:30029 – 30032, 2020.Andrei Barbu, David Mayo, Julian Alverio,William Luo, Christopher Wang, Dan Gut-freund, Josh Tenenbaum, and Boris Katz.Objectnet:A large-scale bias-controlleddataset for pushing the limits of object recog-nition models. In H. Wallach, H. Larochelle,A. Beygelzimer, F. Alch´e-Buc, E. Fox, andR. Garnett,editors, Advances in NeuralInformation Processing Systems, volume 32.Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf.Guido Barbujani and Vincenza Colonna. Ge-netic Basis of Human Biodiversity: An Up-date, pages 97–119. Springer, 08 2011. ISBN978-3-642-20991-8. doi: 10.1007/978-3-642-20992-5 6.Robert Bartlett, Adair Morse, Richard Stanton,and Nancy Wallace. Consumer-lending dis-crimination in the fintech era. Working Pa-per 25943, National Bureau of Economic Re-search, June 2019. URL http://www.nber.org/papers/w25943.Bettina Berendt, Fabien Gandon, Susan Hal-ford, Wendy Hall, Jim Hendler, Katharina E.Kinder-Kurlanda, Eirini Ntoutsi, and SteffenStaab. Web futures:intelligent,sustainable the 2020 manifesto for web sci-ence (dagstuhl perspectives workshop 18262).doi:Dagstuhl Manifestos, 9(1):1–42, 2021.10.4230/DagMan.9.1.1. URL https://doi.org/10.4230/DagMan.9.1.1.Inclusive,Tolga Bolukbasi, Kai-Wei Chang, James Zou,Venkatesh Saligrama, and Adam Kalai. Manis to computer programmer as woman is tohomemaker? debiasing word embeddings. InProceedings of the 30th International Confer-ence on Neural Information Processing Sys-tems, NIPS’16, page 4356–4364, Red Hook,28NY, USA, 2016. Curran Associates Inc. ISBN9781510838819.Styliani Bourli and Evaggelia Pitoura. Biasin knowledge graph embeddings.In MartinAtzm¨uller, Michele Coscia, and Rokia Mis-saoui, editors, IEEE/ACM International Con-ference on Advances in Social Networks Anal-ysis and Mining, ASONAM 2020, The Hague,Netherlands, December 7-10, 2020, pages 6–10. IEEE, 2020. doi: 10.1109/ASONAM49781.2020.9381459. URL https://doi.org/10.1109/ASONAM49781.2020.9381459.Kevin W. Bowyer, Micheal C. King, Walter J.Scheirer, and Kushal Vangara. The “criminal-ity from face” illusion. IEEE Transactions onTechnology and Society, 1(4):175–183, 2020.doi: 10.1109/TTS.2020.3032321.Joy Buolamwini and Timnit Gebru. Gendershades: Intersectional accuracy disparities incommercial gender classification. In Sorelle A.Friedler and Christo Wilson, editors, Confer-ence on Fairness, Accountability and Trans-parency, FAT 2018, 23-24 February 2018, NewYork, NY, USA, volume 81 of Proceedingsof Machine Learning Research, pages 77–91.PMLR, 2018. URL http://proceedings.mlr.press/v81/buolamwini18a.html.Aylin Caliskan, Joanna J. Bryson, and ArvindNarayanan. Semantics derived automaticallyfrom language corpora contain human-like bi-ases. Science, 356(6334):183–186, 2017. ISSN10.1126/science.aal4230.0036-8075.URLhttps://science.sciencemag.Pub-org/content/356/6334/183.Associationforlisher:eprint:the AdvancementSciencehttps://science.sciencemag.org/content/356/6334/183.full.pdf.Americandoi:ofFr´ed´eric Chazal and Bertrand Michel. An in-troduction to topological data analysis: Fun-damental and practical aspects for data sci-entists. Frontiers in Artificial Intelligence, 4,2021. ISSN 2624-8212. doi: 10.3389/frai.2021.667963. URL https://www.frontiersin.org/article/10.3389/frai.2021.667963.29Alexandra Chouldechova. Fair prediction withdisparate impact: A study of bias in recidivismprediction instruments. Big Data, 5 2:153–163,2017.Christopher Clark, Mark Yatskar, and LukeZettlemoyer.Learning to model and ig-nore dataset bias with mixed capacity ensem-bles.In Trevor Cohn, Yulan He, and YangLiu, editors, Proceedings of the 2020 Confer-ence on Empirical Methods in Natural Lan-guage Processing: Findings, EMNLP 2020,Online Event, 16-20 November 2020, pages3031–3045. Association for ComputationalLinguistics, 2020.10.18653/v1/2020.doi:findings-emnlp.272. URL https://doi.org/10.18653/v1/2020.findings-emnlp.272.Renita Coleman. Framing the pictures in ourheads: Exploring the framing and agenda-setting effects of visual images. Doing FrameAnalysis: Empirical and Theoretical Perspec-tives, pages 233–261, 01 2010.Cynthia M. Cook, John J. Howard, Yevgeniy B.Sirotin, Jerry L. Tipton, and Arun R. Ve-mury. Demographic effects in facial recogni-tion and their dependence on image acquisi-tion: An evaluation of eleven commercial sys-tems. IEEE Transactions on Biometrics, Be-havior, and Identity Science, 1(1):32–41, 2019.doi: 10.1109/TBIOM.2019.2897801.Laura Corradi. Specchio delle sue brame: anal-genere,isi socio-politica delle pubblicit`a :Ses-classe, razza, et`a ed eterosessismo.sismo e2012.razzismo. Saggi. Ediesse,ISBN 9788823016576. URL https://books.google.gr/books?id=ckGfpwAACAAJ.Maria Cramer and Micheal Levenson. Yearbookphotos of girls were altered to hide theirchests. The NY Times, May 2021. URLhttps://www.nytimes.com/2021/05/23/us/yearbook-photos-st-johns-girls-altering.html.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li,Imagenet: A large-Kai Li, and Li Fei-Fei.scale hierarchical image database.In 2009IEEE Conference on Computer Vision andPattern Recognition, pages 248–255, 2009. doi:10.1109/CVPR.2009.5206848.Jeff Donahue, Yangqing Jia, Oriol Vinyals, JudyHoffman, Ning Zhang, Eric Tzeng, and TrevorDarrell. Decaf: A deep convolutional acti-vation feature for generic visual recognition.In Eric P. Xing and Tony Jebara, editors,Proceedings of the 31st International Con-ference on Machine Learning, volume 32 ofProceedings of Machine Learning Research,pages 647–655, Bejing, China, 22–24 Jun2014. PMLR. URL http://proceedings.mlr.press/v32/donahue14.html.Pawel Drozdowski, Christian Rathgeb, An-titza Dantcheva, Naser Damer, and ChristophBusch. Demographic bias in biometrics: A sur-vey on an emerging challenge. IEEE Transac-tions on Technology and Society, 1(2):89–103,2020. doi: 10.1109/TTS.2020.2992344.Chris Dulhanty and Alexander Wong. Au-diting imagenet: Towards a model-drivenframework for annotating demographic at-tributes of large-scale image datasets. ArXiv,abs/1905.01347, 2019.Eran Eidinger, Roee Enbar, and Tal Hassner.Age and gender estimation of unfiltered faces.Information Forensics and Security,IEEETransactions on, 9:2170–2179, 12 2014. doi:10.1109/TIFS.2014.2359646.ofFraming:fracturedRobert M. Entman.aof Communication,Towardparadigm.43(4):51–58,doi: https://doi.org/10.1111/j.1460-https:clarificationJournal1993.2466.1993.tb01304.x.//onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-2466.1993.tb01304.x.URLTheodoros Evgeniou and Massimiliano Pontil.Regularized multi–task learning. In Proceed-ings of the Tenth ACM SIGKDD InternationalConference on Knowledge Discovery and DataMining, KDD ’04, page 109–117, New York,NY, USA, 2004. Association for ComputingMachinery. ISBN 1581138881. doi: 10.1145/1014052.1014067. URL https://doi.org/10.1145/1014052.1014067.Timnit Gebru, Jamie H. Morgenstern, Bri-ana Vecchione, Jennifer Wortman Vaughan,H. Wallach, Hal Daum´e, and Kate Craw-ford.ArXiv,abs/1803.09010, 2018.for datasets.DatasheetsMarkos Georgopoulos, Yannis Panagakis, andMaja Pantic. Investigating bias in deep faceanalysis: The kanface dataset and empiricalstudy.Image and Vision Computing, 102:ISSN 0262-8856. doi: https:103954, 2020.//doi.org/10.1016/j.imavis.2020.103954. URLhttps://www.sciencedirect.com/science/article/pii/S026288562030086X.Jake Goldenfein. The profiling potential ofcomputer vision and the challenge of com-putational empiricism.In Proceedings ofthe Conference on Fairness, Accountability,and Transparency, FAT* ’19, page 110–119,New York, NY, USA, 2019. Association forComputing Machinery. ISBN 9781450361255.doi: 10.1145/3287560.3287568. URL https://doi.org/10.1145/3287560.3287568.Ian J. Goodfellow, Jean Pouget-Abadie, MehdiMirza, Bing Xu, David Warde-Farley, SherjilOzair, Aaron Courville, and Yoshua Bengio.Generative adversarial nets.In Proceedingsof the 27th International Conference on Neu-ral Information Processing Systems - Volume2, NIPS’14, page 2672–2680, Cambridge, MA,USA, 2014. MIT Press.Alex Hanna, Emily Denton, Andrew Smart,and Jamila Smith-Loud. Towards a criticalrace methodology in algorithmic fairness. InProceedings of the 2020 Conference on Fair-ness, Accountability, and Transparency, FAT*’20, page 501–512, New York, NY, USA,2020. Association for Computing Machinery.ISBN 9781450369367. doi: 10.1145/3351095.303372826. URL https://doi.org/10.1145/3351095.3372826.Caner Hazirbas, Joanna Bitton, Brian Dolhan-sky, Jacqueline Pan, Albert Gordo, and Cris-tian Canton-Ferrer. Towards measuring fair-ness in ai: the casual conversations dataset.ArXiv, abs/2104.02821, 2021.Miguel A. Hern´an and Jamie M. Robins. CausalInference: What If. Chapman & Hall/CRC,2020.Chealse A. Heuer, Kimberly J. McClure, andRebecca M. Puhl. Obesity Stigma in OnlineNews: a Visual Content Analysis. J HealthCommun, 16(9):976–987, Oct 2011.Kashmir Hill. Wrongfully accused by anThe NY Times, June 2020.https://www.nytimes.com/2020/algorithm.URL06/24/technology/facial-recognition-arrest.html.Xiao Hu, Haobo Wang, Anirudh Vegesana,Somesh Dube, Kaiwen Yu, Gore Kao, Shuo-Han Chen, Yung-Hsiang Lu, George K. Thiru-vathukal, and Ming Yin. Crowdsourcing de-tection of sampling biases in image datasets.In Proceedings of The Web Conference 2020,WWW ’20, page 2955–2961, New York, NY,USA, 2020. Association for Computing Ma-chinery. ISBN 9781450370233. doi: 10.1145/3366423.3380063. URL https://doi.org/10.1145/3366423.3380063.Gary B. Huang, Marwan Mattar, Tamara Berg,and Eric Learned-Miller. Labeled Faces inthe Wild: A Database for Studying FaceRecognition in Unconstrained Environments.In Workshop on Faces in ’Real-Life’ Images:Detection, Alignment, and Recognition, Mar-seille, France, October 2008. Erik Learned-Miller and Andras Ferencz and Fr´ed´eric Ju-rie.URL https://hal.inria.fr/inria-00321923.ACM Conference on Fairness, Accountability,and Transparency, FAccT ’21, page 375–385,New York, NY, USA, 2021. Association forComputing Machinery. ISBN 9781450383097.doi: 10.1145/3442188.3445901. URL https://doi.org/10.1145/3442188.3445901.Jameela Jamil. Viewpoint: Jameela jamil onwhy airbrushing should be illegal. BBC, De-cember 2018. URL https://www.bbc.com/news/world-46349307.Ji Yoon Jang, Sangyoon Lee, and ByungjooLee. Quantification of gender representationbias in commercial films based on image anal-ysis. Proc. ACM Hum.-Comput. Interact.,3(CSCW), November 2019.doi: 10.1145/3359300. URL https://doi.org/10.1145/3359300.Heinrich Jiang and Ofir Nachum. Identifying andcorrecting label bias in machine learning. InAISTATS, 2020.Justin Johnson, Ranjay Krishn, Micheal Stark,Li-Jia Li, Davod A. Shamma, Micheal S. Bern-Image retrieval us-stein, and Li Fei-Fei.ing scene graphs.In 2015 IEEE Confer-ence on Computer Vision and Pattern Recog-nition (CVPR), pages 3668–3678, 2015. doi:10.1109/CVPR.2015.7298990.Fariba Karimi, Claudia Wagner, FlorianLemmerich, Mohsen Jadidi, and MarkusStrohmaier.Inferring gender from nameson the web: A comparative evaluation ofgender detection methods.In Proceedingsof the 25th International Conference Com-panion on World Wide Web, WWW ’16Companion, page 53–54, Republic and Can-ton of Geneva, CHE, 2016.InternationalWorld Wide Web Conferences SteeringCommittee.doi:10.1145/2872518.2889385.URL https://doi.org/10.1145/2872518.2889385.ISBN 9781450341448.Abigail Z. Jacobs and Hanna Wallach. Measure-ment and fairness. In Proceedings of the 2021Kimmo K¨arkk¨ainen and Jungseock Joo. Fair-face: Face attribute dataset for balanced race,31gender, and age for bias measurement andmitigation. In Proceedings of the IEEE/CVFWinter Conference on Applications of Com-puter Vision (WACV), pages 1548–1558, 2021.Atoosa Kasirzadeh and Andrew Smart. The useand misuse of counterfactuals in ethical ma-In Proceedings of the 2021chine learning.ACM Conference on Fairness, Accountability,and Transparency, FAccT ’21, page 228–236,New York, NY, USA, 2021. Association forComputing Machinery. ISBN 9781450383097.doi: 10.1145/3442188.3445886. URL https://doi.org/10.1145/3442188.3445886.Matthew Kay, Cynthia Matuszek, and Sean A.Munson. Unequal representation and genderstereotypes in image search results for occupa-tions. In Bo Begole, Jinwoo Kim, Kori Inkpen,and Woontack Woo, editors, Proceedings of the33rd Annual ACM Conference on Human Fac-tors in Computing Systems, CHI 2015, Seoul,Republic of Korea, April 18-23, 2015, pages3819–3828. ACM, 2015. doi: 10.1145/2702123.2702520. URL https://doi.org/10.1145/2702123.2702520.In Proceedings ofAditya Khosla, Tinghui Zhou, Tomasz Mal-isiewicz, Alexei A. Efros, and Antonio Tor-Undoing the damage of datasetralba.bias.the 12th Euro-pean Conference on Computer Vision - Vol-ume Part I, ECCV’12, page 158–171, Berlin,ISBNHeidelberg, 2012. Springer-Verlag.9783642337178.10.1007/978-3-642-33718-5 12.URL https://doi.org/10.1007/978-3-642-33718-5_12.doi:Byungju Kim, Hyunwoo Kim, Kyungsu Kim,Sungjin Kim, and Junmo Kim.Learningnot to learn: Training deep neural networksIn The IEEE Conferencewith biased data.on Computer Vision and Pattern Recognition(CVPR), June 2019.Barbara Kitchenham. Procedures for performingsystematic reviews. Keele, UK, Keele Univer-sity, 33(2004):1–26, 2004.Brendan Klare, Mark James Burge, Joshua C.Klontz, Richard W. Vorder Bruegge, andFace recognition perfor-Anil K. Jain.mance: Role of demographic information.Inf. Forensics Secur., 7(6):IEEE Trans.1789–1801, 2012.doi: 10.1109/TIFS.2012.2214212. URL https://doi.org/10.1109/TIFS.2012.2214212.Brendan F. Klare, Ben Klein, Emma Taborsky,Austin Blanton, Jordan Cheney, KristenAllen, Patrick Grother, Alan Mah, MarkBurge, and Anil J. Jain. Pushing the frontiersof unconstrained face detection and recogni-tion: Iarpa janus benchmark a. In 2015 IEEEConference on Computer Vision and PatternRecognition (CVPR), pages 1931–1939, 2015.doi: 10.1109/CVPR.2015.7298803.Jon M. Kleinberg, Sendhil Mullainathan, andManish Raghavan. Inherent trade-offs in thefair determination of risk scores.In Chris-tos H. Papadimitriou, editor, 8th Innovationsin Theoretical Computer Science Conference,ITCS 2017, January 9-11, 2017, Berkeley,CA, USA, volume 67 of LIPIcs, pages 43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum f¨urInformatik, 2017. doi: 10.4230/LIPIcs.ITCS.2017.43. URL https://doi.org/10.4230/LIPIcs.ITCS.2017.43.Anders Kofod-Petersen. How to do a structuredliterature review in computer science. Ver. 0.1.October, 1, 2012.Emmanouil Krasanakis, Symeon Papadopoulos,and Ioannis Kompatsiaris. Applying fairnessconstraints on graph node ranks under per-sonalization bias. In International Conferenceon Complex Networks and Their Applications,pages 610–622. Springer, 2020.Ivan Krasin, Tom Duerig, Neil Alldrin, AndreasVeit, Sami Abu-El-Haija, Serge Belongie,David Cai, Zheyun Feng, Vittorio Ferrari,Victor Gomes, Abhinav Gupta, DhyaneshNarayanan, Chen Sun, Gal Chechik, andKevin Murphy. Openimages: A public dataset32for large-scale multi-label and multi-class im-age classification.Dataset available fromhttps://github.com/openimages, 2016.of photothe boundariesDer SpiegelURLMatthias Krug and Stefan Niggemeier. Ex-edit-International, Mayhttps://www.spiegel.ploringing.2013.de/international/world/growing-concern-that-news-photos-are-being-excessively-manipulated-a-898509.html.Neeraj Kumar, Alexander C. Berg, Peter N. Bel-humeur, and Shree K. Nayar. Attribute andsimile classifiers for face verification. In IEEE12th International Conference on ComputerVision, ICCV 2009, Kyoto, Japan, September27 - October 4, 2009, pages 365–372. IEEEComputer Society, 2009. doi: 10.1109/ICCV.2009.5459250. URL https://doi.org/10.1109/ICCV.2009.5459250.Yann LeCun, Yoshua Bengio, and Geoffrey E.Hinton. Deep learning. Nat., 521(7553):436–444, 2015. doi: 10.1038/nature14539. URLhttps://doi.org/10.1038/nature14539.Li Fei-Fei, Rob Fergus, and Pietro Perona.Learning generative visual models from fewtraining examples: An incremental bayesianapproach tested on 101 object categories. In2004 Conference on Computer Vision andPattern Recognition Workshop, pages 178–178, 2004. doi: 10.1109/CVPR.2004.383.Lingyu Liang, Luojun Lin, Lianwen Jin, DuoruiXie, and Men Li. Scut-fbp5500: A diversebenchmark dataset for multi-paradigm facialbeauty prediction. In 2018 24th InternationalConference on Pattern Recognition (ICPR),pages 1598–1603, 2018. doi: 10.1109/ICPR.2018.8546038.Tsung-Yi Lin, M. Maire, Serge J. Belongie,James Hays, P. Perona, D. Ramanan, PiotrDoll´ar, and C. L. Zitnick. Microsoft coco:Common objects in context. In ECCV, 2014.David Lopez-Paz, Krikamol Muandet, BernhardSch¨olkopf, and Ilya O. Tolstikhin. Towards aInlearning theory of cause-effect inference.Francis R. Bach and David M. Blei, editors,Proceedings of the 32nd International Con-ference on Machine Learning, ICML 2015,Lille, France, 6-11 July 2015, volume 37of JMLR Workshop and Conference Pro-ceedings, pages 1452–1461. JMLR.org, 2015.URL http://proceedings.mlr.press/v37/lopez-paz15.html.David Lopez-Paz, Robert Nishihara, SoumithChintala, Bernhard Sch¨olkopf, and L´eon Bot-tou. Discovering causal signals in images. In2017 IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 58–66, 2017. doi: 10.1109/CVPR.2017.14.Eric L´opez-L´opez, Xos´e M. Pardo, Car-andDataset bias ex-IET Bio-https:URLlos V. Regueiro, Roberto Iglesias,Fernando E. Casado.posed in face verification.metrics, 8(4):249–258, 2019.doi://doi.org/10.1049/iet-bmt.2018.5224.https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-bmt.2018.5224.Michele Merler, Nalini Ratha, Rogerio S. Feris,and John R. Smith. Diversity in Faces. arXiv,art. arXiv:1901.10436, 2019.Roberto Merli, Michele Preziosi, and AlessiaAcampora. How do scholars approach thecircular economy?a systematic literaturereview. Journal of Cleaner Production, 178:703 – 722, 2018.doi:https://doi.org/10.1016/j.jclepro.2017.12.112.URLhttp://www.sciencedirect.com/science/article/pii/S0959652617330718.ISSN 0959-6526.Milagros Miceli, Martin Schuessler, and TianlingYang. Between subjectivity and imposition:Power dynamics in data annotation for com-puter vision. Proc. ACM Hum.-Comput. In-teract., 4(CSCW2), October 2020. doi: 10.1145/3415186. URL https://doi.org/10.1145/3415186.33Milagros Miceli, Tianling Yang, Laurens Naudts,Martin Schuessler, Dian Serbanescu, andAlex Hanna. Documenting computer visiondatasets: An invitation to reflexive data prac-tices. In Proceedings of the 2021 ACM Confer-ence on Fairness, Accountability, and Trans-parency, FAccT ’21, page 161–172, New York,NY, USA, 2021. Association for ComputingMachinery.ISBN 9781450383097. doi: 10.1145/3442188.3445880. URL https://doi.org/10.1145/3442188.3445880.Yisroel Mirsky and Wenke Lee. The creation anddetection of deepfakes: A survey. ACM Com-put. Surv., 54(1), January 2021. ISSN 0360-0300. doi: 10.1145/3425780. URL https://doi.org/10.1145/3425780.Ian Model and Lior Shamir. Comparison ofdata set bias in object recognition bench-marks. IEEE Access, 3:1953–1962, 2015. doi:10.1109/ACCESS.2015.2491921.Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju,Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore Ruggieri, FrancoTurini, Symeon Papadopoulos, EmmanouilKrasanakis, Ioannis Kompatsiaris, KatharinaKinder-Kurlanda, Claudia Wagner, FaribaKarimi, Miriam Fern´andez, Harith Alani, Bet-tina Berendt, Tina Kruegel, Christian Heinze,Klaus Broelemann, Gjergji Kasneci, Thanas-sis Tiropanis, and Steffen Staab. Bias in data-driven artificial intelligence systems - an in-troductory survey. Wiley Interdiscip. Rev.Data Min. Knowl. Discov., 10(3), 2020. doi:10.1002/widm.1356. URL https://doi.org/10.1002/widm.1356.Rameswar Panda, Jianming Zhang, Haoxiang Li,Joon-Young Lee, Xin Lu, and Ammit K. Roy-Chowdhury. Contemplating visual emotions:Understanding and overcoming dataset bias.In ECCV, 2018.Danielle Paquette.Thetruthwomen,aboutinhow wesimpleoneuncomfortableview workingsearch.googleThe Washington Post, April 2015. URLhttps://www.washingtonpost.com/news/wonk/wp/2015/04/14/what-one-simple-google-search-tells-us-about-how-we-view-working-women/.Yilang Peng. Same candidates, different faces:Uncovering media bias in visual portrayals ofpresidential candidates with computer vision.Journal of Communication, 68, 10 2018. doi:10.1093/joc/jqy041.Evaggelia Pitoura, Kostas Stefanidis, and Geor-gia Koutrika. Fairness in rankings and recom-menders: Models, methods and research direc-tions. In 37th IEEE International Conferenceon Data Engineering, ICDE 2021, Chania,Greece, April 19-22, 2021, pages 2358–2361.IEEE, 2021. doi: 10.1109/ICDE51399.2021.00265.URL https://doi.org/10.1109/ICDE51399.2021.00265.Vinay Uday Prabhu and Abeba Birhane. Largeimage datasets: A pyrrhic win for computervision? ArXiv, abs/2006.16923, 2020.Joseph P. Robinson, Gennady Livitz, YannHenon, Can Qin, Yun Fu, and Samson Ti-moner. Face recognition: Too bias, or notIn Proceedings of the IEEE/CVFtoo bias?Conference on Computer Vision and PatternRecognition (CVPR) Workshops, 2020.Olga Russakovsky, Jia Deng, Hao Su, JonathanKrause, Sanjeev Satheesh, Sean Ma, Zhi-heng Huang, Andrej Karpathy, Aditya Khosla,Micheal Bernstein, Alexander C. Berg, andLi Fei-Fei. Imagenet large scale visual recog-nition challenge. Int. J. Comput. Vision, 115(3):211–252, 2015.doi:10.1007/s11263-015-0816-y. URL https://doi.org/10.1007/s11263-015-0816-y.ISSN 0920-5691.Ruslan Salakhutdinov, Antonio Torralba, andJoshua B. Tenenbaum. Learning to sharevisual appearance for multiclass object de-In The 24th IEEE Conference ontection.Computer Vision and Pattern Recognition,34CVPR 2011, Colorado Springs, CO, USA, 20-25 June 2011, pages 1481–1488. IEEE Com-puter Society, 2011. doi: 10.1109/CVPR.2011.5995720. URL https://doi.org/10.1109/CVPR.2011.5995720.Shreya Shankar, Yoni Halpern, Eric Breck,James Atwood, Jimbo Wilson, and D. Sculley.No classification without representation: As-sessing geodiversity issues in open data sets forthe developing world. In NIPS 2017 workshop:Machine Learning for the Developing World,2017.Krishna Kumar Singh, Dhruv Mahajan, KristenGrauman, Yong Jae Lee, Matt Feiszli, andDeepti Ghadiyaram. Don’t judge an objectby its context: Learning to overcome contex-tual bias.In Proceedings of the IEEE/CVFConference on Computer Vision and PatternRecognition (CVPR), 2020.Ryan Steed and Aylin Caliskan.Image rep-resentations learned with unsupervised pre-training contain human-like biases.In Pro-ceedings of the 2021 ACM Conference on Fair-ness, Accountability, and Transparency, page701–713, New York, NY, USA, 2021. As-sociation for Computing Machinery.ISBN9781450383097. URL https://doi.org/10.1145/3442188.3445932.Latanya Sweeney. Discrimination in online addelivery. Commun. ACM, 56(5):44–54, May2013. ISSN 0001-0782. doi: 10.1145/2447976.2447990. URL https://doi.org/10.1145/2447976.2447990.Philipp Terh¨orst, Daniel F¨ahrmann, Jan NiklasKolf, Naser Damer, Florian Kirchbuchner, andArjan Kuijper. Maad-face: A massively anno-tated attribute dataset for face images. ArXiv,abs/2012.01030, 2021.Philipp Terh¨orst, Jan Niklas Kolf, Marco Hu-ber, Florian Kirchbuchner, Naser Damer,Aythami Morales, Julian Fi´errez, and ArjanKuijper. A comprehensive study on face recog-nition biases beyond demographics. ArXiv,abs/2103.01592, 2021. URL https://arxiv.org/abs/2103.01592.Christopher Thomas and Adriana Kovashka.Predicting the politics of an image using weblysupervised data. In NeurIPS, 2019.Tatiana Tommasi, Novi Patricia, Barbara Ca-puto, and Tinne Tuytelaars. A deeper lookat dataset bias.In Juergen Gall, Peter V.Gehler, and Bastian Leibe, editors, Pat-tern Recognition - 37th German Conference,GCPR 2015, Aachen, Germany, October 7-10, 2015, Proceedings, volume 9358 of Lec-ture Notes in Computer Science, pages 504–516. Springer, 2015.10.1007/978-3-319-24947-6\ 42. URL https://doi.org/10.1007/978-3-319-24947-6_42.doi:Antonio Torralba and Alexei A. Efros. Un-In The 24thbiased look at dataset bias.IEEE Conference on Computer Vision andPattern Recognition, CVPR 2011, ColoradoSprings, CO, USA, 20-25 June 2011, pages1521–1528. IEEE Computer Society, 2011. doi:10.1109/CVPR.2011.5995347. URL https://doi.org/10.1109/CVPR.2011.5995347.Sahil Verma and Julia Rubin. Fairness defini-tions explained. In Proceedings of the Interna-tional Workshop on Software Fairness, Fair-Ware ’18, page 1–7, New York, NY, USA,2018. Association for Computing Machinery.ISBN 9781450357463. doi: 10.1145/3194770.3194776. URL https://doi.org/10.1145/3194770.3194776.Christian Wachinger,in multi-siteSebastian P¨olsterl.Anna Rieckmann,andDetectandneuroimag-correctbiasImage Analysis,ing datasets.67:101879, 2021.doi:https://doi.org/10.1016/j.media.2020.101879.URLhttps://www.sciencedirect.com/science/article/pii/S1361841520302437.ISSN 1361-8415.MedicalAngelina Wang, Arvind Narayanan, and OlgaRussakovsky. REVISE: A tool for measur-ing and mitigating bias in visual datasets.35European Conference on Computer Vision(ECCV), 2020.Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kaiwei Chang, and Vincente Ordonez. Balanceddatasets are not enough: Estimating and mit-igating gender bias in deep image representa-tions. In 2019 IEEE/CVF International Con-ference on Computer Vision (ICCV), pages5309–5318, 2019. doi: 10.1109/ICCV.2019.00541.Benjamin Wilson, Judy Hoffman, and JamieMorgenstern. Predictive inequity in object de-tection. CoRR, abs/1902.11097, 2019. URLhttp://arxiv.org/abs/1902.11097.Wenying Wu, Pavlos Protopapas, Xheng Yang,and Panagiotis Michalatos. Gender classifica-tion and bias mitigation in facial images. In12th ACM Conference on Web Science, Web-Sci ’20, page 106–114, New York, NY, USA,2020. Association for Computing Machinery.ISBN 9781450379892. doi: 10.1145/3394231.3397900. URL https://doi.org/10.1145/3394231.3397900.Kaiyu Yang, Klint Qinami, Li Fei-Fei, JiaDeng, and Olga Russakovsky. Towards fairerdatasets: Filtering and balancing the distri-bution of the people subtree in the imagenethierarchy. In Proceedings of the 2020 Confer-ence on Fairness, Accountability, and Trans-parency, FAT* ’20, page 547–558, New York,NY, USA, 2020. Association for ComputingMachinery.ISBN 9781450369367. doi: 10.1145/3351095.3375709. URL https://doi.org/10.1145/3351095.3375709.Quanzeng You, Jiebo Luo, Hailin Jin, and Jian-chao Yang. Building a large scale dataset forimage emotion recognition: The fine print andIn Proceedings of the Thir-the benchmark.tieth AAAI Conference on Artificial Intelli-gence, AAAI’16, page 308–314. AAAI Press,2016.Fisher Yu, Haofeng Chen, Xin Wang, WenqiXian, Yingying Chen, Fangchen Liu, VashishtMadhavan, and Trevor Darrell. BDD100K: Adiverse driving dataset for heterogeneous mul-titask learning. In 2020 IEEE/CVF Confer-ence on Computer Vision and Pattern Recog-nition (CVPR), pages 2633–2642, 2020. doi:10.1109/CVPR42600.2020.00271.Yongshun Zhang, Xiu-Shen Wei, Boyan Zhou,and Jianxin Wu. Bag of tricks for long-tailed visual recognition with deep convo-lutional neural networks.In Thirty-FifthAAAI Conference on Artificial Intelligence,AAAI 2021, Thirty-Third Conference on In-novative Applications of ArtificialIntelli-gence, IAAI 2021, The Eleventh Symposiumon Educational Advances in Artificial Intel-ligence, EAAI 2021, Virtual Event, Febru-ary 2-9, 2021, pages 3447–3455. AAAI Press,2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16458.Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vin-cente Ordonez, and Kai-Wei Chang. Menalso like shopping: Reducing gender bias am-Inplification using corpus-level constraints.Proceedings of the 2017 Conference on Em-pirical Methods in Natural Language Pro-cessing, pages 2979–2989, Copenhagen, Den-mark, September 2017. Association for Com-putational Linguistics.10.18653/v1/D17-1323. URL https://www.aclweb.org/anthology/D17-1323.doi:Bolei Zhou, Agata Lapedriza, Aditya Khosla,Aude Oliva, and Antonio Torralba. Places:A 10 million image database for scene recog-nition. IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 40(6):1452–1464,2018. doi: 10.1109/TPAMI.2017.2723009.Xiangxin Zhu, Dragomir Anguelov, and DevaRamanan. Capturing long-tail distributions ofobject subcategories.In 2014 IEEE Confer-ence on Computer Vision and Pattern Recog-nition, CVPR 2014, Columbus, OH, USA,June 23-28, 2014, pages 915–922. IEEE Com-puter Society, 2014. doi: 10.1109/CVPR.2014.36122. URL https://doi.org/10.1109/CVPR.2014.122.37