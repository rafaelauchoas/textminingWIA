Journal Pre-proofsFilterK: a new outlier detection method for k-means clustering of physical ac-tivityPetra J. Jones, Matthew K. James, Melanie J. Davies, Kamlesh Khunti, MikeCatt, Tom Yates, Alex V. Rowlands, Evgeny M. MirkesPII:DOI:Reference:S1532-0464(20)30024-1https://doi.org/10.1016/j.jbi.2020.103397YJBIN 103397To appear in:Journal of Biomedical InformaticsReceived Date:Revised Date:Accepted Date:11 October 20193 February 202024 February 2020Please cite this article as: Jones, P.J., James, M.K., Davies, M.J., Khunti, K., Catt, M., Yates, T., Rowlands, A.V.,Mirkes, E.M., FilterK: a new outlier detection method for k-means clustering of physical activity, Journal ofBiomedical Informatics (2020), doi: https://doi.org/10.1016/j.jbi.2020.103397This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a coverpage and metadata, and formatting for readability, but it is not yet the definitive version of record. This versionwill undergo additional copyediting, typesetting and review before it is published in its final form, but we areproviding this version to give early visibility of the article. Please note that, during the production process, errorsmay be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.© 2020 Published by Elsevier Inc.FilterK: a new outlier detection method for k-means clustering of physical activityPetra J Jonesab, Matthew K James c, Melanie J Davies abd, Kamlesh Khunti ad, Mike Catte, Tom Yatesbd, Alex V Rowlandsbdf and Evgeny M Mirkesga Leicester Diabetes Centre, University Hospitals of Leicester, Leicester General Hospital, Gwendolen Road, Leicester, LE5 4PW, UKbDiabetes Research Centre, University of Leicester, Leicester General Hospital, Gwendolen Road, Leicester, LE5 4PW, UKcSchool of Physics and Astronomy, University of Leicester, University Road, Leicester, LE1 7RH, UK dNIHR Leicester Biomedical Research Centre, Leicester General Hospital, Gwendolen Road, Leicester, LE5 4PW, UK eInstitute of Neuroscience, Henry Wellcome Building, Faculty of Medical Sciences, Newcastle University, Newcastle upon Tyne, NE2 4HH, UK fAlliance for Research in Exercise, Nutrition and Activity (ARENA), Sansom Institute for Health Research, Division of Health Sciences, University of South Australia, Adelaide, AustraliagSchool of Mathematics and Actuarial Science, University of Leicester, University Road, Leicester, LE1 7RD, UK AbstractIn this paper, a new algorithm denoted as FilterK is proposed for improving the purity of k-means derived physical activity clusters by reducing outlier influence. We applied it to physical activity data obtained with body-worn accelerometers and clustered using k-means. We compared its performance with three existing outlier detection methods: Local Outlier Factor, Isolation Forests and KNN using the ground truth (class labels), average cluster and event purity (ACEP). FilterK provided comparable gains in ACEP (0.5810.596 compared to 0.580-0.617) whilst removing a lower number of outliers than the other methods (4% total dataset size vs 10% to achieve this ACEP). The main focus of our new outlier detection method is to improve the cluster purities of physical activity accelerometer data, but we also suggest it may be potentially applied to other types of dataset captured by k-means clustering. We demonstrate our method using a k-means model trained on two independent accelerometer datasets (training n=90) and re-applied to an independent dataset (test n=41). Labelled physical activities include lying down, sitting, standing, household chores, walking (laboratory and non-laboratory based), stairs and running. This type of clustering algorithm could be used to assist with identifying optimal physical activity patterns for health. Corresponding author. e-mail: pj100@leicester.ac.uk11. Introduction Classifying the type and/or intensity of physical activity in data gathered by accelerometer devices can provide important information about optimal lifestyle and physical activity patterns for the prevention and management of chronic disease such as diabetes [1]. Furthermore, accurate classification and quantification of activity assists with emerging precision medicine development, and the refinement of programmes for improving health e.g. maintaining risk factor control [2,3,4].There are natural clusters of human movement which can be considered universal (e.g. walking, running) or very common (e.g. household tasks and activities) [5]. To identify these clusters, we can use unsupervised machine learning methods to cluster data from body-worn accelerometers using models such as k-means. K-means was chosen as our clustering model given its frequent past use in clustering physical activity [6], [7] [8] [9], and its prior successful use in separating sedentary (lying down and seated 89%), ambulatory (walking and taking the stairs 78%) and vigorous activity (running 96%) [10].K-means models can be trained on multiple accelerometer datasets, stored and re-applied to independent datasets to give an indication of physical behaviours e.g. sedentary (lying or sitting-based tasks), ambulatory and vigorous physical activities, provided there is some underlying data harmony (e.g. equivalence of measurement devices and the location of those devices on the body as here). These methods have potential for identifying optimal physical activity patterns for health. However, in our experience, cluster purities can be low, particularly in mixed activity datasets including household and slow ambulatory activities, or when applying clustering models to datasets containing variable demographics [10].Supervised machine learning methods can classify physical activity type and intensity from accelerometer data, but these models are not always transferable to new datasets, experiencing great variability of performance [11]. They are also reliant on labelled data being available, i.e. where the activity of the person is recorded, which can be both time-consuming and costly to acquire. In this study we used labelled data to evaluate the accuracy of method but it is necessary to stress that labelled data are unnecessary either for k-means clustering or the FilterK outlier detection algorithm. The ultimate goal is to cluster unlabeled physical activity by type and/or intensity efficiently with high cluster purity so that physical activity beneficial to health can be quantified in relation to the prevention and progression of chronic diseases. This also is a challenging problem given the differing demographics of populations monitored using body-worn accelerometers; there will be no neat mathematical solution for clustering such data. Certainly as Pliakos and Vens suggest, peculiarities of the data in healthcare and biomedicine can lead to non-variant data representations so that methodologies to handle the uniqueness of biomedical data become necessary [12]. Our goal is to 2 offer a new outlier detection method for use with k-means, with the primary aim of extracting the most universal clusters of accelerometer data collected from three different populations and to maximise both the quality and quantity of physical activity information we capture, by minimising outliers.2. Related work Improving outlier detection in accelerometer studies of physical activity remains a neglected area. Most studies concern outliers within physical activity in other contexts such as fall detection among older people [13], ambulatory care for those with dementia [14], or anomalies in gait, perhaps to trigger device locks. [15] There are only a few studies specifically focused more generally on outlier detection in wearable sensor data for human activity recognition. Typically the numbers involve relatively few types of activity or small numbers of participants from each dataset [16, 17]. For example, Munoz-Organero’s outlier detection method [16] is applied to a dataset of fifteen individuals’ (eight male, seven female) accelerometer data from a waist-worn device. Their model, based on a deep recurrent neural network, aimed to locate fragments of raw signal which denote a very specific type of outlier involving a cessation of movement (e.g. climbing upstairs or downstairs, jogging, walking), a turn-around or significant decrease in speed. It is not a method intended to find more general forms of outlier in physical activity. It also runs contrary to our main objective of clustering free living data, which will certainly include changes in pace, stops, turn-arounds and changes of direction which we do not wish to exclude from ambulatory clusters. Further, Munoz-Organero’s method is evaluated solely by reference to the ground truth of labelled four classes of physical activity and the proportion of designated outliers found. The model’s performance is not compared against other forms of outlier detection. Similarly, Said Abdallah’s model [17] which proposes a supervised machine learning algorithm using within cluster standard deviation, and distance to centroid to agglomeratively combine micro-clusters requires true labelled clusters instances in its early phase and user input in instances of micro clusters with equal votes for belonging to two clusters. Both the two accelerometer datasets in the study were small, consisting of just five and four individuals respectively with four and ten classes of physical activity [17]. Evaluation was limited to the ground truth proportion of class instances correctly classified or misclassification without further comparison with general purpose outlier detection methods. Where general purpose outlier detection methods such as local outlier factor (LOF) (discussed further in more detail under Methods) have been applied to k-means clustering of physical activity, it has been to identify outliers within a specific individual’s data. For example, to build a personalized classifier of light, moderate and vigorous intensity activities and to identify outliers such as falls [8] rather than removing outliers from the physical activity of a large group. LOF is often used for outlier 3detection in k-means clustering more generally [18]. Similarly, the equally popular outlier detection method, Isolation Forests has been used in daily physical activity analysis but again in very specific applications such as urinary tract infection detection in people with dementia [19]. There are a wide range of general-purpose outlier detection methods in existence, most focusing solely on an aspect of the distance or density of a point’s neighbours. We sought a model which explicitly took the clustering found by k-means into account when assessing outliers within the high dimensional space of our accelerometer data. Barai and Dey’s general method (2017) [20] of detecting outliers in k-means is based on calculating the maximum and minimum value of pairwise distances for all observations, using the sum of these two values as a threshold to detect outliers. High-dimensional datasets like accelerometer data increase the propensity for pairwise distances to become more similar [21] so that an ensemble method seems a more appropriate response [22]. Their alternative cluster-based approach simply assumes the smallest cluster is likely to contain a high proportion of outliers, but doesn’t attempt to discern between these and genuine members. One reason for the scarcity of literature analyzing alternate approaches to outlier detection in physical activity, may be the use of machine learning algorithms such as DBSCAN, [23] with its in-built outlier detection where points whose neighbourhood lies below a threshold density are treated as noise. But here too there are only a few applications of DBSCAN to accelerometer data [24, 25], perhaps given a number of limitations: arbitrary assignment of a border object located between two adjacent clusters [26], the difficulty of identifying outliers amid multiple density clusters and no means to gauge the degree or extent of their outlierness. Accelerometer data contains the mixed physical activity of numerous participants and as such, represents a unique challenge for detecting outliers given the great diversity in participant’s gait, movement and so on. This is likely to become an area of increasing importance - in recent years, accelerometer clustering has become the subject of intense interest given both the need to analyse health outcomes across multiple large physical activity datasets [27] and the scarcity of labelled datasets, particularly free living data, and the time and expense involved in their acquisition. 3. 3.1.MethodsData Acquisition Our data were gathered from three independent studies consisting of a training set comprised from two datasets (training sample 1: adults aged 40-65 years; training sample 2: adults aged 20-40 years; test set: children aged 9-14 years). These two training datasets were combined to develop the model, which was tested on the independent testing dataset. The accelerometer data were acquired at a sampling frequency of between 80-100 Hz for three axes using the GENEA or the GENEActiv accelerometer. See Table 1 for details of each sample. The GENEA sensor is the non-commercial 4precursor to the GENEActiv sensor. Both sensors were developed by Unilever Discover (Colworth, United Kingdom) and are triaxial MEMS sensors which provide similar raw acceleration data [28]. The GENEActiv is manufactured and distributed by ActivInsights Ltd (Kimbolton, Cambridgeshire, UK: https://www.activinsights.com/products/geneactiv/). Both are triaxial acceleration sensors housed in a small lightweight casing. TABLE 1Dataset Overview DatasetSampling Rate (Hz)MonitorN Age Range (y)Male / FemaleHeight(cm, mean (SD))Train 180GENEA 6040-6523 / 37Train 2100GENEActiv3020-408 / 22Test80GENEA 419-1417 / 24N = No. of participants. Handedness. L = left. R = right. A = ambidextrous176.2 (6.2)169.4 (0.09)150.2(13.3)Mass(kg, mean (SD))80.6 (11.6)69.2(15.3)43.0(11.2)Handedness Monitor location(s)55R, 5LBoth wrists27R, 3LNon-dominant37R, 2L, 2A Both wristsEach participant within training sample 1 completed an ordered series of 10-12 semi-structured activities [29]. The activities included lying, standing still, seated computer work, treadmill walking (4 km/h, 5 km/h, 6 km/h), outdoor walking (6 km/h), walking up and downstairs, two household activities (from window washing, washing up, shelf stacking, and sweeping), one treadmill run (8 km/h, 10 km/h, 12 km/h), and an optional outdoor run (10 km/h). The lying activity was performed for 10 minutes, whereas all other activities were performed for around 4.5 minutes. Each participant within training sample 2 completed a series of activities, including lying in various positions, sitting again in various positions with and without upper body movement (computer, and mobile phone games), household activities (washing up, cleaning/dusting, or sweeping) and finally, a self-paced free living walk. All activities/postures were five minutes in duration [30]. Training samples 1 and 2 were combined together (hereafter the “training” dataset). Finally, the test set consisted of children carrying out four activities including lying, seated DVD watching, treadmill walking (4 km/h and 6 km/h) and treadmill running. Lying was performed for ten minutes, with three minutes for the other activities. See Phillips et al for full details [31]. To evaluate the quality of the model we used a dataset of labelled physical activities which we could later evaluate relative to the ground truth. We began by annotating our time series data with activity labels and discarded thirty seconds of data from the beginning and end of each labelled activity to eliminate transitional data. Acceleration features were extracted from a 10 second sliding window, (based on median window length from previous studies, see Supplemental Table 1) with no overlap. Two frequency domain features were extracted, specifically dominant frequency and power of 5dominant frequency based on a single-dimensional signal magnitude vector (SVM = max{0, 𝑋2 + 𝑌2 + 𝑍2 ―1}) where negative values were replaced by zero also referred to as the resultant or ENMO (Euclidean Norm Minus One, [32]). Further time domain features were extracted including max, min, mean, median, 10th percentile, 75th percentile, 90th percentile, standard deviation and variance for each axis (feature selection based on previous studies, see Supplemental Table 2). In addition, mean, max and median for SVM and min, max, mean and median for each axis angle were extracted [10]. Pre-processing included Min-Max normalisation. 𝑥′ =𝑥 ― 𝑥𝑚𝑖𝑛,𝑥𝑚𝑎𝑥 ― 𝑥𝑚𝑖𝑛(1)Where the parameters 𝑥𝑚𝑖𝑛 and 𝑥𝑚𝑎𝑥 were defined for the training dataset, stored with the model and then applied to all datasets. 3.2.K-Means Clustering ApproachK-means [33] is a popular machine learning algorithm used for clustering data where the number of clusters (K) is either known, presumed or indicated beforehand (a number of techniques exist including Elbow Method [34], Silhouette Score [35] and Calinski-Harabasz index [36] to assess an optimal K) [37]. Here, we used the Nguyen et al. [38] approach of over-clustering where the number of clusters K (ten) is greater than the number of actual or expected classes (nine, i.e. the number of activity types in the training dataset). Based on the classes observed within each cluster, for evaluation of the model we joined the ten clusters sympathetically into four broader groups: sedentary, mixed, ambulatory and vigorous. The k-means algorithm was initialized ten times using K++ algorithm [39] intended to produce greater consistency of results by selecting the first centroid at random, then using a weighted probability score for the selection of subsequent centroids [40]. The data used for clustering was normalised using min-max normalization – both these steps were taken to minimise the risk of overfitting. The k-means algorithm alternates between two steps until convergence. The first step is the association of each data point x with the closest centroid 𝜇𝑖. Here the set of points in cluster i is denoted as :𝐶𝑖𝐶𝑖 = {𝑥:‖𝑥 ― 𝜇𝑖‖ ≤ ‖𝑥 ― 𝜇𝑗‖, ∀𝑖 ≠ 𝑗}.(2)The second step recalculates the centroids of each cluster to minimise the sum of squared Euclidean distances from the data points of this cluster to the cluster centroid, where |𝐶𝑖| is the number of points 𝑖in the  cluster.6𝜇𝑖 =1|𝐶𝑖|∑𝑥 ∈ 𝐶𝑖𝑥(3)This two-step algorithm minimises the sum of squared Euclidean distances from each data point to the nearest centroid. The fitted clustering model (including parameters of data normalisation for all features, the number of clusters, data normalisations for all features, and the centroids for each cluster) was stored using the Python pickle module and reapplied blind on an independent laboratory dataset. The k-means algorithm was carried out using the Sklearn library in Python [41]. This kind of analysis is valuable in automatically separating an accelerometer dataset into clusters of behaviours exhibiting similar movement patterns, facilitating greater understanding of why certain activities are grouping together through cluster analysis.3.1. Outlier Detection Methods for ComparisonWe will begin by describing our proposed algorithm, FilterK followed by the methodology of three other well-known outlier detection algorithms: Local Outlier Factor (LOF), Isolation Forests (IF) and K-Nearest Neighbours (KNN). 3.1.1FilterKOutlier detection techniques summarised by Xu et al., fall into a number of categories such as “distance based”, “density based” and “clustering based” [42]. Our approach combines a number of these approaches into a new outlier detection model, the rationale being that given the wide variety of age, height, mass and gait of those participating in physical activity, we need to assess outlierness from a number of perspectives. We call the proposed method FilterK. The model was implemented in Python and uses some of the SKlearn library’s functions, notably k-means. NotationLet 𝐷 be a 𝑑 dimensional dataset with 𝑛 observations: 𝑋 = (𝑋1, …, 𝑋𝑛), 𝑋𝑖 = (𝑥𝑖1,…,𝑥𝑖𝑑)Definition 1. (Number of Clusters) 𝑘 is the number of clusters for dataset 𝐷.Definition 2. (Clusters and Centroids) 𝜇𝑖 is the centroid of  cluster. The initial set of centroids can be 𝑖found, for example, by standard k-means. For the Euclidean distance we have 7𝜇𝑖 =1|𝐶𝑖|∑𝑗 ∈ 𝐶𝑖𝑋𝑗(4)where |𝐶𝑖| is the number of points in cluster 𝐶𝑖.𝜇(𝑋𝑖) is the nearest centroid to point 𝑋𝑖: 𝑑𝑘(𝑥) is distance to the 𝑘𝑥 nearest neighbour of point ;𝑟𝑖 is the radius of  cluster 𝑖𝑟𝑖 = max𝑗 ∈ 𝐶𝑖‖𝑋𝑗 ― 𝜇𝑖‖.Definition 3. (Neighbourhood)  is the radius that defines the boundary of a point’s neighbourhood 𝜀𝑁𝜀(𝑋) = {𝑗:‖𝑋𝑗 ― 𝑋‖ ≤ 𝜀} is the set of neighbours of point 𝑋. |𝑁𝜀(𝑋)| is the number of elements in the 𝜀 neighbourhood of point 𝑋Definition 4. (Minimum Points Threshold) 𝑁𝑚𝑖𝑛 is the minimal number of neighbours for a core point: point 𝑋 is a core point if |𝑁𝜀(𝑋)| ≥ 𝑁𝑚𝑖𝑛Definition 5. (Mean Neighbour Distance) The mean neighbour distance for point : 𝑋𝑑(𝑋) =1|𝑁𝑑𝑁𝑚𝑖𝑛(𝑋)| ∑𝑖 ∈ 𝑁𝑑𝑁𝑚𝑖𝑛(𝑋)‖𝑋𝑗 ― 𝑋‖.(5)Definition 6. (Core Points) 𝐶𝜀 is the set of core points: the set of non-outliers found within  neighbourhood that contain more than or equal to 𝑁𝑚𝑖𝑛 points.The first step is to normalise the training dataset, and to store this model for reuse on subsequent test datasets (see pseudocode below). In the first step, the FilterK algorithm automatically estimated  through the data and a specified value 𝜀of 𝑁𝑚𝑖𝑛. For each point 𝑋, 𝑑𝑁𝑚𝑖𝑛(𝑋) is the distance from the point 𝑋 𝑁𝑚𝑖𝑛 to ‘th neighbour. We use the mean value of the distance to the 𝑁𝑚𝑖𝑛‘th neighbour as the  radius: 𝜀𝑛∑𝑑𝑁𝑚𝑖𝑛(𝑋𝑖).(6)𝜀 =1𝑛𝑖 = 18 The value 𝑁𝑚𝑖𝑛0 will later be automatically calculated through 𝑁𝑚𝑖𝑛. As a result, the proposed algorithm itself has only one user defined argument: minimal number of neighbours for a core point 𝑁𝑚𝑖𝑛, although of course, it relies on K-means which necessitates a selection of K. The second step of the FilterK algorithm is the first outlier test: looking for points with a large mean neighbour distance. We have chosen the 90th percentile of the 𝑑(𝑋) set as the minimum neighbour distance threshold (MNDT) given its effectiveness in multivariate outlier detection in skew-normal distributions [43] but other thresholds may be used, and we evaluate percentiles ranging from 85-95 later in this paper (see Supplemental Table 3). The point 𝑋 is outlier suspicious ifThe third step of the FilterK algorithm is the second outlier test: points with a large distance to the 𝑑(𝑋) > MNDT.(7)nearest centroid (NCD). Let us calculate We select the 90th percentile of NCD(𝑋𝑖) as the threshold (NCDT). The point 𝑋 is outlier suspicious ifNCD(𝑋𝑖) = ‖𝑋𝑖 ― 𝜇(𝑋𝑖)‖.(8)NCD(𝑋) > NCDT.(9)The fourth step is the third outlier test: points with a very poor  neighbourhood. We define value 𝜀𝑁𝑚𝑖𝑛0 as the threshold of a “very poor” neighbourhood. The recommended value is Another reasonable choice of 𝑁𝑚𝑖𝑛0 is the 10th percentile of 𝑁𝜀(𝑋). In our models we used 𝑁𝑚𝑖𝑛0 =𝑁𝑚𝑖𝑛0 ≤ 𝑁𝑚𝑖𝑛/2(10)⌊𝑁𝑚𝑖𝑛/2⌋ ― 1 where ⌊𝑥⌋ is rounding down. The point is outlier suspicious if |𝑁𝜀(𝑋)| < 𝑁𝑚𝑖𝑛0(11)The fifth and final step of FilterK defines two important sets of points: Outliers 𝑂 are the points which hold all three outlier conditions: 𝑂 = {𝑖: 𝑑(𝑋𝑖) > MNDT ∧ NCD(𝑋𝑖) > NCDT ∧ |𝑁𝜀(𝑋)| < 𝑁𝑚𝑖𝑛0}(12)It is necessary to stress that FilterK searches for outliers by relying upon the existence of a set of clusters defined by centroids only. FilterK itself does not utilise object labelling (class labels) and cannot itself evaluate cluster or event purity.9Algorithm     Outlier Detection Inputs: Data table 𝐷, vectors of scaling ( ) and shifting ( ) parameters, centroids ( ), and 𝛽𝛼𝜇minimal number of points in a core point’s vicinity 𝑁𝑚𝑖𝑛normalise data table 𝐷 (via Algorithm 2)Criterion 1   for each point 𝑋 from 𝐷:retrieve 𝑁𝑚𝑖𝑛 nearest points 𝑋𝑗calculate mean distance 𝑑(𝑋) from point 𝑋 to these neighbours 𝑋𝑗end forcalculate threshold for mean distance for neighbours (MNDT) using suggested initial value of 90% percentile of 𝑑(𝑋)Criterion 2for each point 𝑋 from 𝐷:retrieve nearest centroid 𝜇(𝑋)calculate distance to nearest centroid: NCD(𝑋) = ‖𝑋 ― 𝜇(𝑋)‖end forcalculate threshold for nearest centroid distance (NCDT) using suggested initial value of 90% percentile of NCD(𝑋).Criterion 3for each point 𝑋 from 𝐷:𝑑𝑁𝑚𝑖𝑛(𝑋𝑖)calculate distance to the neighbour with number 𝑁𝑚𝑖𝑛calculate mean value of distance 𝑑𝑁𝑚𝑖𝑛(𝑋𝑖) and store as parameter  to estimate 𝜀densitycalculate number of neighbours |𝑁𝜀(𝑋)|𝜀 in the vicinity with radius .end fordefine threshold for density evaluation as 𝑁𝑚𝑖𝑛0 = ⌊𝑁𝑚𝑖𝑛/2⌋ ― 1Outliers Detection All points with 𝑑(𝑋) ≥ MNDT outliersOutput: List of outliers and NCD(𝑋) ≥ NCDT and |𝑁𝜀(𝑋)| ≤ 𝑁𝑚𝑖𝑛0 are Core points are the points which contain at least 𝑁𝑚𝑖𝑛 points within  neighbourhoods: 𝜀𝐶 = {𝑖: |𝑁𝜀(𝑋)| ≥ 𝑁𝑚𝑖𝑛}.(13)10It is necessary to stress that some of the points are not outliers but also are not core points, we term these border points. FilterK provides a simple way of searching for points which are (i) comparatively far away from their nearest centroid and (ii) whose nearest neighbours are also more distant than normal, where there are also (iii) few or no neighbours within neighbourhood (determined at the outset from the average distance to 𝑁𝑚𝑖𝑛 neighbours within the dataset), This combination of global (defining a neighbourhood by reference to the global average distance to 𝑁min neighbours) and local density and neighbour distances provides a powerful mechanism to identify outliers. We introduced three measures of outlierness: mean distance to neighbours |𝑁𝜀(𝑋)|and number of neighbours in the neighbourhood centroids NCD(𝑋) 𝑑(𝑋), distance to nearest . In the method above we used thresholds to discriminate outlier suspicious points. We can also calculate the degree of outlierness for each measure and calculate total outlierness index (𝑂𝑢𝑡𝐼𝑛𝑑) as: 𝑂𝑢𝑡𝐼𝑛𝑑(𝑋)NCD(𝑋) ― minNCD(𝑌)𝑌NCD(𝑌) ― min+ 0.33NCD(𝑌)= 0.33max𝑌|𝑁𝜀(𝑋)| ― min𝑌𝑌|𝑁𝜀(𝑌)|(14)𝑑(𝑋) ― min𝑑(𝑌)𝑌max𝑌𝑑(𝑌) ― min𝑑(𝑌)𝑌+ 0.34max𝑌|𝑁𝜀(𝑌)| ― min𝑌|𝑁𝜀(𝑌)|The python code of FilterK and the scripts used to form and test the models [44] are available at:https://github.com/petrajones/filterk 3.1.2Local Outlier Factor LOF [45] is an outlier detection model which assigns each object a degree of outlierness based on its isolation by comparing the local density of an object to the local densities of its neighbours. The neighbourhood is given by k-nearest neighbours. Let us introduce some notation. The distance between points 𝑝 and 𝑞 is 𝑑(𝑝, 𝑞). The distance from point 𝑞 to 𝑘 nearest neighbour of point 𝑞 is 𝑑𝑘(𝑞). The set of 𝑁𝑘(𝑞) = {𝑝:𝑑(𝑝,𝑞) ≤ 𝑑𝑘(𝑞)}.𝑘 nearest neighbours of point 𝑞 is(15)It is important to mention that in the case of a “tie” number of neighbours, the number of elements |𝑁𝑘(𝑞)| in the set 𝑁𝑘(𝑞) can be greater than 𝑘 (|𝑁𝑘(𝑞)| > 𝑘). Now we can define the reachability distance of point 𝑝 from point 𝑞 𝑟𝑑𝑘(𝑝,𝑞) as 11 𝑟𝑑𝑘(𝑝, 𝑞) = max {𝑑𝑘(𝑞), 𝑑(𝑝, 𝑞)}.The local reachability density of 𝑝 is then defined as 𝑙𝑟𝑑𝑘(𝑝) =|𝑁𝑘(𝑝)|𝑞 ∈ 𝑁𝑘(𝑝)𝑟𝑑𝑘(𝑝, 𝑞).∑Finally LOF𝑘(𝑝) is defined as ∑𝑞 ∈ 𝑁𝑘(𝑝)𝑙𝑟𝑑𝑘(𝑞)𝑙𝑟𝑑𝑘(𝑝)|𝑁𝑘(𝑝)|∑𝑞 ∈ 𝑁𝑘(𝑝)𝑙𝑟𝑑𝑘(𝑞)|𝑁𝑘(𝑝)|𝑙𝑟𝑑𝑘(𝑝)=LOF𝑘(𝑝) =(16)(17)(18)LOF𝑘(𝑝) > 1 means a lower density than that of a point’s neighbours and such a point is interpreted as an outlier. LOF has the advantage of being based solely on a single parameter of 𝑘, used in defining the local neighbourhood of an object. 3.1.3.Isolation ForestsIF [46] builds an ensemble of tree structures for a given dataset. Each tree is created by splitting the dataset using a randomly selected attribute and split value. Many trees are created independently. As shown by the authors [46], usually several hundred trees is enough. For each tree and for each point 𝑥the path length ℎ(𝑥) is the number of edges from the tree root to the leaf which isolates point 𝑥(contains point 𝑥 only). The main idea of IF is based upon the observation that outliers usually have a shorter average path length. To normalise the average path length for a dataset with 𝑛 instances the following value is used: 𝑐(𝑛) = 2(ln (𝑛 ― 1) + 0.5772156649) ―(2(𝑛 ― 1)𝑛).The anomaly score is calculated as 𝑠(𝑥, 𝑛) = 2―𝐸(ℎ(𝑥))𝑐(𝑛) ,(19)(20)where 𝐸(ℎ(𝑥)) is the average path length for point. A widely used version of IF uses a random subsample of size instead of the whole dataset. If 𝑠(𝑥,𝑛) is close to 1 then a point 𝑥 is considered as an anomaly. The main preference of IF is that the model requires two variables only: the number of trees to be built and the sub-sampling size. This model utilises neither distance nor density measures to detect anomalies. 3.1.4 K-Nearest Neighbours (KNN)K-Nearest Neighbours (KNN) is an algorithm used for classification, clustering and outlier detection. In classification, the class label of an object is determined by a majority vote based on the classes of its surrounding k neighbours where k has to be chosen [47]. Outliers can be detected by KNN either 12 by assigning a score based on the distance from a point to its Kth neighbour (𝑑𝑘(𝑥)) or sometimes the mean average of the distance to k neighbours (𝑑(𝑋)) [48]. Here our KNN outlier detection model is based on the former, using the KNN outlier detection model from the Python PYOD library [49] to find 10% of points with the greatest distance to the Kth neighbour 𝑑𝑘(𝑥) as outliers.3.2. Assessing Outlier Detection Algorithms Three methods are used to assess and compare the efficacy of each outlier detection method: average cluster and event purity, silhouette score and confusion matrices displaying the proportion of activity classes and the dataset retained after outlier detection. 3.2.1Average Cluster Purity Our aim is to find universal clusters which can be applied to different datasets. To evaluate the quality of these clusters, we use average cluster and event purities, an approach utilised in a number of other studies clustering human activity [7, 38]. This measure can only be calculated for a labelled dataset. Cluster purity is a measure of the extent to which clusters contain a single dominant class [50] whereas event purity is the proportion of a class found within a cluster relative to that found in other clusters (the formulas for Average Cluster Purity (ACP) and Average Event Purity (AEP) are provided below). It should be noted that where the number of clusters (K) is not equal to the number of classes, purity has some limitations, particularly when working with imbalanced data where the relative sizes of the classes are different.However, Average Cluster and Event Purity (ACEP) still provides a useful rough guide to the extent to which a given category of PA constitutes the bulk of a cluster [50]. Let us denote 𝑛𝑖𝑗 as the number of events  in cluster  (number of points of class  in cluster ), 𝑗𝑗𝑖𝑖 𝑁𝑒 is total number of different events (classes), 𝑁𝑐 is number of clusters, is number of elements in cluster , 𝑖 𝑛𝑒𝑗 = ∑𝑁𝑐𝑖 = 1𝑛𝑖𝑗 𝑛𝑐𝑖 = ∑𝑁𝑒𝑗 = 1𝑛𝑖𝑗 𝑗 = 1𝑛𝑒𝑗 = ∑𝑁𝑐𝑖 = 1𝑛𝑐𝑖 𝑗 𝑁 = ∑𝑁𝑒total number of instances (points in (21)number of instances of event (class) , the dataset).The purity of event j epj and purity of cluster  i cpi are 𝑒𝑝𝑗 =1𝑛𝑒2𝑗∑𝑁𝑐𝑖 = 1𝑛2𝑖𝑗,𝑐𝑝𝑖 =1𝑛𝑐2𝑖∑𝑁𝑒𝑗 = 1𝑛2𝑖𝑗.Now we can calculate ACP, AEP and ACEP:13ACP =1𝑁𝑁𝑐∑𝑖 = 1 𝑛𝑐𝑖 × 𝑐𝑝𝑖,  AEP =1𝑁𝑁𝑒∑𝑖 = 1 𝑛𝑒𝑖 × 𝑒𝑝𝑖, ACEP =  ACP × AEP(22)3.2.2Silhouette Score We also use silhouette scores to evaluate cluster validity. Silhouette score does not require any external label information, making it more appropriate for assessment of outlier removal in free living accelerometer data. It has also been shown to have a strong correlation with some external cluster validation metrics [51]. For each data point from cluster we calculate the average distance to other points of the same 𝐶𝑗 𝑋𝑖 cluster: 𝑎(𝑖) =1|𝐶𝑗| ― 1 ∑𝑙 ∈ 𝐶𝑗, 𝑙 ≠ 𝑖‖𝑋𝑖 ― 𝑋𝑙‖.(23)We set 𝑎(𝑖) = 0 for cluster with one element only. For each data point 𝑋𝑖 𝐶𝑗 from cluster we define the minimum of average distances to points of other clusters. 𝑏(𝑖) = min𝑟 ≠ 𝑗Silhouette score of point  is𝑖1|𝐶𝑟|∑𝑙 ∈ 𝐶𝑟‖𝑋𝑖 ― 𝑋𝑗‖.𝑠(𝑖) =𝑏(𝑖) ― 𝑎(𝑖)max {𝑎(𝑖),𝑏(𝑖)}(24)(25)The silhouette score of the clustering model is the average of the silhouette score of all points: 𝑠 =1𝑁𝑁∑𝑠(𝑖).𝑖 = 1(26)The greater the silhouette score the better the quality of the clustering model. 144. Results and Analysis When applied to the accelerometer training dataset the proportion of each labelled activity that fell in each of the ten clusters is labelled A to J (see Table 2). When k-means was applied to the training dataset (Table 2A) nearly all running activity (95.7%) fell in cluster J (vigorous activity marked with darker shading), with sedentary activities in clusters A-E, slow walking in G-H while both brisker forms of treadmill-based and free-living ambulatory (walking/stairs) fell primarily into cluster I. Our initial k-means clustering results (shown top left in Table 2A) reveal that household activities were difficult to isolate and tended to ‘bleed’ across the set of clusters. A difficulty arises from the presence of sizeable quantities of household or walking visible within the vigorous cluster serving to obscure the true quantities of vigorous activity that an individual is engaged in. Our outlier detection algorithm, FilterK was evaluated against LOF, KNN and Isolation Forests using a Global Purity Matrix (an analogue of the Global Confusion Matrix for clustering problems) to show the proportion of the total instances of each physical activity class within each filtered cluster for both the datasets, train (Table 2A) and test (Table 2B). Particular strengths of k-means as an unsupervised machine learning model is that it can be stored and reapplied to multiple accelerometer datasets (portability). Using k-means to cluster accelerometer data, 85-95% of vigorous activity can typically be captured [10]. Significantly, where the training dataset included walking and household activity, small proportions of these activities (2.8% and 6.3% respectively) were subsumed within the vigorous cluster decreasing its purity (Table 2A). When applying the outlier detection models, we found that our model FilterK compares well to established anomaly detection algorithms (see Table 3) producing a mean ACEP across both datasets of 0.596 compared with LOF (0.585) and isolation forests (0.580). KNN managed 0.617 but returned only 90% of non-outliers compared with FilterK’s 96.0% (Table 3). When compared with the proportion of total target physical activity captured, all the outlier detection models reduced the proportion of running found within the vigorous activity cluster (Table 2A and 2B): isolation forests performed worst, capturing only 5.5% (train) and 57.4% (test) of instances of running within the vigorous activity cluster after filtering for outliers (see Table 4). LOF captured 86.3% on average but the cluster purity remained almost unchanged (ACEP 0.7950.803, Table 4). KNN improved vigorous cluster ACEP of 0.922 but at the cost of a 10% reduction in the proportion of running 81.3%) while FilterK produced an ACEP of 0.86 with a drop in the proportion of running captured of just 3.7% (Table 4).15TABLE 2Percentage of each class found within each cluster TABLE 2ATraining Dataset (Two Combined Datasets)K-means (Unfiltered)SedentaryB24.60.20.00.10.10.00.00.00.022.00.10.00.00.10.00.00.00.0C22.213.50.60.80.30.60.00.10.017.711.90.60.70.30.50.00.10.0D16.08.10.30.00.00.00.00.00.015.37.60.20.00.00.00.00.00.0E21.237.00.01.40.60.10.00.10.019.033.90.01.30.30.10.00.00.0Mixed Slow WalkG2.02.629.39.855.41.82.51.80.0F1.412.265.916.90.016.00.417.11.81.09.956.215.60.014.81.315.91.01.62.125.99.153.61.72.01.70.0H0.22.42.444.630.63.59.54.01.20.12.12.243.328.93.29.13.61.0Brisk WalkI0.10.71.115.64.277.884.575.41.20.10.50.412.72.472.273.368.60.6FilterKVig. Outlier SedentaryJ0.00.10.02.86.30.32.20.295.70.00.00.02.65.40.21.70.285.9CBA22.224.212.213.00.123.10.50.00.40.60.08.10.30.02.60.50.00.00.00.00.00.10.01.40.00.00.0Isolation Forests15.420.412.211.80.123.30.30.00.40.60.08.10.30.02.60.40.00.00.00.00.00.00.01.40.00.00.0D16.08.10.30.00.00.00.00.00.016.08.10.30.00.00.00.00.00.0E20.736.50.01.10.60.00.00.10.021.036.90.01.30.60.10.00.10.012.511.514.26.86.47.312.68.611.5Mixed Slow WalkG2.02.529.19.353.11.82.31.80.0F1.412.265.916.90.015.91.417.11.71.112.265.916.90.016.01.417.11.82.02.629.39.454.91.82.51.40.0H0.01.01.932.822.62.98.63.31.00.11.61.132.025.43.29.23.71.0Brisk WalkI0.00.50.915.14.277.584.375.01.10.10.51.015.44.277.684.575.21.2Vig. OutlierJ0.00.00.00.53.50.11.60.092.70.00.00.00.22.10.10.80.15.5O1.43.01.015.713.21.21.91.23.511.72.91.716.110.00.81.81.290.4CLASSLyingSeatedStandingHouseholdSlow walkWalkBrisk walkStairs Running LyingSeatedStandingHouseholdSlow walkWalkBrisk walkStairs Running 1.4Lying12.2Seated65.9Standing16.9Household0.0Slow walk15.9Walk1.4Brisk walk17.1Stairs 1.5Running Household activities are highlighted in bold to show the significant impact of the FilterK method on this class of activity. 15.98.00.30.00.00.00.00.00.022.212.70.50.50.30.40.00.10.00.00.40.814.83.877.283.973.60.60.00.00.00.00.30.00.50.086.40.00.21.418.65.72.15.21.80.320.636.20.01.00.40.00.00.10.01.92.429.09.052.11.82.31.80.024.00.20.00.00.00.00.00.00.01.84.91.931.234.92.56.84.211.316A12.223.30.48.12.60.00.01.40.0LOF10.820.40.37.92.60.00.01.20.0KNN12.222.80.48.02.60.00.01.40.0TABLE 2B Test Set (Child 1)K-means (Unfiltered)SedentaryCLASSLyingSeatedWalkRunning LyingSeatedWalkRunning B24.015.10.00.019.512.20.00.0A6.731.10.00.0LOF5.626.40.00.0KNN6.630.40.00.0C29.338.94.63.925.631.34.33.628.837.54.63.8D0.50.00.00.00.50.00.00.00.50.00.00.0E11.40.50.00.09.10.30.00.011.20.50.00.0LyingSeatedWalkRunning “Vig.” means vigorous23.314.90.00.0Mixed Slow WalkG5.00.30.20.1F15.68.718.20.313.06.318.00.315.68.718.20.34.20.30.20.14.30.20.10.1Brisk WalkI3.11.557.73.92.71.156.73.82.10.856.43.2H3.53.712.64.53.43.512.64.40.10.26.81.4Vig.OutlierFilterKSedentaryC29.238.54.63.9A6.731.10.00.0B23.915.00.00.0Isolation Forests23.015.10.00.06.630.90.00.029.238.54.63.8J0.90.36.787.30.80.36.686.70.00.01.576.1O15.718.21.71.17.56.912.415.0MixedF15.68.718.20.315.68.718.20.3Slow WalkG4.90.30.20.14.80.20.20.1H2.32.611.73.11.52.310.92.6Brisk WalkI2.90.957.53.53.01.057.73.9D0.50.00.00.00.20.00.00.0E11.40.50.00.010.90.50.00.0Vig.OutlierJ0.50.25.182.90.50.33.757.4O2.12.32.76.14.62.64.831.817TABLE 3 Average Cluster and Event Purity (ACCP) and Proportion of Total Physical Activity CapturedPercentage of Non-outliers Train0.547Original k-means0.562FilterK0.572KNN0.552LOF0.545Isolation ForestsWhere proportion (average) is the mean of the percentage of dataset returned Average100.096.090.090.090.0Test0.6140.6290.6610.6190.615ACEPAverage0.5810.5960.6170.5850.580Train100.095.190.090.090.0Test100.096.990.090.090.0TABLE 4 Average Cluster and Event Purity (ACCP) by Type and Proportion of Primary Physical Activity CapturedVigorous Activity (Cluster J)Original k-meansFilterKKNNLOFIsolation ForestsOriginal k-meansFilterKKNNLOFIsolation ForestsACEPAverage0.7950.8600.9220.8030.607ACEPAverage0.6170.6260.6370.6190.625ACEPAverageTrain91.50.79487.80.89581.20.96486.30.80530.90.435Sedentary Activity (Clusters A-E)Test0.7960.8240.8800.8020.779Percentage of Primary Activity (Running)Train95.792.686.485.95.2Test87.383.076.186.756.6Train0.6000.6070.6100.6050.606Ambulatory Activity (Clusters G-I)Average83.983.282.172.380.1Test0.6350.6450.6650.6340.644Percentage of Primary Activities (Lying and seated)Train89.288.087.479.482.7Test78.778.476.865.277.5Train0.570Original k-means0.588FilterK0.609KNN0.574LOFIsolation Forests0.590Mixed activity not shown given contains all classes so not possible to calculate AEP for this clusterAverage0.5430.5640.6000.5500.571Average77.876.471.773.976.6Test0.5160.5410.5910.5260.552Percentage of Primary Activity (All walking, stairs)Train85.283.580.178.284.3Test70.569.463.469.568.918TABLE 5Average Cluster and Event Purity (ACEP) and P-value of T-testDevelopment Dataset MethodAEPUnfilteredFilterKKNNLOFIF0.7070.7170.7200.7130.703p-value of t-testUnfilteredFilterK1.00×10-171.00×10-171.02×10-235.44×10-74.18×10-55.15×10-28.37×10-42.23×10-35KNN1.02×10-235.15×10-23.97×10-72.46×10-42LOF5.44×10-78.37×10-43.97×10-78.55×10-19IF4.18×10-52.23×10-352.46×10-428.55×10-19Test Dataset MethodAEPUnfilteredFilterKKNNLOFIF0.6150.6310.6620.6170.616p-value of t-testUnfilteredFilterK2.44×10-242.44×10-245.85×10-2031.80×10-12.10×10-11.59×10-919.48×10-177.99×10-23KNN5.85×10-2031.59×10-917.86×10-1671.27×10-220LOF1.80×10-19.48×10-177.86×10-1677.83×10-1IF7.99×10-232.15×10-261.27×10-2207.83×10-1p-value of t-testUnfilteredFilterK4.58×10-814.58×10-811.22×10-2171.38×10-59.42×10-22.09×10-403.58×10-515.95×10-99KNN1.22×10-2172.09×10-403.17×10-1702.85×10-246LOF1.38×10-53.58×10-513.17×10-1706.32×10-10IF9.42×10-25.95×10-992.85×10-2466.32×10-10p-value of t-testUnfilteredFilterK3.67×10-143.67×10-144.20×10-991.65×10-35.82×10-11.05×10-452.35×10-51.31×10-16KNN4.20×10-991.05×10-455.88×10-719.54×10-107LOF1.65×10-32.35×10-55.88×10-711.83×10-4IF5.82×10-11.31×10-169.54×10-1071.83×10-4ACEP0.5470.5620.5720.5520.545ACEP0.6140.6290.6610.6190.614ACP0.4230.4400.4550.4270.422ACP0.6140.6280.6590.6200.61219Comparison of absolute values of ACP and AEP for these three methods can be complemented by a statistical test of the significance of those differences. Unfortunately it is impossible to apply these tests to ACEP because of the evaluation method. To check the significance of these differences we applied a t-test with a null hypothesis that ACP and AEP for each two methods are the same (Table 5). Table 2 shows that FilterK selects a lower number of outliers compared with the two other techniques. We applied a t-test using binomial proportion estimations to test the significance of these differences. Table 6 shows that the number of outliers selected by FilterK is statistically significantly less than LOF, IF and KNN. Tables 2, 3 and 4 show that FilterK provides good ACEP (ACP, AEP). This means that FilterK selects more appropriate (influential) outliers and can provide high purity with the loss of fewer points.TABLE 6T-test and P-Value of Outliers Training DatasetFilterKLOF1.49×10-1171.49×10-1171.49×10-1171.49×10-117Test DatasetFilterK11LOF6.07×10-756.07×10-756.07×10-756.07×10-7511FilterKLOFIFKNNFilterKLOFIFKNNIF1.49×10-11711IF6.07×10-7511KNN1.49×10-11711KNN6.07×10-7511When further analysing the classes of the suspected outliers found by FilterK within the training dataset, we find that 15.7% of household activities were isolated. Household activity is prevalent throughout all but one of the clusters within the training dataset (see Table 2A) and FilterK reduces the proportion of household activities found within the training dataset’s vigorous cluster from 2.8% to 0.5%, unlike LOF (2.8%2.6%). IF and KNN improve vigorous cluster purity (Table 4) by considerably reducing the instances of running. Another reason for the improvement, in particular, of the purity of vigorous activity cluster within the test dataset, by FilterK is its reduction of instances of slow walking from 6.7% to 5.1% (Table 2B).Importantly, the purity of sedentary and ambulatory clusters also improve when using FilterK and average ambulatory cluster purity (0.6170.626, 0.5430.564) whilst minimally reducing the proportion of sedentary and ambulation activity (83.9%83.2%, 77.8%76.4%) primarily by removing household activities from these clusters (Table 4). However, none of the three algorithms proved particularly effective at filtering out the large quantities of standing within the ambulatory 20cluster of the training dataset, only LOF had some impact here (standing reduced by 29.3%25.9% within cluster G, Table 2A). We see a smaller improvement in the test dataset cluster purity (0.6350.645) given that the purity here is already fairly high given the absence of any household activity data. A larger adult dataset containing both vigorous and household activity will be required for further testing and confirmation. TABLE 7Fraction of Cases of Activities Recognised as Outliers (Training Dataset)ActivityLying DownSeatedStandingHousehold Ambulatory* RunningFilterKKNNLOF1.4%3.0%1.0%15.7%2.4%3.5%1.8%4.9%1.9%31.2%34.5%11.3%12.5%11.5%14.2%6.8%8.3%11.5%IF11.7%3.0%1.7%16.1%1.8%90.4%*Combined Walking classes (Stairs, Slow Walk, Brisk Walk, Walk)When excluding outliers from the calculation, the KNN algorithm produced the best silhouette scores, closely followed by FilterK although all algorithms provided only a marginal improvement in this measure of the separation and cohesion of clusters (Table 8). FilterK compares reasonably to the other models in terms of the initial runtime to apply and store the model although it is clearly slower to run than LOF and KNN, although the script has yet to be optimised for speed (Table 9). TABLE 8Comparison of Outlier Detection Algorithm Silhouette Scores (Measure of cluster cohesiveness and separation) Outlier Detection Algorithm Training DatasetK-means (unfiltered)FilterKLOFIsolation ForestsKNN0.2960.3130.2990.2940.321Test Dataset0.2260.2310.2270.2300.237TABLE 9Outlier Detection Algorithm Time Consumption Comparison Outlier Detection Algorithm Training DatasetFilterKKNNIsolation ForestsLOF8.16 secs2.94 secs18.13 secs3.09 secsTest Dataset0.87 secs0.39 secs4.45 secs0.53 secs5. Discussion Our proposed outlier detection method has shown some initial promise in improving cluster purity across physical activity categories, for example within the vigorous cluster, through the reduction of suspected outliers such as household, walking and other isolated instances of classes not normally associated with vigorous exercise. The main advantage is that FilterK achieves comparable results 21without discarding so many samples as outliers. It performs well against LOF and IF but does not outperform the method based on KNN. Another limitation is the availability of only a child accelerometer dataset to confirm the results of application to the adult development dataset. Both datasets included core sedentary, walking and running activities, but the adult dataset also included household activities. While the robustness of the model across such diverse samples is a strength, clearly testing on further accelerometer datasets is required – particularly those with a large variety of activities and with diverse populations where clustering by intensity is sought. Importantly, the outlier detection model should be applied to a range of labelled free living datasets covering a range of intensity distributions to see how it performs. Although we acknowledge that our outlier detection method has been limited to data gathered from GENEA accelerometer models, we feel these approaches will be generalizable to other widely used monitors. For example, we have shown that the outputs from the GENEActiv monitor are largely equivalent to the Axivity device that was used to collect data in 100,000 adults as part of UK Biobank [52]. It is our hope that this outlier detection method may prove useful in data mining using partitional clustering with k-means to analyse accelerometer data and in so doing identify relationships types and intensities of activity beneficial to health [53]. In other previous attempts to apply outlier detection methods to accelerometer data, Munoz-Organero applied this methodology only to data from fifteen participants rather than two datasets (n=131) and their recurrent neural network method required a series of estimates for gravity force vectors, acceleration caused by user movement and compensation for variations in sensor orientation. Further, the model was designed to identify very specific types of defined outliers, namely movement involving turning around, a full stop or significant slow-down with performance assessed on the basis of ground truth (F1 score 0.97) [16]. These issues represent significant limitations. In Said Abdallah et al., as part of a cluster-based classification model, an outlier detection method is proposed based on density gain or loss to a cluster resulting from inclusion of a group of points, where changes in density occur. Whilst the method is applied to two datasets, these contain only a total of nine participants. Further, only the classification method evaluation is reported, by means of the ground truth: specifically, the percentage of class instances correctly classified, percentage requiring active learning (necessitating user input in labelling) and error rate [17]. In this paper, we have purposefully chosen popular all-purpose methods with a track record of outlier detection in a wide variety of contexts. Here, we used the ninetieth percentile as a threshold for outlier detection given its use in similar studies [43]. When adjusting this outlier threshold both for the distance from a point to its nearest centroid, or the mean distance to its nearest neighbours, from an outlier threshold of the 90th to the 2295th percentile, we find 1.4% more non-outliers are returned, but ACEP drops by 0.06 (0.595 0.589) (see Supplemental Table 3). Equally, moving from a threshold of the 90th percentile to the 85th, ACEP increases by 0.07 but 4.5% less non-outliers are returned. Varying Nmin between 3 and 8 made only a 0.2% difference to the proportion the dataset treated as outliers (4.8-5.0% train, 3.0-3.2%, test). It may be possible to develop an optimisable hyper-parameter but that is beyond the scope of this paper. 6. Conclusions One of the biggest strengths of FilterK is its ability to improve ACEP across all the clusters with a minimal reduction in the proportion of non-outliers returned when compared with the rival methods of LOF, KNN and IF, through its facility to identify particularly “unusual” points. The algorithm can also be adjusted. FilterK has an outlier score function which assigns a degree of outlierness based on the normalised score derived from each of its three tests: namely, mean distance to neighbours, distance to nearest centroid and neighbourhood density. This enables a minimum threshold outlier score to be set as an exclusion criterion when filtering accelerometer data for anomalies. Kriegel et al. proposed a unification of the outlier scores provided by various models to values in the range [0,1] facilitating enhanced ensembles for outlier detection. Our total outlierness index (𝑂𝑢𝑡𝐼𝑛𝑑) thereby provides an important mechanism through which ensemble voting methods that combine the techniques discussed in this paper might be developed [54]. We have also yet to examine how FilterK performs more broadly on a range of outlier tasks in accelerometer data, to assess its strengths and weaknesses against the full range of outlier detection algorithms that are currently available but these early accelerometer results suggest it may prove a valuable and useful additional to existing outlier detection algorithms in this area. 23AcknowledgementsThe authors thank Dr. Dale Esliger for data collection of development dataset 1; Sarah Bunnewell and James Sanders for assistance with data collection of development dataset 2; Dr Lisa Price (University of Exeter) for sharing independent dataset 1; and Hannah Goodes, Carly Kingdon and Megan Waters for their assistance with data collection of independent datasets 2 and 3. We also thank the participants of all studies who volunteered to take part in this study. We’d also like to thank Professor Jeremy Levesley of University of Leicester’s School of Mathematics and Actuarial Science and Professor Stephen Milan of the School of Physics and Astronomy for both facilitating and encouraging the collaboration that led to the development of this paper. The data collection of dataset 1 was funded by a research grant awarded by Unilever Discover to the School of Sport and Health Sciences, University of Exeter. This research was supported by the National Institute for Health Research (NIHR) Leicester Biomedical Research Centre and the NIHR Collaboration for Leadership in Applied Health Research and Care – East Midlands. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health. The results of the present study do not constitute endorsement by the authors of the products described in this article. There are no conflicts of interest. The results of the study are presented clearly, honestly, and without fabrication, falsification, or inappropriate data manipulation.References[1] I. Lee et al., Impact of physical inactivity on the world’s major non-communicable diseases, Lancet, 380 (2012) 219–229.[2] V.A. Cornelissen, N.A. Smart, Exercise training for blood pressure: A systematic review and meta-analysis, J Am Heart Assoc, 2 (2013) e004473.[3] C.L. Gillies, K.R. Abrams, P.C. Lambert, N.J. Cooper, A.J. Sutton, R.T. Hsu, K. Khunti, Pharmacological and lifestyle interventions to prevent or delay type 2 diabetes in people with impaired glucose tolerance: Systematic review and meta-analysis, BMJ, 334 (2007) 299. [4] X. Lin, X. Zhang, J. Guo, C.K. Roberts, S. McKenzie, W.C. Wu, S. Liu, Y. Song, Effects of exercise training on cardiorespiratory fitness and biomarkers of cardiometabolic health: A systematic review and meta-analysis of randomized controlled trials, J Am Heart Assoc, 4 (2015) pii: e002014.[5] Bonomi AG, Goris AH, Yin B, Westerterp KR, Detection of type, duration, and intensity of physical activity using an accelerometer. MSSE (2009) 41(9):1770–1777. 24[6] F. Concone, S. Gaglio, G. Lo Re, M. Morana, Smartphone data analysis for human activity recognition. 2017. In: Esposito F., Basili R., Ferilli S., Lisi F. (eds) AI*IA 2017 Advances in Artificial Intelligence. AI*IA 2017. Lecture Notes in Computer Science, vol 10640. Springer, Cham. [7] C. Domingo, Unsupervised habitual activity detection in accelerometer data. Mechatronics and Machine Vision in Practice 3 (2018) 253–272. [8] S. Zhao, W. Li, J. Cao, A user-adaptive algorithm for activity recognition based on K-Means clustering, local outlier factor and multivariate Gaussian distribution. Sensors (2018) 18:1850, DOI:10.3390/s18061850. [9] D. Biswas, A. Cranny, N. Gupta, K. Maharatna, J. Achner, J. Klemke, M. Jobges, S. Ortmann, Recognizing upper limb movements with wrist worn inertial sensors using k-means clustering classification. Human Movement Science. 40 (2015) 59-76.[10] P. Jones, E. Mirkes, T. Yates, C.L. Edwardson, M. Catt, M.J. Davies, K. Khunti and A.V. Rowlands, Towards a portable model to discriminate activity clusters from accelerometer data. Sensors. (2019) 19:4504 doi:10.3390/s19204504.[11] D. Van Kuppevelt, J. Heywood, M. Hamer, S. Sabia, E. Fitzsimons, V. Van Hees, Segmenting accelerometer data From daily life with unsupervised machine learning, bioRxiv, 1, (2018) 3–4.[12] K. Pliakos, C. Vens, Mining features for biomedical data using clustering tree ensembles. Journal of Biomedical Informatics 85 (2018) 40–48, 41. [13] S.S. Khan, J. Hoey, Review of fall detection techniques: A data availability perspective. Medical Engineering and Physics (2017) 39:12–22.[14] M. Schinle, I. Papantonis, W. Stork, Personalization of monitoring system parameters to support ambulatory care for dementia patients. IEEE Sensors Applications Symposium (2018). https://doi.org/10.1109/SAS.2018.8336724 [15] W. Xu, Y. Shen, Y. Zhang, N. Bergmann, W. Hu, Gait-watch: A context-aware authentication system for smart watch based on gait recognition. IoTDI ’17 Proceedings of the Second International Conference on Internet-of-Things Design and Implementation 59–70. [16] M. Munroz-Organero, Outlier detection in wearable sensor data for human activity recognition (HAR) based on DRNNs, IEEE Access PP(99) (2019) 1-1, DOI: 10.1109/ACCESS.2019.2921096. [17] Z. Said Abdallah, M.M. Gaber, B. Srinivasan, S. Krishnawamy, StreamAR: Incremental and active learning with evolving sensory data for activity recognition, IEEE 24th International Conference on Tools with Artificial Intelligence (2012) 1163–1170, DOI: 10.1109/ICTAI.2012.169.25[18] A. Diez-Olivan, J.A. Pagan, R. Sanz, B. Sierra, Data-driven prognostics using a combination of constrained K-means clustering, fuzzy modeling and LOF-based Score. Neurocomputing (2017) 241:97–107. [19] S. Enshaeifar, A. Zoha, S. Skillman, A. Markides, S.T. Acton, T. Elsaleh, M. Kenny, H. Rostill, R. Nilforooshan, P. Barnaghi, Machine learning methods for detecting urinary tract infection and analysing daily living activities in people with dementia./ PLoS ONE (2019) 14(1):e0209909. https://doi.org/10.1371/journal.pone.0209909. [20] M. Ester, H-P. Kriegel,J. Sander, X.Xu, A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96) 226–231. [21] A. Barai, L. Dey, Outlier detection and removal algorithm in k-means and hierarchical clustering. World Journal of Computer Application and Technology 5 (2017) 24-29. [22] C.C. Aggarwal, Outlier analysis. Second edition. Springer: Cham, pp17-19. [23] X. Xu, H. Liu, L. Li, M. Yao, A Comparison of outlier detection techniques for high-dimensional data. International journal of computational intelligence systems 11 (2018) 652-662. [24] C. Dobbins, R. Rawassizadeh, Towards clustering of mobile and smartwatch accelerometer data for physical activity recognition. Informatics (2018) 5:29 https://doi:10.3390/informatics5020029.[25] Y. Kwon, K. Kang, C. Bae, Unsupervised learning for human activity recognition using smartphone sensors. Expert Systems with Applications (2014) 41:6067–6074. [26] T.N. Tran, K.Drab, M. Daszykowski, Revised DBSCAN algorithm to cluster data with dense adjacent clusters. Chemometrics and Intelligent Laboratory Systems (2013) 120:92–96. [27] D. Van Kuppervelt, J. Heywood, M. Hamer, S. Sabia, E. Fitzsimons, V. van Hees, Segmenting accelerometer data from daily life with unsupervised machine learning. PLoS ONE (2019) 14(1): e0208692. https://doi.org/10.1371/journal.pone.0208692.[28] J.H. Migueles, A.V. Rowlands, F. Huber, S. Sabia, V. van Hees, GGIR: A research community-driven open-source R-package for generating physical activity and sleep outcomes from multi-day raw accelerometer data. Journal for the Measurement of Physical Behaviours. https://doi.org/10/1123/jmpb.2018-0063.[29] D.W. Esliger, A.V. Rowlands, T.L. Hurst, M. Catt, P. Murray, R.G. Eston, Validation of the GENEA accelerometer, Medicine and Science in Sports and Exercise, 43 (2011) 1085–1103.26[30] K. Bakrania, T.Yates, A.V. Rowlands, D.W. Esliger, S. Bunnewell, J. Sanders, M. Davies, K. Khunti, C.L. Edwardson, Intensity thresholds on raw acceleration data: Euclidean norm minus one (ENMO) and mean amplitude deviation (MAD) Approaches, PLoS One, 11 (2016) e0164045.[31] L.R. Phillips, G. Parfitt, A.V. Rowlands, Calibration of the GENEA accelerometer for assessment of physical activity intensity in children, J Sci Med Sport, 43 (2013) 1085–103. [32] V.T. Van Hees, R. Golubic, U. Ekelund, S. Brage, Impact of study design on development and evaluation of an activity type classifier, J Appl Physiol, 114 (2012) 1042–51.[33] S.P. Lloyd, Least squares quantization in PCM, IEEE Transactions on Information Theory, 28 (1982) 129–37.[34] R.L. Thorndike, Who belongs in the family? Psychometrika. 18 (1953) 267–76. [35] P. Rousseeuw, Silhouettes: A graphical aid to the interpretation and validation of cluster analysis, Computational and Applied Mathematics, 20 (1987) 53–65. [36] T. Calinski, J. Harabasz, A dendrite method for cluster analysis, Communications in Statistics, 3 (1972) 1–27. [37] C. Ming-Tso, B. Mirkin, Intelligent choice of the number of clusters in K-means clustering: An experimental study with different cluster spreads, Journal of Classification, 27 (2010) 3–40. [38] A. Nguyen, D. Moore, I. McCowan, Unsupervised clustering of free-living human activities using ambulatory accelerometry, Proceedings of the 29th Annual International Conference of the IEEE EMBS Cité Internationale, Lyon, (2007) 4895. [39] D. Arthur, S. Vassilvitskii, K-means++: The advantages of careful seeding. Proceedings of the eighteenth annual ACM-SIAM symposium on discrete algorithms, Society for Industrial and Applied Mathematics (2007) 1027–1035.[40] C.C. Aggarwal, C.K. Reddy, Data Clustering: Algorithms and Applications (2014) Paragraph 4.2.3.1, p91 CRC Press: Minneapolis. [41] F. Pedregosa, G. Varoquaux, A. Gramfort et al., Scikit-learn: Machine Learning in Python, JMLR 12 (2011) 2825–2830. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html[42] X. Xu, H. Liu, L. Li, M. Yao, A Comparison of outlier detection techniques for high-dimensional data, International Journal of Computational Intelligence Systems. 11 (2018) 652–62.[43] Y.H. Dovoedo, S. Chakraborti, Outlier detection for multivariate skew-normal data: A comparative study. Journal of Statistical Computation and Simulation (2013) 83(4):773–783. 27[44] P. Jones, E. Mirkes, M. James, FilterK outlier detection algorithm, Available from https://github.com/petrajones/filterk.[45] M.M. Breunig, H-P. Kriegel, R.T. Ng, J. Sander, LOF: Identifying Density-Based Local Outliers, Proceedings of the 2000 ACM SIG MOD International Conference on Management of Data (SIGMOD), (2000) 93–104. [46] F.T. Liu, K.M. Ting, Z-H Zhou, Isolation forests, 2008 Eighth IEEE International Conference on Data Mining, (2008) 413–22. Available from https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf.[47] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets. In Proceedings of the 2000 ACM SIGMOD international conference on management of data, pp.427-438. [48] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces. In: European Conference on Principles of Data Mining and Knowledge Discovery (2000) (pp. 15-27). Springer, Berlin, Heidelberg.[49] Y. Zhao, Z. Nasrulah, Z. Li, PyOD: A python toolbox for scalable outlier detection. Journal of machine learning research (JMLR) (2019) 20(96):1-7. https://pyod.readthedocs.io/en/latest/[50] J. Ajmera, H. Bourlard, I. Lapidot, I. McCowan Unknown-multiple speaker clustering using HMM. 7th International Conference on Spoken Language Processing, ICSLP 2002 – INTERSPEECH (2002) September 16-20. [51] C. Lopez, S. Tucker, T. Salameh, C. Tucker, An unsupervised machine learning method for discovering patient clusters based on genetic signatures. Journal of Biomedical Informatics (2018) 85:30–39, 31.[52] A.V. Rowlands, E. Mirkes, T. Yates, S. Clemes, M. Davies, K. Khunti, C.Edwardson, Accelerometer-assessed physical activity in epidemiology. Medicine & Science in Sports & Exercise (2018) 50(2):257–265. [53] I. Yoo, P. Alafaireet, M. Marinov, K. Pena-Hernandez, R. Gopidi, J-F. Chang, L. Hua, Data mining in healthcare and biomedicine: A survey of the literature. J Med Syst (2012) 36:2431–2448. [54] H-P. Kriegel, P. Kröger, E. Schubert, A. Zimek, Interpreting and unifying outlier scores. Society for Industrial and Applied Mathematics. Proceedings of the 2011 SIAM International Conference on Data Mining (2011) https://epubs.siam.org/doi/abs/10.1137/1.9781611972818.2. 28Conflict of interest statementWe declare that we have no financial and personal relationships with other people or organisations that can inappropriately influence our work, there is no professional or other interest of any nature or kind in any product, service and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled “FilterK, a new outlier detection method for k-means applied to physical activity”. Petra Jones 29Highlights for Reviewnew outlier detection method for use with k-means and physical activity accelerometer datacomparison with three other outlier detection methods, Local outlier Function, Isolation Forests and K-Nearest Neighboursefficient improvement of average cluster and event purity whilst retaining high proportion of original dataset 30Credit Author StatementAuthor Contributions: Conceptualization, P.J., and E.M.; Methodology, P.J., E.M., M.J., A.V.R; Software, P.J., E.M.,M.J.; Validation, E.M., A.V.R., T.Y., and M.J.; Formal analysis, P.J., E.M., A.V.R.; Investigation, P.J., E.M., A.V.R., T.Y.,; Data curation, A.V.R.; Writing—original draft preparation, P.J. and E.M.; Writing—review and editing, all authors; Visualization, P.J. and E.M.; Supervision, E.M., A.V.R. and T.Y.; Project administration, A.V.R.; Funding acquisition, A.V.R. and M.C.31