Artificial Intelligence 171 (2007) 514–534www.elsevier.com/locate/artintRandom constraint satisfaction:Easy generation of hard (satisfiable) instancesKe Xu a,∗,1, Frédéric Boussemart b,2, Fred Hemery b,2, Christophe Lecoutre b,∗,2a National Lab of Software Development Environment, School of Computers, Beihang University, Beijing 100083, Chinab CRIL (Centre de Recherche en Informatique de Lens), CNRS FRE 2499, rue de l’université, SP 16, 62307 Lens cedex, FranceReceived 18 April 2006; received in revised form 30 January 2007; accepted 9 April 2007Available online 14 April 2007AbstractIn this paper, we show that the models of random CSP instances proposed by Xu and Li [K. Xu, W. Li, Exact phase transitionsin random constraint satisfaction problems, Journal of Artificial Intelligence Research 12 (2000) 93–103; K. Xu, W. Li, Manyhard examples in exact phase transitions with application to generating hard satisfiable instances, Technical report, CoRR Reportcs.CC/0302001, Revised version in Theoretical Computer Science 355 (2006) 291–302] are of theoretical and practical interest.Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances ofany arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of anasymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. Inthat case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to havean exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisfiable instanceswhose hardness is similar to unforced satisfiable ones. This analysis is supported by some representative results taken from anintensive experimentation that we have carried out, using complete and incomplete search methods.© 2007 Elsevier B.V. All rights reserved.Keywords: Phase transition; Constraint network; Hard random instances1. IntroductionOver the past ten years, the study of phase transition phenomena has been one of the most exciting areas in Com-puter Science and Artificial Intelligence. Numerous studies have established that for many NP-complete problems(e.g., SAT and CSP), the hardest random instances occur, while a control parameter is varied accordingly, betweenan under-constrained region where all instances are almost surely satisfiable and an over-constrained region whereall instances are almost surely unsatisfiable. In the transition region, there is a threshold where half the instances are* Corresponding authors. Please correspond with the first author for the theory of this paper and the last author for the experiment.E-mail addresses: kexu@nldse.buaa.edu.cn (K. Xu), boussemart@cril.univ-artois.fr (F. Boussemart), hemery@cril.univ-artois.fr (F. Hemery),lecoutre@cril.univ-artois.fr (C. Lecoutre).1 Author partially supported by the National 973 Program of China (Grant No. 2005CB321901), NSFC Grant 60403003 and FANEDD Grant200241.2 Authors supported by the CNRS, the “programme COCOA de la Région Nord/Pas-de-Calais” and by the “IUT de Lens”.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.001K. Xu et al. / Artificial Intelligence 171 (2007) 514–534515satisfiable (and half the instances are unsatisfiable). Generating hard instances is important both for understanding thecomplexity of the problems and for providing challenging benchmarks [11].Another remarkable progress in Artificial Intelligence has been the development of incomplete algorithms forvarious kinds of problems. And, since this progress, one important issue has been to produce hard satisfiable instancesin order to evaluate the efficiency of such algorithms, as the approach that involves exploiting a complete algorithmin order to keep random satisfiable instances generated at the threshold can only be used for instances of limitedsize. Also, it has been shown that generating hard (forced) satisfiable instances is related to some open problems incryptography such as computing a one-way function [11,20].In this paper, we mainly focus on random CSP (Constraint Satisfaction Problem) instances. Initially, four “stan-dard” models, denoted A, B, C and D [16,39], have been introduced to generate random binary CSP instances.However, Achlioptas et al. [3] have identified a shortcoming of all these models. Indeed, they prove that randominstances generated using these models suffer from (trivial) unsatisfiability as the number of variables increases. Toovercome the deficiency of these standard models, several alternatives have been proposed.On the one hand, a model E has been proposed in [3] and a generalized model in [31]. However, the model Edoes not permit to tune the density of the instances and the generalized model requires an awkward exploitationof probability distributions. Also, other alternatives correspond to incorporating some “structure” in the generatedrandom instances. Roughly speaking, it involves ensuring that the generated instances be arc consistent [16] or pathconsistent [15]. The main drawback of all these approaches is that generating random instances is no more quite anatural and easy task.On the other hand, standard models have been revised [14,38,44,45] by controlling the way parameters change asthe problem size increases. The alternative model D scheme [38], where both the domain size and the average degree ofthe constraint graph increase with the number of variables, guarantees the occurrence of an asymptotic phase transitionas the constraint tightness is varied. The two revised models, called RB and RD [44,45] provide the same guaranteeby varying one of two control parameters around a critical value that, in addition, can be computed. Also, in [14],a range of suitable parameter settings is identified which allows to exhibit a non-trivial threshold of satisfiability.Their theoretical results apply to binary instances taken from model A and to “symmetric” binary instances from aso-called model B which, not corresponding to the standard one, associates the same relation with every constraint.The models RB and RD present several nice features:• it is quite easy to generate random instances of any arity as no particular structure has to be integrated, or propertyenforced, in such instances.• the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domainsize and on constraint tightness. For instances involving constraints of arity k, the domain size is required to begreater than the kth root of the number of variables and the (threshold value of the) constraint tightness is requiredto be at most k−1k .• when the asymptotic phase transition exists, a threshold point can be precisely located, and all instances generatedfollowing models RB and RD have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity.• it is possible to generate forced satisfiable instances whose hardness is similar to unforced satisfiable ones.Concerning the last item, note that instances forced to be satisfiable are simply built by randomly generating firsta solution, and then a set of constraints guaranteed to support this solution. We show that under the same conditions,those used to establish the existence of an asymptotic phase transition in model RB, there is an asymptotic similaritybetween forced and unforced satisfiable instances of model RB with respect to the number and the distribution ofsolutions. We believe that this is one important aspect of our contribution.Also, remember that CSP is a generalization of the SAT problem which is the first proven NP-complete problem andplays a central role in computational complexity. In the study of phase transitions in NP-complete problems, random3-SAT has received most attention, both theoretically and experimentally, in the past decade or so. However, untilnow, the existence of the threshold phenomenon in random 3-SAT has not been established, not even the exact valueof the threshold point. As such, much of the work on random 3-SAT has been focused on proving lower bounds andupper bounds for the threshold point. Through a series of hard work, the current best lower bound and upper bound forrandom 3-SAT are 3.42 [22] and 4.506 [13], respectively. In contrast, the existence of phase transitions in model RBhas been established and the threshold points are also known exactly. In fact, these results about model RB are mainly516K. Xu et al. / Artificial Intelligence 171 (2007) 514–534obtained by direct application of the second moment method which, unfortunately, fails on random 3-SAT. The reasonfor this is that the distribution of the number of solutions for model RB is very uniform (i.e. almost all instanceshave the same number of solutions) while for random 3-SAT, this distribution is highly skewed (i.e. a few instanceshave far more solutions than most instances). From the viewpoint of phase transitions in NP-complete problems, it istherefore interesting to further investigate in which aspects random 3-SAT and model RB behave differently, and ifsuch comparisons can shed any light on the fundamental properties of NP-complete problems. In this paper, we willsee an example of such a comparison: when random 3-SAT and model RB are used to generate satisfiable instances,the same strategy can produce very different results.Finally, note that in experimental CSP studies, random instances of model B with varying tightness and densityare often used as benchmarks for algorithms. However, such instances cannot be used to evaluate the asymptoticperformance of algorithms. To fill this need, a good way is to generate random instances of increasing size (i.e.number of variables) which are guaranteed to be hard. An advantage of model RB over model B is that it provides asimple method to generate such asymptotically hard instances (you can download such a generator at http://www.cril.univ-artois.fr/~lecoutre) which are also useful for understanding what makes a problem really hard to solve.This paper (an extended version of [43]) is organized as follows. We first introduce constraint networks and givean overview of the different models that have been proposed to generate random networks (Section 2). Then, weintroduce models RB and RD (Section 3) and present a variant of model RD which can be used to generate randomnetworks involving constraints of any arity (Section 4). Next, we provide a formal analysis about generating bothforced and unforced hard satisfiable instances (Section 5). Finally, we present the algorithms (Section 6) that we haveused for our experimentation (Section 7), and, before concluding, we discuss some related work.2. Models of random constraint networksWe first introduce constraint networks and the Constraint Satisfaction Problem.Definition 1. A Constraint Network is a pair (X, C) where:• X = {X1, . . . , Xn} is a finite set of n variables such that each variable Xi has an associated domain, denoteddom(Xi), which contains the set of values allowed for Xi ,• C = {C1, . . . , Cm} is a finite set of m constraints such that each constraint Cj involves a subset of variables ofX, called scope and denoted vars(Cj ), and has an associated relation, denoted rel(Cj ), which contains the set oftuples allowed for the variables in vars(Cj ).The arity of a constraint C is the number of variables involved in C, i.e., the number of variables of its scope.The tightness of a constraint corresponds to the proportion of disallowed tuples in the associated relation. However,when one generates random constraints, it is possible to define the tightness as the probability that a tuple be disal-lowed.A solution to a constraint network is an assignment of values to all the variables such that all the constraints aresatisfied. The set of solutions of P is denoted sol(P ). A constraint network is said to be satisfiable iff it admits at leastone solution. The Constraint Satisfaction Problem (CSP) is the NP-complete task of determining whether a givenconstraint network is satisfiable. A CSP instance is then defined by a constraint network, and solving it involves eitherfinding one (or more) solution or determining its unsatisfiability.Now, we can give a quick overview of the different models that have been proposed to generate random constraintnetworks. Four conventional models denoted A, B, C and D [16,39] to generate random binary constraint networkshave been introduced. Whatever model is chosen, two steps are necessary to generate such a network. In the firststep, we have to build its macro-structure called constraint hyper-graph, whereas in the second step, we have to buildits micro-structure called consistency hyper-graph. Actually, the two steps can be interleaved. The constraint hyper-graph is composed of n vertices corresponding to variables and m hyper-edges corresponding to constraints; eachhyper-edge links the variables occurring in the scope of the constraint it represents. The consistency hyper-graph|rel(Cj )| hyper-|dom(Xi)| vertices corresponding to all values of all domains and ofis composed ofedges corresponding to all supports (allowed tuples) of all relations. Note that the density of a constraint network (orhyper-graph), assuming that it only involves constraints of arity k (and distinct scopes), is equal to m/(nmj =1ni=1(cid:2)(cid:2)k ).K. Xu et al. / Artificial Intelligence 171 (2007) 514–534517Each random CSP instance is characterized by a tuple (k, n, d, p1, p2) where k denotes the arity of the constraints,n the number of variables, d the uniform domain size, p1 a measure of the density of the constraint graph and p2a measure of the tightness of the constraints. There are four models since p1 and p2 can represent either a probabilityor a proportion. p1 is considered as a probability in models A, C and as a proportion in models B, D while p2 isconsidered as a probability in models A, D and as a proportion in models B, C. Note that, for binary instances, k isusually omitted.Model B is defined as follows:Definition 2 (Model B). A class of random CSP instances of model B will be denoted B(k, n, d, p1, p2) where, foreach instance:• k (cid:2) 2 denotes the arity of each constraint,• n (cid:2) 2 denotes the number of variables,• d (cid:2) 2 denotes the size of each domain,• 1 (cid:2) p1 > 0 determines the number m = p1(n• 1 > p2 > 0 determines the number t = p2d k of disallowed tuples of each relation.k ) of constraints,To generate one instance P ∈ B(k, n, d, p1, p2), we have to build m constraints, each one formed by randomly select-ing (without repetition) a scope of k (distinct) variables and randomly selecting (with repetition) a relation of t distinctdisallowed tuples.Note that p1(n2) and p2d k may have to be rounded to the nearest integer. Then, one can prefer, instead of propor-tions, directly using the number m of constraints and the number t of disallowed tuples.Model D is defined as follows:Definition 3 (Model D). A class of random CSP instances of model D will be denoted D(k, n, d, p1, p2) where, foreach instance:• k (cid:2) 2 denotes the arity of each constraint,• n (cid:2) 2 denotes the number of variables,• d (cid:2) 2 denotes the size of each domain,• 1 (cid:2) p1 > 0 determines the number m = p1(n• 1 > p2 > 0 denotes the constraint tightness in terms of probability.k ) of constraints,To generate one instance P ∈ D(k, n, d, p1, p2), we have to build m constraints, each one formed by randomly select-ing (without repetition) a scope of k (distinct variables) and randomly selecting (with repetition) a relation such thateach one of the d k tuples is not allowed with probability p2.It has been shown [10] that the hardest random instances occur at a so-called phase transition between an under-constrained region where all instances are almost surely satisfiable and an over-constrained region where all problemsare almost surely unsatisfiable. This phase transition occurs when a control parameter is varied accordingly, usuallyp2 or κ which measures the constrainedness of an instance [17]. Furthermore, the peak of difficulty correspondsapproximately to the value of the control parameter where 50% of instances are satisfiable and is referred to as thethreshold or crossover point. The mushy region [34,39] denotes the range of values of the control parameter overwhich the phase transition takes place. Locating the phase transition has been addressed in [39,41].However, Achlioptas et al. [3] have identified a shortcoming of all four standard models. Indeed, they prove thatrandom problem instances generated using these models suffer from (trivial) insolubility as problem size increases.More precisely, they show that, asymptotically, if p2 (cid:2) 1/d, such instances almost surely contain a flawed variablewhen the number of variables increases while other parameters are kept constant. A flawed variable is a variable suchthat each value from its associated domain is flawed, i.e., is not consistent with respect to a constraint of the instance.Therefore, an instance involving a flawed variable is clearly unsatisfiable and this can be shown in polynomial time.518K. Xu et al. / Artificial Intelligence 171 (2007) 514–534To overcome the deficiency of standard models, several alternatives have been proposed. In [3], a model E is in-troduced by selecting, with probability p, each one of the d 2(n2) tuples (X, Y, a, b) where X and Y are two distinctvariables, a ∈ dom(X) and b ∈ dom(Y ). This new model is proved to be asymptotically interesting. However, it pro-vides less flexibility in the construction of the network and involves quickly generating a complete constraint grapheven if p is small. A generalized model has been proposed by Molloy [31] with the introduction of a probability dis-tribution in order to directly select constraints (instead of selecting allowed tuples, one by one). The author proves thatvery well behaved sets of constraints obtained with a probability distribution allow exhibiting a phase transition. Asusing such distributions can be awkward in practice, the issue of generating difficult instances for some distributionsis also addressed in [31].On the other hand, some works have focused on incorporating some “structure” in the generated random instances.Roughly speaking, the principle is to ensure that the generated instances be arc consistent [16] or (strongly) pathconsistent [15]. More precisely, Gent et al. [16] propose variants, called flawless models, of standard models whichprevent the presence of flawed values. They consider that “each value must be supported by at least one unique value,i.e., one value which is not also required to support another value” and achieve it by fixing in each binary constraint, aset S of d allowed pairs such that any pair t1, t2 of elements of S is such that t1[1] (cid:4)= t2[1] ∧ t1[2] (cid:4)= t2[2]. Then, theinstances generated according to flawless models are guaranteed to be arc-consistent and at any value of p2 < 1/2 donot suffer asymptotically from trivial insolubility”. Unluckily, they can be solved in polynomial time as they embedeasy sub-problems [15]. A generalization of this approach has then been proposed by Gao and Culberson [15]. Theauthors show that if each relation is chosen in such a way that the generated instances are strongly path consistent, thensuch instances admit an exponential resolution complexity no matter how large the constraint tightness is. By ensuringthe presence of a l-regular bipartite graph in each generated relation (with a sufficiently large l), the instances areguaranteed to be strongly path consistent. The main drawback of these approaches is that generating random instancesis no more quite a natural and easy task.Finally, in [14,38,44,45] standard models have been revised by controlling the way parameters change as theproblem size increases. The alternative model D scheme proposed by Smith in [38] guarantees the occurrence of aphase transition when some parameters are controlled and when the constraint tightness is within a certain range.The two revised models RB and RD introduced by Xu and Li in [44,45] provide the same guarantee by varying oneof two control parameters around a critical value that can be computed. In the next section, we will see that almostall instances of models RB and RD are hard, i.e., do not present resolution proofs of size less than exponential size.Also, Frieze and Molloy [14] identify a range of suitable parameter settings in order to exhibit a non-trivial thresholdof satisfiability. Their theoretical results apply to binary instances taken from model A and to “symmetric” binaryinstances from a so-called model B which, not corresponding to the standard one, associates the same relation withevery constraint.3. Models RB and RDIn this section, we introduce some theoretical results about two models defined in [44,45]. First, we present themodel RB that represents an alternative to model B. Note that in order to simplify the theoretical analysis, unlikemodel B, model RB allows selecting constraints with identical scopes. However, it may be that the results introducedbelow also hold for model B (i.e. when selection of scopes is performed without repetition). The reason for this is thatthe number of repeated constraints is asymptotically much smaller than the total number of constraints and thus canbe neglected in the analysis. But the main difference of model RB with respect to model B is that the domain size ofeach variable grows polynomially with the number of variables.Definition 4 (Model RB). A class of random CSP instances of model RB is denoted RB(k, n, α, r, p) where, for eachinstance:• k (cid:2) 2 denotes the arity of each constraint,• n (cid:2) 2 denotes the number of variables,• α > 0 determines the domain size d = nα of each variable,• r > 0 determines the number m = r.n. ln n of constraints,• 1 > p > 0 determines the number t = pd k of disallowed tuples of each relation.K. Xu et al. / Artificial Intelligence 171 (2007) 514–534519To generate one instance P ∈ RB(k, n, α, r, p), we have to build m constraints, each one formed by randomly selecting(with repetition) a scope of k (distinct) variables and randomly selecting (with repetition) a relation of t distinctdisallowed tuples.When fixed, α and r give an indication about the growth of the domain sizes and of the number of constraintsas n increases since d = nα and m = rn ln n, respectively. It is then possible, for example, to determine the criticalvalue pcr of p where the hardest instances must occur. Indeed, we have pcr = 1 − e−α/r which is equivalent to theexpression of pcr given in [39]. Note that nα, r.n. ln n and pd k may have to be rounded to the nearest integer.Another model, denoted model RD, is similar to model RB except that p denotes a probability instead of a propor-tion.Definition 5 (Model RD). A class of random CSP instances of model RD is denoted RD(k, n, α, r, p) where, for eachinstance:• k (cid:2) 2 denotes the arity of each constraint,• n (cid:2) 2 denotes the number of variables,• α > 0 determines the domain size d = nα of each variable,• r > 0 determines the number m = r.n. ln n of constraints,• 1 > p > 0 denotes the constraint tightness in terms of probability.To generate one instance P ∈ RD(k, n, α, r, p), we have to build m constraints, each one formed by randomly selecting(with repetition) a scope of k (distinct) variables and randomly selecting (with repetition) a relation such that each oneof the d k tuples is not allowed with probability p.For convenience, in this paper, although all given results hold for models RB and RD, we will exclusively refer tomodel RB. In [44], it is proved that model RB, under certain conditions, not only avoids trivial asymptotic behaviorsbut also guarantees exact phase transitions. More precisely, with Pr denoting a probability distribution, the followingtheorems hold.Theorem 1. If k, α > 1k and p (cid:3) k−1k are constants then(cid:5)(cid:3)P ∈ RB(k, n, α, r, p) is sat(cid:4)=1 if r < rcr0 if r > rcrPrlimn→∞where rcr = − αln(1−p) .Theorem 2. If k, α > 1k and pcr (cid:3) k−1k are constants then(cid:5)(cid:3)P ∈ RB(k, n, α, r, p) is sat(cid:4)=limn→∞Pr1 if p < pcr0 if p > pcrwhere pcr = 1 − e− αr .is equivalent to ke− αRemark that the condition pcr (cid:3) k−1kr (cid:2) 1 given in [44]. Theorems 1 and 2 indicate thata phase transition is guaranteed provided that the domain size is not too small and the constraint tightness or thethreshold value of the constraint tightness not too large. As an illustration, for instances involving binary constraints,the domain size is required to be greater than the square root of the number of variables (as α > 1/2 and d = nα) andthe constraint tightness or threshold value of the tightness is required to be at most 50%. The following table gives thelimits of Theorems 1 and 2 for different arities.k2345α> 1/2> 1/3> 1/4> 1/5p or pcr(cid:3) 1/2(cid:3) 2/3(cid:3) 3/4(cid:3) 4/5520K. Xu et al. / Artificial Intelligence 171 (2007) 514–534The next theorem establishes that unsatisfiable instances of model RB almost surely have the guarantee to be hard.A similar result for model A has been obtained by [14] with respect to binary instances (i.e. k = 2) and Mitchell [29]has described such kind of behavior for instances where constraints have fewer than d/2 disallowed tuples.Theorem 3. If P ∈ RB(k, n, α, r, p) and k, α, r and p are constants, then, almost surely,3 P has no tree-like resolutionof length less than 2(cid:4)(n).The proof, which is based on a strategy following some results of [6,30], is omitted but can be found in [45].To summarize, model RB guarantees exact phase transitions and hard instances at the threshold. It then contradictsthe statement of [15] about the requirement of an extremely low tightness for all existing random models in order tohave non-trivial threshold behaviors and guaranteed hard instances at the threshold.4. Random constraint networks in intentionIn Constraint Programming, most of the experiments performed so far about random constraint networks involvebinary instances. The main reason of this limitation is due to space complexity considerations. Indeed, the spacerequired to store the tuples allowed (or disallowed) by (the relation associated with) a constraint exponentially growswith its arity. For instance, at least 109 memory units are needed to store 1% of the tuples of a 10-ary constraint with10 values per domain since 108 tuples, each one composed of 10 values, must be stored. Hence, in practice, it is notpossible to represent large non-binary random networks when constraints are given in extension.In this section, we propose a variant (initially introduced in [24]) of model RD such that any network involvesconstraints defined in intention, i.e. by a predicate. As an immediate consequence, there is no more obstacle to generateproblem instances with constraints of any arity, and then to perform experiments on such instances.First, we show how it is possible to define a set C of random constraints defined in intention. Note that we consider,without loss of generality, that each constraint C in C has a unique identification number id(C).The function isConsistent (see Algorithm 1) can be viewed as the predicate defining all random constraints de-fined in intention of a problem instance. Indeed, for any k-ary constraint C and any k-tuple t, isConsistent(C, t, p)determines if the tuple t is allowed by C while considering tightness p. To achieve this, a unique real random valuebetween 0 and 1 (exclusive) is computed by a function f whose parameters are the identification number id(C) of Cand the given tuple t. Then, we have simply to compare the computed random value with the constraint tightness pseen as an acceptance boundary.The function f can be implemented in different ways. For instance, we can compute a seed from the parameters off , and then use this seed to get a real value using a pseudo-random number generator. Also, we can compute a hashvalue using a message digest algorithm like MD5 [35] or SHA [33]. These algorithms correspond to secure one-wayhash functions that take arbitrary-sized data, called documents, and output fixed-length hash values called fingerprintsor message digests. For any bit changed in a document, the best hash functions involves modifying at least 50% of thebits occurring in the computed fingerprint. In our context, the document corresponds to the parameters of f , namely,the identification number of the constraint and the given tuple.When using hash functions, one can obtain a real value by applying some simple operations as shown by Algo-rithm 2. After creating a document by simply concatenating the binary description of all parameters of f , a hashfunction (MD5, for instance) is called in order to generate a fingerprint. Then, the exclusive or operation (⊕) is itera-tively applied on each byte of the fingerprint. Finally, a real value between 0 and 1 is calculated by dividing the valueof the resulting byte by 256.It is important to note the correctness of this approach. First, isConsistent always produces the same result for agiven set of parameters since the same real value is always computed by f . Second, each tuple is considered, indepen-dently with probability p, as being not allowed by C. Remark that we need to introduce the constraint identificationnumber as a parameter of f in order to get distinct constraints of same arity.We are now in a position to define a variant, denoted RDint, of model RD that can be exploited in practice for anyarity.3 We say that a property holds almost surely when this property holds with probability tending to 1 as the number of variables tends to infinity.K. Xu et al. / Artificial Intelligence 171 (2007) 514–534521variable realRandomValue : realrealRandomValue ← f (id(C), t)return realRandomValue (cid:2) pAlgorithm 1. Function isConsistent(C: Constraint, t: tuple, p: 0..1): Boolean.variable b : bytevariable document : array of bytesvariable fingerprint : array of bytesdocument ← createDocumentWith(id,t)fingerprint ← MD5(document)b ← fingerprint[1]for i ranging from 2 to length(fingerprint) dob ← b ⊕ fingerprint[i]end forreturn b/256Algorithm 2. Function f (id: integer, t: tuple): real.Definition 6 (Model RDint). A class of random CSP instances of model RDint is denoted RDint(k, n, α, r, p) where,for each instance:• k (cid:2) 2 denotes the arity of each constraint,• n (cid:2) 2 denotes the number of variables,• α > 0 determines the domain size d = nα of each variable,• r > 0 determines the number m = r.n. ln n of constraints,• 1 > p > 0 denotes the constraint tightness in terms of probability.To generate one instance P ∈ RDint(k, n, α, r, p), we have to build m constraints in intention, each one formed by ran-domly selecting (with repetition) a scope of k (distinct) variables and (implicitly) defined by the function isConsistentdescribed above.This new model involves an easy implementation, no space requirement and the possibility to perform experimentswith large arity constraints.5. Generating hard satisfiable instancesFor CSP and SAT (whose task is to determine if a Boolean formula, also called SAT instance, is satisfiable), thereis a natural strategy to generate forced satisfiable instances, i.e., instances on which a solution is imposed. It sufficesto generate first a random (total) assignment t and then a random instance with n variables and m constraints (clausesfor SAT) such that any constraint violating t is rejected. t is then a forced solution.More precisely, the method that we have considered to generate forced satisfiable instances of model RB is asfollows:• generate a random solution t• generate m scopes (of constraints) of size k (with repetition)• for each constraint (of arity k), generate nb allowed tuples with nb being the nearest integer to (1 − p)d k– select as first allowed tuple the one that satisfies the solution t– select (without repetition) nb − 1 other tuples.44 For example, we can associate an integer with each tuple, put all such integers (except the one that satisfies the solution) into a set and randomlypicking (and removing) nb − 1 integers from this set.522K. Xu et al. / Artificial Intelligence 171 (2007) 514–534This strategy, quite simple and easy to implement, allows generating hard forced satisfiable instances of model RBprovided that Theorem 1 or 2 holds. Nevertheless, this statement deserves a theoretical analysis.Assuming that d denotes the domain size (d = 2 for SAT), we have exactly d n possible (total) assignments, de-noted by t1, t2, . . . , tdn , and d 2n possible assignment pairs where an assignment pair (cid:10)ti, tj (cid:11) is an ordered pair of twoassignments ti and tj . We say that (cid:10)ti, tj (cid:11) satisfies an instance if and only if both ti and tj satisfy the instance. Then,the expected (mean) number of solutions Ef [N] for instances that are forced to satisfy an assignment ti (which isthen a forced solution) is:Ef [N ] =dn(cid:6)j =1Pr[(cid:10)ti, tj (cid:11)]Pr[(cid:10)ti, ti(cid:11)]where Pr[(cid:10)ti, tj (cid:11)] denotes the probability that (cid:10)ti, tj (cid:11) satisfies a random instance. Note that Ef [N] should be indepen-dent of the choice of the forced solution ti . So we have:1(cid:2)i,j (cid:2)dn Pr[(cid:10)ti, tj (cid:11)]d n Pr[(cid:10)ti, ti(cid:11)]= E[N 2]E[N]Ef [N] =(cid:2).where E[N 2] and E[N] are, respectively, the second moment and the first moment of the number of solutions forrandom unforced instances.For random 3-SAT, it is known that the strategy mentioned above is unsuitable as it produces a biased sampling ofinstances with many solutions clustered around t [1]. Experiments show that forced satisfiable instances are much eas-ier to solve than unforced satisfiable instances. In fact, it is not hard to show (from the result on satisfying assignmentpairs in [42] that, asymptotically, E[N 2] is exponentially greater than E2[N]. The same conclusion can also be foundin [4]. Thus, the expected number of solutions for forced satisfiable instances is exponentially larger than the one forunforced satisfiable instances. It then gives a good theoretical explanation of why, for random 3-SAT, the strategy ishighly biased toward generating instances with many solutions.For model RB, recall that when the exact phase transitions were established [44], it was proved that E[N 2]/E2[N]is asymptotically equal to 1 below the threshold, where almost all instances are satisfiable, i.e. E[N 2]/E2[N] ≈ 1for r < rcr or p < pcr. Hence, the expected number of solutions for forced satisfiable instances below the thresholdis asymptotically equal to the one for unforced satisfiable instances, i.e. Ef [N] = E[N 2]/E[N] ≈ E[N]. In otherwords, when using model RB, the strategy has almost no effect on the number of solutions and does not lead to abiased sampling of instances with many solutions.In addition to the analysis above, we can also study the influence of the strategy on the distribution of solutionswith respect to the forced solution. Based on Hamming distance, we first define the distance d f (ti, tj ) between twoassignments ti and tj as the proportion of variables that have been assigned a different value in ti and tj . We have0 (cid:3) d f (ti, tj ) (cid:3) 1.For forced satisfiable instances of model RB, with Eδftance from the forced solution (identified as ti , here) is equal to δ, we obtain by an analysis similar to that in [44]:[N] denoting the expected number of solutions whose dis-dn(cid:6)Eδf[N] =(cid:8)Pr[(cid:10)ti, tj (cid:11)]Pr[(cid:10)ti, ti(cid:11)] with d f (ti, tj ) = δ(cid:9) (cid:10)n−nδ(cid:11) + (1 − p)(cid:10)knk(cid:11)(nα − 1)nδ(cid:10)(cid:10)1 − p + p(1 − δ)kr lnn ln n(cid:11)j =1(cid:7)=nnδ(cid:3)= exp(cid:7)1 −(cid:11)+ αδrn ln n(cid:11)(cid:8)(cid:12)(cid:10)n−nδ(cid:11)(cid:10)knk(cid:4)+ O(n).[N], for r < rcr or p < pcr, is asymptotically maximized when δIt can be shown, from the results in [44] that Eδftakes the largest possible value, i.e. δ = 1. The intuition behind this is that the value of Eδ[N] is determined by twoffactors: the first one is the number of assignments whose distance from the forced solution is δ, and the other one isthe probability of such an assignment being a solution. It is easy to see that the larger the distance, the smaller theprobability, which means that an assignment more similar to the forced solution is more likely to be a solution. On theother hand, the number of assignments with a distance δ from the forced solution grows with δ. Then, when r < rcror p < pcr, this factor plays a more dominant role than the probability factor as the number n of variables approachesK. Xu et al. / Artificial Intelligence 171 (2007) 514–534523infinity, which leads to the result as stated. It is worth mentioning that when r > rcr or p > pcr, the total number ofsolutions for a forced satisfiable instance might be greater than 1 and the dominant role will alternate between the[N] is asymptotically maximized at δ = 0. This implies that for a forcedabove two factors, yielding the result that Eδfsatisfiable instance to the right of the threshold, there may exist some solutions other than the forced one and thesesolutions are distributed very closely around the forced one.For unforced satisfiable instances of model RB, with Eδ[N] denoting the expected number of solutions whosedistance from ti (not necessarily a solution) is equal to δ, we have:(cid:7)(cid:8)Eδ[N] =nnδ(cid:3)= expn ln n(nα − 1)nδ(1 − p)rn ln n(cid:11)(cid:10)r ln(1 − p) + αδ(cid:4)+ O(n).It is straightforward to see that the same pattern holds for this case, i.e. Eδ[N] is asymptotically maximized whenδ = 1.Intuitively, with model RB, both unforced satisfiable instances and instances forced to satisfy an assignment t aresuch that most of their solutions distribute far from t. This indicates that, for model RB, the strategy has little effecton the distribution of solutions, and is not biased towards generating instances with many solutions around the forcedone.For random 3-SAT, we have:Eδf[N] =(cid:7)(cid:8)(cid:9) (cid:10)nnδ(cid:11)(cid:11) + 67n−nδ(cid:10)3n3(cid:7)1 −(cid:11)(cid:8)(cid:12)rn(cid:10)n−nδ(cid:11)(cid:10)3n3(cid:7)(cid:9)= poly(n) exp(cid:4)(cid:3)= poly(n) expnf1(r, δ)n−δ ln δ − (1 − δ) ln(1 − δ) + r lnandEδ[N] =(cid:8)(cid:7)(cid:7)nnδ78(cid:8)rn(cid:9)(cid:7)= poly(n) expn−δ ln δ − (1 − δ) ln(1 − δ) + r ln= poly(n) exp(cid:4)(cid:3)nf2(r, δ).(cid:8)(cid:12)6 + (1 − δ)37(cid:8)(cid:12)78[N] and Eδ[N] are asymptotically determined by f1(r, δ) andFrom the above two equations, we can see that Eδff2(r, δ) respectively. Below are two figures (see Figs. 1 and 2) for f1(r, δ) and f2(r, δ) as a function of δ with r = 4.25(the ratio of clauses to variables). It follows from these two figures that as r approaches 4.25, f1(r, δ) and f2(r, δ)are maximized when δ ≈ 0.24 and δ = 0.5, respectively. This means, in contrast to model RB, that when r is near thethreshold, most solutions of forced instances distribute in a place much closer to the forced solution than solutions ofunforced satisfiable instances.6. Solving constraint networksBefore presenting the results of our experimentation, we need to introduce the algorithms that we have used. Tosolve a CSP instance, a depth-first search algorithm with backtracking can then be applied, where at each step ofthe search, a variable assignment is performed followed by a filtering process called constraint propagation. Usually,domains are filtered (reduced) by considering some properties of constraint networks. Such properties are calleddomain filtering consistencies [12] and Generalized Arc Consistency (GAC) remains the central one. For binaryconstraints, this property is simply called Arc Consistency (AC).524K. Xu et al. / Artificial Intelligence 171 (2007) 514–534Fig. 1. f1(r, δ) as a function of δ with r = 4.25.Fig. 2. f2(r, δ) as a function of δ with r = 4.25.Definition 7. Let P = (X, C) be a CN. A pair (X, a), with X ∈ X and a ∈ dom(X), is generalized arc consistent(GAC) iff ∀C ∈ C | X ∈ vars(C), there exists a support of (X, a) in C, i.e., a tuple5 t ∈ rel(C) such that t[X] = a andt[Y ] ∈ dom(Y ) ∀Y ∈ vars(C). P is GAC iff ∀X ∈ X, dom(X) (cid:4)= ∅ and ∀a ∈ dom(X), (X, a) is GAC.Establishing Generalized Arc Consistency on a given network P involves removing all values that are not GAC.Many generic algorithms have been proposed to establish AC and GAC (e.g. see [8,23,27]). When constraints aregiven in extension (usually under the form of tables), it is possible to adopt different propagation schemes [7,26].However, for instances of model RDint, one necessary needs to use the so-called GAC-valid scheme [7,26].5 t[X] denotes the value assigned to X in t .K. Xu et al. / Artificial Intelligence 171 (2007) 514–5345251: P (cid:15) ← GAC(P )2: if P (cid:15) (cid:4)= ⊥ then3:4: end ifsearchMGAC(P (cid:15))Algorithm 3. MGAC(P = (X, C): Constraint Network).select a pair (X, a) such that X ∈ X and a ∈ dom(X)1: if X = ∅ then2:return true3: else4:5: P (cid:15) ← GAC(P |X=a )if P (cid:15) (cid:4)= ⊥ then6:7:8:9: P (cid:15) ← GAC(P |X(cid:4)=a )if P (cid:15) (cid:4)= ⊥ then10:11:12:13:14: end ifreturn falsereturn trueif searchMGAC(P (cid:15)if searchMGAC(P (cid:15)\X) thenwrite(X, a); return true) thenAlgorithm 4. searchMGAC(P = (X, C): Constraint Network): Boolean.GAC(P ) will denote the constraint network obtained after enforcing GAC on a given constraint network P . If thereis a variable with an empty domain in GAC(P ), denoted GAC(P ) = ⊥, then P is clearly unsatisfiable.The MGAC algorithm, i.e., the algorithm which maintains GAC during the search of a solution, is nowadaysconsidered as the best generic algorithm to solve CSP instances (provided that the arity of the constraints be not toohigh). For binary instances (i.e. instances only involving binary constraints), it is called MAC [36]. Algorithms 3and 4 correspond to a recursive version of MGAC (using 2-way branching). One can solve a CSP instance by callingthe MGAC function: one solution is displayed iff the instance is satisfiable. Before giving more details about thealgorithm, we need to introduce the following notations. P |X=a denotes the constraint network obtained from P byrestricting the domain of X to the singleton {a} whereas P |X(cid:4)=a denotes the constraint network obtained from P byremoving the value a from the domain of X. P \X denotes the constraint network obtained from P by removing thevariable X (it means that, not only, X is removed from X but also that X is eliminated from any constraint involvingit by projecting the associated relation over all other involved variables). The recursive algorithm is started by callingthe MGAC function (Algorithm 3) with a constraint network to be solved. If the given constraint network P can bemade generalized arc consistent then the search for a solution begins (Algorithm 4). It involves selecting a pair (X, a)and trying first X = a and then X (cid:4)= a (if no solution has been found with X = a). After any consistent assignment,the assigned variable is eliminated from the network and search is continued (line 7). When the constraint networkbecomes empty (line 1), it means that a solution has been found and that the search can be stopped by returning true(lines 2, 8 and 12).MGAC is a complete search algorithm. It means that one can use this algorithm to find a solution or prove that nosolution exists. However, there exists another category of algorithms which are said to be incomplete. Such algorithmsmay find solutions but cannot prove unsatisfiability. They have been shown to be very efficient on some domains ofapplications. Local search algorithms belong to this category. The principle of local search is to move in the spaceof all possible interpretations (here, an interpretation is the assignment of a value to all variables) until a solution isfound. Usually, a move is selected in the neighborhood of the current interpretation and corresponds to a maximumimprovement of a cost function.Algorithms 5 and 6 correspond to a general description of a local search algorithm called TABU. The algorithmis started by calling the TABU function (Algorithm 5) with a constraint network to be solved. If the given constraintnetwork P can be made arc consistent then the search for a solution begins (Algorithm 6). It involves performingseveral runs such that, initially, for each run, a tabu list is initialized and an initial random interpretation is generated(the function rand randomly selects a value in the domain of the given variable). Then, until the current interpretationcorresponds to a solution or the number of performed moves reaches the number of allowed moves for the current526K. Xu et al. / Artificial Intelligence 171 (2007) 514–5341: P (cid:15) ← GAC(P )2: if P (cid:15) (cid:4)= ⊥ then3:4: end ifsearchTABU(P (cid:15))Algorithm 5. TABU(P = (X, C): Constraint Network).select a pair (X, a) s.t. X ∈ X, a ∈ dom(X), (X, a) /∈ sol and (X, a) /∈ tabureplace (X, b) ∈ sol with (X, a)if sol ∈ sol(P ) thenwrite(sol); returntabu ← ∅sol ← {(X, rand(X)) | X ∈ X}for j ranging from 1 to maxMoves do1: for i ranging from 1 to maxRuns do2:3:4:5:6:7:8:9:10:11:12:13:14:15: end forend iftabu ← tabu ∪ {(X, b)}end ifif isFull(tabu) thentabu ← tabu\firstIn(tabu)end forAlgorithm 6. searchTABU(P = (X, C): Constraint Network).run, a neighbor is selected. Here, the neighborhood is composed of all interpretations which differ from the currentinterpretation by exactly one value. The role of the tabu list is to avoid becoming stuck in local minima by repeatedlyperforming the same cycle of moves. Note that the selected move (represented by a pair (X, b)) cannot correspondto a move recently performed and recorded in the tabu list. After each move, we remove the oldest element of thetabu list (the function firstIn returns this element) if the maximum capacity of the list is reached (the function isFulldetermines if this is the case), and we add this new move to the list.The performance of the MGAC and TABU algorithms highly depend on the heuristics used to select the next pair(X, a) in the main loop of both algorithms. It has been shown that constraint weighting can be exploited in order toconcentrate search on the hard parts of the constraint network. It does respect the fail-first principle: “To succeed, tryfirst where you are most likely to fail” [19]. The idea is to associate a weight with any constraint C and to incrementthe weight of C whenever C is violated during search. As search progresses, the weight of hard constraints becomemore and more important and this particularly helps the heuristic to select variables appearing in the hard part of thenetwork. More information can be found in [9,25,28,32,37,40].7. Experimental resultsAs all introduced theoretical results hold when n → ∞, the practical exploitation of these results is an issue thatmust be addressed. In this section, we give some representative experimental results which indicate that practice meetstheory even if the number n of variables is small. Note that different values of parameters α and r have been selectedin order to illustrate the broad spectrum of applicability of model RB (and model RD). The platform that we haveused for our experimentation (conducted on a PC Pentium IV 2.4 GHz 512Mo under Linux) is called Abscon (seehttp://www.cril.univ-artois.fr/~lecoutre).Theorems 1 and 2 guarantee asymptotically the presence of a phase transition and the exact localization of thethreshold. First, it is valuable to know in practice, to what extent, Theorems 1 and 2 give precise thresholds accordingto different values of α, r and n. The experiments that we have run wrt Theorem 2, as depicted in Fig. 3, suggest thatall other parameters being fixed, the greater the value of α, r or n is, the more precise Theorem 2 is. More precisely,in Fig. 3, the difference between the threshold theoretically located and the threshold experimentally determined isplotted against α ∈ [0.2, 1] (d ∈ [2..20]), against r ∈ [0.8, 2.5] (m ∈ [50..150]) and against n ∈ [8..100]. Note thatthe vertical scale refers to the difference in constraint tightness and that the horizontal scale is normalized (value 0respectively corresponds to n = 8, α = 0.2 and r = 0.8, etc.).K. Xu et al. / Artificial Intelligence 171 (2007) 514–534527Fig. 3. Difference between theoretical and experimental thresholdsagainst α, r and n.Fig. 4. Mean search cost (50 instances) of solving instances in RB(2,{20, 30, 40}, 0.8, 3, p) with MAC.To solve the random instances generated by model RB, we have used the MGAC and TABU algorithms described inprevious section. For MGAC, we have used dom/wdeg [9,25] as variable ordering heuristic, lexico as value orderingheuristic and GAC3.2 [23] as embedded arc consistency algorithm. For TABU, we have used constraint weighting[32,37,40] to perform local moves, set the size of the tabu list to 25 and the maximum number of moves of any run to150,000. Also, every three runs, constraint weights are reset to 0.We have studied the difficulty of solving with MAC the binary instances of model RB generated around the the-oretical threshold pcr ≈ 0.23 given by Theorem 2 for k = 2, α = 0.8, r = 3 and n ∈ {20, 30, 40}. It means that forn = 20, we have d = 11 and m = 180, for n = 30, we have d = 15 and m = 306, and for n = 40, we have d = 19 andm = 443. In Fig. 4, it clearly appears that the hardest instances are located quite close to the theoretical threshold andthat the difficulty grows exponentially with n (note the use of a log scale). A similar behavior is observed in Fig. 5with respect to ternary instances generated around the theoretical threshold pcr ≈ 0.63 for k = 3, α = 1, r = 1 andn ∈ {16, 20, 24}. It means that for n = 16, we have d = 16 and m = 44, for n = 20, we have d = 20 and m = 60, andfor n = 24, we have d = 24 and m = 76.Figs. 6 and 7 show the results obtained with a tabu search with respect to similar instances (compare with Figs. 4and 5). In fact, for ternary instances, we have limited the number of variables to n ∈ {15, 18, 21}. The search effort isgiven by a median cost since when using an incomplete method, there is absolutely no guarantee of finding a solutionin a given limit of time. Remark that all unsatisfiable (unforced) instances below the threshold have been filtered outin order to make a fair comparison. It appears that both complete and incomplete methods behave similarly: the searcheffort grows exponentially with n and forced instances are (almost) as hard as unforced satisfiable ones.To deal with classes of instances of larger arities, we have used the model presented at Section 4. We have studiedthe difficulty of solving with MGAC the instances of model RDint generated around the theoretical threshold pcr ≈0.45 given by Theorem 2 for k ∈ {6, 8}, α = 0.6, r = 1 and n ∈ {10, 15}. It means that for n = 10, we have d = 4 andm = 23, and for n = 15, we have d = 5 and m = 40. In Fig. 8, one can observe that forced instances are as hard asunforced satisfiable ones. Unsurprisingly, unforced unsatisfiable instances are more difficult.It is also interesting to observe the behavior of MGAC when small values of α and r are considered. This is thereason why we have studied the difficulty of solving with MAC the instances of model RD generated around thetheoretical threshold pcr ≈ 0.39 given by Theorem 2 for k = 2, α = 0.51, r = 1 and n ∈ {34, 46, 59, 74, 91, 110}.528K. Xu et al. / Artificial Intelligence 171 (2007) 514–534Fig. 5. Mean search cost (50 instances) of solving instances in RB(3,{16, 20, 24}, 1, 1, p) with MGAC.Fig. 6. Median search cost (50 instances) of solving instances in RB(2,{20, 30, 40}, 0.8, 3, p) using a tabu search.It means that for n = 34, we have d = 6 and m = 120, for n = 46, we have d = 7 and m = 176, for n = 59, wehave d = 8 and m = 241, for n = 74, we have d = 9 and m = 319, for n = 91, we have d = 10 and m = 411, forn = 100, we have d = 11 and m = 517. Fig. 9 shows the obtained results (note that n is in {34, 46, 59, 74, 91, 110}from bottom to top). It shows the wide applicability of the main result of this paper (i.e. forced instances are almostas hard as unforced instances as long as values of parameters are within the range allowed by the theorems) and thatsmaller values of α and r make the instances at the threshold much easier to solve, which implies that in such a case,we have to use a large number of variables to get hard instances. Fig. 10 depicts the phase transitions (note that nis in {34, 46, 59, 74, 91, 110} from left to right). As expected, when n is increased, the size of the phase transitiondecreases.As the number and the distribution of solutions are the two most important factors determining the cost of solvingsatisfiable instances, we can expect, from the analysis given in Section 5, that for model RB, the hardness of solvingforced satisfiable instances should be similar to that of solving unforced satisfiable ones. This is what is observed inFig. 4 for example. To confirm this, we have focused our attention to a point just below the threshold as we havethen some (asymptotic) guarantee about the difficulty of both unforced and forced instances (see Theorems 5 and 6in [45]) and the possibility of generating easily unforced satisfiable instances. Fig. 11 shows the difficulty of solvingwith MAC both forced and unforced instances of model RB at pcr − 0.01 ≈ 0.40 for k = 2, α = 0.8, r = 1.5 andn ∈ [20..50].To confirm the inherent difficulty of the (forced and unforced) instances generated at the threshold, we have alsostudied the runtime distribution produced by a randomized search algorithm on distinct instances [18]. For eachinstance, we have performed 5000 independent runs. Fig. 12 displays the survival function, which corresponds to theprobability of a run taking more than x backtracks, of a randomized MAC algorithm for two representative instancesgenerated at pcr ≈ 0.41 for k = 2, α = 0.8, r = 1.5 and n ∈ {40, 45}. One can observe that the runtime distribution(a log–log scale is used) do not correspond to an heavy-tailed one, i.e., a distribution characterized by an extremelylong tail with some infinite moment. It means that all runs behave homogeneously and, therefore, it suggests that theinstances are inherently hard [18].Then, we have focused on unforced unsatisfiable instances of model RB as Theorem 3 indicates that such instanceshave an exponential resolution complexity. We have generated unforced and forced instances with different constraintK. Xu et al. / Artificial Intelligence 171 (2007) 514–534529Fig. 7. Median search cost (50 instances) of solving instances in RB(3,{15, 18, 21}, 0.8, 3, p) using a tabu search.Fig. 8. Mean search cost (50 instances) of solving instances inRDint({6, 8}, {10, 15}, 0.6, 1, p) using MGAC.Fig. 9. Mean search cost (50 instances) of solving instances in RD(2,{34, 46, 59, 74, 91, 110}, 0.51, 1, p) using MAC.Fig. 10. Phase transitions for RD(2, {34, 46, 59, 74, 91, 110}, 0.51,1, p).530K. Xu et al. / Artificial Intelligence 171 (2007) 514–534Fig. 11. Mean search cost (50 instances) of solving instances in RB(2, [20..50], 0.8, 1.5, pcr − 0.01) using MAC.Fig. 12. Non heavy-tailed regime for instances in RB(2, {40, 45}, 0.8, 1.5, pcr ≈ 0.41).K. Xu et al. / Artificial Intelligence 171 (2007) 514–534531Fig. 13. Mean search cost (50 instances) of solving instances in RB(2, [20..450], 0.8, 1.5, p) using MAC.tightness p above the threshold pcr ≈ 0.41 for k = 2, α = 0.8, r = 1.5 and n ∈ [20..450]. Fig. 13 displays the searcheffort of a MAC algorithm to solve such instances against the number of variables n. It is interesting to note that thesearch effort grows exponentially with n, even if the exponent decreases as the tightness increases. Also, althoughnot currently supported by any theoretical result (Theorems 5 and 6 of [45] hold only for forced instances below thethreshold) it appears here that forced and unforced instances have a similar hardness.To further evaluate the hardness of forced satisfiable instances of model RB, we can perform some experimentswhere the local search algorithm starts with a biased initial assignment which agrees with the hidden assignment on 10,30 and 50% variables respectively. Please note that this happens with exponentially small probability. More precisely,for model RB, it is easy to show that for any constant 0 < c (cid:3) 1, the probability that a random initial assignmentagrees with the hidden assignment on at least cn variables is(cid:7)(cid:8)(cid:7)(cid:8)(cid:8)(cid:7)(cid:7)(cid:7)(cid:8)(cid:8)n(cid:6)i=cnnii1nα1 − 1nαn−in(cid:6)2n<i=cn1nαcn< nn,2nαcwhich converges to 0 exponentially fast as n tends to infinity.In Fig. 14, it appears that unlike random 3-SAT with two complementary hidden solutions [2], the biased initialassignment does not render forced instances of model RB much easier to solve. There is a significant difference, onlywhen the bias reaches 50%.Finally, we will mention that some instances of model RB were used for different solver competitions. Someforced ternary CSP instances of class RB(3, {20, 24, 28}, 1, 1, ≈ 0.63) were used as benchmarks of the 2005 CSPsolver competition. No CSP solver succeeded in solving one instance for n = 28 within 10 minutes. Forced binaryCSP instances of class RB(2, n, 0.8, 0.8/ ln 43 , 0.25) with n ranging from 40 to 59 were also used as benchmarksof this competition, but no solver succeeded in solving one instance at n = 53 (within 10 minutes). These binaryinstances were also encoded into SAT (using the direct encoding method) and submitted to the SAT competition2004 (http://www.nlsde.buaa.edu.cn/~kexu/benchmarks/benchmarks.htm). About 50% of the competing solvers havesucceeded in solving the SAT instances corresponding to n = 40 (d = 19 and m = 410) whereas only one solver hasbeen successful for n = 50 (d = 23 and m = 544).532K. Xu et al. / Artificial Intelligence 171 (2007) 514–534Fig. 14. Median search cost (100 instances) of solving forced instances in RB(2, 40, 0.8, 3, p) using TABU. Initial assignments agree with forcedsolutions on 10, 30 and 50% of the variables.8. Related workIn this paper, we have tried to emphasize the nice theoretical and practical properties of models RB. But, as alreadymentioned in Section 2, other models of random CSP instances exist. However, we believe that all these models donot offer a framework which is as simple as the one proposed by model RB.As a related work, we can mention the recent progress on generating hard satisfiable SAT instances. In [5,21], itis proposed to build random satisfiable 3-SAT instances on the basis of a spin-glass model from statistical physics.Instances are generated while using different probabilities in order to select clauses with 0, 1 or 2 negative literals. An-other approach, quite easy to implement, has also been proposed in [2]: any 3-SAT instance is forced to be satisfiableby forbidding the clauses violated by both an assignment and its complement.Finally, let us mention [1] which propose to build random instances with a specific structure, namely, instances ofthe Quasigroup With Holes (QWH) problem. The hardest instances belong to a new type of phase transition, definedfrom the number of holes, and coincide with the size of the backbone.9. Conclusions and future workIn this paper, we have first shown that the model(s) RB (and RD) can be used to produce, very easily, hard randominstances, i.e., instances whose hardness grows exponentially with the problem size. Importantly, we have demon-strated that the same result holds for instances that are forced to be satisfiable, as long as the existence of the exactphase transition is guaranteed. Then, we have shown that for a wide range of parameter values, practice meets theory.Among other things, our experimental results have confirmed the phase transitions and thresholds predicted by theory,the hardness of forced and unforced satisfiable instances as well as the non heavy-tailed regime for instances of ModelRB.We believe that such results should be of use and value for experimental studies on random CSP instances. We alsothink that although there are some other ways to generate hard satisfiable instances, e.g. QWH [1] or 2-hidden [2]instances, the simple and natural method presented in this paper, based on model RB, should be well worth furtherinvestigation. We propose some interesting directions for future work, including:K. Xu et al. / Artificial Intelligence 171 (2007) 514–534533(1) extending model RB to allow the easy generation of hard random instances involving constraints of differentarities and/or global constraints;(2) investigating if, for model RB, forced instances are almost as hard as unforced ones when there are more than oneforced solution;(3) proving complexity lower bounds for forced and unforced satisfiable instances of model RB.AcknowledgementsWe would like to thank the AIJ anonymous referees for their helpful comments and suggestions. We also wish tothank the anonymous referees of IJCAI’05 for their comments on an earlier version of this paper.References[1] D. Achlioptas, C. Gomes, H. Kautz, B. Selman, Generating satisfiable problem instances, in: Proceedings of AAAI’00, 2000, pp. 256–301.[2] D. Achlioptas, H. Jia, C. Moore, Hiding satisfying assignments: two are better than one, in: Proceedings of AAAI’04, 2004, pp. 131–136.[3] D. Achlioptas, L.M. Kirousis, E. Kranakis, D. Krizanc, M.S.O. Molloy, Y.C. Stamatiou, Random constraint satisfaction: a more accuratepicture, in: Proceedings of CP’97, 1997, pp. 107–120.[4] D. Achlioptas, C. Moore, The asymptotic order of the random k-SAT threshold, in: Proceedings of FOCS’02, 2002, pp. 779–788.[5] W. Barthel, A.K. Hartmann, M. Leone, F. Ricci-Tersenghi, M. Weigt, R. Zecchina, Hiding solutions in random satisfiability problems: a sta-tistical mechanics approach, Physical Review Letters 88 (18) (2002).[6] E. Ben-Sasson, A. Wigderson, Short proofs are narrow—resolution made simple, Journal of the ACM 48 (2) (2001) 149–169.[7] C. Bessiere, J. Régin, Arc consistency for general constraint networks: preliminary results, in: Proceedings of IJCAI’97, 1997, pp. 398–404.[8] C. Bessiere, J.C. Régin, R.H.C. Yap, Y. Zhang, An optimal coarse-grained arc consistency algorithm, Artificial Intelligence 165 (2) (2005)165–185.[9] F. Boussemart, F. Hemery, C. Lecoutre, L. Sais, Boosting systematic search by weighting constraints, in: Proceedings of ECAI’04, 2004,pp. 146–150.[10] P. Cheeseman, B. Kanefsky, W.M. Taylor, Where the really hard problems are, in: Proceedings of IJCAI’91, 1991, pp. 331–337.[11] S.A. Cook, D.G. Mitchell, Finding Hard Instances of the Satisfiability Problem: A Survey, DIMACS Series in Discrete Mathematics andTheoretical Computer Science, vol. 35, 1997.[12] R. Debruyne, C. Bessiere, Domain filtering consistencies, Journal of Artificial Intelligence Research 14 (2001) 205–230.[13] O. Dubois, Y. Boufkhad, J. Mandler, Typical random 3-sat formulae and the satisfiability threshold, in: Proceedings of SODA’00, 2000,pp. 126–127.[14] A.M. Frieze, M. Molloy, The satisfiability threshold for randomly generated binary constraint satisfaction problems, in: Proceedings of Ran-dom’03, 2003, pp. 275–289.[15] Y. Gao, J. Culberson, Consistency and random constraint satisfaction models with a high constraint tightness, in: Proceedings of CP’04, 2004,pp. 17–31.[16] I.P. Gent, E. MacIntyre, P. Prosser, B.M. Smith, T. Walsh, Random constraint satisfaction: flaws and structure, Journal of Constraints 6 (4)(2001) 345–372.[17] I.P. Gent, E. MacIntyre, P. Prosser, T. Walsh, The constrainedness of search, in: Proceedings of AAAI-96, 1996, pp. 246–252.[18] C.P. Gomes, C. Fernández, B. Selman, C. Bessiere, Statistical regimes across constrainedness regions, in: Proceedings of CP’04, 2004, pp. 32–46.[19] R.M. Haralick, G.L. Elliott, Increasing tree search efficiency for constraint satisfaction problems, Artificial Intelligence 14 (1980) 263–313.[20] R. Impagliazzo, L. Levin, M. Luby, Pseudo-random number generation from one-way functions, in: Proceedings of STOC’89, 1989, pp. 12–24.[21] H. Jia, C. Moore, B. Selman, From spin glasses to hard satisfiable formulas, in: Proceedings of SAT’04, 2004.[22] A.C. Kaporis, L.M. Kirousis, E.G. Lalas, The probabilistic analysis of a greedy satisfiability algorithm, in: Proceedings of ESA’02, 2002,pp. 574–585.[23] C. Lecoutre, F. Boussemart, F. Hemery, Exploiting multidirectionality in coarse-grained arc consistency algorithms, in: Proceedings of CP’03,2003, pp. 480–494.[24] C. Lecoutre, F. Boussemart, F. Hemery, Implicit random CSPs, in: Proceedings of ICTAI’03, 2003, pp. 482–486.[25] C. Lecoutre, F. Boussemart, F. Hemery, Backjump-based techniques versus conflict-directed heuristics, in: Proceedings of ICTAI’04, 2004,pp. 549–557.[26] C. Lecoutre, R. Szymanek, Generalized arc consistency for positive table constraints, in: Proceedings of CP’06, 2006, pp. 284–298.[27] A.K. Mackworth, Consistency in networks of relations, Artificial Intelligence 8 (1) (1977) 99–118.[28] B. Mazure, L. Sais, E. Gregoire, Boosting complete techniques thanks to local search methods, Annals of Mathematics and Artificial Intelli-gence 22 (1998) 319–331.[29] D.G. Mitchell, Some random CSPs are hard for resolution, 2000, submitted for publication.[30] D.G. Mitchell, Resolution complexity of random constraints, in: Proceedings of CP’02, 2002, pp. 295–309.[31] M. Molloy, Models for random constraint satisfaction problems, SIAM Journal of Computing 32 (4) (2003) 935–949.[32] P. Morris, The breakout method for escaping from local minima, in: Proceedings of AAAI’93, 1993, pp. 40–45.534K. Xu et al. / Artificial Intelligence 171 (2007) 514–534[33] NIST, Secure Hash Standard. National Institute of Standards and Technology, http://csrc.nist.gov/publications/fips/fips180-2/fips180-2.pdf,2002. FIPS 180-2.[34] P. Prosser, An empirical study of phase transitions in binary constraint satisfaction problems, Artificial Intelligence 81 (1996) 81–109.[35] R. Rivest, The MD5 message-digest algorithm, MIT Laboratory for Computer Science and RSA Data Security, Inc., 1992. Request forComments 1321.[36] D. Sabin, E. Freuder, Contradicting conventional wisdom in constraint satisfaction, in: Proceedings of CP’94, 1994, pp. 10–20.[37] B. Selman, H. Kautz, Domain-independent extensions to GSAT: solving large structured satisfiability problems, in: Proceedings of IJCAI’93,1993, pp. 290–295.[38] B.M. Smith, Constructing an asymptotic phase transition in random binary constraint satisfaction problems, Theoretical Computer Science 265(2001) 265–283.[39] B.M. Smith, M.E. Dyer, Locating the phase transition in binary constraint satisfaction problems, Artificial Intelligence 81 (1996) 155–181.[40] J.R. Thornton, Constraint weighting local search for constraint satisfaction, PhD thesis, Griffith University, Australia, 2000.[41] C. Williams, T. Hogg, Exploiting the deep structure of constraint problems, Artificial Intelligence 70 (1994) 73–117.[42] K. Xu, A study on the phase transitions of SAT and CSP (in Chinese), PhD thesis, Beihang University, 2000.[43] K. Xu, F. Boussemart, F. Hemery, C. Lecoutre, A simple model to generate hard satisfiable instances, in: Proceedings of IJCAI’05, 2005,pp. 337–342.[44] K. Xu, W. Li, Exact phase transitions in random constraint satisfaction problems, Journal of Artificial Intelligence Research 12 (2000) 93–103.[45] K. Xu, W. Li, Many hard examples in exact phase transitions with application to generating hard satisfiable instances, Technical report, CoRRReport cs.CC/0302001, Revised version in Theoretical Computer Science 355 (2006) 291–302.