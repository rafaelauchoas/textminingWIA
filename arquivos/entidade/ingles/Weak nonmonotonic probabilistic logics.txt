Artificial Intelligence 168 (2005) 119–161www.elsevier.com/locate/artintWeak nonmonotonic probabilistic logics ✩Thomas Lukasiewicz 1Dipartimento di Informatica e Sistemistica, Università di Roma “La Sapienza”,Via Salaria 113, 00198 Rome, ItalyReceived 1 October 2004; accepted 31 May 2005Available online 5 July 2005AbstractWe present an approach where probabilistic logic is combined with default reasoning from condi-tional knowledge bases in Kraus et al.’s System P , Pearl’s System Z, and Lehmann’s lexicographicentailment. The resulting probabilistic generalizations of default reasoning from conditional knowl-edge bases allow for handling in a uniform framework strict logical knowledge, default logicalknowledge, as well as purely probabilistic knowledge. Interestingly, probabilistic entailment in Sys-tem P coincides with probabilistic entailment under g-coherence from imprecise probability assess-ments. We then analyze the semantic and nonmonotonic properties of the new formalisms. It turnsout that they all are proper generalizations of their classical counterparts and have similar propertiesas them. In particular, they all satisfy the rationality postulates of System P and some Conditioningproperty. Moreover, probabilistic entailment in System Z and probabilistic lexicographic entailmentboth satisfy the property of Rational Monotonicity and some Irrelevance property, while probabilis-tic entailment in System P does not. We also analyze the relationships between the new formalisms.Here, probabilistic entailment in System P is weaker than probabilistic entailment in System Z,which in turn is weaker than probabilistic lexicographic entailment. Moreover, they all are weakerthan entailment in probabilistic logic where default sentences are interpreted as strict sentences.Under natural conditions, probabilistic entailment in System Z and lexicographic entailment evencoincide with such entailment in probabilistic logic, while probabilistic entailment in System P does✩ This paper is a significantly extended and revised version of a paper in: Proceedings of the 9th InternationalConference on Principles of Knowledge Representation and Reasoning (KR2004), Whistler, Canada, June 2004,AAAI Press, 2004, pp. 141–151.E-mail address: lukasiewicz@dis.uniroma1.it (T. Lukasiewicz).1 Alternate address: Institut für Informationssysteme, Technische Universität Wien, Favoritenstraße 9-11, 1040Vienna, Austria; e-mail: lukasiewicz@kr.tuwien.ac.at.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.005120T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161not. Finally, we also present algorithms for reasoning under probabilistic entailment in System Z andprobabilistic lexicographic entailment, and we give a precise picture of its complexity. 2005 Elsevier B.V. All rights reserved.Keywords: Probabilistic logic; Default reasoning from conditional knowledge bases; Entailment in System P ;Entailment in System Z; Lexicographic entailment; Nonmonotonic probabilistic logics; Inconsistencyhandling; Algorithms; Computational complexity1. IntroductionDuring the recent decades, reasoning about probabilities has started to play an importantrole in AI. In particular, reasoning about interval restrictions for conditional probabilities,also called conditional constraints [49], has been a subject of extensive research efforts.Roughly, a conditional constraint is of the form (ψ|φ)[l, u], where ψ and φ are events, and[l, u] is a subinterval of the unit interval [0, 1]. It encodes that the conditional probabilityof ψ given φ lies in [l, u].An important approach for handling conditional constraints is probabilistic logic, whichhas its origin in philosophy and logic, and whose roots can be traced back to alreadyBoole in 1854 [12]. There is a wide spectrum of formal languages that have been exploredin probabilistic logic, ranging from constraints for unconditional and conditional eventsto rich languages that specify linear inequalities over events (see especially the work byNilsson [54,55], Fagin et al. [19], Dubois and Prade et al. [2,13,16,17], Frisch and Had-dawy [21], and the author [48,49,51]; see also the survey on sentential probability logic byHailperin [35]). The main decision and optimization problems in probabilistic logic are de-ciding satisfiability, deciding logical consequence, and computing tight logically entailedintervals. Recently, column generation techniques from operations research have been suc-cessfully used to solve large problem instances in probabilistic logic (see especially thework by Jaumard et al. [37] and Hansen et al. [36]).Example 1.1 (Eagles). A simple collection of conditional constraints KB may encode thestrict logical knowledge “all eagles are birds” and “all birds have feathers” as well asthe purely probabilistic knowledge “birds fly with a probability of at least 0.95” (cf. Ex-ample 2.1). This collection of conditional constraints KB is satisfiable, and some logicalconsequences in probabilistic logic from KB are “all birds have feathers”, “birds fly with aprobability of at least 0.95”, “all eagles have feathers”, and “eagles fly with a probabilitybetween 0 and 1”; in fact, these are the tightest intervals that follow from KB (cf. Exam-ple 2.2). That is, we especially cannot conclude anything from KB about the ability to flyof eagles.A closely related research area is default reasoning from conditional knowledge bases,which consist of a collection of strict statements in classical logic and a collection of defea-sible rules, also called defaults. The former must always hold, while the latter are rules ofthe kind ψ ← φ, which read as “generally, if φ then ψ”. Such rules may have exceptions,which can be handled in different ways.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161121The literature contains several different proposals for default reasoning from conditionalknowledge bases and extensive work on its desired properties. The core of these proper-ties are the rationality postulates of System P by Kraus, Lehmann, and Magidor [40],which constitute a sound and complete axiom system for several classical model-theoreticentailment relations under uncertainty measures on worlds. They characterize classicalmodel-theoretic entailment under preferential structures [40,64], infinitesimal probabili-ties [1,57], possibility measures [14], and world rankings [33,65]. As shown by Friedmanand Halpern [20], many of these uncertainty measures on worlds are expressible as plausi-bility measures. The postulates of System P also characterize an entailment relation basedon conditional objects [15]. A survey of the above relationships is given in [6,22].Mainly to solve problems with irrelevant information, the notion of rational closure asa more adventurous notion of entailment was introduced by Lehmann [45,47]. It is equiva-lent to entailment in System Z by Pearl [58], to the least specific possibility entailment byBenferhat et al. [5], and to a conditional (modal) logic-based entailment by Lamarre [44].Finally, mainly to solve problems with property inheritance from classes to exceptionalsubclasses, the maximum entropy approach to default entailment was proposed by Gold-szmidt et al. [31]; lexicographic entailment was introduced by Lehmann [46] and Benferhatet al. [4]; conditional entailment was proposed by Geffner [24,26]; and an infinitesimal be-lief function approach was suggested by Benferhat et al. [7]. The following example due toGoldszmidt and Pearl [34] illustrates default reasoning from conditional knowledge bases.Example 1.2 (Penguins). A conditional knowledge base KB may encode the strict logicalknowledge “all penguins are birds” and the default logical knowledge “generally, birds fly”,“generally, penguins do not fly”, and “generally, birds have wings”. Some desirable con-clusions from KB [34] are “generally, birds fly” and “generally, birds have wings” (whichboth belong to KB), “generally, penguins have wings” (since the set of all penguins is asubclass of the set of all birds, and thus penguins should inherit all properties of birds),“generally, penguins do not fly” (since properties of more specific classes should overrideinherited properties of less specific classes), and “generally, red birds fly” (since “red” isnot mentioned at all in KB and thus should be considered irrelevant to the ability to fly ofbirds).There are several works in the literature on probabilistic foundations for default reason-ing from conditional knowledge bases [1,11,31,57], on combinations of Reiter’s defaultlogic [63] with statistical inference [43,67], and on a rich first-order formalism for deriv-ing degrees of belief from statistical knowledge including default statements [3]. However,there has been no work so far that extends probabilistic logic by the capability of handlingdefaults as in conditional knowledge bases.In this paper, we try to fill this gap. We present extensions of probabilistic logic bydefaults as in conditional knowledge bases under Kraus et al.’s System P [40], Pearl’sSystem Z [58], and Lehmann’s lexicographic entailment [46]. The new formalisms allowfor expressing in a uniform framework strict logical knowledge and purely probabilisticknowledge from probabilistic logic, as well as default logical knowledge from default rea-soning from conditional knowledge bases. Informally, strict logical knowledge representssentences that must always hold, while purely probabilistic (resp., default logical) knowl-122T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161edge encodes sentences that may have exceptions, which is expressed in a quantitative(resp., qualitative) way.Example 1.3 (Ostriches). Consider the strict logical knowledge “all ostriches are birds”,the default logical knowledge “generally, birds have legs” and “generally, birds fly”, andthe purely probabilistic knowledge “ostriches fly with a probability of at most 0.05”. Ob-viously, some desired conclusions are “generally, birds have legs”, “generally, birds fly”,and “ostriches fly with a probability of at most 0.05”, since these sentences are explicitlystated above. Two other desired conclusions are “generally, ostriches have legs” (since theproperty of having legs of birds should be inherited down to the subclass of all ostriches)and “generally, red birds fly” (since the property of being red is not mentioned above, andthus it should be irrelevant to the ability to fly). But neither probabilistic logic nor defaultreasoning from conditional knowledge bases can produce all these desired conclusions,since the former cannot handle default logical knowledge, while the latter cannot deal withpurely probabilistic knowledge. However, in the new formalisms of this paper, we can dealwith all the above sentences. In particular, the probabilistic generalization of lexicographicentailment also produces all the above desired conclusions.A companion paper [52] presents similar probabilistic generalizations of default rea-soning from conditional knowledge bases. The formalisms in [52], however, are quitedifferent from the ones in this paper, since they allow for handling default purely prob-abilistic knowledge rather than (strict) purely probabilistic knowledge in addition to strictlogical knowledge and default logical knowledge. For example, the formalisms in [52] al-low for expressing sentences of the form “generally, birds (and special birds) fly with aprobability of at least 0.95” rather than “birds fly with a probability of at least 0.95”. In-tuitively, the former means that being able to fly with a probability of at least 0.95 shouldapply to the class of all birds and all subclasses of birds, as long as this is consistent, whilethe latter says that being able to fly with a probability of at least 0.95 should only apply tothe class of all birds. For this reason, the formalisms in [52] are generally much strongerthan the ones here (cf. Section 8.1). Hence, they can be considered as strong nonmonotonicprobabilistic logics, while the formalisms here are weak nonmonotonic probabilistic logics.Interestingly, probabilistic reasoning in the probabilistic generalization of Kraus et al.’sSystem P in the present paper coincides with probabilistic reasoning under g-coherencefrom imprecise probability assessments in statistics (cf. Section 8.2).The main contributions of this paper can be summarized as follows:• We present combinations of probabilistic reasoning in probabilistic logic with defaultreasoning from conditional knowledge bases under Kraus et al.’s System P [40],Pearl’s System Z [58], and Lehmann’s lexicographic approach [46]. The resultingprobabilistic formalisms, also called weak nonmonotonic probabilistic logics, allowfor handling in a uniform framework strict logical knowledge and purely probabilisticknowledge from probabilistic logic, as well as default logical knowledge from condi-tional knowledge bases.• We explore the nonmonotonic properties of the three weak nonmonotonic probabilis-tic logics. In particular, they all three satisfy the rationality postulates of System PT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161123and have some Conditioning property. Furthermore, probabilistic entailment in Sys-tem Z and probabilistic lexicographic entailment both satisfy the property of RationalMonotonicity and have some Irrelevance property, while probabilistic entailment inSystem P is lacking these two properties.• We analyze the relationships between the three weak nonmonotonic probabilistic log-ics. It turns out that probabilistic entailment in System P is weaker than probabilisticentailment in System Z, which in turn is weaker than probabilistic lexicographic en-tailment. Furthermore, we show that all three formalisms are weaker than entailment inprobabilistic logic from knowledge bases in which all the default sentences are simplyinterpreted as strict sentences.• We show that probabilistic entailment in System Z and probabilistic lexicographic en-tailment coincide with entailment in probabilistic logic, whenever it is consistent tointerpret all relevant default sentences as strict sentences, while probabilistic entail-ment in System P does not have this property. Furthermore, probabilistic entailmentin Systems P and Z as well as probabilistic lexicographic entailment are proper gen-eralizations of their classical counterparts.• Finally, we present algorithms for computing tight intervals under probabilistic en-tailment in System Z and probabilistic lexicographic entailment, which are based onreductions to the standard tasks of deciding model existence and computing tight in-tervals under entailment in probabilistic logic. Furthermore, we draw a precise pictureof the complexity of deciding logical consequence and of computing tight intervalsunder probabilistic entailment in System Z and probabilistic lexicographic entailmentin general as well as restricted cases.The rest of this paper is organized as follows. Section 2 recalls the main concepts fromprobabilistic logic, while Section 3 recalls entailment in Systems P and Z as well as lexico-graphic entailment from default reasoning from conditional knowledge bases. In Section 4,we introduce the novel probabilistic generalizations of entailment in System P , entailmentin System Z, and lexicographic entailment. Section 5 explores the nonmonotonic prop-erties of these new probabilistic formalisms, their relationships, and the relationships totheir classical counterparts. In Sections 6 and 7, we provide algorithms for probabilisticreasoning under the new probabilistic formalisms, and we also analyze its computationalcomplexity, respectively. Section 8 provides a comparison to related work. In Section 9,we finally summarize the main results and give an outlook on future research.In order to not distract from the flow of reading, some technical details and proofs havebeen moved to Appendices A–E.2. Probabilistic logicIn this section, we recall the main concepts from probabilistic logic (see especially thework by Nilsson [54,55], Fagin et al. [19], Dubois and Prade et al. [2,13,16,17], Frisch andHaddawy [21], and the author [48,49,51]). We define a propositional language of logicalconstraints and of Boolean combinations of conditional constraints, which are interpretedin probability distributions over a set of worlds. We also define probabilistic knowledge124T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161bases and the model-theoretic notions of satisfiability and logical entailment for proba-bilistic knowledge bases.2.1. SyntaxWe first formally define the syntax of logical constraints and Boolean combinations ofconditional constraints as well as probabilistic knowledge bases.We assume a set of basic events Φ = {p1, . . . , pl} with l (cid:1) 1. We use ⊥ and (cid:3) to de-note false and true, respectively. We define events by induction as follows. Every elementof Φ ∪ {⊥, (cid:3)} is an event. If φ and ψ are events, then also ¬φ and (φ ∧ ψ). A condi-tional event is of the form ψ|φ with events ψ and φ. A conditional constraint is of theform (ψ|φ)[l, u] with a conditional event ψ|φ and real numbers l, u ∈ [0, 1]. We defineprobabilistic formulas by induction as follows. Every conditional constraint is a proba-bilistic formula. If F and G are probabilistic formulas, then also ¬F and (F ∧ G). Notethat probabilistic formulas will especially be used for defining concepts around probabilityrankings (cf. Section 4.1). We use (F ∨ G) and (F ⇐ G) to abbreviate ¬(¬F ∧ ¬G) and¬(¬F ∧ G), respectively, where F and G are either two events or two probabilistic formu-las, and we adopt the usual conventions to eliminate parentheses. A logical constraint is anevent of the form ψ ⇐ φ. A probabilistic knowledge base KB = (L, P ) consists of a finiteset of logical constraints L and a finite set of conditional constraints P such that (i) l (cid:2) ufor all (ε)[l, u] ∈ P , and (ii) ε1 (cid:9)= ε2 for any two distinct (ε1)[l1, u1], (ε2)[l2, u2] ∈ P .Example 2.1 (Eagles cont’d). The strict logical knowledge “all eagles are birds” and “allbirds have feathers”, and the purely probabilistic knowledge “birds fly with a probabilityof at least 0.95” can be expressed by the probabilistic knowledge base KB = ({bird ⇐eagle, feathers ⇐ bird}, {(fly | bird)[0.95, 1]}).2.2. SemanticsWe next define the semantics of logical constraints and probabilistic formulas. To thisend, we first define the semantics of events in worlds, which are truth assignments to thebasic events. We then define the semantics of logical constraints and probabilistic formulasin probability distributions over such worlds. We also define the model-theoretic notionsof satisfiability and logical entailment for this language and for probabilistic knowledgebases. We finally recall the relationship to model-theoretic logical entailment in ordinarypropositional logic.A world I associates with every basic event in Φ a binary truth value. We extend I byinduction to all events as usual. We denote by IΦ the set of all worlds for Φ. A world Isatisfies an event φ, or I is a model of φ, denoted I |= φ, iff I (φ) = true. We say I satisfiesa set of events L, or I is a model of L, denoted I |= L, iff I is a model of all φ ∈ L. Anevent φ (resp., a set of events L) is satisfiable iff a model of φ (resp., L) exists. An event ψis a logical consequence of φ (resp., L), denoted φ |= ψ (resp., L |= ψ), iff each modelof φ (resp., L) is also a model of ψ. We use φ (cid:9)|= ψ (resp., L (cid:9)|= ψ) to denote that φ |= ψ(resp., L |= ψ) does not hold.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161125A probabilistic interpretation Pr is a probability function on IΦ (that is, a mappingPr : IΦ → [0, 1] such that all Pr(I ) with I ∈ IΦ sum up to 1). The probability of anevent φ in Pr, denoted Pr(φ), is the sum of all Pr(I ) such that I ∈ IΦ and I |= φ. Forevents φ and ψ with Pr(φ) > 0, we write Pr(ψ|φ) to abbreviate Pr(ψ ∧ φ) / Pr(φ), andwe define the conditioning of Pr on φ, denoted Prφ, by Prφ(I ) = Pr(I ) / Pr(φ) for allI ∈ IΦ with I |= φ, and by Prφ(I ) = 0 for all other I ∈ IΦ . The truth of logical constraintsand probabilistic formulas F in Pr, denoted Pr |= F , is inductively defined by (i) Pr |=ψ ⇐ φ iff Pr(ψ ∧ φ) = Pr(φ), (ii) Pr |= (ψ|φ)[l, u] iff Pr(φ) = 0 or Pr(ψ|φ) ∈ [l, u],(iii) Pr |= ¬F iff not Pr |= F , and (iv) Pr |= (F ∧ G) iff Pr |= F and Pr |= G. Observehere that Pr |= ψ ⇐ φ iff Pr |= (ψ|φ)[1, 1]. We say Pr satisfies a logical constraint orprobabilistic formula F , or Pr is a model of F , iff Pr |= F . We say Pr satisfies a set oflogical constraints and probabilistic formulas F , or Pr is a model of F , denoted Pr |= F ,iff Pr is a model of all F ∈ F . We say F is satisfiable iff a model of F exists. A logicalconstraint or probabilistic formula F is a logical consequence of F , denoted F ||= F , iffevery model of F is also a model of F . A probabilistic knowledge base KB = (L, P )is satisfiable iff L ∪ P is satisfiable. The notion of logical entailment for probabilisticknowledge bases KB = (L, P ) is defined as follows. A logical or conditional constraint Fis a logical consequence of KB, denoted KB ||= F , iff L ∪ P ||= F . A conditional constraint(ψ|φ)[l, u] is a tight logical consequence of KB, denoted KB ||=tight (ψ|φ)[l, u], iff l (resp.,u) is the infimum (resp., supremum) of Pr(ψ|φ) subject to all models Pr of L ∪ P withPr(φ) > 0. Note that here we define [l, u] as the empty interval, denoted [1, 0], whenL ∪ P ||= ⊥ ⇐ φ.The following example illustrates the above notions of satisfiability, logical conse-quence, and tight logical consequence. Note that deciding satisfiability and logical con-sequence can be reduced to deciding the solvability of a system of linear constraints, whilecomputing the interval of a tight logical consequence is reducible to solving two linearoptimization problems; cf. especially [19,39,51].Example 2.2 (Eagles cont’d). Consider the probabilistic knowledge base KB = (L, P )from Example 2.1. Then, it is easy to verify that the probabilistic interpretations Pr1, Pr2,and Pr3 shown in Table 1 are models of KB. Hence, KB is satisfiable. Furthermore, somelogical consequences of KB are given as follows:KB ||= (feathers | bird)[1, 1], KB ||= (fly | bird)[0.95, 1],KB ||= (feathers | eagle)[1, 1], KB ||= (fly | eagle)[0, 1].Informally, “all birds have feathers”, “birds fly with a probability of at least 0.95”, “alleagles have feathers”, and “eagles fly with a probability between 0 and 1”. In fact, theseare the tightest intervals that are logically entailed by KB, since Pr1(feathers | bird) = 1,Pr1(fly | bird) = 1, Pr1(feathers | eagle) = 1, Pr1(fly | eagle) = 1, Pr2(fly | bird) = 0.95,and Pr3(fly | eagle) = 0. Finally, observe that the strict logical property of having feathersis inherited from birds down to its subclass eagles, whereas the probabilistic property ofbeing able to fly with a probability of at least 0.95 is not inherited from birds down toeagles.126T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161Table 1Some probabilistic interpretations Pr1, Pr2, and Pr3eagletruetruetruetruetruetruetruetruefalsefalsefalsefalsefalsefalsefalsefalsebirdtruetruetruetruefalsefalsefalsefalsetruetruetruetruefalsefalsefalsefalsefeatherstruetruefalsefalsetruetruefalsefalsetruetruefalsefalsetruetruefalsefalseflytruefalsetruefalsetruefalsetruefalsetruefalsetruefalsetruefalsetruefalsePr11000000000000000Pr20.950.0500000000000000Pr300.050000000.950000000I1I2I3I4I5I6I7I8I9I10I11I12I13I14I15I16Intuitively, the above notion of logical entailment of (ψ|φ)[l, u] from a probabilisticknowledge base KB = (L, P ) is based on the idea of performing a conditioning of everyprobability distribution Pr that satisfies L ∪ P on the premise φ. This result is more for-mally expressed by the following theorem.Theorem 2.3. Let KB = (L, P ) be a probabilistic knowledge base, and (ψ|φ)[l, u] bea conditional constraint. Then, (a) KB ||= (ψ|φ)[l, u] iff Prφ(ψ) ∈ [l, u] for all modelsPr of L ∪ P with Pr(φ)>0; and (b) KB ||=tight (ψ|φ)[l, u] iff l = inf Prφ(ψ) (resp., u =sup Prφ(ψ)) subject to all models Pr of L ∪ P with Pr(φ) > 0.The following result shows that in probabilistic logic, a logical constraint ψ ⇐ φ hasthe same meaning as the conditional constraint (ψ|φ)[1, 1].Theorem 2.4. Let KB = (L, P ) be a probabilistic knowledge base, and (ψ|φ)[1, 1] bea conditional constraint. Then, (a) KB ||= (ψ|φ)[1, 1] iff KB ||= ψ ⇐ φ; and (b) (L, P ∪{(ψ|φ)[1, 1]}) has the same set of models as (L ∪ {ψ ⇐ φ}, P ).The next result says that model-theoretic logical entailment in probabilistic logic gen-eralizes model-theoretic logical entailment in ordinary propositional logic.Theorem 2.5. Let KB = (L, P ) be a probabilistic knowledge base with P = ∅, and letψ ⇐ φ be a logical constraint. Then, KB ||= ψ ⇐ φ iff L |= ψ ⇐ φ.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–1611273. Default reasoning from conditional knowledge basesIn this section, we recall the following formalisms for default reasoning from condi-tional knowledge bases: Kraus et al.’s entailment in System P [40] (which is equivalentto several other formalisms; cf. Section 1), Pearl’s entailment in System Z [34,58] (whichis equivalent to Lehmann’s rational closure [45,47], to the least specific possibility en-tailment by Benferhat et al. [5], and to a conditional (modal) logic-based entailment byLamarre [44]), and Lehmann’s lexicographic entailment [46] (a special case of Benferhatet al.’s lexicographic entailment [4]).These formalisms for default reasoning from conditional knowledge bases all have incommon that they can be defined in terms of world rankings (which are certain mappingsfrom the set of all worlds to {0, 1, . . .} ∪ {∞}), where entailment in System P can beexpressed by a set of world rankings, while entailment in System Z and lexicographicentailment each have an associated unique world ranking.Both Pearl’s entailment in System Z and Lehmann’s lexicographic entailment are moresophisticated than entailment in System P and show a nicer semantic behavior than the lat-ter. The following example illustrates this aspect. Here, we use p-entailment, z-entailment,and lex-entailment to denote entailment in System P , entailment in System Z, and lexico-graphic entailment, respectively.Example 3.1 (Penguins cont’d). Consider again the collection of strict and default logicalsentences KB given in Example 1.2. Some default conclusions of KB under z- and lex-entailment compared to p-entailment are shown in Table 2. Differently from p-entailment,both z- and lex-entailment ignore irrelevant information. Furthermore, lex-entailmentshows a correct property inheritance from birds to penguins, while p-entailment does notshow any property inheritance at all, and z-entailment does not inherit the property of hav-ing wings from the class of all birds to the exceptional subclass of all penguins (and thusshows the problem of inheritance blocking). Finally, the default ¬fly ← penguin is entailedby KB under all three notions of default entailment.3.1. PreliminariesWe now formally define conditional knowledge bases as well as world and default rank-ings along with their admissibility with conditional knowledge bases.Informally, a conditional knowledge base consists of a set of strict statements in classicallogic and a set of defeasible rules (or defaults) of the form “ψ ← φ”, which informallyread as “generally, if φ then ψ”. Such rules may have exceptions, which can be handled inTable 2Some defaults entailed by KB under different semanticsfly ← red ∧ bird−++wings ← penguin−−+p-entailmentz-entailmentlex-entailment¬fly ← penguin+++128T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161different ways. A conditional rule (or default) is an expression of the form ψ ← φ, whereφ and ψ are events. A conditional knowledge base KB = (L, D) consists of a finite setof logical constraints L and a finite set of defaults D. The following example illustratesconditional knowledge bases.Example 3.2 (Penguins cont’d). The strict logical knowledge “all penguins are birds” andthe default logical knowledge “generally, birds fly”, “generally, penguins do not fly”, and“generally, birds have wings” is encoded by the conditional knowledge base KB=({bird ⇐penguin}, {fly ← bird, ¬fly ← penguin, wings ← bird}).A world I satisfies a default ψ ← φ, or I is a model of ψ ← φ, denoted I |= ψ ← φ,iff I |= ψ ⇐ φ. We say I verifies ψ ← φ iff I |= φ ∧ ψ. We say I falsifies ψ ← φ iffI |= φ ∧ ¬ψ (that is, I (cid:9)|= ψ ← φ). We say I satisfies a set of events and defaults K, or I isa model of K, denoted I |= K, iff I satisfies every member of K. We say K is satisfiable iffa model of K exists. An event φ (resp., a default d) is a logical consequence of K, denotedK |= φ (resp., K |= d), iff every model of K is also a model of φ (resp., d). An event φ(resp., a default d) is a logical consequence of a conditional knowledge base KB = (L, D),denoted KB |= φ (resp., KB |= d), iff L ∪ D |= φ (resp., L ∪ D |= d). A set of defaultsD tolerates a default d under a set of logical constraints L iff D ∪ L has a model thatverifies d. A set of defaults D is under L in conflict with a default ψ ← φ iff all models ofD ∪ L ∪ {φ} satisfy ¬ψ.A world ranking κ is a mapping κ : IΦ → {0, 1, . . .} ∪ {∞} such that κ(I ) = 0 for atleast one world I . It is extended to all events φ as follows. If φ is satisfiable, then κ(φ) =min{κ(I ) | I ∈ IΦ , I |= φ}; otherwise, κ(φ) = ∞. A world ranking κ is admissible witha conditional knowledge base KB = (L, D) iff κ(¬φ) = ∞ for all φ ∈ L, and κ(φ) < ∞and κ(φ ∧ ψ) < κ(φ ∧ ¬ψ) for all defaults ψ ← φ ∈ D.Example 3.3 (Penguins cont’d). Table 3 shows the world rankings κ1, κ2, and κ3. It iseasy to verify that κ1 and κ2 are admissible with KB from Example 3.2. Note that κ1and κ2 are the unique world rankings associated with KB in System Z and under lexico-graphic entailment, respectively (see Sections 3.3 and 3.4). But κ3 is not admissible withKB, since L contains the logical constraint bird ⇐ penguin, but κ3(penguin ∧ ¬bird) =min(κ3(I5), κ3(I6), κ3(I7), κ3(I8)) = 4 (cid:9)= ∞. Moreover, D contains the default wings ←bird, but κ3(bird ∧ wings) = 0 = κ3(bird ∧ ¬wings).A default ranking σ on a conditional knowledge base KB = (L, D) maps each d ∈ D toa nonnegative integer. It is admissible with KB iff each D(cid:13) ⊆ D that is under L in conflictwith some d ∈ D contains a default d (cid:13) such that σ (d (cid:13)) < σ (d).Example 3.4 (Penguins cont’d). A default ranking σ on KB from Example 3.2 is givenby σ (fly ← bird) = σ (wings ← bird) = 0 and σ (¬fly ← penguin) = 1. It is not difficult toverify that σ is admissible with KB. Note that σ is in fact the default ranking associatedwith KB in System Z (see Section 3.3).T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161129Table 3Some world rankings κ1, κ2, and κ3I1I2I3I4I5I6I7I8I9I10I11I12I13I14I15I16penguintruetruetruetruetruetruetruetruefalsefalsefalsefalsefalsefalsefalsefalsebirdtruetruetruetruefalsefalsefalsefalsetruetruetruetruefalsefalsefalsefalsewingstruetruefalsefalsetruetruefalsefalsetruetruefalsefalsetruetruefalsefalseflytruefalsetruefalsetruefalsetruefalsetruefalsetruefalsetruefalsetruefalseκ12121∞∞∞∞01110000κ23142∞∞∞∞01120000κ32102∞4∞∞011200003.2. Consistency and entailment in System PWe now describe the notions of consistency and entailment in Kraus et al.’s System P[40], which we call p-consistency and p-entailment, respectively. We define them in termsof world rankings (see especially [24,25] for the equivalence between entailment in SystemP and entailment under world rankings), and we then recall some important equivalentcharacterizations of them.A conditional knowledge base KB is p-consistent iff there exists a world ranking κ onKB that is admissible with KB. It is p-inconsistent iff no such κ exists. A p-consistentconditional knowledge base KB p-entails a default ψ ← φ iff either κ(φ) = ∞ or κ(φ ∧ψ) < κ(φ ∧ ¬ψ) for all world rankings κ admissible with KB.The following result due to Geffner [24] shows that the notion of p-consistency is equiv-alent to the existence of admissible default rankings.Theorem 3.5 (Geffner [24]). A conditional knowledge base KB is p-consistent iff thereexists a default ranking on KB that is admissible with KB.The next characterization of p-consistency is due to Goldszmidt and Pearl [32].Theorem 3.6 (Goldszmidt and Pearl [32]). A conditional knowledge base KB = (L, D) isp-consistent iff an ordered partition (D0, . . . , Dk) of D exists such that either (a) or (b)holds:(a) Every Di , 0 (cid:2) i (cid:2) k, is the set of all d ∈(b) For every i, 0 (cid:2) i (cid:2) k, each d ∈ Di is tolerated under L by(cid:1)kj =i Dj .(cid:1)kj =i Dj tolerated under L by(cid:1)kj =i Dj .130T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161The following characterization of the notion of p-entailment describes a reduction ofp-entailment to p-consistency. This result is essentially due to Adams [1], who formulatedit for L = ∅ and the notions of ε-consistency and ε-entailment (which are equivalent top-consistency and p-entailment, respectively).Theorem 3.7 (Adams [1]). A p-consistent conditional knowledge base KB = (L, D) p-entails a default ψ ← φ iff (L, D ∪ {¬ψ ← φ}) is p-inconsistent.3.3. Entailment in System ZWe next recall Pearl’s entailment in System Z [34,58], denoted z-entailment. In thesequel, let KB = (L, D) be a p-consistent conditional knowledge base.Entailment in System Z is linked to an ordered partition of D, a default ranking z on KB,and a world ranking κ z. The z-partition of KB is the unique ordered partition (D0, . . . , Dk)(cid:1)(cid:1)kkof D such that each Di is the set of all d ∈j =i Dj . Wej =i Dj tolerated under L bynext define z and κ z. For every j ∈ {0, . . . , k}, each d ∈ Dj is assigned the value j under z.The world ranking κ z on all worlds I is defined by:if I (cid:9)|= L;if I |= L ∪ D;otherwise.∞01 + maxd∈D: I (cid:9)|=d z(d)κ z(I ) =A preference relation on worlds I and I (cid:13) is then defined as follows. We say that I is z-preferable to I (cid:13) iff κ z(I ) < κ z(I (cid:13)). A model I of a set of events F (that is, I is a worldthat satisfies F ) is a z-minimal model of F iff no model of F is z-preferable to I . Notethat even though the default ranking z and the world ranking κ z are unique for a given p-consistent conditional knowledge base KB, there are generally several z-minimal modelsof a set of events F .We now use the above preference relation on worlds to define the notion of z-entailmentas follows. A default ψ ← φ is a z-consequence of KB = (L, D), denoted KB |∼ zψ ← φ,iff ψ is true in all z-minimal models of L ∪ {φ}.3.4. Lexicographic entailmentWe finally recall Lehmann’s lexicographic entailment [46], denoted lex-entailment. Inthe sequel, let KB = (L, D) be a p-consistent conditional knowledge base.We use the z-partition (D0, . . . , Dk) of KB to define a lexicographic preference relationon worlds as follows. A world I is lexicographically preferable (or lex-preferable) to aworld I (cid:13) iff some i ∈ {0, . . . , k} exists such that |{d ∈ Di | I |= d}| > |{d ∈ Di | I (cid:13) |= d}|and |{d ∈ Dj | I |= d}| = |{d ∈ Dj | I (cid:13) |= d}| for all i < j (cid:2) k. A model I of a set ofevents F is a lexicographically minimal (or lex-minimal) model of F iff no model of F islex-preferable to I .The lexicographic preference relation (which can also be expressed in terms of a uniqueworld ranking) is then used as follows to define the notion of lex-entailment. A defaultψ ← φ is a lexicographic consequence (or lex-consequence) of KB, denoted KB |∼ lexψ ←φ, iff ψ is true in all lex-minimal models of L ∪ {φ}.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–1611314. Weak nonmonotonic probabilistic logicsIn this section, we present the new probabilistic formalisms, called weak nonmonotonicprobabilistic logics, which allow for dealing with strict logical knowledge, default logi-cal knowledge, and purely probabilistic knowledge in a uniform framework. To this end,we define a new semantics of probabilistic knowledge bases, where probabilistic logic iscombined with Kraus et al.’s entailment in System P , Pearl’s entailment in System Z, andLehmann’s lexicographic entailment.The new semantics of probabilistic knowledge bases KB = (L, P ) is essentially ob-tained by defining probability and conditional constraint rankings, which generalize worldand default rankings, respectively, for conditional knowledge bases. Under the new seman-tics, conditional constraints of the form (ψ|φ)[1, 1] and (ψ|φ)[0, 0] in P then behave asthe defaults ψ ← φ and ¬ψ ← φ, respectively.Example 4.1 (Ostriches cont’d). The probabilistic knowledge base KB = (L, P ) in Table 4encodes the strict logical knowledge “all ostriches are birds”, the default logical knowledge“generally, birds have legs” and “generally, birds fly”, and the purely probabilistic knowl-edge “ostriches fly with a probability of at most 0.05”.It is important to point out that we generally cannot simply interpret KB in probabilisticlogic, since then (ψ|φ)[1, 1] and (ψ|φ)[0, 0] in P have the meaning of the strict sentencesψ ⇐ φ and ¬ψ ⇐ φ in L, and not of the defaults ψ ← φ and ¬ψ ← φ, respectively. Thefollowing example illustrates this aspect.Example 4.2 (Ostriches cont’d). The probabilistic knowledge base KB = (L, P ) in Ta-ble 4 has the probabilistic interpretation Pr1 in Table 6 as a model. This shows that KB issatisfiable. Some logical consequences of KB are given as follows:KB ||= (legs | bird)[1, 1],KB ||= (fly | bird)[1, 1].Table 4Probabilistic knowledge base KBKB = (L, P )L = {bird ⇐ ostrich}P = {(legs | bird)[1, 1], (fly | bird)[1, 1],(fly | ostrich)[0, 0.05]}Type of knowledgestrict logical knowledgedefault logical knowledgepurely probabilistic knowledgeTable 5Tight conclusions from KB under logical and s-entailment, where s ∈ {lex, z, p}(ψ|φ)(legs | bird)(fly | bird)(legs | ostrich)(fly | ostrich)(fly | red ∧ bird)||=tight[1, 1][1, 1][1, 0][1, 0][1, 1](cid:16)∼ lextight[1, 1][1, 1][1, 1][0, 0.05][1, 1](cid:16)∼ ztight[1, 1][1, 1][0, 1][0, 0.05][1, 1](cid:16)∼ ptight[1, 1][1, 1][0, 1][0, 0.05][0, 1]132T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161Since Pr1(legs | bird) = Pr1(fly | bird) = 1, these conditional constraints are in fact tightlogical consequences of KB. They are also the desired conclusions from KB (cf. Exam-ple 1.3). Some other tight logical consequences of KB are as follows:KB ||=tight (legs | ostrich)[1, 0],KB ||=tight (fly | ostrich)[1, 0].Here, the empty interval “[1, 0]” is due to the fact that in probabilistic logic the abil-ity to fly of birds is interpreted as strict logical knowledge, and inherited from birds tothe subclass of ostriches. There, it is incompatible with the purely probabilistic knowl-edge that ostriches are able to fly with a probability of at most 0.05. Thus, our knowledgeabout ostriches is locally inconsistent in the sense that there exists no model Pr of L ∪ Pwith Pr(ostrich) > 0. This is why we obtain (legs | ostrich)[1, 0] and (fly | ostrich)[1, 0]rather than the desired tight conclusions (legs | ostrich)[1, 1] and (fly | ostrich)[0, 0.05](cf. Example 1.3), respectively. Finally, another tight logical consequence of KB is givenby KB ||=tight (fly | red ∧ bird)[1, 1], which is also a desired tight conclusion from KB (cf.Example 1.3). Observe that for this last conclusion, probabilistic interpretations Pr are de-fined over the set of all truth assignments I to the basic events ostrich, bird, legs, fly, andred.4.1. PreliminariesWe now define some probabilistic generalizations of concepts from default reasoningfrom Section 3.1. In particular, we define probability and conditional constraint rankingsas well as their admissibility with probabilistic knowledge bases.A probabilistic interpretation Pr verifies a conditional constraint (ψ|φ)[l, u] iffPr(φ) > 0 and Pr |= (ψ|φ)[l, u]. We say that Pr falsifies (ψ|φ)[l, u] iff Pr(φ) > 0 andPr (cid:9)|= (ψ|φ)[l, u]. A set of conditional constraints P tolerates a conditional constraint Cunder a set of logical constraints L iff L ∪ P has a model that verifies C. We say P isunder L in conflict with C iff no model of L ∪ P verifies C.In the sequel, we use α > 0 to abbreviate the probabilistic formula ¬(α|(cid:3))[0, 0]. Infor-mally, a probabilistic interpretation Pr satisfies α > 0 iff Pr(α) > 0. A probability rankingκ is a function that associates with every probabilistic interpretation Pr on IΦ a valuefrom {0, 1, . . .} ∪ {∞} such that κ(Pr) = 0 for at least one Pr. It is extended to all log-ical constraints and probabilistic formulas F as follows. If F is satisfiable, then κ(F ) =min{κ(Pr) | Pr |= F }; otherwise, κ(F ) = ∞. A probability ranking κ is admissible with aprobabilistic knowledge base KB = (L, P ) iff κ(¬(ψ|φ)[1, 1]) = ∞ for all ψ ⇐ φ ∈ L,as well as κ(φ > 0) < ∞ and κ(φ > 0 ∧ (ψ|φ)[l, u]) < κ(φ > 0 ∧ ¬(ψ|φ)[l, u]) for all(ψ|φ)[l, u] ∈ P . Informally, the latter says that for every (ψ|φ)[l, u] ∈ P , it holds that (i)Pr(φ) > 0 and κ(Pr) < ∞ for some probabilistic interpretation Pr, and (ii) the minimalκ(Pr) of all Pr verifying (ψ|φ)[l, u] is less than the minimal κ(Pr) of all Pr falsifying(ψ|φ)[l, u].Example 4.3 (Ostriches cont’d). Table 6 shows some probabilistic interpretations Pr1, . . . ,Pr8, and Table 7 gives their values under some probability rankings κ1, κ2, and κ3. Ob-serve that κ3 is not admissible with KB = (L, P ) in Table 4, since bird ⇐ ostrich is in L,but κ3(¬(bird | ostrich)[1, 1]) (cid:2) κ3(Pr8) = 1 < ∞. Moreover, (fly | ostrich)[0, 0.05] is inT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161133Table 6Some probabilistic interpretations Pr1, . . . , Pr8ostrichtruetruetruetruetruetruetruetruefalsefalsefalsefalsefalsefalsefalsefalsebirdtruetruetruetruefalsefalsefalsefalsetruetruetruetruefalsefalsefalsefalselegstruetruefalsefalsetruetruefalsefalsetruetruefalsefalsetruetruefalsefalseflytruefalsetruefalsetruefalsetruefalsetruefalsetruefalsetruefalsetruefalsePr10000000010000000Pr20100000000000000Pr3000.050.95000000000000Pr41000000000000000Pr50010000000000000Pr60.50.500000000000000I1I2I3I4I5I6I7I8I9I10I11I12I13I14I15I16Table 7Values of Pr1, . . . , Pr8 under some probability rankings κ1, κ2, and κ3(legs | bird)[1, 1](fly | bird)[1, 1](fly | ostrich)[0, 0.05]Pr1Pr2Pr3Pr4Pr5Pr6Pr7Pr8truetruefalsetruefalsetruefalsetruetruefalsefalsetruetruefalsefalsefalsetruetruetruefalsefalsefalsefalsetrueκ10112222∞Pr7000.50.5000000000000κ20123445∞Pr800.50000.50000000000κ301102221P , but κ3(ostrich > 0 ∧ ¬(fly | ostrich)[0, 0.05]) (cid:2) κ3(Pr4) = 0 (cid:2) κ3(ostrich > 0 ∧ (fly |ostrich)[0, 0.05]). Note that on Pr1, . . . , Pr8, the rankings κ1 and κ2 coincide with theunique rankings associated with KB in probabilistic z- and lex-entailment (cf. Sections 4.3and 4.4), respectively.A conditional constraint ranking on a probabilistic knowledge base KB = (L, P ) is amapping σ that associates with every conditional constraint C ∈ P a nonnegative integer.If P (cid:9)= ∅, then σ is admissible with KB iff every P (cid:13) ⊆ P that is under L in conflict withsome C ∈ P contains some C(cid:13) with σ (C(cid:13)) < σ (C); if P = ∅, then σ is admissible with KBiff L is satisfiable. Notice that conditional constraint rankings σ are defined on the set of allconditional constraints in P and have values from {0, 1, . . .}, while probability rankings κare defined on the set of all probabilistic interpretations Pr on IΦ and have values from{0, 1, . . .} ∪ {∞}.134T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161Example 4.4 (Ostriches cont’d). A conditional constraint ranking σ for the probabilisticknowledge base KB in Table 4 is given by σ ((legs | bird)[1, 1]) = σ ((fly | bird)[1, 1]) =0 and σ ((fly | ostrich)[0, 0.05]) = 1. It is not difficult to see that σ is admissible withKB. In fact, σ is the unique conditional constraint ranking that is associated with KB inprobabilistic z-entailment (cf. Sections 4.3).4.2. Probabilistic consistency and entailment in System PWe now define a semantics of probabilistic knowledge bases, where probabilistic logicis combined with System P [40]. More precisely, we generalize the notions of consistencyand entailment in System P that are based on world rankings to probabilistic knowl-edge bases. We call these generalizations probabilistic p-consistency and probabilisticp-entailment (or simply p-consistency and p-entailment), respectively. Interestingly, theseprobabilistic notions of consistency and entailment coincide with the probabilistic notionsof g-coherence and g-coherent entailment for imprecise probability assessments (cf. Sec-tion 8.2). In the following, we first define the probabilistic generalizations of consistencyand entailment in System P , and we then give some equivalent characterizations of them.In the sequel, let KB = (L, P ) be a probabilistic knowledge base. We say KB is p-consistent iff there exists a probability ranking κ that is admissible with KB. We then definethe notion of p-entailment for p-consistent KB in terms of admissible probability rankingsas follows. A conditional constraint (ψ|φ)[l, u] is a p-consequence of KB, denoted KB (cid:16)∼ p(ψ|φ)[l, u], iff κ(φ > 0) = ∞ or κ(φ > 0 ∧ (ψ|φ)[l, u]) < κ(φ > 0 ∧ ¬(ψ|φ)[l, u])for every probability ranking κ admissible with KB. We say (ψ|φ)[l, u] is a tight p-consequence of KB, denoted KB (cid:16)∼ ptight (ψ|φ)[l, u], iff l = sup l(cid:13) (resp., u = inf u(cid:13)) subjectto KB (cid:16)∼ p(ψ|φ)[l(cid:13), u(cid:13)].The following result is a probabilistic generalization of Theorem 3.5. It says that thenotion of p-consistency of a probabilistic knowledge base KB is equivalent to the existenceof an admissible conditional constraint ranking. It is proved by showing that a probabilityranking κ that is admissible with KB can be used to define a conditional constraint rankingσ that is admissible with KB, and vice versa.Theorem 4.5. A probabilistic knowledge base KB = (L, P ) is p-consistent iff there existsa conditional constraint ranking on KB that is admissible with KB.Based on this result, we also obtain a probabilistic generalization of Theorem 3.6, whichsays that the p-consistency of a probabilistic knowledge base KB = (L, P ) is equivalentto the existence of an ordered partition of P with certain properties.Theorem 4.6. A probabilistic knowledge base KB = (L, P ) is p-consistent iff there existsan ordered partition (P0, . . . , Pk) of P such that either (a) or (b) holds:(a) Every Pi , 0 (cid:2) i (cid:2) k, is the set of all F ∈(b) For every i, 0 (cid:2) i (cid:2) k, each F ∈ Pi is tolerated under L bykj =i Pj tolerated under L by(cid:1)kj =i Pj .(cid:1)(cid:1)kj =i Pj .T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161135Example 4.7 (Ostriches cont’d). The probabilistic knowledge base KB = (L, P ) in Table 4is p-consistent, since condition (a) as well as condition (b) of Theorem 4.6 hold for thefollowing ordered partition (P0, P1) of P :(P0, P1) =(cid:5){(legs | bird)[1, 1], (fly | bird)[1, 1]}, {(fly | ostrich)[0, 0.05]}(cid:6).More precisely, to see that (P0, P1) satisfies (b), observe that Pr1 in Table 6 satisfies L ∪ Pand verifies (legs | bird)[1, 1] and (fly | bird)[1, 1], while Pr2 satisfies L ∪ P1 and verifies(fly | ostrich)[0, 0.05]. To see that also (a) holds, observe that no Pr satisfies L ∪ P andalso verifies (fly | ostrich)[0, 0.05] (cf. Example 4.2).The following two theorems are a probabilistic generalization of Theorem 3.7. Theysay that the notion of p-entailment for probabilistic knowledge bases can be expressed interms of the notion of p-consistency. The first theorem is on the notion of p-consequence,while the second one is on tight p-consequence.Theorem 4.8. Let KB = (L, P ) be a p-consistent probabilistic knowledge base, and let(β|α)[l, u] be a conditional constraint. Then, KB (cid:16)∼ p(β|α)[l, u] iff (L, P ∪ {(β|α)[p, p]})is not p-consistent for all p ∈ [0, l) ∪ (u, 1].Theorem 4.9. Let KB = (L, P ) be a p-consistent probabilistic knowledge base, and let(β|α)[l, u] be a conditional constraint. Then, KB (cid:16)∼ ptight(β|α)[l, u] iff(i) (L, P ∪ {(β|α)[p, p]}) is not p-consistent for all p ∈ [0, l) ∪ (u, 1], and(ii) (L, P ∪ {(β|α)[p, p]}) is p-consistent for all p ∈ [l, u].The next two theorems show that p-consistency and p-entailment coincide with theprobabilistic notions of g-coherence and g-coherent entailment, respectively, for impreciseprobability assessments (cf. Section 8.2). They follow from Theorems 4.5 and 4.8 as wellas similar characterizations of g-coherence and g-coherent entailment through conditionalconstraint rankings, presented in [10,11].Theorem 4.10. Let KB = (L, P ) be a probabilistic knowledge base. Then, KB is p-consistent iff KB is g-coherent.Theorem 4.11. Let KB = (L, P ) be p-consistent, and let (β|α)[l, u] be a conditional con-straint. Then, KB (cid:16)∼ p(β|α)[l, u] iff KB (cid:16)∼ g(β|α)[l, u].The following example illustrates the probabilistic notion of p-entailment. In particular,it shows that p-entailment does not realize an inheritance of default logical knowledgealong subclass relationships. See Section 6 for algorithms for deciding p-consistency andcomputing tight p-consequences.Example 4.12 (Ostriches cont’d). Consider again KB given in Table 4. Some tight p-consequences of KB are shown in Table 5. More precisely, (legs | bird)[1, 1], (fly |136T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161bird)[1, 1], and (fly | ostrich)[0, 0.05] are tight p-consequences of KB, as desired. Fur-thermore, (legs | ostrich)[0, 1] and (fly | red ∧ bird)[0, 1] are also tight p-consequences ofKB. But they differ from the desired ones (legs | ostrich)[1, 1] and (legs | red ∧ bird)[1, 1],respectively. Here, we observe that p-entailment does not inherit default logical knowledgealong subclass relationships.4.3. Probabilistic entailment in System ZWe next extend Pearl’s System Z [34,58] to p-consistent probabilistic knowledge basesKB = (L, P ). The new notion of entailment in System Z, called probabilistic z-entailment(or simply z-entailment), is associated with an ordered partition of P , a conditional con-straint ranking z on KB, and a probability ranking κ z.The z-partition of KB is the unique ordered partition (P0, . . . , Pk) of P such that eachPi , 0(cid:2)i(cid:2)k, is the set of all C ∈(cid:1)kj =i Pj tolerated under L by(cid:1)kj =i Pj .Example 4.13 (Ostriches cont’d). The z-partition of KB in Table 4 is given by the orderedpartition (P0, P1) described in Example 4.7.The conditional constraint ranking z and the probability ranking κ z are defined as fol-lows. For every j ∈ {0, . . . , k}, each C ∈ Pj is assigned the value j under z. The probabilityranking κ z on all probabilistic interpretations Pr is then defined by:κ z(Pr) =∞01 + maxC∈P : Pr(cid:9)|=C z(C)if Pr (cid:9)|= L;if Pr |= L ∪ P ;otherwise.The following lemma shows that z is a conditional constraint ranking on KB that is admis-sible with KB, and κ z is a probability ranking that is admissible with KB.Lemma 4.14. Let KB = (L, P ) be a p-consistent probabilistic knowledge base. Then, (a)z and (b) κ z are both admissible with KB.We define a preference relation on probabilistic interpretations as follows. For proba-bilistic interpretations Pr and Pr(cid:13), we say Pr is z-preferable to Pr(cid:13) iff κ z(Pr) < κ z(Pr(cid:13)).A model Pr of a set of logical constraints and probabilistic formulas F is a z-minimalmodel of F iff no model of F is z-preferable to Pr.We are now ready to define the notion of z-entailment. A conditional constraint(ψ|φ)[l, u] is a z-consequence of KB, denoted KB (cid:16)∼ z(ψ|φ)[l, u], iff every z-minimalmodel of L ∪ {φ > 0} satisfies (ψ|φ)[l, u]. We say (ψ|φ)[l, u] is a tight z-consequenceof KB, denoted KB (cid:16)∼ ztight(ψ|φ)[l, u], iff l (resp., u) is the infimum (resp., supremum) ofPr(ψ|φ) subject to all z-minimal models Pr of L ∪ {φ > 0}.The following example illustrates the probabilistic notion of z-entailment. In particular,it shows that z-entailment differs from p-entailment in the sense that z-entailment realizesan inheritance of default logical properties from classes to non-exceptional subclasses. Butz-entailment does not inherit default logical properties from classes to subclasses that areexceptional relative to some other property (and thus, like its classical counterpart, hasT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161137the problem of inheritance blocking). Algorithms for computing tight intervals under z-entailment are given in Section 6.Example 4.15 (Ostriches cont’d). Some tight conclusions under z-entailment from theprobabilistic knowledge base KB in Table 4 are shown in Table 5. More precisely, we obtainthe desired tight conclusions (legs | bird)[1, 1], (fly | bird)[1, 1], (fly | ostrich)[0, 0.05], and(fly | red ∧ bird)[1, 1]. However, we also obtain the tight conclusion (legs | ostrich)[0, 1]instead of the desired one (legs | ostrich)[1, 1]. Here, the interval “[0, 1]” is due to the factthat the default logical property of having legs is not inherited from birds to its exceptionalsubclass of ostriches.The following theorem characterizes the notion of z-consequence in terms of the prob-ability ranking κ z (and thus relates z-entailment to p-entailment).Theorem 4.16. Let KB = (L, P ) be a p-consistent probabilistic knowledge base, and letC = (ψ|φ)[l, u] be a conditional constraint. Then, KB (cid:16)∼ zC iff κ z(φ > 0) = ∞ or κ z(φ >0 ∧ C) < κ z(φ > 0 ∧ ¬C).4.4. Probabilistic lexicographic entailmentWe finally define a generalization of Lehmann’s lexicographic entailment [46] to p-consistent probabilistic knowledge bases KB = (L, P ), which we call probabilistic lexi-cographic entailment (or simply lex-entailment). Note that, even though we do not useprobability rankings here, the new notion of lex-entailment can be easily expressed througha unique single probability ranking.We use the z-partition (P0, . . . , Pk) of KB to define a lexicographic preference relationon probabilistic interpretations as follows. For probabilistic interpretations Pr and Pr(cid:13), wesay Pr is lexicographically preferable (or lex-preferable) to Pr(cid:13) iff some i ∈ {0, . . . , k}exists such that |{C ∈ Pi | Pr |= C}| > |{C ∈ Pi | Pr(cid:13) |= C}| and |{C ∈ Pj | Pr |= C}| =|{C ∈ Pj | Pr(cid:13) |= C}| for all i < j (cid:2) k. A model Pr of a set of logical constraints andprobabilistic formulas F is a lexicographically minimal (or lex-minimal) model of F iffno model of F is lex-preferable to Pr.We are now ready to define the notion of lex-entailment as follows. A conditionalconstraint (ψ|φ)[l, u] is a lex-consequence of KB, denoted KB (cid:16)∼ lex(ψ|φ)[l, u], iff eachlex-minimal model of L ∪ {φ > 0} satisfies (ψ|φ)[l, u]. We say (ψ|φ)[l, u] is a tighttight (ψ|φ)[l, u], iff l = inf Pr(ψ|φ) (resp., u =lex-consequence of KB, denoted KB (cid:16)∼ lexsup Pr(ψ|φ)) subject to all lex-minimal models Pr of L ∪ {φ > 0}.In the following example, lex-entailment realizes a correct inheritance of default log-ical properties, without showing the problem of inheritance blocking. See Section 6 foralgorithms for computing tight intervals under lex-entailment.Example 4.17 (Ostriches cont’d). Consider again the probabilistic knowledge base KBgiven in Table 4. Some tight lex-consequences are shown in Table 5. Observe that we obtainall the desired tight conclusions (legs | bird)[1, 1], (fly | bird)[1, 1], (legs | ostrich)[1, 1],(fly | ostrich)[0, 0.05], and (fly | red ∧ bird)[1, 1].138T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–1615. Semantic propertiesIn this section, we explore the semantic properties of the new notions of p-, z-, andlex-entailment, and give a comparison to logical entailment in probabilistic logic. We firstdescribe their nonmonotonicity and nonmonotonic properties. We then explore the rela-tionships between the formalisms and to their classical counterparts.5.1. NonmonotonicityIn the sequel, we denote by (cid:16)∼ a generic notion of entailment for probabilisticknowledge bases KB = (L, P ), which relates KB to its entailed conditional constraints(ψ|φ)[l, u]. The notion of logical entailment ||= has the following property of inheritanceof logical knowledge (L-INH) along subclass relationships (recall that KB ||= F iff everymodel Pr of L ∪ P is also a model of F ; see Section 2.2):L-INH. If KB (cid:16)∼ (ψ|φ)[c, c] and φ ⇐ φ(cid:9) is valid, then KB (cid:16)∼ (ψ|φ(cid:9))[c, c],for all events ψ, φ, and φ(cid:9), all probabilistic knowledge bases KB, and all c ∈ {0, 1}. Thenotions of p-, z-, and lex-entailment (cid:16)∼ p, (cid:16)∼ z, and (cid:16)∼ lex are nonmonotonic in the sensethat they all do not satisfy L-INH. Here, p-entailment completely fails L-INH, while z- andlex-entailment realize some weaker form of L-INH.Notice that logical, p-, z-, and lex-entailment all do not have the property of inheritanceof purely probabilistic knowledge (P-INH) along subclass relationships:P-INH.If KB (cid:16)∼ (ψ|φ)[l, u] and φ ⇐ φ(cid:9) is valid, then KB (cid:16)∼ (ψ|φ(cid:9))[l, u],for all events ψ, φ, and φ(cid:9), all probabilistic knowledge bases KB, and all [l, u] ⊆ [0, 1]different from [0, 0], [1, 1], and [1, 0]. See [52] for entailment semantics that satisfy P-INH and restricted forms of P-INH. For example, under such entailment semantics, wecan draw the conclusion (fly | eagle)[0.95, 1] from the probabilistic knowledge base KB =({bird ⇐ eagle}, {(fly | bird)[0.95, 1]}).5.2. Nonmonotonic propertiesWe now explore the nonmonotonic behavior (especially related to the above property L-INH) of the probabilistic formalisms of this paper. We consider the KLM postulates [40],the property Rational Monotonicity (RM) [40], and the properties Irrelevance (Irr) andConditioning (Con) (adapted from [7] and [61], respectively). An overview of the resultson nonmonotonic properties is given in Table 8.The rationality postulates of System P , namely, Right Weakening (RW), Reflexivity(Ref ), Left Logical Equivalence (LLE), Cut, Cautious Monotonicity (CM), and Or pro-posed by Kraus, Lehmann, and Magidor [40], also called KLM postulates, are commonlyregarded as being particularly desirable for any reasonable notion of nonmonotonic entail-ment. The following result shows that the notions of logical, p-, z-, and lex-entailment allsatisfy (probabilistic versions of) these postulates.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161139Table 8Nonmonotonic properties of probabilistic formalismsPropertyKLM postulatesRational MonotonicityIrrelevanceConditioning||=YesYesYesYes(cid:16)∼ lexYesYesYesYes(cid:16)∼ zYesYesYesYes(cid:16)∼ pYesNoNoYesTheorem 5.1. Every notion of entailment (cid:16)∼ among ||=, (cid:16)∼ p, (cid:16)∼ z, and (cid:16)∼ lex satisfiesthe following properties for all probabilistic knowledge bases KB = (L, P ), all events ε,ε(cid:13), φ, and ψ, and all real numbers l, l(cid:13), u, u(cid:13) ∈ [0, 1]:RW. If (φ|(cid:3))[l, u] ⇒ (ψ|(cid:3))[l(cid:13), u(cid:13)] is logically valid and KB (cid:16)∼ (φ|ε)[l, u], then KB (cid:16)∼(ψ|ε)[l(cid:13), u(cid:13)].Ref. KB (cid:16)∼ (ε|ε)[1, 1].LLE. If ε ⇔ ε(cid:13) is logically valid, then KB (cid:16)∼ (φ|ε)[l, u] iff KB (cid:16)∼ (φ|ε(cid:13))[l, u].Cut. If KB (cid:16)∼ (ε|ε(cid:13))[1, 1] and KB (cid:16)∼ (φ|ε ∧ ε(cid:13))[l, u], then KB (cid:16)∼ (φ|ε(cid:13))[l, u].CM. If KB (cid:16)∼ (ε|ε(cid:13))[1, 1] and KB (cid:16)∼ (φ|ε(cid:13))[l, u], then KB (cid:16)∼ (φ|ε ∧ ε(cid:13))[l, u].Or. If KB (cid:16)∼ (φ|ε)[1, 1] and KB (cid:16)∼ (φ|ε(cid:13))[1, 1], then KB (cid:16)∼ (φ|ε ∨ ε(cid:13))[1, 1].Another desirable property is Rational Monotonicity (RM) [40], which describes a re-stricted form of monotony, and allows to ignore certain kinds of irrelevant knowledge.The next theorem shows that logical, z-, and lex-entailment all satisfy RM. Note that hereKB (cid:9)(cid:16)∼ C denotes that KB (cid:16)∼ C does not hold.Theorem 5.2. ||=, (cid:16)∼ z, and (cid:16)∼ lex satisfy the following property for all probabilisticknowledge bases KB = (L, P ) and all events ε, ε(cid:13), and ψ:RM. If KB (cid:16)∼ (ψ|ε)[1, 1] and KB (cid:9)(cid:16)∼ (¬ε(cid:13)|ε)[1, 1], then KB (cid:16)∼ (ψ|ε ∧ ε(cid:13))[1, 1].The notion of p-entailment, however, generally does not satisfy the property RM, as thefollowing example shows.Example 5.3. Consider the following probabilistic knowledge base KB = (L, P ):(L, P ) =(cid:5){bird ⇐ eagle}, {(fly | bird)[1, 1]}(cid:6).Here, (fly | bird)[1, 1] is a logical (resp., p-, z-, and lex-) consequence of KB, and (¬eagle |bird)[1, 1] is not a logical (resp., p-, z-, and lex-) consequence of KB. Observe now that(fly | bird ∧ eagle)[1, 1] is a logical (resp., z- and lex-) consequence of KB, but (fly | bird ∧eagle)[1, 1] is not a p-consequence of KB. Note that (fly | bird ∧ eagle)[1, 1] is a tightlogical (resp., z- and lex-) consequence of KB, while (fly | bird ∧ eagle)[0, 1] is a tightp-consequence of KB.We next consider the property Irrelevance (Irr) adapted from [7]. Informally, Irr saysthat ε(cid:13) is irrelevant to a conclusion “P (cid:16)∼ (ψ|ε)[1, 1]” when they are defined over disjoint140T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161sets of basic events. The following result shows that logical, z-, and lex-entailment allsatisfy the property Irr.Theorem 5.4. ||=, (cid:16)∼ z, and (cid:16)∼ lex satisfy the following property for all probabilisticknowledge bases KB = (L, P ) and all events ε, ε(cid:13), and ψ:Irr. If KB (cid:16)∼ (ψ|ε)[1, 1], and no basic event of KB and (ψ|ε)[1, 1] occurs in ε(cid:13), thenKB (cid:16)∼ (ψ|ε∧ε(cid:13))[1, 1].The notion of p-entailment, however, does not satisfy Irr. This is already clear from thetight p-consequence (fly | red ∧ bird)[0, 1] of KB in Table 4 (cf. Example 4.12). It is alsoshown by the following (less complex) example.Example 5.5. Consider the following probabilistic knowledge base KB = (L, P ):(L, P ) =(cid:5)∅, {(fly | bird)[1, 1]}(cid:6).Here, (fly | bird)[1, 1] is a logical (resp., p-, z-, and lex-) consequence of KB. Observenow that (fly | red ∧ bird)[1, 1] is a logical (resp., z- and lex-) consequence of KB, but(fly | red ∧ bird)[1, 1] is not a p-consequence of KB. Note that (fly | red ∧ bird)[1, 1] is atight logical (resp., z- and lex-) consequence of KB, while (fly | red ∧ bird)[0, 1] is a tightp-consequence of KB.Finally, the properties Conditioning (Con) (adapted from [61]) and Inclusion (Inc) ex-press that KB should entail all its own conditional constraints. The following result showsthat logical, p-, z-, and lex-entailment all satisfy Con and Inc. Obviously, Con implies Inc;conversely, Inc and LLE imply Con.Theorem 5.6. ||=, (cid:16)∼ p, (cid:16)∼ z, and (cid:16)∼ lex satisfy the following properties for all probabilis-tic knowledge bases KB = (L, P ), all events ε, φ, and ψ, and all l, u ∈ [0, 1]:Con. If (ψ|φ)[l, u] ∈ P and ε ⇔ φ is logically valid, then KB (cid:16)∼ (ψ|ε)[l, u].Inc. If (ψ|φ)[l, u] ∈ P , then KB (cid:16)∼ (ψ|φ)[l, u].5.3. Relationships between probabilistic formalismsIn this section, we investigate the relationships between the different probabilisticformalisms. The following theorem shows that logical entailment is stronger than lex-entailment, and that the latter is stronger than z-entailment, which in turn is stronger thanp-entailment. That is, the logical implications illustrated by the upper horizontal line ofarrows in Fig. 1 hold between the probabilistic formalisms. Note that similar logical im-plications hold between their classical counterparts (which are illustrated by the lowerhorizontal line of arrows in Fig. 1).Theorem 5.7. Let KB = (L, P ) be a p-consistent probabilistic knowledge base, and letC = (ψ|φ)[l, u] be a conditional constraint. Then,T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161141Fig. 1. Relationships between probabilistic and classical formalisms.(a) KB (cid:16)∼ pC implies KB (cid:16)∼ zC.(b) KB (cid:16)∼ zC implies KB (cid:16)∼ lexC.(c) KB (cid:16)∼ lexC implies KB ||= C.In general, none of the converse implications holds, as Table 5 immediately shows.However, if L ∪ P has a model where the conditioning event φ has a positive probability,then logical, z-, and lex-entailment of (ψ|φ)[l, u] from KB all coincide. Roughly, in thisspecial case, it is consistent to transform all defaults β ← α in P that are relevant to a con-clusion of (ψ|φ)[l, u] from KB into strict logical constraints β ⇐ α in L. This importantresult is expressed by the following theorem.Theorem 5.8. Let KB = (L, P ) be a p-consistent probabilistic knowledge base, and letC = (ψ|φ)[l, u] be a conditional constraint such that L ∪ P has a model Pr with Pr(φ) >0. Then, KB ||= C iff KB (cid:16)∼ lexC iff KB (cid:16)∼ zC.The following example shows that p-entailment, however, generally does not coincidewith logical entailment when L ∪ P has a model Pr with Pr(φ) > 0.Example 5.9. Consider KB = (L, P ) = ({bird ⇐ eagle}, {(fly | bird)[1, 1]}). Here, L ∪ Phas a model Pr with Pr(eagle) > 0, and (fly | eagle)[1, 1] is a logical (resp., z- and lex-)consequence of KB, but (fly | eagle)[1, 1] is not a p-consequence of KB. Note that (fly |eagle)[1, 1] is a tight logical (resp., z- and lex-) consequence of KB, while (fly | eagle)[0,1]is a tight p-consequence of KB.5.4. Relationships to classical formalismsFinally, we explore the relationships between p-, z-, and lex-entailment and theirclassical counterparts. The following result shows that p-, z-, and lex-entailment forp-consistent probabilistic knowledge bases generalize their classical counterparts for p-consistent conditional knowledge bases. Here, the operator γ on conditional constraints,sets of conditional constraints, and conditional knowledge bases replaces each conditionalconstraint (ψ|φ)[1, 1] by the default ψ ← φ. By Theorems 2.4 and 2.5, logical entailmentin probabilistic logic similarly generalizes its classical counterpart. All this is illustrated bythe vertical arrows in Fig. 1.Theorem 5.10. Let KB = (L, {(ψi|φi)[1, 1] | i ∈ {1, . . . , n}}) be a p-consistent probabilis-tic knowledge base, and let (β|α)[1, 1] be a conditional constraint. Then,142T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161(a) KB (cid:16)∼ p(β|α)[1, 1] iff γ (KB) |∼ pβ ← α.(b) KB (cid:16)∼ z(β|α)[1, 1] iff γ (KB) |∼ zβ ← α.(c) KB (cid:16)∼ lex(β|α)[1, 1] iff γ (KB) |∼ lexβ ← α.6. AlgorithmsIn this section, we provide algorithms for the main reasoning problems in weak non-monotonic probabilistic logics.6.1. OverviewThe main decision and optimization problems of probabilistic reasoning in weak non-monotonic probabilistic logics are summarized as follows:p-CONSISTENCY: Given a probabilistic knowledge base KB, decide whether KB is p-consistent.S-CONSEQUENCE: Given a p-consistent probabilistic knowledge base KB and a condi-tional constraint (β|α)[l, u], decide whether KB (cid:16)∼ s(β|α)[l, u] holds, for somefixed semantics s ∈ {p, z, lex}.TIGHT S-CONSEQUENCE: Given a p-consistent probabilistic knowledge base KB and aconditional event β|α, compute l, u ∈ [0, 1] such that KB (cid:16)∼ s(β|α)[l, u], for somefixed semantics s ∈ {p, z, lex}.The basic idea behind the algorithms below for solving the above decision and optimizationproblems is to perform a reduction to the following standard decision and optimizationproblems in model-theoretic probabilistic logic:POSITIVE PROBABILITY: Given a probabilistic knowledge base KB = (L, P ) and anevent α, decide whether L ∪ P has a model Pr such that Pr(α) > 0.LOGICAL CONSEQUENCE: Given a probabilistic knowledge base KB and a conditionalconstraint (β|α)[l, u], decide whether KB ||= (β|α)[l, u] holds.TIGHT LOGICAL CONSEQUENCE: Given a probabilistic knowledge base KB and a con-ditional event β|α, compute l, u ∈ [0, 1] such that KB ||=tight (β|α)[l, u].The problems POSITIVE PROBABILITY and LOGICAL CONSEQUENCE can be reducedto the problem of deciding whether a system of linear constraints is solvable, while TIGHTLOGICAL CONSEQUENCE is reducible to computing the optimal solutions of two linearoptimization problems; cf. especially [19,39,51].Since the notions of p-consistency and p-entailment coincide with the notions ofg-coherence and g-coherent entailment (cf. Section 8.2), existing algorithms for decid-ing g-coherence and computing tight intervals under g-coherent entailment can be usedfor solving p-CONSISTENCY and TIGHT p-CONSEQUENCE, respectively. Such algo-rithms are shown in Figs. 2 and 4, respectively. Here, the one in Fig. 2 also computesthe z-partition of KB, if KB is p-consistent; it is similar to the algorithm for decidingT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161143Algorithm p-consistency (essentially Biazzo et al. [9])Input: probabilistic knowledge base KB = (L, P ).Output: z-partition of KB, if KB is p-consistent; nil otherwise.if P = ∅ then if L is satisfiable then return () else return nil;1.2. R := P ;i := −1;3.repeat4.i := i + 1;5.D[i] := {(ψ|φ)[l, u] ∈ R | L ∪ R ∪ {φ > 0} is satisfiable};6.R := R \ D[i]7.8. until R = ∅ or D[i] = ∅;9.if R = ∅ then return (D[0], . . . , D[i]) else return nil.Fig. 2. Algorithm p-consistency.Algorithm tight-p-consequence (essentially Biazzo et al. [9])Input: p-consistent probabilistic knowledge base KB=(L, P ), conditional event β|α.Output: interval [l, u] ⊆ [0, 1] such that KB (cid:16)∼ p1. R := P ;repeat2.(cid:11) := {(ψ|φ)[l, u] ∈ R | L ∪ R ∪ {⊥ ⇐ α} ∪ {φ > 0} is satisfiable};3.R := R \ (cid:11)4.5. until (cid:11) = ∅;6.7.compute l, u ∈ [0, 1] such that L ∪ R ||=tight (β|α)[l, u];return [l, u].tight(β|α)[l, u].Fig. 3. Algorithm tight-p-consequence.ε-consistency in default reasoning by Goldszmidt and Pearl [32]. The algorithm in Fig. 4is based on the result that the notion of p-entailment from KB coincides with logical en-tailment from a unique subbase of KB. The decision problem p-CONSEQUENCE can besolved in a similar way.In the next subsection, we provide algorithms for solving the optimization prob-lems TIGHT z- and TIGHT lex-CONSEQUENCE. The decision problems z- and lex-CONSEQUENCE can be solved in a similar way.6.2. Tight z- and lex-consequenceWe now give algorithms for solving TIGHT z- and TIGHT lex-CONSEQUENCE. Inthe sequel, let KB = (L, P ) be a p-consistent probabilistic knowledge base, and let(P0, . . . , Pk) be its z-partition. We first give some preparatory definitions.For G, H ⊆ P , we say G is z-preferable to H iff some i ∈ {0, . . . , k} exists such thatPi ⊆ G, Pi (cid:9)⊆ H , and Pj ⊆ G and Pj ⊆ H for all i < j (cid:2) k. We say G is lex-preferable toH iff some i ∈ {0, . . . , k} exists such that |G ∩ Pi| > |H ∩ Pi| and |G ∩ Pj | = |H ∩ Pj | forall i < j (cid:2) k. For D ⊆ 2P and s ∈ {z, lex}, we say G is s-minimal in D iff G ∈ D and noH ∈ D is s-preferable to G.144T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161Algorithm tight-z-consequenceInput: p-consistent probabilistic knowledge base KB=(L, P ), conditional event β|α.Output: interval [l, u] ⊆ [0, 1] such that KB (cid:16)∼ z(β|α)[l, u].Notation: (P0, . . . , Pk) denotes the z-partition of KB.1. R := L;2.3.4. while j (cid:1) 0 and R ∪ Pj ∪ {α > 0} is satisfiable do begin5.6.7.8.9.end;compute l, u ∈ [0, 1] such that R ||=tight (β|α)[l, u];return [l, u].if R ∪ {α > 0} is unsatisfiable then return [1, 0];j := k;R := R ∪ Pj ;j := j − 1Fig. 4. Algorithm tight-z-consequence.The following theorem shows how TIGHT s-CONSEQUENCE, where s ∈ {z, lex}, can bereduced to POSITIVE PROBABILITY and TIGHT LOGICAL CONSEQUENCE. The key ideaα(KB) ⊆ 2P such that KB (cid:16)∼ s(β|α)[l, u]behind this reduction is that there exists a set Dsiff L ∪ H ||= (β|α)[l, u] for all H ∈ Dsα(KB).Theorem 6.1. Let KB = (L, P ) be a p-consistent probabilistic knowledge base, and letβ|α be a conditional event. Let s ∈ {z, lex}. Let Dsα(KB) be the set of all s-minimalelements in {H ⊆ P | L ∪ H ∪ {α > 0} is satisfiable}. Then, l (resp., u) such thatKB (cid:16)∼ stight(β|α)[l, u] is given as follows:(a) If L ∪ {α > 0} is unsatisfiable, then l = 1 (resp., u = 0).(b) Otherwise, l = min c (resp., u = max d) subject to L ∪ H ||=tight (β|α)[c, d] and H ∈Dsα(KB).For s = z (resp., s = lex), Algorithm tight-s-consequence (see Fig. 4 (resp., 5)) com-putes tight intervals under s-entailment. Step 2 checks whether L ∪ {α > 0} is unsatisfiable.If this is the case, then [1, 0] is returned by Theorem 6.1(a). Otherwise, we computeDsα(KB) along the z-partition of KB in steps 3–7 (resp., 3–15), and the requested tightinterval using Theorem 6.1(b) in step 8 (resp., 16–20).7. Computational complexityIn this section, we draw a precise picture of the computational complexity of the deci-sion and optimization problems described in Section 6.1.7.1. Complexity classesWe assume some basic knowledge about the complexity classes P, NP, and co-NP. Wenow briefly describe some other complexity classes that occur in our results; see especially[23,38,56] for further background.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161145tight(β|α)[l, u].for j := k downto 0 do beginif R ∪ {α > 0} is unsatisfiable then return [1, 0];n := 0 ;H(cid:13) := ∅;for each G ⊆ Dj and H ∈ H doAlgorithm tight-lex-consequenceInput: p-consistent probabilistic knowledge base KB=(L, P ), conditional event β|α.Output: interval [l, u] ⊆ [0, 1] such that KB (cid:16)∼ lexNotation: (P0, . . . , Pk) denotes the z-partition of KB.1. R := L;2.3. H := {∅};4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19.20.21.compute c, d ∈ [0, 1] such that R ∪ H ||=tight (β|α)[c, d];(l, u) := (min(l, c), max(u, d))if R ∪ G ∪ H ∪ {α > 0} is satisfiable thenif n = |G| then H(cid:13) := H(cid:13) ∪ {G ∪ H }end;(l, u) := (1, 0);for each H ∈ H do beginH(cid:13) := {G ∪ H };n := |G|else if n < |G| then beginend;return [l, u].H := H(cid:13)end;;Fig. 5. Algorithm tight-lex-consequence.The class PNP contains all decision problems that can be solved in deterministic poly-nomial time with an oracle for NP. The class PNPcontains the decision problems in PNP(cid:16)where all oracle calls must be first prepared and then issued in parallel. The relationshipbetween these complexity classes is described by the following inclusion hierarchy (notethat all inclusions are currently believed to be strict):P ⊆ NP, co-NP ⊆ PNP(cid:16) ⊆ PNP.To classify problems that compute an output value, rather than a Yes/ No-answer, functionclasses have been introduced. In particular, FP and FPNP are the functional analogs of Pand PNP, respectively.7.2. Overview of complexity resultsIn the complexity analysis, we consider the decision and optimization problems s-CONSEQUENCE and TIGHT s-CONSEQUENCE, where s ∈ {z, lex}. We assume that KBas well as (β|α)[l, u] contain only rational numbers.The complexity results are compactly summarized in Tables 9–10. In detail, theproblems z-CONSEQUENCE and lex-CONSEQUENCE are complete for the classes PNP(cid:16)146T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161Table 9Complexity of z- and lex-CONSEQUENCEProblemz-CONSEQUENCElex-CONSEQUENCEComplexityPNP(cid:16) -completePNP-completeTable 10Complexity of TIGHT z- and lex-CONSEQUENCEProblemTIGHT z-CONSEQUENCETIGHT lex-CONSEQUENCEComplexityFPNP-completeFPNP-completeand PNP, respectively, whereas the problems TIGHT z-CONSEQUENCE and TIGHT lex-CONSEQUENCE are both complete for the class FPNP.The hardness results often hold even in the restricted literal-Horn case, where KB andβ|α are both literal-Horn. Here, a conditional event ψ|φ (resp., logical constraint ψ ⇐ φ)is literal-Horn iff ψ is a basic event (resp., ψ is either a basic event or the negation of abasic event) and φ is either (cid:3) or a conjunction of basic events. A conditional constraint(ψ|φ)[l, u] is literal-Horn iff the conditional event ψ|φ is literal-Horn. A probabilisticknowledge base KB = (L, P ) is literal-Horn iff every member of L ∪ P is literal-Horn.Note that the problems p-CONSISTENCY, p-CONSEQUENCE and TIGHT p-CON-SEQUENCE are complete for NP, co-NP, and FPNP, respectively, in the general case andalso in restricted cases. This is immediate by similar complexity results for g-coherenceand g-coherent entailment [9] and the equivalence of these notions to p-consistency andp-entailment, respectively; cf. Section 8.2. Similarly, also the problems POSITIVE PROBA-BILITY, LOGICAL CONSEQUENCE, and TIGHT LOGICAL CONSEQUENCE in probabilisticlogic are complete for NP, co-NP, and FPNP, respectively, in the general case and also inrestricted cases; cf. especially [51].7.3. Detailed complexity resultsThe following two theorems show that the problems z- and lex-CONSEQUENCE areand PNP fol-(cid:16) - and PNP-hardness of deciding z- and lex-entailment,complete for the classes PNP(cid:16)lows from Theorem 5.10 and PNPrespectively, in classical default reasoning [18].and PNP, respectively. Here, hardness for PNP(cid:16)Theorem 7.1. Given a p-consistent probabilistic knowledge base KB, and a conditionalconstraint (β|α)[l, u], deciding whether KB (cid:16)∼ z(β|α)[l, u] is PNP(cid:16) -complete.Theorem 7.2. Given a p-consistent probabilistic knowledge base KB, and a conditionalconstraint (β|α)[l, u], deciding whether KB (cid:16)∼ lex(β|α)[l, u] is PNP-complete. Hardnessholds even if KB and β|α are literal-Horn.The next two theorems show that TIGHT s-CONSEQUENCE, where s ∈ {z, lex}, is FPNP-complete. Hardness holds by a polynomial reduction from the FPNP-complete travelingsalesman cost problem [56].T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161147Theorem 7.3. Given a p-consistent probabilistic knowledge base KB, and a conditionalevent β|α, computing l, u ∈ [0, 1] such that KB (cid:16)∼ ztight(β|α)[l, u] is FPNP-complete. Hard-ness holds even if KB and β|α are literal-Horn, and L = ∅.Theorem 7.4. Given a p-consistent probabilistic knowledge base KB, and a conditionaltight(β|α)[l, u] is FPNP-complete. Hard-event β|α, computing l, u ∈ [0, 1] such that KB (cid:16)∼ lexness holds even if KB and β|α are literal-Horn, and L = ∅.8. Related workIn this section, we give a comparison to the related works on probabilistic default rea-soning [52] and on probabilistic reasoning under g-coherence [8,27–29].8.1. Strong nonmonotonic probabilistic logicsA companion paper [52] presents similar probabilistic generalizations of Pearl’s en-tailment in System Z, Lehmann’s lexicographic entailment, and Geffner’s conditionalentailment [24,26]. These formalisms, however, are quite different from the ones in thispaper, since they allow for handling default purely probabilistic knowledge rather than(strict) purely probabilistic knowledge in addition to strict logical knowledge and defaultlogical knowledge. More precisely, the formalisms in [52] are an extension of a variantof probabilistic logic (in which purely probabilistic conditional constraints are interpretedas default sentences) by defaults as in conditional knowledge bases, while the formalismshere are an extension of probabilistic logic (in which purely probabilistic conditional con-straints are interpreted as strict sentences) by defaults as in conditional knowledge bases.For example, the formalisms in [52] interpret the purely probabilistic conditional constraint(fly | bird)[0.95, 1] as “generally, birds (and special birds) fly with a probability of at least0.95”, while the formalisms here interpret (fly | bird)[0.95, 1] as “birds fly with a probabil-ity of at least 0.95”. Roughly, the former means that being able to fly with a probability ofat least 0.95 should apply to the class of all birds and all subclasses of birds, as long as thisdoes not create any inconsistencies, while the latter says that being able to fly with a prob-ability of at least 0.95 should only apply to the class of all birds. That is, the formalismsin [52] interpret purely probabilistic conditional constraints in a much stronger way thanthe formalisms here. For this reason, they are generally much stronger than the formalismshere. This is why the formalisms in [52] can be considered as strong nonmonotonic proba-bilistic logics, while the formalisms here are weak nonmonotonic probabilistic logics. Theformer are especially useful where logical entailment in probabilistic logic is too weak, forexample, in probabilistic logic programming [50,51] and probabilistic ontology reasoningin the Semantic Web [30]. Other applications are deriving degrees of belief from statisti-cal knowledge and degrees of belief, handling inconsistencies in probabilistic knowledgebases, and probabilistic belief revision.In particular, in reasoning from statistical knowledge and degrees of belief, the proba-bilistic generalization of Lehmann’s lexicographic entailment in [52], which we call herestrong lex-entailment, shows a similar behavior as reference-class reasoning [41,42,60,62]148T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161in a number of uncontroversial examples. Furthermore, it also avoids many drawbacksof reference-class reasoning [52]. In particular, it can handle complex scenarios and evenpurely probabilistic subjective knowledge as input. Moreover, conclusions are drawn in aglobal way from all the available knowledge as a whole. The following example illustratesthe use of strong lex-entailment for reasoning from statistical knowledge and degrees ofbelief.Example 8.1. Suppose that we have the statistical knowledge “all penguins are birds”,“between 90% and 95% of all birds fly”, “at most 5% of all penguins fly”, and “at least95% of all yellow objects are easy to see”. Furthermore, suppose that our belief is “Sam isa yellow penguin”. What do we then conclude about Sam’s property of being easy to see?Under reference-class reasoning, which is a machinery for dealing with such statisticalknowledge and degrees of belief, we conclude “Sam is easy to see with a probability of atleast 0.95”. This is also exactly what we obtain using the notion of strong lex-entailmentfrom [52]:The above statistical knowledge can be represented by the probabilistic knowl-edge base KB = (L, P ) = ({bird ⇐ penguin}, {(fly | bird)[0.9, 0.95], (fly | penguin)[0,0.05], (easy_to_see | yellow)[0.95, 1]}), where conditional constraints (ψ|φ)[l, u] in Pnow informally read as “generally, the probability of ψ given φ is in [l, u]”. This KB isstrongly p-consistent [52], and under strong lex-entailment from KB, we obtain the tightconclusion (easy_to_see | yellow∧penguin)[0.95, 1], as desired.Note that KB is also satisfiable and p-consistent. However, under every semanticsamong logical and (weak) p-, z-, and lex-entailment from KB, we obtain the tight con-clusion (easy_to_see | yellow ∧ penguin)[0, 1], rather than the desired one.8.2. Probabilistic reasoning under g-coherenceAnother related formalism is probabilistic reasoning under g-coherence. It is an ap-proach to reasoning with imprecise probability assessments, which has been extensivelyexplored especially in the field of statistics, and which is based on the coherence principleof de Finetti and suitable generalizations of it (see, for example, the work by Biazzo andGilio [8], Gilio [27,28], and Gilio and Scozzafava [29]), or on similar principles that havebeen adopted for lower and upper probabilities (Pelessoni and Vicig [59], Vicig [66], andWalley [68]).Interestingly, the notions of p-consistency and p-entailment for probabilistic knowl-edge bases coincide with the notions of g-coherence and g-coherent entailment, respec-tively, for imprecise probability assessments (cf. Theorems 4.10 and 4.11). We now recallthe main concepts from probabilistic reasoning under g-coherence. We start by defining(precise) probability assessments and their coherence. We then define imprecise probabil-ity assessments and the notions of g-coherence and g-coherent entailment for them and forprobabilistic knowledge bases.A probability assessment (L, A) on a set of conditional events E consists of a set oflogical constraints L, and a mapping A that assigns to each ε ∈ E a real number in [0, 1].Informally, L describes logical relationships, while A represents probabilistic knowledge.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161149For {ψ1|φ1, . . . , ψn|φn} ⊆ E with n (cid:1) 1 and n real numbers s1, . . . , sn, let the mappingG : IΦ → R be defined as follows. For every I ∈ IΦ :G(I ) =n(cid:7)i=1si · I (φi) ·(cid:6)(cid:5)I (ψi) − A(ψi|φi).In the framework of betting criterion, G can be interpreted as the random gain corre-sponding to a combination of n bets of amounts s1 · A(ψ1|φ1), . . . , sn · A(ψn|φn) onψ1|φ1, . . . , ψn|φn with stakes s1, . . . , sn. In detail, to bet on ψi|φi , one pays an amountof si · A(ψi|φi), and one gets back the amount of si , 0, and si · A(ψi|φi), when ψi ∧ φi ,¬ψi ∧ φi , and ¬φi , respectively, turns out to be true. The following notion of coherencenow assures that it is impossible (for both the gambler and the bookmaker) to have sure (oruniform) loss. A probability assessment (L, A) on a set of conditional events E is coherentiff for every {ψ1|φ1, . . . , ψn|φn} ⊆ E with n (cid:1) 1 and for all real numbers s1, . . . , sn, thefollowing holds:maxI ∈ IΦ ,I |=L∪{φ1∨···∨φn}i=1n(cid:7)si · I (φi) ·(cid:6)(cid:5)I (ψi) − A(ψi|φi)(cid:1) 0.An imprecise probability assessment (L, A) on a set of conditional events E consists ofa set of logical constraints L and a mapping A that assigns to each ε ∈ E an interval[l, u] ⊆ [0, 1], l (cid:2) u. We say (L, A) is g-coherent iff a coherent precise probability as-sessment (L, A(cid:9)) on E exists with A(cid:9)(ε) ∈ A(ε) for all ε ∈ E. The imprecise probabilityassessment [l, u] on a conditional event γ , denoted {(γ , [l, u])}, is called a g-coherent con-sequence of (L, A) iff A(cid:9)(γ ) ∈ [l, u] for every coherent precise probability assessmentA(cid:9) on E ∪ {γ } such that A(cid:9)(ε) ∈ A(ε) for all ε ∈ E. It is a tight g-coherent consequenceof (L, A) iff l (resp., u) is the infimum (resp., supremum) of A(cid:9)(γ ) subject to all coherentprecise probability assessments A(cid:9) on E ∪{γ } such that A(cid:9)(ε) ∈ A(ε) for all ε ∈ E. Observethat for ε = β|α such that L |= ¬α, every {(ε, [l, u])} with l, u ∈ [0, 1] is a g-coherent con-sequence of (L, A), and {(ε, [1, 0])} is the unique tight g-coherent consequence of (L, A).We now recall the concepts of g-coherence and g-coherent entailment for probabilis-tic knowledge bases from [10,11]. Every imprecise probability assessment IP = (L, A),where L is finite, and A is defined on a finite set of conditional events E, can be representedby the following probabilistic knowledge base:KBIP =(cid:8)(cid:5)L,(ψ|φ)[l, u] | ψ|φ ∈ E, A(ψ|φ) = [l, u](cid:9)(cid:6).Conversely, each probabilistic knowledge base KB = (L, P ) can be expressed by the fol-lowing imprecise probability assessment IPKB = (L, AKB) on EKB:AKB =EKB =(cid:8)(cid:5)(cid:6)ψ|φ, [l, u](cid:9)ψ|φ | ∃ l, u ∈ [0, 1]: (ψ|φ)[l, u] ∈ KB(cid:9)| (ψ|φ)[l, u] ∈ KB,(cid:8).A probabilistic knowledge base KB is g-coherent iff IPKB is g-coherent. For g-coherentprobabilistic knowledge bases KB and conditional constraints (ψ|φ)[l, u], we say (ψ|φ)[l,u] is a g-coherent consequence of KB, denoted KB (cid:16)∼ g(ψ|φ)[l, u], iff {(ψ|φ, [l, u])} is ag-coherent consequence of IPKB. We say (ψ|φ)[l, u] is a tight g-coherent consequence ofKB, denoted KB (cid:16)∼ gtight(ψ|φ)[l, u], iff {(ψ|φ, [l, u])} is a tight g-coherent consequence ofIPKB.150T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–1619. Summary and outlookWe have presented approaches to weak nonmonotonic probabilistic logics, which arecombinations of probabilistic logic with default reasoning in Kraus et al.’s System P ,Pearl’s System Z, and Lehmann’s lexicographic entailment. The new formalisms allowfor handling in a uniform framework strict and default logical knowledge as well as purelyprobabilistic knowledge (for example, such as the strict logical knowledge “all ostrichesare birds”, the default logical knowledge “generally, birds have legs” and “generally, birdsfly”, and the purely probabilistic knowledge “ostriches fly with a probability of at most0.05”). Interestingly, probabilistic entailment in System P coincides with probabilisticentailment under g-coherence from imprecise probability assessments. We have then ana-lyzed the semantic and nonmonotonic properties of the new formalisms. We have shownthat they all are proper generalizations of their classical counterparts, and they have simi-lar properties as them. In particular, they all satisfy the rationality postulates of System Pand a Conditioning property. Moreover, probabilistic entailment in System Z and proba-bilistic lexicographic entailment both satisfy the property of Rational Monotonicity and anIrrelevance property, while probabilistic entailment in System P does not. We have alsoanalyzed the relationships between the new formalisms. Here, probabilistic entailment inSystem P is weaker than probabilistic entailment in System Z, which in turn is weakerthan probabilistic lexicographic entailment. Moreover, they all are weaker than entailmentin probabilistic logic where default sentences are interpreted as strict sentences. Wheneverthis does not create any inconsistencies, both probabilistic entailment in System Z andprobabilistic lexicographic entailment even coincide with such entailment in probabilisticlogic, while probabilistic entailment in System P does not. Finally, we have also presentedalgorithms for reasoning under probabilistic entailment in System Z and probabilistic lex-icographic entailment, and given a precise picture of its computational complexity.In the same spirit as a companion paper [52], this paper has shed light on exciting novelformalisms for probabilistic reasoning with conditional constraints beyond probabilisticlogic. Differently from the formalisms in [52], however, the ones here are a “conservative”integration of probabilistic logic with conditional knowledge bases (cf. Section 8.1). Thatis, they allow for handling in a uniform framework logical and conditional constraints asin probabilistic logic as well as defaults as in conditional knowledge bases. Hence, theyare especially useful for reasoning about degrees of belief and defaults (as in, for example,medical or fault diagnosis). An implementation of reasoning in weak (and also strong)nonmonotonic probabilistic logics is available as a part of the system NMPROBLOG [53].AcknowledgementsThis work was supported by the Marie Curie Individual Fellowship HPMF-CT-2001-001286 of the European Union programme “Human Potential” (disclaimer: The authoris solely responsible for information communicated and the European Commission is notresponsible for any views or results expressed), by a Heisenberg Professorship of the Ger-man Research Foundation, and by the project Z29-N04 of the Austrian Science Fund. I amT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161151thankful to the reviewers of this paper and its KR2004 abstract, whose constructive com-ments helped to improve this work.Appendix A. Proofs for Section 2Proof of Theorem 2.3. Recall that KB ||= (ψ|φ)[l, u] iff every model Pr of L ∪ P is alsoa model of (ψ|φ)[l, u]. The latter is equivalent to Pr(ψ|φ) ∈ [l, u] for every model Pr ofL ∪ P with Pr(φ) > 0, which in turn is equivalent to Prφ(ψ) ∈ [l, u] for every model Prof L ∪ P with Pr(φ) > 0. This argument also shows that KB ||=tight (ψ|φ)[l, u] iff l (resp.,u) is the infimum (resp., supremum) of Prφ(ψ) subject to all models Pr of L ∪ P withPr(φ) > 0. (cid:1)Proof of Theorem 2.4. The two statements of the theorem follow immediately from theobservation that probabilistic interpretations Pr satisfy a logical constraint ψ ⇐ φ iff theysatisfy the conditional constraint (ψ|φ)[1, 1]. (cid:1)Proof of Theorem 2.4. Recall that KB ||= ψ ⇐ φ iff every model Pr of L ∪ P = L is also amodel of ψ ⇐ φ. Consider now any model I ∈ IΦ of L. Let the probabilistic interpretationPr be defined by Pr(I ) = 1 and Pr(J ) = 0 for all other J ∈ IΦ . Then, Pr is a model ofL, and thus also satisfies ψ ⇐ φ. That is, I is a model of ψ ⇐ φ. Conversely, considerany model Pr of L. Hence, every I ∈ IΦ with P r(I ) > 0 is a model of L, and thus also ofψ ⇐ φ. That is, Pr is a model of ψ ⇐ φ. (cid:1)Appendix B. Proofs for Section 4Proof of Theorem 4.5. We first suppose that P = ∅. Recall that the empty mapping σ onsuch P is admissible with KB iff L is satisfiable. The latter is equivalent to the existenceof a probability ranking κ that is admissible with KB, since every probability ranking κsatisfies κ(Pr) = 0 < ∞ for at least one probabilistic interpretation Pr. In the following,we assume that P (cid:9)= ∅.(⇐) Assume that there exists a conditional constraint ranking σ on KB that is admissi-ble with KB. Let the probability ranking κ on all Pr be defined as follows:κ(Pr) =∞01 + maxC∈P : Pr(cid:9)|=C σ (C)if Pr (cid:9)|= L;if Pr |= L ∪ P ;otherwise.We now show that κ is indeed a probability ranking and that κ is also admissible with KB.Let C ∈ P such that σ (C) is minimal. Since σ is admissible with KB, it follows that Cis tolerated by P under L. Hence, in particular, there exists a model Pr of L ∪ P . Thus,κ(Pr) = 0. We next show that κ(¬F ) = ∞ for all F ∈ L. Observe that κ(Pr) = ∞ forall Pr such that Pr (cid:9)|= F (that is, Pr |= ¬F ) for some F ∈ L. Thus, κ(¬F ) = ∞ for allF ∈ L. We finally show that κ(φ > 0) < ∞ and κ(φ > 0 ∧ C) < κ(φ > 0 ∧ ¬C) for allC = (ψ|φ)[l, u] ∈ P . Since {C(cid:13) ∈ P | σ (C(cid:13)) (cid:1) σ (C)} tolerates C under L, it holds that152T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161κ(φ > 0) < ∞ and κ(φ > 0 ∧ C) (cid:2) σ (C). Since Pr (cid:9)|= C for all models Pr of φ > 0 ∧ ¬C,it holds that σ (C) < κ(φ > 0 ∧ ¬C). In summary, κ(φ > 0) < ∞ and κ(φ > 0 ∧ C) <κ(φ > 0 ∧ ¬C) for every C = (ψ|φ)[l, u] ∈ P . This shows that κ is admissible with KB.(⇒) Let κ be a probability ranking admissible with KB. We define the conditionalconstraint ranking σ on KB by σ (C) = κ(φ > 0 ∧ C) for all C = (ψ|φ)[l, u] ∈ P . We nowshow that σ is admissible with KB. Suppose P (cid:13) ⊆ P is in conflict with C = (ψ|φ)[l, u] ∈ Punder L. Towards a contradiction, let σ (C(cid:13)) (cid:1) σ (C) for all C(cid:13) ∈ P (cid:13). Let Pr be a model ofL such that σ (C) = κ(Pr) and Pr |= φ > 0 ∧ C. Assume now Pr (cid:9)|= C(cid:13) for some C(cid:13) =(β|α)[r, s] ∈ L(cid:13). Then, κ(α > 0) < ∞ and κ(α > 0 ∧ ¬C(cid:13)) (cid:2) σ (C) (cid:2) σ (C(cid:13)) = κ(α >0 ∧ C(cid:13)). But this contradicts κ being admissible with KB. Thus, Pr is a model of P (cid:13).But this contradicts P (cid:13) being in conflict with C under L. Hence, σ (C(cid:13)) < σ (C) for someC(cid:13) ∈ P (cid:13). Thus, σ is admissible with KB. (cid:1)Proof of Theorem 4.6. Immediate by Theorem 4.5 and the fact that the existence of anadmissible conditional constraint ranking on KB is equivalent to the existence of an orderedpartition (P0, . . . , Pk) of P such that either (a) or (b) holds. (cid:1)Proof of Theorem 4.8. (⇒) Suppose that (L, P ∪ {(ψ|φ)[p, p]}) is p-consistent for somep ∈ [0, l) ∪ (u, 1]. By Theorem 4.5, there exists a probability ranking that is admissiblewith KB such that κ(φ > 0) < ∞ and κ(φ > 0 ∧ (ψ|φ)[p, p]) < κ(φ > 0 ∧ ¬(ψ|φ)[p, p]).Since κ(φ > 0 ∧ ¬(ψ|φ)[l, u]) (cid:2) κ(φ > 0 ∧ (ψ|φ)[p, p]) and κ(φ > 0 ∧ ¬(ψ|φ)[p, p]) (cid:2)κ(φ > 0 ∧ (ψ|φ)[l, u]), it follows κ(φ > 0) < ∞ and κ(φ > 0 ∧ ¬(ψ|φ)[l, u]) (cid:2) κ(φ >0 ∧ (ψ|φ)[l, u]). That is, KB (cid:16)∼ p(ψ|φ)[l, u] does not hold.(⇐) Suppose that KB (cid:16)∼ p(ψ|φ)[l, u] does not hold. That is, κ(φ > 0) < ∞ and κ(φ >0 ∧ (ψ|φ)[l, u]) (cid:1) κ(φ > 0 ∧ ¬(ψ|φ)[l, u]) for some probability ranking κ admissible withKB. Let Pr be a model of L such that Pr |= φ > 0 ∧ ¬(ψ|φ)[l, u] and κ(Pr) = κ(φ > 0 ∧¬(ψ|φ)[l, u]). We define p ∈ [0, l) ∪ (u, 1] by p = Pr(ψ|φ). It then follows that κ(φ > 0 ∧¬(ψ|φ)[l, u]) = κ(φ > 0∧(ψ|φ)[p, p]). Moreover, it holds that κ(φ > 0∧(ψ|φ)[q, q]) (cid:1)κ(φ > 0 ∧ (ψ|φ)[p, p]) for all q ∈ [0, l) ∪ (u, 1]. In summary, it thus follows that ((cid:9))κ(φ > 0) < ∞ and κ(φ > 0 ∧ ¬(ψ|φ)[p, p]) (cid:1) κ(φ > 0 ∧ (ψ|φ)[p, p]). We now showthat KB(cid:13) = (L, P ∪ {(ψ|φ)[p, p]}) is p-consistent. We define the conditional constraintranking σ on KB by (i) σ (C) = κ(α > 0 ∧ C) for all C = (β|α)[r, s] ∈ P such that κ(α >0 ∧ C) < κ(φ > 0 ∧ (ψ|φ)[p, p]), (ii) σ ((ψ|φ)[p, p]) = κ(φ > 0 ∧ (ψ|φ)[p, p]), and(iii) σ (C) = κ(α > 0 ∧ C) + 1 for all C = (β|α)[r, s] ∈ P with κ(α > 0 ∧ C) (cid:1) κ(φ >0 ∧ (ψ|φ)[p, p]). We now show that σ is admissible with KB(cid:13). It is sufficient to showthat every C ∈ P is tolerated by PC = {C(cid:13) ∈ P ∪ {(ψ|φ)[p, p]} | σ (C(cid:13)) (cid:1) σ (C)} under L.By the proof of Theorem 4.5, it follows that σ restricted to P is admissible with KB.Thus, it is sufficient to show that every C = (β|α)[r, s] ∈ P is tolerated by PC = {C(cid:13) ∈P ∪ {(ψ|φ)[p, p]} | σ (C(cid:13)) (cid:1) σ (C)} under L, where either (a) κ(α > 0 ∧ C) < κ(φ >0 ∧ (ψ|φ)[p, p]), or (b) C = (ψ|φ)[p, p]. Towards a contradiction, assume first that someC = (β|α)[r, s] ∈ P with (a) is not tolerated by PC under L. Let Pr be a model of L suchthat Pr |= α > 0 ∧ C and κ(Pr) = κ(α > 0 ∧ C). Let C(cid:13) = (β(cid:13)|α(cid:13))[r (cid:13), s(cid:13)] ∈ PC such thatPr (cid:9)|= C(cid:13) and (a.i) κ(α(cid:13) > 0 ∧ C(cid:13)) < κ(φ > 0 ∧ (ψ|φ)[p, p]), or (a.ii) C(cid:13) = (ψ|φ)[p, p], or(a.iii) κ(α(cid:13) > 0 ∧ C(cid:13)) (cid:1) κ(φ > 0 ∧ (ψ|φ)[p, p]). It then holds κ(α(cid:13) > 0) < ∞ and κ(α(cid:13) >0 ∧ ¬C(cid:13)) (cid:2) κ(Pr) = σ (C). Furthermore, it holds (a.i) σ (C) (cid:2) σ (C(cid:13)) = κ(α(cid:13) > 0 ∧ C(cid:13)), orT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161153(a.ii) σ (C) < σ (C(cid:13)) = κ(α(cid:13) > 0 ∧ C(cid:13)), or (a.iii) σ (C) + 1 < σ (C(cid:13)) = κ(α(cid:13) > 0 ∧ C(cid:13)) + 1.But in (a.ii) this contradicts ((cid:9)) and in (a.i) and (a.iii) this contradicts κ being admissiblewith KB. Hence, Pr is a model of PC . But this contradicts C not being tolerated by PCunder L. Assume next that (b) C = (ψ|φ)[p, p] is not tolerated by PC under L. Let Pr be amodel of L such that Pr |= φ > 0∧C and κ(Pr) = κ(φ > 0∧C). Let C(cid:13) = (β|α)[r, s] ∈ PCsuch that Pr (cid:9)|= C(cid:13). Observe that κ(α > 0 ∧ C(cid:13)) (cid:1) κ(φ > 0 ∧ (ψ|φ)[p, p]). Thus, κ(α >0) < ∞ and κ(α > 0 ∧ ¬C(cid:13)) (cid:2) κ(Pr) = σ (C) < σ (C(cid:13)) = κ(α > 0 ∧ C(cid:13)) + 1. But thiscontradicts κ being admissible with KB. Hence, Pr is a model of PC . But this contradicts Cnot being tolerated by PC under L. In summary, σ is admissible with KB(cid:13). That is, KB(cid:13) isp-consistent, where p ∈ [0, l) ∪ (u, 1]. (cid:1)Proof of Theorem 4.9. Immediate by Theorem 4.8. (cid:1)Proof of Lemma 4.14. Towards a contradiction, assume that z is not admissible with KB.That is, some P (cid:13) ⊆ P is under L in conflict with some C ∈ P , and P (cid:13) contains no C(cid:13) withz(C(cid:13)) < z(C). Thus, P (cid:13) ⊆ PC = {C(cid:13) ∈ P | z(C(cid:13)) (cid:1) z(C)}. Since PC tolerates C under L,also P (cid:13) tolerates C under L. But this contradicts P (cid:13) being under L in conflict with C.Hence, (a) z is admissible with KB, and by the “⇐”-part of the proof of Theorem 4.5, also(b) κ z is admissible with KB. (cid:1)Proof of Theorem 4.16. Suppose first that L (cid:9)|= ⊥ ⇐ φ. Then, κ z(φ > 0) < ∞, andκ z(φ > 0 ∧ C) < κ z(φ > 0 ∧ ¬C) iff all z-minimal models Pr of L with Pr(φ) > 0 sat-isfy C. Assume next that L |= ⊥ ⇐ φ. Then, κ z(φ > 0) = ∞, and all z-minimal models Prof L with Pr(φ) > 0 satisfy C. (cid:1)Appendix C. Proofs for Section 5While the proofs of the results in Section 4 are often similar to the proofs of theirclassical counterparts in default reasoning, the proofs for Section 5 require some genuinelyprobabilistic reasoning. In particular, in the proof of Theorem 5.1, we use the following no-tations and preliminary results. For probabilistic knowledge bases KB = (L, P ) and eventsα such that L (cid:9)|= ¬α, we denote by Pα(KB) the set of all subsets Pn = {(ψi|φi)[li, ui] | i ∈{1, . . . , n}} of P such that every model Pr of L ∪ Pn with Pr(φ1 ∨ · · · ∨ φn ∨ α) > 0 sat-isfies Pr(α) > 0. For KB = (L, P ) and α such that L |= ¬α, we define Pα(KB) = {∅}. Forevents α and p-consistent probabilistic knowledge bases KB = (L, P ), we denote by KBαthe probabilistic knowledge base (L, P (cid:9)), where P (cid:9) is the greatest element in Pα(KB).Then, the following result says that probabilistic p-entailment of (β|α)[l, u] from KB canbe reduced to logical entailment of (β|α)[l, u] from KBα. It follows immediately froma similar result for g-coherent entailment in [11] and the equivalence of probabilistic p-entailment and g-coherent entailment, by Theorem 4.11.Theorem C.1. Let KB = (L, P ) be a p-consistent probabilistic knowledge base, let(β|α)[l, u] be a conditional constraint, and let KBα be defined as above. Then,154T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161(a) KB (cid:16)∼ p(β|α)[l, u] iff KBα ||= (β|α)[l, u] .(b) KB (cid:16)∼ ptight(β|α)[l, u] iff KBα ||=tight (β|α)[l, u] .Proof of Theorem 5.1. It is easy to verify the result for ||=. In the following, we prove theresult for (cid:16)∼ p, (cid:16)∼ z, and (cid:16)∼ lex.RW. Assume first KB (cid:16)∼ s(φ|ε)[l, u], where s ∈ {z, lex}. That is, Pr |= (φ|ε)[l, u] for alls-minimal models Pr of L ∪ {ε > 0}. Since (φ|(cid:3))[l, u] ⇒ (ψ|(cid:3))[l(cid:13), u(cid:13)] is logically valid,Pr |= (ψ|ε)[l(cid:13), u(cid:13)] for all s-minimal models Pr of L ∪ {ε > 0}. That is, KB (cid:16)∼ s(ψ|ε)[l(cid:13), u(cid:13)].Assume next KB (cid:16)∼ p(φ|ε)[l, u]. That is, by Theorem C.1, KBε ||= (φ|ε)[l, u]. Thus,KBε ||= (ψ|ε)[l(cid:13), u(cid:13)]. That is, KB |=g (ψ|ε)[l(cid:13), u(cid:13)].Ref. Every probabilistic interpretation Pr satisfies (ε|ε)[1, 1]. This shows that KB (cid:16)∼ s(ε|ε)[1, 1] for all s ∈ {p, z, lex}.LLE. Assume first KB (cid:16)∼ s (φ|ε)[l, u], where s ∈ {z, lex}. That is, Pr |= (φ|ε)[l, u] forall s-minimal models Pr of L ∪ {ε > 0}. Since ε ⇔ ε(cid:13) is logically valid, Pr |= (φ|ε(cid:13))[l, u]for all s-minimal models Pr of L ∪ {ε(cid:13) > 0}. That is, KB (cid:16)∼ s(φ|ε(cid:13))[l, u]. Suppose nextKB (cid:16)∼ p(φ|ε)[l, u]. That is, by Theorem C.1, KBε ||= (φ|ε)[l, u]. Since ε ⇔ ε(cid:13) is logicallyvalid, KBε(cid:13) ||= (φ|ε(cid:13))[l, u]. That is, KB |=g (φ|ε(cid:13))[l, u].Cut. Assume first KB (cid:16)∼ s(ε|ε(cid:13))[1, 1] and KB (cid:16)∼ s(φ|ε ∧ ε(cid:13))[l, u], where s ∈ {z, lex}.That is, Pr |= (ε|ε(cid:13))[1, 1] and Pr |= (φ|ε ∧ ε(cid:13))[l, u] for all s-minimal models Pr of L ∪{ε(cid:13) > 0} and L ∪ {ε ∧ ε(cid:13) > 0}, respectively. It thus follows Pr |= (φ|ε(cid:13))[l, u] for all s-minimal models Pr of L ∪ {ε(cid:13) > 0}. That is, KB (cid:16)∼ s(φ|ε(cid:13))[l, u]. Suppose next KB |=g(ε|ε(cid:13))[1, 1] and KB (cid:16)∼ p(φ|ε ∧ ε(cid:13))[l, u]. That is, by Theorem C.1, KBε(cid:13) ||= (ε|ε(cid:13))[1, 1] andKBε∧ε(cid:13) ||= (φ|ε ∧ ε(cid:13))[l, u]. By Theorem C.1, it is then easy to see that KBε(cid:13) = KBε∧ε(cid:13) . Thus,KBε(cid:13) ||= (φ|ε(cid:13))[l, u]. That is, KB (cid:16)∼ p(φ|ε(cid:13))[l, u].CM. Assume first KB (cid:16)∼ s(ε|ε(cid:13))[1, 1] and KB (cid:16)∼ s(φ|ε(cid:13))[l, u], where s ∈ {z, lex}. Thatis, Pr |= (ε|ε(cid:13))[1, 1] and Pr |= (φ|ε(cid:13))[l, u] for all s-minimal models Pr of L ∪ {ε(cid:13) > 0}. Itfollows that Pr |= (φ|ε ∧ ε(cid:13))[l, u] for all s-minimal models Pr of L ∪ {ε ∧ ε(cid:13) > 0}. Thatis, KB (cid:16)∼ s(φ|ε ∧ ε(cid:13))[l, u]. Suppose next KB (cid:16)∼ p(ε|ε(cid:13))[1, 1] and KB (cid:16)∼ p(φ|ε(cid:13))[l, u]. Thatis, by Theorem C.1, KBε(cid:13) ||= (ε|ε(cid:13))[1, 1] and KBε(cid:13) ||= (φ|ε(cid:13))[l, u]. Thus, KBε(cid:13) ||= (φ|ε ∧ε(cid:13))[l, u]. By Theorem C.1, it is easy to see that KBε(cid:13) = KBε∧ε(cid:13) . Thus, KBε∧ε(cid:13) ||= (φ|ε ∧ε(cid:13))[l, u]. That is, KB (cid:16)∼ p(φ|ε ∧ ε(cid:13))[l, u].Or. Assume first KB (cid:16)∼ s(φ|ε)[1, 1] and KB (cid:16)∼ s(φ|ε(cid:13))[1, 1], where s ∈ {z, lex}. Thatis, Pr |= (φ|ε)[1, 1] and Pr |= (φ|ε(cid:13))[1, 1] for all s-minimal models Pr of L ∪ {ε > 0} andL ∪ {ε(cid:13) > 0}, respectively. It then follows Pr |= (φ|ε ∨ ε(cid:13))[1, 1] for all s-minimal models Prof L ∪ {ε ∨ ε(cid:13) > 0}. That is, KB (cid:16)∼ s(φ|ε ∨ ε(cid:13))[1, 1]. Suppose next KB (cid:16)∼ p(φ|ε)[1, 1] andKB (cid:16)∼ p(φ|ε(cid:13))[1, 1]. That is, by Theorem C.1, KBε = (L, Pε) ||= (φ|ε)[1, 1] and KBε(cid:13) =(L, Pε(cid:13)) ||= (φ|ε(cid:13))[1, 1]. By Theorem C.1, it is then easy to see that Pε∨ε(cid:13) ⊇ Pε and Pε∨ε(cid:13) ⊇Pε(cid:13) . Hence, KBε∨ε(cid:13) ||= (φ|ε)[1, 1] and KBε∨ε(cid:13) ||= (φ|ε(cid:13))[1, 1], where KBε∨ε(cid:13) = (L, Pε∨ε(cid:13) ).It thus follows KBε∨ε(cid:13) ||= (φ|ε ∨ ε(cid:13))[1, 1]. That is, KB (cid:16)∼ p (φ|ε ∨ ε(cid:13))[1, 1]. (cid:1)Proof of Theorem 5.2. Assume first KB ||= (ψ|ε)[1, 1] and KB (cid:9)||=¬(ε(cid:13)|ε)[1, 1]. In partic-ular, Pr |= (ψ|ε)[1, 1] for all models Pr of L ∪ P ∪ {ε > 0}. Hence, Pr |= (ψ|ε ∧ ε(cid:13))[1, 1]for all models Pr of L ∪ P ∪ {ε ∧ ε(cid:13) > 0}. That is, KB ||= (ψ|ε ∧ ε(cid:13))[1, 1]. Assume nextKB (cid:16)∼ s(ψ|ε)[1, 1] and KB (cid:9)(cid:16)∼s(¬ε(cid:13)|ε)[1,1], s ∈ {z, lex}. That is, Pr |= (ψ|ε)[1, 1] for alls-minimal models Pr of L∪{ε > 0}, and Pr |= ¬(ε(cid:13)|ε)[0, 0] for some s-minimal models PrT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161155of L ∪ {ε > 0}. So, Pr |= (ψ|ε∧ε(cid:13))[1, 1] for all s-minimal models Pr of L ∪ {ε ∧ ε(cid:13) > 0}.That is, KB (cid:16)∼ s(ψ|ε∧ε(cid:13))[1, 1]. (cid:1)Proof of Theorem 5.4. Assume that ((cid:9)) no atom of KB and (ψ|ε)[1, 1] occurs in ε(cid:13).Suppose first KB ||= (ψ|ε)[1, 1]. That is, Pr |= (ψ|ε)[1, 1] for all models Pr of L ∪ P ∪{ε > 0}. Hence, Pr |= (ψ|ε ∧ ε(cid:13))[1, 1] for all models Pr of L ∪ P ∪ {ε ∧ ε(cid:13) > 0}. Thatis, KB ||= (ψ|ε ∧ ε(cid:13))[1, 1]. Assume next KB (cid:16)∼ s(ψ|ε)[1,1], where s ∈ {z, lex}. That is,Pr |= (ψ|ε)[1, 1] for all s-minimal models Pr of L ∪ {ε > 0}. By ((cid:9)), it follows that Pr |=(ψ|ε ∧ ε(cid:13))[1, 1] for all s-minimal models Pr of L ∪ {ε ∧ ε(cid:13) > 0}. That is, KB (cid:16)∼ s(ψ|ε ∧ε(cid:13))[1, 1]. (cid:1)Proof of Theorem 5.6. Assume that (ψ|φ)[l, u] ∈ P and ε ⇔ φ is logically valid.Clearly, KB ||= (ψ|ε)[l, u]. Since KB is p-consistent, the conditional constraint rank-ing z exists, and (ψ|φ)[l, u] is tolerated by {C ∈ P | z(C) (cid:1) z((ψ|φ)[l, u])} under L.Hence, every s-minimal model Pr of L ∪ {ε > 0} satisfies (ψ|ε)[l, u], where s ∈ {z,lex}. Hence, KB (cid:16)∼ s(ψ|ε)[l, u]. Since (L, P ∪ {(ψ|ε)[p, p]}) is not p-consistent for allp ∈ [0, l) ∪ (u, 1], it also follows KB (cid:16)∼ p(ψ|ε)[l, u]. (cid:1)Proof of Theorem 5.7. (a) Suppose KB (cid:16)∼ pC. By Theorem 4.8, κ(φ > 0) = ∞ orκ(φ > 0 ∧ C) < κ(φ > 0 ∧ ¬C) for every probability ranking κ admissible with KB.By Lemma 4.14, κ z is admissible with KB. Hence, κ z(φ > 0) = ∞ or κ z(φ > 0 ∧ C) <κ z(φ > 0 ∧ ¬C). By Theorem 4.16, it thus holds KB (cid:16)∼ zC.(b) Suppose KB (cid:16)∼ zC. That is, every z-minimal model Pr of L ∪ {φ > 0} satisfies C.Since every lex-minimal model Pr of L∪{φ > 0} is also a z-minimal model of L∪{φ > 0},it follows that every lex-minimal model Pr of L ∪ {φ > 0} satisfies C. That is, KB (cid:16)∼ lexC.(c) Suppose KB (cid:16)∼ lexC. That is, every lex-minimal model Pr of L ∪ {φ > 0} satisfiesC. Assume first Pr(φ) = 0 for every model Pr of L ∪ P . Then, KB ||= C trivially holds.Assume next L ∪ P ∪ {φ > 0} is satisfiable. Thus, Pr is a lex-minimal model of L ∪ {φ > 0}iff it is a model of L ∪ P ∪ {φ > 0}. Hence, every model of L ∪ P ∪ {φ > 0} satisfies C.That is, KB ||= C. (cid:1)Proof of Theorem 5.8. The existence of some model Pr of L ∪ P ∪ {φ > 0} implies that aprobabilistic interpretation Pr is a model of L ∪ P ∪ {φ > 0} iff it is a lex-minimal modelof L ∪ {φ > 0} iff it is a z-minimal model of L ∪ {φ > 0}. (cid:1)Proof of Theorem 5.10. (a) A conditional constraint ranking σ on KB is admissible withKB iff the default ranking σ ◦ γ −1 on γ (KB) is admissible with γ (KB).(b), (c) Observe that (P0, . . . , Pk) is the z-partition of KB iff (γ (P0), . . . , γ (Pk)) is theclassical z-partition of γ (KB). Furthermore, every s-minimal model Pr of L ∪ {α > 0}satisfies (β|α)[1, 1] iff every classical s-minimal model I of L ∪ {α} satisfies β, where s ∈{z, lex}. (cid:1)Appendix D. Proofs for Section 6Proof of Theorem 6.1. (a) If L ∪ {α>0} is unsatisfiable, then KB (cid:16)∼ stight(β|α)[1,0].156T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161(b) Assume L ∪ {α > 0} is satisfiable. It is sufficient to show that Pr is an s-minimalα(KB):model of L ∪ {α > 0} iff Pr is a model of L ∪ H ∪ {α > 0} for some H ∈ Ds(⇒) Let Pr be an s-minimal model of L∪{α > 0}. Let H (cid:13) = {C ∈ P | Pr |= C}. Clearly,Pr |= L ∪ H (cid:13) ∪ {α > 0}. We now show that H (cid:13) ∈ Dsα(KB). Suppose not. That is, someH (cid:13)(cid:13) ⊆ P exists such that L ∪ H (cid:13)(cid:13) ∪ {α > 0} is satisfiable and that H (cid:13)(cid:13) is s-preferable to H (cid:13).Thus, a model Pr(cid:13) of L ∪ H (cid:13)(cid:13) ∪ {α > 0} exists. As H (cid:13)(cid:13) is s-preferable to H (cid:13), the model Pr(cid:13)of L ∪ {α > 0} is s-preferable to Pr. But this contradicts Pr being an s-minimal model ofL ∪ {α > 0}. Thus, H (cid:13) ∈ Ds(⇐) Let Pr be a model of L ∪ H (cid:13) ∪ {α > 0} for some H (cid:13) ∈ Dsα(KB). Clearly, Pr isa model of L ∪ {α > 0}. We now show that Pr is an s-minimal model of L ∪ {α > 0}.Suppose not. That is, there exists a model Pr(cid:13) of L∪{α > 0} that is s-preferable to Pr. Thus,{C ∈ P | Pr(cid:13) |= C} ⊆ P is s-preferable to H (cid:13). But this contradicts H (cid:13) being a member ofDsα(KB). Hence, Pr is an s-minimal model of L ∪ {α > 0}. (cid:1)α(KB).Appendix E. Proofs for Section 7The proofs of Theorems 7.1–7.4 are similar to the proofs of related complexity results in[52]. We first give some preparatory definitions as follows. In the sequel, let KB = (L, P )be a p-consistent probabilistic knowledge base, and let (β|α)[l, u] be a conditional con-straint. Let n denote the cardinality of P . For the following definitions, let L ∪ {α > 0}be satisfiable. An ordered partition (P0, . . . , Pk) of P is admissible with KB iff for each{Pj | j (cid:1) i} is satisfi-i ∈ {0, . . . , k} and each (ψ|φ)[r, s] ∈ Pi , the set L ∪ {φ > 0} ∪(cid:10)ki=0 i · |Pi|. Letable. The weight of an ordered partition (P0, . . . , Pk) of P is defined aswmin denote the least weight w of all ordered partitions of P that are admissible withKB. As in classical default reasoning, the z-partition of KB is the unique ordered parti-tion (P (cid:9)k ) of P that is admissible with KB and that has the weight wmin. Let jmin(cid:1)denote the least j ∈ {0, . . . , k+1} such that L ∪| i (cid:1) j } ∪ {α > 0} is satisfiable.Let nmin = (|P (cid:13) ∩ P (cid:9)0{P (cid:9)i|) for some P (cid:13) ∈ Dlexα (KB).|, . . . , |P (cid:13) ∩ P (cid:9)k0 , . . . , P (cid:9)(cid:1)Proof of Theorem 7.1. Let KB = (L, P ). We first prove membership in PNPrem 6.1, it holds that KB (cid:16)∼ z(β|α)[l, u] iff either (i) or (ii) holds:(cid:16) . By Theo-(i) L ∪ {α > 0} is unsatisfiable.(ii) L ∪ {α > 0} is satisfiable, and L ∪(cid:1){P (cid:9)i| i (cid:1) jmin} ||= (β|α)[l, u].Deciding whether L ∪ {α > 0} is satisfiable can be done with one NP-oracle call. IfL ∪ {α > 0} is satisfiable, then we compute the least weight wmin ∈ {0, . . . , n(n−1)/2}and the value jmin ∈ {0, . . . , n + 1}, which can both be done in deterministic polynomial| i (cid:1)time with O(log n) calls to an NP-oracle. Finally, we decide whether L ∪jmin} ||= (ψ|φ)[l, u], which can be done with one NP-oracle call. Since four rounds of par-allel NP oracle queries can be replaced by a single round of NP queries, this means that theproblem is in PNP(cid:16) .{P (cid:9)i(cid:1)T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161157Hardness for PNP(cid:16)(cid:16) -completeproblem [18]. Given a p-consistent conditional knowledge base KB(cid:13) = (L(cid:13), D(cid:13)) and a de-fault δ ← γ , decide whether KB(cid:13) |∼ zδ ← γ .is proved by a polynomial reduction from the following PNPWe define KB = (L(cid:13), {(ψ|φ)[1, 1] | ψ ← φ ∈ D(cid:13)}) and β|α = δ|γ . By Theorem 5.10,KB(cid:13) |∼ zδ ← γ iff KB (cid:16)∼ z(β|α)[1, 1]. (cid:1)Proof of Theorem 7.2. Let KB = (L, P ). We first prove PNP-membership. By Theo-rem 6.1, it holds that KB (cid:16)∼ lex(β|α)[l, u] iff either (i) or (ii) holds:(i) L ∪ {α > 0} is unsatisfiable.(ii) L ∪ {α > 0} is satisfiable, and L ∪ P (cid:13) |= (β|α)[l, u] for all P (cid:13) ∈ Dlexα (KB).Deciding whether L ∪ {α > 0} is satisfiable can be done with one NP-oracle call. IfL ∪ {α > 0} is satisfiable, then we compute the least weight wmin ∈ {0, . . . , n(n−1)/2},which can be done in deterministic polynomial time with O(log n) calls to an NP-oracle.Moreover, we compute the vector nmin ∈ {0, . . . , n}k. This can be done with k rounds ofbinary search, where each round runs in deterministic polynomial time with O(log n) callsto an NP-oracle. Finally, we decide whether L ∪ P (cid:13) |= (β|α)[l, u] for all P (cid:13) ∈ Dlexα (KB),which can be done with one call to an NP-oracle. In summary, the problem is in PNP.Hardness for PNP is proved by a polynomial reduction from the following PNP-completeproblem [18]. Given a p-consistent conditional knowledge base KB(cid:13) = (L(cid:13), D(cid:13)), where L(cid:13)is a finite set of literal-Horn logical constraints and D(cid:13) is a finite set of literal-Horn defaults(which are of the form ψ ← φ, where ψ is either a basic event or the negation of a basicevent, and φ is either (cid:3) or a conjunction of basic events), and δ ← γ is a literal-Horndefault, decide whether KB(cid:13) |∼ lexδ ← γ .We now construct KB = (L, P ) and C = (β|α)[l, u] as stated in the theorem such thatKB(cid:13) |∼ lex δ ← γ iff KB (cid:16)∼ lexC. We define KB and C by L = L(cid:13) and(cid:9)(cid:8)(p|φ)[0, 0] | ¬p ← φ ∈ D(cid:13), p ∈ Φ(cid:9),(cid:8)P =C =(p|φ)[1, 1] | p ← φ ∈ D(cid:11)(p|γ )[1, 1](p|γ )[0, 0], p ∈ Φif δ = p and p ∈ Φ,if δ = ¬p and p ∈ Φ.∪(cid:13)Notice that KB and C are literal-Horn. By a slight generalization of Theorem 5.10,KB(cid:13) |∼ lexδ ← γ iff KB (cid:16)∼ lexC. (cid:1)Proof of Theorems 7.3 and 7.4. Let KB = (L, P ). We first prove membership in FPNP.Let s ∈ {z, lex}. The interval [l, u] ⊆ [0, 1] such that KB (cid:16)∼ s(β|α)[l, u] can be computedby a variant of Algorithm tight-entailment-opt in [51], which can be done in FPNP. Ratherthan checking the existence of some model Pr of L ∪ P with Pr(α) > 0, we check theexistence of some P (cid:13) ∈ Dsα(KB) and some model Pr of L ∪ P (cid:13) with Pr(α) > 0. Once thez-partition of KB, the value jmin, and the vector nmin are computed (which can be done inFPNP by the proofs of Theorems 7.1 and 7.2) guessing and verifying P (cid:13) ∈ Dsα(KB) is inNP, and thus does not increase the complexity. Hence, the new algorithm can be done inFPNP.Hardness for FPNP is shown by a polynomial reduction from the FPNP-complete travel-ing salesman cost problem [56]. Given a set of n (cid:1) 1 cities V = {1, 2, . . . , n} and a nonneg-158T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161ative integer distance di,j = dj,i between any two cities i and j , we have to compute theni=1 dπ(i),π(σ (i))smallest length d of a tour through all the cities, that is, the minimum ofsubject to all permutations π , where σ (n) = 1, and σ (i) = i + 1 for all i < n. Without lossof generality, we can assume n (cid:1) 3.(cid:10)Let s be the sum of all di,j with i, j ∈ V and i < j . We now construct KB = (L, P )and β|α as stated in the theorem such that the smallest length d of a tour is s · l, where l isgiven by KB (cid:16)∼ ztight(β|α)[l, 1] (and also KB (cid:16)∼ lexLet E = {{i, j }⊆V | i (cid:9)= j } and w{i,j } = di,j /s for all {i, j } ∈ E. The set of basic eventsΦ is defined as Φ1 ∪ Φ2, where Φ1={pi,j | i, j ∈ V } and Φ2={p} ∪ {pe | e ∈ E}. We thendefine a set of literal-Horn conditional constraints P1 = P1,1 ∪ P1,2 ∪ P1,3 that describesthe set of all permutations of the members in V as follows:tight(β|α)[l, 1]).P1,1 =P1,2 =P1,3 =(cid:8)(pi,j | pi,k)[0, 0] | i, j, k ∈ V , j < k(cid:8)(pi,j | pk,j )[0, 0] | i, j, k ∈ V , i < k(cid:8)(pi,j | (cid:3))[1/n, 1/n] | i, j ∈ V(cid:9).(cid:9),(cid:9),Roughly speaking, each world I with Pr1(I ) > 0 for some model Pr1 of P1 correspondsto a permutation of the members in V , and vice versa. We next define a set of literal-Hornconditional constraints P2 = P2,1 ∪ P2,2 ∪ P2,3 that associates each such permutation withits tour length, and the predicate symbol p with the sum of all such tour lengths as follows:P2,1 =P2,2 =P2,3 =| pe2)[0, 0] | e1, e2 ∈ E, e1 (cid:9)= e2(cid:8)(pe1(cid:8)(p{i,j } | pu,i ∧ pσ (u),j )[w{i,j }, w{i,j }] | u ∈ V , {i, j } ∈ E(cid:8)(p | pe)[1, 1] | e ∈ E(cid:9).(cid:9),(cid:9),We finally define KB = (L, P ) = (∅, P1 ∪ P2). Observe that KB and p|(cid:3) are literal-Hornand that L is empty. As proved in [9], KB is p-consistent. This shows in particular thatL ∪ P has a model Pr with Pr((cid:3)) > 0. Hence, by Theorem 5.8, KB ||=tight (p|(cid:3))[l, 1]iff KB (cid:16)∼ ztight (p|(cid:3))[l, 1]). As shown in [51], KB ||=tight(p|(cid:3))[l, 1] iff s · l is the smallest length of a tour through all the cities. In summary,KB (cid:16)∼ ztight(p|(cid:3))[l, 1]) iff s · l is the smallest length of a tourthrough all the cities. (cid:1)tight (p|(cid:3))[l, 1] (iff KB (cid:16)∼ lextight(p|(cid:3))[l, 1] (iff KB (cid:16)∼ lexReferences[1] E.W. Adams, The Logic of Conditionals, Synthese Library, vol. 86, D. Reidel, Dordrecht, 1975.[2] S. Amarger, D. Dubois, H. Prade, Constraint propagation with imprecise conditional probabilities, in: Pro-ceedings UAI-91, Morgan Kaufmann, San Mateo, CA, 1991, pp. 26–34.[3] F. Bacchus, A. Grove, J.Y. Halpern, D. Koller, From statistical knowledge bases to degrees of belief, Artifi-cial Intelligence 87 (1–2) (1996) 75–143.[4] S. Benferhat, C. Cayrol, D. Dubois, J. Lang, H. Prade, Inconsistency management and prioritized syntax-based entailment, in: Proceedings IJCAI-93, Chambéry, France, Morgan Kaufmann, San Mateo, CA, 1993,pp. 640–645.[5] S. Benferhat, D. Dubois, H. Prade, Representing default rules in possibilistic logic, in: Proceedings KR-92,Cambridge, MA, Morgan Kaufmann, San Mateo, CA, 1992, pp. 673–684.[6] S. Benferhat, D. Dubois, H. Prade, Nonmonotonic reasoning, conditional objects and possibility theory,Artificial Intelligence 92 (1–2) (1997) 259–276.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161159[7] S. Benferhat, A. Saffiotti, P. Smets, Belief functions and default reasoning, Artificial Intelligence 122 (1–2)(2000) 1–69.[8] V. Biazzo, A. Gilio, A generalization of the fundamental theorem of de Finetti for imprecise conditionalprobability assessments, Internat. J. Approx. Reason. 24 (2–3) (2000) 251–272.[9] V. Biazzo, A. Gilio, T. Lukasiewicz, G. Sanfilippo, Probabilistic logic under coherence: Complexity andalgorithms, in: Proceedings ISIPTA-01, 2001, pp. 51–61.[10] V. Biazzo, A. Gilio, T. Lukasiewicz, G. Sanfilippo, Probabilistic logic under coherence, model-theoreticprobabilistic logic, and default reasoning, in: Proceedings ECSQARU-01, in: Lecture Notes in Comput.Sci./Lecture Notes in Artificial Intelligence, vol. 2143, Springer, Berlin, 2001, pp. 290–302.[11] V. Biazzo, A. Gilio, T. Lukasiewicz, G. Sanfilippo, Probabilistic logic under coherence, model-theoreticprobabilistic logic, and default reasoning in System P , J. Appl. Non-Class. Logics 12 (2) (2002) 189–213.[12] G. Boole, An Investigation of the Laws of Thought, on which are Founded the Mathematical Theories ofLogic and Probabilities, Walton and Maberley, London, 1854, reprint: Dover Publications, New York, 1958.[13] D. Dubois, H. Prade, On fuzzy syllogisms, Computational Intelligence 4 (2) (1988) 171–179.[14] D. Dubois, H. Prade, Possibilistic logic, preferential models, non-monotonicity and related issues, in: Pro-ceedings IJCAI-91, Sydney, Australia, Morgan Kaufmann, San Mateo, CA, 1991, pp. 419–424.[15] D. Dubois, H. Prade, Conditional objects as nonmonotonic consequence relationships, IEEE Trans. SystemMan Cybernet. 24 (12) (1994) 1724–1740.[16] D. Dubois, H. Prade, L. Godo, R.L. de Màntaras, Qualitative reasoning with imprecise probabilities, J. In-telligent Inform. Syst. 2 (1993) 319–363.[17] D. Dubois, H. Prade, J.-M. Touscas, Inference with imprecise numerical quantifiers, in: Z.W. Ras, M. Ze-mankova (Eds.), Intelligent Systems, Ellis Horwood, 1990, pp. 53–72, Chapter 3.[18] T. Eiter, T. Lukasiewicz, Default reasoning from conditional knowledge bases: Complexity and tractablecases, Artificial Intelligence 124 (2) (2000) 169–241.[19] R. Fagin, J.Y. Halpern, N. Megiddo, A logic for reasoning about probabilities, Inform. Comput. 87 (1990)78–128.[20] N. Friedman, J.Y. Halpern, Plausibility measures and default reasoning, J. ACM 48 (4) (2001) 648–685.[21] A.M. Frisch, P. Haddawy, Anytime deduction for probabilistic logic, Artificial Intelligence 69 (1–2) (1994)93–122.[22] D.M. Gabbay, P. Smets (Eds.), Handbook on Defeasible Reasoning and Uncertainty Management Systems,Kluwer Academic, Dordrecht, 1998.[23] M.R. Garey, D.S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, Free-man, New York, 1979.[24] H. Geffner, Default Reasoning: Causal and Conditional Theories, MIT Press, Cambridge, MA, 1992.[25] H. Geffner, High probabilities, model preference and default arguments, Mind and Machines 2 (1992) 51–70.[26] H. Geffner, J. Pearl, Conditional entailment: Bridging two approaches to default reasoning, Artificial Intel-ligence 53 (2–3) (1992) 209–244.[27] A. Gilio, Probabilistic consistency of conditional probability bounds, in: Advances in Intelligent Computing,in: Lecture Notes in Comput. Sci., vol. 495, Springer, Berlin, 1995, pp. 200–209.[28] A. Gilio, Probabilistic reasoning under coherence in System P , Ann. Math. Artificial Intelligence 34 (1–3)(2002) 5–34.[29] A. Gilio, R. Scozzafava, Conditional events in probability assessment and revision, IEEE Trans. SystemMan Cybernet. 24 (12) (1994) 1741–1746.[30] R. Giugno, T. Lukasiewicz, P-SHOQ(D): A probabilistic extension of SHOQ(D) for probabilistic ontologiesin the Semantic Web, in: Proceedings JELIA-02, in: Lecture Notes in Comput. Sci./Lecture Notes in Artif.Intell., vol. 2424, Springer, Berlin, 2002, pp. 86–97.[31] M. Goldszmidt, P. Morris, J. Pearl, A maximum entropy approach to non- monotonic reasoning, IEEE Trans.Pattern Anal. Mach. Intell. 15 (3) (1993) 220–232.[32] M. Goldszmidt, J. Pearl, On the consistency of defeasible databases, Artificial Intelligence 52 (2) (1991)121–149.[33] M. Goldszmidt, J. Pearl, Rank-based systems: A simple approach to belief revision, belief update and rea-soning about evidence and actions, in: Proceedings KR-92, Cambridge, MA, Morgan Kaufmann, San Mateo,CA, 1992, pp. 661–672.160T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161[34] M. Goldszmidt, J. Pearl, Qualitative probabilities for default reasoning, belief revision, and causal modeling,Artificial Intelligence 84 (1–2) (1996) 57–112.[35] T. Hailperin, Sentential Probability Logic: Origins, Development, Current Status, and Technical Applica-tions, Associated University Presses, London, 1996.[36] P. Hansen, B. Jaumard, G.-B.D. Nguetsé, M.P. de Aragão, Models and algorithms for probabilistic andBayesian logic, in: Proceedings IJCAI-95, Montreal, Quebec, Morgan Kaufmann, San Mateo, CA, 1995,pp. 1862–1868.[37] B. Jaumard, P. Hansen, M.P. de Aragão, Column generation methods for probabilistic logic, ORSA J. Com-put. 3 (1991) 135–147.[38] D.S. Johnson, A catalog of complexity classes, in: J. van Leeuwen (Ed.), Handbook of Theoretical ComputerScience, vol. A, MIT Press, Cambridge, MA, 1990, pp. 67–161, Chapter 2.[39] G. Kern-Isberner, T. Lukasiewicz, Combining probabilistic logic programming with the power of maximumentropy, Artificial Intelligence 157 (1–2) (2004) 139–202.[40] S. Kraus, D. Lehmann, M. Magidor, Nonmonotonic reasoning, preferential models and cumulative logics,Artificial Intelligence 14 (1) (1990) 167–207.[41] H.E. Kyburg Jr., The Logical Foundations of Statistical Inference, D. Reidel, Dordrecht, 1974.[42] H.E. Kyburg Jr., The reference class, Philos. Sci. 50 (1983) 374–397.[43] H.E. Kyburg Jr., C.-M. Teng, Evaluating defaults, in: Proceedings NMR-02, 2002, pp. 257–264.[44] P. Lamarre, A promenade from monotonicity to non-monotonicity following a theorem prover, in: Proceed-ings KR-92, Cambridge, MA, Morgan Kaufmann, San Mateo, CA, 1992, pp. 572–580.[45] D. Lehmann, What does a conditional knowledge base entail?, in: Proceedings KR-89, Morgan Kaufmann,San Mateo, CA, 1989, pp. 212–222.[46] D. Lehmann, Another perspective on default reasoning, Ann. Math. Artificial Intelligence 15 (1) (1995)61–82.[47] D. Lehmann, M. Magidor, What does a conditional knowledge base entail?, Artificial Intelligence 55 (1)(1992) 1–60.[48] T. Lukasiewicz, Local probabilistic deduction from taxonomic and probabilistic knowledge-bases over con-junctive events, Internat. J. Approx. Reason. 21 (1) (1999) 23–61.[49] T. Lukasiewicz, Probabilistic deduction with conditional constraints over basic events, J. Artificial Intelli-gence Res. 10 (1999) 199–241.[50] T. Lukasiewicz, Probabilistic logic programming under inheritance with overriding, in: Proceedings UAI-01,Morgan Kaufmann, San Mateo, CA, 2001, pp. 329–336.[51] T. Lukasiewicz, Probabilistic logic programming with conditional constraints, ACM Trans. Computat. Logic(TOCL) 2 (3) (2001) 289–339.[52] T. Lukasiewicz, Probabilistic default reasoning with conditional constraints, Ann. Math. Artificial Intelli-gence 34 (1–3) (2002) 35–88.[53] T. Lukasiewicz, Nonmonotonic probabilistic logics under variable-strength inheritance with overriding: Al-gorithms and implementation in NMPROBLOG, in: Proceedings ISIPTA-05, 2005.[54] N.J. Nilsson, Probabilistic logic, Artificial Intelligence 28 (1986) 71–88.[55] N.J. Nilsson, Probabilistic logic revisited, Artificial Intelligence 59 (1993) 39–42.[56] C.H. Papadimitriou, Computational Complexity, Addison-Wesley, Reading, MA, 1994.[57] J. Pearl, Probabilistic semantics for nonmonotonic reasoning: A survey, in: Proceedings KR-89, MorganKaufmann, San Mateo, CA, 1989, pp. 505–516.[58] J. Pearl, System Z: A natural ordering of defaults with tractable applications to default reasoning, in: Pro-ceedings TARK-90, Morgan Kaufmann, San Mateo, CA, 1990, pp. 121–135.[59] R. Pelessoni, P. Vicig, A consistency problem for imprecise conditional probability assessments, in: Pro-ceedings IPMU-98, 1998, pp. 1478–1485.[60] J.L. Pollock, Nomic Probabilities and the Foundations of Induction, Oxford University Press, Oxford, 1990.[61] D. Poole, The effect of knowledge on belief: Conditioning, specificity and the lottery paradox in defaultreasoning, Artificial Intelligence 49 (1–3) (1991) 281–307.[62] H. Reichenbach, Theory of Probability, University of California Press, Berkeley, CA, 1949.[63] R. Reiter, A logic for default reasoning, Artificial Intelligence 13 (1–2) (1980) 81–132.[64] Y. Shoham, A semantical approach to nonmonotonic logics, in: Proceedings LICS-87, IEEE Computer So-ciety, 1987, pp. 275–279.T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161161[65] W. Spohn, Ordinal conditional functions: A dynamic theory of epistemic states, in: W. Harper, B. Skyrms(Eds.), Causation in Decision, Belief Change, and Statistics, vol. 2, Reidel, Dordrecht, 1988, pp. 105–134.[66] P. Vicig, An algorithm for imprecise conditional probability assessments in expert systems, in: ProceedingsIPMU-96, 1996, pp. 61–66.[67] G.R. Wheeler, A resource bounded default logic, in: Proceedings NMR-04, 2004, pp. 416–422.[68] P. Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall, 1991.