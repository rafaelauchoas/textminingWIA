Artificial Intelligence 199–200 (2013) 67–92Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintHow much does it help to know what she knows you know?An agent-based simulation studyHarmen de Weerd∗, Rineke Verbrugge, Bart VerheijInstitute of Artificial Intelligence, Faculty of Mathematics and Natural Sciences, University of Groningen, PO Box 407, 9700 AK Groningen,The Netherlandsa r t i c l ei n f oa b s t r a c tArticle history:Received 2 October 2012Received in revised form 8 May 2013Accepted 10 May 2013Available online 14 May 2013Keywords:Agent-based modelsEvolution of theory of mind1. IntroductionIn everyday life, people make use of theory of mind by explicitly attributing unobservablemental content such as beliefs, desires, and intentions to others. Humans are known to beable to use this ability recursively. That is, they engage in higher-order theory of mind, andconsider what others believe about their own beliefs. In this paper, we use agent-basedcomputational models to investigate the evolution of higher-order theory of mind. Weconsider higher-order theory of mind across four different competitive games, includingrepeated single-shot and repeated extensive form games, and determine the advantageof higher-order theory of mind agents over their lower-order theory of mind opponents.Across these four games, we find a common pattern in which first-order and second-ordertheory of mind agents clearly outperform opponents that are more limited in their abilityto make use of theory of mind, while the advantage for deeper recursion to third-ordertheory of mind is limited in comparison.© 2013 Elsevier B.V. All rights reserved.In everyday life, we regularly make use of theory of mind, by reasoning about what other people know and believe.For example, we identify with characters in literature and movies, and accept that they may have beliefs and intentionsdifferent from our own. When telling a joke, a speaker engages in higher-order theory of mind, by believing that the hearerknows that the speaker does not intend to convey an actual fact or opinion. In this paper, we make use of agent-basedcomputational models to explain the evolution of our ability to reason about mental content of others.1In settings where humans and computational agents perform actions that influence each other’s decision-making pro-cess, for example in automated negotiation [3,4], it is necessary to accurately predict the behaviour of others in order torespond appropriately. In artificial intelligence, modeling an opponent explicitly can be achieved through formal approachessuch as for example dynamic epistemic logic [5,6], recursive opponent modeling [7], interactive POMDPs [8], networks ofinfluence diagrams [9], game theory of mind [10], or iterated best-response models such as cognitive hierarchy models[11] and level-n theory [12,13]. These models allow for recursive modeling of an opponent, by modeling the opponent asan opponent-modeling agent itself, creating increasingly complicated models to predict the actions of increasingly sophisti-cated opponents. For cognitive agents that are meant to interact with humans, it is important to know whether these formalmodels of cognition allow for accurate modeling of human reasoning, or whether other models better capture the type ofbounded rationality exhibited by humans [14,15].* Corresponding author. Tel.: +31 50 363 4114; fax: +31 50 363 6687.E-mail addresses: hdeweerd@ai.rug.nl (H. de Weerd), rineke@ai.rug.nl (R. Verbrugge), b.verheij@ai.rug.nl (B. Verheij).1 This research is a continuation of [1,2].0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2013.05.00468H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–921.1. Theory of mind abilities in humans and animalsIn humans, the ability to predict the actions of others by explicitly attributing to them unobservable mental content, suchas beliefs, desires, and intentions, is known in psychology as theory of mind [16]. Experiments in which humans play gamesshow evidence that humans use theory of mind recursively in their decision-making process [17–20]. They take this abilityto a second-order theory of mind, in which they reason about the way others reason about mental content. For example,when asked to search for a hidden object in one of four boxes, participants tend to ignore the most salient box, using theirnested belief that a hider would believe that a seeker would consider the most obvious place to search for a hidden objectto be a box that stands out [21].The use of higher-order (i.e. at least second-order) theory of mind allows individuals to make a second-order attributionsuch as “Alice doesn’t know that Bob knows that she is throwing him a surprise party”. The human ability for higher-ordertheory of mind is well-established, both through false belief tasks [17,22,23] and strategic games [18–20,24]. However, theuse of theory of mind of any kind by non-human species is a controversial matter. Primates [25,26], monkeys [27], butalso goats [28], dogs [29] and corvids [30,31] have been proposed to be able to take the mental content of others intoaccount. However, experiments in which animals behave in a way that is consistent with them having a theory of mind arecriticized for not being able to distinguish between theory of mind and strategies that do not rely on mental state attribution[32,33]. Opponents of attributing theory of mind to animals posit that the animal could have learned the behaviour throughprevious experiences, combined with simple mechanisms such as stress [34,35]. Likewise, experiments in which animals failto show an ability to attribute mental states to others are criticized as well, either for being too complex or ecologically notmeaningful [36].1.2. Evolution of theory of mindThe differences in the ability to make use of theory of mind between humans and other animals raise the issue ofthe reason for the evolution of a system that allows humans to make use of theory of mind recursively, and use higher-order theory of mind to reason about what other people understand about mental content, while other animals, includingchimpanzees and other primates, do not appear to have this ability. Furthermore, whereas recursive opponent modelingcould continue indefinitely, humans appear to use higher-order theory of mind only up to a certain point [18,19,37]. In anevolutionary sense, the costs of using higher orders of theory of mind may therefore outweigh the benefits.One of the hypotheses that explain the emergence of social cognition is the Machiavellian intelligence hypothesis2 [38].According to the Machiavellian intelligence hypothesis, social cognition allows individuals to make use of deception andsocial manipulation to obtain an evolutionary advantage over others. If a parallel can be drawn to higher-order theory ofmind, the evolution of a higher-order theory of mind would then be favored by giving individuals a competitive advantageover others. This way, the ability to make use of higher-order theory of mind would both be beneficial to individuals thathave this trait, as well as detrimental to individuals without such abilities.In this paper, we aim to test the Machiavellian intelligence hypothesis by making use of agent-based modeling in an at-tempt to show that there are reasonably natural competitive settings in which higher-order theory of mind is advantageousfor agents.1.3. Agent-based modelingAgent-based modeling is a simulation technique in which individual agents act and interact based on their own percep-tion of their local situation. By explicitly modeling heterogeneity among individual agents, agent-based models can representsystems that are too complex to capture through equation-based modeling approaches. This technique has proven its use-fulness as a research tool to investigate how behavioral patterns may emerge from the interactions between individuals(cf. [39,40]). Among others, agent-based models have been used to explain fighting in crowds [41], trust in negotiations[42], the evolution of agriculture [43], the evolution of cooperation and punishment [44–46], and the evolution of lan-guage [37,47–49]. In this paper, we consider agent-based computational models to investigate the advantages of makinguse of higher-order theory of mind. The use of agent-based models allows us to precisely control and monitor the mentalcontent, including application of theory of mind, of our test subjects. This allows us to simulate computational agents ingame settings, and determine the extent to which higher-order theory of mind provides individuals with an advantage overcompetitors that are more restricted in their use of theory of mind. By varying game settings, this allows us to determinescenarios in which the ability to make use of theory of mind is beneficial to an agent, as well as whether increasingly higherorders of theory of mind provide individuals with increasing advantages over competitors.To test the Machiavellian intelligence hypothesis, we consider a number of competitive zero-sum games in which we letour computational agents compete to determine whether the ability to make use of higher-order theory of mind is advanta-geous in a competitive setting. We consider four different games. First, we consider three variations on repeated single-shot2 For a discussion of alternative hypotheses, see [37].H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9269Table 1(RPS) Payoff table and graph representation for the rock–paper–scissors game. The tableshows the payoff for the player choosing the row action ‘rock’,‘paper’ or ‘scissors’, forevery possible choice of the player choosing the column action. Arrows in the graph areread as ‘defeats’. For example, the arrow from ‘paper’ to ‘rock’ means that ‘paper’ defeats‘rock’.RockPaperScissorsRockPaperScissors01−1−1011−10rock–paper–scissors (RPS) games. The transparent setup of RPS allows us to relate differences in the effectiveness of higher-order theory of mind more easily to the structure of the game. The fourth game is Limited Bidding, which involves planningover multiple rounds of play. We also consider a more complex, extensive form game to judge how well the evolutionaryadvantage of making use of higher-order theory of mind generalizes across games. The four games are described in detailin Section 2.Agents may benefit from theory of mind in these games by considering the position of their opponent, and determiningwhat mental content they would have if the roles were reversed. This process is first described intuitively in Section 3.A formal description of the model we use is presented in Section 4. To determine whether the use of theory of mindpresents agents with an advantage over opponents without such abilities, we placed agents of different orders of theory ofmind in competition with one another. The results of these can be found in Section 5. Finally, Section 6 provides discussionand gives directions for future research.Throughout this paper, we will be considering agents engaged in a competitive two-player game. To avoid confusion, wewill refer to the focal agent or player as if he were male, and his opponent as if she were female.2. Game settingsWe investigate theory of mind in four game settings. The games we describe are strictly competitive games in the sensethat they are zero-sum games; there is no possibility for a win–win situation in these games. In each of the games wepresent, each player can guarantee an expected outcome of zero, irrespective of how his opponent plays. That is, the valueof each of these games is zero. By playing a mixed strategy, in which the player randomly selects one of the actions he canperform, a player can prevent his opponent from structurally winning the game. However, through repeated games, a playermay learn regularities in his opponent’s strategy over time, which he might be able to use to his advantage.2.1. Rock–paper–scissors variationsIn the following subsections, we describe the well-known game rock–paper–scissors (RPS), as well as two variations.The rock–paper–scissors game, also known as RoShamBo, is a game settings in which the ability to model an opponenthas informally demonstrated its relevance and applicability [50,51]. Although no strategy can consistently defeat an agentthat plays RPS randomly, an agent that repeatedly encounters the same opponent in the setting of an RPS game may useregularities in the opponent’s strategy to its advantage. In programming competitions [50], the random strategy only resultsin an average score. The existence of agents that play according to a non-randomizing strategy allows stronger players toincrease their score at the expense of weaker players. The champion of the programming competition in 2000 made useof strategies that detect regularities in the opponent’s behaviour, but also considered the possibility that the opponent wasusing similar strategies to model the champion’s behaviour [51].2.1.1. Rock–paper–scissorsThe game of rock–paper–scissors (RPS) [52] is a two-player symmetric zero-sum game in which both players simultane-ously choose one of the three possible actions ‘rock’, ‘paper’, or ‘scissors’. If both choose the same action, the game ends ina tie. Otherwise, the player that chooses ‘rock’ wins from the one that chooses ‘scissors’, ‘scissors’ wins from ‘paper’, and‘paper’ wins from ‘rock’. The game can be represented as shown in Table 1, which shows the payoff table and a graph rep-resentation for the RPS game. The matrix shows the payoff for the player choosing the row action for every possible choiceof the player choosing the column action. In the graph, an arrow from action A to action B denotes the relation ‘A defeatsB’.RPS is known to have a unique mixed-strategy Nash equilibrium (see e.g. [53]) in which the player chooses each ofthe options with equal probability. That is, when player strategies are known, there is always a player that can improvehis expected outcome unless both players play by randomly choosing one of the possible actions. When agents repeatedlyplay RPS against the same opponent, an agent that randomizes his actions prevents his opponent from taking advantagesof regularities in his strategy. However, randomizing also prevents the agent from exploiting regularities in his opponent’sbehaviour that may show up over repeated games. By correctly modeling regularities in an opponent’s behaviour, an agent70H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92Table 2(ERPS) Payoff table and graph representation for the elemental rock–paper–scissors game, in which agents choose be-tween five different actions. The table shows the payoff for the player choosing the row action ‘wood’,‘fire’,‘water’, or ‘earth’, for every possible choice of the player choosing the column action. Arrows in the graph are read as‘defeats’. For example, the arrow between ‘wood’ and ‘earth’ means that ‘wood’ defeats ‘earth’.‘metal’,WoodMetalFireWaterEarthWoodMetalFireWaterEarth0100−1−101000−101000−101100−10Table 3(RPSLS) Payoff table and graph representation for the rock–paper–scissors–lizard–Spock game. The table shows thepayoff for the player choosing the row action ‘rock’, ‘paper’, ‘scissors’, ‘lizard’ and ‘Spock’, for every possible choice ofthe player choosing the column action. Arrows in the graph are read as ‘defeats’. For example, the arrow between ‘lizard’and ‘Spock’ means that ‘lizard’ defeats ‘Spock’.RockPaperScissorsLizardSpockRockPaperScissorsLizardSpock01−1−11−1011−11−10−111−110−1−11−110can increase his score at the expense of his opponent. Experimental evidence suggests that human participants are poor atgenerating random sequences [54,55], and play RPS in a non-random way [56,57].We expect that the ability to make use of theory of mind will present an agent with an advantage over opponentswithout such abilities. The champion of the first international RPS programming competition in 2000 made use of a strategythat detected regularities in the opponent’s behaviour, but also considered the possibility that the opponent was usinga similar strategy to model the behaviour of the champion’s [51]. That is, this program engaged in theory of mind, byattributing the intention to win the game to his opponent. However, due to the limited action space, there may be a limitto the effectiveness of theory of mind that is specific to this particular game.2.1.2. Elemental rock–paper–scissorsAlthough the simple structure of RPS is appealing, the limitation to three actions may influence the effectiveness ofhigher-order theory of mind. To address this issue, we also consider elemental rock–paper–scissors (ERPS). ERPS extends RPSsuch that it includes the five actions ‘wood’, ‘metal’, ‘fire’, ‘water’, and ‘earth’, as shown in Table 2. The ERPS game preservesthe property of RPS that each action is defeated by exactly one response. That is, for each action that an opponent mayplay, there exists a unique best response that guarantees a positive outcome for the agent.3As in the case of RPS, the unique mixed-strategy Nash equilibrium for ERPS is to randomize over all possible actions.However, due to the increased action space, ERPS may have an increased support for theory of mind. That is, we expecttheory of mind agents to perform at least as well on ERPS as they would in RPS. Moreover, any differences in the perfor-mance of theory of mind agents playing ERPS, compared to those playing RPS, can be attributed to the differences in thestructure of the games. In particular, increased performance of higher-order theory of mind agents in ERPS indicates that alimited action space influences the effectiveness of theory of mind.2.1.3. Rock–paper–scissors–lizard–SpockRock–paper–scissors–lizard–Spock (RPSLS) [58] is an extension of RPS, which adds the actions ‘lizard’ and ‘Spock’ to theactions ‘rock’, ‘paper’, and ‘scissors’ from RPS. Like ERPS, RPSLS has five actions, but in RPSLS each action wins from exactlytwo other actions, while being defeated by the remaining two other actions. Table 3 shows the payoff matrix and a graphrepresentation of the RPSLS game.Unlike the previous two games, the best response to an action in RPSLS is not unique. This means that when an agentattributes mental content to his opponent, this does not result in a clear prediction of opponent behaviour. An agent thatpredicts his opponent to play ‘paper’ has no preference for playing either ‘scissors’ or ‘lizard’, since either will defeat ‘paper’equally well. As a result, an agent that believes his opponent to believe that the agent will play ‘paper’, will predict that3 The payoffs in elemental rock–paper–scissors are based on the overcoming cycle of elements in the Chinese philosophy Wu Xing.H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9271Fig. 1. Example of the way limited bidding is played. (For interpretation of the references to color in this figure, the reader is referred to the web versionof this article.)she is going to play either ‘scissors’ or ‘lizard’. Similarly, the opponent’s behaviour in RPSLS is less informative than in RPSand ERPS. After all, if an agent plays ‘paper’, it may have believed that his opponent would play ‘rock’, but also that shewould play ‘Spock’. As a result, playing RPSLS repeatedly against the same opponent provides less information about theopponent’s behaviour than in RPS and ERPS. Due to this increased difficulty in modeling the opponent, we expect thattheory of mind agents perform more poorly in RPSLS than in RPS and ERPS.2.2. Limited BiddingUnlike rock–paper–scissors and the two variations on it we presented in the previous subsections, Limited Bidding (LB)4is a game that plays across several rounds. When the game starts, each player is handed an identical set of 5 tokens each,valued 1 to 5. Over the course of 5 rounds, players simultaneously choose one of their own tokens to use as a ‘bid’ for theround. Once both players have made their choice, the tokens selected by the players are revealed and compared, and theround is won by the player that selected the highest value token. In case of a draw, there are no winners. The object of thegame is to win as many rounds as possible while losing as few rounds as possible. However, each token may be used onlyonce per game. This forces players to plan ahead and strategically choose which of the tokens that are still available to themthey should place as the bid. For example, a player that selects the token with value 5 in the first round will make surethat the first round will not result in a win for his opponent. However, this also means that for the remaining 4 rounds, thetoken with value 5 will not be available to this player. Players therefore have to weigh the additional probability that theywill win the current round against the loss of competitive strength in later rounds that results from using a higher valuedtoken. Fig. 1 shows an example of the way LB is played. In this case, the game is won by the light blue player on the right.Note that in LB, it is not possible to win all the rounds. Instead, any player can win a maximum of four rounds, inwhich case the last round is won by his opponent. As a result, a player can achieve a maximum score of 3 in LB. As for thevariations on RPS described earlier, a player can prevent his opponent from winning the game. He can do so by randomlychoosing to play one of the tokens still available to him at each round of the game. Averaged over repeated games, thismixed strategy of randomizing over all available choices will result in a score of zero for both the player and his opponent.2.3. Rational playersIn game theory, it is common to make the assumption that every player is rational, and that this fact is known by allplayers. Moreover, players are assumed to know that everyone knows that every player is rational, continuing in this fashionad infinitum. In terms of theory of mind, this common knowledge of rationality [60,61] means that players possess the abilityto make use of theory of mind of any depth or order. In this section, we will explain how rational players play the LimitedBidding game under the assumption of common knowledge of rationality.For simplicity, we consider a limited bidding game of three tokens. In such a game, players decide what token to playat two moments: once at the start of the game, and again once the result of the first round has been announced. Althoughnew information also becomes available after the second round, the choice of which token to play in the third round is adegenerate one; at the start of the third round both players only have one token left. Since both players have the choice ofthree tokens to play in the first round, there are nine variations of the subgame the agents play at the second round of thegame. We first consider what a rational agent will choose to do at the start of the second round.Since every player tries to maximize the number of rounds won and minimize the numbers of rounds lost, at the end ofeach game, each player receives a payoff equal to the difference between the two. Table 4 lists the payoffs for both playersfor each possible outcome of the game, where each outcome is represented as the concatenation of the tokens in the orderin which the player has played them. Each payoff structure is presented as a tuple (x, y), such that player 1 receives payoffx and player 2 receives payoff y. The subgames that are played at the beginning of the second round are represented as2-by-2 submatrices, highlighted by alternating background color in Table 4.4 Limited Bidding is an adaptation of a game presented in [59].72H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92Table 4Payoff table for the limited bidding game of three tokens. Each outcome of the game corresponds to a tuple in the table. The first value of the tuple is thepayoff for player one, the second is the payoff for player two.Player 1123132213231312321Player 2123(0, 0)(0, 0)(0, 0)(1, −1)(−1, 1)(0, 0)132(0, 0)(0, 0)(1, −1)(0, 0)(0, 0)(−1, 1)213(0, 0)(−1, 1)(0, 0)(0, 0)(0, 0)(1, −1)231(−1, 1)(0, 0)(0, 0)(0, 0)(1, −1)(0, 0)312(1, −1)(0, 0)(0, 0)(−1, 1)(0, 0)(0, 0)321(0, 0)(1, −1)(−1, 1)(0, 0)(0, 0)(0, 0)Table 5Payoff table for the limited bidding game of three tokens once the players have derived thatafter the first round, both players will play randomly.Player 1123Player 21(0.0, 0.0)(0.5, −0.5)(−0.5, 0.5)2(−0.5, 0.5)(0.0, 0.0)(0.5, −0.5)3(0.5, −0.5)(−0.5, 0.5)(0.0, 0.0)Note that whenever the first round of the game ends in a draw, the resulting subgame is a degenerate one. In this case,both players receive zero payoff irrespective of the final outcome. When the first round does not end in a draw, the resultingsubgame is a variation on the matching pennies game [62]. This game is known to have no pure-strategy Nash equilibrium.That is, there is no combination of pure strategies such that each player maximizes his payoff given the strategy of hisopponent. However, there is a unique mixed-strategy Nash equilibrium in which each player plays each possible strategywith equal probability. If both players play either one of their remaining tokens with 50% probability, neither one of themhas an incentive to switch strategies: given that his opponent is playing randomly, a rational agent has no strategy availablethat will yield a better expected payoff than playing randomly as well.Due to the common knowledge of rationality, each player knows that both of them have reached the conclusion thatafter the first round, they will both play randomly. This means we can rewrite the payoff matrix to reflect the results ofeach of the subgames, as shown in Table 5. Note that this is a variation of the rock–paper–scissors game. As before, there isno pure-strategy Nash equilibrium, but the unique mixed-strategy Nash equilibrium is reached when both players play eachstrategy with equal probability. That is, rational agents, under the assumption of common knowledge of rationality, solvethe limited bidding game by playing randomly at each round.This result also holds when the game is played using more than three tokens. That is, to prevent their opponent fromtaking advantage of any regularity in their strategy, rational agents play the limited bidding game randomly.2.4. Hypotheses about the effectiveness of theory of mindIn this section, we described four different games: rock–paper–scissors, elemental rock–paper–scissors, rock–paper–scissors–lizard–Spock and Limited Bidding. In the game of rock–paper–scissors, agents choose from the three possibleactions, each of which is defeated by exactly one of the other actions. The game of elemental rock–paper–scissors re-sembles RPS in that each action is defeated by exactly one other action, but agents playing ERPS have five different actionsto choose from. Rock–paper–scissors–lizard–Spock allows for five different actions as well, but unlike ERPS, each action isdefeated by exactly two other actions. Finally, Limited Bidding is a game that spans several rounds, in which agents decidethe order in which they play tokens from an initial set of five.The game of RPS serves as a transparent base scenario to determine whether theory of mind benefits agents in competi-tive settings. We expect that the ability to make use of theory of mind is advantageous in competitive settings. Specifically,we expect that the ability to make use of higher orders of theory of mind allows an agent to outperform an opponent thatis of a lower order of theory of mind in the game of rock–paper–scissors. In the remainder, we will refer to this expectationas hypothesis H RPS.The small number of actions that agents choose from in RPS may limit the effectiveness of higher-order theory of mind.The ERPS game, in which agents have a larger action space, addresses this issue. We expect that the larger action space inthe ERPS game allows higher-order theory of mind agents to outperform opponents of a lower order of theory of mind atleast as well as in the RPS game, which we will refer to as hypothesis H ERPS.Agents make use of theory of mind to model the opponent in an attempt to predict her behaviour. As a result, theory ofmind is likely to be more effective when the opponent is more predictable. We therefore selected the RPSLS game, in whichthere is no unique best-response to each action, which should make opponent behaviour harder to predict. HypothesisH. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9273Fig. 2. Example of a possible thought process of a zero-order theory of mind agent.Fig. 3. Example of a possible thought process of a first-order theory of mind agent.HRPSLS states that we expect theory of mind agents to have difficulty predicting their opponent in RPSLS, and to performmore poorly compared to performance in RPS and ERPS.Limited Bidding is a multi-stage game, and represents a more complex situation than the single-shot games RPS, ERPS,and RPSLS. This game has been selected to determine whether theory of mind is advantageous, and whether the resultsfrom simple single-shot games translate towards a more complex setting. We expect that the results in Limited Biddingmay be quantitatively different to those of rock–paper–scissors, but that the results will be qualitatively similar; we will callthis hypothesis H LB.3. Playing the games using simulation-theory of mindIn the games described in Section 2, players can prevent their opponent from winning the game by playing randomly.However, this strategy not only prevents an agent from losing the game from his opponent, but also prevents the agentfrom winning the game for himself. As a result, the randomizing strategy only results in an average score in the RoShamBoprogramming competitions [50] discussed in Section 2.1. An agent that believes its opponent to play in a non-random waymay try to predict the opponent’s behaviour to take advantage of regularities in its opponent’s strategy, and win the game.For humans, one way of generating predictions of opponent behaviour is by using simulation-theory of mind [63–65].In simulation-theory of mind, a player takes the perspective of its opponent, and determines what its own decision wouldbe if the player had been in the position faced by its opponent. Using the implicit assumption that the opponent’s thoughtprocess can be accurately modeled by its own thought process, the player then predicts that the opponent will make thesame decision the player would have made if the roles were reversed.In this section, we describe the intuition behind the process of perspective-taking for agents that differ in their abilitiesto explicitly model mental states, and illustrate how this affects their choices in playing RPS. In Section 4, this intuitionis described in a computational model. In the remainder, we will speak of a ToMk agent to indicate an agent that has theability to use theory of mind up to and including the k-th order, but not beyond.3.1. Zero-order theory of mindA zero-order theory of mind (ToM0) agent is unable to model the mental content such as beliefs, desires and intentionsof his opponent. In particular, a ToM0 agent is unable to represent that his opponent has goals that are different from hisown goals. When predicting his opponent’s behaviour, the agent is limited to his memory of previous events. The ToM0agent is intended to model an inexperienced or frustrated player, who only consider his opponent’s behaviour rather thanthinking about the way she reacts to his actions.A ToM0 agent believes that what happened in the past is a good predictor for what is going to happen in the future.This reflects human players’ tendency to interpret repetition as indicative for a pattern [66]. Fig. 2 illustrates a possiblethought process of a ToM0 agent. If a ToM0 agent remembers that his opponent mostly played ‘paper’ in previous RPSgames, he concludes that his opponent is most likely to play ‘paper’ in the next game. Given this belief, the ToM0 agentwould therefore adjust his behaviour to play ‘scissors’.3.2. First-order theory of mindIn contrast to a ToM0 agent, a first-order theory of mind (ToM1) agent considers the possibility that his opponent istrying to win the game for herself, and that she reacts to the choices made by the ToM1 agent. To predict his opponent’sbehaviour, the ToM1 agent puts himself in the position of his opponent, and considers the information available to himfrom her perspective. Fig. 3 shows an example of such a thought process. Suppose that the ToM1 agent remembers that hemostly played ‘paper’ in previous RPS games against the same opponent. He realizes that if the roles were reversed, and74H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92Fig. 4. Example of a possible thought process of a second-order theory of mind agent.he would remember that his opponent mostly played ‘paper’, he would conclude that his opponent would most likely beplaying ‘paper’ again, and that he should play ‘scissors’. The ToM1 agent has the ability to attribute this thought process tohis opponent, and predict that she is likely to play ‘scissors’. Given this prediction, the ToM1 agent should play ‘rock’.Although the ToM1 agent models his opponent as being able use zero-order theory of mind, agents in our setup do notknow the extent of the abilities of their opponents with certainty. Through repeated interaction, a ToM1 agent may come tobelieve that his opponent is not a ToM0 agent, and that she has no beliefs at all. Such an opponent without beliefs could,for example, play ‘rock’ irrespective of what the ToM1 agent has previously played. Based on this belief, a ToM1 agent canchoose to play as if he were a ToM0 agent, and follow a thought process such as the one presented in Fig. 2.3.3. Second-order theory of mindJust as a ToM1 agent models his opponent as having a zero-order theory of mind, a second-order theory of mind (ToM2)agent models a ToM1 opponent. That is, a ToM2 agent considers the possibility that his opponent is putting herself in hisposition, and is modeling him as a ToM0 agent. Fig. 4 depicts a possible though process of a ToM2 agent. If the ToM2 agentremembers his opponent to have mostly played ‘paper’ in previous encounters, he would believe his opponent to predictthat he will be playing ‘scissors’ more often. As a result, the ToM2 agent would predict his opponent to play ‘rock’ moreoften, in which case the agent should play ‘paper’ more often himself.Each additional order of theory of mind allows for deeper recursion of mental state attribution. Note that each orderof theory of mind represents an additional model for opponent behaviour, with a corresponding prediction. A ToM2 agenttherefore considers three predictions of his opponent’s behaviour, based on the application of zero-order, first-order, andsecond-order theory of mind.4. ModelWe implemented computational agents that make use of simulation-theory of mind, and play similarly to intuitive de-scription of Section 3. In this section, we discuss the implementation of these agents, which play the competitive gamesdescribed in Section 2. The agents presented here differ in their ability to explicitly represent beliefs, and therefore in theirability to make use of theory of mind.4.1. Representation of the gamesIn the model we discuss, a game is a tuple G = (cid:3)N , S, A, T , π (cid:4), where:• N = {i, j} is the set of agents, where i denotes the focal agent, and j denotes his opponent;• S is the set of possible states of the game;• A = Ai × A j is the set of possible action pairs, where Ai is the set of actions that can be performed by the agent i,and A j is the set of actions that can be performed by his opponent j;• T is a partial transition function T : S × A → S, which describes the results of the pair of actions of the focal agent andhis opponent on the game state; and• π = (πi, π j) is the pair of payoff functions πi, π j : S × A → R.An instance of a game as played by two players then consists of an initial game state and a sequence of action pairs. Forexample, in Limited Bidding, each game state s ∈ S encodes the tokens that are still available to the agent, as well as thosestill available to his opponent. This allows agents that are playing LB to distinguish between individual rounds, and maketheir beliefs concerning the opponent’s gameplay conditional on the tokens that can still be played.Fig. 5 shows a representation of an instance of an LB game, which corresponds to the example game shown in Fig. 1. InFig. 5, states are represented by boxes, while arrows show the state transitions. Each state transition shows the action pairthat caused the transition, as well as the payoff πi for the focal agent and the payoff π j for his opponent. The game startsin the initial state in which both the agent and his opponent each have an identical set of five tokens. The actions that theH. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9275Fig. 5. Representation of the instance of the Limited Bidding game shown in Fig. 1. The figure shows the action pairs that transition the game from theinitial state to the final state.agent can perform correspond to selecting one of the tokens that is still available to him. Once the agent and his opponenthave selected an action, the game transitions to a new state, and both players receive a payoff. This process is repeateduntil a final state is reached in which no combination of actions leads to a change in game state. For Limited Bidding, thisfinal state is the situation after five rounds, when there are no more tokens to play.The game states S are intended to model the different stages of a multi-stage game such as Limited Bidding. Single-shotgames, such as the variations on rock–paper–scissors described in Section 2, can be represented as a game that containstwo states S = {s0, s1}, such that the game transitions from the start state s0 to the end state s1 through any possible actionpair (ai, a j) ∈ A. That is, T (s0, (ai, a j)) = s1 for all (ai, a j) ∈ A.Additional to transitioning to a new game state, agents receive a payoff based on their actions. The payoff functionsπi, π j : S × A → R determine the payoff πi(s, (ai, a j)) for the focal agent i, and payoff π j(s, (ai, a j)) for his opponent foreach combination of game state s ∈ S and action pair (ai, a j) ∈ A. Note that since we consider only zero-sum games here,πi(s, (ai, a j)) = −π j(s, (ai, a j)).4.2. Zero-order theory of mind agentsIn the present setup, we assume that agents understand the game. Furthermore, we assume no agent considers thepossibility that his opponent does not understand the game, or that his opponent believes that he does not understand thegame, continuing in this fashion ad infinitum. This means that, for example, none of the agents considers it possible that hisopponent will perform an action that is not in her action space A j . Similarly, no agent believes that his opponent considersit a possibility that he himself will play an action that is not in his action space Ai .Note that this assumption is similar to the assumption that the rules and dynamics of the game are common knowledge[5,67,68], which requires that each agent understands the game, and knows that his opponent understands the game, andso forth. However, the simulated agents described here are limited in their ability to make use of theory of mind, and maynot be able to represent their opponent’s mental content. That is, in this case it is not possible to assume that the rules anddynamics of the game are common knowledge. Instead, we assume that no agent has beliefs that conflict with commonknowledge of the rules and dynamics of the game.Agents form beliefs b(0) in the form of a probability distribution over the opponent’s actions A j for every game state,such that b(0)(a j; s) represents what the agent believes to be the probability that his opponent will play action a j ∈ A jgiven that the game is in situation s ∈ S. We assume:b(0)(a j; s) (cid:2) 0(cid:2)b(0)(a j; s) = 1for all a j ∈ A j, s ∈ S.for all s ∈ S.a j ∈A j(1)(2)That is, (1) agents assign non-negative probability to their opponent playing a certain action in a certain game state, and(2) the probabilities assigned to each possible opponent action sum up to 1 for each possible game state.For a ToM0 agent, the belief structure b(0) represents the extent of his beliefs concerning his opponent’s behaviour. Giventhe game’s payoff function and his beliefs about the way his opponent plays the game, a ToM0 agent is able to assign asubjective value Φi(ai; b(0), s) to playing a certain action ai ∈ Ai in game state s ∈ S, given his beliefs b(0) concerning hisopponent’s behaviour. To determine this value, the agent considers how likely he considers it to be that his opponent isgoing to play some action a j ∈ A j . If the opponent would play a j , playing ai would yield the agent an immediate payoff(cid:7) = T (s, (ai, a j)). The agentπi(s, (ai, a j)), but it would also cause the game to move forward, and end up in a new state stakes this into account by planning ahead, and determining the maximum value he can achieve when the game reaches. The combination of immediate payoff πi(s, (ai, a j)) and the maximum value that can be achieved in the statestate sT (s, (ai, a j)) are weighted by what the agent believes to be the probability b(0)(a j; s) that his opponent is actually going toplay action a j . The value Φi(ai; b(0), s) that the focal agent i assigns to playing action ai in game state s, based on his beliefb(0) concerning his opponent’s behaviour, is given by(cid:2)(cid:5)(cid:7)(cid:4)(cid:4)(cid:6)(cid:3)ai; b(0), s(cid:4)Φi=b(0)(a j; s) ·(cid:3)(cid:4)s, (ai, a j)πis, (ai, a j).(3)(cid:3)aΦi(cid:7); b(0), T(cid:3)+ maxa(cid:7)∈Aia j ∈A j76H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92Table 6Possible mental contents of a ToM0 agent and a ToM1 agent in a game of rock–paper–scissors.(a) Mental content of the ToM0 agent in Example 1.(b) Mental content of the ToM1 agent in Example 2.Order of theory of mind kOrder of theory of mind kb(0)(R, s0)b(0)(P , s0)b(0)(S, s0)00.50.30.2ckb(k)(R, s0)b(k)(P , s0)b(k)(S, s0)00.50.30.210.90.40.50.1We assume that agents choose rationally given their beliefs. That is, agents choose to play the action ai that maximizes the∗i , given byvalue function. This is represented by the decision function t(cid:3)ai; b(0), s(cid:3)b(0); sΦi(4)(cid:4)(cid:4)t.∗i= arg maxai ∈AiExample 1. Consider an agent that plays rock–paper–scissors against his opponent. The RPS game consists of two statesS = {s0, s1}, where the first state s0 represents the start of the game, and the second state s1 is the end of the game. Theaction spaces of the agent and his opponent are the same, Ai = A j = {R, P , S}. The transition function T is defined suchthat T (s0, ai, a j) = s1 for all ai ∈ Ai , and a j ∈ A j . The payoffs in state s0 are given by Table 1, while payoffs are zero in states1.We consider a ToM0 agent, whose mental content is listed in Table 6a. The agent’s zero-order beliefs b(0) indicate thatthe agent believes that there is a 50% probability that his opponent is going to play R, a 30% probability that his opponentis going to play P , and a 20% probability that his opponent is going to play S. Based on these zero-order beliefs, the agentcan determine the value for each of the actions R, P , and S, based on the expected payoff. For example, the agent believesthat if he plays R, there is a 30% probability that he will lose because his opponent played P , and a 20% probability that hewill win because his opponent played S. This results in the following values:(cid:3)(cid:3)(cid:3)ΦiΦiΦi(cid:4)R; b(0), s0(cid:4)P ; b(0), s0(cid:4)S; b(0), s0= b(0)(R; s0) · πi(s0, R, R) + b(0)(P ; s0) · πi(s0, R, P ) + b(0)(S; s0) · πi(s0, R, S)= 0.5 · 0 + 0.3 · (−1) + 0.2 · 1 = −0.1= 0.5 · 1 + 0.3 · 0 + 0.2 · (−1) = 0.3= 0.5 · (−1) + 0.3 · 1 + 0.2 · 0 = −0.2The agent then chooses to play the action that has maximum value. In this case:(cid:4)(cid:3)b(0); s0∗it= arg maxai ∈Ai(cid:3)ai; b(0), s0(cid:4)Φi= PThat is, the ToM0 agent described in Table 6a chooses to play P .4.3. First-order theory of mind agentsA ToM1 agent attributes beliefs to his opponent in the form of an additional probability distribution b(1). Here, b(1)(ai; s)represents what the agent believes his opponent to judge what the probability is that he will play action ai ∈ Ai in gamestate s ∈ S. However, a ToM1 agent also has zero-order beliefs b(0) about what his opponent will do. The decision processof the ToM1 agent consists of roughly three steps:1. making a prediction ˆa2. integrating the first-order prediction ˆa3. selecting the action that maximizes the agent’s expected payoff, given his integrated beliefs about opponent behaviour.of opponent behaviour, based on the agent’s first-order beliefs b(1);of opponent behaviour and the zero-order belief b(0); and(1)j(1)jLet us describe each step more precisely.(1) First, the ToM1 agent makes a prediction of opponent behaviour based on his first-order beliefs b(1). Using simulation-to make a prediction of the action his opponent will play. To∈ A j that maximizes the value function from the perspective of the opponent,theory of mind, the agent uses his own decision function t(1)do so, the agent determines the action ˆajgiven that the agent believes his opponent to have zero-order beliefs b(1). That is,∗ˆa(1)j= t∗j(cid:4)(cid:3)b(1); s= arg maxa j ∈A j(cid:3)a j; b(1), s(cid:4).Φ j(5)H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9277Note that Eq. (5) is similar to Eq. (4). That is, the ToM1 agent determines his prediction of opponent behaviour similarto the way a ToM0 agent determines his own behaviour. In calculating the prediction ˆa, the agent makes use of hisown value function and his beliefs b(1). Note that by specifying ˆa, the agent makes a single prediction of the opponent’sbehaviour rather than assigning probabilities to each possible opponent action. This allows the agent to check the validity of(1)his prediction more easily, by comparing the prediction ˆaj with the opponent’s actual behaviour. However, this also meansthat slight differences between the agent’s value function and that of his opponent may render the prediction incorrect.(1)j(1)j(2) A ToM1 agent’s first-order theory of mind provides the agent with a prediction ˆaof opponent behaviour. This pre-diction may conflict with his zero-order beliefs b(0). The extent to which first-order theory of mind governs the decisions ofthe agent’s actions is determined by his confidence 0 (cid:3) c1 (cid:3) 1 that first-order theory of mind accurately predicts his oppo-nent’s behaviour. The value of his confidence c1 allows the agent to distinguish between different types of opponents, andhe weights his zero-order beliefs against the prediction of first-order theory of mind accordingly. This weighting process iscaptured by a belief integration function U . This function integrates the agent’s first-order prediction ˆa j with his zero-orderbeliefs b(0) of opponent behaviour. Compared to his zero-order beliefs b(0), the agent’s integrated belief that his opponentwill be playing action ˆais increased, while his integrated belief that his opponent will be playing any other action isdecreased. Specifically,(1)j(1)j(cid:3)b(0), ˆaU(cid:4)(1)j, c1(a j; s) =(cid:7)(1 − c1) · b(0)(a j; s)(1 − c1) · b(0)(a j; s) + c1if a j (cid:8)= ˆaif a j = ˆa(1)j(1)j,.(6)(3) After integrating his zero-order beliefs b(0) and his prediction of opponent behaviour ˆabased on first-order theoryof mind, the agent chooses what action to play. This decision is made analogously to the way a ToM0 agent decides (Eq. (4)).However, the ToM1 agent decides based on his integrated beliefs U (b(0), ˆa, c1) of opponent behaviour, instead of hiszero-order beliefs b(0) directly. That is, a ToM1 agent chooses to play the action given by(cid:3)b(1); s(cid:3)b(0), ˆa(cid:3)b(0), t= t(1)j(cid:4).(7); s; sUU(cid:4)(cid:4)(cid:3)(cid:4)(cid:3)(cid:4)t(1)j, c1(1)j, c1∗i∗i∗jIn the special case where the agent has no confidence in first-order theory of mind, c1 = 0, the ToM1 agent’s decision isonly influenced by his zero-order beliefs. In this case, the agent chooses as if he were a ToM0 agent.Example 2. Consider a ToM1 agent that plays rock–paper–scissors, similar to the agent in Example 1, whose mental contentis given in Table 6b. The table shows that the ToM1 agent has zero-order beliefs b(0), which indicate the agent’s beliefsconcerning his opponent’s actions, as well as first-order beliefs b(1). For example, since b(1)(R, s0) = 0.4, the agent believesthat his opponent believes that there is a 40% probability that he is going to play R. Taking the perspective of his opponent,the agent determines what he would do in her place. That is, the agent first calculates the value that he would assignto each of the actions available to his opponent, if his first-order beliefs b(1) were actually his zero-order beliefs, and hisopponent’s payoffs were actually his payoffs.(cid:3)(cid:3)(cid:3)Φ jΦ jΦ j(cid:4)(cid:4)R; b(1), s0P ; b(1), s0(cid:4)S; b(1), s0= 0.4 · 0 + 0.5 · (−1) + 0.1 · 1 = −0.4= 0.4 · 1 + 0.5 · 0 + 0.1 · (−1) = 0.3= 0.4 · (−1) + 0.5 · 1 + 0.1 · 0 = 0.1The agent’s first-order theory of mind predicts that his opponent will select the action that will yield her the highest payoff.ˆa(1)j= t∗j(cid:4)(cid:3)b(1); s0= arg maxa j ∈A j(cid:3)a j; b(1), s0(cid:4)Φ j= PUsing his first-order theory of mind, the agent predicts that his opponent is going to play P .Note that the agent’s prediction ˆaconflicts with his zero-order beliefs b(0). According to his first-order theory of mind,his opponent is going to play P , while the agent’s zero-order beliefs assign a 50% probability that his opponent is goingto play R. To be able to make a decision, the agent integrates his first-order prediction with his zero-order beliefs b(0). Inthis case, the agent’s confidence c1 in first-order theory of mind is 0.9. This means that the agent’s integrated beliefs aredetermined for 90% by his prediction based on first-order theory of mind, and for 10% by his zero-order beliefs.(1)j(cid:3)b(0), P , 0.9(cid:3)b(0), P , 0.9(cid:3)b(0), P , 0.9(cid:4)(cid:4)(cid:4)UUU(R; s0) = (1 − 0.9) · b(0)(R; s0) = 0.1 · 0.5 = 0.05(P ; s0) = (1 − 0.9) · b(0)(P ; s0) + 0.9 = 0.93(S; s0) = (1 − 0.9) · b(0)(S; s0) = 0.1 · 0.2 = 0.0278H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92After integrating his zero-order beliefs and first-order prediction, the agent believes there is a 5% probability that his oppo-nent is going to play R, a 93% probability that his opponent is going to play P and a 2% probability that his opponent isgoing to play S.Based on his integrated beliefs, the agent determines the value for playing each of the actions.(cid:3)(cid:3)(cid:3)ΦiΦiΦiR; UP ; US; U(cid:4)(cid:3)b(0), P , 0.9(cid:3)b(0), P , 0.9(cid:4)(cid:3)b(0), P , 0.9, s0(cid:4), s0(cid:4), s0(cid:4)(cid:4)= 0.05 · 0 + 0.93 · (−1) + 0.02 · 1 = −0.91= 0.05 · 1 + 0.93 · 0 + 0.02 · (−1) = 0.03= 0.05 · (−1) + 0.93 · 1 + 0.02 · 0 = 0.88The agent then chooses to play the action that has maximum value. In this case:(cid:3)(cid:3)b(0), P , 0.9(cid:4)U(cid:4); s0∗it= arg maxai ∈Ai(cid:3)ai; U(cid:3)b(0), P , 0.9(cid:4)Φi, s0(cid:4)= SThat is, the ToM1 agent described in Table 6b chooses to play S.4.4. Second-order theory of mind agentsSimilar to the way a ToM1 agent models his opponent as a ToM0 agent, a ToM2 agent considers the possibility that hisopponent may be a ToM1 agent. As such, the ToM2 agent has an explicit model of what beliefs he believes his opponent to beattributing to him. In our model, these beliefs are represented by an additional belief structure b(2). Using simulation-theoryof mind, the agent attributes the decision-making process described by Eq. (7) to his opponent. That is, the agent considersthe game from the perspective of his opponent, and determines what he would do in her position, if he were a ToM1agent.To determine his opponent’s actions, the ToM2 agent needs to know her confidence c1 in first-order theory of mind.In our experiments, we have assumed that all ToM2 agents use a value of 0.8 to determine their opponent’s behaviourplaying as a ToM1 agent.5 Based on second-order theory of mind, the ToM2 agent therefore predicts that his opponent willbe playing (square brackets are used for readability)(cid:3)b(2); s(cid:8)b(1), t, 0.8= t; s(8)ˆaU(cid:3)(cid:4)(cid:4)(cid:9).(2)j∗j∗i(2)This prediction ˆabased on second-order theory of mind is integrated with the ToM2 agent’s zero-order beliefs b(0) andj(1)his prediction ˆabased on first-order theory of mind, before he makes his choice of what action to play. As for the ToM1jagent, a ToM2 agent does not know at which order of theory of mind his opponent is playing. Instead, the extent to whichsecond-order theory of mind governs the decisions of the ToM2 agent’s actions is determined by his confidence 0 (cid:3) c2 (cid:3) 1that second-order theory of mind accurately predicts his opponent’s behaviour. The ToM2 agent weights the integratedbeliefs in Eq. (7) against his prediction of opponent behaviour ˆabased on second-order theory of mind. As a result, theToM2 agent’s integrated beliefs about his opponent behaviour are given by(cid:8)b(1), t(cid:4), 0.8(2)j(9)UUU(cid:3)(cid:4)(cid:4)(cid:9)(cid:9)(cid:8).(cid:3)∗b(0), tj(cid:10), c1(cid:4)∗, tj(cid:10)(cid:3)b(1); s(cid:13)(cid:11)(cid:12)(1)jˆa∗i(cid:3)b(2); s(cid:11)(cid:12)(2)jˆa; s(cid:13), c2The ToM2 agent therefore performs two belief integration steps. First, the agent integrates his zero-order beliefs b(0) con-cerning his opponent’s behaviour with his prediction ˆabased on application of first-order theory of mind. In the secondstep, his prediction ˆamakes his final choice of what action to select based on these beliefs:based on second-order theory of mind is integrated into these beliefs as well. The ToM2 agent then(1)j(2)j(cid:3)(cid:8)UU(cid:3)b(0), ˆa∗it(1)j, c1(cid:4), ˆa(2)j, c2(cid:9)(cid:4).; s(10)Example 3. Consider a ToM2 agent that plays rock–paper–scissors, similar to Example 2, whose mental content is givenin Table 7. When a ToM2 agent considers his opponent’s first-order beliefs about his own actions, the agent performs thedecision process of a ToM1 agent from the viewpoint of his opponent. That is, he calculates what he believes that shepredicts that he will do based on her first-order beliefs. The agent’s model of his opponent’s first-order beliefs are captured5 Results from additional simulations using different values of c1 ∈ [0, 1] turned out to be visually indistinguishable from the ones presented here for anyvalue of c1 over 0.5.H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9279Table 7Possible mental content of a ToM2 agent in a game of rock–paper–scissors, as in Example 3.Order of theory of mind kckb(k)(R, s0)b(k)(P , s0)b(k)(S, s0)00.50.30.210.90.40.50.120.10.30.30.4by b(2). This is what the agent believes his opponent to believe his first-order beliefs to be. Firstly, the agent determineswhat he would do if his second-order beliefs b(2) were actually his zero-order beliefs.(cid:3)(cid:4)(cid:4)(cid:3)ΦiΦiR; b(2), s0P ; b(2), s0(cid:4)S; b(2), s0Φi(cid:3)∗b(2); s0i(cid:3)(cid:4)t= 0.3 · 0 + 0.3 · (−1) + 0.4 · 1 = 0.1= 0.3 · 1 + 0.3 · 0 + 0.4 · (−1) = −0.1= 0.3 · (−1) + 0.3 · 1 + 0.4 · 0 = 0(cid:3)ai; b(2), s0= RΦi(cid:4)= arg maxai ∈AiThat is, the ToM2 agent believes his opponent to predict that he will be playing R.Secondly, the agent determines how his opponent’s prediction that he will be playing R influences her zero-order beliefs.The agent does not explicitly model the opponent’s confidence in first-order theory of mind. Rather, he assumes a valueof 0.8 for this confidence. The agent then integrates his first-order beliefs b(1), which he believes to correspond to hisopponent’s zero-order beliefs, with the prediction that he will play R.(cid:3)b(1), R, 0.8(cid:3)b(1), R, 0.8(cid:3)b(1), R, 0.8(cid:4)(cid:4)(cid:4)UUU(R; s0) = 0.2 · b(1)(R; s0) + 0.8 = 0.88(P ; s0) = 0.2 · b(1)(R; s0) = 0.2 · 0.5 = 0.10(S; s0) = 0.2 · b(1)(R; s0) = 0.2 · 0.1 = 0.02These integrated beliefs specify what the agent believes what his opponent’s beliefs are concerning his actions. For example,based on application of his second-order theory of mind, the ToM2 agent believes that his opponent believes that there isan 88% probability that he himself will play R. From the viewpoint of his opponent, the agent then determines what thevalue would be for playing each of the possible actions, given the integrated beliefs of opponent action.(cid:3)(cid:3)(cid:3)ΦiΦiΦiR; UP ; US; U(cid:4)(cid:3)b(1), R, 0.8(cid:3)b(1), R, 0.8(cid:4)(cid:3)b(1), R, 0.8, s0(cid:4), s0(cid:4), s0(cid:4)(cid:4)= 0.88 · 0 + 0.10 · (−1) + 0.02 · 1 = −0.08= 0.88 · 1 + 0.10 · 0 + 0.02 · (−1) = 0.86= 0.88 · (−1) + 0.10 · 1 + 0.02 · 0 = −0.78The action that maximizes this value represents the agent’s prediction of the action his opponent is going to play accordingto his second-order theory of mind.(cid:4)(cid:3)b(1), R, 0.8= P .= tˆaU(cid:4)(cid:3); s0(2)j∗jBased on second-order theory of mind, the agent therefore believes his opponent will play P .To make a decision, the agent integrates his zero-order beliefs b(0), his first-order prediction ˆa= P (see Example 2),and his second-order prediction ˆa= P . Example 2 shows how the agent’s zero-order beliefs and his first-order predictionof opponent behaviour are integrated. Using this confidence c2, the agent also integrates his belief that his opponent isgoing to play P . In this example, the agent has confidence c2 = 0.1 in second-order theory of mind. This results in thefollowing integrated beliefs:(2)j(1)j(cid:3)(cid:3)(cid:3)UUUUUU(cid:3)b(0), P , 0.9(cid:3)b(0), P , 0.9(cid:3)b(0), P , 0.9(cid:4)(cid:4)(cid:4), P , 0.1, P , 0.1, P , 0.1(cid:4)(cid:4)(cid:4)(R; s0) = 0.9 · 0.05 = 0.045(P ; s0) = 0.9 · 0.93 + 0.1 = 0.937(S; s0) = 0.9 · 0.02 = 0.018Based on these integrated beliefs, the agent determines the value for playing each of the actions.(cid:3)(cid:3)(cid:3)(cid:3)U(cid:3)(cid:3)UUR; UP ; US; UΦiΦiΦi(cid:4)(cid:3)b(0), P , 0.9(cid:3)b(0), P , 0.9(cid:4)(cid:3)b(0), P , 0.9, P , 0.1(cid:4)(cid:4), s0(cid:4)(cid:4)(cid:4), P , 0.1(cid:4), s0(cid:4), P , 0.1, s0= 0.018 − 0.937 = −0.919= 0.045 − 0.018 = 0.027= 0.937 − 0.045 = 0.89280H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92The agent then chooses to play the action that maximizes the value. In this case:(cid:3)(cid:3)UU(cid:3)b(0), P , 0.9(cid:4)∗it, P , 0.1(cid:4)(cid:4); s0= arg maxai ∈Ai(cid:3)(cid:3)ai; UU(cid:3)b(0), P , 0.9(cid:4)Φi(cid:4)(cid:4), P , 0.1, s0= SBased on his integrated beliefs of what the opponent is going to do, the ToM2 agent’s choice is to play S.4.5. Higher orders of theory of mind agentsFor every order of theory of mind available to the agent beyond the second-order, say order k, the agent maintains anadditional belief structure b(k). These beliefs are used to expand his decision process by modeling the decision process ofa (k − 1)st-order theory of mind agent from his opponent’s point of view. The resulting prediction is weighted against thedecision process of (k − 1)st-order of theory of mind from his own point of view. For example, a ToM3 agent expands thedecision process of a ToM2 agent, represented by Eq. (10). He does so by modeling the decision process of a ToM2 agentfrom his opponent’s point of view. That is, the ToM3 agent calculates his prediction of opponent behaviour ˆabased onthird-order theory of mind:(3)jˆa(3)j= t∗j(cid:3)(cid:8)UU(cid:3)b(1), t∗i(cid:3)b(2); s(cid:4), 0.8(cid:4)(cid:3)(cid:8)b(2), tU∗j, t∗i(cid:3)b(3); s(cid:4), 0.8(cid:9); s(cid:4)(cid:9)(cid:4).; s, 0.8(11)Once the ToM3 agent has determined his prediction based on third-order theory of mind, he weights this prediction againstthe decision process of a ToM2 agent, represented by Eq. (10). The extent to which the ToM3 agent’s prediction ˆaof aToM2 opponent’s behaviour is reflected in his own behaviour is determined by his confidence 0 (cid:3) c3 (cid:3) 1 that third-ordertheory of mind yields accurate predictions of his opponent’s behaviour. That is, the choice of a ToM3 agent is given by(cid:4)(cid:9)(3)j(cid:3)(cid:4)(cid:3)(cid:4)(cid:9)(cid:8)(cid:8)∗tUUU(cid:8)b(0), t(cid:3)b(1), tU(cid:3)b(2), c1∗, t(cid:10), 0.8, c2,(cid:13)(cid:4)(cid:3)∗b(1)(cid:10) (cid:11)(cid:12) (cid:13)ˆs(1)(cid:3)b(2); s∗i∗(cid:11)(cid:12)ˆs(2)(cid:8)b(2), t(cid:4)(cid:4), 0.8, t∗i(cid:3)U(cid:11)(cid:12)ˆs(3)(cid:3)(cid:8)UU∗tj(cid:10)(cid:3)b(1), t(cid:4)(cid:3)b(3); s∗j(cid:9)(cid:9)(cid:4), 0.8; s, 0.8(cid:9)(cid:4), c3.(cid:4); s(cid:13)4.6. Belief adjustment and learning speedIn the previous subsections, we discussed how agents of different orders of theory of mind decide what action to play,based on their current beliefs b(k) and confidence levels ck. By placing himself in the position of his opponent, and viewingthe game from her perspective, an agent makes predictions for the action his opponents is going to perform. Each order oftheory of mind available to the agent generates such a prediction. The agent can use the accuracy of these predictions togain information about the opponent’s abilities over repeated games, and adjust his beliefs and confidence levels accordingly.For example, a ToM2 agent may learn that his opponent is not playing as predicted by his second-order theory of mind,but that his first-order theory of mind consistently makes accurate predictions of her actions. In such a case, the ToM2 agentmay start to play as if he were a ToM1 opponent, and ignore predictions from his second-order theory of mind altogether.However, it is important to note that while the ToM2 agent may adjust his behaviour to take advantage of predictablebehaviour of his opponent, his opponent is trying to do the same. In this section, we describe how agents update theirbeliefs b(k) and confidence levels ck when they observe the outcome of a game.When an agent plays against an unfamiliar opponent for the first time, his beliefs b(k) are initialized randomly, while hisconfidence levels ck are initialized at zero. After each round, the actual choice ˜ai of the agent and ˜a j of his opponent arerevealed. At this moment, an agent updates his confidence in theory of mind based on the accuracy of his predictions. AToM1 agent increases his confidence c1 in first-order theory of mind when his first-order prediction ˆacalculated throughEq. (5) was correct. In other cases, his confidence in first-order theory of mind decreases. This process is represented by theupdate(1)j(cid:7)c1 :=(1 − λ) · c1λ + (1 − λ) · c1if ˜a j (cid:8)= ˆaif ˜a j = ˆa(1)j(1)j,,(12)where 0 (cid:3) λ (cid:3) 1 is an agent-specific learning speed. An agent’s learning speed indicates the relative weight of new informa-tion in determining beliefs. An agent with a high learning speed determines whether his opponent is a ToM0 agent basedon his most recent observations. The ToM1 agent’s confidence c1 in first-order theory of mind reflects the accuracy of first-order theory of mind in the most recent games in this case. An agent with a low learning speed depends on experiencebuilt up over a longer period of time.For higher orders of theory of mind, an agent additionally adjusts each of his confidences ck in kth-order theory of mindfor each order k of theory of mind available to him. Similar to the update of his confidence c1 in first-order theory ofmind, an agent reduces his confidence ck in kth-order theory of mind when the corresponding prediction ˆaof opponent(k)jH. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9281(cid:8)= ˜a j . However, an agent only increasesbehaviour based on application of kth-order theory of mind was incorrect, that is ˆahis confidence in kth-order theory of mind when it yields correct predictions, and the predictions made by each order of= ˜a j , thetheory of mind lower than k were incorrect. If there is some lower order n < k of theory of mind for which ˆaagent does not increase his confidence in kth-order theory of mind. That is, theory of mind agents only grow more confidentin the use of higher-order theory of mind when this results in accurate predictions that could not have been made witha lower order of theory of mind. This feature makes agents less likely to overestimate the theory of mind abilities of theiropponent.(n)j(k)j(1 − λ) · ck⎧⎪⎨⎪⎩ck :=ckλ + (1 − λ) · ck otherwise.(k)j ,if ˜a j (cid:8)= ˆaif there is a 1 (cid:3) k < i such that ˜a j = ˆa(n)j= ˆa(k)j ,(13)When the actual choice of the agent ˜ai and his opponent ˜a j are revealed, the agent also updates his beliefs b(k). Since thezero-order beliefs b(0) represent the agent’s beliefs concerning his opponent’s behaviour, these beliefs are updated usinghis opponent’s choice ˜a j . This is done by increasing the belief the opponent will perform action ˜a j in the same game states ∈ S, while decreasing the belief that she will perform any other action. Second-order beliefs b(2) specify what the agentbelieves his opponent to believe about what he believes that she is going to do. That is, an agent’s second-order beliefs b(2)describe beliefs concerning the actions of his opponent, and are therefore updated using her choice ˜a j as well. After thisupdate, the agent believes that his opponent believes that he believes more strongly that she will perform the action ˜a j inthe same game state s ∈ S. This is true for each of the even-numbered orders of theory of mind available to the agent. Thebelief structure b(k) of all even-numbered orders of theory of mind are updated using the opponents choice ˜a j .On the other hand, the odd-numbered orders of theory of mind describe the actions of the agent himself. These beliefsare therefore updated using the agent’s choice ˜ai . For example, after the belief adjustment, the agent believes that hisopponent believes more strongly that he will perform action ˜ai when the same game state s ∈ S is encountered again.Using the belief updating function U , the beliefs are adjusted using the agent’s learning speed λ, such that(cid:3)b(i), ˜a j, λb(k)(a j; s) := U(cid:3)(cid:4)b(i), ˜ai, λb(k)(ai; s) := U(cid:4)(a j; s)(ai; s)for k even and all a j ∈ A j,for k odd and all a j ∈ A j.and(14)(15)That is, the agent adjusts his beliefs based on the forecasting technique of exponential smoothing [69]. Note that theseadjustments only apply to the game state s in which the actions were taken.The agent’s learning speed λ determines how quickly the agent learns. That is, a higher value of λ shows that theagent changes his beliefs more radically based on new information. At the maximum of λ = 1, an agent effectively believesthat the last action his opponent performed determines future behaviour. At the other extreme of λ = 0, the agent doesnot learn, and does not change his beliefs when new information becomes available. This also means that an agent withlearning speed λ = 0 does not change his behaviour.The agents we describe do not actively try to model the learning speed λ of their opponent. Instead, an agent assumesthat his opponent updates her beliefs using the same learning speed as he does himself. That is, our computational agentsdo not consider the possibility that their opponent reacts differently to new information than they do themselves. Thismeans that in general, the beliefs that an agent attributes to his opponent are structurally different from her actual beliefs.An agent makes use of theory of mind by considering the position of his opponent from his own viewpoint. When anagent makes use of second-order theory of mind, he also considers what his opponent knows about his own viewpoint.Since the games we consider have symmetric information, this causes the agent’s second-order beliefs b(2) to resemble hiszero-order beliefs b(0) more closely with each update. That is, the agent eventually believes that a first-order theory of mindopponent knows what his zero-order beliefs are.Due to the restrictions on the learning speed λ, Eqs. (14) and (15) preserve the normalization and non-negativity ofbeliefs. Similarly, the confidences ci in the application of ith-order theory of mind remain limited to the range [0, 1].Example 4. Consider the ToM2 agent from Example 3, whose mental content is given in Table 7. Once both the agent andhis opponent have decided on an action to play, the actions are revealed to both players, and each receives the payoff basedon those actions. Once the outcome of the game is revealed, each agent updates his beliefs based on what is observed.Our calculations showed that the ToM2 agent we discussed in our example has played action ˜ai = S. We assume that hisopponent played ˜a j = P , and that the agent’s learning speed λ = 0.6.Table 8 lists the agent’s predictions, confidences in theory of mind and beliefs before and after the belief update. De-pending on the accuracy of the prediction of application of ith-order theory of mind, the confidence ci in that order oftheory of mind increases or decreases. In our example, first-order theory of mind accurately predicted that the opponentwould play P , since ˜a j = ˆa. As a result, the new confidence c1, as calculated by Eq. (12), becomes(1)jc1 := (1 − λ) · c1 + λ = (1 − 0.6) · 0.9 + 0.6 = 0.96.82H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92Table 8Beliefs and confidences in theory of mind before and after the belief update in the example of a ToM3 agent playing RPS.Order of theory of mind iBefore update00.50.30.21P0.90.40.50.12P0.10.30.30.4(i)ˆajcib(i)(R, s0)b(i)(P , s0)b(i)(S, s0)After update00.200.720.0810.960.200.160.6420.100.120.720.16(2)j= P = ˜a j , and thus that second-order theory of mind also correctly predicted that the opponentTable 8 shows that ˆawould play P . However, since first-order theory of mind is of a lower order than second-order theory of mind, and sincefirst-order theory of mind also correctly predicted the action of the opponent, the confidence c2 in second-order theory ofmind remains unchanged.The actions that were actually played by the agent and his opponent also change the agent’s beliefs. Each even-numberedorder of theory of mind refers to beliefs concerning the opponent’s actions. These beliefs are therefore updated to reflectthe action that the opponent has taken most recently. This is done by increasing the belief that the opponent will performthe same action, in our case P , while decreasing the other beliefs. That is, the agent’s zero-order beliefs b(0) are updated,such that after the updateb(0)(R; s0) := Ub(0)(P ; s0) := Ub(0)(S; s0) := U(cid:4)(cid:3)b(0), P , 0.6(cid:3)b(0), P , 0.6(cid:4)(cid:3)b(0), P , 0.6(R; s0) = (1 − 0.6) · 0.5 = 0.2(cid:4)(P ; s0) = (1 − 0.6) · 0.3 + 0.6 = 0.72(S; s0) = (1 − 0.6) · 0.2 = 0.08This means that after the belief update, the agent believes that there is a 72% probability that his opponent is going torepeat the action P in the next round.The agent’s second-order beliefs b(2) also concern the actions of the opponent. Specifically, the agent’s second-orderbeliefs b(2) determine what the agent believes his opponent to believe what he believes about her actions. The agent usesthe action ˜a j = P actually performed by his opponent to update his second-order beliefs as well.The odd-numbered orders of theory of mind represent beliefs concerning the agent’s own actions. These beliefs aretherefore updated to reflect that the agent chose action ˜ai = S. For the agent’s first-order beliefs b(1), this results inb(1)(R; s0) := Ub(1)(P ; s0) := Ub(1)(S; s0) := U(cid:4)(cid:3)b(1), S, 0.6(cid:3)b(1), S, 0.6(cid:4)(cid:3)b(1), S, 0.6(R; s0) = (1 − 0.6) · 0.5 = 0.20(cid:4)(P ; s0) = (1 − 0.6) · 0.4 = 0.16(S; s0) = (1 − 0.6) · 0.1 + 0.6 = 0.64This means that after the belief update, the agent believes that his opponent believes that there is a 64% probability thathe will repeat the action S in the next round.5. ResultsThe agent model described in Section 4 has been implemented in Java and its performance has been tested in each ofthe settings described in Section 2. For the rock–paper–scissors game, as well as the variations on this game, each trialconsisted of an agent that plays 20 consecutive games against the same opponent.6 An agent’s trial score is the average ofthe agent’s game scores over all games in the trial. The graphs in this section depict the average trial score, averaged over500 trials. Since Limited Bidding is a more complex game, a longer sequence is needed to learn to model the opponent.Each trial in this game consisted of an agent that plays 50 consecutive games against the same opponent. Our results werequalitatively similar if longer trials of 100 games were used instead.In this section, performance is measured as the average trial score of the focal agent, as a function of his learningspeed λi , as well as the learning speed λ j of his opponent. The figures in this section show simulation results for every 0.02step in learning speeds over the range λi, λ j ∈ [0, 1]. We report the results of simulations in which a focal agent is exactlyone order of theory of mind higher than his opponent. In simulations in which the difference in theory of mind ability ofthe focal agent and his opponent was larger than one order, performance of the focal agent turned out to be similar.6 We have compared the results for trials of 20 games to longer trials of 50 and 100 games and found no qualitative differences.H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9283Fig. 6. Average performance of theory of mind agents playing rock–paper–scissors against opponents of a lower order of theory of mind. Performance wasaveraged over 500 trials of 20 consecutive games each. Insignificant results (p > 0.01) are highlighted in red. (For interpretation of the references to colorin this figure legend, the reader is referred to the web version of this article.)5.1. Rock–paper–scissorsFig. 6 shows how the ability to represent mental content of others affects the performance of agents in the RPS game asa function of the learning speed λi of the focal agent and of the learning speed λ j of his opponent. Higher and lighter areasindicate that the focal agent won more games than he lost, while lower and darker areas show that his opponent had theupper hand. To emphasize the shape of the surface, the grid that appears on the bottom plane has been projected onto thesurface, and the plane of zero performance appears as a semi-transparent surface in the figure. Red areas indicate whereperformance was not significantly different from zero, at a significance level α = 0.01.Fig. 6a shows that a ToM1 agent that has a learning speed λi = 0 cannot compete with his opponent. When the agentdoes not learn from his opponent’s behaviour, he loses nearly all rounds. Similarly, his opponent loses nearly all roundswhen she does not learn at all (λ j = 0). This shows that both zero-order theory of mind agents and first-order theory ofmind agents can successfully model an opponent that always performs the same action.84H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92The figure also shows that a ToM1 agent mostly outperforms a ToM0 opponent. Whenever the ToM1 agent’s learningspeed is at least λi > 0.1, he will on average win more rounds than he loses, and obtain a positive score. The ToM1 agent’sscore is particularly high when both he and his opponent learn at a high rate, in which case the agent wins almost allrounds. When his ToM0 opponent learns at a low rate, the average score of the ToM1 agent is reduced.The relatively low performance of the ToM1 agent against slow learning opponents is due to the fact that learning speeddetermines an agent’s memory. A ToM0 agent with high learning speed adapts to new situations quickly, but also quicklyforgets information from previous rounds. When faced with an unpredictable opponent, a ToM0 agent with high learningspeed will therefore choose erratically, but with confidence. In this case, the ToM0 agent believes that his opponent willrepeat the same action she has performed the last time they met. For a ToM1 opponent, this represents a predictablesituation that she can use to her advantage.Conversely, a ToM0 agent with low learning speed retains his former beliefs for a longer time. When encounteringan unpredictable opponent, such a ToM0 agent will therefore start playing with little confidence. That is, the probabilitydistribution modeled by b(0) may gradually come to resemble a uniform distribution. This causes a ToM0 agent with lowlearning speed to play the action that he weakly believes to be a slightly better choice than the rest. This also makes it moredifficult for a ToM1 opponent to predict which token the ToM0 agent with low learning speed will play, since his choice isnot robust against small deviations in his beliefs.Although a ToM1 agent performs better against an opponent that learns quickly than against an opponent that learnsslowly, performance of the ToM1 agent is largely independent of the quality of his model. When a ToM1 agent makesuse of his theory of mind, he assumes that his opponent reacts to new information the same way he does. That is, theagent assumes that he and his opponent share the same learning speed. However, the figure does not show an increase inperformance along the line of equal learning speeds. That is, the cost of assuming equal learning speeds is low in RPS.Fig. 6b shows the performance of a ToM2 agent playing RPS against a ToM1 opponent. Note that Fig. 6b is similar toFig. 6a. As for the ToM1 agent, a ToM2 agent performs best when playing RPS against a ToM1 opponent when both he andhis opponent learn at a high speed, while the ToM2 agent has more difficulty modeling a ToM1 agent that learns slowly.This shows that application of higher-order theory of mind can benefit an agent when playing RPS. Performance of the ToM2agent playing RPS against a ToM1 opponent is nonetheless slightly lower than that of a ToM1 agent playing RPS against aToM0 opponent.Figs. 6a and 6b suggest that application of higher orders of theory of mind benefits an agent. However, performance ofa ToM3 agent playing RPS against a ToM2 agent, as shown in Fig. 6c, is poor in comparison. Although the ToM3 agent stilloutperforms a ToM2 opponent, he does so at a lower margin. The average score of the ToM3 agent only exceeds 0.5 whenhis opponent has learning speed zero. When facing a ToM2 opponent that has a low learning speed, the average score of aToM3 agent that learns quickly even becomes negative.Although the ToM3 agent can still outperform a ToM2 opponent at a small margin, Fig. 6d shows that a ToM4 agent nolonger outperforms a ToM3 opponent in RPS. In this scenario, the outcome of the game is mostly dependent on which ofthe agents has the highest learning speed, and no longer on theory of mind abilities. In Fig. 6d, this can be seen by the factthat the ToM4 agent obtains a positive outcome on average only if his learning speed λi is higher than the learning speedλ j of his opponent.In summary, the ability to make use of theory of mind can benefit an agent in the game of RPS. As we hypothesized (cf.hypothesis HRPS, Section 2.4), both the ToM1 agent and the ToM2 agent outperform opponents of a lower order of theoryof mind. The performance of the ToM3 agent and the ToM4 agent suggests that there may be a limit to the effectiveness ofapplication of higher orders of theory of mind. However, since rock–paper–scissors involves three possible opponent actions,the game leaves room for only three unique predictions of the opponent’s next action. The low performance of the ToM3and ToM4 agents may therefore be caused by specific characteristics of the RPS game, rather than a limit to the effectivenessof application of higher orders of theory of mind. The next section describes a game with more than three actions in orderto differentiate between these alternative explanations.5.2. Elemental rock–paper–scissorsIn Section 2.1.2, we introduced elemental rock–paper–scissors, a variation on the classical RPS game in which agentschoose from an action set of five actions. ERPS preserves the feature of RPS that each action is defeated by exactly oneother action. Differences in performance of theory of mind agents that play RPS and those that play ERPS allow us todetermine whether features of the game structure affect the effectiveness of higher orders of theory of mind in competitivegames.The results for ERPS are shown in Fig. 7. Our expectation that performance of theory of mind agents in playing ERPSwould be at least as good as performance in RPS is only partially correct. Similar to our results of theory of mind agentsplaying RPS, Fig. 7a shows that a ToM1 agent outperforms a ToM0 opponent, while Fig. 7b shows that a ToM2 agent outper-forms a ToM1 opponent as well. However, performance in the game of ERPS is slightly reduced compared to the situationin which they were playing RPS. Especially when either the agent or his opponent learns at a low speed, it is more difficultfor a theory of mind agent to model his opponent in a game of ERPS than it is in RPS.The main qualitative difference between RPS and ERSP is shown by the performance of the ToM3 agent and performanceof the ToM4 agent, depicted in Fig. 7c. Our results in RPS showed that it is difficult for a ToM3 agent to model his oppo-H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9285Fig. 7. Average performance of theory of mind agents playing ERPS against opponents of a lower order of theory of mind. Performance was averaged over500 trials of 20 consecutive games each. Insignificant results (p > 0.01) are highlighted in red.nent correctly. In Fig. 6c, this presents itself as relatively low performance against an opponent with zero learning speedλ = 0. In contrast, Fig. 7c shows that the ToM3 agent does not have this difficulty when playing ERPS against a similaropponent.Since the richer action space of ERPS increases performance of the ToM3 agent when playing against an opponent thatdoes not learn, the structure of the game influences the effectiveness of theory of mind. However, performance of a ToM3agent playing ERPS against a ToM2 opponent is still poor in comparison to performance of the ToM1 and ToM2 agentsplaying ERPS against opponents of a lower order of theory of mind. Although the ToM1 and ToM2 agents clearly out-perform opponents of a lower order of theory of mind, the ToM3 agent outperforms the ToM2 agent at a small marginonly.Fig. 7d shows the performance of a ToM4 agent playing ERPS against a ToM3 opponent. Like the ToM3 agent, the peakperformance of the ToM4 agent when playing against an opponent with learning speed λ j = 0 shown in the figure indicatesthat the ToM4 agent has no difficulty distinguishing agents that have learning speed zero from agents of a lower order oftheory of mind. However, the ability to make use of fourth-order theory of mind does not present an agent with advantagesin ERPS beyond those of third-order theory of mind. Fig. 7d shows that a ToM4 agent that plays ERPS against a ToM386H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92opponent only obtains a positive score on average if his learning speed λiis higher than the learning speed λ j of hisopponent. That is, when a ToM4 agent plays ERPS against a ToM3 opponent, whoever has the highest learning speed isexpected to win.In summary, we investigate the game of ERPS to determine whether the limited choice of actions for agents playing RPShad an effect on the advantage of making use of theory of mind. The results confirm our expectations (cf. hypothesis H ERPS,Section 2.4) that when agents choose from a limited action space, higher orders of theory of mind may experience difficultymodeling their opponent. However, the limited action space does not explain the relatively poor performance of a ToM3agent when playing against a ToM2 opponent, which was found both in RPS and ERPS.5.3. Rock–paper–scissors–lizard–SpockThe game of rock–paper–scissors–lizard–Spock, described in Section 2.1.3, is a variation on ERPS in which each action isdefeated by exactly two other actions. As a result, the best response to each action is not unique. Our expectation was thatit would be harder to predict an opponent’s behaviour in this case, and that performance of theory of mind agents wouldbe reduced. Fig. 8 shows that this is indeed generally the case. In the game of RPSLS, the advantage of making use of theoryof mind is reduced compared to RPS and ERPS.Fig. 8a shows the performance of a ToM1 agent when playing RPSLS against a ToM0 opponent. Unlike in RPS and ERPS,a ToM1 agent performs better when his learning speed matches the learning speed of his opponent. In Fig. 8, this is reflectedby high scores along the line of equal learning speeds λi = λ j . In this case, the ToM1 agent’s model of his opponent’s beliefsmatches her actual beliefs. However, even though modeling his opponent’s beliefs correctly yields the agent a higher score,he is still expected to win in most cases when his learning speed does not match that of his opponent.Performance of the ToM1 agent is particularly low when his opponent has the maximum learning speed λ j = 1. In thiscase, she only considers the agent’s actions in the previous game, and ignores all information from previous games. Forexample, if the ToM1 agent plays ‘paper’ in a game of RPSLS, the ToM0 opponent will believe that he will repeat the sameaction in future games. This means that the ToM0 opponent has two actions,‘lizard’ or ‘scissors’, which maximize herexpected payoff, and chooses either one of these two actions with 50% probability.On the other hand, when the ToM0 opponent learns at a lower speed, λ j < 1, she does not completely replace her beliefswhen new information becomes available. In this case, the ToM0 opponent believes that there is a small probability thatthe ToM1 agent will play some action other than ‘paper’. In general, this prevents two actions from having exactly thesame expected payoffs. Since agents choose the action that yields them the highest expected payoff, this causes the ToM0opponent to choose one of the possible actions with certainty.As Fig. 8a shows, these two distinct types of behaviour make it more difficult for the ToM1 agent to accurately modelhis opponent. In the present model, a ToM1 agent that has a learning speed λi < 1 believes that his opponent has thesame learning speed. As a result, he believes that there is a single action that maximizes the opponent’s expected payoff.However, when his opponent has the maximal learning speed λ j = 1, she actually randomizes her choice over two possibleactions. The ToM1 agent is therefore expected to predict his opponent’s behaviour incorrectly in half the cases.Fig. 8b shows the performance of a ToM2 agent when playing RPSLS against a ToM1 opponent. Similar to the ToM1 agent,performance of the ToM2 agent is low when playing RPSLS against an opponent that learns at maximum speed, λo = 1. TheToM2 agent also has particular difficulties modeling a ToM1 opponent in RPSLS when his own learning speed λi is low. Inthis case, the ToM2 agent is outperformed by an opponent of lower order of theory of mind. However, the ToM2 agent willon average win when his learning speed λi is over 0.7.The low performance of the ToM2 agent in RPSLS when he learns at a low speed translates to a benefit for the ToM3agent. Fig. 8c shows the performance of the ToM3 agent when playing RPSLS against a ToM2 opponent. When his opponent’slearning speed λo is low, the ToM3 agent performs better in RPSLS than he would have in the games of RPS and ERPS.However, the ToM3 agent performs poorly when his ToM2 opponent learns quickly enough. In particular, when facing aToM2 opponent that learns at the maximal learning speed λ j = 1, the ToM3 agent only obtains a positive score on averagewhen he learns at the maximal learning speed λi = 1 as well.Similar to the games of RPS and ERPS, performance of a ToM4 agent playing RPSLS against a ToM3 opponent is mostlydetermined by which player has the highest learning speed, as shown in Fig. 8d. However, unlike in RPS and ERPS, the ToM4agent is at a very small advantage over his ToM3 opponent. That is, when the learning speed λi of the ToM4 agent andthe learning speed λ j of his ToM3 opponent are close together, the ToM4 agent is expected to win more than predicted bychance performance.In summary, our results from the game of RPSLS show that the effectiveness of theory of mind is strongly related to thepredictability of lower-order agents. Theory of mind agents perform more poorly when their opponent is indifferent betweentwo possible actions and her behaviour is less predictable. This confirms our expectations about the relationship betweenthe performance of theory of mind agents and the predictability of their opponents (cf. hypothesis H RPSLS, Section 2.4).5.4. Limited BiddingUnlike the variations on rock–paper–scissors, Limited Bidding is an extensive form game that spans several rounds. Al-though there is a unique best-response to each opponent action, there are multiple responses that yield a positive outcome.H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9287Fig. 8. Average performance of theory of mind agents playing RPSLS against opponents of a lower order of theory of mind. Performance was averaged over500 trials of 20 consecutive games each. Insignificant results (p > 0.01) are highlighted in red. (For interpretation of the references to color in this figurelegend, the reader is referred to the web version of this article.)To determine the advantage of having the ability to explicitly represent mental states of others in the game of LB, agentsthat differ in their order of theory of mind have been placed in competition. Fig. 9 shows the performance of theory of mindagents as a function of the learning speed λi of the focal agent and the learning speed λ j of his opponent. Performance hasbeen normalized to range from 1, which means that the focal agent achieved the maximum possible payoff, to −1, in whichcase his opponent achieved the maximum possible payoff. As before, lighter areas highlight that the agent performed betterthan his opponent, while darker areas show that his opponent obtained a higher average score.Fig. 9a shows that ToM1 agents predominantly obtain a positive score when playing against ToM0 opponents. A ToM1agent performs well when facing an opponent that does not learn, as shown by the high scores when the opponent’slearning speed is zero (λ j = 0). The bright area along the line of equal learning speeds indicates that the advantage of theToM1 agent is also particularly high when learning speeds are equal. In this case, the ToM1 agent’s implicit assumptionthat his opponent has the same learning speed as himself is correct. Fig. 9a shows that even when the ToM1 agent fails toaccurately model his opponent, he will on average obtain a positive score for any learning speed λi > 0.08.88H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92Fig. 9. Average performance of theory of mind agents playing Limited Bidding against opponents of a lower order of theory of mind. Performance wasaveraged over 50 trials of 50 consecutive games each. Insignificant results (p > 0.01) are highlighted in red. (For interpretation of the references to color inthis figure legend, the reader is referred to the web version of this article.)As for the cases of RPS and ERPS described above, applying theory of mind appears to be least effective when a ToM1agent is playing against a ToM0 opponent that has a low learning speed. In LB, a ToM0 opponent with high learning speedchanges her beliefs radically, but with high confidence. That is, the effect of the random initialization of beliefs has lessimpact on opponent behaviour when her learning speed is high than when her learning speed is low. For a ToM1 agent,a ToM0 opponent with a high learning speed represents a more predictable situation, which he can use to his advantage.Fig. 9b shows that a ToM2 agent is at an advantage over a ToM1 opponent. However, although Fig. 9b shows many of thesame features as Fig. 9a, such as the brighter area along the main diagonal of equal learning speeds, ToM2 agents playingagainst ToM1 opponents obtain a score that is on average 0.13 lower than the score of ToM1 agents playing against ToM0agents. As a result, a ToM2 agent needs a higher learning speed of at least λi > 0.12 in order to obtain, on average, apositive score when playing against a ToM1 agent. Note that like a ToM1 agent, a ToM2 agent has more difficulty obtainingan advantage when playing against an opponent with low learning speed than when her learning speed is high.Similar to the results found for the variations on RPS, the application of first-order and second-order theory of mindpresent an agent with a clear advantage over opponents of a lower order of theory of mind. However, the advantage of aH. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9289ToM3 agent over a ToM2 opponent is only marginal. Fig. 9c shows that a ToM3 agent barely outperforms a ToM2 agent, withan average score that only exceeds 0.1 when the ToM2 opponent has zero learning speed. Moreover, although it appears asif a ToM3 agent can still on average obtain a positive score when his learning speed is at least λi > 0.32, Fig. 9c shows thatwhen the ToM2 opponent has learning speed 0 < λ j < 0.1, performance of the ToM3 agent may still fall below the planeof zero performance. That is, a ToM3 agent is no longer guaranteed to win when playing against a ToM2 opponent for anyvalue of his learning speed λ j .Fig. 9d shows that a ToM4 agent fails to obtain an advantage of any kind over a ToM3 agent when playing LB. Whenneither the agent nor his opponent learns at a low speed, the game will, on average, end in a tie. The learning speed of theagent and the learning speed of his opponent do not have a strong effect on the expected outcome of the game.In summary, agent performance in LB clearly shows diminishing returns on higher orders of theory of mind. The useof first-order and second-order theory of mind allows agents to obtain a reliable advantage over opponents that are morelimited in their ability to explicitly represent mental states of others. However, a specialized system for third-order theoryof mind barely allows ToM3 agents to outperform ToM2 agents, while a fourth-order theory of mind does not yield an agentany advantage that could not have been obtained with a third-order theory of mind. Qualitatively, the results are similar tothose described for the RPS game in Section 5.1.5.5. Summary of resultsTo determine the effectiveness of theory of mind, we simulated computational theory of mind agents, as described inSection 4, playing competitive games against one another. In hypothesis H RPS (Section 2.4), we predicted that higher ordersof theory of mind would benefit agents in competitive settings. Our results support this conclusion in the sense that theability to make use of first-order and second-order theory of mind allows agents to obtain a clear advantage over opponentsof a lower order of theory of mind. However, for orders of theory of mind beyond the second, the additional advantage ismarginal.This pattern of results was consistent across the variations on rock–paper–scissors we investigated. As we predictedin hypothesis HERPS, the larger action space of elemental rock–paper–scissors was advantageous for higher-order theoryof mind agents in some instances. However, the larger action space did not remove the diminishing returns on higherorders of theory of mind. Qualitatively similar results were found for the multi-stage limited bidding game, which confirmshypothesis HLB.The relatively limited advantage of ToM3 agents playing against ToM2 opponents appears to be caused by the model thatthe ToM2 opponent holds of the ToM3 agent. Agents start out by playing as if they were ToM0 agents. When a ToM3 agentis in competition with a ToM2 opponent, both of them will notice that their predictions based on first-order theory of mindare correct. This causes both agents to grow more confident in application of first-order theory of mind. As a result, theyboth gradually start to play more as if they were ToM1 agents. When this happens, predictions based on first-order theoryof mind will become less accurate, but predictions based on second-order theory of mind become increasingly accurate,increasing confidence in the application of second-order theory of mind. Both the agent and his opponent will thereforestart playing as if they were ToM2 agents. At this point, the ToM2 opponent can no longer model the behaviour of the agent.That is, she will notice that none of her predictions are correct. Because of this, she will lose confidence in the applicationof both first-order and second-order theory of mind, and gradually start to play as if she were a ToM0 agent again. Whenthe ToM3 agent tries to take advantage of this by playing as if he were a ToM1 agent, the ToM2 opponent is once again ableto recognize this behaviour, and she will grow more confident in her predictions based on second-order theory of mindagain. This causes the ToM2 opponent to constantly keep changing her strategy, which hinders the ToM3 agent in his effortsof trying to model her behaviour.The relation between the performance of a theory of mind agent and the predictability of his opponent’s behaviour isalso reflected in the results of the rock–paper–scissors–lizard–Spock game. As predicted in hypothesis H RPSLS, higher-ordertheory of mind agents perform more poorly in this game than in RPS and ERPS.6. Discussion and conclusionThe Machiavellian intelligence hypothesis [38] on the evolution of theory of mind predicts that there are competitivesettings in which the use of higher-order theory of mind presents individuals with an evolutionary advantage. But thebenefits of making use of higher-order theory of mind may not always outweigh the costs. For example, in settings inwhich a pure-strategy Nash equilibrium exists, individuals that make use of theory of mind are unlikely to outperformindividuals that play the Nash strategy without explicitly reasoning about their opponent’s mental states. In other cases,simple heuristics may be superior to methods that rely on sophisticated cognitive abilities like theory of mind [70,71].However, humans possess the ability to make use of higher-order theory of mind, which suggests that there may be settingsin which this cognitively demanding skill is useful. For example, in using secret codes or negotiating climate change control,heuristics alone may not be enough.In this paper, we have used agent-based models to show how the ability to make use of theory of mind can presentindividuals with an advantage over opponents that lack such an ability in certain competitive settings. The advantage wasfound to be qualitatively similar across the four competitive games we discussed, which included repeated single-shot90H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92games rock–paper–scissors, elemental rock–paper–scissors, rock–paper–scissors–lizard–Spock, and the repeated extensiveform game limited bidding.To our surprise, the results show diminishing returns on higher orders of theory of mind. Although both first-order andsecond-order theory of mind agents clearly outperform opponents that are more limited in their abilities to represent mentalcontent of others, third-order theory of mind agents only marginally outperform second-order theory of mind opponents.Fourth-order theory of mind was only found to be beneficial under specific circumstances. These diminishing returns onhigher orders of theory of mind were found not to be related to the number of actions available to the agents. Increasing theaction space from which agents choose did not increase performance of a third-order theory of mind agent in competitionwith a second-order theory of mind opponent.Although theory of mind allows agents to outperform opponents that are more limited in their ability to explicitlyrepresent mental states, theory of mind may not always be an efficient use of memory capacity. Additional experimentsshow that in simple games such as rock–paper–scissors, an agent seems to benefit more from remembering past behaviourof his opponent rather than representing her mental states. However, for more complex games such as Limited Bidding,theory of mind appears to have benefits that go beyond remembering past opponent behaviour. Agents that are capableof both associative learning strategies and theory of mind strategies may therefore choose not to use their theory of mindwhen the task is simple. Tasks may need to be sufficiently complex to elicit a theory of mind response.In our model, we have assumed that agents choose what action to perform rationally. That is, agents choose to performthe action that they believe to yield them the highest possible payoff. This results in a predictability that benefits theoryof mind agents, as shown by our results in the game of rock–paper–scissors–lizard–Spock. When an opponent is indifferentbetween two actions in the sense that both actions maximize the expected payoff, the effectiveness of theory of mindsuffers. However, when there is a slight asymmetry between the two actions, such that one action appears to be a slightlybetter alternative than the other, this creates a focal point [72] for agents. In this case, the opponent will choose the actionthat she believes to yield the better payoff. However, this behaviour can be predicted by higher-order theory of mindagents.An agent of a lower order of theory of mind may therefore be able to avoid falling victim to an opponent capable oftheory of mind of a higher order when he does not choose what action to play completely rationally. For example, agentscould choose the action to perform with a probability proportional to the expected payoff. Similarly, utility proportionalbeliefs [73] may benefit the effectiveness of theory of mind agents, through the belief that opponents choose an action pro-portionally to its utility. In this case, the theory of mind agent is less reliant on his opponent playing completely rationally.Future research may reveal how a balance can be achieved between exploiting weaknesses in the opponent’s actions, whileremaining unpredictable enough to avoid exploitation.In our model, a zero-order theory of mind agent does not believe that his opponent behaves randomly [10,11], butattempts to model the opponent’s behaviour by assuming her past actions predict what she will do in the future. A higher-order theory of mind agent therefore simultaneously updates his model of the mental content of the opponent and hisbelief about the opponent’s theory of mind abilities. It would be interesting to compare the effectiveness of theory of mindin direct competition with more classical strategies and heuristics.In future work, we aim to investigate whether theory of mind is effective in more complex interaction settings includingvarious partners as well. Theory of mind may play an important role in cooperative settings, for example in teamwork, aswell as mixed-motive settings such as negotiations (cf. [37]). This may provide further insights for automated agents thatshare their environment with human agents, such as in automated negotiation [3,4].AcknowledgementsThis work was supported by the Netherlands Organisation for Scientific Research (NWO) Vici grant NWO 277-80-001,awarded to Rineke Verbrugge for the project ‘Cognitive systems in interaction: Logical and computational models of higher-order social cognition’. We would like to thank the three anonymous reviewers for their helpful comments.References[1] H. de Weerd, B. Verheij, The advantage of higher-order theory of mind in the game of limited bidding, in: J. van Eijck, R. Verbrugge (Eds.), Proc.Workshop Reason. Other Minds: Log. Cogn. Perspect., CEUR Workshop Proceedings, 2011, pp. 149–164.[2] H. de Weerd, R. Verbrugge, B. Verheij, Higher-order social cognition in the game of rock–paper–scissors: A simulation study, in: G. Bonanno, H. vanDitmarsch, W. van der Hoek (Eds.), Proc. 10th Conf. Log. Found. Game Decis. Theory, 2012, pp. 218–232.[3] S. Kraus, Negotiation and cooperation in multi-agent environments, Artif. Intell. 94 (1997) 79–97.[4] R. Lin, S. Kraus, J. Wilkenfeld, J. Barry, Negotiating with bounded rational agents in environments with incomplete information using an automatedagent, Artif. Intell. 172 (2008) 823–851.[5] R. Fagin, J. Halpern, Y. Moses, M. Vardi, Reasoning About Knowledge, MIT Press, Cambridge, MA, 1995; second edition 2003.[6] H. van Ditmarsch, W. van der Hoek, B. Kooi, Dynamic Epistemic Logic, Springer, 2007.[7] P. Gmytrasiewicz, E. Durfee, A rigorous, operational formalization of recursive modeling,in: Proc. First Int. Conf. on Multi-Agent Syst., 1995,pp. 125–132.[8] P. Gmytrasiewicz, P. Doshi, A framework for sequential planning in multiagent settings, J. Artif. Intell. Res. 24 (2005) 49–79.[9] A. Pfeffer, Networks of influence diagrams: A formalism for representing agents’ beliefs and decision-making processes, J. Artif. Intell. Res. 33 (2008)109–147.[10] W. Yoshida, R. Dolan, K. Friston, Game theory of mind, PLoS Comput. Biol. 4 (2008) e1000254.H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–9291[11] C. Camerer, T. Ho, J. Chong, A cognitive hierarchy model of games, Q. J. Econ. 119 (2004) 861–898.[12] D. Stahl, P. Wilson, On players’ models of other players: Theory and experimental evidence, Games Econ. Behav. 10 (1995) 218–254.[13] M. Bacharach, D.O. Stahl, Variable-frame level-n theory, Games Econ. Behav. 32 (2000) 220–246.[14] H. Simon, A mechanism for social selection and successful altruism, Science 250 (1990) 1665–1668.[15] D. Kahneman, Maps of bounded rationality: Psychology for behavioral economics, Am. Econ. Rev. (2003) 1449–1475.[16] D. Premack, G. Woodruff, Does the chimpanzee have a theory of mind? Behav. Brain Sci. 1 (1978) 515–526.[17] J. Perner, H. Wimmer, “John thinks that Mary thinks that. . .”. Attribution of second-order beliefs by 5 to 10 year old children, J. Exp. Child Psychol. 39(1985) 437–471.[18] T. Hedden, J. Zhang, What do you think I think you think?: Strategic reasoning in matrix games, Cognition 85 (2002) 1–36.[19] L. Flobbe, R. Verbrugge, P. Hendriks, I. Krämer, Children’s application of theory of mind in reasoning and language, J. Log. Lang. Inf. 17 (2008) 417–442.[20] B. Meijering, H. van Rijn, N. Taatgen, R. Verbrugge, I do know what you think I think: Second-order theory of mind in strategic games is not thatdifficult, in: Proc. 33rd Annu. Conf. Cogn. Sci. Soc., 2011, pp. 2486–2491.[21] V. Crawford, N. Iriberri, Fatal attraction: Salience, naïvete, and sophistication in experimental “Hide-and-Seek” games, Am. Econ. Rev. (2007) 1731–1750.[22] H. Wimmer, J. Perner, Beliefs about beliefs: Representation and constraining function of wrong beliefs in young children’s understanding of deception,Cognition 13 (1983) 103–128.[23] I. Apperly, Mindreaders: The Cognitive Basis of “Theory of Mind”, Psychology Press, Hove, UK, 2011.[24] B. Meijering, L. Van Maanen, H. Van Rijn, R. Verbrugge, The facilitative effect of context on second-order social reasoning, in: Proc. 32nd Annu. Conf.Cogn. Sci. Soc., 2010, pp. 1423–1429.[25] M. Tomasello, Why We Cooperate, MIT Press, Cambridge, MA, 2009.[26] M. Schmelz, J. Call, M. Tomasello, Chimpanzees know that others make inferences, Proc. Natl. Acad. Sci. USA 108 (2011) 3077–3079.[27] J. Burkart, A. Heschl, Understanding visual access in common marmosets, Callithrix jacchus: Perspective taking or behaviour reading? Anim. Behav. 73(2007) 457–469.[28] J. Kaminski, J. Call, M. Tomasello, Goats’ behaviour in a competitive food paradigm: Evidence for perspective taking? Behaviour 143 (2006) 1341–1356.[29] J. Kaminski, J. Brauer, J. Call, M. Tomasello, Domestic dogs are sensitive to a human’s perspective, Behaviour 146 (2009) 979–998.[30] N. Clayton, J. Dally, N. Emery, Social cognition by food-caching corvids. The western scrub–jay as a natural psychologist, Philos. Trans. R. Soc. B, Biol.Sci. 362 (2007) 507.[31] T. Bugnyar, Knower-guesser differentiation in ravens: Others’ viewpoints matter, Proc. R. Soc. B, Biol. Sci. 278 (2011) 634–640.[32] D. Penn, D. Povinelli, On the lack of evidence that non-human animals possess anything remotely resembling a ‘theory of mind’, Philos. Trans. R. Soc.B, Biol. Sci. 362 (2007) 731.[33] P. Carruthers, Meta-cognition in animals: A skeptical look, Mind Lang. 23 (2008) 58–89.[34] E. van der Vaart, R. Verbrugge, C. Hemelrijk, Corvid re-caching without ‘theory of mind’: A model, PLoS ONE 7 (2012) e32904.[35] M. Balter, ‘Killjoys’ challenge claims of clever animals, Science 335 (2012) 1036–1037.[36] B. Hare, J. Call, M. Tomasello, Do chimpanzees know what conspecifics know? Anim. Behav. 61 (2001) 139–151.[37] R. Verbrugge, Logic and social cognition: The facts matter, and so do computational models, J. Philos. Log. 38 (2009) 649–680.[38] A. Whiten, R. Byrne, Machiavellian Intelligence II: Extensions and Evaluations, Cambridge University Press, Cambridge, 1997.[39] J. Epstein, Generative Social Science: Studies in Agent-based Computational Modeling, Princeton University Press, Princeton, NJ, 2006.[40] J. Epstein, Agent-based computational models and generative social science, Complexity 4 (1999) 41–60.[41] W. Jager, R. Popping, H. Van de Sande, Clustering and fighting in two-party crowds: Simulating the approach-avoidance conflict, J. Artif. Soc. Soc.Simul. 4 (2001) 1–18.[42] M. Harbers, R. Verbrugge, C. Sierra, J. Debenham, The examination of an information-based approach to trust, in: Coord., Organ., Inst., and Norms inAgent Syst. III, 2008, pp. 71–82.[43] E. van der Vaart, B. de Boer, A. Hankel, B. Verheij, Agents adopting agriculture: Modeling the agricultural transition, in: Proc. 9th Int. Conf. from Anim.to Animats: Simul. Adapt. Behav., 2006, pp. 750–761.[44] H. Gintis, Strong reciprocity and human sociality, J. Theor. Biol. 206 (2000) 169–179.[45] R. Boyd, H. Gintis, S. Bowles, P. Richerson, The evolution of altruistic punishment, Proc. Natl. Acad. Sci. 100 (2003) 3531–3535.[46] H. de Weerd, R. Verbrugge, Evolution of altruistic punishment in heterogeneous populations, J. Theor. Biol. 290 (2011) 88–103.[47] A. Cangelosi, D. Parisi, Simulating the Evolution of Language, Springer, 2002.[48] B. de Boer, The Origins of Vowel Systems, Oxford University Press, USA, 2001.[49] I. Slingerland, M. Mulder, E. van der Vaart, R. Verbrugge, A multi-agent systems approach to gossip and the evolution of language, in: Proc. 31st Annu.Meet. Cogn. Sci. Soc., 2009, pp. 1609–1614.[50] D. Billings, The first international RoShamBo programming competition, ICGA J. 23 (2000) 42–50.[51] D. Egnor, Iocaine powder, ICGA J. 23 (2000) 33–35.[52] J. Von Neumann, Zur Theorie der Gesellschaftsspiele, Math. Ann. 100 (1928) 295–320.[53] K. Binmore, Playing for Real, Oxford University Press, Oxford, UK, 2007.[54] W. Wagenaar, Generation of random sequences by human subjects: A critical survey of literature, Psychol. Bull. 77 (1972) 65.[55] A. Rapoport, D. Budescu, Randomization in individual choice behavior, Psychol. Rev. 104 (1997) 603.[56] R. West, C. Lebiere, D. Bothell, Cognitive architectures, game playing, and human evolution, in: Cognition and Multi-Agent Interaction: From CognitiveModeling to Social Simulation, Cambridge University Press, 2006, pp. 103–123.[57] R. Cook, G. Bird, G. Lünser, S. Huck, C. Heyes, Automatic imitation in a strategic context: Players of rock–paper–scissors imitate opponents’ gestures,Proc. R. Soc. B, Biol. Sci. (2011).[58] S. Kass, K. Bryla, Rock paper scissors Spock lizard, http://www.samkass.com/theories/RPSSL.html, 2009, accessed 29/12/2012.[59] E. De Bono, Edward de Bono’s Super Mind Pack: Expand Your Thinking Powers with Strategic Games & Mental Exercises, Dorling Kindersley PublishersLtd., London, UK, 1998.[60] M. Osborne, A. Rubinstein, A Course in Game Theory, MIT Press, Cambridge, MA, 1994.[61] C. Bicchieri, Common knowledge and backward induction: A solution to the paradox, in: Proc. 2nd Conf. Theor. Asp. Reason. Knowl., 1988, pp. 381–393.[62] J. Von Neumann, O. Morgenstern, Theory of Games and Economic Behavior, Princeton University Press, Princeton, NJ, 1944, commemorative edition,2007.[63] M. Davies, The mental simulation debate, Philos. Issues 5 (1994) 189–218.[64] S. Nichols, S. Stich, Mindreading: An Integrated Account of Pretence, Self-Awareness, and Understanding Other Minds, Oxford University Press, USA,2003.[65] S. Hurley, The shared circuits model (SCM): How control, mirroring, and simulation can enable imitation, deliberation, and mindreading, Behav. BrainSci. 31 (2008) 1–22.[66] R. Falk, C. Konold, Making sense of randomness: Implicit encoding as a basis for judgment, Psychol. Rev. 104 (1997) 301.[67] J. Barwise, On the model theory of common knowledge, in: The Situation in Logic, CSLI Press, Stanford, CA, 1989, pp. 201–220.92H. de Weerd et al. / Artificial Intelligence 199–200 (2013) 67–92[68] H. van Ditmarsch, J. van Eijck, R. Verbrugge, Common knowledge and common belief, in: J. van Eijck, R. Verbrugge (Eds.), Discourses on Social Software,Amsterdam University Press, Amsterdam, 2009, pp. 99–122.[69] R. Brown, Smoothing, Forecasting and Prediction of Discrete Time Series, Prentice–Hall, Englewood Cliffs, NJ, 1963.[70] G. Gigerenzer, R. Hertwig, T. Pachur, Heuristics: The Foundations of Adaptive Behavior, Oxford University Press, 2011.[71] D. Kahneman, Thinking, Fast and Slow, Farrar, Straus and Giroux, New York, 2011.[72] R. Sugden, A theory of focal points, Econ. J. 105 (1995) 533–550.[73] C. Bach, A. Perea, Utility proportional beliefs, http://epicenter.name/Research.html, 2011, accessed 29/12/2012.