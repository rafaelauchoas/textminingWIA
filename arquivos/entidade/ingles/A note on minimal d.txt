Artificial Intelligence 174 (2010) 442–448Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA note on minimal d-separation trees for structural learningBinghui Liu a, Jianhua Guo a,∗, Bing-Yi Jing ba Key Laboratory for Applied Statistics of MOE and School of Mathematics and Statistics, Northeast Normal University, Changchun 130024, Jilin Province, Chinab Department of Mathematics, Hong Kong University of Science and Technology, Hong Konga r t i c l ei n f oa b s t r a c tStructural learning of a Bayesian network is often decomposed into problems related to itssubgraphs, although many approaches without decomposition were proposed. In 2006, Xie,Geng and Zhao proposed using a d-separation tree to improve the power of conditionalindependence tests and the efficiency of structural learning. In our research note, we studya minimal d-separation tree under a partial ordering, by which the maximal efficiency canbe obtained. Our results demonstrate that a minimal d-separation tree of a directed acyclicgraph (DAG) can be constructed by searching for the clique tree of a minimal triangulationof the moral graph for the DAG.© 2010 Elsevier B.V. All rights reserved.Article history:Received 2 April 2009Received in revised form 19 January 2010Accepted 23 January 2010Available online 2 February 2010Keywords:Bayesian networkClique treeMinimal d-separation treeMinimal triangulationSeparation treeStructural learning1. IntroductionBayesian networks (BNs), also known as directed acyclic graphical models, are useful probabilistic graphical modelsthat can be represented by DAGs. For a Bayesian network, two components are involved. First, the DAG is the qualitativecomponent which represents dependence and independence relationships: the absence of some directed edges representsthe existence of certain conditional independence relationships between variables, and the presence of edges may representthe existence of direct dependence relationships or causal relationships [10,17]. Second, the joint probability distribution isthe quantitative component that expresses the strength of association between variables. We have special interest in thestructure recovery of DAGs. Several methods for structural learning of DAGs were considered and there are mainly two basicapproaches of structural learning [23]: constraint-based approach and search-and-score approach. We will introduce somemore constraint-based algorithms. As mentioned in [23], in a constraint-based algorithm, search for d-separators of pairs ofvariables is a major problem for orientation of edges and for structure recovery of a DAG. Here a d-separator is a subsetof variables by which the variable pairs are d-separated. Verma and Pearl [21] presented the IC algorithm and searched fora d-separator S from all possible variable subsets such that two variables u and v are conditionally independent on S. Asthe search for d-separators is often time-consuming, many algorithms were proposed to speed up the search. For example,the PC algorithm, an iterative algorithm, searched only for the d-separators contained in the variables that are adjacentto u and v in each step [19], where two variables are said to be adjacent if there is an edge between the two variables.Decomposition approaches [7,22] are also useful approaches to speed up the search. Using these approaches, the originallearning question can be decomposed into some sub-questions with smaller dimensions.For many constraint-based algorithms, there are two steps to recover DAG structures [23]. First, learn the moral graphof the target DAG applying Markov blanket learning algorithms, where the Markov blanket for a variable u is defined to* Corresponding author.E-mail addresses: liubh024@yahoo.com.cn (B. Liu), jhguo@nenu.edu.cn (J. Guo), majing@ust.hk (B.-Y. Jing).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.01.002B. Liu et al. / Artificial Intelligence 174 (2010) 442–448443be the set of variables composed of u’s parents, its children, and its children’s other parents [16]. Second, perform furtherindependence tests for edge orientation based on the moral graph learned in the first step. To speed up the second step,Xie, Geng and Zhao [22] proposed algorithms adopting a decomposition approach, which were supported by a useful andimportant definition of d-separation tree. They introduced this definition to depict separations and conditional independen-cies between sets of random variables, and we will describe the definition again in detail in the following section. Given ad-separation tree T , the problem of searching for d-separators in a large network can be localized to smaller subnetworks,that is, d-separators are only searched for in each node of the tree separately (see Theorem 2 in [22]). Then the number ofthe further independence tests mentioned in the second step can certainly be reduced.Different d-separation trees often carry corresponding efficiencies. The efficiency can be defined from several differentangles for different purposes. For example, if the aim is to search for the tree where the largest node has the minimumsize, the efficiency may be defined inversely proportional to the number of vertices in the largest node of the tree. Ouraim is slightly different from that one and here the efficiency is defined inversely proportional to the maximum numberof possible tests, which will be described in detail in the following section. To describe an appropriate d-separation treewith high efficiency, we first introduce the definition of minimal d-separation trees for a DAG. Our paper focuses on thecharacterization and construction of a minimal d-separation tree. Xie, Geng and Zhao (2006) already proposed a sufficientcondition for d-separation trees. Based on their work, we show that any minimal d-separation tree of a DAG is equivalentto a minimal separation tree of the moral graph for the DAG, which is in turn equivalent to a clique tree of a minimaltriangulation of the moral graph. According to these equivalences, some useful algorithms of searching for minimal trian-gulations and clique trees are available for us to construct a minimal d-separation tree, which leads to a minimal numberof the further independence tests mentioned above required for orientation of edges. The definitions, such as moral graph,clique tree and triangulation [10,12,13], will be introduced in the next section in detail.In Section 2, we introduce the notation and definitions. As our paper is mainly inspired by Xie, Geng and Zhao, wemainly follow the terminology of [22]. Section 3 discusses the characterization and construction of a minimal d-separationtree of a DAG. In Section 4, we make a conclusion for our paper.2. Notation and definitionsWe follow the terminology from [13,22], unless noted otherwise.2.1. Undirected graphs and separation treesWe consider simple and connected graphs. Let G = (V , E) denote an undirected graph, where V is the vertex set andE is the set of undirected edges. An undirected edge between two vertices u and v is denoted by (u, v). A ⊆ V induces asubgraph G A = ( A, E A), where E A = E ∩ ( A × A). A subset of V is called complete if every pair of vertices in the subset isconnected by an edge. A complete subset that is maximal w.r.t. inclusion is called a clique. A path p between two verticesu and v is a sequence of vertices w 0 = u, w 1, . . . , wn = v with (w i−1, w i) ∈ E, for 1 (cid:2) i (cid:2) n (n (cid:3) 1) and w i (cid:6)= w j for1 (cid:2) i, j (cid:2) n. A cycle is a path with w 0 = wn. A chord of the cycle is a pair of vertices w i , w j such that (w i, w j) ∈ E butj (cid:6)= i − 1, i + 1. Two subsets A, B ⊆ V with A ∩ B = ∅ are said to be separated by C ⊆ V in G if all paths from A to Bpass through C , i.e., for each vertex u ∈ A and each vertex v ∈ B, all paths from u to v intersect C at some vertices. Thepartition ( A, B, S) of V is said to be a decomposition of the undirected graph G, if (1) S separates A and B in G, and (2) Sis complete in G. Then G can be decomposed into subgraphs G A∪S and G B∪S .Hh=1 Ch = V . T denotes a tree whose node set is C and edge set is E T . Inspired by thedefinition of d-separation tree, Definition 1 in [22], here we propose a similar definition of separation tree for undirectedgraphs.Let C = {C1, . . . , C H }, such that(cid:2)Definition 1. A tree T = (C, E T ) is said to be a separation tree of an undirected graph G = (V , E) if for each edge (Ci, C j) ∈C , k = 1, 2; C 1 and C 2 are node sets ofE T , V 1\(Ci ∩ C j) and V 2\(Ci ∩ C j) are separated by Ci ∩ C j in G, where V k =T 1 and T 2, which are obtained by removing edge (Ci, C j) from tree T .C∈Ck(cid:2)If for each edge (Ci, C j) ∈ E T , Ci ∩ C j is complete in G, we say that T is a decomposition tree of G.A tree T = (C, E T ) is reduced if each vertex set in C is not contained in another one, that is, red{C} = C, where ‘red’stands for the operation of deleting the smaller of any two sets C1, C2 with C1 ⊆ C2.2.2. Triangulated graphs and clique treesA triangulated graph, also known as a chordal graph, is an undirected graph that contains no cycles of length (cid:3) 4 withouta chord. For any undirected graph G = (V , E) edges can be added so that the resulting graph Gt = (V , E ∪ F ), called a(cid:9)) is nottriangulation of the given graph G, is triangulated. A triangulation Gt of G is said to be minimal if Gtriangulated for every proper subset F(cid:9) = (V , E ∪ Fof F .(cid:9)444B. Liu et al. / Artificial Intelligence 174 (2010) 442–448Fig. 1. An undirected graph G = (V , E) (left) and a minimal triangulation Gt (right) of G.Fig. 2. A DAG G.Example 1. We see that Gt in Fig. 1 is a triangulation of the undirected graph G, because it contains no cycles of length (cid:3) 4without a chord. And Gt is a minimal triangulation, as G 1 = (V , E ∪ {(1, 4)}) and G 2 = (V , E ∪ {(4, 5)}) are not triangulated.We continue to show a graph G 3 that is not triangulated. Let G 3 = (V , E ∪ {(3, 5), (3, 6)}) and it looks very much like atriangulation. However, we see that there exists a cycle (1, 2, 4, 6, 5, 1) without a chord, which is contrary to the definition.Let KG denote the collection of all cliques in an undirected graph G [5]. For any triangulated graph G there exists asubset of the set of trees on KG known as clique trees [5].Definition 2. Assume that G is a triangulated graph. A tree T on KG is said to be a clique tree of G if T satisfies theclique-intersection property: for every pair of distinct cliques K i , K j ∈ KG , the set K i ∩ K j is contained in every clique on thepath connecting K i and K j in the tree.Note that a clique tree of the triangulated graph G is also said to be a junction tree of cliques for G [6] and for a giventriangulated graph G if T is a clique tree of G, T is a decomposition tree of G (see Theorem 4.6 in [6]).There are many efficient algorithms for computing clique trees of a triangulated graph. The most commonly cited one isthe MCS algorithm [20] with an O (n + m) time bound, where n is the number of vertices and m is the number of edges ofthe given triangulated graph.2.3. Directed graphs and d-separation treesLet G = (V , E) denote a DAG, where V is the finite vertex set and E is the set of directed edges. A directed edge from avertex u to v is denoted as (cid:10)u, v(cid:11). If there is a directed edge from u to v, v is said to be a child of u and u is a parent of v.Denote the set of all parents of a vertex v as pa(v) and denote the set of all children of a vertex u as ch(u). We say thatu is an ancestor of v and v is a descendant of u if there is a path between u and v in G and all edges in this path pointin the direction of v. We denote the set of all ancestors of a vertex v as an(v) and the set of all descendants of a vertexu as de(u). For A ⊆ V , if ∀v ∈ A, an(v) ⊆ A, A is said to be an ancestral set. For B ⊆ V , we let An(B) denote the smallestancestral set containing B. The moral graph Gm = (V , Em) of a DAG G is defined to be an undirected graph whose edge setis constructed by marrying parents and dropping directions, that is, connecting vertices that have a common child and thenmaking all edges in the graph undirected [13].A path p is said to be d-separated by a set Z in a D AG G, if and only if (1) p contains a ‘chain’: u → v → w or a ‘fork’:u ← v → w such that the middle vertex v is in Z , or (2) p contains a ‘collider’: u → v ← w such that the middle vertex vis not in Z and no descendant of v is in Z [17]. Two distinct sets X and Y of vertices are said to be d-separated by a set Zin G if Z d-separates every path from any vertex in X to any vertex in Y [17]. We also say that Z d-separates X and Y in G.In fact, Z d-separates X , Y in G if and only if Z separates X and Y in the moral graph of G An( X∪Y ∪Z ), which is writtenas (G An( X∪Y ∪Z ))m [13].Example 2. We show an example of d-separation in Fig. 2. A = {2} and B = {6} are d-separated by C = {1}. In Fig. 2 thereare four paths from {2} to {6}: (2, 1, 5, 6), (2, 4, 3, 1, 5, 6), (2, 4, 7, 6) and (2, 1, 3, 4, 7, 6). We see that (2, 1, 5, 6) containsa fork 2 ← 1 → 5 and (2, 4, 3, 1, 5, 6) contains a fork 3 ← 1 → 5. (2, 4, 7, 6) and (2, 1, 3, 4, 7, 6) both contain a collider4 → 7 ← 6, where the middle vertex 7 is not in C and no descendant of 7 is in C .To depict d-separations and conditional independencies among multiple variable sets, a notion of the d-separation treewas first proposed in [22]. Replacing “an undirected graph” with “a DAG” and replacing “separated” with “d-separated” inDefinition 1, we obtain the definition of d-separation tree of a DAG. We will show examples of d-separation trees of Fig. 2in Fig. 3.B. Liu et al. / Artificial Intelligence 174 (2010) 442–448445Fig. 3. T 1 and T 2 are two d-separation trees of G in Fig. 2, but neither of them is a minimal d-separation tree; T 3 is a minimal d-separation tree of G.The following theorem, Theorem 1 in [22], gives a valuable and important property of the d-separation tree, which iscentral to the decomposing approach to structural learning.Theorem 1. (See Xie et al. [22].) Let T be a d-separation tree of a DAG G. Vertices u and v are d-separated by some S ⊆ V \{u, v} in Gif and only if (1) u and v are not contained together in any node of T , or (2) there exists a node C that contains both u and v such thata subset Sof C d-separates u and v.(cid:9)Theorem 1 implies that the problem of searching for d-separators and constructing the skeleton of a DAG is divided intosome smaller problems in nodes of T so that the efficiency can be vastly improved. By using different d-separation treeswe often obtain different efficiencies. To depict an appropriate d-separation tree with maximal efficiency, we present theminimal d-separation tree of a DAG., let C(cid:9) ∧ C = red{C ∩ C, for each C(cid:9) ⊆ C . We proceed to define a partial order ‘≺’ for two trees T = (C, E T ) and T(cid:9) ∈ C(cid:9)there(cid:9) = (C(cid:9), E T (cid:9) ),and C(cid:9) (cid:6)= C. For any tree T = (C, E T ), we use less(T ) to denote the following collection(cid:9) ∈ C(cid:9)}. Note that if C(cid:9) ∧ C = C(cid:9): C ∈ C, C(cid:9)For two collections of subsets C, C(cid:9)(cid:9) ≺ T if and only if C(cid:9) ∧ C = C(cid:9)must exist a C ∈ C such that Cwhere Tof trees:(cid:3)T(cid:9) =: TC(cid:9)(cid:5)(cid:4), E T (cid:9)(cid:9)is reduced,(cid:7)(cid:9) = V , T(cid:9) ≺ TC.(cid:6)C (cid:9)∈C(cid:9)Definition 3. A reduced d-separation tree T = (C, E T ) of a DAG G = (V , E) is said to be a minimal d-separation tree of G ifany tree T(cid:9) = (C(cid:9), E T (cid:9) ) ∈ less(T ) is not a d-separation tree of G.As mentioned above, given a d-separation tree T = ({C1, . . . , C H }, E T ), the efficiency of structuralimproved. To construct the global skeleton of a DAG we need to execute conditional(cid:8)|C j |−2 times at most, which is obviously smaller than C 2|V | · 2learning can beindependence tests only for|V |−2. Define a cost function f (·) on d-separationHj=1 C 2|C j | · 2trees in this way:f (T ) =H(cid:9)j=1|C j | · 2C 2|C j|−2.And define the efficiency function eff (·) as 1/ f (·). For two reduced d-separation trees T 1 and T 2 of a DAG G = (V , E), ifT 1 ≺ T 2, we have f (T 1) < f (T 2), which will be rigorously proved in Lemma 3 in Section 3. Therefore the d-separation treeT 1 is better than T 2 in terms of efficiency improvement.Computing a d-separation tree with the smallest cost seems to be the best choice for the following searching work.However, since the problem of computing a d-separation tree with the minimum cost is often with great computationalcomplexity, the related polynomially solvable problem of computing a d-separation tree with the minimal cost becomesmore interesting.Example 3. Consider the DAG G in Fig. 2. Three d-separation trees, T i = (Ci, E T i ), i = 1, 2, 3, are shown in Fig. 3, where(cid:10)(cid:10)(cid:10)C1 =C2 =C3 ={1, 2, 3, 4}, {1, 3, 4, 5}, {4, 5, 6}, {4, 6, 7}{1, 2, 3, 4, 5}, {4, 5, 6}, {4, 6, 7}{1, 2, 3, 4}, {1, 4, 5}, {4, 5, 6}, {4, 6, 7}(cid:11)(cid:11),.(cid:11),446B. Liu et al. / Artificial Intelligence 174 (2010) 442–448Taking T 3 for example, {2, 3}, {5, 6, 7} are d-separated by {1, 4}; {1, 2, 3}, {6, 7} are d-separated by {4, 5}; and {1, 2, 3, 5},{7} are d-separated by {4, 6}. C3 = C1 ∧ C3 = C2 ∧ C3, so T 3 ≺ T 1 and T 3 ≺ T 2. Then,f (T 3) < f (T 1) and f (T 3) < f (T 2). Infact, T 3 is a minimal d-separation tree of DAG G.Similarly, we can give the definition of a minimal separation tree of an undirected graph by replacing “d-separation” with“separation” and replacing “a DAG” with “an undirected graph” in Definition 3.Now that we have given the necessary background and definitions for minimal d-separation trees, we move on to themore important topic: constructing minimal d-separation trees.3. Construction of minimal d-separation treesIn this section, we focus on the relationships among minimal d-separation trees, minimal separation trees and cliquetrees of minimal triangulations. Afterwards, we construct minimal d-separation trees taking advantage of algorithms forminimal triangulation, which were widely studied.Lemma 1. In a DAG G = (V , E), the following three statements are equivalent:(1) Tree T = (C, E T ) is a d-separation tree of G;(2) T is a separation tree of Gm;(3) T is a decomposition tree of some triangulation Gt of Gm.Proof. (3) ⇒ (1): Considering each (Ci, C j) ∈ E T , let K = Ci ∩ C j . Then V 1\K and V 2\K are separated by K in Gt . Since Gmhas fewer edges than Gt has, we know that V 1\K and V 2\K are separated by K in Gm. Therefore, T is a d-separation treeof G.(1) ⇒ (2): For each (Ci, C j) ∈ E T , V 1\K and V 2\K are d-separated by K in DAG G, so V 1\K and V 2\K are separated byK in (G An((V 1\K )∪(V 2\K )∪K ))m = Gm. Then T is also a separation tree of Gm.(cid:2)(2) ⇒ (3): Let Gt = (V ,C∈C E C ) where E C = {(a, b): a, b ∈ C and a (cid:6)= b}. Considering each (Ci, C j) ∈ E T and let K =Ci ∩ C j . For each x ∈ V 1\K and each y ∈ V 2\K , (x, y) is not in any C ∈ C. In fact, if there exists a subset C1 ∈ C, such that{x, y} ⊆ C1, we let C1 be a node of T 1 without loss of generality. Then, y ∈ C1\K ⊆ V 1\K , which is contrary to the fact thaty ∈ V 2\K and V 1\K , V 2\K are separated by K in Gm. By the construction of Gt , we know that (x, y) is not an edge of Gt .Then, V 1\K and V 2\K are separated by K in Gt and T is a decomposition tree of Gt . Gt can be decomposed step by stepinto complete subgraphs, therefore, Gt is a triangulated graph.For each (x, y) ∈ Em, there is a C ∈ C such that x, y ∈ C . In fact, if x, y are not in any set in C, then d(x, y) > 0 (seeDefinition A.1 in [22]). Let d(x, y) = d(C3, C4), where x ∈ C3 ∈ C and y ∈ C4 ∈ C. Let C5 be the first node on the path of Tfrom C3 to C4 and K = C3 ∩ C5. Then, C3\K and C4\K are separated by K in Gm. Since x and y are not contained in K ,we get the result that x and y are separated by K in Gm, which is contrary to the fact that (x, y) ∈ Em. Therefore Gt is atriangulation of Gm and T is a decomposition tree of Gt . (cid:2)Lemma 2. In a DAG G = (V , E), the following three statements are equivalent:(1) Tree T = (C, E T ) is a reduced d-separation tree of G;(2) T is a reduced separation tree of Gm;(3) T is a clique tree of some triangulation Gt of Gm.Proof. (3) ⇒ (1): This point was proved in Theorem 2 of [22].C(cid:2)(1) ⇒ (2): T is reduced, so T is also a reduced separation tree of Gm from Lemma 1.(2) ⇒ (3): Let Gt = (V ,of Gm and T is a decomposition tree of Gt , then C = Ksince C is complete in Gt . Furthermore, for each clique K ∈ Kcontained in one. We continue to decompose the subgraph step by step and K is finally contained in some C ∈ C.= {(a, b): a, b ∈ C and a (cid:6)= b}. By the proof of Lemma 1, Gt is a triangulationGt such that C ⊆ K ,Gt , if Gt can be decomposed into two subgraphs, K is certainlyGt . In fact, for each C ∈ C there exists K ∈ KC ) where EtC∈C EtNow we show that T is a clique tree of Gt . Let C and Cbe two arbitrary distinct subsets in C. For each node Ci on theon the path. For edge (Ci, C j), let K = Ci ∩ C j . Then,path of T between C and CV 1\K and V 2\K are separated by K in Gt . Therefore, C ∩ Csuch that xis not in K , we can get the paradoxical result that x and x are separated by K in Gt . By the definition of the clique tree, weknow that T is a clique tree of Gt . (cid:2)(cid:9) ⊆ K ⊆ Ci . In fact, if there exists a vertex x ∈ C ∩ C, let C j be the first node from Ci toward C(cid:9)(cid:9)(cid:9)(cid:9)Lemma 3. Assume that T 1 and T 2 are two reduced d-separation trees of a DAG G = (V , E). If T 1 ≺ T 2, then f (T 1) < f (T 2).is a clique tree of Gti for i = 1, 2.Proof. From Lemma 2, there exist two triangulations Gt1, Gt2 of Gm such that T iFrom Lemma 2.21 in [13], there is an increasing sequence Gt1 = G 0, G 1, . . . , Gk = Gt2 of triangulated graphs that differ byB. Liu et al. / Artificial Intelligence 174 (2010) 442–448447exactly one edge, where k is the difference between the numbers of edges in Gt1 and Gt2. Let G j = (V , E j) for 0 (cid:2) j (cid:2) k.Suppose that E1\E0 = {(α, β)}, and from Lemma 2.19 in [13], in graph G 1, (α, β) is a member of only one clique C. Let∗\{β}, where 1 (cid:2) q (cid:2) n. Let K = {C1, . . . , Cq−1, Cq+1, . . . , Cn, Cq1, Cq2}KG 1and then KG 0, Cq1 = C= red(K). We see that= {C1, . . . , Cn}, Cq = C∗\{α} and Cq2 = C∗∗(cid:9)C∈KG0|C| · 2C 2|C| −(cid:9)C∈KG1(cid:8)|C| · 2C 2|C| (cid:2)(cid:9)|C| · 2C 2|C| −(cid:9)|C| · 2C 2|C|C∈KG1|Cq2| · 2|Cq2| · 2|Cq1| + C 2|Cq1| + C 2= C 2= C 2C∈K|Cq1| · 2|Cq1| · 2(cid:8)|C| −|Cq||Cq2| − C 2|Cq2| − 2C 2|Cq| · 2|Cq| · 2|Cq|−1 < 0.Similarly, we can prove that|C| −2(cid:8)|C| · 2C 2C∈KGk|C| < 0, that is, f (T 1) < f (T 2). (cid:2)C∈KG j−1|C| · 2C 2|C| · 2C 2|C| < 0 for each 1 (cid:2) j (cid:2) k, which implies that(cid:8)C∈KG0|C| ·C 2C∈KG jTheorem 2. In a DAG G = (V , E), the following three statements are equivalent:(1) Tree T = (C, E T ) is a minimal d-separation tree of G;(2) T is a minimal separation tree of Gm;(3) T is a clique tree of some minimal triangulation Gt of Gm.Proof. (3) ⇒ (1): T is a clique tree of Gt and then T is a reduced d-separation tree of G. Suppose that T is not a minimal d-separation tree of G. There exists a tree Tis a reduced d-separation tree of G. By Lemma 2, we know(cid:9) = (V , EC∈C(cid:9) Etthat there is a triangulation GC ,EtC . BecauseCC ∧ C(cid:9) = C(cid:9)has fewer edges than Gt has. Gt is therefore not a minimal triangulation of Gm, which is contraryto the fact above.= {(a, b): a, b ∈ C and a (cid:6)= b}. T is a clique tree of Gt . We thus know that Gt = (V , Et) where Et =in less(T ) such that Tof Gm such that T, and furthermore, Gis a clique tree of G(cid:9) =C∈C Et(cid:9)), where Eand C (cid:6)= C(cid:9), G(cid:2)(cid:2)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)(1) ⇒ (2): Assume that T is not a minimal separation tree of Gm, then there exists a tree Tisis a reduced d-separation tree of G from Lemma 2, which is contrary to the fact(cid:9)(cid:9) ∈ less(T ) such that T(cid:9)a reduced separation tree of Gm. Thus Tthat T is a minimal d-separation tree of G.(cid:2)(2) ⇒ (3): Since T is a minimal separation tree of Gm, we know that T is a clique tree of some triangulation Gt ofC . Suppose that Gt is not a minimal triangulation of Gm, i.e., there exists anotheris a reduced(cid:9) ⊆ C and then C ∧ KG(cid:9) = KG(cid:9) . It is obviousis a minimalGm and Gt = (V , Et) where Et =(cid:9)triangulation Gseparation tree of Gm. For each Cthat C (cid:6)= KG(cid:9) , so T is not a minimal separation tree of Gm, which is contrary to the fact above. Thus, Gttriangulation of Gm, and finally we know that T is a clique tree of the minimal triangulation Gt of Gm. (cid:2)of Gm that has fewer edges than Gt has. Construct a clique tree T(cid:9) ∈ KG(cid:9) , there is some C ∈ C such that C(cid:9) = (KG(cid:9) , E T (cid:9) ) of GC∈C Et. T(cid:9)(cid:9)Example 4. To construct a minimal d-separation tree of the DAG in Fig. 2, we need to compute a minimal triangulation(Fig. 1 (right)) of the moral graph (Fig. 1 (left)), and construct a clique tree of the minimal triangulation. We see that T 3 inFig. 3(c) is just a clique tree of the minimal triangulation.According to Theorem 2, a minimal d-separation tree is equivalent to the clique tree of a minimal triangulation of themoral graph. Then efficient algorithms for finding minimal triangulations and clique trees are available for us to construct aminimal d-separation tree. We therefore continue to introduce some algorithms for minimal triangulation.Several characterizations and algorithms of minimal triangulations on general undirected graphs have been presentedsince 1976. As Heggernes pointed out [9], there are two approaches to compute and characterize minimal triangulations:one through vertex elimination and the other through minimal separators of the given graph.The first characterizations of minimal triangulations were given simultaneously by Ohtsuki [14], Ohtsuki et al. [15], andRose et al. [18] in 1976. These characterizations are strongly connected to vertex elimination. Based on these characteriza-tions, some algorithms, such as the algorithm of Ohtsuki [14], the LEX M algorithm [18] and the MCS-M algorithm [3], wereproposed with time bound O (nm), where n is the number of vertices and m is the number of edges of the given graph.For more than 20 years, O (nm) = O (n3) remained the best known time bound for minimal triangulations. A breakthroughwas made in 2004 when a new implementation of LEX M was described by Kratsch and Spinrad [11], which runs in timeO (n2.69).There are also some useful algorithms for minimal triangulations based on characterizations strongly connected to mini-mal separators, such as the LB-Triang algorithm by Berry et al. [1,4], the vertex incremental minimal triangulation algorithmby Berry et al. [2] and the Fast Minimal Triangulation algorithm by Heggernes et al. [8]. The first two algorithms haverunning times of O (nm) and the third one runs in just o(n2.376), which was a simultaneous breakthrough to the LEX Mimplementation made in 2004.448B. Liu et al. / Artificial Intelligence 174 (2010) 442–4484. ConclusionsWe proposed a partial ordering for d-separation trees, where the fact that one tree is smaller than another impliesthat the smaller one is more efficient. Under this ordering, a definition of minimal d-separation tree was introduced. Weobtained the equivalence between minimal d-separation trees of a DAG G and clique trees of the minimal triangulationsof the moral graph of G in Theorem 2. Therefore many efficient algorithms for minimal triangulations and clique trees canbe directly used for us to construct minimal d-separation trees of DAGs. Accordingly, the work of performing the furtherindependence tests in the second steps of many constraint-based algorithms of structural learning can be carried out withmaximal efficiency.However, generally speaking, the constructed minimal d-separation tree is not a minimum d-separation tree with thelargest efficiency. Our minimal d-separation tree just exhibits a tradeoff between efficiency and computation time.AcknowledgementsThe authors are very grateful to the editor and three reviewers for their insightful comments and helpful suggestions,which greatly improved the quality and presentation of the paper. This research was partially supported by the NationalNatural Science Foundation of China (Grant Numbers 10701022, 10871038, 10826110 and 10926186), National 973 KeyProject of China (2007CB311002) and HK RGC Grant HKUST6117/02.References[1] A. Berry, A wide-range efficient algorithm for minimal triangulation, in: Proceedings of the Tenth Annual ACM–SIAM Symposium on Discrete Algorithms(SODA’99), 1999, pp. S860–S861.[2] A. Berry, P. Heggernes, Y. Villanger, A vertex incremental approach for dynamically maintaining chordal graphs, in: Algorithms and Computation, ISAAC2003, in: Lecture Notes in Comput. Sci., vol. 2906, Springer, Berlin, 2003, pp. 47–57.[3] A. Berry, J.R.S. Blair, P. Heggernes, B.W. Peyton, Maximum cardinality search for computing minimal triangulations of graphs, Algorithmica 39 (4) (2004)287–298.[4] A. Berry, J.P. Bordat, P. Heggernes, G. Simonet, Y. Villanger, A wide-range algorithm for minimal triangulation from an arbitrary ordering, J. Algo-rithms 58 (1) (2006) 33–66.[5] J.R.S. Blair, B. Peyton, An introduction to chordal graphs and clique trees, in: A. George, J.R. Gilbert, J. Liu (Eds.), Graph Theory and Sparse MatrixComputation, Springer-Verlag, New York, 1993.[6] R.G. Cowell, A.P. David, S.L. Lauritzen, D.J. Spiegelhalter, Probabilistic Networks and Expert Systems, Springer Publications, New York, 1999.[7] Z. Geng, C. Wang, Q. Zhao, Decomposition of search for V-structures in DAGs, J. Multivariate Anal. 170 (2005) 422–439.[8] P. Heggernes, J.A. Telle, Y. Villanger, Computing minimal triangulations in time O (nα log n) = o(n2.376), SIAM J. Discrete Math. 19 (4) (2005) 900–913.[9] P. Heggernes, Minimal triangulations of graphs: A survey, Discrete Math. 306 (2006) 297–317.[10] F.V. Jensen, Bayesian Networks and Decision Graphs, Springer-Verlag, 2001.[11] D. Kratsch, J. Spinrad, Minimal fill in O (n2.69) time, Discrete Math. 306 (2006) 366–371.[12] H.G. Leimer, Optimal decomposition by clique separators, Discrete Math. 113 (1993) 99–123.[13] S.L. Lauritzen, Graphical Models, Clarendon Press, Oxford, 1996.[14] T. Ohtsuki, A fast algorithm for finding an optimal ordering in the vertex elimination on a graph, SIAM J. Comput. 5 (1976) 133–145.[15] T. Ohtsuki, L.K. Cheung, T. Fujisawa, Minimal triangulation of a graph and optimal pivoting ordering in a sparse matrix, J. Math. Anal. Appl. 54 (1976)622–633.[16] J. Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, San Mateo, CA, 1988.[17] J. Pearl, Causality: Models, Reasoning and Inference, Cambridge University Press, Cambridge, 2000.[18] D. Rose, R.E. Tarjan, G. Lueker, Algorithmic aspects of vertex elimination on graphs, SIAM J. Comput. 5 (1976) 146–160.[19] P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction, and Search, 2nd edition, MIT Press, 2000.[20] R.E. Tarjan, M. Yannakakis, Simple linear-time algorithms to test chordality of graphs, test acyclicity of hypergraphs, and selectively reduce acyclichypergraphs, SIAM J. Comput. 13 (1984) 566–579.[21] T. Verma, J. Pearl, Equivalence and synthesis of causal models, in: Proceedings of the 6th Conference on Uncertainty in Artificial Intelligence, Elsevier,Amsterdam, 1990, pp. 255–268.[22] X. Xie, Z. Geng, Q. Zhao, Decomposition of structural learning about directed acyclic graphs, Artificial Intelligence 170 (2006) 422–439.[23] X. Xie, Z. Geng, A recursive method for structural learning of directed acyclic graphs, J. Mach. Learn. Res. 9 (2008) 459–483.