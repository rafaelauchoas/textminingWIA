Artificial Intelligence 174 (2010) 850–864Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAllDifferent-based filtering for subgraph isomorphismChristine SolnonUniversité de Lyon, Université Lyon 1, LIRIS, UMR5205 CNRS, F-69622, Francea r t i c l ei n f oa b s t r a c tThe subgraph isomorphism problem involves deciding if there exists a copy of a patterngraph in a target graph. This problem may be solved by a complete tree search combinedwith filtering techniques that aim at pruning branches that do not contain solutions. Weintroduce a new filtering algorithm based on local all different constraints. We show thatthis filtering is stronger than other existing filterings — i.e., it prunes more branches — andthat it is also more efficient — i.e., it allows one to solve more instances quicker.© 2010 Elsevier B.V. All rights reserved.Article history:Received 24 December 2009Received in revised form 3 May 2010Accepted 3 May 2010Available online 6 May 2010Keywords:Subgraph isomorphismConstraint programmingAll different constraint1. IntroductionGraphs are widely used in real-life applications to represent structured objects such as, for example, molecules, images,or biological networks. In many of these applications, one looks for a copy of a pattern graph into a target graph [4]. Thisproblem, known as subgraph isomorphism, is NP-complete in the general case [6].Subgraph isomorphism problems may be solved by a systematic exploration of the search space composed of all possibleinjective matchings from the set of pattern nodes to the set of target nodes: starting from an empty matching, one incre-mentally extends a partial matching by matching a non-matched pattern node to a non-matched target node until eithersome edges are not matched by the current matching (the search must backtrack to a previous choice point and go onwith another extension) or all pattern nodes have been matched (a solution has been found). To reduce the search space,this exhaustive exploration is combined with filtering techniques that aim at removing candidate couples of non-matchedpattern-target nodes. Different levels of filtering may be considered; some are stronger than others (they remove morenodes), but also have higher time complexities.In this paper, we describe and compare existing filtering algorithms for the subgraph isomorphism problem, and weintroduce a new filtering algorithm which is stronger. We experimentally evaluate this new filtering algorithm on a widebenchmark of instances, and we show that it is much more efficient on many instances.2. Definitions and notationsA graph G = (N, E) consists of a node set N and an edge set E ⊆ N × N, where an edge (u, uThe set of neighbors of a node u is denoted adj(u) and is defined by adj(u) = {uconsider non-directed graphs, such that (u, uin Section 5.(cid:3)) ∈ E ⇔ (u(cid:3)) is a couple of nodes.(cid:3)) ∈ E}. In this paper, we implicitly(cid:3), u) ∈ E. The extension of our work to directed graphs is discussed(cid:3) | (u, uA subgraph isomorphism problem between a pattern graph G p = (N p, E p) and a target graph Gt = (Nt, Et) consists indeciding whether G p is isomorphic to some subgraph of Gt . More precisely, one should find an injective matching f : N p →Nt , that associates a different target node to each pattern node, and that preserves pattern edges, i.e.,E-mail address: christine.solnon@liris.cnrs.fr.0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.05.002C. Solnon / Artificial Intelligence 174 (2010) 850–864851(cid:2)∀u, u(cid:3)(cid:3)∈ E p,(cid:2)f (u), f(cid:3)(cid:3)(cid:2)(cid:3)u∈ EtThe function fis called a subisomorphism function.Note that the subgraph is not necessarily induced so that two pattern nodes that are not linked by an edge may bematched to two target nodes which are linked by an edge. This problem is also called subgraph monomorphism or subgraphmatching in the literature.In the following, we assume G p = (N p, E p) and Gt = (Nt, Et) to be the underlying instance of subgraph isomorphism) nodes of G pproblem, and we assume without loss of generality that N p ∩ Nt = ∅. We usually denote u or u(resp. Gt ).(resp. v or vWe denote #S the cardinality of a set S. We also define N = N p ∪ Nt , E = E p ∪ Et , np = #N p , nt = #Nt , e p = #E p ,(cid:3)(cid:3)et = #Et , and dp and dt the maximal degrees of the graphs G p and Gt .3. Filtering for subgraph isomorphismSubgraph isomorphism problems may be modeled as constraint satisfaction problems in a very straightforward way.In this section, we first show how to model and solve subgraph isomorphism problems within a constraint satisfactionframework. Then, we describe different filtering algorithms for subgraph isomorphism in Sections 3.3 to 3.6, and we comparethem in Section 3.7.3.1. Modeling and solving subgraph isomorphism by means of constraintsA constraint satisfaction problem (CSP) is defined by a set of variables, such that each variable is associated with adomain (i.e., the set of values that it may be assigned to), and a set of constraints (i.e., relations that restrict the set ofvalues that may be assigned to some variables simultaneously). Solving a CSP involves finding an assignment of values toall variables such that all constraints are satisfied.A subgraph isomorphism problem may be modeled as a CSP by associating a variable (denoted xu ) with every patternnode u. The domain of a variable xu (denoted D u ) contains the set of target nodes that may be matched to u. Intuitively,assigning a variable xu to a value v corresponds to matching the pattern node u to the target node v. The domain D u isusually reduced to the set of target nodes the degree of which is higher or equal to the degree of u as node u may bematched to node v only if #adj(u) (cid:2) #adj(v).Constraints ensure that the assignment of variables to values corresponds to a subisomorphism function. There are twokinds of constraints:• edge constraints ensure that pattern edges are preserved, i.e.,(cid:2)∀u, u(cid:3)(cid:3)∈ E p,(xu, xu(cid:3) ) ∈ Et• difference constraints ensure that the assignment corresponds to an injective function, i.e.,(cid:2)∀u, u(cid:3)(cid:3)∈ N 2p,u (cid:11)= u(cid:3) ⇒ xu (cid:11)= xu(cid:3)Within this framework, solving a subgraph isomorphism problem involves finding an assignment of the variables that sat-isfies all constraints. We shall consider that a variable is assigned whenever its domain is reduced to a singleton, i.e.,D u = {v} ⇔ xu = v.Subgraph isomorphism problems modeled as CSPs may be solved by building a search tree that explores all possiblevariable assignments until finding a solution. The size of this search tree may be reduced by using filtering techniqueswhich propagate constraints to remove values from domains.We briefly recall some basic principles of constraint propagation in Section 3.2. Then, we describe different filteringtechniques that may be used to solve subgraph isomorphism problems in Sections 3.3 to 3.6. Note that some of thesefilterings (i.e., FC(Diff ), GAC(AllDiff ), FC(Edges), and AC(Edges)) are generic constraint propagation techniques that may beused to solve any CSP whereas some others (i.e., LV2002 and ILF(k)) are dedicated to the subgraph isomorphism problem.3.2. Recalls on constraint propagationConstraint propagation aims at filtering variable domains by removing inconsistent values, that is, values that do notbelong to any solution. This constraint propagation step may be done at each choice point of the search. If it removes allvalues in the domain of a variable, then the search can backtrack to a previous choice.A pioneering work for constraint propagation has been done in 1972 by Waltz for a scene drawing application [19].Since then, many different constraint propagation algorithms have been proposed. These algorithms achieve different partialconsistencies and also have different time and space complexities. In this section, we do not aim at describing all existingpropagation algorithms. We only briefly describe two basic and well-known generic techniques, that is, forward-checking andmaintaining arc-consistency. The reader may refer to [17,10] for more information.852C. Solnon / Artificial Intelligence 174 (2010) 850–864Forward-checking The basic idea of forward-checking is to propagate all constraints involving a variable just after its as-signment in order to remove from the domains of the non-assigned variables any value which is not consistent with thisassignment. More precisely, after the assignment of xi to v i , one propagates binary constraints between xi and any non-assigned variable x j by removing from the domain of x j any value v j such that the assignment {(xi, v i), (x j, v j)} violatesthe constraint holding between xi and x j . When constraints have arities greater than two, one may propagate constraintssuch that all variables but one are assigned.Maintaining arc-consistency A stronger filtering, but also a more expensive one, is obtained by maintaining arc-consistency,also called 2-consistency. Roughly speaking, a binary CSP is arc-consistent if each value v i in the domain of a variable xihas at least one support in the domain of every other variable, thus ensuring that if xi is assigned to v i then each othervariable still has at least one consistent value in its domain. More precisely, given a variable xi ∈ X and a value v i ∈ D(xi),a support of (xi, v i) for a variable x j is a value v j ∈ D(x j) such that the partial assignment {(xi, v i), (x j, v j)} is consistent.A binary CSP ( X, D, C) is arc-consistent if every value in every domain has at least one support in the domain of each othervariable.To maintain arc-consistency while constructing a partial assignment A, we filter variable domains after each variableassignment by removing non-supported values. Such a filtering must be repeated until no more domain is reduced: as soonas a value is removed, we must check that this value is not the only support of some other values. There exist many differentalgorithms for ensuring arc-consistency, which exhibit different time and space complexities. For instance, a widely usedalgorithm for achieving arc consistency of a set of binary constraints is AC4 [14] whose time and space complexities areO(ck2), where c is the number of constraints and k the maximum domain size. Although AC4 is worst-case optimal in time,it always reaches this worst case because of its expensive initialisation phase; many improvements have been proposedsince AC4, leading for example to AC6, AC7 and AC2001 (see [17] for more details). Arc consistency may also be generalizedto non-binary CSPs. In this case, it is called generalized arc consistency.3.3. Propagation of difference constraints (FC(Diff) and GAC(AllDiff))Difference constraints may be propagated by forward-checking (denoted FC(Diff )): each time a pattern node u is matchedto a target node v, FC(Diff ) removes v from the domains of all non-matched nodes. This may be done in O(np).FC(Diff ) propagates each binary difference constraint separately. A stronger filtering may be obtained by propagating thewhole set of difference constraints in order to ensure that all pattern nodes can be assigned to different target nodes. Moreprecisely, achieving the generalized arc consistency of a global AllDifferent constraint (denoted GAC(AllDiff )) removes fromthe domain of every pattern node u every target node v such that, when u is matched to v, the other pattern nodes cannotbe matched to all different target nodes. In [16], Régin has shown how to use the matching algorithm of Hopcroft and Karpfor achieving GAC(allDiff ). The time complexity of this algorithm is O(n2p· n2t ).Example 1. Let us consider four variables x1, x2, x3, and x4 such that D1 = {a}, D2 = D3 = {a, b, c}, and D4 = {a, b, c, d}.FC(Diff ) removes a from the domains of x2, x3, and x4.GAC(AllDiff ) also removes a from the domains of x2, x3, and x4. It further removes b and c from the domain of x4 as ifx4 is assigned to b or c, then x2 cannot be assigned to a value different from both x3 and x4.3.4. Propagation of edge constraints (FC(Edges) and AC(Edges))Edge constraints may be propagated by forward checking (denoted FC(Edges)): each time a pattern node u is matched toa target node v, FC(Edges) removes from the domain of every node adjacent to u any target node that is not adjacent to v.This may be done in O(dp · nt).One may go one step further and maintain the arc consistency of edges constraints (denoted AC(Edges)) so that(cid:2)∀u, u(cid:3)(cid:3)∈ E p, ∀v ∈ D u, ∃v(cid:3) ∈ D u(cid:3) ,(cid:2)v, v(cid:3)(cid:3)∈ EtAs a CSP modeling a subgraph isomorphism problem has e p edge constraints and the maximum domain size is nt , the timecomplexity of AC(Edges) is O(e p · n2t ) when using AC4.Example 2. Let us consider the subgraph isomorphism problem displayed in Fig. 1. Note that this instance has no solutionas G p cannot be mapped into a subgraph of Gt . Let us suppose that node 3 has been matched to node E so that D3 = {E},and that E has been removed from the domains of all other pattern nodes (e.g., by FC(Diff ) or GAC(AllDiff )).FC(Edges) removes B, C , and F from the domains of nodes 1, 2, and 4 because B, C , and F are not adjacent to E whereas1, 2, and 4 are adjacent to 3.Like FC(Edges), AC(Edges) removes B, C , and F from the domains of nodes 1, 2, and 4. It is also able to remove G fromthe domain of 1 as the matching (1, G) has no support for the edge (1, 4). Indeed, none of the adjacent nodes of G (i.e., B,F , and E) belongs to the domain of 4. For the same reasons, AC(Edges) also removes G from the domains of 2 and 4.C. Solnon / Artificial Intelligence 174 (2010) 850–8648533.5. Propagation of a set of edge constraints (LV2002)Fig. 1. Instance of subgraph isomorphism problem.Both FC(Edges) and AC(Edges) propagate each edge constraint separately. A stronger filtering is obtained by propagatingedge constraints in a more global way, i.e., by propagating the fact that a whole set of nodes must be adjacent to a givennode. Indeed, a pattern node u may be matched to a target node v only if the number of nodes adjacent to u is smaller orequal to the number of target nodes that are both adjacent to v and belong to domains of nodes adjacent to u (otherwisesome nodes adjacent to u cannot be matched to nodes adjacent to v). Hence, Larrosa and Valiente have proposed in [12] afiltering algorithm (denoted LV2002) which propagates this constraint. More precisely, they define the setF(u, v) =(cid:4)(cid:2)(cid:3)D u(cid:3) ∩ adj(v)u(cid:3)∈adj(u)F (u, v) is a superset of the set of nodes that may be matched to nodes that are adjacent to u if u is matched to v.Therefore, one can remove v from D u whenever #F (u, v) < #adj(u). One can also remove v from D u whenever there(cid:3) ∈ adj(u) such that D u(cid:3) ∩ adj(v) = ∅, thus enforcing arc consistency of edge constraints. The LV2002exists a pattern node ufiltering algorithm has a time complexity of O(n2p· n2t ).Example 3. Let us consider the subgraph isomorphism problem displayed in Fig. 1. Let us suppose that node 3 has beenmatched to node E so that D3 = {E}, and that E has been removed from the domains of all other pattern nodes (e.g., byFC(Diff ) or GAC(AllDiff )).Like AC(Edges), LV2002 removes nodes B, C , F , and G from the domains of nodes 1, 2, and 4. It is also able to removevalues A and D from the domain of 1. Indeed,F(1, A) = (D2 ∪ D3 ∪ D4) ∩ adj( A) = {D, E}F(1, D) = (D2 ∪ D3 ∪ D4) ∩ adj(D) = { A, E}As, #F (1, A) < #adj(1) and #F (1, D) < #adj(1), both A and D are removed from D1 so that the domain of 1 becomesempty and an inconsistency is detected.3.6. Iterated labeling filtering (ILF(k))Zampelli et al. have proposed in [20] a filtering algorithm (called ILF(k)) which exploits the graph structure in a globalway to compute labels that are associated with nodes and that are used to filter domains. More precisely, a compatibilityrelationship is defined over the set of node labels. This compatibility relationship is used to remove from the domain of apattern node u every target node v such that the label of u is not compatible with the label of v.ILF(k) is an iterative procedure that starts from an initial labeling. This initial labeling may be defined by node degrees.In this case, the compatibility relationship is the classical (cid:2) order. This labeling is used to remove from the domain of apattern node u every target node v such that #adj(u) (cid:2) #adj(v) as u cannot be matched to v if u has more adjacent nodesthan v.This initial labeling is extended to filter more values. Given a labeling l and a compatibility relationship (cid:3) between(cid:3)(u) of a node u is the multiset which contains all labels(cid:3)(v) if for every occurrence x of a label in(cid:3)(v) such that x (cid:3) y. The key point relies on the computation of thedt ) thanks to the matching algorithm of Hopcroftlabels of l, one defines a new labeling lof nodes adjacent to u. The compatibility relationship (cid:3)(cid:3)(cid:3)(u) there exists a different occurrence y of a label in llnew compatibility relationship (cid:3)(cid:3)and Karp (see [20] for more details).(cid:3)(u) (cid:3)(cid:3)is such that l√, which is done in O(np · nt · dp · dt ·such that the new label ll(cid:3)Such labeling extensions are iterated. A parameter k is introduced, that determines the number of labeling extensions.Note that iterated labeling extensions may be stopped before reaching this bound k if some domain has been reduced to an854C. Solnon / Artificial Intelligence 174 (2010) 850–864empty set, or if a fixpoint is reached — such that no more value may be filtered. The ILF(k) procedure has a time complexityof O(min(k, np · nt) · np · nt · dp · dt ·dt ).√[20] also introduces a weaker filtering, called ILF*(k). The idea is to approximate, at each iteration, the label compati-bility relationship by a total order so that the next compatibility relation may be computed by sorting the multisets andsequentially comparing them. The time complexity of this weaker filtering is O(min(k, np · nt) · np · nt · dt).Example 4. Let us consider the subgraph isomorphism problem displayed in Fig. 1. The initial degree-based labeling is thelabeling l such that• l(5) = l(6) = 2,• l(1) = l(3) = l(C) = l(E) = l(F ) = l(G) = 3,• l(2) = l(4) = l( A) = l(B) = l(D) = 4and the order over this set of labels is such that• 2 is compatible with 2, 3, and 4,• 3 is compatible with 3 and 4,• 4 is compatible with 4.Hence, one can remove the target nodes C , E, F , and G from the domains of the pattern nodes 2 and 4.(cid:3)such thatThe extension of this initial degree-based labeling is the labeling l• l• l• l• l• l• l• l(cid:3)(F ) = {{3, 4, 4}},(cid:3)(E) = l(cid:3)(3) = l(cid:3)(4) = {{2, 2, 3, 3}},(cid:3)(6) = {{4, 4}},(cid:3)(1) = l(cid:3)(2) = l(cid:3)(5) = l(cid:3)( A) = {{3, 3, 4, 4}},(cid:3)(B) = l(cid:3)(C) = {{4, 4, 4}},(cid:3)(G) = {{3, 3, 4}}(cid:3)(D) = {{3, 3, 3, 4}},and the order over this set of labels is such that• {{3, 4, 4}} is compatible with {{3, 3, 4, 4}} and {{3, 4, 4}},• {{2, 2, 3, 3}} is compatible with {{3, 3, 4, 4}} and {{3, 3, 3, 4}},• {{4, 4}} is compatible with {{3, 3, 4, 4}}, {{4, 4, 4}} and {{3, 4, 4}}.(cid:3)(B), B is removed from D1. For the same reasons, B, D and G are removed from thecan be further extended, thus removing more values, and finally proving the inconsistency of this(cid:3)(1) is not compatible with lAs ldomains of nodes 1, 3, 5 and 6.(cid:3)This new labeling linstance.3.7. DiscussionMost of the algorithms that have been proposed for solving the subgraph isomorphism problem may be described bymeans of the filtering algorithms described in Sections 3.3 to 3.6. In particular:• McGregor [13] combines FC(Diff ) and FC(Edges);• Ullmann [18] combines FC(Diff ) and AC(Edges);• Régin [15] combines GAC(AllDiff ) and AC(Edges);• Larrosa and Valiente [12] combine GAC(AllDiff ) and LV2002;• Zampelli et al. combine GAC(AllDiff ), AC(Edges), and ILF(k).These different filterings achieve different consistencies. Some of them are stronger than others. In particular,• GAC(AllDiff ) is stronger than FC(Diff );• LV2002 is stronger than AC(Edges) which is stronger than FC(Edges).However, GAC(AllDiff ) and FC(Diff ) are not comparable with FC(Edges), AC(Edges), LV2002, and ILF(k) as they do not propagatethe same constraints.The relations between ILF(k) and other filterings that propagate edge constraints (i.e., LV2002, AC(Edges), and FC(Edges))depend on initial domains: if the initial domain of every variable contains all target nodes, then ILF(k) is stronger thanC. Solnon / Artificial Intelligence 174 (2010) 850–864855LV2002, provided that the number of labeling extensions k is greater or equal to 2.1 However, if some domains have beenreduced (which is usually the case when the filtering is done at a node which is not at the root of the search tree), thenILF(k) is not comparable with LV2002 and AC(Edges).Indeed, ILF(k) does not exploit domains to filter values as labelings and compatibility relationships that are iterativelycomputed do not depend at all on domains. To allow ILF(k) to propagate some domain reductions, the iterative labelingextension process has been combined, before each labeling extension, with the two following steps:• Reduction of the target graph with respect to domains: if a target node v does not belong to any domain, then thisnode and its incident edges are discarded from the target graph.• Strengthening of a labeling with respect to singleton domains: if a domain D u is reduced to a singleton {v}, then nodesu and v are labeled with a new label which is not compatible with any other label, except itself, thus preventing otherpattern nodes from being matched with v.When adding these two steps, ILF(k) is stronger than FC(Edges). However, it is still not comparable with LV2002 andAC(Edges).To propagate more domain reductions, one may start the iterative labeling extension process from an initial labelingwhich fully integrates domain reductions in the compatibility relation, so that if a target node v does not belong to thedomain of a pattern node u, then the label associated with v is not compatible with the label associated with u. Moreformally, Zampelli et al. have defined in [20] such an initial labeling, denoted ldom, as follows:• a different unique label lx is associated with every different (pattern or target) node x ∈ N p ∪ Nt ;• ∀(u, v) ∈ N p × Nt , lu is compatible with lv iff v ∈ D u and #adj(u) (cid:2) #adj(v).They have shown that, in this case, ILF(k) is stronger than LV2002 provided that k (cid:4) 2. However, if this filtering is stronger,it is also very expensive to achieve as the complexity of ILF(k) highly depends on the number of different labels. Indeed, thetheoretical complexity of one iteration of ILF(k) (i.e., O(np · nt · dp · dt ·dt )) corresponds to the worst case where all nodeshave different labels. If the number of different pattern and target labels respectively are l p and lt , then the complexity ofone iteration of ILF(k) is O(e p + l p · lt · dp · dt ·dt ).√√4. Global neighborhood constraints and LAD-filteringWe introduce a global neighborhood constraint in Section 4.1, and we describe a propagation algorithm which achievesthe generalized arc consistency of this constraint in Section 4.2. We compare this consistency with other partial consistenciesin Section 4.3.4.1. Global neighborhood constraintsFor each subisomorphism function f : N p → Nt and for each pattern node u ∈ N p , we have:1. ∀u2. ∀(u(cid:3) ∈ adj(u), f (u(cid:3), u(cid:3)) ∈ adj( f (u)),(cid:3)(cid:3)) ∈ adj(u) × adj(u), u(cid:3) (cid:11)= u(cid:3)(cid:3) ⇒ f (u(cid:3)) (cid:11)= f (u(cid:3)(cid:3)).The first property is a direct consequence of the fact that edges are preserved by subisomorphism functions whereas thesecond property is a direct consequence of the fact that subisomorphism functions are injections.When considering the CSP associated with a subgraph isomorphism problem, these two properties may be expressed bythe following constraint on the neighborhood of u:xu = v ⇒ ∀u(cid:3) ∈ adj(u),(cid:2)(cid:5)∧ allDiffxu(cid:3)xu(cid:3) ∈ adj(v)(cid:6)(cid:7)(cid:3)(cid:6) u(cid:3) ∈ adj(u)Note that the filtering algorithm LV2002 introduced by Larrosa and Valiente in [12] actually propagates this neighborhoodconstraint (although it has not been explicitly introduced in [12]). However, LV2002 only ensures a partial consistency: itbasically ensures that the number of nodes adjacent to u is smaller or equal to the number of target nodes that are bothadjacent to v and belong to domains of nodes adjacent to u. In Section 4.2, we describe a filtering algorithm which ensuresthe generalized arc consistency of neighborhood constraints.1 k must be greater or equal to 2 if the initial labeling from which the iterative labeling extension process is started is the empty labeling, that associatesthe same label to all nodes. If the initial labeling is defined by node degrees, then one iteration is enough to obtain a stronger consistency (see [20] formore details).856C. Solnon / Artificial Intelligence 174 (2010) 850–864Fig. 2. Bipartite graphs associated with (1, G) and (3, E).Example 5. Let us consider the subgraph isomorphism problem displayed in Fig. 1, and let us define initial domains withrespect to node degrees, i.e.D1 = D3 = D5 = D6 = { A, B, C, D, E, F , G}D2 = D4 = { A, B, D}The neighborhood constraint for the couple of nodes (1, G) isx1 = G ⇒ x2 ∈ {B, F , E} ∧ x3 ∈ {B, F , E} ∧ x4 ∈ {B, F , E}{x2, x3, x4}∧ allDiff(cid:3)(cid:2)Achieving the generalized arc consistency of this constraint allows us to remove G from D1: if x1 = G then both x2 andx4 must belong to the singleton {B} (corresponding to the intersection of their domains with {B, F , E}) so that x2 and x4cannot be assigned to different values.Note that on this example, the filtering LV2002 cannot remove G from D1 as F (1, G) = (D2 ∪ D3 ∪ D4) ∩ adj(G) = {B, E, F }so that #F (1, G) (cid:4) #adj(1). Note also that a simple allDiff constraint on the set of variables {x2, x3, x4} cannot be used toremove G from D1: one has to combine this allDiff constraint with the fact that, if 1 is matched to G, then 2, 3, and 4 mustbe matched to nodes that are adjacent to G.4.2. A filtering algorithm for propagating global neighborhood constraintsThe generalized arc consistency of a neighborhood constraint may be ensured by looking for a covering matching in abipartite graph, as proposed by Régin in [16] for the AllDifferent global constraint. Let us recall that a matching of a graphG = (N, E) is a subset of edges m ⊆ E such that no two edges of m share a same endpoint. A matching m ⊆ E covers a setof nodes Ni if every node of Ni is an endpoint of an edge of m. In this case, we shall say that m is a Ni -covering matchingof G.For every couple of nodes (u, v) such that v ∈ D u , we define a bipartite graph that associates a node with every node(cid:3)) such that vadjacent to u or v and an edge with every couple (u(cid:3), v(cid:3) ∈ D u(cid:3) .Definition 1. Given two nodes (u, v) ∈ N p × Nt such that v ∈ D u , we define the bipartite graph G(u,v) = (N(u,v), E(u,v)) suchthat• N(u,v) = adj(u) ∪ adj(v);• E(u,v) = {(u(cid:3), v(cid:3)) ∈ adj(u) × adj(v) | v(cid:3) ∈ D u(cid:3) }.If there does not exist a matching of the bipartite graph G(u,v) that covers adj(u), then the nodes adjacent to u cannotbe matched to all different nodes, and therefore v can be removed from D u .This filtering must be iterated. Indeed, when v is removed from D u , the edge (u, v) is removed from other bipartitegraphs so that some bipartite graphs may no longer have covering matchings. A key point for an incremental implementa-(cid:3) ∈ adj(u) andtion of this filtering lies in the fact that the edge (u, v) only belongs to bipartite graphs G(u(cid:3),v(cid:3)) such that u(cid:3)). Filtering is iterated until either a domain becomes empty — thus detecting an inconsistency — or reach-(cid:3) ∈ adj(v) ∩ D(uving a fixpoint such that generalized arc consistency has been enforced, i.e., such that for every couple (u, v) there exists aadj(u)-covering matching of G(u,v).Example 6. The bipartite graph G(1,G) used to propagate the neighborhood constraint of Example 5 is displayed in the leftpart of Fig. 2. There does not exist a matching of this graph that covers adj(1) because both 2 and 4 can only be matchedto B. As a consequence, one can remove G from D1.The bipartite graph G(3,E) used to propagate the neighborhood constraint associated with the couple (3, E) is displayedin the right part of Fig. 2. There exists a matching of this graph that covers adj(3) (e.g., m = {(1, G), (2, A), (4, D)}) so that EC. Solnon / Artificial Intelligence 174 (2010) 850–864857Algorithm 1 LAD-filteringInput: A set S of couples of pattern/target nodes to be filteredOutput: failure (if an inconsistency is detected) or success.In case of success, domains are filtered so that ∀u ∈ N p, ∀v ∈ D u , there exists a matching of G(u,v) that covers adj(u).beginwhile S (cid:11)= ∅ doRemove a couple of pattern/target nodes (u, v) from Sif there does not exist a matching of G(u,v) that covers adj(u) thenRemove v from D uif D u = ∅ then return failure(cid:3) ∈ adj(u), v(cid:3), vS ← S ∪ {(u(cid:3)) | u(cid:3) ∈ adj(v) ∩ D u(cid:3) }endendreturn successendis not removed from D3. However, once G has been removed from D1, the edge (1, G) is removed from G(3,E) and there nolonger exists a matching that covers adj(3) (as both 1, 2, and 3 can only be matched to A and D). Hence, E is also removedfrom D3.Algorithm 1 describes the resulting filtering procedure, called LAD (Local All Different) filtering. This procedure takes ininput a set S of couples of pattern/target nodes to be filtered. At the root of the search tree, this set should contain allcouples of pattern/target nodes, i.e., S = {(u, v) | u ∈ N p, v ∈ D u}. Then, at each choice point of the search tree, S should beinitialized with the set of all couples (u, v) such that v ∈ D u and a node adjacent to v has been removed from the domainof a node adjacent to u since the last call to LAD-filtering.(cid:3)For each couple of nodes (u, v) that belongs to the set S, LAD-filtering checks if there exists a matching of G(u,v) thatis adjacent to u, andcovers adj(u). If this is not the case, then v is removed from D u , and all couples (uvis adjacent to v and belongs to D u(cid:3) are added to S.The key point is to efficiently implement the procedure that checks if there exists a covering matching of G(u,v). Réginhas shown in [16] that one can use the algorithm of Hopcroft and Karp [7] to find such a matching. The time complexityof this algorithm is O(ab) where a and b respectively are the number of edges and nodes in the bipartite graph. As thebipartite graph G(u,v) has #adj(u) + #adj(v) nodes and, in the worst case (if no domain has been reduced), #adj(u) · #adj(v)edges, and as dt (cid:4) dp (otherwise the subgraph isomorphism problem instance is trivially inconsistent), the complexity ofchecking if there exists a covering matching of G(u,v) is O(dp · dt ·(cid:3)) such that udt ).(cid:3), v√√(cid:3)(cid:8)This complexity may be improved by exploiting the fact that the algorithm of Hopcroft and Karp is incremental: startingfrom an empty matching, it iteratively computes new matchings that contain more edges than the previous matching, untilthe matching is maximal. Each iteration basically consists in a breadth first search and is in O(d p · dt) whereas the numberof iterations is bounded by 2 ·dt + dp . However, if one starts the algorithm from a matching that already contains k edges,and if the maximal matching has l edges, then the number of iterations is also bounded by l − k (as the size of the matchingincreases of at least one at each iteration).We use this property to improve the time complexity of LAD-filtering. More precisely, for each pattern node u ∈ N pand each target node v ∈ D u , we memorize the last computed matching of G(u,v). The space complexity of memorizingthe covering matchings of all bipartite graphs is O(np · nt · dp) (there are at most np · nt bipartite graphs, and the coveringmatching of G(u,v) is composed of #adj(u) edges). As it would be very expensive, both in time and memory, to create acopy of all covering matchings at each choice point, we simply update these covering matchings whenever this is necessary.More precisely, each time we need to check if there exists a covering matching of a bipartite graph G(u,v), we proceed asfollows:1. we scan the last recorded matching of adj(u) and remove every couple (u2. if one or more couples have been removed, then we call Hopcroft Karp to complete it;3. if Hopcroft Karp actually succeeds in completing it, then we record the computed complete matching.(cid:3), v(cid:3)) such that v(cid:3)no longer belongs to D(u(cid:3));Theorem 1. The time complexity of LAD-filtering is O(np · nt · d2p· d2t ).Proof.• The complexity for computing a first covering matching for all bipartite graphs is O(np · nt · dp · dt ·√dt ); this step isperformed once, at the beginning of the search process.• Each time a value v is removed from a domain D u , one has to update the matchings of all bipartite graphs G(u(cid:3),v(cid:3))(cid:3) ∈ D u(cid:3) ∩ adj(v), i.e., of dp · dt bipartite graphs in the worst case, and each update is done• In the worst case, only one value is removed when updating the covering matchings of all neighbors and there are(cid:3) ∈ adj(u) and vsuch that uincrementally in O(dp · dt).np · nt values to remove. (cid:2)858C. Solnon / Artificial Intelligence 174 (2010) 850–8644.3. Comparison of LAD-filtering with other filteringsIn this section, we compare the consistency ensured by LAD-filtering with other partial consistencies.Theorem 2. LAD-filtering (Algorithm 1 with S initialized to all couples (u, v) such that u ∈ N p and v ∈ D u ) ensures the GeneralizedArc Consistency of neighborhood constraints, denoted GAC(Neighborhood).Proof. If there exists a pattern node u ∈ N p such that for every target node v ∈ D u , it is not possible to assign every(cid:3) ∈ adj(u) to a different target node which is adjacent to v and belongs to D u(cid:3) , then LAD-filteringdifferent pattern node uremoves every value v ∈ D u (because every bipartite graph G(u,v) does not have a adj(u)-covering matching), and returnsfailure. Otherwise, it returns success and filters domains so that for every pattern node u ∈ N p and every target node v ∈ D u ,(cid:3) ∈ adj(u) can be assigned to a different target node which is adjacent to v and belongs toevery different pattern node uD u(cid:3) (as every bipartite graph G(u,v) has an adj(u)-covering matching). (cid:2)Theorem 3. GAC(Neighborhood) is stronger than LV2002.Proof. GAC(Neighborhood) is at least as strong as LV2002 because, for each pattern node u ∈ N p and each target node v ∈ D u ,if there exists a adj(u)-covering matching of G(u,v), then all target nodes of this covering matching belong to the set F (u, v)and therefore #F (u, v) (cid:4) #adj(u). It is actually strictly stronger: for example, it is able to detect the inconsistency of theinstance displayed in Fig. 1 whereas LV2002 is only able to reduce the domains of the variables associated with nodes 2 and4 to { A, B, D} whereas the domains of the other variables are not reduced. (cid:2)Theorem 4. GAC(Neighborhood) is as strong as ILF(k) when labeling extensions are started from the initial labeling ldom and whenthey are iterated until reaching a fixpoint, i.e., k = ∞.Proof. The initial labeling ldom associates a unique different label with every node, and the label of a pattern node u is com-patible with the label of a target node v iff #adj(u) (cid:2) #adj(v) and v ∈ D u . With such an initial compatibility relationship,the multiset mu that contains all labels of nodes adjacent to u is compatible with the multiset mv that contains all labelsof nodes adjacent to v iff there exists a covering matching of G(u,v) (as a label of mu is compatible with a label of mv iffthere is an edge between the corresponding nodes in G(u,v)). When a node v is removed from a domain D u , both ILF(∞)and LAD check, for every couple (umay still be matched to adifferent node adjacent to v. In both cases, this is done in an iterative process, until a fixpoint is reached. The differencebetween ILF(∞) and LAD is that ILF(∞) recomputes all matchings, for all possible pattern/target couples, at each iteration,whereas LAD only updates matchings that have actually been impacted by domain reductions. Hence, LAD has a lower timecomplexity. (cid:2)(cid:3)) ∈ adj(u) × adj(v) ∩ D u(cid:3) , that every node adjacent to u(cid:3), v(cid:3)(cid:3)Actually, ILF(k) performs very poorly when it is started from the initial labeling ldom. It performs much better when it isstarted from an initial labeling defined with respect to node degrees: with such an initial labeling, the number of differentlabels is usually strongly reduced and, therefore, the number of compatibility relationships to compute is also stronglyreduced.Theorem 5. GAC(Neighborhood) is weaker than Singleton Arc Consistency of Edge and AllDifferent constraints (denoted SAC(Edges+ AllDiff)).Proof. Let us first recall that singleton arc consistency ensures that we can enforce arc consistency without failure after anyassignment of a value to a variable [1]. Hence, SAC(Edges + AllDiff ) ensures that, ∀u ∈ N p, ∀v ∈ D u , if D u is reduced to thesingleton {v}, then AC(Edges) combined with GAC(AllDiff ) does not detect an inconsistency.• SAC(Edges + AllDiff ) is at least as strong as GAC(Neighborhood): when reducing a domain D u to a singleton {v}, ifAC(Edges) combined with GAC(AllDiff ) does not detect an inconsistency, then there exists a adj(u)-covering matching ofthe bipartite graph G(u,v). Indeed, AC(Edges) will reduce domains of nodes adjacent to u to nodes which are adjacentto v, while GAC(AllDiff ) will ensure that all nodes adjacent to u can be assigned to all different values.• SAC(Edges + AllDiff ) is actually stronger than GAC(Neighborhood) as it is able to detect the inconsistency of the subgraphisomorphism problem instance displayed in Fig. 3 whereas GAC(Neighborhood) does not reduce any domain. (cid:2)However, the optimal worst-case time complexity of enforcing singleton arc consistency of a binary CSP is O(nd3e) wheree is the number of constraints, n is the number of variables and d is the domain size [1]. For our subgraph isomorphismCSP, if we only consider the binary edge constraints, we have n = np , d = nt , and e = e p so that enforcing SAC(Edges)is in O(np · n3· e p). Let us consider the case of fixed-degree graphs such that e p = (np · dp)/2. In this case, the timetC. Solnon / Artificial Intelligence 174 (2010) 850–864859Fig. 3. Instance of subgraph isomorphism problem. Let us suppose that the initial domains are D1 = D 2 = D 3 = { A, B, C, D}. GAC(Neighborhood) does notreduce any domain as every bipartite graph G(u,v) has an adj(u)-covering matching (see, e.g., the bipartite graph G(1,a) displayed on the right part of thefigure). However, SAC(Edges + AllDiff ) detects an inconsistency: if D1 is reduced to { A}, then AC(Edges) reduces D2 and D 3 to nodes that are adjacent to A(i.e., to {C, B}) and the edge (3, 2) is no longer supported (as Gt has no edge between C and B) so that AC(Edges) detects an inconsistency.Algorithm 2 LAD-filtering for directed graphsInput: A set S of triples (u, v, x) such that x ∈ {pred, succ}Output: failure (if an inconsistency is detected) or success.In case of success, domains are filtered so that ∀u ∈ N p , ∀v ∈ D u , there exist a matching of G predcovers succ(u).(u,v) that covers pred(u) and a matching of G succ(u,v) thatbeginwhile S (cid:11)= ∅ doRemove a triple (u, v, x) from Sif there does not exist a matching of G xRemove v from D uif D u = ∅ then return failure(cid:3), vS ← S ∪ {(u(cid:3), succ) | u(u,v) that covers x(u) then(cid:3) ∈ succ(u), v(cid:3) ∈ succ(v) ∩ D u(cid:3) } ∪ {(u(cid:3), v(cid:3), pred) | u(cid:3) ∈ pred(u), v(cid:3) ∈ pred(v) ∩ D u(cid:3) }endendreturn successendcomplexity of enforcing SAC(Edges) is O(n2· dp), which should be compared to the time complexity of LAD-filtering, i.e.,pO(np · nt · d2t ). In the worst case, i.e., if both G p and Gt are complete graphs so that dp = np − 1 and dt = nt − 1, enforcingpSAC(Edges) and LAD-filtering have the same time complexity. However, for sparser graphs, LAD-filtering has a lower timecomplexity.· n3t· d25. Extension to directed graphsLAD-filtering may be extended to directed graphs in a rather straightforward way. In directed graphs, edges are orderedcouples of nodes and, for each node u, one distinguishes the set of successor nodes succ(u) that may be reached by an(cid:3)) ∈ E}), from the set of predecessor nodes pred(u) that may be reached from anoutgoing edge (i.e., succ(u) = {uingoing edge (i.e., pred(u) = {u(cid:3) ∈ N | (u, u(cid:3) ∈ N | (u(cid:3), u) ∈ E}).To extend LAD-filtering to directed graphs, one has to associate two bipartite graphs with every couple (u, v) such thatu ∈ N p and v ∈ D u :• the bipartite graph used to check that each successor of u may be matched to a different successor of v, i.e., G succ(u,v)= {(u• the bipartite graph used to check that each predecessor of u may be matched to a different predecessor of v, i.e.,= succ(u) ∪ succ(v) and E succ(u,v)(u,v)) with N succ(u,v)(cid:3)) ∈ succ(u) × succ(v) | v(u,v), E succ(cid:3) ∈ D u(cid:3) },(N succ(cid:3), v=G pred(u,v)= (N pred(u,v), E pred(u,v)) with N pred(u,v)= pred(u) ∪ pred(v) and E pred(u,v)= {(u(cid:3), v(cid:3)) ∈ pred(u) × pred(v) | v(cid:3) ∈ D u(cid:3) }.Algorithm 2 extends Algorithm 1 to directed graphs. The main difference is that it maintains a set of triples (u, v, x) suchthat x ∈ {pred, succ} instead of a set of couples (u, v). At each iteration, a triple (u, v, x) is removed from the set, and ifthe graph G x(u,v) does not have a covering matching, then v is removed from D u and S is updated by adding all triples(cid:3), x(cid:3)) such that an edge has been removed from the bipartite graph G x(cid:3), v(u(cid:3)(u(cid:3),v(cid:3)).6. Experimental results6.1. Test suiteWe consider 1993 subgraph isomorphism instances that come from three different databases.Scale-free database (classes sf-d-D-n and si-d-D-n) This database has been used in [20] to evaluate ILF(k). Graphs of theseinstances are scale-free networks that have been randomly generated using a power law distribution of degrees P (d = k) =−λ with λ = 2.5 (see [20] for more details). There are 5 classes. Each of the first four classes, denoted sf-d-D-n, containsk20 feasible instances such that the target graph has n nodes which degrees are bounded between d and D, and the patterngraph is extracted from the target graph by randomly selecting 90% of nodes and edges from the target graph in such a860C. Solnon / Artificial Intelligence 174 (2010) 850–864way that the pattern graph is still connected. The fifth class, denoted si-d-D-n, contains 20 non-feasible instances that havebeen generated like instances of the first four classes, excepted that 10% of new edges have been added in pattern graphsin order to obtain infeasible instances.GraphBase database (class LV) This database has been used in [12] to evaluate LV2002. It contains 113 undirected graphswith different properties, i.e., simple, acyclic, connected, biconnected, triconnected, bipartite and planar. We have consideredthe 50 first graphs. This set contains graphs ranging from 10 to 128 nodes. Using these graphs, we have generated 793instances of the subgraph isomorphism problem by considering all couples of graphs (G p, Gt) that are not trivially solved,i.e., such that e p > 0, np (cid:2) nt and dp (cid:2) dt .Vflib database (classes bvg-n, bvgm-n, m4D-n, m4Dr-n, and r-d-n) This database has been used in [2] to evaluate Vflib, aprogram dedicated to graph and subgraph isomorphism problems. It contains 63 classes of instances, and each class containsinstances such that the target graph has from 20 to 1000 nodes. For each class, we have only considered 4 sizes and, foreach size, we have only considered the first 10 instances. We have grouped classes as follows (see [5] for more details onthe original classes):• bvg-n (where n ∈ {100, 200, 400, 800} corresponds to the number of nodes of the target graphs);These classes contain fixed-valence graphs and are composed of the first 10 instances of the original classes six-b y -nwhere x ∈ {2, 4, 6} corresponds to the size of the pattern graph with respect to the target graph (i.e., 20%, 40%, or 60%)and y ∈ {3, 6, 9} corresponds to the valence. Hence, each class bvg-n contains 90 instances.• bvgm-n (where n ∈ {100, 200, 400, 800} corresponds to the number of nodes of the target graphs);These classes contain modified bounded-valence graphs and are composed of the first 10 instances of the originalclasses six-b y m-n where x ∈ {2, 4, 6} corresponds to the size of the pattern graph with respect to the target graph (i.e.,20%, 40%, or 60%) and y ∈ {3, 6, 9} corresponds to the valence. Hence, each class bvgm-n contains 90 instances.• m4D-n (where n ∈ {81, 256, 526, 1296} corresponds to the number of nodes of the target graphs);These classes contain graphs that correspond to 4D regular meshes and are composed of the first 10 instances of theoriginal classes six-m4D-n where x ∈ {2, 4, 6} corresponds to the size of the pattern graph with respect to the targetgraph (i.e., 20%, 40%, or 60%). Hence, each class m4D-n contains 30 instances.• m4Dr-n (where n ∈ {81, 256, 526, 1296} corresponds to the number of nodes of the target graphs):These classes contain graphs that correspond to 4D irregular meshes and are composed of the first 10 instances ofthe original classes six-m4Drr -n where x ∈ {2, 4, 6} corresponds to the size of the pattern graph with respect to thetarget graph (i.e., 20%, 40%, or 60%) and r ∈ {2, 4, 6} corresponds to the degree of irregularity. Hence, each class m4Dr-ncontains 90 instances.• r-p-n (where n ∈ {100, 200, 400, 600} corresponds to the number of nodes and p ∈ {0.01, 0.05, 0.1} corresponds to theprobability of adding an edge between two nodes).These classes contain graphs that have been randomly generated and are composed of the first 10 instances of theoriginal classes six-rand-rp -n where x ∈ {2, 4, 6} corresponds to the size of the pattern graph with respect to the targetgraph (i.e., 20%, 40%, or 60%). Hence, each class r-p-n contains 30 instances.6.2. Considered solversLAD LAD-filtering has been implemented in C and has been integrated in a complete tree search. At each node of the searchtree, the next pattern node to be assigned is chosen with respect to the minDom heuristic, i.e., we choose the non-assignedpattern node that has the smallest number of target nodes in its domain. A choice point is created for each target node thatbelongs to the domain of the variable to be assigned, and these different choice points are explored by increasing order ofvalues. At each node of the search tree, LAD-filtering is combined with GAC(AllDiff ). This search procedure is called LAD.LAD is compared with ILF(k), with k ∈ {1, 2, 4}, Abscon(GAC), Abscon(FC), and Vflib.ILF(k) The original implementation of ILF(k) was in Gecode. We consider here a new implementation in C which uses thesame data structures and the same ordering heuristics as LAD, and which is also combined with GAC(AllDiff ). This newimplementation is much more efficient than the original one. For example, instances of class sf5-8-1000 are solved in 0.19seconds with the new implementation of ILF(1) whereas they were solved in 11.2 seconds with the old implementation.We compare results obtained with different numbers of labeling extension iterations, i.e., with k ∈ {1, 2, 4}. We do notreport results with k > 4 as this never improves the solution process.Abscon Abscon is a generic CSP solver written in Java by Lecoutre and Tabary (see [11] for more details). The fact thatAbscon is implemented in Java whereas all other approaches are implemented in C or C++ must be taken into account sinceJava programs have running times several time larger than their C/C++ counterparts. We consider two variants of this solver:• Abscon(FC) performs a forward checking propagation of the constraints, i.e., FC(Edges) and FC(Diff ).C. Solnon / Artificial Intelligence 174 (2010) 850–864861Table 1Finding all solutions: for each class, the first line gives the number of solved instances (in less than one hour on a 2.26 GHz Intel Xeon E5520), and thesecond line gives the CPU time (average on the completed runs).ClassVflibsf5-8-200sf5-8-600sf5-8-1000sf20-300-300si20-300-300bvg-100bvgm-100m4D-81m4Dr-81r0.01-100r0.05-100r0.1-100All instances1672.450–0–0–0–900.02896.55300.09901.652183.602513.010–33813.83AbsconFC202.0420138.10201651.1116386.876823.20901.998911.06301.08903.7023121.982228.602564.91451118.72AC201.7520135.01201631.8816474.1151046.91902.789016.57301.04902.4028322.672356.7828218.38460144.86ILFk = 1200.00200.07200.19200.3520132.33900.04900.48300.03900.1829158.3523135.8128217.1748034.41k = 2k = 4200.02200.15200.55205.951930.42900.07900.49300.05900.1929170.6322107.1828242.0047831.09200.03200.15200.59208.241948.75900.13900.48300.05900.2029170.5322108.9928243.1247832.07LAD200.02200.29200.83202.562027.77900.75900.53300.02900.1829180.242319.7329148.3848122.34• Abscon(AC) maintains Arc Consistency of edge constraints. For the difference constraints, it maintains a consistency thatis stronger than AC(Diff) but weaker than GAC(AllDiff ). It also uses symmetry breaking techniques.Both variants consider the minDom ordering heuristic for choosing the next variable to assign.Vflib Vflib [2,3] is a solver dedicated to graph and subgraph isomorphism problems, and it is considered as the state-of-the-art for subgraph isomorphism. It basically performs a forward checking propagation of edge and difference constraints, butthis propagation is limited to nodes that are adjacent to already matched nodes for difference constraints. It uses specificvariable and value ordering heuristics: variable and values are chosen so that the subgraph induced by the matched nodesis connected (except when the pattern or the target graphs are composed of different connected components).6.3. Experimental comparison on the problem of finding all solutionsLet us first consider the problem of finding all solutions to an instance, thus allowing a comparison that is less dependenton value ordering heuristics. For this first experiment, we have discarded instances that have too many solutions. Hence, wehave only considered classes from the scale-free database, and the smallest classes of the vflib database (such that the targetgraph has 100 or 81 nodes).Table 1 displays, for each class and each considered approach, the number of instances for which all solutions have beenfound in less than one hour on a 2.26 GHz Intel Xeon E5520, and the average corresponding CPU time. On these classes, LADhas solved 1 (resp. 3, 3, 23, 29, and 143) more instances than ILF(1) (resp. ILF(2), ILF(4), Abscon(AC), Abscon(FC), and Vflib).When comparing CPU times, we note that LAD is slower than the three variants of ILF on classes sf-5-8-* and bvg, but theseinstances are easy ones and LAD solves them in less than one second. However, on harder classes such as si20-300-300,r0.05-100, and r0.1-100, LAD is significantly quicker than ILF. On all classes, LAD and ILF are an order quicker than Abscon.Vflib is competitive on classes bvg-100, m4D-81, and m4Dr-81, but it is not competitive at all on all other classes.Table 2 displays the average number of fail nodes (i.e., the number of times an inconsistency is detected), for each classand each approach except Vflib (because this information is not available in Vflib). On some classes, such as sf5-8-*, LADand ILF have comparable numbers of failed nodes, and this corresponds to the classes that are more quickly solved by ILFthan by LAD. However, on some other instances, such as r*-100, LAD explores many fewer nodes than ILF. The number of fail862C. Solnon / Artificial Intelligence 174 (2010) 850–864Table 2Number of fail nodes (average on the completed runs); numbers in brackets after class names give the average number of solutions of the instances of theclass.Class (#solutions)sf5-8-200 (1.10)sf5-8-600 (1.00)sf5-8-1000 (1.05)sf20-300-300 (4.45)si20-300-300 (0.00)bvg-100 (218)bvgm-100 (145,855)m4D-81 (1253)m4Dr-81 (30,642)r0.01-100 (57,291,325)r0.05-100 (6,062,230)r0.1-100 (30,501,838)AbsconFC3076418557397,844913,73010,0378977190422,92038,853233,044819,714AC10841855729,33861,191286295,61832723,5926,749,220615,8821,985,225ILFk = 15473815,342461641701135610,6212,857,2792,227,792k = 2000136239137966913046717539,5222,224,579k = 400072239122265213006175539,1672,224,408LAD000027012312605243320,067Table 3Finding the first solution of instances of the LV class: #solved is the number of solved instances (in less than one hour on a 2.26 GHz Intel Xeon E5520),Time and #fail respectively give the CPU time and the number of fail nodes (average on the completed runs).Vflib46873.72–AbsconFC64772.511,202,372AC66254.25324,075ILFk = 169830.85297,107#solvedTime#failk = 269931.12182,588k = 469930.77159,493LAD72814.5713,560nodes of both ILF and LAD is an order smaller than Abscon. On some classes, Abscon(AC) has more fail nodes than Abscon(FC),but this corresponds to the fact that Abscon(AC) solves more instances than Abscon(FC) and, for these harder instances, thenumber of fail nodes is significantly higher than for the instances that are solved by both approaches.6.4. Experimental comparison on the problem of finding the first solutionTo illustrate scale-up properties of the different approaches and compare them on a larger set of instances, we nowconsider the problem of finding the first solution (or proving inconsistency). For this comparison, we consider instances ofthe LV class and the larger classes of the vflib database (such that the target graph has more than 100 nodes).Table 3 displays the number of solved instances for the LV class, which contains 793 instances with many differentfeatures (graphs have different properties and sizes; some instances are feasible and have many solutions, some others areinconsistent). For this class, LAD has solved 29 (resp. 29, 30, 66, 81, and 260) more instances than ILF(4) (resp. ILF(2), ILF(1),Abscon(AC), Abscon(FC), and Vflib). This table also shows us that Abscon(AC) and ILF(1) have comparable number of fail nodes,and nearly four times as less as Abscon(FC). ILF(2) and ILF(4) have smaller number of fail nodes but the reduction of thesearch space is not enough to allow ILF(2) and ILF(4) to become competitive. The number of fail nodes of LAD is muchsmaller (more than 20 times as small as Abscon(AC) and ILF(1)).Tables 4 and 5 allow us to compare scale-up properties of the different considered approaches. Table 4 displays results onrather easy classes of the Vflib database. LAD is able to solve the 900 instances of these classes in less than 1.5 seconds onaverage, and it is nearly 4 times as fast as ILF(k). It is also significantly faster than Abscon. Interestingly, Vflib is very efficientand exhibits very good scale-up properties on some classes such as bvg-*, bvgm-*, and m4Dr-*. Actually, Vflib uses variableand value ordering heuristics that are not used by the other approaches: at each iteration, it chooses the next couple (u, v)of nodes to match so that both u and v are adjacent to some nodes that have already been matched (whenever this ispossible). These ordering heuristics may explain the very good behavior of Vflib on some instances when the goal is to findonly one solution. It may also explain the fact that it is able to solve 29 instances of the m4D-256 class in less than 0.01second, whereas it is not able to solve the last instance of this class in one hour.However, Table 5 shows us that the different approaches exhibit different scale-up properties on the random classesr-p-n. Indeed, when the probability p of adding an edge is 0.01, LAD is better than Abscon which is better than ILF, whereaswhen this probability increases, Abscon is better than ILF which is better than LAD. Actually, the denser and the larger thegraphs, and the worse LAD. This may come from the fact that the complexity of LAD-filtering is O(np · nt · d2t ): the degreepis 10 times bigger (on average) for the graphs of classes r0.1-* than for those of classes r0.01-*. Therefore, when graphs arerather sparse, it is worth filtering with LAD whereas when graphs are denser, one has better consider a simpler filteringprocedure such as AC(Edges).· d2C. Solnon / Artificial Intelligence 174 (2010) 850–864863Table 4Finding the first solution of easy instances of the vflib base: for each class, the first line displays the number of solved instances (in less than one hour ona 2.26 GHz Intel Xeon E5520) and the second line the CPU time (average on completed runs).Classbvg-200bvg-400bvg-800bvgm-200bvgm-400bvgm-800m4D-256m4D-526m4D-1296m4Dr-256m4Dr-526m4Dr-1296All instancesVflib900.00900.00900.02900.00900.01900.03290.00234.11200.05900.00900.01900.068820.12AbsconFC900.68902.859054.13900.95893.209012.02302.8830159.7623252.73907.918923.4789193.2789041.95AC900.78902.999054.86900.73891.669012.07301.7330164.9023242.47901.448923.3589188.4489040.60ILFk = 1900.00900.01900.03900.00901.55900.06300.01299.612952.93900.238914.02906.418974.25k = 2k = 4900.00900.01900.04900.00900.01900.04300.013032.932961.21901.058918.31905.468985.55900.00900.01900.05900.00900.01900.03300.012930.722973.33902.248919.08905.438976.04LAD900.14901.06908.41900.01900.04900.19300.04301.71305.67900.06900.33901.639001.43Table 5Finding the first solution of hard instances of the Vflib base: for each class, the first line displays the number of solved instances (in less than one hour ona 2.26 GHz Intel Xeon E5520) and the second line the CPU time (average on completed runs).ClassVflibr0.01-200r0.01-400r0.01-600r0.05-200r0.05-400r0.05-600r0.1-200r0.1-400r0.1-60031735.930–0–0–0–0–0–0–0–All instances31735.93AbsconFC28192.142033.1423226.3825266.7722632.84141915.6527143.3661764.630–165443.14AC300.992969.6823236.1428142.8024647.48141936.9829309.5461972.180–183409.55ILFk = 12827.4814175.8312428.1428125.5725519.04131505.5126320.5251950.670–151414.03k = 22844.0914228.7891069.9628198.6825500.5452319.6826357.7051917.680–140447.36k = 42846.4914214.857806.9628198.6625494.1252304.8526351.1052070.810–138426.67LAD300.043045.5829113.513038.28171190.8812100.6121646.311961.350–159268.48864C. Solnon / Artificial Intelligence 174 (2010) 850–8647. ConclusionWe have introduced a new filtering algorithm for subgraph isomorphism that basically ensures that all nodes adjacentto a same pattern node may be matched to nodes that are all different and that are all adjacent to a same target node.This filtering is stronger than LV2002. Actually, it achieves the same consistency as the strongest variant of ILF(k), i.e., whenthe initial labeling fully integrates domain reductions and when labeling extensions are iterated until reaching a fixpoint.However, this consistency is achieved at a lower cost by updating matchings incrementally instead of recomputing themfrom scratch at each iteration, and by updating only the matchings that are impacted by a domain reduction instead ofrecomputing all matchings.We have experimentally shown on a wide benchmark of 2000 or so instances that this new filtering is able to solve moreinstances quicker, and that it drastically reduces the search space so that many instances are solved without backtracking.However, this filtering is outperformed by arc consistency on the densest random graphs, such that edge density is greateror equal to 10%.This filtering procedure could be easily integrated within a constraint programming language. In particular, we plan tointegrate it in our constraint-based graph matching system [9] that is built on top of Comet [8].We also plan to improve LAD-filtering by studying different strategies for choosing, at each iteration, the next couple(u, v) that is removed from S. In the results reported in this paper, we have considered a basic last in first out strategy asS is implemented with a stack. However, we could use a priority queue that orders couples with respect to the number ofedges that have been removed from the corresponding bipartite graph.Further work will also concern the extension of this filtering procedure to the maximum common subgraph problem,which involves finding the largest graph that is subisomorphic to two given graphs. Indeed, the algorithm of Hopcroft andKarp may be used to compute the maximal matching of bipartite graphs G(u,v), thus giving a bound on the largest numberof edges that may be matched when u is matched to v.AcknowledgementsMany thanks to Yves Deville for enriching discussions, to Jean-Christophe Luquet for having implemented a first versionof ILF, to Christophe Lecoutre for guiding me in the use of Abscon, and to anonymous reviewers for their valuable remarkswho helped me improving this paper. This work was done in the context of project Sattic (Anr grant Blanc07-1_184534).References[1] Christian Bessiere, Romuald Debruyne, Theoretical analysis of singleton arc consistency and its extensions, Artif. Intell. 172 (1) (2008) 29–41.[2] L.P. Cordella, P. Foggia, C. Sansone, M. Vento, Performance evaluation of the vf graph matching algorithm, in: ICIAP ’99: Proceedings of the 10thInternational Conference on Image Analysis and Processing, IEEE Computer Society, Washington, DC, USA, 1999, p. 1172.[3] Luigi Pietro Cordella, Pasquale Foggia, Carlo Sansone, Mario Vento, An improved algorithm for matching large graphs, in: 3rd IAPR-TC15 Workshop onGraph-based Representations in Pattern Recognition, 2001, pp. 149–159.[4] Donatello Conte, Pasquale Foggia, Carlo Sansone, Mario Vento, Thirty years of graph matching in pattern recognition, IJPRAI 18 (3) (2004) 265–298.[5] Pasquale Foggia, Carlo Sansone, Mario Vento, A database of graphs for isomorphism and sub-graph isomorphism benchmarking, in: 3rd IAPR-TC15Workshop on Graph-based Representations in Pattern Recognition, 2001.[6] M. Garey, D. Johnson, Computers and Intractability, Freeman and Co., New York, 1979.[7] John E. Hopcroft, Richard M. Karp, An n5/2 algorithm for maximum matchings in bipartite graphs, SIAM J. Comput. 2 (4) (1973) 225–231.[8] P. Van Hentenryck, L. Michel, Constraint-Based Local Search, The MIT Press, 2005.[9] V. le Clément, Y. Deville, C. Solnon, Constraint-based graph matching, in: 15th Conference on Principles and Practice of Constraint Programming (CP),in: LNCS, vol. 5732, Springer, 2009, pp. 274–288.[10] C. Lecoutre, Constraint Networks: Techniques and Algorithms, ISTE/Wiley, 2009.[11] C. Lecoutre, S. Tabary, Abscon 112: towards more robustness, in: 3rd International Constraint Solver Competition (CSC’08), 2008, pp. 41–48.[12] Javier Larrosa, Gabriel Valiente, Constraint satisfaction algorithms for graph pattern matching, Mathematical Structures in Comp. Sci. 12 (4) (2002)403–422.[13] J.J. McGregor, Relational consistency algorithms and their application in finding subgraph and graph isomorphisms, Inf. Sci. 19 (3) (1979) 229–250.[14] R. Mohr, T.C. Henderson, Arc and path consistency revisited, Artif. Intell. 28 (1986) 225–233.[15] Jean-Charles Régin, Développement d’outils algorithmiques pour l’intelligence artificielle. Application à la chimie organique, PhD thesis, 1995.[16] J.-C. Régin, A filtering algorithm for constraints of difference in CSPs, in: Proc. 12th Conf. American Assoc. Artificial Intelligence, vol. 1, Amer. Assoc.Artificial Intelligence, 1994, pp. 362–367.[17] F. Rossi, P. van Beek, T. Walsh, Handbook of Constraint Programming (Foundations of Artificial Intelligence), Elsevier Science Inc., New York, NY, USA,2006.[18] J.R. Ullmann, An algorithm for subgraph isomorphism, J. ACM 23 (1) (January 1976) 31–42.[19] David L. Waltz, Generating semantic descriptions from drawings of scenes with shadows, Technical Report AI-TR-271, MIT Artificial Intelligence Labo-ratory, 1972.[20] S. Zampelli, Y. Deville, C. Solnon, Solving subgraph isomorphism problems with constraint programming, Constraints (2010), in press, doi:10.1007/s10601-009-9074-3.