Artificial Intelligence 182–183 (2012) 32–57Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintExploiting symmetries for single- and multi-agent Partially ObservableStochastic DomainsByung Kon Kang, Kee-Eung Kim∗Department of Computer Science, KAIST, 373-1 Guseong-dong, Yuseong-gu, Daejeon 305-701, Republic of Koreaa r t i c l ei n f oa b s t r a c tArticle history:Received 29 January 2010Received in revised form 26 January 2012Accepted 26 January 2012Available online 30 January 2012Keywords:POMDPPOSGSymmetryGraph automorphism1. IntroductionWhile Partially Observable Markov Decision Processes (POMDPs) and their multi-agentextension Partially Observable Stochastic Games (POSGs) provide a natural and systematicapproach to modeling sequential decision making problems under uncertainty,thecomputational complexity with which the solutions are computed is known to beprohibitively expensive.In this paper, we show how such high computational resource requirements can bealleviated through the use of symmetries present in the problem. The problem of findingthe symmetries can be cast as a graph automorphism (GA) problem on a graphicalrepresentation of the problem. We demonstrate how such symmetries can be exploited inorder to speed up the solution computation and provide computational complexity results.© 2012 Elsevier B.V. All rights reserved.Markov Decision Processes (MDPs) have been a classical mathematical framework for sequential decision making prob-lems, in which the agent must make action decisions based on environment states. The number of steps at which the agentcan make decisions can either be finite or infinite, leading to finite-horizon and infinite-horizon problems, respectively.However, although computationally tractable, MDPs have often been shown inadequate to successfully model the agent’snoisy perception of the environment state. In order to incorporate the uncertainty about the state perception inherent inthe agent, an extended formalism called Partially Observable MDPs (POMDPs) has emerged [12,34,31].POMDPs provide a model for single-agent sequential decision making under state uncertainty thus turning the decisionmaking problem into one of planning [12]. Different from MDPs, POMDPs do not provide the agent with full observabilityof the states. Instead, the agent must infer which state it is in based on the noisy observations. This results in defining aprobability distribution over the states, defined as a belief state, to represent the uncertainty of the states. With this singleextra assumption, the computational complexity of solving a POMDP problem jumps from P-complete (MDP) to PSPACE-complete even for finite-horizon POMDPs [23]. Solving infinite-horizon POMDPs is known to be undecidable [17].There has been a lot of work on alleviating this intractability by means of computing approximate solutions. One ofthe most well-known works that shows both practicality and theoretical guarantees is Point-Based Value Iteration (PBVI)by Pineau et al. [25]. PBVI proceeds by sampling reachable belief states according to various heuristics in order to avoidthe curse of dimensionality induced by the continuous nature of the belief states. The value backups are performed only onthose sampled belief states before collecting additional belief states. The main factor that determines the performance ofPBVI is the belief point selection heuristic. The heuristics used are intended to capture the reachability of the belief points,thereby avoiding unnecessary computation on unreachable beliefs. One popular heuristic used is the Greedy Error Reduction* Corresponding author. Tel.: +82 42 350 3536; fax: +82 42 350 3510.E-mail addresses: bkkang@ai.kaist.ac.kr (B.K. Kang), kekim@cs.kaist.ac.kr (K.-E. Kim).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2012.01.003B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5733heuristic, which samples belief points that result in the largest error bound. PBVI belongs to a class of algorithms calledpoint-based methods, because value computation is performed on a finite set of belief states, often called belief points.1Heuristic Search Value Iteration (HSVI) by Smith and Simmons [30] is another point-based method that approximatesthe value function via heuristic exploration of belief states. It maintains an upper- and a lower-bound for the true valuefunction, and decreases the bound gaps by recursively selecting belief states. The upper bound is initialized at the cornersof the belief simplex and is maintained as a point set. Update to this bound is performed by adding a new belief point,whose value is computed as a projection onto a convex hull formed by belief-value pairs. The lower bound is a vector set,meaning that the value is updated at the newly added belief, much like the updates performed in PBVI. The belief point tobe added is selected by a depth-first search from the initial belief.Another approach by which the intractability of the POMDP solution can be eased (in a practical manner) is to takeadvantage of the structural regularities present in POMDPs. One popular method uses the concept of homomorphism toreduce the state space itself, thereby forming an equivalent model with a potentially much smaller size. This technique,often called model minimization, has been extensively studied in the MDP domain, making use of stochastic bisimulation ofthe states. Informally, bisimilar states can be grouped together to form a smaller state space than the original MDP, yetthe optimal policies of the original and the reduced MDP are directly related with each other. The structural characteristicsthat allow for state grouping are reward equivalence and block transition equivalence. The former property states that thestates in a group should yield the same reward for any given action, and the latter that grouped states should have thesame transition probability into the group of original destination states. It is known that the optimal policy of the reducedMDP can be lifted to be converted into the optimal policy of the original MDP, hence model reduction results in lesscomputation [8,10].A different structural feature that is of interest to us is automorphism. An automorphism of a model is a homomorphismto itself. By finding the automorphisms, or symmetries, present in the model, one may expect to reduce possibly redundantcomputation performed on the symmetric portion of the solution space. It is this feature that we propose to use on POMDPsin order to reduce computational resources needed for computing optimal solutions. In particular, we are interested in thePOMDP symmetry that is not related to reducing the size of the model, but can nonetheless be exploited to speed upconventional point-based POMDP algorithms introduced above.The subject of symmetry in sequential decision making has not been carried out actively, with a few exceptions: Ravin-dran and Barto [26] were the first to extend the model minimization method to cover symmetries in MDPs. More recently,Narayanamurthy and Ravindran [20] constructively proved that the problem of finding symmetries in MDPs belongs to thecomplexity class graph isomorphism-complete (GI-complete). In this latter work, the authors use a graph-based encoding ofthe MDP to cast the problem of finding MDP symmetries to that of graph automorphism (GA). Our work is similar to this,in that we also reduce the problem to discovering GA, but provides a simpler and more intuitive approach along with apractical guide to applying symmetries to existing algorithms. We also extend the domain to multi-agent settings.Another work similar to ours is that of permutable POMDPs by Doshi and Roy [9]. This work was presented in thecontext of preference elicitation where the belief states have permutable state distribution. Similarly to the approach wepresent, this permutable POMDP framework is based on the idea that the value functions of certain classes of POMDPs arepermutable with respect to the state permutation. That is, the components of the value function can be permuted accordingto the permutation of their corresponding states while maintaining value invariance. While the overall idea is in league withour approach, there are two important differences. First, the permutable POMDP only considers a specific type of symmetrythat can be found in preference elicitation problems and models similar to them. More specifically, they show how certainpreference elicitation problems can be set up to exhibit symmetric properties. That is, they first provide certain conditionsa state permutation should satisfy and show how a preference elicitation POMDP can have its parameters set in order tosatisfy the stated conditions. As opposed to such setting, our research aims to provide an algorithmic framework with whichsymmetries can be discovered and exploited in general POMDP problems. Second, their symmetry definition requires thatthe equality condition hold for all n! permutations, where n is the number of states. This is a very strict condition, and istherefore suitable for only a very limited set of problems. On the other hand, our formulation relaxes this restriction byconsidering the state, action, and observation permutations in groups.Partially Observable Stochastic Games (POSGs) are a multi-agent extension to the POMDPs, where the actions and obser-vations now take a collective form of all agents. This change induces another leap in the complexity hierarchy: planning infinite-horizon Decentralized POMDPs (DEC-POMDPs), which is a special class of POSGs with common payoffs, is known tobe NEXP-complete [3]. Planning in infinite-horizon DEC-POMDP is again undecidable since DEC-POMDPs is a generalizationof POMDPs. Hansen et al. [11] give an exact algorithm for solving POSGs, by means of Multi-Agent Dynamic Programming(MADP). MADP performs dynamic programming backups over an extended, multi-agent belief space, which is a distributionover both the latent state and the policies of other agents. In order to keep memory usage in check, the notion of dominanceis used to prune unnecessary intermediate solutions at each iteration.In this paper, as an extended version of our previous work [13], we extend the algorithm to that of exploiting symmetriesfor POSGs as well. In particular, we will show how the notion of symmetries can be extended to a multi-agent case andhow it affects some of the game-theoretic concepts in POSGs.1 In the sequel, we use the terms “belief points” and “belief states” interchangeably.34B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–572. Formal models: POMDPs and POSGsBefore we present our main results and algorithms in detail, we first review the preliminaries of formal models for thesingle- and multi-agent sequential decision making problems in partially observable environments used in this paper. Wealso define optimal solutions for the models and representations for these solutions.2.1. POMDPsThe partially observable Markov decision process (POMDP) [12] is a model for sequential decision making problems in singleagent settings. It is a generalization of the MDP model that relaxes the assumption that the agent has complete informationabout the environment states.Formally, a POMDP is defined as a tuple (cid:3)S, A, Z , T , O , R, b0(cid:4), where• S is the set of environment states,• A is the set of actions available to the agent,• Z is the set of all possible observations,• T : S × A × S → [0, 1] is the transition function with T (s, a, s(cid:6)sfrom state s by executing action a,(cid:6)) = P (s(cid:6)|s, a) denoting the probability of changing to state• O : S × A × Z → [0, 1] is the observation function with O (s, a, z) = P (z|s, a) denoting the probability of making obser-vation z when executing action a and arriving in state s,• R : S × A → (cid:7) is the reward function where R(s, a) denotes the immediate reward received by the agent when executingaction a in state s,• b0 is the initial state distribution with b0(s) denoting the probability that the environment starts in state s.Since the agent cannot directly observe the states, it has to consider the history of its past actions and observations todecide the current action. The history at time t is defined asht = {a0, z1, a1, z2, . . . , at−1, zt}.The action is determined by a policy π , which is a function that maps from the histories to actions. For finite-horizonproblems, where we assume that the agent can execute actions for a finite time steps, the policy can be represented usinga policy tree, where each node is labeled with the action to execute, and each edge is labeled with the observation that theagent can receive at each time step. Following an observation edge, the agent faces the next level subtree, whose root nodespecifies the action to execute at the next time step. The sequence of action nodes and observation edges traversed whileexecuting the policy naturally becomes the history.The history leads to the definition of a belief state, which is the probability distribution on the states given the history ofactions and observations:bt(s) = P (st = s|ht, b0).Upon executing action at and receiving observation zt+1, the belief state bt+1 = τ (bt , at, zt+1) at the next time step iscomputed by the Bayes rule:(cid:3)(cid:2)(cid:6)s= O (sbt+1where(cid:4)(cid:6), at, zt+1)s∈S T (s, at , s(cid:6))bt(s),P (zt+1|bt, at)P (zt+1|bt, at) =(cid:2)(cid:6)sO(cid:5)s(cid:6)∈S(cid:3)(cid:5)(cid:2), at, zt+1Ts, at, s(cid:6)(cid:3)bt(s).s∈SThe belief state bt constitutes a sufficient statistic for history ht , and can be represented as an |S|-dimensional vector. Wecan thus re-define the policy as a mapping from belief states to actions.The value of a policy is the expected discounted sum of rewards by following the policy starting from a certain beliefstate. The optimal value function is the one obtained by following an optimal policy, and can be defined recursively: giventhe (t − 1)-step optimal value function, the t-step optimal value function is defined as(cid:6)V∗t (b) = maxaR(b, a) + γP (z|b, a)V∗t−1(cid:7)(cid:2)(cid:3)τ (b, a, z)(cid:5)z∈ Z(1)where R(b, a) =(cid:4)s b(s)R(s, a) and γ ∈ [0, 1) is the discount factor.B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57352.2. POSGsThe partially observable stochastic game (POSG) [3,11] is an extension of the POMDP framework to multi-agent settings.More formally, a POSG with n agents is defined as a tuple (cid:3)I, S, b0, { Ai}, {Z i}, T , O , {R i}(cid:4), where• I is the finite set of agents indexed 1, . . . , n.• S is the finite set of environment states.• b0 is the initial state distribution where b0(s) is the probability that the environment starts in state s.• Ai is the finite set of actions available to agent i. Also, the set of joint actions is specified as (cid:9)A =• Z i is the finite set of observations available to agent i. Similarly, the set of joint observations is defined as (cid:9)Z =• T is the transition function where T (s, (cid:9)a, s(cid:6)|s, (cid:9)a), the probability of resulting in state s(cid:6)) = P (si∈I Ai .(cid:8)(cid:6)(cid:8)i∈I Z i .when executing joint• O is the observation function where O (s, (cid:9)a, (cid:9)z) = P ((cid:9)z|(cid:9)a, s), the probability of making joint observation (cid:9)z when executingaction (cid:9)a in state s.joint action (cid:9)a and arriving in state s.• R i is the individual reward function where R i(s, (cid:9)a) denotes the reward (payoff) received by agent i when joint action (cid:9)ais executed in state s.If we restrict every agent to share the same individual reward function, the model becomes the Decentralized POMDP(DEC-POMDP) [3].In POSGs, each agent independently makes its own decision based on the local information available to the agent. Thelocal information at time t for agent i can be represented as the local historyhi,t = {ai,0, zi,1, ai,1, zi,2, . . . , ai,t−1, zi,t}where actions ai,∗ and observations zi,∗ are from the set Ai and Z i , respectively. The local policy (i.e., strategy) πi executedby agent i is then essentially a mapping from the local histories to local actions. A joint policy is a set of local policies foreach agent. Algorithms for POSGs find the joint policy, which is the set of local policies (cid:9)π = {π1, . . . , πn} for each agent, forsolution concepts such as Nash equilibrium or correlated equilibrium. In the case of DEC-POMDPs where the agents have tocooperate, the algorithms search for the optimal joint policy that maximizes the expected sum of rewards over the planninghorizon.The agents in POSGs have to reason about other agents’ policies as well as the true state, since they collectively affectthe rewards and the state transitions, and hence the value. This leads to the definition of multi-agent belief state, which is aprobability distribution over the hidden states and other agents’ policies [19]. Hence, while dynamic programming methodsfor POMDPs involve belief states and value vectors defined only over the system states, methods for POSGs involve multi-agent belief states and value vectors defined over the joint space of the states and other agents’ policies. Thus, for eachi of dimension |S|| (cid:9)Π−i|, where (cid:9)Π−i is the set of policies for all otherpolicy π ∈ Πi of agent i, there exists a value vector V πagents except agent i. In this paper, we focus on finite-horizon problems, and assume the local policy is represented as adecision tree.Formally, agent i’s t-step value function of executing policy π while others are executing policy (cid:9)π−i can be defined asV πi,t(s, (cid:9)π−i) = R i(s, (cid:9)a (cid:9)π ) + γO (s, (cid:9)a (cid:9)π , (cid:9)z)(cid:5)(cid:9)z∈ (cid:9)Z(cid:2)(cid:5)Ts(cid:6)∈S(cid:3)s, (cid:9)a (cid:9)π , s(cid:6)Vπ (zi )i,t−1(cid:2)(cid:3), (cid:9)π−i((cid:9)z−i)(cid:6)s(2)(cid:5)(cid:5)where (cid:9)π = {π , (cid:9)π−i} is the joint policy formed by π for agent i and (cid:9)π−i for other agents, (cid:9)a (cid:9)π is the joint action for thecurrent time step prescribed by the policy (cid:9)π , π (zi) is the (t − 1)-step local policy for agent i after observation of zi , and(cid:9)π−i((cid:9)z−i) is the (t − 1)-step joint policy for other agents after observation of (cid:9)z−i . For a given multi-agent belief state bi , theagent i’s value of executing local policy π is defined asV πi,t(bi) =bi(s, (cid:9)π−i)V πi,t(s, (cid:9)π−i).(3)s∈S(cid:9)π−i ∈ (cid:9)Π−i3. Solution methodsIn this section, we briefly review some important solution techniques for POMDPs and POSGs. There exists a wealthof literature presenting various algorithms on this matter, but in this paper, we only discuss point-based value iteration(PBVI) [25] for POMDPs and multi-agent dynamic programming (MADP) [11] for POSGs, which will be discussed in the latersections.3.1. PBVI for POMDPsThe definition of the optimal value function in Eq. (1) leads to a dynamic programming update to obtain the t-step∗t−1. The dynamic programming update could beoptimal value function Vrepresented as a backup operator H on the value functions, such that given a belief state b,from the (t − 1)-step optimal value function V∗t36B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57Table 1The PBVI algorithm.Require: B init (initial set of belief states), K (maximum number of belief state expansions), and T (maximum number of backups)B = B initΓ = {}for k = 1, . . . , K dofor t = 1, . . . , T doΓ = BACKUP(B, Γ )end forB new = EXPAND(B, Γ )B = B ∪ B newend forReturn Γ(cid:6)V t(b) = H V t−1(b) = maxaR(b, a) + γ(cid:5)z∈ ZP (z|a, b)V t−1(cid:2)(cid:3)τ (b, a, z)(cid:7).Since belief states provide a sufficient statistic for the histories, they can be treated as states in a continuous MDP, namelythe belief state MDP. One shortcoming of this approach is that the belief state is continuous, and so we cannot simply usetabular representation for value functions as in discrete state space MDPs, hence naively performing the backup operationfor every possible belief state becomes intractable. However, Sondik [31] pointed out that the value function for each horizont can be represented by a set Γt = {α0, . . . , αm} of α-vectors, so that the value at a particular belief state b is calculated as:(cid:5)V t(b) = maxα∈Γtα(s)b(s).s∈SThe construction of Γt is carried out via a series of intermediate Γ generation:(cid:11)(s) = R(s, a)(cid:5)αa,∗(cid:12)Γ a,∗=(cid:13)(cid:9)t,(cid:2)(cid:3)(cid:6)(cid:2)(cid:6)Os, a, z(cid:3)(cid:2)(cid:3)(cid:6)sαi, ∀αi ∈ Γt−1,Γ a,zt=αa,zi(s) = γTs, a, ss(cid:6)∈S(cid:10)(cid:10) αa,∗(cid:10)(cid:10)(cid:10) αa,zi(cid:14)z∈ ZΓ a,zt,Γ at+t= Γ a,∗(cid:15)Γt =Γ at ,a∈ Awhere the cross-sum operator ⊕ on sets A and B is defined as:A ⊕ B = {a + b | ∀a ∈ A, b ∈ B}.However, |Γt| can be in the order of O (| A||Γt−1|| Z |) in the worst case, leading to a very high computational cost. Thedoubly exponential growth of |Γt| in t can be alleviated by pruning dominated α-vectors for all possible belief states, butthe effect of pruning is limited in practice. This is mainly due to the fact that the backup is done over all possible beliefstates. Point-based value iteration (PBVI) [25] attempts to limit this growth by performing backups only on a finite set B ofreachable belief states. Hence, in finding Γt for V t , PBVI constructs Γ a,b, ∀a ∈ A, ∀b ∈ B, whose elements are calculated byt(cid:12)Γ a,bt=αab(cid:10)(cid:10)(cid:10) αab(s) = R(s, a) +(cid:16)(cid:5)argmaxα∈Γ a,ztz∈ Z(cid:13)(cid:17)(α · b)(s)and finally compute the best action for each belief state(cid:12)Γ Bt=α(cid:2)(cid:10)(cid:10) α = argmax∈Γ a,ba∈ A, αatbαab· b(cid:3), ∀b ∈ B(cid:13).Using Γt (or Γ Bas an approximation) for V t , the policy simply takes the form of choosing the action associated withtargmaxα∈Γt (α · b). Table 1 outlines PBVI. The BACKUP routine refers to the process of creating Γ B, described above. TheEXPAND routine characterizes the heuristic aspect of PBVI, whose task is to collect reachable belief states from the givenset B of beliefs. Heuristics for EXPAND include: greedy error reduction, where the belief states that reduce the expected errorbound are greedily chosen, and stochastic simulation with explorative action, where the belief states that mostly reduce themaximum distance among sampled belief states are greedily chosen. In later sections, we will modify the BACKUP routinein order to exploit the symmetries in POMDPs.tB.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57373.2. MADP for POSGsHansen et al. [11] propose a multi-agent dynamic programming (MADP) algorithm for POSGs. The dynamic programmingupdate in MADP consists of two stages, first enumerating t-step policies from (t − 1)-step policies and evaluating thesepolicies to obtain value functions, and then eliminating policies that are not useful for any multi-agent belief state.Note that the multi-agent value function in Eq. (3) was represented as the set of |S|| (cid:9)Π−i|-dimensional vectors. Whilethe dynamic programming methods for POMDPs, such as PBVI, involve belief states and value vectors defined only overthe environment states, the methods for POSGs involve multi-agent belief states and value vectors defined over the jointspace of environment states and other agents’ policies. Hence the dimension of value vectors will vary whenever a policy iseliminated in the second stage of dynamic programming update. A more convenient way to represent the value is to preparea value vector for each joint policy (cid:9)π ∈ (cid:9)Πt , so that the state value vectors and belief vectors be of a fixed dimension |S|:(cid:3)(cid:3) (cid:5)(cid:5)(cid:2)(cid:3)(cid:2)(cid:3)(cid:2)(cid:2)V(cid:9)πi,t(s) = R is, (cid:9)a (cid:9)π+ γOs, (cid:9)a (cid:9)π , (cid:9)zTs, (cid:9)a (cid:9)π , s(cid:6)(cid:9)π ((cid:9)z)i,t−1V(cid:6)s(cid:9)z∈ (cid:9)ZThe corresponding value function for a specific belief b ∈ [0, 1]|S|s(cid:6)∈Sis:V(cid:9)πi,t(b) =(cid:5)s∈Sb(s)V(cid:9)πi,t(s)..(4)(5)Notice that given Eq. (2), we can convert it into Eq. (4) by concatenating (cid:9)π−i and π to construct the joint policy (cid:9)π . Also,given a joint policy, a state belief vector of dimension |S| can be computed for any horizon t based on the given initialstate distribution b0 and the action/observation history up to time t. Thus, Eq. (3) can be represented as Eq. (5). We willuse Eq. (5) to represent the value for the rest of the section, for ease of exposition.Given the set (cid:9)Πt−1 = Π1,t−1 × · · · × Πi,t−1 × · · · × Πn,t−1 of (t − 1)-step joint policies and the value vectors V(cid:9)πi,t−1for all (cid:9)π ∈ (cid:9)Πt−1, the first stage of the dynamic programming update exhaustively generates Πi,t using Πi,t−1 for eachagent i, which is the set of t-step local policies for agent i. Assuming tree representations for policies, the t-step localpolicy for agent i can be created by preparing | Ai| root action nodes, and appending all possible combinations of (t − 1)-step local policies to the observation edges of the root action node. The number of exhaustively generated t-step local. Combining Πi,t for all the agents yields the set of t-step joint policies (cid:9)Πt with sizepolicies will be |Πi,t| = | Ai||Πi,t−1|| Z i ||Π1,t||Π2,t| · · · |Πi,t| · · · |Πn,t|. The first stage of dynamic programming update is concluded by computing the values of jointpolicies, Vi,t for all (cid:9)π ∈ (cid:9)Πt and agent i, using Eq. (5).(cid:9)πWith all the necessary policy backup and value computation completed, the update continues to the second stage, wherethe very weakly dominated policies are pruned. A local policy π of agent i is said to be weakly dominated if the agent doesnot decrease its value by switching to some other local policy while all others maintain their own local policies, and thereexists at least one (cid:9)π−i ∈ (cid:9)Π−i,t such that switching away from π strictly increases agent i’s value. A very weakly dominatedpolicy is one where the weak dominance relation holds without the existence requirement of the strict improvement in thevalue. The test for very weak dominance of a local policy π of agent i can be determined by checking the existence of aprobability distribution p on other policies Πi,t\π such that(cid:5)π (cid:6)∈Πi,t \π(cid:3)(cid:2)pπ (cid:6){π (cid:6), (cid:9)π−i }i,tV(s) (cid:2) V{π , (cid:9)π−i }i,t(s), ∀s ∈ S, ∀ (cid:9)π−i ∈ (cid:9)Π−i,t,(6){π , (cid:9)π−i }i,tis the value vector of the joint policy formed by π for agent i and (cid:9)π−i for other agents. If there existswhere Vsuch a distribution, then π is prunable since it is possible for agent i to take a stochastic policy determined by p, whileachieving the value no worse than that of π . This test for dominance is carried out by linear programming (LP). A veryweakly dominated policy can thus be safely pruned without any concern for the loss in value. The pruning proceeds inan iterated fashion where each agent alternately tests for dominance and prunes accordingly. This iteration stops when noagent can prune any more local policies.Table 2 outlines the MADP algorithm for computing the set of T -step joint policies. Note that this algorithm requiresadditional computation to select the joint policy depending on the solution concept such as Nash equilibrium. For DEC-POMDPs that assume cooperative settings, a joint policy with the maximum value for the initial state distribution b0 isselected as an optimal joint policy.4. Symmetries in POMDPs and POSGsIn this section, we show how symmetries are defined in POMDPs and POSGs. We show that finding symmetries forbotch cases is a graph isomorphism complete (GI-complete) problem – the complexity class of finding automorphisms ingeneral graphs. We present the graph encoding of a given POMDP and POSG in order to apply algorithms for finding graphautomorphisms. We also describe how POMDP and POSG algorithms can be extended to exploit the symmetries discoveredin the models.38B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57Table 2The MADP algorithm.Require: Πi,0 = ∅ and Vi,0 = {(cid:9)0} (initial value function) for each agent i.for t = 1, . . . , T do# The first stage of dynamic programming backupfor i = 1, . . . , n doPerform backup on (t − 1)-step local policies Πi,t−1 to produce the exhaustive set of t-step local policies Πi,t .end forLet (cid:9)Πt = Π1,t × · · · × Πi,t × · · · × Πn,t .for all (cid:9)π ∈ (cid:9)Πt doCompute V(cid:9)πi,t (Eq. (5)) and add the value vector to Vi,t .end for# The second stage of dynamic programming backuprepeatfor i = 1, . . . , n dofor all π ∈ Πi,t doPrune π if very weakly dominated (Eq. (6))end forend foruntil no local policy was pruned in the loopend forreturn Sets of T -step policies Πi,T and corresponding value vectors Vi,T for each agent i4.1. Definition of symmetries in POMDPsThere have been a number of works in the past to take advantage of the underlying structure in decision theoretic plan-ning models. Perhaps one of the most extensively studied types of structural regularities would be that of homomorphism.It is directly related to abstraction and model minimization techniques that try to reduce the size of the model.(cid:6)maps the actions, and φ Z : Z → ZA homomorphism φ of a POMDP is defined as (cid:3)φS , φ A, φ Z (cid:4) where φS : S → S(cid:6)is the function that maps the states, φ A :(cid:6)(cid:4)(cid:6) = (cid:3)SA → Ais a reduced model of M if any of the mappings is many-to-one. Because of this property, model minimization methods forPOMDPs search for a homomorphism φ that maps M to an equivalent POMDP Mwith the minimal model size. Dependingon the definition of homomorphism φ, we obtain different definitions of the minimal model.maps the observations. Note that the mapped POMDP M(cid:6), O(cid:6), A(cid:6), R(cid:6), Z(cid:6), TA simple extension of MDP model minimization [10] to POMDPs leads to a homomorphism φ of form (cid:3)φS , 1, 1(cid:4), where 1, φS should satisfy the following constraints:denotes the identity mapping. In order to hold equivalence between M and M(cid:2)(cid:5)(cid:3)(cid:3)(cid:2)(cid:2)(cid:3)(cid:6)(cid:6)(cid:6)(cid:6)T(cid:6)R(cid:6)OφS (s), a, φSs(cid:6)=Ts, a, ss(cid:6)(cid:6)∈φ−1S(s(cid:6))(cid:6)(cid:6),(cid:2)(cid:3)φS (s), a(cid:2)= R(s, a),(cid:3)φS (s), a, z= O (s, a, z).Pineau et al. [24] extend the approach to the case when a task hierarchy is given by an expert, and they achieve a furtherreduction in the state space since some of the actions become irrelevant under the task hierarchy.Wolfe [36] extends the minimization method to compute homomorphism of a more general form (cid:3)φS , φ A, φ Z (cid:4) where theobservation mapping φ Z can change depending on the action. The constraints for the equivalence are given by:(cid:2)(cid:6)TφS (s), φ A(a), φS(cid:3)(cid:3)(cid:2)(cid:6)s=(cid:5)(cid:2)Ts, a, s(cid:3)(cid:6)(cid:6),s(cid:6)(cid:6)∈φ−1S(s(cid:6))(cid:2)(cid:3)φS (s), φ A(a)(cid:2)φS (s), φ A(a), φa= R(s, a),(cid:3)Z (z)(cid:6)R(cid:6)O= O (s, a, z).Note that the above methods are interested in finding many-to-one mappings in order to find a model with reduced size.Hence, they focus on computing partitions of the state, action, and observation spaces of which blocks represent aggregatesof equivalent states, actions, and observations, respectively. Once the partitions are found, we can employ conventionalPOMDP algorithms on the abstract POMDP with reduced number of states, actions, or observations, which in effect reducesthe computational complexities of algorithms.In this paper, we are interested in automorphism, which is a special class of homomorphism:Definition 1. An automorphism φ is defined as (cid:3)φS , φ A, φ Z (cid:4) where the state mapping φS : S → S, the action mappingφ A : A → A, and the observation mapping φ Z : Z → Z are all one-to-one mappings satisfying:B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5739(cid:6) =ss = sLEFTs = sRIGHTT (s, a,LISTEN, s(cid:6))T (s, aLEFT, s(cid:6))T (s, aRIGHT, s(cid:6))sLEFTsRIGHTsLEFTsRIGHTsLEFTsRIGHT1.00.00.01.00.50.50.50.50.50.50.50.5Fig. 1. Transition probabilities of the tiger domain.O (s, aLISTEN, z)O (s, aLEFT, z)O (s, aRIGHT, z)z =s = sLEFTs = sRIGHTzLEFT0.850.15zRIGHTzLEFTzRIGHTzLEFTzRIGHT0.150.850.50.50.50.50.50.50.50.5Fig. 2. Observation probabilities of the tiger domain.a =s = sLEFTs = sRIGHTR(s, a)aLISTEN−1−1aLEFT−10010aRIGHT10−100Fig. 3. Reward function of the tiger domain.(cid:3)(cid:3)(cid:2)Ts, a, s(cid:3)(cid:6)= TO (s, a, z) = O(cid:6)(cid:2)(cid:2)sφS (s), φ A(a), φS(cid:2)(cid:3)φS (s), φ A(a), φ Z (z)(cid:2)(cid:3)φS (s), φ A(a).,,R(s, a) = RHence, φ maps the original POMDP to itself, and there is no assumption regarding the reduction in the size of the model.The classic tiger domain [12] is perhaps one of the best examples to describe automorphisms in POMDPs. The statespace S of the tiger domain is defined as {sLEFT, sRIGHT}, representing the state of the world when the tiger is behind theleft door or the right door, respectively. The action space A is defined as {aLEFT, aRIGHT, aLISTEN}, representing actions foropening the left door, opening the right door, or listening, respectively. The observation space Z is defined as {zLEFT, zRIGHT}representing hearing the sound of the tiger from the left door or the right door, respectively. The specifications of transitionprobabilities, observation probabilities, and the rewards are as given in Figs. 1, 2, and 3. The initial belief is given asb0(sLEFT) = b0(sRIGHT) = 0.5.Note that the tiger domain is already compact in the sense that minimization methods previously mentioned cannotreduce the size of the model: examining the reward function alone, we cannot aggregate aLEFT and aRIGHT since the rewardsare different depending on the current state being either sLEFT or sRIGHT. By a similar argument, we cannot reduce the statespace nor the observation space.However, sLEFT and sRIGHT can be interchanged to yield an equivalent POMDP, while simultaneously changing the corre-sponding actions and observations:(cid:12)φS (s) =(cid:18)if s = sLEFT,if s = sRIGHT,sRIGHTsLEFTaLISTEN if a = aLISTEN,aRIGHTaLEFTif a = aLEFT,if a = aRIGHT,if z = zLEFT,if z = zRIGHT.φ A(a) =(cid:12)φ Z (z) =zRIGHTzLEFTFurthermore, this property yields symmetries in the belief states and α-vectors in the tiger domain, as can be seenin Fig. 4.The automorphism in POMDPs is the type of regularity we intend to discover and exploit in this paper: the symmetryin the model that does not necessarily help the model minimization algorithm further reduce the size of the model. Hence,rather than computing partitions, we focus on computing all possible automorphisms of the original POMDP.Note that if the original POMDP can be reduced in size, we can have exponentially many automorphisms in the numberof blocks in the partition. For example, if the model minimization yields a state partition with K blocks of 2 states each,the number of automorphisms becomes 2K . Hence, it is advisable to compute automorphism after we compute the minimalmodel of POMDP.40B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57Fig. 4. Value function of tiger domain obtained by PBVI with 5 belief states. b1 and b5 are symmetric, hence the corresponding α-vectors α1 and α5 aresymmetric. The same argument applies to b2 and b4. Although the illustration uses an approximate value function computed by PBVI, the value functionfrom exact methods will show the same phenomenon.4.2. Properties of symmetries in POMDPsAs shown in the tiger domain, the automorphisms of POMDPs reveal the symmetries present in belief states and α-vectors; given a POMDP M with automorphism φ = (cid:3)φS , φ A, φ Z (cid:4), let Γ ∗be the set of α-vectors for the optimal valuefunction. In this setting, we provide the following two theorems that can be exploited when computing a solution to agiven POMDP.By a slight abuse of notation, for a vector v of dimension |S|, we let φS (v) be the transformed vector whose elementsare permuted by φS .Theorem 1. If b is a reachable belief state, then φS (b) is also a reachable belief state.Proof. First, given φ = (cid:3)φS , φ A, φ Z (cid:4), note thatz(s) = bbaφ A (a)φZ (z)(cid:2)(cid:3)φS (s),(cid:6))) and O (s, a, z) = O (φS (s), φ A(a), φ Z (z)). This(cid:6)) = T (φS (s), φ A(a), φS (sbecause the automorphism ensures that T (s, a, smeans that the symmetric image of a reachable belief vector b, that is, φS (b), is also reachable from the initial beliefb0 by executing a “symmetric policy”, where the action a is mapped to φ A(a).In other words, if b is reachable from initial belief state b0 by executing a policy tree, φS (b) can also be reached byexecuting the policy tree where action nodes are relabeled using φ A(a) and the observation edges are relabeled usingφ Z (z). (cid:2)Theorem 2. If α ∈ Γ ∗, then φS (α) ∈ Γ ∗.Proof. We prove by induction on horizon t in Γt . By the definition of automorphism, R(s, a) = R(φS (s), φ A(a)). Hence, ifα ∈ Γ0 then φS (α) ∈ Γ0.Suppose that the argument holds for Γt−1. This implies that ∀α ∈ Γ a,z, φS (α) ∈ Γ φ A (a),φ Z (z)ttby the definition of Γ a,z. Iftα ∈ Γt , then by definition, for some a and b,(cid:2)(cid:5)α(s) = αab(s) = R(s, a) +α(cid:6) · b(cid:3).argmaxα(cid:6)∈Γ a,ztz∈ ZConsider its symmetric image defined as(cid:2)(cid:2)(cid:5)(cid:3)φS (s)α= R(cid:3)φS (s), φ A(a)+argmaxφZ (z)∈ Zφ A (a),φZ (z)α(cid:6)(cid:6)∈Γt(cid:2)(cid:3)α(cid:6)(cid:6) · φS (b).For each observation φ Z (z), the argmax will select α(cid:6)(cid:6)b). Hence we have φS (α) ∈ Γt . (cid:2)which is the symmetric image of α(cid:6)selected in the argmaxα(cid:6)∈Γ a,zt(α(cid:6) ·In this work, we specialize the PBVI algorithm to exploit symmetries, as will be shown in later sections. However,the theorems we provide are general enough to be applied to a variety of different value function-based algorithms. WeB.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5741Joint action{a1,LISTEN, a2,LISTEN}OthersLEFT → sRIGHTsLEFT → sLEFTsRIGHT → sLEFTsRIGHT → sRIGHT00.510.500.510.5Fig. 5. State transition probabilities of the Dec-Tiger domain. The second row shows the transition probabilities of all joint actions composed of at least onenon-listen individual action.argue so, because the unifying theme of all value function-based algorithms is the dependence on α-vectors and/or beliefpoints, and the two theorems we presented indicate that the symmetric images of the sampled belief points and α-vectorscontribute equivalently to the overall value. For example, the randomized point-based backup of Perseus [32] can benefitfrom our results by not having to perform redundant backup operation on symmetric beliefs. Symmetries can be exploitedin search based methods such as HSVI or Forward Search Value Iteration (FSVI) [29] in a similar manner. In particular,multiple backups can be performed for a single sampled belief point by taking the symmetric image of that sampled belief.The gist is that, while different value function-based methods provide different sampling approaches, our framework can beuniversally applied to enhance the sampling procedure.4.3. Definition of symmetries in POSGsExtending the definition to POSGs introduces agent-to-agent mappings, where the local actions and observations of anagent are mapped to those of another agent. Formally, the automorphism for POSGs is defined as follows:Definition 2. An automorphism φ for agent i on a POSG is a tuple (cid:3)φI , φS , φ (cid:9)A, φ (cid:9)ZI}, where agent mapping φI : I → I , state mapping φS : S → S, action mappings φ Aiφ Z i: Z i → ZφI (i) are all bijections satisfying(cid:3)(cid:3)(cid:2)(cid:2)(cid:3)(cid:2)(cid:4) with φ (cid:9)A= {φ Ai| i ∈| i ∈ I} and φ (cid:9)Z: Ai → AφI (i), and observation mappings= {φ Z i(cid:6)s, (cid:9)a, sT= TO (s, (cid:9)a, (cid:9)z) = OR i(s, (cid:9)a) = RφI (i), (cid:9)a, and (cid:9)z.(cid:6)(cid:6)φS (s), φ (cid:9)A((cid:9)a), φSs(cid:2)(cid:3)φS (s), φ (cid:9)A((cid:9)a), φ (cid:9)Z ((cid:9)z)(cid:2)(cid:3)φS (s), φ (cid:9)A((cid:9)a),,for all s, sA special case when agent mapping φIis an identity mapping, φ is said to be an intra-agent automorphism. On theother hand, if φI is a non-identity mapping, it is said to be an inter-agent automorphism. Informally speaking, inter-agentautomorphism allows interchanging agents as long as the local actions and observations are interchanged accordingly. Onthe other hand, intra-agent automorphism is confined to interchanging the local actions and observations within an agent.It can be thought that intra-agent automorphism captures the symmetry present in the single-agent POMDP level, while theinter-agent automorphism extends the symmetry to the multi-agent level.To illustrate, we present the decentralized tiger (Dec-Tiger) domain [19]. Dec-Tiger is a multi-agent extension to theclassical tiger domain. There are now two agents, setting the agent set I = {1, 2}, that must make a sequence of decisionsas to whether they should open the door (jointly or separately) or listen. The states are the same as the tiger domain: sLEFTand sRIGHT. Each agent has the same set of actions that are equivalent to the single agent case: {ai,LISTEN, ai,RIGHT, ai,LEFT | i =1 or 2}, where ai, X indicates the action X of agent i. The observation space is duplicated from the single-agent case as well:{zi,LEFT, zi,RIGHT | i = 1 or 2}, with the notations defined similarly.If at least one agent performs an open action, the state resets to either one with 0.5 probability. If both continue with alisten action, then there is no change of state.Each agent individually observes the tiger from the correct room with probability 0.85 when performing a listen action.When both agents perform a joint listen action, then the resulting joint observation probability is computed as a product ofthe individual probabilities. All other joint actions where at least one agent performs a non-listen action result in a uniformdistribution over the joint observations.Rewards are given equally to both agents, and are designed to encourage cooperation. The maximum reward can beattained by cooperatively opening the tiger-free room. If each agent chooses a different room, then a high penalty is given.If they cooperatively open the tiger room, then they still suffer a penalty, but at a much lesser cost. Jointly listening costsa small penalty, whereas opening the tiger-free room while the other agent listens will result in a small reward. If, on theother hand, one agent opens the tiger room while the other is listening, then they incur the worst possible penalty. Thetransition probabilities, observation probabilities, and rewards are summarized in Figs. 5, 6, and 7, respectively.One possible symmetry that exhibits an inter-agent mapping is presented in Fig. 8. For the complete list of symmetriesin Dec-Tiger, we invite the reader to consult Fig. 15 in Section 7 where we report experimental results.42B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57Joint observation{z1,LEFT, z2,LEFT}{z1,LEFT, z2,RIGHT}{z1,RIGHT, z2,LEFT}{z1,RIGHT, z2,RIGHT}sLEFT0.72250.12750.12750.0225sRIGHT0.02250.12750.12750.7225Fig. 6. Observation probabilities of the Dec-Tiger domain for joint action {a1,LISTEN, a2,LISTEN}. The probabilities for other joint actions are uniform, and areomitted.Joint action{a1,RIGHT, a2,RIGHT}{a1,LEFT, a2,LEFT}{a1,RIGHT, a2,LEFT}{a1,LEFT, a2,RIGHT}{a1,LISTEN, a2,LISTEN}{a1,LISTEN, a2,RIGHT}{a1,RIGHT, a2,LISTEN}{a1,LISTEN, a2,LEFT}{a1,LEFT, a2,LISTEN}sLEFTsRIGHT20, 200, 0−100, −100−100, −100−2, −29, 99, 9−101, −101−101, −1010, 020, 20−100, −100−100, −100−2, −2−101, −101−101, −1019, 99, 9Fig. 7. Individual rewards of the Dec-Tiger domain.(cid:12)φI (i) =Agent 2 if i is Agent 1Agent 1 if i is Agent 2⎧⎨φS (s) = Identity mappingφ A1 (a) =φ A2 (a) =⎩⎧⎨⎩(cid:12)φZ1 (z) =(cid:12)φZ2 (z) =a2,LISTEN if a = a1,LISTENif a = a1,RIGHTa2,RIGHTif a = a1,LEFTa2,LEFTa1,LISTEN if a = a2,LISTENif a = a2,RIGHTa1,RIGHTif a = a2,LEFTa1,LEFTif z = z1,LEFTif z = z1,RIGHTif z = z2,LEFTif z = z2,RIGHTz2,RIGHTz2,LEFTz1,RIGHTz1,LEFTFig. 8. An example of an inter-agent symmetry for Dec-Tiger.4.4. Properties of symmetries in POSGsAs with the case with POMDPs, the symmetries in POSGs reveal useful regularities present in the model. In this section,we formally state the properties of symmetries in POSGs, which will be used to extend MADP in the later sections. Again,with a slight abuse of notation, we extend the domain of φ to local and joint policy trees, the output of which is anotherpolicy tree with all the actions and observations permuted accordingly. That is, φ(π ) for any policy tree π is a permutedpolicy tree whose action nodes have been mapped by π A and the observation edges have been permuted by π Z .Theorem 3. Given an automorphism φ = (cid:3)φI , φS , φ (cid:9)A, φ (cid:9)Z(cid:4),V(cid:9)πi,t(s) = Vφ( (cid:9)π )φI (i),t(cid:2)(cid:3)φS (s)for all s at all time steps 1 (cid:3) t (cid:3) T .Proof. We prove by induction on t. For t = 1, only the immediate reward matters:V(cid:9)πi,1(s) = R i(s, (cid:9)a) = RφI (i)(cid:2)(cid:3)φS (s), φ (cid:9)A((cid:9)a)= Vφ( (cid:9)π )φI (i),1(cid:2)(cid:3)φS (s).The first and last equalities follow from the fact that a 1-step policy tree is simply a single action node. The second equalityholds by the definition of automorphism.Assume that the theorem holds for all t’s up to t = k − 1 (i.e. for policy trees of depth k − 1). For t = k, the Bellmanequation unfolds asB.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57R i(s, (cid:9)a) + γ(cid:5)(cid:2)Ts(cid:6)∈S,(cid:9)z∈ (cid:9)Z(cid:2)(cid:3)(cid:6)O(cid:6)s, (cid:9)a, (cid:9)z(cid:3)s, (cid:9)a, s(cid:9)π ((cid:9)z)i,k−1V(cid:3)(cid:2)(cid:6)s= RφI (i)(cid:2)(cid:3)φS (s), φ (cid:9)A((cid:9)a)+ γ⎛⎜⎝(cid:5)(cid:6))∈SφS (sφ (cid:9)Z ((cid:9)z)∈ (cid:9)Z43⎞⎟⎠ .T (φS (s), φ (cid:9)A((cid:9)a), φS (s·O (φS (s·V(cid:6)))(cid:6)), φ (cid:9)A((cid:9)a), φ (cid:9)Z ((cid:9)z))(cid:6)))(φS (sφ( (cid:9)π (φ (cid:9)Z ((cid:9)z)))φI (i),k−1All the terms except the V (·) can be shown equal by the definition of automorphism. The equality of the next-step valueterm is established by the inductive hypothesis, since the subtrees (all of which are (k − 1)-level subtrees) encountered byfollowing (cid:9)z in (cid:9)π are symmetric to the ones encountered by following φ (cid:9)Z ((cid:9)z) in φ( (cid:9)π ). Therefore, the equality holds for allt (cid:2) 1. (cid:2)Because Theorem 3 holds for all values of t, we will henceforth drop the horizon superscript t whenever possible.Based on the above theorem, we can make the following statement regarding very weak dominance under the presence ofsymmetries:Theorem 4. If the local policy π of agent i is very weakly dominated, then the local policy φ(π ) of agent φI (i) is also very weaklydominated for any automorphism φ.Proof. From Eq. (6), the local policy π of agent i is very weakly dominated if there exists a probability distribution p onother local policies Πi\π such that(cid:2)π (cid:6)(s), ∀s, ∀ (cid:9)π−i ∈ (cid:9)Π−i.(s) (cid:2) V(cid:5)Vp(cid:3){π (cid:6), (cid:9)π−i }i{π , (cid:9)π−i }iπ (cid:6)∈Πi \πConsider the local policy φ(π ) of agent φI (i). In order to prove that φ(π ) is very weakly dominated, we have to show thatthere exists a probability distribution p{π (cid:6)(cid:6), (cid:9)π−φI (i)}φI (i)on agent φI (i)’s other local policies ΠφI (i)\φ(π ) such that(s), ∀s, ∀ (cid:9)π−φI (i) ∈ (cid:9)Π−φI (i).{φ(π ), (cid:9)π−φI (i)}φI (i)(s) (cid:2) Vπ (cid:6)(cid:6)(cid:5)Vp(cid:3)(cid:2)(cid:6)(cid:6)π (cid:6)(cid:6)∈ΠφI (i)\φ(π )Note that the local policy π (cid:6)ΠφI (i)\φ(π ), we can always find π (cid:6) ∈ Πi\π such that π (cid:6)(cid:6) = φ(π (cid:6)) since φ is bijective. If we set pπ (cid:6)(cid:6) = φ(π (cid:6)), we have found a probability distribution pof agent i corresponds to the local policy φ(π (cid:6)) of agent φI (i). Hence for each π (cid:6)(cid:6) ∈(cid:6)(π (cid:6)(cid:6)) = p(π (cid:6)) wherethat satisfies the above inequality. (cid:2)(cid:6)From Theorem 4, it follows that a policy tree and all of its symmetric images can be pruned without loss in the value ifany of them is known to be very weakly dominated:Corollary 1. If a policy π can be pruned, then φ(π ) can be pruned as well.As in the case of POMDPs, we adopt MADP to demonstrate the usefulness of symmetries in POSGs. While this approachmay seem algorithm-specific, we argue that the theoretical basis on which such exploitations are made is general enoughto be applied to other algorithms as well.For example, there has been much significant work on solving DEC-POMDPs in recent years, including Bounded PolicyIteration (BPI) [4], Memory-Bounded Dynamic Programming (MBDP) [28], Heuristic Policy Iteration (HPI) [2], Point-BasedBounded Policy Iteration (PB-BPI) [14], Point-Based Policy Generation (PBPG) [37], Constraint Based Policy Backup (CBPB)and Team Decision problem based Policy Iteration (TDPI) [15]. These algorithms often share common computational steps,such as exhaustive or partial dynamic programming backup of policies, pruning dominated policies and improving policiesusing mathematical programming. The theoretical results above can be used to reduce the number of policies generatedby the dynamic programming backup, as well as the number of mathematical programs to solve. We can also apply recentresults on exploiting symmetries to reduce the sizes of mathematical programs themselves [5], but the details are left forfuture work.The symmetries also have various impacts on the game theoretic analysis of the given POSG. To facilitate our discussion,we will convert the given POSG to a normal form game. We will also adhere to the term “policy” for the sake of consistency,although “strategy” is more widely adopted in game theory. As pointed out by Hansen et al. [11], a POSG at time horizont can be converted to a normal form game by enlisting all the policy trees as possible actions. We also include the initialstate distribution in order to have scalar payoffs rather than |S|-dimensional vector payoffs. This is done by taking the inner(cid:9)πi and the initial state distribution b0. This inner product will become the payoff entry intoproduct of each value vector Vour converted game.We denote the payoff of a joint policy (cid:9)π for agent i as ui( (cid:9)π ), or equivalently, ui({πi, (cid:9)π−i}). It follows that ui( (cid:9)π ) =uφI (i)(φ( (cid:9)π )), due to Eq. (7).(cid:5)(cid:5)b0(s)Vs(cid:9)πi (s) =s(cid:2)(cid:3)φS (s)b0Vφ( (cid:9)π )φI (i)(cid:2)(cid:3)φS (s).(7)44B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57For our discussion on symmetries for game theoretic solution concepts, we begin with the Nash equilibrium. A (pure-strategy) Nash equilibrium is a joint policy such that for any fixed agent, that agent has no incentive to unilaterally switchits policy provided that others do not change theirs.Proposition 1. If a joint policy (cid:9)π is a Nash equilibrium in a normal form representation of the given POSG, then its symmetric imageφ( (cid:9)π ) also constitutes a Nash equilibrium.Proof. Given a Nash equilibrium (cid:9)π ∗, the following inequality holds by the definition:(cid:11)(cid:3)(cid:11)(cid:3)(cid:2)(cid:9)(cid:2)(cid:9)uii , (cid:9)π ∗π ∗−i(cid:2) uiπi, (cid:9)π ∗−i, ∀i, πi (cid:13)= π ∗i .The automorphism guarantees ui( (cid:9)π ) = uφI (i)(φ( (cid:9)π )), for any joint policy (cid:9)π . Therefore, uφI (i)(φ( (cid:9)π ∗)) (cid:2) uφI (i)({φ(πi), φ( (cid:9)π ∗∀i, πi (cid:13)= π ∗i , which establishes the fact that φ( (cid:9)π ∗) is a Nash equilibrium as well. (cid:2)−i)}),Proposition 1 easily generalizes to mixed-strategy Nash equilibrium. Note that our notion of symmetries generalize thedefinition used in classical symmetric games, which requires that there exists an invariant action mapping φ A and observationmapping φ Z for all possible permutations of agents. Our theoretical results could be used in making game solvers morescalable, widening the applicability of the techniques by Cheng et al. [7]. The facts presented in this section lead to a moreefficient procedure for finding the equilibria of symmetric POSGs. Instead of searching for every single equilibrium presentin POSGs, we can speed up the process by applying the symmetries of the POSGs to the equilibria that have already beendiscovered.The correlated equilibrium (CE) [21] generalizes the mixed-strategy Nash equilibrium. Whereas the mixed-strategy Nashequilibrium is defined to be an independent probability over the local policies, the CE is a probability over the joint policiesallowing for the dependencies among agents’ local policies. That is, the probability p over the joint policies is a CE if(cid:5)(cid:9)π−ip( (cid:9)π )ui( (cid:9)π ) (cid:2)(cid:5)(cid:9)π−ip( (cid:9)π )ui(cid:2)(cid:9)π (cid:6)i , (cid:9)π−i(cid:11)(cid:3), ∀i and ∀π (cid:6)i(cid:13)= πi.(8)With symmetries present in the normal form game representation of the POSG, we can prove a symmetric property of a CE.Proposition 2. Let p be a CE of the normal form representation of a given POSG. Then there exists a (possibly same) CE p(cid:6)(φ( (cid:9)π )) = p( (cid:9)π ) for any automorphism φ of the given POSG, and any joint policy (cid:9)π .p(cid:6)such thatProof. Given a CE p, we can re-write Eq. (8) as(cid:2)(cid:5)(cid:5)p( (cid:9)π )uφI (i)(cid:3)φ( (cid:9)π )(cid:2)p( (cid:9)π )uφI (i)(cid:2)(cid:9)(cid:2)φ(cid:3)π (cid:6)i(cid:11)(cid:3), φ( (cid:9)π−i), ∀i, ∀π (cid:6)i(cid:13)= πi.(cid:9)π−φI (i)Note that since π (cid:6)iprobability with which φ( (cid:9)π ) is chosen. Therefore, there exists a CE that assigns probability p( (cid:9)π ) to φ( (cid:9)π ). (cid:2)i ) (cid:13)= φ(πi) due to φ being bijective. This modified form states that p( (cid:9)π ) can also be used as a(cid:13)= πi , φ(π (cid:6)(cid:9)π−φI (i)5. Symmetry discovery in the modelsIn this section, we show that finding the symmetries present in POMDPs and POSGs is a graph isomorphism (GI) com-plete problem, the computational complexity class of finding the automorphism groups of general graphs. We thus presentthe graph encoding of a given POMDP and POSG in order to use a graph automorphism algorithm for finding symmetries inthe model.5.1. Graph encoding of a POMDPWe first describe how we can cast the problem of finding automorphisms in POMDPs as that of finding automorphisms ingraphs. Specifically, we will show how we can encode a given POMDP as a vertex-colored graph, so that the automorphismfound in the graph corresponds to the automorphism in the POMDP. Our approach here will prove useful when we discussthe computational complexity of discovering POMDP automorphisms in the later part of this section.A vertex-colored graph G is specified by (cid:3)V , E, C, ψ(cid:4), where V denotes the set of vertices, E denotes the set of edges(cid:3)v i, v j(cid:4), C is the set of colors, and ψ : V → C denotes the color associated with each vertex. An automorphism φ : V → V isa permutation of V with the property that for any edge (cid:3)v i, v j(cid:4) ∈ E, (cid:3)φ(v i), φ(v j)(cid:4) is also in E, and for any vertex v i ∈ V ,ψ(v i) = ψ(φ(v i)).We can encode a POMDP as a vertex-colored graph in order to apply graph automorphism algorithms. The encodedgraph is composed of the following classes of vertices and edges, their counts being presented in parentheses:B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5745Fig. 9. Encoding the tiger domain as a vertex-colored graph. Two vertices have the same color if and only if their shapes and fillings are the same.• States (|S| vertices): for every state s, we prepare vertex v s and make every vertex share the same unique color: ∀s ∈ S,ψ(v s) = cstate.∀a ∈ A, ψ(va) = caction.• Actions (| A| vertices): for every action a, we prepare vertex va and make every vertex share the same unique color:• Next states (|S| vertices and |S| edges): for every state s(cid:6)unique color: ∀s(cid:6) ∈ S, ψ(v s(cid:6) ) = cstate(cid:6) . We connect the next-state vertex v s(cid:6) to the state vertex v s if and only if s(cid:6) = s.• Observations (| Z | vertices): for every observation z, we prepare vertex v z and make every vertex share the same unique, we prepare vertex v s(cid:6) and make every vertex share the samecolor: ∀z ∈ Z , ψ(v z) = cobs.(cid:6), z(cid:6), a(cid:6)(cid:6), a(cid:6)), ∀(s(cid:6)) = T (s• Transition probabilities (|S|2| A| vertices and 3|S|2| A| edges): for every triplet (s, a, s(cid:6)), we prepare vertex v T (s,a,s(cid:6)) that(cid:6)) and assign colors so that two vertices share the same color if and only ifrepresents the transition probability T (s, a, s(cid:6)(cid:6)(cid:6)).(cid:6)(cid:6)(cid:6)), ψ(v T (s,a,s(cid:6))) = ψ(v T (s(cid:6)(cid:6),a(cid:6),s(cid:6)(cid:6))) iff T (s, a, s(cid:6), sthe transition probabilities are the same: ∀(s, a, sWe connect the transition probability vertex v T (s,a,s(cid:6)) to the corresponding state, action, and next-state vertices, v s, vaand v s(cid:6) .• Observation probabilities (|S|| A|| Z | vertices and 3|S|| A|| Z | edges): for every triplet (s, a, z), we prepare vertex v O (s,a,z)that represents the observation probability O (s, a, z) and assign colors so that two vertices share the same color if(cid:6)), ψ(v O (s,a,z)) = ψ(v O (s(cid:6),a(cid:6),z(cid:6))) iff O (s, a, z) =and only if the observation probabilities are the same: ∀(s, a, z), ∀(s(cid:6)). We connect the observation probability vertex v O (s,a,z) to the corresponding state, action, and observationO (svertices, v s, va and v z.• Reward function (|S|| A| vertices and 2|S|| A| edges): for every pair (s, a), we prepare vertex v R(s,a) that represents thereward R(s, a) and assign colors so that two vertices share the same color if and only if the rewards are the same:(cid:6)). We connect the reward vertex v R(s,a) to the corresponding∀(s, a), ∀(sstate and action vertices, v s and va.(cid:6)), ψ(v R(s,a)) = ψ(v R(s(cid:6),a(cid:6))) iff R(s, a) = R(s• Initial state distribution (|S| vertices and |S| edges): for every state s, we prepare vertex vb0(s) that represents theinitial state probability b0(s) and assign colors so that two vertices share the same color if and only if the initial state(cid:6)). We connect the initial state probability vertexprobabilities are the same: ∀s, ∀svb0(s) to the corresponding state vertex v s., ψ(vb0(s)) = ψ(vb0(s(cid:6))) iff b0(s) = b0(s(cid:6)(cid:6), a(cid:6), a(cid:6), a(cid:6), a(cid:6), z(cid:6), s(cid:6)The graph encoding process is mechanical, and the colors and edges are carefully prepared in order to preserve the equiv-alence of the model under any graph automorphism. Fig. 9 shows the result of the graph encoding process for the tigerdomain.The encoded graph is sparse, consisting of O (|S|2| A|| Z |) vertices and O (|S|2| A|| Z |) edges, hence the number of edgesis linear in the number of vertices. Despite super-polynomial running time in the worst case, typical graph automorphismsolvers are efficient for sparse graphs. As we report in Section 7, we used nauty [18] for the graph automorphism solver,and it quickly found automorphisms in the encoded graphs of benchmark POMDP domains with up to 6 × 107 vertices.As a minor remark, note that we choose the colors such that ψ(v T (s,a,s(cid:6))) (cid:13)= ψ(v O (s,a,z)) even if T (s, a, s(cid:6)) = O (s, a, z).This is to prevent the transition probability being permuted with observation probability vertices. Similar restrictions applyto all other vertices of different classes.46B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–575.2. Graph encoding of a POSGSimilar to the POMDP case, the problem of finding POSG automorphisms can be reduced to finding the automorphismgroup of a properly encoded graph. The graph encoding we use here is not so much different from the POMDP approach,with the exception of the vertices that reflect the multi-agent aspects.The encoded graph is composed of the following classes of vertices and their edges:• Agents (|I| vertices): we prepare one vertex per agent, assigning the same unique color.• States (|S| vertices): we prepare one vertex per state, assigning the same unique color.• Next states (|S| vertices and |S| edges): we prepare another vertex per state, assigning the same unique color, howeverdifferent from the color of state vertices. We connect each next-state vertex to the corresponding state vertex, so thatpermuting state vertices yields permuting next-state vertices in the same order.• Actions (| Ai| vertices andi| Ai| edges): we prepare one vertex per action, assigning the same unique color. We(cid:4)i(cid:4)(cid:4)(cid:4)connect each action vertex to the corresponding agent vertex to represent to which agent the action is available.iii• Observations (| Z i| edges): we prepare one vertex per observation, assigning the same uniquecolor. We connect each observation vertex to the corresponding agent vertex to represent to which agent the observationis available.| Z i| vertices and• Transition probabilities (|S|2| Ai| edges): we prepare one vertex per transition prob-ability, assigning the same unique color if and only if they have the same probability. We connect each transitionprobability vertex to the corresponding state, next-state, and action vertices.| Ai| vertices and (|I| + 2)|S|2(cid:8)(cid:8)• Observation probabilities (|S|| Ai|| Z i| edges): we prepare one vertex per obser-vation probability, assigning the same color if and only if they have the same probability. We connect each observationprobability vertex to the corresponding state, action, and observation vertices.| Ai|| Z i| vertices and (2|I| + 1)|S|• Individual reward functions (|I||S|| Ai| edges): we prepare one vertex per individualreward, assigning the same color if and only if they have the same reward. We connect each reward vertex to thecorresponding agent, state, and action vertices.| Ai| vertices and (|I| + 2)|I||S|(cid:8)(cid:8)• Initial state distribution (|S| vertices and |S| edges): we prepare the vertices corresponding to vb0(s) the same way as(cid:8)(cid:8)iiiiithey are for POMDPs.The resulting graph has O (|I||S|2share the same reward function, there will be O (|I||S|2number of vertices.| Ai|| Z i|) vertices and O (|I|2|S|2(cid:8)ii(cid:8)(cid:8)| Ai|| Z i|) edges. For DEC-POMDPs where the agents| Ai|| Z i|) edges so that the number of edges is linear in thei5.3. Computational complexityA recent study on the computational complexity of finding MDP symmetries [20] showed that the problem of finding thesymmetries of a given MDP can be polynomially reduced to the problem of finding the automorphisms of the correspondinggraph encoding. Hence, it is known that the computational complexity of finding the symmetries of an MDP belongs to thegraph isomorphism-complete (GI-complete) class. In this section, we extend the result on MDPs to POMDPs and POSGs,taking a similar but slightly different approach.For ease of exposition, we provide two lemmas that will be useful in proving the main theorem regarding the results forPOSGs. We will use the following definitions for the proof in the first lemma:Definition 3. Given POSG M, G M denotes the vertex-colored undirected graph representation of M. The model vertices ofG M are the vertices corresponding to the state, action, observation, and agents of M. The parameter vertices of G M are theones corresponding to transition, observation, and reward functions of M.We also adjust notations regarding symmetries in order to prevent confusion: Symmetries pertaining to graphs will bedenoted as φG with a G subscript, whereas symmetries of POSGs will retain the notations introduced in Definitions 1 and 2.Lemma 1. A symmetry of M corresponds to a unique automorphism of G M , and vice versa.Proof. First, assume that a symmetry φ of M is given. From this, we can prove the existence of a unique automorphism φGof G M . To construct a unique φG from φ, proceed by first mapping the model vertices according to φ. For example, givenan action vertex vai , we set φG (vai ) ← vφ Aφ(i) (aφ(i)). Note that mapping the agent vertices simultaneously still maintains theedge connectivity because their corresponding action and observation vertices are mapped accordingly. Next, we permutethe parameter vertices that are connected to the model vertices. This latter permutation must be possible because φ pre-serves the probabilities and rewards (whose corresponding vertex colors are the same). To specify the permutations of the(cid:6))(cid:4). By construction of G M ,parameter vertices, consider a pair of tuples t1 = (cid:3)s, (cid:9)a, (cid:9)z, s(cid:6))), respectively, share thethe vertices v T (s,(cid:9)a,s(cid:6)) and v T (φS (s),φ (cid:9)A ((cid:9)a),φS (s(cid:6))), corresponding to T (s, (cid:9)a, s(cid:6)(cid:4) and t2 = (cid:3)φS (s), φ (cid:9)A((cid:9)a), φ (cid:9)Z ((cid:9)z), φS (s(cid:6)) and T (φS (s), φ (cid:9)A((cid:9)a), φS (sB.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5747same color since the two probabilities are equal under φ. This assertion holds for any arbitrary choice of t1, by definitionof POSG symmetry. The only components connected to the relevant parameter vertices are the participating model vertices.Therefore, φG (v T (s,(cid:9)a,s(cid:6))) ← v T (φS (s),φ (cid:9)A ((cid:9)a),φS (s(cid:6))) preserves the color and edge constraints of graph automorphism. The sameargument applies to the observation probability and reward vertices, completing the construction of φG . The construction ofφG is tailored to a specific φ, and is different for some other φ(cid:6) (cid:13)= φ, since the POSG symmetries are bijections. Thus, thereis only one φG for a specific φ.To show the other direction, assume we are given a φG of G M . Consider two tuples t1 = (cid:3)v s, {vai(cid:3)φG (v s), {φG (vai )}, {φG (v zi )}, φG (v s(cid:6) )(cid:4). The set of vertices for both tuples run over all agents i ∈ I . The vertices v s, v s(cid:6) , {vait1 are connected to a transition probability vertex v T (s,(cid:9)a,s(cid:6)) that corresponds to T (s, (cid:9)a, sby concatenating the actions corresponding to the vertices {vaiv T (φS (s),φ (cid:9)A ((cid:9)a),φS (s(cid:6))) of t2. Because there can be only one v T (s,(cid:9)a,s(cid:6)) for the triple (s, (cid:9)a, sto v T (φS (s),φ (cid:9)A ((cid:9)a),φS (s(cid:6))), and no other. It follows that the following two equalities hold:}, v s(cid:6) (cid:4) and t2 =} of(cid:6)), where (cid:9)a is the joint action formed}. The analogue holds for another transition probability vertex(cid:6)), only the vertices in t2 are connected}, {v ziφG (v T (s,(cid:9)a,s(cid:6))) = v T (φS (s),φ (cid:9)A ((cid:9)a),φS (s(cid:6))),(cid:3)(cid:2)φG (v T (s,(cid:9)a,s(cid:6)))ψ= ψ(v T (φS (s),φ (cid:9)A ((cid:9)a),φS (s(cid:6)))).If this were not true, there must exist another vertex v T (cid:6)(cid:6) that is mapped to v T (s,(cid:9)a,s(cid:6)). Then, by the property of graph auto-morphism, the corresponding vertices in t2 should also be connected to v T (cid:6)(cid:6) – otherwise, vertices in t1 cannot be mappedto those of t2. However, this is a contradiction to the way G M is constructed, since the vertices in t2 are connected to(cid:6))),two transition probability vertices. Therefore, there exists an automorphism φ such that T (s, (cid:9)a, swhere φG (v s) = vφS (s), φG (vai ) = vφ (cid:9)A (aφI (i)), ∀i ∈ I, ai ∈ Ai, s ∈ S. The analogous equalities for the observation and rewardfunctions can be proved similarly. Furthermore, similar to the proof of the reverse direction, φ is unique to the given φGbecause φG is a bijection. (cid:2)(cid:6)) = T (φS (s), φ (cid:9)A((cid:9)a), φS (sWe also show that, given any vertex-colored undirected graph G, we can construct POSG so that an automorphism of Gcorresponds to a unique symmetry of the POSG, and vice versa. The constructed POSG consists of a single agent, action, andobservation. Each state of the POSG corresponds to each vertex of G. In more detail, the construction is as follows: Prepare aPOSG state per each v ∈ V . With a slight abuse of notation, we will use the notations for states and vertices interchangeably.We take the agent set to be a singleton set. There is only a single action, a, for this POSG, and the transition probabilitiesare determined as follows: Let deg(v) denote the degree of vertex v. Then T (v, a, u) = 1deg(v) , ∀(v, u) ∈ E. A self-transitionof probability 1 is implicitly assigned to zero-degree vertices. Therefore, the transition probability assignment will needO (|V |D) time, where D = maxv∈V deg(v). This complexity is again upper-bounded by O (|V |2), since there can be at most|V | − 1 edges connected to any given vertex. There is only one observation z, leading to an identical observation probabilityfunction of 1 to all (s, a) pairs. That is, O (v, a, z) = 1, ∀v ∈ V . This assignment is done in O (|V |) time. For the rewardcomponent, we assign the reward according to the color of the vertex at which the action is taken. That is, R(v, a) =N(ψ(v)), where N : C → (cid:7) is taken to be any bijection that maps colors to real numbers.Definition 4. Given a vertex-colored undirected graph G = (cid:3)V , E, C, ψ(cid:4), MG denotes the POSG representation of G viaconstruction steps described above.Lemma 2. An automorphism of MG corresponds to a unique symmetry of M, and vice versa.Proof. First, we show that there is a unique symmetry φ of MG for an automorphism φG of G. Because the verticesconstitute the state space of MG , only the states are permuted. By the edge-preserving property of φG , deg(v) = deg(φG (v))for all vertices in G. It follows that T (v, a, u) = T (φG (v), a, φG (u)), ∀(v, u) ∈ E. By the color-preserving property of φG ,R(v, a) = ψ(v) = ψ(φG (v)) = R(φG (v), a) holds. Lastly, the observation probability remains invariant to any automorphismsince it is constant for all states. Therefore, we can construct φ by permuting the states the way they were permuted by φG .Notice that because φG (v) (cid:13)= φ(cid:6)To prove the other direction, we assume the symmetry φ of MG is given. By definition of φ, T (s, a, s(cid:6)))and R(s, a) = R(φS (s), a) holds. The equivalence of the transition probabilities implies that deg(v s) = deg(vφ(s)) for thevertex v s corresponding to state s. This equality holds for all v ∈ V . To this end, we can set φG (v s) ← vφ(s) as our unique φG .To see that this φG supports edge-preservation, take any (v, u) ∈ E. Let sv and su be the states mapped to v and u,respectively. Then T (sv , a, su) = 1= T (φ(sv ), a, φ(su)). The fact that the last term is non-zero indicatesthat (φG (v), φG (u)) ∈ E as well. Also, for any (v, u) /∈ E, T (sv , a, su) = T (φG (v), a, φG (u)) = 0, hence (φG (v), φG (u)) /∈ E aswell. (cid:2)(cid:6)) = T (φS (s), a, φS (s(cid:13)= φG , φ is unique.G (v), ∀φ(cid:6)1deg(φG (v))deg(v)=GWe now state the main theorem regarding the computational complexity of finding symmetries of POSGs. We denotethe problem of finding the generators2 of automorphism groups of a graph G by AGEN(G), and the problem of finding the2 Simply put, an automorphism generator of a graph is a set of permutations on the vertices such that when applied, yields permuted graphs.48B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57symmetries of a given POSG M by PSYMM(M). It is known that AGEN(G) belongs to the class GI-complete [6]. We use thisfact to prove that the computational complexity of PSYMM(M) is GI-complete as well.To prove that PSYMM(M) is GI-complete, we need to show that PSYMM(M) (cid:3)p AGEN(G M ) and AGEN(G) (cid:3)pPSYMM(MG ), where A (cid:3)p B denotes polynomial reducibility of problem A to problem B.Theorem 5. PSYMM(M) belongs to the class GI-complete.| Ai|| Z i|), which isProof. We first show that PSYMM(M) (cid:3)p AGEN(G M ). The number of vertices in G M is O (|I|2|S|2polynomial in the number of agents, states, individual actions, and observations. Since the complexity of constructing anyundirected graph from n vertices is at most O (n2) (in the case of a complete graph), it takes polynomial time to convertthe POSG to the corresponding vertex colored undirected graph. By Lemma 1, the symmetries of M and the automorphismsof G M are equivalent.The second part of the proof aims to show that AGEN(G) (cid:3)p PSYMM(MG ). For the purpose of parallel argument, weassume that the given graph is vertex-colored, although the argument can be specialized to non-colored graphs. Given avertex-colored undirected graph G = (cid:3)V , E, C, ψ(cid:4), we will construct the corresponding POSG M G . Note that it also takespolynomial time to convert the graph G to the POSG MG . By Lemma 2, the automorphisms of G and the symmetries of MGare equivalent. (cid:2)i(cid:8)By setting |I| = 1, a POSG becomes a POMDP and all of the arguments presented in the proof of Theorem 5 carries overwithout modification. Hence, we can state the same result for an arbitrary POMDP regarding its computational complexity.Corollary 2. The problem of finding the symmetries of a POMDP belongs to the class GI-complete.Although the class GI-complete belongs to NP, it is neither known to be P nor NP-complete. It is however known to bein the low hierarchy of class NP, and there are a number of implementations that can solve GI problems efficiently.6. Exploiting symmetries in the solution methodsIn this section, we present algorithms for POMDPs and POSGs taking advantage of symmetries present in the model.We first show how we can extend PBVI using the characteristics of POMDP symmetries discussed in Section 4.2. We thenpresent an extended version of MADP for POSGs using the properties of POSG symmetries discussed in Section 4.4.6.1. Extending PBVI for symmetry exploitation in POMDPsWith the set of automorphisms Φ that represents the set of all symmetries present in the model, we can modify PBVIto take advantage of the symmetries in belief states and α-vectors: First, when we sample the set of belief states, one ofthe heuristics used by PBVI is to select the belief state with the farthest (cid:15) · (cid:15)1 distance from any belief state already in B.Since we readily know the values at symmetric images of any belief state, we modify the (cid:15) · (cid:15)1 distance computation tohandle symmetries: (cid:15)b − b1 distance.This also allows us to exclude symmetrically identical belief states. Second, since B will exclude symmetrically identicalbelief states, we should modify the backup operation to include symmetric images of α-vectors into Γ at . Table 3 shows thepseudo-code for performing the symmetric backup operation.(cid:6))(cid:15)1. We then select the belief state with the farthest (cid:15) · (cid:15)Φ= minφ,φ(cid:6)∈Φ (cid:15)φ(b) − φ(cid:6)(b(cid:6)(cid:15)Φ1We also added a small but important improvement for the symmetric backup of α-vectors: some of the belief stateswill have the same symmetric image, i.e., b = φ(b). For these belief states, it is often unnecessary to add φ(αb) into Γt ,since φ(αb) is relevant to the belief state φ(b) but b and φ(b) are the same! We thus identified which automorphisms yieldb (cid:13)= φ(b) for each belief state b, and included the symmetric images of α-vectors only for these automorphisms.6.2. Extending MADP for symmetry exploitation in POSGsWe now show how to apply our approach to POSGs. Using the results in Section 4.4, we can expect certain leverages inperformance when using MADP. In particular, we make use of the symmetries in the two stages of the method:• Value computation stage: The first major speed bottleneck occurs during the value computation, where we evaluateall the joint policies generated from the exhaustive backup. However, Theorem 3 states that for any given joint policy,its value vector is merely a permutation of the value vector of its symmetric image. Thus, the value computation forsuch policies can be avoided – we can simply permute the symmetric value vector whenever we need it. Note that inthe case of inter-agent symmetries, all the value vectors of an agent can be obtained by permuting value vectors of itssymmetric agent. The total number of value vectors decreases by a factor of |Φ|.• Pruning stage: An even greater slowdown is due to LP routines for pruning. The existence of symmetries allows us toreduce the number of LP invocations. First, when a local policy π of agent i is pruned, Corollary 1 states that the localpolicy φ(π ) of agent φI (i) can be pruned for all φ. Second, when π is not to be pruned, then all φ(π )’s are not to bepruned as well. Therefore, LP need not be performed on those local policies.B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5749Table 3The backup operation of PBVI taking into account Φ, the set of all symmetries.Require: Γt = backup(B, Γt−1, Φ)(cid:6), a, z)αi (s(cid:6))z∈Z argmaxα∈Γ a,zt(α · b)i(cid:4)(cid:4)(cid:6))O (si αa,zs(cid:6)∈S T (s, a, sfor each a ∈ A, z ∈ Z , αi ∈ Γt−1 dofor each s ∈ S doαa,z(s) = γiend for(cid:28)Γ a,z=tend forΓt = {}for each b ∈ B dofor each a ∈ A, s ∈ S dob (s) = R(s, a) +αaend for∗ = argmaxa(αaaαb = αabif αb /∈ Γt thenΓt = Γt ∪ αbfor each φ = (cid:3) f , g, h(cid:4) ∈ Φ doif f (αb) /∈ Γt thenΓt = Γt ∪ f (αb)end ifend for· b)b∗end ifend forTable 4Dynamic programming for POSG with symmetries. NoPrunei for each agent i maintains the list of policy trees that are found not prunable by the symmetry.Require: Sets of t-step policies Πi,t , corresponding value vectors Vi,t for each agent i, and set of symmetries Φ.# The first stage of dynamic programming backupPerform exhaustive backups to get Πi,t+1 for each i.for all (cid:9)π ∈ (cid:9)Πt+1 doφ( (cid:9)π )if (cid:3)φ ∈ Φ, Vi,t+1 has been computed then(cid:9)πi,t+1 (Eq. (5)) and add the value vector to Vi,t+1.Compute Vend ifend for# The second stage of dynamic programming backupwhile any agent i has a prunable policy doNoPrunek ← {}, ∀k ∈ I .for all π ∈ Πi,t+1 doif π /∈ NoPrunei and π can be pruned (Eq. (6)) thenΠi,t+1 ← Πi,t+1\π .for ∀φ ∈ Φ doΠφI (i),t+1 ← ΠφI (i),t+1\φ(π ).end forelse if π cannot be pruned thenNoPrunei ← NoPrunei ∪ {π }.for ∀φ ∈ Φ doNoPruneφI (i) ← NoPruneφI (i) ∪ {φ(π )}.end forend ifend forend whilereturn Sets of (t + 1)-step policies Πi,t+1 and corresponding value vectors Vi,t+1 for each agent iThe procedure for the multi-agent dynamic programming operator that exploits symmetry is outlined in Table 4.7. ExperimentsIn this section, we empirically show how symmetries in POMDPs and POSGs can help reduce burdens on computationalresources required to compute solutions. The experiments are conducted on a number of standard benchmark domains inPOMDPs and POSGs.7.1. POMDP experimentsBefore we demonstrate the performance gain of the PBVI algorithm by using the symmetric backup operator, we firstreport test results for the existence of automorphisms in standard POMDP benchmark domains. Most of the benchmarkdomains are already compact in the sense that the model minimization algorithm was not able to further reduce the size50B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57DomainTigerTiger-grid2-City-ticketing (perr = 0)2-City-ticketing (perr = 0.1)3-City-ticketing (perr = 0)3-City-ticketing (perr = 0.1)|S|23639739719451945Min |S||V |Nauty exec time235397397194519453998141 624 5451 624 54561 123 60461 123 6040.004 s0.061 s31.872 s23.873 s2585.770 s2601.543 s|Φ|24441212Fig. 10. Model minimization and graph automorphism results on benchmark domains. |S| is the number of states in the original model, Min |S| is thenumber of states in the minimized model, |V | is the number of vertices in the graph encoding of the model, and |Φ| is the number of automorphismsfound by nauty including the identity mapping.DomainTigerTiger-grid2-city-ticketing (perr = 0)2-City-ticketing (perr = 0.1)3-City-ticketing (perr = 0)3-City-ticketing (perr = 0.1)AlgorithmPBVISymm-PBVIPBVISymm-PBVIPBVISymm-PBVIPBVISymm-PBVIPBVISymm-PBVIPBVISymm-PBVI|B|19105903005117104302613627530|Γ |5553252955910374239133Iter8989888516716816716791919191Exec time0.07 s0.05 s359.69 s196.09 s157.80 s57.60 s546.04 s201.97 s43 094.32 s9 395.06 s43 286.92 s16 791.17 sV (b0)(cid:11)6.406.400.800.808.748.747.767.738.088.086.956.940.010.030.020.021.001.00Fig. 11. Performance comparisons of the PBVI algorithm with automorphisms. Symm-PBVI is the PBVI algorithm exploiting the automorphisms, i.e., sym-metric belief collection and symmetric backup. |B| is the number of belief states given to the algorithms, |Γ | is the number of α-vectors comprising thepolicy, Iter is the number of iterations until convergence, V (b0) is the average return of the policy starting from initial belief b0, and (cid:11) is the convergencecriteria of each algorithm for running until maxb∈B |V (n)(b) − V (n−1)(b)| (cid:2) (cid:11). All V (b0)’s are within the 95% confidence interval of the optimal.in most of the domains. For the tiger-grid domain [16], we were able to reduce the size and find symmetries. For the tigerdomain [12], we were not able to reduce the size, but still find symmetries.We further tested for automorphism existence on larger domains.In the spoken dialogue management domainby Williams et al. [35], the user is trying to buy a ticket to travel from one city to another city, and the machine hasto request or confirm information from the user in order to issue the correct ticket. These dialog management problemsare denoted as n-city-ticketing. In this domain, there are n cities, and a human user is trying to book a flight betweentwo cities. The agent, as an automated response system, needs to take one of the following actions: greet, ask-from/ask-to,conf-to-x/conf-from-x, submit-x– y, where x and y are two of the n cities. The user’s response is treated as an observationfor the agent: x, from-x, to-x, from-x-to-y, yes, no, null, where x and y again refer to the cities. The observation function isdependent on how well the speech recognition model performs. The states are factored into three components:• Whether the from has been specified,• Whether the destination, to, has been specified,• Whether the current turn is the first turn or not.We instantiated the domain for n = 2 and n = 3 possible cities, and for two different rates of speech recognition errors perr,where perr = 0 assumes no speech recognition error and perr = 0.1 assumes an error rate of 10%. Note that even in the casewhere perr = 0, the domain is still a POMDP since the user may provide partial information about the request (e.g., origincity only).All of these problems could not be reduced in size, but still had symmetries. Regardless of the value of perr, the graphsencoding the POMDP models were exactly the same. The small differences in the nauty execution times may be due to thedifferences in the orderings of the vertices of the graph. Fig. 10 summarizes the result of automorphism finding experiments.Next, we experimented with the PBVI algorithms on the above benchmark domains using the discovered automorphisms.First, we sampled a fixed number of symmetric belief states (e.g., 300 for the tiger-grid) and ran the symmetric version ofPBVI. We then checked the number of unique belief states if the symmetric belief states were to be expanded by theautomorphisms. We set this number (e.g., 590 for the tiger-grid) as the number of belief states to be used by the non-symmetric version of PBVI, and ran the algorithm in the same setting without automorphisms. Note that our implementationof PBVI slightly differs from the original version in that the original PBVI interleaves the belief state exploration and thevalue iteration, rather than fixing the belief states in the onset of execution. We also gathered the belief states simplyusing breadth-first traversal instead of stochastic simulation. This was to analyze the efficiency of the symmetric backupisolated from the effects of symmetric belief state exploration. Fig. 11 shows the results of the experiments. In summary,automorphisms help significantly improve the performance of PBVI in running time without sacrificing the quality of policy.B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5751Fig. 12. Grid-Small environment.7.2. POSG experimentsFig. 13. Grid-Small3x3 environment.There are no well-known benchmark domains for general POSGs, but there is a wealth of benchmark domains for DEC-POMDPs. Hence, we report the results on our symmetry exploitation in MADP for DEC-POMDPs only: Dec-Tiger [19], Grid-Small [1], and Box-Pushing [27]. By focusing on DEC-POMDPs, we can also rule out issues such as equilibrium selectionproblem in general-sum games.The Dec-Tiger domain is a multi-agent extension of the well-known Tiger domain, which has been introduced in Sec-tion 4.3. The main difference is that the agents suffer less (gain more) by coordinating their actions – e.g., the penalty ismore severe for one agent unilaterally opening the door that leads to the tiger, than for both opening the door to the tiger.The standard Grid-Small domain is set in a 2-by-2 grid world, where the two agents, i = 1 and i = 2, have to spend asmuch time as possible on the same grid cell. There are a total of 16 states (each grid cell either has an agent or not), fiveactions per agent (ai,UP, ai,DOWN, ai,RIGHT, ai,LEFT, ai,STAY), and two observations per agent, denoted zi,LEFT and zi,RIGHT, thatindicate whether the agent senses a wall to its left or right, respectively. The 16 states are encoded as s X Y , where X and Ycan take any one of { A, B, C, D} given in Fig. 12. The X indicates the cell in which agent 1 resides, and Y for agent 2, e.g.,s A D is given in Fig. 12. An extended version of Grid-Small is played in a 3-by-3 grid world. There are a total of 81 states,where the grid cells can take any one of { A, B, C, D, E, F , G, H, I}. The action set remains the same as the 2-by-2 case.There is an additional observation for not sensing a wall on either side, and is denoted as zi,NOTHING for agent i. A visualrepresentation of the state s AC is given in Fig. 13.The Box-Pushing domain requires the two agents, i = 1 or 2, to push two small boxes and one large box to a goalstate. The large box is too heavy for a single agent to move, so the two must coordinate their actions in order tojointly push the large box. There are four actions per robot (ai,LEFT, ai,RIGHT, ai,MOVE, ai,STAY), five observations per robot(zi,SMALL, zi,LARGE, zi,WALL, zi,EMPTY, zi,OTHER), and 100 states, four of which are goal states. The robots can either choose toplace the two small boxes individually into the goal state and receive a small reward, or cooperatively push the large boxand receive a greater reward. The initial state of the Box-Pushing domain is depicted in Fig. 14. In this domain, two robotsR1 and R2 start facing each other in a 3-by-4 grid. Notice that the location of R1 is always left to that of R2. This is because,in order for R1 to be left to R2, it must first move upwards. But since both robots have boxes above them, moving upwardwill cause the box to be positioned in the goal region, terminating the domain. This also accounts for the fact that thecolumn coordinates, labeled 0 to 3, suffice to describe the positions of R1 and R2, since it is impossible for either robot tobe in the above two rows of the grid without having the domain terminated. Thus, we adopt an alpha-numeric encoding todenote a particular non-goal state. All non-goal states will be of the form s X X Y Y , where the first two X s will be the columncoordinates for R1 and R2 in that order, and the last two Y s take values from {r, l, u, d} indicating the robot is facing right,left, up, or down, respectively. E.g., s03rl depicts the initial state given in Fig. 14. The four goal states correspond to: the leftsmall box being in the goal region (sLBox), and the right small box being in the goal region (sRBox), and both small boxesbeing in the goal region (sLRBox), and the large box being in the goal region (sLargeBox).Prior to executing the symmetric MADP algorithm, we ran nauty on the graph encoding of each DEC-POMDP domain.The automorphisms in Dec-Tiger are presented in Fig. 15. The automorphisms discovered included one trivial automorphism– the identity mapping. There are three non-trivial automorphisms, two of them being inter-agent. The inter-agent andintra-agent automorphisms of Grid-Small domain are shown in Figs. 17 and 18, respectively in Appendix A. This domaincontains eight automorphisms, including the identity mapping. Of the seven non-trivial automorphisms, four are inter-52B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57Fig. 14. Initial configuration of the Box-Pushing domain. Immediately above the robots are the left and right small gray boxes, next to the black large box.The hatched region in the grid is the goal region.Symm. typeInter-agentStateIdentity mappingsLEFT ↔ sRIGHTIntra-agentsLEFT ↔ sRIGHTActiona1,LISTEN ↔ a2,LISTENa1,LEFT ↔ a2,LEFTa1,RIGHT ↔ a2,RIGHTa1,LISTEN ↔ a2,LISTENa1,LEFT ↔ a2,RIGHTa1,RIGHT ↔ a2,LEFTa1,LEFT ↔ a1,RIGHTa2,LEFT ↔ a2,RIGHTa1,LISTEN ↔ a1,LISTENa2,LISTEN ↔ a2,LISTENObs.z0,LEFT ↔ z1,LEFTz1,RIGHT ↔ z2,RIGHTz1,LEFT ↔ z2,RIGHTz1,RIGHT ↔ z2,LEFTz1,LEFT ↔ z1,RIGHTz2,LEFT ↔ z2,RIGHTFig. 15. Non-trivial automorphisms in Dec-Tiger. The notation X ↔ Y indicates that X is symmetric to Y .DomainAlgorithmT = 1T = 2#LPTime|V|Dec-TigerGrid-SmallGrid-Small3x3Box-PushingSymm-MADPMADPSymm-MADPMADPSymm-MADPMADPSymm-MADPMADP46281410121 s0 s1 s0 s3 s1 s2 s1 s491111816#LP428461055156290Time1 s1 s0 s1 s1 s1 s1271 s3505 s|V|207810115025507681536T = 3#LP10222371124410189|V|86 175344 250263120 00099 809Time326 s1215 s24 s65 s172800 sN.A.N.A.Fig. 16. Performance comparisons on domains with and without symmetry exploitation. #LP is the number of LP invocations and V is the set of valuevectors produced at the end of each iteration. The first row of each domain shows the results with symmetry exploitation and the second row shows theresults without symmetry. All time records are rounded up to the nearest second.agent. Similarly for Grid-Small3x3, there are seven non-trivial automorphisms. These are shown in Figs. 19 and 21 in theAppendix A as well. For the Box-Pushing domain, the only non-trivial automorphism is an inter-agent automorphism, asshown in Fig. 23 in Appendix A. One notable symmetry of this domain is the interchange of the two states indicating theleft and right small boxes being in the goal region (sLBox and sRbox). In addition, the two agents and their correspondingactions and observations are swapped as well.After computing the symmetries, we compared our proposed algorithm to the MADP algorithm on each domain. Wemeasured the memory usage by counting the number of value vectors created at the end of each iteration. We also countedthe number of LP invocations at each horizon. As can be seen in Fig. 16, both algorithms were able to complete three andtwo horizons for the former two domains and the Box-Pushing domain, respectively. For the Grid-Small3x3 domain, MADPcould not complete horizon three, whereas Symm-MADP could. The running time for all symmetry-exploiting algorithmsinclude the time taken to compute the symmetries using nauty, which explains why Symm-MADP takes slightly longer tocomplete the first time horizon in some cases. A separate field for nauty execution time is omitted, as it was negligible (lessthan 2.5 s) compared to the overall running time. Notice that even with the exploitation of symmetries, proceeding beyondthe horizon attained by MADP is still spatially constrained. For the Dec-Tiger domain, value vectors alone take 70 GB ofmemory by the end of value computation for horizon 4, even with full symmetry exploitation. Such a tendency is due tothe fact that memory usage experiences exponential increase while symmetry only helps by a linear factor at best. However,this issue can be addressed by various approximate algorithms that bound the memory usage, and experimenting with theirsymmetric versions will be left as a future work.Earlier horizons do not exhibit much of the benefit of the symmetries because very few policy trees are generated.However, towards the last horizon, we can see the effect of symmetry exploitation. While the number of value vectorsB.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5753reduced is approximately proportional to that of the symmetries present in the domain, the number of LP invocations andthe execution time do not necessarily follow this trend. This is due to (1) the existence of many self-symmetric policy treesthat do not contribute to multiple removal and LP avoidance, and (2) differing LP sizes, by which the LP solver’s executiontime varies.The size of LP is an important factor that influences the execution time. The size is governed by how many policy treeswere created from exhaustive backup and the domain size itself. For example, the Box-Pushing domain utilizes relativelylarger LPs up to 12 800 constraints, thereby amplifying the effect of symmetries. Since LP solvers usually take a high-orderpolynomial amount of time, reducing a linear number of variables or constraints in LPs will attain super-linear improvementin time.Symm. TypeInter-agentStates A B ↔ sB As AC ↔ sC As A D ↔ sD AsBC ↔ sC BsB D ↔ sD BsC D ↔ sDCs A A ↔ sC Cs A B ↔ sDCs A D ↔ sBCsB A ↔ sC DsB B ↔ sD DsC B ↔ sD As A A ↔ sB Bs AC ↔ sD Bs A D ↔ sC BsBC ↔ sD AsB D ↔ sC AsC C ↔ sD Ds A A ↔ sD Ds A B ↔ sC Ds AC ↔ sB DsB A ↔ sDCsB B ↔ sC CsC A ↔ sD BActiona1,UP ↔ a2,UPa1,DOWN ↔ a2,DOWNa1,LEFT ↔ a2,LEFTa1,RIGHT ↔ a2,RIGHTa1,STAY ↔ a2,STAYa1,UP ↔ a2,DOWNa1,DOWN ↔ a2,UPa1,LEFT ↔ a2,LEFTa1,RIGHT ↔ a2,RIGHTa1,STAY ↔ a2,STAYa1,UP ↔ a2,UPa1,DOWN ↔ a2,DOWNa1,LEFT ↔ a2,RIGHTa1,RIGHT ↔ a2,LEFTa1,STAY ↔ a2,STAYa1,UP ↔ a2,DOWNa1,DOWN ↔ a2,UPa1,LEFT ↔ a2,RIGHTa1,RIGHT ↔ a2,LEFTa1,STAY ↔ a2,STAYObs.z1,LEFT ↔ z2,LEFTz1,RIGHT ↔ z2,RIGHTz1,LEFT ↔ z2,LEFTz1,RIGHT ↔ z2,RIGHTz1,LEFT ↔ z2,RIGHTz1,RIGHT ↔ z2,LEFTz1,LEFT ↔ z2,RIGHTz1,RIGHT ↔ z2,LEFTFig. 17. Non-trivial inter-agent automorphisms in Grid-Small.Symm. typeIntra-agentActiona1,UP ↔ a1,DOWNa2,UP ↔ a2,DOWNObs.Identity mappinga1,LEFT ↔ a1,RIGHTa2,LEFT ↔ a2,RIGHTz1,LEFT ↔ z1,RIGHTz2,LEFT ↔ z2,RIGHTa1,UP ↔ a1,DOWNa2,UP ↔ a2,DOWNa1,LEFT ↔ a1,RIGHTa2,LEFT ↔ a2,RIGHTz1,LEFT ↔ z1,RIGHTz2,LEFT ↔ z2,RIGHTStates A A ↔ sC Cs A B ↔ sC Ds AC ↔ sC As A D ↔ sC BsB A ↔ sDCsB B ↔ sD DsBC ↔ sD AsB D ↔ sD Bs A A ↔ sB Bs A B ↔ sB As AC ↔ sB Ds A D ↔ sBCsC A ↔ sD BsC B ↔ sD AsC C ↔ sD DsC D ↔ sDCs A A ↔ sD Ds A B ↔ sDCs AC ↔ sD Bs A D ↔ sD AsB A ↔ sC DsB B ↔ sC CsBC ↔ sC BsB D ↔ sC AFig. 18. Non-trivial intra-agent automorphisms in Grid-Small.54B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–578. ConclusionWe have presented a graph-theoretical framework for computing and exploiting symmetries for POMDPs and POSGs. Inaddition, we have shown in the experiments that the actual running time and space are significantly reduced by exploitingsymmetries.The computation of the symmetries were done by first encoding the problems into appropriate graph structures. Theautomorphisms of such graphs are then mapped back to the problem domain to represent the symmetries of the problem. Indoing so, we have also provided a theoretical result that relates the computational complexity of symmetry computation tothat of graph isomorphism computation, i.e., the class GI-complete. Additionally, we have extended the concept of symmetryto a multi-agent setting, introducing POSG symmetries. Because of its multi-agent nature, symmetries in POSGs yield variousSymm. typeInter-agentActiona1,UP ↔ a2,UPa1,DOWN ↔ a2,DOWNa1,LEFT ↔ a2,LEFTa1,RIGHT ↔ a2,RIGHTa1,STAY ↔ a2,STAYObs.z1,LEFT ↔ z2,LEFTz1,RIGHT ↔ z2,RIGHTz1,NOTHING ↔ z2,NOTHINGa1,UP ↔ a2,UPa1,DOWN ↔ a2,DOWNa1,LEFT ↔ a2,LEFTa1,RIGHT ↔ a2,RIGHTa1,STAY ↔ a2,STAYz1,LEFT ↔ z2,RIGHTz1,RIGHT ↔ z2,RIGHTz1,NOTHING ↔ z2,NOTHINGa1,UP ↔ a2,DOWNa1,DOWN ↔ a2,UPa1,LEFT ↔ a2,LEFTa1,RIGHT ↔ a2,RIGHTa1,STAY ↔ a2,STAYz1,LEFT ↔ z2,LEFTz1,RIGHT ↔ z2,RIGHTz1,NOTHING ↔ z2,NOTHINGStates A B ↔ sB A , s AC ↔ sC As A D ↔ sD A , s A E ↔ sE As A F ↔ s F A , s AG ↔ sG As A H ↔ sH A , s A I ↔ sI AsBC ↔ sC B , sB D ↔ sD BsB E ↔ sE B , sB F ↔ s F BsBG ↔ sG B , sB H ↔ sH BsB I ↔ sI B , sC D ↔ sDCsC E ↔ sEC , sC F ↔ s F CsC G ↔ sGC , sC H ↔ sH CsC I ↔ sI C , sD E ↔ sE DsD F ↔ s F D , sDG ↔ sG DsD H ↔ sH D , sD I ↔ sI DsE F ↔ s F E , sE G ↔ sG EsE H ↔ sH E , sE I ↔ sI Es F G ↔ sG F , s F H ↔ sH Fs F I ↔ sI F , sG H ↔ sH GsG I ↔ sI G , sH I ↔ sI Hs A A ↔ sC C , s A B ↔ sBCs A D ↔ s F C , s A E ↔ sECs A F ↔ sDC , s AG ↔ sI Cs A H ↔ sH C , s A I ↔ sGCsB A ↔ sC B , sB D ↔ s F BsB E ↔ sE B , sB F ↔ sD BsBG ↔ sI B , sB H ↔ sH BsB I ↔ sG B , sC D ↔ s F AsC E ↔ sE A , sC F ↔ sD AsC G ↔ sI A , sC H ↔ sH AsC I ↔ sG A , sD D ↔ s F FsD E ↔ sE F , sDG ↔ sI FsD H ↔ sH F , sD I ↔ sG FsE D ↔ s F E , sE G ↔ sI EsE H ↔ sH E , sE I ↔ sG Es F G ↔ sI D , s F H ↔ sH Ds F I ↔ sG D , sG G ↔ sI IsG H ↔ sH I , sH G ↔ sI Hs A A ↔ sG G , s A B ↔ sH Gs AC ↔ sI G , s A D ↔ sDGs A E ↔ sE G , s A F ↔ s F Gs A H ↔ sBG , s A I ↔ sC GsB A ↔ sG H , sB B ↔ sH HsBC ↔ sI H , sB D ↔ sD HsB E ↔ sE H , sB F ↔ s F HsB I ↔ sC H , sC A ↔ sG IsC B ↔ sH I , sC C ↔ sI IsC D ↔ sD I , sC E ↔ sE IsC F ↔ s F I , sD A ↔ sG DsD B ↔ sH D , sDC ↔ sI DsD E ↔ sE D , sD F ↔ s F DsE A ↔ sG E , sE B ↔ sH EsEC ↔ sI E , sE F ↔ s F Es F A ↔ sG F , s F B ↔ sH Fs F C ↔ sI F , sG B ↔ sH AsGC ↔ sI A , sH C ↔ sI BFig. 19. Non-trivial inter-agent automorphism in Grid-Small3x3.B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5755implications in the area of game theory. We presented some game-theoretic properties that are exhibited in the presenceof symmetries.Our algorithms that exploit the symmetries are presented as well. These algorithms are modifications of previous well-known algorithms PBVI and MADP for POMDPs and POSGs, respectively. Although we have demonstrated the efficiencyof symmetry exploitation only using PBVI and MADP, the idea can be readily extended to other algorithms. For example,symmetries can have an impact on solution techniques that use heuristic search such as MAA* [33], or Q-value functions forActiona1,UP ↔ a2,DOWNa1,DOWN ↔ a2,UPa1,LEFT ↔ a2,RIGHTa1,RIGHT ↔ a2,LEFTa1,STAY ↔ a2,STAYObs.z1,LEFT ↔ z2,RIGHTz1,RIGHT ↔ z2,LEFTz1,NOTHING ↔ z2,NOTHINGSymm. typeInter (contd.)States A A ↔ sI I , s A B ↔ sH Is AC ↔ sG I , s A D ↔ s F Is A E ↔ sE I , s A F ↔ sD Is AG ↔ sC I , s A H ↔ sB IsB A ↔ sI H , sB B ↔ sH HsBC ↔ sG H , sB D ↔ s F HsB E ↔ sE H , sB F ↔ sD HsBG ↔ sC H , sC A ↔ sI GsC B ↔ sH G , sC C ↔ sG GsC D ↔ s F G , sC E ↔ sE GsC F ↔ sDG , sD A ↔ sI FsD B ↔ sH F , sDC ↔ sG FsD D ↔ s F F , sD E ↔ sE FsE A ↔ sI E , sE B ↔ sH EsEC ↔ sG E , sE D ↔ s F Es F A ↔ sI D , s F B ↔ sH Ds F C ↔ sG D , sG A ↔ sI CsG B ↔ sH C , sH A ↔ sI BFig. 20. Non-trivial inter-agent automorphism in Grid-Small3x3 (continued).Actiona1,LEFT ↔ a1,RIGHTa2,LEFT ↔ a2,RIGHTObs.z1,LEFT ↔ z1,RIGHTz2,LEFT ↔ z2,RIGHTa1,UP ↔ a1,DOWNa2,UP ↔ a2,DOWNIdentity mappingSymm. typeIntra-agentStates A A ↔ sC C , s A B ↔ sC Bs AC ↔ sC A , s A D ↔ sC Fs A E ↔ sC E , s A F ↔ sC Ds AG ↔ sC I , s A H ↔ sC Hs A I ↔ sC G , sB A ↔ sBCsB D ↔ sB F , sBG ↔ sB IsD A ↔ s F C , sD B ↔ s F BsDC ↔ s F A , sD D ↔ s F FsD E ↔ s F E , sD F ↔ s F DsDG ↔ s F I , sD H ↔ s F HsD I ↔ s F G , sE A ↔ sECsE D ↔ sE F , sE G ↔ sE IsG A ↔ sI C , sG B ↔ sI BsGC ↔ sI A , sG D ↔ sI FsG E ↔ sI E , sG F ↔ sI DsG G ↔ sI I , sG H ↔ sI HsG I ↔ sI G , sH A ↔ sH CsH D ↔ sH F , sH G ↔ sH Is A A ↔ sG G , s A B ↔ sG Hs AC ↔ sG I , s A D ↔ sG Ds A E ↔ sG E , s A F ↔ sG Fs AG ↔ sG A , s A H ↔ sG Bs A I ↔ sGC , sB A ↔ sH GsB B ↔ sH H , sBC ↔ sH IsB D ↔ sH D , sB E ↔ sH EsB F ↔ sH F , sBG ↔ sH AsB H ↔ sH B , sB I ↔ sH CsC A ↔ sI G , sC B ↔ sI HsC C ↔ sI I , sC D ↔ sI DsC E ↔ sI E , sC F ↔ sI FsC G ↔ sI A , sC H ↔ sI BsC I ↔ sI C , sD A ↔ sDGsD B ↔ sD H , sDC ↔ sD IsE A ↔ sE G , sE B ↔ sE HsEC ↔ sE I , s F A ↔ s F Gs F B ↔ s F H , s F C ↔ s F IFig. 21. Non-trivial intra-agent automorphism in Grid-Small3x3.56B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–57Actiona1,UP ↔ a1,DOWNa2,UP ↔ a2,DOWNa1,LEFT ↔ a1,RIGHTa2,LEFT ↔ a2,RIGHTObs.z1,LEFT ↔ z1,RIGHTz2,LEFT ↔ z2,RIGHTSymm. typeIntra (contd.)States A A ↔ sI I , s A B ↔ sI Hs AC ↔ sI G , s A D ↔ sI Fs A E ↔ sI E , s A F ↔ sI Ds AG ↔ sI C , s A H ↔ sI Bs A I ↔ sI A , sB A ↔ sH IsB B ↔ sH H , sBC ↔ sH GsB D ↔ sH F , sB E ↔ sH EsB F ↔ sH D , sBG ↔ sH CsB H ↔ sH B , sB I ↔ sH AsC A ↔ sG I , sC B ↔ sG HsC C ↔ sG G , sC D ↔ sG FsC E ↔ sG E , sC F ↔ sG DsC G ↔ sGC , sC H ↔ sG BsC I ↔ sG A , sD A ↔ s F IsD B ↔ s F H , sDC ↔ s F GsD D ↔ s F F , sD E ↔ s F EsD F ↔ s F D , sDG ↔ s F CsD H ↔ s F B , sD I ↔ s F AsE A ↔ sE I , sE B ↔ sE HsEC ↔ sE G , sE D ↔ sE FFig. 22. Non-trivial intra-agent automorphism in Grid-Small3x3.Actiona1,LEFT ↔ a2,LEFTa1,RIGHT ↔ a2,RIGHTa1,MOVE ↔ a2,MOVEa1,STAY ↔ a2,STAYObs.z1,EMPTY ↔ z2,EMPTYz1,OTHER ↔ z2,OTHERz1,SMALL ↔ z2,SMALLz1,LARGE ↔ z2,LARGEStatesLBox ↔ sRBox, s01uu ↔ s23uus01ud ↔ s23du , s01ul ↔ s23rus01ur ↔ s23lu , s01du ↔ s23uds01dd ↔ s23dd, s01dl ↔ s23rds01dr ↔ s23ld, s01lu ↔ s23urs01ld ↔ s23dr , s01ll ↔ s23rrs01lr ↔ s23lr , s01ru ↔ s23uls01rd ↔ s23dl, s01rl ↔ s23rls01rr ↔ s23ll, s02uu ↔ s13uus02ud ↔ s13du , s02ul ↔ s13rus02ur ↔ s13lu , s02du ↔ s13uds02dd ↔ s13dd, s02dl ↔ s13rds02dr ↔ s13ld, s02lu ↔ s13urs02ld ↔ s13dr , s02ll ↔ s13rrs02lr ↔ s13lr , s02ru ↔ s13uls02rd ↔ s13dl, s02rl ↔ s13rls02rr ↔ s13ll, s03ud ↔ s03dus03ul ↔ s03ru , s03ur ↔ s03lus03dl ↔ s03rd, s03dr ↔ s03lds03ll ↔ s03rr , s12ud ↔ s12dus12ul ↔ s12ru , s12ur ↔ s12lus12dl ↔ s12rd, s12dr ↔ s12lds12ll ↔ s12rrFig. 23. Non-trivial automorphism in Box-Pushing. It is an inter-agent automorphism.DEC-POMDPs [22]. Another interesting area of application would be to apply symmetries to a finite controller representationof policies [1].While symmetry exploitation greatly reduces computational and spatial burden on solving POMDPs and POSGs, it islimited by the fact that not all problems come with symmetries. One promising direction of research would be to computeapproximate symmetries, along with the theoretical error bound.AcknowledgementsWe are indebted to the anonymous reviewers for their helpful comments in improving this article. This work was sup-ported by Korea Research Foundation Grant KRF-D00527, and by Defense Acquisition Program Administration and Agencyfor Defense Development of Korea under contract UD080042AD.Appendix A. AppendixFigs. 17–23 show the automorphisms in Grid-Small, Grid-Small3x3, and Box-Pushing, accompanying the results in theexperiments section.B.K. Kang, K.-E. Kim / Artificial Intelligence 182–183 (2012) 32–5757References[1] C. Amato, S. Zilberstein, Heuristic policy iteration for infinite-horizon decentralized POMDPs, in: Proceedings of the AAMAS 2008 Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains, 2008, pp. 1–15.[2] D.S. Bernstein, C. Amato, E.A. Hansen, S. Zilberstein, Policy iteration for decentralized control of Markov decision processes, Journal of Artificial Intelli-gence Research 34 (2009) 89–132.[3] D.S. Bernstein, R. Givan, N. Immerman, S. Zilberstein, The complexity of decentralized control of Markov decision processes, Mathematics of OperationsResearch 27 (4) (2002) 819–840.[4] D.S. Bernstein, E.A. Hansen, S. Zilberstein, Bounded policy iteration for decentralized POMDPs, in: Proceedings of the 19th International Joint Conferenceon Artificial Intelligence, 2005, pp. 1287–1292.[5] R. Bödi, K. Herr, M. Joswig, Algorithms for highly symmetric linear and integer programs, Tech. Rep., arXiv:1012.4941.[6] K.S. Booth, C.J. Colbourn, Problems polynomially equivalent to graph isomorphism, Tech. Rep. CS-77-04, University of Waterloo, 1979.[7] S.-F. Cheng, D.M. Reeves, Y. Vorobeychik, M.P. Wellman, Notes on equilibria in symmetric games, in: Proceedings of AAMAS 2004 Workshop on Game-Theoretic and Decision Theoretic Agents, 2004, pp. 23–28.[8] T. Dean, R. Givan, Model minimization in Markov decision processes, in: Proceedings of the 14th National Conference on Artificial Intelligence, 1997,pp. 106–111.[9] F. Doshi, N. Roy, The permutable POMDP: fast solutions to POMDPs for preference elicitation, in: Proceedings of the 7th International Conference onAutonomous Agents and Multi-Agent Systems, 2008, pp. 493–500.[10] R. Givan, T. Dean, M. Greig, Equivalence notions and model minimization in Markov decision processes, Artificial Intelligence 147 (1–2) (2003) 163–223.[11] E.A. Hansen, D.S. Bernstein, S. Zilberstein, Dynamic programming for partially observable stochastic games, in: Proceedings of the 19th National Con-ference on Artificial Intelligence, 2004, pp. 709–715.[12] L.P. Kaelbling, M.L. Littman, A.R. Cassandra, Planning and acting in partially observable stochastic domains, Artificial Intelligence 101 (1–2) (1998)99–134.[13] K.-E. Kim, Exploiting symmetries in POMDPs for point-based algorithms, in: Proceedings of the 23rd AAAI Conference on Artificial Intelligence, 2008,pp. 1043–1048.[14] Y. Kim, K.-E. Kim, Point-based policy iteration for decentralized POMDPs, in: Proceedings of the 11th Pacific Rim International Conference on ArtificialIntelligence, 2010, pp. 614–619.[15] A. Kumar, S. Zilberstein, Point-based backup for decentralized POMDPs: Complexity and new algorithms, in: Proceedings of the 9th InternationalConference on Autonomous Agents and Multi-Agent Systems, 2010, pp. 1315–1322.[16] M.L. Littman, A.R. Cassandra, L.P. Kaelbling, Learning policies for partially observable environments: Scaling up, in: Proceedings of the 12th InternationalConference on Machine Learning, 1995, pp. 362–370.[17] O. Madani, S. Hanks, A. Condon, On the undecidability of probabilistic planning and related stochastic optimization problems, Artificial Intelli-gence 147 (1–2) (2003) 5–34.[18] B.D. McKay, Nauty user’s guide, version 2.4 (online document), http://cs.anu.edu.au/~bdm/nauty/nug.pdf, 2007.[19] R. Nair, M. Tambe, M. Yokoo, D. Pynadath, S. Marsella, Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings, in:Proceedings of the 18th International Joint Conference on Artificial Intelligence, 2003, pp. 705–711.[20] S.M. Narayanamurthy, B. Ravindran, On the hardness of finding symmetries in Markov decision processes, in: Proceedings of the 25th InternationalConference on Machine Learning, 2008, pp. 688–695.[21] N. Nisan, T. Roughgarden, E. Tardos, V.V. Vazirani, Algorithmic Game Theory, Cambridge University Press, New York, NY, USA, 2007.[22] F.A. Oliehoek, N. Vlassis, Q-value functions for decentralized POMDPs, in: Proceedings of the 6th International Conference on Autonomous Agents andMulti-Agent Systems, 2007, pp. 838–845.[23] C.H. Papadimitriou, J.N. Tsitsiklis, The complexity of Markov decision processes, Mathematics of Operations Research 12 (3) (1987) 441–450.[24] J. Pineau, G. Gordon, S. Thrun, Policy-contingent abstraction for robust robot control, in: Proceedings of the 19th Conference on Uncertainty in ArtificialIntelligence, 2003, pp. 477–484.[25] J. Pineau, G. Gordon, S. Thrun, Anytime point-based approximation for large POMDPs, Journal of Artificial Intelligence Research 27 (2006) 335–380.[26] B. Ravindran, A.G. Barto, Symmetries and model minimization in Markov decision processes, Tech. Rep. CMPSCI 01-43, University of Massachusetts,Amherst, 2001.[27] S. Seuken, S. Zilberstein, Improved memory-bounded dynamic programming for decentralized POMDPs, in: Proceedings of the 23rd Conference onUncertainty in Artificial Intelligence, 2007, pp. 344–351.[28] S. Seuken, S. Zilberstein, Memory-bounded dynamic programming for DEC-POMDPs, in: Proceedings of the 20th International Joint Conference onArtificial Intelligence, 2007, pp. 2009–2015.[29] G. Shani, R.I. Brafman, S.E. Shimony, Forward search value iteration for POMDPs, in: Proceedings of the 20th International Joint Conference on ArtificialIntelligence, 2007, pp. 2617–2624.[30] T. Smith, R. Simmons, Heuristic search value iteration for POMDPs, in: Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence,2004, pp. 520–527.[31] E.J. Sondik, The optimal control of partially observable Markov decision processes, PhD thesis, Department of Electrical Engineering, Stanford University,1971.[32] M.T.J. Spaan, N. Vlassis, Perseus: Randomized point-based value iteration for POMDPs, Journal of Artificial Intelligence Research 24 (2005) 195–220.[33] D. Szer, F. Charpillet, S. Zilberstein, MAA*: A heuristic search algorithm for solving decentralized POMDPs, in: Proceedings of 21st Conference onUncertainty in Artificial Intelligence, 2005, pp. 576–583.[34] C. White, A survey of solution techniques for the partially observed Markov decision process, Annals of Operations Research 32 (1–4) (1991) 215–230.[35] J.D. Williams, P. Poupart, S. Young, Factored partially observable Markov decision processes for dialogue management, in: Proceedings of the IJCAI 2005Workshop on Knowledge and Reasoning in Practical Dialogue Systems, 2005, pp. 76–82.[36] A.P. Wolfe, POMDP homomorphisms, in: Proceedings of the NIPS 2006 Workshop on Grounding Perception, Knowledge and Cognition in Sensory-MotorExperience, 2006.[37] F. Wu, S. Zilberstein, X. Chen, Point-based policy generation for decentralized POMDPs, in: Proceedings of the 9th International Conference on Au-tonomous Agents and Multi-Agent Systems, 2010, pp. 1307–1314.