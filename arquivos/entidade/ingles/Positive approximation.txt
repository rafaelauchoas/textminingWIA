Artificial Intelligence 174 (2010) 597–618Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintPositive approximation: An accelerator for attribute reduction in roughset theoryYuhua Qian a,c, Jiye Liang a,∗, Witold Pedrycz b, Chuangyin Dang ca Key Laboratory of Computational Intelligence and Chinese Information Processing of Ministry of Education, Taiyuan, 030006, Shanxi, Chinab Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canadac Department of Manufacturing Engineering and Engineering Management, City University of Hong Kong, Hong Konga r t i c l ei n f oa b s t r a c tArticle history:Received 15 July 2009Received in revised form 6 April 2010Accepted 7 April 2010Available online 9 April 2010Keywords:Rough set theoryAttribute reductionDecision tablePositive approximationGranular computing1. IntroductionFeature selection is a challenging problem in areas such as pattern recognition, machinelearning and data mining. Considering a consistency measure introduced in rough settheory, the problem of feature selection, also called attribute reduction, aims to retain thediscriminatory power of original features. Many heuristic attribute reduction algorithmshave been proposed however, quite often, these methods are computationally time-consuming. To overcome this shortcoming, we introduce a theoretic framework based onrough set theory, called positive approximation, which can be used to accelerate a heuristicprocess of attribute reduction. Based on the proposed accelerator, a general attributereduction algorithm is designed. Through the use of the accelerator, several representativeheuristic attribute reduction algorithms in rough set theory have been enhanced. Notethat each of the modified algorithms can choose the same attribute reduct as its originalversion, and hence possesses the same classification accuracy. Experiments show that thesemodified algorithms outperform their original counterparts. It is worth noting that theperformance of the modified algorithms becomes more visible when dealing with largerdata sets.© 2010 Elsevier B.V. All rights reserved.Feature selection, also called attribute reduction, is a common problem in pattern recognition, data mining and machinelearning. In recent years, we encounter databases in which both the number of objects becomes larger and their dimen-sionality (number of attributes) gets larger as well. Tens, hundreds, and even thousands of attributes are stored in manyreal-world application databases [6,12,37]. Attributes that are irrelevant to recognition tasks may deteriorate the perfor-mance of learning algorithms [44,45]. In other words, storing and processing all attributes (both relevant and irrelevant)could be computationally very expensive and impractical. To deal with this issue, as was pointed out in [20], some at-tributes can be omitted, which will not seriously impact the resulting classification (recognition) error, cf. [20]. Therefore,the omission of some attributes could not only be tolerable but even desirable relatively to the costs involved in suchcases [32].In feature selection, we encounter two general strategies, namely wrappers [16] and filters. The former employs alearning algorithm to evaluate the selected attribute subsets, and the latter selects attributes by being guided by some sig-nificance measures such as information gain [23,46], consistency [6,41], distance [15], dependency [30], and others. These* Corresponding author. Tel.: +86 0351 7018176; fax: +86 0351 7018176.E-mail addresses: jinchengqyh@sxu.edu.cn (Y.H. Qian), ljy@sxu.edu.cn (J.Y. Liang), pedrycz@ee.ualberta.ca (W. Pedrycz), mecdang@cityu.edu.hk(C.Y. Dang).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.018598Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618measures can be divided into two main categories: distance-based measures and consistency-based measures [20]. Roughset theory by Pawlak [33–36] is a relatively new soft computing tool for the analysis of a vague description of an object,and has become a popular mathematical framework for pattern recognition, image processing, feature selection, neuro-computing, data mining and knowledge discovery from large data sets [7,11,31]. Attribute reduction in rough set theoryoffers a systematic theoretic framework for consistency-based feature selection, which does not attempt to maximize theclass separability but rather attempts to retain the discernible ability of original features for the objects from the universe[13,14,53].Generally speaking, one always needs to handle two types of data, viz. those that assume numerical values and symbolicvalues. For numerical values, there are two types of approaches. One relies on fuzzy rough set theory, and the other isconcerned with the discretization of numerical attributes. In order to deal with numerical attributes or hybrid attributes,several approaches have been developed in the literature. Pedrycz and Vukovich regarded features as granular rather thannumerical [37]. Shen and Jenshen generalized the dependency function in classical rough set framework to the fuzzy caseand proposed a fuzzy-rough QUICKREDUCT algorithm [13,14,48]. Bhatt and Gopal provided a concept of fuzzy-rough setsformed on compact computational domain, which is utilized to improve the computational efficiency [3,4]. Hu et al. pre-sented a new entropy to measure of the information quantity in fuzzy sets [21] and applied this particular measure toreduce hybrid data [22]. Data discretization is another important approach to deal with numerical values, in which we usu-ally discretize numerical values into several intervals and associate the intervals with a set of symbolic values, see [5,28]. Inthe “classical” rough set theory, the attribute reduction method takes all attributes as those which assume symbolic values.Through preprocessing of original data, one can use the classical rough set theory to select a subset of features that is themost suitable for a given recognition problem.In the last twenty years, many techniques of attribute reduction have been developed in rough set theory. The conceptof the β-reduct proposed by Ziarko provides a suite of reduction methods in the variable precision rough set model [60].An attribute reduction method was proposed for knowledge reduction in random information systems [57]. Five kinds ofattribute reducts and their relationships in inconsistent systems were investigated by Kryszkiewicz [18], Li et al. [24] andMi et al. [29], respectively. By eliminating some rigorous conditions required by the distribution reduct, a maximum distri-bution reduct was introduced by Mi et al. in [29]. In order to obtain all attribute reducts of a given data set, Skowron [49]proposed a discernibility matrix method, in which any two objects determine one feature subset that can distinguish them.According to the discernibility matrix viewpoint, Qian et al. [42,43] and Shao et al. [47] provided a technique of attributereduction for interval ordered information systems, set-valued ordered information systems and incomplete ordered infor-mation systems, respectively. Kryszkiewicz and Lasek [17] proposed an approach to discovery of minimal sets of attributesfunctionally determining a decision attribute. The above attribute reduction methods are usually computationally very ex-pensive, which are intolerable for dealing with large-scale data sets with high dimensions. To support efficient attributereduction, many heuristic attribute reduction methods have been developed in rough set theory, cf. [19,20,22,25,26,39,52,54–56]. Each of these attribute reduction methods can extract a single reduct from a given decision table.1 For convenience,from the viewpoint of heuristic functions, we classify these attribute reduction methods into four categories: positive-regionreduction, Shannon’s entropy reduction, Liang’s entropy reduction and combination entropy reduction. Hence, we reviewonly four representative heuristic attribute reduction methods.(1) Positive-region reductionThe concept of positive region was proposed by Pawlak in [33], which is used to measure the significance of a conditionattribute in a decision table. While the idea of attribute reduction using positive region was originated by J.W. Grzymala-Busse in [9] and [10], and the corresponding algorithm ignores the additional computation required for selecting significantattributes. Then, Hu and Cercone [19] proposed a heuristic attribute reduction method, called positive-region reduction,which remains the positive region of target decision unchanged. The literature [20] gave an extension of this positive-region reduction for hybrid attribute reduction in the framework of fuzzy rough set. Owing to the consistency of ideas andstrategies of these methods, we regard the method from [19] as their representative. These reduction methods are the firstattempt to heuristic attribute reduction algorithms in rough set theory.(2) Shannon’s entropy reductionThe entropy reducts have first been introduced in 1993/1994 by Skowron in his lectures at Warsaw University. Basedon the idea, Slezak introduced Shannon’s information entropy to search reducts in the classical rough set model [50–52].Wang et al. [54] used conditional entropy of Shannon’s entropy to calculate the relative attribute reduction of a decisioninformation system. In fact, several authors also have used variants of Shannon’s entropy or mutual information to measureuncertainty in rough set theory and construct heuristic algorithm of attribute reduction in rough set theory [22,55,56]. Here1 The attribute reduct obtained preserves a particular property of a given decision table. However, as Prof. Bazan said, from the viewpoint of stability ofattribute reduct, the selected reduct may be of bad quality [1,2]. To overcome this problem, Bazan developed a method for dynamic reducts to get a stableattribute reduct from a decision table. How to accelerate the method for dynamic reducts is an interesting topic in further work.Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618599we only select the attribute reduction algorithm in the literature [54] as their representative. This reduction method remainsthe conditional entropy of target decision unchanged.(3) Liang’s entropy reductionLiang et al. [25] defined a new information entropy to measure the uncertainty of an information system and appliedthe entropy to reduce redundant features [26]. Unlike Shannon’s entropy, this information entropy can measure both theuncertainty of an information system and the fuzziness of a rough decision in rough set theory. This reduction method canpreserve the conditional entropy of a given decision table. In fact, the mutual information form of Liang’s entropy also canbe used to construct a heuristic function of an attribute reduction algorithm. For simplicity, we here ignore its discussion.(4) Combination entropy reductionIn general, the objects in an equivalence class cannot be distinguished each other, but the objects in different equivalenceclasses can be distinguished each other in rough set theory. Therefore, in a broad sense, the knowledge content of a givenattribute set can be characterized by the entire number of pairs of the objects which can be distinguished each other onthe universe. Based on this consideration, Qian and Liang [39] presented the concept of combination entropy for measuringthe uncertainty of information systems and used its conditional entropy to select a feature subset. This reduction methodcan obtain an attribute subset that possesses the same number of pairs of the elements which can be distinguished eachother as the original decision table. This measure focuses on a completely different point of view, which is mainly based onthe intuitionistic knowledge content nature of information gain.Each of these above methods preserves a particular property of a given information system or a given decision table.However, these above methods are still computationally very expensive, which are intolerable for dealing with large-scaledata sets with high dimensions. In this paper, we are not concerned with how to discretize numerical attributes and con-struct a heuristic function for attribute reduction. Our objective is to focus on how to improve the time efficiency of aheuristic attribute reduction algorithm. We propose a new rough set framework, which is called positive approximation.The main advantage of this approach stems from the fact that this framework is able to characterize the granulation struc-ture of a rough set using a granulation order. Based on the positive approximation, we develop a common accelerator forimproving the time efficiency of a heuristic attribute reduction, which provides a vehicle of making algorithms of roughset based feature selection techniques faster. By incorporating the accelerator into each of the above four representativeheuristic attribute reduction methods, we construct their modified versions. Numerical experiments show that each of themodified methods can choose the same attribute subset as that of the corresponding original method while greatly reducingcomputing time. We would like to stress that the improvement becomes more profoundly visible when the data sets underdiscussion get larger.The study is organized as follows. Some basic concepts in rough set theory are briefly reviewed in Section 2. In Section 3,we establish the positive approximation framework and investigate some of its main properties. In Section 4, throughanalyzing the rank preservation of four representative significance measures of attributes, we develop a general modifiedattribute reduction algorithm based on the positive approximation. Experiments on nine public data sets show that thesemodified algorithms outperform their original counterparts in terms of computational time. Finally, Section 5 concludes thispaper by bringing some remarks and discussions.2. PreliminariesIn this section, we will review several basic concepts in rough set theory. Throughout this paper, we suppose that theuniverse U is a finite nonempty set.Let U be a finite and nonempty set called the universe and R ⊆ U × U an equivalence relation on U . Then K = (cid:4)U , R(cid:5) iscalled an approximation space [33–36]. The equivalence relation R partitions the set U into disjoint subsets. This partitionof the universe is called a quotient set induced by R, denoted by U /R. It represents a very special type of similarity betweenelements of the universe. If two elements x, y ∈ U (x (cid:7)= y) belong to the same equivalence class, we say that x and y areindistinguishable under the equivalence relation R, i.e., they are equal in R. We denote the equivalence class including xby [x]R . Each equivalence class [x]R may be viewed as an information granule consisting of indistinguishable elements [59].The granulation structure induced by an equivalence relation is a partition of the universe.Given an approximation space K = (cid:4)U , R(cid:5) and an arbitrary subset X ⊆ U , one can construct a rough set of the set on theuniverse by elemental information granules in the following definition:(cid:2)(cid:3)(cid:3)R X =R X ={[x]R | [x]R ⊆ X},{[x]R | [x]R ∩ X (cid:7)= ∅},where R X and R X are called R-lower approximation and R-upper approximation with respect to R, respectively. The orderpair (cid:4)R X, R X(cid:5) is called a rough set of X with respect to the equivalence relation R. Equivalently, they also can be writtenas(cid:2)R X = {x | [x]R ⊆ X},R X = {x | [x]R ∩ X (cid:7)= ∅}.600Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618There are two kinds of attributes for a classification problem, which can be characterized by a decision table S = (U , C ∪D) with C ∩ D = ∅, where an element of C is called a condition attribute, C is called a condition attribute set, an elementof D is called a decision attribute, and D is called a decision attribute set. Each nonempty subset B ⊆ C determines anequivalence relation in the following way:(cid:4)R B =(x, y) ∈ U × U | a(x) = a( y), ∀a ∈ B,(cid:5)where a(x) and a( y) denote the values of objects x and y under a condition attribute a, respectively. This equivalencerelation R B partitions U into some equivalence classes given by(cid:4)(cid:5)U /R B =[x]B | x ∈ U,for simplicity, U /R B will be replaced by U /B,(cid:2)where [x]B denotes the equivalence class determined by x with respect to B, i.e., [x]B = { y ∈ U | (x, y) ∈ R B }.Assume the objects are partitioned into r mutually exclusive crisp subsets {Y 1, Y 2, . . . , Y r} by the decision attributes D.Given any subset B ⊆ C and R B is the equivalence relation induced by B, then one can define the lower and upper approx-imations of the decision attributes D asR B D = {R B Y 1, R B Y 2, . . . , R B Yr},R B D = {R B Y 1, R B Y 2, . . . , R B Yr}.(cid:3)ri=1 R B Y i , it is called the positive region of D with respect to the condition attribute set B.Denoted by POSB (D) =We define a partial relation (cid:2) on the family {B | B ⊆ C} as follows: P (cid:2) Q (or Q (cid:3) P ) if and only if, for every P i ∈U /P , there exists Q j ∈ U /Q such that P i ⊆ Q j , where U /P = {P 1, P 2, . . . , Pm} and U /Q = {Q 1, Q 2, . . . , Q n} are partitionsinduced by P , Q ⊆ C , respectively [40]. In this case, we say that Q is coarser than P , or P is finer than Q . If P (cid:2) Q andU /P (cid:7)= U /Q , we say Q is strictly coarser than P (or P is strictly finer than Q ), denoted by P ≺ Q (or Q (cid:13) P ).It becomes clear that P ≺ Q if and only if, for every X ∈ U /P , there exists Y ∈ U /Q such that X ⊆ Y , and there existsX0 ∈ U /P , Y 0 ∈ U /Q such that X0 ⊂ Y 0.3. Positive approximation and its propertiesA partition induced by an equivalence relation provides a granulation world for describing a target concept [38]. Thus,a sequence of granulation worlds stretching from coarse to fine granulation can be determined by a sequence of attributesets with granulations from coarse to fine in the power set of attributes, which is called a positive granulation world. If thegranulation worlds are arranged from the fine to the coarse one, then the sequence is called converse granulation worlds[27,40].In this section, we introduce a new set-approximation approach called positive approximation and investigate some ofits important properties, in which a given set (also called a target concept in rough set theory) is approximated by a positivegranulation world. Given a decision table S = (U , C ∪ D), U /D = {Y 1, Y 2, . . . , Y r} is called a target decision, in which eachequivalence class Y i (i (cid:4) r) can be regarded as a target concept. These concepts and properties will be helpful to understandthe notion of a granulation order and set approximation under a granulation order.Definition 1. Let S = (U , C ∪ D) be a decision table, X ⊆ U and P = {R1, R2, . . . , Rn} a family of attribute sets with R1 (cid:3)R2 (cid:3) · · · (cid:3) Rn (R i ∈ 2C ). Given P i = {R1, R2, . . . , R i}, we define P i -lower approximation P i( X) and P i -upper approximationP i( X) of P i -positive approximation of X as(cid:6)(cid:3)ik=1 Rk Xk,P i(X) =P i(X) = R i X,where X1 = X and Xk = X −(cid:3)k−1j=1 R j X j , k = 2, 3, . . . , n, i = 1, 2, . . . , n.Correspondingly, the boundary of X is given asBN P i (X) = P i(X) − P i(X).Theorem 1. Let S = (U , C ∪ D) be a decision table, X ⊆ U and P = {R1, R2, . . . , Rn} a family of attribute sets with R1 (cid:3) R2 (cid:3) · · · (cid:3)Rn (R i ∈ 2C ). Given P i = {R1, R2, . . . , R i}, then ∀P i (i = 1, 2, . . . , n), we haveP i(X) ⊆ X ⊆ P i(X),P 1(X) ⊆ P 2(X) ⊆ · · · ⊆ P i(X).Fig. 1 visualizes the mechanism of the positive approximation.In Fig. 1, let P 1 = {R1} and P 2 = {R1, R2} with R1 (cid:3) R2 be two granulation orders. R1 X1 is the lower approximationof X1 obtained by the equivalence relation R1, and R2 X2 is the lower approximation of X2 obtained by the equivalenceY.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618601Fig. 1. Sketch map of the positive approximation.relation R2. Hence, P 2( X) = R1 X1 ∪ R2 X2 = R2 X . The mechanism characterizes the structure of a rough set approximation,which can be used to gradually compute the lower approximation of a target concept/decision.Theorem 2. Let S = (U , C ∪ D) be a decision table, X ⊆ U and P = {R1, R2, . . . , Rn} a family of attribute sets with R1 (cid:3) R2 (cid:3) · · · (cid:3)Rn (R i ∈ 2C ). Given P i = {R1, R2, . . . , R i}, then ∀P i (i = 1, 2, . . . , n), we haveαP 1 (X) (cid:4) αP 2 (X) (cid:4) · · · (cid:4) αP i (X),where αP i ( X) =|P i ( X)||P i ( X)| is the approximation measure of X with respect to P .In order to illustrate that the essence of the positive approximation is concentrated on the changes in the construction ofthe target concept X (equivalence classes in the lower approximation of X with respect to P i ), one can redefine P i -positiveapproximation of X by using some equivalence classes on U . The structures of P i -lower approximation P i( X) and P i -upperapproximation P i( X) of P i -positive approximation of X can be represented as follows(cid:2)S(P i(X)) = {[x]RkS(P i(X)) = {[x]R i| [x]Rk| [x]R i⊆ Rk Xk, k (cid:4) i, X1 = X, Xk+1 = Rk Xk},∩ X (cid:7)= ∅},where [x]R i represents the equivalence class including x in the partition U /R i .Example 1. Let U = {e1, e2, e3, e4, e5, e6, e7, e8}, X = {e1, e2, e3, e4, e7, e8} and U /R1 = {{e1}, {e2}, {e3, e4}, {e5, e6, e7, e8}},U /R2 = {{e1}, {e2}, {e3, e4}, {e5, e6}, {e7, e8}} be two partitions on U .Obviously, R1 (cid:3) R2 holds. Thus, one can construct two granulation orders (a family of equivalence relations) P 1 = {R1}and P 2 = {R1, R2}.By computing the positive approximation of X , one can easily obtain that(cid:5),=(cid:7)S(cid:8)P 1(X)(cid:8)P 1(X)(cid:8)P 2(X)(cid:8)P 2(X)(cid:7)(cid:7)(cid:7)SSS(cid:4){e1}, {e2}, {e3, e4}(cid:4){e1}, {e2}, {e3, e4}, {e5, e6, e7, e8}(cid:4){e1}, {e2}, {e3, e4}, {e7, e8}(cid:4){e1}, {e2}, {e3, e4}, {e7, e8}(cid:5)(cid:5).and(cid:5),===That is to say, the target concept X can be described by using granulation orders P 1 = {R1} and P 2 = {R1, R2}, respectively.Definition 2. Let S = (U , C ∪ D) be a decision table, P i = {R1, R2, . . . , R i} a family of attribute sets with R1 (cid:3) R2 (cid:3) · · · R i(R i ∈ 2C ) and U /D = {Y 1, Y 2, . . . , Y r}. Lower approximation and upper approximation of D with respect to P i are defined as(cid:2)P i D = {P i(Y 1), P i(Y 2), . . . , P i(Yr)},P i D = {P i(Y 1), P i(Y 2), . . . , P i(Yr)}.P i D is also called the positive region of D with respect to the granulation order P i , denoted by POSUP i(D) =(cid:3)rk=1 P i Yk.602Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618Theorem 3 (Recursive expression principle). Let S = (U , C ∪ D) be a decision table, X ⊆ U and P = {R 1, R2, . . . , Rn} a family ofattribute sets with R1 (cid:3) R2 (cid:3) · · · (cid:3) Rn (R i ∈ 2C ). Given P i = {R1, R2, . . . , R i}, we havePOSUP i+1(D) = POSUP i(D) ∪ POSU i+1R i+1(D),where U 1 = U and U i+1 = U − POSUP i(D).Example 2. Let S = (U , C ∪ D) be a decision table, where U = {e1, e2, e3, e4, e5, e6, e7, e8}, C = {a1, a2}, U /D = {{e1, e2,e3, e4, e7, e8}, {e5, e6}} and U /{a1} = {{e1}, {e2}, {e3, e4}, {e5, e6, e7, e8}}, U /C = {{e1}, {e2}, {e3, e4}, {e5, e6}, {e7, e8}} be twopartitions on U .Obviously, {a1} (cid:3) C holds. Thus, one can construct two granulation orders (a family of equivalence relations) P 1 = {a1}and P 2 = {a1, a2}.By computing the positive approximation of D, one can easily obtain that(D) = {e1, e2, e3, e4}, where U 1 = U ,(D) = POSU 1P 1(D) = {e5, e6, e7, e8},POSUP 1U 2 = U − POSUP 1POSU 2P 2(D) = {e5, e6}.Hence,POSUP 2(D) = {e1, e2, e3, e4, e5, e6} = POSUP 1(D) ∪ POSU 2P 2(D).That is to say, the target decision D can be positively approximated by using granulation orders P 1 and P 2 on the graduallyreduced universe, respectively. This mechanism implies the idea of the accelerator proposed in this paper for improving thecomputing performance of a heuristic attribute reduction algorithm.The dependency function (or level of consistency [5]) is used to characterize the dependency degree of an attributesubset with respect to a given decision [8,33]. Given a decision table S = (U , C ∪ D), the dependency function of conditionattributes C with respect to the decision attribute D is formally defined as γC (D) = |POSUC (D)|/|U |. Using this notation, wegive the definition of dependency function of a granulation order P with respect to D in the following.Definition 3. A dependency function involving a granulation order P and D is defined asγP (D) =|POSUP (D)||U |,where | · | denotes the cardinality of a set and 0 (cid:4) γP (D) (cid:4) 1.The dependency function reflects the granulation order P ’s power to dynamically approximate D. When γ = 1, one saysD completely depends on the granulation order P . It means that the decision can be precisely described by the informationgranules generated by the granulation order P . This dependency function can be used to measure the significance of cate-gorical attributes relative to the decision and construct a heuristic function for designing an attribute reduction algorithm.4. FSPA: feature selection based on the positive approximationEach feature selection method preserves a particular property of a given information system, which is based on a certainpredetermined heuristic function. In rough set theory, attribute reduction is about finding some attribute subsets that havethe minimal attributes and retain some particular properties. For example, the dependency function keeps the approxima-tion power of a set of condition attributes. To design a heuristic attribute reduction algorithm, three key problems should beconsidered, which are significance measures of attributes, search strategy and stopping (termination) criterion. As there aresymbolic attributes and numerical attributes in real-world data, one needs to proceed with some preprocessing. Throughattribute discretization, it is easy to induce an equivalence partition. However, the existing heuristic attribute reductionmethods are computationally intensive which become infeasible in case of large-scale data. As already noted, we do notreconstruct significance measures of attributes and design new stopping criteria, but improve the search strategies of theexisting algorithms by exploiting the proposed concept of positive approximation.4.1. Forward attribute reduction algorithmsIn rough set theory, to support efficient attribute reduction, many heuristic attribute reduction methods have been de-veloped, in which forward greedy search strategy is usually employed, cf. [19,20,22,25,26,39,52]. In this kind of attributereduction approaches, two important measures of attributes are used for heuristic functions, which are inner importanceY.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618603Fig. 2. The process of forward greedy attribute reduction algorithm.measure and outer importance measure. The inner importance measure is applicable to determine the significance of everyattribute, while the outer importance measure can be used in a forward feature selection. It is deserved to point out thateach kind of attribute reduction tries to preserve a particular property of a given decision table.In each forward greedy attribute reduction approach, starting with the attribute with the maximal inner importance,we take the attribute with the maximal outer significance into the attribute subset in each loop until this feature subsetsatisfies the stopping criterion, and then we can get an attribute reduct. Formally, a forward greedy attribute reductionalgorithm can be written as follows.Algorithm 1. A general forward greedy attribute reduction algorithm.Input: Decision table S = (U , C ∪ D);Output: One reduct red.Step 1: red ← ∅; //red is the pool to conserve the selected attributes;Step 2: Compute Siginner(ak, C, D), k (cid:4) |C|; //Siginner(ak, C, D) is the inner importance measure of the attribute ak;Step 3: Put ak into red, where Siginner(ak, C, D, U ) > 0;Step 4: While EF(red, D) (cid:7)= EF(C, D) Do //This provides a stopping criterion.{red ← red ∪ {a0}, where Sigouter(a0, red, D) = max{Sigouter(ak, red, D), ak ∈ C − red}}; //Sigouter(ak, C, D)is the outer importance measure of the attribute ak;Step 5: Return red and end.This algorithm can obtain an attribute reduct from a given decision table. Fig. 2 displays the process of attribute re-duction based on the forward greedy attribute reduction algorithm in rough set theory, which is helpful for more clearlyunderstanding the mechanism of the algorithm.Remark. Given any definition of attribute reduct and heuristic function, using the above attribute reduction framework,one can heuristically find an attribute reduct (a feature subset) that preserves a particular property of a decision table. Ifwe survey the attribute reduct from the viewpoint of rough classifiers, these attribute reduction algorithms may lead tooverfitting in the approximation of concepts, which will weaken the generalization ability of rough classifiers induced bythe attribute reducts obtained. This problem could be caused by two cases. One is that the attribute subset induced byan attribute reduction algorithm with forward greedy searching strategy may be redundant. That is to say, there are someredundant attributes in the attribute subset obtained from the definition of a given attribute reduct. The other is that thedefinition of each of attribute reductions does not take into account the generalization ability of the rough classifier inducedby the attribute reduct obtained. These two situations yield the same overfitting problem as a decision tree does when thetree has too long paths. Hence, it is very desirable to solve the overfitting problem of feature selection for learning a roughclassifier in the framework of rough set theory. This issue will be addressed in future work.4.2. Four representative significance measures of attributesFor efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory,see [19,20,22,25,26,39,52,54–56]. For convenience, as was pointed out in the introduction part of this paper, we only focuson the four representative attribute reduction methods here.Given a decision table S = (U , C ∪ D), one can obtain the condition partition U /C = { X1, X2, . . . , Xm} and the decisionpartition U /D = {Y 1, Y 2, . . . , Yn}. Through these notations, in what follows we review four types of significance measures ofattributes.The idea of attribute reduction using positive region was first originated by Grzymala-Busse in Refs. [9] and [10], and thecorresponding algorithm ignores the additional computation of choice of significant attributes. Hu and Cercone proposeda heuristic attribute reduction method, called positive-region reduction (PR), which remains the positive region of targetdecision unchanged [19]. In this method, the significance measures of attributes are defined as follows.604Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618Definition 4. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ B. The significance measure of a in B is defined asSiginner1(a, B, D) = γB (D) − γB−{a}(D),where γB (D) = |POSB (D)||U |.Definition 5. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ C − B. The significance measure of a in B is defined asSigouter1(a, B, D) = γB∪{a}(D) − γB (D).As Shannon’s information entropy was introduced to search reducts in classical rough set model [52], Wang et al. used itsconditional entropy to calculate the relative attribute reduction of a decision information system [54]. In fact, several authorsalso have used variants of Shannon’s entropy to measure uncertainty in rough set theory and construct heuristic algorithm ofattribute reduction. Here we only select the attribute reduction algorithm in the literature [54] as their representative. Thisreduction method remains the conditional entropy of target decision unchanged, denoted by SCE, in which the conditionalentropy reads asH(D|B) = −m(cid:9)i=1p(Xi)n(cid:9)j=1p(Y j| Xi) log(cid:7)(cid:8)p(Y j| Xi),where p( Xi) = | Xi |expressed in the following way.|U | and p(Y j| Xi) = | Xi ∩Y j || Xi |. Using the conditional entropy, the definitions of the significance measures areDefinition 6. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ B. The significance measure of a in B is defined asDefinition 7. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ C − B. The significance measure of a in B is defined asSiginner2(a, B, D) = HD|B − {a}− H(D|B).Sigouter2(a, B, D) = H(D|B) − HD|B ∪ {a}(cid:8).As was pointed out in [25], Shannon’s entropy is not a fuzzy entropy, and cannot measure the fuzziness of a roughdecision in rough set theory. Hence, Liang et al. defined another information entropy and its conditional entropy to measurethe uncertainty of an information system and applied the proposed entropy to reduce redundant features [25,26]. Thisreduction method can preserve the conditional entropy of a given decision table, denoted here by LCE. The conditionalentropy used in the study is defined asm(cid:9)n(cid:9)E(D|C) =i=1j=1|Y j ∩ Xi||U ||Y cj|∩ X ci|U |.The corresponding significance measures are listed as follows.Definition 8. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ B. The significance measure of a in B is defined as(cid:7)(cid:7)(cid:8)(cid:7)(cid:8)(cid:7)Definition 9. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ C − B. The significance measure of a in B is defined asSiginner3(a, B, D) = ED|B − {a}− E(D|B).Sigouter3(a, B, D) = E(D|B) − ED|B ∪ {a}(cid:8).Based on the intuitionistic knowledge content nature of information gain, Qian and Liang in [39] presented the conceptof combination entropy for measuring the uncertainty of information systems and used its conditional entropy to obtaina feature subset. This reduction method can obtain an attribute subset that possesses the same number of pairs of theelements which can be distinguished each other as the original decision table, denoted here by CCE. The following definitionof the conditional entropy is consideredCE(D|C) =(cid:10)m(cid:9)i=1| Xi||U |C 2| Xi |C 2|U |−n(cid:9)j=1| Xi ∩ Y j||U |C 2| Xi ∩Y j |C 2|U |(cid:11),| Xi | = | Xi |×(| Xi |−1)where C 22equivalence class Xi .denotes the number of the pairs of the objects which are not distinguishable each other in theThe conditional entropy also can be used to construct the corresponding significance measures of attributes in decisiontables.Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618605Fig. 3. The relationship between the core and all attribute reducts.Definition 10. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ B. The significance measure of a in B is defined asSiginner4(a, B, D) = CED|B − {a}− CE(D|B).(cid:7)Sigouter4(a, B, D) = CE(D|B) − CED|B ∪ {a}.(cid:8)(cid:8)(cid:7)Definition 11. Let S = (U , C ∪ D) be a decision table, B ⊆ C and ∀a ∈ C − B. The significance measure of a in B is defined asAll the definitions used above are used to select an attribute in a heuristic attribute reduction algorithm. For a givendecision table, the intersection of all attribute reducts is said to be indispensable and is called the core. Each attribute inthe core must be in every attribute reduct of the decision table. The core may be an empty set. The relationship betweenthe core and all attribute reducts can be displayed by Fig. 3.The above four kinds of significance measures can be used to find the core attributes. The following theorem is of interestwith this regard.Theorem 4. (See [26,33,39,54].) Let S = (U , C ∪ D) be a decision table and a ∈ C . If Siginnera core attribute of S in the context of type (cid:5).(cid:5) (a, C, D) > 0 ((cid:5) = {1, 2, 3, 4}), then a isFrom the definition of the core, one can see that each attribute in the core must be in every attribute reduct of the(cid:5) (a, C, D) = 0 ((cid:5) = {1, 2, 3, 4}), then one still can find at least one attribute(cid:5) (a, C, D) > 0 ((cid:5) = {1, 2, 3, 4}), then the attribute a is indispensable in all attributedecision table. It is well known that, if Siginnerreduct when a is deleted. If Siginnerreducts. Therefore, the attribute a must be a core attribute of S in the context of type (cid:5).In a heuristic attribute reduction algorithm, based on the above theorem, one can find an attribute reduct by graduallyadding selected attributes to the core attributes.4.3. Rank preservation of significance measures of attributesAs mentioned above, each of significance measures of attributes provides some heuristics to guide the mechanism offorward searching a feature subset. Unlike the discernibility matrix, the computational time of the heuristic algorithms hasbeen largely reduced when only one attribute reduct is needed. Nevertheless, these algorithms still could be very timeconsuming. To introduce an improved strategy of heuristic attribute reductions, we concentrate on the rank preservation ofthe four significance measures of attributes based on the positive approximation encountered in a decision table.Firstly, we investigate the rank preservation of significance measures of attributes based on the dependency measure. For(cid:5) (a, B, D, U ) ((cid:5) = {1, 2, 3, 4}), whichmore clear representation, we denote the significance measure of an attribute by Sigouterdenotes the value of the significance measure on the universe U . One can prove the following theorem of rank preservation.Theorem 5. Let S = (U , C ∪ D) be a decision table, B ⊆ C and U(cid:16)).Sigouter1(b, B, D, U ), then Sigouter(cid:16)) (cid:5) Sigouter(b, B, D, U(a, B, D, U11(cid:16) = U − POSUB (D). For ∀a, b ∈ C − B, if Sigouter1(a, B, D, U ) (cid:5)Proof. From the definition of Sigouterfunction γB (D) = |POSB (D)|Therefore, we have. Since U|U |1(a, B, D) = γB∪{a}(D) − γB (D), we know that its value only depends on the dependencyB (D) = ∅ and POSUB (D).B (D), one can know POSUB∪{a}(D) = POSUB∪{a}(D) − POSU(cid:16) = U − POSU(cid:16)(cid:16)Sigouter1Sigouter1(a, B, D, U )(a, B, D, U (cid:16))=B∪{a}(D) − γ Uγ UB∪{a}(D) − γ U (cid:16)γ U (cid:16)B (D)B (D)606Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618===(cid:16)||U|U |(cid:16)||U|U |(cid:16)||U|U ||POSU|POSU (cid:16)|POSU|POSUB∪{a}(D)| − |POSUB∪{a}(D)| − |POSU (cid:16)B∪{a}(D)| − |POSUB∪{a}(D)| − |POSUB (D)|B (D)|B (D)|B (D)|.Because(cid:16)||U|U | (cid:5) 0 and if Sigouter1pletes the proof. (cid:2)(a, B, D, U ) (cid:5) Sigouter1(b, B, D, U ), then Sigouter1(a, B, D, U(cid:16)) (cid:5) Sigouter1(b, B, D, U(cid:16)). This com-Secondly, we research the rank preservation of significance measures of attributes based on the Shannon’s conditionalentropy. The following theorem elaborates on the rank preservation of this measure.Theorem 6. Let S = (U , C ∪ D) be a decision table, B ⊆ C and U(cid:16)).Sigouter2(b, B, D, U ), then Sigouter(cid:16)) (cid:5) Sigouter(b, B, D, U(a, B, D, U22(cid:16) = U − POSUB (D). For ∀a, b ∈ C − B, if Sigouter2(a, B, D, U ) (cid:5)Proof. Let U /B = { X1, X2, . . . , X p, X p+1, . . . , Xm}, U /D = {Y 1, Y 2, . . . , Yn}, where X p+1, X p+2, . . . , Xm ⊆ POSUB (D). Since foreach equivalence class X ⊆ POSB (D), there exists a decision class Y such that X ∩ Y = X . Let us denote Shannon’s conditionalentropy in the universe U by H U (D|B). Then it follows thatH U (D|B) = −m(cid:9)p(Xi)p(Y j| Xi) log(cid:7)(cid:8)p(Y j| Xi)n(cid:9)j=1i=1(cid:10)p(cid:9)= −(cid:10)= −(cid:10)= −i=1p(cid:9)i=1p(cid:9)i=1p(Xi)n(cid:9)j=1p(Y j| Xi) log(cid:7)(cid:8)p(Y j| Xi)+m(cid:9)p(Xi)i=p+1n(cid:9)j=1p(Y j| Xi) log(cid:7)(cid:8)p(Y j| Xi)(cid:11)(cid:11)| Xi||U || Xi||U |n(cid:9)j=1n(cid:9)j=1| Xi ∩ Y j|| Xi|log| Xi ∩ Y j|| Xi|+| Xi ∩ Y j|| Xi|log| Xi ∩ Y j|| Xi|+m(cid:9)i=p+1m(cid:9)i=p+1| Xi||U || Xi||U |n(cid:9)j=1n(cid:9)j=1| Xi ∩ Y j|| Xi|log| Xi ∩ Y j|| Xi|(cid:11)| Xi|| Xi| log| Xi|| Xi|= −= −p(cid:9)i=1(cid:16)||U|U || Xi||U |p(cid:9)i=1n(cid:9)j=1| Xi||U (cid:16)|| Xi ∩ Y j|| Xi|log| Xi ∩ Y j|| Xi|n(cid:9)j=1| Xi ∩ Y j|| Xi|log| Xi ∩ Y j|| Xi|=(cid:16)(cid:16)||U|U | H U(D|B).Therefore, we have thatSigouter2(a, B, D, U(cid:16)) (cid:5) Sigouter2(a,B,D,U )(a,B,D,U (cid:16))Sigouter2Sigouter2(b, B, D, U(cid:16)). This completes the proof. (cid:2)= |U(cid:16)||U | . Thus,if Sigouter2(a, B, D, U ) (cid:5) Sigouter2(b, B, D, U ), ∀a, b ∈ C − B,thenThen, we obtain the rank preservation of significance measures of attributes based on Liang’s conditional entropy, whichis given by the following theorem.Theorem 7. Let S = (U , C ∪ D) be a decision table, B ⊆ C and U(cid:16)).Sigouter3(b, B, D, U ), then Sigouter(cid:16)) (cid:5) Sigouter(b, B, D, U(a, B, D, U33(cid:16) = U − POSUB (D). For ∀a, b ∈ C − B, if Sigouter3(a, B, D, U ) (cid:5)Proof. Suppose U /B = { X1, X2, . . . , X p, X p+1, . . . , Xm}, U /D = {Y 1, Y 2, . . . , Yn}, where X p+1, X p+2, . . . , Xm ⊆ POSUB (D). Foreach equivalence class X ⊆ POSB (D), there exists a decision class Y such that X ∩ Y = X , i.e., X ⊆ Y . We denote the Liang’sconditional entropy in the universe U by E U (D|B). Then, one hasY.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618607E U (D|B) =======m(cid:9)n(cid:9)i=1m(cid:9)j=1n(cid:9)i=1p(cid:9)j=1n(cid:9)i=1p(cid:9)j=1n(cid:9)i=1p(cid:9)j=1n(cid:9)i=1j=1|Y j ∩ Xi||U ||Y cj|− X ci|U ||Y j ∩ Xi||U || Xi − Y j||U ||Y j ∩ Xi||U || Xi − Y j||U |+|Y j ∩ Xi||U || Xi − Y j||U |+m(cid:9)n(cid:9)i=p+1m(cid:9)j=1n(cid:9)i=p+1j=1|Y j ∩ Xi||U || Xi − Y j||U ||Y j ∩ Xi||U ||∅||U ||Y j ∩ Xi||U || Xi − Y j||U |p(cid:9)n(cid:9)i=1j=1|Y j ∩ Xi||U (cid:16)|| Xi − Y j||U (cid:16)|(cid:16)E U(D|B).(cid:16)|2|U|U |2(cid:16)|2|U|U |2Hence, we haveSigouter3(a, B, D, U(a,B,D,U )(a,B,D,U (cid:16))Sigouter3Sigouter3(cid:16)) (cid:5) Sigouter3(b, B, D, U(cid:16)). This completes the proof. (cid:2)= |U(cid:16)|2|U |2 . Therefore, for any a, b ∈ C − B, if Sigouter3(a, B, D, U ) (cid:5) Sigouter3(b, B, D, U ), thenFinally, similar to the above three theorems, one can show that significance measures of attributes based on the condi-tional combination entropy also possesses the property of rank preservation, which is shown as follows.Theorem 8. Let S = (U , C ∪ D) be a decision table, B ⊆ C and U(cid:16)).Sigouter4(b, B, D, U ), then Sigouter(cid:16)) (cid:5) Sigouter(b, B, D, U(a, B, D, U44(cid:16) = U − POSUB (D). For ∀a, b ∈ C − B, if Sigouter4(a, B, D, U ) (cid:5)Proof. Suppose U /B = { X1, X2, . . . , X p, X p+1, . . . , Xm}, U /D = {Y 1, Y 2, . . . , Yn}, where X p+1, X p+2, . . . , Xm ⊆ POSUB (D).Hence, for any equivalence class X ⊆ POSB (D), there exists a decision class Y such that X ∩ Y = X , i.e., X ⊆ Y . Denotethe conditional combination entropy in the universe U by CEU (D|B). Then, one has(cid:10)m(cid:9)CEU (D|B) =(cid:11)(cid:11)(cid:11)(cid:11)++−−−−n(cid:9)j=1n(cid:9)j=1n(cid:9)j=1n(cid:9)j=1| Xi ∩ Y j||U || Xi ∩ Y j||U || Xi ∩ Y j||U || Xi ∩ Y j||U |C 2| Xi ∩Y j |C 2|U |C 2| Xi ∩Y j |C 2|U |C 2| Xi ∩Y j |C 2|U |C 2| Xi ∩Y j |C 2|U |(cid:10)m(cid:9)i=p+1m(cid:9)(cid:10)i=p+1(cid:11)| Xi||U (cid:16)|C 2| Xi |C 2|U (cid:16)|−n(cid:9)j=1| Xi ∩ Y j||U (cid:16)|C 2| Xi ∩Y j |C 2|U (cid:16)|(cid:11)C 2| Xi ∩Y j |C 2|U |(cid:11)| Xi||U || Xi||U |C 2| Xi |C 2|U |C 2| Xi |C 2|U |−−n(cid:9)j=1n(cid:9)j=1| Xi ∩ Y j||U || Xi||U |C 2| Xi |C 2|U || Xi||U || Xi||U || Xi||U |C 2| Xi |C 2|U |C 2| Xi |C 2|U |C 2| Xi |C 2|U |i=1p(cid:9)(cid:10)i=1p(cid:9)(cid:10)i=1p(cid:9)(cid:10)C 2| Xi |C 2|U |(cid:10)p(cid:9)i=1| Xi||U ||U (cid:16)|i=1(cid:16)|C 2|U|U |C 2|U |(cid:16)|C 2|U|U |C 2|U ||U (cid:16)|=====(cid:16)CEU(D|B).In the sequel we obtainthen Sigouter4(a, B, D, USigouter4Sigouter4(cid:16)) (cid:5) Sigouter=(a,B,D,U )(a,B,D,U (cid:16))(b, B, D, U4(cid:16)|(cid:16)|C 2|U|U|U |C 2(cid:16)). This completes the proof. (cid:2)|U |. Therefore, for any a, b ∈ C − B, if Sigouter(a, B, D, U ) (cid:5) Sigouter4(b, B, D, U ),4608Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618From Theorems 5–8, one can see that they can be uniformly represented by the following theorem.Theorem 9 (Rank preservation). Let S = (U , C ∪ D) be a decision table, B ⊆ C and USigouter(cid:5) (b, B, D, U ) ((cid:5) = {1, 2, 3, 4}), then Sigouter(cid:5) (a, B, D, U ) (cid:5) Sigouter(cid:5) (a, B, D, U(cid:16)) (cid:5) Sigouter(cid:16) = U − POSU(cid:5) (b, B, D, U(cid:16)).B (D). For ∀a, b ∈ C − B, ifFrom this theorem, one can see that the rank of attributes in the process of attribute reduction will remain unchanged af-ter reducing the lower approximation of positive approximation. This mechanism can be used to improve the computationalperformance of a heuristic attribute reduction algorithm, while retaining the same selected feature subset.Besides the above four kind attribute reducts, as we know, there are many other different kind of attribute reducts. Wedo not discuss the rank preservation of each existing reducts taking into account the compact of the paper. In fact, rankpreservation of attributes selected is based on the monotonicity of positive region of target decision in a heuristic featureselection process. In other words, the rank of attributes selected will be unchanged when the scale of positive region oftarget decision increases as the number of selected attributes becomes bigger.4.4. Attribute reduction algorithm based on the positive approximationThe objective of rough set-based feature selection is to find a subset of attributes which retains some particular proper-ties as the original data and without redundancy. In fact, there may be multiple reducts for a given decision table. It hasbeen proven that finding the minimal reduct of a decision table is a NP hard problem. When only one attribute reduct isneeded, based on the significance measures of attributes, some heuristic algorithms have been proposed, most of which aregreedy and forward search algorithms. These search algorithms start with a nonempty set, and keep adding one or severalattributes of high significance into a pool each time until the dependence has not been increased.From the discussion in the previous subsection, one knows that the rank preservation of attributes in the context ofthe positive approximation. Hence, we can construct an improved forward search algorithm based on the positive approx-imation, which is formulated as follows. In this general algorithm framework, we denote the evaluation function (stopcriterion) by EFU (B, D) = EFU (C, D). For example, if one adopts Shannon’s conditional entropy, then the evaluation functionis H U (B, D) = H U (C, D). That is to say, if EFU (B, D) = EFU (C, D), then B is said to be an attribute reduct.Algorithm Q1. A general improved feature selection algorithm based on the positive approximation (FSPA).Input: Decision table S = (U , C ∪ D);Output: One reduct red.Step 1: red ← ∅; //red is the pool to conserve the selected attributes;Step 2: Compute Siginner(ak, C, D, U ), k (cid:4) |C|;Step 3: Put ak into red, where Siginner(ak, C, D, U ) > 0;// These attributes form the core of the given decision table;Step 4: i ← 1, R1 = red, P 1 = {R1} and U 1 ← U ;Step 5: While EFU i (red, D) (cid:7)= EFU i (C, D) Do(D),{Compute the positive region of positive approximation POSUP iU i = U − POSUP ii ← i + 1,red ← red ∪ {a0}, where Sigouter(a0, red, D, U i) = max{Sigouter(ak, red, D, U i), ak ∈ C − red},R i ← R i ∪ {a0},P i ← {R1, R2, . . . , R i}};(D),Step 6: Return red and end.(cid:12)|C|i=1|U i|(|C| − i + 1)). Thus the time complexity of FSPA is O (|U ||C| +Computing the significance measure of an attribute Siginner(ak, C, D, U ) is one of the key steps in FSPA. Xu et al.in [57] gave a quick algorithm with time complexity O (|U |). Hence, the time complexity of computing the core inStep 2 is O (|C||U |). In Step 5, we begin with the core and add an attribute with the maximal significance into theset in each stage until finding a reduct. This process is called a forward reduction algorithm whose time complexity is|U i|(|C| − i + 1)). However, the time com-O (|U |(|C| − i + 1)). Obviously, the time complexity of FSPA is muchplexity of a classical heuristic algorithm is O (|U ||C| +lower than that of each of classical heuristic attribute reduction algorithms. Hence, one can draw a conclusion that the gen-eral feature selection algorithm based on the positive approximation (FSPA) may significantly reduce the computational timefor attribute reduction from decision tables. To stress these findings, the time complexity of each step in original algorithmsand FSPA is shown as Table 1.(cid:12)|C|i=1(cid:12)|C|i=1To support the substantial contribution of the general improved attribute reduction algorithm based on the positiveapproximation, we summarize three factors of speedup of this accelerator as follows.(1) One can select the same attribute in each loop of the improved algorithm and that of the original one. This provides arestriction of keeping the result of an attribute reduction algorithm.Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618609Step 2O (|C||U |)O (|C||U |)Step 3O (|C|)O (|C|)Step 5(cid:12)|C|i=1(cid:12)|C|i=1O (O (|U |(|C| − i + 1))|U i |(|C| − i + 1))Other stepsConstantConstantTable 1The complexities description.AlgorithmsEach of original algorithmsFSPATable 2Data sets description.123456789Data setsMushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata2000Cases5644958358319668337658000200005822FeaturesClasses22934369359168522622197262(2) Computational time of significance measures of attributes is significantly reduced, which is because that it is onlyconsidered on the gradually reduced universe. It is one key factor of the accelerated algorithm.(3) Time consumption of computing the stopping criterion is also significantly reduced via gradually decreasing the size ofdata set. This is the other important factor of the improved algorithm.Based on the above three speedup factors, we draw such a conclusion that: the general modified algorithm can signif-icantly reduce the computational time of each existing attribute reduction algorithm while producing the same attributereducts and classification accuracies as those coming from the original ones.It is deserved to point out that each of these modified algorithms can not solve the overfitting problem in the approxi-mation of concepts and improve the generalization ability of the rough classifier induced by the obtained attribute reduct.The general improved algorithm only devotes to largely reducing the computational time of original attribute reductionalgorithms (see Algorithm 1).4.5. Time efficiency analysis of algorithmsMany heuristic attribute reduction methods have been developed for symbolic data [19,20,22,25,26,39,52,54–56]. Thefour heuristic algorithms mentioned in Section 4.2 are very representative. The objective of the following experimentsis to show the time efficiencies of the proposed general framework for selecting a feature subset. The data used in theexperiments are outlined in Table 2, which were all downloaded from UCI Repository of machine learning databases.In this subsection, in order to compare the above four representative attribute reduction algorithms (PR, SCE, LCE andCCE) with the modified ones, we employ nine UCI data sets from Table 2 to verify the performance of time reductionof the modified algorithms, which are all symbolic data (Shuttle and Ticdata2000 are preprocessed by discretization withentropy). In these nine data sets, Mushroom and Breast-cancer-wisconsin are two data sets with missing values. For uniformtreatment of all data sets, we remove the objects with missing values.From the rank preservation of significance measures of attributes, we know that each modified attribute reduction algo-rithm can obtain the same attribute reduct as its original version. Therefore, in the following experiments, we only considerattribute reducts obtained and computational time, and do not compare their classification accuracies.For any heuristic attribute reduction algorithm in rough set theory, the computation of classification is the first keystep. To date, many quick classification algorithms have been proposed for improving the efficiency of attribute reduction.However, the computation of classification is only the pretreatment of data in any heuristic attribute reduction algorithm.Hence, for convenient comparison, we will adopt the classification algorithm with the time complexity O (|C||U |) [58]. Inwhat follows, we apply each of the original algorithms along with its modified version for searching attribute reducts. Todistinguish the computational times, we divide each of these nine data sets into twenty parts of equal size. The first partis regarded as the 1st data set, the combination of the first part and the second part is viewed as the 2nd data set, thecombination of the 2nd data set and the third part is regarded as the 3rd data set, . . . , the combination of all twenty partsis viewed as the 20th data set. These data sets can be used to calculate time used by each of the original attribute reductionalgorithms and the corresponding modifications one and show it vis-a-vis the size of universe. These algorithms are runon a personal computer with Windows XP and Inter(R) Core(TM)2 Quad CPU Q9400, 2.66 GHz and 3.37 GB memory. Thesoftware being used is Microsoft Visual Studio 2005 and Visual C #.610Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618Table 3The time and attribute reduction of the algorithms PR and FSPA-PR.Data setsOriginal featuresPR algorithmSelected featuresMushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata200022934369359168538102941041124Time (s)24.87500.35940.843828.03130.12500.6563906.0625282.6406886.4531FSPA-PR algorithmSelected features38102941041124Time (s)20.45310.31250.437521.57810.09380.4219712.2500112.6250296.3750Fig. 4. Times of PR and FSPA-PR versus the size of data.4.5.1. PR and FSPA-PRIn the sequence of experiments, we compare PR with FSPA-PR on the nine real world data sets shown in Table 2. Theexperimental results of these nine data sets are shown in Table 3 and Fig. 4. In each of these sub-figures, the x-coordinatepertains to the size of the data set (the 20 data sets starting from the smallest one), while the y-coordinate concerns thecomputing time. Table 2 shows the comparisons of selected features and computational time with original algorithm PRand the accelerated algorithm FSPA-PR on nine data sets. While Fig. 4 displays more detailed change trend of each of twoalgorithms with size of data set becoming increasing.It is easy to note from Table 3 and Fig. 4 that the computing time of each of these two algorithms increases withthe increase of the size of data. Nevertheless this relationship is not strictly monotonic. For example, as the size of data setvaries from the 18th to the 19th in sub-figure (f), the computing time dropped. We observe the same effect in sub-figures (c)and (e). One could envision that this situation must have occurred because different numbers of features selected.Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618611As one of the important advantages of the FSPA, as shown in Table 3 and Fig. 4, we see that the modified algorithms aremuch more faster than their original counterparts on the basis of selecting the same feature subset. Sometimes, the effectof this reduction can reduce over half the computational time. For example, the reduced time achieves 170.0156 secondson the data set Letter-recognition, while the reduced time is 590.0781 seconds on the data set Ticdata200. Furthermore thedifferences are profoundly larger when the size of the data set increases. Owing to the rank preservation of significancemeasures of attributes, the feature subset obtained by each of the modified algorithm is the same as the one produced bythe original algorithm.4.5.2. SCE and FSPA-SCEIt is well known that, the attribute reduct induced by Shannon’s information entropy keeps the probabilistic distributionof original data set, which is based on a more strict definition of attribute reduct. Hence, the attribute reduct obtained bythis approach is often much longer than one induced by the positive-region reduction.In what follows, we compare SCE with FSPA-SCE on those nine real world data sets shown in Table 2 from computationaltime and selected feature subsets. Table 4 presents the comparisons of selected features and computational time withoriginal algorithm SCE and the accelerated algorithm FSPA-SCE on nine data sets. While Fig. 5 gives more detailed changetrendline of each of two algorithms with size of data set becoming increasing.From Table 4 and Fig. 5, it is easy to see that the modified algorithms is consistently faster than their original coun-terparts. Sometimes, the reduced time can almost achieves seven-eighths of the original computational time. For example,the reduced time achieves 4275.4531 seconds on the data set Letter-recognition, and the reduced time achieves 7109.7657seconds on the data set Ticdata2000. In particular, the feature subset obtained by each of the modified algorithm is thesame as the one produced by the original algorithm, which benefits from the rank preservation of significance measures ofattributes based on the positive approximation. Furthermore the differences are profoundly larger when the size of the dataset increases. Hence, attribute reduction based on the accelerator should be a good solution.4.5.3. LCE and FSPA-LCEThe attribute reduct induced by Liang’s information entropy also keeps the probabilistic distribution of original data set,which is based on a more strict definition of attribute reduct. The attribute reduct obtained by this approach is often muchlonger than one induced by the positive-region reduction.In the following experiments, we will compare LCE with FSPA-LCE on the nine real world data sets shown in Table 2.The comparisons of selected features and computational time with original algorithm SCE and the accelerated algorithmFSPA-SCE on nine data sets are shown in Table 5, and more detailed change trendline of each of two algorithms with sizeof data set becoming increasing are given in Fig. 6.As one of the important advantages of the FSPA, as shown in Table 5 and Fig. 6, we see that the modified algorithmsare much more faster than their original counterparts. Furthermore the differences are profoundly larger when the size ofthe data set increases. Sometimes, the computational time of the modified algorithm only is almost one-fifteenths of thecomputational time of algorithm LCE. On the data set Ticdata2000, for example, FSPA-LCE only needs 1805.5625 seconds,while LCE uses 27962.6250 seconds. Like FSPA-PR and FSPA-SCE, the attribute reduct obtained by the modified algorithmFSPA-LCE is the same as the one proceed by the original algorithm LCE owing to the rank preservation of significancemeasures of attributes.4.5.4. CCE and FSPA-CCEFinally, we compare CCE with FSPA-CCE on the nine real world data sets shown in Table 2. In Table 6, it is shown thatthe comparisons of selected features and computational time with original algorithm SCE and the accelerated algorithmFSPA-SCE on nine data sets. In Fig. 7, we display more detailed change trendline of each of two algorithms with size of dataset becoming increasing. Similarly, in each of these sub-figures (a)–(i), the x-coordinate pertains to the size of the data set(the 20 data sets starting from the smallest one), while the y-coordinate concerns the computing time.From Table 6 and Fig. 7, it is easy to see that the modified algorithm is consistently faster than its original counterpart.Sometimes, the reduced time can be over seven-eighths of the original computational time. For example, the reduced timeachieves 7213.4688 seconds on the data set Ticdata2000 and the reduced time achieves 4507.9062 seconds on the data setLetter-recognition. In particular, the feature subset obtained by the modified algorithm is the same as the one produced bythe original algorithm, which is guaranteed by the rank preservation of significance measures of attributes based on thepositive approximation. Furthermore the differences are profoundly larger when the size of the data set increases. One cansay that attribute reduction based on the accelerator should be a good solution.As one of the important advantages of the FSPA, as shown in Table 6 and Fig. 7, we see that the modified algorithm ismuch faster than its original counterpart. Furthermore the differences are profoundly larger when the size of the data setincreases. Owing to the rank preservation of significance measures of attributes, the feature subset obtained by each of themodified algorithm is the same as the one produced by the original algorithm.4.6. Stability analysis of algorithmsThe stability of a heuristic attribute reduction algorithm determines the stability of its classification accuracy. The ob-jective of this suite of experiments is to compare the stability of the computing time and attribute reduction of each of612Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618Table 4The time and attribute reduction of the algorithms SCE and FSPA-SCE.Data setsOriginal featuresSCE algorithmFSPA-SCE algorithmSelected featuresTime (s)Selected featuresTime (s)MushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata200022934369359168548112941041124162.64064.50005.3125149.62501.34384.359412665.39067015.70318153.656348112941041124159.59383.10941.9844105.98440.84381.765610153.17192740.25001043.8906Fig. 5. Times of SCE and FSPA-SCE versus the size of data.the modified algorithms with those obtained when running the original methods, We use the nine real-world data sets asshown in Table 2.In the experiments, in order to evaluate the stability of feature subset selected with 10-fold cross validation, we introduceseveral definitions and necessary notations. Let X1, X2, . . . , X10 be the 10 data sets coming from a given universe U . Wedenote the reduct induced by the universe U by C0. The reducts induced by the data set Xi will be denoted by Ci (i (cid:4) 10),respectively. To measure the difference of two reducts Ci and C j , we use the following distance:D(Ci, C j) = 1 −|Ci ∩ C j||Ci ∪ C j|.Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618613Table 5The time and attribute reduction of the algorithms LCE and FSPA-LCE.Data setsOriginal featuresLCE algorithmFSPA-LCE algorithmSelected featuresTime (s)Selected featuresTime (s)MushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata200022934369359168548102951041224300.21888.734410.45311156.12503.12509.843824883.625015176.765627962.625048102951041224294.00005.78133.7500191.12501.67193.218820228.39065558.78131805.5625Fig. 6. Times of LCE and FSPA-LCE versus the size of data.Next we calculate the mean value of the 10 distances:μ = 110(cid:13)10(cid:9)i=11 −|Ci ∩ C0||Ci ∪ C0|(cid:14),where C0 is the reduct induced by the universe U .This standard deviation(cid:15)(cid:16)(cid:16)(cid:17) 110σ =(cid:8)D(Ci, C0) − μ210(cid:9)(cid:7)i=1614Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618Table 6The time and attribute reduction of the algorithms CCE and FSPA-CCE.Data setsOriginal featuresCCE algorithmFSPA-CCE algorithmSelected featuresTime (s)Selected featuresTime (s)MushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata20002293436935916854810294941124166.92196.76565.8281149.75001.35944.578113718.87507118.26568262.04694810294941124159.64063.14062.2656105.75000.89061.984410948.92192610.35941048.5781Fig. 7. Times of CCE and FSPA-CCE versus the size of data.is used to characterize the stability of the reduct result induced by a heuristic attribute reduction algorithm. The lower thevalue of the standard deviation, the higher the stability of the algorithm. Similarly, we also can use the standard deviationto evaluate the stability of the computing time.As before we used the same heuristic attribute reduction algorithms along with their modifications. The results reportedin Tables 7–10 are obtained using the 10-fold cross validation.Table 7 reveals that FSPA-PR comes with a far lower mean time and the standard deviation than the ones produced bythe original PR. The FSPA-PR’s stability is the same as the one reported for the PR. In other words, as an accelerator forattribute reduction, the positive approximation can be used to significantly reduce the time consumption of the algorithmPR. The much smaller standard deviation implies that the modified algorithm FSPA-PR exhibits a far better robustness thanthe original PR. We also note that the modified algorithm has not affected the stability of reducts induced by the originalmethod (we obtained the same attribute reduct on the same data set). The mechanism can be well interpreted by theY.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618615Table 7The stabilities of the time and attribute reduction of algorithms PR and FSPA-PR.Data setsPR’s timeMushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata200016.8359 ± 0.22460.3234 ± 0.02220.8234 ± 0.049425.0781 ± 4.34000.1156 ± 0.01040.6344 ± 0.0788778.6959 ± 29.4587224.1219 ± 7.3887698.1016 ± 54.8386FSPA-PR’s time14.8438 ± 0.21300.2391 ± 0.02620.3922 ± 0.010916.2438 ± 0.22320.0813 ± 0.00940.3891 ± 0.0331551.6750 ± 10.677090.5797 ± 1.5252248.8391 ± 6.5261Table 8The stabilities of the time and attribute reduction of algorithms SCE and FSPA-SCE.Data setsSCE’s timeFSPA-SCE’s timeMushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata2000130.6234 ± 0.98703.8359 ± 0.06144.0500 ± 0.3197126.7734 ± 15.77521.2156 ± 0.08943.7234 ± 0.39199749.1705 ± 308.81285891.5906 ± 181.04427107.3904 ± 105.7970126.1625 ± 0.88732.5045 ± 0.06171.6266 ± 0.042283.2891 ± 0.95010.7500 ± 0.06771.4188 ± 0.06558158.8490 ± 209.56852282.8141 ± 73.0362861.2000 ± 9.7081Table 9The stabilities of the time and attribute reduction of algorithms LCE and FSPA-LCE.Data setsLCE’s timeMushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata2000241.9891 ± 1.34257.3328 ± 0.06018.2875 ± 0.6289228.9547 ± 27.49342.5969 ± 0.04937.9094 ± 0.494917717.9594 ± 391.462812334.2729 ± 80.650419582.6515 ± 385.2873FSPA-LCE’s time236.0313 ± 1.68684.7531 ± 0.10073.0938 ± 0.0617154.4984 ± 2.04171.4031 ± 0.05542.7109 ± 0.174614392.4496 ± 99.21634252.5578 ± 71.40541463.7391 ± 14.5646Table 10The stabilities of the time and attribute reduction of algorithms CCE and FSPA-CCE.Data setsCCE’s timeFSPA-CCE’s timeMushroomTic-tac-toeDermatologyKr-vs-kpBreast-cancer-wisconsinBackup-large.testShuttleLetter-recognitionTicdata2000133.9672 ± 0.93313.8391 ± 0.02974.6469 ± 0.3029130.0047 ± 17.56681.1969 ± 0.08653.8016 ± 0.31559564.8752 ± 68.53685956.0833 ± 43.78666726.4778 ± 42.1287129.3531 ± 1.23432.5172 ± 0.04391.8016 ± 0.033586.3641 ± 0.92970.7406 ± 0.02981.5875 ± 0.10187440.3281 ± 25.00012171.0000 ± 36.5273859.4672 ± 10.7790PR’s stability0.0000 ± 0.00000.0000 ± 0.00000.2142 ± 0.16920.0675 ± 0.06520.1733 ± 0.27360.4187 ± 0.18300.0250 ± 0.07500.2222 ± 0.20200.2058 ± 0.0862SCE’s stability0.0000 ± 0.00000.1111 ± 0.11110.5312 ± 0.10000.0675 ± 0.06520.3562 ± 0.30990.3599 ± 0.25210.0250 ± 0.07500.1689 ± 0.18230.2485 ± 0.0830LCE’s stability0.0000 ± 0.00000.1778 ± 0.08890.1852 ± 0.17830.0675 ± 0.06520.2333 ± 0.15280.1617 ± 0.16300.0250 ± 0.07500.1914 ± 0.14360.1744 ± 0.1192CCE’s stability0.0000 ± 0.00000.1778 ± 0.08890.2735 ± 0.16980.0733 ± 0.07800.1200 ± 0.16000.3426 ± 0.17800.0250 ± 0.07500.1370 ± 0.14500.1742 ± 0.0894FSPA-PR’s stability0.0000 ± 0.00000.0000 ± 0.00000.2142 ± 0.16920.0675 ± 0.06520.1733 ± 0.27360.4187 ± 0.18300.0250 ± 0.07500.2222 ± 0.20200.2058 ± 0.0862FSPA-SCE’s stability0.0000 ± 0.00000.1111 ± 0.11110.5312 ± 0.10000.0675 ± 0.06520.3562 ± 0.30990.3599 ± 0.25210.0250 ± 0.07500.1689 ± 0.18230.2485 ± 0.0830FSPA-LCE’s stability0.0000 ± 0.00000.1778 ± 0.08890.1852 ± 0.17830.0675 ± 0.06520.2333 ± 0.15280.1617 ± 0.16300.0250 ± 0.07500.1914 ± 0.14360.1744 ± 0.1192FSPA-CCE’s stability0.0000 ± 0.00000.1778 ± 0.08890.2735 ± 0.16980.0733 ± 0.07800.1200 ± 0.16000.3426 ± 0.17800.0250 ± 0.07500.1370 ± 0.14500.1742 ± 0.0894rank preservation of the significance measures of attributes used in the algorithms PR and FSPA-PR (see Theorem 5 andTheorem 6). From Tables 8–10, one draw the same conclusions.4.7. Related discussionIn this subsection, we summarize the advantages of the accelerator-positive approximation for attribute reduction andoffer some explanatory comments. Based on the experimental evidence, we can affirm that:• Each of the accelerated algorithms preserves the attribute reduct induced by the corresponding original one.616Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618In Section 4.3, we have proved the rank preservation of the four significance measures of attributes, which implies thatone selects the same attribute in each loop of each of modified algorithms and that of the corresponding original one.Naturally, one can obtain the same attribute reduct on the same data set. Hence, each of the accelerated algorithms doesnot affect the attribute reduct induced by the corresponding method.• Each of the accelerated algorithms usually comes with a substantially reduced computing time when compared withamount of time used by the corresponding original algorithm.Through using the accelerator-positive approximation, the size of data set could be reduced in each loop of each ofmodified algorithms. Therefore, the computational time for determining partitions, significance measures of attributes andjudging stopping criterion in the reduced data set would be much smaller than that encountered for the entire data set.Evidently, these modified algorithms outperform the previous methods.• The performance of these modified algorithms is getting better in presence of larger data sets; the larger the data set,the more profound computing savings.The stopping criterion of attribute reduction will be stricter when the data set becomes larger, and the number ofattributes in the reduct induced by a heuristic attribute reduction algorithm usually is much bigger. In this situation, eachof the modified algorithms can delete much more objects from the data set in all loops, and hence can take far less time forattribute reduction. The greater the size of the data set is, the larger the number of attributes selected, and the better theperformance of these modified algorithms becomes when it comes to computing time. Hence, these accelerated algorithmsare particularly suitable for dealing with attribute reduction in large-scale data sets with high dimensions.5. ConclusionsTo overcome the limitations of the existing heuristic attribute reduction schemes, in this study, a theoretic frameworkbased on rough set theory have been proposed, called the positive approximation, which can be used to accelerate algo-rithms of heuristic attribute reduction. Based on this framework, a general heuristic feature selection algorithm (FSPA) hasbeen presented. Several representative heuristic attribute reduction algorithms encountered in rough set theory have beenrevised and modified. Note that each of the modified algorithms can choose the same feature subset as the original at-tribute reduction algorithm. Experimental studies pertaining to nine UCI data sets show that the modified algorithms cansignificantly reduce computing time of attribute reduction while producing the same attribute reducts and classificationaccuracies as those coming from the original methods. The results show that the attribute reduction based on the positiveapproximation is an effective accelerator and can efficiently obtain an attribute reduct.AcknowledgementsWe would like to thank the anonymous reviewers for their valuable comments and suggestions. The authors also wishto thank PhD candidate Feng Wang for numeric experiments and data statistics for several months and Key Laboratory ofComputational Intelligence and Chinese Information Processing of Ministry of Education for the usage of about 1000 CPUhours on two computers (Inter(R) Core(TM)2 Quad CPU Q9400, 2.66 GHz and 3.37 GB memory) for the empirical study.This work was supported by the National Natural Science Foundation of China (Nos. 60773133, 60903110, 70971080), Na-tional Key Basic Research and Development Program of China (973) (No. 2007CB311002), GRF: CityU 113308 of the Govern-ment of Hong Kong SAR, the National High Technology Research and Development Program of China (No. 2007AA01Z165),and the Natural Science Foundation of Shanxi Province, China (Nos. 2008011038, 2009021017-1).References[1] J.G. Bazan, A comparison of dynamic and non-dynamic rough set methods for extracting laws from decision tables, in: L. Polkowski, A. Skowron (Eds.),Rough Sets in Knowledge Discovery 1: Methodology and Applications, Studies in Fuzziness and Soft Computing, Physica-Verlag, Heidelberg, Germany,1998, pp. 321–365.[2] J.G. Bazan, H.S. Nguyen, S.H. Nguyen, P. Synak, J. Wróblewski, Rough set algorithms in classification problems, in: L. Polkowski, S. Tsumoto, T.Y. Lin (Eds.),Rough Set Methods and Applications: New Developments in Knowledge Discovery in Information Systems, Springer-Verlag, Heidelberg, Germany, 2000,pp. 49–88.[3] R.B. Bhatt, M. Gopal, On fuzzy-rough sets approach to feature selection, Pattern Recognition Letters 26 (2005) 965–975.[4] R.B. Bhatt, M. Gopal, On the compact computational domain of fuzzy-rough sets, Pattern Recognition Letters 26 (2005) 1632–1640.[5] M.R. Chmielewski, J.W. Grzymala Busse, Global discretization of continuous attributes as preprocessing for machine learning, International Journal ofApproximate Reasoning 15 (4) (1996) 319–331.[6] M. Dash, H. Liu, Consistency-based search in feature selection, Artificial Intelligence 151 (2003) 155–176.[7] I. Düntsch, G. Gediga, Uncertainty measures of rough set prediction, Artificial Intelligence 106 (1998) 109–137.[8] G. Gediga, I. Düntsch, Rough approximation quality revisited, Artificial Intelligence 132 (2001) 219–234.[9] J.W. Grzymala-Busse, An algorithm for computing a single covering, in: J.W. Grzymala-Busse (Ed.), Managing Uncertainty in Expert Systems, KluwerAcademic Publishers, 1991, p. 66.Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618617[10] J.W. Grzymala-Busse, LERS—a system for learning from examples based on rough sets, in: R. Slowinski (Ed.), Intelligent Decision Support: Handbook ofApplications and Advances of the Rough Set Theory, Kluwer Academic Publishers, 1992, pp. 3–18.[11] J.W. Guan, D.A. Bell, Rough computational methods for information systems, Artificial Intelligence 105 (1998) 77–103.[12] I. Guyon, A. Elisseeff, An introduction to variable feature selection, Journal of Machine Learning Research 3 (2003) 1157–1182.[13] R. Jensen, Q. Shen, Semantics-preserving dimensionality reduction: rough and fuzzy-rough-based approaches, IEEE Transactions on Knowledge andData Engineering 16 (12) (2004) 1457–1471.[14] R. Jensen, Q. Shen, Computational Intelligence and Feature Selection: Rough and Fuzzy Approaches, IEEE Press/Wiley & Sons, 2008.[15] K. Kira, L.A. Rendell, The feature selection problem: traditional methods and a new algorithm, Proc. AAAI 92 (1992) 129–134.[16] R. Kohavi, G.H. John, Wrappers for feature subset selection, Artificial Intelligence 97 (1–2) (1997) 273–324.[17] M. Kryszkiewicz, P. Lasek, FUN: fast discovery of minimal sets of attributes functionally determining a decision attribute, Transactions on Rough Sets 9(2008) 76–95.[18] M. Kryszkiewicz, Comparative study of alternative type of knowledge reduction in inconsistent systems, International Journal of Intelligent Systems 16(2001) 105–120.[19] X.H. Hu, N. Cercone, Learning in relational databases: a rough set approach, International Journal of Computational Intelligence 11 (2) (1995) 323–338.[20] Q.H. Hu, Z.X. Xie, D.R. Yu, Hybrid attribute reduction based on a novel fuzzy-rough model and information granulation, Pattern Recognition 40 (2007)3509–3521.[21] Q.H. Hu, D.R. Yu, Z.X. Xie, J.F. Liu, Fuzzy probabilistic approximation spaces and their information measures, IEEE Transactions on Fuzzy Systems 14 (2)(2006) 191–201.[22] Q.H. Hu, D.R. Yu, Z.X. Xie, Information-preserving hybrid data reduction based on fuzzy-rough techniques, Pattern Recognition Letters 27 (5) (2006)414–423.[23] C.K. Lee, G.G. Lee, Information gain and divergence-based feature selection for machine learning-based text categorization, Information Processing andManagement 42 (2006) 155–165.[24] D.Y. Li, B. Zhang, Y. Leung, On knowledge reduction in inconsistent decision information systems, International Journal of Uncertainty, Fuzziness andKnowledge-Based Systems 12 (5) (2004) 651–672.[25] J.Y. Liang, K.S. Chin, C.Y. Dang, C.M. Yam Richid, A new method for measuring uncertainty and fuzziness in rough set theory, International Journal ofGeneral Systems 31 (4) (2002) 331–342.[26] J.Y. Liang, Z.B. Xu, The algorithm on knowledge reduction in incomplete information systems, International Journal of Uncertainty, Fuzziness andKnowledge-Based Systems 10 (1) (2002) 95–103.[27] J.Y. Liang, Y.H. Qian, C.Y. Chu, D.Y. Li, J.H. Wang, Rough set approximation based on dynamic granulation, Lecture Notes in Computer Science 3641(2005) 701–708.[28] H. Liu, R. Setiono, Feature selection via discretization, IEEE Transactions on Knowledge and Data Engineering 9 (4) (1997) 642–645.[29] J.S. Mi, W.Z. Wu, W.X. Zhang, Comparative studies of knowledge reductions in inconsistent systems, Fuzzy Systems and Mathematics 17 (3) (2003)54–60.[30] M. Modrzejewski, Feature selection using rough set theory, in: Proceedings of European Conference on Machine Learning, 1993, pp. 213–226.[31] H.S. Nguyen, Approximate Boolean reasoning: Foundations and applications in data mining, Lecture Notes in Computer Science 3100 (2006) 334–506.[32] T. Pavlenko, On feature selection, curse-of-dimensionality and error probability in discriminant analysis, Journal of Statistical Planning and Infer-ence 115 (2003) 565–584.[33] Z. Pawlak, Rough Sets: Theoretical Aspects of Reasoning about Data, Kluwer Academic Publishers, Boston, 1991.[34] Z. Pawlak, A. Skowron, Rudiments of rough sets, Information Sciences 177 (1) (2007) 3–27.[35] Z. Pawlak, A. Skowron, Rough sets: some extensions, Information Sciences 177 (2007) 28–40.[36] Z. Pawlak, A. Skowron, Rough sets and boolean reasoning, Information Sciences 177 (1) (2007) 41–73.[37] W. Pedrycz, G. Vukovich, Feature analysis through information granulation and fuzzy sets, Pattern Recognition 35 (2002) 825–834.[38] L. Polkowski, On convergence of rough sets, in: R. Slowinski (Ed.), Intelligent Decision Support: Handbook of Applications and Advances of Rough SetTheory, vol. 11, Kluwer, Dordrecht, 1992, pp. 305–311.[39] Y.H. Qian, J.Y. Liang, Combination entropy and combination granulation in rough set theory, International Journal of Uncertainty, Fuzziness andKnowledge-Based Systems 16 (2) (2008) 179–193.[40] Y.H. Qian, J.Y. Liang, C.Y. Dang, Converse approximation and rule extraction from decision tables in rough set theory, Computer and Mathematics withApplications 55 (2008) 1754–1765.[41] Y.H. Qian, J.Y. Liang, C.Y. Dang, Consistency measure, inclusion degree and fuzzy measure in decision tables, Fuzzy Sets and Systems 159 (2008)2353–2377.[42] Y.H. Qian, J.Y. Liang, C.Y. Dang, Interval ordered information systems, Computer and Mathematics with Applications 56 (2008) 1994–2009.[43] Y.H. Qian, J.Y. Liang, C.Y. Dang, D.W. Tang, Set-valued ordered information systems, Information Sciences 179 (2009) 2809–2832.[44] Y.H. Qian, J.Y. Liang, D.Y. Li, H.Y. Zhang, C.Y. Dang, Measures for evaluating the decision performance of a decision table in rough set theory, InformationSciences 178 (2008) 181–202.[45] Y.H. Qian, J.Y. Liang, C.Y. Dang, Incomplete multi-granulations rough set, IEEE Transactions on Systems, Man and Cybernetics: Part A 40 (2) (2010)420–431.[46] R. Quinlan, Induction of decision rules, Machine Learning 1 (1) (1986) 81–106.[47] M.W. Shao, W.X. Zhang, Dominance relation and rules in an incomplete ordered information system, International Journal of Intelligent Systems 20(2005) 13–27.[48] Q. Shen, R. Jensen, Selecting informative features with fuzzy-rough sets and its application for complex systems monitoring, Pattern Recognition 37(2004) 1351–1363.[49] A. Skowron, Extracting laws from decision tables: a rough set approach, Computational Intelligence 11 (1995) 371–388.[50] D. Slezak, Approximate reducts in decision tables, Research report, Institute of Computer Science, Warsaw University of Technology, 1995.[51] D. Slezak, Foundations of entropy-based Bayesian networks: theoretical results & rough set based extraction from data, in: IPMU’00, Proceedings of the8th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, vol. 1, Madrid, Spain, 2000, pp.248–255.[52] D. Slezak, Approximate entropy reducts, Fundamenta Informaticae 53 (3–4) (2002) 365–390.[53] R.W. Swiniarski, A. Skowron, Rough set methods in feature selection and recognition, Pattern Recognition Letters 24 (2003) 833–849.[54] G.Y. Wang, H. Yu, D.C. Yang, Decision table reduction based on conditional information entropy, Chinese Journal of Computer 25 (7) (2002) 759–766.[55] G.Y. Wang, J. Zhao, J.J. An, A comparative study of algebra viewpoint and information viewpoint in attribute reduction, Fundamenta Informaticae 68 (3)(2005) 289–301.[56] S.X. Wu, M.Q. Li, W.T. Huang, S.F. Liu, An improved heuristic algorithm of attribute reduction in rough set, Journal of System Sciences and Informa-tion 2 (3) (2004) 557–562.618Y.H. Qian et al. / Artificial Intelligence 174 (2010) 597–618[57] W.Z. Wu, M. Zhang, H.Z. Li, J.S. Mi, Knowledge reduction in random information systems via Dempster–Shafer theory of evidence, Information Sci-ences 174 (2005) 143–164.[58] Z.Y. Xu, Z.P. Liu, B.R. Yang, W. Song, A quick attribute reduction algorithm with complexity of max(O (|C||U |), O (|C|2|U /C|)), Chinese Journal of Com-puter 29 (3) (2006) 391–398.[59] Y.Y. Yao, Information granulation and rough set approximation, International Journal of Intelligent Systems 16 (1) (2001) 87–104.[60] W. Ziarko, Variable precision rough set model, Journal of Computer and System Sciences 46 (1993) 39–59.