ELSEVIER Artificial Intelligence 88 (1996) 143-161 Artificial Intelligence Propagating imprecise probabilities in Bayesian networks * Gernot D. Kleiter * Institut fiir Psychologie, Universitdt Salzburg, Hellbrunner.xtr. 34, 5020 Salzburg, Austria Received March 1995; revised February 1996 Abstract in the networks “exact” probabilities; Often experts are incapable of providing likewise, in networks are based must often be small and preliminary. the probabilities probabilities probability uncertainty about probabilities. The problem of how to propagate point probabilities network now Bayesian networks. to use beta or Dirichlet distributions samples on which the In such cases can be handled by second order the in a Bayesian in to propagate Dirichlet distributions the problem of how are imprecise. The It is convenient is transformed distributions. imprecision to express into first order probabilities It is shown that the propagation of Dirichlet distributions in Bayesian networks with incomplete and Dirichlet distributions. Ap- and their second order probability density functions are obtained are in a system of probability mixtures of beta-binomial data results proximate by stochastic discussed by the use of examples. An important property increases pruning criterion small. Thus, imprecision may be used as an Ockam’s razor in Bayesian networks. simulation. A number of properties of the propagation of imprecise probabilities is that the imprecision of inferences rapidly as new premises are added to an argument. The imprecision to keep the number of variables in an inferential argument can be used as a in a network involved 1. Introduction Bayesian belief networks represent and process probabilistic knowledge. Their rep- resentational components belong to one of two domains, a qualitative or a quantitative * Thanks are due to the Fonds zur Ferderung der wissenschaftlichen Forschung, Vienna, for the financial support. Thanks are also due to the hospitality of the Department of Psychology, Bowling Green State University, Ohio, especially to Michael E. Doherty. * E-mail: gemot.kleiter@sbg.ac.at. 0004-3702/96/$15.00 Copyright @ 1996 Elsevier Science B.V. All rights reserved. PII SOOO4-3702(96)00021-S 144 G.D. Klerter/Art@crcd lntrlligencr RR (I 996) 143-161 Fig. I. Bayestan network: Cooper’< medical diagnosis example Table I Weight tables associated with Cooper’s example I. the numbers were choosen so that two conditions the ratios of the weights preserve the probabilities of the original version of Cooper’s example in Rg. are fullfilled: (i) and (ii) the total sum of all elementary weights is 120 (A) ytl t, 96 24 (B. A) -h h -0 77 I9 ii 5 I9 !C. A! 70 Y( c ‘)I 5 <I 19 5 (E, C) 7r Y yc 44 66 (’ 2 8 probability dependence distributions (conditional) relationships like diseases, are organized of (conditional) of the involved from a specific metastatic the associated quantitative in Fig. I and Table 1 [7,34,37]. specifications. Assume we have investigated to the nodes of the graph. The tables are not “visible” the example shown the dependencies variables and independence in the visual language of graph theory. The quantitative one. The basic qualitative between variables are expressed specifications tables and attached ical representation. Consider in Fig. 1 represents network represent clinical absent/present I contains patients suspected of suffering 24 actually have developed metastatic in which the metastatic and 77 do not, etc. These and the remaining main purpose of a Bayesian belief network patient one or more of the variables are observed and are known for certain, the probabilities ties propagate through diagnosis, prediction, or explanation Bayesian network. Bayesian belief networks belong tic models intelligence in in the graph- The in a graphical model. The nodes A-E test results, or symptoms. Table 120 form of cancer. It turns out that the total serum calcium and 5 do not. Of those patients total serum calcium in Table 1. The If for a this affects states in the graph. Evidence and updated probabili- like medical in a for tutorials and related work on uncertainty //www . auai . org page and the references given there). to the class of graphical probabilis- in artificial are special cases of propagating probabilities ( [ 6, 1 1, 15,30,43]; see the http: the network. Various kinds of probabilistic form and 96 have not. Of those having is to perform probabilistic form, 19 show increased form was not observed, 19 show increased of the neighboring the metastatic are contained frequencies inference. inference G.D. Kleiter/Artificial Intelligence 88 (1996) 143-161 14.5 I- 10 20 30 40 50 100 * Probability Fig. 2. The beta distribution Be(4.41.40.56) example. together with a 99% highest density interval for Cooper’s Usually, the probabilities in Bayesian networks are treated as though they were known instead. Probability precisely. In the present paper we analyse Bayesian networks are not known precisely. Experts often cannot provide exact point probabilities, intervals estimates derived small sample considered made how bounds [ 17,21,25,26,33,39,40]. from empirical data are often based on in a Bayesian network cannot be several proposals have been such as lower and upper to be precise point values. to handle in which the probabilities providing [ 8,3 1,381, and second order distributions In the literature, the probabilities In such cases in dependency of variances propagation is provided imprecision structures, [ 9,12,42], A tutorial in [ 161. sizes. function statistics the uncertain We treat probabilities that are not known precisely is attached. The distributions the distribution in the same way they are treated [ 31, as uncertain quantities to which a (second order) probability If little is known If much the distribution small. The use of a second order in Bayesian statistics and there is nothing distribution exciting about it. The procedure actually goes back to Thomas Bayes. He was probability density in Bayesian density about is known, probability especially one of the first who plotted a continuous (upside-down) function, a beta distribution is tight and its variance is a standard procedure over the unit interval. the imprecision. its variance is flat and quantity, express large. The method proposed in this paper allows the derivation of the following inferences: If of a metastatic fall into comas (-d) but suffers from severe headaches is an the probability imprecision associated with this estimate. We can be 99% sure that the true lies in the interval 0.0134 and 0.227. The standard deviation of the estimate e] = a patient does not intermittently (e), then appreciable probability is 0.0436. The imprecision may be expressed by Be( 4.41,40.56), where the brackets are used as a shorthand notation to the probability density [al+, for “the probability of a given function of the parameter the beta distribution is 0.098. However, corresponding -d and cancer there (a) 146 G.D. Klrrter/Artijicid intelli#wcr 88 (1996) lJ.?-161 corresponds content 0.99). Further analysis the full example inference is hascd on a total sample size of 120 cases the precision e”. While to a sample size of 45 cases only. Fig. 2 shows of the present interval interval the beta distribution are that the severe headaches with probability on -d not really essential with mean 0.087 and standard alone deviation 0.0387. The precision only as compared when both D and E are instantiated. We will come back to this at first sight counterintuitive together with a 99% highest density shows the metastatic increases when D is instantiated leads to the distribution cancer. Conditioning = Be(4.50.47.44) even slightly (the shortest inferences property. [n]ld] about for 2. Basic model (nodes, variables) V and a set of directed edges E between variables) defined on V x V. The vertices and the (DAG). With each node X E V and the set of its , U,,!). If (X, I/) is If the arcs do not contain cycles, , U,,,} we associate a weight table (X, 1/t,. by a graph G = l VE). dependencies a set of vertices is a directed acyclic graph We consider (arcs, probabilistic the edges are represented graph parents pa(X) = {U,, a two-dimensional weight table. then we denote if W = (Xi, X2.. More generally, the subset { ZI , . marginal along We follow [ 141 and denote a probability and marginal distributions , X,,) is an ,I-dimensional weight table, we denote , X,l} by (X, ,X2,. . Z,,,} C {X,, the marginal of X along U by (X, (i)~‘. the . . , X,,)‘{z~....~znf~. (pdf) by brackets. Joint, respec- are written as [X, Y], [ XjY], and [Xl, is denoted by +, e.g., [X, Y] = [X/Y] * [Y] etc. The [ Xlpa( _u) ] for each variable X. We conceive in a table as the shape parameters of Dirichlet distributions. Dirichlet dis- are defined for binary cases, the beta distributions, (second order) pdfs function density and their special versions tables define conditional, tively. The product of densities weight the weights tributions as follows: Definition 1 (Dirichlet distribution). plex S’= of reals with vector is given by (VI > 0,. y,~): K, < 0, i= {(yt...., Let ( Yl, I ,.... d;Cf!,y, , XI ) be a random vector on the sim- 6 I) and (v,,.... vg) a vector I. If the density of the random , Y!) > O), where d = D - P(.YI,...,.vd) = f(Vl t~.+vf)) ,,I” ’ T(Vll . ..I’(?/“). - 1 , “,‘_I ,I ] _ -& & I’,> - I , i ,=I i (1) we say that (U, , . [Y, . . . . . Yd] =Di(v , Yd) follows a d-variate Dirichlet distribution. We use the shorthand ,,.... v~). Definition 2 (Beta distribution). density is given by Let Y be a random variable on the unit interval. If its (2) G.D. Kleiter/Art@cinl Intelligence 88 (1996) 143-161 147 Y is beta distributed with shape parameters VI and ~2. We write Be( vt,v2). Its mean and variance are given by for short [Y] = E(Y) = -?-- VI + v2’ var(Y) = VIP2 (VI + v2>2(v, + v2 + 1). (4) In a beta distribution we interpret available about a proposition, extensively the point probability VI /( vt + ~9). VI is the weight or a hypothesis, discussed by Keynes and v2 the weight against [20]. the sum vt + v2 as the total amount of evidence in favor of an event, it. Weights of evidence were The beta or Dirichlet distributions underlying parameters probability pdf is a marginal dimensions. the number of possible values of its parents order mc x ml x . . . x m,. distribution. implement the network. If the node has n parents a system of second order pdfs on the the If a node X has no parents, table has it + 1 is me and is of table the weight then If the number of possible values of the node under consideration then the weight is ml, . . . , m, vector underlying continuous parameters that represents The probability the probabilities (~1, . . . , %-D-l ) (one dimension random variables. Of course, should be represented hidden den variables variable with D possible values we should attach a parent probability dimensional cause add up to one). The (second order) probability the vector weight though, (propositional Because lationship hidden variables the twin nodes would unneccessarily the network are not directly observable but in the graph of a Bayesian network also the hid- random by nodes. To each discrete propositional l)- is lost be- of in the the hidden nodes and their children, of the discrete states of the child nodes are equal to the values of the hidden variables: P( xi]rri) = r;. the re- the in the graph of a weighted Bayesian network. Drawing is a Dirichlet distribution. table of the node. The relationship is redundant: variable has a twin hidden variable and because the hidden variable and the propositional It is specified by the numbers the conditional probabilities every propositional complicate graph. are not drawn is redundant distribution the (D - variables) contained between between variable The space defined by the hidden probability variables for the domain of propositional it respects Bayesian belief networks It is a subspace, and not the full space, because in the network. The Dirichlet distributions The qualitative graph), probabilities the propagation uncertain parameters. Methods how to learn such structures data were described by Heckerman, Geiger and Chickering are taken as uncertain quantities of posterior densities is taken for sure. The numerical independence/dependence in Bayesian (that is a subspace of all possible variables under consideration. independencies the conditional are treated as Bayesian posterior distributions. is visually structure of the underlying specifications (conditional) represented in the that are not known for sure. We investigate networks with given structure but and from prior knowledge [ 171. If all the weight tables are obtained database with no missing cases, ables and all conditional distributions frequency from then the joint distribution counts in one big complete over the domain of all vari- are Dirichlet [ 441. The propagation of probabilities 14X G.D. Kleiter/Artijicinl lntellipwce X8 (1996) 143-161 can be performed by one of the usual methods of propagating point probabilities; weights of the Dirichlet distributions probabilities though, from several sources, may have been combined jective expertize etc. We next introduce concepts solution the resulting by the total sample size, i.e.. by taking expected frequencies. Completeness, in practical applications. A database it may contain objective data and sub- that are helpful are obtained by simply multiplying is a strong assumption. to find an approximate for incomplete data. It is often violated the 3. Natural children and natural neighbors In a complete contingency table the marginals statistics the marginals to the sums of the corresponding hold for the weights tionship between the cells and of the member distribution. With additive weights beta or Dirichlet distribution distributions. What shall we do when up? In inferential The treatment of the missing data can be related averages of complete the weighted space of the missing data. This results purposes expectation maximization viding maximum (variance) In large networks are multiple [II]. found We combine of the estimates can be approximated the iterative algorithm are too complicated. algorithms local maxima and it is difficult local noniterative the solutions likelihood estimates (EM) cell counts. A similar in a weighted network. The case of a perfectly along any of its dimensions are equal relationship may or may not rela- treatment is also a additive allows an easy mathematical the member distribution and we thus stay within the same family of probability this case occurs when some of the data are missing the cell counts and the marginals do not add [29]. to the complete case by calculating is performed over the [ 261. For computational solutions. where averaging in probability mixtures Incomplete data are usually analyzed by [ IO]. EM is an iterative procedure pro- in the presence of missing data. The precision [ 291. EM has several disadvantages. is slow. Furthermore, there to find out that a global maximum has been in large networks estimation with Gibbs sampling. The probabilities at each node in the network depend only upon the states of the neighbor nodes blanket). The estimation data can be done without of the conditional between cell weights and marginals. We introduce parents, and natural neighbors. (Markov of conditional in a Markov blanket with missing iteration. We use the 6 method to estimate means and variances relationships the concepts of a natural child, natural probabilities. We next give a definition of the additive probabilities Definition 3 (Natural child). Let Y have XX E pa(Y) have the parents pa( Xk) = {1/r,, marginal weights of Xk in the table (Xk,pa(Xk)) along pa(Xk) are identical: the parents pa(Y) = {Xi,. let , U,,,}. Y is a natural child of X, if the (I: pa( Y)) along {Ypa( Y) \ Xk} and in the table , X,!,} and . #pa(Y)) ltZPaY)\XkJ = (Xk,pa(Xk))lPa(XkI, (5) If X has no parents, then Y is a natural child of X if (xX)1’ = (X). G.D. Kleiter/Artificial Intelligence 88 (1996) 143-161 149 Fig. 3. The plates drawn around A and C indicating about C (middle), and more is known about A. that A is a natural child of C (left), more is known simultaneously Visually, we represent a parent [ 51 (who gives credit to Spiegelhalter). is instantiated together with its natural child in a plate. A plate is a rectangle drawn around a set of nodes with a repetition number N written in its left lower Plates corner. Plates were introduced by Buntine indicate a data set of the same kind. The set of nodes or repeatedly by N observations. A plate shows a complete set of N data. Often N is a in a weight table. In the left panel of Fig. sample size, or it is the total sum of weights the nodes A and C. A node 3 a plate with the repetition number NAC is drawn around the table Y is a nonnatural child of the parent & containing & and the parents of & are (i) the table containing Y and the parents of Y. We say that in the first case, we have more the parents, and that in the second case, we have more information information some cases in a sample of observations about the child. This is the case if, for example, are missing or if additional In the middle panel of Fig. 3 an extra cases are available. plate is drawn around node C to indicate NC additional observed cases on C only. More is known about C than about A. In the right panel an extra plate is drawn around A. More is known about A than about C. if the marginal weights of & within larger or (ii) smaller than those within about natural. It follows if all its children- If a parent has two or more children taking also the parents of these children If a parent has two or more natural children, then their repetition numbers must be that the parent and the natural children can be put into a single plate. identical. into We call a parent natural in respect account-are to all their parents, that the then their repetition numbers must be identical. parent and the children can be put into a single plate. In the left panel of Fig. 4 node E has the natural children A, B, and C. The three children are also natural to their parents D and F. A, B, C, D, E, and F can thus be put into a single plate. The parents of a node, its children and the parents of these children are called the neighbors or the Markov blanket of the node. The probability distribution of the states of the node depends on the state of nodes in the Markov blanket and on these only. The condition in which the Markov blanket builds a plate is important: that are natural in respect It follows Definition 4 (Natural neighbors). A node has natural neighbors in respect all its children are natural to all their parents. (Markov blanket) if G.D. Kle~ter/Art$ficinl /ntellip!twr X8 (1996) 143-161 Fig. 4. The plates drawn around E and its natural children A, .!I. and C can be represented by a single plate drawn around A, R, C. D, E. and F. 4. Member distribution To propagate the second order pdf in a weighted graph we use stochastic simulation. theorem, or what is called “generalized It turns out that in stochastic simulation Bayes’ Bayes’ theorem” for a Bayes’ parameter tained for this Bayes’ parameter and extend corresponding to the generalized Bayes’ [34], plays a central theorem. in [21,24,261. We will make use of the previous role. We have investigated second order pdfs results ob- them to the slightly more general situation 4.1. Member parameter Consider a disease A that can be present or absent, and a symptom B that also can be is (Y, the marginal or base rate probability of A being present present or absent. Assume and the conditional is /?I, and not present patient suffers from disease A is given by Bayes’ symptom probability symptom probability of the symptom B given the disease the conditional is p2. If we observe a patient showing symptom B the probability of the symptom given the disease is present is that the theorem (6) We call ,U the member probabilities LY, pt. and p2 are imprecise and the imprecision beta distributions It is just Bayes’ theorem parameter. in a parametric form. If the is expressed by the three LY - Be(al,a2), PI -Be(h1,012). PZ - Be(b21.1322), (7) we want to infer the distribution of the member parameter evidence al, ~2, 611, 012, b21, and b2~. We call this distribution The member distribution how precise our posterior probability is a second order pdf defined is. ,LL given B and the weights of the member distribution. for Bayes’ formula. It tells us G.D. Kleiter/Art@ial Intelligence 88 (1996) 143-161 151 4.2. Beta and mixed beta member distributions Let A and B be two binary random variables. We have shown the following theorem [23,26]: Theorem 5 (Natural child). ( 1) If CX, pt, &, and ,LL are the probability parameters underlying the propositional variables A, B( A, B 17 A, and A) b, respectively, and if the second order pdfs three parameters are (Y - Be( al, a2), /?I N Be( bl I, b12), and p2 N of the first and Be(b21,b22), if the three parameters a, /I,, and p2 are independent, and if B is a natural child of A, i.e., a] = bl] + b12 and a2 = b21 + b22, (2) (3) then the pdf of the Bayes’ parameter t_~ is given by ,u - Be( b, 1, b21). That is if B is a natural child of A then the member distribution and its parameters table of A is not needed. can directly be read off from B’s weight is a beta distribution table. Note that the weight to natural parents, The theorem directly generalizes etc. to locate cells in the weight tables. We finally denote the corresponding the notation, however, becomes more cumbersom. We assume that the nodes are binary and use upper case characters “A”, “B”, “_A,,, S‘7B99, etc. to denote node variables. We use lower case characters like “AB”, “a” “-a”, “b”, “+P, etc. to denote cell “AiB”, weights by “(A)“, “(TA)“, “(a)“, “(la)“. Using this notation Theorem 5 reads: If [A] = then Be( (A), (TA)), [ Alb] = Be( (Ab), (TAb)). in node value bl and the the cell indexed by the variable node value A, the instantiated node values of the parents of bl, etc. and [BI-A] = Be( (TAB), theorem instantiated nodes. We use conjunctions (A-B)), In the following (Abl f (6, )) denotes [BIA] = Be( (AB), the weight (TA-B)), its children are , B,, , and if the underlying probability parameters are independent and Dirichlet If A has natural neighbors, if Theorem 6 (Natural neighbors). Bl,... distributed, then [A(b,,...,b,,l =Be((Ablf(b,).. .Ab,,f(b,,)),(-Ablf(bl) . ..-Ab.,f(bn,))). (8) In the case in which the weights of evidence are not additive, the member distribution (i) the marginals less is known about than about the conditional probabilities, distribution. Two cases must be distinguished: [ 261. The mixing weights follow a Polya-Eggenberger is a mixture of beta distributions probability is known about probabilities. in which of the disease In the first case we may know more about that is al > bll + b,2. The difference than about D = al - (bl I + b,2) is positive. D may be conceived as the number of missing data, i.e., to be present but do not know the as the number of cases for which we know the disease symptom. of that is al < bll + b12. In the disease the case in which more the case and (ii) In the second case we may know less about the conditional (for example) than about the presence than about the conditional symptom probabilities, symptom probabilities, the conditional (for example) the marginals the presence IS2 G.D. Klr~/er/Art~fkicd fnfei/i~ence 88 (1996) 143-161 the marginal probabilities about statistics is random this situation may arise if in a contingency for CII cases but fixed by the experimenter in the statistical all 01 I + 0 12 cases can be used. In the first and inferential marginals blI + b12 - al cases. For inferences can be used probabilities missing data can be predicted probabilistically. of a future sample given an observed one may be shown that the predictive distribution distribution distributions where the mixing weights are Polya-Eggenberger [ 191. The member distribution analysis. For inferences In statistics about is called a predictive distribution [ I, 21. It in both our cases is a Polya-Eggenberger turns out to be a probability mixture of beta probabilities [ 261: table the sampling of the for the remaining D = only the al cases symptom the in the second case the conditional the probability distribution Theorem 7 (Nonnatural neighbors). ( I ) If a, PI , &, and ,U are the probability parameters underlying the propositional variables A, BJA, BITA, and A/b, respectively, and if the second order pdfs and /32 N of thefirst three parameters are a N Be(nl,uz), /31 N Be(bll,blz), (2) /?I, and & are independent. then the pdf of the Bayes’ parameter J.L is and Be(b21,h), if a, given bl lxlX( 11, 1 mnx( <I> 1 p=c c dj=min(df ) d2=min(d?) PE(DI,bil +s1dl,b12 + DI - sldl) x PE( Dz, b21 f s2d2,bzz -k D2 - s2d2) xBe(bl, +.sld,,b21 +s2d2). (9) Let Bi = b;l + b,:! and Di = lai - B, /. Then the range of dl and d2 is constrained by min(di) = 0, max(O,b,, + D, ---B,), il u, > 8. $a1 < B,, max(di) = Di, min(b;l,D,), [f a, > B;. $a, < Bj, .s, = 1, 0, -1. i if a, > B, . if a; = B;. if a, < B;. The Polya-Eggenberger distribution is defined as follows: Definition 8 (Polyu-Eggenberger its distribution is given by distribution ). Let Y be a discrete random variable. If g(g+ Is)(g+2s) ,..(g+ (Y - 1)s) (g+h)(g+h+Is)...(g+h+(n- 1)s) x h(h+ Is)(h+2s)..,(h+(n-y- I)s), (10) it is a Polya-Eggenberger distribution and we write Y - PE( n, g, h). G.D. Kleiter/Artijcial Intelligence 88 (1996) 143-161 153 For s = 1 the Polya-Eggenberger distribution is equivalent to a beta-binomial distri- (see, e.g., [2] >, for s = 0 to a binomial, bution distribution. For more details we refer to [ 261. In a more general structure may be obtained by linear programming. observations, the calculation though, Below, we employ an approximation and for s = - 1 to a hypergeometric the constaints In a large network containing many missing of the exact beta mixtures becomes cumbersom. based on the S method. 5. Stochastic simulation The use of stochastic simulation [ 181 has shown that the stochastic simulation in Bayesian networks was proposed by Pearl in a Bayesian network It has extensively been employed to Bayesian networks [34]. is a special [ 13, Hrycej case of Gibbs sampling. 36,39,41]. At the start each instantiated node is set to an arbitrary value. Then, is clamped to its constant value and each non- steps are the following iteratively, node the means m(p) node, e.g., in the alphabetical order of the node names. Select a nonclamped Compute for each of the values of this node, given the current values of the neighbor nodes. are In a Bayesian network containing obtained by the generalized Bayes’ theorem and the variances var( p) of the member distributions the local probabilities point probabilities, instantiated performed: (1) (2) P(Xlrest(x)) = KP(XlPa(n))~P(Y;lf,(x)). ( 11.) .j=l represents of X, fi(x) factor, pa(x) the means of the distributions the parents of yj, and rest(x) the second order pdfs we use a completely The upper case letters refer to random nodes, the lower case letters to instantiated nodes. K is a normalizing children except X. For to determine calculated by the 6 method Determine ability value. Build value of the current node. a new value for the node by selecting a random number. The prob- of this the parents of X, y,i the all variables formula are the sum of the means and variances of the member distribution to the mean of the member distribution at each node. The variances that is described below. for each value represents analogous for each is equal (3) (4) The sums are averaged, and finally beta distributions Be(p, q) are fitted to the means m( ,u) and variances var( ,u) ; p and q are obtained from: N= m(p)(l -m(p)) VMF) -1 and p=m(,x)N, q=N-p. (12) It is interesting We turn to the determination to note the central of the variances in step (2). role Bayes’ theorem plays in stochastic simulation. 5.1. The 6 method a Gibbs according incomplete to employ it is possible to a distribution data. Principally, in turn, would determine Gibbs sampling allows the propagation of first order probabilities in Bayesian networks to random for example. The random child two-level sampling process. instead. We directly employ variance estimates obtained at each In this section we at with obtain second order densities. At the hidden nodes we would have to generate probabilities probabilities, nodes. This would lead to a computationally We will use a shortcut node to calculate describe each node given to incomplete data. law, a beta distribution, the state probabilities very expensive the precision of the second order distributions. the variances of the second order distribution the method of how to obtain its natural or nonnatural neighbors. Nonnatural at the associated sampler also correspond neighbors in Eq. (6) ,U as introduced The member parameter is a nonlinear function g of the variables LY, /3t, and /?2. For each of these parameters the pdf is known. Can we derive the mean and the variance of the member parameter” For linear functions g of a random is not true if g is not linear. In many variable X we have E[g(X) cases, though, the mean and the variance ofg( X) can be approximated by the S method 14,321: ] = ,q( ElX] 1. This Definition 9 (6 rule). Let (XI.. (V, , , E,,) and variances (El. that can be partially differentiated ,f( X1,. is asymptotically , X,,) . X,, ) bc independent . v,). If f’( Xi, at ,f(E). random variables with means . , X,,) is a function of the variables then . E,,) with respect to El,. . , E,, normal with mean E,,x I..., x,,, = .f‘(El,. .E,,,) and variance I, var.f(x ,,..., x,,) = CC df(E c: ;=I I,..., J-5 E,,) 2 i (13) (14) The mean of each variable X in a Bayesian network conditioned other variables can be approximated upon the generalized Bayes’ Bayes’ theorem in the form of expectations: by applying the formula for the expectation theorem ( 1 I ). We only need to rewrite on the state of all (13) the generalized E[Xlrest(x)l = KE[Xlpa(x)l ~Ely,/.i’,(x) ,=I 1. (15) The expectations of the member distribution are estimates of the underlying is obtained parameters. The variance from Eq. ( 14) as a sum of variance components: probability var[X/rest(x)l = var[Xlpa(x) 1 f Cvar_[.,.;l,fjCa)], (16) G.D. Kleiter/Artijkial Intelligence 88 (1996) 143-161 155 i.e., from the sum of the variance components sums of the variance components the instantiated obtained by applying the 6 rule to one variable only values of their parents. The variance component of the jointly instantiated parents and the of is due to the parents of each of the children under proper consideration var[Xlpa(x) 1 = V[Xlpa(x) 1 ~f(E[Xlpa(~)l,E[~~lf~(~)l,...,E[~,,lf,,~~)l) WXlpa(x) 1 2 > (17) and similarily the variance components due to the children are obtained from = Vryjlfj(4 1 ~f(E[X(pa(x)l,E[y~lfl(x)l,...,E[y,,lf,(x)l) ~E[~.ilfiWl 2 . ) (18) Building partial derivatives and collecting terms finally leads to the following expressions var[Xlrest(x) I = 1 (19) (20) (21) (22) where n is the number of children and m the number of possible values of X, and A=E[Xlpa(x)l(l -E[Xlpa(x)l), B.i =E[~.iIfi(x>l(l -E[y,jIfi(x)l), D = E[Xlpa(x)l fiELY,i(fi(l)l .j= I + (~-E~Xlp4n)l)fi~~ -E[.Yjjfj(X)l). .j= I The E[ .I.] terms are easily obtained by dividing + b12). The V.I.1 are variances of beta distributions, bii/(bii b12)2 (bl1 + b12 + 1) 1. The square bracket notation symbols and to stay as close as possible E[ X]rest(x)] probabilities. other nodes except X. stands for the member parameter is used to avoid to the symbols used in the literature e.g., e.g., bl1 b12/[ (611 + too many greek for point ,u at node X given all cell weights by marginals, 5.2. Program The propagation by a program written the graph is supported by the Raima Database Manager oriented and supports is performed in C. The navigation [ 351. This database through is network In that the definition and processing of directed graphs by pointers. IS6 G.D. Kleiter/ArtiJiiciul Intellipxce RX (1996) 143-161 140 96.42 81 2, 34.6 74.5, 31.9 71.6. 30.6 60 18.42 252.58 8 261.655 29.2, 68.2 Fig, 5. Simple chain with base rate weights ((ij = 140. (~1) = 60, and conditional weights (bin) = 98, (7hlcl) = 42, (~J-YJ) = 18. and (-nl-U) = 42. etc. it is different respect simulation was performed with 1000 iterations. from relational databases. For the numerical examples stochastic Stochastic simulation may not work well when network are close to zero or one that is closer to beta mixtures. Especially, all its neigbhors may be obtained directly of the means and variances of the mixtures in the Polya-Eggenberger look promising. [ 71. We intend to replace the estimated probabilities in the the 6 method by a method the mean and the variances at each node given the calculation of many r-terms from the mixtures. But even requires the determination weights and the direct programming of the formulas does not 6. Examples 6.1. A simple chair1 Consider the chain and 0.3/0.7, the full graph consisting in Fig. 5. Denote consisting respectively. Without any nodes instantiated of all five nodes by of I. 2, 3, or 4 nodes by Cl, G2, Gj, and G4, Gs. Denote any subgraph 140 respectively. Assume 200 cases were observed under natural sampling conditions, probabilities A = a cases, 60 A = -a cases etc. At each node are the probability of a is 0.7/0.3 is much distributed in the flatter than the marginal distribution of u without B, C. D, and E being is [alG[] = Be( 140,60). When B = b is clamped we obtain system, which of course the same when, additionally, C, D, [ajh, Gs] = Be(98,18) are made and E are clamped. to we obtain [alb, ($1. remains system grow in which and the distribution If we let the reference with mean 0.71. Note that the distribution to the Markov property as [a[Gs] = Be(79,32) [alb, Gz] = Be(98,18) the conditional which due is identical inferences included There is very slow learning at the beginning [a(d,Gj] Be(81,31), [a(c,Gsl Be( 98, 18) again. The situation of the top-down in GS is [e[Gs] =Be(61,58), = Be(84,30), and then we obtain = Be(89,27), inferences [eln,Gs] Be(64,55), equivalent [e(c,Gs] = Be(68,48), to the distribution [eld,Gg] directly provided = Be(72,31). to the system. as we instantiated bottom-up: and finally [ale, Gs] = [alb,Gsl = is different: The marginal of e = is [elb,Gsl [eld,Gsl =Be(61,58), Of course, forward The “long distance” to the first order probability which is 6 I /( 61 + 58) = 0.5 1, a value that is practically equal to 0.5. The value is equal to the base rate of e. The “long distance” backward inference 8 I /( 81 + 31) = 0.72 from E to A is also noninformative; from A to E is noninformative in the probability in respect it results inference G.D. Kleiter/Art$ciul Intelligence 88 (1996) 143-161 1.57 is practically (backward which diagnosis in both cases the precision E, is much worse than in the l-node identical inference) to the base rate of a. Prediction at worst results and (forward in the base rate probabilities. However, the nodes A, B, C, D, and inference) in the 5-node system containing system containing only A or E, respectively. We recognize the Markov property in the Bayesian network: The distributions at its parent and its child-the or any other decendent, any node of the chain depends only upon or any other predecessors, and grandchild, additional equal because of the independence The weight information. The distributions [ a]b, Gs] , [ ajb, G4], in the chain. tables of the chain have natural children only. The local member distri- butions by the S method or exactly by Theorem 5 or 6 for natural sampling. There were practically no differences between both methods. process may be determined in the stochastic approximately [al b, G3], simulation structure grandparent, do not provide [ alb, Gz] are If we change given b changes divide the marginal weights of a from 140/60 from Be(98,18) to Be(47,9). That to 35/15 of a is, if we in the present example the distribution the marginal weights by 4 we have to divide the conditional ones by about 2. 6.2. A simple triangle Consider the network in Fig. 6. Denote of all three nodes by Gs. The marginal distribution [u]GI] = Be( 140,60), the subgraph consisting the subgraph consisting of A and its marginal of A and B by Gz and the graph in all of A is different 13), and [alGs] = Be(37,16). [u(G2] =Be(77, distributions and [ujb,Gx] =Be(49,9). of A given B in the two graphs G:! and G3 The distributionof [ulb,c,G3] to note that the distributions they are determined. Generally we observe are not invariant with respect that the the resulting weights of evidence. The limiting condition are independent just remain In highly connected the same. This happened of the structural extension. Then their first in the chain but not such as large cliques, e.g., the in which many structures, by system extensions will be larger than in systems the conditional weights only by Gt, denote consisting three structures: Accordingly, are [ulb,G2] =Be(98,16) is Be(69, 13). It is important to the reference system in which larger occurs when and second order distributions in the triangle loss in imprecision variables are independent. the inferences the smaller the system structure. If we reduce the marginal weights of A from 140/60 of A in G3 becomes Be(22,lO). distribution to be more precise-has We have stated of [al b, c, Gs] [22] decreased appreciably. that the imprecision The distribution to 35/ 15 the marginal distribution is Be(31.5,6), of [u\b,Gg] and the is Be( 39,7). The precision of the inferences about A-or LY, therefore circumstances-increases wise comparable get more complex. We should The trade-off between complexity networks by the minimum trade-off may be illustrated by an example Can more data make us more uncertain problem: description of probabilistic as the systems strive to keep the inferential and accuracy has recently been studied inferences-under other- in which they are embedded systems simple. in Bayesian length criterion [ 271. From our viewpoint the that at first looks terribly counterintuitive. about our inferences? Consider the following 66.6, 29.4, 29.4, 12.6 12.6, 5 4, 29.2, 12.6 Fig. 6. Simple triangle with base rare weights ((I) = 140, (70) = 60 an d conditional weights (bla) = 98, (hlv) = 18. etc. and (Y/>/W) = 42. and weights ((,itr. h) = 68.6, (COO, +) = 29.4, (c17cl, /J) = 29.4, (cl-tr, +) = 12.6. etc. 7. Bad news Imagine that you are a doctor on a remote is brought in to see you. After a careful from disease A. However, a definite diagnosis can only be made after laboratory blood tests and you do not have the expensive island. One of the residents is suffering technical equipment. you suspect investigation the patient Since you have arrived on the island, you have investigated 40 similar cases. Later you found out that 30 cases actually had A, and IO laboratory checking) (after careful did not. that your patient is suffering from A‘? ( I ) What is the probability The probability is is limited (2) As your experience to 40 cases only, your estimate cannot be absolutely precise. Give a confidence for your estimate! I am 90% sure that the true value of the probability interval lies between . and . . For some time you thought that the diagnostic diagnosis of A. You found out that in the 30 cases suffering symptom and 2 I did not. Of the IO cases not suffering and 7 did not. You realize ( I ) What is the probability that your patient that your patient the diagnostic is showing sign B. sign B might be relevant for the the from A, only 9 showed from A, 3 showed symptom B is suffering from A given that the patient shows B? The probability (2) Give a confidence ._ is interval for your estimate! I am 90% sure that the true value lies between Of course, both the first and the second point probability . . . and . . . estimates should be 0.75. than your first one, and the though, should be be less precise therefore be wider than the first one. Assuming Your second estimate, second should interval is expressed by beta distributions distribution Be(9,3) with the same mean of course and the confidence second distribution In the example on the nondiagnostic is flatter than the first one. Its variance the observed data the first distribution interval is 0.75 and the 90% confidence is nondiagnostic. data seems to make things worse-which is Be(30,lO). is (0.64,0.86). interval is larger. The probabilistic the uncertainty The mean of the is The posterior The (0.56,0.94). conditioning is counterintuitive. We G.D. Kleiter/Art@cial Intelligence 88 (1996) 143-161 159 that additional new information assume We need a principle is not necessarily Inferences become more noisy. neutral to our arguments. that protects us from considering can never be bad for the quality of our inferences. data the imprecision of an argument. irrelevant data. Nondiagnostic It increases 7.1. The resolution of the paradox In the first part the cover story describes a reference system Gr consisting of only one i.e., the disease A together with the counts al = 30 then [alGIl node and its associated and a2 = 10. If we assume the improper prior Be(O,O), is distributed frequencies, as [alGIl = [+,a21 =Be(al,a~) =Be(30,10). In the second part the cover story introduces with the associated counts bl I = 9, b12 = 21, b21 = 3, and b22 = 7. The resulting system G2 now has two nodes. Let us investigate that our case shows symptom B the information the symptom B, together reference to of (Y in G2 prior a second node, the distribution [alG21 = [alal,a2;bll,bl:!,b21.b221. The first order marginal probability estimates of B may be estimated by the compound probability P(B) =P(A)P(BIA) +P(lA)P(BI-A) =0.3. that this value would be known exactly. If with probability the member distribution for a moment Let’s assume 0.3 we observe B then with this probability we will observe Be(9,3) probability has 0.3 x 0.01442 + 0.7 x 0.00647 = 0.00885. and P( -B) = 0.7 we will observe the mean 21/28 the mean 9/12 which has and the variance 0.01442. Similarily, with the member distribution Be(21,7) the which is therefore the variance 0.00647. The expected variance If we fit a beta distribution we obtain [alG2] = Be( 14.13,6.06). in G:! resulting is not known exactly is the expected posterior distribution This As P(B) distribution. Note that the mean of this distribution marginal distribution 20.19 cases. This is only half of 40, the total number of cases effective symptom B or 1B analysis. in the is again 0.7, i.e., it is the mean of the to only 14.13 +6.06 = there will actually be some more variability in G2 leads to the member distributions of A in Gt . Its precision in GI . Observing from a preposterior though corresponds [alb,Gz] = Be(9,3) and [al-b,G2] = Be(2,7), respectively. These distributions having clamped symptom b and lb, should not be interpreted to the same reference system. are the posterior distributions in the system G2 after respectively. The distribution of [ alGil and [ a\Gz] they do not belong because as prior and posterior distributions What makes this problem counterintuitive? Intuitively we discard the additional formation as soon as we have realized that it does not change the probability in- estimate. I60 G.D. Kleiter/Artijicicd Intelligence 88 (1996) 143-161 about [aiGlJ the situation the information reduces on irrelevant the problem we tend Thus, not conditionalize When we discuss compare This may be misleading. The conditioning prior and the posterior do not belong mation is associated with an improved of “monotone knowledge the precision. We have to compare distributions in CI with the information to Gt and the accuracy does not change, of course. We do information. We perform an elementary pruning process. in GI and G2. We in Cz. about is not based on the same information. The additional infor- in the principle to better shows that additional data can decrease to the same system. state of knowledge. We believe and less uncertainty. The example in the sense is equivalent information” information that more to protect Intuitively inferential [a[Gz] systems impact upon from variables with low diagnosticity. the first order probabilities low positive Such but may have is a and the loss in second inferences. There negative impact upon the improvement the precision of the system’s in first order probabilities have variables considerable trade-off between order precision. References 1 I I J. Aitchison, The Statistictrl Anc~~wrs of Contl”‘.~itiorftrl Dotct (Chapman and Hall, London, 1986). 12 1 J. Aitchison and I.R. Dunsmore. Statistical Prrdictim (Cambridge University Press, Cambridge, Arw/ysis 1975). I 3 J J.M. Bernard0 and A.F.M. Smith, Btryesim Theory 141 Y.M. Bishop, SE. Fienberg and PW. Holland. Discrete Multivurime (Wiley, Chichester. 1994). Analwis: Theory and Pmctice (MIT Press, Cambridge, MA, 1975). [ 5 I W.L. Buntine, Operations for learning with graphical models, .I. ArtlJ’: Me/l. Rex 2 ( 1994) 159-225. 161 W. Buntine, A guide to the learning probabilistic networks literature on from data, IEEE Trans. Knowledge Duta Eng. (to appear). I7 1 R.M. Chavez and G.E Cooper, A randomized approximation algorithm for probabilistic inference on Bayesian belief networks, Nenuorks 20 ( 1990) 66 I-68.5. 18 I P Che, R.E. Neapolitan, J. Kenevan and M. Evens, An implementation of a method for computing in: D. Heckerman and A. Mamdami, eds., in belief networks, in inferred probabilities the uncertainty Uncertainty in Artificial intelligence (Kaufmann, San Mateo, CA, 1993) 292-300. 19 I G. Coletti, A. Gilio and R. Scozzafava. Conditional events with vague information in expert systems, in: B. Bouchon-Meunier and R.R. Yager. eds.. Uncertairzt~ in Knowledge Bases (Springer, Berlin, 199 I ) 106-114. I IO] A.P Dempster, N.M. Laird and D.B. Rubin, Maximum likelihood estimation from incomplete data via the EM algorithm (with discussion), J. Roy. Stat. SM. Ser. B 39 ( 1977) I-38. fntroduction to Graphiccd ModellinK I I I I C. Edwards, I I2 I K.W. Fertig and J.S. Breese, Interval influence diagrams, in: H. Henrion, R.D. Shachter, L.N. Kanal and 5 (North-Holland. Amsterdam, 1990) 149-161. Intelligence I I3 I A. Gammerman, Z. Luo, C.G.G. Aitken and M.J. Brewer, Exact and approximate algorithms and their Reasoning and in: A. Gammerman. ed.. Probabilistic J.F. Lemmer, eds., Uncertainty (Springer, New York, 199.5). in mixed graphical models, implementations in Arti$cial Bayesian Beliqf Networks (Alfred Wailer, Henley-on-Thames, 1995) 33-S3. I 141 A.E. Gelfand and A.F.M. Smith, Sampling-based approaches to calculating marginal densities, J. Am. Stuf. Assoc. 85 ( 1990) 398-409. I IS I P Hajek, T. Havranek and R. JirouSek, Uncertain frzformarion Processing in Experr Sysrems (CRC Press, Boca Raton, FL, 1992). I 161 D. Heckerman, A tutorial on learning Bayesian networks, Microsoft Research, Advanced Technology Division, Microsoft Corporation, Redmond, WA ( 1995) (heckerma@microsof t . corn). G.D. Kleiter/Artijicial Intelligence 88 (1996) 143-161 161 1 I7 1 D. Heckerman, D. Geiger and D. Chickering, Learning Bayesian networks: the combination of knowledge and statistical data, Much. Learn. 20 ( 1995) 197-243. 1 181 T. Hrycej, Gibbs sampling in Bayesian networks, Art$ Intell. 46 ( 1990) 351-363. [ 191 N.L. Johnson and S. Katz, Urn Models and their Application (Wiley, New York, 1977). 1201 J.M. Keynes, A Treatise on Probability (MacMillan, London, 1921). 1211 G.D. Kleiter, Bayesian diagnosis in artificial intelligence, Artif: Infell. 54 ( 1992) l-32. ( 221 G.D. Kleiter, Properties of probabilistic imprecision, in: B. Buchon-Meunier, L. Valverde and R.R. Yager, eds., Uncertainty in lnfelligent Systems (North-Holland, Amsterdam, 1993) 155-170. 1231 G.D. Kleiter, Natural sampling: rationality without base rates, in: G.H. Fischer and D. Laming, eds., Contributions to Mathematical Psychology, Psychometrics, and Methodology (Springer, New York, 1994) 375-388. [ 24 1 G.D. Kleiter, The precision of Bayesian classification: the multivariate normal case, Inf. J. General Syst. 22 (1994) 139-157. [ 251 G.D. Kleiter, Expressing imprecision in probabilistic knowledge, .I. Italian Stat. Sot. 2 ( 1994) 213-232. [ 261 G.D. Kleiter and M. Kardinal, A Bayesian approach to imprecision in belief nets, in: V. Mammitzsch and H. SchneeweiO, eds., Symposia Gaussiana, Proceedings of the 2nd Gauss Symposium, Conference B: Statistical Sciences (De Gruyter, Berlin, 1995) 91-105. [ 271 W. Lam and F. Bacchus, Learning Bayesian belief networks: an approach based on the MDL principle, Comput. Intell. 10 ( 1994) 269-293. [ 281 S.L. Lauritzen and N. Wermuth, Graphical models for associations between variables, some of which are qualitative and some quantitative, Ann. Stat. 17 (1989) 31-57. [ 291 R.J.A. Little and D.B. Rubin. Sfatistical Analysis with Missing Data (Wiley, New York, 1987). ]30 131 R.E. Neapolitan, Probabilistic Reasoning in Expert Systems (Wiley, New York, 1990). R.E. Neapolitan and J.R. Kenevan, Investigations of variances in belief networks, in: Uncertainty in Artijkiul Intelligence (North-Holland, Amsterdam, 199 I ) 232-240. G.W. Oehlert, A note on the delta method, Am. Stat. 46 (1992) 27-29. G. PaaS, Second order probabilities for uncertain and conflicting evidence, in: PP. Bonissone, M. Hemion, L.N. Kanal and J.F. Lemmer, eds., Uncertainty in Artificial Infe[ligence 6 (North-Holland, Amsterdam, I991 ) 447-456. 132 133 1341 J. Pearl, Probabilistic Reasoning in fnrelligent Systems (Morgan Kaufman, San Mateo, CA, 1988). [ 351 Raima Database Manager, 1605 NW Sammamish Rd. Suite 200, Isaaquah, WA, 98027, USA ( 1994). [ 361 A. Runnalls, A survey of sampling methods for inference on directed graphs, in: P Cheeseman and R.W. Oldford, eds., Selecting Models from Dafa (Springer, New York, 1994) 153-162. 1371 D.J. Spiegelhalter, Probabilistic reasoning in predictive expert systems, in: L.N. Kanal and J.F. Lemmer, eds., Uncertainty in Arftjicial Intelligence (North-Holland, Amsterdam, 1986) 47-68. [ 38 1 D.J. Spiegelhalter, A unified approach to imprecision and sensitivity of beliefs in expert systems, MCR Biostatistics Unit, Cambridge ( 199 1) [ 391 D. Spiegelhalter, A. Dawid, S. Lauritzen and R. Cowell, Bayesian analysis in expert systems, Star. Sci. (1993) 219-283. [ 401 D.J. Spiegelhalter, R.C.G. Franklin and K. Bull, Assessment, criticism and improvement of imprecise subjective probabilities for a medical expert system, in: M. Hemion, R.D. Shachter, L.N. Kanal and J.F. Lemmer, eds., Uncertainty in Artificial Intelligence 5 (North-Holland, Amsterdam, 1990) 285-294. ]4l] A. Thomas, D. Spiegelhalter and W. Gilks, BUGS: a program to perform Bayesian inference using Gibbs sampling, in: J. Bemardo, J. Berger, A. Dawid and AIM. Smith, eds., Buyesian Statistics 4 (Oxford University Press, Oxford, 1992) 837-842. 1421 P Walley, Statistical Reasoning with Imprecise Probabilities (Chapman and Hall, London, 1991). [ 431 J. Whittaker, Graphical Models in Applied Multivariate Statistics (Wiley, Chichester, 1990). 1441 S.S. Wilks, Mathematical Statistics (Wiley, New York, 1962). 