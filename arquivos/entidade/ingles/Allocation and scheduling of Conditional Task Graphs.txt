Artificial Intelligence 174 (2010) 500–529Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAllocation and scheduling of Conditional Task GraphsMichele Lombardi, Michela Milano∗DEIS Universita’ di Bologna, Viale Risorgimento, 2, 40136 Bologna, Italya r t i c l ei n f oa b s t r a c tArticle history:Received 29 October 2008Received in revised form 22 February 2010Accepted 22 February 2010Available online 24 February 2010Keywords:Constraint ProgrammingProbabilistic reasoningScenariosConditional Task GraphsConditional constraintsOptimizationWe propose an original, complete and efficient approach to the allocation and schedulingof Conditional Task Graphs (CTGs). In CTGs, nodes represent activities, some of themare branches and are labeled with a condition, arcs rooted in branch nodes are labeledwith condition outcomes and a corresponding probability. A task is executed at run timeif the condition outcomes that label the arcs in the path to the task hold at scheduleexecution time; this can be captured off-line by adopting a stochastic model. Tasks needfor their execution either unary or cumulative resources and some tasks can be executedon alternative resources. The solution to the problem is a single assignment of a resourceand of a start time to each task so that the allocation and schedule is feasible in eachscenario and the expected value of a given objective function is optimized. For thisproblem we need to extend traditional constraint-based scheduling techniques in twodirections: (i) compute the probability of sets of scenarios in polynomial time, in orderto get the expected value of the objective function; (ii) define conditional constraintsthat ensure feasibility in all scenarios. We show the application of this framework onproblems with objective functions depending either on the allocation of resources to tasksor on the scheduling part. Also, we present the conditional extension to the timetableglobal constraint. Experimental results show the effectiveness of the approach on a set ofbenchmarks taken from the field of embedded system design. Comparing our solver with ascenario based solver proposed in the literature, we show the advantages of our approachboth in terms of execution time and solution quality.© 2010 Elsevier B.V. All rights reserved.1. IntroductionConditional Task Graphs (CTG) are directed acyclic graphs whose nodes represent activities, linked by arcs representingprecedence relations. Some of the activities are branches and are labeled with a condition; at run time, only one of the suc-cessors of a branch is chosen for execution, depending on the occurrence of a condition outcome labeling the correspondingarc. The truth or the falsity of those condition outcomes is not known a priori: this sets a challenge for any off-line designapproach, which should take into account the presence of such elements of uncertainty. A natural answer to this issue isadopting a stochastic model. Each activity has a release date, a deadline and needs a resource to be executed. The problemis to find a resource assignment and a start time for each task such that the solution is feasible whatever the run timescenario is and such that the expected value of a given objective function is optimized. We take into account differentobjective functions: those depending on the resource allocation of tasks and those depending on the scheduling side of theproblem.* Corresponding author.E-mail address: michela.milano@unibo.it (M. Milano).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.02.004M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529501Fig. 1. Some pseudo-code and a its translation into a CTG.CTG are ubiquitous to a number of real life problems. In compilation of computer programs [13], for example, CTGs areused to explicitly take into account the presence of conditional instructions. Similarly, in the field of system design [39],CTGs are used to describe applications with if-then-else statements; in this case tasks represent processes and arcs are datacommunications. Once a hardware platform and an application is given, to design a system amounts to allocate platformresources to processes and to compute a schedule; in this context, taking into account branches allows better resource usage,and thus lower costs. CTG may be used also in the Business Process Management (BPM) [34] and in workflow management[30], as a mean of describing operational business processes with alternative control paths.For solving the allocation and scheduling problem of CTG we need to extend the traditional constraint based techniqueswith two ingredients. First, to compute the expected value of the objective function, we need an efficient method forreasoning on task probabilities in polynomial time. For example, we have to compute the probability a certain task executesor not, or, more in general, the probability of a given set of scenarios with uniform features (e.g. the same objective functionvalue). Second, we need to extend traditional constraints to take into account the feasibility in all scenarios.For this purpose, we define a data structure called Branch/Fork Graph – BFG. We show that if the CTG satisfies a propertycalled Control Flow Uniqueness – CFU, the above mentioned probabilities can be computed in polynomial time. CFU is aproperty that holds in a number of interesting applications, such as for example the compilation of computer programs,embedded system design and in structured business processes.The paper is organized as follows: Section 2 presents some applications where CTG is a convenient representation ofproblem entities and their relations; in Section 3 we provide some preliminary notions on Constraint-Based Scheduling.Section 4 introduces the concept of Conditional Task Graphs, Control Flow Uniqueness, sample space and scenarios anddefines the scheduling and allocation problem we consider. In Section 5 we define the data structure used for implementingefficient probabilistic reasoning, namely the Branch/Fork Graph and related algorithms. In Section 6 we use these algorithmsfor efficiently computing the expected value of three objective function types, while in Section 7 we exploit the BFG forimplementing the conditional variant of the timetable global constraint. Section 8 discusses related work and Section 9shows experimental results and a comparison with a scenario based approach.2. Applications of CTGsConditional Task Graphs can be used as a suitable data structure for representing activities and their temporal relationsin many real life applications. In these scenarios, CTG allocation and scheduling becomes a central issue.In compilation of computer programs [13], for example, CTGs are used to explicitly take into account the presenceof conditional instructions. For instance, Fig. 1 shows a simple example of pseudo-code and a natural translation intoa CTG; here each node corresponds to an instruction and each branch node to an “if” test; branch arcs are label withthe outcome they represent. In this case, probabilities of condition outcomes can be derived from code profiling. Clearly,computer programs may contain loops that are not treated in CTGs, but modern compilers adopt the loop unrolling [17]technique that can be used here for obtaining cycle free task graphs.Similarly, in the field of embedded system design [39] a common model to describe a parallel application is the taskgraph. The task graph has a structure similar to a data flow graph, except that the tasks in a task graph represent largerunits of functionality. However, a task graph model that has no control dependency information can only capture datadependency in the system specification. Recently, some researchers in the co-synthesis domain have tried to use conditionaltask graph to capture both data dependencies and control dependencies of the system specification [42,18]. Once a hardwareplatform and an application is given, to design a system amounts to allocate platform resources to processes and to computea schedule; in this context, taking into account branches allows better resource usage, and thus lower costs. However, thepresence of probabilities makes the problem extremely complex since the real time and quality of service constraints shouldbe satisfied for any execution scenario. Embedded system design applications will be used in this paper to experimentallyevaluate the performance and quality of our approach.CTG appear also in Business Process Management (BPM) [34] and in workflow management [30] as a mean of describingoperational business processes with alternative control paths. Workflows are instances of workflow models, that are repre-sentations of real-world business processes [41]. Basically workflow models consist of activities and the ordering amongstthem. They can serve different purposes: they can be employed for documentation of business processes or can be used asinput to a Workflow Management System that allows their machine-aided execution.502M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529One of the most widely used systems for representing business processes is BPEL [27]. BPEL is a graph-structured lan-guage and allows to define a workflow model using nodes and edges. The logic of decisions and branching is expressedthrough transition conditions and join conditions. Transition conditions and join conditions are both Boolean expressions.As soon as an activity is completed, the transition conditions on their outgoing links are evaluated. The result is set as thestatus of the link, which is true or false. Afterwards, the target of each link is visited. If the status of all incoming links isdefined, the join condition of the activity is evaluated. If the join condition evaluates to false, the activity is called dead andthe status of all its outgoing links is set to false. If the join condition evaluates to true, the activity is executed and thestatus of each outgoing link is evaluated. CTGs behave exactly in the same fashion and can be used to model BPEL workflowmodels. In addition CTG provide probabilities on branches. Such numbers, along with task durations and resource consump-tion and availability can be extracted from process event logs. The CTG allocation and scheduling proposed in this papercan be used in the context of workflow management as a mean to predict the completion time of the running instances, asdone in [1], or for scheduling tasks to obtain the minimal expected completion time.3. Preliminaries on constraint-based schedulingIn this paper we show how to extend constraint-based scheduling techniques for dealing with probabilistic informationand with conditional task graphs. We therefore provide some preliminary notions on Constraint-Based Scheduling.Constraint-Based Scheduling [4] is a subfield of the area of Constraint Programming (CP). Generally speaking, CP is con-cerned with solving Constraint Satisfaction Problems (CSPs). A CSP is a triple (cid:3) X, D, C(cid:4), where X is a set of variables, D isthe set of their domains and C is a set of constraints; a constraint denotes a relation between the values of the variablesit refers to. Solving a CSP consists of assigning values to variables such that all constraints are satisfied simultaneously.In CP, constraints are actively exploited to reduce the domains of the variables and to detect inconsistencies via constraintpropagation. For example, let D( X) denote the domain of variable x; then given D(x) = D( y) = {0, 1, 2, 3} and a constraintx < y, after constraint propagation D(x) is reduced to {0, 1, 2} and D( y) to {1, 2, 3}. Note that detecting all problem in-consistencies is as difficult as solving the original problem. Thus, constraint propagation enforces only partial (namely local)consistency [11]; consequently, one needs to perform some kind of search to determine whether the CSP instance at handhas a solution or not.Scheduling problems over a set of activities are classically modeled in CP by introducing for every activity three vari-ables representing the start time (start), end time (end) and duration (dur). In this context a solution (or schedule) is anassignment of all start and end variables. “Start”, “end” and “duration” variables must satisfy the constraint end = start + dur.Activities require a certain amount of resources for their execution. We consider in this paper both unary resources anddiscrete (or cumulative) resources. Unary resources have capacity equal to one and two tasks using the same unary resourcecannot overlap in time, while cumulative resources have finite capacity that cannot be exceeded at any point in time.Scheduling problems often involve precedence relations and alternative resources; precedence relations are modeled bymeans of constraints between the start and end variables of different activities, while special resource constraints guaranteethe capacity of each resource is never exceeded in the schedule; a number of different propagation algorithms for temporaland resource constraints [3,21] enable an effective reduction of the search space. Finally, special scheduling oriented searchstrategies [4] have been devised to efficiently find consistent schedules or to prove infeasibility.4. Problem descriptionThe problem we consider is the scheduling of Conditional Task Graphs (CTG) in presence of unary and cumulativealternative resources. In the following, we introduce the definitions needed in the rest of the paper. In Section 4.1 weprovide some notions about Conditional Task Graphs, Section 4.2 concerns Control Flow Uniqueness, a CTG property thatenables the definition of polynomial time CTG algorithms, Section 4.3 introduces the concept of sample space and scenarioswhile Section 4.4 describes the scheduling and allocation problem considered in the paper.4.1. Conditional Task GraphA CTG is a directed acyclic graph, where nodes are partitioned into branch and fork nodes. Branches in the executionflow are labeled with a condition. Arcs rooted at branch nodes are labeled with condition outcomes, representing whatshould be true in order to traverse that arc at execution time, and their probability. Intuitively, fork nodes originate parallelactivities, while branch nodes have mutually exclusive outgoing arcs.More formally:Definition 1. A CTG is a directed acyclic graph that consists of a tuple (cid:3)T , A, C, P (cid:4), where• T = T B ∪ T F is a set of nodes; ti ∈ T B is called a branch node, while ti ∈ T F is a fork node. T B and T F partition set T ,i.e., T B ∩ T F = ∅. Also, if T B = ∅ the graph is a deterministic task graph.• A is a set of arcs as ordered pairs ah = (ti, t j).• C is a set of pairs (cid:3)ti, ci(cid:4) for each branch node ti ∈ T B . ci is the condition labeling the node.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529503Fig. 2. A: example of CTG; B: probabilities of condition outcomes.• P is a set of triples (cid:3)ah, Out, Prob(cid:4) each one labeling an arc ah = (ti, t j) rooted in a branch node ti . Out = Outi j is apossible outcome of condition ci labeling node ti , and Prob = pi j is the probability that Outi j is true (pi j ∈ [0, 1]).The CTG always contains a single root node (with no incoming arcs) that is connected (either directly or indirectly) toFor each branch node ti ∈ T B with condition ci every outgoing arc (ti, t j) is labeled with one distinct outcome Outi j sucheach other node in the graph.(cid:2)that(ti ,t j ) pi j = 1.Intuitively, at run time, only a subgraph of the CTG will execute, depending on the branch node condition outcomes. Eachtime a branch node is executed, its condition is evaluated and only one of its outgoing arcs is evaluated to true. In Fig. 2A ifcondition a is true at run time, then arc (t1, t2) status is true and node t2 executes, while arc (t1, t5) status is false and nodet5 does not execute. Without loss of generality, all examples throughout this paper target graphs where every condition, saya, has exactly two outcomes, a = true or a = false. However, we can model multiple alternative outcomes, say a = 1 or a = 2or a = 3 provided that they are mutually exclusive (i.e., only one of them is true at run time).In Fig. 2A t0 is the root node and it is a fork node that always executes at run time. Arcs (t0, t1) and (t0, t12) rooted inan executing fork node are always evaluated to true. Node t1 is a branch node, labeled with condition a. With an abuse ofnotation we have omitted the condition in the node and we have labeled arc (t1, t2) with the outcome a meaning a = trueand (t1, t5) with ¬a meaning a = false. The probability of a = true is 0.5 and the probability of a = false is also 0.5.Let A+(ti) be the set of outgoing arcs of node ti , that is A+(ti) = {ak ∈ A | ak = (ti, t j)}; similarly let Aof ingoing arcs of node ti , i.e., Aingoing arc), ti is a tail node if | A−(ti) = {ak ∈ A | ak = (t j, ti)}. Then ti is a said to be a root node if | A+(ti)| = 0 (ti has no outgoing arc).−(ti) be the set−(ti)| = 0 (ti has noWithout loss of generality, we restrict our attention to CTGs such that every node ti with two or more ingoing arcs+(ti)| > 1) is either an and-node or an or-node. The concept of and/or-nodes, that of executing node and the arc status(| Acan be formalized in a recursive fashion:Definition 2. Run time execution of nodes, arc status, and/or node definition:• The root node always executes.• The status of arc (ti, t j) rooted in a fork node ti ∈ T F is true if node ti executes.• The status of arc (ti, t j) rooted in a branch node ti ∈ T B is true if node ti executes and the outcome Outi j labeling thearc is true.• A node ti with | A• An or-node ti executes if any arc in A• A node ti with | A• An and-node executes if all arcs in A+(ti) has a status equal to true.−(ti) have a status equal to true.−(ti)| (cid:2) 1 is an and-node if it is possible that all the ingoing arcs status are true at run time.−(ti)| > 1 is an or-node if either none or only one of the ingoing arcs status can be true at run-time.Note that the definition is recursive: deciding whether a node ti with | A+(ti)| is an and/or-node depends on whetherits predecessors can execute, and deciding whether a node can execute requires to know whether the predecessors areand/or-nodes. The system is however consistent as both concepts only depend on information concerning the predecessorsof the considered node ti ; as the root node by definition always executes and the CTG contains no cycle, both the conceptof and/or-node and that of executing node/status of an arc are well defined.504M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Observe that ingoing arcs in an or-node are always mutually exclusive; mixed and/or-nodes are not allowed but can bemodeled by combining pure (possibly fake) and-nodes and or-nodes. Note also that nodes with a single ingoing arc areclassified as and-nodes. Again, for the sake of simplicity in the paper we have used examples with only two ingoing arcs inand/or-nodes, but the presented results are valid in general and apply for any number of ingoing arcs.In Fig. 2A t15 is an or-node since at run time either the status of (t14, t15) or the one of (t13, t15) is true (dependingon the outcome of condition d); t21 is an and-node since, if condition a has outcome false, arc (t20, t21) is true and arc(t10, t21) status is true if the outcome c = true holds. Therefore, it is possible that both incoming arcs are true at run time.t15 executes if any of the ingoing arcs status is true, while t21 executes only if both the ingoing arc status evaluate to true.4.1.1. Activation event of a nodeFor modeling purposes, it is useful to express the combination of outcomes determining the execution of a node as acompact expression. As outcomes are logical entities (either they are true or false at run time) it is convenient to formulatesuch combination of outcomes as a logical expression, referred here to as activation event.The activation event of a node ti is denoted as ε(ti) and can be obtained in a recursive fashion, similarly to definition ofexecuting node and and/or-nodes. In practice:ε(ti) =⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩true(cid:7)ak=(t j ,ti )∈ A−(ti )(cid:8)ak=(t j ,ti )∈ A−(ti )−if | A(ti)| = 0 (ti is the root node)ε(ak)if ti is an or-nodeε(ak)if ti is an and-node or if | A−(ti)| = 1and ε(ak) is the activation event of an arc ak and is defined as follows:(cid:9)(cid:10)ak = (ti, t j)ε=(cid:11)ε(ti)ε(ti) ∧ Outi jif ti is a forkif ti is a branchFor example, the activation event of task t2 in Fig. 2A is ε(t2) = a, while the activation event of t21 is ε(t21) = ((¬a ∧b) ∨ (¬a ∧ ¬b)) ∧ (¬a ∧ c) = (¬a ∧ b ∧ c) ∨ (¬a ∧ ¬b ∧ c) = ¬a ∧ c ∧ (b ∨ ¬b) = ¬a ∧ c.In general we need to express activation events in Disjunctive Normal Form (DNF), that is a disjunction of one or moreconjunctions of one or more literals.4.2. Control flow uniquenessEven if many of the definitions and algorithms we present in this paper work in the general case, we are interested inspecific CTG satisfying a property called Control Flow Uniqueness (CFU).1 Intuitively, CFU is satisfied if no node ti in the graphrequires for its execution the occurrence of two outcomes found on separated paths from the root to ti . More formally:Definition 3. A CTG satisfies the CFU if for each and-node ti , there is a single arc a ∈ Aarcs a−(ti):(cid:11) ∈ Astatus of arc a is true ⇒ status of arc a(cid:11)is true−(ti) such that, for all other incoming−(ti) is logically responsible ofwhere the symbol “⇒” denotes the logical implication. Intuitively a single ingoing arc a ∈ Athe execution of the and-node ti ; if the status of such arc becomes true at some point of time, the status of all other ingoingarcs will become (or have become) true as well. Note the actual run time execution of ti only occurs once all ingoing arcshave become true. As a consequence there is also only one path from the root to the and-node that is logically responsiblefor the execution of that node. More formally:Corollary. If a CTG satisfies CFU, then for each task ti each conjunction of condition outcomes in its activation event ε(ti) (in DNF) canbe derived by collecting condition outcomes following a single path from the root node to ti .For example in Fig. 3A, task t8 is an and-node; its activation event is ε(t8) = (a ∧ b) ∨ (¬a ∧ b) = b, thus CFU holds.Conversely, in Fig. 3B both ¬a and b are strictly required for the execution of t7 and they do not appear in sequence alongany path from the root to t7; hence CFU is not satisfied.In many practical cases CFU is not a restrictive assumption; for example, when the graph results from the parsing of acomputer program written in a high level language (such as C++, Java, C#) CFU is quite naturally enforced by the scope rulesof the language, or can be easily made valid by proper modeling. For example, consider again the pseudo-code in Fig. 1.1 In the rest of the paper, we will explicitly underline which algorithms/properties need the CFU.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529505Fig. 3. A: a CTG which satisfies CFU; B: a CTG which does not satisfy CFU.One can easily check that 1) CFU is satisfied and 2) there exist no simple translation of the pseudo code violating CFU aseach conditional instruction (if) has a collector node (end if).Moreover, in some application domains (e.g., business process management, embedded system design), a common as-sumption is to consider so called structured graphs, i.e., graphs with a single collector node for each conditional branch. Inthis case, the CFU is trivially satisfied. Note how a structured graph cannot model early exits (e.g. in case of error), as theone reported in Fig. 1.4.3. Sample space and scenariosOn top of a CTG we define the sample space S.Definition 4. The sample space of a CTG is the set of events occurring during all possible executions of a CTG, each eventbeing a set of condition outcomes.For example, the sample space defined on top of the CTG in Fig. 2A can be computed by enumerating all possible graphexecutions and contains 20 events. Again using an abuse of notation we refer to the outcome a = true with a and to theoutcome a = false with ¬a. Also, for sake of clarity we have removed the logical conjunctions among conditions: the terma ∧ b ∧ e has been simplified in abe. Therefore, the sample space associated to the CTG in Fig. 2A is the following.S = {ade, ad¬e, a¬de, a¬d¬e, ¬abcde, ¬abc¬de, ¬abcd¬e,¬abc¬d¬e, ¬a¬bcde, ¬a¬bc¬de, ¬a¬bcd¬e, ¬a¬bc¬d¬e,¬ab¬cde, ¬ab¬c¬de, ¬ab¬cd¬e, ¬ab¬c¬d¬e, ¬a¬b¬cde,¬a¬b¬c¬de, ¬a¬b¬cd¬e, ¬a¬b¬c¬d¬e}We need now to associate a probability to each element of the sample space.∀s ∈ Sp(s) =(cid:12)pi jOuti j ∈sFor instance, with reference to Fig. 2A, the probability of event ade is 0.5 ∗ 0.3 ∗ 0.7 = 0.105.Each event in the sample space of the CTG is associated to a scenario. A scenario corresponds to a deterministic taskgraph containing the set of nodes and arcs that are active in the scenario. We have to define how to build such a task graph.This task graph is defined recursively.Definition 5. Given a CTG=(cid:3)T , A, C, P (cid:4), and an event s ∈ S the deterministic task graph TG(s) associated with s is defined asfollows:• The CTG root node always belongs to the TG(s).• A CTG arc(ti, t j) belongs to TG(s) if either◦ ti is a fork node and ti belongs to TG(s) or◦ ti is a branch node, Outi j ∈ s and ti belongs to TG(s).arc ak ∈ A−(ti) is in TG(s).• A CTG node ti belongs to TG(s) if it is an and-node and all arcs ak ∈ A−(ti) are in TG(s) or if it is an or-node and anyTG(s) is called scenario associated with the event s.506M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Fig. 4. The deterministic task graph associated with the run time scenario a = true, d = true and e = false, for the CTG of Fig. 2.With an abuse of notation, in the following we refer to the event s also as scenario. The deterministic task graph derivedfrom the CTG in Fig. 2A associated to the run time scenario a = true, d = true and e = false (or equivalently ad¬e) is depictedin Fig. 4.Often we are interested in identifying a set of scenarios, such as for instance all scenarios where a given task executes.We have to start by identifying the events associated to scenarios where task ti executes. This set is defined as S i = {s ∈ S |ti ∈ TG(s)}. The probability that a node ti executes (let this be p(ti)) can then be computed easily: p(ti) =p(s). Forexample, let us consider task t2 in Fig. 2A; then S(t2) = {ade, ad¬e, a¬de, a¬d¬e} and p(t2) = 0.5 · 0.3 · 0.7 + 0.5 · 0.3 · 0.3 +0.5 · 0.7 · 0.7 + 0.5 · 0.7 · 0.3 = 0.5. Alternatively, the probability p(ti) can be computed starting from the activation event;for example, ε(t2) = a, hence p(t2) = p(a) = 0.5, or ε(t8) = ¬a ∧ b, hence p(t8) = p(¬a) · p(b) = 0.5 · 0.4 = 0.2.s∈S i(cid:2)For modeling purposes, we also define for each task an activation function fti (s); this is a stochastic function fti: S →{0, 1} such that(cid:11)fti (s) =1 if ti ∈ TG(s)0 otherwiseFinally, we need to define the concept of mutually exclusive tasks:Definition 6. Two tasks ti and t j are said to be mutually exclusive mutex(ti, t j) iff there is no scenario TG(s) where bothtasks execute, i.e., where ti ∈ TG(s) and t j ∈ TG(s) or equivalently, fti (s) = ft j (s) = 1.4.4. Scheduling and allocation of a CTG: Problem definition and modelThe allocation and scheduling problem we face is defined on a conditional task graph whose nodes are interpreted asactivities (also referred to as tasks) and arcs are precedence relations between pairs of activities. The CTG is annotated witha number of activity features, such as duration, due and release dates, alternative resource sets and resource consumption.We have to schedule tasks and assign them resources from the alternative resource set such that all temporal and resourceconstraints in any run time scenario are satisfied and the expected value of a given objective function is optimized. Moreformally:Definition 7. An instance of the CTG allocation and scheduling problem is a tuple (cid:3)CTG, Obj, Dur, Rel, Due, ResSet, ResCons(cid:4).In the CTG = (cid:3)T , A, C, P (cid:4) T represents the set of non-preemptive tasks to be allocated and scheduled, A represents theset of precedence constraints between pairs of tasks, C is the set of conditions labeling the nodes and P is the set ofoutcomes and probabilities labeling the arcs. Obj is the objective function. Dur, Rel, Due, ResSet and ResCons are functionsmapping each task in T to the respective duration, release date, due date, alternative resource set and resource consumption.Given a task ti ∈ T , its duration is referred to as Duri , its release date as Reli , its due date as Duei , its alternative resourceset to as ResSeti and its resource consumption ResConsi ; with the exception of ResSet all mentioned functions have values inN+.For sake of simplicity, we assume each task ti needs a single resource taken from its ResSeti for its execution; however,the results presented in this paper can be easily extended to tasks requiring more than one resource. More in detail, supposeeach task requires up to m of types of resource; provided separate ResSet and ResCons functions, all the presented resultsapply to each type in a straightforward fashion.4.4.1. Modeling tasks and temporal constraintsAs far as the model is concerned, each node in the CTG corresponds to a task (also called activity). Similarly to constraint-based scheduling, a task ti is associated to a time interval [start(ti), end(ti)) where start(ti) is a decision variable denotingthe starting time of the task and end(ti) is a variable linked to start(ti) as follows: end(ti) = start(ti) + Duri . Depending onthe problem, the duration may be known in advance or may be a decision variable. In this paper we consider fixed, knownin advance, durations.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529507The start time variable of a task ti has a domain of possible values ranging from the earliest start time est(ti) to thelatest start time lst(ti). Similarly, the end time variable has a domain ranging from earliest to the latest end times, referredto as eet(ti) and let(ti ). Initially start(ti) and end(ti) range on the whole schedule horizon (from the time point 0 to the endof horizon [0..eoh]).Each arc (ti, t j) in the CTG corresponds to a precedence constraint on decision variables and has the form start(ti) +Duri (cid:3) start(t j). Due dates and release dates translate to constraints start(ti) + Duri (cid:3) Duei and start(ti) (cid:2) Reli .4.4.2. Modeling alternative resourcesBeside start time of activities, an additional decision variable res(ti) represents the resource assigned to the activity ti .The domain of possible values of res(ti) is ResSeti .Resources in the problem can be either discrete or unary. Discrete resources (also referred to as cumulative resources)have a known maximal capacity. A certain amount of resource ResConsi is consumed by activity ti assigned to a discreteresource res(ti) at the start time of activity ti and the same quantity is released at its end time.A unary resource has a unit capacity. It imposes that all activities requiring the same unary resource are totally ordered.Given a resource R its capacity is referred to as Cap(R).4.4.3. Classical objective function typesDepending on the problem, the objective function may depend on the temporal allocation of the activities, i.e., decisionson variables start (or equivalently variables end if the duration is fixed), or on the resource assignments, i.e., decisions onvariables res.In constraint-based scheduling, a widely studied objective function is the makespan, i.e., the length of the whole sched-ule. It is the maximum value of the end(ti) variables.Obj1= maxti ∈Tend(ti)(1)Another example of objective function is the sum of costs of resource assignments to single tasks. As an example, considerthe case where running a task on a given resource consumes a certain amount of energy or power.(cid:13)Obj2=(cid:9)(cid:10)ti, res(ti)cost(2)ti ∈TIn the hypothesis to have a cost matrix where each element ci j is the cost of assigning resource j to task ti , res(ti) = j ↔cost(ti, res(ti)) = ci j .A third example that we will consider in this paper still depends on resource assignment, but on pairs of assignments.(cid:9)(cid:10)ti, res(ti), t j, res(t j)cost(3)(cid:13)Obj3=ak=(ti ,t j )∈ AFor instance, suppose that arcs represent communication activities between two tasks. If ti and t j are assigned to thesame resource, their communication cost is zero, while if they are assigned to different resources, the communication costincreases. Suppose we have a vector where each element ck is the cost of arc arck = (ti, t j) if ti and t j are assigned todifferent resources.(cid:9)(cid:10)ti, res(ti), t j, res(t j)=cost(cid:11)ck0if res(ti) (cid:17)= res(t j)otherwiseOther objective functions could be considered as well. For example there could be a cost for having at least one task usinga certain resource (e.g. to turn the resource “on”). In this case the cost is associated to the execution of sets of tasks; theobjective function can be considered a generalization of Obj3 and is dealt with by means of the same techniques.Clearly, having probabilities and conditional branches, we have to take into account all possible run time scenarios andoptimize the expected value of the objective function. Therefore, given the objective function Obj and a scenario s, we refer(s)to the objective function computed in the scenario s to as Obj(s). For example, in Obj1 , the maximum of end variables isrestricted only to those tasks that are active in the scenario s (those belonging to TG(s)).Obj(s)1= maxti ∈TG(s)end(ti) = maxti ∈Tfti (s)end(ti)SimilarlyObj=(s)2and(cid:13)ti ∈TG(s)cost(ti) =(cid:13)ti ∈Tfti (s)cost(cid:9)(cid:10)ti, res(ti)508M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529(s)Obj3==(cid:13)(cid:9)(cid:10)res(ti), res(t j)costak=(ti ,t j )∈TG(s)(cid:13)fti (s) ft j (s)costak=(ti ,t j )∈ A(cid:10)(cid:9)ti, res(ti), t j, res(t j)Finally, by recalling the definition of “expected value” in probability theory, we state that:Definition 8. The expected value of a given objective function Obj is a weighted sum of the Obj(s) weighted by scenarioprobabilities E(Obj) =(cid:2)s∈S p(s)Obj(s).4.4.4. Solution of the CTG scheduling problemThe solution of the CTG scheduling problem can be given either in terms of a scheduling and allocation table [42] whereeach task is assigned to a different resource and a different start time, depending on the scenario, or as a unique allocationschedule where each task is assigned a single resource and a single start time independently on the run time scenario. Thefirst solution is much more precise and able to better optimize the expected value of the objective function. Unfortunately,the size of such a table grows exponentially as the number of scenarios increases, making the problem of computing anoptimal scheduling table P-SPACE complete (it is analogous to finding an optimal policy in stochastic constraint programming[40]). We therefore chose to provide a more compact solution, where each task is assigned a unique resource and a uniquestart time feasible in every possible run time scenario. this keeps the problem NP-hard. This choice goes in the line of thenotion of strong controllability defined in [37] for temporal constraint networks with uncertainty; in particular, a networkis said to be strongly controllable if there exists a single control sequence satisfying the temporal constraints for everyscenario. In addition, for some classes of problems such as compilation of computer programs, this is the only kind ofsolution which can be actually implemented and executed [31]. More formally, we provide the following definition.Definition 9. The solution of the allocation and scheduling problem(cid:14)(cid:3)T , A, C, P (cid:4), Obj, Dur, Rel, Due, ResSet, ResCons(cid:15)is an assignment to each task ti ∈ T of a start time start(ti) ∈ [0..eoh] and of a resource res(ti) ∈ ResSeti such that(1) ∀ti ∈ T start(ti) (cid:2) Reli ;(2) ∀ti ∈ T start(ti) + Duri (cid:3) Duei ;(3) ∀(ti, t j) ∈ A start(ti) + Duri (cid:3) start(t j);(4) ∀t = 0 . . . eoh ∀s ∈ S ∀R ∈(cid:16)ti ∈TG(s) ResSeti :(cid:13)ti ∈TG(s):res(ti )=Rstart(ti )(cid:2)t<end(ti )ResConsi (cid:3) Cap(R)Constraints (1) and (2) ensure each task is executed within its release date and due date. Constraints (3) enforce prece-dence constraints, constraints (4) enforce resource capacity restrictions in all scenarios and at every time instant t on thetime line. A solution is optimal if E(Obj) is minimized (resp. maximized).For example, in Fig. 5A we show a small CTG scheduling problem and in Fig. 5B the corresponding solution. Note that alltasks have a unique start time and a unique resource assignment independent from the scenario, but feasible in all scenarios.For instance, tasks t1 and t2 are mutually exclusive, as they cannot appear in the same scenario. Therefore, although theyuse the same unary resource, their execution can overlap in time.5. Probabilistic reasoningThe model presented in Section 4 cannot be solved via traditional constraint-based scheduling techniques. In fact, thereare two aspects that require probabilistic reasoning and should be handled efficiently: the resource constraints to be en-forced in all scenarios and the computation of the expected value of the objective function (a weighted sum on all scenarios).In both cases, in principle, we should be able to enumerate all possible scenarios, whose number is exponential. Thus, weneed a more efficient way to cope with this expression.One contribution of this paper is the definition of a data structure, called Branch/Fork Graph (BFG), that compactly rep-resents all scenarios, and one parametric polynomial time algorithm working on the BFG that enables efficient probabilisticreasoning. For instance, we instantiate the parametric algorithm for the computation of the probability of a given set ofscenarios, such as the probability of all scenarios where a given set of tasks execute (resp. do not execute).M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529509Fig. 5. A: a CTG scheduling problem; B: a possible solution.Fig. 6. A: the CTG from Fig. 2; B: the associated BFG; C: probabilities of condition outcomes.5.1. Branch/Fork GraphA Branch/Fork Graph (BFG) intuitively represents the skeleton of all possible control flows and compactly encodes allscenarios of the corresponding CTG; for example Fig. 6B shows the BFG associated to the CTG from Fig. 2A (reported againin Fig. 6A for simplicity).A BFG is an acyclic directed graph. Nodes are either branch nodes (B-nodes, dots in Fig. 6B) or fork nodes (F nodes,circles in Fig. 6B). There is a branch node in the BFG for each branch node in the CTG. F-nodes instead represent sets ofevents and group CTG nodes executing at all such events. For example in Fig. 6B Fa groups together nodes t2, t3 and t4 asthey all execute in all scenarios where a = true.More formally:Definition 10. A Branch/Fork Graph is a directed, acyclic graph associated to a CTG = (cid:3)T = T B ∪ T F , A, C, P (cid:4) with two typesof nodes, referred to as B-nodes and F-nodes.BFG nodes satisfy the following conditions:(1) The graph has a B-node B i for every branch node t ∈ T B in the associated CTG.(2) Let S be the sample space of the CTG; then the BFG has an F-node F i for every subset of events σ ∈ 2S , unless:(a) (cid:2)ti ∈ T such that ∀s ∈ σ : ti ∈ TG(s).510M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529(b) ∃ck ∈ C such that (I) more than one outcome of ck appear in scenarios in σ and (II) not all the outcomes of ckappear in σ .(c) ∃σ (cid:11) ∈ 2S such that (a) and (b) are not satisfied (i.e. hence, an F-node would be built) and σ ⊂ σ (cid:11).The CTG branch node corresponding to a B-node B i is denoted as t(B i). The F-nodes are said to “represent” the set of eventsσ they correspond to, denoted as σ (F i). The set of CTG nodes ti such that ∀s ∈ σ (F i), ti ∈ TG(s) is said to be “mapped on”F i and is denoted as t(F i).2BFG arcs satisfy the following conditions:(1) An F-node F i is connected to a B-node B j by an arc (F i, B j) if t(B j) ∈ t(F i).(2) A B-node B i is connected by means of an arc labeled with the outcome Outt(Bi ),k to an F-node F j such that tk ∈ t(F j).(3) An F-node F i is connected to an F-node F j such that no path from F i to F j already exists and:(a) σ (F j) ⊂ σ (F i).(b) There exists no F-node Fk such that σ (F j) ⊂ σ (Fk) ⊂ σ (F i).Condition on BFG nodes (1) tells the BFG has one B-node for each branch in the associated CTG. Following condition (2),each F-node models a subsets of events σ ∈ 2S ; there is however no need to model a subset σ if any of the three conditions(2a)–(2c) holds. In particular:(2a) There is no need to model sets of events σ such that no task in the graph would be mapped to the resulting F-node;such sets of events are of no interest, as the ultimate purpose of the BFG is to support reasoning about task executionsand their probability.(2b) There is no need to model a set of events σ , if two or more outcomes of a condition ck appear in σ and still there issome outcome of ck not in σ . In fact if two or more (but not all) outcomes of ck are in σ , then σ still depends on ckand one could model this by using several F-nodes, each one referring to a single outcome. If however all outcomes ofck are in σ , then σ is independent on ck.(2c) Provided neither condition (2a) nor (2b) holds, there is still no need to build an F-node if there exist a larger set ofsuch that neither condition (2a) nor (2b) holds as well. In practice, due to condition (2c) F-nodes alwaysevents σ (cid:11)model maximal sets of events.For instance, according to the definition, the BFG corresponding to the CTG in Fig. 6A contains a branch node for eachbranch: B0 corresponds to t1 (i.e. t(B0) = t1), B1 to t6, B2 to t7, B3 to t12, B4 to t15. As for F-nodes, F 0 represents the wholesample space and nodes t0, t1, t12, t15 are mapped to it (i.e. t(F 0) = {t0, t1, t12, t15}), as they execute in all scenarios. F-nodeFb corresponds to the set of events {¬abcde, ¬abc¬de, ¬abcd¬e, ¬abc¬d¬e, ¬ab¬cde, ¬ab¬c¬de, ¬ab¬cd¬e, ¬ab¬c¬d¬e},that is the set of events where outcomes ¬a and b are both true; t8 is the only task in t(Fb) as it executes in all suchscenarios (condition (2a)) and does not execute in any superset of scenarios (condition (2c)).Concerning the BFG connectivity, condition (1) intuitively states that every B-node has an ingoing arc from all F-nodeswhere the corresponding CTG branch is mapped; in the BFG of Fig. 6B condition (1) yields all arcs from F-nodes to B-nodes.Condition (2) defines instead the connectivity from B-nodes to F-nodes: it tells that every B-node has an outgoing arc foreach outcome of the corresponding CTG branch; the destination of such BFG arc is the F-node where the destination ofthe arc with that outcome (task tk) is mapped to. The BFG arc is labeled with the corresponding CTG outcome. In Fig. 6Bcondition (2) yields all arcs from B-nodes to F-nodes.Finally condition (3) (that never happens in CTG satisfying the CFU) defines connectivity between F-nodes and other F-nodes linked by no path resulting from conditions (1) and (2). In particular, arcs (F i, F j) are built where F j is the destinationis the “minimal” (see condition (3b)) F-node such that σ (F j) ⊂ σ (F i) (see condition (2b)). Observe thatF-node, and F iparents of B-nodes are always F-nodes, parents of F-node can be both F-nodes and B-nodes; children of B-nodes are alwaysF-nodes, children of F-nodes can be both F-nodes and B-nodes. As an example consider Fig. 7 where we have links betweenF-nodes in the BFG corresponding to a CTG that does not satisfy CFU.Some properties follow from the BFG definition. First of all, given a CTG, its associated BFG is uniquely defined. The resultcomes from the fact that node condition (1) univocally defines the set of B-nodes, node condition (2c) univocally selects theset of scenarios to which every F-node corresponds to and the graph connectivity is univocally defined once F-nodes andB-nodes are given.The family of mappings of CTG nodes to F-nodes in general does not partition nodes in the CTG; Fig. 8 shows an examplegraph where a node (namely t3) is mapped to more than one F-node in the BFG (F ¬a, F ¬b). The following theorem holds:Theorem 1. If a CTG node ti is mapped to more than one F-node, such F-nodes are mutually exclusive, and represent pairwise disjointsets of scenarios.2 Note that t(B i ) is a node, while t(F i ) is a set.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529511Fig. 7. A: the non-CFU CTG from Fig. 3B; B: the associated BFG.Fig. 8. Task t3 is mapped on two F-nodes (F ¬a, F ¬b ).Proof. Suppose a ti is both mapped on F-nodes F j and Fh; this can be true only if no set of events σ (cid:11)exists such that (2a),(2b), (2c) are satisfied. From condition (2b), ti ∈ TG(s) ∀s ∈ σ (F j) ∪ σ (Fh), hence σ (cid:11) = σ (F j) ∪ σ (Fh) satisfies (2b), and ofcourse (2c). Hence σ (cid:11) = σ (F j) ∪ σ (Fh) must violate (2a), or ti would not be mapped to F j and Fh. Therefore, there must bea condition ck (let O be the set of outcomes) such that events in σ (F j) ∪ σ (Fh) do not contain the whole set of outcomesof ck and σ (F j), σ (Fh) contains exactly one outcome of ck. As different outcomes of the same condition generate mutuallyexclusive events, σ (F j) and σ (Fh) are mutually exclusive. (cid:2)Note also that in general F-nodes and B-nodes can have more than one parent (despite this is not the case for F-nodesin Figs. 6 and 8), as well as more than one child. In particular:Theorem 2. Parents of B-nodes are always mutually exclusive; parents of F-nodes are never mutually exclusive.Now, let F i be an F-node, with F-node parents FProof. Let B i be a B-node; due to connectivity condition (1), its parents are the F-nodes where the corresponding CTGbranch t(B i) is mapped; due to Theorem 1 those F-nodes are mutually exclusive., then σ (F i) ⊂ σ (F(cid:11)) (cid:3) σ (F(cid:11)(cid:11)), due to connectivityand F(cid:11)(cid:11)) (cid:17)= ∅. Thereforecondition (3). Note that the strict inclusion holds, hence σ (Fthe two parents are non-mutually exclusive, as they share some event. The reasoning still holds when one parent (or both)is a B-node B j , by substituting σ (F(cid:11)) and σ (F i) ⊂ σ (F(cid:11)) with {s ∈ σ (F(cid:11)) | t(B j) ∈ t(F(cid:11)(cid:11)) and σ (F(cid:11)(cid:11)) (cid:3) σ (F(cid:11)) ∩ σ (F(cid:11)(cid:11)), σ (F(cid:11))}. (cid:2)(cid:11)(cid:11)(cid:11)5.2. BFG and control flow uniquenessControl flow uniqueness translates into additional properties for the BFG:Theorem 3. If CFU holds, every F-node has a single parent and it is always a B-node.Proof. [Every F-node has a single parent]Suppose a node F i has two parents F(cid:11)(cid:11)(cid:11), F(see the proof of Theorem 2 for how to adapt the reasoning to B-nodes).512M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Fig. 9. (A) BFG for the graph in Fig. 6 – (B) the s-tree associated to the scenario ¬a¬b¬c¬de – (C) a subgraph (set of s-trees) associated to scenarios where¬ace holds.Let t j be a CTG and-node ∈ t(F i), and consider two incoming arcs aand t(cid:11), t j) and a(forare in t(F i) as well, or they are mapped to some ancestors of F i ; in this case theyCFU to hold); either the parents texecute in the events represented by any descendant of such ancestors (as a consequence of connectivity conditions), hencethey also execute on some parents of F i .(cid:11)(cid:11), t j) such that a(cid:11)(cid:11) = (t(cid:11) ⇒ a(cid:11) = (t(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)It is therefore always possible to identify a t j such that:(a) t j ∈ t(F i);(cid:11)(b) parents t(cid:11)(cid:11)and trespectively execute in all events in σ (F(cid:11)) and σ (F(cid:11)(cid:11)).Now, since a(cid:11)(cid:11)) (cid:3) σ (Fσ (F(cid:11) ⇒ a(cid:11)(cid:11), in terms of set of events this implies σ (F(cid:11)(cid:11)) ⊂ σ (F(cid:11)). However, due to Theorem 2, we know that and(cid:11)), which leads to a contradiction. Hence, if CFU holds, every F-node has a single parent. (cid:2)[The parent is a B-node]Suppose there exists an F-node F i with a single, F-node, parent Fis the only parent of F i and there is no(cid:11)) as well. At the same time, due to connectivity condition (3),(cid:11)); hence such a node F i , if existent, would fail to satisfy node condition (2c). Therefore, the single parent ofintermediate B-node, every node t j ∈ t(F i) executes in σ (Fσ (F i) ⊂ σ (Fevery F-node is a B-node. (cid:2). As F(cid:11)(cid:11)From Theorem 3 we deduce that if CFU holds the BFG is a bi-chromatic alternate graph. Moreover, since every branchnode with m outgoing arcs originates exactly m F-nodes, the BFG has exactly no + 1 F-nodes, where no is the number ofcondition outcomes; for this reason, when CFU is satisfied, one can denote F-nodes (other than the root) by the outcomethey refer to; for example an F-node referring to outcome a = true will be denoted as Fa, if referring to b = false as F ¬b andso on.CFU is also a necessary condition for the structural property listed above to hold; therefore we can check CFU by tryingto build a BFG with a single parent for each F-node: if we cannot make it, then the original graph does not satisfy thecondition. The BFG construction procedure in case CFU is satisfied is outlined in Appendix A.5.3. BFG and scenariosThe most interesting feature of a BFG is that it can be used to select and encode groups of scenarios in which arbitrarilychosen nodes execute. A specific algorithm can then be applied to such scenarios, in order to compute the correspondingprobability, or any other feature of interest.Groups of scenarios are encoded in the BFG as sets of s-trees:Definition 11 (s-tree). An s-tree is any subgraph of the BFG satisfying the following properties:(1) The subgraph includes the root node.(2) If the subgraph includes an F-node, it includes also all its children.(3) If the subgraph includes an F-node, it includes also all its parents.(4) If the subgraph includes a B-node, it includes also one and only one of its children.Note that the s-tree associated to a scenario s is the BFG associated to the deterministic task graph TG(s).Despite its name, an s-tree is not necessarily a tree: this is always the case only if CFU holds (which is not required byDefinition 11) (see Fig. 9A/B, where CFU holds and F-nodes are labeled with the condition outcome they refer to).M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529513By relaxing condition (4) in Definition 11 and allowing the inclusion of more than one condition per B-node, we geta subgraph representing a set of s-trees; a single s-tree (and hence a scenario) can be derived by choosing from thesubgraph a single outcome per branch condition. For example from the subgraph in Fig. 9C one can extract the set ofs-trees corresponding to ¬abcde, ¬abc¬de, ¬a¬bcde, ¬a¬bc¬de. This encoding method is sufficient to represent sets ofscenarios of practical interest (e.g. those required by the algorithm and constraints discussed in the paper). The importanceof s-trees mainly lies in the fact that they are required for the algorithm presented in the forthcoming Section 5.5.5.4. Querying the BFGWe now restrict our attention to CTG satisfying Control Flow Uniqueness; namely we want to provide a way to select aset of s-trees representing a set of scenarios which include or exclude a specified group of nodes; once such a subgraph isavailable, the execution probability can be extracted by a proper algorithm. We consider selection rules specified by meansof either conjunctions or disjunctions of positive and negative terms.3 Each basic term of the query can be either ti (withmeaning “task ti executes”) or ¬ti (with meaning “task ti does not execute”). Some examples of valid queries are:q0 = ti ∧ t jq1 = ti ∧ ¬t j ∧ ¬tkq2 = ti ∨ t jA query returns the BFG subgraph representing the events where the specified tasks execute/do not execute, or null in caseno such event exists. The idea of the query processing procedure is that, since the complete BFG represents all possiblescenarios, we can select a subset of them by removing F-nodes which do not satisfy the boolean query. Thus, in order to beprocessed, queries are first negated:¬q0 = ¬ti ∨ ¬t j¬q1 = ¬ti ∨ t j ∨ tk¬q2 = ¬ti ∧ ¬t jEach element in the negated disjunction now has to be mapped to a set of F-nodes to be removed from the BFG. This canbe efficiently done by pre-computing for each BFG node an inclusion label and an exclusion label:1. Inclusion labelsA CTG task ti is in the inclusion label i(F j) of an F-node F j either if it is directly mapped on it, ti ∈ t(F j), or if ti is in theinclusion label of any of its parents.A CTG task ti is in the inclusion label i(B j) of a B-node B j if ti is in the inclusion label of all of its parents.In practice, ti ∈ i(F j) (resp. i(B j)) if it does execute in all scenarios corresponding to every s-tree containing F j (resp. B j ).2. Exclusion labelsA CTG task ti is in the exclusion label e(F j) of an F-node F j either if parents of F j are F-nodes4 and ti is in the exclusionlabel of any parent, or if the parent of F j is a B-node and it exists a brother node Fk such that ti is mapped on a descendant(either direct or not) of Fk and ti is not mapped on a descendant (either direct or not) of F j .A CTG task ti is in the exclusion label e(B j) of a B-node B j if ti is in the exclusion label of all of its parents.In practice, ti ∈ e(F j) (resp. e(B j)) if it cannot execute in the scenario correspondent to any s-tree containing F j(resp. B j ).For example in Fig. 6B (reproduced in Fig. 10A for sake of clarity), the inclusion label of node F 0 is i(F 0) = {t0, t1, t12, t15}and i(B3) is equal to i(F 0); then i(Fd) = i(F 0) ∪ {t14} and i(F ¬d) = i(F 0) ∪ {t13}; i(B4) is again equal to i(F 0), since neithert13 nor t14 are mapped on both parents of B 4. As for the exclusion labels: e(F 0) = ∅ and e(B0) = ∅; e(F ¬a) = {t2, t3, t4},since those tasks are mapped on the brother node Fa and they are not mapped on any descendant of F ¬a.Once inclusion and exclusion labels are computed, each (conjunctive) term of the query (e.g. ti ∧ ¬t j ∧ . . .) is mapped toa set of F/B-nodes satisfying ti ∈ i(F j) (or i(B j)) for every positive element ti in the term, and ti ∈ e(F j) (or e(B j)) for eachnegative element ¬ti in the term. For example:(cid:18)B j | ti ∈ i(B j)(cid:17)∪(cid:18)F j | ti ∈ i(F j)(cid:17)(cid:17)(cid:17)ti →¬ti →ti ∧ tk →ti ∧ ¬tk →∪(cid:18)F j | ti ∈ e(F j)(cid:17)(cid:18)B j | ti ∈ e(B j)(cid:18)F j | ti, tk ∈ i(F j)(cid:17)(cid:18)F j | ti ∈ i(F j), tk ∈ e(F j)(cid:18)B j | ti, tk ∈ i(B j)(cid:17)∪∪(cid:17)(cid:18)B j | ti ∈ i(B j), tk ∈ e(B j)3 Mixed queries are also allowed by converting them to groups of conjunctive queries representing disjoint sets of scenarios, but paying an exponentialcomplexity blow-up, depending on the size and the structure of the query. Pure conjunctive and disjunctive queries are however enough for managingcases of practical interest as shown in the rest of the paper.4 This property holds for general CTG, while if CFU is satisfied parents of F-nodes are always B-nodes.514M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Fig. 10. (A) The BFG from Fig. 6B, (B) the subgraph from Fig. 9C.Note that an (originally) conjunctive query is mapped to a set of terms, each consisting of a single positive or negativetask literal; the query is processed by removing from the complete BFG the F and B-nodes corresponding to each term.Conversely, an (originally) disjunctive query yields a single term consisting of a conjunction of positive of negative task;the query is processed by removing from the BFG the F and B-nodes corresponding to the term. For example, on the graphof Fig. 6B (reproduced in Fig. 10A for sake of clarity), the query q = t21 ∧ ¬t3 ∧ ¬t16 = ¬(¬t21 ∨ t3 ∨ t16) is processed byremoving from the BFG F ¬c , Fa and F ¬e , since t21 ∈ e(F ¬c), t3 ∈ i(Fa) and t16 ∈ i(F ¬e). The resulting subgraph is the oneshown in Fig. 9C (reproduced in Fig. 10B).Disconnected nodes are removed at the end of the process. During query processing, one has to check whether at somestep any B-node loses all children; in such case the output is null, as the returned BFG subgraph would contain a B-nodewith no allowed outcome and this is impossible. Similarly, the result is null if all BFG nodes are removed. A query is alwaysprocessed in linear time. Finally, the following theorem holds:Theorem 4. If a query returns a BFG subgraph, this represents a set of s-trees.Proof. Assume the query result is not null and remember we consider CFU to be satisfied; then condition (1) in Defini-tion 11 is trivially satisfied, as a non-empty query always includes the root node. Conditions (2) and (3) are satisfied as, in agraph satisfying CFU, children and parents of F-nodes are always B-nodes, and B-nodes are never removed when processinga query. Finally, condition (4) is satisfied as query processing may remove some children of a B-node, but not all of them(or null would be returned). (cid:2)As the result of a query is always a set of s-trees, it can be used as input for the backward visit algorithm.5.5. Visiting the BFGMany algorithms along the paper are based on a backward visit of the BFG. During these visits each algorithm collectssome attributes stored in F- and B-nodes. We therefore propose a meta algorithm, using a set of parameters which haveto be defined case by case. All backward visit based algorithms assume CFU is satisfied and require as input a subgraphrepresenting a set of s-trees (hence a BFG as a particular case).In particular, Algorithm 1 shows the generic structure of a backward visit of a given BFG. The visit starts from the leavesand proceeds to the root; every predecessor node is visited when all its successors are visited (lines 12–13). The metaalgorithm is parametric in the five-tuple (cid:3) A, init F , initB , updateF , updateB(cid:4). In particular A is a set of possible attribute valuescharacterizing each F and B-node, and A(n) denotes the values of the attributes for node n; the function init F : {F i} → A: {F i} × {B j} → Aassociates to an F-node an element in A, that is values for each of its attributes, and the function updateFassociates to an F-node and a B-node an element in A. The functions initB : {B i} → A and updateB: {B i} × {F j} → A aredefined similarly to init F and updateF for B-nodes. In the algorithm, init F and initB are used to assign an initial value ofattributes in A to each F and B-node (line 2); the function updateF is used to update the attribute values of the parent ofan F-node (which is a B-node — line 6) and initB is used to update the attributes of the parent of a B-node (which is anF-node — line 8). In the following, we will use Algorithm 1 with different parameter settings for different purposes.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529515Algorithm 1: Backward visit ( A, init F , initB , updateF , updateB )A(np) = updateF (n, np)else if n is a B-node thenpick a node n ∈ Lif n is an F-node with parent np then1: let L be the set of nodes to visit and V the one of visited nodes. Initially L contains all subgraph leaves and V = ∅2: for each F and B-node, store the values of attributes in A. Initially set A(n) = init F (n) for all F nodes, A(n) = initB (n) for all B-nodes3: while L (cid:17)= ∅ do4:5:6:7:8:9:10:11:12:13:14:end for15: end whileend ifV = V ∪ {n}L = L \ {n}for every parent np doif all children of np are in V then L = L ∪ {np}for every parent np : A(np) = updateB (n, np)5.6. Computing subgraph probabilitiesIn the following we show how to compute the probability for a given BFG, or part of it (sets of s-trees derived fromquerying the BFG).The probability of a subgraph can be computed via a backward visit which is an instantiation of the meta Algorithm 1.In particular, a single attribute p, representing a probability, is stored in F and B-nodes, and thus A = [0, 1]. The result ofthe algorithm is the probability value of the root node. The init and update functions are as follows:init F (F i) = the probability of the outcome labeling the arc from the single B-node parent of F i and F i itselfinit B (B i) = 0updateF (F i, B j) = p(B j) + p(F i)updateB (B i, F j) = p(F j) · p(B i)As an example, consider the subgraph of Fig. 9C (also reported in Fig. 10B, together with the probabilities). The computationstarts from the leaves; for example at the beginning p(Fb) = 0.4, p(F ¬b) = 0.6, p(F c) = 0.6 (set by init F ). Then, proba-bilities of B-nodes are the weighted sum of those of their children (see update F ); for example p(b1) = p(Fb) + p(F ¬b) =0.4 + 0.6 = 1 and p(b2) = p(F c) = 0.6. Probabilities of F-nodes are instead the product of those of their children (seeupdateB ), and so p(F ¬a) = p(b1)p(b2) = 0.6. The visit proceeds backwards until p(F 0) is computed, which is also the prob-ability of the subgraph.6. Objective functionOne of the purposes of the probabilistic reasoning presented so far is to derive the expected value of a given objectivefunction efficiently. We consider in this section three examples of objective functions that are commonly used in constraintbased scheduling, described in Section 4.4.3: the minimization of costs of single task-resource assignments, the minimizationof the assignment cost of pairs of tasks, and the makespan. We refer to the first two examples as objective functions dependingon the resource allocation while we refer to the third case as objective function depending on the task schedule.This first and the second case are easier since we can transform the expected value of the objective function in a deter-ministic objective function provided that we are able to compute the probability a single task executes and the probabilitiesthat a pair of tasks executes respectively. The third example is much more complicated since there is not a declarative de-scription of the objective function that can be computed in polynomial time. Therefore, we provide an operational definitionof such expected value by defining an expected makespan constraint, and the corresponding filtering algorithm.6.1. Objective function depending on the resource allocationWe first consider an objective function depending on single tasks assignments and on the run time scenario; for example,suppose there is a fixed cost for the assignment of each task ti to a resource res(ti), as it is the case for objective (2) inSection 4.4.3. The general form of the objective function on a given scenario s isObj(s) =(cid:13)costti ∈TG(s)(cid:9)(cid:10)ti, res(ti)=(cid:13)fti (s)costti ∈T(cid:9)(cid:10)ti, res(ti)We remind that fti (s) = 0 if ti /∈ TG(s). According to Definition 8, the expected value of the objective function is(cid:9)(cid:10)ti, res(ti)p(s)Obj(s) =E(Obj) =p(s)(cid:13)(cid:13)(cid:13)fti (s)costs∈Ss∈Sti ∈T516M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529We remind that S i = {s | ti ∈ TG(s)} is the set of all scenarios where task i executes. Thus,(cid:20)(cid:19)E(Obj) =(cid:13)costti ∈T(cid:9)ti, res(ti)(cid:10) (cid:13)p(s)s∈S iNow every stochastic dependence is removed and the expected value is reduced to a deterministic expression. Note that(cid:2)p(s) is simply the probability of execution of node/task i. This probability can be efficiently computed by runningAlgorithm 1 instantiated as explained in Section 5.6, on the BFG sub-graph resulting from the query q = ti .s∈S iAs a second example, we suppose the objective function is related to arcs and to the run time scenario; again, we assumethere is a fixed cost for the activation of an arc, as it is the case for the objective (3) in Section 4.4.3. The general form ofthe objective function is(cid:13)(cid:9)(cid:10)res(ti), res(t j)Obj(s) =costak=(ti ,t j )∈TG(s)(cid:13)=fti (s) ft j (s)costak=(ti ,t j )∈ A(cid:9)(cid:10)ti, res(ti), t j, res(t j)The expected value of the objective function isE(Obj) =(cid:13)s∈Sp(s)(cid:13)fti (s) ft j (s)cost(cid:9)(cid:10)ti, res(ti), t j, res(t j)ak=(ti ,t j )∈ Anote that cost(ti, res(ti), t j, res(t j)) is a cost that we can derive from a cost matrix cE(Obj) =(cid:13)(cid:19)(cid:9)ti, res(ti), t j, res(t j)(cid:10) (cid:13)cost(cid:20)p(s)ak=(ti ,t j )∈ As∈S i ∩S j(cid:2)Now every stochastic dependence is removed and the expected value is reduced to a deterministic expression. Note thatp(s) is the probability that both tasks i and j execute. Again this probability can be efficiently computed usings∈S i ∩S jAlgorithm 1 on the BFG sub-graph resulting from query q = ti ∧ t j .6.2. Objective function depending on the task scheduleFor a deterministic task graph, the makespan is simply the end time of the last task; it can be expressed as: makespan =max{end(ti) | ti ∈ T }. If the task graph is conditional the last task depends on the occurring scenario. Remember we areinterested in finding a single assignment of start times, valid for each execution scenario; in this case each scenario sidentifies a deterministic Task Graph (TG(s)) and its makespan is max{end(ti) | ti ∈ TG(s)}. Thus, the most natural declarativeexpression for the expected makespan would be:(cid:13)E(makespan) =p(s) maxs∈S(cid:17)(cid:18)end(ti) | ti ∈ TG(s)(4)where p(s) is the probability of the scenario s. Note that the expression can be simplified by considering only tail tasks (i.e.+(ti)| = 0). For example, consider the CTG depicted in Fig. 11A, the scenarios are {a}, {¬a, b}, {¬a, ¬b},tasks such that | Aand the expected makespan can be expressed as:E(makespan) = p(a) max(cid:17)(cid:18)end(t2), end(t6)(cid:17)+ p(¬a ∧ b) max(cid:18)end(t4), end(t6)(cid:17)+ p(¬a ∧ ¬b) max(cid:18)end(t5), end(t6)Unluckily the number of scenarios is exponential in the number of branches, which limits the direct use of expression (4)to small, simple instances. Therefore, we defined an expected makespan global constraint(cid:22)end(t1), . . . , end(tn)exp_mkspan_cst, emkspan(cid:9)(cid:21)(cid:10)whose aim is to compute legal bounds on the expected makespan variable emkspan and on the end times of all tasks(end(ti)) in a procedural fashion. We devised a filtering algorithm described in Section 6.2.1 whose aim is to prune theexpected makespan variable on the basis of the task end variables, and vice-versa, see Section 6.2.2.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529517Fig. 11. Temporal task grouping.6.2.1. Filtering the expected makespan variableThe filtering algorithm is based on a simple idea: the computation of the expected makespan is tractable when theorder of tasks, and consequently of end variables, is known. Consider the schedule in Fig. 11B, where all tasks use a unaryresource URes0: since t5 is the last task, the makespan of all scenarios containing t5 is end(t5). Similarly, since t4 is the lastbut one task, end(t4) is the makespan value of all scenarios containing t4 and not containing t5, and so on.The computation can be done even if start times have not yet been assigned, as long as the end-order of tasks is known;in general, let t0, t1, . . . , tnt −1 be the sequence of CTG tasks ordered by increasing end time, then:E(makespan) =nt −1(cid:13)i=0p(ti ∧ ¬ti+1 ∧ · · · ∧ ¬tnt −1)end(ti)(5)The expected makespan can therefore be computed as a weighted sum of end times, where the weight of task ti is givenby the probability that 1) ti executes 2) none of the task ending later than ti executes. The sum contains nt terms, wherent is the number of tasks; again this number can be decreased by considering tail tasks only.Hence, once the end order of tasks is fixed, we can compute the expected makespan in polynomial time, we just needto be able to efficiently compute the probability weights in expression (5): if CFU holds, this can be done as explainedin Section 5 by running Algorithm 1 (in its probability computation version) on the BFG subgraph resulting from queryq = ti ∧ ¬ti+1 ∧ · · · ∧ ¬tnt −1.In general, however, during search the order of tasks is not fixed, but it is always possible to identify possibly infeasibletask schedules whose makespan, computed with expression (5), can be used as a bound for the expected makespan variable.We refer to these schedules as Smin and Smax, see Fig. 12. In particular, Smin is a schedule where all tasks are assumedto end at the minimum possible time and are therefore sorted by increasing min(end(ti)); conversely, in Smax tasks areassumed to end at the maximum possible time, hence they are ordered according to max(end(ti)). Obviously both situationswill likely be infeasible, but have to be taken into account. Moreover, the following theorem holds:Theorem 5. The expected makespan assumes the maximum possible value in the Smax schedule, the minimum possible value in theSmin schedule.Proof. Let us take into account Smax. Let t0, . . . , tn−1 be the respective task order; the corresponding expected makespanvalue due to expression (5) is a weighted sum of (maximum) end times:emkspan(Smax) = w 0 · max+ · · · + wn−1 · max(cid:9)(cid:10)end(t0)(cid:9)(cid:10)end(tn−1)(cid:2)i w i = 1 as weights are probability; also note weights are univocally defined by the task order. If Smax wereNote thatnot the expected maximum makespan schedule, it should be possible to increase the expected makespan value by reducingthe end time of some tasks. Now, let us gradually decrease max(end(ti)) while maintaining max(end(ti)) (cid:2) max(end(ti−1)):as long as w i does not change the expected makespan value necessarily decreases. When max(end(ti)) gets lower thanmax(end(ti−1)), weights w i and w i−1 change as follows:w i = p(ti ∧ ¬ti+1 ∧ · · · ∧ ¬tn−1) → p(ti ∧ ¬ti−1 ∧ ¬ti+1 ∧ · · · ∧ ¬tn−1)w i−1 = p(ti−1 ∧ ¬ti ∧ ¬ti+1 ∧ · · · ∧ ¬tn−1) → p(ti−1 ∧ ¬ti+1 ∧ · · · ∧ ¬tn−1)(cid:2)is constant and equal to 1 both before and after the swap,hence w i−1 gets higher, w i gets lower. As the sumw i−1 grows exactly by the amount by which w i shrinks; in other terms, some of the weight of ti is transferred to ti−1 or,equivalently, ti−1 “steals” some weight from ti . From now on, if we keep on decreasing max(end(ti)) the expected makespanwill still decrease, just at a slower pace due to the lower value w i , until w i will become 0. Hence by reducing the end timeof a single time variable the expected makespan can only get worse. Moving more tasks complicates the situation, but thereasoning still holds. An analogous proof can be done for the expected makespan for the S min schedule. (cid:2)i w i518M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Fig. 12. A: Example of S min and S max sequences. B: An update of end(t1) causes a swap in S max.Fig. 13. Upper bound on end variables.We can therefore prune the expected makespan variable by enforcing:emkspan(Smin) (cid:3) emkspan (cid:3) emkspan(Smax)(6)In order to improve computational efficiency, we can use F-nodes of the BFG instead of tasks in the computation ofemkspan(Smin) and emkspan(Smax), exploiting the mapping between tasks (CTG nodes) and F-nodes; for details see Ap-pendix B.Pruning the makespan variable requires to compute the makespan of the two schedules S min, Smax; this is done byperforming a BFG query (complexity O (nt )) and a probability computation (complexity O (nt )) for each task (O (nt) iter-ations). The basic worst case complexity is therefore O (n2t ), which can be reduced to O (nt log(nt )) by exploiting cachingand dynamic updates during search. As an intuition, all probability weights in the BFG can be computed at the root of thesearch tree and cached. Then, each time a variable end(ti) changes, possibly some nodes change their positions in Smin,Smax (see Fig. 12B, where max(end(t1)) changes and becomes lower than max(end(t3)), thus the two nodes are swapped);in such a situation, the probabilities of all the re-positioned nodes have to be updated. Each update is done in O (log(nt ))by modifying the probability weights on the BFG; as no more than nt nodes can move between a search node and any ofits children, the overall complexity is O (nt log(nt)).6.2.2. Filtering end time variablesWhen dealing with a makespan minimization problem, it is crucial for the efficiency of the search process to exploit themakespan variable domain updates (e.g. when a new bound is discovered) to filter the end variables domains.Bounds for end(ti) can be computed again with expression (5); for example to compute the upper bound for end(ti)we have to subtract from the maximum expected makespan value (max(mkspan)) the minimum contribution of all tasksexcept ti :max(emkspan) −end(ti) (cid:3)j(cid:17)=i p(t j ∧ ¬t j+1 ∧ . . .) min(end(t j))p(ti ∧ ¬ti+1 ∧ . . .)(cid:2)(7)where t0, . . . , ti−1, ti, ti+1, . . . is the sequence where the contribution of t j , j (cid:17)= i is minimized. Unfortunately, this sequenceis affected by the value of end(ti). In principle, we should compute a bound for all possible assignments of end(ti), whilekeeping the contribution of other nodes minimized.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529519Note that the sequence where the contribution of all tasks is minimized is that of the S min schedule; we can computea set of bounds for end(ti) by “sweeping” its position in the sequence, and repeatedly applying formula (7). An exampleis shown in Fig. 13, where a bound is computed for t0 (step 1 in Fig. 13). We start by computing a bound based on thecurrent position of t0 in the sequence (step 2 in Fig. 13); if such a bound is less than min(end(t1)), then max(end(t0)) ispruned, otherwise we swap t0 and t1 in the sequence and update the probabilities (the original probability w 0 becomesw 1) according to expression (5). The process continues by comparing t0 with t2 and so on until max(end(t0)) is pruned orthe end of Smin is reached. Lower bounds for min(end(ti)) can be computed similarly, by reasoning on Smax.A detailed description of the filtering procedure is given in Algorithm 2. The tasks are processed as they appear in S min(line 2); for each t j the algorithm starts to scan the next intervals (line 6). For each interval we compute a bound (lines 7 to11) based on the maximum makespan value (max(mkspan)), the current task probability/weight (wgt) and the contributionof all other tasks to the makespan lower bound (rest).If the end of the list is reached or the bound is within the interval (line 12) we prune the end variable of the currenttask (line 13) and the next task is processed. If the bound exceeds the current interval, we move to the next one. In thetransition the current task possibly gains weight by “stealing” it from the activity just crossed (lines 15 to 18); wgt and restare updated accordingly.Algorithm 2: End variables pruning (upper bound)1: let S min = t0, t1, . . . , tk−12: for j = 0 to k − 1 do3:4:5:6:7:compute result of query q = t j ∧ ¬t j+1 ∧ · · · ∧ tk−1 and probability p(q)wgt = p(q)rest = mkspan(S min) − min(end(t j ))w gtfor h = j to k − 1 doif wgt > 0 thenUB = max(mkspan) − restwgtelseUB = ∞end ifif h = (k − 1) or UB (cid:2) min(end(th+1)) thenset UB as upper bound for t jremove element ¬th+1 from query q and update p(q)newwgt = p(q)rest = rest − (newwgt − wgt) min(end(th+1))wgt = newwgt8:9:10:11:12:13:14:15:16:17:18:19:20:21: end forelseend ifend forThe algorithm takes into account all tasks (complexity O (nt)) and for each of them it analyzes the subsequent intervals(complexity O (nt)); weights are updated at each transition with complexity O (log(nt)) taking care of the fact that a taskcan be mapped to more than one F-node (note that directly working on F nodes avoids this issue). The overall complexityis O (n2o log(no)), where no is thenumber of condition outcomes in the CTG, see Appendix B.t log(nt )); by manipulating F-nodes instead of tasks it can be reduced down to O (nt + n27. Conditional constraintsTo tackle scheduling problems of conditional task graphs we introduced the so called conditional constraints, which extendtraditional constraints to take into account the feasibility in all scenarios.Let C be a constraint defined on a set of variables X , let S be the set of scenarios of a given CTG, let X(s) ⊆ X be the setof variables related to tasks appearing in the scenario s ∈ S. The conditional constraint corresponding to C must enforce:∀s ∈ S C| X(s)where C| X(s) denotes the restriction of constraint C to variables in X(s).A very simple example is the disjunctive conditional constraint [18] that models temporal relations between tasks ti andt j that need the same unary resource for execution. The disjunctive constraint enforces:(cid:9)(cid:9)(cid:10)end(ti) (cid:3) start(t j)∨(cid:10)end(t j) (cid:3) start(ti)mutex(ti, t j) ∨where mutex(ti, t j) holds if tasks ti and t j are mutually exclusive (see Definition 6) so that they can access the sameresource without competition.520M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529As another example, let us consider the cumulative constraint modeling limited capacity resources. The traditional re-Fig. 14. Capacity of a cumulative resource on a CTG.source constraint enforces for each time instant t:(cid:23)(cid:13)∀ time t, ∀R ∈ResSetititi :start(ti )(cid:2)t<end(ti )res(ti )=RResConsi (cid:3) Cap(R)while its conditional version enforces:∀ time t, ∀s ∈ S, ∀R ∈(cid:23)ResSetiti ∈T G(s)(cid:13)ti ∈TG(s):start(ti )(cid:2)t<end(ti )res(ti )=RResConsi (cid:3) Cap(R)where the same constraint as above must hold for every scenario; this indeed amounts to a relaxation of the deterministiccase.As a consequence, resource requirements of mutually exclusive tasks are not summed, since they never appear in thesame scenario. In principle, a conditional constraint can be implemented by checking the correspondent non-conditionalconstraint for each scenario; however, the number of scenarios in a CTG grows exponentially with the number of branchnodes and a case by case check is not affordable in practice. Therefore, implementing conditional constraints requires anefficient tool to reason on CTG scenarios; this is provided by the BFG framework, described in Section 5.1.We have defined and implemented the conditional version of the timetable constraint [23] for cumulative resourcesdescribed in the following section; other conditional constraints can be implemented by using the BFG framework andtaking inspiration from existing filtering algorithms.7.1. Timetable constraintA family of filtering algorithms for cumulative resource constraints is based on timetables, data structures storing theworst case usage profile of a resource over time [23]. While timetables for traditional resources are relatively simple andvery efficient, computing the worst usage profile in presence of alternative scenarios and mutually exclusive activities is nottrivial, since it varies in a non-linear way; furthermore, every activity has its own resource view.Suppose for instance we have the CTG in Fig. 14A; tasks t0, . . . , t4 and t6 have already been scheduled: their start timeand durations are reported in Fig. 14B; all tasks require a single cumulative resource of capacity 3, and the requirementsare reported next to each node in the graph. Tasks t5 and t7 have not been scheduled yet; t5 is present only in scenario¬a, where the resource usage profile is the first one reported in Fig. 14B; on the other hand, t7 is present only in scenarioa, b, where the usage profile is the latter in Fig. 14B. Therefore the resource view at a given time depends on the activitywe are considering. In case an activity is present in more than one scenario, the worst case at each time instant has to beconsidered.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529521We introduce a new global timetable constraint for cumulative resources and conditional tasks in the non-preemptivecase. The global constraint keeps a list of all known starting and ending points of activities (in particular their latest starttimes and earliest end times); given an activity ti , if lst(ti) (cid:3) eet(ti) then the activity has an obligatory part from lst(ti) toeet(ti) contributing to the resource profile.The filtering algorithm is described in Algorithm 3. All along the algorithm ti is the target activity, the variable “time”represents the time point currently under exam and “finish” is finish line value (when it is reached the filtering is over);finally “firstPStart” represents the first time point where ti can start and “good” is a flag whose value is false if the resourcecapacity is exceeded at the last examined time point.Algorithm 3 keeps on scanning meaningful end points of all obligatory parts in the interval [est(ti), finish) until (line 4):(1) The resource capacity is exceeded in the current time point (good = false) and the current time point has gone beyondthe latest start time of ti (in this case the constraint fails).(2) The resource capacity is not exceeded in the current time point (good = true) and the finish line has been reached(time (cid:2) finish).Next, the resource usage is checked at the current time point (line 5); in case the capacity is exceeded this is recorded(good = false at line 7) and the algorithm moves to the next end point of an obligatory part (eet(t j)) in the hope theresource will be freed by that time. In case the capacity is not exceeded: (A) the current time point becomes suitable forthe activity to start (line 10) and (B) the finish line is updated (line 11) to the current time value plus the duration of theactivity; then the algorithm keeps on checking the starting time of obligatory parts (see line 14). If the finish line is reachedwithout reporting a resource over-usage, then the start time of ti can be updated (line 18).Algorithm 3: Filtering algorithm for the conditional timetable constraintelseif good = false thenlet time = next eet(t j )let good = falseif ResConsi + resUsage(ti , time) > resCapacity then1: let time = est(ti ), finish = eet(ti )2: let firstPStart = time3: let good = true4: while ¬[(good = false ∧ time > lst(ti )) ∨ (good = true ∧ time >= finish)] do5:6:7:8:9:10:11:12:13:14:15:16: end while17: if good = true then18:19: else20:fail21: end iflet firstPStart = timelet finish = max(finish, time + Duri )let good = trueend iflet time = next lst(t j )let est(ti ) = firstPStartend ifAlgorithm 3 treats the computation of the resource usage as a black box: the resUsage(ti, time) denotes the worst caseusage at time time, as seen by task ti ; the worst case usage of a cumulative resource as seen by the current activity can becomputed efficiently via a backward visit, as described in Algorithm 1, on a BFG whose F-nodes are labeled with a weightvalue as follows.To compute the worst case usage of a resource at time t we first have to “load” the requirement of each task ti executingat time t (such that lst(ti) (cid:3) t (cid:3) eet(ti)) on each F-node F j such that ti belongs to the node inclusion label (ti ∈ i(F j)). Forthe computation of the maximum weight of a scenario each F and B-node has a single attribute w representing a weightvalue (in particular A = [0, ∞)). The init and update functions are defined as follows:init F (F i) =(cid:13)ResConsilst(t j )(cid:2)t(cid:2)eet(t j ),t j ∈i(F i )init B (B i) = 0(cid:9)updateF (F i, B j) = maxupdateB (B i, F j) = w(F j) + w(B i)(cid:10)w(B j), w(F i)At the end of the process the weight of the root node is the worst case resource usage.522M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Basically Algorithm 1, instantiated as described, performs a backward visit of the BFG, summing up the weight of eachchild for every F-node (see updateB ) and choosing for each B-node the maximum weight among those of its children(see updateF ). As each outcome has to be processed once, the complexity is O (no); loading a CTG task on an F-node hascomplexity O (no). The timetable filtering algorithm (Algorithm 3) in the worst case loads a CTG node and computes aweight for each task, hence nt times; therefore the complexity of the filtering algorithm for the time window of a singletask is O (nt(no + no)) = O (ntno). This value can be reduced by caching the results and updating the data structures when atime window (say of task ti ) is modified; this is done by updating data on F-nodes and propagating the change backwardalong the BFG; due to its tree like structure this is done in O (log(no)) and the overall complexity is reduced to O (nt log(no)).8. Related workThis paper is a substantially revised and extended version of two previous papers: in [25] we propose a similar frame-work for dealing with objective functions depending on task allocation in the field of embedded system design, while in[24] we face the makespan minimization problem. In the present paper, we recall some ideas of these previous papers, butin addition we describe conditional constraints, we formalize the overall stochastic framework and we perform extensiveevaluation.The area where CTG allocation and scheduling has received more attention is most probably the one of embedded systemdesign. In this context, Conditional Task Graphs represent a functional abstraction of embedded applications that should beoptimally mapped onto multi core architectures (Multi Processor Systems on Chip – MPSoCs). The optimal allocation andschedule guarantees high performances for the entire life time of the system. The problem has been faced mainly withincomplete approaches: in particular, [12] is one of the earliest works were CTGs are referred to as Conditional ProcessGraphs; there the focus is on minimizing the worst case completion time and a solution is provided by means of a branchoutcome dependent “schedule table”; a list scheduling based heuristic is provided and inter tasks communications aretaken into account as well. In [42] a genetic algorithm is devised on the basis of a conditional scheduling table whose(exponential number of) columns represent the combination of conditions in the CTG and whose rows are the starting timesof activities that appear in the scenario. The size of such a table can indeed be reasonable in real world applications. Anotherincomplete approach is described in [39] that proposes a heuristic algorithm for task allocation and scheduling based on thecomputation of mutual exclusion between tasks. Finally, [31] describes an incomplete algorithm for minimizing the energyconsumption based on task ordering and task stretching.To our knowledge, beside our previous work on CTG, the only complete approach to the CTG allocation and schedulingproblem is proposed in [18] and is based on Constraint Programming. The solving algorithm used only scales up to smalltask graphs (∼ 10 activities) and cumulative resources are not taken into account. Only a simple unary resource constraintis implemented in the paper. Also, the expected value of the objective function is not taken into account.Another complete CP based approach is described in [19] and targets low level instruction scheduling with Hierarchi-cal Conditional Dependency Graphs (HCDG); conditional dependencies are modeled in HCDGs by introducing special nodes(guards), to condition the execution of each operation; complexity blowup is avoided by providing a single schedule whereoperations with mutually exclusive guards are allowed to execute at the same time step even if they access the sameresource. We basically adopted the same approach to avoid scheduling each scenario independently. Mutual exclusions rela-tions are listed in HCDGSs for each pair of tasks and are computed off line by checking compatibility of guard expressions,whereas in CTGs they are deduced from the graph structure; note the in-search computation described in the paper is justused to support speculative execution. Pairwise listing of exclusion relations is a more general approach, but lacks somenice properties which are necessary to efficiently handle non-unary capacity resources; in particular computing worst caseusage of such a resource is a NP-complete problem if only pairwise mutual exclusions are known; in fact, in [19] only unaryresources are taken into account.An interesting framework where CTG allocation and scheduling can fit is the one presented in [9]. The framework isgeneral taking into account also other forms of stochastic variables (say for example task durations) and can integrate threegeneral families of techniques to cope with uncertainty: proactive techniques that use information about uncertainty togenerate and solve a problem model; revision techniques that change decisions when it is relevant during execution andprogressive techniques that solve the problem piece by piece on a gliding time horizon. Our paper is less general andmore focused on the efficient solution of a specific aspect of the framework, namely conditional branches and alternativeactivities.Conditional Task Graph may arise in the context of conditional planning [28]. Basically, in conditional planning wehave to check that each execution path (what we call scenario) is consistent with temporal constraints. For this purposeextensions to the traditional temporal constraint based reasoning have been proposed in [36] and [35]. However, theseapproaches do not take into account the presence of resources, which is conversely crucial in constraint based scheduling.Other graph structures similar to CTG have been considered in [20,7]. These graphs contain the so called optional activitiesbut their choice during execution is decided by the scheduler and is not based on the condition outcome. Basically, con-straint based scheduling techniques should be extended to cope with these graphs, but no probability reasoning is required.For graphs with optional activities, an efficient unary resource constraint filtering algorithm is proposed in [38].Close in spirit to optional activities are Temporal Networks with Alternatives (TNA), introduced in [5]. TNA augmentSimple Temporal Networks with alternative subgraphs, consisting of a “principal node” and several arcs to the same numberM. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529523of “branching nodes”, from which the user has to choose one for run-time execution. Like in optional activities, the user isresponsible for choosing a node; unlike in optional activities, exactly one branching node has to be selected. TNA do notallow early exits (as our approach does) and do not require any condition such as CFU. The follow-up work [6] proposesa heuristic algorithm to identify equivalent nodes in a TNA, similarly to what we do with the BFG. Note however that F-nodes do not necessarily represent equivalence classes (think of the fact that a CTG node can be mapped to more than oneF-node), but rather elementary groups of scenarios where a subset of tasks execute.Speaking more generally, stochastic problems have been widely investigated both in the Operations Research communityand in the Artificial Intelligence community.Operations Research has extensively studied stochastic optimization. The main approaches can be grouped under threecategories: sampling [2] consisting of approximating the expected value with its average value over a given sample; the l-shaped method [22] which faces stochastic problems with recourse, i.e. featuring a second stage of decision variables, whichcan be fixed once the stochastic variables become known. The method is based on Benders Decomposition [8]; the masterproblem is a deterministic problem for computing the first phase decision variables. The subproblem is a stochastic problemthat assigns the second stage decision variables minimizing the average value of the objective function. A third method isbased on branch and bound extended for dealing with stochastic variables [26].In the field of stochastic optimization an important role is played by stochastic scheduling. This field is motivated byproblems arising in systems where scarce resources must be allocated over time to activities with random features. Theaim of stochastic scheduling problems is to come up with a policy that, say, prioritize over time activities awaiting service.Mainly three methodologies have been developed:• Models for scheduling a batch of stochastic jobs, where the tasks to be scheduled are known but their processing timeis random, with a known distribution (see the seminal papers [32,29]).• Multi armed bandit models [14] that are concerned with the problem of optimally allocating effort over time to acollection of projects which change state in a random fashion.• Queuing scheduling control models [10] that are concerned with the design of optimal service discipline, where the setof activities to be executed is not known in advance but arrives in a random fashion with a known distribution.Temporal uncertainty has also been considered in the Artificial Intelligence community by extending Temporal ConstraintNetworks (TCSP) to allow contingent constraints [37] linking activities whose effective duration cannot be decided by thesystem but is provided by the external world. In these cases, the notion of consistency must be redefined in terms ofcontrollability; intuitively, a network is controllable if it is consistent in any situation (i.e. any assignment of the whole setof contingent intervals) that may arise in the external world. Three levels of controllability must be distinguished, namelythe strong, the weak and the dynamic one. We ensure in this paper strong controllability since we enforce consistency inall scenarios.The Constraint Programming community has recently faced stochastic problems: in [40] stochastic constraint program-ming is formally introduced and the concept of solution is replaced with the one of policy. In the same paper, two algorithmshave been proposed based on backtrack search. This work has been extended in [33] where an algorithm based on the con-cept of scenarios is proposed. In particular, the paper shows how to reduce the number of scenarios and still provide areasonable approximation of the value of optimal solution. We will compare our approach to the one reported in this paperboth in terms of efficiency and solution quality.9. Experimental resultsOur approach, referred to as conditional solver, has been implemented using the state of the art ILOG Cplex 11.0,Solver 6.3 and Scheduler 6.3. We tested the approach on a number of instances representing several variants of a realworld hardware design problem, where a multi task application (described by means of a CTG) has to be scheduled on amultiprocessor hardware platform. The problem features complex precedence relations (representing data communications),unary resources (the processors) and a single cumulative resource (modeling a shared communication channel whose ca-pacity is its total bandwidth).Instances for all problem variants are “realistic”, meaning that they are randomly generated on the base of real worldinstances [15]. We designed groups of experiments for two variants of the problem (described respectively in Sections 9.1and 9.2), to evaluate the conditional timetable constraint and objective functions presented in this paper and the perfor-mance of the BFG framework. Also, we compare our approach with a scenario based solver [33] that explicitly considers allscenarios or a subset of them.9.1. Bus traffic minimization problemIn the first problem variant hardware resources like processing elements and memory devices have to be allocated totasks in order to minimize the expected bus traffic. Once all resources are assigned, tasks have to be scheduled and aspecified global deadline must be met. The objective depends only on the allocation choices and counts two contributions:524M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Table 1Performance tests for the expected bus traffic minimization problem.tasksarcsactsscensproc10–1212–1414–1515–1818–1920–2121–2324–2525–2828–298–1210–1612–1713–2216–2216–2519–2820–2922–3023–3326–3632–4638–4941–6250–6352–7159–7964–8369–8874–951–63–92–92–184–303–246–244–365–728–482–22–22–33–33–34–44–44–55–55–5timemin0.020.040.030.110.310.291.270.730.192.49Table 2Performance tests for the expected bus traffic minimization problem.scensproc1–33–44–66–66–88–99–1212–1212–2424–722–32–42–52–52–42–52–53–53–53–5S100TT cond1.300.981.331.060.960.891.241.380.981.21>TLinf12201633233545656862S80TT cond1.311.220.851.311.151.001.110.960.961.04>TLinf12101644333545656862med0.070.120.220.431.781.681.273.252.3711.49ZZcond1.001.000.970.960.930.880.960.910.870.81max25.41610.9633.702.5687.52741.49641.74479.16382.3678.46S50TT cond0.850.571.040.831.200.480.790.970.590.89> TL0101223244>TLinf11000534333144646852inf31772411574ZZcond0.860.740.690.780.690.620.790.730.790.64one depending on single task-resource assignments, one depending on pairs of task-resource assignments. Basically, theobjective function captures both cases described in Section 6.1.We faced the problem by means of Logic Based Benders’ Decomposition [16] as explained in [25], where the masterproblem is the resource allocation and the subproblem is the computation of a feasible schedule.We implemented a conditional solver based on BFG and a scenario based one [33]. In the first case the stochastic ob-jective function in the master problem is reduced to a deterministic expression where scenario probabilities are computedas described in Section 6.1; in the scheduling subproblem unary resources (the processors) are modeled with conditionaldisjunctive constraints, while the communication channel is considered a cumulative resource and is modeled with a con-ditional timetable constraint.In the scenario based solver the objective function is a sum of an exponential number of linear terms, one for eachscenario. Processors are modeled again with conditional disjunctive constraints, while for the communication channel acollection of discrete resources (one per scenario) is used. A simple scenario reduction technique is implemented, so thatthe solver can be configured to take into account only a portion of the most likely scenarios.We generated 200 instances for this problem, ranging from 10 to 29 tasks, 8 to 33 arcs, which amounts to 26 to95 activities in the scheduling subproblem (tasks and arcs are split into several activities). All instances satisfy ControlFlow Uniqueness, and the number of scenario ranges from 1 to 72. The CTG generation process works by first building adeterministic Task Graph, and then randomly selecting some fork nodes to be turned into branches (provided CFU remainssatisfied); outcome probabilities are chosen randomly according to a uniform distribution. The number of processors in theplatform goes from 2 to 5. We ran experiments on a Pentium IV 2 GHz with 512 MB of RAM with a time limit of 900seconds.The first set of experiments, reported in Table 1, has the goal to test the performance of the conditional solver. Each rowrefers to a group of 20 instances and reports the minimum and maximum number of tasks, arcs, scheduling activities sce-narios and processors (columns: tasks, arcs, acts, scens and proc); minimum (column: min), median (med) and maximum(max) computation time for the instances solved to optimality, included the time to perform pre-processing and build theBFG. Then the number of instances not solved within the time limit follows (>TL) and the number of infeasible instances(inf). As one can see the median computation time is pretty low and grows with the size of the instance, while its maxi-mum has a more erratic behavior, influenced by the presence of uncommonly difficult instances. The number of timed-outinstances intuitively grows with the size of the graph.Then we compared the conditional solver we realized with a scenario based one for the same problem: the results forthis second group of tests are shown in Table 2. Each row reports results for a group of 20 instances, for which it shows theminimum and maximum number of scenarios (column scens), the minimum and maximum number of processors (procs),and some data about the scenario based solver when 100% (S100) 80% (S80) and 50% (S50) of the most likely scenarios areM. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529525Table 3Performance tests.acts37–4545–5050–5454–5757–6060–6565–6969–7676–8181–8686–9393–109scens1–21–31–32–41–61–62–83–121–203–242–364–135proc3–43–53–54–54–54–64–64–65–65–65–65–6T (C)1.542.679.0025.6829.7824.0332.1296.45144.67143.31165.74185.56F (C)3115494317 50552 94977 30228 51447 123101 800134 235130 561119 930127 321>TL00011021421172528CW0.830.880.880.880.940.850.900.860.900.840.930.93stc CW0.800.840.850.850.900.800.840.820.860.750.870.87TT condconsidered. In particular we report for each scenario based solver the average solution time ratio with the conditional solver, on the instances solved by both approaches), the number of timed out instances (>TL, not considered in the average(time computation) and the number of infeasible instances (inf). For the S50 and S80 solvers also the average solutionquality ratio is shown (). Note that in Table 2, instances are sorted by number of scenarios, rather than by size; as aconsequence, the first rows do not necessarily refer to the smallest nor the easiest scheduling problems.ZZcondOn average, the conditional solver improves the scenario based one by a 13% factor; this is not an impressive improve-ment. Also the improvement does not occur in all cases; the reason is that the computation time for this problem isdominated by that of finding an optimal resource allocation, and with regard to this subproblem the conditional approachonly offers a more efficient way to build the same objective function expression. We expect to have much better results asthe importance of the scheduling subproblem grows.Note how the use of scenario reduction techniques speeds up the solution process for S80 and S50, but introduces). Also, some infeasibleinaccuracies in the objective function value, which is lower than it would be (see columninstances are missed (the value of the “inf” column for S50 is lower than S100).ZZcond9.2. Makespan minimization problemIn the second problem variant we consider the minimization of the expected makespan, that is the expected applicationcompletion time. This is indeed much more complex than the previous case, since this objective function depends on thescheduling related variables. We therefore chose to limit ourselves to computing an optimal schedule for a given resourceallocation.As we did for the previous problem variant, we implemented both a conditional and a scenario based solver. In theconditional solver the makespan computation is handled by the global constraint described in Section 6.2, whereas in thescenario based solver the makespan is the sum of the completion time of each possible scenario, weighted by the scenarioprobability (see expression 4). Processors and bus constraints are modeled as described in Section 9.1.For this problem we generated 800 instances, ranging from 37 to 109 tasks, 2 to 5 “heads” (tasks with no predecessor),3 to 11 “tails” (tasks with no successor), 1 to 135 scenarios. The number of processors (unary resources) ranges from 3 to 6.Again all instances satisfy the control flow uniqueness. We ran experiments with a time limit of 300 seconds; all tests wereexecuted on a AMD Turion 64, 1.86 GHz.We performed a first group of tests to evaluate the efficiency of the expected makespan conditional constraint and thequality of the solutions provided (in particular the amount of gain which can be achieved by minimizing the expectedmakespan compared to worst case based approaches); a second group of experiment was then performed to compare theperformances of the conditional solver with the scenario-based one. Table 3 shows the results for the first group of tests;here we evaluate the performance of the solver using the conditional timetable constraint (referred to as C) and compare thequality of the computed schedules versus an identical model where the deterministic makespan is minimized (referred toas W). In this last case, no expected makespan constraint is used; the objective function is thus deterministic and amountsto minimizing the worst case makespan (hence the objective for the deterministic model will be necessarily worse thanthe conditional one). The models for C and W are identical with every other regard (they both use conditional resourceconstraints and assign a fixed start time to every task). Each row identifies a group of 50 instances. For each group wereport the minimum and maximum number of activities (acts), of scenarios (scens) and of unary resources (proc), theaverage solution time (T (C)), the average number of fails (F (C)) and the number of instances which could not be solvedwithin the time limit (>TL) by the conditional solver.In column C/W we report the makespan value ratio which shows an average improvement of 12% over the deterministicobjective. The gain is around 16% if we consider only the instances where the makespan is actually improved (column stcC/W). The computing time of the two approaches is surprisingly roughly equivalent for all instances.526M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Table 4Comparison with scenario based solver.scens1–22–33–44–55–66–66–88–99–1212–1414–2020–135T (C)41.0066.0243.8049.9466.3951.2638.8557.7852.96117.9395.74178.88>TL585695595171124T (S100)T (C)22.6019.8535.0573.7548.746.5282.2166.3289.5245.6032.6266.19>TL51089128111013222237T (S80)T (C)22.6619.9335.1273.6316.646.1171.0963.7086.9743.0231.8565.56>TL5108912891013222137S80C1.001.001.001.000.980.960.980.980.980.970.991.00T (S50)T (C)0.581.699.1957.030.7741.9984.4126.7640.4337.3528.7622.09>TL375888396181535S50C0.770.800.790.800.820.800.800.850.850.840.900.912Table 4 compares the conditional model with a scenario based solver; we remind that in this second case the cumulativeresource is implemented with one constraint per scenario and the expected makespan is expressed with the declarativeformula (4). In both models unary resources (processors) are implemented with conditional constraints.Again, rows of Table 4 report average results for groups of 50 instances; instances are grouped and sorted by increasingnumber of scenarios; hence, once again the results on the first row do not necessarily refer to the easiest/smallest instances.The table reports the solution time of the conditional solver (T (C)) and the performance ratios w.r.t. the scenario basedsolver with 100% (S100), 80% (S80) and 50% (S50) of the most likely scenarios. The four columns “>TL” show the number ofinstances not solved within the time limit for each approach. Finally, columns S50/C and S80/C show the accuracy of thesolution provided by S50 and S80 solvers.As it can be seen the conditional model now outperforms the scenario based one by an average factor of 49.08. Forthis problem, in fact, the conditional approach provides a completely different and more efficient representation of theobjective function, rather then just a more efficient procedure to build the same expression (as it was the case for thetraffic minimization).By reducing the number of considered scenarios the performance gap decreases; nevertheless, the conditional solverremains always better than S80; it is outperformed by S50 when the number of scenarios is low, but the solution providedhas an average 17% inaccuracy. Moreover, neither S50 nor S80 guarantee feasibility in all cases, since some scenarios are notconsidered at all in the solution.10. ConclusionCTG allocation and scheduling is a problem arising in many application areas that deserves a specific methodology for itsefficient solution. We propose to use a data structure, called Branch/Fork Graph, enabling efficient probabilistic reasoning.BFG and related algorithms can be used for extending traditional constraint to the conditional case and for the computationof the expected value of a given objective function.The experimental results show that the conditional solver is effective in practice, and that it outperforms a scenariobased solver for the same problem. The performance gap becomes significant when the makespan objective function isconsidered.Current research is devoted to taking into account other problems where the stochastic variables are task durationsor resource availability. Also, the application of CTG allocation and scheduling to the time prediction for business processmanagement is a subject of our current research activity.Appendix A. BFG construction procedure if CFU holdsThe BFG construction procedure has exponential time complexity in case the CTG satisfies CFU since the number ofpossible conjunctions in the activation events is exponential; in practice we can devise a polynomial time algorithm ifControl Flow Uniqueness holds. In this case we know the BFG contains an F-node for each condition outcome in the originalCTG, plus an F root node; therefore we can design an algorithm to build a BFG with an F-node for each condition outcome,and check at each step whether CFU actually holds; if a violation is encountered we return an error.In the following, we suppose that each CTG node ti is labeled with the set of condition outcomes in all paths from theroot node to ti (or “upstream conditions”); this can be easily done in polynomial time by means of a forward visit of thegraph.A polynomial time complexity BFG building procedure for graphs satisfying CFU is shown in Algorithm 4. The algorithmperforms a forward visit of the Conditional Task Graph starting from the root node; as the visit proceeds the BFG is builtand the CTG nodes are mapped to F-nodes. The acyclicity of the CTG ensures that whenever a CTG node ti is visited, allF-nodes needed to map it have already been built.M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529527Fig. 15 shows an example of the procedure, where the input is a subgraph of the CTG from Fig. 2A. Initially (Fig. 15B) theL set only contains the root task t5 and V = ∅. The CTG node t5 is visited (line 5) and mapped to the pre-built F-node F 0.The mapping_set function at line 6 returns the set of F-nodes in the current BFG on which t5 has to be mapped; detailson how to compute this set will be given later. In the next step (Fig. 15C) t6 is processed and mapped on F 0; however, beingt6 a branch, a new B-node is built (line 9) and an F-node for each condition outcome (Fb, F ¬b). In Fig. 15D t7 is visited and,similarly to t6, a new B-node and two new F-nodes are built. Next, t8 is visited and mapped on Fb (Fig. 15E); similarly t9 ismapped on F ¬b, t10 on F c and t11 on F ¬c (those steps are not shown in the figure). Finally, in Figs. 15F and 15G, nodes t20and t21 are visited; since the first is triggered by both the outcomes b and ¬b it is mapped directly on F 0; t21 is instead anand node, whose main predecessor is t10, therefore the mapping_set function maps it on the same F-node as t10.Algorithm 4: Building a BFG1: input: a Conditional Task Graph with all nodes labeled with the set of their upstream conditions2: build the root F-node F 03: let L be the set of nodes to visit and V the one of visited nodes. Initially L contains the CTG root and V = ∅4: while L (cid:17)= ∅ do5:pick the first node ti ∈ L, remove ti from Llet F (ti) = mapping_set(ti) be the set of F-nodes ti has to be mapped on6:7: map ti on all F-nodes in F (ti)8:if ti is a branch thenbuild a B-node B ibuild an F-node F Out for each condition outcome Out of the branchconnect each F-node in F (ti) to B iend ifadd ti to Vfor all child node t j of ti doif all parent nodes of t j are in V , add t j to L9:10:11:12:13:14:15:end for16:17: end whileThe set F (ti) for each CTG node is computed by means of a two-phase procedure. In first place an extended set ofF-nodes is derived by a forward visit of the BFG built so far. The visit starts from the root F-node. At each step: (A) if anF-node is visited, then all its children are also visited; (B) if a B-node is visited, then each child node is visited only if thecorresponding condition outcome appears in the label of ti (which reports the “upstream outcomes”). The CTG node ti isinitially mapped to all leaves reached by the visit; for example, with reference to Fig. 15G, CTG node t21 is first mapped toFb, F ¬b, F c .In the second phase this extended set of F-nodes is simplified by recursively applying two simplification rules; in order tomake the description clearer we temporarily allow CTG branch nodes to be mapped on B-nodes: B-node mappings, however,will be discarded at the end of the simplification process.rule 1:rule 2:if a B-node B i is the only parent of F-nodes F 0, F 1, . . . and a task t j is mapped on all of them, then add B i toF (t j). For example, with reference to Fig. 15G, where initially F (t21) = {Fb, F ¬b, F c}, after an application of rule 1we have F (t21) = {Bleft, Fb, F ¬b, F c}.if a task t j is mapped on a B-node B i with parent F 0, F 1, . . . , then B i and all its descendants can be removedfrom F (t j). If at the end of this process no descendant of F 0, F 1, etc. is in F (t j), then map t j on F 0, F 1, etc. Forexample, after the application of rule 2, F (t21) becomes {F c}.Once all simplifications are done, all remaining F-nodes in F (ti) must be mutually exclusive, as said in Section 5.1 andshown in Fig. 8: if this fails to occur it means the BFG has not enough F-nodes for the mapping, which in turn means theoriginal graph does not meet CFU. In this case an error is reported.Appendix B. Improving the efficiency of the expected makespan constraintIn order to improve the computational efficiency of all filtering algorithms used in the expected makespan constraint(see Section 6.2.1), we can use F-nodes instead of tasks in the computation of emkspan(S min) and emkspan(Smin). Rememberthat there is a mapping between tasks (CTG nodes) and F-nodes. Each F-node can therefore be assigned a minimum and amaximum end value computed as follows:(cid:10) (cid:24)(cid:18)(cid:24) ti ∈ t(F j)(cid:10) (cid:24)(cid:18)(cid:24) ti ∈ t(F j)(cid:17)maxend(F j) = max(cid:17)minend(F j) = maxmax(cid:9)minend(ti)end(ti)(cid:9)The rationale behind the formulas is that tasks mapped to an F-node F i all execute in events in σ (F i); therefore the endtime of the set of tasks will always be dominated by the one ending as last.528M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529Fig. 15. BFG building procedure.The two schedules Smin, Smax can store F-nodes (sorted by minend and maxend) instead of activities and their size canbe reduced to at most no + 1 (where no is the number of condition outcomes, often no (cid:23) nt ): this is in fact the number ofF-nodes in a BFG if CFU holds (see Section 4.2).Each time the end variable of a task ti mapped to F j changes, values maxend(F j) and minend(F j) are updated andpossibly some nodes are swapped in Smin, Smax (similarly to what Fig. 12B shows for tasks). These updates can be donewith complexity O (max(nt , no)), where nt is the number of tasks. The makespan bound calculation of constraints (6) canbe done by substituting tasks with F-nodes in expression (5), as shown in expression 8:E(makespan) =p(F i ∧ ¬F i+1 ∧ · · · ∧ ¬Fno−1) end(F i)(8)iwhere end(F i) ∈ [minend(F i), max end(F i)] and probabilities p(F i ∧ ¬F i+1 ∧ · · · ∧ ¬Fno−1) can be computed by querying theBFG with q = F i ∧ ¬F i+1 ∧ · · · ∧ ¬Fno−1. BFG queries involving F-nodes can be processed similarly to usual queries; basically,whilst each task is mapped to one or more F-nodes, an F-node is always mapped to a single F-node (i.e. itself); thus, (a) theinclusion and exclusion labels can be computed as usual and (b) every update of the weight or the time window of anF-node is performed in strictly logarithmic time. The same algorithms devised for tasks can be used to prune the makespanand the end variables, but the overall complexity goes down to O (max(nt , n2o log(no))).References[1] W.M.P. van der Aalst, M.H. Schonenberg, M. Song, Time prediction based on process mining, BPM Center Report BPM-09-04, BPMcenter.org, 2009,http://is.tm.tue.nl/staff/wvdaalst/BPMcenter/reports.htm.[2] S. Ahmed, A. Shapiro, The sample average approximation method for stochastic programs with integer recourse, in: Optimization on Line, 2002.[3] P. Baptiste, C. Le Pape, Constraint propagation and decomposition techniques for highly disjunctive and highly cumulative project scheduling problems,Constraints 5 (1/2) (2000) 119–139.[4] P. Baptiste, C. Le Pape, W. Nuijten, Constraint-Based Scheduling, Kluwer Academic Publisher, 2003.[5] R. Bartak, Ondrej Cepek, Temporal networks with alternatives: Complexity and model, in: FLAIRS 2007, 2007, pp. 641–646.[6] R. Bartak, O. Cepek, P. Surynek, Modelling alternatives in temporal networks, in: IEEE Symposium on Computational Intelligence in Scheduling, SCIS07,2007, pp. 129–136.(cid:13)M. Lombardi, M. Milano / Artificial Intelligence 174 (2010) 500–529529[7] J.C. Beck, M.S. Fox, Constraint-directed techniques for scheduling alternative activities, Journal of Artificial Intelligence (AI) 121 (1/2) (2000) 211–250.[8] J.F. Benders, Partitioning procedures for solving mixed-variables programming problems, Numerische Mathematik 4 (1962) 238–252.[9] J. Bidot, T. Vidal, P. Laborie, J.C. Beck, A general framework for scheduling in a stochastic environment, in: Proc. of the Intl. Joint Conference on ArtificialIntelligence, IJCAI 2007, 2007, pp. 56–61.[10] D.R. Cox, W.L. Smith, Queues, Chapman & Hall, 1961.[11] R. Dechter, Constraint Processing, Morgan Kaufmann, 2003.[12] P. Eles, Z. Peng, Bus access optimization for distributed embedded systems based on schedulability analysis, in: International Conference on Designand Automation in Europe, DATE2000, IEEE Computer Sociery, 2000.[13] P. Faraboschi, J.A. Fisher, C. Young, Instruction scheduling for instruction level parallel processors, Proceedings of the IEEE 89 (11) (2001) 1638–1659.[14] J.C. Gittins, D.M. Jones, A dynamic allocation index for the sequential design of experiments, in: Progress of Statistics, Colloq. Math. Soc. Janos Bolyai 9(1974) 241–255.[15] A. Guerri, M. Lombardi, M. Milano, Challenging scheduling problem in the field of system design, in: Workshop on Scheduling a Scheduling Competi-tion, ICAPS 2007, 2007, available at http://www.lia.deis.unibo.it/Staff/MicheleLombardi/, in the “Task Graph Generator” section.[16] J.N. Hooker, G. Ottosson, Logic-based benders decomposition, Mathematical Programming 96 (2003) 33–60.[17] K. Kennedy, R. Allen, Optimizing Compilers for Modern Architectures: A Dependence-based Approach, Morgan Kaufmann, 2001.[18] K. Kuchcinski, Constraints-driven scheduling and resource assignment, ACM Transactions on Design Automation of Electronic Systems 8 (2003).[19] K. Kuchcinski, C. Wolinski, Global approach to assignment and scheduling of complex behaviors based on HCDG and constraint programming, Journalof Systems Architecture 49 (12–15) (2003) 489–503.[20] J. Kuster, D. Jannach, G. Friedrich, Handling alternative activities in resource constrained project scheduling problems, in: Proc. of the Intl. Joint Confer-ence on Artificial Intelligence, 2007, pp. 1960–1965.[21] P. Laborie, Algorithms for propagating resource constraints in AI planning and scheduling: Existing approaches and new results, Journal of ArtificialIntelligence 143 (2003) 151–188.[22] G. Laporte, F.V. Louveaux, The integer l-shaped method for stochastic integer programs with complete recourse, Operations Research Letters 13 (1993).[23] C. Le Pape, Implementation of resource constraints in ILOG SCHEDULE: A library for the development of constraint-based scheduling systems, IntelligentSystems Engineering 3 (2) (1994) 55–66.[24] M. Lombardi, M. Milano, Scheduling conditional task graphs, in: Proc. of CP 2007, 2007, pp. 468–482.[25] M. Lombardi, M. Milano, Stochastic allocation and scheduling for conditional task graphs in MPSoCs, in: Proc. of CP 2006, 2006, pp. 299–313.[26] V.I. Norkin, G. Pflug, A. Ruszczynski, A branch and bound method for stochastic global optimization, Mathematical Programming 83 (1998).[27] Organization for the Advancement of Structured Information Standards (OASIS), Web Services Business Process Execution Language Version 2.0, OASISStandard, 2007, http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.html.[28] M.A. Peot, D.E. Smith, Conditional nonlinear planning, in: Proc. of Intl. Conference on AI Planning and Scheduling, 1992, pp. 189–197.[29] M.H. RothKopf, Scheduling with random service times, Management Science 12 (1966) 707–713.[30] N. Russell, W.M.P. van der Aalst, A.H.M. ter Hofstede, D. Edmond, Workflow data patterns: Identification, representation and tool support, in: Proc. ofthe 17th Intl. Conference on Advanced Information Systems Engineering, 2005, pp. 216–232.[31] D. Shin, J. Kim, Power-aware scheduling of conditional task graphs in real-time multiprocessor systems, in: International Symposium on Low PowerElectronics and Design (ISLPED), ACM, 2003, pp. 408–413.[32] W.E. Smith, Various optimizer for single stage production, Naval Research Logistic Quarterly 3 (1956) 59–66.[33] A. Tarim, S. Manandhar, T. Walsh, Stochastic constraint programming: A scenario-based approach, Constraints 11 (2006) 53–80.[34] A.H.M. Ter Hofstede, M. Weske, Business process management: A survey, in: Proc. of the 1st International Conference on Business Process Management,in: LNCS, vol. 2678, 2003, pp. 1–12.[35] I. Tsamardinos, M.E. Pollack, J.F. Horty, Merging plans with quantitative temporal constraints, temporally extended actions, and conditional branches,in: Proc. of the 5th International Conference on AI Planning Systems, 2000, pp. 264–272.[36] I. Tsamardinos, T. Vidal, M.E. Pollack, CTP: A new constraint-based formalism for conditional temporal planning, Constraints 8 (4) (2003) 365–388.[37] T. Vidal, H. Fargier, Handling contingencies in temporal constraint network: from consistency to controllability, Journal of Experimental and TheoreticalArtificial Intelligence 11 (1999) 23–45.[38] P. Vilim, R. Bartak, O. Cepek, Extension of o(n. log n) filtering algorithms for the unary resource constraint to optional activities, Constraints 10 (2005)403–425.[39] Y. Xie, W. Wolf, Allocation and scheduling of conditional task graph in hardware/software co-synthesis, in: Proc. of Design, Automation and Test inEurope, DATE2001, 2001, pp. 620–625.[40] T. Walsh, Stochastic constraint programming, in: Proc. of the European Conference on Artificial Intelligence, ECAI, 2002.[41] M. Weske, Business Process Management: Concepts, Languages, Architectures, Springer, Berlin, 2007.[42] D. Wu, B. Al-Hashimi, P. Eles, Scheduling and mapping of conditional task graph for the synthesis of low power embedded systems, Computers andDigital Techniques, IEEE Proceedings 150 (5) (2003) 262–273.