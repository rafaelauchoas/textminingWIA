Artificial Intelligence 173 (2009) 1539–1558Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintUncertainty modelling for vague concepts: A prototype theory approachJonathan Lawry a,∗, Yongchuan Tang ba Department of Engineering Mathematics, University of Bristol, Bristol BS8 1TR, UKb College of Computer Science, Zhejiang University, Hangzhou 310027, PR Chinaa r t i c l ei n f oa b s t r a c tArticle history:Received 26 April 2008Received in revised form 28 July 2009Accepted 29 July 2009Available online 5 August 2009Keywords:Epistemic vaguenessPrototype theoryLabel semanticsRandom sets1. IntroductionAn epistemic model of the uncertainty associated with vague concepts is introduced.Label semantics theory is proposed as a framework for quantifying an agent’s uncertaintyconcerning what labels are appropriate to describe a given example. An interpretationof label semantics is then proposed which incorporates prototype theory by introducinguncertain thresholds on the distance between elements and prototypes for descriptionlabels. This interpretation naturally generates a functional calculus for appropriatenessmeasures. A more general model with distinct threshold variables for different labels isdiscussed and we show how different kinds of semantic dependence can be captured inthis model.© 2009 Elsevier B.V. All rights reserved.Natural language is a powerful, flexible and robust mechanism for communicating ideas, concepts and information. Yetthe meaning conveyed by even simple words is often inherently uncertain. This uncertainty is reflected in the variation andinconsistency in the use of words by different individuals. For example, Parikh [30] reports an experiment where a sampleof people are shown a chart with different coloured squares and asked to count the number of red and the number ofblue squares. The results differ significantly across the group. Similar inconsistencies in the use of colour categories are alsodescribed in the work of Belin and Kay [1] and Kintz et al. [20]. We believe that this uncertainty about the appropriate useof words arises as a natural consequence of the distributed and case-based manner by which an understanding of languageis acquired.Language is, to a large degree, learnt through the experience of our interactions with other speakers from which wecan make inferences about the implicit rules and conventions of language use [29]. Exposure to formal grammar rules andexplicit dictionary definitions comes relatively late in our education and requires a priori a basic vocabulary on the part ofthe student. It is perhaps not surprising then that such a process results in significant semantic uncertainty. We cannot real-istically expect that the boundaries of linguistic concepts, as perhaps represented by their extensions in a multi-dimensionalconceptual space [11], should be precisely and unambiguously defined by a finite set of often conflicting examples. It isour view then, that the uncertainty about word meanings which naturally result from such an empirical learning process isthe underlying source of concept vagueness. Consequently we adopt an epistemic perspective on vagueness, to some extentin accordance with the views of Williamson [37], whereby crisp concept boundaries are assumed to exist but where theirprecise definition is uncertain. Furthermore, as pointed out by Parikh [29,30], empirical learning requires extrapolation frompreviously encountered examples of word use to other new but similar cases. Hence, the notion of similarity is also funda-mental to any model of vagueness. Prototype theory [32,33] provides a powerful tool to understand the role of typicalityin concept definitions, resulting in a natural ordering on possible exemplars of concepts e.g. Bill is taller than Mary, but* Corresponding author.E-mail addresses: j.lawry@bris.ac.uk (J. Lawry), tyongchuan@gmail.com (Y. Tang).0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.07.0061540J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558Mary is richer than Bill. In this paper we attempt to provide a formal framework for representing the epistemic uncertaintyassociated with vague concepts, which incorporates elements of prototype theory.The modelling of concept vagueness in Artificial Intelligence has been dominated by ideas from fuzzy set theory asoriginally proposed by Zadeh [38]. In that approach the extension of a concept is represented by a fuzzy set which has agraded characteristic or membership function with values ranging between 0 and 1. This allows for intermediate membership(values in (0, 1)) in vague concepts resulting in intermediate truth values for propositions involving vague concepts (fuzzylogic). The calculus for fuzzy set theory is truth-functional which means that the full complement of Boolean laws cannot allbe satisfied [4].1 Furthermore, fuzzy set theory and fuzzy logic do not in their narrowest manifestations adopt an epistemicview of vagueness. Hájek [14], for example, argues that membership values of fuzzy categories are primitives quantifyinggradedness of membership according to which it is meaningless to refer to unknown or uncertain crisp boundaries of vagueconcepts, since such boundaries are inherently fuzzy. On the other hand, many of the proposed interpretations of fuzzy setsimplicitly adopt an epistemic position. In particular, the random set model of fuzzy sets (see [12,13] and [27]) according towhich fuzzy set membership functions correspond to single point coverage functions of a random set, inherently assumesthe existence of an uncertain but crisp set representing the extension of a vague concept. The basis of the label semanticstheory [21] outlined in this paper is also a random set model of vagueness but where the intention is to quantify uncertaintyconcerning the applicability or appropriateness of labels to describe a given example. Such a theory cannot result in a truth-functional calculus but can be functional in a weaker sense in the presence of certain assumptions concerning the semanticdependence between labels.A principal motivation for this paper is to explore the relationship between prototype theory and label semantics. Hence,there will be a focus on mathematical results demonstrating a clear link between these two theories in the case whencategorization (labeling) involves thresholding of a measure of similarity to prototypes. Furthermore, we will argue froman Artificial Intelligence perspective, that the proposed framework could be a suitable model for rational intelligent agentswho use concept labels and label expressions to describe elements of their environment with the aim of communicatinginformation to their fellow agents.An outline of the paper is as follows: Section 2 describes a variant of the epistemic theory of vagueness which providesthe philosophical underpinnings for the formal models we propose [23]. Section 3 provides an overview of label semanticsas first proposed in [21] and [22]. Section 4 discusses the relationship between prototype theory, typicality, uncertainty andvagueness. Section 5 describes a new prototype theory interpretation of label semantics and finally Section 6 gives someconclusions and possible directions for future work.2. An epistemic theory of vaguenessIn our everyday use of language we are continually faced with decisions about the best way to describe objects andinstances in order to convey the information we intend. For example, suppose you are witness to a robbery, how shouldyou describe the robber so that police on patrol in the streets will have the best chance of spotting him? You will havecertain labels that can be applied, for example tall, short, medium, fat, thin, blonde, etc., some of which you may view asinappropriate for the robber, others perhaps you think are definitely appropriate while for some labels you are uncertainwhether they are appropriate or not. On the other hand, perhaps you have some ordered preferences between labels so thattall is more appropriate than medium which is in turn more appropriate than short. Your choice of words to describe therobber should surely then be based on these judgments about the appropriateness of labels. Yet where does this knowledgecome from and more fundamentally what does it actually mean to say that a label is or is not appropriate? In the sequelwe shall propose an interpretation of vague description labels based on a particular notion of appropriateness and suggesta measure of subjective uncertainty resulting from an agent’s partial knowledge about what labels are appropriate to assert.Furthermore, we will suggest that the vagueness of these description labels lies fundamentally in the uncertainty about ifand when they are appropriate as governed by the rules and conventions of language use.‘The robber is blonde’) or to agree to a classification (e.g.It seems undeniable that humans posses some kind of mechanism for deciding whether or not to make certain assertions(e.g.‘Yes he was tall’). Furthermore, although the underlyingconcepts are often vague the decisions about assertions are, at a certain level, bivalent. That is to say for a particularexample x and description θ , you are either willing to assert that ‘x is θ ’ or not. Of course in general this decision maydepend on many factors associated with the context in which the communication is taking place. For example, you arelikely to be much more cautious in your use of language when describing a robber to the police than in describing acolleague to a close friend. Also, your motives may be much more complex than purely to communicate information. Forexample, you may have recognized the robber as a family member so that your aim when describing him is to throw thepolice off the scent. Nonetheless, there seems to be an underlying assumption that some things can be correctly assertedwhile others cannot. Exactly where the dividing line lies between those labels that are and those that are not appropriate touse may be uncertain, but the assumption that such a division exists would be a natural precursor to any decision makingprocess of the kind just described.1 Except in the case where truth-values are restricted to {0, 1} when fuzzy set theory reduces to classical set theory.J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15581541The above argument brings us very close to the epistemic view of vagueness as expounded by Timothy Williamson [37].Williamson assumes that for the extensions of a vague concept there is a precise but unknown dividing boundary betweenit and the extension of the negation of that concept. For example, consider the set of heights which are classified as beingtall, then there is according to the epistemic view a precise but unknown height for which all values less than this heightare not tall while all those greater than it are tall. From this viewpoint Sorities problems are resolved by denying theassumption that practically indistinguishable elements satisfy the same vague predicates. Hence, for a finite sequence ofincreasing heights xi: i = 1, . . . , k where x1 is not tall, xk is tall and xi+1 − xi (cid:2) (cid:3), for some very small positive number (cid:3), itholds that: ∃i for which ‘xi is not tall’ and ‘xi+1 is tall’. Although the exact value of i will be virtually impossible for anyoneto identify precisely.2.1. The epistemic stanceWhile there are marked similarities between the epistemic view and that proposed in this paper, there are also someimportant differences. For instance, the epistemic view would seem to assume the existence of some objectively correct,but unknown, set of criteria for determining whether or not a given instance satisfies a vague concept. Instead of this weargue that individuals when faced with decision problems regarding assertions find it useful as part of a decision makingstrategy to assume that there is a clear dividing line between those labels which are and those which are not appropriateto describe a given instance. In other words, in deciding what to assert agents behave as if the epistemic view of vaguenessis correct. We refer to this strategic assumption across a population of communicating agents as the epistemic stance [22,23],a concise statement of which is as follows:Each individual agent in the population assumes the existence of a set of labeling conventions, valid across the whole population,governing what linguistic labels and expressions can be appropriately used to describe particular instances.In practice these rules and conventions underlying the appropriate use of labels would not be imposed by some outsideauthority. In fact, they may not exist at all in a formal sense. Rather they are represented as a distributed body of knowl-edge concerning the assertability of predicates in various cases, shared across a population of agents, and emerging as theresult of interactions and communications between individual agents all adopting the epistemic stance. The idea is that thelearning processes of individual agents, all sharing the fundamental aim of understanding how words can be appropriatelyused to communicate information, will eventually converge to some degree on a set of shared conventions. The very processof convergence then to some extent vindicates the epistemic stance from the perspective of individual agents. Of course,this is not to suggest complete or even extensive agreement between individuals as to these appropriateness conventions.However, the overlap between agents should be sufficient to ensure the effective transfer of useful information. Indeed,such effective communication does not require perfect agreement. For example, [30] illustrates how two individuals withdifferent notions of the colour blue can still effectively use the concept to pass information between them, by consideringhow the search space of one person looking for a book required by the other is reduced when they learn that the book isblue.In many respects our view is quite close to that of Rohit Parikh [29,30] where he argues for an anti-representational viewof vagueness, focusing on the notion of assertability rather than that of truth. Parikh argues that it is almost unavoidablethat different speakers will use the same predicate in different ways because of the manner in which language is learnt.Since vague predicates lack a clear definition we tend to learn the ‘usage of these words in some few cases and thenwe extrapolate’. With reference to Dewey and Wittgenstein, Parikh argues for a view of language where truth is relativelyunimportant, but where communication is best thought of in terms of a set of social practices. What is important then isnot whether a particular expression is true but whether it is assertible. To quote Parikh directly:“Certain sentences are assertible in the sense that we might ourselves assert them and other cases of sentences whichare non-assertible in the sense that we ourselves (and many others) would reproach someone who used them. But therewill also be the intermediate kind of sentences, where we might allow their use.”Hence, the epistemic stance requires that agents make decisions on what is or is not appropriate to assert, based on theirpast experience of language use and on the assumption that there are existing linguistic conventions that should be adheredto if they do not wish to be misunderstood or contradicted. This decision problem would naturally lead agents to considertheir subjective beliefs concerning the appropriateness of the available description labels in a given context. Uncertaintyabout such beliefs could then be quantified by using subjective probabilities, as proposed originally by de Finetti [3] andRamsey [31] for other types of epistemic uncertainty. In the next section we introduce label semantics as a subjectiveprobability based model of an agent’s uncertainty about the appropriateness of expressions to describe a given instance,consistent with them adopting the epistemic stance.To summarize, both the epistemic theory and the theory outlined in this paper identify vagueness as being a type ofignorance. For the epistemic theory this ignorance concerns the objective (but partially unknown) boundaries of concepts.While in our approach the focus is on an individual agent’s ignorance of the underlying linguistic conventions governing theuse of concept labels as part of communications between agents. In both cases, the association of vagueness with ignorance1542J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558strongly contrasts with the many-valued/fuzzy logic approach in which the applicability of concept labels is viewed as beinga matter of degree. Over recent years an extensive literature has emerged focusing on logical aspects of the fuzzy approach[14,28], including for example, embedding probability theory within a many-valued logic framework [9]. This work, however,is not within the scope of our paper.3. The label semantics frameworkLabel semantics proposes two fundamental and inter-related measures of the appropriateness of labels as descriptions ofan object or value. Given a finite set of labels LA a set of compound expressions LE can then be generated through recursiveapplications of logical connectives. The labels Li ∈ LA are intended to represent words such as adjectives and nouns whichcan be used to describe elements from the underlying universe Ω . In other words, Li correspond to description labels forwhich the expression ‘x is Li ’ is meaningful for any x ∈ Ω . This is exactly the type of expressions that Zadeh considers inhis work in linguistic variables [40–42]. For example, if Ω is the set of all possible rgb values then LA could consist of thebasic colour labels such as red, yellow, green, orange etc. In this case LE then contains those compound expression such asred & yellow, not blue nor orange etc. Note that in contrast to Kit Fine [8] in his discussion of penumbral connections wedo not make the a priori assumption that the labels LA are mutually exclusive and exhaustive. This potentially allows forcompound expressions such as red & orange to explicitly refer to boundaries between labels. The measure of appropriatenessof an expression θ ∈ LE as a description of instance x is denoted by μθ (x) and quantifies the agent’s subjective belief thatθ can be used to describe x based on his/her (partial) knowledge of the current labeling conventions of the population.From an alternative perspective, when faced with an object to describe, an agent may consider each label in LA and attemptto identify the subset of labels that are appropriate to use. Let this set be denoted by Dx. In the face of their uncertaintyregarding labeling conventions the agent will also be uncertain as to the composition of Dx, and in label semantics thisis quantified by a probability mass function mx : 2LA → [0, 1] on subsets of labels. The relationship between these twomeasures will be described below.Definition 1 (Label expressions). Given a finite set of labels LA the corresponding set of label expressions LE is definedrecursively as follows:• If L ∈ LA then L ∈ LE.• If θ, ϕ ∈ LE then ¬θ, θ ∧ ϕ, θ ∨ ϕ ∈ LE.The mass function mx on sets of labels then quantifies the agent’s belief that any particular subset of labels contains alland only the labels with which it is appropriate to describe x i.e. mx(F ) is the agent’s subjective probability that Dx = F .Definition 2 (Mass function on labels). ∀x ∈ Ω a mass function on labels is a function mx : 2LA → [0, 1] such that(cid:2)F ⊆L A mx(F ) = 1.The appropriateness measure, μθ (x), and the mass function mx are then related to each other on the basis that asserting‘x is θ ’ provides direct constraints on Dx. For example, asserting ‘x is L’ for L ∈ LA implies that L is appropriate to describex and hence that L ∈ Dx. Furthermore, asserting ‘x is L1 ∧ L2’, for labels L1, L2 ∈ LA is taken as conveying the informationthat both L1 and L2 are appropriate to describe x so that {L1, L2} ⊆ Dx. Similarly, ‘x is ¬L’ implies that L is not appropriateto describe x so L /∈ Dx. In general we can recursively define a mapping λ : LE → 22LAfrom expressions to sets of subsets oflabels, such that the assertion ‘x is θ ’ directly implies the constraint Dx ∈ λ(θ) and where λ(θ) is dependent on the logicalstructure of θ . For example, if LA = {low, medium, high} then λ(medium∧¬high) = {{low, medium}, {medium}} correspondingto those sets of labels which include medium but do not include high.Definition 3 (λ-mapping). λ : LE → 22LAis defined recursively as follows: ∀θ, ϕ ∈ LE• ∀Li ∈ LA λ(Li) = {F ⊆ LA: Li ∈ F }.• λ(θ ∧ ϕ) = λ(θ) ∩ λ(ϕ).• λ(θ ∨ ϕ) = λ(θ) ∪ λ(ϕ).• λ(¬θ) = λ(θ)c .Based on the λ mapping we then define μθ (x) as the sum of mx over those set of labels in λ(θ).Definition 4 (Appropriateness measure). The appropriateness measure defined by mass function mx is a function μ : LA × Ω →[0, 1] satisfying∀θ ∈ LE, ∀x ∈ Ω μθ (x) =(cid:3)mx(F )F ∈λ(θ )where μθ (x) is used as shorthand notation for μ(θ, x).J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15581543Note that in label semantics there is no requirement for the mass associated with the empty set to be zero. Instead, mx(∅)quantifies the agent’s belief that none of the labels are appropriate to describe x. We might observe that this phenomenaoccurs frequently in natural language, especially when labeling perceptions generated along some continuum. For example,we occasionally encounter colours for which none of our available colour descriptors seem appropriate. Hence, the valuemx(∅) is an indicator of the describability of x in terms of the labels LA.Semantic relations |(cid:13) meaning ‘more specific than’ and ≡ meaning ‘equivalent to’ can be defined on LE in the classicalmanner. Let Val be the set of valuation functions v : LA → {0, 1} where for Li ∈ LA, v(Li) = 1 means that Li is appropriatein the current context. In particular, the epistemic stance dictates that for each x ∈ Ω there would be a correspondingvaluation v x (partially unknown to the agent) determining which labels are appropriate to describe x. A valuation v ∈ Valnaturally determines an extension v : LE → {0, 1} defined recursively as follows: For θ, ϕ ∈ LE; v(θ ∨ ϕ) = max(v(θ), v(ϕ)),v(θ ∧ ϕ) = min(v(θ), v(ϕ)), and v(¬θ) = 1 − v(θ). We can now define |(cid:13) and ≡ as follows:Definition 5. ∀θ, ϕ ∈ LE• θ |(cid:13) ϕ if ∀v ∈ Val v(θ) = 1 ⇒ v(ϕ) = 1.• θ ≡ ϕ if ∀v ∈ Val v(θ) = v(ϕ).• θ is a tautology if ∀v ∈ Val v(θ) = 1.• θ is a contradiction if ∀v ∈ Val v(θ) = 0.Given Definitions 4 and 5 it can be shown that appropriateness measures have the following general properties [21,22]:Theorem 6 (General properties of appropriateness measures). ∀θ, ϕ ∈ LE the following properties hold:• If θ |(cid:13) ϕ then ∀x ∈ Ω μθ (x) (cid:2) μϕ(x).• If θ ≡ ϕ then ∀x ∈ Ω μθ (x) = μϕ(x).• If θ is a tautology then ∀x ∈ Ω μθ (x) = 1.• If θ is a contradiction then ∀x ∈ Ω μθ (x) = 0.• If θ ∧ ϕ is a contradiction then ∀x ∈ Ω μθ∨ϕ(x) = μθ (x) + μϕ(x).• ∀x ∈ Ω μ¬θ (x) = 1 − μθ (x).• For F ⊆ LA let θF = (¬Li) then mx(F ) = μθF (x).Li ∈F Li) ∧ (Li /∈F(cid:4)(cid:4)From Theorem 6 it can be seen that for a fixed x ∈ Ω , appropriateness measures correspond to probabilities on LE. Morespecifically, μθ (x) can be interpreted as the subjective conditional probability that θ is an appropriate expression given thatthe element x is being described [21,22]. This naturally links label semantics to the conditional probability interpretationof fuzzy sets proposed by Hisdal [19] and widely developed by Coletti and Scozzafava [2]. However, as we will see in thefollowing section, the mass function based definition of appropriateness measures enables us to explore links with randomset theory and to formulate certain consonance assumptions in a straightforward manner. The random set approach is alsowell suited to exploring the link between label semantics and prototype theory as shown in Section 5.3.1. Functionality of appropriateness measuresFrom Definition 4 we see that in order to be able to evaluate the appropriateness measure of any expression θ ∈ LE as adescription for x ∈ Ω we must potentially know the value of mx for all subsets of LA. Hence, we are, in principle, required|LA| − 1 values for mx. For large basic label sets this is clearly computationally infeasible. One solutionto specify of order 2to this problem would be to make additional assumptions about the definition of the mass assignment mx so that thereexists a functional relationship between the appropriateness measure for the basic labels (i.e. μL(x): L ∈ LA) and mx. Thiswould result in a functional calculus for appropriateness measures according to which the appropriateness of any compoundexpression could be determined directly from the appropriateness of the basic labels in the following sense:Definition 7 (Functional measures). A measure μ : LE × Ω → [0, 1] is said to be functional if ∀θ ∈ LE there exists a functionfθ : [0, 1]|LA| → [0, 1] such that ∀x ∈ Ω μθ (x) = fθ (μL(x): L ∈ LA).Now fuzzy logic is clearly functional in the sense of Definition 7 but it also satisfies the stronger property of truth-functionality. Truth-functionality defines the mapping fθ to be a recursive combination of functions representing eachconnective, as determined by the logical structure of the expression θ . More formally:Definition 8 (Truth-functional measures). A measure μ : LE × Ω → [0, 1] is said to truth-functional if there exists mappingsf¬ : [0, 1] → [0, 1], f∧ : [0, 1]2 → [0, 1] and f∨ : [0, 1]2 → [0, 1] and such that ∀θ, ϕ ∈ LE:• ∀x ∈ Ω μ¬θ (x) = f¬(μθ (x)).1544J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558• ∀x ∈ Ω μθ∧ϕ(x) = f∧(μθ (x), μϕ(x)).• ∀x ∈ Ω μθ∨ϕ(x) = f∨(μθ (x), μϕ(x)).Now from Theorem 6 it follows that appropriateness measures must satisfy the laws of excluded middle and idempo-tence. Hence, by the following theorem due to Dubois and Prade [4] they cannot be truth-functional except in the trivialcase where all appropriateness values are either 0 or 1.Theorem 9. (See Dubois and Prade [4].) If μ : LE × Ω → [0, 1] is a truth-functional measure and satisfies both idempotence and thelaw of excluded middle then ∀θ ∈ LE, ∀x ∈ Ω , μθ (x) ∈ {0, 1}.However, Theorem 9 does not apply to all functional measures, only those which are truth-functional, hence it maystill be possible to define a functional calculus for appropriateness measures consistent with both Definitions 4 and 7. Toinvestigate this possibility further we consider the relationship between appropriateness measures of compound expressionsand those of the basic labels, imposed by Definition 4.From Definition 3 the lambda mapping for a basic label Li ∈ LA is given by λ(Li) = {F ⊆ LA: Li ∈ F } and hence the massfunction mx must satisfy the following constraint imposed by the appropriateness measures for the basic labels μLi (x):Li ∈ LA:∀x ∈ Ω, ∀Li ∈ LA μLi (x) =(cid:3)mx(F )F ⊆LA: Li ∈FThis constraint, however, is not sufficient to identify a unique mass function mx given values for μLi (x): Li ∈ LA. Indeed,there are in general an infinite set of mass functions satisfying the above equation for a given set of basic label appropri-ateness values. Hence, in this context, the assumption of a functional calculus for appropriateness measures is equivalent tothe assumption of a selection function which identifies a unique mass function from this set [21,22].Definition 10 (Selection function). Let M be the set of all mass functions on 2LA. Then a selection function is a functionη : [0, 1]|L A| → M such that if ∀x ∈ Ωη(μLi (x) : Li ∈ LA) = mx then∀x ∈ Ω ∀Li ∈ LAmx(F ) = μLi (x)(cid:3)F ⊆LA: Li ∈FNow since the value of μθ (x) for any expression θ ∈ LE can be evaluated directly from mx, then given a selection functionη we have a functional method for determining μθ (x) from the basic label appropriateness values, where fθ in Definition 7is given by:(cid:5)fθμLi (x): Li ∈ LA(cid:6)=(cid:3)(cid:5)ημLi (x): Li ∈ LA(cid:6)(F )Two examples of selection functions are the consonant and the independent selection functions as defined below:F ∈λ(θ )Definition 11 (Consonant selection function). Given non-zero appropriateness measures on basic labels μLi (x): i = 1, . . . , nordered such that μLi (x) (cid:3) μLi+1 (x) for i = 1, . . . , n then the consonant selection function identifies the mass function,(cid:5)(cid:6)(cid:5)mx{L1, . . . , Ln}(cid:6){L1, . . . , Li}mxmx(∅) = 1 − μL1 (x)= μLn (x)= μLi (x) − μLi+1 (x)for i = 1, . . . , nDefinition 12 (Independent selection function). Given appropriateness measures on basic labels μLi (x): Li ∈ LA then the inde-pendent selection function identifies the mass function,(cid:7)(cid:7)∀F ⊆ LA mx(F ) =μLi (x) ×(cid:5)(cid:6)1 − μLi (x)Li ∈FLi /∈FThe consonant selection function corresponds to the assumption that for each x ∈ Ω an agent first identifies a totalordering on the appropriateness of labels. They then evaluate their belief values mx about which labels are appropriate todescribe x in such a way so as to be consistent with this ordering. More formally, let (cid:16)x denote the appropriateness orderingon LA for element x so that L1 (cid:16)x L2 means that L2 is at least as appropriate as L1 for describing x. When evaluating mx(F )for F ⊆ LA the agent then makes the assumption that the mass value is non-zero only if for every label Li ∈ F it also holdsthat L j ∈ F for every L j ∈ LA for which Li (cid:16)x L j .J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15581545The independent selection function simply assumes that when judging the appropriateness of a label an agent does nottake into account the level of appropriateness of any other label. Although this may seem difficult to justify, it could bereasonable in cases where labels relate to different facets of the object. For example, the appropriateness of the label thinmight well be assumed to be independent of the appropriateness of the label rich.The following theorems show that the consonant and independent selection functions result in familiar combinationoperators for restricted sets of expressions (see [21,22]).Theorem 13. (See [21,36].) Let LEIf ∀x ∈ Ω , mx is determined from μL(x): L ∈ LA according to the consonant selection function then ∀θ, ϕ ∈ LEthat:∧,∨ ⊆ LE denote those expressions generated recursively from LA using only the connectives ∧ and ∨., ∀x ∈ Ω it holds∧,∨(cid:5)μθ ∧ϕ(x) = min(cid:6)μθ (x), μϕ(x)(cid:5)and μθ ∨ϕ(x) = max(cid:6)μθ (x), μϕ(x)Theorem 14. (See [22].) If ∀x ∈ Ω , mx is determined from μL(x): L ∈ LA according to the independent selection function then forlabels L1, . . . , Ln ∈ LA we have that ∀x ∈ Ω :μL1∧L2∧···∧Ln (x) =μL1∨L2∨···∨Ln (x) =n(cid:7)i=1μLi (x)(cid:3)(−1)|S|−1∅(cid:17)=S⊆{L1,...,Ln}(cid:7)Li ∈SμLi (x)One interpretation of selection functions is that they provide a means of encoding the semantic dependence betweenlabels. From this perspective, the consonant selection function assumes that the appropriateness of all labels are assessed onthe basis of the same set of shared attributes i.e. that they can be represented within a single conceptual space [11]. Typicalexamples might be height labels such as tall, medium and short or colour labels. Alternatively, the independent selectionfunction assumes a set of labels where the appropriateness of each label is judged on the basis of a set of attributes inde-pendent from those employed to assess the appropriateness of any other label i.e. we have a set of independent conceptualspaces (one for each label) with no shared or dependent attributes. Intermediate cases between the consonant and inde-pendent selection functions can also be considered, perhaps resulting in the kind of partial orderings on appropriatenessproposed in [23]. We shall return to the discussion of semantic dependence in the sequel where we will consider the issuefrom a prototype theory perspective.3.2. Assertability decisionsFor any given x ∈ Ω and expression θ ∈ LE it remains unclear exactly how an agent would use their evaluation of theappropriateness measure μθ (x) in order to reach a decision as to whether or not the statement ‘x is θ ’ is assertible. Onepossibility would be to use a threshold based approach according to which a positive decision to assert ‘x is θ ’ wouldrequire that the appropriateness measure μθ (x) be sufficiently close to 1. More formally, an agent would be willing toassert ‘x is θ ’ at certainty level α (cid:3) 0.5, denoted Assertα(x is θ), if μθ (x) (cid:3) α. For a tautology θ ∨ ¬θ we would then haveAssertα(x is θ ∨ ¬θ) holding at any certainty level α. However, for α (cid:3) max(μθ (x), 1 − μθ (x)) neither ‘x is θ ’ nor ‘x is ¬θ ’is assertible at certainty level α i.e. Assertα(x is θ) ∨ Assertα(x is ¬θ) does not hold. This is consistent with the intuitionthat, for example, an agent would happily concede that any given colour is either red or not red even though for certainborderline cases they would be unwilling to use either the description red or the description not red. In general, for x ∈ Ωand expressions θ, ϕ ∈ LE Assertα(x is θ ∨ ϕ) is not equivalent to Assertα(x is θ) ∨ Assertα(x is ϕ) since the former requiresthat μθ∨ϕ(x) (cid:3) α while the latter requires that max(μθ (x), μϕ(x)) (cid:3) α. Indeed, if the consonant selection function is appliedit can be seen from Theorem 13 that these two assertability statements are only equivalent for expressions θ and ϕ whichdo not involve negation i.e. θ, ϕ ∈ LE∧,∨.Notice that although μθ (x) is an indication of the assertability of the statement ‘x is θ ’ it should not be interpreted as theprobability that this assertion will actually occur during communications aimed at describing x. Instead, μθ (x) quantifies thebelief that the expression θ is appropriate to describe x, where appropriateness is judged purely on the basis of the agent’sinterpretation of the meaning of θ . This understanding of meaning would in turn be based on an internal representation2of the labels in LA, as inferred from the agent’s past experience of communications involving these labels. This is not thesame as taking μθ (x) as corresponding to the probability of assertion ‘x is θ ’ actually occurring. For example, given a basiclabel Li ∈ LA then from a semantic perspective the expressions Li and ¬¬Li are equally appropriate to describe any givenx (with equal appropriateness measures). However, for those x with high appropriateness measures for these equivalentexpressions, the assertion ‘x is Li ’ is perhaps much more likely to be actually used than the assertion ‘x is ¬¬Li ’, forreasons of syntactic simplicity. Indeed, Lawry [24] proposes a model for evaluating the probability of assertions, based on2 In the sequel we shall suggest that this internal representation might be based on prototypes.1546J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558Fig. 1. NCS colour spindle [35]: A conceptual space for representing colour categories with dependent attributes Hue, Chromaticness and Intensity.appropriateness measures, which takes into account a prior weighting on expressions dependent, for example, on theirrelative syntactic complexity. This, however, is beyond the scope of the current paper.4. Prototype theory and vaguenessThe central tenet of Prototype theory (Rosch [32,33]) is that concepts, rather than being defined by formal rules or map-pings, are represented by prototypes and that categorization is based on similarity to these prototypes (see Hampton [15]for an overview). By taking typicality to be a decreasing function of distance from prototypes, this approach would naturallyexplain the fact that some instances are seen as being more typical exemplars of a concept than others. For example, robinsare more typical exemplars of birds than penguins, since the latter have certain atypical characteristics such as the inabilityto fly. This notion of typicality is also strongly related to concept vagueness where borderline cases have an intermediaterange of typicality values. In other words, such cases are not sufficiently similar to the concept prototypes to be judged ashaving certain membership in the category but are also not sufficiently dissimilar to the prototypes to be ruled as beingcertainly outside the category.Gärdenfors [11] has recently introduced the notion of conceptual spaces for concept representation, corresponding to ametric space of (possibly) dependent attributes or features of elements from the underlying universe Ω . For example, theNCS colour spindle [35] is a proposed conceptual space for colour categories based on polar attributes Hue and Chromaticnessand where Chromaticness is constrained by a third attribute, Intensity (see Fig. 1). Within a conceptual space properties arerepresented by convex regions. This provides a natural link to prototype theory since given a convex set of prototypes foreach property the space is partitioned into convex regions each defined as the set of points closest to the prototypes for agiven property. Such partitions are referred to as Voronoi tessellations.Similarity to prototypes has been widely suggested as a possible basis for membership functions in fuzzy logic. Duboisand Prade [6] and also Dubois et al. [7] identify such an interpretation as one of the three main semantics for membershipdegrees. For example, Ruspini [34] introduces a semantics for fuzzy reasoning where, given a measure of similarity betweenelements with a range between 0 (totally dissimilar) and 1 (totally similar), the membership degree of an element x in aconcept corresponds to the supremum of the similarity values between x and the prototypes for the concept. Hampton [16]proposed a thresholding model for categorization whereby an instance is positively classified as belonging to a category ifits similarity to the prototypes of that category exceeds a certain threshold. In the sequel we demonstrate the relationshipbetween label semantics and exactly such a thresholding model.5. A prototype theory interpretation of label semanticsIn this interpretation it is proposed that the basic labels in LA correspond to natural categories each with an associatedset of prototypes. A label L is then deemed to be an appropriate description of an element x ∈ Ω provided that x is suffi-ciently similar to the prototypes for L. The requirement of being ‘sufficiently similar’ is clearly imprecise and is modelled hereby introducing an uncertain threshold on distance from prototypes. In keeping with the epistemic stance this uncertainty isassumed to be probabilistic in nature. In other words, an agent believes that there is some optimal threshold of this kind ac-cording to which he or she is best able to abide by the conventions of language when judging the appropriateness of labels.However, the agent is uncertain as to exactly what this threshold should be and instead defines a probability distribution onpotential threshold values. Notice that the idea of an uncertain threshold fits well with the epistemic theory of vaguenesssince it naturally results in uncertain (but crisp) concept boundaries. Although, as discussed in Section 2, the assumption ofan unknown objective definition of the concept is not required. Instead within such a model the agent’s knowledge of thethreshold probability distribution and also of the label prototypes would be derived from their experience of language useacross a population of communicating agents each adopting the epistemic stance.J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15581547Fig. 2. Identifying D(cid:3){L1, L2, L3, L4}.x as (cid:3) varies, where P i = {pi } for Li ∈ LA are singletons; For (cid:3)1, (cid:3)2 and (cid:3)3 shown in the diagram D(cid:3)1x= ∅, D(cid:3)2x= {L1, L2}, D(cid:3)3x=In this section we begin by introducing a special case of the prototype model that results in a calculus for appropriatenessmeasures consistent with the consonant selection function (Definition 11). We then propose a more general theory whichcan represent a range of semantic dependencies between labels.5.1. A consonant modelSuppose that a distance function3 d is defined on Ω such that d : Ω 2 → [0, ∞) and satisfies d(x, x) = 0 andd(x, y) = d( y, x) for all elements x, y ∈ Ω . This function is then extended to sets of elements such that for S, T ⊆ Ω ,d(S, T ) = inf{d(x, y): x ∈ S and y ∈ T }. For each label Li ∈ LA let there be a set P i ⊆ Ω corresponding to prototypicalelements for which Li is certainly an appropriate description. Within this framework Li is deemed to be appropriate todescribe an element x ∈ Ω provided x is sufficiently close or similar to a prototypical element in P i . This is formalized bythe requirement that x is within a maximal distance threshold (cid:3) of P i . I.e. Li is appropriate to describe x if d(x, P i) (cid:2) (cid:3)where (cid:3) (cid:3) 0. From this perspective an agent’s uncertainty regarding the appropriateness of a label to describe a value x ischaracterised by his or her uncertainty regarding the distance threshold (cid:3). In fact, it is also possible that an agent could beuncertain regarding the definition of the prototype sets P i . However, for this paper we make the simplification assumptionthat no uncertainty is associated with the prototypes. Here we assume that (cid:3) is a random variable and that the uncertaintyis represented by a probability density function δ for (cid:3) defined on [0, ∞).4 Within this interpretation a natural definitionof the complete description of an element Dx and the associated mass function mx can be given as follows:Definition 15. For (cid:3) ∈ [0, ∞) D(cid:3)x= {Li ∈ LA: d(x, P i) (cid:2) (cid:3)} and ∀F ⊆ LA mx(F ) = δ({(cid:3): D(cid:3)x= F }).5Intuitively speaking D(cid:3)x identifies the set of labels with prototypes lying within (cid:3) of x. Fig. 2 shows D(cid:3)x in a hypotheticalconceptual space as (cid:3) varies. Notice that the sequence D(cid:3)x as (cid:3) varies generates a nested hierarchy of label sets. Furthermore,the distance metric d naturally generates a total ordering on the appropriateness of labels for any element x, according towhich label L j is as least as appropriate to describe x as label Li if x is closer (or equidistant) to P j than to P i i.e. Li (cid:16)x L jiff d(x, P i) (cid:3) d(x, P j) as suggested in Section 3.1.Also notice from Definition 15, that for Li ∈ LA the appropriateness measure μLi (x) is given by δ({(cid:3): Li ∈ D(cid:3)}). Conse-quently, if we view D(cid:3)x as a random set from [0, ∞) into 2LA then μLi (x) corresponds to the single point coverage function6of D(cid:3)x . This provides us with a link to the random set interpretation of fuzzy sets (see [6,7,12,13,27]) except that in this casethe random set maps to sets of labels rather than sets of elements. Hence, the interpretation of label semantics as proposedabove provides a link between random set theory and prototype theory. A further consequence of this relationship withrandom set theory is that ∀x ∈ Ω , the mass function mx must satisfy the conditions given in Definition 11. This followsfrom the well-known fact that the mass function of a nested (or consonant) finite random set, in this case D(cid:3)x , can bedetermined uniquely from its single point coverage function (see [22] for an exposition).xThe following results show how the appropriateness of an expression θ ∈ LE to describe an element x is equivalent to aconstraint (cid:3) ∈ I(θ, x), for a measurable subset I(θ, x) of [0, ∞) defined as follows:3 d may also be a distance metric if it satisfies the triangular inequality ∀x, y, z ∈ Ω d(x, z) (cid:2) d(x, y) + d( y, z), but this is not strictly required.4 Even though (cid:3) is a random variable there is no suggestion of any underlying stochastic process. Instead an agent’s uncertainty concerning (cid:3) is epistemicin nature and dependent on their experience of language use as discussed in Section 2.1.(cid:8)5 For Lebesgue measurable set I , we denote δ(I) =6 For a finite random set S into 2U, where U is the underlying finite universe, the associate mass function m : 2I δ((cid:3)) d(cid:3) i.e. we also use δ to denote the probability measure induced by density function δ.is the probability that S = T . The single point coverage function then corresponds to the probability that z ∈ S, for z ∈ U , and is given byIn label semantics U = LA and S = D(cid:3)x .U → [0, 1] is such that for T ⊆ U , m(T )T : z∈T m(T ).(cid:2)1548J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558Definition 16. ∀x ∈ Ω and θ ∈ LE, I(θ, x) ⊆ [0, ∞) is defined recursively as follows: ∀θ, ϕ ∈ LE• ∀Li ∈ LA I(Li, x) = [d(x, P i), ∞).• I(θ ∧ ϕ, x) = I(θ, x) ∩ I(ϕ, x).• I(θ ∨ ϕ, x) = I(θ, x) ∪ I(ϕ, x).• I(¬θ, x) = I(θ, x)c .Theorem 17.∀θ ∈ LE, ∀x ∈ Ω I(θ, x) =(cid:9)(cid:3): D(cid:3)x(cid:10)∈ λ(θ)Proof. LetLE(1) = LA and LE(k) = LE(k−1) ∪(cid:9)θ ∧ ϕ, θ ∨ ϕ, ¬θ : θ, ϕ ∈ LE(k−1)(cid:10)We now prove the result by induction on k.Limit Case: k = 1 For Li ∈ LA we have thatI(Li, x) =(cid:11)d(x, P i), ∞(cid:6)(cid:9)=(cid:3): d(x, P i) (cid:2) (cid:3)(cid:10)(cid:9)=(cid:3): Li ∈ D(cid:3)(cid:10)(cid:9)=(cid:3): D(cid:3)x(cid:10)∈ λ(Li)xInductive Step: Assume true for k For Φ ∈ LE(k+1) either Φ ∈ LE(k), in which case the result holds trivially by the inductivehypothesis, or one of the following holds:= {(cid:3): D(cid:3)x• Φ = θ ∧ ϕ so that I(Φ, x) = I(θ ∧ ϕ, x) = I(θ, x) ∩ I(ϕ, x) = {(cid:3): D(cid:3)x∈ λ(Φ)}.• Φ = θ ∨ ϕ so that I(Φ, x) = I(θ ∨ ϕ, x) = I(θ, x) ∪ I(ϕ, x) = {(cid:3): D(cid:3)x∈ λ(Φ)}.∈ λ(θ) ∩ λ(ϕ)} = {(cid:3): D(cid:3)x∈ λ(θ ∧ ϕ)} = {(cid:3): D(cid:3)x∈ λ(θ) ∪ λ(ϕ)} = {(cid:3): D(cid:3)x∈ λ(θ ∨ ϕ)} = {(cid:3): D(cid:3)x= {(cid:3): D(cid:3)x• Φ = ¬θ so that I(Φ, x) = I(¬θ, x) = I(θ, x)c = {(cid:3): D(cid:3)xλ(Φ)}. (cid:2)∈ λ(θ)} ∩ {(cid:3): D(cid:3)x∈ λ(ϕ)} (by the inductive hypothesis)∈ λ(θ)} ∪ {(cid:3): D(cid:3)x∈ λ(ϕ)} (by the inductive hypothesis)∈ λ(θ)}c = {(cid:3): D(cid:3)x∈ λ(θ)c} = {(cid:3): D(cid:3)x∈ λ(¬θ)} = {(cid:3): D(cid:3)x∈Corollary 18.∀θ ∈ LE, ∀x ∈ Ω μθ (x) = δ(cid:5)(cid:6)I(θ, x)Proof. The result follows trivially from Theorem 17 and Definition 15. (cid:2)Furthermore, in the case that we restrict ourselves to expressions in LE∧,∨(Theorem 13) the following result shows thatI(θ, x) simply identifies a lower bound on (cid:3).Definition 19. We define lb : LE∧,∨ × Ω → [0, ∞) recursively as follows: ∀x ∈ Ω , ∀θ, ϕ ∈ LE∧,∨• ∀Li ∈ L A lb(Li, x) = d(x, P i).• lb(θ ∧ ϕ, x) = max(lb(θ, x), lb(ϕ, x)) and lb(θ ∨ ϕ, x) = min(lb(θ, x), lb(ϕ, x)).Theorem 20. ∀x ∈ Ω , ∀x ∈ LE∧,∨, then I(θ, x) = [lb(θ, x), ∞).Proof. LetLE∧,∨1∧,∨= LA and LEk= LE∧,∨k−1∪(cid:9)θ ∧ ϕ, θ ∨ ϕ: θ, ϕ ∈ LE∧,∨k−1(cid:10)We now prove the result by induction on k.Limit Case: k = 1 For Li ∈ LA I(Li, x) = [d(x, P i), ∞) = [lb(Li, x), ∞) as required.Inductive Step: Assume true for k For Φ ∈ LEinductive hypothesis or one of the following holds:∧,∨k+1 either Φ ∈ LE∧,∨kin which case the result follows trivially from the∧,∨• Φ = θ ∧ ϕ where θ, ϕ ∈ LEk∧,∨• Φ = θ ∨ ϕ where θ, ϕ ∈ LEkhypothesis)= [max(lb(θ, x), lb(ϕ, x)), ∞) = [lb(θ ∧ ϕ, x), ∞) as required.hypothesis)= [min(lb(θ, x), lb(ϕ, x)), ∞) = [lb(θ ∨ ϕ, x), ∞) as required. (cid:2). In this case I(Φ, x) = I(θ, x) ∩ I(ϕ, x) = [lb(θ, x), ∞) ∩ [lb(ϕ, x), ∞) (by the inductive. In this case I(Φ, x) = I(θ, x) ∪ I(ϕ, x) = [lb(θ, x), ∞) ∪ [lb(ϕ, x), ∞) (by the inductiveJ. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15581549Fig. 3. Area under the density function δ corresponding to μLi ∧¬L j (x).Fig. 4. Let LA = {L1, L2, L3, L4} and L4 (cid:16)x L3 (cid:16)x L2 (cid:16)x L1. This figure shows the values of mx as areas under δ.Notice, from Theorem 20 we have that ∀θ, ϕ ∈ LE∧,∨ μθ∨ϕ(x) = δ([lb(θ ∨ ϕ, x), ∞)) = δ([min(lb(θ, x), lb(ϕ, x)), ∞)) =max(δ([lb(θ, x), ∞)), δ([lb(ϕ, x), ∞))) = max(μθ (x), μϕ(x)). Similarly, μθ∧ϕ(x) = min(μθ (x), μϕ(x)) as is consistent withTheorem 13.Example 21.I(Li, x) =I(Li ∨ L j, x) =I(Li ∧ ¬L j, x) =,(cid:6)(cid:11)d(x, P i), ∞I(¬Li, x) =(cid:5)(cid:6)(cid:11)d(x, P i), d(x, P j)min(cid:12)[d(x, P i), d(x, P j))∅, ∞(cid:6)(cid:11)0, d(x, P i)(cid:6),I(Li ∧ L j, x) =(cid:11)(cid:5)(cid:6)d(x, P i), d(x, P j)max, ∞(cid:6)if d(x, P i) < d(x, P j)otherwise(cid:4)(cid:4)From Theorem 6 we have that for F ⊆ LA mx(F ) = μθF (x) where θF = (¬L). Hence, mx(F ) = δ(I(θF , x))where I(θF , x) = [max{d(x, P i): Li ∈ F }, min{d(x, P i): Li /∈ F }) provided that max{d(x, P i): Li ∈ F } < min{d(x, P i): Li /∈ F }and = ∅ otherwise.L∈F L) ∧ (L /∈FFigs. 3 and 4 show the areas under δ corresponding to μLi ∧¬L j (x) and the values of the mass function mx respectively.Example 22. Let Ω = [0, 10] and the labels LA = {L1, L2, L3} be defined such that P 1 = {4}, P 2 = [5, 7] and P 3 = [7.5, 8] andlet d(x, y) = (cid:19)x − y(cid:19). Also let δ be a Gaussian density with mean 0 and standard deviation 0.6 renormalised so that areaunder [0, 10] is 1 (see Fig. 5). Now consider the element x = 4.5 then D(cid:3)4.5 is defined as follows:1550J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558Fig. 5. Density function δ, normalised from a Gaussian with mean 0 and standard deviation 0.6.Fig. 6. Appropriateness measures for L1, L2 and L3 derived from a Gaussian density δ and prototypes P 1 = {4}, P 2 = [5, 7] and P 3 = [7.5, 8].⎧⎨⎩D(cid:3)4.5=∅{L1, L2}{L1, L2, L3} (cid:3) (cid:3) 3(cid:3) < 0.50.5 (cid:2) (cid:3) < 3The associated mass function m4.5 is then given bym4.5 = ∅: δ(cid:5)(cid:6)[0, 0.5)≈ 0.5953,{L1, L2}: δ(cid:5)(cid:6)[0.5, 3)≈ 0.4047,{L1, L2, L3}: δ(cid:5)(cid:6)[3, 10]≈ 0Fig. 6 shows the appropriateness measures for labels L1, L2 and L3 corresponding to(cid:5)(cid:11)(cid:6)(cid:6)(cid:5)(cid:11)(cid:16)(cid:6)μLi (x) = δd(x, P i), ∞= δd(x, P i), 10J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–155815515.2. Neighbourhoods of prototypesFig. 7. N (cid:3)Lias a neighbourhood of P i .Another perspective on the prototype theory view of label semantics results from considering the set of elements fromΩ which can be appropriately described by a label or expression. For an expression θ ∈ LE this subset of Ω would, ineffect, correspond to the extension of the concept represented by θ . Now for the model proposed in Section 5.1 the setof elements which can be appropriately described by a label Li ∈ LA corresponds to the neighbourhood of P i (see Fig. 7)defined by:(cid:9)N (cid:3)Li=x ∈ Ω: d(x, P i) (cid:2) (cid:3)(cid:10)This can be naturally extended to any label expression θ ∈ LE as follows:Definition 23.∀θ ∈ LE N (cid:3)θ=(cid:9)x ∈ Ω: D(cid:3)x(cid:10)∈ λ(θ)=(cid:9)(cid:10)x ∈ Ω: (cid:3) ∈ I(θ, x)The following theorem shows that N (cid:3)θ can be determined recursively from neighbourhoods N (cid:3)LiLi ∈ LA.for the basic labelsTheorem 24. ∀x ∈ Ω, ∀θ, ϕ ∈ LE, ∀(cid:3) ∈ [0, ∞) the following hold:(i) N (cid:3)θ∧ϕ(ii) N (cid:3)θ∨ϕ(iii) N (cid:3)¬θ= N (cid:3)θ= N (cid:3)θ= (N (cid:3)θ )c .∩ N (cid:3)ϕ .∪ N (cid:3)ϕ .Proof.(i) N (cid:3)(ii) N (cid:3)θ∧ϕ= {x: D(cid:3)xDefinition 23).= {x: D(cid:3)xDefinition 23).= {x: D(cid:3)xθ∨ϕ(iii) N (cid:3)¬θ∈ λ(θ ∧ ϕ)} = (by Definition 3) {x: D(cid:3)x∈ λ(θ) ∩ λ(ϕ)} = {x: D(cid:3)x∈ λ(θ)} ∩ {x: D(cid:3)x∈ λ(ϕ)} = N (cid:3)θ∩ N (cid:3)ϕ (by∈ λ(θ ∨ ϕ)} = (by Definition 3) {x: D(cid:3)x∈ λ(θ) ∪ λ(ϕ)} = {x: D(cid:3)x∈ λ(θ)} ∪ {x: D(cid:3)x∈ λ(ϕ)} = N (cid:3)θ∪ N (cid:3)ϕ (by∈ λ(¬θ)} = (by Definition 3) {x: D(cid:3)x∈ λ(θ)c} = {x: D(cid:3)x∈ λ(θ)}c = (N (cid:3)θ )c (by Definition 23). (cid:2)Theorem 25.∀θ ∈ LE, ∀x ∈ Ω μθ (x) = δ(cid:5)(cid:9)(cid:3): x ∈ N (cid:3)θ(cid:10)(cid:6)1552J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558Fig. 8. For P i = {pi } and P j = {p j } x and y are elements such that x ∈ N(cid:3)and N(cid:3)(cid:21)(cid:2) N(cid:3).Li ∧¬L jLi ∧¬L jLi ∧¬L jbut x /∈ N(cid:3)(cid:21)Li ∧¬L j, and y /∈ N(cid:3)Li ∧¬L jbut y ∈ N(cid:3)(cid:21)Li ∧¬L j. Hence N(cid:3)Li ∧¬L j(cid:2) N(cid:3)(cid:21)Li ∧¬L jProof.(cid:5)(cid:9)δ(cid:3): x ∈ N (cid:3)θ(cid:10)(cid:6)= (by Definition 23) δ(cid:5)(cid:9)(cid:3): D(cid:3)x(cid:10)(cid:6)∈ λ(θ)=(cid:3)(cid:5)(cid:9)(cid:3): D(cid:3)xδ(cid:10)(cid:6)= F= (by Definition 15)F : F ∈λ(θ )mx(F ) = μθ (x)(cid:2)(cid:3)F : F ∈λ(θ )Theorem 25 provides us with an alternative characterisation of appropriateness measure μθ (x) as the single point cov-θ . Again this shows a link with the work of Goodman and Nguyen (seeerage function of the random set neighbourhood N (cid:3)[12,13] and [27]) and their proposed interpretation of fuzzy sets as single point coverage functions of random sets.The following theorem shows that for expressions θ not involving negation the random set neighbourhood N (cid:3)θ is nested.Theorem 26. ∀θ ∈ LE∧,∨and ∀(cid:3)(cid:21) (cid:3) (cid:3) (cid:3) 0 N (cid:3)θ⊆ N (cid:3)(cid:21)θ .Proof.N (cid:3)θ(cid:9)(cid:10)∈ λ(θ)=x: D(cid:3)x= (by Theorem 20)(cid:10)x: (cid:3)(cid:21) ∈ I(θ, x)=(cid:9)= (by Theorem 17)(cid:10)⊆x: lb(θ, x) (cid:2) (cid:3)(cid:9)= N (cid:3)(cid:21)θ (by Definition 23)(cid:9)(cid:10)x: (cid:3) ∈ I(θ, x)(cid:9)x: lb(θ, x) (cid:2) (cid:3)(cid:21)(cid:2)(cid:10)∧,∨, so that x ∈ N (cid:3)For expressions θ /∈ LEit is not in general the case that N (cid:3)θ, while y /∈ N (cid:3)Li. Hence, N (cid:3)forms a nested sequence as (cid:3) varies. For example,consider the expression Li ∧ ¬L j and suppose there are elements x, y ∈ Ω together with real values (cid:3)(cid:21) > (cid:3) > 0 such thatd(x, P i) (cid:2) (cid:3) < min(d(x, P j), d( y, P i)) and max(d( y, P i), d(x, P j)) (cid:2) (cid:3)(cid:21) < d( y, P j) (see for example Fig. 8). In this case x ∈ N (cid:3)Liand x /∈ N (cid:3), whileL jand y /∈ N (cid:3)(cid:21)y ∈ N (cid:3)(cid:21)(cid:2)L jLisince x ∈ N (cid:3)N (cid:3). On the other hand x ∈ N (cid:3)(cid:21)L jsince y ∈ N (cid:3)(cid:21)and y /∈ N (cid:3)Li ∧¬L jIn fuzzy set theory the idea of α-cuts is frequently applied in order to extend classical methods to the fuzzy case [5].The α-cut of a fuzzy set A, denoted Aα , is given by the set of domain elements with membership in A greater than orequal to α. In label semantics there is a analogous idea where the α-cut of an expression θ ∈ LE is given by the set ofelements of Ω for which the appropriateness measure of θ is greater than or equal to α i.e. θα = {x ∈ Ω: μθ (x) (cid:3) α}.7 Inthe following we show that, provided δ satisfies certain smoothness conditions, then there is a direct mapping between theα-cuts θα and the neighbourhoods N (cid:3)Li ∧¬L jso that y ∈ N (cid:3)(cid:21)and x /∈ N (cid:3)(cid:21)so that y /∈ N (cid:3)(cid:2) N (cid:3)(cid:21)Li ∧L j. Also, N (cid:3)(cid:21)so that x /∈ N (cid:3)(cid:21)θ for expressions θ ∈ LELi ∧¬L jLi ∧¬L jLi ∧¬L jLi ∧¬L jLi ∧¬L jLi ∧¬L jLi ∧¬L jLi ∧¬L jLi ∧L j∧,∨..Definition 27. Let (cid:11) : [0, ∞) → [0, 1] such that (cid:11)((cid:3)) = δ([(cid:3), ∞)).Notice that since (cid:11) is an integral (i.e. (cid:11)((cid:3)) =(cid:8) ∞(cid:3) δ((cid:3)) d(cid:3)) it is a continuous decreasing function into [0, 1] and that forlabels Li ∈ LAμLi (x) = (cid:11)(d(x, P i)). This suggests a strong relationship with the threshold model proposed by Hampton [16]7 Given the assertability model proposed in Section 3.2, then for α (cid:3) 0.5 θα = {x ∈ Ω: Assertα (x is θ)} which corresponds to those elements x for which‘x is θ ’ can be asserted at certainty level α.J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15581553to explain the relationship between the typicality of an instance in a category (as proportional to its similarity to a set ofprototypes) and its membership in that category. Specifically, Hampton proposed that the membership of an instance in acategory L should be defined as a function m of the similarity of the instance to the prototypes of L, and where m is anincreasing function into [0, 1]. Clearly if we replace similarity with distance from prototypes this would naturally suggestof distance into [0, 1], as is consistent with label semantics proto-that membership should be a decreasing function m(cid:21) = (cid:11). Interestingly, re-analysing an experiment of McCloskey and Glucksberg [25], Hampton [16,17]type model where mrequired subjects to make binary categorization decisions for 482 items into 17 categories. They also independently madeassessments of typicality. Category membership was then taken as corresponding to the probability of a positive catego-rization. In general, the results were consistent with a threshold model where m was a cumulative normal distribution.8Notice, also that if δ is assumed to be a normalised normal distribution (as in Example 22) then (cid:11) is one minus the corre-sponding normalised cumulative normal distribution. Hence, the experimental results of McCloskey and Glucksberg appearto be consistent with individuals making the binary decision that ‘x is Li ’, for category label Li , provided that d(x, P i) (cid:2) (cid:3)for normally distributed threshold (cid:3).(cid:21)Lemma 28. ∀θ ∈ LE∧,∨, ∀α ∈ (0, 1] (θ)α = N(cid:3)∗θ where (cid:3)∗ = sup{(cid:3): (cid:11)((cid:3)) (cid:3) α}.Proof. Now clearly (cid:11) is differentiable and hence continuous. Therefore, {(cid:3): (cid:11)((cid:3)) (cid:3) α} = [0, (cid:3)∗] where (cid:3)∗ = sup{(cid:3): (cid:11)((cid:3)) (cid:3)α}. Hence by Theorems 17 and 20,(cid:9)(cid:9)(cid:10)(cid:10)(cid:10)(cid:9)(cid:5)(θ)α ==(cid:9)=x: μθ (x) (cid:3) α(cid:10)x: lb(θ, x) (cid:2) (cid:3)∗x: δ(cid:9)(cid:6)(cid:3) αI(θ, x)(cid:10)x: (cid:3)∗ ∈ I(θ, x)==(cid:9)=x: (cid:11)x: D(cid:3)∗x(cid:6)(cid:5)lb(θ, x)(cid:10)∈ λ(θ)(cid:3) α= N(cid:3)∗θ(cid:2)Theorem 29. If δ is such that (cid:11)| J : J → (0, 1] is a strictly decreasing function where (cid:11)| J is the restriction of (cid:11) to J = {(cid:3): (cid:11)((cid:3)) > 0}then ∀θ ∈ LE, (cid:22)θα, (cid:3), (0, 1](cid:23) is isomorphic to (cid:22)N (cid:3)∧,∨θ , (cid:2), J (cid:23).Proof. Since (cid:11)| Jfunction (cid:11)|−1(cid:3)∗ = (cid:11)|−1Jis strictly decreasing and continuous on J then (cid:11)| J : J → (0, 1] is a bijection. Consequently, the inverseis a strictly decreasing bijective function. Hence, for (cid:3)∗ = sup{(cid:3): (cid:11)| J ((cid:3)) (cid:3) α} we have that: (0, 1] → JJ (α). Consequently, by Lemma 28 we have that θα = N (cid:11)|−1θJ (α)as required. (cid:2)Corollary 30. For (cid:11)| J satisfying the conditions of Theorem 29. If α is a random variable into (0, 1] defined such that α = (cid:11)| J ((cid:3)) thenα is a uniformly distributed random variable.Proof.∀α∗ ∈ (0, 1](cid:5)Pα (cid:2) α∗(cid:6)(cid:5)= P(cid:3) (cid:3) (cid:11)|−1J(cid:6)(cid:6)(cid:5)α∗= (cid:11)| J(cid:5)(cid:5)(cid:11)|−1Jα∗(cid:6)(cid:6)= α∗(cid:2)In [5] the use of α-cuts is proposed as a method for extending set functions to fuzzy sets. Given a set function f : 2Ω →R and a fuzzy set A on Ω the value f ( A) is defined by:f ( A) =1(cid:17)0f ( Aα) dαThis is the expected value of f ( Aα) assuming a uniform distribution α. However, no clear justification is given in [5] as towhy α should be uniformly distributed. Now the concept of prototype neighbourhoods suggests a method for extending setfunctions to expressions in LE by taking the expected value f (N (cid:3)θ ) as follows:f (θ) =∞(cid:17)0(cid:6)(cid:5)N (cid:3)θfδ((cid:3)) d(cid:3)From Corollary 30 we that, for the restricted class of expressions LE∧,∨, this is equivalent to:f (θ) =1(cid:17)0f (θα) dα8 In the case of biological categories, there was also evidence that categorization probability was also affected by non-similarity based knowledge e.g.where an animal looked superficially like one category but was actually in another.1554J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558In other words, we see from Corollary 30 that whatever the distribution δ on (cid:3), a uniform distribution should be selectedwhen averaging over α-cuts of expressions in LE∧,∨.5.3. Domain information from vague expressionsOne of the principle uses of vague concepts in language is to convey information about the underlying universe Ω inspecific cases or contexts. Parikh [30] gives the example of two college lecturers Ann and Bob, where Ann asks Bob tobring in her blue book from her library at home. How can Bob use the information that the ‘book is blue’ to restrict thepossible books of interest? The prototype interpretation of label semantics naturally suggests an imprecise restriction on thedomain Ω imposed by a constraint ‘x is θ ’ where θ ∈ LE. Specifically, given the information ‘x is θ ’ then, provided θ is nota contradiction, an agent infers that x ∈ N (cid:3)(cid:17)= ∅. From the latter inference the agent naturallygenerates a posterior distribution δθ on (cid:3) conditional on the information that the neighbourhood of θ is non-empty, where:θ and consequently that N (cid:3)θ∀(cid:3) ∈ [0, ∞)δθ =(cid:18)δ((cid:3))1−δ{(cid:3): N (cid:3)θ0=∅}N (cid:3)θ(cid:17)= ∅otherwiseHence, conditioning on the information ‘x is θ ’ results in the following random set model:Definition 31. For θ ∈ LE Nθ(cid:22)Aθ , U θ , Mθ (cid:23) where B is the σ -algebra of Borel subsets of [0, ∞), δθAθ = {{N (cid:3)(cid:3) is a random set from the probability space (cid:22)B, [0, ∞), δθ (cid:23) into9 the probability spaceθ : (cid:3) ∈ [0, ∞)},θ : (cid:3) ∈ I}: I ∈ B} and ∀ A ∈ Aθ Mθ ( A) = δθ ({(cid:3): N (cid:3)θis defined as above, U θ = {N (cid:3)∈ A}).The single point coverage function of the random set in Definition 31 then indicates the probability that a value x ∈ Ωis a possible referent in the assertion ‘x is θ ’:(cid:10)(cid:6)(cid:5)(cid:9)(cid:5)(cid:9)(cid:10)(cid:6)N (cid:3)∀x ∈ Ω spcθ (x) = Mθ= δθNotice, from Theorem 26, that in the case of θ ∈ LEθ is nested and therefore spcθ (x) is a possibility distributionon Ω . Hence, for this restricted set of expressions our approach is in accordance with that of Zadeh [39] who argues thatfuzzy information generates possibilistic constraints on the underlying domain of discourse.(cid:3): x ∈ N (cid:3)θthen N (cid:3)θ : x ∈ N (cid:3)θ∝ μθ (x)∧,∨5.4. A general modelSuppose for each label Li ∈ LA there is a distinct function di : Ω 2 → [0, ∞) and a neighbourhood threshold given byrandom variable (cid:3)i . Also suppose (cid:25)(cid:3) = ((cid:3)1, . . . , (cid:3)n) has a joint density function δ defined on [0, ∞)n. In this model we havea natural definition of the complete description of an element and the neighbourhood of an expression as follows:Definition 32.∀x ∈ Ω D (cid:25)(cid:3)∀θ ∈ LE N (cid:25)(cid:3)θx(cid:9)==Li: di(x, P i) (cid:2) (cid:3)i(cid:9)x ∈ Ω: D (cid:25)(cid:3)(cid:10)∈ λ(θ)x(cid:10)and mx(F ) = δ(cid:5)(cid:9)(cid:25)(cid:3): D (cid:25)(cid:3)x= F(cid:10)(cid:6)Notice from Definition 32 that if δi denotes the marginal density of (cid:3)i derived from joint density δ then the appropri-ateness measure for label Li ∈ LA can be evaluated directly from δi according to:(cid:5)(cid:9)μLi (x) = δ= δi((cid:3)1, . . . , (cid:3)n): Li ∈ D((cid:3)1,...,(cid:3)n)(cid:5)(cid:11)(cid:6)(cid:6)di(x, pi), ∞x(cid:10)(cid:6)= δ(cid:5)(cid:9)((cid:3)1, . . . , (cid:3)n): di(x, Li) (cid:2) (cid:3)i, (cid:3) j ∈ [0, ∞) for j (cid:17)= i(cid:10)(cid:6)The joint density δ encodes dependencies between the random variables (cid:3)i which in turn reflect semantic dependenciesbetween the labels. For the consonant model we have total dependence where di = d and (cid:3)i = (cid:3) for all Li ∈ LA. This suggeststhat the labels in LA all describe the same characteristic of the elements in Ω as represented by attribute values in a singleconceptual space. Typical examples where such a dependent model may be appropriate are colour labels based on attributesfrom a multi-dimensional space such as the so-called colour spindle [11] or on rgb values. Other examples include labelsfor one-dimensional characteristics such height, weight, income and, of course, number of hairs on your head. The followingvariant on the approach proposed in Section 5.1 also results in a consonant model but where there is variation in scalebetween the threshold variables (cid:3)i .9 I.e. for a fixed θ , N (cid:3)θ is a function from [0, ∞) into U θ which satisfies ∀I ∈ B, {N (cid:3)θ : (cid:3) ∈ I} ∈ Aθ .J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–155815555.4.1. A general consonant modelAssume that di = d for all Li ∈ LA and that (cid:3)i = f i((cid:3)) for some increasing function f i : [0, ∞) → [0, ∞) of randomvariable (cid:3) into [0, ∞). Further suppose that (cid:3) has density function δ defined on [0, ∞). Then we define:(cid:9)(cid:10)Li: d(x, P i) (cid:2) f i((cid:3))D(cid:3)x=so that mx(F ) = δ(cid:5)(cid:9)(cid:3): D(cid:3)x= F(cid:10)(cid:6)and N (cid:3)θ=(cid:9)x ∈ Ω: D(cid:3)x(cid:10)∈ λ(θ)Now clearly since f i is an increasing function for Li ∈ LA it holds that D(cid:3)xas (cid:3) varies.⊆ D(cid:3)(cid:21)xfor (cid:3) (cid:2) (cid:3)(cid:21). So D(cid:3)x forms a nested hierarchyThe following theorem shows how one label Li can be defined as a restriction of another label L jin the consonant. For example, the label navy blue is a restriction of the label blue in this sense. In suchmodel so that ∀(cid:3) (cid:3) 0 N (cid:3)Licases L j is an appropriate description whenever Li is appropriate and ∀x ∈ Ω μ¬Li ∨L j (x) = 1.⊆ N (cid:3)L jTheorem 33. For labels Li, L j ∈ LA where ∀(cid:3) (cid:3) 0 f j((cid:3)) (cid:3) sup{d(x, P j): x ∈ N (cid:3)Li} then ∀(cid:3) (cid:3) 0 N (cid:3)Li⊆ N (cid:3)L j.Proof. For (cid:3) (cid:3) 0 suppose x ∈ N (cid:3)Lithen d(x, P j) (cid:2) f j((cid:3)) ⇒ x ∈ N (cid:3)L j. (cid:2)Notice that one simple case of nested labels is where P i ⊆ P j and f i (cid:2) f j . In this case sup{d(x, P j): d(x, P i) (cid:2) f i((cid:3))} (cid:2)sup{d(x, P i): d(x, P i) (cid:2) f i((cid:3))} (since ∀x d(x, P j) (cid:2) d(x, P j)) (cid:2) f i((cid:3)) (cid:2) f j((cid:3)).5.4.2. An independence modelSuppose for each label Li ∈ LA there is a distinct metric di : Ω 2 → [0, ∞). Further suppose that the threshold distancevalues for each metric are independent random variables (cid:3)i with density δi , so that the joint density δ =(cid:19)Li ∈LA δi .Theorem 34. If (cid:3)i: Li ∈ LA are independent random variables then:∀F ⊆ LA mx(F ) =(cid:7)Li ∈FμLi (x) ×(cid:7)(cid:5)(cid:6)1 − μLi (x)Li /∈FProof. W.l.o.g. assume F = {L1, . . . , Lk} thenmx(F ) = δ= δ(cid:5)(cid:9)(cid:5)(cid:9)(cid:10)(cid:6)x= {L1, . . . , Lk}((cid:3)1, . . . , (cid:3)n): D((cid:3)1,...,(cid:3)n)((cid:3)1, . . . , (cid:3)n): d1(x, P 1) (cid:2) (cid:3)1, . . . , dk(x, P k) (cid:2) (cid:3)k, di(x, P i) > (cid:3)i, for i > k(cid:6)1 − μLi (x)d(x, P i), ∞μLi (x) ×0, d(x, P i)(cid:7)(cid:5)(cid:7)(cid:7)(cid:6)(cid:6)(cid:6)(cid:6)(cid:5)(cid:11)(cid:5)(cid:11)×=δiδi(cid:10)(cid:6)(cid:2)(cid:7)=i(cid:2)ki>ki(cid:2)ki>kClearly we see from Theorem 34 that the assumption of independence between (cid:3)i: Li ∈ LA results in a model consistentwith the independent selection function (Definition 12). This type of model would seem to be relevant in the case wheredifferent labels refer to different independent characteristics of the elements in Ω . For example, if Ω corresponds to a set ofpeople, then judgments concerning the appropriateness of labels rich and tall would be made on the basis of independentmetrics comparing people in terms of their wealth and their height respectively.5.4.3. Semantic dependenceSemantic dependence between labels as modelled by the joint distribution δ on (cid:3)i: Li ∈ LA captures the relationshipbetween the various characteristics described by the different labels in LA. If all labels describe the same characteristic ofelements in Ω , as represented by vectors of attribute values in a shared conceptual space, then an assumption of strongsemantic dependence such as in the consonant model or the general consonant model (Section 5.4.1) is appropriate. Onthe other hand, if each label refers to a different characteristic as represented by points in distinct independent conceptualspaces then an assumption of independence between (cid:3)i , as in Section 5.4.2, is valid. For more diverse label sets LA, wewould expect there to be more complex dependencies between (cid:3)i: Li ∈ LA than the two extreme cases described above.For example, the characteristics described by different labels may have conceptual spaces which, while not being identical,have attributes in common or which have dependencies between attributes. It is certainly not the case that every jointdistribution δ will result in a selection function as given in Definition 10, and hence the resulting calculus for appropriate-ness measures may not be functional. In such cases it would be interesting to investigate the use of graphical models suchas Bayesian Networks to represent the dependencies between the different threshold variables. However, this possibility isbeyond the scope this paper and remains to be explored as part of future work.1556J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15585.5. Discussion of the label semantics prototype modelTypically in prototype theory the prototypes are not necessarily seen as corresponding to actual examples experiencedby the agent but instead as abstractions derived from experience (see Hampton [15]). This is in keeping with many unsu-pervised learning algorithms where class labels based on clusters are represented by centroids aggregated from actual data.Alternatively, exemplar models (Medin and Shaffer [26]) are a variant on prototype theory where agents have a memorystore consisting of actually encountered exemplars of particular description labels. The k-nearest neighbours algorithm isan obvious example of a classification method based on this philosophy. Now the application of prototype theory in thispaper does not depend on any distinction between actual exemplars and abstractions provided both are members of theunderlying space Ω . Consequently, the label semantics approach can be applied to both variants of prototype theory.The model presented in Section 5 defines prototypes only for the basic labels LA and not for general compound expres-sions in LE − LA. For expression θ ∈ LE − LA the neighbourhood N (cid:3)is defined recursively on the basis of distance fromθprototypes for the labels which occur in θ rather than being based on distance from a set of prototypes for θ itself. This isconsistent with the underlying philosophy of label semantics which assumes that the labels LA represent primitive conceptsand that the appropriateness of compound expressions is derived as a function only of the appropriateness of these labels.Indeed there is a close analogy here with the distinction that Gärdenfors [11] draws between properties which correspondto convex regions of conceptual space, and hence can potentially be represented by prototypes, and concepts which are con-structed as combinations of properties. However, in its original conception [32] prototype theory assumes that compoundexpressions such as not tall or orange & not red are also defined directly in terms of proximity to a (set of) prototypicalelement(s). In the label semantics framework this would correspond to the assumption that for every expression θ ∈ LEthere exists prototypes P θ ⊆ Ω such that the set of elements which can be appropriately described by θ are exactly thoseelements which lie with a threshold (cid:3) of P θ i.e. N (cid:3)= {x: d(x, P θ ) (cid:2) (cid:3)}. We shall refer to this assumption as the compoundθprototypes hypothesis.ϕ∧ψ∩ N (cid:3)⊆ N (cid:3)ϕ∩ N (cid:3)¬ϕ∩ N (cid:3)¬ϕϕ and y ∈ N (cid:3)ϕ (i.e. ∀(cid:3) (cid:3) 0 N (cid:3)ϕ¬ϕ and hence N (cid:3)ϕUpon reflection the compound prototypes hypothesis reveals itself to be problematic in a number of ways: For example,consider negated expressions where θ ≡ ¬ϕ for some ϕ ∈ LE. In this case, the epistemic stance would require that N (cid:3)¬ϕshould not overlap with N (cid:3)= ∅), since an agent would never be willing to assert both ‘x is ϕ’and ‘x is ¬ϕ’ at any threshold level. This requirement, however, is inconsistent with the compound prototypes hypothesis.To see this consider y ∈ P ¬ϕ and let (cid:3) (cid:3) d( y, P ϕ), then y ∈ N (cid:3)⊇ { y} (cid:17)= ∅. In otherwords, if ¬ϕ is defined in terms of the distance from a set of prototypes P ¬ϕ then it will always be possible to selecta threshold (cid:3) sufficiently large that P ¬ϕ and N (cid:3)ϕ overlap. Another difficulty with the compound prototypes hypothesisarises when considering conjunctions where θ ≡ ϕ ∧ ψ . If we take the view that an agent would only be willing to assert‘x is ϕ ∧ ψ ’ if they are willing to independently assert both ‘x is ϕ’ and ‘x is ψ ’ then we should assume that ∀(cid:3) (cid:3) 0N (cid:3)ψ . However, suppose we consider a simple scenario where Ω = R, d(x, y) = (cid:19)x − y(cid:19) (Euclidean distance),P ϕ = {a} and P ψ = {b} where b > a. We might think of ϕ and ψ as representing imprecise numbers about a and about brespectively. In this context it would be rather natural to assume that P ϕ∧ψ = {c} where a < c < b. However, selecting0 < (cid:3) < d(a,c)= ∅ while N (cid:3)∩ N (cid:3)ψ . It should be noted that theϕ∧ψψrequirement ∀(cid:3) (cid:3) 0 N (cid:3)∩ N (cid:3)ψ is only valid since ϕ ∧ ψ is a true conjunctive. In natural language it is often thecase that compound expressions which would appear to be conjunctions from a syntactic perspective, do not semanticallycorrespond to classical conjunctions. For example, an expressions such as small elephant, while appearing to be in the formof a conjunction, clearly does not refer to elements of a domain which can be both independently referred to as beingsmall and as being an elephant. Here the label small is in some sense secondary to the label elephant, whereby the latteridentifies the context in which the former is defined. In a recent paper Freund [10] refers to this ordering of properties inthe definition of a concept as determination. This notion would also seem to be relevant to the type of expressions studiedin Hampton [18], where experimental results suggest that descriptions such as office furniture are not treated as classicalconjunctions. A detailed treatment of such expressions is, however, beyond the scope of this paper and the question of howto extend label semantics so as to include them remains to be explored in future work.2 we have that N (cid:3)ϕ⊆ N (cid:3)ϕ⊇ {c} (cid:17)= ∅, so that N (cid:3)(cid:2) N (cid:3)ϕ∩ N (cid:3)ϕ∧ψϕ∧ψAdopting the compound prototypes hypothesis also requires us to consider how an agent could determine the prototypesP θ for every expression θ ∈ LE. In the label semantics model the implicit assumption is that for the basic labels Li ∈ LA, P iwould be determined by a learning process involving interaction and communication with other agents. Using compoundprototypes could potentially provide an agent with more flexible and relevant concept definitions if they could base theirchoice of prototypes on actual experience of language use. The difficulty here, however, is the potentially huge amounts ofdata required to determine P θ for every semantically distinct expression θ ∈ LE. For instance, if |LA| = n then there 22nse-mantically distinct expressions in LE. For an agent to obtain specific data on the use of each of these would seem to beinfeasible even for moderately large n. Consequently, it would seem likely that to some extent agents would need to defineP θ for compound expression θ based on the prototypes of its component expressions. In effect this would require some vari-ant of the functionality assumption that for each θ ∈ LE there is a prototype generating function gθ : 2Ω × · · · × 2Ω → 2Ωaccording to which P θ = gθ (P i: Li ∈ LA). A special case of functionality would then be full prototype compositionalitywhereby generating functions g∧ : 2Ω × 2Ω → 2Ω , g∨ : 2Ω × 2Ω → 2Ω and g¬ : 2Ω → 2Ω would be defined, according towhich P θ∧ϕ = g∧(P θ , P ϕ), P θ∨ϕ = g∨(P θ , P ϕ) and P ¬θ = g¬(P θ ). Now aside from the difficulty of defining these generatingfunctions in an intuitive manner, as discussed at length in Lawry [22], it is difficult to see what advantages as a representa-J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–15581557tion framework a functional approach to prototype generation has over the kind of recursive definition of neighbourhoodsdescribed in Section 5.6. ConclusionsWe have argued that concept vagueness in natural language is a manifestation of uncertainty about the appropriateuse of labels, which naturally arises as a result of the distributed and example based manner in which language is learnt.For this reason we adopt an epistemic viewpoint according to which the uncertainty associated with vague concepts isquantified by a measure of belief that the relevant labels or expressions are appropriate to describe a given example, andwhere appropriateness is governed by the emergent conventions of language use. The label semantics framework has thenbeen introduced to provide a formal calculus for appropriateness measures of this kind.Prototype theory is an effective tool by which we can understand the role of similarity and typicality in the definitionof natural categories. The use of uncertain thresholds on the distance between elements and prototypes in order to de-fine boundaries for labels provides a clear link between prototype theory and the epistemic view of vagueness, as well asproviding an intuitive interpretation of appropriateness measures. This prototype model can be understood from two per-spectives characterised by the random sets D(cid:3)x identifies those labels inLA with prototypes sufficiently close (i.e. within (cid:3)) to x for them to be deemed appropriate descriptions of x. On the otherhand, for a given expression θ the neighbourhood N (cid:3)θ contains those elements for which θ is an appropriate description.Both approaches provide characterisations of appropriateness measures as single point coverage functions of random sets.Intuitively, an agent would focus on Dx when attempting to identify appropriate descriptions of a specific instance that theyare presented with. Alternatively, the agent would focus on N (cid:3)θ when making inferences about the underlying domain Ω ,given an assertion ‘x is θ ’.θ respectively. For an element x ∈ Ω , D(cid:3)x and N (cid:3)By allowing different distance functions and thresholds for each label in LA we can model the semantic dependencebetween labels by a joint distribution on the cross product space of threshold random variables. In this paper we havediscussed in detail two specific types of semantic dependence each of which results in a particular selection function formass values. These are total dependence where all threshold variables and distance metrics coincide and total independencewhere all threshold variable are statistically independent. The semantic dependence being modelled here is based on therelationship between the characteristics being described by the various labels, as represented by different conceptual spaces.So for the consonant (dependent) model all labels describe the same characteristic, while for the independent model alllabels describe different completely independent characteristics. Intermediate cases of dependence may not result in aselection function and consequently the resulting calculus for appropriateness measures will not be functional.As already mentioned in Section 5.4.3 one of the most interesting and promising areas to be explored as part of futurestudies are the richer models of semantic dependence allowed within the general framework proposed in Section 5.4. Theuse of graphical approaches such as Bayesian networks to model the joint distribution on threshold variables, while notnecessarily resulting in a functional calculus, may provide computationally tractable methods for evaluating appropriatenessmeasures. Another potential avenue for research is the development of rule-based methods incorporating the label semanticsprototype theory. In this case new rule-learning methods could be investigated which combine clustering algorithms withlabel semantics to provide the definition of natural description labels.AcknowledgementsWe would like to thank the reviewers for their many insightful comments on earlier drafts of this paper.Yongchuan Tang is funded by the National Natural Science Foundation of China (NSFC) under Grant 60604034, the jointfunding of NSFC and MSRA under Grant 60776798, and the Science and Technology Program of Zhejiang Province underGrant 2007C223061.References[1] B. Berlin, P. Kay, Basic Color Terms: Their Universality and Evolution, University of California Press, 1977.[2] G. Coletti, R. Scozzafava, Conditional probability, fuzzy sets, and possibility: A unifying view, Fuzzy Sets and Systems 144 (2004) 227–249.[3] B. de Finetti, Fondamenti logici del ragionamento probabilistico, Bollettino dell’ Unione Matematica Italiana 9 (1930) 258–261.[4] D. Dubois, H. Prade, An introduction to possibility and fuzzy logics, in: P. Smets, et al. (Eds.), Non-Standard Logics for Automated Reasoning, AcademicPress, 1988, pp. 742–755.[5] D. Dubois, H. Prade, Measuring properties of fuzzy sets: A general technique and its use in fuzzy query evaluation, Fuzzy Sets and Systems 38 (1990)137–152.[6] D. Dubois, H. Prade, The three semantics of fuzzy sets, Fuzzy Sets and Systems 90 (2) (1997) 141–150.[7] D. Dubois, L. Godo, H. Prade, F. Esteva, An information-based discussion of vagueness, in: H. Cohen, C. Lefebre (Eds.), Handbook of Categorization inCognitive Science, 2005, pp. 892–913.[8] K. Fine, Vagueness, truth and logic, Synthese 30 (1975) 265–300.[9] T. Flaminio, L. Godo, A logic for reasoning about the probability of fuzzy events, Fuzzy Sets and Systems 158 (6) (2007) 625–638.[10] M. Freund, On the notion of concept I, Artificial Intelligence 172 (2008) 570–590.[11] P. Gärdenfors, Conceptual Spaces: The Geometry of Thought, MIT Press, 2000.[12] I.R. Goodman, H.T. Nguyen, Uncertainty Model for Knowledge Based Systems, North Holland, 1985.[13] I.R. Goodman, Fuzzy sets as equivalence classes of random sets, in: R. Yager (Ed.), Fuzzy Set and Possibility Theory, 1982, pp. 327–342.1558J. Lawry, Y. Tang / Artificial Intelligence 173 (2009) 1539–1558[14] P. Hájek, Ten questions and one problem on fuzzy logic, Annals of Pure and Applied Logic 96 (1999) 157–165.[15] J.A. Hampton, Concepts as prototypes, in: B.H. Ross (Ed.), The Psychology of Learning and Motivation: Advances in Research and Theory, vol. 46, 2006,pp. 79–113.[16] J.A. Hampton, Typicality, graded membership and vagueness, Cognitive Science 31 (3) (2007) 355–384.[17] J.A. Hampton, Similarity-based categorization and fuzziness of natural categories, Cognition 65 (1998) 137–165.[18] J.A. Hampton, Overextension of conjunctive concepts: Evidence for a unitary model of concept typicality and class inclusion, Journal of ExperimentalPsychology: Learning, Memory and Cognitio 14 (1) (1988) 12–32.[19] E. Hisdal, Are grades of membership probabilities, Fuzzy Sets and Systems 25 (1988) 325–348.[20] R. Kintz, J. Parker, R. Boynton, Information transmission in spectral color naming, Perception and Psychophysics 5 (1969) 241–245.[21] J. Lawry, A framework for linguistic modelling, Artificial Intelligence 155 (2004) 1–39.[22] J. Lawry, Modelling and Reasoning with Vague Concepts, Springer, 2006.[23] J. Lawry, Appropriateness measures: An uncertainty model for vague concepts, Synthese 161 (2) (2008) 255–269.[24] J. Lawry, An overview of computing with words using label semantics, in: H. Bustince, F. Herrera, J. Montero (Eds.), Fuzzy Sets and Their Extensions:Representation, Aggregation and Models, Springer, 2008, pp. 65–87.[25] M. McCloskey, S. Glucksberg, Natural categories: Well defined or fuzzy sets? Memory and Cognition 6 (1978) 462–472.[26] D.L. Medin, M.M. Schaffer, Concept theory of classification learning, Psychological Review 85 (3) (1978) 207–238.[27] H.T. Nguyen, On modeling of linguistic information using random sets, Information Science 34 (1984) 265–274.[28] V. Novák, Towards formalized integrated theory of fuzzy logic, in: Z. Bien, K. Min (Eds.), Fuzzy Logic and its Applications to Engineering, InformationSciences, and Intelligent Systems, Kluwer, Dordrecht, 1995, pp. 353–363.[29] R. Parikh, Vague predicates and language games, Theoria (Spain) XI (27) (1996) 97–107.[30] R. Parikh, Vagueness and utility: The semantics of common nouns, Linguistics and Philosophy 17 (1994) 521–535.[31] F.P. Ramsey, in: R.B. Braithwaite (Ed.), The Foundations of Mathematics, and Other Logical Essays, Kegan Paul, Trench, Trubner and Company Ltd.,London, 1931.[32] E.H. Rosch, Natural categories, Cognitive Psychology 4 (1973) 328–350.[33] E.H. Rosch, Cognitive representation of semantic categories, Journal of Experimental Psychology: General 104 (1975) 192–233.[34] E.H. Ruspini, On the semantics of fuzzy logic, International Journal of Approximate Reasoning 5 (1991) 45–88.[35] L. Sivik, C. Taft, Color naming: A mapping in the NCS of common color terms, Scandinavian Journal of Psychology 35 (1994) 144–164.[36] Y. Tang, J. Zheng, Linguistic modelling based on semantic similarity relation amongst linguistic labels, Fuzzy Sets and Systems 157 (2006) 1662–1673.[37] T. Williamson, Vagueness, Routledge, 1994.[38] L.A. Zadeh, Fuzzy sets, Information and Control 8 (3) (1965) 338–353.[39] L.A. Zadeh, Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets and Systems 1 (1978) 3–28.[40] L.A. Zadeh, The concept of linguistic variable and its application to approximate reasoning, Part 1, Information Sciences 8 (1975) 199–249.[41] L.A. Zadeh, The concept of linguistic variable and its application to approximate reasoning, Part 2, Information Sciences 8 (1975) 301–357.[42] L.A. Zadeh, The concept of linguistic variable and its application to approximate reasoning, Part 3, Information Sciences 9 (1976) 43–80.