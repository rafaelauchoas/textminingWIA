Automated Segmentation of Retinal Layers from Optical CoherentTomography Images Using Geodesic DistanceJinming Duan∗, Christopher Tench†, Irene Gottlob‡, Frank Proudlock‡, Li Bai∗∗School of Computer Science, University of Nottingham, UK†School of Medicine, University of Nottingham, UK‡Ophthalmology Department, University of Leicester, UK6102peS7]VC.sc[1v41220.9061:viXraAbstractOptical coherence tomography (OCT) is a non-invasive imaging technique that can produce images of theeye at the microscopic level. OCT image segmentation to localise retinal layer boundaries is a fundamentalprocedure for diagnosing and monitoring the progression of retinal and optical nerve disorders. In this paper,we introduce a novel and accurate geodesic distance method (GDM) for OCT segmentation of both healthyand pathological images in either two- or three-dimensional spaces. The method uses a weighted geodesicdistance by an exponential function, taking into account both horizontal and vertical intensity variations. Theweighted geodesic distance is efficiently calculated from an Eikonal equation via the fast sweeping method.The segmentation is then realised by solving an ordinary differential equation with the geodesic distance.The results of the GDM are compared with manually segmented retinal layer boundaries/surfaces. Extensiveexperiments demonstrate that the proposed GDM is robust to complex retinal structures with large curvaturesand irregularities and it outperforms the parametric active contour algorithm as well as the graph theoreticbased approaches for delineating the retinal layers in both healthy and pathological images.Key words: optical coherence tomography segmentation; geodesic distance; Eikonal equation; partial differentialequation; ordinary differential equation; fast sweeping1 IntroductionOptical coherence tomography (OCT) is a powerful imaging modality used to image biological tissues to obtainstructural and molecular information [1]. By using the low coherence interferometry, OCT can provide high-resolution cross-sectional images from backscattering profiles of biological samples. Over the past two decades,OCT has become a well-established imaging modality and widely used by ophthalmologists for diagnosis of retinaland optical nerve diseases. One of the OCT imaging biomarkers for retinal and optical nerve disease diagnosis isthe thickness of the retinal layers. Automated OCT image segmentation is therefore necessary to delineate theretinal boundaries.Since the intensity patterns in OCT images are the result of light absorption and scattering in retinal tissues,OCT images usually contain a significant amount of speckle noise and inhomogeneity, which reduces the imagequality and poses challenges to automated segmentation to identify retinal layer boundaries and other specificretinal features. Retinal layer discontinuities due to shadows cast by the retinal blood vessels, irregular retinalstructures caused by pathologies, motion artefacts and sub-optimal imaging conditions also complicate the OCTimages and cause inaccuracy or failure of automated segmentation algorithms.Over the past two decades a number of automatic and semi-automatic OCT segmentation approaches havebeen proposed. These approaches can be roughly categorised into three families: A-scan based methods, B-scan based methods and volume based methods, as illustrated in Figure 1. A-scan based methods [2–8] detectintensity peak or valley points on the boundaries in each A-scan profile and then form a smooth and continuousboundary by connecting the detected points using model fitting techniques. These methods can be inefficiencyand lack of accuracy. Common approaches for segmenting two-dimensional (2D) B-scans include active contourmethods [9–14], shortest-path based graph search [15, 16] and statistical shape models [17–19] (i.e. active shapeand appearance models [20, 21]). B-scans methods outperform A-scan methods in general. However, they areprone to the intrinsic speckle noise in OCT images and more likely to fail in detection of pathological retinalstructures. Three-dimensional (3D) scan of the retina is now widely used in commercial OCT devices. Existingvolume based segmentation methods mainly use 3D graph based methods [22–30] and pattern recognition [31–35].Benefiting from contextual information represented in the analysis graph, graph based methods provide optimalsolutions and ideal for volumetric data processing. However, the computation can be very complex and slow.Pattern recognition methods normally require training data manually segmented by experts in order to learn afeasible model for classification. These approaches also suffer in accuracy and efficiency. Segmentation of retinallayers in OCT images thereby remains a challenging problem.1   Figure 1: A en-face fundus image (left) with lines overlaid representing the locations of each B-scan within avolumetric OCT data. The red line corresponds to the B-scan in the image (right top). One vertical A-scan ofthe B-scan is shown in the plot (right bottom). The fovea region is characterised by a depression in the centreof the retina surface.In this paper, we propose an algorithm for retinal layer segmentation based on a novel geodesic distanceweighted by an exponential function. As opposed to using a single horizontal gradient as in other works [15,29,30],the exponential function employed in our method integrates both horizontal and vertical gradient informationand can thus account for variations in the both directions. The function plays the role of enhancing the fovealdepression regions and highlighting weak and low contrast boundaries. As a result, the proposed geodesic distancemethod (GDM) is able to segment complex retinal structures with large curvatures and other irregularities causedby pathologies. We compute the weighted geodesic distance via an Eikonal equation using the fast sweepingmethod [36–38]. A retinal layer boundary can then be detected based on the calculated geodesic distance bysolving an ordinary differential equation via a time-dependent gradient descent equation. A local search regionis identified based on the detected boundary to delineate all the nine retinal layer boundaries and overcome thelocal minima problem of the GDM. We evaluate the proposed GDM through extensive numerical experimentsand compare it with state-of-the-art OCT segmentation approaches on both healthy and pathological images.In the following sections, we shall first review the state-of-the-art methods that are to be compared withthe proposed GDM, such as parallel double snakes [14], Chiu’s graph search [15], Dufour’s method [27], andOCTRIMA3D [29, 30]. This will be followed by the details of the proposed GDM, ground-truth validation,numerical experimental results, and comparison of the GDM with the state-of-the-art methods.2 Literature ReviewIn this section, we will provide an overview of the state-of-the-art methods (i.e. parallel double snakes [14], Chiu’smethod [15], OCTRIMA3D [29, 30], Dufour’s method [27]) that will be compared with our proposed GDM inSection 3. For a complete review on the subject, we refer the reader to [39]. Among the four methods reviewed,the first two can only segment B-scans, while the latter two are able to extract retinal surfaces from volumetricOCT data. We note that the term ‘surface’ refers to a set of voxels that fall on the interface between two adjacentretinal layer structures. The retinal layer boundaries to be delineated are shown in Figure 2.Parallel double snakes (PDS): Rossant et al. [14] detected the pathological (retinitis pigmentosa) cellularboundaries in B-scan images by minimising an energy functional that includes two parallel active parametriccontours. Their proposed PDS model consists of a centreline C(s) = (x(s), y(s)) parametrised by s and twoparallel curves C1(s) = C(s) + b(s)n(s) and C2(s) = C(s) − b(s)n(s) with b(s) being a spatially varying half-thickness and n(s) = (nx(s), ny(s)) the normal vector to the the centreline C(s). Specifically, their PDS modelis defined asE(C, C1, C2, b) = EImage(C1) + EImage(C2) + EInt(C) + R (C1, C2, b)(2.1)where the image energy EImage(C1) = − (cid:82) 10 |∇I(C1)|2ds (∇ is the image gradient operator) attracts the para-metric curve C1 towards one of retinal borders of the input B-scan I, whilst EImage(C2) handles curve C2 which(cid:82) 10 |Css (s)|2ds imposes both first and secondis parallel to C1. The internal energy EInt(C) = α2order smooth regularities on the central curve C, with α and β respectively controlling the tension and rigidity(cid:82) 10 |b(cid:48) (C)|2ds is a parallelism constraint imposed on C1 and C2. Nine retinalof this curve. R (C1, C2, b) = ϕ2borders have been delineated by the method, i.e., ILM, RNFLo, IPL-INL, INL-OPL, OPL-ONL, ONL-IS, IS-OS,OS-RPE and RPE-CH.(cid:82) 10 |Cs (s)|2ds+ β2Chiu’s method: Chiu et al. [15] modelled the boundary detection problem in OCT retinal B-scan asdetermining the shortest-path that connects two endpoints in a graph G = (V, E), where V is a set of nodes andE is a set of undirected weights assigned to each pair of two nodes in the graph. Node V corresponds to each2A-scanxzxyzFoveaFoveaB-scanFigure 2: An example cross-sectional B-Scan OCT image centred at the macula, showing nine target intra-retinal layer boundaries detected by the proposed method. The names of these boundaries labelled as notationsB1,B2...B9 are summarised in Table 1. Knowledge of these layer boundaries allows us to calculate the retinallayer thickness, which is imperative for detecting and monitoring ocular diseases.Table 1: Notations for nine retinal boundaries/surfaces, their corresponding names and abbreviationsNotationName of retinal boundary/surfaceAbbreviationB1B2B3B4B5B6B7B8B9internal limiting membraneouter boundary of the retinal nerve fibre layerinner plexiform layer-inner nuclear layerinner nuclear layer-outer plexiform layerouter plexiform layer-outer nuclear layerouter nuclear layer-inner segments of photoreceptorsinner segments of photoreceptors-outer segments of photoreceptorsouter segments of of photoreceptors-retinal pigment epitheliumretinal pigment epithelium-choroidILMRNFLoIPL-INLINL-OPLOPL-ONLONL-ISIS-OSOS-RPERPE-CHpixel in the B-scan image, whilst weight E is calculated from the intensity gradient of the image in its verticaldirection. Each node is connected with its eight nearest neighbours and all other node pairs are disconnected,resulting in a sparse adjacency matrix of graph weights of vertical intensity variation. For example, an M × Nsized image has an M N × M N sized adjacency matrix with 8M N non-zero filled entries. Mathematically, theweights between two nodes used in their method are calculated based on the pure vertical gradient value, definedas(cid:26) 2 − (ga + gb) + wminw (a, b) =0√if |a − b| ≤otherwise2(2.2)where g is the vertical gradient of a B-scan image; a and b denote receptively two separate nodes in V andwmin is a small positive value added to stabilise the system. The most prominent boundary is then detected asthe minimum weighed path from the first to the last vertex in V using Dijkstra’s Algorithm. A similar regionrefinement technique to Section 3.4 was used to detect seven retinal boundaries, i.e., ILM, RNFLo, IPL-INL,INL-OPL, OPL-ONL, IS-OS and RPE-CH.Dufour’s method: Dufour et al. [27] proposed a modification of optimal graph search approach [40] tosegment retinal surfaces in OCT volume data. By using soft constraints and adding prior knowledge learnedfrom a model, they improve the accuracy and robustness of the original framework. Specifically, their Markovrandom field based model is given byE (S) =n(cid:88)i=1(Eboundary (Si) + Esmooth (Si)) +n−1(cid:88)n(cid:88)i=1j=i+1Einter (Si, Sj)where S is a set of surfaces S1 to Sn. The external boundary energy Eboundary (Si) is computed from the input3D image data. The surface smoothness energy Esmooth (Si) guarantees the connectivity of a surface in 3Dand regularises the surface. The interaction energy Einter (Si, Sj) integrates soft constraints that can regularisethe distances between two simultaneously segmented surfaces. This model is then built from training datasetsconsisting of fovea-centered OCT slice stacks. Their algorithm is capable to segment six retinal surfaces (n = 6 inabove formulation) in both healthy and macular edema subjects, i.e., ILM, RNFLo, IPL-INL, OPL-ONL, IS-OSand RPE-CH.OCTRIMA3D: Tian et al. [29, 30] proposed a real-time automatic segmentation of OCT volume data.The segmentation was done frame-by-frame in each 2D B-Scan by considering the spatial dependency betweeneach two adjacent frames. Their work is based on Chiu’s graph search framework [15] for B-Scan OCT images.However, in addition to Chiu’s work they introduce the inter-frame flattening to reduce the curvature in thefovea region and thus the accuracy of their algorithm has been improved. Moreover, they apply inter-frame or3intra-frame information to limit the search region in current or adjacent frame so that the computational speedof their algorithm can be increased. Furthermore, the biasing and masking techniques are developed so as tobetter attain retinal boundaries within the same search region. A totally eight retinal surfaces, i.e., ILM, RNFLo,IPL-INL, INL-OPL, OPL-ONL, IS-OS, OS-RPE and RPE-CH, can be delineated by the method. To sum up,Table 2 reports the retinal boundaries/surfaces segmented by the four methods as well as our GDM proposed inthe next section.Table 2: Target boundaries/surfaces of the five methods compared in this paper (check mark means the bound-ary/surface can be segmented, while cross mark means the boundary/surface cannot be segmented).MethodOS-RPE (B8) RPE-CH (B9)PDS [14]Chiu’s method [15]Dufour’s method [27]OCTRIMA3D [29, 30]GDMILM (B1)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)RNFLo (B2)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)IPL-INL (B3)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)INL-OPL (B4) OPL-ONL (B5) ONL-IS (B6)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)×××(cid:88)(cid:88)(cid:88)×(cid:88)(cid:88)IS-OS (B7)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)××(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)3 The Proposed Geodesic Distance Method (GDM)In this section, we propose a novel framework using the geodesic distance to detect from OCT images nine retinallayer boundaries defined in Figure 2 and Table 1. As the proposed methodology applies equally to both 2D and3D segmentation, we will illustrate the approach for 2D segmentation here, as the steps would be the same for3D segmentation. Numerical implementation of the approach is given in Appendix.3.1 Geodesic distanceWe use geodesic distance to identify the pixels on the boundaries of retinal layers in OCT images. The geodesicdistance d is the smallest integral of a weight function W over all possible paths from two endpoints (i.e. s1and s2). The weight function determines how the path goes from s1 to s2. Small weight at one point indicatesthat the path has high possibility of passing that point. Specifically, the weighted geodesic distance between twopixels/endpoints s1 and s2 is given asD (s1, s2) = minC(cid:90) 10W −1 (C (s)) ds(3.1)where C (s) is the set of all the paths that link s1 to s2, and the path length is normalised and the start andend locations are C(0) = s1 and C(1) = s2, respectively. The infinitesimal contour length ds is weighted by anon-negative function W (C (s)). This minimisation problem can be interpreted as finding a geodesic curve (i.e.a path with the smallest weighted length) in a Riemannian space. In geometrical optics, it has proven that thesolution of (3.1) satisfies the Eikonal equation (3.3).The retinal layers of OCT images are normally near horizontal. The gradient in the vertical direction thuscan be considered as a good candidate for computing weight W in (3.1). For instance, each of the two prominentboundaries, e.g. ILM (B1) and IS-OS (B7) in Figure 3 (a) and (e), is at the border of a dark layer above a brightlayer. As a result, pixels in the region around the two boundaries will have high gradient values, as shown inFigure 3 (b) and (f). As the retinal layers at each side of the boundary are either transiting from dark to brightor bright to dark, the non-negative weight function W in this paper is defined based on intensity variation asfollowsW (x) =(cid:26) 1 − exp (−λ (1 − n (∇xI)) n (|∇yI|)) dark-to-brightbright-to-darkexp (−λ (1 − n (∇xI)) n (|∇yI|))(3.2)where I is an input OCT image; n (·) is a linear stretch operator used to normalise values to between 0 and 1;exp is the exponential function and λ is a user-define parameter, together they enhance the foveal depressionregions and highlight the weak retinal boundaries [41]; and ∇x and ∇y are the first-order gradient operatoralong x (vertical) and y (horizontal) direction, respectively. The two gradient operators are discretised using acentral finite difference scheme under the Neumann boundary condition. (3.2) also includes the positive horizontalgradient information n (|∇yI|), without which only vertical direction is accounted for and it is thus only applicableto flat retinal boundaries. Consequently, our proposed method is robust against curved features (e.g. the centralregion of the fovea) as well as other irregularities (e.g. bumps or large variations of boundary locations) causedby pathologies. In other words, the proposed method with the weight W defined in (3.2) can deal with bothnormal and pathological images, as illustrated in Figure 3 as well as in the experimental section.4Figure 3: Illustrating the effectiveness of the weight W defined in (3.2). (a) and (e): normal B-scan OCT dataand pathological B-scan from an eye with dry age-related macular degeneration (drye-AMD); (b) and (f): verticaldark-to-bright gradient maps of (a) and (e), respectively; (c) and (g): dark-to-bright gradient maps calculatedusing equation (3.2) with λ = 1. Note the gradient values of pixels have been enhanced in the region with strongcurvature and big bumps; (d) and (h): boundary detection results via the method described in Section 3.3 usingdifferent gradient maps. Yellow lines are computed using (b) and (f), whilst red lines using (c) and (g).3.2 Selection of endpoints s1 and s2For fully automated segmentation, it is essential to find a way to initialise the two endpoints s1 and s2 automati-cally. Since the retinal boundaries in the OCT images used in this paper run across the entire width of the image,we add an additional column on each side to the gradient map computed from (3.2). As the the minimal weightedpath is sought after, a weight Wmax larger than any of the non-negative weights calculated from (3.2) is thereforeassigned to each of the newly added vertical columns (note that we use W −1 for the geodesic distance 3.1, theminimal weighted path thereby prefers large weights). This forces the path traversal in the same direction as thenewly added vertical columns with maximal weights, and also allows the start and end points to be arbitrarilyassigned in the two columns. Once the retinal layer boundary is detected, the two additional columns can beremoved. Figure 4 shows two examples of endpoint initialisation.Figure 4: Two segmentation examples using different automatic endpoints initialisations on a drak-to-brightgradient map. s1 and s2 are start and end points, respectively.3.3 Eikonal equation and minimal weighted pathThe solution of (3.1) can be obtained by solving the Eikonal equation after the endpoints are determined.Specifically, over a continuous domain, the distance map D(x) to the seed start point s1 is the unique solutionof the following Eikonal equation in the viscosity sense|∇D (x)| = W −1 (x) , ∀x /∈ s1(3.3)with D (s1) = 0. The equation is a first order partial differential equation and its solution can be found via theclassical fast marching algorithm [42,43] using an upwind finite difference approximation with the computationalcomplexity O(M N log(M N )) (MN is the total number of grid points). Recently, the fast sweeping algorithm[36, 37] has been proposed. This technique is based on a pre-defined sweep strategy, replacing the heap priorityqueue to find the next point to process, and thereby has the linear complexity of O(M N ). In this paper, weapply fast sweep for (3.3) and its detailed 3D implementation has been given in Appendix. Figure 5 shows twodistance maps calculated using the dark-to-bright weight defined in (3.2) and two different start points as shownin the examples in Figure 4.5(ܽ)(ܾ)(ܿ)(݀)(݁)(݂)(݃)(ℎ)ݏଵݏଶݏଵݏଶOnce the geodesic distance map to the start point s1 has been computed, the minimal weighted path (geodesiccurve) between point s1 and s2 can be extracted from the following ordinary differential equation through thetime-dependent gradient descentγ(cid:48) (t) = −ηt∇D (γ (t)) , γ (0) = s2(3.4)where ηt > 0 controls the parametrisation speed of the resulting curve. To obtain unit speed parametrisation,we use ηt = |∇D (γ (t))|−1ε . Since distance map D is nonsmooth at point s1, a small positive constant ε is addedto avoid dividing by zero. Note the point s1 is guaranteed to be found from this ordinary differential equationbecause the distance field is monotonically increasing from s1 to s2, which can be observed in Figure 5. Thistechnique can achieve sub-pixel accuracy for the geodesic path even if the grid is discrete.Figure 5: Two distance maps calculated using the dark-to-bright weight W −1 and two different start points inthe two exmaples in Figure 4, respectively. The distance values are expanded to [0, 800] for better visualisation.The geodesic curve is then numerically computed using a discretised gradient descent, which defines a discretecurve γk usingγk+1 = γk − τ G (cid:0)γk(cid:1)where γk is a discrete approximation of γ(t) at time t = kτ , and the time step size τ > 0 should be small enough.ε parametrised by the arc length. Once γk+1 reachesG (x) is the normalised gradient ∇D (γ (t))s1, one of the retinal boundaries can be found. The following Algorithm 1 concludes the proposed geodesicdistance algorithm for extracting one retinal boarder in OCT images.|∇D (γ (t))|−1(3.5)(cid:46)Algorithm 1: the proposed GDM for one retinal boundary detection1: Input OCT data I (i.e. B-scan or volume)2: calculate dark-to-bright or bright-to-dark weight W using (3.2)3: pad two new columns to the weight and assign large values to them4: select two endpoints s1 and s2 on the two newly padded columns5: calculate distance map D in (3.3) using fast sweeping algorithm6: find one retinal layer boundary γ using the gradient descent flow (3.5)7: remove the additional columns in the edge detection result3.4 Detection of nine retinal layer boundariesWe have introduced how the proposed geodesic distance algorithm (3.1) can find the minimal weighted pathacross the whole width of the OCT image for one retinal layer boundary. In this section, we shall describe theimplementation details of the proposed approach to delineate nine retinal layer boundaries as shown in Figure 2and Table 1. Since the proposed model (3.1) is not convex, its solution can easily get stuck in local optima. Forexample, Figure 3 (c) and (g) have high gradient values in the region around both the ILM and IS-OS boundaries.However, in Figure 3 (d) the algorithm detected the ILM boundary while in Figure 3 (h) it detected the IS-OS.In order to eliminate such uncertainty, we dynamically define the search region based on the detected boundaries.The following section describes the proposed method in detail.3.4.1 Detection of the IS-OS boundaryThe intensity variation between two layers divided by the IS-OS (B7) border are normally the most prominentin OCT B-scans. However, due to the fact that OCT images are always corrupted by speckle noise as a resultof light absorption and scattering in the retinal tissue, it is not always the case. For example, the intensityvariation around the IML (B1) border sometimes can be more obvious than that around the IS-OS, as shown6ݏଵݏଶݏଵݏଶin the gradient image Figure 3 (c). To make sure the segmentation of the IS-OS boundary we first enhance theIS-OS via a simple local adaptive thresholding approach1, which is given as followsp =(cid:26) 01ls (I, ws) − I > Cotherwise(3.6)where I is the input OCT image, and ls (p, ws) means that I is convolved with a suitable operator, i.e. the meanor median filter. ws is the window size of the filter and C a user-defined threshold value. In the paper, we usethe mean filter with the window size ws = 100 and set C = 0.01. The enhanced image can be then obtained bymultiplying the original image I with p. The first two images in Figure 6 illustrates that the contrast of the IS-OSboarder has been enhanced and the most obvious intensity variation now takes place around the IS-OS boundary.The IS-OS boundary then is detected on the dark-to-bright gradient image. Consequently, the delineated line isguaranteed to pass the IS-OS in both cases, as shown in the last two images in Figure 6.Figure 6: Detecting the IS-OS boarders in the normal and pathological subjects after image enhancement via alocal adaptive thresholding method (3.6).3.4.2 Detection of the RPE-CH, OS-RPE and ONL-IS boundariesOnce the IS-OS (B7) is segmented, it can be used as a reference to limit the search region for segmenting theRPE-CH (B9), OS-RPE (B8) and ONL-IS (B6) boundaries. RPE-CH and OS-RPE are below the IS-OS and theyare delineated in the following way: the RPE-CH can be extracted by applying the geodesic distance algorithmwith the bright-to-dark gradient weights (3.2) obtained from the region pixels below the detected IS-OS (i.e. thebright-to-dark gradient weights are set to zeros above the IS-OS); the OS-RPE is then delineated on the bright-to-dark gradient map in the region between the detected IS-OS and RPE-CH (i.e. the bright-to-dark gradientweights are set to zeros outside of the region between the IS-OS and RPE-CH). The dark-to-bright ONL-IS isabove the IS-OS. The search region can be constructed between the IS-OS boundary and a parallel line aboveit with a diameter of 15 pixels. The dark-to-bright gradient weights outside of the region are then set to zeros.Hence, the only boundary in the search region of the dark-to-bright gradient image is the ONL-IS which can beextracted.3.4.3 Detection of the ILM and INL-OPL boundariesBoth the ILM (B1) and INL-OPL (B4) are at the border of a darker layer above a bright layer. The intensityvariation around the IML boundary is much more prominent and thus the IML is segmented first. The detectedONL-IS (B6) edge is taken as a reference and the dark-to-bright gradient weights below the ONL-IS is set tozeros. The ILM can then be obtained via the proposed method. The INL-OPL can then be easily detected onthe dark-to-bright gradient map by simply limiting the search region between the ILM and ONL-IS (i.e. thedark-to-bright gradient values are set to zeros outside of the region between the ILM and ONL-IS).3.4.4 Detection of the OPL-ONL, IPL-INL and RNFLo boundariesThe OPL-ONL (B5), IPL-INL (B3) and RNFLo (B2) demonstrate a bright layer above a darker layer and thuscan be detected on the bight-to-dark gradient map defined in (3.2). The segmented INL-OPL (B4) and ONL-IS(B6) are taken as two reference boundaries, and the OPL-ONL edge can be found by limiting the search regionbetween the INL-OPL and ONL-IS. The search region for the IPL-INL can be then constructed between theINL-OPL boundary and a parallel line above it with a diameter of 20 pixels. The IPL-INL can be located on abright-to-dark gradient map which is set to zeros outside of the search region constructed. Finally, the RNFLo(B2) can be found in the search region between the two reference boundaries IPL-INL and IML (B1). However,because the IPL-INL and IML boundaries are very close to each other in the central region of the fovea, thesearch region for the RNFLo are sometimes missing around the fovea region. This leads to segmentation errorsof the RNFLo, as shown in Figure 7 (a). These errors however can be avoided by simply removing the spuriouspoints detected on the RNFLo in the region above the IML, as shown in Figure 7 (b). The proposed methodsfor segmenting nine retinal layer boundaries can be summarised in the flow chart as shown in Figure 8.1http://homepages.inf.ed.ac.uk/rbf/HIPR2/adpthrsh.htm7Figure 7: The segmentation results of the nine retinal layer boundaries on both normal and dye-AMD pathologicalB-scans, as shown in (a) and (c). The detection of the RNFLo boundary however shows errors due to the absenceof a search region for this boundary in (a). (b) shows that these errors have been corrected.Figure 8: The overview of the proposed framework for dynamically delineating nine retinal layer boundariesdefined in Figure 2 and Table 1. Section 3.4 describes this flow chart in detail.4 Experiment SetupTo evaluate the performance of the proposed GDM qualitatively and quantitatively, numerical experiments areconducted to compare it with the state-of-the-art approaches reviewed in Section 2 on both healthy and patho-logical OCT retinal images. As the GDM is able to segment both 2D and 3D OCT images, we perform numericalexperiments on both B-scan and volumetric OCT data. An anisotropic total variation [44] is used to reduce noiseprior to determining the layers boundaries/surfaces for all segmentation methods. In the following, we introducethe detailed procedure of OCT data acquisition, the evaluation metrics used to quantify the segmentation results,the final numerical results, and the computational complexity of different methods.4.1 Clinical Data30 Spectralis SDOCT (ENVISU C class 2300, Bioptigen, axial resolution = 3.3m, scan depth = 3.4mm, 32, 000A-scans per second) B-scans from 15 healthy adults (mean age = 39.8 years, SD = 8.6 years; 7 male, 8 female)were used for the research. All the data was collected after informed consent was obtained and the study adheredto the tenets of the Declaration of Helsinki and Ethics Committee approval was granted.2D B-scan data: The normal vivo B-scan OCT data was imaged from the left and right eye of 15 healthy8(ܽ)(ܾ)(ܿ)IMLIPL-INLINL-OPLONL-ISOS-RPERPE-CHOPL-ONLIS-OSRNFLoInput OCT DataLocal adaptive thresholding (2.6)Detect IS-OS (B7) border using Algorithm 1Detect RPE-CH (B9) border blew IS-OS (B7)Detect ONL-IS (B6) border in the search region constructed above IS-OS (B7)Detect OS-RPE (B8) border between IS-OS (B7) and RPE-CH (B9) Detect ILM (B1) border above ONL-IS (B6)Detect INL-OPL (B4) border between ILM (B1) and ONL-IS (B6)Detect OPL-ONL (B5) border between INL-OPL (B4) and ONL-IS (B6)Detect IPL-INL (B3) border in the search region constructed above INL-OPL (B4)Detect RNFLo (B2) border between ILM (B1) and IPL-INL (B3) Output 9 retinal bordersadults using a spectral domain OCT device with a chin rest to stabilise the head. The B-scan located at thefoveal centre was identified from the lowest point in the foveal pit where the cone outer segments were elongated(indicating cone specialisation). To reduce the speckle noise and enhance the image contrast, every B-scan wasthe average of aligned images scanned at the same position. In addition to the 30 OCT images from the healthysubjects, another 20 B-scans from subjects with pathologies are also used to compare the proposed GDM withother approaches in pathological cases. These B-scans are from an eye with dry age-related macular degeneration(drye-AMD), which is available from Dufour’s software package’s website2. The accuracy of segmentation resultsobtained by the three automated 2D methods (i.e. PDS, Chiu’s method and GDM) over these healthy andpathological B-scans is evaluated using the ground truth datasets, which were manually delineated with extremecarefulness by one observer.3D Volume data: 10 Spectralis SD-OCT (Heidelberg Engineering GmbH, Heidelberg, Germany) volumedata sets from 10 healthy adult subjects are used in this study. Each volume contains 10 B-scans, and the OCTA-scans outside the 6mm × 6mm (lateral × azimuth) area and centred at the fovea were cropped to removelow signal regions. All volumetric data can be downloaded from [29], where also contains the results of theOCTRMA3D, and the manual labellings from two graders. In this study we choose the manual labelling ofgrader 1 as the 3D ground truth.4.2 Evaluation MetricsPerformance metrics are defined to demonstrate the effectiveness of the proposed method and compare it withthe existing methods. Three commonly used measures of success for OCT boundary detection are signed error(SE), absolute error (AE) and Hausdorff distance (HD). Among them, SE indicates the bias and variability ofthe detection results. AE is the absolute difference between the automatic detection results and ground truth,while HD measures the distance between the farthest point of a set to the nearest point of the other and viceversa. Specifically, these metrics are denoted as(cid:16)SE(cid:16)AE(cid:17)Bi, ˜Bi(cid:17)Bi, ˜Bi(cid:18)(cid:26)= 1n= 1nn(cid:80)j=1n(cid:80)j=1(cid:16)Bij − ˜Bij(cid:17)(cid:16)(cid:12)(cid:12)Bij − ˜Bij(cid:12)(cid:27)(cid:17)(cid:12)(cid:12)(cid:12)(cid:26)(cid:27)(cid:19)(cid:16)Bi, ˜Bi(cid:17)HD= maxmaxx∈Biminy∈ ˜Bi(cid:107)x − y(cid:107), maxx∈ ˜Biminy∈Bi(cid:107)x − y(cid:107)where Bi and ˜Bi are respectively the detected boundaries and ground truth boundaries (i.e. manual labellings).n is the number of pixels/volexs that fall on the retinal boundary/surface. Statistically, when the SE value isclose to zero, the difference between Bi and ˜Bi is small.In this case, the detection result is less bias. Themeasurements of AE and HD (varies from 0 to ∞ theoretically) signify the difference between two boundaries,e.g., 0 indicates that both retinal structures share exactly the same boundaries, and larger AE and HD valuesmean larger distances between the measured boundaries. We also monitor the overall SE (OSE), AE (OAE) andHD (OHD) during all the experiments. They are defined asOSE =OAE =OHD =1s1s1ss(cid:88)i=1s(cid:88)i=1s(cid:88)i=1(cid:16)SEBi, ˜Bi(cid:17)(cid:16)Bi, ˜Bi(cid:17)AE(cid:16)Bi, ˜Bi(cid:17)HDwhere s is the total number of retina boundaries one method can delineate.4.3 Parameter SelectionThere are five parameters in the PDS model: three smooth parameters α, β, ϕ and two time step sizes γC andγb used within the gradient descent equations to minimise the functional (2.1) with respect to C and b. In thisstudy we use α = 10, β = 0, ϕ = 700, γC = 10 and γb ≥ 2 suggested in [14]. In addition, as the PDS is anonconvex model and its segmentation results depend on initialisation. We initialise the parallel curves veryclosely to the true retinal boundaries for fair comparison with other methods. A maximal number of iterationsnumber 500 is used to ensure convergence of the PDS model. The graph theoretic based methods, i.e., Chiu’smethod, OCTRIMA3D and Dufour’s method, require no parameter input. Finally, our GDM has two build-inparameters: λ in (3.2) and τ in (3.5). We set λ = 10 and τ = 0.8 to detect the retinal layers in the OCT images.2http://pascaldufour.net/Research/software data.html94.4 Numerical ResultsWe first visually compare the segmentation results of the proposed GDM method, the PDS (2.1) and Chiu’sgraph search method on both the healthy and pathological B-scans, which are shown in Figure 9 (a)-(d). ThePDS results as shown in (e)-(h) have some errors on some of the boundaries detected. For instance, the B1 andB2 cannot converge to the true retinal boundaries around the central fovea region, as shown in (f) and (h). Thisis because the PDS is the classical snake-driven model which has difficulty handling boundary concavity problem.Moreover, due to the fact that the B7 has a much stronger image gradient than the B6 and B8, some parts ofthese two boundaries have been mistakenly attracted to the B7. As Chiu’s graph search method only considersthe intensity changes in the pure vertical direction (2.2), it also fails segment the fovea region layers with strongcurvature, as shown in (i)-(l). Moreover, the algorithm cannot handle irregular bumps caused by pathologiesvery well, which can be observed from the bottom B9 line delineated in (k) and (l). In general, Chiu’s methodworks very nicely when the retinal structures are flat or smooth without big changes on boundary locations. Theresults by the proposed GDM method, as shown in (m)-(p), performs better than the PDS and Chiu’s methodswhen compared with the ground truth in the last row. As analysed in Section 3, the gradient weights definedin (3.2) account for both vertical and horizontal variations, making it very suitable for both flat and nonflatretinal structures. Hence, the GDM is a better clinical tool for detecting retinal boundaries from both normal orpathological subjects.Figure 9: Comparison of different segmentation methods on healthy and pathological 2D OCT B-scans. 1st row:healthy (i.e. first two) and pathological (i.e.last two) B-scans; 2nd row: results by the PDS model (2.1); 3rdrow: results by Chiu’s method; 4th row: results by the proposed GDM; 5th row: ground truth.10(a)(b)(c)(d)(e)(f)(g)(h)(i)(j)(k)(l)(m)(n)(o)(p)(q)(r)(s)(t)The accuracy of the segmentation results by different methods against ground truth over 30 healthy and 20pathological B-scans is indicated in Table 3 and Table 4, respectively. In order to make the comparison clearer,we plot the data in the two tables in Figure 10 and Figure 11, respectively.In Table 3 and Figure 10, the SE shows that the PDS leads to very large segmentation bias with the largesterror being 6.01µm, whilst the bias of the GDM is less than 1.22µm for all the retinal layer boundaries. Moreover,the mean SE plot of the GDM is close to zero, which means the GDM are less biased than the other two methods.Large errors of the PDS normally take place at the B1, B2, B6 and B8, which is consistent with visual inspectionon the healthy scans in Figure 9. Furthermore, the mean AE quantities and plots show that the GDM performsthe best for all the boundaries. Particularly at the B1 and B2 where the curved fovea region is located, the HDvalues of the GDM (3.702±1.62µm, 7.340±2.16µm) are significantly lower than those of the PDS (36.56±15.9µm,29.00±11.6µm) and Chiu’s method (22.12±9.23µm, 21.25±5.98µm). However, the accuracy of different methodsare comparable at flat or smooth retinal boundaries such as B4, B7 and B9. Finally, as the manual segmentationtraces the small bumps of the true boundaries but the segmentation results by the PDS are however very smooth,the overall accuracy of the method is the lowest among all the approaches compared.Table 3: Mean and standard deviation of SE (µm), AE (µm) and HD (µm) calculated using the results of differentmethods (the PDS, Chiu’ method and GDM) and the ground truth manual segmentation, over 30 healthy OCTB-scans.BoundaryPDSSE (µm)Chiu et al. GDMPDSAE (µm)Chiu et al. GDMPDSHD (µm)Chiu et al. GDM-3.92±1.90 -1.22±0.68 0.273±0.33 4.615±2.03 2.605±1.12 0.924±0.26 36.56±15.9 22.12±9.23 3.702±1.62ILM (B1)-2.57±1.38 -1.67±1.34 -0.53±0.37 3.864±1.49 2.676±0.82 1.262±0.34 29.00±11.6 21.25±5.98 7.340±2.16RNFLo (B2)IPL-INL (B3)-0.55±0.83 -1.04±1.21 -0.38±0.61 1.876±0.60 2.020±0.79 1.314±0.32 8.619±3.77 10.53±5.25 7.258±1.92INL-OPL (B4) 0.012±0.58 -0.90±0.61 -0.71±0.71 1.708±0.39 1.699±0.40 1.807±0.51 6.772±2.53 7.036±2.84 7.505±2.96OPL-ONL (B5) -0.23±1.29 -1.51±1.30 -1.12±1.17 2.127±1.00 2.133±1.05 1.949±0.94 10.22±3.70 9.044±3.48 7.463±3.246.010±0.83ONL-IS (B6)4.630±1.05—-0.09±0.61 0.194±0.49 0.291±0.63 0.823±0.29 0.720±0.25 0.771±0.36 3.676±1.63 3.240±1.60 2.611±0.74IS-OS (B7)3.601±0.96—5.202±2.25OS-RPE (B8)-0.31±0.79 -0.84±0.58 -0.74±0.69 1.291±0.25 1.228±0.47 1.213±0.45 4.237±1.47 4.027±1.31 3.831±1.08RPE-CH (B9)0.394±0.39 -1.00±0.54 -0.49±0.23 3.103±0.74 1.869±0.59 1.305±0.32 13.11±4.25 11.04±3.75 5.327±1.11Overall1.376±0.36 9.969±1.581.125±0.36 8.913±2.28-0.78±0.47 5.570±1.76-0.73±0.49 6.055±0.86————Mean valueStandard deviationFigure 10: Plots of mean and standard derivation obtained by different methods in Table 3 for healthy B-scans.The 1st and 2nd rows respectively denote the mean and standard derivation of the SE (µm), AE (µm) and HD(µm) for segmentation of boundary B1 to B9 using the PDS, Chiu’s mehtod and GDM. The overall value is theaverage result over all boundaries.In Table 4 and Figure 11, the mean and standard deviation plots show that the GDM is more accurate androbust compared with the other two methods for pathological data. However, larger errors have been found at11Target Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallSigned Error (SE)-4-202468PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallAbsolute Error (AE)01234567PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallHausdorff Distance (HD)0510152025303540PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallSigned Error (SE)00.511.522.5PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallAbsolute Error (AE)00.511.522.5PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallHausdorff Distance (HD)0246810121416PDSChiu et al.GDMthe last four boundaries B6, B7, B8 and B9 for all the segmentation methods. This is because the dry age-relatedmacular degeneration has led irregularities to these retinal boundaries, making these methods less accurate androbust. The overall accuracy measured by the three quantities has decreased compared with the correspondingmeasurements listed in Table 3. Chiu’s graph search method using Dijkstra’s algorithm can be deemed as adiscrete approximation of the proposed GDM. This makes its final results comparable to those of the GDM atsome flat retinal boundaries and better than those of the PDS. However, the fast sweeping algorithm used tosolve the Eikonal equation guarantees local resolution for the geodesic distance, which significantly reduces thegrid bias and achieves sub-pixel accuracy for the geodesic path of the GDM. In addition to the novel weightfunction proposed in (3.2), the GDM also resolves the metrisation problem caused by discrete graph method andthus can obtain more accurate results than Chiu’s method for delineating cellular layers from both normal orpathological subjects.Table 4: Mean and standard deviation of SE (µm), AE (µm) and HD (µm) calculated using the results ofdifferent methods (the PDS, Chiu’s method and GDM) and the ground truth manual segmentation, over 20pathological OCT B-scans.BoundaryPDSSE (µm)Chiu et al. GDMPDSAE (µm)Chiu et al. GDMPDSHD (µm)Chiu et al. GDM-0.41±0.59 -0.34±0.25 -0.36±0.29 0.932±0.44 0.796±0.17 0.683±0.09 6.461±4.86 4.087±1.01 3.337±1.10ILM (B1)-0.93±0.93 -0.38±0.33 -0.49±0.50 1.792±0.63 1.717±0.53 1.257±0.32 6.145±1.84 8.464±4.55 6.109±2.49RNFLo (B2)-0.23±0.62 -0.22±0.27 -0.32±0.32 1.228±0.21 1.149±0.20 0.926±0.16 7.640±1.31 5.857±0.98 5.151±1.82IPL-INL (B3)INL-OPL (B4) 0.578±0.64 0.555±0.39 0.392±0.26 1.546±0.28 1.563±0.30 1.419±0.16 7.165±1.07 8.194±1.36 5.942±1.32OPL-ONL (B5) -0.04±1.08 0.286±0.55 -0.07±0.64 2.371±0.76 2.255±0.60 2.019±0.65 11.28±1.95 9.858±2.76 9.281±2.253.339±1.226.205±1.01—ONL-IS (B6)-0.23±0.86 1.030±1.06 0.350±0.50 2.415±1.25 2.399±1.05 1.055±0.22 15.95±10.2 17.66±11.3 6.795±4.65IS-OS (B7)OS-RPE (B8)9.673±1.30—2.371±4.17RPE-CH (B9) 3.315±2.59 3.011±2.98 0.027±0.35 4.797±2.59 5.146±2.70 2.252±0.46 31.23±12.9 32.63±13.2 13.19±3.500.863±0.59 0.563±0.44 -0.11±0.22 2.832±0.83 2.146±0.70 1.430±0.20 13.75±4.72 12.39±4.06 7.300±0.67Overall1.442±0.34 15.23±4.030.028±0.41 5.927±2.341.821±0.47 22.63±12.9-0.57±0.72 4.484±0.50————Mean valueStandard deviationFigure 11: Plots of mean and standard derivation obtained by different methods in Table 4 for pathologicalB-scans. The 1st and 2nd rows respectively denote the mean and standard derivation of the SE (µm), AE (µm)and HD (µm) for segmentation of boundary B1 to B9 using the PDS, Chiu’s mehtod and GDM. The overallvalue is the average result over all boundaries.In the next section, the proposed GDM is used to segment the OCT volume dataset that includes samplesfrom ten healthy adult subjects, named as Volume 1 to 10 respectively. Dufour’s and OCTRIMA3D methods arealso used to segment the same dataset for comparison purposes. In Figure 12, we demonstrate four representativesegmentation results of GDM on Volume 1, 2, 7 and 9.12Target Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallSigned Error (SE)-1-0.500.511.522.533.5PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallAbsolute Error (AE)0123456PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallHausdorff Distance (HD)05101520253035PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallSigned Error (SE)00.511.522.533.544.5PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallAbsolute Error (AE)00.511.522.53PDSChiu et al.GDMTarget Intra-retinal Layer BoundaryB1B2B3B4B5B6B7B8B9OverallHausdorff Distance (HD)02468101214PDSChiu et al.GDMFigure 12: 3D rendered images of human in vivo intra-retinal layer surfaces obtained through segmenting Spec-tralis SD-OCT volumes with the proposed GDM method. Samples are named Volume 1, Volume 2, Volume 7and Volume 9. The color used for each individual retinal surface is the same as in Figure 2.Figure 13: Two B-scans extracted from the Volume 4 sample. The left shows the en-face representation of theOCT scan with two lines (green and red) overlaid representing the corresponding locations of two B-scans withina volume present in the right.Figure 14: The comparison between Dufour’s method (left), OCTRIMA3D (middle) and GDM (right) on thetwo B-scans in Figure 13. The segmentation results by these methods are marked with red lines while the groundtruth using manual labelling with green lines.13xzxzxyThe segmentation results of the three approaches on an exemplary sample (Volume 4) are shown in twodistinctive B-scans in Figure 13 and 14, where one B-scan retinal structures are quite flat and the other containsthe nonflat fovea region. Dufour’s method has lower accuracy than the OCTIMA3D and GDM for both cases.OCTRIMA3D extends Chiu’s method to 3D space and improves it by reducing the curvature in the fovea regionusing the inter-frame flattening technique, so the method performs very well for both flat and nonflat retinalstructures. However, there still exist some obvious errors on the 5th boundary B5. OCTRIMA3D is able toflatten the B1 and in the meanwhile it also increases the curvature of its adjacent boundaries such as B5, whichmight be the reason leading to the errors. Compared with the other two, the GDM’s results show less green lines,verifying that the results are closer to ground truth and thus it is the most accurate among the three compared.In addition to the 2D visualisation, the 3D rendering of the results segmented by the three approaches is given inFigure 15. The experiment furthermore shows that Dufour’s results deviate much from ground truth, while theOCTRIMA3D is better than Dufour’s method and comparable to the GDM. The GDM results cover less greyground truth and are thereby the best.(a)(b)(c)(d)(e)(f)(g)Figure 15: The 3D comparison between Dufour’s method, OCTRIMA3D and GDM by segmenting the intra-retinal layer surfaces from the Volumes 4 sample. Column (a)-(d) are respectively Dufour’s results, OCTRIMA3Dresults, GDM results and ground truth. Column (e)-(g) are respectively the segmentation results of the threecompared methods, overlaid with ground truth. Row 1-6 are the results for the individual surface B1, B2, B3,B5, B7 and total retina surfaces, respectively.Table 5: The SE (µm), AE (µm) and HD (µm) calculated using the results of different methods (Dufour’smethod, OCTRMIA3D and GDM) and the ground truth manual segmentation, for the OPL-ONL (B5) surfacein each of the 10 OCT volumes.Volume # Dufour et al. OCTRIMA3DGDMDufour et al. OCTRIMA3DGDMDufour et al. OCTRIMA3DGDMSE (µm)AE (µm)HD (µm)12345678910-1.194-2.170-2.576-2.296-1.680-2.623-2.326-0.636-4.206-2.6480.4559-0.0360.41821.09871.32881.07320.52941.13550.30770.67010.3782-0.1280.59830.67740.59090.29740.45290.68330.08590.26062.38164.52503.61293.81854.33274.06823.15062.39554.58134.49031.34900.90891.32371.51751.50121.48380.93781.44551.07801.0627141.07200.78141.09891.07530.90050.94930.74331.00690.76780.787725.68856.66725.20351.52256.22343.07031.78225.48143.22341.01715.27311.57016.71918.36411.88919.2018.670117.9308.969411.66610.4497.09389.53269.61518.84199.52816.480311.6855.719110.961Table 6: The SE (µm), AE (µm) and HD (µm) calculated using the results of different methods (Dufour’smethod, OCTRMIA3D and GDM) and the ground truth manual segmentation, for the IS-OS (B7) surface ineach of the 10 OCT volumesSE (µm)HD (µm)AE (µm)Volume # Dufour et al. OCTRIMA3DGDMDufour et al. OCTRIMA3DGDMDufour et al. OCTRIMA3DGDM12345678910-0.4320.7476-0.3110.36520.60570.9825-1.247-0.311-0.755-0.099-0.148-0.276-0.291-0.116-0.098-0.592-0.536-0.069-0.111-0.220-0.019-0.079-0.1060.33630.0994-0.1390.02370.17400.14070.10281.10132.03291.43471.69541.75672.49701.38951.04380.80681.29410.53910.55390.54060.52710.47560.72470.75010.40530.54220.56090.44370.39710.46290.46010.35000.40660.37160.34660.39390.424616.55920.30918.43227.85326.55623.48710.01615.0443.521013.3134.76165.20932.97905.36723.75735.93013.13984.23013.42633.12104.58053.77434.01762.78823.41503.92973.69804.39403.38683.5361Table 7: The OSE (µm), OAE (µm) and OHD (µm) calculated using the results of different methods (Dufour’smethod, OCTRMIA3D and GDM) and the ground truth manual segmentation, for overall retina surfaces in eachof the 10 OCT volumesVolume # Dufour et al. OCTRIMA3DGDMDufour et al. OCTRIMA3DGDMDufour et al. OCTRIMA3DGDMOSE (µm)OAE (µm)OHD (µm)12345678910-1.271-1.161-1.513-1.431-1.020-1.434-2.010-1.031-1.951-1.5130.36070.0246-0.0520.42720.63690.42160.00590.58150.05420.10220.43380.06400.34560.35600.50210.39690.32830.57850.20140.21091.83582.53802.14702.52782.41192.67542.24581.74622.13682.33151.12040.96520.93431.03741.07941.13710.96821.10630.87710.83970.95380.72380.78380.86670.82890.86060.74070.90670.69220.659617.48629.68219.98531.34632.60728.62921.78817.61021.34424.8419.33587.79878.34919.40428.68229.52677.064410.1005.74826.32507.91636.12676.99207.31307.13797.25486.82798.51125.47946.7132Figure 16: Boxplots for the SE (µm), AE (µm), HD (µm), OSE (µm), OAE (µm) and OHD (µm) obtainedby different methods in Table 5-7 for the 10 OCT volumes. 1st row: boxplots of Table 5; 2nd row: boxplots ofTable 6; 3rd row: boxplots of Table 7.Table 5-7 contain quantitative information for comparing the accuracy of the three methods on the 10 OCT15Dufour et al.OCTRIMA3DGDMSigned Error (SE)-4-3-2-101Dufour et al.OCTRIMA3DGDMAbsolute Error (AE)11.522.533.544.5Dufour et al.OCTRIMA3DGDMHausdorff Distance (HD)1020304050Dufour et al.OCTRIMA3DGDMSigned Error (SE)-1-0.500.51Dufour et al.OCTRIMA3DGDMAbsolute Error (AE)0.511.522.5Dufour et al.OCTRIMA3DGDMHausdorff Distance (HD)510152025Dufour et al.OCTRIMA3DGDMOverall Signed Error (OSE)-2-1.5-1-0.500.5Dufour et al.OCTRIMA3DGDMOverall Absolute Error (OAE)11.522.5Dufour et al.OCTRIMA3DGDMOverall Hausdorff Distance (OHD)51015202530volumes. Table 5 lists the quantities for the surface B5 around the fovea region, and Table 6 presents thenumerical results for the surface B7 that is flatter and smoother. In Table 5, the SE quantity indicates thatDufour’s method produces larger segmentation bias than the OCTRIMA3D and GDM. The SE values by theGDM are in the range of [-0.128µm 0.6833µm], showing less variability than those by the other two methods.Moreover, the GDM leads to the smallest AE and HD quantities in all 10 cases, indicating that the GDM isthe best among all the methods. Compared with those in Table 5, the quantities in Table 6 show a significantimprovement of all the methods. For example, the range of the HD quantity by Dufour’s method has droppedfrom [25.688µm 56.667µm] to [3.521µm 27.853µm]. In addition, the accuracy gap between the OCTRIMA3Dand GDM has been reduced. The HD values of Volume 3, 7 and 10 by the OCTRIMA has even become smallerthan the corresponding values by the GDM. These improvements are the fact that the retinal surface B5 is flatand the voxel values remain fairly constant. From the OAE and OHD in Table 7 we can observe that the accuracyof the GDM is the highest for the total retina surfaces among the existing approaches.The corresponding boxplots of Table 5-7 are shown in Figure 16. It is clear that the proposed GDM methodperforms consistently better, with higher accuracy and lower error rates for both flat and nonflat retina layers.The boxplots show that there is little variation in performance across the modelled structures and that evenin the worst case scenario the proposed method yields lower error rate than the average performance of othermethods. Furthermore, in Figure 17 we present the 3D plots of the SE, AE and HD quantities computed by thethree methods on the 10 OCT volumes. The SE values by the GDM are closer to zero and the AE and HD valuesby it remain smaller. The overall distribution of these discrete data points also indicates that the GDM resultsare less oscillating. We can thus conclude from Figure 17 that the GDM is the best among all the methodscompared for extracting intra-reintal layer surfaces from 3D OCT volume data in terms of both accuracy androbustness.Figure 17: 3D plots of the SE (µm), AE (µm) and HD (µm) obtained using 10 OCT volumes using Dufour’method, OCTRMIA3D and GDM.16OverallB9B8B7Target Intra-retinal Layer SurfaceB5B4B3B2B10Volume #523-5-4-3-2-10110Signed Error (SE)Dufour et al.OCTRIMA3DGDMOverallB9B8B7Target Intra-retinal Layer SurfaceB5B4B3B2B10Volume #554023110Absolute Error (AE)Dufour et al.OCTRIMA3DGDMOverallB9B8B7Target Intra-retinal Layer SurfaceB5B4B3B2B10Volume #5010203040506010Hausdorff Distance (HD)Dufour et al.OCTRIMA3DGDM4.5 Computational Complexity AnalysisThe experimental results in Section 4.4 have shown that the performance of our algorithm is superior over othersin terms of accuracy. In this section the performance of the different approaches in terms of the computationaltime is demonstrated. We implemented PDS, Chiu’s method and GDM using Matlab 2014b on a Windows 7platform with an Intel Xeon CPU E5-1620 at 3.70GHz and 32GB memory. For a 633 × 496 sized B-scan, withinitialisation close to the true retinal boundaries, it takes 3.625s (500 iterations) for PDS to delineate two parallelboundaries. Chiu’s method needs 1.962s to detect one boundary, while the GDM only takes 0.415s. Note thatthe time complexity of Chiu’s graph search method is O(|E|log(|V |)), where |V | and |E| are the number of nodesand edges. In the context of boundary detection, |V | = M N and |E| = 8M N . Hence the time complexity ofthe method is O(M N log(M N )). In contrast, our GDM solved using fast sweeping only has linear complexityInstead of directly doing segmentation in 3D, theof O(M N ), which is more efficient than Chiu’s method.OCTRMIA3D explores spatial dependency between two adjacent B-scans and applies Chiu’s method to each 2Dframe independently. The OCTRMIA3D is thus able to track retinal boundaries in 3D volume efficiently. It hasbeen reported in [29] that the processing time of the OCTRMIA3D for the whole OCT volume of 496 × 644 × 51voxels was 26.15s, which is faster than our GDM (40.25s is used to segment a 496 × 633 × 10 sized volume).However, such procedure in the OCTRMIA3D complicates the whole 3D segmentation process and might makethe algorithm less general. Finally, Dufour’s graph method needs 14.68s to detect the six intra-retinal layerssurfaces on a 496 × 633 × 10 sized volume. Dufour’s method was implemented using a different programminglanguage (C) and delineated different number of retinal surfaces from those of GDM so comparison can not bemade between the two methods.5 ConclusionWe have presented a new automated segmentation framework based on the geodesic distance for delineatingretinal layer boundaries in 2D/3D OCT images. The framework integrates horizontal and vertical gradient in-formation and can thus account for changes in the both directions. Further, the exponential weight functionemployed within the framework enhances the foveal depression regions and highlights the weak and low contrastboundaries. As a result, the proposed method is able to segment complex retinal structures with large curva-tures and other irregularities caused by pathologies. Extensive numerical results, validated with ground truth,demonstrate the effectiveness of proposed framework for segmenting both normal and pathological OCT images.The proposed method has achieved higher segmentation accuracy than existing methods, such as the parameteractive contour model and the graph theoretic based approaches. Ongoing research includes integrating the seg-mentation framework into a system for detection and quantification of retinal fractures and other diseases of theretina.6 AppendixWe present the 3D fast sweeping algorithm to solve the Eikonal equation (3.3). Given a seed point s1, its distancefunction d(x) satisfies the following Eikonal equation|∇d(x)| = f (x), x /∈ s1(6.1)with d(s1) = 0 and f (x) = W −1(x) where W is defined in (3.2). (6.1) is a typical partial differential equation andit can be solved efficiently by using the fast sweeping algorithm proposed by Zhao [36]. To do so, the Godunovupwind difference scheme is used to discretise (6.1) as follows(cid:2)(dni,j,k − dnxmin)+(cid:3)2+ (cid:2)(dni,j,k − dnymin)+(cid:3)2+ (cid:2)(dni,j,k − dnzmin)+(cid:3)2= f 2i,j,k(6.2)In equation (6.2), dnxmin = min(dni,j+1,k, dni,j−1,k), dnymin = min(dni+1,j,k, dni−1,j,k), dnzmin = min(dni,j,k+1, dni,j,k−1). Boundary conditions need to be handled in the computational grid space. One-sidedand x+ =(cid:26) x x > 00 x ≤ 0upwind difference is used for each of the 6 boundary faces of the grid space. For example, at the left boundaryface, a one-sided difference along the x direction is computed as(cid:2)(dni,1,k − dni,2,k)+(cid:3)2+ (cid:2)(dni,1,k − dnymin)+(cid:3)2+ (cid:2)(dni,1,k − dnzmin)+(cid:3)2= f 2i,1,kdnxmin, dnSo, the unique solution to (6.2) is given as follows:ymin and dnzmin are then sorted in increasing order and the sorted version is recorded as a1, a2 and a3.dn+1i,j,k = min(dni,j,k, (cid:93)di,j,k)(6.3)17where (cid:101)di,j,k is a piecewise function containing three parts(cid:16)a1 + a2 + a3 +i,j,k − (a1 − a2)2 − (a1 − a3)2 − (a2 − a3)2(cid:17)(cid:93)di,j,k =13(cid:16)a1 + a2 +12a1 + fi,j,k3f 2(cid:113)i,j,k − (a1 − a2)2(cid:17)(cid:113)2f 2The three parts correspond to the following intervals, respectivelyf 2i,j,k ≥ (a1 − a3)2 + (a2 − a3)2(a1 − a2)2 ≤ f 2i,j,k < (a1 − a3)2 + (a2 − a3)2i,j,k < (a1 − a2)2f 2To solve (6.3), which is not in analytical form, the fast Gauss-Seidel iteration with alternating sweepingorderings is used. For initialization, the value of the seed point s1 is set to zero, and this value is fixed in latercalculations. The rest of the points are set to large values, and these values will be update later. The whole 3Dgrid is traversed in the following orders for the Gauss-Seidel iteration(1) i = 1 : M, j = 1 : N, k = 1 : H; (2) i = M : 1, j = N : 1, k = H : 1(3) i = M : 1, j = 1 : N, k = 1 : H; (4) i = 1 : M, j = N : 1, k = H : 1(5) i = M : 1, j = N : 1, k = 1 : H; (6) i = 1 : M, j = 1 : N, k = H : 1(7) i = 1 : M, j = N : 1, k = 1 : H; (8) i = M : 1, j = 1 : N, k = H : 1References[1] David Huang, Eric A Swanson, Charles P Lin, Joel S Schuman, William G Stinson, Warren Chang, Michael RHee, Thomas Flotte, Kenton Gregory, Carmen A Puliafito, and James G Fujimoto. Optical coherencetomography. Science, 254(5035):1178–1181, 1991.[2] Michael R Hee, Joseph A Izatt, Eric A Swanson, David Huang, Joel S Schuman, Charles P Lin, Car-men A Puliafito, and James G Fujimoto. Optical coherence tomography of the human retina. Archives ofophthalmology, 113(3):325–332, 1995.[3] Dara Koozekanani, Kim Boyer, and Cynthia Roberts. Retinal thickness measurements from optical coherencetomography using a markov boundary model. Medical Imaging, IEEE Transactions on, 20(9):900–916, 2001.[4] Hiroshi Ishikawa, Scott Piette, Jeffrey M Liebmann, and Robert Ritch. Detecting the inner and outerborders of the retinal nerve fiber layer using optical coherence tomography. Graefe’s archive for clinical andexperimental ophthalmology, 240(5):362–371, 2002.[5] Hiroshi Ishikawa, Daniel M Stein, Gadi Wollstein, Siobahn Beaton, James G Fujimoto, and Joel S Schuman.Macular segmentation with optical coherence tomography. Investigative ophthalmology & visual science,46(6):2012–2017, 2005.[6] Mahnaz Shahidi, Zhangwei Wang, and Ruth Zelkha. Quantitative thickness measurement of retinal layersimaged by optical coherence tomography. American journal of ophthalmology, 139(6):1056–1061, 2005.[7] Delia Cabrera Fern´andez, Harry M Salinas, and Carmen A Puliafito. Automated detection of retinal layerstructures on optical coherence tomography images. Optics Express, 13(25):10200–10216, 2005.[8] MA Mayer, RP Tornow, R Bock, J Hornegger, and FE Kruse. Automatic nerve fiber layer segmenta-tion and geometry correction on spectral domain oct images using fuzzy c-means clustering. InvestigativeOphthalmology & Visual Science, 49(13):1880–1880, 2008.[9] Delia Cabrera Fernandez. Delineating fluid-filled region boundaries in optical coherence tomography imagesof the retina. IEEE Trans. Med. Imaging, 24(8):929–945, 2005.[10] Mircea Mujat, Raymond C Chan, Barry Cense, B Hyle Park, Chulmin Joo, Taner Akkin, Teresa C Chen, andJohannes F de Boer. Retinal nerve fiber layer thickness map determined from optical coherence tomographyimages. Optics Express, 13(23):9480–9491, 2005.[11] Akshaya Mishra, Alexander Wong, Kostadinka Bizheva, and David A Clausi. Intra-retinal layer segmentationin optical coherence tomography images. Optics express, 17(26):23719–23728, 2009.18[12] Azadeh Yazdanpanah, Ghassan Hamarneh, Benjamin Smith, and Marinko Sarunic. Intra-retinal layer seg-mentation in optical coherence tomography using an active contour approach. In Medical Image Computingand Computer-Assisted Intervention–MICCAI 2009, pages 649–656. Springer, 2009.[13] Itebeddine Ghorbel, Florence Rossant, Isabelle Bloch, Sarah Tick, and Michel Paques. Automated segmen-tation of macular layers in oct images and quantitative evaluation of performances. Pattern Recognition,44(8):1590–1603, 2011.[14] Florence Rossant, Isabelle Bloch, Itebeddine Ghorbel, and Michel Paques. Parallel double snakes. applicationto the segmentation of retinal layers in 2d-oct for pathological subjects. Pattern Recognition, 48(12):3857–3870, 2015.[15] Stephanie J Chiu, Xiao T Li, Peter Nicholas, Cynthia A Toth, Joseph A Izatt, and Sina Farsiu. Automaticsegmentation of seven retinal layers in sdoct images congruent with expert manual segmentation. Opticsexpress, 18(18):19413–19428, 2010.[16] Qi Yang, Charles A Reisman, Zhenguo Wang, Yasufumi Fukuma, Masanori Hangai, Nagahisa Yoshimura,Atsuo Tomidokoro, Makoto Araie, Ali S Raza, Donald C Hood, et al. Automated layer segmentation ofmacular oct images using dual-scale gradient information. Optics express, 18(20):21293–21307, 2010.[17] Vedran Kaji´c, Boris Povaˇzay, Boris Hermann, Bernd Hofer, David Marshall, Paul L Rosin, and WolfgangDrexler. Robust segmentation of intraretinal layers in the normal human fovea using a novel statisticalmodel based on texture and shape analysis. Optics express, 18(14):14730–14744, 2010.[18] Vedran Kaji´c, Marieh Esmaeelpour, Boris Povaˇzay, David Marshall, Paul L Rosin, and Wolfgang Drexler.Automated choroidal segmentation of 1060 nm oct in healthy and pathologic eyes using a statistical model.Biomedical optics express, 3(1):86–103, 2012.[19] Matth¨aus Pilch, Yaroslava Wenner, Elisabeth Strohmayr, Markus Preising, Christoph Friedburg, Erd-muthe Meyer zu Bexten, Birgit Lorenz, and Knut Stieger. Automated segmentation of retinal blood vesselsin spectral domain optical coherence tomography scans. Biomedical optics express, 3(7):1478–1491, 2012.[20] Timothy F Cootes, Christopher J Taylor, David H Cooper, and Jim Graham. Active shape models-theirtraining and application. Computer vision and image understanding, 61(1):38–59, 1995.[21] Timothy F Cootes, Gareth J Edwards, and Christopher J Taylor. Active appearance models. IEEE Trans-actions on Pattern Analysis & Machine Intelligence, (6):681–685, 2001.[22] Mona Haeker, Michael Abr`amoff, Randy Kardon, and Milan Sonka. Segmentation of the surfaces of theretinal layer from oct images. In Medical Image Computing and Computer-Assisted Intervention–MICCAI2006, pages 800–807. Springer, 2006.[23] Mona K Garvin, Michael D Abr`amoff, Randy Kardon, Stephen R Russell, Xiaodong Wu, and Milan Sonka.Intraretinal layer segmentation of macular optical coherence tomography images using optimal 3-d graphsearch. Medical Imaging, IEEE Transactions on, 27(10):1495–1505, 2008.[24] Mona Kathryn Garvin, Michael David Abr`amoff, Xiaodong Wu, Stephen R Russell, Trudy L Burns, andMilan Sonka. Automated 3-d intraretinal layer segmentation of macular spectral-domain optical coherencetomography images. Medical Imaging, IEEE Transactions on, 28(9):1436–1447, 2009.[25] Gw´enol´e Quellec, Kyungmoo Lee, Martin Dolejsi, Mona K Garvin, Michael D Abr`amoff, and Milan Sonka.identification of fluid-filled regions in sd-oct of theThree-dimensional analysis of retinal layer texture:macula. Medical Imaging, IEEE Transactions on, 29(6):1321–1330, 2010.[26] Bhavna J Antony, Michael D Abr`amoff, Milan Sonka, Young H Kwon, and Mona K Garvin. Incorporationof texture-based features in optimal graph-theoretic approach with application to the 3d segmentation ofintraretinal surfaces in sd-oct volumes.In SPIE Medical Imaging, pages 83141G–83141G. InternationalSociety for Optics and Photonics, 2012.[27] Pascal A Dufour, Lala Ceklic, Hannan Abdillahi, Stephan Schroder, Sandro De Dzanet, Ute Wolf-Schnurrbusch, and Janusz Kowal. Graph-based multi-surface segmentation of oct data using trained hardand soft constraints. Medical Imaging, IEEE Transactions on, 32(3):531–543, 2013.[28] Raheleh Kafieh, Hossein Rabbani, Michael D Abramoff, and Milan Sonka. Intra-retinal layer segmentation of3d optical coherence tomography using coarse grained diffusion map. Medical image analysis, 17(8):907–928,2013.19[29] Jing Tian, Bogl´arka Varga, G´abor M´ark Somfai, Wen-Hsiang Lee, William E Smiddy, and Delia CabreraDeBuc. Real-time automatic segmentation of optical coherence tomography volume data of the macularregion. PloS one, 10(8):e0133908, 2015.[30] Jing Tian, Boglarka Varga, Erika Tatrai, Palya Fanni, Gabor Mark Somfai, William E Smiddy, andDelia Cabrera Debuc. Performance evaluation of automated segmentation software on optical coherencetomography volume data. Journal of biophotonics, 9(5):478–489, 2016.[31] KA Vermeer, J van der Schoot, JF De Boer, and HG Lemij. Automated retinal and nfl segmentation in octvolume scans by pixel classification. Investigative Ophthalmology & Visual Science, 51(13):219–219, 2010.[32] KA Vermeer, J Van der Schoot, HG Lemij, and JF De Boer. Automated segmentation by pixel classificationof retinal layers in ophthalmic oct images. Biomedical optics express, 2(6):1743–1756, 2011.[33] Alfred R Fuller, Robert J Zawadzki, Stacey Choi, David F Wiley, John S Werner, and Bernd Hamann.Segmentation of three-dimensional retinal image data. Visualization and Computer Graphics, IEEE Trans-actions on, 13(6):1719–1726, 2007.[34] Maciej Szkulmowski, Maciej Wojtkowski, Bartosz Sikorski, Tomasz Bajraszewski, Vivek J Srinivasan, AnnaSzkulmowska, Jakub J Ka(cid:32)lu˙zny, James G Fujimoto, and Andrzej Kowalczyk. Analysis of posterior retinallayers in spectral optical coherence tomography images of the normal retina and retinal pathologies. Journalof biomedical optics, 12(4):041207–041207, 2007.[35] Andrew Lang, Aaron Carass, Matthew Hauser, Elias S Sotirchos, Peter A Calabresi, Howard S Ying, andJerry L Prince. Retinal layer segmentation of macular oct images using boundary classification. Biomedicaloptics express, 4(7):1133–1152, 2013.[36] Hongkai Zhao. A fast sweeping method for eikonal equations. Mathematics of computation, 74(250):603–627,2005.[37] Yen-Hsi Richard Tsai, Li-Tien Cheng, Stanley Osher, and Hong-Kai Zhao. Fast sweeping algorithms for aclass of hamilton–jacobi equations. SIAM journal on numerical analysis, 41(2):673–694, 2003.[38] Jinming Duan, Ben Haines, Wil OC Ward, and Li Bai. Surface reconstruction from point clouds using a novelvariational model. In Research and Development in Intelligent Systems XXXII, pages 135–146. Springer,2015.[39] Delia Cabrera DeBuc. A review of algorithms for segmentation of retinal image data using optical coherencetomography.[40] Qi Song, Xiaodong Wu, Yunlong Liu, Milan Sonka, and Mona Garvin. Simultaneous searching of globallyoptimal interacting surfaces with shape priors. In Computer Vision and Pattern Recognition (CVPR), 2010IEEE Conference on, pages 2879–2886. IEEE, 2010.[41] Jinming Duan, Zhaowen Qiu, Wenqi Lu, Guodong Wang, Zhenkuan Pan, and Li Bai. An edge-weightedsecond order variational model for image decomposition. Digital Signal Processing, 49:162–181, 2016.[42] James A Sethian. A fast marching level set method for monotonically advancing fronts. Proceedings of theNational Academy of Sciences, 93(4):1591–1595, 1996.[43] James Albert Sethian. Level set methods and fast marching methods: evolving interfaces in computationalgeometry, fluid mechanics, computer vision, and materials science, volume 3. Cambridge university press,1999.[44] Tom Goldstein and Stanley Osher. The split bregman method for l1-regularized problems. SIAM journalon imaging sciences, 2(2):323–343, 2009.20