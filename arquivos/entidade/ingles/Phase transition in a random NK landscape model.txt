Artificial Intelligence 172 (2008) 179–203www.elsevier.com/locate/artintPhase transition in a random NK landscape model ✩Sung-Soon Choi a, Kyomin Jung b, Jeong Han Kim c,∗,1a School of Computer Science and Engineering, Seoul National University, Seoul, 151-742 Koreab Department of Mathematics, Massachusetts Institute of Technology, Cambridge, MA 02139, USAc Department of Mathematics, Yonsei University, Seoul, 120-749 KoreaReceived 31 March 2006; received in revised form 26 March 2007; accepted 13 June 2007Available online 27 June 2007AbstractAn analysis for the phase transition in a random NK landscape model, NK(n, k, z), is given. This model is motivated frompopulation genetics and the solubility problem for the model is equivalent to a random (k + 1)-SAT problem. Gao and Culberson[Y. Gao, J. Culberson, An analysis of phase transition in NK landscapes, Journal of Artificial Intelligence Research 17 (2002)√309–332] showed that a random instance generated by NK(n, 2, z) with z > z0 = 27−7is asymptotically insoluble. Based on4empirical results, they conjectured that the phase transition occurs around the value z = z0. We prove that an instance generatedby NK(n, 2, z) with z < z0 is soluble with positive probability by providing a polynomial time algorithm. Using branching processarguments, we prove again that an instance generated by NK(n, 2, z) with z > z0 is asymptotically insoluble. The results show thephase transition around z = z0 for NK(n, 2, z). In the course of the analysis, we introduce a generalized random 2-SAT formula,which is of self interest, and show its phase transition phenomenon.© 2007 Elsevier B.V. All rights reserved.5Keywords: NK landscape; Fitness function; Solubility; Phase transition; Satisfiability problem1. Introduction1.1. NK landscape modelsA fitness landscape is a function that assigns each genetic composition (genotype) with the fitness of the expression(phenotype) of the genetic composition in an environment. The fitness landscape sometimes refers to its graphicalrepresentation as the word “landscape” indicates. The notion of fitness landscape was first introduced by Wright [47]for the analysis of population genetics. Afterwards, mathematical models to study the evolution on fitness landscapehave been proposed by many researchers including Franklin and Lewontin [20], Lewontin [34], Ewens [17], Kauffmanand Weinberger [30], and Macken and Perelson [35]. Among them, the NK model proposed by Kauffman [28] hasattracted considerable attention. The NK model generates fitness landscapes with correlation structures in which we✩ This work was partially carried on in Microsoft Research and partially supported by a grant of Research Institute of Mathematics funded byMicrosoft Korea.* Corresponding author.E-mail addresses: sschoi@soar.snu.ac.kr (S.-S. Choi), kmjung@mit.edu (K. Jung), jehkim@yonsei.ac.kr (J.H. Kim).1 This work was partially supported by Yonsei University Research Funds 2006-1-0078 and 2007-1-0025, and by the second stage of the BrainKorea 21 Project in 2007, and by the Korea Research Foundation Grant funded by the Korean Government (MOEHRD) (KRF-2006-312-C00455).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.06.002180S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203can control the degree of interactions between genes and so, indirectly, the ruggedness and correlation degrees of thelandscapes.An NK landscape is a real-valued function defined on the set of binary n-tuples, {0, 1}n, which is of the formf (x1, x2, . . . , xn) =(cid:3)(cid:4)xi, Π(xi).fin(cid:2)i=1It is a summation of local fitness functions fi ’s, where each fi depends on its main variable xi and the variables inthe neighborhood of xi . Here the neighborhood Π(xi) is a subset of the set {x1, x2, . . . , xn} \ {xi} and its size |Π(xi)|is k. Two ways have been introduced to choose the variables in the neighborhood Π(xi); adjacent neighborhood andrandom neighborhood. In the NK models with adjacent neighborhood, Π(xi) consists of the closest k variables (witha certain tie-break) to the main variable xi with respect to the indices modulo n. In the NK models with randomneighborhood, Π(xi) is composed of the k variables chosen uniformly at random from {x1, x2, . . . , xn} \ {xi}. Localfitness functions are constructed independently of each other. For each local fitness function, a random value from aprobability distribution is assigned for each input. In general, it is independently (or nearly independently) assignedfor each of 2k+1 inputs and its expectation has small absolute value. In the Kauffman’s original model, the uniformdistribution between zero and one was used as the underlying distribution for local fitness functions. Later, it has beenreplaced with various probability distributions in the contexts of analysis and applications, inducing variants of theNK model.The name, “NK landscape”, embodies two parameters n and k. In terms of biology, each variable xi is regarded asa gene. The parameter n represents the number of genes that an organism has. The local fitness function fi quantifiesthe fitness of a character that is determined (or expressed in a biological term) by the gene xi affected by k other genesin Π(xi). A genotype of an organism is the values of genes xi ’s. Strictly speaking, the phenotype corresponding toa genotype is the characters expressed by the genotype. In practice, especially in this paper, the phenotype may beregarded as an organism that has the characters.Generally, the parameter k plays a role in controlling the degree of interactions between genes. The larger the valueof k is, the more genes interact one another in constructing the fitness landscape. Consider the case that k is small.Given two genotypes (or assignments) with the identical values for most of the genes, most of fi ’s produce the samevalues for the genotypes. Since the values of fi ’s are small relatively to the overall fitness f in absolute value, the twogenotypes have similar fitnesses, which implies that the landscape has strong correlation structure. On the other hand,if k is n − 1, each fi has (nearly) independent values for the two genotypes, which induces the landscape consistingof 2n (nearly) independent random values. Through experiments in the original NK model, Kauffman suggested thatthe ruggedness of the landscape generally increases as k increases [28].Kauffman [28] further analyzed various features of adaptive walks in the original NK model. Weinberger [42] andFontana et al. [19] carried out more detailed analysis of such walks. The asymptotic properties of the global and localoptima in NK landscapes were analyzed in various random NK landscape models. The differences between models aremainly due to the underlying distributions for local fitness functions. Evans and Steinsaltz [16], Durrett and Limic [13],Skellet et al. [39], and Kaul and Jacobson [31,32] used the exponential, negative exponential, uniform, and both ofnormal and uniform distributions in their works, respectively. Weinberger [43] and, later, Wright et al. [46] studiedthe computational complexities of problems related to NK landscapes. Gao and Culberson [22] showed a treewidthresult for NK landscapes in a probabilistic way.NK models have been used in biology, physics, and so on. In biology, NK models explain evolutions of biologicalobjects including amino acid sequences [29,30,35], protein or RNA sequences [6,18,19,37,41], and molecular quasi-species [14]. NK models have been served as a reference point for understanding the properties of those biologicalobjects. In statistical physics, models of spin-glasses are investigated from the viewpoint of NK models in [42]. Theevolution of organizations in a business environment is modeled based on an NK model [33]. NK models have beenused as a benchmark for evaluating various encoding schemes and genetic operators on the evolutionary algorithmand comparing them in the evolutionary computation area [5,24,36]. They have been also served as a basis for thedesign of problem difficulty measures for evolutionary algorithms [26,40] and the design of epistasis measures [38].S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–2031811.2. Solubility problem and phase transition in a random NK modelOne of the most interesting questions regarding NK landscape is the solubility problem that asks whether thereexists a genotype, or an assignment of values xi ’s, that maximizes the values of all local fitness functions. In otherwords, the problem asks whether there exists an organism that perfectly fits to a given environment, which seems tobe the most natural question regarding the NK landscape. For the problem it is enough to consider binary local fitnessfunctions having only two values 0 and 1, since one may replace each local fitness function by an auxiliary binaryfitness function that is 1 if and only if the value of the local fitness function is its maximum. An NK landscape f iscalled soluble if there is such an assignment. Otherwise, it is insoluble.Weinberger [43] and Wright et al. [46] proved that the problem for the NK landscapes with arbitrary neighborhoodis NP-complete for k (cid:2) 2. To investigate the difficulties of the solubility problems for typical NK landscapes with ran-dom neighborhood, Gao and Culberson [21] proposed two random NK landscape models and provided results aboutthe phase transition in the models. A phase transition in probabilistic combinatorial theory refers to the phenomenonthat the probability of a property being satisfied in the random model rapidly changes as the order parameter changesaround a certain value. The two random models are the uniform probability model and the fixed ratio model inspiredby the two random graph models of Erd˝os-Rényi type, G(n, p) and G(n, m), respectively. In the uniform probabilitymodel, the fitness value of each input for a local fitness function is independently assigned to zero with probability pand one with probability 1 − p. This process is independently repeated for each local fitness function. It was shownthat an instance generated by this model is asymptotically insoluble or, if it is soluble, a solution can be found inpolynomial time with high probability. However, unless p decreases very rapidly with n, it is easy to see that, withhigh probability, a random instance has a local fitness function that takes zero values for all inputs. This makes therandom instance insoluble with high probability. For this reason, the model is not desirable as a model for representingtypical instances.The fixed ratio model overcomes the drawback of the uniform probability model by fixing the ratio of zero valuesfor each local fitness function. The fixed ratio model NK(n, k, z) is as follows. The value of z ranges in [0, 2k+1]. If zis an integer, for each local fitness function fi , we choose z tuples of 2k+1 possible assignments uniformly at randomwithout replacement and independently of other fj ’s. Then fi = 0 for those tuples and fi = 1 for the other tuples. If zis not an integer so that z = (cid:4)z(cid:5) + h (0 < h < 1), we specify the fitness values of (cid:4)(1 − h)n(cid:5) local fitness functions asif they were local fitness functions in NK(n, k, (cid:4)z(cid:5)) and those of the rest of the local fitness functions as if they werein NK(n, k, (cid:4)z(cid:5) + 1). Another way to specify the fitness values of local fitness functions is that we regard each localfitness function as if it were a local fitness function in NK(n, k, (cid:4)z(cid:5)) with probability 1 − h and in NK(n, k, (cid:4)z(cid:5) + 1)with probability h, independently of all others. For example, if z = 2 + h, then each local fitness function has zerovalues for two random assignments with probability 1 − h and for three random assignments with probability h. Thisnew model is denoted by NK(n, 2, z). It is easy to see that NK(n, 2, z) is essentially the same as NK(n, 2, z).√For k = 2, it was proved [21] that an instance generated by the fixed ratio model with z > z0 = 27−754≈ 2.837is almost always insoluble, where “an event An almost always occurs” means that limn→∞ Pr[An] = 1. And it wasempirically suggested that the instances generated by the model with z < z0 are soluble and the solutions are found inpolynomial time with probability close to one. From these, Gao and Culberson conjectured that the phase transitiontakes place around z = z0 in the fixed ratio model with k = 2.1.3. Contribution and approachIn this paper, we prove that an instance generated by the fixed ratio model with z < z0 is soluble with positiveprobability by providing a polynomial time algorithm. This settles the conjecture in an affirmative way. Using branch-ing process arguments, we also give an another way to prove that an instance generated by the model with z > z0 isalmost always insoluble:Theorem 1. If 0 < z < z0, then there exists α > 0 depending on z such that the probability of NK(n, 2, z) beingsoluble is at least α as n goes to infinity. If z > z0, then NK(n, 2, z) is almost always insoluble.Though it is a very interesting question, we have no idea whether α can be arbitrarily close to 1 or not.182S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203Let an NK landscape f =To prove Theorem 1, we reduce the solubility problem of an NK landscape to the (k + 1)-SAT problem as in [21].For given Boolean variables, the variables and their complements are called literals. Two literals are strictly distinct iftheir underlying variables are different. A k-clause is a disjunction of k strictly distinct literals and a k-SAT formula isa conjunction of k-clauses. Given a k-SAT formula F , the k-SAT problem is to ask whether there is a truth assignmentsatisfying F .(cid:5)ni=1 fi(xi, Π(xi)). For each local fitness function fi , we construct the (k + 1)-clauseswith the literals of the main variable and neighborhood variables of fi such that fi is equal to zero only for theassignments that do not satisfy one of the clauses. For example, suppose that a local fitness function fi(xi, xj , xk)has zero value only when (xi, xj , xk) is one of (0, 0, 0), (0, 1, 0), and (1, 1, 0). Then, we construct three 3-clauses(xi ∨ xj ∨ xk), (xi ∨ xj ∨ xk), and (xi ∨ xj ∨ xk) for fi(xi, xj , xk). We take the conjunction of all the (k + 1)-clausesobtained from all the fi ’s to construct a (k + 1)-SAT formula F . It is easy to check that f is soluble if and only if Fis satisfiable. Thus, it is sufficient to consider the phase transition for the satisfiability of the 3-SAT formula F .There have been many studies for the phase transition of the satisfiability of the random 3-SAT formula, in whichthe 3-clauses are chosen independently and uniformly at random. (See, for example, [1–3,12,25,27].) In verifyinglower bounds of the threshold, many results were obtained by applying variants of the unit clause algorithm that werefirst analyzed by Chao and Franco [8,9]. We will apply a variant of the unit clause algorithm to the 3-SAT formulareduced from the random NK landscape NK(n, 2, z) in the subcritical region of the phase transition. In Section 2,we describe the unit clause algorithm and investigate some properties of the reduced 3-SAT formula that should beconsidered when the unit clause algorithm is applied to it. The properties suggest that it is useful to consider fourtypes of random 2-clauses or random equalities (of truth values of variables).In Section 3, we introduce a generalized random 2-SAT formula consisting of the random 2-clauses and the randomequalities presented in Section 2. It generalizes the well-known random 2-SAT formula in which the 2-clauses arechosen independently and uniformly at random [7,10]. After a parameter D is introduced, a threshold phenomenonresult is obtained: A random 2-SAT formula generated by the model is satisfiable with positive probability if D < 1and almost always unsatisfiable if D > 1. It turns out that the threshold is not sharp.In Section 4, we provide the threshold phenomenon result for the satisfiability of the reduced 3-SAT formula,or equivalently, the proof of Theorem 1. To obtain the result for the subcritical region, we use similar approachesdeveloped in Section 3. For the supercritical region, we introduce another random 2-SAT model, which is similar tothe generalized random 2-SAT model presented in Section 2. The formula generated according to the model consistsof random 2-clauses resolved from the 3-SAT formula reduced from NK(n, 2, z).2. The unit clause algorithmIn the subcritical region, we will apply a variant of unit clause algorithm to the 3-SAT formula F (n, 2, z) reducedfrom a random instance of NK(n, 2, z), and show that the algorithm finds a satisfying assignment with positive prob-ability. The 3-clauses in the formula are to be regarded as ordered 3-tuples and (copies of) literals came from mainvariables are placed in the first coordinate of the corresponding 3-clauses. Those (copies of) literals are called main(copies of) literals.Now we consider the unit clause algorithm (UC). UC takes as input a formula F over n variables and outputsa satisfying assignment of F , or outputs “Cannot determine”. UC consists of one loop of n iterations and in eachiteration of the loop, UC chooses a literal l contained in a unit clause chosen uniformly at random among all the unitclauses. If there is no unit clause, it chooses a literal l uniformly at random among all the literals not assigned truthvalues and sets l to be true. Then all the clauses containing l are satisfied and all the clauses containing ¯l are shortenedto the clauses without ¯l. UC fails to produce a satisfying assignment if and only if a 0-clause, a clause with no literal,is created.Fig. 1 describes the pseudo code of UC. For a literal l, let var(l) be the underlying variable of l. For a set V ={x1, . . . , xn} of Boolean variables, let L(V ) denote the set of 2|V | literals on the variables of V . For i (cid:2) 0, let Ci(t)denote the collection of all the i-clauses of F at the end of the tth iteration. When F = F (n, 2, z), C3(0) is thecollection of all the clauses of F and the other Ci(0)’s are empty. In general, it is easy to see thatCi(t + 1) =(cid:6)(cid:7)c | (c ∈ Ci(t), l /∈ c, and ¯l /∈ c) or (c ∨ ¯l) ∈ Ci+1(t).S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203183UC (F )// F : a formulaV ← {x1, x2, . . . , xn};S ← ∅;for t = 0, 1, . . . , n − 1if |C1(t)| (cid:12)= 0choose a literal l uniformly at random from C1(t);V ← V − {var(l)};elsechoose a literal l uniformly at random from L(V );V ← V − {var(l)};satisfy all clauses of F containing l;remove ¯l from all clauses of F ;S ← S ∪ {l};if C0(n) = ∅ output solution S;else output “cannot determine”;Fig. 1. Pseudo code of the unit clause algorithm.When we apply UC to F (n, 2, z), there are three main distinctive properties to be considered. First, there may be apair of 3-clauses of the form (l1 ∨ l2 ∨ l3) and (l1 ∨ l2 ∨ l3). If l3 is set to be 1, then two clauses (l1 ∨ l2) and (l1 ∨ l2)would be created. The conjunction of the two clauses is equivalent to the unit clause (l2). So we will regard it asthe unit clause. This property is called sublimation. If a local fitness function is the conjunction of three clauses like(l1 ∨ l2 ∨ l3), (l1 ∨ l2 ∨ l3), (l1 ∨ l2 ∨ l3) and l1 is set to be 1, we have three 2-clauses (l2 ∨ l3), (l2 ∨ l3), (l2 ∨ l3). Bythe sublimation, we meant that these three clauses are replaced by two unit clauses (l2) and (l3), as the conjunctionof the three 2-clauses is logically equivalent to the conjunction of the two unit clauses. (Though the conjunction of(l2 ∨ l3) and (l2 ∨ l3) induces l2 = l3, we do not add the equality since it is redundant by the fact that the conjunctionof the two unit clauses implies the equality.) If we apply UC without sublimation, UC almost always fails to satisfyF , as shown in the following. Consider a pair of 3-clauses of the form (l1 ∨ l2 ∨ l3) and (l1 ∨ l2 ∨ l3) in C3(0). In theprocess of UC, with constant probability, l3 is set to be true before var(l1) and var(l2) are set to have truth values,which induces two clauses (l1 ∨ l2) and (l1 ∨ l2). Again, with constant probability, l2 is set to be true before var(l1) isset to have a truth value and the two clauses are reduced to a pair of unit clauses (l1) and (¯l1). Then, UC fails to satisfyF . Since there are (cid:4)(n) pairs of 3-clauses of this form in C3(0), without sublimation, it is not difficult to show thatUC almost always fails to satisfy F .Second, there may be a pair of 3-clauses of the form (l1 ∨ l2 ∨ l3) and (l1 ∨ l2 ∨ l3). Again, if ¯l3 is set to be true,then two clauses (l1 ∨ l2) and (l1 ∨ l2) would be produced. The conjunction of the two clauses is equivalent to theequality l1 = l2. Third, the main (copies of) literals from different local fitness functions are strictly distinct. This factturns out to increase the threshold value compared to the case without main literals.In the process of UC, 2-clauses are produced from the 3-clauses of F . Some pair of 2-clauses will become equalitiesby the second property. Due to the third property, 2-clauses with main variables will appear so that the literals in theirfirst places are strictly distinct. Similarly, equalities with main variables will appear too. Pairs of two clauses like(l1 ∨ l2) and (l1 ∨ l2) do not appear because of the sublimation property. Motivated by these facts, we will separatelyconsider a generalized random 2-SAT formula consisting of random 2-clauses and equalities, both with and withoutmain variables.As explained in Section 3, unit clauses consisting of main (copies of) literals and unit clauses consisting of other(copies of) literals have different properties. So we will consider two types of unit clauses. Unit clauses consisting ofmain literals and the (copies of) literals therein are to be colored red. The other unit clauses and the (copies of) literalstherein are to be colored blue. Then Fig. 2 is the flow diagram of clauses in the process of UC.3. A generalized random 2-SAT formulaIn this section, we define a generalized random 2-SAT formula and examine its satisfiability. As mentioned inSection 2, the generalized random 2-SAT formula has four types of random 2-clauses or equalities. Here 2-clausesand equalities are to be regarded as ordered pairs. The first type consists of typical uniform random clauses, that is,184S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203Fig. 2. Flow diagram of clauses in the process of UC.clauses chosen uniformly at random among all the 2-clauses. The second type consists of uniform random equalitiesover all the literals. The third and fourth types are the same as the first and the second types, respectively, except thatthe copy of literals in the first places of the clauses or the equalities are pairwise strictly distinct. Those copies of literalsare called main literals. Let c1, c2, c3 and c4 be nonnegative real numbers with c3 + c4 (cid:3) 1. Denote Fi = Fi(n, ci) theconjunction of cin 2-clauses or equalities of type i, 1 (cid:3) i (cid:3) 4. Denoted by F (n, c1, c2, c3, c4) is the conjunction ofthe four random formulae with pairwise strictly distinct main literals.If c2 = c3 = c4 = 0, it is well known [10,11,23] that F (n, c1, 0, 0, 0) is almost always satisfiable if c1 < 1 andalmost always unsatisfiable if c1 > 1. It turns out that the parameterD = c1 + 2c2 + c3 + 2c4 − (c3 + 2c4)24plays a similar role in the general case, as D essentially determines the branching ratio. Roughly speaking, the branch-ing ratio is the expected number of unit clauses produced when a literal is set to be true. This is why a variant of UCsucceeds with positive probability if D < 1, and it almost always fails if D > 1. More precisely, we have the followingtheorem.Theorem 2. If D < 1, then there exists α > 0 depending on ci ’s so that the probability of F (n, c1, c2, c3, c4) beingsatisfiable is at least α as n goes to infinity. If D > 1, then the random formula is almost always unsatisfiable.It is not difficult to show that α cannot be arbitrarily close to 1 if c2 > 0 or c4 > 0. When c2 > 0, let Xij k bethe random variable representing that the equalities xi = xj , xj = xk, and xk = xi exist among the c2n uniform] −random equalities, and let X =]) = O(1/n). Hence, by Chebyshev inequality [4], Pr[X > 0] is bounded below by some positive]E[Xi2j2k2E[Xi1j1k1constant and so the probability of F (n, c1, c2, c3, c4) being satisfiable is bounded above by some constant less than 1.Similar argument holds for the case when c4 > 0.Xi,j,k. Then, E[X] = (cid:4)(1) and Var[X] = E[X2] − E[X]2 =(E[Xi1j1k1 Xi2j2k2(cid:5)(cid:5)Theorem 2, in particular, says that the existence of main literals makes the random formula easier to be satisfied.For example, if there are 0.1n uniform random 2-clauses and n random 2-clauses with main literals, then the randomformula is satisfiable with positive probability. On the other hand, if there are 1.1n uniform random 2-clauses, therandom formula is almost always unsatisfiable. If one equality is regarded as its corresponding two 2-clauses thenc1 + 2c2 + c3 + 2c4 represents the total number of 2-clauses. The extra term −(c3 + 2c4)2/4 is the effect of theexistence of main literals.3.1. Subcritical regionNow we prove the first part of Theorem 2. Without loss of generality, we may assume c1 + c2 > 0 and c3 + c4 > 0.Otherwise, some uniform random 2-clauses or random 2-clauses with main literals might be added to F while theconditions D < 1 and c3 + c4 (cid:3) 1 are kept. Here we define some notations. “At time t” means after t times of iterationof UC, or equivalently, after t literals have been set. Let V (t) denote the set of variables not assigned truth values atS.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203185UC with switching server policy (F )// F : a formulaS ← ∅;for t = 0, . . . , n − 1χ(t) ← 1 with probability p, χ(t) ← 0 otherwise;if χ(t) = 1if B(t) (cid:12)= ∅choose a unit clause (l) uniformly at random from B(t);elsechoose a literal l uniformly at random from L(V (t));if χ(t) = 0if R(t) (cid:12)= ∅choose a unit clause (l) uniformly at random from R(t);elseif V (t) − VM (t) = ∅output “cannot determine” and terminate;elsechoose a literal l uniformly at random from L(V (t) − VM (t));satisfy clauses of F containing l;remove all the copies of ¯l and sublimate if possible;S ← S ∪ {l};if C0(n) = ∅ output solution S;else output “cannot determine”;Fig. 3. Pseudo code of UCS.time t. For 1 (cid:3) i (cid:3) 4, let Fi(t) denote the conjunction of remaining 2-clauses or equalities of Fi at time t. Define|Fi(t)| to be the number of 2-clauses or equalities in Fi(t). Let F (t) = F1(t) ∧ F2(t) ∧ F3(t) ∧ F4(t).As in Section 2, unit clauses consisting of main (copies of) literals and the main (copies of) literals themselves arecolored red. The other unit clauses and the (copies of) literals therein are colored blue. Let B(t) and R(t) denote theset of blue unit clauses and red unit clauses at time t, respectively. Let VM (t) denote the set of the underlying variablesof the main literals of F3(t) and F4(t).As mentioned, we apply a variant of UC that uses a different literal selection policy. Think of UC as an imaginaryserver whose task is satisfying one unit clause, if any, at each time. We regard B(t) and R(t) as two task queues thatthe server works for. The server will work for one queue at a time and the queue selection is made randomly with agiven probability p, which will be specified later. We call this modified UC UC with switching server policy (UCS).Fig. 3 describes the pseudo code of UCS. Note that if c3 + c4 = 1 and p < 1, then UCS may encounter the case that aliteral in L(V (t) − VM (t)) must be chosen while V (t) − VM (t) is empty. We first consider the case that c3 + c4 < 1.Let(cid:8)p =c1 + 2c2 +c1 + 2c2 + c3 + 2c4 +(cid:8)(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4)(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4).(1)Note that 0 < p < 1. We defined p so that the expected number of blue unit clauses produced at each time is less thanp and the expected number of red unit clauses produced at each time is less than 1 − p. Using these facts and by acoupling argument, we will show that, with positive probability, no 0-clause is produced until (1 − (cid:6))n variables areassigned truth values, for a small constant (cid:6) > 0. When (1 − (cid:6))n variables are assigned truth values, the remainingformula is very sparse and it is easy to show that the formula is satisfiable with positive probability.It is not hard to see that at each time t, F (t) has the same distribution as F (n − t, c1(t), c2(t), c3(t), c4(t)), whereci(t) = |Fi(t)|/(n − t): Suppose a variable is set to be 1 or 0. For the literals of the variable appeared in the firstcoordinates or in the second coordinates of 2-clauses or equalities without main variables, the other literals are uniformrandom among all remaining literals. For the literals of the variable appeared in the second coordinates of 2-clausesand equalities with main variables, we first specify the number a of such 2-clauses and equalities and then, conditionedon the number, all a main variables are equally likely to be in the first coordinate of the 2-clauses and equalities. Thismay be also illustrated using a card game described in [2].186S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203The distributions of the numbers of blue and red unit clauses produced at each time highly depend on the sizesof Fi(t)’s. So, we first show that |Fi(t)|’s are highly predictable using Wormald’s theorem [45]. Let H (t) denote thehistory of Fi(t)’s, i.e., the matrix (cid:15) (cid:16)F (0), . . . , (cid:16)F (t)(cid:17), where (cid:16)F (t) = (F1(t), . . . , F4(t)). The distributions of |Fi(t +1)| − |Fi(t)| (1 (cid:3) i (cid:3) 4) conditioned on H (t) play important role here.Lemma 1. For any small (cid:6) > 0, we have for all 0 (cid:3) t (cid:3) (1 − (cid:6))n,(cid:9)(cid:10)(cid:10)(cid:10)F1(t + 1)(cid:10) −(cid:10)(cid:9)(cid:10)(cid:10) −(cid:10)F2(t + 1)(cid:9)(cid:10)(cid:10)(cid:10)F3(t + 1)(cid:10) −(cid:9)(cid:10)(cid:10)(cid:10)F4(t + 1)(cid:10) −EEEE(cid:10)(cid:10)(cid:11)(cid:10)F1(t)(cid:10) | H (t)(cid:10)(cid:10)(cid:11)(cid:10)F2(t)(cid:10) | H (t)(cid:10)(cid:10)(cid:11)(cid:10)F3(t)(cid:10) | H (t)(cid:10)(cid:10)(cid:11)(cid:10)F4(t)(cid:10) | H (t)= − 2|F1(t)|n − t= − 2|F2(t)|n − t,,= −(1 + p)= −(1 + p)|F3(t)|n − t|F4(t)|n − t+ o(1),+ o(1).Proof. Suppose that UCS sets a literal l to be true at time t. Then |F1(t +1)|−|F1(t)| = −X1, where X1 is the numberof 2-clauses of F1(t) that contain l or l. For a 2-clause (l1 ∨ l2) of F1(t), Pr[l1 = l or l1 = l or l2 = l or l2 = l] = 2n−t .And the 2-clauses of F1(t) are independent from one another. So X1 has a binomial distribution Bin[|F1(t)|, 2].n−tThe same argument can be applied to see that |F2(t + 1)| − |F2(t)| = −X2, where X2 has a binomial distributionBin[|F2(t)|, 2]. Also, |F3(t + 1)| − |F3(t)| = −Y3 − Z3, where Y3 is the number of 2-clauses of F3(t) whose mainn−tliterals are equal to l or ¯l, and Z3 is the number of 2-clauses of F3(t) whose second literals are equal to l or ¯l. Herewe divide into two cases according to the value of χ(t), which is defined in Fig. 3. First, suppose that χ(t) = 1. Then|F3(t)|since l and ¯l are equal to at most one of the main literals of F3(t).Y3 has Bernoulli distribution with densityn−tAnd Z3 has a binomial distribution Bin[|F3(t)|,] according to whether Y3 = 0 or1(n−t−1)Y3 = 1. Suppose that χ(t) = 0. Then Y3 = 0 because l and ¯l cannot be equal to any of main literals of F3(t). AndZ3 has a binomial distribution Bin[|F3(t)|,]. The distribution and the expectation of |F4(t + 1)| − |F4(t)| areobtained in the same way. So the lemma follows. (cid:2)] or Bin[|F3(t)| − 1,1(n−t−1)1(n−t−1)Now we state Wormald theorem. A function g is said to satisfy a Lipschitz condition on an open set D0 ⊂ Rk+1 if|ui − vi|, for all (u1, . . . , uk+1)there exists a constant L > 0 such that |g(u1, . . . , uk+1) − g(v1, . . . , vk+1)| (cid:3) Land (v1, . . . , vk+1) in D0.k+1i=1(cid:5)Theorem 3 (Wormald). For 1 (cid:3) j (cid:3) m, where m is a fixed number, let Yj (t) (which also depends on n) be a sequenceof real-valued random variables such that for all j , all t with 0 (cid:3) t (cid:3) t0 = t0(n), and n, |Yj (t)| (cid:3) C0n for some con-stant C0. Let H (t) denote the history of sequences, i.e. the matrix (cid:15) (cid:16)Y (0), . . . , (cid:16)Y (t)(cid:17), where (cid:16)Y (t) = (Y1(t), . . . , Ym(t)).Let D0 be some bounded connected open set of Rm+1 containing the closure of {(0, z1, . . . , zm) | zj = Yj (0)n ,1 (cid:3) j (cid:3) m, for some n}. Let gj : Rm+1 → R, 1 (cid:3) j (cid:3) m, and suppose that the followings are true for some t0 = t0(n).(cid:9)E(i) For all j and uniformly over all 0 (cid:3) t < t0,(cid:11)Yj (t + 1) − Yj (t) | H (t)(ii) For all j and uniformly over all 0 (cid:3) t < t0,(cid:10)(cid:9)(cid:10)(cid:11)(cid:10) > n(cid:10)Yj (t + 1) − Yj (t)5 | H (t)= gjPr1= o(n−3).(cid:3)t/n, Y1(t)/n, . . . , Ym(t)/n(cid:4)+ o(1).(iii) For each j , gj is continuous and satisfies a Lipschitz condition on D0.Then the followings hold.(a) For (0, ˆz(1), . . . , ˆz(m)) ∈ D0 the system of differential equationsdzjds= gj (s, z1, . . . , zm),1 (cid:3) j (cid:3) mS.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203187has a unique solution in D0 for zj : R → R passing through zj (0) = ˆz(j ), 1 (cid:3) j (cid:3) m, and which extends to pointsarbitrarily close to the boundary of D0.(b) Almost always Yj (t) = zj ( tsolution in (a) with ˆz(j ) = Yj (0)n ) · n + o(n) uniformly for 0 (cid:3) t (cid:3) min{σ n, t0} and for each i, where zj (s) is then , and σ = σ (n) is the supremum of those s to which the solution can be extended.Using Lemma 1 and applying Wormald theorem, we may have(cid:13)(cid:12)Lemma 2. For any small (cid:6) > 0, almost always we have uniformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n,|F2(t)|n − t|F4(t)|n − t1 − tn(cid:12)1 − tn|F1(t)|n − t|F3(t)|n − t1 − tn1 − tn+ o(1),(cid:13)+ o(1),+ o(1),+ o(1).= c1= c2= c4= c3(cid:12)(cid:13)(cid:12)(cid:13)pp(2)Proof. To apply Wormald theorem to our situation, let m = 4, Yi(t) = |Fi(t)| (1 (cid:3) i (cid:3) 4), C0 = c1 + c2 + c3 + c4,and t0 = (1 − (cid:6))n. Let(cid:6)(s, z1, z2, z3, z4) | −(cid:6) < s < 1, −(cid:6) < zi < ci + (cid:6)D0 =(cid:7),andg1(s, z1, z2, z3, z4) = − 2z11 − s,g3(s, z1, z2, z3, z4) = −(1 + p)z31 − s,g2(s, z1, z2, z3, z4) = − 2z21 − s,g4(s, z1, z2, z3, z4) = −(1 + p)z41 − s.By the expectations and distributions of |Fi+1(t)| − |Fi(t)|, the conditions in Wormald theorem are easily satisfieddirectly. Then we get ϕi : [0, 1 − (cid:6)] → R, the solution of the following system of differential equations,⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩dϕ1dxdϕ2dxdϕ3dxdϕ4dx= − 2ϕ1(x)1−x= − 2ϕ2(x)1−x= −(1 + p) ϕ3(x)1−x= −(1 + p) ϕ4(x)1−xϕ1(0) = c1,ϕ2(0) = c2,ϕ3(0) = c3,ϕ4(0) = c4such that, almost always |Fi(t)| = ϕi( tϕ1(x) = c1(1 − x)2, ϕ2(x) = c2(1 − x)2, ϕ3(x) = c3(1 − x)1+p, and ϕ4(x) = c4(1 − x)1+p. (cid:2)n ) · n + o(n) holds uniformly for 0 (cid:3) t (cid:3) (1 − (cid:6))n and 1 (cid:3) i (cid:3) 4. Note thatAs limx→1(cid:5)4i=1ϕi (x)1−x= 0, we may choose (cid:6) > 0 so that almost always the total number of 2-clauses and equalitiesremaining at time t = (1 − (cid:6))n is less than 0.01(cid:6)n. From Lemma 2, almost always the following holds:(cid:10)(cid:10)(cid:10)F4(t)(cid:10) (cid:3) (c3 + c4)(n − t) + o(n) < n − t =(cid:10)(cid:10)(cid:10) =(cid:10)VM (t)(cid:10)(cid:10)(cid:10)F3(t)(cid:10) +(cid:10)(cid:10)(cid:10),(cid:10)V (t)uniformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. So, for 0 (cid:3) t (cid:3) (1 − (cid:6))n, almost always UCS does not encounter the case thatχ(t) = 0 but V (t) − VM (t) is empty.For 1 (cid:3) i (cid:3) 4, let bi(t) be the number of blue unit clauses coming from Fi(t) at time t and let ri(t) be the numberof red unit clauses coming from Fi(t) at time t. We will obtain the expectations and distributions of bi(t)’s andri(t)’s, conditioned on |Fi(t)|’s. Suppose that UCS sets a literal l to be true at time t. Then b1(t) is the number of2-clauses in F1(t) that contain ¯l. And r1(t) = 0 since there is no main literal in F1(t). Note that, for each 2-clause(l1 ∨ l2) ∈ F1(t), Pr[¯l = l1 or ¯l = l2] = 1n−t . And the 2-clauses in F1(t) are independent from one another. So b1(t)]. The same argument can be applied to have that r2(t) = 0 and b2(t) hashas a binomial distribution Bin[|F1(t)|,1(n−t)a binomial distribution Bin[|F2(t)|,]. For b3(t), observe that b3(t) is the number of 2-clauses in F3(t) whose2(n−t)main literals are ¯l. Here we consider two cases according to the value of χ(t). First, suppose that χ(t) = 1. Then|F3(t)|2(n−t) since ¯l may be equal to at most one of the main literals in F3(t).b3(t) has a Bernoulli distribution with densityWhen χ(t) = 0, b3(t) = 0 since ¯l cannot be equal to any of the main literals in F3(t) and hence only unit clauses188S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203with main literals are produced. For r3(t), observe that r3(t) is the number of 2-clauses in F3(t) whose second literalsare equal to ¯l. Suppose that l is not strictly distinct with one of main variables of F3(t). Then, since exactly one2-clause in F3(t) has l or ¯l as main literal, r3(t) has a binomial distribution Bin[|F3(t)| − 1,]. Otherwise,r3(t) has a binomial distribution Bin[|F3(t)|,]. Thus, the distribution of r3(t) is a linear combination ofBin[|F3(t)| − 1,]. The same argument can be applied to obtain the distributions of|F4(t)|b4(t) and r4(t). If χ(t) = 1, b4(t) has a Bernoulli distribution with density(n−t) and if χ(t) = 0, then b4(t) = 0. And] if l is not strictly distinct with one of the main variablesr4(t) has a binomial distribution Bin[|F4(t)| − 1,]. Thus, the distribution of r4(t) is a linearof F4(t). Otherwise, r4(t) has a binomial distribution Bin[|F4(t)|,1(n−t−1)combination of Bin[|F4(t)| − 1,].12(n−t−1)12(n−t−1)] and Bin[|F4(t)|,] and Bin[|F3(t)|,12(n−t−1)12(n−t−1)1(n−t−1)i (t)’s and r ∗i (t)’s (and r ∗] and r ∗Observe that bi(t)’s and ri(t)’s (over t) are dependent random variables, which makes our analysis difficult. For-tunately, the dependency is weak and it is possible to bypass this obstacle using couplings. It is not hard to see thatwe may take mutually independent random variables b∗i (t)’s) as follows: The random variable b∗1(t) hasa binomial distribution Bin[(1 + δ)c1n(1 − t1 (t) = 0, where δ > 0 is a small constant defined later.Though b∗i (t)’s, i = 2, 3, 4, may be similarly defined, we just list their distributions for completeness. The] and r ∗random variable b∗2n )2,3(t) is χ times(n−t)n )p, and r ∗a random variable that has a Bernoulli distribution with density (1 + δ) c32 (1 − t3 (t) has a binomial distribu-tion Bin[(1 + δ)c3n(1 − t14(t) is χ times a random variable that has a Bernoulli distribution2(n−t−1)n )p, and r ∗with density (1 + δ) c3]. We couplebi(t) and b∗i (t)). Inparticular, almost always4 (t) has a binomial distribution Bin[(1 + δ)c4n(1 − ti (t)) so that if |Fi(t)|’s satisfy Eq. (2), then bi(t) (cid:3) b∗2(t) has a binomial distribution Bin[(1 + δ)c1n(1 − t2 (1 − ti (t), (and ri(t) and r ∗n )1+p,1(n−t−1)i (t) (and ri(t) (cid:3) r ∗2 (t) = 0. And, b∗]. Finally, b∗n )1+p,1(n−t)n )2,1(n−t−1)1(n−t−1)∗i (t),bi(t) (cid:3) bri(t) (cid:3) runiformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. The expectations of b∗(cid:18)(cid:19)(cid:19)(cid:18)∗i (t)E[b∗E[r ∗i (t)]i (t)]= T∗i (t) ·p1 − p+ o(1),i (t) and r ∗i (t) are as follows:where,(cid:19)10(cid:13) (cid:18)(cid:12)∗1 (t) = c1(1 + δ)T(cid:12)∗3 (t) = c3(1 + δ)1 − tn1 − t11n(cid:5)i (t), and r ∗(t) =4(cid:5)always, b(t) (cid:3) b∗(t) and r(t) (cid:3) r ∗(t) for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. By letting T ∗(t) =1212(cid:5)4i=1 bi(t), r(t) =(cid:12)∗2 (t) = c2(1 + δ)(cid:12)∗4 (t) = c4(1 + δ)(cid:5)1 − tn1 − tni=1 ri(t), b∗(t) =Then for b(t) =i=1 b∗2 20 0(cid:18)10(cid:18)(cid:13) (cid:18)012(cid:13)(cid:13)(cid:19)TTTpp,4(cid:19),(cid:19).01(cid:5)i=1 r ∗4i=1 T ∗4(cid:18)(cid:19)E[b∗(t)]E[r ∗(t)](cid:18)(cid:19)= T∗(t) ·p1 − p+ o(1).i (t), it is clear that, almosti (t), we have thatLemma 3. There exists a constant δ > 0 such that, E[b∗(t)] < p − δ and E[r ∗(t)] < 1 − p − δ for all 0 (cid:3) t (cid:3) (1 − (cid:6))n.(cid:9)(cid:11)(cid:9)(cid:11)Proof. Since T ∗(t) ·each pair of the entries, it suffices to show that(cid:19)(cid:3) T ∗(0) ·p1−pp1−p(cid:19)(cid:18)(cid:18), where the inequality for the vectors means that the inequality holds for∗(0) ·Tp1 − p<p1 − p.Clearly,∗T(0) = (1 + δ)(cid:18)c1 + 2c2 + 12 c3 + c412 c3 + c4c1 + 2c22 c3 + c41(cid:19)S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203189has two nonnegative eigenvalues,c1 + 2c2 + c3 + 2c4 ±(1 + δ)(cid:8)(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4)2.(cid:9)Let λ(δ) be the larger one. Since D < 1 implies λ(0) < 1, we may choose a small constant δ so that λ(δ) < 1. Notethat(cid:11)is an eigenvector of T ∗(0) corresponding to λ(δ). So,(cid:19)(cid:19)(cid:18)(cid:18)(cid:18)(cid:19)p1−p∗(0) ·Tp1 − p= λ(δ)p1 − p<p1 − p,as desired. (cid:2)Then using these facts, we show that almost always the sizes of|R(t)| are O(n). In the course ofthat, we use a simplified version of Lazy-server lemma, which was introduced by Achlioptas [1]. Suppose that there isa server so that the probability that the server would work at time t is w(t) and, if it works, it can handle one task perunit time. And the expected number of tasks that arrive to the server at time t is z(t). Then Lazy-server lemma saysthat if z(t) is bounded above by w(t) uniformly for all t, then almost always the sum of sizes of the task queue overall t would be bounded linearly.|B(t)| and(cid:5)(cid:5)Lemma 4 (Lazy-server lemma). Let Z(0), Z(1), . . . be a sequence of random variables and denote z(t) = E[Z(t)].Let W (0), W (1), . . . be a sequence of independent Bernoulli random variables with density w(t), i.e. W (t) = 1 withprobability w(t), and 0 otherwise. Let Q(0), Q(1), . . . be a sequence of random variables defined by Q(0) = 0 andQ(t + 1) = max(Q(t) − W (t), 0) + Z(t). Assume that(i) there exist a constant ρ > 0 such that for all t (cid:2) 0,z(t) < w(t) − ρ;(ii) there exist constants a, b, c > 0 such that for any fixed 0 (cid:3) j1 (cid:3) j2 and β > 0,(cid:20)j2(cid:2)Prt=j1Z(t) > (1 + β)(cid:21)(cid:22)z(t)< exp−aβb(cid:22)j2(cid:2)(cid:23)(cid:23)cz(t).t=j1j2(cid:2)t=j1Then there exists constant C and K depending on ρ, a, b, c such that for every m (cid:2) 1,(cid:20)m−1(cid:2)(cid:21)PrQ(t) > Cm= O(m−2),(cid:9)Prt=0max0(cid:2)t<m(cid:11)Q(t) > logK m= O(m−2).From the Lazy-server lemma, we haveLemma 5. We almost always have(1−(cid:6))n(cid:2)(cid:10)(cid:10)(cid:10) < Cn,(cid:10)B(t)t=0(1−(cid:6))n(cid:2)(cid:10)(cid:10)(cid:10) < Cn,(cid:10)R(t)t=0(cid:10)(cid:10)(cid:10) < logK n,(cid:10)B(t)max0(cid:2)t(cid:2)(1−(cid:6))n(cid:10)(cid:10)(cid:10) < logK n,(cid:10)R(t)max0(cid:2)t(cid:2)(1−(cid:6))nfor some constants C, K.190S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203Proof. We apply Lazy-server lemma with Z(t) = b∗(t), W (t) = χ(t), w(t) = p for 0 (cid:3) t (cid:3) (1 − (cid:6))n. Condition (i)follows by Lemma 3. And condition (ii) follows by Chernoff bound. So almost always for some constants C1 = C1((cid:6))and K1 = K1((cid:6)),(1−(cid:6))n(cid:2)Q(t) < C1n andmax0(cid:2)t(cid:2)(1−(cid:6))nt=0Q(t) < logK1 n.Note that |B(t + 1)| = max(|B(t)| − χ(t), 0) + b(t), and almost always b(t) (cid:3) b∗(t) for all 0 (cid:3) t (cid:3) (1 − (cid:6))n. So, byinduction, almost always |B(t)| (cid:3) Q(t),(1−(cid:6))n(cid:2)(cid:10)(cid:10)(cid:10) < C1n,(cid:10)B(t)andt=0(cid:10)(cid:10)(cid:10) < logK1 n.(cid:10)Q(t)max0(cid:2)t(cid:2)(1−(cid:6))nSame argument can be applied to obtain that almost always there exist constants C2 = C2((cid:6)) and K2 = K2((cid:6)) suchthat(1−(cid:6))n(cid:2)t=0(cid:10)(cid:10)(cid:10) < C2n and(cid:10)R(t)(cid:10)(cid:10)(cid:10) < logK2 n.(cid:10)R(t)max0(cid:2)t(cid:2)(1−(cid:6))nThen for C = C((cid:6)) = C1 + C2 and K = K((cid:6)) = max{K1 + 1, K2 + 1}, almost always(1−(cid:6))n(cid:2)(cid:10)(cid:3)(cid:10)(cid:10) +(cid:10)B(t)(cid:10)(cid:10)(cid:10)(cid:10)R(t)(cid:4)t=0< Cn andmax0(cid:2)t<(1−(cid:6))n(cid:10)(cid:3)(cid:10)(cid:10) +(cid:10)B(t)(cid:10)(cid:10)(cid:10)(cid:10)R(t)(cid:4)< logK n.(cid:2)Now we prove that with positive probability no 0-clause is produced until t = (1 − (cid:6))n. Under the condition thatno 0-clause is produced until time t − 1, the probability that the same holds until time t is at least(cid:12)1 −12(n − t − 1)(cid:13)|B(t)|(cid:12)1 −|R(t)|2(n − t − 1)(cid:13)(cid:12)1 − 2(cid:6)n(cid:2)(cid:13)|B(t)|+|R(t)|,(3)for the literals in R(t) are strictly distinct. Therefore, the probability that no 0-clause is produced until t = (1 − (cid:6))n isat least(cid:12)(cid:13)(cid:5)(cid:13)1 − 2(cid:6)nn−(cid:6)nt=0 (|B(t)|+|R(t)|)(cid:12)1 − 2(cid:6)n(cid:2)Cn= e− 2C(cid:6) + o(1).We complete the proof of the first part of Theorem 2 by showing that when no 0-clause is produced until t =(1 − (cid:6))n, the remaining formula at t = (1 − (cid:6))n is satisfiable with positive probability. Remind that the total numberof 2-clauses and equalities remaining at the time is almost always less than 0.01(cid:6)n. Think of a multi graph G definedby the following rules. The vertices represent the Boolean variables in V ((1 − (cid:6))n). For each 2-clause or equality ofF ((1 − (cid:6))n), we set an edge between the two vertices that appear in the 2-clause or equality. Notice that if G is acyclicand no pair of unit clauses at time t = (1 − (cid:6))n belongs to the same connected component, then there exists a truthassignment of V ((1 − (cid:6))n) which satisfies F ((1 − (cid:6))n).Note thatPr[G has a cycle] (cid:3) E[number of cycles of G] =(cid:2)possible cycles CGPr[CG appears in G].The last summand is at most(cid:12)(cid:13)(cid:6)n(cid:2)k(cid:2)(cid:10)(cid:10)2VM(cid:3)(1 − (cid:6))n(cid:12)(cid:4)(cid:10)(cid:10)j ((cid:6)n)k−j(cid:12)(cid:13)j1(cid:6)n − 1|F1(n − (cid:6)n)| + |F2(n − (cid:6)n)|(cid:4)(cid:3)(cid:6)n2(cid:13)k−j,where k is the length of cycle CG, and j is the number of edges from F3((1 − (cid:6))n) or F4((1 − (cid:6))n). This summand isat most(cid:12)(cid:13)(cid:12)(cid:13)(cid:12)j0.02(cid:6)n(cid:6)n − 10.02(cid:6)n(cid:6)n − 1(cid:13)k−j< 0.1.kjkjk=2j =0∞(cid:2)k(cid:2)k=2j =0S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203191(cid:3)(cid:4)Let Ai ’s be the connected components of G. Since there are (cid:6)n vertices and at most 0.01(cid:6)n edges in G, we canshow that, almost always, |Ai| = O(log n) for all i and thus|Ai|2 = O(n log2 n) by using a similar argument as inthe well-known result in random graphs [15]. Hence, for each pair of the existing unit clauses (v1) and (v2) at timet = (1 − (cid:6))n, the probability that v1 and v2 are in Ai for some i is O. So the probability that there is a pair(cid:4)logK nof unit clauses at time t = (1 − (cid:6))n that belong to the same connected component is at most= o(1).O2Therefore, with probability higher than 0.9, the remaining formula at the time is satisfiable.log2 nnlog2 nn(cid:5)iNow consider the case that c3 + c4 = 1. Since V (0) − VM (0) is empty in this case as mentioned above, UCSmay encounter the case that χ(t) = 0 but V (t) − VM (t) is empty unless p = 1. However, when we set p as in (1),UCS may not encounter the case that χ(t) = 0 but V (t) − VM (t) is empty: Initially, |V (0) − VM (0)| = 0. At eachstep t of the first δn steps, if χ(t) = 1, then the expected change of |V (t) − VM (t)| is 1 + O(δ), as one uniformrandom literal eliminates 2 + O(δ) 2-clauses or equalities with main literals, in expectation. The other effects aresmall enough if δ is small enough. If χ(t) = 0, then the expected change is O(δ), as a nonmain literal eliminates1 + O(δ) 2-clause or equality with main literal, in average. Thus, at each step, |V (t) − VM (t)| increases by p + O(δ),in average, and hence |V (t) − VM (t)| > 0 for t (cid:2) 1 with positive probability. Notice that UCS produces 0-clause inthe first δn steps with probability O(δ) (cf. Lazy-server lemma). Therefore, with positive probability, UCS proceedsto the first δn steps without encountering χ(t) = 0 and V (t) − VM (t) = ∅. After t = δn steps, it is easy to see thatc3(t) + c4(t) (cid:3) (1 − (cid:6)0)(n − t) for some constant (cid:6)0 > 0, which is covered in the previous case.(cid:4)(cid:3)(cid:3)3.2. Supercritical regionFor a 2-SAT formula F , setting a literal x to be true produces the unit clause (l) after x is removed from eachclause (x ∨ l) in F . Again, setting the literal l to be true yields other unit clauses and the process repeats for other unitclauses, if any. The process terminates if there is no more unit clause. This process is called an implication processstarting from x, or simply an implication process if the identity of x is clear in the context. Strictly speaking, animplication process depends on the order of unit clauses chosen. We assume that there is a fixed order among allthe unit clauses, as our argument below does not depend on a particular order. For the generalized random 2-SATformula we are dealing with, there are two types of unit clauses colored blue and red as explained in the proof forthe subcritical region. So we call the process in which blue and red unit clauses are distinguished an implicationprocess with two types. If the implication process starting from x produces a 0-clause, there does not exist a satisfyingassignment setting x to be true for the formula F . In addition, if the implication process starting from x also produces a0-clause, there does not exist a satisfying assignment setting x to be true (equivalently setting x to be false) for F and,consequently, the formula F is unsatisfiable. In the following, we will prove the unsatisfiability of F (n, c1, c2, c3, c4)with D > 1 by showing that there almost always exists such a variable x that both of x and x produce 0-clauses in therandom formula. To maintain the independence among the implications in the implication process, once a 2-clause oran equality is used in the process, we remove it from the formula and do not consider it in the subsequent process.Before investigating the implication process with two types, we consider a branching process with two types, asimpler than but very close to the implication process. In the branching process, there are two types of organisms, saycolored blue and red, and each type produces both types of organisms according to a certain probability distributionindependently of the other. For an organism x, an organism is in the first generation if it is produced by x. In general,an organism is in the kth generation if it is produced by an organism in the (k − 1)th generation. We use the term“after k generations” when all organisms of kth or less generation are exposed. Suppose that a blue organism producesa blue and c red organisms in expectation and a red organism produces b blue and d red organisms in expectation.Then, the branching ratios of the process are represented by a 2 × 2 matrix(cid:18)(cid:19)A =acbd.We have the following result.Lemma 6. Suppose a two-type branching process with the branching ratio matrix(cid:18)(cid:19)A =acbd,192S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203with a (cid:2) d, and the larger eigenvalue of A is larger than 1. Then, there is a constant κ such that the expected numberof the blue organisms that are produced after κ generations, starting from one blue organism, is larger than 1.Proof. Let Bb(k) (Rb(k), resp.) be the expected numbers of blue (red, resp.) organisms produced after k generations,starting from one blue organism and Br (k) (Rr (k), resp.) be the expected numbers of blue (red, resp.) organismsproduced after k generations, starting from one red organism. Then, by induction on k,(cid:18)(cid:19)(cid:18)(cid:19)(cid:18)(cid:19)(cid:18)(cid:19)Bb(k)Rb(k)= Ak10,andBr (k)Rr (k)= Ak01.Let λA be the larger eigenvalue of A and [u, v] be the corresponding eigenvector with u + v = 1, that is,λA = a + d +(a − d)2 + 4bc,(cid:8)2and(cid:18)(cid:19)(cid:18)uv= μ(cid:8)a − d +(a − d)2 + 4bc2c(cid:19),where μ is a positive constant.If b = 0 or c = 0, a (cid:2) d and λA > 1 imply that Bb(1) = a > 1. Thus we may take κ = 1. Suppose that b > 0 andc > 0. We first choose the minimum integer K such that (λA)K min{u, v} > 2, i.e., setK =(cid:24)log2min{u,v}(cid:25)log λA+ 1.If Bb(K) > 1, we can set κ = K. If Rr (K) > 1, it is easy to see that there is a constant α so that Rr (αK) > 1.1/(bc).As, in expectation, one blue organism produces c red organisms, and one red organisms produces Rr (αK) red organ-isms after αK generations, and then each of those red organism produces b blue organisms, we have bcRr (αK), whichis at least 1.1, blue organisms in expectation after αK + 2 generations. Suppose now Bb(K) (cid:3) 1 and Rr (K) (cid:3) 1. Notethat(cid:18)(cid:19)(cid:18)(cid:19)(cid:18)(cid:19)(cid:18)uvAK10On the other hand, as [u, v] is an eigenvector of A,= uAK+ vAK01=u · Bb(K) + v · Rr (K)u · Rb(K) + v · Rr (K)(cid:19).(cid:18)(cid:19)(cid:18)AKuv=(λA)K u(λA)K v(cid:19).As (λA)K min{u, v} > 2 and all of u, v, Bb(K), and Rr (K) are less than or equal to 1, Br (K) > 1 and Rb(K) > 1 andhence(cid:18)(cid:19)(cid:18)(cid:19)(cid:18)(cid:19)(cid:18)Bb(2K)Rb(2K)= A2K10= AK AK10=Bb(K)2 + Br (K) · Rb(K)Bb(K) · Rb(K) + Rb(K) · Rr (K)(cid:19).Therefore, Bb(2K) (cid:2) Br (K)Rb(K) > 1 and we can set κ = 2K. (cid:2)It is well known that a branching process with branching ratio larger than one continues forever with positiveprobability [4]. From Lemma 6, we have an analogy for branching processes with two types.Corollary 1. Suppose a two-type branching process with the branching ratio matrix satisfying the condition inLemma 6. Then, the branching process, starting from one blue organism, continues forever with positive probabil-ity.Now we prove the second part of Theorem 2. Consider a random formula F = F (n, c1, c2, c3, c4) with D > 1and the implication process in F . Involved in the implication process, a κ-generation implication process (shortly,κ-implication process) is defined as follows. The κ-implication process consists of ordinary rounds followed by onesupplemental round. It starts from a literal chosen uniformly at random (or equivalently, the literal in a blue unitS.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203193clause). In the first ordinary round, the unit clauses in the κth or less generations from the literal are all exposed andthe literals in them are assigned truth values. In each subsequent ordinary round, if there remain blue unit clauses, oneof them is chosen, the literal in it is set, and the literals in the unit clauses in the κth or less generations are all assignedtheir truth values. Otherwise, the process is terminated. If the process proceeds by the first n2/3 ordinary rounds, allthe literals in the existing blue unit clauses are assigned their truth values in the supplemental round and the processis terminated.Lemma 7. Given F = F (n, c1, c2, c3, c4) with D > 1, for some constant κ, the κ-implication process in F startingfrom a variable x chosen uniformly at random proceeds to the supplemental round with positive probability.Proof. Consider the implication process in F starting from a variable x chosen uniformly at random. A blue or redunit clause being assigned at time t in the implication process may be regarded as if UCS sets a literal with χ(t) = 1or 0. As in proving Lemma 2, we see that, almost always,(cid:10)(cid:10)(cid:10)Fi(t)(cid:10) =(cid:10)(cid:10)(cid:10)Fi(0)(cid:10) + o(n) = cin + o(n)(4)(1 (cid:3) i (cid:3) 4) uniformly for all 0 (cid:3) t (cid:3) n1−(cid:6) for arbitrarily small (cid:6) > 0. For the implication ratios in the process, letˆa(t) and ˆc(t) ( ˆb(t) and ˆd(t), resp.) be the numbers of blue and red unit clauses produced by a blue (red, resp.) unitclause at time t. Recall that b(t) and r(t), the numbers of blue and red unit clauses coming from F (t) at time t in theprocess of UCS, were investigated in the previous section. In fact, ˆa(t) and ˆc(t) have the same distributions as b(t)and r(t) with χ(t) = 1, respectively. And, ˆb(t) and ˆd(t) have the same distributions as b(t) and r(t) with χ(t) = 0,]ˆai(t), where ˆa1(t) and ˆa2(t) have binomial distributions Bin[|F1(t)|, 1respectively. For example, ˆa(t) =n−t|F4(t)|and Bin[|F2(t)|, 2], respectively, and ˆa3(t) and ˆa4(t) have Bernoulli distributions with densities,n−tn−trespectively. Similarly, the distributions of ˆb(t), ˆc(t), and ˆd(t) can be obtained from the results in the previous section.Combined with Eq. (4), we see that the implication ratio matrix at time t is almost always(cid:19)|F3(t)|2(n−t) and(cid:5)4i=1(cid:18)T (t) =c1 + 2c2 + c3+2c4c3+2c422c1 + 2c2c3+2c42+ o(1)uniformly for all 1 (cid:3) t (cid:3) n1−(cid:6) .Consider the branching process with two types whose branching ratio matrix isA = (1 − δ)(cid:18)c1 + 2c2 + c3+2c4c3+2c422c1 + 2c2c3+2c42(cid:19).The eigenvalues of A arec1 + 2c2 + c3 + 2c4 ±(1 − δ)(cid:8)(c1 + 2c2)2 + 2(c1 + 2c2)(c3 + 2c4)2.Let ˆλ(δ) be the larger one. Since D > 1 implies ˆλ(0) > 1, we choose δ > 0 so that ˆλ(δ) > 1. Then, by Lemma 6, thereis a constant κ such that the expected number of the blue organisms produced after κ generations, starting from oneblue organism, is larger than one in the branching process. Note that, for sufficiently large n, the entries of T (t) arelarger than or equal to the corresponding entries of A for all 1 (cid:3) t (cid:3) n1−(cid:6) . By coupling the implication process withthe branching process and using Corollary 1, we see that the κ-implication process in F proceeds to the supplementalround with positive probability. (cid:2)Consider the κ-implication process in F starting from x chosen uniformly at random, where κ is specified as inLemma 7. Now consider the condition that the κ-implication process proceeds to the supplemental round. Let thestrictly distinct literals in the blue unit clauses existing in the beginning of the supplemental round be y1, . . . , yL,where L is the number of the literals. Since the process almost always produces in expectation more than one blueunit clause in each of the n2/3 ordinary rounds, by the large deviation result [4], L is almost always (cid:4)(n2/3). Let thevariables that have not occurred in the ordinary rounds be z1, . . . , zM , where M is the number of such variables andalmost always n + o(n). Note that D > 1 implies c1 > 0 or c2 > 0. Suppose that c1 > 0. Denote by ˆF1 the formulaconsisting of the 2-clauses remaining in F1 in the beginning of the supplemental round. Then, | ˆF1| is almost always194S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203c1n + o(n). Define the random variable Xi for 1 (cid:3) i (cid:3) M such that Xi = 1 if the unit clauses (zi) and (zi) areproduced from the clauses in ˆF1 by setting yj ’s and Xi = 0 otherwise. Define the random variable Xi,1 (Xi,2, resp.)for 1 (cid:3) i (cid:3) M such that Xi,1 = 1 (Xi,2 = 1, resp.) if the unit clause (zi) ((zi), resp.) is produced from the clauses inˆF1 by setting yj ’s and Xi,1 = 0 (Xi,2 = 0, resp.) otherwise. Note thatˆF1 consists of 2-clauses chosen uniformly at(cid:20)s. Thus,random among all the 2-clauses over the literals yi ’s, yi ’s, zi ’s, and zi(cid:9)neither (yj ∨ zi) nor (zi ∨ yj ) are not in ˆF1 for all 1 (cid:3) j (cid:3) LPr[Xi,1 = 0] = Pr(cid:11)(cid:12)(cid:13)| ˆF1|=1 − 2LUU )| ˆF1| and Pr[Xi,1 = 0 and Xi,2 = 0] =,where U = (2M + 2L)(2M + 2L − 2). Similarly, Pr[Xi,2 = 0] = (1 − 2L(1 − 4LU )| ˆF1|. Hence,Pr[Xi = 1] = Pr[Xi,1 = 1 and Xi,2 = 1](cid:12)= 1 − Pr[Xi,1 = 0 or Xi,2 = 0](cid:12)1 − 2LU= 1 −(cid:13)| ˆF1|(cid:12)1 − 4LU−2(cid:13)| ˆF1|(cid:13).(5)Again, consider the condition that L = (cid:4)(n2/3), M = n + o(n), and | ˆF1| = c1n + o(n), which hold almost always.(cid:3)| ˆF1|(cid:4)U )2 − (cid:4)(n−1) = (cid:4)(n−2/3) by the binomial expansion with( LMi=1 Xi . Since E[Xi] = Pr[Xi = 1] = 8(cid:5)2Let X =Eq. (5),E[X] = (cid:4)(M · n−2/3) = (cid:4)(n1/3).And, we see that Var[Xi] = E[X2i] − E[Xi]2 = (cid:4)(n−2/3) − (cid:4)(n−4/3) = (cid:4)(n−2/3). SinceE[XiXj ] = Pr[Xi = 1 and Xj = 1] = 1 − Pr[Xi,1 = 0 or Xi,2 = 0 or Xj,1 = 0 or Xj,2 = 0],which is1 −and(cid:12)(cid:12)41(cid:13)(cid:12)1 − 2LU(cid:13)| ˆF1|(cid:12)42−(cid:13)(cid:12)1 − 4LU(cid:13)| ˆF1|(cid:12)+(cid:13)(cid:12)431 − 6LU(cid:13)| ˆF1|(cid:12)(cid:13)(cid:12)44−1 − 8LU(cid:13)| ˆF1|(cid:13),E[Xi] = Pr[Xi = 1] = 1 −2the binomial expansion says that(cid:12)(cid:12)1 − 2LU(cid:13)| ˆF1|(cid:12)−1 − 4LU(cid:13)| ˆF1|(cid:13),Cov[Xi, Xj ] = E[XiXj ] − E[Xi]E[Xj ] = E[XiXj ] − E[Xi]2 = −(cid:4)(n−7/3).Thus,Var[X] =M(cid:2)i=1Var[Xi] +(cid:2)i(cid:12)=jCov[Xi, Xj ] = (cid:4)(M · n−2/3) − (cid:4)(M 2 · n−7/3) = (cid:4)(n1/3).By Chebyshev inequality [4],Pr[X = 0] (cid:3) Var[X]E[X]2= (cid:4)(n−1/3).Hence, a 0-clause is almost always produced in the supplemental round. For the case that c1 = 0 and c2 > 0, this factcan be obtained in a similar way by considering the equalities remaining in F2.Now we have that the κ-implication process in F starting from a variable x chosen uniformly at random producesa 0-clause with positive probability. Suppose that the process produces a 0-clause. In the case that F consists ofequalities only, the fact that setting x to be true produces a 0-clause implies that setting x to be true also producesa 0-clause. In the other cases, we remove all the 2-clauses, which contain the underlying variables of the literalsoccurred in the κ-implication process, from F , except for the 2-clauses that contain the literal x. Since there remainS.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203195(cid:4)(n) uniform random 2-clauses or uniform random equalities almost always, setting x to be true produces a blueunit clause in at most two generations with positive probability and we have another κ-implication process startingfrom the blue unit clause. Note that, for the new process, the distributions mentioned in the above argument almostalways do not change asymptotically. Hence, by the same argument, the new process produces a 0-clause with positiveprobability. In summary, with positive probability, 0-clauses are produced from both x and x for a variable x.In the case that the κ-implication process starting from a variable x does not produce any 0-clause, we removeall the 2-clauses and equalities that contain the underlying variables of the literals occurred in the process. Then, wechoose one of the remaining variables uniformly at random and consider the κ-implication processes starting fromthe variable and its negation. Since the expectation of the number of the exposed variables in a κ-implication processis O(n2/3), the probability that the number of the exposed variables is greater than n3/4 is O(n−1/12) by Markovinequality [4]. This means that, when we consider the successive κ-implication processes starting from (cid:4)(log n)different literals, the distributions mentioned in the above argument almost always do not change asymptotically. Sothe probability, that there does not occur a variable such that 0-clauses are produced from the variable and its negationuntil log n variables are successively chosen, approaches to zero as n goes to infinity. This means that the randomformula F is almost always unsatisfiable.4. Solubility of NK(n, 2, z)In this section, we prove Theorem 1 for the model NK(n, 2, z). This is enough as NK(n, 2, z) is essentially the√same as NK(n, 2, z). Recall z0 = 27−7≈ 2.837. In the first subsection, the result for the subcritical region z < z0 is4proven. The next subsection is for another proof for the supercritical region. By the monotonicity of the solubility ofNK(n, 2, z), it is enough to consider cases 2 < z < z0 and z0 < z < 3.54.1. Subcritical regionAs in NK(n, 2, z), a 3-SAT formula F can be reduced from a random instance f of NK(n, 2, z). More precisely, a3-SAT formula Lj is reduced from each local fitness function fj of f and F is the conjunction of Lj ’s. We call Lja local formula. Then each local formula consists of two 3-clauses with probability 1 − h and three 3-clauses withprobability h where z = 2 + h, independently of all other local formulae. Main variables or its negations appearingin a local formula are called main (copies of) literals of the local formula. Note that any pair of main literals fromdifferent Lj ’s are strictly distinct. As in the generalized random 2-SAT model, we apply UCS to F and show thatUCS succeeds to find a satisfying assignment of F with positive probability. In the process of UCS, there appearfour types of 2-clauses or equalities as presented in the generalized 2-SAT problem. Denoted by Fi(t) (1 (cid:3) i (cid:3) 4)is the 2-SAT formula consisting of the 2-clauses or equalities of type i at time t. Denoted by F5(t) is the 3-SATformula consisting of the remaining local formulae at time t. Let |Fi(t)| be the number of the 2-clauses or equalitiesin Fi(t) and |F5(t)| be the number of the local formulae in F5(t) at time t. Then it is clear that Fi(0) is empty for1 (cid:3) i (cid:3) 4 and |F5(0)| = n. Let VM (t) be the set of the underlying variables of the main literals of F3(t), F4(t) andF5(t). We consider that in the process of UCS, unit clauses consisting of main literals and the copies of literals thereinare colored red and other unit clauses and the copies of literals therein are colored blue. As in Section 3, we let B(t)(R(t), respectively) be the set of blue (red, respectively) unit clauses at time t.For this problem, we run UCS withp = p(t) = p0 − t10n,√5−12where p0 =≈ 0.618. As one can see later, we defined p(t) so that the expected number of blue (red, re-spectively) unit clauses produced at each time t is uniformly bounded above by p(t) (1 − p(t), respectively) for1 (cid:3) t (cid:3) (1 − (cid:6))n, where (cid:6) is a small constant. Then by a coupling argument and Lazy-server lemma, we obtain that|R(t)| are O(n). Then we show that, with positive probability, no 0-clause isthe sizes ofproduced until t = (1 − (cid:6))n. At t = (1 − (cid:6))n, the remaining formula is sparse enough that it is satisfiable with positiveprobability.|B(t)| and(1−(cid:6))nt=0(1−(cid:6))nt=0(cid:5)(cid:5)As in the case of generalized 2-SAT problem, it is not hard to see that at each time t, F1(t) consists of uniformrandom 2-clauses over V (t), and F2(t) consists of uniform random equalities over V (t). The formula F3(t) consists196S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203of random 2-clauses with main variables over V (t), and F4(t) consists of random equalities with main variables overV (t), where the main literals in F3(t) and F4(t) and F5(t) are pairwise strictly distinct: For clauses and equalities inFi(t), i = 1, . . . , 4, one may use the same argument as in the case of the generalized 2-SAT problem. Once a variableis set to be 0 or 1, a local formula may be reduced to 0, 1, 2 or 3 2-clauses before sublimation. If three 2-clauses arecreated, then they may be further reduced to two unit clauses. (See the sublimation procedure described in Section2.) Clearly, those unit clauses have the desired randomness by the same argument. If two 2-clauses are created, thenthey are further reduced to a unit clause or an equality, but not both and, in either case, the desired randomness holds.If one 2-clause is created, then it clearly has the randomness. The case that no 2-clause is created has no need to beconcerned.Suppose a variable is set to be 1 or 0. For the literals of the variable appeared in the first coordinates or in othercoordinates of clauses or equalities without main variables, the other literals are uniform random among all remainingliterals. For the literals of the variable appeared in the second or third coordinates of clauses and equalities with mainvariables, we first specify the number a of such clauses and equalities and then, conditioned on the number, all a mainvariables are equally likely to be in the first coordinate of the clauses and equalities and the other variables in such3-clauses are uniform random.It is not hard to see that at each time t, F (t) has the same distribution as F (n − t, c1(t), c2(t), c3(t), c4(t)), whereci(t) = |Fi(t)|/(n − t): Suppose a variable is set to be 1 or 0. For the literals of the variable appeared in the firstcoordinates or in the second coordinates of 2-clauses or equalities without main variables, the other literals are uniformrandom among all remaining literals. For the literals of the variable appeared in the second coordinates of 2-clausesand equalities with main variables, we first specify the number a of such 2-clauses and equalities and then, conditionedon the number, all a main variables are equally likely to be in the first coordinate of the 2-clauses and equalities. Thismay be also illustrated using a card game described in [2].Let bi(t) and ri(t) be the numbers of blue and red unit clauses coming from Fi(t) at time t, respectively. Asmentioned in Section 2, during the execution of UCS, there occur some sublimations. So we also consider b5(t)(r5(t), respectively), the number of blue (red, respectively) unit clauses produced by sublimations at time t.As in the generalized 2-SAT model, we investigate E[|Fi(t + 1)| − |Fi(t)|], E[bi(t)] and E[ri(t)] (1 (cid:3) i (cid:3) 5) anduse Wormald theorem to obtain approximations of |Fi(t)|, and then obtain approximations of E[bi(t)], and E[ri(t)].For 1 (cid:3) i (cid:3) 4, let ui(t) be the number of 2-clauses or equalities that come from F5(t) to Fi(t) at time t, and di(t) be thenumber of 2-clauses or equalities that are removed from Fi(t) at time t. Then for 1 (cid:3) i (cid:3) 4, E[|Fi(t + 1)| − |Fi(t)|] =E[ui(t)] − E[di(t)]. As we already obtained E[di(t)], E[bi(t)] and E[ri(t)] (1 (cid:3) i (cid:3) 4) in Section 3, we only need toobtain E[ui(t)] (1 (cid:3) i (cid:3) 4), E[b5(t)], E[r5(t)] and E[|F5(t + 1)| − |F5(t)|].Lemma 8. For any small (cid:6) > 0, we have for all 0 (cid:3) t (cid:3) (1 − (cid:6))n,(cid:10)(cid:9)(cid:10)(cid:10) −(cid:10)F1(t + 1)(cid:9)(cid:10)(cid:10)(cid:10)F2(t + 1)(cid:10) −(cid:10)(cid:9)(cid:10)(cid:10) −(cid:10)F3(t + 1)(cid:9)(cid:10)(cid:10)(cid:10)F4(t + 1)(cid:10) −(cid:9)(cid:10)(cid:10)(cid:10)F5(t + 1)(cid:10) −EEEEE(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)(cid:10)(cid:10)(cid:10)F1(t)(cid:10)(cid:10)(cid:10)(cid:10)F2(t)(cid:10)(cid:10)(cid:10)(cid:10)F3(t)(cid:10)(cid:10)(cid:10)(cid:10)F4(t)(cid:10)(cid:10)(cid:10)(cid:10)F5(t)(cid:10)= − 2|F1(t)|n − t= − 2|F2(t)|n − t+ p(t)(cid:12)+ p(t)= −= −= −(cid:3)1 + p(t)(cid:3)1 + p(t)(cid:3)2 + p(t)(cid:4) |F3(t)|n − t(cid:4) |F4(t)|n − t(cid:4) |F5(t)|n − t(cid:12)(cid:13)47114+,,|F5(t)|n − t(cid:13)|F5(t)|n − t(cid:13)− h7+ h14(cid:12)− 2h877(cid:13)+ h177(cid:12)|F5(t)|n − t|F5(t)|n − t++ o(1).+ o(1),+ o(1),Proof. Let L be a local formula of F5(t). Recall that the probability that L is a conjunction of two (or three) 3-clausesis 1 − h (or h) independently of all other local formulae. Denoted by |L| is the number of 3-clauses of L. We writem(L) for the main variable of L. Suppose a literal l is set to be true at time t. For convenience, we define an indexI (L, l) to be an ordered pair (a, b), where a = 1(2, respectively) means that l and the main (a neighborhood) variableof L are not strictly distinct, and b indicates how many copies of l appears in the 3-clauses of L. For example,I (L, l) = (1, 2) means that m(L) and l are not strictly distinct and l appears in two 3-clauses of L.S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203197Now we obtain E[u1(t)]. Note that one uniform random 2-clause is produced from L only when |L| = 2, χ(t) =1, I (L, l) = (1, 1) or |L| = 3, χ(t) = 1, I (L, l) = (1, 2). So the corresponding expected numbers of new uniformrandom 2-clauses that come from F5(t) are(1 − h) × p(t) ×|F5(t)|n − t× 4 · 428and h × p(t) ×|F5(t)|n − t×and hence(cid:9)(cid:11)u1(t)E(cid:12)= p(t)(cid:13)|F5(t)|n − t.47− h7(cid:4)(cid:3)44256,Similarly, contributions for u2(t) from L are made only when |L| = 2, χ = 1, I (L, l) = (1, 0) or |L| = 3, χ =1, I (L, l) = (1, 1). So the corresponding expected numbers of new uniform random equalities that come from F5(t)are(1 − h) × p(t) ×|F5(t)|n − t× 228and h × p(t) ×|F5(t)|n − t× 2 · 456,and hence(cid:9)(cid:11)u2(t)E(cid:12)= p(t)114+ h14(cid:13)|F5(t)|n − t.For u3(t), the cases are |L| = 2, χ = 1, I (L, l) = (2, 1) and |L| = 2, χ = 0, I (L, l) = (2, 1) and |L| = 3, χ =1, I (L, l) = (2, 2) and |L| = 3, χ = 0, I (L, l) = (2, 2). The corresponding expected numbers of new random2-clauses with main literals that come from F5(t) are(1 − h) × p(t) × 2|F5(t)|n − t× 4 · 428,(1 − h) ×(cid:4)(cid:3)1 − p(t)× 2|F5(t)|n − t − 1× 4 · 428andh × p(t) × 2|F5(t)|n − t×(cid:4)4 ·(cid:3)4256,(cid:4)(cid:3)1 − p(t)h ×× 2|F5(t)|n − t − 1×4 ·(cid:3)(cid:4)4256,and hence(cid:9)(cid:11)u3(t)E=(cid:12)87− 2h7(cid:13)|F5(t)|n − t+ o(1).For u4(t), the cases are |L| = 2, χ = 1, I (L, l) = (2, 0)) and |L| = 2, χ = 0, I (L, l) = (2, 0) and |L| = 3, χ = 1,I (L, l) = (2, 1) and |L| = 3, χ = 0, I (L, l) = (2, 1). The corresponding expected numbers are(1 − h) × p(t) × 2|F5(t)|n − t× 228,(1 − h) ×(cid:4)(cid:3)1 − p(t)× 2|F5(t)|n − t − 1× 228andh × p(t) × 2|F5(t)|n − t× 2 · 456,(cid:4)(cid:3)1 − p(t)h ×× 2|F5(t)|n − t − 1× 2 · 456,and hence(cid:9)(cid:11)u4(t)E=(cid:12)(cid:13)|F5(t)|n − t17− h7+ o(1).Similarly,(cid:11)(cid:9)b5(t)E=(cid:9)(cid:11)r5(t)E=(cid:12)(cid:4)(cid:3)1 + p(t)(cid:12)(cid:13)17+ 2h7+ 2h177|F5(t)|n − t(cid:13)|F5(t)|n − t+ o(1),+ o(1),and(cid:9)(cid:10)(cid:10)(cid:10)F5(t + 1)(cid:10) −E(cid:11)(cid:10)(cid:10)(cid:10)F5(t)(cid:10)= −(cid:3)2 + p(t)(cid:4) |F5(t)|n − t+ o(1).(cid:2)198S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203For any small constant (cid:6) > 0 we can apply Wormald’s theorem to approximate |Fi(t)| uniformly for 1 (cid:3) t (cid:3)(1 − (cid:6))n. For the solution ϕi(x) : [0, 1 − (cid:6)] → R of the following system of differential equations,ϕ5(x)1−x(cid:4)⎧(cid:4)⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩dϕ1dxdϕ2dxdϕ3dxdϕ4dxdϕ5dx= − 2ϕ1(x)1−x= − 2ϕ2(x)1−x(cid:3)+ (p0 − 0.1x)(cid:3)+ (p0 − 0.1x)= −(1 + p0 − 0.1x) ϕ3(x)1−x= −(1 + p0 − 0.1x) ϕ4(x)1−x= −(2 + p0 − 0.1x) ϕ5(x)1−x47114++− h7+ h14(cid:3)− 2h7(cid:4)+ h78717(cid:3)ϕ5(x)1−x(cid:4)ϕ5(x)1−xϕ5(x)1−xϕ1(0) = 0,ϕ2(0) = 0,ϕ3(0) = 0,ϕ4(0) = 0,ϕ5(0) = 1,almost always(cid:10)(cid:10)(cid:10)Fi(t)(cid:10) = ϕi(cid:12)(cid:13)tn· n + o(n)uniformly for all 1 (cid:3) t (cid:3) (1 − (cid:6))n and 1 (cid:3) i (cid:3) 5.(cid:5)5i=1ϕi (1−(cid:6))Now we choose (cid:6) > 0 so that(cid:6) < 0.01, i.e., the total number of 2-clauses, equalities, and local formulaeremaining at time t = (1 − (cid:6))n is almost always less than 0.01(cid:6)n.Lemma 9. There exists a constant δ > 0 such that, E[b(t)] < p(t) − δ and E[r(t)] < 1 − p(t) − δ, almost always anduniformly for all 0 (cid:3) t (cid:3) (1 − (cid:6))n.Proof. The expectations of bi(t) and ri(t) are as follows.(cid:18)(cid:19)(cid:19)(cid:18)E[bi(t)]E[ri(t)]= Ti(t) ·p(t)1 − p(t)+ o(1),whereT1(t) =T3(t) =(cid:19),T2(t) =,(cid:18)T4(t) =(cid:19)(cid:18)(cid:18)1 10 0(cid:19)|F1(t)|(n − t)|F3(t)|(n − t)( 171021122+ 2h7 )|F5(t)|(n − t)|F2(t)|(n − t)|F4(t)|(n − t)(cid:18)(cid:19)2 20 0(cid:18),(cid:19)1 01 1,Let b(t) =(cid:18)T5(t) =(cid:5)5i=1 bi(t) and r(t) =(cid:19)p(t)1 − p(t)E[b(t)]E[r(t)]= T (t) ·(cid:18).2 11 1(cid:5)5i=1 ri(t). Then for T (t) =(cid:19)(cid:5)5i=1 Ti(t),+ o(1).So(cid:18)E[b(t)] = 1n − t+ p(t)(cid:12)= 1n − t(cid:9)E(cid:11)r(t)(cid:12)(cid:13)(cid:10)(cid:10)(cid:10) + 2(cid:10)F1(t)(cid:12)+|F3(t)|2(cid:10)(cid:10)(cid:10) + 2(cid:10)F3(t)(cid:10)(cid:10)(cid:10)F2(t)(cid:10) +(cid:10)(cid:10)(cid:10) +(cid:10)F4(t)(cid:12)(cid:10)(cid:10)(cid:10)F4(t)(cid:10) +17(cid:12)1717(cid:10)(cid:10)+ 2h(cid:10)(cid:10)F5(t)7(cid:13)(cid:10)(cid:10)+ 2h(cid:10)(cid:10)F5(t)7(cid:13)(cid:10)(cid:10)+ 2h(cid:10)(cid:10)F5(t)7(cid:13)(cid:13)(cid:19)+ o(1),+ o(1).(6)Note that(cid:18)p01 − p0(cid:20)(cid:19)=(cid:21)√(5−1)2√1 − (5−1)2is an eigenvector of(cid:12)T (0) =17+ 2h7S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203199(cid:13) (cid:18)(cid:19)2 11 1and the corresponding eigenvalue is less than one if and only if z < z0.From (6), by letting(cid:12)ϕb(x) = 11 − xϕ1(x) + 2ϕ2(x) +and(cid:12)ϕr (x) = 11 − xϕ3(x) + 2ϕ4(x) +(cid:12)(cid:12)17+ 2h7(cid:13)ϕ5(x) + p0 − 0.1x2(cid:12)ϕ3(x) + 2ϕ4(x) +(cid:12)17+ 2h7(cid:13)(cid:13)(cid:13)ϕ5(x)(cid:13)(cid:13)ϕ5(x),17+ 2h7we obtain that E[b(t)] = ϕb(t/n) + o(1) and E[r(t)] = ϕr (t/n) + o(1). So, if we show that ϕb(x) < p0 − 0.1x andϕr (x) < 1 − (p0 − 0.1x) for 0 (cid:3) x < 1, we obtain the required result. By inserting the formulae for dϕidx ’s and bythe fact that ϕi(x)’s are nonnegative, we obtain that ϕb(0) < p0, ϕ(cid:20)b (x) < 0 for 0 (cid:3) x < 1. Soϕb(x) < p0 − 0.1x. Similarly, ϕr (0) < 1 − p0, ϕ(cid:20)r (0) < 0.1 and ϕ(cid:20)(cid:20)r (x) < 0. So ϕr (x) < 1 − (p0 − 0.1x). It is worthnoting that intuitively we have defined p(t) so that p(nx) is a line between the curves ϕb(x) and 1 − ϕr (x), using thefacts that ϕb(x) and ϕr (x) are convex. (cid:2)b(0) < −0.1 and ϕ(cid:20)(cid:20)Now as in the generalized random 2-SAT model with D < 1 and c3 + c4 = 1, for some small constant δ > 0, at eachstep t of the first δn steps |V (t) − VM (t)| increases in average. Hence by the same argument, with positive probabilityUCS proceeds to the first δn steps without encountering χ(t) = 0 and V (t) − VM (t) = ∅, and without producing any0-clauses.Note that |V (t)| = n − t and almost always, |VM (t)| = |F3(t)| + |F4(t)| + |F5(t)| = n(ϕ3(t/n) + ϕ4(t/n) +ϕ5(t/n)) + o(n). And we can obtain that for all 0 < x < 1 − (cid:6), ϕ3(x) + ϕ4(x) + ϕ5(x) < 1 − x, by using the formulaefor dϕidx ’s (i = 3, 4, 5) and the fact that ϕi(x)’s are nonnegative. So under the condition that UCS proceeds to the firstδn steps, almost always UCS does not encounter the case that χ(t) = 0 and V (t) − VM (t) = ∅ for δn (cid:3) t (cid:3) (1 − (cid:6))n.Then as in the generalized random 2-SAT model with D < 1, using the coupling argument and the Lazy-server(cid:5)|R(t)| are bounded by O(n). Now we show thatlemma, we obtain that almost alwayswith positive probability no 0-clause is produced until t = (1 − (cid:6))n. Under the condition that no 0-clause is produceduntil time t − 1, the probability that the same holds until time t is at least|B(t)| and(1−(cid:6))nt=1(1−(cid:6))nt=1(cid:5)(cid:12)1 −12(n − t − 1)(cid:13)|B(t)|(cid:12)1 −|R(t)|2(n − t − 1)(cid:13)(cid:12)(cid:2)1 − 2(cid:6)n(cid:13)|B(t)|+|R(t)|.(7)Hence, the probability that no 0-clause is produced until t = (1 − (cid:6))n is at least(cid:12)1 − 2(cid:6)n(cid:12)1 − 2(cid:6)nn−(cid:6)nt=0 (|B(t)|+|R(t)|)(cid:6) + o(1),= e(cid:13)(cid:5)− 2C(cid:2)Cn(cid:13)which is a positive constant.Now we only need to show that under the condition that no 0-clause is produced until t = (1 − (cid:6))n, the remainingformula at time t = (1 − (cid:6))n is satisfiable with positive probability. Remind that from our choice of (cid:6), the total numberof 2-clauses, equalities, and local formulae remaining at time t = (1 − (cid:6))n is almost always less than 0.01(cid:6)n. For eachlocal formula remaining as in the original form at time t = (1 − (cid:6))n, we consider conjunction of 2-clauses obtainedby removing the main literals of the local formula. Then, apply the sublimation or replace two 2-clauses like (l1 ∨ l2),(l1 ∨ l2) by the equality l1 = l2, but not both. (It is easy to check that only one of the sublimation and the replacementis needed.) As in the proof of the generalized random 2-SAT model with D < 1, we think of the following multi-graphG. The vertices represent the Boolean variables in V ((1 − (cid:6))n). For each 2-clause or equality at time t = (1 − (cid:6))nincluding equality reduced from the local formula, we set an edge between the two variables that appear in the 2-clauseor equality. Notice that if G is acyclic, and no pair of unit clauses at time t = (1 − (cid:6))n belongs to the same connectedcomponent of G, then there exists a truth assignment of V ((1 − (cid:6))n) which satisfies the remaining formula at timet = (1 − (cid:6))n. The same argument used to analyze the generalized random 2-SAT model with D < 1 yields the desiredevents occur simultaneously with positive probability.200S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–2034.2. Supercritical regionAs in the previous section, let F be the 3-SAT formula reduced from a random instance f of NK(n, 2, z). Forz0 < z < 3, local formulae in F consist of two 3-clauses or three 3-clauses. A local formula with two 3-clausesmay have the form of (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3), which is equivalent to the 2-clause (l2 ∨ l3). According to theposition of the negation, the 2-clause is a uniform random 2-clause or a random 2-clause with main literal. A localformula with three 3-clauses may have the form of (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3), which is equivalent to(l1 ∨ l2) ∧ (l1 ∨ l3). We call such a conjunction of two 2-clauses a cherry as the shape of the relations among literalslooks like it. According to the positions of the negations, the literal appearing twice in a cherry may be a main literalor not. If it is a main literal, the cherry is called a symmetric cherry. Otherwise, it is called an asymmetric cherry.A local formula with three 3-clauses may have the form of (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3) ∧ (l1 ∨ l2 ∨ l3), which impliesthe 2-clause (l2 ∨ l3). (The converse does not hold.) The 2-clause is a uniform random 2-clause or a random 2-clausewith main literal according to the positions of the negations.This way, we resolve the local formulae in F into four types of 2-clauses and cherries: uniform random 2-clauses,asymmetric cherries, random 2-clauses with main literals, and symmetric cherries. Note that the unsatisfiability of the2-SAT formula consisting of the 2-clauses and cherries obtained from the resolution implies the unsatisfiability of F .In fact, we prove the unsatisfiability of F by showing the unsatisfiability of the 2-SAT formula. To do this, we definea generalized random 2-SAT formula and examine its satisfiability. The second part of Theorem 1 is obtained as acorollary.The generalized random 2-SAT formula has four types of random 2-clauses and cherries. The first type consists ofuniform random 2-clauses. The second type consists of random asymmetric cherries of the form (ri ∨ui1)∧(ui1 ∨ui2),where ui1 and ui2 being strictly distinct with ri are chosen uniformly at random. The third type consists of random2-clauses with main literals. The fourth type consists of random symmetric cherries of the form (ri ∨ ui1) ∧ (ri ∨ ui2),where ui1 and ui2 being strictly distinct with ri are chosen uniformly at random. The copies of the literals ri ’s incherries and the literals in the first places in random 2-clauses with main literals are pairwise strictly distinct andthey are called main literals in the random 2-SAT formula. Let c1, c2, c3 and c4 be nonnegative real numbers withc2 + c3 + c4 (cid:3) 1. Denote Fi = Fi(n, ci) the conjunction of cin 2-clauses or cherries of type i (1 (cid:3) i (cid:3) 4). Denotedby F (n, c1, c2, c3, c4) is the conjunction of the four random formulae with pairwise strictly distinct main literals. TheparameterD = (c1 + c2) + (c2 + c3 + 2c4) − (c2 + c3 + 2c4)24essentially determines the branching ratio in F (n, c1, c2, c3, c4). The satisfiability of F (n, c1, c2, c3, c4) can be de-scribed in terms of D as follows.Theorem 4. If D < 1, then there exists α > 0 depending on ci ’s so that the probability of F (n, c1, c2, c3, c4) beingsatisfiable is at least α as n goes to infinity. If D > 1, then the random formula is almost always unsatisfiable.Theorem 4 can be proved in a similar way as the proof of Theorem 2. We present the proof for the second part ofTheorem 4 in Appendix for the completeness of the proof in this section.Now we prove the second part of Theorem 1. As mentioned above, a local formula in F is resolved into a uniformrandom 2-clause (l2 ∨l3) when it has the form of (l1 ∨l2 ∨l3)∧(l1 ∨l2 ∨l3) or (l1 ∨l2 ∨l3)∧(l1 ∨l2 ∨l3)∧(l1 ∨l2 ∨l3).If we let z = 2 + h (z0 − 2 < h < 1), this takes place with probability 17 . So, the probability that alocal formula is resolved into a uniform random 2-clause is 17 . In a similar way, we see the following: The probabilityof a local formula being resolved into an asymmetric cherry is 27 h, the probability for a 2-clause with main literalis 27 h. Then, the expected numbers of uniform random 2-clauses,asymmetric cherries, 2-clauses with main literals, and symmetric cherries obtained from F are 17 n, and17 hn, respectively.7 , and the probability for a symmetric cherry is 17 (1 − h) = 1Since each local formula is created independently of other local formulae, the numbers of 2-clauses and cherriesobtained from F are highly concentrated around their expectations. Let c1 = 17 h, andci(δ) = ci − δ for 1 (cid:3) i (cid:3) 4. Then, by the large deviation result [4], the numbers of 2-clauses and cherries of four7 h, c3 = 27 , c2 = 27 , c4 = 17 h + 17 hn, 27 n, 2S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203201types are almost always larger than c1(δ)n, c2(δ)n, c3(δ)n, and c4(δ)n, respectively, for arbitrarily small δ > 0. Sincez0 − 2 < h < 1 implies that c2 + c3 + c4 (cid:3) 1 and D > 1, we may choose δ > 0 so that c2(δ) + c3(δ) + c4(δ) (cid:3) 1 and(cid:3)(cid:4)c1(δ) + c2(δ)(cid:4)(cid:3)c2(δ) + c3(δ) + 2c4(δ)+− (c2(δ) + c3(δ) + 2c4(δ))24> 1.Then, the second part of Theorem 4 implies that the random 2-SAT formula resolved from F is almost always unsat-isfiable and the proof completes.In that the 2-SAT formula resolved from F is exploited in the proof, our approach is similar to that of Gao andCulberson [21]. However, they counted the number of unsatisfiable subformulae in the resolved 2-SAT formula, whichleads to complex computation. We used branching process arguments to derive the proof in more intuitive and naturalway.5. Conclusion and remarksIn this paper, we analyzed the phase transition in NK landscape on the fixed ratio model, NK(n, 2, z). We alsoproposed a generalized random 2-SAT model and introduced a corresponding parameter D. Then a phase transitionresult for the model is obtained, that is, if D < 1, the formula is satisfiable with positive probability, and if D > 1,the formula is almost always unsatisfiable. For the proof of the subcritical region, we provided a variant of the unitclause algorithm, the unit clause algorithm with switching server policy, and analyzed it. For the supercritical region,a branching process argument was used.Using a similar argument as in the generalized random 2-SAT model, it was proved that a random instance gener-√ated by NK(n, 2, z) with z < z0 = 27−7is soluble with positive probability. To the best of our knowledge, this is4the first mathematical result that describes the behavior of NK(n, 2, z) with z < z0. We also present an another wayto prove that a random instance generated by NK(n, 2, z) with z > z0 is almost always insoluble using a branchingprocess argument. This approach is a novel one and is simpler than that of Gao and Culberson. From these results, weestablished the threshold value, z0, of the phase transition in NK(n, 2, z).We believe that our approach used for NK(n, k, z) with k = 2 works for general k (cid:2) 3 to obtain at least partial5results for the phase transition phenomenon.It is expected that NK(n, k, z) with z < z0 is soluble with probability close to 1. One of typical ways to prove sucha result is to use the sharp threshold criterion developed by Friedgut [44]. If the criterion is satisfied, then our theoremwould have implied the desired result. Though we have tried to show that our case satisfies the criterion, we are unableto prove it. A nontrivial idea seems to be required for a rigorous proof.Appendix A. The proof of the second part of Theorem 4(cid:5)Here, we prove the second part of Theorem 4. Consider a random formula F = F (n, c1, c2, c3, c4) with D > 1 andthe implication process in F starting from a literal x chosen uniformly at random. For 1 (cid:3) i (cid:3) 4, let Fi(t) denotethe conjunction of remaining 2-clauses or cherries of Fi at time t. Define |Fi(t)| to be the number of 2-clauses orcherries in Fi(t). Let F (t) = F1(t) ∧ F2(t) ∧ F3(t) ∧ F4(t). We first investigate the implication ratios in the processconditioned on Fi(t)’s. Let a(t) and c(t) (b(t) and d(t), resp.) be the numbers of blue and red unit clauses producedby a blue (red, resp.) unit clause at time t. For 1 (cid:3) i (cid:3) 4, let bi,b(t) and ri,b(t) (bi,r (t) and ri,r (t), resp.) be the numbersof blue and red unit clauses produced from Fi(t) at time t by a blue (red, resp.) unit clause. Then, a(t) =bi,b(t),c(t) =ri,r (t).bi,r (t), and d(t) =ri,b(t), b(t) =For bi,b(t)’s, we see that b1,b(t) has a binomial distribution Bin[|F1(t)|, 1n−tof a random variable with a Bernoulli distribution with densitydistribution Bin[|F2(t)|, 1n−trandom variable that has a Bernoulli distribution with densityand r4,b(t) have binomial distributions Bin[|F2(t)|,For bi,r (t)’s, b1,r (t) and b2,r (t) have binomial distributions Bin[|F1(t)|, 1n−t], b3,b(t) has a Bernoulli distribution with density], Bin[|F3(t)|,12(n−t)], b2,b(t) is a linear combination|F2(t)|2(n−t) and a random variable with a binomial|F3(t)|2(n−t) , and b4,b(t) is two times a|F4(t)|2(n−t) . For ri,b(t)’s, r1,b(t) = 0 and r2,b(t), r3,b(t),], respectively.], and Bin[|F4(t)|, 1n−t], respectively,1n−t−1] and Bin[|F2(t)|,12(n−t)(cid:5)(cid:5)(cid:5)202S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203and b3,r (t) = b4,r (t) = 0. For ri,r (t)’s, r1,r (t) = 0 and r2,r (t), r3,r (t), and r4,r (t) have binomial distributionsBin[|F2(t)|,], and Bin[|F4(t)|,], Bin[|F3(t)|,], respectively.12(n−t−1)12(n−t−1)Using similar arguments as in the proof of Theorem 2, we see that the implication ratio matrix at time t is almost1n−t−1always(cid:18)T (t) =c1 + c2 + c2+c3+2c4c2+c3+2c422c1 + c2c2+c3+2c42(cid:19)+ o(1)uniformly for all 1 (cid:3) t (cid:3) n1−(cid:6) for arbitrarily small (cid:6) > 0. Consider the branching process with two types whosebranching ratio matrix is(cid:18)A = (1 − δ)c1 + c2 + c2+c3+2c4c2+c3+2c422c1 + c2c2+c3+2c42(cid:19).The eigenvalues of A arec1 + 2c2 + c3 + 2c4 ±(1 − δ)(cid:8)(c1 + c2)2 + 2(c1 + c2)(c2 + c3 + 2c4)2.Let λ(δ) be the larger one. Since D > 1 implies λ(0) > 1, we choose δ > 0 so that λ(δ) > 1. Then, by couplingthe implication process with the branching process and using Corollary 1, we see that, for some constant κ, theκ-implication process starting from x proceeds to the supplemental round with positive probability. Conditioned thatthe κ-implication process proceeds to the supplemental round, the second moment method says that a 0-clause isalmost always produced in the supplemental round. Hence, we have that the κ-implication process starting fromx produces a 0-clause with positive probability. By considering successive κ-implication processes starting from(cid:4)(log n) different literals, we can show that a 0-clause is almost always produced from F in the same way as theproof of Theorem 2.References[1] D. Achlioptas, Setting two variables at a time yields a new lower bound for random 3-SAT, in: Proceedings of the 32nd Annual ACMSymposium on Theory of Computing, 2000, pp. 28–37.[2] D. Achlioptas, Lower bounds for random 3-SAT via differential equations, Theoretical Computer Science 265 (1–2) (2001) 159–185.[3] D. Achlioptas, G.B. Sorkin, Optimal myopic algorithms for random 3-SAT, in: Proceedings of the 41st Symposium on the Foundations ofComputer Science, 2000, pp. 590–600.[4] N. Alon, J. Spencer, The Probabilistic Method, Wiley, New York, 1992.[5] L. Altenberg, NK fitness landscapes, in: T. Bäck, D. Fogel, Z. Michalewicz (Eds.), Handbook of Evolutionary Computation, Oxford UniversityPress, 1997.[6] C. Amitrano, L. Peliti, M. Saber, Population dynamics in a spin-glass model of chemical evolution, Journal of Molecular Evolution 29 (1989)513–525.[7] B. Bollobás, C. Borgs, J. Chayes, J.H. Kim, D.B. Wilson, The scaling window of the 2-SAT transition, Random Structures and Algorithms 18(2001) 201–256.[8] M.T. Chao, J. Franco, Probabilistic analysis of two heuristics for the 3-satisfiability problem, SIAM Journal on Computing 15 (4) (1986)1106–1118.[9] M.T. Chao, J. Franco, Probabilistic analysis of a generalization of the unit-clause literal selection heuristics for the k-satisfiability problem,Information Science 51 (3) (1990) 289–314.[10] V. Chvátal, B. Reed, Mick gets some (the odds are on his side), in: Proceedings of the 33th Annual Symposium on Foundations of ComputerScience, 1992, pp. 620—627.[11] W. Fernandez de la Vega, On random 2-SAT, Unpublished manuscript, 1992.[12] O. Dubois, Y. Boufkhad, J. Mandler, Typical random 3-SAT formulae and the satisfiability threshold, in: Proceedings of the 11th AnnualACM–SIAM Symposium on Discrete Algorithms, 2000, pp. 126–127.[13] R. Durrett, V. Limic, Rigorous results for the NK model, Annals of Probability 31 (4) (2003) 1713–1753.[14] M. Eigen, J. McCaskill, P. Schuster, The molecular quasispecies, Advanced Chemical Physics 75 (1989) 149–263.[15] P. Erd˝os, A. Rényi, On the evolution of random graphs, in: Publication of the Mathematical Institute of the Hungarian Academy of Science,1960, pp. 17–61.[16] S.N. Evans, D. Steinsaltz, Estimating some features of NK fitness landscapes, Annals of Applied Probability 12 (2002) 1299–1321.[17] W. Ewens, Mathematical Population Genetics, Springer-Verlag, 1979.[18] H. Flyvbjerg, B. Lautrup, Evolution in a rugged fitness landscape, Physical Review A 46 (1992) 6714–6723.[19] W. Fontana, P.F. Stadier, E.G. Bornberg-Bauer, T. Griesmacher, I.L. Hofacker, M. Tacker, P. Tarazona, E.D. Weinberger, P. Schuster, RNAfolding and combinatory landscapes, Physical Review E 47 (1993) 2083–2099.S.-S. Choi et al. / Artificial Intelligence 172 (2008) 179–203203[20] I. Franklin, R. Lewontin, Is the gene the unit of selection? Genetics 65 (1970) 707–734.[21] Y. Gao, J. Culberson, An analysis of phase transition in NK landscapes, Journal of Artificial Intelligence Research 17 (2002) 309–332.[22] Y. Gao, J. Culberson, On the treewidth of NK landscapes, in: Proceedings of the Genetic and Evolutionary Computation Conference, 2003,pp. 948–954.[23] A. Goerdt, A threshold for unsatisfiability, Journal of Computer and System Sciences 53 (3) (1996) 469–486.[24] W. Hordijk, A measure of landscapes, Evolutionary Computation 4 (4) (1997) 335–360.[25] S. Janson, Y.C. Stamatiou, M. Vamvakari, Bounding the unsatisfiability threshold of random 3-SAT, Random Structures and Algorithms 17(2000) 103–116.[26] T. Jones, S. Forrest, Fitness distance correlation as a measure of problem difficulty for genetic algorithms, in: Proceedings of the SixthInternational Conference on Genetic Algorithms, 1995, pp. 184–192.[27] A. Kaporis, L. Kirousis, Y. Stamatiou, M. Vamvakari, M. Zito, The unsatisfiability threshold revisited, submitted for publication.[28] S.A. Kauffman, Adaptation on rugged fitness landscapes, in: D. Stein (Ed.), Lectures in the Sciences of Complexity, Addison Wesley, 1989,pp. 527–618. Santa Fe Institute Studies in the Sciences of Complexity.[29] S.A. Kauffman, S. Levin, Towards a general theory of adaptive walks on rugged landscapes, Journal of Theoretical Biology 128 (1987) 11–45.[30] S.A. Kauffman, E.D. Weinberger, A.S. Perelson, Maturation of the immune response via adaptive walks on affinity landscapes, in: A.S. Perel-son (Ed.), Theoretical Immunology I, Addison Wesley, 1988. Santa Fe Institute Studies in the Sciences of Complexity.[31] H. Kaul, S.H. Jacobson, Global optima results for the Kauffman NK model, Mathematical Programming 106 (2) (2006) 319–338.[32] H. Kaul, S.H. Jacobson, New global optima results for the Kauffman NK model: Handling dependency, Mathematical Programming (2006),in press.[33] D.A. Levinthal, Adaptation on rugged landscapes, Management Science 43 (1997) 934–950.[34] R. Lewontin, The Genetic Basis of Evolutionary Change, Columbia University Press, 1974.[35] C.A. Macken, A.S. Perelson, Protein evolution on rugged landscapes, Proceedings of the National Academic of Science, USA 86 (1989)6191–6195.[36] P. Merz, B. Freisleben, On the effectiveness of evolutionary search in high-dimensional NK-landscapes, in: Proceedings of the IEEE Interna-tional Conference on Evolutionary Computation, 1998, pp. 741–745.[37] P. Schuster, P.F. Stadler, Landscapes: Complex optimization problems and biopolymer structures, Computational Chemistry 18 (1994) 295–324.[38] D.I. Seo, Y.H. Kim, B.R. Moon, New entropy-based measures of gene significance and epistasis, in: Proceedings of the Genetic and Evolu-tionary Computation Conference, 2003, pp. 1345–1356.[39] B. Skellet, B. Cairns, N. Geard, B. Tonkes, J. Wiles, Maximally rugged NK landscapes contain the highest peaks, in: Proceedings of theGenetic and Evolutionary Computation Conference, 2005, pp. 579–584.[40] T. Smith, P. Husbands, P. Layzell, M. O’Shea, Fitness landscapes and evolvability, Evolutionary Computation 10 (1) (2002) 1–34.[41] E.D. Weinberger, A more rigorous derivation of some properties of uncorrelated fitness landscapes, Journal of Theoretical Biology 134 (1988)125–129.[42] E.D. Weinberger, Local properties of Kauffman’s NK model, a tuneably rugged energy landscape, Physical Review A 44 (10) (1991) 6399–6413.[43] E.D. Weinberger, NP completeness of Kauffman’s NK model, a tuneably rugged fitness landscape, Technical Report 96-02-003, Santa FeInstitute, Santa Fe, 1996.[44] E. Friedgut, Sharp thresholds of graph properties, and the k-SAT problem, Journal of the American Mathematical Society 12 (1999) 1017–1054 (with appendix by J. Bourgain).[45] N.C. Wormald, Differential equations for random processes and random graphs, Annals of Applied Probability 5 (4) (1995) 1217–1235.[46] A.H. Wright, R.K. Thompson, J. Zhang, The computational complexity of NK fitness functions, IEEE Transactions on Evolutionary Compu-tation 4 (4) (2000) 373–379.[47] S. Wright, The roles of mutation, inbreeding, crossbreeding, and selection in evolution, in: Proceedings of the Sixth International Congress onGenetics, vol. 1, 1932, pp. 356–366.