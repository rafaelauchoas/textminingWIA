Artificial Intelligence 174 (2010) 639–669Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintPartial observability and learnability ✩Loizos MichaelOpen University of Cyprus, Cyprusa r t i c l ei n f oa b s t r a c tArticle history:Received 11 November 2007Received in revised form 30 March 2010Accepted 30 March 2010Available online 1 April 2010Keywords:Partial observabilityAppearanceRealityMasking processSensingMissing informationAutodidactic learningProbably approximately correctInformation recoveryReductionWhen sensing its environment, an agent often receives information that only partiallydescribes the current state of affairs. The agent then attempts to predict what it has notsensed, by using other pieces of information available through its sensors. Machine learningtechniques can naturally aid this task, by providing the agent with the rules to be used formaking these predictions. For this to happen, however, learning algorithms need to bedeveloped that can deal with missing information in the learning examples in a principledmanner, and without the need for external supervision. We investigate this problem herein.We show how the Probably Approximately Correct semantics can be extended to deal withmissing information during both the learning and the evaluation phase. Learning examplesare drawn from some underlying probability distribution, but parts of them are hiddenbefore being passed to the learner. The goal is to learn rules that can accurately recoverinformation hidden in these learning examples. We show that for this to be done, oneshould first dispense the requirement that rules should always make definite predictions;“don’t know” is sometimes necessitated. On the other hand, such abstentions should not bedone freely, but only when sufficient information is not present for definite predictions tobe made. Under this premise, we show that to accurately recover missing information, itsuffices to learn rules that are highly consistent, i.e., rules that simply do not contradictthe agent’s sensory inputs. It is established that high consistency implies a somewhatdiscounted accuracy, and that this discount is, in some defined sense, unavoidable, anddepends on how adversarially information is hidden in the learning examples.Within our proposed learning model we prove that any PAC learnable class of monotone orread-once formulas is also learnable from incomplete learning examples. By contrast, weprove that parities and monotone-term 1-decision lists, which are properly PAC learnable,are not properly learnable under the new learning model. In the process of establishing ourpositive and negative results, we re-derive some basic PAC learnability machinery, such asOccam’s Razor, and reductions between learning tasks. We finally consider a special caseof learning from partial learning examples, where some prior bias exists on the manner inwhich information is hidden, and show how this provides a unified view of many previouslearning models that deal with missing information.the proposed learning model goes beyond a simple extension ofWe suggestsupervised learning to the case of incomplete learning examples. The principled andgeneral treatment of missing information during learning, we argue, allows an agent toemploy learning entirely autonomously, without relying on the presence of an externalteacher, as is the case in supervised learning. We call our learning model autodidactic toemphasize the explicit disassociation of this model from any form of external supervision.© 2010 Elsevier B.V. All rights reserved.that✩A preliminary version of this work appeared as: Loizos Michael, Learning from partial observations, in: Manuela M. Veloso (Ed.), Proceedings of theTwentieth International Joint Conference on Artificial Intelligence (IJCAI’07), January 2007, pp. 968–974. This work was completed while the author was atthe School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA, and was supported by grant NSF-CCF-04-27129.E-mail address: loizos@ouc.ac.cy.0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.03.004640L. Michael / Artificial Intelligence 174 (2010) 639–6691. IntroductionIt can be argued that a central aspect of a fully autonomous agent is the ability to learn the rules that govern its envi-ronment, without any form of external supervision. An autonomous agent senses its environment and obtains informationthat is often incomplete, which serves, then, as input to the learning process. Such settings necessitate, thus, the use oflearning algorithms that can deal with such incomplete learning examples.In this work we propose a framework within which learning from incomplete learning examples can be formally studied.For concreteness, our framework can be viewed as an extension of the Probably Approximately Correct semantics [28]. Ourgoal is to show that it is possible to learn rules that accurately predict information missing in an agent’s sensory readings,and that these rules can be obtained efficiently, and be accompanied by formal PAC-like guarantees, irrespectively of howinformation is hidden in the learning examples available during the learning and evaluation phases. We note, however, andpoint out throughout this work, that the problem of learning from incomplete learning examples goes beyond learningclassification rules as in the original PAC model. We view the results of this work as a first step towards the more ambitiousgoal of devising learning algorithms that can identify more general rules.Our exposition starts in Section 2, where the problem of learning from incomplete information is put into context,as the problem underlying the process of scientific discovery: identifying the structure of some underlying reality, givenonly partial appearances of that reality. We continue to show how the PAC semantics can be extended to this effect.As in the PAC model, learning examples are drawn independently at random from some underlying probability distribu-tion. Unlike the PAC model, these examples are never directly accessible by an agent. Instead, some arbitrary stochasticprocess hides parts of these examples, giving rise to what we call partial observations. These observations are then givento the agent, both during the learning phase as a means to facilitate learning, and during the evaluation phase as theinput on which learned rules are to be applied to make predictions, and against which these predictions are to betested.Due to lack of complete information during the evaluation phase, we allow learned rules to make “don’t know” pre-dictions, but only when the rules cannot be unambiguously evaluated on a given observation. Under this provision, wedefine a rule to be consistent with an observation if the rule’s prediction does not directly contradict what is stated inthe observation. In particular, if the observation does not offer any information on some target attribute, then any pre-diction is consistent. Learning is successful if highly consistent rules can be obtained efficiently in the relevant learningparameters.We then consider a stronger notion oflearnability, that of deriving rules that make predictions in a mannernot only consistent with an observation, but accurate with the underlying example. Thus, even if the observationdoes not offer any information on some target attribute, the prediction may be accurate or not depending on whatthe hidden underlying value of the target attribute is. We show that this more stringent notion oflearnability isinformation-theoretically unattainable when information is hidden adversarially in observations. We introduce a metriccalled concealment to capture the extent of this adversity, and show that consistency, accuracy, and concealment aretied together in a natural manner: consistency implies accuracy discounted by some factor determined by the conceal-ment. This allows us to focus on the conceptually simpler notion of consistent learnability for the remaining of thiswork.Section 3 discusses some of the choices we have made in our learning model, and contrasts them against existing work inStatistical Analysis and Learning Theory. Three main aspects are discussed: (i) when are “don’t know” predictions allowed,and what does it mean to predict “don’t know”; (ii) to what extent is autonomy possible when learning; and (iii) howmuch regularity is assumed in the way information is missing in learning examples. This discussion shows, in particular,that unlike most previous work, our learning framework does without the assumption of an external teacher. We call thelearning framework autodidactic in recognition of this property.The two subsequent sections provide positive and negative learnability results for autodidactic learnability. Section 4establishes that certain machinery available in the PAC model applies also, in some form, in the context of autodidacticlearnability. In particular, Occam’s Razor [4] applies unchanged as in PAC learnability, while reductions between learningtasks [23] can be formalized in a way that accommodates the more stringent requirements that need to be met for auto-didactic learnability. Using reductions we then establish that any PAC learnable concept class that contains only monotoneor read-once formulas can also be learned autodidactically. Hence, in a broad set of domains, the lack of complete infor-mation does not render learnability any harder. By contrast, Section 5 establishes that incomplete information may in somecases diminish learnability. We show that, although they are properly PAC learnable, the concept classes of parities andmonotone-term 1-decision lists are not properly learnable in the autodidactic model, unless RP = NP.The case where information is not hidden completely arbitrarily in learning examples is examined in Section 6. We argue,and demonstrate, that depending on how structured such information hiding is, the semantics of missing information maybe significantly altered, to the extent that missing information may, even, make learnability easier than in the case ofcomplete information. Related learning models are then presented along with the assumptions they make on this structure.We conclude in Section 7 with a list of open problems, and some pointers to future work.L. Michael / Artificial Intelligence 174 (2010) 639–6696412. Autodidactic learnabilityThe dichotomy between appearance and reality is inherent in the process of scientific discovery. Appearances are partialdepictions of the reality that governs our world, and through such appearances scientists attempt to derive a model, orhypothesis, of the structure present in the underlying reality. The hypothesis is then applied on these appearances to makepredictions about unobserved properties of the world. In physics, for instance, these predictions might concern spatiallyor temporally distant properties of the world, for which readings cannot be obtained through our sensors. Central in thisprocess seem to be certain premises:(i) Structure exists in the underlying reality of the environment, and not necessarily in the way that sensors hide informa-tion about this reality to give rise to appearances.(ii) This underlying structure cannot be learned if it remains perpetually inaccessible through sensing.(iii) Any attempt to discover this structure should rely solely on the partial information of the structure that is providedthrough the sensors, without any external supervision during the learning phase.(iv) Developed hypotheses aim to model the structure of the underlying reality, and not necessarily the way that sensorshide information about this reality.(v) A hypothesis about the underlying structure is applied to predict some of the missing information not present inappearances, given only whatever other partial information is available in appearances.Machine learning research seems to have largely ignored these premises. In the words of McCarthy [18]:Our senses give us only partial information about the objects present in our vicinity. In particular, vision gives only a 2-dimensionalview of 3-dimensional objects. Our visual systems and our brains use sense information to learn about the 3-dimensional objects.Also humans and dogs can represent objects that are not presently visible. (The evidence about dogs is that if a thrown ballgoes out of sight, the dog will look for it.) Humans can infer the existence of objects that are out of sight, and human learning fromexperience often involves learning about the hidden reality behind some phenomenon. This is what science is usually about, but itoccurs in common sense reasoning as well.Machine learning research, to my knowledge, has so far involved classifying appearances, and has not involved inferring realitybehind experience. Classifying experience is inadequate as a model of human learning and also inadequate for robotic applications.[. . .] Another way of looking at it is that we use observations to recognize patterns in the world, not just patterns in the observations.We propose next a learning model that makes explicit the dichotomy between appearance and reality, and respects thepremises set forth above. We argue that such a model goes beyond simply being able to cope with missing information inlearning examples. Instead, it shows that despite the use of supervised learning techniques, it is possible for such learningto be carried entirely autonomously — as is the case in scientific discovery — without the supervision of some externalteacher. We call the new learning model autodidactic in recognition of this fact. We discuss other conceptual merits ofautodidactic learning throughout this work.2.1. Learning from partial observationsIn the PAC learning model [28], an agent is given access to learning examples randomly drawn from an arbitrary, butfixed, probability distribution D over binary vectors exm ∈ {0, 1}|A|, for some fixed set of binary attributes A. The examplesare structured, in the sense that the value of a designated target attribute xt is determined by some unknown, but fixed,function ϕ ∈ C of the remaining attributes A \ {xt}; the function ϕ is known as the target concept, and the class C of allpossible target concepts is known as the concept class. During an initial learning phase an agent is expected to efficientlyproduce a hypothesis function h ∈ H that is highly accurate with respect to D, in the sense that it predicts with highprobability the value of the target attribute xt in evaluation examples drawn from D, given access to the values of only theremaining attributes A \ {xt}; the class H of all possible hypotheses is known as the hypothesis class.Implicit in the definition of the PAC learning model is the premise that an agent has access to complete information onthe values of attributes. Each example contains sufficient information to determine the value of the target attribute xt ; theprimary challenge of the learning task, thus, is that of forming a hypothesis of how to determine the value of the targetattribute given the values of the remaining attributes. In most realistic domains, however, an agent is burdened with anadditional challenge: some of the information necessary to determine the value of the target attribute is missing in theexamples. The agent has, therefore, access only to partial depictions of the learning examples, and this is the case duringboth the learning and the evaluation phase. These partial depictions of the learning examples we shall call observations.Although in the general case observations could also be noisy with respect to the learning examples, we shall not considersuch a scenario in this work, and we will henceforth assume that observations are only incomplete.Observations are ternary vectors obs ∈ {0, 1, ∗}|A|, with the value ∗ indicating that the corresponding attribute is un-observed or “don’t know”. The mapping from examples to observations happens through a masking process, a (stochastic)process mask : {0, 1}|A| → {0, 1, ∗}|A|aimed to model an agent’s sensors. The masking process mask induces a probabilitydistribution mask(exm) over observations that may depend on the example exm; we write obs ← mask(exm) to denote642L. Michael / Artificial Intelligence 174 (2010) 639–669that observation obs is drawn from this probability distribution with non-zero probability. The noiseless nature of sensingamounts to insisting that whenever obs ← mask(exm) and for every attribute xi ∈ A, obs[i] ∈ {exm[i], ∗}, where obs[i]and exm[i] correspond, respectively, to the value of the i-th attribute according to obs and exm. An observation obs thatis an image of an example exm under some masking process is said to mask exm. Each attribute xi ∈ A in an observationobs with obs[i] = ∗ is said to be masked in obs.Masking processes are, in general, many-to-many mappings from examples to observations. Given, for instance, theexamples exm1 = 0010110 and exm2 = 0100100, and the observations obs1 = 0 ∗ 10 ∗ ∗0, obs2 = ∗10 ∗ ∗00, andobs3 = 0 ∗ ∗01 ∗ ∗, one masking process mask is the following: on input exm1 it returns obs1 with probability 0.3 andobs3 with probability 0.7; on input exm2 it returns obs2 with probability 0.6 and obs3 with probability 0.4. The one-to-many nature of masking processes is intended to capture the stochastic nature of sensing. An agent attempting to sense thesame reality twice (e.g., exm1) may end up with two different appearances (e.g., obs1 and obs3). On the other hand, theirmany-to-one nature is intended to capture the loss of information due to an agent’s limited sensing abilities. Two distinctrealities (e.g., exm1 and exm2) may appear to be the same (e.g., obs3) to an agent; no indication is given in the obtainedappearance as to which reality was the one that was actually sensed.The loss of information due to masking happens during both the learning and the evaluation phase. Thus, the agent neverdirectly observes the learning examples, but has access only to observations that mask the learning examples. Yet, as in thePAC model, the agent is expected to produce a hypothesis for predicting the value of a target attribute. We emphasizethat the hypothesis is a function over boolean attributes as is the case in the PAC model. In some sense, the agent istrying to encode in this hypothesis knowledge about the structure of the underlying examples — not knowledge about thestructure of observations and the way the masking process hides information. Indeed, the central premise of this work isthat information in observations is hidden in an arbitrary manner. The central question, then, is whether the structure ofthe underlying examples can still be learned in the PAC sense given such arbitrarily selected partial information. The PACmodel can be viewed as the special case of our model when observations do not contain ∗ values.To formalize the way that structure is present in examples (i.e., the requirement that the value of the target attribute isdetermined by some function of the remaining attributes), but also the way that predictions are made through a hypothesisfunction, we follow the PAC model and employ boolean formulas: syntactic objects over the set of attributes A, associatedwith the typical semantics for evaluating them given a complete assignment of values to their attributes. Given a formulaϕ and an example exm, we write val(ϕ | exm) to denote the value of ϕ on exm. Unlike the PAC semantics, however, itis necessary to define also the value val(ϕ | obs) of a formula ϕ on an observation obs, since in the general case theagent will make predictions by evaluating a learned hypothesis on such a partial observation. Note that it is possible forval(ϕ | obs) to have a value even if obs does not offer {0, 1} values for all the attributes in ϕ. On the other hand, ifval(ϕ | obs) remains undetermined due to missing information, then we define val(ϕ | obs) to equal ∗, to indicate a“don’t know” value for ϕ on obs.Note that evaluating some formula on some observation cannot necessarily be done efficiently, even if the formula isefficiently evaluatable on every example. Indeed, evaluating 3-CNF formulas on the observation in which all attributes aremasked is as hard as deciding whether 3-CNF formulas have a satisfying assignment; an NP-complete problem [8]. Mostdefinitions and results that we later state do not rely on actually evaluating formulas (efficiently); hence, they are notconditioned on the formulas involved being efficiently evaluatable. When formulas need to be efficiently evaluatable fora result to hold, this is stated explicitly in the conditions of the result. It remains open whether formulas that are notefficiently evaluatable on observations, but are so on examples, can be learned in the sense defined later on.We proceed now to define how (a particular type of) structure is encoded in examples.Definition 2.1 (Supported concept classes). A target attribute xt ∈ A is expressed by a formula ϕ over A \ {xt} w.r.t. a proba-bility distribution D if(cid:2)Prval(ϕ | exm) = exm[t] | exm ← D(cid:3)= 1.A probability distribution D supports a concept class C of formulas over A \ {xt} for a target attribute xt ∈ A if there existsa formula c ∈ C such that xt is expressed by c w.r.t. D; c is the target concept for xt under D.We view the values of all attributes as being drawn from some probability distribution D. We regard this approachas corresponding more closely to what conceptually happens in certain domains, than the approach typically taken bysupervised learning models: The attributes that encode the state of affairs are not a priori distinguished into target andnon-target attributes; they are all equivalent, and nature, as captured by the probability distribution D, assigns a value toeach of these attributes. An agent’s sensors may then mask some of the attributes without distinguishing any one of them.The distinction of a target attribute, and the assumption that this attribute is somehow correlated with the rest of theattributes serve only as premises of a particular type of learning task, and such a correlation is imposed by appropriatelyrestricting D. Indeed, under this view it is possible to easily generalize Definition 2.1 to encode other types of correlationbetween attributes, like, for instance, that the number of attributes in A that are assigned the value 1 in an exampleexm ← D is divisible by 3 with probability 0.95. Although we find these types of correlation intriguing, and meritingfurther investigation, we focus in this work on the type of correlation stated in Definition 2.1.L. Michael / Artificial Intelligence 174 (2010) 639–669643To complete the description of our learning model, we need to define when a prediction of a formula is consistent withrespect to an observation. In other words, we wish to determine when the two sources of information available to an agent,its sensor readings, and the conclusions it draws through (learned) rules, agree with each other. We define a formula ϕto have a consistency conflict with a target attribute xt w.r.t. an observation obs if {val(ϕ | obs), obs[t]} = {0, 1}; thevalue of the formula and the observed value of the target attribute are both {0, 1}, and differ from each other. In all othercases, either the two suggested {0, 1} values agree, or at least one of the two suggested values is “don’t know”. We extendthe notion of consistency to apply to a probability distribution over observations. This probability distribution we denote bymask(D) to indicate that it is induced by the probability distribution D from which examples are drawn, and the maskingprocess mask through which these examples are mapped to observations.Definition 2.2 (Degree of consistency). A formula ϕ over A \ {xt} is (1 − ε)-consistent with a target attribute xt ∈ A under aprobability distribution D and a masking process mask if(cid:2)(cid:4)(cid:5)val(ϕ | obs), obs[t](cid:3)= {0, 1} | exm ← D; obs ← mask(exm)Pr(cid:2) ε.We now state formally the learning requirements under the autodidactic learning model that we consider. In whatfollows we will denote a learning task over A by a triple (cid:6)xt, C, H(cid:7), where xt is a target attribute in A, C is a concept classof formulas over A \ {xt}, and H a hypothesis class of formulas over A \ {xt}.Definition 2.3 (Consistent learnability). An algorithm L is a consistent learner for a learning task (cid:6)xt, C, H(cid:7) over A if forevery probability distribution D supporting C for xt , every masking process mask, every real number δ ∈ (0, 1], and everyreal number ε ∈ (0, 1], algorithm L has the following property: given access to A, (cid:6)xt, C, H(cid:7), δ, ε, and an oracle returningobservations drawn from mask(D), algorithm L runs in time polynomial in 1/δ, 1/ε, |A|, and the size of the target conceptfor xt under D, and returns, with probability 1 − δ, a hypothesis h ∈ H that is (1 − ε)-consistent with xt under D and mask.The concept class C over A \ {xt} is consistently learnable on the target attribute xt ∈ A by the hypothesis class H overA \ {xt} if there exists a consistent learner for (cid:6)xt, C, H(cid:7) over A.The definition of consistent learnability follows closely the PAC semantics, with the added requirement that learnabilitysucceeds for an arbitrary masking process mask, and not only when mask is the identity mapping (as is the case underthe PAC semantics). Although the added requirement might at first seem too arduous, recall that exactly in those situationswhere learnability becomes harder due to missing information, formulas may make “don’t know” predictions more freely,avoiding thus consistency conflicts. We emphasize, however, that “don’t know” predictions cannot be abused, since a learnermay not produce a hypothesis that arbitrarily chooses to abstain from making predictions. It is the masking process thatgives a formula the ability to make “don’t know” predictions, and this is beyond the control of the learner. We later contrastour approach to other models of learning where hypotheses actively choose when to abstain from making predictions. In suchmodels, one is required to introduce a second metric for measuring success of a hypothesis: its degree of completeness; theprobability with which a {0, 1} prediction is made.2.2. Are accurate predictions possible?With a complete proposal for a learning model, we now revisit our original motivation for developing a model forlearning from partial observations: to recover missing information in the incomplete sensory inputs of an agent. Doesour definition of consistent learnability address this goal? Recall that a highly consistent formula is guaranteed to makepredictions that are consistent with randomly drawn observations. In particular, in those cases where the problem of missinginformation is interesting, namely when the target attribute is masked in an observation, the consistency guarantee seemsto offer essentially nothing, since any prediction is consistent with such an observation. What we need, therefore, is a notionof predictive correctness, not with respect to the observations, but with respect to the underlying examples: an agent wishesto be able to match the unobserved reality behind the appearances of its environment.We define a formula ϕ to have an accuracy conflict with a target attribute xt w.r.t. an observation obs obtained froman example exm if {val(ϕ | obs), exm[t]} = {0, 1}; the value of the formula and the actual value of the target attributeare both {0, 1}, and differ from each other. In all other cases, either the two suggested {0, 1} values agree, or the valueof the formula is “don’t know”. As in the case of consistency, we extend the notion of accuracy to apply to a probabilitydistribution over observations.Definition 2.4 (Degree of accuracy). A formula ϕ over A \ {xt} is (1 − ε)-accurate w.r.t. a target attribute xt ∈ A under aprobability distribution D and a masking process mask if(cid:2)(cid:4)(cid:5)val(ϕ | obs), exm[t](cid:3)= {0, 1} | exm ← D; obs ← mask(exm)Pr(cid:2) ε.It is now natural to ask that our definition of learnability be revised so that highly accurate (instead of highly consistent)hypotheses be returned. A naive revision would, however, lead to a vacuous definition, where learnability would be trivially644L. Michael / Artificial Intelligence 174 (2010) 639–669unattainable. Indeed, when the masking process is such that the target attribute and only the target attribute is masked inall observations, then clearly the learning algorithm has no access to any information about the value of the target attribute.Yet, any formula over the remaining attributes will be forced to make a {0, 1} prediction. It is, then, impossible to determinewhich one amongst two formulas has a higher degree of accuracy, as both formulas will always make {0, 1} predictions,but no feedback will be provided as to which, if any, of the two formulas makes a correct prediction. This compromiseslearnability even in domains where the concept and hypothesis classes contain only two formulas.Theorem 2.1 (Statistical indistinguishability in adversarial settings). Consider a target attribute xt ∈ A, a class F of formulas overA \ {xt}, and two formulas ϕ1, ϕ2 ∈ F such that ϕ1 /∈ {ϕ2, ϕ2}. For every real number ε ∈ [0, 1], there exist probability distributionsD1, D2, and a masking process mask0, such that:(i) ϕ1 is 1-accurate and ϕ2 is not more than (1 − ε)-accurate, both w.r.t. xt under D1 and mask0;(ii) ϕ1 is not more than (1 − ε)-accurate and ϕ2 is 1-accurate, both w.r.t. xt under D2 and mask0;(iii) mask0(D1) = mask0(D2), and no attribute in A \ {xt} is masked in any drawn observation.Proof. Let S be the set of truth-assignments to the attributes A \ {xt} for which ϕ1, ϕ2 are assigned different truth-values.Fix any probability distribution D over all truth-assignments to the attributes A \ {xt} that assigns probability ε to theset S, and probability 1 − ε to the complement of S; since ϕ1 /∈ {ϕ2, ϕ2}, both S and its complement are non-empty.For each i ∈ {1, 2}, extend D to the probability distribution Di over examples from {0, 1}|A|, by completing the truth-assignments to the attributes A \ {xt} so as to assign the induced truth-value of ϕi to the target attribute xt . Choose mask0to be the masking process that maps each example exm to an observation obs in which xtis masked if and only ifval(ϕ1 | exm) (cid:8)= val(ϕ2 | exm), and no attribute in A \ {xt} is masked. By construction of mask0, for each example exmand each observation obs ← mask0(exm), it holds that xt is masked in obs if and only if val(ϕ1 | obs) (cid:8)= val(ϕ2 | obs).By construction of D1, D2, and mask0, all three conditions of the claim follow. (cid:2)It becomes evident that although masking processes may arbitrarily hide information, completely ignoring the extent towhich information is hidden may prevent us from attaining a meaningful and useful notion of learnability. Given a moment’sthought, this is a natural conclusion. Structure exists in examples, yet a learner attempts to learn this structure given accessonly to observations. Thus, learnability becomes possible only if the masking process allows some of the structure of theunderlying examples to carry over to the observations. In other words, we expect the observations to occasionally providesome feedback, according to the structure of the underlying examples, as to whether a candidate hypothesis is indeed highlyaccurate. The extent to which such feedback is provided depends on the masking process, and is quantified next.Feedback is necessary only when a candidate hypothesis errs, i.e., has an accuracy conflict with the target attribute.Recall that an agent is not necessarily aware of an accuracy conflict. By way of illustration, if the target attribute x3 ismasked in an observation 10 ∗ 1010, then ϕ has an accuracy conflict with the observation depending on which of theexamples 1001010, 1011010 the observation was obtained from; the agent is oblivious to the choice of example, andhence to the existence of an accuracy conflict. In case of such accuracy conflicts, we expect that with some probability thevalue of the target attribute will be made known to the agent, so that the conflict might be detected. That is, we expectthat any particular reality will not be indefinitely sensed by an agent without the agent realizing that it is making a wrongprediction.Definition 2.5 (Degree of concealment). A masking process mask is (1 − η)-concealing for a target attribute xt ∈ A w.r.t. aclass F of formulas over A \ {xt} if η ∈ [0, 1] is the minimum value of(cid:5)val(ϕ | obs), exm[t]obs[t] (cid:8)= ∗ | obs ← mask(exm);= {0, 1}Pr(cid:4)(cid:3)(cid:2)across all choices of an example exm ∈ {0, 1}|A|, and a formula ϕ ∈ F .1The concealment degree in Definition 2.5 is a worst-case bound, across all possible examples, and all possible formulasused for making predictions. Each pair of a formula ϕ and an example exm imposes a constraint on η, and implies a lowerbound on the concealment degree 1 − η of the masking process mask. Note that a probability distribution D may assignzero probability to the examples that “bring out” the adversarial nature of a masking process, making it, thus, look lessadversarial than in the worst case. Note, also, that the concealment degree of a masking process may vary arbitrarily acrosstarget attributes.As an illustration, consider a particular domain in which the target attribute is x6, one of the formulas is ϕ = (x2 ∨ x4) ⊕x7, where ⊕ denotes the “exclusive or” binary operator, and one of the examples is exm = 0110011. The masking process1 A conditional probability is undefined when the event in its condition occurs with probability 0. In such cases, we define the conditional probabilityto equal 1. In the context of Definition 2.5, this choice implies that cases in which a formula does not have an accuracy conflict with the target attributew.r.t. any of the observations obtained from a particular example, can be safely ignored, since such cases do not constrain the concealment degree of themasking process in any way.L. Michael / Artificial Intelligence 174 (2010) 639–669645Table 1Observations obtained from the example exm = 0110011 by applying a particular masking process mask,and the corresponding predictions of formula ϕ = (x2 ∨ x4) ⊕ x7 for the target attribute x6.obs ← mask(exm)Prediction for x6observation0 ∗ 1 0 ∗ 1 1∗ 1 ∗ 0 ∗ 1 1∗ 1 1 ∗ 0 ∗ 10 ∗ 1 0 ∗ 1 ∗0 ∗ 1 ∗ 0 ∗ 1probabilityval(ϕ | obs)accuracy conflict0.270.150.330.040.21000∗∗yesyesyesnonomask is such that exm is mapped to observations as shown in Table 1. According to mask, the observations that give riseto accuracy conflicts are drawn with a total probability of 0.75. Among those, the observations in which the target attributex6 is not masked are drawn with a total probability of 0.42. By the law of conditional probabilities, it follows that(cid:2)Probs[t] (cid:8)= ∗ | obs ← mask(exm);(cid:4)(cid:5)val(ϕ | obs), exm[t]= {0, 1}(cid:3)= 0.420.75= 0.56.Definition 2.5 now implies that η (cid:2) 0.56, and the masking process mask is at least 0.44-concealing. Additional formulasand examples may impose extra bounds on η, which, in turn, may increase the concealment degree of mask. If none ofthe extra bounds on η is smaller than 0.56, then the definition of η would imply that the masking process mask is exactly0.44-concealing for the particular target attribute x6.2.3. Going from consistency to accuracyGiven a crisp metric of the degree of feedback that a masking process provides to an agent, it is easy to see that thenegative learnability result of Theorem 2.1 holds precisely because it appeals to a 1-concealing masking process. As inother learning models were some parameterized resource renders learnability impossible when the parameter riches somerelevant limit (e.g., in the case of random classification noise [1], learnability becomes impossible when the noise ratebecomes 1/2), we could extend the definition of learnability to allow resources that grow inversely with the distance of thisparameter from its limit. In the case of the concealment degree the limit is 1, and η defines the distance of the concealmentdegree of a (1 − η)-concealing masking process mask from this limit. Hence, we could revise the definition of learnability sothat highly accurate (instead of highly consistent) hypotheses be returned, but at the same time allow additional resourcesthat grow with 1/η to account for the adversarial nature with which mask may hide information. A learner expected toreturn a (1 − ε)-accurate hypothesis could then exploit the additional resources in order to obtain a (1 − η · ε)-consistenthypothesis, and then appeal to the following result to establish that this hypothesis is, in fact, (1 − ε)-accurate. The proofof the next result builds on the natural realization that a prediction is consistent if and only if it is either accurate or thetarget attribute is masked. Informally, then, in set-theoretic terms it holds that consistency = accuracy ∪ concealment.Theorem 2.2 (The relation of consistency and accuracy). Consider a target attribute xt ∈ A, and a class F of formulas over A \ {xt}.For every real number η ∈ [0, 1], and every masking process mask that is (1 − η)-concealing for xt w.r.t. F , the following conditionshold:(i) for every probability distribution D, and every formula ϕ ∈ F , it holds that: ϕ is (1 − ε)-accurate w.r.t. xt under D and mask forsome real number ε ∈ [0, 1], if ϕ is (1 − η · ε)-consistent with xt under D and mask, and η (cid:8)= 0;(ii) there exists a probability distribution D0, and a formula ϕ0 ∈ F , such that: ϕ0 is (1 − ε)-accurate w.r.t. xt under D0 and maskfor some real number ε ∈ [0, 1], only if ϕ0 is (1 − η · ε)-consistent with xt under D0 and mask.Proof. For every formula ϕ ∈ F , every example exm ∈ {0, 1}|A|that masks exm,and every probability distribution D, denote by Edr(exm, obs, D) the event that exm ← D and obs ← mask(exm), byEcc(ϕ, obs) the event that ϕ has a consistency conflict with xt w.r.t. obs, by Eac(ϕ, exm, obs) the event that ϕ has anaccuracy conflict with xt w.r.t. obs obtained from exm, and by Enm(obs) the event that xt is not masked in obs. Clearly,the event Ecc(ϕ, obs) holds exactly when the events Eac(ϕ, exm, obs) and Enm(obs) hold simultaneously. In particular,this is true even when exm and obs are restricted so that the event Edr(exm, obs, D) is true. Thus,(cid:2)(cid:3)Eac(ϕ, exm, obs) ∧ Enm(obs) | Edr(exm, obs, D)(cid:3)Ecc(ϕ, obs) | Edr(exm, obs, D), every observation obs ∈ {0, 1, ∗}|A|= PrPr(cid:2).From the law of conditional probabilities, the right hand side of the equation equals(cid:2)(cid:3)Enm(obs) | Edr(exm, obs, D) ∧ Eac(ϕ, exm, obs)Pr(cid:2)(cid:3)Eac(ϕ, exm, obs) | Edr(exm, obs, D).· PrWe proceed to derive bounds for the first term of the product above. In the first direction, Definition 2.5 implies that forevery example exm ∈ {0, 1}|A|, and every formula ϕ ∈ F , it holds that:646(cid:2)Probs[t] (cid:8)= ∗ | obs ← mask(exm);(cid:4)(cid:5)val(ϕ | obs), exm[t]= {0, 1}(cid:3)(cid:3) η.L. Michael / Artificial Intelligence 174 (2010) 639–669Now, if exm is drawn from any given probability distribution D, the overall probability that obs[t] (cid:8)= ∗ given that {val(ϕ |obs), exm[t]} = {0, 1} remains lower bounded by η. Thus,(cid:2)(cid:3)Enm(obs) | Edr(exm, obs, D) ∧ Eac(ϕ, exm, obs)Pr(cid:3) η.In the other direction, Definition 2.5 implies that there exists an example exm0 ∈ {0, 1}|A|(cid:4)(cid:3)(cid:2)(cid:5)val(ϕ0 | obs), exm0[t]= {0, 1}(cid:2) η.Probs[t] (cid:8)= ∗ | obs ← mask(exm0);, and a formula ϕ0 ∈ F , such that:(1)Now, if exm0 is replaced with an example exm drawn from the probability distribution D0 that is defined so that it assignsprobability 1 to exm0 being drawn, the probability that obs[t] (cid:8)= ∗ given that {val(ϕ0 | obs), exm[t]} = {0, 1} remainsupper bounded by η. Thus,(cid:2)(cid:3)Enm(obs) | Edr(exm, obs, D0) ∧ Eac(ϕ0, exm, obs)Pr(cid:2) η.(2)Finally, we proceed to establish that the conditions of the claim hold. For Condition (i), fix an arbitrary probabilitydistribution D, and an arbitrary formula ϕ ∈ F , and assume that ϕ is (1 − η · ε)-consistent with xt under D and mask forsome real number ε ∈ [0, 1], and that η (cid:8)= 0. Then, Pr[Ecc(ϕ, obs) | Edr(exm, obs, D)] (cid:2) η · ε, or equivalently(cid:2)(cid:3)Enm(obs) | Edr(exm, obs, D) ∧ Eac(ϕ, exm, obs)Pr(cid:2)(cid:3)Eac(ϕ, exm, obs) | Edr(exm, obs, D)· Pr(cid:2) η · ε.Since η (cid:8)= 0, Inequality (1) immediately implies that Pr[Eac(ϕ, exm, obs) | Edr(exm, obs, D)] (cid:2) ε. Therefore ϕ is (1 − ε)-accurate w.r.t. xt under D and mask.For Condition (ii), consider the probability distribution D0, and the formula ϕ0 ∈ F , both as defined in the context ofInequality (2), and assume that ϕ0 is (1 − ε)-accurate w.r.t. xt under D0 and mask for some real number ε ∈ [0, 1]. Then,Pr[Eac(ϕ0, exm, obs) | Edr(exm, obs, D0)] (cid:2) ε. Inequality (2) immediately implies that(cid:2)(cid:3)Enm(obs) | Edr(exm, obs, D0) ∧ Eac(ϕ0, exm, obs)Pr(cid:2)(cid:3)Eac(ϕ0, exm, obs) | Edr(exm, obs, D0)· Pr(cid:2) η · ε,or equivalently Pr[Ecc(ϕ0, obs) | Edr(exm, obs, D0)] (cid:2) η · ε. Therefore ϕ0 is (1 − η · ε)-consistent with xt under D0 andmask. The claim follows. (cid:2)Condition (i) of Theorem 2.2 provides a formal implication from highly consistent hypotheses to highly accurate ones.There is, however, a caveat to this implication. The degree of accuracy of the predictions is not necessarily as high astheir degree of consistency. Given a moment’s thought, this makes perfect sense. The requirement of making accuratepredictions is a stronger one as compared to that of making consistent predictions. In those cases that the target attributeis not masked, accuracy conflicts and consistency conflicts are equivalent, whereas in those cases that the target attributeis masked, consistency conflicts never occur, while accuracy conflicts are still possible. What is perhaps more intriguing isthe fact that the extent to which the degree of accuracy diminishes with respect to the degree of consistency, dependson the degree of concealment of the masking process. Assuming that our sensors and physical world do not adversariallyhide information from us, one may interpret the above result as corroborating that the approach humans follow in usingconsistent theories for recovering missing information in their appearances is a rational strategy.The dependence of accuracy on the concealment degree explains also why it is possible in certain cases for the implica-tion from consistency to accuracy to be violated. This happens exactly in those cases where the concealment degree is high,and thus η is close to zero. Condition (ii) of Theorem 2.2 establishes that there exist domains in which certain formulaswith a high degree of consistency have a low degree of accuracy. At the same time, Condition (ii) suggests also that the useof consistent hypotheses is, in the worst case, an optimal strategy for recovering missing information; in certain domains,the bound on the degree of accuracy that Condition (i) guarantees for a highly consistent formula, is tight. Furthermore, thisoptimality is guaranteed without any knowledge of the concealment degree, which, as we will later discuss in Section 3.2,may be hard or even impossible to determine, for all but very simple masking processes.Obtaining highly accurate hypotheses through highly consistent hypotheses (cf. Definition 2.3) is, thus, a valid and op-timal, in the worst case, approach. Still, why should a direct approach of learning highly accurate hypotheses not be usedinstead? The answer is simple. Consistency is a much more natural notion to work with, and avoids complications arisingfrom having to deal with the degree of concealment of a masking process. More importantly, a formula’s degree of con-sistency can be reliably empirically estimated as the following simple result shows, while its degree of accuracy cannot (cf.Theorem 2.1), as that would require access to the value of the target attribute even when the target attribute is masked inobservations.Definition 2.6 (Degree of consistency (sample version)). A formula ϕ over A \ {xt} is (1 − ε)-consistent with a target attributext ∈ A given a sample O of observations if ϕ has a consistency conflict with xt w.r.t. at most an ε fraction of the observa-tions in O.L. Michael / Artificial Intelligence 174 (2010) 639–669647Theorem 2.3 (Empirical estimability of consistency degree). Consider a target attribute xt ∈ A, a formula ϕ over A \ {xt }, a probabilitydistribution D, a masking process mask, and a sample O of observations drawn independently from mask(D). For every pair of realnumbers ε, γ ∈ [0, 1], if ϕ is (1 − ε)-consistent with xt given O, then, with probability at least 1 − e, it holds that ϕ is(1 − (ε + γ ))-consistent with xt under D and mask.−2|O|γ 2(cid:6)|O|(cid:6)|O|Proof. For each observation obsi ∈ O, let the random variable Xi be the indicator variable for the event Ecc(ϕ, obsi) thatϕ has a consistency conflict with xt w.r.t. obsi ; by construction of O the random variables are independent. Define therandom variable X (cid:4) |O|−1 ·i=1 Xi to be the mean of these random variables. By linearity of expectations, E[ X] is themean |O|−1 ·i=1 E[ Xi] of the expectations of these random variables. By standard Hoeffding concentration bounds [12],. Clearly, X is the fraction of the observations in O w.r.t. which ϕ hasthe probability that | X − E[ X]| > γ is at most ea consistency conflict with xt , and therefore X (cid:2) ε. By definition of the random variables, E[ X] = E[ X1] = Pr[Ecc(ϕ, obs1)];−2|O|γ 2thus ϕ is (1 − E[ X])-consistent with xt under D and mask. Since X (cid:2) ε, it follows that with probability 1 − e,E[ X] (cid:2) ε + γ , as needed. (cid:2)−2|O|γ 2Whether a formula’s degree of consistency can be reliably empirically estimated in an efficient manner is an orthogonalissue, and depends on whether the formula can be evaluated efficiently on partial observations.3. Discussion and related workThe problem of missing information in learning settings had been recognized early on in the literature. Valiant [28]himself in the paper that introduced PAC learning had, in fact, considered some form of learning from partial observations.Various frameworks developed since then have offered solutions to related problems. Within the Learning Theory commu-nity, extensions of the PAC model have been proposed to deal, to varying degrees, with the problem of missing information.Within the broader Machine Learning community the problem of dealing with missing information has received significantattention, especially in devising practical solutions in real-world settings. Other communities within the area of ArtificialIntelligence and within Computer Science at large have also offered solutions to problems related to the manipulation ofincomplete data. Fields outside Computer Science have also dealt with the problem, especially within the area of StatisticalAnalysis. It is beyond the scope of this work to do a full survey of the problems that have been examined and the solutionsthat have been offered. In this section we will mostly focus on discussing work closely related to ours: extensions of thePAC model that deal with missing information.In the sequel we defend certain modelling choices within our framework. We then contrast the degree of concealmentas a measure of the degree of missing information to other standard metrics found in the Statistical Analysis literature. Wefinally consider related PAC learning frameworks and discuss how those relate to autodidactic learning. We identify threedimensions along which these frameworks may be compared and contrasted to each other and to autodidactic learning:(i) the semantics of “don’t know” predictions, (ii) the degree of supervision while learning, and (iii) the regularity on howinformation is hidden in observations.3.1. Are “don’t know” predictions justified?Our choices as to when “don’t know” predictions are considered justified, and as to how such predictions are accountedfor when measuring a formula’s degree of accuracy (cf. Definition 2.4), may raise certain objections. We discuss here threeclasses of objections that we have identified.A first objection relates to our choice of when “don’t know” predictions are allowed. Recall that a formula ϕ predicts ∗on an observation obs0 if and only if val(ϕ | obs0) is undetermined. It could be argued that a “don’t know” predictionmight not be justified if, for instance, ϕ evaluates to 1 on the vast majority (but not all, since val(ϕ | obs0) is undefined)of the examples masked by obs0; would it not be more reasonable to define the value of ϕ on obs0 to be 1 in thiscase? Our answer is no. Just because the majority of the possible underlying examples exm of observation obs0 are suchthat val(ϕ | exm) = 1, it does not follow that such examples will be drawn with high, or even non-zero, probability fromthe underlying probability distribution D. That is, there is no way to exclude the eventuality that the agent’s environmentwill supply the agent only with examples exm such that val(ϕ | exm) = 0, which would then completely undermine thereason for choosing to define the value of ϕ on obs0 to be 1. Thus, determining the value of a formula on obs0 simply bycounting the number of examples masked by obs0 that exhibit a certain property is not meaningful.Following the argument above, a refined version of the objection could be raised. Consider a probability distribution Dand a masking process mask such that(cid:2)Prval(ϕ | exm) = 1 | exm ← D; obs ← mask(exm); obs = obs0(cid:3)(cid:3) 0.999;that is, in those cases that the particular observation obs0 is drawn, formula ϕ evaluates to 1 on the underlying examplewith overwhelming probability. This situation is often illustrated via a toy domain of observing birds, without, however,observing whether the birds are penguins. In this domain the underlying probability distribution D is taken to be such that648L. Michael / Artificial Intelligence 174 (2010) 639–669most observed birds are not penguins, and thus have the ability to fly. The question, then, is whether it would be morereasonable to define the value of ϕ (the formula that we employ to make predictions on the ability of observed birds to fly)on the observation obs0 of the bird Tweety, to be 1. Our answer remains no. There exists no theoretical justification as towhy a probability of 0.999, or any other probability for that matter, is high enough for a prediction of 1 to be made overa “don’t know” prediction. It is easy to devise scenarios where it is preferable for an agent to predict “don’t know” and beaware of this lack of certainty, rather than predicting a {0, 1} value and risking a wrong prediction without knowing thatthis is happening. In such scenarios it is preferable for a “don’t know” prediction to be made, for the lack of certainty tobe recorded, and only then for the “don’t know” prediction to be replaced with a {0, 1} value by the agent’s deliberationmechanism, in case such a value is believed to be likely true.Suppose that we even subscribe to the view that for all practical purposes a probability of, say, at least 0.75 wouldbe appropriate for ϕ to predict 1, and a probability of, say, at most 0.25 would be appropriate for ϕ to predict 0; ϕwould predict “don’t know” only in the remaining cases. That is, suppose that for some domains it is more preferableto risk making a wrong prediction with a small probability, over making a “don’t know” prediction. This setting is stillnot meaningful. An agent has no access to the underlying probability distribution D from which examples are drawn,and thus the problem of determining the probability in question given only obs0 is generally impossible. That is, giventhat val(ϕ | obs0) = ∗, it is not possible to estimate what the risk of making a wrong prediction is, and thus thereis no argument in favor of choosing to make a {0, 1} prediction over the “don’t know” prediction that is suggested byval(ϕ | obs0) = ∗.A second objection relates to our choice of how to measure the degree of accuracy. Recall that the degree of accuracy ofa formula ϕ is the probability with which it predicts the correct underlying value of the target attribute, or it predicts “don’tknow”. It could be argued that the degree of accuracy should be computed with respect only to the {0, 1} predictions madeby a formula, ignoring all “don’t know” predictions; that is, the degree of accuracy should be defined to be the percentageof correct predictions among the {0, 1} predictions. After all, would it not be more natural to account for the percentage of“don’t know” predictions by introducing a second metric, call it the degree of completeness of a formula, that captures theprobability with which {0, 1} predictions are made? We agree that this alternative approach does have an appeal. However,we opted not to follow it for two main reasons:(i) The degree of completeness 1 − ω of a formula, the degree of its accuracy 1 − ε under our proposed definition, and theunder the alternative definition discussed above, can be trivially derived from each other,degree of its accuracy 1 − ε(cid:13)since ε(cid:13) = ε/(1 − ω);(ii) While the alternative degree of accuracy would not be meaningful by itself without the associated degree of complete-ness, our proposed degree of accuracy encompasses both metrics in one, building on the existence of a natural degreeof completeness for a formula, as this follows from the fact that formulas cannot choose to abstain from making {0, 1}predictions, and predict ∗ only when sufficient information is missing for such a “don’t know” prediction to be justified(cf. the first objection).A third objection relates to our choice of the requirements for learnability. Recall that we require the returned hypothesisto be highly accurate only. It could be argued that among the formulas that achieve the same degree of accuracy, one shouldprefer those formulas that have a higher degree of completeness (cf. the second objection), punishing, thus, the formulasthat predict “don’t know” more often. For instance, consider the case where the target concept is a parity ϕ that dependson a strict subset of all the attributes A, and assume that the masking process is such that exactly one attribute is maskedin each observation. Clearly, the parity ϕ0 over all the attributes A will always predict “don’t know”, and will, thus, be1-accurate. Note, however, that the parity ϕ is also 1-accurate, and does not always predict “don’t know” since the formulaϕ might not depend on the attribute that is masked in some observations. Should we not prefer that a learning algorithmreturns ϕ instead of ϕ0? Yes, we should. However, we argue that the way to do this is not by imposing an additionalrequirement on the learner, but by restricting the hypothesis class to capture our prior knowledge that the target conceptdoes not depend on all the attributes A. This would exclude the parity ϕ0 from being considered as a possible hypothesis.Looking at the same argument from a different angle, if some target concept cannot be a priori excluded (and be removedfrom the hypothesis class), then it is not possible during the learning phase to make a case for one hypothesis over another;after all, the hypothesis that makes more “don’t know” predictions might be the actual target concept that the learner islooking for.It could, nonetheless, be argued that a learning algorithm need not necessarily identify the actual target concept, but anyhypothesis that is highly accurate. Among those that satisfy this requirement, then, would it not be meaningful to insist thatthe returned hypothesis is as complete as possible? We agree that it would be desirable to obtain a hypothesis that achievesthe highest possible degree of completeness. At the same time, however, we note that insisting that this be the case ingeneral is not reasonable. Unlike the feedback that observations provide on how the accuracy of a hypothesis compares tothe optimal accuracy (which is achieved by the target concept), observations provide no indication on how the completenessof a hypothesis compares to the optimal completeness (which is achieved by some hypothesis that might differ from thetarget concept). In the general case, then, one cannot expect a learning algorithm to provide the same type of guaranteesfor completeness as it does for accuracy. The characterization of domains for which certain completeness guarantees on thelearned hypothesis could be meaningfully insisted upon remains open.L. Michael / Artificial Intelligence 174 (2010) 639–6696493.2. Qualitative characteristics of maskingIn addition to their concealment degree, masking processes may be characterized based on qualitative criteria. We brieflydiscuss next some representative types of masking processes along two dimensions that have been traditionally consideredin the Statistical Analysis literature [16,26]. In the first dimension, the pattern of masked attributes is considered: in aunivariate pattern only a single attribute is masked, or more generally, attributes in a fixed set are either all masked or allnon-masked; in an arbitrary pattern any of the attributes may be masked without any constraints.2 In the second dimension,the nature of the dependence of the masked attributes on the underlying examples is considered: attributes are maskedcompletely at random (MCAR) if the masking of attributes is independent of the underlying example; attributes are masked atrandom (MAR) if the masking of attributes may depend on the values of the non-masked attributes only; finally, attributesare masked not at random (MNAR) if the masking of attributes depends on the values of the masked attributes in theunderlying example.In the simplest scenario (Type 1: univariate pattern and MCAR), a masking process mask maps examples to observationsthat mask only some target attribute xt , and this happens randomly with some fixed probability p, and independently ofthe example being mapped. Since every accuracy conflict is observed with probability at least 1 − p, and since there areaccuracy conflicts that are observed with probability at most 1 − p, it follows that mask is p-concealing for the targetattribute xt w.r.t. any class F of formulas.The preceding scenario captures situations where some component in an agent’s sensors randomly fails to provide areading. In a natural variation (Type 2: univariate pattern and MAR (possibly not MCAR)), a component does not provide areading in a manner possibly dependent on the readings of other components, but still independent of its own reading. Thedegree of concealment of masking processes with such dependence properties may take any value in the range [0, 1].In a different scenario (Type 3: arbitrary pattern and MCAR with independent masking), each attribute xi is masked inobservations randomly with some fixed probability pi , which may differ across attributes. The attributes are still maskedindependently of the underlying example, and of each other. For any given target attribute xt , formulas may now predict“don’t know” on some observations, since the remaining attributes may also be masked. This fact seemingly makes thecalculation of the concealment degree of a Type 3 masking process mask for xt more involved than for a Type 1 maskingprocess. Yet, the independence of masking across attributes, imposed by the product distribution, implies that mask is, infact, pt -concealing for each particular target attribute xt w.r.t. any class F of formulas.It is possible for attributes to be masked independently of the underlying example, but not from each other (Type 4:arbitrary pattern and MCAR with correlated masking). For instance, a masking process could map examples to observationsso that with probability 0.23 the first half of the attributes are masked, with probability 0.36 those attributes indexed witha prime number are masked, and with probability 0.41 attribute x4397 is masked. Due to the correlation, the evaluation ofthe concealment degree of this masking process is far from straightforward, and depends on the target attribute xt , and theclass F of formulas.In the most general case (Type 5: arbitrary pattern and MNAR), each attribute xi is masked in observations in a mannerthat depends on its value in the underlying example. The human eyes exhibit such a behavior, by masking readings that arevery bright, through the closing of the eyelids. A survey, viewed as a sensor, also behaves thus, since the lack of response ina question may be correlated with the answer. Masking processes of this type may hide information adversarially, and mayhave a high degree of concealment.Although not an exhaustive list of types, the aforementioned discussion illustrates that the qualitative characteristics of amasking process are largely orthogonal to its degree of concealment, and to the ease with which the degree of concealmentmay be calculated. To emphasize this point further, consider the deterministic masking process mask that maps examplesto observations so that some target attribute xtis either always or never masked, depending on whether the GoldbachConjecture is true or false under the standard axioms of mathematics. It is easy to see that attributes in observations aremasked completely at random in a univariate pattern, yet, the concealment degree of mask for xt w.r.t. any class F offormulas is either 0 or 1. In fact, as far as we know, the truth of the Goldbach Conjecture might be independent of thestandard axioms of mathematics, in which case so would be the actual concealment degree of mask, meaning that it wouldbe impossible to mathematically prove what the concealment degree of mask is.3.3. Semantics of “don’t know” predictionsRecall that in autodidactic learning, a formula makes a “don’t know” prediction if and only if insufficient informationexists in an observation for the formula to be evaluated — we have already defended this choice earlier in this section. Thus,although a learner may return a hypothesis that abstains from making predictions in certain observations, the abstentionis beyond the control of the learner or the hypothesis, and depends only on the masking process and the observations towhich it gives rise.2 A third possibility exists, that of a monotone pattern. This pattern appears in certain statistical studies where an attribute masked in some observationremains so in all subsequent observations. Monotone patterns are not meaningful in the setting we consider, where observations are assumed to be drawnindependently from each other.650L. Michael / Artificial Intelligence 174 (2010) 639–669An immediate alternative is to consider a setting where the learner returns a program, rather than a hypothesis. Theprogram receives as input an observation, and decides whether to predict “don’t know”, or a {0, 1} value. In this setting,the program may decide to predict “don’t know” even on observations that offer complete information on all attributes. Itis then evident that simply measuring how accurate the predictions of a program are is not sufficient, as the program maysimply choose to always abstain from making predictions. A second metric of completeness is needed, that measures howoften “don’t know” predictions are made. In a PAC-like model, then, one would expect not only that the degree of accuracyof the program is sufficiently high, but that the same is true also for the degree of completeness of the program. Overall, wewould expect a learner to produce a program that makes only a few “don’t know” predictions, and only a few inaccurate{0, 1} predictions. Put differently, we may simply consider both “don’t know” predictions and inaccurate predictions aswrong predictions, and then ask that the program produced by a learner makes only a few such wrong predictions. Rivestand Sloan [24] consider a treatment of when “don’t know” predictions are made similar to that discussed above, in thecontext of complete information.In a second alternative, the learner is expected to produce a hypothesis (or program) that never predicts “don’t know”.Thus, a {0, 1} prediction is made even on observations that offer incomplete information for the value of the target attributeto be uniquely determined. The PAC-like guarantees that one may expect in such a setting is to achieve a probabilityof making an accurate prediction that is sufficiently close to the information-theoretically optimal probability that canbe achieved. Since the value of the target attribute does not follow deterministically from the available information, theinformation-theoretically optimal probability is, in general, not 1. Such a treatment is followed, for instance, by Kearns andSchapire [13].A third alternative exists also, where the goal of a learner is to produce a hypothesis that given an observation predictswhether the target attribute is masked in that observation. Thus, the hypothesis no longer attempts to accurately recoverthe missing value of the target attribute. The learning goal is no longer that of identifying the structure that exists in theunderlying examples. Instead, the goal is that of identifying the structure that exists in the observations. In some sense,then, in this setting one assumes that the way information is sensed by an agent, including what this information is andwhat parts of it are missing, is structured. For instance, the target attribute might be masked if and only if certain otherattributes are masked. Note that this setting resembles the case of learning from complete information as in the originalPAC model, with the only difference that instead of learning a boolean formula over boolean attributes, one learns a ternaryformula over ternary attributes. This approach has been taken, for instance, in the work of Valiant [29] on Robust Logics,and the work of Goldman et al. [9].3.4. Degree of supervision while learningThe focus of the autodidactic learning model is to provide a framework for studying what can be learned in a trulyautonomous manner, where no teacher is ever present. This, in particular, implies that the value of the target attributemight not be always available, not even during the learning phase. Varying the degree of availability of the value of thetarget attribute gives rise to various settings that relate to certain learning models in the literature. The orthogonal issue ofhow non-target attributes are masked is dealt with later on.On the one extreme, one may consider a setting where the target attribute is always masked in the observations availableto a learner. According to the autodidactic learning model, in any non-trivial setting the degree of concealment of themasking process that gives rise to these observations is 1. It is, thus, impossible to learn to predict the missing value of thetarget attribute accurately (cf. Theorem 2.1).Such a scenario may be contrasted to the unsupervised learning model, where it is also the case that the target attributeis always masked. Despite this lack of information, in unsupervised learning one is expected to group the available obser-vations in meaningful clusters that maximize some metric. The fundamental difference between the goal of unsupervisedlearning and autodidactic learning is that the former does not attempt to uncover some hidden, but definite, reality aboutthe value of the target attribute. It simply partitions observations into clusters without associating each cluster with a valuefor the target attribute. That is, even if the clusters are identified perfectly, i.e., all observations where the masked targetattribute has a hidden value of 0 are grouped together, it is impossible to know whether a given cluster corresponds tothe target attribute having a 0 value or a 1 value. Overall, then, the focus of unsupervised learning is different from thatof autodidactic learning, and the predictive guarantees that unsupervised learning offers are unrelated to those required forthe task of information recovery that is examined in this work.On the other extreme, one may consider a setting where the target attribute is never masked in the observations availableto a learner. According to the autodidactic learning model, the degree of concealment of the masking process that gives riseto these observations is 0. Thus, learning to predict the missing value of the target attribute accurately is not compromised,and, in fact, accuracy degenerates to consistency.The assumption that the value of the target attribute is available during the learning phase is that followed by super-vised learning models. Similar to autodidactic learning, supervised learning seeks to identify how the value of the targetattribute is determined by the values of the rest of the attributes. The fundamental difference between the goal of super-vised learning and autodidactic learning lies in the availability of the value of the target attribute during the evaluationphase. In autodidactic learning, the observations during the evaluation phase come from the same source as the observa-tions during the learning phase, namely the sensors of an agent. Thus, by assuming that the target attribute is never maskedL. Michael / Artificial Intelligence 174 (2010) 639–669651in observations during the learning phase, it follows that the same is true during the evaluation phase. This implication,then, makes the goal of learning to predict the value of the target attribute superfluous; the value of the target attribute isalways observed, and need not be predicted. In supervised learning, on the other hand, it is valid to assume that the valueof the target attribute is not available during the evaluation phase; thus, the goal of learning to predict the value of thetarget value is meaningful. The natural way to interpret the discrepancy between the learning and the evaluation phase insupervised learning is to take the approach that the learner’s sensors never provide the agent with the value of the targetattribute. However, during the learning phase an external teacher supervises the learner and provides it with the values ofthe target attribute. Overall, then, the premises of supervised learning are in contrast to our goal of developing a frameworkfor learning in a completely autonomous manner. Yet, this does not restrict us from using within the context of autodidacticlearning, techniques and algorithms that were developed for supervised learning models.3.5. Regularity on how information is hiddenIn a truly autonomous learning setting, where no teacher is available to offer the agent sufficient information to aid thelearning task, few or no assumptions can be made as to how information is hidden in observations when an agent sensesits environment. In autodidactic learning this is reflected by making no assumptions on what the masking process lookslike; in this respect, we follow the treatment of the PAC model, where no assumption is made on what the distribution overexamples is, and thus, on how nature chooses the states of the environment that will be sensed by the agent. Nonetheless, incertain domains, most notably those involving a teacher, some regularity may be assumed on the sensors of an agent; moreprecisely, the regularity might be a result of the combined workings of an agent’s sensors and a teacher that completes someof the information missing in the agent’s sensory inputs (cf. the discussion on supervised learning earlier in this section). Inthese teacher-based settings, thus, the regularity of how information is hidden often differs between the learning and theevaluation phase. We examine next the assumptions that some learning models place on the regularity of how informationis hidden during the learning phase.On the one extreme we have learning models where complete observations are assumed. The original PAC model isoften taken to fall in this category, as well as most of its variants in the Learning Theory literature (e.g., [24]). Despite this,Valiant’s seminal paper that introduced the PAC model [28] also discusses learnability from partial observations, where acertain benign type of missing information in observations is considered. It is assumed that information may be hidden onnon-target attributes as long as the non-masked attributes provide enough information for the target concept to evaluate to1 when the value of the target attribute is 1 in an observation. So, examples in which the value of the target attribute is 1are masked by observations in a manner that no essential information is lost, whereas examples in which the value of thetarget attribute is 0 might be masked by arbitrary observations.We are not aware of any other learning models beyond autodidactic learning that lie on the other end of the spectrum,and make essentially no assumptions on how information is missing. Among the approaches that we are aware of, concep-tually closer to autodidactic learning are learning models that assume some independence on the way the various attributesare masked. Such is the case in the work of Decatur and Gennaro [6], where each attribute is masked in observations ran-domly and independently of the underlying example and other attributes, with some fixed probability p that is constantacross attributes.Other learning models make no assumptions on how information is hidden in observations for non-target attributes, butrestrict the target attribute is some way that depends on the masked non-target attributes. This is the case in the model oflearning from examples with unspecified attribute values [9], where the target attribute is masked if and only if the valueof the target concept cannot be determined by the values that are available on the rest of the attributes. This asymmetry isbest explained by the presence of a teacher that ensures that this constraint is met. The Robust Logics framework [29] doesnot a priori impose a similar restriction on the masking of any particular target attribute, but it implicitly assumes that thisis the case for learnability to be possible. In both cases, a learner attempts to learn to predict when the value of the targetattribute is “don’t know” by learning the structure of observations, and by allowing hypotheses to condition their predictionson whether the values of certain non-target attributes are “don’t know”. Contrast this to the autodidactic learning modelwhere the goal is to learn the structure of the underlying examples, and where the “don’t know” values of attributes cannotbe explicitly taken into account by hypotheses.Schuurmans and Greiner [27] consider a model where the target attribute is never masked, and examine various caseson how the remaining attributes are masked: arbitrarily, according to some product distribution (cf. Type 3 masking inSection 3.2), or not masked at all. Ben-David and Dichterman [2] consider a different setting where k attributes are notmasked in each observation, but the choice of which attributes are not masked may change across observations, and can beactively chosen by the learner.A number of other models that on the surface consider complete observations, can be effectively viewed as dealing witha special class of partial observations, where the masked attributes are those in a fixed unknown subset. If one attemptsthen to compute the value of the target attribute as a function of the values of the non-masked attributes, then one caninterpret the lack of existence of a deterministic function as being due to either classification noise [1], the existence of onlya probabilistic function [13], or the existence of a deterministic function that is occasionally switched [3]. Fig. 1 shows howmissing information may manifest itself in these three ways.652L. Michael / Artificial Intelligence 174 (2010) 639–669Manifestations of Missing Information Assume that the set of attributes required to fully describe an environ-(cid:13)(cid:13)(cid:13)ment is partitioned into a set of hidden attributes A(cid:13) = {x|A(cid:13)|}, always assigned a “don’t know” value2, . . . , x1, xin observations, and a set of known attributes A = {x1, x2, . . . , x|A|}, always assigned a {0, 1} value in observa-tions. Let obs ∈ {0, 1}|A|be the unique corresponding partialobservation, and exm(cid:13) ∈ {0, 1}|A(cid:13)∪A|that is randomly drawn fromsome underlying probability distribution. Then:be a complete observation, obs(cid:13) ∈ {0, 1, ∗}|A(cid:13)∪A|be an example among those masked by obs(cid:13)(i) When the target attribute xt ∈ A is expressed by the target concept ϕ(cid:13) ↔ ϕ, where ϕ(cid:13)is a formula overA(cid:13), and ϕ is a formula over A, then partial observations over A(cid:13) ∪ A may manifest themselves as completeobservations over A, with xt being noisily expressed by the manifested target concept ϕ. The hidden value of ϕ(cid:13)on exm(cid:13)determines whether xt obtains the value of the manifested target concept ϕ, or a noisy value, on themanifested complete observation obs.(ii) When the target attribute xt ∈ A is expressed by the target concept ϕ(cid:13), thenpartial observations over A(cid:13) ∪ A may manifest themselves as complete observations over A, with xt being ex-pressed by a manifested probabilistic target concept that evaluates to 1 on obs with some probability pϕ(cid:13) (obs).The probability with which the hidden value of ϕ(cid:13)is 1 on exm(cid:13)determines the probability pϕ(cid:13) (obs) with whichxt obtains the value 1 on the manifested complete observation obs.is a formula over A(cid:13), where ϕ(cid:13)} is(iii) When the target attribute xt ∈ A is expressed by the target concepta set of formulas over A(cid:13)exactly one of which evaluates to 1 on any given truth-assignment to the attributes A(cid:13),and {ϕ1, ϕ2, . . . , ϕk} is a set of formulas over A, then partial observations over A(cid:13) ∪ A may manifest themselvesas complete observations over A, with xt switching between being expressed by one of the manifested targetconcepts in {ϕ1, ϕ2, . . . , ϕk}. The unique formula ϕ(cid:13)determines the manifestedtarget concept ϕ j of which xt obtains the value on the manifested complete observation obs.j whose hidden value is 1 on exm(cid:13)→ ϕ j ), where {ϕ(cid:13)2, . . . , ϕ(cid:13)j=1(ϕ(cid:13)1, ϕ(cid:13)kj(cid:7)kFig. 1. Various ways in which missing information may manifest itself.4. Learnability results and toolsWe continue in this section to establish some learnability results in the autodidactic learning model. In the spirit of thePAC model, which the autodidactic learning model extends, the goal is to establish that certain concept classes are learnable,despite the arbitrary manner in which information is hidden in observations. Since our ultimate goal is that of predictingaccurately the value of the target attribute, we expect learned hypotheses to be highly accurate. By Theorem 2.2, then, itsuffices to learn highly consistent hypotheses.Our learnability results are derived through existing PAC learning algorithms and techniques, which we extend to thecase of partial observability. As a motivating example of how this can be done, we consider the following algorithm forproperly learning monotone conjunctions under the PAC semantics.Consider only those observations that assign the value 1 to the target attribute xt . Out of the attributes in A\{xt}, removethose that are assigned the value 0 in any observation during the learning phase. Return the hypothesis comprised ofthe conjunction of all remaining attributes.This algorithm was proposed and was proved correct under the PAC semantics in Valiant’s seminal paper [28]. The ideabehind the algorithm is essentially the following: identify a hypothesis that agrees with all given learning examples, andthen appeal to an Occam’s Razor type of argument.Consider the application of the algorithm above on partial observations. Clearly, observations obs with obs[t] = ∗ areignored. Since this is also the case for observations obs with obs[t] = 0, one could safely replace all ∗ values of the targetin observations, with the value 0, without affecting the outcome of the algorithm. By an entirely analogousattribute xtargument, replacing all ∗ values of the attributes A \ {xt} in observations, with the value 1, would not affect the outcome ofthe algorithm, since those attributes are ignored by the algorithm. Overall, there exist certain “default” values that may beassigned to masked attributes so that the algorithm will face complete observations, without this affecting the algorithm’sbehavior. This suggests that it may be possible to obtain consistent learners by reducing the learning problem to oneover complete observations. It suggests also that the principle known as Occam’s Razor may apply to the case of partialobservations. Consequently, certain positive results and certain learnability techniques under the PAC semantics may belifted to the case of consistent learnability from partial observations.4.1. Learnability through Occam’s RazorIntuitively, one may see that the ideas behind Occam’s Razor [4] do not rely on the learning observations being com-plete. For completeness of the presentation, we reproduce below one version of Occam’s Razor for the case of consistentlearnability. Although we do not actually employ this technique to derive our positive learnability results later on, someof those results could have also been derived through an Occam’s Razor argument. It remains an interesting prospect toderive novel autodidactic learnability results that cannot be established through the other techniques that we consider inthis section.L. Michael / Artificial Intelligence 174 (2010) 639–669653Definition 4.1 (Compressibility). An algorithm L is a compressor for a learning task (cid:6)xt , C, H(cid:7) over A if there exists a realnumber β ∈ [0, 1) such that for every sample O of observations given which a formula c ∈ C is 1-consistent with xt ,every real number δ ∈ (0, 1], and every real number ε ∈ (0, 1], algorithm L has the following property: given access to A,(cid:6)xt, C, H(cid:7), δ, ε, and O, algorithm L runs in time polynomial in 1/δ, 1/ε, |A|, size(c), and |O|, and returns, with probability1 − δ, a hypothesis h ∈ H that is (1 − ε)-consistent with xt given O, and its size is linear in |O|β and polynomial in 1/δ,1/ε, |A|, and size(c). The concept class C over A \ {xt} is compressible on the target attribute xt ∈ A by the hypothesis classH over A \ {xt} if there exists a compressor for (cid:6)xt, C, H(cid:7) over A.Unlike a consistent learner (cf. Definition 2.3), a compressor is not given oracle access to observations, but instead asample O of observations, with no mention of an underlying reality from which observations are obtained. The consistencyguarantees of the returned hypothesis are expected to be with respect to the given sample, while it is assumed that aperfectly consistent formula from the concept class exists. The compressor is also allowed to expend resources that increasewith the size of O. The compression requirement is accounted for by insisting that the size of the returned hypothesis growsonly linearly in |O|β , for some non-negative constant β less than 1. We next establish that a compressor for a learning taskis essentially a consistent learner for that learning task.Theorem 4.1 (Learning through compression). Consider a learning task (cid:6)xt, C, H(cid:7) over A. The concept class C is consistently learnableon xt by H if the concept class C is compressible on xt by H.Proof. Consider an algorithm L(cid:13)that is a compressor for the learning task (cid:6)xt , C, H(cid:7) over A. We construct an algorithm Las follows. Fix a probability distribution D supporting C for xt , with c being the target concept for xt under D, a maskingprocess mask, a real number δ ∈ (0, 1], and a real number ε ∈ (0, 1]. Then, algorithm L, given access to A, (cid:6)xt, C, H(cid:7), δ, ε,and an oracle returning observations drawn from mask(D), proceeds as follows:Algorithm L draws a sample O of a number of observations (to be determined later) from the oracle, and simulatesalgorithm L(cid:13)returns a hypothesis h, algorithm Lreturns the hypothesis h, and terminates.with input A, (cid:6)xt, C, H(cid:7), δ(cid:13) = δ/2, ε(cid:13) = ε/2, and O. When algorithm L(cid:13)and ε(cid:13)Consider the set H(cid:13)We now prove that algorithm L is a consistent learner for the learning task (cid:6)xt , C, H(cid:7) over A. To do so, we prove that,with probability 1 − δ, the returned hypothesis h is (1 − ε)-consistent with xt under D and mask, and that algorithm Lruns in time polynomial in 1/δ, 1/ε, |A|, and size(c).By construction of algorithm L, the simulated algorithm L(cid:13)is given access to the sample O. Clearly, c ∈ C is 1-consistentwith xt given O. By the choice of δ(cid:13), and by Definition 4.1, there exists a constant β ∈ [0, 1) such that algorithm L(cid:13)runs in time polynomial in 2/δ, 2/ε, |A|, size(c), and |O|, and returns, with probability 1 − δ/2, a hypothesis h ∈ H that is(1 − ε/2)-consistent with xt given O, and its size is linear in |O|β and polynomial in 2/δ, 2/ε, |A|, and size(c).|O|β and polynomial in 2/δ, 2/ε, |A|, and size(c). By Theorem 2.3, and setting γ = ε/2, each formula ϕ ∈ H(cid:13)with probability eprobability |H(cid:13)|ereturn, except with probability δ/2 + |H(cid:13)|eof all formulas in H that are (1 − ε/2)-consistent with xt given O, and their size is linear inis, except−|O|ε2/2, such that ϕ is (1 − ε)-consistent with xt under D and mask. By a union bound, except with−|O|ε2/2, every formula ϕ ∈ H(cid:13)is (1 − ε)-consistent with xt under D and mask. Overall, algorithm L will−|O|ε2/2, a hypothesis h ∈ H that is (1 − ε)-consistent with xt under D and mask.contains formulas with size only linear in |O|β and polynomial in 2/δ, 2/ε, |A|, and size(c), it follows thatlog |H(cid:13)| (cid:2) m|O|β · poly(2/δ, 2/ε, |A|, size(c)), for some constant m. Thus, the probability that algorithm L will return ahypothesis that is not (1 − ε)-consistent with xt under D and mask, is at mostSince H(cid:13)δ/2 + 2m|O|β ·poly(2/δ,2/ε,|A|,size(c))e−|O|ε2/2 (cid:2) δ/2 + 2−|O|β (|O|1−β ε2/2−m·poly(2/δ,2/ε,|A|,size(c))).Fixing |O| to be the least positive integer that exceeds the quantity(cid:8)(cid:8)2ε2log2δ(cid:9)+ m · poly(cid:10)2/δ, 2/ε, |A|, size(c)(cid:11)(cid:11) 11−β,trivially implies that |O|β (cid:3) 1, and also ensures that |O|1−βε2/2 − m · poly(1/δ, 1/ε, |A|, size(c)) (cid:3) log 2δ , making the prob-ability that algorithm L will return a hypothesis that is not (1 − ε)-consistent with xt under D and mask, be at most δ, asrequired.The running time of algorithm L comprises the time required to draw the sample O of observations, and the time. Both tasks are carried out in time polynomial in 2/δ, 2/ε, |A|, size(c), and |O|, and sincerequired to simulate algorithm L(cid:13)|O| is also polynomial in 1/δ, 1/ε, |A|, and size(c), the claim follows. (cid:2)4.2. Learnability through reductionsEfficiently reducing one problem to another is a natural approach to establish that solving the first problem is not harderthan solving the second one. Thinking of each input of a problem as corresponding to an instance of that problem, we term654L. Michael / Artificial Intelligence 174 (2010) 639–669the mapping between inputs an instance mapping. Thinking of an output of a problem as corresponding to a solution ofan instance of that problem, we term the mapping between outputs a solution mapping. Such reductions have been widelyemployed within the area of computational complexity, but also within the context of PAC learning [23], where the oracleavailable to the first learner is transformed to an oracle to be made available to the second learner, while the returnedhypotheses of the second learner are transformed to hypotheses to be returned by the first learner. Other inputs requiredduring learning (e.g., δ, ε) may also be transformed through the instance mapping. In the case of reductions betweenlearning problems, a solution mapping is referred to as a hypothesis mapping.The transformation of oracles in reductions in the context of PAC learning is relatively straightforward. Each completeobservation drawn from the original oracle is mapped to another complete observation, and the latter one is thought ofas being drawn from the transformed oracle. In the case of autodidactic learning, however, an oracle returns observationsdrawn from some probability distribution mask1(D1), where mask1 is not necessarily the identity mapping. Transformingthis oracle to another one could, in principle, be done by simply mapping each drawn observation to another observation.This, however, does not suffice. The induced probability distribution over the resulting observations needs to be express-ible in the form mask2(D2), so that the resulting observations may be thought of as masking examples drawn from someunderlying distribution D2. The following definition captures this requirement. For generality, we define one-to-many reduc-tions, where one learning problem may be transformed to many others. This generality is not invoked later on to obtain ourpositive learnability results. In fact, we are unaware of any learnability results that were obtained through a one-to-manyreduction. Whether one-to-many reductions are more powerful than one-to-one reductions in the context of learnabilityremains an interesting open problem.t , C j, H j(cid:7) overDefinition 4.2 (Reductions between learning tasks). A learning task (cid:6)xt, C, H(cid:7) over A is reducible to a set {(cid:6)xj=1 of learning tasks if there exists a hypothesis mapping g : H1 × · · · × Hr → H, and for every j: 1 (cid:2) j (cid:2) r thereA j}rexists an instance mapping f j : {0, 1, ∗}|A| → {0, 1, ∗}|A j |, and for every probability distribution D supporting C for xt ,with c being the target concept for xt under D, and every masking process mask, there exists a probability distribution D jsupporting C j for xt under D j , and a masking process mask j , such that:jt , with c j being the target concept for xjj(i) for every tuple (cid:6)h1, . . . , hr(cid:7) ∈ H1 × · · · × Hr , and every observation obs ∈ {0, 1, ∗}|A|, it holds that g((cid:6)h1, . . . , hr(cid:7)) hasja consistency conflict with xt w.r.t. obs only if there exists j: 1 (cid:2) j (cid:2) r such that h j has a consistency conflict with xtw.r.t. f j(obs);(ii) for every j: 1 (cid:2) j (cid:2) r, the induced probability distribution f j(mask(D)) is equal to mask j(D j);(iii) each of the instance and hypothesis mappings is computable in time polynomial in |A|, size(c), and the size of itsinput; both r, and size(c j) for every j: 1 (cid:2) j (cid:2) r, are polynomial in |A| and size(c).Roughly speaking, the three conditions of Definition 4.2 correspond, respectively, to the following requirements: Condi-tion (i) ensures that the transformations of inputs and outputs between the involved learning problems are such that highlyconsistent hypotheses in the resulting problems correspond to a highly consistent hypothesis in the original problem. At thesame time, Condition (ii) ensures that the instance mappings respect the requirement that the resulting observations maskexamples drawn from some appropriate probability distribution. Finally, Condition (iii) ensures that the entire reduction iscarried out efficiently. One may note that Definition 4.2 does not dictate how parameters δ and ε, which are, also, part ofthe inputs of a learning problem, are transformed. Indeed, given that the three aforementioned conditions hold, δ and εmay always be transformed appropriately; the proof of the following result illustrates this.Theorem 4.2 (Learning through reductions). Consider a learning task (cid:6)xt, C, H(cid:7) over A that is reducible to the set of learning tasksj=1. The concept class C is consistently learnable on xt by H if for every j: 1 (cid:2) j (cid:2) r, the concept class C j is{(cid:6)xjt , C j, H j(cid:7) over A j}rconsistently learnable on xjt by H j .t by H j , and let algorithm L jProof. Assume that for every j: 1 (cid:2) j (cid:2) r the concept class C j is consistently learnable on xt , C j, H j(cid:7) over A j . Let g : H1 × · · · × Hr → H be the hypothesis mapping, and for everybe a learner for the learning task (cid:6)xj: 1 (cid:2) j (cid:2) r, let f j : {0, 1, ∗}|A| → {0, 1, ∗}|A j |be the instance mapping, whose existence is guaranteed by Definition 4.2.We construct an algorithm L as follows. Fix a probability distribution D supporting C for xt , with c being the target conceptfor xt under D, a masking process mask, a real number δ ∈ (0, 1], and a real number ε ∈ (0, 1]. Then, algorithm L, givenaccess to A, (cid:6)xt , C, H(cid:7), δ, ε, and an oracle returning observations drawn from mask(D), proceeds as follows:jjFor every j: 1 (cid:2) j (cid:2) r, algorithm L simulates algorithm L j with input A j , (cid:6)xt , C j, H j(cid:7), δ j = δ/r, ε j = ε/r, and an oraclereturning observations. Whenever algorithm L j accesses the oracle and requests an observation, algorithm L draws anobservation obs from its own oracle, and passes f j(obs) to algorithm L j . When each simulated algorithm L j returnsa hypothesis h j , algorithm L computes and returns the hypothesis g((cid:6)h1, . . . , hr(cid:7)), and terminates.jL. Michael / Artificial Intelligence 174 (2010) 639–669655j(cid:6)rWe now prove that algorithm L is a consistent learner for the learning task (cid:6)xt , C, H(cid:7) over A. To do so, we prove that,with probability 1 − δ, the returned hypothesis g((cid:6)h1, . . . , hr(cid:7)) is (1 − ε)-consistent with xt under D and mask, and thatalgorithm L runs in time polynomial in 1/δ, 1/ε, |A|, and size(c).jt w.r.t.jt , with c j being the target concept for xt under D j and mask j . By a union bound it follows that with probability 1 −By construction of algorithm L, each simulated algorithm L j is given access to the oracle f j(mask(D)). By Condition (ii)jof Definition 4.2, there exists a probability distribution D j supporting C j for xt underD j , and a masking process mask j , such that f j(mask(D)) = mask j(D j). By Definition 2.3, algorithm L j runs in time poly-nomial in 1/δ j , 1/ε j , |A j|, and size(c j), and returns, with probability 1 − δ j , a hypothesis h j ∈ H j that is (1 − ε j)-consistentj=1 δ j = 1 − δ, every algorithm L j willwith xjreturn a hypothesis h j ∈ H j that is (1 − ε j)-consistent with xt under D j and mask j . Assume, now, that g((cid:6)h1, . . . , hr(cid:7)) isnot (1 − ε)-consistent with xt under D and mask. Thus, the probability that g((cid:6)h1, . . . , hr(cid:7)) has a consistency conflict withxt w.r.t. an observation obs ← mask(D) is more than ε. By Condition (i) of Definition 4.2, it follows that the probabilityt w.r.t. f j(mask(D)) = mask j(D j) is more thanthat there exists j: 1 (cid:2) j (cid:2) r such that h j has a consistency conflict with xε. By the pigeonhole principle it follows that there exists j: 1 (cid:2) j (cid:2) r such that the probability that h j has a consistencyjconflict with xt underD j and mask j . This event, however, happens with probability at most δ. Therefore, with probability 1 − δ, the returnedhypothesis g((cid:6)h1, . . . , hr(cid:7)) is (1 − ε)-consistent with xt under D and mask, which establishes the first claim.f j(mask(D)) = mask j(D j) is more than ε/r = ε j ; so, h j is not (1 − ε j)-consistent with xThe running time of algorithm L comprises the running time of the r simulated algorithms, the time required to simulateall the oracle calls of those algorithms through the application of the instance mappings, and the time required to obtainthe hypothesis g((cid:6)h1, . . . , hr(cid:7)) through the application of the hypothesis mapping. By Condition (iii) of Definition 4.2, eachof the instance mappings is computable in time polynomial in |A| and size(c); thus, the size |A j| of the set of attributes ineach of the resulting learning tasks is polynomial in |A| and size(c). By the same condition, the size size(c j) of the targetconcept in each of the resulting learning tasks is also polynomial in |A| and size(c). Since the same condition implies that ris polynomial in |A| and size(c), it follows that both 1/δ j = r/δ and 1/ε j = r/ε are polynomial in 1/δ, 1/ε, |A|, and size(c).Therefore, the input of each simulated algorithm L j is polynomial in 1/δ, 1/ε, |A|, and size(c), and since the running timeof algorithm L j is polynomial in its input, it is also polynomial in 1/δ, 1/ε, |A|, and size(c). This, in turn, implies that eachalgorithm L j accesses its oracle a number of times that is polynomial 1/δ, 1/ε, |A|, and size(c); hence, all applications ofthe instance mapping f j are computable in time polynomial 1/δ, 1/ε, |A|, and size(c). Furthermore, the running time of thesimulated algorithms implies that the size of (cid:6)h1, . . . , hr(cid:7) is polynomial in 1/δ, 1/ε, |A|, and size(c), and by Condition (iii)of Definition 4.2 the hypothesis mapping is computable in time polynomial in 1/δ, 1/ε, |A|, and size(c). In conclusion,algorithm L runs in time polynomial in 1/δ, 1/ε, |A|, and size(c). This concludes the proof. (cid:2)jOur motivating example of an algorithm for learning monotone conjunctions suggests the special case of reductionswhere observations in all the resulting learning tasks are complete. In terms of Definition 4.2, this corresponds to having,for each resulting learning task, an instance mapping whose codomain is that of complete observations.Definition 4.3 (Total reductions between learning tasks). As a special case of Definition 4.2, a reduction from a learning task(cid:6)xt, C, H(cid:7) over A to a set of learning tasks {(cid:6)xj=1 is total if for every j: 1 (cid:2) j (cid:2) r, the instance mappingis of the form f j : {0, 1, ∗}|A| → {0, 1}|A j |t , C j, H j(cid:7) over A j}r.jEstablishing total reductions is of particular interest for two reasons: (i) from a philosophical point of view, total re-ductions establish links between partial and complete observability, allowing one to identify conditions under which thelack of complete information does not affect learnability; (ii) from a more pragmatic point of view, these established linksalso relate autodidactic learnability to the well-studied PAC semantics, allowing one to carry positive results from the lattermodel to the former one.4.3. Monotonicity preserves learnabilityBy using reductions we now establish a general result, showing that monotonicity of the concept class compensatesfor missing information in observations, in the sense that if a concept class of monotone formulas is learnable under thestandard PAC semantics, then it remains so under the autodidactic learning semantics.A formula ϕ over a set of attributes A is monotone if for every pair of examples exm1, exm2 ∈ {0, 1}|A|such that{i | xi ∈ A; exm1[i] = 1} ⊆ {i | xi ∈ A; exm2[i] = 1}, it holds that val(ϕ | exm1) (cid:16) val(ϕ | exm2), where (cid:16) imposes thenatural ordering over the values {0, 1}. In words, changing the input of a monotone formula so that more attributes areassigned the value 1 may result in the formula’s value only remaining the same, or changing from 0 to 1. Consider, now,the value of a monotone formula ϕ on an observation obs. If val(ϕ | obs) ∈ {0, 1}, then clearly mapping all attributesmasked in obs to any {0, 1} value will not affect the value of the formula. Furthermore, if val(ϕ | obs) = ∗, then mappingattributes masked in obs either all to 0, or all to 1 will result in the formula obtaining the respective value. These simpleproperties suggest that partial observations may be replaced with complete observations so that the value of a monotoneformula is affected in a predictable manner. This predictability, then, facilitates the existence of total reductions.656L. Michael / Artificial Intelligence 174 (2010) 639–669Theorem 4.3 (Total self-reduction of monotone formulas). A learning task (cid:6)xt, C, H(cid:7) over A is total reducible to a learning task(cid:13)= xt , C(cid:13) = C, H(cid:13) = H, and the hypothesis mapping is restricted to be the identity mapping,(cid:6)xif the concept class C and the hypothesis class H are classes of monotone formulas, and C does not contain the tautology formula (cid:17).(cid:13)such that A(cid:13) = A, xtt, C(cid:13), H(cid:13)(cid:7) over A(cid:13)(cid:13), and every attribute xiProof. We first define the constructs whose existence is required by Definition 4.2. Define the hypothesis mapping g : H(cid:13) →H to be the identity mapping. Define the instance mapping f : {0, 1, ∗}|A| → {0, 1}|A(cid:13)|so that for every observation obs ∈{0, 1, ∗}|A|, it holds that: f (obs)[i] = 0 if obs[t] = ∗; f (obs)[i] = obs[t] if obs[i] = ∗ andobs[t] ∈ {0, 1}; f (obs)[i] = obs[i] if obs[i] ∈ {0, 1} and obs[t] ∈ {0, 1}. For every probability distribution D supporting Cfor xt , with c being the target concept for xt under D, and every masking process mask, define the probability distributionD(cid:13)to be the identitymapping.to be equal to the induced probability distribution f (mask(D)), and define the masking process mask(cid:13)∈ A(cid:13)We proceed to prove some properties of monotone formulas with respect to the instance mapping f . For every observa-tion obs ∈ {0, 1, ∗}|A|(cid:13), and every attribute xi∈ A(cid:13), the definition of f directly implies that:(cid:13)• if val(xi | obs) ∈ {0, 1} and obs[t] ∈ {0, 1}, then val(xi(cid:13)• if val(xi | obs) = ∗ and obs[t] ∈ {0, 1}, then val(xi| f (obs)) = val(xi | obs);| f (obs)) = obs[t].Thus, for every observation obs ∈ {0, 1, ∗}|A|(cid:13), and every formula ϕ(cid:13) ∈ {xt} ∪ C(cid:13) ∪ H(cid:13), it holds that:• if val(ϕ | obs) ∈ {0, 1} and obs[t] ∈ {0, 1}, then val(ϕ(cid:13) | f (obs)) = val(ϕ | obs);• if val(ϕ | obs) = ∗ and obs[t] ∈ {0, 1}, then val(ϕ(cid:13) | f (obs)) = obs[t].For Condition (i) of Definition 4.2, consider a hypothesis hhas a consistency conflict with xt w.r.t. obs. Then, {val(g(hit follows that val(hf (obs))} = {0, 1}, and, therefore, h(cid:13) | f (obs)) = val(g(h(cid:13)(cid:13)) | obs) and val(xt(cid:13)and an observation obs ∈ {0, 1, ∗}|A|(cid:13))(cid:13) ∈ H(cid:13)such that g(h(cid:13)) | obs), obs[t]} = {0, 1}. By the properties discussed above,|| f (obs)) = obs[t]. Hence, {val(h(cid:13)(cid:13) | f (obs)), val(xt(cid:13)t w.r.t. f (obs), as needed.has a consistency conflict with xCondition (ii) of Definition 4.2 follows trivially by definition of the probability distribution D(cid:13), and the masking process(cid:13)(cid:13)is the target concept for xt , we show that the formula cfor xt under. Consider any fixed observation obs drawn from mask(D). Clearly, c does not have a consistency conflict with xt w.r.t.(cid:13) | obs), obs[t]} = {val(c | obs), obs[t]} (cid:8)= {0, 1}. We proceed by case analysis on the remaining. To establish that D(cid:13)supports C(cid:13)(cid:13) = c ∈ C(cid:13)mask(cid:13)D(cid:13)obs, and thus {val(cpossibilities.• In the first case assume that obs[t] = ∗. By definition of the instance mapping f , it follows that val(x| f (obs)) = 0. Thus, any monotone formula ϕ other than the tautology is such that val(ϕ | f (obs)) = 0. In particular,∈ A(cid:13)(cid:13)i(cid:13)for all xival(c(cid:13)(cid:13) | f (obs)) = 0 = val(xt| f (obs)).• In the second case assume that obs[t] ∈ {0, 1} and val(c | obs) = obs[t]. By the properties discussed earlier, it follows(cid:13) | f (obs)) =| f (obs)) = obs[t]. The assumption implies that val(c(cid:13)(cid:13) | f (obs)) = val(c | obs) and val(xtthat val(c(cid:13)val(xt| f (obs)).• In the third case assume that obs[t] ∈ {0, 1} and val(c | obs) = ∗. By the properties discussed earlier, it follows that| f (obs)) = obs[t]. Therefore, val(c(cid:13)(cid:13) | f (obs)) = obs[t] and val(xt(cid:13)(cid:13) | f (obs)) = val(xt| f (obs)).val(c(cid:13)(cid:13) | f (obs)) = val(xt(cid:13)(cid:13) | exm(cid:13)) = val(xt; thus, c| f (obs)) for every observation f (obs) drawn fromIn each case we have established that val(cf (mask(D)). Since observations drawn from f (mask(D)) are complete, and since D(cid:13) = f (mask(D)), we obtain thatval(cis expressed by(cid:13)cw.r.t. D(cid:13)With regards to Condition (iii) of Definition 4.2, the instance mapping f and the hypothesis mapping g are clearly(cid:13)) is equal tocomputable in time linear in the size of their inputs, the number r of resulting learning tasks is 1, and size(csize(c). At this point the reduction has been established.| exm(cid:13)) for every example exm(cid:13)t under D(cid:13).(cid:13). Definition 2.1 implies that xt(cid:13)is the target concept for xdrawn from D(cid:13)(cid:13)The totality of the established reduction follows by Definition 4.3 and by definition of the instance mapping f . Thisconcludes the proof. (cid:2)Theorem 4.3 establishes that the monotonicity of formulas in the concept and hypothesis classes is a sufficient conditionunder which the lack of complete information does not affect learnability. Interestingly enough, consistent learnability frompartial observations reduces to consistent learnability of the same concept class from complete observations. Equally intrigu-ing is the fact that a hypothesis learned (from complete observations) in the resulting learning task, applies unmodified formaking predictions (on partial observations) in the original learning task. Since this same hypothesis is appropriate also forinformation recovery (cf. Theorem 2.2), it follows that a concrete strategy to accurately recover missing information is tosimply assign appropriate default truth-values to masked attributes during the learning phase, consistently learn from theresulting complete observations, and employ the learned hypothesis as is to make predictions.Two technical points are worth discussing here. The first one relates to the requirement for the tautology formula not tobe part of the concept class. This restriction is without loss of generality. An agent attempting to learn the structure of itsL. Michael / Artificial Intelligence 174 (2010) 639–669657environment may always employ sampling to determine, with high probability, whether the target concept for a given targetattribute could be the tautology, and employ Theorem 4.3 only when this is not the case. The second technical point relatesto the encoding of the value of the target attribute in certain attributes of the resulting learning task. Although agnostic tothis fact, an agent learning in the resulting learning task utilizes the value of the target attribute in a much more involvedmanner than its typical use as a means to test the predictions of hypotheses. What makes the established result non-trivial,is the fact that the returned hypothesis does not depend on the target attribute in the context of the original learning task,in which the hypothesis is eventually employed for making predictions.A useful sufficient condition for consistent learnability follows immediately by Theorems 4.2 and 4.3.Corollary 4.4 (Sufficient condition for consistent learnability). Consider a learning task (cid:6)xt, C, H(cid:7) over A. The concept class C isconsistently learnable on xt by H, if the concept class C is learnable by H under the Probably Approximately Correct semantics, andboth C and H are classes of monotone formulas.Building on known learnability results under the PAC semantics [5,14,28], Corollary 4.4 implies learnability results forcertain concept classes under the consistent learnability semantics. Distribution-specific PAC learnability results — wherelearning is expected to succeed only for a particular (often the uniform) probability distribution — do not fall under theauspices of Corollary 4.4, since the reduction that transforms a learning task in the context of consistent learnability to oneunder the PAC semantics, distorts the probability distribution over examples.Corollary 4.5 (Proper consistent learnability of certain concept classes). Each of the concept classes in {conjunctions, disjunctions,k-CNF, k-DNF, linear thresholds} of monotone formulas over A \ {xt}, is properly consistently learnable on the target attribute xt ∈ A.4.4. Shallowness preserves learnabilityOne of the tools employed by humans in modelling their environment is that of abstraction. The same tool can beemployed while learning. Structure captured by a complex formula can be abstracted into a monotone disjunction, witheach disjunct representing a complex situation. In some of these cases, the ability to learn the latter type of formulas(cf. Corollary 4.5) might imply the ability to learn the former one — this is so when abstraction is applied with certainmoderation. We develop next the notions necessary to model the process of abstraction during learning, and to determinethe degree of moderation that preserves learnability.In terms of a given formula ϕ over a set of attributes A, abstraction may be thought of as the process of substitutingnew attributes for sub-formulas of ϕ, in a manner prescribed by a set M of substitutions. Recall that we think of formulasas syntactic objects; equivalently, we may think of each formula as corresponding to a particular circuit that computes theformula. Abstraction, then, is the process of replacing parts of the representation of a formula (i.e., certain sub-circuits with(cid:13)their associated inputs) with new attributes. Each substitution in M is of the form xi(ψ)/ψ , and indicates that attribute(cid:13)is to be substituted for the sub-formula ψ . We require that M induces a bijection from sub-formulas ψ toxi(ψ)(cid:13), so that the substitution process is invertible, a property that is critical for the abstraction to makeattributes xi(ψ)sense. In the general case, substitutions may be applied non-deterministically on a formula ϕ, and more than one possibleresulting formula may be produced. The unique maximal subset of all such resulting formulas that obeys the followingconstraints is known as the basis of ϕ given M, and is denoted by basis(ϕ | M):∈ A(cid:13)∈ A(cid:13)(cid:13)(i) for every formula ϕ(cid:13) ∈ basis(ϕ | M), and every pair ψ1, ψ2 of sub-formulas of ϕ that belong in the set {ψ | xi(ψ)/ψ ∈(cid:13)M; xi(ψ) appears in ϕ(cid:13)}, there is no attribute xi ∈ A that is shared by ψ1 and ψ2;(cid:13)i(ψ)/ψ ∈ M}.| x(cid:13)(ii) each formula ϕ(cid:13) ∈ basis(ϕ | M) is over the new set of attributes A(cid:13) = {xi(ψ)Roughly speaking, Condition (i) asks that the abstracted components are independent of each other, a restriction imposedto ensure that learnability is preserved, while Condition (ii) asks that all attributes in ϕ are replaced during the substitutionprocess, a restriction imposed for notational convenience. Note that Condition (i) is trivially satisfied for a read-once formulaϕ, in which each attribute appears at most once. A number of valid and invalid sets of substitutions are illustrated inTable 2. An intuitive graphical illustration of the substitution process and the constraints it is defined to respect is depictedin Fig. 2.Definition 4.4 (Shallowness in classes of formulas). A class F of formulas over a set of attributes A is shallow for a class F (cid:13)of formulas over set of attributes A(cid:13)ϕ∈F basis(ϕ | M) such that foreach formula ϕ ∈ F , there exists a formula ϕ(cid:13) ∈ basis(ϕ | M) ∩ F (cid:13)w.r.t. a set M of substitutions, if F (cid:13)is a subset of(cid:12).A class F formulas that is shallow for a class F (cid:13)of formulas w.r.t. a set M of substitutions, contains formulas thatexhibit structure not fundamentally different (as determined by M) from the structure exhibited by formulas in F (cid:13)(cf.Table 2). Thus, any given class of read-once formulas is shallow for the same class of monotone formulas w.r.t. the set of658L. Michael / Artificial Intelligence 174 (2010) 639–669Table 2The bases of the formula (x80 → x5) ∨ (x7 ∧ x2) ∨ (x9 ⊕ x56), given various sets of substitutions. Whenever a set ofsubstitutions is valid, the formula’s underlying disjunctive nature is preserved in its basis.Set M of substitutions(cid:13)(cid:13)(cid:13){x3/(x9 ⊕ x56)}2/(x7 ∧ x2), x1/(x80 → x5), x(cid:13)(cid:13)(cid:13)2/(x9 ⊕ x56)}2/(x7 ∧ x2), x1/(x80 → x5), x{x(cid:13)(cid:13)(cid:13)(cid:13){x4/(x80 → x5)}3/(x9 ⊕ x56), x2/(x7 ∧ x2), x1/(x80 → x5), x(cid:13)(cid:13)2/(x7 ∧ x2)}1/(x80 → x5) ∨ (x9 ⊕ x56), x{x(cid:13)(cid:13)(cid:13)(cid:13)(cid:13){x5/x56}1/(x80 → x5), x4/x9, x3/x2, x2/x7, x(cid:13)(cid:13)(cid:13)3/x9}2/(x7 ∧ x2), x1/(x80 → x5), x{xBasis of formula given M(cid:13)(cid:13)(cid:13)}{x∨ x∨ x123(cid:13)invalid M: x2 not invertible(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∨ x∨ x{x∨ x3, x2124(cid:13)(cid:13)∨ x{x}21(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)5)}3) ∨ (x∨ (x{x∧ x⊕ x421{}(x56 is not replaceable)(cid:13)∨ x3}(cid:13)i(ψ)/ψ ∈ MFig. 2. Graphical illustration of the substitution process operating on a circuit that implements a formula ϕ over A. Each substitution x(cid:13)corresponds to a gadget that implements formula ψ , with xi(ψ) standing for the name of that gadget. For notational convenience, we assume that for every(cid:13)(cid:13)attribute xi ∈ A, the substitution xi in M. The substitution process amountsi /xi belongs in M; hence, each circuit input xi is implemented by gadget xto employing gadgets from M to replace the shaded parts of the circuit. Once a circuit input has been replaced with a gadget, it becomes unavailable, sothat other gadgets that refer to that input may no longer be employed. Nonetheless, gadgets may internally refer to the same input multiple times. Whenall the circuit inputs have been replaced with gadgets, we are left with a new circuit over new inputs that correspond to the names of the gadgets in M.The resulting circuit may vary, depending on the choice of gadgets that were employed. Every new circuit is a truncated version, and hence an abstraction,of the original circuit, and the formula ϕ(cid:13)that it implements is an element of the basis of ϕ given M.substitutions that replace each possible literal with a new attribute. Similarly, the class of read-once formulas in disjunctivenormal form is shallow for the class of disjunctions w.r.t. the set of substitutions that replace each possible term (i.e.,conjunction of literals) with a new attribute.In the context of learning, F and F (cid:13)may be viewed as representing the possible structures of two different envi-ronments; the structure of the first environment corresponds to some formula in F , while the structure of the secondenvironment corresponds to some formula in F (cid:13)w.r.t. M, then, implies that thestructure of the second environment is essentially an abstraction of the structure of the first one. As we have alreadypointed out, if the extent of this abstraction is moderate, then it might be possible to establish that learnability in theenvironment with the more abstract structure carries over to the environment with the more refined structure. The follow-ing definition describes conditions under which this is possible, in terms of M, which is what ultimately determines therelation between F and F (cid:13). Establishing that F is shallow for F (cid:13).Definition 4.5 (Moderately shallow learning tasks). A learning task (cid:6)xt , C, H(cid:7) over A is moderately shallow for a learning task(cid:13)(cid:6)x, if there exists a set M of substitutions such that:t, C(cid:13), H(cid:13)(cid:7) over A(cid:13)(cid:13)(i) basis(xt | M) = {xt(ii) M is enumerable in time polynomial in |A|;(cid:13)(iii) for every x}, C is shallow for C(cid:13)i(ψ)/ψ ∈ M, and every observation obs ∈ {0, 1, ∗}|A|w.r.t. M, and H is shallow for H(cid:13)w.r.t. M;, it holds that val(ψ | obs) is computable in timepolynomial in |A|.Theorem 4.6 (Reduction of moderately shallow learning tasks). A learning task (cid:6)xt, C, H(cid:7) over A is reducible to a learning task(cid:13)(cid:6)x, if the former is moderately shallow for the latter.t, C(cid:13), H(cid:13)(cid:7) over A(cid:13)Proof. Assume that the learning task (cid:6)xt, C, H(cid:7) over A is moderately shallow for the learning task (cid:6)xlet M be the set of substitutions whose existence is guaranteed by Definition 4.5.(cid:13)t , C(cid:13), H(cid:13)(cid:7) over A(cid:13), andL. Michael / Artificial Intelligence 174 (2010) 639–669659, there exists a hypothesis h ∈ H such that hWe first define the constructs whose existence is required by Definition 4.2. By Definition 4.4, for every hypothesis(cid:13) ∈ H(cid:13)(cid:13) ∈ basis(h | M), and by definition of the set M of substitutions,hh is unique; define the hypothesis mapping g : H(cid:13) → H to map formula hto this unique formula h. Define the instancemapping f : {0, 1, ∗}|A| → {0, 1, ∗}|A(cid:13)|(cid:13), and every attribute x, iti(ψ)holds that f (obs)[i(ψ)] = val(ψ | obs); for every example exm masked by obs, the definition of formula evaluationimplies that val(ψ | obs) ∈ {val(ψ | exm), ∗}, which by definition of the instance mapping fimplies that f (obs)[i(ψ)] ∈{ f (exm)[i(ψ)], ∗}, and thus that f (obs) masks f (exm). For every probability distribution D supporting C for xt , withc being the target concept for xt under D, and every masking process mask, define the probability distribution D(cid:13)tobe equal to the induced probability distribution f (D), and define the masking process mask(cid:13)so that for every exampleexm ∈ {0, 1}|A|iswell-defined.maps f (exm) to f (obs), where obs ← mask(exm); note that f (obs) masks f (exm), so mask(cid:13)so that for every observation obs ∈ {0, 1, ∗}|A|, mask(cid:13)∈ A(cid:13)(cid:13)We proceed to prove some properties of formulas with respect to the instance mapping f . For every observation obs ∈{0, 1, ∗}|A|(cid:13)i(ψ)/ψ ∈ M, the definition of f directly implies that:, and every x(cid:13)• val(xi(ψ)| f (obs)) = val(ψ | obs).Now, fix a formula ϕ ∈ {xt} ∪ C ∪ H, a formula ϕ(cid:13) ∈ basis(ϕ | M), and an observation obs ∈ {0, 1, ∗}|A|. We continue toshow how for each example exm(cid:13) ∈ {0, 1}|A(cid:13)|that is masked by f (obs), one may construct an example exm ∈ {0, 1}|A|that is masked by obs, and is such that val(ϕ(cid:13) | exm(cid:13)) = val(ϕ | exm). The sought example exm is obtained by startingfrom observation obs and proceeding as follows:(cid:13)(cid:13)i(ψ)/ψ ∈ M such that attribute xFor every x(cid:13)obs that appear in ψ so that ψ will evaluate to val(xi(ψ)arbitrary {0, 1} value to obtain exm.i(ψ) appears in ϕ(cid:13), and val(ψ | obs) = ∗, fix the masked attributes of| exm(cid:13)). Fix any remaining masked attributes of obs to anyBy the requirement that the sub-formulas of ϕ that were replaced to obtain ϕ(cid:13)do not share any attributes, it follows thatthe construction of example exm is well-defined. It is also clear that exm is masked by obs. Also, for each sub-formula ψ of(cid:13)(cid:13)| exm(cid:13)) = val(ψ | exm). Indeed, either val(ψ | obs) = ∗,ϕ that was replaced with an attribute xi(ψ) it holds that val(xi(ψ)in which case the construction of exm guarantees the claimed condition holds, or val(ψ | obs) ∈ {0, 1}, in which case theclaimed condition follows, since: val(ψ | exm) = val(ψ | obs), since obs masks exm; val(x| f (obs)) = val(ψ |(cid:13)obs), by definition of the instance mapping f ; val(x. Thus,i(ψ)for every example exm(cid:13) ∈ {0, 1}|A(cid:13)|obs, such that val(ϕ(cid:13) | exm(cid:13)) = val(ϕ | exm). This now implies that:(cid:13)i(ψ)| f (obs)), since f (obs) masks exm(cid:13)that is masked by f (obs), there exists an example exm ∈ {0, 1}|A|(cid:13)| exm(cid:13)) = val(xi(ψ)that is masked by• for every formula ϕ(cid:13) ∈ basis(ϕ | M), val(ϕ(cid:13) | f (obs)) = val(ϕ | obs) if val(ϕ | obs) ∈ {0, 1}.(cid:13) ∈ H(cid:13)For Condition (i) of Definition 4.2, consider a hypothesis h(cid:13)) has a consistency conflict with xt w.r.t. obs. Then, {val(g(h(cid:13)) ∈ H is such that h(cid:13) ∈ basis(g(h(cid:13)) | M). By the properties discussed above,and an observation obs ∈ {0, 1, ∗}|A|such that(cid:13)) | obs), obs[t]} = {0, 1}. By definition of the hy-it follows that| f (obs))} = {0, 1},(cid:13)(cid:13) | f (obs)), val(xt| f (obs)) = obs[t]. Hence, {val(h(cid:13)t w.r.t. f (obs), as needed.has a consistency conflict with x(cid:13)(cid:13)) | obs) and val(xtg(hpothesis mapping, g(hval(hand, therefore, h(cid:13) | f (obs)) = val(g(h(cid:13)Condition (ii) of Definition 4.2 follows immediately by definition of the probability distribution D(cid:13)process mask(cid:13), since D(cid:13) = f (D), and mask(cid:13)( f (D)) = f (mask(D)). To establish that D(cid:13)(cid:13) ∈ basis(c | M) ∩ C(cid:13)any formula cis the target concept for xt under D(cid:13)(cid:13)t under D(cid:13)is the target concept for x, Definition 2.1 implies that xt is expressed by c w.r.t. D, and thussupports C(cid:13), and the masking(cid:13)t , we show thatfor x; by Definition 4.4, such a formula exists. Since c(cid:2)Prval(c | exm) = exm[t] | exm ← D(cid:3)= 1.By the properties discussed above, it follows that val(cexample exm ∈ {0, 1}|A|. Hence,(cid:13)(cid:13) | f (exm)) = val(c | exm) and val(xt| f (exm)) = exm[t], for every(cid:2)(cid:9)valc(cid:10)(cid:13) | f (exm)(cid:9)= valPr(cid:10)| f (exm)(cid:13)xt| exm ← D(cid:3)= 1.Since it also holds that D(cid:13) = f (D), we conclude that(cid:2)(cid:9)valcPr(cid:13) | exm(cid:13)(cid:10)(cid:9)= val(cid:13)(cid:10)(cid:3)(cid:13)xt| exm(cid:13)| exm(cid:13) ← D(cid:13)(cid:13)is the target concept for x= 1.t under D(cid:13).Definition 2.1 implies that cWith regards to Condition (iii) of Definition 4.2, the instance mapping fis computable in the time required to traversethe set M of substitutions, and evaluate each of the associated sub-formulas on an observation; by Conditions (ii) and (iii)of Definition 4.5, both operations can be carried out in time polynomial in |A|. The hypothesis mapping g is computable in660L. Michael / Artificial Intelligence 174 (2010) 639–669the time required to read its input, and traverse the set M of substitutions to identify the sub-formula to be substituted foreach attribute in the input formula; by Condition (ii) of Definition 4.5, each sub-formula can be identified in time polynomial(cid:13)) is at most equal to size(c). At this point the reduction hasin |A|. The number r of resulting learning tasks is 1, and size(cbeen established, and the proof is complete. (cid:2)A generalized version of the sufficient condition for consistent learnability that was established by Corollary 4.4 followswhen Corollary 4.4 is taken in conjunction with Theorems 4.2 and 4.6.Corollary 4.7 (Generalized sufficient condition for consistent learnability). Consider a learning task (cid:6)xt, C, H(cid:7) over A. The concept classt, C(cid:13), H(cid:13)(cid:7)C is consistently learnable on xt by H, if the learning task (cid:6)xt , C, H(cid:7) over A is moderately shallow for the learning task (cid:6)xover A(cid:13)are classes of monotoneformulas.under the Probably Approximately Correct semantics, and both C(cid:13)is learnable by H(cid:13)and H(cid:13), C(cid:13)(cid:13)This generalized sufficient condition implies the consistent learnability of additional concept classes. For the classes ofconjunctions, disjunctions, and linear thresholds, the following result generalizes Corollary 4.5, by retracting the mono-tonicity assumption. For the classes of k-CNF and k-DNF formulas, the following result provides new consistently learnablesubclasses that are incomparable to the subclasses whose consistent learnability was established by Corollary 4.5, by sub-stituting the read-once property for the monotonicity property; k-CNF and k-DNF formulas may have none, either, or bothof these two properties. As for Corollary 4.4, distribution-specific PAC learnability results do not fall under the auspices ofCorollary 4.7.Corollary 4.8 (Proper consistent learnability of additional concept classes). Each of the concept classes in {conjunctions, disjunctions,read-once k-CNF, read-once k-DNF, linear thresholds} of formulas over literals in A \ {xt}, is properly consistently learnable on thetarget attribute xt ∈ A.In a preliminary version of this work it was incorrectly reported that the general classes of k-CNF and k-DNF formulasare properly consistently learnable. We find it informative to discuss the subtle, but critical, point that prevents our resultsfrom generalizing to these classes. Consider the formula ϕ1 ∨ ϕ2. When this formula is evaluated on an example exm,by definition it holds that val(ϕ1 ∨ ϕ2 | exm) = val(ϕ1 | exm) ∨ val(ϕ2 | exm); similar properties hold for other logicalconnectives. Observe, however, that such local evaluation of the formula cannot be carried out on partial observations. Indeed,if val(ϕ1 | obs) = val(ϕ2 | obs) = ∗, then val(ϕ1 ∨ ϕ2 | obs) cannot, in general, be uniquely determined. If, for instance,ϕ1 is semantically the negation of ϕ2, then val(ϕ1 ∨ ϕ2 | obs) = 1, whereas if ϕ1 and ϕ2 are semantically equivalent,then val(ϕ1 ∨ ϕ2 | obs) = ∗. Note that the locality of evaluation is restored if the formulas ϕ1 and ϕ2 are assumed notto share any attributes. Although we are unaware of any relevant formal result in the learning literature, it seems naturalto conjecture that locality of formula evaluation is essential for reductions to go through in learning settings. This, in turn,explains why while reductions can establish the learnability of general k-CNF and k-DNF formulas under the PAC semantics,they seem to be able to establish the learnability of only their read-once counterparts under the autodidactic learningsemantics.5. Negative learnability resultsWe have already pointed out that learnability under the PAC semantics is a special case of autodidactic learnability. Sofar, we have not excluded the possibility that the two learning models are equivalent in terms of the concept classes that arelearnable. On the contrary, our general positive learnability results indicate that the two models are equivalent on a broadset of concept classes, tempting one to conjecture that lack of information during learning does not render learnabilityany harder — this we have shown to be true, for instance, for concept classes of monotone formulas. In this section wemake some progress towards disproving such a conjecture. We show that two particular concept classes that are properlylearnable under the PAC semantics are not properly learnable under the autodidactic learning semantics (i.e., if one insiststhat the concept and hypothesis classes coincide). Such representation-specific non-learnability results have been studiedbefore in the context of PAC learnability [22], and do not preclude the possibility that such concept classes are non-properlylearnable. The non-proper learnability under the autodidactic learning semantics of the two concept classes discussed inthis section remains open.The negative results that we prove are with respect to consistent learnability. Note that accuracy implies consistency,irrespectively of the concealment degree of the masking process. Thus, our results also imply that learning accurately is notpossible in certain cases. It is worth emphasizing that the established negative results do not require the use of masking pro-cesses with a high degree of concealment. Indeed, observations with a masked target attribute do not constrain the learnerin any way (since any hypothesis makes consistent predictions on such observations), and thus using a masking processthat masks the target attribute in any observation does not offer any advantage. All our results employ only 0-concealingmasking processes. We also note that our results do not rely on using formulas that cannot be efficiently evaluated onobservations.We start with a general result that we later use to obtain specific negative autodidactic learnability results.L. Michael / Artificial Intelligence 174 (2010) 639–669661Theorem 5.1 (Sufficient condition for hard learning tasks). Fix an arbitrary positive integer n ∈ N. Consider a set of attributes A of sizepolynomial in n, and a learning task (cid:6)xt, C, H(cid:7) over A such that log |C| is of size polynomial in n. Assume that there exists an algorithmthat on input a 3-CNF formula χ of size n runs in time polynomial in n and outputs a set of observations O(χ ) ⊆ {0, 1, ∗}|A|suchthat:(i) every formula in H is evaluatable in time polynomial in n on every observation in O(χ );(ii) χ is satisfiable if there exists a formula in H that is 1-consistent with xt given O(χ );(iii) χ is satisfiable only if there exists a probability distribution D over {0, 1}|A|C for xt , and mask(D) is the uniform distribution over O(χ )., and a masking process mask such that D supportsThen, the concept class C is not consistently learnable on the target attribute xt by the hypothesis class H, unless RP = NP.Proof. Assume that the concept class C is consistently learnable on the target attribute xt by the hypothesis class H.Let L be a consistent learner for the learning task (cid:6)xt, C, H(cid:7) over A, and let q(·, ·, ·, ·) be the associated polynomial thatdetermines the running time of algorithm L given its input parameters. Consider the algorithm Lsat defined as follows:On input a 3-CNF formula χ of size n, algorithm Lsat constructs the set of observations O(χ ). It then proceeds tosimulate algorithm L with input A, (cid:6)xt , C, H(cid:7), δ = 1/3, ε = 1/2|O(χ )|, and an oracle returning observations. The sim-ulation is interrupted after q(1/δ, 1/ε, |A|, log |C|) time-steps. During the simulation, whenever algorithm L accessesthe oracle and requests an observation, algorithm Lsat draws an observation obs uniformly at random from O(χ ), andpasses obs to algorithm L. If the simulated algorithm L returns a hypothesis h ∈ H, algorithm Lsat checks and returnswhether h does not have a consistency conflict with xt w.r.t. any observation in O(χ ). If the simulation of algorithm Lis interrupted, algorithm Lsat returns false. In either case, algorithm Lsat terminates after returning a truth-value.We now prove that algorithm Lsat runs in time polynomial in n and determines whether a given arbitrary 3-CNF formulaχ of size n is satisfiable so that: if χ is unsatisfiable, then algorithm Lsat will return false with probability 1; if χ issatisfiable, then algorithm Lsat will return true with probability at least 2/3.Assume first that χ is unsatisfiable. By Condition (ii), every formula in H has a consistency conflict with xt w.r.t. someobservation in O(χ ). Therefore, algorithm Lsat will return false, as expected, irrespectively of whether algorithm L re-turns some hypothesis h ∈ H, or its simulation is interrupted.Assume now that χ is satisfiable. By Condition (iii), there exists a probability distribution D and a masking processmask such that D supports C for xt , and the oracle of algorithm L draws observations from mask(D); let c be the targetconcept for xt under D. By Definition 2.3, algorithm L will run in time q(1/δ, 1/ε, |A|, size(c)), and return, with probability1 − δ, a hypothesis h ∈ H that is (1 − ε)-consistent with xt under D and mask. Since size(c) (cid:2) log |C|, then the simulation ofalgorithm L will not be interrupted, and algorithm Lsat will obtain h. Since ε is strictly less than the probability with whichany particular observation from O(χ ) is drawn, it follows that, with probability 1 − δ, the returned hypothesis h ∈ H willhave no consistency conflict with xt w.r.t. any observation in O(χ ), and algorithm Lsat will verify this and return true, asexpected. Since δ = 1/3, the probability with which algorithm Lsat will correctly report that χ is satisfiable is at least 2/3.Since 1/δ = 3, 1/ε = 2|O(χ )| = poly(n), |A| = poly(n), and log |C| = poly(n), it follows that q(1/δ, 1/ε, |A|, log |C|) ispolynomial in n. The set of observations O(χ ) is constructible in time polynomial in n, observations are uniformly samplablefrom O(χ ) in time polynomial in n, and, by Condition (i), returned hypotheses are testable for consistency conflicts withxt w.r.t. observations in O(χ ) in time polynomial in n. Hence, algorithm Lsat runs in time polynomial in n. In conclusion,we have established the existence of an algorithm, namely algorithm Lsat, that solves an NP-complete problem within theresource constraints allowed for problems in RP. This implies that RP = NP, and concludes the proof. (cid:2)Under the standard computational complexity assumption that RP (cid:8)= NP, we present next intractability results on theproper consistent learnability of certain explicit concept classes that are known to be properly PAC learnable. Our resultshold even if at most three attributes are masked in observations, none of which is the target attribute, suggesting that theproperty of an agent’s sensing process that compromises consistent learnability is not the frequency with which informationis missing in the obtained appearances, but rather the context in which this happens.3 This realization is further corrobo-rated when viewed in conjunction with, and in contrast to, certain results from the literature that establish that learnabilityis not severely impaired when information in observations is missing independently at random on each attribute, despitethis giving rise to observations with possibly many simultaneously masked attributes [6].3 Recall, by Theorem 2.1, that a similar phenomenon occurs also when learned hypotheses are eventually employed by an agent for making predictions.The predictive accuracy of hypotheses, even highly consistent ones, is compromised in a manner that depends not only on the frequency with whichsensors hide information, but mainly on the context in which this happens. Intriguing is also the fact that in obtaining the impossibility of learning highlyaccurate hypotheses it suffices for the target attribute to be masked, whereas in obtaining the intractability of learning highly consistent hypotheses itsuffices for non-target attributes to be masked. Hence, masked “features” are, in some sense, associated with a more fundamental reason for unlearnabilityas compared to masked “labels” in learning instances.662L. Michael / Artificial Intelligence 174 (2010) 639–6695.1. Non-learnability of paritiesA parity formula over a set of attributes A is a formula of the form xi1⊕ · · · ⊕ xir , where ⊕ denotes the “exclusive or”binary operator. A parity formula evaluates to 1 on an example exm exactly when an odd number of the formula’s attributesare assigned the value 1 in exm.The concept class of parity formulas is one associated with numerous open problems in the learning literature. Nonethe-less, the concept class is known to be properly learnable under the PAC semantics [7,11], albeit using techniques that relycritically on the availability of an explicit set of complete observations. In particular, the concept class of parity formulas isknown to be unconditionally non-learnable in the representation-independent sense in the Statistical Query model [15], andnon-evolvable [31] indicating the singularity of this concept class. A justification of its singularity may appeal to the ex-treme sensitivity of parity formulas on their attributes; independently of the values of the remaining attributes, the changeof an attribute’s value affects the value of a parity formula. It is this property that the following result exploits. Interest-ingly, parity formulas are highly non-monotone, which is in accordance with the consistent learnability of concept classesof monotone formulas.Theorem 5.2 (Intractability of proper consistent learnability of parities). The concept class C of parities over A \ {xt} is not properlyconsistently learnable on the target attribute xt ∈ A, unless RP = NP.Proof. Fix an arbitrary positive integer n ∈ N. Let V = {v 1, v 2, . . . , vn} be the set of variables over which instances of 3-SAT−+of size n are defined. Construct the learning task (cid:6)xt , C, H(cid:7) over A as follows: Define A (cid:4) {x| v i ∈ V } ∪ {xt}, C to bei , xithe set of all parities over A \ {xt}, and H (cid:4) C.−+i and a(v i) (cid:4) xFor each variable v i ∈ V , we define a(v i) (cid:4) xi ; by construction, the mapping from the set of literals overV to the set of attributes A \ {xt} is bijective. For every 3-CNF formulam(cid:13)χ =(l j,1 ∨ l j,2 ∨ l j,3),j=1where each l j,k is a literal over V , denote by O(χ ) the set of observations that contains exactly the following:−+(i) for every i: 1 (cid:2) i (cid:2) n, the observation obsvar(i) that assigns the value 1 to attributes xi , xi−+and the value 0 to attributes in A \ {xi , xt};i , x(ii) for every j: 1 (cid:2) j (cid:2) m, the observation obscls( j) that assigns the value ∗ to attributes a(l j,1), a(l j,2), a(l j,3), the value 1to attribute xt , and the value 0 to attributes in A \ {a(l j,1), a(l j,2), a(l j,3), xt}., the value 1 to attribute xt ,(cid:14)We continue to establish that formula χ is satisfiable if (and only if) there exists a parity formula ϕ ∈ H that does nothave a consistency conflict with the target attribute xt w.r.t. any observation in O(χ ). Consider any set of literals τ over{a(l) | l ∈ τ } ∈ H be the corresponding parity formula; by the bijective property of a, the mapping fromV , and let ϕτ (cid:4)the set of literal-sets over V to the set H of hypotheses is bijective. We next prove certain properties of this mapping.The following derivation establishes a correspondence between truth-assignments induced by τ for χ , and the lack ofconsistency conflicts of ϕτ with xt w.r.t. the observations in O(χ ) that are of type (i):τ induces a truth-assignment for χ⇔for every i: 1 (cid:2) i (cid:2) n, exactly one of v i, v i belongs in τ⇔−+for every i: 1 (cid:2) i (cid:2) n, exactly one of xi belongs in ϕτi , x⇔for every i: 1 (cid:2) i (cid:2) n, ϕτ does not have a consistency conflict with xt w.r.t. obsvar(i).A second derivation establishes a correspondence between sets of literals determined by τ that would satisfy χ , and thelack of consistency conflicts of ϕτ with xt w.r.t. the observations in O(χ ) that are of type (ii):τ contains at least one literal from each clause of χ⇔for every j: 1 (cid:2) j (cid:2) m, at least one of l j,1, l j,2, l j,3 belongs in τ⇔L. Michael / Artificial Intelligence 174 (2010) 639–669663for every j: 1 (cid:2) j (cid:2) m, at least one of a(l j,1), a(l j,2), a(l j,3) belongs in ϕτ⇔for every j: 1 (cid:2) j (cid:2) m, val(ϕτ | obscls( j)) = ∗⇔for every j: 1 (cid:2) j (cid:2) m, ϕτ does not have a consistency conflict with xt w.r.t. obscls( j).Together, the two derivations imply that τ induces a satisfying truth-assignment for χ if and only if ϕτ does not have aconsistency conflict with xt w.r.t. any observation in O(χ ). This conclusion then, along with the bijection property of themapping, leads to the following derivation, which establishes the claim:χ is satisfiable⇔there exists a set of literals τ over V that induces a satisfying truth-assignment for χ⇔there exists τ such that ϕτ does not have a consistency conflict with xt w.r.t. any observation in O(χ )⇔there exists ϕ ∈ H that does not have a consistency conflict with xt w.r.t. any observation in O(χ ).To conclude the proof it suffices to show that the conditions of Theorem 5.1 are satisfied. Clearly, |A| is polynomialin n, and so is log |C|, since there are at most 2formulas in C. Also, the set of observations O(χ ) is constructible intime polynomial in n. For Condition (i) of Theorem 5.1, note that each parity ϕ ∈ H can be evaluated on each observationobs ∈ O(χ ) in time polynomial in n. Indeed, either an attribute in ϕ is masked in obs, in which case val(ϕ | obs) = ∗,or none of the attributes in ϕ is masked in obs, in which case the number of attributes that are assigned the value 1 inobs determines val(ϕ | obs).|A|Condition (ii) of Theorem 5.1 follows directly from the last derivation above. For Condition (iii) of Theorem 5.1, assumethat χ is satisfiable, and let τ be a set of literals over V that induces a satisfying truth-assignment for χ . Consider the setof examples that contains exactly the following:−+(i) for every i: 1 (cid:2) i (cid:2) n, the example exmvar(i) that assigns the value 1 to attributes xi , xi−+and the value 0 to attributes in A \ {xi , xt};i , x(ii) for every j: 1 (cid:2) j (cid:2) m, the example exmcls( j) that assigns the value 1 to the least (under some fixed ordering of theattributes in A) attribute a(l j,k) in the set {a(l j,1), a(l j,2), a(l j,3)} ∩ {a(l) | l ∈ τ }, the value 1 to attribute xt , and the value0 to attributes in A \ {a(l j,k), xt}., the value 1 to attribute xt ,Define the probability distribution D that returns each of these examples with equal probability. Define the masking processmask that maps each example to the observation in O(χ ) with the same subscript with probability 1. Clearly, mask(D)is uniform over O(χ ), and the target attribute xt is expressed by the parity formula ϕτ ∈ C w.r.t. the probability distribu-tion D; thus, D supports C for xt , and ϕτ is the target concept for xt under D. Condition (iii) of Theorem 5.1 follows, andthe proof is complete. (cid:2)When compared with related results from the literature (see, e.g., [22]), the existence of partial observations complicatesthe required reduction from an NP-complete problem that lies in the heart of the proof of Theorem 5.2. Yet, partial obser-vations also allow for a more flexible manipulation of the learning algorithm in the proof of Theorem 5.1 that is invoked inthe reduction, which then explains the ability to establish the particular intractability result. More precisely, the reductionrelies on constructing observations that in order to be explained consistently, require the learned hypothesis to depend onany non-empty subset of the masked attributes, without, however, the observations specifying which such subset is to bechosen. Such constraints allude to a combinatorial problem, which is precisely what the learning algorithm is expected tosolve. It is the case that with complete observations one may still force the learned hypothesis to depend on certain subsetsof attributes, but the possible dependencies on these attributes are necessarily restricted by the observations themselves inwhat seems to be a subtle, yet critical, manner.5.2. Non-learnability of decision listsA k-decision list over a set of attributes A is an ordered sequence (cid:6)c1, v 1(cid:7) . . . (cid:6)cr, vr(cid:7)(cid:6)(cid:17), vr+1(cid:7) of pairs comprising a con-dition ci that is a term of at most k ∈ N literals over A, and an associated decision v i that is a {0, 1} value. A decision listevaluates on an example exm to the value v i associated with the least-indexed condition ci that evaluates to 1 on exm;the tautology formula that appears as the last condition ensures that the evaluation process is well-defined.664L. Michael / Artificial Intelligence 174 (2010) 639–669The concept class of k-decision lists, for any constant k ∈ N, is known to be properly learnable under the PAC semantics[25]. The proper learnability is retained even if only monotone-term k-decision lists are considered. Unlike parity formulas,(monotone-term) k-decision lists are learnable under the Statistical Query semantics [15], and in the presence of randomclassification noise [1].Rivest [25], who introduced this concept class and established its PAC learnability, asked whether learnability is preservedwhen instead of complete observations one considers partial observations. In our notation, he defined agreement4 of aformula ϕ with an observation obs to mean val(ϕ | obs) = obs[t], assuming that the target attribute xt is never maskedin drawn observations. As posed, the question almost always admits a trivial negative answer: an observation obs generallymasks a set of examples across which the value of ϕ varies, implying that val(ϕ | obs) = ∗, and making ϕ “disagree” withobs. We recast the notion of “agreement” to what, we believe, is a more appropriate (and possibly the intended) form:a formula ϕ agrees with an observation obs if ϕ does not have a consistency conflict with the target attribute xt underobs. This notion of “agreement” is weaker, as it requires that val(ϕ | obs) = obs[t] only when val(ϕ | obs) ∈ {0, 1} andobs[t] ∈ {0, 1}. We partially answer this new question in the negative, by showing the concept class of monotone-term1-decision lists not to be properly consistently learnable from partial observations, unless RP = NP. The negative answercarries to Rivest’s original question, due to the stronger notion of “agreement” that he used.Theorem 5.3 (Intractability of proper consistent learnability of monotone-term 1-decision lists). The concept class C of monotone-term1-decision lists over A \ {xt} is not properly consistently learnable on the target attribute xt ∈ A, unless RP = NP.Proof. Fix an arbitrary positive integer n ∈ N. Let V = {v 1, v 2, . . . , vn} be the set of variables over which instances of 3-SAT−+of size n are defined. Construct the learning task (cid:6)xt , C, H(cid:7) over A as follows: Define A (cid:4) {x| v i ∈ V } ∪ {xt}, C to bei , xithe set of all monotone-term 1-decision lists over A \ {xt}, and H (cid:4) C.−+i and a(v i) (cid:4) xFor each variable v i ∈ V , we define a(v i) (cid:4) xi ; by construction, the mapping from the set of literals overV to the set of attributes A \ {xt} is bijective. For every 3-CNF formulam(cid:13)χ =(l j,1 ∨ l j,2 ∨ l j,3),j=1where each l j,k is a literal over V , denote by O(χ ) the set of observations that contains exactly the following:(i) the observation obszero that assigns the value 0 to attributes in A;−+(ii) for every i: 1 (cid:2) i (cid:2) n, the observation obsvar(i,0) that assigns the value 1 to attributes xi , xi−+xt , and the value 0 to attributes in A \ {xi , xt};i , x−+(iii) for every i: 1 (cid:2) i (cid:2) n, the observation obsvar(i,1) that assigns the value ∗ to attributes xi , xi−+xt , and the value 0 to attributes in A \ {xi , xt};i , x, the value 0 to attribute, the value 1 to attribute(iv) for every j: 1 (cid:2) j (cid:2) m, the observation obscls( j) that assigns the value ∗ to attributes a(l j,1), a(l j,2), a(l j,3), the value 1to attribute xt , and the value 0 to attributes in A \ {a(l j,1), a(l j,2), a(l j,3), xt}.Without loss of generality, for the remainder of this proof we consider only read-once monotone-term 1-decision lists,where no attribute appears in conditions more than once; for any monotone-term 1-decision list that violates this assump-tion one may simply drop all but the first condition out of those that contain any particular attribute, and obtain a newmonotone-term 1-decision list that respects the assumption and is equivalent to the first one.We continue to establish that formula χ is satisfiable if there exists a monotone-term 1-decision list ϕ ∈ H that doesnot have a consistency conflict with the target attribute xt w.r.t. any observation in O(χ ). Consider any monotone-term1-decision list ϕ ∈ H that does not have a consistency conflict with xt w.r.t. any observation in O(χ ), and let τϕ (cid:4) {l |(cid:6)a(l), 1(cid:7) ∈ ϕ} be a set of literals over V . We continue to verify that τϕ induces a satisfying assignment for χ . For eachobservation obs ∈ O(χ ), we identify the conclusions that follow given that ϕ ∈ H does not have a consistency conflict withxt w.r.t. obs. We proceed by case analysis on the four types of observations in O(χ ):for otherwise ϕ would evaluate to 1 on obsvar(i,0);(i) from observation obszero, it follows that the value corresponding to the tautology condition of ϕ is 0;(ii) for every i: 1 (cid:2) i (cid:2) n, from observation obsvar(i,0), it follows that ϕ does not contain both of the pairs (cid:6)x−+i , 1(cid:7),i , 1(cid:7), (cid:6)x−+i , 1(cid:7),i , 1(cid:7), (cid:6)xfor otherwise only the tautology condition of ϕ would be satisfied, and by Conclusion (i), ϕ would evaluate to 0 onobsvar(i,1);(iii) for every i: 1 (cid:2) i (cid:2) n, from observation obsvar(i,1), it follows that ϕ contains at least one of the pairs (cid:6)x4 Rivest [25] actually used the term “consistency” in his work, rather than the term “agreement” that is employed here. We avoid, however, the use ofthe term “consistency” in this context, as this term has a different meaning in our framework.L. Michael / Artificial Intelligence 174 (2010) 639–669665(iv) for every j: 1 (cid:2) j (cid:2) m, from observation obscls( j), it follows that ϕ contains at least one of the pairs (cid:6)a(l j,1), 1(cid:7), (cid:6)a(l j,2),1(cid:7), (cid:6)a(l j,3), 1(cid:7), for otherwise only the tautology condition of ϕ would be satisfied, and by Conclusion (i), ϕ wouldevaluate to 0 on obscls( j).Conclusions (ii) and (iii) imply that τϕ induces a truth-assignment for χ , and Conclusion (iv) implies that the inducedtruth-assignment is a satisfying one for χ ; thus, χ is satisfiable, as needed.To conclude the proof it suffices to show that the conditions of Theorem 5.1 are satisfied. Clearly, |A| is polynomial in|A||A|! formulas in C. Also, the set of observations O(χ ) is constructiblen, and so is log |C|, since there are at most 2 · 3in time polynomial in n. For Condition (i) of Theorem 5.1, note that each monotone-term 1-decision list ϕ ∈ H can beevaluated on each observation obs ∈ O(χ ) in time polynomial in n. Indeed, for each attribute in ϕ that is assigned thevalue 1 in obs one may prune the suffix of ϕ that follows the tuple (cid:6)ci, v i(cid:7) of ϕ whose condition ci is the attribute, andreplace the condition of the said tuple with the tautology formula (cid:17), without affecting the value of ϕ on obs. Similarly,for each attribute in ϕ that is assigned the value 0 in obs one may drop the tuple (cid:6)ci, v i(cid:7) of ϕ whose condition ci is theattribute, without affecting the value of ϕ on obs. After the monotone-term 1-decision list ϕ has been thus processed towill be masked in obs. By construction, val(ϕ | obs) = val(ϕ(cid:13) | obs), and clearlyobtain ϕ(cid:13), all remaining attributes in ϕ(cid:13)are the value 1, is 0 if all decisions in ϕ(cid:13)val(ϕ(cid:13) | obs) is 1 if all decisions in ϕ(cid:13)are the value 0, and is ∗ otherwise.(cid:15)Condition (ii) of Theorem 5.1 follows directly from the conclusions above. For Condition (iii) of Theorem 5.1, assumethat χ is satisfiable, and consider any set of literals τ over V that induces a satisfying truth-assignment for χ . Letϕτ (cid:4){(cid:6)a(l ), 0(cid:7)(cid:6)a(l), 1(cid:7) | l ∈ τ }(cid:6)(cid:17), 0(cid:7), where multiplication between tuples is taken to correspond to their noncommu-tative concatenation, and l is taken to traverse τ in some fixed order over the literals over V . By construction, for every+−v i ∈ V , the conditions xi appear in ϕτ , with the condition appearing second with a corresponding value 1 being theione associated with the truth-value of v i as determined by τ . By inspection, val(ϕτ | exm) = exm[t] for each example exmin the set of examples that contains exactly the following:, x(i) the example exmzero that assigns the value 0 to attributes in A;−+(ii) for every i: 1 (cid:2) i (cid:2) n, the example exmvar(i,0) that assigns the value 1 to attributes xi , xi−+and the value 0 to attributes in A \ {xi , xt};i , x±(iii) for every i: 1 (cid:2) i (cid:2) n, the example exmvar(i,1) that assigns the value 1 to the single attribute xi±{a(l) | l ∈ τ }, the value 1 to attribute xt , and the value 0 to attributes in A \ {xi , xt};that assigns the value 1 to all attributes Acls( j)the example exmcls( j)(iv) for every j: 1 (cid:2) j (cid:2) m,−+in the set {xi , xi} ∩in the set, the value 0 to attribute xt ,{a(l j,1), a(l j,2), a(l j,3)} ∩ {a(l) | l ∈ τ }, the value 1 to attribute xt , and the value 0 to attributes in A \ (Acls( j) ∪ {xt}).Define the probability distribution D that returns each of these examples with equal probability. Define the masking processmask that maps each example to the observation in O(χ ) with the same subscript with probability 1. Clearly, mask(D) isuniform over O(χ ), and the target attribute xt is expressed by the monotone-term 1-decision list ϕτ ∈ C w.r.t. the proba-bility distribution D; thus, D supports C for xt , and ϕτ is the target concept for xt under D. Condition (iii) of Theorem 5.1follows, and the proof is complete. (cid:2)The intractability result of Theorem 5.3 provides yet another indication that learnability from partial observations isharder than learnability from complete observations. This indication remains true even when learnability from completeobservations is restricted to the use of statistical queries [15], or the use of complete observations with random classificationnoise [1].6. Sensor-restricted learnabilityWe have taken the approach that in many domains an agent cannot a priori make any assumptions on the nature ofinformation loss that results from its imperfect sensors. This premise is reflected in the definition of learnability that wehave introduced, which asks that learning be possible for every masking process. In this section we turn our attention todomains where some bias exists on the way information is hidden when an agent senses its environment. Such a biasexists, for instance, in the way the human eye provides information on our surroundings in a spatially-dependent manner,hiding the values of those properties of the environment that lie outside the range of our sight. A cryptanalyst attemptingto break some decrypting device through the use of probes, may gain some insight on the internal workings of the deviceby obtaining readings independently at random from each of the probes attached to the device. A piece of text, viewed asan appearance of some underlying reality, presumably hides information asymmetrically, so that, for instance, the propertiesof the underlying reality that are false are hidden more often than those that are true.Bias on an agent’s sensors may be captured by letting the masking process that models them be a member of a classS of masking processes, known as the sensor class; the class contains the possible masking processes out of which one isused to obtain observations. Consistent learnability may then be redefined so that learning will be expected to be successfulonly if the employed masking process is a member of S; the sensor class S is available to the learner, in the same way thatthe concept and hypothesis classes are.666L. Michael / Artificial Intelligence 174 (2010) 639–669Definition 6.1 (Consistent learnability with restricted sensor class). An algorithm L is a consistent learner for a learning task(cid:6)xt, C, H(cid:7) over A with sensors in S if for every probability distribution D supporting C for xt , every masking processmask ∈ S, every real number δ ∈ (0, 1], and every real number ε ∈ (0, 1], algorithm L has the following property: givenaccess to A, (cid:6)xt, C, H(cid:7), S, δ, ε, and an oracle returning observations drawn from mask(D), algorithm L runs in timepolynomial in 1/δ, 1/ε, |A|, and the size of the target concept for xt under D, and returns, with probability 1 − δ, ahypothesis h ∈ H that is (1 − ε)-consistent with xt under D and mask. The concept class C over A \ {xt} is consistentlylearnable on the target attribute xt ∈ A by the hypothesis class H over A \ {xt} with sensors in in the sensor class S ifthere exists a consistent learner for (cid:6)xt, C, H(cid:7) over A with sensors in S.The existence of a restricted sensor class S may critically affect what is learnable, and thus, what information can berecovered in partial observations. First, the restrictions obeyed by S may guarantee that the agent’s sensors do not hideinformation in an entirely arbitrary manner, due to the exclusion of certain sensors from S. This might, then, imply that theagent obtains more information than what would have been the case had S been unrestricted. Second, the restricted sensorclass S may allow the agent to employ a learning algorithm tailored to S, while no learning algorithm may be known, oreven exist, for the general case. Through learned rules the agent may then be able to recover yet more information.6.1. Parameterized sensor classesIt is conceivable for an agent to have a bias on the characteristics of its sensors that depends on the structure of its envi-ronment. Consider, for instance, a student in an introductory Artificial Intelligence course, during the lecture that discusseswhat an “agent” is. The teacher, acting as the student’s sensors, presents positive and negative instances of “agents”, alongwith various properties of the entity depicted in each instance. In trying to learn what constitutes an “agent”, the studenthas a bias as to the sensing process through which appearances are obtained. For one, the sensing process never hidesinformation on the property of interest that states whether an entity is an “agent” or not; this bias is readily representablein terms of a restricted sensor class, as per Definition 6.1. The student, however, has an additional, more subtle, type ofbias on the sensing process: it never hides those properties that are important in defining what an “agent” is; the teacherensures that this is the case. The bias on the type of the student’s sensors depends on the definition of an “agent”, and thisbias would have been different had the definition been different.(cid:12)Formalizing a structure-dependent sensor class is straightforward. We simply update Definition 6.1 so that S is the unionc∈C Sc of subclasses, one for each possible structure c ∈ C of the environment, and then ask that learning succeeds forevery masking process mask ∈ Sc , where c is the target concept for xt under D. The learning algorithm is given accessonly to the sensor class S, and not the particular subclass Sc , since the actual structure of the agent’s environment remainsunknown. One may, in fact, generalize the definition even further, by allowing distribution-dependent sensor classes, wherethe bias on the characteristics of an agent’s sensors depends not only on the structure of the agent’s environment, but alsoon the precise probability distribution from which the reality is obtained; note that this probability distribution determinesalso the structure of the environment. We may then update Definition 6.1 so that S is the unionD SD of subclasses, onefor each possible probability distribution D, and ask that learning succeeds for every masking process mask ∈ SD . Similarto the case of structure-dependent sensor classes, the learning algorithm is still given access only to the sensor class S, andnot the particular subclass SD .(cid:12)Appearances provided by teachers do not only hide information about the underlying reality, but may also convey addi-tional information on the structure of the environment that would not normally be available. A teacher presenting an entityas a positive or negative instance of what an “agent” is, presents only a subset of the entity’s properties, but also conveysthe message that hidden information is irrelevant. This additional piece of information would not have been available to thestudent had complete information on the entity been presented. In a teacher-assisted learning context, the “don’t know”interpretation of hidden properties does no longer characterize the nature of the value ∗. Instead, depending on the setting,the value ∗ may be better interpreted as a new, distinct, and information-baring value, indicating, for instance, a value thatis “irrelevant”, “the most probable”, “non-deducible”, “costly to obtain”, or “always hidden”; in all these cases the value ∗provides implicit information that may, in fact, play a critical role in facilitating learnability.As a proof of concept, consider the concept class C of formulas in disjunctive normal form. The learnability of C byany hypothesis class under the PAC semantics remains one of the long-standing open questions in Computational LearningTheory, while the proper learnability of C when the DNF formulas are restricted to contain at most k ∈ N terms is knownto be intractable for every constant k (cid:3) 2, unless RP = NP [22]. Yet, a teacher with the power to determine which parts of arandomly drawn example will be made visible to a student might assist the learning process by hiding information that isirrelevant in each example. More precisely, given the learning task (cid:6)xt, C, H(cid:7) over A, where C and H are classes of k-termDNF formulas, consider a teacher that is modelled by the following structure-dependent sensor class Sc∈C Sc ,where for every masking process mask ∈ Sc , and every observation obs in the range of mask, the target attribute xt isnot masked in obs, if obs[t] = 1 then a maximal subset of attributes such that val(c | obs) = 1 is masked in obs, and ifobs[t] = 0 then no attribute is masked in obs. The next result shows the power of structure-dependent sensor classes.relevant(cid:12)=L. Michael / Artificial Intelligence 174 (2010) 639–669667Theorem 6.1 (Proper consistent learnability of k-term DNF formulas with sensors in Srelevant). The concept class C of formulas overA \ {xt} in disjunctive normal form with at most k ∈ N terms is properly consistently learnable on the target attribute xt ∈ A withsensors in Srelevant.Proof. We construct an algorithm L as follows. Fix a probability distribution D supporting C for xt , with c being the targetconcept for xt under D, a masking process mask ∈ Sc ⊆ Srelevant, a real number δ ∈ (0, 1], and a real number ε ∈ (0, 1].Then, algorithm L, given access to A, (cid:6)xt , C, C(cid:7), Srelevant, δ, ε, and an oracle returning observations drawn from mask(D),proceeds as follows:Algorithm L draws a sample O of a number of observations (to be determined later) from the oracle, constructs andxi ∈A\{xt };obs[i]=1 xi)∧ (returns the hypothesis h =xi ∈A\{xt };obs[i]=0 xi)), and terminates.obs∈O;obs[t]=1(((cid:16)(cid:7)(cid:7)We now prove that algorithm L is a consistent learner for the learning task (cid:6)xt, C, C(cid:7) over A with sensors in Srelevant.We show that: the returned hypothesis h ∈ C; h, with probability 1 − δ, is (1 − ε)-consistent with xt under D and mask;and algorithm L runs in time polynomial in 1/δ, 1/ε, |A|, and size(c).Note first that each observation obs ← mask(D) with obs[t] = 1 encodes one of the terms of the DNF formula c.Indeed, by construction of mask, whenever obs[t] = 1, it also holds that val(c | obs) = 1, and thus at least one term of cevaluates to 1 on obs. Thus, none of the attributes of this term are masked in obs. By construction of mask the rest of theattributes are masked. It follows that (xi ∈A\{xt };obs[i]=0 xi) is precisely a term of c, and hence h isa DNF formula with a subset of the terms in c; in particular, h ∈ C. Therefore, whenever c evaluates to 0 on an observation,so does h.xi ∈A\{xt };obs[i]=1 xi) ∧ ((cid:7)(cid:7)Consider now an observation obs ← mask(D) w.r.t. which h has a consistency conflict with xt , so that {val(h |obs), obs[t]} = {0, 1}. By construction of mask, it holds that obs[t] = val(c | obs), and hence {val(h | obs), val(c |obs)} = {0, 1}. By our preceding discussion, it holds that val(c | obs) = 1 and val(h | obs) = 0. Thus, obs encodes aterm of c that does not appear in h, and this implies that this term was not encoded in any of the observations in O.Consequently, if the probability of drawing such an observation is more than ε, then the probability of this observation notbeing part of O is less than (1 − ε)|O| (cid:2) e. When |O| = (cid:20)(1/ε) · ln(1/δ)(cid:21), this probability is less than δ, as needed.−ε|O|The running time of algorithm L is clearly linear in 1/δ, 1/ε, and |A|. This concludes the proof. (cid:2)Obtaining consistent learnability results with sensors in a sufficiently restricted sensor class is trivial. One need ensure onlythat enough attributes are masked in each drawn observation so that either the target attribute is masked, or the returnedhypothesis predicts “don’t know”. We emphasize that the proof of Theorem 6.1 does not rely on this technique. All maskingprocesses in Srelevant are 0-concealing for the target attribute xt w.r.t. any class of formulas, so that xt is never masked, andhighly consistent hypotheses are equally highly accurate (by Theorem 2.2). One may also verify that the returned hypothesesare simultaneously highly consistent and highly complete, in that they have a consistency conflict or predict “don’t know”with a total probability less than the input parameter ε. Thus, even if the returned hypotheses are forced to make {0, 1}predictions during the evaluation phase (either by changing the masking process to the identity mapping, or by assigningarbitrary values to the masked attributes), the accuracy of the returned hypotheses will not suffer. Overall, Theorem 6.1implies a learning algorithm for obtaining hypotheses that are highly accurate even when tested on complete observations,as per the PAC semantics.Theorem 6.1 exemplifies that attempts to examine learnability in domains where teachers choose which parts of theunderlying reality students are to observe, a situation that was shown to be naturally modelled through the parameterizationof sensor classes, may lead to a distortion of the meaning of missing information. Such domains essentially turn the value∗ into a third, in addition to {0, 1}, distinguished value, which may then be employed to transmit side information toan agent. This side information may then allow the agent to learn in environments where learnability would be provablyimpossible even with complete observations, as already illustrated. Such a treatment of missing information lies outside thescope of autodidactic learnability, as demonstrated by our choice not to permit the use of parameterized sensor classes inDefinition 6.1. This problem is further dealt with in the work of Greiner et al. [10].6.2. Special classes of sensorsThe assumption that the sensor class S is somehow restricted underlies many previous attempts in the literature tomodel learnability from partial observations. Most previous work implicitly assumes the existence of a teacher in the processof learning, and as such the corresponding learning settings are naturally modelled as relying on a parameterized sensorclass. The various learning models differ primarily on the type of restrictions they impose on the sensor class S, and thetype of parameterization they consider. These restrictions, in turn, explain why stronger learnability results may be obtainedin other learning models, when compared to those we have obtained under the autodidactic learning semantics. We revisitsome of the learning models discussed in Section 3, and summarize in Table 3 how their approach of partial observabilityduring the learning phase can be viewed under the unifying prism of a restricted or parameterized sensor class.668L. Michael / Artificial Intelligence 174 (2010) 639–669Table 3A unified view of various models that deal with the problem of learning from partial observations.Each model is associated with the restrictions and parameterization it imposes on the sensor class.Learning model[This work][28][9][29][6][27][2][1,3,13]Imposed restrictions and parameterization on sensor class S(cid:12)c∈C Sc , and each Sc is restricted so that:None.S =for every mask ∈ Sc , and every obs in the range of mask,obs[t] ∈ {0, 1}, andobs[t] = 1 if and only if val(c | obs) = 1.S =for every mask ∈ Sc , and every obs in the range of mask,obs[t] = val(c | obs).c∈C Sc , and each Sc is restricted so that:(cid:12)None a priori, but makes an implicit assumption as in [9]for learnability to be possible.For any given probability p, S = {maskbernoulli(p)} so that:for every example exm, and every xi ∈ A,Pr[obs[i] = ∗ | obs ← maskbernoulli(p)(exm)] = p.S is restricted so that:for every mask ∈ S, and every obs in the range of mask,obs[t] ∈ {0, 1}.For any given model parameter k, S is restricted so that:for every mask ∈ S, and every obs in the range of mask,|{xi | xi ∈ A; obs[i] (cid:8)= ∗}| = k.Effectively, when the set of attributes A is extended to A(cid:13) ∪ A,S = {maskhide}, where maskhide is the unique masking processthat maps each example to the unique observation obs thatmasks exm and is such that {xi | xi ∈ A(cid:13) ∪ A; obs[i] (cid:8)= ∗} = A.7. Outlook and future directionsWe have presented the autodidactic learning model that offers a principled treatment of partial information in a PAC-like learning setting. Although it allows the use of supervised learning techniques, the autodidactic learning model doesnot assume the presence of an external teacher, since supervision (i.e., the label of the target attribute) is provided only tothe extent that an agent’s sensors do so. Within this learning model we have shown that the principle known as Occam’sRazor, and the technique of reductions among learning problems are still applicable. Through reductions we have shownthat monotone and read-once formulas that are PAC learnable remain learnable even if learning examples are arbitrarilyincomplete. On the other hand, parities and monotone-term 1-decision lists, which are properly PAC learnable, are notproperly learnable from incomplete learning examples, even if the values of only three attributes are hidden.Numerous questions remain open: To what extent can shallowness be used to establish further learnability results?Are one-to-many reductions more beneficial than one-to-one reduction in the context of learnability? What other generaltechniques can be used to establish positive or negative learnability results? Can PAC learnable concept classes of formulasthat are not efficiently (e.g., general 3-CNF) or locally (e.g., general 2-CNF) evaluatable on partial observations be learned,or can a general result be proven that excludes the possibility of learning such formulas? Can learnability be improvedunder reasonable assumptions on the masking process, without sacrificing the autonomy of learning? Does it make sense toattempt to learn the structure of the masking process, in addition to the structure of the underlying examples? Is it possibleto establish the representation-independent non-learnability of some PAC-learnable concept class? Under what conditionscan a certain degree of completeness be guaranteed for learned hypotheses?Endowing learning algorithms with certain properties would significantly improve their practical applicability: One prop-erty would be to achieve running time independent of the observation size, and dependent on the number of only thenon-masked attributes. It is an easy exercise to show that the Winnow algorithm [17] for learning linear thresholds can bemodified to obtain this property. Another property would be the ability to exploit information in observations where thetarget attribute is masked; existing techniques for semi-supervised and unsupervised learning from complete informationsuggest that this direction is fruitful. Noise could also be dealt with. Due to the equivalent treatment of all attributes inthe autodidactic learning model, it might be harder to justify the consideration of certain forms of noise considered in theliterature, such as classification noise [1]. On the other hand, random noise across all attributes could be meaningfully con-sidered [6]. The extent to which reductions preserve noise-resilience could also be investigated. Conceivably, our obtainedalgorithms are, to some extent, noise-resilient since they build on existing noise-resilient PAC algorithms. Noise-resiliencecould alternatively be established by formulating a corresponding Statistical Query model as in the case of PAC learning[15]. Finally, it would be interesting to examine whether learning is possible from examples where attributes obey moregeneral types of correlation than that considered in this work. The role of learned rules in this setting may then changefrom an explanatory one that explains why the value of the target attribute is what it is given the values of the remainingattributes, to a descriptive one that simply describes what holds in examples.L. Michael / Artificial Intelligence 174 (2010) 639–669669We believe that the treatment of partial observability introduced herein may provide the basis for addressing certainbroader issues, both in the theoretical understanding and actual implementation of systems that sense their environmentvia the use of imperfect sensors, for which existing solutions may be problematic or artificial. Learning rules in parallel formultiple distinct target attributes cannot be expressed in typical PAC-like supervised learning models, because the targetattribute is a priori distinguished and treated differently; this is not the case in autodidactic learning. The use of learnedrules for reasoning, so that their conclusions can be chained is not meaningfully supported in learning models that assumecomplete information. Autodidactic learning, on the other hand, naturally accommodates reasoning as the process throughwhich some of the missing information is completed. Finally, domains where machine learning is typically employed, suchas that of the autonomous acquisition of unaxiomatized or commonsense knowledge from large corpora of text [19,30], canbe understood in a conceptually cleaner manner through autodidactic learning. Text can be naturally viewed as a partialdepiction of some underlying and not directly accessible reality, and, then, commonsense knowledge acquisition amounts tolearning to infer what holds in this reality [21]. Some of these considerations have been investigated [20].AcknowledgementsThe author is grateful to Leslie Valiant for his advice, and for valuable suggestions and remarks on this research. Usefulfeedback was also received from the anonymous IJCAI and AIJ reviewers.References[1] Dana Angluin, Philip D. Laird, Learning from noisy examples, Machine Learning 2 (4) (April 1988) 343–370.[2] Shai Ben-David, Eli Dichterman, Learning with restricted focus of attention, Journal of Computer and System Sciences 56 (3) (April 1998) 277–298.[3] Avrim Blum, Prasad Chalasani, Learning switching concepts, in: Proceedings of the Fifth Annual Workshop on Computational Learning Theory (COLT’92),July 1992, pp. 231–242.[4] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, Manfred K. Warmuth, Occam’s Razor, Information Processing Letters 24 (6) (April 1987) 377–380.[5] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, Manfred K. Warmuth, Learnability and the Vapnik–Chervonenkis dimension, Journal of theACM 36 (4) (October 1989) 929–965.[6] Scott E. Decatur, Rosario Gennaro, On learning from noisy and incomplete examples, in: Proceedings of the Eighth Annual Conference on ComputationalLearning Theory (COLT’95), July 1995, pp. 353–360.[7] Paul Fischer, Hans-Ulrich Simon, On learning ring-sum expansions, SIAM Journal on Computing 21 (1) (February 1992) 181–192.[8] Michael R. Garey, David S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, W.H. Freeman & Co, New York, USA,1979.[9] Sally A. Goldman, Stephen S. Kwek, Stephen D. Scott, Learning from examples with unspecified attribute values, Information and Computation 180 (2)(January 2003) 82–100.[10] Russell Greiner, Adam J. Grove, Alexander Kogan, Knowing what doesn’t matter: Exploiting the omission of irrelevant data, Artificial Intelligence 97 (1–2) (December 1997) 345–380.[11] David P. Helmbold, Robert H. Sloan, Manfred K. Warmuth, Learning integer lattices, SIAM Journal on Computing 21 (2) (April 1992) 240–266.[12] Wassily Hoeffding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (301) (March1963) 13–30.[13] Michael J. Kearns, Robert E. Schapire, Efficient distribution-free learning of probabilistic concepts, Journal of Computer and System Sciences 48 (3)(June 1994) 464–497.[14] Michael J. Kearns, Umesh V. Vazirani, An Introduction to Computational Learning Theory, The MIT Press, Cambridge, Massachusetts, USA, 1994.[15] Michael J. Kearns, Efficient noise-tolerant learning from statistical queries, Journal of the ACM 45 (6) (November 1998) 983–1006.[16] Roderick J.A. Little, Donald B. Rubin, Statistical Analysis with Missing Data, 2nd ed., John Wiley & Sons, Inc., New York, USA, 2002.[17] Nick Littlestone, Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm, Machine Learning 2 (4) (April 1988) 285–318.[18] John McCarthy, Appearance and reality, John McCarthy’s home page http://www-formal.stanford.edu/jmc/appearance.html, 30 August 2006.[19] Loizos Michael, Leslie G. Valiant, A first experimental demonstration of massive knowledge infusion, in: Proceedings of the Eleventh InternationalConference on Principles of Knowledge Representation and Reasoning (KR’08), September 2008, pp. 378–388.[20] Loizos Michael, Autodidactic learning and reasoning, PhD thesis, School of Engineering and Applied Sciences, Harvard University, USA, May 2008.[21] Loizos Michael, Reading between the lines, in: Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence (IJCAI’09), July2009, pp. 1525–1530.[22] Leonard Pitt, Leslie G. Valiant, Computational limitations on learning from examples, Journal of the ACM 35 (4) (October 1988) 965–984.[23] Leonard Pitt, Manfred K. Warmuth, Prediction-preserving reducibility, Journal of Computer and System Sciences 41 (3) (December 1990) 430–467.[24] Ronald L. Rivest, Robert Sloan, A formal model of hierarchical concept learning, Information and Computation 114 (1) (1994) 88–114.[25] Ronald L. Rivest, Learning decision lists, Machine Learning 2 (3) (November 1987) 229–246.[26] Joseph L. Schafer, John W. Graham, Missing data: Our view of the state of the art, Psychological Methods 7 (2) (June 2002) 147–177.[27] Dale Schuurmans, Russell Greiner, Learning default concepts, in: Proceedings of the Tenth Canadian Conference on Artificial Intelligence (AI’94), May1994, pp. 99–106.[28] Leslie G. Valiant, A theory of the learnable, Communications of the ACM 27 (11) (November 1984) 1134–1142.[29] Leslie G. Valiant, Robust logics, Artificial Intelligence 117 (2) (March 2000) 231–253.[30] Leslie G. Valiant, Knowledge infusion, in: Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI’06), July 2006, pp. 1546–1551.[31] Leslie G. Valiant, Evolvability, Journal of the ACM 56 (1) (January 2009) 3.1–3.21.