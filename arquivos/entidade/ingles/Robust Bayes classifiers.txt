Artificial Intelligence 125 (2001) 209–226Research NoteRobust Bayes classifiersMarco Ramoni a;(cid:3), Paola Sebastiani ba Children’s Hospital Informatics Program, Harvard Medical School, 300 Longwood Avenue,Boston, MA 02115, USAb Department of Mathematics and Statistics, University of Massachusetts, Amherst, MA 01003, USAReceived 12 November 1999; received in revised form 24 October 2000AbstractNaive Bayes classifiers provide an efficient and scalable approach to supervised classificationproblems. When some entries in the training set are missing, methods exist to learn these classifiersunder some assumptions about the pattern of missing data. Unfortunately, reliable information aboutthe pattern of missing data may be not readily available and recent experimental results show thatthe enforcement of an incorrect assumption about the pattern of missing data produces a dramaticdecrease in accuracy of the classifier. This paper introduces a Robust Bayes Classifier (RBC) ableto handle incomplete databases with no assumption about the pattern of missing data. In order toavoid assumptions, the RBC bounds all the possible probability estimates within intervals using aspecialized estimation method. These intervals are then used to classify new cases by computingintervals on the posterior probability distributions over the classes given a new case and by rankingthe intervals according to some criteria. We provide two scoring methods to rank intervals and adecision theoretic approach to trade off the risk of an erroneous classification and the choice of notclassifying unequivocally a case. This decision theoretic approach can also be used to assess theopportunity of adopting assumptions about the pattern of missing data. The proposed approach isevaluated on twenty publicly available databases. (cid:211) 2001 Elsevier Science B.V. All rights reserved.Keywords: Bayes classifier; Missing data; Probability intervals1. IntroductionSupervised classification is the task of assigning a class label to unclassified casesdescribed as a set of attribute values. This task is typically performed by first traininga classifier on a set of classified cases and then using it to label unclassified cases. The* Corresponding author.E-mail addresses: marco_ramoni@harvard.edu (M. Ramoni), sebas@math.umass.edu (P. Sebastiani).0004-3702/01/$ – see front matter (cid:211)PII: S 0 0 0 4 - 3 7 0 2 ( 0 0 ) 0 0 0 8 5 - 02001 Elsevier Science B.V. All rights reserved.210M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226supervisory component of this classifier resides in the training signal, which provides theclassifier with a way to assess a dependency measure between attributes and classes. NaiveBayes classifiers (NBCs) [4,11] have been among the first supervised classification methodsand, during the past few years, they have enjoyed a renewed interest and consideration [6].The training step for a NBC consists of estimating the conditional probability distributionsof each attribute given the class from a training data set. Once trained, the NBC classifiesa case by computing the posterior probability distribution over the classes via Bayes’Theorem and assigning the case to the class with the highest posterior probability.NBCs assumes that the attributes are conditionally independent given the class and thisassumption renders very efficient both training and classification. Unfortunately, when thetraining set is incomplete, that is, some attribute values or the class itself are reportedas unknown, both efficiency and accuracy of the classifier can be lost. Simple solutionsto handle missing data are either to ignore the cases including unknown entries or toascribe these entries to an ad hoc dummy state of the respective variables [15]. Both thesesolutions are known to introduce potentially dangerous biases in the estimates, see [9] for adiscussion. In order to overcome this problem, Friedman et al. [6] suggest the use of the EMalgorithm [3], gradient descent [20] or, we add, Gibbs sampling [7]. All these methods relyon the assumption that data are Missing at Random (MAR) [13], that is, the database is leftwith enough information to infer the missing entries from the recorded ones. Unfortunately,there is no way to verify that data are actually MAR in a particular database and, when thisassumption is violated, these estimation methods suffer of a dramatic decrease in accuracywith the consequence of jeopardizing the performance of the resulting classifier [21].This paper introduces a new type of NBC, called Robust Bayes Classifier (RBC), whichdoes not rely on any assumption about the missing data mechanism. The RBC is based onthe Robust Bayes Estimator (RBE) [18], an estimator that returns intervals containing allthe estimates that could be induced from all the possible completions of an incompletedatabase. The intuition behind the RBE is that, even with no information about the missingdata mechanism, an incomplete data set can still constrain the set of estimates that canbe induced from all its possible completions. However, in this situation, the estimator canonly bound the posterior probability of the classes. The first contribution of this paper isto provide a specialized closed-form, interval-based estimation procedure for NBCs, whichtakes full advantage of their conditional independence assumptions. Once trained, theseclassifiers are used to classify unlabeled cases. Unfortunately, Bayes’ Theorem cannotbe straightforwardly extended from standard point-valued probabilities to interval-valuedprobabilities. Nonetheless, the conditional independence assumptions underlying the NBCallows for a closed-form solution for the classification task, too. The second contributionof this paper is a new propagation algorithm to compute posterior probability intervalscontaining all the class posterior probabilities that could be obtained from the exactcomputation of all possible completions of the training set. These intervals are then rankedaccording to a score and a new case is assigned to the class associated with the highestranked interval. We provide two scoring methods: the first, based on the strong dominancecriterion [10], assigns a case to the class whose minimum posterior probability is higherthan the maximum posterior probability for all other classes. This criterion preserves therobustness of the classifier but may leave some cases unclassified and hence we provide aweaker criterion to improve the coverage. We also introduce a general decision-theoreticM. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226211framework to select the most appropriate criterion by trading off accuracy and coverage.As a by-product, this decision-theoretic approach provides a principled way to asses theviability of the MAR assumption for a given training set. We also show that, when thedatabase is complete, the RBC estimates reduce to the standard Bayesian estimates andtherefore the RBC subsumes the standard NBC as a special case. This approach is evaluatedon twenty publicly available databases.2. Naive Bayes classifiersAn NBC is better understood if we regard the m attributes and the set of q mutuallyexclusive and exhaustive classes as discrete stochastic variables. In this way, we candepict a NBC as a Bayesian network [6]—a directed acyclic graph where nodes representstochastic variables and arcs represent dependency relationships between variables—as shown in Fig. 1. In this network, the root node represents the set C of mutuallyexclusive and exhaustive classes and each attribute is a child node Ai . Each value cjof the variable C is a class and each attribute Ai bears a set of si values Ai D ak. Asshorthand, we will denote C D cj by cj and Ai D ak by aik. The graphical structure of theBayesian network representing the NBC encodes the assumption that each attribute Ai isconditionally independent of the other attributes given the class. The classifier, therefore,is defined by the marginal probability distribution fp.cj /g of the variable C and by a setof conditional probability distributions fp.aik D cj /g of each attribute Ai given each classcj . A consequence of the independence assumption is that all these distributions can beestimated from a training set D, independently from each other, as follows.Let n.aik; cj / be the frequency of cases in the training set D in which the attribute Aiappears with value aik and the class is cj and let n.cj / be the frequency of cases in thetraining set with class cj . When the training set D is complete, the Bayesian estimates ofp.aik j cj / and p.cj / arep.aik j cj / DP(cid:11)ij k C n.aik; cj /T(cid:11)ij h C n.aih; cj /Uhand p.cj / DP(cid:11)j C n.cj /T(cid:11)l C n.cl/Ul;(1)respectively. The quantities (cid:11)ij k and (cid:11)j can be regarded as frequencies of pair aik; cj and ofthe class cj , respectively, in an imaginary sample, representing the prior information aboutthe distributions of the attributes and the classes. The size (cid:11) of this imaginary sampleis called global prior precision. Further details are in [17]. Once the classifier has beenFig. 1. A Bayesian network representing an NBC with attributes A1; : : : ; A9 and a set C of classes.212M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226trained, we can use it for the classification of new cases. If we represent a case as a setof attribute values e D fa1k; : : : ; amkg, Bayes’ theorem yields the posterior probability of aclass cj given e asp.cj j e/ DQp.cj /PqhD1 p.ch/miD1 p.aik j cj /QmiD1 p.aik j ch/(2)and the case is assigned to the class with the highest posterior probability.From the computational point of view,the training of the classifier reduces tosummarizing the whole database D into m contingency tables Ti of dimension .q (cid:2) si /,each cell .j; k/ of the table Ti collecting the frequency of the pair .aik; cj /. In this way(i) the estimation of the q probability distributions of each attribute Ai conditional onthe classes c1; : : : ; cq can be done locally using the frequencies n.aik; cj / in thetable Ti , as the frequencies n.ahk; cj / in all other tables Th are irrelevant;(ii) the estimation of each probability distribution of the attribute Ai conditional on theclass cj can be done independently of the other classes, by using the frequenciesn.aik; cj / in the row j , and(iii) the estimation of the marginal distribution of the classes can be done in any one ofthe tables Ti , by using its row totals n.cj /.In other words, the estimation procedure can be performed table by table and, within eachtable, row by row. These properties were termed global and local parameter independenceby [22] and they are the source of the computational efficiency of the training process.When some entries in the training set D are missing, both accuracy and efficiencyof the NBC are under threat. The reasons for this situation become clear if we regardthe incomplete database as the result of a deletion process occurred on a complete(yet unknown) database. The received view on missing data [13] is based on thecharacterization of the deletion process. According to this approach, data are MissingCompletely at Random (MCAR), if the probability that an entry is missing is independentof both observed and unobserved values. They are Missing at Random (MAR), if thisprobability is at most a function of the observed values in the database. In all other cases,data are Informatively Missing. Under the assumption that data are either MAR or MCAR,the values of the unknown entries can be estimated from the observed ones and the deletionprocess is called ignorable. This property guarantees that the available data are sufficientto train the classifier but, unfortunately, it does not enjoy any longer the properties ofglobal and local parameter independence. Indeed, unknown entries induce three types ofincomplete cases:(i) cases in which the attribute Ai is observed and the class is missing;(ii) cases in which the class cj is observed and the value of the attribute Ai is missing;(iii) cases in which both the value of the attribute Ai and the class are missing.We denote the frequency of these cases by n.aik; ?/, n.?; cj / and n.?; ?/, respectively.Suppose now we had some estimation method able to compute the estimates in Eq. (1) byassigning a proportion of the frequencies n.aik; ?/, n.?; cj / and n.?; ?/ to the cell .j; k/ ineach contingency table Ti . As the reconstructed marginal frequency of each class needs tobe equal in all tables, the estimation cannot be done locally any longer, and the propertiesof local and global parameter independence are lost. One exception arises when the classis observed in all cases.M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226213Theorem 1. Suppose that the class is observed in all cases of the training set D, and thatthe entries are MAR. Then, the estimates in Eq. (1), with n.aik; cj / being the frequency offully observed pairs aik; cj and n.cj / being the class frequency, are the exact Bayesianestimates.The proof appears in [22]. When also some classes are missing, we can use one of theapproximate methods mentioned in the Introduction to compute the estimates in Eq. (1).However, these methods require the deletion process to be ignorable. When data areinformatively missing, the available entries are no longer sufficient to train the classifier.Furthermore, there is no way, yet, to check whether the deletion process responsible for themissing data is actually ignorable. These are the motivations behind the introduction of theRobust Bayesian Estimator (RBE) [18] and its application, in this paper, to the developmentof a robust version of the NBC.3. Robust estimationRecall that a NBC is trained by estimating the conditional probability distributionsfp.aik j cj /g and fp.cj /g from an database D. This section describes how to perform thistask when the database D is incomplete. We need the following definitions.Definition 1 (Consistency). Let D be an incomplete data set and let p.x/ be a probabilitythat we wish to estimate from D.(1) A consistent completion of D is any complete data set Dc from which D is obtainedvia some deletion process.(2) A consistent estimate of p.x/ is an estimate computed in a consistent completion ofD.(3) A consistent probability interval for p.x/ is an interval Tpinf .x/; psup.x/U containingall consistent estimates. A consistent interval is non-trivial if pinf .x/ > 0 andpsup.x/ < 1.(4) A consistent probability interval is tight when it is the smallest consistent probabilityinterval Tp.x/; p.x/U for p.x/.The difference between a consistent and a tight consistent probability interval is that, inthe former, the interval extreme points are lower and upper bounds for the set of consistentestimates, while in the latter, the extreme points are reached in some consistent completionof the database. The rest of this section is devoted to the construction of tight, consistentprobability intervals for the quantities p.aik j cj / and p.cj / defining an NBC.In order to estimate the conditional probability p.aik j cj / from an incomplete trainingset D, the RBE collects the frequencies n.aik; ?/, n.?; cj / and n.?; ?/ of incomplete casesinto the virtual frequencies n.aik; cj / and n.aik; cj /. These frequencies are then used tocompute the extreme points of the tight consistent probability interval for p.aik j cj /.The quantity n.aik; cj / is the maximum number of incomplete cases .Ai; C/ that can becompleted as .aik; cj / and it is given byn.aik; cj / D n.?; cj / C n.aik; ?/ C n.?; ?/:(3)214M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226On the other hand, the virtual frequency n.aik; cj / is the maximum number of incompletecases .Ai; C/ that can be ascribed to cj without increasing the frequency n.aik; cj / and itisn.aik; cj / D n.?; cj / Cn.aih; ?/ C n.?; ?/:(4)Xh6DkThe virtual frequencies are used to compute the values p.aik j cj / and p.aik j cj / that are,respectively, the minimum and the maximum estimate of p.aik j cj / that can be found inthe consistent completions of D and they are(cid:11)ij k C n.aik; cj /p.aik j cj / DPp.aik j cj / DP;T(cid:11)j h C n.aih; cj /U C n.aik; cj /h(cid:11)ij k C n.aik; cj / C n.aik; cj /T(cid:11)ij h C n.aih; cj /U C n.aik; cj /h:(5)XIt has been shown [18] that the interval Tp.aik j cj /; p.aik j cj /U is tight and consistent.We now consider the estimation of p.cj / and note that the virtual frequencies n.cj / andn.cj / are both equal to the number n.?/ of cases in D in which the class is not observed.We obtain tight consistent probability intervals for p.cj / by setting:p.cj / Dp.cj / DPP(cid:11)j C n.cj /T(cid:11)l C n.cl/U C n.?/l(cid:11)j C n.cj / C n.?/T(cid:11)l C n.cl/U C n.?/l;:(6)When the training set is complete, Eqs. (5) and (6) reduce to Eq. (1). Each set given by themaximum probability for the class cj and the minimum probabilities of the other classes,say f p.cj /; p.ch/; h 6D j g defines a probability distributionp.cj / Cp.ch/ D 1 for all j ,(7)h6Djso that the probability intervals Tp.cj /; p.cj /U are reachable, as defined by [2]. Bydefinition, if the probability p.aik j cj / is at its maximum value p.aik j cj /, then the virtualcounter n.aik; cj / absorbs the frequencies n.aik; ?/ and n.?; ?/ so that p.cj / D p.cj /and, for any other class ch, we have that p.aik j ch/ < p.aik j ch/ and p.ch/ D p.cj /.Similarly, if the probability p.aik j cj / is at its minimum value p.aik j cj /, then for anyother class ch, we have that p.aik j ch/ > p.aik j ch/. However, if the class is alwaysobserved, the virtual frequencies n.aik; cj / and n.aik; cj / are both equal to n.?; cj /,because n.aik; ?/ D n.?; ?/ D 0, for all i and k. In this case, the probabilities p.aik j cj / canvary independently and maxima and minima can be reached at the same time, for differentclasses cj .4. Robust classificationOnce trained, the classifier can be used to label unclassified cases. Given a new case, anNBC performs this task in two steps: first it computes the posterior probability of each classM. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226215given the attribute values, and then it assigns the case to the class with the highest posteriorprobability. In this section, we first show how to compute posterior probability intervals ofeach class and then how to rank these intervals to classify new cases.4.1. Posterior probability intervalsLet e D fa1k; : : : ; amkg be attribute values of a case e that we wish to classify. Withpoint-valued probabilities, the expression of the posterior probability of a class cj , given e,is given in Eq. (2). The next Theorem identifies non-trivial consistent probability intervalsfor the classes. The result generalizes the solution provided by [18] for Boolean classes. Wethen show that, when the training set D reports always the class, these consistent intervalsare also tight.Theorem 2. Let D be an incomplete data set. Then, the probability interval Tpinf .cj j e/;psup.cj j e/U withpsup.cj j e/ Dp.cj /andpinf .cj j e/ Dp.cj /Qp.cj /QmiD1 p.aik j cj / CmiD1 p.aik j cj /Ph6Dj p.ch/QmiD1 p.aik j ch/QQmiD1 p.aik j cj /p.cj /miD1 p.aik j cj / C maxffg; g 6D j g;(8)(9)where the set ffg; g 6D j g contains the q (cid:0) 1 quantitiesp.cg/mYiD1p.aik j cg/ CXl6Dj;gp.cl/mYiD1p.aik j cl/for g 6D j D 1; : : : ; q, is non-trivially consistent.Proof. To prove the theorem, we need to show that the interval Tpinf .cj j e/; psup.cj j e/Ucontains all the posterior probabilities p.cj j e/ that can be derived from the possiblecompletions of the training set and that pinf .cj j e/ > 0 and psup.cj j e/ < 1. The lasttwo inequalities are a simple consequence of the property 0 < p.aik j cj / 6 p.aik j cj / < 1and 0 < p.cj / 6 p.cj / < 1 enjoyed by the robust estimates. Hence, it is sufficient to showthat, for each j , pinf .cj j e/ 6 p.cj j e/ 6 psup.cj j e/, the quantity p.cj j e/ being anyclass posterior probability that can be computed from the consistent completions of thetraining set D. From Eq. (2), we can write p.cj j e/ asyj xjPh6Dj yhxhQmwhere yj D p.cj / and xj DiD1 p.aik j cj /. For fixed yj , the function f .xj ; yj /is concave, increasing in xj and decreasing in xh for h 6D j . From standard convexanalysis [19], it follows that, if the variables xj are constrained to vary in a hyper-rectangle,maxima and minima of the function are obtained in the extreme points of the constrainedregion. In particular, the function f .xj ; yj / is maximized by maximizing xj and byf .xj ; yj / Dyj xj C(10);216M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226PPh6Dj xh, and it is minimized by minimizing xj and by maximizingh6Dj xh.minimizingThis argument grounds the intuition of the proof: we will find maxima and minima of thefunction f .xj ; yj / in a hyper-rectangle containing the region of definition of the variablesxj , for yj fixed, and these maxima and minima induce upper and lower bounds for thefunction f .xj ; yj /. We then maximize and minimize these bounds with respect to yj . Thefirst step is to find this hyper-rectangle.If the probabilities p.aik j cj / could vary independently within the intervals Tp.aik j cj /;p.aik j cj /U, then the variables xj would vary independently in the Cartesian product C ofthe intervalsT xj xj U D"mYp.aik j cj /mYp.aik j cj /QiD1QmiD1 p.aik j cj / and xh DiD1#:QQmThus, setting xj DiD1 p.aik j ch/ yields the maximum of thefunction f .xj ; yj / in the hyper-rectangle C, for yj fixed. However, as noted in Section 3,the probabilities p.aik j cj / cannot vary independently so that the function f .xj ; yj / isdefined in a subset of C and the quantityyjyjQPPf1.yj / DmiD1 p.aik j cj /QmiD1 p.aik j cj / Ch6Dj yhis only an upper bound. Now we maximize the function f1.yj / with respect to yj , subjectj yj D 1 that is imposed by the fact that the probability intervalsto the constraintTp.cj /; p.cj /U are reachable, as shown in Eq. (7). This maximization yields the upperbound in Eq. (8). The minimum of the function f .yj ; xj / in the hyper-rectangle C, formiD1 p.aik j cj / and by maximizingyj fixed, is given by setting xj Dh6Dj xh. The latterQmiD1 p.aik j ch/, so thatquantities ish6DjmiD1 p.aik j ch/ and it is maximized bymiD1 p.aik j ch/h6DjPPPQQyjyjQPf2.yj / DmiD1 p.aik j cj / CmiD1 p.aik j cj /Qh6Dj yhis a lower bound for f .yj ; xj /. We minimize the function f2.yj / with respect to yj ,and the minimum is given by setting yj D p.cj / and by maximizing the function f3 DPl p.cl/ D 1 (cid:0) p.cj /.The function f3 is linear in the probabilities p.ch/ and hence its maximum is found byevaluating it in the extreme points of the constrained region, from which lower bound inEq. (9) follows. 2miD1 p.aik j ch/, subject to the constraint p.cg/ CmiD1 p.aik j ch/h6Dj p.ch/PQWhen the training set is complete, the RBE intervals reduce to the point estimates givenin Section 2, and the quantities in Eqs. (8) and (9) become identical to the posteriorprobability in Eq. (2). The interval Tpinf .cj j e/; psup.cj j e/U is consistent, as it containsall posterior probabilities p.cj j e/ that we would obtain by applying Bayes’ Theoremto all consistent estimates p.aik j cj / and p.cj /. The proof of Theorem 2 uses theconstraints imposed by the class probability intervals Tp.cj /; p.cj /U and mixes maximumand minimum probabilities coherently. However, the probabilities p.aik j cj /, for varyingj , are minimized and maximized independently and, in general, this may produce loosebounds. Still, when the class is observed in all cases, we can prove the tightness of thesebounds.M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226217Theorem 3. If the class cj is reported for every case e, the probability intervals definedbyp.cj j e/ Dp.cj /and byp.cj j e/ Dp.cj /QQp.cj /miD1 p.aik j cj / CmiD1 p.aik j cj /QPl6Dj p.cl/miD1 p.aik j cl/Qp.cj /QmiD1 p.aik j cj / CmiD1 p.aik j cj /Ph6Dj p.ch/QmiD1 p.aik j ch/(11)(12)are tight and consistent.Proof. If the class is always observed, we have that p.cj / D p.cj / D p.cj / and, as notedin Section 3, the probabilities p.aik j cj / can vary independently as j varies, so that theupper and lower bounds in Eqs. (8) and (9) are the maximum and minimum values of thefunction in Eq. (10). Note further that Eqs. (8) and (9) reduce to Eqs. (11) and (12). 24.2. Ranking intervalsThe previous section has shown how to compute consistent posterior probabilityintervals for the classes given a set e of attribute values. We can now use these intervalsto assign a case to a class, by associating each interval to a score and using the followingclassification rule.Definition 2 (Interval-based classification rule). Let e be a set of attribute values and lets.cj j e/ be scores associated with the probability intervals Tpinf .cj j e/; psup.cj j e/U. Eachcase with attribute values e is assigned to the class associated with the largest score.The interval-based classification rule is based on the intuition that the score s.cj j e/associated with the probability intervals Tpinf .cj j e/; psup.cj j e/U is a “meaningful”summary of the global information contained in the probability intervals. However, thisis not the unique requirement. Since the standard NBC classifies cases on the basis ofthe posterior probabilities of the classes given the attribute values, we require that theset of scores associated with the probability intervals Tpinf .cj j e/; psup.cj j e/U defines aprobability distribution, and henceXs.cj j e/ > 0for all j ;s.cj j e/ D 1:(13)jTheorem 2 ensures that the interval Tpinf .cj j e/; psup.cj j e/U contains all possibleconditional probabilities p.cj j e/ that can be computed from the consistent completionsof the training set D, and the variability within the intervals is due to the uncertainty aboutthe missing data mechanism. A conservative score derived from the strong dominancecriterion [10] provides a classification rule that does not require any assumption about themissing data mechanism.218M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226Definition 3 (Strong dominance score). Given a set of q consistent posterior probabilityintervals Tpinf .cj j e/; psup.cj j e/U, we define the strong dominance score as:8<sd .cj j e/ D:1 if pinf .cj j e/ > psup.ch j e/ for all h 6D j;0 if pinf .cj j e/ 6 psup.ch j e/ for some h 6D j:The interval-based classification rule induced by the strong dominance score classifiesa new case as cj if and only if the probability pinf .cj j e/ is larger than the probabilitypsup.ch j e/, for any h 6D j . Strong dominance is a safe criterion since it returns theclassification that we would obtain from all consistent completions of the training set D.However, when the probability intervals are overlapping, the strong dominance score isnot defined and we face a situation of undecidability. Moreover, the strong dominancescore is too conservative because the condition pinf .cj j e/ > psup.ch j e/, for all h 6D j ,is sufficient to yield the classification we would obtain, the complete training set beingknown, but it is not necessary. In order to increase the coverage of the classifier, we canweaken this criterion by making the minimal assumption that all missing data mechanismsare equally possible, thus making all values within the intervals Tpinf .cj j e/; psup.cj j e/Uequally likely. In this way, we summarize the interval into an average point by defining thescoresu.cj j e/ D psup.cj j e/ (cid:0) k(cid:0)psup.cj j e/ (cid:0) pinf .cj j e/(cid:1)D .1 (cid:0) k/psup.cj j e/ C kpinf .cj j e/;where k is chosen so that the scores fsu.cj j e/g satisfy the properties of Eq. (13). Hence,k DPP1 (cid:0)h pinf .ch j e/h.psup.ch j e/ (cid:0) pinf .ch j e//:A consequence of the consistency of the probability intervals Tpinf .cj j e/; psup.cj j e/Uthe extreme probabilities pinf .cj j e/ and psup.cj j e/ and the probabilityis thatp.cj j e/ that we would compute, from a complete training set D, are in the relationshippinf .cj j e/ 6 p.cj j e/ 6 psup.cj j e/. It follows thatj psup.cj j e/and, hence, that the quantity k is in the open interval .0; 1/. This last finding guaranteesthat the score su.cj j e/ is in the interior of the interval Tpinf .cj j e/; psup.cj j e/U and,consequently, that it cannot produce a classification rule that does not correspond to anyE-admissible classification rule compatible with the intervals Tpinf .cj j e/; psup.cj j e/U[12]. Note that the Hurwicz’s Optimism–Pessimism criterion—the usual solution for thesecircumstances [14,16]—does not guarantee this property. As the score su.cj j e/ alwaysleads to a decision, we term it a complete-admissible score.j pinf .cj j e/ 6 1 6PPDefinition 4 (Complete-admissible score). Given a set of q consistent posterior probabilityintervals Tpinf .cj j e/; psup.cj j e/U, we define the quantitysu.cj j e/ D psup.cj j e/ (cid:0)a complete-admissible score..psup.cj j e/ (cid:0) pinf .cj j e//.1 (cid:0)h pinf .ch j e//Ph.psup.ch j e/ (cid:0) pinf .ch j e//PM. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226219It is worth noting that the classification based on the complete-admissible score sub-sumes the one based on the strong dominance score because if pinf .cj j e/ > psup.ch j e/,for all h 6D j , then su.cj j e/ > su.ch j e/ for all h 6D j . When the condition to apply thestrong dominance score does not hold, the complete-admissible score lets us classify thecases left unclassified by the strong dominance score. This strategy may result in an in-creased classification coverage at the price of a lower accuracy.4.3. Which score?Both the strong dominance and the complete-admissible score provide a sensible basisfor robust classification. Strong dominance is safe at the price of leaving cases unclassifiedwhile the complete-admissible score increases the classification coverage by loosingrobustness. The choice of an interval-scoring method depends on the features of theproblem at hand and, in this section, we provide a principled way to choose the bestinterval-based classification strategy.A classification system is typically evaluated on the basis of its classification accuracy (cid:18)and its coverage (cid:13) . The former is the probability of correctly classifying a case while thelatter is the probability of classifying one case. Let (cid:18)d and (cid:13)d be respectively the accuracyand the coverage of an RBC with the strong dominance score (RBCd ). The accuracy (cid:18)d isindependent of the missing data mechanism. Similarly, let (cid:18)u be the accuracy of the RBCwith the complete-admissible score, say RBCu. The accuracy (cid:18)u of the RBCu is given bytwo components. The first component is the probability of correctly classifying one casewhen we can use the strong dominance score and, hence, it is weighted by the coverage (cid:13)d .The second component is the probability of correctly classifying one case when we cannotuse the strong dominance score and, therefore, it is weighted by 1 (cid:0) (cid:13)d . Thus,(cid:18)u D (cid:18)d (cid:13)d C (cid:18)ul.1 (cid:0) (cid:13)d /;(14)where (cid:18)ul is the classification accuracy of the RBCu on the cases left unclassified by theRBCd and we term it residual accuracy. Residual accuracy provides a measure of thegain/loss of classification accuracy achieved by the RBCu when one relaxes the strongdominance criterion to increase coverage.The decomposition in Eq. (14) provides a first basis to choose the scoring method. Forexample, a simple rule could be to adopt the complete-admissible score if (cid:18)ul is greaterthan 1=q, so that the cases left unclassified by the strong dominance score are classifiedby the complete-admissible score better than at random. The intuition behind this rule isthat accuracy is more valuable than coverage and, hence, we would not prefer a methodthat classifies randomly just because it always classifies a case. The rationale is that weexpect the consequence of a wrong classification to be worse than the inability to classifyone case. This argument can be used formally to choose between the strong dominance orthe complete-admissible score by introducing mis-classification costs and costs incurredfor the inability to classify one case. Suppose that the cost incurred for not being ableto classify a case with attribute values e is a quantity Ci , while the cost for a wrongclassification is Cw. Since the former event occurs with probability 1 (cid:0) (cid:13)d and the latteroccurs with probability .1 (cid:0) (cid:18)d /(cid:13)d , the expected cost incurred on using the RBCd isC.RBCd / D Cw.1 (cid:0) (cid:18)d /(cid:13)d C Ci .1 (cid:0) (cid:13)d /220M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226if correct classification yields no cost. On the other hand, the expected cost incurred by anRBCu achieving 100% coverage with accuracy (cid:18)u isC.RBCu/ D Cw.1 (cid:0) (cid:18)u/:In order to minimize the cost, RBCd is to be preferred to RBCu when C.RBCd / 6 C.RBCu/.This is true if and only if (cid:18)u (cid:0) (cid:18)d (cid:13)d D (cid:18)ul.1 (cid:0) (cid:13)d / 6 .1 (cid:0) (cid:13)d /.1 (cid:0) Ci=Cw/ and it yields thedecision rule given in the next theorem.Theorem 4. Let Ci and Cw denote respectively the cost of a wrong classification and thecost of not being able to classify a case. The interval based classification rule which usesthe strong dominance score yields minimum expected cost if and only if:(cid:18)ul 6 .1 (cid:0) Ci=Cw/where (cid:18)ul is the accuracy of the RBCu on the cases left unclassified by the RBCd .For example, if Ci D Cw, the best decision is to choose the RBCd whenever (cid:18)ul > 0.Compared to the simpler rule described above, the decision now takes into account thetrade-off between accuracy and coverage. In practical applications, the quantities (cid:18)d , (cid:18)uand (cid:13)d can be estimated from the available data using cross validation, as shown in thenext section. Suppose now the quantity (cid:18)a is the accuracy of any other NBCa trainedon an incomplete data set under some assumption about the missing data mechanism.For example, (cid:18)a could be the accuracy of an NBC trained on an incomplete data setunder the assumption that data are MAR. We can use the same decision rule to helpone decide whether the RBC with the strong dominance or the complete-admissible scoreyields minimum expected costs. As a by-product, the decision rule can be interpreted asan evaluation of the consequences of enforcing the MAR assumption. The comparisonbetween the accuracy measures (cid:18)a and (cid:18)u is cost-independent, as we compare C.RBCu/ DCw.1 (cid:0) (cid:18)u/ and C.NBCa/ D Cw.1 (cid:0) (cid:18)a/ and the minimum expected cost is achievedby the system having the highest accuracy. If we now compare the expected costs ofthe RBCd and the NBCa , and apply the decision rule in Theorem 4, we have that theNBCa is to be preferred to the RBCd whenever (cid:18)ul > .1 (cid:0) Ci =Cw/ and the quantity.1 (cid:0) (cid:13)d /T(cid:18)ul (cid:0) .1 (cid:0) Ci =Cw/U is the cost incurred in enforcing the assumption about themissing data mechanism. This solution can be easily extended to cases in which mis-classification costs vary with the classes.5. EvaluationThis section reports the results of an experimental evaluation of the RBC on twentyincomplete data sets. The aim of the evaluation is to compare the performance of theRBC with that of two NBCs, using the most common solutions to handle missing data [6,15]: remove the missing entries (NBCm) and assign the missing entries to a dummy state(NBC(cid:3)). Since all data sets always report the classes for every case, by Theorem 1 NBCm isa faithful implementation of the MAR assumption. NBC(cid:3), on the other hand, assumes someknowledge on the missing data mechanism since the missing data are treated as a category“other”, not reported in the observed data.M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–2262215.1. Materials and methodsThe experimental evaluation was conducted on the twenty databases reported in Table 1,available from the UCI Machine Learning repository [1]. The database KDD99 consists of4704 cases on 31 variables selected from the database used for the 1999 KDD cup. Thesedatabases offer a variety of different data types: all attributes of the database Voting record(Vote) are binary, all attributes in Breast Cancer Wisconsin (B.Cancer) and Lung Cancer(L.Cancer) and Bridge are nominal, all attributes in Hepatitis and Mushrooms are discrete,while for example Annealing, Credit, Cylinder, Horse Colic, and Sick offer a good mixtureof continuous, discrete and nominal attributes. The size of these databases ranges from the32 case on 56 attributes of Lung Cancer to the 48842 cases on 14 variables in Mushrooms.Continuous attributes were discretized by dividing the observed range into four bins withthe same proportion of entries.Following current practice [8], we compared the accuracy of classifiers by running, oneach data set D, 5 replicates of a 5-fold cross validation experiment. On each database,we ran four tests: one training the NBC on a database with the missing entries removed(NBCm), one assigning the missing entries to a dummy state (NBC(cid:3)), one using the strongdominance score (RBCd ) and one using the complete-admissible score (RBCu). In all cases,we computed the estimates using a uniformly distributed global prior precision (cid:11) D 1. Foreach test, we report two values: accuracy—estimated as the average number of cases thatwere correctly classified in the test sets—and coverage—given by the ratio between thenumber of cases classified and the total number of cases in the data set. The 95% confidencelimits are based on a Normal approximation of a proportion estimator [8].5.2. Results and discussionTable 1 reports the results. The accuracy of RBCd is overall the highest, with a gainranging from 0.02% (Breast Cancer), in which there are only 6 missing entries in a dataset of 699 cases, to 16.77% (Horse Colic), in which data are heavily missing. Except forAudiology, Breast Cancer, and Lung Cancer, the accuracy gain of RBCd is statisticallysignificant in all cases, as shown by the non overlapping confidence intervals. This gainof accuracy is counter-balanced by a loss of coverage that can be as small as 6.51% inHorse Colic. The complete-admissible score increases the coverage to 100% at the price ofreducing the accuracy, so that in Audiology, Breast Cancer, and Credit it is out-performedby the standard NBC. However, the difference in accuracy is within the sampling variability,as the associated confidence limits are roughly the same, and probably data are MAR inthese data sets. On the other hand, the accuracy gain of RBCu over NBCm and NBC(cid:3) issignificant in all the other data sets, and reaches 10.19% in the Annealing data set, thusconfirming the potential danger of wrongfully enforcing the MAR assumption.As noted in Section 4.3, the strong dominance score partitions the data into two parts.One part comprises the cases on which there is no classification ambiguity and theaccuracy is only model-dependent. The remaining part comprises those cases that cannotbe classified without some assumption about the missing data mechanism. Using thenotation of Section 4.3, the accuracy on these cases of the other systems achieving 100%coverage is given by the quantity222M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226Table 1Accuracy of NBCm, NBC(cid:3), RBCd , RBCu. Maximum values are reported in boldfaceDatabaseNBCmNBC(cid:3)RBCdRBCu1 Adult2 Annealing3 Arythmia4 AudiologyAccuracyAccuracyAccuracyCoverageAccuracy81:74 (cid:6) 0:2381:22 (cid:6) 0:2286:51 (cid:6) 0:2181:72 (cid:6) 0:1882:50 (cid:6) 0:2086:54 (cid:6) 2:8880:88 (cid:6) 3:3297:53 (cid:6) 1:5149:12 (cid:6) 4:8796:73 (cid:6) 1:2464:40 (cid:6) 2:2561:05 (cid:6) 2:7676:09 (cid:6) 3:2539:82 (cid:6) 2:3066:19 (cid:6) 2:3358:34 (cid:6) 3:4955:50 (cid:6) 3:5163:41 (cid:6) 5:3234:78 (cid:6) 3:4855:50 (cid:6) 3:515 Automobile60:48 (cid:6) 3:4158:05 (cid:6) 3:4568:49 (cid:6) 3:8471:22 (cid:6) 3:1661:96 (cid:6) 3:396 B.Cancer97:42 (cid:6) 0:6697:42 (cid:6) 0:6697:49 (cid:6) 0:6799:65 (cid:6) 5:2397:23 (cid:6) 0:677 Bridge8 Credit9 Cylinder67:62 (cid:6) 4:5764:76 (cid:6) 4:6680:00 (cid:6) 4:7866:67 (cid:6) 4:6069:52 (cid:6) 4:4984:88 (cid:6) 1:3084:88 (cid:6) 1:3087:48 (cid:6) 1:7295:40 (cid:6) 5:2184:70 (cid:6) 1:3173:70 (cid:6) 3:7173:00 (cid:6) 3:7491:71 (cid:6) 4:1431:30 (cid:6) 6:9774:26 (cid:6) 0:6710 Echocardiogram87:23 (cid:6) 2:9488:54 (cid:6) 2:7893:58 (cid:6) 2:3583:21 (cid:6) 3:2788:54 (cid:6) 2:7811 Heart-C12 Heart-H13 Heart-S14 Hepatitis54:13 (cid:6) 2:8653:80 (cid:6) 2:8658:97 (cid:6) 2:8995:71 (cid:6) 1:1658:07 (cid:6) 2:8383:33 (cid:6) 2:0081:29 (cid:6) 2:2785:88 (cid:6) 2:1186:73 (cid:6) 1:9883:67 (cid:6) 2:1138:29 (cid:6) 4:3836:59 (cid:6) 4:3447:37 (cid:6) 11:4515:44 (cid:6) 3:2642:28 (cid:6) 4:4585:03 (cid:6) 2:0985:16 (cid:6) 2:0890:50 (cid:6) 2:8476:45 (cid:6) 9:7385:55 (cid:6) 2:0815 Horse Colic75:79 (cid:6) 1:6275:79 (cid:6) 1:6392:56 (cid:6) 0:596:51 (cid:6) 2:0577:73 (cid:6) 1:6116 KDD9917L.Cancer84:68 (cid:6) 0:5284:80 (cid:6) 0:5089:22 (cid:6) 0:6745:42 (cid:6) 0:7084:85 (cid:6) 0:5243:75 (cid:6) 17:8843:75 (cid:6) 17:8846:67 (cid:6) 17:8593:75 (cid:6) 8:6643:75 (cid:6) 17:8818 Mushrooms98:53 (cid:6) 0:1298:40 (cid:6) 0:1299:04 (cid:6) 0:1598:88 (cid:6) 1:5398:70 (cid:6) 0:1119 Sick20 Vote91:60 (cid:6) 0:5190:87 (cid:6) 0:5397:53 (cid:6) 0:3486:30 (cid:6) 2:2992:46 (cid:6) 0:4990:02 (cid:6) 1:0590:21 (cid:6) 1:0492:05 (cid:6) 1:7594:94 (cid:6) 6:4790:21 (cid:6) 1:04(cid:18)al DO(cid:18)a (cid:0) O(cid:18)d O(cid:13)d1 (cid:0) O(cid:13)d;where O(cid:18)a is the (estimated) accuracy of NBCm, NBC(cid:3) or RBCu, while O(cid:18)d and O(cid:13)d arethe estimated accuracy and coverage of RBCd . Table 2 reports these accuracy values forNBCm, NBC(cid:3), and RBCu in the data sets used in this experiment. The sixth column reportsthe maximum cost ratio Ci =Cw to make RBCd the best classification system in terms ofminimum expected costs and, for reference, the last two columns note the proportion ofcases left unclassified by RBCd and size of the database. If the cost ratio is higher thanthe reported value, then the best system is the one with the highest accuracy (cid:18)al, and it isreported in bold face in the table.In the data sets B.Cancer and Credit, RBCd is the best choice if Cw > 4:44Ci andCw > 1:45Ci, respectively. If these conditions are not satisfied, then NBCm or, equivalently,NBC(cid:3), are the best systems. In the B.Cancer data set, the complete-admissible scoreM. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226223Table 2Residual accuracy of NBCm, NBC(cid:3) and RBCu. The sixth column reports the maximum value on the cost ratioCi=Cw that makes RBCd the classification system with minimum expected cost. If the cost ratio Ci =Cw issuperior to this value, then the system corresponding to the bold-faced accuracy is the best choice. The last twocolumns report the percentage of cases left unclassified by RBCd and the database sizeDatabase1 Adult2 Annealing3 Arythmia4 Audiology5 Automobile6 B.Cancer7 Bridge8 Credit9 Cylinder10 Echocardiogram11 Heart-C12 Heart-H13 Heart-S14 Hepatitis15 Horse Colic16L.Cancer17 KDD9918 Mushrooms19 Sick20 Vote(cid:18)ml0.60400.75930.56660.55640.40660.77490.42860.30960.64480.55760.00000.66670.36630.67820.74620.00000.80900.41900.48920.5569(cid:18)(cid:3)l0.57570.64810.51000.51280.32210.77490.34280.30960.65490.63560.00000.51290.34620.67270.74620.00000.81120.53500.54240.5193(cid:18)ul0.64570.95960.59640.51280.45800.23200.48560.27050.66310.63560.37990.69230.41350.69480.76700.00000.81210.68680.60520.5569Ci =Cw.1 (cid:0) (cid:13)d /1000.35430.04040.40360.44360.54200.22510.51440.69040.33690.36440.62010.30770.58650.30520.23301.00050.18780.31320.39480.443118.2850.8860.1865.2228.780.3533.334.6068.7016.794.2913.2784.5623.5593.496.2554.581.2215.705.06Size4884279845220020569910559851213130329412315536832470481242800435performs very poorly on the cases left unclassified by the strong dominance score, whilethe enforcement of the MAR assumption allows the standard NBC to exploit the informationprovided by the available data and reaches an accuracy of 77.49%. This data set has,however, only 6 cases with missing entries. In the Credit data set, NBCm, NBC(cid:3) and RBCuachieve an accuracy lower than 50% so that, if the the mis-classification cost is lowerthan 1:45Ci, a random assignment of the cases left unclassified by RBCd is preferable. InAudiology, RBCd is the minimum expected cost system if Cw > 2:25Ci. When the conditionon the cost ratio is not satisfied, that NBCm is the classification system to adopt. In the dataset L.Cancer, the accuracy (cid:18)al is null for all systems, and hence the choice of RBCd is neverunder discussion. This is also confirmed by the fact that the maximum value on the costratio Ci =Cw, which makes RBCd the system with minimum expected cost, is 1.005. Hence,RBCd is the best whenever Cw > 0:995Ci. As this data set is of medical nature, one can224M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226hardly imagine a situation in which not making an automatic analysis is less costly thanmaking the wrong one. In the remaining data sets, RBCu is always the second best choice,if the cost ratio Ci=Cw is superior to the value reported in the last column of the table. Inthe data set Annealing, for example, if the cost for not classifying a case is smaller than 25times—given by 1=0:0404—the cost for a wrong classification, RBCu is the best choice andachieves an accuracy 0.9596 on the cases left unclassified by RBCd . This is a gain of about20% compared to NBCm. Again this result confirms that the MAR assumption on this dataset has a negative effect on the accuracy. A similar result is shown in the Mushroom dataset, in which either assigning the missing entries to a dummy value or enforcing the MARassumption yields essentially a random classification of the cases left unclassified by RBCd ,while the use of the complete-admissible score rises the residual accuracy to 68.68%. TheSick data set reports a similar result, while the accuracy of RBCu is only slightly superiorto the NBC(cid:3) in the data sets Automobile, Cylinder, Hepatitis, and Horse Colic, and is nonein the Vote data set.These results suggest that the RBC based on the strong dominance criterion delivers thehighest accuracy, at risk of a decreased coverage. The use of the complete-admissible scoreimproves coverage by decreasing accuracy, and it appears to achieve better results thanstandard solutions, except when the proportion of missing data is small. However, theredoes not seem to be a consistently superior classifier and the solution to adopt needs totake into account features of the data at hand. Nonetheless, our decision theoretic approachprovides a principled way to choose the most appropriate solution.6. ConclusionsThis paper introduced the RBC: a generalization of the standard NBC which is robustwith respect to the missing data mechanism. The RBC performs the training step froman incomplete data set resulting in a classification system quantified by tight consistentprobability intervals. Then, the RBC classifies new cases by reasoning with probabilityintervals. We provided an interval propagation algorithm to identify bounds on the setof the classes posterior probabilities that can be computed from all possible completionsof the data, and two scoring methods for interval-based classification. The choice of thescoring methods that best suits the problem at hand is based on a decision-theoretic rulethat takes into account costs of mis-classification and cost incurred for not being able toclassify a case, and can be extended to make a cost-analysis of the implications of the MARassumption on the classification accuracy. The experimental evaluations showed the gainof accuracy that can be achieved by the RBC compared to standard solutions. However, theresults also showed that there is no uniformly better classification strategy when the dataare incomplete, and we expect that the principled way to choose the solution that best suitsthe problem at hand will become common practice in real applications.Although the robust solution that we presented in this paper is limited to the NBC, itis straightforward to extend it to tree-structured classification systems in which attributesare binary and the classification problem is to choose between two classes. This can bedone by training the classifier with the RBE and by computing bounds on the posteriorprobability of the classes using the 2U algorithm of [5]. The classification can be done byM. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226225choosing one of the interval-based classification rules that we presented here, in the sameprincipled way. The extension to more general classification models is the real challengeand essentially requires the development of interval propagation algorithms that returns nottoo loose bounds on the class posterior probability. The methods described in this paperhave been implemented in the computer program 1 distributed, to date, in over 2000 copies.AcknowledgementsAuthors are very grateful to Paul Snow for his invaluable contribution to the develop-ment of the complete-admissible score, and to the anonymous referees and the editor fortheir helpful suggestions.References[1] C. Blake, E. Keogh, C.J. Merz, UCI Repository of machine learning databases, University of California,Irvine, Department of Information and Computer Sciences, 1998.[2] L. Campos, J. Huete, S. Moral, Probability intervals: A tool for uncertain reasoning, Internat. J. Uncertainty,Fuzziness and Knowledge-Based Systems 2 (1994) 167–196.[3] A.P. Dempster, D. Laird, D. Rubin, Maximum likelihood from incomplete data via the EM algorithm (withdiscussion), J. Royal Statist. Soc. Ser. B 39 (1977) 1–38.[4] R.O. Duda, P.E. Hart, Pattern Classification and Scene Analysis, Wiley, New York, 1973.[5] E. Fagiuoli, M. Zaffalon, 2U: An exact interval propagation algorithm for polytrees with binary variables,Artificial Intelligence 106 (1998) 77–108.[6] N. Friedman, D. Geiger, M. Goldszmidt, Bayesian network classifiers, Machine Learning 29 (1997) 131–163.[7] S. Geman, D. Geman, Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images,IEEE Transactions on Pattern Analysis and Machine Intelligence 6 (1984) 721–741.[8] R. Kohavi, A study of cross-validation and bootstrap for accuracy estimation and model selection, in: Proc.IJCAI-95, Montreal, Quebec, Morgan Kaufmann, San Francisco, CA, 1995, pp. 1146–1151.[9] R. Kohavi, B. Becker, D. Sommerfield, Improving simple Bayes, in: M. van Someren, G. Widmer (Eds.),Poster Papers of the ECML-97, Charles University, Prague, 1997, pp. 78–87.[10] H.E. Kyburg, Rational belief, Behavioral and Brain Sciences 6 (1983) 231–273.[11] P. Langley, W. Iba, K. Thompson, An analysis of Bayesian classifiers, in: Proc. AAAI-92, San Jose, CA,AAAI Press, Menlo Park, CA, 1992, pp. 223–228.[12] I. Levi, On indeterminate probabilities, J. Philos. 71 (1974) 391–418.[13] R.J.A. Little, D.B. Rubin, Statistical Analysis with Missing Data, Wiley, New York, 1987.[14] M. Pittarelli, An algebra for probabilistic databases,IEEE Transactions on Knowledge and DataEngineering 6 (2) (1994) 293–303.[15] J.R. Quinlan, C4.5: Programs for Machine Learning, Morgan Kaufmann, San Francisco, CA, 1993.[16] M. Ramoni, Ignorant influence diagrams, in: Proc. IJCAI-95, Montreal, Quebec, Morgan Kaufmann, SanFrancisco, CA, 1995, pp. 1808–1814.[17] M. Ramoni, P. Sebastiani, Bayesian methods, in: M. Berthold, D.J. Hand (Eds.), Intelligent Data Analysis.An Introduction, Springer, New York, 1999, pp. 129–166.[18] M. Ramoni, P. Sebastiani, Robust learning with missing data, Machine Learning (2000), to appear.[19] R.T. Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ, 1970.1 Available from Bayesware Limited (www.bayesware.com).226M. Ramoni, P. Sebastiani / Artificial Intelligence 125 (2001) 209–226[20] S. Russell, J. Binder, D. Koller, K. Kanazawa, Local learning in probabilistic networks with hiddenvariables, in: Proc. IJCAI-95, Montreal, Quebec, Morgan Kaufmann, San Francisco, CA, 1995, pp. 1146–1151.[21] D.J. Spiegelhalter, R.G. Cowell, Learning in probabilistic expert systems, in: J. Bernardo, J. Berger, A.P.Dawid, A.F.M. Smith (Eds.), Bayesian Statistics 4, Oxford University Press, Oxford, UK, 1992, pp. 447–466.[22] D.J. Spiegelhalter, S.L. Lauritzen, Sequential updating of conditional probabilities on directed graphicalstructures, Networks 20 (1990) 157–224.