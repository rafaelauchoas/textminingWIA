Artificial Intelligence 172 (2008) 945–954www.elsevier.com/locate/artintReachability analysis of uncertain systems using bounded-parameterMarkov decision processesDi Wu, Xenofon Koutsoukos ∗EECS Department, Vanderbilt University, Nashville, TN 37235, USAReceived 8 September 2006; received in revised form 8 December 2007; accepted 17 December 2007Available online 23 December 2007AbstractVerification of reachability properties for probabilistic systems is usually based on variants of Markov processes. Currentmethods assume an exact model of the dynamic behavior and are not suitable for realistic systems that operate in the presenceof uncertainty and variability. This research note extends existing methods for Bounded-parameter Markov Decision Processes(BMDPs) to solve the reachability problem. BMDPs are a generalization of MDPs that allows modeling uncertainty. Our resultsshow that interval value iteration converges in the case of an undiscounted reward criterion that is required to formulate the prob-lems of maximizing the probability of reaching a set of desirable states or minimizing the probability of reaching an unsafe set.Analysis of the computational complexity is also presented.© 2007 Elsevier B.V. All rights reserved.Keywords: Reachability analysis; Uncertain systems; Markov decision processes1. IntroductionVerification of reachability properties for probabilistic systems is usually based on variants of Markov processes.Probabilistic verification aims at establishing bounds on the probabilities of certain events. Typical problems includethe maximum and the minimum probability reachability problems, where the objective is to compute the controlpolicy that maximizes the probability of reaching a set of desirable states, or minimize the probability of reaching anunsafe set. Such problems are important in many application domains such as planning for autonomous systems [1],system biology [2], and finance [3].Algorithms for verification of MDPs have been presented in [4,5]. Several other probabilistic models based onvariants of MDPs also have been considered [6,7]. These methods assume exact values of the transition probabilitieswhich typically are computed either based on detailed models using discrete approximation techniques [8] or have tobe estimated from data [9]. However, realistic systems often operate in the presence of uncertainty and variability, andmodeling and estimation errors can affect the transition probabilities and impact the solution. Such systems can bebest described using uncertain transition probabilities. An uncertain MDP which describes the routing of an aircraftbased on past weather data is presented in [10]. Computing uncertain transition probabilities for a robot path-finding* Corresponding author.E-mail addresses: wudipku@gmail.com (D. Wu), xenofon.koutsoukos@vanderbilt.edu (X. Koutsoukos).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.12.002946D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954example based on a model of the continuous dynamics is described in [11]. Existing reachability analysis methods areinsufficient for dealing with such uncertainty.This research note extends existing methods for Bounded-parameter Markov Decision Processes (BMDPs) to solvethe reachability problem. Proposed by Givan, Leach and Dean [12], BMDPs are a generalization of MDPs that allowsmodeling uncertainty. A BMDP can be viewed as a set of exact MDPs (sharing the same state and action space)specified by intervals of transition probabilities and rewards and policies are compared on the basis of interval valuefunctions. An overview of BMDPs is presented in Section 2.The paper focuses on the problem of maximizing the probability of reaching a set of desirable states. The resultspresented in [12] are for dynamic programming methods assuming a discounted reward criterion. A discount factorensures the convergence of the iterative methods for the interval value functions. Probabilistic verification can beformulated based on the Expected Total Reward Criterion (ETRC) [13]. Under ETRC, the discount factor is set to 1,and the convergence of the iterative algorithms for BMDPs is more involved because the contraction property ofthe iteration operators does not hold globally and the interval value function may not be well defined unless properrestrictions on the intervals of transition probabilities and rewards are applied. The interval expected total reward forBMDPs is analyzed in Section 3. Further, proving the polynomial computational complexity of the algorithm requiresa different method using an appropriate weighted norm. Based on the ETRC, this paper presents a detailed analysis ofthe convergence and the computational complexity for the maximum probability reachability problem in Sections 4and 5 respectively. Minimum probability reachability and other problems based on the ETRC [13] can be addressedin a similar fashion. A simplified robot path-finding example and numerical results that illustrate the approach can befound in [11].Optimal solutions to several variants of uncertain MDP problems have been studied previously. MDPs with uncer-tain transition probabilities and a discounted reward criterion have been considered in [14,15]. Related methods thatconsider a discounted reward include the work in [16] which computes the optimal policy in models with compactconvex uncertain sets, the approach in [17] which computes the Pareto optimal policy which maximizes the averageexpected reward over all stationary policies under a specific partial order, and the work in [10] which solves a robustcontrol problem. The average reward problem for BMDPs has been studied in [18] and a similar average performancecriterion has been considered in [19]. An algorithm based on real-time dynamic programming for uncertain stochasticshortest path problems is presented in [20]. The algorithm requires that a goal state is reachable from any visited stateand proposes a reachability analysis pre-processing step which is based on graph analysis. Probabilistic reachabilityanalysis of uncertain MDPs is a significant problem which requires an undiscounted reward criterion and cannot betreated with these algorithms.Probabilistic verification of uncertain systems has been addressed also using model checking methods. A variantof uncertain MDPs has been presented in [21,22]. The main characteristic of the model is that uncertainty is resolvedthrough nondeterminism, i.e. at every step an adversary picks a probability distribution that satisfies the uncertaintransition probabilities. This differs from BMDPs where the transition probabilities are uncertain for a given actionselected by an external agent. The approach presented in [21] computes the probability distribution over the statesfor finite number of steps while the algorithms in [22] reduce the uncertain system to an MDP of a larger size forverifying a subset of probabilistic computation tree logic specifications without steady state operators.2. Bounded-parameter Markov decision processesWe first review some basic notions of BMDPs from [12] and establish the notation. A BMDP is defined as M =(cid:3)Q, A, ˆF , ˆR(cid:4) where Q is a set of states, A is a set of actions, ˆR is an interval reward function that maps each q ∈ Qto a closed interval of real values [R(q), R(q)], and ˆF is an interval state-transition distribution so that for p, q ∈ Qand α ∈ A,F p,q (α) (cid:2) Pr(Xt+1 = q|Xt = p, Ut = α) (cid:2) F p,q (α).For any action α and state p, the sum of the lower bounds of ˆFp,q (α) over all states q is required to be less than orequal to 1, while the sum of the upper bounds is required to be greater than or equal to 1.A BMDP M defines a set of exact MDPs. Let M = (cid:3)QM , AM , F M , RM (cid:4) be an MDP. If QM = Q, AM = A,RM (p) ∈ ˆR(p), and F Mp,q (α) ∈ ˆFp,q (α) for any α ∈ A and p, q ∈ Q, then we say M ∈ M. To simplify the presenta-tion, the rewards are assumed to be tight, however, the results can be easily generalized to the case of interval rewards.D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954947Policies are defined as π : Q → A and are restricted into the set of stationary Markov policies Π . Let V denote theset of value functions on Q. For an exact MDP M, policy π , and discount factor γ ∈ (0, 1), the value function is thesolution of the equationVM,π (p) = R(p) + γ(cid:4)(cid:3)VM,π (q)π(p)F Mp,q(cid:2)q∈Qand can be computed by iteratively applying the policy evaluation operator denoted as VIM,π : V → V. For any policyπ and state p, the interval value function of the BMDP M for π at p is the closed interval(cid:5)ˆVπ (p) =infM∈MVM,π (p), supM∈M(cid:6).VM,π (p)(1)An MDP M ∈ M is π -maximizing if for any M (cid:7) ∈ M, VM,π (cid:3)dom VM (cid:7),π1 and likewise, M is π -minimizing iffor any M (cid:7) ∈ M, VM,π (cid:2)dom VM (cid:7),π . It is proved in [12] (Theorem 7 and Corollary 1) that for any policy π ∈ Π andany ordering of the states Q, there exist a π -maximizing MDP M(π) and a π -minimizing MDP M(π). This impliesthat for input V (or V ) there exists a single MDP independent of V (or V ) which simultaneously maximizes (orminimizes) VM,π (p) for all states p ∈ Q. Therefore, we can define the interval policy evaluation operator (cid:7)IVIπ as(cid:7)IVIπ ( ˆV )(p) =where(cid:6)(cid:5)IVIπ (V )(p), IVIπ (V )(p)IVIπ (V ) = minM∈MVIM,π (V ) = VIM(π),π (V )and IVIπ (V ) = maxM∈MVIM,π (V ) = VIM(π),π (V ).In order to define the optimal value function for a BMDP, two different orderings on closed real intervals areintroduced: [l1, u1] (cid:2)opt [l2, u2] ⇐⇒ (u1 < u2 ∨ (u1 = u2 ∧ l1 (cid:2) l2)) and [l1, u1] (cid:2)pes [l2, u2] ⇐⇒ (l1 < l2 ∨ (l1 =l2 ∧ u1 (cid:2) u2)). In addition, ˆU (cid:2)optˆV (q)) for each q ∈ Q.Then the optimistic optimal value function ˆVopt and the pessimistic optimal value function ˆVpes are defined as theupper bounds over all stationary policies using (cid:2)opt and (cid:2)pes respectively to order interval value functions, i.e.ˆV ) if and only if ˆU (q) (cid:2)optˆV (q) ( ˆU (q) (cid:2)pesˆV ( ˆU (cid:2)pesˆVopt = maxπ∈Π,(cid:2)optˆVπ and ˆVpes = minπ∈Π,(cid:2)pesˆVπ ,respectively.The value iteration for ˆVopt is used when the objective is to maximize the upper bound V while ˆVpes is used tomaximize the lower bound V . In the subsequent sections, we focus on the optimistic case for the optimal intervalvalue functions. Results for the pessimistic case can be inferred analogously.The interval value iteration operator (cid:7)IVIopt for each state p is defined as(cid:7)IVIopt( ˆV )(p) = maxα∈A,(cid:2)optminM∈MVIM,α(V )(p), maxM∈M(cid:5)(cid:6)VIM,α(V )(p).(2)Due to the nature of (cid:2)opt, (cid:7)IVIopt evaluates actions primarily based on the interval upper bounds, breaking ties on thelower bounds. For each state, the action that maximizes the lower bound is chosen from the subset of actions thatequally maximize the upper bound. To capture this behavior, we define the action selection functionρW (p) = arg maxα∈AmaxM∈MVIM,α(W )(p)andIVIopt(V )(q) = maxα∈AmaxM∈MVIM,α(V )(q),IVI opt,V (V )(q) = maxα∈ρV (q)minM∈MVIM,α(V )(q).Then (2) can be rewritten as(cid:7)IVIopt( ˆV ) =(cid:6)(cid:5)IVI opt,V (V ), IVIopt(V ).1 V (cid:3)dom U if and only if for all q ∈ Q, V (q) (cid:3) U (q); (cid:2)dom is defined similarly.(3)(4)948D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–9543. Interval expected total reward for BMDPsIn this paper, we are primarily interested in the problem of maximizing the probability that the system will reacha desirable set of states. By solving this problem, we can establish bounds on the probabilities of reaching desirableconfigurations used in probabilistic verification of discrete systems. This problem can be formulated using the Ex-pected Total Reward Criterion (ETRC) for BMDPs. The ERTC can be viewed as the expected total discounted rewardwith a discount factor γ = 1. For γ = 1 the convergence results in [12] no longer hold, because the iteration operators(cid:7)IVIπ , (cid:7)IVIopt and (cid:7)IVIpes are not global contraction mappings. Furthermore, the interval value function may not be welldefined unless proper restrictions on the intervals of the transition probabilities and rewards are applied.To simplify the notation, we use vector notation where R and V are column vectors, whose ith element is the scalarreward and value function of the ith state respectively. FM is the transition probability function of MDP M, and FM,πis the transition probability matrix given a policy π . For an exact MDP M and a policy π , the value function for theETRC is the solution of the equationVM,π = R + FM,π Vand can be computed using the policy evaluation operator VIM,π [13]. The interval value function is defined by Eq. (1)similarly to the discounted case. Further, because the existence of a π -minimizing and a π -maximizing MDP doesnot depend on the discount factor [12], we can define a π -maximizing MDP M(π) and a π -minimizing MDP M(π)in M for the ETRC.For an MDP M and a policy π , we denote EM,πqthe expectation of functionals given the initial state q. Under theETRC, we compare policies on the basis of the interval expected total reward ˆV = [V π , V π ] where for any q ∈ QV π (q) = EM(π),πqR(Xt )and V π (q) = EM(π),πq(cid:9)(cid:8)∞(cid:2)t=1(cid:8)∞(cid:2)(cid:9)R(Xt ).t=1Let R+(q) = max{R(q), 0} and R−(q) = max{−R(q), 0}. We define the expected total rewards for R+ and R− byV±π (q) ≡ limN→∞EM(π),πq(cid:8)(cid:4)(cid:3)Xt (q)±RN −1(cid:2)t=1(cid:9),that is V + ignores negative rewards and V + ignores positive rewards. Since the summands are non-negative, bothof the above limits exist.2 The limit defining V π (q) exists whenever at least one of V +π (q) is finite, inπ (q), V −π (q). V +which case V π = V +π (q), and V π (q) can be similarly defined. Noting this, we impose thefollowing finiteness assumption which assures that ˆVπ is well defined.π (q) and V −π (q) − V −Assumption 1. For all π ∈ Π and q ∈ Q, (a) either V +π (q) or V −π (q) is finite, and (b) either V +π (q) or V −π (q) is finite.Let ˆVopt denote the optimal interval value function for the ETRC. The following theorem establishes the optimalityequation for the ETRC and shows that the optimal interval value function is a solution of the optimality equation.3Theorem 1. Suppose Assumption 1 holds. Then(a) The upper bound of the optimal interval value function V opt satisfies the equationV = supπ∈ΠmaxM∈MVIM,π (V ) = supπ∈Π{R + FM(π),π V } ≡ IVIopt(V ),(b) The lower bound of the optimal interval value function V opt,W satisfies the equationV = supπ∈ρWminM∈MVIM,π (V ) = supπ∈ρW{R + FM(π),π V } ≡ IVI opt,W (V )for any value function W and the associated action selection function (3).2 This includes the case when the limit is ±∞.3 All proofs are presented in detail in Appendix A for readability.D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954949Based on Theorem 1, the value iteration operator (cid:7)IVIopt can be defined as in Eq. (2) and the following lemmaestablishes the monotonicity of the operator.Lemma 2. Suppose U and V are value functions in V with U (cid:2)dom V , then(a) IVIopt(U ) (cid:2)dom IVIopt(V ),(b) IVI opt,W (U ) (cid:2)dom IVI opt,W (V )for any value function W and the associated action selection function (3).Clearly, Assumption 1 is necessary for any computational approach. In the general case of the expected total rewardcriterion (ETRC), we cannot validate that the assumption holds. However, in the maximum probability reachabilityproblem, the (interval) value function is interpreted as (interval) probability and therefore Assumption 1 can be easilyvalidated as shown in Section 4.4. Maximum probability reachability problemThe maximum probability reachability problem is based on a special case of a class of BMDP models known asthe non-negative models (named similarly to MDP models [13]). A BMDP model is called non-negative if it satisfiesAssumption 1 and its rewards are all non-negative.In order to prove convergence of the value iteration, we consider the following assumptions in addition to Assump-tion 1:Assumption 2. For all q ∈ Q, R(q) (cid:3) 0.Assumption 3. For all q ∈ Q and π ∈ Π , V +π (q) < ∞.If a BMDP is consistent with both Assumptions 2 and 3, it is called a non-negative BMDP model, and its valuefunction under the ETRC is called non-negative interval expected total reward. Note that Assumptions 2 and 3 implyAssumption 1, so Theorem 1 and Lemma 2 hold for non-negative BMDP models. In the following, Lemma 3 showsthat ˆVopt is the solution of the optimality equation and Theorem 4 establishes the convergence result of interval valueiteration for non-negative BMDPs.Lemma 3. Suppose Assumptions 2 and 3 hold. Then(a) V opt is the minimal solution of V = IVIopt(V ) in V +, where V + = {V ∈ V : 0 (cid:2) V (p) < ∞ for each p ∈ Q},(b) V opt,W is the minimal solution of V = IVI opt,W (V ) in V + for any value function W and the associated actionselection function (3).Theorem 4. Suppose Assumptions 2 and 3 hold. Then for ˆV 0 = [0, 0], the sequence { ˆV n} defined by ˆV n = (cid:7)IVInconverges point-wise and monotonically to ˆVopt.opt( ˆV 0)An instance of the maximum probability reachability problem for BMDPs consists of a BMDP M = (cid:3)Q, A, ˆF , R(cid:4)together with a destination set T ⊆ Q. The objective of maximum probability reachability problem is to determine,for all p ∈ Q, the maximum interval probability of starting from p and finally reaching any state in T , i.e.ˆU maxM,opt(p) = supπ∈Π,(cid:2)opt(cid:5)(cid:6)U M,π (p), U M,π (p)whereU M,π (p) = minM∈MPrM,π(cid:3)∃t.Xt (p) ∈ T(cid:4)and U M,π (p) = maxM∈MPrM,π(cid:3)∃t.Xt (p) ∈ T(cid:4).U M,π and U M,π are probabilities and therefore by definition take values in [0, 1], and thus, the interval valuefunction satisfies Assumption 1. Note that U M,π (p) can be computed recursively by(cid:8)(cid:10)U M,π (p) =minM∈M1q∈Q F Mp,q (π(p))U M,π (q)if p ∈ Q − Tif p ∈ T .(5)950D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954In order to transform the maximum probability reachability problem to a problem solvable by interval value iter-ation, we add a terminal state r with transition probability 1 to itself on any action, let all the destination states in Tbe absorbed into the terminal state, i.e., transition to r with probability 1 on any action, and set the reward of eachdestination state to be 1 and of every other state to be 0. Thus, we form a new BMDP model (cid:11)M = (cid:3) ˜Q, ˜A, ˜F , ˜R(cid:4),where ˜Q = Q ∪ {r},˜R(p) =(cid:8)10˜A = A and for any p, q ∈ ˜Q, and α ∈ AˆFp,q (α)[0, 0][1, 1]if p ∈ Tif p /∈ T ,˜Fp,q (α) =⎧⎪⎨and⎪⎩if p /∈ T ∪ {r}if p ∈ T ∪ {r} and q (cid:17)= rif p ∈ T ∪ {r} and q = r.(6)Since ˜R(r) = 0, by the structure of ˜Fp,q , it is clear that V (cid:11)M,π (r) will not be affected by the value function of any(cid:17)other states. For any p ∈ Q, we haveV (cid:11)M,π (p) = minM∈ (cid:11)M(cid:16)˜R(p) +(cid:2)Specifically, for p ∈ TV (cid:11)M,π (p) = minM∈ (cid:11)M(cid:16)˜R(p) +(cid:4)(cid:3)π(p)F Mp,qq∈ ˜Q(cid:2)q∈ ˜Q(cid:4)(cid:3)π(p)F Mp,qV M,π (q).(cid:17)V M,π (q)= ˜R(p) + V (cid:11)M,π (r) = 1.(7)(8)From (6), (7) and (8), it follows that U M,π is equivalent to V (cid:11)M,π . Similarly, U M,π is equivalent to V (cid:11)M,π . ThereforeˆV (cid:11)M,opt= supπ∈Π,(cid:2)opt(cid:5)V (cid:11)M,π , V (cid:11)M,π(cid:6)(cid:5)U M,π , U M,π(cid:6)= supπ∈Π,(cid:2)opt= ˆUM,opt.(9)The BMDP (cid:11)M constructed above satisfies Assumptions 2 and 3, so the interval value function for each stateexists, and further, (cid:11)M the maximum probability reachability problem can be solved by interval value iteration and theconvergence is assured by Theorem 4.Note that we do not assume the existence of a proper policy [23]. Convergence is guaranteed without this assump-tion. The reason for that is twofold: (i) rewards are all 0 except for the destination state, and (ii) the destination stategoes with probability 1 to the terminal state that is absorbing and reward-free. Therefore, even if there is a cycle, itdoes not add to the total reward.5. Computational complexityIn this section, we show the polynomial time complexity of the interval value iteration for the ETRC. Our discussionis based on BMDP models in which there is a reward-free and absorbing state, i.e. the terminal state. Without lossof generality, we assume that q1 is the terminal state. The interval value function of the destination state is set to be[1, 1]. The initial interval value of each of the other states is set to be [0, 0].States from which the terminal state is not accessible do not affect the result of the interval value iteration algorithm,because their interval value functions will remain [0, 0] as the algorithm proceeds. This is because if the terminal stateis not accessible from a state then the destination states are also not accessible from it.In order to prove the polynomial complexity of the interval value iteration, we consider the following assumption:Assumption 4. The terminal state is accessible from all the other states.Given a BMDP, it is possible that there exists a set of states such that once the set is entered, there exists a policythat will keep the state in the set for ever. In MDPs, such sets of states are called end components [5] or controllablyrecurrent states [4] and can be defined similarly for BMDPs. In the verification problem considered in this paper,such an end component will incur zero-reward and it can be replaced by a state so that the transformed model satisfiesAssumption 4 [5]. End components can be computed in polynomial time [5], and therefore, if we will show that theinterval value iteration algorithms are polynomial under Assumption 4 then it is polynomial for every BMDP.D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954951Lemma 5. Let X = {x ∈ Rn | x (cid:3)dom 0, x1 = 0} and suppose Assumption 4 holds. Then (a) IVIopt is a contractionmapping with respect to some weighted norm (cid:18) · (cid:18)w∞ over X, (b) for any value function W and the associated actionselection function (3), IVIW,opt is a contraction mapping with respect to some weighted norm (cid:18) · (cid:18)w∞ over X.The following theorem shows that interval value iteration algorithm is polynomial and is based on a similar argu-ment of [12].Theorem 6. Interval value iteration converges to the desired interval value function in a number of steps polynomialin the number of states, the number of actions, and the number of bits used to represent the BMDP parameters.6. ConclusionsThe results described in this paper show that interval value iteration with proper restrictions on the reward andtransition functions can be used to solve BMDPs under the expected total reward criterion. These results allow us tosolve a variety of new problems for BMDPs. The paper focuses on the maximum probability reachability problem foruncertain systems. Additional problems and extension to other probabilistic models used for verification are subjectof current and future work.AcknowledgementsThis work was partially supported in part by the National Science Foundation under NSF Career awardCNS-0347440. We would also like to thank the anonymous reviewer for identifying errors in the presentation ofthe technical results.Appendix A. ProofsProof of Theorem 1. Denote by e the vector [1, 1, . . . , 1]. For any ε > 0 there exists a π1 ∈ Π which satisfies(cid:3)dom V opt − εe. By the definition of V opt, we have V opt (cid:3)dom VIM(π),π (V π1 ) = R + FM(π),π V π1 for any π ∈ Π .V π1It follows that{R + FM(π),π V π1V opt (cid:3)dom supπ∈ΠVIM(π),π (V opt) − εe = IVIopt(V opt) − εe.} (cid:3)dom supπ∈Π= supπ∈Π{R + FM(π),π V opt} − εeHence V opt (cid:3)dom IVIopt(V opt). For any fixed q ∈ Q and ε > 0 there exists a π ∈ Π which satisfies V π (q) (cid:3)V opt(q) − ε. It follows thatV opt(q) − ε (cid:2) VIM(π),π (V π )(q) (cid:2) VIM(π),π (V opt)(q) (cid:2) supπ∈ΠVIM(π),π (V opt)(q) = IVIopt(V opt)(q).Hence V opt (cid:2)dom IVIopt(V opt). Since both V opt (cid:2)dom IVIopt(V opt) and V opt (cid:3)dom IVIopt(V opt) hold, (a) follows. Theproof of (b) is similar. (cid:2)Proof of Lemma 2. For any ε > 0, there exist M1 ∈ M and π1 ∈ Π such thatIVIopt(U ) = supπ∈ΠVIM(π),π (U ) (cid:2)dom R + FM1,π1 U + εe(cid:2)dom R + FM1,π1 V + εe (cid:2)dom supπ∈ΠVIM(π),π (V ) + εeIVIopt(V ) + εe.Since ε was chosen arbitrarily, (a) holds. The proof of (b) is similar. (cid:2)952D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954Proof of Lemma 3. We denote F nThe upper bound of the interval value function for any π ∈ Π can be defined as V π = limN→∞by Assumption 2 the limit is well defined. Further, by Assumption 3 V π < ∞ and we haveM,π the n-step transition probability matrix and V n the sequence V n = VInn=1 F n−1M(π),π(cid:10)NM,π V 0.R sinceV opt = supπ∈ΠV π (cid:3)dom 0.(A.1)Suppose that there exists a V ∈ V +, for which V = IVIopt(V ). By definition of IVIopt, V = IVIopt(V ) (cid:3)domVIM(π),π (V ) = R + FM(π),π V for all π ∈ Π . HenceV (cid:3)dom R + FM(π),π V (cid:3)dom R + FM(π),π R + F 2M(π),πVN(cid:2)(cid:3)domn=1Since V ∈ V +, F NM(π),πF n−1M(π),πR + F NM(π),πV = V N +1π+ F NM(π),πV .V (cid:3)dom 0, so that V (cid:3)dom V N +1πfor all N . Thus V (cid:3)dom Vπ for all π ∈ Π . Hence∀V ∈ V +.V = IVIopt(V ) (cid:20)⇒ V (cid:3)dom supπ∈ΠVπ = V opt.(A.2)Theorem 1 shows that V opt satisfies the optimality equation. Together with (A.1) and (A.2), V opt is the minimalsolution of V = IVIopt(V ). The proof of (b) is similar. (cid:2)Proof of Theorem 4. We first proof that the sequence {V n} defined by V n = IVInopt(V 0) converges point-wise andmonotonically to V opt. By Assumption 2, IVIopt(0) (cid:3)dom 0, so according to Lemma 2(a), {V n} increases monoton-ically. Also, for each q ∈ Q, V n(q) is finite. Hence V n(q) is a monotonically increasing and bounded series,thus limn→∞ V n(q) = supn{V n(q)} = V (q) exists. Since V (cid:3)dom V n, Lemma 2(a) implies that IVIopt(V ) (cid:3)domIVIopt(V n) = V n+1 for all n. Therefore IVIopt(V ) (cid:3)dom V . For any π ∈ Π , and all n,(cid:10)IVIπ (V n) (cid:2)dom IVIopt(V n) = V n+1 (cid:2)dom V (cid:2)dom IVIopt(V ).(A.3)According to the monotone convergence theorem, limn→∞ IVIπ (V n) = IVIπ (V ) for each π ∈ Π . Together with(A.3) we have IVIopt(V ) = supπ∈Π IVIπ (V ) (cid:2)dom V (cid:2)dom IVIopt(V ), which implies that V is a fixed point ofIVIopt. Choosing ε > 0 and a sequence {εn} which satisfiesn=1 εn = ε, there exists a policy π , which satis-opt(0) for all N . Hence Vπ + εe (cid:3)dom V , implying V (cid:2)dom V opt. ByfiesLemma 3(a), V opt is the minimal solution of the optimality equation, so V = V opt. Similarly, we can prove that thesequence {V n} defined by V n = IVInopt,W (V 0) converges point-wise and monotonically to V opt,W for any value func-opt( ˆV 0) musttion W and the associated action selection function (3). By the definition of (cid:7)IVIopt in Section 2, ˆV n = (cid:7)IVInconverge point-wise and monotonically to ˆVopt. (cid:2)Nn=1 εne (cid:3)dom IVINNn=0 F n(cid:10)∞R +M(π),π(cid:10)Proof of Lemma 5. We follow the approach of [24]. Suppose (cid:7)M = (cid:3)Q, A, ˆF , R(cid:4) is a BMDP and q1 is the terminalstate for which ˆFq1,q1 (α) = [1, 1] for any α ∈ A and R(q1) = 0. For any fixed M ∈ M, the set of remaining statesmM such that for any 1 (cid:2) i (cid:2) mM , p ∈ QMQ\{q1} = {q2, . . . , qn} can be partitioned into nonempty subsets QMiand α ∈ A, there exists some state r ∈ {q1} ∪ QMp,r (α) > 0 (the choice of r depends on both1p and α). Construct a set of weights {wM1 , . . . , QMi−1 such that F M∪ · · · ∪ QM} as2 , . . . , wMnmM(cid:2)(cid:18)1 − (ηM )2i(cid:19)IQMi(qj )wMj=i=1where(cid:18)ηM = minp,q (α) | p, q ∈ Q, α ∈ A such that F MF Mp,q (α) > 0(cid:19).(A.4)Since 0 < ηM < 1, from (A.4) we have 0 < wMi (cid:2) mM ). For any α ∈ A, there exists some state ql ∈ {q1} ∪ QM1j < 1 for any 2 (cid:2) j (cid:2) n. Suppose qj (2 (cid:2) j (cid:2) n) is in QMi(1 (cid:2)(α) > 0. Let γ M =∪ · · · ∪ QMi−1 such that F Mqj ,qlD. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954953(1 − η2mM −1)/(1 − η2mM). We have(cid:20)(cid:21)(cid:20)(cid:22)n(cid:2)n(cid:2)k=1F Mqj ,qk(α)wMk(α) + F Mqj ,ql(α)wMl(cid:21)(cid:22)wMjF Mqj ,qkwMj(cid:2)=(cid:2)k=1,k(cid:17)=l(cid:3)1 + F Mqj ,ql(cid:18)1 − (ηM )2i−1(cid:4)(cid:22)(α)(wMl(cid:19)(cid:22)wMj(cid:2)wM− 1)j(cid:18)1 − (ηM )2i−1(cid:3)1 + ηM (wMl(cid:19)(cid:22)(cid:18)1 − (ηM )2i− 1)(cid:19)(cid:4)(cid:22)wMj(cid:2) γ M ,where the second inequality follows from the fact F MwMl(cid:2) 1 − (ηM )2i−2.Let ˆU ∈ (cid:23)V and ˆV ∈ (cid:23)V be interval value functions. For fixed p (cid:17)= q1, assume IVIopt( ˆU )(p) (cid:2) IVIopt( ˆV )(p). Select(α) (cid:3) ηM and the third inequality follows from the factqj ,qlM ∈ M and α ∈ A to maximize the expression VIM,α(V )(p). Then0 (cid:2) IVIopt( ˆV )(p) − IVIopt( ˆU )(p)= maxα∈A(cid:2) R(p) +VIM,α(V )(p) − maxmaxα∈AM∈M(cid:4)− R(p) −p,q (α)V (q)maxM∈M(cid:3)F M(cid:2)VIM,α(U )(p)(cid:2)(cid:3)F M(cid:4)p,q (α)U (q)(cid:2)=q(cid:17)=q1(cid:2) γ M wMq∈Q(cid:3)p,q (α)wMF Mq(cid:18)(cid:3)p maxq(cid:4)(cid:3)(cid:4)/wMU (q) − V (q)qq∈Q(cid:4)U (q) − V (q)(cid:19)./wMqIt follows that(cid:24)(cid:24)(cid:24)wM(cid:24)IVIopt( ˆU ) − IVIopt( ˆV )(cid:2) γ M (cid:18)U − V (cid:18)wM,denotes the maximum norm (cid:18) · (cid:18) scaled by wM = (1, wM2 , . . . ,n )(cid:18). Note that the maximizing MDP M is independent of V and γ M depends only on the transition matrix ofn ), i.e. (cid:18)x(cid:18)wM = (cid:18)(x1, x2/wMwhere (cid:18) · (cid:18)wMxn/wMM, therefore IVIopt is a contraction mapping.The proof of (b) is similar. The construction of the weights w(cid:7)M = (1, w(cid:7)Mn ) used here is a little differentfrom that of wM , in the sense that the set of available choices of actions for each state p ∈ Q is no longer in A but inρW (p). (cid:2)2 , . . . , w(cid:7)M2 , . . . , wMProof of Theorem 6. By Eq. (4), the iteration operator (cid:7)IVIopt works in the following way: the upper bound of theinterval value function will first converge; once the upper bound has converged, the iteration will continue until thelower bound converges. The first stage is polynomial because: (a) By Lemma 5, IVIopt is a contraction mapping onthe upper bound value function with respect to some weighted norm over a subset of Rn, and thus the successiveestimates of V opt produced converge geometrically to the unique fixed-point. (b) By Theorem 4, the unique fixed-point is the desired upper bound value function. (c) The upper bound of the true ˆVopt is the optimal value function inπ -maximizing MDP in (cid:7)M. (d) The parameters for the MDPs in (cid:7)M can be specified with a number of bits polynomialin the number of bits used to specify the BMDP parameters. (e) Since IVIopt is a contraction mapping, by an argumentsimilar to [24], the upper bound will converge in a number of steps that is polynomial in the number of states, thenumber of actions, and the number of bits used to represent the BMDP parameters. Similarly, the second stage is alsopolynomial. Hence the value iteration algorithm is polynomial. (cid:2)References[1] H.L.S. Younes, D.J. Musliner, Probabilistic plan verification through acceptance sampling, in: Proceedings of the AIPS-02 Workshop onPlanning via Model Checking, 2002, pp. 81–88.[2] J. Heath, M. Kwiatkowska, G. Norman, D. Parker, O. Tymchyshun, Probabilistic model checking of complex biological pathways, in: Proc.International Conference on Computational Methods in Systems Biology, 2006.954D. Wu, X. Koutsoukos / Artificial Intelligence 172 (2008) 945–954[3] H. Pham, Minimizing shortfall risk and applications to finance and insurance problems, The Annals of Applied Probability 312 (1) (2002)143–172.[4] C. Courcoubetis, M. Yannakakis, Markov decision processes and regular events, IEEE Transaction on Automatic Control 43 (10) (1998)1399–1418.[5] L. de Alfaro, Formal verification of probabilistic systems, PhD thesis, Tech. Rep. STAN-CS-TR-98-1601, Stanford University, Stanford, CA,1997.[6] J. Rutten, M. Kwiatkowska, G. Norman, D. Parker, Mathematical Techniques for Analyzing Concurrent and Probabilistic Systems, CRMMonograph Series, vol. 23, American Mathematical Society, 2004.[7] M. Jaeger, Probabilistic decision graphs—combining verification and AI techniques for probabilistic inference, International Journal of Un-certainty, Fuzziness and Knowledge-Based Systems 12 (2004) 19–42.[8] H.J. Kushner, P. Dupuis, Numerical Methods for Stochastic Control Problems in Continuous Time, second ed., Springer-Verlag, New York,2001.[9] E. Feinberg, A. Shwartz, Handbook of Markov Decision Processes: Methods and Applications, Kluwer Academic Publishers, Boston, MA,2002.[10] A. Nilim, L.E. Ghaoui, Robust control of Markov decision processes with uncertain transition matrices, Operations Research 53 (5).[11] D. Wu, X.D. Koutsoukos, Probabilistic verification of bounded-parameter Markov decision processes, in: V. Torra, Y. Narukawa, S. Miyamoto(Eds.), Modeling Decisions for Artificial Intelligence, MDAI 2006, Tarragona, Catalonia, Spain, April 3–5, 2006, in: Lecture Notes in ArtificialIntelligence, vol. 3885, Springer, 2006, pp. 283–294.[12] R. Givan, S. Leach, T. Dean, Bounded-parameter Markov decision process, Artificial Intelligence 122 (1–2) (2000) 71–109.[13] M.L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, John Wiley & Sons, Inc., New York, 1994.[14] J.K. Satia, R.E. Lave, Markovian decision processes with uncertain transition probabilities, Operations Research 39 (1953) 1095–1100.[15] C.C. White, H.K. Eldeib, Markov decision processes with imprecise transition probabilities, Operations Research 43 (1994) 739–749.[16] J.A. Bagnell, A.Y. Ng, J.G. Schneider, Solving uncertain Markov decision problems, Tech. Rep. CMU-RI-TR-01-25, Robotics Institute,Carnegie Mellon University, Pittsburgh, PA, August 2001.[17] M. Kurano, M. Yasuda, J. Nakagami, Interval methods for uncertain Markov decision processes, Markov Processes and Controlled MarkovChains (2002) 223–232.[18] A. Tewari, P.L. Bartlett, Bounded parameter Markov decision processes with average reward criterion, in: Proceedings of the 20th AnnualConference on Learning Theory, in: Lecture Notes in Computer Science, vol. 4539, Springer, 2007, pp. 263–277.[19] S. Kalyanasundaram, E.K.P. Chong, N.B. Shroff, Markovian decision processes with uncertain transition rates: Sensitivity and robust control,in: Proc. 41th IEEE Conference on Decision and Control, 2002.[20] O. Buffet, Reachability analysis for uncertain ssps, in: ICTAI ’05: Proceedings of the 17th IEEE International Conference on Tools withArtificial Intelligence, 2005, pp. 515–522.[21] I.O. Kozine, L.V. Utkin, Interval-valued finite Markov chains, Reliable Computing 8 (2).[22] K. Sen, M. Viswanathan, G. Agha, Model-checking Markov chains in the presence of uncertainties, in: TACAS, in: Lecture Notes in ComputerScience, vol. 3920, Springer, 2006, pp. 394–410.[23] D.P. Bertsekas, J.N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods, Prentice-Hall, Inc., Upper Saddle River, NJ, 1989.[24] P. Tseng, Solving H-horizon, stationary Markov decision problems in time proportional to log(H)*, Operations Research Letters 9 (5) (1990)287–297.