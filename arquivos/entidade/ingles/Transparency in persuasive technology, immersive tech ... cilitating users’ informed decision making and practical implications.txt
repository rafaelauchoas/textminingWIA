Contents lists available at ScienceDirect Computers in Human Behavior journal homepage: www.elsevier.com/locate/comphumbeh Transparency in persuasive technology, immersive technology, and online marketing: Facilitating users’ informed decision making and practical implications Ruijie Wang a,b,*, Reece Bush-Evans b, Emily Arden-Close b, Elvira Bolat c, John McAlaney b, Sarah Hodge b, Sarah Thomas d, Keith Phalp b a Poole House P104a, Poole, BH12 5BB, United Kingdom b Faculty of Science and Technology, Bournemouth University, United Kingdom c The Bournemouth University Business School, United Kingdom d Faculty of Health and Social Sciences, Bournemouth University, United Kingdom  A R T I C L E I N F O  A B S T R A C T  Keywords: transparency Impact on user Persuasive technology Immersive technology Online marketing Informed decision making In the current age of emerging technologies and big data, transparency has become an important issue for technology users and online consumers. However, there is a lack of consensus on what constitutes transparency across domains of research, not to mention transparency guidelines for designers and marketers. In this review, we explored the question of what transparency means in current research and practices by reviewing the literature in three domains: persuasive technology, immersive technology and online marketing. Literature reviewed, including both empirical research and position articles, covered multidisciplinary areas including computer science and information technology, psychology, healthcare, human computer interaction, business and management, law and public health. In this paper, we summarized our findings through a framework of transparency and provided insights into the different aspects of transparency, categorized into ten themes (i.e., Organizational Transparency, Information Transparency, Transparency of System Design, Data Privacy and Informed Consent, Transparency of Online Advertising, Potential Risks, User Autonomy, Informed Decision Making, Information Visualization, Personalization and User-centered design) along three dimensions (i.e., Types of transparency, Impact on User and Potential Solutions). Addressing aspects of transparency will facilitate users’ autonomy and contribute to their informed decision making.  1. Introduction Digital technologies and services have increased accessibility of in-formation and created considerable opportunities for technology users to receive personalized services and for marketers to send targeted ad-vertisements. The Internet and related technologies have made online activities and interaction a part of daily lives, whereby user activities can be continuously tracked and traced. Users can use technologies for tracking purposes such as using wearable technologies to track heath conditions (Bakhshian & Lee, 2021), but user choice plays little part in tracking decisions, in some cases. For example, websites can use cookie-less tracking to bypass users’ consent (Papadogiannakis et al., 2021). This has created questions regarding transparency in relation to online systems and technologies. Researchers have increasingly noted the importance of transparency with the uptake of emerging technologies in the commercial world not only for users’ privacy but also their impression of trustworthiness, integrity and good conduct (DiStaso & Bortree, 2012; Rawlins, 2008; Seizov & Wulf, 2020). Lack of transparency in online communications can cause various issues such as mistrust of the communicator, leading to individuals seeking information from alternative, potentially unreli-able sources (Berger et al., 2020). Transparency can also contribute to adoption of technologies relating to effective management of public health. For example, a recent literature review (Oyibo et al., 2022) on adoption of COVID-19 contact tracing apps (CTAs) concluded that future CTA iterations need to protect privacy via transparency to increase adoption and their privacy statements need to be transparent and informative (Sharma et al., 2020). Technologies and online * Corresponding author. Poole House P104a, Poole, BH12 5BB, United Kingdom. E-mail address: rwang3@bournemouth.ac.uk (R. Wang). https://doi.org/10.1016/j.chb.2022.107545 Received 6 July 2022; Received in revised form 15 October 2022; Accepted 19 October 2022  ComputersinHumanBehavior139(2023)107545Availableonline29October20220747-5632/©2022TheAuthors.PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).R. Wang et al.                                                                                                                  environments, while providing users with 24/7 accessibility and con-venience, also have the power to change attitudes and behaviors without raising awareness about potential risks and harm. This can prevent people from being able to make informed choices (Harris et al., 2017). For example, the rise of the online gambling industry in the United Kingdom is a testament to technological advancements and lack of transparency in online environments that have increased the incidence of gambling-related harm. According to the latest industry statistics (Gambling Commission, 2022a), remote Casino, Betting and Bingo (RCBB) accrued close to £7 billion Gross Gambling Yield (GGY) from April 2020 to March 2021, with an increase of 18.4% compared to the previous year. Gambling-related harms include financial harms, rela-tionship harms, mental and physical health harms, employment and educational harms, criminal activities and led to approximately £1.27 billion of overall economic burden to society in England (expressed in 2019–2020 prices), a number that is likely underestimated due to lack of available evidence (GOV.UK, 2021). A report by the House of Commons (2019) stated, “for an industry generating such high revenues from so many millions of players worldwide, that lack of transparency is unacceptable” with regard to the core design principles “that have been scientifically proven to create repetitive behaviors, and the effect that this might have on the meaningful exercise of choice” (p.63). a imitates technology physical world In particular, the concepts of persuasive technology, immersive technology, and online marketing are all connected to online behavior and have the potential to be used to promote for-profit outcomes that may cause harm in the form of addiction-like behaviors in some con-texts, such as online gambling and shopping. Excessive interaction with technologies and engagement in certain online activities may indicate addictions. For example, Internet gaming disorder and gambling disor-der are recognized as behavioral addictions in the fifth edition of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5; Amer-ican Psychiatric Association [APA], 2013), and problem gambling has been considered a public health issue in the United Kingdom (UK Parliament, 2021). Persuasive technology can take on diverse forms of technology resources including websites, mobile phones or tablets and smart devices integrated into everyday life to persuade users to change their perceptions, attitudes and behavior using persuasive techniques. Immersive using computer-generated simulation and creates a sense of immersion in the form of Virtual Reality (VR), Augmented Reality (AR) or Mixed Reality (MR). Online marketing refers to any effort to spread corporate names using the Internet to reach the public and can take different forms such as search engine marketing and social media marketing. These concepts have been applied to online environments extensively to capture users’ attention, facilitate interaction between users and the digital world and can be a source of online harms. For example, persuasive technologies can be used to increase exposure for online game players via leader boards and badges as compensation for perceived failure and low self-esteem in the real world (Drosatos et al., 2018). Meanwhile, if used appropriately, persuasive design has the potential to moderate against problematic gambling behavior via real-time tracking of user data and persuasive interventions, such as pop-up messages and warning labels to facilitate time and monetary limit adherence (Drosatos et al., 2018; Wohl et al., 2014). Immersive technology users have been found to experience harms such as bullying and harassment. For example, VR used to facilitate social interaction, known as “social VR” (House of Commons, 2019), has raised concerns around player safety, and research undertaken by Oultaw (2018) found that 49% of female VR users had reported perceived sexual harassment within an immersive experience Online marketing involves high volumes of advertisements, personalized targeting compared to other media and may involve unfair practices, fraud, misleading information and privacy issues (Adshead et al., 2020). This can also negatively affect sustainable development of an industry by reducing consumers’ informed choices and trust. For example, online gambling is perceived by consumers to require stricter advertising regulation, “due to ease of access and a perceived lack of barriers to risky play” (Gambling Commission, 2022b). Transparency is a concept related to interpretability and explain-ability. Recently, researchers have shown increasing interest in the field of Explainable Artificial Intelligence (XAI) that aims to increase trans-parency in intelligent systems and inform users about the decision- making process for system behavior, predictions, or recommendations. These involve providing explanations about recommender systems in educational settings (Karga & Satratzemi, 2019; Zheng & Toribio, 2021), autonomous driving systems (Schneider et al., 2021), clinical decision-support systems (Bussone et al., 2015), and robots (Kaptein et al., 2017), among others. However, transparency involves far more than XAI. Specifically, it refers to the explainability of any decision in online systems which can be made by both algorithms and human beings and may affect consumers or end users. To the best of our knowledge, there is a distinct lack of consensus on what constitutes transparency across the domains of emerging technology and online marketing. In this interdisciplinary literature review and synthesis, we do not make dis-tinctions between the different forms of these technologies and online marketing efforts. Instead, we aim to establish a better understanding of transparency as a concept by reviewing the existing literature and syn-thesizing knowledge and evidence across the domains of persuasive technology, immersive technology and online marketing. While each domain approaches transparency from a different perspective, bringing together insights from these respective domains can help to gain greater consensus on its conceptualization. Given limited studies that explore transparency as a concept in emerging technologies and online marketing, and the variation in methodological approaches and quality, we conducted an integrative narrative review to synthesize as much relevant knowledge from the literature as possible. Key aims were to generate meaningful conclusions for the interdisciplinary domains being explored in the present review, and to demonstrate the themes of transparency and potential solutions to protect users from “black boxes” and potential manipulation in technologies and provide suggestions for how marketing can facilitate informed decision making. 2. Literature search and selection 2.1. Eligibility criteria and search strategy To be eligible for inclusion in the review, articles needed to be relevant to transparency; in relation to the domains of persuasive technology, immersive technology or online marketing; and written in English. There was no restriction on publication year. Relevance to transparency was evaluated based on the practices studied, imple-mented, or discussed regarding transparency requirements or lack of transparency. Transparency could appear in various forms in articles, such as ethical considerations or human factors in design. Our literature search was conducted in Web of Science during July and August in 2020. Web of Science covers literature in all disciplinary areas (Cornell University Library, 2020) and has been widely used and reported in review studies (Gusenbauer & Haddaway, 2020). Two experienced reviewers independently screened titles and abstracts. For potentially eligible studies, one reviewer read the full texts. If the two reviewers disagreed on whether to include any article or if one reviewer was uncertain about eligibility of any article, it was discussed amongst all co-authors. For transparency in persuasive technology, a preliminary search using ‘“transparency” AND “persuasive technology”’ generated only three results. “Persuasive technology” was then searched jointly with each of the following terms, one at a time, using “AND”: “explainab*“, “interpretab*“, “informed decision”, “human factors”, “ethics”. For transparency in immersive technology, a preliminary search using “transparency” AND “immersive technology” generated only seven results. “Immersive technology” was then searched jointly with each of the following terms, one at a time using “AND”: “explainab*“, ComputersinHumanBehavior139(2023)1075452R. Wang et al.                                                                                                                  “interpretab*“, “informed decision”, “human factors”, and “ethics”. the next section. For transparency in online marketing, a preliminary search showed research on transparency in marketing is relatively well established, so the terms “online marketing” and “transparency” were combined using the AND function. 2.2. Literature search results In total, 71 articles were selected for this review. There were no disagreements between reviewers relating to eligibility. This is likely due to the nature of the literature review being a narrative review which attempts to integrate, synthesize and categorize all relevant findings, resulting in little ambiguity around the eligibility criteria. For transparency in persuasive technology, of 98 records identified from the literature search, 25 were excluded based on title and abstract screening, and 2 duplicates were removed. Of the 71 full-text articles further assessed for eligibility, 23 articles were classified as eligible. For transparency in immersive technology, of 157 records identified from the literature search, 95 were excluded based on title and abstract screening, and 2 duplicates were removed. Of the 60 full-text articles further assessed for eligibility, 13 articles were classified as eligible. The results for transparency in persuasive technology and immersive tech-nology covered multidisciplinary areas, including computer science and information technology, psychology, healthcare, and human computer interaction (HCI). For transparency in online marketing, of 202 records identified from the literature search, 124 were excluded based on title and abstract screening. Of the 78 full-text articles assessed for eligibility, 35 were identified as eligible. The articles included were mainly in marketing, communication, business, and management, and covered other subjects such as information technology, law and public health. From the present review and synthesis, we grouped the findings from the literature into themes of transparency following the integrative narrative synthesis approach (Toronto & Remington, 2020). Specif-ically, we extracted information (e.g., findings/positions) from the eligible articles and then analyzed the information for similarities and differences (i.e., patterns) in relation to our review purpose (i.e., to establish a better understanding of transparency as a concept). We then synthesized these patterns by moving from mere facts to a conceptual level of knowledge related to transparency as a concept. These themes are presented via a conceptual framework of transparency, detailed in 3. Framework of transparency From this literature review, we conceptualized transparency by categorizing the findings into ten themes representing different aspects of transparency. They are Organizational Transparency, Information Transparency, Transparency of System Design (including 2 sub-themes: Computing process, Persuasive intent and techniques), Data Privacy and Informed Consent (including 2 sub-themes: Information collected and purpose of collection, Data usage and storage), Transparency of Online Advertising (including 2 sub-themes: Advertising intent, Targeted advertising), Potential Risks, User Autonomy, Informed Decision Mak-ing, Information Visualization, Personalization and User-Centered Design. These themes are clustered into three major dimensions: Types of Transparency, Impact on User, and Potential Solutions. The proposed framework is shown in Fig. 1. An overview of the themes, definitions and examples of evidence from the literature reviewed is shown in Table 1. 3.1. Types of Transparency 3.1.1. Organizational Transparency Organizational transparency has been defined as the service pro-vider’s openness about business practices and values, organizational efforts and relationships (Lamming et al., 2001). It concerns communi-cation, interactions, and engagement with external and internal stake-holders. Transparency in communication is an ideal and considered essential for good relationships with consumers (Harris & Rae, 2009; Seizov & Wulf, 2020). In contrast, non-transparent communication can sour relationships with consumers and business partners (Roloff & Aβlander, 2010). For example, public confidence and trust in non-profit sectors has decreased, indicating the need for greater transparency for improving donor decision making (Blouin et al., 2018). Organizers of crowdfunding campaigns can use two transparency tools, updates and certification, to attract donors (Mejia et al., 2019). Updates are a form of operational transparency communicating the campaign’s work to do-nors, while certification is a form of conventional transparency to ensure the campaign truly benefits a charitable purpose (Mejia et al., 2019). Esterhuyse (2019) assessed corporates’ transparency intention based on whether there is a link between two types of “messages”: Fig. 1. Framework of transparency. ComputersinHumanBehavior139(2023)1075453R. Wang et al.                                                                                                                  Table 1 Themes of transparency, definitions, and examples from literature. Dimensions Themes Definitions Examples of quotes from literature Types of Transparency Organizational Transparency Information Transparency Transparency of System Design The extent to which “a stakeholder perceives an organization provides learning opportunities about itself” (Parris et al., 2016, p.233). “The level of availability and accessibility of market information to its participants” (Granados et al., 2010, p.6); the concept originated in Business and Marketing, but it is viewed more broadly in this review involving both marketing information and other information relevant to persuasive technology and immersive technology. The extent to which users are provided with information about system design, which can include persuasive intent and techniques, the computing process to generate recommendations and decisions Data Privacy and Informed Consent Transparency of information collected and purpose of collection, data usage and storage Transparency of Online Advertising The extent to which advertising intent is disclosed for online content, and how personal data is used for targeted advertising, which relates to data privacy and informed consent Impact on User Potential Risks Potential risks relating to technologies should be disclosed to users and assigned equal value and weighting for ethical consideration compared with benefits of technologies User Autonomy Autonomy should be granted to enable users to make decisions with freedom and maintain control in any enrolment process or any data usage involved in using technologies Informed Decision Making Users and potential consumers should be given sufficient transparency for informed decision making on their choices and behaviors Potential Solutions Information Visualization Using techniques to convey data in a concise, visualized format and thus help lay users understand key information better to enhance transparency Personalization and User-Centered Design Adopting a user-centered design method to provide personalized solutions and grant autonomy for different target user groups “An organization achieves transparency by communicating to improve understanding” (DiStaso & Bortree, 2012, p.513). Information revealed from online reviews to help visitors make purchasing decisions should be accurate, targeting visitors who were motivated by suggestions in reviews (Tsao, 2019). Certain technology-mediated nudges working through manipulating behavior raise ethical concerns as users may not be able to recognize their intentions and effects (Caraban et al., 2019); Explainable AI includes transparency in terms, format and language that users can understand about decisions, causality, potential bias based on shortcomings of training data or objective function, fairness in the AI-based decisions and safety/confidence in the AI reliability (Wierzynski, 2020). If users’ information is used in an attempt to (probabilistically) control their mental state, they are likely to view lack of consent as problematic due to overlooking or not respecting their autonomy ( Burr & Cristianini, 2019); Security, integrity and accessibility of personal data to be collected are of fundamental importance (Tapsell et al., 2018); Consent to collect and use personal data for various vague purposes may be provided inadvertently (Khalil et al., 2018). Consumers’ negative reactions due to recognition of advertising intent were mitigated by their perception of sponsorship transparency of a native ad (Campbell & Evans, 2018); A standardized Instagram disclosure for commercial relationships could enhance ad recognition and positively affect consumers’ brand memory and intentions to engage with the post (Boerman, 2020). Given the immersion created from the nature of the VR device covering the visual field and its impact on the user’s sense of reality, it can be particularly challenging to ask co-designers of VR to imagine and predict what might go wrong, and this risk is magnified when these technologies are designed for sensitive settings, such as aged care, mental health, and clinical rehabilitation (Waycott et al., 2018); Gamification may conflict with human flourishing and that it could be “morally corrosive by adversely impacting character” ( Selinger et al., 2014); A deep understanding is required about how our own and others’ values and goals are shaped and influenced by the increasingly information-intense world, and how virtual realities can change affect, cognition, and life (Bostrom, 2003). Persuasion should be based on prior user consent and offer as much autonomy as possible to the user (Spahn, 2011); Studies of human interactions with driving automation executed in an immersive, fullmotion simulated environment have observed a lack of trust in participants when they perceived they had insufficient control (Metcalfe et al., 2010); Data collection process in systems can be made intelligible to the users and modifiable or interruptible at their will (Jacucci et al., 2014). A user interface design which communicated to savers the long- term implications of their decision about retirement savings led users to adjust their behavior more frequently to achieve their saving goals more effectively (Gunaratne & Nov 2015); Using subliminal techniques to influence consumer behavior is highly controversial (Dijksterhuis et al., 2005). A web application was developed with visualization of children’s health behavior data to help children understand their health behavior and that of their peers better (Wang et al., 2017); A novel shape changing handheld haptic navigation device, the “Animotus,” was developed within an immersive environment to help with “real-world” navigation for both vision-impaired and sighted pedestrians (Spiers et al., 2018). A personalized and human-centered engineering method is required to assign equal weighting to users’ profiles and preferences in relation to their business roles (Shahri et al., 2016); The accuracy of AI-based decisions used in technologies or targeted advertising could be improved through users’ feedback ( Stumpf et al., 2007, pp. 1–26). ComputersinHumanBehavior139(2023)1075454R. Wang et al.                                                                                                                  communications targeted at investors (financial stakeholders) and those targeted at non-financial stakeholders. They found that companies satisfying the Socially Responsible Investment (SRI) Index requirements in sustainability reporting to the public were more likely to have better transparency in communicating with not only non-financial stake-holders, but also investors. Previously, transparency in marketing has been defined simply as “the availability of information necessary […] at the right time and in the right way” (Beulens et al., 2005, p.484). However, this general definition lacks practicalities for achieving actual transparency. “Simply providing information does not guarantee transparency. Rather, an organization ach-ieves transparency by communicating to improve understanding” (DiStaso & Bortree, 2012, p.513). Advances in Information and Communication Technology (ICT) have enabled bi-directional information exchange between corporations and their stakeholders and increasingly empow-ered both external and internal stakeholders who expect higher levels of transparency and openness. Consumers’ roles are transforming from passive recipients of information to equal parties with active under-standing of information, suggesting the concept of corporate trans-parency needs revisiting (Seizov & Wulf, 2020). Consumers have a right to transparency especially regarding corporate strategies and activities that might directly impact their quality of life (Vaccaro & Madsen, 2009). Rawlins (2008) offered a more practice-oriented definition of transparency in corporate marketing: “Transparency is the deliberate attempt to make available all legally releasable information – whether posi-tive or negative in nature – in a manner that is accurate, timely, balanced and unequivocal, for the purpose of enhancing the reasoning ability of publics and holding organizations accountable for their actions, policies, and practices” (p.75). More succinctly, transparency is “the extent to which a stakeholder perceives an organization provides learning opportunities about itself” (Parris et al., 2016, p.233). Cohen and Hiller (2009) proposed a two-way collaborative model of corporate transparency and stated that corpo-rate transparency policies should advance the possibility of stakeholders interacting with companies to correct and enrich publicly available in-formation. Another example is the 3T (Timeliness, Transparency and Trust) framework of managing online customer complaints (Stevens et al., 2018). Transparency involves actions to maintain the public re-cord, promote customer-to-customer interaction, and empower brand advocates to defend the brand reputation. These definitions and models empower the recipients with information, emphasize the individual subjectivity of the perception process, and thus contribute to the prog-ress of informed decisions. 3.1.2.Information Transparency As shown above, organizational transparency emphasizes trans-parency in communication of organizations’ practices and marketing and is highly related to information transparency. In the domain of business and marketing, Information Transparency has been defined as “the level of availability and accessibility of market information to its par-ticipants” (Granados et al., 2010, p.6). Granados et al. (2010) proposed a framework based on existing knowledge across disciplines for the business-to-consumer strategy of Information Transparency. This con-sists of “Information From” (Supplier/Intermediary), “Information To” “Elements” (Customers/Competitors), (Product/Price/Inventor-y/Cost/Process), “Actions” (Disclose/Distort/Bias/Conceal), “Systems & Mechanism Design”, “Transparency Regime of a market or industry”, and “Complementary Strategies about other managerial decisions” (e.g., product design or pricing strategy). Information transparency is of great importance in promoting con-sumer satisfaction and informed decision making. For example, Tsao (2019) suggested that the information revealed from online reviews to help visitors make purchasing decisions should be accurate, targeting visitors who were motivated by suggestions in reviews. Transparency can be improved by more explicit information in reviews. In the online financing and banking sector, Malik and Ahsan (2019) found that access to information, risk assessment and transparency are necessary for a co-creation approach inviting customers’ opinions to promote innova-tion in products or services and improve customers’ satisfaction. In an investigation of the relationships of Jordanian banks with Small and Medium-sized Enterprises (SMEs) customers, Alnsour (2018) identified 6 critical constructs of Relationship Quality: trust, commitment, satis-faction, transparency, communication, understanding and cooperation. Transparency regarding information shared with customers was one of the most significant determinants of relationship quality, and commu-nication was identified as the biggest contributor. In a broader sense, information transparency involves not only market information but also other information in the domains of persuasive technology and immersive technology that may impact users’ decisions, attitudes, or behaviors. Therefore, the other themes under Types of Transparency in the Framework shown in Fig. 1 are covered by information transparency and they are also related to organizational transparency when involving communication of such information within an organization or between organizations. 3.1.3. Transparency of System Design Ensuring system transparency and users’ understanding of system design is fundamental for users to use technologies appropriately and maintain control of such technologies. This can involve persuasive principles, simple interaction metaphors in a VR system to provide users with instantaneous knowledge about how to interact with the system, and accessible documents for both experienced and inexperienced users. For example, Hoffmann et al. (2006) noted that users expected more from a VR system than just stereoscopic visualization of virtual worlds. Transparency was implied several times, for example, “VR applications must be useable by inexperienced beginners in a few hours” “Use of simple interaction metaphors the users are familiar with” “Access to external systems and documents (Database, project management systems, etc.)” (p.171), indicating that transparency of the system, including user interaction with the system, is a prerequisite of VR systems. System design can involve persuasive intent and techniques in persuasive technology, and the computing process to generate AI-based recommendation or conclusions for users. True persuasion does not mislead the user and ethical evaluations should consider not only the consequences but also the persuasive intent of the technology (Berdi-chevsky & Neuenschwander, 1999). One primary consideration involves analyzing the types of intentions. Oinas-Kukkonen and Harjumaa (2009) discussed different intentions in persuasive environments: autogenous intentions, exogenous intentions and endogenous intentions. Autoge-nous intentions are driven by people who adopt and use the technology. Persuasive technology with autogenous intentions comprises a natural transparency towards the system intentions as the technology is used as a tool to facilitate changes of attitudes or behaviors already intended by the user (e.g., to reduce the use of alcohol). Exogenous intentions are from people who distribute, or grant users access to the system (e.g., as personal learning websites). Endogenous intentions are from people who created the system (e.g., for promoting purchases of a product). The interpretation of the intent depends on the user’s experience, and sys-tems with this intent should be designed to fit the overall goal of respecting the voluntary nature of users’ changes of attitudes or behavior. It is when considering this nuance of intentions that trans-parency becomes essential to the discussion of persuasive systems. Transparency and ethics are fundamental qualities of persuasion (Benedikt, 2002; Gram-Hansen, 2019). Different approaches to behavior changes in persuasive systems may differ in transparency. The rhetorical concept of “peithenanke” refers to a non-transparent approach of behavior design to persuasion involving potential manipulation (Ehninger, 1972; Fafner, 1997; Gram-Hansen, 2019). In contrast, transparent “persuasive design” adopts a more ethical approach (Gram-Hansen, 2019) and has significant potential for digital behavior design in domains such as health and sustainability (Gram-Hansen & Gram-Hansen, 2013; Miller, 2002; Spahn, 2011). Caraban et al. (2019) ComputersinHumanBehavior139(2023)1075455R. Wang et al.                                                                                                                  identified 23 mechanisms of technology-mediated nudging, clustered in 6 categories: facilitate, confront, deceive, social influence, fear, and reinforce. Some nudges working through manipulating behavior raise ethical concerns as users may not be able to recognize their intentions and effects. For example, users may be automatically enrolled in a procedure while unaware of the enrolment and opt-out policies. In addition to persuasive intent, information about how Artificial Intelligence (AI) based consequences in technologies or targeted advertising are computed is usually not transparent to users. Jacucci et al. (2014) proposed the concept of “symbiotic interaction” that can be realized by combining computation, sensing technology, and interaction design for deep perception, awareness, and understanding between humans and computers. Transparency, as a dimension of symbiotic interaction, makes computing accountable and helps answer questions such as: is the system a black box? What is it doing? Is it configurable? Is it reciprocal so that the user can use system resources (computational constructs) and the system can use user resources (physiology, sublim-inal processes, history, etc.)? Transparency in explaining the AI decision making process to users to generate “white/transparent box” models for AI to be confidently rolled out by industries and governments is termed XAI, as mentioned in Introduction Section. Actions of XAI can be easily understood and analyzed by humans (Hagras, 2018). Explanations include transparency in terms, format and language that users can un-derstand about decisions, causality (including both model inferences and underlying phenomena), potential bias based on shortcomings of training data or objective function, fairness in the AI-based decisions and safety/confidence in the AI reliability (Wierzynski, 2020). Naiseh et al. (2020) reviewed the literature on delivery methods (e.g., auton-omous, on-demand) and modalities (e.g., dialogue) of XAI recommen-dations and found that explanations mainly emphasized the benefit of AI-based recommendations while potential risks such as over-reliance on machines needed more investigation. Furthermore, an algorithm’s accuracy is usually directly linked to the reliability of decisions or rec-ommendations, and this information can be communicated to users alongside the decisions or recommendations. 3.1.4. Data Privacy and Informed Consent Information transparency is related to ethical considerations. There are two types of interrelationships: dependence and regulation. The former refers to the fact that certain information is required to endorse ethical principles, which is relevant to transparency in legal information disclosure. The latter means that ethical principles govern information flow by restricting the access, usage, dissemination and storage of in-formation (Turilli & Floridi, 2009). One of the leading ethical concerns across the three domains is the issue of privacy. This involves information collected and the purpose of collection, data usage and storage. Data usage should be considered along with the context and purpose of usage. For example, if users’ in-formation is used in an attempt to (probabilistically) control their mental states, they are likely to view lack of consent as problematic due to their autonomy being overlooked or not respected (Burr & Cristianini, 2019). When there were no clear laws on privacy, research suggested users of AR technology should be concerned about their privacy regarding issues such as disclosure, ownership, and intended use of private information (Pase et al., 2014). The European Union’s data privacy law, the General Data Protection Regulation (GDPR) has established compliance guidelines for companies to provide protection for individual rights regarding data privacy (GDPR.EU, 2018). However, there remains a long way to go before immersive technologies are compliant with the regulations. For example, privacy considerations need to be incorporated into design, and users should be given the op-portunity to fully understand issues around data privacy. Companies have used personal data about individuals’ activities and preferences widely to provide targeted and personalized online services. In this context, consumers authorize companies to collect their personal data to receive, in return, personalized or context-aware services. However, security, integrity and accessibility of personal data to be collected are of fundamental importance (Tapsell et al., 2018). For example, Tutty et al. (2019) analyzed websites offering personal genomic testing for nutrition and wellness to Australians. The content on the websites was found to be emotive and lacking transparency for informed consent regarding the scientific and ethical aspects of infor-mation shared with healthcare providers for the testing. They argued that ethical information along with service information and technical information should be made available to potential clients (Tutty et al., 2019). Companies’ success in acquiring client data relies on the 3 Ts: Transparency, Type of data, and Trust (Mazurek & Małagocka, 2019). Where usage of personal data is involved, informed consent should be obtained from individuals prior to data collection. This is both best practice and the law according to GDPR and other national regulations. Companies have the responsibility to share with consumers precisely how their data will be used and how data will be processed and/or may be transferred to third parties. Although numerous strategies for protecting personal privacy rely on regulatory frameworks, consent, and anonymizing data, they are not always effective. For example, Terms and Conditions often lag behind evolution of technology, software, and user behaviors. Consent to collect and use personal data for various vague purposes may be provided inadvertently (Khalil et al., 2018). Khalil et al. (2018) reviewed 4 Massive Open Online Course (MOOC) providers from different contexts, and found that students’ consent involved not only usage of their data but also the ownership and copyright of the content they produced on the platforms, which might be provided unknowingly. This study emphasized the responsibility of MOOC providers to clarify the potential uses and sharing of personal data and the need for a higher level of transparency for users to grant consent at the point of registration. Pollach (2005) summarized 4 linguistic patterns in privacy policies that could harm transparency in communication of data usage: mitigation & enhancement (e.g., emphasis of qualities when speaking of “carefully selected” third parties); obfuscation of reality (e.g., attempt to avoid responsibility about data misuse via passive voice); relationship building (e.g., attempt to involve consumers emotionally by using first-person pronouns); and persuasive appeals (e.g., attempt to convince con-sumers of their trustworthiness by stating certain data usage is standard practice), and suggested that companies redesign their privacy state-ments to improve transparency in communication of data handling practices, laying the foundation for informed consent. 3.1.5. Transparency of Online Advertising Transparency of online advertising involves advertising intent and targeted advertising. Online platforms and AI techniques have enabled marketers to provide personalized, targeted advertising. For example, consumers’ online behavior data such as browsing history can be collected and used to generate ads for similar products. In this context, ad transparency involves disclosure of personal data collection and usage. Kim et al. (2019) demonstrated that the benefit of advertising transparency depends on whether the marketing practices made trans-parent violated norms about information flow, i.e., consumers’ beliefs about how their information moves between parties. Specifically, con-sumers evaluate acceptability of information flow based on: 1) where the information is collected (i.e., within or outside of the website dis-playing the ad), and 2) whether the information is reported by the consumer or inferred by the company. Busser and Shulga (2019) found that participating in Consumer-Generated Advertising (CGA) had the potential to improve loyalty and trust of both brand customers and noncustomers. The results also identified established CGA contests as a relational marketing tool for hospitality brands. Transparency can also benefit advertisers other than consumers. As a proxy between adver-tisers and customers, a mercenary ad broker could arbitrarily make up advertising rates to overcharge advertisers. Huang et al. (2019) ComputersinHumanBehavior139(2023)1075456R. Wang et al.                                                                                                                  proposed a fair online advertising scheme to avoid this possibility of collusion by generating a unique acknowledgment for downloading the ad, which would then be made publicly available and verifiable to both advertisers and ad brokers to ensure the fairness and transparency of online advertising. Native advertising is a relatively new form of advertising that is displayed surrounding non-advertising content (Campbell & Evans, 2018). It has the risk of deceiving consumers because it is mixed with context and thus consumers viewing it may not be aware of its adver-tising intent. Campbell and Evans (2018) found that inclusion of a companion banner in native advertising boosted consumers’ ad recog-nition, and consumers’ negative reactions due to ad recognition were mitigated by their perception of sponsorship transparency of a native ad, which made it easier for them to recognize the paid nature of the ad. Social media influencers often include sponsored messages in their videos, but potential consumers may not be aware when a video includes advertising. An experiment (Boerman & van Reijmersdal, 2020) involving children showed that advertising disclosure in a YouTuber’s video increased children’s brand memory through ad recognition, but there was a decrease in desire for the advertised product caused by understanding the selling and persuasive intent. Likewise, an experi-ment involving adults (Boerman, 2020) showed that a standardized Instagram disclosure for a commercial relationship could enhance ad recognition and positively affect consumers’ brand memory and in-tentions to engage with the post. 3.2. Impact on User In addition to the types of transparency described above, themes were also generated in relation to how transparency or lack of trans-parency can impact users’ thoughts or behaviors. “User” here includes not only those using persuasive and immersive technologies but also consumers or potential consumers targeted by online marketing, i.e., users of the Internet or digital technologies communicating marketing information. 3.2.1. Potential Risks Interaction between humans and technology should be considered in relation to the social and cultural context. Potential risks and benefits relating to emerging technologies should be assigned equal value and weighting for ethical considerations. Behavior-steering or persuasive technology may be perceived as threats to individual freedom and rights as it is designers rather than democratically elected representatives who influence users’ behaviors when they create such technologies (Pet-tersen & Boks, 2008; Verbeek, 2006). If the technology aims to change individuals’ behavior based on others’ intentions or to convince users to accept their goals and values, information provided can be used to manipulate individuals into action, inaction, or changing their beliefs and attitudes (Pettersen & Boks, 2008). This also supports the previously mentioned importance of distinguishing autogenous and endogenous intentions (Gram-Hansen, 2019). Another interesting aspect under debate is the application of persuasive technologies to promoting moral progress. Moral technolo-gies refer to those with diverse interventions for aiding people to behave more morally with less effort (e.g., sensors with biofeedback as a reminder of an employee’s rising stress level; social robots acting as an adviser for moral coaching). Frank (2020) argued that a world saturated with moral technologies will lead to fewer moral struggles, as it becomes increasingly easier to do what is morally right. This would mean that users will lose the independent learning from the process of experi-encing such struggles. Gamification has been widely applied to technologies to facilitate users’ adoption and sustained use with the potential to promote positive behavior change, especially with regards to youth and mental health (Brown et al., 2016; D’Alfonso et al., 2019; Fleming et al., 2016). However, there are also potential risks and ethical concerns, such as the argument that gamification conflicts with human flourishing and that it could be “morally corrosive by adversely impacting character” (Selinger et al., 2014). Specifically, users’ characters can be weakened by technology-mediated assistance when they develop gamified habits and become dependent on digital willpower (e.g., relying on Apple’s Siri as a reminder of daily activities and weather may negatively impact a user’s mental ability and sensibilities). Human robot interaction also lacks transparency about the potential risks of interacting with an engaging or persuasive robot. For example, more transparency is required to answer questions such as whether these robots are beneficial for users in the long term, how robot addiction, similar to other digital addictions, can be prevented, and whether robotics companies will prioritize ethical considerations over short-term financial benefits (Sandoval, 2019). Additionally, robots’ realistic physical appearance can impact users’ feelings. For example, the “uncanny valley” hypothesis suggested that robots made imperfectly similar to real humans may provoke feelings of revulsion (Mori et al., 2012). However, other researchers argued that realistically depicted humanlike robotics would serve as a highly refined metric for assisting in understanding human social perception and cognition (Hanson et al., 2005). Immersive technology, on the one hand, provides unprecedented immersive experiences many of which users can rarely experience in the real world. On the other hand, as the world of immersive technology becomes deeper and more intense, applications and problems that come with the developing industry will bring about increasing concerns. For example, legal systems argue about virtual crime (i.e., whether it is ethical to permit illegal behavior in a simulated environment). AR’s immersive and potentially persuasive nature creates ethical issues in terms of its impact on society and users’ perceptions and behaviors, including how they will be affected, informed, manipulated, or persuaded by the technology. This relates to both physical and psy-chological safety and wellbeing of end users and those surrounding them (Pase et al., 2014). Blurring the lines between the real world and the artificial one is necessary for AR to immerse users within a 3D environment and create immersive experiences. However, this will potentially be another sig-nificant concern in the future. Given the immersion created from the nature of the VR device covering the visual field and its impact on the user’s sense of reality, it can be particularly challenging to ask VR de-signers to imagine and predict what might go wrong. This risk is magnified when these technologies are designed for sensitive settings, such as care of older adults, mental health, and clinical rehabilitation (Waycott et al., 2018). There is also a risk of provoking trauma for people with posttraumatic stress disorder, particularly in immersive VR environments where it is hard for them to “escape”. For example, an under-water virtual environment trying to provide a soothing medita-tive experience can be dangerous for users who have had near-drowning experiences. In the context of gambling, slot machines are one of the most common type of games played by problem gamblers (Heidrich et al., 2019). Slot machines powered by VR technology allow players to immerse themselves in 3D gaming environments and interact with the game features in the environments. Emerging technologies (e.g., immersive VR), increase possibilities to exploit players’ erroneous be-liefs. Nonetheless, the risk potential of VR-based gambling has rarely been explored. Heidrich et al. (2019), in a study of a slot machine realized both as a desktop 3D and as an immersive VR version, revealed significantly greater effects on dissociation, dark flow, and urge to gamble in the VR version. These harm-inducing factors worsened by VR should be made transparent and incorporated into educational materials. Another example involves the debate on intelligent virtual agents that may be used in immersive technologies (e.g., pedagogic agents in immersive virtual learning environments). An intelligent virtual agent is an AI that can make decisions or perform services without human guidance to aid ubiquitous communication. However, there are poten-tial issues with intelligent virtual agents such as privacy and ComputersinHumanBehavior139(2023)1075457R. Wang et al.                                                                                                                  relinquished authority. Bostrom (2003) argued that to create moral agents in immersive technology, a deep understanding of how our own and others’ values and goals are shaped and influenced by the increas-ingly information-intense world, and how virtual realities can change affect, cognition, and life is required. Otherwise, informed decision making may be replaced by persuasion without reason. Some potentially negative effects of technologies are still unknown and further research is needed to enhance understanding and trans-parency of the risks and resolutions. For example, Cho et al. (2012) showed that familiarity with 3D games and VR type technology affected users’ ability to perceive depth accurately in a volumetric dataset. Such effects on volume data perception have received scant attention in the literature. 3.2.2. User Autonomy Autonomy is a basic psychological need that helps to maintain well- being and motivation according to Self Determination Theory (Deci & Ryan, 2000; Ryan & Deci, 2017). Nudges communicating social norms for healthy behavior change without satisfying people’s sense of au-tonomy may cause adverse psychological reactions (Gelfand et al., 2020). Autonomy is granted to enable users to make decisions with freedom and maintain control in any enrolment process or data usage involved in using technologies. Persuasion should be based on prior user consent and offer as much autonomy as possible to the user (Spahn, 2011). Algorithms used for persuasive technologies have also raised concerns about uncertainty and subjectivity. Though professionals have a duty to do good, paternalistic medicine should be avoided where possible (Boers et al., 2020). In AI-based clinical decision support sys-tems that provide personalized outcomes, the decision-making process and algorithms can be invalid, biased, or even discriminatory as certain groups can be excluded from data analysis raising issues regarding the representativity and applicability of the algorithms. Studies of human interactions with driving automation executed in an immersive, full-motion simulated environment have observed that participants lacked trust when they perceived they had insufficient control (Metcalfe et al., 2010). On the one hand, encouraging partici-pants to make changes at any time at their will is important to ensure transparency. However, on the other hand, this raises issues in terms of safety in certain settings such as driving. Even with accurate prediction of the preferred control modes that are most likely to lead to better performance, there is currently no solution about how to reliably elicit users’ selection of that mode, if full transparency and user control is required. This is made especially challenging because of individual biases. Although their influence is weak, visual indicators might be useful to provide transparency so that users could understand when system recommendations were particularly strong and when users’ preferences represented only a weak advantage for one mode over the other (Metcalfe et al., 2017). Furthermore, users’ understanding and autonomy should be granted regarding data collection and usage, including the information being collected, when it is collected and what it is used for (i.e., diagnosis, prediction, persuasion, or control), and how it is stored, anonymized and removed. For example, the data collection process in systems can be made intelligible to the users and modifiable or interruptible at their will (Jacucci et al., 2014). The same is true for data processing and storage. Tapsell et al. (2018) proposed a framework to establish a transparent and robust relationship between consumers and organizations. This would help achieve a balance between consumers’ autonomy to control their data and organizations’ goals to deliver targeted services based on consumers’ personal data with high quality and efficiency. They advo-cated that empowering consumers with control over their personal data will not only benefit organizations in ensuring that they conform to GDPR but also by contributing to a positive brand image of transparency and openness. 3.2.3.Informed Decision Making Addressing different aspects of transparency described above will enhance users’ understanding of key information, facilitate user au-tonomy and thus contribute to their informed decision making. As stated, information transparency involves both availability and acces-sibility of information. Given sufficient information available, good quality information can be primarily achieved by appropriate design and potentially enhanced by technology. Work on persuasive technology (Fogg, 2002) has also influenced HCI research studying how technology can change behavior (Gunaratne & Nov 2015). In this regard, persuasive technology can facilitate communicating more effective information to the user to help with informed decision making or behavior change. Prior HCI research explored how to motivate individuals to change their behavior through design interventions in areas such as healthcare informatics and envi-ronmental sustainability. HCI researchers and designers can provide interventions to help people make informed and effective decisions, such as decisions about their water usage (Froehlich et al., 2012), food choices (Lee et al., 2011) and retirement savings. Gunaratne and Nov (2015) who applied the behavioral economic theories of endowment effect and loss aversion to the design of novel retirement saving user interfaces, found that designs which communicated to savers the long-term implications of their decision about retirement savings led users to adjust their behavior more frequently to achieve saving goals more effectively. It is evident that appropriate design of user interfaces with more effective information and forms of communicating this information is good practice to help users make informed decisions. However, ques-tions remain about what information is included and how this infor-mation is delivered to users, as this may impact users’ information processing and decision making. Appropriate design cannot be achieved without considering dark design patterns. Gray et al. (2018) looked at the “dark patterns” of User Experience (UX) design, where user value is supplanted in favor of shareholder value. UX designers could easily be involved in facilitating manipulation or unethical persuasion. Five dark pattern strategies used in UX design have been summarized, including Nagging (i.e., actions such as pop-ups and audio notices that obstruct or redirect users’ focus), Obstruction (i.e., barriers such as disabled functionality to persuade an action such as paying for membership), Sneaking (i.e., attempts of hid-ing or delaying the disclosure of information such as additional undis-closed costs to impede informed decisions), Interface interference (i.e., manipulation of user interfaces to privilege specific actions over others such as preselecting atypical user choices by default), and Forced action (i.e., situations where users have to perform specific actions to gain access to specific functionality such as levels in video games that are impossible to achieve without buying powerups or extra lives). Likewise, using subliminal techniques to influence consumer behavior is highly controversial (Dijksterhuis et al., 2005), and subliminal priming can be used as a “dark design pattern” to attempt to manipulate users (Brignull, 2011; Caraban et al., 2018; Greenberg et al., 2014). For example, de-signers might use subliminal priming to make it easier for users to prefer one product over others without their consent (Pinder, 2017). Caraban et al. (2018) suggested that any application using dark patterns should ensure they address user fears and misunderstandings in the first place. They highlighted that it is the responsibility of researchers and designers to ensure that interventions in persuasive systems are delivered in an ethical, transparent fashion. In addition to users, designers may also not be aware of the potential dark side and negative social impact of these design strategies due to a lack of formal education of ethics in UX and HCI. Immersive technology per se can be used to facilitate participatory design and informed decision making. This is because of its capability to visualize information (explained in the next Section) in 3D environ-ments to promote understanding. For example, MR applications have been used to explore new immersive co-design methodologies and ComputersinHumanBehavior139(2023)1075458R. Wang et al.                                                                                                                  meaningful trajectories for participatory processes in space (Fricker, 2019). Melenbrink and King (2015) developed an integrated real-time computational workflow for architectural design and used a simulated spatial environment to give designers the illusion of being in a space that was being designed. This allows for iterative design based on experi-entially informed decision making. Using 3D modelling and immersive VR technologies could help engage citizens in participatory urban planning (Van Leeuwen et al., 2018). These developments imply that immersive technologies can be used for informed design decision in participatory design. From the end users’ perspective, these design de-cision processes can also be made available and explainable to users at their will. As involving a sample of users in the design process can also help the design address users’ needs and preferences, this technology can help convey information to lay users in the user-centered design process. Immersive technology can also be used to improve the trans-parency of product experience to facilitate potential consumers’ informed purchasing decisions. For example, VR technologies have been used to provide users with an interactive, immersive, and realistic product experience at low cost. The VR experience also leads to greater confidence, information, and realism in consumers’ preference judge-ments compared to traditional 2D forms of product representation or a feature list (Tovares et al., 2013). 3.3. Potential Solutions The last dimension is potential solutions to ensuring appropriate design of user interfaces and information for enhancing transparency and informed decision making, including Information Visualization, Personalization and User-Centered Design. 3.3.1.Information Visualization Information Visualization techniques can convey data in a concise, visualized format and thus help lay users understand key information better to enhance transparency. For example, a web application was developed with visualization of children’s health behavior data to help children understand their own and their peers’ health behavior better (Wang et al., 2017). The visualization, including various visual repre-sentations of data with gamified features, encouraged children to interact with the visualized data for both entertainment and education. Moere et al. (2004) used a novel exploratory information visualization technique called infoticles within immersive VR environments, which stood for information particles and represented data objects. Infoticles helped visualize the time-varying characteristics of large, dynamic datasets in a cognitively distinguishable and interpretable manner. Immersive technology combined with multimodal channels to convey information has the potential to help users with their daily ac-tivities in certain circumstances. For example, a novel shape-changing handheld haptic navigation device, the “Animotus” was developed within an immersive environment to help with “real-world” navigation for both vision-impaired and sighted pedestrian (Spiers et al., 2018). The form of the device was modifiable in the user’s grasp to convey infor-mation about heading to and proximity to navigational targets. The study suggested that more structured device familiarization, especially for vision-impaired users, could help enhance performance and reduce incorrect expectations of the technology. If users of immersive technology are students, such technology can serve or facilitate the purpose of communicating and sharing knowl-edge. Experiential methods based on ICT such as virtual strategic games are good for enhancing knowledge and filling the gap between theory and practice. For example, immersive simulated reality scenarios for enhancing student nurses’ experience of people with learning disabil-ities have an advantage in blended learning and collaborative teaching (Saunder & Berridge, 2015). Holdsworth and Apeh (2017) developed an immersive Cyber Security Awareness learning platform with gamifica-tion elements to reduce security breaches caused by human error by improving employee learners’ awareness of threats and potential implications. Gupta et al. (2019) discussed the adoption of information-centric systems engineering principles to design a cyber-human systems-based simulator framework and demonstrated the effectiveness of using such frameworks to train orthopedic surgery medical staff in haptic and immersive VR learning platforms. 3.3.2. Personalization and User-Centered Design Shahri et al. (2016) conceptualized software-based motivation within enterprises and argued that a personalized and human-centered engineering method is required to assign equal weighting to users’ profiles and preferences in relation to their business roles. The individ-ual need for personalization of system transparency has also been raised and could potentially become a solution to the lack of system trans-parency. Adopting a user-centered design method could provide personalized solutions and grant user autonomy for different target user groups instead of a one-size-fits-all solution for all users or asking de-signers to imagine and predict potential harm. To ensure the transparency and user awareness of ongoing data collection processes, the potential solutions also involve ensuring transparency in the criteria used by technologies to create profiles, which again remains in the hands of designers (Jacucci et al., 2014). User-centered and participatory tools and processes of design disciplines such as interaction design and participatory design can be beneficial to ensure transparency and optimal user experience and facilitate deeper mutual understanding, cooperation, and independent agency in the human-computer relationship (Pettersen & Boks, 2008). In a broader sense, user-centered design implies a human-centered engineering approach to the design and development of technologies. In AI-based systems, if users themselves could participate in the process of opti-mizing algorithms, the accuracy of AI-based decisions used in technol-ogies or targeted advertising could be improved, and the users’ understanding and trust of the systems could also be improved. This could be achieved through rich interactions between users and systems involving a system’s explanation of the reasoning process, the user’s critiques and adjustments and reasoning correction based on user feedback (Stumpf et al., 2007, pp. 1–26). Furthermore, user-centered design can lead to better accommodation of users’ needs and re-quirements. However, research also suggests that high degrees of user involvement can cause a decrease in the flexibility, effectiveness and chance of success and innovations of a project (Preece et al., 2002). Pettersen and Boks (2008) argued that when designers translate abstract concepts and complex information into visualized representations and physical shapes, simplify jargon, technical schemes or political struc-tures to fit the knowledge and experience of lay co-designers like most users, there is a chance that the lay co-designers’ understanding of the concepts may be biased by the intention of designers and facilitators. 4. Discussion This review provides insights into the different aspects of trans-parency involved in the domains of persuasive technology, immersive technology, and online marketing through a conceptual framework of transparency. Addressing these aspects of transparency will facilitate users’ autonomy and trust and contribute to their informed decision making. While investigating the theoretical framework of transparency, we also formed observations regarding the overlaps across the three domains, the practical implications and reflections including design features of transparency online, limitations of full transparency and the role of technology. This section shares and discusses these observations. 4.1. Overlaps across domains There has been a lack of consensus on what constitutes transparency across domains. Based on our findings, we synthesized different aspects of transparency and generated a holistic picture to facilitate under-standing of overlaps across the domains of persuasive technology, ComputersinHumanBehavior139(2023)1075459R. Wang et al.                                                                                                                  immersive technology, and online marketing, shown in Fig. 2. Best practices for transparency described above are transferable across the three domains given their overlaps. For example, technologies have been extensively applied to online marketing. Specifically, in on-line marketing, websites are created for the purpose of persuading users to make purchases online, and persuasion is implemented via applying persuasive techniques such as social proof (e.g., linking a product to other consumers’ reviews), both grounded in the psychology of persuasion (Cialdini, 2007), and authority (e.g., offering expert advice) or utilizing an AI-based recommendation system for targeted advertising as discussed. Transparency of online advertising especially native ads should be improved in terms of the advertising intent and nature. Likewise, immersive technology has been employed by companies for their brand marketing efforts. For example, Snapchat has launched AR shopping lenses that allow users to interact with the products and brands (Dodoo & Youn, 2020). Algorithms used to filter online content have also raised concerns. Recently, He (2022) argued that in the context of the cultural censorship system in China, it is crucial to mandate trans-parency for algorithm-made decisions in filtering systems for online content to build the trust of the public for future algorithmic decision-makers, though it lacks clarity and discussions about which aspects of “transparency” should be mandated. Data privacy is a common concern across all three domains. As dis-cussed, where usage of users’ personal data is involved, informed con-sent including the purpose of data collection, what data are collected, and how the data are stored, anonymized and removed should be ob-tained from users prior to data collection to maintain user autonomy. Information transparency and informed decision making are at the core of all three domains that can benefit multiple parties. For instance, recent research (Correia et al., 2022) highlighted that upgrading trans-parency on micro-loans can contribute to the efficiency of the online credit marketplace, both democratizing access to finance for borrowers and protecting lenders. Both the quantity of the information available and the quality of medium and communication regarding information accessibility are essential to transparency and users’ informed decision making. If sufficient information is available, transparency remains in the hands of designers. Potential solutions to improving transparency involve information visualization techniques and a human-centered, personalized design and engineering approach to the design of tech-nologies, and the presentation and communication of information to multiple stakeholders in marketing contexts. 4.2. Design features for transparency in online environments To ensure transparency in online environments, users should be given access to information, and communication of the information should help users understand the criteria necessary to achieve user au-tonomy and freedom. However, due to the gaps in technical literacy and the lack of specific guidance on how information disclosures should be designed or formulated, there are risks of accidental or purposeful obfuscation in traders’ communication with consumers. Transparency in communication can be promoted by “clear and understandable messages, free of specific legal or technological terms” (Mazurek & Małagocka, 2019, p.7). The device where information is displayed and the design of online information strongly affect its transparency. Seizov and Wulf (2020) provided recommendations on the design and presentation of text and webpages based on evidence from corporate marketing, communication science, and empirical legal studies. These include responsive web design in terms of clear text structures on mobile screens, setting hard limits to legible font sizes for various screen sizes, avoiding unusual font types, making all vital contractual information no more than “one click away”, general information including the privacy policy being acces-sible from any webpage of the trader and using hyperlinks for additional content to the consumers’ advantage. Features found to directly impact information transparency include language, text length, information Fig. 2. Overview of transparency with overlaps across Persuasive Technology, Immersive Technology and Online Marketing. ComputersinHumanBehavior139(2023)10754510R. Wang et al.                                                                                                                  presentation, grammar and syntax. In brief, keeping it short, simple and easy on the eyes is important to transparent information disclosure (Seizov & Wulf, 2020). The implementation of information obligations involves four broad aspects: content (e.g., text, sentence structures and linguistic choices), presentation (e.g., font types, sizes, colors and hyperlinks), compre-hensibility (e.g., adjustment to devices and responsive design), and the human element in these areas (i.e., authors, recipients and interpreters of information) (Seizov et al., 2019). These aspects are essential to consumer protection in terms of lowering consumer burden and improving information transparency. Disciplines such as communica-tion science and information design, critical linguistics, eye-tracking research and neuroscience should be employed to improve online in-formation design (Seizov et al., 2019). Specifically, eye-tracking and neuroscience research has found that novice users need learning op-portunities and should be provided with more detailed guidance and information, while experts should be granted the freedom to select their own learning style and pace (Kalyuga et al., 2003). Regarding the in-formation layout, communication science and information design im-plies that complex information should be presented with a clear hierarchy of headings and subheadings, along with varied fonts to highlight important information; also, visual representation and multi-modal transfer of information could greatly enhance users’ under-standing (Bateman, 2017). In terms of the language used, critical linguistics suggests short and simple sentences should be utilized in in-formation disclosures wherever possible and that obscure terminology, excessive use of modal verbs, rhetorical questions, and the passive voice should be avoided in such a way that consumers’ doubt is minimized (Elshout et al., 2016; Micklitz et al., 2017). From the perspective of companies trying to design selling mecha-nisms to maximize profit, providing product information has been found to lessen price pressures resulting from Internet-enabled price compar-isons (Granados et al., 2012). Granados et al. (2012) suggested that for brick-and-mortar companies with online services, a sound multichannel strategy should include the design of online selling mechanisms that make product attributes transparent to consumers. They noted that even the opaque online travel agents have implemented transparent selling mechanisms to increase competitiveness in this dimension otherwise the opaqueness of information on product attributes and quality can result in a very price-sensitive market. In terms of information on social media, the inclusion of hashtags, photos and videos in messages positively af-fects the number of likes; conversely, the use of URL, mentions and photos in posts negatively affects citizens’ commenting behavior (Lap-pas et al., 2018). Interestingly, the authors found that although photos increased attention and likes, they should not be used to facilitate online discussions. Evidence based on popularity (likes), commitment (comments) and virality (shares), suggests that effective Facebook communication should increase transparency, provide general information, and include multimedia (Ellison & Hardey, 2014). More recent research (Karagür et al., 2022), examining differences between types of information disclosure on social media, found that disclosure using Instagram’s branded content tool enhances the noticeability of the advertising intent more than the influencer’s self-generated in-text disclosures. Addition-ally, Van Reijmersdal and colleagues (2020), in an eye tracking study, found that sponsorship disclosure prior to the start of social influencer videos was better processed by children and understood with more vi-sual attention. They suggested that policy makers increase transparency of online embedded advertising regarding sponsorship disclosures to minors. 4.3. Full transparency and trade-offs Despite the benefits of transparency, we should be mindful that providing users with full transparency or full autonomy may cause more risks than benefits if the specific application context is not considered. One such example of context is online gambling. Gambling behavior is maintained by cognitive distortions regardless of negative outcomes (Jacobsen et al., 2007). Cognitive distortions, defined as a state wherein “habitual ways of thinking function to support core beliefs and assumptions by generalizing, deleting, and/or distorting internal and external stimuli” (Yurica & DiTomasso, 2005, p.118). In the context of gambling, it refers to gamblers’ various erroneous beliefs. One example is the gamblers’ fallacy: when random events have deviated from the population average for a period of time, individuals believe that the opposite deviation is “due” (Tversky & Kahneman, 1971). Specifically, when a roulette ball has fallen on a red slot several consecutive times, gamblers may believe that a black winner is more likely to appear. Another example is gam-blers’ illusion of control, which is the belief that the probability of personal success is unjustifiably higher than the objective probability should warrant (Goodie, 2005; Langer, 1975), which could involve the principles of sympathetic magic (Wohl & Enzle, 2002). In other words, where control over outcomes is important, sympathetic magic allows gamblers to consider causal forces such as personal luck that are un-recognized in the world of physical laws and linear causality and to erroneously believe that their personal luck will lead to a satisfactory outcome (Wohl & Enzle, 2002). Therefore, it is possible that trans-parency on previous betting results offered to gamblers in online gambling websites might lead to them using the information in mal-adaptive ways due to their cognitive distortions. In this regard, offering transparency on educational information surrounding the nature of gambling and chance of winning could potentially be a complementary solution. Additionally, there are unanswered challenges and trade-offs to be determined. For example, ensuring full explainability may compromise the complexity of algorithms and decrease the possibility of optimiza-tion to achieve the highest accuracy. If algorithms are complex and full explainability is not feasible, users can act as a sanity check for evalu-ating algorithms, especially ruling out false results instead of achieving full explainability and catching all possible errors. In such cases, user plus system is better than either on their own, as it facilitates users’ understanding and trust in algorithms without compromising the necessary complexity of the algorithms. Boers et al. (2020) argued that an adequate balance should be struck between using persuasive tech-nology to do good for users and fostering user autonomy, especially in the healthcare domain. For example, surgeons may make medical de-cisions for patients without providing them with full transparency and autonomy under certain circumstances, such as if they are unconscious following an accident. 4.4. Technology: enhance or inhibit transparency The emergence of and advances in the Internet and technologies have resulted in growing availability and accessibility of information and thus enhanced transparency. For example, internet-enabled price comparison and CGA have improved information transparency in online marketing. Also, immersive technology has enabled 3D visualization of information and facilitated information communication and under-standing for purposes such as informed purchase decision making, learning and teaching, and participatory design. For instance, a recent study (Zhang et al., 2021) on virtual surgical training systems enabled by modern medical technologies and VR shows that the design of collision detection and force feedback algorithms is essential to ensure transparency and immersion for trainees. The Virtual Experiences Lab-oratory (VXLab) at RMIT University visualized time series data as static images, and the solar wind imagery in a looping video format, providing an immersive space science experience for students that fosters learning, cooperation and “transparent data sharing” (Carter et al., 2022). Addi-tionally, new technologies provide opportunities to solve problems regarding lack of transparency. For example, blockchain has the po-tential to solve lack of transparency in copyright ownership of digital content (Savelyev, 2018). Specifically, blockchain makes it possible for ComputersinHumanBehavior139(2023)10754511R. Wang et al.                                                                                                                  any individual to record a certain event taking place at a certain time in a public, immutable manner. The information about copyright owner-ship can be provided by the so-called “Trusted Timestamping”, which is a way of securely tracking the creation and modification time of a document, thus enabling anyone to define the presumption of author-ship and resolve disputes (Savelyev, 2018). Recent research (Amor & Yahia, 2022) has investigated the added value of blockchain features to online transaction platforms and found positive effects regarding perceived privacy and security, highlighting that blockchain can in-crease trust and transparency online. However, digital technologies used to deliver information have also increased the possibility of inaccurate information, loss of privacy, identity theft and disinhibited information (Grimani et al., 2020). In addition, the pervasiveness of the Internet and technologies in everyday life has also provided the possibility for people to engage in covert be-haviors and activities online such as the illicit drugs trade. Using ano-nymizing and encryption software, vendors and customers can operate relatively secretly in online drug markets via covert electronic communication and encrypted virtual currencies, while the infrastruc-ture of Darknet Marketplaces (DNMs) allows information on drugs such as prices and shipping information to be published in detail (Tzanetakis et al., 2016). This transparency paradox implies both social and tech-nical challenges on the existing system to control purchase and supply of illicit products. 5. Limitations and future research 5.1. Limitations While the current review is an integrative narrative review rather than a systematic review narrative reviews “have a place in science” to facilitate discussion (Faggion et al., 2017). The aim of this review is to encourage awareness, provoke new discussions on relevant topics and apply the concept of transparency across new fields. Researchers such as Nukarinen et al. (2022) and Marsh et al. (2022) also applied a narrative approach to their recent literature reviews for similar reasons. We believe it is the best choice for our research question enabling us to synthesize diverse methodologies and data sources. We note that the application of stringent quality and keyword criteria, as is required for systematic reviews, could have led to relevant papers being missed and thus would act as a barrier to obtaining insights from a wide range of perspectives, which Singh et al. (2021) identified as a limitation in their review. 5.2. Future research This review highlights the developments of research in this area and the need for future research on transparency in emerging technologies and online marketing. Six key suggestions and considerations for future research were identified. First, research and methodologies from different disciplines such as psychology, HCI, computer science, communication science and infor-mation design will be necessary to draw a comprehensive conclusion regarding theories and practical guidelines. For example, recent research shows that information in tutorial videos, property compari-sons and transparent transaction costs provided by the online market, along with simplistic navigation with autonomous customization to improve information accessibility, can help to boost user satisfaction and purchases on online real estate platforms (Ullah et al., 2021), and it can be seen that implementing these measures requires multidisci-plinary research and collaborations. Highlighting the considerations related to the human factors (e.g., how much information and what information should be communicated to the user), the computing factors (how can this be designed and implemented into the system and inter-face to communicate information to the user effectively), and the interaction between the two (how often, what points and in which format should this information be communicated considering UX factors such as feedback and security depending on application scenarios) is crucial. Second, most studies in the review are qualitative in nature, implying a need for other methods in data collection to address the role of transparency across different contexts. Quantitative studies would have the benefit of developing an understanding of which elements of transparency would benefit the user and how best to utilize data in a way to complement transparency. A recent study (Anshu et al., 2022) pro-vided an example of such quantitative studies, showing that trans-parency can significantly moderate associations between perceived risk and consumers’ resistance behavior in the so-called “value co-creation” process where consumers generate, share, and gain access to resources and conversations with the business (Barile et al., 2021). Furthermore, more studies are necessary to investigate whether communicating transparency in data collection and usage to users (and in which format) will improve the users’ informed consent and trust in the long run. Third, the reviewed articles tended to discuss transparency relating to information disclosure (i.e., what information is presented and how). Little research has investigated factors from human elements and users’ information processing and perceived transparency at the time of interacting with a technology or online platform. For instance, for a specific target group (e.g., users with cognitive impairment or mental health conditions), it is necessary to examine how design affects these users’ cognitive processing specifically to ensure not only effective but also efficient communication and interaction. Fourth, potential risks of new technologies and their applications remain unclear or debatable to researchers themselves, not to mention transparency for users. Further research is required to achieve full un-derstanding of the potential impact of such technologies. This also ap-plies to the uncertainty of marketing innovations, which makes companies averse to innovations suggested by research, as they fear such innovations may negatively impact their profit. Therefore, more large- scale studies in applicable contexts are necessary to generate reliable guidelines. Finally, future research and guidelines on transparency need to consider different applications and contexts including social, economic, cultural, and environmental factors. It may therefore be challenging to achieve adequate trade-offs regarding balancing the potential risks or compromises (e.g., algorithmic accuracy and efficiency) involved in enhancing transparency with its benefits. 6. Conclusions Transparency is currently more a utopian concept than a reality due to a lack of consensus and implementation of good practices. Trans-parency in persuasive technology, immersive technology and online marketing is expected to be realized with stronger regulatory frame-works around user protection and increasingly open conversations around the hidden aspects of technology design, to benefit the produc-tion and consumption of online information and new technologies and thus lead to satisfactory user experiences and the sustainability of in-dustries. In short, potential solutions to improving transparency involve information visualization and a human-centered, personalized approach. Our findings are also transferable to other contexts relating to information communication in the digital world. Declaration of competing interest and funding source The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. This research is funded by GambleAware. GambleAware is a grant- making charity using best-practice in commissioning, including needs assessment, service-planning, evaluation and outcome-reporting to support effective, evidence-informed, quality-assured prevention of ComputersinHumanBehavior139(2023)10754512R. Wang et al.                                                                                                                  gambling harms. Guided by a public health model, GambleAware commissions integrated prevention services on a national scale and in partnership with expert organizations and agencies, including the UK National Health Service, across three areas of activity: universal pro-motion of a safer environment (primary); selective intervention for those who may be ’at risk’ (secondary); and, direct support for those directly affected by gambling disorder (tertiary). http://www.about.gamble aware.org The authors alone are responsible for the views expressed in this article, which do not necessarily represent the views, decisions or policies of the institutions with which they are affiliated. Data availability No data was used for the research described in the article. References Adshead, S., Chan, Y. S., Lavender, T., Wilkinson, L., & Schoentgen, A. (2020). Mapping online advertising issues, and the industry and regulatory initiatives aimed at addressing them. Plum Consulting. https://assets.publishing.service.gov.uk/gov ernment/uploads/system/uploads/attachment_data/file/898866/Mapping_online_ advertising_issues__and_the_industry_and_regulatory_initiatives_aimed_at_addressi ng_them.pdf. Alnsour, M. (2018). Internet-based relationship quality: A model for Jordanian business- to-business context. Marketing and Management of Innovations, 4(January 2018), 161–178. https://doi.org/10.21272/mmi.2018.4-15 American Psychiatric Association [APA]. (2013). Diagnostic and statistical manual of mental disorders (5th ed.). American Psychiatric Association. Amor, N., & Yahia, I. (2022). Investigating blockchain technology effects on online platforms transactions: Do risk aversion and technophilia matter? Journal of Internet Commerce, 21(3), 271–296. https://doi.org/10.1080/15332861.2021.1961188 Anshu, K., Shankar, A., Behl, A., Pereira, V., & Laker, B. (2022). Impact of barriers of value co-creation on consumers’ innovation resistance behavior: Investigating the moderation role of the dart model. Technological Forecasting and Social Change, 184. https://doi.org/10.1016/j.techfore.2022.122033 Bakhshian, S., & Lee, Y. A. (2021). Social acceptability and product attributes of smart apparel: Their effects on consumers’ attitude and use intention. Journal of the Textile Institute, 1–10. https://doi.org/10.1080/00405000.2021.1898138 Barile, S., Bassano, C., Piciocchi, P., Saviano, M., & Spohrer, J. C. (2021). Empowering value co-creation in the digital age. Journal of Business & Industrial Marketing, ahead- of-print. https://doi.org/10.1108/JBIM-12-2019-0553 Bateman, J. A. (2017). Multimodality and genre. Issues for information design. In A. Black, P. Luna, O. Lund, & S. Walker (Eds.), Information design: Research and practice (pp. 204–225). London: Routledge. Benedikt, A. F. (2002). On doing the right thing at the right time. In P. Sipiora, & J. S. Baumlin (Eds.), Rhetcoric and kairos, essays in history, theory and praxis (pp. 226–236). Albany: State University of New York Press. Berdichevsky, D., & Neuenschwander, E. (1999). Towards an ethics of persuasive technology. Communications of the ACM, 42, 51–58. Berger, Z., Evans, N., Phelan, A., & Silverman, R. (2020). Covid-19: Control measures must be equitable and inclusive. BMJ, 368, m1141. https://doi.org/10.1136/bmj. m1141 Beulens, A., Broens, D.-F., Folstar, P., & Hofstede, G. (2005). Food safety and transparency in food chains and networks: Relationships and challenges. Food Control, 16(6), 481–486. https://doi.org/10.1016/j.foodcont.2003.10.010 Blouin, M. C., Lee, R. L., & Erickson, G. S. (2018). The impact of online financial disclosure and donations in nonprofits. Journal of Nonprofit & Public Sector Marketing, 30(3), 251–266. https://doi.org/10.1080/10495142.2018.1452819 Boerman, S. C. (2020). The effects of the standardized instagram disclosure for micro-and meso-influencers. Computers in Human Behavior, 103, 199–207. https://doi.org/ 10.1016/j.chb.2019.09.015 Boerman, S. C., & van Reijmersdal, E. A. (2020). Disclosing influencer marketing on YouTube to children: The moderating role of para-social relationship. Frontiers in Psychology, 10(January), 1–15. https://doi.org/10.3389/fpsyg.2019.03042 Boers, S. N., Jongsma, K. R., Lucivero, F., Aardoom, J., Büchner, F. L., de Vries, M., Honkoop, P., Houwink, E. J. F., Kasteleyn, M. J., Meijer, E., Pinnock, H., Teichert, M., van der Boog, P., van Luenen, S., van der Kleij, R. M. J. J., & Chavannes, N. H. (2020). Series: eHealth in primary care. Part 2: Exploring the ethical implications of its application in primary care practice. The European Journal of General Practice, 26(1), 26–32. https://doi.org/10.1080/13814788.2019.1678958 Bostrom, A. (2003). Future risk communication. Futures, 35(6), 553–573. https://doi. org/10.1016/S0016-3287(02)00100-3 Brignull, H. (2011). Dark patterns: Deception vs. Honesty in UI design. Interaction design, usability. https://alistapart.com/article/dark-patterns-deception-vs-honesty-in-ui-de sign/. Brown, M., O’Neill, N., van Woerden, H., Eslambolchilar, P., Jones, M., & John, A. (2016). Gamification and adherence to web-based mental health interventions: A systematic review. JMIR Ment Health, 3(3). https://doi.org/10.2196/mental.5710 Burr, C., & Cristianini, N. (2019). Can machines read our minds?. In Minds and machines (Vol. 29)Springer Netherlands. https://doi.org/10.1007/s11023-019-09497-4. Issue 3. Busser, J. A., & Shulga, L. V. (2019). Involvement in consumer-generated advertising: Effects of organizational transparency and brand authenticity on loyalty and trust. International Journal of Contemporary Hospitality Management, 31(4), 1763–1784. https://doi.org/10.1108/IJCHM-10-2017-0685 Bussone, A., Stumpf, S., & O’Sullivan, D. (2015). The role of explanations on trust and reliance in clinical decision support systems. In 2015 international conference on healthcare informatics (pp. 160–169). https://doi.org/10.1109/ICHI.2015.26 Campbell, C., & Evans, N. J. (2018). The role of a companion banner and sponsorship transparency in recognizing and evaluating article-style native advertising. Journal of Interactive Marketing, 43(2018), 17–32. https://doi.org/10.1016/j. intmar.2018.02.002 Caraban, A., Karapanos, E., Campos, P., & Gonçalves, D. (2018). Exploring the feasibility of subliminal priming on web platforms. In ACM international conference proceeding series. https://doi.org/10.1145/3232078.3232095 Caraban, A., Karapanos, E., Gonçalves, D., & Campos, P. (2019). 23 ways to nudge: A review of technology-mediated nudging in human-computer interaction. In Conference on human factors in computing systems - proceedings. https://doi.org/ 10.1145/3290605.3300733. May. Carter, B. A., Iles, G. N., Raju, R., Afful, A. M., Maj, R., Dao, T., Terkildsen, M., Lobzin, V., Bouya, Z., Parkinson, M., Le May, S., Choy, S., Hordyniec, P., Hordyniec, B., Currie, J., Skov, T., & Peake, I. D. (2022). Rmit university’s practical space weather prediction laboratory. Journal of Space Weather and Space Climate, 12. https://doi. org/10.1051/swsc/2022025 Cho, I., Dou, W., Wartell, Z., Ribarsky, W., & Wang, X. (2012). Evaluating depth perception of volumetric data in semi-immersive VR. In AVI ’12: Proceedings of the international working conference on advanced visual interfaces (pp. 266–269). https:// doi.org/10.1145/2254556.2254606 Cialdini, R. B. (2007). Influence: The psychology of persuasion. New York: Collins. Cohen, R., & Hiller, J. S. (2009). What’s mine is mine; what’s yours is mine: Private ownership of ICTs as a threat to transparency. Ethics and Information Technology, 11 (2), 123–131. https://doi.org/10.1007/s10676-009-9196-8 Cornell University Library. (2020). BEE 3299: Sustainable Development: Google vs. Web of Science (and other library databases), what’s the difference?. https://guides.library. cornell.edu/c.php?g=519668&p=3553730. Correia, F., Martins, A., & Waikel, A. (2022). Online financing without fintech: Evidence from online informal loans. Journal of Economics and Business, 121. https://doi.org/ 10.1016/j.jeconbus.2022.106080 D’Alfonso, S., Phillips, J., Valentine, L., Gleeson, J., & Alvarez-Jimenez, M. (2019). Moderated online social therapy: Viewpoint on the ethics and design principles of a web-based therapy system. Journal of Medical Internet Research, 21(12). https://doi. org/10.2196/14866 Deci, E. L., & Ryan, R. M. (2000). The " what " and " why " of goal pursuits: Human needs and the self-determination of behavior. Psychological Inquiry, 11(4), 227–268. https://doi.org/10.1207/S15327965PLI1104_01 Dijksterhuis, A., Aarts, H., & Smith, P. K. (2005). The power of the subliminal: On subliminal persuasion and other potential applications. In R. R. Hassin, J. S. Uleman, & J. A. Bargh (Eds.), The new unconscious (pp. 1–51). Oxford University Press. DiStaso, M. W., & Bortree, D. S. (2012). Multi-method analysis of transparency in social media practices: Survey, interviews and content analysis. Public Relations Review, 38 (3), 511–514. https://doi.org/10.1016/j.pubrev.2012.01.003 Dodoo, N. A., & Youn, S. (2020). Snapping and chatting away: Consumer motivations for and outcomes of interacting with Snapchat AR ad lens. Telematics and Informatics, 57. Drosatos, G., Nalbadis, F., Arden-Close, E. J., Baines, V., Bolat, E., Vuillier, L., Kostoulas, T., Budka, M., Wasowska, S., Bonello, M., Brown, J., Corner, T., McAlaney, J., Phalp, K., & Ali, R. (2018). Enabling responsible online gambling by real-time persuasive technologies. Complex Systems Informatics and Modeling Quarterly, 17, 44–68. https://doi.org/10.7250/csimq.2018-17.03 Ehninger, D. (1972). Contemporary rhetoric: A reader’s coursebook. Glenview, IL: Scott, foresman and company. Scott, Foresman and Company. Ellison, N., & Hardey, M. (2014). Social media and local government: Citizenship, consumption and democracy. Local Government Studies, 40(1), 21–40. https://doi. org/10.1080/03003930.2013.799066 Elshout, M., Elsen, M., Leenheer, J., Loos, M., & Luzak, J. (2016). Study on consumers’ attitudes towards terms and conditions (T&Cs). Final report. European Commission. htt ps://ec.europa.eu/info/sites/default/files/terms_and_conditions_final_report_en.pdf. Esterhuyse, L. (2019). Towards corporate transparency: The link between inclusion in a socially responsible investment index and investor relations practices. Bottom Line, 32(4), 290–307. https://doi.org/10.1108/BL-03-2019-0081 Fafner, J. (1997). Retorikkens brændpunkt (Vol. 2). Rhetorica Scandinavia. Faggion, C. M. J., Bakas, N. P., & Wasiak, J. (2017). A survey of prevalence of narrative and systematic reviews in five major medical journals. BMC Medical Research Methodology, 17(176). https://doi.org/10.1186/s12874-017-0453-y Fleming, T., Bavin, L., Stasiak, K., Hermansson-Webb, E., Merry, S., & Cheek, C. (2016). Serious games and gamification for mental health: Current status and promising directions. Frontiers in Psychiatry, 7(215). https://doi.org/10.3389/ fpsyt.2016.00215 Fogg, B. J. (2002). Persuasive technology: Using computers to change what we think and do. Ubiquity, 5. https://doi.org/10.1145/764008.763957 Frank, L. E. (2020). What do we have to lose? Offloading through moral technologies: Moral struggle and progress. Science and Engineering Ethics, 26(1), 369–385. https:// doi.org/10.1007/s11948-019-00099-y Fricker, P. (2019). Virtual reality for immersive data interaction. Landscape Architecture Frontiers, 7(2), 153–159. https://doi.org/10.15302/J-LAF-20190216 Froehlich, J. E., Findlater, L. F., Ostergren, M., Ramanathan, S., Peterson, J., Wragg, I., Larson, E., Fu, F., & Bai, M. (2012). The design and evaluation of prototype eco- feedback displays for fixture-level water usage data. CHI ’12: Proceedings of the ComputersinHumanBehavior139(2023)10754513R. Wang et al.                                                                                                                  SIGCHI Conference on Human Factors in Computing Systems, 2367–2376. https://doi. org/10.1145/2207676.2208397 Gambling Commission. (2022a). Industry statistics – july 2022 revision. The latest statistics cover the period between April 2020 to March 2021. https://www.gambling commission.gov.uk/statistics-and-research/publication/industry-statistics-july-2 022-revision. Gambling Commission. (2022b). Consumer attitudes towards gambling advertising (2019 research). https://www.gamblingcommission.gov.uk/statistics-and-research/publi cation/consumer-attitudes-towards-gambling-advertising-2019-research. GDPR.EU. (2018). General data protection regulation (GDPR). Proton technologies AG. https://gdpr.eu/tag/gdpr/. Gelfand, M., Jackson, J., Pan, X., Nau, D., Dagher, M., Van Lange, P., & Chiu, C. (2020). The importance of cultural tightness and government efficiency for understanding COVID-19 growth and death rates. PsyArXiv[Preprint] https://doi.org/10.31234/osf. io/m7f8a. Goodie, A. S. (2005). The role of perceived control and overconfidence in pathological gambling. Journal of Gambling Studies, 21, 481–502. https://doi.org/10.1007/ s10899-005-5559-1 GOV.UK. (2021). Gambling-related harms evidence review: Summary. https://www.gov. uk/government/publications/gambling-related-harms-evidence-review/gambli ng-related-harms-evidence-review-summary#:~:text=Our%20economic%20analysi s%20estimated%20that,million%20and%20%C2%A32.12%20billion. Gram-Hansen, S. B. (2019). Family wearables–what makes them persuasive? Behaviour & Information Technology, 1–13. https://doi.org/10.1080/0144929X.2019.1694993, 0 (0). Gram-Hansen, S. B., & Gram-Hansen, L. B. (2013). On the role of ethics in Persuasive Design. Ethicomp, 2013. Granados, N., Gupta, A., & Kauffman, R. J. (2010). Information transparency in B2C markets: Concepts, framework, and research agenda. Information Systems Research, 21(2), 207–226. Granados, N., Gupta, A., & Kauffman, R. J. (2012). Online and offline demand and price elasticities: Evidence from the air travel industry. Information Systems Research, 23 (1), 164–181. https://doi.org/10.1287/isre.1100.0312 Gray, C. M., Kou, Y., Battles, B., Hoggatt, J., & Toombs, A. L. (2018). The dark (patterns) side of UX design. In Conference on human factors in computing systems - proceedings, 2018-april(april). https://doi.org/10.1145/3173574.3174108 Greenberg, S., Boring, S., Vermeulen, J., & Dostal, J. (2014). Dark patterns in proxemic interactions: A critical perspective. DISP, ’14, 523–532. https://doi.org/10.1145/ 2598510.2598541 Grimani, A., Gavine, A., & Moncur, W. (2020). An evidence synthesis of strategies, enablers and barriers for keeping secrets online regarding the procurement and supply of illicit drugs. International Journal of Drug Policy, 75, Article 102621. https://doi.org/10.1016/j.drugpo.2019.102621 Gunaratne, J., & Nov, O. (2015). Informing and improving retirement saving performance using behavioral economics theory-driven user interfaces. In Conference on human factors in computing systems - proceedings. https://doi.org/10.1145/ 2702123.2702408, 2015-April(April 2015), 917–920. Gupta, A., Cecil, J., Pirela-Cruz, M., & Ramanathan, P. (2019). A virtual reality enhanced cyber-human framework for orthopedic surgical training. IEEE Systems Journal, 13 (3), 3501–3512. https://doi.org/10.1109/JSYST.2019.2896061 Gusenbauer, M., & Haddaway, N. (2020). Which academic search systems are suitable for systematic reviews or meta-analyses? Evaluating retrieval qualities of google scholar, PubMed and 26 other resources. Research Synthesis Methods, 11, 181–217. https://doi.org/10.1002/jrsm.1378 Jacucci, G., Spagnolli, A., Freeman, J., & Gamberini, L. (2014). Symbiotic interaction: A critical definition and comparison to other human-computer paradigms. In G. Jacucci, L. Gamberini, J. Freeman, & A. Spagnolli (Eds.), Symbiotic interaction. Symbiotic 2015. Lecture notes in computer science (Vol. 8820, pp. 3–20). Cham: Springer. https://doi.org/10.1007/978-3-319-13500-7. Kalyuga, S., Ayres, P., Chandler, P., & Sweller, J. (2003). The expertise reversal effect. Educational Psychologist, 38, 23–31. Kaptein, F., Broekens, J., Hindriks, K., & Neerincx, M. (2017). Personalised self- explanation by robots: The role of goals versus beliefs in robot-action explanation for children and adults. In 2017 26th IEEE international symposium on robot and human interactive communication (pp. 676–682). RO-MAN. https://doi.org/10.1109/ ROMAN.2017.8172376. Karagür, Z., Becker, J.-M., Klein, K., & Edeling, A. (2022). How, why, and when disclosure type matters for influencer marketing. International Journal of Research in Marketing, 39(2), 313–335. https://doi.org/10.1016/j.ijresmar.2021.09.006 Karga, S., & Satratzemi, M. (2019). Using explanations for recommender systems in learning design settings to enhance teachers’ acceptance and perceived experience. Education and Information Technologies, 24, 2953–2974. https://doi.org/10.1007/ s10639-019-09909-z Khalil, M., Prinsloo, P., & Slade, S. (2018). User consent in MOOCs - micro, meso, and macro perspectives. International Review of Research in Open and Distance Learning, 19 (5), 62–79. https://doi.org/10.19173/irrodl.v19i5.3908 Kim, T., Barasz, K., & John, L. K. (2019). Why am i seeing this ad? The effect of ad transparency on ad effectiveness. Journal of Consumer Research, 45(5), 906–932. https://doi.org/10.1093/jcr/ucy039 Lamming, R. C., Caldwell, N. D., Harrison, D. A., & Phillips, W. (2001). Transparency in supply relationships: Concept and practice. Journal of Supply Chain Management, 37 (3), 4–10. https://doi.org/10.1111/j.1745-493X.2001.tb00107.x Langer, E. J. (1975). The illusion of control. Journal of Personality and Social Psychology, 32, 311–328. Lappas, G., Triantafillidou, A., Deligiaouri, A., & Kleftodimos, A. (2018). Facebook content strategies and citizens’ online engagement: The case of Greek local governments. The Review of Socionetwork Strategies, 12(1), 1–20. https://doi.org/ 10.1007/s12626-018-0017-6 Lee, M. K., Kiesler, S., & Forlizzi, J. (2011). Mining behavioral economics to design persuasive technology for healthy choices. In Proceedings of the international conference on human factors in computing systems. CHI 2011. https://doi.org/ 10.1145/1978942.1978989. Malik, M. I., & Ahsan, R. (2019). Towards innovation, co-creation and customers’ satisfaction: A banking sector perspective. Asia Pacific Journal of Innovation and Entrepreneurship, 13(3), 311–325. https://doi.org/10.1108/APJIE-01-2019-0001 Marsh, E., Vallejos, E. P., & Spence, A. (2022). The digital workplace and its dark side: An integrative review. Computers in Human Behavior, 128. https://doi.org/10.1016/j. chb.2021.107118 Mazurek, G., & Małagocka, K. (2019). What if you ask and they say yes? Consumers’ willingness to disclose personal data is stronger than you think. Business Horizons, 62 (6), 751–759. https://doi.org/10.1016/j.bushor.2019.07.008 Mejia, J., Urrea, G., & Pedraza-Martinez, A. J. (2019). Operational transparency on crowdfunding platforms: Effect on donations for emergency response. Production and Operations Management, 28(7), 1773–1791. https://doi.org/10.1111/poms.13014 Melenbrink, N., & King, N. (2015). Fulldome interfacing: A real-time immersive environment as a tool for design. In CAADRIA 2015 - 20th international conference on computer-aided architectural design research in asia: Emerging experiences in the past, present and future of digital architecture (pp. 221–230). Hagras, H. (2018). Toward human-understandable, explainable AI. Computer, 51(9), Metcalfe, J. S., Alban, J., Cosenzo, K., Johnson, T., & Capstick, E. (2010). Field testing of 28–36. https://doi.org/10.1109/MC.2018.3620965 Hanson, D., Olney, A., Pereira, I. A., & Zielke, M. (2005). Upending the uncanny valley. Proceedings of the National Conference on Artificial Intelligence, 1728–1729. Harris, A., Islam, S., Qadir, J., & Khan, U. (2017). Persuasive technology for human development: Review and case study. EAI Endorsed Transactions on Serious Games. https://doi.org/10.4108/eai.8-12-2017.153401 Harris, L., & Rae, A. (2009). Social networks: The future of marketing for small business. Journal of Business Strategy, 30(5), 24–31. https://doi.org/10.1108/ 02756660910987581 He, T. (2022). Online content platforms, copyright decision-making algorithms and fundamental rights protection in China. Law, Innovation and Technology, 14(1), 71–94. https://doi.org/10.1080/17579961.2022.2047519 Heidrich, D., Oberdorfer, S., & Latoschik, M. E. (2019). The effects of immersion on harm-inducing factors in virtual slot machines. In 26th IEEE conference on virtual reality and 3D user interfaces, VR 2019 - proceedings (pp. 793–801). https://doi.org/ 10.1109/VR.2019.8798021 Hoffmann, H., Stefani, O., & Patel, H. (2006). Extending the desktop workplace by a portable virtual reality system. International Journal of Human-Computer Studies, 64 (3), 170–181. https://doi.org/10.1016/j.ijhcs.2005.08.003 Holdsworth, J., & Apeh, E. (2017). An effective immersive cyber security awareness learning platform for businesses in the hospitality sector. In IEEE 25th international requirements engineering conference workshops (REW) (pp. 111–117). https://doi.org/ 10.1109/REW.2017.47 House of Commons. (2019). Immersive and addictive technologies fifteenth Report of session 2017–19. Digital, culture, media and sport committee. House of Commons. https://p ublications.parliament.uk/pa/cm201719/cmselect/cmcumeds/1846/1846.pdf. Huang, C., Ni, J., Lu, R., & Shen, X. S. (2019). Online advertising with verifiable fairness. In ICC 2019 - 2019 IEEE international conference on communications (ICC) (pp. 1–6). https://doi.org/10.1109/ICC.2019.8761762 Jacobsen, L. H., Knudsen, A. K., Krogh, E., Pallesen, S., & Molde, H. (2007). An overview of cognitive mechanisms in pathological gambling. Nordic Psychology, 59, 347–361. tele-operation versus shared and traded control for military assets: An evaluation involving real-time embedded simulation and soldier assessment. In Proc. SPIE 7692, unmanned systems technology XII. https://doi.org/10.1117/12.850597, 769206. Metcalfe, J. S., Marathe, A. R., Haynes, B., Paul, V. J., Gremillion, G. M., Drnec, K., Atwater, C., Estepp, J. R., Lukos, J. R., Carter, E. C., & Nothwang, W. D. (2017). Building a framework to manage trust in automation. Micro- and Nanotechnology Sensors, Systems, and Applications, IX(May), 10194. https://doi.org/10.1117/ 12.2264245, 101941U. Micklitz, H.-W., Palka, P., & Panagis, Y. (2017). The empire strikes back: Digital control of unfair terms of online services. Journal of Consumer Policy, 40(3), 367–388. Miller, G. R. (2002). On being persuaded, some basic distinctions. In J. P. Dillard, & M. Pfau (Eds.), The persuasion handbook, developments in theory and practice. Moere, A. V., Mieusset, K. H., & Gross, M. (2004). Visualizing abstract information using motion properties of data-driven infoticles. In Proc. SPIE 5295, visualization and data analysis 2004. https://doi.org/10.1117/12.539238 Mori, M., MacDorman, K. F., & Kageki, N. (2012). The uncanny valley [from the field]. IEEE Robotics and Automation Magazine, 19(2), 98–100. Naiseh, M., Jiang, N., Ma, J., & Ali, R. (2020). Explainable recommendations in intelligent systems: Delivery methods, modalities and risks. In The 14th international …, Query date: 2020-04-16 13:43:28. https://www.researchgate.net/profile/Raian _Ali/publication/340264457_Explainable_Recommendations_in_Intelligent_Systems _Delivery_Methods_Modalities_and_Risks/links/5e80ac2792851caef4aa2d24/Expl ainable-Recommendations-in-Intelligent-Systems-Delivery-Method. Nukarinen, T., Rantala, J., Korpela, K., Browning, M., Istance, H., Surakka, V., & Raisamo, R. (2022). Measures and modalities in restorative virtual natural environments: An integrative narrative review. Computers in Human Behavior, 126. https://doi.org/10.1016/j.chb.2021.107008 Oinas-Kukkonen, H., & Harjumaa, M. (2009). Persuasive systems design: Key issues, process model, and system features. Communications of the Association for Information Systems, 24(1). ComputersinHumanBehavior139(2023)10754514R. Wang et al.                                                                                                                  Oultaw, J. (2018). Virtual harassment: The social experience of 600+ regular virtual reality (VR) users. The Extended Mind. https://www.extendedmind.io/the-extende d-mind-blog/2018/04/04/2018-4-4-virtual-harassment-the-social-experience-of- 600-regular-virtual-reality-vrusers. Spiers, A. J., Van Der Linden, J., Wiseman, S., & Oshodi, M. (2018). Testing a shape- changing haptic navigation device with vision-impaired and sighted audiences in an immersive theater setting. IEEE Transactions on Human-Machine Systems, 48(6), 614–625. https://doi.org/10.1109/THMS.2018.2868466 Oyibo, K., Sahu, K. S., Oetomo, A., & Morita, P. P. (2022). Factors influencing the Stevens, J. L., Spaid, B. I., Breazeale, M., & Esmark Jones, C. L. (2018). Timeliness, adoption of contact tracing applications: Systematic review and recommendations. Frontiers in Digital Health, 4. https://doi.org/10.3389/fdgth.2022.862466, 862466–862466. Papadogiannakis, E., Papadopoulos, P., Kourtellis, N., & Markatos, E. P. (2021). User tracking in the post-cookie era: How websites bypass GDPR consent to track users. In Proceedings of the web conference 2021 (pp. 2130–2141). https://doi.org/10.1145/ 3442381.3450056. WWW ’21. Parris, L., Dapko, J., Arnold, R., & Arnold, D. (2016). Exploring transparency: A new framework for responsible business management. Management Decision, 54(1), 222–247. https://doi.org/10.1108/MD-07-2015-0279 Pase, S., Hare, G., Hogg, J. L., Thoennes, S., & Connors, C. (2014). Panel - ethics and emerging technology: Ethical concerns from a cognitive, media & technology focused psychology perspective concerning augmented reality, privacy, and sigularity. In 2014 IEEE international symposium on ethics in science, technology and engineering (pp. 14–16). ETHICS 2014. https://doi.org/10.1109/ ETHICS.2014.6893438. Pettersen, I. N., & Boks, C. (2008). The expert-layperson divide in design for sustainable behaviour: Related risks and the value of involvement. Proceedings of NordDesign 2008 Conference, 190–199. transparency, and trust: A framework for managing online customer complaints. Business Horizons, 61(3), 375–384. https://doi.org/10.1016/j.bushor.2018.01.007 Stumpf, S., Rajaram, V., Li, L., Wong, W., Burnett, M., Dietterich, T., Sullivan, E., & Herlocker, J. (2007). Interacting meaningfully with machine learning systems. Tapsell, J., Akram, R. N., & Markantonakis, K. (2018). Consumer centric data control, tracking and transparency - a position paper. In Proceedings - 17th IEEE international conference on trust, security and privacy in computing and communications and 12th IEEE international conference on big data science and engineering (pp. 1380–1385). Trustcom/BigDataSE. https://doi.org/10.1109/TrustCom/BigDataSE.2018.00191, 2018. Toronto, C. E., & Remington, R. (Eds.). (2020). A step-by-step guide to conducting an integrative review. Springer. https://doi.org/10.1007/978-3-030-37504-1. Tovares, N., Cagan, J., & Boatwright, P. (2013). Capturing consumer preference through experiential conjoint analysis. In ASME 2013 international design engineering technical conferences and computers and information in engineering conference. https://doi.org/ 10.1115/DETC2013-12549 Tsao, W. Y. (2019). Building the long bridge between visitors and customers through online general reviews. Online Information Review, 43(2), 201–218. https://doi.org/ 10.1108/OIR-01-2016-0028 Pinder, C. (2017). The anti-influence engine: Escaping the diabolical machine of Turilli, M., & Floridi, L. (2009). The ethics of information transparency. Ethics and pervasive advertising. In CHI EA ’17: Proceedings of the 2017 CHI conference extended abstracts on human factors in computing systems (pp. 770–781). https://doi.org/ 10.1145/3027063.3052762 Pollach, I. (2005). A typology of communicative strategies in online privacy (Vol. 62, pp. 221–235). https://doi.org/10.1007/sl0551-005-7898-3, 3. Preece, J., Rogers, Y., & Sharp, H. (2002). Interaction design: Beyond human-computer interaction. John Wiley & Sons, Inc. https://arl.human.cornell.edu/879Rea dings/Interaction Design - Beyond Human-Computer Interaction.pdf. Information Technology, 11(2), 105–112. https://doi.org/10.1007/s10676-009-9187- 9 Tutty, E., Hickerton, C., Adamski, M. M., & Metcalfe, S. A. (2019). Personal genomic testing for nutrition and wellness in Australia: A content analysis of online information. Nutrition and Dietetics, 76(3), 263–270. https://doi.org/10.1111/1747- 0080.12516 Tversky, A., & Kahneman, D. (1971). Belief in the law of small numbers. Psychological Bulletin, 76, 105–110. https://doi.org/10.1037/h0031322 Rawlins, B. (2008). Give the emperor a mirror: Toward developing a stakeholder Tzanetakis, M., Kamphausen, G., Werse, B., & von Laufenberg, R. (2016). The measurement of organizational transparency. Journal of Public Relations Research, 21 (1), 71–99. https://doi.org/10.1080/10627260802153421 van Reijmersdal, E. A., Rozendaal, E., Hudders, L., Vanwesenbeeck, I., Cauberghe, V., & van Berlo, Z. M. C. (2020). Effects of disclosing influencer marketing in videos: An eye tracking study among children in early adolescence. Journal of Interactive Marketing, 49, 94–106. https://doi.org/10.1016/j.intmar.2019.09.001 Roloff, J., & Aßlander, M. (2010). Corporate autonomy and buyer-supplier relationships: The case of unsafe mattel toys. Journal of Business Ethics, 97(4), 517–534. https:// doi.org/10.1007/s10551-010-0522-1 Ryan, R. M., & Deci, E. L. (2017). Self-determination theory: Basic psychological needs in motivation, development, and wellness. The Guilford Press. https://doi.org/10.1521/ 978.14625/28806 Sandoval, E. B. (2019). Addiction to social robots: A research proposal. In ACM/IEEE international conference on human-robot interaction (Vols. 526–527). https://doi.org/ 10.1109/HRI.2019.8673143, 2019-March. Saunder, L., & Berridge, E.-J. (2015). Immersive simulated reality scenarios for enhancing students’ experience of people with learning disabilities across all fields of nurse education. Nurse Education in Practice, 15(6), 397–402. https://doi.org/ 10.1016/j.nepr.2015.04.007 Savelyev, A. (2018). Copyright in the blockchain era: Promises and challenges. Computer Law & Security Report, 34(3), 550–561. https://doi.org/10.1016/j.clsr.2017.11.008 Schneider, T., Hois, J., Rosenstein, A., Ghellal, S., Theofanou-Fülbier, D., & Gerlicher, A. (2021). ExplAIn yourself! Transparency for positive UX in autonomous driving. In CHI ’21: Proceedings of the 2021 CHI conference on human factors in computing systems (pp. 1–12). https://doi.org/10.1145/3411764.3446647 Seizov, O., & Wulf, A. J. (2020). Communicating legal information to online customers transparently: A multidisciplinary multistakeholderist perspective. Journal of International Consumer Marketing, 33(2), 159–177. https://doi.org/10.1080/ 08961530.2020.1742841 Seizov, O., Wulf, A. J., & Luzak, J. (2019). The transparent trap: A multidisciplinary perspective on the design of transparent online disclosures in the EU. Journal of Consumer Policy, 42(1), 149–173. https://doi.org/10.1007/s10603-018-9393-0 Selinger, E., Sadowski, J., & S, T. (2014). Gamification and morality. In S. Walz, & S. Deterding (Eds.), The gameful world: Approaches, issues, applications (pp. 371–392). Cambridge: MIT Press. Shahri, A., Hosseini, M., Phalp, K., Taylor, J., & Ali, R. (2016). Exploring and conceptualising software-based motivation within enterprise. Lecture Notes in Business Information Processing, 267, 241–256. https://doi.org/10.1007/978-3-319- 48393-1_17 Sharma, S., Singh, G., Sharma, R., Jones, P., Kraus, S., & Dwivedi, Y. K. (2020). Digital health innovation: Exploring adoption of COVID-19 digital contact tracing apps. IEEE Transactions on Engineering Management. https://doi.org/10.1109/ TEM.2020.3019033 Singh, S., Paul, J., & Dhir, S. (2021). Innovation implementation in asia-pacific countries: A review and research agenda. Asia Pacific Business Review, 27(2), 180–208. https:// doi.org/10.1080/13602381.2021.1859748 Spahn, A. (2011). And lead us (not) into persuasion…? Persuasive technology and the ethics of communication. Science and Engineering Ethics, 18(4). transparency paradox. Building trust, resolving disputes and optimising logistics on conventional and online drugs markets. International Journal of Drug Policy, 35, 58–68. https://doi.org/10.1016/j.drugpo.2015.12.010 UK Parliament. (2021). Public health England: Gambling-related harms review. House of Lords Library. https://lordslibrary.parliament.uk/public-health-england-gambling -related-harms-review/. Ullah, F., Sepasgozar, S. M. E., Shirowzhan, S., & Davis, S. (2021). Modelling users’ perception of the online real estate platforms in a digitally disruptive environment: An integrated kano-sisqual approach. Telematics and Informatics, 63. https://doi.org/ 10.1016/j.tele.2021.101660 Vaccaro, A., & Madsen, P. (2009). Corporate dynamic transparency: The new ICT-driven ethics? Ethics and Information Technology, 11(2), 113–122. https://doi.org/10.1007/ s10676-009-9190-1 Van Leeuwen, J. P., Hermans, K., Jylh¨a, A., Quanjer, A. J., & Nijman, H. (2018). Effectiveness of virtual reality in participatory urban planning. ACM International Conference Proceeding Series, 128–136. https://doi.org/10.1145/3284389.3284491. February 2019. Verbeek, P.-P. (2006). Persuasive Technology and Moral Responsibility toward an ethical framework for persuasive technologies. In PERSUASIVE’06: Proceedings of the first international conference on persuasive technology for human well-being. https://pdfs. semanticscholar.org/a913/dda4d6d16996d9c22a0a27c4019f48932d18.pdf. Wang, R., Nacenta, M., Zhu, X., & Dai, Q. (2017). Visualization of health behavior data for children and young adults. In 2017 computing conference (pp. 1207–1216). https://doi.org/10.1109/SAI.2017.8252244 Waycott, J., Wadley, G., Baker, S., Ferdous, H. S., Hoang, T., Gerling, K., Headleand, C. J., & Simeone, A. L. (2018). Manipulating reality? Designing and deploying virtual reality in sensitive settings. In DIS 2018 - companion publication of the 2018 designing interactive systems conference (pp. 411–414). https://doi.org/ 10.1145/3197391.3197401 Wierzynski, C. (2020). The challenges and opportunities of explainable AI. https://ai.intel. com/the-challenges-and-opportunities-of-explainable-ai/. Wohl, M. J. A., & Enzle, M. E. (2002). The deployment of personal luck: Sympathetic magic and illusory control in games of pure chance. Personality and Social Psychology Bulletin, 28(10), 1388–1397. https://doi.org/10.1177/014616702236870 Wohl, M. J. A., Parush, A., Kim, H. S., & Warren, K. (2014). Building it better: Applying human–computer interaction and persuasive system design principles to a monetary limit tool improves responsible gambling. Computers in Human Behavior, 37, 2124–2132. https://doi.org/10.1016/j.chb.2014.04.045 Yurica, C. L., & DiTomasso, R. A. (2005). Cognitive distortions. In A. Freeman, S. H. Felgoise, C. M. Nezu, A. M. Nezu, & M. A. Reinecke (Eds.), Encyclopedia of cognitive behavior therapy (pp. 117–122). Springer. https://doi.org/10.1007/0-306- 48581-8_36. Zhang, Y., Luo, D., Li, J., & Li, J. (2021). Study on collision detection and force feedback algorithm in virtual surgery. Journal of Healthcare Engineering, 1–12. https://doi.org/ 10.1155/2021/6611196 Zheng, Y., & Toribio, J. R. (2021). The role of transparency in multi-stakeholder educational recommendations. User Modeling and User-Adapted Interaction: The Journal of Personalization Research, 31(3), 513–540. https://doi.org/10.1007/ s11257-021-09291-x ComputersinHumanBehavior139(2023)10754515