Artificial Intelligence 116 (2000) 67–85Think!: A unified numerical–symbolic knowledgerepresentation scheme and reasoning systemChristian Vilhelm a;(cid:3), Pierre Ravaux a, Daniel Calvelo b,Alexandre Jaborska b, Marie-Christine Chambrin b, Michel Boniface aa Lab. Biomathématiques, Faculté de Pharmacie de Lille II,3, rue du Pr. Laguesse, BP83, 59006 Lille Cedex, Franceb UPRES EA n 2689, ITM, Batiment Vancostenobel, CH&U Lille, 59037 Lille Cedex, FranceReceived 27 November 1997; received in revised form 7 May 1999AbstractMore and more applications of artificial intelligence technologies are made in biomedical softwareand equipment. These applications are multiple: intelligent alarms, intelligent monitoring, diagnosissupport, . . . . Several different knowledge representation schemes are already in use: decision trees,first-order logic expert systems, calculations (mathematical modeling), trained neural networksimulations, . . . . All these techniques have their own preferred field of application, and they donot overlap. Building a complete diagnosis support tool would require the use of several of thesetechniques. The problem is therefore communication between these very different systems and thecomplexity of the composite result. This paper describes the Think! formalism: a unified symbolic-connectionist representation scheme which tries to subsume some of the precited formalisms. Beingable to integrate these knowledge representation schemes in a single model enables us to use existingknowledge bases and existing knowledge extraction techniques to make them communicate and worktogether. (cid:211) 2000 Published by Elsevier Science B.V. All rights reserved.Keywords: Knowledge representation; Symbolic reasoning; Numeric reasoning; Truth propagation1. IntroductionOne of the main problems in an intensive care unit (ICU) is the multiplicity of the sourcesof information in the room and the service itself (pieces of biomedical equipment, papers,(cid:3)Corresponding author. Email: cvilhelm@phare.univ-lille2.fr. This work was funded by the Centre HospitalierRégional Universitaire de Lille through the Institut de Technologie Médicale.0004-3702/00/$ – see front matter (cid:211)PII: S 0 0 0 4 - 3 7 0 2 ( 9 9 ) 0 0 0 9 5 - 82000 Published by Elsevier Science B.V. All rights reserved.68C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85wall blackboard, the medical staff, the patient himself). All this information has to beread, integrated, interpreted and synthesized by the clinician before he can take therapeuticaction. The amount of information available renders its interpretation increasingly difficult.To assist the staff, ICU rooms have been equipped with computers running monitoringsoftware. These monitoring systems can have several objectives, requiring differenttechnologies:– Basic display centralisation stations with alarm monitoring, currently available fromequipment manufacturers (HP, SpaceLabs, . . . ): typically these do not use knowledge-based techniques to interpret data but rather advanced display techniques along withsignal analysis.– Building more advanced monitoring systems with data interpretation requires know-ledge-based techniques to be used [6]. But, systems using a single knowledgerepresentation scheme tend to be limited to a single monitoring task, such asforecasting, alarm analysis or therapeutic planning. Specialization stems fromadapting the representation formalism used to the type of acquired data andthe objectives of the task. Thus, the global monitoring systems proposed [4,9]generally make use of several knowledge representation techniques to completethese different analysis tasks requiring different abstraction levels. Knowledgerepresentation formalisms are described in [12]. Problems when dealing with multipleknowledge sources mainly concern communication between different representationformalisms, the choice of each subsystem competence, and the scheduling of all thesesources. These lead to very complex systems that need high performance hardware torun, and are difficult to maintain.As part of the Aiddiag project [11], we are trying to build a central low-cost workstation,placed at the patient’s bedside, acting as a unique information display and interpretationsystem, to help the clinician.– The system will have to acquire and interpret different types of data, thus requiringthe simultaneous use of several different knowledge representation techniques. Theamount of work done in the ICU monitoring field should be taken into account, andexisting knowledge bases should be used when available. But we should be able to addnew parts to the knowledge base when developed, without having to stop the wholesystem for testing and modifications.– All these different knowledge-based subsystems using different formalisms shouldbe capable of interoperating easily. The resulting system must be kept as simple aspossible so that it can be integrated into a low cost platform or embedded hardware,and be easily maintained.– The knowledge base should be easily understandable, so that the clinician can judgethe relevance of rules, and possibly develop his own rules and integrate them into thesystem to test their accuracy.In this paper, we have tried to explore a way of dealing with the multiplicity ofknowledge representation schemes, data types and learning strategies by proposing aunified representation model [5] subsuming several formalisms currently used in ICUmonitoring systems. By integrating all these representation techniques into a uniqueformalism, we can make them communicate and cooperate effectively, without the needfor run-time translations or scheduling.C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8569Think! is based on a connectionist structure, but loosely connected to have explicit paths.We have introduced symbolic representation objects into this network, together with theconcept of propagating truth values associated with these symbols. An early model wasdescribed in [8]. What we describe here is a more complex model, but with a wider range ofrepresentation and reasoning possibilities, together with an implementation of the model.We shall first describe this formalism, beginning with a global overview of the baseelements. We shall see how these are generalizations of existing concepts used inexisting formalisms (neurons, links), with added capabilities. We shall then examine moreadvanced concepts and the dynamics of the system, how forward and backward chainingare implemented, as well as learning. We shall explain how existing formalisms areimplemented using the Think! system and how they can be made to interoperate.2. Structural and operational definition of conceptsThe model is based on three structure elements: containers, processes and tubes.The structure elements define a network representing the knowledge base. Reasoning isachieved by propagating excitations through the network from one element to another andby making calculations on these excitations. Excitations, the basic data of the network,are a pair consisting of a numerical value and an associated truth value. Fig. 1 showsan example of a Think! network. Containers hold the information (excitations), processesmake calculations, tubes transport the information. Groups can be defined as sub-networks,representing functional units of the global network (we can see four groups in Fig. 1defining two production rules, a totally connected neural network and a decision tree).These sub-networks exchange information through tubes.We shall now describe the elements constituting the network, the dynamics of thesystem, and some advanced capabilities of the model.Fig. 1. A sample Think! network.70C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85Fig. 2. Container definition and graphical representation.2.1. Structure elements2.1.1. Containers as holders and input/output slots of the networkContainers are symbol holders with a persistent internal state. They are the memoryof the system, and the elements by which an external system can communicate with thenetwork.A container is defined by:(cid:15) A symbol, naming the container.(cid:15) A numerical value and a truth value, representing the state of the symbol (container),its excitation.(cid:15) One or more output tubes, transmitting the container’s state (excitation) to otherelements of the network.(cid:15) A maximum of one input tube, receiving new values that change the internal state ofthe container.(cid:15) An activation procedure, executed whenever the internal state of the containerchanges.(cid:15) A firing function, executed when a question arrives in the container. The firingfunction is responsible for communicating the internal state of the container to theelement requiring it, and, if this state is unknown (truth value D Unknown), forestablishing it.(cid:15) Inheritance relations, to organize containers with father–child ‘is-a’ relations.(cid:15) Group membership, to organize structure elements in functional groups.(cid:15) The capacity to pass or block. Blocking containers filter input and propagate it onlyif it is different from the current internal state of the container, this input replacingthe internal stored value. Passing containers just propagate whatever input theyreceive.Containers do not provide means of implementing learning. Input/output from anexternal program is carried out by modifying container excitation, by querying it, or byusing the activation function, which can be an external program.2.1.2. Processes as calculation elementsProcesses are the active elements of the network, performing calculations and tests onthe excitations.C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8571Fig. 3. Process definition and graphical representation.A process is defined by:(cid:15) One or more input excitations (cid:18)i .i D 1; : : : ; n/ coming from input tubes. Each inputis active by default (triggering execution of the activation procedure upon arrivalof an excitation) but can be specified as inactive (it will not trigger the activationprocedure upon arrival of an excitation). Input tubes are labeled and input excitationsare dated upon arrival, enabling calculations that rely on delays between incomingpropagations, or on the age of an input value (e.g., to determine outdated excitations).(cid:15) One output excitation (cid:27) , transmitted through one or more output tubes; output tubesare not labeled.(cid:15) A threshold value.(cid:15) A condition test.(cid:15) An activation function fa, computing (cid:27) from (cid:18)i which decides if this output excitationshould be propagated to the output tubes, using the condition and threshold asparameters.(cid:15) A firing function ff, responsible for answering questions directed at the process aboutthe status of its output excitation.(cid:15) A learning function fl , responsible for making modifications to the network or to theprocess characteristics to adapt its calculated output to the preferred output receivedfrom one of the output tubes.(cid:15) Group membership and hormone sensitivities, to organize structure elements infunctional groups and to apply global modifiers to elements in these groups.In fact, processes are a generalization of formal neurons as found in neural networksimulations. This enables us to implement neural network architectures and behaviourdirectly. The difference from classic neural networks is that the activation, firing andlearning functions can vary from process to process, and can be specific (whatever theircomplexity) to processes or subnetworks.2.1.3. Tubes as special linksTubes are oriented links, propagating excitations from one element (container orprocess) to other elements of the network (containers, process or tubes), and attenuatingor amplifying the excitations propagated. A tube has a length, and propagations travel intothe tube at a given speed, after respecting a given delay before beginning.72C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85Fig. 4. Tube definition and graphical representation.Tubes are a generalization of synapses used in neural network simulators. Theintroduction of delay, length and speed (thus a certain ‘travel schedule’ of excitationsgoing through the tube) enables us to introduce temporal notions and dependencies intothe knowledge base. Length alone makes it possible to model networks that have a certainphysical coherence, with notions of proximity and distance.2.2. Dynamic elements2.2.1. ExcitationsAn excitation is the association of a numerical value with its truth value.Truth value is a continuous value between True and False, with two additional specialvalues: Unknown and Mu. Unknown means that there is no information about the truthvalue, which is different from the median value between True and False, meaning neithertrue nor false. The Mu value means that the information carried has no sense, so therecan be no truth value. These special truth values must be taken into account in theimplementation of logical operators used in the process functions.Due to implementation constraints, all numerical values used in Think! must respect thesame format. If we choose to implement the value contained in the excitations as 4-pointfuzzy intervals, all other numbers will have to be 4-point fuzzy numbers: truth values,weights, thresholds, group memberships, hormone concentrations.When an excitation is in transit in a tube, it is called a propagation.2.2.2. Ticks and intrinsic time baseThe network is updated in three steps:(1) all propagations are moved further down the tube they are in (according to theirspeed and the length of the tube),(2) all required functions (and only those required) are carried out,(3) new excitations are given to tubes.These three basic steps constitute a tick. Ticks do not have a fixed duration, because thesystem waits for these three steps to finish before proceeding to the next tick.We evaluated the possibility of fixed-duration ticks, but this would require takingdeferred propagations into account, in case the program, or computer, does not have enoughtime to perform all the necessary propagations during the time allocated to the tick. Wedetermined that the complexity of managing desynchronized propagations (deferred one ormore clock steps) is much greater than the added complexity of taking an external sourcefor a real-time clock. This real-time clock can be transferred to where it is needed—from aC. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8573passing container propagating the value of the clock to the inactivated inputs of processesthat need it.2.3. Advanced concepts2.3.1. Groups determine subnetworksStructure elements (containers, processes, tubes) can be grouped together. Each of theelements in the group has a degree of membership, thus defining a fuzzy group, or set. Thisleads to several functionalities:(cid:15) By defining functional groups, one can isolate different separated sub-networks eachperforming a specific task in the global system. This way, complex networks becomemore understandable. It is possible to copy whole functional groups to make themrepeat the same functionality for other data, or to make minor modifications to thestructure or characteristics of elements and compare the behaviour of the two groups.Network design becomes modular.(cid:15) With the combination of hormones, one can make global modifications in thebehaviour of elements included in the group.2.3.2. Hormones introduce global parametrizationsHormones behave like global variables for all elements contained in a specific groupwhere the hormone is present. It is possible for some objects (processes and tubes) to definehormone sensitivities. When a certain hormone concentration is specified for a certaingroup, the hormone concentration applied to an object is the concentration of the hormonein the group multiplied by the object’s degree of membership in the group.The hormone concentration can serve as a parameter in any of the functions ofthe process—activation, firing or learning. They can serve as modifiers to certaincharacteristics of the processes (threshold), or tubes (propagated truth value, propagatednumerical value, weight, speed, delay).The modification induced is always calculated from the base value of the modifiedparameter. When an object has several hormone sensitivities for the same parameter, allthe modifications are calculated separately from the base value and added.modification D base value (cid:3) group membership(cid:3) hormone concentration in the group(cid:3) hormone sensitivityfinal value D base value C modification2.3.3. Inheritance leads to immediate ‘is-a’ relationshipsContainers may be organized in father–child relationships, which can be consideredas ‘is-a’ relations. A child container will then inherit all inheritable output tubes fromits fathers (inherited tubes are inheritable, allowing for multi-level inheritance). Theseinherited tubes will propagate the new excitations of the container as well as the outputtubes directly connected to the container.74C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85When several containers try to propagate an excitation through the same inherited tube,a conflict resolution must occur. The algorithm used is implementation-dependent, andcurrently uses a ‘last-win’ strategy.3. Dynamics of the system3.1. Forward chainingThe main operating mode of a Think! network is a feed-forward system, with addedtime-related possibilities given by delays and speeds, and with advantages from thedifferentiation of all processes thanks to a potentially wide variety of functions, fromcomplex calculus to logical functions (operators).Inputs are introduced into the system by exciting the input containers, i.e., changing theirexcitation and making them propagate and initiate execution of the activation function ofprocesses.When a blocking container gets an excitation, whether from its input tube or from anexternal program, it tests if this new excitation is different from the currently stored one.If it is different, the new excitation replaces the old one, the activation procedure is carriedout and the new excitation is given to the output tubes. When a passing container receivesan excitation, it performs the activation procedure and propagates the excitation withouttesting. The activation procedure can be used to display the new value, or to activatesome external module (program), making it possible for a Think! network to control itsenvironment.Excitation arrives in a tube as input and then becomes propagation.The tube has a certain delay, defined in ticks. Propagation waits for ‘delay’ ticks beforebeginning to travel. After this delay, propagation advances ‘speed’ distance units at eachtick.When the propagation arrives at the end of the tube (after length/speed ticks), either thenumerical value or the truth value is multiplied by the weight of the tube, depending on theaction defined for the tube. Weight, transmitted numerical value or transmitted truth valuecan be modified by various hormone actions when the tube is included in one or moregroups. The resulting excitation is then transmitted to the output objects of the tube.When the output object of a tube is another tube (for example, tube1 going fromcontainer A to tube2, tube2 going from process B to process C), any element of the outputexcitation of the tube (numerical value, truth value of tube1) can modify or replace anyelement of the output tube (numerical value, truth value, weight, speed, delay of tube2), asspecified when creating the source tube (tube1). This is in fact similar to hormone actions,but on a more local scale, hormones being global modifiers.When an excitation arrives on an active input of a process, the activation function of theprocess is performed. The process activation function, fa, computes.(cid:27); (cid:28) / D fa.(cid:18)1; : : : ; (cid:18)n; condition; threshold/where (cid:18)i = (value, truth, tick of arrival) from tube i and n = number of input tubes. (cid:27) is theoutput excitation of the process, (cid:28) is the decision (boolean) whether to propagate (cid:27) or not.C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8575fa can be any function. This activation function is executed no more than once per tick,if at least one new excitation has come from an active input. Inputs being labeled, fa cancompute nonsymmetrical functions, such as subtractions, divisions or comparisons.When an excitation arrives on an inactive input of a process, the activation function ofthe process is not performed. But, when it occurs at a later tick, the input value from theinactive tube will be the most recent excitation received from this tube. In fact, excitationsarriving at processes are retained at the interface between the tube and the process, so thatfunctions performed have a value for each input and this value is the latest excitation tohave arrived at the input.Activation functions of processes are performed only if needed (if propagation comesfrom an active input). This is interesting from a performance point of view. Furthermore,it allows for the detection of active reasoning paths, where propagations often travel, andless active zones of the network, only activated when needed. This is very useful to debugthe system, by detecting falsely active processes or tubes, according to the current inputsof the system.3.2. Backward chainingAnother mode of reasoning used in knowledge based systems is backward chaining,either for deduction systems (goal driven), or to obtain learning capabilities (neuralnetwork simulations). Think! integrates both operating modes through the firing andlearning functions of its elements and by transmitting information in the reverse directionof tubes, by propagating what we call reverse propagations. These reverse propagations areof two types:– Questions: meaning that the destination of the tube transmitting the reverse propaga-tion requires the source’s value and truth to establish its own value and truth.– Learning: learning reverse propagation contains a numerical value and truth value,representing the preferred output of the element. This means that the destination ofthe tube (origin of the reverse propagation) specifies to the process that its output isincorrect, and gives it the right values.Reverse propagations when arriving at processes are handled by specific functions.(cid:15) The firing function ff, is used when a question arrives from one of the output tubes. Ifthe activation function decides to propagate (cid:27) , it is propagated to the output tube fromwhich the question came. If fa decides not to propagate (cid:27) , the question is consideredpending and questions are reverse propagated to one or more input tubes of theprocess. The question is pending until an excitation is propagated to the output tube.This extends backward chaining to production systems, with the ability to modify thesolution search strategy from process to process, adapting it to the semantics of theinputs.(cid:15) The learning function fl , is used when a learning reverse propagation arrives from oneof the output tubes. The learning function should reduce the error between the outputof the activation function (cid:27) and the excitation transmitted by the learning reversepropagation (cid:18) 0. The long term goal of the learning function is to have(cid:27) D (cid:18) 0 D fa.(cid:18)1; :::; (cid:18)n; condition; threshold/:76C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85To achieve this goal, it has several possibilities:(cid:0) By modifying the weights of the input tubes, which is similar to the gradientbackpropagation algorithm applied in neural network simulations [12], with theadded possibility of modifying weight differently on each input tube, or ignoringsome input tubes, depending on their meaning for the activation function. The rateparameter used in neural network simulations can be implemented as a hormoneconcentration or by a specific input of the process. This offers the advantage ofchanging the rate parameter during learning, and specifying different rates fordifferent processes.(cid:0) By sending learning reverse propagations to one or several input tubes. This willbe used when correcting only a small part of the error, letting elements situatedfurther away the inputs correct the rest.(cid:0) By adjusting the hormone sensitivities of the process, or the sensitivities of inputtubes. Or by modifying hormone concentrations.(cid:0) By changing other characteristics of the elements, such as the delays or speeds oftubes, or by altering the structure of the network.All these possibilities do not have to be used in a single function: standard learningfunctions will use only one or two simultaneously to avoid learning functions that aretoo complex and CPU-hungry. It is interesting however that subsystems can be madeto use different learning techniques.4. Examples and translating existing formalismsWe are now going to show how existing knowledge representation schemes can be‘translated’ into our formalism and retain all their capabilities.4.1. Feed-forwardA forward-chaining production rules system can be implemented easily. Fig. 5 shows avery simple production rule.Fig. 5. Simple production rule: IF Container1 AND Container2 THEN Container3.C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8577Fig. 6. Decision tree generic node.There is no pattern-matching since applicable rules (tubes) are directly connected to thefacts (containers). Of course, more complex rules are possible. Because the inputs of aprocess are labeled, it is possible to have very complex rules with any number of premisesand complex relationships between them (multiple and, or, . . . ), but this makes the networkmuch less readable, and thus less explicit.Rules can be inherited from upper containers, making it possible to establish generalrules.Another example of a feed-forward production system is the decision tree generatedby induction algorithms, like C4.5 [7]. The translation between C4.5 output files and theThink! formalism has been implemented in an automatic translator directly reading theoutput files from C4.5 software. The tree node implementation is shown in Fig. 6.Results obtained by the Think! network are exactly the same as the results obtained byC4.5 software, with the same test set, showing the adequation of the formalism to representproduction rules.Implementation of trained neural network simulations is fairly straightforward. Neuralnets are usually organized in layers and we have three types of neurons: input neurons,output neurons and neurons hidden in several layers.Input neurons just transmit their values to the first layer. These neurons are connected toseveral or all neurons in the next layer. Fig. 7 shows the implementation of the three neurontypes: input neurons just transmit their values without modification (containers), layer andoutput neurons modify their outputs (processes). The activation function computes the sumof the inputs and propagates if the numerical value is positive. An output neuron gives itsvalue to a container. The container is visible from the outside of the system, displaying theresult, or activates some external program.This was tested by setting-up an automatic translator program that takes a save file froman existing network simulation application [10] and creates the corresponding network inFig. 7. Input, layer, output neurons expressed in the Think! formalism.78C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85our formalism. The functions used for activation and learning functions are the same asin the original neural network. Of course, neural networks with other architectures can beimplemented as well, the only constraint being the containers needed for input and output.We have also implemented rules from a fuzzy expert system described in [13]. Thevalues contained in the excitations were 4-point fuzzy numbers. The rule implementation inThink! was again straightforward and the results obtained are the same as those describedby those authors.4.2. Backward chainingBackward chaining is obtained using the firing function, when an output container isasked a question. The firing function of the process is responsible for choosing the rightsolution-search strategy (depth first, breadth first, or any other type of search); it is possibleto have a mixed-search method by having different firing functions in the same network fordifferent processes. The firing function of the container determines if the question shouldbe sent upwards through tubes or should be sent to an external program (to ask the userthrough a graphical interface for example).For the Fig. 5 example: if Container3 is asked a question, the firing function sends aquestioning reverse propagation to the Process. The firing function of the process thendecides to send questions simultaneously to Container1 and Container2, thus making abreadth-first search. As Container1 and Container2 have no inputs, each of them will askthe user (or an external program) a question.When a container’s excitation is computed by a combination of subsystems (as in Fig. 1),a question addressed to one of the output containers can pass through many subsystemsbefore arriving at the input containers, where the question is asked outside the network.4.3. LearningThe implementation of untrained neural networks is the same as the implementation oftrained neural networks, but all tube weights are set at 1.0 at creation time and the learningfunction is an implementation of the gradient back-propagation algorithm (that modifiestube weights). The rate parameter can be specified with a hormone concentration in thegroup containing the whole neural network. The activation function can be the standardsigmoid function or any other. Training is carried out by exciting the input containers withthe input pattern of the training set and, when the network has computed its output, byreverse propagating the response pattern from output containers.Another learning possibility is to adapt the weights of tubes used in production rules(representing confidence or certainty). By hiding the conclusions of the network andintroducing (reverse-propagating) the conclusions found by the clinicians (entered throughan advanced graphical interface) [2], the learning functions of the processes used in therules will adapt the weights to progressively better the answers of the network (reinforcingthe rules leading to correct conclusions by increasing the weights of the tubes used tofind these conclusions, or weakening the rules leading to false conclusions). In this way,we could implement rules given by the clinicians and let the system refine them. After acertain learning time, it would be possible to remove inadequate rules.C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85794.4. Composite systemTo show the integration capabilities of Think! and its ability to implement all the steps ofa system, part of the reimplementation of our respiratory monitoring system RespAid [1] isshown, as it is now integrated into the Aiddiag system. The first stage of RespAid includesthe filtering and symbolising of a numerical parameter acquired from a monitoring device(through a serial interface). The first part of the network (Fig. 8) takes the raw numbersgiven by the monitoring device and filters them by computing a weighted average of 3successive measure points. The weights are directly implemented as tube weights andFig. 8. Signal filtering.Fig. 9. Parameter symbolisation.80C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85the length of the tubes is chosen so that all weighted measures arrive at the same tickin the averaging process. These filtered points are then sent back to the Aiddiag systemand displayed.A second stage (Fig. 9) evaluates the trend of the parameter’s temporal evolution as asymbol (“stable”, “unstable”, “increasing” or “decreasing”).First, the network takes three values of the filtered parameter, at time t (now), t (cid:0) 5minutes (300 seconds) and t (cid:0) 10 minutes (600 seconds): x D p.tm/, x0 D p.t (cid:0) 5m/,x00 D p.t (cid:0) 10m/. The “t-” processes query the Aiddiag system to collect the values of theparameter. The parameter is considered stable if the standard deviation of the three pointsis below a certain value, which is computed by processes “psqr” (standard deviation) and“pthr” (threshold). The networks computes x (cid:0)x0 and x0 (cid:0)x00 (processes diff) and then testsif these differences are all negative (process pneg) or all positive (ppos). If ppos outputsthe truth value ‘true’, then the parameter is considered to be increasing. If pneg outputsthe truth value ‘true’, the parameter is considered to be decreasing. If ppos and pneg bothoutput the truth value ‘false’ (tested by the process pfalse), the parameter is consideredunstable. The lengths of the tubes are set so that all conclusions arrive simultaneously inthe final containers. Weights of the tubes are all neutral (1.0).These trends are then transmitted to a decision tree to detect potential medicalcomplications. The trends are also used by the display mechanism.We have shown here experimentally the ability of the model to integrate severalsubsystems using different formalisms in a single network, and yet retaining their specialcapabilities. By getting all these different subsystems to use a single formalism and datatype, it is simple to connect them with tubes to let them interchange data, conclusions orfacts. We have shown that different types of subsystems could be constructed, ranging frommathematical calculus networks to expert systems and learning neural network simulations.We will now examine how this formalism has been implemented.5. ImplementationThe definition of the formalism was made parallel to its implementation, with stresson high performance and low memory usage, offering a program able to run on low costhardware.The current implementation is based on a client-server model. A knowledge server looksafter all the reasoning and the management of the knowledge network. Clients then accessthe server to create, delete or modify structure elements, modify or read the excitation ofcontainers concurrently. The server is constantly running, performing activation, learningand firing functions as needed. Functions are executed during a tick only when one of theirinputs has been modified. When loops exist in the Think! network, the server is alwaysputting forward some propagations and executing some functions, but due to the nature ofthe reasoning mechanism, these loops do not have adverse effects on the performance ofthe program, in fact, loops are necessary for closed systems where the final conclusion isre-injected at one of the inputs, or for two subsystems controlling each other.The client is linked to a C library providing the API (Application ProgrammingInterface) to access the server through a network socket. A special C variable type has beenC. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8581created to be able to use special types for the numerical and truth values of an excitation,such as 4-point fuzzy intervals.The client also has the possibility of providing the server with special process functions(activation, firing or learning). When the function needs to be executed, the server sendsa message to the client along with all the function parameters. The client executes thefunction and sends back a message to the server containing the result. It is useful to havesome very specific functions performed by clients to test them before their integration inthe server. In a networked environment, it helps to distribute the calculation load.The application is written in C language and is available for the Linux operating system(Free Unix compatible, see http://www.linux.org). This implementation has been placedunder the GPL (GNU Public License).Any program or language able to link to a standard Unix library is able to accessa knowledge base system using our formalism. The current implementation has beenextensively tested on PC compatible computers equipped with a simple Pentium processorrunning at 133 MHz, with 32 Mbyte memory. The program performs about 15,000propagations per second and 15,000 activations par second with activation functionsintegrated into the server. Performance does not vary with the size of the network and scaleswell with processor performance. The steps of a tick are highly parallelizable and accordingto the locality of the processing, performance should scale well on multiprocessorcomputers, but this has not been tested. The program has a memory footprint of 1400kbyte and storage of the knowledge base is also memory efficient (84 bytes/propagation,468 bytes/container, 360 bytes/tube, 580 bytes/process).6. Discussion and conclusionWe have shown experimentally that several knowledge representation schemes can betranslated into our formalism and retain all their capabilities. We have also shown by ourimplementation of the Respaid system that several of these representation schemes couldinteroperate together and with numerical calculus parts in the same system.– The graphical representation is easily understandable by non-AI specialists. They canalso easily express their knowledge on paper using this graphical representation, but aC coder is necessary to implement the knowledge base into the program. In fact, one ofthe main drawbacks of our system is the lack of a graphical user interface. Conceivingand implementing such a graphical knowledge modeler is one of our urgent objectives.– Adding delays, speeds and lengths to the tubes makes it possible to enforce timingdependencies (order) between the activation of conclusions, or temporal reasoning.Of course, Think! does not have hard real-time capabilities, due to the nonconstantduration of a tick. But, with the introduction of an external clock or time source, aThink! network can take into account clock (wall) time and can be made to outputconclusions at fixed frequencies.– Adding weights helps to strengthen or to weaken conclusions found.The static nature of existing systems renders the evolution of the knowledge basedifficult. It is not rare to be forced to stop a program to add, delete or modify the knowledgebase used. The implementation of the Think system has been achieved in a totally dynamic82C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85manner, any object or characteristic of object or even the structure of the network can bemodified at run-time:– It is possible to attach or detach parts of a Think! network dynamically. This is usedin our Aiddiag system when we add or remove the knowledge base concerning acertain piece of biomedical equipment as it is required. For example, when Aiddiagis monitoring a patient and the clinical staff enters the room to connect a ventilator,Aiddiag dynamically adds the respiratory knowledge base to the pool of knowledgecurrently in use, and this without stopping or restarting the system. The new rules areimmediately available and used. When the respirator is disconnected, the respiratoryknowledge base is removed from the system without restarting the program.– By changing functions dynamically we can test several solutions one after the other.Even if the tested function is not included in the server, we can implement it in theclient and change it at run time without interrupting the server. It is possible to testseveral different structures by switching the connections between groups to make thesystem use one or the other.– By temporarily relocating the carrying-out of functions from the server to the clientone can trace what is happening in the network (content of propagations arriving,output of the function, . . . ), meaning that run-time debugging of the knowledge baseand of the functions can take place.– It is possible to remove unused knowledge or to dynamically remove rules leading tofalse conclusions, or just to redirect their output to another part of the network.– The dynamic nature of the implementation does not hamper its performance.Altering the structure or modifying characteristics of elements is done without anyperformance loss.Currently used systems or formalisms mainly use only a fixed, predetermined set ofoperators (disjunction and conjunction, sigmoid function and its inverse function. . . ). Sincewe want to be able to run all these formalisms in one unique system, we need to integrateall possible operators and functions. In the definition of the Think! formalism and in itsimplementation we decided not to put any restriction on the functions or operators usedthroughout the network.– Because there is no limitation on the activation functions, use can be made of onlysimple operators (AND, OR), or only simple calculations, like the sum-thresholdfunction in neural networks. But it is also possible to have other operators (min,max, . . . ) or more complex calculations (the membership function in a fuzzy interval).It is also possible to have very complex calculations made in a single process,like a polynomial equation solver where the coefficients are given by the excitationtransmitted by the input tubes.– The same possibility exists for the learning functions, no limits are put on them. Itis then possible to implement any learning algorithm by just changing the learningfunction, and to have different learning strategies in different sub-networks (groups).Due to the dynamic nature of the implementation, it is also possible to changethe learning strategy during run-time to adapt it to the current situation. It is theresponsibility of the learning function to choose between the several possibilitiesit has to change its inputs (tube weights, hormones, structure modification, . . . ). Itshould be noted that standard learning functions such as the neural network learningC. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8583function only uses a few. A Think! learning function is not forced to use all techniquesavailable.– Differentiating the firing functions makes a mixed-search backtracking strategypossible by changing the firing function from process to process. Firing questionsinto all input tubes at the same time is a breadth-first search, firing questions into thetubes one at a time is a depth-first search. It is possible to fire questions into severaltubes at once but not into all tubes.– Of course, this freedom can be a drawback. By differentiating the process functionstoo much or inadequately, the network can be rendered difficult to understand. It isthe responsibility of the conceiver of knowledge bases to limit the use of complexfunctions, or to choose a fixed set of operators and functions. Also, this freedom foroperators prevent us from proving the global completeness or consistency of a Think!network (it should be possible on subnetworks using only certain operators).7. PerspectivesAs we have shown in the examples, the Think! formalism enables us to use existingknowledge bases by translating them. It enables us to reuse existing knowledge extractiontechniques, like induction algorithms, pasting their results into our global system aftertranslation. The development of automatic translators enables people to use the Think!program to test their existing knowledge bases and exchange them with other people usingthe Think! program to compare their systems and results. When there is no translator, the CAPI provided by the implementation is very easy to use, and it is then possible to integrateThink! capabilities into existing programs.Expressing all these distinct knowledge bases in a single formalism and implementingthem in a single system means they are tightly integrated. Communication is carried out bysimple connections with tubes; conclusions of one system can be directly used by another.This integration makes it possible to compare results from different subsystems usingdifferent formalisms. This is a first and important step in creating a centralized monitoringworkstation.The graphical nature of this formalism and the simplicity of the dynamics (feed-forward) means non-AI specialists can express knowledge directly into simple Think!networks and understand existing rules expressed in the formalism (implementation ofthese networks still requires a C programmer). By providing a catalogue of existingprocesses implementing certain functions (operators, tests, simple calculus, display,interaction), we hope to provide the final users with a toolbox that will enable them toconceive whole programs by just connecting containers (representing variables) to chainsof processes expressing the required algorithm. This will require a graphical user interfaceand debugger.The freedom given to the available functions has enabled us to implement signalprocessing sub-networks directly connected to the symbolic interpretation system, withouthaving separate programs. The client–server architecture and the inherently parallelizablereasoning mechanism allows for the use of multiprocessor computers or networks ofcomputers to handle very large knowledge bases or very complex and processor hungry84C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–85activation functions without hampering the performance of the global system. As theimplementation is made in standard C language, it should be easily portable on othercomputer architectures.The flexibility of the implementation means functions are tested before being integratedinto the server; also, several different algorithms can be tested by modifying dynamicallythe functions or the structure of the network during run-time. This is important fordebugging the network and for testing new solutions. We are now investigating how tochange network structures dynamically to achieve advanced learning abilities and theblending of knowledge bases.The implementation has been integrated into the Aiddiag medical data acquisition, reportand interpretation system, in the RespAid module, to monitor the respiratory function of thepatient. The system is constantly fed with data acquired from biomedical equipment. Weare now implementing more complex knowledge bases which take into account additionalacquired parameters to make better interpretations of signals. We are also investigating thepossibility of incorporating Think! networks into the Aiddiag system to fully control theuser interface, by deciding which items to display, and to determine the screen layout fromthe medical status of the individual patient and the individual user in front of the computer(physician, nurse, . . . ).The Think! formalism and its implementation have been designed in respect of therequirements of the biomedical field. But we believe the resulting system is usable in awide variety of fields, due to the great flexibility of the model and its implementation.References[1] M.C. Chambrin, P. Ravaux, C. Chopin, J. Mangalaboyi, P. Lestavel, F. Fourrier, RESPAID: Computer-aided evaluation of respiratory data in ventilated critically ill patients, Internat. J. Clinical Monitoring andComputing 6 (1989) 211–215.[2] M.C. Chambrin, P. Ravaux, A. Jaborska, C. Beugnet, P. Lestavel, C. Chopin, M. Boniface, Introductionof knowledge bases in patient’s data management system: Role of the user interface, Internat. J. ClinicalMonitoring and Computing 12 (1995) 11–16.[3] M. Dojat, F. Pachet, Z. Guessoum, D. Touchard, A. Harf, L. Brochard, NéoGanesh: A working system forthe automated control of assisted ventilation in ICUs, Artificial Intelligence in Medicine 11 (2) (1997).[4] B. Hayes-Roth, R. Washington, D. Ash, R. Hewett, A Collinot, A. Viña, A. Seiver, Guardian: A prototypeintelligent agent for intensive-care monitoring, Artificial Intelligence in Medicine 4 (2) (1992) 165–185.[5] M. Minsky, Logical vs. analogical or symbolic vs. connectionist or neat vs. scruffy, Artificial Intelligence atMIT, Expanding Frontiers, Vol. 1, MIT Press, Cambridge, MA, 1990.[6] F.A. Mora, G. Passariello, G. Carrault, J.-P. Le Pichon, Intelligent patient monitoring and managementsystems: A review, IEEE Engineering in Medicine and Biology (1993) 23–32.[7] J.R. Quinlan, Improved use of continuous attributes in C4.5, J. Artificial Intelligence Res. 4 (1996).[8] P. Ravaux, C. Vilhelm, M. Boniface, M.-C. Chambrin, A neural approach to knowledge base systems, in:Proc. 14th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Paris, France, 1992, pp. 928–930.[9] D.F. Sittig, N.L. Pace, R.M. Gardner, E. Beck, A.H. Morris, Implementation of a computerized patientadvice system using the HELP clinical information system, Computers and Biomedical Research 22 (1989)474–487.[10] SNNS Stuttgart Neural Network Simulator: Users guide.[11] C. Vilhelm, A. Jaborska, M.-C. Chambrin, P. Ravaux, A software architecture for a medical data acquisition,report and interpretation system in intensive care unit, in: Proc. 18th Annual International Conference of theIEEE Engineering in Medicine and Biology Society, Amsterdam, The Netherlands, 1996.C. Vilhelm et al. / Artificial Intelligence 116 (2000) 67–8585[12] P.H. Winston, Artificial Intelligence, Addison-Wesley, Reading, MA, 1992.[13] Z. Zalila, P. Lezy, Contrôle longitudinal d’un véhicule autonome par régulateur hybride flou / classique, in:Proc. 5ème Conférence Internationale IPMU, Information Processing and Management of Uncertainty inKnowledge-Based Systems, Vol. 1, 1994, pp. 478–484.